{
  "title": "COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning",
  "url": "https://openalex.org/W3094751268",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4309956960",
      "name": "Ging, Simon",
      "affiliations": [
        "University of Freiburg"
      ]
    },
    {
      "id": "https://openalex.org/A2775477076",
      "name": "Zolfaghari, Mohammadreza",
      "affiliations": [
        "University of Freiburg"
      ]
    },
    {
      "id": "https://openalex.org/A2745122089",
      "name": "Pirsiavash, Hamed",
      "affiliations": [
        "University of Maryland, Baltimore County"
      ]
    },
    {
      "id": "https://openalex.org/A2745345130",
      "name": "Brox, Thomas",
      "affiliations": [
        "University of Freiburg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962934715",
    "https://openalex.org/W2900413183",
    "https://openalex.org/W2885775891",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2138621090",
    "https://openalex.org/W1906515132",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2804268694",
    "https://openalex.org/W2964073646",
    "https://openalex.org/W2112912048",
    "https://openalex.org/W877909479",
    "https://openalex.org/W2963389687",
    "https://openalex.org/W2965458216",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W2546938941",
    "https://openalex.org/W2964089981",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2964241990",
    "https://openalex.org/W2975706270",
    "https://openalex.org/W1573040851",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3002095953",
    "https://openalex.org/W3035356601",
    "https://openalex.org/W2769342113",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2962811161",
    "https://openalex.org/W2963017553",
    "https://openalex.org/W2139501017",
    "https://openalex.org/W1527575280",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2784025607",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2895347732",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2111078031",
    "https://openalex.org/W2949178656",
    "https://openalex.org/W2883910824",
    "https://openalex.org/W2803595284",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2963863119",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2970373903",
    "https://openalex.org/W2963843052",
    "https://openalex.org/W2144935315",
    "https://openalex.org/W2897439619",
    "https://openalex.org/W2975813532",
    "https://openalex.org/W2123024445",
    "https://openalex.org/W3035339529",
    "https://openalex.org/W2706729717",
    "https://openalex.org/W2948859046",
    "https://openalex.org/W1957706851",
    "https://openalex.org/W2963285578",
    "https://openalex.org/W2962869524",
    "https://openalex.org/W2994689640",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2965628639",
    "https://openalex.org/W2889185481",
    "https://openalex.org/W2808399042",
    "https://openalex.org/W3098232790",
    "https://openalex.org/W2115613106",
    "https://openalex.org/W2950111065",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3035635319",
    "https://openalex.org/W2796207103",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W262578090",
    "https://openalex.org/W2964700958",
    "https://openalex.org/W3016960123",
    "https://openalex.org/W2963916161",
    "https://openalex.org/W2613031639",
    "https://openalex.org/W3035265375",
    "https://openalex.org/W2950855501",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W3035237998",
    "https://openalex.org/W2828845773",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2963989428"
  ],
  "abstract": "Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities. The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters. All code is available open-source at https://github.com/gingsi/coot-videotext",
  "full_text": "COOT: Cooperative Hierarchical Transformer for\nVideo-Text Representation Learning\nSimon Ging1*, Mohammadreza Zolfaghari1*, Hamed Pirsiavash2, Thomas Brox1\n1University of Freiburg, 2University of Maryland Baltimore County\n1{gings, zolfagha, brox}@cs.uni-freiburg.de, 2 hpirsiav@umbc.edu\nAbstract\nMany real-world video-text tasks involve different levels of granularity, such as\nframes and words, clip and sentences or videos and paragraphs, each with distinct\nsemantics. In this paper, we propose a Cooperative hierarchical Transformer\n(COOT) to leverage this hierarchy information and model the interactions between\ndifferent levels of granularity and different modalities. The method consists of three\nmajor components: an attention-aware feature aggregation layer, which leverages\nthe local temporal context (intra-level, e.g., within a clip), a contextual transformer\nto learn the interactions between low-level and high-level semantics (inter-level,\ne.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to\nconnect video and text. The resulting method compares favorably to the state of\nthe art on several benchmarks while having few parameters. All code is available\nopen-source at https://github.com/gingsi/coot-videotext\n1 Introduction\nRepresentation learning based on both vision and language has many potential beneﬁts: visual\ngrounding[1–4]; visual learning with a more natural, almost self-supervised annotation process; and\ndirect applicability to cross-modal tasks, such as video retrieval by text[ 5–9], video summariza-\ntion [10], and automated indexing. This research direction has recently boomed [8, 11–17] also due\nto the success of self-attention in text analysis [18, 19] with its almost immediate applicability in the\ncross-modal context. Many different research foci are currently developing in this area, where some\nare concerned with large-scale pretraining to leverage the abundant data available [ 8, 11, 16, 17]\nto learn a joint embedding space, and others to bring in more explicit structure [ 20–22] or new\nlosses [17, 23] into the learning process.\nIn this paper, we focus on long-range temporal dependencies and propose a hierarchical model that\ncan exploit long-range temporal context both in videos and text when learning the joint cross-modal\nembedding. For instance, the action of “making tea” involves boiling water, pouring it into a cup,\nand then adding a tea bag. This action can take a long time and may have lots of details that\ndistinguish a particular style of making tea from other styles. To capture the whole temporal context,\nwe leverage the idea of a hierarchical model with losses that enforce the interaction within and\nbetween different hierarchy levels. The idea of such a hierarchy is generic and has been explored\nby several works [21, 22, 24] in the context of video-text learning. In addition, we use alignment\nlosses from Zhang et al. [21] and extend our baseline model with a new feature aggregation method\nfor the intra-level interactions between features and a new transformer-based module for inter-level\ninteractions (between local and global semantics). We consider three different levels of hierarchy:\nframe/word, clip/sentence and video/paragraph, visualized by the three blocks in Figure 1.\n*Equal contribution\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2011.00597v1  [cs.CV]  1 Nov 2020\nT-Transformer\nContextual\nTransformer\nIntra-level \nInteractions\nInter-level \nInteractions\nVideo\n \nFrame features Clip features\nvideo \nembedding\nAll frame features\nEncoder\nT-Transformer\n Attention-FA\nAttention-FA\nT-Transformer Attention-FA\nEncoder\nT-Transformer\nContextual\nTransformer\nText\n \nWord features Sentence features\nparagraph \nembedding\nAll word features\nEncoder\nT-Transformer\n Attention-FA\nAttention-FA\nT-Transformer Attention-FA\nEncoder\nAdd & Norm\nFeed Forward\nAdd & Norm\nMulti-Head \nAttention\nTemporal Transformer\n... ... ... ...\nFC\nPositional\nEncoding\nCross-modality\n cycle-consistency\nAlignment losses\nShared weights\nGlobal context \nfeatures flow\nAttention-FA\nAttention-aware \nFeature \naggregation\nT-Transformer Temporal \nTransformer\nFigure 1: Overview of COOT model (best viewed in color). The model consist of two branches:\none for video input (top) and one for text input (bottom). Given a video and a corresponding\ntext, we encode them to frame-level/word-level features. Features belonging to each segment\n(clip/sentence) are fed to a standard temporal transformer (T-Transformer) followed by the proposed\nfeature aggregation module (Attention-FA) to obtain clip/sentence-level features. Finally, a new\ncontextual transformer produces the ﬁnal video/paragraph embedding based on interactions between\nlocal context (clip/sentence features) and global context (all frames/words features). ℓL\nalign, ℓH\nalign,\nℓg\nalign and ℓCMC enforce the model to align the representations at different levels.\nTo model intra-level cooperation, we introduce an attention-aware feature aggregation layer to focus\non temporal interactions between low-level entities (Figure 1-Attention-FA).\nThis component replaces traditional sequence representation aggregation methods in transformers\nsuch as using a [CLS] token [11, 14, 15, 19] or mean pooling [ 25] with an attention-aware fu-\nsion. It leverages temporal context to encourage important entities to contribute more to the ﬁnal\nrepresentation of a sequence of frames or words.\nFor the inter-level cooperation, we introduce a contextual attention module, which enforces the\nnetwork to highlight semantics relevant to the general context of the video and to suppress the\nirrelevant semantics. This is done by modeling the interaction between low-level (clips-sentences)\nand high-level entities (global contexts), as shown in Figure 1-green region.\nIn addition to this architectural contributions, we introduce a new cross-modal cycle-consistency loss\nto enforce interaction between modalities and encourage the semantic alignment between them in the\nlearned common space. We show that enforcing two domains to produce consistent representations\nleads to substantially improved semantic alignment.\nIn summary, this paper contributes:\n• a hierarchical transformer architecture with a new attention-aware feature aggregation layer\nand a new contextual attention module;\n• a cross-modal cycle-consistency loss that encourages semantic alignment between vision\nand text features in the joint embedding space;\n• state-of-the-art results on video-text retrieval.\n2 Cooperative Hierarchical Transformer\nVideos and text descriptions naturally involve different levels of granularity. Every paragraph\ncontains multiple sentences, and each sentence is composed of several words. Similarly, videos have\na hierarchical semantic structure, even if it is not as exactly deﬁned as for text. Figure 1 illustrates the\noverview of the COOT model which consists of three levels: 1) A temporal transformer that captures\nthe relationships between frame/word features, 2) attention-aware feature aggregation to produce\nclip/sentence features (Section 2.2) and 3) a contextual transformer to produce ﬁnal video and text\nembeddings (Sec. 2.3). We use alignment losses from Zhang et al. [21] to align representations at\ndifferent granularity levels. In addition, we introduce a new cross-model cycle-consistency loss to\nconnect video and text (Sec. 3). In this section, we brieﬂy summarize the alignment losses from\nZhang et al. [21] and the standard transformer.\n2\n2.1 Preliminaries\nSemantic Alignment Losses. For the video-text alignment, Zhang et al. [21] leverage a contrastive\nloss to enforce the positive samples to stay in a close neighborhood and negative samples far apart [26–\n29]. Assuming the positive pair P = ( x,y), two negative pairs (x,y′) and (x′,y) expressed as\nN= {(x,y′),(x′,y)}, and a margin α, they deﬁne the following loss:\nL(P,N,α) = max(0,α + D(x,y) −D(x′,y)) + max(0,α + D(x,y) −D(x,y′)) (1)\nwhere D(x,y) = 1 −x⊺y/(∥x∥∥y∥) is the cosine distance of two vectors.\nTo align representations at clip-sentence (ϑk\ni, δk\ni), video-paragraph (ϑk, δk) and global context (gv,\ngp) levels, Zhang et al. [21] use the following losses:\nℓL\nalign =\n∑\nk∈D,i,k′̸=k,i′̸=i\nL((ϑk\ni,δk\ni),{(ϑk\ni,δk′\ni′ ),(ϑk′\ni′ ,δk\ni)},β)\nℓH\nalign =\n∑\nk∈D,k′̸=k\nL((ϑk,δk),{(ϑk,δk′\n),(ϑk′\n,δk)},α)\nℓg\nalign =\n∑\nk∈D,k′̸=k\nL((gk\nv,gk\np),{(gk\nv,gk′\np ),(gk′\nv ,gk\np)},αg)\n(2)\nHere, ϑk\ni denotes the embedding for the i-th clip of the k-th video and similarly δk\ni is the embedding\nof the i-th sentence of the k-th paragraph. α, αg and βare constant margins, and Dis a dataset of\nvideos with corresponding text descriptions. Zhang et al. [21] employed an additional loss to model\nthe clustering of low-level and high-level semantics in the joint embedding space:\nℓcluster =\n∑\nk∈D,i,k′̸=k,i′̸=i\nL((1,1),{(ϑk\ni,ϑk′\ni′ ),(δk′\ni′ ,δk\ni)},γ)\n+\n∑\nk∈D,k′̸=k\nL((1,1),{(ϑk,ϑk′\n),(δk′\n,δk)},η)\n(3)\nwhere γ and η both are constant margins. The (1,1) pairs denote that positive samples are not\nchanged. In short, the goal of this loss is to push apart embeddings for negative samples.\nNote. Due to the symmetrical design of the video and text branches in our model, from now on, we\nexplain only the video branch. For simplicity, we assume a single head in transformer formulations.\nAll transformers use residual connections.\nTemporal Transformer. We use standard attention-blocks [18] to learn frame and word represen-\ntations, as shown in Fig 1-Right. We learn two temporal transformers (T-Transformer); one for the\nvideo branch and another one for the text branch. Both have the same architecture. All T-Transformers\nin each branch share their weights. This module draws the relationship between temporal features\nand yields improved representations as output. Given a video vk, we ﬁrst encode all its frames to\nobtain the frame-level features {fk\ni,:}n\ni=1, where fk\ni,: are all frame-level features of the i-th clip for\nvideo vk (orange parts in Figure 1). We also consider all frame features (fk\n: ) of a video as extra input\nfor the global context computation (green parts in Figure 1). This yields {ˆfk\ni,:}n\ni=1 and ˆfk\n: .\n2.2 Intra-Level Cooperation\nStandard feature fusion methods consider each feature independently by average pooling or max\npooling. Hence, they miss the relationship between features to highlight the relevant features. Recent\ntransformers use a [CLS] token [11, 14, 15, 19] or average pooling [ 25] to obtain the aggregated\nfeatures. For example, when a person is cooking, objects on the table are more relevant than objects\non the wall or in the background. Therefore, we need to attend to speciﬁc features depending on\nthe context. There have been some attempts in other domains to design a context-aware feature\nfusion method [ 30–33]. However, we introduce an attention-aware feature aggregation module\n(Attention-FA in Fig. 1) for video-text transformers.\n3\nA woman is speaking about how to mix \ningredients and make chocolate cookies\nA woman speaking \nabout \nmaking \nchocolate chip cookies. \nSame woman combines \nand mixes the ingredients \nwith a hand mixer.\nShe adds \nit to the \ncookies dough and \nmixes it by hand\nPlaced in the oven \nand after they cool \nyou have cookies\nAdd & Norm\nFeed Forward\nAdd & Norm\nMulti-Head \nAttention\nAdd & Norm\nFeed Forward\nAdd & Norm\nMulti-Head \nAttention\nContextual Transformer\nMean\nConcat\nPositional\nEncoding\nFigure 2: Contextual Transformer (CoT). This module (right) encourages the model to optimize\nthe representations with respect to interactions between local and global context. In the third sentence,\nto know the type of dough ( cookie) the model should have information about the general context\nof the video (making chocolate cookies). Likewise, in the second sentence, to know that she is the\n\"same woman\", the model must be aware of the person’s identity throughout the video.\nSuppose we have a sequence with T feature vectors, denoted by X = {x1,...,x T}(e.g. ˆfk\ni,: =\n{ˆfk\ni,1,..., ˆfk\ni,T}). We set key K = X and utilize two learnable transformation weights W2 and W1\ntogether with two biases b1 and b2. The attention matrix Ais computed as:\nA= softmax(W2Q+ b2)T, Q = GELU(W1KT + b1), K= X (4)\nWe compute the ﬁnal feature as ˆx= ∑T\ni=1 ai ⊙xi, where ⊙denotes element-wise multiplication\nand ai is the i-th attention vector of Afor the i-th feature. This module differs from attention [18] in\ntwo aspects: (1) we use only two learnable weights for query (Q) and key (K) and then aggregate the\nvalues based on calculated scores; (2) the query equals to transformed keys (K) and then we apply\nthe activation function GELU [34, 35]. We feed {ˆfk\ni,:}n\ni=1 and ˆfk\n: to this component and obtain the\nclip-level ({ϑk\ni}n\ni=1) features and the global context for the video (gν).\n2.3 Inter-Level Cooperation\nBy modeling the interactions between local and global context, the network learns to highlight\nsemantics relevant to the general context of the video and to suppress the irrelevant ones: interactions\nbetween clip embeddings and the general context of the video; interactions between sentence\nembeddings and the general context of the text. As shown in Figure 2-Left, without knowing the\nglobal context, just from observing the frame in the third clip, there is no information about what\ntype of \"dough\" is involved. Also the \"same woman\" in the second clip could not be related to the\nwoman seen in the ﬁrst clip.\nThus, we propose a Contextual Transformer (CoT) in Figure 2-Right to model the interactions\nbetween low-level and high-level semantics. More formally, we build the Contextual Transformer\nwith two modules FLocal and FGlobal. We append the positional embedding to the inputs of FLocal.\nThe goal of FLocal is to model the short-term interactions between low-level semantics ({ϑk\ni}n\ni=1),\nwhereas FGlobal models the interactions between local and global context ( gν) to highlight the\nimportant semantics.\nGiven local representations {ϑk\ni}n\ni=1 ∈Rn×d, where nis the number of clips and dindicates the\nfeature dimension, FLocal applies multi-head attention followed by a feed-forward layer and a\nnormalization layer on top of both layers and produces embeddings {hi}n\ni=1.\nWe compute key (K)-value(V) pairs based on these embeddings {hi}n\ni=1 ∈Rn×d and query(Q)\nbased on the global context gv. FGlobal produces the attention output as follows,\nHattn = softmax(QKT\n√\nd\n)V, Q = Wqgv, K= Wk{hi}n\ni=1, V= Wv{hi}n\ni=1 (5)\nwhere Wq, Wk, and Wv are the embedding weights. Hattn is a weighted sum of values (local\nsemantics), where the weight of each value is calculated based on its interaction with the global context\nquery Q. Hattn is further encoded by a feed-forward layer to produce the contextual embedding\nHcontext. We calculate the mean of {hi}n\ni=1 and concatenate it with Hcontext to obtain the ﬁnal\nvideo embedding ϑk = concat(mean({hi}n\ni=1),Hcontext); see Figure 2.\n4\n3 Cross-Modal Cycle Consistency\nWe introduce a cross-modal cycle-consistency loss to enforce the semantic alignment between clips\nand sentences, as illustrated in Figure 3. It replaces the cross-modal attention units used in [8, 14]. A\npair of clip and sentence will be identiﬁed as semantically aligned if they are nearest neighbors in the\nlearned common spaces. Consider as input a sequence of clip embeddings {ϑi}n\ni=1 = {ϑ1,...,ϑ n}\nand sentence embeddings {δi}m\ni=1 = {δ1,...,δ m}. As the sentences of a paragraph have a temporal\norder, given a sentence embedding δi on this sequence, we ﬁrst ﬁnd its soft nearest neighbor [36–38]\n¯ϑδi =\nn∑\nj=1\nαjϑj where αj = exp(−∥δi −ϑj∥2)∑n\nk=1 exp(−∥δi −ϑk∥2) (6)\nin the clip sequence {ϑi}n\ni=1. αj is the similarity score of clip ϑj to sentence δi. We then cycle back\nfrom ¯ϑδi to the sentence sequence {δi}m\ni=1 and calculate the soft location\nµ=\nm∑\nj=1\nβjj where βj = exp(−∥¯ϑ−δj∥2)∑m\nk=1 exp(−∥¯ϑ−δk∥2). (7)\nThe sentence embedding δi is semantically cycle consistent if and only if it cycles back to the original\nlocation, i.e., i= µ. We penalize deviations from cycle-consistency for sampled sets of clips and\nsentences, which encourages the model to learn semantically consistent representations.\nA nice and lush field is \nshown and in the distance\nthe same man rides up \non a different horse\nLossNearest semantic\nNearest semantic\na young man on \na horse comes\nThe field is shown \nagain \nFigure 3: Cross-Modality Cycle-Consistency.\nStarting from a sentence si, we ﬁnd its nearest\nneighbor in the clip sequence and again its neigh-\nbor in the sentence sequence. Deviations from the\nstart index are penalized as alignment error.\nOur objective is the distance between the source\nlocation iand the soft destination location µ.\nℓCMC = ∥i−µ∥2 (8)\nComputing nearest neighbors as soft nearest\nneighbors makes the loss differentiable [36–38].\nWe can use this loss in both supervised and\nself-supervised scenarios. In the self-supervised\ncase, we split each video uniformly into several\nclips and each paragraph into sentences. Beside\nthe cycle-consistency from text to video, we also\ncalculate ℓCMC from video to text. Therefore,\nthe ﬁnal ℓCMC loss includes both cycles.\nThe ﬁnal training loss for the overall model is:\nℓfinal = ℓL\nalign + ℓH\nalign + ℓg\nalign + ℓcluster + λℓCMC (9)\n4 Experimental Setup\nDatasets. We evaluate our method on the datasetsActivityNet-captions [39] and Youcook2 [40].\nActivityNet-captions consists of 20k YouTube videos with an average length of 2 minutes, with\n72k clip-sentence pairs. There are ∼10k, ∼5k and ∼5k videos in train, val1 and val2, respectively.\nYoucook2 contains 2000 videos with a total number of 14k clips. This dataset is collected from\nYouTube and covers 89 types of recipes. There are ∼9.6k clips for training and ∼3.2k clips for\nvalidation. For each clip there is a manually annotated textual description.\nEvaluation Metrics. We measure the performance on the retrieval task with standard retrieval\nmetrics, i.e., recall at K (R@K e.g. R@1, R@5, R@10) and Median Rank (MR).\nText encoding. We feed paragraphs consisting of several sentences into a pretrained \"BERT-Base,\nUncased\" model [19] and use the per-token outputs of the last 2 layers, resulting in 1536-d features.\nVideo encoding. For Activitynet-Captions, we use the 2048-d features provided by Zhang et\nal. [21] (at 3.8 FPS). For Youcook2, we test two approaches: (A) We follow Miech et al., 2019 [ 16]\nand concatenate 2D (Resnet-152 pretrained on ImageNet [ 41]) and 3D (ResNext-101 model [42]\npretrained on Kinetics [ 43]) outputs to obtain 4096-d features at 3 FPS; ( B) We use the video\nembedding network provided by Miech et al., 2020 [ 17] pretrained on video-text learning on the\nHowto100m dataset to obtain 512-d features at 0.6 FPS.\nFor each clip as well as for the entire video, we sample up to 80 frame features. If needed, we split\nthe frames into 80 equal length intervals and uniformly sample a frame from each interval during\ntraining or take the center frame during validation.\n5\nTable 1: Ablation study on ActivityNet-captions (val1).We quantify the individual contributions of\nthe attention-aware feature aggregation (AF), the Contextual Transformer (CoT), and the cross-modal\ncycle-consistency loss (CMC). HSE results are reproduced by us. Disabling CoT means removing\nthe cross-attention layer between local and global context.\nModel Pooling CMC CoT Paragraph =⇒ Video Video =⇒ Paragraph Param (M)\nLowlvl R@1 R@5 R@50 R@1 R@5 R@50\nHSE Max \u0017 \u0017 45.6±0.3 76.1±0.7 96.0±0.3 44.9±0.5 75.8±1.2 95.8±0.4 26.1\nHSE Max \u0013 \u0017 46.6±0.4 78.1±0.3 97.3±0.1 46.4±0.3 77.6±0.3 97.1±0.3 26.1\nCOOT CLS \u0017 \u0017 49.4±1.4 77.7±1.3 95.7±0.2 49.7±1.9 77.8±0.9 95.8±0.3 4.9\nCOOT A VG \u0017 \u0017 52.6±0.6 80.6±0.4 97.0±0.2 52.1±0.4 80.8±0.2 97.0±0.2 4.9\nCOOT Max \u0017 \u0017 58.2±0.5 84.9±0.2 98.1±0.1 58.7±0.5 86.0±0.2 98.2±0.1 4.9\nCOOT AFA \u0017 \u0017 59.0±0.5 85.4±0.2 98.2±0.0 59.8±0.6 85.8±0.8 98.2±0.1 5.8\nCOOT Max \u0013 \u0013 59.4±0.9 86.1±0.6 98.3±0.0 60.5±0.1 87.1±0.2 98.5±0.1 6.7\nCOOT AFA \u0017 \u0013 59.8±1.1 86.3±0.3 98.5±0.1 60.1±0.1 87.1±0.4 98.5±0.1 7.6\nCOOT AFA \u0013 \u0017 59.5±0.5 85.5±0.4 98.1±0.0 60.5±0.7 86.2±0.5 98.2±0.1 5.8\nCOOT AFA \u0013 \u0013 60.8±0.6 86.6±0.4 98.6±0.1 60.9±0.3 87.4±0.5 98.6±0.0 7.6\nTraining. Similar to [21] we set all margins α= αg = β = γ = µ= 0.2. We use a mini-batch\nsize of 64 video/paragraph pairs and sample all corresponding clips and sentences. All possible\ncombinations of embeddings with non-matching indices in a batch are used as negative samples for\nthe contrastive loss. To apply the cycle-consistency loss, we found that sampling 1 clip per video and\n1 sentence per paragraph works best. The optimal loss weight λdepends on architecture and dataset.\nAs activation function, we found GELU [34] to perform best. We set the hidden size to 384 and use a\npointwise linear layer to reduce the input feature dimension. We use one self-attention layer for the\nT-Transformer and one self-attention and one cross-attention layer for CoT. For further details on\noptimization and hyperparameters we refer the interested reader to the supplementary material.\nVideo-Language Retrieval. For video-text retrieval, the query is a paragraph and the task is to\nﬁnd the most relevant video from a database. Alternatively, the query can be a video and the task is to\nretrieve the most relevant paragraph. We follow the experimental protocol from Zhang et al. [21] to\nevaluate the models. We use the ﬁnal embedding output of our model (ϑk,δk) to do the retrieval.\nClip-sentence retrieval. For Youcook2, we also evaluate the quality of our model when retrieving\na short video clip given a single sentence. For this experiment, we use the intermediate low-level\nembeddings produced by our model (ϑk\ni,δk\ni) to do the retrieval.\n5 Results\nInﬂuence of each component. We show results of a model ablation study in Table 1. First, to\nvalidate the general effectiveness of the proposed cross-modal cycle consistency loss (CMC), we\napply it to the HSE architecture [21]. The ℓCMC loss provides a signiﬁcant boost in performance\nfor both HSE and COOT, which indicates that it will be beneﬁcial if plugged into other video-text\nrepresentation learning methods. Second, the Attention-FA module shows better performance (7.2%\naverage improvement on R@1 for paragraph =⇒video and video =⇒paragraph tasks) than common\naverage pooling. Third, we observe that integrating the Contextual Transformer into the overall model\nimproves the performance. This conﬁrms that interactions between local and global context help the\nmodel to highlight the relevant semantics (more in supp. material).\nComparison to the state of the art. Table 2 summarizes the results of paragraph to video and\nvideo to paragraph retrieval tasks on the ActivityNet-captions dataset. For a fair comparison, our\nmodel utilizes the same video features as HSE [ 21]. Our method signiﬁcantly outperforms all\nprevious methods across different evaluation metrics. COOT obtains on average 16.6% better R@1\nin comparison to HSE [21] while having fewer parameters. We believe the major gain comes from\nour attention-aware feature aggregation component and the ℓCMC loss.\nWe further provide retrieval results on the Youcook2 [40] dataset in Table 3. We compare our model\nunder two settings: (1) with features pretrained on classiﬁcation (2) with features from a pretrained\nSOTA video-text model.\n6\nTable 2: Video-paragraph retrieval results on AcitvityNet-captions dataset (val1).\nParagraph =⇒ Video Video =⇒ Paragraph\nMethod R @1 R @5 R @50 MR R@1 R @5 R @50 MR\nLSTM-YT [52] 0.0 4.0 24.0 102.0 0.0 7.0 38.0 98.0\nNo Context [53] 5.0 14.0 32.0 78.0 7.0 18.0 45.0 56.0\nDENSE [39] 14.0 32.0 65.0 34.0 18.0 36.0 74.0 32.0\nVSE [54]( [5]) 11.7 34.7 85.7 10 - - - -\nFSE [21] 18.2 44.8 89.1 7 16.7 43.1 88.4 7\nHSE [21] 44.4 ±0.5 76.7±0.3 97.1±0.1 2 44.2±0.6 76.7±0.3 97.0±0.3 2\nCOOT 60.8±0.6 86.6±0.4 98.6±0.1 1 60.9±0.3 87.4±0.5 98.6±0.0 1\nWithout HowTo100M pretrained features.We use features (A) explained in Section 4 and train the\nCOOT model on the YouCook2 dataset. Using the same training set, COOT outperforms Miech\net al. [16] and HGLMM [44] on both paragraph-to-video and sentence-to-clip tasks. This supports\nour rationale that modeling interactions between different hierarchy levels is crucial for capturing\nlong-term semantics.\nWith HowTo100M pretrained features.In Table 3, we compare our method with the recently proposed\nSOTA methods MIL-NCE [17], ActBERT [8], and Miech et al. [16], which utilize pretraining on\nthe huge HowTo100M dataset. We use features (B) (Sec. 4) and train the model on the YouCook2\ndataset. Note that the paragraph to video results of other methods are computed by us. Training our\nmodel with features of a model pretrained on the HowTo100M dataset clearly improves over training\nwith features of a model pretrained on classiﬁcation and over the state-of-the-art. We can see that our\nmodel outperforms MIL-NCE [17] 16.4% on R@1 score for paragraph-to-video task, which veriﬁes\nthat COOT beneﬁts from hierarchy interactions. This shows that the contributions of this paper are\ncomplementary to works that focus on large-scale pretraining.\nTime complexity and number of parameters. The COOT model has 10.6M, parameters which\nis 60% less than the HSE method (Table 1). Training is fast and takes less than 3 hours on two\nGTX1080Ti GPUs (without data I/O).\n5.1 Video Captioning\nTo show that the learned representations contain meaningful information for other tasks than re-\ntrieval, we use the learned representations for video captioning building upon the captioning model\nMART [45]. The original method uses appearance (RGB) and optical ﬂow features extracted from\nResNet-200 [41] and BN-Inception [46], respectively.\nWe use the clip (ϑk\ni) and optionally the video (ϑk) representation generated with our COOT model.\nIn comparison to MART, we input about 100 times less features per video into the captioning model.\nWe use the standard language evaluation metrics BLEU@3/4 [47], RougeL [48], METEOR [49],\nCIDEr-D [50] and R@4 [51] which measures the degree of n-gram repetition. Our results in Table 4\nand Table 5 show that the MART method using our representations improves over using appearance\nand optical ﬂow video features. Generated captions in Table 6 show that our video representations\nencapsulate richer information about the video while being more compact.\n6 Related Work\nImage and Language. Many self-supervised visual-language representation learning methods have\nfocused on improving one representation with the help of the other [25, 44, 57–61]. These approaches\nlearn joint image-text embeddings or map images and sentences into a common space. Recently, there\nhas been a surging interest in utilizing Transformers for image-text representation learning [62–66].\nViLBERT [13] and VisualBERT [12] pretrain a BERT-like architecture on an image-text dataset and\nthen transfer learned representations to different downstream tasks. LXMERT [14] uses additional\npretraining tasks and an object-relationship component. In contrast to [13, 14], VL-BERT [15] does\nnot utilize the task of sentence-image relationship prediction and additionally pretrains the model on\ntext-only datasets.\n7\nTable 3: Retrieval results on YouCook2 dataset. Results with * are computed by us. △we use\nfeatures of a video-text model [17] pretrained on the HowTo100m dataset.\nParagraph=⇒Video Sentence =⇒Clip\nMethod TrainSet R @1 R @5 R @10 MR R @1 R @5 R @10 MR\nRandom - 0.21 1.09 2.19 229 0.03 0.15 0.3 1675\nMiech et al. [16] HowTo100M 43.1* 68.6* 79.1* 2* 6.1 17.3 24.8 46\nActBERT [8] HowTo100M - - - - 9.6 26.7 38.0 19\nMIL-NCE [17] HowTo100M 61.9* 89.4* 98.9* 1* 15.1 38.0 51.2 10\nHGLMM [44] YouCook2 - - - - 4.6 14.3 21.6 75\nMiech et al. [16] YouCook2 32.3* 59.2* 70.9* 4* 4.2 13.7 21.5 65\nCOOT YouCook2 50.4 ±2.6 79.4±0.6 87.4±0.8 1.3±0.6 5.9±0.7 16.7±0.6 24.8±0.8 49.7±2.9\nMiech et al. [16]HowTo100M+ 59.6* 86.0* 93.6* 1* 8.2 24.5 35.3 24YouCook2\nCOOT HowTo100M△+ 77.2±1.0 95.8±0.8 97.5±0.3 1.0±0.0 16.7±0.4 40.2±0.3 52.3±0.5 9.0±0.0\nYouCook2\nTable 4: Captioning results on the YouCook2 dataset (val split). Results with * are computed by\nus. △we use features of a video-text model [17] pretrained on the HowTo100m dataset. \"MART w/o\nre\" denotes a MART variant without recurrence.\nFeatures Method TrainSet B @3 B @4 RougeL METEOR CIDEr-D R@4↓\nRGB+Flow VTransformer [55] YouCook2 13.08* 7.62 32.18* 15.65 32.26 7.83\nRGB+Flow TransformerXL [56] YouCook2 11.46* 6.56 30.78* 14.76 26.35 6.30\nRGB+Flow MART [45] YouCook2 12.83* 8.00 31.97* 15.90 35.74 4.39\nCOOT clip MART YouCook2 14.17 8.69 33.01 16.11 38.28 8.07\nCOOT video+clip MART YouCook2 15.75 9.44 34.32 18.17 46.06 6.30\nCOOT clip MART H100M △+YC2 17.12 10.91 37.59 18.85 54.07 5.11\nCOOT clip MART w/o re. H100M △+YC2 17.16 10.69 37.43 19.18 54.85 5.45\nCOOT clip VTransformer H100M △+YC2 17.62 11.09 37.63 19.34 54.67 4.57\nCOOT video+clip VTransformer [55] H100M△+YC2 17.79 11.05 37.51 19.79 55.57 5.69\nCOOT video+clip MART H100M △+YC2 17.97 11.30 37.94 19.85 57.24 6.69\nVideo and Language. The multi-modal nature of video is a great source of self-supervision.\nModalities such as audio, text and motion provide strong cues to learn richer spatio-temporal\nfeatures [9, 11, 16, 17, 67–69]. Aytar et al. [70] leverage natural synchronization to learn rich repre-\nsentations across vision, sound, and language. VideoBERT [11] learns joint video-text representations\nbased on predicting whether the linguistic sentence is temporally aligned with the visual sentence.\nThese approaches [8, 11, 23] focus on self-supervised pretraining and require a large set of paired\nvideo clips and texts to learn a good representation model [17].\nThere has been growing interest in temporal localization of natural language in videos [2, 7, 71–73].\nMoment localization identiﬁes a time window given a text query [ 7, 71, 74]. Most related to our\nwork are methods that focus on joint video-text embeddings and perform video-text retrieval or\ncaptioning [4–6, 9, 21, 52, 69, 75, 76]. Several works tried to utilize the temporal structure of video\nand text for the alignment task [ 4, 70, 77–79]. Miech et al. [ 68] proposed a mixture of experts\napproach to learn text-video representations. Likewise, CE [20] proposes a mixture-of-experts model\nto aggregate information from pretrained experts (e .g. object, action, audio) with a gating mechanism.\nIn our work, we use a similar hierarchy as CMHSE [21, 22, 24] and extend their design by proposing\nthree new components to learn the interactions between different levels of the hierarchy.\nCycle-Consistency. Cycle-Consistency uses transitivity as an objective for training [38, 80–82].\nThe assumption of cyclical structure has been used in various works [83–85]. Wang et al . [80] obtain\nthe supervision for visual correspondence by tracking forward and backward. Shah et al. [81] enforce\nconsistency between the generated and the original question in visual question answering. To prevent\nmode collapse, cycle-consistency is activated only after a certain number of training iterations [81].\nTCC [38] employs cycle-consistency for temporal video alignment. In contrast to TCC, which works\nonly in the video domain, we align video and text. To the best of our knowledge, this is the ﬁrst work\nwhich introduces cycle-consistency to the video-text domain.\n8\nTable 5: Captioning results on the ActivityNet-Captions dataset (ae-test split of MART [45]).\nResults with * are computed by us. \"MART w/o re\" denotes a MART variant without recurrence.\nFeatures Method TrainSet B @3 B @4 RougeL METEOR CIDEr-D R @4↓\nRGB+Flow VTransformer [55] ActivityNet 16.27* 9.31 29.18* 15.54 21.33 7.45\nRGB+Flow TransformerXL [56] ActivityNet 16.71* 10.25 30.53* 14.91 21.71 8.79\nRGB+Flow MART ActivityNet 16.43* 9.78 30.63* 15.57 22.16 5.44\nCOOT video+clip TransformerXL [56] ActivityNet 16.94 10.57 30.93 14.76 22.04 15.85\nCOOT video+clip VTransformer [55] ActivityNet 16.80 10.47 30.37 15.76 25.90 19.14\nCOOT clip MART w/o re. ActivityNet 15.41 9.37 28.66 15.61 22.05 12.03\nCOOT video+clip MART w/o re. ActivityNet 16.59 10.33 29.93 15.64 25.41 17.03\nCOOT clip MART ActivityNet 16.53 10.22 30.68 15.91 23.98 5.35\nCOOT video+clip MART ActivityNet 17.43 10.85 31.45 15.99 28.19 6.64\nTable 6: Captioning samples, more accurate (left) and less accurate (right) cases. First row:\nActivityNet (ae-test split), second row: YouCook2 (val split). Red/bold indicates content errors,\nblue/italic indicates repetitive patterns.\nMART: A person is driving the car . A boy is\nholding a bottle of wood.\nCOOT (Ours): A woman is seen kneeling down next\nto a car while others stand around her. The woman\nthen pushes the tire back and fourth.\nGT: A girl is shown trying to change a tire.\nShe successfully removes the tire, then replaces\nit with a spare, showing off their dirty hands afterward.\nMART: A man is kneeling down on a ﬂoor. He is\nkneeling down on the ground.\nCOOT (Ours): A man is seen kneeling down on\nthe ground and begins putting shoes on . The man\ncontinues to put on the shoes and ends by putting his\nshoes on.\nGT: A person is seen bending over a ﬂoor placing\ntiles down over the plaster. The person continues\nlaying tiles down and pushing down on the ﬂoor to\nmake sure it’s sturdy.\nMART: Heat up a pan and cook until golden brown.\nAdd onions to the pan. Add ﬂour salt and pepper to\nthe pan. Add rice to the pan and stir.\nCOOT (Ours) : Boil the potatoes in water. Add\nchopped potatoes to the pan. Add butter and mash.\nAdd some milk and mash.\nGT: Boil some small pieces of potatoes in water.\nMash the potato. Add some butter and salt and stir.\nGradually add milk while stirring the potatoes.\nMART: Cut the salmon into thin slices. Cut the\nsalmon into thin slices. Cut the salmon into thin slices.\nCut the salmon into thin slices.\nCOOT (Ours) : Cut the salmon in half. Cut the\nsalmon in half. Cut the salmon into thin slices. Cut\nthe salmon into thin slices.\nGT: Slice the ﬁsh into smaller pieces. Chop the tail\nend off. Cut the ﬁsh at an angle. Cut the ﬁsh into thin\npieces.\n7 Conclusions\nWe have presented a cooperative hierarchical transformer architecture for learning a joint video\nand text embedding space where similar semantics are aligned. The architecture is designed to\nencourage the use of long-range temporal context in a cross-level manner. Our approach uses two new\ncomponents to model the interactions within and between hierarchy levels; an attention-aware feature\naggregation module to model the interactions between frames and words, a contextual transformer to\nmodel the interactions between local contexts and global context. In addition, we have introduced a\nnew cross-modal cycle-consistency loss which enforces the semantic alignment of clips and sentences.\nWe have shown that both components contribute – jointly and individually – to an improved retrieval\nperformance. As a result, our approach achieves state-of-the-art retrieval and captioning performance\non two challenging datasets.\n9\nBroader Impact\nThis work contributes fundamental research and does not present any foreseeable societal consequence.\nIn the long run, this line of research can contribute to services on video search and video organisation.\nAcknowledgments\nWe thank Ehsan Adeli for helpful comments, Antoine Miech for providing details on their retrieval\nevaluation, and Facebook for providing us a GPU server with Tesla P100 processors for this research\nwork.\nReferences\n[1] Satwik Kottur, Ramakrishna Vedantam, José M. F. Moura, and Devi Parikh. Visual word2vec (vis-w2v):\nLearning visually grounded word embeddings using abstract scenes. CoRR, abs/1511.07067, 2015.\n[2] Zhenfang Chen, Lin Ma, Wenhan Luo, Peng Tang, and Kwan-Yee K. Wong. Look closer to ground better:\nWeakly-supervised temporal grounding of sentence in video. CoRR, abs/2001.09308, 2020.\n[3] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt Schiele. Grounding of\ntextual phrases in images by reconstruction. In European Conference on Computer Vision, pages 817–834.\nSpringer, 2016.\n[4] Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.\nGrounding action descriptions in videos. Transactions of the Association for Computational Linguistics,\n1:25–36, 2013.\n[5] Dian Shao, Yu Xiong, Yue Zhao, Qingqiu Huang, Yu Qiao, and Dahua Lin. Find and focus: Retrieve and\nlocalize video events with natural language queries. In The European Conference on Computer Vision\n(ECCV), September 2018.\n[6] Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained video-text retrieval with hierarchical graph\nreasoning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[7] Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang, and Larry S Davis. Man: Moment alignment network\nfor natural language moment retrieval via iterative graph adjustment. InThe IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019.\n[8] Yi Yang Linchao Zhu. Actbert: Learning global-local video-text representations. In CVPR, 2020.\n[9] Niluthpol Chowdhury Mithun, Juncheng Li, Florian Metze, and Amit K. Roy-Chowdhury. Learning joint\nembedding with multimodal cues for cross-modal video-text retrieval. In Proceedings of the 2018 ACM on\nInternational Conference on Multimedia Retrieval, ICMR ’18, page 19–27, New York, NY , USA, 2018.\nAssociation for Computing Machinery.\n[10] B. A. Plummer, M. Brown, and S. Lazebnik. Enhancing video summarization via vision-language\nembedding. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages\n1052–1060, 2017.\n[11] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model\nfor video and language representation learning. In Proceedings of the IEEE International Conference on\nComputer Vision, pages 7464–7473, 2019.\n[12] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and\nperformant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.\n[13] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\nrepresentations for vision-and-language tasks. In Advances in Neural Information Processing Systems,\npages 13–23, 2019.\n[14] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers.\narXiv preprint arXiv:1908.07490, 2019.\n[15] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of\ngeneric visual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019.\n10\n[16] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic.\nHowto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV,\n2019.\n[17] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman.\nEnd-to-end learning of visual representations from uncurated instructional videos. In CVPR, 2020.\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference\non Neural Information Processing Systems, NeurIPS 2017, page 6000–6010, Red Hook, NY , USA, 2017.\nCurran Associates Inc.\n[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, Jun 2019. Association for\nComputational Linguistics.\n[20] Y . Liu, S. Albanie, A. Nagrani, and A. Zisserman. Use what you have: Video retrieval using representations\nfrom collaborative experts. In arXiv preprint arxiv:1907.13487, 2019.\n[21] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In\nComputer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018,\nProceedings, Part XIII, pages 385–401, 2018.\n[22] P. Pan, Z. Xu, Y . Yang, F. Wu, and Y . Zhuang. Hierarchical recurrent neural encoder for video representation\nwith application to captioning. In 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 1029–1038, 2016.\n[23] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Contrastive bidirectional transformer for\ntemporal representation learning. arXiv preprint arXiv:1906.05743, 2019.\n[24] Jiwei Li, Thang Luong, and Dan Jurafsky. A hierarchical neural autoencoder for paragraphs and documents.\nIn Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the\n7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages\n1106–1115, Beijing, China, Jul 2015. Association for Computational Linguistics.\n[25] Donghyun Kim, Kuniaki Saito, Kate Saenko, Stan Sclaroff, and Bryan A. Plummer. Mule: Multimodal\nuniversal language embedding, 2019.\n[26] R. Hadsell, S. Chopra, and Y . LeCun. Dimensionality reduction by learning an invariant mapping. In2006\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2006), volume 2,\npages 1735–1742, 2006.\n[27] Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Leung, Sergey Ioffe, and Saurabh Singh. No fuss\ndistance metric learning using proxies. CoRR, abs/1703.07464, 2017.\n[28] G. Kordopatis-Zilo, S. Papadopoulos, I. Patras, and Y . Kompatsiaris. Near-duplicate video retrieval with\ndeep metric learning. In 2017 IEEE International Conference on Computer Vision Workshops (ICCVW),\npages 347–356, 2017.\n[29] Weifeng Ge, Weilin Huang, Dengke Dong, and Matthew R. Scott. Deep metric learning with hierarchical\ntriplet loss. CoRR, abs/1810.06951, 2018.\n[30] Antoine Miech, Ivan Laptev, and Josef Sivic. Learnable pooling with context gating for video classiﬁcation.\nCoRR, abs/1706.06905, 2017.\n[31] Mateusz Malinowski and Mario Fritz. Learnable pooling regions for image classiﬁcation. In ICLR\nworkshop, May 2013.\n[32] Qian Chen, Zhen-Hua Ling, and Xiaodan Zhu. Enhancing sentence embedding with generalized pooling.\nCoRR, abs/1806.09828, 2018.\n[33] Hayoung Eom and Heeyoul Choi. Alpha-pooling for convolutional neural networks.CoRR, abs/1811.03436,\n2018.\n[34] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,\n2016.\n11\n[35] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners, 2019.\n[36] I. Rocco, M. Cimpoi, R. Arandjelovi´c, A. Torii, T. Pajdla, and J. Sivic. Neighbourhood consensus networks.\nProceedings of the 32nd Conference on Neural Information Processing Systems, 2018.\n[37] Jacob Goldberger, Geoffrey E Hinton, Sam T. Roweis, and Russ R Salakhutdinov. Neighbourhood\ncomponents analysis. In L. K. Saul, Y . Weiss, and L. Bottou, editors, Advances in Neural Information\nProcessing Systems 17, pages 513–520. MIT Press, 2005.\n[38] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. Temporal\ncycle-consistency learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1801–1810, 2019.\n[39] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events\nin videos. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017.\n[40] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web\ninstructional videos. In AAAI Conference on Artiﬁcial Intelligence, 2018.\n[41] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.\n[42] K. Hara, H. Kataoka, and Y . Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?\nIn 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6546–6555, 2018.\n[43] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In\n2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4724–4733, 2017.\n[44] B. Klein, G. Lev, G. Sadeh, and L. Wolf. Associating neural word embeddings with deep image representa-\ntions using ﬁsher vectors. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 4437–4446, 2015.\n[45] Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara L Berg, and Mohit Bansal. Mart: Memory-augmented\nrecurrent transformer for coherent video paragraph captioning. In ACL, 2020.\n[46] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In Proceedings of the 32nd International Conference on International Conference\non Machine Learning - Volume 37, ICML’15, page 448–456. JMLR.org, 2015.\n[47] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation\nof machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational\nLinguistics, pages 311–318, Philadelphia, Pennsylvania, USA, Jul 2002. Association for Computational\nLinguistics.\n[48] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches\nOut, pages 74–81, Barcelona, Spain, Jul 2004. Association for Computational Linguistics.\n[49] Michael Denkowski and Alon Lavie. Meteor universal: Language speciﬁc translation evaluation for any\ntarget language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376–380,\nBaltimore, Maryland, USA, Jun 2014. Association for Computational Linguistics.\n[50] R. Vedantam, C. L. Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation. In 2015\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4566–4575, 2015.\n[51] Yilei Xiong, Bo Dai, and Dahua Lin. Move forward and tell: A progressive generator of video descriptions.\nIn ECCV, 2018.\n[52] Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, and Kate\nSaenko. Sequence to sequence-video to text. In Proceedings of the IEEE international conference on\ncomputer vision, pages 4534–4542, 2015.\n[53] Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate\nSaenko. Translating videos to natural language using deep recurrent neural networks. arXiv preprint\narXiv:1412.4729, 2014.\n[54] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc'Aurelio Ranzato, and Tomas\nMikolov. Devise: A deep visual-semantic embedding model. In C. J. C. Burges, L. Bottou, M. Welling,\nZ. Ghahramani, and K. Q. Weinberger, editors,Advances in Neural Information Processing Systems 26,\npages 2121–2129. Curran Associates, Inc., 2013.\n12\n[55] Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher, and Caiming Xiong. End-to-end dense video\ncaptioning with masked transformer, 2018.\n[56] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V . Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context, 2019.\n[57] Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. Unifying visual-semantic embeddings with\nmultimodal neural language models. arXiv preprint arXiv:1411.2539, 2014.\n[58] Andrej Karpathy, Armand Joulin, and Li F Fei-Fei. Deep fragment embeddings for bidirectional image\nsentence mapping. In Advances in neural information processing systems, pages 1889–1897, 2014.\n[59] Gil Sadeh, Lior Fritz, Gabi Shalev, and Eduard Oks. Joint visual-textual embedding for multimodal style\nsearch. CoRR, abs/1906.06620, 2019.\n[60] Liwei Wang, Yin Li, and Svetlana Lazebnik. Learning deep structure-preserving image-text embeddings.\nCoRR, abs/1511.06078, 2015.\n[61] Bryan A Plummer, Arun Mallya, Christopher M Cervantes, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Phrase localization and visual relationship detection with comprehensive image-language cues. In\nProceedings of the IEEE International Conference on Computer Vision, pages 1928–1937, 2017.\n[62] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing\nLiu. Uniter: Universal image-text representation learning, 2019.\n[63] Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. Imagebert: Cross-modal pre-training\nwith large-scale weak-supervised image-text data, 2020.\n[64] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. Uniﬁed\nvision-language pre-training for image captioning and vqa. arXiv preprint arXiv:1909.11059, 2019.\n[65] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image\npixels with text by deep multi-modal transformers, 2020.\n[66] Kazuki Miyazawa, Tatsuya Aoki, Takato Horii, and Takayuki Nagai. lambert: Language and action\nlearning using multimodal bert. arXiv preprint arXiv:2004.07093, 2020.\n[67] Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He, Gang Yang, and Xun Wang. Dual encoding\nfor zero-example video retrieval. InIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2019.\n[68] Antoine Miech, Ivan Laptev, and Josef Sivic. Learning a text-video embedding from incomplete and\nheterogeneous data. In arXiv, 2018.\n[69] Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, and Yong Rui. Jointly modeling embedding and translation\nto bridge video and language. CoRR, abs/1505.01861, 2015.\n[70] Yusuf Aytar, Carl V ondrick, and Antonio Torralba. See, hear, and read: Deep aligned representations.\nCoRR, abs/1706.00932, 2017.\n[71] Jonghwan Mun, Minsu Cho, , and Bohyung Han. Local-global video-text interactions for temporal\ngrounding. In CVPR, 2020.\n[72] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell.\nLocalizing moments in video with natural language. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), 2017.\n[73] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. TALL: temporal activity localization via\nlanguage query. In ICCV, 2017.\n[74] Mingfei Gao, Larry Davis, Richard Socher, and Caiming Xiong. Wslln:weakly supervised natural language\nlocalization networks. Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),\n2019.\n[75] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video\nretrieval, 2020.\n13\n[76] Ran Xu, Caiming Xiong, Wei Chen, and Jason J. Corso. Jointly modeling deep video and compositional\ntext to bridge vision and language in a uniﬁed framework. In Proceedings of the Twenty-Ninth AAAI\nConference on Artiﬁcial Intelligence, AAAI’15, page 2346–2352. AAAI Press, 2015.\n[77] Piotr Bojanowski, Rémi Lajugie, Edouard Grave, Francis Bach, Ivan Laptev, Jean Ponce, and Cordelia\nSchmid. Weakly-supervised alignment of video with text. In ICCV - IEEE International Conference on\nComputer Vision, pages 4462–4470, Santiago, Chile, dec 2015. IEEE.\n[78] M. Tapaswi, M. Bäuml, and R. Stiefelhagen. Book2movie: Aligning video scenes with book chapters. In\n2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1827–1835, 2015.\n[79] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering\nand retrieval. In The European Conference on Computer Vision (ECCV), September 2018.\n[80] Xiaolong Wang, Allan Jabri, and Alexei A. Efros. Learning correspondence from the cycle-consistency of\ntime. In CVPR, 2019.\n[81] Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. Cycle-consistency for robust visual question\nanswering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n6649–6658, 2019.\n[82] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using\ncycle-consistent adversarial networks, 2017.\n[83] Y . Chen, Y . Lin, M. Yang, and J. Huang. Crdoco: Pixel-level domain transfer with cross-domain consistency.\nIn 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1791–1800,\n2019.\n[84] Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma. Dual learning\nfor machine translation. In Proceedings of the 30th International Conference on Neural Information\nProcessing Systems, NIPS’16, page 820–828, Red Hook, NY , USA, 2016. Curran Associates Inc.\n[85] Duyu Tang, Nan Duan, Zhao Yan, Zhirui Zhang, Yibo Sun, Shujie Liu, Yuanhua Lv, and Ming Zhou.\nLearning to collaborate for question answering and asking. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 1564–1574, New Orleans, Louisiana, jun 2018. Association\nfor Computational Linguistics.\n[86] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efﬁcient hyperparameter optimization at\nscale. CoRR, abs/1807.01774, 2018.\n[87] Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural\nnetworks. CoRR, abs/1706.02515, 2017.\n[88] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by\nexponential linear units (elus). In Yoshua Bengio and Yann LeCun, editors, 4th International Conference\non Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track\nProceedings, 2016.\n[89] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\nOn the variance of the adaptive learning rate and beyond. In Proceedings of the Eighth International\nConference on Learning Representations (ICLR 2020), April 2020.\n[90] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word\nrepresentation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, 2014.\n[91] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning\nresearch, 9(Nov):2579–2605, 2008.\n14\nA Appendix\nA.1 Implementation Details\nHyperparameters. To select hyperparameters for our model, we used a combination of manual\nsearch and BOHB [86] to explore the hyperparameters space. In Table 7, we provide an overview of\nour hyperparameter search.\nAfter testing different activation functions ReLU, SELU [87], ELU [88] and GELU [34] we found\nGELU to perform best. Increasing the capacity of the model by using a higher attention dimension\ncan help improve the results, but comes at the cost of higher memory requirements and more difﬁcult\noptimization.\nTable 7: Hyperparameters. This table shows the hyperparameter ranges we considered and the\nﬁnal choices for our three best models (ActivityNet-captions, YouCook2-Resnet/Resnext features,\nYoucook2-Howto100m features.). ROP denotes the Reduce on Plateau Scheduler we used. Dimen-\nsions given in multiples (1x, 2x) refer to multiples of the Attention Dimension parameter. FF denotes\nFeed-Forward. AF is our Attention-aware Feature Aggregation module.\nHyperparameter Considered Range ActivityNet Youcook2\nResnet/ResneXt Howto100m\nOptimizer Adam, RAdam, SGD Adam RAdam RAdam\nLearning rate 1e-5 1e-2 1e-3 3.6e-4 9e-4\nWeight Decay 0 1e-2 2e-5 2e-5 0\nMomentum 0.5 0.99 0.9 0.56 0.56\nAdam Beta2 0.9 0.9999 0.999 0.98 0.98\nAdam Epsilon 1e-10 1e-7 1e-8 1.5e-9 1.5e-9\nWarmup Epochs 0 8 3 0 0\nROP Patience 2 10 2 5 5\nROP Cooldown 0 3 3 3 3\nAttention Layers 1 3 1 1 1\nAttention Dimension 256 1024 384 384 384\nAttention Heads 1 8 8 8 8\nAttention FF Dimension 1x 2x 1x 1x 1x\nAF Dimension 1x 2x 2x 2x 2x\nAF Heads 1 8 2 2 2\nNumber of AF modules 1 2 1 1 1\nDropout 0% 10% 2.5% 1% 5%\nGaussian Noise on Frame Features 0 1 0 0.01 0\nOptimization. We tried several optimizers such as Adam, RAdam [ 89] and SGD. If carefully\nconﬁgured, RAdam can improve over Adam.\nWe schedule the Learning Rate with a Reduce on Plateau approach: Whenever our validation\nmetric does not improve for a certain number of epochs, we reduce the learning rate by a factor of\n10. After no improvements for 15 epochs, we terminate the training process. As relevant metric\nwe deﬁned the sum of R@1 Retrieval Score for video-paragraph and paragraph-video retrieval on\nActivitynet-Captions and the sum of R@1 Retrieval Score for clip-sentence and sentence-clip retrieval\non Youcook2. Careful tuning of the optimizer parameters, using an automated search method like\nBOHB [86] to search parts of the parameters space, was crucial to train the models properly.\nStrength of the cross-modal cycle-consistency loss. For Activitynet we set λ = 0 .01 and for\nYoucook2 we set λ= 0.001.\nFor weight initialization, we utilized Uniform, Normal and Truncated Normal distributions. The best\nresults were obtained with initializing weights randomly from the Truncated Normal distribution with\na standard deviation of 0.01, redrawing all samples with more than 2 standard deviations.\nTo cope with the overﬁtting problem, the different regularization methods (Dropout, Weight Decay,\nCMC-loss, Gaussian Noise on Frame Features) need to be traded off carefully to obtain good results\n(see Table 7).\nPreprocessing. For ActivityNet captions, we found it helpful to expand all clips to be at least 10\nframes long. Expanding is done by iteratively adding frames to the start and end of the clip until we\nreach the desired length.\n15\nTable 8: Text feature ablation study on ActivityNet-captions (val1). We evaluate our choice of\ntext encoding and show that Bert [19] outperforms GloVe [90] on both models and all metrics.\nModel Text Paragraph =⇒ Video Video =⇒ Paragraph\nR@1 R@5 R@50 R@1 R@5 R@50\nHSE GloVe 45.7 ±0.3 76.1±0.7 96.0±0.3 44.9±0.5 75.8±1.2 95.8±0.4\nHSE Bert 47.0 ±1.1 77.0±1.5 96.1±0.4 46.9±0.8 77.2±1.1 95.9±0.6\nCOOT GloVe 56.5 ±1.1 84.1±1.3 98.0±0.3 57.3±1.8 84.5±1.4 98.2±0.2\nCOOT Bert 60.8±0.6 86.6±0.4 98.6±0.1 60.9±0.3 87.4±0.5 98.6±0.0\nTable 9: Loss function ablation study on ActivityNet-captions (val1). We analyse performance\nof the COOT model while removing loss components with different base models. CoT denotes\nusing global attention in the contextual transformer. AF is our Attention-aware Feature Aggregation\nmodule.\n# Pooling CMC CoT Alignment Clustering Par. =⇒ Video Video =⇒ Par.\nLowlvl High Low Ctx High Low R@1 R@5 R@1 R@5\n1 Avg \u0017 \u0017 \u0013 \u0017 \u0017 \u0017 \u0017 30.4±3.2 58.4±4.5 29.9±3.3 58.7±4.5\n2 Avg \u0017 \u0017 \u0013 \u0017 \u0013 \u0013 \u0017 49.7±0.7 79.0±0.6 48.6±0.5 79.1±0.9\n3 Avg \u0017 \u0017 \u0013 \u0017 \u0013 \u0013 \u0013 49.2±0.7 78.9±0.2 48.6±0.6 78.9±0.6\n4 Avg \u0017 \u0017 \u0013 \u0013 \u0017 \u0013 \u0013 50.6±1.1 79.8±0.8 50.8±1.0 79.8±0.8\n5 Avg \u0017 \u0017 \u0013 \u0013 \u0013 \u0017 \u0017 51.5±0.7 80.2±0.4 52.0±0.8 80.5±0.3\n6 Avg \u0017 \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 52.6±0.6 80.6±0.4 52.1±0.4 80.8±0.2\n7 Avg \u0013 \u0017 \u0013 \u0017 \u0017 \u0017 \u0017 27.4±2.1 55.3±2.4 27.3±1.6 56.0±2.3\n8 Avg \u0013 \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 54.1±0.8 82.0±0.1 54.7±0.2 82.1±0.1\n9 Avg \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 53.6±0.1 81.7±0.0 53.5±0.5 81.7±0.7\n10 Max \u0017 \u0017 \u0013 \u0017 \u0017 \u0017 \u0017 47.9±0.7 76.9±0.1 48.3±0.2 77.5±0.6\n11 Max \u0017 \u0017 \u0013 \u0017 \u0013 \u0013 \u0017 56.5±0.3 84.5±0.2 56.6±0.4 85.2±0.1\n12 Max \u0017 \u0017 \u0013 \u0017 \u0013 \u0013 \u0013 54.4±0.9 83.3±0.9 55.4±1.4 84.0±0.8\n13 Max \u0017 \u0017 \u0013 \u0013 \u0017 \u0013 \u0013 55.3±0.8 83.0±0.8 56.4±1.1 83.7±1.2\n14 Max \u0017 \u0017 \u0013 \u0013 \u0013 \u0017 \u0017 56.1±0.2 83.3±0.2 57.0±0.3 83.9±0.5\n15 Max \u0017 \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 58.2±0.5 84.9±0.2 58.7±0.5 86.0±0.2\n16 Max \u0013 \u0017 \u0013 \u0017 \u0017 \u0017 \u0017 46.3±1.0 76.2±0.9 47.7±0.9 77.2±0.7\n17 Max \u0013 \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 57.5±0.5 84.8±0.2 58.1±1.0 85.3±0.4\n18 Max \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 59.4±0.9 86.1±0.6 60.5±1.0 87.1±0.2\n19 AF \u0017 \u0017 \u0013 \u0017 \u0017 \u0017 \u0017 47.1±0.7 76.7±0.6 47.6±0.2 77.4±0.2\n20 AF \u0017 \u0017 \u0013 \u0017 \u0013 \u0013 \u0017 56.3±0.3 84.0±0.2 56.8±0.7 84.7±0.3\n21 AF \u0017 \u0017 \u0013 \u0017 \u0013 \u0013 \u0013 55.2±0.2 83.3±0.1 55.8±0.5 83.6±0.2\n22 AF \u0017 \u0017 \u0013 \u0013 \u0017 \u0013 \u0013 57.8±0.3 84.6±0.2 58.1±0.3 85.1±0.2\n23 AF \u0017 \u0017 \u0013 \u0013 \u0013 \u0017 \u0017 58.8±0.4 85.3±0.4 59.1±0.6 85.8±0.4\n24 AF \u0017 \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 59.0±0.5 85.4±0.2 59.8±0.6 85.8±0.8\n25 AF \u0013 \u0017 \u0013 \u0017 \u0017 \u0017 \u0017 47.8±0.5 76.4±0.4 47.8±0.2 77.5±0.5\n26 AF \u0013 \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 59.5±0.5 85.5±0.4 60.5±0.7 86.2±0.5\n27 AF \u0013 \u0013 \u0013 \u0017 \u0017 \u0017 \u0017 53.9±0.7 82.6±0.6 53.8±0.6 83.0±0.5\n28 AF \u0013 \u0013 \u0013 \u0013 \u0013 \u0017 \u0017 55.1±5.3 83.4±3.6 55.5±4.7 83.8±3.2\n29 AF \u0013 \u0013 \u0013 \u0013 \u0017 \u0013 \u0013 58.5±1.1 85.2±0.5 58.5±0.7 85.5±0.7\n30 AF \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 60.8±0.6 86.6±0.4 60.9±0.3 87.4±0.5\nRetrieval. We L2-normalize the output embeddings of our model so the squared elements sum\nto 1. Retrieval is done by cosine similarity, e.g. given video embedding v, we retrieve paragraph\nembedding\np= max\nˆp∈D\nv⊤ˆp (10)\nA.2 Ablation Studies\nIn this section, we provide ablation studies on the importance of low-level supervision, different text\nencoders performance, impact of different alignment losses in our ﬁnal training loss and analysis on\nsequence pooling.\n16\nTable 10: Evaluation of different sequence pooling methods on ActivityNet-captions (val1). We\nswitch both the low level (frames, words) and high level (clips, sentences) pooling methods and\nobserve the changes in performance. In experiments denoted with *, we used a different optimizer\nsetting.\nPooling CMC CoT Par. =⇒ Video Video =⇒ Par.\nLow High R@1 R@5 R@1 R@5\nAF AF x1 \u0013 \u0013 42.6±0.4 76.5±0.3 42.1±0.8 76.9±0.7\nAF AF x2 \u0013 \u0013 42.8±0.1 76.0±0.7 42.8±0.1 76.4±0.6\nAF* AF x1 \u0013 \u0013 48.7±1.0 82.2±0.6 50.1±0.4 82.6±0.4\nAF* AF x2 \u0013 \u0013 50.5±0.4 82.3±0.4 51.4±1.4 82.9±0.6\nMax Max \u0013 \u0013 40.9±0.7 75.3±0.1 42.2±0.5 76.2±0.6\nAF Max \u0013 \u0013 43.3±0.9 76.3±1.0 42.5±0.6 77.2±1.2\nCLS Avg \u0017 \u0017 49.4±1.4 77.7±1.3 49.7±1.9 77.8±0.9\nCLS Avg \u0013 \u0013 49.7±0.5 79.4±0.2 51.2±0.1 79.6±0.1\nAvg Avg \u0017 \u0017 52.6±0.6 80.6±0.4 52.1±0.4 80.8±0.2\nAvg Avg \u0013 \u0013 53.6±0.1 81.7±0.0 53.5±0.5 81.7±0.7\nMax Avg \u0017 \u0017 58.2±0.5 84.9±0.2 58.7±0.5 86.0±0.2\nMax Avg \u0013 \u0013 59.4±0.9 86.1±0.6 60.5±1.0 87.1±0.2\nAF Avg \u0013 \u0013 60.8±0.6 86.6±0.4 60.9±0.3 87.4±0.5\nTable 11: Evaluation of different averagepooling methods. We modify our exact approach to\naveragepooling in the high-level and evaluate the results.\nActivityNet-captions dataset:\nSum Pad Divide Par. =⇒ Video Video =⇒ Par.\nR@1 R@5 R@1 R@5\nAll Max(Batch, 16) All 44.2 ±2.5 75.4±2.5 44.1±2.0 75.9±2.0\nAll Max(Batch, 16) Nonzero 44.1 ±0.7 76.2±0.9 44.7±1.1 76.9±0.9\nAll Batch All 48.3 ±0.2 76.8±0.8 47.9±0.8 77.7±0.7\nNonzero Batch Nonzero 42.0 ±0.5 76.3±0.3 41.6±0.6 77.0±0.7\nAll Batch Nonzero 60.8±0.6 86.6±0.4 60.9±0.3 87.4±0.5\nYoucook2 dataset:\nSum Pad Divide Par. =⇒ Video Sent. =⇒ Clip\nR@1 R@5 R@1 R@5\nAll Max(Batch, 16) All 77.6±0.7 96.3±0.4 17.5±0.3 40.7±0.1\nAll Max(Batch, 16) Nonzero 74.7 ±2.0 95.0±0.6 16.9±0.5 39.7±0.8\nAll Batch All 77.4 ±1.5 96.2±1.6 17.2±0.6 39.9±0.3\nNonzero Batch Nonzero 74.2 ±2.6 94.7±0.7 16.8±0.3 40.2±0.6\nAll Batch Nonzero 77.2 ±1.0 95.8±0.8 16.7±0.4 40.2±0.3\nImportance of low-level supervision. In Fig. 4, we study the effect of adding uniform noise to the\nstart and end frame index of each clip in ActivityNet-captions from the interval [−Nf ∗P,+Nf ∗P].\nNf is the total number of video frames and P is the noise percentage. We also perform a \"full\" noise\nexperiment where we drop the temporal alignment labels of clips and sentences completely. We\nobserve that increasing the noise from 0% to 40% consistently decreases the performance as labels\nget less reliable. For noise more than 40%, we do not observe signiﬁcant changes in performance\nanymore. This is probably because at this noise level the labels become useless and are ignored. Still\na good performance is obtained.\nThe study shows that the model is robust to noisy and missing low-level supervision and COOT is\nstill able to capture useful dynamics between low-level and high-level semantics.\nImpact of Text Encoding. We conduct ablation experiments to evaluate the importance of the\ntext encoder for representation learning task. The ablation study results are shown in Table 8. We\nﬁrst evaluate the COOT model and the HSE [21] model with GloVe [90] features. We then replace\n17\nTable 12: Video-paragraph retrieval results on AcitvityNet-captions dataset (val2).\nMethod Par. =⇒ Video Video =⇒ Par.\nR@1 R@5 R@1 R@5\nFSE 11.5 31.0 11.0 30.6\nHSE 32.9 62.7 32.6 63.0\nCOOT 48.5 78.9 48.9 79.5\nFigure 4: Noise vs Performance study on ActivityNet-captions dataset (val1)\n0 10 20 30 40 50 60 70 80 90 100 200 Full\nNoise %\n45\n50\n55\n60\n65\n70\n75\n80\n85Retrieval %\nVideo R@1\nVideo R@5\nText R@1\nText R@5\nGloVe features with features obtained from a pretrained Bert [ 19] model. Note that we feed an\nentire paragraph consisting of several sentences into Bert, leveraging high-level context. Our results\nshow that replacing ﬁxed word embeddings with the context-aware Bert features can signiﬁcantly\nimprove model performance over different architectures. Both models are relatively shallow (1 layer\nof attention / GRU respectively), which may be the reason why the deeper Bert model (13 layers) can\nhelp understand the text better.\nImpact of Alignment Losses. We study the effect of alignment losses on the performance in\nTable 9. To give a more diverse picture, we evaluate the losses under different settings: We use\nthree different low level pooling methods (Averagepool, Maxpool and Attention-aware Feature\nAggregation) and selective disable the Cross-Modal Cycle Consistency loss and the global context\nattention in the Contextual Transformer.\nWe found that removing any or all of the three alignment losses signiﬁcantly decreases performance.\nIn addition, we observed that clustering losses have a positive impact on the performance of the\nmodel.\nNote that we also tried clustering the global context and found it to be not helpful. It might be a too\nstrong constraint on our low-level embedding network.\nStudy on sequence pooling methods. In Table 10 we replace our low-level (frames, words) and\nhigh-level (clips, sentences) pooling methods and evaluate the performance. Interestingly, we get\nthe best results with our AF module on the low level, while averagepooling outperforms it on the\nhigh level. Removing our components (CMC, CoT) and replacing AF with maxpooling provides a\nconsiderably strong baseline compared to our full model.\nThe sequence length is higher on the low level (e.g. up to 80 frames) than on the high level (on average\n3.6 clips per video). Additionally, there are stronger temporal relationships between semantics in the\nlow level. The AF module can learn to capture these relationships and improve the features. However\non the high-level, the semantics have more independent meanings which makes it much harder for\nAF to model the temporal relationships between them.\nNote that to give a more fair comparison, we change the optimizer setting when adding AF on the\nhigh level, as denoted with *. We observe that concatenating the output of 2 AF modules on the high\nlevel improves the performance, suggesting that the two modules learn to attend to different features.\n18\nWe also vary our approach on averagepooling on the high level and report results In Table 11. Working\non variable length inputs, there are a number of design choices to make. We evaluate the following\nones: 1) Summing over the unmasked sequence elements (nonzero inputs) only or summing over\nboth sequence and padding elements (zero inputs). 2) Minimum padding to the maximum sequence\nlength in the minibatch or to a length of at least 16. 3) Obtaining the average by dividing the sum by\nthe length of nonzero elements or by the length of all elements.\nOn ActivityNet-captions (split val1, average sequence length 3.6), we show our non-standard\napproach of including padding tokens in the sum but dividing by the length of non-padding tokens\nworks well. Note that in all other reported experiments, we use this version of averagepooling.\nOn Youcook2 (split val, average sequence length 7.6), we cannot reproduce this large gain in\nperformance but the approach still works reasonably well. The good results when padding to a\nminimum length of 16 might be due to the average length being closer to 16 than in ActivityNet-\ncaptions.\nA.3 Retrieval on ActivityNet-captions (split val2)\nWe provide retrieval results for ActivityNet-Captions (val2 split) in Table 12.\nA.4 Qualitative Results\nActivityNet-Captions. To further check whether our COOT model can learn the semantic align-\nment of video and text well, we provide qualitative examples for the retrieval task on the ActivityNet-\ncaption dataset (val1 split, 4917 video-paragraph pairs). Note that any spelling errors in the dataset\nare not corrected. As shown in Table 13 and Table 14, the model learns to semantically align the\nvideo and paragraph embeddings. Even for imperfect rankings, the model retrieves semantically\nsimilar items.\nYouCook2. We also present a set of qualitative clip-to-sentence and sentence-to-clip retrieval\nexamples for the YouCook2 dataset (val split, 3492 clip-sentence pairs, 457 video-paragraph pairs).\nTable 15 and Table 16 show several examples where we can reasonably retrieve similar semantics,\neven when the wrong object is recognized (Table 15-Right).\nt-SNE Visualization of Embeddings. We project the video embeddings of Activitynet dataset to\n2D space using t-SNE [91] and visualize each point with a sample frame from the video. As shown in\nFigure 5, the embeddings are clustered semantically around activities and videos with similar content\nare in close neighborhood.\nA.5 Captioning Results\nTo expand upon the qualitative captioning results, we provide evaluation on samples that are not\ncherry-picked for Youcook2 (val split) and ActivityNet (ae-val and ae-test split) in Tables 17, 18,\n19.\n19\nTable 13: Qualitative Results on Activitynet for Paragraph-to-Video Retrieval. For each text\nquery, we show some frames from the top three ranked videos together with the correct video. For\nclariﬁcation, we show video results with text. Left: The correct video has a high rank and all top\nresults are very relevant to the query. Right: Even though the correct video is ranked low, the top\nvideos are semantically similar to the text query.\nQuery: A man is standing inside a workshop. He leans\nover, welding a piece of metal. Sparks ﬂy as he welds.\nQuery: A person is kneeling down painting something\non the ground. They smooth out the paint. They con-\ntinue painting layers on top of the paint.\nRank\nScore\nRetrieved Video Rank\nScore\nRetrieved Video\n1\n0.827\n1\n0.654\n2\n0.821\n2\n0.643\n3\n0.816\n3\n0.640\n4\n0.783\n48\n0.438\n20\nTable 14: Retrieval Video to Paragraph on Activitynet. Long paragraphs have been shortened, as\nindicated by \"[...]\". Left: The correct paragraph is identiﬁed with a considerable score margin to the\n2nd place. Right: The top results are from the same activity as the input video (dancing).\nQuery: Query:\nRank\nScore\nRetrieved Text Rank\nScore\nRetrieved Text\n1\n0.813\nA woman is resting next to crashing water.\nShe is smoking a pipe. She blows out a\nplume of smoke.\n1\n0.717\nA woman stands in front of a crowd of people\non a public sidewalk and dances with a male\ndance partner in ballroom style dance. [. . . ]\n2\n0.654\nA close up of a man’s chin is shown followed\nby him smoking a hookah pipe. He takes the\npipe out of his mouth and blows the smoke into\nthe camera.\n2\n0.678\nA woman in a leather dress and hat dances\nin a public station. A man joins her, dancing\nside to side in a ﬂamenco style dance. They\ncontinue dancing as a small crowd gathers to\nwatch. [. . . ]\n3\n0.641\nA close up of tin foil is shown leading a woman\ntaking a large hit out of a hookah hose. She\ncontinues smoking out of the hookah [. . . ]\n3\n0.608\nA large group of people are seen standing\naround a city center waiting for people to ar-\nrive. Girls dancing are seen walking through\nthe parade as other people watch on the side.\n[. . . ]\n4\n0.601\nA woman is laying back in a chair getting her\nlip pierced. The piercer removes the tool and\npulls on her lip.\n16\n0.496\nPeople are dancing in a street. People are\nstanding on the sidelines watching them.\nThey continue dancing on a street.\n21\nTable 15: Sentence-to-Clip Retrieval on Youcook2. For clariﬁcation, we show clip results with\ncorresponding text. Left: The model ranks the correct video at the top and even distinguishes it from\nother videos about the same activity. Right: The slicing task is correctly recognized, but the model is\nnot able to understand which object is being chopped (bamboo shots).\nQuery: melt butter in the pan Query: slice the bamboo shoots into strips\nRank\nScore\nRetrieved Clip Rank\nScore\nRetrieved Clip\n1\n0.642\n1\n0.621\n2\n0.583\n2\n0.610\n3\n0.561\n3\n0.609\n4\n0.553\n168\n0.326\n22\nTable 16: Clip-to-Sentence Retrieval on Youcook2 val set. Left: The model gives high relative\nscore to the relevant text but has problems visually distinguishingapples from potatoes. Right:: Wine\nis confused with oil and the herbs cannot be identiﬁed precisely to be bay leaves and thyme. Identical\nsentences can produce different results, since the Bert [19] text encoder takes paragraph context into\naccount and therefore the model inputs differ.\nQuery: Query:\nRank\nScore\nRetrieved Text Rank\nScore\nRetrieved Text\n1\n0.523\nplace the potato wedges into a pan of hot oil 1\n0.705\nadd oil and herbs to a pan\n2\n0.514\ncook the apple slices in the pan 2\n0.622\nheat oil to 365 in a pan\n3\n0.510\nremove the potatoes from the oil and place on\npaper towel\n3\n0.603\nheat some oil in a pan\n4\n0.497\nadd oil to the pan and fry the hash browns 4\n0.579\nheat some oil in a pan\n5\n0.495\nfry the potatos in oil 5\n0.575\nadd oil to a pan\n6\n0.480\nadd the potatoes to the pan 6\n0.570\nheat some olive oil in a pan\n7\n0.477\nheat the apple in a pan with some oil 7\n0.567\nheat some oil in a pan\n8\n0.475\npierce the knife inside the potatoes and ﬁnd if\nthe potatoes are cooked properly\n8\n0.564\nheat oil in a pan\n9\n0.474\nmelt little butter and olive oil in a pan 9\n0.564\nheat some oil cumin seeds and coriander seeds\nin a pan\n10\n0.470\nfry the potatoes in a deep fryer 85\n0.385\nadd white wine onions a bay leaf and thyme\nto the pot\n23\nTable 17: Random Captioning samples on YouCook2 (val split).\nMART: Cook the bacon in a pan. Add chopped\nonions to the pan. Add chopped carrots. Add chopped\ntomatoes to the pan. Add the chicken to the pan.\nCOOT (Ours): Fry the beef in a pan. Add onion\nand carrot to the pan. Add the chicken to the pan.\nAdd the tomatoes and stir. Add the potatoes to the pan.\nGT: Brown 400gm of sliced beef on a hot pan. Fry\nonions until golden then add garlic carrots and red\npepper fry for 5 mins. Now add the beef 2 tbsp of\nﬂour 1 tsp of paprika 1 tbsp of tomato puree 2 bay\nleaves and 300ml beef stock. Add 200 gram canned\ntomato 100ml red wine sour cream and mix well let it\nsimmer for 1 5 hour. Now add 400gm of baby potato\nand mix it let it cook for 30 more min.\nMART: Add ﬂour to a bowl and whisk. Cut the\nchicken into pieces. Coat the chicken in ﬂour. Coat\nthe chicken in ﬂour egg and breadcrumbs. Fry the\nchicken in a pan. Drizzle the sauce on top of the\nbread. Add sauce to the pizza. Bake the dish in the\noven.\nCOOT (Ours): Mix parmesan cheese black pepper\nand garlic powder. Cover the chicken in the bag.\nCoat the chicken in the ﬂour. Coat the chicken in\nthe egg and coat with ﬂour. Place the chicken in a\npan and fry it on a pan. Pour sauce on top of the\nchicken and top with mozzarella cheese. Sprinkle\nparmesan cheese on top. Bake the chicken in the oven.\nGT: Mix bread crumbs and parmesan cheese. Pound\nthe chicken. Rub salt and pepper onto the chicken.\nRub ﬂour onto the chicken dip it in egg and coat with\nbreadcrumbs. Fry the chicken in a pan. Spread sauce\nover the chicken. Top the chicken with mozzarella\ncheese. Bake the chicken in the oven.\nMART: Add tomatoes and beef to a pot. Add water to\nthe pan. Add tomato puree and salt. Add the beef and\nparsley to the soup. Add the beef to the pot. Add water\nto the soup and let it simmer. Add the soup to the soup.\nCOOT (Ours): Add the tomatoes and onions to a\nfood processor and blend them. Add the tomatoes and\na bay leaf to the pot. Add the tomatoes and simmer.\nRemove the tomatoes from the pot and let it cook.\nRemove the tomatoes from the pot and let it cook.\nStrain the soup to a boil and let it boil. Turn on the\nheat and heat to a boil.\nGT: Add tomato onion green chili and rice to a pan.\nAdd water to the pan. Boil the ingredients and then\nturn down the heat. Strain the ingredients. Blend the\ningredients. Add the water to the mixture and strain.\nBoil the soup.\nMART: Add ﬂour to a bowl and whisk. Cut the\nchicken into pieces. Coat the chicken in ﬂour. Coat\nthe chicken in ﬂour egg and breadcrumbs. Fry the\nchicken in a pan. Drizzle the sauce on top of the\nbread. Add sauce to the pizza. Bake the dish in the\noven.\nCOOT (Ours): Mix parmesan cheese black pepper\nand garlic powder. Cover the chicken in the bag.\nCoat the chicken in the ﬂour. Coat the chicken in\nthe egg and coat with ﬂour. Place the chicken in a\npan and fry it on a pan. Pour sauce on top of the\nchicken and top with mozzarella cheese. Sprinkle\nparmesan cheese on top. Bake the chicken in the oven.\nGT: Mix bread crumbs and parmesan cheese. Pound\nthe chicken. Rub salt and pepper onto the chicken.\nRub ﬂour onto the chicken dip it in egg and coat with\nbreadcrumbs. Fry the chicken in a pan. Spread sauce\nover the chicken. Top the chicken with mozzarella\ncheese. Bake the chicken in the oven.\n24\nTable 18: Random Captioning samples on ActivityNet (ae-val split).\nMART: A man is seen speaking to the camera\nand leads into him holding up various objects and\npresenting them to. He then cuts the knife and cuts\nthe sandwich while still speaking to the camera. He\nthen puts the sandwich into the pan and cuts it in half.\nHe then puts the sandwich into the sandwich and puts\nit in the end.\nCOOT (Ours): A chef demonstrates how to make\na sandwich using bread , then he puts a knife in a\nkitchen and. Then , the man puts the bread on a\nbread and cuts it in half. After , the man puts the\nsandwich in the bread and put it in a plate. Next ,\nthe man cuts the bread and put on top of the sandwich .\nGT: A man shows ingredients for a mortadella\nsandwich. The man cuts the bred in four pieces and\nputs mustard and then brown on the stove. Then, the\nman fries an egg and puts it on the bread as well the\nmortadella, green leaves, cheese and ketchup. After,\nthe man cuts the sandwich in two and eat one.\nMART: A person is seen sitting in front of a large\npile of grass and holding a stick. The person then puts\nthe tire on the machine and begins putting the tire on.\nCOOT (Ours): A person is seen using a tool on a\nmachine and piecing together with the camera. The\nman continues to use the machine on the machine and\nends by taking out more out of the machine.\nGT: A person is seen walking in with a tire on a plank\nand painting the tire. The person then un does the tire\nand places the rubber tightly around the side.\nMART: A small group of people are seen swimming\naround a pool throwing a ball around to one another.\nThe people continue playing with one another and\nend by throwing the ball back and fourth.\nCOOT (Ours): A large group of people are seen\nswimming around a pool throwing a ball around\nto one another. The people continue playing with\none another and ends with a large group of people\nwatching on the sides.\nGT: A video of water polo is shown in the gym. A\nfew people watch and the ball goes back and forth.\nMART: A person is seen sitting in front of a large\npile of grass and holding a stick. The person then puts\nthe tire on the machine and begins putting the tire on.\nCOOT (Ours): A person is seen using a tool on a\nmachine and piecing together with the camera. The\nman continues to use the machine on the machine and\nends by taking out more out of the machine.\nGT: A person is seen walking in with a tire on a plank\nand painting the tire. The person then undoes the tire\nand places the rubber tightly around the side.\n25\nTable 19: Random Captioning samples on ActivityNet (ae-test split).\nMART: A woman stands on front a house talking.\nThe woman drives the lawn mower with a mower.\nThe woman drives the lawn mower. The woman\npushes the lawn mower along the grass. The woman\ntalks to the camera.\nCOOT (Ours): We see the title on the ﬁeld , white\nand white text. We then see a man mowing his lawn.\nThe man stops and talks to the camera. The man stops\nand turns around. We then see the grass again.\nGT: The video begins with a picture of a lawn along\nwith a company name and website. The video cuts\nto a man riding a lawnmower, cutting the grass in a\nnice neighborhood. When he begins, some kids are\nplaying in the road. At one point, a car passes by. The\nvideo ends with the picture of the lawn showing the\ncompany name and website.\nMART: A group of women are dancing on a stage.\nThey are dancing together in a room. They are\ndancing together.\nCOOT (Ours) : A large group of girls are seen\nstanding together followed by a woman dancing and\nperforming a dance routine. The woman continues\nspeaking to the camera while more people are seen\ndancing around and leads into a group of. The group\ncontinues dancing with one another and ends with a\nwoman speaking to the camera.\nGT: Several girls are in a classroom dancing and\ndoing ballet. The instructor then comes to talk brieﬂy\nbefore continuing on coaching the girls. After,the\nexercises continue and the girls do leaps and jumps\nin the room before the outside of the dance studio is\nshown.\nMART: People are gathered around a street watching.\nThey are holding ﬂags in their hands. A man in a\nwhite shirt is standing next to a fence.\nCOOT (Ours): A man plays bagpipes while people\nwatch on the sidewalk. A person in a black shirt plays\nthe bagpipes. A person in a white shirt walks past the\nperson.\nGT: A man on stilts is playing the bag pipes on a\nstreet. A bus passes on the street behind the man. A\nstreet sign on a pole is shown.\nMART: A group of women are dancing on a stage.\nThey are dancing together in a room. They are\ndancing together.\nCOOT (Ours) : A large group of girls are seen\nstanding together followed by a woman dancing and\nperforming a dance routine. The woman continues\nspeaking to the camera while more people are seen\ndancing around and leads into a group of. The group\ncontinues dancing with one another and ends with a\nwoman speaking to the camera.\nGT: Several girls are in a classroom dancing and\ndoing ballet. The instructor then comes to talk brieﬂy\nbefore continuing on coaching the girls. After,the\nexercises continue and the girls do leaps and jumps\nin the room before the outside of the dance studio is\nshown.\n26\nFigure 5: Visualization of the video embedding space with t-SNE on ActivityNet-Captions. We\napply t-SNE to reduce the video embedding space to 2 dimensions and visualize videos by one\nsample frame.\n27",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7366711497306824
    },
    {
      "name": "Transformer",
      "score": 0.6016842722892761
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5745640397071838
    },
    {
      "name": "Granularity",
      "score": 0.48942747712135315
    },
    {
      "name": "Feature learning",
      "score": 0.44684356451034546
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4439641237258911
    },
    {
      "name": "Sentence",
      "score": 0.4171789586544037
    },
    {
      "name": "Natural language processing",
      "score": 0.36999261379241943
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}