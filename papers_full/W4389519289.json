{
  "title": "Select, Prompt, Filter: Distilling Large Language Models for Summarizing Conversations",
  "url": "https://openalex.org/W4389519289",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2516602895",
      "name": "Minh Quang Pham",
      "affiliations": [
        null
      ]
    },
    {
      "id": null,
      "name": "Sathish Indurthi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2478561158",
      "name": "Shamil Chollampatt",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A3137935677",
      "name": "Marco Turchi",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2127170756",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W4381558659",
    "https://openalex.org/W3208860256",
    "https://openalex.org/W4287854421",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2250473257",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4362707075",
    "https://openalex.org/W3196820561",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3137501073",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4225934689",
    "https://openalex.org/W4385571411"
  ],
  "abstract": "Large language models (LLMs) like ChatGPT can be expensive to train, deploy, and use for specific natural language generation tasks such as text summarization and for certain domains. A promising alternative is to fine-tune relatively smaller language models (LMs) on a particular task using high-quality, in-domain datasets. However, it can be prohibitively expensive to get such high-quality training data. This issue has been mitigated by generating weakly supervised data via knowledge distillation (KD) of LLMs. We propose a three-step approach to distill ChatGPT and fine-tune smaller LMs for summarizing forum conversations. More specifically, we design a method to selectively sample a large unannotated corpus of forum conversation using a semantic similarity metric. Then, we use the same metric to retrieve suitable prompts for ChatGPT from a small annotated validation set in the same domain. The generated dataset is then filtered to remove low-quality instances. Our proposed select-prompt-filter KD approach leads to significant improvements of up to 6.6 ROUGE-2 score by leveraging sufficient in-domain pseudo-labeled data over a standard KD approach given the same size of training data.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12257–12265\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSelect, Prompt, Filter: Distilling Large Language Models\nfor Summarizing Conversations\nMinh-Quang Pham∗, Sathish Reddy Indurthi∗,\nShamil Chollampatt, Marco Turchi\nZoom Video Communications\nminhquang.pham@zoom.us, sathishreddy.indurthi@zoom.us,\nshamil.chollampatt@zoom.us, marco.turchi@zoom.us\nAbstract\nLarge language models (LLMs) like ChatGPT\ncan be expensive to train, deploy, and use for\nspecific natural language generation tasks such\nas text summarization and for certain domains.\nA promising alternative is to fine-tune relatively\nsmaller language models (LMs) on a particu-\nlar task using high-quality, in-domain datasets.\nHowever, it can be prohibitively expensive to\nget such high-quality training data. This issue\nhas been mitigated by generating weakly su-\npervised data via knowledge distillation (KD)\nof LLMs. We propose a three-step approach\nto distill ChatGPT and fine-tune smaller LMs\nfor summarizing forum conversations. More\nspecifically, we design a method to selectively\nsample a large unannotated corpus of forum\nconversation using a semantic similarity met-\nric. Then, we use the same metric to retrieve\nsuitable prompts for ChatGPT from a small an-\nnotated validation set in the same domain. The\ngenerated dataset is then filtered to remove low-\nquality instances. Our proposed select-prompt-\nfilter KD approach leads to significant improve-\nments of up to 6.6 ROUGE-2 score by leverag-\ning sufficient in-domain pseudo-labelled data,\nover a standard KD approach given the same\nsize of training data.\n1 Introduction\nLarge language models (LLMs) such as the GPT-\nseries models have demonstrated great strengths in\na range of natural language understanding and gen-\neration tasks with their ability to do few-shot or in-\ncontext learning (ICL, Brown et al., 2020). In this\nscenario, the input for a desired task is enhanced by\nincorporating a few examples (or demonstrations)\nof that particular task given as the prompt to the\nLLM. This helps to customize and optimize the\nLLM at inference time, leading to higher-quality\nresults. Many of these LLMs, however, are astro-\nnomically expensive and environmentally unsus-\ntainable to train and use (Bender et al., 2021; Wu\n∗Equal contribution\nChatGPT\nModelRAW\nDATASET\nannotation\nfine-tuning\nPROMPT\nVALID\nDATASET\nDISTILL\nDATASETSELECT\nFILTER\nFigure 1: Schematic diagram of select-prompt-filter\nknowledge distillation.\net al., 2022; Li et al., 2023). An alternate strategy is\nto fine-tune relatively much smaller language mod-\nels (LM) like BART (Lewis et al., 2020) or T5 (Raf-\nfel et al., 2020) on the target task. These fine-tuned\nmodels, however, require large annotated training\ndatasets to achieve competitive results, preventing\nthe adoption of this approach for a wide variety of\ntasks and domains. Obtaining sufficient annotated\ndata from human annotators can also be substan-\ntially expensive (Wang et al., 2021).\nAn approach to address this issue of limited\nhigh-quality annotated data involves creating syn-\nthetic data that extracts and distills knowledge from\nICL-capable LLMs to smaller specialized models.\nThis technique of knowledge distillation (Hinton\net al., 2015; Kim and Rush, 2016) helps specialize\nsmaller pre-trained LMs to particular tasks using\nlarge amounts of such synthetic annotations. For\nexample, Wang et al. (2021) use GPT-3 to annotate\ntraining data for nine NLP tasks, including summa-\nrization, estimating up to 96% reduction in costs\nover using crowdsourcing and achieving similar\nperformance. Ding et al. (2022) use GPT-3 to get\nlabels and even generate data from scratch for se-\nquence tagging tasks. However, previous work on\ndistillation addresses the data annotation process\nwithout careful data selection, prompt engineering,\n12257\nor data filtering, leading to suboptimal results.\nTo get a better-distilled model and optimize the\ncost of synthetic data annotation, we propose a sys-\ntematic distillation approach, named select-prompt-\nfilter knowledge distillation (SPF KD), for summa-\nrization. We use ChatGPT as the ICL-capable LLM\nthat we distill. In this paper, we focus on summariz-\ning online messaging conversations (e.g., forums)\nfor which human-annotated datasets are not pub-\nlicly available, though our method can be applied\nto other low-resource domains as well. First, a suit-\nable subset of conversations is selected for distilla-\ntion from a large unannotated raw corpus, based on\nsimilarity to a small annotated validation set. Then,\nfor each conversation in the selected subset, seman-\ntically similar conversations are retrieved from the\nvalidation set and sent to ChatGPT as the prompt\nto get its synthetic summary. Liu et al. (2022a)\nuse a similar prompt-retrieval approach based on\nvarious sentence embeddings to improve GPT-3\nperformance on downstream tasks, although not in\nthe context of synthetic data annotation. Thereafter,\nwe apply a filter on the generated conversation-\nsummary pairs to remove spurious and low-quality\nannotations. Figure 1 shows a schematic overview\nof our approach. Our proposed method uses a\nsemantic similarity score (Cer et al., 2018) and\na reference-less evaluation measure (Egan et al.,\n2022) at different steps. We evaluate our approach\non three forum and email conversation test sets. We\nshow that the proposed KD method is able to sig-\nnificantly improve summarization performance of\nup to 6.6 ROUGE-2 scores compared to a standard\nKD approach.\n2 Approach\nOur method addresses the lack of high-quality\nannotated summarization datasets on certain do-\nmains to fine-tune smaller summarization models\nvia sequence-level knowledge distillation (Kim and\nRush, 2016). We propose a three-step approach to\nget the annotated training data from ICL-capable\nLLMs like ChatGPT, which acts as our teacher\nmodel that we distill. The three steps (Figure 2)\nin our proposed select-prompt-filter (SPF) knowl-\nedge distillation approach are (i) data selection (ii)\nprompt retrieval, and (iii) data filtering. These\nsteps are done to get high-quality synthetic sum-\nmaries for the target task of summarizing conver-\nsations. The synthetic dataset will then be used to\nfine-tune a smaller pretrained LM (student model).\nFundamental to our approach are two metrics that\nwe use in the above steps:\nUSE-Cosine: We use Universal Sentence Encoder1\n(USE, Cer et al. 2018) to get the embeddings of\na conversation given as a text string. USE inter-\nnally uses a Deep Averaging Network (Iyyer et al.,\n2015) that averages the embeddings of the words\nand bigrams and passes it through a feed-forward\nnetwork. We compute the cosine distance between\nthe two conversation embeddings. We refer to this\nmetric as USE-cosine, which we use for computing\nconversation-conversation similarity in the selec-\ntion and prompt retrieval steps.\nShannon Score : Shannon Score 2 (Egan et al.,\n2022) is a reference-free summary evaluation met-\nric that measures the difference in the ability of a\nlanguage model to generate the conversationCwith\nand without the summary S. Specifically, GPT-2 is\nused to compute theinformation difference, i.e., the\ndifference between log probabilities of generating\nCgiven Sas the prompt and generatingCwithout it.\nThe final metric, Shannon Score (SS), is the ratio\nof this value to the information difference when the\nC itself is used as the summary. This is under the\nassumption that the conversation conveys the entire\ninformation and thus the denominator becomes the\nupper bound of the information difference.\nSS(C, S) =log P(C|S) −log P(C)\nlog P(C|C) −log P(C) (1)\nWe use Shannon Score to compute the\nconversation-summary relevance and filter\nthe synthetic annotations generated by ChatGPT\nthat do not satisfy a certain threshold.\nIn our proposed approach, we rely on the avail-\nability of a large unannotated training dataset of\nforum threads (Dt) for knowledge distillation for\nour target task. We use a large corpus of forum\nconversations for this. We also assume the avail-\nability of a small annotated validation set Dv with\nM conversation-summary pairs. We describe the\nthree steps used in our approach in greater detail\nbelow.\n2.1 Data Selection\nTo control costs when using ChatGPT for annota-\ntion, we use a similarity-based selection approach\nto select N conversations from Dt for generating\nsynthetic summaries. As shown in Figure 2 (left),\n1https://tfhub.dev/google/\nuniversal-sentence-encoder/4\n2https://github.com/PrimerAI/blanc/\n12258\nTraining Data\nValidation Set\ntop-k\nSelected Training Data\nValidation Set\ntop-2\nPrompt Retrieval\nChatGPT\nConversation Summary\nData Selection Data Filtering\n \nFigure 2: The three steps of our proposed approach: (i) data selection, (ii) prompt retrieval, and (iii) data filtering.\nfor each validation conversation in Dv we select\ntop k conversations (k ≈N/M) from Dt without\nreplacement using the USE-cosine similarity. In\nthis way, we ensure that we represent every exam-\nple of the validation set independently to capture\nmaximum variance in the training data. N is deter-\nmined based on the available budget. This selected\ntraining data is denoted by Ds, which is a subset of\nDt.\n2.2 Prompt Retrieval\nFor each conversation in the selected training data\nDs, we apply a similarity-based prompt retrieval\ntechnique similar to Liu et al. (2022c) to construct\nthe prompt. We select one or more examples from\nthe Dv that are closest to each conversation in Ds\naccording to the USE-cosine metric (see Figure 2,\ncenter). The retrieved examples along with the cor-\nresponding training conversation to be summarized\nare passed to ChatGPT as the prompt to get the\nsummary (see example prompt in Appendix A.1).\nThus, we get an annotated training set D∗\ns which\nis a set of forum threads and their corresponding\nChatGPT generated summaries.\n2.3 Data Filtering\nSome of the ChatGPT-generated summaries in D∗\ns\ncan be of poor quality and diverge from the con-\nversation significantly. We filter out such spurious\nexamples that do not satisfy the certain threshold\nof Shannon Score, to get a smaller annotated train-\ning dataset D∗\nf (see Figure 2, right). D∗\nf is the\ndistilled training data from the teacher that is then\nused to finetune a student model specialized for\nsummarizing conversations.\n3 Experimental Setup\n3.1 Data\nWe use conversations from StackExchange fo-\nrums3, except non-English language forums and\nStackOverflow due to its size, as the raw unanno-\ntated dataset. For validation and testing, we use\nthe Ubuntu and NYC forum summarization cor-\npora (Bhatia et al. 2014, 100 instances each), and\nthe BC3 email summarization corpus (Ulrich et al.\n2008, 40 instances). We divide each dataset equally\nfor validation and testing. We combine the three\nvalidation sets into a single validation set of 120\ninstances.\nWe use OpenAI ChatGPT as our teacher model\nfor distillation. We restrict to selecting 100K con-\nversations for ChatGPT annotation from StackEx-\nchange to demonstrate a limited budget scenario.\nAnnotating 100K conversations costs about 200\nUSD4 (gpt-3.5-turbo) assuming each annota-\ntion input is under 1000 tokens. Our baseline is\na standard knowledge distillation (Standard KD)\napproach, where a random 100K subset is selected\nfrom StackExchange for distillation. For the base-\nline, we select two random examples as the prompt\nfrom our validation set to be sent to ChatGPT for\ngetting synthetic summaries and we do not filter\nthe results. During the data selection step of our\nproposed SPF KD approach, for each conversation\nin our validation set, we select k = 833 exam-\nples (100000\n120 ≈833) from StackExchange based on\nUSE-cosine similarity (Section 2.1). For each se-\nlected conversation, we prompt ChatGPT with two\nexamples from the validation set that are closest\nto it based on the USE-Cosine measure. For data\nfiltering, we set a Shannon Score threshold of 0.15\nto retain at least 50% of annotated data.\n3https://archive.org/download/stackexchange\n4https://openai.com/pricing\n12259\nUbuntu NYC BC3\nR-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L\nChatGPT (zero-shot) 39.9 17.4 29.1 40.7 14.5 26.2 43.6 14.5 27.0\nChatGPT (one-shot, random) 44.6 21.2 31.9 43.3 16.0 27.5 43.3 15.2 28.3\nChatGPT (two-shot, random) 47.3 24.4 36.0 42.5 16.4 27.9 44.8 16.2 28.5\nChatGPT (two-shot, USE-cosine) 49.0 25.9 37.2 42.9 16.6 28.3 46.9 16.0 29.9\nPEGASUS (Zhang et al., 2020) 27.9 17.0 23.0 32.0 10.6 19.9 27.8 7.8 16.6\nBRIO (Liu et al., 2022b) 33.4 17.6 24.0 35.0 12.0 21.0 32.3 10.8 20.0\nStandard KD 48.8 26.8 37.5 41.5 17.3 27.7 44.7 17.3 29.9\nSPF KD 55.4 33.4 43.5 41.7 18.2 28.3 47.2 19.8 32.3\nTable 1: Performance of our proposed select-prompt-filter (SPF) KD and baselines.\n3.2 Model\nFor both Standard and SPF KD, we fine-tune one\nstudent model each with the corresponding dis-\ntillation data and evaluate on all three test sets.\nSimilar to the findings of Demeter et al. (2023),\nour preliminary investigations indicated that a fine-\ntuned BART model produces better-quality sum-\nmaries compared to fine-tuning models such as\nPEGASUS (Zhang et al., 2020) Hence, we use\nan XSum-finetuned (Narayan et al., 2018) BART-\nlarge5 (406.3M params, Lewis et al. 2020) as the\nstudent model for both KD approaches. For each\nof our KD experiments, we further fine-tune this\nBART model on the corresponding distilled dataset\nfor 5 epochs, beyond which we observed overfit-\nting on our validation set. We used a batch size 16\nand learning rate 2e-5.\nWe report results using various prompting strate-\ngies for ChatGPT: with no priming examples (zero-\nshot), with one or two randomly-selected priming\nexample(s) from the validation set (one or two-shot,\nrandom), and with two validation examples closest\nto the test conversation based on USE-cosine mea-\nsure (two-shot, USE-cosine). ChatGPT (two-shot,\nrandom) is the teacher model for Standard KD due\nto the randomly selected prompting examples, and\nChatGPT (two-shot, USE-cosine) is the teacher\nmodel in SPF KD as it incorporates the prompt\nretrieval step. We also include results of two com-\npetitive off-the-shelf summarization models: PE-\nGASUS6 (570.8M params, Zhang et al. 2020) and\nBRIO7 (406.3M params, Liu et al. 2022b).\n5https://hf.co/facebook/bart-large-xsum\n6https://hf.co/google/pegasus-large\n7https://hf.co/Yale-LILY/brio-cnndm-uncased\n4 Results and Analysis\nIn Table 1, we report the results of the proposed\nSPF-KD approach compared to standard KD, both\nusing a pre-trained BART model fine-tuned on\nannotations generated by ChatGPT. Our baseline,\nstandard KD, outperforms PEGASUS (Zhang et al.,\n2020), and BRIO (Liu et al., 2022b) showing the\nefficacy of KD and the importance of adding Stack-\nExchange data. Compared to standard KD, our\nSPF KD achieves substantial gains of 6.6, 0.9, and\n2.5 ROUGE-2 scores on Ubuntu, NYC, and BC3,\nrespectively, showing the strength of our proposed\napproach. The larger magnitude of improvement\non Ubuntu can be attributed to a large proportion of\nsimilar forums in our StackExchange dataset that\nour data selection step may have over-represented.\nSimilar to Wang et al. (2021), we also observe that\nKD using ChatGPT summaries outperforms the\ncorresponding ChatGPT teacher model itself. We\nattribute this to fine-tuning the smaller model on\nconversational (forums) data and being specialized\nto do only the summarization task. We provide a\nfew examples of the outputs of the models in Ap-\npendix A.2. This makes KD, especially our SPF\nKD, an effective and cheap strategy to build spe-\ncialized summarization models without relying on\nexternal services.\n4.1 Ablation study\nWe tease apart the contribution of each step of\nour proposed approach to the overall performance\nimprovement over the standard KD approach (see\nTable 2). We find that each of the three steps\nimproves on the three datasets. Moderate gains\nare observed in NYC on adding each step. The\ndata selection step notably improves BC3. Data\nfiltering consistently improves performance across\nthe three test sets. The biggest improvements are\n12260\nUbuntu NYC BC3\nStandard KD 26.8 17.3 17.3\n+ Data Selection 28.5 17.4 19.7\n+ Prompt Retrieval 32.4 17.8 19.7\n+ Data Filtering 33.4 18.2 19.8\nTable 2: ROUGE-2 results of using each step in the\nselect-prompt-filter KD approach.\nFigure 3: Average ROUGE-2 scores vs. percentage of\ndistillation data used\nobserved in the Ubuntu test set at each step. We\nattribute this to the abundance of Ubuntu-related\nforums in StackExchange which are adequately\nexploited in our approach.\n4.2 Distillation Cost\nWe study the performance of the standard KD\n(Baseline) and our SPF KD approach under lower\nChatGPT-annotation budgets. To do this, we train\nthe student model using randomly sampled subsets\nof varying sizes of the distillation data obtained\nusing both Standard KD and SPF KD (after filter-\ning) and compute the average ROUGE-2 across\nthe three test sets. This is plotted in Figure 3. We\nfind that fine-tuning the student model with 50% of\nSPF KD training data achieves a similar ROUGE-2\nscore compared to utilizing the entire Standard KD\ntraining data.\n4.3 Domain Relevance\nWe can use the aggregated USE-cosine metrics\nto estimate the relevance of the selected training\ndata for each domain represented by each valida-\ntion set. For every validation example in the three\ndomains (Ubuntu, NYC, and BC3), we compute\nthe mean USE-cosine across the 833 correspond-\ning selected training examples. We then average\nthis value across all validation examples in the cor-\nresponding domain. We find that we get 0.7 for\nUbuntu, 0.4 for NYC, and 0.5 for BC3. This shows\nwe get more similar training examples for Ubuntu\ncompared to the other two domains. Interestingly,\nthe final ROUGE-2 gains using our SPF-KD ap-\nproach also follow the same ranking as this ag-\ngregated USE-cosine measure (Ubuntu > BC3 >\nNYC).\n5 Conclusion\nWe propose a methodical knowledge distillation\napproach that finetunes a smaller LM for summa-\nrizing conversations with synthetic summaries gen-\nerated by ChatGPT. Our approach selects a suitable\nsubset to be distilled, retrieves matching prompts\nfor ChatGPT, and filters the ChatGPT-generated\nsummaries to yield better-quality training data. Our\nproposed method substantially improves over a\nstandard distillation approach on three test sets cov-\nering forum and email conversations.\nLimitations\nThe limitations of our paper are:\n• Our test sets are relatively small, with only\n120 samples. However, this limitation is due\nto the lack of data in the conversational sce-\nnario.\n• Our experiments have been run using 100K\ntraining points while having access to 7 mil-\nlion unlabelled data points.\n• We do not explore a wide variety of prompting\nstrategies.\n• We only validate our data-centric approach on\nthe BART-large model while it can be applied\nto any large pretrained language model.\n• We only use metrics from the ROUGE family\nto evaluate models in our experiments.\nReferences\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency.\nSumit Bhatia, Prakhar Biyani, and Prasenjit Mitra. 2014.\nSummarizing online forum discussions – can dialog\nacts of individual messages help? In Proceedings\nof the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\n12261\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Advances\nin Neural Information Processing Systems.\nDaniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil.\n2018. Universal sentence encoder. arXiv preprint\narXiv:1803.11175.\nDavid Demeter, Oshin Agarwal, Simon Ben Igeri,\nMarko Sterbentz, Neil Molino, John M. Conroy,\nand Ani Nenkova. 2023. Summarization from\nleaderboards to practice: Choosing a representation\nbackbone and ensuring robustness. arXiv Preprint\narxiv:2306.10555.\nBosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing,\nShafiq Joty, and Boyang Li. 2022. Is GPT-3 a good\ndata annotator? arXiv Preprint arxiv:2212.10450.\nNicholas Egan, Oleg Vasilyev, and John Bohannon.\n2022. Play the Shannon game with language mod-\nels: A human-free approach to summary evaluation.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.\nDistilling the knowledge in a neural network. In\nNIPS Deep Learning and Representation Learning\nWorkshop.\nMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,\nand Hal Daumé III. 2015. Deep unordered compo-\nsition rivals syntactic methods for text classification.\nIn Proceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers).\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics.\nPengfei Li, Jianyi Yang, Mohammad A. Islam, and\nShaolei Ren. 2023. Making AI less \"thirsty\": Uncov-\nering and addressing the secret water footprint of AI\nmodels. arXiv Preprint arxiv:2304.03271.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022a. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extraction\nand Integration for Deep Learning Architectures.\nYixin Liu, Pengfei Liu, Dragomir Radev, and Graham\nNeubig. 2022b. BRIO: Bringing order to abstractive\nsummarization. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers).\nYongtai Liu, Joshua Maynez, Gonçalo Simões, and\nShashi Narayan. 2022c. Data augmentation for low-\nresource dialogue summarization. In Findings of the\nAssociation for Computational Linguistics: NAACL\n2022.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing. Association for Computational\nLinguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nJan Ulrich, Gabriel Murray, and Giuseppe Carenini.\n2008. A publicly available annotated corpus for su-\npervised email summarization. In Proceedings of\nthe 23th AAAI Conference on Artiicial Intelligence\nin Enhanced Messaging Workshop.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want to reduce la-\nbeling cost? GPT-3 can help. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021.\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta,\nBilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-\nria Chang, Fiona Aga, Jinshi Huang, Charles Bai,\net al. 2022. Sustainable AI: Environmental implica-\ntions, challenges and opportunities. Proceedings of\nMachine Learning and Systems.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter\nLiu. 2020. PEGASUS: Pre-training with extracted\ngap-sentences for abstractive summarization. In In-\nternational Conference on Machine Learning, pages\n11328–11339.\nA Appendix\nA.1 Prompt Structure\nThe format of the prompt given to ChatGPT is\ngiven below:\n12262\nGiven one example of summarization:\nConversation:\n<example_conversation>\nSummary:\n<example_summary>\n***\nPlease summarize the following conversation:\n<conversation>\nWe pass a prompt in the above format as part of\nthe user’s input to ChatGPT to get a response from\nit.\nA.2 System Outputs\nIn Table 3 and Table 4 we show output summaries\ngenerated by ChatGPT (two-shot, USE-cosine),\nStandard KD, and SPF KD.\n12263\nConversation Summary\n<userA> : I’m driving from Cleveland. I want to stay downtown and see a few sights on\nSaturday, drive to the game on Sunday (unless there’s a shuttle?), then head to the casino or\nNiagara Falls. How far is Seneca from downtown Buffalo? Are there any other casinos? How\nfar is Niagara Falls? Can I see enough from the US side, or should I suck it up and get a\npassport and go into Canada? Any recommendations would be appreciated.\n<userB> : You can pick up a special Game Day bus to Ralph Wilson Stadium:\nhttp://www.nfta.com/metro/gameday-bills.asp Niagara Falls is about 20 minutes from down-\ntown Buffalo. You can enjoy the falls from the U.S. side, but if you want to go see the\nCanadian falls, you won’t need a passport – all you’ll need is a birth certificate and a\ngovernment-issued photo ID. I’ll let someone else answer your casino questions.\n<userA> : Thanks. Can you recommend a hotel that is close to the metro station downtown\nso we can hop the metro to go to the game? The game is November 17—-what weather\nshould I expect?\n<userD> : You can expect the same weather in Buffalo as you get in Cleveland. We get your\nweather probably the day after you get it (from the west). It could rain, it could snow, it\ncould be a nice, sunny day... who knowsRegarding hotels downtown, here’s a list from a\nprevious post: 1) Hampton Inn tripadvisor.comHotel_Review-g60974-d224389-... 2) Adam’s\nMark tripadvisor.comHotel_Review-g60974-d93115-R... 3) Hyatt Regency Hotel tripadvi-\nsor.comHotel_Review-g60974-d93122-R... 4) Comfort Suites tripadvisor.comHotel_Review-\ng60974-d93128-R... You can catch the train on Main Street and it’s free as long as you stay\nabove ground. The bus station where you will most likely catch the shuttle to the game is near\nMain and N. Division St, across from Church St. Niagara Falls: The Seneca Niagara Casino\nis within walking distance of the actual Falls. If it’s a nice day and you feel like walking, it’s\nan easy walk to the Canadian side across the Rainbow Bridge. Just bring you birth cert and\ndriver’s license so they let you back into the USA. There is a casino in downtown Bflo, but\nit’s nothing special, from what I’ve been told.\n<userE> : the metro train does not go out to ralph wilson stadium. the stadium is in orchard\npark, about 25 min south of buffalo. (you’ll pass it on the 90 as you drive up)if you drive to\nthe game, leave your hotel early as traffic can get thick. consider driving, tailgating is art in\nbuffalo! it’s a lot of fun to experience. i would recommend the hampton inn. good central\nlocation for night life.\n<userF> : Don’t waste your time or money at the Canadian casinos. The slots are VERY\ntight and you will never see a minimum below 15/20$. Also, if you’re a smoker, they are\nnon-smoking and do NOT offer free drinks, whereas the Seneca casino on the US side does.\nGo Bills! Boo Browns!\n<userG> : No offense to Gear but the Canadian Casinos are far superior to the Seneca if your\nmotivation is a nice atmoshphere, shopping, and beauty. I can not speak for the gambling\noutcomes. To the Seneca’s credit , they have great restaurants and a very nice concert venue.\nIf you are into casinos, I would check out both....\n<userA> : Thanks for all of your advice. We went ahead and booked the Country Inn near\nthe Falls in Canada for 3 nights (I figure we can check out the Falls since the only time I was\nthere was when I was a toddler). We’ll drive back on Monday and check in to the hotel in\nHamburg (Holiday Inn....cheap) and then go over to the stadium early for some tailgating.\nIt looks like it’s only about 5 miles away, so even with crazy traffic I’m hoping its not too\nbig of a headache. Is alcohol permitted in the parking lot? Little gas grills? And what about\nthe restroom situation—-are there port-o-johns available?? And how soon do they open the\nstadium? Thanks so much for all of your help!\n<userG> : drinks-yes, grills-yes, the lots open at the crack of dawn....it is full contact tail-\ngaiting in Buffalo and I think after experiencing it, you will change your team affiliation.\nAfter going to a few Browns games...I believe Buffalo has the tailgaiting title...but it is very\nclose.....Go Toronto...oh I mean Buffalo....!!\"\nChatGPT: The conversation is about a person who is\nplanning to visit Buffalo to attend a Sabres game. They\nwere looking for recommendations for restaurants near their\nhotel. Participants made several recommendations, including\nSalsarita, Papaya, Buffalo Chop House, and E.B. Green’s.\nThey also asked for advice on the free train that could take\nthem to the arena and back to their hotel. They were also\ninterested in visiting the casino or Niagara Falls. There was\na discussion about the Canadian Casinos versus the Seneca\ncasino on the US side. They also wanted to know about\ntailgating and if alcohol is permitted in the parking lot.\nStandard KD: The conversation is about a person who wants\nto go to Buffalo for a Buffalo Bills game and wants to know\nhow far it is from downtown Buffalo, if there are any other\ncasinos, how far is Niagara Falls, and if they can see enough\nfrom the US side or if they should get a passport and go\ninto Canada. <userB> recommends a special Game Day\nbus to Ralph Wilson Stadium and Niagara Falls. <userD>\nrecommends hotels downtown, including Hampton Inn,\nAdam’s Mark, Hyatt Regency Hotel, and Comfort Suites.\nThe Seneca Niagara Casino is within walking distance of\nthe actual Falls and there is a casino in downtown Buffalo.\n<userF> advises against going to Canadian casinos as they\nare tight and do not offer free drinks. <userG> recommends\nchecking out both Canadian and Seneca casinos. <userA>\nthanks everyone for their advice and decides to book the\nCountry Inn near the Falls in Canada for three nights. They\nwill drive back on Monday and check in to the hotel in\nHamburg and then go over to the stadium early for some\ntailgating.\nSPF KD: A person is driving from Cleveland to Buffalo\nand wants to see a few sights on Saturday, drive to the game\non Sunday, then head to the casino or Niagara Falls. The\nperson wants to know how far Seneca is from downtown\nBuffalo, if there are any other casinos, and if they can see\nenough from the US side, or should they get a passport\nand go into Canada. A reply suggests that the person can\npick up a special Game Day bus to Ralph Wilson Stadium,\nand Niagara Falls is about 20 minutes from downtown\nBuffalo. The reply also recommends a hotel that is close\nto the metro station downtown, and the weather in Buffalo\nis the same as in Cleveland. The Seneca Niagara Casino\nis within walking distance of the actual Falls. If it’s a nice\nday and the person feels like walking, it’s an easy walk to\nthe Canadian side across the Rainbow Bridge. There is a\ncasino in downtown Buffalo but it’s nothing special. Another\nreply suggests leaving the hotel early as traffic can get thick.\nThey recommend the Hampton Inn, which is a good central\nlocation for night life. The Canadian casinos are far superior\nto the Seneca if the person’s motivation is a nice atmoshphere,\nshopping, and beauty.\nReference: A person was driving from Cleveland to Buffalo\nto attend the game on Sunday. Further he was planning to\ngo casino or Niagara Falls. He wanted to how far be Seneca\nfrom downtown Buffalo, were there any other casinos, how\nfar was Niagara Falls and was passport required to go into\nCanada? Someone replied that he can pick up a special Game\nDay bus to Ralph Wilson Stadium. Niagara Falls was about 20\nminutes from downtown Buffalo. To travel to Canadian side\nNiagara fall, birth certificate and a government-issued photo\nID will be needed. The other reply was that there were many\nhotels in the downtown. The trains and buses were available\nfor transportation. The Seneca Niagara Casino was within\nwalking distance of the actual fall. The other suggestion was\nnot to waste time or money at the Canadian casinos. There\nwere some disagreements over the quality of the Canadian\ncasinos.\nTable 3: Summaries generated by ChatGPT (two-shot, USE-cosine), Standard KD, and SPF KD on an example\nNYC forum conversation (usernames are anonymized).\n12264\nConversation Summary\n<userA>: Since yesterday (when I installed the latest Ubuntu ’edgy’ kernel security update), my Serval\nPerformance system has failed to boot several times. This problem required a hard shutdown and reboot.\nThe first time this occurred, I tried booting into recovery mode, which seemed to work around the issue.\nThe second time, even recovery mode did not work; three times I attempting to boot without success.\nSince I could see the boot status this time I noticed the message \"\"BUG: soft lockup on CPU#0\"\". This\ndid not sound very good to me. Fortunately, I have just booted my system successfully again after\nflipping the wireless card switch on the chassis to the ’on’ position. I had previously turned it ’off’\nsince I was not using it at the time and had noticed an apparent CPU bug whereby one of my cores was\nalways at 100% frequency even when not in use according to the system monitor. Anyhow, I don’t\nknow if the switch was the cause/cure or not, because when the problem first began I still managed\nto boot with the switch ’off’ and so far I have only had one success with it ’on’. (I need to do some\nimportant work before I can test this theory and risk not being able to boot again.) Has anyone else\nheard of or experienced a similar problem? -<userA>-\n<userB>: Hi <userA> - I have one other report of \"\"BUG: soft lockup on CPU#0\"\" although it was a\ndifferent machine. I’m trying to re-create the problem on a Serval now and will get back to you shortly.\n<userC>: Perhaps it was caused by this? http://ubuntuforums.org/showthread.php?t=318206\n<userB>: <userA>- we’re not experiencing the problem using nVidia’s driver from the repos - does\nhackmasters thread look relevant?\n<userA>: I do have the driver version mentioned in that notice. However, neither of the files noted\nare missing from /usr/lib/xorg/modules. Also, my boot process was failing well before attempting to\ninitialize X. I now reinstalled the nvidia-glx package just in case. Since flipping the wireless card switch\nback to the ’on’ position, I have not had this problem. I haven’t yet tried switching it ’off’ again to see\nif I can reproduce it. Regarding the issue of one cpu core running at 100% frequency all the time, my\nbest guess now is that it may not be related to the wireless card/driver. I have recently noticed that it the\nissue seems to be related to coming out of hibernation. If I boot my system fresh, I do not observe the\nbehavior, but if I hibernate and resume then I notice it. Many thanks to Carl and hackmeister for your\nhelp. If you happen to discover any more information about the error message I initially posted, let me\nknow. Otherwise, I have my workaround for now. -<userA>-\nChatGPT: The user’s Serval Performance system\nfailed to boot several times after installing the\nlatest Ubuntu ’edgy’ kernel security update. The\nuser noticed the message \"BUG: soft lockup on\nCPU#0\". The user tried booting into recovery\nmode, which worked the first time but not the\nsecond time. The user turned on the wireless card\nswitch, and the problem stopped occurring. The\nuser also noticed that one CPU core was always at\n100% frequency when coming out of hibernation.\nThe user reinstalled the nvidia-glx package. The\nproblem seemed to be related to hibernation.\nAnother user suggested that the problem could be\ncaused by missing files in /usr/lib/xorg/modules.\nStandard KD: <userA>’s Serval Performance\nsystem has failed to boot several times since\ninstalling the latest Ubuntu ’edgy’ kernel security\nupdate. The problem required a hard shutdown and\nreboot. The first time, <userA> tried booting into\nrecovery mode, which seemed to work around the\nissue. The second time, even recovery mode did\nnot work; three times <userA> attempted to boot\nwithout success. <userA> noticed the message\n\"BUG: soft lockup on CPU#0\" and flipped the\nwireless card switch on the chassis to the ’on’\nposition. However, he is unsure if the switch was\nthe cause/cure or not. <userB> and <userC>\nsuggested that the problem may have been caused\nby a CPU bug where one of the cores was always at\n100% frequency even when not in use according to\nthe system monitor. The issue seems to be related\nto coming out of hibernation. If <userA> boots his\nsystem fresh, he does not observe the behavior, but\nif he hibernates and resumes, he notices it.\nSPF KD: <userA> installed the latest Ubuntu\n’edgy’ kernel security update and the Serval\nPerformance system failed to boot several times.\nThe problem required a hard shutdown and reboot.\nThe user tried booting into recovery mode, which\nseemed to work around the issue, but the second\ntime it did not work, and the user noticed the\nmessage \"BUG: soft lockup on CPU#0\". The user\nhas just booted the system successfully again after\nflipping the wireless card switch on the chassis to\nthe ’on’ position. However, the user does not know\nif the switch was the cause/cure or not. The issue\nseems to be related to coming out of hibernation,\nand if the user boots the system fresh, they do not\nobserve the behavior, but if they hibernate and\nresume, then they notice it. <userA> has reinstalled\nthe nvidia-glx package just in case.\nReference: The user’s serval performance system\nhas failed to boot several times. The problem\nrequired a hard shutdown and reboot. When the\nproblem occurred first time, the user tried booting\ninto recovery mode, which seemed to work around\nan issue. When the problem occurred second time,\neven recovery mode did not work. The user tried\nbooting the system after flipping the wireless card\nswitch on the chassis to the ’on’ position. The user\nnoticed the message BUG: soft lockup on CPU#0.\nThe user also noticed that one of the cores was\nalways at 100% frequency, even when not in use\naccording to the system monitor due to the CPU\nbug. The other user found the report which had\nmentioned problem due to the same bug and was\ntrying to recreate the problem on a serval. The cause\nof the problem might be mentioned in this webpage.\nhttp://ubuntuforums.org/showthread.php?t=318206\nThe user had the driver version information. The\nuser felt that the wireless card/driver was not related\nto the CPU frequency issue but it was related to the\nsystem coming out of hibernation. The user found\nthat the system was booted the problem was not\nobserved but when the system was resumed from\nhibernation, the problem was observed.\nTable 4: Summaries generated by ChatGPT (two-shot, USE-cosine), Standard KD, and SPF KD on an example\nUbuntu forum conversation (usernames are anonymized).\n12265",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8464757204055786
    },
    {
      "name": "Automatic summarization",
      "score": 0.7844771146774292
    },
    {
      "name": "Metric (unit)",
      "score": 0.6875461935997009
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.5634903311729431
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5610898733139038
    },
    {
      "name": "Conversation",
      "score": 0.5451220870018005
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5385860204696655
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5383623242378235
    },
    {
      "name": "Natural language processing",
      "score": 0.5263294577598572
    },
    {
      "name": "Language model",
      "score": 0.5177261233329773
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.49499791860580444
    },
    {
      "name": "Task (project management)",
      "score": 0.4921962320804596
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.463928759098053
    },
    {
      "name": "Natural language generation",
      "score": 0.4556442201137543
    },
    {
      "name": "Training set",
      "score": 0.4406076967716217
    },
    {
      "name": "Natural language",
      "score": 0.3997981548309326
    },
    {
      "name": "Machine learning",
      "score": 0.39443328976631165
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}