{
  "title": "Ensemble of Deep Masked Language Models for Effective Named Entity Recognition in Health and Life Science Corpora",
  "url": "https://openalex.org/W3217155045",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2107579764",
      "name": "Nona Naderi",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "SIB Swiss Institute of Bioinformatics"
      ]
    },
    {
      "id": "https://openalex.org/A2414130666",
      "name": "Julien Knafou",
      "affiliations": [
        "SIB Swiss Institute of Bioinformatics",
        "University of Geneva",
        "HES-SO University of Applied Sciences and Arts Western Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A2531790239",
      "name": "Jenny Copara",
      "affiliations": [
        "University of Geneva",
        "SIB Swiss Institute of Bioinformatics",
        "HES-SO University of Applied Sciences and Arts Western Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A1751439148",
      "name": "Patrick Ruch",
      "affiliations": [
        "SIB Swiss Institute of Bioinformatics",
        "HES-SO University of Applied Sciences and Arts Western Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A2234087575",
      "name": "Douglas Teodoro",
      "affiliations": [
        "University of Geneva",
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "SIB Swiss Institute of Bioinformatics"
      ]
    },
    {
      "id": "https://openalex.org/A2107579764",
      "name": "Nona Naderi",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "SIB Swiss Institute of Bioinformatics"
      ]
    },
    {
      "id": "https://openalex.org/A2414130666",
      "name": "Julien Knafou",
      "affiliations": [
        "University of Geneva",
        "SIB Swiss Institute of Bioinformatics",
        "HES-SO University of Applied Sciences and Arts Western Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A2531790239",
      "name": "Jenny Copara",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "University of Geneva",
        "SIB Swiss Institute of Bioinformatics"
      ]
    },
    {
      "id": "https://openalex.org/A1751439148",
      "name": "Patrick Ruch",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "SIB Swiss Institute of Bioinformatics"
      ]
    },
    {
      "id": "https://openalex.org/A2234087575",
      "name": "Douglas Teodoro",
      "affiliations": [
        "SIB Swiss Institute of Bioinformatics",
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "University of Geneva"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2418550336",
    "https://openalex.org/W6785288928",
    "https://openalex.org/W2080848531",
    "https://openalex.org/W2347081127",
    "https://openalex.org/W6761260114",
    "https://openalex.org/W3164477379",
    "https://openalex.org/W6767306326",
    "https://openalex.org/W6691217215",
    "https://openalex.org/W6783655397",
    "https://openalex.org/W6786980158",
    "https://openalex.org/W6781369179",
    "https://openalex.org/W2904867915",
    "https://openalex.org/W6763821806",
    "https://openalex.org/W6785628835",
    "https://openalex.org/W2104381725",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W3096329100",
    "https://openalex.org/W2952436323",
    "https://openalex.org/W6784810426",
    "https://openalex.org/W3048179169",
    "https://openalex.org/W6755782946",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W6776277508",
    "https://openalex.org/W2406836706",
    "https://openalex.org/W2734608416",
    "https://openalex.org/W3080429061",
    "https://openalex.org/W6784662919",
    "https://openalex.org/W3086727830",
    "https://openalex.org/W3146601652",
    "https://openalex.org/W2911871734",
    "https://openalex.org/W2979250794",
    "https://openalex.org/W6799466331",
    "https://openalex.org/W6763853743",
    "https://openalex.org/W2107435951",
    "https://openalex.org/W6761724051",
    "https://openalex.org/W6631851648",
    "https://openalex.org/W6717409122",
    "https://openalex.org/W3101381894",
    "https://openalex.org/W2149369282",
    "https://openalex.org/W6751005982",
    "https://openalex.org/W6697155078",
    "https://openalex.org/W6675914680",
    "https://openalex.org/W2169491861",
    "https://openalex.org/W6738256779",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6780045380",
    "https://openalex.org/W6665917979",
    "https://openalex.org/W6754351053",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W6774083458",
    "https://openalex.org/W6787165328",
    "https://openalex.org/W6767391440",
    "https://openalex.org/W6784489988",
    "https://openalex.org/W6760791844",
    "https://openalex.org/W6785052501",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W6780032311",
    "https://openalex.org/W2293205736",
    "https://openalex.org/W6782835986",
    "https://openalex.org/W2088212442",
    "https://openalex.org/W6748634344",
    "https://openalex.org/W2528773445",
    "https://openalex.org/W6748787957",
    "https://openalex.org/W2101553882",
    "https://openalex.org/W6779955285",
    "https://openalex.org/W6785263730",
    "https://openalex.org/W6785452417",
    "https://openalex.org/W2955483668",
    "https://openalex.org/W6785937767",
    "https://openalex.org/W6785735546",
    "https://openalex.org/W2152274790",
    "https://openalex.org/W6600347082",
    "https://openalex.org/W6769846984",
    "https://openalex.org/W6759052712",
    "https://openalex.org/W3102400851",
    "https://openalex.org/W2114039834",
    "https://openalex.org/W2168041406",
    "https://openalex.org/W6787140147",
    "https://openalex.org/W2574633938",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6784265554",
    "https://openalex.org/W6767388388",
    "https://openalex.org/W3036685585",
    "https://openalex.org/W6784997998",
    "https://openalex.org/W2947903144",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W6776960202",
    "https://openalex.org/W6785036732",
    "https://openalex.org/W2970374239",
    "https://openalex.org/W2339543475",
    "https://openalex.org/W6652710017",
    "https://openalex.org/W6633714834",
    "https://openalex.org/W6755617350",
    "https://openalex.org/W3114551148",
    "https://openalex.org/W2786103134",
    "https://openalex.org/W2935052563",
    "https://openalex.org/W3094701858",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287705547",
    "https://openalex.org/W3102147106",
    "https://openalex.org/W2964242047",
    "https://openalex.org/W3173156538",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W2989464093",
    "https://openalex.org/W2008639146",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W3101058639",
    "https://openalex.org/W3099296241",
    "https://openalex.org/W3116252395",
    "https://openalex.org/W2251220668",
    "https://openalex.org/W1530032857",
    "https://openalex.org/W3099647347",
    "https://openalex.org/W2890494294",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2962798796",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2970228048",
    "https://openalex.org/W8550301",
    "https://openalex.org/W2898108532",
    "https://openalex.org/W3035375600",
    "https://openalex.org/W3098886914",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2984582583",
    "https://openalex.org/W2950419928",
    "https://openalex.org/W2061848042",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3037747687",
    "https://openalex.org/W3096291042",
    "https://openalex.org/W2949759300",
    "https://openalex.org/W4289360595",
    "https://openalex.org/W3096554490",
    "https://openalex.org/W3096304772",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3097072143",
    "https://openalex.org/W2559145072",
    "https://openalex.org/W3097517073",
    "https://openalex.org/W2615487675",
    "https://openalex.org/W3102717038",
    "https://openalex.org/W562001009"
  ],
  "abstract": "The health and life science domains are well known for their wealth of named entities found in large free text corpora, such as scientific literature and electronic health records. To unlock the value of such corpora, named entity recognition (NER) methods are proposed. Inspired by the success of transformer-based pretrained models for NER, we assess how individual and ensemble of deep masked language models perform across corpora of different health and life science domains—biology, chemistry, and medicine—available in different languages—English and French. Individual deep masked language models, pretrained on external corpora, are fined-tuned on task-specific domain and language corpora and ensembled using classical majority voting strategies. Experiments show statistically significant improvement of the ensemble models over an individual BERT-based baseline model, with an overall best performance of 77% macro F1-score. We further perform a detailed analysis of the ensemble results and show how their effectiveness changes according to entity properties, such as length, corpus frequency, and annotation consistency. The results suggest that the ensembles of deep masked language models are an effective strategy for tackling NER across corpora from the health and life science domains.",
  "full_text": "Ensemble of Deep Masked Language\nModels for Effective Named Entity\nRecognition in Health and Life Science\nCorpora\nNona Naderi1,2*, Julien Knafou1,2,3, Jenny Copara4,1,2, Patrick Ruch1,2 and\nDouglas Teodoro4,1,2*\n1Information Science Department, University of Applied Sciences and Arts of Western Switzerland (HES-SO), Geneva,\nSwitzerland, 2Swiss Institute of Bioinformatics, Geneva, Switzerland,3Computer Science Department, University of Geneva,\nGeneva, Switzerland, 4Department of Radiology and Medical Informatics, University of Geneva, Geneva, Switzerland\nThe health and life science domains are well known for their wealth of named entities found\nin large free text corpora, such as scientiﬁc literature and electronic health records. To\nunlock the value of such corpora, named entity recognition (NER) methods are proposed.\nInspired by the success of transformer-based pretrained models for NER, we assess how\nindividual and ensemble of deep masked language models perform across corpora of\ndifferent health and life science domains— biology, chemistry, and medicine— available in\ndifferent languages— English and French. Individual deep masked language models,\npretrained on external corpora, areﬁned-tuned on task-speciﬁc domain and language\ncorpora and ensembled using classical majority voting strategies. Experiments show\nstatistically signiﬁcant improvement of the ensemble models over an individual BERT-\nbased baseline model, with an overall best performance of 77% macro F1-score. We\nfurther perform a detailed analysis of the ensemble results and show how their\neffectiveness changes according to entity properties, such as length, corpus\nfrequency, and annotation consistency. The results suggest that the ensembles of\ndeep masked language models are an effective strategy for tackling NER across\ncorpora from the health and life science domains.\nKeywords: named entity recognition, deep learning, patent text mining, transformers, clinical text mining, chemical\npatents, clinical NER, wet lab protocols\n1 INTRODUCTION\nIn the health and life science domains, most of the information is encoded in unstructured reports.\nFor example, it is estimated that around 90% of electronic health records (EHR) data are available as\nfree text. While text format facilitates capturing information, it makes the secondary use of the data\nchallenging. To support data structuring and to unlock the value of textual databases in secondary\nusage applications, named entity recognition (NER) methods have been proposed. NER is the task\nfor detecting entities in text and assigning concept names, or categories, to them. The health and life\nscience domains are notoriously known for their wealth of named entities and synonyms, such as\nmicroorganism taxonomies, drug brands, and gene names, to name a few. This richness of named\nentities together with the variety of formats, abbreviations, and (mis)spellings makes NER in health\nand life science corpora, like EHR, lab protocols, and scientiﬁc publications, a challenging task.\nEdited by:\nJiayuan (Estrid) He,\nRMIT University, Australia\nReviewed by:\nYonghui Wu,\nUniversity of Florida, United States\nAhmed Abdeen Hamed,\nNorwich University, United States\n*Correspondence:\nNona Naderi\nnona.naderi@hesge.ch\nDouglas Teodoro\ndouglas.teodoro@unige.ch\nSpecialty section:\nThis article was submitted to\nText-mining and Literature-based\nDiscovery,\na section of the journal\nFrontiers in Research Metrics and\nAnalytics\nReceived: 01 April 2021\nAccepted: 11 October 2021\nPublished: 19 November 2021\nCitation:\nNaderi N, Knafou J, Copara J, Ruch P\nand Teodoro D (2021) Ensemble of\nDeep Masked Language Models for\nEffective Named Entity Recognition in\nHealth and Life Science Corpora.\nFront. Res. Metr. Anal. 6:689803.\ndoi: 10.3389/frma.2021.689803\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 6898031\nORIGINAL RESEARCH\npublished: 19 November 2021\ndoi: 10.3389/frma.2021.689803\nBasic NER approaches use the construction of dictionaries of\nnamed entities and the speciﬁcation of tagging rules (Quimbaya\net al., 2016). They normally require domain knowledge and\nfeature engineering. While they are effective for simple and\nsmall corpora, their effectiveness is often limited when entities\nare complex and available in large numbers, as it is often the case\nin health and life sciences. Moreover, as the corpus evolves, it is\nhard to maintain the rules. More sophisticated methods are based\non classical machine learning models, such as support vector\nmachines, decision trees, hidden Markov models (Zhao, 2004),\nand conditional randomﬁelds (CRFs) (Li et al., 2008; Rocktäschel\net al., 2012; Leaman et al., 2015). In these methods, annotated\nexamples of text passages with entity classes are used to train the\nmodels. Textual features are combined with entity annotations to\nincrease the model’s performance. As the models are trained only\non the annotated corpus, which is usually small, they struggle to\ngeneralize to out-of-sample data. Thus, they are currently mainly\nused to provide a baseline for more effective model evaluation or\nin combination with more powerful models.\nMore recently, deep masked language models trained on large\ncorpora have achieved state-of-the-art in most NLP-related tasks,\nincluding NER. Bidirectional Encoder Representations from\nTransformers (BERTs) (Devlin et al., 2019) were the ﬁrst to\nexplore the transformer architecture as a general framework for\nNLP (Vaswani et al., 2017). Once the model is trained (or\npretrained in the BERTology parlance) on a large corpus, it\ncan be adapted and effectively ﬁne-tuned on specialized\ndownstream NLP tasks, such as question-answering, text\nclassiﬁcation, and NER by leveraging the feature representations\nlearned by the model during the pretraining phase in combination\nwith examples of the speciﬁc task. Since the advent of BERT, a\nmyriad of transformer-based masked language models have been\nproposed (Alsentzer et al., 2019; Liu et al., 2019; Yang et al., 2019).\nThese models vary mostly in the tokenization used, in how the\nmasking is performed, and in the trained data used during the\npretraining phase. Language models pretrained on a specialized\ncorpus, such as Medline, often tend to outperform models trained\non a generic corpus for biomedical-related tasks.\nIn this study, our goal is to perform an empirical evaluation of\nhow individual BERT-like models perform in the NER task across\ndifferent health and life science corpora available in different\nlanguages. More speciﬁcally, we aim to assess how individual\nmodels compare to ensemble strategies in such scenarios. To do\nso, we leverage deep language models pretrained on the external\ntext and ﬁne-tune them on speci ﬁc health and life science\ncorpora. Then, their predictions are combined to create\nensembles of named entity recognizers. We evaluate our\nmodels in chemistry, clinical, and wet lab corpora provided in\nthe context of the ChEMU (Cheminformatics Elsevier Melbourne\nUniversity) (He et al., 2020b), DEFT (Déﬁ Fouille de Textes)\n(Grabar et al., 2018), and WNUT (Workshop on Noisy User-\ngenerated Text) (Tabassum et al., 2020) challenges, respectively.\nOur results show that the ensembles of named entity recognizers\nbased on masked language models can outperform individual\nlanguage models and achieve effective NER performances in these\ndifferent domains and languages. We further perform an analysis\nof certain entity properties, including entity length, corpus\nfrequency, and annotation consistency, to have a better\nunderstanding of the model’s performance.\n2 RELATED WORK\nDeep learning approaches trained on large unstructured corpora\nhave shown considerable success in NLP problems, including\nNER (Lample et al., 2016; Beltagy et al., 2019; Devlin et al., 2019;\nJin et al., 2019; Liu et al., 2019; Andrioli de Souza et al., 2020).\nThese models learn contextual token and sentence\nrepresentations using often a self-supervised masked language\nalgorithm, in which they attempt to predict masked tokens within\nsentences. This step is usually referred to as pretraining. The\nlearned representations can then be reused in a supervised setting\nfor downstream tasks, such as question-answering, NER, and text\nclassiﬁcation. For domain-speci ﬁc tasks, models originally\npretrained on general corpora, such as BERT, can be further\npretrained or specialized on domain-speciﬁc corpora to improve\nthe originally learned representations according to the domain\nspeciﬁcity (Alsentzer et al., 2019; Lee et al., 2019; Gururangan\net al., 2020). There exist also models pretrained only on domain-\nspeciﬁc data (Beltagy et al., 2019; Gu et al., 2021), which reduces\nthe overall training time as domain-speciﬁc corpora tend to be\nsmaller in favor of lower generalization power. In both cases, in\npractice, those models are further trained orﬁne-tuned with task-\nspeciﬁc examples. In this case, the model is no-longer trained to\npredict masked tokens but rather the actual NLP task, such as\ntoken classiﬁcation in the case of NER.\nSeveral models are proposed for cross-domain NER (Pan et al.,\n2013; Lin and Lu, 2018; Jia et al., 2019; Liu et al., 2020, 2021).\nThese models are usually trained to leverage embeddings from\nthe different domains via a transfer learning process to improve\nentity tagging. Only a few of these studies focus on health and life\nscience NER. One study is that ofLee et al. (2018), in which the\nauthors utilize the idea of transfer learning to identify named\nentities in the i2b2 2014/2016 corpus using a model trained on the\nMIMIC dataset. In this study, we adopt a different approach for\nthe cross-domain problem. Instead of bene ﬁting from joint\nnamed-entity learning, we investigate a methodology based on\nthe ensemble of deep masked language models and show how it\ncan be effectively applied across complex NER domains.\nMoreover, we believe this is theﬁrst work proposing a generic\nand robust approach for NER across chemical, clinical, and wet\nlab corpora available in English and French.\n2.1 Chemical Named Entity Recognition\nTo further improve the performance of traditional approaches\nbased on hand-crafted features for the extraction of chemical\nentities (Rocktäschel et al., 2012; Leaman et al., 2015; Habibi et al.,\n2016; Zhang et al., 2016; Akhondi et al., 2016), a number of\nstudies leverage the power of word embeddings created using\nneural networks, such as word2vec (Mikolov et al., 2013), in\ncombination with traditional approaches like CRF (Leaman et al.,\n2015; Rocktäschel et al., 2012) in a single recurrent network\nmodel, usually based on the long short-term memory (LSTM)\narchitecture (Habibi et al., 2017; Corbett and Boyle, 2018; Zhai\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 6898032\nNaderi et al. Ensemble of Masked Language Models for NER\net al., 2019; Hemati and Mehler, 2019). These methods have\nshown a signiﬁcant improvement over the traditional methods on\nmultiple datasets, such as CHEMDNER patent (Krallinger et al.,\n2015a,b) and BioSemantics (Akhondi et al., 2014). For example,\non the chemical domain,Habibi et al. (2017)report about 5%\nimprovement in F1-score using an LSTM-CRF model with word\nembeddings over a CRF with BANNER features (Leaman and\nGonzalez, 2008), such as part-of-speech and character n-grams.\nZhai et al. (2019) extended the Bidirectional LSTM-CRF\n(BiLSTM-CRF) model with contextualized word\nrepresentations of Embeddings from Language Models (ELMo)\n(Peters et al., 2018) and reported an F1-score improvement of 3.7\npercentage point over BiLSTM-CRF and LSTM character models.\nRecently, the ChEMU evaluation lab ( He et al., 2020b )\norganized an information extraction task from patent\ndocuments for the identiﬁcation of chemical compounds and\ntheir speciﬁc roles in chemical reactions. The named entities in\nthis task consist of four categories, includingchemical compounds\ninvolving in a chemical reaction, conditions of the chemical\nreaction, yields for the ﬁnal chemical product, and example\nlabels. Teams participating in the task were evaluated based on\nboth strict and relaxed span matching conditions. Various\napproaches have been proposed in the competition, including\nrule-based models (Dönmez et al., 2020; Wang et al., 2020),\nBiLSTM-CNN-CRF (Dao and Nguyen, 2020; Mahendran et al.,\n2020), and transformer-based models ( Copara et al., 2020b;\nDönmez et al., 2020; Ruas et al., 2020).\n2.2 Clinical Named Entity Recognition\nVarious NER challenges and shared tasks, such as the i2b2 and\nn2c2 NLP challenges (Uzuner et al., 2010; Suominen et al., 2013;\nKelly et al., 2014; Bethard et al., 2015; Névéol et al., 2015; Henry\net al., 2020), fostered the development of NER methods (De\nBruijn et al., 2011; Jiang et al., 2011; Kim et al., 2015; Van\nMulligen et al., 2016; El Boukkouri et al., 2019) for the clinical\ndomain in different languages (Lopes et al., 2019; Sun and Yang,\n2019; Andrioli de Souza et al., 2020; Schneider et al., 2020). The\nDEFT challenge proposed an information extraction task for the\nFrench clinical corpus, with entities distributed across four\ncategories: anatomy, clinical practices , treatments, and time\n(Cardon et al., 2020 ). Several teams participated in the\nchallenge and the proposed approaches relied on rule-based\nmodels (Lemaitre et al., 2020; Royan et al., 2020; Hiot et al.,\n2021), CRF-based models (Minard et al., 2020), and transformer-\nbased models (Copara et al., 2020a; Nzali, 2020).\nSimilar to the chemical domain, word embeddings helped\nimprove the recognition of entities in clinical corpora.Roberts\n(2016) used the combination of a general domain and in-domain\nword2vec embeddings and showed improvement over only in-\ndomain embeddings. Using the i2b2 NLP dataset (Uzuner et al.,\n2011), El Boukkouri et al. (2019)showed that the concatenation of\noff-the-shelf ELMo contextualized representations (Peters et al., 2018)\nand word2vec embeddings trained on i2b2 task outperformed ELMo\nembeddings alone. Contextualized embeddings provided by ELMo\nwere also used byZhu et al. (2018). The authors used an ELMo\nversion trained on medical articles from Wikipedia and clinical notes\nand reported the state-of-the-art on MIMIC-III.Wei et al. (2020)\nused three approaches to identify entities on n2c2 dataset: a CRF, a\nBiLSTM, and a joint BiLSTM-CRF model. They investigated different\nensemble strategies to combine those models and found that the best\nresults were achieved using a majority voting.\nAs in other NLP tasks, recent studies to extract entities from\nclinical corpora focus mostly on the use of deep masked language\nmodels. Si et al. (2019)trained BERT on MIMIC-III and showed\nfurther improvement over the previous models on MIMIC-III.\nAlsentzer et al. 2019 trained BERT and BioBERT (Lee et al.,\n2019), on MIMIC notes, and showed that Bio + Clinical BERT\nperformed better than BERT and BioBERT trained on MedNLI\ndataset and i2b2 2010 datasets. Similarly,Schneider et al. (2020)\ndemonstrated that theﬁne-tuned BERT using Portuguese clinical\nnotes outperformed BERT trained on general corpora.\n2.3 Wet Lab Named Entity Recognition\nNLP approaches have only been applied to experimental protocols\nrelatively recently (Soldatova et al., 2014; Kulkarni et al., 2018).\nLuan et al. (2019)introduced a model based on a dynamic span\ngraph to jointly extract named entities and relations on wet lab\nprotocols and other corpora.Wadden et al. (2019)built uponLuan\net al. (2019)’s model by combining BERT and dynamic span graph.\nDai et al. (2019)computed the similarity of the pretrained data and\nthe data of the target application to investigate the effectiveness of\npretrained word vectors. Their results showed that the word\nFIGURE 1 |An example of a patent passage of the ChEMU dataset with entity annotations. The annotations are color-coded, representing the different entities in\nthe dataset.\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 6898033\nNaderi et al. Ensemble of Masked Language Models for NER\nvector’s effectiveness depends on the vocabulary overlap of the\nsource and target domains.\nIn contrast to the chemical and clinical domains,\nchallenges and shared tasks are not as common for wet lab\nprotocol corpora. Recently, WNUT-2020 ( Tabassum et al.,\n2020) introduced a NER task for analyzing Wet Lab protocols.\nThe task covers entity types from ﬁve categories of Action,\nConstituents, Quantiﬁers, Speciﬁers, and Modiﬁers. More than\na hundred manually annotated protocols were used to evaluate\nthe submissions of 13 teams. Most of the participants used\nNER models based on contextualized word representations\n(Knafou et al., 2020 ; Singh and Wadhawan, 2020 ;\nSohrab et al., 2020; Vaidhya and Kaushal, 2020; Zeng et al.,\n2020). A few participants used CRF-based models (Acharya,\n2020).\n3 MATERIAL AND METHODS\n3.1 Datasets\nIn this section, we present the datasets used to train and assess the\nindividual and ensembles of masked languages models for the\nextraction of named entities in chemical, clinical, and wet lab\ndomains. Theﬁrst dataset, provided in the context of the ChEMU\n2020 challenge, consists of a collection of English chemistry\npatents annotated with chemical reaction entities. The second\ndataset, provided in the context of the DEFT 2020 challenge,\nconsists of a collection of French EHR notes annotated with\nclinical entities. Finally, the third dataset, provided in the context\nof the WNUT 2020 challenge, consists of English laboratory\nprotocols annotated with wet lab entities.\n3.1.1 Benchmark for Chemical Entity\nRecognition—ChEMU 2020 Dataset\nThe ChEMU 2020 benchmark dataset contains snippets sampled\nfrom 170 English patents from the European Patent Ofﬁce and\nthe United States Patent and Trademark Ofﬁce (He et al., 2020b,a,\n2021; Verspoor et al., 2020). As shown inFigure 1, these snippets\nare annotated with several chemical reaction entities, including\nreaction_product, starting_material, and temperature. The\ntraining and test set of the ChEMU dataset contains a total of\n1,500 snippets annotated with 26,857 entities using the BRAT\nstandoff format (Stenetorp et al., 2012).\nTable 1 shows the entity distribution for the training and test\nsets. The majority of the annotations are provided for the\nother_compound, reaction_product,a n dstarting_materialentities,\ncovering 52% of the examples in the training and test datasets. In\ncontrast, example_label, yield_other,a n d yield_percent entities\nrepresent together only 18% of entities in the training and test sets.\nTABLE 1 |Entity distribution in the ofﬁcial training and test sets of ChEMU\nbenchmark dataset.\nEntity Train Test\nCount % Count %\nEL example_label 1,104 5.5 349 5.2\nOC other_compound 5,720 28.3 1,931 28.9\nRP reaction_product 2,558 12.7 855 12.8\nRC reagent_catalyst 1,570 7.8 504 7.6\nSo Solvent 1,390 6.9 428 6.4\nSM starting_material 2,167 10.7 711 10.7\nTe Temperature 1,861 9.2 612 9.2\nTi Time 1,311 6.5 452 6.8\nYO yield_other 1,322 6.5 440 6.6\nYP yield_percent 1,183 5.9 389 5.8\nTotal 20,186 100.0 6,671 100.0\nFIGURE 2 |An example of a clinical narrative of the DEFT dataset with entity annotations. The annotations are color-coded, representing the different entitiesi nt h e\ndataset. Notice that some entities are nested.\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 6898034\nNaderi et al. Ensemble of Masked Language Models for NER\n3.1.2 Benchmark for Clinical Entity\nRecognition—DEFT 2020 Dataset\nThe DEFT benchmark dataset is a subset of the CAS corpus\n(Grabar et al., 2018), containing 100 French clinical documents\nmanually annotated with the 8,098 entities in the following\ncategories: pathologie, sosy (symptoms and signs), anatomie,\ndose, examen, mode, moment, substance, traitement, and\nvaleur. An example of a clinical note annotation is shown in\nFigure 2. We can notice that nested entities appear in the\nannotations.\nTable 2 shows the distribution of annotations among the\nentities in the training and test datasets. The majority of\nannotations come from the sosy, anatomie, and examen\nentities, which compose together 54% of the training data. On\nthe other hand, mode, dose, and pathologie represent together\nonly 13% of the training dataset. In contrast to the ChEMU data,\nthe distribution of the training and test sets varies signiﬁcantly.\n3.1.3 Benchmark for Wet Lab Entity\nRecognition—WNUT 2020 Dataset\nThe WNUT benchmark dataset is composed of 727 unique\nEnglish wet lab protocols that describe experimental\nprocedures (Kulkarni et al., 2018). The dataset was manually\nannotated with the 102,957 entities in the following categories:\nAction, Amount, Concentration, Device, Generic-Measure,\nLocation, Measure-Type, Mention, Modiﬁer, Numerical,\nReagent, Seal, Size, Speed, Temperature, Time, and pH.A n\nexample of a lab protocol annotation is shown inFigure 3.\nIn Table 3, we see the distribution of the 18 entities by each\nsubset. As it is commonly found in the health and life science\ndomains, there is a signiﬁcant class imbalance, with only two\nclasses (Action and Reagent) representing more than 50% of\nannotations in the training set. Similar to the ChEMU dataset, the\nproportions of entities are fairly similar across the training and\ntest subsets.\n3.2 Proposed Methodology\nFigure 4shows a high-level view of our proposed ensemble model\nto recognize entities in health and life science corpora. In step 1\n(data), documents are preprocessed to create small text units\nusing a sentence-splitting algorithm. In step 2 (training), the\nresulting sentences with entity annotations are used toﬁne-tune\nthe individual deep neural masked language models. In the\ntraining process, sentences are tokenized according to the\nspeciﬁc language model tokenizer algorithm, and each token is\nassigned a label (entity class label or no-entity) based on the\ntraining annotations. Then, in step 3 (prediction), sentences are\nfed to the individual models previouslyﬁne-tuned, which split\nthem into tokens and assign an entity class. Finally, in step 4\n(ensemble), the predictions created for each token are aligned\nusing a majority voting algorithm.\nIn the following, we describe the methodology toﬁne-tune a\nsingle deep masked language model to recognize named entities\nin the chemical, clinical, and wet lab domains in English and\nFrench corpora. Then, we detail how these differentﬁne-tuned\nlanguage models were combined to provide an ensemble\nNER model.\n3.2.1 Single Deep Masked Language Model for Named\nEntity Recognition\nTo build the ensemble NER model, we ﬁne-tuned different\nindividual masked language models based on the transformers\narchitecture (Vaswani et al., 2017). In the case of NER, masked\nlanguage models are ﬁne-tuned using a specialized training\nset— in our case, the chemical, clinical, and wet lab annotated\nTABLE 2 |Entity distribution in the ofﬁcial training and test sets of the DEFT\nbenchmark dataset.\nEntity Train Test\nCount % Count %\nAn Anatomie 1,298 17.5 174 25.7\nDo Dose 342 4.6 5 0.7\nEx Examen 1,081 14.6 137 20.2\nMod Mode 238 3.2 11 1.6\nMom Moment 440 5.9 54 8.0\nPa Pathologie 351 4.7 184 27.2\nSo Sosy 1,647 22.2 33 4.9\nSu Substance 968 13.0 22 3.3\nTr Traitement 494 6.7 52 7.7\nVa Valeur 562 7.6 5 0.7\nTotal 7,421 100.0 677 100.0\nFIGURE 3 |An example of a wet lab protocol of the WNUT dataset with entity annotations. The annotations are color-coded, representing the different entities inthe\ndataset.\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 6898035\nNaderi et al. Ensemble of Masked Language Models for NER\ncorpora— to classify tokens according to the named entity classes.\nTable 4lists the individual deep neural language models assessed\nin our experiments for each domain task. We used deep language\nmodels based on or derived from the BERT architecture. BERT\nwas originally pretrained on a large corpus of English text\nextracted from BookCorpus (Zhu et al., 2015) and Wikipedia,\nwith the different number of attention heads for the base and\nlarge types (12 and 24 transformer layers and hidden\nrepresentations of 768 and 1,024 dimensions, respectively).\nTo ﬁne-tune a particular masked language model for the NER\ntask, we leverage the token representation created in its\npretraining phase. A fully connected layer is added on top of\nthe token representations and trained to classify whether a token\nbelongs to a class or not. As transformers usually use tokenizers\nthat work on word bits (or sub-tokens), during prediction, the\nentity label with the highest probability will be assigned to all sub-\ntokens of a word, and the sub-tokens will be then merged to build\nback the original word with the respectively assigned label.\nFinally, in a given sequence, if two adjacent words were given\nthe same entity prediction, we would consider the two words as a\nphrase related to that entity.\nFollowing this approach, the masked language model is thenﬁne-\ntuned on the domain-speciﬁcd a t a— chemical, clinical, and wet\nlab— using the training datasets previously discussed (ChEMU,\nDEFT, and WNUT). The ﬁne-tuning is performed with the\nmaximum sequence length of 265 tokens. The only preprocessing\ndone was sentence-splitting. For the chemical and wet lab NER\nexperiments, for which no nested entities were considered, we used a\nsoftmax function. Conversely, for the clinical NER, for which a token\ncould be assigned to more than one entity, we used a sigmoid\nfunction to provide a multi-class classiﬁer.\nTABLE 3 |Entity distribution in the ofﬁcial training and test sets of the WNUT\nbenchmark dataset.\nEntity Train Test\nCount % Count %\nAc Action 20,504 25.7 5,346 23.0\nAm Amount 5,712 7.2 1,223 5.3\nCo. Concentration 2,287 2.9 701 3.0\nDe Device 2,836 3.6 888 3.8\nGM Generic-Measure 759 1.0 173 0.8\nLo Location 6,643 8.3 1,657 7.1\nMT Measure-Type 1,453 1.8 720 3.1\nMen Mention 396 0.5 142 0.6\nMet Method 2,716 3.4 1,059 4.6\nMo Modi ﬁer 7,736 9.7 3,416 14.7\nNu Numerical 1,322 1.7 513 2.2\nRe Reagent 18,710 23.5 5,012 21.6\nSe Seal 366 0.5 119 0.5\nSi Size 498 0.6 232 1.0\nSp Speed 1,032 1.3 238 1.0\nTe Temperature 2,610 3.3 744 3.2\nTi Time 4,011 5.0 951 4.1\npH pH 166 0.2 66 0.3\nTotal 79,757 100.0 23,200 100.0\nFIGURE 4 |Schematic presentation of the ensemble model. Individual models areﬁne-tuned with speciﬁc task data. Then, they are used to classify tokens\nindependently. The predictions are then combined using majority voting.\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 6898036\nNaderi et al. Ensemble of Masked Language Models for NER\n3.2.2 Ensemble of Deep Masked Language Models for\nNamed Entity Recognition\nOur ensemble method is based on a voting strategy, where\neach model votes with its predictions and a simple majority of\nvotes is necessary to assign the predictions ( Copara et al.,\n2020b,a; Knafou et al., 2020 ). In other words, for a given\ndocument, our models infer their predictions independently\nfor each entity (as shown inFigure 4). Then, a set of passages\n(token or phrases) that received at least a vote for the named\nentities is taken into consideration for casting votes. This means\nthat, for a given document and a given entity, we end up with\nmultiple passages associated with a number of votes. Then, again\nfor a given entity, the ensemble method will assign labels to all\nthe passages that get the majority of votes. Note that each entity\nis predicted independently and that the voting strategy allows a\npassage to be labeled as positive for multiple entities at once.\nThus, our ensemble strategy is also capable of assigning labels to\nnested entities.\n3.3 Experimental Setup\n3.3.1 Training Details\nWe conduct experiments using the three datasets listed in\nTables 1 , 2,a n d 3 for the individual models listed in\nTable 4 .A ss h o w ni nTable 5 , we split the annotated\ncollection into train, dev,a n d test sets and trained our\nmodels using subsets ( train split) of the three datasets\nseparately. The individual models of chemical, clinical, and\nwet lab NERs were ﬁne-tuned on ChEMU, DEFT, WNUT\ntrain splits, respectively. The train, dev, and test sets were\nused to train the model weights, set the hyperparameters,\nand ﬁnd the best ensemble con ﬁguration, respectively.\nThe ensemble threshold for chemical and clinical NER was\nset to 3 and for wet lab NER to 4. More information about\nthe ﬁne-tuning of the models and the hyperparameter\nsettings can be found in Copara et al. (2020b,a) and Knafou\net al. (2020).\n3.3.2 Evaluation Details\nA blind test set (blind test split in Table 5), provided as part\nof the of ﬁcial evaluation for the respective challenges, was\nused to evaluate our models. Results are reported using the\ncompetition of ﬁcial metrics — precision, recall, and F1-\nscore— considering the exact span matching, that is, both the\nstarting and the end offsets of the text spans of the predicted\nand gold standard reference entities must match. They\nwere computed using the BRAT eval tool\n1, and the evaluation\ncode was provided by WNUT organizers against the blind\ntest set split. The ensemble models created for the different\ndomains are compared to the respective individual language\nmodels participating in the ensemble. The Student ’s t-test\nis used to assess the signi ﬁcance of the results. Results\nare considered statistically signi ﬁcant for p-values smaller\nthan 0.05.\nTABLE 4 |Pretrained models used for NER in the ChEMU, DEFT, and WNUT benchmark datasets.\nDataset Pretrained model Model size Corpus type\nChEMU BERT-base-cased Devlin et al. (2019) Base general\nBERT-base-uncased Devlin et al. (2019) Base\nCNN —\nDEFT BERT-base-multilingual-cased Base —\nCamemBERT Martin et al. (2020) Base general\nlarge\nCamemBERT-bio Copara et al. (2020a) Base bio + medical\nlarge\nWNUT RoBERTa Liu et al. (2019) Base general\nlarge\nXLNet Yang et al. (2019) large general\nBioBERT Lee et al. (2019) — bio\nBio + Clinical BERTAlsentzer et al. (2019) — bio + clinical\nPubMedBERT Gu et al. (2021) — bio + medical\nBioMed RoBERTa Gururangan et al. (2020) — bio + medical\nTABLE 5 |Distribution of samples in the train, dev, and test collections for the different NER tasks. Train: collection used to train model parameters. Dev: collection used to\ntune model hyperparameters. Test: collection used to deﬁne the ensemble models. Blind test: collection used to evaluate models.\nDataset Split # Patent snippets # EHR notes # Wet lab protocols\nTraining Train 800 80 370\nTraining Dev 100 10 123\nTraining Test 225 10 123\nTest Blind test 375 67 111\n1https://bitbucket.org/nicta_biomed/brateval/\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 6898037\nNaderi et al. Ensemble of Masked Language Models for NER\n4 RESULTS\n4.1 Individualvs. Ensemble Models\nTable 6 presents the NER results for the chemical, clinical, and\nweb lab corpora obtained using the ofﬁcial blind evaluation set\nfrom the ChEMU, DEFT, and WNUT challenges, respectively.\nIndividual model performance is compared with their respective\nensembles for each corpus using the ofﬁcial challenge metrics. As\nthe ofﬁcial test set of ChEMU is not yet publicly available, we also\nprovide the performance of the ensemble and all its respective\nindividual models on the ofﬁcial development set, taken as a blind\ntest set, so that the chemical NER ensemble performance can be\ncompared against all its individual models. In this case, the ofﬁcial\ntraining set was split into train and dev sets (as shown inTable 5).\nAs we can notice, the ensemble models consistently outperform\nthe individual models across the different domains and\nlanguages (English: ChEMU and WNUT; French: DEFT), with\nperformance varying between 75.47 and 92.30% (considering\nonly the ofﬁcial blind test evaluation). These results suggest\nthat the ensemble strategy is a robust methodology for NER in\nthe health and life science domains.\nConsidering each domain, the ensemble model on the\nchemical corpus outperforms its respective individual\nmodels, achieving 92.30% of the exact F1-score on the\nofﬁcial blind test set and yielding 1.3 percentage point\nimprovement over the BERT-base-cased baseline ( p /equals\n0.005). The ensemble model on the clinical corpus achieves\nan F1-score of 75.47%, outperforming the multilingual BERT\nbaseline by 6.5 percentage point ( p /equals0.025). The best\nperformance among the participating individual model in\nthe clinical NER ensemble is achieved by CamemBERT-\nlarge, around 1.1 percentage point below the ensemble. The\nclinical NER has the worse performance among the different\ndomains assessed. We believe it could be due to two factors.\nFirst, clinical corpora are notoriously complex, with many\nabbreviations and heterogeneous writing style, particularly\ncompared to patents and protocols, in which documents are\nexpected to follow a more formal structure and writing\nstandard. Second, the clinical NER might suffer from the\nknown problem of lack of resources for non-English\nlanguages (worsened in the case of clinical corpora). Similar\nto the other domains, the ensemble model on the wet lab\ncorpus outperforms its respective individual models (p /equals0.05),\nachieving an overall F1-score of77.99%. Among the individual\nmodels, the best performance is achieved by the ﬁne-tuned\nPubMedBERT followed by the ﬁne-tuned BioBERT.\nInterestingly, the best recall is achieved by individual models\nin all tasks assessed, though not consistently across individual\nmodels. We believe that by combining the individual models in\nthe ensemble, we restrict the predictions, taking only the ones that\nare more likely true, having thus a signiﬁcant positive impact on\nthe precision with an eventual negative impact on the recall for\nsome models. For the particular case of wet lab protocols, the best\nrecall is achieved by theﬁne-tuned PubMedBERT. Among all the\ncontextualized models, PubMedBERT is the only model trained\non biomedical text from scratch, and consequently, it has a more\nspeciﬁc vocabulary set (Gu et al., 2021), whereas the other models\nTABLE 6 |Comparison of the ensemble model with the individual models on three\ndatasets (ChEMU, DEFT, and WNUT). *Individual model was taken as\nreference for the individual model’s baseline. ** The ofﬁcial test set of ChEMU is not\npublicly available, so we report the results on the development set that was used\nas a test set (the training set was split into training and development sets). The\ncomparison of individual models and ensemble for the DEFT and WNUT\nchallenges are on the ofﬁcial test sets.\nModel P RF 1\nChEMU Test set\nBERT-base-cased* 90.83 91.14 90.98\nEnsemble (t /equals3) 93.78 90.87 92.30\nDev set**\nBERT-base-cased 91.37 91.44 91.40\nBERT-base-uncased 90.93 91.33 91.13\nCNN 91.39 74.06 81.82\nEnsemble (t /equals3) 94.36 91.39 92.85\nDEFT BERT-base-multilingual-cased* 68.62 69.27 68.94\nCamemBERT-base 71.93 69.72 70.81\nCamemBERT-large 74.12 74.70 74.41\nCamemBERT-bio-base 68.81 71.05 69.91\nCamemBERT-bio-large 73.74 73.67 73.70\nEnsemble (t /equals3) 78.75 72.46 75.47\nWNUT BioBERT 78.45 72.66 75.44\nBio + Clinical BERT* 77.09 71.44 74.16\nPubMedBERT 79.12 73.70 76.32\nRoBERTa base 76.66 70.69 73.55\nRoBERTa large 77.57 71.75 74.55\nBioMed RoBERTa 76.92 71.78 74.26\nXLNet 79.51 71.53 75.31\nEnsemble (t /equals4) 84.73 72.25 77.99\nBold in shows the best results.\nTABLE 7 |Test phase results of the ensemble model compared to other\nparticipants for datasets of ChEMU, DEFT, and WNUT challenges.\nTeam P R F1\nChEMU Wang et al. (2020) 95.71 95.70 95.70\nDao and Nguyen (2020) 94.62 94.05 94.33\nRuas et al. (2020) 93.27 94.57 93.92\nOurs 93.78 90.87 92.30\nLowe and Mayﬁeld (2020) 90.42 89.24 89.83\nBANNER Baseline He et al. (2020b) 90.71 87.23 88.93\nDEFT Wajsbürt et al. (2020) 79.50 73.30 76.30\nOurs 78.80 72.50 75.50\nMinard et al. (2020) 83.90 61.30 70.80\nRoyan et al. (2020) 69.50 57.30 62.80\nCao et al. (2020) 41.50 31.40 35.80\nWNUT Ours 84.73 72.25 77.99\nSingh and Wadhawan (2020) 81.36 74.12 77.57\nSohrab et al. (2020) 83.69 70.62 76.60\nKabir 78.79 72.20 75.35\nVaidhya and Kaushal (2020) 77.00 72.93 74.91\nBIO-BIO 78.49 71.06 74.59\nZeng et al. (2020) 76.21 71.76 73.92\nSudeshnaTCS 74.99 71.43 73.16\nB-NLP 77.95 63.93 70.25\nAcharya (2020) 73.68 63.98 68.48\nIBS 74.26 62.55 67.90\nDSC-IITISM 64.20 57.07 60.42\nmahab 50.19 52.96 51.54\nBold in shows the best results.\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 6898038\nNaderi et al. Ensemble of Masked Language Models for NER\nare ﬁrst trained on the general text and then further pretrained on\nbiomedical, medical, and clinical texts.\n4.2 Comparison With State-of-the-Art\nTable 7 shows the comparative results of our ensemble models\nagainst the teams participating in the ChEMU, DEFT, and\nWNUT competitions ranked by exact F1-score. The best\nresults in the ChEMU competition were achieved by Wang\net al. (2020) , whose models were based on BioBERT ﬁne-\ntuned on ChEMU data and BiLSTM-CRF. Their predictions\nwere further post-processed using hand-written rules, a step\nmissing in our pipeline. Dao and Nguyen (2020) used\nBiLSTM-CNN-CRF with Word2Vec and Elmo embeddings\ntrained on patent data. Ruas et al. (2020) also used BioBERT\nﬁne-tuned on ChEMU data to extract the entities. BioBERT is\ntrained on PubMed and PMC, and these datasets provide a better\npretraining dataset for the chemical domain than the General\nbook and Wikipedia datasets. The competition baseline model is\npresented as BANNER (He et al., 2020b). Our ensemble model\npresented asOurs outperforms the BANNER baseline by 3.37% in\nterms of exact F1-score.\nOn the clinical dataset, our ensemble model achieved the\nsecond place in terms of F1-score. The best performing model\nin this corpus relied on a BiLSTM-CRF model and features\nprovided by contextualized embeddings of the CamemBERT\nmodel (Wajsbürt et al., 2020). Finally, our ensemble model on\nthe wet lab dataset achieved the best performance among the\nparticipants in terms of F1-score. The next model was based on\nBiLSTM-CRF architecture and features provided by the\ncontextualized word embeddings of PubMedBERT (Singh and\nWadhawan, 2020). As we can notice, the addition of a BiLSTM-\nCRF layer also provides a consistently high-performing strategy\nin such domains.\n4.3 Entity Type Performance\nTable 8 shows the performance of our ensemble models for all\nclasses in the chemical, clinical, and wet lab NER tasks. In the\nchemical NER, the performance of the ensemble model ranges\nbetween 87% forstarting_material and 99.74% foryield_percent.\nError analysis on the training data shows that the\nstarting_material entity is often confused with the\nreagent_catalyst entity. From the chemistry point of view, both\nstarting material (reactants) and catalysts (reagents) entities are\npresent at the start of the reaction, with the difference that the\nlatter is not altered by the reaction. These terms are often used\ninterchangeably though, which could be the reason for the\nconfusion.\nIn the clinical NER, the highest F1-score in the blind test set is\nachieved for the valeur entity (85.61%). This entity represents\n7.6% of the annotations in the training collection. One could\nassume that entities with annotation examples above this\nthreshold would perform well; however, when looking at the\nresults for thesubstance (13.0% of the annotations) category, we\nnotice an important drop in performance (63.79%). Thus, it\nseems that the number of training data examples alone is not\nsufﬁcient to learn an entity automatically. The lowest\nperformance for the ensemble method is found for the dose\nentity. This can be due to the variety of values in the\nannotated data, combining numbers and words (e.g.,de 0,5 à\n0,75 L), measure units (e.g.,1 mg/kg/j), or simply words that could\nbe easily associated with a nonentity word (e.g.,24 paquets/année\nor 02).\nThe performance of the ensemble model for the classes of wet\nlab NER ranges between 30.72 and 95.31%. Surprisingly, the\nentity with the highest F1-score,pH (95.31%), has only 0.2% of\nthe annotations in the training sample. Again, the number of\nexamples is not associated with the performance of the test set.\nTABLE 8 |Performance of the ensemble models in terms of exact precision, recall, and F1-score for the entities of the ChEMU, DEFT, and WNUT ofﬁcial test sets.\nChEMU DEFT WNUT\nEntity P R F1 Entity P R F1 Entity P RF 1\nEL 97.11 96.28 96.69 An 79.60 81.80 80.69 Ac 91.17 84.43 87.67\nOC 91.97 86.59 89.20 Do 60.00 46.15 52.17 Am 79.52 93.13 85.79\nRP 89.42 85.96 87.66 Ex 76.39 70.50 73.33 Co. 88.40 90.78 89.57\nRC 92.68 87.90 90.22 Mod 81.36 53.93 64.86 De 82.20 57.30 67.53\nSo 96.20 94.63 95.41 Mom 85.71 72.73 78.69 GM 57.02 39.20 46.46\nSM 88.86 85.23 87.01 Pa 57.50 55.42 56.44 Lo 70.89 68.98 69.92\nTe 97.69 96.90 97.29 So 71.98 63.25 67.33 MT 80.70 50.34 62.01\nTi 98.46 99.12 98.79 Su 77.27 54.31 63.79 Men 70.51 75.86 73.09\nYO 97.76 99.09 98.42 Tr 67.47 55.26 60.76 Met 65.71 38.07 48.21\nYP 99.74 99.74 99.74 Va 87.26 84.03 85.61 Mo 84.28 42.88 56.84\n— ——— — ——— Nu 64.78 39.62 49.16\n— ——— — ——— Re 85.71 85.69 85.70\n— ——— — ——— Se 81.58 78.15 79.83\n— ——— — ——— Si 69.12 19.75 30.72\n— ——— — ——— Sp 86.19 85.83 86.01\n— ——— — ——— Te 98.12 89.47 93.60\n— ——— — ——— Ti 94.62 89.89 92.19\n— ——— — ——— pH 98.39 92.42 95.31\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 6898039\nNaderi et al. Ensemble of Masked Language Models for NER\nIndeed, the best performing entities for the wet lab\nNER— Temperature, Time, and pH— are responsible together\nfor only 8.5% of the annotation examples. The performance of\nthe ensemble model is low for theGeneric-Measure, which is\nsimilar todose in clinical NER task, getting various forms, such as\nmeasure units ( volume), measurements ( 30 kDa, 2.5 bars,\n∼250–500 bp), and ratios (1:2, 1/500 to 1/1,000), which could\nalso justify its low score.\n4.4 Entity Property Analyses\nTo better understand our results across the different corpora,\nwe performed a deeper analysis of the reference individual\nbaseline and the ensemble model using different entity\nproperties: frequency, length, and label consistency.\nFigure 5 shows the comparison of the BERT baselines and\nthe ensemble models based on the entity frequency and\nlength. For both DEFT and WNUT collections, on average,\nthe highest performance gains over the individual model\nhappen for the less frequent entities, whereas the opposite\nhappens for the ChEMU collection. Concerning the entity\nlength property, we notice that the average length is shorter in\nthe WNUT dataset. The ChEMU dataset, as expected,\nincludes the longest average entity lengths, necessary to\nrepresent molecules. For all datasets, as the entity length\nincreases, the performance of the ensemble models\nimproves over the individual models.\nFinally, Figure 6 shows the frequency of passages that were\nassigned more than one label for the evaluated datasets. Here,\nwe consider “passage ” as a token or a sequence of tokens that\nFIGURE 5 | (A): Performance of the BERT modelvs. the ensemble model based on the entity frequency on the training data.(B): Performance of the BERT model\nvs. the ensemble model based on the entity length on the training data. In both, the individual BERT for the three datasets is BERT-base-cased for ChEMU, BERT-base-\nmultilingual-cased for DEFT, and Bio + Clinical BERT for WNUT.\nFIGURE 6 |The number of labels assigned to each passage for the\ntraining set of the three datasets (ChEMU, DEFT, and WNUT).\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 68980310\nNaderi et al. Ensemble of Masked Language Models for NER\nwere assigned a label, for example,“triethylamine ” annotated\nas reagent_catalyst and other_compound and “sodium\nhydrogen carbonate ” annotated as reagent_catalyst and\nother_compound in ChEMU dataset. As more than one class\nis assigned to the same passage, we expect that they would be\nmore ambiguous and therefore harder for the models to\nrecognize. After the analyses of the training set, we notice\nthat the ChEMU and WNUT corpora include passages that\nwere assigned two or more labels for almost 10% of the\nexamples. This happens for around only 1% of the\nannotations in the DEFT corpus. Hence, we would expect a\nbetter performance for the latter compared with the former. As\nit is not the case, it seems that the deep masked language\nmodels might actually be able to recognize those passages\ncorrectly using contextual information.\n5 DISCUSSION\nWe compared the effectiveness of individual masked language\nmodels and ensemble models based on the majority vote strategy\nfor the NER task in multiple health and life science domains and\nlanguages. The ensemble model showed a robust performance\nacross the assessed domains and languages, achieving an overall\nmacro F1-score of 76.94% and improving the individual models\nby 6.0 percentage point (considering the BERT-based-cased,\nBERT-base-multilingual-cased, and Bio + Clinical BERT as\nreference for the individual models in the ChEMU, DEFT, and\nWNUT datasets, respectively) (p /equals0.005). Out of the 38 entity\nclasses assessed, 50% had an F1-score equal or higher than 85%\nfor the ensemble model (compare to 34% for the individual BERT\nmodel).\nThe performance of the models on the French clinical\ncorpus is lower than on the chemical and wet lab corpora.\nWe believe this is likely due to the known issue of reduced\nFrench language resources compared to English, both in terms\nof the corpora to pretrain the masked language models and\nalso to ﬁne-tune for the clinical NER. As seen in entity\ndistribution tables (Tables 1, 2,a n d3), the training data for\nchemical and wet lab NER are larger, which results in better\nperformance for the individual language models and\nconsequently for the ensemble models. Additionally, the\nclinical dataset includes nested entities, which are known to\nbe recognized more effectively using graph-based models (Yu\net al., 2020). Nevertheless, it is for the clinical dataset that we\nnotice the highest relative gain in performance for the\nensemble model (9.5% of F1-score).\nOur analysis shows that specialized language models\nachieve the best performance across the health and life\nscience domains. Moreover, in terms of model architecture,\nBiLSTM-CRF-based models with contextualized language\nmodels for feature extraction achieve competitive results.\nThese results are aligned w ith the current knowledge\navailable in the literature ( Fu et al., 2020b ; Hahn and\nOleynik, 2020 ). That said, existing methods for chemical,\nclinical, and wet lab NER focus mostly on a single domain\nand language. Here, we introduced a novel and generic NER\nmethodology for diverse and complex corpora in multiple\ndomains and languages. We believe that such an approach\ncan be expanded to other domains and languages with similar\neffectiveness.\nThe detailed analysis of entity types shows that the models\nhave often difﬁculties recognizing infrequent entities, such as\ndose (clinical corpus) and Generic-Measure (wet lab corpus),\nwhich is in-line with previous work ( Fu et al., 2020a ).\nHowever, we notice that for some entities, particularly in\nthe wet lab corpora, the highest scores were provided by\ninfrequent entities. Indeed, as shown by Fu et al. (2020a),a\nsingle holistic measure of F1-score cannot tell the details of the\nperformance of different models. Diverse entity attributes,\nsuch as length , frequency , sentence length ,a n d out-of-\nvocabulary (OOV) density , are important for further model\nanalyses. Thus, we further examined three meta-features:\nentity frequency and length, and label consistency. There\nwas a consistent performance gain brought by the ensemble\nas the entity length increased. As deep masked language\nmodels work at the sub-word level, the longer the entity\nsize, the more correct classi ﬁcations are needed by the\nindividual model to provide an exact match NER. By\ncombining the different models, the ensemble seems to\nbe able to leverage the correct classi ﬁcations among the\nmodels. Moreover, despite a relatively frequent multi-\nl a b e l i n gf o rp a s s a g e s( 2o rm o r e )i nt h ec h e m i c a la n d\nwet lab corpora compared to the clinical corpus, their\nperformance was signi ﬁcantly higher than the latter. This\nresult suggests that, as expected, the deep masked language\nmodels were able to distinguish the homographs by their\ncontext.\nThe main limitation of our results comes from the\nheterogeneity of both corpora and models used. We used\ndifferent baseline models across domains, partly due to the\nnature of the datasets (different languages). Additionally, the\ndistribution of entities differs signiﬁcantly across the datasets. All\nof this hinders the comparison of the results. Nevertheless, we\nbelieve the overall methodology gives a strong indication of the\nrobustness of the ensemble of deep language models for NER in\nmulti-domain and -lingual corpora.\n6 CONCLUSION\nIn this work, we propose a generic and robust approach for\nnamed entity recognition in the health and life science domain\nbased on deep masked language models combined in a majority\nvoting strategy. We compared the performance of individual\nBERT models and their siblings against the proposed ensemble\nmodels for three types of corpora— chemical, clinical, and wet\nlab— available in English and French languages. We show a\nsigniﬁcant performance improvement of 6.0 percentage point\n(p /equals0.005) using the ensemble models compared to a strong\nbaseline based on individual BERT models, with the ensemble\nmodels having 50% of entities assessed with an F1-score of 85% or\nmore. We further performed a detailed analysis of the\nperformance of the models based on a set of entity properties.\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 68980311\nNaderi et al. Ensemble of Masked Language Models for NER\nWe found that ensemble models can be more beneﬁcial for longer\nentities.\nDATA AVAILABILITY STATEMENT\nPublicly available datasets were analyzed in this study. The data\nused for chemical NER can be found at: http://chemu2020.eng.\nunimelb.edu.au/, https://deft.limsi.fr/2020/, http://noisy-text.\ngithub.io/2020/wlp-task.html.\nAUTHOR CONTRIBUTIONS\nNN drafted the manuscript, implemented the models, and analyzed\nthe results. JK designed and implemented the models, and analyzed\nthe results. JC implemented the models and analyzed the results. PR\nanalyzed the results. DT drafted the manuscript and analyzed the\nresults. All authors reviewed and contributed to the writing.\nFUNDING\nFunding for this work is provided by the CINECA project (No.\nH2020 No 825775) and Innosuisse project funding number\n46966.1 IP-ICT.\nACKNOWLEDGMENTS\nThe authors would like to thank the reviewers for their valuable\ncomments and suggestions.\nREFERENCES\nAcharya, K. (2020). “WNUT 2020 Shared Task-1: Conditional Random\nField(CRF) Based Named Entity Recognition(NER) for Wet Lab Protocols,”\nin Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT\n2020) (Online: Association for Computational Linguistics), 286 –289.\ndoi:10.18653/v1/2020.wnut-1.37\nAkhondi, S. A., Klenner, A. G., Tyrchan, C., Manchala, A. K., Boppana, K., Lowe,\nD., et al. (2014). Annotated Chemical Patent Corpus: A Gold Standard for Text\nMining. PLoS ONE 9, e107477. doi:10.1371/journal.pone.0107477\nAkhondi, S. A., Pons, E., Afzal, Z., van Haagen, H., Becker, B. F., Hettne, K. M., et al.\n(2016). Chemical Entity Recognition in Patents by Combining Dictionary-\nBased and Statistical Approaches. Database 2016, baw061. doi:10.1093/\ndatabase/baw061\nAlsentzer, E., Murphy, J., Boag, W., Weng, W., Jindi, D., Naumann, T., et al. (2019).\n“Publicly Available Clinical BERT Embeddings,” in Proceedings of the 2nd\nClinical Natural Language Processing Workshop Minneapolis, Minnesota,\nUnited States: Association for Computational Linguistics, 72–78.\nAndrioli de Souza, J. V., Terumi Rubel Sch n e i d e r ,E . ,O l i v e i r aC e z a r ,J . ,S i l v ae\nO l i v e i r a ,L .E . ,B o n e s c k iG u m i e l ,Y . ,C a b r e r aP a r a i s o ,E . ,e ta l .( 2 0 2 0 ) .“A\nMultilabel Approach to Portuguese Clinical Named Entity Recognition,” in\nProceedings of the XVII Congresso Brasileiro de Informática em Saúde\n(CBIS 2020). published in Journal of health informatics, 7-11 December\n2020.\nBeltagy, I., Lo, K., and Cohan, A. (2019).“SciBERT: A Pretrained Language Model\nfor Scientiﬁc Text,” in Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), 3606–3611.\ndoi:10.18653/v1/d19-1371\nBethard, S., Derczynski, L., Savova, G., Pustejovsky, J., and Verhagen, M. (2015).\n“Semeval-2015 Task 6: Clinical Tempeval, ” in proceedings of the 9th\nInternational Workshop on Semantic Evaluation (SemEval 2015), 806–814.\ndoi:10.18653/v1/s15-2136\nCao, D., Benamar, A., Boumghar, M., Bothua, M., Ouali, L. O., and Suignard, P.\n(2020). “Participation d’EDF R&D à DEFT 2020,” in 6e conférence conjointe\nJournées d’Études sur la Parole (JEP, 33e édition), Traitement Automatique des\nLangues Naturelles (TALN, 27e édition), Rencontre des Étudiants Chercheurs\nen Informatique pour le Traitement Automatique des Langues (RÉCITAL, 22e\nédition). Atelier DÉﬁ Fouille de Textes (ATALA; AFCP), 26–35.\nCardon, R., Grabar, N., Grouin, C., and Hamon, T. (2020).“Présentation de la\ncampagne d’évaluation DEFT 2020: similarité textuelle en domaine ouvert et\nextraction d’information précise dans des cas cliniques (Presentation of the\nDEFT 2020 Challenge: open domain textual similarity and precise information\nextraction from clinical cases),” in Actes de la 6e conférence conjointe Journées\nd’Études sur la Parole (JEP, 33e édition), Traitement Automatique des Langues\nNaturelles (TALN, 27e édition), Rencontre des Étudiants Chercheurs en\nInformatique pour le Traitement Automatique des Langues (RÉCITAL, 22e\nédition). Atelier DÉﬁ Fouille de Textes, 1–13.\nCopara, J., Knafou, J., Naderi, N., Moro, C., Ruch, P., and Teodoro, D. (2020a).\n“Contextualized French Language Models for Biomedical Named Entity\nRecognition,\n” in 6e conférence conjointe Journées d ’Études sur la Parole\n(JEP, 33e édition), Traitement Automatique des Langues Naturelles (TALN,\n27e édition), Rencontre des Étudiants Chercheurs en Informatique pour le\nTraitement Automatique des Langues (RÉCITAL, 22e édition). Atelier DÉﬁ\nFouille de Textes. Editors R. Cardon, N. Grabar, C. Grouin, and T. Hamon\n(Nancy, France: ATALA), 36–48.\nCopara, J., Naderi, N., Knafou, J., Ruch, P., and Teodoro, D. (2020b).“Named\nEntity Recognition in Chemical Pat ents Using Ensemble of Contextual\nLanguage Models, ” in Working notes of the CLEF 2020, 22-25 September\n2020.\nCorbett, P., and Boyle, J. (2018). Chemlistem: Chemical Named Entity Recognition\nUsing Recurrent Neural Networks.J. Cheminf. 10, 1–9. doi:10.1186/s13321-\n018-0313-8\nDai, X., Karimi, S., Hachey, B., and Paris, C. (2019).“Using Similarity Measures to\nSelect Pretraining Data for NER,” in Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies. Long and Short Papers. (Minneapolis,\nMinnesota: Association for Computational Linguistics), 1460 –1470.\ndoi:10.18653/v1/N19-1149\nDao, M. H., and Nguyen, D. Q. (2020).“VinAI at ChEMU 2020: An Accurate\nSystem for Named Entity Recognition in Chemical Reactions from Patents,” in\nCLEF (Working Notes).\nDe Bruijn, B., Cherry, C., Kiritchenko, S., Martin, J., and Zhu, X. (2011). Machine-\nlearned Solutions for Three Stages of Clinical Information Extraction: the State\nof the Art at I2b2 2010.J. Am. Med. Inform. Assoc.18, 557–562. doi:10.1136/\namiajnl-2011-000150\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).“BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding, ” in\nProceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies.\nLong and Short Papers, 4171–4186.\nDönmez, H., Köksal, A., Özkirimli, E., and Özgür, A. (2020).“BOUN-REX at\nCLEF-2020 ChEMU Task 2: Evaluating Pretrained Transformers for Event\nExtraction,” in CLEF (Working Notes).\nEl Boukkouri, H., Ferret, O., Lavergne, T., and Zweigenbaum, P. (2019).\n“Embedding Strategies for Specialized Domains: Application to Clinical\nEntity Recognition,” in Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics: Student Research Workshop,\n295–301. doi:10.18653/v1/p19-2041\nFu, J., Liu, P., and Neubig, G. (2020a).“Interpretable Multi-Dataset Evaluation for\nNamed Entity Recognition, ” in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), 6058–6069.\ndoi:10.18653/v1/2020.emnlp-main.489\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 68980312\nNaderi et al. Ensemble of Masked Language Models for NER\nFu, S., Chen, D., He, H., Liu, S., Moon, S., Peterson, K. J., et al. (2020b). Clinical\nConcept Extraction: a Methodology Review.J. Biomed. Inform.109, 103526.\ndoi:10.1016/j.jbi.2020.103526\nGrabar, N., Claveau, V., and Dalloux, C. (2018).“CAS: French Corpus with Clinical\nCases,” in Proceedings of the Ninth International Workshop on Health Text\nMining and Information Analysis (Brussels, Belgium: Association for\nComputational Linguistics), 122–128. doi:10.18653/v1/W18-5614\nGu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., et al. (2021). Domain-\nSpeciﬁc Language Model Pretraining for Biomedical Natural Language\nProcessing ACM Trans. Comput. Healthcare.3 ,1 –23.\nGururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D.,\net al. (2020).“Don’t Stop Pretraining: Adapt Language Models to Domains and\nTasks,” in Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, 8342–8360. doi:10.18653/v1/2020.acl-main.740\nHabibi, M., Wiegandt, D. L., Schmedding, F., and Leser, U. (2016). Recognizing\nChemicals in Patents: a Comparative Analysis.J. Cheminf.8, 1–15. doi:10.9734/\nbjmcs/2016/25967\nHabibi, M., Weber, L., Neves, M., Wiegandt, D. L., and Leser, U. (2017). Deep\nLearning with Word Embeddings Improves Biomedical Named Entity\nRecognition. Bioinformatics 33, i37–i48. doi:10.1093/bioinformatics/btx228\nHahn, U., and Oleynik, M. (2020). Medical Information Extraction in the Age of\nDeep Learning.Yearb. Med. Inform.29, 208–220. doi:10.1055/s-0040-1702001\nHe, J., Nguyen, D. Q., Akhondi, S. A., Druckenbrodt, C., Thorne, C., Hoessel, R.,\net al. (2020a). “An Extended Overview of the CLEF 2020 ChEMU Lab:\nInformation Extraction of Chemical Reactions from Patents, ” in\nProceedings of the Eleventh International Conference of the CLEF\nAssociation (CLEF 2020).\nHe, J., Nguyen, D. Q., Akhondi, S. A., Druckenbrodt, C., Thorne, C., Hoessel, R., et al.\n(2020b). “Overview of ChEMU 2020: Named Entity Recognition and Event\nExtraction of Chemical Reactions from Patents,” in Experimental IR Meets\nMultilinguality, Multimodality, and Interaction: Proceedings of the Eleventh\nInternational Conference of the CLEF Association (CLEF 2020). Lecture Notes\nin Computer Science . Editors A. Arampatzis, E. Kanoulas, T. Tsikrika,\nS. Vrochidis, H. Joho, C. Lioma, et al. 12260. doi:10.1007/978-3-030-58219-7_18\nHe, J., Nguyen, D. Q., Akhondi, S. A., Druckenbrodt, C., Thorne, C., Hoessel, R.,\net al. (2021). ChEMU 2020: Natural Language Processing Methods Are\nEffective for Information Extraction from Chemical Patents. Front. Res.\nMetrics Anal. 6, 12. doi:10.3389/frma.2021.654438\nHemati, W., and Mehler, A. (2019). LSTMVoter: Chemical Named Entity\nRecognition Using a Conglomerate of Sequence Labeling Tools.J. Cheminf.\n11, 1–7. doi:10.1186/s13321-018-0327-2\nHenry, S., Buchan, K., Filannino, M., Stubbs, A., and Uzuner, O. (2020). 2018 N2c2\nShared Task on Adverse Drug Events and Medication Extraction in Electronic\nHealth Records.J. Am. Med. Inform. Assoc.27, 3–12. doi:10.1093/jamia/ocz166\nHiot, N., Minard, A.-L., and Badin, F. (2021).“DOING@ DEFT: utilisation de\nlexiques pour une classi ﬁcation ef\nﬁcace de cas cliniques, ” in Traitement\nAutomatique des Langues Naturelles(ATALA), 41–53.\nJia, C., Liang, X., and Zhang, Y. (2019).“Cross-domain NER Using Cross-Domain\nLanguage Modeling,” in Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 2464–2474. doi:10.18653/v1/p19-\n1236\nJiang, M., Chen, Y., Liu, M., Rosenbloom, S. T., Mani, S., Denny, J. C., et al. (2011).\nA Study of Machine-Learning-Based Approaches to Extract Clinical Entities\nand Their Assertions from Discharge Summaries.J. Am. Med. Inform. Assoc.18,\n601–606. doi:10.1136/amiajnl-2011-000163\nJin, Q., Dhingra, B., Cohen, W., and Lu, X. (2019). “Probing Biomedical\nEmbeddings from Language Models,” in Proceedings of the 3rd Workshop\non Evaluating Vector Space Representations for NLP, 82–89. doi:10.18653/v1/\nw19-2011\nKelly, L., Goeuriot, L., Suominen, H., Schreck, T., Leroy, G., Mowery, D. L., et al.\n(2014). “Overview of the ShARe/CLEF eHealth Evaluation Lab 2014, ” in\nInternational Conference of the Cross-Language Evaluation Forum for\nEuropean Languages (Springer), 172–191. doi:10.1007/978-3-319-11382-1_17\nKim, Y., Riloff, E., and Hurdle, J. F. (2015).“A Study of Concept Extraction across\nDifferent Types of Clinical Notes,” in AMIA Annual Symposium Proceedings\n(American Medical Informatics Association), 737.\nKnafou, J., Naderi, N., Copara, J., Teodoro, D., and Ruch, P. (2020).“BiTeM at\nWNUT 2020 Shared Task-1: Named Entity Recognition over Wet Lab\nProtocols Using an Ensemble of Contextual Language Models, ” in\nProceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT\n2020) (Online: Association for Computational Linguistics). doi:10.18653/v1/\n2020.wnut-1.40\nKrallinger, M., Rabal, O., Leitner, F., Vazquez, M., Salgado, D., Lu, Z., et al. (2015a).\nThe CHEMDNER Corpus of Chemicals and Drugs and its Annotation\nPrinciples. J. Cheminf. 7, 1–17. doi:10.1186/1758-2946-7-S1-S2\nKrallinger, M., Rabal, O., Lourenço, A., Perez, M. P., Rodriguez, G. P., Vazquez, M.,\net al. (2015b).“Overview of the CHEMDNER Patents Task,” in Proceedings of\nthe ﬁfth BioCreative challenge evaluation workshop, 63–75.\nKulkarni, C., Xu, W., Ritter, A., and Machiraju, R. (2018).“An Annotated Corpus\nfor Machine reading of Instructions in Wet Lab Protocols,” in Proceedings of\nthe 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies. (Short Papers)\n(New Orleans, Louisiana: Association for Computational Linguistics), 97–106.\ndoi:10.18653/v1/n18-2016\nLample, G., Ballesteros, M., Subramanian, S., Kawakami, K., and Dyer, C. (2016).\n“Neural Architectures for Named Entity Recognition,” in Proceedings of the\n2016 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, 260 –270.\ndoi:10.18653/v1/n16-1030\nLeaman, R., and Gonzalez, G. (2008). “BANNER: an Executable Survey of\nAdvances in Biomedical Named Entity Recognition,”\nin Biocomputing 2008\n(World Scientiﬁc), 652–663.\nLeaman, R., Wei, C.-H., and Lu, Z. (2015). tmChem: a High Performance\nApproach for Chemical Named Entity Recognition and Normalization.\nJ. Cheminf. 7, 1–10. doi:10.1186/1758-2946-7-S1-S3\nLee, J. Y., Dernoncourt, F., and Szolovits, P. (2018).“Transfer Learning for Named-\nEntity Recognition with Neural Networks,” in Proceedings of the Eleventh\nInternational Conference on Language Resources and Evaluation (LREC 2018).\nLee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., et al. (2019). BioBERT: a Pre-\ntrained Biomedical Language Representation Model for Biomedical Text\nMining. Bioinformatics 36, 1234–1240. doi:10.1093/bioinformatics/btz682\nLemaitre, T., Gosset, C., Lafourcade, M., Patel, N., and Mayoral, G. (2020).“DEFT\n2020-Extraction d’information ﬁne dans les données cliniques: terminologies\nspécialisées et graphes de connaissance,” in Atelier DÉﬁ Fouille de Textes\n(ATALA; AFCP), 55–65.\nLi, D., Savova, G., and Kipper, K. (2008).“Conditional Randomﬁelds and Support\nVector Machines for Disorder Named Entity Recognition in Clinical Texts,” in\nProceedings of the workshop on current trends in biomedical natural language\nprocessing, 94–95. doi:10.3115/1572306.1572326\nLin, B. Y., and Lu, W. (2018).“Neural Adaptation Layers for Cross-Domain Named\nEntity Recognition,” in Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, 2012–2022. doi:10.18653/v1/d18-1226\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., et al. (2019). Roberta: A\nRobustly Optimized Bert Pretraining Approach. arXiv preprint arXiv:\n1907.11692.\nLiu, Z., Winata, G. I., and Fung, P. (2020).“Zero-resource Cross-Domain Named\nEntity Recognition,” in Proceedings of the 5th Workshop on Representation\nLearning for NLP, 1–6. doi:10.18653/v1/2020.repl4nlp-1.1\nLiu, Z., Xu, Y., Yu, T., Dai, W., Ji, Z., Cahyawijaya, S., et al. (2021).“CrossNER:\nEvaluating Cross-Domain Named Entity Recognition,” in Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, 13452–13460.\nLopes, F., Teixeira, C., and Oliveira, H. G. (2019).“Contributions to Clinical\nNamed Entity Recognition in Portuguese,” in Proceedings of the 18th BioNLP\nWorkshop and Shared Task, 223–233. doi:10.18653/v1/w19-5024\nLowe, D. M., and Mayﬁeld, J. (2020).“Extraction of Reactions from Patents Using\nGrammars,\n” in CLEF (Working Notes).\nLuan, Y., Wadden, D., He, L., Shah, A., Ostendorf, M., and Hajishirzi, H. (2019).“A\nGeneral Framework for Information Extraction Using Dynamic Span Graphs,”\nin Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies.\nLong and Short Papers, 3036–3046. doi:10.18653/v1/n19-1308\nMahendran, D., Gurdin, G., Lewinski, N., Tang, C., and McInnes, B. T. (2020).\n“NLPatVCU CLEF 2020 ChEMU Shared Task System Description,” in CLEF\n(Working Notes).\nMartin, L., Muller, B., Ortiz Suárez, P. J., Dupont, Y., Romary, L., Villemonte de La\nClergerie, É., et al. (2020).“CamemBERT: a Tasty French Language Model,” in\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 68980313\nNaderi et al. Ensemble of Masked Language Models for NER\nThe 58th Annual Meeting of the Association for Computational Linguistics\n(ACL 2020) (Seattle, Washington, United States). doi:10.18653/v1/2020.acl-\nmain.645\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efﬁcient Estimation of\nWord Representations in Vector Space.arXiv preprint arXiv:1301.3781.\nMinard, A.-L., Roques, A., Hiot, N., Alves, M. H. F., and Savary, A. (2020).\n“DOING@ DEFT: cascade de CRF pour l ’annotation d’entités cliniques\nimbriquées,” in 6e conférence conjointe Journées d ’Études sur la Parole\n(JEP, 33e édition), Traitement Automatique des Langues Naturelles (TALN,\n27e édition), Rencontre des Étudiants Chercheurs en Informatique pour le\nTraitement Automatique des Langues (RÉCITAL, 22e édition). Atelier DÉﬁ\nFouille de Textes (ATALA; AFCP), 66–78.\nNévéol, A., Grouin, C., Tannier, X., Hamon, T., Kelly, L., Goeuriot, L., et al. (2015).\n“CLEF eHealth Evaluation Lab 2015 Task 1b: Clinical Named Entity\nRecognition,” in CLEF (Working Notes).\nNzali, M. T. (2020).“DEFT 2020: détection de similarité entre phrases et extraction\nd’information (DEFT 2020: sentence similarity detection and information\nretrieval),” in Actes de la 6e conférence conjointe Journées d’Études sur la\nParole (JEP, 33e édition), Traitement Automatique des Langues Naturelles\n(TALN, 27e édition), Rencontre des Étudiants Chercheurs en Informatique\npour le Traitement Automatique des Langues (RÉCITAL, 22e édition). Atelier\nDÉﬁ Fouille de Textes, 91–96.\nPan, S. J., Toh, Z., and Su, J. (2013). Transfer Joint Embedding for Cross-Domain\nNamed Entity Recognition.ACM Trans. Inf. Syst. (Tois)31, 1–27. doi:10.1145/\n2457465.2457467\nPeters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., et al. (2018).\n“Deep Contextualized Word Representations,” in Proceedings of the 2018\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies. (Long Papers)\n(New Orleans, Louisiana: Association for Computational Linguistics),\n2227–2237. doi:10.18653/v1/N18-1202\nQuimbaya, A. P., Múnera, A. S., Rivera, R. A. G., Rodríguez, J. C. D., Velandia, O.\nM. M., Peña, A. A. G., et al. (2016). Named Entity Recognition over Electronic\nHealth Records through a Combined Dictionary-Based Approach. Proced.\nComput. Sci. 100, 55–61. doi:10.1016/j.procs.2016.09.123\nRoberts, K. (2016).“Assessing the Corpus Size vs. Similarity Trade-Off for Word\nEmbeddings in Clinical Nlp,” in Proceedings of the Clinical Natural Language\nProcessing Workshop (ClinicalNLP), 54–63.\nRocktäschel, T., Weidlich, M., and Leser, U. (2012). ChemSpot: a Hybrid System\nfor Chemical Named Entity Recognition. Bioinformatics 28, 1633 –1640.\ndoi:10.1093/bioinformatics/bts183\nRoyan, C., Langé, J.-M., and Abidi, Z. (2020). “Extraction d’information de\nspécialité avec un système commercial générique, ” in 6e conférence\nconjointe Journées d’Études sur la Parole (JEP, 33e édition), Traitement\nAutomatique des Langues Naturelles (TALN, 27e édition), Rencontre des\nÉtudiants Chercheurs en Informatique pour le Traitement Automatique des\nLangues (RÉCITAL, 22e édition). Atelier DÉﬁ Fouille de Textes (ATALA;\nAFCP), 79–90.\nRuas, P., Lamurias, A., and Couto, F. M. (2020).“LasigeBioTM Team at CLEF2020\nChEMU Evaluation Lab: Named Entity Recognition and Event Extraction from\nChemical Reactions Described in Patents Using BioBERT NER and RE,” in\nCLEF (Working Notes).\nSchneider, E. T. R., de Souza, J. V. A., Knafou, J., e Oliveira, L. E. S., Copara, J.,\nGumiel, Y. B., et al. (2020).“BioBERTpt-A Portuguese Neural Language Model\nfor Clinical Named Entity Recognition,” in Proceedings of the 3rd Clinical\nNatural Language Processing Workshop, 65 –72. doi:10.18653/v1/\n2020.clinicalnlp-1.7\nSi, Y., Wang, J., Xu, H., and Roberts, K. (2019). Enhancing Clinical Concept\nExtraction with Contextual Embeddings.J. Am. Med. Inform. Assoc. JAMIA26,\n1297. doi:10.1093/jamia/ocz096\nSingh, J., and Wadhawan, A. (2020).“PublishInCovid19 at WNUT 2020 Shared\nTask-1: Entity Recognition in Wet Lab Protocols Using Structured Learning\nEnsemble and Contextualised Embeddings, ” in Proceedings of the Sixth\nWorkshop on Noisy User-generated Text (W-NUT 2020), 273 –280.\ndoi:10.18653/v1/2020.wnut-1.35\nSohrab, M. G., Nguyen, A.-K. D., Miwa, M., and Takamura, H. (2020).“Mgsohrab\nat WNUT 2020 Shared Task-1: Neural Exhaustive Approach for Entity and\nRelation Recognition over Wet Lab Protocols,” in Proceedings of the Sixth\nWorkshop on Noisy User-generated Text (W-NUT 2020), 290 –298.\ndoi:10.18653/v1/2020.wnut-1.38\nSoldatova, L. N., Nadis, D., King, R. D., Basu, P. S., Haddi, E., Baumlé, V., et al.\n(2014). EXACT2: the Semantics of Biomedical Protocols.BMC Bioinf.15, 1–11.\ndoi:10.1186/1471-2105-15-S14-S5\nStenetorp, P., Pyysalo, S., Topić, G., Ohta, T., Ananiadou, S., and Tsujii, J. (2012).\n“BRAT: a Web-Based Tool for NLP-Assisted Text Annotation,” in Proceedings\nof the Demonstrations at the 13th Conference of the European Chapter of the\nAssociation for Computational Linguistics, 102–107.\nSun, C., and Yang, Z. (2019).“Transfer Learning in Biomedical Named Entity\nRecognition: An Evaluation of Bert in the Pharmaconer Task,” in Proceedings\nof The 5th Workshop on BioNLP Open Shared Tasks, 100–104. doi:10.18653/\nv1/d19-5715\nS u o m i n e n ,H . ,S a l a n t e r ä ,S . ,V e l u p i l l a i ,S . ,C h a p m a n ,W .W . ,S a v o v a ,G . ,\nElhadad, N., et al. (2013). “Overview of the ShARe/CLEF eHealth\nEvaluation Lab 2013, ” in International Conference of the Cross-\nLanguage Evaluation Forum for European Languages (Springer),\n212–231. doi:10.1007/978-3-642-40802-1_24\nTabassum, J., Lee, S., Xu, W., and Ritter, A. (2020).“WNUT-2020 Task 1 Overview:\nExtracting Entities and Relations from Wet Lab Protocols,” in Proceedings of\nEMNLP 2020 Workshop on Noisy User-generated Text (WNUT).\ndoi:10.18653/v1/2020.wnut-1.33\nUzuner, Ö., Solti, I., and Cadag, E. (2010). Extracting Medication Information from\nClinical Text. J. Am. Med. Inform. Assoc. 17, 514 –518. doi:10.1136/\njamia.2010.003947\nUzuner, Ö., South, B. R., Shen, S., and DuVall, S. L. (2011). 2010 I2b2/va challenge\non Concepts, Assertions, and Relations in Clinical Text.J. Am. Med. Inform.\nAssoc.\n18, 552–556. doi:10.1136/amiajnl-2011-000203\nVaidhya, T., and Kaushal, A. (2020).“IITKGP at W-NUT 2020 Shared Task-1:\nDomain Speciﬁc BERT Representation for Named Entity Recognition of Lab\nProtocol,”in Proceedings of the Sixth Workshop on Noisy User-generated Text\n(W-NUT 2020) (Online: Association for Computational Linguistics), 268–272.\ndoi:10.18653/v1/2020.wnut-1.34\nVan Mulligen, E. M., Afzal, Z., Akhondi, S., Vo, D., and Kors, J. (2016).“Erasmus\nMC at CLEF eHealth 2016: Concept Recognition and Coding in French Texts,”\nin CLEF (Working Notes).\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention Is All You Need,” in Proceedings of the 31st International\nConference on Neural Information Processing Systems, 6000–6010.\nVerspoor, K., Nguyen, D. Q., Akhondi, S. A., Druckenbrodt, C., Thorne, C.,\nHoessel, R., et al. (2020). ChEMU Dataset for Information Extraction from\nChemical Patents. Mendeley Data 2, 10–17632. doi:10.17632/wy6745bjfj.1\nWadden, D., Wennberg, U., Luan, Y., and Hajishirzi, H. (2019).“Entity, Relation,\nand Event Extraction with Contextualized Span Representations, ” in\nProceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 5788 –5793. doi:10.18653/v1/d19-\n1585\nWajsbürt, P., Taillé, Y., Lainé, G., and Tannier, X. (2020). “Participation de\nl’équipe du limics à deft 2020, ” in 6e conférence conjointe Journées\nd’Études sur la Parole (JEP, 33e édition), Traitement Automatique des\nLangues Naturelles (TALN, 27e édition), Rencontre des Étudiants\nChercheurs en Informatique pour le Traitement Automatique des\nLangues (RÉCITAL, 22e édition). Atelier DÉ ﬁ Fouille de Textes\n(ATALA; AFCP), 108–117.\nWang, J., Ren, Y., Zhang, Z., and zhang, Y. (2020).“Melaxtech: a Report for CLEF\n2020–ChEMU Task of Chemical Reaction Extraction from Patent,” in CLEF\nWorking Notes.\nWei, Q., Ji, Z., Li, Z., Du, J., Wang, J., Xu, J., et al. (2020). A Study of Deep Learning\nApproaches for Medication and Adverse Drug Event Extraction from Clinical\nText. J. Am. Med. Inform. Assoc.27, 13–21. doi:10.1093/jamia/ocz063\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., and Le, Q. V. (2019).\nXLNet: Generalized Autoregressive Pretraining for Language\nUnderstandingAdv. Neural Inf. Process. Syst., 32.\nYu, J., Bohnet, B., and Poesio, M. (2020). “Named Entity Recognition as\nDependency Parsing,” in Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics (Online: Association for\nComputational Linguistics), 6470–6476. doi:10.18653/v1/2020.acl-main.577\nFrontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 68980314\nNaderi et al. Ensemble of Masked Language Models for NER\nZeng, Q., Fang, X., Liang, Z., and Meng, H. (2020).“Fancy Man Launches Zippo at\nWNUT 2020 Shared Task-1: A Bert Case Model for Wet Lab Entity Extraction,”\nin Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT\n2020) (Online: Association for Computational Linguistics), 299 –304.\ndoi:10.18653/v1/2020.wnut-1.39\nZ h a i ,Z . ,N g u y e n ,D .Q . ,A k h o n d i ,S .A . ,T h o r n e ,C . ,D r u c k e n b r o d t ,C . ,C o h n ,T . ,e ta l .\n(2019). Improving Chemical Named E ntity Recognition in Patents with\nContextualized Word Embeddings.BioNLP 2019, 328. doi:10.18653/v1/w19-5035\nZhang, Y., Xu, J., Chen, H., Wang, J., Wu, Y., Prakasam, M., et al. (2016).\nChemical Named Entity Recognition in Patents by Domain Knowledge and\nUnsupervised Feature Learning. Database 2016, baw049. doi:10.1093/\ndatabase/baw049\nZhao, S. (2004).“Named Entity Recognition in Biomedical Texts Using an Hmm\nModel,” in Proceedings of the International Joint Workshop on Natural\nLanguage Processing in Biomedicine and its Applications (NLPBA/BioNLP),\n87–90. doi:10.3115/1567594.1567613\nZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., et al.\n(2015). “Aligning Books and Movies: Towards story-like Visual Explanations\nby Watching Movies and reading Books, ” in Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV) (USA: IEEE\nComputer Society), 19–27. doi:10.1109/iccv.2015.11\nZhu, H., Paschalidis, I. C., and Tahmasebi, A. (2018). \"Clinical Concept Extraction\nWith Contextual Word Embedding,\" InNIPS Machine Learning for Health\nWorkshop.\nConﬂict of Interest:The authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be construed as a\npotential conﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their afﬁliated organizations, or those of\nthe publisher, the editors and the reviewers. Any product that may be evaluated in\nthis article, or claim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nCopyright © 2021 Naderi, Knafou, Copara, Ruch and Teodoro. This is an open-\naccess article distributed under the terms of the Creative Commons Attribution\nLicense (CC BY). The use, distribution or reproduction in other forums is permitted,\nprovided the original author(s) and the copyright owner(s) are credited and that the\noriginal publication in this journal is cited, in accordance with accepted academic\npractice. No use, distribution or reproduction is permitted which does not comply\nwith these terms.Frontiers in Research Metrics and Analytics | www.frontiersin.org November 2021 | Volume 6 | Article 68980315\nNaderi et al. Ensemble of Masked Language Models for NER",
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.5138359665870667
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5118734240531921
    },
    {
      "name": "Computer science",
      "score": 0.4797895550727844
    },
    {
      "name": "Linguistics",
      "score": 0.42722028493881226
    },
    {
      "name": "Philosophy",
      "score": 0.11057636141777039
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I173439891",
      "name": "HES-SO University of Applied Sciences and Arts Western Switzerland",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I12708293",
      "name": "SIB Swiss Institute of Bioinformatics",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I114457229",
      "name": "University of Geneva",
      "country": "CH"
    }
  ]
}