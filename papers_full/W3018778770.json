{
  "title": "Jointly Trained Transformers Models for Spoken Language Translation",
  "url": "https://openalex.org/W3018778770",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5014351336",
      "name": "Hari Krishna Vydana",
      "affiliations": [
        "Brno University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5065253688",
      "name": "Martin Karafiát",
      "affiliations": [
        "Brno University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5067790073",
      "name": "Kateřina Žmolíková",
      "affiliations": [
        "Brno University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5042273299",
      "name": "Lukáš Burget",
      "affiliations": [
        "Brno University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5030520583",
      "name": "Honza Černocký",
      "affiliations": [
        "Brno University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6769840780",
    "https://openalex.org/W6755559483",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2972389417",
    "https://openalex.org/W2972780808",
    "https://openalex.org/W6775452034",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2995427493",
    "https://openalex.org/W2892213699",
    "https://openalex.org/W3016137827",
    "https://openalex.org/W6730083651",
    "https://openalex.org/W2973048981",
    "https://openalex.org/W6761897164",
    "https://openalex.org/W2936969148",
    "https://openalex.org/W6776039324",
    "https://openalex.org/W6775187155",
    "https://openalex.org/W3015522062",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W3042657922",
    "https://openalex.org/W2943493972",
    "https://openalex.org/W2978265966",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "Conventional spoken language translation (SLT) systems are pipeline based systems, where we have an Automatic Speech Recognition (ASR) system to convert the modality of source from speech to text and a Machine Translation (MT) systems to translate source text to text in target language. Recent progress in the sequence-sequence architectures have reduced the performance gap between the pipeline based SLT systems (cascaded ASR-MT) and End-to-End approaches. Though End-to-End and cascaded ASR-MT systems are reaching to the comparable levels of performances, we can see a large performance gap using the ASR hypothesis and oracle text w.r.t MT models. This performance gap indicates that the MT systems are prone to large performance degradation due to noisy ASR hypothesis as opposed to oracle text transcript. In this work this degradation in the performance is reduced by creating an end to-end differentiable pipeline between the ASR and MT systems. In this work, we train SLT systems with ASR objective as an auxiliary loss and both the networks are connected through the neural hidden representations. This train ing would have an End-to-End differentiable path w.r.t to the final objective function as well as utilize the ASR objective for better performance of the SLT systems. This architecture has improved from BLEU from 36.8 to 44.5. Due to the Multi-task training the model also generates the ASR hypothesis which are used by a pre-trained MT model. Combining the proposed systems with the MT model has increased the BLEU score by 1. All the experiments are reported on English-Portuguese speech translation task using How2 corpus. The final BLEU score is on-par with the best speech translation system on How2 dataset with no additional training data and language model and much less parameters.",
  "full_text": "arXiv:2004.12111v1  [cs.SD]  25 Apr 2020\nJointly T rained T ransformers models for Spoken Language T ranslation\nHari Krishna Vydana, Martin Karaﬁ ´at, Katerina Zmolikova, Luk ´aˇ s Burget, “Honza” ˇCernock´y\nBrno University of T echnology, Faculty of Information T ech nology, IT4I Centre of Excellence, Czechia\nvydana@fit.vutbr.cz\nAbstract\nConventional spoken language translation (SL T) systems ar e\npipeline based systems, where we have an Automatic Speech\nRecognition (ASR) system to convert the modality of source\nfrom speech to text and a Machine Translation (MT) systems to\ntranslate source text to text in target language. Recent pro gress\nin the sequence-sequence architectures have reduced the pe rfor-\nmance gap between the pipeline based SL T systems (cascaded\nASR-MT) and End-to-End approaches. Though End-to-End\nand cascaded ASR-MT systems are reaching to the comparable\nlevels of performances, we can see a large performance gap us -\ning the ASR hypothesis and oracle text w .r.t MT models. This\nperformance gap indicates that the MT systems are prone to\nlarge performance degradation due to noisy ASR hypothesis a s\nopposed to oracle text transcript.\nIn this work this degradation in the performance is reduced b y\ncreating an end to-end differentiable pipeline between the ASR\nand MT systems. In this work, we train SL T systems with ASR\nobjective as an auxiliary loss and both the networks are con-\nnected through the neural hidden representations. This tra in-\ning would have an End-to-End differentiable path w .r.t to th e\nﬁnal objective function as well as utilize the ASR objective for\nbetter performance of the SL T systems. This architecture ha s\nimproved from BLEU from 36.8 to 44.5. Due to the Multi-task\ntraining the model also generates the ASR hypothesis which a re\nused by a pre-trained MT model. Combining the proposed sys-\ntems with the MT model has increased the BLEU score by 1.\nAll the experiments are reported on English-Portuguese spe ech\ntranslation task using How2 corpus. The ﬁnal BLEU score is\non-par with the best speech translation system on How2 datas et\nwith no additional training data and language model and much\nless parameters.\nIndex T erms: Spoken Language Translation, Transformers,\nJoint training, How2 dataset, Auxiliary loss, ASR objectiv e,\nCoupled decoding, End-to-End differentiable pipeline.\n1. Introduction\nSpoken Language Translation (SL T) refers to the task of\ntranscribing the spoken utterance in source language with\nthe text in the target language. SL T systems are typically\ncategorized in two types: Cascade systems and End-to-End\nsystems. Cascade SL T systems in their popular form comprise s\nof an automatic speech recognition (ASR) system followed by\na machine translation (MT) system. The initial ASR system\ngenerates the text sequence for the spoken utterance and the\ngenerated text sequence is translated to the target languag e\nby a machine translation (MT) system. Speech recognition\ntask has a monotonic alignment between the spoken sequence\nand the labeling sequence. But this property is not present\nin MT and SL T tasks. ASR system relies more on the local\nThe work was supported by Czech National Science Foundation\n(GACR) project “NEUREM3 No. 19-26934X\ncontext to transcribe the speech while MT systems would\nneed the wider context of a word to translate it. In a cascade\napproach, both ASR and MT could be trained separately . The\nimprovements in both ASR and MT performance could be\neasily translated to the SL T model. End-to-End SL T system is\na single model that directly transcribes the spoken utteran ce\nwith the text in target language. End-to-End SL T models have\nto learn the complex mapping between the spoken sequence\nand the labeling sequence in the target language which invol ves\nthe word splitting, merging and reordering. End-to-End SL T\nsystems could result in simpler models to train, have low\nlatency and could lead to direct optimization of the require d\ntask. Using the the source language text while optimizing th e\nEnd-to-End SL T model could improve the performance.\nPresently cascade SL T systems perform better than the End-\nto-End SL T systems. Popular End-to-End SL T systems are\nsequence-to-sequence with attention models. With the rece nt\nadvancements in these models the performance gap between\ncascade and End-to-End SL T systems is decreasing. But the\ncascade SL T system suffers from a large performance gap when\nMT models are evaluated using ASR hypotheses compared to\nusing the oracle text. This large performance gap is due to th e\nerroneous ASR hypotheses and the MT system which is not able\nto handle the errors ASR hypotheses. In this study , we explor e\napproaches to reduce this performance gap by creating an End -\nto-End differentiable pipeline between ASR and MT systems.\nSL T systems are trained with ASR objective as an auxiliary\nloss and both the networks are connected through the neural\nhidden representations. By this training the model could ha ve\ndifferentiable path between input(speech) and the ﬁnal lab eling\nsequence and also utilize the ASR objective for better perfo r-\nmance of SL T system. During the inference, both the models\nare connected through N-best hypotheses i.e., N-best hypot he-\nses from ASR model are translated by the MT system in paral-\nlel. The likelihood of every N-best translation is conditio ned on\nthe likelihood from the ASR hypothesis as mentioned in equa-\ntion 7.\nPresently most state-of-the-art MT systems use transforme r\nmodel [19]. Transformer is a recurrent-free encoder-decod er\nmodel. Transformer employs self-attention to model the de-\npendencies in the sequences and cross-attention to model th e\nco-relations across the sequences. With the use of causal-\nity masks to prevent the model from peaking in to the future,\ntransformer model could be parallelized while training. Th e\nmodel has shown capabilities to model the long-term context s.\nTransformer model has been adapted for training ASR sys-\ntems [10, 14]. Transformer models have been used for devel-\noping End-to-End SL T systems [8, 13]. In this study , we use\ntransformer models for training ASR, MT and and SL T systems.\nThe performances of End-to-End and cascade SL T models are\nanalyzed. The SL T systems in this study are developed with\ntwo different granularities i.e., character/BPE units. Th e perfor-\nmances of these models are analyzed on English to Portuguese\ntranslation tasks.\nRecent Developments in SL T systems Some of the recent\nSL T models submitted to IWSL T -2019 are described below:\nThe cascade SL T system with a pipeline of ASR, punctuation\nmodel and MT has the best performance in IWSL T 2019 [3].\nThe ASR used in this work is an ensemble of LSTM-based en-\ncoder decoder model and a big transformer model(¿150M pa-\nrameters) trained with stochastic depth [15]. The MT model\nused is a multi-lingual model(¿200M) trained for two lan-\nguages(German and Portuguese). End-to-End SL T systems are\ntrained using LSTM-encoder-decoder models with character s as\ntarget units in [4]. Transformer models have been adapted fo r\ntraining End-to-End SL T systems [9]. A convolutional front -\nend is used to sub-sample the acoustic sequence by a factor of 4\nand the models are optimized with an addition distance penal ty\nfor the attention to focus more on the local context. Aug-\nmentation methods such as spec-augment [7] and use of back-\ntranslated synthetic text has improved the performance [2] . The\nperformances End-to-End SL T systems have been improved by\ninitializing the parameters of the model from independentl y\ntrained ASR and MT systems [11]. Block multi-task learn-\ning (BMTL) has been used for training MT systems [1]. BMTL\nis an encoder-decoder networks where encoder is shared by de -\ncoders which generates tokens with multiple granularities such\nas characters,sub-words. A Multi-Engine Machine Transla-\ntion (MEMT) is a machine translation that uses inputs in mult i-\nple granularities and produces word based translations. Us e of\npre-trained language models in encoder decoder models have\nbeen explored for developing MT models in [5]. Multi-model\nSL T systems were studied in [6], an LSTM-based encoder-\ndecoder ASR is followed by a multi-model Transformer model.\nT ext and video features have been used for developing multi-\nmodel machine translation systems.\n2. Database\nAll the experiments in this paper are conduced using HOW2\ndata-set [17]. The data-set comprises of train, dev and test sets.\nThe train, dev and test sets consists of 185K, 2305 and 2022\nsentences which amounts to 298, 3 and 4 hours of speech data.\nAll the sentences in the data-set have parallel Portuguese t rans-\nlations. All the models in this work are trained using train s et\nand early stopping is done using validation set.\nPre-processing and Feature Extraction: All the text is\nlower-cased and the punctuation symbols are removed. The se n-\ntence piece model is trained to obtain a sub-word vocabulary of\n5000 tokens for both English and Portuguese texts. Audio fro m\nthe videos are extracted and 16 kHz signal is used in this expe r-\niments. 40-Dimensional Mel-ﬁlter bank features are extrac ted\nusing Kaldi-toolkit with 25 ms window size and 10 ms over-\nlap. Cepstral mean and variances of the features are normali zed\nper-video. The performances of all the translation systems are\nmeasured using Sacre-BLEU.\n3. T ransformer Model\nASR systems used in this work are trained using transformer\nmodels. Transformer is an encoder-decoder model that mod-\nels the temporal dependencies using self-attention mechan ism.\nT ypical encoder-decoder model has three blocks i.e., encod er,\ndecoder and an attention module. The encoder models the se-\nquential dependencies in the input and produces a hidden sta te\nsequence H. The decoder in conditioned on the past predicted\nlabel sequence y1, y2, y3, ..., yi and the context vector ci to pro-\nduce the present label yi+1. The context vector ci is produced\nby summarizing the sub-sequence H which is relevant to pro-\nduce the correct label. Attention mechanism provides a way t o\npeek in to the input sequence and the predicted output sequen ce\nwhile generating a new label.\nLet X=[x1, x2, x3 ,......., xT ] be the input acoustic features\nand H=[ h1, h2, h3, ........, hT ] be the hidden representation. The\nhidden representation H is used by the decoder to produce the\nlabel sequence Y=[ y1, y2, y3, ......, yL].\nSelf-attention mechanism produces the output sequence\nas a weighted sum of the input sequence. Where the weights\nare derived from the similarity between each query and key\nvectors. The self-Attention mechanism can be described by t he\nbelow equation:\nAttention(Q, K, V) = softmax(QKT\n√ dk\n)V. (1)\nWhere Q,K and V are the query , key and value respectively .\nHere query , key has the same dimensionality and key , value ha s\nthe same sequence length. The scalar 1\n√\ndk\nis a normalizing\nfactor to prevent the softmax from entering to the regions wi th\nsmall gradients.\nThe Self-attention mechanism does not have an explicit no-\ntion of position in the sequence. The information about the p osi-\ntion in the sequence is conveyed by ﬁxed trigonometric posit ion\nembeddings. The position embeddings used in this work are\ndescribed below:\nPE(pos,i) =\n{ sin( pos\n100002i/d model ); 0 ≤ i < d model/2\ncos( pos\n100002i/d model ); dmodel/2 ≤ i < d model\n(2)\nIn the above sequence, i is the index of dimension and pos rep-\nresents the position in the sequence. The encoding scheme ge n-\nerates a vector for each position and this vectors are summed to\nthe input feature sequence.\nTransformer employs a Multi-Head attention to learn the\ndependencies between the same sequence and different se-\nquences. The Multi-Head attention computes scaled dot prod uct\nattention multiple times in parallel. Before computing the atten-\ntion, queries, keys and values are passed through linear pro jec-\ntions. Each time the scaled dot-product attention is comput ed,\nthe outputs of all the heads are concatenated and passed thro ugh\na linear layer with dimension dmodel.\nMultiHead(Q, K, V) = Concat[head1, head2, ..., headh]WO\n(3)\nwhere each head is given by\nhi = Attention(QWq\nl , KWk\nl , VWv\nl ) (4)\nA position-wise feed-forward network consists of two linea r\nlayers with rectiﬁed linear activation units (ReLU) as show n in\nthe equation5.\nFFN(x) = max(0, W1x + b1)W2 + b2 (5)\nWhere W1 ∈ Rdmodel× dff and W2 ∈ Rdff × dmodel , b1,b2 are\nthe biases for the weights W1 and W2 respectively . Here df f is\nthe dimensionality of position-wise feed-forward network . The\nblocks described above are connected as shown in the Figure. 1.\nFigure 1: Block diagram of the Transformer models used in this\nstudy.\nWhile training with speech inputs (i.e., for ASR and End-\nto-End SL T the models) convolution front-end is used. The\nfront-end convolution module contains two convolution lay ers\nwith a kernel size of 3 and stride of 2. The convolution lay-\ners used have 64 output channels and are trained with ReLU.\nThese layers reduce the sequence length of the speech featur es\nand memory requirements of the transformer. The positional\nencoding is added to the features after the convolution laye rs.\nWhile training with text inputs (i.e.,for MT systems) an em-\nbedding layer is used as the input layer to convert the discre te\nsymbol sequence to the sequence of continuous embeddings.\nThe model is optimized using ADAM optimizer with\nβ1=0.9 and β2=0.99 and ǫ = 10− 9. The learning rate schedule\nused is deﬁned below:\nlrate = k× dmodel × min(step− 0.5, step × warmup− 1.5)\n(6)\nThe models are trained with 25000 warmup steps and the value\nof k is set to 1. A label smoothing of 0.1 is used for training th e\nmodels. Both the residual and attention dropout values are s et\nto 0.1.\n4. Comparing the Performance of Cascade\nand End-to-END SL T Systems\n4.1. T ransformer ASR systems\nASR systems are trained with characters or BPE units as tar-\nget units. Both the models are trained with 12 encoder layers\nand 6 decoder layers. The models are trained for 150 epochs.\nEach batch comprised of 7000 target units. The models from\nthe last ten epochs are averaged and the averaged weights are\nused during the decoding. ASR hypothesis are decoded with a\nbeam size of 10. The start-of-the-sequence(¡SOS¿) and end- of-\nthe-sequence (¡EOS¿) are modeled by additional tokens. The\ndata-set has some very long sequences with 400-500 charac-\nters and decoding these sentences increases the decoding ti me.\nT o reduce the decoding time the vectorized beam search de-\nscribed in [18] has been used. Decoding with larger beam size\nhas been counter productive, the preﬁx with the noisy EOS to-\nken has higher log-likelihood compared to the actual sequen ce\nin the beam. A threshold mechanism described in [12] has been\nused .i.e., the EOS is considered only if the probability of E OS\nis γ times the top candidate. The value of γ is set to be 1 and\nthis is obtained by ﬁne tuning on the dev set. The length penal -\nties of 1 and 0.8 are used for character and BPE models. No\nlanguage model has been used in this work. The performance\nof both the models presented below T able. 1.\nT able 1: ASR systems trained using Transformer Models.\nDev set(WER) T est set(WER)\nTDNN-LFMMI - 13.7\nS2S-Attention [17] - 19.4\nTransformer (character) 17.47 17.87\nTransformer (BPE) 16.85 16.52\nRow 3 of T able.1 is the performance of the LSTM-based\nsequence-to-sequence model presented in [17]. Row 4 & 5 are\nthe performances of ASR systems developed with transformer\nmodels with characters and BPE (5K) units as target units. Th e\nperformance of transformer model with BPE units as targets\nhas performed better. Row 2 is the ASR systems trained with\nTDNN-LFMMI using Kaldi-recipes this ASR is not used in any\nSL T systems, it is presented to compare the difference in the\nperformances of the ASR systems.\n4.2. T ransformer Machine T ranslation Systems\nTransformer models have been used for training Machine\nTranslation (MT) systems. MT systems are trained with\nthree different input-output granularities such as charac ters-\ncharacters, characters-BPE and BPE-BPE. The details about\nthe architecture of the networks are described as follows: T he\nmodel has 6-encoder layers, 6-decoder layers and the dimen-\nsionality of dmodel and df f are 512, 1024 respectively . The\nmodels are trained with 8 parallel heads. The models are trai ned\nfor 150 epochs and each batch comprised of 7000 target units.\nThe model from the last ten epochs are averaged and the av-\neraged weights are used for decoding. The beam size and\nlength peanlities for decoding are derived from the develop -\nment set. The optimal beam size for all the granularities is 5 ,\nbut the length penalties has varied, for characters-charac ters,\ncharacters-BPE and BPE-BPE the length penalties are 1.2, 1,\n1.2 respectively . The tokens predicted from the model are co n-\nverted back to the text and the Sacre BLEU is computed. The\nEOS threshold described in section 4.1 has been used. The per -\nformances of the MT systems trained are presented in T able. 2 .\nT able 2: MT systems trained using Transformer Models.\nDev\nSacre-BLEU\nT est\nSacre-BLEU\nS2S-Attention-(BPE) 54.4 [16]\nTransformer-(characters-characters) 53.08 52.08\nTransformer-(BPE-BPE) 53.16 52.01\nTransformer-(characters-BPE) 55.32 54.80\nFrom T able.2, it can be observed that both character-\ncharacter and BPE-BPE based models had comparable perfor-\nmances. The charecter based models have taken longer time to\ntrain and decode compared to the BPE models. The models with\ncharacter input and BPE as output has performed better than t he\nother two systems.\n4.3. Cascade SL T models\nCascaded SL T comprises of an ASR system followed by an MT .\nT wo different cascades systems have been trained i.e., ASR s ys-\ntems with characters, BPE’s as output units along with the co r-\nresponding MT model. The performance of cascaded SL T sys-\ntem is presented in the T able. 3. The ASR and MT models\ndescribed in section 4.1, section 4.2 are used in the cascade d\npipeline. The ASR hypothesis is decoded and the decoded hy-\npothesis is used by the MT model to produce the translation.\nT able 3: Cascaded Sp-MT models trained using Transformer .\nArchitecture\n(Input-Output) Granularity\n1-best n-best ranked n-best\nDev set T est set Dev set T est set Dev set T est set\nTransformer\n(characters-characters) 39.12 39.18 - - - -\nTransformer\n(characters-BPE) 41.21 41.31 39.72 40.73 42.52 42.31\nTransformer\n(BPE-BPE) 41.86 41.71 39.16 39.56 43.68 43.6\nFrom T able 3, a large performance degradation can be ob-\nserved between the cascade SL T systems and the MT systems\ndescribed in section 4.2. Due to the use of ASR hypothesis as\ninput text the performance of MT systems a degradation in the\nBLEU ¿ 10 has been observed. Column 1 is the cascade-SL T\nsystem, Column 2, 3 is the performance of SL T systems using n-\nbest hypothesis. Columns 4, 5 are the performance of SL T sys-\ntems which uses the n-best hypothesis which are ranked based\non the log-likelihood scores. The per-sentence log-probab ilities\nfrom the n-best ASR hypothesis is used as a ranking score.\nThe preﬁxes that correspond to the ASR hypothesis are initia l-\nized with their corresponding n-best ranking score. The gen eral\npipeline search is described below:\nP (y|x) = arg maxy\n∑\nz∈ ˆZ(x)) P (y|z)P (z|x)))\n≡ arg maxy\n∑\nz∈ ˆZ(x)) log(P (y|z)) +log(P (z|x)))) (7)\nIn the above equation, x is source speech and y is the target\ntoken sequence and z is the source token sequence. Using the\nN-best hypothesis without ranking scores from the ASR has de -\ngraded the performance which can be seen in column 3,4. Using\nN-best hypothesis with ranking scores has improved the perf or-\nmance. The training procedure described in section 5 is trai ning\nthe above objective function by considering only the one-be st\nhypothesis.\n4.4. End-to-End SL T models\nEnd-to-End SL T systems are trained using transformer model s.\nThe architectural details of the model are described below: the\nmodel has 12-encoders, 6 decoder layers with dmodel, df f are\n256 and 2048 with 4 heads. The models are trained to predict\nthe Portuguese characters/BPE units. The models are traine d for\n150 epochs and each batch comprised of 7000 target units. The\nmodels form the last ten epochs are averaged and the averaged\nweight is used for decoding the hypothesis. The hypotheses a re\ndecoded with a beam of 10 and length penalty of 1. The beam\nT able 4: Transformer based End-to-End SLT .\nArchitecture (T arget unit) (No.of parameters) Dev set T est set\nS2S-Attention-BPE [16] - 36.0\nTransformer-12enc-6dec-(char)(24M) 33.4 33.8\nTransformer-12enc-8dec-(char)(29M) 33.97 33.98\nTransformer-8enc-6dec-(BPE)(24M 36.91 37.45\nTransformer-12enc-6dec-(BPE)(30M) 38.95 39.06\nTransformer-20enc-10dec-(BPE)(46M) 40.03 40.19\nTransformer-20enc-10dec-dmodel-512 39.94 40.59\nsearch and EOS threshold described in section 4.1 are used. T he\nperformance of the End-to-End SP-MT are tabulated in T able. 4.\nFrom the T able. 4, it can be observed that the models with\nBPE as target units has performed better than the models with\ncharacters as target units. The performance of these models\nis worse than the performance of the cascaded SL T models.\nT o increase the model capacity , additional layers a have bee n\nadded to encoder and decoder and the architecture of the mode l\nis in column 1. Row 8 of T able. 4 is a large transformer model\nwith 20 encoder and 10 decoder layers and dmodel is 512 and\nrest of the models are trained dmodel is 256.\n4.5. Augmented T raining for Cascaded SL T\nT o reduce the mismatch between the ASR hypothesis and the\noracle text, the training corpus for MT systems are augmente d\nwith the ASR hypothesis. ASR hypothesis for the training dat a\nis obtained using the ASR model described in section 4.1. The\nperformance of these models is presented in T able 5.\nT able 5: Cascaded SLT systems trained using augmented data.\nOracle T ext ASR hypothesis\nDev set T est set Dev set T est set\nTransformer-(char-char) 50.21 49.44 39.67 39.54\nTransformer-(BPE-BPE) 49.59 49.41 39.12 39.82\nTransformer-(char-BPE) 54.2 53.98 41.04 41.30\nRows 1,2 and 3 are the different cascaded SL T systems with\ndifferent input/output granularites. Column 2, 3 are the pe r-\nformances of cascaded SL T system evaluated with oracle text\nand column 4, 5 are the performances of cascaded SL T systems\nevaluated with ASR hypothesis. From the table .5, the perfor -\nmances of the systems trained with augmented training has no t\nsigniﬁcantly improved the performance of the systems and ha s\nreduced the performance of the systems when evaluated with o r-\nacle text. The models trained from scratch with the augmente d\ndata and the models which are initially trained on clean data\nand later ﬁne-tuned for augmented data have given the simila r\nperformance.\n4.6. Using A veraged Embeddings\nApart from relying on the individual embedding, if the model\ncan also rely on the information from the context of the embed -\nding then the model could be robust to noisy ASR hypothesis.\nT o enforce this in the model, we have randomly selected 10% of\nembedding from the input sequence and each embedding is av-\neraged with the some other randomly selected embedding from\nthe embedding matrix. This operation creates the uncertain ty at\nthe present embedding so that the model has to rely more on the\ncontext to retrieve information about the present label. Th is op-\neration is done both at the encoder and decoder and the result s\nare reported in the T able.6.\nT able 6: SLT models trained using Transformer by averaging\nthe embeddings.\nenc-random-avg dec-random-avg\nOracle text ASR hypothesis Oracle text ASR hypothesis\nDev set T est set Dev set T est set Dev set T est set Dev set T est set\nchar-char 51.94 50.61 39.12 39.18 53.44 52.02 40.1 39.47\nBPE-BPE 50.63 50.2 40.46 40.63 52.44 51.57 41.31 41.23\nchar-BPE 53.8 52.6 41.27 41.25 54.87 55.79 41.85 41.38\nIn T able.6, Column 2-5 are the performances of MT sys-\ntems trained by averaging the encoder embeddings. Columns\n2-3 and 4-5 are the performances of systems when evaluated\nusing oracle text and ASR hypothesis text. Column 6-9 are the\nMT systems trained by averaging the decoder embeddings. The\ntrained systems are evaluated using oracle text and ASR hy-\npothesis and the results are presented in columns 6-7 and 8-9\nrespectively . From the T able.6, it can be observed that the a v-\neraging the encoder embeddings has degraded the performanc e\nof the models. A veraging the embedings at the decoder has im-\nproved the performances of most of the models. Though there\ncan be some improvements these improvements are not signiﬁ-\ncantly better than the cascaded model.\n5. Multi-T ask T raining of SL T systems with\nASR objective as an Auxiliary Loss\nFrom the above sections, it can be observed that the perfor-\nmance of the pipeline based systems is higher than the End-\nto-End approaches. Though the performance of the pipeline\nbased approach is higher it can also observed that the perfor -\nmance gap of the MT systems when evaluated with oracle text\nand ASR hypothesis is higher than 10 BLEU score. T o reduce\nthis performance gap, we have trained a model with End-to-\nEnd differentiable pipeline between the spoken sequence an d\nthe target token sequence. This architecture uses the ASR ob -\njective as an auxiliary loss. The ASR and MT models described\nin the above sections are connected. The hidden representat ion\nfrom the ASR decoder is used as input to train the MT model.\nThe block-diagram describing the model is presented in Fig-\nure. 2. This model would not make any discrete decisions from\nthe ASR model, and the model is optimized for the ﬁnal objec-\ntive.\nAs shown in the block diagram, the neural hidden represen-\ntation from the decoder of the ASR model is used as the input\nto the MT model. The models are optimized with a multi-task\nloss function. The ASR model in the pipeline has two gradient s\ni.e., from ASR objective and MT objective and the MT model\nin the pipeline has gradients w .r.t to MT objective only . The\nmodels are trained for 150 epochs and the weights are average d\nand averaged weights are used for decoding the model. The\nmodel has two decoders in the pipeline, the decoder produces\nthe n-best hypothesis from the ASR with a beam size of 10 and\nneural hidden representation from each of the n-best output is\nconsidered as a separate input to the MT . The MT model pro-\nFigure 2: Block diagram of the Transformer based Sp-MT\nmodel with ASR objective as auxiliary loss function.\nduces the n-best hypothesis for each input with a beam size of\n5. All the hypothesis produced by the MT model are combined\nand the best hypothesis from the MT model is used as the out-\nput hypothesis. The performance of the proposed SL T systems\nare tabulated in the T able. 7. The block-diagram describing the\nproposed architecture is presented in Figure.3.\nT able 7: SLT systems jointly trained with a Multi-T ask objective\nand ASR as an auxilary loss\nSacre-BLEU WER\nDev set T est set Dev set T est set\nTransformer-End-to-End 36.2 36.8 - -\nTransformer-(char-BPE) 40.48 40.46 24.27 24.50\nTransformer-(BPE-BPE) 44.9 44.18 17.99 18.09\nFrom the T able. 7, it can be observed that the performance\nof this model is better than the standard pipeline systems. T he\nproposed model has an improvement in the BLEU score of 4-\n5 compared to End-to-End systems. From the T able.7, it can\nalso be observed that the performance of the systems using bo th\nASR and MT systems trained with BPE units is higher than the\nperformance of the systems trained using ASR with character s\nand the MT systems with BPE units. This difference in the\nperformance can be attributed to correspondence between th e\ntokens of ASR and MT objectives. As the data-set has parallel\ntext for training, there could be correspondence between th e top\n5k BPE units in English and Portuguese tokens and optimiz-\ning with the joint loss has optimized the ASR model with BPE\nas targets better than model optimized with characters as ta rget\ntokens. From the T able 7, the performance of the model is bet-\nter than the pipeline based systems. This can also be observe d\nfrom columns 3-4 of T able 7, these are the WER’s obtained\nfrom the jointly-trained model. From the joint training the ASR\nmodel with BPE target units has better performance than the\nASR with character units and this has lead to better inferenc e\nof the bottlenecks and the MT target sequence. T o improve thi s\nperformance the ASR and MT models could be ensembled with\na separately trained ASR and MT models. The ensembling and\njoint decoding is presented in section 6.\n6. SL T Systems with Multi-T ask T raining\nand Joint Decoding\nFrom T able.7, it can be observed that the performance of the\nASR model in the joint training is not on-par with the per-\nformance in subsection 4.1. T o improve this performance the\nASR model is ensembled with the ASR model trained in sub-\nsection 4.1. During the inference for the softmax distribut ions\nfrom both the models are computed for each preﬁx and the dis-\ntributions are averaged and averaged distribution is used f or\nthe beam search. The proposed model also produces charac-\nters/BPE units as outputs along with the neural-hidden repr e-\nsentations. Along the the neural hidden representations, t he\ncharacters/BPE can also be used for translation and the MT\nmodels described in the subsection. 4.2 can also be is used to\ngenerate the translations. Both the models are ensembled an d\nthe performance are tabulated in the below T able.8. The Bloc k\ndiagram for the joint decoding is presented in Figure. 3. ASR\nmodel is decoded with a beam size of 10 and every hypothesis\nis then decoded by a MT model with a beam size of 5.\nFigure 3: Block diagram describing the Multi-T ask training\nwith ASR objective as an auxiliary loss.\nT able 8: Multi-T ask training with ASR as a auxiliary loss and\njoint decoding\nstand-alone Ens-ASR Ens-MT Ens-ASR-Ens-MT\nDev set T est set Dev set T est set Dev set T est set Dev set T est set\nTransformer-\n(char-BPE) 40.84 40.81 41.92 41.70 43.04 42.31 43.72 43.06\nTransformer-\n(BPE-BPE) 45.32 44.69 46.93 46.22 45.8 45.7 47.33 46.9\nFrom T able .8, column 2, 3 are the performances of jointly\ntrained model. Column 4-5, 6-7 are the performances of SL T\nsystems with ASR, MT ensembling respectively . Column 8-9\nare the performances of SL T systems with both ASR and MT\nensembling. From the table. 8, it can be observed that using\njoint decoding using both the models has improved the perfor -\nmances of both the models.\n7. Using Pre-trained ASR & MT models for\nMulti-T ask T raining of SL T systems\nThe SL T model described in section 5, is initialized using th e\npre-trained ASR and MT models.The performance of these\nmodels is presented in T able.9. The pre-trained models are c on-\nnected as described in the block diagram 2. The two models\nare connected with an additional linear layer or self-atten tion\nlayers. The models are trained either by ﬁne-tuning only the\nadditional layers or ﬁne-tuning the whole network. The mod-\nels are trained for 100 epochs and decoded with same hyper-\nparameters as mentioned in section 5.\nT able 9: Multi-T ask training with ASR as a auxilary loss. The\nmodels are initialized with pre-trained ASR and MT models.\nstand-alone Ens-ASR Ens-MT\nEns-ASR+\nEns-MT\nDev T est Dev T est Dev T est Dev T est\nTransformer-(char-BPE) 40.84 40.81 41.92 41.70 43.04 42.31 43.72 43.06\nTransformer-(char-BPE)\npre-training+linear 42.59 42.86 42.49 42.22 42.42 42.62 41.99 41.88\nTransformer-(characters-BPE)\npre-training+ self-attn layers 42.74 42.87 42.49 42.33 42.43 42.33 41.95 41.7\nTransformer-(char-BPE)\nﬁne-tuning+linear 43.18 42.91 43.81 43.22 43.08 42.67 43.36 42.98\nTransformer-(char-BPE)\nﬁne-tuning+ self-attn layers 42.65 42.93 42.82 42.82 43.81 43.22 43.36 42.98\nTransformer-(BPE-BPE) 45.32 44.69 46.93 46.22 45.8 45.7 47.33 46.9\nTransformer-(BPE-BPE)\npre-training+linear 30.08 30.19 30.07 30.39 44.15 44.39 44.09 44.46\nTransformer-(BPE-BPE)\npretraining+selfattn 45 44.7 45.15 44.99 44.67 44.84 44.73 44.79\nTransformer-(BPE-BPE)\nﬁnetuning+linear 45.02 44.45 45.8 45.4 45.41 45.21 46.12 45.97\nTransformer-(BPE-BPE)\nﬁnetuning+selfattn 45.05 44.7 45.35 45.77 45.23 45.05 45.6 45.5\nFrom T able.9, row 3, 8 are the performances of systems\nwhich are trained from scratch. Rows 4,5 and 9,10 are the per-\nformances of the model obtained by ﬁne-tuning the inserted l ay-\ners. Rows 6,7 and 11, 12 are the performances obtained by ﬁne-\ntuning whole pre-trained network along with the inserted la yers.\nFrom Column 2, 3 of T able. 9, it can be observed the initializi ng\nthe weights of the joint model from a separately trained ASR\nand MT has shown improvements, when the outputs of ASR\nare characters i.e., rows 4-7 have better performances than 3 but\nrows 9-11 have a slight degradation compared to row 8. This\ndifference in performance can be attributed to the fact that the\nrepresentations produced by the network for the characters are\nmore generic to task compared to BPE units. This could also be\nseen in a sharp drop in the performance of stand-alone system in\nrow 9 and the performance is recovered when the discrete BPE-\nunits are decoded and translated through MT model. The joint\nmodels trained from scratch have shown better improvements\nwhen emsembled with other independent ASR and MT mod-\nels as in rows 3,8. But the joint model with weights initializ ed\nfrom separately trained ASR and MT models as in rows 4-7\nand 9-12 did not have much improvements from ensembling as\nboth the models are similar. Ensembling with MT models have\nshown better improvements in BLEU score compared to ensem-\nbling with ASR models. This improvements could be attribute d\nto the reason that MT model in joint model uses the continu-\nous representations from the ASR module while the ensembled\nMT models uses discrete representations, these modules are us-\ning different modalities and also the MT model is the last in\nthe whole pipeline. Using the n-best hypothesis and doing a\ncoupled search between ASR and MT has given better perfor-\nmances than using the 1-best hypothesis. Training the model in\nthe proposed approach has give the best BLEU-score of 47.33\nand 46.9 on dev and test sets of HOW2 data-set. By far this is\non-par with the best performing systems [3] in IWSL T -2019 on\nHow2 data-set. The total parameters of the model (ASR+MT)\nare around 70M which is much lesser than the (¿350M) param-\neters in the models [3].\n8. Conclusion& Future scope\nThe performance gap between End-to-End SL T models have\nbeen improving but still pipeline based approaches are bett er\nthan the End-to-End approaches. A large performance gap can\nbe observed when machine translation models are evaluated\nwith ASR hypothesis and oracle text transcription. Propose d\nsystems aims to reduce this gap by training models with end-\nto-end differentiable pipeline between ASR and MT models.\nUsing the ASR objective as auxiliary loss while optimizing t he\nmodels have improved the performance of Joint SL T . The per-\nformance gains are higher when both ASR and MT models are\ntrained with BPE as target units. As all the models are trans-\nformer models, they could be replaced with Non-Auto regres-\nsive models which could reduce the latency of decoding. A\nmechanism to force the transformer to retrieve the informat ion\nfrom the context in the presence of less conﬁdent embedding\ncould help the model to reduce the performance gap of MT\nmodels when used with oracle text and the noisy ASR hypothe-\nsis. While training with the noisy transcripts, using a sent ence-\nwise conﬁdence metric and lowering the scale of penalizing t he\nmodel conditioning on the conﬁdence metric could improve th e\nperformance of MT when dealing with noisy transcriptions. U s-\ning unpaired additional data for training ASR and MT models\ncould improve the performance of the ensemble.\n9. References\n[1] CMU’s Machine Translation System for IWSLT 2019 . Zenodo,\nNovember 2019.\n[2] Data Augmentation for End-to-End Speech Translation:\nFBK@IWSLT ’19 . Zenodo, November 2019.\n[3] The IWSLT 2019 KIT Speech Translation System . Zenodo,\nNovember 2019.\n[4] ON-TRAC Consortium End-to-End Speech Translation Systems\nfor the IWSLT 2019 Shared T ask . Zenodo, November 2019.\n[5] The LIG system for the English-Czech T ext Translation T ask o f\nIWSLT 2019 . Zenodo, November 2019.\n[6] Transformer-based Cascaded Multimodal Speech Translatio n.\nZenodo, November 2019.\n[7] M Di Gangi, Matteo Negri, V iet Nhat Nguyen, Amirhossein T ebb-\nifakhr, and Marco Turchi. Data augmentation for end-to-end\nspeech translation: Fbk@ iwslt19. In Proc. The International\nConference on Spoken Language Translation , 2019.\n[8] Mattia A Di Gangi, Matteo Negri, and Marco Turchi. Adapt-\ning transformer to end-to-end spoken language translation . pages\n1133–1137, 2019.\n[9] Mattia A Di Gangi, Matteo Negri, and Marco Turchi. Adapti ng\ntransformer to end-to-end spoken language translation. Proc. IN-\nTERSPEECH, pages 1133–1137, 2019.\n[10] Linhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: a\nno-recurrence sequence-to-sequence model for speech reco gni-\ntion. In Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing ,\npages 5884–5888. IEEE, 2018.\n[11] Hirofumi Inaguma, Xuan Zhang, Zhiqi W ang, Adithya Rend uch-\nintala, Shinji W atanabe, and Kevin Duh. The jhu/kyotou spee ch\ntranslation system for iwslt 2018. In Proc. The International Con-\nference on Spoken Language Translation , 2018.\n[12] Jacob Kahn, Ann Lee, and A wni Hannun. Self-training for end-to-\nend speech recognition. arXiv preprint arXiv:1909.09116 , 2019.\n[13] Shigeki Karita, Nanxin Chen, T omoki Hayashi, T akaaki H ori,\nHirofumi Inaguma, Ziyan Jiang, Masao Someki, Nelson En-\nrique Y alta Soplin, Ryuichi Y amamoto, Xiaofei W ang, et al. A\ncomparative study on transformer vs rnn in speech applicati ons.\narXiv preprint arXiv:1909.06317 , 2019.\n[14] T omohiro Nakatani. Improving transformer-based end- to-end\nspeech recognition with connectionist temporal classiﬁca tion and\nlanguage model integration. pages 1408–1412, 2019.\n[15] Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues, Markus\nMuller, and Alex W aibel. V ery deep self-attention networks for\nend-to-end speech recognition. arXiv preprint arXiv:1904.13377 ,\n2019.\n[16] V ikas Raunak, Sang Keun Choe, Quanyang Lu, Y i Xu, and Flo -\nrian Metze. On leveraging the visual modality for neural mac hine\ntranslation. arXiv preprint arXiv:1910.02754 , 2019.\n[17] Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmo nd El-\nliott, Lo¨ ıc Barrault, Lucia Specia, and Florian Metze. How 2: a\nlarge-scale dataset for multimodal language understandin g. arXiv\npreprint arXiv:1811.00347 , 2018.\n[18] Hiroshi Seki, T akaaki Hori, Shinji W atanabe, Niko Mori tz, and\nJonathan Le Roux. V ectorized beam search for ctc-attention -\nbased speech recognition. In Proc. INTERSPEECH , pages 3825–\n3829, 2019.\n[19] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko reit,\nLlion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polo-\nsukhin. Attention is all you need. In Proc. Advances in Neural\nInformation Processing Systems , pages 5998–6008, 2017.",
  "topic": "Speech translation",
  "concepts": [
    {
      "name": "Speech translation",
      "score": 0.8188124895095825
    },
    {
      "name": "Computer science",
      "score": 0.8137721419334412
    },
    {
      "name": "Pipeline (software)",
      "score": 0.6718133687973022
    },
    {
      "name": "Machine translation",
      "score": 0.6550890803337097
    },
    {
      "name": "Speech recognition",
      "score": 0.6145668029785156
    },
    {
      "name": "Language model",
      "score": 0.5838594436645508
    },
    {
      "name": "Oracle",
      "score": 0.5591351389884949
    },
    {
      "name": "Spoken language",
      "score": 0.5349586606025696
    },
    {
      "name": "Transformer",
      "score": 0.46673309803009033
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43931296467781067
    },
    {
      "name": "End-to-end principle",
      "score": 0.4214757978916168
    },
    {
      "name": "Natural language processing",
      "score": 0.3905346393585205
    },
    {
      "name": "Programming language",
      "score": 0.08840858936309814
    },
    {
      "name": "Engineering",
      "score": 0.07658180594444275
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}