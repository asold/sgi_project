{
  "title": "Shifted Chunk Transformer for Spatio-Temporal Representational Learning",
  "url": "https://openalex.org/W3193761636",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Zha, Xuefan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2225634571",
      "name": "Zhu Wen-tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226555074",
      "name": "Lv, Tingxun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2013861969",
      "name": "Yang Sen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106270453",
      "name": "Liu, Ji",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W3154596443",
    "https://openalex.org/W3200690474",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2126579184",
    "https://openalex.org/W2998027361",
    "https://openalex.org/W3194591991",
    "https://openalex.org/W24089286",
    "https://openalex.org/W2553594924",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W2963820951",
    "https://openalex.org/W2510185399",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W2962934715",
    "https://openalex.org/W2625366777",
    "https://openalex.org/W2887051120",
    "https://openalex.org/W2118323718",
    "https://openalex.org/W2963321993",
    "https://openalex.org/W2895243423",
    "https://openalex.org/W3035619757",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2970206392",
    "https://openalex.org/W2156303437",
    "https://openalex.org/W3034572008",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2307035320",
    "https://openalex.org/W2736334449",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W2945792291",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3035303837",
    "https://openalex.org/W3109304426",
    "https://openalex.org/W3089748971",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2883429621",
    "https://openalex.org/W2948048211",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3147387781",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2066941820",
    "https://openalex.org/W2963155035",
    "https://openalex.org/W2962711930"
  ],
  "abstract": "Spatio-temporal representational learning has been widely adopted in various fields such as action recognition, video object segmentation, and action anticipation. Previous spatio-temporal representational learning approaches primarily employ ConvNets or sequential models,e.g., LSTM, to learn the intra-frame and inter-frame features. Recently, Transformer models have successfully dominated the study of natural language processing (NLP), image classification, etc. However, the pure-Transformer based spatio-temporal learning can be prohibitively costly on memory and computation to extract fine-grained features from a tiny patch. To tackle the training difficulty and enhance the spatio-temporal learning, we construct a shifted chunk Transformer with pure self-attention blocks. Leveraging the recent efficient Transformer design in NLP, this shifted chunk Transformer can learn hierarchical spatio-temporal features from a local tiny patch to a global video clip. Our shifted self-attention can also effectively model complicated inter-frame variances. Furthermore, we build a clip encoder based on Transformer to model long-term temporal dependencies. We conduct thorough ablation studies to validate each component and hyper-parameters in our shifted chunk Transformer, and it outperforms previous state-of-the-art approaches on Kinetics-400, Kinetics-600, UCF101, and HMDB51.",
  "full_text": "Shifted Chunk Transformer for\nSpatio-Temporal Representational Learning\nXuefan Zha\nKuaishou Technology\nzhaxuefan@kuaishou.com\nWentao Zhu\nKuaishou Technology\nwentaozhu@kuaishou.com\nTingxun Lv\nKuaishou Technology\nlvtingxun@kuaishou.com\nSen Yang\nKuaishou Technology\nsenyang@kuaishou.com\nJi Liu\nKuaishou Technology\nji.liu.uwisc@Gmail.com\nAbstract\nSpatio-temporal representational learning has been widely adopted in various ﬁelds\nsuch as action recognition, video object segmentation, and action anticipation.\nPrevious spatio-temporal representational learning approaches primarily employ\nConvNets or sequential models, e.g., LSTM, to learn the intra-frame and inter-\nframe features. Recently, Transformer models have successfully dominated the\nstudy of natural language processing (NLP), image classiﬁcation, etc. However,\nthe pure-Transformer based spatio-temporal learning can be prohibitively costly\non memory and computation to extract ﬁne-grained features from a tiny patch. To\ntackle the training difﬁculty and enhance the spatio-temporal learning, we construct\na shifted chunk Transformer with pure self-attention blocks. Leveraging the recent\nefﬁcient Transformer design in NLP, this shifted chunk Transformer can learn\nhierarchical spatio-temporal features from a local tiny patch to a global video\nclip. Our shifted self-attention can also effectively model complicated inter-frame\nvariances. Furthermore, we build a clip encoder based on Transformer to model\nlong-term temporal dependencies. We conduct thorough ablation studies to validate\neach component and hyper-parameters in our shifted chunk Transformer, and it\noutperforms previous state-of-the-art approaches on Kinetics-400, Kinetics-600,\nUCF101, and HMDB51.\n1 Introduction\nSpatio-temporal representational learning tries to model complicated intra-frame and inter-frame re-\nlationships, and it is critical to various tasks such as action recognition [21], action detection [57, 54],\nobject tracking [24], and action anticipation [22]. Deep learning based spatio-temporal representation\nlearning approaches have been largely explored since the success of AlexNet on image classiﬁca-\ntion [25, 11]. Previous deep spatio-temporal learning can be mainly divided into two aspects: deep\nConvNets based methods [ 37, 15, 16] and deep sequential learning based methods [ 57, 30, 31].\nDeep ConvNets based methods are primarily integrated various factorization techniques [ 53, 36],\nor a priori [16] for efﬁcient spatio-temporal learning [15]. Some works focus on extracting effec-\ntive spatio-temporal features [43, 8] or capturing complicated long-range dependencies [51]. Deep\nsequential learning based methods try to formulate the spatial and temporal relationships through\nadvanced deep sequential models [30] or the attention mechanism [31].\nOn the other hand, the Transformer has become the de-facto standard for sequential learning tasks\nsuch as speech and language processing [ 46, 12, 56, 20]. The great success of Transformer on\nnatural language processing (NLP) has inspired computer vision community to explore self-attention\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2108.11575v5  [cs.CV]  29 Oct 2021\n.\nFigure 1: The framework of the proposed shifted chunk Transformer which involves two main\ncomponents, frame encoder (dark grey) and clip encoder. The frame encoder consists ofN alternative\nblocks of image chunk self-attention (left) and shifted multi-head self-attention (MSA).\nstructures for several vision tasks, e.g., image classiﬁcation [13, 42, 38], object detection [6], and\nsuper-resolution [35]. The main difﬁculty in pure-Transformer models for vision is that Transformers\nlack the inductive biases of convolutions, such as translation equivariance, and they require more\ndata [13] or stronger regularisation [42] in the training. It is only very recently that, vision Transform\n(ViT), a pure Transformer architecture, has outperformed its convolutional counterparts in image\nclassiﬁcation when pre-trained on large amounts of data [ 13]. However, the hurdle is aggravated\nwhen the pure-Transformer design is applied to spatio-temporal representational learning.\nRecently, a few attempts have been made to design pure-Transformer structures for spatio-temporal\nrepresentation learning [4, 5, 14, 2]. Simply applying Transformer to 3D video domain is compu-\ntationally intensive [4]. The Transformer based spatio-temporal learning methods primarily focus\non designing efﬁcient variants by factorization along spatial- and temporal-dimensions [ 4, 5], or\nemploying a multi-scale pyramid structure for a trade-off between the resolution and channel capacity\nwhile reducing the memory and computational cost [14]. The spatio-temporal learning capacity can\nbe further improved by extracting more effective ﬁne-grained features through advanced and efﬁcient\nintra-frame and inter-frame representational learning.\nIn this work, we propose a novel spatio-temporal learning framework based on pure-Transformer,\ncalled shifted chunk Transformer as illustrated in Fig. 1, which extracts effective ﬁne-grained intra-\nframe features with a low computational complexity leveraging the recent advance of Transformer\nin NLP [ 23]. Specially, we divide each frame into several local windows called image chunks,\nand construct a hierarchical image chunk Transformer, which employs locality-sensitive hashing\n(LSH) to enhance the dot-product attention in each chunk and reduces the memory and computation\nconsumption signiﬁcantly. To fully consider the motion effect of object, we design a robust self-\nattention module, shifted self-attention, which explicitly extracts correlations from nearby frames.\nWe further design a pure-Transformer based frame-wise attention module, clip encoder, to model the\ncomplicated inter-frame relationships with a minimal extra computational cost. Our contributions\ncan be summarized as follows:\n• We construct an image chunk self-attention to mine ﬁne-grainedintra-frame features leverag-\ning the recent advance of Transformer. The hierarchical image chunk Transformer employs\nlocality-sensitive hashing (LSH) [3] to reduce the memory and computation consumption\nsigniﬁcantly, which enables an effective spatio-temporal learning directly from a tiny patch.\n• We build a shifted self-attention to fully consider the motion effect of objects, which yields\neffective modeling of complicated inter-frame variances in the spatio-temporal representa-\n2\ntional learning. Furthermore, a clip encoder with a pure-Transformer structure is employed\nfor frame-wise attention, which models complicated and long-term inter-frame relationships\nat a minimal extra cost.\n• The shifted chunk Transformer with pure-Transformer outperforms previous state-of-the-\nart approaches on several action recognition benchmarks, including Kinectics-400 [ 21],\nKinetics-600 [7], UCF101 [41] and HMDB51 [26].\n2 Related Work\nConventional deep learning based action recognitionConventional deep spatio-temporal repre-\nsentational learning mainly involves two aspects: deep sequential learning based methods [57, 30, 31]\nand deep ConvNet based methods [ 37, 15, 16]. The recurrent networks can be extended to 3D\nspatio-temporal domain for action recognition [30]. In deep ConvNet based methods, two-stream\nConvNet employs two branches of 2D ConvNets and explicitly models motion by optical ﬂow [40].\nThe C3D [43] and I3D [8] directly extend 2D ConvNets to 3D ConvNets, which is natural for 3D\nspatio-temporal representational learning [9]. However, the 3D ConvNet requires signiﬁcantly more\ncomputation and more training data to achieve a desired accuracy [53]. Thus, P3D [36] and S3D [53]\nattempt to factorize the 3D convolution into a 2D spatial convolution and a 1D temporal convolution.\nSlowFast network [16] and X3D [ 15] conduct trade-offs among resolution, temporal frame rate\nand the number of channels for the efﬁcient video recognition. Non-local network [ 51] proposes\nto add non-local operations in deep network and captures long-range dependencies. The recent\npure-Transformer based spatio-temporal learning enables longer dependency relationship modeling\nand further increases the accuracy of action recognition [4].\nVision Transformers NLP community has witnessed the great success of pre-training by Trans-\nformer [46, 12], and it has been emerging for image classiﬁcation [13, 42, 38], object detection [6],\nand image super-resolution [35]. CPVT [10] employs pre-deﬁned and independent input tokens to\nincrease the generalization for image classiﬁcation. Pure-Transformer network has no inductive bias\nor prior as ConvNets. ViT [13] pre-trains on large amounts of data and attains excellent results on\nimage classiﬁcation. CvT [ 52] introduces convolutions into ViT to yield better performance and\nefﬁciency. DeiT [42] and MViT [14] instead employ distillation and multi-scale to cope with the\ntraining difﬁculty. PVT [49] and segmentation Transformer [55] further extend Transformer to dense\nprediction tasks, e.g., object detection and semantic segmentation. Simply applying Transformer to\n3D video spatio-temporal representational learning aggravates the training difﬁculty signiﬁcantly,\nand it requires advanced model design for pure-Transformer based spatio-temporal learning.\nTransformer based action recognitionRecently, only a few works have been conducted using\npure-Transformer for spatio-temporal learning [4, 14, 5, 2]. Most of the efforts focus on designing\nefﬁcient Transformer models to reduce the computation and memory consumption. ViViT [ 4]\nand TimeSformer [5] study various factorization methods along spatial- and temporal-dimensions.\nMViT [14] conducts a trade-off between resolution and the number of channels, and constructs a\nmulti-scale Transformer to learn a hierarchy from simple dense resolution and ﬁne-grained features\nto complex coarse features. V ATT [2] conducts unsupervised multi-modality self-supervised learning\nwith a pure-Transformer structure. In this work, we extract ﬁne-grained intra-frame features from\neach tiny patch and model complicated inter-frame relationship through efﬁcient and advanced\nself-attention blocks.\n3 Method\nIn this section, we describe each component of our shifted chunk Transformer for spatio-temporal\nrepresentation learning in video based action recognition.\n3.1 Overview\nLet X = [X1, X2, ··· , XT ] ∈RT×H×W×3 be one input clip of T RGB frames sampled from a\nvideo, where Xi ∈RH×W×3 is the i-th frame in the clip, and H and W are the frame size. To\ndesign an efﬁcient pure-Transformer based spatio-temporal learning, we construct a shifted chunk\nTransformer, including image chunk self-attention blocks, shifted multi-head self-attention blocks,\n3\nand a clip encoder, as illustrated in Fig. 1. We ﬁrst construct an image chunk self-attention block\nleveraging the advanced efﬁcient Transformer design in NLP [23], which is illustrated in the left of\nFig. 1. The locality-sensitive hashing (LSH) [3] in the image chunk self-attention enables a relatively\nsmall patch as a token, thus it is capable of extracting ﬁne-grained intra-frame features. A linear\npooling layer is designed to adaptively reduce the resolution after LSH attention. After that, a shifted\nself-attention is designed to extract motion related inter-frame features. Our shifted self-attention\nconsiders the motion of objects across nearby frames and explicitly models the temporal relationship\ninto self-attention. The frame encoder shown in dark grey color can be an effective feature extractor\nwhich can be stacked for several times. The hierarchical frame encoder and image chunk self-attention\nfurther learns an effective multi-level feature from local to global abstraction. Lastly, we employ a\npure-Transformer to learn complicated inter-frame relationships and frame-wise attention along the\ntemporal dimension. We use multi-head self-attention (MSA) in all the blocks.\n3.2 Image Chunk Self-Attention\nTable 1: ViT-B [13] with a smaller patch size yields\nhigher accuracy (%).\nCrop size Patch size K400 UCF101\n224 16 75.3 95.3\n224 8 78.4 97.0\nTransformer can learn complicated long range\ndependencies which can be computed through\nthe high efﬁcient matrix product [ 46]. Differ-\nent from convolution which has inherent induc-\ntive bias [13], Transformer learns the entire fea-\ntures from data. The main challenge for a pure-\nTransformer based vision model mainly involves\ntwo aspects: 1) how to design an efﬁcient model\nto learn effective features from the entire image,\nbecause simply treating each pixel as a token is computationally intensive, 2) how to train this power-\nful model and learn various effective features from data. ViT [13] treats each patch of size 16 ×16 as\none token and pre-trains the model with large amounts of data. We argue that Transformer with a\nsmaller patch as a token can extract ﬁne-grained features which improves spatio-temporal learning\nfor action recognition. For ViT-B-16 [13] of crop size 224 ×224 as a frame encoder followed by a\nshifted MSA and a clip encoder, a smaller patch size of 8 ×8 yields better accuracy on Kinectics-400\nand UCF101 as shown in Table 1.\nThe Transformer computes each pairwise correlation through a dot product, thus it has a high\ncomputational complexity of O(L2), where L is the totally number of tokens. In natural language\nprocessing (NLP), LSH attention [23] employs locality-sensitive hashing (LSH) bucketing [3] and\nchunk sorting for queries and keys to approximate the attention matrix computation. Leveraging the\nefﬁcient LSH approximation, the LSH attention reduce the computation complexity to O(L log L).\nTo preserve locality property and learn transition and rotation invariant low-level features from images,\nwe ﬁrstly design a visual local transformer (ViLT) with shared parameters along different image local\nwindows, or image chunks. Each image chunk consists of multiple tiny patches as illustrated in the\nleft bottom block of Fig. 1. We intend to employ patches of a small size in the ViLT which is the ﬁrst\nlevel abstraction of the input, so that the model extracts a ﬁne-grained representation which enhances\nthe entire spatial-temporal learning. Employing the small patch yields large number of tokens in\nthe following self-attention, and we construct an image locality-sensitive hashing (LSH) attention\nleveraging advanced design of Transformer in NLP [23]. The image LSH attention can efﬁciently\nextract higher-level and plentiful intra-frame features ranging from a tiny patch to the entire image.\nThe framework of image chunk self-attention is illustrated in the left part of Fig. 1.\nVisual local transformer (ViLT)In the shifted chunk Transformer, we ﬁrstly construct a ViLT\nwhich slides one self-attention for each tiny patch along the whole image. The ViLT is illus-\ntrated in the bottom block of Fig. 1. Let h, w be the height and width of the tiny image patch\np ∈ Rh×w×3. Following the success of ViT [ 13], we treat each patch as one dimensional\nvector of length h ×w ×3. Suppose each chunk consists of m ×n tiny patches, denoted as\n{p1,1, p1,2, ··· , p1,n; p2,1, ··· , p2,n; ··· ; pm,1, ··· , pm,n}. After ﬂattening the chunk into a list of\ntiny patches, we denote the chunk as {p1, p2, ··· , pL}without loss of generality, where L = m ×n.\nIn ViLT, we use a learnable 1D position embeddingEpos ∈RL×D to retain position information\nz0 = [p1E; p2E; ··· ; pm×nE] +Epos, E ∈R(h×w×3)×D, (1)\n4\nwhere E is the linear patch embedding matrix, and D is the embedding dimension. Then we can\nconstruct alternating layers of multi-head self-attention (MSA), MLP with GELU [19] non-linearity,\nLayernorm (LN) and residual connections [46] for the chunk as\nz′\nm = MSA(LN(zm−1)) +zm−1, zm = MLP(LN(z′\nm)) +z′\nm, m = 1, ··· , M, (2)\nwhere M is the number of blocks. We conduct the ViLT sliding the entire image without over-\nlapping. The parameters of the ViLT are shared among all the image chunks, which forces the\nchunk self-attention to learn translation and rotation invariant, and ﬁne-grained features. The\ntiny patch-wise feature extraction preserves the locality property, which is a strong prior for nat-\nural images. After the ViLT, we obtain the extracted features for the entire image denoted as\ny = [y1; y2; ··· ; yL; ··· ; yL×L′ ], where the entire image can be split into L′ = m′×n′ =\n⌈ H\nh×m⌉×⌈ W\nw×n⌉chunks, and we conduct zero padding for the last chunks in each row and column.\nThe ViLT forces to learn image locality features which is a desired property for a low-level feature\nextractor [27]. For pure-Transformer based vision system, it also reduces the memory consumption\nsigniﬁcantly because it restricts the correlation of one tiny patch within the local chunk. Therefore,\nthe memory and computational complexity of dot-product attention in ViLT can be reduced to O(L2)\ncompared to the complexity of conventional self-attention O((L′×L)2).\nImage locality-sensitive hashing (LSH) attentionAfter ViLT blocks, we obtain local ﬁne-grained\nfeatures of length L′×L. Since the patch size is tiny, the total number of patches can be large, which\nleads to more difﬁcult training than other vision Transformers [4, 13]. On the other hand, the problem\nof ﬁnding nearest neighbors quickly in high-dimensional spaces can be solved by locality-sensitive\nhashing (LSH), which hashes similar input items into the same “buckets” with high probability. In\nNLP, LSH attention [23] is proposed to handle quite long sequence data, which employs locality-\nsensitive hashing (LSH) bucketing approximation and bucket sorting to reduce the computational\ncomplexity of matrix product between query and key in self-attention.\nIn dot-product attention, the softmax activation function pushes the attention weights close to 1 or\n0, which means the attention matrix is typically sparse. The query and key can be approximated by\nlocality-sensitive hashing (LSH) [3] to reduce the computational complexity. Furthermore, through\nbucketing sort, the attention matrix product can be accelerated by a chunk triangular matrix product,\nwhich has been validated by LSH attention [23].\nThe used multi-head image LSH attention can be constructed as\ny′= MSA(LSHAtt(LN(y))) +y, s = MLP(LN(y′)) +y′, (3)\nwhere LSH attention LSHAtt(·) employs angular distance to conduct LSH hashing [3]. The image\nLSH attention reduces the memory and time complexity to O(L′×L log(L′×L)), compared with\nthat of conventional dot-product attention O((L′×L)2). The image LSH attention reduces the\ncomplexity signiﬁcantly because the patch size is tiny and the number of tiny patches L′×L are\nlarge. The image level LSH attention in the second level learns relatively global features from the\nﬁrst level’s local ﬁne-grained features.\nThe hierarchical feature learning from local to global has been validated as an effective principle for\nvision system design [27, 47]. Inspired by hierarchical abstraction in ConvNets [27], we construct\na linear pooling layer which ﬁrstly conducts squeeze then employs linear projection for feature\ndimension reduction. The linear pooling adaptively squeezes the sequence length by 1\n4 .\nReshape: s = [s1,1, ··· , s1,n×n′ ; ··· ; sm×m′,1, ··· , sm×m′,n×n′ ],\nSqueeze: s′= [s′\n1,1, ··· , s′\n1,n×n′/2; ··· ; s′\nm×m′/2,1, ··· , s′\nm×m′/2,n×n′/2],\nLinear: t = [s′\n1,1Et, ··· , s′\n1,n×n′/2Et; ··· ; s′\nm×m′/2,1Et, ··· , s′\nm×m′/2,n×n′/2Et],\n(4)\nwhere s′\ni,j in s′concatenates s2i−1,2j−1, s2i−1,2j, s2i,2j−1 and s2i,2j from s, Et is the linear projec-\ntion matrix to adaptively reduce the number of dimensions after squeeze by half. The reshape is to\nretain the spatial relationship of each patch, and the squeeze reduces the number of patches by 1\n4 and\nenlarges the feature dimensions by four times. The linear pooling layer forces the model to learn\nhigh-level global features in the following layers.\n3.3 Shifted Multi-Head Self-Attention\nConsidering the motion effect of objects, we explicitly construct a shifted multi-head self-attention\n(MSA) for spatio-temporal learning after image chunk self-attention as illustrated in Fig. 1 and Fig. 2.\n5\nFor video classiﬁcation, a special classiﬁcation token ([CLS]) [12] can be prepended into the feature\nsequence. To learn frame-wise spatio-temporal representations, we prepend the [CLS] token to each\nframe. Without loss of generality, we denote the image chunk self-attention feature to be a list of\n[ti,1; ti,2; ··· ; ti,L×L′/4] for the i-th frame. Then, we obtain the input of T frames for shifted MSA\nas\nt′= [t′\n1,1; ··· ; t′\n1,1+L×L′/4; ··· ; t′\nT,1; ··· ; t′\nT,1+L×L′/4]\n= [z1,cls; t1,1; ··· ; t1,L×L′/4; ··· ; zT,cls; tT,1; ··· ; tT,L×L′/4]. (5)\nThe shifted multi-head self-attention explicitly considers the inter-frame motion of objects, which\ncomputes the correlation between the current frame and the previous frame in the attention matrix,\nwhich can be formulated as\nqi\nt,p = LN(t′\nt,p)Wi\nQ, ki\nt,p = LN(t′\nt−1,p)Wi\nK, vi\nt,p = LN(t′\nt,p)Wi\nV , (6)\nwhere Wi\nQ, Wi\nK and Wi\nV are projection matrices for headi, and we employ a cyclic way to calculate\nthe key of the ﬁrst frame t = 1by deﬁning t′\n0,p = t′\nT,p. By concatenating qi\nt,p, ki\nt,p vi\nt,p into matrices\nQi\nt, Ki\nt, Vi\nt along patch location p, the shifted MSA for frame t can be calculated as\nat = Concat(Attention(Q1\nt , K1\nt , V1\nt ), ··· , Attention(Ql\nt, Kl\nt, Vl\nt))WO,\na′\nt = MLP(LN(at)) +at,\n(7)\nFigure 2: Illustration of shifted MSA, which ex-\nplicitly extracts ﬁne-grained motion information\nalong two frames.\nwhere l is the number of heads in multi-head\nself-attention. The shifted self-attention com-\npensates object motion and spatial variances.\nWe explicitly integrate motion shift into self-\nattention, which extracts robust features for\nspatio-temporal learning. The block with al-\nternating layers of image chunk self-attention\nand shifted MSA can be stacked for multiple\ntimes to fully extract effective hierarchical ﬁne-\ngrained features from tiny local patches to the\nwhole clip in our shifted chunk Transformer.\n3.4 Clip Encoder for Global Clip Attention\nTo learn complicated inter-frame relationship\nfrom the extracted frame-level features, we de-\nsign a clip encoder based on a pure-Transformer\nstructure to adaptively learn frame-wise atten-\ntion. To facilitate the video classiﬁcation, we\nprepend a global special classiﬁcation token\n([CLS]) into the frame-level feature sequence.\nIn this module, we employ the classiﬁcation feature a′\nt1 corresponding to zt,cls as the frame-level\nfeature for frame t. To consider frame position, we also employ a standard learnable 1D position\nembedding as each frame position embedding. The clip encoder can be formulated as\nb0 = [bcls; a′\n11E′; ··· ; a′\nT 1E′] +E′\npos, E′\npos ∈R(T+1)×D′\n,\nb′\nm = MSA(LN(bm−1)) +bm−1, bm = MLP(LN(b′\nm)) +b′\nm, m = 1, ··· , M′,\nc = MLP(LN(bM′ 1))\n(8)\nwhere E′is the linear frame embedding matrix, D′is the clip encoder embedding size, M′is the\nnumber of blocks, and bM′ 1 is the clip-level classiﬁcation feature for the classiﬁcation token bcls,\nc is the video classiﬁcation logit for softmax. We use dropout [ 25] for the second last layer and\ncross-entropy loss with label smoothing [34] for training. The clip encoder can be efﬁcient with a\nminimal computational cost in Appendix to achieve a powerful inter-frame representation learning.\n4 Experiment\nWe evaluate our shifted chunk Transformer, denoted as SCT, on ﬁve commonly used action recognition\ndatasets: Kinetics-400 [21], Kinetics-600 [7], Moment-in-Time [33] (Appendix), UCF101 [41] and\n6\nTable 2: Hyper-parameters of data processing and optimization.\nK400 K600 U101 H51 MMT\nFrame Rate 5 5 10 10 8\nFrame Stride 10 10 8 8 10\n#Warmup epochs 2 2 3 4 2\nLearning rate 0.3 0.3 0.25 0.25 0.3\nLabel smoothing 0.1 0.1 0 0 0.3\nDropout 0.2 0.2 0 0 0.2\nTable 3: The model structure of three shifted chunk Transformers.\nModel D MLP\nsize M D ′ Clip MLP\nsize #Heads #Param GFLOPs\nSCT-S 96 384 4 192 768 [4 6 8 8] 18.72M 88.18\nSCT-M 128 512 6 192 768 [4 8 8 8] 33.48M 162.90\nSCT-L 192 768 4 192 768 [4 6 8 8] 59.89M 342.58\nHMDB51 [26]. We adopt ImageNet-21K for the pre-training [39, 11] because of large model capacity\nof SCT. The default patch size for each image token is 4 ×4. In the training, we use a synchronous\nstochastic gradient descent with momentum of 0.9, a cosine annealing schedule [32], and the number\nof epochs of 50. We use batch size of 32, 16 and 8 for SCT-S, SCT-M and SCT-L, respectively. And\nthe frame crop size is set to be 224 ×224. For data augmentation, we randomly select the start frame\nto generate the input clip. In the inference, we extract multiple views from each video and obtain\nthe ﬁnal prediction by averaging the softmax probabilistic scores from these multi-view predictions.\nThe details of initial learning rate, optimization and data processing are shown in Table 2. All the\nexperiments are run on 8 NVIDIA Tesla V100 32 GB GPU cards.\nWe construct three shifted chunk Transformers, SCT-S, SCT-M, and SCT-L, in terms of various\nmodel sizes and computation complexities. We employ four consecutive blocks with alternating one\nimage chunk self-attention and one shifted MSA. The patch embedding size D, MLP dimension\nof these self-attentions, the number of ViLT layers M, the clip encoder embedding size D′, MLP\ndimension of clip encoder, the numbers of heads in ViLT, LSH attention, shifted MSA and clip\nencoder are shown in Table 3. Each image chunk self-attention consists of M layers ViLT followed\nby an image LSH attention and a linear pooling layer to reduce the number of spatial dimensions. We\nuse four-layer clip encoders to obtain the video classiﬁcation results as validated in the Appendix.\nValidating frame feature extractorWe compare the frame encoder of our shifted chunk Trans-\nformer (SCT) with ViT [13] in Table 4. For a fair comparison, we only replace the ViLT with ViT,\nand remain all other components the same. From Table 4, we observe that 1) large models, ViT-L-16\nand SCT-L, yield higher accuracy than base models, ViT-B-16 and SCT-S; 2) ViT with a small patch\nsize achieves better accuracy than ViT with a large patch; 3) SCT-L improves the accuracy of ViT-L\nby 4% while using much less number of parameters and FLOPs. The tiny patch and enforced locality\nprior in ViLT are validated to be effective for spatio-temporal learning.\nTable 4: The effect of frame extractor and the number of tokens on Top-1 accuracy (%).\nMethod Patch size Chunk size #Tokens #Param GFLOPs Kinetics-400\nViT-B-16 16 ×16 - 14 ×14 114.25M 405.06 75.33\nViT-B-16 12 ×12 - 18 ×18 114.25M 665.78 77.12\nViT-B-16 8 ×8 - 28 ×28 114.25M 1603.67 78.95\nViT-L-16 16 ×16 - 14 ×14 328.63M 1413.45 79.15\nSCT-S 4 ×4 7 ×7 8 ×8 18.72M 88.18 78.41\nSCT-M 4 ×4 7 ×7 8 ×8 33.48M 162.90 81.26\nSCT-L 4 ×4 7 ×7 8 ×8 59.89M 342.58 83.02\n7\nTable 5: The effect of the number of shifted MSA layers and shifted frames on Top-1 accuracy (%).\nMethod #Shifted\nMSA\n#Shifted\nframe K400 U101\nSCT-S 0 0 76.91 97.01\nSCT-S 1 1 78.41 98.02\nSCT-S 1 5 77.02 97.15\nSCT-S 2 1 77.45 97.33\nFigure 4: Visualization of the patch and frame attention maps (the second and fourth rows).\nEffect of shifted MSAWe conduct experiments on Kinectis-400 and UCF101 to validate the hyper-\nparameters of the shifted MSA layer, including the number of shifted MSA layers, and the number of\nshifted frames used in the calculation of key in equation (6). All other network conﬁgurations follow\nthe Table 3. The #Shifted MSA of 0 and #Shifted frame of 0 in Table 5 mean that one standard MSA is\nused instead of shifted MSA. From Table 5, we observe that 1) a shifted MSA improves the accuracy\nup to 1.5% compared with the conventional MSA; 2) one layer shifted MSA with the shifted number\nof frames of one yields the best accuracy. The shifted MSA explicitly formulates the motion effect of\nobject by considering the nearby frames, which improves the accuracy for video classiﬁcation. We\nuse one layer shifted MSA with the number of shifted frames of one in our experiment.\nFigure 3: The effect of varying the number of input\nframes and temporal views.\nVarying the number of input frames and\ntemporal views In our experiments so far, we\nhave kept the number of input frames ﬁxed to 24\nacross different datasets. To discuss the effect\nof the number of input frames on video level\ninference accuracy, we validate the number of\ninput frames of 24, 48, 96, and the number of\ntemporal views from 1 to 8. Fig. 3 shows that as\nwe increase the number of frames, the accuracy\nusing a single clip increases, since the network is\nincorporated longer temporal information. How-\never, as the number of used views increases, the\naccuracy difference is reduced. We use the num-\nber of frames of 24 and the number of temporal\nviews of 4 in our experiment.\nPatch and frame attention Our shifted\nchunk Transformer (SCT) can detect ﬁne-grained discriminative regions for each frame in the\nentire clip in Fig. 4. Speciﬁcally, we average attention weights of the shifted MSA across all heads\nand then recursively multiply the weight matrices of all layers [1], which accounts for the attentions\nthrough all layers. The designed framework of SCT leads to an easy diagnosis and explanation for\n8\nTable 6: Top-1 and Top-5 accuracy (%) comparisons to state-of-the-art approaches on Kinectics-400.\nMethod TFLOPs ×Views #Param Runtime (s) Top1 Top5\nTEA [28] 0.07 ×10×3 - - 76.1 92.5\nI3D NL [51] - 54M - 77.7 93.3\nCorrNet-101 [48] 0.224 ×10×3 - - 79.2 -\nip-CSN-152 [44] 0.109 ×10×3 33M - 79.2 93.8\nSlowFast [16] 0.234 ×10×3 60M - 79.8 93.9\nX3D-XXL [15] 0.194 ×10×3 20M 0.176 80.4 94.6\nTimeSformer [5] 2.38 ×1×3 121M 0.475 80.7 94.7\nMViT-B 64×3 [14] 0.455 ×3×3 37M 0.153 81.2 95.1\nViViT-L [4] 399.2 ×4×3 89M - 81.3 94.7\nSCT-S 0.088 ×4×3 19M 0.051 78.4 93.8\nSCT-M 0.163 ×4×3 33M 0.072 81.3 94.5\nSCT-L 0.343 ×4×3 60M 0.106 83.0 95.4\nTable 7: Classiﬁcation accuracy (%) comparisons to state-of-the-art approaches on Kinectics-600,\nUCF101 (3 splits) and HMDB51 (3 splits). ‘K400’ denotes pretraining on Kinetics-400 and ImageNet.\n‘K600’ denotes pretraining on Kinetics-600 and ImageNet.\nMethod Views #Para Top1 Top5\nAttenNAS [50] - - 79.8 94.4\nLGD-3D [37] - - 81.5 95.6\nSlowFast [16] 10 ×3 60M 81.8 95.1\nX3D-XL [15] 10 ×3 11M 81.9 95.5\nTimeSformer [5] 1 ×3 121M 82.4 96.0\nViViT-L [4] 4 ×3 89M 83.0 95.7\nSCT-S 4 ×3 19M 77.5 93.1\nSCT-M 4 ×3 33M 81.7 95.5\nSCT-L 4 ×3 60M 84.3 96.3\nMethod Pretrain U101 H51\nI3D [8] K400 95.4 74.5\nResNeXt [18] K400 94.5 70.2\nR(2+1)D [45] K400 96.8 74.5\nS3D-G [53] K400 96.8 75.9\nLGD-3D [37] K600 97.0 75.7\nSCT-S ImageNet 98.0 76.5\nSCT-L ImageNet 97.7 81.4\nSCT-S K400 98.3 81.5\nSCT-M K400 98.5 83.2\nSCT-L K400 98.7 84.6\nthe prediction, which potentially makes SCT applicable to various critical ﬁelds, e.g., healthcare and\nautonomous driving.\nComparison to state-of-the-art approaches We compare our shifted chunk Transformer (SCT)\nto the current state-of-the-art approaches based on the best hyper-parameters validated in the previous\nablation studies. We obtain the results of previous state-of-the-art approaches from their papers. We\nobtain the actual runtime (s) in one single NVIDIA V100 16GB GPU by averaging 50 inferences\nwith batch size of one. In Kinetics-400 and Kinetics-600, we initialize our ViLT and LSH attention\ntrained on ImageNet-21K.\nOur shifted chunk Transformers (SCT) surpass previous state-of-the-art approaches including both\nrecent Transformer based video classiﬁcation and previous deep ConvNets based methods by 2.7%,\n1.3%, 1.7% and 8.9% on Kinectis-400, Kinectic-600, UCF101 and HMDB51 in Table 6-7 based on\nRGB frames, respectively. The local scheme and LSH approximation in image chunk self-attention\nenables to use patches of a small size. Because of the efﬁcient model design, SCT achieves the best\naccuracy even only using pretraining on the ImageNet on UCF101 and HMDB51. Besides the higher\naction recognition accuracy, the SCT employs less number of parameters and FLOPs than ViViT\nbecause we employ less number of channels and our SCT is effective for spatio-temporal learning.\n5 Conclusion\nIn this work, we proposed a new spatio-temporal learning called shifted chunk Transformer inspired\nby the recent success of vision Transformer in image classiﬁcation. However, the current pure-\nTransformer based spatio-temporal learning is limited by computational efﬁciency and feature\n9\nrobustness. To address these challenges, we propose several efﬁcient and powerful components for\nspatio-temporal Transformer, which is able to learn ﬁne-grained features from a tiny image patch\nand model complicated spatio-temporal dependencies. We construct an image chunk self-attention\nwhich leverages locality-sensitive hashing to efﬁciently capture ﬁne-grained local representation with\na relatively low computation cost. Our shifted self-attention can effectively model complicated inter-\nframe variances. Furthermore, we build a clip encoder based on pure-Transformer for frame-wise\nattention and long-term inter-frame dependency modeling. We conduct thorough ablation studies\nto validate each component and hyper-parameters in our shifted chunk Transformer. It outperforms\nprevious state-of-the-art approaches including both pure-Transformer architectures and deep 3D\nconvolutional networks on various datasets in terms of accuracy and efﬁciency.\n6 Acknowledgement\nThis work is supported by Kuaishou Technology. No external funding was received for this work.\nMoreover, we would like to thank Hang Shang for insightful discussions.\nReferences\n[1] Samira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics, pages 4190–4197,\n2020.\n[2] Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and\nBoqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video,\naudio and text. arXiv preprint arXiv:2104.11178, 2021.\n[3] Alexandr Andoni, Piotr Indyk, TMM Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt.\nPractical and optimal lsh for angular distance. In Advances in Neural Information Processing\nSystems (NIPS 2015), pages 1225–1233. Curran Associates, 2015.\n[4] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu ˇci´c, and Cordelia\nSchmid. Vivit: A video vision transformer. arXiv preprint arXiv:2103.15691, 2021.\n[5] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for\nvideo understanding? arXiv preprint arXiv:2102.05095, 2021.\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In European Conference on\nComputer Vision, pages 213–229. Springer, 2020.\n[7] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A\nshort note about kinetics-600. arXiv preprint arXiv:1808.01340, 2018.\n[8] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the\nkinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 6299–6308, 2017.\n[9] R Christoph and Feichtenhofer Axel Pinz. Spatiotemporal residual networks for video action\nrecognition. Advances in Neural Information Processing Systems, pages 3468–3476, 2016.\n[10] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. Do we really need explicit\nposition encodings for vision transformers? arXiv preprint arXiv:2102.10882, 2021.\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern\nRecognition, pages 248–255. Ieee, 2009.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.\n10\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. In International\nConference on Learning Representations, 2021.\n[14] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and\nChristoph Feichtenhofer. Multiscale vision transformers. arXiv preprint arXiv:2104.11227,\n2021.\n[15] Christoph Feichtenhofer. X3d: Expanding architectures for efﬁcient video recognition. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n203–213, 2020.\n[16] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for\nvideo recognition. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 6202–6211, 2019.\n[17] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne\nWestphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag,\net al. The\" something something\" video database for learning and evaluating visual common\nsense. In Proceedings of the IEEE International Conference on Computer Vision , pages\n5842–5850, 2017.\n[18] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the\nhistory of 2d cnns and imagenet? In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 6546–6555, 2018.\n[19] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415, 2016.\n[20] Yufang Huang, Wentao Zhu, Deyi Xiong, Yiye Zhang, Changjian Hu, and Feiyu Xu. Cycle-\nconsistent adversarial autoencoders for unsupervised text style transfer. In Proceedings of the\n28th International Conference on Computational Linguistics, pages 2213–2223, 2020.\n[21] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-\nnarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human\naction video dataset. arXiv preprint arXiv:1705.06950, 2017.\n[22] Qiuhong Ke, Mario Fritz, and Bernt Schiele. Time-conditioned action anticipation in one shot.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 9925–9934, 2019.\n[23] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In\nInternational Conference on Learning Representations, 2020.\n[24] Matej Kristan, Jiri Matas, Ales Leonardis, Michael Felsberg, Roman Pﬂugfelder, Joni-Kristian\nKamarainen, Luka Cehovin Zajc, Ondrej Drbohlav, Alan Lukezic, Amanda Berg, et al. The\nseventh visual object tracking vot2019 challenge results. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision Workshops, pages 0–0, 2019.\n[25] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. Advances in Neural Information Processing Systems, 25:1097–\n1105, 2012.\n[26] Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso Poggio, and Thomas Serre.\nHmdb: a large video database for human motion recognition. In International Conference on\nComputer Vision, pages 2556–2563. IEEE, 2011.\n[27] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning\napplied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[28] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal\nexcitation and aggregation for action recognition. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 909–918, 2020.\n11\n[29] Yingwei Li, Yi Li, and Nuno Vasconcelos. Resound: Towards action recognition without\nrepresentation bias. In Proceedings of the European Conference on Computer Vision (ECCV),\npages 513–528, 2018.\n[30] Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang. Spatio-temporal lstm with trust gates for\n3d human action recognition. In European Conference on Computer Vision, pages 816–833.\nSpringer, 2016.\n[31] Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, and Alex C Kot. Global context-aware attention\nlstm networks for 3d action recognition. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 1647–1656, 2017.\n[32] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In\nInternational Conference on Learning Representations, 2017.\n[33] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal,\nTom Yan, Lisa Brown, Quanfu Fan, Dan Gutfreund, Carl V ondrick, et al. Moments in time\ndataset: one million videos for event understanding. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 42(2):502–508, 2019.\n[34] Rafael Müller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help? In\nAdvances in Neural Information Processing Systems, 2019.\n[35] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku,\nand Dustin Tran. Image transformer. In International Conference on Machine Learning, pages\n4055–4064. PMLR, 2018.\n[36] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d\nresidual networks. In proceedings of the IEEE International Conference on Computer Vision,\npages 5533–5541, 2017.\n[37] Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xinmei Tian, and Tao Mei. Learning spatio-temporal\nrepresentation with local and global diffusion. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 12056–12065, 2019.\n[38] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and\nJonathon Shlens. Stand-alone self-attention in vision models. InAdvances in Neural Information\nProcessing Systems, 2019.\n[39] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining\nfor the masses. arXiv preprint arXiv:2104.10972, 2021.\n[40] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recog-\nnition in videos. In Proceedings of the 27th International Conference on Neural Information\nProcessing Systems-Volume 1, pages 568–576, 2014.\n[41] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human\nactions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\n[42] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervé Jégou. Training data-efﬁcient image transformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\n[43] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spa-\ntiotemporal features with 3d convolutional networks. In Proceedings of the IEEE International\nConference on Computer Vision, pages 4489–4497, 2015.\n[44] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. Video classiﬁcation with channel-\nseparated convolutional networks. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5552–5561, 2019.\n[45] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A\ncloser look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 6450–6459, 2018.\n12\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\ntion Processing Systems, volume 30, pages 5998–6008, 2017.\n[47] Andrea Vedaldi and Brian Fulkerson. Vlfeat: An open and portable library of computer vision\nalgorithms. In Proceedings of the 18th ACM International Conference on Multimedia, pages\n1469–1472, 2010.\n[48] Heng Wang, Du Tran, Lorenzo Torresani, and Matt Feiszli. Video modeling with correlation\nnetworks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 352–361, 2020.\n[49] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction\nwithout convolutions. arXiv preprint arXiv:2102.12122, 2021.\n[50] Xiaofang Wang, Xuehan Xiong, Maxim Neumann, AJ Piergiovanni, Michael S Ryoo, Anelia\nAngelova, Kris M Kitani, and Wei Hua. Attentionnas: Spatiotemporal attention cell search for\nvideo classiﬁcation. In European Conference on Computer Vision, pages 449–465. Springer,\n2020.\n[51] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n7794–7803, 2018.\n[52] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang.\nCvt: Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\n[53] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spa-\ntiotemporal feature learning: Speed-accuracy trade-offs in video classiﬁcation. In Proceedings\nof the European Conference on Computer Vision (ECCV), pages 305–321, 2018.\n[54] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei. End-to-end learning of action\ndetection from frame glimpses in videos. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 2678–2687, 2016.\n[55] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2021.\n[56] Wentao Zhu, Tianlong Kong, Shun Lu, Jixiang Li, Dawei Zhang, Feng Deng, Xiaorui Wang,\nSen Yang, and Ji Liu. Speechnas: Towards better trade-off between latency and accuracy for\nlarge-scale speaker veriﬁcation. In ASRU, 2021.\n[57] Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng, Yanghao Li, Li Shen, and Xiaohui Xie.\nCo-occurrence feature learning for skeleton based action recognition using regularized deep\nlstm networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30,\n2016.\n13\n7 Appendix\nVarying the number of clip encoder layersSee Table 8. From the table, adding two more layers\nof clip encoder only increases the number of parameters by 1M. The clip encoder using four layers\nyields better accuracy. We use four layered clip encoder in our experiment.\nTable 8: Effect of clip encoder on the Top-1 accuracy (%)\nName #Layer #Param FLOPs K400 U101\nSCT-S 2 17.67M 86.14G 76.32 97.35\nSCT-S 4 18.72M 88.18G 78.41 98.02\nPretrained model analysis See Table 9. Pretraining on large amount of data yields better top-1\naccuracy.\nTable 9: Pretrain Model Result\nUCF101\nName Pretrain Type Top-1 Acc\nSCT-S ImageNet 98.02%\nSCT-M ImageNet 97.45 %\nSCT-L ImageNet 97.70 %\nSCT-S ImageNet+Kinetics-400 98.33 %\nSCT-M ImageNet+Kinetics-400 98.45 %\nSCT-L ImageNet+Kinetics-400 98.71 %\nHMDB51\nSCT-S ImageNet 76.52 %\nSCT-M ImageNet 78.31 %\nSCT-L ImageNet 81.42 %\nSCT-S ImageNet+Kinetics-400 81.54 %\nSCT-M ImageNet+Kinetics-400 83.22 %\nSCT-L ImageNet+Kinetics-400 84.61 %\nResults on Moments in Time [33]See Table 10. We achieve comparable accuracy with much less\nnumber of parameters and GFLOPs on Moments in Time [33].\nTable 10: Results on Moments in Time [33]\nMethod #Param GFLOPs Top-1 (%) Top-5 (%)\nI3D [8] 25M - 29.5 56.1\nViViT-L [4] 88.9M 1446 38.0 64.9\nV ATT-L [2] 306.1M 29800 41.1 67.7\nSCT-M 33.48M 162.9 36.8 61.2\nSCT-L 59.89M 342.6 37.3 65.1\nAblation study on ViLTWe further compare the ViLT with convolution variants and one Trans-\nformer variant, i.e., LSH attention. We compare ViLT (78.4%, 98.3%) with various convolution\nvariants in SCT-S on the Kinetics-400 and UCF101 datasets. We have convolution (73.9%, 94.9%),\nconvolution + bn (74.3%, 95.0%), and residual convolution block (75.1%, 95.8%), which sufﬁciently\ndemonstrates the effectiveness of our ViLT. From the perspective of receptive ﬁeld size, without\npooling, a four layered ConvNet with 3x3 kernel has receptive ﬁeld size of 9x9, and our ViLT is able\n14\nto fully model the information from 28x28 of each chunk. Replacing ViLT with image LSH attention\nobtains 76.6% and 96.1% Top-1 accuracy, because the LSH self-attention reduces the computation\nby approximating the dense matrix with an upper triangular matrix.\nAblation study on image LSH attentionTo conduct ablation studies for image LSH attention, we\na) remove the ViLT and obtain 63.2% and 85.2% Top-1 accuracy on Kinectics-400 and UCF101,\nbecause it fails to capture low-level ﬁne grained features, b) replace LSH attention with ConvNets and\nobtain convolution (75.3%, 96.6%), convolution + bn (75.6%, 96.9%), and residual convolution block\n(76.9%, 97.0%), because ConvNets have limited receptive ﬁeld size compared with Transformers, c)\nremove the LSH attention in SCT-S and achieve (76.2%, 96.5%) Top-1 accuracy. The global attention\nbrought by LSH attention in each frame helps spatio-temporal learning.\nAblation study on shifted MSACompared with the conventional self-attention only modeling\nthe intra-frame patches (space attention), or divided space-time attention only modeling the same\nposition along different frames which cannot handle big motions, our shifted self-attention explicitly\nmodels the motion and focuses the main objects in the video. We also validate the effectiveness of\nour shifted attention through ablation study and comparison with previous state-of-the-art methods.\nEmpirically, we compare the shifted MSA with various attentions, i.e., space attention (conventional\nself-attention, 77.02%), time attention [5] (77.62%), and concatenated feature from space and time\nattentions [5] (77.35%) with ﬁxed other components in SCT-S on the Kinetics-400 dataset, which\ndemonstrates the advantages of explicitly effective motion modeling in the shifted attention. The\nattention map visualization in Fig. 4 also veriﬁes the effective motion capture of the main object in\nthe video.\nResults on SSv2 and Diving-48 We further conduct experiments on Something-Something-\nV2 [17] and Diving-48 [ 29], which are more dynamic datasets and rely heavily on the temporal\ndimension. Our SCT-L with Kinetics-600 pretrained model obtain 68.1% and 81.9% accuracy on the\ntwo datasets, respectively, compared with TEA [28] (65.1%, N/A), SlowFast [16] (61.7%, 77.6%),\nViViT-L/16x2 [4] (65.4%, N/A), TimeSformer-L [5] (62.4%, 81.0%), and MViT-B, 32x3 [14] (67.8%,\nN/A). Our SCT-L achieves the best Top-1 accuracy on the two datasets.\nHyper-parameters of shifted MSAIn our experiment, the frame rate of each input clip is varied\nfrom 5-10, which is 0.1-0.3s. From the perspective of human vision system, the typical duration\nof persistence of vision is 0.1-0.4s. The experiment validates the best numbers of shifted MSA\nand shifted frames are 1, which is consistent with our vision system and the bigger number of\nshifted frames could misses the motion information for some actions. From the perspective of model\ncomplexity, we have the multi-layer clip encoder after shifted MSA to speciﬁcally model complicated\ninter-frame dependencies. The shifted MSA is forced to learn ﬁne-grained motion information. In\nthe future work, developing multi-scale shifted MSA is an interesting topic.\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7683054804801941
    },
    {
      "name": "Transformer",
      "score": 0.7086788415908813
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6419126987457275
    },
    {
      "name": "Feature learning",
      "score": 0.5233439803123474
    },
    {
      "name": "Encoder",
      "score": 0.5101823806762695
    },
    {
      "name": "Segmentation",
      "score": 0.49455878138542175
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4137086868286133
    },
    {
      "name": "Machine learning",
      "score": 0.33622124791145325
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 7
}