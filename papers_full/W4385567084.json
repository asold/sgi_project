{
  "title": "Continual Training of Language Models for Few-Shot Learning",
  "url": "https://openalex.org/W4385567084",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2798848506",
      "name": "Zixuan Ke",
      "affiliations": [
        "University of Illinois Chicago",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2110839940",
      "name": "Haowei Lin",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2412288611",
      "name": "Yijia Shao",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2116153423",
      "name": "Hu Xu",
      "affiliations": [
        "Google (United States)",
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A1978909455",
      "name": "Lei Shu",
      "affiliations": [
        "Google (United States)",
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A1973602695",
      "name": "Bing Liu",
      "affiliations": [
        "University of Illinois Chicago",
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2554863749",
    "https://openalex.org/W2886342729",
    "https://openalex.org/W2963559848",
    "https://openalex.org/W2939137134",
    "https://openalex.org/W4225620948",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2785567898",
    "https://openalex.org/W3205616434",
    "https://openalex.org/W4295883599",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W4309443149",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4312744085",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3212560468",
    "https://openalex.org/W2808556605",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2971146255",
    "https://openalex.org/W2951796662",
    "https://openalex.org/W2619508703",
    "https://openalex.org/W4287814827",
    "https://openalex.org/W3019510943",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W3099543642",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3161024824",
    "https://openalex.org/W3010801434",
    "https://openalex.org/W4285176332",
    "https://openalex.org/W3023133114",
    "https://openalex.org/W2952357537",
    "https://openalex.org/W4309443145",
    "https://openalex.org/W2962783425",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W3100156920",
    "https://openalex.org/W4300485654",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2583761661",
    "https://openalex.org/W4285169833",
    "https://openalex.org/W2964189064",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W4205340316",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W3197680083",
    "https://openalex.org/W2962851930",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2997591266",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W3098170909",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3190540921",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2856961199",
    "https://openalex.org/W4286981949",
    "https://openalex.org/W2963813679",
    "https://openalex.org/W3171057731"
  ],
  "abstract": "Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications. Adapting or posttraining an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills. The goal is to improve the few-shot end-task learning in these domains. The resulting system is called CPT (Continual PostTraining), which to our knowledge, is the first continual post-training system. Experimental results verify its effectiveness.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10205–10216\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nContinual Training of Language Models for Few-Shot Learning\nZixuan Ke1, Haowei Lin2, Yijia Shao2, Hu Xu1, Lei Shu1∗ and Bing Liu1\n1Department of Computer Science, University of Illinois at Chicago\n2Wangxuan Institute of Computer Technology, Peking University\n1{zke4,hxu48,liub}@uic.edu\n2{linhaowei, shaoyj}@pku.edu.cn\nAbstract\nRecent work on applying large language mod-\nels (LMs) achieves impressive performance\nin many NLP applications. Adapting or post-\ntraining an LM using an unlabeled domain cor-\npus can produce even better performance for\nend-tasks in the domain. This paper proposes\nthe problem of continually extending an LM\nby incrementally post-train the LM with a se-\nquence of unlabeled domain corpora to expand\nits knowledge without forgetting its previous\nskills. The goal is to improve the few-shot\nend-task learning in these domains. The re-\nsulting system is called CPT (Continual Post-\nTraining), which to our knowledge, is the first\ncontinual post-training system. Experimental\nresults verify its effectiveness.1\n1 Introduction\nRecent work has shown that large LMs have the\nability to perform few-shot (or even zero-shot)\nlearning well (Brown et al., 2020b; Rae et al.,\n2021; Smith et al., 2022). Post-training (a.k.a.,\ndomain-adaptive pre-training or pre-finetuning) an\nLM with a large unlabeled domain corpus before\nend-task fine-tuning in the domain achieves better\nresults (Xu et al., 2019; Gururangan et al., 2020a)\nthan directly fine-tuning the LM. This paper goes\na step further to study the problem of improving\nan LM’s ability to handle new and ever emerging\ndomains. For this, one needs to continually post-\ntrain the LM with a sequence of domains. A key\nissue associated with this problem is catastrophic\nforgetting (CF).2 This paper thus investigates how\nto continually extend the LM’s knowledge with-\nout suffering from CF. From a broader perspective,\nsince training a large LM from scratch is extremely\n∗Now at Google Research leishu@google.com\n1https://github.com/UIC-Liu-Lab/CPT\n2CF means that learning a new task/domain may need to\nmodify the existing network, which degrades the performance\nof previous tasks/domains (McCloskey and Cohen, 1989).\nexpensive and computation intensive, incremen-\ntally updating the LM with the latest language data\nreflecting the ever changing development of the\nlanguage itself, social events and the knowledge\nfrom different fields is becoming more and more\ncritical. As humans are very effective at incremen-\ntal learning, if we can imitate this human capability\nwith little or no forgetting, we will be pushing the\nAI research forward significantly.\nThe proposed system, called CPT, is a continual\nlearning (CL) system for post-training. Starting\nfrom a pre-trained LM (e.g., RoBERTa (Liu et al.,\n2019b)), it incrementally post-trains the LM with a\nsequence of domains using their unlabeled corpora.\nOnce a task (a domain in our case) 3 is trained,\nits data is no longer accessible. At any time, the\nresulting continually post-trained LM can be used\nby end-tasks in the trained domains. This is in\nthe task-incremental learning (TIL) setting of CL,\nwhere the task id (domain id in our case) is pro-\nvided when the learned model of a task needs to\nbe used later (the use of domain id is discussed\nin Sec. 2.1).4 This paper proposes an effective ap-\nproach called CPT and focuses on the challenging\nand practical scenario offew-shot end-task learning\nafter post-training a sequence of domains.\nContinual post-training is different from conven-\ntional CL (Chen and Liu, 2018). The key difference\nis that in conventional CL, each task is an end-task,\nbut in our case the end-task involves fine-tuning\nthe continual post-trained LM (called p-LM). This\ncauses major forgetting, which we call the catas-\ntrophic butterfly effect (CBE) and does not happen\nin conventional CL. Our proposed system, CPT,\ncan solve both CF and CBE, based on a novel hard\nmasking mechanism (Sec. 2.2) and can achieve\nno forgetting. As shown in Sec. 3.3, naively ap-\n3We will use the termdomain in this paper to be consistent\nwith the post-training literature\n4CL has two other settings: class-incremental learning and\ndomain-incremental learning (van de Ven and Tolias, 2019).\n10205\nplied existing CL systems cannot effectively pre-\nvent CF (even though some existing techniques\nhave shown almost perfect CF prevention ability in\nconventional CL).\nExperiments in 4 domains and their correspond-\ning end-tasks demonstrate the effectiveness of the\nproposed CPT system.\nRelated Work. Overcoming CF is a major goal\nof CL (Chen and Liu, 2018). There are many ex-\nisting approaches, e.g., regularization-based ap-\nproaches (Kirkpatrick et al., 2016; Seff et al., 2017),\nreplay-based approaches (Rebuffi et al., 2017;\nLopez-Paz and Ranzato, 2017) and parameter iso-\nlation based approaches (Serrà et al., 2018; Fer-\nnando et al., 2017). Our CPT is based on parameter\nisolation and uses masks in continual post-training.\nRecently, CL has drawn attention in NLP. It has\nbeen used for slot filling (Shen et al., 2019), lan-\nguage learning (Li et al., 2019), sentence embed-\nding (Liu et al., 2019a), translation (Khayrallah\net al., 2018), cross-lingual modeling (Liu et al.,\n2020b), question answering (Greco et al., 2019)\nand text classification (Ke et al., 2021a,b; Sun et al.,\n2020; Huang et al., 2021; Chuang et al., 2020;\nMehta et al., 2021; Madotto et al., 2020). How-\never, none of them tries to improve an LM.\nCPT is closely related to ELLE (Qin et al., 2022),\nwhich does continual pre-training. The key differ-\nence is that ELLE starts from random initialization,\nwhile our CPT starts from a pre-trained LM. We\ntried to adapt ELLE for continual post-training by\nlearning from a pre-trained RoBERTa but it fails to\nconverge. This also indicates it is non-trivial to do\nwell in our setting. Readers can refer to Appendix\nA for a full coverage of the related work.\n2 Proposed CPT System\nCPT continually post-trains RoBERTa (Liu et al.,\n2019b). This is achieved by two continual learn-\ning plug-in (called CL-plugin) modules inserted\ninto each transformer layer of RoBERTa. CL-\nplugin is inspired by adapters in (Houlsby et al.,\n2019). While adapters can isolate different tasks,\none needs to allocate a new adapter for each task\nand no knowledge can be shared among different\ntasks’ adapters. The CL-plugin, however, is a CL\nsystem that learns a sequence of tasks with adapters\nshared by all domains. Figure 1 gives the CPT ar-\nchitecture with two CL-plugins added to RoBERTa.\nSequential vs. Parallel CL-plugin. Instead of\nfollowing the original sequential adapter (Houlsby\n(A) Continual Post-trainingMLM Head\nHidden States\nAttention\nAdd & Layer Norm\nFFN\nCL-Plugin\nCL-Plugin\nAdd & Layer Norm\n+\n+\n× L\nHidden States\nAttention\nAdd & Layer Norm\nFFN\nCL-Plugin\nCL-Plugin\nAdd & Layer Norm\n+\n+\n× L\nClassification Head (B) Individual Fine-tuning\nt\nCL-Plugin\n01 0\n0 1 01\nt\nCL-Plugin\n0 1 0 1 0\n1 0 0 0 0\nFixed Modules\n0 0\n0\nFigure 1: Architecture of CPT, which has two CL-\nplugins inserted in the transformer layers of RoBERTa\nin a parallel manner (FFN: feed-forward network). (A)\nCPT for continual post-training . It uses a masked\nlanguage model (MLM) head for unsupervised post-\ntraining of the CL-plugins only. (B) CPT for individual\nfine-tuning. CPT is evaluated by the corresponding in-\ndividual end-task performance of all post-trained tasks.\nEach CL-plugin has numbers and colors indicating its\nmasks and is illustrated in Appendix B.\net al., 2019), CL-plugin adopts the parallel adapter\nidea in (He et al., 2021). The difference is that the\nformer inserts an adapter after the FFN/attention\nlayer while the latter inserts it before FFN/attention\nlayer (see Fig. 1). We choose the parallel version\nas it performs better (see Sec. 3.3).\nIn post-training, only the two CL-plugins are\ntrained. The components of the original pre-trained\nRoBERTa are fixed. In end-task fine-tuning, all\ncomponents are trainable. A CL-plugin is a two-\nlayer fully connected network with a task mask\nmechanism. It takes two inputs: (1) hidden states\nh(t) from the feed-forward layer in a transformer\nlayer and (2) task ID tneeded by task incremen-\ntal learning (TIL). Inside a CL-plugin, task masks\n(TMs), which indicate task- specific neurons, are\nused to deal with CF. Since TMs is differentiable,\nthe whole CPT can be trained end-to-end.\n2.1 Task Masks (TMs)\nIn each layer of a CL-plugin, task masks are used\nto protect those neurons that are important for pre-\n10206\nvious tasks to overcome CF. The masks basically\nforbid gradient updates to those neurons during\nbackpropagation in learning a new task. Note that\na task is also a domain in our case.\nLearning a new task/domain consists of two\nmain steps: (1) apply the mask in each layer for\neach old task to block off the gradient flow to pro-\ntect the model for the old task, and (2) learn domain\ntand its masks for future use. We present (2) first.\nLearning Task Masks for Overcoming CF.In\nlearning each task t, a mask (a “soft” binary mask)\nm(t)\nl is trained for the task at each layer l in CL-\nplugin, indicating the neurons that are important\nfor the task. We borrow the hard attention idea in\n(Serrà et al., 2018) and leverage the task ID em-\nbedding to train the mask. For a task ID t, its em-\nbedding e(t)\nl consists of differentiable deterministic\nparameters that can be learned together with other\nparts of the network. To generate the task mask\nm(t)\nl from e(t)\nl , Sigmoid is used as a pseudo-gate\n(mask) function. m(t)\nl is computed with\nm(t)\nl = σ(e(t)\nl /τ), (1)\nwhere τis a temperature variable, linearly annealed\nfrom 1 to τmin (a small positive value).\nIn the forward pass, given the output of each\nlayer l, k(t)\nl , we element-wise multiply mask m(t)\nl ,\no(t)\nl = k(t)\nl ⊗m(t)\nl . (2)\nThe masked output o(t)\nl of the last layer in CL-\nplugin is fed to the next layer of the RoBERTa with\na skip-connection. After learning task t, the final\nm(t)\nl is saved and added to the set {m(t)\nl }.\nApplying Task Masks. Before learning a new\ntask t, we first accumulate and set the masksm(iprev)\nl\non the neurons in each layer lfor all old tasks iprev\nso that in backpropagation, the gradient g(t)\nl for\ntask twill not flow to these neurons. Since m(iprev)\nl\nis pseudo binary, we use max-pooling to achieve\nthe accumulation and condition the gradient:\ng\n′(t)\nl = g(t)\nl ⊗(1 −(MaxPool({m(iprev)\nl }))). (3)\nThose gradients corresponding to the 1 entries in\nMaxPool({m(iprev)\nl }) are set to 0 (to block off gra-\ndient flow) while the others remain unchanged. In\nthis way, neurons in old tasks are protected. Note\nthat we expand (copy) the vector m(ta)\nl to match\nthe dimensions of g(t)\nl .\n2.2 Catastrophic Butterfly Effect in\nFine-tuning\nTo perform an end-task in a post-trained domain,\nwe fine-tune the mask-protected model of the do-\nmain, which is indicated by the task/domain id. The\nfine-tuning uses the corresponding domain neurons\nfor the specific end-task by setting τ = τmin and\ncondition the output via Eq. 2. With the masks,\nthere should be no forgetting for continual post-\ntraining and the end-task fine-tuning performance\nshould be similar to post-train each domain sepa-\nrately. However, we found that this is not the case.5\nOur investigation found that the problem is due to\nthe pseudo-gate function in Eq. 1. No matter how\nsmall τ is, Eq. 1 can only gives us a mask almost 0\n(or 1). This causes the following: (1) During post-\ntraining, the gradients for used neurons in Eq. 3 are\nnot exactly 0 but a very small number. (2) During\nfine-tuning, we cannot make use of the correspond-\ning neurons for the specific end-task by simply\nsetting τ = τmin. The small change in the neu-\nrons for old domains during post-training caused\nby (1) is neglect-able in conventional CL because\nin conventional CL we evaluate the model using\ntest sets and no weights update involved. How-\never, in CPT, the end-task needs to fine-tune the\ncontinually post-trained LM model (p-LM), which\ninvolves weight updating. A small change to the\np-LM during continual post-training can result in a\ndifferent initialization for the end-task fine-tuning\nand give totally different fine-tuning results. We\ncall this butterfly effect inspired by the term indi-\ncating a small state change in nature (e.g., the flap\nof a butterfly’s wings in Brazil) can result in large\ndifferences in a later state (e.g., a tornado in Texas).\nWe propose a simple method to solve it, i.e.,\nadding a threshold θto the m(t)\nl to make it a hard\nbinary mask,\nm(t)\nl =\n{\n1, m(t)\nl >θ,\n0, m(t)\nl <θ.\n(4)\nWe then apply it to Eq. 3 in gradient manipulation\nand Eq. 2 in end-task fine-tuning. θcan be easily\nset (we use 0.5) since Eq. 1 already gives a pseudo-\nbinary mask. Note that this has almost no effect\non post-training as it is used to block the backward\n5For example, fine-tuning an end restaurant sentiment clas-\nsification task achieves macro-F1 (MF1) of 0.64 right after\npost-training the restaurant domain but its fine-tuning MF1\ndrops to 0.44 after post-training three more domains.\n10207\npass gradient flow during post-training and select\nthe corresponding neurons during fine-tuning.\n3 Experiments\nThe proposed paradigm uses a different evaluation\nfrom that of conventional continual learning (CL).\nAfter unsupervised continual post-training of an\nLM (RoBERTa in our case) with a sequence of\ndomains, the resulting p-LM is used to fine-tune\nan end few-shot classification task from any post-\ntrained domain. There is no CL during end-task\nfine-tuning. Each fine-tuning task is done sepa-\nrately.\n3.1 Datasets and Baselines\nDatasets: We use 4 unlabeled domain datasets:\nYelp Restaurant (Xu et al., 2019), AI Papers (Lo\net al., 2020), ACL Papers (Lo et al., 2020) and AG-\nNews (Zhang et al., 2015) and their 4 corresponding\nend-task classification datasets.6\nBaselines. Since no existing method can perform\nour task, we use 6 non-CL and 7 adapted CL meth-\nods as our baselines. The non-CL baselines in-\nclude (1) RoBERTa and (2) Adapter where we\ndirectly fine-tune the pre-trained model or adpater\n(without any post-training); (3) RoBERTa-ONE,\n(4) Adapter-ONE and (5) Prompt-ONE, where\nwe build a model for each task using a separate\nnetwork. It has no knowledge transfer or CF. (6)\nDEMIX (Gururangan et al., 2021) trains a sepa-\nrate adapter for each task and initializes the adapter\nfrom its most similar previous task adapter. The 7\nadapted CL baselines include (7) RoBERTa-NCL\nand (8) Adapter-NCL, where we post-train the do-\nmains one by one with no mechanism to deal with\nCF/transfer. Other are state-of-the-art CL baselines\nand we adapt them for continual post-training.7\n3.2 Implementation Details\nArchitecture. We adopt RoBERTaBASE as our\nbackbone LM. A masked language model head is\napplied for post-training. The fine-tuning follows\nthe standard practice (Devlin et al., 2019), where\nwe pass the final layer </s> token representation\n6These are popularly used in related works. Details of the\ndatasets are given in Appendix C. We conduct experiments\nusing few-shot learning end-tasks. Following (Gu et al., 2021),\nwe use 32 training samples for Restaurant and AGNews, 48\ntraining samples for ACL and 56 training samples for AI due\nto different numbers of classes in each dataset.\n7Readers can refer to Appendix D for the detailed of each\nof these baselines.\nto a task-specific feed-forward layer for prediction.\nThe feed-forward layer with softmax output is used\nas the classification heads, together with the cate-\ngorical cross-entropy loss. Note that for the aspect\nsentiment classification task (see Table 3), we adopt\nthe ASC formulation in (Xu et al., 2019), where\nthe aspect (e.g., “sound”) and review sentence (e.g.,\n“The sound is great”) are concatenated via </s>.\nHyperparameters. Unless otherwise stated, the\nsame hyper-parameters are used in all experiments.\nWe use 0.0025 for τmin in Eq. 1 and θis set to 0.5\nin Eq. 4 in the main paper. As shown in Figure\n1, there are two CL-plugins for each Transformer\nlayer (one at the bottom in parallel with attention\nand one at the top in parallel with FFN). We search\nthe CL-plugin size within {128, 256, 512, 768,\n1024} and adopt 512 for the bottom one and 768\nfor the top one based on validation experiments.\nThe task id embeddings have the same size as the\nhidden layer dimension of the CL-plugin. The\nmaximum input length is set to 164 which is long\nenough for all datasets. We use Adam optimizer\nand set the learning rate to 1e-4 for post-training\nand 5e-5 for fine-tuning. The batch size is set to\n48 for post-training and 20 for fine-tuning. Since\neach of our domain-specific dataset has a differ-\nent size, we train CPT on each task/domain for\n1 epoch for post-training, which is approximately\n13K steps, following (Gururangan et al., 2020b;\nXu et al., 2019). We train on end-task fine-tuning\ndatasets for 20 epochs and take the results for the\nlast epoch, assuming no validation sets. We em-\npirically found 20 epochs can give us a relatively\nstable results.\n3.3 Evaluation Results and Analysis\nWe report the average results of the 4 different fine-\ntuning tasks (or datasets) in accuracy and Macro-F1\nafter post-training on all unlabeled domain datasets\nin Table 1. The forgetting rate (forget R.) (Liu et al.,\n2020a) is also reported. The higher the forgetting\nrate is, the more forgetting it has. Negative rates\nindicate positive knowledge transfer.8\nSuperiority of CPT. Clearly, CPT outperforms\nall baselines and achieves no forgetting. More\nspecifically, CPT markedly outperforms the two\n8Forgetting rate is computed as as follows (Liu et al.,\n2020a), 1\nt−1\n∑t−1\ni=1 Ai,i − At,i, where Ai,i is the end-task\nperformance right after its domain i is post-trained, and At,i\nis the performance of the end-task of domain i after post-\ntraining the last domain. We average over all end-tasks except\nthe last one as the last domain has no forgetting.\n10208\nCategory Domain Restaurant AI ACL AGNews Average Forget R.Model MF1 Acc MF1 Acc MF1 Acc MF1 Acc MF1 Acc MF1 Acc\nNon-CL\nRoBERTa50.61 74.77 27.88 28.44 32.19 34.59 64.19 65.95 43.72 50.94 —Adapter 45.40 67.28 23.69 24.56 24.99 27.5564.53 66.5039.65 46.48 —RoBERTa-ONE53.6376.7329.86 30.11 33.05 35.72 62.57 65.13 44.78 51.92 —Adapter-ONE52.19 74.20 30.80 31.59 36.59 36.99 61.66 63.94 45.31 51.68 —Prompt-ONE28.93 59.79 21.06 22.10 28.02 29.22 60.70 62.58 34.68 43.42 —DEMIX 53.14 75.28 27.68 27.29 37.63 38.57 63.18 65.13 45.41 51.57 —\nCL\nRoBERTa-NCL42.59 67.5631.57 31.6233.07 34.54 60.18 63.50 41.85 49.30 3.27 2.82Adapter-NCL47.42 70.23 29.56 29.90 35.92 37.58 61.73 64.45 43.65 50.54 2.21 1.69HAT 50.45 71.78 28.33 29.41 34.93 37.15 62.97 65.05 44.17 50.85 2.43 2.04BCL 51.70 74.34 29.66 30.96 32.85 34.82 63.60 65.47 44.45 51.40 1.47 0.82KD 39.75 67.11 29.63 29.3338.30 42.0962.85 65.39 42.63 50.98 4.92 3.07EWC 48.32 71.59 30.96 31.01 35.96 38.05 62.29 64.95 44.38 51.40 1.40 0.80DER++ 48.09 71.79 30.71 30.54 34.25 35.77 64.24 66.11 44.32 51.05 1.79 1.62CPT 53.9075.13 30.42 30.89 37.56 38.53 63.77 65.7946.41 52.59 0.00 0.00\nTable 1: End-task macro-F1 (MF1), accuracy and forgetting rate results for all domains after continual post-training\nof all domains. The results are averages of 5 random seeds (the domain training order is as they appear in the\nfirst row). Due to space limits, the results for different domain orders and the standard deviations are reported in\nAppendix E and Appendix F, respectively). Non-CL baselines has no forgetting.\nbaselines without post-training (RoBERTa and\nAdapter), indicating CPT can learn new domains\nwell. These two baselines are also significantly\nworse than other baselines, indicating that fine-\ntuning the pre-trained RoBERTa alone is weak.\nComparing with CL baselines, CPT achieves no\nforgetting (we can see the forgetting rate is 0),\nindicating the high effectiveness of the proposed\napproach. We also note that CPT is even slightly\nbetter than those ONE baselines, indicating some\npositive knowledge transfer in CPT.\n3.4 Ablations\nIn Table 2, we give the ablation results. We are\ninterested in the following:\n(1) Catastrophic butterfly effect (CBE). The\nthird row with “w/o butterfly” shows results with-\nout the hard binary mask mechanism in Eq. 4.\nClearly, the results are worse and the model suf-\nfers from forgetting. This indicates CBE and our\napproach is effective.\n(2) Different Architecture. CPT is based on\nCL-plugin, which is inspired by adapters. Another\npopular way to use adapters is to make it sequential\n(Houlsby et al., 2019). Sequential adapter (first\nrow) is clearly poorer than our parallel one. This\nconforms to the observation in (He et al., 2021).\n(3) Different Orders. Table 1 only re-\nports the results of one fixed domain order\n(Restaurant→AI→ACL →AGNews). We are in-\nterested in how the order impacts CPT results. We\ngive the detailed results for all the other baselines\nand detailed domain orders in Appendix E. We can\nsee the results of CPT does not change much and it\nstill outperforms other baselines. This indicates the\nModel Final Performance\nMF1 Acc\nCPT (Sequential Adapter)43.00 50.25\nCPT (w/o butterfly)44.17 50.85\nCPT (w/o masking)43.65 50.54\nCPT 46.41 52.58\nTable 2: Ablation experiment results.\nCPT’s robustness to domain orders in post-training.\n4 Conclusion\nThis paper proposed to continually post-train an\nLM with a sequence of domains using their unla-\nbeled domain corpora. An effective method (CPT)\nis also proposed. An end-task from any post-trained\ndomain can fine-tune the resulting LM. Experimen-\ntal results demonstrate the effectiveness of CPT.\n5 Limitations\nWe list two limitations of CPT. First, CPT adds CL-\nplugins for continual post-training with no change\nto the underlying LM in training. Although a CL-\nplugin is small compared to an LM, it is still inter-\nesting and may be more effective to explore the idea\nof updating the LM directly without any additional\nmodules. Second, domain ids are needed in both\ntraining and testing for CPT. In some applications,\nit may be hard to provide a domain id for each\nfine-tuning end-task. We will explore these in our\nfuture work as specializing and/or incrementally\nimproving an LM is an important problem.\nAcknowledgments\nThe work of Zixuan Ke and Bing Liu was supported\nin part by three National Science Foundation grants\n(IIS-1910424, IIS-1838770, and CNS-2225427).\n10209\nReferences\nAntreas Antoniou, Massimiliano Patacchiola, Mateusz\nOchal, and Amos J. Storkey. 2020. Defining bench-\nmarks for continual few-shot learning. CoRR,\nabs/2004.11967.\nSagie Benaim and Lior Wolf. 2018. One-shot unsu-\npervised cross domain translation. In Advances in\nNeural Information Processing Systems 31: Annual\nConference on Neural Information Processing Sys-\ntems 2018, NeurIPS 2018, December 3-8, 2018, Mon-\ntréal, Canada, pages 2108–2118.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020a. Language models are few-shot\nlearners. Advances in neural information processing\nsystems.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020b. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual.\nPietro Buzzega, Matteo Boschini, Angelo Porrello, Da-\nvide Abati, and Simone Calderara. 2020. Dark expe-\nrience for general continual learning: a strong, simple\nbaseline. arXiv preprint arXiv:2004.07211.\nZhiyuan Chen and Bing Liu. 2018. Lifelong machine\nlearning. Synthesis Lectures on Artificial Intelligence\nand Machine Learning, 12(3):1–207.\nYung-Sung Chuang, Shang-Yu Su, and Yun-Nung Chen.\n2020. Lifelong language knowledge distillation.\narXiv preprint arXiv:2010.02123.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT.\nChrisantha Fernando, Dylan Banarse, Charles Blundell,\nYori Zwols, David Ha, Andrei A. Rusu, Alexander\nPritzel, and Daan Wierstra. 2017. Pathnet: Evolution\nchannels gradient descent in super neural networks.\nCoRR.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, pages\n3816–3830. Association for Computational Linguis-\ntics.\nTianyu Gao, Xu Han, Ruobing Xie, Zhiyuan Liu, Fen\nLin, Leyu Lin, and Maosong Sun. 2020. Neural\nsnowball for few-shot relation learning. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 7772–\n7779. AAAI Press.\nClaudio Greco, Barbara Plank, Raquel Fernández, and\nRaffaella Bernardi. 2019. Psycholinguistics meets\ncontinual learning: Measuring catastrophic forget-\nting in visual question answering. arXiv preprint\narXiv:1906.04229.\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.\n2021. Ppt: Pre-trained prompt tuning for few-shot\nlearning. arXiv preprint arXiv:2109.04332.\nYiduo Guo, Bing Liu, and Dongyan Zhao. 2022. Online\ncontinual learning through mutual information maxi-\nmization. In International Conference on Machine\nLearning, pages 8109–8126. PMLR.\nSuchin Gururangan, Mike Lewis, Ari Holtzman,\nNoah A. Smith, and Luke Zettlemoyer. 2021. Demix\nlayers: Disentangling domains for modular language\nmodeling.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020a. Don’t stop pretraining:\nadapt language models to domains and tasks. arXiv\npreprint arXiv:2004.10964.\nSuchin Gururangan, Ana Marasovic, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020b. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nACL.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2021. Towards a\nunified view of parameter-efficient transfer learning.\nCoRR, abs/2110.04366.\nXu He and Herbert Jaeger. 2018. Overcoming catas-\ntrophic interference using conceptor-aided backprop-\nagation. In ICLR.\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2(7).\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In\nICML.\n10210\nZikun Hu, Xiang Li, Cunchao Tu, Zhiyuan Liu, and\nMaosong Sun. 2018. Few-shot charge prediction\nwith discriminative legal attributes. In Proceedings\nof the 27th International Conference on Computa-\ntional Linguistics, COLING 2018, Santa Fe, New\nMexico, USA, August 20-26, 2018, pages 487–498.\nAssociation for Computational Linguistics.\nYufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi Wang,\nand Diyi Yang. 2021. Continual learning for text clas-\nsification with information disentanglement based\nregularization. arXiv preprint arXiv:2104.05489.\nXisen Jin, Bill Yuchen Lin, Mohammad Rostami, and\nXiang Ren. 2021. Learn continually, generalize\nrapidly: Lifelong knowledge accumulation for few-\nshot learning. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 16-20 November,\n2021, pages 714–729. Association for Computational\nLinguistics.\nDavid Jurgens, Srijan Kumar, Raine Hoover, Daniel A.\nMcFarland, and Dan Jurafsky. 2018. Measuring the\nevolution of a scientific field through citation frames.\nTACL.\nZixuan Ke, Bing Liu, Nianzu Ma, Hu Xu, and Lei Shu.\n2021a. Achieving forgetting prevention and knowl-\nedge transfer in continual learning. NeurIPS.\nZixuan Ke, Bing Liu, Hu Xu, and Lei Shu. 2021b. Clas-\nsic: Continual and contrastive learning of aspect sen-\ntiment classification tasks. In EMNLP.\nZixuan Ke, Hu Xu, and Bing Liu. 2021c. Adapting bert\nfor continual learning of a sequence of aspect sen-\ntiment classification tasks. In NAACL, pages 4746–\n4755.\nHuda Khayrallah, Brian Thompson, Kevin Duh, and\nPhilipp Koehn. 2018. Regularized training objec-\ntive for continued training for domain adaptation in\nneural machine translation. In Proceedings of the\n2nd Workshop on Neural Machine Translation and\nGeneration, pages 36–44.\nJames Kirkpatrick, Razvan Pascanu, Neil C. Rabi-\nnowitz, Joel Veness, Guillaume Desjardins, Andrei A.\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\n2016. Overcoming catastrophic forgetting in neural\nnetworks. CoRR.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In EMNLP.\nYuanpeng Li, Liang Zhao, Kenneth Church, and Mo-\nhamed Elhoseiny. 2019. Compositional language\ncontinual learning. In International Conference on\nLearning Representations.\nTianlin Liu, Lyle Ungar, and Joao Sedoc. 2019a. Con-\ntinual learning for sentence representations using con-\nceptors. arXiv preprint arXiv:1904.09187.\nYaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and\nQianru Sun. 2020a. Mnemonics training: Multi-class\nincremental learning without forgetting. In CVPR,\npages 12242–12251. IEEE.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR.\nZihan Liu, Genta Indra Winata, Andrea Madotto, and\nPascale Fung. 2020b. Exploring fine-tuning tech-\nniques for pre-trained cross-lingual models via con-\ntinual learning. arXiv preprint arXiv:2004.14218.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel S. Weld. 2020. S2ORC: the semantic\nscholar open research corpus. In ACL.\nDavid Lopez-Paz and Marc’Aurelio Ranzato. 2017.\nGradient episodic memory for continual learning. In\nNIPS.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identification of entities,\nrelations, and coreference for scientific knowledge\ngraph construction. In ACL.\nAndrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Se-\nungwhan Moon, Paul Crook, Bing Liu, Zhou Yu,\nEunjoon Cho, and Zhiguang Wang. 2020. Continual\nlearning in task-oriented dialogue systems. arXiv\npreprint arXiv:2012.15504.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learn-\ning and motivation.\nSanket Vaibhav Mehta, Darshan Patil, Sarath Chandar,\nand Emma Strubell. 2021. An empirical investigation\nof the role of pre-training in lifelong learning. In\nICML CL Workshop.\nChengwei Qin and Shafiq Joty. 2021. LFPT5: A\nunified framework for lifelong few-shot language\nlearning based on prompt tuning of T5. CoRR,\nabs/2110.07298.\nYujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2022. ELLE: ef-\nficient lifelong pre-training for emerging data. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 2789–2810. Association for Computa-\ntional Linguistics.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, H. Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\n10211\nJonathan Uesato, John Mellor, Irina Higgins, Antonia\nCreswell, Nat McAleese, Amy Wu, Erich Elsen, Sid-\ndhant M. Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake A. Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam S. Isaac, Edward Lockhart, Simon Osindero,\nLaura Rimell, Chris Dyer, Oriol Vinyals, Kareem\nAyoub, Jeff Stanway, Lorrayne Bennett, Demis Hass-\nabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021.\nScaling language models: Methods, analysis & in-\nsights from training gopher. CoRR, abs/2112.11446.\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nSperl, and Christoph H. Lampert. 2017. icarl: In-\ncremental classifier and representation learning. In\nCVPR.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, EACL 2021, Online, April 19 - 23, 2021, pages\n255–269. Association for Computational Linguistics.\nAri Seff, Alex Beatson, Daniel Suo, and Han Liu. 2017.\nContinual learning in generative adversarial nets.\nCoRR, abs/1705.08395.\nJoan Serrà, Didac Suris, Marius Miron, and Alexan-\ndros Karatzoglou. 2018. Overcoming catastrophic\nforgetting with hard attention to the task. In ICML.\nYilin Shen, Xiangyu Zeng, and Hongxia Jin. 2019. A\nprogressive model to enable continual learning for\nsemantic slot filling. In EMNLP-IJCNLP.\nHanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon\nKim. 2017. Continual learning with deep generative\nreplay. In NIPS.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, Elton Zheng, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia\nSong, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro.\n2022. Using deepspeed and megatron to train\nmegatron-turing NLG 530b, A large-scale genera-\ntive language model. CoRR, abs/2201.11990.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to fine-tune bert for text classification? In\nChina national conference on Chinese computational\nlinguistics, pages 194–206. Springer.\nFan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. 2020.\nLamol: Language modeling is all you need for life-\nlong language learning. In ICLR.\nEleni Triantafillou, Richard S. Zemel, and Raquel Urta-\nsun. 2017. Few-shot learning through an information\nretrieval lens. In Advances in Neural Information\nProcessing Systems 30: Annual Conference on Neu-\nral Information Processing Systems 2017, December\n4-9, 2017, Long Beach, CA, USA, pages 2255–2265.\nGido M. van de Ven and Andreas S. Tolias.\n2019. Three scenarios for continual learning.\nhttps://arxiv.org/pdf/1904.07734.pdf.\nZirui Wang, Sanket Vaibhav Mehta, Barnabás Póczos,\nand Jaime Carbonell. 2020. Efficient meta lifelong-\nlearning with limited memory. In EMNLP.\nCongying Xia, Wenpeng Yin, Yihao Feng, and Philip S.\nYu. 2021. Incremental few-shot text classification\nwith multi-round new classes: Formulation, dataset\nand system. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, Online, June 6-11,\n2021, pages 1351–1360. Association for Computa-\ntional Linguistics.\nHu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2019.\nBERT post-training for review reading comprehen-\nsion and aspect-based sentiment analysis. In NAACL-\nHLT.\nWenpeng Yin, Jia Li, and Caiming Xiong. 2022. Con-\ntintin: Continual learning from task instructions. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 3062–3072. Association for Computa-\ntional Linguistics.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In NIPS.\n10212\nA Related Work\nOur work is related to continual learning, post-\ntraining and few-shot learning.\nContinual learning (CL). In general, overcom-\ning CF is a major goal in CL (Chen and Liu,\n2018). (1) Regularization methods (Kirkpatrick\net al., 2016; Seff et al., 2017) add a regularization\nto ensure minimal changes to weights for previous\ntasks. (2) Replay methods retain (Rebuffi et al.,\n2017; Lopez-Paz and Ranzato, 2017; Wang et al.,\n2020; Guo et al., 2022) or generate some data of\nold tasks (Shin et al., 2017; He and Jaeger, 2018)\nand use them in learning a new task. (3) Parame-\nter isolation methods (Serrà et al., 2018; Fernando\net al., 2017) allocate parameters for different tasks\nand mask them out in learning a new task. Our\nCPT is based on (3) and uses masks in continual\npost-training. Recently, CL has drawn attention in\nNLP. It has been used for slot filling (Shen et al.,\n2019), language learning (Li et al., 2019), sentence\nembedding (Liu et al., 2019a), translation (Khayral-\nlah et al., 2018), cross-lingual modeling (Liu et al.,\n2020b), question answering (Greco et al., 2019)\nand text classification (Ke et al., 2021a,b; Sun et al.,\n2020; Huang et al., 2021; Chuang et al., 2020;\nMehta et al., 2021; Madotto et al., 2020). How-\never, none of them tries to improve an LM.\nPost-training is an effective approach to miti-\ngate the discrepancies between pre-trained domains\nand the target domain. Researchers have applied\npost-training to many domains, e.g., reviews (Xu\net al., 2019; Sun et al., 2019), news and academic\npapers (Gururangan et al., 2020b), and shown im-\nproved end-task results. However, none of them\nconsider the continual learning paradigm.\nFew-shot learning (FL) aims to learn tasks with\na few labeled examples. The main issue of FL is\nover-fitting, due to the scarcity of labeled training\ndata. Existing methods to overcome over-fitting fall\nin three main families: (i) model-based methods\ntry to reduce the hypothesis space of the few-shot\ntask (Triantafillou et al., 2017; Hu et al., 2018),\n(ii) data-based methods try to augment additional\ndata to the few-shot set (Benaim and Wolf, 2018;\nGao et al., 2020), and (iii) algorithm-based solu-\ntions try to improve strategies for searching for\nthe best hypothesis. Recently, a new paradigm\nusing prompts achieves promising results for few-\nshot language learning as shown in GPT-3 (Brown\net al., 2020a), PET (Schick and Schütze, 2021) and\nLM-BFF (Gao et al., 2021). However, none of\nthem does few-shot fine-tuning in continual post-\ntraining.\nContinual few-shot learning. Several re-\nsearchers have studied this problem recently (An-\ntoniou et al., 2020; Qin and Joty, 2021; Jin et al.,\n2021; Xia et al., 2021; Yin et al., 2022). It contin-\nually learns a sequence of few-shot tasks. How-\never, this is very different from our continual post-\ntraining because our continual learning happens in\nthe post-training stage instead of the end-task fine-\ntuning stage. We only evaluate the proposed CPT\nsystem after continual post-training by conducting\nfew-shot learning tasks individually by fine-tuning\nthe post-trained language model (p-LM) in each of\nthe post-trained domains. No continual learning is\ninvolved in few-shot learning.\nB Illustration of Task Masks\nFigure 2 illustrates the CPT architecture and the\ntask mask learning. Note that fine-tuning is for\nevaluating the domain post-training and should\nnot affect any parameters of post-training. Dur-\ning continual post-training (Figure 2 (A)), after\ntraining domain/task 1, we obtain its useful neu-\nrons indicated by the 1 entries. Before training\ndomain/task 2, those useful neurons for domain\n1 are first masked (those previous 1’s entries are\nturned to 0’s). After training domain 2, two neu-\nrons with 1 are used by the domain. When domain\ntarrives, all used neurons by domains 1 and 2 are\nmasked before training, i.e., their entries are set to\n0. After training domain t, we see that domains\nt and 1 have a shared neuron (the cell with two\ncolors, red and green), which is used by both of do-\nmains. After continual post-training, we evaluate\nCPT by individual fine-tuning. During fine-tuning\n(Figure 2 (B)), we only make use of those neurons\nthat are useful for domain/task id t(red cells) and\nfreeze all other neurons (grey cells).\nC Dataset Statistics\nTable 3 shows the statistics of theunlabeled domain\ndatasets and end-task classification datasets. Note\nthat the full AGNews is very large. We use only\nits author provided training split as our domain-\nspecific datasets as our unlabeled AGNews dataset\nfor continual post-training. The remaining testing\nset is used as the labeled end-task ( AGNews-FT).\nThe other three corresponding end task datasets are\nSemEval-res (Xu et al., 2019), ACL-ARC (Jurgens\net al., 2018), and SCIERC(Luan et al., 2018).\n10213\nMLM Head\nHidden States\nAttention\nAdd & Layer Norm\nFFN\nCL-Plugin\nCL-Plugin\nAdd & Layer Norm\n+\n+\n× L\n1\nCL-Plugin\n1 0 0 0 0\n0 0 1 0 0\n2\n0 1 0 0 0\n0 0 0 1 0\nt\n0 1 0 1 0\n1 0 0 0 0\nDomain 1\nDomain 2\nDomain t\nHidden States\nAttention\nAdd & Layer Norm\nFFN\nCL-Plugin\nCL-Plugin\nAdd & Layer Norm\n+\n+\n× L\nClassification Head\nt\n1\n11\nFixed Modules\n(A) Continual Post-training (B) Individual Fine-tuning\n2\n1\n1\n1\nCL-Plugin\n1\n1\n0 0 00\n0 0 0\n0 0 0\n0 0 0\n0\n0\n0\n0 0 0 0\n0 0 0\nFigure 2: Architecture of CPT, which has two CL-plugins inserted in the transformer layers of RoBERTa in a parallel\nmanner. (A) CPT for continual post-training. It uses a masked language model (MLM) head for unsupervised\npost-training of the plugins only. (B) CPT for individual fine-tuning. The performance of CPT is evaluated by the\ncorresponding individual end-task performance of all post-trained tasks using the final post-trained model (with\ndifferent mask). Each CL-plugin module (to the right of the transformer) has two fully connected layers and a\nskip connection. On top of each fully connected layer, there is a mask computed from task ID twith the same size\nas the fully connected layer.\nUnlabeled Domain Datasets End-Task Classification Datasets\nDataset Source #training Dataset Task #training #testing #classes\nYelp Restaurant Yelp Review 1,132,359SemEval-res Aspect Sentiment Classification 32 1,120 3\nAI AI Papers 707,368 SCIERC Relation Classification 56 2,388 7\nACL ACL Papers 1,208,449 ACL-ARC Citation Intent Classification 48 421 6\nAGNews News Article 73,750 AGNews-FT News Classification 32 7,568 4\nTable 3: Statistics for unlabeled domain datasets and end task supervised classification datasets.\nD Details of the CL baselines\nNon-Continual Learning Baselines : Each of\nthese baselines builds a separate model for each\ntask independently. It thus has no CF.\n(1,2) RoBERTa, Adapter (Liu et al.,\n2019b; Houlsby et al., 2019) use the origi-\nnal RoBERTa/Adapter for the end-task fine-tuning\nwithout any post-training. These are the only\ntwo without any post-training. All the following\nbaselines use the masked language model loss\n(MLM) for post-training.\n(3) RoBERTa-ONEis the existing post-training\nmethod in (Gururangan et al., 2020b). To our\nknowledge, the existing post-training systems are\nall based on the MLM loss.\n(4) Adapter-ONE (Madotto et al., 2020;\nHoulsby et al., 2019) adds small adapter layers\nbetween layers of Transformer for post-training.\nWe follow the adapter design in (Madotto et al.,\n2020; Houlsby et al., 2019). An adapter is simply\ntwo fully connected layers. During post-training,\nthe Transformer is fixed, only the added adapters\nare trainable. The bottleneck size (adapter size)\nis set to 128. During end-task fine-tuning, both\nRoBERTa and the adapters are trainable to ensure\nfair comparison.\n(5) Prompt-ONE (Lester et al., 2021) adds a\nsequence of real vector tokens (called virtual tokens\nor prompt tokens) to the end of the original input\nsequence. In post-training, RoBERTa (the LM)\nis fixed and only the prompt tokens are trained.\nIn end-task fine-tuning, both LM and the trained\nprompt are trainable. We initialize 100 tokens and\nset the learning rate of the prompt token to 0.3 in\n10214\nCategory Domain Restaurant AI ACL AGNews Average\nModel MF1 Acc MF1 Acc MF1 Acc MF1 Acc MF1 Acc\nNon-CL\nRoBERTa ±0.0456±0.0274±0.0208±0.0233±0.0391±0.0338±0.0121 ±0.014 ±0.0066±0.0062\nAdapter ±0.0214±0.0223±0.0111±0.0102±0.0375±0.0386±0.0221±0.0224±0.0155±0.0142\nRoBERTa-ONE±0.0095±0.0087±0.0364±0.0358±0.0382±0.0432±0.0169±0.0162±0.0197±0.0187\nAdapter-ONE±0.0292±0.0223±0.0207±0.0222±0.0076±0.0063±0.0141±0.0157±0.0074±0.0054\nPrompt-ONE±0.0427±0.0991±0.0297±0.0254±0.0386±0.0325±0.0115±0.0100±0.0151±0.0292\nDEMIX ±0.0329±0.0293±0.0259±0.0283±0.0297±0.0367±0.0336±0.0309±0.0152±0.0165\nCL\nRoBERTa-NCL±0.0374±0.0238±0.0156±0.0158±0.0293±0.0349±0.0218±0.0154±0.0130±0.0160\nAdapter-NCL±0.0250±0.0194±0.0232±0.0184±0.0183±0.0264±0.0136±0.0151±0.0095±0.0137\nHAT ±0.0264 ±0.012 ±0.0236±0.0251±0.0294±0.0287±0.0106 ±0.009 ±0.0078±0.0112\nBCL ±0.0255±0.0124±0.0121±0.0105±0.0182±0.0126±0.0100±0.0069±0.0094±0.0032\nKD ±0.0642±0.0435±0.0295±0.0233±0.0271±0.0267±0.0160±0.0133±0.0117±0.0109\nEWC ±0.0324±0.0259±0.0281±0.0189±0.0177±0.0196±0.0041±0.0096±0.0079±0.0062\nDER++ ±0.0250±0.0183±0.0231±0.0319±0.0116±0.0163±0.0196±0.0178±0.0126±0.0128\nCPT ±0.0264±0.0120±0.0236±0.0251±0.0294±0.0287±0.0106±0.0090±0.0078±0.0112\nTable 4: Standard deviations of the corresponding metrics of the proposed CPT system and the baselines.\nModel Final Performance\nMF1 Acc\nCPT (Sequential Adapter)±0.0347±0.0350\nCPT (w/o butterfly)±0.0102±0.0079\nCPT (w/o masking)±0.0095±0.0137\nCPT ±0.0078±0.0112\nTable 5: Standard deviations of the corresponding met-\nrics of the proposed CPT system and the ablations.\npost-training, following the setting in (Lester et al.,\n2021).\n(6) DEMIX (Gururangan et al., 2021) is a recent\nmodel to adapt a pre-trained LM with new domains.\nIt adds a new adapter once a new domain arrives\n(network expansion is needed) and initializes the\nnew adapter with the parameters of the previous\ntrained adapter nearest to the new domain data.\nThey use the perplexity on held-out samples to\nchoose the most probable adapter.\nContinual Learning (CL) Baselines.\n(7) RoBERTa-NCL (Naive continual learning)\nis a naive extension of (Gururangan et al., 2020b),\nwhich continually/incrementally post-trains the LM\nto learn all domains using the MLM loss with no\nmechanism to deal with forgetting or CF.\n(8) Adapter-NCL (Houlsby et al., 2019) is sim-\nilar to the Adapter based system. The only dif-\nference is that the same set of adapters is shared\nacross all domains, rather than using a new adapter\nfor each new domain.\n(9) Hard attention to overcome forgetting\n(HAT) is derived from HAT (Serrà et al., 2018), the\nstate-of-the-art parameter-isolation based method\nwith almost no forgetting. However, HAT suffers\nfrom forgetting in continual post-training due to\nthe catastrophic butterfly effect.\n(10) BCL (Ke et al., 2021c) is a continual learn-\ning model that can avoid forgetting and encourage\nknowledge transfer. It is similar to Adapter-NCL.\nThe difference is that its adapters consist of two\nmodules, one is a capsule network (a new capsule\nis added once a new domain arrives) to encourage\ntransfer, and the other is similar to HAT to avoid\nforgetting. Similar to HAT, task/domain informa-\ntion is needed in end-task fine-tuning. We replace\nthe backbone network from BERT with RoBERTa\nfor fair comparison.\n(11) Knowledge distillation (KD) (Hinton et al.,\n2015) minimizes the representational deviation be-\ntween the learned representation and the new rep-\nresentation in post-training. We compute the KL\ndivergence between the representations (the output\nbefore the masked language model prediction head)\nof each token of the previous post-trained LM and\ncurrent LM as the distillation loss.\n(12) EWC (Buzzega et al., 2020) is a popu-\nlar regularization-based continual learning method\nthat adopts elastic weights consolidation to add L2\nregularization to penalize parameter changes.\n(13) DER++ (Buzzega et al., 2020) is a recent\nreplay method using distillation to regularize the\nnew task training. We store 16.4K tokens for each\nlearned domain as the memory, which is the largest\nmemory we can use for the system to run.\nE Results for Different Domain Orders\nTable 1 in the main paper reported the results for\nthe order Restaurant →AI →ACL →AGnews.\nWe now look at how the order affects the results.\nTable 6 shows baselines and CPT’s results of 4\ndifferent orders. Note that the results for the Non-\nCL baselines are the same across different orders\n(and the same as those in Table 1) because they\n10215\nCategoryOrder AI→ACL→Restaurant→AGNewsRestaurant→AI→AGNews→ACLAI→ACL→AGNews→RestaurantAGNews→ACL→Restaurant→AI AverageMetric Performance Forget R.Performance Forget R.Performance Forget R.Performance Forget R.Performance Forget R.Model MF1 Acc MF1 AccMF1 Acc MF1 AccMF1 Acc MF1 AccMF1 Acc MF1 AccMF1 Acc MF1 Acc\nNon-CL\nRoBERTa43.72 50.94 — 43.72 50.94 — 43.72 50.94 — 43.72 50.94 — 43.72 50.94 —Adapter39.65 46.48 — 39.65 46.48 — 39.65 46.48 — 39.65 46.48 — 39.65 46.48 —RoBERTa-ONE44.78 51.92 — 44.78 51.92 — 44.78 51.92 — 44.78 51.92 — 44.78 51.92 —Adapter-ONE45.31 51.68 — 45.31 51.68 — 45.31 51.68 — 45.31 51.68 — 45.31 51.68 —Prompt-ONE34.68 43.42 — 34.68 43.42 — 34.68 43.42 — 34.68 43.42 — 34.68 43.42 —DEMIX45.41 51.57 — 45.41 51.57 — 45.41 51.57 — 45.41 51.57 — 45.41 51.57 —\nCL\nRoBERTa-NCL42.62 49.95 2.45 1.7942.22 49.52 3.10 2.3342.88 50.11 0.29 0.1844.33 51.51 1.76 1.2143.01 50.28 1.90 1.38Adapter-NCL44.71 51.67 1.71 1.0844.61 51.07 1.14 1.2344.91 51.57 1.41 1.2345.5252.150.72 0.44 44.94 51.62 1.25 0.99HAT 45.10 51.50 1.66 1.1943.29 49.96 2.76 2.0946.0652.070.50 0.2144.94 51.45 0.86 0.2544.85 51.25 1.45 0.93BCL 43.97 50.74 2.20 1.5045.30 51.54 0.36 -0.1445.28 51.79 0.36 0.1145.59 51.61 0.08 0.1145.04 51.42 0.75 0.40KD 42.09 50.22 0.57 0.0845.1852.681.22 0.5742.63 50.45-0.31 -0.5642.93 50.70 1.10 0.3243.21 51.01 0.64 0.10EWC 43.97 50.74 0.16 0.0343.65 50.29-0.29 -0.2045.52 51.36 0.17 0.1543.42 49.85 0.12 0.1044.14 50.56 0.04 0.02DER++44.56 50.13 2.95 2.3144.02 49.99 1.24 1.1243.98 50.23 1.44 1.2744.32 50.13 1.32 1.0944.22 50.12 1.74 1.45CPT 46.49 52.47 0.00 0.0045.7151.71 0.00 0.0046.1551.93 0.00 0.0045.8951.860.00 0.00 46.06 51.99 0.00 0.00\nTable 6: CPT performance averaged over all domains after the final post-trained with different orders (averaged\nover 5 random seeds) and the average of these orders.\nare not effected by orders. We can see CPT is\nalways better than other baselines, and achieve 0\nforgetting rate, demonstrating the effectiveness of\nCPT. We also note that some baselines in some\nsequence has negative forgetting rate, indicating\nthey have some backward transfer (new domain\nlearning helps learned domains). However, their\nfinal results are much worse than CPT’s.\nF Standard Deviations\nTable 4 reports the standard deviations of the corre-\nsponding results in Table 1 (in the main paper) of\nCPT and the considered baselines over 5 runs with\nrandom seeds. We can see the results of CPT are\nstable. Some baselines (e.g., RoBERTa, RoBERTa-\nONE) can have quite large standard deviations.\nTable 5 reports the standard deviations of the cor-\nresponding results in Table 2 (in the main paper) of\nCPT and the considered baselines over 5 runs with\nrandom seeds. We can see the results of sequential\nadapters has a high variance while CPT and other\nvariants are stable.\n10216",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8419209718704224
    },
    {
      "name": "Forgetting",
      "score": 0.7685747742652893
    },
    {
      "name": "Task (project management)",
      "score": 0.7150022983551025
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6619510054588318
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5802561044692993
    },
    {
      "name": "Language model",
      "score": 0.5754477381706238
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4874160587787628
    },
    {
      "name": "Natural language processing",
      "score": 0.48600849509239197
    },
    {
      "name": "Machine learning",
      "score": 0.469265878200531
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4630465805530548
    },
    {
      "name": "Shot (pellet)",
      "score": 0.4508192837238312
    },
    {
      "name": "Training set",
      "score": 0.41518110036849976
    },
    {
      "name": "Domain knowledge",
      "score": 0.4125708341598511
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ]
}