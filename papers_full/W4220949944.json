{
  "title": "Shared computational principles for language processing in humans and deep language models",
  "url": "https://openalex.org/W4220949944",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287161261",
      "name": "Goldstein, Ariel",
      "affiliations": [
        "Google (United States)",
        "Princeton University"
      ]
    },
    {
      "id": null,
      "name": "Zada, Zaid",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A3160851967",
      "name": "Buchnik Eliav",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4287273295",
      "name": "Schain, Mariano",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5107060503",
      "name": "Price Amy",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": null,
      "name": "Aubrey, Bobbi",
      "affiliations": [
        "New York University",
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A4302742090",
      "name": "Nastase Samuel A.",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A4227775901",
      "name": "Feder, Amir",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": null,
      "name": "Emanuel, Dotan",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4221702777",
      "name": "Cohen, Alon",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2747944831",
      "name": "Jansen, Aren",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": null,
      "name": "Gazula, Harshvardhan",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A4298914353",
      "name": "Choe, Gina",
      "affiliations": [
        "New York University",
        "Princeton University"
      ]
    },
    {
      "id": null,
      "name": "Rao, Aditi",
      "affiliations": [
        "Princeton University",
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4260847058",
      "name": "Kim Catherine",
      "affiliations": [
        "Princeton University",
        "New York University"
      ]
    },
    {
      "id": null,
      "name": "Casto, Colton",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A3106916384",
      "name": "Fanda Lora",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A5098172194",
      "name": "Doyle Werner",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2126179849",
      "name": "Friedman Daniel",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A5099575568",
      "name": "Dugan Patricia",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2320109686",
      "name": "Melloni Lucia",
      "affiliations": [
        "Max Planck Institute for Empirical Aesthetics"
      ]
    },
    {
      "id": "https://openalex.org/A4221608541",
      "name": "Reichart, Roi",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5099575567",
      "name": "Devore Sasha",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4379641106",
      "name": "Flinker Adeen",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": null,
      "name": "Hasenfratz, Liat",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A4202210949",
      "name": "Levy, Omer",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A4223699707",
      "name": "Hassidim, Avinatan",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2179368879",
      "name": "Brenner, Michael",
      "affiliations": [
        "Google (United States)",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A4223699708",
      "name": "Matias, Yossi",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4293345850",
      "name": "Norman, Kenneth A.",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2922364774",
      "name": "Devinsky Orrin",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4293345849",
      "name": "Hasson, Uri",
      "affiliations": [
        "Princeton University",
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3018827121",
    "https://openalex.org/W4229781645",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2963325985",
    "https://openalex.org/W2782213998",
    "https://openalex.org/W3013691153",
    "https://openalex.org/W2970648593",
    "https://openalex.org/W2991642412",
    "https://openalex.org/W2805003518",
    "https://openalex.org/W3037273551",
    "https://openalex.org/W3154773080",
    "https://openalex.org/W3088418428",
    "https://openalex.org/W3004619146",
    "https://openalex.org/W3108978252",
    "https://openalex.org/W2971568142",
    "https://openalex.org/W2119728020",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4230449744",
    "https://openalex.org/W2204668899",
    "https://openalex.org/W3027231994",
    "https://openalex.org/W3096052559",
    "https://openalex.org/W2153791616",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2030330674",
    "https://openalex.org/W2752448390",
    "https://openalex.org/W2141138276",
    "https://openalex.org/W2000387713",
    "https://openalex.org/W1507893557",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2479874407",
    "https://openalex.org/W1969221307",
    "https://openalex.org/W2103716492",
    "https://openalex.org/W2747342147",
    "https://openalex.org/W2007997142",
    "https://openalex.org/W2170167574",
    "https://openalex.org/W2063951486",
    "https://openalex.org/W1980276147",
    "https://openalex.org/W2110065044",
    "https://openalex.org/W1847618513",
    "https://openalex.org/W4239075771",
    "https://openalex.org/W2575773685",
    "https://openalex.org/W6732146497",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2978368159",
    "https://openalex.org/W6768391230",
    "https://openalex.org/W4206415992",
    "https://openalex.org/W2146172595",
    "https://openalex.org/W2543665853",
    "https://openalex.org/W2061656302",
    "https://openalex.org/W2884989790",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W3028871071",
    "https://openalex.org/W2102040782",
    "https://openalex.org/W3101065397",
    "https://openalex.org/W2029996593",
    "https://openalex.org/W2001796002",
    "https://openalex.org/W2166073443",
    "https://openalex.org/W2162799770",
    "https://openalex.org/W2953042022",
    "https://openalex.org/W2072307422",
    "https://openalex.org/W2937285331",
    "https://openalex.org/W2038201838",
    "https://openalex.org/W4289638300",
    "https://openalex.org/W2259559558",
    "https://openalex.org/W3103536442"
  ],
  "abstract": null,
  "full_text": "Articles\nhttps:/ / doi.org/10.1038/s41593-022-01026-4\n1Department of Psychology and the Neuroscience Institute, Princeton University, Princeton, NJ, USA. 2Google Research, Mountain View, CA, USA. \n3New Y ork University Grossman School of Medicine, New Y ork, NY , USA. 4Max Planck Institute for Empirical Aesthetics, Frankfurt, Germany. 5Faculty of \nIndustrial Engineering and Management, T echnion, Israel Institute of T echnology, Haifa, Israel. 6Blavatnik School of Computer Science, T el Aviv University, \nT el Aviv, Israel. 7School of Engineering and Applied Science, Harvard University, Cambridge, MA, USA. 8These authors contributed equally: Zaid Zada,  \nEliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, Samuel A. Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, Aren Jansen.  \n✉e-mail: ariel.y.goldstein@gmail.com\nT\nhe outstanding success of autoregressive (predictive) DLMs is \nstriking from theoretical and practical perspectives because \nthey have emerged from a very different scientific paradigm \nthan traditional psycholinguist models\n1. In traditional psycholin -\nguistic approaches, human language is explained with interpreta -\nble models that combine symbolic elements (for example, nouns, \nverbs, adjectives and adverbs) with rule-based operations\n2,3. In \ncontrast, autoregressive DLMs learn language from real-world \ntextual examples ‘in the wild’ , with minimal or no explicit prior \nknowledge about language structure. Autoregressive DLMs do not \nparse words into parts of speech or apply explicit syntactic trans\n-\nformations. Rather, they learn to encode a sequence of words into \na numerical vector, termed a contextual embedding, from which \nthe model decodes the next word. After learning, the next-word \nprediction principle allows the generation of well-formed, novel, \ncontext-aware texts\n1,4,5.\nAutoregressive DLMs have proven to be extremely effective in \ncapturing the structure of language 6–9. It is unclear, however, if the \ncore computational principles of autoregressive DLMs are related \nto the way the human brain processes language. Past research \nhas leveraged language models and machine learning to extract \nsemantic representation in the brain\n10–18. But such studies did not \nview autoregressive DLMs as feasible cognitive models for how the \nhuman brain codes language. In contrast, recent theoretical papers \nargue that there are fundamental connections between DLMs and \nhow the brain processes language\n1,19,20.\nIn agreement with this theoretical perspective, we provide \nempirical evidence that the human brain processes incoming \nspeech similarly to an autoregressive DLM (Fig. 1). In particular, the \nhuman brain and autoregressive DLMs share three computational \nprinciples: (1) both are engaged in continuous context-dependent \nnext-word prediction before word onset; (2) both match pre-onset \npredictions to the incoming word to induce post-onset surprise \n(that is, prediction-error signals); (3) both represent words using \ncontextual embeddings. The main contribution of this study is \n \nthe provision of direct evidence for the continuous engagement of \nthe brain in next-word prediction before word onset (computa\n-\ntional principle 1). In agreement with recent publications 14,16,21–24,  \nwe also provide neural evidence in support of computational  \nprinciples 2 and 3.\nPrinciple 1: next-word prediction before word onset. The constant \nengagement in next-word prediction before word onset is the bed -\nrock objective of autoregressive DLMs6–9,25. Similarly, the claim that \nthe brain is constantly engaged in predicting the incoming input is \nfundamental to numerous predictive coding theories26–30. However, \neven after decades of research, behavioral and neural evidence for \nthe brain’s propensity to predict upcoming words before they are \nperceived during natural language processing has remained indirect. \nOn the behavioral level, the ability to predict upcoming words has \ntypically been tested with highly controlled sentence stimuli (that is, \nthe cloze procedure\n31–33). Thus, we still do not know how accurate \nShared computational principles for language \nprocessing in humans and deep language models\nAriel Goldstein   1,2 ✉, Zaid Zada   1,8, Eliav Buchnik2,8, Mariano Schain2,8, Amy Price   1,8, \nBobbi Aubrey1,3,8, Samuel A. Nastase   1,8, Amir Feder2,8, Dotan Emanuel2,8, Alon Cohen2,8, \nAren Jansen2,8, Harshvardhan Gazula1, Gina Choe1,3, Aditi Rao1,3, Catherine Kim1,3, Colton Casto1, \nLora Fanda   3, Werner Doyle3, Daniel Friedman3, Patricia Dugan3, Lucia Melloni   4, Roi Reichart5, \nSasha Devore3, Adeen Flinker3, Liat Hasenfratz1, Omer Levy   6, Avinatan Hassidim2, \nMichael Brenner2,7, Y ossi Matias2, Kenneth A. Norman   1, Orrin Devinsky3 and Uri Hasson   1,2\nDeparting from traditional linguistic models, advances in deep learning have resulted in a new type of predictive (autoregres-\nsive) deep language models (DLMs). Using a self-supervised next-word prediction task, these models generate appropriate \nlinguistic responses in a given context. In the current study, nine participants listened to a 30-min podcast while their brain \nresponses were recorded using electrocorticography (ECoG). We provide empirical evidence that the human brain and autore-\ngressive DLMs share three fundamental computational principles as they process the same natural narrative: (1) both are \nengaged in continuous next-word prediction before word onset; (2) both match their pre-onset predictions to the incoming \nword to calculate post-onset surprise; (3) both rely on contextual embeddings to represent words in natural contexts. T ogether, \nour findings suggest that autoregressive DLMs provide a new and biologically feasible computational framework for studying \nthe neural basis of language.\nNAtURE NEUROSCiENCE | VOL 25 | MARCH 2022 | 369–380 | www.nature.com/ natureneuroscience 369\nArticles NaTUrE NEUrosCIENCE\nlisteners’ predictions are in open-ended natural contexts. The first \nsection of the paper provides new behavioral evidence that humans \ncan predict forthcoming words in a natural context with remarkable \naccuracy, and that, given a sufficient context window, next-word \npredictions in humans and an autoregressive DLM (GPT-2)\n8 match. \nOn the neuronal level, we provide new evidence that the brain is \nspontaneously engaged in next-word prediction before word onset \nduring the processing of natural language. These findings provide \nthe missing evidence that the brain, like autoregressive DLMs, is \nconstantly involved in next-word prediction before word onset as it \nprocesses natural language (Fig. 1).\nPrinciple 2: pre-onset predictions are used to calculate \npost-word-onset surprise. Detecting increased neural activity \n \n400 ms after word onset for unpredictable words, documented \nacross many studies34,35, has traditionally been used as indirect evi -\ndence for pre-onset predictions. Recent development of autoregres-\nsive DLMs, like GPT-2, provides a powerful new way to quantify the \nsurprise and confidence levels for each upcoming word in natural \nlanguage\n14,22,23. Specifically, autoregressive DLMs use the confidence \nin pre-onset next-word predictions to calculate post-word-onset \nsurprise level (that is, prediction error; Fig. 1)\n14,22,23. Here we map the \ntemporal coupling between confidence level (entropy) in pre-onset \nprediction and post-word-onset surprise signals (cross-entropy). \nOur findings provide compelling evidence that, similarly to DLMs, \nthe biological neural error signals after word onset are coupled to \npre-onset neural signals associated with next-word predictions.\nPrinciple 3: contextual vectorial representation in the brain. \nAutoregressive DLMs encode the unique, context-specific meaning \nof words based on the sequence of prior words. Concurrent find\n-\nings demonstrate that contextual embeddings derived from GPT-2 \nprovide better modeling of neural responses in multiple brain \nareas than static (that is, non-contextual) word embeddings\n16,17. \nOur paper goes beyond these findings by showing that contextual \nembeddings encapsulate information about past contexts as well \nas next-word predictions. Finally, we demonstrate that contextual \nembeddings are better than non-contextual embeddings at predict\n-\ning word identity from cortical activity (that is, decoding) before \n(and after) word onset. Taken together, our findings provide com\n-\npelling evidence for core computational principles of pre-onset  \nprediction, post-onset surprise, and contextual representation, \nshared by autoregressive DLMs and the human brain. These results \nsupport a unified modeling framework for studying the neural \n \nbasis of language.\nResults\nPrediction before word onset. Comparison of next-word prediction \nbehavior in autoregressive deep language models and humans . We \ndeveloped a sliding-window behavioral protocol to directly quan -\ntify humans’ ability to predict every word in a natural context (Fig. \n2a,b). Fifty participants proceeded word by word through a 30-min \ntranscribed podcast (‘Monkey in the Middle’ , This American Life \npodcast\n36) and provided a prediction of each upcoming word. The \nprocedure yielded 50 predictions for each of the story’s 5,113 words \n(Fig. 2c and Methods). We calculated the mean prediction perfor -\nmance for each word in the narrative, which we refer to as a ‘predict-\nability score’ (Fig. 2d). A predictability score of 100% indicates that \nall participants correctly guessed the next word, and a predictability \nscore of 0% indicates that no participant predicted the upcoming \nword. This allowed us to address the following questions: First, how \ngood are humans at next-word prediction? Second, how closely do \nhuman predictions align with autoregressive DLM predictions?\nWord-by-word behavioral prediction during a natural story. \nParticipants were able to predict many upcoming words in a com\n-\nplex and unfamiliar story. The average human predictability score \nwas 28% (s.e. = 0.5%), in comparison to a predictability score of 6% \nfor blindly guessing the most frequent word in the text (‘the’). About \n600 words had a predictability score higher than 70%. Interestingly, \nhigh predictability was not confined to the last words in a sentence \nand applied to words from all parts of speech (21.44% nouns, 14.64% \nverbs, 41.62% function words, 4.35% adjectives and adverbs, and \n17.94% other). This suggests that humans are proficient in predict\n-\ning upcoming words in real-life contexts when asked to do so.\nHuman and deep language model next-word predictions and prob -\nabilities. Next, we compared human and autoregressive DLM pre -\ndictions of the words of the podcast as a function of prior context. \nAs an instance of an autoregressive DLM, we chose to work with \nDLM\n(GPT-2)\nGPT-2’s predictions\nProbabilities\nOur story begins deep in the\nNeural responses\nTop-k P\nWord onset (0 ms)\nIncoming wordContextSpoken narrative\nPre-onset\nNext-word\nprediction\nSurprise\n(prediction error)\nrainforest\nrainforest\nSurprise\nPost-onset\nContextual\nembeddings\nContextual embeddings\n1. ocean\n2. jungle\n3. rainforest\n4. winter\n5. heart\n6. wild\n0.35\n0.28\n0.21\n0.08\n0.06\n0.04\nInput to the brain and the model\nFig. 1 | Shared computational principles between the brain and \nautoregressive deep language models in processing natural language. \nFor each sequence of words in the text, GPT-2 generates a contextual \nembedding (blue), which is used to assign probabilities to the identity \nof the next word (green box). Next, GPT-2 uses its pre-onset predictions \nto calculate its surprise level (that is, error signal) when the next word is \nrevealed (purple box). The minimization of the surprise is the mechanism \nby which GPT-2 is trained to generate well-formed outputs. Each colored \nbox and arrow represents the relationship between a given computational \nprinciple of the autoregressive DLM and the neural responses. The green \nboxes represent that both the brain and autoregressive DLMs are engaged in \ncontext-dependent prediction of the upcoming word before word onset. The \ngreen arrow indicates that we take the actual predictions from GPT-2 (for \nexample, the top-one prediction ‘ocean’ for the example sentence ‘our story \nbegins deep in the…’) to model the neural responses before word onset \n(Fig. 4). The purple boxes represent that both the brain and autoregressive \nDLMs are engaged in the assessment of their predictions after word \nonset. The purple arrow indicates that we take the actual perceived next \nword (‘rainforest’ in our example), as well as GPT-2’s surprise level for the \nperceived word (cross-entropy) to model the neural responses after word \nonset (Figs. 4b and 5b). The blue boxes represent that both the brain and \nautoregressive DLMs use contextual embeddings to represent words. The \nblue arrow indicates that we take the contextual embeddings from GPT-2 \nto model the neural responses (Figs. 6 and 8). We argue here that, although \nthe internal word-processing mechanisms are not the same for the brain and \nDLMs, they do share three core computational principles: (1) continuous \ncontext-dependent next-word prediction before word onset; (2) reliance on \nthe pre-onset prediction to calculate post-word-onset surprise; and finally, \n(3) context-specific representation of meaning.\nNAtURE NEUROSCiENCE | VOL 25 | MARCH 2022 | 369–380 | www.nature.com/ natureneuroscience370\nArticlesNaTUrE NEUrosCIENCE\nGPT-2 (ref. 8). GPT-2 is a pretrained autoregressive language model \nwith state-of-the-art performance on tasks related to reading com -\nprehension, translation, text summarization and question answer -\ning. GPT-2 is trained by maximizing the log-probability of a token \ngiven its 1,024 past tokens (context, for a full description see ref. 7).  \nFor each word in the transcript, we extracted the most probable \nnext-word prediction as a function of context. For example, GPT-2 \nassigned a probability of 0.82 to the upcoming word ‘monkeys’ when \nit received the preceding words in the story as contextual input: ‘…\nSo after two days of these near misses, he changed strategies. He put \nhis camera on a tripod and threw down some cookies to try to entice \nthe _______. ’ . Human predictability scores and GPT-2 estimations \nof predictability were highly correlated (Fig. 2e; r = 0.79, P < 0.001). \nIn this case, the most probable next-word prediction for both GPT-2 \nand humans was ‘monkeys’ . In 49.1% of the cases, the most prob\n-\nable human prediction and the most probable GPT-2 prediction \nmatched (irrespective of accuracy). For baseline comparison, we \nreported the same agreement measure with human prediction for \n2- to 5-gram models in Extended Data Fig. 1 (Methods). Regarding \naccuracy, GPT-2 and humans jointly correctly and incorrectly pre\n-\ndicted 27.6% and 54.7% of the words, respectively. Only 9.2% of the \nwords that humans predicted correctly were not correctly predicted \nby GPT-2, and only 8.4% of the words correctly predicted by GPT-2 \nwere not correctly predicted by humans (Extended Data Fig. 2).\nFinally, we compared the match between the confidence level and \nthe accuracy level of GPT-2 and human predictions. For example, if \n(Ira Glass) So there's some places where animals almost\nnever go, places that are designed by humans for humans.\nThis act ends up in a place like that, but it starts about as far\nfrom there as you can get. Dana Chivvis explains.\n(Dana Chivvis) Our story begins deep in the rainforests of\nIndonesia on an island called Sulawesi. A few years ago, the\nphotographer David Slater traveled there from his home in\nEngland to photograph a troop of monkeys.\nTranscript\na\ne\nd\nb c\nf g\nNext-word prediction task\n51\n52\n53\nPrediction54\n55\n100100\n50\nPredictability score (%)\n0 0 500\n1.0\n0.8\n0.6\nGPT -2 prediction\n0.4\n0.2\n0\n0 0.2 0.4\nHuman prediction\n0.6 0.8 1.0\nr = 0.79\n1,0245123002752502252001751501251007550255432\nPredictability level:\nhumans versus GPT -2\n1,000 1,500 2,000 2,500 3,000 3,500 4,000\nContext window (previous words)Probability assigned\nGPT -2\n0.45\n0.50\n0.55\n0.60\nHuman vs GPT -2 corr.\n0.65\n0.70\n0.75\n0.80\nn-gram\n4,500\nPredictability match\nas a function of context\nProbability-to-accuracy\nmatching\n5,000\n500480460\n0\nBehavioral predictability of each word in the podcast\nBehavior\nTarget\nIndonesia\non\nan an\nisland\ncalled called\nBrazil far\nin there\nthe\nisland island\na\nfull\namazon\nand\narea\na\npopulated\nsouth...\n...\n...\n...\n...\nwhere\nisland\n0.16\n0.1\n0.02\n0.06\n0.62\n0.02\n0.23\n0.01\n0.003\n0.43\nwhere\nPt_1 Pt_2 Pt_3 Pt_50 Human DLM (GPT -2)\nProbability index\nChivvis explains. Our story begins deep in the rainforests of\nExplains. Our story begins deep in the rainforests of Indonesia\nOur story begins deep in the rainforests of Indonesia on\nStory begins deep in the rainforests of Indonesia on an\nBegins deep in the rainforests of Indonesia on an island\nHumans\nGPT -2\n1.00.80.60.40.20\n0\n0.2\nAccuracy0.4\n0.6\n0.8\n1.0\nFig. 2 | Behavioral assessment of the human ability to predict forthcoming words in a natural context. a, The stimulus was transcribed for the behavioral \nexperiment. b, A ten-word sliding window was presented in each trial, and participants were asked to type their prediction of the next word. Once entered, \nthe correct word is presented, and the window slides forward by one word. c, For each word, we calculated the proportion of participants that predicted \nthe forthcoming word correctly. d, Human predictability scores across words. e, Human predictability scores versus GPT-2’s predictability scores for each \nupcoming word in the podcast. f, Match between assigned probability for humans and GPT-2 and the actual accuracy for their top-one predictions.  \ng, Correlation between human predictions and GPT-2 predictions (as reported in d) for different context window lengths ranging from 2 to 1,024 preceding \ntokens (blue). Correlation between human predictions and 2- to 5-gram model predictions (orange).\nNAtURE NEUROSCiENCE | VOL 25 | MARCH 2022 | 369–380 | www.nature.com/ natureneuroscience 371\nArticles NaTUrE NEUrosCIENCE\nthe model (or humans) assigned a 20% probability, would it (or they) \nproduce only 20% correct predictions? Both humans and GPT-2 \nhad a remarkably similar confidence-to-accuracy function (Fig. 2f). \n \nSpecifically, GPT-2 and humans displayed under-confidence in \ntheir predictions and were above 95% correct when the probabili\n-\nties were higher than 40%. These analyses suggest that next-word \npredictions of GPT-2 and humans are similar in natural contexts.\nPrediction as a function of contextual window size. In natural com\n-\nprehension (for example, listening to or reading a story), predictions \nfor upcoming words are influenced by information accumulated \nover multiple timescales: from the most recent words to the infor\n-\nmation gathered over multiple paragraphs 37. We tested if GPT-2’s \npredictions would improve as a function of the context window as \nthey do in humans. To that end, we varied GPT-2’s input window \nsize (from 2 tokens up to 1,024 tokens) and examined the impact \nof contextual window size on the match with human behavior. The \ncorrelation between human and GPT-2 word predictions improved \nas the contextual window increased (from r = 0.46, P < 0.001 at \n2-word context to an asymptote of r = 0.79 at 100-word context; \n \nFig. 2g). For baseline comparison, we also plotted the correlations of \n2- to 5-gram models with human predictions (Fig. 2g and Methods).\nNeural activity before word onset reflects next-word predictions. \nThe behavioral study indicates that listeners can accurately predict \nupcoming words in a natural open-ended context when explicitly \ninstructed. Furthermore, it suggests human predictions and autore\n-\ngressive DLM predictions are matched in natural contexts. Next, \nwe asked whether the human brain, like an autoregressive DLM, is \ncontinuously engaged in spontaneous next-word prediction before \nword onset without explicit instruction. To that end, we used elec\n-\ntrocorticography signals from nine participants with epilepsy who \nvolunteered to participate in the study (see Fig. 3a for a map of all \nelectrodes). All participants listened to the same spoken story used \nin the behavioral experiment. In contrast to the behavioral study, \nthe participants engaged in free listening—with no explicit instruc\n-\ntions to predict upcoming words. Comprehension was verified \nusing a post-listening questionnaire. Across participants, we had \nbetter coverage in the left hemisphere (1,106 electrodes) than in the \nright hemisphere (233 electrodes). Thus, we focused on language \n \nprocessing in the left hemisphere, but we also present the encod -\ning results for the right hemisphere in Extended Data Fig. 3.  \nThe raw signal was preprocessed to reflect high-frequency broad -\nband (70–200 Hz) power activity (for full preprocessing proce-\ndures, see Methods).\nBelow we provide multiple lines of evidence that the brain, like \nautoregressive DLMs, is spontaneously engaged in next-word pre -\ndiction before word onset. The first section focuses solely on the \npre-onset prediction of individual words by using static (that is, \nnon-contextual) word embeddings (GloVe\n38 and word2vec39). In the \nthird section, we investigate how the brain adjusts its responses to \nindividual words as a function of context, by relying on contextual \nembeddings.\nLocalizing neural responses to natural speech. We used a linear \nencoding model and static semantic embeddings (GloVe) to local\n-\nize electrodes containing reliable responses to single words in the \nnarrative (Fig. 3a,b and Methods). Encoding models learn a map\n-\nping to predict brain signals from a representation of the task or \nstimulus\n40. The model identified 160 electrodes in early auditory \nareas, motor cortex and language areas in the left hemisphere (see \nFig. 3c for left-hemisphere electrodes and Extended Data Fig. 3 for \nright-hemisphere electrodes).\nEncoding neural responses before word onset.  In the behavioral \nexperiment (Fig. 2 ), we demonstrated people’s capacity to predict \n \nupcoming words in the story. Next, we tested whether the neu -\nral signals also contain information about the identity of the \npredicted words before they are perceived (that is, before word \nonset). The word-level encoding model (based on GloVe word \nembeddings) yielded significant correlations with predicted neu\n-\nral responses to upcoming words up to 800 ms before word onset \n(Fig. 4a; for single electrodes encoding models see Extended Data \nFig. 4). The encoding analysis was performed in each electrode \nwith significant encoding for GloVe embeddings (n  = 160), and \nthen averaged across electrodes (see map of electrodes in Fig. 3c). \nPeak encoding performance was observed 150–200 ms after word \nonset (lag 0), but the models performed above chance up to 800 ms \nbefore word onset. As a baseline for the noise level, we randomly \nshuffled the GloVe embeddings, assigning a different vector to the \noccurrence of each word in the podcast. The analysis yielded a flat \nencoding value around zero (Fig. 4a). The encoding results using \nGloVe embeddings were replicated using 100-dimensional static \nembeddings from word2vec (Extended Data Fig. 5). To control \nfor the contextual dependencies between adjacent words in the \nGloVe embeddings, we demonstrated that the significant encod\n-\ning before word onset holds even after removing the information \nof the previous GloVe embedding (Extended Data Fig. 6a). This \nsupports the claim that the brain continuously predicts semantic \ninformation about the meaning of upcoming words before they \nare perceived.\nTo test whether GloVe-based encoding is affected by the seman\n-\ntic knowledge embedded in the model, we shuffled the word embed-\ndings. Interestingly, when assigning a non-match GloVe embedding \n(from the story) to each word such that multiple occurrences of the \nsame word received the same (but non-match) GloVe embedding, \nthe encoding decreased (Extended Data Fig. 7). This indicates that \nthe relational linguistic information encoded in GloVe embeddings \nis also embedded in the neural activity.\nEncoding neural responses before word onset. To test if the significant \nencoding before word onset is driven by contextual dependencies \nbetween adjacent words in the GloVe embeddings, we also trained \nencoding models to predict neural responses using 50-dimensional \nstatic arbitrary embeddings, randomly sampled from a uniform \n \n[−1, 1] distribution. Arbitrary embeddings effectively removed the \ncontextual information about the statistical relationship between \nwords included in GloVe embeddings (Fig. 4a). Even for arbitrary \n \nembeddings, we were able to obtain significant encoding before word \nonset as to the identity of the upcoming word (for single-electrode \nencoding models, see Extended Data Fig. 4). To make sure that the \nanalysis does not rely on local dependencies among adjacent words, \nwe repeated the arbitrary-based encoding analysis after removing \nbi-grams that repeated more than once in the dataset (Extended \nData Fig. 6b). The ability to encode the neural activity for the \nupcoming words before word onset with the arbitrary embeddings \nremained significant.\nTo further demonstrate that predicting the next word before \nword onset goes above and beyond the contextual information \nembedded in the previous word, we ran an additional control \nanalysis. In the control analysis, we encoded the neural activity \nusing the arbitrary word embedding assigned to the previous \nword (Extended Data Fig. 6c). Next, we ran an encoding model \nusing the concatenation of the previous and current word embed\n-\ndings (Extended Data Fig. 6c). We found a significant difference \nbetween these two models before word onset. This indicates that \nthe neural responses before word onset contained information \nrelated to the next word above and beyond the contextual infor\n-\nmation embedded in the previous word. Together, these results \nsuggest that the brain is constantly engaged in the prediction  \nof upcoming words before they are perceived as it processes  \nnatural language.\nNAtURE NEUROSCiENCE | VOL 25 | MARCH 2022 | 369–380 | www.nature.com/ natureneuroscience372\nArticlesNaTUrE NEUrosCIENCE\nPredictive neural signals for listeners’ incorrect predictions. Pre-onset \nactivity associated with next-word prediction should match the pre-\ndiction content even when the prediction was incorrect. In contrast, \npost-onset activity should match the content of the incoming word, \neven if it was unpredicted. To test this hypothesis, we divided the sig\n-\nnal into correct and incorrect predictions using GPT-2 (Methods) \nand computed encoding models. We also ran the same analyses \nusing human predictions. We modeled the neural activity using: (1) \nthe GloVe embeddings of the correctly predicted words (Fig. 4b); in \nthis condition, the pre-onset word prediction matched the identity \nof the perceived incoming word; (2) the GloVe embedding for the \nincorrectly predicted words (Fig. 4b); and (3) the GloVe embedding \nElectrode coverage Encoding results (GloVe)\nEncoding model\nEmbeddings\nBeta\nweights\nActual neural signal\nReconstructed neural signal\nDimensions\nIn\nthe\nnature\nof\nboth\nWords\nhumans\nand\nmonkeys\nIn\nthe\nnature\nof\nboth\nhumans\nand r\nmonkeys\n1 2 3 50\nPredicting neural signals from word embeddings at each lag\nRHLH\na\nb\nc\nLH\nCorr. actual vs reconstructed signals\n0.49\n0.21\nCorr. (r)\nCorr. (r)\n0.07\nP < 0.01\nFDR corrected\nN = 160\n‘Monkeys’\nPt_1\nPt_2\nPt_3\nPt_4\nPt_5\nPt_6\nPt_7\nPt_8\nPt_9\nElectrode 1\nElectrode 2\nElectrode 3\nElectrode 4\nElectrode 4\n–2,000 2,0000\n0\n0.2\n0.4\nBefore After–100\nWord onset\nThreshold\nPredictive\nsignal\n(Average input: –100 ms to 100 ms)\n0\nWord onset\n200 ms\n× =\n/uni03B21\n/uni03B22\n/uni03B23\n/uni03B250\nFig. 3 | Linear encoding model used to predict the neural responses to each word in the narrative before and after word-onset. a, Brain coverage consisted \nof 1,339 electrodes (across nine participants). The words are aligned with the neural signal; each word’s onset (moment of articulation) is designated at \nlag 0. Responses are averaged over a window of 200 ms and provided as input to the encoding model. b, A series of 50 coefficients corresponding to the \nfeatures of the word embeddings is learned using linear regression to predict the neural signal across words from the assigned embeddings. The model was \nevaluated by computing the correlation between the reconstructed signal and the actual signal for a held-out test word. This procedure was repeated for \neach lag and each electrode, using a 25-ms sliding window. The dashed horizontal line indicates the statistical threshold (q < 0.01, FDR corrected). Lags of \n−100 ms or more preceding word onset contained only neural information sampled before the word was perceived (yellow). c, Electrodes with significant \ncorrelation at the peaked lag between predicted and actual word responses for semantic embeddings (GloVe). LH, left hemisphere; RH, right hemisphere.\nNAtURE NEUROSCiENCE | VOL 25 | MARCH 2022 | 369–380 | www.nature.com/ natureneuroscience 373\nArticles NaTUrE NEUrosCIENCE\nof the actual unpredictable words that humans perceived (Fig. 4b) \nbecause in the incorrect predictions condition the predicted word \ndid not match the identity of the perceived word.\nThe neural responses before word onset contained information \nabout human predictions regarding the identity of the next word. \nCrucially, the encoding was high for both correct and incorrect \npredictions (Fig. 4b and Extended Data Fig. 8). This demonstrated \nthat pre-word-onset neural activity contains information about \nwhat listeners actually predicted, irrespective of what they subse\n-\nquently perceived. Similar results were obtained using human pre -\ndictions (Extended Data Fig. 8). In contrast, the neural responses \nafter word onset contained information about the words that were \nactually perceived, irrespective of GPT-2’s predictions (Fig. 4b). The \nanalysis of the incorrect predictions unequivocally disentangles the \npre-word-onset processes associated with word prediction from the \npost-word-onset comprehension-related processes. Furthermore, it \ndemonstrates how autoregressive DLMs predictions can be used for \nmodeling human predictions at the behavioral and neural levels.\nIn summary, these multiple pieces of evidence, which are based \non encoding analyses, suggest that the brain, like autoregressive \nDLMs, is constantly predicting the next word before onset as it pro\n-\ncesses incoming natural speech. Next, we provide more evidence for \ncoupling between pre-onset prediction and post-onset surprise level \nand error signals.\nPre-onset predictions and post-onset word surprise. \nAutoregressive language models provide a unified framework for \nmodeling pre-onset next-word predictions and post-onset surprise \n \n(that is, prediction-error signals). We used pretrained GPT-2’s \ninternal estimates for each upcoming word (Fig. 1) to establish a \nconnection between pre-onset prediction and post-onset surprise \nat the neural level.\nIncreased activity for surprise 400 ms after word onset. Autoregressive \nDLMs, such as GPT-2, use their pre-onset predictions to calculate \nthe post-onset surprise level as to the identity of the incoming word. \nIt was already shown that the activation level after onset is corre\n-\nlated with the surprise level14,21–23,41. We replicated this finding in our \ndata. In addition, high-quality intracranial recordings allowed us to \nlink pre-onset confidence level and the post-onset surprise level. \nPre-onset confidence level was assessed using entropy (Methods), \nwhich is a measure of GPT-2’s uncertainty level in its prediction \nbefore word onset. High entropy indicates that the model is uncer\n-\ntain about its predictions, whereas low entropy indicates that the \nmodel is confident. Post-onset surprise level was assessed using a \ncross-entropy measure that depends on the probability assigned \nto the incoming word before it is perceived (Fig. 1 and Methods). \nAssigning a low probability to the word before word onset will result \nin a post-onset high surprise when the word is perceived, and vice \nversa for high-probability words.\nPre-onset activity (using the same 160 electrodes used for Fig. \n4a,b) increased for correct predictions, whereas, in agreement with \nprior research, post-onset activity increased for incorrect predictions \n(Fig. 5a). The activity level was averaged for all words that were cor\n-\nrectly or incorrectly predicted. We observed increased activity for \nincorrect predictions 400 ms after word onset (Fig. 5a). In addition, \nGloVe embeddings\nIncorrect predictions\nCorrect predictions\n(GPT -2’s prediction = perceived word)\nGPT -2’s prediction\nPerceived wordArbitrary embeddings\nRandom embeddings\n2,0001,0000\nWord onset (ms) Word onset (ms)\nAll wordsa b\nEncoding neural activity for individual words\nPrediction versus perception\n0\n0.1 0.1\n0.05\n0\nCorrelation\nCorrelation\n–1,000\nWord processing SurprisePrediction Prediction\n–2,000 2,000\n400 ms\n1,0000–1,000–2,000\nFig. 4 | Modeling of neural signals before and after word onset for predictable, unpredictable and incorrectly predicted words. a, Estimating neural \nsignals for all individual words from word embeddings (encodings). The encoding analysis was performed in each electrode with significant encoding for \nGloVe embeddings (n = 160), and then averaged across electrodes (see map of electrodes in Fig. 3c). The shaded regions indicate the s.e. of the encoding \nmodels across electrodes. Using arbitrary embeddings, we managed to encode information as to the identity of the incoming word before and after word \nonset. Using word embeddings (GloVe), which contain contextual information as to the relation among words in natural language, further improves the \nencoding models before and after word onset. Furthermore, we observed a robust encoding to upcoming words starting −1,000 ms before word onset. \nThe horizontal continuous black line specifies the statistical threshold. Black asterisks indicate lags for which the encoding based on GloVe embeddings \nsignificantly outperformed the encoding based on arbitrary embeddings. b, Estimating neural signals for correctly predicted words (blue), incorrectly \npredicted words (magenta) and the actual unexpected perceived word (red). Note that encoding before word onset was aligned with the content of the \npredicted words, whereas the encoding after word onset was aligned with the content of the perceived words. Moreover, we observed an increase in \nencoding performance for surprising words compared to predicted words 400 ms after word onset. Magenta asterisks represent significant differences \nbetween incorrect GPT-2 predictions (magenta line) and correct predictions (blue line). Red asterisks represent significantly higher values for incorrectly \npredicted words (red line) than correctly predicted words (blue line). Blue asterisks represent significantly higher values for correctly predicted words \n(blue line) than incorrectly predicted words (red line). The shaded regions indicate the s.e. of the encoding models across electrodes.\nNAtURE NEUROSCiENCE | VOL 25 | MARCH 2022 | 369–380 | www.nature.com/ natureneuroscience374\nArticlesNaTUrE NEUrosCIENCE\nGPT-2’s uncertainty (entropy) was negatively correlated with the \nactivation level before word onset (Fig. 5b). In other words, before \nonset, the higher the confidence (low uncertainty), the higher the \nactivation level. In contrast, after word onset, the level of surprise \n(cross-entropy) was correlated with activation, and peaked around \n400 ms (Fig. 5b). Because uncertainty correlates with surprise, we \ncomputed partial correlations between entropy, surprise and neural \nsignals. This analysis directly connects GPT-2’s internal predictions \nand neural activity before word onset and GPT-2’s internal surprise \nand the surprise (that is, prediction error) embedded in the neural \nresponses after word onset.\nIn summary, based on encoding and event-related activity, we \nintroduce multiple pieces of evidence to link pre-onset next-word \nprediction processes with post-onset surprise processes using GPT-\n2’s internal estimates. This section further supports the claim that \nautoregressive DLMs can serve a theoretical framework for lan\n-\nguage comprehension-related processes. Next, we provide more \nevidence that GPT-2 tracks human neural signals and specifically \nthat humans represent words in a context-dependent fashion, simi\n-\nlarly to DLMs.\nContextual representation. Contextual embeddings predict neural \nresponses to speech. A next-word prediction objective enables autore-\ngressive DLMs to compress a sequence of words into a contextual \nembedding from which the model decodes the next word. The pres-\nent results have established that the brain, similarly to autoregres -\nsive DLMs, is also engaged in spontaneous next-word prediction as \nit listens to natural speech. Given this shared computational prin\n-\nciple, we investigated whether the brain, like autoregressive DLMs, \ncompresses word sequences into contextual representation.\nIn natural language, each word receives its full meaning based on \nthe preceding words\n42–44. For instance, consider how the word ‘shot’ \ncan have very different meanings in different contexts, such as ‘tak-\ning a shot with the camera’ , ‘drinking a shot at the bar’ or ‘making \nthe game-winning shot’ . Static word embeddings, like GloVe, assign \none unique vector to the word ‘shot’ and, as such, cannot capture \nthe context-specific meaning of the word. In contrast, contextual \nembeddings assign a different embedding (vector) to every word as \na function of its preceding words. Here we tested whether autore\n-\ngressive DLMs that compress context into contextual embeddings \nprovide a better cognitive model for neural activity during linguis\n-\ntic processing than static embeddings. To test this, we extracted the \ncontextual embeddings from an autoregressive DLM (GPT-2) for \neach word in the story. To extract the contextual embedding of a \nword, we provided the model with the preceding sequence of all \nprior words (up to 1,024 tokens) in the podcast and extracted the \nactivation of the top embedding layer (Methods).\nLocalizing neural responses using contextual embeddings. Replacing \nstatic embeddings (GloVe) with contextual embeddings (GPT-2) \nimproved encoding model performance in the prediction of neural \nresponses to words (Fig. 6a and Extended Data Fig. 3). Encoding \nbased on contextual embeddings resulted in statistically significant \ncorrelations in 208 electrodes in the left hemisphere (and 34 in the \nright hemisphere), 71 of which were not significantly predicted by \nthe static embeddings (GloVe). The additional electrodes revealed \nby contextual embedding were mainly located in higher-order \nlanguage areas with long processing timescales along the inferior \nfrontal gyrus, temporal pole, posterior superior temporal gyrus, \nparietal lobe and angular gyrus\n37. In addition, there was a noticeable \nimprovement in the contextual embedding-based encoding model \nin the primary and supplementary motor cortices. The improve\n-\nment was seen both at the peak of the encoding model and in the \nmodel’s ability to predict neural responses to words up to 4 s before \nword onset (for the 160 electrodes with significant GloVe encod\n-\ning; Fig. 6b and Extended Data Figs. 4 and 9). The improvement \nin the ability to predict neural signals to each word while relying \non autoregressive DLM’s contextual embeddings was robust and \napparent even at the single-electrode level (Extended Data Fig. 4). \nThese results agree with concurrent studies demonstrating that con\n-\ntextual embeddings model neural responses to words better than \nstatic semantic embeddings\n15,16,45,46. Next, we asked which aspects of \nthe contextual embedding drive the improvement in modeling the \nneural activity.\n2,000 2,0001,000 1,000\n400 ms 400 ms\n–1,000\nPrediction PredictionSurprise Surprise\n–1,000\nCorrect predictions\nIncorrect predictions\nUncertainty\nSurprise (cross-entropy)\n–2,000 –2,000\n0.1\na b\n0.1\n–0.1\n00.05Power\nCorrelation\n0\n0\nWord onset (ms) Word onset (ms)\n0\nCorrelation of activation level for individual\nwords with uncertainty and surprise\nAverage activation\nCorrect and incorrect predictions\nFig. 5 | Uncertainty and surprise levels computed by GPt-2 correlate with pre-onset and post-onset neural activity respectively. a, T rigger-averaged \nactivity (and s.e.) across 160 electrodes with significant GloVe encoding for words (Fig. 3c). Averaging was performed separately for words correctly \npredicted and incorrectly predicted. Note the increase in signal activity for predictable words before onset and for unpredictable words 400 ms after \nword onset. b, Partial correlations between uncertainty (entropy) and signal power controlling for cross-entropy (green line). Partial correlations between \nsurprise (cross-entropy) and neural signals controlling for correlation with entropy (red and purple lines). Asterisks indicate correlation significance (FDR \ncorrected, q < 0.01).\nNAtURE NEUROSCiENCE | VOL 25 | MARCH 2022 | 369–380 | www.nature.com/ natureneuroscience 375\nArticles NaTUrE NEUrosCIENCE\nModeling the context versus predicting the upcoming word. The \nimproved ability to predict neural responses before word onset \nusing contextual embedding can be attributed to two related factors \nthat are absent in the static word embeddings (for example, GloVe): \n(1) the brain, like GPT-2, aggregates information about the preced\n-\ning words in the story into contextual embeddings; and (2) GPT-2 \nembeddings contain additional predictive information, not encoded \nin static embeddings, about the identity of the upcoming word in \nthe sequence. By carefully manipulating the contextual embeddings \nand developing an embedding-based decoder, we show how both \ncontext and next-word prediction contribute to contextual embed\n-\ndings’ improved ability to model the neural responses.\nRepresenting word meaning in unique contexts. Going above and \nbeyond the information encoded in GloVe, GPT-2’s capacity for \nrepresenting context captures additional information in neural \nresponses. A simple way to represent the context of prior words is to \ncombine (that is, concatenate) the static embeddings of the preced\n-\ning sequence of words. To test this simpler representation of context, \nwe concatenated GloVe embeddings for the ten preceding words \nin the text into a longer ‘context’ vector and compared the encod\n-\ning model performance to GPT-2’s contextual embeddings (after \nreducing both vectors to 50 dimensions using principal-component \nanalysis). While the concatenated static embeddings were better in \npredicting the prior neural responses than the original GloVe vec\n-\ntors, which only capture the current word, they still underperformed \nGPT-2’s encoding before word articulation (Extended Data Fig. 9). \nThis result suggests that GPT-2’s contextual embeddings are bet\n-\nter suited to compress the contextual information embedded in the \nneural responses than static embeddings (even when concatenated).\nA complementary way to demonstrate that contextual embed\n-\ndings uncover aspects of the neural activity that static embeddings \ncannot capture is to remove the unique contextual information \nfrom GPT-2 embeddings. We removed contextual information from \nGPT-2’s contextual embeddings by averaging all embeddings for \neach unique word (for example, all occurrences of the word ‘mon\n-\nkey’) into a single vector. This analysis was limited to words that \nhave at least five repetitions (Methods). Thus, we collapsed the con\n-\ntextual embedding into a ‘static’ embedding in which each unique \nword in the story is represented by one unique vector. The resulting \nembeddings were still specific to the overall topic of this particular \npodcast (unlike GloVe). Still, they did not contain the local context \nfor each occurrence of a given word (for example, the context in \nwhich ‘monkey’ was used in sentence 5 versus the context in which \nit was used in sentence 50 of the podcast). Indeed, removing con\n-\ntext from the contextual embedding by averaging the vector for each \nunique word effectively reduced the encoding model’s performance \nto that of the static GloVe embeddings (Fig. 6b).\nFinally, we examined how the specificity of the contextual infor\n-\nmation in the contextual embeddings improved the ability to model \nthe neural responses to each word. To that end, we scrambled the \nembeddings across different occurrences of the same word in the \nstory (for example, switched the embedding of the word ‘monkey’ \nin sentence 5 with the embedding for the word ‘monkey’ in sen\n-\ntence 50). This manipulation tests whether contextual embeddings \nare necessary for modeling neural activity for a specific sequence \nof words. Scrambling the same word occurrences across contexts \nsubstantially reduced the encoding model performance (Fig. 6b), \npointing to the contextual dependency represented in the neural \nsignals. Taken together, these results suggest that contextual embed\n-\ndings provide us with a new way to model the context-dependent \nneural representations of words in natural contexts.\nPredicting words from neural signal using contextual embeddings. \nFinally, we applied a decoding analysis to demonstrate that, in addi\n-\ntion to better modeling the neural responses to context, contextual \nembeddings also improved our ability to read information from the \nneural responses as to the identity of upcoming words. This dem\n-\nonstrates the duality of representing the context and the next-word \nprediction in the brain.\nThe encoding model finds a mapping from the embedding space \nto the neural responses that is used during the test phase for predict\n-\ning neural responses. The decoding analysis inverts this procedure \nto find a mapping from neural responses, across multiple electrodes \nand time points, to the embedding space\n47. This decoding analy -\nsis provides complementary insights to the encoding analysis by \naggregating across electrodes and quantifies how much predictive \ninformation about each word’s identity is embedded in the spatio\n-\ntemporal neural activity patterns before and after word onset.\nThe decoding analysis was performed in two steps. First, we \ntrained a deep convolutional neural network to aggregate neural \nresponses (Fig. 7a and Appendix I) and mapped this neural signal \nto the arbitrary, static (GloVe-based) and to the contextual (GPT-\n2-based) embedding spaces (Fig. 7b). To conservatively compare the \n–2,000 2,000Before\n0.2\nba Embeddings (50 d)\nContextual (GPT -2)\nStatic (GloVe)\nAveraged context (within label)\nShuffled context (within label)\n0.1\nCorr. (r)\nCorr. (r)\n0\nLH\n0.49\n0.21\n0.07\nP < 0.01\nFDR corrected N = 208\n0\nOnset (ms)\nAfter\nFig. 6 | Contextual (GPt-2) embeddings improve the modeling of neural responses before word onset. a, Peak correlation between predicted and actual \nword responses for the contextual (GPT-2) embeddings. Using contextual embeddings significantly improved the encoding model’s ability to predict \nthe neural signals for unseen words across many electrodes. b, Encoding model performance for contextual embeddings (GPT-2) aggregated across \n160 electrodes with significant encoding for GloVe (Fig. 3c): contextual embeddings (purple), static embeddings (GloVe, blue), contextual embeddings \naveraged across all occurrences of a given word (orange), contextual embeddings shuffled across context-specific occurrence of a given word (black). The \nshaded regions indicate the s.e. of the encoding models across electrodes.\nNAtURE NEUROSCiENCE | VOL 25 | MARCH 2022 | 369–380 | www.nature.com/ natureneuroscience376\nArticlesNaTUrE NEUrosCIENCE\nperformance of GPT-2-based embedding to GloVe-based embed -\nding, we used as input the signal from the electrodes that were \nfound to be significant for GloVe-based encoding. To further ensure \nthat the decoding results were not affected by the electrode selec\n-\ntion procedure, for each test fold, we selected the electrodes using \nthe remaining 80% of the data (Methods). To obtain a reliable esti\n-\nmation of accuracy per word label we included words with at least \nfive repetitions, which included 69% of the words in the story (for \nthe full list of words, see Appendix II). Second, the predicted word \nembeddings were used for word classification based on their cosine \ndistance from all embeddings in the dataset (Fig. 7c). Although we \nevaluated the decoding model using classification, the classifier pre\n-\ndictions were constrained to rely only on the embedding space. This \nis a more conservative approach than an end-to-end word classifi\n-\ncation, which may capitalize on acoustic information in the neural \nsignals that are not encoded in the language models.\nUsing a contextual decoder greatly improved our ability to \nclassify word identity over decoders relying on static or arbitrary \nembeddings (Fig. 8). We evaluated classification performance using \nthe receiver operating characteristic (ROC) analysis with corre\n-\nsponding area under the curve (AUC). A model that only learns to \nuse word frequency statistics (for example, only guessing the most \nfrequent word) will result in a ROC-AUC curve that falls on the \ndiagonal line (AUC = 0.5) suggesting that the classifier does not dis\n-\ncriminate between the words48. Classification using GPT-2 (average \nAUC of 0.74 for lag 150) outperformed GloVe and arbitrary embed-\ndings (average AUC of 0.68 and 0.68, respectively) before and after \nword onset. To compare the performance of the classifiers based on \nGPT-2 and GloVe at each lag, we performed a paired-sample t-test \nbetween the AUCs of the words in the two models. Each unique \nword (class) in each lag had an AUC value computed from the \nGloVe-based model and an AUC value computed from the GPT-\n2-based model. The results were corrected for multiple tests by con\n-\ntrolling the false discovery rate (FDR)49.\nA closer inspection of the GPT-2-based decoder indicated that \nthe classifier managed to detect reliable information about the iden-\ntity of words several hundred milliseconds before word onset (Fig. 8).  \nIn particular, starting at about −1,000 ms before word onset, when \nConv\n1D\n3 2 2\n160 × 3\n160 160 160\n160 × 2 160 × 2\nReLu Batch Dropout\n20% 20%\nConv\n1D\nReLu Batch BatchDropout Locally\nconnected\nReLu\nInput\nDecoding model\nInput to the modela\nb\nc\nPredicting word embeddings from neural signals\nConvolution layer 1 Convolution layer 2\nMax-pooling\nLocally connected layer\nWord onset\n0 62.5–300 ms\nInput\nLH\n300 ms\nElectrode 1\n‘Monkeys’\nElectrode 2\nElectrode 159\nElectrode 160\nWord\nembeddings\nMSE\n(*Split 80% for training, 20% for testing—5 folds)\n50 d50 d50 d\nMonkeys\nCosine\nCourtClassification\nTime\n(10 ×  62.5-ms bins)\nSpace\n(160 elect.)\nROC-AUC\nTree\nPredicted\nembedding\nObjective\nfunction\nFully connected\nGlobal max-pooling\nNorm\nlayer\nDense\nlayer\nFig. 7 | Deep nonlinear decoding model used to predict words from neural responses before and after word onset. a, Neural data from left-hemisphere \nelectrodes with significant encoding model performance using GloVe embeddings were used as input to the decoding model. For each fold, electrode \nselection was performed on 80% of the data that were not used for testing the model. The stimulus was segmented into individual words and aligned to \nthe brain signal at each lag. b, Schematic of the feed-forward deep neural network model that learns to project the neural signals for the words into the \narbitrary embedding, static semantic embedding (GloVe) or contextual embedding (GPT-2) space (Appendix I). The input (currently represented as a \n160 × 10 matrix) changes its dimensions for each of the five folds based on the number of significant electrodes for each fold. The model was trained to \nminimize the mean squared error (MSE) when mapping the neural signals into the embedding space. c, The decoding model was evaluated using a word \nclassification task. The quality of word classification is based on the embedding space used to construct ROC-AUC scores. This enabled us to assess how \nmuch information about specific words is extractible from the neural activity via the linguistic embedding space.\nNAtURE NEUROSCiENCE | VOL 25 | MARCH 2022 | 369–380 | www.nature.com/ natureneuroscience 377\nArticles NaTUrE NEUrosCIENCE\nthe neural signals were integrated across a window of 625 ms, the \nclassifier detected predictive information about the next word’s \nidentity. The information about the next word’s identity gradually \nincreased and peaked at an average AUC of 0.74 at a lag of 150 ms \nafter word onset, when the signal was integrated across a window \nfrom −162.5 ms to 462.5 ms. GloVe embeddings showed a similar \ntrend with a marked reduction in classifier performance (Fig. 8). \nThe decoding model’s capacity to classify words before word onset \ndemonstrates that the neural signal contains a considerable amount \nof predictive information about the meaning of the next word, up \nto a second before it is perceived. At longer lags (more than 2 s), all \ndecoders’ performance dropped to chance.\nDiscussion\nDLMs provide a new modeling framework that drastically departs \nfrom classical psycholinguistic models. They are not designed \nto learn a concise set of interpretable syntactic rules to be imple\n-\nmented in novel situations, nor do they rely on part of speech con -\ncepts or other linguistic terms. Rather, they learn from surface-level \nlinguistic behavior to predict and generate the contextually appro\n-\npriate linguistic outputs. The current paper provides compelling \nbehavioral and neural evidence for shared computational principles \nbetween the way the human brain and autoregressive DLMs process \nnatural language.\nSpontaneous prediction as a keystone of language processing. \nAutoregressive DLMs learn according to the simple self-supervised \nobjective of context-based next-word prediction. The extent to \nwhich humans are spontaneously engaged in next-word predictions \nas they listen to continuous, minutes-long, natural speech has been \nunderspecified. Our behavioral results revealed a robust capacity for \nnext-word prediction in real-world stimuli, which matches a mod\n-\nern autoregressive DLM (Fig. 2). Neurally, our findings demonstrate \nthat the brain constantly and actively represents forthcoming words \nin context during listening to natural speech. The predictive neural \nsignals are robust, and can be detected hundreds of milliseconds \nbefore word onset. Notably, the next-word prediction processes are \nassociated with listeners’ contextual expectations and can be dis\n-\nsociated from the processing of the actually perceived words after \nword onset (Fig. 4b and Extended Data Fig. 8).\nPre-onset prediction is coupled with post-onset surprise. \nAutoregressive DLMs (like GPT-2) provide a unified computational \nframework that connects pre-onset word prediction with post-onset \nsurprise (error signals). Our results show that we can rely on GPT-\n2’s internal pre-onset confidence (entropy) and post-onset surprise \n(cross-entropy) to model the brain’s internal neural activity as it \nprocesses language. The correlations between GPT-2’s internal sur\n-\nprise level and the neural signals corroborate the link between the \ntwo systems\n50.\nContext-specific meaning as a keystone of language processing. \nAs each word attains its full meaning in the context of preceding \nwords over multiple timescales, language is fundamentally con -\ntextual51. Even a single change to one word or one sentence at the \nbeginning of a story can alter the neural responses to all subsequent \nsentences\n43,52. We demonstrated that the contextual word embed -\ndings learned by DLMs provide a new way to compress linguistic \ncontext into a numeric vector space, which outperforms the use of \nstatic semantic embeddings (Figs. 6b and 8 and Extended Data Figs. \n4 and 9). While static embeddings and contextual embeddings are \ndifferent, our neural results also hint at how they relate to each other. \nOur results indicate that both static and contextual embeddings can \npredict neural responses to single words in many language areas\n16 \nalong the superior temporal cortex, parietal lobe and inferior frontal \ngyrus. Switching from static to contextual embeddings boosted our \nability to model neural responses during natural speech process\n-\ning across many of these brain areas. Finally, averaging contextual \nembeddings associated with a given word removed the contextual \ninformation and effectively changed GPT-2’s contextual embedding \nback into static word embeddings (Fig. 6b). Taken together, these \nresults suggest that the brain is coding for the semantic relation\n-\nship among words contained in static embeddings while also being \ntuned to the unique contextual relationship between the specific \nword and the preceding words in the sequence\n53.\nUsing an autoregressive language model as a cognitive model. We \ndescribe three shared computational principles that reveal a strong \nlink between the way the brain and DLMs process natural language. \nThese shared computational principles, however, do not imply that \nthe human brain and DLMs implement these computations in a \n–10,000–8,000–6,000–4,000–2,000–1,800–1,600–1,400–1,200–1,000–900–800–700–600–500–400–300–200–100\n0 1002003004005006007008009001,0001,2001,4001,6001,8002,0004,0006,0008,00010,000\n0.50\n100%\n0%\nProportion before Proportion after\n0.55\n0.60ROC-AUC\n0.65\n0.70\n0.75\nWord-level classification\nEmbeddings (50 d)\nContextual (GPT-2)\nStatic (GloVe)\nArbitrary\nWord onset (ms)\nFig. 8 | Using a decoding model for classification of words before and after word onset. Word-level classification. Classification performance for \ncontextual (GPT-2), static (GloVe) and arbitrary (green) embeddings. The averaged values were weighted by the frequency of the words in the test \nset. The x-axis labels indicate the center of each 625-ms window used for decoding at each lag (between −10 and 10 s). The colored stripe indicates \nthe proportion of pre-word onset (yellow) and post-word onset (blue) time points in each lag. Shaded regions denote the s.e. across five test folds. \nNote that contextual embeddings improved classification performance over GloVe both before and after word onset. Significance was assessed using a \npaired-sample t-test of the AUCs for each unique word, comparing the AUCs of the GloVe-based decoding and GPT-2-based decoding. The comparison \nwas performed for each leg separately and results were FDR corrected (q < .01).\nNAtURE NEUROSCiENCE | VOL 25 | MARCH 2022 | 369–380 | www.nature.com/ natureneuroscience378\nArticlesNaTUrE NEUrosCIENCE\nsimilar way. For example, many state-of-the-art DLMs rely on trans-\nformers, a type of neural network architecture developed to solve \nsequence transduction. Transformers are designed to parallelize a \ntask that is largely computed serially, word by word, in the human \nbrain. Therefore, while transformer models are an impressive engi\n-\nneering achievement, they are not biologically feasible. Many other \nways, however, are possible to transduce a sequence into a con -\ntextual embedding vector. To the extent that the brain relies on a \nnext-word prediction objective to learn how to use language in con\n-\ntext, it likely uses a different implementation54.\nPsycholinguistic models versus deep language models. DLMs \nwere engineered to solve a fundamentally different problem than \npsycholinguistic language models. Psycholinguistic language mod-\nels aim to uncover a set of generative (learned or innate) rules to be \nused in infinite, novel situations\n55. Finding a set of linguistic rules, \nhowever, was challenging. There are numerous exceptions for every \nrule, conditioned by discourse context, meaning, dialect, genre, \nand many other factors\n51,56–58. In contrast, DLMs aim to provide the \nappropriate linguistic output given the prior statistics of language \nuse in similar contexts 20,59. In other words, psycholinguistic theo -\nries aim to describe observed language in terms of a succinct set \nof explanatory constructs. DLMs, in contrast, are performance ori\n-\nented and are focused on learning how to generate formed linguistic \noutputs as a function of context while de-emphasizing interpret\n-\nability60. The reliance on performance creates an interesting con -\nnection between DLMs and usage (context)-based constructionist \napproaches to language\n58,61. Furthermore, DLMs avoid the circular-\nity built into many psycholinguistic language models that rely on \nlinguistic terms to explain how language is encoded in neural sub\n-\nstrates19,62. Remarkably, the internal contextual embedding space in \nDLMs can capture many aspects of the latent structure of human \nlanguage, including syntactic trees, voice, co-references, morphol\n-\nogy and long-range semantic and pragmatic dependencies 1,63,64. \nThis discussion demonstrates the power (over the more traditional \napproaches) of applying brute-force memorization and interpola\n-\ntion for learning how to generate the appropriate linguistic outputs \nin light of prior contexts\n20.\nObservational work in developmental psychology suggests that \nchildren are exposed to tens of thousands of words in contextu -\nalized speech each day, creating a large data volume available for \nlearning\n65,66. The capacity of DLMs to learn language relies on \ngradually exposing the model to millions of real-life examples. Our \nfinding of spontaneous predictive neural signals as participants lis\n-\nten to natural speech suggests that active prediction may underlie \nhumans’ lifelong language learning. Future studies, however, will \nhave to assess whether these cognitively plausible, prediction-based \nfeedback signals are at the basis of human language learning and \nwhether the brain is using such predictive signals to guide language \nacquisition. Furthermore, as opposed to autoregressive DLMs, it is \nlikely that the brain relies on additional simple objectives at differ\n-\nent timescales to facilitate learning20,67.\nConclusion\nThis paper provides evidence for three shared core computational \nprinciples between DLMs and the human brain. While DLMs may \nprovide a building block for our high-level cognitive faculties, they \nundeniably lack certain central hallmarks of human cognition. \nLinguists were primarily interested in how we construct well-formed \nsentences, exemplified by the famous grammatically correct but \nmeaningless sentence ‘colorless green ideas sleep furiously’ , com\n-\nposed by Noam Chomsky 2. Similarly, DLMs are generative in the \nnarrow linguistic sense of being able to generate new sentences that \nare grammatically, semantically and even pragmatically well-formed \nat a superficial level. However, although language may play a \ncentral organizing role in our cognition, linguistic competence \n \nis insufficient to capture thinking. Unlike humans, DLMs cannot \nthink, understand or generate new meaningful ideas by integrating \nprior knowledge. They simply echo the statistics of their input\n68. \nGoing beyond the importance of language as having a central orga-\nnizing role in our cognition, DLMs indicate that linguistic compe -\ntence may be insufficient to capture thinking. A core question for \nfuture studies in cognitive neuroscience and machine learning is \nhow the brain can leverage predictive, contextualized linguistic rep\n-\nresentations, like those learned by DLMs, as a substrate for generat-\ning and articulating new thoughts.\nOnline content\nAny methods, additional references, Nature Research report -\ning summaries, source data, extended data, supplementary infor -\nmation, acknowledgements, peer review information; details of \nauthor contributions and competing interests; and statements of \ndata and code availability are available at https://doi.org/10.1038/\ns41593-022-01026-4.\nReceived: 13 January 2021; Accepted: 27 January 2022;  \nPublished online: 7 March 2022\nReferences\n 1. Linzen, T. & Baroni, M. Syntactic structure from deep learning. Annu. Rev. \nLinguist. 7, 195–212 (2021).\n 2. Chomsky, N. Syntactic structures. https://doi.org/10.1515/9783112316009 \n(1957).\n 3. Jacobs, R. A. & Rosenbaum, P . S. English Transformational Grammar \n(Blaisdell, 1968).\n 4. Brown, T. B. et al. Language models are few-shot learners. Adv. Neural Inf. \nProcess. Syst. 33, 1877–1901 (2020).\n 5. Cho, W . S. et al. Towards coherent and cohesive long-form text generation. in \nProceedings of the First Workshop on Narrative Understanding https://doi.\norg/10.18653/v1/w19-2401 (2019).\n 6. Y ang, Z. et al. XLNet: generalized autoregressive pretraining for language \nunderstanding. in Advances in Neural Information Processing Systems (eds. \nWallach, H. et al.) 5753–5763 (Curran Associates, 2019).\n 7. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving language \nunderstanding by generative pre-training. OpenAI Blog (2018).\n 8. Radford, A. et al. Language models are unsupervised multitask learners. \nOpenAI Blog (2019).\n 9. Rosset, C. Turing-nlg: A 17-billion-parameter language model by microsoft. \nMicrosoft Blog (2019).\n 10. Pereira, F . et al. Toward a universal decoder of linguistic meaning from brain \nactivation. Nat. Commun. 9, 963 (2018).\n 11. Makin, J. G., Moses, D. A. & Chang, E. F . Machine translation of cortical \nactivity to text with an encoder–decoder framework. Nat. Neurosci. 23, \n575–582 (2020).\n 12. Schwartz, D. et al. Inducing brain-relevant bias in natural language processing \nmodels. in Advances in Neural Information Processing Systems (eds. Wallach, \nH. et al.) 14123–14133 (Curran Associates, 2019).\n 13. Gauthier, J. & Levy, R. Linking artificial and human neural representations of \nlanguage. in Proceedings of the 2019 Conference on Empirical Methods in \nNatural Language Processing and the 9th International Joint Conference  \non Natural Language Processing https://doi.org/10.18653/v1/d19-1050  \n(2019).\n 14. Donhauser, P . W . & Baillet, S. Two distinct neural timescales for predictive \nspeech processing. Neuron 105, 385–393 (2020).\n 15. Jain, S. & Huth, A. G. Incorporating context into language encoding models \nfor fMRI. in Advances in Neural Information Processing Systems https://doi.\norg/10.1101/327601 (2018).\n 16. Schrimpf, M. et al. Artificial neural networks accurately predict language \nprocessing in the Brain. Cold Spring Harbor Laboratory  https://doi.\norg/10.1101/2020.06.26.174482 (2020).\n 17. Caucheteux, C., Gramfort, A. & King, J. -R. GPT-2’s activations predict the \ndegree of semantic comprehension in the human brain. https://doi.\norg/10.1101/2021.04.20.440622 (2021).\n 18. Athanasiou, N., Iosif, E. & Potamianos, A. Neural activation semantic models: \ncomputational lexical semantic models of localized neural activations. in \nProceedings of the 27th International Conference on Computational Linguistics \n2867–2878 (Association for Computational Linguistics, 2018).\n 19. McClelland, J. L., Hill, F ., Rudolph, M., Baldridge, J. & Schütze, H. Placing \nlanguage in an integrated understanding system: next steps toward \nhuman-level performance in neural language models. Proc. Natl Acad. Sci. \nUSA 117, 25966–25974 (2020).\nNAtURE NEUROSCiENCE | VOL 25 | MARCH 2022 | 369–380 | www.nature.com/ natureneuroscience 379\nArticles NaTUrE NEUrosCIENCE\n 20. Hasson, U., Nastase, S. A. & Goldstein, A. Direct fit to nature: an \nevolutionary perspective on biological and artificial neural networks. Neuron  \n105, 416–434 (2020).\n 21. Heilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P . & de Lange, F . P . A \nhierarchy of linguistic predictions during natural language comprehension. \nPreprint at bioRxiv https://doi.org/10.1101/2020.12.03.410399 (2020).\n 22. Weissbart, H., Kandylaki, K. D. & Reichenbach, T. Cortical tracking of \nsurprisal during continuous speech comprehension. J. Cogn. Neurosci. 32, \n155–166 (2020).\n 23. Frank, S. L., Otten, L. J., Galli, G. & Vigliocco, G. The ERP response to the \namount of information conveyed by words in sentences. Brain Lang.  140, \n1–11 (2015).\n 24. Caucheteux, C., Gramfort, A. & King, J.-R. GPT-2’s activations predict the \ndegree of semantic comprehension in the human brain. Preprint at bioRxiv \nhttps://doi.org/10.1101/2021.04.20.440622 (2021).\n 25. Lewis, M. et al. BART: denoising sequence-to-sequence pretraining for \nnatural language generation, translation, and comprehension. Preprint at \nhttps://arxiv.org/abs/1910.13461 (2019).\n 26. Huang, Y . & Rao, R. P . N. Predictive coding. Wiley Interdiscip. Rev. Cogn. Sci. \n2, 580–593 (2011).\n 27. Lupyan, G. & Clark, A. Words and the world: predictive coding and the \nlanguage–perception–cognition interface. Curr. Dir. Psychol. Sci. 24, 279–284 \n(2015).\n 28. Barron, H. C., Auksztulewicz, R. & Friston, K. Prediction and memory: a \npredictive coding account. Prog. Neurobiol. 192, 101821 (2020).\n 29. Goldstein, A., Rivlin, I., Goldstein, A., Pertzov, Y . & Hassin, R. R. Predictions \nfrom masked motion with and without obstacles. PLoS ONE 15, e0239839 \n(2020).\n 30. Clark, A. Whatever next? Predictive brains, situated agents, and the future of \ncognitive science. Behav. Brain Sci. 36, 181–204 (2013).\n 31. Taylor, W . L. ‘Cloze Procedure’: a new tool for measuring readability. Journal \nQ. 30, 415–433 (1953).\n 32. Kliegl, R., Nuthmann, A. & Engbert, R. Tracking the mind during reading: \nthe influence of past, present, and future words on fixation durations. J. Exp. \nPsychol. Gen. 135, 12–35 (2006).\n 33. Laurinavichyute, A. K., Sekerina, I. A., Alexeeva, S., Bagdasaryan, K. & Kliegl, \nR. Russian sentence corpus: benchmark measures of eye movements in \nreading in Russian. Behav. Res. Methods  51, 1161–1178 (2019).\n 34. Kutas, M. & Federmeier, K. D. Thirty years and counting: finding meaning in \nthe N400 component of the event-related brain potential (ERP). Annu. Rev. \nPsychol. 62, 621–647 (2011).\n 35. Kutas, M. & Hillyard, S. A. Reading senseless sentences: brain potentials \nreflect semantic incongruity. Science 207, 203–205 (1980).\n 36. Chivvis & Dana. ‘So a Monkey and a Horse Walk Into a Bar’ (2017).\n 37. Hasson, U., Chen, J. & Honey, C. J. Hierarchical process memory: memory as \nan integral component of information processing. Trends Cogn. Sci. 19, \n304–313 (2015).\n 38. Pennington, J., Socher, R. & Manning, C. GloVe: global vectors for word \nrepresentation. in Proceedings of the 2014 Conference on Empirical Methods in \nNatural Language Processing 1532–1543 (Association for Computational \nLinguistics, 2014).\n 39. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. & Dean, J. Distributed \nRepresentations of Words and Phrases and their Compositionality. in \nAdvances in Neural Information Processing Systems (eds. Burges et al.) \n3111–3119 (Curran Associates, 2013).\n 40. van Gerven, M. A. J. A primer on encoding models in sensory neuroscience. \nJ. Math. Psychol. 76, 172–183 (2017).\n 41. Willems, R. M., Frank, S. L., Nijhof, A. D., Hagoort, P . & van den Bosch, A. \nPrediction during natural language comprehension. Cereb. Cortex 26, \n2506–2516 (2016).\n 42. Chen, J., Hasson, U. & Honey, C. J. Processing timescales as an organizing \nprinciple for primate cortex. Neuron 88, 244–246 (2015).\n 43. Y eshurun, Y ., Nguyen, M. & Hasson, U. Amplification of local changes along \nthe timescale processing hierarchy. Proc. Natl Acad. Sci. USA 114, 9475–9480 \n(2017).\n 44. Hasson, U., Y ang, E., Vallines, I., Heeger, D. J. & Rubin, N. A hierarchy of \ntemporal receptive windows in human cortex. J. Neurosci. 28, 2539–2550 (2008).\n 45. Wehbe, L., Vaswani, A., Knight, K. & Mitchell, T. Aligning context-based \nstatistical models of language with brain activity during reading. in \nProceedings of the 2014 Conference on Empirical Methods in Natural Language \nProcessing 233–243 (Association for Computational Linguistics, 2014).\n 46. Toneva, M. & Wehbe, L. Interpreting and improving natural-language \nprocessing (in machines) with natural language-processing (in the brain). in \n33rd Conference on Neural Information Processing Systems (2019).\n 47. Naselaris, T., Kay, K. N., Nishimoto, S. & Gallant, J. L. Encoding and \ndecoding in fMRI. Neuroimage 56, 400–410 (2011).\n 48. Mandrekar, J. N. Receiver operating characteristic curve in diagnostic test \nassessment. J. Thorac. Oncol. 5, 1315–1316 (2010).\n 49. Benjamini, Y . & Hochberg, Y . Controlling the false discovery rate: a practical \nand powerful approach to multiple testing. J. R. Stat. Soc. Ser. B Stat. \nMethodol. 57, 289–300 (1995).\n 50. Schwartz, D. & Mitchell, T. Understanding language-elicited EEG data by \npredicting it from a fine-tuned language model. Proceedings of the Conference \nof the North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies 1, 43–57 (2019).\n 51. Goldberg, A. E. Explain Me This: Creativity, Competition, and the Partial \nProductivity of Constructions (Princeton University Press, 2019).\n 52. Y eshurun, Y . et al. Same story, different story. Psychol. Sci. 28, 307–319 \n(2017).\n 53. Ethayarajh, K. How contextual are contextualized word representations? \nComparing the geometry of BERT, ELMo, and GPT-2 embeddings. Preprint \nat https://arxiv.org/abs/1909.00512 (2019).\n 54. Richards, B. A. et al. A deep learning framework for neuroscience. Nat. \nNeurosci. 22, 1761–1770 (2019).\n 55. Chomsky, N. Aspects of the theory of syntax. https://doi.org/10.21236/\nad0616323 (1964).\n 56. Ten Hacken, P . Andrew Radford. Syntactic Theory and the Structure of \nEnglish: a minimalist approach (Cambridge University Press, 1997). Andrew \nRadford. Syntax: a Minimalist Introduction. (Cambridge University Press, \n1997). Natural Language Engineering, 7, 87–97 (2001).\n 57. Boer, Bde & de Boer, B. The atoms of language: the mind’s hidden rules of \ngrammar; foundations of language: brain, meaning, grammar, evolution. Artif. \nLife 9, 89–91 (2003).\n 58. Bybee, J. & McClelland, J. L. Alternatives to the combinatorial paradigm of \nlinguistic theory based on domain general principles of human cognition. The \nLinguistic Review 22, 381–410 (2005).\n 59. Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L. & Lewis, M. \nGeneralization through memorization: nearest neighbor language models. in \nInternational Conference on Learning Representations (2020).\n 60. Breiman, L. Statistical modeling: the two cultures (with comments and a \nrejoinder by the author). SSO Schweiz. Monatsschr. Zahnheilkd. 16, 199–231 \n(2001).\n 61. Goldberg, A. E. Explain me This: Creativity, Competition, and the Partial \nProductivity of Constructions (Princeton University Press, 2019).\n 62. Hasson, U., Egidi, G., Marelli, M. & Willems, R. M. Grounding the \nneurobiology of language in first principles: the necessity of \nnon-language-centric explanations for language comprehension. Cognition \n180, 135–157 (2018).\n 63. Manning, C. D., Clark, K., Hewitt, J., Khandelwal, U. & Levy, O. Emergent \nlinguistic structure in artificial neural networks trained by self-supervision. \nProc. Natl Acad. Sci. USA 117, 30046–30054 (2020).\n 64. Mamou, J. et al. Emergence of separable manifolds in deep language \nrepresentations. ICML (2020).\n 65. Hart, B. & Risley, T. R. Meaningful Differences In The Everyday Experience of \nYoung American Children (Brookes Publishing, 1995).\n 66. Weisleder, A. & Fernald, A. Talking to children matters: early language \nexperience strengthens processing and builds vocabulary. Psychol. Sci. 24, \n2143–2152 (2013).\n 67. Tan, H. & Bansal, M. Vokenization: improving language understanding with \ncontextualized, visual-grounded supervision. EMNLP (2020).\n 68. Marcus, G. F . The Algebraic Mind: Integrating Connectionism and Cognitive \nScience (MIT Press, 2019).\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to \nthe Creative Commons license, and indicate if changes were made. The images or other \nthird party material in this article are included in the article’s Creative Commons license, \nunless indicated otherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons license and your intended use is not permitted by statu-\ntory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this license, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2022\nNAtURE NEUROSCiENCE | VOL 25 | MARCH 2022 | 369–380 | www.nature.com/ natureneuroscience380\nArticlesNaTUrE NEUrosCIENCE\nMethods\nTranscription and alignment. The stimuli for the behavioral and \nelectrocorticography experiments were extracted from the story ‘So a Monkey and \na Horse Walk Into a Bar: Act One, Monkey in the Middle’ . The story was manually \ntranscribed. Sounds such as laughter, breathing, lip smacking, applause and silent \nperiods were also marked to improve the alignment’s accuracy. The audio was \ndownsampled to 11 kHz and the Penn Phonetics Lab Forced Aligner was used  \nto automatically align the audio to the transcript\n69. After automatic alignment  \nwas complete, the alignment was manually evaluated and improved by an \nindependent listener.\nBehavioral word-prediction experiment. To obtain a continuous measure of \nprediction, we developed a sliding-window behavioral paradigm where healthy \nadult participants made predictions for each upcoming word in the story. A total \nof 300 participants completed a behavioral experiment on Mechanical Turk for a \nfee of $10 (data about age and gender were not collected). We divided the story \ninto six segments and recruited six nonoverlapping groups of 50 participants to \npredict every upcoming word within each segment of the story. The first group was \nexposed to the first two words in the story and then asked to predict the upcoming \nword. After entering their prediction, the actual next word was revealed, and \nparticipants were asked again to predict the next upcoming word in the story. Once \nten words were displayed on the screen, the left-most word was removed and the \nnext word was presented (Fig. 2b). The procedure was repeated, using a sliding \nwindow until the first group provided predictions for each word in the story’s \nfirst segment. Each of the other five groups listened uninterruptedly to the prior \nsegments of the narrative and started to predict the next word at the beginning of \ntheir assigned segments. Due to a technical error, data for 33 words were omitted, \nand thus the final data contained 5,078 words. Importantly, before calculating the \nscores we used Excel’s spellchecker to locate and correct spelling mistakes.\nn-gram models. We trained 2- to 5-gram models using the NLTK Python package \nand its built-in ‘Brown’ corpus. All punctuations were removed and letters \nlowercased. We trained separate models using no-smoothing, Laplace smoothing \nor Kneser–Ney smoothing. Then we used each model to extract the probability of a \nword given its preceding n − 1 context in the podcast transcript. We also extracted \nthe most likely next word prediction to compare agreement with human responses.\nElectrocorticography experiment. Ten participants (five females, aged 20–48 years) \nlistened to the same story stimulus from beginning to end. Participants were \nnot explicitly made aware that we would be examining word prediction in our \nsubsequent analyses. One participant was removed from further analyses due to \nexcessive epileptic activity and low signal-to-noise ratio across all experimental \ndata collected. All participants volunteered for this study via the New Y ork \nUniversity School of Medicine Comprehensive Epilepsy Center. All participants \nhad elected to undergo intracranial monitoring for clinical purposes and provided \noral and written informed consent before study participation, according to the New \nY ork University Langone Medical Center Institutional Review Board. Participants \nwere informed that participation in the study was unrelated to their clinical care \nand that they could withdraw from the study at any point without affecting their \nmedical treatment.\nFor each participant, electrode placement was determined by clinicians based \non clinical criteria. One participant consented to have a US Food and Drug \nAdministration-approved hybrid clinical-research grid implanted, which includes \nstandard clinical electrodes as well as additional electrodes in between clinical \ncontacts. The hybrid grid provides a higher spatial coverage without changing \nclinical acquisition or grid placement. Across all participants, 1,106 electrodes were \nplaced on the left hemisphere and 233 on the right hemisphere. Brain activity was \nrecorded from a total of 1,339 intracranially implanted subdural platinum–iridium \nelectrodes embedded in silastic sheets (2.3-mm-diameter contacts, Ad-Tech \nMedical Instrument; for the hybrid grids 64 standard contacts had a diameter of \n2 mm and additional 64 contacts were 1 mm in diameter, PMT). Decisions related \nto electrode placement and invasive monitoring duration were determined solely \non clinical grounds without reference to this or any other research study. Electrodes \nwere arranged as grid arrays (8 × 8 contacts, 10- or 5-mm center-to-center spacing), \nor linear strips.\nRecordings from grid, strip and depth electrode arrays were acquired using \none of two amplifier types: NicoletOne C64 clinical amplifier (Natus Neurologics), \nband-pass filtered from 0.16–250 Hz, and digitized at 512 Hz; NeuroWorks \nQuantum Amplifier recorded at 2,048 Hz, high-pass filtered at 0.01 Hz and then \nresampled to 512 Hz. Intracranial electroencephalography signals were referenced \nto a two-contact subdural strip facing toward the skull near the craniotomy site. \nAll electrodes were visually inspected, and those with excessive noise artifacts, \nepileptiform activity, excessive noise or no signal were removed from subsequent \nanalysis (164 of 1,065 electrodes removed).\nPresurgical and postsurgical T1-weighted magnetic resonance imaging (MRI) \nscans were acquired for each participant, and the location of the electrodes relative \nto the cortical surface was determined from co-registered magnetic resonance \nimaging or computed tomography scans following the procedure described by \nY ang and colleagues\n70. Co-registered, skull-stripped T1 images were nonlinearly \nregistered to an MNI152 template and electrode locations were then extracted \nin Montreal Neurological Institute space (projected to the surface) using the \nco-registered image. All electrode maps were displayed on a surface plot of the \ntemplate, using the electrode localization toolbox for MATLAB available at https://\ngithub.com/HughWXY/ntools_elec/.\nPreprocessing. Data analysis was performed using the FieldTrip toolbox\n71, along \nwith custom preprocessing scripts written in MATLAB 2019a (MathWorks). In \ntotal, 66 electrodes from all participants were removed due to faulty recordings. \nThe analyses described are at the electrode level. Large spikes exceeding four \nquartiles above and below the median were removed and replacement samples \nwere imputed using cubic interpolation. We then re-referenced the data to account \nfor shared signals across all channels using either the common average referencing \nmethod\n71,72 or an independent component analysis-based method73 (based on the \nparticipant’s noise profile). High-frequency broadband power frequency provided \nevidence for a high positive correlation between local neural firing rates and high \ngamma activity\n74.\nBroadband power was estimated using six-cycle wavelets to compute the \npower of the 70–200 Hz band, excluding 60, 120 and 180 Hz line noise. Power \nwas further smoothed with a Hamming window with a kernel size of 50 ms. To \npreserve the temporal structure of the signal, we used zero-phase symmetric \nfilters. The estimate of the broadband power using wavelets and symmetric filters, \nby construction, induces some temporal uncertainty, given that information over \ntens of milliseconds is combined. The amount of temporal uncertainty, however, is \nsmall relative to the differences between pre-onset and post-onset effects reported \nin the paper. First, as the wavelet computation was done using six cycles and the \nlower bound of the gamma band was 70 Hz, the wavelet computation introduces \na 6/70-s uncertainty window centered at each time point. Thus, there is a leak \nfrom no more than 43 ms of future signal to data points in the preprocessed signal. \nSecond, the smoothing procedure applied to the broadband power introduces \na leak of up to 50 ms from the future. Overall, the leak from the future is at a \nmaximum of 93 ms. As recommended by Cheveigné et al.\n75, this was empirically \nverified by examining the preprocessing procedure on an impulse response \n(showing a leak of up to ~90 ms,; Extended Data Fig. 10).\nThe procedure is as follows:\nDespike\n•\t Remove recordings that deviate more than three times the interquartile range \nfrom the mean value of the electrode.\n•\t Interpolate the removed values using cubic interpolation.\nDetrend\n•\t Common average referencing/remove independent component analysis \ncomponents.\nBroadband power\n•\t Use six-cycle wavelets to compute the power of the 70–200 Hz band, excluding \n60 and 180 Hz.\n•\t Natural log transformation\n•\t z-score transformation\nTemporal smoothing\n•\t Use a filter to smooth the data with a Hamming window with a kernel size \nof 50 ms. Apply the filter in forward and reverse directions to maintain the \ntemporal structure, specifically the encoding peak onset (zero phase).\nEncoding analysis. In this analysis, a linear model is implemented for each lag for \neach electrode relative to word onset, and is used to predict the neural signal from \nword embeddings (Fig. 3b). The calculated values are the correlations between \nthe predicted signal and the held-out actual signal at each lag (separately for each \nelectrode), indicating the linear model’s performance. Before fitting the linear \nmodels for each time point, we implemented running window averaging across a \n200-ms window. We assessed the linear models’ performance (model for each lag) \nin predicting neural responses for held-out data using a tenfold cross-validation \nprocedure. The neural data were randomly split into a training set (that is, 90% \nof the words) for model training and a testing set (that is, 10% of the words) \nfor model validation. On each fold of this cross-validation procedure, we used \nordinary least-squares multiple linear regression to estimate the regression weights \nfrom 90% of the words. We then applied those weights to predict the neural \nresponses to the other 10% of the words. The predicted responses for all ten folds \nwere concatenated so a correlation between the predicted signal and actual signal \nwas computed over all the words of the story. This entire procedure was repeated at \n161 lags from −2,000 to 2000 ms in 25-ms increments relative to word onset.\nPart of the encoding analysis involves the selection of words to include in the \nanalysis. For each analysis, we included the relevant words. Figure 4a includes all \nthe words in the transcription that have a GloVe embedding totaling 4,843 words. \nFigure 4b–d comprises 2,886 accurately predicted words (796 unique words) and \n1,762 inaccurately predicted words (562 unique words). Lastly, Fig. 6b comprises \n3,279 words (165 unique words) that have both GloVe and GPT-2 embeddings, to \nallow for comparison between the two, and at least five repetitions for the average \ncontext and shuffle context conditions.\nNAtURE NEUROSCiENCE | www.nature.com/ natureneuroscience\nArticles NaTUrE NEUrosCIENCE\nAccuracy split. To model the brain’s prediction, the podcast’s transcription words \nwere split into two groups. Each word was marked whether it was one of the \ntop-five most probable words in the distribution that GPT-2 predicted given its \npast context (up to 1,024 previous tokens) or not (Fig. 1). Around 62% of the words \nincluded in the top-five predicted words given their context and were classified as \ncorrectly predicted using this accuracy measure. The other words were classified \nas incorrectly predicted (38%). To control possible confounds stemming from the \naccurately predicted words that are bigger than the inaccurately predicted group, \nwe also report results from classifying the words according to top-one probability \nin Extended Data Fig. 8. Using the top-one measure for human prediction, we \nobtained a group of 36% correctly predicted words.\nConfidence and surprise measures. We associate pre-onset neural activity with \nconfidence in prediction and post-onset neural activity with surprise (prediction \nerror). Both could be estimated using GPT-2. Given a sequence of words, \nautoregressive DLMs induce a distribution of probabilities for the next possible \nword. We used the entropy of this distribution as a measure for the confidence in \nprediction:\n14,22,41\nH(X)=\nn∑\ni=1\nP(xi)× logP(xi)\nWhere n is the vocabulary size and P(xi) is the probability (assigned by the model) \nof the i-th word in the vocabulary.\nTo estimate the surprise, we used the cross-entropy measure. Cross-entropy \nis the loss function used to attenuate the autoregressive DLMs weights, given its \npredictions (that is, the distribution) and the actual word. The lower the probability \nof the actual word before its onset, the higher the surprise it induces. It is  \ndefined by:\nCross-entropy(xactual)= −log(xactual)\nWhile entropy represents the distance of a distribution from the uniform \ndistribution, the cross-entropy describes the distance between the distribution to \nthe 1-hot distribution.\nSignificance tests. To identify significant electrodes, we used a randomization \nprocedure. At each iteration, we randomized each electrode’s signal phase uniform \ndistribution, thus disconnecting the relationship between the words and the brain \nsignal but preserving the autocorrelation in the signal\n76. Then, we performed the \nentire encoding procedure for each electrode. We repeated this process 5,000 \ntimes. After each iteration, the encoding model’s maximal value across all 161 \nlags was retained for each electrode. We then took the maximum value for each \npermutation across electrodes. This resulted in a distribution of 5,000 values, \nwhich was used to determine significance for all electrodes. For each electrode, \na P value (Fig. 3c and 6a and Extended Data Figs. 3 and 4) was computed as the \npercentile of the non-permuted maximum value of the encoding model across all \nlags from the null distribution of 5,000 maximum values. Performing a significance \ntest using this randomization procedure evaluates the null hypothesis such that \nthere is no systematic relationship between the brain signal and the corresponding \nword embedding. This procedure yielded a P value for each electrode. To correct \nfor multiple electrodes, we used the FDR\n49. Electrodes with q values less than 0.01 \nwere considered significant.\nTo test each lag’s significance for two different encoding models for the same \ngroup of electrodes (Figs. 4a,b and Extended Data Figs. 5, 8 and 9), we used a \npermutation test. Each electrode has encoding values for two encoding models. \nWe randomly swapped the assignment of the encoding values between the two \nmodels. Then we computed the average of the pairwise differences to generate a \nnull distribution at each lag. To account for multiple tests across lags, we adjusted \nthe resulting P values to control the FDR\n49. A threshold was chosen to control the \nFDR at q = 0.01.\nTo set a threshold above which average encoding values are significant (Fig. 4  \nand Extended Data Figs. 6 and 7), we used a bootstrapping method77. For each \nbootstrap, a sample matching the subset size was drawn with replacement from \nthe encoding performance values for the subset of electrodes. The mean of each \nbootstrap sample was computed. This resulted in a bootstrap distribution of  \n5,000 mean performance values for each lag. The bootstrap distribution was then \nshifted by the observed value to perform a null hypothesis test\n77. To account for \nmultiple tests across lags, we adjusted the resulting P values to control the FDR49.  \nA threshold was chosen to control the FDR at q = 0.01.\nTo statistically assess the pre-onset prediction for neural responses to correctly \npredicted words (Fig. 5), we completed a permutation test (such as the one \ndescribed for Fig. 4a,b); however, we were also constrained to lags at which the \nneural responses were significant on their own (not with respect to the neural \nresponse of the inaccurate conditional brain response). The same procedure was \nimplemented for the significant test of post-onset surprise.\nContextual embedding extraction. We extracted contextualized word embeddings \nfrom GPT-2 for our analysis. We used the pretrained version of the model \nimplemented in the Hugging Face environment\n78. We first converted the words \nfrom the raw transcript (including punctuation and capitalization) to tokens which \nwere either whole words or sub-words. We used a sliding window of 1,024 tokens, \nmoving one token at a time, to extract the embedding for the final word in the \nsequence. Encoding these tokens into integer labels, we then fed them into the \nmodel, and in return, we received the activations at each layer in the network (also \nknown as a hidden state). GPT-2 has 48 layers, but we focused only on the final \none, before the classification layer. Finally, the token of interest was the final word \nof the sequence, yet we used the second-to-last token as the hidden state for the last \nword because it was the same activation embedding that was used to predict that \nword. With embeddings for each word in the raw transcript, we aligned this list \nwith our spoken-word transcript that did not include punctuation, thus retaining \nonly full words.\nDecoding analysis. The input neural data were averaged in ten 62.5-ms bins \nspanning 625 ms for each lag. Each bin consisted of 32 data points (the neural \nrecording sampling rate was 512 Hz). The neural network decoder (Appendix I)  \nwas trained to predict a word’s embedding from the neural signal at a specific \nlag. The data were split into five nonoverlapping temporal folds and used in a \ncross-validation procedure. Each fold consisted of a mean of 717.04 training words \n(s.d. = 1.32). Three folds were used for training the decoder (training set), one \nfold was used for early stopping (development set) and one fold was used to assess \nmodel generalization (test set). The neural net was optimized to minimize the MSE \nwhen predicting the embedding. The decoding performance was evaluated using a \nclassification task assessing how well the decoder can predict the word label from \nthe neural signal. We used the ROC-AUC.\nTo ensure that the decoding ability was not affected by the electrode selection \nprocedure, we used the training and validation folds (80% of the data) to choose \nthe electrodes for each model. We used the same significance test as the one used \nto locate GloVe-based significant encodings. This procedure yielded a different \nnumber of electrodes ranging from 114 to 132.\nTo calculate the ROC-AUC, we computed the cosine distance between each of \nthe predicted embeddings and the embeddings of all instances of each unique word \nlabel. The distances were averaged across unique word labels, yielding one score \nfor each word label (that is, logit). We used a Softmax transformation on these \nscores (logits). For each label (classifier), we used the logits and the information of \nwhether the instance matched the label to compute a ROC-AUC for the label. We \nplotted the weighted ROC-AUC according to the word’s frequency in the test set. \nTo obtain reliable ROC-AUC scores, we chose words with at least five repetitions in \nthe training set (69% of the words in the transcript).\nTo improve the performance of the decoder, we implemented an ensemble \nof models. We independently trained ten decoders with randomized weight \ninitializations and randomized the batch order. This procedure generated ten \npredicted embeddings. Thus, for each predicted embedding, we repeated the \ndistance calculation from each word label ten times. These ten values were \naveraged and later used for ROC-AUC.\nReporting Summary. Further information on research design is available in the \nNature Research Reporting Summary linked to this article.\nData availability\nThe dataset will become available 6 months after paper publication. Pending \nanonymization process.\nCode availability\nAll the scripts for analyses can be found at https://github.com/orgs/hassonlab/\nrepositories/.\nReferences\n 69. Yuan, J. & Liberman, M. Speaker identification on the SCOTUS corpus.  \nJ. Acoust. Soc. Am. 123, 3878–3878 (2008).\n 70. Y ang, A. I. et al. Localization of dense intracranial electrode arrays using \nmagnetic resonance imaging. NeuroImage 63, 157–165 (2012).\n 71. Oostenveld, R., Fries, P ., Maris, E. & Schoffelen, J.-M. FieldTrip: open-source \nsoftware for advanced analysis of MEG, EEG, and invasive \nelectrophysiological data. Comput. Intell. Neurosci. 2011, 156869 (2011).\n 72. Lachaux, J. P ., Rudrauf, D. & Kahane, P . Intracranial EEG and human brain \nmapping. J. Physiol. Paris 97, 613–628 (2003).\n 73. Michelmann, S. et al. Data-driven re-referencing of intracranial EEG based \non independent component analysis. J. Neurosci. Methods 307, 125–137 \n(2018).\n 74. Jia, X., Tanabe, S. & Kohn, A. Gamma and the coordination of spiking \nactivity in early visual cortex. Neuron  77, 762–774 (2013).\n 75. Cheveigné, Ade, de Cheveigné, A. & Nelken, I. Filters: when, why and how \n(not) to use them. Neuron  102, 280–293 (2019).\n 76. Gerber, E. M. PhaseShuffle (https://www.mathworks.com/matlabcentral/fileex\nchange/71738-phaseshuffle), MATLAB Central File Exchange (2021).\n 77. Hall, P . & Wilson, S. R. Two guidelines for bootstrap hypothesis testing. \nBiometrics 47, 757–762 (1991).\nNAtURE NEUROSCiENCE | www.nature.com/ natureneuroscience\nArticlesNaTUrE NEUrosCIENCE\n 78. Tunstall, L., von Werra, L. & Wolf, T. Natural Language Processing With Trans  \nformers: Building Language Applications With Hugging Face (O’Reilly, 2022).\nAcknowledgements\nWe thank A. Goldberg, R. Goldstein, S. Michelmann, M. Meshulam, M. Kumar, M. \nSlaney and A. Huth for technical and conceptual assistance that motivated and informed \nthis paper’s writing. This work was supported by the National Institutes of Health under \naward numbers DP1HD091948 (to A.G., Z.Z., A.P ., B.A., G.C., A.R., C.K., F .L., A.F . and \nU.H.), R01MH112566 (to S.A.N.) and R01NS109367-01 to A.F ., Finding A Cure for \nEpilepsy and Seizures (FACES) and Schmidt Futures Foundation DataX Fund.\nAuthor contributions\nA.G. devised the project, performed experimental design and data analysis, and wrote \nthe paper; Z.Z., E.B. and M.S. performed data analysis; A.P . designed behavioral test \nand performed data analysis; B.A. performed data collection and data analysis; S.A.N. \ncritically revised the article; A.F ., D.E., A.C., A.J. and H.G. performed data analysis; \nG.C., A.R. and C.K. conducted data collection; C.C. performed data collection and data \nanalysis; L.F . performed data collection; W .D. performed neurosurgery; D.F . and P .D. \nconducted data collection; L.M. devised the project; R.R. critically revised the article;  \nS.D. conducted data collection and provided comments on the manuscript; A.F . devised \nthe project, performed data collection and critically revised the article; L.H. critically \nrevised the article\n;, O.L. conducted experiment design; A.H., M.B. and Y .M. devised \nthe project; K.A.N. and O.D. devised the project and performed critical revision of the \narticle; and U.H. devised the project and wrote the paper.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at https://doi.org/10.1038/s41593-022-01026-4.\nSupplementary information The online version contains supplementary material \navailable at https://doi.org/10.1038/s41593-022-01026-4.\nCorrespondence and requests for materials should be addressed to Ariel Goldstein.\nPeer review information Nature Neuroscience thanks Alona Fyshe and the other, \nanonymous, reviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at www.nature.com/reprints.\nNAtURE NEUROSCiENCE | www.nature.com/ natureneuroscience\nArticles NaTUrE NEUrosCIENCE\nExtended Data Fig. 1 | Figure S1. Comparing agreement with human prediction between most probable predictions based on n-grams or GPt-2. The \nplots show higher agreement between human predictions and GPT-2’s top-1 predictions than all the n-gram model predictions we trained.\nNAtURE NEUROSCiENCE | www.nature.com/ natureneuroscience\nArticlesNaTUrE NEUrosCIENCE\nExtended Data Fig. 2 | Figure S2. Comparing GPt-2 predictions and human predictions. Coloring the scatter plot according to GPT-2/human accuracy. \nGPT-2 and humans jointly predicted correctly 27.6% of the words (green). GPT-2 and humans jointly predicted incorrectly and disagreed on the next \nword for 48.8% of the words (red). GPT-2 and humans jointly predicted incorrectly and agreed on the next word for 5.9% of the words (black) 9.2% of \nthe words humans predicted correctly were not correctly predicted by GPT-2 (blue). 8.4% of the words correctly predicted by GPT-2 were not correctly \npredicted by humans.\nNAtURE NEUROSCiENCE | www.nature.com/ natureneuroscience\nArticles NaTUrE NEUrosCIENCE\nExtended Data Fig. 3 | Figure S3. Left and right hemisphere encoding results show an advantage for contextual (GPt-2) embeddings over static \n(GloVe) and arbitrary embeddings. Right Hemisphere maps for correlation between. A. Predicted and actual word responses for the arbitrary embeddings \n(nonparametric permutation test; q < .01, FDR corrected). B. Correlation between predicted and actual word responses for the static (GloVe) embeddings. \nC. Correlation between predicted and actual word responses for the contextual (GPT-2) embeddings. Using contextual embeddings significantly improved \nthe encoding model’s ability to predict the neural signals for unseen words across many electrodes. Given that we had fewer electrodes in the right \nhemisphere relative to the left hemisphere, this study is not set up to test differences in language lateralization across hemispheres.\nNAtURE NEUROSCiENCE | www.nature.com/ natureneuroscience\nArticlesNaTUrE NEUrosCIENCE\nExtended Data Fig. 4 | Figure S4. Contextual embedding significantly improves the modeling of neural signals. Map of the electrodes in the left \nhemisphere with significant encoding for 1) all three types of embeddings (GPT-2 ∩ GloVe ∩ arbitrary, red); 2) for static and contextual embeddings \n(GPT-2 ∩ GloVe, but not arbitrary, yellow); 3) and contextual only (GPT-2, purple) embeddings. Note the three groups do not overlap. A sampling of \nencoding performance for selected individual electrodes across different brain areas: inferior frontal gyrus (IFG), temporal pole (TP), middle superior \ncentral gyrus (mSTG), superior temporal sulcus (STS), lateral sulcus (LS), middle temporal gyrus (MTG), posterior superior temporal gyrus (pSTG), \nangular gyrus (AG), post central gyrus (postCG), precentral gyrus (PreCG), and middle frontal sulcus (MFS). (Green - encoding for the arbitrary \nembeddings, blue - encoding for static (GloVe) embeddings; purple - encoding for contextual (GPT-2) embeddings).\nNAtURE NEUROSCiENCE | www.nature.com/ natureneuroscience\nArticles NaTUrE NEUrosCIENCE\nExtended Data Fig. 5 | Figure S5. Comparison of GloVe and word2vec-based static embeddings. The encoding procedure was repeated for two additional \nstatic embeddings using the electrodes that were found significant for GloVe-50 encoding on the left hemisphere (Fig. 3B). Each line indicates the \nencoding model performance averaged across electrodes for a given type of static embedding at lags from -2000 to 2000 ms relative to word onset. The \nerror bars indicate the standard error of the mean across the electrodes at each lag. 100-dimensional word2vec and GloVe embeddings resulted in similar \nencoding results to the initial 50-dimensional GloVe embeddings. This suggests that results obtained with static embeddings are robust to the specific \ntype of static embeddings used.\nNAtURE NEUROSCiENCE | www.nature.com/ natureneuroscience\nArticlesNaTUrE NEUrosCIENCE\nExtended Data Fig. 6 | Figure S6. Controlling for correlations among adjacent GloVe embeddings. T o ensure that the signal predicted before word-onset \nis not a result of a correlation among adjacent GloVe embeddings we ran the following additional control analyses: A. We projected (by inner product) \nand then subtracted the GloVe embedding of the previous word from each word and re-ran the encoding analysis. The analysis demonstrates that the \nsignificant encoding before word onset holds even after removing local contextual dependencies in the GloVe embedding of consecutive words. The \nerror bars indicate the standard error of the encoding models across electrodes. The horizontal line indicates the significance threshold calculated using \na permutation test and FDR corrected for multiple comparisons (q < .01). B. We trained an encoding model using arbitrary embeddings on our dataset \nafter removing all bi-grams that repeated more than once. The encoding before word onset remained significant after the removal of the bi-grams. The \nerror bars indicate the standard error of the encoding models across electrodes. The horizontal line indicates the significance threshold calculated using \na permutation test and FDR corrected for multiple comparisons (q < .01). C. We compared an encoding model based on arbitrary embeddings using \nthe previous word embedding (blue line), to an encoding model where we concatenated previous and current word embeddings (red line). The error \nbars indicate the standard error of the encoding models across electrodes. Red asterisks mark significant differences using a permutation test and FDR \ncorrection (q < .01). The significant difference between these two models before word onset is another evidence that there is predictive information in the \nneural activity as to the upcoming word, above and beyond the contextual information embedded in the previous word. The horizontal line indicates the \nsignificance threshold calculated using permutation test and FDR corrected for multiple comparisons (q < .01).\nNAtURE NEUROSCiENCE | www.nature.com/ natureneuroscience\nArticles NaTUrE NEUrosCIENCE\nExtended Data Fig. 7 | Figure S7. GloVe’s space embedding attributes. It can be argued that GloVe based encoding outperforms arbitrary-based encoding \ndue to a general property of the space that GloVe embeddings induce (for example, they are closer / further away from each other). T o control for this \npossible confound, we consistently mismatched the labels of the embeddings of GloVe and used the mismatched version for encoding. This means that \neach unique word was consistently matched with a specific vector that is actually an embedding of a different label (for example, matching each instance \nof the word ‘David’ with the embedding of the word ‘court’). This manipulation uses the same embedding space that GloVe uses and also induces a \nconsistent mapping of words to embeddings (as in the arbitrary-based encoding). The matched GloVe (blue) outperformed the mismatched GloVe \n(black), supporting the claim that GloVe embedding carries information about word statistics that is useful for predicting the brain signal.. The error bars \nindicate the standard error of the encoding models across electrodes.\nNAtURE NEUROSCiENCE | www.nature.com/ natureneuroscience\nArticlesNaTUrE NEUrosCIENCE\nExtended Data Fig. 8 | Figure S8. Encoding for correct / incorrect predictions. This is a variation of Fig. 4B where: A. We classify words as correctly \npredicted if they are the most predictable words by humans’ ratings. The error bars indicate the standard error of the encoding models across electrodes. \nB. We classify words as correctly predicted if they are the most predictable by GPT-2 (instead of top-5). The error bars indicate the standard error of the \nencoding models across electrodes.\nNAtURE NEUROSCiENCE | www.nature.com/ natureneuroscience\nArticles NaTUrE NEUrosCIENCE\nExtended Data Fig. 9 | Figure S9. Comparison of GPt-2 and concatenation of static embeddings. The increased performance of GPT-2 based contextual \nembeddings encoding may be attributed to the fact that it consists of information about the previous words’ identity. T o examine this possibility, we \nconcatenated the GloVe embeddings of the 10 previous words and current word, and reduced their dimensionality to 50 features. GPT-2 based encoding \noutperformed mere concatenation before word onset, suggesting that GPT-2’s ability to compress the contextual information improves the ability to model \nthe neural signals before word onset. The error bars indicate the standard error of the encoding models across electrodes.\nNAtURE NEUROSCiENCE | www.nature.com/ natureneuroscience\nArticlesNaTUrE NEUrosCIENCE\nExtended Data Fig. 10 | Figure S10. Preprocessing procedure applied to an impulse response. The plot demonstrates the temporal uncertainty introduced \nby the preprocessing procedure (especially by the wavelet and smoothing procedures). At sample 45 after onset (dashed line) the value is back to zero, \nconsidering the 512 HZ sampling rate this means that the leak from the future is bounded by 93 ms.\nNAtURE NEUROSCiENCE | www.nature.com/ natureneuroscience\n1 nature research  |  reporting summaryApril 2020\nCorresponding author(s): Ariel Goldstein\nLast updated by author(s): 2021/12/19\nReporting Summary\nNature Research wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency \nin reporting. For further information on Nature Research policies, see our Editorial Policies and the Editorial Policy Checklist.\nStatistics\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\nn/a Confirmed\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\nThe statistical test(s) used AND whether they are one- or two-sided \nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\nA description of all covariates tested\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) \nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \nGive P values as exact values whenever suitable.\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\nOur web collection on statistics for biologists contains articles on many of the points above.\nSoftware and code\nPolicy information about availability of computer code\nData collection Behavioral data were collected Mechanical Turk.\nData analysis Data were preprocessed using Matlab 2019b and The Fieldtrip toolbox. Data was analyzed using python packages are specified here https://\ngithub.com/hassonlab/247-main/blob/main/env.yml. Brain plots were done using toolbox for MATLAB available at (https://github.com/\nHughWXY/ntools_elec). All scripts for analyses are available at:All the scripts for analyses can be found at: https://github.com/orgs/hassonlab/\nrepositories \nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and \nreviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code & software for further information.\nData\nPolicy information about availability of data\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \n- Accession codes, unique identifiers, or web links for publicly available datasets \n- A list of figures that have associated raw data \n- A description of any restrictions on data availability\nWe will make the data available 6 month after paper publication. Pending anonymization process.  \n2 nature research  |  reporting summary April 2020 \nField-specific reporting\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\nLife sciences Behavioural & social sciences  Ecological, evolutionary & environmental sciences\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\nLife sciences study design\nAll studies must disclose on these points even when the disclosure is negative.\nSample size No statistical methods were used to pre-determine sample sizes but our final sample sizes are similar to those reported in previous \npublications. For exmaple, Michelmann et. al. (2021). Moment-by-moment tracking of naturalistic learning and its underlying hippocampo-\ncortical interactions. Nature communications.\nData exclusions One patient was removed  according   pre-established exclusion criteria of excessive epileptic activity  .\nReplication Pre-existing behavioral data that were analyzed in this study were replicated in a new sample. The neural results are replicated at the single \npatient level (which can be considered as replication). No failed replications have be found. \nRandomization Randomization was not applicable as patients freely hear a strory when their neural activity is measured. Behavior pilot for predictability \nscores also did not require randomization\nBlinding Blinding was not relevant in this study, because participants and experimenter do not directly interact in online-experiments. Patients were \nnot assigned to conditions. \nReporting for specific materials, systems and methods\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, \nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \nMaterials & experimental systems\nn/a Involved in the study\nAntibodies\nEukaryotic cell lines\nPalaeontology and archaeology\nAnimals and other organisms\nHuman research participants\nClinical data\nDual use research of concern\nMethods\nn/a Involved in the study\nChIP-seq\nFlow cytometry\nMRI-based neuroimaging\nHuman research participants\nPolicy information about studies involving human research participants\nPopulation characteristics Population characteristics Nine epilleptic patients who volunteered to participate in the study (5 female; 20–48 years old); \nFor the behavioral experiment we used Mechanical Turk, no details about age and gedner were collected.\nRecruitment Behavioral participants were recruited via Amazon Mechanical Turk. They were payed 10$ for their participation. Thier age \nand gender were not collected. The quality of behavior is assessed an no self selection criteria was found relevant.  Patients \nwere recruited via the Comprehensive \nEpilepsy Center of the New York University School of Medicine. Patients are asked whehte they want to listen to a podcast \nduring their time in the hospital and are consenting that the neural data will be used for analyses. No relevent self selection is \nknown with this regard.\nEthics oversight Princeton University and New York University School of Medicine respective Institutional Review Boards approved the \nstudies. Board/committee (Princeton university IRB) that approved the protocol: Daniel Notterman (Chair), Calvin \nChin,Elizabeth Davis,Patricia Fernandez-Kelly,Michael Graziano, Kyle Jamieson, Jacqueline Kariithi, Vikki Lovvoll, Brandt \nMcCabe, Chad Pettengill, Pascale Poussart, Naila Rahman, Rusty Reeves, Carrie Siegler, Jordan Taylor, Rory Truex \n \n \nNote that full information on the approval of the study protocol must also be provided in the manuscript.\n3 nature research  |  reporting summary April 2020 \nClinical data\nPolicy information about clinical studies\nAll manuscripts should comply with the ICMJE guidelines for publication of clinical research and a completed CONSORT checklist must be included with all submissions.\nClinical trial registration n.a.\nStudy protocol n.a.\nData collection n.a.\nOutcomes n.a.",
  "topic": "Surprise",
  "concepts": [
    {
      "name": "Surprise",
      "score": 0.7370201349258423
    },
    {
      "name": "Computer science",
      "score": 0.6579132676124573
    },
    {
      "name": "Autoregressive model",
      "score": 0.5903779864311218
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5803427696228027
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5734432935714722
    },
    {
      "name": "Computational model",
      "score": 0.5623598098754883
    },
    {
      "name": "Natural language processing",
      "score": 0.5550205111503601
    },
    {
      "name": "Word (group theory)",
      "score": 0.523067057132721
    },
    {
      "name": "Language model",
      "score": 0.5072250366210938
    },
    {
      "name": "Natural language",
      "score": 0.46396979689598083
    },
    {
      "name": "Natural language understanding",
      "score": 0.4554850459098816
    },
    {
      "name": "Task (project management)",
      "score": 0.45243126153945923
    },
    {
      "name": "Computational linguistics",
      "score": 0.4199882745742798
    },
    {
      "name": "Cognitive psychology",
      "score": 0.35515087842941284
    },
    {
      "name": "Psychology",
      "score": 0.33178332448005676
    },
    {
      "name": "Speech recognition",
      "score": 0.32616809010505676
    },
    {
      "name": "Linguistics",
      "score": 0.15719327330589294
    },
    {
      "name": "Communication",
      "score": 0.14567753672599792
    },
    {
      "name": "Econometrics",
      "score": 0.09478351473808289
    },
    {
      "name": "Mathematics",
      "score": 0.07665720582008362
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}