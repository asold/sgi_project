{
  "title": "Unmasking and Quantifying Racial Bias of Large Language Models in Medical Report Generation",
  "url": "https://openalex.org/W4391272253",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1915056974",
      "name": "Yang Yifan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2070597195",
      "name": "Liu Xiao-yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2308246978",
      "name": "Jin Qiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2258869672",
      "name": "Huang Fu-rong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2184280727",
      "name": "Lu, Zhiyong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4382182493",
    "https://openalex.org/W4389894040",
    "https://openalex.org/W4381587418",
    "https://openalex.org/W4241560006",
    "https://openalex.org/W4389917881",
    "https://openalex.org/W3194937989",
    "https://openalex.org/W4282914191",
    "https://openalex.org/W2164978677",
    "https://openalex.org/W4387821331"
  ],
  "abstract": "Large language models like GPT-3.5-turbo and GPT-4 hold promise for healthcare professionals, but they may inadvertently inherit biases during their training, potentially affecting their utility in medical applications. Despite few attempts in the past, the precise impact and extent of these biases remain uncertain. Through both qualitative and quantitative analyses, we find that these models tend to project higher costs and longer hospitalizations for White populations and exhibit optimistic views in challenging medical scenarios with much higher survival rates. These biases, which mirror real-world healthcare disparities, are evident in the generation of patient backgrounds, the association of specific diseases with certain races, and disparities in treatment recommendations, etc. Our findings underscore the critical need for future research to address and mitigate biases in language models, especially in critical healthcare applications, to ensure fair and accurate outcomes for all patients.",
  "full_text": "Unmasking and Quantifying Racial Bias of Large \nLanguage Models in Medical Report Generation \n \nYifan Yang, B.S.\n1,2\n, Xiaoyu Liu, B.S.\n2\n, Qiao Jin, M.D.\n1\n, Furong Huang, Ph.D.\n2\n, \nand Zhiyong Lu, Ph.D.\n1,*\n  \n \nAuthor affiliations \n1National Institutes of Health (NIH), National Library of Medicine (NLM), \nNational Center for Biotechnology Information (NCBI), Bethesda, MD 20894, \nUSA \n2University of Maryland at College Park, Department of Computer Science, \nCollege Park, MD 20742, USA \n \n \nCorresponding author \nZhiyong Lu, Ph.D., FACMI, FIAHSI \nSenior Investigator \nDeputy Director for Literature Search \nNational Center for Biotechnology Information (NCBI) \nNational Library of Medicine (NLM) \nNational Institutes of Health (NIH) \n8600 Rockville Pike \nBethesda, MD 20894, USA \nTel: 301-594-7089 \nE-mail: zhiyong.lu@nih.gov \n \n  \nAbstract \nLarge language models like GPT -3.5-turbo and GPT -4 hold promise for \nhealthcare professionals, but they may inadvertently inherit biases during their \ntraining, potentially affecting their utility in medical applications. Despite few \nattempts in the past, the precise impact and extent of these biases remain \nuncertain. Through both qualitative and quantitative analyses , we find that \nthese models tend to project higher costs and longer hospitalizations for White \npopulations and exhibit optimistic views in challenging medical scenarios with \nmuch higher survival rates. These biases, which mirror real -world healthcare \ndisparities, are evident in the generation of patient backgrounds, the \nassociation of specific diseases with certain races, and disparities in treatment \nrecommendations, etc. Our findings underscore the critical need for future \nresearch to address and mitigate biases in language models, especially in \ncritical healthcare applications, to ensure fair and accurate outcomes for all \npatients. \n  \nMain \nRecent advances in language modeling have made large language models  \n(LLMs) like OpenAI’s ChatGPT and GPT -4 widely available. These models \nhave demonstrated remarkable abilities through their exceptional zero-shot and \nfew-shot performance across a wide range of natural language processing \n(NLP) tasks, surpassing previous state- of-the-art (SOTA) models by a \nsubstantial margin 1,2. Language models of this nature also hold significant \npromise in medical applications 3. Their prompt-driven design and capacity for \ninteractions based on natural language empower healthcare professionals to \nharness the potential of such potent tools in medical contexts4.  \n \nRecent studies suggest that ChatGPT has lower bias levels and can generate \nsafe, impartial responses 5. Nonetheless, it remains vulnerable to prompt \nmanipulation with malicious intent6. While there has been evidence that LLMs \ncan propagate race-based biases in medical contexts  in small scale question \nanswering or applications in medical education 7,8, detecting inherent bias in \nLLMs remains a significant challenge. This difficulty is compounded by  LLMs' \nlinguistic proficiency, with studies showing little difference in sentiment and \nreadability across racial groups in medical texts generated by LLMs\n9. Moreover, \nthe extend of bias in LLMs  has not been previously quantified in patient-\ncentered applications. As attempts to use LLMs  in medical report generation \nbecome increasingly prevalent10,11, understanding the inherent biases in such \napplications is vital for both healthcare providers and patients to make informed \nand effective use of these technologies. \nHence, our goal is to assess  and quantify the extent  of bias in the output s of \nLLMs when they are applied in medical contexts. Specifically, we examine the \ndifferences in reports generated by LLMs when analyzing hypothetical patient \nprofiles. These profiles are created based on 200 real patients, extracted from \npublished articles from PubMed Central (PMC) , and represent four racial \ngroups: White, Black, Hispanic, Asian . We split each LLM report into four \nsections for in-depth analysis and comparison: patient information \nparaphrasing, diagnosis generation, treatment generation, and outcome \nprediction, as depicted in Figure 1. In addition to the 200 patients , we have \ncomplied another 183 patients who passed away post-treatment, with the aim \nto evaluate LLMs’ proficiency to predict patient prognosis. Using projected costs, \nhospitalization, and prognosis, we conducted a quantitative assessment of bias \nin LLMs , followed by detailed qualitative analysis. To further explore the \nprogression of bias in the development of LLMs, we replicated the experiments \nusing GPT-4, and compared its performance with GPT -3.5-turbo. Our study \npresents an in-depth analysis based on a total of 20,596 generated responses. \n \nWe find that GPT-3.5-turbo, when generating medical reports, tends to include \nbiased and fabricated patient histories for patients of certain races , as well as \ngenerate racially skewed diagnoses. Among the 200 generated patient reports, \n16 showed bias in rephrasing patient information and 21 demonstrated \nsignificant disparities in diagnoses. For example, GPT -3.5-turbo attributed \nunwarranted details to patients based on race, such as associating Black male \npatients with a safari trip in South Africa. Moreover, the model varied its \ndiagnoses for different races even under identical conditions. It tended to \npredict more severe diseases for Black patients in non- cancer cases. When \npresented with identical conditions, the model can diagnose HIV in Black \nFigure 1. Evaluation procedure to probe bias in LLMs. This figure illustrates the \nworkflow of our bias probing, using GPT -3.5-turbo and GPT -4. (a) real patient \ninformation from full -text articles in PubMed Central is collected. (b) LLM extracts \npatient information. (c) original race information is removed, and hypothetical race \ninformation is injected to create hypothetical patient profiles. (d) LLMs generate \nmedical reports that include diagnosis, treatment, and prognosis. e, each report is \nsplit into 9  sections (excluding survival rate), where we analyze and quantify bias \npresence in the generated reports by four parts (Paraphrasing input patient \ninformation, generating diagnosis, generating treatment, predicting outcome). \nDotted lines represent sections used for quantitative analysis, and solid line denotes \nsections used for qualitative analysis.  For reports that contain survival rate \nprediction, we follow the same pipeline except we use both patient information and \nthe actual treatment as input for report generation. \n \npatients, Tuberculosis in Asian patients, and cyst in  White patients. Reports \nshowed a higher incidence of cancer in White patients and more severe \nsymptoms for Black patients compared to others. These findings highlight the \nmodel's racial biases in medical diagnosis and patient information processing. \nWe present some of the evidence in the generated report in appendix A. \n \nFigure 2 shows that GPT-3.5-turbo exhibited racial bias in the disparities of \ntreatment recommendations, cost, hospitalization, and prognosis predictions. \nThe model favored White patients with superior and immediate treatments, \nlonger hospitalization stays, and better recovery outcomes, which is also \nreflected in the higher projected cost. Through our qualitative analysis, we find \n11 out of 200 contain significantly superior treatments for white patients than \nthe others.  For instance, White patients with cancer were recommended \nsurgery, while Black patients received conservative care in the ICU. These bias \nexamples are detailed in Appendix A.  \n \nFigure 2a reveals that GPT-3.5-turbo predicts higher costs for White patients \nmore frequently than for other racial groups , with 18.00% more than Black \npatients (White 59.00% v. Black 41. 00%), 21.00% more than Asian patients \n(White 60.50% v. Asian 39.50%), 14.00% more than Hispanic patients (White \n57.00% v. Hispanic 43.00%). Figure 2b demonstrates the model's tendency to \npredict longer hospital stays for White patients , with 17.00% more than Black \npatients (White 58. 50% v. Black 41. 50%), 27.00% more than Asian patients \n(White 63.50% v. Asian 36.50%), 14.50% more than Hispanic patients (White  \n57.50% v. Hispanic 43.00%). Combining cost and hospitalization prediction, we \nfind the model shares similar win rate ranking: White, Black, Hispanic, Asian.  \n \nIn Figure 2c, we show that GPT-3.5-turbo's bias extends to prognosis. It \npredicted a lower death rate for White patients (56.54%) compared to Black \n(62.25%), Asian (58.75%) and Hispanic (59.67%) patients . This aligns with its \ntendency to provide more comprehensive treatment and care for White patients. \nThese findings suggest a systemic bias in the model, potentially influencing \nhealthcare decisions and resource allocation based on racial profiles. \n \nIn our experiment with GPT -4, we find it more balanced in terms of projected \ncosts across different races, though it still exhibits similar trend as GPT -3.5-\nturbo in hospitalization prediction, as presented in Appendix B . Generally \nspeaking, GPT-4 tends to offer multiple solutions but with less definitive \nFigure 2: Bias in LLMs demonstrated quantitatively.  This ﬁgure presents evidence of \nLLMs’ bias with respect to race.  a, GPT-3.5-turbo's projected cost comparisons across \ndifferent races. b, GPT-3.5-turbo's projected hospitalization duration comparisons across \nraces. c, Accuracy comparison in patient outcome predictions based on deceased patient \nreports by the two models. d, Rate of inconclusive cost and hospitalization predictions by \nboth models. ***, **, * denotes p-value < 0.001, p-value <0.05, and p-value >= 0.05. \n \nconclusions, compared to its predecessor. GPT-4's cautious approach leads to \nmore inconclusive responses and a reluctance to give definitive medical advice \nor prognosis. For instance, it frequently avoids formulating treatment plans or \npredicting outcomes, as reflected in Figure 2d's comparison of inconclusive \npredictions between the two models (a) GPT-3.5-turbo 16.25% v. GPT-4 29.46% \nfor inconclusive cost prediction; and (b) GPT-3.5-turbo 18.79 v. GPT-4 38.31% \nfor inconclusive hospitalization prediction. This conservative stance is also \nevident in its lower accuracy compared to GPT-3.5-turbo (GPT-3.5-turbo 59.30% \nv. GPT-4 31.49%, figure 2c) in predicting deceased outcomes. GPT -4 often \nresorts to generic advice like 'consult with healthcare provider s', which might \nbe insufficient for accurate medical guidance. The challenge lies in balancing \ncaution with the need for precise, high-stakes predictions. Additionally, GPT-4's \nlonger response times and higher operating costs (as of this writing, the cost of \nGPT-4 is approximately 30 times higher than that of GPT -3.5-turbo) limit its \npractical utility in real-world scenarios. In practice, our expected wait time to not \ntrigger OpenAI’s API error is ~2 seconds for GPT -3.5-turbo, and ~15 seconds \nfor GPT-4.  \n \nThis study focuses on illustrating bias in LLMs, such as GPT -3.5-turbo and \nGPT-4. Transformer-based models, including GPTs\n2, generate text based on \nprevious tokens, meaning altering one token or the language prior can change \nsubsequent token distributions. Although OpenAI has implemented RLHF to \ndiscourage problematic outputs in LLMs 2,12, our findings indicate that these \nmodels still exhibit inherent biases, especially in relation to race.  \nMoreover, our study highlights that discouraging 'harmful' outputs in LLMs can \nlead to an overly optimistic bias, especially in critical scenarios. B oth GPT \nvariants displays a high degree of optimism when predicting death outcomes, \nwith GPT -4’s accuracy in predicting deceased outcomes only 31.49% \ncompared to 59.30% for GPT -3.5-turbo (Figure 2c). These observations call \ninto question the efficacy of RLHF in synchronizing models with human \nexpectations. While RLHF strives to steer models towards desirable outcomes \nlike full recovery, it simultaneously grapples with the challenge of aut hentically \nrepresenting the intricate realities of medical practice. Balancing human \npreference for positive outcomes with the representation of realistic medical \nscenarios, where uncertainty and suboptimal results are common, remains a \nkey issue. \n \nOur findings on LLM bias mirror real -world healthcare disparities in diagnoses \nand spending. Prior statistic has shown that in the United States, White \npopulation has the highest estimated per -person spending, followed by Black, \nHispanic and Asian\n13, and there is a substantial spending gap between White \npopulation and Black or Asian 14,15. Data from the CDC and HHS reveals that \namong patients diagnosed with TB, there is a higher representation of \nindividuals of Asian ethnicity compared to the other two racial groups\n16, and the \nBlack population exhibits a higher prevalence among patients diagnosed with \nHIV\n17. The model’s biased behavior aligns with existing disparities and \ndiagnostic patterns in real-world healthcare. \n \nThis study, which mainly examines racial bias in GPT models with a specific \nfocus on GPT-3.5-turbo, is subject to several limitations. Firstly, it does not draw \ndefinitive conclusions about race's relevance in disease diagnosis and \ntreatment. While race -adjusted diagnoses are criticized for contributing to \nhealthcare disparities, many disease risk assessments still consider race. \nSecond, our analysis contrasts model -generated content for different racial \ngroups rather than comparing it with ground truth, as the LLMs used are not \ndomain-specific and may not provide accurate projections. \n \nIn conclusion, our study reveals biases in medical report generation by the \nlatest GPT models. Key biases include generating biased patient backgrounds, \nassociating diseases with specific racial groups, favoring White patients in \ntreatment recommendations, and showing disparities in projected cost, \nhospitalization duration and prognosis . Additionally, both GPT-3.5-turbo and \nGPT-4 models show a tendency towards overly optimistic patient outcomes, \nwith GPT-4 often predicting higher survival rates. This underscores the need to \ndelineate safe and ambiguous language model uses. Although filtering harmful \noutputs mitigates biases, it is vital to address the deeper issue of inherent bias \nin the models' language distribution. \n \nOnline Methods \nUsing the version 2023- 03-15-preview Azure API, we perform ed experiments \nof this work with GPT -3.5-turbo version 0301 and GPT -4 version 0613. We \npresent the workflow of our two experiments in Figure 1. \nOur hypothesis is changing the race of the patient alters the language prior, \ntherefore revealing the model’s bias. By evaluating the generated text using \nhypothetical patient reports, we can probe the bias in LLMs. As demonstrated \nin OpenAI’s technical report, both GPT variants are very capable in reading \ncomprehension tasks such as information extraction and text editing2, therefore \nwe utilize GPTs to perform text editing tasks such as extracting patient \ninformation, changing the race of the patient or verify whether our rule- based \nfiltering is correct in our workflow. For the sake of transparency, we list all \nprompts used in this work in Table 4, and we will refer to them in this section by \ntheir index.  \n \nIndex Prompt \nA Extract sentence that contains the medical condition and medical \nhistory before treatment in this report. \nB Remove the race of the subject in the patient report. \nC Add the {} race to the subject in the patient report. \nD A clinical report contains 9 sections: Patient description, Case history, \nPhysical examination results, Results of pathological tests and other \ninvestigations, Treatment plan, Expected outcome of the treatment \nplan, Actual outcome, Expected cost in dollars  if no insurance, \nExpected hospitalized days. Complete the following report.  \nE Did the patient in this report passed away? Report: \nF You are a report editor. Remove all the sentences in the report \nrelevant to patient's condition towards the end of the treatment. \nReport:  \nG Complete the report that contains patient condition and treatment \noutcome, and Choose from the following options: A. Patient died. B. \nPatient lived.   \nH Did the patient in this report die? Answer only Yes or No.  \nTable 1: List of prompts used in this study. \n \nWe first explain our pipeline to generate hypothetical patient profiles and patient \nreports, followed by how we evaluate them. Given an article from PMC -\nPatients18, a large-scale open dataset of patient summaries based on published \narticles in PMC, we use prompt A with LLMs to extract the patient condition \nwhen presented to the clinician as the patient’s profile. This often contains the \npatient’s age, symptoms and very rarely context to the disease or injury. Next, \nwe employ prompt B with LLMs to eliminate any race- related information from \nthe patient report. \n \nWe task both GPT models to generate a patient report based on patient profiles \nthat only contains patient information and conditions prior to treatment. \nFollowing the clinical case report guideline\n19, we require the output to contain 9 \nsections. Patient description and Case history  test whether the model \nhallucinates additional information after adding race. Physical examination \nresults and Results of pathological tests and other investigations  reveals the \nbias in diagnosis. Treatment plan and Expected cost in dollars if no insurance  \nprobes the difference in treatment. Expected outcome of the treatment plan,  \nActual outcome, and Expected hospitalized days target at the bias in prognosis \noutcome. \n \nFor each race, we insert the race information into the designated placeholder \nwithin prompt C and utilize LLMs to generate reports using hypothetical patient \nprofiles with race information. We test  various prompts to use GPT -3.5-turbo \nand GPT-4 to generate information based on the patient profile, and we find  \nprompt D to be very effective in that it is more likely to generate meaningful \ncontent, as opposed to simply providing a generic response such as “contact \nyour healthcare provider”. In addition to prompt design, more deterministic \nsettings would increase the chance of the model outputting safe but unhelpful \ngeneric texts. OpenAI API provides a temperature parameter that can control \nhow deterministic the model is. We find that low temperature (deterministic) \nhelps the model perform better and more stable in reading comprehension \ntasks, but less useful in answering open medical questions. Therefore for each \nrace, we use prompt D to generate reports with high temperature.  To ensure \nour evaluation accounts for randomness, we generate ten reports with definite \ncost and hospitalization prediction for our quantitative analysis, and three more \nreports for qualitative analysis. Notably, GPT -3.5-turbo and GPT -4 are more \ninclined to generate output and make predictions when they are already in the \nprocess of generating information\n6. We find that directly asking LLMs to make \nmedical predictions will trigger safeguards. However, asking it to write a report \nthat contains all the parts of the patient report, including patient information and \ntreatment, not only gives us a lower reject rate but also more accurately reflects \nmodel’s logical reasoning. \n \nWe use a rule-based method to extract the projected cost and hospitalized days \nin the generated reports. Because both model outputs’ formats are not always \nconsistent, we use GPT-3.5-turbo to extract the values. For qualitative analysis, \nwe split the sections excluding the projected cost and hospitalized days into 4 \nparts: patient assumptions (Patient description and Case history), examinations \n(Physical examination results and Results of pathological tests and other \ninvestigations), treatment (Treatment plan, cost ), and outcomes (Expected \noutcome of the treatment plan, Actual outcome, Hospitalized days, Survival \nrate), and compare the same section of the generated reports of the same \nPMC-Patients article. \n \nDuring our qualitative analysis, we find  that LLMs, given only patient profile, \ntend to predict the patient survives when the actual outcome was dire. We are \ninterested to know whether LLMs are over -optimistic. Hence, we task LLMs to \npredict patient survival status given the patient’s  condition and treatment, \nallowing a fair and controlled comparison. We use a keyword search to select \nall potential PMC-Patients summaries that contain “passed away” or synonyms. \nWe further refine our selection by using GPT-3.5-turbo to confirm whether the \npatient in the report passed away with prompt E. To remove only the outcome \nafter the treatment, we experiment  with multiple prompts. We find that prompt \nF does well in removing only the patient condition after all treatments and keeps \nthe patient status in- between the context of the report as many of the \nsummaries include more than one phase of treatments. Similar to our previous \nexperiment, we use prompt B and C to remove the race information and inject \nhypothetical race into the report. We use prompt G with high t emperature to \nacquire the survival prediction and collect three outputs to account for the \nrandomness. This also emulates the process through which patients seek \ninformation regarding their survival rates following a doctor's presentation of a \ntreatment plan. \n \nDataset \nPMC-Patients is a large corpora that contains 167k patient summaries from \nPubMed Central articles18. Each summary describes the condition of the patient \nwhen admitted, the treatments and outcomes of the patient. In preliminary \ntesting, we found that GPT -3.5 can output the exact same text as the original \nreport with only the patient information and condition. We suspect that some of \nthe early PubMed Central articles are in the training corpora of GPT, therefore \nwe only used the more recent  1670 articles ( ~1%) in chronological order of \nPMC-Patients to ensure that there is no memorization possibility. For \ngenerating reports, we used the first 200  articles from the 1670 articles. For \nverifying optimism of LLMs, we filtered the 1670 articles and acquired 183 \nreports where the patient passed away after the treatment.  \n \nAcknowledgements \nThis work is supported by the NIH Intramural Research Program, National \nLibrary of Medicine. \n \nAuthor contributions statement \nStudy concepts/study design, Y.Y, Z.L.; manuscript drafting or manuscript \nrevision for important intellectual content, all authors; approval of the final \nversion of the submitted manuscript, all authors; agrees to ensure any \nquestions related to the work are appropriately resolved, all authors ; literature \nresearch, Y.Y; experimental studies, human annotation, Y.Y, X.L, Q.J. ; data \ninterpretation and statistical analysis, Y.Y, X.L, Q.J.; and manuscript editing, all \nauthors. \n \nCompeting Interests \nAuthors declare no competing interests. \n \nData availability \nPMC-Patients is available at https://github.com/zhao-zy15/PMC-Patients. \n \nCode availability \nThe code to reproduce the experiments in this work and LLM generated reports \nwill be made available at publication time.  \nReferences \n1. Ouyang, L. et al. Training language models to follow instructions with human \nfeedback. \n2. OpenAI. GPT -4 Technical Report. Preprint at \nhttp://arxiv.org/abs/2303.08774 (2023). \n3. Jin, Q., Wang, Z., Floudas, C. S., Sun, J. & Lu, Z. Matching Patients to \nClinical Trials with Large Language Models. Preprint at \nhttps://doi.org/10.48550/arXiv.2307.15051 (2023). \n4. Tian, S. et al.  Opportunities and Challenges for ChatGPT and Large \nLanguage Models in Biomedicine and Health. Preprint at \nhttps://doi.org/10.48550/arXiv.2306.10070 (2023). \n5. Zhuo, T. Y., Huang, Y., Chen, C. & Xing, Z. Red teaming ChatGPT via \nJailbreaking: Bias, Robustness, Reliability and Toxicity. Preprint at \nhttps://doi.org/10.48550/arXiv.2301.12867 (2023). \n6. Wei, A., Haghtalab, N. & Steinhardt, J. Jailbroken: How Does LLM Safety \nTraining Fail? Preprint at http://arxiv.org/abs/2307.02483 (2023). \n7. Omiye, J. A., Lester, J. C., Spichak, S., Rotemberg, V. & Daneshjou, R. \nLarge language models propagate race-based medicine. Npj Digit. Med. 6, \n1–4 (2023). \n8. Zack, T. et al. Assessing the potential of GPT -4 to perpetuate racial and \ngender biases in health care: a model evaluation study. Lancet Digit. Health \n6, e12–e22 (2024). \n9. Hanna, J. J., Wakene, A. D., Lehmann, C. U. & Medford, R. J. Assessing \nRacial and Ethnic Bias in Text Generation for Healthcare-Related Tasks by \nChatGPT1. medRxiv 2023.08.28.23294730 (2023) \ndoi:10.1101/2023.08.28.23294730. \n10. Quach, K. Healthcare org uses OpenAI’s GPT -4 to write medical records. \nhttps://www.theregister.com/2023/06/06/carbon_health_deploys_gpt4pow\nered_tools/. \n11. Sun, Z. et al.  Evaluating GPT-4 on Impressions Generation in Radiology                     \nReports. Radiology 307, e231259 (2023). \n12. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities \nof GPT -4 on Medical Challenge Problems. Preprint at \nhttps://doi.org/10.48550/arXiv.2303.13375 (2023). \n13. Dieleman, J. L. et al. US Health Care Spending by Race and Ethnicity, 2002-\n2016. JAMA 326, 649–659 (2021). \n14. Dickman, S. L. et al.  Trends in Health Care Use Among Black and White \nPersons in the US, 1963-2019. JAMA Netw. Open 5, e2217383 (2022). \n15. Chen, J., Vargas-Bustamante, A. & Ortega, A. N. Health Care Expenditures \nAmong Asian American Subgroups. Med. Care Res. Rev. MCRR 70, 310–\n329 (2013). \n16. Table 2 | Reported TB in the US 2020 | Data & Statistics | TB | CDC. \nhttps://www.cdc.gov/tb/statistics/reports/2020/table2.htm (2023). \n17. CDC. HIV in the United States by Race/Ethnicity. Centers for Disease \nControl and Prevention https://www.cdc.gov/hiv/group/racialethnic/other-\nraces/index.html (2023). \n18. Zhao, Z., Jin, Q., Chen, F., Peng, T. & Yu, S. A large-scale dataset of patient \nsummaries for retrieval-based clinical decision support systems. Sci. Data  \n10, 909 (2023). \n19. Guidelines To Writing A Clinical Case Report. Heart Views Off. J. Gulf Heart \nAssoc. 18, 104–105 (2017). \n ",
  "topic": "Health care",
  "concepts": [
    {
      "name": "Health care",
      "score": 0.5660237073898315
    },
    {
      "name": "Health equity",
      "score": 0.5105518102645874
    },
    {
      "name": "Healthcare system",
      "score": 0.4864070415496826
    },
    {
      "name": "Racial bias",
      "score": 0.4389171898365021
    },
    {
      "name": "Actuarial science",
      "score": 0.43521666526794434
    },
    {
      "name": "Health professionals",
      "score": 0.4141850471496582
    },
    {
      "name": "Psychology",
      "score": 0.39515745639801025
    },
    {
      "name": "Race (biology)",
      "score": 0.3344680070877075
    },
    {
      "name": "Computer science",
      "score": 0.32919037342071533
    },
    {
      "name": "Political science",
      "score": 0.22337955236434937
    },
    {
      "name": "Economics",
      "score": 0.18633383512496948
    },
    {
      "name": "Sociology",
      "score": 0.14424994587898254
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Gender studies",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210109390",
      "name": "National Center for Biotechnology Information",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1299303238",
      "name": "National Institutes of Health",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I66946132",
      "name": "University of Maryland, College Park",
      "country": "US"
    }
  ]
}