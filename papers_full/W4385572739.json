{
    "title": "Making Pretrained Language Models Good Long-tailed Learners",
    "url": "https://openalex.org/W4385572739",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2096971521",
            "name": "Chen Zhang",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2134464700",
            "name": "Lei Ren",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2124326937",
            "name": "Jingang Wang",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A1993208100",
            "name": "Wei Wu",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A1980846011",
            "name": "Dawei Song",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3173777717",
        "https://openalex.org/W3164972323",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W3167602185",
        "https://openalex.org/W2279639021",
        "https://openalex.org/W3104182862",
        "https://openalex.org/W3042207670",
        "https://openalex.org/W3182414670",
        "https://openalex.org/W3114651185",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W4287028759",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W3194836374",
        "https://openalex.org/W2102605133",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W3155510817",
        "https://openalex.org/W4304730858",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W2946659172",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2995197345",
        "https://openalex.org/W4385970309",
        "https://openalex.org/W1677182931",
        "https://openalex.org/W3034601242",
        "https://openalex.org/W2252066972",
        "https://openalex.org/W3101398262",
        "https://openalex.org/W3034328552",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2962933664",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W3126074026",
        "https://openalex.org/W1921523184",
        "https://openalex.org/W4297801719",
        "https://openalex.org/W1520090767"
    ],
    "abstract": "Prompt-tuning has shown appealing performance in few-shot classification by virtue of its capability in effectively exploiting pre-trained knowledge. This motivates us to check the hypothesis that prompt-tuning is also a promising choice for long-tailed classification, since the tail classes are intuitively few-shot ones. To achieve this aim, we conduct empirical studies to examine the hypothesis. The results demonstrate that prompt-tuning makes pretrained language models at least good long-tailed learners. For intuitions on why prompt-tuning can achieve good performance in long-tailed classification, we carry out in-depth analyses by progressively bridging the gap between prompt-tuning and commonly used finetuning. The summary is that the classifier structure and parameterization form the key to making good long-tailed learners, in comparison with the less important input structure. Finally, we verify the applicability of our finding to few-shot classification.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3298–3312\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nMaking Pretrained Language Models Good Long-tailed Learners\nChen Zhang\n , Lei Ren\n , Jingang Wang\n , Wei Wu\n , Dawei Song\nBeijing Institute of Technology\n{czhang,dwsong}@bit.edu.cn\nMeituan NLP\n{wangjingang02,wuwei30}@meituan.com\nrenlei_work@163.com\nAbstract\nPrompt-tuning has shown appealing perfor-\nmance in few-shot classification by virtue of its\ncapability in effectively exploiting pre-trained\nknowledge. This motivates us to check the hy-\npothesis that prompt-tuning is also a promising\nchoice for long-tailed classification, since the\ntail classes are intuitively few-shot ones. To\nachieve this aim, we conduct empirical studies\nto examine the hypothesis. The results demon-\nstrate that prompt-tuning makes pretrained lan-\nguage models at least good long-tailed learn-\ners. For intuitions on why prompt-tuning can\nachieve good performance in long-tailed clas-\nsification, we carry out in-depth analyses by\nprogressively bridging the gap between prompt-\ntuning and commonly used finetuning. The\nsummary is that the classifier structure and pa-\nrameterization form the key to making good\nlong-tailed learners, in comparison with the\nless important input structure. Finally, we ver-\nify the applicability of our finding to few-shot\nclassification.1\n1 Introduction\nPretrained language models (PLMs) with CLS-\ntuning (i.e., finetuning a PLM by applying a clas-\nsifier head over the [CLS] representation) have\nachieved strong performance in a wide range of\ndownstream classification tasks (Devlin et al., 2019;\nLiu et al., 2019a; Raffel et al., 2020; Wang et al.,\n2019b). However, they have been less promising\nin the long-tailed scenario (Li et al., 2020). The\nlong-tailed scenario is different from common sce-\nnarios due to the long tail phenomenon exhibited\nin the class distribution, as illustrated in Figure 1.\nThe long-tailed class distribution prevents PLMs\nfrom achieving good performance, especially in\nJingang Wang and Dawei Song are the corresponding\nauthors.\n1Good long-tailed l earners are abbreviated as\n GLEE .\nCode and data are available at https://github.com/\nGeneZC/Glee.\n0 20 40 60 80 100 120\nClass index (from head to tail).\n0\n250\n500\n750\n1000\n1250\n1500\n1750#Examples.\nHead Tail\nFigure 1: An example long-tailed class distribution from\nIFLYTEK dataset (Xu et al., 2020), where we can distin-\nguish tail classes from head classes.\ntail classes that only allow learning with very few\nexamples (dubbed tail bottleneck).\nRecent advances on Prompt-tuning2 have wit-\nnessed a surge of making PLMs better few-shot\nlearners (Schick and Schütze, 2021; Gao et al.,\n2021; Scao and Rush, 2021). Prompt-tuning re-\nquires PLMs to perform classification in a cloze\nstyle, thus is superior to CLS-tuning in two as-\npects: 1) it aligns the input structure with that of\nmasked language modeling (MLM); and 2) it re-\nuses the classifier structure and parameterization\nfrom the pretrained MLM head. These two merits\nof Prompt-tuning equip PLMs to better exploit pre-\ntrained knowledge and hence gain a better few-shot\nperformance than that with CLS-tuning.\nThe success of Prompt-tuning in few-shot sce-\nnarios motivates us to hypothesize that Prompt-\ntuning can relieve the tail bottleneck and thus make\nPLMs good long-tailed learners. The reason why\nwe make such a hypothesis is that the tail classes\nare intuitively few-shot ones. However, long-tailed\n2Prompt-tuning can be an ambiguous term regarding the\nparameter-efficient prompt tuning (Lester et al., 2021). How-\never, we insist on the use of it to accord with CLS-tuning.\n3298\nclassification is different from few-shot classifica-\ntion to a certain extent, as it allows the possibility to\ntransfer knowledge from head classes to tail ones.\nWe empirically examine the hypothesis by con-\nducting empirical evaluations on three long-tailed\nclassification datasets. The comparison results\nshow that PLMs can be good long-tailed learn-\ners with Prompt-tuning, which outperforms PLMs\nwith CLS-tuning by large margins. Besides, Prompt-\ntuning even exhibits a better performance than that\nof CLS-tuning with appropriate calibrations (e.g.,\nfocal loss Lin et al. 2017). The widely accepted\ndecoupling property (Kang et al., 2020) claims that\na good long-tailed learner should enjoy a nearly\nuniform distribution in terms of weight norms, oth-\nerwise the norms of head classes can be way much\nlarger than the tail ones’. It is therefore expected\nthat the weights tuned with Prompt-tuning own a\nflat distribution. We validate the property ofPrompt-\ntuning by visualizing the norms of trained classifi-\ncation weights across classes. With the compelling\nresults, we put that our hypothesis is valid.\nWe also provide further intuitions by asking why\nPrompt-tuning could be so promising, as shown in\nthe above empirical investigations. Through in-\ndepth analyses, we uncover that re-using the classi-\nfier structure and parameterization from the MLM\nhead is a key for attaining the good long-tailed\nperformance, largely outweighing the importance\nof aligning the input structure with that of MLM.\nCLS-tuning, with classifier structure derived and\nparameters partly initialized from the MLM head,\napproximates the performance of Prompt-tuning.\nWe believe that this observation would as well shed\nlight on related work that aims to improve Prompt-\ntuning itself. As such, we finally present the com-\nparison results of the improved CLS-tuning and\nPrompt-tuning in the few-shot scenario, suggest-\ning the applicability of the improved CLS-tuning to\nfew-shot classification.\n2 Background\n2.1 Long-tailed Classification\nLong-tailed classification basically follows a classi-\nfication setting. Given a dataset D= {(xi,yi)}i in\nwhich (x,y) ∼P(X,Y), a model Mis required\nto learn to approximate P(Y|X ) as accurate as\npossible so that it can correctly predict the label\nfrom the input. However, the long-tailed classifica-\ntion differs from the common classification setting\nin that P(Y) is a long-tailed one, prohibiting M\nfrom achieving a good optimization, especially on\ntail classes.\n2.2 Finetuning\nCLS-tuning Pretrained with the special token\n[CLS] for overall semantics, PLMs can be fine-\ntuned with classifiers over the [CLS] representa-\ntions for classification (Figure 2(a) left).\nThe optimization objective can be depicted as:\nLCLS = −log P(yi |xi; M), (1)\nwhich is exactly a cross entropy loss. Here, Mcan\nbe disassembled to a backbone Eand a classifier\nC. While Eis a PLM producing [CLS] represen-\ntation, Cis a Tanh-activated MLP. Here, the MLP\ntypically consists of two feed-forward linear lay-\ners. To be more specific, we name the classifier for\nCLS-tuning as CLS head (Figure 2(a) right), and\ngenerally the last layer of the classifier as predictor.\nWe also name the input as CLS input for brevity.\nPrompt-tuning For PLMs that are pretrained\nwith an MLM objective, it is natural to finetune\nthe PLMs in an MLM-like cloze style for better\nexploitation of the pretrained knowledge.\nTo reach the goal, a template T and a verbalizer\nVare introduced (Schick and Schütze, 2021). The\ntemplate converts the original input to an input\nwith one [MASK] token that should be recovered,\nin other words MLM input. The verbalizer maps\nall labels to their corresponding tokens, and the\nmodel should predict the token corresponding to\nthe correct label. In particular, for a label that\nis mapped to multiple tokens, one [MASK] should\nbe faced with the issue of inability of multi-token\ncompletion. We are inspired by the average strategy\nin Chen et al. (2021); Hu et al. (2021), and treat the\naverage of logit values for multiple tokens as the\nlogit value for the label.\nFor example (Figure 2(b) left), the template for\nsentiment classification can be:\nT(x) =x. It was [MASK]., (2)\nAccordingly, the verbalizer can be:\nV(y) =\n{\ngreat yis label:positive\nterrible yis label:negative , (3)\nThereby, the optimization objective is described\nas:\nLPrompt = −log P(V(yi) |T (xi); M), (4)\n3299\n[CLS]  The movie tonight is fascinating ! [SEP]\nCLS \nhead\n[CLS]  The movie tonight is fascinating ! It was  [MASK]  . [SEP]\nMLM \nhead\n• label:positive \n• label:negative\n• great (label:positive) \n• terrible (label:negative)\nVerbalizer\nMapper\nLinear\nMLM head\nLN\nGELU\nLinear\nLinear\nCLS head\nTanh\nLinear\n(a) CLS-tuning\n[CLS]  The movie tonight is fascinating ! [SEP]\nCLS \nhead\n[CLS]  The movie tonight is fascinating ! It was  [MASK]  . [SEP]\nMLM \nhead\n• label:positive \n• label:negative\n• great (label:positive) \n• terrible (label:negative)\nVerbalizer\nMapper\nLinear\nMLM head\nLN\nGELU\nLinear\nLinear\nCLS head\nTanh\nLinear (b) Prompt-tuning\nFigure 2: Illustration of two finetuning schemes.\nwhere Ein Mgenerates the [MASK] representa-\ntion, and Cis the pretrained MLM head. The\nMLM head (Figure 2(b) right) is activated with\na GELU (Hendrycks and Gimpel, 2016) and nor-\nmalized with a layer normalization (Ba et al., 2016;\nVaswani et al., 2017; Devlin et al., 2019). Note\nthat Eand Cshare a part of parameters (i.e., the\nword embeddings in Eand the predictor over the\nvocabulary in C).\n2.3 Research Hypothesis\nAs discussed in the leading Section 1, we observe\nthat, in a long-tailed class distribution, each of the\ntail classes is provided with very few examples,\ntypically fewer than one tenth of the number of\na common class. This brings challenges in long-\ntailed classification. Meanwhile, Prompt-tuning has\nbeen demonstrated to make PLMs better few-shot\nlearners by exploiting pretrained knowledge.\nTherefore, we are inspired to hypothesize that\nPrompt-tuning can make PLMs good long-tailed\nlearners, as pretrained knowledge is intuitively\nlearned from a long-tailed vocabulary. In the fol-\nlowing, we present a series of empirical examina-\ntions to test whether our hypothesis is valid or not\nin Section 3, and why it is so in Section 4.\n3 Empirical Examination\n3.1 Setup\nDatasets We conduct examinations on five long-\ntailed classification datasets, ranging from Chinese\nto English ones. The first one is a medical in-\ntent question detection dataset (CMID ) (Chen et al.,\n2020). The second one is an application category\nclassification dataset ( IFLYTEK ) maintained by\nCLUE (Xu et al., 2020). The third one is a clinical\ntrial criterion categorization dataset (CTC) (Zong\net al., 2021). The fourth one is an entity typing\ndataset (MSRA ) originally released as a named en-\ntity recognition dataset (Levow, 2006). The last\none is a document topic classification dataset (R52)\nessentially derived from Reuters 21578 dataset (De-\nbole and Sebastiani, 2004).\nFor datasets that originally do not include a test\nset (e.g., IFLYTEK ), we use the development set as\ntest set and randomly take 10% of the training set\nas development set. The statistics of these datasets\nare listed in Table 1.\nTemplates and Verbalizers For Prompt-tuning,\nexample templates for the three datasets separately\nare shown as below:\n• CMID : x? The intent of the question is\n[MASK].\n• IFLYTEK : x. The mentioned application be-\nlongs to [MASK].\n• CTC: x. The category of the criterion is\n[MASK].\n• M SRA : x. The ein the sentence is [MASK].\n• R52: x. This is [MASK].\nwhere xdenotes the input, and edenotes the men-\ntioned entity in the sentence offered in MSRA .\nHere, necessary English translations of Chinese\ntemplates are used, and the according Chinese tem-\nplates are listed in Appendix A.\nSince there are many classes for each dataset,\nwe leave the details on verbalizers to Appendix A.\nBasically, the verbalizers are deduced from class\ndescriptions after removal of some less meaningful\ntokens (e.g., punctuations).\nImplementation Experiments are carried out on\nan Nvidia Tesla V100. All models are imple-\nmented with PyTorch3 and Transformers4 libraries.\nWe initialize models with the Google-released\nbert-base-chinese and bert-base-uncased\ncheckpoints5. For parameter settings, the batch\nsize is 32, the learning rate is 1e-5, the weight de-\ncay is 0, and lastly the gradient norm is constrained\n3https://github.com/pytorch/pytorch\n4https://github.com/huggingface/transformers\n5https://github.com/google-research/bert\n3300\nTable 1: Statistics of the long-tailed datasets.\nDataset #Train exam. #Dev exam. #Test exam. #Avg. tokens #Classes\nCMID 8,678 1,226 2,450 32.0 36\nIFLYTEK 10,920 1,213 2,599 289.2 119\nCTC 20,666 2,296 7,682 27.2 44\nMSRA 106,301 11,811 8,419 82.9 26\nR52 5,943 617 2,570 114.8 52\nto 1. We train the models for 10 epochs with pa-\ntience of 2 epochs. In order to stabilize the training\nprocedure, we add a linear warm-up for 1 epoch.\nThe maximum sequence length is set according\nto the dataset, specifically, 64 for CMID , 512 for\nIFLYTEK , 64 for CTC, 128 for MSRA , and 256 for\nR52.\nMetrics Since we are more concerned with\nmodel performance across different classes, we\nuse the macro F1 scores as main performance met-\nric. We also offer the macro F1 scores of head\n(Head scores) and tail classes (Tail scores) sepa-\nrately to gain a fine-grained understanding of the\nmodel performance. To separate head classes from\nthe tail classes, we sort all classes in descending\norder according to the number of examples within\neach class. According to the power law, we should\nget head classes that take up 80% of all examples.\nHowever, we find some tail classes with very lim-\nited examples can be included in this manner. So\nwe manually determine the percentage for each\ndataset, specifically, 55% for MSRA and R52, 65%\nfor CMID and IFLYTEK , and 80% for CTC. In\naddition, we also gather the accuracy scores (Acc\nscores) for reference. We take average scores over\n5 runs as the results, attached with variances.\n3.2 Comparison Results\nIn order to examine whether our hypothesis that\nPrompt-tuning can make PLMs good long-tailed\nlearners is valid or not, we first conduct an evalu-\nation on the long-tailed datasets. The results are\ngiven in Table 2.\nA key finding from the comparison results is\nthat Prompt-tuning outperforms CLS-tuning by large\nmargins across datasets in terms of F1 scores.\nAnd Prompt-tuning in fact owns performance with\nlower variances compared withCLS-tuning. Prompt-\ntuning even exhibits better F1 scores than calibrated\nCLS-tuning (e.g., focal loss in our case). Besides,\nfocal loss does not bring further improvement over\nPrompt-tuning, implying Prompt-tuning is a suffi-\n0 20 40 60 80 100 120\nClass index (from head to tail).\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Weight norm.\nHead Tail\nCLS-tuning\nCLS-tuning w.  focal loss\nPrompt-tuning\nFigure 3: Weight norm visualization. Classification\nweight norms of the model trained on IFLYTEK .\nciently good long-tailed learner. Contrarily, cali-\nbration methods can unexpectedly degrade perfor-\nmance of CLS-tuning (on MSRA ) due to inadequate\nhyperparameter search. Further, the comparison of\nHead and Tail scores hints that Prompt-tuning im-\nproves long-tailed performance by keeping a better\ntrade-off between the head and tail, where Prompt-\ntuning achieves much better results on the tail. Yet,\nPrompt-tuning could slightly give a negative impact\nto Head scores.\nOverall speaking, we can put that our hypothe-\nsis is valid, indicating a positive effect of Prompt-\ntuning.\n3.3 Weight Norm Visualization\nFor sake of a deeper investigation of the hypothe-\nsis, we visualize the weight norms customized to\ndifferent classes. The weights are essentially de-\nrived from the predictor of the classifier C. The\nmotivation behind the visualization is rooted on the\nwidely accepted decoupling property (Kang et al.,\n2020), which claims that the learning of the back-\nbone and classifier is in fact decoupled from each\nother. In other words, the long-tailed class distribu-\ntion affects the classifier a lot, but might have little\nimpact on the backbone. To this end, it is largely\n3301\nTable 2: Comparison results. AVG denotes average results over all datasets. The best AVG scores are boldfaced. The\nvariances are attached as subscripts.\nDataset C MID IFLYTEK CTC MSRA R52 A VG\nMetric Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1\nCLS-tuning 51.1 0.4 37.32.3 58.70.4 33.71.6 84.60.3 77.22.9 99.00.1 97.51.0 95.30.2 67.31.3 77.7 62.6\nw/η-norm 51.1 0.5 37.42.0 59.10.3 35.71.6 84.70.2 77.33.1 99.00.1 97.40.9 95.40.3 68.91.9 77.9 63.3\nw/ focal loss 51.00.7 42.11.3 58.80.3 36.01.6 84.30.4 78.52.4 99.00.1 96.81.2 95.70.2 72.82.3 77.8 65.2\nPrompt-tuning 49.3 0.7 43.40.7 61.20.6 44.41.0 84.20.1 80.90.1 99.10.0 97.80.3 95.70.1 85.30.6 77.9 70.4\nw/ focal loss 48.606 42.50.6 59.70.6 43.90.7 83.50.6 80.20.7 99.00.1 97.20.7 95.50.3 82.62.4 77.3 69.3\nMetric Head Tail Head Tail Head Tail Head Tail Head Tail Head Tail\nCLS-tuning 50.3 1.0 34.13.0 61.80.6 27.41.9 87.70.2 74.13.7 99.20.1 97.41.1 99.00.1 66.61.3 79.6 59.9\nw/η-norm 50.3 0.9 34.32.7 62.10.4 29.72.0 87.80.2 74.34.0 99.20.1 97.31.0 99.00.1 68.31.9 79.7 60.8\nw/ focal loss 49.80.8 40.21.5 62.00.4 30.21.9 87.50.3 75.93.1 99.30.0 96.71.3 99.00.0 72.32.4 79.5 63.1\nPrompt-tuning 48.4 1.0 42.20.7 63.60.4 40.11.2 87.40.2 79.00.2 99.20.1 97.70.3 98.60.1 85.00.6 79.4 68.8\nw/ focal loss 47.10.8 41.40.8 62.30.6 39.80.8 86.70.5 78.30.7 99.30.1 97.10.7 98.80.1 82.32.4 78.8 67.8\nacknowledged in previous studies (Lin et al., 2017;\nKang et al., 2020) that a good long-tailed learner\nshould make weight norms roughly in the same\nscale, while a bad one sees a weight norm decay\nfrom the head to tail. The visualization is shown in\nFigure 3.\nFrom the plot, we discover that CLS-\ntuning makes PLMs bad long-tailed learners, as it\npossesses large weight norms for the head while\nsmall ones for the tail. Applied to CLS-tuning, the\nfocal loss slightly flattens the weight norm slope.\nAs expected, Prompt-tuning makes PLMs own a\nway more flat distribution. This may be the reason\nwhy focal loss does not bring improvement over\nPrompt-tuning. While one may wonder whether\nweight norm regularization could boost long-tailed\nperformance, η-norm is actually a calibration\nmethod to adjust weight norms so that they can be\nin similar scales, showcasing minor improvement\nof weight norm regularization.\nThe contrast indicates that Prompt-tuning makes\nPLMs good long-tailed learners.\n4 Analyses\nAlthough we have verified that our hypothesis is\nvalid, we are more of thirst to see why Prompt-\ntuning can be so promising and provide further\nintuitions. To this demand, we consider three re-\nsearch questions here and carry out in-depth analy-\nses to answer them. We also hope the analyses can\nshed light on the design of Prompt-tuning itself in\nrelated areas.\n• RQ1: Does the shared embedding contribute\nto Prompt-tuning?\n• RQ2: Does the input structure (i.e., MLM\ninput) contribute to Prompt-tuning?\n• RQ3: Does the classifier structure and param-\neterization (e.g., layer normalization used in\nMLM head) contribute to Prompt-tuning?\n4.1 Impact of Shared Embedding\nThe first question comes into our mind is that\nwhether it is the parameter sharing between the clas-\nsifier and backbone that helps Prompt-tuning sur-\nvive from collapsing. Hence, we decouple the\nparameters shared by the classifier and backbone\n(i.e., without shared embedding during optimiza-\ntion) and compare the results in Table 3 before and\nafter the parameter decoupling.\nWe observe that the decoupling somehow has\nlittle impact on the performance (Prompt-tuning v.s.\nPrompt-tuning w/ ed.). Reversely, ed. even de-\ngrades the performance of Prompt-tuning. The\nphenomenon gives a possibly negative response\nto RQ1.\n4.2 Impact of Input Structure\nWe explore whether the input structure is a signifi-\ncant factor regarding long-tailed performance. To\nthis end, we arm CLS-tuning with MLM input so\nthat CLS-tuning may share the input structure and\nrepresentation for classification as Prompt-tuning.\nThe results in Table 3 demonstrate that the MLM\ninput somewhat affects the performance of CLS-\ntuning, in terms of both Acc and F1 scores ( CLS-\ntuning v.s. CLS-tuning w/ prompt). We attribute\nthe performance detriment to mismatch of the CLS\nhead and MLM input. That is, PLMs are not pre-\n3302\nTable 3: Analysis results. AVG denotes average results over all datasets. The best AVG scores are boldfaced. The\ncontent after CLS-tuning indicates the activation that is being used, where T is Tanh and R is ReLU. LN stands\nfor layer normalization. pt. is short for pretrained and ed. is short for embedding decoupling. The variances are\nattached as subscripts.\nDataset C MID IFLYTEK CTC MSRA R52 A VG\nMetric Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1\nCLS-tuning◦T 51.1 0.4 37.32.3 58.70.4 33.71.6 84.60.3 77.22.9 99.00.1 97.51.0 95.30.2 67.31.3 77.7 62.6\nw/η-norm 51.1 0.5 37.42.0 59.10.3 35.71.6 84.70.2 77.33.1 99.00.1 97.40.9 95.40.3 68.91.9 77.9 63.3\nw/ focal loss 51.00.7 42.11.3 58.80.3 36.01.6 84.30.4 78.52.4 99.00.1 96.81.2 95.70.2 72.82.3 77.8 65.2\nCLS-tuning◦R 50.9 0.4 34.51.4 58.70.3 33.31.1 84.40.4 77.11.0 99.00.1 97.70.5 94.20.4 56.22.5 77.4 59.8\nw/η-norm 50.9 0.5 34.81.8 58.40.3 33.31.0 84.60.4 78.01.5 99.10.0 97.80.5 94.30.3 56.31.9 77.5 60.0\nw/ focal loss 51.00.5 40.11.5 58.80.4 34.60.1 84.60.3 76.90.6 99.00.1 97.01.5 95.10.3 66.02.7 77.7 62.9\nCLS-tuning◦R\nw/ prompt 49.7 0.5 33.10.4 58.40.3 32.81.0 84.60.1 77.23.0 99.00.1 96.90.3 94.10.3 54.53.1 77.2 58.9\nw/ LN 51.3 0.6 42.01.4 59.70.6 39.10.8 84.60.5 79.42.2 99.10.1 97.10.8 96.10.2 77.73.5 78.2 67.1\nw/ pt. LN 50.8 0.6 42.51.2 59.40.4 41.40.9 84.40.5 79.71.5 99.10.1 97.70.5 96.20.2 82.01.8 78.0 68.7\nPrompt-tuning 49.3 0.7 43.40.7 61.20.6 44.41.0 84.20.1 80.90.1 99.10.0 97.80.3 95.70.1 85.30.6 77.9 70.4\nw/ ed. 49.4 0.7 43.60.7 61.00.7 44.41.0 84.20.4 80.50.9 99.00.2 96.91.4 95.70.2 84.91.0 77.9 70.1\nMetric Head Tail Head Tail Head Tail Head Tail Head Tail Head Tail\nCLS-tuning◦T 50.3 1.0 34.13.0 61.80.6 27.41.9 87.70.2 74.13.7 99.20.1 97.41.1 99.00.1 66.61.3 79.6 59.9\nw/η-norm 50.3 0.9 34.32.7 62.10.4 29.72.0 87.80.2 74.34.0 99.20.1 97.31.0 99.00.1 68.31.9 79.7 60.8\nw/ focal loss 49.80.8 40.21.5 62.00.4 30.21.9 87.50.3 75.93.1 99.30.0 96.71.3 99.00.0 72.32.4 79.5 63.1\nCLS-tuning◦R 50.7 0.4 30.61.7 61.70.6 26.81.3 87.50.2 74.11.3 99.20.1 97.60.5 99.00.1 55.42.6 79.6 56.9\nw/η-norm 50.4 0.7 31.02.2 61.70.7 26.81.4 87.70.3 75.11.9 99.20.1 97.70.5 99.00.2 55.52.0 79.6 57.2\nw/ focal loss 50.30.5 37.61.7 61.90.3 28.41.6 87.60.3 73.70.8 99.20.1 96.91.6 99.00.2 65.32.8 79.6 60.4\nCLS-tuning◦R\nw/ prompt 49.7 0.9 29.10.5 61.50.9 26.31.3 87.50.1 74.23.9 99.20.1 96.80.3 99.00.1 53.63.2 79.4 56.0\nw/ LN 50.0 0.7 40.01.6 63.10.6 33.70.9 87.60.4 77.02.7 99.30.2 97.00.8 99.00.1 77.33.6 79.8 65.0\nw/ pt. LN 49.5 0.5 40.81.4 62.70.3 36.61.1 87.50.4 77.41.9 99.20.1 97.60.6 99.00.1 81.61.9 79.6 66.8\nPrompt-tuning 48.4 1.0 42.20.7 63.60.4 40.11.2 87.40.2 79.00.2 99.20.1 97.70.3 98.60.1 85.00.6 79.4 68.8\nw/ ed. 48.2 1.0 42.50.7 63.60.4 40.11.2 87.40.3 78.51.1 99.30.1 96.81.5 98.70.2 84.61.0 79.4 68.5\ntrained in the way that CLS head should decode\nthe MLM input. And the results naturally suggest\na possibly negative response toRQ2.\n4.3 Impact of Classifier Structure and\nParameterization\nWe also investigate the impact of the classifier struc-\nture and parameterization, given structural and pa-\nrameter differences between classifier heads used\nby CLS-tuning and Prompt-tuning. We aim to check\nwhether it is the discrepancy between classifiers\nthat biases the learning. To study the impact, we\nreplace the Tanh in CLS-tuning with ReLU. We use\nReLU here as an alternative of GELU owing to the\nfact that ReLU is more prevalent in the finetuning\nstage. Then, by adding a layer normalization af-\nter the ReLU activation, we fill the structural gap\nbetween two classifiers. We also perform a natu-\nral follow-on action, re-using the statistics of the\nlayer normalization from the MLM head to further\nenhance the classifier. The results are presented in\nTable 3.\nIt is revealed that the ReLU variant sometimes\nyields surprisingly deteriorated results when com-\npared to CLS-tuning with Tanh. However, when\nthe ReLU variant is additionally armed with a suc-\nceeding layer normalization (CLS-tuning w/ LN),\nit can surpass the original CLS-tuning by certain\nmargins. Notably, CLS-tuning w/ LN has better\nAcc scores than Prompt-tuning does, potentially\nsuggesting the balanced use of CLS-tuning w/ LN\nin real-world applications. Besides, by re-using\nthe MLM layer normalization ( CLS-tuning w/ pt.\nLN), CLS-tuning approximates Prompt-tuning at\nonce. The results imply an absolutely positive re-\nsponse toRQ3. We conjecture the observation is\nunderpinned by an information perspective towards\nregulated features.\nFor the ReLU variant, negative features will be\nzero out, leading to a cut-down of information. The\n3303\n0 5\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nCLS-tuning  Tanh\n0 5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nCLS-tuning  ReLU\n0 5\n0\n1\n2\n3\n4\nCLS-tuning  ReLU\nw.  LN\n0 5\n2\n0\n2\n4\n6\n8\n10\n12\nCLS-tuning  ReLU\nw.  pt. LN\nFeature index.\nFeature value.\nHead class Tail class\nFigure 4: An illustration of sampled feature distribu-\ntions, for both the head and tail.\ninformation cut-down can be referred to as “dying\nReLU” problem (Lu et al., 2019; Kayser, 2017)\nwhen negative features take a large portion. The\ninformation loss may be problematic to the head,\nand even more unfriendly to the tail that is under-\nrepresented (i.e., with much fewer examples). As\na consequence, under-represented classes can be\nrepresented with high bias (and potentially high\nvariance). In contrast, Tanh manipulates features\nwithout any tailoring, but restricts values to a con-\nstant range (i.e., from -1 to 1). Despite the reduced\nlearning burden, Tanh suffers from large saturation\narea (Xu et al., 2016). Thereby, the information of\nsome tail classes can be compressed with detriment.\nExisting literature (Girshick et al., 2014; He et al.,\n2015; Xu et al., 2015) calibrates the situation by\nrelaxing negative features. On the other hand, the\nlayer normalization can compensate the informa-\ntion loss caused by ReLU. With learned affinities\n(a.k.a., element-wise weights and biases), the layer\nnormalization shall properly re-locate and re-scale\nthe ReLU-activated features so that knowledge at-\ntained from the head can be transferred to the tail\nand the debuff of ReLU can be alleviated.\nWe add an illustration via Figure 4 for a more\nintuitive understanding of the above explanation.\nTaking examples of a sampled head class ( com-\nmerce) and a sampled tail class ( delivery) from\nIFLYTEK , we plot the distributions of the first 10\nfeatures in feature vectors (i.e., final hidden state\nvectors correspond to [CLS] tokens) derived from\nthese examples. From the plot, we can see that\nReLU certainly drops information of quite a few\nfeatures of the tail (6 out of 10) and Tanh com-\npulsively has features descend into the pre-defined\nrange to involve themselves in optimization, both\nTable 4: Statistics of few-shot datasets.\nDataset #Test exam. #Avg. tokens\nECOM 610 47.7\nRTE 277 52.3\nBOOLQ 3,270 105.3\nlimiting the expressiveness of features. The ran-\ndomly initialized layer normalization re-activates\nthe dead features from the ReLU by re-locating\nthem, but largely leaving them in fixed scales (6 out\nof 10). Furthermore, the pretrained layer normal-\nization from the MLM head transfers knowledge\nfrom the head to tail and makes the features of the\ntail diversely distributed by re-scaling them.\n5 Applicability to Few-shot Classification\nFrom the analyses above, we come up with an al-\nternative to CLS-tuning that owns a comparable\neffectiveness with Prompt-tuning for long-tailed\nclassification, by equipping CLS-tuning with the\nstructure and the layer normalization affinities of\nthe pretrained MLM head. With this much simpler\nsurrogate classifier, we prefer to retrospectively\nexplore how the classifier would perform in the\nfew-shot scenario, especially when compared with\nPrompt-tuning. The intuition is that, while the layer\nnormalization can not transfer knowledge from the\nhead to tail any longer in few-shot classification,\nits innate friendliness to the tail can probably be\napplied to few-shot classes for generalization.\nTherefore, we conduct experiments on three\nfew-shot datasets, ECOM from FewCLUE (Xu\net al., 2021), and RTE, BOOL Q from Super-\nGLUE (Wang et al., 2019a; Schick and Schütze,\n2021). ECOM is a review sentiment classification\ndataset, RTE (Wang et al., 2019b) is a two-way en-\ntailment dataset, and BOOL Q (Clark et al., 2019)\nis a yes-or-no question answering dataset.\nWhile ECOM has already been designed for few-\nshot learning, the others are not originally. For\nthe latter two, we treat the original development\nset as the test set following Gao et al. (2021),\nand randomly sample 32 examples uniformly from\nthe original training set as the training set follow-\ning Schick and Schütze (2021). To strictly fol-\nlowing a true few-shot setting (Perez et al., 2021),\n32 examples that do not overlap with those in our\ntraining set are used to form the development set.\nThe implementation is akin to the one we used for\nthe long-tailed experiments. The batch size is re-\n3304\nTable 5: Applicability to few-shot classification results.\nAVG denotes average results over all datasets. The best\nAVG scores are boldfaced. The variances are attached\nas subscripts.\nDataset E COM RTE BOOLQ A VG\nMetric Acc Acc Acc Acc\nCLS-tuning◦T 63.6 10.5 48.63.9 60.11.3 57.4\nCLS-tuning◦R 64.7 2.7 48.73.6 55.27.5 56.2\nw/ prompt 60.0 5.5 49.72.3 59.94.3 56.5\nw/ pt. LN 68.7 5.0 55.01.8 59.11.7 60.9\nPrompt-tuning 77.3 9.3 51.81.8 60.71.0 63.3\nduced to 2 due to the small scale of training data.\nbert-base-uncased is leveraged as the backbone\nfor English datasets. The maximum length is set to\n64, 128, and 256 respectively for ECOM , RTE, and\nBOOL Q. The statistics of these datasets are listed\nin Table 4.\nThe example templates and verbalizers are listed\nin Appendix B.\nMoreover, since few-shot experiments are sen-\nsitive to the choice of hyperparameters, we again\ntake average accuracy scores over 5 runs as the\nresults, attached with variances.\nWe can observe from Table 5 that CLS-\ntuning with ReLU and pretrained layer normaliza-\ntion can perform better than other CLS-tuning base-\nlines, but may not necessarily hold across datasets.\nThe filled gap between CLS-tuning and Prompt-\ntuning shows the scalability of our finding to few-\nshot classification. However, we encourage future\nwork to explore the regime for a more comprehen-\nsive understanding.\n6 Related Work\nPLMs have brought classification tasks to a brand-\nnew stage where the solutions to these tasks are\nway much simpler than ever (Devlin et al., 2019;\nWang et al., 2019b). However, PLMs are still sub-\noptimal for some corner cases, such as few-shot\nclassification (Zhang et al., 2021) and long-tailed\nclassification (Li et al., 2020).\nFor few-shot classification,Prompt-tuning, which\nfinetunes models in a language modeling fashion, is\nincreasingly taking a central role in the mainstream\nmethods (Liu et al., 2021). Since PLMs are mostly\ntrained with language modeling objectives, Prompt-\ntuning becomes, as water is to fish, the key to un-\nearthing the few-shot or even zero-shot learning\ncapabilities of PLMs (Brown et al., 2020; Schick\nand Schütze, 2021; Scao and Rush, 2021). Instantly\nafter the success, the engineering of prompts drives\nrelated research on prompt search/generation (Jiang\net al., 2020; Shin et al., 2020; Gao et al., 2021)\nand various downstream applications such as text\ngeneration (Li and Liang, 2021), relation extrac-\ntion (Chen et al., 2020), and entity typing (Ding\net al., 2021). Recently, Prompt-tuning also serves\nas an alternative way for parameter-efficient fine-\ntuning by only finetuning parameters of the in-\nserted continuous prompts (Lester et al., 2021; Ma\net al., 2022), in place of previously adopted adapter-\ntuning (Houlsby et al., 2019). The parameter effi-\nciency brought by Prompt-tuning has blazed a trail\nfor increasingly large language models.\nIn contrast, little work has been investigated to\nmake PLMs good long-tailed learners. Intuitively,\nthe tail classes are essentially few-shot ones. Thus,\nwe speculate that Prompt-tuning is also a promising\nchoice to make PLMs good long-tailed learners via\ntransferring knowledge of head classes. As long-\ntailed classification is a long-standing problem in\nthe general area of machine learning (Lin et al.,\n2017; Liu et al., 2019b; Zhou et al., 2020; Kang\net al., 2020; Tang et al., 2020) and the long-tailed\nphenomenon also exists in the domain of natural\nlanguage processing, we believe that a systematic\nexploration on whether and whyPrompt-tuning can\nmake PLMs good long-tailed learners will facilitate\nfurther advances in the related areas.\n7 Conclusions\nInspired by the success of Prompt-tuning in few-\nshot learning, we empirically examine whether\nPrompt-tuning can make PLMs good long-tailed\nlearners in this work. The results validate the hy-\npothesis. We also conduct in-depth analyses on\nwhy Prompt-tuning benefits PLMs for long-tailed\nclassification, from the perspectives of coupling,\nclassifier, and input respectively, to offer further\nintuitions. Based on the analyses, we summarize\nthat the classifier structure and parameterization\nare crucial for enhancing long-tailed performance\nof PLMs, in contrast to other factors. Extended em-\npirical evaluation results on few-shot classification\nshow that our finding would shed light on related\nwork that seeks to boost Prompt-tuning.\nLimitations\nSince prompt-tuning is shown sensitive to small\nvariations of templates, it should be performed with\n3305\nreasonable templates. However, we do not study\nthe impact of different templates since our work\nis not concerned with finding a good template for\nlong-tailed classification.\nAcknowledgements\nWe would like to thank Yujia Qin and Zhiyuan\nLiu from Tsinghua University for their helpful sug-\ngestions. This research was supported in part by\nNatural Science Foundation of Beijing (grant num-\nber: 4222036) and Huawei Technologies (grant\nnumber: TC20201228005).\nReferences\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization. arXiv, 1607.06450.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nNeurIPS.\nNan Chen, Xiangdong Su, Tongyang Liu, Qizhi Hao,\nand Ming Wei. 2020. A benchmark dataset and case\nstudy for chinese medical question intent classifica-\ntion. BMC Medical Informatics Decision Making,\n20-S(3):125.\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng,\nYunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. 2021. Knowprompt: Knowledge-\naware prompt-tuning with synergistic optimization\nfor relation extraction. arXiv, 2104.07650.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In NAACL-\nHLT, pages 2924–2936.\nFranca Debole and Fabrizio Sebastiani. 2004. An analy-\nsis of the relative difficulty of reuters-21578 subsets.\nIn LREC.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT, pages 4171–4186.\nNing Ding, Yulin Chen, Xu Han, Guangwei Xu,\nPengjun Xie, Hai-Tao Zheng, Zhiyuan Liu, Juanzi\nLi, and Hong-Gee Kim. 2021. Prompt-learning for\nfine-grained entity typing. arXiv, 2108.10604.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In ACL-IJCNLP, pages 3816–3830.\nRoss B. Girshick, Jeff Donahue, Trevor Darrell, and\nJitendra Malik. 2014. Rich feature hierarchies for\naccurate object detection and semantic segmentation.\nIn CVPR, pages 580–587.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Delving deep into rectifiers: Surpassing\nhuman-level performance on imagenet classification.\nIn ICCV, pages 1026–1034.\nDan Hendrycks and Kevin Gimpel. 2016. Bridging non-\nlinearities and stochastic regularizers with gaussian\nerror linear units. arXiv, 1606.08415.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In\nICML, volume 97 of PMLR, pages 2790–2799.\nShengding Hu, Ning Ding, Huadong Wang, Zhiyuan\nLiu, Juanzi Li, and Maosong Sun. 2021. Knowl-\nedgeable prompt-tuning: Incorporating knowledge\ninto prompt verbalizer for text classification. arXiv,\n2108.02035.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know. TACL, 8:423–438.\nBingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng\nYan, Albert Gordo, Jiashi Feng, and Yannis Kalan-\ntidis. 2020. Decoupling representation and classifier\nfor long-tailed recognition. In ICLR.\nMike Kayser. 2017. What is the “dying relu” problem in\nneural networks? https://qr.ae/pv2H1S. Quora.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In EMNLP, pages 3045–3059.\nGina-Anne Levow. 2006. The third international\nchinese language processing bakeoff: Word seg-\nmentation and named entity recognition. In\nSIGHAN@COLING/ACL, pages 108–117.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nACL/IJCNLP, pages 4582–4597.\nXiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang,\nFei Wu, and Jiwei Li. 2020. Dice loss for data-\nimbalanced NLP tasks. In ACL, pages 465–476.\nTsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming\nHe, and Piotr Dollár. 2017. Focal loss for dense\nobject detection. In ICCV, pages 2999–3007.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv, 2107.13586.\n3306\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019a.\nRoberta: A robustly optimized BERT pretraining\napproach. arXiv, 1907.11692.\nZiwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang,\nBoqing Gong, and Stella X. Yu. 2019b. Large-scale\nlong-tailed recognition in an open world. In CVPR,\npages 2537–2546.\nLu Lu, Yeonjong Shin, Yanhui Su, and George E. Kar-\nniadakis. 2019. Dying relu and initialization: Theory\nand numerical examples. arXiv, abs/1903.06733.\nFang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan\nWang, Wei Wu, Xiaojun Quan, and Dawei Song.\n2022. Xprompt: Exploring the extreme of prompt\ntuning. arXiv, abs/2210.04457.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. arXiv,\n2105.11447.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR, 21:140:1–140:67.\nTeven Le Scao and Alexander M. Rush. 2021. How\nmany data points is a prompt worth? In NAACL-\nHLT, pages 2627–2636.\nTimo Schick and Hinrich Schütze. 2021. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In NAACL-HLT, pages 2339–2352.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In EMNLP, pages\n4222–4235.\nKaihua Tang, Jianqiang Huang, and Hanwang Zhang.\n2020. Long-tailed classification by keeping the good\nand removing the bad momentum causal effect. In\nNeurIPS.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS, pages 5998–6008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019a. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. In NeurIPS, pages 3261–3275.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In ICLR.\nBing Xu, Ruitong Huang, and Mu Li. 2016. Revise sat-\nurated activation functions. arXiv, abs/1602.05980.\nBing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. 2015.\nEmpirical evaluation of rectified activations in con-\nvolutional network. arXiv, abs/1505.00853.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao,\nYudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong\nYu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi,\nYiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang,\nWeijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,\nYiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao,\nQipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang\nYang, Kyle Richardson, and Zhenzhong Lan. 2020.\nCLUE: A chinese language understanding evaluation\nbenchmark. In COLING, pages 4762–4772.\nLiang Xu, Xiaojing Lu, Chenyang Yuan, Xuanwei\nZhang, Hu Yuan, Huilin Xu, Guoao Wei, Xiang Pan,\nand Hai Hu. 2021. Fewclue: A chinese few-shot\nlearning evaluation benchmark. arXiv, 2107.07498.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Wein-\nberger, and Yoav Artzi. 2021. Revisiting few-sample\nBERT fine-tuning. In ICLR.\nBoyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min\nChen. 2020. BBN: bilateral-branch network with\ncumulative learning for long-tailed visual recognition.\nIn CVPR, pages 9716–9725.\nHui Zong, Jinxuan Yang, Zeyu Zhang, Zuofeng Li, and\nXiaoyan Zhang. 2021. Semantic categorization of\nchinese eligibility criteria in clinical trials using ma-\nchine learning methods. BMC Medical Informatics\nDecision-Making, 21(1):128.\n3307\nA Templates and Verbalizers for\nLong-tailed Datasets\nThe templates and verbalizers for three long-tailed\ndatasets are separately listed in Table 6, Table 7,\nTable 8, Table 9, and Table 10.\nB Templates and Verbalizers for Few-shot\nDatasets\nThe templates are:\n• ECOM : x. 这个评论中的物品[MASK]好.\n(x. The product mentioned in the review is\n[MASK] good.)\n• RTE & BOOL Q: p. Question: q? The answer:\n[MASK].\nwhere p denotes the passage or premise, and q\ndenotes the question or hypothesis. Accordingly,\nthe verbalizers are:\n• E COM : {label:positive→很(very),\nlabel:negative→不(not)}\n• R TE: {label:entailment→yes,\nlabel:not_entailment→no}\n• B OOL Q: {label:true→yes,label:false→no}\nHere, English translations are additionally used for\nECOM .\n3308\nDataset C MID\nTemplate x?这个问题的意图是[MASK].\nVerbalizer\nlabel:病症治疗方法→病症治疗方法(disease treatment),\nlabel:病症定义→病症定义(disease definition),\nlabel:病症临床表现(病症表现)→病症临床表现病症表现(disease symptom),\nlabel:药物适用症→药物适用症(medicine applicability),\nlabel:其他无法确定→其他无法确定(others undefined),\nlabel:病症禁忌→病症禁忌(disease contradiction),\nlabel:病症相关病症→病症相关病症(disease related diseases),\nlabel:其他对比→其他对比(others contrast),\nlabel:药物副作用→药物副作用(medicine side-effect),\nlabel:药物禁忌→药物禁忌(medicine contradiction),\nlabel:其他多问→其他多问(others multiple questions),\nlabel:病症病因→病症病因(disease cause),\nlabel:治疗方案化验/体检方案→治疗方案化验体检方案(treatment exmination),\nlabel:治疗方案恢复→治疗方案恢复(treatment recovery),\nlabel:病症严重性→病症严重性(disease severity),\nlabel:病症治愈率→病症治愈率(disease cure rate),\nlabel:药物用法→药物用法(medicine usage),\nlabel:药物作用→药物作用(medicine effect),\nlabel:其他两性→其他两性(others sex),\nlabel:治疗方案正常指标→治疗方案正常指标(treatment normal indication),\nlabel:其他养生→其他养生(others health),\nlabel:治疗方案方法→治疗方案方法(treatment method),\nlabel:病症传染性→病症传染性(disease infectivity),\nlabel:药物成分→药物成分(medicine component),\nlabel:病症预防→病症预防(disease prevention),\nlabel:治疗方案恢复时间→治疗方案恢复时间(treatment recovery duration),\nlabel:病症推荐医院→病症推荐医院(disease recommende hospital),\nlabel:治疗方案费用→治疗方案费用(treatment cost),\nlabel:治疗方案临床意义→治疗方案临床意义(treatment significance),\nlabel:其他设备用法→其他设备用法(others device usage),\nlabel:治疗方案疗效→治疗方案疗效(treatment efficacy),\nlabel:药物价钱→药物价钱(medicine price),\nlabel:治疗方案有效时间→治疗方案有效时间(treatment effective duration),\nlabel:其他整容→其他整容(others cosmetic),\nlabel:病症所属科室→病症所属科室(disease department),\nlabel:治疗方案治疗时间→治疗方案治疗时间(treatment duration)\nTable 6: Template and verbalizer for CMID (with necessary translations).\n3309\nDataset I FLYTEK\nTemplate x.这句话描述的物品属于[MASK].\nVerbalizer\nlabel:打车→打车(taxi),label:导航→导航(navigation),label:WIFI→wifi,\nlabel:租车→租车(car rent),label:同城→同城(urban service),label:快递→快递(express),\nlabel:婚庆→婚庆(wedding),label:家政→家政(house service),\nlabel:公共交通→公共交通(public transport),label:政务→政务(government affair),\nlabel:社区服务→社区服务(community service),label:薅羊毛→薅羊毛(deal hunter),\nlabel:魔幻→魔幻(magic game),label:仙侠→仙侠(warrior game),\nlabel:卡牌→卡牌(card game),label:空战→空战(flight game),\nlabel:射击→射击(shooting game),label:休闲→休闲(leisure game),\nlabel:动作→动作(action game),label:体育→体育(sports game),\nlabel:棋牌→棋牌(board game),label:养成→养成(simulation game),\nlabel:策略→策略(strategy game),label:MOBA→MOBA,label:辅助工具→辅助工具(aid tool),\nlabel:约会→约会(date),label:通讯→通讯(communication),label:工作→工作(work),\nlabel:论坛→论坛(BBS),label:婚恋→婚恋(marriage),label:情侣→情侣(lover),\nlabel:社交→社交(social),label:生活→生活(life),label:博客→博客(blog),\nlabel:新闻→新闻(news),label:漫画→漫画(cartoon), label:小说→小说(novel),\nlabel:技术→技术(technology),label:教辅→教辅(teaching),label:问答→问答(QA),\nlabel:搞笑→搞笑(fun),label:杂志→杂志(magazine),label:百科→百科(wikipedia),\nlabel:影视→影视(TV),label:求职→求职(job),label:兼职→兼职(part-time),\nlabel:视频→视频(video),label:短视频→短视频(clips),label:音乐→音乐(music),\nlabel:直播→直播(live),label:电台→电台(radio),label:K歌→k歌(KTV),\nlabel:成人→成人(adult),label:中小学→中小学(school),label:职考→职考(exam),\nlabel:公务员→公务员(civil servant),label:视频教育→视频教育(video edu.),\nlabel:高等教育→高等教育(advanced edu.),label:成人教育→成人教育(adult edu.),\nlabel:艺术→艺术(art),label:语言(非英语)→语言非英语(non-english),\nlabel:英语→英语(english),label:旅游→旅游(travel),label:预定→预定(preservation),\nlabel:民航→民航(flight),label:铁路→铁路(railway),label:酒店→酒店(hotel),\nlabel:行程→行程(route),label:民宿→民宿(BnB),label:出国→出国(abroad),\nlabel:工具→工具(general tool),label:亲子→亲子(kid),label:母婴→母婴(infant),\nlabel:驾校→驾校(driver),label:违章→违章(violation),label:汽车→汽车(vehicle),\nlabel:汽车交易→汽车交易(vehicle trade),label:养车→养车(vehicle maintainance),\nlabel:行车→行车(driving aid),label:租房→租房(house rent),\nlabel:买房→买房(house purchase),label:装修→装修(house decoration),\nlabel:问诊→问诊(inquiry),label:养生→养生(health preservation),\nlabel:医疗→医疗(medical),label:减肥→减肥(slim),label:美妆→美妆(makeup),\nlabel:菜谱→菜谱(menu),label:餐饮→餐饮(restaurant),\nlabel:健身→健身(fit),label:支付→支付(payment),label:保险→保险(insurance),\nlabel:股票→股票(stock),label:借贷→借贷(loan),label:理财→理财(finance management),\nlabel:彩票→彩票(lottery),label:记账→记账(acount keeping),label:银行→银行(bank),\nlabel:剪辑→剪辑(film editing),label:修图→修图(photo editing),\nlabel:相机→相机(camera),label:绘画→绘画(painting),label:二手→二手(second-hand),\nlabel:电商→电商(e-commerce),label:团购→团购(group purchase),\nlabel:外卖→外卖(take-out),label:美颜→美颜(beauty),label:电子→电子(electronics),\nlabel:电影→电影(movies),label:超市→超市(market),label:购物→购物(shopping),\nlabel:笔记→笔记(notes),label:办公→办公(office),label:日程→日程(schedule),\nlabel:女性→女性(female),label:经营→经营(management),label:收款→收款(check),\nlabel:体育咨讯→体育咨讯(sports info.),label:其他→其他(others)\nTable 7: Template and verbalizer for IFLYTEK (with necessary translations).\n3310\nDataset C TC\nTemplate x.这个标准的类别是[MASK].\nVerbalizer\nlabel:治疗或手术→治疗或手术(therapy or surgery),\nlabel:体征→体征(body sign),label:成瘾行为→成瘾行为(addictive behavior),\nlabel:年龄→年龄(age),label:疾病→疾病(disease),\nlabel:器官组织状态→器官组织状态(organ or tissue status),\nlabel:过敏耐受→过敏耐受(allergy tolerance),\nlabel:依存性→依存性(compliance with protocol),\nlabel:风险评估→风险评估(risk assessment),\nlabel:怀孕相关→怀孕相关(pregnancy-related activity),\nlabel:诊断→诊断(diagnostic),label:复合→复合(multiple criteria),\nlabel:实验室检查→实验室检查(laboratory examination),\nlabel:知情同意→知情同意(consent),label:献血→献血(blood donation),\nlabel:参与其它试验→参与其它试验(enrollment in other studies),\nlabel:药物→药物(pharmaceutical substance or drug),\nlabel:能力→能力(capacity),label:饮食→饮食(diet),\nlabel:特殊病人体征→特殊病人体征(special patient characteristic),\nlabel:疾病分期→疾病分期(non-Neoplasm disease stage),\nlabel:研究者决定→研究者决定(researcher decision),\nlabel:数据可及性→数据可及性(data accessiblility),\nlabel:预期寿命→预期寿命(life expectancy),\nlabel:肿瘤进展→肿瘤进展(neoplasm status),\nlabel:读写能力→读写能力(literacy),\nlabel:病例来源→病例来源(patience source),\nlabel:锻炼→锻炼(exercise),label:症状→症状(symptom),\nlabel:受体状态→受体状态(receptor status),\nlabel:口腔相关→口腔相关(oral related),\nlabel:种族→种族(ethnicity),\nlabel:健康群体→健康群体(the healthy),\nlabel:残疾群体→残疾群体(disabilities),\nlabel:设备→设备(device),label:性别→性别(gender),\nlabel:吸烟状况→吸烟状况(smoking status),\nlabel:性取向→性取向(sex related),\nlabel:护理→护理(nursing),label:睡眠→睡眠(bedtime),\nlabel:酒精使用→酒精使用(alcohol consumption),\nlabel:居住情况→居住情况(living condition),\nlabel:教育状况→教育状况(education),\nlabel:伦理审查→伦理审查(ethical audit)\nTable 8: Template and verbalizer for CTC (with necessary translations).\n3311\nDataset M SRA\nTemplate x.这句话中的e是[MASK].\nVerbalizer\nlabel:integer→整数(integer),label:ordinal→序数(ordinal),\nlabel:location→地点(location),label:date→日期(date),\nlabel:organization→机构(organization),label:person→人物(person),\nlabel:money→钱款(money),label:duration→时段(duration),\nlabel:time→时间(time),label:length→长度(length),\nlabel:age→年龄(age),label:frequency→频率(frequency),\nlabel:angle→角度(angle),label:phone→电话(phone),\nlabel:percent→百分数(percent),label:fraction→分数(fraction),\nlabel:weight→重量(weight),label:area→面积(area),\nlabel:capacity→容积(capacity),label:decimal→小数(decimal),\nlabel:measure→其他度量(other measure),label:speed→速度(speed),\nlabel:temperature→温度(temperature),label:postal code→邮政编码(postal code),\nlabel:rate→比率(rate),label:www→网址(website)\nTable 9: Template and verbalizer for MSRA .\nDataset R52\nTemplate x. This is[MASK].\nVerbalizer\nlabel:copper→copper,label:livestock→livestock,\nlabel:gold→gold,label:money-fx→money fx,\nlabel:tea→tea,label:ipi→ipi,\nlabel:trade→trade,label:cocoa→cocoa,\nlabel:iron-steel→iron steel,label:reserves→reserves,\nlabel:zinc→zinc,label:nickel→nickel,\nlabel:ship→ship,label:cotton→cotton,\nlabel:platinum→platinum,label:alum→alum,\nlabel:strategic-metal→strategic metal,label:instal-debt→instal debt,\nlabel:lead→lead,label:housing→housing,\nlabel:gnp→gnp,label:sugar→sugar,\nlabel:rubber→rubber,label:dlr→dlr,\nlabel:tin→tin,label:interest→interest,\nlabel:income→income,label:crude→crude,\nlabel:coffee→coffee,label:jobs→jobs,\nlabel:meal-feed→meal feed,label:lei→lei,\nlabel:lumber→lumber,label:gas→gas,\nlabel:nat-gas→nat gas,label:veg-oil→veg oil,\nlabel:orange→orange,label:heat→heat,\nlabel:wpi→wpi,label:cpi→cpi,\nlabel:earn→earn,label:jet→jet,\nlabel:potato→potato,label:bop→bop,\nlabel:money-supply→money supply,label:carcass→carcass,\nlabel:acq→acq,label:pet-chem→pet chem,\nlabel:grain→grain,label:fuel→fuel,\nlabel:retail→retail,label:cpu→cpu,\nTable 10: Template and verbalizer for R52.\n3312"
}