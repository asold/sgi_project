{
    "title": "ProtHyena: A fast and efficient foundation protein language model at single amino acid Resolution",
    "url": "https://openalex.org/W4391097114",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5029645244",
            "name": "Y. T. Zhang",
            "affiliations": [
                "National Institute of Information and Communications Technology",
                "Tokyo Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5035876897",
            "name": "Manabu Okumura",
            "affiliations": [
                "National Institute of Information and Communications Technology",
                "Tokyo Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2730472814",
        "https://openalex.org/W2912990441",
        "https://openalex.org/W2097632784",
        "https://openalex.org/W2984761660",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2527759989",
        "https://openalex.org/W4205773061",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W3187251571",
        "https://openalex.org/W4281758439",
        "https://openalex.org/W4313442864",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3040739508",
        "https://openalex.org/W3111553449",
        "https://openalex.org/W2147526198",
        "https://openalex.org/W3208827570",
        "https://openalex.org/W3034573343",
        "https://openalex.org/W2794004073",
        "https://openalex.org/W4327550249",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3095583226",
        "https://openalex.org/W3198971594",
        "https://openalex.org/W3101509328",
        "https://openalex.org/W4382603228",
        "https://openalex.org/W4321593177",
        "https://openalex.org/W4311000453",
        "https://openalex.org/W2951433247",
        "https://openalex.org/W3111174583",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W2735621019",
        "https://openalex.org/W4287117648",
        "https://openalex.org/W2379594833",
        "https://openalex.org/W1816313093",
        "https://openalex.org/W3174418826",
        "https://openalex.org/W4312847199",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W4296907865",
        "https://openalex.org/W4297243391"
    ],
    "abstract": "Abstract The emergence of self-supervised deep language models has revolutionized natural language processing tasks and has recently extended its applications to biological sequence analysis. Traditional models, primarily based on the Transformer and BERT architectures, demonstrate substantial effectiveness in various applications. However, these models are inherently constrained by the attention mechanismâ€™s quadratic computational complexity O ( L 2 ), limiting their efficiency and the length of context they can process. Addressing these limitations, we introduce ProtHyena , a novel approach that leverages the Hyena operator. This innovative methodology circumvents the constraints imposed by attention mechanisms, thereby reducing the time complexity to a subquadratic, enabling the modeling of extra-long protein sequences at the single amino acid level without the need to compress data. ProtHyena is able to achieve, and in many cases exceed, state-of-the-art results in various downstream tasks with only 10% of the parameters typically required by attention-based models. The architecture of ProtHyena presents a highly efficient solution for training protein predictors, offering a promising avenue for fast and efficient analysis of biological sequences.",
    "full_text": null
}