{
    "title": "LLM Instruction-Example Adaptive Prompting (LEAP) Framework for Clinical Relation Extraction",
    "url": "https://openalex.org/W4389885297",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5026404757",
            "name": "Huixue Zhou",
            "affiliations": [
                "University of Minnesota"
            ]
        },
        {
            "id": "https://openalex.org/A5101907703",
            "name": "Mingchen Li",
            "affiliations": [
                "University of Minnesota"
            ]
        },
        {
            "id": "https://openalex.org/A5113636793",
            "name": "Yongkang Xiao",
            "affiliations": [
                "University of Minnesota"
            ]
        },
        {
            "id": "https://openalex.org/A5100462722",
            "name": "Han Yang",
            "affiliations": [
                "University of Minnesota"
            ]
        },
        {
            "id": "https://openalex.org/A5100421963",
            "name": "Rui Zhang",
            "affiliations": [
                "University of Minnesota"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2768488789",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W3213921583",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W4221153690",
        "https://openalex.org/W4388605937",
        "https://openalex.org/W4387500346",
        "https://openalex.org/W4387617694",
        "https://openalex.org/W4388092533",
        "https://openalex.org/W4319663047",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4386120650",
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4386081793",
        "https://openalex.org/W4353112507",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4367365615",
        "https://openalex.org/W4386566526",
        "https://openalex.org/W4308244910",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4368755537",
        "https://openalex.org/W4382322340",
        "https://openalex.org/W4376122554",
        "https://openalex.org/W4389523858",
        "https://openalex.org/W4368618872",
        "https://openalex.org/W4387688064",
        "https://openalex.org/W4225603598",
        "https://openalex.org/W4285247752",
        "https://openalex.org/W4287208373",
        "https://openalex.org/W3205270560",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2170189740",
        "https://openalex.org/W2132267839",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4385571451",
        "https://openalex.org/W4393187333"
    ],
    "abstract": "ABSTRACT Objective To investigate the demonstration in Large Language Models (LLMs) for clinical relation extraction. We focus on examining two types of adaptive demonstration: instruction adaptive prompting, and example adaptive prompting to understand their impacts and effectiveness. Materials and Methods The study unfolds in two stages. Initially, we explored a range of demonstration components vital to LLMs‚Äô clinical data extraction, such as task descriptions and examples, and tested their combinations. Subsequently, we introduced the Instruction-Example Adaptive Prompting (LEAP) Framework, a system that integrates two types of adaptive prompts: one preceding instruction and another before examples. This framework is designed to systematically explore both adaptive task description and adaptive examples within the demonstration. We evaluated LEAP framework‚Äôs performance on the DDI and BC5CDR chemical interaction datasets, applying it across LLMs such as Llama2-7b, Llama2-13b, and MedLLaMA_13B. Results The study revealed that Instruction + Options + Examples and its expanded form substantially raised F1-scores over the standard Instruction + Options mode. LEAP framework excelled, especially with example adaptive prompting that outdid traditional instruction tuning across models. Notably, the MedLLAMA-13b model scored an impressive 95.13 F1 on the BC5CDR dataset with this method. Significant improvements were also seen in the DDI 2013 dataset, confirming the method‚Äôs robustness in sophisticated data extraction. Conclusion The LEAP framework presents a promising avenue for refining LLM training strategies, steering away from extensive finetuning towards more contextually rich and dynamic prompting methodologies.",
    "full_text": "LLM Instruction-Example Adaptive Prompting (LEAP) Framework for \nClinical Relation Extraction \nHuixue Zhou, BMed1, Mingchen Li, MS2, Yongkang Xiao MS1, Han Yang, MS1, Rui Zhang, \nPhD2* \n1Institute for Health Informatics, University of Minnesota, Minneapolis, Minnesota, USA, \n2Division of Computational Health Sciences, Department of Surgery, University of Minnesota, \nMinneapolis, Minnesota, USA, \nCorresponding author: \n*Dr. Rui Zhang, PhD \nDivision of Computational Health Sciences, Department of Surgery \nUniversity of Minnesota \n11-132 Phillips-Wangensteen Building, 516 Delaware St SE, Minneapolis, MN 5545 \nEmail: zhan1386@umn.edu \nOffice Phone: 612-626-4209 \nKeywords:  \nClinical Relation Extraction, Large Language Model, Instruction-Example Adaptive Prompting, \nInstruction Tuning \nWord count: 3666 \n \n \n \n \n \n \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nABSTRACT \nObjective: To investigate the demonstration in Large Language Models (LLMs) for clinical \nrelation extraction. We focus on examining two types of adaptive demonstration : instruction \nadaptive prompting , and example adaptive prompting to understand their impacts and \neffectiveness. \nMaterials and Methods : The study unfolds in two stages. Initially, we explored a range of \ndemonstration components vital to LLMs‚Äô clinical data extraction, such as task descriptions and \nexamples, and tested their combinations. Subsequently, we introduced the Instruction -Example \nAdaptive Prompting (LEAP) Framework, a system that integrates two types of adaptive prompts: \none preceding  instruction and another before examples. This framework is designed to \nsystematically explore both adaptive task description and adaptive examples w ithin the \ndemonstration. We evaluated LEAP framework‚Äôs performance on the DDI and BC5CDR chemical \ninteraction datasets , applying it across LLMs such as Llama2 -7b, Llama2 -13b, and \nMedLLaMA_13B. \nResults: The study revealed that Instruction + Options + Examples and its expanded form \nsubstantially raised F1 -scores over the standard Instruction + Options  mode. LEAP framework \nexcelled, especially with example adaptive prompting that outdid traditional instruction tuning \nacross models. Notably, the MedLLAMA -13b model scored an impressive 95.13 F1 on the \nBC5CDR dataset with this method. Significant improvements were also seen in the DDI 2013 \ndataset, confirming the method‚Äôs robustness in sophisticated data extraction. \nConclusion: The LEAP framework presents a promising avenue for refining LLM training \nstrategies, steering away from extensive finetuning towards more contextually rich and dynamic \nprompting methodologies. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nINTRODUCTION \nClinical relation extraction (RE), a natural language processing task, has emerged as a crucial \ntask within healthcare informatics due to its significant role in deciphering drug interactions, side \neffects, and treatment outcomes [1]. The evolution of this field has been largely influenced by \nthe advent of transformer-based models such as BERT, along with its specialized variants like \nBioBERT, BiolinkBERT, SciBERT, and PubmedBERT, which have established new \nperformance benchmarks [2-6]. \nThe rise of Generative Large Language Models (LLMs) has generated considerable interest in \ntheir application within clinical contexts, primarily due to their versatility and adaptability in \nprocessing complex medical data [7-14]. These models distinguish themselves from previous \npre-trained models, such as BERT, by incorporating instructions into their input, providing \nexplicit guidance for task completion, and generating expected responses. Typically, the input \nfor generative LLMs consists of a task description followed by input data, often supplemented \nwith examples to serve as demonstration. \nCurrent research on generative LLMs places a significant focus on manual instruction crafting or \ninstruction tuning, aimed at enhancing the efficiency of LLMs across diverse NLP tasks [15-18]. \nMore sophisticated techniques have emerged, such as the automated search methods developed \nby Prasad et al. [19] and Wei et al. [20], which identify optimal instructions or word choices for \ngenerative LLM instruction. \nTechnological advancements like Chain-of-Thought (CoT) and in-context learning have \nsignificantly propelled the development of generative LLMs. The CoT methodology has \nparticularly been a game-changer, encouraging LLMs to unfold complex reasoning step by step, \nwhich enhances their ability to handle multifaceted tasks. Several Studies [21-14] have \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nhighlighted CoT‚Äôs effectiveness in promoting such detailed reasoning processes within \ngenerative LLMs. In a similar vein, in-context learning has also proven to be instrumental. By \nstrategically providing LLMs with pertinent information, such as related entities [22,25,26] and \nillustrative examples [26,27], this approach improves models‚Äô contextual understanding and their \napplication of knowledge to RE. \nAdditionally, emergence of retrieval based LLMs represents a cutting-edge and promising \ndevelopment in the field of generative LLMs for RE. This strategy gains its strength from \nsynergizing with in-context learning, aiming to access and incorporate highly relevant \nsupplementary resources, thereby enriching the model‚Äôs comprehension and analytical \ncapabilities [27-29]. These studies illustrate the transformative impact of combining retrieval \nmethodologies with generative LLMs, effectively enhancing their performance by providing \nthem with a more nuanced and contextually rich knowledge base.  \nDespite these advancements, challenges persist in crafting manual instruction or retrieval-based \ndemonstrations. Developing high-quality instructions that capture intended behaviors is complex, \nand instruction datasets often lack size, diversity, and creativity [30]. While in previous study, \nwhen the manually crafted prompt meet its limit in transformer-based models, studies introduced \na more dynamic method, soft prompt, which involves learning a prompt embedding matrix to \ndynamically adapt the PLMs [31-33].   \nBuilding upon the concept of the soft prompt, our work introduces an Instruction-Example \nAdaptive Prompting Framework. This framework dynamically learns the embedding of \ninstructions and examples to adapt the generative LLMs for clinical relation extraction. This \nstudy is driven by the goal of optimizing the use and application of demonstrations within LLMs \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nto fully exploit their capabilities in this complex domain. Our study is anchored on two primary \ncontributions: \n1) Demonstration Diversity Operation: We designed various demonstration components and \nundertook a comprehensive analysis to ascertain their impact on enhancing LLM \nperformance in clinical relation extraction tasks. This part of our research involves \ndefining and experimenting with different configurations of demonstration elements to \ndetermine the most effective combinations of demonstration elements for relation \nextraction task. \n2) Development of LLM Instruction-Example Adaptive Prompting (LEAP) Framework: \nThis framework is specifically designed to assess the impact of demonstration clarity on \nLLM performance. By focusing on the nuances of task descriptions and the integration of \nexamples, the LEAP framework aims to provide deeper insights into how instructional \ndesign and in-context examples influences the accuracy and efficiency of LLMs in \nparsing and understanding complex clinical relations. \nTo the best of our knowledge, LEAP framework is the first framework to implement adaptive \nprompt in demonstration of generative LLMs. Through these contributions, our study seeks to \npush the boundaries of current understanding and application of LLMs in clinical relation \nextraction, particularly in deciphering intricate chemical-chemical interactions. \n \nBACKGROUND \nSoft prompt \nIn the realm of pre-trained language models (PLMs), fine-tuning (FT) has historically been the \ngo-to method, which involves the updating of all model parameters for a specific task. However, \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nas the scale of models expands rapidly, alternative training strategies like prompting and prompt-\noriented fine-tuning have gained prominence [34]. Prompting entails freezing all parameters of a \nPLM and utilizing a natural language prompt to query the model [35]. In the concept of soft \nprompt tuning, only continuous prompts are adjusted. Pioneered by Liu et al. [31], Lester et al. \n[32], and by Liu et al. [33], this involves adding trainable continuous embeddings, also known as \ncontinuous prompts, to the input word embeddings sequence.  \nInstruction tuning (InsT) \nA major challenge with generative LLMs is the mismatch between their training goals and user \nrequirements [30].  To address this issue, InsT has become a crucial approach. InsT improves the \ncapabilities and control of LLMs by training them with (DEMONSTRATION+INPUT \nSENTENCE, OUTPUT) pairs. The DEMONSTRATION here includes instruction and \nexamples: Instruction refers to the user‚Äôs directive, these instructions act as a guiding framework, \naligning the model‚Äôs outputs with the desired response criteria or specific domain knowledge, \nthereby giving users the ability to direct the model‚Äôs responses. While OUTPUT is the model‚Äôs \nideal response adhering to this instruction. For instance, in a RE task, an Instruction might state, \n‚ÄúYou are an excellent linguist. The task is to predict relationship between the given head entity \nand tail entity within a given sentence, this relation which must be in (‚Äòmechanism‚Äô, ‚Äòeffect‚Äô, \n‚Äòadvice‚Äô, ‚Äòint‚Äô, ‚ÄòNone‚Äô).‚Äù The examples within a demonstration typically include an input \nsentence paired with its desired output, illustrating the model‚Äôs target response. For example, \n‚ÄúInput: In the sentence: Milk, milk products, and calcium-rich foods or drugs may impair the \nabsorption of EMCYT. The relationship between calcium and EMCYT is?  Response: \nmechanism.‚Äù \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nMETHODS \nOverview of methods. \nOur methodology is structured into two phases. In the initial phase, we concentrated on \ninvestigating various combinations of demonstration components, which is a fundamental step \ntoward the phase II . This exploration aimed to identify the most effective arrangements of these \ndemonstration components in guiding LLMs for clinical relation extraction tasks.  The second \nphase involved the development and implementation of the LEAP framework. This framework \nwas designed to facilitate a more dynamic interaction between the LLM and the  demonstration, \nallowing for an adaptive approach to both the instructions and the examples provided to the LLM \nmodels. Through this method, we aimed to comprehensively assess and enhance the efficiency and \naccuracy of LLMs in clinical relation extraction.  In the following sections, we will introduce the \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \ndataset and task, phase I: Demonstration Diversity Operation, and phase II: Implement of LEAP \nframework, and Experiments and evaluation. \n \nFigure 1. Overview of Study. \nDatasets and task formulation \nDDI Extraction 2013 Corpus (DDI 2013): This dataset, introduced by Herrero-Zazo [36], is used \nfor the drug-drug interaction task. The training dataset of DDI extraction consists of 714 texts (572 \nfrom DrugBank and 142 MedLIne abstracts) and test dataset consists of 158 DrugBank Texts and \nand 33 MedLine abstracts . We implement a train/validation/test split of 543/181/181 files with \nthose texts, respectively. \nBC5CDR: Introduced by Taboureau et al. [37], BC5CDR is a dataset curated for chemical-disease \nrelation extraction tasks. It contains 1,500 documents evenly split into training, validation, and test \nsets, each consisting of 500 documents. \nWe deploy LLMs for the purpose of RE, with the goal of identifying the relationship between two \ndesignated entities within a given text context. The input for the LLM consists of a context labeled \nas ùê∂, incorporating a narrative passage p containing two entities, head entity/subject E1 and tail \nentity/object E2. The LLM ‚Äôs role is to interpret the context and produce a descriptive relation r \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nthat accurately reflects the connection between the entities.  The output is expected to be a string \nor label that categorizes the relationship between E1 and E2, adhering to a pre -defined set of \nrelationship categories. In cases where the entities ‚Äô relationship does not correspond to any \npredefined categories, the LLM should output ‚ÄúNULL‚Äù or an equivalent term. \nDemonstration diversity operation \nIn this section, we delineate the constituent elements of demonstration and the various \ndemonstration modes and examine their impact on the performance of LLMs in clinical RE tasks. \nThe foundational structure of a demonstration here is composed of a task description and options. \nThe task description explicates the nature of the task, setting the stage for the LLM ‚Äôs operation. \nThe options component enumerates all possible relations as defined by the corresponding dataset, \nwhich is represented in Figure 2. Furthermore, we have enriched the demonstration with additional \ncomponents to enhance the RE process. A key addition is the inclusion of option descriptions, \nwhich serve to elucidate clinical entity relationships that may not be immediately apparent to the \nLLM. For instance, the relation type ‚Äúint‚Äù within the DDI dataset could be perplexing for the LLM, \nas it refers a confirmed drug-drug interaction that, without further detail, remains ambiguous. \nAnother crucial component is the inclusion of examples, which supply supplementary in-context \ninformation, thereby enhancing the LLM‚Äôs comprehension of the task at hand. The demonstration \ndesign also accounts for the potential influence of reasoning CoT methodologies, which further \nrequire the LLMs to provide extra explanation for the relation of entities . Subsequently, we  \nconstructed various prompt modes through diverse amalgamations of these instructional \ncomponents, each designed to test and optimize the LLM ‚Äôs performance in accurately extracting \nclinical relations.  This systematic approach allowed us to measure the impact of different \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \ninstructional designs and set the stage for developing the LLM Instruction -Example Adaptive \nPrompting Framework (LEAP) in the subsequent phase of the study. \n \nFigure 2. Example of Demonstration Diversity Operation.  \nImplement of Instruction-Example Adaptive Prompting (LEAP) Framework \nIn the traditional instruction tuning shown in Figure 3a, the Instruction + Example demonstration \nand input sentence are fed into the LLM to generate the relation type between two entities in a \nsentence. The instructions and examples are consistently crafted in natural language. To enable \ndemonstration dynamically to update the LLM and mitigate input limitations arising from \nlengthy examples, we introduce LEAP framework. LEAP is designed to enhance the generative \nLLM‚Äôs output by strategically integrating adaptive prompts into the input context. This approach \nassists the model in generating more precise relational outputs. \nGiven the embedding of Instruction + Example demonstration  ùê∏ = {ùêº!, ‚Ä¶ , ùêº\", ‚Ä¶ , ùëí!, ‚Ä¶ , ùëí#} \nwhere  ùêº\" denotes the token embedding for each token in the instruction, and  ùëí#  denotes the \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \ntoken embedding for each token in the randomly sampled example from a valid dataset.   We \ncrafted two distinct methods for integrating these adaptive prompts:  \n1. Instruction Adaptive Tuning Method (Figure 3b): This method involves the insertion of \nsoft vectors before the task description. The demonstration sequence embedding is \nrepresented ùê∏ = {ùë£!, ‚Ä¶ , ùë£$, ‚Ä¶ , ùêº!, ‚Ä¶ , ùêº\", ‚Ä¶ , ùëí!, ‚Ä¶ , ùëí#} where ùë£$ denotes the soft prompt \nvectors, which is randomly initialized from the embedding layer ùëÄ of LLM and is \nlearnable during the training process. ùêº\" denotes the token embedding for each token in \nthe instruction.  ùëí# denotes the token embedding for each token in the randomly sampled \nexample from valid dataset.  This configuration is designed to prime the LLM with \nadditional task-relevant information before it processes the actual context. \n2. Example Adaptive Tuning Method (Figure 3c): The second method positions the soft \nvectors between the task description and the examples in the context embeddings.   The \ninput sequence embedding is represented as:\tùê∏ = {ùêº!, ‚Ä¶ , ùêº\", ùë£!, ‚Ä¶ , ùë£$, ‚Ä¶ , ùëí!, ‚Ä¶ , ùëí#}   \nwhere ùë£$ denotes the soft prompt vectors, which is randomly initialized from the \nembedding layer ùëÄ of LLM and is learnable during the training process. Here, the soft \nprompt vectors\tùë£$ serve as the traditional cue that aims to add additional example-\nrelevant information, and bridge the task description with the examples, potentially \nenhancing the LLM‚Äôs ability to generalize from the provided examples to new instances. \nThe relation ùë¶ of entities pair in the input sentence is generated by the function  ùëù(ùë¶|ùë•) =\nùëÉ(ùë¶|ùë•, ùê∏) where ùê∏ is the embedding of demonstration, ùë• is the input sentence, and the text y is \nthe desire output (relation types) for the input sentence x. \nThe objective of LEAP framework is  ‚Ñí = ‚àí ‚àë ùë¶%\n$&! log(ùë¶:) where y represents the true label. \nùë¶:\trepresents the generated text of LEAP.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \n \nFigure 3. Instruction tuning (a) and LLM Instruction-Example Adaptive Prompting (b, c).  \n \nExperiments and evaluation \nDuring our research, we engaged with a suite of sophisticated LLMs. Noteworthy among these is \nLlama 2 [38], an innovative open-source language model from Meta AI acclaimed for its \ndistinguished performance across numerous benchmarks, encompassing reasoning, coding, \nlanguage mastery, and information retrieval tasks. We incorporated two models from the Llama \n2 in our study: the Llama2-7b with 7 billion parameters, and the more expansive Llama2-13b \nwith 13 billion parameters. We also employed MedLLaMA_13B [39], a specialized version of \nthe Llama2-13b tailored with a Medical Corpus to enhance its medical query response \ncapabilities. \nMoreover, we included OpenAI‚Äôs GPT3.5 Turbo (hereafter referred to as GPT3.5) and GPT4 in \nour arsenal, both being LLMs celebrated for their remarkable language understanding and \ngeneration abilities.  \nIn particular, we leveraged Llama2-7b [38], MedLLaMA_13B [39], GPT3.5, and GPT4 for the \nDemonstration Diversity Operation and selected Llama2-7b, Llama2-13b, and MedLLaMA_13B \nfor Instruction-Example Adaptive Prompting. These models are at the forefront of language \nmodel architecture and are especially appropriate for the intricate task of relation extraction. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nIn evaluating the effectiveness of our LEAP framework, we benchmarked against a number of \nformidable baselines, including: \n1) The BERT Family: A series of BERT variants, namely PubmedBERT [2], BioBERT [3], \nSciBERT [5], and BiolinkBert [6], were assessed for their performance on the datasets \nused in our research.  \n2) The Llama 2 Family: This includes the aforementioned Llama 2 models and the \nMedLLaMA_13B [39], which have demonstrated superior performance across various \nbenchmarks. We applied instruction tuning to the Instruction + Example demonstration \nmode, without incorporating additional soft vectors. The approach is depicted in Figure \n3a. \n3) OpenAI's GPT Models: The GPT3.5 and GPT4 served as baselines, noted for their \ncomprehensive language processing capabilities. \nFor the tuning of LEAP, we experimented with the insertion of 5 to 20 soft tokens and tested \nlearning rates of 1e-4 and 1e-5. Training was conducted with batch sizes ranging from 1 to 4. \nOur chosen evaluation metric was the micro-averaged F1 score, which provides a harmonic \nmean of precision and recall. \n \nRESULTS  \nThe comparative analysis presented in Figure 4 delineates the performance differentials across \nvarious demonstration modes. The experiment‚Äôs performance metrics highlighted the superior \nefficacy of certain demonstration modes. Specifically, the Instruction + Options + examples and \nInstruction + Options + options description+ examples mode led to a notable increase in F1-\nscores across most models when compared to the baseline Instruction + Options mode. In the \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nBC5CDR dataset, incorporating examples or options descriptions proved beneficial, with the \nmode Instruction + options + options description + example particularly standing out in the \nLlama2-13b model, achieving an impressive F1 score of 0.5070. Adding only examples was \ngenerally more advantageous than just adding the options description across all the models. For \ninstance, the Llama2-7b model saw an F1 score increase to 0.353 from 0.073 with the addition of \nexamples, while adding the options description increase to 0.215. Similarly, In the DDI 2013 \ndataset, the inclusion of examples consistently improved performance across the models, while \nthe addition of options description had a more variable impact. Specifically, the instruction + \noptions + example mode excelled in the Medllama_13B model, boosting the F1 score to an \nimpressive 0.359. For larger models such as GPT3.5 and GPT4 in BC5CDR and DDI 2013, there \nwas a noticeable trend where the more information provided, the better the models performed. \nThe instruction + options + example + options explanation mode was best for GPT4, which \nachieved a peak F1 score of 0.770 in BC5CDR and 0.760 in DDI 2013, underscoring its ability \nto utilize detailed prompts effectively.  \nThe Chain-of-Thought prompt mode exhibits varied effectiveness, with its impact differing \nsignificantly across models and datasets. For example, in the BC5CDR dataset, this \ndemonstration mode did not lead to performance improvements for Llama2-7b and GPT3.5, \nwhere the F1 scores remained relatively low at 0.0020 and 0.3900, respectively. However, \nMedLLaMA_13B showed a substantial boost with a 50% increase in the F1 score when using \nthe Chain-of-Thought mode, suggesting that this model may be better at processing and \nbenefiting from the step-by-step reasoning this prompt structure provides. In the DDI 2013 \ndataset, the results were similarly mixed. The Chain-of-Thought prompts led to minimal changes \nfor Llama2-7b and Llama2-13b, with F1 scores of 0.014 and 0.006, respectively, which does not \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nrepresent a clear advantage over other prompt types. Yet, for MedLLaMA_13B, the Chain-of-\nThought prompt resulted in an improvement, with an F1 score of 0.065, although this was not as \nhigh as the increase seen with the examples-only prompts. \n \nFigure 4. performance of demonstration diversity operation. \nTable 1. Performance of RE in BC5CDR and DDI 2013. \nMethod Model BC5CDR DDI 2013 \n   Micro \nP \nMicro \nR \nMicro \nF1 \nMicro \nP \nMicro \nR \nMicro \nF1 \nFine-tuning \nBioBERT 79.42 79.42 79.42 85.68 85.68 85.68 \nSciBERT  82.82 82.82 82.82 85.98 85.98 85.98 \nBioLinkBERT  83.54 83.54 83.54 85.88 85.88 85.88 \nPubmedBERT 82.36 82.36 82.36 85.31 85.31 85.31 \nZero-shot GPT4 76.00 76.00 76.00 77.00 77.00 77.00 \nInstruction \ntuning \nLlama2-7b 89.93 89.93 89.93 89.44 89.44 89.44 \nLlama2-13b 91.35 91.35 91.35 91.73 91.73 91.73 \nMedLLaMA_13B 92.70 92.70 92.70 91.33 91.33 91.33 \nLEAP \nmethod 1 \nLlama2-7b IAT* 90.11 90.11 90.11 89.40 89.40 89.40 \nLlama2-13b IAT 85.53 85.53 85.53 92.24 92.24 92.24 \nMedLLaMA_13B IAT 91.44 91.44 91.44 91.91 91.91 91.91 \nLEAP \nmethod 2 \nLlama2-7b EAT* 92.04 92.04 92.04 90.80 91.12 91.12 \nLlama2-13b EAT 94.70 94.70 94.70 91.76 91.76 91.76 \nMedLLaMA_13B EAT 95.13 95.13 95.13 92.20 92.20 92.20 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \n* IAT: instruction adaptive tuning, EAT: example adaptive tuning \nTable 1 offers a detailed performance for two distinct tuning approaches: the instruction adaptive \ntuning and the example adaptive tuning. These strategies demonstrate that fine-tuning LLMs \n(Llama2-7b, Llama2-13b, MedLLaMA_13B) and employing adaptive tuning can significantly \nsurpass the performance of traditionally fine-tuned LMs (BioBERT, SciBERT, BioLinkBERT, \nPubmedBERT) and even zero-shot GPT4, as evidenced by the metrics provided. Specifically, in \nthe BC5CDR dataset, MedLLaMA_13B using example adaptive tuning reached the pinnacle of \nperformance, boasting an F1 score of 95.13, while in the DDI 2013 dataset, Llama2-13b \nemploying instruction adaptive tuning achieved the top F1 score of 92.24. \nIn the BC5CDR dataset, the example adaptive tuning strategy, especially when applied to \nMedLLaMA_13B, significantly outperformed the fine-tuned models, achieving a superior F1 \nscore of 95.13 compared to 92.70 for the fine-tuned version. Llama2-7b with its example \nadaptive tuning and Llama2-13b with its example adaptive tuning also reached notable high \nperformance, with F1 score of 92.04 and 94.70, respectively, surpassing the results of instruction \ntuning, where Llama2-7b and Llama2-13b scored 89.93 and 91.35. Similarly, in the DDI 2013 \ndataset, the example adaptive tuning maintained superiority over the fine-tuning approach for all \nmodels tested.  The example adaptive tuning also demonstrated a slight edge over the fine-tuning \napproach in models MedLLaMA_13B and Llama2-13b, affirming the potential of adaptive \ntuning in enhancing LLM performance.  \nWhile Instruction adaptive tuning exhibited fluctuating outcomes across both datasets. In the \nBC5CDR dataset, its advantage was observed solely in the Llama2-7b model, where it \noutperformed the same model utilizing standard instruction tuning. In the case of the DDI 2013 \ndataset, the deployment of instruction adaptive tuning on both Llama2-13b and \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nMedLLaMA_13B models yielded better results than when these models were subjected to \nconventional instruction tuning. \n \nDISCUSSION \nThe discussion of the potential applications of generative LLMs in RE has gained momentum, as \ncurrent research often gravitates towards deploying state-of-the-art methodologies CoT reasoning \nor retrieval-based techniques to construct hard instructions [21-29]. These methods aim to enhance \ngenerative LLMs‚Äô ability to comprehend and process complex data. Not ably, there is emerging \nevidence suggesting that prefix prompts may surpass the performance of fine -tuned models in \nclinical RE [40]. However, the literature still lacks a comprehensive exploration of adaptive tuning \nin demonstration and, more specifically, example adaptive tuning with the latest and most \nsophisticated generative LLM frameworks, such as Llama. \nIn the initial phase of our study, we have delved into various demonstration paradigms, seeking to \nunderstand their influence on RE tasks. With the Instruction + Options + Examples and Instruction \n+ Options + Examples +Options Description modes leading to a notable increased performance, \nour findings resonate with the insights garnered from our prior research  [27], indicating that the \nintegration of examples within demonstration is not merely beneficial but critical for the success \nof RE tasks. This alignment underscores the significance of tailored examples in enhancing model \ncomprehension and suggests a pivotal role for example -based learning in LLMs‚Äô operational \nframeworks. \nMoving into the second stage, our results show that the example adaptive tuning configuration \nyields superior performance for MedLLaMA_13B, Llama2 -7b, and Llama2 -13b models when \njuxtaposed with the instruction finetuning versions of the same models. This enhancement in \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nperformance indicates that example -based learning may provide a more robust framework for \nthese models to understand and process complex tasks. \nIn the BC5CDR and DDI 2013  dataset, the instruction adaptive tuning‚Äôs performance varied \namong the models and did not yield the same level of enhancement as the example adaptive tuning. \nThis variation suggests that within the domain of biomedical RE, prompts augmented with well -\ncrafted examples are more critical than mere task explanations. This aligns with our initial phase \nfindings, where the performance did not significantly improve with the addition of explanations \nfor various relation labels compared to the mode adding the examples, reaffirming the importance \nof examples in RE task. \nThe implications of these findings are far -reaching. They invite us to reconsider the traditional \nreliance on extensive finetuning in favor of more dynamic, example -rich, and context -aware \nprompting strategies. By embracing this shift, we could potentially streamline the training process, \nreduce computational overheads, and accelerate the deployment of generative LLMs in real-world \nmedical applications. Further research is warranted to validate these preliminary results and to \nexplore the scalability of the se approaches, potentially leading to a new paradigm in the training \nand application of generative LLMs for specialized tasks such as medical relation extraction. \nThis study, while offering valuable insights, does acknowledge certain constraints: Firstly, the \nscope did not extend to assessing the impact that varying lengths of soft prompts may have. Second, \na comprehensive examination of how model size influences outcomes was not conducte d. For \ninstance, we did not implement the Llama 70b model in our experiments. \nCONCLUSION \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nOur investigation into the application of LLMs for RE tasks has unveiled promising avenues for \nenhancing model performance through LEAP adaptive tuning strategies. The study‚Äôs comparative \nanalysis between instruction adaptive and example adaptive tuning on state-of-the-art models like \nLlama highlights the substantial benefits of incorporating contextually rich examples into \ndemonstration. Our findings not only corroborate previous research advocating for example -\ncentric approaches but also showcase the pote ntial of these strategies to outperform traditional \nfinetuning methods. \n \nDATA AVAILABILITY \nNot applicable. \nFUNDING STATEMENT \nThis work was supported by the National Institutes of Health‚Äôs National Center for \nComplementary and Integrative Health grant number R01AT009457, National Institute on Aging \ngrant number R01AG078154 and National Cancer Institute grant number R01CA287413. T he \ncontent is solely the responsibility of the authors and does not represent the official views of the \nNational Institutes of Health. \nCONTRIBUTORSHIP STATEMENT \nHZ designed the study and drafted the manuscript. HZ, ML, and YX executed the experiments. \nAll authors reviewed and finalized the manuscript. \nACKNOWLEDGEMENTS \nWe would also like to acknowledge to the staff at BPIC of University of Minnesota and the \nprogrammers from Rui Zhang‚Äôs group for their technical support. \nCOMPETING INTERESTS STATEMENT \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nThe authors state that they have no competing interests to declare. \nREFERENCES \n1. Wang, Y., Wang, L., Rastegar-Mojarad, M., Moon, S., Shen, F., Afzal, N., Liu, S., Zeng, \nY., Mehrabi, S., Sohn, S., & Liu, H. (2018). Clinical information extraction applications: \nA literature review. Journal of Biomedical Informatics, 77, 34‚Äì49. \nhttps://doi.org/10.1016/j.jbi.2017.11.011 \n2. Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J., & \nPoon, H. (2022). Domain-Specific Language Model Pretraining for Biomedical Natural \nLanguage Processing. ACM Transactions on Computing for Healthcare, 3(1), 1‚Äì23. \nhttps://doi.org/10.1145/3458754 \n3. Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., & Kang, J. (2020). BioBERT: A \npre-trained biomedical language representation model for biomedical text mining. \nBioinformatics, 36(4), 1234‚Äì1240. https://doi.org/10.1093/bioinformatics/btz682 \n4. Roy, A., & Pan, S. (2021). Incorporating medical knowledge in BERT for clinical \nrelation extraction. Proceedings of the 2021 Conference on Empirical Methods in Natural \nLanguage Processing, 5357‚Äì5366. https://doi.org/10.18653/v1/2021.emnlp-main.435 \n5. Beltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A Pretrained Language Model for \nScientific Text. Proceedings of the 2019 Conference on Empirical Methods in Natural \nLanguage Processing and the 9th International Joint Conference on Natural Language \nProcessing (EMNLP-IJCNLP), 3613‚Äì3618. https://doi.org/10.18653/v1/D19-1371 \n6. Yasunaga, M., Leskovec, J., & Liang, P. (2022). LinkBERT: Pretraining Language \nModels with Document Links. Proceedings of the 60th Annual Meeting of the \nAssociation for Computational Linguistics (Volume 1: Long Papers), 8003‚Äì8016. \nhttps://doi.org/10.18653/v1/2022.acl-long.551 \n7. Zhou, H., Austin, R., Lu, S.-C., Silverman, G. M., Zhou, Y., Kilicoglu, H., Xu, H., & \nZhang, R. (2023). Complementary and Integrative Health Information in the literature: Its \nlexicon and named entity recognition. Journal of the American Medical Informatics \nAssociation, ocad216. https://doi.org/10.1093/jamia/ocad216 \n8. Clusmann, J., Kolbinger, F. R., Muti, H. S., Carrero, Z. I., Eckardt, J.-N., Laleh, N. G., \nL√∂ffler, C. M. L., Schwarzkopf, S.-C., Unger, M., Veldhuizen, G. P., Wagner, S. J., & \nKather, J. N. (2023). The future landscape of large language models in medicine. \nCommunications Medicine, 3(1), 141. https://doi.org/10.1038/s43856-023-00370-1 \n9. Demszky, D., Yang, D., Yeager, D. S., Bryan, C. J., Clapper, M., Chandhok, S., \nEichstaedt, J. C., Hecht, C., Jamieson, J., Johnson, M., Jones, M., Krettek-Cobb, D., Lai, \nL., JonesMitchell, N., Ong, D. C., Dweck, C. S., Gross, J. J., & Pennebaker, J. W. (2023). \nUsing large language models in psychology. Nature Reviews Psychology, 2(11), 688‚Äì\n701. https://doi.org/10.1038/s44159-023-00241-5 \n10. Li, M., Chen, M., Zhou, H., & Zhang, R. (2023). PeTailor: Improving Large Language \nModel by Tailored Chunk Scorer in Biomedical Triple Extraction. \nhttps://doi.org/10.48550/ARXIV.2310.18463 \n11. Mbakwe, A. B., Lourentzou, I., Celi, L. A., Mechanic, O. J., & Dagan, A. (2023). \nChatGPT passing USMLE shines a spotlight on the flaws of medical education. PLOS \nDigital Health, 2(2), e0000205. https://doi.org/10.1371/journal.pdig.0000205 \n12. Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N., \nTanwani, A., Cole-Lewis, H., Pfohl, S., Payne, P., Seneviratne, M., Gamble, P., Kelly, \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \nC., Babiker, A., Sch√§rli, N., Chowdhery, A., Mansfield, P., Demner-Fushman, D., ‚Ä¶ \nNatarajan, V. (2023). Large language models encode clinical knowledge. Nature, \n620(7972), 172‚Äì180. https://doi.org/10.1038/s41586-023-06291-2 \n13. Tang, L., Sun, Z., Idnay, B., Nestor, J. G., Soroush, A., Elias, P. A., Xu, Z., Ding, Y., \nDurrett, G., Rousseau, J. F., Weng, C., & Peng, Y. (2023). Evaluating large language \nmodels on medical evidence summarization. Npj Digital Medicine, 6(1), 158. \nhttps://doi.org/10.1038/s41746-023-00896-7 \n14. Thirunavukarasu, A. J., Ting, D. S. J., Elangovan, K., Gutierrez, L., Tan, T. F., & Ting, \nD. S. W. (2023). Large language models in medicine. Nature Medicine, 29(8), 1930‚Äì\n1940. https://doi.org/10.1038/s41591-023-02448-8 \n15. Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, \nF., & Wang, G. (2023). Instruction Tuning for Large Language Models: A Survey. \nhttps://doi.org/10.48550/ARXIV.2308.10792 \n16. Lou, R., Zhang, K., & Yin, W. (2023). Is Prompt All You Need? No. A Comprehensive \nand Broader View of Instruction Learning. https://doi.org/10.48550/ARXIV.2303.10475 \n17. Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., & \nLe, Q. V. (2021). Finetuned Language Models Are Zero-Shot Learners. \nhttps://doi.org/10.48550/ARXIV.2109.01652 \n18. Li, M., & Huang, L. (2023). Understand the Dynamic World: An End-to-End Knowledge \nInformed Framework for Open Domain Entity State Tracking. \nhttps://doi.org/10.48550/ARXIV.2304.13854 \n19. Prasad, A., Hase, P., Zhou, X., & Bansal, M. (2023). GrIPS: Gradient-free, Edit-based \nInstruction Search for Prompting Large Language Models. Proceedings of the 17th \nConference of the European Chapter of the Association for Computational Linguistics, \n3845‚Äì3864. https://doi.org/10.18653/v1/2023.eacl-main.277 \n20. Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., & Ba, J. (2022). Large \nLanguage Models Are Human-Level Prompt Engineers. \nhttps://doi.org/10.48550/ARXIV.2211.01910 \n21. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & \nZhou, D. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language \nModels. https://doi.org/10.48550/ARXIV.2201.11903 \n22. Wan, Z., Cheng, F., Mao, Z., Liu, Q., Song, H., Li, J., & Kurohashi, S. (2023). GPT-RE: \nIn-context Learning for Relation Extraction using Large Language Models. \nhttps://doi.org/10.48550/ARXIV.2305.02105 \n23. Chen, F., & Feng, Y. (2023). Chain-of-Thought Prompt Distillation for Multimodal \nNamed Entity Recognition and Multimodal Relation Extraction. \nhttps://doi.org/10.48550/ARXIV.2306.14122 \n24. Wadhwa, S., Amir, S., & Wallace, B. C. (2023). Revisiting Relation Extraction in the era \nof Large Language Models. https://doi.org/10.48550/ARXIV.2305.05003 \n25. Meng, S., Hu, X., Liu, A., Li, S., Ma, F., Yang, Y., & Wen, L. (2023). RAPL: A \nRelation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation \nExtraction (arXiv:2310.15743). arXiv. http://arxiv.org/abs/2310.15743 \n26. Xu, X., Zhu, Y., Wang, X., & Zhang, N. (2023). How to Unleash the Power of Large \nLanguage Models for Few-shot Relation Extraction? \nhttps://doi.org/10.48550/ARXIV.2305.01555 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \n27. Li, M., Chen, M., Zhou, H., & Zhang, R. (2023). PeTailor: Improving Large Language \nModel by Tailored Chunk Scorer in Biomedical Triple Extraction. \nhttps://doi.org/10.48550/ARXIV.2310.18463 \n28. Gao, C., Fan, X., Sun, J., & Wang, X. (2023). PromptRE: Weakly-Supervised Document-\nLevel Relation Extraction via Prompting-Based Data Programming. \nhttps://doi.org/10.48550/ARXIV.2310.09265 \n29. Rubin, O., Herzig, J., & Berant, J. (2021). Learning To Retrieve Prompts for In-Context \nLearning. https://doi.org/10.48550/ARXIV.2112.08633 \n30. Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, \nF., & Wang, G. (2023). Instruction Tuning for Large Language Models: A Survey. \nhttps://doi.org/10.48550/ARXIV.2308.10792 \n31. Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., & Tang, J. (2022). P-Tuning: Prompt \nTuning Can Be Comparable to Fine-tuning Across Scales and Tasks. Proceedings of the \n60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short \nPapers), 61‚Äì68. https://doi.org/10.18653/v1/2022.acl-short.8 \n32. Lester, B., Al-Rfou, R., & Constant, N. (2021). The Power of Scale for Parameter-\nEfficient Prompt Tuning. https://doi.org/10.48550/ARXIV.2104.08691 \n33. Liu, X., Ji, K., Fu, Y., Tam, W. L., Du, Z., Yang, Z., & Tang, J. (2021). P-Tuning v2: \nPrompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks. \nhttps://doi.org/10.48550/ARXIV.2110.07602 \n34. Schick, T., & Sch√ºtze, H. (2021). Exploiting Cloze-Questions for Few-Shot Text \nClassification and Natural Language Inference. Proceedings of the 16th Conference of \nthe European Chapter of the Association for Computational Linguistics: Main Volume, \n255‚Äì269. https://doi.org/10.18653/v1/2021.eacl-main.20 \n35. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, \nA., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., \nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., ‚Ä¶ Amodei, D. \n(2020). Language Models are Few-Shot Learners. \nhttps://doi.org/10.48550/ARXIV.2005.14165 \n36. Herrero-Zazo, M., Segura-Bedmar, I., Mart√≠nez, P., & Declerck, T. (2013). The DDI \ncorpus: An annotated corpus with pharmacological substances and drug‚Äìdrug \ninteractions. Journal of Biomedical Informatics, 46(5), 914‚Äì920. \nhttps://doi.org/10.1016/j.jbi.2013.07.011 \n37. Taboureau, O., Nielsen, S. K., Audouze, K., Weinhold, N., Edsgard, D., Roque, F. S., \nKouskoumvekaki, I., Bora, A., Curpan, R., Jensen, T. S., Brunak, S., & Oprea, T. I. \n(2011). ChemProt: A disease chemical biology database. Nucleic Acids Research, \n39(Database), D367‚ÄìD372. https://doi.org/10.1093/nar/gkq906 \n38. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., \nBatra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., \nCucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., ‚Ä¶ Scialom, T. (2023). Llama 2: \nOpen Foundation and Fine-Tuned Chat Models. \nhttps://doi.org/10.48550/ARXIV.2307.09288 \n39. Chaoyi-wu/MedLLaMA_13B ¬∑ Hugging Face. (n.d.). Retrieved December 13, 2023, \nfrom https://huggingface.co/chaoyi-wu/MedLLaMA_13B \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint \n40. Peng, C., Yang, X., Smith, K. E., Yu, Z., Chen, A., Bian, J., & Wu, Y. (2023). Model \nTuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and \nRelation Extraction. https://doi.org/10.48550/ARXIV.2310.0623 \n \nFIGURE LEGENDS \nFigure 1. Overview of Study. \nFigure 2. Example of Instruction Searching.  \nFigure 3. Instruction tuning (a) and LLM Instruction-Example Adaptive Prompting (b, c).  \nFigure 4. performance of demonstration diversity operation. \n \n \n \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300059doi: medRxiv preprint "
}