{
  "title": "How Relevant Are Selectional Preferences for Transformer-based Language Models?",
  "url": "https://openalex.org/W3116605602",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3189384047",
      "name": "Eleni Metheniti",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2631743907",
      "name": "Tim Van de Cruys",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2260675962",
      "name": "Nabil Hathout",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964275548",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2113522550",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W2170471032",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2985703188",
    "https://openalex.org/W2251735937",
    "https://openalex.org/W2095657353",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2051534348",
    "https://openalex.org/W1722459797",
    "https://openalex.org/W2096423148",
    "https://openalex.org/W2177614019",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2003698958",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2147298557",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W1754931551",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2151170651",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W1564174355",
    "https://openalex.org/W3154772965",
    "https://openalex.org/W2998230451",
    "https://openalex.org/W2119610041",
    "https://openalex.org/W2128050560",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2983004086",
    "https://openalex.org/W1591659308",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1992871189",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W2054690340",
    "https://openalex.org/W2168185617",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2073321553"
  ],
  "abstract": "Selectional preference is defined as the tendency of a predicate to favor particular arguments within a certain linguistic context, and likewise, reject others that result in conflicting or implausible meanings. The stellar success of contextual word embedding models such as BERT in NLP tasks has led many to question whether these models have learned linguistic information, but up till now, most research has focused on syntactic information. We investigate whether Bert contains information on the selectional preferences of words, by examining the probability it assigns to the dependent word given the presence of a head word in a sentence. We are using word pairs of head-dependent words in five different syntactic relations from the SP-10K corpus of selectional preference (Zhang et al., 2019b), in sentences from the ukWaC corpus, and we are calculating the correlation of the plausibility score (from SP-10K) and the model probabilities. Our results show that overall, there is no strong positive or negative correlation in any syntactic relation, but we do find that certain head words have a strong correlation and that masking all words but the head word yields the most positive correlations in most scenarios –which indicates that the semantics of the predicate is indeed an integral and influential factor for the selection of the argument.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 1266–1278\nBarcelona, Spain (Online), December 8-13, 2020\n1266\nHow Relevant Are Selectional Preferences\nfor Transformer-based Language Models?\nEleni Metheniti♣♦\nIRIT (CNRS)♦\nUniversit´e Toulouse -\nPaul Sabatier (UT3)\n31400 Toulouse, France\nTim Van de Cruys♠\nKU Leuven ♠\nFaculty of Arts\nDepartment of Linguistics\nLeuven.AI institute\nB-3000 Leuven, Belgium\nfirstname.lastname@{univ-tlse2.fr♣,irit.fr♦,kuleuven.be♠}\nNabil Hathout♣\nCLLE-CNRS ♣\nUniversit´e Toulouse -\nJean Jaur`es (UT2J)\nMaison de la Recherche\n31058 Toulouse, France\nAbstract\nSelectional preference is deﬁned as the tendency of a predicate to favour particular arguments\nwithin a certain linguistic context, and likewise, reject others that result in conﬂicting or implau-\nsible meanings. The stellar success of contextual word embedding models such as B ERT in NLP\ntasks has led many to question whether these models have learned linguistic information, but\nup till now, most research has focused on syntactic information. We investigate whether B ERT\ncontains information on the selectional preferences of words, by examining the probability it\nassigns to the dependent word given the presence of a head word in a sentence. We are using\nword pairs of head-dependent words in ﬁve different syntactic relations from the SP-10K corpus\nof selectional preference (Zhang et al., 2019b), in sentences from the ukWaC corpus, and we are\ncalculating the correlation of the plausibility score (from SP-10K) and the model probabilities.\nOur results show that overall, there is no strong positive or negative correlation in any syntactic\nrelation, but we do ﬁnd that certain head words have a strong correlation, and that masking all\nwords but the head word yields the most positive correlations in most scenarios—which indicates\nthat the semantics of the predicate is indeed an integral and inﬂuential factor for the selection of\nthe argument.\n1 Introduction\nMotivated by their semantics, the vast majority of predicates (such as verbs) have a tendency to favour\ncertain arguments to others. Consider the following examples:\n(1) The athlete runs a marathon.\n(2) The bassoon runs a banana.\nMost native speakers would readily accept example (1) as a well-formed English sentence, while exam-\nple (2), even though it is syntactically correct, is more likely to be judged as awkward and ill-formed. The\nﬁrst example is semantically felicitous (since it is perfectly normal for athletes to run, and a marathon is\nsomething that can be run), while the second example is semantically infelicitous (both a bassoon and\na banana are inanimate entities without a literal capability of motion). This preference of predicates for\nparticular arguments is known as selectional preference(Katz and Fodor, 1963). A proper understand-\ning of this phenomenon is important within various natural language processing (NLP ) applications, and\nselectional preferences have indeed been used as an additional knowledge source for various NLP tasks,\nsuch as word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and\nJurafsky, 2002).\nWhile language processing architectures prior to the neural network paradigm primarily made use of a\nsequential NLP pipeline, where designated modules sequentially provide increasingly complex linguistic\nannotations (such as part of speech tagging and syntactic parsing), more recent approaches tend to tackle\nNLP problems with a single, overarching neural network architecture: words are modeled as multi-\ndimensional embeddings that are fed to the neural network architecture, without any additional linguistic\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creative\ncommons.org/licenses/by/4.0/.\n1267\nannotations; all the linguistic knowledge—that the neural network might exploit for its ﬁnal decisions—is\nmodeled implicitly throughout the neural network’s hidden layers. This line of research has culminated\nin the recent contextual embedding language architectures, such as ELMo (Peters et al., 2018), B ERT\n(Devlin et al., 2019) and RoBERTa (Liu et al., 2019), which adapt the individual word embeddings in\norder to yield a ﬁne-grained representation of the meaning within the context of a speciﬁc text fragment,\nand these embeddings can subsequently be exploited for language prediction tasks.\nThe implicit nature of the linguistic knowledge contained within these recent approaches opens up\na number of questions with regard to the relevance of selectional preferences in the neural network\nparadigm. Whereas previous NLP approaches generally considered selectional preferences as a stand-\nalone module that may be easily evaluated in vitro, this is not the case for current neural approaches: the\nneural network functions as a black box, and as such it is impossible to ﬁnd out exactly what kind of\ninformation has led the model to make a ﬁnal decision, and whether selectional preference information\nplayed a signiﬁcant role in it. It is therefore necessary to design speciﬁc experimental settings in order to\ninspect what information the network actually made use of. Lately, there has been a great deal of interest\nin comprehending the way contextual language models encode linguistic knowledge. However, most\nwork has focused on syntactic phenomena, such as the identiﬁcation of constituents and dependencies,\nand syntactic agreement; investigating how these models encode semantics proves to be a more difﬁcult\nissue, and research into the question is much less common (Mickus et al., 2020).\nIn this paper, our goal is to investigate whether Transformer models include selectional preference\ninformation in their embeddings and whether they rely on it in order to make their predictions—and if\nso, to what extent that information is relevant or vital. We focus on the B ERT architecture, and consider\nthe inﬂuence of selectional preferences for the model’s standardmasked language modelpretraining ob-\njective. Our sentences consist of pairs of head and dependent words from the SP-10K dataset, annotated\nwith a plausibility score by human judges, and spanning ﬁve different types of syntactic relations (Zhang\net al., 2019b). We create prompt sentences based on these word pairs and a syntactically annotated ver-\nsion of the ukWaC corpus (Ferraresi et al., 2008) in order to extract full sentences with said pairs. We\nthen feed the prompt sentences masking the dependent word of the word pair to BERT. Then, we are able\nto observe whether there is a strong correlation between the plausibility score of a word pair (how likely\nit is for the head and the dependent word to exist in an utterance) and the probability of the dependent\nword in the context of a sentence, as assigned by BERT. Subsequently, we can make assumptions on how\nmuch the head word contributed to the prediction of the dependent word, by applying attention masks\nto the input sequences of the model. Our ﬁndings show that, over all sentences and word pairs, there\nis no strong positive or negative correlation between the plausibility score (as retrieved from SP-10K)\nand the probability of the dependent word; however, we make note of some head words and phenomena\nthat showed signiﬁcant positive or negative correlation ( >0.4) in some syntactic relations. Moreover,\nour results show that masking all words but the head word yields the most positive correlations in most\nscenarios, which indicates that the semantics of the predicate is indeed an integral and inﬂuential factor\nfor the selection of the argument.\nThis paper is structured as follows. In Section 2, we provide an overview of related work, both with\nregard to selectional preference modeling and with regard to interpretability of neural architectures. In\nSection 3, we outline the methodology of our research. Section 4 provides an overview of the results, in-\ncluding a quantitative comparison of the various experimental settings as well as a qualitative discussion.\nSection 5 describes some of the challenges that we faced in our research, as well as a general discussion\nof our results. Finally, Section 6 summarizes our ﬁndings and our key points, and provides a number of\navenues for future work.\n2 Previous Work\n2.1 Selectional Preference Induction\nA seminal approach to selectional preference induction makes use of posterior probabilities: Resnik\n(1996) relies on WordNet synsets in order to generate generalized noun clusters. The selectional prefer-\nence strengthof a speciﬁc verb vin a particular relation is calculated by computing the Kullback–Leibler\n1268\ndivergence between the posterior cluster distribution of the verb and the prior cluster distribution:\nSR(v) ==\nc\np(c∣v)log p(c∣v)\np(c) (1)\nwhere cstands for a noun cluster, and Rstands for a given predicate-argument relation. The selectional\nassociation of a particular noun cluster is the contribution of that cluster to the verb’s preference strength.\nAR(v,c) =\np(c∣v)log p(c∣v)\np(c)\nSR(v)\n(2)\nThe model’s generalization relies entirely on WordNet, and there is no generalization among the verbs.\nA number of other researchers have equally exploited WordNet for generalization (Li and Abe, 1998;\nClark and Weir, 2001; ´O S´eaghdha and Korhonen, 2012). Most researchers, however, acknowledge the\nshortcomings of hand-crafted resources, and focus on the acquisition of selectional preferences from\ncorpus data. Rooth et al. (1999) propose an Expectation–Maximization (EM) clustering algorithm for\nselectional preference acquisition based on a probabilistic latent variable model. In their model, both\npredicate and argument are generated from a latent variable, where the latent variables represent clusters\nof tight verb–argument interactions.\np(v,o) = =\nc∈C\np(c,v,o ) = =\nc∈C\np(c)p(v∣c)p(o∣c) (3)\nThe use of latent variables allows the model to generalize to predicate–argument tuples that have not been\nseen during training. The latent variable distribution—and the probabilities of predicates and argument\ngiven the latent variables—are automatically induced from data using EM.\nErk (2007) and Erk et al. (2010) describe a method that uses corpus-driven distributional similarity\nmetrics for the induction of selectional preferences. The key idea is that a predicate-argument tuple\n(v,o) is felicitous if the predicate vappears in the training corpus with arguments o′ similar to o, i.e.\nS(v,o) = =\no′∈Ov\nwt(v,o′)\nZ(v) ⋅sim(o,o′) (4)\nwhere Ov represents the set of arguments that have been attested with predicate v, wt(⋅) represents\nan appropriate weighting function (in its simplest form the frequency of the (v,o′) tuple), and Z is a\nnormalization factor.\nVan de Cruys (2009) presents a model based on tensor factorization, which is able to model multi-way\nselectional preferences. Three-way co-occurrences of subjects, verbs, and objects are represented as a\nthree-way tensor (the generalization of a matrix), and a latent tensor factorization model is applied in\norder to generalize to unseen instances.\nA number of researchers presented models that are based on the framework of topic modeling. ´O\nS´eaghdha (2010) describes three models for selectional preference induction based on Latent Dirichlet\nAllocation, which model the selectional preference of a predicate and a single argument. Ritter et al.\n(2010) equally present a selectional preference model based on topic modeling, but they tackle multi-\nway selectional preferences (of transitive predicates, which take two arguments) instead.\nMore recently, neural network based approaches have equally been used. Van de Cruys (2014) presents\nan approach based on feed-forward neural networks; predicates and arguments are represented as em-\nbeddings, and serve as input to a simple feed-forward neural network architecture, which yields a single\nselectional preference value. And Zhang et al. (2019a) present multiplex word embeddings for selectional\npreference modeling. The key idea is to create, for each word, a ‘central embedding’ that represents the\nword’s global semantics, and several ‘relational’ embeddings that represent how the word relates to other\nwords within a speciﬁc syntactic relation.\nFor the evaluation of selectional preference models, researchers have exploited two different kinds of\nevaluation tasks: pseudo-disambiguation (Rooth et al., 1999; Ritter et al., 2010; Van de Cruys, 2014),\n1269\nand comparison to human judgements (McRae et al., 1998; Zhang et al., 2019b). The goal of the for-\nmer is to discriminate actually attested selectional preference pairs (extracted from a large corpus) from\nrandomly-constructed, corrupted pairs. For the latter kind, selectional preference judgements of the\nmodel are compared to manually labeled datasets of human judgements, using a correlation measure. In\nthis research, we make use of the latter kind of evaluation.\n2.2 Interpretability of Neural Models\nThe recent emergence of contextual embedding models such as B ERT, and their subsequent applica-\ntion and improvement they brought on many NLP tasks, has been followed by extensive research on\nwhether these embeddings accommodate an implicit understanding of linguistic and semantic knowl-\nedge. A great deal of research has been focused on the syntactic knowledge learned by B ERT; Goldberg\n(2019) has found that BERT (a Transformer model) is more robust in syntactic tasks than a simple LSTM\narchitecture (a Recurrent Neural Network), and with a series of probing tasks on different datasets he\nproved that there is some syntactic knowledge beyond semantic and contextual relations. B ERT was\nespecially more successful in such tasks compared to other contextual embedding models, because of its\nbi-directional architecture (Wolf, 2019). Further research on learned syntactic information showed that\nBERT captures phrase-level information in the lower layers, and learns more sophisticated relations in\nhigher layers (Jawahar et al., 2019). Coenen et al. (2019) found that the attention matrices output by\nbert-base-uncased contain syntactic representations, with certain directions in space representing\nspeciﬁc relations, and they were also able to locate similar sub-spaces for semantic relations. Petroni\net al. (2019) report that B ERT contains enough relational knowledge to compete with knowledge-based\nmethods on tasks such as open-type questions, which leads them to the conclusion that the model has\nacquired a certain level of semantic knowledge. And Ettinger (2020) presents a number of experiments\nin which in many cases B ERT makes good predictions with regard to semantic ﬁt, such as hypernyms\nand subject-object nouns.\nHowever, McCoy et al. (2019) question the ability of BERT—and similar pretrained models—to truly\ncapture deep linguistic structures and semantic information, as past bibliography has suggested. Tenney\net al. (2019) also investigated pretrained models on their performance on both syntactic and semantic\nphenomena, and concluded that simple syntactic phenomena were successfully identiﬁed, but phenom-\nena which mostly relied on semantic relations were not as easily learned. Ettinger (2020) has also pointed\nout that BERT performance on predictions dropped in cases of true/false statements and negations. Zhang\net al. (2019c) created SemBERT, a BERT model with integrated explicit contextual semantics, supporting\nthe fact that external semantic knowledge was more useful than manipulating inherent model knowledge\nto achieve better results in semantics-related tasks. Mickus et al. (2020) delve further into exploring the\nembeddings of BERT, and report that it is uncertain whether the embeddings are able to properly repre-\nsent semantic similarities on a word-base level (as the theory of distributional semantics would suggest),\ndue to the inﬂuence of the context sentence on the distributional semantics space (even without meaning\ncorrelates).\n3 Methodology\n3.1 Selectional Preference Corpus\nOut of the several datasets of syntactic-semantic relations which have been released throughout the years\nfor linguistics and NLP research, such as MST98 (McRae et al., 1998), F-Inst (Ferretti et al., 2001), P07\n(Pad´o, 2007) and GDS-all (Greenberg et al., 2015), we decided to use SP-10K (Zhang et al., 2019b).\nSP-10K is the largest dataset available to date for evaluating the selectional preference abilities of natural\nlanguage processing tasks. It is composed of slightly over 10K pairs of words,1 evenly split into ﬁve dif-\nferent types of syntactic relations: nsubj (verb and noun as verb + subject), dobj (verb and noun as verb\n+ direct object), amod (noun and adjective where the adjective is a modiﬁer to the noun), nsubj amod\n1The authors had initially created 2,000 word pairs for each category, and later they added 124 more word pairs for the\nnsubj amod and dobj amod categories, from the Winograd Schema Challenge dataset (Levesque et al., 2012).\n1270\n(verb and adjective where the adjective is a modiﬁer to a noun which is the verb’s subject; the noun re-\nmains undeﬁned), and dobj amod (verb and adjective where the adjective is a modiﬁer to a noun which\nis the verb’s direct object; the noun remains undeﬁned). While the ﬁrst three categories deal with one-hop\nsyntactic relations, the two latter represent higher-level, two-hop dependencies, which the authors claim\nto also include meaningful semantic connections in certain cases and contexts. The words composing\nthe word pairs are 2,500 frequent words, lemmatized, and all of the word pairs are annotated with aplau-\nsibility score, i.e. a value between 0 and 10, which is derived from human judgements on how plausible\nthe dependent word (noun or adjective) is as an argument or modiﬁer to the head word (verb or noun).\n3.2 Prompt sentence corpus\nOur goal is to investigate the relative importance of selectional preference information on B ERT’s pre-\ndictions for a masked word within the context of a complete sentence. Therefore, the word pairs of the\nSP-10K corpus do not sufﬁce; additionally, we need to ﬁnd appropriate sentences that include the word\npairs in the correct syntactic positions. We want grammatical sentences, with varied contexts, in order to\nexamine in which cases selectional preferences have an inﬂuence on the prediction of the masked word.\nWe decided not to compose our own prompt sentences, as this would be a very large-scale effort and\ncould have introduced some unwanted biases. On the other hand, existing datasets of prompt sentences\nwere either too small to include a sufﬁcient amount of the SP-10K word pairs (such as the Corpus of\nLinguistic Acceptability, Warstadt et al. (2018)) or too specialized on semantic relations (such as the\nLPAQA corpus, Jiang et al. (2020)).\nThus, we decided to use a large corpus and extract sample sentences for each word pair. The ukWaC\ncorpus was created by crawling websites in the .uk domain, and it includes a variety of texts in English\n(articles, titles, user reviews, etc.) and over 2 billion words (Ferraresi et al., 2008). In order to ﬁnd the\nSP-10K word pairs in the ukWaC sentences in the correct syntactic positions and relations, we used a\nsyntactically annotated version of the corpus, which was parsed using the Malt parser (Nivre et al., 2006).\nOut of the 85 million sentences in the ukWaC corpus, we looked for short sentences (4 to 15 tokens),\nin order to stay well under B ERT’s limit of 512 tokens per sequence, but also in order to ensure that the\nsentences would not be erroneously composed of multiple sentences (due to segmentation errors), or be\ncomposed of multiple clauses, or include complex and distant dependencies. In an effort to eliminate\nsyntactic complexity as an extraneous factor of prediction difﬁculty, we considered excluding some spe-\nciﬁc dependency labels, such as xcomp (open clausal complement) and acl:rel (for relative subclauses),\nbut our selected sentences were already short enough to not broadly include more complex syntactic phe-\nnomena. We did not exclude passive clauses, because automatic parsing is rarely able to label passive\nstructures correctly, and we considered that for most of our syntactic relations, their existence would not\ncause a problem. Also, we determined that the distance between the two words of the pair in the sentence\nshould be between one and ﬁve words, allowing enough positions for determiners and modiﬁers, but not\ntoo distant to raise complexity.\nAt this stage in the collection of sentences, we decided to investigate the quality of the selected sen-\ntences. One problem we faced was parsing errors—which inevitably occur in automatic dependency\nparsing, but when repeated on the same word pair, they potentially produce many false prompt sen-\ntences. Additionally, we noticed a number of problems with regard to the quality of word pairs in the\nSP-10K corpus. For example, we noticed that some of the word pairs, in the context of the speciﬁed syn-\ntactic relation, should have been assigned with the lowest possible score (0), but were, in several cases,\nconsidered felicitous or at least plausible. We are aware that some of the word pairs were intentionally\ndesigned to be of low frequency or low plausibility; we are referring to falsely tagged syntactic struc-\ntures (e.g. “look way” is incorrectly included as a plausibledobj word pair, but the dependency between\nthese words is always verb and adverbial modiﬁer). For these reasons, we decided it was imperative\nto perform a quick and non-exhaustive manual evaluation of the SP-10K word pairs and the resulting\nextracted sentences. We provide an elaborate discussion on the challenges we faced with our datasets in\nSection 5.1.\nIn Table 1, we present the number of sentences, for each type of SP-10K word pairs, for which we\n1271\nfound at least one sentence containing the word pair in the given dependency relation and parts of speech,\nwith the criteria of length and distance that we previously determined, and the ﬁnal counts of the sen-\ntences after our manual evaluation.\nType Word pairs in ukWaC Found sents Final sents Avg. plaus. score\nnsubj 958 / 2,000 38,613 30,526 6.64\ndobj 980 / 2,000 70,250 56,777 7.39\namod 1,030 / 2,000 29,403 23,110 7.62\nnsubj amod 956 / 2,061 15,265 12,911 5.75\ndobj amod 922 / 2,063 28,336 21,839 6.32\nTOTAL 4,846 / 10,124 181,867 145,163\nTable 1: The number of SP-10K word pairs which were found in sentences of the ukWaC corpus (out of\nthe total number of word pairs), the initial number of found sentences and how many of those sentences\ninclude the word pairs in the correct syntactic positions (after our evaluation). In the last column is the\naverage value of the plausibility scores over all sentences.\n3.3 B ERT\nBERT (Devlin et al., 2019) is a Transformer-based bi-directional encoder, which is trained by randomly\nsampling positions in the input sequence and learning to ﬁll the word in the masked position. The\npretrained version has been trained on the Toronto BookCorpus (Zhu et al., 2015) and the English edition\nof Wikipedia, and pretrained models have been made available with 12 layers of attention (bert-base)\nor 24 layers (bert-large), and trained on lower-cased corpora (uncased) or as is (cased).\nFor our experiments, we used the bert-base-uncased model for English, as provided by Hug-\ngingFace’stransformers Python library (Wolf et al., 2019); this model has been used by Goldberg\n(2019) for syntactic tasks and performed well, and has also been favoured by the researchers probing for\nthe semantic tasks mentioned in Section 2 (e.g. McCoy et al. (2019)). Some preliminary experiments\nwe performed with bert-large-uncased did not show signiﬁcantly different results, thus for the\nsake of time-conservation we used the computationally-lighter base model. We add the special B ERT\ntokens [CLS] (to indicate the start of a sentence) and [SEP] (to indicate the end of it). We make use of\nthe model’s built-in tokenizer,BertTokenizer, and we do not perform any ﬁnetuning of the encoder\nweights, but make use of the pretrained model as it is made available.\n3.4 Correlation of SP-10K score and BERT probability\nFor each example sentence in our corpus, we mask the dependent word of the word pair using a [MASK]\ntoken, and we retrieve the probability that is assigned to the target word in the focal position. The\nprobability is computed by passing the last hidden state through a softmax function, a feature that is also\nused by Wang and Cho (2019) in the context of language modeling. We are making the assumption that\nthis result is to be treated as the conditional probability of a bi-directional language model (similar to\nwhat a traditional language model would return) even though we are aware that B ERT’s bi-directional\nnature means that this assumption is not self-evident.\nNext, we compute the correlation of the masked word’s probability and the plausibility score of the\nword pair, using the Kendall rank correlation coefﬁcient as implemented by the scipy Python library.\nKendall τ (tau) correlation is a non-parametric measure of the monotonicity of the relationship between\ntwo datasets. The p-value roughly indicates the probability of an uncorrelated system producing datasets\nthat have a correlation at least as extreme as the one computed from these datasets. 2 Values close to 1\nindicate strong positive correlation, while values close to -1 indicate strong disagreement. Intuitively, we\nare looking for a strong positive correlation, meaning that the higher the plausibility score of the word\npair, the higher the probability of the dependent word in the context of the head word.\n2In the remainder of this paper, signiﬁcant results are deﬁned as p <0.01.\n1272\nsentence the ﬁlm tells the story of that trial\nstandard [CLS] the ﬁlm tells the [MASK] of that trial [SEP]\nhead [CLS] the ﬁlm the [MASK] of that trial [SEP]\ncontext [CLS] tells [MASK] [SEP]\ncontrol [CLS] [MASK] [SEP]\nFigure 1: Illustration of the four attention mask settings, for a sentence with the word pair “tell story” (as\na dobj relation). Greyed out words indicate blocked attention.\nIn order to determine the relative importance of selectional preference information, we compute prob-\nabilities within different attention settings by using attention masks; the attention mask is an array of\n1s and 0s indicating which tokens we do not wish to incorporate in the way the model attends to the\nsequence. By using this feature, we are able to “block” certain tokens of the sentence from Bert’s self-\nattention mechanism, and examine the impact it brings to the probability scores and the correlation. We\nuse four different settings: the standard setting does not involve any masking, thus the model can attend\nto the whole sequence; the head setting blocks attention to the head word of the pair, so prediction needs\nto be based on other context words; the context setting masks all the context words, so prediction needs\nto be based on the head word (and B ERT’s special tokens), and the control setting masks all the words\nof the sequence (except for the special tokens), so that no adequate prediction should be possible (as a\nsanity check). A graphical illustration of the four different attention settings is given in Figure 1.\nWe compute correlation scores between the model’s probabilities and the plausibility scores for all\nsentences, and provide both micro- and macro-averaged results. As mentioned before, the number of\nextracted example sentences per word pair differs signiﬁcantly. The micro-averaged results are computed\nover the entire set of sentences, without taking into account this variable number of sentences. For the\nmacro-averaged results, we ﬁrst compute the average for each word pair, and then provide a global\naverage for all the pairs (hence treating all word pairs as equally important). We will consider as a strong\npositive correlation a value above 0.4, and as strong negative correlation a value below −0.4.\n4 Results\nAs seen in Table 2, we do not observe a strong positive or negative correlation for any of the ﬁve syntactic\nrelation categories and for any of the attention mask scenarios. Out of all the attention mask scenarios,\ncontext mask (masking attention for the entire sequence except for the head word and B ERT tokens)\nshows the highest positive values (fair correlation up to +/- .30), while the head attention mask scenario\n(masking attention for the head word and attending to the context for prediction) showed no correlation\nbut was biased to negative values. Compared to the standard setting (all tokens are attended to), we\nnotice the effect that the head word has in predictions; the context words affect the probability of the\nprediction, but the absence of the head word is detrimental. This observation supports the argument\nthat, for the syntactic relations involving verbs as the head word, the verb’s selectional preferences are\nrelatively important and inﬂuential enough in the selection of constituents, and that B ERT is aware of\nthese preferences and constraints, and uses them to assign a proportionate probability to the dependent\nword.\nNote that for the amod syntactic relations (noun and its modiﬁer) the correlation scores are among the\nhighest, reasonably so, as the noun should place stronger constraints on its modiﬁer. Thensubj relations\nshow slightly more positive correlations than the dobj ones, probably due to the constraint of animacy\nfor a subject (word pairs included a variety of animate and inanimate subjects). The two-hop relations,\nnsubj amod and dobj amod show lower correlations, still relatively high for thecontext attention mask\nsetting, which is in accordance with the hypothesis of Zhang et al. (2019b) that selectional preferences\nspan further than one-hop relations.\nTaking a closer look at the head words of the word pairs, we searched for strong positive or negative\ncorrelations for each head word that exists in at least two different word pairs, per syntactic relation. We\n1273\nstandard head context control\nnsubj 0.03 -0.02 0.16 -0.01\ndobj 0.05 -0.07 0.05 -0.05\namod 0.04 -0.06 0.24 -0.04\nnsubj amod -0.01 -0.13 0.29 -0.00\ndobj amod 0.06 0.01 -0.03 0.02\n(a) Micro-averaged results\nstandard head context control\nnsubj 0.19 0.15 0.29 0.08\ndobj 0.16 0.04 0.27 0.05\namod 0.15 0.03 0.35 0.03\nnsubj amod 0.01 -0.04 0.22 0.06\ndobj amod 0.14 0.10 0.20 0.07\n(b) Macro-averaged results\nTable 2: Kendall τ (tau) correlation coefﬁcient of masked word probability and word pair plausibility\nscore.\nexamined whether speciﬁc verbs and nouns affected positively or negatively the correlation of probability\nand plausibility, and whether there were common features between these head words (e.g. semantic\nsimilarity, common semantic class). We grouped the probabilities and scores of sentences per head\nword, and calculated the correlation coefﬁcient for head words that were present in at least two different\nword pairs. Overall, for all ﬁve syntactic categories of our experiment, we do not notice distinct classes,\nsemantic or syntactic, that the words with strong correlations could be grouped with.\nFor nsubj relations, verbs of semantic similarity (in at least one of their meanings) did not demon-\nstrate similar patterns of probability and correlation; for example, the verbs of violence (in some contexts)\n“kill”, “strike”, “grab”, “ﬁre” show a strong positive correlation, while the verbs of the same semantic\nclass “shoot” and “confront” have a strong negative correlation – this could be caused by the different\nmetaphorical meanings that these two words might have, or the dependent words that they were paired\nwith in the SP-10K dataset (favorable for “kill”, detrimental for “shoot”). Concerning the type of sub-\njects, the animate subject “man” had a high plausibility score in the SP-10K dataset and high probability\nscores for “kill” and “shoot”, causing a strong positive correlation. The inanimate subjects had mid-range\nplausibility scores (“earthquake”, “explosion”) or low scores (“ﬁlm”, “tragedy”) but the probability var-\nied based on the sentence and metaphorical use; for the word pair “strike tragedy” which existed in many\nsentences of our dataset, the plausibility score was 5.25 and the assigned probability for “tragedy” was\nrelatively low, even though the idiomatic phrase “tragedy struck” is fairly common.\nExamining the dobj relations, verbs (head words) showed inconsistent correlations among the different\nattention mask scenarios; out of the few verbs that showed consistently positive or negative correlation,\nwe were not able to identify semantic clusters of verbs, neither differences based on verb transitiv-\nity (monotransitive/ditransitive). The presence of a strong correlation relied more on B ERT’s semantic\nknowledge rather than world knowledge or utterance plausibility; for example, the word pair “blame cus-\ntomer” has (correctly) a moderate plausibility score (6.75), is found twice in the ukWaC corpus, but the\nassigned probability by B ERT of the word “customer” is very low in the standard and context attention\nmask scenarios. The word pair “blame management”, on the other hand, with slightly lower plausibility\n(6.25) is assigned a proportionally good probability. This leads us to the conclusion that, even though\nboth syntactic pairs are grammatically correct and have commonly used words, the pretrained model has\nlearned that “management” (someone in control and responsible of a service) is a more probable direct\nobject for the verb “blame” than the word “customer (the receiver of a service), especially when the only\ngiven context is the verb. When attention to the head word was removed, there was no strong negative\ncorrelation between “blame” and the given plausibility score.\nConcerning the amod word pairs, again no semantic class of nouns appear consistently in the positive\nor relative correlation groups. An interesting observation is that high-frequency adjectives of size and\nage, such as “small”, “big”, “old” and “new” were almost always assigned a high probability by BERT,\nbut the variations in plausibility score (from 8.25 to 4.25) led to strong positive or negative correlations,\nespecially since word pairs with these adjectives are quite frequent in our corpus, for example “new\nhouse”, “small bird” and “new face” had many occurrences in the corpus and a strong positive correlation\n(high plausibility/high probability), “new material” (6.5) and “old daughter” (4.25) had lower plausibility\nscores and subsequently lower probabilities, in all attention mask scenarios.\n1274\nIn the nsubj amod word pairs, again we see that high-frequency descriptive adjectives (dependent\nwords) are still assigned higher probabilities, even though the plausibility scores are more mediocre for\nthe word pairs of this relation, therefore high-frequency adjectives can be found in both the strong posi-\ntive and negative correlation groups. We also do not notice distinctive semantic classes among the verbs\n(head words), and neither can we make assumptions based on the animacy of the subject, since the adjec-\ntive modiﬁers do not follow such a constraint (“new”, “local”, “national”, “exact”, “different”) and the\ngiven verbs do not have the animacy constraint either (“bring”, “attract” had a strong positive correlation,\n“increase”, “reﬂect” a strong negative). Some verbs that do prefer animate objects were found to have\na strong positive correlation (e.g. “compare”, “operate”), others to have a strong negative (e.g. “lift”).\nConcerning the different attention scenarios, there is a noticeable positive shift in correlations (+.30,\n+.20) with the context attention mask compared to no mask or masking the head word, which hints at the\ninﬂuence that the verb had in the predictions, and how the context (including the one-hop dependency to\nthe verb subject) produced less polarizing probabilities.\nFinally, for the dobj amod word pairs, as in the direct object word pairs, we do not notice verb\ngrouping based on semantics or transitivity. Many of the verbs with strong positive (“teach”, “promise”)\nor negative correlation (“claim”, “conﬁrm”) are verbs with varied subcategorization frames. In this\nsyntactic category, we observe the smallest positive shifts with the use of thecontext attention mask, and\neven a decrease in correlation (-.03) in the micro-averaged results. However, the results still show a weak\npositive correlation similar to the ones of the other syntactic relations, for the most part; this observation\nsupports the fact that the role of the verb is quite important for predictions.\n5 Discussion\n5.1 The limitations of our datasets\nAs mentioned in Section 3.2, we would like to elaborate on the issues we faced during the creation of our\nprompt sentences, how we dealt with them, and whether they could seriously impede our results. First\nof all, we noticed some problematic word pairs in the SP-10K corpus, which were included in a group\nwith a certain syntactic relation, which they could not possess. For example, some word pairs under\nthe verb-direct object relation included intransitive verbs such as laugh, walk, smile, or verbs that could\nnot accept the dependent word as a direct object such as look way, think time, and these pairs were still\nassigned plausible scores when the plausibility should have been zero (e.g. look way, where way had a\nscore of 6.5). We assume that these problematic high scores were given by naive crowdsource annotators,\nbecause distributionally there is indeed a strong correlation between the words of a pair (which makes\nthem highly likely to co-occur in a text), but little attention was paid to the proposed syntactic relation.3\nThese word pairs with problematic head or dependent words were removed from our query for sentences,\nin order to make sure that they were not accidentally found in a sentence with a wrong parse tree.\nOn the other hand, some word pairs, especially the ones which were by design of low plausibility and\nhad a low plausibility score, are not found in the corpus, as shown in Table 1 – almost half of the word\npairs for all types of syntactic relation. However, some word pairs are very common in the corpus, and\nare found in disproportionately more sentences. For this reason, we provided both micro- and macro-\naveraged results in Section 4. In addition, there are several word pairs which are parts of idiomatic,\nlexicalized phrases, and are very frequent in the ukWaC corpus and almost exclusively found in the\ncontext of these idiomatic phrases, but were assigned a low score. As an example, for thensubj relation,\nin the pairs “weather permit” ( 4.06) and “study ﬁnd” ( 4.0), the subjects are inanimate (whereas the\nverbs generally require an animate subject) but in this speciﬁc context they are acceptable. Interestingly\nenough, contrary to our personal intuition, B ERT seems to be in accordance with the SP-10K scores,\nbecause it does assign a moderate to low probability to the dependent words, even in thecontext attention\nmask setting. This could imply that B ERT is able to capture, to some extent, a verb’s preferences and\nconstraints, and can make predictions based on them, when the use is not metaphorical and conﬂicting\nwith usual, literal cases.\n3The Mechanical Turk workers of the SP-10K project were presented with word pairs, without any other context, and asked\nto evaluate the plausibility of the second word being dependent to the ﬁrst with a speciﬁc syntactic relation.\n1275\n5.2 Does B ERT really learn selectional preference?\nIn our experiment, we studied how changing the way the input sequence is attended to could shift predic-\ntions and the probability of a masked word. Our goal was to observe how much the head word affected\nthe probability of the dependent word, and whether context played a more important role than the head\nword itself. The fact that the highest positive correlation values almost always came from allowing atten-\ntion only to the head word (and the non-lexical BERT tokens) signiﬁes that the head word is identiﬁed as\nan integral and inﬂuential part of the sequence when it comes to selecting a masked word. Interestingly\nenough, the nsubj amod and dobj amod categories showed, for the most part, similar positive correla-\ntions (especially in the context attention block scenario) as the one-hop syntactic relations. As Zhang et\nal. (2019b) have also mentioned, these two-hop relations also fall under the inﬂuence of a word’s selec-\ntional preferences, and the fact that the head word in these cases is the head of the sequence (the verb, in\nour simple prompt sentences) could have impacted the selection of a modiﬁer to a greater extent than the\ncontext could.\n6 Conclusion\nIn this paper, we explored whether selectional preference information makes part of the linguistic in-\nformation that is learned by a Transformer model, by examining the correlation of the plausibility of a\nhead-dependent word pair and the assigned probability of the dependent word in a sequence including\nthe head word. Our overall results did not show a strong correlation that would deﬁnitively prove or\ndisprove the presence of selectional preferences, but there are indications that B ERT’s embeddings have\ncaptured enough syntactic-semantic information to be able to assign probability based on “the right ﬁt”\nfor a head word. In addition, some speciﬁc cases between syntactic relations and metaphorical uses have\ngiven us the incentive to further investigate these phenomena, and to delve deeper in the architecture\nof the model for answers. The code we used for our experiments will be made available at a GitHub\nrepository4.\nAs for future work, a further exploration of the attention output would be quite beneﬁcial in under-\nstanding the attention weights during prediction, in each scenario, and how they differ per layer and\nattention head. Researchers who have attempted to thoroughly analyze B ERT’s attention behavior in\nsyntactic probing tasks have noted that the attention maps have “a fairly thorough representation of En-\nglish syntax” [sic] (Clark et al., 2019), and have noticed that speciﬁc attention heads and layers specialize\non learning speciﬁc linguistic knowledge, such as syntactic dependencies (Kovaleva et al., 2019). Visu-\nalizing attention for 12 layers and 12 attention heads, over 145K sentences, is a complicated task which\nwe intend to tackle soon, with the help of visualization libraries (Vig, 2019).\nAcknowledgements\nThis work has been funded by CNRS (80 ∣PRIME-2019 project MoDiCLI). Experiments presented in\nthis paper were carried out using the OSIRIM platform 5 that is administered by IRIT and supported\nby CNRS, the Region Midi-Pyr ´en´ees, the French Government, and ERDF. We would like to thank our\nreviewers for their insightful comments and suggestions.\nReferences\nStephen Clark and David Weir. 2001. Class-based probability estimation using a semantic hierarchy. In Proceed-\nings of the second meeting of the North American Chapter of the Association for Computational Linguistics on\nLanguage technologies, pages 95–102. Association for Computational Linguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. 2019. What Does BERT Look at?\nAn Analysis of BERT’s Attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pages 276–286.\n4https://github.com/lenakmeth/bert selectional preferences\n5https://osirim.irit.fr/\n1276\nAndy Coenen, Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Adam Pearce, and Been Kim. 2019.\nVisualizing and measuring the geometry of BERT. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-\nBuc, E. Fox, and R. Garnett, editors,Advances in Neural Information Processing Systems 32, pages 8594–8603.\nCurran Associates, Inc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirec-\ntional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186.\nKatrin Erk, Sebastian Pad ´o, and Ulrike Pad ´o. 2010. A ﬂexible, corpus-driven model of regular and inverse\nselectional preferences. Computational Linguistics, 36(4):723–763.\nKatrin Erk. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of the 45th\nAnnual Meeting of the Association of Computational Linguistics, pages 216–223, Prague, Czech Republic,\nJune. Association for Computational Linguistics.\nAllyson Ettinger. 2020. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language\nmodels. Transactions of the Association for Computational Linguistics, 8:34–48.\nAdriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating\nukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus Workshop\n(WAC-4) Can we beat Google, pages 47–54.\nTodd R Ferretti, Ken McRae, and Andrea Hatherell. 2001. Integrating verbs, situation schemas, and thematic role\nconcepts. Journal of Memory and Language, 44(4):516–547.\nDaniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational linguistics,\n28(3):245–288.\nYoav Goldberg. 2019. Assessing BERT’s Syntactic Abilities. arXiv preprint arXiv:1901.05287.\nClayton Greenberg, Vera Demberg, and Asad Sayeed. 2015. Verb polysemy and frequency effects in thematic ﬁt\nmodeling. In Proceedings of the 6th Workshop on Cognitive Modeling and Computational Linguistics, pages\n48–57.\nGanesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. 2019. What Does BERT Learn about the Structure of Lan-\nguage? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages\n3651–3657.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How Can We Know What Language Models\nKnow? Transactions of the Association for Computational Linguistics, 8:423–438.\nJerrold J. Katz and Jerry A. Fodor. 1963. The structure of a semantic theory. Language, 39(2):170–210.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of BERT.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4365–4374, Hong\nKong, China, November. Association for Computational Linguistics.\nHector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thirteenth\nInternational Conference on the Principles of Knowledge Representation and Reasoning.\nHang Li and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the MDL principle.Computational\nlinguistics, 24(2):217–244.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nDiana McCarthy and John Carroll. 2003. Disambiguating nouns, verbs, and adjectives using automatically ac-\nquired selectional preferences. Computational Linguistics, 29(4):639–654.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics\nin natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pages 3428–3448, Florence, Italy, July. Association for Computational Linguistics.\n1277\nKen McRae, Michael J Spivey-Knowlton, and Michael K Tanenhaus. 1998. Modeling the inﬂuence of thematic ﬁt\n(and other constraints) in on-line sentence comprehension. Journal of Memory and Language, 38(3):283–312.\nTimothee Mickus, Denis Paperno, Mathieu Constant, and Kees van Deemter. 2020. What do you mean, BERT?\nAssessing BERT as a Distributional Semantics Model. Proceedings of the Society for Computation in Linguis-\ntics, 3(1):350–361.\nJoakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency\nparsing. In LREC, volume 6, pages 2216–2219.\nDiarmuid ´O S ´eaghdha and Anna Korhonen. 2012. Modelling selectional preferences in a lexical hierarchy. In\nProceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of\nthe main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on\nSemantic Evaluation, pages 170–179. Association for Computational Linguistics.\nDiarmuid ´O S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings of the 48th An-\nnual Meeting of the Association for Computational Linguistics, pages 435–444. Association for Computational\nLinguistics.\nUlrike Pad ´o. 2007. The integration of syntax and semantic plausibility in a wide-coverage model of human\nsentence processing. Doctoral thesis.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLT, pages 2227–2237.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander\nMiller. 2019. Language Models as Knowledge Bases? In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2463–2473.\nPhilip Resnik. 1996. Selectional constraints: An information-theoretic model and its computational realization.\nCognition, 61:127–159, November.\nAlan Ritter, Mausam, and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences.\nIn Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424–434,\nUppsala, Sweden, July. Association for Computational Linguistics.\nMats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically\nannotated lexicon via EM-based clustering. In Proceedings of the 37th annual meeting of the Association\nfor Computational Linguistics on Computational Linguistics, pages 104–111. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin\nVan Durme, Samuel R Bowman, Dipanjan Das, et al. 2019. What do you learn from context? probing for\nsentence structure in contextualized word representations. arXiv preprint arXiv:1905.06316.\nTim Van de Cruys. 2009. A non-negative tensor factorization model for selectional preference induction. In\nProceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 83–90, Athens,\nGreece, March. Association for Computational Linguistics.\nTim Van de Cruys. 2014. A neural network approach to selectional preference acquisition. In Proceedings of the\n2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26–35, Doha, Qatar,\nOctober. Association for Computational Linguistics.\nJesse Vig. 2019. A Multiscale Visualization of Attention in the Transformer Model. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 37–42.\nAlex Wang and Kyunghyun Cho. 2019. BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field\nLanguage Model. In Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language\nGeneration, pages 30–36.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2018. Neural network acceptability judgments.CoRR,\nabs/1805.12471.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace’s Transformers: State-of-\nthe-art Natural Language Processing. ArXiv, abs/1910.03771.\n1278\nThomas Wolf. 2019. Some additional experiments extending the tech report ”Assessing BERT’s syntactic abili-\nties” by Yoav Goldberg. Technical report, Huggingface Inc.\nHongming Zhang, Jiaxin Bai, Yan Song, Kun Xu, Changlong Yu, Yangqiu Song, Wilfred Ng, and Dong Yu. 2019a.\nMultiplex word embeddings for selectional preference acquisition. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pages 5247–5256, Hong Kong, China, November. Association for\nComputational Linguistics.\nHongming Zhang, Hantian Ding, and Yangqiu Song. 2019b. SP-10K: A large-scale evaluation set for selec-\ntional preference acquisition. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pages 722–731, Florence, Italy, July. Association for Computational Linguistics.\nZhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi Zhou, and Xiang Zhou. 2019c.\nSemantics-aware BERT for language understanding. arXiv preprint arXiv:1909.02209.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.\n2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. In Proceedings of the IEEE international conference on computer vision, pages 19–27.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7138671875
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6471471190452576
    },
    {
      "name": "Natural language processing",
      "score": 0.6439430713653564
    },
    {
      "name": "Sentence",
      "score": 0.6150474548339844
    },
    {
      "name": "Predicate (mathematical logic)",
      "score": 0.5497636198997498
    },
    {
      "name": "Linguistics",
      "score": 0.4481451213359833
    },
    {
      "name": "Transformer",
      "score": 0.41998013854026794
    },
    {
      "name": "Correlation",
      "score": 0.41821885108947754
    },
    {
      "name": "Context (archaeology)",
      "score": 0.41052156686782837
    },
    {
      "name": "Mathematics",
      "score": 0.1922524869441986
    },
    {
      "name": "Philosophy",
      "score": 0.08406606316566467
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}