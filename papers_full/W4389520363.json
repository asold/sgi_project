{
  "title": "Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model",
  "url": "https://openalex.org/W4389520363",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1996695599",
      "name": "Qi Jia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2226651894",
      "name": "Siyu Ren",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2139720748",
      "name": "Yizhu Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3039551673",
      "name": "Kenny Zhu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4361230777",
    "https://openalex.org/W3173529047",
    "https://openalex.org/W3175541789",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W4389519239",
    "https://openalex.org/W3197332064",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4307106504",
    "https://openalex.org/W3101551503",
    "https://openalex.org/W3173844397",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4224947967",
    "https://openalex.org/W4385573249",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W3137501073",
    "https://openalex.org/W3034407283",
    "https://openalex.org/W4287887686",
    "https://openalex.org/W4378508605",
    "https://openalex.org/W4401043228",
    "https://openalex.org/W3091889354",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4287854583",
    "https://openalex.org/W2947681066",
    "https://openalex.org/W3099766584",
    "https://openalex.org/W2951211142",
    "https://openalex.org/W3156450987",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3168251909",
    "https://openalex.org/W2101807845",
    "https://openalex.org/W4320858112",
    "https://openalex.org/W4377297670",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W3106234277",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W4362679631",
    "https://openalex.org/W4382202592",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W3205944346",
    "https://openalex.org/W3034188538",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W4385574184",
    "https://openalex.org/W3170432046",
    "https://openalex.org/W3035725000",
    "https://openalex.org/W4205477024",
    "https://openalex.org/W4288379066"
  ],
  "abstract": "Despite tremendous improvements in natural language generation, summarization models still suffer from the unfaithfulness issue. Previous work evaluates faithfulness either using models trained on the other tasks or in-domain synthetic data, or prompting a large model such as ChatGPT. This paper proposes to do zero-shot faithfulness evaluation simply with a moderately-sized foundation language model. We introduce a new metric FFLM, which is a combination of probability changes based on the intuition that prefixing a piece of text that is consistent with the output will increase the probability of predicting the output. Experiments show that FFLM performs competitively with or even outperforms ChatGPT on both inconsistency detection and faithfulness rating with 24x fewer parameters. FFLM also achieves improvements over other strong baselines.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11017–11031\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nZero-shot Faithfulness Evaluation for Text Summarization with\nFoundation Language Model\nQi Jia Siyu Ren\nShanghai Jiao Tong University, China\n{Jia_qi, roy0702}@sjtu.edu.cn\nYizhu Liu\nMeituan, China\nliuyizhu@meituan.com\nKenny Q. Zhu∗\nUniversity of Texas at Arlington, USA\nkenny.zhu@uta.edu\nAbstract\nDespite tremendous improvements in natural\nlanguage generation, summarization models\nstill suffer from the unfaithfulness issue. Pre-\nvious work evaluates faithfulness either using\nmodels trained on the other tasks or in-domain\nsynthetic data, or prompting a large model\nsuch as ChatGPT. This paper proposes to do\nzero-shot faithfulness evaluation simply with a\nmoderately-sized foundation language model.\nWe introduce a new metric FFLM, which is\na combination of probability changes based\non the intuition that preﬁxing a piece of text\nthat is consistent with the output will increase\nthe probability of predicting the output. Ex-\nperiments show that FFLM performs compet-\nitively with or even outperforms ChatGPT on\nboth inconsistency detection and faithfulness\nrating with 24x fewer parameters. FFLM also\nachieves improvements over other strong base-\nlines.\n1 Introduction\nFaithfulness evaluation for text summarization\naims at measuring if the information in a sum-\nmary is fully covered by and consistent with the\nsource document 1. Although automatic text sum-\nmarization has achieved remarkable improvements\nwith pre-trained language models (Zhang et al.,\n2020; Lewis et al., 2020; Liu et al., 2021, 2022a;\nZhang et al., 2023) in recent years, especially in the\naspect of ﬂuency and informativeness. However,\nthese neural models tend to generate unfaithful\nsummaries. An effective faithfulness evaluation\nmetric not only helps for implementing summariza-\ntion systems in real applications but also plays a\nkey role in developing more faithful summarization\nmodels, such as by data ﬁltering (Matsumaru et al.,\n∗ The corresponding author.\n1We use the words “faithfulness”, “consistency” and\n“(without) hallucination” interchangeably. Extrinsic hallucina-\ntions that are correct to the world knowledge are regarded as\nunfaithfulness in this work.\n2020) or doing post-hoc corrections (Chaudhury\net al., 2022).\nMost previous work for faithfulness evaluation\neither takes advantage of models trained on re-\nlated tasks for zero-shot evaluation (Goodrich et al.,\n2019; Falke et al., 2019; Wang et al., 2020), or\ndoes weakly-supervised evaluation with synthetic\nin-domain data (Kry´sci´nski et al., 2020). The for-\nmer requires transferring out-of-box models to the\nsummarization domain (Mishra et al., 2021), which\nlacks guarantees on the models’ performance and\nsuffers from error propagation (Ji et al., 2023). The\nlatter one shows poor generalization ability (La-\nban et al., 2022) as a result of the limited synthetic\nrules that couldn’t cover various kinds of halluci-\nnations. Recently, as ChatGPT (OpenAI, 2022)\nhas shown amazing generation abilities on various\ntasks, researchers attempt to do human-like evalu-\nation by designing prompts to query the model in\nthe zero-shot manner (Luo et al., 2023). However,\nsuch strong language models are still sensitive to\nnuances, showing unstable performance with dif-\nferent wording of prompts (Gao et al., 2023; Chen\net al., 2023).\nConsidering the above weaknesses, we think that\nan ideal faithfulness evaluation metric for sum-\nmarization should be independent of other tasks\nand dataset-speciﬁc expertise, be able to general-\nize among different benchmarks and robust for the\nsame document-summary pair. Zhou et al. (2023)\nconcludes that instruction tuning is just to teach\nthe model to produce high-quality output while al-\nmost all of the knowledge has been learned during\npre-training for large language models. Based on\ntheir ﬁndings, we wonder: can we get rid of the\npopular prompting approaches and calculate the\nfaithfulness score simply with a foundation lan-\nguage model, which meets the above expectations?\nIn this work, we propose a metric named FFLM\nfor zero-shot faithfulness evaluation with a founda-\ntion language model. The intuition behind FFLM\n11017\nis that the generation probability of a piece of text\nwill increase when preﬁxing another piece of con-\nsistent text. Following this intuition, we classify\ndifferent kinds of probability changes into changes\nwith prior probability and changes with conditional\nprobability. The former contains a comparison be-\ntween the vanilla sequence-to-sequence probabili-\nties of the summary given document and uncondi-\ntional probabilities of the summary, and a similar\ncomparison by changing the position of the docu-\nment and the summary. The latter calculates the\nvanilla sequence-to-sequence probability with an-\nother conditional probability by adding a piece of\npreﬁx text. Similar intuition has been considered in\nprevious works (She et al., 2023; Son et al., 2022).\nThe major differences are that their metrics were\ncarried out on models ﬁne-tuned by summariza-\ntion data and they only consider a single kind of\nprobability changes. Our FFLM is based on the\nfoundation language model, and we hypothesize\nthat these different probability changes capture dif-\nferent hallucinations (see Sec. 4.4) which should\nbe considered as a whole.\nOn top of these three components of probability\nchanges, we introduce a feasible design of FFLM\nby re-weighting each token and each component\nto get the ﬁnal faithfulness score. We did experi-\nments in both the inconsistency detection setting\nand the faithfulness rating setting for summariza-\ntion evaluation. The results show the favorable\nperformance of our FFLM across different settings\nand datasets 2. Our contributions are as follows:\n• We propose to do zero-shot faithfulness\nevaluation based on a foundation language\nmodel(Sec. 4.6).\n• We introduce a comprehensive evaluation\nmetric FFLM by calculating the probability\nchanges of the desired output in different\nways(Sec. 2) and verify the rationality of our\nmetric design(Sec.4.3).\n• Experiments on different evaluation settings\nshow that FFLM based on LLaMa with only\n7 billion parameters can achieve competitive\nperformances or even outperforms ChatGPT\namong different datasets(Sec 4.1 and 4.2).\n2The code and dataset for this paper are available athttps:\n//github.com/JiaQiSJTU/FaithEval-FFLM.\n2 Approach\nGiven a source document X = {x1,...,x n}and\nthe corresponding summary Y = {y1,...,y m}, the\ngoal of this work is to design a metric FFLM mea-\nsuring the faithfulness of Y based on the founda-\ntion model LM(·). We adopt LM(·) under the\nteacher-forcing strategy, which can provide a se-\nquence of generation probabilities pof a given text\nwith or without other conditional inputs. We ﬁrst\nintroduce three probability changes for faithfulness\nmeasurements and then propose a feasible design\nof our comprehensive metric FFLM. Scores pro-\nposed by She et al. (2023) and Son et al. (2022) are\nin Appendix A.\n2.1 Faithfulness Measurements via\nProbability Changes\nThe intuition is that the generation probability of\na piece of text will increase when providing more\nrelated and consistent information. On the contrary,\nthe generation probability will drop when condi-\ntioned on inconsistent information. Accordingly,\nwe considered three different probability changes\nin two categories as follows.\nChanges with Prior Probability: The prior\nprobability of Y can be estimated by the foundation\nmodel LM(·):\nplm\nY = LM(Y) = {plm\nyi }|m\ni=1 (1)\nand the sequence-to-sequence probability of Y\ngiven X is:\nps2s\nY = LM(Y|X) = {ps2s\nyi }|m\ni=1 (2)\nIf Y is a faithful summary, the sequence-to-\nsequence probability ps2s\nY should be larger than the\nprior probability plm\nY as more information consis-\ntent to Y is given by conditioning on X. Therefore,\na faithfulness measurement can be deﬁned as:\n∆prior\npY = 1\nm\n∑m\ni=1\nps2s\nyi −plm\nyi (3)\nFrom another point of view, we expect that the\ngeneration of Y highly relies on X, instead of para-\nmetric knowledge stored in LM which is a main\nresource of hallucinations (Ji et al., 2023).\nSimilarly, a faithful Y can support the contents\nin X. Thus, the differences between the sequence-\nto-sequence probability of Xgiven Y and the prior\n11018\nprobability of X is another reasonable measure-\nment:\nplm\nX = LM(X) = {plm\nxi }|n\ni=1\nps2s\nX = LM(X|Y) = {ps2s\nxi }|n\ni=1\n∆prior\npX = 1\nn\n∑n\ni=1\nps2s\nxi −plm\nxi\n(4)\nChanges with Conditional Probability: In-\nstead of comparing sequence-to-sequence gener-\nation probabilities with prior probabilities, another\nway is to add more information P besides the in-\nput document X, leading to an inﬂuence on the\ngeneration probability of Y. Following She et al.\n(2023), we simply set P = Y. In this way, if Y\nis inconsistent with X, preﬁxing P will cause in-\nformation contradictions in the input and decrease\nthe probability of Y compared to a consistent one.\nMathematically, the third measurement is:\nppref\nY = LM(Y|P,X) = {ppref\nyi }|m\ni=1\n∆cond\npY = 1\nm\n∑m\ni=1\nps2s\nyi −ppref\nyi\n(5)\nWe didn’t considerXand Y reversely here. The\nmain reason is that inputting the sequence [P =\nX,Y,X ] to LM(·) is much more costly and may\nexceed the max sequence length of most models\nsince X is much longer than Y, i.e., n≫m.\n2.2 A Feasible Design of FFLM\nGoyal et al. (2022) found that high-loss tokens\ngenerally correspond to unfaithful contents during\ntraining a summarization model. Inspired by this\nﬁnding and the success of the loss truncation train-\ning algorithms (Kang and Hashimoto, 2020), we\nthink that more attention should be paid to such\nhigh-loss (or low-probability) tokens when calcu-\nlating the faithfulness scores. So, instead of simply\naveraging the probability changes to get the ﬁnal\nscore for an (X, Y) pair, we adopt two operations.\nFirst, we take the logarithm of the probabilities be-\nfore subtraction, which will magnify changes on\nthe low-probability tokens. Second, we re-weight\neach token based on ps2s\nY and ps2s\nX correspondingly.\nWe get:\n∆prior\nY = 1\nm\n∑m\ni=1\neps2s\nyi (log ps2s\nyi −log plm\nyi )\n∆prior\nX = 1\nn\n∑n\ni=1\neps2s\nxi (log ps2s\nxi −log plm\nxi )\n∆cond\nY = 1\nm\n∑m\ni=1\neps2s\nyi (log ps2s\nyi −log ppref\nyi )\n(6)\nFinally, FFLM is a combination of these metrics:\nFFLM = α∆prior\nY + β∆prior\nX + δ∆cond\nY (7)\nwhere α, β, and δare weighting parameters in the\nrange of 0 to 1 and α+ β+ δ = 1. These three\nweights can be tuned on a validation set, or set\nmanually as hyper-parameters.\n3 Experiment Setup\nWe present two evaluation settings considered by\nprevious work for faithfulness evaluation ﬁrst, with\nthe implementation details of FFLM for them later.\n3.1 Inconsistency Detection\nInconsistency detection regards the faithfulness\nevaluation as a binary classiﬁcation problem. In\nother words, human annotators or automatic met-\nrics only need to recognize whether the summary\nis faithful to the document or not.\nDatasets: The SUMMAC Benchmark (Laban\net al., 2022) is a benchmark consisting of six\nsummarization evaluation datasets, including Co-\nGenSumm Falke et al. (2019), SummEval (Fabbri\net al., 2021), FRANK (Pagnoni et al., 2021), Poly-\ntope (Huang et al., 2020), FactCC (Kry´sci´nski et al.,\n2020) and XSumfaith (Maynez et al., 2020). It stan-\ndardized these datasets by changing their original\nlabels into a binary label and split each dataset into\na validation set and a test set. Most of the original\ndatasets are labeled by three or more annotators,\nexcept Polytope and FactCC.\nEvaluation Metric: Balanced accuracy (Broder-\nsen et al., 2010) is adopted as the primary evalua-\ntion metric, which requires binary labels for com-\nputation. For approaches with continuous scores, a\nthreshold can be selected via the validation set.\nBaselines: We borrowed the baselines from\nLaban et al. (2022)’s work, including linguistic\nfeature-based metrics NER-Overlap (Laban et al.,\n2021) and DAE (Goyal and Durrett, 2020), NLI-\nbased metric MNLI-doc (Kry´sci´nski et al., 2020)\nand SUMMACZS (Laban et al., 2022), QA-based\nmetrics FEQA (Durmus et al., 2020) and QuestE-\nval (Scialom et al., 2021), prompting with Chat-\nGPT (OpenAI, 2022) 3, and two weakly-supervised\nbaselines FactCC-CLS (Kry ´sci´nski et al., 2020)\n3As Chen et al. (2023) shows that faithfulness evaluation\nis less reasoning-intensive and chain-of-though (Wei et al.,\n2023) prompting even hurts performances, we only compared\nwith ChatGPT using a vanilla prompt.\n11019\nSetting Dataset Val Test Source\nInconsistency\nDetection\n(SUMMAC\nBenchmark)\nCoGenSum 1281 400 C\nSummEval 850 850 C\nFRANK 671 1575 C+X\nPolytope 634 634 C\nFactCC 931 503 C\nXSumFaith 1250 1250 C\nFaithfulness\nRating\nFRANKCNN - 1250 C\nQAGSCNN - 235 C\nSummEval - 1600 C\nFRANKXSUM - 996 X\nQAGSXSUM - 239 X\nTable 1: Statistics of the datasets. “C” and “X”\nare short for CNN/DM (Nallapati et al., 2016) and\nXSum (Narayan et al., 2018) respectively.\nand SUMMACCONV (Laban et al., 2022). Be-\nsides, we implemented the language modeling-\nbased metric BARTScore (Yuan et al., 2021)\nand metrics based on probability changes include\nCoP (She et al., 2023) and HaRiM (Son et al.,\n2022). These three metrics were suggested to use\nthe CNN/DM (Nallapati et al., 2016) ﬁne-tuned\nBART model 4 for calculation. We also improved\nthe latter two metrics with our proposal by calcu-\nlating with a foundation language model, LLaMa,\nfor comparisons.\n3.2 Faithfulness Rating\nFaithfulness rating deﬁnes the evaluation as a Lik-\nert scale coring problem. Annotators or metrics\nscore each summary according to its faithfulness.\nGenerally, the higher, the more faithful.\nDatasets: Following Son et al. (2022), we exper-\nimented on ﬁve different datasets: FRANKCNN\nand FRANKXSUM from Pagnoni et al. (2021),\nQAGSCNN and QAGSXSum from Wang et al.\n(2020), and SummEval (Fabbri et al., 2021). For\nthe ﬁrst four datasets, human judgments were orig-\ninally done on the sentence level. The faithfulness\nrating of the whole summary is collected by doing\nmajority voting on each summary sentence among\nannotators and averaging among sentences. Sum-\nmEval contains human scores in the range of 1 to\n5 in the aspect of consistency. More details are in\nTable 1.\nEvaluation Metrics: Pearson(γ), Spearman(ρ),\nand Kendall(τ) correlation coefﬁcients are used to\nmeasure the alignments between faithfulness rat-\nings annotated by annotators and automatic metrics.\nThe correlations are the higher the better. We con-\nsider the summary-level correlations for all datasets.\nBesides, system-level correlations are calculated\n4https://huggingface.co/facebook/\nbart-large-cnn\non SummEval which contains annotations for 16\nextractive or abstractive summarization models.\nBaselines: Rouge-2 F1 (Lin, 2004), Me-\nteor (Banerjee and Lavie, 2005), BLEU (Papineni\net al., 2002) and BERTScore F1 (Zhang et al.,\n2019a) are widely-accepted summarization eval-\nuation metrics. We report their best results in Son\net al. (2022) by calculating between the summary\nand the source document. QAGS (Wang et al.,\n2020) is another QA-based metric. Others are the\nsame as the ones for inconsistency detection.\n3.3 Implementation Details\nWe implemented FFLM with the foundation lan-\nguage model LLaMa (Touvron et al., 2023). It con-\ntains models with different sizes, where LLaMa7b\nis selected for our main experiments. We add\n\"TL;DR\" between the conditional sequence and\nthe target sequence. The weights in Eq. 7 are de-\ntermined in {0.0,0.1,..., 1.0}according to the per-\nformance on the corresponding validation set for\ninconsistency detection. For faithfulness rating, we\nset α, β, δas 0.25, 0.25, 0.5 respectively, with the\nintuition that the former two are from the same cat-\negory as introduced in Sec. 2.1. Our experiments\nare done on a single RTX 3090.\n4 Results and Analysis\nThis section includes the main results for incon-\nsistency detection and faithfulness rating, together\nwith an ablation study, an analysis of error types,\nand comparisons of different model sizes of our\nFFLM. We also discussed our metric and the\nprompting approach with or without instruction\ntuning under the same model size.\n4.1 Performance on Inconsistency Detection\nThe results on inconsistency detection are in Ta-\nble 2. Our proposed metric FFLM achieves state-\nof-the-art performance on 3 datasets including Co-\nGenSum, SummEval, and FRANK, and outper-\nforms ChatGPT on 5 out of 6 datasets from the\nSUMMAC benchmark except XSumFaith.\nBoth Polytope and FactCC are only labeled by a\nsingle annotator. As a result, their labels may not\nbe as convincing as the other datasets. Although\nQuestEval, the best QA-based metric, achieves the\ntop-1 accuracy on Polytope, it performs mediocrely\non the rest. The weakly-supervised baselines\nFactCCCLS and SummaC Conv are trained with\nsynthetic data constructed with human expertise\n11020\nMetric CoGenSum SummEval FRANK Polytope FactCC XSumFaith\nNER Overlap 53.0 56.8 60.9 52.0 55.0 63.3\nMNLI-doc 57.6 66.6 63.6 61.0 61.3 57.5\nFactCCCLS 63.1 60.1 59.4 61.0 75.9 57.6\nDAE 63.4 70.3 61.7 62.8 75.9 50.8\nFEQA 61.0 53.8 69.9 57.8 53.6 56.0\nQuestEval 62.6 72.5 82.1 70.3 66.6 62.1\nSummaCZS 70.4 78.7 79.0 62.0 83.8 58.4\nSummaCConv 64.7 81.7 81.6 62.7 89.5 66.4\nBARTScore 62.5 66.7 80.2 57.3 68.4 56.9\nCoPBART 65.3 63.9 77.7 60.0 69.0 61.5\nHaRiMBART 58.9 76.6 81.8 55.8 67.3 56.2\nChatGPT 63.3 76.5 80.9 56.9 74.7 64.7\nCoPLLaMa 65.4 83.6 83.1 55.4 78.6 54.1\nHaRiMLLaMa 57.1 80.0 83.4 58.8 69.8 53.4\nFFLM 71.8 84.4 83.9 61.5 77.3 58.9\nTable 2: Balanced accuracy(%) on the SUMMAC benchmark. The best result for each dataset is in bold. Scores\nof FFLM better than other metrics based on the foundation model are underlined.\nFRANKCNN QAGSCNN SummEval FRANKXSUM QAGSXSUM\nMetric γ ρ τ γ ρ τ γ ρ τ γ ρ τ γ ρ τ\nRouge-2 33.1 32.7 24.9 47.5 42.7 31.5 24.7 25.2 19.5 1.2 3.3 2.7 10.7 9.1 6.9\nMeteor 23.0 22.9 17.4 27.7 32.4 23.4 14.3 12.2 11.2 -0.5 0.5 0.4 -1.5 -7.1 -5.2\nBLEU 9.3 20.2 15.3 18.0 33.7 24.5 11.7 7.3 9.1 -4.2 -4.6 -3.8 4.7 -18.6 -13.9\nBERTScore 51.4 46.4 35.8 55.6 49.3 36.5 29.2 29.5 23.0 15.7 13.7 11.1 -4.8 -5.4 -4.0\nFactCCCLS 49.2 43.8 37.6 - - - 32.0 34.0 - 7.2 7.2 7.1 - - -\nFEQA -1.8 -1.0 -0.8 - - - - - - 2.6 0.8 0.6 - - -\nQAGS 31.4 26.7 20.6 46.6 38.2 27.4 17.7 12.7 - -2.2 -0.7 -0.6 21.7 20.3 15.3\nQuestEval - - - 49.2 44.5 - 37.0 33.9 - - - - 7.0 9.6 -\nDAE 44.0 44.7 34.2 - - - 20.0 27.0 - 5.8 11.3 9.2 - - -\nBARTScore 56.1 53.0 41.3 67.3 61.3 47.0 24.9 26.2 19.7 17.4 16.8 13.7 8.0 9.7 7.2\nCoPBART 56.1 51.0 39.4 73.0 65.3 53.2 23.6 22.6 18.0 22.8 20.8 17.0 26.6 25.3 20.7\nHaRiMBART 61.0 53.9 42.1 67.4 58.2 47.1 42.7 37.6 29.8 14.8 13.9 11.4 15.8 16.0 13.1\nChatGPT 50.0 46.0 - - - - 49.0 35.0 - 34.0 27.0 - - - -\nCoPLLaMa 59.7 54.5 42.6 74.3 67.6 54.8 55.1 46.4 37.0 24.6 23.1 18.8 19.0 18.1 14.7\nHaRiMLLaMa 56.9 51.9 40.3 68.6 60.0 48.2 56.1 45.5 36.4 18.6 16.7 13.6 9.1 10.0 8.2\nFFLM 62.2 56.0 43.7 72.3 65.3 53.0 56.3 46.9 37.4 27.0 25.3 20.6 28.3 27.1 22.2\nTable 3: Summary-level correlations(%) on the faithfulness rating datasets.\nthat may have certain similarities with the FactCC\ndataset. Therefore, FactCC CLS shows strong per-\nformance on the FactCC dataset while relatively\nweak on the others including datasets in Table 3,\nthe same as the ﬁndings in Laban et al. (2022).\nAlso, that’s why SummaCConv shows around 12%\nsigniﬁcant improvements over our FFLM.\nConcentrating on the metrics based on proba-\nbility changes, zero-shot metrics CoP BART and\nHaRiMBART perform not badly compared with\nprevious SOTA SummaCZS, showing the poten-\ntial of using probability changes for faithfulness\nevaluation. After introducing the foundation lan-\nguage model, their performances don’t drop in\nmost cases, indicating that ﬁne-tuning with in-\ndomain data is not necessary. However, the leading\nperformance between these two metrics is unsta-\nble among datasets. HaRiM LLaMa outperforms\nCoPLLaMa on FRANK and Polytope, while on the\nrest datasets, the opposite is true. FFLM, as a com-\nprehensive metric, successfully achieves improve-\nments over both of them on 5 out of 6 datasets.\n4.2 Performance on Faithfulness Rating\nSummary-level results are in Table 3. The results\nof ChatGPT borrowed from Luo et al. (2023) show\nits inconsistency improvements among datasets: It\ndoesn’t exceed previous baselines on FRANKCNN,\nperforms similarly on SummEval, and achieves\nconspicuous gains on FRANKXSUM. Besides,\nsimilar to the above analysis for comparisons\namong probability change-based metrics, our\nFFLM induces performance gains on 4 out of 5\ndatasets over CoPLLaMa and HaRiMLLaMa, espe-\ncially on datasets sourced from XSum. Unfor-\ntunately, FFLM still lags behind ChatGPT with\n175 billion parameters on FRANKXSUM, showing\nChatGPT’s strong ability on dealing with highly\nabstractive summaries. This is also in line with\nChatGPT’s favorable performance on XSumFaith\nin Table 2. After all, FFLM achieves the best scores\non FRANKCNN, SummEval, and QAGSXSUM,\nand performs competitively on the other datasets.\nWe also report the system-level results on Sum-\nmEval in Table 5. FFLM performs similarly to\nChatGPT according to the Spearman correlation.\n11021\nFRANKCNN QAGSCNN SummEval FRANKXSUM QAGSXSUM\nMetric γ ρ τ γ ρ τ γ ρ τ γ ρ τ γ ρ τ\nFFLM 62.2 56.0 43.7 72.3 65.3 53.0 56.3 46.9 37.4 27.0 25.3 20.6 28.3 27.1 22.2\nAblations on the metric components\n∆prior\nY 32.0 26.4 20.1 11.9 7.5 5.7 24.9 20.5 16.1 20.0 19.2 15.6 20.9 21.3 17.4\n∆prior\nX 34.4 36.0 27.4 27.7 28.1 21.8 23.7 26.3 20.7 10.2 11.2 9.2 4.8 3.2 2.6\n∆cond\nY 59.9 54.6 42.7 74.3 67.9 55.2 54.8 46.3 36.9 24.7 23.3 19.0 19.4 18.0 14.7\n∆prior\nY , ∆prior\nX 34.7 29.2 22.3 17.7 13.0 9.9 28.3 23.6 18.6 20.4 19.7 16.0 20.7 21.3 17.4\n∆prior\nY , ∆cond\nY 61.0 54.2 42.3 68.1 60.2 48.4 54.4 45.1 36.0 28.3 26.5 21.6 29.4 28.6 23.4\n∆prior\nX , ∆cond\nY 60.3 54.7 42.6 73.9 66.5 54.0 54.7 46.6 37.1 24.7 23.2 18.9 19.8 18.3 14.9\nAblations on the metric designs\n- w/o w 61.2 54.8 42.7 68.4 60.1 48.6 54.3 45.4 36.3 26.9 25.0 20.4 26.4 26.00 21.3\n- w/o log 57.5 52.3 40.5 69.2 60.3 48.5 56.9 45.9 36.7 19.7 17.5 14.3 11.5 12.2 10.0\n- w/o wand log 56.3 51.3 39.6 66.5 57.6 46.4 54.4 45.0 36.0 18.8 16.6 13.5 11.0 12.4 10.2\nAblations on the combination weights (α, β, δ)\nsame 60.9 54.0 42.1 67.5 58.6 47.1 54.1 44.9 35.8 28.3 26.4 21.5 29.2 28.7 23.5\nTable 4: Ablations of FFLM on faithfulness rating. The highest scores are in bold.\nMetric γ ρ τ\nRouge-2 50.0 59.9 68.8\nMeteor 46.7 51.3 62.1\nBLEU 45.0 28.7 62.1\nBERTScore 68.3 68.0 86.8\nBARTScore 30.1 25.9 18.3\nCoPBART 19.9 35.9 25.0\nHaRiMBART 75.9 61.2 45.0\nChatGPT - 83.3 -\nCoPLLaMa 88.3 81.8 63.3\nHaRiMLLaMa 89.9 84.4 66.7\nFFLM 90.4 83.2 65.0\nTable 5: System-level correlations between metrics and\nhuman ratings on the SummEval dataset.\nHaRiMLLaMa achieves a bit higher Spearman and\nKendall correlation than FFLM, while FFLM per-\nforms better on Pearson correlation showing bet-\nter linear correlation with human scores. More-\nover, FFLM is more robust than HaRiMLLaMa on\ndifferent evaluation settings considering the poor\nsummary-level performance of HaRiM LLaMa es-\npecially for FRANKXSUM and QAGSXSUM in\nTable 3. Another observation is that although CoP\nand HaRiM backed on BART perform closely with\nthem backed on LLaMa on the summary-level eval-\nuation, they perform poorly on the system-level\nevaluation. This can be attributed to the fact that\nmetrics based on CNN/DM ﬁne-tuned BART have\ninductive bias (Son et al., 2022). They tend to\nprefer summaries generated by abstractive models,\nwhile extractive models are generally more faithful.\nMeanwhile, metrics based on the foundation lan-\nguage model don’t show this bias, leading to the\nbest results for ranking summarization systems.\nTo recap, our FFLM generalizes well among\ndifferent task settings and different datasets, show-\ning favorable performance over the baselines. It is\nbacked on LLaMa with only 7 billion parameters\nand performs competitively with or even outper-\nforms ChatGPT with 175 billion parameters, which\nis much more efﬁcient for faithfulness evaluation.\n4.3 Ablation Study on Metric Designs\nWe carried out ablation studies of FFLM on faith-\nfulness rating in Table 4. The ablation results on\ninconsistency detection are in Appendix B.\nAblations on the metric components:We test\ndifferent combinations of the three probability\nchanges. The results show that ∆cond\nY is the most\npowerful component of FFLM. Its combination\nwith ∆prior\nY ranks ﬁrst among ablations on both\nFRANKXSUM and QAGSXSUM. Together with\n∆prior\nX , our metric FFLM shows over 5% increases\nin Spearman correlation on QAGSCNN, 1.8% on\nFRANKCNN and SummEval, without much loss\non the other two datasets, records more robust re-\nsults. Moreover, combining different probability\nchanges induces performance gains in most cases,\nreﬂecting the necessity of designing a comprehen-\nsive metric(More in Sec 4.4).\nAblations on the metric designs:We use w\nand log to annotate the token-level weights and the\nlogarithm operation introduced in Sec 2.2. Both\noperations contribute to the ﬁnal FFLM, where log\nis more effective for datasets sourced from XSum\nand wfor the others.\nAblations on the combination weights:For\nthe faithfulness rating task where we empirically\nset the weights α, β and δ as 0.25, 0.25 and 0.5,\nwe compared it with the equaling weights, i.e.,\nα= β = δ= 1\n3 . FFLM performs relatively better.\n4.4 Analysis on Error Types\nBy taking a look at the correlations between pairs\nof the metric components in Figure 6, we can see\nthat the correlations vary among different datasets.\nNone of the pairs show a high degree of correla-\ntion, indicating that these components may capture\n11022\nFRANKCNN QAGSCNN SummEval FRANKXSUM QAGSXSUM\nMetric γ ρ τ γ ρ τ γ ρ τ γ ρ τ γ ρ τ\n∆prior\nY , ∆prior\nX 44.7 46.2 32.0 32.5 35.8 23.8 29.5 34.4 23.7 28.3 31.1 20.9 28.0 24.9 17.0\n∆prior\nY , ∆cond\nY 26.5 19.9 12.9 14.7 2.3 1.2 20.1 15.6 10.1 23.4 20.1 13.4 -6.2 -2.4 -1.5\n∆prior\nX , ∆cond\nY 47.2 49.9 34.6 35.2 41.2 28.0 36.5 40.6 27.9 38.7 38.3 25.9 -0.4 3.8 2.5\nTable 6: Correlations between pairs of the metric components on faithfulness rating.\nΔ!\"#$%#\nΔ&\"#$%#\nΔ!'%()\nΔ!\"#$%#\nΔ&\"#$%#\nΔ!'%()\n(a) FRANKCNN\nΔ!\"#$%#\nΔ&\"#$%#\nΔ!'%()\nΔ!\"#$%#\nΔ&\"#$%#\nΔ!'%() (b) FRANKXSUM\nFigure 1: Spearman correlation(%) of different error\ntypes on FRANKCNN and FRANKXSUM. Highest\ncorrelations for each ∆ is highlighted with red boxes.\nunfaithfulness from different aspects.\nTo ﬁgure out if different probability changes cor-\nrelate well with different error types in the gen-\nerated summaries, we take advantage of labels\nin the FRANKCNN and FRANKXSUM datasets.\nPagnoni et al. (2021) divides the factual errors in\nthe generated summaries into three groups. Seman-\ntic frame errors(Sem) include errors on the predi-\ncate, entities, and additional information about the\ncircumstance. Discourse errors( Disc) consist of\ncoreference errors and discourse link errors. Con-\ntent veriﬁability errors( CVer) are closely related\nto extrinsic hallucinations (Ji et al., 2023), contain-\ning the out-of-article error and grammatical error.\nWe randomly picked 50 error cases and 10 error\ncases for each error type from FRANKCNN and\nFRANKXSUM respectively, and mixed them with\nthe rest faithful summaries. Spearman correlations\naveraged over 10 times are in Fig. 1.\nWe observed that∆cond\nY captures different errors\nbest, which is accord with the ablation results in Ta-\nble 4. Comparing among the scores for each ∆ hor-\nizontally, we can see that the probability changes\nwith prior probability is good at CVer errors on both\ndatasets, and ∆cond\nY at Sem errors or Disc errors.\nThe differences among datasets reﬂect their dif-\nferent characteristics (Pagnoni et al., 2021). Sum-\nmaries in FRANKCNN are made up of multiple\nsentences, resulting in more diverse and challeng-\ning situations for Disc errors than FRANKXSUM\nwith single-sentence summaries. Thus, ∆cond\nY in-\ncreases dramatically from 14.2% on FRANKCNN\n54555657\n3B7B13B\nFRANKCNN\n63646566\n3B7B13B\nQAGSCNN\n45464748\n3B7B13B\nSummEval\n23242526\n3B7B13B\nFRANKXSUM\n22242628\n3B7B13B\nQAGSXSUM\nFigure 2: Spearman correlation(%) of FFLM with dif-\nferent model sizes on faithfulness rating datasets.\nto 41.7% on FRANKXSUM for Disc.\nFFLM made further improvements over ∆cond\nY\non both Sem and CVer, showing that combining\ndifferent probability changes is reasonable and ef-\nfective in most cases except Discs.\n4.5 Performance on Different Model Sizes\nTo test FFLM’s performance on different models\nsizes, we select LLaMa with 3 billion(3B), 7 bil-\nlion(7B) and 13 billion(13B) parameters 5 that are\ntrained on the same data volume with 1 trillion\ntokens and draw the diagram in Fig. 2 for faith-\nfulness rating datasets. The scores consistently\nincrease from LLaMa-3B to LLaMa-7B across the\nﬁve datasets, while the improvements are not con-\nsistent for LLaMa-13B. Given a certain amount\nof data, increasing the number of parameters can\nenhance the model’s language modeling ability and\nbe helpful to faithfulness evaluation. On the other\nhand, when the model size keeps scaling up, more\nunexpected biases in the pre-training corpus may\nbe memorized and will hurt the performance. This\nhas also been pointed out by Ranaldi et al. (2023)\nand Nadeem et al. (2021).\nIn this way, we think that using larger foundation\nmodels may not be the best choice for faithfulness\nevaluation on summarization, which is also closely\nrelated to the research on investigating the optimal\n5The corresponding checkpoints from hugging face are\nopenlm-research/open_llama_3b, decapoda-research/llama-\n7b-hf, and decapoda-research/llama-13b-hf.\n11023\nFRANKCNN QAGSCNN SummEval FRANKXSUM QAGSXSUM\nMetric γ ρ τ γ ρ τ γ ρ τ γ ρ τ γ ρ τ\nPrompting Approach\nLLaMa-7B -1.3 -0.2 -0.2 2.4 2.0 1.9 9.5 11.5 10.9 3.0 3.0 3.0 -6.9 -7.0 -6.9\nVicuna-7B 17.6 17.4 15.5 18.9 19.8 17.7 13.1 11.9 11.1 7.0 5.5 5.1 10.2 8.4 8.0\nAlpaca-7B 5.4 6.8 6.2 10.2 9.3 8.7 3.0 6.5 5.6 3.8 3.4 3.4 2.3 1.4 1.4\nOur Approach\nLLaMa-7B 62.2 56.0 43.7 72.3 65.3 53.0 56.3 46.9 37.4 27.0 25.3 20.6 28.3 27.1 22.2\nVicuna-7B 62.7 56.7 44.3 73.1 67.2 54.6 55.3 47.2 37.7 25.8 23.9 19.4 23.5 22.5 12.8\nAlpaca-7B 61.4 55.3 43.2 71.4 66.0 52.6 55.8 47.1 37.6 26.2 24.6 20.0 24.2 25.4 20.7\nTable 7: Comparisons with prompting and instruction-tuning techniques under the same model size. The highest\ncorrelations are in bold in each column and are underlined among each kind of approach.\nmodel size and dataset size for training foundation\nlanguage models (Hoffmann et al., 2022).\n4.6 Comparisons with Prompting and\nInstruction-tuning\nWe compare our metric with prompting and\ninstruction-tuning techniques under the same\nmodel size in Table 7 for faithfulness rating. Here,\nLLaMa-7B is the vanilla foundation language\nmodel. Vicuna-7B (Chiang et al., 2023) and\nAlpaca-7B (Taori et al., 2023) are initialized from\nLLaMa-7B and instruction-tuned with data col-\nlected in different ways. We present the maximum\nscores for each dataset among different prompts de-\nsigned by previous works (Chen et al., 2023; Gao\net al., 2023; Luo et al., 2023). The detailed prompts\nfor each evaluation task are listed in Appendix C.\nFirst, we observe that using models containing 7\nbillion parameters, FFLM outperforms the prompt-\ning approach across different models and datasets.\nThe prompting results here lag behind the perfor-\nmance of ChatGPT dramatically. This leads to\nthe conclusion that the effectiveness of prompting\napproaches relies highly on much larger models,\nwhile our metric FFLM can be a cheaper alternative\nwith smaller models. Second, instruction tuning is\nimportant for improving the prompting approach,\nwhile is not necessary for our FFLM. It enhances\nthe models’ understanding ability on instruction\ntemplates in the prompts by further tuning with\nrelatively small datasets. However, such manually\ncollected datasets may contain unconscious bias\nand hurt FFLM’s performance.\n5 Related Work\n5.1 Faithfulness Evaluation for\nSummarization\nFaithfulness evaluation metrics can be classiﬁed\ninto zero-shot ones and weakly-supervised ones.\nZero-shot evaluation metrics mainly take advan-\ntage of the models trained with related natural lan-\nguage tasks. Goodrich et al. (2019) adopted in-\nformation extraction tools to extract the fact tu-\nples from both the source document and the sum-\nmary. Tuple mismatches reﬂect the hallucina-\ntions. The intuition behind question-answering-\nbased metrics (Wang et al., 2020; Durmus et al.,\n2020; Scialom et al., 2021) is that identical an-\nswers should be generated when asking the same\nquestion to a summary and the corresponding doc-\nument respectively. Natural language inference\nalso shares commonalities with faithfulness eval-\nuation in the way that information in a consistent\nsummary should be entirely entailed by the source\ndocument (Falke et al., 2019; Mishra et al., 2021;\nLaban et al., 2022). However, all of these metrics\nhighly rely on the domain-transfer ability of out-of-\nbox models and suffer from error propagation.\nInstead, weakly-supervised approaches choose\nto train classiﬁers by constructing synthetic in-\ndomain data with heuristics by experts. Differ-\nent kinds of inconsistency errors are simulated\nby perturbing the reference document-summary\npairs (Kry´sci´nski et al., 2020; Utama et al., 2022;\nYin et al., 2021). The limited heuristic makes it\nhard to cover all kinds of errors and shows poor\ngeneralization ability among datasets (Laban et al.,\n2022).\nAs language modeling-based metrics (Egan\net al., 2022; Liu et al., 2022b) receive more atten-\ntion, another small group of work for faithfulness\nevaluation computes probability changes with mod-\nels ﬁne-tuned on summarization datasets (She et al.,\n2023; Son et al., 2022; Xie et al., 2021), showing a\nbiased preference for abstractive summaries. Based\non this line of work, we propose FFLM based on\nthe foundation language model. Our zero-shot met-\nric doesn’t require further training with in-domain\nor synthetic data and shows a strong generalization\nability.\n11024\n5.2 Evaluation with Large Language Models\nWith orders of magnitude more parameters and ex-\ntensive training on large-scale data, large language\nmodels (LLMs) (Brown et al., 2020; Touvron et al.,\n2023) have exhibited surprising abilities that may\nnot be observed in previous small language models.\nThe strong capability in language comprehension\nnaturally spurs research in exploring LLMs as bet-\nter automatic evaluators for various text generation\nsystems (Wang et al., 2023).\nThere are also some attempts of faithfulness\nevaluation by prompting large models (Luo et al.,\n2023) with different templates and strategies, such\nas adding detailed deﬁnitions (Gao et al., 2023)\nand chain-of-thought (Chen et al., 2023). None of\nthese strategies achieve consistent improvements\nover the original prompt. Besides, neural models\nare sensitive to the choices of words (Chen et al.,\n2023), resulting in unstable performances(See Ap-\npendix D).\nOur FFLM takes advantage of the strong capa-\nbility of LLMs for faithfulness evaluation in a dif-\nferent way and shows competitive performance re-\nquiring a much smaller number of parameters than\nthe well-known ChatGPT (OpenAI, 2022).\n6 Conclusion\nThis paper focuses on zero-shot faithfulness eval-\nuation for summarization and introduces a novel\nevaluation metric FFLM which is simply based on\nthe foundation language model. Experiments on\nboth the inconsistency detection benchmark and\nfaithfulness rating datasets show the strong general-\nization ability of FFLM across various task settings\nand different datasets. It also shows favorable per-\nformance over strong baselines including ChatGPT.\nUsing our proposed metric for more ﬁne-grained\nconsistency detection and designing more faithful\nsummarization systems are future directions.\nLimitations\nThe main idea of this work is to do faithfulness\nevaluation based on a foundation language model\nby a combination of different probability changes.\nFFLM is just a feasible but not perfect metric de-\nsign. Although it makes improvements over each\n∆ on almost all of the datasets in Table 4, it failed\non the errors related to discourse errors on the\nFRANKCNN and FRANKXSUM dataset accord-\ning to Fig. 1. Designing better aggregation metrics\nbased on speciﬁc analysis of different error types\nwill be considered in the future.\nBesides, in this work, our FFLM only calculates\na single score for the whole summary without pin-\npointing the exact erroneous words or the speciﬁc\nerror type. Considering the success of CoP (She\net al., 2023) on token-level inconsistency detec-\ntion and detailed inconsistency category evaluation,\nwe hypothesize that our metric FFLM can be also\nused for these evaluation scenarios by adjusting\nthe aggregation weights or combining it with the\nprompting approach.\nMoreover, we limit our scope to faithfulness eval-\nuation for text summarization in this paper because\nthe deﬁnition of faithfulness evaluation for other\ngeneration tasks has some non-trivial differences.\nFor example, the chit-chat utterances in dialogue\ngeneration (Dziri et al., 2022) are supposed to be\nacceptable under the evaluation for faithfulness, in-\nstead of being regarded as extrinsic hallucinations.\nThe evaluation for sentence paraphrasing (Zhang\net al., 2019b) should be bi-directional, i.e., the ﬁrst\nsentence has to be consistent with the second one,\nand vice versa. We consider transferring FFLM\nwith adjustment on the other tasks as future work.\nReferences\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings\nof the acl workshop on intrinsic and extrinsic evalu-\nation measures for machine translation and/or sum-\nmarization, pages 65–72.\nKay Henning Brodersen, Cheng Soon Ong, Klaas Enno\nStephan, and Joachim M Buhmann. 2010. The bal-\nanced accuracy and its posterior distribution. In\n2010 20th international conference on pattern recog-\nnition, pages 3121–3124. IEEE.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nSubhajit Chaudhury, Sarathkrishna Swaminathan, Chu-\nlaka Gunasekara, Maxwell Crouse, Srinivas Rav-\n11025\nishankar, Daiki Kimura, Keerthiram Murugesan,\nRamón Fernandez Astudillo, Tahira Naseem, Pavan\nKapanipathi, et al. 2022. X-factor: A cross-metric\nevaluation of factual correctness in abstractive sum-\nmarization. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7100–7110.\nShiqi Chen, Siyang Gao, and Junxian He. 2023. Eval-\nuating factual consistency of summaries with large\nlanguage models. arXiv preprint arXiv:2305.14069.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nEsin Durmus, He He, and Mona Diab. 2020. Feqa: A\nquestion answering evaluation framework for faith-\nfulness assessment in abstractive summarization. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5055–\n5070.\nNouha Dziri, Ehsan Kamalloo, Sivan Milton, Os-\nmar Zaiane, Mo Yu, Edoardo M Ponti, and Siva\nReddy. 2022. Faithdial: A faithful benchmark\nfor information-seeking dialogue. Transactions\nof the Association for Computational Linguistics ,\n10:1473–1490.\nNicholas Egan, Oleg Vasilyev, and John Bohannon.\n2022. Play the shannon game with language mod-\nels: A human-free approach to summary evaluation.\nIn Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 36, pages 10599–10607.\nAlexander Richard Fabbri, Wojciech Kry ´sci´nski,\nBryan McCann, Caiming Xiong, Richard Socher,\nand Dragomir Radev. 2021. Summeval: Re-\nevaluating summarization evaluation. Transactions\nof the Association for Computational Linguistics ,\n9:391–409.\nTobias Falke, Leonardo FR Ribeiro, Prasetya Ajie\nUtama, Ido Dagan, and Iryna Gurevych. 2019.\nRanking generated summaries by correctness: An in-\nteresting but challenging application for natural lan-\nguage inference. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2214–2220.\nMingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin,\nShiping Yang, and Xiaojun Wan. 2023. Human-\nlike summarization evaluation with chatgpt. arXiv\npreprint arXiv:2304.02554.\nBen Goodrich, Vinay Rao, Peter J Liu, and Moham-\nmad Saleh. 2019. Assessing the factual accuracy\nof generated text. In Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge\nDiscovery & Data Mining, pages 166–175.\nTanya Goyal and Greg Durrett. 2020. Evaluating fac-\ntuality in generation with dependency-level entail-\nment. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 3592–3603.\nTanya Goyal, Jiacheng Xu, Junyi Jessy Li, and Greg\nDurrett. 2022. Training dynamics for text summa-\nrization models. In Findings of the Association for\nComputational Linguistics: ACL 2022, pages 2061–\n2073.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, et al. 2022. An empirical analy-\nsis of compute-optimal large language model train-\ning. Advances in Neural Information Processing\nSystems, 35:30016–30030.\nDandan Huang, Leyang Cui, Sen Yang, Guangsheng\nBao, Kun Wang, Jun Xie, and Yue Zhang. 2020.\nWhat have we achieved on text summarization? In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 446–469.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of hallu-\ncination in natural language generation. ACM Com-\nputing Surveys, 55(12):1–38.\nDaniel Kang and Tatsunori B Hashimoto. 2020. Im-\nproved natural language generation via loss trun-\ncation. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 718–731.\nWojciech Kry´sci´nski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332–9346.\nPhilippe Laban, Tobias Schnabel, Paul Bennett, and\nMarti A Hearst. 2021. Keep it simple: Unsupervised\nsimpliﬁcation of multi-paragraph text. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 6365–6378.\nPhilippe Laban, Tobias Schnabel, Paul N Bennett, and\nMarti A Hearst. 2022. Summac: Re-visiting nli-\nbased models for inconsistency detection in summa-\nrization. Transactions of the Association for Compu-\ntational Linguistics, 10:163–177.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\n11026\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nYizhu Liu, Qi Jia, and Kenny Q. Zhu. 2021. Keyword-\naware abstractive summarization by extracting set-\nlevel intermediate summaries. In WWW ’21: The\nWeb Conference 2021, Virtual Event / Ljubljana,\nSlovenia, April 19-23, 2021 , pages 3042–3054.\nACM / IW3C2.\nYizhu Liu, Qi Jia, and Kenny Q. Zhu. 2022a. Opin-\nion summarization by weak-supervision from mix-\nstructured data. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022 , pages 3086–3096.\nAssociation for Computational Linguistics.\nYizhu Liu, Qi Jia, and Kenny Q. Zhu. 2022b.\nReference-free summarization evaluation via seman-\ntic correlation and compression ratio. In Proceed-\nings of the 2022 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, NAACL\n2022, Seattle, WA, United States, July 10-15, 2022 ,\npages 2109–2115. Association for Computational\nLinguistics.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023. Chatgpt as a factual inconsistency evaluator\nfor abstractive text summarization. arXiv preprint\narXiv:2303.15621.\nKazuki Matsumaru, Sho Takase, and Naoaki Okazaki.\n2020. Improving truthfulness of headline genera-\ntion. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 1335–1346.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919.\nMengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua\nZhou, and Jie Zhou. 2021. Prevent the language\nmodel from being overconﬁdent in neural machine\ntranslation. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 3456–3468.\nAnshuman Mishra, Dhruvesh Patel, Aparna Vijayaku-\nmar, Xiang Lorraine Li, Pavan Kapanipathi, and Kar-\ntik Talamadupula. 2021. Looking beyond sentence-\nlevel natural language inference for question answer-\ning and text summarization. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1322–1336.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoset: Measuring stereotypical bias in pre-\ntrained language models. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 5356–5371.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gulçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nrnns and beyond. In Proceedings of the 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290.\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807.\nOpenAI. 2022. Online chatgpt: Optimizing language\nmodels for dialogue. OpenAI Blog.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with frank: A benchmark for fac-\ntuality metrics. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4812–4829.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Compu-\ntational Linguistics, pages 311–318.\nLeonardo Ranaldi, Elena Soﬁa Ruzzetti, Davide Ven-\nditti, Dario Onorati, and Fabio Massimo Zanzotto.\n2023. A trip towards fairness: Bias and de-\nbiasing in large language models. arXiv preprint\narXiv:2305.13862.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, Jacopo Staiano, Alex Wang,\nand Patrick Gallinari. 2021. Questeval: Summariza-\ntion asks for fact-based evaluation. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 6594–6604.\nShuaijie She, Xiang Geng, Shujian Huang, and Jiajun\nChen. 2023. Cop: Factual inconsistency detection\nby controlling the preference. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence.\nSeonil Simon Son, Junsoo Park, Jeong-in Hwang,\nJunghwa Lee, Hyungjong Noh, and Yeonsoo Lee.\n2022. Harim+: Evaluating summary quality with\nhallucination risk: Evaluating summary quality with\nhallucination risk. In Proceedings of the 2nd Confer-\nence of the Asia-Paciﬁc Chapter of the Association\n11027\nfor Computational Linguistics and the 12th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 895–924.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023. Alpaca:\nA strong, replicable instruction-following model.\nStanford Center for Research on Foundation Models.\nhttps://crfm. stanford. edu/2023/03/13/alpaca. html ,\n3(6):7.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman , Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efﬁ-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nPrasetya Utama, Joshua Bambrick, Naﬁse Sadat\nMoosavi, and Iryna Gurevych. 2022. Falsesum:\nGenerating document-level nli examples for recog-\nnizing factual inconsistency in summarization. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 2763–2776.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5008–5020.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou,\net al. 2023. Chain-of-thought prompting elicits rea-\nsoning in large language models. In Advances in\nNeural Information Processing Systems.\nYuexiang Xie, Fei Sun, Yang Deng, Yaliang Li, and\nBolin Ding. 2021. Factual consistency evaluation\nfor text summarization via counterfactual estimation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 100–110.\nWenpeng Yin, Dragomir Radev, and Caiming Xiong.\n2021. Docnli: A large-scale dataset for document-\nlevel natural language inference. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 4913–4922.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text gener-\nation. Advances in Neural Information Processing\nSystems, 34:27263–27277.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter Liu. 2020. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. In In-\nternational Conference on Machine Learning, pages\n11328–11339. PMLR.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019a. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy\nLiang, Kathleen McKeown, and Tatsunori B\nHashimoto. 2023. Benchmarking large language\nmodels for news summarization. arXiv preprint\narXiv:2301.13848.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019b.\nPaws: Paraphrase adversaries from word scrambling.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) , pages 1298–\n1308.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, et al. 2023. Lima: Less is more for align-\nment. arXiv preprint arXiv:2305.11206.\n11028\nA Preliminaries\nCoP proposed by (She et al., 2023) is calculated\nbased on ∆cond\npY . Although they only showed∆cond\npY\nin their paper, they took the logarithm of probabil-\nities during implementation without analysis and\nexplanations. Overall, CoP is:\nCoP = 1\nm\n∑m\ni=1\nlog ps2s\nyi −log ppref\nyi (8)\nHaRiM proposed by (Son et al., 2022) is derived\nfrom ∆prior\npY :\nHaRiM = 1\nm\n∑m\ni=1\n(1−ps2s\nyi )[1−(ps2s\nyi −pprior\nyi )]\n(9)\nOur FFLM is different from HaRiM in three ways:\nFirst, we propose to do faithfulness evaluation with\nfoundation language models while they suggested\nto evaluate with models ﬁne-tuned on summariza-\ntion data. Second, HaRiM only considers one of\nthe changes with prior probability, i.e.,∆prior\npY . Our\nmetric not only adds a back constraint of P(X|Y)\nadditionally to HaRiM, but also considers the prob-\nability changes with conditional probability. Third,\nSpeciﬁcally for the formula, the intuition of adding\nweights behind HaRiM and our FFLM are dif-\nferent regardless of the function-form variations.\nHaRiM’s weight (1 −ps2s\nyi ) was originally intro-\nduced to adjust the loss scale for training better\nneural machine translation models by Miao et al.\n(2021). FFLM adopted the weight eps2s\nyi and the\nlogarithm to pay more attention to low-probability\ntokens which generally correspond to unfaithful\ncontents according to (Goyal et al., 2022).\nXie et al. (2021) introduced another similar\nmetric CoCo which is also based on probability\nchanges. Instead of using the prior probability of\nY in ∆prior\npY , they condition Y on the masked doc-\nument X′by removing Y-related information. The\nmasking strategy is hard to design since locating\nthe Y-related information is uneasy. Their compli-\ncated operation also largely slows down the infer-\nence speed.\nWe compared these three metrics backed on\nBART for faithfulness ranting in Table 8. Since\nCoCo’s performances vary a lot with different\nmasking strategies and none of them completely\noutperform the other two metrics on a single\ndataset, we didn’t compare with it thoroughly in\nour work.\nB Analysis on Inconsistency Detection\nAblation studies of FFLM for inconsistency detec-\ntion are in Table 9. Fig. 3 illustrates the perfor-\nmances of FFLM on variable sizes of the founda-\ntion language model. Results with the prompting\napproach and comparison to the instruction-tuning\ntechnique are in Table 10.\n63666972\n3B7B13B\nCoGenSum\n75798387\n3B7B13B\nSummEval\n82838485\n3B7B13B\nFRANK\n60616263\n3B7B13B\nPolytope\n75777981\n3B7B13B\nFactCC\n56586062\n3B7B13B\nXSumFaith\nFigure 3: Spearman correlation(%) of FFLM with dif-\nferent model sizes on inconsistency detection datasets.\nThe observations for inconsistency detection are\nin line with those for faithfulness ranting. An-\nother ﬁnding is that the gap between prompting\napproaches and FFLM under this setup is much\nsmaller than that under the faithfulness rating setup.\nThis indicates that inconsistency detection is easier\nthan faithfulness rating with less number of an-\nswer choices when doing faithfulness evaluation\nby prompting large models.\nC A Collection of Prompts\nThe prompts we collected are in Table 11. We\nmade tiny changes to some of the prompts to en-\nhance their probability of generating an acceptable\nanswer, such as adding a hint like \"Marks:\" and\nmoving the answer choices to the end of the prompt.\nWe recognize the faithfulness score by analyzing\nthe generations with simple rules. If there isn’t\nan acceptable answer, we regard the summary as\nunfaithful, i.e., “No” for inconsistency detection\nand “1” for faithfulness rating.\nD Performance of Different Prompts\nWe show the performances of different prompts\nwith Vicuna-7B for inconsistency detection in Ta-\nble 12 and for faithfulness rating in Table 13. The\nperformance with different prompts for the same\ntask varies a lot. And some prompts used in previ-\nous works with ChatGPT failed with smaller mod-\nels, showing the high-level language understanding\n11029\nFRANKCNN QAGSCNN SummEval FRANKXSUM QAGSXSUM\nMetric γ ρ τ γ ρ τ γ ρ τ γ ρ τ γ ρ τ\nCoP 56.1 51.0 39.4 73.0 65.3 53.2 23.6 22.6 18.0 22.8 20.8 17.0 26.6 25.3 20.7\nHaRiM 61.0 53.9 42.1 67.4 58.2 47.1 42.7 37.6 29.8 14.8 13.9 11.4 15.8 16.0 13.1\nCoCotoken 55.9 50.1 39.0 64.7 53.1 42.6 41.3 36.8 29.1 8.0 6.8 5.6 21.4 22.8 18.7\nCoCospan 56.9 50.3 39.1 66.4 56.0 45.0 39.5 35.3 27.9 12.7 11.8 9.6 25.3 26.1 21.4\nCoCosent 60.9 54.1 42.2 71.7 62.0 50.2 39.4 35.0 27.6 16.3 15.8 12.9 16.5 14.9 12.2\nCoCodoc 59.9 54.0 42.1 71.6 61.9 50.2 39.1 34.8 27.5 18.5 17.2 14.1 22.1 21.5 17.6\nTable 8: Correlations(%) of comparisons among CoP, HaRiM, and CoCo with BART for faithfulness rating. The\nhighest scores are in bold.\nMetric CoGenSum SummEval FRANK Polytope FactCC XSumFaith\nFFLM 71.8 83.9 84.4 61.5 77.3 58.9\nAblations on the metric components\n∆prior\nY 52.2 64.6 76.2 54.8 55.8 60.5\n∆prior\nX 49.5 67.9 73.4 62.0 55.4 58.6\n∆cond\nY 64.4 82.9 83.1 56.8 79.0 53.5\n∆prior\nY , ∆prior\nX 54.7 65.6 77.4 62.0 55.0 58.9\n∆prior\nY , ∆cond\nY 70.5 83.5 84.4 56.8 77.3 59.8\n∆prior\nX , ∆cond\nY 64.4 83.5 83.6 61.5 78.6 58.6\nAblations on the metric design\n- w/o w 69.5 83.3 83.5 66.7 77.4 57.8\n- w/o log 68.7 78.5 83.5 60.9 74.2 58.1\n- w/o wand log 65.3 80.9 83.1 64.1 75.2 56.6\nTable 9: Ablations of FFLM for inconsistency detection. The highest scores are in bold.\nMetric CoGenSum SummEval FRANK Polytope FactCC XSumFaith\nPrompting Approach\nLLaMa-7b 54.3 50.0 53.6 53.7 51.7 51.7\nVicuna-7b 56.9 58.1 69.2 54.6 69.0 55.5\nAlpaca-7b 57.8 50.0 57.5 52.6 58.8 51.1\nOur Approach\nLLaMa-7b 71.8 83.9 84.4 61.5 77.3 58.9\nVicuna-7b 68.6 83.2 83.8 58.3 77.2 58.9\nAlpaca-7b 65.2 85.0 83.9 59.1 78.5 60.7\nTable 10: Comparisons with prompting and instruction-tuning techniques under the same model size for inconsis-\ntency detection. The highest scores are in bold in each column and are underlined among each kind of approach.\nand generation ability requirements for prompting\nlarge language models.\n11030\nCitation Prompt\nInconsistency Detection\nChen et al. (2023)\n{Document}\nQ: Can the following statement be inferred from\nthe above document? Yes or No?\n{Summary}\nA:\nGao et al. (2023)\nIs the sentence supported by the article?\nArticle: {Document}\nSentence: {Summary}\nAnswer \"Yes\" or \"No\":\nLuo et al. (2023)\nDecide if the following summary is consistent with\nthe corresponding article.\nArticle: {Document}\nSummary: {Summary}\nAnswer (yes or no):\nFaithfulness Rating\nGao et al. (2023)\nEvaluate the quality of summaries written for a\nnews article. Rate each summary on consistency.\nYou should rate on a scale from 1 (worst) to 5\n(best).\nArticle: {Document}\nSummary: {Summary}\nMarks:\nLuo et al. (2023)\nScore the following summary given the\ncorresponding article with respect to consistency\nfrom 1 to 10. Note that consistency measures how\nmuch information included in the summary is\npresent in the source article. 10 points indicate the\nsummary contains only statements that are entailed\nby the source document.\nSummary: {Summary}\nSource Article: {Document}\nMarks:\nTable 11: A collection of prompts.“{}” marks place-\nholders for corresponding contents.\nDataset Chen et al. (2023) Gao et al. (2023) Luo et al. (2023)\nCoGenSum 56.9 49.8 49.4\nSummEval 58.1 50.2 54.5\nFRANK 69.2 46.9 57.8\nPolytope 54.6 51.0 52.8\nFactCC 69.0 51.7 53.8\nXSumFaith 52.2 49.6 55.5\nTable 12: Balanced accuracy(%) of Vicuna-7B with dif-\nferent prompts for inconsistency detection. The highest\naccuracy on each dataset is in bold.\nDataset Gao et al. (2023) Luo et al. (2023)\nFRANKCNN 17.4 -5.0\nQAGSCNN 19.8 4.6\nSummEval 11.9 -0.9\nFRANKXSUM 5.5 -3.6\nQAGSXSUM 8.4 -3.3\nTable 13: Spearman correlation(%) of Vicuna-7B with\ndifferent prompts for faithfulness rating. The highest\ncorrelation on each dataset is in bold.\n11031",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9201139211654663
    },
    {
      "name": "Computer science",
      "score": 0.7015329599380493
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5732699632644653
    },
    {
      "name": "Natural language processing",
      "score": 0.5398665070533752
    },
    {
      "name": "Language model",
      "score": 0.5304070711135864
    },
    {
      "name": "Intuition",
      "score": 0.5264918208122253
    },
    {
      "name": "Metric (unit)",
      "score": 0.5083507895469666
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.4799671173095703
    },
    {
      "name": "Random forest",
      "score": 0.4197629392147064
    },
    {
      "name": "Machine learning",
      "score": 0.3970705270767212
    },
    {
      "name": "Linguistics",
      "score": 0.09834864735603333
    },
    {
      "name": "Engineering",
      "score": 0.07458183169364929
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}