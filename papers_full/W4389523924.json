{
  "title": "CoLLiE: Collaborative Training of Large Language Models in an Efficient Way",
  "url": "https://openalex.org/W4389523924",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2005715589",
      "name": "Kai Lv",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2103570438",
      "name": "Shuo Zhang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2332612249",
      "name": "Tianle Gu",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A5108902766",
      "name": "Shuhao Xing",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2222438657",
      "name": "Jia-Wei Hong",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2112269733",
      "name": "Keyu Chen",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2096084573",
      "name": "Xiaoran Liu",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2094451174",
      "name": "Yuqing Yang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2109725212",
      "name": "Honglin Guo",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2104342925",
      "name": "Tengxiao Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097179453",
      "name": "Yu Sun",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2115738570",
      "name": "Qipeng Guo",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2116385994",
      "name": "Hang Yan",
      "affiliations": [
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2115470192",
      "name": "Xipeng Qiu",
      "affiliations": [
        "Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281758439",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4378468563",
    "https://openalex.org/W4292105403",
    "https://openalex.org/W4386709668",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W4381250586",
    "https://openalex.org/W2969388332",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4384648639",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4378509498",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4381714872",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W2991040477",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W4321011699",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4380136143",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4366456545",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4385571157",
    "https://openalex.org/W4295312788"
  ],
  "abstract": "Kai Lv, Shuo Zhang, Tianle Gu, Shuhao Xing, Jiawei Hong, Keyu Chen, Xiaoran Liu, Yuqing Yang, Honglin Guo, Tengxiao Liu, Yu Sun, Qipeng Guo, Hang Yan, Xipeng Qiu. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 527–542\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCoLLiE: Collaborative Training of Large Language Models\nin an Efficient Way\nKai Lv1∗†, Shuo Zhang1∗†, Tianle Gu2, Shuhao Xing1, Jiawei Hong1, Keyu Chen1\nXiaoran Liu1, Yuqing Yang1, Honglin Guo1, Tengxiao Liu1, Yu Sun1\nQipeng Guo1, Hang Yan2‡, Xipeng Qiu1‡\n1 School of Computer Science, Fudan University, 2 Shanghai AI Laboratory\n1{klv21, szhang22, hongjw21, kychen22, liuxr22}@m.fudan.edu.cn\n1{shxing, hlguo20, qpguo16, xpqiu}@fudan.edu.cn, 2{gutianle, yanhang}@pjlab.org.cn\nAbstract\nLarge language models (LLMs) are increas-\ningly pivotal in a wide range of natural lan-\nguage processing tasks. Access to pre-trained\nmodels, courtesy of the open-source commu-\nnity, has made it possible to adapt these mod-\nels to specific applications for enhanced per-\nformance. However, the substantial resources\nrequired for training these models necessitate\nefficient solutions. This paper introduces CoL-\nLiE, an efficient library that facilitates collab-\norative training of large language models us-\ning 3D parallelism, parameter-efficient fine-\ntuning (PEFT) methods, and optimizers such as\nLion, Adan, Sophia, and LOMO. With its mod-\nular design and comprehensive functionality,\nCoLLiE offers a balanced blend of efficiency,\nease of use, and customization. CoLLiE has\nproven superior training efficiency in compari-\nson with prevalent solutions in pre-training and\nfine-tuning scenarios. Furthermore, we provide\nan empirical evaluation of the correlation be-\ntween model size and GPU memory consump-\ntion under different optimization methods, as\nwell as an analysis of the throughput. Lastly,\nwe carry out a comprehensive comparison of\nvarious optimizers and PEFT methods within\nthe instruction-tuning context. CoLLiE is avail-\nable at https://github.com/OpenLMLab/collie.\n1 Introduction\nLarge language models (LLMs) have demonstrated\nremarkable abilities across various natural lan-\nguage processing tasks and showcased potential as\nintelligent assistants. Thanks to the vibrant open-\nsource community, multiple excellent large lan-\nguage models’ weights are accessible, including\nOPT (Zhang et al., 2022), BLOOM (Scao et al.,\n2022), LLaMA (Touvron et al., 2023), etc. Despite\nthe impressive general capabilities of pre-trained\nLLMs, training for particular application scenarios\n∗Equal contribution.\n†Work done during internship at Shanghai AI Lab.\n‡Corresponding authors.\nChinese LLMs\nMedical LLMs\nFinancial LLMs\n···\nPre-training\nFurther Instruction\nTuning\nPre-trained LLMs\nLLaMA\nGLM\nInternLM\n···\nAI Chatbot\nAI Doctor\nAI Analyst\n···\nFigure 1: The two stages of training pre-trained lan-\nguage models, during which CoLLiE exhibits efficiency.\ncan lead to even more outstanding performance.\nAs shown in Figure 1, the training process can be\ndivided into two stages: 1. Further pre-training,\nwhich supplements specific domain knowledge and\nexpands the vocabulary to enhance tokenization\nefficiency; 2. Instruction-tuning, which adapts\nthe model to downstream tasks and improves its\ninstruction-following ability.\nWith the scaling of language models, the re-\nsources required for training have increased sub-\nstantially, making it infeasible to train the entire\nmodel on a single GPU. Model parallelism ad-\ndresses this issue by partitioning the model across\ndifferent GPUs, distributing the training work-\nload among these GPUs. This can be achieved\nthrough three methods: tensor parallelism (TP,\nShoeybi et al. (2019)), pipeline parallelism (PP,\nHuang et al. (2019); Narayanan et al. (2019)), and\nstage 3 of zero redundancy optimizer (ZeRO-3,\nRajbhandari et al. (2020)). In addition, during\nthe instruction-tuning stage, there are resource-\nefficiency and training-effectiveness trade-off ap-\nproaches (Sun et al., 2023b): parameter-efficient\nfine-tuning (PEFT) methods (Ding et al., 2023).\nThese methods selectively choose or add a few pa-\nrameters for training, effectively reducing the GPU\nmemory required to train large language models.\nIn this context, we introduce CoLLiE, an easy-\nto-use library for Collaborative training of Large\nLanguage models in an Efficient way. The library\nnot only integrates the previously mentioned three\nparallelism strategies and PEFT methods, but also\n527\nimplements efficient optimizers such as Lion (Chen\net al., 2023), Adan (Xie et al., 2022), Sophia (Liu\net al., 2023), and LOMO (Lv et al., 2023). We have\nrestructured multiple mainstream open-source mod-\nels to support TP and PP and incorporated FlashAt-\ntention (Dao et al., 2022; Dao, 2023) to further\nboost efficiency, while retaining interfaces similar\nto HuggingFace models within CollieModelclass.\nTraining efficiency is one of the most distinctive\nfeature of CoLLiE, boasting a significantly higher\ntraining throughput compared to current popular\nsolutions. CoLLiE also offers a wide range of func-\ntionalities, including data preprocessing, model\ntraining, checkpoint saving and monitoring and\nevaluation during training process. CoLLiE’s mod-\nular design allows for flexible combinations of par-\nallelism strategies, PEFT methods, and training\nhyperparameters, which can be configured simply\nby modifying the CollieConfig class. Further-\nmore, CoLLiE is purposefully designed with exten-\nsibility, providing customizable functionalities. In\nsummary, CoLLiE offers a comprehensive solution\nthat caters to the needs of both beginners and ex-\nperienced professionals. Our contributions can be\nsummarized as follows:\n• We introduce CoLLiE, an efficient and easy-\nto-use library for collaborative training of\nlarge language models.\n• We empirically provide the relationship be-\ntween model size and the actual GPU mem-\nory consumption using different optimization\nmethods in real training scenarios.\n• We compared the throughput of CoLLiE and\nthe current prevailing solutions in (further)\npre-training and fine-tuning scenarios, and\nCoLLiE demonstrates higher efficiency.\n• We conducted a comprehensive comparison\nof different optimizers and PEFT methods in\nthe context of instruction-tuning.\n2 Background\nPEFT Methods There has been a rise in using\nparameter-efficient fine-tuning (PEFT) techniques\nto adapt models for instruction-tuning by adjust-\ning partial parameters. One of the early success is\nadapter tuning (Houlsby et al., 2019), which inserts\ntrainable neural modules into transformers layers\nwhile keeping the original model unchanged. In\nline with adapter tuning, LoRA (Hu et al., 2022)\nreparameterizes the dense layers and only updates\nlow rank matrices while introducing no latency dur-\ning inference. Prefix-tuning (Li and Liang, 2021)\ntrains a task specific prefix prepended to each layer\nof the transformer encoder and achieves compara-\nble performance with full parameter fine-tuning on\ngenerative tasks. Similarly, prompt-tuning (Lester\net al., 2021) simplifies the additional prefix to the\ninput embeddings, and only updates the parameters\ncorresponding to the prompts.\nWhile the PEFT library (Mangrulkar et al., 2022)\nimplements these algorithms at the model level, it\nrelies on HuggingFace models and lacks a compre-\nhensive functionality, particularly the necessary in-\ntegration with model parallelism to facilitate train-\ning of extremely large models.\nParallelism Strategies Parallelism strategies re-\nfer to the methodology of utilizing multiple GPUs\nto execute training or inference tasks. Data paral-\nlelism involves distributing the input data to differ-\nent GPUs for computation. However, each GPU\nstores an identical copy of the optimizer state and\nmodel weights, which limits the maximum model\nsize that can be trained with data parallelism to that\nof a single GPU. To mitigate this redundancy, Rajb-\nhandari et al. (2020) proposes a parallelism strategy\nin the three stages of ZeRO, evenly partitioning the\noptimizer states, gradients, and weights across dif-\nferent GPUs. Tensor parallelism also partitions\nthe weights evenly, while it varies the approach to\npartition and communicate. Specifically, whereas\nZeRO-3 gathers the weight matrices, tensor paral-\nlelism all reduces the intermediate computational\nresults. Pipeline parallelism partitions the model\nby layers across GPUs, requiring communication\nonly between the layers at the split points. This\nstrategy yields the least communication overhead.\nExisting toolkits, such as HuggingFace’s\nTrainer (Wolf et al., 2020) and LMFlow (Diao et al.,\n2023), choose ZeRO-3 as parallel method. ZeRO-3\nis preferred because it does not impose specific re-\nquirements on the model structure, allowing direct\nusage of HuggingFace models. However, it ex-\nhibits lower throughput compared to the combina-\ntion of TP and PP in scenarios involving large batch\nsize pre-training or constrained communication.\nCoLLiE supports the hybrid application of data\nparallelism, tensor parallelism, and pipeline paral-\nlelism, collectively termed as 3D parallelism, with\nthe parallel sizes adjustable via CollieConfig.\n528\n3 CoLLiE\nIn this section, we will introduce the imple-\nmentation and features of CoLLiE. Figure 2(a)\npresents an overview of the Collie’s overall struc-\nture, centered around the Trainer class. The\nCollieConfig, CollieModel, CollieDataset,\nand Optimizer classes serve as inputs to the\nTrainer. CoLLiE also provides a set of conve-\nnient plugins for theTrainer, including Callback,\nMonitor, Evaluator, and Probe, enabling users\nto customize the training process. Depending\nupon the configurations and selected plugins, the\nTrainer performs the training process, saves\nmodel checkpoints, and records system metrics,\nincluding loss, throughput, and evaluation results.\nAs shown in Figure 2(b), based on Py-\nTorch (Paszke et al., 2019) and DeepSpeed (Rasley\net al., 2020), CoLLiE employs a collaborative ap-\nproach using various techniques to facilitate the\nmore efficient training of large language models.\nSpecifically, CoLLiE integrates FlashAttention to\nenhance efficiency. It implements ZeRO-powered\nDP, TP, and PP to support 3D parallelism. Addition-\nally, LOMO and PEFT methods are incorporated\nto realize memory-efficient fine-tuning approaches.\nAppendix A provides a brief tour demonstrating\nhow to use CoLLiE for training.\n3.1 Collaborative Tuning Methods\n3.1.1 3D Parallelism\nWhile distributed training frameworks such as\nDeepSpeed and Colossal-AI (Bian et al., 2021)\nsupport 3D parallelism, models in HuggingFace\ncan only opt for ZeRO-3 for model parallelism due\nto structural constraints. To fully support 3D par-\nallelism and meet the distributed training needs\nunder different scenarios, CoLLiE rewrites the\nmodels using Megatron-LM (Shoeybi et al., 2019)\nand restructures them according to DeepSpeed’s\nstructure requirements for pipeline models. In the\nrewriting process, we have maintained the inter-\nface to be essentially consistent with the Hugging-\nFace models, and have allowed the direct use of\nthe from_pretrained method to load pre-trained\nmodels from the HuggingFace hub. This approach\nsignificantly reduces the learning curve for users.\n3.1.2 Parameter-efficient Fine-tuning\nThe PEFT library implements state-of-the-art PEFT\nmethods at the model level, but lacks distributed\ntraining capabilities. CoLLiE has integrated the\nPEFT library into CollieModel, and made neces-\nsary patches to enable distributed training.\n3.1.3 Efficient Optimizers\nIn addition to the popular AdamW (Kingma and\nBa, 2015) optimizer, several other optimizers have\nbeen proposed for the purpose of saving memory,\nimproving optimization results, or accelerating con-\nvergence. The implementation of the optimizers in\nCoLLiE is decoupled from other parts, and incorpo-\nrates a variety of novel optimizers including Adan,\nLion, LOMO, and Sophia. The effectiveness of\nthese optimizers in training large language models\nis verified and compared in Section 4.3.\n3.2 Models\nIn addition to the above-mentioned model imple-\nmentations, CoLLiE has also replaced the naïve\nself-attention implementation with FlashAttention.\nGiven that FlashAttention has strict requirements\nregarding hardware and CUDA versions, for users\nwithout newer training equipment, we have added\nthe ‘use_flash’ option to the CollieConfig to al-\nlow for one-click disabling of FlashAttention us-\nage. Currently, CoLLiE has implemented a variety\nof language models, including but not limited to\nLLaMA, InternLM (Team, 2023), ChatGLM (Du\net al., 2022), and MOSS (Sun et al., 2023a), with\nthe intention to support more models in the future.\n3.3 Configuration\nCoLLiE offers a unified class, CollieConfig, to\nmanage configurations including model config, par-\nallelism strategy, DeepSpeed configuration, PEFT\nconfiguration, and training hyperparameters. Based\non the contents of CollieConfig, CollieModel\nwill automatically adjust the partitioning of model\nparameters and the structure of the model, and the\nTrainerwill modify the training process. Through\nCollieConfig, users can conveniently combine\ndifferent pre-trained language models, fine-tuning\nmethods, and hyperparameters.\nModel config refers to parameters that\ndescribe the model structure, such as\nhidden_size, num_attention_heads, and\nnum_hidden_layers. The model config is fixed\nfor pre-trained language models, and we provide\na from_pretrained interface, identical to Hug-\ngingFace’s, to initialize model config. Users can\nalso specify the model config to customize their\nmodels, intended for training from scratch without\nthe use of pre-trained models. Below is a code\n529\nPlugins\nTrainer Input-related Class Trainer Output\nCollieConfig\nTrainerCollieModel\nCollieDataset\nOptimizer\nCheckpoints\nSystem Metrics\nEval Results\nCallback Monitor Evaluator Probe\n(a) Architecture of CoLLiE. The blocks represent different modularly\ndesigned classes or the outputs of the Trainer.\nPyTorch Machine Learning Framework\nDeepSpeed Distributed Training Library\nFlashAttention Efficient Transformer\nPipeline Parallelism\n3D ParallelismTensor Parallelism\nLOMO\nMemory-efficient Fine-tuning\nPEFT\nZeRO-powered Data Parallelism\n(b) Features of CoLLiE. CoLLiE supports a collabo-\nrative suite of high-efficiency optimization features.\nFigure 2: Overall architecture and features of CoLLiE. Features in (b) are color-coded to match corresponding part\nin (a), indicating where each feature is implemented.\nexample of downloading the model config from the\nHuggingFace hub, initializing the CollieConfig,\nand setting up to use FlashAttention.\nconfig = CollieConfig.from_pretrained(\n'meta-llama/Llama-2-7b-hf '\n)\nconfig.use_flash = True\nCollieConfigstreamlines the setup for 3D par-\nallelism as followings. CoLLiE will automatically\nconfigure the distributed environment and parti-\ntion the parameters according, relieving users from\nmanaging the complexities of distributed training.\nThe number of GPUs required for training is equal\nto the product of the three parallelism sizes.\nconfig.dp_size = 1\nconfig.tp_size = 8\nconfig.pp_size = 2\nCoLLiE implements distributed training based\non DeepSpeed, and DeepSpeed-related configura-\ntions can be set via ds_config. The configura-\ntions related to PEFT methods can also be set via\npeft_config. Below is an example for mixed-\nprecision training with FP16 and LoRA.\nconfig.ds_config = {\n'fp16 ': { 'enabled ': True}\n}\nconfig.peft_config = LoraConfig(\nr=4,\nlora_alpha=32,\ntarget_modules=[ 'q_proj ', 'v_proj '],\nbias= 'none ',\ntask_type= 'CAUSAL_LM '\n)\nThe training hyperparameters can also be\nconfigured through CollieConfig. Loading\nCollieConfigfrom a file is supported and we pro-\nvide a convenient Command Line Interface (CLI)\nto generate the required configuration file.\n3.4 Dataset\nTo facilitate data processing, CoLLiE provides\nthree Dataset classes for training, evaluation of\ngeneration tasks, and evaluation of classification\ntasks respectively: CollieDatasetForTraining,\nCollieDatasetForGeneration, CollieDataset\nForClassification. These three classes can ei-\nther read data from a JSON file or a list of dictio-\nnaries, process it, and store the results on disk for\ndirect reading next time.\nCollieDatasetForTraining accepts two\nforms of input, one with the field “ text”, and\nthe other with fields “ input” and “ output”.\nThe loss of tokens in the field “ text” or\n“output” will be computed, corresponding\nto pre-training and instruction-tuning tasks,\nrespectively. CollieDatasetForGeneration\nand CollieDatasetForClassification both\ninherit from the CollieDatasetForTraining\nclass, serving as the datasets for generation\ntasks and classification tasks, respectively. The\nCollieDatasetForGeneration can accept\n“text” as a required field and “ target” as an\noptional field. The model generates output\nbased on the “text” and the “target” is used to\ncompute metrics in Evaluator. On the other hand,\nCollieDatasetForClassification can accept\n“input”, “ output”, and “ target” fields. The\n“input’ represents the question, “output” includes\nall possible options, and “target” indicates which\n530\noption should be chosen.\n3.5 Controller\nIn this section, we will introduce three modu-\nlarly designed classes centered around Trainer.\nTrainer calls the Evaluator and Server classes\nunidirectionally to serve the purposes of evaluation\nor manual probing of the model during training.\n3.5.1 Trainer\nDistributed training, including the initialization of\nthe distributed environment, training loop, and the\nsaving of model weights and checkpointing, can\nbe complex. CoLLiE provides a Trainerto allevi-\nate this burden on users. The Trainer wraps the\nrelatively fixed training loop and offers multiple in-\nterfaces for users to further customize the training\nprocess. These include the train_fn function that\nobtains output based on a given batch of input and\nthe loss_fnfunction that obtains loss based on the\nbatch and output from train_fn. Moreover, CoL-\nLiE offers several plugins to enrich functionality.\nMonitor The Monitorclass tracks various metrics\nsuch as loss, learning rate, throughput, and memory\nusage during the training process, and records them\nto Tensorboard, WandB, or local CSV files.\nCallback The Callback class can be invoked at\nvarious callback points during the training process,\nallowing users to customize the training loop. CoL-\nLiE has implemented callbacks that save model\nweights and training checkpoints when necessary,\nor load the model weights of the best evaluated\nresults after training. These callbacks are all imple-\nmented based on the same base class. Users can\ninherit from this base class and override different\nmethods to choose the callback timing and actions.\n3.5.2 Evaluator\nThe Evaluator class is used in conjunction with\nthe Metric class for assessing model performance.\nWe implemented three types of Evaluator, in-\ntended for generation tasks, classification tasks,\nand perplexity assessment, by subclassing the\nEvaluatorbase class and overriding the eval_fn\nmethod. The return value of theeval_fnmethod in\nthe Evaluatoris accepted as input by the update\nfunction of the Metric class. The Metric class’s\nupdate method updates the variables necessary\nfor calculating the metric after processing each\nbatch from the evaluation dataset. After the eval-\nuation dataset is fully processed, the get_metric\nfunction is employed to compute the metric. The\n0 10 20 30 40 50 60 70\nModel Parameters (Billion)\n0\n500\n1000\n1500\n2000\n2500Memory Requirement (GB)\nMem=34.5*Params\nMem=30.5*Params\nMem=2.1*Params\nAdan, Sophia\nAdam, Lion\nLOMO, PEFT\nFigure 3: Memory requirements when training models\nwith different parameters under various configurations.\nEvaluator class can either be provided to the\nTrainer for assessment during the training pro-\ncess or it can evaluate the model independently\nwithout the dependency on the Trainer.\n3.5.3 Server\nThe Serverclass offers web-based, interactive and\nstreaming generated sequences feature, enabling\nusers to conveniently deploy trained models for\nweb-based use, as well as manually probe model\nperformance during training. The DataProvider\nclass supplies asynchronous inference data for\nServer as a subprocess. When the Server is inte-\ngrated into the Trainer, users can input prompts\nvia the web interface. Once the current batch train-\ning is completed, an output will be generated based\non the user’s input prompt and returned to the web\ninterface for user’s review.\n3.6 Documentation\nWe provide API documentation and easily un-\nderstandable tutorials1 for users who are new to\nCoLLiE and distributed training. Comprehensive\ncode examples2 including vocabulary expansion,\ninstruction-tuning, and downstream tasks such as\nsummary and translation are also available.\n4 Evaluation\n4.1 Memory Requirement\nWhile Rajbhandari et al. (2020) estimates the total\nGPU memory required for model training as 18\ntimes the number of model parameters in bytes,\nmore GPU memory is consumed in reality. This\nis because this estimation only considers memory\nused by parameters, gradients, and the optimizer\n1https://openlmlab-collie.readthedocs.io/zh_CN/latest\n2https://github.com/OpenLMLab/collie/tree/main/examples\n531\n7 13 30 65\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000056/uni00000003/uni0000000b/uni00000025/uni0000004c/uni0000004f/uni0000004f/uni0000004c/uni00000052/uni00000051/uni0000000c\n0\n500\n1000\n1500\n2000\n2500\n3000/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000037/uni0000002a/uni00000036/uni0000000c\n/uni00000037/uni00000048/uni00000056/uni00000057/uni00000048/uni00000047/uni00000003/uni00000052/uni00000051/uni00000003/uni00000024/uni00000014/uni00000013/uni00000013/uni00000003/uni0000000b/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni00000014/uni00000015/uni0000001b/uni0000000c\n7 13 30 65\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000056/uni00000003/uni0000000b/uni00000025/uni0000004c/uni0000004f/uni0000004f/uni0000004c/uni00000052/uni00000051/uni0000000c\n0\n500\n1000\n1500\n2000\n2500\n3000/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000037/uni0000002a/uni00000036/uni0000000c\n/uni00000037/uni00000048/uni00000056/uni00000057/uni00000048/uni00000047/uni00000003/uni00000052/uni00000051/uni00000003/uni00000024/uni00000014/uni00000013/uni00000013/uni00000003/uni0000000b/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni00000014/uni00000013/uni00000015/uni00000017/uni0000000c\n7 13 30\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000056/uni00000003/uni0000000b/uni00000025/uni0000004c/uni0000004f/uni0000004f/uni0000004c/uni00000052/uni00000051/uni0000000c\n0\n50\n100\n150\n200/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000037/uni0000002a/uni00000036/uni0000000c\n/uni00000037/uni00000048/uni00000056/uni00000057/uni00000048/uni00000047/uni00000003/uni00000052/uni00000051/uni00000003/uni00000035/uni00000037/uni0000003b/uni00000010/uni00000016/uni00000013/uni0000001c/uni00000013/uni00000003/uni0000000b/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni00000014/uni00000015/uni0000001b/uni0000000c\n7 13 30\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000056/uni00000003/uni0000000b/uni00000025/uni0000004c/uni0000004f/uni0000004f/uni0000004c/uni00000052/uni00000051/uni0000000c\n0\n50\n100\n150\n200/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000037/uni0000002a/uni00000036/uni0000000c\n/uni00000037/uni00000048/uni00000056/uni00000057/uni00000048/uni00000047/uni00000003/uni00000052/uni00000051/uni00000003/uni00000035/uni00000037/uni0000003b/uni00000010/uni00000016/uni00000013/uni0000001c/uni00000013/uni00000003/uni0000000b/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni00000014/uni00000013/uni00000015/uni00000017/uni0000000c\nHuggingFace with ZeRO-3 CoLLiE with ZeRO-3 CoLLiE with TP/PP\nFigure 4: Throughput tested on A100 and RTX-3090.\nstates, and neglects other components such as acti-\nvation values and buffers used for communication.\nIn this section, we profile the actual memory re-\nquirements for training models under different con-\nfigurations to facilitate users in more accurately es-\ntimating the model size that their devices can train.\nAs depicted in Figure 3, the most commonly used\nAdam optimizer requires 30.5 times the amount of\nmemory relative to the model parameters, which\nis consistent with Lion. Adan and Sophia opti-\nmizers use 4 times more memory for intermediate\nvariables when updating parameters, amounting to\n34.5 times the parameter size. The LOMO opti-\nmizer, without storing any optimizer state or gradi-\nent, only requires 2.1 times the parameter size in\nmemory, almost all of which is consumed by the\nhalf-precision parameters. PEFT methods, which\nupdate only a small proportion of parameters, have\na memory usage similar to LOMO.\n4.2 Throughput Analyses\nWe take HuggingFace models with ZeRO-3 as a\nbaseline to analyse the throughput of CoLLiE dur-\ning pre-training (with batch size of 1024) and fine-\ntuning (with batch size of 128). The corpus we\nused consists of the first 10,000 entries from the\nPile (Gao et al., 2021). The throughput is measured\nby the number of tokens processed by each GPU\nper second, referred to as TGS.\nAs shown in Figure 4, on the A100 connected\nby NVLink, CoLLiE’s throughput significantly sur-\npasses the baseline attributed to the integration of\nFlashAttention. On the RTX-3090, where com-\nmunication is limited by PCIe, CoLLiE achieves\nMMLU BBH GSM HumanEval AlpacaFarm Avg.\nVanilla 62.4 56.9 53.9 20.7 4.7 39.7\nLoRA 62.7 58.7 60.5 32.9 69.6 56.9\nLOMO 62.1 56.9 57.6 28.1 65.2 54.0\nLion 58.2 52.6 41.3 25.0 66.2 48.7\nAdan 57.3 51.9 37.3 21.3 62.5 46.1\nAdam 63.0 58.0 55.3 28.1 73.1 55.5\nTable 1: Comparison of different training methods on\nGPT4-Alpaca. Instruction-tuning significantly enhances\nthe instruction-following ability of vanilla LLaMA-65B.\nsubstantially higher throughput by a more appro-\npriate parallelism approach, namely TP and PP.\n4.3 Empirical Assessment of Effectiveness\nWe employ various training methods on GPT-4-\nAlpaca (Peng et al., 2023) for the LLaMA-65B\nmodel and evaluate the factual knowledge, reason-\nning abilities, code capabilities, and instruction-\nfollowing abilities using MMLU (Hendrycks et al.,\n2021), BBH (Suzgun et al., 2023), GSM8K (Cobbe\net al., 2021), HumanEval (Chen et al., 2021), and\nAlpacaFarm (Dubois et al., 2023). The hyperpa-\nrameters and templates for training and evaluating\ncan be found in Appendix B.3 and Appendix C,\nrespectively.\nThe results in Table 1 demonstrate that while\nthe vanilla LLaMA-65B already exhibits substan-\ntial capabilities, it struggles to effectively follow\ninstructions from actual users. The performance\nof the models significantly improves on average\nafter instruction-tuning. Training methods such as\nLoRA, LOMO, and AdamW significantly enhance\nthe model’s ability to follow instructions without\ncompromising its other performance.\n5 Conclusion\nWe have introduced CoLLiE, a library for collab-\noratively training large language models in an ef-\nficient way. CoLLiE offers efficient models with\nFlashAttention and structurally supportive for 3D\nparallelism. Moreover, CoLLiE provides a compre-\nhensive and customizable Trainer to assist users\nthroughout the training process, supporting various\ntraining methods. We have tested the relationship\nbetween the GPU memory requirements and model\nparameter sizes as a reference for users. In terms\nof throughput, CoLLiE is significantly more effi-\ncient than HuggingFace’s parallel solutions. The\neffectiveness of different training methods are also\nempirically assessed on instruction-tuning tasks.\n532\nLimitations\nWe discuss the limitations of this paper from the\nfollowing two aspects:\n1) Although we profile the memory usage un-\nder real training conditions in this paper, a more\nfine-grained memory allocation situation is not pro-\nvided. In the future, we plan to develop a fine-\ngrained memory monitor to assist users in training.\n2) Due to resource and time constraints, this pa-\nper only presents the instruction-tuning results of\nLLaMA-65B with different training methods. This\nrestricts users from comparing the performance of\nmodels of different sizes. We will provide per-\nformances of more models under various train-\ning methods and continuously update them on our\nGithub repository for user reference. Furthermore,\nwhile CoLLiE has implemented the Sophia opti-\nmizer to enhance pre-training efficiency, we have\nnot conducted extensive experiments under costly\npre-training tasks.\nAcknowledgements\nThis work was supported by the National Key\nResearch and Development Program of China\n(No.2022ZD0160102) and National Natural Sci-\nence Foundation of China (No.62022027).\nReferences\nZhengda Bian, Hongxin Liu, Boxiang Wang, Haichen\nHuang, Yongbin Li, Chuanrui Wang, Fan Cui, and\nYang You. 2021. Colossal-ai: A unified deep learn-\ning system for large-scale parallel training. CoRR,\nabs/2110.14883.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Pondé de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nXiangning Chen, Chen Liang, Da Huang, Esteban Real,\nKaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong,\nThang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V .\nLe. 2023. Symbolic discovery of optimization algo-\nrithms. CoRR, abs/2302.06675.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. CoRR, abs/2110.14168.\nTri Dao. 2023. Flashattention-2: Faster attention with\nbetter parallelism and work partitioning. CoRR,\nabs/2307.08691.\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,\nand Christopher Ré. 2022. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nIn NeurIPS.\nShizhe Diao, Rui Pan, Hanze Dong, Kashun Shum,\nJipeng Zhang, Wei Xiong, and Tong Zhang. 2023.\nLmflow: An extensible toolkit for finetuning and\ninference of large foundation models. CoRR,\nabs/2306.12420.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nXiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei\nChen, Yang Liu, Jie Tang, Juanzi Li, and Maosong\nSun. 2023. Parameter-efficient fine-tuning of large-\nscale pre-trained language models. Nat. Mac. Intell.,\n5(3):220–235.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320–335.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\nIshaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Alpaca-\nfarm: A simulation framework for methods that learn\nfrom human feedback. CoRR, abs/2305.14387.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021. The pile: An\n800gb dataset of diverse text for language modeling.\nCoRR, abs/2101.00027.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring massive multitask language\nunderstanding. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\n533\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Pro-\nceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings\nof Machine Learning Research, pages 2790–2799.\nPMLR.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. In The Tenth International\nConference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan\nFirat, Dehao Chen, Mia Xu Chen, HyoukJoong Lee,\nJiquan Ngiam, Quoc V . Le, Yonghui Wu, and Zhifeng\nChen. 2019. Gpipe: Efficient training of giant neural\nnetworks using pipeline parallelism. In Advances\nin Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 103–112.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021, pages 3045–\n3059. Association for Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, pages 4582–\n4597. Association for Computational Linguistics.\nHong Liu, Zhiyuan Li, David Hall, Percy Liang, and\nTengyu Ma. 2023. Sophia: A scalable stochas-\ntic second-order optimizer for language model pre-\ntraining. CoRR, abs/2305.14342.\nKai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao,\nQipeng Guo, and Xipeng Qiu. 2023. Full parameter\nfine-tuning for large language models with limited\nresources. CoRR, abs/2306.09782.\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut,\nYounes Belkada, and Sayak Paul. 2022. Peft: State-\nof-the-art parameter-efficient fine-tuning methods.\nhttps://github.com/huggingface/peft.\nDeepak Narayanan, Aaron Harlap, Amar Phanishayee,\nVivek Seshadri, Nikhil R. Devanur, Gregory R.\nGanger, Phillip B. Gibbons, and Matei Zaharia. 2019.\nPipedream: generalized pipeline parallelism for DNN\ntraining. In Proceedings of the 27th ACM Sympo-\nsium on Operating Systems Principles, SOSP 2019,\nHuntsville, ON, Canada, October 27-30, 2019, pages\n1–15. ACM.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward Z.\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Informa-\ntion Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada, pages\n8024–8035.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\nGPT-4. CoRR, abs/2304.03277.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: memory optimizations\ntoward training trillion parameter models. In Pro-\nceedings of the International Conference for High\nPerformance Computing, Networking, Storage and\nAnalysis, SC 2020, Virtual Event / Atlanta, Georgia,\nUSA, November 9-19, 2020, page 20. IEEE/ACM.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In KDD ’20: The 26th\nACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, Virtual Event, CA, USA, August\n23-27, 2020, pages 3505–3506. ACM.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ilic, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, Jonathan Tow, Alexander M. Rush,\nStella Biderman, Albert Webson, Pawan Sasanka Am-\nmanamanchi, Thomas Wang, Benoît Sagot, Niklas\nMuennighoff, Albert Villanova del Moral, Olatunji\nRuwase, Rachel Bawden, Stas Bekman, Angelina\nMcMillan-Major, Iz Beltagy, Huu Nguyen, Lucile\nSaulnier, Samson Tan, Pedro Ortiz Suarez, Vic-\ntor Sanh, Hugo Laurençon, Yacine Jernite, Julien\nLaunay, Margaret Mitchell, Colin Raffel, Aaron\nGokaslan, Adi Simhi, Aitor Soroa, Alham Fikri\nAji, Amit Alfassy, Anna Rogers, Ariel Kreisberg\nNitzav, Canwen Xu, Chenghao Mou, Chris Emezue,\nChristopher Klamm, Colin Leong, Daniel van Strien,\nDavid Ifeoluwa Adelani, and et al. 2022. BLOOM:\nA 176b-parameter open-access multilingual language\nmodel. CoRR, abs/2211.05100.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\n534\nparameter language models using model parallelism.\nCoRR, abs/1909.08053.\nTianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li,\nQinyuan Cheng, Hang Yan, Xiangyang Liu, Yun-\nfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen,\nYining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan,\nYunhua Zhou, Linyang Li, Xiaogui Yang, Lingling\nWu, Zhangyue Yin, Xuanjing Huang, and Xipeng\nQiu. 2023a. Moss: Training conversational language\nmodels from synthetic data. https://github.com/\nOpenLMLab/MOSS.\nXianghui Sun, Yunjie Ji, Baochang Ma, and Xian-\ngang Li. 2023b. A comparative study between full-\nparameter and lora-based fine-tuning on chinese in-\nstruction data for instruction following large language\nmodel. CoRR, abs/2304.08109.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V . Le, Ed Chi, Denny\nZhou, and Jason Wei. 2023. Challenging big-bench\ntasks and whether chain-of-thought can solve them.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 13003–13051. Association for Computa-\ntional Linguistics.\nInternLM Team. 2023. Internlm: A multilingual lan-\nguage model with progressively enhanced capabili-\nties. https://github.com/InternLM/InternLM.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack\nHessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A. Smith,\nIz Beltagy, and Hannaneh Hajishirzi. 2023. How\nfar can camels go? exploring the state of instruction\ntuning on open resources. CoRR, abs/2306.04751.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nXingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and\nShuicheng Yan. 2022. Adan: Adaptive nesterov mo-\nmentum algorithm for faster optimizing deep models.\nCoRR, abs/2208.06677.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin,\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-\nter, Daniel Simig, Punit Singh Koura, Anjali Srid-\nhar, Tianlu Wang, and Luke Zettlemoyer. 2022.\nOPT: open pre-trained transformer language mod-\nels. CoRR, abs/2205.01068.\n535\nA Code Example\nListing 1 presents the simplest code example for\ntraining with CoLLiE.\nB Hyperparameters\nTraining\nMethods LR Batch\nSize\nWeight\nDecay Epochs\nLoRA 3e-4 128 1e-2 3\nLOMO 1e-2 16 - 5\nLion 3e-6 128 3e-2 3\nAdan 5e-5 128 2e-2 3\nAdam 1e-5 128 1e-2 3\nTable 2: Hyperparameters for training.\nB.1 Memory Requirements\nWe choose the combination of Tensor Parallelism\n(TP) and Pipeline Parallelism (PP) as our paral-\nlelism strategy. The batch size is set to 2048, and\nthe gradient accumulation steps are set to 2. It’s\nworth noting that increasing the value of the gra-\ndient accumulation steps would not significantly\nincrease the memory usage.\nB.2 Throughput\nIn our throughput tests, we consistently employ\nAdam as the optimizer. We utilize the default\nsettings of DeepSpeed for ZeRO-3 and strive to\nmaximize the micro batch size to enhance through-\nput. For Tensor Parallelism/Pipeline Parallelism\n(TP/PP), we ensure that the gradient accumula-\ntion steps are more than four times the number\nof pipeline stages to minimize the bubble. The\nspecific configurations are illustrated in Table 4.\nB.3 Instruction-tuning\nAs shown in Table 2, we have adopted the learning\nrates and batch sizes from the Tulu (Wang et al.,\n2023) and Alpaca-LoRA projects3 for AdamW and\nLoRA. To achieve better performance for LoRA,\nwe have replaced all modules with LoRA layers,\nnot just the q-v module. For Lion and Adan, we\nhave used the learning rates recommended in the\npaper. Specifically, the learning rate for Lion is\n3-10 times smaller than that of AdamW, with the\nweight decay correspondingly 3-10 times larger.\nThe learning rate for the Adan optimizer is 5-10\n3https://github.com/tloen/alpaca-lora\nTemplate for entries with input\nBelow is an instruction that describes a task,\npaired with an input that provides further con-\ntext. Write a response that appropriately com-\npletes the request.\n### Instruction:\n{instruction}\n### Input:\n{input}\n### Response:{response}\nTemplate for entries without input\nBelow is an instruction that describes a task.\nWrite a response that appropriately completes\nthe request.\n### Instruction:\n{instruction}\n### Response:{response}\nTable 3: Templates used for training.\ntimes larger than that of AdamW, with a weight\ndecay of 0.02. For the LOMO optimizer, which is\nsimilar to SGD, we have utilized a larger learning\nrate and a smaller batch size.\nC Templates\nC.1 Alpaca\nWe follow the template provided by the Alpaca\nrepository4 for training, as shown in Table 3.\nC.2 Evaluation\nWe modify the evaluation template based on the\ntemplate used during training, as shown in Table 5.\nThe template used for evaluate on AlpacaFarm is\nidentical to that of training on Alpaca.\n4https://github.com/tatsu-lab/stanford_alpaca\n536\nListing 1: An example for training with CoLLiE.\nimport torch\nfrom collie.config import CollieConfig\nfrom collie.models import LlamaForCausalLM\nfrom collie.controller import Trainer\nmodel_name_or_path = 'meta-llama/Llama-2-7b-hf '\n# load model config from huggingface hub\nconfig = CollieConfig.from_pretrained(model_name_or_path)\n# set pipeline parallelism size via config\nconfig.pp_size = 8\n# load pre-trained weights from huggingface hub\n# and partition the weights into 8 stages for pipeline parallelism\nmodel = LlamaForCausalLM.from_pretrained(\nmodel_name_or_path,\nconfig=config\n)\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n# one of the two formats collie defined for training\ntrain_dataset = [\n{'text ': 'Collie␣is␣a␣package␣for␣training␣large␣language␣models. '}\nfor _ in range(100)\n]\ntrainer = Trainer(\nmodel=model,\noptimizer=optimizer,\nconfig=config,\ntrain_dataset=train_dataset,\n)\n# start the training process\ntrainer.train()\nModel Params 7B 13B 30B 65B\nDevice A100\nMode Fine-tune / Pre-train\n# GPU 4 8 16 32\nHuggingFace with ZeRO-3 Batch Size, GAS 64,2 / 64,16 64,2 / 64,16 64, 2 / 128,8 128,1 / 128,8\nCoLLiE with ZeRO-3 Batch Size, GAS 64,2 / 64,16 128,1 / 64,16 128,1 / 128,8 128,1 / 128,8\nCoLLiE with TP/PP Batch Size, GAS 4,32 / 8,128 2,64 / 16,64 1,128 / 2,512 1,128 / 1,1024\nDevice RTX-3090\nMode Fine-tune / Pre-train\n# GPU 8 24 48 -\nHuggingFace with ZeRO-3 Batch Size, GAS 8,16 / 8,128 24,6 / 24,43 48,3 / 48,22 -\nCoLLiE with ZeRO-3 Batch Size, GAS 8,16 / 8,128 24,6 / 24,43 48,3 / 48,22 -\nCoLLiE with TP/PP Batch Size, GAS 1,128 / 1,1024 1,128 / 1,1024 1,128 / 1,1024 -\nTable 4: Hyperparameters for testing throughput. We report the number of model parameters (Model Params),\ndevice (Device), mode (Mode) and number of GPU (#GPU). We also report the corresponding batch size (Batch\nSize) and GAS (Gradient Accumulation Steps) for HuggingFace with ZeRO-3, CoLLiE with ZeRO-3 and CoLLiE\nwith TP/PP.\n537\nMMLU\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nThe following is a multiple choice question (with answers) about abstract algebra. You need to answer the question by\nselecting the correct option.\n### Input:\nFind all c in Z_3 such that Z_3[x]/(xˆ 2 + c) is a field.\nA. 0\nB. 1\nC. 2\nD. 3\n### Response: B\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nThe following is multiple choice question (with answers) about abstract algebra. You need to answer the question by\nselecting the correct option.\n### Input:\nStatement 1 | If aH is an element of a factor group, then |aH| divides |a|. Statement 2 | If H and K are subgroups of G\nthen HK is a subgroup of G.\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n### Response: B\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nThe following is multiple choice question (with answers) about abstract algebra. You need to answer the question by\nselecting the correct option.\n### Input:\nStatement 1 | Every element of a group generates a cyclic subgroup of the group. Statement 2 | The symmetric group\nS_10 has 10 elements.\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n### Response: C\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nThe following is multiple choice question (with answers) about abstract algebra. You need to answer the question by\nselecting the correct option.\n538\n### Input:\nStatement 1| Every function from a finite set onto itself must be one to one. Statement 2 | Every subgroup of an abelian\ngroup is abelian.\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n### Response: A\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nThe following is multiple choice question (with answers) about abstract algebra. You need to answer the question by\nselecting the correct option.\n### Input:\nFind the characteristic of the ring 2Z.\nA. 0\nB. 3\nC. 12\nD. 30\n### Response: A\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nThe following is multiple choice question (with answers) about abstract algebra. You need to answer the question by\nselecting the correct option.\n### Input:\nInput\n### Response:\nBBH\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nEvaluate the result of a random Boolean expression.\n### Input:\nnot ( ( not not True ) ) is\n### Response: Let’s think step by step.\nRemember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from\nhighest priority to lowest priority is \"not\", \"and\", \"or\", respectively.\nWe first simplify this expression \"Z\" as follows: \"Z = not ( ( not not True ) ) = not ( ( A ) )\" where \"A = not not True\".\nLet’s evaluate A: A = not not True = not (not True) = not False = True.\nPlugging in A, we get: Z = not ( ( A ) ) = not ( ( True ) ) = not True = False. So the answer is False.\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n539\n### Instruction:\nEvaluate the result of a random Boolean expression.\n### Input:\nTrue and False and not True and True is\n### Response: Let’s think step by step.\nRemember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from\nhighest priority to lowest priority is \"not\", \"and\", \"or\", respectively.\nWe first simplify this expression \"Z\" as follows: \"Z = True and False and not True and True = A and B\" where \"A =\nTrue and False\" and \"B = not True and True\".\nLet’s evaluate A: A = True and False = False.\nLet’s evaluate B: B = not True and True = not (True and True) = not (True) = False.\nPlugging in A and B, we get: Z = A and B = False and False = False. So the answer is False.\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nEvaluate the result of a random Boolean expression.\n### Input:\nnot not ( not ( False ) ) is\n### Response: Let’s think step by step.\nRemember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from\nhighest priority to lowest priority is \"not\", \"and\", \"or\", respectively.\nWe first simplify this expression \"Z\" as follows: \"Z = not not ( not ( False ) ) = not not ( A )\" where \"A = not ( False )\".\nLet’s evaluate A: A = not ( False ) = not False = True.\nPlugging in A, we get: Z = not not ( A ) = not not (True) = not not False = True. So the answer is True.\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nEvaluate the result of a random Boolean expression.\n### Input:\n{input}\n### Response: Let’s think step by step.\nRemember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from\nhighest priority to lowest priority is \"not\", \"and\", \"or\", respectively.\nGSM8K\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nGiven a problem scenario with numerical data, perform the necessary calculations and provide a detailed step-by-step\nsolution, ending the response with ’The answer is’.\n### Input:\nAngelo and Melanie want to plan how many hours over the next week they should study together for their test next\nweek. They have 2 chapters of their textbook to study and 4 worksheets to memorize. They figure out that they should\ndedicate 3 hours to each chapter of their textbook and 1.5 hours for each worksheet. If they plan to study no more than\n4 hours each day, how many days should they plan to study total over the next week if they take a 10-minute break\nevery hour, include 3 10-minute snack breaks each day, and 30 minutes for lunch each day?\n540\n### Response: Let’s think step by step. Angelo and Melanie think they should dedicate 3 hours to each of the 2\nchapters, 3 hours x 2 chapters = 6 hours total. For the worksheets they plan to dedicate 1.5 hours for each worksheet,\n1.5 hours x 4 worksheets = 6 hours total. Angelo and Melanie need to start with planning 12 hours to study, at 4 hours\na day, 12 / 4 = 3 days. However, they need to include time for breaks and lunch. Every hour they want to include a\n10-minute break, so 12 total hours x 10 minutes = 120 extra minutes for breaks. They also want to include 3 10-minute\nsnack breaks, 3 x 10 minutes = 30 minutes. And they want to include 30 minutes for lunch each day, so 120 minutes\nfor breaks + 30 minutes for snack breaks + 30 minutes for lunch = 180 minutes, or 180 / 60 minutes per hour = 3 extra\nhours. So Angelo and Melanie want to plan 12 hours to study + 3 hours of breaks = 15 hours total. They want to study\nno more than 4 hours each day, 15 hours / 4 hours each day = 3.75 They will need to plan to study 4 days to allow for\nall the time they need. The answer is 4\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nGiven a problem scenario with numerical data, perform the necessary calculations and provide a detailed step-by-step\nsolution, ending the response with ’The answer is’.\n### Input:\nMark’s basketball team scores 25 2 pointers, 8 3 pointers and 10 free throws. Their opponents score double the\n2 pointers but half the 3 pointers and free throws. What’s the total number of points scored by both teams added\ntogether?\n### Response: Let’s think step by step. Mark’s team scores 25 2 pointers, meaning they scored 25*2= 50 points\nin 2 pointers. His team also scores 6 3 pointers, meaning they scored 8*3= 24 points in 3 pointers They scored 10\nfree throws, and free throws count as one point so they scored 10*1=10 points in free throws. All together his team\nscored 50+24+10= 84 points Mark’s opponents scored double his team’s number of 2 pointers, meaning they scored\n50*2=100 points in 2 pointers. His opponents scored half his team’s number of 3 pointers, meaning they scored 24/2=\n12 points in 3 pointers. They also scored half Mark’s team’s points in free throws, meaning they scored 10/2=5 points\nin free throws. All together Mark’s opponents scored 100+12+5=117 points The total score for the game is both team’s\nscores added together, so it is 84+117=201 points The answer is 201\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nGiven a problem scenario with numerical data, perform the necessary calculations and provide a detailed step-by-step\nsolution, ending the response with ’The answer is’.\n### Input:\nBella has two times as many marbles as frisbees. She also has 20 more frisbees than deck cards. If she buys 2/5 times\nmore of each item, what would be the total number of the items she will have if she currently has 60 marbles?\n### Response: Let’s think step by step. When Bella buys 2/5 times more marbles, she’ll have increased the number of\nmarbles by 2/5*60 = 24 The total number of marbles she’ll have is 60+24 = 84 If Bella currently has 60 marbles, and\nshe has two times as many marbles as frisbees, she has 60/2 = 30 frisbees. If Bella buys 2/5 times more frisbees, she’ll\nhave 2/5*30 = 12 more frisbees. The total number of frisbees she’ll have will increase to 30+12 = 42 Bella also has 20\nmore frisbees than deck cards, meaning she has 30-20 = 10 deck cards If she buys 2/5 times more deck cards, she’ll\nhave 2/5*10 = 4 more deck cards. The total number of deck cards she’ll have is 10+4 = 14 Together, Bella will have a\ntotal of 14+42+84 = 140 items The answer is 140\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nGiven a problem scenario with numerical data, perform the necessary calculations and provide a detailed step-by-step\nsolution, ending the response with ’The answer is’.\n### Input:\nA group of 4 fruit baskets contains 9 apples, 15 oranges, and 14 bananas in the first three baskets and 2 less of each\nfruit in the fourth basket. How many fruits are there?\n541\n### Response: Let’s think step by step. For the first three baskets, the number of apples and oranges in one basket is\n9+15=24 In total, together with bananas, the number of fruits in one basket is 24+14=38 for the first three baskets.\nSince there are three baskets each having 38 fruits, there are 3*38=114 fruits in the first three baskets. The number of\napples in the fourth basket is 9-2=7 There are also 15-2=13 oranges in the fourth basket The combined number of\noranges and apples in the fourth basket is 13+7=20 The fourth basket also contains 14-2=12 bananas. In total, the\nfourth basket has 20+12=32 fruits. The four baskets together have 32+114=146 fruits. The answer is 146\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nGiven a problem scenario with numerical data, perform the necessary calculations and provide a detailed step-by-step\nsolution, ending the response with ’The answer is’.\n### Input:\n{question}\n### Response: Let’s think step by step.\nHumanEval\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nComplete the following python code.\n### Input:\nCheck if in given list of numbers, are any two numbers closer to each other than given threshold.\n>>> has_close_elements([1.0, 2.0, 3.0], 0.5) False\n>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True\n### Response:\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n\"\"\" Check if in given list of numbers, are any two numbers closer to each other than given threshold.\n>>> has_close_elements([1.0, 2.0, 3.0], 0.5) False\n>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True \"\"\"\nTable 5: The templates used for evaluation.\n542",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.7425001263618469
    },
    {
      "name": "Computer science",
      "score": 0.5987088680267334
    },
    {
      "name": "Chen",
      "score": 0.524520754814148
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37717723846435547
    },
    {
      "name": "Natural language processing",
      "score": 0.374192476272583
    },
    {
      "name": "History",
      "score": 0.12946131825447083
    },
    {
      "name": "China",
      "score": 0.11686050891876221
    },
    {
      "name": "Ecology",
      "score": 0.07623100280761719
    },
    {
      "name": "Biology",
      "score": 0.06718006730079651
    },
    {
      "name": "Archaeology",
      "score": 0.05789509415626526
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    }
  ],
  "cited_by": 3
}