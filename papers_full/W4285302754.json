{
    "title": "Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages",
    "url": "https://openalex.org/W4285302754",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3200826005",
            "name": "Ehsan Aghazadeh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2014750693",
            "name": "Mohsen Fayyaz",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2398033691",
            "name": "Yadollah Yaghoobzadeh",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4289375491",
        "https://openalex.org/W2799124508",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W1996908104",
        "https://openalex.org/W4303182341",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W2964210975",
        "https://openalex.org/W2170962714",
        "https://openalex.org/W2050797400",
        "https://openalex.org/W3024566755",
        "https://openalex.org/W2252218513",
        "https://openalex.org/W3045699325",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2115340919",
        "https://openalex.org/W2971344868",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2264869955",
        "https://openalex.org/W3088342637",
        "https://openalex.org/W2470413457",
        "https://openalex.org/W3046209160",
        "https://openalex.org/W3017779903",
        "https://openalex.org/W3177283195",
        "https://openalex.org/W2971019533",
        "https://openalex.org/W2806592685",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3167364369",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W2126530744",
        "https://openalex.org/W2250382531",
        "https://openalex.org/W3202070718",
        "https://openalex.org/W3015777882",
        "https://openalex.org/W3187134297",
        "https://openalex.org/W2949442961",
        "https://openalex.org/W3037492894",
        "https://openalex.org/W3087873698",
        "https://openalex.org/W3104235057",
        "https://openalex.org/W2578317340",
        "https://openalex.org/W3034408878",
        "https://openalex.org/W2951941824",
        "https://openalex.org/W3046079573",
        "https://openalex.org/W4205424278",
        "https://openalex.org/W2946359678"
    ],
    "abstract": "Human languages are full of metaphorical expressions. Metaphors help people understand the world by connecting new concepts and domains to more familiar ones. Large pre-trained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems. In this paper, we investigate this hypothesis for PLMs, by probing metaphoricity information in their encodings, and by measuring the cross-lingual and cross-dataset generalization of this information. We present studies in multiple metaphor detection datasets and in four languages (i.e., English, Spanish, Russian, and Farsi). Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers. The knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets. Our findings give helpful insights for both cognitive and NLP scientists.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2037 - 2050\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nMetaphors in Pre-Trained Language Models:\nProbing and Generalization Across Datasets and Languages\nEhsan Aghazadeh⋆ Mohsen Fayyaz⋆ Yadollah Yaghoobzadeh\nSchool of Electrical and Computer Engineering,\nCollege of Engineering,\nUniversity of Tehran, Tehran, Iran\n{eaghazade1998, mohsen.fayyaz77, y.yaghoobzadeh}@ut.ac.ir\nAbstract\nHuman languages are full of metaphorical ex-\npressions. Metaphors help people understand\nthe world by connecting new concepts and\ndomains to more familiar ones. Large pre-\ntrained language models (PLMs) are therefore\nassumed to encode metaphorical knowledge\nuseful for NLP systems. In this paper, we in-\nvestigate this hypothesis for PLMs, by probing\nmetaphoricity information in their encodings,\nand by measuring the cross-lingual and cross-\ndataset generalization of this information. We\npresent studies in multiple metaphor detection\ndatasets and in four languages (i.e., English,\nSpanish, Russian, and Farsi). Our extensive ex-\nperiments suggest that contextual representa-\ntions in PLMs do encode metaphorical knowl-\nedge, and mostly in their middle layers. The\nknowledge is transferable between languages\nand datasets, especially when the annotation is\nconsistent across training and testing sets. Our\nﬁndings give helpful insights for both cogni-\ntive and NLP scientists.\n1 Introduction\nPre-trained language models (PLMs) (Peters et al.,\n2018; Devlin et al., 2019), are now used in almost\nall NLP applications, e.g., machine translation (Li\net al., 2021), question answering (Zhang et al.,\n2020), dialogue systems (Ni et al., 2021), and sen-\ntiment analysis (Minaee et al., 2020). They have\nsometimes been referred to as “foundation models”\n(Bommasani et al., 2021) due to their signiﬁcant\nimpact on research and industry.\nMetaphors are important aspects of human lan-\nguages. In conceptual metaphor theory (CMT)\n(Lakoff and Johnson, 2008), metaphor is deﬁned as\na cognitive phenomenon associating two different\nconcepts or domains. This phenomenon is built in\ncognition and expressed in language. The creativ-\nity and problem solving (i.e., generalization to new\n⋆ Equal contribution.\nFigure 1: An illustration of our probing and gener-\nalization scenarios for metaphorical knowledge.\nproblems) depend on the analogies and metaphors\na cognitive system, like our brain, relies on. Mod-\neling metaphors is therefore essential in building\nhuman-like computational systems that can relate\nemerging concepts to the more familiar ones.\nSo far, there has been no comprehensive analysis\nof whether and how PLMs represent metaphori-\ncal information. We intuitively assume that PLMs\nmust encode some information about metaphors\ndue to their great performance in metaphor detec-\ntion and other language processing tasks. Con-\nﬁrming that experimentally is a question that we\naddress here. Speciﬁcally, we aim to know whether\ngeneralizable metaphorical knowledge is encoded\nin PLM representations or not. The outline of our\nwork is presented in Figure 1.\nWe ﬁrst do probing experiments to answer ques-\ntions such as: (i) with which accuracies and ex-\ntractablities do different PLMs encode metaphor-\nical knowledge? (ii) how deep is the metaphori-\ncal knowledge encoded in PLM multi-layer repre-\nsentations? We take two probing methods, edge\nprobing (Tenney et al., 2019b) and minimum de-\nscription length (V oita and Titov, 2020), and apply\nthem to four metaphor detection datasets, namely\nLCC (Mohler et al., 2016), TroFi (Birke and Sarkar,\n2006), VUA pos, and VUA Verbs (Steen, 2010).\n2037\nTo better estimate the generalization of\nmetaphorical knowledge in PLMs, we design two\nsetups in which testing comes from a different\ndistribution than training data: cross-lingual and\ncross-dataset metaphor detection. Each setup can\nreveal important information on whether or not the\nmetaphorical knowledge is encoded consistently\nin PLMs. Four languages (English, Farsi, Russian\nand Spanish) and four datasets (LCC, TroFi, VUA\npos, and VUA Verbs) are considered in these gen-\neralization experiments.\nIn summary, this paper makes the following con-\ntributions:\n• For the ﬁrst time, and through careful probing\nanalysis, we conﬁrm that PLMs do encode\nmetaphorical knowledge.\n• We show that metaphorical knowledge is en-\ncoded better in the middle layers of PLMs.\n• We evaluate the generalization of metaphori-\ncal knowledge in PLMs across four languages\nand four dataset sources, and ﬁnd out that\nthere is considerable transferability for the\npairs with consistent data annotation even if\nthey are in different languages. 1\n2 Related Work\nMetaphor detection using PLMs. The\nmetaphor detection task (Mason, 2004; Birke\nand Sarkar, 2007; Shutova et al., 2013) is a good\nﬁt for analyzing the metaphorical knowledge.\nUsing PLMs for metaphor detection has been\ncommon in recent years, setting new state-of-\nthe-art results, indicating implicitly that PLMs\nrepresent metaphorical information. Choi et al.\n(2021) introduce a new architecture that integrates\nmetaphor detection theories with BERT. They\nuse the deﬁnitions as well as example usages of\nwords jointly with PLM representations. Similarly,\nSong et al. (2021) presents a new perspective on\nmetaphor detection task by framing it as relation\nclassiﬁcation, focusing on the verbs. These\napproaches beat the earlier work of using PLMs\n(Su et al., 2020; Chen et al., 2020; Gong et al.,\n2020), RNN-based (Wu et al., 2018; Mao et al.,\n2019) and feature-based approaches (Turney et al.,\n2011; Shutova et al., 2016). Note that our goal is\nnot to compete with these models, but to probe and\nanalyze the relevant knowledge in PLMs.\n1Our implementation is available at https://github.\ncom/EhsanAghazadeh/Metaphors_in_PLMs\nTsvetkov et al. (2014) present cross-lingual\nmetaphor detection models using linguistic fea-\ntures and word embeddings. Bilingual dictionaries\nmap different languages. Their datasets are quite\nsmall ( ˜1000 training and ˜200 testing examples),\nmaking them unsuitable for a robust evaluation.\nHowever, this paper still remains as the only cross-\nlingual evaluation of metaphor detection, to the\nbest of our knowledge. Here, using multilingual\nPLMs, we perform zero-shot cross-lingual trans-\nfer for metaphor detection. Our goal is to test if\nPLMs represent metaphorical knowledge transfer-\nable across languages.\nProbing methods in NLP. Probing is an analyti-\ncal tool used for assessing linguistic knowledge in\nlanguage representations. In probing, the informa-\ntion richness of the representations is inspected by\nthe quality of a supervised model in predicting lin-\nguistic properties based only on the representations\n(K¨ohn, 2015; Gupta et al., 2015; Yaghoobzadeh and\nSch¨utze, 2016; Conneau et al., 2018; Tenney et al.,\n2019b,a; Yaghoobzadeh et al., 2019; Hewitt and\nManning, 2019; Zhao et al., 2020; Belinkov, 2022).\nHere, we apply probing to perform our study on\nwhether metaphorical knowledge is present in PLM\nrepresentations, and whether that is generalizable\nacross languages and datasets.\nA popular probing method introduced by Tenney\net al. (2019b) is edge probing (Figure 2). They\npropose a suite of span-level tasks, including POS\ntagging and coreference resolution. Despite the\nwidespread use of edge probing and other conven-\ntional probes, the question of whether the probing\nclassiﬁer is learning the task itself rather than iden-\ntifying the linguistic knowledge raises concerns.\nAn Information-theoretic view can solve this is-\nsue (V oita and Titov, 2020) by reformulating prob-\ning as a data transmission problem. They consider\nthe effort needed to extract linguistic knowledge in\naddition to the ﬁnal quality of the probe, showing\nthat this approach is more informative and robust\nthan normal probing methods. We employ both\nedge and MDL probing in this work.\nProbing multilingual PLMs. The application of\nprobing methods in NLP is extended to multilin-\ngual PLMs as well (Pires et al., 2019; Eichler et al.,\n2019; Ravishankar et al., 2019a,b; Choenni and\nShutova, 2020). Choenni and Shutova (2020) in-\ntroduce probing tasks for typological features of\nmultiple languages in multilingual PLMs. Ravis-\n2038\nhankar et al. (2019a,b) extend the probing tasks\nof Conneau et al. (2018), to a few other lan-\nguages. Pires et al. (2019) study the generaliza-\ntion of multilingual-BERT across languages when\nperforming cross-lingual downstream tasks. Here,\nas part of our study, we probe the generalization\nof metaphorical knowledge in XLM-R (Conneau\net al., 2020), a notable multilingual PLM.\nOut-of-distribution generalization. There has\nbeen no earlier work on studying or evaluating out-\nof-distribution generalization in metaphor detec-\ntion. This generalization refers to scenarios where\ntesting and training sets come from different distri-\nbutions (Duchi and Namkoong, 2018; Hendrycks\net al., 2021, 2020). Here, we have scenarios where\ntesting and training data are in different languages\nor domains / datasets. These are challenging eval-\nuation scenarios for the generalization of encoded\ninformation (metaphoricity in our case).\n3 Inspecting Metaphorical Knowledge in\nPLMs\nMetaphors are used frequently in our everyday lan-\nguage to convey our thoughts more clearly. There\nare related theories in linguistics and cognitive sci-\nence. Following linguistic theories, metaphoric-\nity is mostly annotated using metaphor identiﬁ-\ncation procedure (MIP). MIP identiﬁes a word\nin a given context as a metaphor if it has a ba-\nsic or literal meaning that contrasts with its con-\ntext words. Based on conceptual metaphor the-\nory (CMT) (Lakoff and Johnson, 2008), one target\ndomain (e.g., ARGUMENT) is explained using a\nsource domain (e.g., W AR). The source domain is\nusually more concrete or physical, while the tar-\nget is more abstract. Relating these two theories,\nmetaphors are expressed in language connecting\ntwo contrasting domains. For example, in “We\nwon the argument”, the domain of ARGUMENT\nis linked to the domain of W AR by using the word\n“won”. The word “won” is a “metaphor” here since\nits primary domain contrasts with its contextual\ndomain. The same word “won” in a sentence like\n“The Allies won the war” refers to its literal mean-\ning and therefore is not a metaphor. The task of\nmetaphor detection is deﬁned to do this classiﬁca-\ntion of “literal” and “metaphor”.\nAccordingly, when designing a metaphor detec-\ntion system, to ﬁgure out if a token is a metaphor\nin a particular context, we assume following a pro-\ncess like: (i) ﬁnding if the token has multiple mean-\nFigure 2: Probing architecture for metaphors em-\nployed in edge probing and MDL probing.\nings in different domains, including a more basic,\nconcrete, or body-related meaning. For example,\n“ﬁght”, “win” and “mother” satisfy this condition.\n(ii) ﬁnding if the source domain of the token con-\ntrasts with the target domain. Here the contrast\nis important and ﬁnding the exact domains might\nnot be necessary. The source domain, in which its\nliteral / basic meaning resides, is a non-contextual\nattribute, while the target domain is mainly found\nusing the contextual clues (W AR and ARGUMENT\nfor “won” in the above example).\nHere, we use the metaphor detection datasets\nannotated based on these theories and analyze\nthe PLM representations to see if they encode\nmetaphorical knowledge and if the encoding is\ngeneralizable. To do so, we ﬁrst probe PLMs for\ntheir metaphorical information, generally and also\nacross layers. This gives us intuition on how well\nmetaphoricity is encoded and how local or con-\ntextual that is. Then, we test if the knowledge\nof metaphor detection can be transferred across\nlanguages and if multilingual PLMs capture that.\nFinally, the generalization of metaphorical knowl-\nedge across datasets is examined to see if the theo-\nries and annotations followed by different datasets\nare consistent, and if PLMs learn generalizable\nknowledge rather than dataset artifacts.\n3.1 Probing\nHere, we aim to answer general questions about\nmetaphors in PLMs: do PLMs encode metaphori-\ncal information and, if so, how it is distributed in\n2039\ntheir layers. We do not attempt to achieve the best\nmetaphor detection results but to analyze layers of\nPLMs to test if they contain the necessary infor-\nmation to perform this task. In trying to answer\nthis question, we apply probing methods, discussed\nas follows, to focus on the representation itself by\nfreezing the PLM parameters and training classi-\nﬁers on top.\nWe hypothesize that metaphorical information\ndoes exist in PLM layers and more in the middle\nlayers. As we discussed earlier, metaphor detection\ndepends on contrast prediction between source and\ntarget domains of a token. We assume that this pre-\ndiction is made mainly based on the initial layers\nof PLM representations of either the token itself\nor its context or both. In higher layers of PLMs,\nthe representations are dominated by contextual\ninformation, making it hard to retrieve the source\ndomain, and so, reasoning about the contrast of the\nsource and target domains becomes difﬁcult.\nMethods We employ edge probing (Tenney et al.,\n2019b) and MDL (V oita and Titov, 2020). Edge\nprobing consists of a classiﬁer in which word repre-\nsentations obtained from PLMs are fed as inputs af-\nter projecting to 256-dimensional vectors ﬁrst. The\nquality of the classiﬁer illustrates how well the rep-\nresentations encode a speciﬁc linguistic knowledge.\nThis method is designed for span-level tasks, i.e.,\nthe classiﬁer can only access the representations\nof a limited part of the input sentence speciﬁed in\nthe dataset. Edge probing has two pooler sections\nfor making ﬁxed-sized vectors; one pools represen-\ntations across the words in the span and the other\npools representations across the layers.\nThe Minimum Description Length (MDL) prob-\ning is based on information theory and combines\nthe quality of the classiﬁer and the amount of effort\nneeded to achieve this quality.\nV oita and Titov (2020) propose two methods for\ncomputing MDL: “variational coding” and “online\ncoding.” The former computes the complexity of\nthe classiﬁer with a Bayesian model. In the latter,\nthe classiﬁer is trained gradually on different por-\ntions of the dataset, and the code length will be the\nsum of the cross-entropies, each for a data portion.\nV oita and Titov (2020) show that the two methods’\nresults are consistent with each other. Accordingly,\nwe opted for the “online coding” method since it is\nmore straightforward in implementation. Since the\ncode length is related to the size of the dataset N,\nwe report the “compression”, which is equal to 1\nfor a random classiﬁer and larger for better models,\nand is deﬁned as: compression = N·log2(K)\nMDL See\nextra details in V oita and Titov (2020).\n3.2 Generalization\nTo see if PLMs encode generalizable metaphori-\ncal knowledge, we evaluate them in settings where\ntesting and training data are in different distribu-\ntions. We explore transferability analysis across\nlanguages and datasets as two sources of distribu-\ntion. We explain each in the following sections.\n3.2.1 Cross-lingual\nMultilingual encoders project the representations\nin multiple languages into a shared space so that\nsemantically similar words and sentences across\nlanguages end up close to each other. If we use a\nmultilingual PLM model, and our classiﬁer shows\nthat representations in language S are informative\nabout metaphoricity, what happens if we apply this\nclassiﬁer to the representations in language T? We\nhypothesize that if the representation is rich in both\nlanguages, the annotation of metaphor is consis-\ntent, and the concept of metaphor is transferable\nacross languages, then the classiﬁer would be able\nto predict metaphoricity in language T from what\nit learns in S.\nWhen testing cross-lingual generalization, the\nlinguistic and cultural differences of metaphoricity\nare important as well. We assume that metaphors\nare conceptualized in a similar process across lan-\nguages, and metaphor detection is deﬁned consis-\ntently. The lexicalization is, of course, different,\nbut that is something that multilingual PLMs are\nsupposed to handle to some extent.\n3.2.2 Cross-dataset\nWhen training and testing on the same distribution,\nany learning model often uses heuristics and an-\nnotation biases. The consequence is the recurring\noverestimation of the capabilities of PLMs in doing\nhard tasks. This might be the case for our probing\nexperiments as well. Therefore, another generaliza-\ntion dimension we consider is cross-dataset trans-\nfer, i.e., training on dataset S and testing on dataset\nT. S and T could be annotated by different peo-\nple with possibly different goals in mind, and their\nraw sentences could come from different domains.\nHowever, they must be annotated for the same task\nof metaphor detection.\nIn our case, the four datasets discussed more\nin §4.1 differ in their distribution of the candidate\n2040\nVUA Verbs He [ﬁnds]1 it hard to communicate with people , not least his separated parents .→ 1\nHe ﬁnds it hard to [communicate]1 with people , not least his separated parents .→ 0\nVUA POS They picked up power from a[spider]1 ’s web of unsightly overhead wires .→ 1\nThey picked up power from a spider ’s web of unsightly overhead[wires]1 . → 0\nTroFi “ Locals[absorbed]1 a lot of losses , ” said Mr. Sandor of Drexel→ nonliteral\nVitamins could be passed right out of the body without being[absorbed]1 → literal\nLCC\nLawful gun ownership is not a[disease]1 . → 3.0\nBut the Supreme Court says it’s not a way to[hurt]1 the Second Amendment→ 2.0\nIs he angry that gun rights[progress]1 has been done without him?→ 1.0\nI mean the 2nd amendment[suggests]1 a level playing ﬁeld for all of us.→ 0.0\nTable 1: Examples of sentences, spans, and target labels for each probing dataset.\ndataset POS Sizes\nLCC (en) ALL 28,096 / 4,014 / 8,028\nLCC (fa) ALL 12,238 / 1,802 / 3,604\nLCC (es) ALL 12,238 / 2,236 / 4,474\nLCC (ru) ALL 12,238 / 1,748 / 3,498\nTroFi V 3,838 / 548 / 1,096\nVUA Verbs V 9,176 / 1,310 / 2,622\nVUA POS ALL 21,036 / 3,006 / 6,010\nTable 2: Statistics of the datasets. We label-balance\neach to have 50% metaphor. Number of instances\nfor train / dev / test sets and the types of POS are\ngiven as well. N: Noun, V: Verb, ALL: Noun, Verb,\nAdjective, Adverb.\nPOS types (e.g., TroFi is only verbs, but LCC is\nnot). Further, the annotation process is different\nas each follows its own guidelines. However, the\nessential task of metaphor detection, i.e., distin-\nguishing metaphor and literal usages, is the same\nfor all. Therefore, we expect some transferability\nacross datasets but with differences aligned with\ntheir mismatches.\n4 Experimental Setup and Results\n4.1 Datasets and Setup\nDatasets We use four metaphor detection\ndatasets in our study. The annotations of LCC\n(Mohler et al., 2016) are done mostly on web\ncrawled data as well as news corpora. It provides\nmetaphoricity scores including 0 as no , 2 as con-\nventional, and 3 as clear metaphor. 2 We use the\nexamples with score 0 as literal, and others as\nmetaphor.\nTroFi dataset (Birke and Sarkar, 2006) consists\n21 is weak metaphor and as Mohler et al. (2016) describe\nmetaphors with 0.5 ≤ score <1.5 as unclear, we ignore it.\nof metaphoric and literal usages of 51 English verbs\nfrom WSJ. VUA (Steen, 2010) corpus consists of\nwords in the academic, ﬁction, and news subdo-\nmains of the British National Corpus (BNC). The\nauthors published two versions: VUA POS and\nVUA Verbs.\nLCC contains annotations in four languages: En-\nglish, Russian, Spanish, and Farsi. The other three\ndatasets, TroFi, VUA Verbs and VUA POS, are\nin English only. We have label-balanced all the\ndatasets to get a more straightforward interpreta-\ntion of results (the accuracy of a fair-coin random\nbaseline is 50% in all cases) and have split the\ndatasets to train / dev / test sets with ratios of 0.7 /\n0.1 / 0.2.\nThe statistics of the datasets are shown in Ta-\nble 2. Example sentences with the corresponding\nannotations can be seen in Table 1.\nSetup In implementing the edge probe, we use\nbatch size = 32 and learning rate = 5e-5 and train for\nﬁve epochs in all experiments. For the MDL probe,\nthe same structure of edge probing is employed.\nWe apply a logarithm to the base two instead of the\nnatural logarithm in cross-entropy loss to have all\nthe obtained code lengths in bits (see extra details\nin V oita and Titov 2020). Our experiments are done\nusing the GPUs provided by Google Colab free and\npro.\n4.2 Probing Results\nHere, BERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019), and ELECTRA (Clark et al., 2020)\nrepresent our PLMs. Due to our resource limi-\ntations, we conduct all experiments on the base\nversion of the models (12 layers, 768 hidden size,\n110M parameters) implemented in HuggingFace’s\nTransfomers (Wolf et al., 2020). We employ edge\nprobing for evaluating overall metaphorical knowl-\n2041\nBaseline BERT RoBERTa ELECTRA\nDataset Acc. Comp. Acc. Comp. Acc. Comp. Acc. Comp.\nLCC (en) 74.86 1.05 2 88.25 1.85 6 88.06 1.96 5 89.30 2.05 5\nTroFi 67.34 1.01 4 68.58 1.074 68.46 1.096 68.07 1.08 3\nVUA POS 65.92 1.03 0 80.32 1.43 5 81.72 1.48 6 83.03 1.51 4\nVUA Verbs 65.97 1.04 9 78.29 1.28 9 78.88 1.345 79.96 1.314\nTable 3: Edge probing accuracy results for various metaphoricity datasets in BERT, RoBERTa, and\nELECTRA. Baseline is a randomly initialized BERT. The edge probing results are the average of three\nruns. The compression result is the best across layers, and the subscript denotes the best layer.\n1.4\n1.6\n1.8\n2\nLCC (en)\n1.01\n1.05\n1.09\nTroFi\n1.3\n1.4\n1.5\nVUA POS\n0 1 2 3 4 5 6 7 8 9 10 11 12\n1.1\n1.2\n1.3\nLayers\nVUA Verbs\nBERT\nRoBERTa\nELECTRA\nFigure 3: MDL compression across layers of three\nPLMs in four metaphor detection datasets. Higher\nnumber means better quality and extractability.\nedge in our selected PLMs, and MDL for the layer-\nwise comparisons. MDL is shown to be more effec-\ntive for layer-wise probing (Fayyaz et al., 2021).\nTable 3 shows the edge probing accuracy and\nMDL probing compression results for our three\nPLMs. Accordingly, RoBERTa and ELECTRA are\nshown to encode metaphorical knowledge better\nthan BERT on both metrics. This is consistent with\ntheir better performance on various tasks, acquired\nby having better pre-training objectives and / or en-\njoying more extensive pre-training data. The higher\nprobing quality of ELECTRA’s representations, is\nalso consistent with Fayyaz et al. (2021) results\non various linguistic knowledge tasks, including\ndependency labeling, named entity recognition, se-\nmantic role labeling, and coreference resolution.\nMDL probing compression across layers is\ndemonstrated in Figure 3. We see the numbers\nincrease mostly at the ﬁrst 3 to 6 layers, depend-\ning on the dataset, but it decreases afterwards 3.\nIn other words, metaphorical information is more\nconcentrated in the middle layers, where the repre-\nsentations are relatively contextualized but not as\nmuch as higher layers. To put this in perspective,\nwe can consider Tenney et al. (2019a) and Fayyaz\net al. (2021) where the best layers for various lin-\nguistic knowledge tasks in BERT are within 4 and\n9. This shows that metaphor detection in PLM\nrepresentations can be resolved earlier than some\nbasic linguistic tasks.\nIn §3.1, we elaborated a hypothesis that the pro-\ncess of detecting metaphors is not very deep since\nwhat it needs to do is mainly contrast prediction\nbetween source and target domains, and the deep\nlayers do not represent the source domain well. Our\nreported probing results conﬁrm that metaphor de-\ntection is not deep in PLM layers. To further evalu-\nate our reasoning, we probe the domain knowledge\nin PLM representations across layers. We employ\nLCC’s annotation of source and target domains,\nand run a similar MDL probing on different PLMs\nbut for domain prediction. The obtained results,\nshown in Figure A.1 in appendix, demonstrate that\nthe source domain information is represented in the\ninitial layers (2-6), conﬁrming that the source do-\nmain is dominated by other information in higher\nlayers. On the other hand, target domain informa-\ntion generally increases across layers. Therefore,\nthe middle layers can be the best place for contrast-\ning source and target domains.\n3For RoBERTa and in the case of TroFi and VUA Verbs,\nwe see exceptional increases in the last layers.\n2042\nTrain Lang\nen es fa ru\nTest Lang\nen 85.14 (65.37) 79.31 (52.71) 77.59 (50.22) 80.51 (52.40)\nes 79.40 (53.17) 84.59 (66.09) 76.70 (50.32) 79.68 (53.32)\nfa 75.70 (50.07) 75.29 (52.65) 81.04 (65.91) 77.14 (50.36)\nru 83.92 (53.25) 80.54 (51.48) 76.61 (51.05) 88.36 (67.98)\nTable 4: Cross-lingual metaphor detection accuracies after ﬁve epochs of training for XLM-R and (its\nrandom version). For each test language, we bold its in-distribution (e.g., en → en), and underline the best\nout-of-distribution (e.g., ru → en) numbers.\n4.3 Generalization Experiments\nAs our PLMs, we use XLM-R (Conneau et al.,\n2020) for cross-lingual and BERT for cross-dataset\nexperiments. To compare the cross-lingual and\ncross-dataset transferability, in §4.3.3, we employ\nthe same setup, including using XLM-R as PLM for\nboth. The results in §4.3.1 and 4.3.2 are not compa-\nrable. We apply the same edge probing architecture\nas in the probing experiments. We sometimes refer\nto both language and dataset as distribution.\nWe run two experiments for each case of a source\ndistribution S and a target distribution T: one with\nthe PLM and one with a randomized version of\nthe PLM where weights are set to random val-\nues. Randomly initialized Transformers with the\nsame architecture as PLMs are common baselines\nin the community. The difference between the\ntwo gives evidence about the helpfulness of the\nencoded knowledge gained during pre-training in\ndoing the task. When S = T, this effect is mea-\nsured for in-distribution and when S ̸= T, for\nout-of-distribution generalization. Comparing re-\nsults of in-distribution (e.g., training and testing on\nEnglish data) and out-of-distribution (e.g., training\non Spanish and testing on English) setups demon-\nstrates how generalizable the metaphorical knowl-\nedge in PLM is and how consistent the annotations\nare.\n4.3.1 Cross-lingual\nThe four LCC datasets corresponding to four lan-\nguages are used here. We subsample from the\ndatasets to have the same number of examples in\nthe training sets, i.e., 12,238 which is the size of\nthe Russian training set. The results are shown in\nTable 4. The random baseline is acquired using a\nrandomly initialized XLM-R.\nWe observe that XLM-R signiﬁcantly outper-\nforms the random, conﬁrming that metaphorical\nknowledge learned during the pre-training is trans-\nferable across languages. This considerable trans-\nferability can be attributed to the ability of XLM-R\nto build language-universal representations useful\nfor metaphoricity transfer. Moreover, the innate\nsimilarities of metaphors in distinct languages can\ncontribute to higher transferability, despite the lexi-\ncalization differences. E.g., analogizing a concept\nto a tool (en) occurs the same way in other lan-\nguages like instrumento (es), /afii57425.isol/afii57415.isol/afii57426.fina/afii57416.init/afii57415.isol(fa) and инстру-\nмент(ru). Finally, the constraints of the dataset\nproducers in, for instance, keeping the languages in\nrelatively similar target and source domains, could\nbe inﬂuential. (See Figures A.2 and A.3).\nAn interesting observation is that training on\nRussian shows the best out-of-distribution results\nwhen testing on other languages. We analyze this\nfurther. First, we observe that LCC(ru) has almost\nthe closest target domain distribution to all other\nlanguages (See Table A.2 in Appendix).\nSecond, the reported results can also be inﬂu-\nenced by the amount of data from each of these\nlanguages in the pre-training data of XLM-R. Rus-\nsian has the second largest size after English (Con-\nneau et al., 2020). Finally, for English, the higher-\nresource language with closer target domain distri-\nbution, we ﬁnd out that there are considerable num-\nber of examples in the LCC(en) related to “GUNS”\nand “CONTROL OF GUNS”. These domains are\nnot covered in other LCC datasets (See Figure A.3\nin Appendix).\n4.3.2 Cross-dataset\nSimilar to the cross-lingual evaluations, here we\nhave four datasets used as sources and targets. We\nset the train size of each to the minimum of all, i.e.,\n3,838. For each pair, we run two experiments: one\nwith randomized and one with pre-trained BERT\nas our PLM. Results are shown in Table 5.\n2043\nTrain Dataset\nLCC(en) TroFi VUA POS VUA Verbs\nTest Dataset\nLCC(en) 84.26 (54.93) 62.04 (50.05) 70.35 (50.69) 70.37 (50.14)\nTroFi 59.49 (50.58) 68.73 (64.96) 55.38 (49.45) 59.67 (53.68)\nVUA POS 62.23 (51.47) 55.29 (50.47) 76.86 (56.01) 71.6 (53.47)\nVUA Verbs 60.20 (50.88) 54.55 (51.73) 72.6 (56.01) 75.21 (60.03)\nTable 5: Cross-dataset edge probing accuracy results on BERT is shown in pairs: pre-trained model\nand, in the parenthesis, the randomly initialized model. We set the training size to the minimum among\ndatasets, i.e., TroFi. For each test dataset, we bold its in-distribution (e.g., VUA Verbs→ VUA Verbs),\nand underline the best out-of-distribution (e.g., VUA POS → VUA Verbs) numbers.\nPLM is much better than random in all out-\nof-distribution cases, suggesting the presence of\ngeneralizable metaphorical information. As ex-\npected, VUA Verbs and POS achieve the best re-\nsults when mutually tested, because, apart from\nthe POS, they have the same distribution. VUA\ndatasets and LCC(en) show good transferability,\nbut the gap with in-distribution results is still con-\nsiderable ( >13% absolute). VUA Verbs is the\nbest source for TroFi, likely because of the POS\nmatch between them. Overall, apart from the two\nVUA datasets, the gap between in- and out-of-\ndistribution performance is large.\nThe random PLM accuracies range from about\n54%-64% and 50%-56% for in- and out-of-\ndistribution cases. We hypothesize that this drop in\nthe out-of-distribution is related to the annotation\nbiases, which a randomly initialized classiﬁer can\nleverage better when testing and training sets are\nfrom the same distribution. When the sets have\ndifferent distributions, the biases do not transfer\nwell.\n4.3.3 Comparing cross-dataset and\ncross-lingual\nLCC(en) LCC(es) LCC(fa) LCC(ru)\n82.31 78.02 77.3 78.04\nTroFi VUA POS VUA Verbs\n60.54 68.61 67.15\nTable 6: Comparing cross-dataset and cross-lingual\nscenarios using the same model (XLM-R), train-\ning size, testing set, i.e., LCC(en), and different\ntraining sources.\nAs additional transferability analysis, we com-\npare cross-lingual and cross-dataset results, by\nusing XLM-R and evaluating different training\nsources on LCC(en) test set. We make the size\nof each train set to be the same (3,838). The results\nare shown in Table 6, where the ﬁrst and second\nrows belong to cross-lingual and cross-dataset, re-\nspectively. To base our results, we include the\nin-distribution result of training on LCC(en), i.e.,\n82.31%.\nClearly, there is a substantial gap between cross-\nlingual and cross-dataset accuracies. The annota-\ntion guideline is consistent in the LCC language\ndatasets, while for the cross-dataset settings, we\nhave datasets that differ in many aspects, including\nannotation procedure and deﬁnitions, covered part-\nof-speeches (e.g., Troﬁ and VUA Verbs vs. LCC\nand VUA POS) and sentence lengths (LCC: 25.9,\nVUA: 19.4, Troﬁ: 28.3).\n5 Discussion and Conclusion\nMetaphors are important in human cognition, and\nif we seek to build cognitively inspired or plausi-\nble language understanding systems, we need to\nwork more on their best integration in the future.\nTherefore, any work in this regard is impactful.\nOur probing experiments showed that PLMs do\nin fact represent the information necessary to do\nthe task of metaphor detection. We assume this\ninformation is related to metaphorical knowledge\nlearned during pre-training. Further, the layer-wise\nanalysis conﬁrmed our hypothesis that middle lay-\ners are more informative.\nEven though our probing experiments did show\nthat metaphorical knowledge is present in PLMs,\nit was still unclear if this knowledge is generaliz-\nable beyond the training data. So, to probe the\nprobe and evaluate generalization, we ran cross-\nlingual and cross-dataset experiments. Our results\nshowed that the transferability across languages\n2044\nworks quite well for the four languages in LCC an-\nnotation. However, when the deﬁnitions and anno-\ntations were inconsistent across different datasets,\nthe cross-dataset results were not satisfactory.\nOverall, we conclude that metaphorical knowl-\nedge does exist in PLM representations and in mid-\ndle layers mainly, and it is transferable if the an-\nnotation is consistent across training and testing\ndata. We will explore more the cross-lingual trans-\nfer of metaphors and the impact of cross-cultural\nsimilarities in the future. Also, the application\nof metaphorical knowledge for text generation is\nsomething important that we will address.\nAcknowledgements\nWe would like to thank the anonymous reviewers\nand action editors who helped us greatly in improv-\ning our work with their comments.\nReferences\nYonatan Belinkov. 2022. Probing Classiﬁers:\nPromises, Shortcomings, and Advances. Computa-\ntional Linguistics, pages 1–13.\nJulia Birke and Anoop Sarkar. 2006. A clustering ap-\nproach for nearly unsupervised recognition of non-\nliteral language. In 11th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, Trento, Italy. Association for Computa-\ntional Linguistics.\nJulia Birke and Anoop Sarkar. 2007. Active learn-\ning for the identiﬁcation of nonliteral language. In\nProceedings of the Workshop on Computational\nApproaches to Figurative Language , pages 21–28,\nRochester, New York. Association for Computa-\ntional Linguistics.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, Erik Brynjolfsson, Shya-\nmal Buch, Dallas Card, Rodrigo Castellon, Ni-\nladri Chatterji, Annie S. Chen, Kathleen Creel,\nJared Quincy Davis, Dorottya Demszky, Chris Don-\nahue, Moussa Doumbouya, Esin Durmus, Stefano\nErmon, John Etchemendy, Kawin Ethayarajh, Li Fei-\nFei, Chelsea Finn, Trevor Gale, Lauren Gillespie,\nKaran Goel, Noah D. Goodman, Shelby Grossman,\nNeel Guha, Tatsunori Hashimoto, Peter Henderson,\nJohn Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu,\nJing Huang, Thomas Icard, Saahil Jain, Dan Juraf-\nsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff\nKeeling, Fereshte Khani, Omar Khattab, Pang Wei\nKoh, Mark S. Krass, Ranjay Krishna, Rohith Kudi-\ntipudi, and et al. 2021. On the opportunities and\nrisks of foundation models. CoRR, abs/2108.07258.\nXianyang Chen, Chee Wee (Ben) Leong, Michael\nFlor, and Beata Beigman Klebanov. 2020. Go ﬁg-\nure! multi-task transformer-based architecture for\nmetaphor detection using idioms: ETS team in 2020\nmetaphor shared task. In Proceedings of the Sec-\nond Workshop on Figurative Language Processing ,\npages 235–243, Online. Association for Computa-\ntional Linguistics.\nRochelle Choenni and Ekaterina Shutova. 2020. What\ndoes it mean to be language-agnostic? probing mul-\ntilingual sentence encoders for typological proper-\nties. CoRR, abs/2009.12862.\nMinjin Choi, Sunkyung Lee, Eunseong Choi, Heesoo\nPark, Junhyuk Lee, Dongwon Lee, and Jongwuk\nLee. 2021. Melbert: Metaphor detection via con-\ntextualized late interaction using metaphorical iden-\ntiﬁcation theories. In Proceedings of the 2021 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2021, Online,\nJune 6-11, 2021, pages 1763–1773. Association for\nComputational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Prob-\ning sentence embeddings for linguistic properties.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 2126–2136. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJohn C. Duchi and Hongseok Namkoong. 2018.\nLearning models with uniform performance\nvia distributionally robust optimization. CoRR,\nabs/1810.08750.\n2045\nMax Eichler, G ¨ozde G ¨ul S ¸ahin, and Iryna Gurevych.\n2019. LINSPECTOR WEB: A multilingual prob-\ning suite for word representations. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP): System Demonstrations,\npages 127–132, Hong Kong, China. Association for\nComputational Linguistics.\nMohsen Fayyaz, Ehsan Aghazadeh, Ali Modarressi,\nHosein Mohebbi, and Mohammad Taher Pilehvar.\n2021. Not all models localize linguistic knowl-\nedge in the same place: A layer-wise probing on\nBERToids’ representations. In Proceedings of the\nFourth BlackboxNLP Workshop on Analyzing and\nInterpreting Neural Networks for NLP , pages 375–\n388, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nHongyu Gong, Kshitij Gupta, Akriti Jain, and Suma\nBhat. 2020. IlliniMet: Illinois system for metaphor\ndetection with contextual and linguistic information.\nIn Proceedings of the Second Workshop on Figura-\ntive Language Processing , pages 146–153, Online.\nAssociation for Computational Linguistics.\nAbhijeet Gupta, Gemma Boleda, Marco Baroni, and\nSebastian Pad´o. 2015. Distributional vectors encode\nreferential attributes. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 12–21, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nDan Hendrycks, Steven Basart, Norman Mu, Saurav\nKadavath, Frank Wang, Evan Dorundo, Rahul De-\nsai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn\nSong, Jacob Steinhardt, and Justin Gilmer. 2021.\nThe many faces of robustness: A critical analysis of\nout-of-distribution generalization. In Proceedings\nof the IEEE/CVF International Conference on Com-\nputer Vision (ICCV), pages 8340–8349.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2744–2751, Online. Association for Computa-\ntional Linguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nArne K¨ohn. 2015. What’s in an embedding? analyzing\nword embeddings through multilingual evaluation.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2067–2073, Lisbon, Portugal. Association for Com-\nputational Linguistics.\nGeorge Lakoff and Mark Johnson. 2008. Metaphors\nwe live by. University of Chicago press.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong\nWen. 2021. Pretrained language model for text gen-\neration: A survey. In Proceedings of the Thirti-\neth International Joint Conference on Artiﬁcial Intel-\nligence, IJCAI-21 , pages 4492–4499. International\nJoint Conferences on Artiﬁcial Intelligence Organi-\nzation. Survey Track.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nRui Mao, Chenghua Lin, and Frank Guerin. 2019. End-\nto-end sequential metaphor identiﬁcation inspired by\nlinguistic theories. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3888–3898, Florence, Italy. Asso-\nciation for Computational Linguistics.\nZachary J. Mason. 2004. CorMet: A computational,\ncorpus-based conventional metaphor extraction sys-\ntem. Computational Linguistics, 30(1):23–44.\nShervin Minaee, Nal Kalchbrenner, Erik Cambria, Nar-\njes Nikzad, Meysam Chenaghlu, and Jianfeng Gao.\n2020. Deep learning based text classiﬁcation: A\ncomprehensive review. CoRR, abs/2004.03705.\nMichael Mohler, Mary Brunson, Bryan Rink, and\nMarc T. Tomlinson. 2016. Introducing the LCC\nmetaphor datasets. In Proceedings of the Tenth In-\nternational Conference on Language Resources and\nEvaluation LREC 2016, Portoroˇz, Slovenia, May 23-\n28, 2016 . European Language Resources Associa-\ntion (ELRA).\nJinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue,\nVinay Adiga, and Erik Cambria. 2021. Recent ad-\nvances in deep learning based dialogue systems: A\nsystematic survey. CoRR, abs/2105.04387.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nVinit Ravishankar, Memduh G ¨okırmak, Lilja Øvrelid,\nand Erik Velldal. 2019a. Multilingual probing of\n2046\ndeep pre-trained contextual encoders. In Proceed-\nings of the First NLPL Workshop on Deep Learn-\ning for Natural Language Processing , pages 37–\n47, Turku, Finland. Link¨oping University Electronic\nPress.\nVinit Ravishankar, Lilja Øvrelid, and Erik Velldal.\n2019b. Probing multilingual sentence representa-\ntions with X-probe. In Proceedings of the 4th\nWorkshop on Representation Learning for NLP\n(RepL4NLP-2019), pages 156–168, Florence, Italy.\nAssociation for Computational Linguistics.\nEkaterina Shutova, Douwe Kiela, and Jean Maillard.\n2016. Black holes and white rabbits: Metaphor iden-\ntiﬁcation with visual features. In Proceedings of the\n2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies , pages 160–170, San\nDiego, California. Association for Computational\nLinguistics.\nEkaterina Shutova, Simone Teufel, and Anna Korho-\nnen. 2013. Statistical metaphor processing. Compu-\ntational Linguistics, 39(2):301–353.\nWei Song, Shuhui Zhou, Ruiji Fu, Ting Liu, and\nLizhen Liu. 2021. Verb metaphor detection via\ncontextual relation learning. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Vir-\ntual Event, August 1-6, 2021, pages 4240–4251. As-\nsociation for Computational Linguistics.\nGerard Steen. 2010. A method for linguistic metaphor\nidentiﬁcation: From MIP to MIPVU , volume 14.\nJohn Benjamins Publishing.\nChuandong Su, Fumiyo Fukumoto, Xiaoxi Huang, Jiyi\nLi, Rongbo Wang, and Zhiqun Chen. 2020. Deep-\nMet: A reading comprehension paradigm for token-\nlevel metaphor detection. In Proceedings of the Sec-\nond Workshop on Figurative Language Processing ,\npages 30–39, Online. Association for Computational\nLinguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019b. What do you\nlearn from context? Probing for sentence structure\nin contextualized word representations. In Interna-\ntional Conference on Learning Representations.\nYulia Tsvetkov, Leonid Boytsov, Anatole Gershman,\nEric Nyberg, and Chris Dyer. 2014. Metaphor detec-\ntion with cross-lingual model transfer. In Proceed-\nings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics, ACL 2014, June 22-\n27, 2014, Baltimore, MD, USA, Volume 1: Long Pa-\npers, pages 248–258. The Association for Computer\nLinguistics.\nPeter Turney, Yair Neuman, Dan Assaf, and Yohai Co-\nhen. 2011. Literal and metaphorical sense identiﬁca-\ntion through concrete and abstract context. In Pro-\nceedings of the 2011 Conference on Empirical Meth-\nods in Natural Language Processing , pages 680–\n690, Edinburgh, Scotland, UK. Association for Com-\nputational Linguistics.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description length.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 183–196, Online. Association for Computa-\ntional Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nChuhan Wu, Fangzhao Wu, Yubo Chen, Sixing Wu,\nZhigang Yuan, and Yongfeng Huang. 2018. Neu-\nral metaphor detecting with CNN-LSTM model. In\nProceedings of the Workshop on Figurative Lan-\nguage Processing , pages 110–114, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nYadollah Yaghoobzadeh, Katharina Kann, T. J. Hazen,\nEneko Agirre, and Hinrich Sch ¨utze. 2019. Probing\nfor semantic classes: Diagnosing the meaning con-\ntent of word embeddings. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5740–5753, Florence,\nItaly. Association for Computational Linguistics.\nYadollah Yaghoobzadeh and Hinrich Sch ¨utze. 2016.\nIntrinsic subspace evaluation of word embedding\nrepresentations. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 236–\n246, Berlin, Germany. Association for Computa-\ntional Linguistics.\nZhuosheng Zhang, Hai Zhao, and Rui Wang. 2020.\nMachine reading comprehension: The role of con-\ntextualized language models and beyond. CoRR,\nabs/2005.06249.\nMengjie Zhao, Philipp Dufter, Yadollah\nYaghoobzadeh, and Hinrich Sch¨utze. 2020. Quanti-\nfying the contextualization of word representations\n2047\nwith semantic class probing. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 1219–1234, Online. Association for\nComputational Linguistics.\nA Appendices\n1.8\n2\n2.2\n2.4\n2.6\n2.8\n3\n3.2\nSource Detection\n0 1 2 3 4 5 6 7 8 9 10 11 12\n1\n1.5\n2\n2.5\n3\nLayers\nTarget Detection\nBERT\nRoBERTa\nELECTRA\nFigure A.1: MDL probing compression across\nlayers for source and target domain detection for\nLCC(en) dataset.\nen es fa ru\nen 0.0000\nes 0.1622 0.0000\nfa 0.1851 0.1688 0.0000\nru 0.1833 0.2239 0.2244 0.0000\nTable A.1: Jensen–Shannon divergence between\nsource domain frequency distribution of different\nlanguages. The datasets are the same ones used in\ncross-lingual experiments where train set sizes are\nset to 12,238. Bold denotes the closest distributions\nand underline denotes the furthest distributions.\nen es fa ru\nen 0.0000\nes 0.4116 0.0000\nfa 0.5004 0.2148 0.0000\nru 0.4291 0.1209 0.2141 0.0000\nTable A.2: Jensen–Shannon divergence between\ntarget domain frequency distribution of different\nlanguages. The datasets are the same ones used in\ncross-lingual experiments where train set sizes are\nset to 12,238. Bold denotes the closest distributions\nand underline denotes the furthest distributions.\n2048\nLanguage Sentence Annotations\nfaafii57446.isol/afii57415.fina/afii57418.medi/afii57427.medi/afii57446.init/afii57415.fina/afii57434.medi/afii57441.init/afii57415.isol /afii57425.isol/afii57423.isol /afii57415.fina/uni06A9.medi/uni06CC.init/afii57425.fina/afii57445.init/afii57415.isol /afii57415.fina/afii57445.init/afii57415.isol\n. /afii57418.fina/afii57427.init/afii57415.isol /afii57470.isol/afii57423.fina/afii57445.init/afii57410.isol2] /uni06CC.fina/afii57427.init/afii57415.isol/afii57425.fina/uni06A9.init/afii57448.fina/afii57445.init/afii57423.isol [1] /afii57421.isol/afii57444_afii57415.fina/afii57427.init [\nScore: 3.0\nSrc Concept: W AR(3.0)\nTarget Concept: DEMOCRACY\nPolarity: NEUTRAL\nIntensity: 1.0\nes [atorado]1 en la [deuda]2 p´ublica\ny sin avances en Estado de Derecho\nScore: 3.0\nSrc Concept: BARRIER(3.0)\nTarget Concept: DEBT\nPolarity: NEGATIVE\nIntensity: 2.0\nru Мировые [деньги]2 [мечутся]1 ,\nне зная , куда вложиться .\nScore: 3.0\nSrc Concept: MOVEMENT(3.0)\nTarget Concept: MONEY\nPolarity: NEGATIVE\nIntensity: 2.0\nTable A.3: Examples of sentences, spans, and annotations for LCC dataset in Farsi, Spanish, and Russian.\n2049\nFigure A.2: Source domain frequency in training\nset of cross-lingual datasets.\nFigure A.3: Target domain frequency in training\nset of cross-lingual datasets.\n2050"
}