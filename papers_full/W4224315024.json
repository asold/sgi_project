{
  "title": "LitMC-BERT: Transformer-Based Multi-Label Classification of Biomedical Literature With An Application on COVID-19 Literature Curation",
  "url": "https://openalex.org/W4224315024",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2115254388",
      "name": "Qing-Yu Chen",
      "affiliations": [
        "National Institutes of Health",
        "National Center for Biotechnology Information"
      ]
    },
    {
      "id": "https://openalex.org/A2343127410",
      "name": "Jingcheng Du",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2303193765",
      "name": "Alexis Allot",
      "affiliations": [
        "National Institutes of Health",
        "National Center for Biotechnology Information"
      ]
    },
    {
      "id": "https://openalex.org/A2150237561",
      "name": "Zhiyong Lu",
      "affiliations": [
        "National Institutes of Health",
        "National Center for Biotechnology Information"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2123579157",
    "https://openalex.org/W3040692138",
    "https://openalex.org/W3036605699",
    "https://openalex.org/W3097438085",
    "https://openalex.org/W3009926341",
    "https://openalex.org/W3105753785",
    "https://openalex.org/W6779959438",
    "https://openalex.org/W3047932682",
    "https://openalex.org/W3126293031",
    "https://openalex.org/W2168905447",
    "https://openalex.org/W2096951189",
    "https://openalex.org/W4244393794",
    "https://openalex.org/W2114315281",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W3102793471",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2767245172",
    "https://openalex.org/W2137484796",
    "https://openalex.org/W1999954155",
    "https://openalex.org/W1491576965",
    "https://openalex.org/W3103184668",
    "https://openalex.org/W2900758626",
    "https://openalex.org/W1576514601",
    "https://openalex.org/W3157132663",
    "https://openalex.org/W4244771797",
    "https://openalex.org/W2914171828",
    "https://openalex.org/W3017463390",
    "https://openalex.org/W2034269086",
    "https://openalex.org/W2515248967",
    "https://openalex.org/W2944400536",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6780541474",
    "https://openalex.org/W3106092787",
    "https://openalex.org/W6775071770",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3150641585",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W6752178629",
    "https://openalex.org/W2883255752",
    "https://openalex.org/W6756600319",
    "https://openalex.org/W2965851497",
    "https://openalex.org/W3085973060",
    "https://openalex.org/W6922433603",
    "https://openalex.org/W6768851824",
    "https://openalex.org/W4200445293",
    "https://openalex.org/W3093259129",
    "https://openalex.org/W3011296786",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2989987546",
    "https://openalex.org/W3102503200",
    "https://openalex.org/W2804823968",
    "https://openalex.org/W3011499675",
    "https://openalex.org/W3037426344",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3037129827",
    "https://openalex.org/W3161987470"
  ],
  "abstract": "The rapid growth of biomedical literature poses a significant challenge for curation and interpretation. This has become more evident during the COVID-19 pandemic. LitCovid, a literature database of COVID-19 related papers in PubMed, has accumulated over 200,000 articles with millions of accesses. Approximately 10,000 new articles are added to LitCovid every month. A main curation task in LitCovid is topic annotation where an article is assigned with up to eight topics, e.g., Treatment and Diagnosis. The annotated topics have been widely used both in LitCovid (e.g., accounting for ∼18% of total uses) and downstream studies such as network generation. However, it has been a primary curation bottleneck due to the nature of the task and the rapid literature growth. This study proposes LITMC-BERT, a transformer-based multi-label classification method in biomedical literature. It uses a shared transformer backbone for all the labels while also captures label-specific features and the correlations between label pairs. We compare LITMC-BERT with three baseline models on two datasets. Its micro-F1 and instance-based F1 are 5% and 4% higher than the current best results, respectively, and only requires ∼18% of the inference time than the Binary BERT baseline. The related datasets and models are available via https://github.com/ncbi/ml-transformer.",
  "full_text": "LitMC-BERT: Transformer-Based Multi-Label\nClassiﬁcation of Biomedical Literature With An\nApplication on COVID-19 Literature Curation\nQingyu Chen , Jingcheng Du, Alexis Allot, and Zhiyong Lu\nAbstract— The rapid growth of biomedical literature poses a signiﬁcant challenge for curation and interpretation. This has become\nmore evident during the COVID-19 pandemic. LitCovid, a literature database of COVID-19 related papers in PubMed, has accumulated\nover 200,000 articles with millions of accesses. Approximately 10,000 new articles are added to LitCovid every month. A main curation\ntask in LitCovid is topic annotation where an article is assigned with up to eight topics, e.g., Treatment and Diagnosis. The annotated\ntopics have been widely used both in LitCovid (e.g., accounting for/C24 18% of total uses) and downstream studies such as network\ngeneration. However, it has been a primary curation bottleneck due to the nature of the task and the rapid literature growth. This study\nproposes LITMC-BERT , a transformer-based multi-label classiﬁcation method in biomedical literature. It uses a shared transformer\nbackbone for all the labels while also captures label-speciﬁc features and the correlations between label pairs. We compare LITMC-\nBERT with three baseline models on two datasets. Its micro-F1 and instance-based F1 are 5% and 4% higher than the current best\nresults, respectively, and only requires/C24 18% of the inference time than the Binary BERT baseline. The related datasets and models\nare available via https://github.com/ncbi/ml-transformer .\nIndex Terms— Biomedical text mining, COVID-19, multi-label classiﬁcation, BERT , transformer\nÇ\n1I NTRODUCTION\nT\nHE rapid growth of biomedical literature signiﬁcantly\nchallenges manual curation and interpretation [1], [2].\nThese challenges have become more evident under the con-\ntext of the COVID-19 pandemic. Speciﬁcally, the median\nacceptance time of COVID-19 papers is about 2-time and\n20-time faster than the acceptance time for papers about\nEbola and cardiovascular disease [3]. The number of articles\nin the literature related to COVID-19 is growing by about\n10000 articles per month [4]. LitCovid [5], [6], a literature\ndatabase of COVID-19 related papers in PubMed, has accu-\nmulated a total of more than 100000 articles, with millions\nof accesses each month by users worldwide. LitCovid is\nupdated daily, and this rapid growth signiﬁcantly increases\nthe burden of manual curation. In particular, annotating\neach article with up to eight possible topics, e.g., Treatment\nand Diagnosis, has been a bottleneck in the LitCovid cura-\ntion pipeline. Fig. 1. shows the characteristics of topic anno-\ntations in LitCovid; we will explain the related curation\nprocess below. The annotated topics have been used both in\nLitCovid directly and downstream studies widely. For\ninstance, topic-related searching and browsing account for\nover 18% of LitCovid user behaviors [6], and the topics\nhave also been used downstream studies such as citation\nanalysis and knowledge network generation. application\n[7], [8], [9]. Therefore, it is important to develop automatic\nmethods to overcome this issue.\nInnovative text mining tools have been developed to\nfacilitate biomedical literature curation for over two decades\n[2], [10], [11], [12]. Topic annotation in LitCovid is a stan-\ndard multi-label classiﬁcation task, which aims to assign\none or more labels to each article [13]. To facilitate manual\ntopic annotation, we previously employed the deep learn-\ning model Bidirectional Encoder Representations from\nTransformers (BERT) [14]. We used one BERT model per\ntopic, known as Binary BERT, and previously demonstrated\nthis method achieved the best performance of the available\nmodels for LitCovid topic annotations [6]; other studies\nhave reported consistent results [15]. Indeed, existing stud-\nies in biomedical text mining have demonstrated Binary\nBERT achieves the best performance in multi-label classiﬁ-\ncation tasks [16], [17]. However, this method has two pri-\nmary limitations. First, by training each topic individually,\nthe model ignores the correlation between topics, especially\nfor topics that often co-occur, biasing the predictions and\nreducing generalization capability. Second, using eight\nmodels signiﬁcantly increases the inference time, causing\nthe daily curation in LitCovid to require signiﬁcant compu-\ntational resources.\nTo this end, this paper proposes a LITMC-BERT, a trans-\nformer-based multi-label classiﬁcation method in biomedi-\ncal literature. It uses a shared BERT backbone for all the\n/C15 Qingyu Chen, Alexis Allot, and Zhiyong Lu are with the National Center\nfor Biotechnology Information (NCBI), National Library of Medicine\n(NLM), National Institutes of Health (NIH), Bethesda, MD 20892 USA.\nE-mail: {qingyu.chen, alexis.allot, zhiyong.lu}@nih.gov.\n/C15 Jingcheng Du is with the School of Biomedical Informatics, UT Health,\nHouston, TX 77030 USA. E-mail: jingcheng.du@uth.tmc.edu.\nManuscript received 11 March 2021; revised 19 April 2022; accepted 22 April\n2022. Date of publication 10 May 2022; date of current version 7 October 2022.\nThis work was supported by the NIH Intramural Research Program, National\nLibrary of Medicine.\n(Corresponding author: Zhiyong Lu.)\nDigital Object Identiﬁer no. 10.1109/TCBB.2022.3173562\n2584 IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. 19, NO. 5, SEPTEMBER/OCTOBER 2022\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nlabels while also captures label-speciﬁc features and the cor-\nrelations between label pairs. It also leverages multi-task\ntraining and label-speciﬁc ﬁne-tuning to further improve\nthe label prediction performance. We compared LITMC-\nBERT to three baseline methods using two sets of evaluation\nmetrics (label-based and example-based) commonly used\nfor multi-label classiﬁcation on two datasets: the LitCovid\ndataset consists of over 30000 articles and the HoC (the Hall-\nmarks of Cancers) dataset consists of over 1500 articles (the\nonly benchmark dataset for multi-label classiﬁcation meth-\nods in biomedical text mining).. It achieved the highest per-\nformance on both datasets: its instance-based F1 and\naccuracy are 3 and 8% higher than the BERT baselines,\nrespectively. Importantly, it also achieves the SOTA perfor-\nmance in the HoC dataset: its micro-F1 and instance-based\nF1 are 5% and 4% higher than the current best results\nreported in the literature, respectively. In addition, it\nrequires only/C24 15% of the inference time needed for Binary\nBERT, signiﬁcantly improving the inference efﬁciency.\nLITMC-BERT has been employed in the LitCovid produc-\ntion system, making the curation more sustainable. We also\nmake the related datasets, codes, and models available via\nhttps://github.com/ncbi/ml-transformer.\n2R ELATED BACKGROUND\n2.1 Multi-Label Classiﬁcation\nMulti-label classiﬁcation is a standard machine learning\ntask where each instance is assigned with one or more\nlabels. Multi-label classiﬁcation methods can be categorized\ninto two broad groups [18]: problem transformation, trans-\nforming the multi-label classiﬁcation problem into relatively\nsimpler problems such as single-label classiﬁcation and\nalgorithm adaptation, adapting the methods (such as chang-\ning the loss function) for multi-label data.\nThe methods under the problem transformation group\nare traditional approaches to address multi-label classiﬁca-\ntion. Most popular methods include (1) binary relevance,\nwhere each label requires to train a corresponding binary\nclassiﬁcation model [19], (2) label powerset, where a binary\nclassiﬁcation model is trained for every combination of the\nlabels [20], and (3) classiﬁer chains, where the output of a\nbinary classiﬁcation model is used as the input to train a fur-\nther binary classiﬁcation model [21]. Such methods have\nachieved promising performance in a range of multi-label\nclassiﬁcation tasks [13], [22]. Indeed, existing studies have\nshown binary relevance BERT achieved the best perfor-\nmance for topic annotation in LitCovid [6], [15]. However, it\nis computationally expensive and transforming multi-label\nclassiﬁcation tasks into binary classiﬁcation may ignore the\ncorrelations among labels. Recently, an increasing number\nof deep learning methods under the algorithm adaptation\ngroup have been proposed which predict all the labels\ndirectly as the output [23], [24], ]25].\n2.1.1 Multi-Label Text Classiﬁcation in the Domain of\nBiomedical Text Mining\nText classiﬁcation methods have been widely applied in bio-\nmedical text mining for biomedical document triage [26],\nretrieval [27], and curation [28]. Compared to the general\ndomain, text classiﬁcation especially for multi-label classiﬁ-\ncation in biomedical text mining has three primary chal-\nlenges: (1) domain-speciﬁc language ambiguity, e.g., a gene\nmay have over 10 different synonyms mentioned in the lit-\nerature; conversely, a gene and a chemical could share the\nsame name [29]; (2) limited benchmark datasets for method\ndevelopment and validation; e.g., the HoC dataset [30] is\nthe only multi-label text dataset among commonly-used\nbenchmark datasets for biomedical text mining [16], [17]\nand it only has about 1500 PubMed documents; and (3)\ndeployment difﬁculty, i.e., an important contribution of bio-\nmedical text mining is to make open-source tools and serv-\ners such that biomedical researchers can readily apply.\nTherefore, the designed methods should be scalable to mas-\nsive biomedical literature and are also efﬁcient in standard\nresearch tool production settings where computational\nresources are limited such as the graphics processing unit\n(GPU) is not commonly available for method deployment.\nSuch challenges impact the method development in bio-\nmedical text mining. Most of the existing methods focus on\ntransfer learning which employs word embeddings (such as\nWordVec) or transformer-based models (such as BERT) that\nare pre-trained in biomedical corpora to extract text repre-\nsentations [31], [32], [33], [34]. Indeed, this also applies to\nthe multi-label classiﬁcation methods in biomedical text\nmining. Existing studies have mostly used Binary BERT\n[16], [17] for multi-label classiﬁcation: for each label, it trains\na corresponding BERT (or other types of transformers) clas-\nsiﬁcation model. The evaluation results show that the\nFig. 1. Characteristics of topic annotations in LitCovid up to September 2021. (A) shows the frequencies of topics; (B) demonstrates topic co-occur-\nrences; and (C) illustrates the distributions of the number of topics assigned per document. The ﬁgure is adapted from [37].\nCHEN ET AL.: LITMC-BERT: TRANSFORMER-BASED MULTI-LABEL CLASSIFICATION OF BIOMEDICAL LITERATURE WITH AN... 2585\nBERT-related approaches achieve the best performance for\nmulti-label classiﬁcation in biomedical text mining com-\npared to other multi-label classiﬁcation methods that have\nbeen used in the general domain [35], [36].\n2.2 LitCovid Curation Pipeline\nA primary focus of this study is to develop a multi-label\nclassiﬁcation method to facilitate COVID-19 literature cura-\ntion such as topic annotation in LitCovid. Here we summa-\nrize the topic annotation in the LitCovid curation pipeline\nand its challenges.\nThe detailed LitCovid curation pipeline is summarized\nin [6]. For topic annotation, an article in LitCovid is consid-\nered for one or more of eight topics (General information,\nMechanism, Transmission, Diagnosis, Treatment, Preven-\ntion, Case report, or Epidemic forecasting) when applicable.\nFig. 1. shows the characteristics of topic annotations of the\narticles in LitCovid by the end of September 2021. Preven-\ntion, Treatment, and Diagnosis are the topics with the high-\nest frequency. Over 20% of the articles have more than one\ntopic annotated. Some topics co-occur frequently, such as\nTreatment and Mechanism, where papers describe underly-\ning biological pathways and potential COVID-19 treatment\n(https://www.ncbi.nlm.nih.gov/pubmed/33638460). The\nannotated topics have been demonstrated to be effective for\ninformation retrieval and have been used in many down-\nstream applications. Speciﬁcally, topic-related searching\nand browsing account for over 18% of LitCovid user behav-\niors among millions of accesses and it has been the second\nmost accessed features in LitCovid [6]. The topics have also\nbeen used downstream studies such as evidence attribution,\nliterature inﬂuence analysis, and knowledge network gener-\nation. application [7], [8], [9].\nHowever, annotating these topics has been a primary bot-\ntleneck for manual curation. First, compared to the general\ndomain, biomedical literature has domain-speciﬁc ambiguities\nand difﬁculties of understanding its semantics. For example,\nthe Treatment topic can be described in different ways includ-\ning patient outcomes (e.g., ‘these factors may impact in guid-\ning the success of vaccines and clinical outcomes in COVID-19\ninfections’), biological pathways (e.g., ‘virus-speciﬁc host\nresponses and vRNA-associated proteins that variously pro-\nmote or restrict viral infection), and biological entities (e.g.,\n‘unique ATP-binding pockets on NTD/CTD may offer prom-\nising targets’). Second, compared to other curation tasks in Lit-\nCovid (document triage and entity recognition), topic\nannotation is more difﬁcult due to the nature of the task\n(assigning up to eight topics) and the ambiguity of natural lan-\nguages (such as different ways to describe COVID-19 treat-\nment procedures). Initially, the annotation was done manually\nby two curators with little machine assistance. To keep up with\nthe rapid growth of COVID-19 literature, Binary BERT has\nbeen developed to support manualannotation. However, pre-\nvious evaluations show that it has an F1-score of 10% lower\nthan the tools assisting other curation tasks in LitCovid [6].\nAlso, Binary BERT requires a signiﬁcant amount of inference\ntime because each label needs a separate BERT model for pre-\ndiction. This challenges the LitCovid curation pipeline, which\nmay have thousands of articles to curate within a day.\n3. D ATA AND METHOD\n3.1 Experiment Datasets\nWe used LitCovid BioCreative and HoC datasets for\nmethod development and evaluation. Table 1 provides the\ncharacteristics.\nFor the LitCovid BioCreative dataset [37], it contains\n24960, 6239, and 2500 PubMed articles in the training, devel-\nopment, and testing sets, respectively. The topics were\nassigned using the above annotation approach consistently.\nAll the articles contain both titles and abstracts available in\nTABLE 1\nCharacteristics of the Experiment datasets\nTrain Valid Test All\n#Articles Label (%) #Articles Label (%) #Articles Label (%) #Articles Label (%)\nLitCovid BioCreative (7 labels) 24960 - 6239 - 2500 - 33699 -\nCase Report 2063 (8.27%) 482 (7.73%) 197 (7.88%) 2742 (8.14%)\nDiagnosis 6193 (24.81%) 1546 (24.78%) 722 (28.88%) 8461 (25.11%)\nEpidemic Forecasting 645 (2.58%) 192 (3.08%) 41 (1.64%) 878 (2.61%)\nMechanism 4438 (17.78%) 1073 (17.2%) 567 (22.68%) 6078 (18.04%)\nPrevention 11102 (44.48%) 2750 (44.08%) 926 (37.04%) 14778 (43.85%)\nTransmission 1088 (4.36%) 256 (4.1%) 128 (5.12%) 1472 (4.37%)\nTreatment 8717 (34.92%) 2207 (35.37%) 1035 (41.4%) 11959 (35.49%)\nHoc (10 labels) 1108 - 157 - 315 - 1580 -\nActivating invasion & metastasis 199 (17.96%) 35 (22.29%) 57 (18.1%) 291 (18.42%)\nAvoiding immune destruction 77 (6.95%) 14 (8.92%) 17 (5.40%) 108 (6.84%)\nCellular energetics 76 (6.86%) 10 (6.37%) 19 (6.03%) 105 (6.65%)\nEnabling replicative immortality 82 (7.40%) 15 (9.55%) 18 (5.71%) 115 (7.28%)\nEvading growth suppressors 174 (15.7%) 22 (14.01%) 46 (14.60%) 242 (15.32%)\nGenomic instability & mutation 239 (21.57%) 24 (15.29%) 70 (22.22%) 333 (21.08%)\nInducing angiogenesis 97 (8.75%) 15 (9.55%) 31 (9.84%) 143 (9.05%)\nResisting cell death 302 (27.26%) 45 (28.66%) 83 (26.35%) 430 (27.22%)\nSustaining proliferative signal 338 (30.51%) 41 (26.11%) 83 (26.35%) 462 (29.24%)\nTumor promoting inﬂammation 162 (14.62%) 24 (15.29%) 54 (17.14%) 240 (15.19%)\n#Articles: the number of articles; Label (%): the proportion of the articles with a speciﬁc label; some label names of the hoc dataset are shortended for representation\npurpose.\n2586 IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. 19, NO. 5, SEPTEMBER/OCTOBER 2022\nPubMed and have been manually reviewed by two curators.\nThe only difference is that the datasets do not contain the\nGeneral Information topic since the priority of the topic\nannotation is given to the articles with abstracts available in\nPubMed [6]. In addition, the testing set contains the articles\nthat have been added to LitCovid from 16th June to 22nd\nAugust after the construction of the training and develop-\nment datasets. Using incoming articles to generate the testing\nset will facilitate the evaluation of the generalization capabil-\nity of automatic methods. To our knowledge, this dataset is\none of the largest multi-label classiﬁcation datasets on bio-\nmedical English scientiﬁc text.. We have made this dataset\npublicly available to the community via https://ftp.ncbi.\nnlm.nih.gov/pub/lu/LitCovid/biocreative/. For the HoC\ndataset, it contains 1580 PubMed abstracts with 10 currently\nknown hallmarks of cancer that were annotated by two cura-\ntors. The data set is available via https://www.cl.cam.ac.\nuk//C24 sb895/HoC.html. We used the same dataset split from\nprevious studies [16], [24]; the training, development, and\ntesting sets contain 1108, 157, and 315 articles, respectively.\nAs mentioned, it is the only dataset used for multi-label clas-\nsiﬁcation in biomedical literature from commonly-used\nbenchmark datasets [16], [17], [24].\n3.2 LITMC-BERT Architecture\nThe architecture of LITMC-BERT is summarized in Figs. 2\nand 3. Fig. 2 compares its architecture with other approaches\n(we will also use them as baselines), whereas Fig. 3 details its\nunderlying modules. The detailed hyperparameters are also\nsummarized in Table 2 and 3.4.1.\nAs mentioned, most biomedical text mining studies have\nused Binary BERT (Fig. 2A) for multi-label classiﬁcation\n[16], [17]. Indeed, we have applied Binary BERT to annotate\ntopics in the LitCovid curation pipeline as well [6]. An alter-\nnative approach is to uses a shared BERT model with a Sig-\nmoid function (or other similar activation functions) to\noutputs all the label probabilities directly (Fig. 2B), which\nwe denote it as Linear BERT (Fig. 2B): it uses a Sigmoid\nfunction (or other similar activation functions) followed by\na shared BERT model which outputs all the label probabili-\nties directly. Linear BERT also forms the basis of LITMC-\nBERT (Fig. 2C). In contrast, for LITMC-BERT, each label has\nits own module (Label Module) to capture label-speciﬁc\nrepresentations; and the label representations are further\nused (Label Pair Module) to predict whether a pair of labels\nco-occurs. It also leverages multi-task training and label-\nbased ﬁne-tuning. We explain each in detail below.\n3.2.1 Transformer Backbone\nThe Transformer Backbone applies a transformer-based\nmodel to get a general representation of an input text; in\nthis case, the input text is the title and abstract (if available)\nof an article. In this study, the transformer-based model is\nBioBERT [38], which a BERT model pre-trained on PubMed\nand PMC articles. We evaluated a range of BERT variants\nFig. 2. An overview of BERT-based multi-classiﬁcation models for biomedical literature using an example of classifying two labels (Labels 1 and 2).\n(A): Binary BERT: train a BERT model for each label; (B) Linear BERT: train a shared BERT model and output all the label probabilities at once;\n(C): LitMC-BERT (our proposed approach): train a shared BERT model, capture label-based features (Label Module) and models pair relations\n(Label Pair Module), and also predict both labels and their co-occurrences via co-training.\nFig. 3. The illustration of the Label Module and Label Pair Module. MLP: multi-layer perceptron. The detailed hyperparameters are provided in\nTable 2.\nCHEN ET AL.: LITMC-BERT: TRANSFORMER-BASED MULTI-LABEL CLASSIFICATION OF BIOMEDICAL LITERATURE WITH AN... 2587\nand BioBERT (v1.0) gave the overall highest performance as\nthe backbone model.\n3.2.2 Label Module\nEach label has a label module to capture its speciﬁc repre-\nsentations for the ﬁnal label classiﬁcation. Fig. 3A shows its\ndetail. Essentially, the Label Module combines the ﬁnal hid-\nden vector for the CLS token of a BERT backbone (Fig. 3A\n(1); we call it CLS vector) and the label-speciﬁc vector\n(Fig. 3A (2)) to produce the ﬁnal label feature vector\n(Fig. 3A (5)).\nUsing the CLS vector of a BERT backbone is recom-\nmended by the authors of BERT for classiﬁcation tasks [14].\nFor LITMC-BERT, it is shared by all the labels (since a\nshared BERT model is used as the backbone). In addition,\nfor each label, a Multi-head Self-Attention [39] and a global\naverage pooling layer are applied to the last encoder layer\nof the BERT backbone (Fig. 3A (2)) to get a label-speciﬁc vec-\ntor. This is designed to capture speciﬁc features for each\nlabel. We further normalize the CLS vector and label-spe-\nciﬁc vector with a multi-layer perceptron (MLP) consisting\nof a few dense layers (Fig. 3A (3)) (Fig. 3A (4)). This\napproach has been demonstrated to be effective for combin-\ning feature vectors from different sources [28]. The normal-\nized vectors are summed up to produce the ﬁnal label\nvector (Fig. 3A (5)).\n3.2.3 Label Pair Module\nThe Label Pair Module further uses the label representa-\ntions from the Label Module and captures correlations\nbetween label pairs. Fig. 3B shows its detail.\nFor a pair of labels 1 and 2, the Label Pair Module ﬁrst\nuses their corresponding feature representations produced\nby the Multi-head Self-Attention in the Label Module (Fig.\nA) as inputs. Then it performs co-attentions (Fig. 3B (2)) and\nglobal average pooling (Fig. 3B (3)) to get two vectors from\nthe inputs. The co-attention mechanism is an adaption of\nthe Multi-head Self-Attention whereas the query and key\ncomponents of the Self-Attention are the label pairs in this\ncase (e.g., the attention from label 1 to label 2 and the\nattention from label 2 to label 1 in Fig. 3B (2)). This has been\ndemonstrated to be effective for modeling correlations\nbetween pairs [40], [41]. Then, the two vectors are fused\nusing the same method above (Figs. 3B (4) and 3B(5)) to get\nthe ﬁnal label pair vector (Fig. 3B (6)). The label pair vector is\nused to predict whether the labels 1 and 2 co-occur as auxil-\niary tasks for the multi-training process introduced below.\nAuxiliary tasks are not directly related to primary tasks (label\npredictions in this case) but have shown effective for multi-\ntask training to make the shared representation more gener-\nalizable [42]. In addition, while the relations between label\npairs are important, it does not necessarily apply to every\nlabel pair. We deﬁne a hyperparameter called label pair\nthreshold: the Label Pair Module is only applied to a label\npair if above the threshold. For a pair of labels 1 and 2, the\nthreshold is calculated by the number of instances that labels\n1 and 2 co-occur dividing by the minimum number between\nthe number of instances of labels 1 and 2 in the training set.\n3.2.4 Multi-task Training and Label-Based Fine-Tuning\nThe LITMC-BERT training process employs multi-task\ntraining where it trains and predicts the labels (main tasks)\nand co-occurrence (auxiliary tasks) simultaneously. The\nloss during the multi-task training is the total loss of main\ntasks and auxiliary tasks). Given that main tasks are the\nfocus, we deﬁne a hyperparameter called auxiliary task\nweight (from 0 to 1) which takes a proportion of auxiliary\ntask losses. The full hyperparameters and baselines are pro-\nvided below. When the multi-task training converges, it fur-\nther ﬁne-tunes the Label Module for each label while\nfreezing the weights of other modules . Such training\napproach has been shown effective in both text mining and\ncomputer vision applications [43], [44].\n3.3 Baseline Models\nWe compared LITMC-BERT to three baseline models: ML-\nNet (a shallow deep learning multi-label classiﬁcation\nmodel which has achieved superior performance in bio-\nmedical literature) [24], Binary BERT (Fig. 2A), and Linear\nBERT (Fig. 2B).\nTABLE 2\nHyperparameters of the methods\nML-Net Binary BERT Linear BERT LITMC-BERT (ours)\nShared hyperparameters\nMax seq len 2000 characters 512 tokens 512 tokens 512 tokens\nBackbone ELMO BioBERT BioBERT BioBERT\nBatch size 16 16 16 16\nLearning rate 1e-3 5e-2 5e-2 5e-2\nActivation function ReLU Sigmoid Sigmoid Sigmoid\nLoss function Log-sum-exp\npairwise\nCross-entropy Cross-entropy Label predictions Cross-entropy\nPair predictions Focal loss\nSpeciﬁc hyperparameters RNN units 50 Early stop 2 Early stop 2 Early stop 2\nAttention units 50 MLP units (3 layers) 512, 256, 128\nMulti-head number 16\nLabel pair threshold 0.40\nAuxiliary task weight 0.25\nSeq len: sequence Length. BioBERT: we used BioBERT v1.0; MLP: multilayer perceptron with three hidden layers, each with 512, 256, and 128 hidden units,\nrespectively.\n2588 IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. 19, NO. 5, SEPTEMBER/OCTOBER 2022\nML-Net is an end-to-end deep learning framework\nwhich has achieved favorably state of the art (SOTA) perfor-\nmance in a few biomedical multi-label text classiﬁcation\ntasks [24]. ML-Net ﬁrst maps texts into high dimensional\nvectors through deep contextualized word representations\n(ELMo) [33], and then combines a label prediction network\nand label count prediction to infer an optimal set of labels\nfor each document.\nBinary BERT and Linear BERT are introduced in 3.2. For N\nlabels, Binary BERT trains N BERT classiﬁcation models (one\nlabel each) whereas Linear BERT provides all the N label pre-\ndictions in one model. Note that previous studies mostly have\nused Binary BERT in biomedical literature [16], [17]. It also has\nbeen the state-of-the-art (SOTA) model for multi-label classiﬁ-\ncation in biomedical literature [15], [16], [17] and was also\nused in the LitCovid production system previously [6].\n3.3.1 Hyperparameters\nFor each model, we performed hyperparameter tuning on\nthe datasets and selected the best sets of hyperparameter\nbased on the validation set loss. Table 2 provides the hyper-\nparameter values in the LitCovid BioCreative dataset; the\nconﬁguration ﬁles of the hyperparameters are also provided\nin the github repository. Importantly, for BERT-related\nmodels (Binary BERT, Linear BERT, and LITMC-BERT), we\ncontrolled their shared hyperparameters (BERT backbone,\nmaximum sequence length, learning rate, early stop steps,\nand batch size) to ensure a fair and direct comparison.\n3.4 Evaluation Metrics and Reporting Standard\nThere are a number of evaluation measures for multi-label\nclassiﬁcation tasks [13], [45], [46], which can be broadly\ndivided into two groups: (1) label-based measures, which\nevaluate the classiﬁer’s performance on each label and (2)\nexample-based measures (also called instance-based meas-\nures), which aim to evaluate the multi-label classiﬁer’s\nperformance on each test instance. Both groups comple-\nment each other: in the case of topic annotation, label-\nbased measures quantify the speciﬁc performance for each\ntopic, whereas example-based measures quantify the effec-\ntiveness of models at document level (which may contain\nseveral topics). We employed representative metrics from\nboth groups to provide a broader spectrum on the\nperformance.\nSpeciﬁcally, we used six evaluation measures as the main\nmetrics. They consist of four label-based measures: macro-\nF1, macro-Average Precision (AP), micro-F1, and micro-AP\nand two instance-based measures: instance-based F1 and\naccuracy (also stands for exact match ratio and the comple-\nment of zero one loss in this case). We further reported six\nevaluation measures that have been used to calculate the\nmain metrics as additional metrics. They consist of four\nlabel-based measures: macro-Precision, macro-Recall,\nmicro-Precision, and micro-Recall and two label-based\nmeasures: instance-based Precision and instance-based\nRecall. Their calculation formulas are summarized below.\n3.4.1 Label-Based Measures\nLabel-based measures evaluate the multi-label classiﬁer’s\nperformance separately on each label by calculating their\ntrue positive (TP), false positive (FP) and false negative\n(FN) on the test set. For thej-th labelyj, we calculated the\nfollowing four metrics:\nPrecisionj ¼ TPj\nTPj þ FPj\nRecallj ¼ TPj\nTPj þ FNj\nF1j ¼ 2 /C1 Precisionj /C1 Recallj\nPrecisionj þ Recallj\nAPj ¼\nX\nn\nRecalljn /C0 Recalljn/C0 1\nPrecisionjn\nF1 and AP are aggregated measures using both Precision\nand Recall in the calculation. AP is also a threshold-based\nmeasure which summarizes a Precision-Recall curve at each\nthreshold (denoted as n in the formula).\nTo measure the overall metrics for all the labels, we\ncalculated both macro-averaged (using unweighted aver-\naging across labels) and micro-averaged scores (counting\nTP, FP and FN globally rather than at label level) for the\nlabels.\n3.4.2 Example-Based Measures\nThe example-based metrics evaluate the multi-label classi-\nﬁer’s performance separately by comparing the predicted\nlabels with the gold-standard labels for each test example.\nWe focus on the following four metrics:\nPrecision ¼ 1\np\nXp\ni¼1\nYi \\ ^Y i\n/C12/C12 /C12/C12\n^Y i\n/C12/C12 /C12/C12 ¼ 1\np\nXp\ni¼1\nTP\nTP þ FP\nRecall ¼ 1\np\nXp\ni¼1\nYi \\ ^Y i\n/C12/C12 /C12/C12\nYijj ¼ 1\np\nXp\ni¼1\nTP\nTP þ FN\nF1 ¼ 2 /C1 Precision /C1 Recall\nPrecision þ Recall\nAccuracy ¼ # correctly predicted instances\n# all the instances\nwhere p is the number of documents in the test set;Yi refers\nto the true label set for the i-th document in the test set;\nand bYi refers to the predicted label set for the i-th document\nin the test set.\n3.4.3 Statistic Test and Reporting Standard\nWe repeated each model 10 times, reported the mean and\nmax values of the repeats for each evaluation measure\nabove, and conducted the Wilcoxon rank-sum test (Conﬁ-\ndence Interval at 95%; one-tail) following previous studies\n[40], [47]\n4R ESULTS AND DISCUSSIONS\n4.1 Overall Performance\nTable 3 demonstrates the overall performance of the models\non both datasets. As mentioned, we used six main metrics\nCHEN ET AL.: LITMC-BERT: TRANSFORMER-BASED MULTI-LABEL CLASSIFICATION OF BIOMEDICAL LITERATURE WITH AN... 2589\nand reported their mean and max results (i.e., 12 evaluation\nmeasurement results). Out of these 12 measurement results,\nLITMC-BERT consistently achieved the highest results in 10\nof them in the LitCovid BioCreative dataset and all the 12 in\nthe HoC dataset. On average, its macro F1-score is about\n10% higher than ML-Net in both datasets; the same applies\nto other measures such as macro-AP and accuracy. Com-\npared with Binary BERT, its label-based measures are about\n2% and 4% higher on the LitCovid BioCreative and HoC\ndatasets, respectively. Its instance-based measures on the\nHoC dataset show a larger difference; e.g., its accuracy is up\nto 10% higher. The observations are similar when compar-\ning LITMC-BERT with Linear BERT: e.g., its macro-F1 and\naccuracy are up to 2% and 4% higher on the HoC dataset,\nrespectively. In terms of comparing Binary BERT with Lin-\near BERT, Binary BERT achieved overall better performance\non the LitCovid BioCreative dataset, which is consistent\nwith the literature [6], [15], whereas Linear BERT achieved\nover better performance on the HoC dataset.\nIn addition, Fig. 4 shows the distribution of macro F1-\nscores of the models and theP-values of the Wilcoxon rank\n-sum test. On both datasets, LITMC-BERT consistently had\na better macro-F1 score than ML-Net (P-values close to 0)\nand both Binary BERT and Linear BERT (P-values smaller or\nclose to 0.001).\nFurther, comparing with the current SOTA results on the\nHoC dataset, LITMC-BERT also achieved higher perfor-\nmance. For LitCovid BioCreative, LITMC-BERT achieved\nbetter performance than the results reported by the chal-\nlenge overview from 80 system submissions worldwide\n[37]. Existing studies on the HoC dataset used different\nmeasures and only reported one evaluation result (without\nrepetitions to report the average performance or perform\nstatistic tests). One study used instance-based F1 and\nreported that BlueBERT (base) and BlueBERT (large)\nachieved the highest instance-based F1 of 0.8530 and 0.8730,\nrespectively, compared with other BERT variants [16]. In\ncontrast, LITMC-BERT achieved a mean instance-based F1\nof 0.9030 and a maximum instance-based F1 of 0.9169, con-\nsistently higher than the reported performance. Similarly,\nanother study used micro-F1 on a slightly different version\nof the HoC dataset (this is different from other studies [16],\n[24]) and reported that PubMedBERT achieved the highest\nmicro-F1 of 0.8232 [17]. The mean and maximum of micro-\nF1 of LITMC-BERT are 0.8648 and 0.8787, respectively. We\nmanually examined the results and ﬁnd that one possible\nreason is that the existing studies use the BERT model at\nsentence-level and then aggregate the predictions to the\nabstract-level for the HoC dataset [16]; this may ignore the\ninter-relations among sentences and cannot capture the con-\ntext at abstract-level. In contrast, we directly applied\nthe models at the abstract-level which overcomes the\nlimitations.\n4.2 Additional Measures, Label-Speciﬁc Results,\nand an Ablation Analysis\nTable 4 provides additional measures to complement the\nmain metrics. As mentioned, we reported the mean and\nmaximum of six additional metrics (i.e., 12 in total). Out of\nthese 12 additional measurement results, LITMC-BERT\nachieved the highest results in 7 of them in the LitCovid\nBioCreative dataset and 11 of them in the HoC dataset,\nwhich is consistent with the main measurement results in\nTable 3.\nIn addition, we further analyzed the performance of each\nindividual label. Figs. 5 and 6 show F1s of each label in the\nLitCovid BioCreative and HoC datasets, respectively. Out\nof the seven labels in the LitCovid BioCreative dataset,\nLITMC-BERT had the highest F1 in four of them. Similarly,\nit had the highest F1 in seven out of 10 labels in the HoC\ndataset. The results also demonstrate that LITMC-BERT had\nmuch better performance for labels with low frequencies.\nFor the LitCovid BioCreative dataset, its F1s are up to 9%\nand 6% higher for the Epidemic Forecasting (accounting for\n1.64% of the testing set) and Transmission (5.12%) labels\nTABLE 3\nThe Overall Performance (Main Evaluation Measures) of the Methods on the Litcovid and Hoc datsets\nLabel-based measures Instance-based measures\nMacro-F1 Macro-AP Micro-F1 Micro-AP F1 Accuracy\nMean Max Mean Max Mean Max Mean Max Mean Max Mean Max\nLitCovid BioCreative\nML-Net 0.7655 0.7750 - - 0.8437 0.8470 - - 0.8678 0.8706 0.7019 0.7108\nBinary BERT 0.8597 0.8773 0.7825 0.8059 0.9132 0.9186 0.8557 0.8655 0.9278 0.9330 0.7984 0.8120\nLinear BERT 0.8569 0.8791 0.7796 0.8066 0.9067 0.9163 0.8461 0.8607 0.9254 0.9341 0.7915 0.8072\nLitMC-BERT (ours) 0.8776 0.8921 0.8048 0.8223 0.9129 0.9212 0.8553 0.8663 0.9314 0.9384 0.8022 0.8188\nHoc\nML-Net 0.7618 0.7665 - - 0.7449 0.7560 - - 0.7931 0.8003 0.4990 0.5429\nBinary BERT 0.8530 0.8686 0.7581 0.7811 0.8453 0.8583 0.7368 0.7568 0.8733 0.8850 0.6251 0.6476\nLinear BERT 0.8599 0.8711 0.7690 0.7875 0.8554 0.8637 0.7547 0.7670 0.8941 0.9018 0.6695 0.6857\nLitMC-BERT (ours) 0.8733 0.8882 0.7894 0.8118 0.8648 0.8787 0.7697 0.7905 0.9036 0.9169 0.6854 0.7270\nReported SOTA performance on\nHoc\nBlueBERT (base) ----- - --- 0.8530 --\nBlueBERT (large) ----- - --- 0.8730 --\nPubMedBERT ----- 0.8232/C3 ------\n/C3 : The reported results was on a slightly different version of the hoc dataset.\n2590 IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. 19, NO. 5, SEPTEMBER/OCTOBER 2022\nthan Binary BERT and Linear BERT, respectively. For the\nHoC dataset, its F1s are also up to 6% and 5% higher for the\nAvoiding immune destruction (5.40%) and Enabling repli-\ncative immortality (5.71%) labels than Binary BERT and Lin-\near BERT, respectively. This suggests that LITMC-BERT\nmight be more robust to the class imbalance issue. Indeed,\nexisting studies have demonstrated the class imbalance\nissue remains an open challenge for multi-label classiﬁca-\ntions and it is more difﬁcult to improve the classiﬁcation\nperformance for rare classes [21], [22]. This is more evident\nthat BERT-related models can already achieve F1-scores of\nclose to or above 90% for labels with high frequencies on\nboth datasets. Therefore, the performance of topics with\nlow frequencies is arguably more critical.\nFurther, we performed an ablation analysis to quantify\nthe effectiveness of the LITMC-BERT modules. Speciﬁcally,\nwe compared the performance of LITMC-BERT without the\nLabel Module, the Label Pair Module, or both (i.e., Linear\nBERT) using the same evaluation procedure above. Table 5\nshows the results. Recall that Linear BERT uses the same\nBERT backbone and does not capture label-speciﬁc features\nor correlations between labels; therefore, we can directly\ncompare the effectiveness of the Label Module and the\nLabel Pair Module with Linear BERT. On both datasets,\nLITMC-BERT with both modules had the highest perfor-\nmance in all the measures. For instance, the Label Module\nincreased the average macro-F1 by 2.1% and 0.5% in LitCo-\nvid BioCreative and HoC, respectively; the Label Pair Mod-\nule increased the average macro-F1 by 0.7% and 0.5% in\nLitCovid BioCreative and HoC, respectively. Consistent\nobservations are also shown in other metrics; for example,\nthe Label Module increased the average macro-AP by 2.5%\nand 1.5% in LitCovid BioCreative and HoC, respectively.\nThis suggests that the two modules complement to each\nother and combining both is effective. In addition, removing\neither module dropped the performance in both datasets;\nremoving both of them had the lowest performance on aver-\nage. This suggests both modules are effective. Also, the\nresults suggest that the Label Module is more effective in\nthe LitCovid BioCreative dataset (e.g., the maco-F1 is\nreduced by up to 2% if removing the Label Module)\nwhereas the Label Pair Module is more effective in the HoC\ndataset (e.g., the maco-F1 is reduced by up to 1% if remov-\ning the Label Pair Module).\nFig. 4. The distributions of macro-F1s for each model on the LitCovid BioCreative (A) and HoC datasets (B). Each model was repeated 10 times and\nthe Wilcoxon rank sum test (Conﬁdence Interval at 95%; one-tail) was performed. TheP-values are shown in the ﬁgure.\nTABLE 4\nAdditional Evaluation Measures of the Methods on the Litcovid and Hoc datsets\nLabel-based measures Instance-based measures\nMacro-\nPrecision\nMacro-Recall Micro-\nPrecision\nMicro-Recall Precision Recall\nMean Max Mean Max Mean Max Mean Max Mean Max Mean Max\nLitCovid BioCreative\nML-Net 0.8364 0.8559 0.7309 0.7632 0.8756 0.8827 0.8142 0.8227 0.8849 0.8901 0.8514 0.8591\nBinary BERT 0.9103 0.9498 0.8350 0.8690 0.9304 0.9448 0.8969 0.9173 0.9349 0.9408 0.9210 0.9381\nLinear BERT 0.9071 0.9388 0.8354 0.8701 0.9276 0.9396 0.8870 0.9093 0.9368 0.9443 0.9143 0.9308\nLitMC-BERT (ours) 0.9131 0.9226 0.8574 0.8814 0.9313 0.9366 0.8952 0.9145 0.9418 0.9480 0.9212 0.9355\nHoc\nML-Net 0.7949 0.8356 0.7389 0.7622 0.7667 0.8053 0.7253 0.7448 0.8045 0.8296 0.7826 0.8013\nBinary BERT 0.8471 0.8644 0.8661 0.8834 0.8363 0.8562 0.8548 0.8724 0.8565 0.8735 0.8909 0.9095\nLinear BERT 0.8772 0.8930 0.8475 0.8630 0.8614 0.8812 0.8496 0.8619 0.8929 0.9116 0.8955 0.9024\nLitMC-BERT (ours) 0.8868 0.8983 0.8641 0.8910 0.8718 0.8938 0.8582 0.8787 0.9035 0.9148 0.9038 0.9193\nCHEN ET AL.: LITMC-BERT: TRANSFORMER-BASED MULTI-LABEL CLASSIFICATION OF BIOMEDICAL LITERATURE WITH AN... 2591\n4.3 Generalization and Efﬁciency Analysis\nThe above evaluations show that LITMC-BERT achieved\nconsistently better performance on both datasets. We also\nfurther evaluated its generalization and efﬁciency in the Lit-\nCovid production environment. While transformer-based\nmodels have achieved SOTA results in many applications,\ntheir inference time is signiﬁcantly longer than other types of\nmodels [48], [49]. It is thus important to measure its efﬁciency\nin practice. Reducing inference time is also critical to the Lit-\nCovid curation pipeline, which may have thousands of\narticles to curate within a day (the peak was over 2500\narticles in a single day) [6]. A random sample of 3000 articles\nin LitCovid was collected between October and December,\n2021, which was independent to the training set. We used it\nas an external validation set and measured the accuracy and\nefﬁciency of these models. As mentioned, Binary BERT was\nused in LitCovid for topic annotations. We used a single pro-\ncessor on CPU with a batch size of 128, which is consistent\nwith the LitCovid production setting, and tracked the infer-\nence time accordingly. Table 6 details the performance.\nLITMC-BERT achieved the best performance in all the accu-\nracy-related measures, took/C24 18% of the prediction time of\nBinary BERT, and was only 0.05 sec/doc slower than Linear\nBERT as the trade-off. Note that Binary BERT was previously\nused in the LitCovid production. It took about 3.4 seconds on\naverage to predict topics for an article. Note that this does\nnot include overhead time (e.g., switching into other auto-\nmatic curation tasks) and post-processing time (e.g., sorting\nthe probabilities and showing the related articles for manual\nreview). Therefore, just predicting the topics for a large batch\nof articles may take over an hour, delaying the daily curation\nof LitCovid. In contrast, it only takes LITMC-BERT about 0.5\nseconds on average for inference, which accounts for/C24 15%\nof the time used by Binary BERT. We have employed\nLITMC-BERT into the LitCovid production system given its\nsuperior performance on both effectiveness and efﬁciency.\n4.4 Limitations and Future Work\nWhile LITMC-BERT achieved the best overall performance in\nboth datasets compared with other competitive baselines, it\ndoes have certain limitations that we plan to address in the\nfuture. First, from the method level, it still relies on transfer\nlearning from BERT backbones given the scale of multi-label\nclassiﬁcation datasets in biomedical literature. In contrast,\nFig. 5. The performance (F1) of the methods for each label in the LitCovid dataset.\nFig. 6. The performance (F1) of the methods for each label in the HoC dataset.\n2592 IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. 19, NO. 5, SEPTEMBER/OCTOBER 2022\nsome methods used other domains include label clustering\n[50] and label graph attentions [51]. We plan to investigate\nthese methods and quantify whether they are effective for bio-\nmedical literature. Second, it has more hyperparameters to\ntune (such as ﬁnding the optimal label pair threshold) com-\npared with other straightforward BERT-based models. It\nwould be better to incorporate dynamic modules that learn\nthese hyperparameters adaptively. Third, the Label Pair Mod-\nule focuses only on co-occurred labels which may miss more\ncomplex scenarios such as labels in n-ary relations.\n5C ONCLUSION\nIn this paper, we propose a novel transformer-based multi-\nlabel classiﬁcation method on biomedical literature, LITMC-\nBERT. Compared to the existing multi-label classiﬁcation\nmethods in biomedical literature, it captures label-speciﬁc fea-\ntures and also captures the correlations between label pairs.\nThe multi-task training approach also makes it more efﬁcient\nthan binary models. LITMC-BERT achieved the highest over-\nall performance on two datasets than three baselines. Also, it\nonly takes/C24 18% of the inference time taken by the previous\nbest model for COVID-19 literature. LITMC-BERT has been\nemployed in the LitCovid production system for more sus-\ntainability and effectiveness. We plan to further improve the\nmethod such that it is more dynamic and capable of handling\nmore complex relations among labels and further quantify its\neffectiveness on multi-label classiﬁcation tasks beyond bio-\nmedical literature (such as clinical notes).\nACKNOWLEDGMENTS\nThe authors thank Dr. Robert Leaman for proofreading the\nmanuscript.\nREFERENCES\n[1] C.-H. Wei et al., “Accelerating literature curation with text-mining\ntools: A case study of using pubtator to curate genes in pubmed\nabstracts,” Database, vol. 2012, 2012, Art. no. bas041.\n[2] Q. Chen et al., “Quality matters: Biocuration experts on the\nimpact of duplication and other data quality issues in biologi-\ncal databases,” Genomic. Proteomic. Bioinf., vol. 18, no. 2, 2020,\nArt. no. 91.\n[3] A. Palayew, O. Norgaard, K. Safreed-Harmon, T. H. Andersen,\nL. N. Rasmussen, and J. V. Lazarus, “Pandemic publishing poses\na new COVID-19 challenge,”Nature Human Behav., vol. 4, no. 7,\npp. 666–669, 2020.\n[4] Q. Chen et al., “Artiﬁcial intelligence (AI) in action: Addressing\nthe COVID-19 pandemic with natural language processing\n(NLP),” 2020,arXiv:2010.16413.\n[5] Q. Chen, A. Allot, and Z. Lu, “Keep up with the latest coronavirus\nresearch,” Nature, vol. 579, no. 7798, pp. 193–193, 2020.\n[6] Q. Chen, A. Allot, and Z. Lu, “LitCovid: An open database of\nCOVID-19 literature,” Nucleic Acids Res. , vol. 49, no. D1,\npp. D1 534–D1 540, 2021.\n[7] N. Fabiano et al., “An analysis of COVID-19 article dissemination\nby twitter compared to citation rates,”medRxiv, 2020.\nTABLE 5\nAblation Analysis on the Effectiveness of the Label Module and Label Pair modules\nLabel-based measures Instance-based measures\nMacro-F1 Macro-AP Micro-F1 Micro-AP F1 Accuracy\nMean Max Mean Max Mean Max Mean Max Mean Max Mean Max\nLitCovid BioCreative\nLitMC-BERT (ours) 0.8776 0.8921 0.8048 0.8223 0.9129 0.9212 0.8553 0.8663 0.9314 0.9384 0.8022 0.8188\n-No Label Module 0.8635 0.8741 0.7868 0.7984 0.9071 0.9117 0.8474 0.8528 0.9258 0.9288 0.7942 0.8068\n-No Label Pair 0.8752 0.8838 0.8018 0.8121 0.9116 0.9166 0.8536 0.8606 0.9298 0.9326 0.8011 0.8104\n-No both modules 0.8569 0.8791 0.7796 0.8066 0.9067 0.9163 0.8461 0.8607 0.9254 0.9341 0.7915 0.8072\nHoc\nLitMC-BERT (ours) 0.8733 0.8882 0.7894 0.8118 0.8648 0.8787 0.7697 0.7905 0.9036 0.9169 0.6854 0.7270\n-No Label Module 0.8691 0.8796 0.7860 0.8027 0.8580 0.8638 0.7602 0.7717 0.8984 0.9046 0.6803 0.7016\n-No Label Pair 0.8644 0.8803 0.7762 0.8005 0.8590 0.8700 0.7618 0.7807 0.8998 0.9071 0.6756 0.7206\n-No both modules 0.8599 0.8711 0.7690 0.7875 0.8554 0.8637 0.7547 0.7670 0.8941 0.9018 0.6695 0.6857\nNote That ‘no both modules’ is linear bert.\nTABLE 6\nPerformance Evaluation on an Independent Sample of 3000 Articles in litcovid\nLabel-based measures Instance-based\nmeasures\nTime measures\nin the production\nenvironment (in sec)\nMacro-F1 Macro-AP Micro-F1 Micro-AP F1 Accuracy\nBinary BERT 0.8116 0.7251 0.8570 0.7733 0.8719 0.6327 8912.22\n(2.97 sec/doc)\nLinear BERT 0.8299 0.7449 0.8539 0.7701 0.8759 0.6354 1536.24\n(0.51 sec/doc)\nLitMC-BERT (ours) 0.8413 0.7690 0.8628 0.7813 0.8862 0.6512 1673.98\n(0.56 sec/doc)\nTime was measured in the production environment where only cpus are available.\nCHEN ET AL.: LITMC-BERT: TRANSFORMER-BASED MULTI-LABEL CLASSIFICATION OF BIOMEDICAL LITERATURE WITH AN... 2593\n[8] L. Yeganova et al., “Navigating the landscape of COVID-19\nresearch through literature analysis: A bird’s eye view,”\n2020, arXiv:2008.03397.\n[9] M. H.-C. Ho and J. S. Liu, “The swift knowledge development\npath of COVID-19 research: The ﬁrst 150 days,”Scientometrics,\nvol. 126, no. 3, pp. 2391–2399, 2021.\n[10] A. M. Cohen and W. R. Hersh, “A survey of current work in bio-\nmedical text mining,”Brief. Bioinf., vol. 6, no. 1, pp. 57–71, 2005.\n[11] C.-C. Huang and Z. Lu, “Community challenges in biomedical\ntext mining over 10 years: Success, failure and the future,”Brief.\nBioinf., vol. 17, no. 1, pp. 132–144, 2016.\n[12] International Society for Biocuration, “Biocuration: Distilling data\ninto knowledge,”PLoS Biol., vol. 16, no. 4, 2018, Art. no. e2002846.\n[13] M.-L. Zhang and Z.-H. Zhou, “A review on multi-label learning algo-\nrithms,”IEEE Trans. Knowl. Data Eng., vol. 26, no. 8, pp. 1819–1837,\n2013.\n[14] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” 2018,arXiv:1810.04805.\n[15] B. J. Gutierrez, J. Zeng, D. Zhang, P. Zhang, and Y. Su, “Document\nclassiﬁcation for covid-19 literature,” 2020,arXiv:2006.13816.\n[16] Y. Peng, S. Yan, and Z. Lu, “Transfer learning in biomedical natu-\nral language processing: An evaluation of BERT and ELMo on ten\nbenchmarking datasets,” 2019,arXiv:1906.05474.\n[17] Y. Gu et al., “Domain-speciﬁc language model pretraining for bio-\nmedical natural language processing,” 2020,arXiv:2007.15779.\n[18] G. Tsoumakas, I. Katakis, and I. Vlahavas, “A review of multi-\nlabel classiﬁcation methods,” inProc. 2nd ADBIS Workshop Data\nMining Knowl. Discov., 2006, pp. 99–109.\n[19] M.-L. Zhang, Y.-K. Li, X.-Y. Liu, and X. Geng, “Binary relevance\nfor multi-label learning: An overview,” Frontiers Comput. Sci.,\nvol. 12, no. 2, pp. 191–202, 2018.\n[20] E. A. Cherman, M. C. Monard, and J. Metz, “Multi-label problem\ntransformation methods: A case study,”CLEI Electron. J., vol. 14,\nno. 1, pp. 4–4, 2011.\n[21] J. Read, B. Pfahringer, G. Holmes, and E. Frank, “Classiﬁer chains for\nmulti-label classiﬁcation,” Mach. Learn.,v o l .8 5 ,n o .3 ,2 0 1 1 ,\nArt. no. 333.\n[22] E. Gibaja and S. Ventura, “Multi-label learning: A review of the\nstate of the art and ongoing research,”Wiley Interdiscipl. Rev.: Data\nMining Knowl. Discov., vol. 4, no. 6, pp. 411–444, 2014.\n[23] J. Du et al., “Use of deep learning to analyze social media discus-\nsions about the human papillomavirus vaccine,” JAMA Netw.\nOpen, vol. 3, no. 11, pp. e2022025–e2022025, 2020.\n[24] J. Du, Q. Chen, Y. Peng, Y. Xiang, C. Tao, and Z. Lu, “ML-Net: Multi-\nlabel classiﬁcation of biomedical texts with deep neural networks,”J.\nAmer. Med. Inform. Assoc., vol. 26, no. 11, pp. 1279–1285, 2019.\n[25] J. Nam, J. Kim, E. L. Menc/C19ıa, I. Gurevych, and J. F€urnkranz,\n“Large-scale multi-label text classiﬁcation—Revisiting neural\nnetworks,” in Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discov.\nDatabases, 2014, pp. 437–452.\n[26] A. Allot, K. Lee, Q. Chen, L. Luo, and Z. Lu, “LitSuggest: A web-\nbased system for literature recommendation and curation using\nmachine learning,”Nucleic Acids Res., vol. 49, pp. W352–W358, 2021.\n[27] W. Hersh, “Information retrieval,” inBiomedical Informatics, Berlin,\nGermany: Springer, 2021, pp. 755–794.\n[28] R. Islamaj Do/C21gan et al., “Overview of the biocreative VI precision\nmedicine track: Mining protein interactions and mutations for\nprecision medicine,”Database, vol. 2019, 2019, Art. no. bay147.\n[29] Q. Chen, K. Lee, S. Yan, S. Kim, C.-H. Wei, and Z. Lu,\n“BioConceptVec: Creating and evaluating literature-based bio-\nmedical concept embeddings on a large scale,”PLoS Comput. Biol.,\nvol. 16, no. 4, 2020, Art. no. e1007617.\n[30] D. Hanahan and R. A. Weinberg, “The hallmarks of cancer,”Cell,\nvol. 100, no. 1, pp. 57–70, 2000.\n[31] B. Chiu, G. Crichton, A. Korhonen, and S. Pyysalo, “How to train\ngood word embeddings for biomedical NLP,” inProc. 15th Work-\nshop Biomed. Natural Lang. Process., 2016, pp. 166–174.\n[32] Y. Zhang, Q. Chen, Z. Yang, H. Lin, and Z. Lu, “BioWordVec,\nimproving biomedical word embeddings with subword informa-\ntion and MeSH,”Sci. Data, vol. 6, no. 1, pp. 1–9, 2019.\n[33] M. E. Peters et al., “Deep contextualized word representations,”\n2018, arXiv:1802.05365.\n[34] A. Neves, A. Lamurias, and F. M. Couto, “Biomedical question\nanswering using extreme multi-label classiﬁcation and ontologies\nin the multilingual panorama,” inProc. SIIRH2020 Workshop, 2020,\npp. 1–3.\n[35] I. Chalkidis, M. Fergadiotis, S. Kotitsas, P. Malakasiotis, N. Ale-\ntras, and I. Androutsopoulos, “An empirical study on large-scale\nmulti-label text classiﬁcation including few and zero-shot labels,”\n2020, arXiv:2010.01653.\n[36] A. Singh, M. Guntu, A. R. Bhimireddy, J. W. Gichoya, and S. Pur-\nkayastha, “Multi-label natural language processing to identify\ndiagnosis and procedure codes from MIMIC-III inpatient notes,”\n2020, arXiv:2003.07507.\n[37] Q. Chen, A. Allot, R. Leaman, R. I. Do/C21gan, and Z. Lu, “Overview\nof the biocreative VII litcovid track: Multi-label topic classiﬁcation\nfor COVID-19 literature annotation,”inProc. 7th BioCreative Chal-\nlenge Eval. Workshop, 2021, pp. 266–271.\n[38] J. Lee et al., “BioBERT: A pre-trained biomedical language repre-\nsentation model for biomedical text mining,” Bioinformatics,\nvol. 36, no. 4, pp. 1234–1240, 2020.\n[39] A. Vaswani et al., “Attention is all you need,” inProc. Adv. Neural\nInform. Process. Syst., 2017, pp. 5998–6008.\n[40] Q. Chen et al. , “Multimodal, multitask, multiattention (M3)\ndeep learning detection of reticular pseudodrusen: Toward\nautomated and accessible classiﬁcation of age-related macular\ndegeneration,” J. Amer. Med. Inform. Assoc.,v o l .2 8 ,p p .1 1 3 5 –\n1148, 2021.\n[41] H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder\nrepresentations from transformers,” 2019,arXiv:1908.07490.\n[42] L. Liebel and M. K€orner, “Auxiliary tasks in multi-task learning,”\n2018, arXiv:1805.06334.\n[43] J. Rao, F. Ture, and J. Lin, “Multi-task learning with neural net-\nworks for voice query understanding on an entertainment\nplatform,” in Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discov.\nData Mining, 2018, pp. 636–645.\n[44] Q. Chen, Y. Peng, T. Keenan, S. Dharssi, and E. Agro, “A multi-task\ndeep learning model for the classiﬁcation of Age-related macular\ndegeneration,” AMIA Summits Transl. Sci. Proc., vol. 2019, 2019,\nArt. no. 505.\n[45] B. P. Nguyen, “Prediction of FMN binding sites in electron trans-\nport chains based on 2-D CNN and PSSM proﬁles,”IEEE/ACM\nTrans. Comput. Biol. Bioinf., vol. 18, no. 6, pp. 2189–2197, Nov./\nDec. 2021.\n[46] N. Q. K. Le, D. T. Do, F.-Y. Chiu, E. K. Y. Yapp, H.-Y. Yeh, and C.-\nY. Chen, “XGBoost improves classiﬁcation of MGMT promoter\nmethylation status in IDH1 wildtype glioblastoma,”J. Personalized\nMed., vol. 10, no. 3, 2020, Art. no. 128.\n[47] Y. Wang et al., “Overview of the BioCreative/OHNLP challenge\n2018 task 2: Clinical semantic textual similarity,”Proc. BioCreative/\nOHNLP Challenge, vol. 2018, pp. 2–5, 2018.\n[48] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a dis-\ntilled version of BERT: Smaller, faster, cheaper and lighter,”\n2019, arXiv:1910.01108.\n[49] Q. Chen, A. Rankine, Y. Peng, E. Aghaarabi, and Z. Lu, “Benchmarking\neffectiveness and efﬁciency of deeplearning models for semantic tex-\ntual similarity in the clinicald o m a i n :V a l i d a t i o ns t u d y , ”JMIR Med.\nInform., vol. 9, no. 12, 2021, Art. no. e27386.\n[50] F. Ding, X. Kang, S. Nishide, Z. Guan, and F. Ren, “A fusion\nmodel for multi-label emotion classiﬁcation based on BERT and\ntopic clustering,” Proc. Int. Symp. Artif. Intell. Robot., vol. 11574,\np. 115740D, 2020.\n[51] A. Pal, M. Selvakumar, and M. Sankarasubbu, “Multi-label text\nclassiﬁcation using attention-based graph neural network,” 2020,\narXiv:2003.11644.\nQingyu Chen received the PhD degree in bio-\nmedical informatics from the University of Mel-\nbourne. He is currently a research fellow with the\nBioNLP Lab, National Center for Biotechnology\nInformation (NCBI), National Library of Medicine\n(NLM), and the lead instructor of text mining\ncourses at the Foundation for Advanced Educa-\ntion in the Sciences. His research interests\ninclude biomedical text mining, medical image\nanalytics, and biocuration. He has published\nmore than 30 ﬁrst-authored papers and 50\npapers in total. He serves as the associate editor of Frontiers in Digital\nHealth, ECR editor of Applied Clinical Informatics and PC member for\nthe IEEE International Conference on Healthcare Informatics and ACL\nBioNLP workshop.\n2594 IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. 19, NO. 5, SEPTEMBER/OCTOBER 2022\nJingcheng Du received the PhD degree in health\ninformatics from the University of Texas Health\nScience Center at Houston (UTHealth), TX, USA.\nHe is an Assistant Professor in health informatics\nwith the UTHealth School of Biomedical Informat-\nics. His research interest include machine learn-\ning, biomedical natural language processing and\nknowledge representation.\nAlexis Allot received the PhD degree in bioinfor-\nmatics from the University of Strasbourg, France,\nin 2015. He then worked at EMBL/EBI as Bioin-\nformatician and is now working as postdoctoral\nfellow with NIH. His research interests include\nbiomedical text mining, data mining and web\ndevelopment.\nZhiyong Lu received the PhD degree in bioinfor-\nmatics from the School of Medicine, University of\nColorado in 2007. He is currently deputy director\nof Literature Search, National Center for Biotech-\nnology Information (NCBI) with the National\nLibrary of Medicine (NLM), directing R&D efforts\nof improving literature searches such as PubMed\nand LitCovid. He is also an NIH senior investiga-\ntor (with early tenure), leading the Natural Lan-\nguage Processing (NLP) and Machine Learning\nresearch at NLM/NIH. He has published more\nthan 200 scientiﬁc articles and books. He serves as associate editor for\nthe journals Bioinformatics and Artiﬁcial Intelligence in Medicine. He is\nan elected fellow of the American College of Medical Informatics.\n\" For more information on this or any other computing topic,\nplease visit our Digital Library at www.computer .org/csdl.\nCHEN ET AL.: LITMC-BERT: TRANSFORMER-BASED MULTI-LABEL CLASSIFICATION OF BIOMEDICAL LITERATURE WITH AN... 2595",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7294303774833679
    },
    {
      "name": "Bottleneck",
      "score": 0.6196630001068115
    },
    {
      "name": "Transformer",
      "score": 0.5878849029541016
    },
    {
      "name": "Inference",
      "score": 0.5833674669265747
    },
    {
      "name": "Annotation",
      "score": 0.5587561130523682
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.5293724536895752
    },
    {
      "name": "Natural language processing",
      "score": 0.4431777000427246
    },
    {
      "name": "Baseline (sea)",
      "score": 0.4396298825740814
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43508657813072205
    },
    {
      "name": "Information retrieval",
      "score": 0.42806482315063477
    },
    {
      "name": "F1 score",
      "score": 0.42773306369781494
    },
    {
      "name": "Machine learning",
      "score": 0.3265305757522583
    },
    {
      "name": "Biology",
      "score": 0.10999232530593872
    },
    {
      "name": "Medicine",
      "score": 0.0741419792175293
    },
    {
      "name": "Disease",
      "score": 0.0
    },
    {
      "name": "Fishery",
      "score": 0.0
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    }
  ]
}