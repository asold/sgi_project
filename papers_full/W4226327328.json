{
    "title": "Spam Detection Using Bidirectional Transformers and Machine Learning Classifier Algorithms",
    "url": "https://openalex.org/W4226327328",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2115293496",
            "name": "Yanhui Guo",
            "affiliations": [
                "University of Illinois at Springfield"
            ]
        },
        {
            "id": "https://openalex.org/A4381437185",
            "name": "Zelal Mustafaoglu",
            "affiliations": [
                "University of Illinois at Springfield"
            ]
        },
        {
            "id": "https://openalex.org/A2756580918",
            "name": "Deepika Koundal",
            "affiliations": [
                "University of Petroleum and Energy Studies"
            ]
        },
        {
            "id": "https://openalex.org/A2115293496",
            "name": "Yanhui Guo",
            "affiliations": [
                "University of Illinois at Springfield"
            ]
        },
        {
            "id": "https://openalex.org/A4381437185",
            "name": "Zelal Mustafaoglu",
            "affiliations": [
                "University of Illinois at Springfield"
            ]
        },
        {
            "id": "https://openalex.org/A2756580918",
            "name": "Deepika Koundal",
            "affiliations": [
                "University of Petroleum and Energy Studies"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3161935901",
        "https://openalex.org/W4210312870",
        "https://openalex.org/W2962880853",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2017317305",
        "https://openalex.org/W2991305957",
        "https://openalex.org/W3123084617",
        "https://openalex.org/W2724222212",
        "https://openalex.org/W4200074822",
        "https://openalex.org/W3027128361",
        "https://openalex.org/W4206833365",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3193099849",
        "https://openalex.org/W2006958905",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4213145421"
    ],
    "abstract": "Spam email has accounted for a high percentage of email traffic and has created problems worldwide. The deep learning transformer model is an efficient tool in natural language processing. This study proposed an efficient spam detection approach using a pretrained bidirectional encoder representation from transformer (BERT) and machine learning algorithms to classify ham or spam emails. Email texts were fed into the BERT, and features obtained from the BERT outputs were used to represent the texts. Four classifier algorithms in machine learning were employed to classify the features of the text into ham or spam categories. The proposed model was tested using two public datasets in the experiments. The results of the evaluation metrics demonstrate that the logistic regression algorithm achieved the best classification performance in both datasets. They also justified the efficient ability of the proposed model in detecting spam emails. Received: 16 March 2022 | Revised: 21 April 2022 | Accepted: 22 April 2022 Conflicts of Interest Yanhui Guo is an editorial board member for Journal of Computational and Cognitive Engineering, and was not involved in the editorial review or the decision to publish this article. The authors declare that they have no conflicts of interest to this work.",
    "full_text": "Received: 16 March 2022 | Revised: 21 April 2022 | Accepted: 22 April 2022 | Published online: 24 April 2022\nRESEARCH ARTICLE\nSpam Detection Using Bidirectional\nTransformers and Machine\nLearning Classifier Algorithms\nYanhui Guo1,*, Zelal Mustafaoglu1 and Deepika Koundal2\n1Department of Computer Science, University of Illinois Springfield, USA\n2Department of Systemics, University of Petroleum and Energy Studies, India\nAbstract: Spam email has accounted for a high percentage of email traffic and has created problems worldwide. The deep learning transformer\nmodel is an efficient tool in natural language processing. This study proposed an efficient spam detection approach using a pretrained\nbidirectional encoder representation from transformer (BERT) and machine learning algorithms to classify ham or spam emails. Email\ntexts were fed into the BERT, and features obtained from the BERT outputs were used to represent the texts. Four classifier algorithms in\nmachine learning were employed to classify the features of the text into ham or spam categories. The proposed model was tested using\ntwo public datasets in the experiments. The results of the evaluation metrics demonstrate that the logistic regression algorithm achieved\nthe best classification performance in both datasets. They also justified the efficient ability of the proposed model in detecting spam emails.\nKeywords: spam detection, transfer learning, transformer, BERT, classifier, machine learning\n1. Introduction\nEmail is a popular communication mode. The number of email\nusers is projected to grow to 4.6 billion in 2025, and around 306 billion\nemails were exchanged globally in 2020 (Statista Research Department,\n2021). Individuals and organizations all over the world use email for both\ncasual and formal correspondence and exchange significant amounts of\ndata. However, this popularity comes with a problem: almost 85% of\nemails are spam emails that are unsolicited and often malicious,\ncosting businesses $20.5 billion every year (Cveticanin,2022). The\nremaining 15% of emails are legitimate and referred to as ham emails.\nSpam emails take up memory and waste computing power.\nAdvertisement emails make up 36% of spam emails, making it the most\ncommon spam category (Cveticanin,2022). Other categories of spam\nemails, such as phishing, scams, fraud, and identity theft, have malicious\nintentions and potentially more dire consequences. These incidents may\nharm individuals and institutions both financially and personally.\nAs the number of email users continues to grow, spam emails are\nincreasing in number, and attackers are becoming increasingly clever\nwith their tricks. The variety in the content of spam emails and the\nintentions of attackers introduces complexity. It is significant to\nidentify spam emails with high accuracy, and there are various\ndetection tools and techniques. The existing spam detection models\ncan be classified into computational models, machine learning\n(ML) models, and deep learning (DL) models.\nNumerous computational models use the negative selection\nalgorithm (NSA) for spam detection. Idris and Selamat (2014)\nproposed a spam email detection model that combined a particle swarm\noptimization (PSO) with a NSA to achieve stability, consistency, and\naccuracy. The real-value NSA is used to randomly create candidate\ndetectors. Particle swarm optimization (PSO) improves the detector\ngeneration process, with the local best as the optimum solution of the\nsystem. A local outlier factor (LOF) works as a fitness function to\nobtain the best features. Experimental results showed that it has 91.22%\naccuracy and an F1 score of 74.95%, which is higher than the PSO\nmodel at 81.32% accuracy and an F1 score of 71.84% and the NSA\nmodel at 68.86% accuracy and an F1 score of 36.01%. The NSA– PSO\nmodel outperformed another model proposed by Idris et al. (2014),\nwhich achieved 80.66% accuracy and an F1 score of 69.76%.\nML models use a variety of approaches and algorithms for spam\nclassification. Ahmed et al. (2022) surveyed ML techniques used in\nemail and discussed their findings. The survey demonstrated that\nsupervised learning was the most common approach with 57% of the\nanalyzed studies implementing supervised models. In comparison,\nunsupervised learning came in second at 29% and reinforcement\nlearning came last at 14%. Moreover, Naive Bayes (NB), logistic\nregression, and support vector machine (SVM) were the most\ncommonly used algorithms, with SVM generally achieving the best\nperformance out of the three. Karim et al. ( 2019)r e v i e w e d\nbioinspired spam detection algorithms and discussed some key\ninsights derived from their analysis. Supervised learning approach\ntakes a ratio of 67% in email spam detection methods compared to\nunsupervised learning with 19% and semisupervised learning with\n14%. In addition, NB was the most common algorithm followed by\nSVM, random forest (RF), and artificial neural network (ANN).\nMadhavan et al. (2021) validated the performance of k-nearest\nneighbor (KNN), NB, SVM, and rough set classifier for email spam\ndetection. The NB algorithm achieved the best performance with\n*Corresponding author: Yanhui Guo, Department of Computer Science,\nUniversity of Illinois Springfield, USA. Email:yguo56@uis.edu\nJournal of Computational and Cognitive Engineering\n2023, Vol. 2(1) 5– 9\nDOI: 10.47852/bonviewJCCE2202192\n© The Author(s) 2022. Published by BON VIEW PUBLISHING PTE. LTD. This is an open access article under the CC BY License (https://creativecommons.org/\nlicenses/by/4.0/).\n05\n99.46% accuracy, 98.46% recall, and 99.66% precision. KNN\nachieved 96.20% accuracy, 97.14% recall, and 87% precision;\nSVM achieved 96.90% accuracy, 95% recall, and 93.12%\nprecision; rough set classifier achieved 97.42% accuracy, 92.26%\nrecall, and 98.70% precision. Olatunji (2017) proposed a model\nbased on SVM, a statistical ML technique that can be used to model\ncomplex relationships among variables, which achieved an accuracy\nof 94.06%. Amjad and Gharehchopogh (2019) proposed a model\nthat uses improved KNN to classify test samples, which selected the\nk closest training examples in feature space and combined with the\nscatter search algorithm and feature selection for spam detection.\nThe model achieved 94.54% accuracy, 94.23% precision, 95.27%\nrecall, and an F1 score of 94.74% compared to other models such as\nRF at 93.89% accuracy and decision tree at 91.71% accuracy. Wang\net al. (2021) proposed a model that uses the LEP manifold learning\nalgorithm to extract the feature and employed the SVM algorithm for\nclassification. The model achieved 94.7% accuracy on the Enron-\nSpam dataset, 96.9% accuracy on the PU1 dataset, and 95.1%\naccuracy on the GenSpam dataset.\nDL-based models have yielded state-of-the-art results for spam\nemail detection. Sumathi and Pugalendhi (2020) proposed a DL\nmodel and employed the RF algorithm to calculate the features’\nattribute scores. The model achieved 88.59% accuracy,\noutperforming other classifier models such as KNN and SVM.\nSiddique et al. ( 2021) employed a long short-term memory\n(LSTM) and trained two ML models, NB and SVM, and a DL\nmodel, convolutional neural network (CNN), for comparison\npurposes. The LSTM model achieved 98.40% accuracy compared\nto NB at 98.00%, SVM at 97.50%, and CNN at 96.20%. In\naddition, the SVM model achieved 97% precision, 92.50% recall,\nand an F1 score of 95%; the NB model achieved 96.50%\nprecision, 95% recall, and an F1 score of 96%. Magdy et al.\n(2022) applied an ANN model to create a 3-fold classifier. The\nmodel achieved a maximum accuracy of 99.57% and a maximum\nprecision, recall, and F1 score of 99.68% on the Spambase dataset.\nA novel approach to spam detection involves transformers, and\nDL models that use the self-attention mechanism to perform natural\nlanguage processing (NLP) tasks. Transformers have a limited\nchoice of architectures during pretraining. Bidirectional encoder\nrepresentations from transformers (BERT) achieves greater\nperformance by using a masked language model to use pretrained\ndeep bidirectional representations (Devlin et al.,2019).\nAbdulNabi and Yaseen (2021) developed a BERT-based spam\ndetection method. The BERT-based model was compared to a\nbidirectional long short-termmemory (BiLSTM) model and a KNN-\nbased model and an NB-based model. Two datasets, Spambase from\nthe UCI Machine Learning Repository and the Spam Filter Dataset\nfrom Kaggle, were used. The BERT-based model outperformed the\nother models with an accuracy of 98.67% and an F1 score of 98.66%.\nThe BiLSTM model achieved 96.43% accuracy, the KNN model\nachieved 92.92% accuracy, and the NB model achieved 94.69% accuracy.\nTida and Hsu (2022) used a pretrained BERT uncased model. It\ncontains three fully connected linear layers with batch normalization\nlayers, fourdropout layers, and the ReLUandthe log softmaxactivation\nfunctions. The model attained 97% accuracy, 97% recall, and 95%\nprecisiononthecombineddataset;its highest F1scorewas 96.08%.The\nmodel achieved 97% accuracy on the Enron dataset, and an accuracy of\n98% on the SpamAssassin, LingSpam, and SpamText datasets.\nIn this study, we propose a BERT-based model to classify ham\nor spam emails. The novel contributions are as follows:\n A spam detection model based on a pretrained BERT is\nconstructed and trained on two different datasets.\n The performance of four classifier algorithms, SVM, KNN\nclassifiers, RF, and logistic regression, is compared.\n The results are analyzed using three evaluation metrics: precision,\nrecall, and F1 score.\nThe experimental results demonstrate that the logistic\nregression algorithm achieved the best classification performance\nwith 97.86% precision, 97.83% recall, and an F1 score of 97.84%\non the first dataset and 95.95% precision, 96% recall, and an F1\nscore of 95.92% on the second dataset.\nThe following parts are structured as follows: we introduce the\nmodel in Section 2, discuss our experimental results in Section 3, and\nconclude our study in Section 4.\n2. Proposed Method\n2.1. BERT\nRecently, DL methods, particularly recurrent neural networks\n(RNNs), have been applied in NLP. Many models have been\nproposed to solve the NLP tasks such as language modeling,\nmachine translation, and question answering.\nA transformer was introduced in Vaswani et al. (2017) and has\nreplaced RNN models for NLP problems.\nA transformer improves the self-attention mechanism, differentially\nweighting the significance of the input. It has no convolutional and\nrecurrent layers and uses a self-attention mechanism to build\nrelationships between all words in a sentence Wolf et al. (2020).\nBERT, one of the most popular transformer-based models, is an\nencoder stack of transformer structure and applies the bidirectional\ntraining of transformer to language modeling (Devlin et al.,2019).\nBERT architectures have extensive feedforward networks and\nattention heads. It takes a classification (CLS) token and a sequence\nof words as input. Each layer uses self-attention and passes the result\nthrough a feedforward network to the next encoder. The output\ncorresponding to the CLS token can be used for the classification task.\n2.2. Feature extraction\nTransfer learning uses the trained model to acquire knowledge\nfor a specific application whereas pretrained models were usually\ntrained using big datasets.\nIn this study, a pretrained BERT model produces word\nembedding from email texts, and they are then used as features to\nrepresent the texts for further processing.\n2.3. Classification\nAfter the features were obtained on the email text using a\npretrained BERT model, spam detection becomes a classification\nproblem, and a classifier in ML is used to solve it via classifying\nthe feature vectors into spam or ham categories. Supervised classi-\nfiers were first trained on the feature sets, and then the tuned classifiers\nwere employed to classify the unknown samples.\n3. Result and Discussion\n3.1. Dataset\nThis study used two publicly available datasets. After the\npreprocessing, the samples’ content is used for training and testing.\nThe first dataset, the Enron-Spam dataset, was published by\nAndroutsopoulos et al. ( 2006). The Enron dataset has 33,716\nemails, including 17,171 spam mails and 16,545 ham mails.\nJournal of Computational and Cognitive EngineeringVol. 2 Iss. 1 2023\n06\nThe second dataset, the spam or not spam dataset, was\npublished by Raftogiannis (2021). This dataset consists of 2,999\nvalid samples: 499 spam and 2,500 ham. The spam or not spam\ndataset is available on the Kaggle website.\n3.2. Experiment platform and settings\nAll experiments were implemented using Python and taken on a\nserver with an Intel Xeon processor with 128 GB memory and an\nNVIDIA Tesla K40 GPU having 12 GB memory.\nFive-folder cross-validation experiments were taken to validate\nthe performance of four famous supervised classifiers on two datasets.\n3.3. Evaluation results\nThree evaluation metrics, precision, recall, and F1 score, are\ndefined to evaluate the classification performance of different\nclassifier algorithms, which are defined as:\nPrecision ¼\nTP\nTP þ FP (1)\nRecall ¼ TP\nTP þ FN (2)\nF1 ¼ 2 /C2 Precision /C2 Recall\nPrecision þ Recall (3)\nwhere TP is true positive where spam samples are predicted as spam, TN\nis true negative where ham samples are predicted as ham, FP is false\npositive where spam samples are wrongly predicated as ham, and FN\nis false negative where ham samples are wrongly predicted as spam.\nA receiver operating characteristic curve (ROC) is employed to\nmeasure the diagnostic ability of a classifier with varied\ndiscrimination thresholds. It shows the relationship between the\ntrue-positive rate against the false-positive rate at various\nthreshold values. The area under the ROC curve is named AUC\nand used to compare the classifiers. The higher the AUC, the\nbetter the classification ability the classifier has.\nFour famous classifier algorithms, SVM, k-nearest neighbor\nclassifiers, random forest, and logistic regression are used to\nclassify spam emails and their performances are evaluated\nquantitatively. The metric results with average and standard\ndeviation on dataset 1 are compared in Table1 and the results on\ndataset 2 are compared in Table2. We can see that the logistic\nregression algorithm attains the highest precision, F1 score, and\nAUC values on both datasets. SVM achieves the highest recall\nvalue on the second dataset. The ROCs for four algorithms are\nshown in Figures1, 2, 3 and 4.\nTable 1\nEvaluation results for different machine learning algorithms on dataset 1\nAlgorithm Precision Recall F1 score AUC\nSVM 0.9772 ± 0.0102 0.9769 ± 0.0102 0.9770 ± 0.0102 0.9964 ± 0.0028\nLogistic Regression 0.9786 ± 0.0081 0.9783 ± 0.0081 0.9784 ± 0.0081 0.9971 ± 0.0024\nRandom forest 0.9639 ± 0.0204 0.9634 ± 0.0204 0.9635 ± 0.0204 0.9946 ± 0.006\nKNN 0.9654 ± 0.0308 0.9637 ± 0.0343 0.964 ± 0.034 0.9905 ± 0.0069\nTable 2\nEvaluation results for different machine learning algorithms on dataset 2\nAlgorithm Precision Recall F1 score AUC\nSVM 0.9553 ± 0.0278 0.9656 ± 0.037 0.9596 ± 0.0127 0.9943 ± 0.0053\nLogistic Regression 0.9595 ± 0.0337 0.9600 ± 0.0338 0.9592 ± 0.0232 0.9950 ± 0.0045\nRandom forest 0.9591 ± 0.0365 0.8692 ± 0.0296 0.9064 ± 0.0284 0.9847 ± 0.0137\nKNN 0.9372 ± 0.0545 0.9251 ± 0.0394 0.9307 ± 0.0437 0.9794 ± 0.0139\nFigure 1\nROC of the SVM algorithm on two datasets\nJournal of Computational and Cognitive EngineeringVol. 2 Iss. 1 2023\n07\n4. Conclusion\nIn this study, an efficient spam detection model was proposed\nbased on a BERT model and supervised learning classifier to detect\nspam emails. Email texts were represented via the features obtained\nfrom the BERT outputs, and classifier algorithms in machine\nlearning were employed to classify the feature vectors into ham or\nspam categories. The experimental results demonstrate that the\nlogistic regression algorithm achieved the best classification\nperformance in two publicly available datasets. To sum up, there\nis a promotion to use the BERT model and classifier in spam\ndetection.\nThis study can be extended to various applications, e.g., spam\nmessages detection in a mobile system and fake news detection in\nsocial media platforms. This study demonstrates the high ability\nof the BERT model to interpret text and provides salient features\nfor future processing. Further research in combing more\ncomprehensive layers inside the BERT is encouraged to further\nvalidate the proposed framework.\nConflicts of Interest\nYanhui Guo is an editorial board member for Journal of\nComputational and Cognitive Engineering, and was not involved\nin the editorial review or the decision to publish this article. The\nauthors declare that they have no conflicts of interest to this work.\nFigure 2\nROC of the logistic regression algorithm on two datasets\nFigure 3\nROC of the random forest algorithm on two datasets\nFigure 4\nROC of the k-nearest neighbor algorithm on two datasets\nJournal of Computational and Cognitive EngineeringVol. 2 Iss. 1 2023\n08\nReferences\nAbdulNabi, I., & Yaseen, Q. (2021). Spam email detection using\ndeep learning techniques. Procedia Computer Science, 184,\n853– 858. https://doi.org/10.1016/j.procs.2021.03.107\nAhmed, N., Amin, R., Aldabbas, H., Koundal, D., Alouffi, B., &\nShah, T. (2022). Machine learning techniques for spam\ndetection in email and IOT platforms: Analysis and research\nchallenges. Security and Communication Networks , 2022,\n1– 19. https://doi.org/10.1155/2022/1862888\nAndroutsopoulos, I., Metsis, V., & Paliouras, G. (2006). The\nEnron-spam datasets. Retrieved from: http://www2.aueb.gr/\nusers/ion/data/enron-spam/\nAmjad, S., & Gharehchopogh, F. S. (2019). A novel hybrid approach\nfor email spam detection based on scatter search algorithm and\nk-nearest neighbors. Journal of Advances in Computer\nEngineering and Technology, 5(3), 181– 194. https://journals.\nsrbiau.ac.ir/article_14397.html\nCveticanin, N. (2022). What ’s on the other side of your\ninbox - 20 spam statistics for 2022.https://dataprot.net/statistics/\nspam-statistics/\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT:\nPre-training of deep bidirectional transformers for language\nunderstanding. Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\n1, 4171– 4186. https://arxiv.org/abs/1810.04805\nIdris, I., & Selamat, A. (2014). Improved email spam detection\nmodel with negative selection algorithm and particle swarm\noptimization. Applied Soft Computing, 22,1 1– 27. https://doi.\norg/10.1016/j.asoc.2014.05.002\nIdris, I., Selamat, A., & Omatu, S. (2014). Hybrid email spam\ndetection model with negative selection algorithm and\ndifferential evolution. Engineering Applications of Artificial\nIntelligence, 28,9 7– 110. https://doi.org/10.1016/j.engappai.\n2013.12.001\nKarim, A., Azam, S., Shanmugam, B., Kannoorpatti, K., & Alazab,\nM. (2019). A comprehensive survey for intelligent spam email\ndetection. IEEE Access, 7, 168261– 168295. https://doi.org/10.\n1109/access.2019.2954791\nMadhavan, M. V., Pande, S., Umekar, P., Mahore, T., &\nKalyankar, D. (2021). Comparative analysis of detection of\nemail spam with the aid of machine learning approaches.\nIOP Conference Series: Materials Science and Engineering,\n1022(1), 012113. https://doi.org/10.1088/1757-899x/1022/1/\n012113\nMagdy, S., Abouelseoud, Y., & Mikhail, M. (2022). Efficient spam\nand phishing emails filtering based on deep learning.Computer\nNetworks, 206, 108826. https://doi.org/10.1016/j.comnet.\n2022.108826\nOlatunji, S. O. (2017). Improved email spam detection model based\non support vector machines. Neural Computing and\nApplications, 31(3), 691 – 699. https://doi.org/10.1007/\ns00521-017-3100-y\nRaftogiannis, N. (2021). Simple spam email classifier ∼= 95%.\nRetrieved from: https://www.kaggle.com/nikosraftogiannis/\nsimple-spam-email-classifier-95\nSiddique, Z. B., Khan, M. A., Din, I. U., Almogren, A., Mohiuddin,\nI., & Nazir, S. (2021). Machine learning-based detection of\nspam emails. Scientific Programming, 2021,1 – 11. https://\ndoi.org/10.1155/2021/6508784\nSumathi, S., & Pugalendhi, G. K. (2020). Cognition based Spam\nMail text analysis using combined approach of deep neural\nnetwork classifier and random forest. Journal of Ambient\nIntelligence and Humanized Computing, 12(6), 5721– 5731.\nhttps://doi.org/10.1007/s12652-020-02087-8\nStatista Research Department. (2021). Number of e-mail users\nworldwide 2017-2025. Retrieved from: https://www.statista.\ncom/statistics/255080/number -of-e-mail-users-worldwide/\nTida, V. S., & Hsu, S. H. (2022). Universal spam detection using\ntransfer learning of Bert model. In Proceedings of the\nAnnual Hawaii International Conference on System Sciences.\nhttps://doi.org/10.24251/hicss.2022.921\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\nGomez, A. N.,::: , & Polosukhin, I. (2017). Attention is all\nyou need. Advances in Neural Information Processing\nSystems, 30.https://doi.org/10.48550/arXiv.1706.03762\nWang, C., Li, Q., Ren, T. Y., Wang, X. H., & Guo, G. X. (2021).\nHigh efficiency spam filtering: A manifold learning-based\napproach. Mathematical Problems in Engineering , 2021,\n1– 7. https://doi.org/10.1155/2021/2993877\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,\nMoi, A., ::: , & Rush, A. (2020). Transformers: State-of-\nthe-art natural language processing. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations (pp. 38– 45).\nhttps://doi.org/10.18653/v1/2020.emnlp-demos.6\nHow to Cite:Guo, Y., Mustafaoglu, Z., & Koundal, D. (2023). Spam Detection\nUsing Bidirectional Transformers and Machine Learning Classifier Algorithms.\nJournal of Computational and Cognitive Engineering2(1), 5– 9, https://doi.org/\n10.47852/bonviewJCCE2202192\nJournal of Computational and Cognitive EngineeringVol. 2 Iss. 1 2023\n09"
}