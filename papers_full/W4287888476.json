{
  "title": "Few-shot Subgoal Planning with Language Models",
  "url": "https://openalex.org/W4287888476",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2396426249",
      "name": "Lajanugen Logeswaran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101904927",
      "name": "Yao Fu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2513695887",
      "name": "Moontae Lee",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A2117954069",
      "name": "Honglak Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4301259831",
    "https://openalex.org/W3121854931",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3126325318",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964915587",
    "https://openalex.org/W3104055253",
    "https://openalex.org/W4287079536",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2998557583",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1933065844",
    "https://openalex.org/W4287026929",
    "https://openalex.org/W3112356180",
    "https://openalex.org/W3173798466",
    "https://openalex.org/W2971077754",
    "https://openalex.org/W3156012351",
    "https://openalex.org/W3154669786",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3173781631",
    "https://openalex.org/W2962846267",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W4287026640",
    "https://openalex.org/W2122223050",
    "https://openalex.org/W3043172396",
    "https://openalex.org/W4293566037",
    "https://openalex.org/W2222235228",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4205181707",
    "https://openalex.org/W3034758614",
    "https://openalex.org/W3099204253",
    "https://openalex.org/W2995264919",
    "https://openalex.org/W3099130708",
    "https://openalex.org/W3166792158",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4286905705",
    "https://openalex.org/W3091195047"
  ],
  "abstract": "Lajanugen Logeswaran, Yao Fu, Moontae Lee, Honglak Lee. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5493 - 5506\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nFew-shot Subgoal Planning with Language Models\nLajanugen Logeswaran∗, Yao Fu‡, Moontae Lee∗†, Honglak Lee∗‡\nLG AI Research∗, University of Illinois at Chicago†, University of Michigan‡\nAbstract\nPre-trained language models have shown suc-\ncessful progress in many text understanding\nbenchmarks. This work explores the capability\nof these models to predict actionable plans in\nreal-world environments. Given a text instruc-\ntion, we show that language priors encoded\nin pre-trained models allow us to infer fine-\ngrained subgoal sequences. In contrast to re-\ncent methods which make strong assumptions\nabout subgoal supervision, our experiments\nshow that language models can infer detailed\nsubgoal sequences from few training sequences\nwithout any fine-tuning. We further propose\na simple strategy to re-rank language model\npredictions based on interaction and feedback\nfrom the environment. Combined with pre-\ntrained navigation and visual reasoning com-\nponents, our approach demonstrates compet-\nitive performance on subgoal prediction and\ntask completion in the ALFRED benchmark\ncompared to prior methods that assume more\nsubgoal supervision.\n1 Introduction\nDeveloping autonomous agents that can complete\nspecific tasks given goal descriptions embodies\nhuman-level intelligence. Successful agents in this\nsetting require multiple reasoning capabilities in-\ncluding natural language understanding, visual rea-\nsoning, and acting over long temporal horizons.\nTraining black-box models that map instructions\nand observations to suitable actions has proven to\nbe difficult due to challenges in interpreting and\nreasoning with multimodal information, especially\nin the absence of strong supervision. Thus, general-\nization in this setting demands effective strategies\nfor planning, exploration, and incorporating feed-\nback from the environment.\nGeneralization in human agents, on the other\nhand, stems from our ability to naturally brainstorm\n∗Correspondence to llajan@lgresearch.ai\nMove a heated slice of apple to the white table with \nshelving below\n1) slice apple, heat in microwave, put in side table.\n2) slice apple, heat in microwave, put in shelf.\n3) slice apple, heat in microwave, put in dining table.\n…\n1) slice apple, heat in microwave, put in dining table.\n2) slice apple, heat in microwave, put in side table.\n3) slice apple, heat in microwave, put in shelf.\n…\nIn-context learning with language model\nPut a rinsed slice of apple on the table = slice apple, clean it, put in …\nPut a plate with a watch on it on the table = pick up watch, put in plate, ..\n …\nRe-rank based on interaction with environment\nCandidate \nhypotheses\nRe-ranked \nhypotheses\n…turn \nleft \npickup \nknife\nput\nshelf\nsuccess/failure \nfeedback\nQuery \nInstruction\nFigure 1: Overview of subgoal prediction approach. (a) A pre-\ntrained language model prompted with a sequence of training\ninstances, i.e., (instruction, subgoal sequence) pairs, and a\nquery instruction predicts top-k hypotheses using beam search.\n(b) These predictions are then re-ranked by incorporating\ninformation about the environment.\nabstract subgoals, better calibrating executable ac-\ntions and their sequences. Planning at the level\nof subgoals instead of low-level actions allows us\nto better adapt to unfamiliar settings. We posit\nthat language supervision can help realize such\nplanning capabilities effectively in artificial agents.\nFirst, text is a natural API for interacting with intel-\nligent agents that act in the real world to complete\ntasks. Knowledge available in the form of text cor-\npora, descriptions and instructions can be exploited\nto build better agents (Branavan et al., 2012; Zhong\net al., 2020). Second, strong language priors are\nuseful to reason about causal sequences of events\n(Li et al., 2021). Language priors can further in-\nform about object affordances (e.g. an apple is\nsliceable, whereas a table is not) and other contex-\ntual knowledge (e.g. a slicing task is more likely to\nbe performed in a kitchen than a bathroom) (Chen\n5493\net al., 2020). Recent advances have demonstrated\nthat large language models are able to capture such\npriors, as evidenced by their strong capabilities in\nlanguage understanding and beyond (Devlin et al.,\n2019; Radford et al., 2019; Brown et al., 2020;\nBommasani et al., 2021). This leads to the natu-\nral question of whether priors learned by language\nmodels can help reason about subgoals.\nWe study the ability of language models to rea-\nson about plans composed of a sequence of inter-\nmediate goals for completing basic object manipu-\nlation tasks in a household environment specified\nusing text instructions. In particular, we use the\nin-context learning ability (Brown et al., 2020) of\nlarge pre-trained language models to reason about\nsubgoals. In contrast to prior methods that fine-\ntune language models to predict subgoals/actions\n(Jansen, 2020; Yao et al., 2020), we show that\nthey can predict subgoal sequences effectively with-\nout any fine-tuning. We teach the model how in-\nstructions translate into subgoal sequences by con-\nstructing a prompt using few examples. Given the\nprompt and a query instruction the language model\npredicts likely subgoal sequences (see Figure 1 for\nan illustration).\nWhile language models are capable of generat-\ning strong hypotheses, we observe that these predic-\ntions may not be directly usable by agents acting in\nreal environments. First, they suffer from calibra-\ntion issues: Language models have a tendency to\nrepeat content from the prompt (Zhao et al., 2021).\nWe show that mutual-information inspired metrics\nhelp mitigate calibration issues and lead to better\nranking of model generated hypotheses.\nSecond, real-world agents have to update their\nbeliefs and predictions based on interaction and\nfeedback from the environment. Without such feed-\nback we cannot expect the predicted plan to be\nexecutable in the environment. We execute plans\nproposed by the language model in the environ-\nment using a pre-trained low-level policy and col-\nlect feedback about task success/failure. We use\nthis feedback as a learning signal to train a ranking\nmodel that re-ranks language model predictions. In\ncontrast to prior methods that rely on strong sub-\ngoal supervision and task level expert trajectories,\nwe show that combining subgoal predictions with\na pre-trained subgoal execution policy leads to a\nstrong embodied agent baseline.\nWe make the following contributions in this\nwork. We show that\n• Large language models can predict subgoals from\ntext instructions with very little supervision using\nin-context learning.\n• Incorporating a small amount of feedback from\ninteraction with the environment such as agent\nstate and task success/failure outcome improves\nlanguage model predictions.\n• Combining predicted subgoals with a pre-trained\nlow-level policy for navigation and visual reason-\ning leads to a simple modular agent policy that\nperforms well on an embodied learning setting.\n2 Related work\nLanguage models for planning and interaction\nThe use of language models for planning and action\nprediction has been explored in prior work. Jansen\n(2020) fine-tuned a language model to predict sub-\ngoal sequences for text instructions from the AL-\nFRED benchmark. Micheli and Fleuret (2021) take\na similar approach, but show that imitation learning\nwith few instances combined with reinforcement\nlearning produces models that work well on the\nALFWorld benchmark (Shridhar et al., 2021). Yao\net al. (2020) demonstrate a similar approach for in-\nteractive fiction games (Hausknecht et al., 2020). In\ncontrast to these prior methods, our approach does\nnot assume strong supervision and we demonstrate\ngeneralization with limited training examples. Fur-\nthermore, in order to exploit the generalization ca-\npabilities of large language models, we do not fine-\ntune these models and instead use their in-context\nlearning ability. Finally, our approach allows us to\nbuild policies that inherit the strong generalization\ncapabilities of these large pre-trained models such\nas compositional generalization.\nLarge language models and few-shot learning\nBrown et al. (2020) showed that pre-trained large\nlanguage models have few-shot learning capabil-\nities. Given a few examples {(xi,yi = f(xi))}\nthat define a task f such as classification or trans-\nlation and a query instance xq, prompting a lan-\nguage model with a string such as \"x1 = y1; x2 =\ny2; ...; xn = yn; xq =\" leads to meaningful com-\npletions by the language model yq ≈f(xq). This\nfew-shot learning capability of language models\nhas since then been studied and improved upon\nwith approaches like prefix engineering (Schick\nand Schütze, 2021), prompt tuning (Li and Liang,\n2021), model calibration (Zhao et al., 2021) and\nother methods (Min et al., 2021a). We adopt a\nsimilar approach for few-shot subgoal inference.\n5494\nWe assume that subgoal supervision is available\nfor a small number of training tasks and use the\nlanguage model to infer subgoals for unseen tasks.\nInstruction following There is rich literature on\nagents that follow language instructions (Branavan\net al. (2009); Mei et al. (2016); Fried et al. (2018);\nSuhr et al. (2019) inter alia1). Recent develop-\nments in simulated environments and benchmarks\nwith human annotated instructions have driven\nprogress in embodied agents that learn from text\ninstructions (Shridhar et al., 2020; Kolve et al.,\n2017). Successful agents in these settings require\nmultiple reasoning capabilities including language\nunderstanding, visual reasoning and learning to act\nover long time-horizons. Recent embodied learning\nliterature exploit subgoal supervision, pre-trained\nvisual reasoning components and pre-trained trans-\nformer models to do well on the task (Singh et al.,\n2020; Suglia et al., 2021; Zhang and Chai, 2021;\nCorona et al., 2021; Blukis et al., 2022). Unlike\nthese methods, we do not assume access to strong\nsubgoal supervision or task level expert supervision.\nWe combine language model predictions with pre-\ntrained low-level navigation and interaction poli-\ncies to obtain a competitive agent policy.\nFew-shot semantic parsing Subgoal inference\nfrom text instructions can be considered a seman-\ntic parsing problem where the subgoal sequences\nserves as a formal representation of text. Shin et al.\n(2021) show that few-shot semantic parsers can\nbe derived from language models and demonstrate\ntheir applicability on text-to-SQL (Finegan-Dollak\net al., 2018) and SCAN (Lake and Baroni, 2018)\nbenchmarks. Furrer et al. (2020) and Herzig et al.\n(2021) further study the compositional generaliza-\ntion ability of such semantic parsers. In our work\nwe make use of ideas introduced in these works\nsuch as dynamic prompt creation, constrained de-\ncoding and intermediate representations.\n3 Approach\nWe first consider subgoal inference as a semantic\nparsing problem where a text instruction needs to\nbe translated to a sequence of subgoals and propose\nan approach to few-shot subgoal inference based\non pre-trained language models in Section 3.1. We\nextend this setting to an agent acting in a simulated\nenvironment which can execute these subgoals, ob-\nserve feedback, and improve upon language model\n1See Luketina et al. (2019) for a comprehensive survey\npredictions for more accurate subgoal inference in\nSection 3.2.\n3.1 Few-shot subgoal inference\nSubgoals We are interested in a particular sub-\nclass of instruction following problems which in-\nvolve performing a sequence of object interac-\ntions in an embodied environment. Each object\ninteraction requires navigating to a particular ob-\nject and performing an action on it. A task is\nconsidered successfully completed if the state of\nobjects in the end satisfy a set of task-specific\nconstraints (for instance, objects that need to be\nsliced/warmed/cooled/cleaned have the appropriate\nstate change). It is thus natural to define a subgoal\nas one or more sequence of object interactions. A\nsubgoal gis specified as g= (b,o) ∈B×O where\nb∈B = {Pickup, Clean, Heat, ..} is one of a pre-\ndefined set of abstract actions and o∈O ={Apple,\nMicrowave, DeskLamp, Ottoman, ..} is an object\ncategory.\nSubgoal inference problem Given a text instruc-\ntion τ, subgoal inference seeks to predict a se-\nquence of subgoals τ ↦→g = (g(1),...,g (n)). To\nperform in-context learning with a language model,\nwe consider a representation v(g) of gthat looks\nlike natural text, where vis a pre-defined invertible\nmapping. Such representations have been referred\nto in the literature as verbalizers (Min et al., 2021a),\nintermediate representations (Herzig et al., 2021)\nand canonical representations (Shin et al., 2021),\nwhere the purpose is to represent the output in a\nformat the language model understands. In a slight\nabuse of notation, we will use gto refer to either a\nsubgoal sequence or it’s textual representation v(g)\ndepending on the context.\nGenerating subgoals We assume that a small\namount of training data {(τ1,g1),··· ,(τn,gn)}is\ngiven. The language model is prompted with a\ncomma separated concatenation of the training ex-\namples, each in the format \"τi = gi\", followed by\na query τ, formatted as \"τ =\". We assume that the\nprobability of a hypothesis h(i.e., text representa-\nton of a subgoal sequence) can be modeled as in\nEquation (1), where hi is the ith token of hand the\ntoken probabilities are derived from the language\nmodel.\np(h|τ) =\n∏\ni\npLM(hi|h<i,τ, {τj,gj}n\nj=1) (1)\nWe use beam search to identify the top-k hypothe-\nses according to p(h|τ). Generated hypotheses are\n5495\nTask\nMove a heated slice of apple to the white table with \nshelving below\nSubgoals completed: slice apple, heat in microwave\nLM predictions\n1. slice apple, heat in microwave, put in shelf.\n2. slice apple, heat in microwave, put in dining table.\n3. pick up apple, put in shelf. \n…\nRe-ranked predictions\n1. slice apple, heat in microwave, put in dining table.\n2. slice apple, heat in microwave, put in shelf.\n…\nAgent \nstate\nUpdate subgoals\n(Put,diningtable)\nRe-rank\nExecute \nsubgoal\nFigure 2: Re-ranking language model predictions with interaction and feedback from the environment. Given the task, language\nmodel predictions, completed subgoals and the agent state we come up with a ranked list of subgoal sequences. The agent then\nexecutes the next subgoal from the highest ranked plan. The completed subgoal is added to the partial plan and the process\ncontinues until stop subgoal is encountered. During training the agent receives a positive reward if the task is successfully\ncompleted, which we use as supervision to train the ranking model.\nconstrained to be valid representations of subgoal\nsequences by considering only tokens which lead\nto a valid partial prefix of a subgoal sequence at\neach step of beam search.\nRe-ranking predictions Recent studies have\nfound that language models have popularity and\nrecency biases: the tendency to repeat content men-\ntioned in the prompt, especially content appearing\nlater in the prompt (Zhao et al., 2021). They consid-\nered a simple approach to mitigate such biases in\nmodel predictions for classification tasks by com-\nparing the likelihood of an output label with and\nwithout the query. In contrast to this ‘direct model’\nwhich models the probability of a label given the in-\nput p(y|x), Min et al. (2021a) showed that a ‘chan-\nnel model’ which models p(x|y) leads to better,\nmore stable models.\nInspired by these observations, we propose to\nuse p(τ|h) to score hypotheses in addition to\np(h|τ). Mutual Information based ranking metrics\nare a natural candidate and they have been explored\nin the text generation literature (Li et al., 2016; Li\nand Jurafsky, 2016). We generate multiple hypothe-\nses from the model using p(h|τ) and the generated\nhypotheses are re-scored using the weighted mutual\ninformation metric (1−λ)log p(h|τ)+λlog p(τ|h)\nwhere λis a hyperparameter2. To compute p(τ|h),\nwe again use the language model prompted with\n\"g1 = τ1,...,g n = τn,h =\" as the query and com-\npute the conditional probability of τ. We expect\nthis paradigm of generating a set of strong hypothe-\nses, followed by accurate re-ranking is more gen-\nerally applicable to other few-shot language under-\nstanding problems.\n2Appendix A details the connection to Mutual Information.\n3.2 Agent policy and incorporating\nenvironment feedback\nWe next consider building an agent that acts in a\nvisual environment to complete tasks given text in-\nstructions. While Section 3.1 treated the language\nmodel as a knowledge extraction system, in the real\nworld plans need to be updated based on interac-\ntion and feedback from the environment. We thus\npropose a method to improve language model pre-\ndictions based on environment interaction. Since\nour goal is to learn the planning component of the\nagent, we assume a pre-trained low-level policy\nis provided and optimize over the space of plans.\nJointly learning both components is beyond the\nscope of this work and left as future work.\nWe assume that a representation of the agent\nstate sis available. The state representation cap-\ntures information about the environment (e.g. ob-\njects present and their locations) estimated based\non the agent’s visual observations. As the agent\nexplores the environment and collects new obser-\nvations the state representation is updated. As-\nsuming that a low-level policy πL pre-trained to\nexecute a given subgoal is provided, our goal is\nto train a high-level policy πH which proposes\nthe subgoals to be executed by the low-level pol-\nicy. More formally, the high-level policy models\nπH(g(t)|τ,st,g(<t)) where τ is a text instruction,\nst is the state representation and g(<t) is the se-\nquence of subgoals completed so far at high-level\ntime-step t3.\nWhile the language model can generate com-\npelling subgoal hypotheses, it doesn’t take into\naccount information about the environment. For\ninstance, knowledge about the type of room the\n3Alternatively, this can be framed as a POMDP in a hierar-\nchical reinforcement learning setting.\n5496\nagent is in (kitchen, bathroom, etc.) and the ob-\njects present in it are useful to infer the kind of\ntasks and subgoals that can be performed. We\npropose to re-rank hypotheses generated by the\nlanguage model based on information from the en-\nvironment to construct πH. The plans generated\nby the language model are executed in the envi-\nronment using πL. The success/failure outcomes\nof these plan executions are used to construct a\nlabeled dataset of instructions τ, plans gand agent\nstate s. A supervised ranking model f(g,τ,s ; θ)\nis trained using this data to re-rank the language\nmodel predictions. We represent the ranking model\nas f(g,τ,s ; θ) = θTconcat(fstate(s),ftext(τ,g))\nwhere fstate(s) is a state embedding, ftext(τ,g) is\na joint encoding of τ and g produced by a text\nencoder and θ is a parameter vector. Although\na text embedding can be derived from the lan-\nguage model, we use a BERT encoder in favor\nof obtaining a smaller dimensional representation\n(ftext = BERTCLS). See Appendix C for more\ndetails.\nDuring inference, an instruction τ is given, and\nwe use the procedure in Section 3.1 to generate\ntop-k hypotheses. At each step, hypotheses incon-\nsistent with the sequence of subgoals executed so\nfar are pruned and the remaining hypotheses are\nre-ranked based on the current agent state using\nf. The agent attempts the next subgoal proposed\nby the top hypothesis. The process ends when the\nstop subgoal is predicted. See Figure 2 for an illus-\ntration and Appendix D for more details about the\ntraining and inference algorithms.\n4 Experiments\n4.1 Data\nWe use data from the ALFRED benchmark pro-\nposed by Shridhar et al. (2020) in our experiments.\nThe ALFRED task requires an agent to execute\ninstructions specified in text to accomplish basic\ntasks in an embodied environment. A given task is\ndescribed using a high-level language directive as\nwell as low-level step-by-step instructions (We only\nuse the high-level description). The dataset consists\nof 7 task types (and 11 fine-grained types), and has\nmore than 20k natural language task descriptions\ncollected from human annotators. In addition, ex-\npert demonstrations computed by a planner are also\nmade available. Tasks require acting over many\ntime-steps, with an average of 50 actions, and the\nlongest tasks require 100+ steps.\nThe ground truth subgoal sequences in the\ndataset consist of both navigation subgoals and ob-\nject interaction subgoals. We discard the navigation\nsubgoals and only retain the interaction subgoals\nfor the following reasons. First, the interaction\nsubgoals are sufficient for an agent to successfully\ncomplete the task. Second, predicting navigation\nsubgoals from the text instruction alone may not al-\nways be possible as they often depend on the scene\nlayout.\nSubgoal representation A subgoal gs is spec-\nified as gs = (b,o) ∈ B×O where |B| = 7\nand |O| = 80. We define a textual representa-\ntion v(b) of each action type (e.g. v(Pickup) =\n‘pick up’, v(Heat) = ‘heat in’). The object types\noare identified by a text string v(o) in the dataset\nand we directly use them as the text representa-\ntion with minimal pre-processing (e.g. v(apple) =\n‘apple’, v(desklamp) = ‘desk lamp’). The sub-\ngoal is represented as v(gs) =‘v(b) v(o)’ (e.g.\nv((Pickup, apple)) = ‘pick up apple’). A sub-\ngoal sequence g = (g(1),...,g (n)) is represented\nas v(g) =‘v(g(1)), ..., v(g(n)).’. Text representa-\ntions of all subgoals are given in Appendix B. Note\nthat there are many plausible choices for the repre-\nsentation vand a different set of choices can lead\nto different results.\nMetrics We use top-k recall to evaluate the abil-\nity of language models to generate plans from in-\nstructions by comparing against ground truth plans.\nIn addition, we also evaluate the performance of an\nagent acting in the AI2-Thor (Kolve et al., 2017)\nsimulator to complete tasks using task success rate\n(the percentage of tasks successfully completed).\n4.2 Few-shot subgoal inference\nWe construct a training set of N = 22instances by\nrandomly choosing two instances per fine-grained\ntask type. The language model is prompted with\na concatenation of these training examples and\nthe query instance. We perform constrained beam\nsearch decoding with a beam size of 10 to generate\nsubgoal sequences. At each step of beam search,\nonly tokens which lead to a valid partial prefix of a\nsubgoal sequence are considered. All model gener-\nated hypotheses thus correspond to valid subgoal\nsequences. We evaluate models on the valid-seen\nsplit of the dataset which has 800 instances.\nTable 2 shows subgoal inference results cate-\ngorized by task type. We use publicly available\npre-trained transformer language models GPT2-XL\n(Radford et al., 2019) and GPT-J (Wang and Komat-\n5497\nTop-krecall Ranking criteria\nGPT2-XL GPT-J\nN = 11 N = 22 N = 33 N = 11 N = 22 N = 33\nk= 10 log p(h|τ) 47.93 58.54 59.51 64.02 71.10 72.07\nk= 1 log p(h|τ) 16.71 23.29 23.41 30.12 36.10 35.85\nk= 1 log p(τ|h) 19.88 22.07 25.61 34.15 34.39 35.00\nk= 1 1\n2 log p(h|τ) + 1\n2 logp(τ|h) 29.88 32.20 33.17 44.02 47.80 49.63\nTable 1: Re-ranking model generated hypotheses using different criteria. The first section shows top-10 recall of generated\nhypotheses (using p(h|τ)). The second section shows top-1 recall after re-ranking these hypotheses using different criteria.\nResults are shown for GPT2-XL and GPT-J models when the number of training instances N is varied.\nTask\nGPT2-XL GPT-J\ntop-1 top-10 top-1 top-10\nlook at obj in light 34.04 80.85 42.55 80.85\npick and place simple 25.35 59.15 48.59 73.24\npick two obj and place 28.23 68.55 26.61 70.16\npick heat then place 27.10 71.03 47.66 85.98\npick cool then place 30.95 65.87 39.68 80.16\npick clean then place 11.61 50.00 34.82 73.21\npick place movable 6.09 17.39 12.17 35.65\nOverall 23.29 58.54 36.10 71.10\nTable 2: Top-k recall for subgoal sequences predicted by\nGPT2-XL and GPT-J models categorized by task type.\nsuzaki, 2021) via the HuggingFace library (Wolf\net al., 2020), which respectively have 1.5B and 6B\nparameters, in our experiments. The first six of the\nseven task types have two object arguments each.\nThe pick place movable task type has three object\narguments and hence a lower recall than the other\ntask types. The top-10 recall of GPT2-XL and GPT-\nJ are respectively 59% and 71%, which shows that\nlarge language models have strong ability to reason\nabout plans from few training examples.\nRe-ranking hypotheses The top-k recall perfor-\nmance reported in Table 2 is based on log p(h|τ).\nWe confirmed that the biases reported in the liter-\nature such as predicting content from the prompt\nare present in model predictions (Zhao et al., 2021).\nConsider the query example Place a martini glass\nwith a fork on it on the table . The following two\nare among the top generated hypotheses:\na) pick up fork, put in cup, pick up cup, put in sink.\nb) pick up fork, put in cup, pick up cup, put in\ntable.\nWhen the prompt contains training examples that\nmention ‘sink’, the model assigns the following log\np(h|τ) to these hypotheses: a) -2.4 and b) -4.3.\nHowever, when all training instances in the prompt\ninvolving ‘sink’ are removed, the log probabilities\nnow become, a) -13.7 and b) -9.1 The incorrect hy-\npotheses involving ‘sink’ is now ranked below the\ncorrect hypothesis involving table. While language\nmodels can retrieve strong hypotheses as indicated\nby the high top-10 recall, this observation shows\nthat the ranking of these hypotheses, as determined\nby p(h|τ), may not be accurate. We thus consider\nmutual information based ranking approaches. Ta-\nble 1 shows top-1 recall when model generated\nhypotheses are ranked according to different crite-\nria. We also vary the number of training examples\nN by randomly choosing respectively 1, 2 and 3\ninstances per fine-grained task type.\nWe first observe that p(τ|h) ranks hypotheses\nbetter than p(h|τ) with very limited supervision\n(N = 11). However, it is often worse when more\nsupervision is available. In contrast, combining the\ntwo log probabilities withλ= 1\n2 yields consistently\nbetter performance across models and number of\ntraining examples. This shows that generating a\nlarge number of hypotheses with a language model,\nfollowed by more accurate re-ranking using Mutual\nInformation inspired metrics can be an effective\nparadigm for few-shot generation tasks with in-\ncontext learning.\nComparison with prior work We compare our\nprediction performance against prior work in Ta-\nble 3. Jansen (2020) fine-tunes a GPT2-Medium\nmodel (325M parameters) to predict subgoals from\ninstructions and report prediction results4 when the\nmodel is trained on varying amounts of training\ndata: 1%, 10%, 25%, 100% of the training set,\nwhich has 7793 instances. We ignore the naviga-\ntion subgoals in this evaluation and only compare\nthe sequence of object interactions. We report pre-\ndiction performance of GPT-J using our approach\non the same test set. The results show that large\nlanguage models encode useful knowledge that can\nhelp plan from instructions effectively when su-\npervision is limited. However, fine-tuning can be\neffective when more supervision is available due\nto the fixed context length limitation of in-context\n4https://github.com/cognitiveailab/alfred-gpt2\n5498\nModel Top-1 Training\nrecall instances\nJansen (2020)\n(Fine-tuned GPT2-Medium)\n5.07 77\n41.92 779\n53.80 1948\n61.00 7793\nOurs\n(In-context GPT-J)\n44.02 11\n47.80 22\n49.63 33\nTable 3: Comparison against subgoal prediction performance\nof Jansen (2020).\nlearning. See Section 5 for ablations and more\ndiscussion about fine-tuning.\nPrediction errors We examine prediction errors\nin identifying task type and object type. Key\nsources of model errors include annotation issues\nand ambiguity in object types. Table 4 shows the\nobject types that have the least prediction accu-\nracy, along with the object categories the model is\nconfused about. Annotations can fail to correctly\nidentify the target object - identifying a butter knife\nas a knife or a pepper shaker as salt shaker. Am-\nbiguity can also arise from identifying an object\nwith different names, depending on the context.\nFor instance, depending on the scene layout, the\nargument for a look at object in light task can be a\nfloor lamp or a desk lamp. Unless the type of lamp\nis identified precisely in the instruction, it is not\npossible to correctly predict the type of lamp. Cor-\nrectly identifying these objects requires feedback\nfrom interaction with the environment.\nThe experiments so far evaluate the ability of a\nlanguage model to retrieve ground truth subgoal\nsequences. Next we examine embodied agents that\nmake use of these predictions and collect more su-\npervision in order to improve subgoal predictions.\n4.3 Agent policy and incorporating\nenvironment feedback\nWe now use subgoal predictions to construct an\nagent policy that acts in a simulated environment\nto complete tasks. The agent state representation\nand pre-trained low-level subgoal policy are bor-\nrowed from the HLSM model proposed in Blukis\net al. (2022). HLSM represents the agent state\nas a spatial persistent voxel representation of the\nroom which models the location and category of ob-\njects present. The representation is constructed us-\ning modules that estimate segmentation and depth\nmaps and other visual reasoning components and is\nupdated as the agent gathers new observations. We\nObj category Confusion categories\nwateringcan pencil, kettle\nglassbottle vase\ncart shelf, sidetable, microwave, cart, fridge\nbutterknife knife, butterknife\nfloorlamp desklamp, floorlamp\nvase pencil, vase, bowl, winebottle, pot\nladle spoon, ladle\npot pot, pan\nsoapbottle soapbottle, winebottle\nTable 4: Object categories the model makes most errors on\nand the top object categories it confuses with.\nuse pre-trained models made available by the au-\nthors5 for state estimation and the low-level policy\nin our experiments.\nWe combine subgoal predictions with the pre-\ntrained HLSM low-level policy and evaluate the\noverall agent policy on the ALFRED task in Ta-\nble 5. Unlike the results reported in Section 4.2\nwhich were based on the static dataset, these re-\nsults are based on subgoals executed against the\nAI2-Thor simulator. In addition to task success\nrate, we also report the percentage of goal condi-\ntions satisfied, which rewards the model for partial\ntask completions.\nWe compare against the following baselines\non the ALFRED task. Seq2seq (Shridhar et al.,\n2020) is a simple sequence-to-sequence baseline\ntrained to map text instructions to low-level ac-\ntions. MOCA (Singh et al., 2020) improves on\nSeq2seq using subgoal supervision and pre-trained\nvisual reasoning components. Recent work such\nas HLSM (Blukis et al., 2022) and FiLM (Min\net al., 2021b) build and use spatial semantic state\nrepresentations and achieve stronger performance\non the task. Note that, unlike these prior methods\n(MOCA, HLSM, FiLM) that rely on full subgoal\nsupervision (20k instances), our approach is based\non a small amount of subgoal supervision and addi-\ntional supervision collected using active interaction\nwith the environment. In addition, our approach\ndoes not require task-level expert trajectories and\nonly assumes that a subgoal execution policy is\nprovided.\nUsing the top language model prediction as is\nwithout using any information from the environ-\nment leads to 20% success rate. Next, we collect\nplan execution feedback for 1000 text instructions\nto train the ranking model described in Section 3.2.\nRe-ranking language model predictions using the\ntrained ranking model improves the performance\n5https://hlsm-alfred.github.io\n5499\nModel\nSuccess rate\nTask Goal-Cond\nSeq2seq (Shridhar et al., 2020) 3.7 10.0\nMOCA (Singh et al., 2020) 19.2 28.5\nFiLM (Min et al., 2021b) 24.6 37.2\nHLSM (Blukis et al., 2022) 29.6 38.8\nHLSM\nlow-level\npolicy\nPredicted subgoals 19.8 31.4\nRe-ranked subgoals 23.9 35.0\nOracle subgoals 37.2 48.2\nTable 5: Task completion and goal condition success rates\nof models on the ALFRED validation seen split (results are\nbased on task executions in the AI2-Thor simulator). The\nperformance of our subgoal predictions combined with the\nHLSM low-level policy are shown at the bottom. We show\nthe performance before and after re-ranking language model\npredictions based on agent state. Oracle subgoals shows the\nperformance upper bound.\nto 24%, which shows the importance of incorpo-\nrating feedback from environment interaction. In\ncomparison, the HLSM model with full subgoal su-\npervision has success rate 30%. Although our pre-\ndictions fall short of HLSM, they are competitive\nwith the other baselines with subgoal supervision.\nThe performance upper bound estimated using or-\nacle subgoals is 37%, which shows the room for\nimprovement over our predictions. These results\nshow that accurate subgoal inference coupled with\npre-trained low-level components leads to agents\nthat perform well in embodied environments.\nFigure 1 shows an example where the ranking\nmodel uses environment information to identify\nbetter plans. In this example, the instruction am-\nbiguously specifies the receptacle as ‘white table\nwith shelving’. The language model’s top two pre-\ndictions for the target receptacle are ‘side table’\nand ‘shelf’, neither of which are present in the envi-\nronment. The agent state captures this information\nand helps identify ‘dining table’ as the correct re-\nceptacle.\n5 Ablations\nWe perform a series of ablations to identify the\nrobustness of model predictions. We compare the\nperformance of in-context learning to a sequence-\nto-sequence model fine-tuned to translate instruc-\ntions to subgoal sequences. In addition, we observe\nthe effect of varying the number of training exam-\nples and choice of training examples.\nNumber of training examples Figure 3 shows\nmodel recall for varying number of training ex-\namples. For zero training examples, the prompt\nconsists of just the query instruction and the model\n11 22 33 440\n20\n40\n60\n80\nNumber of training examples\nRecall\nGPT-J in-context (top-1)\nT5-large finetune (top-1)\nGPT-J in-context (top-10)\nT5-large finetune (top-10)\nFigure 3: Comparison between GPT-J with in-context learning\nand a fine-tuned T5-large model for varying number of training\nexamples. See text for details.\ndecodes subgoals. Beyond 44 examples (4 exam-\nples per task type), the model prompt no longer\nfits the sequence length restriction (1024 tokens) of\nGPT models. A steady increase in performance can\nbe initially observed when increasing the number\nof training examples and the performance saturates\ntowards the end. In-context learning further has\nthe limitation of not being able to accommodate\na larger number of training examples due to the\nlength restriction. It would be interesting to ex-\nplore how to make effective use of large number of\ntraining examples in future work.\nChoice of training examples We also estimate\nperformance variance by varying the random seed\nfor choosing examples randomly from the training\nset and compute standard deviation based on five\nrandom seeds for each setting. The plot shows that\ntop-1 predictions from in-context learning have\nlower variance compared to fine-tuning.\nComparison with fine-tuning In order to under-\nstand how well the in-context learning approach\ncompares to fine-tuned models, we fine-tune a T5-\nlarge model (Raffel et al., 2019) with 770M param-\neters on varying amounts of training data (this was\nthe largest model we could fine-tune on our com-\npute infrastructure). Note that this is not a head-to-\nhead comparison between in-context learning and\nfine-tuning due to the difference in model size. Fur-\nthermore, there are other fine-tuning mechanisms\nsuch as prompt tuning and head tuning (Min et al.,\n2021a) which are not considered here. However,\nthe result suggests that in-context learning with\nlarge pre-trained models can be favorable when\ncomputational constraints do not allow full fine-\ntuning of large models.\nThese ablations show that the in-context learn-\ning ability of large language models leads to pre-\n5500\ndictions that are accurate, robust and stable in the\npresence of a small amount of training data.\n6 Conclusion\nThis work explores the use of pre-trained lan-\nguage models for planning in real-world tasks. We\nshowed that language models have strong capabil-\nity to reason about subgoal sequences given a small\nnumber of training examples. We further demon-\nstrated some simple mechanisms to incorporate\nfeedback from interaction with the environment\nand show that this leads to more usable predictions.\nFinally, we show that combining subgoal predic-\ntions with a pre-trained low-level policy yields a\nstrong baseline for embodied agent learning.\nOur ablations demonstrate that in-context learn-\ning with a small amount of subgoal demonstrations\nhas robust generalization properties. However, we\nalso point out that in-context learning has the limi-\ntation of not being able to incorporate a large num-\nber of training examples due to the fixed context\nlength restriction. It would further be beneficial to\nperform end-to-end learning with language model\nbased subgoal prediction and a low-level policy,\nwhich would be interesting to explore in future\nwork.\nReferences\nValts Blukis, Chris Paxton, Dieter Fox, Animesh Garg,\nand Yoav Artzi. 2022. A persistent spatial semantic\nrepresentation for high-level natural language instruc-\ntion execution. In Conference on Robot Learning ,\npages 706–717. PMLR.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nS.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and\nRegina Barzilay. 2009. Reinforcement learning for\nmapping instructions to actions. In Proceedings of\nthe Joint Conference of the 47th Annual Meeting\nof the ACL and the 4th International Joint Confer-\nence on Natural Language Processing of the AFNLP,\npages 82–90, Suntec, Singapore. Association for\nComputational Linguistics.\nSRK Branavan, David Silver, and Regina Barzilay. 2012.\nLearning to win by reading manuals in a monte-carlo\nframework. Journal of Artificial Intelligence Re-\nsearch, 43:661–704.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901.\nHaonan Chen, Hao Tan, Alan Kuntz, Mohit Bansal,\nand Ron Alterovitz. 2020. Enabling robots to un-\nderstand incomplete natural language instructions\nusing commonsense reasoning. In 2020 IEEE In-\nternational Conference on Robotics and Automation\n(ICRA), pages 1963–1969. IEEE.\nRodolfo Corona, Daniel Fried, Coline Devin, Dan Klein,\nand Trevor Darrell. 2021. Modular networks for com-\npositional instruction following. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1033–1040,\nOnline. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nCatherine Finegan-Dollak, Jonathan K. Kummerfeld,\nLi Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui\nZhang, and Dragomir Radev. 2018. Improving text-\nto-SQL evaluation methodology. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 351–360, Melbourne, Australia. Association\nfor Computational Linguistics.\nDaniel Fried, Jacob Andreas, and Dan Klein. 2018. Uni-\nfied pragmatic models for generating and following\ninstructions. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 1951–1963,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nDaniel Furrer, Marc van Zee, Nathan Scales, and\nNathanael Schärli. 2020. Compositional generaliza-\ntion in semantic parsing: Pre-training vs. specialized\narchitectures. arXiv preprint arXiv:2007.08970.\nMatthew Hausknecht, Prithviraj Ammanabrolu, Marc-\nAlexandre Côté, and Xingdi Yuan. 2020. Interactive\nfiction games: A colossal adventure. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 34, pages 7903–7910.\n5501\nJonathan Herzig, Peter Shaw, Ming-Wei Chang, Kelvin\nGuu, Panupong Pasupat, and Yuan Zhang. 2021. Un-\nlocking compositional generalization in pre-trained\nmodels using intermediate representations. arXiv\npreprint arXiv:2104.07478.\nPeter Jansen. 2020. Visually-grounded planning with-\nout vision: Language models infer detailed plans\nfrom high-level instructions. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 4412–4417, Online. Association for Computa-\ntional Linguistics.\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-\nderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon,\nYuke Zhu, Abhinav Gupta, and Ali Farhadi. 2017.\nAi2-thor: An interactive 3d environment for visual ai.\narXiv preprint arXiv:1712.05474.\nBrenden Lake and Marco Baroni. 2018. Generalization\nwithout systematicity: On the compositional skills\nof sequence-to-sequence recurrent networks. In In-\nternational conference on machine learning, pages\n2873–2882. PMLR.\nBelinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021.\nImplicit representations of meaning in neural lan-\nguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1813–1827, Online. Association for\nComputational Linguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nJiwei Li and Dan Jurafsky. 2016. Mutual information\nand diverse decoding improve neural machine trans-\nlation. arXiv preprint arXiv:1601.00372.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nJelena Luketina, Nantas Nardelli, Gregory Farquhar,\nJakob Foerster, Jacob Andreas, Edward Grefenstette,\nShimon Whiteson, and Tim Rocktäschel. 2019. A\nsurvey of reinforcement learning informed by natural\nlanguage. arXiv preprint arXiv:1906.03926.\nHongyuan Mei, Mohit Bansal, and Matthew R Walter.\n2016. Listen, attend, and walk: Neural mapping\nof navigational instructions to action sequences. In\nThirtieth AAAI Conference on Artificial Intelligence.\nVincent Micheli and Francois Fleuret. 2021. Language\nmodels are few-shot butlers. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 9312–9318, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2021a. Noisy channel language\nmodel prompting for few-shot text classification.\narXiv preprint arXiv:2108.04106.\nSo Yeon Min, Devendra Singh Chaplot, Pradeep Raviku-\nmar, Yonatan Bisk, and Ruslan Salakhutdinov. 2021b.\nFilm: Following instructions in language with modu-\nlar methods. arXiv preprint arXiv:2110.07342.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nTimo Schick and Hinrich Schütze. 2021. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nRichard Shin, Christopher Lin, Sam Thomson, Charles\nChen, Subhro Roy, Emmanouil Antonios Platanios,\nAdam Pauls, Dan Klein, Jason Eisner, and Benjamin\nVan Durme. 2021. Constrained language models\nyield few-shot semantic parsers. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 7699–7715, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nMohit Shridhar, Jesse Thomason, Daniel Gordon,\nYonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke\nZettlemoyer, and Dieter Fox. 2020. Alfred: A bench-\nmark for interpreting grounded instructions for ev-\neryday tasks. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition,\npages 10740–10749.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté,\nYonatan Bisk, Adam Trischler, and Matthew\nHausknecht. 2021. ALFWorld: Aligning Text and\nEmbodied Environments for Interactive Learning.\nIn Proceedings of the International Conference on\nLearning Representations (ICLR).\nKunal Pratap Singh, Suvaansh Bhambri, Byeonghwi\nKim, Roozbeh Mottaghi, and Jonghyun Choi. 2020.\nMoca: A modular object-centric approach for in-\nteractive instruction following. arXiv preprint\narXiv:2012.03208.\n5502\nAlessandro Suglia, Qiaozi Gao, Jesse Thomason,\nGovind Thattai, and Gaurav Sukhatme. 2021. Em-\nbodied bert: A transformer model for embodied,\nlanguage-guided visual task completion. arXiv\npreprint arXiv:2108.04927.\nAlane Suhr, Claudia Yan, Jack Schluger, Stanley Yu,\nHadi Khader, Marwa Mouallem, Iris Zhang, and\nYoav Artzi. 2019. Executing instructions in situ-\nated collaborative interactions. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2119–2130, Hong Kong,\nChina. Association for Computational Linguistics.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 Billion Parameter Autoregressive\nLanguage Model. https://github.com/\nkingoflolz/mesh-transformer-jax.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and\nKarthik Narasimhan. 2020. Keep CALM and ex-\nplore: Language models for action generation in text-\nbased games. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 8736–8754, Online. Association\nfor Computational Linguistics.\nYichi Zhang and Joyce Chai. 2021. Hierarchical task\nlearning from language instructions with unified\ntransformers and self-monitoring. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 4202–4213, Online. Association\nfor Computational Linguistics.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\nVictor Zhong, Tim Rocktäschel, and Edward Grefen-\nstette. 2020. Rtfm: Generalising to new environment\ndynamics via reading. In International Conference\non Learning Representations.\n5503\nA Mutual Information based scoring\nMutual Information between random variables\nX,Y is defined as in Equation (2). We consider a\nweighted Mutual Information metric as defined as\nin Equation (3) similar to Li and Jurafsky (2016)\nand introduce the hyperparameter λ. Identifying Y\nthat maximizes the weighted Mutual Information\nis equivalent to maximizing the expression in Equa-\ntion (4). We use this metric to rank hypotheses\ngenerated by the language model.\nMI(X,Y ) =log p(x,y)\np(x)p(y) (2)\nwMI(X,Y ) =log p(x,y)\np(x)p(y)λ (3)\nargmax\ny\nwMI(X,Y )\n= argmax\ny\nlog p(x,y)\np(x)p(y)λ\n= argmax\ny\nlog\n(p(x,y)\np(x)\n)1−λ(p(x,y)\np(y)\n)λ 1\np(x)λ\n= argmax\ny\n(1 −λ)log p(y|x) +λlog p(x|y)\n−λlog p(x)\n= argmax\ny\n(1 −λ)log p(y|x) +λlog p(x|y)\n(4)\nB Subgoal representation\nTable 6 shows the subgoal representation we use in\nthis work.\nSubgoal Representation\n(Pickup, X) pick up X\n(Put, X) put in X\n(Heat, X) heat in X\n(Cool, X) cool in X\n(Clean, X) clean in X\n(Slice, X) slice X\n(ToggleOn, X) turn on X\nTable 6: Subgoals and corresponding text represen-\ntation. X represents an object argument.\nC Ranking model: Architecture\nState embedding HLSM represents the agent\nstate as a semantic voxel representation s ∈\n[0,1]X×Y×Z×C where the value s(x,y,z,c ) rep-\nresents if there is an object of type c at position\n(x,y,z ) of the room layout. We pool across the\nspatial dimensions of the representation and project\nit using a linear mapping to obtain fstate(s). We\nuse this encoding as the state embedding.\nInstruction and subgoal sequence encoding\nThe instruction τ and a candidate subgoal se-\nquence g are jointly processed using a BERT\nencoder and the CLS representation is used as\na representation vector BERTCLS(τ,g). The\nranking model is represented as f(g,τ,s ; θ) =\nθTconcat(fstate(s),BERTCLS(τ,g)) where θ is a\nparameter vector.\nD Ranking Model: Training and\nInference\nFormally, the learning problem is a MDP\n(S,G,L,R,T), where st ∈S is the agent state,\ng(t) ∈ Gis a subgoal, τ ∈ Lis a text instruc-\ntion, R(τ,st) is a reward function that provides\nsuccess/failure feedback for completing a given\ninstruction, T : (st,g(t)) →st+1 is a state tran-\nsition function where st+1 is computed by a low-\nlevel policy πL pre-trained to execute a given sub-\ngoal g. Our goal is to train a high-level policy\nπH(g(t)|τ,st,g(<t)) where st is the agent state and\ng(<t) is the sequence of subgoals completed so far\nat high-level time-step t.\nAlgorithm 1 describes how we collect training\ndata to train the ranking model. Algorithm 2 shows\nhow the ranking model is used during inference.\n5504\nAlgorithm 1 Training\nGiven: epochs = 100, Dins (set of instructions)\nCollect training data\nD←{} (Initialize training set)\nfor τ in Dins do\nGenerate plans and re-rank using mutual information metric g1,...,g k ∼pLM(·|τ)\nfor i= 1...k do\nInitialize agent state s\nS ←{s}(Record agent states)\nfor j = 1... |gi|do\ns←T (s,g(j)\ni ) (Execute g(j)\ni using πL)\nS ←S∪{s}\nend for\nif R(τ,s) >0 then (Task succeeded)\nD←D∪{ (gi,τ,s )|s∈S}\nbreak\nend if\nend for\nend for\nTrain model\nfor i= 1... epochs do\nloss ←0\nfor (g,τ,s ) ∈D do\nGenerate plans g1,..,g k ∼pLM(·|τ)\nloss ←loss −log exp f(g,τ,s;θ)∑k\ni=1 exp f(gi,τ,s;θ)\nend for\nθ←Optimizer Update(θ,∇θf)\nend for\nreturn f\nAlgorithm 2 Inference\nGiven: Instruction τ, ithresh = 10\nGenerate plans g1,...,g k ∼pLM(·|τ)\nG←{g1,...,g k}\nInitialize agent state s\ni←0 (subgoal index)\ng←argmaxg∈Gf(g,τ,s ; θ)\nwhile |G|̸= 0and g(i) ̸= <stop> and i<i thresh do\ns←T (s,g(i)) (Execute g(i) using πL)\nG←{h|h∈Gand h(i) = g(i)}(Retain plans consistent with subgoals completed so far)\ng←argmaxg∈Gf(g,τ,s ; θ)\ni←i+ 1\nend while\nreturn g,s\n5505\nE Analysis of model errors\nFigure 4 shows the confusion matrix for object type prediction. Predictions are from top-1 subgoal\nsequences predicted by GPT-J.\ndishsponge\nnewspaper\nremotecontrol\nwinebottle\ncd\ncreditcard\nhandtowel\nspatula\narmchair\nfork\ntoiletpaper\ncellphone\nsafe\nbaseballbat\nplunger\nstatue\nspraybottle\nsoapbar\nkeychain\nlettuce\ntoilet\ndrawer\nfridge\ntomato\nbread\napple\nfloorlamp\nmicrowave\nknife\nbook\ngarbagecan\npillow\nalarmclock\nwatch\ncoffeetable\npotato\ncoffeemachine\ncountertop\negg\nsinkbasin\ndesk\nbox\nlaptop\ncabinet\ndiningtable\npan\npencil\nstoveburner\ncandle\nplate\ntissuebox\nbathtubbasin\nshelf\nsofa\ntennisracket\nbed\ncloth\ntoiletpaperhanger\nmug\nbowl\nspoon\ncup\npen\nottoman\npot\nladle\nvase\nsidetable\ndesklamp\nbutterknife\ndresser\nsoapbottle\ncart\nwateringcan\nkettle\nglassbottle\nPredicted label\ndishsponge\nnewspaper\nremotecontrol\nwinebottle\ncd\ncreditcard\nhandtowel\nspatula\narmchair\nfork\ntoiletpaper\ncellphone\nsafe\nbaseballbat\nplunger\nstatue\nspraybottle\nsoapbar\nkeychain\nlettuce\ntoilet\ndrawer\nfridge\ntomato\nbread\napple\nfloorlamp\nmicrowave\nknife\nbook\ngarbagecan\npillow\nalarmclock\nwatch\ncoffeetable\npotato\ncoffeemachine\ncountertop\negg\nsinkbasin\ndesk\nbox\nlaptop\ncabinet\ndiningtable\npan\npencil\nstoveburner\ncandle\nplate\ntissuebox\nbathtubbasin\nshelf\nsofa\ntennisracket\nbed\ncloth\ntoiletpaperhanger\nmug\nbowl\nspoon\ncup\npen\nottoman\npot\nladle\nvase\nsidetable\ndesklamp\nbutterknife\ndresser\nsoapbottle\ncart\nwateringcan\nkettle\nglassbottle\nTrue label\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ndishsponge\nnewspaper\nremotecontrol\nwinebottle\ncd\ncreditcard\nhandtowel\nspatula\narmchair\nfork\ntoiletpaper\ncellphone\nsafe\nbaseballbat\nplunger\nstatue\nspraybottle\nsoapbar\nkeychain\nlettuce\ntoilet\ndrawer\nfridge\ntomato\nbread\napple\nfloorlamp\nmicrowave\nknife\nbook\ngarbagecan\npillow\nalarmclock\nwatch\ncoffeetable\npotato\ncoffeemachine\ncountertop\negg\nsinkbasin\ndesk\nbox\nlaptop\ncabinet\ndiningtable\npan\npencil\nstoveburner\ncandle\nplate\ntissuebox\nbathtubbasin\nshelf\nsofa\ntennisracket\nbed\ncloth\ntoiletpaperhanger\nmug\nbowl\nspoon\ncup\npen\nottoman\npot\nladle\nvase\nsidetable\ndesklamp\nbutterknife\ndresser\nsoapbottle\ncart\nwateringcan\nkettle\nglassbottle\nPredicted label\ndishsponge\nnewspaper\nremotecontrol\nwinebottle\ncd\ncreditcard\nhandtowel\nspatula\narmchair\nfork\ntoiletpaper\ncellphone\nsafe\nbaseballbat\nplunger\nstatue\nspraybottle\nsoapbar\nkeychain\nlettuce\ntoilet\ndrawer\nfridge\ntomato\nbread\napple\nfloorlamp\nmicrowave\nknife\nbook\ngarbagecan\npillow\nalarmclock\nwatch\ncoffeetable\npotato\ncoffeemachine\ncountertop\negg\nsinkbasin\ndesk\nbox\nlaptop\ncabinet\ndiningtable\npan\npencil\nstoveburner\ncandle\nplate\ntissuebox\nbathtubbasin\nshelf\nsofa\ntennisracket\nbed\ncloth\ntoiletpaperhanger\nmug\nbowl\nspoon\ncup\npen\nottoman\npot\nladle\nvase\nsidetable\ndesklamp\nbutterknife\ndresser\nsoapbottle\ncart\nwateringcan\nkettle\nglassbottle\nTrue label\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 4: Object type prediction confusion matrix.\n5506",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6001139879226685
    },
    {
      "name": "Computational linguistics",
      "score": 0.550487220287323
    },
    {
      "name": "Linguistics",
      "score": 0.48410260677337646
    },
    {
      "name": "Natural language processing",
      "score": 0.4220521152019501
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3831396698951721
    },
    {
      "name": "Cognitive science",
      "score": 0.33722227811813354
    },
    {
      "name": "Philosophy",
      "score": 0.22677862644195557
    },
    {
      "name": "Psychology",
      "score": 0.11483445763587952
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    }
  ]
}