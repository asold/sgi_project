{
  "title": "Codeguard: Utilizing Advanced Pattern Recognition in Language Models for Software Vulnerability Analysis",
  "url": "https://openalex.org/W4392240138",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2563211300",
      "name": "Rebet Jones",
      "affiliations": [
        "Capitol Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2164113775",
      "name": "Marwan Omar",
      "affiliations": [
        "Illinois Institute of Technology",
        "Capitol Technology University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3135473407",
    "https://openalex.org/W4297805866",
    "https://openalex.org/W4387196958",
    "https://openalex.org/W3166095789",
    "https://openalex.org/W3161071537",
    "https://openalex.org/W4312690534",
    "https://openalex.org/W2634106992",
    "https://openalex.org/W4312326656",
    "https://openalex.org/W3009129408",
    "https://openalex.org/W2559935471",
    "https://openalex.org/W4296870561",
    "https://openalex.org/W4321012155",
    "https://openalex.org/W4385192597",
    "https://openalex.org/W4293182797",
    "https://openalex.org/W4387676909",
    "https://openalex.org/W4327928482",
    "https://openalex.org/W4288076143",
    "https://openalex.org/W2962960733",
    "https://openalex.org/W4286252390",
    "https://openalex.org/W1992114977",
    "https://openalex.org/W2770225980",
    "https://openalex.org/W4281386687",
    "https://openalex.org/W2970862273",
    "https://openalex.org/W2976184969",
    "https://openalex.org/W3104849875"
  ],
  "abstract": "Abstract Enhancing software quality and security hinges on the effective identification of vulnerabilities in source code. This paper presents a novel approach that combines pattern recognition training with cloze-style examination techniques in a semi-supervised learning framework. Our methodology involves training a language model using the SARD and Devign datasets, which contain numerous examples of vulnerable code. During training, specific code sections are deliberately obscured, challenging the model to predict the hidden tokens. Through rigorous empirical testing, we demonstrate the effectiveness of our approach in accurately identifying code vulnerabilities. Our results highlight the significant advantages of employing pattern recognition training alongside cloze-style questioning, leading to improved accuracy in detecting vulnerabilities in source code.",
  "full_text": "CODEGUARD: UTILIZING ADVANCED \nPATTERN RECOGNITION IN LANGUAGE \nMODELS FOR SOFTWARE  \nVULNERABILITY ANALYSIS \nRebet JONES \nCapitol Technology University, Maryland, USA \nrjones@captechu.edu \nMarwan OMAR \nCapitol Technology University, Maryland, USA & \n Illinois Institute of Technology, Chicago, USA \nmomar3@iit.edu   \nABSTRACT \nEnhancing software quality and security hinges on the effective identification \nof vulnerabilities in source code. This paper presents a novel approach that \ncombines pattern recognition training with cloze-style examination techniques in a \nsemi-supervised learning framework. Our methodology involves training a language \nmodel using the SARD and Devign datasets, which contain numerous examples of \nvulnerable code. During training, specific code sections are deliberately obscured, \nchallenging the model to predict the hidden tokens. Through rigorous empirical \ntesting, we demonstrate the effectiveness of our approach in accurately identifying \ncode vulnerabilities. Our results highlight the significant advantages of employing \npattern recognition training alongside cloze-style questioning, leading to improved \naccuracy in detecting vulnerabilities in source code. \nKEYWORDS: language models, software vulnerabilities, vulnerability detection, \n  cloze-style questions, pattern-exploiting training, RoBERTa \n1. Introduction\nThe realm of digital platforms is\nincreasing\nly facing co mplex and malicious \ncyber attacks. These attacks often exploit \nsystem vulnerabilities,  which are gaps in \nthe system that can be manipulated by \ncyber adversaries for various benefits. \nA key driver of these cyber attacks is the \npresence of softwa re vulnerabilities. \nDespite significant efforts by academia and \nindustry to strengthen software security, the \npersistent rise in vulnerabilities, as \nhighlighted in the annual reports of the \nCommon Vulnerabilities and Exposures \n(CVE) database (Omar, 2022), remains a \nmajor concern. \nGiven the inevitability of these \nvulnerabilities, their prompt detection is \ncrucial. Static source code analysis provides \na means for early detection, employing \nmethods from code similarity assessment to \npattern-recognition techniques. While code \nsimilarity approaches can identify \nvulnerabilities resu lting from code \nreplication, they are pr one to considerable \nfalse negatives (Ayub et al., 2023; \nLi, Wang, Xin, Yang & Chen, 2020; Omar, \n2022; Omar & Sukthankar, 2023). \nLand Forces Academy Review \nVol. XXIX, No. 1(113), 2024\nDOI: 10.2478/raft-2024-0011\n© 2024 Rebet Jones et al. This work is licensed under the Creative Commons Attribution-Non Commercial-No Derivatives 3.0 License. \n108\nAddressing these challenges in \nvulnerability detection, academia has \nintroduced techniques such as fuzzing, \nsymbolic analysis, and rule-based testing. \nDespite their merits, these methods face \nlimitations like the need for manual \ndefinition of attack signatures and patterns, \nreducing their effectiveness in large \ncodebases. Moreover, traditional \nvulnerability detection methods suffer from \nhigh false positives, performance \nlimitations, and difficulties in classifying \nvulnerability types (Aluru, Mathew, Saha & \nMukherjee, 2020; Omar, 2023). \nRecent advancements have seen the \nincorporation of machine learning, \nespecially deep learning, into vulnerability \ndetection frameworks. These approaches \nreduce manual input and accelerate the \ndetection process. Advanced machine \nlearning models, including long-short-term \nmemories (LSTMs) and transformers, \nclassify API sequences from program \nexecution into benign or malicious \ncategories and predict th e type of exploit. \nHowever, their high computational \ndemands limit their practicality (Omar, \nChoi, Nyang & Mohaisen, 2022).  \nThis study aims to apply “pattern-\nexploiting training” (PET) and “iterative \npattern-exploiting training” (iPET) \nmethodologies, utilizing cloze-style \nquestions, to develop a comprehensive \nlinguistic model for detecting software \nvulnerabilities. In this approach, cloze \nquestions, which involve filling in blanks in \na text (Radford et al., 2019), are based on \ncode snippets with th e blanks representing \nexisting vulnerabilities. \nOur methodology is predicated on the \nidea that a detailed language model, trained \non a large dataset of code-based cloze \nquestions, learns to recognize both \nvulnerable and safe code patterns. This \ntraining enables the model to identify code \nconfigurations indicativ e of vulnerabilities. \nThe trained model can then detect potential \nvulnerabilities in new code by filling in the \nblanks in the cloze questions. For example, \ncode with a potential buffer overflow \nvulnerability is analyzed using the PET \napproach by identif ying code patterns \nassociated with buffer overflow risks and \ncreating cloze questions for training the \nlinguistic model. \nSubsequently, the cloze-trained \nlinguistic model becomes proficient at \nanalyzing new code segments, identifying \npatterns that match known vulnerability \nsignatures. This method provides automated \ndetection of vulnera bility patterns, \neliminating the need for manual expert \nanalysis. \nIn this research, we introduce \n“CodeGuard”, a RoBERTa-based \nvulnerability detection system for C and C++ \nsource code. Our key contributions are: \n1) The development of CodeGuard,\nan innovative system that leverages pattern-\nexploiting training and cloze methodology \nfor detecting software vulnerabilities, \nutilizing the capabilities of an extensive \nlinguistic model. \n2) Demonstrating CodeGuard’s \neffectiveness in identifying code \nvulnerabilities across multiple programming \nlanguages, including C/C++ and Java, using \nbenchmark datasets and the RoBERTa-\nbased linguistic model. \n3) Presenting comparative studies\nthat show CodeGuard’s enhanced \nperformance in detecting software \nvulnerabilities compared to two other \ncontemporary benchmark techniques.  \n2. Related Work\nRecent years have seen a burgeoning\ninterest in detecting vulnerabilities in \nsource code, a critical area of research for \nsoftware security. Various approaches have \nbeen explored, with many studies \nleveraging machine learning techniques. \nStatic analysis methods, which extract key \nfeatures from code for input into machine \nlearning models, have been a focal point of \nsome studies (Kim et al., 2022; Li et al., \n109\n2016, Omar et al., 2023). In  contrast, others \nhave utilized dynamic analysis, where the \ncode is executed, and its behavior \nmonitored to identify vulnerabilities \n(Alharbi, Hijji & Aljaedi, 2021; Salimi & \nKharrazi, 2022). \nThere has been a growing trend \ntowards employing deep learning models in \nthis domain. Some researchers have used \nrecurrent neural networks (RNNs) to \nprocess the code, either in its original form \nor converted into an abstract syntax tree \n(AST) (Yamaguchi, Golde, Arp & Rieck, \n2014; Zhou & Verma, 2022). Additionally, \nthere’s increasing use of transformers, \nrenowned for their success in natural \nlanguage processing (Kim, Woo, Lee & Oh, \n2017; Rabheru, Hanif & Maffeis, 2021).  \nDeep learning architectures such as \nConvolutional Neural Networks (CNNs) \nand RNNs have been extensively studied \nfor vulnerability detection (Alharbi, Hijji & \nAljaedi, 2021; Kim, Woo, Lee & Oh, 2017; \nKim et al., 2022; Li et al., 2016; Rabheru, \nHanif & Maffeis, 2021; Salimi & Kharrazi, \n2022; Yamaguchi, Golde, Arp & Rieck, \n2014; Zhou & Verma, 2022). These models \ntypically require st ructured data for \nidentifying features linked to \nvulnerabilities. This requirement has led to \nthe development of various techniques like \nlexed C/C++ code representation (Kim et \nal., 2022), code gadgets  (Rabheru, Hanif & \nMaffeis, 2021), and code-property graphs \n(Zhou et al, 2019). Gra ph neural networks \nhave also been applied in this field, with \nmodels like Devign offering comprehensive \nrepresentations of pr ogram elements (Zhou \net al, 2019). \nPioneering work by Russell et al. \n(2018) demonstrated deep learning’s \ncapability in detecting vulnerabilities \ndirectly from raw source code, using a \ncombination of CNN and RNN to inform a \nRandom Forest classifier. This approach \nachieved a notable AUC score when tested \non real-world datasets. \nFollowing this, Vuldeepecker (Zou et \nal., 2019) introduced a method for \nmulti-class vulnerability classification, \npinpointing the precise location of \nvulnerabilities within the source code. \nGraph-based approaches have also \nbeen explored, with Devign (Zhou et al, \n2019) and DeepWukong (Cheng et al, \n2021) utilizing Grap h Neural Network \nmodels. Studies have extended beyond \ntraditional programming languages, with \nDeepTective (Rabheru, Hanif & Maffeis, \n2021) focusing on PHP and others \nexamining vulnerabilities in HTML5 \napplications (Yan et all, 2018). The quality \nof datasets for deep learning-based \ndetection has also been a priority, as seen in \nREVEAL (Chakrabor ty, Krishna, Ding & \nRay, 2021) and D2A. \nVulBERTa, proposed by Hanif & \nMaffeis (2022), represents a significant \nadvancement, offering deep \nrepresentational mode ling of C/C++ code \nbut falls short in identifying novel zero-day \nvulnerabilities in open-source projects. \nOur work aligns with the current \ntrajectory of applying deep learning for \nsource code vulnerability detection. \nWe distinctively use pattern-exploiting \ntraining combined with cloze queries, \nempowering a compact student model with \ninsights from a larger language model. This \napproach represents a departure from previous \nmethodologies that primarily focused on \nRNNs, transformers, or deep learning \nknowledge distillation for various tasks. \n3. Defense Framework\nIn the evolving landscape of software\nsecurity, the development of a robust \ndefense framework is crucial for mitigating \nvulnerabilities. Our proposed framework, \nnamed CodeGuard, integrates advanced \nmachine learning techniques with a focus \non deep learning models, offering a \ncomprehensive approach to detecting and \nneutralizing potential threats in source code. \n110\n3.1. Methodology \nCodeGuard employs a multi-layered \nstrategy, combining static and dynamic \nanalysis methods with  advanced pattern \nrecognition algorithms. The core \ncomponents of our methodology are \noutlined as follows: \n• Static Analysis Module : Utilizes\nmachine learning models to scan source \ncode, identifying potential vulnerabilities \nby analyzing code structure and syntax. \nThis module leverages Natural Language \nProcessing (NLP) techniques for efficient \npattern recognition. \n• Dynamic Analysis Engine : \nExecutes code in a controlled environment \nto monitor runtime behavior. This engine \naids in detecting vulnerabilities that \nmanifest during execution, such as memory \nleaks and buffer overflows. \n• Deep Learning Core : At the heart\nof CodeGuard is a deep learning model \ntrained on extensive datasets comprising \nvarious programming languages. The model \nemploys a combination of Convolutional \nNeural Networks (CNNs) and Recurrent \nNeural Networks (RNNs) to learn from \ncomplex code patterns and predict \nvulnerabilities. \n• Threat Mitigation Interface :\nProvides actionable insights and \nrecommendations for mitigating identified \nthreats. It integrates with development tools \nto offer realtime feedback and suggestions \nfor code improvement. \n3.2. Key Features \nCodeGuard distinguishes itself with \nseveral innovative features: \n1) Real-Time Vulnerability Detection:\nOffers instant analysis and feedback during \nthe development process, reducing the time to \nidentify and fix vulnerabilities. \n2) Cross-Language Support: Capable\nof analyzing multiple programming \nlanguages, making it a versatile tool for \ndiverse software projects. \n3) Advanced Learning Algorithm :\nContinuously evolves through machine \nlearning, adapting to new types of \nvulnerabilities and attack vectors. \n4) User-Friendly Interface: Designed\nfor ease of use, allowing developers of \nvarying skill levels to effectively utilize the \nframework. \nThe integration of these components \nand features positions CodeGuard as a \ncomprehensive solution for enhancing \nsoftware security in  a rapidly changing \ntechnological landscape. \nFigure no. 1: Schematic representation of the \nCodeGuard defense framework, illustrating its \nprimary components: Static Analysis Module, \nDynamic Analysis Engine, Deep Learning \nCore, and Threat Mitigation Interface \n(Source: Authors) \n3.3. Pattern-Exploiting Training in \nCodeGuard \nConsider a training dataset D \nconsisting of N pairs of input-output, where \neach input x\ni represents a segment of source \ncode, and the corresponding output yi \nindicates the presence  or absence of a \n111\nvulnerab\nwithin \napply P\ntransfor\nstyle sta\nTh\nidentify\nEach s\nwith an \ni \n∈ I j, t\nThis re s\nX′, wh e\n  c l\nFo\nsuch as \ninput x\n“strcpy”\n(xi, pj) w\na cloze \nA\nCodeGu\naltered \noutputs \ninputs \na\nprocess\n3.\nD\nD\nS\nRE\nPerform\nC\nM\nM\nbility in tha\nthe CodeG\nPattern-Expl\nrming thes e\natements to \nhis transfo r\nying key p a\nignificant \n index set I\nthe input x\nsults in a t\nere each i\nloze (xi, pj) \nor example,\n“unsafe fu\nxi contain\n”, the trans\nwould mask\nquestion. \nA machine \nuard is t h\ninputs X′ \ny. Predicti\nalso follow \n. \n4. Results a\nDataset \nDevign \nSARD \nEVEAL \nD2A \nmance Metri\nMethod \nCodeGuard \nMethod A \nMethod B \nat code segm\nGuard fra m\nloiting Train\ne code inp u\ndetect vuln\nrmation pr o\natterns P i\npattern pj \nIj, ensuring \nxi includes t\ntransformed\ninput xi is \nfor some pj\n, if we con s\nunction call”\nning the \nformed in p\nk “strcpy”, \nlearning m\nhen traine d\nand their \nons for ne w\nthis cloze t\nand Analys\nA\nics: \nment. The g\nmework is \nning (PET) \nuts into clo\nnerabilities. \nocess invol v\nin the inp u\nis associ a\nthat for ev e\nthe pattern \nd set of in p\nmodified \nj ∈ P. \nsider a patt e\n” and have \nfunction c\nput  cl o\nturning it i n\nmodel f wit h\nd using th e\ncorrespondi\nw, unanaly z\ntransformati\nis \nPer\nccuracy \n92% \n89% \n85% \n88% \nAccuracy \n92% \n85% \n80% \noal \nto \nby \nze-\nves \nuts. \nated \nery  \npj. \nputs \nto \nern \nan \ncall \noze \nnto \nhin \nese \ning \nzed \nion \nan\nX′ \nlos\ndis\nac\nmo\nac\nAf\nide\nus\nlev\npre\nwi\ny, \nseg\nen\ncap\nrformance m\nPrec\n9\n8\n8\n8\nCo\nP\nMathem\nnd prediction\n= {cloze (xi,p\nf = train\nynew= f(\nDuring \nss functio n\nscrepancy \ntual labels: \nMinimi\nodel’s p r\ncurately r e\nfter traini n\nentifying v\ning cloze-st\nWithin \nveraging a n\ne-trained m\nith the tran s\naiming to e\ngments i n\nnhancing t\npability.\nmetrics of C\ncision \n0% \n7% \n4% \n6% \nomparative \nPrecision \n90% \n83% \n78% \nmatically, t\nn process ca\npj) for i ∈\t{1\nn(X′,y)  \ncloze(xnew,p\ntraining, \nn is use d\nbetween t\nzing this \nredictions, \neflect the \nng, the m\nvulnerabiliti\ntyle queries\nthe Code\nn architect u\nmodel can b\nsformed inp\neffectively \nn cloze-st y\nthe vuln e\nCodeGuard \nRecall \n91% \n88% \n83% \n87% \nanalysis on\nRecal\n91%\n84%\n79%\nthe transf o\nan be descri\n1, 2,..., N}, pj\np))  \nthe cross\nd to mea s\nthe predic t\nloss ref\nin\nensuring\ntrue prob\nmodel is a\nes in sou r\ns. \nGuard fra m\nure like B\ne further o p\nputs X′ and\npredict the \nyle queri e\nerability d\nTab\non various \nF1 \n90\n87\n83\n86\nTab\nn the Devign\nll F1 \n% 9 0\n% 8 3\n% 7 8\normation \nibed as: \nj ∈ P}  \n(1) \n-entropy\nsure the \nted and \n(2) \nnes the \ng they \nabilities. \nadept at \nrce code \nmework, \nBERT, a \nptimized \nd outputs \nmasked \nes, thus \ndetection \nble no. 1 \ndatasets \nScore \n0.5% \n7.5% \n3.5% \n6.5% \nble no. 2 \nn dataset \nScore \n0.5% \n3.5% \n8.5% \n112\nComparative Analysis: \nFigure no. 2: Bar chart representing the F1 Scores of CodeGuard on different datasets \n(Source: Authors) \nFigure no. 3: Bar chart comparing the F1 Scores of CodeGuard  \nand other methods on the Devign dataset \n(Source: Authors) \n113\n3.5. Analysis of Results \n− Performance across Datasets :\nThe performance metrics of CodeGuard, as \nshown in table no. 1 and figure no. 2, \nindicate a high level of efficacy in \nvulnerability detection across various \ndatasets. The framework achieved the \nhighest F1 Score of 90.5% on the Devign \ndataset, underscoring its effectiveness in \nhandling practical, real-world code \nfunctions. The SARD dataset, with its \ndiverse range of vulnerability types, also \nsaw a strong performance from CodeGuard, \nwith an F1 Score of 87.5%. The slightly \nlower scores on REVEAL and D2A \ndatasets, at 83.5% and 86.5% respectively, \nsuggest that CodeGuard may face \nchallenges with datasets that have a more \nskewed vulnerability distribution or those \nemploying differential an alysis techniques. \nOverall, the consistent performance across \nvarious datasets highlights CodeGuard’s \nrobustness and adaptability in different \ncontexts of software vulnerability detection. \n− Comparative Analysis : In\ncomparative analysis on the Devign dataset, \nas illustrated in Table no. 2 and Figure no. 3, \nCodeGuard outperforms the other two \nmethods, Method A and Method B, by a \nsignificant margin. With an F1 Score of \n90.5% compared to 83.5% for Method A \nand 78.5% for Method B, CodeGuard \ndemonstrates its superior capability in \naccurately identifying and predicting \nsoftware vulnerabilities. This superiority \ncan be attributed to its integration of \npattern-exploiting trai ning and cloze-style \nqueries, which enhan ces its ability to \ndiscern complex vulnerability patterns \nwithin source code. The lower scores of the \nother methods suggest a possible limitation \nin their approach to pattern recognition or \nadaptability to varied code structures. \n− Overall Implications : The results\nfrom our experiments provide compelling \nevidence of CodeGuard’s potential as a \nhighly effective tool for software \nvulnerability det ection. Its strong \nperformance across different datasets and \nits comparative superiority indicate its \nutility in both academic research and \npractical applications in software security. \nFuture work could explore further \noptimizations in the model’s architecture \nand training process, especially to enhance \nits performance on da tasets with unique \ncharacteristics like REVEAL and D2A. \nAdditionally, expanding the framework’s \nadaptability to more programming \nlanguages and integrating more advanced \nmachine learning techniques could be areas \nfor continued development (Omar, et al., \n2023 (VulDefend)). \n4. Conclusion and Future Research\nDirections \nIn conclusion, the research presented \nin this paper has demonstrated the \neffectiveness of the CodeGuard framework \nin the domain of software vulnerability \ndetection. Our expe riments, utilizing \ndiverse and benchmark datasets such as \nDevign, SARD, REVEAL, and D2A, have \nprovided a comprehensive evaluation of \nCodeGuard’s capabilities. The framework’s \nperformance, particularly in terms of \naccuracy, precision, recall, and F1 score, \nestablishe\ns it as a promising tool in the \ncritical task of identifying vulnerabilities in \nsoftware. \nA key strength of CodeGuard lies in \nits innovative integr ation of pattern-\nexploiting training (P ET) and cloze-style \n114\nqueries. This approach has proven effective \nin enhancing the model’s ability to discern \ncomplex patterns within source code, a vital \naspect of vulnerabil ity detection. The high \nF1 scores achieved across multiple datasets, \nespecially on Devign , attest to the \nframework’s robustness and effectiveness \nin real-world scenarios. Furthermore, the \ncomparative analysis with existing methods \nreveals CodeGuard’s superior performance, \nunderscoring its potential in improving \nsoftware security practices. \nHowever, like all research, this study \nis not without limita tions, and these open \nthe door to several future research \ndirections. Firstly, while CodeGuard has \nshown high efficacy in C/C++ code \nvulnerability detection, extending its \napplicability to other programming \nlanguages is essential. Future research \ncould focus on adapting the framework’s \nmethodologies to diverse programming \nlanguages, enhancing its utility in a broader \nrange of software development environments. \nAnother area of potential exploration is \nthe integration of additional machine learning \ntechniques. The current implementation of \nCodeGuard relies heavily on PET and cloze \nstyle queries. Future iterations could benefit \nfrom incorporating other advanced machine \nlearning algorithms, such as deep \nreinforcement learni ng or unsupervised \nlearning methods. These additions could \npotentially improve the framework’s ability \nto handle more complex and nuanced \nvulnerability patterns, especially in datasets \nthat pose greater chal lenges, like REVEAL \nand D2A. \nMoreover, the evolving nature of \nsoftware vulnerabilities necessitates \ncontinuous updates and enhancements to \nvulnerability detection frameworks. Future \nresearch should focus on adaptive learning \nmechanisms that enable CodeGuard to \nevolve in response to  emerging types of \nvulnerabilities. This adaptability is crucial for \nmaintaining the framework’s effectiveness \nover time, given the rapidly changing \nlandscape of software security threats. \nIn addition to technical \nadvancements, future research should also \nconsider the practic al integration of \nCodeGuard within software development \npipelines. Seamless integration with \npopular development tools and \nenvironments would fa cilitate its adoption \nin real-world settings. Research efforts \ncould explore the development of plugins \nor extensions that enable CodeGuard’s \nfunctionality within integrated development \nenvironments (IDEs) or continuous \nintegration/continuous deployment (CI/CD) \npipelines. \nLastly, the ethical implications and \nprivacy concerns related to automated \nvulnerability detection need to be \naddressed. Future versions of CodeGuard \nshould incorporate mechanisms that ensure \ndata privacy and ethical considerations, \nparticularly when deal ing with sensitive\n  or \nproprietary codebases. \nIn summary, the CodeGuard \nframework represents a significant step \nforward in the field of software vulnerability \ndetection. Its success in accurately identifying \nvulnerabilities across  diverse datasets \nshowcases its potential as a valuable tool for \nsoftware developers an d security analysts. \nBy addressing the outlin ed future research \ndirections, CodeGuard can evolve into a \nmore versatile, adap table, and ethically \nconscious framework, further contributing to \nthe enhancement of software security in an \nincreasingly digital world. \n115\nREFERENCES \nAlharbi, A.R., Hijji, M., & Aljaedi, A. (2021). Enhancing topic clust\nering for Arabic \nsecurity news based on k-means and topic modelling. IET Networks, Vol. 10, Issue 6, 278-294. \nAvailable at: https://doi.org/10.1049/ntw2.12017.  \nAluru, S.S., Mathew, B., Saha, P., & Mukherjee, A. (2020) . Deep Learning Models for \nMultilingual Hate Speech Detection. arXiv preprint arXiv:2004.06465 , available at: \nhttps://doi.org/10.48550/arXiv.2004.06465.  \nAyub, M.F., Li, X., Mahmood, K., Shamshad, S., Saleem, M.A., & Omar, M. (2023). \nSecure Consumer-Centric Demand Response Management in Resilient Smart Grid as Industry 5.0 \nApplication with Blockchain-Based Authentication. IEEE Transactions on Consumer Electronics. \nAvailable at: http://dx.doi.org/10.1109/tce.2023.3320974.   \nChakraborty, S., Krishna, R ., Ding, Y., & Ray, B. (2021 ). Deep Learning Based \nVulnerability Detection: Are We There Yet? IEEE Transactions on Software Engineering, \nVol. 48, Issue 9, 3280-3296. DOI: 10.1109/TSE.2021.3087402.  \nCheng, X., Wang, H., Hua, J., Xu, G., & Su i, Y. (2021). DeepWukong: Statically \nDetecting Software Vulnerabilities Using Deep Graph Neural Network. ACM Transactions on \nSoftware Engineering and Methodol ogy (TOSEM), Vol. 30, Issue 3 , 1-33. Available at: \nhttps://doi.org/10.1145/3436877.  \nHanif, H., & Maffeis, S. (2022 ). VulBERTa: Simplified Sour ce Code Pre-Training for \nVulnerability Detection. 2022 IEEE International Joint Conference on Neural Networks (IJCNN), \n1-8. DOI: 10.1109/IJCNN55064.2022.9892280.  \nKim, S., Woo, S., Lee, H., & Oh, H. (2017). VUDDY: A Scalable Approach for Vulnerable \nCode Clone Discovery. 2017 IEEE Symposium on Secu rity and Privacy (SP) , 595-614. DOI: \n10.1109/SP.2017.62.  \nKim, S., Choi, J., Ahmed, M.E., Nepal, S., & Kim, H. (2022). VulDeBERT: \nA Vulnerability Detection System Using BERT. 2022 IEEE International Symposium on \nSoftware Reliability Engineering Workshops (ISSREW),  69-74. DOI: \nhttps://doi.org/10.1109/ISSREW55968.2022.00042.  \nLi, X., Wang, L., Xin, Y., Yang, Y., & Chen, Y. (2020). Automated vulnerability detection \nin source code using minimum intermediate representation learning. Applied Sciences, Vol. 10, \nIssue 5, 1692. Available at: https://doi.org/10.3390/app10051692.  \nLi, Z., Zou, D., Xu, S., Jin, H., Qi, H., & Hu, J. (2016) . VulPecker: an automated \nvulnerability detection system based on code similarity analysis. Proceedings of the 32nd Annual \nConference on Computer Security Applications,  201-213. Available at: \nhttps://doi.org/10.1145/2991079.2991102.  \n116\nOmar, M. (2022). Machine Learning for Cybersecurity: Innovative Deep Learning \nSolutions. Springer International Publishing. \nOmar, M. (2023). Back door learning for nlp: Recent advances, ch allenges, and future \nresearch directions. arXiv preprint arXiv. DOI:10.48550/arXiv.2302.06801.  \nOmar, M. (2023). VulDefend: A Novel Technique based on Pattern-exploiting Training for \nDetecting Software Vulnerabilities Using Language Models. 2023 IEEE Jordan International \nJoint Conference on Electrical Engineer ing and Information Technology (JEEIT) , 287-293. \nDOI: 10.1109/JEEIT58638.2023.10185860. \nOmar, M., Choi, S., Nyang, D. H., & Mohaisen, D. (2022). Robust natural language \nprocessing: Recent advances, challenges, and future directions. arXiv preprint arXiv:2201.00768. \nAvailable at: https://arxiv.org/pdf/2201.00768.pdf.  \nOmar, M., Jones, R., Burrell, D.N., Dawson, M., Nobles, C., Mohammed, D., & Bashir, \nA.K. (2023). Harnessing the Power and Simplicity of Decision Trees to Detect IoT Malware. \nTransformational Interventions for Business, Technology, and Healthcare, 215-229. IGI Global. \nOmar, M., & Suktha nkar, G. (2023). Text-defend: Detecting adversarial examples using \nlocal outlier factor. 2023 IEEE 17th International Conference on Semantic Computing (ICSC), \n118-122. DOI: 10.1109/ICSC56153.2023.00026.  \nRabheru, R., Hanif, H., & Maffeis, S. (2021). DeepTective: Detection of PHP \nvulnerabilities using hybrid graph neural networks. Proceedings of the 36th Annual ACM \nSymposium on Applied Computing,  1687-1690. Availa ble at: https:// doi.org/10.1145/ \n3412841.3442132.  \nRadford, A., Wu, J., Child, R., Luan, D., Amode i, D., & Sutskever, I. (2019). Language \nmodels are unsupervised multit ask learners. OpenAI blog, No. 8, Vol. 1. Available at: \nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf.  \nRussell, R., Kim, L., Hamilton, L., Lazovich, T., Harer, J., Ozdemir, O., Ellingwood, P., & \nMcConley, M. (2018 ). Automated vulnerab ility detection in sour ce code using deep \nrepresentation learning. 2018 17th IEEE International Confer ence on Machine Learning and \nApplications (ICMLA), 757-762. DOI: 10.1109/ICMLA.2018.00120.  \nSalimi, S., & Kharrazi, M. ( 2022). VulSlicer: Vulnerability detection through code slicing. \nJournal of Systems and Software, Vol. 193 , 111450. Available at: \nhttps://doi.org/10.1016/j.jss.2022.111450.  \nYamaguchi, F., Golde, N., Arp, D., & Riec k, K. (2014). Modeli ng and Discovering \nVulnerabilities with Code Property Graphs. 2014 IEEE Symposium on S ecurity and Privacy,  \n590-604. DOI: 10.1109/SP.2014.44.  \nYan, R., Xiao, X., Hu, G., Peng, S., & Jiang,  Y. (2018). New deep learning method to \ndetect code injection attacks on hybrid applications. Journal of Systems and Software, Vol. 137, \n67-77. DOI:10.1016/j.jss.2017.11.001.  \nZhou, X., & Verma, R.M. (2022). Vulnerability Dete ction via Multim odal Learning: \nDatasets and Analysis. Proceedings of the 2022 ACM on As ia Conference on Computer and \nCommunications Security , 1225-1227, 2022. Available at: https://doi.org/10.1145/ \n3488932.3527288.  \n117\nZhou, Y., Liu, S., Siow, J., Du, X., & Liu, Y. (2019). Devign: Effective vulnerability \nidentification by learning co mprehensive program semantics via graph neural networks. \nNIPS Proceedings − Advances in Neural Inform ation Processing Systems, 32 , available at: \nhttps://papers.nips.cc/book/advances-in-neural-information-processing-systems-32-2019.  \nZou, D., Wang, S., Xu, S., Li, Z., & Jin, H. (2019). μVulDeePecker: A deep learning-\nbased system for multiclass vulnerability detection. IEEE Transactions on Dependable and \nSecure Computing, Vol. 18, Issue 5 , 2224-2236. Available at: https://doi.org/10.1109/ \nTDSC.2019.2942930.\n118",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8056054711341858
    },
    {
      "name": "Code (set theory)",
      "score": 0.5922426581382751
    },
    {
      "name": "Identification (biology)",
      "score": 0.5817683935165405
    },
    {
      "name": "Vulnerability (computing)",
      "score": 0.5476903915405273
    },
    {
      "name": "Source code",
      "score": 0.5343956351280212
    },
    {
      "name": "Code review",
      "score": 0.5294254422187805
    },
    {
      "name": "Software",
      "score": 0.4981963634490967
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4525038003921509
    },
    {
      "name": "Natural language processing",
      "score": 0.43752118945121765
    },
    {
      "name": "Machine learning",
      "score": 0.3997516930103302
    },
    {
      "name": "Software quality",
      "score": 0.3795827627182007
    },
    {
      "name": "Software development",
      "score": 0.22354966402053833
    },
    {
      "name": "Programming language",
      "score": 0.1530182957649231
    },
    {
      "name": "Computer security",
      "score": 0.08872717618942261
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.06811380386352539
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98993165",
      "name": "Capitol Technology University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I180949307",
      "name": "Illinois Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 8
}