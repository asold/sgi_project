{
    "title": "Optimizing Knowledge Extraction in Large Language Models Using Dynamic Tokenization Dictionaries",
    "url": "https://openalex.org/W4399590448",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5099107811",
            "name": "Harold Chiappe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5113249383",
            "name": "Gabriel Lennon",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4392366668",
        "https://openalex.org/W4319301677",
        "https://openalex.org/W4380559074",
        "https://openalex.org/W4391407054",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4390268970",
        "https://openalex.org/W4398178191",
        "https://openalex.org/W4399154003",
        "https://openalex.org/W4378499842",
        "https://openalex.org/W4377164404",
        "https://openalex.org/W4396859483",
        "https://openalex.org/W4392593764",
        "https://openalex.org/W4387559390",
        "https://openalex.org/W4398173773",
        "https://openalex.org/W4390190613",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4392196198",
        "https://openalex.org/W4391901128",
        "https://openalex.org/W4398774455"
    ],
    "abstract": "Tokenization methods have long been a critical component in the performance of language models, yet traditional static approaches often fall short in capturing the dynamic nature of language. The novel concept of implementing a dynamic tokenization dictionary within the Llama model presents a significant advancement, offering real-time adaptability in response to evolving linguistic patterns. The adaptive tokenization algorithm continuously updates the token set based on frequency and context, thereby enhancing the model's ability to generate coherent and contextually relevant outputs. Comprehensive evaluation across multiple benchmark datasets reveals substantial improvements in metrics such as perplexity, F1 Score, BLEU Score, and ROUGE Score, underscoring the efficacy of dynamic tokenization. The implications of these findings extend to various domains, including healthcare, legal analysis, education, and customer service, demonstrating the broad applicability and transformative potential of dynamic tokenized dictionaries. This research not only advances the understanding of tokenization processes but also provides a robust framework for enhancing the efficiency and accuracy of large language models in real-world applications.",
    "full_text": null
}