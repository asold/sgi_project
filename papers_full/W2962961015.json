{
  "title": "Skeleton-Based Gesture Recognition Using Several Fully Connected Layers with Path Signature Features and Temporal Transformer Module",
  "url": "https://openalex.org/W2962961015",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2109258858",
      "name": "Chenyang Li",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2002306827",
      "name": "Xin Zhang",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2900559767",
      "name": "Lufan Liao",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2110317904",
      "name": "Lianwen Jin",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2106914346",
      "name": "Weixin Yang",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2109258858",
      "name": "Chenyang Li",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2002306827",
      "name": "Xin Zhang",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2900559767",
      "name": "Lufan Liao",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2110317904",
      "name": "Lianwen Jin",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2106914346",
      "name": "Weixin Yang",
      "affiliations": [
        "South China University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2781400102",
    "https://openalex.org/W2074884967",
    "https://openalex.org/W2467634805",
    "https://openalex.org/W2415469094",
    "https://openalex.org/W1950788856",
    "https://openalex.org/W6640416349",
    "https://openalex.org/W1995113806",
    "https://openalex.org/W2558718173",
    "https://openalex.org/W3106415951",
    "https://openalex.org/W203345490",
    "https://openalex.org/W1462226831",
    "https://openalex.org/W2587969154",
    "https://openalex.org/W1990429558",
    "https://openalex.org/W2770491442",
    "https://openalex.org/W2806985529",
    "https://openalex.org/W2787731280",
    "https://openalex.org/W6619091367",
    "https://openalex.org/W2139794777",
    "https://openalex.org/W6651679925",
    "https://openalex.org/W6750156365",
    "https://openalex.org/W2587376528",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W2951208315",
    "https://openalex.org/W2048821851",
    "https://openalex.org/W2475715656",
    "https://openalex.org/W2606294640",
    "https://openalex.org/W2143267104",
    "https://openalex.org/W2526041356",
    "https://openalex.org/W2765354204",
    "https://openalex.org/W2768642967",
    "https://openalex.org/W2413546514",
    "https://openalex.org/W6692078629",
    "https://openalex.org/W2895445581",
    "https://openalex.org/W2145546283",
    "https://openalex.org/W2610104114",
    "https://openalex.org/W2160511393",
    "https://openalex.org/W2119818729",
    "https://openalex.org/W2004095444",
    "https://openalex.org/W2133463914",
    "https://openalex.org/W2963370140",
    "https://openalex.org/W2963177663",
    "https://openalex.org/W2798498022",
    "https://openalex.org/W846669277",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W1926645898",
    "https://openalex.org/W3123822062",
    "https://openalex.org/W4293182332",
    "https://openalex.org/W2964304707",
    "https://openalex.org/W2952587893",
    "https://openalex.org/W2735590100",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2297500149",
    "https://openalex.org/W2771248633",
    "https://openalex.org/W2770472008"
  ],
  "abstract": "The skeleton based gesture recognition is gaining more popularity due to its wide possible applications. The key issues are how to extract discriminative features and how to design the classification model. In this paper, we first leverage a robust feature descriptor, path signature (PS), and propose three PS features to explicitly represent the spatial and temporal motion characteristics, i.e., spatial PS (S PS), temporal PS (T PS) and temporal spatial PS (T S PS). Considering the significance of fine hand movements in the gesture, we propose an ”attention on hand” (AOH) principle to define joint pairs for the S PS and select single joint for the T PS. In addition, the dyadic method is employed to extract the T PS and T S PS features that encode global and local temporal dynamics in the motion. Secondly, without the recurrent strategy, the classification model still faces challenges on temporal variation among different sequences. We propose a new temporal transformer module (TTM) that can match the sequence key frames by learning the temporal shifting parameter for each input. This is a learning-based module that can be included into standard neural network architecture. Finally, we design a multi-stream fully connected layer based network to treat spatial and temporal features separately and fused them together for the final result. We have tested our method on three benchmark gesture datasets, i.e., ChaLearn 2016, ChaLearn 2013 and MSRC-12. Experimental results demonstrate that we achieve the state-of-the-art performance on skeleton-based gesture recognition with high computational efficiency.",
  "full_text": "The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)\nSkeleton-Based Gesture Recognition Using Several Fully Connected\nLayers with Path Signature Features and Temporal Transformer Module\nChenyang Li, Xin Zhang,∗ Lufan Liao, Lianwen Jin, Weixin Yang\nSchool of Electronic and Information Engineering, South China University of Technology\neexinzhang@scut.edu.cn\nAbstract\nThe skeleton based gesture recognition is gaining more pop-\nularity due to its wide possible applications. The key issues\nare how to extract discriminative features and how to de-\nsign the classiﬁcation model. In this paper, we ﬁrst leverage\na robust feature descriptor, path signature (PS), and propose\nthree PS features to explicitly represent the spatial and tem-\nporal motion characteristics, i.e., spatial PS (S PS), tempo-\nral PS (T PS) and temporal spatial PS (T S PS). Consider-\ning the signiﬁcance of ﬁne hand movements in the gesture,\nwe propose an ”attention on hand” (AOH) principle to de-\nﬁne joint pairs for the S PS and select single joint for the\nT PS. In addition, the dyadic method is employed to extract\nthe T PS and T S PS features that encode global and local\ntemporal dynamics in the motion. Secondly, without the re-\ncurrent strategy, the classiﬁcation model still faces challenges\non temporal variation among different sequences. We propose\na new temporal transformer module (TTM) that can match\nthe sequence key frames by learning the temporal shifting pa-\nrameter for each input. This is a learning-based module that\ncan be included into standard neural network architecture. Fi-\nnally, we design a multi-stream fully connected layer based\nnetwork to treat spatial and temporal features separately and\nfused them together for the ﬁnal result. We have tested our\nmethod on three benchmark gesture datasets, i.e., ChaLearn\n2016, ChaLearn 2013 and MSRC-12. Experimental results\ndemonstrate that we achieve the state-of-the-art performance\non skeleton-based gesture recognition with high computa-\ntional efﬁciency.\n1 Introduction\nWith the development of intelligent device ( e.g., AR, VR\nand smart-home devices), hand gesture interaction is at-\ntracting more attention because of its wide applications\nfor human/computer interaction and communications. Hand\ngesture recognition is an important and classic problem.\nRecently, the accurate vision based pose/skeleton estima-\ntion gains more popularity due to cost-effective depth sen-\nsor (like Microsoft Kinect and Intel RealSense) and reli-\nable real-time body pose estimation development (Wei et\nal. 2016). Comparing with RGB-D sequence based gesture\nrecognition, the skeleton based methods are robust to illumi-\nnation changes and view variations, and avoid motion am-\nCopyright c⃝ 2019, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The ﬂowchart of our algorithm.\nbiguity. In this paper, we focus on the skeleton-based iso-\nlated hand gestures recognition, that is, one gesture per one\nsequence. The key issues are how to extract discriminative\nspatial temporal features and how to design the classiﬁcation\nmodel.\nHand gestures can be quite different among various users\nand application scenario. Since human action and hand ges-\nture are similar in terms of motion representation and prob-\nlem formulation, here we discuss them together. The gesture\nrecognition framework usually involves the feature descrip-\ntion and temporal dependency based classiﬁcation model.\nTraditional skeleton-based action recognition approaches in-\nvolve hand-crafted feature extraction. The joint trajectory\ncovariance matrix (Hussein et al. 2013), pairwise relative po-\nsitions (Wang et al. 2012), 3D joint histogram (Xia, Chen,\nand Aggarwal 2012) and Lie group embedding (Vemula-\npalli, Arrate, and Chellappa 2014) are used to represent the\nskeleton sequences. Human-crafted features are straightfor-\nward but with limited representative abilities, which usu-\nally require the domain knowledge. Recently, convolutional\nneural network (CNN) and its extensions are widely used\nfor feature extraction, like 2D-CNN, 3D-CNN, C3D etc.\nC3D (Tran et al. 2015) is a deep 3D convolutional net-\nwork model based spatial-temporal feature. It is a generic,\ncompact and implicit representation but requires the large-\nscale training data. Regarding the action temporal dynam-\nics, Fourier temporal pyramid (FTP) (Veeriah, Zhuang, and\nQi 2015) and hidden Markov model (HMM) (Xia, Chen,\nand Aggarwal 2012) are used with hand-crafted features. For\nthe deep learning method, different structures of Recurrent\nNeural Networks (RNN),e.g., hierarchical RNN (Du, Wang,\nand Wang 2015), RNN with regularization (Zhu et al. 2016)\ndifferential RNN (Veeriah, Zhuang, and Qi 2015), two-\n8585\nstream RNN (Wang and Wang 2017) and Long Short-Term\nMemory (LSTM) (Weng et al. 2018), are popular choice\nto explore the temporal dependency for recognition. These\nframeworks have reached state-of-the-art recognition results\nbut the computational complexity may be unacceptable in\nreal-world applications. Hence, we need simple, compact\nand explicit features to represent global body movements\nand ﬁne hand motions. Also, the classiﬁcation model should\nbe simple with temporal dependency.\nIn this paper, we propose the path-signature feature based\nhand gesture recognition framework with only few fully\nconnected layers. The ﬂowchart of our algorithm is shown\nin Fig. 1. The main contributions are as follows:\n• We introduce the three different path signature (PS) fea-\ntures , i.e., spatial (S PS), temporal (T PS) and tempo-\nral spatial PS (T S PS) features, to explicitly characterize\nspatial conﬁguration and temporal dependency of hand\ngestures. We also propose an AOH principle to deﬁne\njoint pairs for the S PS and select single joint for the\nT PS. In addition, the dyadic method is employed to\nextract the T PS and T S PS features that encode both\nglobal and local temporal dynamics.\n• We propose the temporal transformer module (TTM) that\ncan actively produce an appropriate temporal transforma-\ntion for each input sequence. This is a learning-based\nmodule that can be included into standard neural network\narchitecture.\n• We propose an extremely simple multi-streams architec-\nture as the classiﬁer with only several fully connected\n(FC) layers. Different features have their own channels\nand the FC layer are used for ﬁnal fusion.\n• By only using skeleton data, our method obtains the\nstate-of-the-art results on three major benchmarks, i.e.,\nChaLearn 2013, ChaLearn 2016 and MSRC 12. Further,\nour model requires less ﬂoating-point multiplication-adds\nand training memory.\n2 Related Work\n2.1 Gesture recognition\nSkeleton based hand gesture recognition methods are much\nless than those dealing with the full body skeleton based\naction recognition (Wang et al. 2018). It is limited by the\ndataset availability and gesture unique property. Hand ges-\nture mainly involves the ﬁnger, palm and hand motion,\nwhich only has 1-3 joints in the skeleton obtained from\ndepth data. In (De Smedt, Wannous, and Vandeborre 2016),\nseveral skeleton-based features are used together as temporal\npyramid, including shape of connected joints, histogram of\nhand direction and histogram of wrist rotation. In 2D skele-\nton are superimposed onto original image as dynamic sig-\nnatures. These features aim to describe the hand motion in\ndetail but the description and representation abilities are lim-\nited.\nDeep learning have made great process in the area of ac-\ntion recognition. Considering the sequential property, it is\nnatural to apply the RNN, LSTM and their extensions to\nlearn temporal dynamics. In 2017 ChaLearn LAP RGB-D\nisolated gesture recognition competition (Wan et al. 2017),\nthe largest hand gesture recognition contest, most partici-\npants (including the winner) used C3D and/or LSTM neu-\nral networks. C3D architecture has been widely used in\nthe action recognition for appearance and motion model-\ning because it is more suitable for spatial-temporal fea-\nture extraction than 2D CNN. The model inputs are multi-\nmodalities including RGB, depth, optical ﬂow and/or skele-\nton. Recently, (Weng et al. 2018) proposed a deformable\npose traversal convolution method based on 1D convolu-\ntion and LSTM. These recognition networks usually have\nmultiple LSTMs and temporal streams channels and the\nﬁnal result is multi-stream average fusion. In (Wang and\nWang 2017), RNN architecture not only characterizes the\ntemporal dynamics but also considers the spatial conﬁgura-\ntion in the two-stream architecture. With the proper model-\ning of skeleton structure and spatial dependency of the ac-\ntion, recognition accuracy increased. In the latest hand ges-\nture recognition research (Narayana, Beveridge, and Draper\n2018), by using RGB-D and their ﬂow as inputs, the net-\nwork has 12 channels representing the large body movement\nand ﬁne hand motions individually. The fusion channel is\na sparsely connected network with one weight per gesture\nand channel. The RNN/LSTM frameworks deliver the state-\nof-the-art performance on most action and gesture recog-\nnition datasets, indicating the excellent feature and depen-\ndency learning capabilities. The only concerns are the archi-\ntecture complexity, training data requirement and computa-\ntional efﬁciency.\n2.2 Path signature feature\nThe path signature (PS) was ﬁrst proposed in (Chen 1958)\nin the form of noncommutative formal power series. Af-\nter that PS was used to solve differential equations driven\nby rough paths (Lyons 1998; Garrido 2010). Recently, the\npath signature has been successfully used as a trajectory de-\nscriptor and applied to many tasks in the ﬁeld of machine\nlearning and pattern recognition, such as quantitative ﬁnance\n(Gyurk´o et al. 2013; Lyons, Ni, and Oberhauser 2014), hand-\nwriting recognition (Lai, Jin, and Yang 2017; Yang et al.\n2015), writer identiﬁcation (Yang, Jin, and Liu 2015; 2016;\nLiu, Jin, and Xie 2017), human action (Yang et al. 2017) and\nhand gesture recognition (Li, Zhang, and Jin 2017). (Yang\net al. 2017) is the pioneer work employing the path signature\nfeature for skeleton-based action recognition. All joint pairs\nand temporal joint evolution are considered as path and the\ncorresponding path signatures are computed as features. The\nconcatenation of all path signatures are the input vector for\nclassiﬁcation. In (Li, Zhang, and Jin 2017), the path signa-\nture is the ﬁrstly used in the gesture recognition by deﬁning\nthe hand trajectory as the path. Path signature can provide\nthe informative representation of sequential data but how to\ndeﬁne proper paths and how to deal with their high dimen-\nsionality is worthy to be explored.\n3 Overview of Path Signature\nIn this section, we will brieﬂy introduce the mathematical\ndeﬁnition, geometric interpretation and some properties of\n8586\npath signature (PS), which is mainly referred to (Chevyrev\nand Kormilitzin 2016).\nAssume a path P : [t1,t2] →Rd, where [t1,t2] is a time\ninterval. The coordinate paths are denoted by (P1\nt ,...,P d\nt ),\nwhere each Pi : [ t1,t2] → R is a real-value path. For\nan integer k ≥1 and the collection of indices i1,...,i k ∈\n{1,...,d }, the k-fold iterated integral of the path along in-\ndices i1,...,i k can be deﬁned as:\nS(P)i1,...,ik\nt1,t2 =\n∫\nt1<ak<t2\n...\n∫\nt1<a1<a2\ndPi1\na1 ...dPik\nak (1)\nwhere t1 <a1 <a2 <...<a k <t2.\nThe signature of path P, denoted by S(P)t1,t2 , is the col-\nlection (inﬁnite series) of all the iterated integrals of P:\nS(P)t1,t2 =(1,S(P)1\nt1,t2 ,S(P)2\nt1,t2 ,...,S (P)d\nt1,t2 ,\nS(P)1,1\nt1,t2 ,...,S (P)1,d\nt1,t2 ,...,S (P)d,d\nt1,t2 ,\n...,\nS(P)1,...,1\nt1,t2 ,...,S (P)i1,...,ik\nt1,t2 ,...,S (P)d,...,d\nt1,t2 ,\n...)\n(2)\nThe k-th level PS is the collection (ﬁnite series) of all the k-\nfold iterated integralof path P. The 1-st and 2-nd level rep-\nresents path displacement and path curvature respectively.\nBy increasing k, higher levels of path information can be\nextracted, but the dimensionality of iterated integrals enlarge\nrapidly as well. Note that the0-th level PS of pathP is equal\nto 1 by convention.\nIn practice, we often truncate the S(P)t1,t2 at level mto\nensure the dimensionality of the PS feature in a reasonable\nrange. The dimensionality of S(P)t1,t2 truncated at level m\nis calculated through M = d+ ··· + dm (without zeroth\nterm).\nThe path is considered as the piecewise linear path after\nsampling. The PS of a discrete path with ﬁnite length can\nbe easily calculate based on linear interpolation and Chen’s\nidentity (Chen 1958). For each straight line of a path, the\nelement of its PS can be calculates by:\nS(P)i1,...,ik\nt,t+1 = 1\nk!\nk∏\nj=1\nS(P)ij\nt,t+1 (3)\nFor the entire path, Chen’s identity states that for any\n(ts,tm,tu) satisfying: ts <tm <tu, then,\nS(P)i1,...,ik,...,in\nts,tu =\nn∑\nk=0\nS(P)i1,...,ik\nts,tm S(P)ik+1,ik+2,...,in\ntm,tu\n(4)\nPS has two excellent properties for path expression. First,\nPS is the unique representation of a non tree-like path (Ham-\nbly and Lyons 2010). A tree-like path is a trajectory that re-\ntraces itself (such as clapping). For time-sequential data, it’s\nnatural and effective to add an extra time dimension into the\noriginal path to avoid the tree-like situation. Second, shufﬂe\nproduct identity (Lyons et al. 2004) indicates that the prod-\nuct of two signature of lower level can be expressed as a lin-\near combination of some higher level terms. Hence, adoption\nof higher level terms of PS actually brings more nonlinear\nprior knowledge, which reduces the need for the classiﬁer of\nhigh complexity. More properties and related details can be\nfound in (Chevyrev and Kormilitzin 2016).\nWe recommend an open-source python library named\niisignature, which can be easily installed through pip.\n4 Approach\nIn this section, we ﬁrst introduce an ”attention on hand”\n(AOH) principle for PS extraction, which deals with global\nbody movements and ﬁne hand motions. Then we propose\na novel temporal transformer module (TTM) to alleviate the\nsequence temporal variation. Finally a multi-stream archi-\ntecture is presented to fuse different types of features.\n4.1 AOH principle and PS extraction\nAOH principle Before calculating the PS feature, we need\nto consider about what path to be used and how to design\npaths efﬁciently for the recognition. In (Yang et al. 2017),\nthe ﬁrst work leveraging PS features in the human action\nrecognition problem, single joint, joint pair and joint triple\nare utilized to deﬁne paths. They use allNjoints and exhaust\nall the possible pairs and triples ( i.e., C2\nN and C3\nN ), which\nbrings performance improvement but increases dimension-\nality dramatically. In the context of gesture recognition, we\npropose the AOH principle to select single joints and joint\npairs.\nFor the single joint, only the joint belongs to the hand part\n(including elbow, wrist and hand 3 joints, i.e., NJ = 3 ·2 =\n6) are selected, as Fig. 2 (a) shows. For the joint pair, three\nkinds of pairs are considered (as depicted in Fig. 2 (b) 1)-\n3)). The ﬁrst kind of joint pairis inside the same hand part,\ndescribing the geometric characteristics of hand explicitly.\nThe second kind of joint pairis from two hand parts, indi-\ncating the relative state of two hands. The third kind of joint\npair is across hand part and body part (upper body joints ex-\ncept hand part), characterizing the related location of hand\nand body. The selected single joint and joint pairs deﬁned\nby AOH principle can not only model global hand relative\nbody movements and ﬁne hand motions but also make the\nPS features more compact.\nPath deﬁnition and PS feature extraction Based on the\nselected single joint and joint pairs obtained by AOH princi-\nple, we further deﬁne one spatial path and two kinds of tem-\nporal paths for PS feature extraction. We regard each joint\npair as a spatial path for the PS feature extraction. The ﬁrst\ntype of temporal path is the evolution of each selected single\njoint along the time, as shown in Fig. 2 (c). Another type of\ntemporal path is the evolution of spatial correlations among\njoints, as shown in Fig. 2 (d). The summary of three PS fea-\ntures are shown in Table 1.\nSpatial PS features The fundamental description of spa-\ntial structure is the d-dimensional raw coordinates. We con-\ncatenate the coordinates of single joints in each frame as a\nRC vector obeying chronological order as Fig. 2 (c) shows.\nFurther, due to the poor characterization ability of RC and\nnoise interference, we extract PS features over selected joint\n8587\nFigure 2: The illustration of PS features extraction (TPS, S PS and T S PS). (a) and (b) are single joints and joint pairs selected\nfollowing AOH. (c) and (d) are the temporal paths of single joint and each dimension of S PS. Note that TTM (proposed in\nSection 4.2 and illustrated in Fig. 4) should be implemented between AOH and PS extractors, but we omit it here for clarity.\npairs to explore the spatial relation between joints. The im-\nplementations of spatial PS feature extractor (Box A in\nFig. 2) are as follows: i) Select elements that need to be\ncalculated in Eq.1 according to the truncated level mS. ii)\nCalculate the truncated spatial PS of a joint pair (a straight\nline in Eq.3) by Eq.1 and Eq.3 (The start and end points are\ndeﬁned according to the predeﬁned order in Fig. 2). iii) Fi-\nnally concatenate the truncated spatial PS of all input joint\npairs as the spatial PS (S PS) feature.\nDyadic temporal PS features The dyadic method with\nPS was ﬁrstly used in (Yang et al. 2016) for the writer ID\nidentiﬁcation. Since the gesture always contain global and\nlocal variation, we employ the dyadic method for temporal\nPS feature extraction.\nThe dyadic method divides the entire path into dyadic\npieces and set up a hierarchical representation of the path.\nIt can extract both the global and local feature of entire\npath, and reduce the feature dimensionality as well. If the\ndyadic level is LD, then an entire path can be divided into\n2(LD+1) −1 subpaths.\nTo characterize the temporal dynamic of single joint, the\nevolution of each single joint is treated as an entire temporal\npath as shown in Fig. 2 (c). An extra monotone time dimen-\nsion is added to ensure the path uniqueness ( i.e., to avoid\ntree-like path as discussed in Section 3).\nTo further explore kinematic constraints of the joint pairs,\nthe evolution of each dimension in the S PS of every frame\nalso can be regarded as another kind of entire temporal path,\nas Fig. 2 (d) shows. As a result, we acquire a series of 1D\npaths. However, the signature of a 1D path is just the in-\ncrements to a certain power, which can be easily get from\nEq. 3. To alleviate this problem, we use the lead-lag trans-\nformation as (Yang et al. 2017) does over the 1D path to\nenrich the temporal contextual information.\nThe implementations of temporal PS feature extractor\n(Box B in Fig. 2) are as follows: i) Select elements that need\nto be calculated in Eq.1 according to the truncated level mT\nor mT S. ii) Every subpath generated by dyadic method is\nan entire path (consist of several straight lines) in Eq. 4. Ac-\ncording to Eq. 1, Eq. 3 and Eq. 4, the truncated temporal\nPS of a subpath can be calculated. iii) Concatenate the trun-\ncated temporal PS of all subpaths as temporal PS (T PS) or\ntemporal spatial PS (T S PS) feature.\nTable 1: The proposed feature for skeletal hand gesture\nrecognition.\nFeature types Feature description\nRaw coordinates\n(RC) The d-dimensional coordinates\nof NJ single joints.\nSpatial PS\n(S PS) The PS over each predeﬁned joint\npair truncated at level mS.\nTemporal PS\n(T PS) The PS over the temporal evolu-\ntion of each single joint truncated\nat level mT .\nTemporal Spatial PS\n(T S PS) The PS over the temporal evolu-\ntion of each dimension of S PS\ntruncated at level mT S.\n4.2 Temporal Transformer Module (TTM)\nMotivation Although deep neural network achieves break\nthrough in the sequential recognition task, it’s still limited\n8588\nby the lack of ability to be temporally invariant to the input\nsequence in a computationally and parameter efﬁcient man-\nner. In the context of action recognition, the time-stamps of\nkeyframes are variant among different clips, which makes\nthe model difﬁcult to catch the key information along time\ndimension.\nThere are mainly two existing types of methods to solve\nthis problem: structure driven method and data driven\nmethod. The structure driven method mostly use LSTM to\nmodel the temporal contextual dependence of sequence data.\nThe data driven method is to provide more diverse samples\nby temporal shift data enhancement. However, LSTM model\nrequires large training data and unnegligible training cost.\nIf we used a simple network like FC layer as the classiﬁer,\nthe temporal consistency is also learned as part of features,\nwhich is the unwanted result. For example, we visualize the\nweight matrix of the ﬁrst FC layer of a trained one-stream\nnetwork (will be introduced in the following), which takes\nRC as input, as shown in Fig. 3. The x-axis denotes the input\ndimensionality (obey chronological order), and the y-axis\ndenotes the neuron number (64 in the ﬁrst FC layer). The\nbrighter position means the corresponding weight is larger,\nthat is, this connection is more important. The FC layer pay\nmore attention on several time stamp, indicating the position\nof key frames. If there is the temporal variation between the\nmodel and testing sequence, the recognition result is worse.\nEven if the training data is augmented by temporal shift, the\nmodel capacity is too small to ﬁt any arbitrary temporal sit-\nuation.\nRecent work (Cao et al. 2017) proposed a spatiotemporal\ntransform method to deal with the spatiotemporal variation,\nbut their method is for RGB-D video. As mentioned in Sec-\ntion 4.1, we have employed the temporal PS features to rep-\nresent temporal dynamics within the sequence. This is what\nexactly (Yang et al. 2017) has done. The inter-sequence tem-\nporal difference might be alleviated by the temporal trans-\nformation. To this end, we design a differentiable module\ncalled temporal transformer module (TTM). This module\ncan actively transform the input data temporally and ﬁnally\nadjust the key frame to the best time stamp for the network.\nProposed TTM Inspired by STN (Jaderberg et al. 2015),\nTTM is a differentiable module that applies a temporal trans-\nform to RC. The TTM contains two steps: Localization net-\nwork (LN) and temporal shifting.\nFirstly, we use LN to learn the temporal transform factor\ndelta ( ∆), as shown in Fig.4 (b). It takes the input vector\nI ∈ RDRC (DRC denotes the dimensionality of RC) and\noutput ∆ as shown in Fig. 4 (b),i.e., ∆ = fLN (I). Note that\nthe network function fLN () can take any form, such as FC\nlayer or 1D convolution layer, but should ﬁnally regress to\none neuron.\nSecondly, the input vector I is reshaped as a matrix Vi ∈\nRd·NJ×F , where NJ is the number of single joints ( i.e., 6\nin Fig. 2) and F denotes the frame number. Each column of\nVi is a vector vi\nx which consists of the coordinates of single\njoints in the same frame, x ∈[1,F]. And if we denote the\nmatrix after shifting as Vo, then each column of it can be\nFigure 3: The visualization of the weight matrix of ﬁrst fully\nconnected layer in a trained one-stream network.\ncalculate by:\nvo\nx = fI(x−∆,V i) = (1 −α) ·vi\n⌊x−∆⌋+ α·vi\n⌈x−∆⌉ (5)\nHere, because ∆ is a decimal, we use linear interpola-\ntion function fI() to generate the Vo. α is calculated by\n(x−∆) −⌊x−∆⌋. Note that x−∆ is clip by value [1,\nF]. Eventually, Vo is reshaped back as a output vector O.\nThe code of TTM is made publicly available1.\n4.3 Multi-stream Architecture\nAs discussed in Section 4.1, we deﬁne spatial and temporal\npath based on AOH principle. Corresponding PS features en-\ncode the spatial and temporal information of the action. Pre-\nvious work (Yang et al. 2017; Li, Zhang, and Jin 2017) con-\ncatenates these features together as whole and one classiﬁ-\ncation model is employed,e.g, FC layers. We believe tempo-\nral and spatial features should be treated separately because\none represents the joint evolution and the other describes the\nbody conﬁguration. The ﬁnal result is fusion of these multi-\nple channels.\nSo we utilize the multi-stream architecture to process dif-\nferent kinds of information separately. As a result, we de-\nsign three kinds of network architectures, one-stream net-\nwork (1s net), two-stream network (2s net) and three-stream\nnetwork (3s net), as shown in Fig. 4(c)-(e) . The 1s net di-\nrectly concatenates all the features as one input vector and\nfeed it to a 2-fc-layer network, similar to (Yang et al. 2017).\nThe 2s net has two inputs, RC and PS, representing basic in-\nformation and extracted compact information. As deﬁned in\nTable 1, the T PS and T S PS represent temporal informa-\ntion and S PS is spatial feature. Hence, the 3s net has three\nstreams with two FC layers separately. The ﬁnal fusion result\nis obtained through a FC layer as the weighted summation.\n5 Experimental Results and Discussion\n5.1 Datasets\nChaLearn 2013 dataset: It is the ChaLearn 2013 Multi-\nmodel gesture dataset (Escalera et al. 2013), which contains\n23 hours of Kinect data with 27 persons performing 20 Ital-\nian gestures. This dataset provides RGB, depth, foreground\nsegmentation and Kinect skeletons. Here, we only use skele-\ntons for gesture recognition as done in the literature (Wang\nand Wang 2017).\nChaLearn 2016 dataset:ChaLearn Isolated dataset (Wan et\nal. 2016), the largest gesture recognition dataset consisting\nof RGB and depth videos, includes 35,878 training, 5,784\nvalidation and 6,271 test videos for totally 249 gestures.\n1https://github.com/LiChenyang-Github/Temporal-\nTransformer-Module\n8589\nFigure 4: Three different types of network architectures: one-stream network (1s net), two-stream network (2s net) and three-\nstream network (3s net). TTM is the temporal transformer module. LN is the localization network that generates the transfor-\nmation parameters ∆. ⨂ and ⨁ denote temporal shift and weighted sum. The fc2 in each stream denotes a FC layer with a\nsoftmax activation function.\nWe use Openpose (Wei et al. 2016) to estimate the skele-\nton joints in all videos as (Lin et al. 2018) did. It can be\ndownloaded from our homepage2.\nMSRC-12 dataset: MSRC-12 gesture dataset (Fothergill\net al. 2012) includes 6 iconic and 6 metaphoric gestures\nperformed by 30 people. We use 6 iconic gestures from\nthe dataset that amounts to 3,034 instances and employ 5-\nfold leave-person-out cross-validation as in (Jung and Hong\n2014).\n5.2 Data Preprocessing and Network Setting\nWe ﬁrst normalize skeletons by subtracting the central joint,\nwhich is the average position of all joints in a video clip.\nThen all coordinates are further normalized to the range of [-\n1, 1] over the entire video clip. Finally, we sample all videos\nclips to 39 frames by linear interpolation or uniform sam-\npling. The data enhancement methods we use are three-fold.\nThe ﬁrst one is temporal augmentation by randomly tem-\nporal shift the frame in range of [-5, 5]. The second one is\nadding Gaussian noise with a standard deviation of 0.001 to\njoints coordinates. The last one is rotating coordinates along\nx, y, z three axes in range of [−π/36, π/36], [−π/18, π/18]\nand [−π/36, π/36].\nFor ChaLearn 2013 and MSRC-12 two datasets, we set\nthe neuron number of each 2-fc-layer stream to 64 and\nC (64 fc-C fc), where C is the gesture class number. For\nthe largest dataset ChaLearn 2016, we use 256 fc-C fc. We\nadopt 64 fc-1 fc architecture for fLN (). DropOut (Hinton et\nal. 2012) layer is added after the ﬁrst FC layer of each stream\nto avoid over ﬁtting. The mini-batch size and dropout rate\nare set to 56 and 0.5. We use the method of stochastic gradi-\nent descent with a momentum value equal to 0.7. The learn-\n2http://www.hcii-lab.net/data/\ning rate updates in accordance to α(n) = α(0) ·exp(−λn),\nwhere nis a positive integer starting from 1 and increasing\nby 1 for each 1/2 epoch.α(0) and λare set to 0.01 and 0.001.\nThe LD for T PS and T S PS calculation are set to 3 and 2.\nThe lead-lag dimensionality is set to 2.\n5.3 Ablation Study\nWe do some ablation experiments on the ChaLearn 2013\ndataset to explore the truncated level of PS and examine the\neffectiveness of PS, TTM and multi-stream architecture.\nInvestigation of the truncated level of PS We utilize the\none-stream network without TTM (1s net w/o TTM) to ex-\nplore the contributions of different truncated level PS. The\nvalidation accuracy rate is 77.84% with only RC. The accu-\nracy rate after adding different PS are shown in Table 2. It\nis noted that the T S PS is calculated from S PS truncated\nat level 2 (The abbreviation can be referred to Table 1). The\nperformance improves after adding any type of PS truncated\nat any level, which indicates the effectiveness of PS. The last\ncolumn of each row illustrates that all types of PS feature are\ncomplementary.\nIt is worthy to note that the contributions trend to be neg-\nligible and even vanish when truncated level is greater than\na certain value. There is a trade-off between validation per-\nformance and feature dimensionality. As a result, we choose\nto set mT , mS, mT S as 4, 2, 3.\nInvestigation of the TTM We use the 1s net with RC as\ninput to examine the effectiveness of TTM. Results are pre-\nsented in Table 3. 1s net w/o TTM can be roughly regarded\nas the method proposed by (Yang et al. 2017).\nFirstly, we use temporal augmentation to test whether the\ndata driven methods can make the improvement. We shift\n8590\nTable 2: The ablation study of PS features on ChaLearn\n2013. The truncated level of PS can be referred to Section\n3.\nPS truncated level +T PS +S PS +T S PS +AllPS\n1 77.98 82.30 85.35 85.50\n2 81.28 87.62 87.80 88.76\n3 84.42 88.12 88.81 89.17\n4 85.27 88.06 88.09 89.20\nthe samples in range of [-5, 5] to provide more diverse sam-\nples. However, it doesn’t improve the performance. Hence,\ndirectly data driven methods cannot work well.\nThen we directly add TTM to the 1s net (1s net TTM),\nand the result improves from 77.84% to 80.55%. The tem-\nporal transformation parameter learned by TTM can ﬁt the\nkey moment of an action and the active part of the FC layer.\nAt last we add the same temporal augmentation for\n1s net TTM, and the accuracy rate increases from 80.55%\nto 81.33%. This attractive observation indicates that TTM\nmakes good use of the diverse samples provided by temporal\nenhancement. In other words, the network is more adaptive\nafter adding TTM.\nTable 3: The ablation study of TTM on ChaLearn 2013.\nTemporal Enhancement is abbreviated to Temp. Enh.\nComponents Accuracy rate (%)\n1s net w/o TTM 77.84\n1s net w/o TTM + Temp. Enh. 77.46\n1s net TTM 80.55\n1s net TTM + Temp. Enh. 81.33\nInvestigation of different network architectures For the\nestimation of different architecture, we use network without\nTTM, with RC and all PS selected above as input, which can\nbe roughly regarded as the architecture utilized by (Yang et\nal. 2017). As shown in Table 4, the performance improves\nclearly, which indicates that the multi-stream architectures\nallow each stream to dig deeply into one type of feature and\nﬁnally provide more discriminative information.\nTable 4: The ablation study of network architectures.\nComponents 1s net 2s net 3s net\nAccuracy rate (%) 89.43 89.63 90.19\n5.4 Comparison with the State-of-the-arts\nIn this subsection, we use the best parameter setting and net-\nwork structure getting from our ablation study. We also do\nall the data augmentation methods mentioned above for our\nnetwork.\nChaLearn 2013 The results on the ChaLearn 2013 dataset\nare shown in Table 5. Currently, methods achieving the\nbest performance are mainly beneﬁted from powerful char-\nacterization ability of CNN and LSTM models. (Du, Fu,\nand Wang 2015) organizes the raw coordinates as the spa-\ntial temporal feature maps then feeds it to the hierarchi-\ncal spatial-temporal adaptive ﬁlter banks CNN architecture.\n(Wang and Wang 2017) propose a two-stream LSTM to\nmodel temporal dynamics and spatial conﬁgurations sepa-\nrately. Compared with these method, our FC based network\nachieves the best results with less multiplication-adds oper-\nation as Table 6 shows. Note that the AOH principle dramat-\nically reduces the Multiplication-Adds compared with the\nmethod without AOH (Yang et al. 2017).\nTable 5: Comparison of methods on the ChaLearn 2013.\nMethod Accuracy\nrate (%)\nHiVideoDarwin Wang, Wang, and Wang 74.90\nVideoDarwin Fernando et al. 75.30\nD-LSDA Su et al. 76.80\nCNN for Skeleton Du, Fu, and Wang 91.20\nTwo-stream LSTM Wang and Wang 91.70\n3s net TTM 92.08\nTable 6: Comparison of the Multiplication-Adds.[1]: Du, Fu,\nand Wang [2]: Wang and Wang1: PS calculation.\nMethod CNN[1] 2sLSTM[2] 3snet w/o AOH 3snet\nMult-adds\n(Million) 11.54 358.34 14.891+15.06 2.691+2.00\nChaLearn 2016 The results on ChaLearn 2016 are sum-\nmarized in Table 7. Our model outperforms the skeleton\nbased method (Lin et al. 2018) by around 4.5%. We also\nnotice that the accuracies of skeleton based methods are in-\nferior to video frame based models. The reasons are mainly\ntwo-fold. Firstly, the precision of OpenPose is affected by\nthe drastic background and illumination changes. Secondly,\na lot of gesture classes requires recognizing the static hand\ngesture instead of dynamic hand motion. The recognition\nperformance on these classes mainly depends on the esti-\nmation precision of hand joints. It is worth noting that our\nmodel is the simplest one. We argue that the performance\nwill improve if more accurate joints locations are provided.\nTable 7: Comparison on ChaLearn 2016 dataset.[1]: Wang et\nal. [2]: Miao et al. [3]: Narayana, Beveridge, and Draper [4]:\nLin et al. RGB, depth, optical ﬂow and skeleton are abbrevi-\nated as R, D, O and S.\nMethod Test acc.\n(%)\nModality ModelR D O S\nAMRL[1] 65.59 √ √ 8*CNN+4ConvLSTM\nASU[2] 67.71 √ √ √ 4*C3D+2*TSN+1*SVM\nFOANet[3] 82.07 √ √ √ 12*CNN\nSkeLSTM[4] 35.39 √ 1*LSTM\n3snetTTM 39.95 √ 3*FC\n8591\nMSRC-12 (Wang et al. 2016) proposed Joint Trajectory\nMaps (JIM), which are a set of 2D images that encode spa-\ntiotemporal information carried by 3D skeleton sequences in\nthree orthogonal planes. In (Jung and Hong 2014), a novel\nframework called Enhanced Sequence Matching (ESM) is\nleveraged to align and compare action sequences based on\na set of elementary Moving Poses (eMP). (Garcia-Hernando\nand Kim 2017) proposed ”transition forests”, an ensemble\nof randomized tree classiﬁers that learnt both static pose in-\nformation and temporal transitions. All these methods show\nthe importance of spatio-temporal information modelling.\nOur method extracts spatial, temporal and joint spatial-\ntemporal features and achieves the state-of-the-art accuracy\nof 99.01%, as shown in Table 8.\nTable 8: Comparison of methods on the MSRC-12 dataset.\nMethod Accuracy\nrate (%)\nJTM Wang et al. 93.12\nDFM Lehrmann, Gehler, and Nowozin 94.04\nESM Jung and Hong 96.76\nRJP Garcia-Hernando and Kim 97.54\nMP Garcia-Hernando and Kim 98.25\n3s net TTM 99.01\n6 Conclusion\nIn this paper, we ﬁrst leverage S PS, T PS and T S PS three\nPS features to explicitly represent the spatial and temporal\nmotion characteristics. In the path deﬁnition, we propose the\nAOH principle to select single joint and joint pairs, which\nensures the feature robust and compact. Furthermore, the\ndyadic method employed to extract the T PS and T S PS\nfeatures that encode global and local temporal dynamics\nwith less dimensionality. Secondly, we propose a differen-\ntiable module TTM to match the sequence key frames by\nlearning the temporal shifting parameter for each input. Fi-\nnally, we design a multi-stream FC layer based network to\ntreat spatial and temporal features separately. The ablation\nstudy has shown the effective of every contribution. We have\nachieved the best result on skeleton-based gesture recog-\nnition with high computational efﬁciency on three bench-\nmarks. We will explore the possible combination of the at-\ntention scheme and PS features.\n7 Acknowledgments\nThis work is supported by GD-NSF (2016A010101014,\n2017A030312006, 2018A030313295), the Science and\nTechnology Program of Guangzhou (2018-1002-SF-0561),\nthe National Key Research and Development Program\nof China (2016YFB1001405), the MSRA Research Col-\nlaboration Funds (FY18-Research-Theme-022), Fundamen-\ntal Research Funds for Central Universities of China\n(2017MS050). We thank Prof. Terry Lyons from University\nof Oxford and Dr. Hao Ni from UCL for their great help. We\nthank anonymous reviewers for their careful reading and in-\nsightful comments.\nReferences\nCao, C.; Zhang, Y .; Wu, Y .; Lu, H.; and Cheng, J. 2017. Egocen-\ntric gesture recognition using recurrent 3d convolutional neural net-\nworks with spatiotemporal transformer modules. InProceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition,\n3763–3771.\nChen, K.-T. 1958. Integration of paths–a faithful representation of\npaths by noncommutative formal power series. Transactions of the\nAmerican Mathematical Society89(2):395–407.\nChevyrev, I., and Kormilitzin, A. 2016. A primer on the signature\nmethod in machine learning. arXiv preprint arXiv:1603.03788.\nDe Smedt, Q.; Wannous, H.; and Vandeborre, J.-P. 2016. Skeleton-\nbased dynamic hand gesture recognition. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition\nWorkshops, 1–9.\nDu, Y .; Fu, Y .; and Wang, L. 2015. Skeleton based action recog-\nnition with convolutional neural network. In Pattern Recognition\n(ACPR), 2015 3rd IAPR Asian Conference on, 579–583. IEEE.\nDu, Y .; Wang, W.; and Wang, L. 2015. Hierarchical recurrent neu-\nral network for skeleton based action recognition. In Proceedings\nof the IEEE conference on computer vision and pattern recogni-\ntion, 1110–1118.\nEscalera, S.; Gonz`alez, J.; Bar´o, X.; Reyes, M.; Lopes, O.; Guyon,\nI.; Athitsos, V .; and Escalante, H. 2013. Multi-modal gesture\nrecognition challenge 2013: Dataset and results. In Proceedings\nof the 15th ACM on International conference on multimodal inter-\naction, 445–452. ACM.\nFernando, B.; Gavves, E.; Oramas, J. M.; Ghodrati, A.; and Tuyte-\nlaars, T. 2015. Modeling video evolution for action recognition.\nIn Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 5378–5387.\nFothergill, S.; Mentis, H.; Kohli, P.; and Nowozin, S. 2012. In-\nstructing people for training gestural interactive systems. In Pro-\nceedings of the SIGCHI Conference on Human Factors in Comput-\ning Systems, 1737–1746. ACM.\nGarcia-Hernando, G., and Kim, T.-K. 2017. Transition forests:\nLearning discriminative temporal transitions for action recognition\nand detection. InIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 432–440.\nGarrido, A. 2010. System control and rough paths (oxford mathe-\nmatical monographs).\nGyurk´o, L. G.; Lyons, T.; Kontkowski, M.; and Field, J. 2013. Ex-\ntracting information from the signature of a ﬁnancial data stream.\narXiv preprint arXiv:1307.7244.\nHambly, B., and Lyons, T. 2010. Uniqueness for the signature of\na path of bounded variation and the reduced path group. Annals of\nMathematics 109–167.\nHinton, G. E.; Srivastava, N.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. R. 2012. Improving neural networks by\npreventing co-adaptation of feature detectors. arXiv preprint\narXiv:1207.0580.\nHussein, M. E.; Torki, M.; Gowayyed, M. A.; and El-Saban, M.\n2013. Human action recognition using a temporal hierarchy of\ncovariance descriptors on 3d joint locations. In IJCAI, volume 13,\n2466–2472.\nJaderberg, M.; Simonyan, K.; Zisserman, A.; et al. 2015. Spatial\ntransformer networks. In Advances in neural information process-\ning systems, 2017–2025.\nJung, H.-J., and Hong, K.-S. 2014. Enhanced sequence matching\nfor action recognition from 3d skeletal data. In Asian Conference\non Computer Vision, 226–240. Springer.\n8592\nLai, S.; Jin, L.; and Yang, W. 2017. Toward high-performance\nonline hccr: A cnn approach with dropdistortion, path signature\nand spatial stochastic max-pooling. Pattern Recognition Letters\n89:60–66.\nLehrmann, A. M.; Gehler, P. V .; and Nowozin, S. 2014. Efﬁcient\nnonlinear markov models for human motion. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition,\n1314–1321.\nLi, C.; Zhang, X.; and Jin, L. 2017. Lpsnet: A novel log path sig-\nnature feature based hand gesture recognition framework. In Com-\nputer Vision Workshop (ICCVW), 2017 IEEE International Confer-\nence on, 631–639. IEEE.\nLin, C.; Wan, J.; Liang, Y .; and Li, S. Z. 2018. Large-scale isolated\ngesture recognition using a reﬁned fused model based on masked\nres-c3d network and skeleton lstm. In Automatic Face & Gesture\nRecognition (FG 2018), 2018 13th IEEE International Conference\non, 52–58. IEEE.\nLiu, M.; Jin, L.; and Xie, Z. 2017. Ps-lstm: Capturing essential se-\nquential online information with path signature and lstm for writer\nidentiﬁcation. In Document Analysis and Recognition (ICDAR),\n2017 14th IAPR International Conference on, volume 1, 664–669.\nIEEE.\nLyons, T.; Caruana, M.; L´evy, T.; and Picard, J. 2004. Differential\nequations driven by rough paths. Ecole d’´et´e de Probabilit´es de\nSaint-Flour XXXIV1–93.\nLyons, T.; Ni, H.; and Oberhauser, H. 2014. A feature set for\nstreams and an application to high-frequency ﬁnancial tick data.\nIn Proceedings of the 2014 International Conference on Big Data\nScience and Computing, 5. ACM.\nLyons, T. J. 1998. Differential equations driven by rough signals.\nRevista Matem´atica Iberoamericana14(2):215–310.\nMiao, Q.; Li, Y .; Ouyang, W.; Ma, Z.; Xu, X.; Shi, W.; Cao, X.; Liu,\nZ.; Chai, X.; Liu, Z.; et al. 2017. Multimodal gesture recognition\nbased on the resc3d network. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, 3047–3055.\nNarayana, P.; Beveridge, J. R.; and Draper, B. A. 2018. Gesture\nrecognition: Focus on the hands. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, 5235–5244.\nSu, B.; Ding, X.; Wang, H.; and Wu, Y . 2018. Discriminative\ndimensionality reduction for multi-dimensional sequences. IEEE\nTrans. Pattern Anal. Mach. Intell.40(1):77–91.\nTran, D.; Bourdev, L.; Fergus, R.; Torresani, L.; and Paluri, M.\n2015. Learning spatiotemporal features with 3d convolutional net-\nworks. In Proceedings of the IEEE international conference on\ncomputer vision, 4489–4497.\nVeeriah, V .; Zhuang, N.; and Qi, G.-J. 2015. Differential recurrent\nneural networks for action recognition. In Proceedings of the IEEE\ninternational conference on computer vision, 4041–4049.\nVemulapalli, R.; Arrate, F.; and Chellappa, R. 2014. Human action\nrecognition by representing 3d skeletons as points in a lie group. In\nProceedings of the IEEE conference on computer vision and pat-\ntern recognition, 588–595.\nWan, J.; Zhao, Y .; Zhou, S.; Guyon, I.; Escalera, S.; and Li, S. Z.\n2016. Chalearn looking at people rgb-d isolated and continuous\ndatasets for gesture recognition. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition Workshops,\n56–64.\nWan, J.; Escalera, S.; Anbarjafari, G.; Escalante, H. J.; Bar ´o, X.;\nGuyon, I.; Madadi, M.; Allik, J.; Gorbova, J.; Lin, C.; et al. 2017.\nResults and analysis of chalearn lap multi-modal isolated and con-\ntinuous gesture recognition, and real versus fake expressed emo-\ntions challenges. In ICCV Workshops, 3189–3197.\nWang, H., and Wang, L. 2017. Modeling temporal dynamics and\nspatial conﬁgurations of actions using two-stream recurrent neural\nnetworks. In e Conference on Computer Vision and Pa ern Recog-\nnition (CVPR).\nWang, J.; Liu, Z.; Wu, Y .; and Yuan, J. 2012. Mining actionlet\nensemble for action recognition with depth cameras. In Computer\nVision and Pattern Recognition (CVPR), 2012 IEEE Conference\non, 1290–1297. IEEE.\nWang, P.; Li, Z.; Hou, Y .; and Li, W. 2016. Action recognition\nbased on joint trajectory maps using convolutional neural networks.\nIn Proceedings of the 2016 ACM on Multimedia Conference, 102–\n106. ACM.\nWang, H.; Wang, P.; Song, Z.; and Li, W. 2017. Large-scale multi-\nmodal gesture recognition using heterogeneous networks. In Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 3129–3137.\nWang, P.; Li, W.; Ogunbona, P.; Wan, J.; and Escalera, S. 2018.\nRgb-d-based human motion recognition with deep learning: A sur-\nvey. Computer Vision and Image Understanding.\nWang, H.; Wang, W.; and Wang, L. 2015. Hierarchical motion\nevolution for action recognition. In Pattern Recognition (ACPR),\n2015 3rd IAPR Asian Conference on, 574–578. IEEE.\nWei, S.-E.; Ramakrishna, V .; Kanade, T.; and Sheikh, Y . 2016.\nConvolutional pose machines. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, 4724–4732.\nWeng, J.; Liu, M.; Jiang, X.; and Yuan, J. 2018. Deformable pose\ntraversal convolution for 3d action and gesture recognition. In Eu-\nropen Conference on Computer Vision (ECCV), volume 2, 6.\nXia, L.; Chen, C.-C.; and Aggarwal, J. K. 2012. View invariant hu-\nman action recognition using histograms of 3d joints. In Computer\nvision and pattern recognition workshops (CVPRW), 2012 IEEE\ncomputer society conference on, 20–27. IEEE.\nYang, W.; Jin, L.; Xie, Z.; and Feng, Z. 2015. Improved deep\nconvolutional neural network for online handwritten chinese char-\nacter recognition using domain-speciﬁc knowledge. arXiv preprint\narXiv:1505.07675.\nYang, W.; Jin, L.; Ni, H.; and Lyons, T. 2016. Rotation-free on-\nline handwritten character recognition using dyadic path signature\nfeatures, hanging normalization, and deep neural network. In Pat-\ntern Recognition (ICPR), 2016 23rd International Conference on,\n4083–4088. IEEE.\nYang, W.; Lyons, T.; Ni, H.; Schmid, C.; Jin, L.; and Chang, J.\n2017. Leveraging the path signature for skeleton-based human ac-\ntion recognition. arXiv preprint arXiv:1707.03993.\nYang, W.; Jin, L.; and Liu, M. 2015. Chinese character-level\nwriter identiﬁcation using path signature feature, dropstroke and\ndeep cnn. In Document Analysis and Recognition (ICDAR), 2015\n13th International Conference on, 546–550. IEEE.\nYang, W.; Jin, L.; and Liu, M. 2016. Deepwriterid: An end-to-end\nonline text-independent writer identiﬁcation system. IEEE Intelli-\ngent Systems31(2):45–53.\nZhu, W.; Lan, C.; Xing, J.; Zeng, W.; Li, Y .; Shen, L.; Xie, X.;\net al. 2016. Co-occurrence feature learning for skeleton based\naction recognition using regularized deep lstm networks. In AAAI,\nvolume 2, 6.\n8593",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7697069644927979
    },
    {
      "name": "Discriminative model",
      "score": 0.6513926982879639
    },
    {
      "name": "Artificial intelligence",
      "score": 0.61924147605896
    },
    {
      "name": "Gesture",
      "score": 0.6038127541542053
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5895603895187378
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5630630850791931
    },
    {
      "name": "ENCODE",
      "score": 0.5062726736068726
    },
    {
      "name": "Gesture recognition",
      "score": 0.4818481206893921
    },
    {
      "name": "Feature learning",
      "score": 0.45166635513305664
    },
    {
      "name": "Speech recognition",
      "score": 0.33756789565086365
    },
    {
      "name": "Computer vision",
      "score": 0.33175814151763916
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ]
}