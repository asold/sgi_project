{
    "title": "How to Protect Copyright Data in Optimization of Large Language Models?",
    "url": "https://openalex.org/W4393160370",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2314141849",
            "name": "Timothy Chu",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2099261842",
            "name": "Zhao Song",
            "affiliations": [
                "Adobe Systems (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2484182397",
            "name": "Chi-Wun Yang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2314141849",
            "name": "Timothy Chu",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2099261842",
            "name": "Zhao Song",
            "affiliations": [
                "Adobe Systems (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2484182397",
            "name": "Chi-Wun Yang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2899748887",
        "https://openalex.org/W2898324627",
        "https://openalex.org/W4310285840",
        "https://openalex.org/W6843350042",
        "https://openalex.org/W2946840143",
        "https://openalex.org/W3214693004",
        "https://openalex.org/W3128934904",
        "https://openalex.org/W3157428307",
        "https://openalex.org/W1859580884",
        "https://openalex.org/W4226014375",
        "https://openalex.org/W4296567394",
        "https://openalex.org/W2620857596",
        "https://openalex.org/W3169042981",
        "https://openalex.org/W3087124347",
        "https://openalex.org/W2886067286",
        "https://openalex.org/W4283701832",
        "https://openalex.org/W4281664107",
        "https://openalex.org/W3021189130",
        "https://openalex.org/W2920497684",
        "https://openalex.org/W3025993830",
        "https://openalex.org/W3021293129",
        "https://openalex.org/W4380136302",
        "https://openalex.org/W2945554113",
        "https://openalex.org/W3006670224",
        "https://openalex.org/W2950987997",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4383180694",
        "https://openalex.org/W4206178588",
        "https://openalex.org/W4318149317",
        "https://openalex.org/W4386047807",
        "https://openalex.org/W4385571189",
        "https://openalex.org/W4389520123",
        "https://openalex.org/W2948046738",
        "https://openalex.org/W4362679267",
        "https://openalex.org/W4386081455",
        "https://openalex.org/W4378770452",
        "https://openalex.org/W4384390036",
        "https://openalex.org/W4378771755",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W2949804919",
        "https://openalex.org/W4378509498",
        "https://openalex.org/W3036165826",
        "https://openalex.org/W4379540180",
        "https://openalex.org/W4385473649",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W3093860203",
        "https://openalex.org/W4366735484",
        "https://openalex.org/W4289436733",
        "https://openalex.org/W4321593464",
        "https://openalex.org/W3103682594",
        "https://openalex.org/W4361806691",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4387995158",
        "https://openalex.org/W3108032709",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4295838474",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4318621194",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4377121468",
        "https://openalex.org/W2947461788",
        "https://openalex.org/W4375959325",
        "https://openalex.org/W4381253585",
        "https://openalex.org/W4384648039",
        "https://openalex.org/W4386148634",
        "https://openalex.org/W4387427772",
        "https://openalex.org/W4313303565",
        "https://openalex.org/W2975935587",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W4377864415",
        "https://openalex.org/W4384644215",
        "https://openalex.org/W3098047114",
        "https://openalex.org/W4322716742",
        "https://openalex.org/W4361230827",
        "https://openalex.org/W4385572321",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4385474090",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4200630731",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function. In this paper, we observe that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.",
    "full_text": "How to Protect Copyright Data in Optimization of Large Language Models?\nTimothy Chu1, Zhao Song 2, Chiwun Yang 3\n1 Google, Mountain View, CA\n2Adobe Research, San Jose, CA\n3 Sun Yat-sen University, China\nhungryTOAN@gmail.com, zsong@adobe.com, christiannyang37@gmail.com\nAbstract\nLarge language models (LLMs) and generative AI have\nplayed a transformative role in computer research and ap-\nplications. Controversy has arisen as to whether these mod-\nels output copyrighted data, which can occur if the data the\nmodels are trained on is copyrighted. LLMs are built on the\ntransformer neural network architecture, which in turn relies\non a mathematical computation called Attention that uses the\nsoftmax function.\nIn this paper, we observe that large language model train-\ning and optimization can be seen as a softmax regression\nproblem. We then establish a method of efﬁciently perform-\ning softmax regression, in a way that prevents the regression\nfunction from generating copyright data. This establishes a\ntheoretical method of training large language models in a way\nthat avoids generating copyright data.\n1 Introduction\nLarge language models have changed the world, with the\nrise of generative AI models such as ChatGPT, GPT-4,\nLlama, BERT, BARD, PaLM, and OPT (ChatGPT 2022;\nBubeck et al. 2023; Devlin et al. 2018; Touvron et al.\n2023b,a; BARD 2023; Chowdhery et al. 2022; Anil et al.\n2023; Zhang et al. 2022). These models are able to pro-\ncess natural language effectively, handling a wide range\nof tasks including story generation, code creation, machine\ntranslation, and elementary mathematical problem solving\n(Brown et al. 2020; Svyatkovskiy et al. 2020; Wu et al.\n2016; Wei et al. 2022). One core component in the large\nlanguage model is the transformer architecture (Vaswani\net al. 2017), which is built on a computational step known\nas attention. Transformers have been used in a wide variety\nof tasks outside of large language models, including gener-\native image systems such as DALL-E (Research 2021) and\nDALL-E2 (Research 2022). Recent research has integrated\nthe transformer architecture with scalable diffusion-based\nimage generation models (Bao et al. 2023; Cao et al. 2022;\nWu et al. 2023a; Han et al. 2022; Dosovitskiy et al. 2020).\nOnce challenge in generative AI is guaranteeing that out-\nputs are protected from copyright infringement and intellec-\ntual property issues (Hattenbach and Glucoft 2015; Hristov\n2016; Sag 2018; Gillotte 2019; Vyas, Kakade, and Barak\nCopyright © 2024, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n2023). Generative models trained on large corpora of data\ncan inadvertently generate outputs that are direct copies, or\nclose variants, of copyrighted text or images that the model\nis trained on. Removing copyrighted material from training\nmay also be undesirable: while one can achieve good perfor-\nmance without using copyrighted data, the inclusion of such\ndata can signiﬁcantly enhance the performance of generative\nAI models. For example, incorporating literary works, which\nare often copyrighted, into the training dataset of a language\nmodel can enhance performance. Copyright infringement is-\nsues regarding outputs of generative AI have led to contro-\nversy in using it, and past researchers have considered mod-\nels and theoretical frameworks for evaluating whether gen-\nerative models are copying data, and how to evaluate and\navoid copyright issues that arise (Vyas, Kakade, and Barak\n2023).\nOur paper has two main contributions:\n1. We provide an approach for solving general regression\nproblems in a way that avoids generating copyright data.\nWe term this approach copyright regression.\n2. We show how to protect copyright data in the optimiza-\ntion and training of transformer-based architectures (in-\ncluding most large language models), by ﬁnding an algo-\nrithm to solve copyright regression for the softmax func-\ntion.\nSolving the copyright regression problem for the softmax\nfunction is the key technical contribution of our paper. To\nestablish the copyright regression framework, we provide a\nnew optimization objective for a general regression problem\nwhere some outputs are copyrighted. Such a case can arise\nwhen regression outputs are images or sentences, which oc-\ncurs in transformer-based architectures for language genera-\ntion and image generation. We also review literature linking\nthe softmax regression problem to the training of transform-\ners.\nTo ﬁnd an algorithm for solving copyright regression for\nthe softmax function, we used an approach based on con-\nvex optimization and gradient descent. We show that the ob-\njective function of the softmax copyright regression is con-\nvex, and that its Hessian is bounded. Showing this convexity\nis non-trivial, and requires intricate bounding of key matrix\nand vector quantities that arise in the softmax copyright re-\ngression problem. Establishing convexity and the bounded\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17871\nHessian property of the objective function in softmax copy-\nright regression allows us to use gradient-based methods to\nefﬁciently solve this problem, with guaranteed bounds on\nconvergence and good stability properties.\nThe code of experiments in this paper is open-\nsourced at https://github.com/ChristianYang37/chiwun/tree/\nmain/src/Copyright-Regression. For the proofs of all lem-\nmas and theorems in this paper, please refer to (Chu, Song,\nand Yang 2023b).\n2 Related Work\nThis section brieﬂy reviews the related research work on pri-\nvacy and security of AI, theoretical large language model\nwork, and optimization of neural networks. These topics\nhave a close connection to our work.\nPrivacy and Security. Generative AI has achieved impres-\nsive results in various domains, including images, text, and\ncode. However, preventing copyright infringement is a chal-\nlenge that needs to be addressed (Hattenbach and Glu-\ncoft 2015; Hristov 2016; Sag 2018; Gillotte 2019). (Sag\n2018) discusses whether data mining and machine learning\non copyrighted text qualify as ”fair use” under U.S. law.\n(Gillotte 2019) investigates copyright infringement in AI-\ngenerated artwork and argues that using copyrighted works\nduring the training phase of AI programs does not result\nin infringement liability. To mitigate the potential harms of\nlarge language models, in (Kirchenbauer et al. 2023), a wa-\ntermarking framework is introduced that facilitates the em-\nbedding of signals within the generated text. This frame-\nwork aims to enhance the detection of output from Lan-\nguage Model (LLM) systems, thereby mitigating potential\nmisuse or abuse. Building upon this foundation, subsequent\nresearch (He et al. 2022a,b) has contributed to the devel-\nopment of more robust and less intrusive watermark em-\nbedding algorithms. These advancements seek to improve\nthe stability and minimize any adverse effects associated\nwith the process of embedding watermarks. Such endeav-\nors are important in ensuring the integrity and responsible\nutilization of LLM technology. (Vyas, Kakade, and Barak\n2023) proposes a framework that provides stronger protec-\ntion against sampling protected content, by deﬁning near\naccess-freeness (NAF) and developing generative model\nlearning algorithms. Experiments demonstrate promising re-\nsults with some impact on output quality for both language\nand image generative models. Recently, (Gao, Song, and\nYang 2023) focuses on this issue of sampling protected con-\ntent, and proposes a provable method for privately comput-\ning the attention matrix using differential privacy. (Xu et al.\n2023) trains language models (LMs) with federated learning\n(FL) and differential privacy (DP) in the Google Keyboard\n(Gboard).\nTheoretical LLM. Since the explosion of large language\nmodels, theoretical research on transformer has been one\nmajor component of improving language model perfor-\nmance (Kitaev, Kaiser, and Levskaya 2020; Chen et al. 2020;\nTay et al. 2020; Noci et al. 2022; Deng, Li, and Song 2023;\nPanigrahi et al. 2023; Arora and Goyal 2023; Sun et al. 2023;\nSanford, Hsu, and Telgarsky 2023; Jiang, Ren, and Lin\n2023; Alman and Song 2023; Brand, Song, and Zhou 2023;\nZelikman et al. 2023; Malladi et al. 2023; Liu et al. 2023a;\nRafailov et al. 2023; Ignat et al. 2023; Gao, Song, and Yin\n2023; Zhao et al. 2023; Deng et al. 2023; Gao et al. 2023;\nWu et al. 2023b; Liu et al. 2023b). (R ¨uckl´e et al. 2020) pro-\nposes AdapterDrop, a method that removes adapters from\nlower transformer layers during training and inference to re-\nduce computational overhead, while still maintaining task\nperformance. (Tay et al. 2021) shows that random align-\nment matrices perform competitively and learning attention\nweights from token-token interactions is not highly signiﬁ-\ncant. So they propose Synthesizer, a model that learns syn-\nthetic attention weights without token-token interactions and\nperforms well in various tasks. (Chen et al. 2021) proposes\nScatterbrain, a way to balance model quality and efﬁciency\nin approximating long sequences. Recent work (Arora and\nGoyal 2023) explores the emergence of new skills in lan-\nguage models through scaling up their parameters and train-\ning data. This demonstrates through mathematical analysis\nthat the Scaling Laws provide a strong inductive bias, en-\nabling efﬁcient learning in pre-trained models. they term this\nphenomenon ”slingshot generalization,” as it seems to vio-\nlate traditional generalization theory.\nOptimization and Convergence of Deep Neural Net-\nworks. Prior research (Li and Liang 2018; Du et al. 2018;\nAllen-Zhu, Li, and Song 2019a,b; Song and Yang 2019; Cai\net al. 2019; Zhang, Martens, and Grosse 2019; Cao and Gu\n2019; Zou and Gu 2019; Oymak and Soltanolkotabi 2020;\nJi and Telgarsky 2019; Lee et al. 2020; Huang et al. 2021;\nZhang et al. 2020b; Brand et al. 2020; Zhang et al. 2020a;\nSong, Zhang, and Zhang 2021; Alman et al. 2023; Munteanu\net al. 2022; Zhang 2022; Gao, Mahadevan, and Song 2023;\nLi, Song, and Zhou 2023; Qin, Song, and Yang 2023) on the\noptimization and convergence of deep neural networks has\nbeen crucial in understanding their exceptional performance\nacross various tasks. These studies have also contributed to\nenhancing the safety and efﬁciency of AI systems. In (Gao,\nMahadevan, and Song 2023) they deﬁne a neural function\nusing an exponential activation function and apply the gradi-\nent descent algorithm to ﬁnd optimal weights. In (Li, Song,\nand Zhou 2023), they focus on the exponential regression\nproblem inspired by the attention mechanism in large lan-\nguage models. They address the non-convex nature of stan-\ndard exponential regression by considering a regularization\nversion that is convex. They propose an algorithm that lever-\nages input sparsity to achieve efﬁcient computation. The al-\ngorithm has a logarithmic number of iterations and requires\nnearly linear time per iteration, making use of the sparsity of\nthe input matrix.\n3 Preliminaries\nIn this section, we present preliminary concepts and deﬁ-\nnitions for our paper. We begin by introducing the notations\nwe utilize in Section 3.1. In Section 3.2 we provide the prob-\nlem deﬁnition that we aim to solve.\n3.1 Notations\nWe use the following notations and deﬁnitions: The `p\nnorm of a vector x is denoted as kxkp, for examples,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17872\nkxk1 := Pn\ni=1 |xi|, kxk2 := ( Pn\ni=1\nx2\ni )1/2 and kxk1 :=\nmaxi2[n] |xi|. For a vector x 2 Rn, exp(x) 2 Rn denotes\na vector where whose i-th entry is exp(xi) for all i 2 [n].\nFor n>k , for any matrix A 2 Rn⇥k, we denote the spec-\ntral norm of A by kAk, i.e., kAk := sup x2Rk kAxk2/kxk2.\nWe denote \u0000min(A) as the minimum singular value of A.\nFor two vectors x, y 2 Rn, we denote hx, yi = Pn\ni=1 for\ni 2 [n]. Given two vectors x, y 2 Rn, we denote x \u0000 y as a\nvector whose i-th entry is xiyi for all i 2 [n]. Let x 2 Rn\nbe a vector. For a vector x 2 Rn, diag(x) 2 Rn⇥n is de-\nﬁned as a diagonal matrix with its diagonal entries given by\ndiag(x)i,i = xi for i =1 ,. . . ,n, and all off-diagonal en-\ntries are 0. A symmetric matrix A 2 Rn⇥n is said to be\npositive deﬁnite (PD) when A \u0000 0, for all non-zero vectors\nx 2 Rn, we have x>Ax > 0. Similarly, a symmetric matrix\nA 2 Rn⇥n is said to be positive semideﬁnite (PSD) when\nA ⌫ 0, for all vectors x 2 Rn, we have x>Ax \u0000 0.\n3.2 Problem Deﬁnition\nTo achieve a successful copyright infringement claim in\nthe United States and many other jurisdictions, the plaintiff\nmust provide evidence that demonstrates two key elements.\nFirstly, they must establish that the defendant had access to\nthe plaintiff’s copyrighted work. Secondly, they must show\nthat there are substantial similarities between the defendant’s\nwork and the original elements of the plaintiff’s work (for\nthe 9th circuits 2022).\nWhile access to high-quality copyrighted data is essen-\ntial for enhancing the performance of AI models, it also in-\ntroduces legal risks. Therefore, when considering the safety\nand legality of AI systems, it is imperative to ensure that\nthe ideal language model can effectively learn from all data\nwithout producing output that resembles copyrighted mate-\nrial present in its training set. By adhering to these consid-\nerations, we can maintain both the integrity of intellectual\nproperty rights and the lawful operation of AI technologies.\nFor convenience, we denote training dataset D, copyright\ndata C⇢ D and other data O = D\u0000 C. Our objective is to\nensure a model f, satisﬁes: for any input x, given a metric L,\nthe model’s output f(x) will not exhibit substantial similar-\nity to any copyrighted content present in its training set. We\nenforce this by deﬁning a strict gap ⌧ such that the metric\nL(f(x),C ), where C 2C , is greater than or equal to ⌧ plus\nthe metric L(f(x),O ), where O 2O . That is\nL(f(x),C ) \u0000 ⌧ + L(f(x),O ).\nThe choice of metric L depends on the speciﬁc task, such as\nCross-Entropy loss for text generation, mean absolute error\nor mean square error for regression problems, and Kullback-\nLeibler divergence or image similarity for image generation,\netc.\nTo ensure compliance with copyright laws, we apply ⌧\nto the average metric L calculated over both C and O, thus\nimplementing a formal and conservative deﬁnition. And we\nconvert dataset D to an input matrix A 2 R\nn⇥d and a target\nvector b 2 Rn, where n is the size of the dataset, d is the\ndimension of input data. We now provide the deﬁnition of\nproblem below.\nDeﬁnition 1 (⌧-Copyright-Protected). Given matrix A 2\nR\nn⇥d and vector b 2 Rn that A =\n\nA1\nA2\n\u0000\n, and b =\n\nb1\nb2\n\u0000\n,\nwhere A1 2 Rn1⇥d, A2 2 Rn2⇥d, b1 2 Rn1, b2 2 Rn2 and\nn = n1 + n2. A1, b1 are the data has copyright issue and\nA2, b2 are the data does not have copyright issue. Denote\nthe training objective L. Denote ⌧> 0 as a scalar.\nIf there is a trained model f✓ with parameter ✓ that satis-\nﬁes L(f✓(A1),b1)\nn1\n\u0000 ⌧ + L(f✓(A2),b2)\nn2\nthen we say this model\nf✓ is ⌧-Copyright-Protected.\n4 Methodology: Copyright Regression\nA prominent existing approach, as outlined in the work by\n(Vyas, Kakade, and Barak 2023), introduces an algorithm\nthat involves training an additional generative model, de-\nnoted as p, using non-copyrighted data. This algorithm em-\nploys rejection sampling to effectively manage the probabil-\nity of the model generating copyrighted data. However, it is\nimportant to note that this method has certain limitations.\nSpeciﬁcally, it incurs higher computational costs during the\ndecoding process and necessitates the retraining of an addi-\ntional model. Now we introduce our method, a simple mod-\niﬁcation to the standard training objective of generative lan-\nguage models to ensure that their outputs do not infringe\nupon copyright laws.\nIn accordance with the ﬁndings presented in (Deng, Li,\nand Song 2023), our approach involves decomposing the\nmechanism of Attention (Vaswani et al. 2017), into a re-\ngression problem termed Softmax Regression. This decom-\nposition enables a deeper examination of the learning pro-\ncess underlying attention training. By adopting this method,\nwe gain valuable insights into the intricacies of attention and\nits associated learning mechanisms.\nWe propose a modiﬁcation to the standard training objec-\ntive of generative language models based on the principles\nof Softmax Regression. The objective is to train the model\nto generate desired outputs, denoted as f(A)= b. However,\nin the case of copyrighted data, represented by A\n1 2 Rn1⇥d\nand b1 2 Rn1, we aim to prevent the model from learn-\ning to generate these speciﬁc outputs. To address this con-\ncern, we introduce an additional term L(f(A\n1),b 1)\u00001 to\nthe training objective to discourage the model from gen-\nerating outputs matching the copyrighted data. To control\nthe level of this protection, we introduce a scalar coefﬁcient\n\u0000\nc > 0. Consequently, the modiﬁed training objective be-\ncomes L(f(A),b )+ \u0000cL(f(A1),b 1)\u00001. This modiﬁcation\nserves to strike a balance between achieving the desired\noutputs and avoiding the generation of copyrighted data.\nThe addition of the inverse term in the training objective\nhelps mitigate the model’s tendency to generate prohibited\noutputs, while the coefﬁcient \u0000\nc allows for ﬁne-tuning the\nlevel of protection. Compared to (Vyas, Kakade, and Barak\n2023), our approach does not necessitate training additional\nmodels and impacts the generation speed of the model dur-\ning decoding. It offers a simple and practical method that\ncan be plug-and-play applied to all training objectives and\nalgorithms in attention-based models, to prevent the outputs\nof models from outputting copyrighted data.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17873\nIn Section 4.1, we present the deﬁnition of Softmax Re-\ngression. In Section 4.2, we present the deﬁnition of Copy-\nright Regression. In Section 4.3, we present the regulariza-\ntion of parameters for better optimization.\n4.1 Softmax Regression\nIn (Deng, Li, and Song 2023), Softmax Regression applies\na softmax function, denoted as f, to the product of the in-\nput matrix A and the parameter vector x. The training ob-\njective is then deﬁned as minimizing the squared Euclidean\ndistance between f(x) and the target vector b, represented as\nhf(x) \u0000 b, f(x) \u0000 bi. By optimizing this objective, Softmax\nRegression aims to gain insights into the learning process of\nthe attention mechanism.\nWe deﬁne Softmax Regression as follows\nDeﬁnition 2 (Softmax Regression in (Deng, Li, and Song\n2023)). Given a matrix A 2 R\nn⇥d, we deﬁne\nf(x): =h exp(Ax), 1ni\u00001 exp(Ax)\nFor the convenience of calculation, we deﬁne a interme-\ndiate operator c(x) as follows\nDeﬁnition 3. Given a matrix A 2 Rn⇥d and a vector b 2\nRn, let f(x) be deﬁned as Deﬁnition 2, we deﬁne c(x): =\nf(x) \u0000 b.\nWe deﬁne the training objective of Softmax Regression as\nfollows\nDeﬁnition 4 (Training Objective of Softmax Regression in\n(Deng, Li, and Song 2023)). Given matrix A 2 Rn⇥d and\nvector b 2 Rn, let c(x) be deﬁned as Deﬁnition 3, we deﬁne\n`(x)=h c(x),c (x)i.\nFor more details to spell out the equivalence of Softmax\nregression and the training of transformers, please refer to\nSection 3 of (Chu, Song, and Yang 2023a).\n4.2 Copyright Regression\nGiven a matrix A 2 Rn⇥d and a vector b 2 Rn that A =\nA1\nA2\n\u0000\n, and b =\n\nb1\nb2\n\u0000\n, where A1 2 Rn1⇥d, A2 2 Rn2⇥d,\nb1 2 Rn1, b2 2 Rn2 and n = n1 + n2. A1, b1 are the data\nthat has copyright issue and A2, b2 are the data does not have\ncopyright issue. Now to distinguish between train objective\nof A\n1, b1 and A2, b2, we follow what we did in Section 4.1.\nWe ﬁrst provide the deﬁnition of Softmax Regression func-\ntion on Copyright Data as follows\nDeﬁnition 5 (Softmax Regression function on Copyrighted\nData). Given all data matrix A 2 R\nn⇥d and copyrighted\ndata matrix A1 2 Rn1⇥d, we deﬁne\nf1(x): =h exp(Ai,⇤x), 1ni\u00001 exp(Ax)\nwhere i 2 [1,n 1] denote a integer.\nAlso, we provide the deﬁnition of the intermediate opera-\ntor c(x) as follows\nDeﬁnition 6. Given all data matrix A 2 Rn⇥d and copy-\nrighted data matrix A1 2 Rn1⇥d and vector b1 2 Rn,\nlet f1(x) be deﬁned as Deﬁnition 5, we deﬁne c1(x): =\nf1(x) \u0000 b1.\nNow we have ofﬁcially provided our deﬁnition of Copy-\nright Regression below, which can prevent language mod-\nels from infringing copyright with controllable performance\ndamage and without occupying more resources.\nDeﬁnition 7. We denote `(x) as Deﬁnition 4. The function\nc\n1(x) is deﬁned as Deﬁnition 6, and we denote `1(x)=\nhc1(x),c 1(x)i and `2(x): =` (x)\u0000`1(x). Let \u0000c > 0 denote\na parameter that controls loss related to copyright data.\nWe consider the following copyright loss\nLcopyright(x): =0 .5`1(x)+\u0000 c · `1(x)\u00001 +0 .5`2(x)\nAdditionally, by adjusting the value of \u0000c, one can easily\ncontrol the learning of copyrighted data within the model.\nThis ﬂexibility allows for a more effective and data-sensitive\napproach to training language models.\n4.3 Regularization\nTo make sure the stability during training, we add a regular-\nization term on L\ncopright(x). We deﬁne Lreg as follows\nDeﬁnition 8. Given a matrix A 2 Rn⇥d. Given a vector\nw 2 Rn, we deﬁne W = diag( w). We deﬁne Lreg : Rd !\nR as follows Lreg := 0 .5kW Axk2\n2.\nAfter adding the regularization term, we deﬁne our ﬁnal\nobjective L as follows\nDeﬁnition 9. We denote Lcopyright(x) as Deﬁnition 7, let\nLreg be deﬁned as Deﬁnition 8, then we deﬁne L :=\nLcopyright(x)+L reg.\nMinimizing L is the softmax regression on copyrighted\ndata problem.\n5 Optimization Properties of Objective\nFunction L\nThe main contribution of this section involves addressing the\nconvexity of the objective function L, which allows for more\nefﬁcient and reliable optimization of L. This achievement\nnot only enables us to optimize the objective more effec-\ntively but also validates the feasibility of utilizing Copyright\nRegression for achieving convergence in LLM (Language\nModel) training. For instance, we can leverage popular op-\ntimization algorithms such as gradient descent, Newton’s\nmethod, and their variants to solve the optimization problem\nefﬁciently (see Section 8 in (Deng, Li, and Song 2023)).\nIn Section 5.1, we compute the gradient and hessian of our\ntrain objective. In Section 5.2, we show our result that the\nHessian of our train objective is Positive Deﬁnite. In Sec-\ntion 5.3, we show our result that the Hessian of our train\nobjective is Lipschitz. Thus, we can say our train objective\nL is convex.\n5.1 Gradient and Hessian of L\nIn order to calculate the convergence and optimization of L,\nwe ﬁrst compute the rL and r2L. We show our results as\nfollows\nLemma 10 (Gradient of L). Given matrix A 2 Rn⇥d that\nA =[ A>\n1 A>\n2\n]>, where A1,A 2 2 Rn2⇥d and n = n1 + n2.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17874\nAlso, we are given a vector b 2 Rn with b =[ b>\n1 b>\n2\n]>,\nwhere b1,b 2 2 Rn2.\nWe denote `1(x) and `2(x) as Deﬁnition 7, denote L\nas Deﬁnition 9, denote f(x) as Deﬁnition 2, denote c(x)\nas Deﬁnition 3. Give a vector w 2 Rn, we deﬁne W =\ndiag(w).\nWe have\ndL\ndx = A>\n⇤,i(\u0000f(x)c(x)>f(x) + diag( f(x))c(x))\n+2 \u0000c`1(x)\u00002 · A1\n>\n⇤,i(f1(x)c1(x)>f1(x)\n\u0000 diag(f1(x))c1(x)) + A>W2Ax\nwhere i 2 [1,n ] denote a integer.\nFor convenient, we deﬁne B(x) and Bc(x) (B(x) func-\ntion on copyrighted data)\nDeﬁnition 11 (Deﬁnition 6.1 in (Deng, Li, and Song 2023)).\nGiven matrix A 2 Rn⇥d and vector b 2 Rn that A =\n\nA1\nA2\n\u0000\n,\nand b =\n\nb1\nb2\n\u0000\n, where A1 2 Rn1⇥d, A2 2 Rn2⇥d, b1 2 Rn1,\nb2 2 Rn2 and n = n1+n2. A1, b1 are the data has copyright\nissue and A2, b2 are the data does not have copyright issue.\nDenote f(x) as Deﬁnition 2, denote c(x) as Deﬁnition 3,\ndenote f1(x) as Deﬁnition 5, denote c1(x) as Deﬁnition 6.\nWe deﬁne B(x) as follows\nB(x)= h3f(x) \u0000 2b, f(x)i·f (x)f(x)>\n+ hf(x) \u0000 b, f(x)i·diag( f(x))\n+ diag((2f (x) \u0000 b) \u0000 f(x))\n+( b \u0000 f(x)) · f(x)> + f(x) · (b \u0000 f(x))>\nand then we also deﬁne Bc(x) as follows\nBc(x)= h3f1(x) \u0000 2b1,f 1(x)i·f 1(x)f1(x)>\n+ hf1(x) \u0000 b1,f 1(x)i·diag( f1(x))\n+ diag((2f1(x) \u0000 b1) \u0000 f1(x))\n+( b1 \u0000 f1(x)) · f1(x)> + f1(x) · (b1 \u0000 f1(x))>\nWith B(x) and Bc(x), we can abbreviate our compute\nresult of Hessian of L as follows\nLemma 12 (Hessian of L). Given matrix A 2 Rn⇥d that\nA =\n\nA1\nA2\n\u0000\n, where A1,A 2 2 Rn2⇥d and n = n1 + n2.\nAlso, we are given a vector b 2 Rn with b =\n\nb1\nb2\n\u0000\n, where\nb1,b 2 2 Rn2.\nDenote `1(x) and `2(x) as Deﬁnition 7, denote L as Deﬁ-\nnition 9, denote f(x) as Deﬁnition 2, denote c(x) as Deﬁni-\ntion 3, denote B(x) and Bc(x) be deﬁned as Deﬁnition 11.\nGiven a vector w 2 Rn, we deﬁne W = diag(w ).\nWe have\nd2L\ndxidxi\n= A>\n⇤,iB(x)A>\n⇤,i\n+ A>W2A\n+2 \u0000c`1(x)\u00002(16 · `1(x)\u00001(A1\n>\n⇤,i(\u0000f1(x)c1(x)>f1(x)\n+ diag(f1(x))c1(x)))2 \u0000 A1\n>\n⇤,i\nBc(x)A1\n>\n⇤,i\n)\nwhere i 2 [0,n ] denote a integer.\nAnd we also have\nd2L\ndxidxj\n= A>\n⇤,iB(x)A>\n⇤,j\n+ A>W2A\n+2 \u0000c`1(x)\u00002(16 · `1(x)\u00001A1\n>\n⇤,i(\u0000f1(x)c1(x)>f1(x)\n+ diag(f1(x))c1(x)) · A1\n>\n⇤,j\n(\u0000f1(x)c1(x)>f1(x)\n+ diag(f1(x))c1(x)) \u0000 A1\n>\n⇤,i\nBc(x)A1\n>\n⇤,j\n)\nwhere i, j 2 [1,n ] denote two integers, i 6= j.\n5.2 Hessian of L is Positive Deﬁnite\nAfter computing the Hessian of L, we now show our result\nthat can conﬁrm it is positive deﬁnite, which implies that\nr2L \u0000 0. Therefore, we have strong evidence that L satis-\nﬁes the condition of convexity, which is a desirable property\nfor optimization purposes.\nLemma 13 (Hessian is positive deﬁnite). Given matrix A 2\nR\nn⇥d and vector b 2 Rn. Denote \u0000 2 (0, 1) a scalar. Given\na vector w, denote W = diag(w ) 2 Rn⇥n. We deﬁne w2\ni,i\nas the i-th diagonal entry of matrix W2 2 Rn⇥n. Let l> 0\ndenote a scalar.\nIf for all i 2 [n], w2\ni \u0000 8 + 200 \u0000c\u0000\u00003 + l/\u0000min(A)2, we\nhave r2L ⌫ l · Id\n5.3 Hessian of L is Lipschitz\nWe now show our result that conﬁrms the Hessian of L is\nLipschitz, which is a desirable property in optimization. This\nindicates that the second derivatives of L change smoothly\nwithin a deﬁned range. By leveraging this Lipschitz prop-\nerty, we can employ gradient-based methods with guar-\nanteed convergence rates and improved stability. Overall,\nthis ﬁnding validates the feasibility of utilizing Copyright\nRegression for achieving convergence in LLM (Language\nModel) training.\nLemma 14 (Hessian is Lipschitz-continuous). Denote R \u0000\n4 denote a scalar. Given a matrix A 2 R\nn⇥d and a vector\nb 2 Rn, kAk R, kbk2  1. Given x, y 2 Rd be two\nvector parameter for Copyright Regression with conditions\nkxk2  R, kyk2  R and kA(x \u0000 y)k1  0.01. Let L\nbe deﬁned as Deﬁnition 9, let \u0000 2 (0, 1), let \u0000 2 (0, 0.1).\nDenote H(x): =r 2L(x). Then,\nkH(x) \u0000 H(y)k\n (13344\u0000c + 2)\u0000\u00004\u0000\u00002n1.5 exp(40R2)kx \u0000 yk2\n6 Optimization and Copyright Protection\nGuarantees\nWe have already established the convexity of the training ob-\njective L in Section 5, providing a strong foundation to con-\nﬁdently pursue the global optimal value of L through opti-\nmization techniques. Now we present the main results of this\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17875\npaper: 1) the minimization guarantee of L, 2) the copyright\nprotection efﬁciency of Copyright Regression.\nFirstly, in Section 6.1, our objective is to minimize L to its\noptimal value, ensuring that we achieve the most favorable\noutcome in terms of our training process. The minimization\nguarantee of L conﬁrms our main result on optimization of\nCopyright Regression, it also demonstrates the ease of use\nof Copyright Regression, which can be optimized on any\nattention-based model. At the same time, denote x\n⇤ as the\noptimal solution of training objective L, analyzing L(x⇤)’s\nperformance on copyright data can help us to understand\nhow the trained Copyright Regression can avoid copyright\ninfringement.\nSecondly, in Section 6.2, we aim to demonstrate that the\noptimal L provides robust protection for its outputs, safe-\nguarding them from potential copyright infringement. By\ndelineating this boundary, we can quantitatively assess the\nextent to which Copyright Regression preserves the integrity\nand exclusivity of copyrighted content. This analysis will\nprovide valuable insights into the effectiveness of our ap-\nproach and its ability to strike a balance between data pro-\ntection and the need for authorized access.\n6.1 Minimizing Loss Guarantee\nWe provide our minimum training objective theorem below.\nTheorem 15 (Minimizing training objective L). Suppose we\nhave matrix A 2 Rn⇥d and A1 2 Rn1⇥d, n1  n, vec-\ntor b, w 2 Rn. Let L be deﬁned as Deﬁnition 9, denote\nx⇤ as the optimal solution of L where g(x⇤)=0 d and\nkx⇤k R. Denote R \u0000 10 be a positive scalar. Denote\nM = n1.5 exp(60R2), Let x0 be denoted as an initial point\nwhere Mkx0 \u0000 x⇤k2  0.1l, where l> 0 denoted a scalar.\nFor any accuracy ✏ 2 (0, 0.1) and any failure probabil-\nity \u0000 2 (0, 0.1), there exists a randomized algorithm, with\nprobability 1 \u0000 \u0000, it runs T = log(kx 0 \u0000 x⇤k2/✏) iteration\nand outputs a vector ex 2 Rd such that kex\u0000x⇤k✏ and the\ntime cost of each iteration is\nO((nnz(A)+d w) · poly(log(n/\u0000)))\nHere w is the exponent of matrix multiplication. Currently\nw ⇡ 2.373.\n6.2 L is ⌧c-Copyright-Protected\nNow we provide a boundary that illustrates the efﬁcacy\nof Copyright Regression in safeguarding copyrighted data,\nwhile also addressing the criteria outlined in Deﬁnition 1,\nwhich serves as our deﬁnition of copyright protection in this\npaper.\nWe set `(x) in Deﬁnition 4 as a `\n2 metric for measur-\ning parameter x on learning data A. Now we present our re-\nsult to conﬁrm that training using our Copyright Regression\nmethod can ensure that the model’s outputs do not infringe\ncopyright. Speciﬁcally, we can assert that the trained model\nL is protected against copyright infringement with a thresh-\nold of ⌧\nc based on Theorem 16 below.\nTheorem 16. Let x⇤ be denoted the optimal parameter on\nCopyright Regression. We deﬁne `(x) as Deﬁnition 4, denote\n`(x) as the original train objective of Softmax Regression.\nDenote ✏2 2 (0, 0.1) a scalar. Denote ⌧c := p2\u0000c/n1 \u0000\n✏2/n2, we have `1(x⇤)\nn1\n\u0000 ⌧c + `2(x⇤)\nn2\n, so x⇤ in Copyright\nRegression is ⌧c-Copyright-Protected.\nNow we have provided evidence of the copyright protec-\ntion achieved through training under the Copyright Regres-\nsion objective. This method has been rigorously proven and\noffers complete control over copyright infringement. How-\never, concerns may arise regarding the potential impact of\nthe Copyright Regression approach on the model’s over-\nall performance, particularly when copyright data includes\nhigh-quality novels and images that contribute signiﬁcantly\nto the model’s performance. In fact, language models can-\nnot remember all their training data. Their training loss has\na considered range instead of equal to 0. Based on this, we\nonly need to let model’s performance on copyrighted data\nbe different from model’s performance on other data, even if\nthis difference is very small, then we can ascertain whether\nthe model has access to these copyright data during out-\nput generation and intentionally avoids outputting them. The\ndifference, namely ⌧, can be easily controlled by adjusting\nthe value of \u0000\nc and n1/n, we will continue to explain that\nwhy we say this in Section 7.\n7 Experiment\nIn order to evaluate and demonstrate the effectiveness of our\nproposed Copyright Regression approach, we conducted ex-\ntensive experiments using Softmax Regression. By varying\nthe values of n\n1 (representing the number of data instances\nwith copyright issues) and \u0000c (the coefﬁcient used to control\nthe Copyright Regression), we compared the results against\na baseline model. The experimental ﬁndings clearly indicate\nthe efﬁcacy of our method in providing effective copyright\nprotection.\nIn Section 7.1, we provided the details of our experiment.\nIn Section 7.2, we provided experimental results and ana-\nlyzed the effectiveness of Copyright Regression.\n7.1 Setup\nModel and Baseline. We applied our approach Copyright\nRegression on a pretrained generative language model GPT-\n2 (Radford et al. 2019), namely CR-GPT-2. To evaluate\nthe effectiveness of our approach, we conduct a compara-\ntive analysis against a baseline method referred to as GPT-2\nwhere we directly evaluate GPT-2 model on copyright issues\nwithout any additional training.\nDataset. We employed an open-source dataset Wikitext2\n(Merity et al. 2016) to ﬁne-tune our model, and evaluate\nthe performance of our model on its test set. Speciﬁcally,\nwe randomly selected some data as fake copyrighted data to\nsimulate real-world cases, and denote n\n1/n as the propor-\ntion of copyrighted data in the whole training dataset.\nhyper-parameters. To assess the inﬂuence of copyright\ndata with different proportions during training, we varied\nthe value of n1/n to be n1/n 2{ 0.1, 0.2, 0.4, 0.6, 0.8}.\nAdditionally, to evaluate the impact of different values\nof \u0000\nc on copyright protection, we consider \u0000c values of\n{0.1, 0.2, 0.3, 0.4, 0.5}. In addition, we ﬁxed random seeds\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17876\nFigure 1: Copyright Regression experiment result\nand conducted multiple experiments to record the maxi-\nmum, minimum, and average values to ensure stable results\nwere obtained.\nMetrics. We use two evaluation metrics, including ⌧\nCE =\nLCE1\nn1\n\u0000 LCE2\nn2\nand ”Perplexity”, where the LCE1 denotes the\nsum of Cross Entropy loss on the copyright dataset and the\nLCE2 denotes the sum of Cross Entropy loss on the non-\ncopyright dataset.\n7.2 Results and Analysis\nImpact of \u0000c. The left top image of Figure 1 depicts the rela-\ntionship between the variables \u0000c and the difference metric\n⌧CE. In this experiment, we set the value of n1/n =0 .1.\nRemarkably, the observed trend aligns closely with the re-\nsult we derived in Section 6.2. Our derived result, stated\nas ⌧\nCE = LCE1\nn1\n\u0000 LCE2\nn2\n\u0000\np2\u0000c\nn1\n\u0000 ✏2\nn2\n, afﬁrms that our\nCopyright Regression approach effectively encourages the\nmodel to avoid copyright infringement while still maintain-\ning a controllable level of performance degradation. Further-\nmore, the left bottom image of Figure 1 indicates inappro-\npriate \u0000\nc may have a slight negative impact on the overall\nperformance of the model. But in the case of model conver-\ngence, this impact is limited and can be controlled by \u0000\nc.\nImpact of the proportion of copyright data. n1/n impacts\non model performance are illustrated in the right top and\nright bottom images of Figure 1. This image showcases the\nrelationship between n\n1/n and the difference metric ⌧CE. In\nthis experiment, we set the value of \u0000c =0 .2. The ﬁndings\nshow that as the proportion n1/n increases, there is basi-\ncally no signiﬁcant change in the model’s perplexity on the\ntest set and the ⌧\nCE. Especially, the right top image of Fig-\nure 1 shows the comparison between CR-GPT-2 and base-\nline GPT-2 on copyright data with metric ⌧CE. The ⌧CE\nof CR-GPT-2 is stable above 0.5 while the ⌧CE of GPT-\n2 is around 0. This ﬁnding provides compelling evidence\nthat our Copyright Regression approach effectively prevents\nthe occurrence of the ”inﬁnite monkey” phenomenon, ensur-\ning that the model’s outputs consistently avoid copyright in-\nfringement. By maintaining a reliable level of performance\non copyright data, our method demonstrates its ability to\nstrike a crucial balance between performance and copyright\nprotection.\n8 Conclusion\nOur work shows that the training of transformers can be\nviewed as a softmax regression problem. We provide a no-\ntion of copyright regression, which encourages regression\nfunctions to avoid outputting copyrighted data. Then, we\ncombine the two to perform copyright regression on the soft-\nmax function, which allows us to train transformers in a way\nthat avoids outputting copyright data. The main idea to solve\ncopyright regression on the softmax function, was to show\nthat the copyright regression problem is convex and that the\nHessian is Lipschitz. This guarantees that gradient descent\nmethods will have guaranteed convergence to the optimal\nsolution with good stability properties. The experimental re-\nsults of applying our method on GPT-2 show that our algo-\nrithm performs well in preventing copyright issues.\nAcknowledgements\nWe would like to thank Yanxi Shen and the anonymous re-\nviewers for helpful discussions and feedback.\nReferences\nAllen-Zhu, Z.; Li, Y .; and Song, Z. 2019a. A convergence theory\nfor deep learning via over-parameterization. In ICML.\nAllen-Zhu, Z.; Li, Y .; and Song, Z. 2019b. On the convergence rate\nof training recurrent neural networks. NeurIPS, 32.\nAlman, J.; Liang, J.; Song, Z.; Zhang, R.; and Zhuo, D. 2023. By-\npass exponential time preprocessing: Fast neural network training\nvia weight-data correlation preprocessing. In NeurIPS.\nAlman, J.; and Song, Z. 2023. Fast Attention Requires Bounded\nEntries. arXiv preprint arXiv:2302.13214.\nAnil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin, D.; Passos,\nA.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen, Z.; et al. 2023. Palm\n2 technical report. arXiv preprint arXiv:2305.10403.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17877\nArora, S.; and Goyal, A. 2023. A Theory for Emergence of Com-\nplex Skills in Language Models. arXiv preprint arXiv:2307.15936.\nBao, F.; Nie, S.; Xue, K.; Cao, Y .; Li, C.; Su, H.; and Zhu, J. 2023.\nAll are worth words: A vit backbone for diffusion models. In\nCVPR, 22669–22679.\nBARD. 2023. Try BARD, an AI experiment by Google. Google.\nBrand, J. v. d.; Peng, B.; Song, Z.; and Weinstein, O. 2020. Train-\ning (overparametrized) neural networks in near-linear time. arXiv\npreprint arXiv:2006.11648.\nBrand, J. v. d.; Song, Z.; and Zhou, T. 2023. Algorithm and Hard-\nness for Dynamic Attention Maintenance in Large Language Mod-\nels. arXiv preprint arXiv:2304.02207.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhari-\nwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al.\n2020. Language models are few-shot learners. NeurIPS, 33: 1877–\n1901.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.; Horvitz, E.;\nKamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg, S.; et al. 2023.\nSparks of artiﬁcial general intelligence: Early experiments with\ngpt-4. arXiv preprint arXiv:2303.12712.\nCai, T.; Gao, R.; Hou, J.; Chen, S.; Wang, D.; He, D.; Zhang,\nZ.; and Wang, L. 2019. Gram-gauss-newton method: Learning\noverparameterized neural networks for regression problems. arXiv\npreprint arXiv:1905.11675.\nCao, H.; Wang, J.; Ren, T.; Qi, X.; Chen, Y .; Yao, Y .; and Zhang,\nL. 2022. Exploring vision transformers as diffusion learners. arXiv\npreprint arXiv:2212.13771.\nCao, Y .; and Gu, Q. 2019. Generalization bounds of stochastic\ngradient descent for wide and deep neural networks. NeurIPS, 32.\nChatGPT. 2022. Optimizing Language Models for Dialogue. Ope-\nnAI Blog.\nChen, B.; Dao, T.; Winsor, E.; Song, Z.; Rudra, A.; and R ´e,\nC. 2021. Scatterbrain: Unifying sparse and low-rank attention.\nNeurIPS, 34: 17413–17426.\nChen, B.; Liu, Z.; Peng, B.; Xu, Z.; Li, J. L.; Dao, T.; Song, Z.; Shri-\nvastava, A.; and Re, C. 2020. Mongoose: A learnable lsh frame-\nwork for efﬁcient neural network training. In International Con-\nference on Learning Representations.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.;\nRoberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.;\net al. 2022. Palm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311.\nChu, T.; Song, Z.; and Yang, C. 2023a. Fine-tune language mod-\nels to approximate unbiased in-context learning. arXiv preprint\narXiv:2310.03331.\nChu, T.; Song, Z.; and Yang, C. 2023b. How to Protect Copyright\nData in Optimization of Large Language Models? arXiv preprint\narxiv:2308.12247.\nDeng, Y .; Li, Z.; Mahadevan, S.; and Song, Z. 2023. Zero-th Or-\nder Algorithm for Softmax Attention Optimization. arXiv preprint\narXiv:2307.08352.\nDeng, Y .; Li, Z.; and Song, Z. 2023. Attention scheme inspired\nsoftmax regression. arXiv preprint arXiv:2304.10411.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert:\nPre-training of deep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.;\nZhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold,\nG.; Gelly, S.; et al. 2020. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929.\nDu, S. S.; Zhai, X.; Poczos, B.; and Singh, A. 2018. Gradient\ndescent provably optimizes over-parameterized neural networks.\narXiv preprint arXiv:1810.02054.\nfor the 9th circuits, U. S. C. 2022. Copying—Access and Sub-\nstantial Similarity. https://www.ce9.uscourts.gov/jury-instructions/\nnode/274/. Accessed: 2022-12-31.\nGao, Y .; Mahadevan, S.; and Song, Z. 2023. An over-parameterized\nexponential regression. arXiv preprint arXiv:2303.16504.\nGao, Y .; Song, Z.; and Yang, X. 2023. Differentially private atten-\ntion computation. arXiv preprint arXiv:2305.04701.\nGao, Y .; Song, Z.; Yang, X.; and Zhang, R. 2023. Fast\nQuantum Algorithm for Attention Computation. arXiv preprint\narXiv:2307.08045.\nGao, Y .; Song, Z.; and Yin, J. 2023. GradientCoin: A Peer-\nto-Peer Decentralized Large Language Models. arXiv preprint\narXiv:2308.10502.\nGillotte, J. L. 2019. Copyright infringement in ai-generated art-\nworks. UC Davis L. Rev., 53: 2655.\nHan, K.; Wang, Y .; Chen, H.; Chen, X.; Guo, J.; Liu, Z.; Tang, Y .;\nXiao, A.; Xu, C.; Xu, Y .; et al. 2022. A survey on vision trans-\nformer. PAMI, 45(1): 87–110.\nHattenbach, B.; and Glucoft, J. 2015. Patents in an era of inﬁnite\nmonkeys and artiﬁcial intelligence. Stan. Tech. L. Rev., 19: 32.\nHe, X.; Xu, Q.; Lyu, L.; Wu, F.; and Wang, C. 2022a. Protect-\ning intellectual property of language generation apis with lexical\nwatermark. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 36, 10758–10766.\nHe, X.; Xu, Q.; Zeng, Y .; Lyu, L.; Wu, F.; Li, J.; and Jia, R. 2022b.\nCater: Intellectual property protection on text generation apis via\nconditional watermarks. NeurIPS, 35: 5431–5445.\nHristov, K. 2016. Artiﬁcial intelligence and the copyright dilemma.\nIdea, 57: 431.\nHuang, B.; Li, X.; Song, Z.; and Yang, X. 2021. Fl-ntk: A neural\ntangent kernel-based framework for federated learning analysis. In\nICML.\nIgnat, O.; Jin, Z.; Abzaliev, A.; Biester, L.; Castro, S.; Deng, N.;\nGao, X.; Gunal, A.; He, J.; Kazemi, A.; et al. 2023. A PhD Stu-\ndent’s Perspective on Research in NLP in the Era of Very Large\nLanguage Models. arXiv preprint arXiv:2305.12544.\nJi, Z.; and Telgarsky, M. 2019. Polylogarithmic width sufﬁces for\ngradient descent to achieve arbitrarily small test error with shallow\nrelu networks. arXiv preprint arXiv:1909.12292.\nJiang, D.; Ren, X.; and Lin, B. Y . 2023. LLM-Blender: Ensembling\nLarge Language Models with Pairwise Ranking and Generative Fu-\nsion. arXiv preprint arXiv:2306.02561.\nKirchenbauer, J.; Geiping, J.; Wen, Y .; Katz, J.; Miers, I.; and Gold-\nstein, T. 2023. A watermark for large language models. arXiv\npreprint arXiv:2301.10226.\nKitaev, N.; Kaiser, Ł.; and Levskaya, A. 2020. Reformer: The efﬁ-\ncient transformer. arXiv preprint arXiv:2001.04451.\nLee, J. D.; Shen, R.; Song, Z.; Wang, M.; and Yu, Z. 2020. Gener-\nalized leverage score sampling for neural networks. NeurIPS.\nLi, Y .; and Liang, Y . 2018. Learning overparameterized neural net-\nworks via stochastic gradient descent on structured data. NeurIPS.\nLi, Z.; Song, Z.; and Zhou, T. 2023. Solving regularized exp, cosh\nand sinh regression problems. arXiv preprint arXiv:2303.15725.\nLiu, H.; Li, Z.; Hall, D.; Liang, P.; and Ma, T. 2023a. Sophia: A\nScalable Stochastic Second-order Optimizer for Language Model\nPre-training. arXiv preprint arXiv:2305.14342.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17878\nLiu, Z.; Wang, J.; Dao, T.; Zhou, T.; Yuan, B.; Song, Z.; Shrivas-\ntava, A.; Zhang, C.; Tian, Y .; Re, C.; et al. 2023b. Deja vu: Con-\ntextual sparsity for efﬁcient llms at inference time. In ICML.\nMalladi, S.; Gao, T.; Nichani, E.; Damian, A.; Lee, J. D.; Chen,\nD.; and Arora, S. 2023. Fine-Tuning Language Models with Just\nForward Passes. arXiv preprint arXiv:2305.17333.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2016. Pointer\nsentinel mixture models. arXiv preprint arXiv:1609.07843.\nMunteanu, A.; Omlor, S.; Song, Z.; and Woodruff, D. 2022.\nBounding the width of neural networks via coupled initialization\na worst case analysis. In ICML, 16083–16122.\nNoci, L.; Anagnostidis, S.; Biggio, L.; Orvieto, A.; Singh, S. P.; and\nLucchi, A. 2022. Signal propagation in transformers: Theoretical\nperspectives and the role of rank collapse. NeurIPS, 35.\nOymak, S.; and Soltanolkotabi, M. 2020. Toward moderate overpa-\nrameterization: Global convergence guarantees for training shallow\nneural networks. IEEE Journal on Selected Areas in Information\nTheory, 1(1): 84–105.\nPanigrahi, A.; Malladi, S.; Xia, M.; and Arora, S. 2023. Trainable\nTransformer in Transformer. arXiv preprint arXiv:2307.01189.\nQin, L.; Song, Z.; and Yang, Y . 2023. Efﬁcient SGD Neural Net-\nwork Training via Sublinear Activated Neuron Identiﬁcation. arXiv\npreprint arXiv:2307.06565.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language Models are Unsupervised Multitask\nLearners.\nRafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning,\nC. D.; and Finn, C. 2023. Direct preference optimization: Your\nlanguage model is secretly a reward model. arXiv preprint\narXiv:2305.18290.\nResearch, O. 2021. DALL·E: Creating images from text. https:\n//openai.com/research/dall-e/. Accessed: 2021-01-05.\nResearch, O. 2022. DALL·E 2 pre-training mitigations. https://\nopenai.com/research/dall-e-2-pre-training-mitigations. Accessed:\n2022-01-28.\nR¨uckl´e, A.; Geigle, G.; Glockner, M.; Beck, T.; Pfeiffer,\nJ.; Reimers, N.; and Gurevych, I. 2020. Adapterdrop: On\nthe efﬁciency of adapters in transformers. arXiv preprint\narXiv:2010.11918.\nSag, M. 2018. The new legal landscape for text mining and ma-\nchine learning. J. Copyright Soc’y USA, 66: 291.\nSanford, C.; Hsu, D.; and Telgarsky, M. 2023. Representa-\ntional Strengths and Limitations of Transformers. arXiv preprint\narXiv:2306.02896.\nSong, Z.; and Yang, X. 2019. Quadratic sufﬁces for over-\nparametrization via matrix chernoff bound. arXiv preprint\narXiv:1906.03593.\nSong, Z.; Zhang, L.; and Zhang, R. 2021. Training multi-layer\nover-parametrized neural network in subquadratic time. arXiv\npreprint arXiv:2112.07628.\nSun, A. Y .; Zemour, E.; Saxena, A.; Vaidyanathan, U.; Lin, E.;\nLau, C.; and Mugunthan, V . 2023. Does ﬁne-tuning GPT-3 with\nthe OpenAI API leak personally-identiﬁable information? arXiv\npreprint arXiv:2307.16382.\nSvyatkovskiy, A.; Deng, S. K.; Fu, S.; and Sundaresan, N. 2020.\nIntellicode compose: Code generation using transformer. In ESEC-\nFSE.\nTay, Y .; Bahri, D.; Metzler, D.; Juan, D.-C.; Zhao, Z.; and Zheng, C.\n2021. Synthesizer: Rethinking self-attention for transformer mod-\nels. In ICML, 10183–10192. PMLR.\nTay, Y .; Dehghani, M.; Abnar, S.; Shen, Y .; Bahri, D.; Pham, P.;\nRao, J.; Yang, L.; Ruder, S.; and Metzler, D. 2020. Long range\narena: A benchmark for efﬁcient transformers. arXiv preprint\narXiv:2011.04006.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.;\nLacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al.\n2023a. Llama: Open and efﬁcient foundation language models.\narXiv preprint arXiv:2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.;\net al. 2023b. Llama 2: Open foundation and ﬁne-tuned chat models.\narXiv preprint arXiv:2307.09288.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is\nall you need. NeurIPS, 30.\nVyas, N.; Kakade, S.; and Barak, B. 2023. Provable copyright pro-\ntection for generative models. arXiv preprint arXiv:2302.10870.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.;\nLe, Q. V .; Zhou, D.; et al. 2022. Chain-of-thought prompting elicits\nreasoning in large language models. NeurIPS, 35: 24824–24837.\nWu, J.; Fu, R.; Fang, H.; Zhang, Y .; and Xu, Y . 2023a. Medsegdiff-\nv2: Diffusion based medical image segmentation with transformer.\narXiv preprint arXiv:2301.11798.\nWu, J.; Yu, T.; Wang, R.; Song, Z.; Zhang, R.; Zhao, H.; Lu, C.; Li,\nS.; and Henao, R. 2023b. InfoPrompt: Information-Theoretic Soft\nPrompt Tuning for Natural Language Understanding. In NeurIPS.\nWu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .; Norouzi, M.;\nMacherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey, K.; et al.\n2016. Google’s neural machine translation system: Bridging the\ngap between human and machine translation. arXiv preprint\narXiv:1609.08144.\nXu, Z.; Zhang, Y .; Andrew, G.; Choquette-Choo, C. A.; Kairouz, P.;\nMcMahan, H. B.; Rosenstock, J.; and Zhang, Y . 2023. Federated\nLearning of Gboard Language Models with Differential Privacy.\narXiv preprint arXiv:2305.18465.\nZelikman, E.; Huang, Q.; Liang, P.; Haber, N.; and Goodman, N. D.\n2023. Just One Byte (per gradient): A Note on Low-Bandwidth\nDecentralized Language Model Finetuning Using Shared Random-\nness. arXiv preprint arXiv:2306.10015.\nZhang, G.; Martens, J.; and Grosse, R. B. 2019. Fast convergence\nof natural gradient descent for over-parameterized neural networks.\nNeurIPS, 32.\nZhang, J.; Karimireddy, S. P.; Veit, A.; Kim, S.; Reddi, S.; Kumar,\nS.; and Sra, S. 2020a. Why are adaptive methods good for attention\nmodels? NeurIPS, 33: 15383–15393.\nZhang, L. 2022. Speeding up optimizations via data structures:\nFaster search, sample and maintenance. Ph.D. thesis, Master’s the-\nsis, Carnegie Mellon University.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen,\nS.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al. 2022. Opt:\nOpen pre-trained transformer language models. arXiv preprint\narXiv:2205.01068.\nZhang, Y .; Plevrakis, O.; Du, S. S.; Li, X.; Song, Z.; and Arora, S.\n2020b. Over-parameterized adversarial training: An analysis over-\ncoming the curse of dimensionality. NeurIPS, 33: 679–688.\nZhao, H.; Panigrahi, A.; Ge, R.; and Arora, S. 2023. Do Trans-\nformers Parse while Predicting the Masked Word? arXiv preprint\narXiv:2303.08117.\nZou, D.; and Gu, Q. 2019. An improved analysis of training over-\nparameterized deep neural networks. NeurIPS, 32.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17879"
}