{
  "title": "Test-Time Personalization with a Transformer for Human Pose Estimation",
  "url": "https://openalex.org/W3181047098",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100743270",
      "name": "Miao Hao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101457179",
      "name": "Yizhuo Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5014510735",
      "name": "Zonglin Di",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5067011413",
      "name": "Nitesh B. Gundavarapu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100424261",
      "name": "Xiaolong Wang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2916798096",
    "https://openalex.org/W3176892444",
    "https://openalex.org/W2964341382",
    "https://openalex.org/W3035707711",
    "https://openalex.org/W3117707723",
    "https://openalex.org/W3113653931",
    "https://openalex.org/W2997051132",
    "https://openalex.org/W2962742544",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2326925005",
    "https://openalex.org/W3129059619",
    "https://openalex.org/W2604346130",
    "https://openalex.org/W2962730651",
    "https://openalex.org/W3034399482",
    "https://openalex.org/W3015340111",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2962843838",
    "https://openalex.org/W3035435924",
    "https://openalex.org/W3122887115",
    "https://openalex.org/W3139517252",
    "https://openalex.org/W2963419579",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3119543788",
    "https://openalex.org/W2963402313",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2951707615",
    "https://openalex.org/W2964185410",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2964221239",
    "https://openalex.org/W2962849564",
    "https://openalex.org/W3156282961",
    "https://openalex.org/W2742737904",
    "https://openalex.org/W2113325037",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W2982220924",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3158818292",
    "https://openalex.org/W2971202257",
    "https://openalex.org/W2011700682",
    "https://openalex.org/W3210246279",
    "https://openalex.org/W2137591992",
    "https://openalex.org/W2982697283",
    "https://openalex.org/W2962896489",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W2963759896",
    "https://openalex.org/W3105320009",
    "https://openalex.org/W2890967717",
    "https://openalex.org/W3202782837",
    "https://openalex.org/W2331128040",
    "https://openalex.org/W602397586",
    "https://openalex.org/W1992016704",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2293220651",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2756050327",
    "https://openalex.org/W2963474899",
    "https://openalex.org/W2993728126",
    "https://openalex.org/W3102696055",
    "https://openalex.org/W3105732956",
    "https://openalex.org/W2330154883",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2578797046",
    "https://openalex.org/W3009014607",
    "https://openalex.org/W2964304707",
    "https://openalex.org/W3117088788",
    "https://openalex.org/W3126676912"
  ],
  "abstract": "We propose to personalize a human pose estimator given a set of test images of a person without using any manual annotations. While there is a significant advancement in human pose estimation, it is still very challenging for a model to generalize to different unknown environments and unseen persons. Instead of using a fixed model for every test case, we adapt our pose estimator during test time to exploit person-specific information. We first train our model on diverse data with both a supervised and a self-supervised pose estimation objectives jointly. We use a Transformer model to build a transformation between the self-supervised keypoints and the supervised keypoints. During test time, we personalize and adapt our model by fine-tuning with the self-supervised objective. The pose is then improved by transforming the updated self-supervised keypoints. We experiment with multiple datasets and show significant improvements on pose estimations with our self-supervised personalization.",
  "full_text": "Test-Time Personalization with a Transformer for\nHuman Pose Estimation\nYizhuo Li‚àó\nShanghai Jiao Tong University\nliyizhuo@sjtu.edu.cn\nMiao Hao‚àó\nUC San Diego\nmhao@ucsd.edu\nZonglin Di‚àó\nUC San Diego\nzodi@ucsd.edu\nNitesh B. Gundavarapu\nUC San Diego\nnbgundav@ucsd.edu\nXiaolong Wang\nUC San Diego\nxiw012@ucsd.edu\nAbstract\nWe propose to personalize a 2D human pose estimator given a set of test images\nof a person without using any manual annotations. While there is a signiÔ¨Åcant\nadvancement in human pose estimation, it is still very challenging for a model\nto generalize to different unknown environments and unseen persons. Instead of\nusing a Ô¨Åxed model for every test case, we adapt our pose estimator during test\ntime to exploit person-speciÔ¨Åc information. We Ô¨Årst train our model on diverse\ndata with both a supervised and a self-supervised pose estimation objectives jointly.\nWe use a Transformer model to build a transformation between the self-supervised\nkeypoints and the supervised keypoints. During test time, we personalize and\nadapt our model by Ô¨Åne-tuning with the self-supervised objective. The pose is then\nimproved by transforming the updated self-supervised keypoints. We experiment\nwith multiple datasets and show signiÔ¨Åcant improvements on pose estimations\nwith our self-supervised personalization. Project page with code is available at\nhttps://liyz15.github.io/TTP/.\n1 Introduction\nRecent years have witnessed a large advancement in human pose estimation. A lot of efforts have\nbeen spent on learning a generic deep network on large-scale human pose datasets to handle diverse\nappearance changes [59, 64, 7, 15, 43]. Instead of learning a generic model, another line of research\nis to personalize and customize human pose estimation for a single subject [10]. For a speciÔ¨Åc person,\nwe can usually have a long video (e.g., instructional videos, news videos) or multiple photos from\npersonal devices. With these data, we can adapt the model to capture the person-speciÔ¨Åc features for\nimproving pose estimation and handling occlusion and unusual poses. However, the cost of labeling\nlarge-scale data for just one person is high and unrealistic.\nIn this paper, we propose to personalize human pose estimation with unlabeled video data during\ntest time, namely, Test-Time Personalization. Our setting falls in the general paradigm of Test-Time\nAdaptation [58, 35, 61, 69], where a generic model is Ô¨Årst trained with diverse data, and then it is\nÔ¨Åne-tuned to adapt to a speciÔ¨Åc instance during test time without using human supervision. This\nallows the model to generalize to out-of-distribution data and preserves privacy when training is\ndistributed. SpeciÔ¨Åcally, Sun et al. [ 58] propose to generalize image classiÔ¨Åcation by performing\njoint training with a semantic classiÔ¨Åcation task and a self-supervised image rotation prediction\ntask [18]. During inference, the shared network representation is Ô¨Åne-tuned on the test instance with\nthe self-supervisory signal for adaptation. While the empirical result is encouraging, it is unclear how\n‚àóEqual contribution\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2107.02133v3  [cs.CV]  8 Nov 2021\nTraining Inference\nPredictPersonalize\nnon-continuous continuous\nFigure 1: Test-Time Personalization. Our model is Ô¨Årstly trained on diverse data with both super-\nvised and self-supervised keypoint estimation tasks. During test time, we personalize the model\nusing only the self-supervised target in single person domain and then predict with the updated\nmodel. During Test-Time Personalization, no continuous data is required but only unlabeled samples\nbelonging to the same person are needed. Our method boosts performance at test time without costly\nlabeling or sacriÔ¨Åcing privacy.\nthe rotation prediction task can help image classiÔ¨Åcation, and what is the relation between two tasks\nbesides sharing the same feature backbone.\nGoing beyond feature sharing with two distinct tasks, we introduce to perform joint supervised and\nself-supervised human keypoint estimation [ 26] tasks where the supervised keypoint outputs are\ndirectly transformed from the self-supervised keypoints using a Transformer [60]. In this way, when\nÔ¨Åne-tuning with the self-supervised task in test time, the supervised pose estimation can be improved\nby transforming from the improved self-supervised keypoints.\nWe adapt the self-supervised keypoint estimation task proposed by Jakab et al. [26]. The task is built\non the assumption that the human usually maintains the appearance but changes poses across time in\na video. Given a video frame, it trains a network to extract a tight bottleneck in the form of sparse\nspatial heatmaps, which only contain pose information without appearance. The training objective\nis to reconstruct the same frame by combining the bottleneck heatmaps and the appearance feature\nextracted from another frame. Note while this framework can extract keypoints to represent the human\nstructure, they are not aligned with the semantic keypoints deÔ¨Åned in human pose estimation. Building\non this model, we add an extra keypoint estimation objective which is trained with human supervision.\nInstead of simply sharing features between two objectives as [58], we train a Transformer model on\ntop of the feature backbone to extract the relation and afÔ¨Ånity matrix between the self-supervised\nkeypoint heatmap and the supervised keypoint heatmap. We then use the afÔ¨Ånity matrix to transform\nthe self-supervised keypoints as the supervised keypoint outputs. With our Transformer design, it\nnot only increases the correlation between two tasks when training but also improves Test-Time\nPersonalization as changing one output will directly contribute to the the output of another task.\nWe perform our experiments with multiple human pose estimation datasets including Human\n3.6M [24], Penn Action [ 71], and BBC Pose [ 8] datasets. As shown in Figure 1, our Test-Time\nPersonalization can perform on frames that continuously exist in a video and also with frames that\nare non-continuous as long as they are for the same person. We show that by using our approach for\npersonalizing human pose estimation in test time, we achieve signiÔ¨Åcant improvements over baselines\nin all datasets. More interestingly, the performance of our method improves with more video frames\nappearing online for the same person during test time.\n2 Related Work\nHuman Pose Estimation. Human pose estimation has been extensively studied and achieved great\nadvancements in the past few years [ 59, 64, 7, 15, 43, 67, 45, 21, 65, 13, 57, 75, 44, 5, 14]. For\nexample, Toshev et al. [59] propose to regress the keypoint locations from the input images. Instead\nof direct location regression, Wei et al. [64] propose to apply a cascade framework for coarse to Ô¨Åne\nheatmap prediction and achieve signiÔ¨Åcant improvement. Building on this line of research, Xiao et\nal. [65] provides a simple and good practice on heatmap-based pose estimation, which is utilized as\n2\nour baseline model. While in our experiments we utilize video data for training, our model is a single-\nimage pose estimator and it is fundamentally different from video pose estimation models [1, 19, 62]\nwhich take multiple continuous frames as inputs. This gives our model the Ô¨Çexibility to perform pose\nestimation on static images and thus it is not directly comparable to approaches with video inputs.\nOur work is also related to personalization on human pose estimation from Charles et al. [10], which\nuses multiple temporal and continuity constraints to propagate the keypoints to generate more training\ndata. Instead of tracking keypoints, we use a self-supervised objective to perform personalization in\ntest time. Our method is not restricted to the continuity between close frames, and the self-supervision\ncan be applied on any two frames far away in a video as long as they belong to the same person.\nTest-Time Adaptation. Our personalization setting falls into the paradigm of Test-Time Adaptation\nwhich is recently proposed in [ 51, 50, 3, 58, 35, 61, 69, 28, 42, 20] for generalization to out-of-\ndistribution test data. For example, Shocher et al. [51] propose a super-resolution framework which\nis only trained during test time with a single image via down-scaling the image to create training\npairs. Wang et al. [61] introduce to use entropy of the classiÔ¨Åcation probability distribution to provide\nÔ¨Åne-tuning signals when given a test image. Instead of optimizing the main task itself during test time,\nSun et al. [58] propose to utilize a self-supervised rotation prediction task to help improve the visual\nrepresentation during inference, which indirectly improves semantic classiÔ¨Åcation. Going beyond\nimage classiÔ¨Åcation, Joo et al. [ 30] propose a method that proposes test time optimization for 3D\nhuman body regression. In our work for pose personalization, we try to bridge the self-supervised and\nsupervised objectives close. We leverage a self-supervised keypoint estimation task and transform\nthe self-supervised keypoints to supervised keypoints via a Transformer model. In this way, training\nwith self-supervision will directly improve the supervised keypoint outputs.\nSelf-supervised Keypoint Estimation. There are a lot of recent developments on learning keypoint\nrepresentations with self-supervision [ 55, 72, 26, 38, 32, 27, 68, 36, 40]. For example, Jakab et\nal. [26] propose a video frame reconstruction task which disentangles the appearance feature and\nkeypoint structure in the bottleneck. This work is then extended for control and Reinforcement\nLearning [32, 36, 40], and the keypoints can be mapped to manual deÔ¨Åned human pose via adding\nadversarial learning loss [27]. While the results are encouraging, most of the results are reported\nin relatively simple scenes and environments. In our paper, by leveraging the self-supervised task\ntogether with the supervised task, we can perform human pose personalization on images in the wild.\nTransformers. Transformer has been widely applied in both language processing [ 60, 16] and\ncomputer vision tasks [ 63, 46, 23, 49, 56, 17, 11, 4, 73, 6, 37], speciÔ¨Åcally for pose estimation\nrecently [66, 54, 41, 33]. For example, Li et al. [33] propose to utilize the encoder-decoder model in\nTransformers to perform keypoint regression, which allows for more general-purpose applications\nand requires less priors in architecture design. Inspired by these works, we apply Transformer to\nreason the relation and mapping between the supervised and self-supervised keypoints.\n3 Method\nOur method aims at generalizing better for pose estimation on a single image by personalizing with\nunlabeled data. The model is Ô¨Årstly trained with diverse data on both a supervised pose estimation\ntask and a self-supervised keypoint estimation task, using a proposed Transformer design to model\nthe relation between two tasks. During inference, the model conducts Test-Time Personalization\nwhich only requires the self-supervised keypoint estimation task, boosting performance without costly\nlabeling or sacriÔ¨Åcing privacy. The whole pipeline is shown in Figure 2.\n3.1 Joint Training for Pose Estimation with a Transformer\nGiven a set of N labeled images of a single person I = {I1,I2 ...,I N }, a shared encoder œÜmaps\nthem into feature space F = {F1,F2 ...,F N }, which is shared by both a supervised and a self-\nsupervised keypoint estimation tasks. We introduce both tasks and the joint framework as follows.\n3.1.1 Self-supervised Keypoint Estimation\nFor the self-supervised task, we build upon the work from Jakab et al. [ 26] which uses an image\nreconstruction task to perform disentanglement of human structure and appearance, which leads to\nself-supervised keypoints as intermediate results. Given two images of a single person Is and It, the\ntask aims at reconstructing It using structural keypoint information from target It and appearance\ninformation from source Is. The appearance information Fapp\ns of source image Is is extracted with a\n3\nùúô\nTransformer\nùúô!\"\"ùêπ!\"##\nùúì#$%&ùêπ'\nùêª#(\"\nùêª#$%&\nConcatenateProduct\nùêπ$%#\nùúô)$*+$)\ntargetsource\nValueùêπ'\n‚Ñí#$%&\n‚Ñí#(\"\nreconstruction\nSelf-supervised TaskSupervised Task\nQueryùëÑ#(\"\nKeyùêπ' ùëä\nùêº'\nùêº, )ùêº'\nFigure 2: The proposed pipeline. 1) Self-supervised task for personalization. In the middle stream,\nthe encoder œÜ encodes the target image into feature Ft. Then Ft is fed into the self-supervised\nhead œàself obtaining self-supervised keypoint heatmaps Hself. Passing Hself into a keypoint encoder\n(skipped in the Ô¨Ågure) leads to keypoint feature Fkp\nt . In the bottom stream, a source image is\nforwarded to an appearance extractor œÜapp which leads to appearance feature Fapp\nt . Together, a\ndecoder reconstructs the target image using concatenated Fapp\ns and Fkp\nt . 2) Supervised task with\nTransformer.On the top stream, a Transformer predicts an afÔ¨Ånity matrix given learnable keypoint\nqueries Qsup and Ft. The Ô¨Ånal supervised heatmaps Hsup is given as weighted sum of Hself using W.\nsimple extractor œÜapp (see the bottom stream in Figure 2). The extraction of keypoints information\nfrom the target image follows three steps as below (also the see the middle stream in Figure 2).\nFirstly, the target image It is forwarded to the encoder œÜ to obtain shared feature Ft. The self-\nsupervised head œàself further encodes the shared feature Ft into heatmaps Hself\nt . Note the number of\nchannels in the heatmap Hself\nt is equal to the number of self-supervised keypoints. Secondly, Hself\nt is\nnormalized using a Softmax function and thus becomes condensed keypoints. In the third step, the\nheatmaps are replaced with Ô¨Åxed Gaussian distribution centered at condensed points, which serves as\nkeypoint information Fkp\nt . These three steps ensure a bottleneck of keypoint information, ensuring\nthere is not enough capacity to encode appearance features to avoid trivial solutions.\nThe objective of the self-supervised task is to reconstruct the target image with a decoder using both\nappearance and keypoint features: ÀÜIt = œÜrender\n(\nFapp\ns ,Fkp\nt\n)\n. Since the bottleneck structure from the\ntarget stream limits the information to be passed in the form of keypoints, the image reconstruction\nenforces the disentanglement and the network has to borrow appearance information from source\nstream. The Perceptual loss [29] and L2 distance are utilized as the reconstruction objective,\nLself = PerceptualLoss\n(\nIt,ÀÜIt\n)\n+\nÓµπÓµπÓµπIt ‚àíÀÜIt\nÓµπÓµπÓµπ\n2\n(1)\nInstead of self-supervised tasks like image rotation prediction [18] or colorization [70], choosing an\nexplicitly related self-supervised key-point task in joint training naturally preserves or even improves\nperformance, and it is more beneÔ¨Åcial to test-time personalization. Attention should be paid that our\nmethod requires only label of one single image and unlabeled samples belonging to the same person.\nCompared to multiple labeled samples of the same person or even more costly consecutively labeled\nvideo, acquiring such data is much more easier and efÔ¨Åcient.\n3.1.2 Supervised Keypoint Estimation with a Transformer\nA natural and basic choice for supervised keypoint estimation is to use an unshared supervised head\nœàsup to predict supervised keypoints based on Ft. However, despite the effectiveness of multi-task\nlearning on two pose estimation tasks, their relation still stays plain on the surface. As similar tasks\ndo not necessarily help each other even when sharing features, we propose to use a Transformer\ndecoder to further strengthen their coupling. The Transformer decoder models the relation between\n4\ntwo tasks by learning an afÔ¨Ånity matrix between the supervised and the self-supervised keypoint\nheatmaps.\nGiven the target imageIt, its feature Ft and self-supervised heatmap Hself\nt ‚ààRh√ów√ókself\nare extracted\nusing encoder œÜand self-supervised head œàself respectively, where h,w,k self are the height, width\nand number of keypoints of the heatmap. The Transformer module learns the afÔ¨Ånity matrix based on\nlearnable supervised keypoint queries Qsup ‚ààRksup√óc and context feature Ft.\nA standard transformer decoder layer consists of a multi-head attention layer and a feed-forward\nnetwork. The spatial feature Ft is Ô¨Çattened to n tokens such that Ft ‚ààRn√óc. In a single-head\nattention layer,\nQ= QsupTQ, K= FtTK, V= FtTV (2)\nwhere TQ,TK,TV ‚ààRc√óc are weight matrices. We use Qsup as the query input and the network\nfeature Ft as the key and value inputs. The attention weights Aand attention results attn is given by,\nA= Softmax\n(\nQK‚ä§)\n(3)\nattn (Qsup,Ft,Ft) =AV (4)\nIn multi-head attention MHA(), Qsup and Ft is split to Qsup\n1 ,...,Q sup\nM and F(t,1),...,F (t,M), where\nM is the number of heads and every part is split to dimension c‚Ä≤= c/M,\nÀúQsup =\n[\nattn1(Qsup\n1 ,F(t,1),F(t,1)); ... ; attnM (Qsup\nM ,F(t,M),F(t,M))\n]\n(5)\nMHA (Qsup,Ft,Ft) = LayerNorm\n(\nQsup + Dropout\n(\nÀúQL\n))\n(6)\nwhere LayerNorm is layer normalization [2], Dropout is dropout operation [53] and L‚ààRc√óc is\na projection. Passing the result to a feed-forward network which is effectively a two layer linear\nprojection with ReLU activation followed also by residual connection, Dropout and LayerNorm\ncompletes the Transformer decoder layer. Stacking multiple layers gives us the afÔ¨Ånity feature\nFaff ‚ààRksup√óc. Then Faff is linearly projected to the space of supervised keypoints by weight matrix\nP and transformed using Softmax function among self-supervised keypoints into afÔ¨Ånity matrix,\nW = Softmax\n(\nFaffP\n)\n(7)\nEach row in W ‚ààRksup√ókself\nrepresents the relation between self-supervised keypoints and correspond-\ning supervised keypoint. Typically we have ksup ‚â§kself for higher Ô¨Çexibility. The Ô¨Ånal supervised\nheatmaps is given by,\nHsup\nt = Hself\nt W‚ä§ (8)\nThat is, supervised heatmaps are a weighted sum or selection of the self-supervised heatmaps. This\npresents supervised loss as,\nLsup =\nÓµπÓµπHsup\nt ‚àíHgt\nt\nÓµπÓµπ2\n(9)\nwhere Hgt\nt is the ground truth keypoint heatmap of target image built by placing a 2D gaussian at\neach joint‚Äôs location [43, 65].\nOur Transformer design explicitly models the relation between supervised and self-supervised tasks.\nBasic feature sharing model, even with the self-supervised task replaced by a similar pose estimation\ntask, still fails to make sure that two tasks will cooperate instead of competing with each other.\nLearning an afÔ¨Ånity matrix aligns self-supervised keypoints with supervised ones, avoiding the\nconÔ¨Çicts in multi-task training. During Test-Time Personalization, basic feature sharing model often\nlacks Ô¨Çexibility and is faced with the risk of overÔ¨Åtting to self-supervised task, due to the decoupling\nstructure of two task heads. Our method, however, enforces the coupling between tasks using an\nafÔ¨Ånity matrix and maintains Ô¨Çexibility as typically there are more self-supervised keypoints than\nsupervised ones. Besides, compared to convolution model, Transformer shows superior ability to\ncapture global context information, which is particularly needed when learning the relation between\none supervised keypoint and all self-supervised ones.\nFinally, we jointly optimize those two tasks during training. For a training sample, besides the\nsupervised task, we randomly choose another sample belonging to the same person as the target to\nreconstruct. The Ô¨Ånal loss is given by\nL= Lsup + ŒªLself (10)\nwhere Œªis a weight coefÔ¨Åcient for balancing two tasks.\n5\n3.2 Test-Time Personalization\nDuring inference with a speciÔ¨Åc person domain, we apply Test-Time Personalization by Ô¨Åne-tuning\nthe model relying solely on the self-supervised task. Given a set of Ntest images of the same person\nItest\n1 ,...,I test\nNtest , where Ntest > 1, we Ô¨Årst freeze the supervised Transformer part and update the\nshared encoder œÜ and the self-supervised head œàself with the reconstruction loss Lself. Then the\nupdated shared encoder œÜ‚àóand self-supervised head œàself‚àóare used along with the supervised head\nfor Ô¨Ånal prediction. SpeciÔ¨Åcally, during prediction, the updated features and self-supervised head\nwill output improved keypoint heatmaps which leads to better reconstruction. These improved\nself-supervised heatmaps will go through the Transformer at the same time to generate improved\nsupervised keypoints.\nDuring the personalization process, we propose two settings including the online scenario which\nworks in a stream of incoming data and the ofÔ¨Çine scenario which performs personalization on an\nunordered test image set. We illustrate the details below.\n(i) The online scenario, which takes input as a sequence and requires real-time inference such as an\nonline camera. In this setting, we can only choose both source Itest\ns and target Itest\nt with the constraint\ns‚â§T,t ‚â§T at time T for Ô¨Åne-tuning. Prediction is performed after each updating step.\n(ii) The ofÔ¨Çine scenario, which has access to the whole person domain data and has no requirement\nof real-time inference, assuming we have access to an ofÔ¨Çine video or a set of unordered images for a\nperson. In this setting, we shufÔ¨Çe the images in the dataset and perform ofÔ¨Çine Ô¨Åne-tuning, and then\nwe perform prediction at once for all the images.\nCompared to online scenario, ofÔ¨Çine scenario beneÔ¨Åts from more diverse source and target sample\npairs and avoids the variance drifts when updating the model. Since our method is designed to\npersonalize pose estimation, the model is initialized with diversely trained weights when switching\nperson identity. In each scenario, different re-initialization strategies can also be applied to avoid\noverÔ¨Åtting to a local minimum. The various combination of scenarios and reinitializing strategies\nengifts our method with great Ô¨Çexibility.\nIt should be noted that our method has no requirement of consecutive or continuous frames but only\nunlabeled images belonging to the same person, which is less costly and beyond the reach of temporal\nmethods such as 3D convolution with multiple frames. Test-Time Personalization can be done at\ninference without annotations and thus is remarkably suitable for privacy protection: The process can\nbe proceeded locally rather than uploading data of your own for annotating for specialization.\n4 Experiment\n4.1 Datasets\nOur experiments are performed on three human pose datasets with large varieties to prove the\ngenerality and effectiveness of our methods. While the datasets are continuous videos, we emphasize\nthat our approach can be generalized to discontinuous images. In fact, we take the datasets as\nunordered image collections when performing ofÔ¨Çine Test-Time Personalization. All input images\nare resized to 128√ó128 with the human located in the center.\nHuman 3.6M [24] contains 3.6 million images and provides both 2D and 3D pose annotations. We\nonly use 2D labels. Following the standard protocol [74, 34], we used 5 subjects for training and 2\nsubjects for testing. We sample the training set every 5 frames and the testing set every 200 frames.\nWe use the Percentage of Correct Key (PCK) as the metric and the threshold we use is the 20%\ndistance of the torso length.\nPenn Action [71] contains 2,236 video sequences of different people. 13 pose joints are given for\neach sample in the annotations. We use the standard training/testing split and also use PCK with\nthreshold distance of half distance of torso as the evaluation metric.\nBBC Pose [8] consists of 20 videos of different sign language interpreter. We use 610,115 labeled\nframes in the Ô¨Årst ten videos for training, and we use 2,000 frames in the remaining ten videos (200\nframes per video) with manual annotation for testing. The testing frames are not consecutive. The\nevaluation method of BBC Pose is the joint accuracy with dpixels of ground truth where dis 6\nfollowing [9, 12, 48, 26].\n6\nTable 1: Evaluation results on pose estimation. Our proposed method is denoted as Transformer (key-\npoint). For Human 3.6M and Penn Action datasets, mPCK is employed as the metric while for BBC\nPose we use mAcc. The proposed method not only performs better on the validation set but also\nenjoys more gain in Test-Time Personalization.\nMethod TTP Scenario Human 3.6M Penn Action BBC Pose\nBaseline w/o TTP 85.42 85.23 88.69\nw/o TTP 87.37 (+1.95) 84.90 (‚àí0.33) 89.07 (+0.38)\nFeat. shared (rotation) Online 88.01 (+2.59) 85.86 (+0.63) 89.34 (+0.65)\nOfÔ¨Çine 88.26 (+2.84) 85.93 (+0.70) 88.90 (+0.21)\nw/o TTP 87.41 (+1.99) 85.78 (+0.55) 89.65 (+0.96)\nFeat. shared (keypoint) Online 89.43 (+4.01) 87.27 (+2.04) 91.48 (+2.79)\nOfÔ¨Çine 89.05 (+3.63) 88.12 (+2.89) 91.65 (+2.96)\nw/o TTP 87.90 (+2.48) 86.16 (+0.93) 90.19 (+1.50)\nTransformer (keypoint) Online 91.70 (+6.28) 87.75 (+2.52) 92.51 (+3.82)\nOfÔ¨Çine 92.05 (+6.63) 88.98 (+3.75) 92.21 (+3.52)\n4.2 Implementation Details\nNetwork Architecture. We use ResNet [22] followed by three transposed convolution layers as\nencoder œÜ. Every convolution layer has 256 channels, consisting of BatchNorm and ReLU activation\nand upsampling 2 times to generate the Ô¨Ånal featureF of size 256√ó32√ó32 and c= 256. Considering\nthe diversity of datasets, we use ResNet50 for Penn Action and ResNet18 for both Human 3.6M\nand BBC Pose. We use one convolution layer as the supervised headœàsup and another convolution\nlayer for self-supervised head œàself, where the supervised head œàsup denotes the standard structure\nused in Appendix A. For all three datasets, the output channel for self-supervised keypoints is\nkself = 30. We adopt a 1-layer Transformer with 4 heads and the hidden layer in feed-forward has\n1024 dimensions. The weight of self-supervised loss is set to Œª = 1√ó10‚àí3 for Penn Action and\nBBC Pose, Œª= 1√ó10‚àí5 for Human 3.6M.\nJoint Training. We apply the same training schedule across methods. For all datasets, we use batch\nsize of 32, Adam [31] optimizer with learning rate 0.001 and decay the learning rate twice during\ntraining. We use learning schedule [18k, 24k, 28k], [246k, 328k, 383k] and [90k, 120k, 140k] for\nBBC Pose, Penn Action, and Human 3.6M respectively. We divide the learning rate by 10 after each\nstage. The training schedule of BBC Pose is shortened since the data is less diverse.\nTest-Time Personalization (TTP).During Test-Time Personalization, we use Adam optimizer with\nÔ¨Åxed learning rate 1 √ó10‚àí4. The supervised head œàsup and Transformer are frozen at this stage.\nTest-Time Personalization is applied without weight reset unless speciÔ¨Åed. In ofÔ¨Çine scenario, even\nthough the model can be updated for arbitrary steps, we adopt the same number of steps as the online\nscenario for a fair comparison. See Appendix C for more details.\nBatching Strategy. In joint training and TTP ofÔ¨Çine scenario, both target and source images are\nrandomly chosen and are different within a batch. In TTP online scenario, the target images are\nalways the current frame, which are the same within a batch, whereas the source images are randomly\nchosen from the previous frames and are different in a batch. In all the scenarios, each target-source\npair is performed data augmentation with same rotation angle and scale factor for the two images to\nmake reconstruction easier.\n4.3 Main Results\nTo better analyze the proposed method, in Table 1 we compare it with three baselines: (i)Baseline.\nThe plain baseline trained with supervised loss only. (ii) Feat. shared (rotation). Instead of self-\nsupervised keypoint estimation, we use rotation prediction to compute the self-supervised loss Lself\nin Eq. 10 following Sun et al. [58]. Rotation is predicted with a standalone supervised head œàsup. The\ntwo tasks have no direct relation except they share the same feature backbone. Weight coefÔ¨Åcient Œªis\nset to 1 √ó10‚àí4 for better performance. (iii) Feat. shared (keypoint). We use the self-supervised\nkeypoint estimation task [26] as the self-supervised objective. However, supervised keypoints are\nstill estimated with a standalone supervised head œàsup instead of our Transformer design. The two\ntasks are only connected by sharing the same feature backbone. See Appendix A for more details.\nFinally, our proposed method is denoted as Transformer (keypoint).\n7\n0 200 400 600 800 1000\nHuman 3.6M Frame ID\n2\n4\n6\n8\n10\n12Imprv. in mPCK\n0 20 40 60 80 100 120\nPenn Action Frame ID\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5Imprv. in mPCK\n0 50 100 150 200\nBBC Pose Frame ID\n0\n5\n10\n15Imprv. in Acc\nFigure 3: Improvement vs Frame ID in online scenario for 3 datasets. We plot the gap between the\nTest-Time Personalization and the baseline model for each frame step. We adopt the averaged metric\nacross all test videos. In most cases, we observe TTP improves more over time.\n0 200 400 600 800 1000\nBBC Pose Video Length\n91.0\n91.5\n92.0\n92.5\n93.0mAcc\n0 500 1000 1500\nHuman 3.6M Video Length\n89\n90\n91\n92mPCK\nFigure 4: Test-Time Personalization with different numbers\nof unlabeled test samples. Left: mAcc for different video\nlength on BBC Pose. Right: mPCK for different video\nlength on Human 3.6M. Test-Time Personalization beneÔ¨Åts\nfrom utilizing more unlabeled test samples.\nTable 2: Test-Time Personalization in\nonline scenario with different update it-\nerations.\nIters Penn Action BBC Pose\n1 87.75 92.51\n2 88.01 92.64\n3 76.27 92.59\n4 76.13 92.53\nDespite using calibrated self-supervised task weight, Feat. shared (rotation) still shows limited and\neven degraded performance on all three datasets, indicating that a general self-supervised task without\na speciÔ¨Åc design is likely to hurt the performance of supervised one. On the other hand, Feat. shared\n(keypoint) presents superior performance over Baseline, even without Test-Time Personalization. This\ndemonstrates the hypothesis that selecting a related or similar self-supervised task can facilitate the\noriginal supervised task and thus naturally leads to better performance in Test-Time Personalization.\nThe results of Test-Time Personalization show the personalizing ability of our method. Personalizing\nfor a single person results in signiÔ¨Åcant improvement.\nTransformer (keypoint) further boosts performance with Test-Time Personalization, with an improve-\nment of 6.63 mPCK on Human 3.6M, 3.75 mPCK on Penn Action, and 3.82 mAcc on BBC Pose.\nMore importantly, our design of learning an afÔ¨Ånity matrix not only improves the performance of\njoint training but also achieves a higher improvement in Test-Time Personalization. For example,\nTTP in online scenario has an improvement of 2.32 mAcc with Transformer (keypoint) compared\nto an improvement of 1.83 mAcc with Feat. shared (keypoint) for BBC Pose. This demonstrates\nthat using the proposed Transformer, two tasks cooperate better in joint training and have higher\nÔ¨Çexibility in Test-Time Personalization.\nIn terms of different scenarios for Test-Time Personalization, it is found that the ofÔ¨Çine scenario\ndoes not always surpass online scenario. For example in BBC Pose, both online scenario and ofÔ¨Çine\nscenario improve performance, yet in ofÔ¨Çine scenario, there is a small decrease in mAcc. This is\nexpected for two reasons. Firstly the major advantage of ofÔ¨Çine scenario comes from the diversity of\ntest samples while BBC Pose has a nonconsecutive validation set selected speciÔ¨Åcally for diversity.\nSecondly, we set the learning rate based on the performance of online scenario and follow it in all\nsettings to demonstrates the generality of our method. Better results can be achieved if the learning\nrate is adjusted more carefully.\n4.4 Analysis on Test-Time Personalization\nNumber of Unlabeled Test Samples. Our method exploits personal information using unlabeled\nsamples in a single person domain. We observe that more unlabeled samples can further improve\nthe performance in Test-Time Personalization. We study the number of unlabeled samples using\nextra validation samples of BBC Pose and Human 3.6M. We emphasize that although labels are also\nprovided for extra validation samples, we only use images without labels. All experiments have the\nsame setting as Transformer in online scenario and the prediction and evaluation are on the same\nÔ¨Åxed test set. In Figure 4, we report results of TTP by using different video lengths of samples in\nÔ¨Åne-tuning in an online manner. For video length smaller than the actual test sequences, we apply\n8\nFigure 5: Visualization on Penn Action. The images from the left to the right are: the original\nimage, the image with 30 self-supervised keypoints, the image with 13 supervised keypoints, and\nthe reconstructed image from the self-supervised task. The arrows between keypoints indicate their\ncorrespondences obtained from the afÔ¨Ånity matrix with the Transformer. Warmer color indicates\nhigher conÔ¨Ådence.\nreinitializing strategy to simulate shorter videos. We observe that for Human 3.6M, the performance\nof our model increases as the number of unlabeled samples grows. Similar results appear in BBC\nPose except that the performance reduces slightly after using more than 600 frames in Ô¨Åne-tuning.\nThe reason is that the changes of the person images in BBC Pose are very small over time, which\nleads to overÔ¨Åtting.\nImprovement in Online Scenario. Figure 3 shows the improvement curve within each test video\nin the online scenario with respect to the ID (n-th update) of frames in TTP. We compute the metric\ngap between our method using TTP and baseline without TTP for each ID. The results are averaged\nacross all the test videos. In Human 3.6M, we report on a single subject S9. The curves are smoothed\nto reduce variance for better visualization. The result suggests the gap keeps increasing within a\nsingle test video, as the model updates at every frame. Moreover, in later frames, the model has seen\nmore test samples, which helps enlarge the performance gap.\nIn Human 3.6M, which has much more samples in a single person domain, the performance improves\nat the beginning but the improvement starts to drop a bit at 600 time steps due to overÔ¨Åtting in later\nframes. This phenomenon is expected since the examples in Human 3.6M are also quite similar. Note\nthat the gap still exists for later frames, it is only the improvement that becomes smaller.\nTransfer Across Datasets. We test the generalization of the model by conducting the full training\non one dataset and applying TTP on another dataset. We train our model on the Penn Action, and\nthen perform TTP to transfer the model to Human 3.6M. As shown in Table 6, our method using\nTTP can improve over the baselines by a large margin. Our ofÔ¨Çine approach achieves around 8%\nimprovement over the baseline without using Transformer and TTP. This shows TTP can signiÔ¨Åcantly\nhelp generalize and adapt the pose estimator across data from different distribution.\nUpdate Iterations. We show the ablation on update iterations in Table 2. Note that in our online\nscenario setting, we only update the model once for every incoming test image. We present results\nwhere we update more iterations in Table 2. It suggests that more update iterations do not help much.\nSpeciÔ¨Åcally, for Penn Action the performance drops when we update 3 to 4 iterations. The reason\n9\nTable 5: Smoothing results for Penn Action. Our\nmethod is complementary to temporal methods.\nMethod TTP Scenario mPCK w/ smoothing\nBaseline w/o TTP 85.23 85.68 (+0.45)\nTransformer w/o TTP 86.16 86.58 (+0.42)\nTransformer Online 87.75 88.31 (+0.56)\nTransformer OfÔ¨Çine 88.98 89.51 (+0.53)\nTable 6: The results of transferring the\nmodel trained on Penn to Human3.6M.\nMethod TTP Scenario mPCK\nBaseline w/o TTP 52.60\nTransformer w/o TTP 56.27\nTransformer Online 60.14\nTransformer OfÔ¨Çine 61.04\nis, in each step of online setting, we only perform training on one single frame, which can lead to\noverÔ¨Åtting to a particular image.\n4.5 Comparisons with Video Models\nTable 3: Comparisons with state-of-\nthe-art on Penn Action.\nMethod Penn Action\nBaseline 85.2\nOurs 89.0\nvideo-based methods\nIqbal et al. [25] 81.1\nSong et al. [52] 96.5\nLuo et al. [39] 97.7\nTable 4: Comparisons with state-of-\nthe-art on BBC Pose. Result with\n(*) is reported in [26]. Ours (best)\nis with extra unlabeled samples.\nMethod BBC Pose\n*Charles et al. [9] 79.9\nBaseline 88.7\nOurs 92.5\nOurs (best) 93.1\nvideo-based methods\nPÔ¨Åster et al. [48] 88.0\nCharles et al. [10] 95.6\nVisualization. We provide visualization on Penn Action ex-\nperiments in Figure 5. We visualize the self-supervised key-\npoints and the supervised keypoints (2nd and 3rd columns). The\narrows from the self-supervised keypoints and supervised key-\npoints indicate the keypoint correspondence, according to the\nafÔ¨Ånity matrix in the Transformer. We show arrows (correspon-\ndence) where the probability is larger than 0.1 in the afÔ¨Ånity\nmatrix. We use warmer color to indicate larger conÔ¨Ådence for\nboth keypoints and arrows. The last column visualizes the re-\nconstructed target image in our self-supervised task, which has\nthe size as the network inputs cropped from the original images.\nSee Appendix B for more visualization results.\nComplementary to Temporal Methods. Even though our\nmethod is designed for single image input and requires no\nconsecutive frames like videos, it is complementary to tempo-\nral methods such as 3D convolution [ 47] or smoothing tech-\nniques [48]. We apply Savitzky‚ÄìGolay Ô¨Ålter for smoothing\nalong with our methods for demonstration. In Table 5 we\npresent the results on Penn Action, as Penn Action is the only\ndataset here with completely consecutive test samples. After\nsmoothing, our method presents a similar improvement to base-\nline. The increase in accuracy when performing smoothing\nis too small so the performance gain of our method does not\ncome from temporal information and can be further improved\ncombined with temporal methods.\nComparisons with State-of-the-Art. In Table 3 and Table 4 we compare our best results with\nstate-of-the-art models on Penn Action and BBC Pose. Note that most of the methods on both datasets\nuse multiple video frames as inputs and use larger input resolutions, which makes them not directly\ncomparable with our method. We report the results for references. We argue that our approach with\nsingle image input has more Ô¨Çexibility and can be generalized beyond video data. Most works on\nHuman 3.6M focus on 3D pose estimation thus are not reported.\n5 Conclusion\nIn this paper, we propose to personalize human pose estimation with unlabeled test samples during\ntest time. Our proposed Test-Time Personalization approach is Ô¨Årstly trained with diverse data, and\nthen updated during test time using self-supervised keypoints to adapt to a speciÔ¨Åc subject. To\nenhance the relation between supervised and self-supervised tasks, we propose a Transformer design\nthat allows supervised pose estimation to be directly improved from Ô¨Åne-tuning self-supervised\nkeypoints. Our proposed method shows signiÔ¨Åcant improvement over baseline on multiple datasets.\nSocietal Impact. The proposed method improves the performance on the human pose estimation\ntask, which has a variety of applications such as falling detection, body language understanding,\nautonomously sports and dancing teaching. Furthermore, our method utilizes unlabeled personal data\nand thus can be deployed locally, reducing human effort and avoiding privacy violation. However,\nthe proposed method can also be used for malicious purposes, such as surveillance. To avoid possible\nharmful applications, we are committed to limit our methods from any potential malicious usage.\n10\nAcknowledgments and Funding Transparency Statement. This work was supported, in part, by\ngifts from Qualcomm, TuSimple and Picsart.\nReferences\n[1] Bruno Artacho and Andreas Savakis. Unipose: UniÔ¨Åed human pose estimation in single images\nand videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7035‚Äì7044, 2020. 3\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016. 5\n[3] David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, and\nAntonio Torralba. Semantic photo manipulation with a generative image prior. ACM Trans.\nGraph., 38(4), 2019. 3\n[4] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet: Non-local networks\nmeet squeeze-excitation networks and beyond. In Proceedings of the IEEE/CVF International\nConference on Computer Vision Workshops, pages 0‚Äì0, 2019. 3\n[5] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Openpose: realtime\nmulti-person 2d pose estimation using part afÔ¨Ånity Ô¨Åelds. IEEE transactions on pattern analysis\nand machine intelligence, 43(1):172‚Äì186, 2019. 2\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In European Conference on\nComputer Vision, pages 213‚Äì229. Springer, 2020. 3\n[7] Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, and Jitendra Malik. Human pose estimation\nwith iterative error feedback. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 4733‚Äì4742, 2016. 1, 2\n[8] James Charles, Tomas PÔ¨Åster, Mark Everingham, and Andrew Zisserman. Automatic and\nefÔ¨Åcient human pose estimation for sign language videos. International Journal of Computer\nVision, 110(1):70‚Äì90, 2014. 2, 6\n[9] James Charles, Tomas PÔ¨Åster, D. Magee, David C. Hogg, and Andrew Zisserman. Domain\nadaptation for upper body pose tracking in signed tv broadcasts. In BMVC, 2013. 6, 10\n[10] James Charles, Tomas PÔ¨Åster, Derek Magee, David Hogg, and Andrew Zisserman. Personalizing\nhuman video pose estimation. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3063‚Äì3072, 2016. 1, 3, 10\n[11] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya\nSutskever. Generative pretraining from pixels. In International Conference on Machine\nLearning, pages 1691‚Äì1703. PMLR, 2020. 3\n[12] Xianjie Chen and Alan Yuille. Articulated pose estimation by a graphical model with image\ndependent pairwise relations. arXiv preprint arXiv:1407.3399, 2014. 6\n[13] Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, and Jian Sun. Cascaded\npyramid network for multi-person pose estimation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 7103‚Äì7112, 2018. 2\n[14] Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S Huang, and Lei Zhang.\nHigherhrnet: Scale-aware representation learning for bottom-up human pose estimation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n5386‚Äì5395, 2020. 2\n[15] Xiao Chu, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. Structured feature learning for\npose estimation. In CVPR, 2016. 1, 2\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018. 3\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 3\n11\n[18] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by\npredicting image rotations. arXiv preprint arXiv:1803.07728, 2018. 1, 4\n[19] Rohit Girdhar, Georgia Gkioxari, Lorenzo Torresani, Manohar Paluri, and Du Tran. Detect-and-\ntrack: EfÔ¨Åcient pose estimation in videos. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 350‚Äì359, 2018. 3\n[20] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Aleny√†, Pieter Abbeel, Alexei A. Efros,\nLerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. In\nInternational Conference on Learning Representations, 2021. 3\n[21] Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick. Mask r-cnn. In Proceedings of\nthe IEEE international conference on computer vision, pages 2961‚Äì2969, 2017. 2\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770‚Äì778, 2016. 7\n[23] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\ndetection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 3588‚Äì3597, 2018. 3\n[24] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large\nscale datasets and predictive methods for 3d human sensing in natural environments. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 36(7):1325‚Äì1339, jul 2014. 2, 6\n[25] Umar Iqbal, Martin Garbade, and Juergen Gall. Pose for action-action for pose. In 2017 12th\nIEEE International Conference on Automatic Face & Gesture Recognition (FG 2017), pages\n438‚Äì445. IEEE, 2017. 10\n[26] Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea Vedaldi. Unsupervised learning of\nobject landmarks through conditional image generation. arXiv preprint arXiv:1806.07823, 2018.\n2, 3, 6, 7, 10\n[27] Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea Vedaldi. Self-supervised learning of\ninterpretable keypoints from unlabelled videos. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8787‚Äì8797, 2020. 3\n[28] Hanwen Jiang, Shaowei Liu, Jiashun Wang, and Xiaolong Wang. Hand-object contact con-\nsistency reasoning for human grasps generation. arXiv preprint arXiv:2104.03304 , 2021.\n3\n[29] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer\nand super-resolution. In European conference on computer vision, pages 694‚Äì711. Springer,\n2016. 4\n[30] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Exemplar Ô¨Åne-tuning for 3d human model\nÔ¨Åtting towards in-the-wild 3d human pose estimation. arXiv preprint arXiv:2004.03686, 2020.\n3\n[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014. 7\n[32] Tejas D Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds,\nAndrew Zisserman, and V olodymyr Mnih. Unsupervised learning of object keypoints for\nperception and control. Advances in neural information processing systems, 32:10724‚Äì10734,\n2019. 3\n[33] Ke Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, and Zhuowen Tu. Pose recognition\nwith cascade transformers. arXiv preprint arXiv:2104.06976, 2021. 3\n[34] Sijin Li and Antoni B Chan. 3d human pose estimation from monocular images with deep\nconvolutional neural network. In Asian Conference on Computer Vision , pages 332‚Äì347.\nSpringer, 2014. 6\n[35] Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xiaolong Wang, Ming-Hsuan Yang, and\nJan Kautz. Online adaptation for consistent mesh reconstruction in the wild. arXiv preprint\narXiv:2012.03196, 2020. 1, 3\n[36] Yunzhu Li, Antonio Torralba, Animashree Anandkumar, Dieter Fox, and Animesh Garg. Causal\ndiscovery in physical systems from videos. arXiv preprint arXiv:2007.00631, 2020. 3\n12\n[37] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg\nHeigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with\nslot attention. arXiv preprint arXiv:2006.15055, 2020. 3\n[38] Dominik Lorenz, Leonard Bereska, Timo Milbich, and Bjorn Ommer. Unsupervised part-based\ndisentangling of object shape and appearance. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 10955‚Äì10964, 2019. 3\n[39] Yue Luo, Jimmy Ren, Zhouxia Wang, Wenxiu Sun, Jinshan Pan, Jianbo Liu, Jiahao Pang, and\nLiang Lin. Lstm pose machines. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 5207‚Äì5215, 2018. 10\n[40] Lucas Manuelli, Yunzhu Li, Pete Florence, and Russ Tedrake. Keypoints into the fu-\nture: Self-supervised correspondence in model-based reinforcement learning. arXiv preprint\narXiv:2009.05085, 2020. 3\n[41] Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, and Zhibin Wang. Tfpose:\nDirect human pose estimation with transformers. arXiv preprint arXiv:2103.15320, 2021. 3\n[42] Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, and Xiaolong\nWang. A-sdf: Learning disentangled signed distance functions for articulated shape representa-\ntion. arXiv preprint arXiv:2104.07645, 2021. 3\n[43] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose\nestimation. In European conference on computer vision, pages 483‚Äì499. Springer, 2016. 1, 2, 5\n[44] Xuecheng Nie, Jiashi Feng, Jianfeng Zhang, and Shuicheng Yan. Single-stage multi-person\npose machines. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 6951‚Äì6960, 2019. 2\n[45] George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris\nBregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the wild. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages\n4903‚Äì4911, 2017. 2\n[46] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku,\nand Dustin Tran. Image transformer. In International Conference on Machine Learning, pages\n4055‚Äì4064. PMLR, 2018. 3\n[47] Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose\nestimation in video with temporal convolutions and semi-supervised training. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7753‚Äì7762,\n2019. 10\n[48] Tomas PÔ¨Åster, James Charles, and Andrew Zisserman. Flowing convnets for human pose\nestimation in videos. In Proceedings of the IEEE International Conference on Computer Vision,\npages 1913‚Äì1921, 2015. 6, 10\n[49] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and\nJonathon Shlens. Stand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909,\n2019. 3\n[50] Assaf Shocher, Shai Bagon, Phillip Isola, and Michal Irani. Ingan: Capturing and remapping\nthe \"dna\" of a natural image, 2018. 3\n[51] Assaf Shocher, Nadav Cohen, and Michal Irani. \"zero-shot\" super-resolution using deep internal\nlearning. In CVPR, pages 3118‚Äì3126. IEEE Computer Society, 2018. 3\n[52] Jie Song, Limin Wang, Luc Van Gool, and Otmar Hilliges. Thin-slicing network: A deep\nstructured model for pose estimation in videos. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 4220‚Äì4229, 2017. 10\n[53] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: A simple way to prevent neural networks from overÔ¨Åtting. Journal of Machine\nLearning Research, 15(56):1929‚Äì1958, 2014. 5\n[54] Lucas StofÔ¨Ç, Maxime Vidal, and Alexander Mathis. End-to-end trainable multi-instance pose\nestimation with transformers. arXiv preprint arXiv:2103.12115, 2021. 3\n[55] Omer Sumer, Tobias Dencker, and Bjorn Ommer. Self-supervised learning of pose embeddings\nfrom spatiotemporal relations in videos. In Proceedings of the IEEE International Conference\non Computer Vision, pages 4298‚Äì4307, 2017. 3\n13\n[56] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A\njoint model for video and language representation learning. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 7464‚Äì7473, 2019. 3\n[57] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning\nfor human pose estimation. In CVPR, 2019. 2\n[58] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time\ntraining with self-supervision for generalization under distribution shifts. In International\nConference on Machine Learning, pages 9229‚Äì9248. PMLR, 2020. 1, 2, 3, 7\n[59] Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep neural\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 1653‚Äì1660, 2014. 1, 2\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need.arXiv preprint arXiv:1706.03762,\n2017. 2, 3\n[61] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Fully\ntest-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020. 1, 3\n[62] Manchen Wang, Joseph Tighe, and Davide Modolo. Combining detection and tracking for\nhuman pose estimation in videos. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11088‚Äì11096, 2020. 3\n[63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition , pages\n7794‚Äì7803, 2018. 3\n[64] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Convolutional pose\nmachines. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,\npages 4724‚Äì4732, 2016. 1, 2\n[65] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines for human pose estimation and\ntracking. In Proceedings of the European conference on computer vision (ECCV) , pages\n466‚Äì481, 2018. 2, 5\n[66] Sen Yang, Zhibin Quan, Mu Nie, and Wankou Yang. Transpose: Towards explainable human\npose estimation by transformer. arXiv preprint arXiv:2012.14214, 2020. 3\n[67] Wei Yang, Shuang Li, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. Learning feature\npyramids for human pose estimation. In proceedings of the IEEE international conference on\ncomputer vision, pages 1281‚Äì1290, 2017. 2\n[68] Xianfang Zeng, Yusu Pan, Mengmeng Wang, Jiangning Zhang, and Yong Liu. Realistic face\nreenactment via self-supervised disentangling of identity and pose. In Proceedings of the AAAI\nConference on ArtiÔ¨Åcial Intelligence, volume 34, pages 12757‚Äì12764, 2020. 3\n[69] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea\nFinn. Adaptive risk minimization: A meta-learning approach for tackling group distribution\nshift. arXiv preprint arXiv:2007.02931, 2020. 1, 3\n[70] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European\nconference on computer vision, pages 649‚Äì666. Springer, 2016. 4\n[71] Weiyu Zhang, Menglong Zhu, and Konstantinos G Derpanis. From actemes to action: A\nstrongly-supervised representation for detailed action understanding. In Proceedings of the\nIEEE International Conference on Computer Vision, pages 2248‚Äì2255, 2013. 2, 6\n[72] Yuting Zhang, Yijie Guo, Yixin Jin, Yijun Luo, Zhiyuan He, and Honglak Lee. Unsupervised\ndiscovery of object landmarks as structural representations. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 2694‚Äì2703, 2018. 3\n[73] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 10076‚Äì10085, 2020. 3\n[74] Xingyi Zhou, Qixing Huang, Xiao Sun, Xiangyang Xue, and Yichen Wei. Towards 3d human\npose estimation in the wild: A weakly-supervised approach. In The IEEE International\nConference on Computer Vision (ICCV), Oct 2017. 6\n14\n[75] Xingyi Zhou, Dequan Wang, and Philipp Kr√§henb√ºhl. Objects as points. arXiv preprint\narXiv:1904.07850, 2019. 2\n15\nA Pipeline of the Alternative Method\nFor clariÔ¨Åcation, we show the alternative method we discussed and compared the proposed method\nwith. It is denoted as Feat. shared (keypoint) in Section 4.3. Instead of using a Transformer to model\nthe relation between two sets of keypoints, we simply use a supervised head œàsup to predict Hsup.\nTwo tasks are only connected by sharing a feature backbone.\nùúô\nùúô!\"\"ùêπ!\"##\nùúì#$%&ùêπ'\nùêª#(\"\nùêª#$%&\nùêπ$%#\nùúô)$*+$)\ntargetsource\n‚Ñí#$%&\n‚Ñí#(\"\nreconstruction\nSelf-supervised TaskSupervised Task\nùêº'\nùêº, 'ùêº'\nùúì#(\"\nFigure 6: The alternative pipeline Feat. shared (keypoint) we discussed and compared the proposed\nmethod with.\nB Visualization\nIn Figure 7 and Figure 8 we visualize our predictions on Penn Action validation set. From top to\nbottom, the images are: (i) target image It, i.e. the input image. (ii) source image Is, which provides\nappearance. (iii) reconstruction ÀÜIt. (iv) self-supervised keypoints. There are 30 self-supervised\nkeypoints in our setting. (v) supervised keypoints. (vi) ground-truth.\nFor self-supervised keypoints, we show the contribution of each keypoint to the Ô¨Ånal pose estimation\nwith color. This is computed as follows. Recall that the Transformer models the relation between two\ntasks as the afÔ¨Ånity matrix\nW ‚ààRksup√ókself\n, (11)\nwhere ksup and kself are the number of supervised and self-supervised keypoints. Also recall that\nHsup\nt = Hself\nt W‚ä§. (12)\nAn entry Wi,j actually represents the weight of j-th self-supervised keypoint in computing the i-th\nsupervised keypoint. We then deÔ¨Åne the contribution of j-th self-supervised keypoint to the Ô¨Ånal pose\nprediction as follows\ncj =\nksup\n‚àë\ni=1\nWi,j. (13)\nThe keypoints with larger cj are more important to the Ô¨Ånal pose prediction. Whereas the keypoints\nwith smaller cj are less important to the Ô¨Ånal pose prediction and serve to facilitate the self-supervised\ntask of reconstruction.\nIn Figure 7 and Figure 8 we show the self-supervised keypoints with their contribution to the\nÔ¨Ånal pose estimation in the fourth row. It is clear that the points that align with the position of\nsupervised keypoints usually have higher contribution. Other points with deviated positions have\n16\nsourcereconstructionunsup.sup.GT target\nFigure 7: Visualization of our proposed method on Penn Action validation set.\nlower contribution. They appear to serve more to facilitate the self-supervised task. Interestingly,\nsome keypoints with minor contribution locate the non-human objects in Penn Action (barbell,\nbowling ball).\nC More Implementation Details\nMore Model Details. Normally, the images Is and It are 128 √ó128. The heatmaps Hsup\nt and Hself\nt\nare 32 √ó32. In self-supervised task, appearance information Fapp\ns and keypoint information Fkp\nt has\nsize 16 √ó16 with 256 channels. For the perceptual loss, we use a VGG-16 network pretrained on\nImageNet to extract semantic informations. We do not use Ô¨Çip test during inference.\n17\nsourcereconstructionunsup.sup.GT target\nFigure 8: Visualization of our proposed method on Penn Action validation set.\n18",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7346818447113037
    },
    {
      "name": "Estimator",
      "score": 0.7049523591995239
    },
    {
      "name": "Personalization",
      "score": 0.6961965560913086
    },
    {
      "name": "Pose",
      "score": 0.6910752058029175
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6549822092056274
    },
    {
      "name": "Machine learning",
      "score": 0.5995061993598938
    },
    {
      "name": "Transformer",
      "score": 0.5737311244010925
    },
    {
      "name": "Exploit",
      "score": 0.562039852142334
    },
    {
      "name": "Supervised learning",
      "score": 0.4560615122318268
    },
    {
      "name": "Test set",
      "score": 0.43845245242118835
    },
    {
      "name": "Mathematics",
      "score": 0.11711519956588745
    },
    {
      "name": "Engineering",
      "score": 0.0983099639415741
    },
    {
      "name": "Artificial neural network",
      "score": 0.0902906060218811
    },
    {
      "name": "Statistics",
      "score": 0.08169758319854736
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 11
}