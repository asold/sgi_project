{
  "title": "Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering",
  "url": "https://openalex.org/W4323927170",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5089221572",
      "name": "Maciej P. Polak",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A5028372112",
      "name": "Dane Morgan",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2015197254",
    "https://openalex.org/W2523785361",
    "https://openalex.org/W2999645992",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W3200122731",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4384807943",
    "https://openalex.org/W4225409008",
    "https://openalex.org/W2903448849",
    "https://openalex.org/W2766362701",
    "https://openalex.org/W4220813361",
    "https://openalex.org/W4392002118",
    "https://openalex.org/W4320558475",
    "https://openalex.org/W4281476575",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2992302948",
    "https://openalex.org/W4225378608",
    "https://openalex.org/W4379087871",
    "https://openalex.org/W2936166854",
    "https://openalex.org/W3112329799",
    "https://openalex.org/W4283074682",
    "https://openalex.org/W3115677442",
    "https://openalex.org/W3127365350",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W3185307588",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4224909481",
    "https://openalex.org/W4394314011",
    "https://openalex.org/W3026048580",
    "https://openalex.org/W4311409687",
    "https://openalex.org/W3100221827",
    "https://openalex.org/W3008287297",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4281617541",
    "https://openalex.org/W4226293470",
    "https://openalex.org/W4318620992",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4280555259",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3094582681",
    "https://openalex.org/W4307139584"
  ],
  "abstract": "There has been a growing effort to replace manual extraction of data from research papers with automated data extraction based on natural language processing, language models, and recently, large language models (LLMs). Although these methods enable efficient extraction of data from large sets of research papers, they require a significant amount of up-front effort, expertise, and coding. In this work we propose the ChatExtract method that can fully automate very accurate data extraction with minimal initial effort and background, using an advanced conversational LLM. ChatExtract consists of a set of engineered prompts applied to a conversational LLM that both identify sentences with data, extract that data, and assure the data's correctness through a series of follow-up questions. These follow-up questions largely overcome known issues with LLMs providing factually inaccurate responses. ChatExtract can be applied with any conversational LLMs and yields very high quality data extraction. In tests on materials data we find precision and recall both close to 90% from the best conversational LLMs, like ChatGPT-4. We demonstrate that the exceptional performance is enabled by the information retention in a conversational model combined with purposeful redundancy and introducing uncertainty through follow-up prompts. These results suggest that approaches similar to ChatExtract, due to their simplicity, transferability, and accuracy are likely to become powerful tools for data extraction in the near future. Finally, databases for critical cooling rates of metallic glasses and yield strengths of high entropy alloys are developed using ChatExtract.",
  "full_text": "Nature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\nExtracting Accurate Materials Data from Research Papers with Conversational\nLanguage Models and Prompt Engineering\nMaciej P. Polak ∗ and Dane Morgan †\nDepartment of Materials Science and Engineering,\nUniversity of Wisconsin-Madison, Madison, Wisconsin 53706-1595, USA\nThere has been a growing effort to replace manual extraction of data from research papers with\nautomated data extraction based on natural language processing, language models, and recently,\nlarge language models (LLMs). Although these methods enable efficient extraction of data from large\nsets of research papers, they require a significant amount of up-front effort, expertise, and coding. In\nthis work we propose theChatExtract method that can fully automate very accurate data extraction\nwith minimal initial effort and background, using an advanced conversational LLM. ChatExtract\nconsists of a set of engineered prompts applied to a conversational LLM that both identify sentences\nwith data, extract that data, and assure the data’s correctness through a series of follow-up questions.\nThese follow-up questions largely overcome known issues with LLMs providing factually inaccurate\nresponses. ChatExtract can be applied with any conversational LLMs and yields very high quality\ndata extraction. In tests on materials data we find precision and recall both close to 90% from the\nbest conversational LLMs, like GPT-4. We demonstrate that the exceptional performance is enabled\nby the information retention in a conversational model combined with purposeful redundancy and\nintroducing uncertainty through follow-up prompts. These results suggest that approaches similar\nto ChatExtract, due to their simplicity, transferability, and accuracy are likely to become powerful\ntools for data extraction in the near future. Finally, databases for critical cooling rates of metallic\nglasses and yield strengths of high entropy alloys are developed using ChatExtract.\nI. INTRODUCTION\nAutomated data extraction is increasingly used to de-\nvelop databases in materials science and other fields [1].\nMany databases have been created using natural lan-\nguage processing (NLP) and language models (LMs) [2–\n24]. Recently, the emergence of large language models\n(LLMs) [25–29] has enabled significantly greater ability\nto extract complex data accurately [30, 31]. Previous au-\ntomated methods require a significant amount of effort\nto set up, either preparing parsing rules (i.e. pre-defining\nlists of rules for identifying relevant units or particular\nphrases that identify the property, etc.), fine-tuning or\nre-training a model, or some combination of both, which\nspecializes the method to perform a specific task. Fine-\ntuning is resource and time consuming and requires ex-\ntensive preparation of training data, which may not be\naccessible to the majority of researchers. With the emer-\ngence of conversational LLMs such as ChatGPT, which\nare broadly capable and pretrained for general tasks,\nthere are opportunities for significantly improved infor-\nmation extraction methods that require almost no initial\neffort. These opportunities are enabled by harnessing\nthe outstanding general language abilities of conversa-\ntional LLMs, including their inherent capability to per-\nform zero-shot (i.e., without additional training) classi-\nfication, accurate word references identification, and in-\nformation retention capabilities for text within a conver-\nsation. These capabilities, combined with prompt en-\ngineering, which is the process of designing questions\n∗ mppolak@wisc.edu\n† ddmorgan@wisc.edu\nand instructions (prompts) to improve the quality of re-\nsults, can result in accurate data extraction without the\nneed for fine-tuning of the model or significant knowledge\nabout the property for which the data is to be extracted.\nPrompt engineering has now become a standard prac-\ntice in the field of image generation [32–34] to ensure\nhigh quality results. It has also been demonstrated that\nprompt engineering is an effective method in increasing\nthe accuracy of reasoning in LLMs [35].\nIn this paper we demonstrate that using conversational\nLLMs such as ChatGPT in a zero-shot fashion with a well-\nengineered set of prompts can be a flexible, accurate\nand efficient method of extraction of materials proper-\nties in the form of the triplet Material, Value, Unit. We\nwere able to minimize the main shortcoming of these con-\nversational models, specifically errors in data extraction\n(e.g. improperly interpreted word relations) and hallu-\ncinations (i.e. responding with data not present in the\nprovided text), and achieve 90.8% precision and 87.7%\nrecall on a constrained test data set of bulk modulus,\nand 91.6% precision and 83.6% recall on a full practical\ndatabase construction example of critical cooling rates\nfor metallic glasses. These results were achieved by iden-\ntifying relevant sentences, asking the model to extract\ndetails about the presented data, and then checking the\nextracted details by asking a series of follow-up questions\nthat suggest uncertainty of the extracted information and\nintroduce redundancy. This approach was first demon-\nstrated in a preprint of this paper [36], and since then a\ngroup from Microsoft has described a similar idea, but for\nmore general tasks than materials data extraction [37].\nWe work with short sentence clusters made up of a tar-\nget sentence, the preceding sentence, and the title, as we\nhave found these almost always contain the fullMaterial,\narXiv:2303.05352v3  [cs.CL]  21 Feb 2024\nNature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\nValue, Unit triplet of data we seek. We also separated\ncases with single and multiple data values in a sentence,\nthereby greatly reducing certain types of errors. In addi-\ntion, by encouraging a certain structure of responses we\nsimplified automated post-processing the text responses\ninto a useful database. We have put these approaches\ntogether into a single method we call ChatExtract - a\nworkflow for a fully automated zero-shot approach to\ndata extraction. We provide an example ChatExtract\nimplementation in a form of a python code (See. Data\nAvailability for more details). The prompt engineering\nproposed here is expected to work for essentially all Ma-\nterial, Value, Unit data extraction tasks. For different\ntypes of data extraction this prompt engineering will\nlikely need to be modified. However, we believe that\nthe general method, which is based on simple prompts\nthat utilized uncertainty-inducing redundant questioning\napplied within an information retaining conversational\nmodel, will provide an effective and efficient approach to\nmany types of information extraction.\nThe ChatExtract method is largely independent of the\nconversational LLM used and is expected to improve as\nthe LLMs improve. Therefore, the astonishing rate of\nLLM improvement is likely to further support the adop-\ntion of ChatExtract and similar approaches to data ex-\ntraction. Prompt engineering has now become a standard\npractice in the field of image generation [32–34] to ensure\nhigh quality results. A parallel situation may soon occur\nfor data extraction. Specifically, a workflow such as that\npresented here withChatExtract, which includes prompt\nengineering utilized in a conversational set of prompts\nwith follow-up questions, may become a method of choice\nto obtain high quality data extraction results from LLMs.\nII. RESULTS AND DISCUSSION\nA. Description of the Data Extraction Workflow\nFigure 1 shows a simplified illustration of the\nChatExtract workflow. The full workflow with all of\nthe steps is shown in Fig. 2 so here we only summarize\nthe key ideas behind this workflow. The initial step is\npreparing the data and involves gathering papers, remov-\ning html/xml syntax and dividing into sentences. This\ntask is straightforward, standard for any data extraction\neffort, and described in detail in other works[31].\nThe data extraction is done in two main stages:\n(A) Initial classification with a simple relevancy prompt,\nwhich is applied to all sentences to weed out those that\ndo not contain data.\n(B) A series of prompts that control the data extrac-\ntion from the sentences categorized in stage (A) as posi-\ntive (i.e., as relevant to the materials data at hand). To\nachieve high performance in Stage (B) we have developed\na series of engineered prompts and the key Features of\nthe major Stage (B) are summarized here:\n(1) Split data into single- and multi-valued, since texts\ncontaining a single entry are much more likely to be ex-\ntracted properly and do not require follow up prompts,\nwhile extraction from texts containing multiple values is\nmore prone to errors and requires further scrutinizing and\nverification.\n(2) Include explicitly the possibility that a piece of the\ndata may be missing from the text. This is done to dis-\ncourage the model from hallucinating non-existent data\nto fulfill the task.\n(3) Use uncertainty-inducing redundant prompts that\nencourage negative answers when appropriate. This lets\nthe model reanalyze the text instead of reinforcing pre-\nvious answers.\n(4) Embed all the questions in a single conversation as\nwell as representing the full data in each prompt. This\nsimultaneously takes advantage of the conversational in-\nformation retention of the chat tool while each time re-\ninforcing the text to be analyzed.\n(5) Enforce a strict Yes/No format of answers to reduce\nuncertainty and allow for easier automation.\nStage (A) is the first prompt given to the model. This\nfirst prompt is meant to provide information whether\nthe sentence is relevant at all for further analysis, i.e.\nwhether it contains the data for the property in question\n(value and units). This classification is crucial because,\neven in papers that have been extracted to be relevant by\nan initial keyword search, the ratio of relevant to irrele-\nvant sentences is typically about 1:100. Therefore elim-\nination of irrelevant sentences is a priority in the first\nstep. Then, before starting Stage (B), we expand the\ntext on which we are operating to a passage consisting\nof three sentences: the paper’s title, the sentence preced-\ning the positively classified sentence from the previous\nprompt, and the positive sentenceitself. This expansion\nis primarily useful for making sure we include text with\nthe material’s name, which is sometimes not in the sen-\ntence targeted in Stage (A) but is most of the time in\nthe preceding sentence or title. While in some cases the\ntext passage built this way may not contain all the infor-\nmation to produce a complete datapoint, for example if\nthe materials name is mentioned earlier in the text or in\na subsection where samples are described, we found this\nto be a relatively rare occurrence. While technically ex-\npanding the passage to ensure extraction of complete dat-\napoints is possible, we found that operating on as short\nof a text passage as possible results in the most accurate\nextraction, and the small gain in recall from expanding\nthe text passage was not worth the cost of loss of preci-\nsion of overall extraction. That said, tuning of the text\nselection approach to different LLMs and/or target prop-\nerties could likely achieve improvements in some cases.\nThe relevant texts vary in their structure and we found\nit necessary to use different strategies for data extrac-\ntion for those sentences that contain a single value and\nthose sentences that contain multiple values (Feature (1)\nabove). The texts containing only a single value are much\nsimpler since the relation between material, value, and\nunit does not need to be analyzed. The LLMs tend to\nNature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\nFIG. 1. A simplified flowchart describing our ChatExtract method of extracting structured data using a conversational large\nlanguage model. Only the key ideas for each of the steps are shown, with the fully detailed workflow presented in Fig. 2\nextract such data accurately and a single well-engineered\nprompt for each of the fields asked only once tends to\nperform very well. Texts containing multiple values in-\nvolve a careful analysis of the relations between words\nto determine which values, materials, and units corre-\nspond to one another. This complexity sometimes leads\nto errors in extraction or hallucinated data and requires\nfurther scrutiny and prompting with follow-up questions.\nThus the first prompt in Stage (2) aims at determining\nwhether there are multiple data points included in a given\nsentence, and based on that answer one of two paths is\ntaken, different for single-valued and multi-valued sen-\ntences. As a concrete example of how often this happens,\nour bulk modulus dataset studied below has 70% multi-\nvalued and 30% single-valued sentences. Our follow-up\nquestion approach proved to be very successful for the\nconversational ChatGPT models.\nNext, the text is analyzed. For a single-valued text,\nwe directly ask questions about the data in the text, ask-\ning separately for the value, its unit, and the material’s\nname. It is important to explicitly allow for an option\nof a negative answer (Feature (2) above), reducing the\nchance that the model provides an answer even though\nnot enough data is provided, limiting the possibility of\nhallucinating the data. If a negative answer is given to\nany of the prompt questions, the text is discarded and no\ndata is extracted. For the case of a multi-valued sentence,\ninstead of directly asking for data, we ask the model to\nprovide structured data in a form of a table. This helps\norganize the for further processing but can produce fac-\ntually incorrect data, even if explicitly allowing negative\nresponses. Therefore, we scrutinize each field in the pro-\nvided table by asking follow-up questions (this is the re-\ndundancy of Feature (3) above) whether the data and\nits referencing is really included in the provided text.\nAgain, we explicitly allow for a negative answer and, im-\nportantly, plant a seed of doubt that it is possible that\nthe extracted table may contain some inaccuracies. Simi-\nlarly as before, if any of the prompt answers are negative,\nwe discard the sentence. It is important to notice that\ndespite the capability of the conversational model to re-\ntain information throughout the conversation, we repet-\nitively provide the text with each prompt (Feature (4)\nabove). This repetition helps in maintaining all of the de-\ntails about the text that is being analyzed, as the model\ntends to pay less attention to finer details the longer the\nconversation is continued. The conversational aspect and\ninformation retention improves the quality of the answers\nand reinforces the format of short structured answers and\npossibility of negative responses. The importance of the\ninformation retention in a conversation is proven later\nin this work by repeating our analysis exercise but with\na new conversation started for each prompt, in which\ncases both precision and recall are significantly lowered.\nIt is also worth noticing that we enforce a strictly Yes\nor No format of answers for follow up questions (Fea-\nture (5) above), which enables automating of the data\nextraction process. Without this constraint the model\ntends to answer in full sentences which are challenging\nto automatically analyze.\nThe prompts described in the flowchart (Fig. 2) are\nengineered by optimizing the accuracy of the responses\nthrough trial and error on various properties of varying\ncomplexity. Obviously, we have not exhausted all op-\ntions, and it is likely that further optimization is possi-\nble. We have, however, noticed that contrary to intu-\nition, providing more information about the property in\nthe prompt usually results in worse outcomes, and we be-\nlieve that the prompts proposed here are a reliable and\ntransferable set for many data extraction tasks.\nB. Performance Evaluation and Model Comparison\nWe have investigated the performance of the\nChatExtract approach on multiple property examples,\nincluding bulk modulus, metallic glass critical cooling\nrate, and high-entropy alloy yield stress. For bulk modu-\nlus the data is highly restricted so we can collect complete\nstatistics on performance, and the other two cases rep-\nresent applications of the method to full database gen-\neration. The bulk modulus test dataset has been cho-\nsen as a representative and particularly demanding test\ncase for several reasons. Papers investigating mechani-\ncal properties, such as bulk modulus, very often report\nother elastic properties, such as the Young’s modulus or\nshear modulus, which have similar names, ranges of val-\nues, and the same units of pressure, and are therefore\neasy to confuse with bulk modulus. In addition, those\nsource documents very often describe measurements per-\nformed under pressure and other forms of stress, which\nhave the same pressure units as bulk modulus. Finally,\nbulk modulus data is very often accompanied with infor-\nmation on the derivative of bulk modulus, which is easily\nconfused as well. Therefore, the bulk modulus serves\nas a test example in which the sought property is often\nNature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\nFIG. 2. A flowchart describing our ChatExtract method of extracting structured data using a conversational large language\nmodel. Blue boxes represent prompts given to the model, grey boxes are instructions to the user, ”Yes”, ”No”, and ”None”\nboxes are model’s responses. The bold text in ”[]” are to be replaced with appropriate values of the named item, which includes\none of sentence (the target sentence being analyzed); text (the expanded text consisting of Title, Preceding sentence, and target\nsentence); name of the property; extracted material, value, or unit.\nNature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\npresented alongside numerous other, irrelevant but very\neasily mistakable values, presenting a challenge for accu-\nrate extraction. Our bulk modulus example data is taken\nfrom a large body of sentences extracted from hundreds\nof papers with bulk modulus data. To allow for an ef-\nfective assessment we wanted a relatively small number\nof relevant sentences (containing data) of around a 100,\nfrom which we could manually extract to provide ground\ntruth. We manually extracted data until we reached 100\nrelevant sentences, during which a corresponding number\nof 9164 irrelevant sentences (not containing data) was\nalso labeled. We then post-processed the irrelevant sen-\ntences to remove ones that do not contain any numbers\nwith a simple regular expression to obtain 1912 irrelevant\nsentences containing numbers. This preserves resources\nand saves time by not running the language model on\nsentences that obviously do not contain any data at all\n(since the values to be extracted are numbers), and does\nnot impact the results of the assessment, as in our exten-\nsive tests the model never returns any datapoints from\nsentences that do not contain numbers. The model is\nexplicitly instructed not to do so in the prompts (see\nFig. 2), even if it mistakenly classifies the sentence as\nrelevant in the very first classification prompt (which is\nvery rarely the case for sentences without numbers). In\nthese 100 sentences with data, there were a total of 179\ndata points (a complete triplet of material, bulk modu-\nlus, and unit combination), which we extracted by hand\nto serve as a ground truth data set. We investigated the\nperformance of multiple versions of ChatGPT models (see\nTab. I) by following the approach as described above and\nin Fig. 2. For true positive sentences we divide the results\ninto categories by type of text: single-valued and multi-\nvalued, and provide the overall performance over the en-\ntire dataset. These results are summarized in Tab. I.\nNote that single- and multi-values columns represent per-\nformance on input passages that have data, which is of\ninterest for understanding model behavior. The statis-\ntic that best represents the model performance on real\nsentences is the overall column, where the input contains\nsentences both with and without data. We applied what\nwe consider to be quite stringent criteria for assessing the\nperformance against ground truth, the details of which\nan be found in the Methods section.\nThe best LLM ( ChatGPT-4) achieved 90.8% precision\nat 87.7% recall, which is very impressive for a zero-shot\napproach that does not involve any fine-tuning. Single-\nvalued sentences tend to be extracted with slightly higher\nrecall (100% and 85.5% in ChatGPT-4 and ChatGPT-3.5,\nrespectively) compared to multi-valued sentences with a\nrecall of 82.7% and 55.9% for the same models.\nWe believe that there are two core features of ChatGPT\nthat are being used in ChatExtract to make this ap-\nproach so successful. The first feature, and we believe the\nmost important one, is the use of redundant prompts that\nintroduce the possibility of uncertainty about the previ-\nously extracted data. By engineering the set of prompts\nand follow-up questions in this way, they substantially\nimprove the factual correctness, and therefore precision,\nof the extracted information. The second feature is the\nconversational aspect, in which information about pre-\nvious prompts and answers is retained. This allows the\nfollow-up questions to relate to the entirety of the con-\nversation, including the model’s previous responses.\nIn order to demonstrate that the follow-up questions\napproach is essential to the good performance we re-\npeated the exercise for both ChatGPT models without\nany follow-up questions (directly asking for structurized\ndata only, in the same manner as before, only without\nasking the follow-up prompts in the multi-value branch\n(long light green box on right side of Fig. 2)). The re-\nsults are denoted as (no follow-up) in Tab. I. The dom-\ninant effects of removing follow-up questions is to allow\nmore extracted triplets to make it to the final extracted\ndatabase. This generally incre/ases recall across all cases\n(single-valued, multi-valued, and overall). For passages\nwith data (single-valued, multi-valued) these additional\nkept triplets are very few and almost all correct, lead-\ning to just slightly lower precisions. However, for the\nlarge number of passages with no data the additional kept\ntriplets represent many false positives, and therefore dra-\nmatically reduce precision in the overall category. Specif-\nically, removing follow-up questions decreases the overall\nprecision to just 42.7% and 26.5% for ChatGPT-4 and\nChatGPT-3.5, respectively, from the values of 90.8% and\n70.1%, respectively, resulting from a full ChatExtract\nworkflow. These large reductions in precision demon-\nstrate that follow-up questions are critical, and the anal-\nysis shows that their role is primarily to avoid the model\nerroneously hallucinating data in passages where none\nwas actually given.\nIn order to demonstrate that the information retention\nprovided by the conversational model is important to the\ngood performance we repeated our approach but modi-\nfied it to start a new conversation for each prompt, which\nmeant that no conversation history was available during\neach prompt response. The results are denoted (no chat)\nin Tab. I, This test was performed on ChatGPT-3.5 and\nhad little or no reduction in precision. However, there\nwas a significant loss in recall in all categories (e.g., over-\nall recall dropped by 10.7% to 54.7%). This loss is recall\nis because the multiple redundant questions tend to re-\nject too many cases of correctly extracted triplets when\nthe questions are asked without model knowing they are\nconnected through a conversation. We did not perform\nthis test onChatGPT-4 to reduce overall time and expense\nas the implications results on ChatGPT-3.5 seemed clear.\nWhile currently the OpenAI GPT models, in particu-\nlar GPT4, are considered to be the most capable and\nare the most widely used, the fact that they are en-\ntirely proprietary, with a limited access dependent on\nOpenAI servers, and of limited transparency on their\ninternal workings. Their default versions also tend to\nchange their performance over time [38], which we over-\ncome by using version snapshots (see Sec. IV. Meth-\nods), however there is no guarantee for their availabil-\nNature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\nity in the future. As an alternative model to assess,\nwe chose LLaMA2-chat (70B) [39], a model developed\nby GenAI (Meta), which has extensive documentation\n[40], and is available to download for free and use locally\nwithout limits. The performance of the LLaMA2-chat\nmodel is summarized alongside other models in Tab. I,\nwhere an overall precision and recall of 61.5% and 62.9%\nwas achieved. The performance is close, but slightly\nworse than that of ChatGPT-3.5, which is expected based\non the overall assessment of LLaMA2 capabilities [29].\nWhile the ChatGPT-4 model is still the most capable and\nperforms with significantly better outcomes, this demon-\nstrates that alternative models are also capable of data\nextraction, and their accuracy is likely to improve as they\ncatch up to the state-of-the-art. It is worth noting that\nalthough the method and the prompts have been de-\nveloped to be general and applicable to any LLM, the\nprompts have been optimized based on GPT models.\nTherefore, it is possible that further prompt optimiza-\ntion could improve performance for a specific LLM, such\nas LLaMA2-chat presented here, making the compari-\nson not entirely fair for LLaMA2-chat. However, we do\nnot expect this effect to be very significant. In order\nto compare the performance of ChatExtract to previous\nstate-of-the-art data extraction methods, we performed\nan assessment of the performance of ChemDataExtrac-\ntor2 (CDE2) on our test bulk modulus dataset. CDE2\nrequires, at minimum, a specifier expression and units to\nbe explicitly specified. The specifier expression used here\nincluded all the ways we found the bulk modulus is ad-\ndressed in our test data: ’bulk modulus’, ’B’, ’B0’, ’B 0’,\n’K’, ’K0’, and ’K 0’. We also created a new unit type\nSingle-\nvalued\nMulti-\nvalued Overall\nChatGPT-4\n(gpt-4-0314)\nP=100%\nR=100%\nP=100%\nR=82.7%\nP=90.8%\nR=87.7%\nChatGPT-3.5\n(gpt-3.5-turbo-0301)\nP=100%\nR=88.5%\nP=97.3%\nR=55.9%\nP=70.1%\nR=65.4%\nLLaMA2-chat\n(70B)\nP=74.1%\nR=87.7%\nP=87.3%\nR=53.5%\nP=61.5%\nR=62.9%\nChatGPT-4 (no follow-up)\n(gpt-4-0314)\nP=100%\nR=100%\nP=99.2%\nR=98.4%\nP=42.7%\nR=98.9%\nChatGPT-3.5 (no follow-up)\n(gpt-3.5-turbo-0301)\nP=97.9%\nR=88.5%\nP=94.0%\nR=74.0%\nP=26.5%\nR=78.2%\nChatGPT-3.5 (no chat)\n(gpt-3.5-turbo-0301)\nP=100%\nR=76.9%\nP=86.6%\nR=45.7%\nP=70.0%\nR=54.7%\nTABLE I. Precision (P) and recall (R) for different types of\ntext passages: containing single- and multi-valued data, and\noverall, which includes all analyzed text passages, both con-\ntaining data and not. Bold font represents final results of\nmodels used within the ChatExtract workflow, while the re-\nmaining demonstrate the importance of redundant follow-up\nquestioning (no follow-up) and conversational information re-\ntention aspect (no chat).\nfor units of pressure, which included all units we encoun-\ntered in our test data: ’GPa’, ’MPa’, ’Pa’, ’kbar’, and\n’bar’. CDE2 was then ran on the same text passages from\nour bulk modulus dataset as ChatExtract. The overall\nprecision and recall were found to be 57% and 31% re-\nspectively, slightly lower but close to the low range results\nreported for thermoelectric properties (78% and 31%, re-\nspectively) obtained in Ref. [6] by the authors of CDE2.\nWe note that in this paper we use a more strict definition\nfor a false negative datapoint than the authors of CDE2,\nwhich results in a slightly lower recall. Even though the\nperformance of ChatExtract is better, it is worth noting\nthat CDE2 can be efficiently executed on a personal com-\nputer with a single CPU, while the use of LLMs at the\ntime of writing this article requires significantly higher\ncomputational power.\nC. Application to Tables and Figures\nData is not necessarily always contained within the\ntext of the paragraph, and may be found in other struc-\ntures, in particular in tables and figures. Since tables\nalready contain structured datapoints, LLMs can cer-\ntainly assist in their efficient extraction from the docu-\nment. The analysis of figures, on the other hand, is not a\nlanguage processing task, and is an ongoing challenge for\nmachine learning and artificial intelligence. LLMs can,\nhowever, help identify relevant figures for further human\nanalysis. Figure 3 shows workflows for tables (a) and\nfigures (b). Here, we utilize a simple workflow for table\nextraction - tables and their captions are gathered sep-\narately from the texts of the papers, and then they are\nused in classification, in a similar fashion to sentences\n(first step in the general ChatExtract workflow, Fig. 2\nfor whether they do contain the relevant data or not.\nIn a case of positive classification, the text of the table\nand its caption is provided to the LLM and the model is\ninstructed to only extract the relevant data for the spec-\nified property, in the form of a table, in the same way\nas in the general ChatExtract workflow. This step en-\nsures that only the relevant data is extracted, as tables\noften contain more than just one column or one property\nand have to be further postprocessed. Since the data is\nalready structured and the probability of an incorrect ex-\ntraction is low, the redundant follow-up verification does\nnot seem to be helpful and is not performed, similar to\nour approach for sentences for single values. For figures,\nonly the figure caption is used in the classification, where\nIn the case of a positive classification of a figure caption,\nthe figure is downloaded for later manual data extraction.\nThe accuracy for table extraction using the model which\nperformed best for text extraction (GPT4) is quite high,\nas extracting structured data from an already structured\ntable poses fewer challenges than extraction from texts.\nOut of 163 tables contained in the same papers which\nserved as a source for the text bulk modulus data, we\nmanually classified 58 as containing bulk modulus data.\nNature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\nFIG. 3. A flowchart describing our ChatExtract method of extracting structured data from: (a) tables, (b) figures; using a\nconversational large language model. Blue boxes represent prompts given to the model, grey boxes are instructions to the user,\n”Yes”, ”No” boxes are model’s responses. The bold text in ”[]” are to be replaced with appropriate values of the named item.\nFrom these tables we were able to manually extract 500\nstructured bulk modulus datapoints. Using ChatExtract\nwe were able to achieve a precision and recall for table\nclassification of 95% and 98%. The precision and recall\nwhen counting structured data extraction for individual\ndatapoints reached 91% and 89%, respectively. The low-\nering of the statistics, besides the consequence of the spo-\nradic improper classification, was almost entirely due to\nan improper formatting of tables when converted from\nxml to raw text. While it did not happen very often,\nin the cases when it did, it was impossible for humans\nto extract data from these wrongly formatted tables as\nwell. Even though these are not directly the method’s\nfault, they are still counted as false positives and false\nnegatives in our assessment, as they will inevitably be\npresent in the final extracted database, and this is what\nultimately matters the most. Assessment of accuracy for\nfigure classification is more difficult, as figures usually\npresent more complex data than the simple ’material,\nvalue, unit’ triplets we discuss here. Therefore the crite-\nrion for a successful classification was whether the figure\ncontained the relevant property on any of the axes, in\nthe legend, written somewhere in the figure, or in the\ncaption itself. Out of 436 figures contained in the same\npapers which served as a source for the text bulk modulus\ndata, we manually classified 45 as containing bulk modu-\nlus data. Using the model which performed best for text\nextraction (GPT4) we found a 82% recall and 80% pre-\ncision for the figure relevancy classification. While these\nresults are very encouraging, it is worth noting that this is\nnot full data extraction from figures, which is a very chal-\nlenging task overall. In the case of our test bulk modulus\ndata, for example, the bulk modulus was often contained\nin the pressure or energy as a function of volume plots as\none of the parameters in the fitted equation of state, sim-\nply written next to the curve, while the figure caption de-\nscribing the figure only says that it contains the pressure\nor energy as a function of volume. While a human with\nknowledge in the field knows that such figures represent\nequations of state and bulk modulus is one of the param-\neters in the equation of state and may expect its value in\nsuch a plot, which even a human without expertise would\nnot be able to do. Nevertheless, in our evaluation, we\nconsidered such figures as relevant and containing data,\nwhich negatively impacted the recall. Interestingly most\nof the reduction in precision came from a similar reason\n- the figure would be explicitly captioned as containing a\nfitted equation of state curve, and a model would classify\nsuch a figure positively (since bulk modulus is the key pa-\nrameter in the fitting) yet the figure would not directly\ncontain the bulk modulus data.\nTo further demonstrate the utility of the ChatExtract\napproach we used the method to extract two materials\nproperty databases, one for critical cooling rates of bulk\nmetallic glasses and one for yield strength of high entropy\nalloys. Before sharing the results of these data extrac-\ntions it is useful to consider in more detail different types\nof desirable database of a materials property that might\nbe extracted from text.\nDifferent types of databases can be achieved with dif-\nferent levels of post-processing after automated data ex-\ntraction. Here we describe three type of databases that\nwe believe cover most database use cases. At one extreme\nis a database that encompasses all relevant mentions of\na specific property, which is useful when initiating re-\nsearch in a particular field to assess the extent of data\nNature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\navailable in the literature, the span of values (including\noutliers), and the various material groups investigated.\nEntries included in such database might contain ranges,\nlimits or approximate values, families of materials, etc.\nThis is typically what the ChatExtract directly extracts,\nand we will refer to this as the raw database. At the\nother extreme is a strict standardized database, which\ncontains uniquely defined materials with standard for-\nmat, machine-readable compositions, and discrete val-\nues (i.e. not ranges or limits) with standardized units\n(which also helps remove the very rare occurrence where\na triplet with wrong units is extracted). A standard-\nized database facilitates easy interpretation and usage\nby computer codes and might be well-suited to, e.g., ma-\nchine learning analysis. A standardized database can be\ndeveloped from a raw data collection and will be a sub-\nset of that raw data. A third type of dataset, which is\nintermediate between raw and standardized, is a cleaned\ndatabase, which removes duplicate triplets derived from\nwithin a single paper from the raw data, as these are al-\nmost always the exact same entry repeated multiple, e.g.\nin the Discussion and Conclusions sections) and are ob-\nviously undesirable. While the cleaned database can be\ndone automatically, the standardized database may re-\nquire some manual post-processing of the data extracted\nwith the ChatExtract method.\nD. Results of Real-life Data Extraction\nIn this study we provide two materials property\ndatabases: critical cooling rates of metallic glasses, and\nyield strength of high entropy alloys. Both databases\nare presented in all three of the above forms: raw data\nwhich is what is directly extracted by the ChatExtract\napproach, cleaned data from which single paper dupli-\ncate values have been removed, and standardized data\nwhere all materials that were uniquely identifiable are in\na standard form of AXBYCZ... (where A,B,C,... are el-\nements and X,Y,Z,... are mole fractions). The standard-\nization required post-processing which we accomplished\nmanually and with a combination of further prompting\nwith an LLM, text processing with regular expression\nand pymatgen [41]. While this standardization approach\nmay introduce some additional errors, it provides a very\nuseful form for the data with modest amounts of human\neffort. While we were able to employ further prompting\ncombined with LLMs and text analysis tools to make this\nconversion, the approaches necessitate substantial addi-\ntional prompt engineering and coding, which was time\nconsuming and likely not widely applicable without sig-\nnificant changes. Given these limitations of our present\napproach to generating a standardized database from a\nraw database we do not discuss the details of our ap-\nproach or attempt to provide a guide on how to do this\nmost effectively. Automating developing a standardized\ndatabase from a raw database is an important topic for\nfuture work.\nTo limit the scope of critical cooling rates to just metal-\nlic glasses, and yield strengths to high entropy alloys, we\nfirst limited the source papers by providing a specified\nquery to the publishers database to return only papers\nrelevant to the family of systems we were after, and then\nwe applied a simple composition-based filter to the final\nstandardized database. Details about both these steps\nare given in the following section when discussing the\nrespective databases.\nThe first database is of critical cooling rates in the con-\ntext of metallic glasses. In addition, the critical cooling\nrate database serves as a larger scale, real-life case assess-\nment of ChatExtract, complementing the test bulk mod-\nulus dataset in evaluating the method’s effectiveness. A\nfully manually extracted dataset of critical cooling rates\nwas prepared to serve as ground truth to compare to\nthe data extracted with ChatExtract. The details of the\ncomparison and evaluation are given later in this sec-\ntion. The critical cooling rate dataset has been chosen\ndue to several aspects that also make it a representa-\ntive and demanding example. The critical cooling rate is\noften determined as a result of experimenting with dif-\nferent cooling rates (not critical cooling rates), which are\nnumbers of similar magnitude and with the same units\nas critical cooling rates, making them very easy to con-\nfuse. In addition, critical cooling rates are sometimes\ndescribed as a cooling rate for vitrification, or a cooling\nrate for amorphization, making extraction of the proper\nvalue very challenging. Critical cooling rate is also often\npresent alongside other critical values, such as critical\ncasting diameters. The units of critical cooling rates are\nalso not straightforward as they often contain the degree\nsymbol, while the unit of time in the denominator is often\nexpressed in the form of an exponent.\nTo obtain source research articles we performed a\nsearch query ”bulk metallic glass”+”critical cooling rate”\nfrom Elsevier’s ScienceDirect database which returned\n684 papers, consisting of 110126 sentences.\nA reference database (ground truth), which we will\ncall Rc1 , was developed using a thorough manual data\nextraction process based on text processing and regular\nexpressions and aided by a previous database of critical\ncooling rates extracted with a more time consuming and\nless automated approach that involved significant human\ninvolvement [31]. This laborious process done by an ex-\nperienced researcher although highly impractical, labor-\nintensive and time consuming, is capable of providing the\nmost complete and accurate reference database, allowing\nto accurately evaluate the performance of ChatExtract\nin a real database extraction scenario, which is the most\nrelevant assessment of the method.\nTo develop the critical cooling rate database with\nChatExtract the ChatExtract approach was applied\nidentically as to the bulk modulus case except that the\nphrase ”bulk modulus” was replaced with ”critical cool-\ning rate”. We call this dataset R c2 . In comparing\nRc2 data to Rc1 ground truth the same rules for equiva-\nlency of triplet datapoints have been applied in the same\nNature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\nway as the benchmark bulk modulus data (see Meth-\nods): equivalent triplets had to have identical units and\nvalues (including inequality symbols, if present), and\nmaterial names had to be similar enough to allow en-\ntries to be uniquely identified as the same materials sys-\ntem (e.g. ”Mg 100−xCuxGd10 (x=15)” was the same as\nMg85Cu15Gd10, but not the same as ”Mg-Cu-Gd metal-\nlic glass” and ”Zr-based bulk metallic glass” was the\nsame as ”Zr-based glass forming alloy” but not the same\nas Zr41.2Ti13.8Cu12.5Ni10.0Be22.5). Critical cooling rates\nfor bulk metallic glasses proved to be quite a challeng-\ning property to extract. The analyzed papers very often\n(much more often than in the other properties we worked\non) contained values of critical cooling rates described as\nranges or limits, and the materials were often families or\nbroad group of materials, in particular in the Introduc-\ntion sections of the papers. The ChatExtract workflow is\naimed at extracting triplets of materials, value, and units\nwithout specifying further what do these mean exactly,\nas will be discussed in the next paragraph. To provide\nthe most comprehensive assessment, the human curated\ndatabase contains all mentions of critical cooling rates\nthat are accompanied by any number, no matter how\nvague or specific. This manually extracted, very challeng-\ning raw database contained 721 entries. ChatExtract\napplied on the same set of research papers resulted in\n634 extracted values with 76.9% precision and 63.1% re-\ncall. The vast majority of reduction in precision and\nrecall comes from the more ambiguous material names\nsuch as the above mentioned broad groups or families of\nmaterials or ranges and limits of values. In many cases\nthe error in extraction was minor, such as a missing in-\nequality sign (e.g. ” <0.1” in R c1 but ”0.1” in R c2 ),\nextracting only one value from a range (e.g. ”10-100”\nin Rc1 but only ”10” in R c2 ), or missing details in ma-\nterials described as a group or family (e.g. ”Zr-based\nmetallic glasses” in R c1 but only ”Metallic glasses” R c2\n). Even though these could be regarded as minor errors,\nwe still consider such triplets to be incorrect. The per-\nformance is slightly improved for the cleaned database\nwhere a precision of 78.1% and 64.3% recall is obtained\nwith 637 and 553 entries in R c1 and Rc2 , respectively.\nThe most relevant standardized version of the database,\nwhen extracted with ChatExtract yielded a final preci-\nsion of 91.9% and 84.2% recall, with 313 and 286 en-\ntries subject for comparison in R c1 and R c2 , respec-\ntively. This large reduction in the size of the standardized\ndatabase when compared to cleaned, and the improve-\nment in performance, are both due to the large amount\nof material groups/families and ranges/limits of values.\nThese cases do not classify as uniquely identifiable ma-\nterial compositions and discrete values so they do not\nsatisfy the requirements for the standardized database,\nand as mentioned before, they were the most problem-\natic for ChatExtract to extract (as they were for the\nhuman curating Rc1 ). It is important to note that in or-\nder to provide an accurate assessment of the extraction\nperformance, as mentioned previously, the triplets are\nnot matched by themselves, but they also have to orig-\ninate from the same text passage. Therefore both the\nground truth and the ChatExtract extracted databases\nwere standardized separately, and if either contained a\nstandardized value, it was considered in the assessment,\nmaking the comparison more challenging. The perfor-\nmance of ChatExtract for the standardized database of\ncritical cooling rates is close to that for bulk modulus\npresented in Tab. I and demonstrates the transferabilty\nof ChatExtract to different properties.\nIn addition, we extracted 348 raw datapoints from ta-\nbles, some which were duplicates of values already ex-\ntracted from text data, adding only 277 new points to the\nstandardized database and consisting of 97 new unique\ncompositions. We also positively classified 208 figures as\nrelevant and provided their source document and caption,\nbut data from figures has not been manually extracted.\nThe final standardized database obtained with\nChatExtract consists of 557 datapoints. Duplicate val-\nues originating from within a single paper have al-\nready been removed for the cleaned database, dupli-\ncate triplets originating from different papers are still\npresent. We believe it is important to keep all values,\nas it allows for an accurate representation of the fre-\nquency at which different systems are studied and for\naccurate averaging if necessary. If the duplicates were to\nbe removed, 309 unique triplets would be left, with the\nmany duplicates being for an industry standard system\nZr41.2Ti13.9Cu12.5Ni10Be22.5 (Vit1). The values in the fi-\nnal database ranged from 10−3 Ks−1 (for Ni40P20Zr40) to\n4.619·1013 Ks−1 (for CuZr2),with an average 10 2 Ks−1,\nall quite reasonable values. An additional standardized-\nMG database is given, in which all non-metallic mate-\nrials have been removed. In the case of this modest-\nsized database, simply removing oxygen containing sys-\ntems proved to be enough and 5 non-metallic oxide ma-\nterials have been removed. Out of the 309 unique dat-\napoints, there were 222 unique material compositions\n(some compositions had multiple values originating from\ndifferent research papers) in the standardized database,\nand after removing non-metallic systems standardized-\nMG database contained 298 unique datapoints for 217\nunique material compositions. This size of 217 unique\ncompositions is significantly larger than the previous\nlargest hand-curated database published by Afflerbach,\net al. [42], which had just 77 entries. This result shows\nthat, at least in this case, ChatExtract can generate\nmore quality data with much less time than human ex-\ntraction efforts. To further demonstrate the robustness\nof ChatExtract and compare with other methods, we ap-\nplied CDE2 on the critical cooling dataset as well. CDE2\nperformance on the critical cooling rate dataset was con-\nsistent with the previous assessment on the bulk modulus\ndataset, with overall precision and recall of 49.2% and\n35.1% respectively. The details on the usage of CDE2\ncan be found in Sec. III.\nFinally, we developed a database of yield strength of\nhigh entropy alloys (HEAs) using the ChatExtract ap-\nNature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\nproach. This database does not have any readily available\nground truth for validation but represents a very different\nproperty and alloy set than either bulk modulus or crit-\nical cooling rate and therefore further demonstrates the\nefficacy of theChatExtract approach. In the first step we\nsearched for a combination of the phrase ”yield strength”\nand (”high entropy alloys” or ”compositionally complex\nalloys” or ”multi-principle component alloys”) in the El-\nsevier’s ScienceDirect API. The search returned 4029\nresearch papers consisting of 840431 sentences. 10269\nraw data points were extracted. The cleaned database\nconsisted of 8900 datapoints. Further post-processing\nyielded 4275 datapoints that constitute the standardized\ndata, where we assumed that all compositions were given\nas atomic %, unless otherwise stated in the analyzed text\n(which was infrequent). The 4275 standardized data-\npoints contained a number of alloys that were not were\nnot HEAs, with HEA defined as a systems containing\n5 or more elements. The non-HEA systems are not an\nerror in ChatExtract as the data was generally in the\npapers, despite their being extracted by the above ini-\ntial keyword search. By restricting the database to only\nHEAs we obtained a final standardized-HEA database of\n2442 values. The standardized-HEA database had 636\nmaterials with unique compositions. The values ranged\nfrom 12 MPa for Al0.4Co1Cu0.6Ni1Si0.2 to 19.16 GPa for\nFe7Cr31Ni23Co34Mn5. These values are extreme but not\nunphysical and we have confirmed that both these ex-\ntremes are extracted correctly. The distribution of yield\nstress values resembles a positively skewed normal dis-\ntribution with a maximum around 400 MPa, which is a\nphysically reasonable distribution shape with a peak at a\ntypical yield stress for strong metal alloys. Additionally\n2456 raw datapoints were extracted from tables, many of\nwhich were duplicates of values already extracted from\ntext data, adding only 195 new unique HEA composi-\ntions. We positively classified 1848 figures as relevant\nand provide their source document and caption, but data\nfrom figures has not been manually extracted.\nA large automatically extracted database of general\nyield strengths, not specific to HEAs, has been devel-\noped previously [5]. Direct quantitative comparison is\nnot straightforward, but the histogram of values ob-\ntained from the previous database exhibits a very sim-\nilar shape to the data obtained here, further support-\ning that our data is reasonable. The database of yield\nstrengths for HEAs developed here is significantly larger\nthan databases developed for HEAs previously, for exam-\nple, databases containing yield strengths for 169 unique\nHEA compositions from 2018 [43] and containing yield\nstrength for 419 unique HEA compositions from 2020\n[44]. The ChatExtract generated databases are avail-\nable in Figshare [45] (See. Data Availability).\nNow that we developed and analyzed these databases\nit is easier to understand the utility of ChatExtract .\nChatExtract was developed to be general and transfer-\nable, therefore it tackles a fundamental type of data ex-\ntraction - a triplet of Material, Value, Unitfor a specified\nproperty, without imposing any other restrictions. The\nlack of specificity when extracting ”Material” or ”Value”\nallows for extraction of data from texts where the mate-\nrials are presented both as exact chemical compositions,\nor broad groups or families of systems. Similarly val-\nues may be discrete numbers, or ranges or limits. How-\never, certain restrictions are often desired in developing\na database, and we believe that these fall into two broad\ncategories with respect to the challenges of integrating\nthem into the present ChatExtract workflow. The first\ncategory is restrictions based on the extracted data, for\nexample, targeting only desired compositions or ranges\nof a property value. Such restrictions are trivial to in-\ntegrate with ChatExtract by either limiting the initial\nsearch query in the publisher’s database, limiting the fi-\nnal standardized database, or both, based on the restric-\ntion. For example, in our HEA database we assured only\nHEAs in final data by both limiting the search query\nin the publisher’s database and applying a composition-\nbased rule on the final standardized database. The sec-\nond category is where we want a property value when\nsome other property conditions hold, for example, the\ninitial property should be considered at a certain temper-\nature and pressure. This situation is formally straightfor-\nward for ChatExtract as it can be captured by generaliz-\ning the problem from finding the triplet: material, prop-\nerty, unit, to finding the multiplet: material, property1,\nunit1, property2, unit2, .... The ChatExtract workflow\ncan then be generalized to apply to these multiplets by\nadding more steps to both the left and right branches in\nFig. 2, for example if a temperature at which the data\nwas obtained was relevant, the left branch would con-\ntain two more boxes, the first being: Give the number\nonly without units, do not use a full sentence. If the\nvalue is not present type ”None”. What is the value of\nthe temperature at which the value of [property] is given\nin the following text?, followed by a second similar one\nprompting for the unit. The first prompt in the right\nbranch would ask for a table that also included a tem-\nperature value and temperature unit, followed by two val-\nidation prompts for those two columns. This approach\ncould be expanded into extracting non-numerical data\nas well, such as sample crystalinity or processing condi-\ntions. While these generalizations are formally straight-\nforward we have made no assessment of their accuracy\nin this work, and some changes to ChatExtract might\nbe needed to implement them effectively. For example,\nany additional constraints or information would have to\nbe included in the text being examined by the LLM, and\nthe more information that is required, the less likely it\nis that it will all be contained in the examined text pas-\nsage. Thus the examined text passage may need to be\nexpanded, or sometimes the required additional data may\nbe missing from the paper altogether.\nNature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\nE. Conclusions\nThis paper demonstrates that conversational LLMs\nsuch as ChatGPT, with proper prompt engineering and\na series of follow-up questions, such as the ChatExtract\napproach presented here, are capable of providing high\nquality materials data extracted from research texts with\nno additional fine-tuning, extensive code development\nor deep knowledge about the property for which the\ndata is extracted. We present such a series of well-\nengineered prompts and follow-up questions in this paper\nand demonstrate its effectiveness resulting in a best per-\nformance of over 90% precision at 87.7% recall on our\ntest set of bulk modulus data, and 91.6% precision and\n83.6% recall on a full database of critical cooling rates.\nWe show that the success of the ChatExtract method\nlies in asking follow-up questions with purposeful redun-\ndancy and introduction of uncertainty and information\nretention within the conversation by comparing to re-\nsults when these aspects are removed. We further develop\ntwo databases using ChatExtract - a database of criti-\ncal cooling rates for metallic glasses and yield strengths\nfor high entropy alloys. The first one was modest-sized\nand served as a benchmark for full database development\nsince we were able to compare it to data we extracted\nmanually. The second one was a large database, to our\nknowledge the largest database of yield strength of high\nentropy alloys to date. The high quality of the extracted\ndata and the simplicity of the approach suggests that\napproaches similar to ChatExtract offer an opportunity\nto replace previous, more labor intensive, methods. Since\nChatExtract is largely independent of the used model, it\nis also likely improve by simply applying it to newer and\nmore capable LLMs as they are developed in the future.\nIII. METHODS\nThe main statistical quantities used to assess perfor-\nmance of ChatExtract were precision and recall, defined\nas:\nPrecision = True Positive\nTrue Positive + False Positive\nRecall = True Positive\nTrue Positive + False Negative.\n(1)\nIn our assessment we defined true positives (for preci-\nsion) and false negatives (for recall) in terms of each in-\nput text passage, which we define above to consist of\na target sentence, its preceding sentence, and the title.\nThe exact approach can be confusing so we describe it\nconcretely for every case. For a given input text passage\nthere are zero, one or multiple unique datapoint triplets\nof material, value, and unit. We take the hand extracted\ntriplets as the ground truth. We then process the text\npassage with ChatExtract to get a set of zero or more ex-\ntracted triplets. If the ground truth has zero triplets and\nthe extracted data has zero triplets, this is a true neg-\native. Every extracted triplet from a passage with zero\nground truth triplets is counted as a false positive. If\nthe ground truth has one triplet and the extracted data\nhas zero triplets this is counted as a false negative. If\nthe ground truth has one triplet and the extracted data\nhas one equivalent triplet (we will define ”equivalent” be-\nlow) then this is counted as a single true positive. If the\nground truth has one triplet and the extracted data has\none inequivalent triplet then this is counted as a single\nfalse positive. If the ground truth has one triplet and\nthe extracted data has multiple triplets they are each\ncompared against the ground truth sequentially, assign-\ning them as a single true positive if they are equivalent to\nthe ground truth triplet and a single false positive if they\nare not equivalent to the ground truth triplet. However,\nonly one match (a match is an equivalent pair of triplets)\ncan be made of an extracted triplet to each ground truth\ntriplet for a given sentence, i.e., we consider the ground\ntruth triplet to be used up after one match. Therefore,\nany further extracted triplets that are equivalent to the\nground truth triplet are still counted as each contribut-\ning a single false positive. Finally, we consider the case\nwhere ground truth has multiple triplets. In this case, if\nthe extracted data has no triplets it is counted as a mul-\ntiple false negatives. If the extracted data has one triplet\nand it is equivalent to any ground truth triplet that is\ncounted as one true positive. If the extracted data has\nmultiple triplets each one is compared to each ground\ntruth triplet. If a given extracted triplet is equivalent\nto any one of the ground truth triplets that extracted\ntriplet is counted as a true positive. However, as above,\neach ground truth triplet can only be matched once, and\nany addition matches of extracted triplets to an already\nmatched ground truth triplet are counted as one addi-\ntional false positive.\nIn the above we defined ”equivalent” triplets in the fol-\nlowing way. First, equivalent triplets had to have identi-\ncal units and values (if uncertainty was present, it did not\nhave to be extracted, but if it was extracted it had to be\nextracted properly as well). Second, equivalent triplets\nhad to have materials names in the ground truth and ex-\ntracted text that uniquely identified the same materials\nsystem (e.g., Li 17Si(4−x)Gex (x=2.3) and Li 17Si1.7Ge2.3\nwould be equivalent but ”Zr-Ni alloy” and ”Zr 62Ni38”\nwould not). These requirements for equivalent triplets\nare quite unforgiving. In particular, in many cases where\nwe identified false positives the LLM extracted data that\nwas partially right or had just small errors. This fact sug-\ngests that better precision and recall might be obtained\nwith some human input or further processing. Overall\nwe believe the above methods provide a rigorous and de-\nmanding assessment of the ChatExtract approach.\nOpenAI ChatGPT API was used within Python 3.10.6.\nTo maximize reproducibility and consistency in responses\nwe specifically used the gpt-3.5-turbo-0301 snapshot\nmodel of GPT-3.5, and gpt-4-0314 snapshot model of\nGPT-4, in both of which the model parameters were set\nNature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\nas follows: temperature = 0.0, frequency penalty = 0.0,\npresence penalty = 0.0, top p = 1.0, logprobs = False,\nn = 1, logit bias and stop has been set to the default\nnull. For the LLaMA2-chat 70B model, temperature =\n0.0, and top p = 1.0 were used, with a batch size of 6.\nNo system prompts (empty strings) were used in any of\nthe models. When using ChemDataExtractor2 for ex-\ntracting critical cooling rates, the specifier expression was\nprepared based on how we found the critical cooling rate\nis addressed in our test data: ’critical cooling rate’, ’Rc’,\n’R c’, ’R c’, ’RC’, ’R C’, ’R C’, ’R c’, ’CCR’. A new unit\ntype for units of cooling rate was prepared, which in-\ncluded all units we encountered in our test data: ’C/s’,\n’K/min’, ’K/s’, ’Kˆ(-1)’, ’Kminˆ(-1)’, ’Ks-1’, ’Ksˆ(-1)’,\n’°C/min’, ’°C/s’, ’°Csˆ(-1)’.\nD a t a A v ailability: The extracted databases of\ncritical cooling rates of metallic glasses and yield\nstrength for HEAs, as well as data used in the\nassessment of the models is available on figshare\n[45]: https://doi.org/10.6084/m9.figshare.22213747 In\nthe case of data related to figures, we do not provide\nthe figure file due to copyrights, but instead provide the\nfigure numbers, figure captions, and and the DOI of the\nsource document, which allows for an easy and precise\nidentification of the figures. In that repository, we also\nprovide a version of the python code we used for data ex-\ntraction that follows the workflow presented in Fig. 2, and\ninvolves additional simple post-processing of theChatGPT\nresponses to follow the workflow and provide a more con-\nvenient output. The post-processing included in the ex-\nample code is relatively simple, and while it worked well\nfor the properties we studied here, there may be cases\nwhen it fails in processing responses for different datasets,\nsince occasionally ChatGPT may format its response in an\nunexpected way. The provided code is just a simple ex-\nample of how ChatExtract could be implemented and\nhas limited error handling.\nA ckno wledgments: The research in this work was\nprimarily supported by the National Science Foundation\nCyberinfrastructure for Sustained Scientific Innovation\n(CSSI) Award No. 1931298. This work used Bridges-2\n[46] at Pittsburgh Supercomputing Center through al-\nlocation MCA09X001 from the Advanced Cyberinfras-\ntructure Coordination Ecosystem: Services & Support\n(ACCESS) program, which is supported by National Sci-\nence Foundation grants #2138259, #2138286, #2138307,\n#2137603, and #2138296.\nA uthor Contributions St a tement: M. P. P.\nconceived the study, performed the modeling, tests and\nprepared/analyzed the results, D. M. guided and super-\nvised the research. Writing of the manuscript was done\nby M. P. P. and D. M.. The authors declare no competing\ninterests.\n[1] E. A. Olivetti, J. M. Cole, E. Kim, O. Kononova,\nG. Ceder, T. Y.-J. Han, and A. M. Hiszpanski, Data-\ndriven materials research enabled by natural language\nprocessing and information extraction, Applied Physics\nReviews 7, 041317 (2020).\n[2] M. C. Swain and J. M. Cole, Chemdataextractor: A\ntoolkit for automated extraction of chemical information\nfrom the scientific literature, Journal of Chemical Infor-\nmation and Modeling 56, 1894 (2016).\n[3] J. Mavraˇ ci´ c, C. J. Court, T. Isazawa, S. R. Elliott, and\nJ. M. Cole, Chemdataextractor 2.0: Autopopulated on-\ntologies for materials science, Journal of Chemical Infor-\nmation and Modeling 61, 4280 (2021).\n[4] C. Court and J. Cole, Magnetic and superconducting\nphase diagrams and transition temperatures predicted\nusing text mining and machine learning, npj Comput\nMater 6, 18 (2020).\n[5] P. Kumar, S. Kabra, and J. Cole, Auto-generating\ndatabases of yield strength and grain size using chem-\ndataextractor, Sci Data 9, 292 (2022).\n[6] O. Sierepeklis and J. Cole, A thermoelectric materials\ndatabase auto-generated from the scientific literature us-\ning chemdataextractor, Sci Data 9, 648 (2022).\n[7] J. Zhao and J. M. Cole, Reconstructing chromatic-\ndispersion relations and predicting refractive indices us-\ning text mining and machine learning, Journal of Chem-\nical Information and Modeling 62, 2670 (2022).\n[8] J. Zhao and J. Cole, A database of refractive indices and\ndielectric constants auto-generated using chemdataex-\ntractor, Sci Data 9, 192 (2022).\n[9] E. Beard and J. Cole, Perovskite- and dye-sensitized\nsolar-cell device databases auto-generated using chem-\ndataextractor, Sci Data 9, 329 (2022).\n[10] Q. Dong and J. Cole, Auto-generated database of semi-\nconductor band gaps using chemdataextractor, Sci Data\n9, 193 (2022).\n[11] E. J. Beard, G. Sivaraman, A. Vazquez-Mayagoitia,\net al., Comparative dataset of experimental and compu-\ntational attributes of uv/vis absorption spectra, Sci Data\n6, 307 (2019).\n[12] Z. Wang, O. Kononova, K. Cruse, et al., Dataset of\nsolution-based inorganic materials synthesis procedures\nextracted from the scientific literature, Sci Data 9, 231\n(2022).\n[13] H. Huo, C. J. Bartel, T. He, A. Trewartha, A. Dunn,\nB. Ouyang, A. Jain, and G. Ceder, Machine-learning ra-\ntionalization and prediction of solid-state synthesis con-\nditions, Chemistry of Materials 34, 7323 (2022).\n[14] J. E. Saal, A. O. Oliynyk, and B. Meredig, Machine learn-\ning in materials discovery: Confirmed predictions and\ntheir underlying approaches, Annual Review of Materi-\nals Research 50, 49 (2020).\n[15] D. Morgan and R. Jacobs, Opportunities and challenges\nfor machine learning in materials science, Annual Review\nof Materials Research 50, 71 (2020).\n[16] J. Zhao and J. M. Cole, Reconstructing chromatic-\ndispersion relations and predicting refractive indices us-\ning text mining and machine learning, Journal of Chem-\nical Information and Modeling 62, 2670 (2022).\nNature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\n[17] Z. Wang, O. Kononova, K. Cruse, T. He, H. Huo, Y. Fei,\nY. Zeng, Y. Sun, Z. Cai, W. Sun, and G. Ceder, Dataset\nof solution-based inorganic materials synthesis proce-\ndures extracted from the scientific literature, Scientific\nData 9, 231 (2022).\n[18] C. Karpovich, Z. Jensen, V. Venugopal, and\nE. Olivetti, Inorganic synthesis reaction condi-\ntion prediction with generative machine learning\n10.48550/ARXIV.2112.09612 (2021).\n[19] A. B. Georgescu, P. Ren, A. R. Toland, S. Zhang, K. D.\nMiller, D. W. Apley, E. A. Olivetti, N. Wagner, and\nJ. M. Rondinelli, Database, features, and machine learn-\ning model to identify thermally driven metal–insulator\ntransition compounds, Chemistry of Materials 33, 5591\n(2021).\n[20] O. Kononova, T. He, H. Huo, A. Trewartha, E. A.\nOlivetti, and G. Ceder, Opportunities and challenges of\ntext mining in materials research, iScience 24, 102155\n(2021).\n[21] E. Kim, Z. Jensen, A. van Grootel, K. Huang, M. Staib,\nS. Mysore, H.-S. Chang, E. Strubell, A. McCallum,\nS. Jegelka, and E. Olivetti, Inorganic materials synthesis\nplanning with literature-trained neural networks, Journal\nof Chemical Information and Modeling 60, 1194 (2020).\n[22] E. Kim, K. Huang, A. Saunders, A. McCallum, G. Ceder,\nand E. Olivetti, Materials synthesis insights from scien-\ntific literature via text extraction and machine learning,\nChemistry of Materials 29, 9436 (2017).\n[23] Z. Jensen, E. Kim, S. Kwon, T. Z. H. Gani, Y. Rom´ an-\nLeshkov, M. Moliner, A. Corma, and E. Olivetti, A ma-\nchine learning approach to zeolite synthesis enabled by\nautomatic literature data extraction, ACS Central Sci-\nence 5, 892 (2019).\n[24] L. P. J. Gilligan, M. Cobelli, V. Taufour, and S. San-\nvito, A rule-free workflow for the automated generation\nof databases from scientific literature (2023).\n[25] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Ka-\nplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sas-\ntry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,\nT. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu,\nC. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei, Language mod-\nels are few-shot learners 10.48550/ARXIV.2005.14165\n(2020).\n[26] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L.\nWainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, J. Schulman, J. Hilton, F. Kel-\nton, L. Miller, M. Simens, A. Askell, P. Welinder,\nP. Christiano, J. Leike, and R. Lowe, Training lan-\nguage models to follow instructions with human feedback\n10.48550/ARXIV.2203.02155 (2022).\n[27] B. Workshop, :, T. L. Scao, A. Fan, C. Akiki,\nE. Pavlick, S. Ili´ c, D. Hesslow, R. Castagn´ e, A. S.\nLuccioni, F. Yvon, and M. Gall´ e et al., Bloom: A\n176b-parameter open-access multilingual language model\n10.48550/ARXIV.2211.05100 (2022).\n[28] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen,\nS. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, T. Mi-\nhaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig,\nP. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer,\nOpt: Open pre-trained transformer language models\n10.48550/ARXIV.2205.01068 (2022).\n[29] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.\nLachaux, T. Lacroix, B. Rozi` ere, N. Goyal, E. Ham-\nbro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and\nG. Lample, Llama: Open and efficient foundation lan-\nguage models 10.48550/arXiv.2302.13971 (2023).\n[30] A. Dunn, J. Dagdelen, N. Walker, S. Lee, A. S.\nRosen, G. Ceder, K. Persson, and A. Jain, Struc-\ntured information extraction from complex scien-\ntific text with fine-tuned large language models\n10.48550/ARXIV.2212.05238 (2022).\n[31] M. P. Polak, S. Modi, A. Latosinska, J. Zhang, C.-W.\nWang, S. Wang, A. D. Hazra, and D. Morgan, Flexi-\nble, model-agnostic method for materials data extrac-\ntion from text using general purpose language models\nhttps://doi.org/10.48550/arXiv.2302.04914 (2023).\n[32] Midjourney, https://www.midjourney.com, [Online; ac-\ncessed 08-Feb-2023].\n[33] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and\nM. Chen, Hierarchical text-conditional image generation\nwith clip latents 10.48550/ARXIV.2204.06125 (2022).\n[34] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and\nB. Ommer, High-resolution image synthesis with la-\ntent diffusion models, in 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR)\n(2022) pp. 10674–10685.\n[35] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwa-\nsawa, Large language models are zero-shot reasoners\nhttps://doi.org/10.48550/arXiv.2205.11916 (2022).\n[36] M. P. Polak and D. Morgan, Extracting accurate ma-\nterials data from research papers with conversational\nlanguage models and prompt engineering, (2023),\narXiv:2303.05352.\n[37] B. Li, R. Wang, J. Guo, K. Song, X. Tan, H. Hassan,\nA. Menezes, T. Xiao, J. Bian, and J. Zhu, Deliberate\nthen generate: Enhanced prompting framework for text\ngeneration, (2023), arXiv:2305.19835.\n[38] L. Chen, M. Zaharia, and J. Zou, How is chatgpt’s be-\nhavior changing over time? (2023), arXiv:2307.09009\n[cs.CL].\n[39] H. Touvron, L. Martin, K. Stone, P. Albert, A. Alma-\nhairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,\nS. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen,\nG. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu,\nB. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn,\nS. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez,\nM. Khabsa, I. Kloumann, A. Korenev, P. S. Koura,\nM.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu,\nY. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Moly-\nbog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta,\nK. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Sub-\nramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams,\nJ. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan,\nM. Kambadur, S. Narang, A. Rodriguez, R. Stojnic,\nS. Edunov, and T. Scialom, Llama 2: Open founda-\ntion and fine-tuned chat models (2023), arXiv:2307.09288\n[cs.CL].\n[40] facebookresearch, Llama: Inference code for llama\nmodels, https://github.com/facebookresearch/llama\n(2023).\n[41] S. P. Ong, W. D. Richards, A. Jain, G. Hautier,\nM. Kocher, S. Cholia, D. Gunter, V. L. Chevrier, K. A.\nPersson, and G. Ceder, Python materials genomics (py-\nmatgen): A robust, open-source python library for mate-\nrials analysis, Computational Materials Science 68, 314\nNature Communications (2024) 15:1569 https://doi.org/10.1038/s41467-024-45914-8\n(2013).\n[42] B. T. Afflerbach, C. Francis, L. E. Schultz, J. Speth-\nson, V. Meschke, E. Strand, L. Ward, J. H. Perepezko,\nD. Thoma, P. M. Voyles, I. Szlufarska, and D. Morgan,\nMachine learning prediction of the critical cooling rate\nfor metallic glasses from expanded datasets and elemen-\ntal features, Chemistry of Materials 34, 2945 (2022).\n[43] S. Gorsse, M. Nguyen, O. Senkov, and D. Miracle,\nDatabase on the mechanical properties of high entropy\nalloys and complex concentrated alloys, Data in Brief21,\n2664 (2018).\n[44] C. K. H. Borg, C. Frey, J. Moh, T. M. Pollock, S. Gorsse,\nD. B. Miracle, O. N. Senkov, B. Meredig, and J. E. Saal,\nExpanded dataset of mechanical properties and observed\nphases of multi-principal element alloys, Scientific Data\n7, 430 (2020).\n[45] M. P. Polak and D. Morgan, Datasets and Supporting\nInformation to the paper entitled ’Using conversational\nAI to automatically extract data from research papers\n- example of ChatGPT’ 10.6084/m9.figshare.22213747\n(2023).\n[46] S. T. Brown, P. Buitrago, E. Hanna, S. Sanielevici,\nR. Scibek, and N. A. Nystrom, Bridges-2: A platform\nfor rapidly-evolving and data intensive research, in Prac-\ntice and Experience in Advanced Research Computing,\nPEARC ’21 (Association for Computing Machinery, New\nYork, NY, USA, 2021).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7655822038650513
    },
    {
      "name": "Correctness",
      "score": 0.609840989112854
    },
    {
      "name": "Data extraction",
      "score": 0.5810493230819702
    },
    {
      "name": "Natural language processing",
      "score": 0.5557137727737427
    },
    {
      "name": "Data quality",
      "score": 0.5198264718055725
    },
    {
      "name": "Data science",
      "score": 0.46300557255744934
    },
    {
      "name": "Information extraction",
      "score": 0.4286070466041565
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4053023159503937
    },
    {
      "name": "Data mining",
      "score": 0.37438854575157166
    },
    {
      "name": "Information retrieval",
      "score": 0.34661710262298584
    },
    {
      "name": "Programming language",
      "score": 0.12250587344169617
    },
    {
      "name": "Engineering",
      "score": 0.10898891091346741
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Metric (unit)",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "MEDLINE",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I135310074",
      "name": "University of Wisconsin–Madison",
      "country": "US"
    }
  ]
}