{
  "title": "Implementing a Resource-Light and Low-Code Large Language Model System for Information Extraction from Mammography Reports: A Case Study",
  "url": "https://openalex.org/W4409351450",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2979755547",
      "name": "Fabio Dennstädt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2602136299",
      "name": "Simon Fauser",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1979772484",
      "name": "Nikola Cihoric",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5115007095",
      "name": "Max Schmerder",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2347137224",
      "name": "Paolo Lombardo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2471701764",
      "name": "Grazia Maria Cereghetti",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3026599530",
      "name": "Sandro von Däniken",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1669810134",
      "name": "Thomas Minder",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5063762867",
      "name": "Jaro Meyer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743091090",
      "name": "Lawrence Chiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114568996",
      "name": "Roberto Gaio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5107584988",
      "name": "Luc Lerch",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2883952506",
      "name": "Irina Filchenko",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3037697253",
      "name": "Daniel Reichenpfader",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2080582034",
      "name": "Kerstin Denecke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5117107544",
      "name": "Caslav Vojvodic",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5117107545",
      "name": "Igor Tatalovic",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136831514",
      "name": "Andre Sander",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2251524424",
      "name": "Janna Hastings",
      "affiliations": [
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2034485990",
      "name": "Daniel M. Aebersold",
      "affiliations": [
        "University of Bern",
        "University Hospital of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A4208804203",
      "name": "Hendrik von Tengg-Kobligk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A231822142",
      "name": "Knud Nairz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2602136299",
      "name": "Simon Fauser",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1979772484",
      "name": "Nikola Cihoric",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5115007095",
      "name": "Max Schmerder",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2347137224",
      "name": "Paolo Lombardo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1669810134",
      "name": "Thomas Minder",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5063762867",
      "name": "Jaro Meyer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743091090",
      "name": "Lawrence Chiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114568996",
      "name": "Roberto Gaio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5107584988",
      "name": "Luc Lerch",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2883952506",
      "name": "Irina Filchenko",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3037697253",
      "name": "Daniel Reichenpfader",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2080582034",
      "name": "Kerstin Denecke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5117107544",
      "name": "Caslav Vojvodic",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5117107545",
      "name": "Igor Tatalovic",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136831514",
      "name": "Andre Sander",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2251524424",
      "name": "Janna Hastings",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2034485990",
      "name": "Daniel M. Aebersold",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A231822142",
      "name": "Knud Nairz",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4220658643",
    "https://openalex.org/W4388945115",
    "https://openalex.org/W3200880448",
    "https://openalex.org/W4300819100",
    "https://openalex.org/W1869282115",
    "https://openalex.org/W1979503336",
    "https://openalex.org/W3129922404",
    "https://openalex.org/W4229451765",
    "https://openalex.org/W4404183871",
    "https://openalex.org/W4403626237",
    "https://openalex.org/W4323350039",
    "https://openalex.org/W4388759569",
    "https://openalex.org/W4385266429",
    "https://openalex.org/W4399702760",
    "https://openalex.org/W4399781895",
    "https://openalex.org/W4404787535",
    "https://openalex.org/W4403298606",
    "https://openalex.org/W4401324453",
    "https://openalex.org/W4408197148",
    "https://openalex.org/W3016706368",
    "https://openalex.org/W4386910269",
    "https://openalex.org/W4407164652",
    "https://openalex.org/W4401847921",
    "https://openalex.org/W4405043570",
    "https://openalex.org/W4391971084",
    "https://openalex.org/W4294214983",
    "https://openalex.org/W4395688625",
    "https://openalex.org/W4391258561",
    "https://openalex.org/W4402713323",
    "https://openalex.org/W4396722287",
    "https://openalex.org/W4390919701",
    "https://openalex.org/W2554251665",
    "https://openalex.org/W4406856062",
    "https://openalex.org/W4382199475"
  ],
  "abstract": "ABSTRACT Background Large Language Models (LLMs) have been successfully used to extract structured data from free-text radiology reports. Most of current studies were conducted with private models accessed via Application Programming Interface (API). We aimed to evaluate the feasibility of using open-source LLMs, deployed on limited local hardware resources for extraction of structured information from free-text mammography reports, according to a Common Data Elements (CDE)-based framework. Methods Seventy-nine CDEs were defined by an interdisciplinary expert panel, reflecting real-world reporting practice. Sixty-one reports were classified by two independent researchers with 1533 classifications assigned to establish ground truth. Five different open-source LLMs deployable on a single GPU were used for data extraction using the general-classifier Python package. Extractions were performed for two different prompt approaches with classification metrics calculated overall and on subgroups. Additional analyses were conducted using thresholds for the relative probability of classifications. Results High inter-rater agreement was observed between manual classifiers (Cohen’s Kappa 0.83). Using default prompts, the LLMs achieved accuracies of 59.23–72.86%. Adapting prompts to better explain classification tasks improved performance for all models, with accuracies of 64.71–85.32%. Setting certainty thresholds further improved accuracies to &gt;90% but reduced the coverage rate to &lt;50%. Conclusion Locally deployed open-source LLMs can effectively extract information from mammography reports with good accuracy, addressing data privacy concerns while maintaining compatibility with limited computational resources. Prompt engineering substantially increases performance, highlighting the importance of optimization in clinical applications. Using a CDE-based framework provides clear semantics and structure, facilitating interoperability and consistent data extraction.",
  "full_text": "1 \n \nImplementing a Resource-Light and Low-Code Large Language \nModel System for Information Extraction from Mammography \nReports: A Case Study  \nAUTHORS: Fabio Dennstädt 1,2*, Simon Fauser 1, Nikola Cihoric 1, Max Schmerder 3, Paolo Lombardo 3, \nGrazia Maria Cereghetti 3, Sandro von Däniken 3, Thomas Minder 3, Jaro Meyer 3, Lawrence Chiang 3, \nRoberto Gaio 1, Luc Lerch 3, Irina Filchenko 4, Daniel Reichenpfader 5,6, Kerstin Denecke 6, Caslav \nVojvodic7, Igor Tatalovic 7, André Sander 8, Janna Hastings 2,9,10, Daniel M Aebersold 1, Hendrik von \nTengg-Kobligk3, Knud Nairz3 \n1 – Department of Radiation Oncology, Inselspital, Bern University Hospital, University of Bern, Bern, Switzerland. \n2 – School of Medicine, University of St. Gallen, St. Gallen, Switzerland.  \n3 – Department of Diagnostic, Interventional and Pediatric Radiology (DIPR), Inselspital, Bern University Hospital, \nUniversity of Bern, Bern, Switzerland. \n4– Department of Neurology, Inselspital, Bern University Hospital and University of Bern, Switzerland. \n5 – Institute for Patient-Centered Digital Health, Bern University of Applied Sciences, Biel/Bienne, Switzerland.  \n6 – Faculty of Medicine, University of Geneva, Geneva, Switzerland. \n7 – Wemedoo AG, Steinhausen, Switzerland \n8 – ID Berlin AG, Berlin, Germany \n9 – Institute for Implementation Science in Health Care, University of Zurich, Zurich, Switzerland.  \n10 – Swiss Institute of Bioinformatics, Lausanne, Switzerland. \n*corresponding author; e-mail: fabio.dennstaedt@insel.ch \nABSTRACT \nBackground: Large Language Models (LLMs) have been successfully used to extract structured data \nfrom free-text radiology reports. Most of current studies were conducted with private models accessed via \nApplication Programming Interface (API). We aimed to evaluate the feasibility of using open-source \nLLMs, deployed on limited local hardware resources for extraction of structured information from free-\ntext mammography reports, according to a Common Data Elements (CDE)-based framework.  \nMethods: Seventy-nine CDEs were defined by an interdisciplinary expert panel, reflecting real-world \nreporting practice. Sixty-one reports were classified by two independent researchers with 1533 \nclassifications assigned to establish ground truth. Five different open-source LLMs deployable on a single \nGPU were used for data extraction using the general-classifier Python package. Extractions were \nperformed for two different prompt approaches with classification metrics calculated overall and on \nsubgroups. Additional analyses were conducted using thresholds for the relative probability of \nclassifications. \nResults: High inter-rater agreement was observed between manual classifiers (Cohen's Kappa 0.83). \nUsing default prompts, the LLMs achieved accuracies of 59.23–72.86%. Adapting prompts to better \nexplain classification tasks improved performance for all models, with accuracies of 64.71–85.32%. \nSetting certainty thresholds further improved accuracies to >90% but reduced the coverage rate to <50%. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n2 \n \nConclusion: Locally deployed open-source LLMs can effectively extract information from \nmammography reports with good accuracy, addressing data privacy concerns while maintaining \ncompatibility with limited computational resources. Prompt engineering substantially increases \nperformance, highlighting the importance of optimization in clinical applications. Using a CDE-based \nframework provides clear semantics and structure, facilitating interoperability and consistent data \nextraction. \nKeywords: large language models, natural language processing, artificial intelligence, \nmammography, data extraction \n \nINTRODUCTION \nSynoptic reporting and automated extraction of data from radiological documents \nSynoptic or structured reporting is an approach to give a structured overview of a radiological report for a \ngiven radiological examination. Synoptic reporting has numerous advantages over free-text reporting, \nincluding improved consistency, clarity, and completeness of information [1]. Despite these advantages, \nsynoptic reporting is not widely adopted in radiology [2] due to the increased time requirements, \ntechnological challenges, as well as the lack of standardization [3], [4].  \nAutomated extraction of information from radiological reports is therefore of longstanding interest, as it \ncould automatically generate corresponding synoptic reports in a standardized and precise way from free-\ntext radiological documents [5]. Classical Natural Language Processing (NLP) techniques for text \nclassification, like bag-of-words models or term-frequency-inversed document frequency (TF-IDF) have \nachieved success in extracting standardized data (e.g., Names, Dates, etc.) [6]. However, for more \ncomplex relevant information in radiological reports (e.g., peculiarities regarding certain morphological \naspects etc.), the performance of such classical NLP approaches has been limited [7]. (Small) language \nmodels such as BERT, which, when given training for specific extraction tasks, can also perform very \nwell, albeit not in a general muti-purpose instruction-following way [8] [9]. \nRecent advances in large language models (LLMs) opened new opportunities for automation of tasks that \ntraditionally require human-level context understanding and reasoning. LLMs have been shown to be \nuseful in structuring radiology reports [10], as well as in other settings, including writing clinical letters \n[11], clinical decision-support [12], medical education [13] or screening medical literature [14]. \nFurthermore, LLMs were shown to be highly effective for extracting information from unstructured \nclinical documents, substantially facilitating the automated generation of synoptic reports [15]. Several \nstudies have demonstrated successful solutions using proprietary LLMs provided by private companies, \nsuch as OpenAI's GPT-4 [16] or Anthropic's Claude 3.5 [17], which are accessible via application \nprogramming interfaces (APIs). While such models demonstrate powerful performance, they rely on \nexternal data transmission thus raising critical concerns about clinical applications, particularly when \ndealing with sensitive patient data. To comply with data privacy regulations and prevent the transmission \nof sensitive information to external stakeholders, LLMs used within healthcare institutions should ideally \noperate on local hardware [18].  \nTherefore, the research for practical implementation is now focusing on locally deployed open-source \nLLMs for extraction of data from clinical documents [19]. However, many of the most capable LLMs are \nvery large and for several tasks a correlation between model size and performance has been reported [20]. \nMany powerful models thus demand substantial computational resources and advanced hardware \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n3 \n \ninfrastructure to operate efficiently [21]. This highlights the importance of balancing technical feasibility \nwith practical considerations in clinical applications [22]. \nClear data definitions using the Common Data Elements (CDE) concept \nRegardless of the NLP technique used for data extraction, clearly defined data concepts with precise \nmeanings are crucial. The National Institutes of Health (NIH) introduced the concept of Common Data \nElements (CDEs) in 2011. As stated in the definition of the NIH a “ Common Data Element (CDE) is a \nstandardized, precisely defined question, paired with a set of allowable responses, used systematically \nacross different sites, studies, or clinical trials to ensure consistent data collection ” [23]. Initially \ndesigned for clinical trials, CDEs are increasingly applied for the collection of real-world data (RWD) \n[24]. \nMoreover, CDEs facilitate seamless implementation in information technology systems through \nstandardized data formats such as JSON or XML, making them particularly relevant in the era of big data \nand machine learning [23].  \nFor these reasons, CDEs were chosen as the fundamental semantic concept for collection and \nmanagement of RWD within the SMARAGD (Smart Radiology Goes Digital) initiative [25]. \nSMARAGD is a collaborative effort be tween academia and industry, that leverages state-of-the-art NLP \ntechniques to extract and manage clinical data, with a particular focus on the care pathway of breast \ncancer patients undergoing radiological examinations. \nPractical implementation of LLMs for automated information retrieval from mammography \nreports in a local setting \nAs part of the SMARAGD initiative, this case study focuses on the practical realization of an LLM-based \nsystem for extracting information from free text mammography reports. By using the modular and \ninteroperable framework provided by CDEs, this system has been integrated into a broader data \nmanagement architecture, demonstrating the potential of advanced NLP techniques in enhancing clinical \nworkflows. \nThe primary objective of this work was to create a practical, easy-to-deploy, and cost-effective system for \nextracting categorical CDE values from free text radiology reports. This system ensures that all data is \nretained within the local environment and can be deployed on local hardware with limited computational \nresources. By leveraging open-source LLMs, the system eliminates external dependencies, effectively \naddressing data privacy concerns while also meeting the need for affordability in clinical applications. \nMETHODS \nDevelopment of CDE resources and data structure \nAs the first step, an interdisciplinary expert panel comprising physicians, healthcare informaticians, and \ncomputer scientists developed resources for generating synoptic reports with categorical (“Value List”) \nCDEs suitable for the clinical environment based on published NIH guidelines [23] [26].  \nThe focus of CDE development was to ensure alignment with real-world clinical documentation and \nestablished reporting standards, specifically the guidelines of the American College of Radiology (ACR) \non mammography reporting [27]. Based on previous research for defining common data elements of real-\nworld clinical documents [28], the panel created and iteratively refined CDE resources until consensus \nwas reached among the involved experts. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n4 \n \nAs the second step, CDEs were further evaluated by radiologists who used them in clinical practice for \nlocal documentation. Their feedback informed iterative revisions and the finalization of the data concepts.  \nAs the third step, data elements were organized into a hierarchical structure [29], consistent with the \nNIH's CDE framework (corresponding to CDE forms [30]). \nLLM-based data extraction system in a local setting \nGeneral approach \nAs previously shown, LLMs can be used for effective data extraction from clinical documents [31]. For \nour use-case, the LLM was implemented as a basic data extraction and classification system as part of a \nbroader framework, based on previous studies [14], [32]. We used the recently published general-\nclassifier python package [33], which allows for easy-to-use deployment of LLMs for text classification. \nIt facilitates the classification of a text (i.e., in our case the mammography report) into user-defined topics \nwith mutually exclusive categories.  \nThe Python package has been used for biomedical literature classification with high levels of accuracy \nwith details on the functionality reported previously [32]. In brief, the library provides an easy-to-use \ninterface for defining structured templates within which an LLM is executed; for each template, the LLM \nis provided with a prompt containing an instruction and the text from the medical document and is asked \nto answer a question about this text. This structure can be saved as a machine processable JSON file and \nlater be imported for performing a classification. In general, LLMs function by predicting how a text \n(being a sequence minimal text-segments, so-called tokens) is likely to be continued. The probability of \nthe next token to a given token sequence is calculated using a neural network and is based on the data \nused during model training. The probability of the next tokens associated to the value of given categories \nfor a classification task can be calculated. The LLM can be used in this way as a general classification \nsystem by selecting the category with the highest probability.  \nFigure 1a illustrates the approach for a “Value List” CDE with three possible values (categories) out of \nwhich the LLM has to select the correct value. For each of the values the probability is calculated and the \nvalue with the highest relative probability is selected. An example for such a classification task might be \nthe CDE “laterality of known breast cancer” with the possible values/answers “left”, “right”, and \n“unknown (no information provided in the document)”. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n5 \n \n \nFigure 1 a - Schematic illustration of the LLM-based approach for extracting the value of a categorical “Value List” CDE from a \nclinical document using LLMs. b – Principle of using the general-classifier framework for the data extraction. With three lines of \ncode an LLM is initialized, the data structure (in .JSON) is loaded and the clinical documents (in .CSV) are analyzed accordingly.\n \nScript and Hardware \nA simple Python script was developed to implement the approach ( Figure 1b; available on GitHub [34] ).\nBased on the data structure for the reports a corresponding .JSON structure containing the classification\ntopics, categories and hierarchies was created for the use of the general-classifier Python package\n(version 0.1.10). \nThe system was deployed on a local Linux server with an RTX6000 Nvidia GPU (with 48GB RAM) to\nensure optimal processing capacity for running an open- source LLM. The software environment\ncomprised Python (version 3.11.5) and PyTorch (version 2.4.0) [35] alongside dependencies specific to\nthe open-source LLMs used.  \nModels \nFive different modern open-\nsource LLMs were used in the study. The LLMs were selected because of\nreported high performances on different relevant benchmarks such as MMLU [36], IFEval [37] and\nGPQA [38]. \n a \nof \nly. \n). \non \nge \nto \nnt \n to \nof \nnd \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n6 \n \n• Rombos-LLM-V2.6-Qwen-14b (Rombos) is a fine-tuned version of Alibaba's Qwen2.5-14B \nmodel for precise instruction following [39]. The Qwen series excels in language understanding, \ngeneration, multilingual tasks, coding, math, and reasoning [35]. It has 14.8 billion parameters \nand a size of 29.57 GB. \n• SOLAR Pro Preview Instruct (Solar Preview) is an instruction-tuned LLM by Upstage, reported \nto be the “most intelligent LLM on a single GPU” [40]. It's a preview version of the upcoming \nSOLAR Pro model, optimized for following user instructions and generating helpful responses. It \nhas 22.1 billion parameters and a size of 44.43 GB. \n• Phi-4 is a compact AI language model developed by Microsoft released in February 2025 [41]. It \ndelivers strong performance on reasoning, coding, and math tasks despite its small size. Trained \non high-quality data, it offers capabilities comparable to much larger models while requiring \nfewer computational resources. It has 14.7 billion parameters and a size of 29.34 GB. \n• Li-14B-v0.4 (Li) is an LLM developed by the Chinese company Century Innovation [42]. Like \nthe Rombos model it is a fine-tuned version of the Qwen2.5-14B model. It strikes a balance \nbetween computational efficiency and performance capabilities. At the time of the study (state \nFebruary 2025), it was ranked on the first place of LLMs up to 15B parameters on the Open LLM \nLeaderboard provided by Huggingface [43]. It has 14.8 billion parameters and a size of 29.52 \nGB. \n• Lamarck 14B v0.7 (Lamarck) is a high-performing language model optimized for limited \nexecution on hardware with limited resources [44]. Created through sophisticated merging \ntechniques, it combines influences from multiple top models including Virtuoso-Small [45], \nDeepSeek [46], and DRT-o1 [47]. The model excels in multi-step reasoning, prose generation, \nand multilingual capabilities, using a custom toolchain of Low-Rank Adaptations (LoRAs) and \ntargeted layer merges to achieve its balanced performance profile. It has 14.8 billion parameters \nand a size of 29.51 GB. \nPrompting \nAs the LLM-based data extraction system depends on the output of the LLM, it also depends on the input \nprompt given to the model. As shown, the performance of LLMs on various tasks can be substantially \nincreased by optimization of the prompt, so-called prompt engineering [48] [49]. In a first evaluation the \ndefault prompt for classification tasks provided in the general-classifier package was used. This default \nprompt contains a general instruction to conduct a classification with the name of the topic and the \npossible categories; see also [33]. For a second evaluation run, the prompts were adapted to better \ndescribe the classification task for individual CDEs.  \nThe adapted prompts were systematically created mimicking a question-answer dialog using the \nfollowing structure: \n• INSTRUCTION – containing information that the task is to answer a specific question based on \nthe information provided in a provided German mammography report \n• QUESTION – containing the defined question that is to be answered. \n• TEXT – the text of the mammography report; based on the general-classifier  Python package the \nstring “[TEXT]” is inserted, which will be replaced with the given text for the classification. \n• ANSWER – starting the text answer which would be given to the question. This text ends the \nprompt and provides a direct context for the classification. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n7 \n \nMore sophisticated prompt-techniques (e.g., Chain-of-thought prompting or few-shot prompting) were \nnot applied. The .JSON files containing all the default and adapted prompts used in the study are available \non GitHub [34]. \nDataset and manual value assignment \nFor creation of the dataset used in the study, 61 anonymized mammography reports were used. Patients \nwere referred to breast diagnostics concerning potential tumor lesions. Based on the resulting \nmammography reports, the task of obtaining the corresponding CDE values was assigned to two \nphysicians independently. In case of disagreement between the two clinicians a final decision was made \nby the study coordinators. A final ground truth dataset with a total of 1533 CDE values assigned to the 61 \nreports was created. Conduction of the study was approved by the local ethics committee (BASEC Project \nID 2022-01621). \nStatistical Analysis \nThe Inter-rater agreement (IRA) between the two researchers was calculated using Cohen’s Kappa [50]. \nThe accuracy, as well as the micro-average values for recall and F1-Score were calculated for individual \nCDEs as well as for groups and subgroups defined in the data structure. It should be noted that not all \nCDEs had values for all reports in the dataset as some CDEs depended on the values of other CDEs \nwithin the hierarchy of the data structure (e.g. “laterality of previously conducted biopsy” is only \nprovided if the CDE “previously conducted biopsy” is “true”).  \nThe classification metrics were calculated on all evaluations given by the LLMs. In a second sub-analysis \nthe accuracies were calculated only  for the cases for which the LLM provided a high relative probability \nfor the selected category (=probability for the selected category divided by the sum of probabilities for all \ncategories). A threshold for this parameter which reflects the “certainty of the model”  when performing \nthe classification was set at 90%, 99% and 99.9% for additional sub-analyses. Apart from the \nclassification accuracies, the proportion of classifications above the thresholds (coverage rate) was \ncalculated. \nTime measurement and cost estimation of used hardware \nThe required time for the data extraction was calculated using the developed Python script [34]. A basic \ncost calculation of the used hardware infrastructure was conducted. A basic cost estimation for the used \ncomponents was conducted in Swiss Francs, with conversions into euros and US dollars based on the \nexchange rates from January 16, 2025. \nRESULTS \nCDE-based data structure for the presentation of the mammography reports \nThe finalized CDE-based data structure consisted of 79 CDEs organized into two main groups and five \nsub-groups. The structure of the analyzed mammography reports included a pre-text (anamnesis) that \noutlines the patient's history and reason for admission, followed by the main report text. Consequently, \nthe two main groups of the data structure were defined as \"Anamnesis,\" with the sub-groups \"Family \nanamnesis\" and \"Therapeutic anamnesis,\" and \"Report,\" with the sub-groups \"Breast composition,\" \n\"BIRADS,\" and \"Findings.\" A schematic overview of the data structure with a translated artificial sample \nof a mammography report is provided in Figure 2. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n8 \n \nFigure 2: Basic hierarchical structure for the data extracted from the mammography reports. Grouping of CDEs with the number \nof CDEs per group and subgroup. \n \nA detailed list of CDEs and data structure is provided in Appendix 1. \nInterrater agreement \nA high level of IRA with a Cohen’s Kappa of 0.83 was observed between the two researchers doing the\nmanual value assignment. The smallest level of agreement was seen for the group “Findings” with a value\nof 0.71. An overview of the values for the individual groups is provided in Table 1. Detailed results on all\nindividual CDEs are provided in Supplementary Table 1. \nTable 1: Inter-rater agreement on the different (sub-)groups of CDEs between the two independent researchers assigning the \nvalues. \n Cohen’s \nKappa \nn \nAnamnesis 0.90 544 \n Family anamnesis 0.94 74 \n Therapeutic anamnesis 0.90 470 \nReport 0.80 989 \n Breast composition 0.92 122 \n Findings 0.71 745 \n BIRADS 0.95 122 \n  \nOverall 0.83 1533 \n \n \n \n \nhe \nue \nall \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n9 \n \nPerformance of the LLM-based data extraction with default prompts \nUsing the same default classification prompt of the general-classifier Python package (See also [33] ) for\nall CDEs, the system achieved an overall accuracy of 72.86% for Rombos,  of 61.58% for Solar Preview,\nof 59.23 % for Phi-4, of 64.71% for Li and of 68.49% for Lamarck. Further details with results on the\nindividual groups and subgroups are presented in Figure 3. The results regarding Micro Recall and Micro\nF1-Score are provided in the Supplementary Figures 1-2. \n \n \nFigure 3: Accuracy of the LLM-based classification system with the five LLMs on the different groups, sub-groups and overall in \nclassifying the mammography reports. Results for the default prompt. \nPerformance of the LLM-based data extraction with adapted prompts \nAdaptation of the prompts to better explain the classification tasks improved the performance of the\nsystem for all models. A comparison between default and adapted prompt for the CDE “Previously\nconducted mammography mentioned?” is provided in Figure 4, for which accuracies increase from 55.74\n– 72.13% to 62.30 – 96.72%. \nUsing the adapted prompts, the system reached overall accuracies of 85.32% for Rombos, of 64.71% for\nSolar Preview, of 65.04% for Phi-4, of 77.43% for Li and of 81.60% for Lamarck. Detailed results are\npresented in the Supplementary Figures 3-5. \nfor \nw, \nhe \nro \n in \nhe \nsly \n74 \nfor \nre \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n10 \n \nSub-analysis with threshold for relative probability \nIn the sub-analysis using a threshold for the relative probability (=“certainty” ), an increase of the \naccuracy compared to the baseline was seen across all models and thresholds.  \n \nFigure 4: Comparison of default prompt provided by the general-classifier and adapted prompt better explaining the classification\ntask for the CDE “Previously conducted mammography mentioned?”. Comparisons of accuracies for the different LLMs. The \ngeneral-classifier replaces [TOPIC] with the classification topic „Zuvor erfolgte Mammographie erwähnt?“ (=”Previously \nconducted mammography mentioned?”) and [CATEGORIES] with the classification categories „ja, nein“ (=”yes, no”). \n \nUsing the default prompt, compared to baseline, focusing only on the cases with >99.9% relative \nprobability led to 86.50% accuracy (+13.64 %; coverage rate of 43.97%) for Rombos, to 94.38% for \nSolar-Preview-Instruct (+29.04%; coverage rate of 10.44%), to 80.00% for phi-4 (+32.50%; \ncoverage rate\nof 1.96%) and to 79.86% for Lamarck (+11.37%; coverage rate of 37.90%).  \nSimilar improvements were observed when using the adapted prompts with accuracies of 94.53% \n(+9.21%; 43.97% coverage rate) for Rombos, of 93.75% for Solar Preview (+29.04%; coverage rate of \n12.52%), of 97.54% for Phi-4 (+32.50%; coverage rate of 7.96%), of 92.43% for Li (+15.00%; coverage \nrate of 20.68%) and of 91.72% for Lamarck (+10.12%; coverage rate of 39.40%). \nThe results are presented in Figure 5 (default prompt) and Figure 6 (adapted prompt) for Rombos as well \nas in the Supplementary Figures 6-13 for the other LLMs. \n \non \nte \nell \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n11 \n \n \nFigure 5: Rate of accuracy and coverage for the classifications using the default prompt with the Rombos model, depending on \nthe threshold for the relative probability. \n \n \nFigure 6: Rate of accuracy and coverage for the classifications using the adapted prompt with the Rombos model, depending on \nthe threshold for the relative probability.\n \nTime and costs \nFor the default prompt, the script was executed in 2665.69 seconds for Rombos (43.70 seconds/report), in\n6581.68 seconds for Solar-Preview (107.90 seconds/report), in 2489.52 seconds for phi-4 (40. 81\nseconds/report), in 2625.25 seconds for Li (43.04 seconds/report) and in 2746.58 seconds for Lamarck\n(45.03 seconds/report). \nSimilar results were obtained for the adapted prompts with 2614.33 seconds for Rombos ( 42.86\nseconds/report), 6491.45 seconds for Solar-Preview (106.42 seconds/report), 2423.45 seconds for phi- 4\nin \n81 \nck \n86 \n4 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n12 \n \n(48.08 seconds/report), 2607.59 seconds for Li (42.75 seconds/report) and 2718.58 seconds (39.73 \nseconds/report) for Lamarck. \nThe overall hardware costs of the used components were estimated at 7600 CHF with details provided in \nTable 2.  \nTable 2: Basic cost estimation of the used hardware components.  \n Costs in CHF \nCore Components  \nCPU: Intel Core i7-13700K ≈ 350 \nMotherboard: Z790 ≈ 250 \nRAM: 64 GB DDR5 ≈ 320 \nStorage: 1 TB NVMe SSD ≈ 120 \nGPU: NVIDIA RTX 6000 \n(Ada) \nWith 48GB on-board RAM \n≈ 6000 \nPSU: 1000W ≈ 190 \nCase / Chassis: ≈ 100 \nCooling ≈ 80 \nPeripherals  \nMonitor ≈ 150 \nKeyboard & Mouse ≈ 40 \n \nTotal ≈ 7600 (ca. 8100 € or 8300 USD) \n \n \nDISCUSSION \nLLM-based data extraction \nDue to the recent advancements in the performance of LLMs these models can now successfully perform \nvarious clinical tasks that for a human would require reasoning and contextual understanding. Various \ngroups have demonstrated successful extraction of clinical data from mammography, CT and MRI reports \n[31] [51] [52]. Many of these studies have used closed LLMs only accessible via API, such as OpenAI’s \nGPT models. While API-based models provide a ready-to-use and affordable opportunity to implement \nLLMs, such systems rely on external stakeholders and (at least anonymized) data has to be sent outside of \nthe local data environment. To avoid external transmission of data the research interest has grown in \nusing locally deployed LLMs for data extraction [53] [54]. In a recent study Wo\nź nicki et al. used the \nopen-source LLM Llama-2-70B-chat to extract data from chest radiograph reports [19]. They achieved an \nF1 score of 0.70 for English and 0.68 for German reports and, similarly to our study, noticed considerable \nvariations in performance and understanding of semantics depending on the data to be extracted. \nObviously, these results are not comparable, since different data fields were extracted and different \ndatasets were used. However, the fact that we were able to achieve similar high accuracies and F1-scores \nwith LLMs having \n/i3 – ⅓  of the parameters, aligns with the overall trend in LLM research with models \nrapidly becoming more efficient and more powerful [55]. While the work of Wo ź nicki et al. was \npublished just a few months ago, Llama-2-70B-chat is not considered anymore state-of-the-art with \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n13 \n \nLlama-3 [56] and other open-source models like DeepSeek-R1 [46] released, demonstrating the rapid \npace of development in the field. \nAs part of the SMARAGD initiative, we developed an LLM-based data extraction system using an open-\nsource model executed on local hardware. We also focused on the practical implementation considering \nenvironments of limited hardware resources and addressing data privacy issues by keeping medical data \nwithin a local environment.  \nPerformance and practical implementation \nEven without any optimization, an accuracy of >70% in answering medical questions about a report was \nachieved in our study, with the possibility to adapt the prompts and improve the performance. However, \nthe system failed to consistently give the correct answers. Since a perfect system with 100% accuracy will \nnot be possible, it is therefore important to mitigate the risks and better understand the situations when \nthere is a high risk of incorrect classifications. The possibility to calculate the “relative probability” for a \ncategory, resembling the “certainty” of an LLM when doing a classification, is very valuable for this \npurpose. As we have seen, setting a threshold with high certainty of 90%, 99% or 99.9% can indeed be \nused to achieve higher performances. Depending on how critical a classification is, higher thresholds \ncould be demanded.  While this does not completely solve the issue (and may lead to an unacceptable low \ncoverage rate), the risks can be mitigated. From a practical point of view, classifications with “low \ncertainty” could be flagged for human verification. \nSome of the lowest levels of performance were seen for the group “Findings”, which is also the group \nwith the lowest IRA. This might indicate that questions that are less clear and difficult to answer for \nhumans are ambiguous for LLMs as well. \nIt should be noted that the approach itself can be easily applied to many other documents and scenarios \n(Figure 1b ). As we have seen, a performant LLM-based system for data extraction from free text of \nmammography reports can be realized with a budget for the hardware components of less than 8000 CHF. \nWhile we used an NVIDIA RTX 6000 (Ada) with 48GB on-board RAM, the Rombos, Phi-4, Li and \nLamarck models could have also been executed on a smaller (and cheaper) GPU with 32 GB on-board \nRAM. Of course, this cost estimation only includes the hardware and no personnel costs. \nThe classification script itself is easy-to-deploy using only a few lines of code [34]. The source code for \nthe project is publicly available under an open-source license and can be directly implemented for other \nscenarios of data extraction after adjustment of the relevant topics and categories.  \nWe hope that this work serves as a guide for researchers and clinicians interested in setting up a local \nLLM-based data extraction system. Active and practical participation of clinicians in the implementation \nof AI into healthcare systems will be crucial to ensure the  systems meet real-world requirements and \nultimately lead to the best possible outcome for patients [57]. \nCDEs for synoptic reporting and IT integration \nThe performance of the system depends on the individual question asked (or CDE defined). Having a \nclear data structure is fundamental for structuring medical information. It further helps to identify for \nwhich groups and CDEs the system performs well and for which it does not. CDEs are increasingly \nrelevant for collecting medical data, not only within scientific trials, but also in clinical practice [24]. \nParticularly for RWD in radiology, CDEs are highly valuable, as they provide clear semantics and \nstructure for documentation [58]. They are increasingly being used for integration of radiological AI \nsystems [59]. Due to the increasing relevance of CDEs in radiology, the Radiological Society of North \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n14 \n \nAmerica (RSNA) in collaboration with the ACR have started developing their own repository of \nradiological CDEs [60]. CDEs can build a shared framework for synoptic reporting, similar as the \ndatasets of the International Collaboration on Cancer Reporting (ICCR) are used for structured reporting \nin pathology [61] (in principle the ICCR datasets are equivalent to the CDE concept defined by the NIH). \nFurthermore, as CDEs are directly machine-readable, they can be used for implementation of information \ntechnology systems (including generative AI) [62]. LLM-based as well as classical NLP systems for data \nextraction can directly be linked [63]. As reported, the CDE concept can be successfully applied for \ncollection of RWD via implementation into clinical IT and AI systems [64], [28].  \nCDEs provide an ideal means to support data interoperability. Rather than seeing radiology reports as a \nseparate entity, information from different medical disciplines can be converted into CDE values. \nCombining extracted CDE values from various diagnostic and therapeutic paths thus may be a solution to \ncope with the challenges of data-driven medicine [25]. \nLimitations  \nOur study has several limitations. In comparison to other approaches for data extraction like rule-based \nsystems or named-entity recognition (NER), LLMs are black boxes, and it is very challenging to \nunderstand exactly, why a conclusion was r eached by the model. There might be ways address this using \nsystems like the LLM Transparency Tool of Meta [65], but the issue cannot be completely solved. \nFurthermore, LLMs are way more resource intensive and some of the CDE values in our study could \nprobably also have been accurately extracted using rule-based systems. \nIt should also be noted that the script used in this study can only extract categorical data values (”Value \nList CDEs”). To retrieve other relevant information (e.g., number values) further adaptation would be \nneeded. \nFurthermore, while 61 mammography reports from clinical practice were used, no external validation of \nthe performance with a second independent dataset was done. While 1,533 classification tasks were \nperformed by two independent researchers and the LLMs, the number of classifications for each \nindividual CDE is limited, which restricts the ability to draw more comprehensive conclusion. \nGeneralization of the results may therefore be limited. However, it is likely that the general approach \nwould work well in other settings, as the used LLMs are general models and already perform well without \nspecific optimization for our use-case. It should be noted that the LLMs used in the study were not \nspecifically trained on internal data. The recent literature on LLM-based data extraction furthermore \nsuggests that this kind of approach successfully works in various settings [31].  \nOne strength of our study is the clear definition of data concepts within a structured hierarchy using the \nCDE concept. If clear data standards are used on a broader level this can be used to facilitate \ninteroperability, clear semantics and comparison of results. \nCONCLUSIONS \nLocally deployed, open-source LLMs can effectively extract information from unstructured \nmammography reports for structuring within a CDE-based framework. By retaining all data locally, the \napproach addresses key data privacy concerns and remains compatible with settings that have limited \ncomputational resources. Prompt engineering substantially increases accuracy and highlights the \nimportance of iterative optimization in real-world clinical applications. Going forward, wider adoption of \nstandardized data definitions and locally implementable LLMs holds promise for enhancing \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n15 \n \ninteroperability, streamlining clinical workflows, and advancing research through reliable and secure data \nextraction. \n \nList of abbreviations \nACR – American College of Radiology \nAI – Artificial Intelligence \nAPI – Application Programming Interface \nCDE – Common Data Element \nIRA – Inter-rater agreement \nIT – information technology \nLLM – Large Language Model \nLoRA – Low-Rank Adaptation \nNER – Named entity recognition \nNIH – National Institutes of Health \nNLP – Natural Language Processing \nRSNA – Radiological Society of North America \nRWD – Real-world Data \n \n \n \nCode and data availability statement \nThe source code for the project as well as the .JSON files containing the relevant data about the data \nstructure with classification topics, categories and prompts are provided at GitHub [34] . Information \nregarding the used anonymized mammography reports can be obtained from the authors upon reasonable \nrequest. \n \nAuthor contributions \nConceptualization – FD, NC, MS, HVTK, DA, SD, DR, KD, KN, GM \nData structuring – FD, SF, NC, MS, HVTK, PL, SD, TM, JM, LC, RG, LL, HB, CV, IT, AS, \nDR, KD, KN \nMethodology, technical implementation – FD, MS, JM, LC, RG, LL \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n16 \n \nMethodology, creation of datasets – SF, TM \nStatistical Analysis – FD \nWriting, original draft preparation – FD, NC, MS, KN \nWriting, illustrations – FD, IF \nWriting, review and editing – All authors \nProject administration – FD, NC, KN \n \nAcknowledgement \nNot applicable. \n \nFunding \nThis study was supported by Innosuisse grant 59228.1/120.503 \"SMARAGD\". \n \nDisclosure of potential conflicts of interest \nDr. Cihoric is a technical lead for the SmartOncology project and medical advisor for Wemedoo AG, \nSteinhausen AG, Switzerland.  \nThe authors declare no other conflicts of interest. \n \nEthics approval \nAn ethics approval for the study was granted by the Ethics Committee of the Canton of Bern (BASEC \nnumber 2022-01621), in alignment with the principles outlined in the Declaration of Helsinki. \n \nInformed consent \nThe data utilized in this study comprised anonymized mammography reports. All patients provided \ninformed consent for the use of their data for research purposes, in accordance with ethical standards.\n \n \nREFERENCES \n \n1. Neri E, Granata V, Montemezzi S, Belli P, Bernardi D, Brancato B, et al. Structured reporting of x-\nray mammography in the first diagnosis of breast cancer: a Delphi consensus proposal. Radiol med. \n2022 May;127(5):471–83.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n17 \n \n2. European Society of Radiology (ESR), Dos Santos DP, Kotter E, Mildenberger P, Martí-Bonmatí L. \nESR paper on structured reporting in radiology—update 2023. Insights Imaging. 2023 Nov \n23;14(1):199.  \n3. Dromain C, Vullierme M, Hicks RJ, Prasad V, O’Toole D, De Herder WW, et al. ENETS \nstandardized (synoptic) reporting for radiological imaging in neuroendocrine tumours. J \nNeuroendocrinology. 2022 Mar;34(3):e13044.  \n4. Woller IA, Woller SC, Stevens SM, Lloyd JF, Conner KE, Gordon BH, et al. Synoptic reporting \naccuracy for computed tomography pulmonary arteriography among patients suspected of pulmonary \nembolism. J Am Coll Emerg Physicians Open. 2022 Oct;3(5):e12801.  \n5. Hassanpour S, Langlotz CP. Information extraction from multi-institutional radiology reports. \nArtificial Intelligence in Medicine. 2016 Jan;66:29–39.  \n6. Métais E. Enhancing information systems management with natural language processing techniques. \nData & Knowledge Engineering. 2002 Jun;41(2–3):247–72.  \n7. Casey A, Davidson E, Poon M, Dong H, Duma D, Grivas A, et al. A systematic review of natural \nlanguage processing applied to radiology reports. BMC Med Inform Decis Mak. 2021 Dec;21(1):179.  \n8. Kuling G, Curpen B, Martel AL. BI-RADS BERT and Using Section Segmentation to Understand \nRadiology Reports. J Imaging. 2022 May 9;8(5):131.  \n9. Reichenpfader D, Knupp J, Von Däniken S, Gaio R, Dennstädt F, Cereghetti GM, et al. Enhancing \nBERT with Frame Semantics to Extract Clinically Relevant Information from German \nMammography Reports: Algorithm Development and Validation (Preprint) [Internet]. 2024 [cited \n2025 Jan 15]. Available from: http://preprints.jmir.org/preprint/68427 \n10. Busch F, Hoffmann L, Dos Santos DP, Makowski MR, Saba L, Prucker P, et al. Large language \nmodels for structured reporting in radiology: past, present, and future. Eur Radiol [Internet]. 2024 Oct \n23 [cited 2025 Jan 18]; Available from: https://link.springer.com/10.1007/s00330-024-11107-6 \n11. Ali SR, Dobbs TD, Hutchings HA, Whitaker IS. Using ChatGPT to write patient clinic letters. The \nLancet Digital Health. 2023 Apr;5(4):e179–81.  \n12. Benary M, Wang XD, Schmidt M, Soll D, Hilfenhaus G, Nassir M, et al. Leveraging Large Language \nModels for Decision Support in Personalized Oncology. JAMA Netw Open. 2023 Nov \n17;6(11):e2343689.  \n13. Safranek CW, Sidamon-Eristoff AE, Gilson A, Chartash D. The Role of Large Language Models in \nMedical Education: Applications and Implications. JMIR Med Educ. 2023 Aug 14;9:e50945.  \n14. Dennstädt F, Zink J, Putora PM, Hastings J, Cihoric N. Title and abstract screening for literature \nreviews using large language models: an exploratory study in the biomedical domain. Syst Rev. 2024 \nJun 15;13(1):158.  \n15. Bhayana R, Nanda B, Dehkharghanian T, Deng Y, Bhambra N, Elias G, et al. Large Language \nModels for Automated Synoptic Reports and Resectability Categorization in Pancreatic Cancer. \nFowler K, editor. Radiology. 2024 Jun 1;311(3):e233117.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n18 \n \n16. OpenAI: GPT-4 [Internet]. Available from: https://openai.com/index/gpt-4/ \n17. Anthropic: Claude 3.5 [Internet]. Available from: https://www.anthropic.com/claude/sonnet \n18. Riedemann L, Labonne M, Gilbert S. The path forward for large language models in medicine is \nopen. npj Digit Med. 2024 Nov 27;7(1):339.  \n19. Wo ź nicki P, Laqua C, Fiku I, Hekalo A, Truhn D, Engelhardt S, et al. Automatic structuring of \nradiology reports with on-premise open-source large language models. Eur Radiol [Internet]. 2024 \nOct 10 [cited 2025 Mar 5]; Available from: https://link.springer.com/10.1007/s00330-024-11074-y \n20. Mahapatra J, Garain U. Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text \nGeneration: A State-of-the-Art Investigation [Internet]. arXiv; 2024 [cited 2025 Mar 5]. Available \nfrom: http://arxiv.org/abs/2407.14088 \n21. Zhou Z, Ning X, Hong K, Fu T, Xu J, Li S, et al. A Survey on Efficient Inference for Large \nLanguage Models [Internet]. arXiv; 2024 [cited 2025 Jan 15]. Available from: \nhttp://arxiv.org/abs/2404.14294 \n22. Dennstädt F, Hastings J, Putora PM, Schmerder M, Cihoric N. Implementing large language models \nin healthcare while balancing control, collaboration, costs and security. npj Digit Med. 2025 Mar \n6;8(1):143.  \n23. NIH CDE Repository [Internet]. Available from: https://cde.nlm.nih.gov/home \n24. Snyder JM, Pawloski JA, Poisson LM. Developing Real-world Evidence-Ready Datasets: Time for \nClinician Engagement. Curr Oncol Rep. 2020 May;22(5):45.  \n25. The SMARAGD project [Internet]. Available from: https://smaragd-switzerland.ch/ \n26. Common Data Elements (CDE) Program [Internet]. Available from: \nhttps://heal.nih.gov/data/common-data-elements \n27. American College of Radiology, D’Orsi CJ, Sickles EA, Mendelson EB, Morris EA, editors. ACR \nBI-RADS atlas: breast imaging reporting and data system\n/i3 ; mammography, ultrasound, magnetic \nresonance imaging, follow-up and outcome monitoring, data dictionary. 5th edition. Reston, Va.: \nACR, American College of Radiology; 2013.  \n28. Dennstädt F, Putora PM, Heuser M, Vlaskou Badra E, Baumert BG, Leiser D, et al. Extraction of \nInteroperable Data from Healthcare Documents by Identifying Common Data Elements: An Analysis \nof Radiation Therapy Planning CT Physician Order Entry Records. Oncology. 2024;102(4):327–36.  \n29. Dennstädt F, Schmalfuss M, Zink J, Hastings J, Gaio R, Schmerder M, et al. A CDE-based data \nstructure for radiotherapeutic decision-making in breast cancer [Internet]. 2025 [cited 2025 Mar 6]. \nAvailable from: http://medrxiv.org/lookup/doi/10.1101/2025.02.04.25321635 \n30. NIH: CDE Forms [Internet]. Available from: https://cde.nlm.nih.gov/form/search \n31. Reichenpfader D, Müller H, Denecke K. A scoping review of large language model based approaches \nfor information extraction from radiology reports. npj Digit Med. 2024 Aug 24;7(1):222.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n19 \n \n32. Dennstädt F, Windisch P, Filchenko I, Zink J, Putora PM, Shaheen A, et al. Application of a general \nLLM-based classification system to retrieve information about oncological trials [Internet]. 2024 \n[cited 2025 Jan 15]. Available from: http://medrxiv.org/lookup/doi/10.1101/2024.12.03.24318390 \n33. PyPI: General-Classifier Package [Internet]. Available from: https://pypi.org/project/general-\nclassifier/ \n34. GitHub: low-code-LLM-CDE-extraction-mammography [Internet]. Available from: \nhttps://github.com/Smart-Radiology-Goes-Digital-SMARAGD/low-code-LLM-CDE-extraction-\nmammography \n35. PyTorch [Internet]. Available from: https://pytorch.org/ \n36. Hendrycks D, Burns C, Basart S, Zou A, Mazeika M, Song D, et al. Measuring Massive Multitask \nLanguage Understanding [Internet]. arXiv; 2021 [cited 2025 Mar 5]. Available from: \nhttp://arxiv.org/abs/2009.03300 \n37. Zhou J, Lu T, Mishra S, Brahma S, Basu S, Luan Y, et al. Instruction-Following Evaluation for Large \nLanguage Models [Internet]. arXiv; 2023 [cited 2025 Mar 5]. Available from: \nhttp://arxiv.org/abs/2311.07911 \n38. Rein D, Hou BL, Stickland AC, Petty J, Pang RY, Dirani J, et al. GPQA: A Graduate-Level Google-\nProof Q&A Benchmark [Internet]. arXiv; 2023 [cited 2025 Mar 5]. Available from: \nhttp://arxiv.org/abs/2311.12022 \n39. Huggingface: Rombos-LLM-V2.6-Qwen-14b [Internet]. Available from: \nhttps://huggingface.co/rombodawg/Rombos-LLM-V2.6-Qwen-14b \n40. Huggingface: SOLAR Pro Preview Instruct [Internet]. Available from: \nhttps://huggingface.co/upstage/solar-pro-preview-instruct \n41. Abdin M, Aneja J, Behl H, Bubeck S, Eldan R, Gunasekar S, et al. Phi-4 Technical Report [Internet]. \narXiv; 2024 [cited 2025 Mar 5]. Available from: http://arxiv.org/abs/2412.08905 \n42. Huggingface: Li-14B v0.4 [Internet]. Availa ble from: https://huggingface.co/wanlige/li-14b-v0.4 \n43. Huggingface: OpenLLM Leaderboard [Internet]. Available from: https://huggingface.co/spaces/open-\nllm-leaderboard/open_llm_leaderboard#/ \n44. Huggingface: Lamarck-14B-v0.6Lamarck 14B v0.6 [Internet]. Available from: \nhttps://huggingface.co/sometimesanotion/Lamarck-14B-v0.6Lamarck 14B v0.6 \n45. Huggingface: Virtuoso-Small [Internet]. Available from: https://huggingface.co/arcee-ai/Virtuoso-\nSmall \n46. DeepSeek-AI, Guo D, Yang D, Zhang H, Song J, Zhang R, et al. DeepSeek-R1: Incentivizing \nReasoning Capability in LLMs via Reinforcement Learning [Internet]. arXiv; 2025 [cited 2025 Mar \n5]. Available from: http://arxiv.org/abs/2501.12948 \n47. Wang J, Meng F, Liang Y, Zhou J. DRT: Deep Reasoning Translation via Long Chain-of-Thought \n[Internet]. arXiv; 2025 [cited 2025 Mar 5]. Available from: http://arxiv.org/abs/2412.17498 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n20 \n \n48. Wang L, Chen X, Deng X, Wen H, You M, Liu W, et al. Prompt engineering in consistency and \nreliability with the evidence-based guideline for LLMs. npj Digit Med. 2024 Feb 20;7(1):41.  \n49. Promptingguide.ai [Internet]. Available from: https://www.promptingguide.ai/de \n50. McHugh ML. Interrater reliability: the kappa statistic. Biochem Med (Zagreb). 2012;22(3):276–82.  \n51. Keshavarz P, Bagherieh S, Nabipoorashrafi SA, Chalian H, Rahsepar AA, Kim GHJ, et al. ChatGPT \nin radiology: A systematic review of performance, pitfalls, and future perspectives. Diagnostic and \nInterventional Imaging. 2024 Jul;105(7–8):251–65.  \n52. Mallio CA, Bernetti C, Sertorio AC, Zobel BB. ChatGPT in radiology structured reporting: analysis \nof ChatGPT-3.5 Turbo and GPT-4 in reducing word count and recalling findings. Quant Imaging \nMed Surg. 2024 Feb 1;14(2):2096–102.  \n53. Wiest IC, Ferber D, Zhu J, Van Treeck M, Meyer SK, Juglan R, et al. Privacy-preserving large \nlanguage models for structured medical information retrieval. npj Digit Med. 2024 Sep 20;7(1):257.  \n54. Le Guellec B, Lefèvre A, Geay C, Shorten L, Bruge C, Hacein-Bey L, et al. Performance of an Open-\nSource Large Language Model in Extracting Information from Free-Text Radiology Reports. Radiol \nArtif Intell. 2024 Jul;6(4):e230364.  \n55. Ding T, Chen T, Zhu H, Jiang J, Zhong Y, Zhou J, et al. The Efficiency Spectrum of Large Language \nModels: An Algorithmic Survey [Internet]. arXiv; 2024 [cited 2025 Mar 6]. Available from: \nhttp://arxiv.org/abs/2312.00678 \n56. Grattafiori A, Dubey A, Jauhri A, Pandey A, Kadian A, Al-Dahle A, et al. The Llama 3 Herd of \nModels [Internet]. arXiv; 2024 [cited 2025 Mar 26]. Available from: http://arxiv.org/abs/2407.21783 \n57. Bhayana R. Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and \nResearch Applications. Radiology. 2024 Jan 1;310(1):e232756.  \n58. Rubin DL, Kahn CE. Common Data Elements in Radiology. Radiology. 2017 Jun;283(3):837–44.  \n59. Mehdiratta G, Duda JT, Elahi A, Borthakur A, Chatterjee N, Gee J, et al. Automated Integration of \nAI Results into Radiology Reports Using Common Data Elements. J Digit Imaging Inform med \n[Internet]. 2025 Jan 27 [cited 2025 Apr 7]; Available from: https://link.springer.com/10.1007/s10278-\n025-01414-9 \n60. Radelement.org [Internet]. Available from: https://radelement.org/home \n61. International Collaboration on Cancer Reporting (ICCR) [Internet]. Available from: \nhttps://www.iccr-cancer.org/ \n62. NCI/NIH: Semantics Series—The Role of Common Data Elements and Artificial Intelligence \n[Internet]. Available from: https://datascience.cancer.gov/news-events/blog/semantics-series-role-\ncommon-data-elements-and-artificial-intelligence \n63. Wang Y, Huang J, He H, Zhang V, Zhou Y, Hao X, et al. CDEMapper: Enhancing NIH Common \nData Element Normalization using Large Language Models [Internet]. arXiv; 2024 [cited 2025 Jan \n15]. Available from: http://arxiv.org/abs/2412.00491 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n21 \n \n64. Dennstädt F, Putora PM, Cihoric N. (Common) Data Elements in Radiation Oncology: A Systematic \nLiterature Review. JCO Clinical Cancer Informatics. 2023 Jun;(7):e2300008.  \n65. GitHub: FacebookResearch: LLM Transparency Tool [Internet]. Available from: \nhttps://github.com/facebookresearch/llm-transparency-tool \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.08.25325371doi: medRxiv preprint ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6736541390419006
    },
    {
      "name": "Code (set theory)",
      "score": 0.5998333692550659
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.5304115414619446
    },
    {
      "name": "Information extraction",
      "score": 0.5020692348480225
    },
    {
      "name": "Extraction (chemistry)",
      "score": 0.4720383882522583
    },
    {
      "name": "Mammography",
      "score": 0.4518045485019684
    },
    {
      "name": "Natural language processing",
      "score": 0.40263935923576355
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3496400713920593
    },
    {
      "name": "Programming language",
      "score": 0.3368845582008362
    },
    {
      "name": "Information retrieval",
      "score": 0.33440858125686646
    },
    {
      "name": "Medicine",
      "score": 0.13033166527748108
    },
    {
      "name": "Cancer",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Breast cancer",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": []
}