{
  "title": "Constraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference",
  "url": "https://openalex.org/W4382322807",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1986714075",
      "name": "Junyan Li",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2593484867",
      "name": "Li Lyna Zhang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2143352901",
      "name": "Jiahang Xu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2096802215",
      "name": "Yujing Wang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2328601642",
      "name": "Shaoguang Yan",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2129890741",
      "name": "Yunqing Xia",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2094451174",
      "name": "Yuqing Yang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2110394442",
      "name": "Ting Cao",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2070768212",
      "name": "Hao Sun",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2114996910",
      "name": "Weiwei Deng",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1964204209",
      "name": "Qi Zhang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2108289083",
      "name": "Mao Yang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3174488167",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3174708387",
    "https://openalex.org/W3180037928",
    "https://openalex.org/W1493526108",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3159727696",
    "https://openalex.org/W2898073868",
    "https://openalex.org/W2979691890",
    "https://openalex.org/W3167266074",
    "https://openalex.org/W3171750540",
    "https://openalex.org/W6781533629",
    "https://openalex.org/W4306317920",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W4226126941",
    "https://openalex.org/W4245255589"
  ],
  "abstract": "Deploying pre-trained transformer models like BERT on downstream tasks in\\nresource-constrained scenarios is challenging due to their high inference cost,\\nwhich grows rapidly with input sequence length. In this work, we propose a\\nconstraint-aware and ranking-distilled token pruning method ToP, which\\nselectively removes unnecessary tokens as input sequence passes through layers,\\nallowing the model to improve online inference speed while preserving accuracy.\\nToP overcomes the limitation of inaccurate token importance ranking in the\\nconventional self-attention mechanism through a ranking-distilled token\\ndistillation technique, which distills effective token rankings from the final\\nlayer of unpruned models to early layers of pruned models. Then, ToP introduces\\na coarse-to-fine pruning approach that automatically selects the optimal subset\\nof transformer layers and optimizes token pruning decisions within these layers\\nthrough improved $L_0$ regularization. Extensive experiments on GLUE benchmark\\nand SQuAD tasks demonstrate that ToP outperforms state-of-the-art token pruning\\nand model compression methods with improved accuracy and speedups. ToP reduces\\nthe average FLOPs of BERT by 8.1x while achieving competitive accuracy on GLUE,\\nand provides a real latency speedup of up to 7.4x on an Intel CPU.\\n",
  "full_text": "Constraint-aware and Ranking-distilled Token Pruning for\nEfficient Transformer Inference\nJunyan Liâˆ—\nZhejiang University\nlijunyan668@outlook.com\nLi Lyna Zhangâ€ \nMicrosoft Research\nlzhani@microsoft.com\nJiahang Xu\nMicrosoft Research\njiahangxu@microsoft.com\nYujing Wang\nMicrosoft\nyujwang@microsoft.com\nShaoguang Yan\nMicrosoft\nshaoyan@microsoft.com\nYunqing Xia\nMicrosoft\nyxia@microsoft.com\nYuqing Yang\nMicrosoft Research\nyuqing.yang@microsoft.com\nTing Cao\nMicrosoft Research\nting.cao@microsoft.com\nHao Sun\nMicrosoft\nhasun@microsoft.com\nWeiwei Deng\nMicrosoft\ndedeng@microsoft.com\nQi Zhang\nMicrosoft\nzhang.qi@microsoft.com\nMao Yang\nMicrosoft Research\nmaoyang@microsoft.com\nABSTRACT\nDeploying pre-trained transformer models like BERT on down-\nstream tasks in resource-constrained scenarios is challenging due\nto their high inference cost, which grows rapidly with input se-\nquence length. In this work, we propose a constraint-aware and\nranking-distilled token pruning method ToP, which selectively re-\nmoves unnecessary tokens as input sequence passes through layers,\nallowing the model to improve online inference speed while pre-\nserving accuracy. ToP overcomes the limitation of inaccurate token\nimportance ranking in the conventional self-attention mechanism\nthrough a ranking-distilled token distillation technique, which dis-\ntills effective token rankings from the final layer of unpruned mod-\nels to early layers of pruned models. Then, ToP introduces a coarse-\nto-fine pruning approach that automatically selects the optimal\nsubset of transformer layers and optimizes token pruning decisions\nwithin these layers through improved ğ¿0 regularization. Extensive\nexperiments on GLUE benchmark and SQuAD tasks demonstrate\nthat ToP outperforms state-of-the-art token pruning and model\ncompression methods with improved accuracy and speedups. ToP\nreduces the average FLOPs of BERT by 8.1Ã—while achieving com-\npetitive accuracy on GLUE, and provides a real latency speedup of\nup to 7.4Ã—on an Intel CPU. Code is available at here 1.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Artificial intelligence.\nâˆ—Work was done during the internship at Microsoft Research\nâ€ Corresponding author\n1https://github.com/microsoft/Moonlit/tree/main/ToP\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0103-0/23/08. . . $15.00\nhttps://doi.org/10.1145/3580305.3599284\nKEYWORDS\nTransformer; Token Pruning; Inference Acceleration\nACM Reference Format:\nJunyan Li, Li Lyna Zhang, Jiahang Xu, Yujing Wang, Shaoguang Yan, Yun-\nqing Xia, Yuqing Yang, Ting Cao, Hao Sun, Weiwei Deng, Qi Zhang, and Mao\nYang. 2023. Constraint-aware and Ranking-distilled Token Pruning for Ef-\nficient Transformer Inference. In Proceedings of the 29th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining (KDD â€™23), August\n6â€“10, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 11 pages.\nhttps://doi.org/10.1145/3580305.3599284\n1 INTRODUCTION\nPre-trained transformer models [7, 20, 22, 25] have achieved great\nsuccess for a wide variety of NLP tasks. However, the superior\nperformance comes at the cost of increasingly larger model sizes and\ncomputation overhead, making it difficult to efficiently deploy them\non different downstream tasks in various latency-critical scenarios\nsuch as online servers and edge devices.\nAccelerating transformer inference is often achieved through\nmodel compression methods such as pruning [ 19, 30], quantiza-\ntion [4, 17, 31], and knowledge distillation [15, 29]. These techniques\naim to reduce the size of the model, with quantization and distilla-\ntion resulting in a smaller, fixed model. Structured pruning, which\neliminates redundant heads or dimensions, can effectively meet\ndeployment requirements [40, 44]. However, structured pruning\nmay not guarantee optimal accuracy, particularly for small trans-\nformers or long input sequences, as the attention mechanism has\na ğ‘‚(ğ‘›2)computation complexity with input token length ğ‘›. This\nmeans a significant portion of the model must be pruned to meet\ntight deployment constraints, potentially compromising accuracy.\nRecently, a promising subfield in NLP has emerged that focuses\non reducing latency during model inference by pruning input to-\nkens. Itâ€™s based on the intuition that not all tokens in the input\nsequence are critical for making a final prediction. As tokens pass\nthrough the encoder layers, some tokens have been captured by\nother tokens via attention in the early layer and do not require\narXiv:2306.14393v1  [cs.CL]  26 Jun 2023\nKDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA Junyan Li et al.\nfuture modeling in a higher layer [9, 28]. Pruning these uninforma-\ntive tokens within each layer can increase the modelâ€™s inference\nspeed without sacrificing accuracy. Moreover, the removal of these\ntokens in each layer will also reduce the computation and memory\nrequirements in its subsequent layers, resulting in linear or even\nquadratic reductions and providing greater compression benefits.\nSome prior works [9, 10, 16, 18, 42] have examined the potential\nof layer-wise token pruning of input sequences. However, these ap-\nproaches face several limitations. First, they treat all layers equally,\nleading to a vast design space, as pruning decisions must be made\nfor each token at every layer through the use of token-level masks.\nSecond, existing methods primarily aim to minimize accuracy drop\nand use regularization loss terms to encourage maximum token\npruning [9, 10, 18], lacking effective control over the given token\nsparsity ratio. This can be problematic in real-world scenarios where\na specific sparsity ratio or deployment constraint is often required.\nFinally, existing token importance scoring criterion struggles\nto achieve both high accuracy and real inference efficiency. Atten-\ntion value-based approaches [9, 18], which utilize the self-attention\nmechanism to score token importance, can be efficient, but may\ninadvertently remove important tokens that receive little attention\nin early layers [28], leading to a huge drop in accuracy. On the other\nhand, prediction module-based approaches [10, 42], which insert\nextra neural networks to predict token importance scores, can be\nmore accurate. But they also come with the cost of introducing\nconsiderable additional inference cost (i.e., âˆ¼30% latency overhead\non GPU) and may impede overall speedup. Given the tight latency\nconstraints in many real-world deployments, attention value-based\nscoring is a more promising method for achieving efficiency. How-\never, the challenge of improving accuracy for determining token\nimportance in early layers remains unresolved.\nIn our work, we introduce ToP (Token Pruning), a deployment-\nfriendly and constraint-aware token pruning approach that ad-\ndresses all the above challenges. ToP trains an optimal token prun-\ning decision based on our improved attention value-based scoring,\nenabling dynamic removal of unnecessary tokens layer-by-layer\nduring inference, while preserving accuracy and meeting deploy-\nment constraints. Our approach incorporates two key techniques.\nFirst, we introduce a new token distillation method calledranking-\naware token distillation to enhance the ability of self-attention val-\nues to rank token importance in early layers, thereby resolving the\nissue of unintended removal of top-ranked tokens during inference\nwhich can affect the modelâ€™s accuracy. Our solution is inspired\nby the observation that attention values in deeper layers rank to-\nken importance more accurately. We thus utilize the importance\nrankings generated by the final layers of the unpruned model as\nknowledge and distill it to the early layers. Conventional distilla-\ntion methods [32, 40] commonly use the MSE loss to measure the\nlayer-wise representationsâ€™ difference between teacher and student,\nbut this may not be effective for transformer models such as BERT,\nwhich capture different levels of information across early and deep\nlayers. Directly minimizing the MSE loss of absolute attention val-\nues may lead to suboptimal results. Instead, our method proposes\na ranking-aware distillation loss that minimizes the differences in\ntoken importance rankings between the final layer of the teacher\nand the early layers of the student using the LambdaRank loss [37].\nThis distillation effectively retains the most important tokens and\nresults in significant accuracy improvements.\nNext, we present a generic learning algorithm that optimizes\ntoken pruning decisions based on our improved attention value-\nbased importance scoring. Different from prior works, we utilize\ntwo-tier binary masks, consisting of coarse-grained gate masks\nand fine-grained token ranking masks, to automatically determine\nthe optimal subset of transformer layers for fine-grained token\npruning. The gate masks act as layer selectors, while the ranking\nmasks dynamically identify which sorted tokens (i.e., based on the\nattention value scores) within the selected layers to be pruned.\nThis design allows for a more flexible pruning space and eases the\noptimization compared to learning all token masks equally. To find\nthe optimal mask values while achieving the desired pruning ratio,\nwe solve an end-to-end optimization problem using improved ğ¿0\nregularization [39], which jointly learns these masks and updates\nmodel parameters on the target downstream tasks, resulting in\nbetter model accuracy. We summarize our contributions as follows:\nâ€¢For the first time, we propose ranking-aware token distilla-\ntion to effectively improve token importance rankings based\non attention values, which greatly enhances the effectiveness\nof token pruning methods relying on attention values.\nâ€¢We further propose a constraint-aware token pruning algo-\nrithm (ToP). For a given deployment constraint, ToP auto-\nmatically selects the optimal subset of transformer layers\nand optimizes token pruning decisions within these layers\nthrough improved ğ¿0 regularization.\nâ€¢Extensive experiments on GLUE benchmark [34] and SQuAD\nv2.0 [26] demonstrate that ToP consistently outperform state-\nof-the-art token pruning and model compression baselines\nwith higher accuracy and speedups. By removing unneces-\nsary tokens, ToP improves accuracy by up to 4.3% on GLUE\nand reduces FLOPs by an average of 6.7Ã—. Furthermore, ToP\ndelivers substantial real latency reduction, with up to 7.4Ã—\nacceleration for BERT inference on CPU.\n2 RELATED WORKS\n2.1 Model Compression\nTo reduce the inference cost of pre-trained transformer models, a\nvariety of compression techniques have been proposed, including\nweight pruning [ 8, 19, 30], quantization [ 4, 17, 31] and distilla-\ntion [15, 29]. Token-level pruning has been shown to complement\nknowledge distillation and quantization [ 18]. Here, we focus on\npruning and distillation and briefly discuss the related work.\nWeight pruning is categorized into 1) unstructured and 2) struc-\ntured pruning. Unstructured methods [8, 30] achieve high sparsity\nwithout accuracy drop but offer minimal latency benefits due to\nirregular sparse patterns. In contrast, structured pruning removes\ncoherent weight groups, reducing latency without special hardware\nsupport. CoFi [40] achieves 10Ã—speedup with a small accuracy drop\nby jointly pruning layers, attention heads, FFN, and hidden units.\nSwiftPruner [44] is a latency-aware pruning method that finds opti-\nmal layer-wise pruning policies under a given latency requirement\nthrough AutoML. However, structure pruning may result in a loss\nof accuracy when the deployment requirements are highly con-\nstrained and the downstream task has a long input sequence. This\nConstraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA\nis because the model complexity increases quadratically with token\nlength. When the token length is long, the original model must be\ncompressed to a high ratio, which can cause accuracy loss.\nKnowledge distillation [12, 32, 33] aims to transfer knowledge\nfrom a large teacher model to a small student model. It is well known\nthat model pruning with a distillation objective can significantly\nimprove accuracy [19, 30]. Common distillation objectives include\ncross-entropy loss for output probability distributions [12, 29] and\nMSE loss for layer-wise representations [15, 32, 40]. However, the\ncombination of distillation with token pruning has not been widely\nexplored. Our aim is to transfer the knowledge of token importance\nrankings from the teacherâ€™s final layer to the early layers of the\nstudent model during token pruning, which poses a new challenge\nand requires new distillation objective functions.\n2.2 Token Pruning\nExisting token pruning works can be categorized into two classes\nbased on token removal or retention criteria. The first class uses\nattention value-based scoring [9, 18, 36] to identify unimportant\ntokens. For instance, SpAtten [36] ranks tokens using importance\nscores and retains the top-k highest-scoring tokens. PoWER-BERT [9]\nlearns a layer-wise token pruning ratio, and prunes all input se-\nquences to the same length. LTP [18] improves PoWER-BERT by\nintroducing a learnable layer-wise threshold, enabling adaptive\npruning length. However, these approaches rely on the effective-\nness of token importance scoring. As shown in Fig. 1, crucial tokens\nmay receive little attention in early layers, leading them to be mis-\nclassified as redundant tokens. Removing these essential tokens\ncan result in a drastic accuracy loss.\nThe second class of token pruning methods [ 10, 42] inserts a\nprediction module before each transformer layer to provide a more\naccurate token importance score prediction. Transkimmer [10] is a\nnotable example that inserts a 2-layer MLP network at each layer\nas the prediction module. However, the extra prediction module\ncan also introduce considerable inference latency overhead, which\nis unfriendly on resource-limited devices.\nToP addresses all the above limitations by introducing the ranking-\naware token distillation technique. Since it can effectively improve\nthe effectiveness of attention value-based scoring, we can achieve\nthe same or better level of model accuracy as prediction-based\napproaches, while also being more efficient and inference-friendly.\nIn addition, these works primarily aim to minimize accuracy\nloss while reducing as many numbers of tokens as possible. They\nintroduce a regularization term with hyper-parameter ğœ†to balance\naccuracy and efficiency. However, this approach lacks effective con-\ntrol on pruning tokens to a desired computation budget. LAT [16]\naddress this with an evolutionary search strategy. In contrast, ToP\nsolves this limitation using a different approach - a constraint-aware\ntoken pruning algorithm, which is an optimization-based solution.\n2.3 Efficient Transformers\nSince the computation and memory cost in self-attention is qua-\ndratic in the token length, there have been a number of attempts in\ndesigning sparse attention. Sparse Transformer [5], Longformer [3],\nand Big Bird [43] employ sparse attention to allow the model to\nhandle long sequences. However, these methods only reduce the\nCUDA memory but cannot be faster than the full attention. Other\nefforts [35, 41] leverage neural architecture search to design effi-\ncient transformer models with smaller depths and fewer heads. ToP\nis orthogonal to these techniques on the input dimension reduction.\n3 BACKGROUND AND MOTIVATIONS\n3.1 Background\nTransformer models, such as BERT [7], are stacked up with multi-\nple encoder layers. A basic transformer layer wraps a multi-head\nself-attention (MHA) and feed-forward (FFN) layer with residual\nconnection and layer normalization (LN). Given an input sequence\nof ğ‘›tokens and hidden size of ğ‘‘, the hidden state of the ğ‘–ğ‘¡â„ layer,\nğ‘¿ğ‘– = (ğ‘¥1,ğ‘¥2,...ğ‘¥ğ‘›)âˆˆ Rğ‘›Ã—ğ‘‘, is computed from the previous layer:\nğ‘¿â€²\nğ‘–âˆ’1 = LN(ğ‘¿ğ‘–âˆ’1 +MHA(ğ‘¿ğ‘–âˆ’1))\nğ‘¿ğ‘– = LN(ğ‘¿â€²\nğ‘–âˆ’1 +FFN(ğ‘¿â€²\nğ‘–âˆ’1)), (1)\nSpecifically, an MHA consists of ğ‘â„ heads, where each head â„\nis associated with query ğ‘¾â„ğ‘, key ğ‘¾â„\nğ‘˜ and value ğ‘¾â„ğ‘£ matrix. Each\nhead â„first computes an attention probability matrix ğ‘¨â„ and then\ncomputes the self-attention mechanism as follows:\nğ‘¨â„ = softmax((ğ‘¿ Ã—ğ‘¾â„\nğ‘)Ã—( ğ‘¿ Ã—ğ‘¾â„\nğ‘˜)ğ‘‡)\nAttentionâ„ = ğ‘¨â„ Ã—(ğ‘¿ Ã—ğ‘¾â„\nğ‘£ )\n(2)\nComplexity analysis. The above self-attention measures the pair-\nwise importance of each token on every other token in the input,\nand the total complexity of MHA layer is ğ‘‚(ğ‘‘2ğ‘› +ğ‘›2ğ‘‘), which\nis quadratic with ğ‘›. For FFN layer, the computation complexity\nis ğ‘‚(ğ‘›ğ‘‘2), which is linear with ğ‘›. When applied to long input se-\nquences (i.e., a largeğ‘›), the computation and memory of MHA layer\ngrow quadratically and become very expensive.\nTo address this limitation, our work introduces token pruning,\nwhere unimportant tokens are gradually dropped as the inference\nproceeds. For each transformer layer, which initially has ğ‘›tokens,\nwe aim to remove a specific number of unimportant tokens from\nthem. These removed tokens will not be considered in subsequent\nlayers. This leads to a linear (for FFN) or quadratic (for MHA) reduc-\ntion in operations, resulting in significantly faster model inference.\nIdentifying unimportant tokens to be discarded is a major chal-\nlenge in token pruning. Current methods address this by either\nusing self-attention values to assign a score to each token, or by\nadding a prediction module to predict scores. However, both meth-\nods have their limitations, which will be discussed in detail.\n3.2 Limitations of attention value-based\nmethods\nToken importance scoring . Attention value-based methods [9,\n18, 36] define the importance score of token ğ‘¥ğ‘– in layer ğ‘™ as:\nğ‘ ğ‘™(ğ‘¥ğ‘–)= 1\nğ‘â„\n1\nğ‘›\nğ‘â„âˆ‘ï¸\nâ„=1\nğ‘›âˆ‘ï¸\nğ‘—=1\nğ‘¨ğ‘™\nâ„[ğ‘¥ğ‘–,ğ‘¥ğ‘—] (3)\nwhere ğ‘â„ denotes the number of heads. ğ‘¨ğ‘™\nâ„[ğ‘¥ğ‘–,ğ‘¥ğ‘—]indicates the\nattention received by token ğ‘¥ğ‘— from ğ‘¥ğ‘– on head â„. Thus, token ğ‘¥ğ‘– is\nconsidered important if it receives more attention from all tokens\nacross all heads. The term ğ‘¨ğ‘™\nâ„[ğ‘¥ğ‘–,ğ‘¥ğ‘—]can reuse the results from\nthe self-attention mechanism in Equation 2. Therefore, attention\nvalue-based scoring in Equation 3 is computationally lightweight.\nKDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA Junyan Li et al.\nFigure 1: Comparison of token importance score distribu-\ntions on the GLUE MRPC dataset using attention values from\nthe BERTğ‘ğ‘ğ‘ ğ‘’ model. Note that the y-axis is log-scaled to bet-\nter visualization of tokens with low importance scores.\nHowever, the attention values scoring in Equation 3 can inac-\ncurately measure token importance in early transformer layers.\nPrevious works [6, 28] have shown that some important tokens,\nsuch as [SEP], receive little attention in early layers but gain in-\ncreased attention in deeper layers. As such, these critical tokens\nobtain low importance scores in Equation 3 initially and higher\nscores in deeper layers. However, this creates a problem where\ncrucial tokens may be misclassified as unimportant in early layers\nand removed before they have the chance to reach deeper layers,\nwhere they would have received the correct importance scores.\nTo validate this, we analyze importance score distribution in a\ntrained BERTğ‘ğ‘ğ‘ ğ‘’ model finetuned with the GLUE MRPC dataset.\nFig. 1 compares the distribution of importance scores at layers 1,\n2, 9, and 12. Tokens are selected based on their ranking positions\naccording to the importance scores. The scores of top3 tokens in\ndeep layers, such as layers 9 and 12, are significantly higher than\nthose of less important tokens ranked in the top10-15 and top20-25.\nHowever, at early layers such as layers 1 and 2, the scores for top3\ntokens are not well distinguishable from those of other low-ranked\ntokens. This indicates that early layers tend to assign relatively\nsimilar scores to both top-ranked and low-ranked tokens, poten-\ntially resulting in important tokens being deemed unimportant.\nAs a result, while being computationally efficient, using attention\nvalue-based scoring can be problematic at early layers.\n3.3 Limitations of prediction-based methods\nIn contrast to the attention value-based scoring method, prediction-\nbased methods [10, 27, 42] incorporate an additional neural network\nto predict the importance score for each token. The recently pro-\nposed Transkimmer [10] adds an extra prediction module before\neach transformer layer, which composes of 2 linear layers with\na layernorm [2], GeLU activation [ 11] and GumbelSoftmax [ 13].\nThese prediction modules gradually update their parameters and\nlearn to predict which tokens should be pruned, which has been\nshown to outperform attention value-based approaches.\nHowever, prediction-based token pruning faces limitations in\nachieving real latency reduction. First, the prediction module it-\nself introduces additional inference latency and FLOPs. As shown\nin Table 1, the additional FLOPs and latency introduced by Tran-\nskimmer prediction modules account for 8.43% and âˆ¼30% of BERT,\nrespectively. This suggests that token pruning needs to prune much\nModel FLOPs Intel CPU Latency A100 GPU Latency\nBERTğ‘ğ‘ğ‘ ğ‘’ (12L-768H) +8.43% +11.82% +29.35%\nBERT6 (6L-512H) +8.48% +12.95% +28.79%\nBERT4 (4L-256H) +8.63% +23.53% +30.50%\nTable 1: Compared to the original model ( input token\nlength=256), Transkimmer [ 10] introduces significant infer-\nence latency on both CPU and GPU due to prediction module\nscoring. FLOPs are calculated using thop [ 1], while latency\nmeasurements are obtained using onnxruntime [24].\nmore tokens to counteract the 30% latency slowdown. Second, Tran-\nskimmerâ€™s dynamic token pruning relies on the computationally-\nintensive GumbelSoftmax operation, necessitating specialized run-\ntime and hardware support for efficient implementation.\nIn our work, our goal is to implement token pruning in practical\napplications that accelerate real inference latency. While prediction-\nbased approaches can be challenging in achieving actual latency\nreduction, we instead leverage attention values for token pruning.\n4 METHODOLOGY\nIn this section, we present ToP, a novel token pruning approach\nthat incorporates two key techniques for learning the optimal token\npruning decisions. First, we introduce the end-to-end token pruning\nalgorithm that leverages ğ¿0 regularization. Then, we describe the\nranking-aware token distillation approach that enhances the ability\nof self-attention values to rank token importance.\n4.1 Constraint-aware token pruning\nFor a given deployment constraint, we propose constraint-aware\ntoken pruning to remove a set of unimportant tokens so that the\nmodel with the retained tokens can achieve the best accuracy under\nthe constraint. The basic idea is (1) we introduce a set of binary\ndecision masks ğ‘€ âˆˆ{0,1}to represent the sparsity ratio and indicate\nwhether to drop ( ğ‘€ = 0) or keep each token ( ğ‘€ = 1); (2) use these\nmasks to construct a constraint-aware loss function; (3) optimize\nthe constraint-aware loss using an improved ğ¿0 regularization [ 23]\nmethod. Next, we will introduce the details.\nUnlike prior works [9, 10, 16, 18, 36, 42] that treat all layers in\nthe same manner, leading to a vast design space, we introduce\na novel coarse-to-fine token pruning scheme, as shown in Fig. 2.\nSpecifically, gate masks are used to select a subset of layers for\ntoken pruning, while token ranking masks dynamically determine\nwhich specific tokens within the selected layers should be pruned.\nLayer gate mask ğ‘€ğ‘”ğ‘ğ‘¡ğ‘’. We introduce a gate mask ğ‘€ğ‘”ğ‘ğ‘¡ğ‘’ for each\ntransformer layer to control whether token pruning is performed\nin that layer. Concretely, ifğ‘€ğ‘–\nğ‘”ğ‘ğ‘¡ğ‘’ is set to 1, token pruning will be\napplied to layer ğ‘–. If ğ‘€ğ‘–\nğ‘”ğ‘ğ‘¡ğ‘’ is 0, layer ğ‘– will be skipped, and retain\nthe tokens from its previous layer ğ‘–âˆ’1.\nThe design choice is built on two insights. First, it has been\nobserved that pre-trained transformer models have varying levels\nof token redundancy across different layers [9, 28]. Second, pruning\ntokens across all layers significantly expands the design space and\nposes unnecessary difficulty in optimization, particularly when\ntrying to maintain a low or medium level of compression sparsity.\nIn our experiments, we observe an interesting trend where as\nthe target sparsity increases (i.e., constraint becomes tighter), the\nConstraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA\nFigure 2: Our approach learns layer gate masks and token\nranking masks to prune tokens under a desired constraint.\nWhen a layer gate turns off (i.e., mask=0), we skip the current\nlayer. When a layer gate turns on (i.e., mask=1), unimportant\ntokens are removed after the self-attention mechanism.\ngate masks progressively activate earlier layers, starting from the\ndeepest one. Eventually, the gate masks activate all transformer\nlayers under high pruning sparsity. Inspired by the observation,\nwe gradually increase the sparsity level from 0 to the target during\npruning. By doing this, gate masks provide improved decision-\nmaking for pruning fewer tokens within early layers. This is because\ngate masks prioritize learning masks in deeper layers, where token\nimportance is more easily predictable by self-attention values.\nToken ranking position mask ğ‘€ğ‘Ÿğ‘ğ‘›ğ‘˜. For fine-grained token\npruning, assigning a mask to each token and removing those with a\nmask value of 0 may seem an intuitive solution. However, it can lead\nto a problem known as \"static token pruning. \" This occurs because\nthe final mask values are fixed after training, causing tokens to be\npruned at the same positions for all input sequences in the dataset\nduring inference. This can be problematic as informative tokens\ncan appear in different positions across different input sequences.\nRemoving tokens at the same positions for all input sequences\ncan also inadvertently remove important tokens and result in a\nsignificant loss of accuracy.\nTo address this challenge, we follow PoWER-BERT [9] to use\nranking masks (see Fig. 2). Instead of applying a mask to each\ntoken directly, we mask tokensâ€™ ranking positions based on their\nimportance score, which is computed by utilizing attention values,\nas outlined in Equation 3. The insight is that by scoring tokens\nbased on their importance to the final model prediction, crucial\ntokens will always rank in the topmost positions after sorting. This\nmeans that although informative tokens may appear in different\npositions based on input content, their ranking positions are static\n(i.e., topmost) and can be indicated by a static mask. For example,\ngiven an input sequence of ğ‘‹ğ‘–âˆ’1 = (ğ‘¥1,ğ‘¥2,...ğ‘¥ğ‘›)for layer ğ‘–, we\nsort the tokens by their importance scores, resulting in a ranking\nof (ğ‘›,3,...1). The corresponding ranking masks are then defined\nas (ğ‘€ğ‘–,ğ‘›\nğ‘Ÿğ‘ğ‘›ğ‘˜,ğ‘€ğ‘–,3\nğ‘Ÿğ‘ğ‘›ğ‘˜,...ğ‘€ğ‘–,1\nğ‘Ÿğ‘ğ‘›ğ‘˜), where the value of ğ‘€ğ‘–,ğ‘—\nğ‘Ÿğ‘ğ‘›ğ‘˜ indicates\nwhether prune or keep the ğ‘—ğ‘¡â„ ranked tokens for layer ğ‘–.\nFLOPs-aware constraint. With these masks, we can calculate the\nnumber of retained tokens and use them to measure the computa-\ntion and memory cost that is required for model inference. In our\nwork, we use FLOPs as a metric for evaluating the cost of model in-\nference due to its ease of use. Formally, letğ‘´ = {ğ‘€1\nğ‘”ğ‘ğ‘¡ğ‘’,...ğ‘€ ğ¿\nğ‘”ğ‘ğ‘¡ğ‘’,ğ‘€1,1\nğ‘Ÿğ‘ğ‘›ğ‘˜,...},\ndenoting all the inserted masks. Then the expected model FLOPs\nafter token pruning can be calculated from ğ‘´ as follows:\nğ‘(ğ‘´)= Î£ğ¿\nğ‘–=1 (4 Â·d2 Â·ğ‘‡ğ‘– +2 Â·d Â·ğ‘‡2\nğ‘– +ğ‘â„ Â·ğ‘‡2\nğ‘– )\n+Î£ğ¿\nğ‘–=1 (2 Â·d Â·ğ‘‘â€²Â·ğ‘‡ğ‘–)\n(4)\nwhere the two items calculate the FLOPs of MHA and FFN layers,\nrespectively. ğ‘‘ denotes the hidden size, ğ‘â„ is the number of heads\nin MHA layer, ğ‘‘â€²denotes FFN intermediate size. ğ‘‡ğ‘– represents the\nnumber of tokens that are retained for the ğ‘–ğ‘¡â„ layer, which can be\neasily computed by multiplying E(ğ‘€ğ‘–\nğ‘Ÿğ‘ğ‘›ğ‘˜ > 0)with the original\ntoken length ğ‘›. Note that when the gate mask ğ‘€ğ‘–\nğ‘”ğ‘ğ‘¡ğ‘’ turns off fine-\ngrained token pruning, ğ‘‡ğ‘– keeps the same number of tokens with\nits previous layer (i.e., ğ‘‡ğ‘– =ğ‘‡ğ‘–âˆ’1).\nLearning masks under a desired constraint . Now we introduce\nhow to determine the values of gate masks and ranking masks\nfor minimal accuracy loss under a given FLOPs constraint. Let ğœ½\ndenote the original model and ğ‘´ denote all the pruning masks.\nWe formalize the task of token pruning as an end-to-end learning\nproblem by adding a regularization term ğ¿ğ‘Ÿğ‘’ğ‘”:\nğ¿= ğ¿ğ‘‘ğ‘œğ‘¤ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘’ğ‘ğ‘š (ğœ½,ğ‘´)+ğœ†ğ¿ğ‘Ÿğ‘’ğ‘”(ğ‘´) (5)\nOptimizing the above loss function requires the masksğ‘€ğ‘”ğ‘ğ‘¡ğ‘’ and\nğ‘€ğ‘Ÿğ‘ğ‘›ğ‘˜ to be differentiable. However, the original masks are discrete\nbinary values. To overcome this, we use the ğ¿0 reparameterization\nmethod proposed by [23], which is specifically designed for model\npruning. Following the standard ğ¿0 reparameterization, masks ğ‘´\nare regulated using the hard concrete distribution as follows\nğ’– âˆ¼ğ‘ˆ(0,1)\ns = sigmoid((log ğ’–\n1 âˆ’ğ’– +logğœ¶ )/ğ›½)\nËœğ‘  = ğ‘ Ã—(ğ‘Ÿ âˆ’ğ‘™)+ğ‘™\nğ‘´ = min(1, max(0,Ëœğ‘ ))\n(6)\nwhere ğ‘ˆ(0,1)is a uniform distribution in the interval [0,1]; ğ‘™ < 0\nand ğ‘Ÿ > 0 are two constants that stretch the sigmoid output into the\ninterval (ğ‘™,ğ‘Ÿ). ğ›½ is a hyperparameter that controls the steepness of\nthe sigmoid function. We adopt the common practice of settingğ‘™ to\n-0.1, ğ‘Ÿ to 1.1 and ğ›½to 2\n3 . ğœ¶ = {ğ›¼ğ‘—}|ğ‘´|\nğ‘—=1 are the main learnable param-\neters. The hard concrete distribution assigns a significant portion of\nprobability mass on the integer values {0,1}, which serves as a good\ncontinuous approximation of the binary (Bernoulli) distribution.\nDuring training, the hard concrete parameters ğœ¶ and ğ’– deter-\nmine the values of masks ğ‘´. We learn masks ğ‘´ by updating these\nlearnable parameters of the distributions from which the masks are\nsampled in the forward pass. Moreover, these learnable parame-\nters and masks can be jointly optimized with the original model\nparameters, resulting in better performance.\nIn prior token pruning works [ 9, 10, 18], ğ¿1 is widely used as\nthe regularization loss in Equation 5, and ğœ†controls the trade-off\nbetween final model accuracy and the achieved sparsity. However, it\nrequires careful hyper-parameter tuning to make sure it converges\nto a desired sparsity [39, 40], which lacks effective control on the\ncomplexity of final model inference.\nKDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA Junyan Li et al.\nFigure 3: Our ranking-aware token distillation uses impor-\ntance rankings generated from the unpruned modelâ€™s final\nlayer and distill it to early layers during the training.\nIn our work, we aim to control the achieved model FLOPs after\ntoken pruning. We follow [39] to replace the vanilla ğ¿0 objective\nwith a Lagrangian multiplier. Letğ¶ be the target FLOPs, ğ‘(ğ‘´)be\nthe expected FLOPs determined by the masks ğ‘´ in Equation 4. We\nimpose an equality constraint ğ‘(ğ‘´)= ğ¶ by introducing a penalty:\nğ¿ğ‘Ÿğ‘’ğ‘”(ğ‘´)= ğœ†1 Â·(ğ‘(ğ‘´)âˆ’ğ¶)+ğœ†2 Â·(ğ‘(ğ‘´)âˆ’ğ¶)2 (7)\nwhere the masks ğ‘´ are determined by hard concrete parameters ğœ¶\nand ğ’– in Equation 6.\n4.2 Distillation of token importance rankings\nIn our proposed constraint-aware token pruning algorithm, the\nranking masks are applied to the sorted tokens based on their im-\nportance scores. Ideal importance scoring mechanism that provides\nreliable token importance rankings is essential for maintaining the\nperformance of the final model prediction. The most crucial tokens\nwill be at the top and retained by the ranking masks.\nIn ToP, we utilize self-attention values to calculate token impor-\ntance (Equation 3) for ease of hardware deployment. However, as\ndiscussed in Sec. 3.2, accurately ranking the importance of tokens\nusing the self-attention mechanism is challenging in the early lay-\ners of the transformer. Despite the potential for improved token\nrankings in deeper layers, crucial tokens may be mistakenly ranked\nas unimportant and removed before they have the opportunity to\nreach these layers, leading to a substantial decrease in accuracy.\nTo overcome this limitation, we propose a method to distill more\naccurate ranking decisions to the early layers during the training\nprocess, enhancing the ability of self-attention values to rank token\nimportance. To this end, we enable the preservation of the most\nimportant tokens during inference, thus improving overall accuracy.\nRanking-aware token distillation . Before distillation, we need\nto obtain accurate token importance rankings as the knowledge.\nHowever, obtaining such ground truth or labeled rankings is chal-\nlenging. Our experiments, as shown in Fig. 1, suggest that attention\nvalues in final layers can effectively identify important tokens. Thus,\nwe use the token importance rankings generated from the final\ntransformer layer of the unpruned model as our knowledge source.\nThe next challenge is to develop an effective distillation objective.\nPrevious studies [19, 30, 40] have shown that combining distillation\nwith model weight pruning can improve performance. These studies\nhave used distillation objectives based on cross-entropy loss [12, 29]\nor Mean Squared Error (MSE) loss between the student and teacher\nlayer-wise representations [15, 32, 40]. However, when it comes to\ntoken distillation, the conventional MSE loss may not be suitable\ndue to the different levels of information captured by early and deep\nlayers of transformer models like BERT [14]. The representations\nof early and deep layers are meant to be distinct, making it difficult\nto use conventional distillation objectives.\nInstead, we use token importance rankings from the final layer\nof the teacher model as the information to be distilled and develop\na ranking-aware distillation loss. Our objective is to align the token\nimportance rankings of the early transformer layers with those of\nthe final layer in the teacher model. This way, self-attention layers\nare encouraged to give more importance to the crucial tokens,\nincreasing the chance of retaining them in early layers.\nFig. 3 illustrates the overview process of our proposed ranking-\naware token distillation. To effectively distill the knowledge, we\nadopt a ranking loss as the distillation objective. Specifically, we use\nthe LambdaLoss as proposed in [38] to optimize Normalized Dis-\ncounted Cumulative Gain (NDCG), which is a widely used ranking\nmetric in information retrieval systems that places more emphasis\non the importance of top-ranked tokens.\nSpecifically, let ğ‘¹ denote the rankings obtained by sorting the\nimportance scores of each token in the final layer in the teacher\nmodel. ğ‘ºğ‘– refers to the token importance scores in the ğ‘–ğ‘¡â„ layer of\nthe student model, and ğ‘¿ represents a mini-batch of training data.\nWe define the distillation loss as follows:\nğ¿ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘™ğ‘™ =\nğ¿â€²\nâˆ‘ï¸\nğ‘–=1\nğ¿ğ‘ğ‘šğ‘ğ‘‘ğ‘ğ¿ğ‘œğ‘ ğ‘ (ğ‘¹,ğ‘ºğ‘–,ğ‘¿) (8)\nIn our experiments, we empirically set the first 1/3 layers (ğ¿â€²=\n1\n3 ğ¿) as the early layers for distillation.\n4.3 Training and inference\nWe now describe the full training process of our ToP. Given a deploy\nconstraint, ToP simultaneously trains the masks and model param-\neters, allowing for effective token pruning and improved model\nadaptation to token sparsification. Additionally, we incorporate\nranking-aware distillation to enhance the ability of the attention\nvalues in determining the importance of tokens in early layers. The\nfull training objective is a combination of the above objectives:\nğ¿= ğ¿ğ‘‘ğ‘œğ‘¤ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘’ğ‘ğ‘š (ğœ½,ğ‘´)+ğ¿ğ‘Ÿğ‘’ğ‘”(ğ‘´)+ğœ†ğ¿ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘™ğ‘™ (9)\nwhere ğ¿ğ‘Ÿğ‘’ğ‘”(ğ‘´)is defined as in Equation 7, and ğœ†is a hyperparam-\neter controlling the contribution of distillation loss. In particular,\nthe two hyperparameters ğœ†1 and ğœ†2 introduced by ğ¿ğ‘Ÿğ‘’ğ‘”(ğ‘´)are au-\ntomatically adjusted using the AdamW optimizer. Therefore, the\nonly additional hyperparameter that needs to be tuned is ğœ†.\nOnce the training finishes, the token pruning decisions are deter-\nmined by combining the gate masks and ranking masks. Specifically,\nonly the layers chosen by the gate masks with ğ‘€ğ‘Ÿğ‘ğ‘›ğ‘˜ = 1 are as-\nsigned with ranking masks for token selection. The other layers\nare not considered for token pruning.\nDuring the inference, we use self-attention values to calculate\ntoken importance scores in the selected layers. We then sort the\ntokens based on these scores. By discarding the tokens through use\nof ranking masks (ğ‘€ = 0), we can effectively eliminate unnecessary\ntokens and improve the efficiency of the model.\nConstraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA\n(a) Token pruning on BERT ğ‘ğ‘ğ‘ ğ‘’\nModel Method CoLA RTE QQP MRPC SST-2 MNLI QNLI STS-B\nMatthews FLOPs Acc. FLOPs Acc. FLOPs F1 FLOPs Acc. FLOPs Acc. FLOPs Acc. FLOPs Pearson FLOPs\nBERTğ‘ğ‘ğ‘ ğ‘’ - 57.8 1.00 Ã— 65.7 1.00 Ã— 91.3 1.00 Ã— 88.9 1.00 Ã— 93.0 1.00 Ã— 84.9 1.00 Ã— 91.4 1.00 Ã— 88.6 1.00 Ã—\nPoWER-BERT Atten-value 52.3 4.50 Ã— 67.4 3.40 Ã— 90.2 4.50 Ã— 88.1 2.70 Ã— 92.1 2.40 Ã— 83.8 2.60 Ã— 90.1 2.00 Ã— 85.1 2.00 Ã—\nLAT Atten-value - - - - - - - - 92.8 2.90 Ã— 84.4 2.80 Ã— - - - -\nLTP Atten-value 52.3 8.66 Ã— 63.2 6.84 Ã— 90.4 7.44 Ã— 87.1 6.02 Ã— 92.3 3.59 Ã— 83.9 3.74 Ã— 89.3 3.91 Ã— 87.5 5.25 Ã—\nToP Atten-value 60.5 9.62 Ã— 70.0 7.10 Ã— 91.2 8.04 Ã— 89.2 6.32 Ã— 93.5 3.82 Ã— 84.7 4.27 Ã— 90.6 4.35 Ã— 87.8 5.32 Ã—\nTranskimmer Prediction 58.9 9.81 Ã— 68.9 11.22 Ã— 90.8 11.72 Ã— 88.5 7.45 Ã— 92.3 4.07 Ã— 83.2 6.65 Ã— 90.5 6.01 Ã— 87.4 7.24 Ã—\nToP Atten-value 60.0 10.05 Ã— 67.9 9.94 Ã— 90.9 12.57 Ã— 88.8 7.47 Ã— 93.2 4.52 Ã— 83.2 6.65 Ã— 89.1 6.14Ã— 86.4 7.30Ã—\n(b) Token pruning on RoBERTa ğ‘ğ‘ğ‘ ğ‘’\nRoBERTağ‘ğ‘ğ‘ ğ‘’ - 61.8 1.00 Ã— 78.0 1.00 Ã— 90.4 1.00 Ã— 92.1 1.00 Ã— 94.3 1.00 Ã— 87.5 1.00 Ã— 92.9 1.00 Ã— 90.9 1.00 Ã—\nLTP Atten-value - - 78.0 6.93 Ã— 89.7 8.70 Ã— 91.6 4.99 Ã— 93.5 5.28 Ã— 86.5 6.09 Ã— 92.0 4.61 Ã— 90.0 3.81 Ã—\nTranskimmer Prediction 61.3 8.55 Ã— 76.2 6.74 Ã— 91.0 19.40 Ã— 91.9 6.22 Ã— 93.5 5.26 Ã— 86.7 7.01 Ã— 91.7 7.00 Ã— 90.5 5.23 Ã—\nToP Atten-value 64.5 8.91 Ã— 79.4 7.16 Ã— 90.6 19.42Ã— 92.3 6.38 Ã— 93.8 5.56 Ã— 86.9 7.04 Ã— 92.8 5.02Ã— 89.6 4.02 Ã—\nTable 2: Performance and FLOPs reduction compared to state-of-the-art token pruning methods on GLUE benchmark.\n5 EXPERIMENTS\n5.1 Experiment Setup\nDatasets and Models . We evaluate ToP on a diverse set of datasets,\nincluding 8 classification and regression tasks from the GLUE bench-\nmark [34], the SQuAD-v2.0 [ 26] extractive question answering\ndataset, and the 20News [21] long sequence classification dataset.\nThe diversity of these tasks and the range of token lengths (i.e.,\n64, 128, 256, 384, 512) showcase the general applicability of our\nmethod. A detailed summary of these datasets can be found in\nAppendix. To demonstrate the generality of our approach on both\nlarge and small pre-trained language models, we implement ToP\non RoBERTağ‘ğ‘ğ‘ ğ‘’ [22], BERTğ‘ğ‘ğ‘ ğ‘’ and BERT6.\nTraining setup. Following existing works [9, 10, 18], we perform\nfine-tuning on the downstream datasets before the token pruning.\nThen we conduct token pruning with the optimization objective\nin Equation 9 under multiple FLOPs constraints. We use FLOPs\nsparsity to denote the constraint, which is defined as the ratio\nof FLOPs that have been pruned or removed from the model. We\nalso define FLOPs reduction as the ratio of the full model FLOPs\nunder the original input sequence length to the remaining FLOPs.\nThe token pruning process begins with a warmup stage where\nwe gradually increase sparsity to the target value using a linear\nscheduler. The initial value of ğœ†is determined by hyper-parameter\ngrid search from [1e-2, 1e-3, 1e-4] on development set with 20%\ndata randomly picked from training set. At the start, layer gate\nmasks are initialized to 0, and ranking masks are initialized to 1,\nmeaning all tokens are retained. After reaching the final sparsity\nlevel, we fix it and continue training the model and pruning masks\nuntil convergence (i.e., fine-tuning stage). See Appendix for detailed\nsettings of other hyper-parameters. We conduct all experiments\nwith random seed 57.\nBaselines. We compare ToP with the state-of-the-art token prun-\ning and model compression methods. Token pruning baselines\ninclude attention value-based methods such as PoWER-BERT [9],\nLAT [16], LTP [18], and strong prediction-based method Transkim-\nmer [10]. Model compression baselines include DistilBERT [29], and\nCoFi [40], which is a state-of-the-art structured pruning method.\nModel Method SQuADv2.0 20News\nF1 FLOPs Acc. FLOPs\nBERTğ‘ğ‘ğ‘ ğ‘’ - 77.1 1.00 Ã— 86.7 1.00 Ã—\nTranskimmer Prediction 75.7 4.67 Ã— 86.1 8.11Ã—\nPoWER-BERT Atten-value - - 86.5 2.91 Ã—\nLTP Atten-value 75.6 3.10 Ã— 85.2 4.66 Ã—\nToP Atten-value 75.9 4.12 Ã— 87.0 8.26 Ã—\nTable 3: Results by different token pruning methods on down-\nstream tasks with long sequence length.\n5.2 Main results\nOverall performance . We evaluate ToP on both BERTğ‘ğ‘ğ‘ ğ‘’ and\nRoBERTağ‘ğ‘ğ‘ ğ‘’, and compare with the state-of-the-art token pruning\nbaselines. LTP lacks an implementation for BERTğ‘ğ‘ğ‘ ğ‘’, we follow\ntheir official code and implement for BERTğ‘ğ‘ğ‘ ğ‘’. For a fair compari-\nson, we follow the provided instructions and conduct a grid search\nfor the optimal hyper-parameters. To prove ToPâ€™s effectiveness on\nvarying input sequence lengths, we conduct experiments on both\nthe GLUE benchmark and two long sequence tasks.\nWe start by comparing ToP to the original BERT and RoBERTa.\nAs shown in Table 2 and Table 3, ToP achieves comparable accu-\nracy on GLUE benchmark, SQuAD and 20News while significantly\nreducing FLOPs. For instance, ToP even outperforms the original\nBERT with +2.7%, +4.3%, +0.3%, and +0.5% improvement on CoLA,\nRTE, MRPC, and SST-2 respectively, and achieves an average FLOP\nreduction of 6.7Ã—. On other datasets, the accuracy drop is minimal\nat < 0.8%. These results demonstrate the effectiveness of ToP in\nreducing computational cost while maintaining accuracy.\nCompared to other token pruning methods based on attention\nvalues, ToP significantly surpasses strong baselines such as PoWER-\nBERT, LAT, and LTP across all datasets, despite being based on the\nsame approach. This is mainly due to our ranking-aware token\ndistillation mechanism, which greatly enhances the ability of self-\nattention values to rank token importance.Also, as can be seen from\nTable 2 and Table 3, current attention value-based methods cannot\nsurpass the prediction method-based Transkimmer. However, ToP\noutperforms Transkimmer on many tasks. Specifically, under the\nKDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA Junyan Li et al.\nFigure 4: Comparison of token pruning methods under various FLOPs sparsity ratios.\nsame-level FLOPs reduction, ToP on BERT achieves +2.9%, +0.1%,\n+0.3%, +0.9%, +0.2%, +0.9% higher accuracy on CoLA, QQP, MRPC,\nSST-2, MNLI, SQuAD and 20News, respectively.\nItâ€™s worth noting that ToP also outperforms Transkimmer in\nterms of real inference latency improvement. As discussed in Sec. 3.3,\nitâ€™s challenging to deploy Transkimmer for real latency reduction.\nIn contrast, ToP delivers real latency improvement of up to 7.4Ã—,\nwhich will be further discussed in later sections.\nUnder various deployment constraints. We now evaluate the\neffectiveness of ToP under various FLOPs sparsity and compare\nit to two state-of-the-art methods: LTP (representing attention\nvalue-based methods) and Transkimmer (representing prediction-\nbased methods). For a fair comparison, we use their official code\nimplementations and perform a grid search to find the optimal\nhyper-parameters that achieve the desired token sparsity.\nFig. 4 summarizes the results. ToP surpasses both LTP and Tran-\nskimmer at all FLOPs sparsity levels. At equivalent levels of accu-\nracy on the 20News long sequence, ToP reduces 17% and 20% more\nFLOPs compared to Transkimmer and LTP respectively. Without\nany loss in BERTğ‘ğ‘ğ‘ ğ‘’ accuracy, ToP reduces FLOPs by 86%, 78%,\n76%, and 88% on MRPC, SST-2, MNLI, and 20News, respectively.\nComparison with model compression . In addition to token\npruning baselines, we compare ToP with state-of-the-art model\ncompression techniques, including structured pruning and knowl-\nedge distillation (i.e., the DistilBERT 6). We specifically compare\nwith CoFi [40], a top-performing structured pruning method. We\nevaluate CoFi and ToP on two model sizes: BERTğ‘ğ‘ğ‘ ğ‘’, a large trans-\nformer model, and BERT6, a small model.\nTable 4 shows the results by different methods. ToP and CoFi\noutperform DistilBERT6 in terms of higher compression ratios.\nToP consistently surpasses the original BERTğ‘ğ‘ğ‘ ğ‘’ consistently with\nhigher accuracy and a 6.7Ã—average FLOPs reduction, whereas CoFi\nsees a significant drop in accuracy for CoLA and 20News at high\ncompression ratios. On the smaller BERT 6 model, ToP exhibits\na much smaller accuracy loss compared to CoFi, showcasing its\neffectiveness and robustness across models of various sizes.\n5.3 Ablation study\nThe effectiveness of coarse-to-fine grained pruning strategy .\nCompared to prior works, ToP introduces a new coarse-to-fine\npruning approach that uses gate masks to choose the best subset of\ntransformer layers for fine-grained token pruning. To evaluate the\nModel CoLA MRPC MNLI 20News\nMatthews FLOPs F1 FLOPs Acc. FLOPs Acc. FLOPs\nBERTğ‘ğ‘ğ‘ ğ‘’ 57.8 1.0 Ã— 88.9 1.0 Ã— 84.9 1.0 Ã— 86.7 1.0 Ã—\nDistilBERT6 49.0 2.0 Ã— 86.9 2.0 Ã— 82.6 2.0 Ã— 85.8 2.0 Ã—\nCoFi 39.8 9.1 Ã— 90.0 4.0 Ã— 84.7 4.0 Ã— 86.4 7.7 Ã—\nToP 60.0 10.0 Ã— 90.9 4.1 Ã— 85.0 4.3 Ã— 87.0 8.2 Ã—\nBERT6 49.0 1.0 Ã— 90.4 1.0 Ã— 82.6 1.0 Ã— 87.3 1.0 Ã—\nCoFi 38.0 9.1 Ã— 86.3 7.7 Ã— 80.1 6.6 Ã— 85.9 5.9 Ã—\nToP 48.9 9.5 Ã— 87.8 7.7 Ã— 82.0 6.6 Ã— 86.8 6.0 Ã—\nTable 4: Comparison with state-of-the-art model compres-\nsion methods on both large and small pre-trained models.\nMethod CoLA RTE QQP MRPC SST-2 MNLI QNLI STS-B\nMatthews Acc. Acc. F1 Acc. Acc. Acc. Peason\nToP 60.0 67.9 90.9 88.8 93.2 83.2 89.1 86.4\nToP âˆ’Gate Masks 58.9 61.7 89.4 85.3 93.5 81.7 87.3 83.5\nToP 60.0 67.9 90.9 88.8 93.2 83.2 89.1 86.4\nToP âˆ’Distill 59.4 60.6 90.7 86.4 92.9 83.0 88.0 82.7\nToP (MSE Distill loss) 57.6 62.8 90.6 87.5 93.0 82.9 87.9 73.5\nToP (CE Distill loss) 59.0 60.3 90.5 88.3 92.3 82.7 88.1 84.8\nTable 5: Alabtion studies of different methods under the same\ntoken pruning sparisity. Above: effectiveness of our coarse-\nto-fine grained pruning strategy. Below: ablation study of\ndifferent token importance distillation objectives.\nimpact of this design on performance, we compare the results with\nand without gate masks by removing the masks and pruning all\nlayers at the fine-grained level. We run all experiments under the\nsame FLOPs sparsity (tables 2 and 3) and hyper-parameter settings.\nTable 5 shows the accuracy results on GLUE benchmark. Remov-\ning gate masks causes a noticeable accuracy drop on all datasets\nexcept SST-2. Specifically, the accuracy drops by a substantial 6.2%,\n3.5%, 2.9%, and 2.9% on RTE, MRPC, QNLI, and STS-B, respec-\ntively. These results highlight the effectiveness of our coarse-to-fine\ngrained approach in making much better token selection decisions.\nDifferent token distillation approaches . We also ablate on the\nranking-aware token distillation component to evaluate its con-\ntribution to the performance of ToP. We first remove the entire\ndistillation process from the token pruning process. As shown in\nTable 5, the removal of our proposed ranking-aware token distil-\nlation results in accuracy drops across all datasets. Moreover, we\nobserve that the effect of ranking-aware token distillation varies\nConstraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA\nFigure 5: The retained tokens as input for each layer in BERT.\nbased on the length of the input sequences. On datasets with rel-\natively short sample lengths (see Table 7 in Appendix), such as\nCoLA, QQP, SST-2, and MNLI, token distillation slightly improves\naccuracy by 0.6%, 0.2%, 0.3%, and 0.2%, respectively. However, on\ndatasets with longer sample lengths, such as RTE, MRPC, QNLI, and\nSTS-B, token distillation has a crucial role in improving accuracy\nby 7.2%, 4.4%, 1.1%, and 3.7%, respectively. The underlying reason is\nthat itâ€™s much easier for conventional attention-based scoring meth-\nods to determine the significance of tokens in shorter sequences\ncompared to longer sequences. However, when the task involves\nlonger sequences, accurately identifying the most critical tokens at\nearly transformer layers becomes challenging. In such cases, our\nproposed ranking-aware token distillation effectively tackles this\nproblem and leads to a significant improvement in accuracy.\nMoreover, we compare the use of conventional distillation ob-\njectives to the ğ¿ğ‘ğ‘šğ‘ğ‘‘ğ‘ğ¿ğ‘œğ‘ ğ‘  in Equation 8. Our alternatives include\nthe MSE loss [40] which seeks to reduce the discrepancy between\nteacherâ€™s and studentâ€™s token importance scores, and the general\ncross-entropy (CE) loss [12] that aims to minimize the KL diver-\ngence between teacherâ€™s and studentâ€™s token importance score\ndistributions. Table 5 indicates that relying on conventional distil-\nlation objectives fails to improve the accuracy effectively.\n5.4 Retained Tokens Visualization and Analysis\nTo further understand the behavior of ToP, we analyze the number\nof retained tokens in each layer. We conduct experiments on four\ndatasets using two groups of FLOPs sparsity ratios: (i) low sparsity\n45%, 20%, 20%, 50% and (ii) high sparsity 65%, 65%, 60%, 80% for QQP,\nSST-2, MNLI, and 20News respectively. The low sparsity numbers\nare used for experiments in Fig. 4 and the high sparsity numbers\nare evaluated in Table 2 and Table 3. The model accuracy loss is\nnegligible as demonstrated in previous sections. Notably, for better\nvisualization, we excludePAD tokens when analyzing ToPâ€™s pruning\ndecision on the original effective input tokens.\nFig. 5 illustrates the number of remaining tokens used as input\nfor each layer in BERTğ‘ğ‘ğ‘ ğ‘’ under different levels of sparsity ratios.\nDataset Input Token\nlength BERT BERT with ToP Latency speedup\n(Acc. compared to BERT)\nMRPC 128 84.0 ms 29.1 ms 2.9 Ã—(-0.1%)\nRTE 256 162.8 ms 27.8 ms 5.8 Ã—(+2.2%)\nSQuAD 384 239.4 ms 58.2 ms 4.1 Ã—(-0.2%)\n20News 512 347.6 ms 47.0 ms 7.4 Ã—(+0.3%)\nTable 6: Real inference latency on an 8-core Intel CPU.\nInterestingly, we discover common token pruning patterns in the\nmodels: (1) deep transformer layers have high token redundancy.\nUnder high sparsity, over 95% of tokens are pruned when they arrive\nat layer 8, indicating that earlier layers have effectively extracted\ntheir information, making them non-informative for deep layers.\nThis observation is consistent with the conclusions of studies on\nBERT behaviours [6, 28], indicating that ToP is capable of automat-\nically identifying the optimal patterns for token pruning. (2) ToP\nprioritizes token pruning in deeper layers over an even distribution\nacross all layers. We observe that ToP selects different layers for\ntoken pruning under varying levels of sparsity ratios. In ToP, token\npruning is initially performed in deeper layers when the sparsity\nis low, and as the sparsity increases, it gradually extends to earlier\nlayers. For example, on the SST-2 task, when the sparsity is set to\n20%, token pruning is not applied to layers 1 to 7. However, when\nthe sparsity increases to 65%, token pruning is activated in layers 3\nto 7, while only layers 1 and 2 are excluded from pruning.\n5.5 Inference latency on Hardware\nFinally, we assess the practical efficiency of ToP in resource-limited\nenvironments by evaluating BERT on MRPC, RTE, SQuAD, and\n20News datasets using an 8-core Intel(R) Xeon(R) CPU@2GHz. The\nlearned ranking masks are applied layer-by-layer to discard tokens,\nand latency is measured using the high-performance Onnxruntime\ninference engine [24]. The batch size is set to 1 for simplicity.\nAs shown in Table 6, BERT inference without token pruning\nincurs a high latency on the CPU, particularly for long input se-\nquences. However, ToP significantly reduces this latency by dis-\ncarding unnecessary tokens. Specifically, ToP delivers inference ac-\nceleration of 2.9Ã—, 5.8Ã—, 4.1Ã—, 7.4Ã—on MRPC, RTE, SQuAD, 20News\nrespectively, with minimal impact on accuracy. The latency reduc-\ntion increases with the input length, indicating the big potential of\nToP in handling long sequence tasks.\n6 CONCLUSION\nIn this paper, we propose ToP, a novel token pruning approach\nthat leverages ğ¿0 regularization to determine the optimal token re-\nmoval under a target inference constraint. ToP adopts hierarchical\npruning, where it selects the ideal subset of transformer layers for\nfine-grained token pruning. Coupled with ranking-aware token\nimportance distillation, ToP significantly enhances the ability of\nself-attention values to rank token importance, leading to superior\naccuracy even at high compression ratios. Extensive evaluations\non the GLUE benchmark and SQuAD have shown that ToP out-\nperforms existing token pruning and model compression baselines.\nWith the removal of unnecessary tokens, ToP achieves up to 12.6Ã—\nFLOP reduction for BERT with less than 1% drop in accuracy.\nKDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA Junyan Li et al.\nREFERENCES\n[1] 2022. thop. https://github.com/Lyken17/pytorch-OpCounter\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-\ntion. arXiv preprint arXiv:1607.06450 (2016).\n[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-\ndocument transformer. arXiv preprint arXiv:2004.05150 (2020).\n[4] Wentao Chen, Hailong Qiu, Jian Zhuang, Chutong Zhang, Yu Hu, Qing Lu,\nTianchen Wang, Yiyu Shi, Meiping Huang, and Xiaowe Xu. 2021. Quantization of\nDeep Neural Networks for Accurate Edge Computing. arXiv:2104.12046 [cs.CV]\n[5] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating\nLong Sequences with Sparse Transformers. URL https://openai.com/blog/sparse-\ntransformers (2019).\n[6] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019.\nWhat Does BERT Look at? An Analysis of BERTâ€™s Attention. In Proceedings of\nthe 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks\nfor NLP .\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[8] Mitchell A. Gordon, Kevin Duh, and Nicholas Andrews. 2020. Compress-\ning BERT: Studying the Effects of Weight Pruning on Transfer Learning.\narXiv:2002.08307 [cs.CL]\n[9] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan Chakar-\navarthy, Yogish Sabharwal, and Ashish Verma. 2020. PoWER-BERT: Accelerating\nBERT Inference via Progressive Word-vector Elimination. InProceedings of the\n37th International Conference on Machine Learning (Proceedings of Machine Learn-\ning Research, Vol. 119) , Hal DaumÃ© III and Aarti Singh (Eds.). PMLR, 3690â€“3699.\n[10] Yue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin, and Minyi Guo. 2022. Tran-\nskimmer: Transformer Learns to Layer-wise Skim. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). Association for Computational Linguistics, 7275â€“7286.\n[11] Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus).\narXiv preprint arXiv:1606.08415 (2016).\n[12] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the Knowl-\nedge in a Neural Network. ArXiv abs/1503.02531 (2015).\n[13] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical Reparameterization\nwith Gumbel-Softmax. In International Conference on Learning Representations .\nhttps://openreview.net/forum?id=rkE3y85ee\n[14] Ganesh Jawahar, BenoÃ®t Sagot, and DjamÃ© Seddah. 2019. What Does BERT Learn\nabout the Structure of Language?. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics .\n[15] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang\nWang, and Qun Liu. 2020. TinyBERT: Distilling BERT for Natural Language\nUnderstanding.\n[16] Gyuwan Kim and Kyunghyun Cho. 2021. Length-Adaptive Transformer: Train\nOnce with Length Drop, Use Anytime with Search. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long\nPapers). Association for Computational Linguistics, 6501â€“6511.\n[17] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.\n2021. I-bert: Integer-only bert quantization. arXiv preprint arXiv:2101.01321\n(2021).\n[18] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph\nHassoun, and Kurt Keutzer. 2022. Learned Token Pruning for Transformers. In\nProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data\nMining (Washington DC, USA)(KDD â€™22). Association for Computing Machinery,\n784â€“794.\n[19] Francois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M. Rush. 2021. Block\nPruning For Faster Transformers. In EMNLP.\n[20] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\nSharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning\nof language representations. arXiv preprint arXiv:1909.11942 (2019).\n[21] Ken Lang. 1995. NewsWeeder: Learning to Filter Netnews. (1995), 331â€“339.\n[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[23] Christos Louizos, Max Welling, and Diederik P. Kingma. 2018. Learning Sparse\nNeural Networks throughğ¿0 Regularization. In International Conference on Learn-\ning Representations .\n[24] Microsoft. 2022. onnxruntime. https://onnxruntime.ai/\n[25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits\nof Transfer Learning with a Unified Text-to-Text Transformer.Journal of Machine\nLearning Research 21, 140 (2020), 1â€“67. http://jmlr.org/papers/v21/20-074.html\n[26] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Donâ€™t Know:\nUnanswerable Questions for SQuAD. InProceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Volume 2: Short Papers) . Association\nfor Computational Linguistics, 784â€“789.\n[27] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh.\n2021. Dynamicvit: Efficient vision transformers with dynamic token sparsification.\nAdvances in neural information processing systems 34 (2021), 13937â€“13949.\n[28] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A Primer in BERTology:\nWhat We Know About How BERT Works. Transactions of the Association for\nComputational Linguistics (2020).\n[29] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020.\nDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\narXiv:1910.01108 [cs.CL]\n[30] Victor Sanh, Thomas Wolf, and Alexander M Rush. 2020. Movement pruning:\nAdaptive sparsity by fine-tuning. In NeurIPS.\n[31] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,\nMichael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of the AAAI Conference on Artificial\nIntelligence.\n[32] S. Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient Knowledge Distillation\nfor BERT Model Compression. In Conference on Empirical Methods in Natural\nLanguage Processing .\n[33] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-\nRead Students Learn Better: The Impact of Student Initialization on Knowledge\nDistillation. ArXiv abs/1908.08962 (2019).\n[34] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.\nBowman. 2019. GLUE: A Multi-Task Benchmark and Analysis Platform for Natu-\nral Language Understanding. In International Conference on Learning Representa-\ntions. https://openreview.net/forum?id=rJ4km2R5t7\n[35] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan,\nand Song Han. 2020. HAT: Hardware-Aware Transformers for Efficient Natural\nLanguage Processing. In Annual Conference of the Association for Computational\nLinguistics.\n[36] Hanrui Wang, Zhekai Zhang, and Song Han. 2021. Spatten: Efficient sparse atten-\ntion architecture with cascade token and head pruning. In2021 IEEE International\nSymposium on High-Performance Computer Architecture (HPCA) . IEEE, 97â€“110.\n[37] Xuanhui Wang, Cheng Li, Nadav Golbandi, Michael Bendersky, and Marc Najork.\n2018. The LambdaLoss Framework for Ranking Metric Optimization. In Pro-\nceedings of the 27th ACM International Conference on Information and Knowledge\nManagement (Torino, Italy) (CIKM â€™18) . Association for Computing Machinery,\n1313â€“1322.\n[38] Xuanhui Wang, Cheng Li, Nadav Golbandi, Mike Bendersky, and Marc Najork.\n2018. The LambdaLoss Framework for Ranking Metric Optimization. In Proceed-\nings of The 27th ACM International Conference on Information and Knowledge\nManagement (CIKM â€™18) . 1313â€“1322.\n[39] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2020. Structured Pruning of Large\nLanguage Models. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) .\n[40] Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022. Structured Pruning Learns\nCompact and Accurate Models. In Association for Computational Linguistics\n(ACL).\n[41] Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, and Tie-Yan Liu. 2021.\nNas-BERT: task-agnostic and adaptive-size BERT compression with neural archi-\ntecture search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge\nDiscovery & Data Mining . 1933â€“1943.\n[42] Deming Ye, Yankai Lin, Yufei Huang, and Maosong Sun. 2021. TR-BERT: Dy-\nnamic Token Reduction for Accelerating BERT Inference. In Proceedings of the\n2021 Conference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies . Association for Computational\nLinguistics, 5798â€“5809.\n[43] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris\nAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer sequences. Advances in Neural\nInformation Processing Systems 33 (2020).\n[44] Li Lyna Zhang, Youkow Homma, Yujing Wang, Min Wu, Mao Yang, Ruofei Zhang,\nTing Cao, and Wei Shen. 2022. SwiftPruner: Reinforced Evolutionary Pruning for\nEfficient Ad Relevance. In Proceedings of the 31st ACM International Conference\non Information & Knowledge Management . 3654â€“3663.\nConstraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA\nA APPENDIX\nA.1 Datasets\nTable 7 summarizes the datasets used in our experiments. Vari-\nous types of tasks (classification, regression, and QA) with various\ndataset sizes and input sequence lengths are included to demon-\nstrate the wide applicability of our method.\nDataset Task Avg Sample\nLength\nInput Sequence\nLength\nCoLA Acceptability 11 64\nRTE NLI 64 256\nQQP Similarity 30 128\nMRPC Paraphrase 53 128\nSST-2 Sentiment 25 64\nMNLI NLI 39 128\nQNLI QA 51 128\nSTS-B Similarity 31 64\nSQuAD v2.0 QA 152 384\n20News Sentiment 551 512\nTable 7: Dataset statistics.\nA.2 Hyperparameters\nWe follow CoFi [40] for setting the total pruning epochs. We use\n100 pruning epochs for small GLUE datasets (such as CoLA, RTE,\nMRPC, and STS-B), and 60 epochs for large datasets (such as QQP,\nSST-2, MNLI, QNLI, and 20News). For the warmup stage, we use\nhalf of the total epochs for small datasets, and 25% for large datasets.\nThe hyper-parameter ğœ†in Equation 9 is used to balance the sig-\nnificance of token importance distillation. We use a linear scheduler\nthat reduces the value of ğœ† from its initial value throughout the\nwarmup stage. This setting allows the model parameters to adapt\nmore effectively to the teacherâ€™s token importance rankings in the\nearly pruning stages, while shifting focus to fine-tuning the model\nparameters and pruning masks for improved accuracy in the later\nstages.\nWe report the hyperparameters used in our experiments in Ta-\nble 8. ğœ†for rank-distillation loss is chosen from {1e-2, 1e-3, 1e-4}.\nLTP, Transkimmer, and CoFiPruning are trained based on their\nimplementation details in the paper and open-source code.\nHyperparameter GLUE SQuADv2.0 20news\nlearning rate {1,2,4,8}e-5 4e-5 6e-5\nbatch size 32 12 12\nwarmup epoch 50 (small), 10 (large) 5 10\ntotal epoch 100 (small), 60 (large) 10 60\nTable 8: Hyperparameters in the experiments.\nA.3 Case Study\nOne example from SQuADv2.0 dataset is presented in Table 9 to\nshow the reason why we use the attention score from the last layer\nto distill the attention score of the first few layers. For finetuned\nğµğ¸ğ‘…ğ‘‡ğ‘ğ‘ğ‘ ğ‘’ model, the attention score for the answer token ranks\n114th in layer 2 among all the attention scores of the total 170 tokens,\nwhile in layer 12 it ranks 24th. The answer token is a very important\ntoken, and should not be pruned during inference. However, it ranks\nlow in layer 2, so it will be probably pruned for a high sparsity,\nresulting in a wrong answer. After applying rank distillation, its\nrank goes higher, making it less likely to be pruned.\nQuestion: What century did the Normans first gain their separate\nidentity?\nContext: The Normans (Norman: Nourmands; French: Normands; Latin:\nNormanni) were the people who in the 10th and 11th centuries gave their\nname to Normandy, a region in France. They were descended from Norse\n(\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark,\nIceland and Norway who, under their leader Rollo, agreed to swear fealty\nto King Charles III of West Francia. Through generations of assimilation\nand mixing with the native Frankish and Roman-Gaulish populations,\ntheir descendants would gradually merge with the Carolingian-based\ncultures of West Francia. The distinct cultural and ethnic identity of the\nNormans emerged initially in the first half of the 10th century, and it\ncontinued to evolve over the succeeding centuries.\nAnswer: 10th.\nBERTğ‘ğ‘ğ‘ ğ‘’ Layer 2 Layer 12\nRank of Answer Token 114 (w/o distill) 58 (w/ distill) 24\nTable 9: Post-hoc case study of SQuADv2.0 QA task. The an-\nswer is highlighted in the context. Rank distillation makes\nthe rank of the answer token higher in the early layer.\nA.4 Inference on GPU\nWe implement a simple inference version using PyTorch to measure\nlatency on V100 GPU. Our ToP approach demonstrates latency\nspeedup on GPUs, achieving 1.2x speedup on RTE (20.04 ms) and\n1.24x speedup on MRPC (11.00 ms), although the acceleration ratio\nis not as high as on CPUs.\nWe believe that there is still potential for improving GPU infer-\nence acceleration with high-performance inference engines and\nsystem optimizations. We found that token pruning requires some\nmemory operations, such as removing tokens with mask value 0\nfrom hidden states. Although this operation requires no computa-\ntion, it is time-consuming on GPU. In our future work, we plan to\nutilize high-performance inference engines and leverage system\noptimizations to achieve greater GPU inference acceleration.",
  "topic": "Security token",
  "concepts": [
    {
      "name": "Security token",
      "score": 0.7272507548332214
    },
    {
      "name": "Inference",
      "score": 0.6124860048294067
    },
    {
      "name": "Computer science",
      "score": 0.6099553108215332
    },
    {
      "name": "Constraint (computer-aided design)",
      "score": 0.5872999429702759
    },
    {
      "name": "Transformer",
      "score": 0.5455571413040161
    },
    {
      "name": "Pruning",
      "score": 0.5078058838844299
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.4634411633014679
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4367567002773285
    },
    {
      "name": "Mathematics",
      "score": 0.23297721147537231
    },
    {
      "name": "Computer security",
      "score": 0.15564125776290894
    },
    {
      "name": "Engineering",
      "score": 0.09042161703109741
    },
    {
      "name": "Biology",
      "score": 0.07068473100662231
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ]
}