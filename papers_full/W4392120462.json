{
  "title": "Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization",
  "url": "https://openalex.org/W4392120462",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2100271886",
      "name": "Jiliang Li",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2102177540",
      "name": "Yifan Zhang",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2469176531",
      "name": "Zachary Karas",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2101030568",
      "name": "Collin McMillan",
      "affiliations": [
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A1983588154",
      "name": "Kevin Leach",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2106566374",
      "name": "Yu Huang",
      "affiliations": [
        "Vanderbilt University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2947430838",
    "https://openalex.org/W6752934852",
    "https://openalex.org/W2898936689",
    "https://openalex.org/W2079887492",
    "https://openalex.org/W2240536489",
    "https://openalex.org/W6627919676",
    "https://openalex.org/W6761280018",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W4210440021",
    "https://openalex.org/W1901145490",
    "https://openalex.org/W2362779523",
    "https://openalex.org/W3092785544",
    "https://openalex.org/W6600339963",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4285490465",
    "https://openalex.org/W6601158483",
    "https://openalex.org/W2949547800",
    "https://openalex.org/W4283751459",
    "https://openalex.org/W3031696893"
  ],
  "abstract": "Recent language models have demonstrated proficiency in summarizing source code. However, as in many other domains of machine learning, language models of code lack sufficient explainability. Informally, we lack a formulaic or intuitive understanding of what and how models learn from code. Explainability of language models can be partially provided if, as the models learn to produce higher-quality code summaries, they also align in deeming the same code parts important as those identified by human programmers. In this paper, we report negative results from our investigation of explainability of language models in code summarization through the lens of human comprehension. We measure human focus on code using eye-tracking metrics such as fixation counts and duration in code summarization tasks. To approximate language model focus, we employ a state-of-the-art model-agnostic, black-box, perturbation-based approach, SHAP (SHapley Additive exPlanations), to identify which code tokens influence that generation of summaries. Using these settings, we find no statistically significant relationship between language models' focus and human programmers' attention. Furthermore, alignment between model and human foci in this setting does not seem to dictate the quality of the LLM-generated summaries. Our study highlights an inability to align human focus with SHAP-based model focus measures. This result calls for future investigation of multiple open questions for explainable language models for code summarization and software engineering tasks in general, including the training mechanisms of language models for code, whether there is an alignment between human and model attention on code, whether human attention can improve the development of language models, and what other model focus measures are appropriate for improving explainability.",
  "full_text": "Do Machines and Humans Focus on Similar Code? Exploring\nExplainability of Large Language Models in Code Summarization\nJiliang Li\nVanderbilt University\nNashville, Tennessee, USA\njiliang.li@vanderbilt.edu\nYifan Zhang\nVanderbilt University\nNashville, Tennessee, USA\nyifan.zhang.2@vanderbilt.edu\nZachary Karas\nVanderbilt University\nNashville, Tennessee, USA\nz.karas@vanderbilt.edu\nCollin McMillan\nUniversity of Notre Dame\nNotre Dame, Indiana, USA\ncmc@nd.edu\nKevin Leach\nVanderbilt University\nNashville, Tennessee, USA\nkevin.leach@vanderbilt.edu\nYu Huang\nVanderbilt University\nNashville, Tennessee, USA\nyu.huang@vanderbilt.edu\nABSTRACT\nRecent language models have demonstrated proficiency in summa-\nrizing source code. However, as in many other domains of machine\nlearning, language models of code lack sufficient explainability â€”\ninformally, we lack a formulaic or intuitive understanding of what\nand how models learn from code. Explainability of language models\ncan be partially provided if, as the models learn to produce higher-\nquality code summaries, they also align in deeming the same code\nparts important as those identified by human programmers. In this\npaper, we report negative results from our investigation of explain-\nability of language models in code summarization through the lens\nof human comprehension. We measure human focus on code using\neye-tracking metrics such as fixation counts and duration in code\nsummarization tasks. To approximate language model focus, we\nemploy a state-of-the-art model-agnostic, black-box, perturbation-\nbased approach, SHAP (SHapley Additive exPlanations), to identify\nwhich code tokens influence that generation of summaries. Using\nthese settings, we find no statistically significant relationship be-\ntween language modelsâ€™ focus and human programmersâ€™ attention.\nFurthermore, alignment between model and human foci in this\nsetting does not seem to dictate the quality of the LLM-generated\nsummaries. Our study highlights an inability to align human focus\nwith SHAP-based model focus measures. This result calls for future\ninvestigation of multiple open questions for explainable language\nmodels for code summarization and software engineering tasks in\ngeneral, including the training mechanisms of language models for\ncode, whether there is an alignment between human and model\nattention on code, whether human attention can improve the devel-\nopment of language models, and what other model focus measures\nare appropriate for improving explainability.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Artificial intelligence.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nICPC â€™24, April 15â€“16, 2024, Lisbon, Portugal\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0586-1/24/04. . . $15.00\nhttps://doi.org/10.1145/3643916.3644434\nKEYWORDS\nNeural Code Summarization, Language Models, Explainable AI,\nSHAP, Human Attention, Eye-Tracking\nACM Reference Format:\nJiliang Li, Yifan Zhang, Zachary Karas, Collin McMillan, Kevin Leach, and Yu\nHuang. 2024. Do Machines and Humans Focus on Similar Code? Exploring\nExplainability of Large Language Models in Code Summarization. In 32nd\nIEEE/ACM International Conference on Program Comprehension (ICPC â€™24),\nApril 15â€“16, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 5 pages.\nhttps://doi.org/10.1145/3643916.3644434\n1 INTRODUCTION\nRecent language models for code have shown promising perfor-\nmance on several code-related tasks [ 31]. Among these tasks is\nneural code summarization, where a language model generates a\nshort natural language summary describing a given code snippet.\nThis is often an indicative task demonstrating a modelâ€™s ability\nto comprehend code. Currently, the majority of assessments for\nhow well a language model understands code directly measures the\nquality of code summaries generated by the models, and compares\nthem with human-written summaries [31]. Comparatively little is\nknown about why and how the language models reason about code\nto generate such summaries. Similar to many other downstream\ndomains of machine learning in software engineering, understand-\ning and explaining how and why language models for code work\n(or fail) is critical to improving model architecture, reducing bias,\nand preventing undesirable model behavior.\nHuman programmers typically achieve a strong understanding\nof code. Thus, proficient language models might be explained if\nthey focus on the same parts of code that humans would [21]. Eye-\ntracking studies have been conducted to analyze programmersâ€™\nvisual patterns while reading code [2, 22]. Specifically, the duration\nand frequency of a programmerâ€™s eye gaze on a part of code in a\nspatially-stable manner, referred to asfixation duration and fixation\ncount respectively, are indicative of cognitive load [24]. Thus, these\nmeasures of eye-tracking can indicate the parts of code on which\nhuman programmers focus. In contrast, there is a lack of consensus\non how to measure a language modelâ€™s reasoning about code (see\nSection 2.2). Most existing works extract the self-attention layers in\nlanguage models for code to measure the model attention [21, 20, 9].\narXiv:2402.14182v1  [cs.SE]  22 Feb 2024\nICPC â€™24, April 15â€“16, 2024, Lisbon, Portugal Jiliang Li, Yifan Zhang, Zachary Karas, Collin McMillan, Kevin Leach, and Yu Huang\nSuch methods require direct access to the internal layers of a lan-\nguage model, limiting the possibility to investigate interpretability\nof many state-of-the-art proprietary models (e.g., ChatGPT).\nIn this paper, to investigate how proprietary language models\nreason about code, we employ a state-of-the-art perturbation-based\nmethod, SHAP [15] (SHapley Additive exPlanations), that treats\neach language model as a black-box function. With SHAP, we an-\nalyze the feature attribution (i.e., which parts of code are deemed\nimportant by the model) in six different state-of-the-art language\nmodels for code. We use a set of Java methods to task both the\nlanguage models and human programmers with writing code sum-\nmaries. The feature attribution in the language models, measured\nby SHAP, is then compared with human developersâ€™ focus, col-\nlected from eye-tracking. We hypothesize that sufficiently large\nmodels may learn to focus on parts of code similarly to humans. If\nvalidated, language model behavior can thus be described in terms\nof human behavior, ultimately helping to explain and improve\nlanguage models. However, we find that explainability cannot be\nprovided through this lens and find no statistically significant evi-\ndence suggesting the hypothesized alignment. Furthermore, we did\nnot find that language modelsâ€™ focus exhibits a statistically signifi-\ncant correlation with human focus in general. For future research\nthat aims to explore the explainability of language models for code\nsummarization, especially for those leveraging human attention,\nour findings might suggest the following: (1) though widely used\nin AI, SHAP may not be an optimal method to investigate where\nlanguage models focus during code summarization, or alternatively,\n(2) a misalignment between language models and human develop-\ners in reasoning about code may provide insights for improving AI\nmodels for code summarization.\n2 BACKGROUND AND RELATED WORK\n2.1 Neural Models for Code Summarization\nAdvancements in deep learning have enabled machine learning\nmodels to generate summaries for source code. Among the state-\nof-the-art models, NeuralCodeSum (NCS) first introduced the use\nof Transformers in neural code summarization [3]. With the rise\nof large language models (LLMs), ServiceNow and HuggingFace\nreleased a 15.5B parameter LLM for code, StarCoder [13], and Meta\nreleased a 7B parameter LLM, Code LLama [23], both of which can\nserve to summarize code. Although not inherently an LLM for code,\nGPT3.5 [18] and GPT4 [19] are also capable of code summarization.\nIn this paper, we investigate how all the aforementioned models\nreason about code when tasked to generate code summaries.\n2.2 Interpretability of Language Models\nExisting works on interpretable language models generally seek to\ninvestigate the relative importance of each input token for model\nperformance [29, 8, 16]. Such works can be commonly categorized\ninto two types: white-box vs. black-box. White-box approaches\nrequire access to a language modelâ€™s internal layers [25, 28], often\ndirectly investigating the self-attention scores in Transformer-based\nmodels [7, 33, 32]. However, Transformer-based modelsâ€™ inherent\ncomplexity has led to a lack of consensus on how to aggregate\nattention weights [30, 35, 33]. For the general research community,\nwhite-box approaches preclude proprietary models (e.g., ChatGPT).\nIn contrast, state-of-the-art black-box approaches like SHAP [15]\n(SHapley Additive exPlanations) apply game-theoretic principles\nto assess the impact of input variations on a modelâ€™s output. SHAP\nevaluates the effects of different combinations of input features\nâ€” such as tokens in a text sequence â€” by observing how their\npresence or absence (simulated by token masking) alters the modelâ€™s\nprediction from an expected result. This process helps to ascertain\nthe relative contribution of each feature to the output, allowing for\nan analysis of the model without requiring access to its internal\narchitecture [14, 11]. In this paper, to investigate proprietary models,\nwe employ SHAP to measure where language models focus on code.\n2.3 Comparing Human vs. Machine Attention\nPrevious papers have examined the alignment between human\nand model attention in code comprehension tasks. Paltenghi et\nal. [20] found that CodeGenâ€™s [17] self-attention layers attend to\nsimilar parts of code compared to human programmersâ€™ visual\nfixations when answering comprehension questions about code.\nSimilarly, Huber et al. [9] discovered overlaps in attention patterns\nbetween neural models and humans when repairing buggy pro-\ngrams. Notably, Paltenghi and Prasdel [ 21] compared language\nmodelsâ€™ self-attention weights and humansâ€™ visual attention during\ncode summarization. They found that model attention, measured\nby self-attention weights, does not align well with human attention.\nHowever, this work is limited by investigating only small CNN and\ntransformer models. Most importantly, all aforementioned studies\nused white-box approaches towards interpretability of open-source\nmodels, limiting applicability to state-of-the-art proprietary models.\nRecently, Kou et al. [11] utilized both white-box and black-box\nperturbation-based approaches to measure LLMsâ€™ focus in code\ngeneration tasks, and discovered a consistent misalignment with\nhumansâ€™ attention. In general, these works have demonstrated that\nwhether human and machine attention align depends heavily on\nthe methods employed to approximate machine focus, as well as\nthe specific code comprehension task examined. In this paper, we\nbuild upon former works by examining whether human attention\ncorrelates with feature attribution in language models, measured by\na black-box perturbation-based approach, in code summarization.\n3 EXPERIMENTAL DESIGN\n3.1 Measuring Human Visual Focus\nWe used eye-tracking data measuring human attention from a con-\ntrolled human study with 27 programmers. The study obtained\nIRB approval, and asked participants to read Java methods and\nwrite accompanying summaries [4]. Each participant summarized\n24â€“25 Java methods from the FunCom dataset [ 12], yielding 671\ntrials of eye-tracking data in total. Considering data quality, two\nauthors with five and eight years of Java experience cooperatively\nremoved participant data associated with five summaries that did\nnot demonstrate an understanding of the Java code.\nIn this work, we sought to measure where humans and language\nmodels focus on code as they summarize it. We first used thesrcML\nparser to convert each Java method into its corresponding Abstract\nSyntax Tree (AST) representation [6]. The AST provides structural\ncontext for each token literal (i.e., â€˜Hello Worldâ€™ âˆ’ â†’String Literal).\nWith the gaze coordinates collected from the eye-tracker [1], we\nDo Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization ICPC â€™24, April 15â€“16, 2024, Lisbon, Portugal\nmeasured humansâ€™ focus on each AST token. Typically, researchers\nuse fixations to quantify human visual focus [24]. A fixation is de-\nfined as a spatially stable eye-movement lasting 100â€“300ms. Most\ncognitive processing occurs during fixations [24], so researchers\nconsider their frequency and duration in making inferences about\nhuman cognition. In our analyses, we computed the average count\nand duration of programmersâ€™ fixations on each AST token. Conse-\nquently, for each Java method, we obtained two visual focus vectors\nwith lengths equal to the number of AST tokens, respectively, which\nrepresent fixation counts and durations on each token1.\n3.2 Measuring Model Focus\nAs mentioned in Section 2.2, we choose SHAPâ€™s official, default\nimplementation of the TeacherForcing method to measure feature\nattribution in language models, treating each as a black-box func-\ntion. For each language model, we pass in each of the 68 Java\nmethods (also read by human programmers) as input, along with\nnecessary prompting for the model to output summaries of source\ncode. For each Java method passed into each language model, we\nlet ğ‘– denote an input token (in code) and ğ‘œ denote an output token\n(in summary). For each (ğ‘–, ğ‘œ) pair, SHAP produces an importance\nscore, denoted ğ‘£ (ğ‘–,ğ‘œ ), signifying how much ğ‘–â€™s presence or absence\nalters the presence of ğ‘œ. Then, the importance score of each input\ntoken2, ğ‘£ğ‘– , is calculated such that ğ‘£ğ‘– = Ã\nğ‘œ |ğ‘£ (ğ‘–,ğ‘œ )|. Note that now ğ‘£ğ‘–\nis associated with a language model token, and each AST token may\nconsist of several language model tokens. Thus, for each AST token,\nwe calculate its score 1\nğ‘›\nÃğ‘›\nğ‘—=1 ğ‘£ ğ‘— , where ğ‘£1, Â·Â·Â· , ğ‘£ğ‘› are scores of lan-\nguage model tokens constituting the AST token. Consequently, for\neach language model on each Java method, we obtain a focus vector\n(with a length equal to the number of AST tokens) representing\nhow influential each AST token is to the model.\nIn total, we investigated the model focus of six different models:\nGPT4, GPT-few-shot, GPT3.5, StarCoder, Code Llama, and NCS.\nHere, GPT-few-shot is a GPT3.5 model, but in an attempt for the\nmodel to produce code summaries more similar to those of humans,\nwe used few-shot prompting to instruct the model to provide sum-\nmaries similar to two randomly selected human-written summaries.\nThe other five state-of-the-art LLMs are introduced in Section 2.1\nand implemented with their default parameters.\n3.3 Comparing Human and Model Foci\nFor brevity, we refer to two human visual focus measurements\n(i.e., fixation duration and count) and six language models as eight\n\"focus sources. \" For each source, we obtained 68 focus vectors, each\ncorresponding to a Java method. These vectors were normalized\nto sum to 1, and reflect how important each AST token is for the\nhuman/model. We answer these research questions:\nâ€¢RQ1: Is there a general correlation between human and ma-\nchine focus patterns for code summarization?\nâ€¢RQ2: Do the code summaries increase in quality when ma-\nchine focus becomes more aligned with that of humans?\n3.3.1 RQ1. We assess the correlation between human and machine\nfoci across the 68 Java methods. Specifically, for each pair of focus\n1Our analyses do not include brackets or semi-colons, or other such syntactic elements.\n2We use the absolute value by choice, without which experiments show similar results.\nsources, we iterate through each Java method and calculate the\nSpearmanâ€™s rank coefficient (ğœŒ) [27] between the two sourcesâ€™ vec-\ntors for that method. Then, for each pair of focus sources, we report:\n(1) The mean and standard deviation of Spearmanâ€™sğœŒ across all Java\nmethods where correlation is statistically significant ( ğ‘ â‰¤0.05),\nand (2) the proportion of Java methods demonstrating a statistically\nsignificant correlation (ğ‘ â‰¤0.05).\nIn addition, we group all AST tokens into 18 semantic categories\n(e.g., method call, operator, etc.) and investigate how much hu-\nmans3 and language models focus on each semantic category. The\nfocus score assigned to each semantic category is the sum of the\nfocus scores assigned to each AST token belonging to that seman-\ntic category. To counter biases where certain semantic categories\ncontain more AST tokens or appear more frequently, we report\nthe relative difference between machine and human foci for each\nsemantic category. That is, we average the six language modelsâ€™\nfocus scores per category and report |ğ‘“ ğ‘œğ‘ğ‘šğ‘ğ‘â„ğ‘–ğ‘›ğ‘’âˆ’ğ‘“ ğ‘œğ‘â„ğ‘¢ğ‘šğ‘ğ‘›\nğ‘“ ğ‘œğ‘â„ğ‘¢ğ‘šğ‘ğ‘›\n|.\n3.3.2 RQ2. Here, a human expert provides quality ratings for sum-\nmaries generated by each language model for every Java method\nusing four criteria: accuracy, completeness, conciseness, and read-\nability. Next, we calculate the Spearmanâ€™sğœŒ between each language\nmodelâ€™s focus vector and the human fixation duration vector across\nall Java methods where correlation is significant ( ğ‘ â‰¤0.05). We\nthen append all such statistically significantğœŒâ€™s to form a vector, de-\nnoted ğ‘£ğ‘ğ‘œğ‘Ÿ , to represent the degrees of alignment between machine\nand human foci across the Java methods investigated4.\nSubsequently, we determine whether this alignment is correlated\nwith the rated quality of summaries. Specifically, we construct four\nother vectors, {ğ‘£ğ‘ğ‘ğ‘, ğ‘£ğ‘ğ‘œğ‘š, ğ‘£ğ‘ğ‘œğ‘› , ğ‘£ğ‘Ÿğ‘’ğ‘ }, containing the accuracy, com-\npleteness, conciseness, and readability scores respectively. At each\nindex ğ‘–, {ğ‘£ğ‘ğ‘œğ‘Ÿ [ğ‘–], ğ‘£ğ‘ğ‘ğ‘ [ğ‘–], ğ‘£ğ‘ğ‘œğ‘š [ğ‘–], ğ‘£ğ‘ğ‘œğ‘› [ğ‘–], ğ‘£ğ‘Ÿğ‘’ğ‘ [ğ‘–]}are respectively\nthe Spearmanâ€™s ğœŒ, summary accuracy, completeness, conciseness,\nand readability of the same language model applied on the same\nJava method. We then measure and report the Spearmanâ€™s rank\ncorrelation between ğ‘£ğ‘ğ‘œğ‘Ÿ and ğ‘£ğ‘– , where ğ‘£ğ‘– âˆˆ{ğ‘£ğ‘ğ‘ğ‘, ğ‘£ğ‘ğ‘œğ‘š, ğ‘£ğ‘ğ‘œğ‘› , ğ‘£ğ‘Ÿğ‘’ğ‘ }.\n4 RESULTS\n4.1 RQ1: General Correlation\nAs shown in Table 1, there is a general lack of correlation between\nhuman and machine foci. We highlight that the means and standard\ndeviations in Table 1 are only calculated from Java methods where\nthe Spearmanâ€™s ğœŒ is statistically significant (with ğ‘ â‰¤ 0.05). In\npractice, between any pair of human-LLM focus sources, at most\n22% of the 68 Java methods yield a Spearmanâ€™s ğœŒ with ğ‘ â‰¤0.05.\nAs a baseline, the Spearmanâ€™s ğœŒ has ğ‘ â‰¤0.05 for all Java methods\nbetween human duration and fixation focus vectors, and for 85% of\nJava methods between any two language modelâ€™s focus vectors. This\nimplies that any existing correlation between human and machine\nfoci is not widespread across the Java methods studied.\nFurthermore, among those Java methods where the correlation is\nstatistically significant, the mean Spearmanâ€™sğœŒ is small with a large\n3We use fixation durations to represent human focus. We empirically verify that using\nfixation count yields similar results.\n4Note that ğ‘£ğ‘ğ‘œğ‘Ÿ contains Spearmanâ€™s ğœŒâ€™s obtained from all six language models. We\nempirically verify that conducting the analogous analysis for each language model\nseparately yields a similar result.\nICPC â€™24, April 15â€“16, 2024, Lisbon, Portugal Jiliang Li, Yifan Zhang, Zachary Karas, Collin McMillan, Kevin Leach, and Yu Huang\nTable 1: Pair-wise correlation among focus sources; â€œDura-\ntionâ€ and â€œCountâ€ are human visual focus. Each cell shows\nthe means and standard deviations of Spearmanâ€™s ğœŒ for all\nJava methods showing significant correlation ( ğ‘ â‰¤0.05).\nDuration Count GPT4 GPT-few GPT3.5 StarCoder CodeLlama NCS\nDuration 1.00Â±0.00 0.88Â±0.06 -0.11Â±0.41 -0.13Â±0.42 -0.09Â±0.52 -0.18Â±0.48 -0.18Â±0.42 -0.24Â±0.40\nCount â€” 1.00Â±0.00 0.01Â±0.45 -0.24Â±0.33 -0.10Â±0.48 -0.31Â±0.29 -0.13Â±0.43 -0.33Â±0.33\nGPT4 â€” â€” 1.00 Â±0.00 0.68 Â±0.12 0.76 Â±0.12 0.67 Â±0.14 0.67 Â±0.14 0.55 Â±0.13\nGPT-few â€” â€” â€” 1.00 Â±0.00 0.72 Â±0.12 0.62 Â±0.15 0.64 Â±0.15 0.55 Â±0.13\nGPT3.5 â€” â€” â€” â€” 1.00 Â±0.00 0.65 Â±0.16 0.67 Â±0.15 0.58 Â±0.13\nStarCoder â€” â€” â€” â€” â€” 1.00 Â±0.00 0.66 Â±0.15 0.59 Â±0.11\nCode Llama â€” â€” â€” â€” â€” â€” 1.00 Â±0.00 0.56 Â±0.14\nNCS â€” â€” â€” â€” â€” â€” â€” 1.00 Â±0.00\n0 50 100 150 200\nDifference (%)\nLiteral\nAssignment\nOperator\nComment\nOperation\nException Handling\nMethod Call\nExternal Variable/Method\nReturn\nVariable\nExternal Class\nArgument\nConditional Block\nLoop\nConditional Statement\nVariable Declaration\nMethod Declaration\nParameter\nSemantic Category\nRelative Difference in Machine vs. Human Focus on Semantic Categories\nFigure 1: How much more/less do language models focus on\neach semantic category compared to humans?\nstandard deviation. In fact, for most such methods where a model\nand human show significant correlation in focus, the Spearmanâ€™s\nğœŒ is often either around 0.5 or âˆ’0.5, but rarely in between. This\nsuggests the relationship between human and machine foci varies\nsignificantly depending on the specific Java method.\nInterestingly, although few-shot-alignment in GPT-few-shot ren-\nders the modelâ€™s generated summaries more similar to those of\nhumans, this does not lead to higher correlations between model\nand human foci. In addition, feature attribution in all language\nmodels is moderately or strongly positively correlated with each\nother on a majority of Java methods, which intuitively makes sense\nsince all six models are based on the Transformer architecture.\nWe also investigate how language modelsâ€™ focus on each seman-\ntic category differs from that of humans. As shown in Figure 1,\nlanguage modelsâ€™ generation of code summaries seems to be more\nreliant on comments, return values, and specific statements such\nas literals and assignments, and less reliant on method calls and\nvariables/methods not defined explicitly within the Java method.\nDiscussion Point 1: We find no evidence that feature attribution\nin language models is correlated with programmersâ€™ visual focus.\nSeveral possible interpretations can be inferred: (1) Alternative\nmethods may be needed to assess feature influence in black-box\nlanguage models for code summarization, aiming for better align-\nment with human attention. (2) Access to the internal workings of\nproprietary models might become critical if white-box models offer\nmore human-aligned insights into explainable language models\nfor code [20]. (3) It is possible that language models and humans\nreason about code differently when summarizing source code.\nTable 2: Correlation between human-machine focus align-\nment and summary quality (assessed by four metrics).\nAccuracy Completeness Conciseness Readability\nSpearmanâ€™sğœŒ -0.1279 0.1309 0.0194 -0.0717\nğ‘-value 0.3862 0.3753 0.8960 0.6280\nGPT4 GPT-few-shot GPT3.5 StarCoder Code Llama NCS\nLanguage Models\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5Ratings\nRatings of Language Models on a Scale from 1-4\nAccuracy\nCompleteness\nConciseness\nReadability\nFigure 2: Average ratings of model-generated summaries.\n4.2 RQ2: Summary Qualities\nThere is also a lack of correlation between the quality of summaries\ngenerated by language models and how well their focus on code\naligns with humansâ€™. The large p-values in Table 1 suggest that,\nregardless of which metric is used to assess summary quality, there\nis a lack of statistically significant correlation between the quality\nof a model-generated summary on a Java method and how well\nthe modelâ€™s focus aligns with that of humans on that Java method.\nFurthermore, Figure 2 shows that NCS produces worse summaries\nthan the other five models. Although Table 1 seems to suggest that\nNCSâ€™s focus is more negatively aligned with human attention, we\nfind no statistically significant metrics supporting such a claim,\npartially due to the small sample size of Java methods yielding\nstatistically significant Spearmanâ€™s ğœŒ.\nIn general, Table 1 suggests that feature attribution in NCS is still\nmoderately positively aligned with that in other language models\non a majority of Java methods. This indicates the likelihood that\naspects other than feature attribution are more indicative of and\ncritical to a language modelâ€™s performance in code summarization.\nDiscussion Point 2: With a substantial body of work in NLP\nshowing that aligning neural models with human visual patterns\ncan lead to performance improvement [26, 34, 10, 5], we contain\nour conclusion to the SHAP measure of feature attribution and the\nhuman attention as measured in an eye-tracking experiment. The\nlink between human attention and feature attribution to machine\nmodels is a subject of intense scientific investigation. We contribute\nto the debate with this finding that SHAP did not correlate with\nhuman eye attention in the measures or models we studied.\n5 CONCLUSION\nIn this paper, we use a state-of-the-art, black-box, perturbation-\nbased method to assess feature attribution in language models on\ncode summarization tasks. We then compare the model-determined\nimportant AST tokens with those identified by human visual focus,\nas measured through eye-tracking. The results suggest that using\nSHAP to measure feature attribution does not provide explainabil-\nity of language models through establishing correlations between\nmachine and human foci. Generally, our work can be interpreted\nin two ways. First, feature attribution measured by SHAP may not\nbe the best way to interpret a language modelâ€™s focus during code\nsummarization as it fails to establish similarities with human focus.\nAlternatively, it may be the case that machines reason about code\ndifferently from humans when tasked to summarize source code.\n6 ACKNOWLEDGEMENT\nThis research was supported by NSF CCF-2211429, NSF CCF-2211428,\nNSF CCF-2100035 and NSF SaTC-2312057.\nDo Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization ICPC â€™24, April 15â€“16, 2024, Lisbon, Portugal\nREFERENCES\n[1] Tobii pro fusion user manual, Jun 2023.\n[2] Abid, N. J., Maletic, J. I., and Sharif, B.Using developer eye movements to\nexternalize the mental model used in code summarization tasks. In Proceedings of\nthe 11th ACM Symposium on Eye Tracking Research & Applications (2019), pp. 1â€“9.\n[3] Ahmad, W. U., Chakraborty, S., Ray, B., and Chang, K.-W.A transformer-based\napproach for source code summarization. arXiv preprint arXiv:2005.00653 (2020).\n[4] Bansal, A., Su, C.-Y., Karas, Z., Zhang, Y., Huang, Y., Li, T. J.-J., and McMil-\nlan, C.Modeling programmer attention as scanpath prediction. arXiv preprint\narXiv:2308.13920 (2023).\n[5] Barrett, M., Bingel, J., Hollenstein, N., Rei, M., and SÃ¸gaard, A.Sequence\nclassification with human attention. In Proceedings of the 22nd conference on\ncomputational natural language learning (2018), pp. 302â€“312.\n[6] Collard, M. L., Decker, M. J., and Maletic, J. I.srcml: An infrastructure for the\nexploration, analysis, and manipulation of source code: A tool demonstration. In\n2013 IEEE International conference on software maintenance (2013), IEEE, pp. 516â€“\n519.\n[7] Galassi, A., Lippi, M., and Torroni, P.Attention in natural language processing.\nIEEE transactions on neural networks and learning systems 32 , 10 (2020), 4291â€“4308.\n[8] Hooker, S., Erhan, D., Kindermans, P.-J., and Kim, B.Evaluating feature im-\nportance estimates.\n[9] Huber, D., Paltenghi, M., and Pradel, M.Where to look when repairing\ncode? comparing the attention of neural models and developers. arXiv preprint\narXiv:2305.07287 (2023).\n[10] Klerke, S., Goldberg, Y., and SÃ¸gaard, A.Improving sentence compression by\nlearning to predict gaze. arXiv preprint arXiv:1604.03357 (2016).\n[11] Kou, B., Chen, S., Wang, Z., Ma, L., and Zhang, T.Is model attention aligned\nwith human attention? an empirical study on large language models for code\ngeneration. arXiv preprint arXiv:2306.01220 (2023).\n[12] LeClair, A., and McMillan, C.Recommendations for datasets for source code\nsummarization. arXiv preprint arXiv:1904.02660 (2019).\n[13] Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone,\nM., Akiki, C., Li, J., Chim, J., et al.Starcoder: may the source be with you! arXiv\npreprint arXiv:2305.06161 (2023).\n[14] Liu, Y., Tantithamthavorn, C., Liu, Y., and Li, L.On the reliability and explain-\nability of automated code generation approaches. arXiv preprint arXiv:2302.09587\n(2023).\n[15] Lundberg, S. M., and Lee, S.-I.A unified approach to interpreting model predic-\ntions. Advances in neural information processing systems 30 (2017).\n[16] Molnar, C.Interpretable machine learning . Lulu. com, 2020.\n[17] Nijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S.,\nand Xiong, C.Codegen: An open large language model for code with multi-turn\nprogram synthesis. arXiv preprint arXiv:2203.13474 (2022).\n[18] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt/, 2023. Accessed:\n11/20/2023.\n[19] OpenAI, R.Gpt-4 technical report. arxiv 2303.08774. View in Article 2 (2023).\n[20] Paltenghi, M., Pandita, R., Henley, A. Z., and Ziegler, A.Extracting meaning-\nful attention on source code: An empirical study of developer and neural model\ncode exploration. arXiv preprint arXiv:2210.05506 (2022).\n[21] Paltenghi, M., and Pradel, M.Thinking like a developer? comparing the atten-\ntion of humans with neural models of code. In 2021 36th IEEE/ACM International\nConference on Automated Software Engineering (ASE) (2021), IEEE, pp. 867â€“879.\n[22] Rodeghero, P., and McMillan, C.An empirical study on the patterns of eye\nmovement during summarization tasks. In 2015 ACM/IEEE International Sympo-\nsium on Empirical Software Engineering and Measurement (ESEM) (2015), IEEE,\npp. 1â€“10.\n[23] Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y.,\nLiu, J., Remez, T., Rapin, J., et al.Code llama: Open foundation models for code.\narXiv preprint arXiv:2308.12950 (2023).\n[24] Sharafi, Z., Shaffer, T., Sharif, B., and GuÃ©hÃ©neuc, Y.-G.Eye-tracking metrics\nin software engineering. In 2015 Asia-Pacific Software Engineering Conference\n(APSEC) (2015), IEEE, pp. 96â€“103.\n[25] Shrikumar, A., Greenside, P., and Kundaje, A.Learning important features\nthrough propagating activation differences. InInternational conference on machine\nlearning (2017), PMLR, pp. 3145â€“3153.\n[26] Sood, E., Tannert, S., MÃ¼ller, P., and Bulling, A.Improving natural language\nprocessing tasks with human gaze-guided neural attention. Advances in Neural\nInformation Processing Systems 33 (2020), 6327â€“6341.\n[27] Spearman, C.The proof and measurement of association between two things.\n[28] Sundararajan, M., Taly, A., and Yan, Q.Axiomatic attribution for deep net-\nworks. In International conference on machine learning (2017), PMLR, pp. 3319â€“\n3328.\n[29] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,\nKaiser, Å., and Polosukhin, I.Attention is all you need. Advances in neural\ninformation processing systems 30 (2017).\n[30] Wang, Y., Wang, K., and Wang, L.Wheacha: A method for explaining the\npredictions of code summarization models. arXiv preprint arXiv:2102.04625 (2021).\n[31] Xu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J.A systematic evalua-\ntion of large language models of code. In Proceedings of the 6th ACM SIGPLAN\nInternational Symposium on Machine Programming (2022), pp. 1â€“10.\n[32] Zeng, Z., Tan, H., Zhang, H., Li, J., Zhang, Y., and Zhang, L.An extensive study\non pre-trained models for program understanding and generation. In Proceedings\nof the 31st ACM SIGSOFT international symposium on software testing and analysis\n(2022), pp. 39â€“51.\n[33] Zhang, K., Li, G., and Jin, Z.What does transformer learn about source code?\narXiv preprint arXiv:2207.08466 (2022).\n[34] Zhang, Y., and Zhang, C.Using human attention to extract keyphrase from\nmicroblog post. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics (2019), pp. 5867â€“5872.\n[35] Zhang, Z., Zhang, H., Shen, B., and Gu, X.Diet code is healthy: Simplifying\nprograms for pre-trained models of code. In Proceedings of the 30th ACM Joint\nEuropean Software Engineering Conference and Symposium on the Foundations of\nSoftware Engineering (2022), pp. 1073â€“1084.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8387921452522278
    },
    {
      "name": "Automatic summarization",
      "score": 0.809027373790741
    },
    {
      "name": "Focus (optics)",
      "score": 0.6908882856369019
    },
    {
      "name": "Code (set theory)",
      "score": 0.5776705741882324
    },
    {
      "name": "Language model",
      "score": 0.5394424200057983
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5208804607391357
    },
    {
      "name": "Natural language processing",
      "score": 0.5006468296051025
    },
    {
      "name": "Source code",
      "score": 0.47427257895469666
    },
    {
      "name": "Source lines of code",
      "score": 0.4330001771450043
    },
    {
      "name": "Programming language",
      "score": 0.36511683464050293
    },
    {
      "name": "Software",
      "score": 0.3556289076805115
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ]
}