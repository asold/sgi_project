{
  "title": "SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization",
  "url": "https://openalex.org/W4389520387",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2741638872",
      "name": "Philippe Laban",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2970530574",
      "name": "Wojciech Kryscinski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2186761436",
      "name": "Divyansh Agarwal",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4287890854",
      "name": "Alexander Fabbri",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2095665791",
      "name": "Caiming Xiong",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1208744602",
      "name": "Shafiq Joty",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2532837580",
      "name": "Chien-Sheng Wu",
      "affiliations": [
        "Salesforce (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4226118367",
    "https://openalex.org/W4310823636",
    "https://openalex.org/W4283385703",
    "https://openalex.org/W3101551503",
    "https://openalex.org/W4366823941",
    "https://openalex.org/W4288379066",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3091889354",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4361806892",
    "https://openalex.org/W3171639395",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W3170432046",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W4287816639",
    "https://openalex.org/W2918408501",
    "https://openalex.org/W4385572850",
    "https://openalex.org/W3101600240",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W4361230777",
    "https://openalex.org/W4366549021",
    "https://openalex.org/W2971034336",
    "https://openalex.org/W3153947101",
    "https://openalex.org/W4285240908",
    "https://openalex.org/W4386576630",
    "https://openalex.org/W4385572316",
    "https://openalex.org/W2989743967",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4297435087",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W3099396524",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W4296711106",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W4287854670",
    "https://openalex.org/W4307106504",
    "https://openalex.org/W2951211142",
    "https://openalex.org/W3106234277",
    "https://openalex.org/W3034188538",
    "https://openalex.org/W4205477024",
    "https://openalex.org/W3170180819",
    "https://openalex.org/W4385573863",
    "https://openalex.org/W3199926081"
  ],
  "abstract": "Philippe Laban, Wojciech Kryscinski, Divyansh Agarwal, Alexander Fabbri, Caiming Xiong, Shafiq Joty, Chien-Sheng Wu. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9662–9676\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSUMM EDITS : Measuring LLM Ability at Factual Reasoning\nThrough The Lens of Summarization\nPhilippe Laban Wojciech Kry ´sci´nski Divyansh Agarwal Alexander R. Fabbri\nCaiming Xiong Shafiq Joty Chien-Sheng Wu\nSalesforce AI\n{plaban, divyansh.agarwal, afabbri, cxiong, sjoty, wu.jason}@salesforce.com\nAbstract\nWith the recent appearance of LLMs in prac-\ntical settings, having methods that can effec-\ntively detect factual inconsistencies is crucial\nto reduce the propagation of misinformation\nand improve trust in model outputs. When\ntesting on existing factual consistency bench-\nmarks, we find that a few large language mod-\nels (LLMs) perform competitively on classifi-\ncation benchmarks for factual inconsistency de-\ntection compared to traditional non-LLM meth-\nods. However, a closer analysis reveals issues\nwith existing evaluation benchmarks, affecting\nevaluation precision. To address this, we pro-\npose a new protocol for inconsistency detec-\ntion benchmark creation and implement it in\na 10-domain benchmark called SUMM EDITS .\nThis new benchmark is 20 times more cost-\neffective per sample than previous benchmarks\nand highly reproducible, as we estimate inter-\nannotator agreement at about 0.9. Most LLMs\nstruggle on SUMM EDITS , with performance\nclose to random chance. The best-performing\nmodel, GPT-4, is still 8% below estimated\nhuman performance, highlighting the gaps in\nLLMs’ ability to reason about facts and detect\ninconsistencies when they occur.\n1 Introduction\nWith recent progress in generation capabilities\nof LLMs, automatic summarization is making\nits appearance in practical information consump-\ntion situations such as summarizing work meet-\nings (Arabzadeh et al., 2022), health records (Jain\net al., 2022), or scientific documents (Cachola et al.,\n2020). To ensure the safe and effective implementa-\ntion of these applications, it is essential to limit the\nreach of factually inconsistent summaries, a known\nissue with generated summaries (Kry´sci´nski et al.,\n2019; Maynez et al., 2020).\nPrior work (Kry´sci´nski et al., 2020; Fabbri et al.,\n2021; Gao and Wan, 2022) has annotated corpora\nof model summaries with labels of factual con-\nHuman \nPerform.\n100%\n50%\nSummEdits Benchmark \nPerformance\n90.9 \n82.4 \nOpenAI\n70.7 \n60.1 \n51.9 Dav1\nDav2\nDav3/ \nTurbo\nGPT4\nGoogle\nBard59.8 \nAnthropic\nClaude \n v1.3 55.1 Cmd-\nXL\nCohere\n56.1 \nVicuna(random\nchance)\n63.7 \nBest non-LLM: 65.769.0 Bison\n73.6 Claude \n v2\nMistral\n57.8 \nMeta\n50.4 7b\n58.4 13b\nLlama2\n13b7b\nFigure 1: SUMM EDITS is a benchmark to evaluate the\nfactual reasoning abilities of LLMs, measuring if mod-\nels detect factual inconsistencies when they occur in\nsummaries. Capable detection models can help build\nmore reliable NLG systems.\nsistency, finding that most abstractive summariza-\ntion systems produce a non-negligible portion of\ninconsistent summaries. In turn, such corpora\nare used to instantiate tasks such as inconsistency\ndetection (ID) (Laban et al., 2022a; Tang et al.,\n2022), in which models are given (document,\nsummary) pairs, and must identify whether the\nsummary is consistent with the document.\nRecent investigations of using LLMs for evalua-\ntion have shown promising results across different\nNLP tasks (Liu et al., 2023; Fu et al., 2023), includ-\ning factual consistency (Luo et al., 2023). In this\nwork, we continue this line of research and explore\napplying LLMs as factuality evaluators in the con-\ntext of text summarization. We first establish base-\nline performance for a suite of LLMs on two exist-\ning benchmarks – AggreFact (Tang et al., 2022) and\nDialSummEval (Gao and Wan, 2022) – with results\nconfirming that some LLMs perform competitively\nwith state-of-the-art specialized methods such as\nQAFactEval (Fabbri et al., 2022). However, a man-\nual analysis of conflict cases in AggreFact reveals\na significant number of mislabeled samples (7+%)\nof factual inconsistencies undetected by annotators\nduring dataset creation that the GPT4 explanations\nreveal. This lack of quality of benchmarks lim-\n9662\nits the precise evaluation of model performance at\nfactual inconsistency detection.\nTo address this issue, we introduce a protocol\ndesigned to create challenging benchmarks while\nensuring the reproducibility of the labels. The pro-\ntocol involves manually verifying the consistency\nof a small set of seed summaries and subsequently\ngenerating numerous edited versions of these sum-\nmaries. We discover that assessing the consistency\nof edited summaries is relatively straightforward\nand easy to scale for human annotators, thus guar-\nanteeing low cost and high agreement among anno-\ntators, yet keeping the task challenging for models.\nWe create theSUMM EDITS benchmark by imple-\nmenting the protocol in ten diverse textual domains,\nincluding the legal, dialogue, academic, financial,\nand sales domains. Figure 1 summarizes experi-\nmental results on the benchmark, which indicate\nthat SUMM EDITS presents a challenge for all mod-\nels, with only four LLMs outperforming the special-\nized model QAFactEval. Our estimate of human\nperformance of 90%+ largely outperforms models,\nsuggesting current LLMs are not yet proficient at\ncomplex factual reasoning, and cannot assess the\nfactual validity of summaries with precision.\nWe believe SUMM EDITS can serve as a tool to\nevaluate LLMs’ abilities to detect factual inconsis-\ntencies when they (inevitably) occur and encourage\nLLM developers to report their performance on\nthe benchmark. For practitioners requiring specific\ndomain expertise, the protocol can be adapted to\ngenerate low-cost, in-domain benchmarks that can\ncheck model capabilities prior to production use.\nWe release the code and benchmark publicly1.\n2 Related Work\nAnnotating Factuality of Summaries. With ad-\nvances in language models and the increase in flu-\nency and abstractiveness of summarizers, prior\nwork showed that one of the key challenges in\nsummarization was enforcing factual consistency\n(Kry´sci´nski et al., 2019), particularly with mod-\nels trained on datasets with unfaithful references\n(Maynez et al., 2020). Several efforts – such as\nFactCC (Kry´sci´nski et al., 2020), SummEval (Fab-\nbri et al., 2021), Polytope (Huang et al., 2020),\nFRANK (Pagnoni et al., 2021), and CLIFF (Cao\nand Wang, 2021) – annotated the generated sum-\nmaries of tens of model, finding that most models\n1https://github.com/salesforce/\nfactualNLG\nproduce a non-negligible portion of inconsistent\nsummaries. Although most annotation effort has\nfocused on the summarization of news, some prior\nwork also looked at dialogue summarization (Gao\nand Wan, 2022), or the medical domain (Tang et al.,\n2023). In most work, scalable high-quality anno-\ntation is challenging, due to low inter-annotator\nagreement when relying on crowd-workers, with\nsome work showing that 10+ annotators are re-\nquired to achieve some level of consensus (Falke\net al., 2019), and some work recommending solely\nrelying on experts (Fabbri et al., 2021). At the heart\nof the issue, annotating the factual consistency of a\nsummary is challenging: it requires careful reading\nof long documents and the detection and interpre-\ntation of nuanced facts. In this work, we propose\na new protocol to annotate factual consistency re-\nsources and show that it lowers the cost and in-\ncreases reproducibility by minimizing the amount\nof reasoning required for each annotation.\nSome work has also annotated inconsistency of\npairs on text on non-summarization tasks, such\nas paraphrasing (Zhang et al., 2019), document-\ngrounded dialogue (Honovich et al., 2021; Dziri\net al., 2022), and Wikipedia-editing (Schuster et al.,\n2021). Follow-up work has then aggregated and\nstandardized annotations into benchmarks such as\nSummaC (Laban et al., 2022a), AggreFact (Tang\net al., 2022) and TRUE (Honovich et al., 2022).\nDetecting Factual Errors. Some work has taken\nan automated approach to the detection of inconsis-\ntencies, with approaches falling into two main cate-\ngories: question and entailment-based. In question-\nbased approaches, questions are generated with the\nexpectation that paired documents and summaries\nshould provide consistent answers. QAFactEval\n(Fabbri et al., 2022) unified prior work (Wang et al.,\n2020; Scialom et al., 2021; Honovich et al., 2021)\nby systematically evaluating each element of the\npipeline and proposing a best-performing combi-\nnation. Entailment-based methods either rely on\nentailment on dependency parses – DAE (Goyal\nand Durrett, 2020) – or directly leverage natural-\nlanguage entailment models, such as SummaC (La-\nban et al., 2022a). We include these three represen-\ntative models in our experiments, finding that even\nwith several orders of magnitudes fewer parameters\nthan LLMs, they can reach similar performances\non benchmarks.\n9663\n3 Limits of Crowd-Based Benchmarks\nWe first analyze model performance on two popular\nbenchmarks for factual consistency detection in\nsummarization: AggreFact (Tang et al., 2022) and\nDialSummEval (Gao and Wan, 2022) and uncover\nlimitations that guide the design principles of the\nSUMM EDITS benchmark.\n3.1 Experimental Setup\nWe include in our experiments three special-\nized non-LLM approaches: DAE, SummaC, and\nQAFactEval and ten LLM models from recent\nLLM families. We include Cohere’s Command-\nXL, Anthropic’s Claude V1.3 (Bai et al., 2022),\nGoogle’s Bard and PaLM2-Bison (Thoppilan\net al., 2022), Vicuna-13b (Chiang et al., 2023),\nand OpenAI’s DaVinci001 (Brown et al., 2020),\nDaVinci002 (Ouyang et al., 2022), DaVinci003,\nGPT3.5-turbo, and GPT-4. Appendix A provide\neach model’s method of access and model card.\nTo minimize the computational cost of experi-\nments, we select a single Zero-Shot prompt that is\nused for all LLM models. We make this choice in-\nstead of optimizing the prompt for each model for\ntwo reasons: (1) there’s no guarantee that prompt\nquality will transfer across benchmarks, and us-\ning a single common prompt removes variance\nfrom prompt optimization that does not measure\nunderlying model ability, and (2) more complex\nprompts would require adaptation to each domain\n(e.g. domain-specific few-shot examples), and re-\nstrict the evaluation of models with shorter maxi-\nmum sequence lengths due to longer prompts.\n3.2 AggreFact\nAggreFact-SOTA (Tang et al., 2022) is a factual\nconsistency benchmark focused on the news do-\nmain, modified from SummaC (Laban et al., 2022a)\nto focus on summaries generated by SOTA models\n(i.e., models based on pre-trained Transformers),\nas analysis showed older models’ summaries were\nless relevant to the field of consistency detection.\nTable 1 reports the balanced accuracy of spe-\ncialized models and LLMs on AggreFact. At first\nglance, the specialized models still outperform\nLLMs, even though increasing LLM size leads to\nperformance improvements and helps close the gap,\nwith GPT-4 performing within 2.4% points of the\nspecialized DAE. However, all models perform rel-\natively poorly, with no model reaching a balanced\naccuracy of 80% on a binary classification task.\nAggreFact DialSummEval\nModel Name %BAcc. %BAcc. Corr.\nDAE 76.0 56.2 0.44\nSummaC 71.6 62.7 0.35\nQAFactEval 73.9 64.4 0.59\nCohere-cmd-XL 63.1 56.6 0.36\nClaude V1.3 50.6 56.8 0.30\nBard 62.7 59.5 0.26\nPaLM2-Bison 57.0 55.6 0.57\nDav001 53.3 52.9 0.11\nDav002 54.3 59.2 0.49\nVicuna-13b 60.3 58.6 0.36\nDav003 64.8 60.9 0.51\nGPT3.5-turbo 70.2 62.0 0.56\nGPT-4 73.6 68.4 0.58\nTable 1: Performance of models on the AggreFact, Di-\nalSummEval consistency benchmarks reported in bal-\nanced accuracy (%Bacc.) and correlation (corr.).\nTo inspect performance on the AggreFact bench-\nmark further, we hired an annotator to manually\ninspect the cases where GPT4 disagrees with the\nlabel of AggreFact. More precisely, we manually\ninspected the explanations provided by GPT4 for\nthe 101 summaries it judged were inconsistent but\nlabeled as consistent in the dataset. Appendix B\nprovides more detail on the annotation protocol.\nOf the 101 samples, 80 were labeled by the an-\nnotator as correct or partially correct explanations\nthat identify and explain a factual inconsistency in\nthe summary. In other words, this manual anal-\nysis of a subset of AggreFact reveals that a min-\nimum of 6% of the samples in AggreFact are\nmislabeled. The low reliability of labels in crowd-\nsourced benchmarks like AggreFact is a known\nissue (Pagnoni et al., 2021) stemming from task\ncomplexity that requires the annotator to carefully\nread and understand an entire document and ac-\ncompanying summary, leading to low repeatability\nand inter-annotator agreement.\nThis analysis reveals the potential for LLMs as\npart of dataset creation. In some cases, an LLM ex-\nplanation that is verifiable – such as an explanation\nfor an identified factual inconsistency – can accel-\nerate and improve the quality of annotation. LLM\nexplanations might not be valuable in all cases,\nsuch as when a model asserts a summary is consis-\ntent, manual verification is still required to assure\nquality. In Section 5, we introduce a protocol for\nbenchmark creation that can involve an LLM.\nBased on the low reliability of labels in Ag-\ngreFact, we note that a key requirement for future\n9664\nAverage Annotator Likert Score\nModel 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nDav001 68.1 78.4 84.6 90.2 83.6 84.9 86.0 88.9\nCohere-cmd-XL46.2 51.0 70.3 83.6 88.6 89.2 91.7 96.3\nDAE 30.8 56.9 63.7 83.6 86.8 94.3 90.3 94.2\nPaLM2-bison25.3 35.3 56.0 78.7 93.6 97.2 98.4 95.8\nDav002 13.2 29.4 47.3 62.3 77.7 83.0 88.3 90.0\nDav003 4.4 17.6 28.6 31.1 63.2 69.3 84.9 81.6\nGPT3.5-turbo8.8 15.7 29.7 45.9 73.6 76.4 88.5 90.0\nGPT4 2.2 5.9 6.6 24.6 45.9 54.2 80.9 87.9\nQAFactEval3.3 5.9 17.6 24.6 44.5 54.7 70.3 74.7\nVicuna-13b 8.8 15.7 17.6 37.7 50.9 54.2 65.5 66.8\nSummaC 4.4 5.9 20.9 21.3 27.7 40.1 43.7 58.9\nClaude V1.31.1 9.8 11.0 13.1 33.6 37.3 47.1 45.8\nBard 9.9 7.8 5.5 9.8 18.2 21.2 36.5 42.6\nTable 2: Percent of summaries classified as consistent in\nDialSummEval, bucketed by average Likert consistency\nscore. Models interpret the Likert range differently.\nbenchmarks is to improve label reliability, which\ncan be demonstrated with high annotator agreement\nwhen multiple annotators are involved.\n3.3 DialSummEval\nThe DialSummEval (Gao and Wan, 2022) bench-\nmark is a summarization evaluation benchmark cre-\nated following the format of SummEval (Fabbri\net al., 2021) for the domain of dialogue summa-\nrization. In DialSummEval, each (dialogue,\nsummary) tuple is evaluated by three annotators,\neach assigning a Likert score (1-5) assessing the\nconsistency of the summary. The authors of the\nbenchmark report an agreement level of 0.67 Krip-\npendorff’s alpha on the labels, indicating a moder-\nate amount of agreement among annotators.\nWe evaluate model performance in two ways:\n(1) correlation between model predictions and the\naverage annotator score, and (2) we follow Laban\net al. (2022a) to transform the annotation into a\nbinary classification task, amenable to the balanced\naccuracy metric. Results summarized in Table 1.\nEchoing results on AggreFact, increasing model\nsize leads to minor performance gains, with most\nLLMs underperforming specialized methods. In ab-\nsolute terms, all methods struggle to achieve strong\nperformance, with accuracies all below 70%.\nIn Figure 2, we aggregate model predictions into\n0.5-width buckets on the Likert scale. We find that\nmost models achieve strong performance on non-\nborderline buckets ([1.0, 1.5), [1.5, 2.0], [4.0, 4.5],\n[4.5, 5.0]), assigning a vast majority of samples to\nthe correct class (inconsistent for low buckets, con-\nsistent for high buckets). The borderline buckets\n([2.0, 4.0]) however are less clear-cut: most mod-\nSource\nDocument\nSeed\nSummary\nIs the seed factually \nconsistent with the document?\nAny ﬂaws with the summary?\n(ﬂuency, format, etc.)\n  No\n  Yes\nHuman \nAnno\nLLM\nEdited Summaries\nHuman\nAnno\nSummEdits \nBenchmarkDoes the edit introduce a factual\ninconsistency?\n✔ Consistent✘Inconsistent\nFigure 2: SUMM EDITS protocol diagram, a three-step\nprotocol to create summarization ID benchmarks. See\nTable 3 for example samples produced by the protocol.\nels assign large proportions of samples from each\nbucket into consistent and inconsistent classes.\nWe argue that annotating the consistency of\nsummaries using a Likert scale limits the quality\nand interpretability of the benchmark , as it is\nnot evident to interpret the differences between\nscores, limiting reproducibility, which is reflected\nin the moderate Kripendorff’s alpha. Instead, we\nfavor framing factual consistency benchmarks as\na detection task. In the detection task, identifying\nany factual inconsistency between the document\nand summary leads to an overall assessment of the\nsummary being inconsistent. If no inconsistency is\ndetected, the summary is consistent. The detection\nframing also allows for models to provide natural\nlanguage explanations when identifying a summary\nas inconsistent, which can be manually verified to\nconfirm model reasoning ability.\nIn the next section, we propose a novel protocol\nto create factual consistency benchmarks, incorpo-\nrating lessons learned from existing benchmarks.\n4 S UMM EDITS Protocol\n4.1 Design Principles\nWe set several design principles that help create\nhigher-quality factual consistency benchmark:\nP1. Binary Classification Task: In the bench-\n9665\nConsistentEdited SummaryInconsistentEdited Summary\nThe charactersdiscussponder the\nconsequences of banishing Mar-\ncius, with Cominius warning that\nhisalliancecollaboration with the\nV olscians will bring great danger to\nRome.\nThe characters discuss the con-\nsequences of banishing Marcius,\nwith Cominius warning that his\nalliance with theV olsciansRo-\nmans will bring great danger to\nRometheV olscians.- Entity\nManipulation\nWe introduced anovelnew, sim-\nple, and efficient data augmenta-\ntion method thatboostsimproves\nthe performances of existing GANs\nwhen training data is limited and di-\nverse.\nWe introduced a novel, simple, and\nefficient data augmentation method\nthat boosts the performances of ex-\nisting GANs when training data is\nlimitedabundant and diverse.-\nAntonym Swap\nEmployees of the European Com-\nmission are nowforcedinstructed\ntodeleteremove TikTok from their\nwork devices, anddeletegetridof\nit from their personal devices too if\nthey have work-relatedappsappli-\ncations installed.\nEmployees of the European Com-\nmission arenowforcednotre-\nquired to delete TikTok from their\nwork devices,anddeletebutshould\nstillremove it from their per-\nsonal devicestoo if they have\nwork-related apps installed.-\nHallucinated Fact\nA conversation between a sales\nagent and apotentialclientpossible\ncustomer. The sales agent provides\ninformation on different home in-\nsuranceplansoptions and pricing,\nas well as available discounts for\nclients with good credit scores and\nother factors.\nA conversation between a sales\nagent and a potential client. The\nsales agent provides information on\ndifferent home insurance plansand,\nbutnoton pricing, aswellasor\navailable discounts for clients with\ngood credit scores and other factors.\n- Negation Insertion\nTable 3: Example edited summaries – deletions, inser-\ntions – for from SUMM EDITS (domains top-to-bottom:\nShakespeare Plays, SciTLDR, News, Sales Call). Incon-\nsistent summaries are labeled with an Edit Type.\nmark, a summary should either be labeled\nas inconsistent if any factual inconsistency\nis identified with the document or consistent\notherwise, to improve label interpretability.\nP2. Focus on Factual Consistency: Summaries\nin the benchmark should be flawless on as-\npects unrelated to consistency, such as fluency,\ncoherence, and formatting, to avoid confound-\ning effects on the quality of the benchmark.\nP3. Reproducibility: Benchmark labels should\nnot depend on annotator identity, and high an-\nnotator agreement should confirm the validity\nof the benchmark, as well as estimate human\nperformance on the benchmark.\nP4. Benchmark Diversity: Inconsistency errors\nin the benchmark should represent a wide\nrange of errors in realistic textual domains,\nto increase understanding of model strengths\nand weaknesses, and better establish gaps in\nperformance between models and human an-\nnotators at factual reasoning, if there are any.\n4.2 Creation Procedure\nWe now describe the creation procedure for\nSUMM EDITS – illustrated in Figure 2 – which sat-\nisfies the design principles stated above.\nThe procedure consists of three steps: (1) seed\nsummary verification, (2) generation of summary\nedits, and (3) annotation of edited summaries.\nSeed Summary Verification. Benchmark cre-\nators select a small collection of documents in a\ndomain of choice, and a seed summary for each\ndocument, which can be human-written or model\ngenerated. An annotator answers two questions\nabout each (document, seed summary) tu-\nple: (a) “Are there any flaws with the summary?\n(fluency, format, etc.)”, (b) “Is the summary factu-\nally consistent with the document?”. If the annota-\ntor identifies a flaw or an inconsistency, the tuple is\nfiltered out (P2), otherwise, it proceeds to Step 2.\nEditing Summaries. The second step consists\nin generating multiple minor edits of the summary,\nwhich might or might not affect the summary’s con-\nsistency. This step can be carried out manually, or\nautomatically with an LLM. Proposed edits should\nbe atomic and localized, not entirely rewriting a\nnovel summary. Table 3 gives examples of edits.\nAnnotation of Edited Summaries. The anno-\ntator who completed Step 1 reviews each edited\nsummary, assigning one of three labels: (a) consis-\ntent if an edit does not lead to an inconsistency, (b)\ninconsistent if the edit modifies the seed summary\nin a way that introduces a factual inconsistency, (c)\nborderline for any other case such as the edit mak-\ning the summary unclear, or requiring subjectivity.\nCrucially, a single annotator should complete\nboth Steps 1 and 3, as once they have invested\nthe time in reading the (document, summary\nseed) tuple, judging the consistency of edits is\na simpler task. We recommend including a large\nnumber of edits (e.g., 30 edits) to maximize edit\ndiversity (P4) and encouraging annotators to assign\nthe borderline label if they are unsure about any\naspect of an edit to maximize reproducibility (P3).\nA benchmark can be formed by retaining edited\nsummaries that are labeled as consistent and incon-\nsistent and filtering out borderline cases.\nThe procedure requires a small number of docu-\nments and seed summaries which are derived into\nmany edited summaries. This flexibility facilitates\nthe creation of factual consistency benchmarks in\napplication domains that lack such resources.\n9666\nDomain N %Balance IAA\nNews 819 39.2% 0.91\nPodcast 500 32.6% 0.91\nBillsum 853 42.3% 0.90\nSamsum 664 36.4% 0.90\nShakespeare 814 46.4% 0.96\nSciTLDR 466 31.1% 0.93\nQMSum 431 42.5% 0.92\nECTSum 668 38.0% 0.96\nSales Email 613 29.2% 0.87\nSales Call 520 33.3% 0.93\nOverall 6,348 37.10% 0.92\nTable 4: Statistics of the ten domains included in\nthe SUMM EDITS benchmark, including the number of\nsamples (N), the percentage of consistent summaries\n(%Balance), and the inter-annotator agreement (IAA).\n5 S UMM EDITS Benchmark\n5.1 Benchmark Creation\nWe implemented the SUMM EDITS protocol on ten\nrealistic summarization domains to explore the re-\nliability of the protocol. For five domains, seed\nsummaries are automatically generated due to the\nlack or low quality of existing reference summaries.\nIn such cases, we used GPT3.5-turbo and domain-\nspecific prompts to generate seed summaries. We\nnote that the quality of seed summaries is ultimately\nmanually confirmed in step 1 of the protocol.\nFor all domains, we use GPT3.5-turbo2 for Step\n2. We experimented with integrating multiple\nLLMs in the edit generation process, but prelim-\ninary results indicated that many LLMs were not\nsuccessful at generating minorly edited summaries\nand often attempted to write entirely novel sum-\nmaries, which led us to solely use GPT3.5-turbo.\nMore on this choice in Section 7.\nWe hired two professional annotators compen-\nsated at a rate of $20/hour to perform Steps 1 and\n3. Three authors of the paper also participated in\nthe annotation for quality control purposes. Ap-\npendix C has further detail on annotation proto-\ncol and an overview of the annotation interface.\nWe next introduce the ten domains included in the\nSUMM EDITS benchmark.\nNews To avoid selecting documents that are\nin the training corpora of evaluated models, we\nfollow prior work (Goyal et al., 2022) and se-\nlect (document, summary) tuples from re-\n2The prompts will be listed in our open-source release.\ncent news articles. We obtained news articles from\nthe Google News top events in February 2023, se-\nlecting at most one per news source to increase\ncoverage diversity (Laban et al., 2023). Seed sum-\nmaries are extracted from article metadata.\nPodcast (Clifton et al., 2020) We collected 40\npodcast transcripts from Spotify’s podcast sum-\nmarization dataset’s test set. We generated seed\nsummaries due to low reference summary quality.\nBillSum (Kornilova and Eidelman, 2019) We\ncollected 40 US bills and their summaries as seeds\nfrom the training portion of BillSum, a challenging\ndataset for summarization in the legal domain.\nSamSum (Gliwa et al., 2019) We collected 40\ndialogues and summaries from the training portion\nof SamSum, a common dialogue summarization\ndataset for messenger-like conversations.\nShakespeare (Karpathy, 2015) We collected\n40 scenes from Shakespeare plays from the Tiny\nShakespeare corpus, each roughly 700 words long.\nWe generated seed summaries automatically.\nSciTLDR (Cachola et al., 2020) We collected 40\nresearch paper abstracts and corresponding TLDRs\nfrom the training portion of SciTLDR, a dataset for\nscientific paper summarization.\nQMSum (Zhong et al., 2021) We collected 40\ndocument and seed summaries from QMSum, a\ndataset for query-based meeting summarization.\nECTSum (Mukherjee et al., 2022) We collected\n40 documents from the ECTSum dataset, a sum-\nmarization dataset for the financial earnings call\ntranscripts. Due to low reference summary quality,\nwe generated seed summaries automatically.\nSales Call & Email We generated 40 fictional\nsales call transcripts, 40 sales emails, and corre-\nsponding seed summaries using ChatGPT. These\ndomains evaluate the protocol’s validity with en-\ntirely synthetic textual data in targeted domains\nthat lack pre-existing summarization datasets.\n5.2 S UMM EDITS Statistics\nTable 4 provides statistics of the SUMM EDITS\nbenchmark. Each domain yielded between 400-\n900 edited summaries, depending on the fraction\nof seed summaries that pass Step 1 (58% overall\npass rate) and the percentage of edited summaries\nthat are filtered out as borderline in Step 3 (around\n9667\nModel Podcast BillSum SAMSum News Sales C Sales E Shkspr SciTLDR QMSum ECTSum Avg. (↓)\nDAE 54.9 55.1 59.5 61.7 50.8 55.0 54.5 55.2 52.0 58.6 55.7\nSummaC 58.5 55.7 54.7 62.1 59.0 57.7 59.3 59.7 56.6 64.4 58.8\nQAFactEval 64.0 54.4 66.3 74.6 68.5 64.2 61.9 67.5 62.4 72.9 65.7\nDav001 53.3 50.2 51.0 54.4 55.3 52.5 50.0 51.0 50.3 50.9 51.9\nCohere-cmd-XL51.1 52.7 52.0 52.6 60.3 59.5 50.0 60.5 53.9 60.5 55.1\nVicuna-13b 52.8 52.6 50.8 63.0 58.1 51.8 55.5 59.7 54.0 62.5 56.1\nClaude v1.3 59.9 52.1 64.1 63.3 61.7 56.6 58.0 57.6 56.9 67.8 59.8\nDav002 56.4 53.9 57.1 61.9 65.1 59.1 56.6 64.6 60.6 66.2 60.1\nBard 50.0 58.3 61.3 72.8 73.8 69.0 58.4 66.1 53.9 73.1 63.7\nPaLM2-bison 66.0 62.0 69.0 68.4 74.5 68.1 61.6 78.1 70.2 72.3 69.0\nDav003 65.7 59.9 67.5 71.2 78.8 69.4 69.6 74.4 72.2 77.9 70.7\nGPT3.5-turbo 68.4 63.6 69.1 74.5 79.7 65.5 68.1 75.6 69.2 78.9 71.3\nGPT4 83.3 71.1 82.9 83.3 87.6 80.1 84.6 82.4 80.4 88.0 82.4\nGPT4 Oracle 90.2 85.5 86.3 88.3 91.1 83.5 96.6 86.3 89.9 91.7 88.9\nHuman Perf. 90.8 87.5 89.4 90.0 91.8 87.4 96.9 89.3 90.7 95.4 90.9\nTable 5: Balanced accuracy of models on the SUMM EDITS benchmark. Top three are non-LLM specialized models,\nmiddle section are LLMs, bottom section reports a GPT4 oracle performance and an estimate of human performance.\n6%). In the five domains where seed summaries\nwere generated by GPT3.5-turbo, 17.8% of the seed\nsummaries were labeled as inconsistent, indicating\nthat modern LLMs like GPT3.5-turbo struggle to\nremain consistent when summarizing documents.\nFor each domain, the seed summaries of at least\nten seed summaries were annotated by multiple an-\nnotators, corresponding for each domain to at least\n20% of the samples in the benchmark. In total,\n1,419 of the 6,348 samples in SummEdits received\nmultiple annotations, allowing us to measure agree-\nment levels. When considering all three labels (con-\nsistent, inconsistent, borderline), Cohen’s Kappa\nin each domain varies between 0.72-0.90, averag-\ning 0.82. When removing samples annotated as\nborderline by any annotator, the average Cohen’s\nKappa rises to 0.92, empirically validating the\nimportance of filtering out borderline samples\nto create a reproducible benchmark.\nThe edited summaries have and average of 3.6\nwords inserted, and 3.5 words deleted. These edit\nstatistics do not vary widely based on the consis-\ntency label, as consistent edited summaries have an\naverage of 3.6 words inserted, 3.7 words deleted,\nand inconsistent edited summaries have 3.6 words\ninserted, 3.4 words deleted. These statistics that\nmodels could not rely on structural signals to pre-\ndict the consistency of a summary, and required\nfactual reasoning to accomplish the task.\nIn the final benchmark, 37% of summaries are\nconsistent, approaching our objective of a balanced\nbenchmark to facilitate robust evaluation and mini-\nmize metric fluctuations (Luque et al., 2019).\nThe total annotation cost of SUMM EDITS is\naround USD 3,000, representing around 150 hours\nof annotator work. The average cost of adding a\ndomain to SUMM EDITS is around USD 300, within\nreach for NLP practitioners looking to evaluate the\nmodel ability in their domain of choice. Authors\nof the FRANK benchmark (Pagnoni et al., 2021)\n– samples of which are in AggreFact – estimate\nthat each FRANK sample required 30 minutes of\nannotator time. At similar annotator pay, the anno-\ntation of a new domain of similar size to ones in\nSummEdits would cost an estimated USD 6,000:\ntwenty times more. This cost analysis reveals the\ndual advantage of our protocol: by focusing the an-\nnotation task on atomic edits, the cost is drastically\nreduced and high reproducibility is maintained.\n5.3 S UMM EDITS Results\nTable 5 reports the performance of specialized mod-\nels, LLMs with a zero-shot prompt, an oracle ver-\nsion of GPT4, and an estimate of human perfor-\nmance on the samples with multiple annotations.\nOverall, model performance on the benchmark is\nlow, with only GPT4 getting within 10% of human\nperformance. Larger or more recent LLMs perform\nbetter on the benchmark, as is illustrated by the\ngradual improvements observed with each model\ngeneration in the OpenAI model family.\nPaLM2-Bison, Dav003, ChatGPT, and GPT4 are\nthe only four LLMs that outperform the best non-\nLLM approach QAFactEval, providing evidence\nthat most LLMs are not yet capable to reason\nout-of-the-box about the consistency of facts.\nAll three specialized models achieve their high-\nest performance in the news domain, unlike LLM\nmodels. The specialized models are likely cali-\nbrated to the news domain, which they are most fre-\n9668\nInconsistent Edit Type\nModel EntMod Anto Hallu Neg\nDAE 52.0 53.0 52.9 53.9\nSummaC 56.8 56.8 55.3 57.3\nQAFactEval 61.4 65.0 64.3 70.4\nDav001 50.0 50.9 50.8 53.7\nCohere-cmd-XL 53.7 55.8 55.5 63.8\nVicuna-13b 55.2 57.1 56.2 61.0\nClaude v1.3 58.8 60.3 61.5 66.7\nDav002 58.3 61.4 62.4 72.0\nBard 63.2 65.3 65.6 71.3\nPaLM2-Bison 67.0 70.0 71.7 80.3\nDav003 69.2 71.1 76.3 83.3\nGPT3.5-turbo 70.7 70.6 74.2 79.7\nGPT4 82.2 81.3 87.0 92.7\nAverage 61.4 62.9 64.1 69.7\nTable 6: Balanced accuracy of models on the SUMM ED-\nITS benchmark, broken down by type of factual error:\nEntity Modification (EntMod), Antonyms (Anto), Hal-\nlucination (Hallu) and Negation (Neg) insertion.\nquently tested on (Goyal and Durrett, 2020; Laban\net al., 2022a; Fabbri et al., 2022). This confirms the\nimportance of creating multi-domain benchmarks\nto measure model ability in realistic scenarios.\nSome domains such as Shakespeare’s plays or\nthe legal BillSum are more challenging to the ma-\njority of models, with the latter seeing no model\nscore higher than 71.1%. Yet, factual reasoning in\nthe legal domain is an important application area\nof NLP (Chalkidis et al., 2020; Shen et al., 2022).\nWe experiment with an oracle setting in which\nwe append the seed summary to the end of the in-\nput document and input the concatenation to the\nmodel. The seed summary serves as an information\nscaffold, enabling the model to view modifications\nbetween the seed and edited summaries. GPT4\nachieves a significant boost under the oracle setting,\nwith the model performing within 2% of human\nperformance. This confirms that high model per-\nformance on SUMM EDITS is attainable and that\nthe challenge lies in aligning the facts of the edited\nsummary with the document, without knowing that\nit has been edited.\n5.4 Edit Type Analysis\nWe annotated each inconsistent sample in\nSUMM EDITS with tags of edit types.\nThe four edit types are: (1) Entity Modifica-\ntion in which an entity or phrase in the summary\nhas been changed in a meaning-altering way, (2)\nAntonym Swap when a word or phrase is replaced\nby a word of opposite meaning, (3) hallucinated\nfact insertion, when a novel fact is introduced in\nthe summary which is not supported by the docu-\nment, and (4) negation insertion when any negator\nword (e.g., not, neither) which modifies summary\nmeaning is inserted. Figure 3 provides an example\nof each edit type in SUMM EDITS .\nTo annotate the entire benchmark, one author\nof the paper first manually annotated 200 samples\nof the dataset, which was used to evaluate several\nGPT4-based Zero-Shot and Few-Shot approaches.\nThe best-performing prompt provides the defini-\ntion of each edit type and a canonical example of\neach, and it achieved a performance of 0.85 F-1\nand 0.92 recall, which was deemed sufficient for\nanalysis purposes. GPT4 was used to annotate all\ninconsistent summaries in SUMM EDITS .\nOverall, 78% of inconsistent summaries contain\nan entity modification, 48% an antonym swap, 22%\nhallucinated fact insertion, and 18% a negator in-\nsertion. The distribution of edit types is highly\ninfluenced by the LLM and prompt used to pro-\nduce the edits in Step 2 of the protocol. Table 6\nsummarizes model performance by the edit type.\nAll models detect inconsistencies due to nega-\ntor insertions the best, a sign that such errors are\nmore discernable to models. Fact hallucinations\nare relatively harder to detect for non-LLM models\nbut gradually become more evident to more perfor-\nmant LLMs. Finally, the entity modification and\nantonym error types generally see the lowest rate\nof detection by models across the board, perhaps\ndue to such edits modifying an existing consistent\nfact in a more nuanced way.\n5.5 Number of Edits Effect\nIn SUMM EDITS , it is common for the LLM to in-\ntroduce multiple edits in each of its candidate sum-\nmaries, as can be seen in the examples in Table 3,\nin which each edited summary contains multiple\ninserted and deleted words. In Appendix D, we\nanalyze the effect of the number of edit types on\nmodel performance. In short, as the number of\nedit types in a summary increases, most models see\nsizable performance improvements, with average\nperformance increasing from 59.2 to 74.1 when the\nnumber of edit types goes from 1 to 4.\nThis analysis confirms the perspective the task\nin SUMM EDITS corresponds to a detection task: as\nthe number of introduced errors increases, model\n9669\nperformance increases as there is generally more\nevidence of inconsistencies for the models to de-\ntect. In turn, future work looking to create more\nchallenging benchmarks using a similar protocol\ncan focus on editing summaries with a single edit\ntype.\n6 Conclusion\nIn this work, we explore the capabilities of LLMs\nto act as factual reasoners through the lens of fac-\ntual evaluation in text summarization. As part of\nthis analysis, we uncover and discuss shortcomings\nof existing benchmarks. Using those insights we\ndevelop a new protocol for creating inconsistency\ndetection benchmarks, which we implement in a\n10-domain benchmark called SUMM EDITS . The\nSUMM EDITS benchmark is highly reproducible\nand more cost-effective per sample than previous\nbenchmarks. Our experiments show that the bench-\nmark is challenging for most current LLMs, with\nthe best-performing model, GPT-4, still 8% below\nestimated human performance. We believe that\nSUMM EDITS can serve as a valuable tool for eval-\nuating LLMs’ abilities to reason about facts, detect\nfactual errors and promote more reliable NLG sys-\ntems. We encourage LLM developers to report\ntheir performance on the benchmark.\n7 Limitations\nWhy not fix existing benchmarks? In Section 3,\nanalysis reveals limitations with existing bench-\nmarks that in theory can be fixed to yield improved\nversions of known benchmarks. The analysis we\nperformed however only helps us invalidate a sub-\nset of samples in an opportunistic way, by looking\nat samples where benchmark labels and GPT4 dis-\nagree. However, this methodology cannot help\nus efficiently correct or confirm all samples, and\nimproving existing benchmarks would require re-\nannotating a large portion of the benchmarks, and\nwe do not have a guarantee that new annotations\nwould improve on previous ones. By designing\na new protocol for sample annotation that relies\non clear, atomic edits, we simplify the annotation\nprocess, improving reproducibility.\nEffect of LLM in benchmark creation. Step 2\nof the protocol described in Section 4 relies on an\nLLM to generate many edits of the seed summary,\nwhich are subsequently manually annotated and in-\ncluded in the benchmark. The choice of LLM likely\nhas an effect on the benchmark which could favor\na subset of LLMs most similar to the one used for\nbenchmark creation. Initial attempts to use a pool\nof LLMs to produce edits were unsuccessful as we\nfound that only ChatGPT and GPT4 were currently\ncapable of following editing instructions that do not\nfully rewrite summaries. Future iterations on simi-\nlar benchmarks should consider including diverse\npools of LLMs in benchmark creation processes\nto avoid model-specific bias. Beside the edit sum-\nmaries, we leveraged ChatGPT to generate the seed\nsummaries in five of the ten domains in SUMM ED-\nITS , due to the low-quality or non-existence of\nhuman-written summaries. All seed summaries\nare manually inspected by our annotators, and we\ndid not find a gap in model performance dependent\non the origin of the seed summaries.\nBeyond Binary Classification. SUMM EDITS fo-\ncuses on a binary classification formulation of fac-\ntual reasoning (i.e., determining whether a sum-\nmary is consistent/inconsistent). Binary classifica-\ntion has multiple advantages, including the ability\nto benchmark both generative and non-generative\nmodels, requiring limited adaptation of previous\nsystems, and supporting well-established evalua-\ntion metrics such as balanced accuracy. However,\nthe edit-based protocol of SUMM EDITS could be\nbeneficial in instantiating more advanced factual\ninconsistency tasks. For example, SUMM EDITS\ncould be modified into an “error localization” task\nwhich would require models to identify edit spans\nthat render the summary inconsistent, or an “error\ncorrection” task, which would require a generative\nmodel to undo problematic edits, removing edit\nspans that lead to factual errors. These more ad-\nvanced task formulations would require crafting\nreliable metrics, which was out of the scope of the\ncurrent project.\nEvaluating Summarizers. Previous annotation\nefforts in factual consistency of summarization\nwere in part collected to evaluate which summa-\nrization models are least likely to generate fac-\ntual inconsistencies (Falke et al., 2019). Since the\nsummaries in SUMM EDITS are synthetic modifi-\ncations of summaries, the benchmark cannot di-\nrectly provide insights on summarizers and their\nability to remain consistent, and the main purpose\nof SUMM EDITS is to measure LLM ability to rea-\nson about facts, and detect factual inconsistencies\nin text pairs. Future work could explore using meth-\n9670\nods such as Near-Negative Distinction (NND) (La-\nban et al., 2022b) to adapt SUMM EDITS into a set\nof tests to evaluate summarizer performance, and\nmodel ability to avoid generating inconsistent sam-\nples in the first place.\nBuild Your Own Benchmark. The initial release\nof SUMM EDITS consists of ten diverse domains\nwe hope span common summarization domains.\nThe current benchmark is however limited, as it\nonly includes documents and summaries in English,\nand mostly limits document length to below 2,000\nwords. We have however shown that the protocol\ncan be adapted to widely different textual domains –\nfrom US legal bills to Shakespeare plays – and pro-\nduce domain-specific benchmarks at low cost. We\nhope that others will adopt and adapt the protocol\nto new domains, languages, and NLP tasks.\nEthical Considerations\nThe models and datasets utilized in the project pri-\nmarily reflect the culture of the English-speaking\npopulace. Gender, age, race, and other socio-\neconomic biases may exist in the dataset, and mod-\nels trained on these datasets may propagate these\nbiases. Text generation tasks such as summariza-\ntion have previously been shown to contain these\nbiases.\nIn Section 3 and Section 5, we recruited pro-\nfessional annotators to perform labeling with re-\nspect to summaries’ factual consistency label or\nLLM reasoning explaining factual inconsistencies.\nWe ensured to remunerate the participants fairly\n($20/hour). Participants could communicate with\nus to voice concerns, could work at their own pace,\nand choose to stop working on the project at any\ntime. Finally, we ensured to anonymize the an-\nnotations by not including personally identifiable\ninformation in any version of the dataset (annota-\ntor identity is instead marked as annotator1,\nannotator2, etc.).\nIn our work, we relied on several datasets as\nwell as pre-trained language models. We explicitly\nverified that all datasets and models are publicly\nreleased for research purposes and that we have\nproper permission to reuse and modify the datasets.\nReferences\nNegar Arabzadeh, Ali Ahmadvand, Julia Kiseleva, Yang\nLiu, Ahmed Hassan Awadallah, Ming Zhong, and Mi-\nlad Shokouhi. 2022. Preme: Preference-based meet-\ning exploration through an interactive questionnaire.\narXiv preprint arXiv:2205.02370.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini,\nCameron McKinnon, et al. 2022. Constitutional\nai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nIsabel Cachola, Kyle Lo, Arman Cohan, and Daniel S\nWeld. 2020. Tldr: Extreme summarization of sci-\nentific documents. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4766–4777.\nShuyang Cao and Lu Wang. 2021. Cliff: Contrastive\nlearning for improving faithfulness and factuality in\nabstractive summarization. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 6633–6649.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. Legal-bert: The muppets straight out of law\nschool. ArXiv, abs/2010.02559.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n2023. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality.\nAnn Clifton, Aasish Pappu, Sravana Reddy, Yongze\nYu, Jussi Karlgren, Ben Carterette, and Rosie Jones.\n2020. The spotify podcast dataset. arXiv preprint\narXiv:2004.04270.\nNouha Dziri, Hannah Rashkin, Tal Linzen, and David\nReitter. 2022. Evaluating attribution in dialogue sys-\ntems: The begin benchmark. Transactions of the\nAssociation for Computational Linguistics, 10:1066–\n1083.\nAlexander R Fabbri, Wojciech Kry´sci´nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021. Summeval: Re-evaluating summariza-\ntion evaluation. Transactions of the Association for\nComputational Linguistics, 9:391–409.\nAlexander Richard Fabbri, Chien-Sheng Wu, Wenhao\nLiu, and Caiming Xiong. 2022. Qafacteval: Im-\nproved qa-based factual consistency evaluation for\nsummarization. In Proceedings of the 2022 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2587–2601.\n9671\nTobias Falke, Leonardo FR Ribeiro, Prasetya Ajie\nUtama, Ido Dagan, and Iryna Gurevych. 2019. Rank-\ning generated summaries by correctness: An interest-\ning but challenging application for natural language\ninference. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2214–2220.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166.\nMingqi Gao and Xiaojun Wan. 2022. Dialsummeval:\nRevisiting summarization evaluation for dialogues.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5693–5709.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\nsander Wawer. 2019. Samsum corpus: A human-\nannotated dialogue dataset for abstractive summa-\nrization. EMNLP-IJCNLP 2019, page 70.\nTanya Goyal and Greg Durrett. 2020. Evaluating factu-\nality in generation with dependency-level entailment.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of\ngpt-3. arXiv preprint arXiv:2209.12356.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. True: Re-evaluating factual\nconsistency evaluation. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3905–3920.\nOr Honovich, Leshem Choshen, Roee Aharoni, Ella\nNeeman, Idan Szpektor, and Omri Abend. 2021.\nQ2:: Evaluating factual consistency in knowledge-\ngrounded dialogues via question generation and ques-\ntion answering. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 7856–7870.\nDandan Huang, Leyang Cui, Sen Yang, Guangsheng\nBao, Kun Wang, Jun Xie, and Yue Zhang. 2020.\nWhat have we achieved on text summarization? In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 446–469.\nRaghav Jain, Anubhav Jangra, Sriparna Saha, and Adam\nJatowt. 2022. A survey on medical document sum-\nmarization. ArXiv, abs/2212.01669.\nAndrej Karpathy. 2015. char-rnn. https://\ngithub.com/karpathy/char-rnn.\nAnastassia Kornilova and Vladimir Eidelman. 2019.\nBillsum: A corpus for automatic summarization of\nus legislation. In Proceedings of the 2nd Workshop\non New Frontiers in Summarization, pages 48–56.\nWojciech Kry´sci´nski, Nitish Shirish Keskar, Bryan Mc-\nCann, Caiming Xiong, and Richard Socher. 2019.\nNeural text summarization: A critical evaluation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 540–551.\nWojciech Kry´sci´nski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332–9346.\nPhilippe Laban, Tobias Schnabel, Paul N Bennett, and\nMarti A Hearst. 2022a. Summac: Re-visiting nli-\nbased models for inconsistency detection in summa-\nrization. Transactions of the Association for Compu-\ntational Linguistics, 10:163–177.\nPhilippe Laban, Chien-Sheng Wu, Wenhao Liu, and\nCaiming Xiong. 2022b. Near-negative distinction:\nGiving a second life to human evaluation datasets. In\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nPhilippe Laban, Chien-Sheng Wu, Lidiya Mu-\nrakhovs’ Ka, Xiang ’Anthony’ Chen, and Caiming\nXiong. 2023. Designing and evaluating interfaces\nthat highlight news coverage diversity using discord\nquestions. In Proceedings of the 2023 CHI Confer-\nence on Human Factors in Computing Systems, pages\n1–21.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. Gpteval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv preprint arXiv:2303.16634.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023. Chatgpt as a factual inconsistency evaluator\nfor text summarization.\nAmalia Luque, Alejandro Carrasco, Alejandro Martín,\nand Ana de Las Heras. 2019. The impact of class im-\nbalance in classification performance metrics based\non the binary confusion matrix. Pattern Recognition,\n91:216–231.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919.\nRajdeep Mukherjee, Abhinav Bohra, Akash Banerjee,\nSoumya Sharma, Manjunath Hegde, Afreen Shaikh,\nShivani Shrivastava, Koustuv Dasgupta, Niloy Gan-\nguly, Saptarshi Ghosh, et al. 2022. Ectsum: A new\nbenchmark dataset for bullet point summarization\nof long earnings call transcripts. arXiv preprint\narXiv:2210.12467.\nSharan Narang and Aakanksha Chowdhery. 2022. Path-\nways language model (palm): Scaling to 540 billion\nparameters for breakthrough performance.\n9672\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with frank: A benchmark for\nfactuality metrics. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4812–4829.\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021.\nGet your vitamin c! robust fact verification with\ncontrastive evidence. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 624–643.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, Jacopo Staiano, Alex Wang,\nand Patrick Gallinari. 2021. Questeval: Summariza-\ntion asks for fact-based evaluation. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 6594–6604.\nZejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg,\nMargo Schlanger, and Doug Downey. 2022. Multi-\nlexsum: Real-world summaries of civil rights law-\nsuits at multiple granularities. arXiv preprint\narXiv:2206.10883.\nLiyan Tang, Tanya Goyal, Alexander R Fabbri, Philippe\nLaban, Jiacheng Xu, Semih Yahvuz, Wojciech Kry´s-\nci´nski, Justin F Rousseau, and Greg Durrett. 2022.\nUnderstanding factual errors in summarization: Er-\nrors, summarizers, datasets, error detectors. arXiv\npreprint arXiv:2205.12854.\nLiyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G Nestor,\nAli Soroush, Pierre A Elias, Ziyang Xu, Ying Ding,\nGreg Durrett, Justin Rousseau, et al. 2023. Eval-\nuating large language models on medical evidence\nsummarization. medRxiv, pages 2023–04.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023. Alpaca: A\nstrong, replicable instruction-following model. Stan-\nford Center for Research on Foundation Models.\nhttps://crfm. stanford. edu/2023/03/13/alpaca. html.\nMosaicML NLP Team. 2023. Introducing mpt-7b: A\nnew standard for open-source, ly usable llms. Ac-\ncessed: 2023-05-16.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5008–5020.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPaws: Paraphrase adversaries from word scrambling.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 1298–1308.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia\nMutuma, Rahul Jha, Ahmed Hassan, Asli Celikyil-\nmaz, Yang Liu, Xipeng Qiu, et al. 2021. Qmsum: A\nnew benchmark for query-based multi-domain meet-\ning summarization. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 5905–5921.\nA Model Access Detail\nWe experiment with a wide range of models. For\neach model, we specify its model card, and how it\nwas accessed.\nNon-LLM models. The three specialized models\n– SummaC3, DAE4, and QAFactEval5 – were im-\nplemented through their online public repositories,\nand run locally on a multi-GPU machine (with 2\nV-100 GPUs).\nOpen-source Models. We experimented\nwith five open-source LLM models:\nLLama-13b (Touvron et al., 2023), Alpaca-\n13b (Taori et al., 2023), Dolly-V2-12b\n(databricks/dolly-v2-12b), Vicuna-\n13b (Chiang et al., 2023), and MosaicML’s\nMPT-7b-chat (Team, 2023). All models were\naccessed through the public, online demonstration\nof LMSys.org6. Model responses were collected\nbetween April 15th, 2023, and May 15th, 2023.\n3https://github.com/tingofurro/summac\n4https://github.com/tagoyal/\nfactuality-datasets\n5https://github.com/salesforce/\nQAFactEval\n6https://chat.lmsys.org/\n9673\nGoogle Models. We experiment with two Google\nmodels, the Bard (Thoppilan et al., 2022) which\nwe accessed through a web-based interface7 which\ndoes not specify an exact model card, but model\nresponses were collected between April 15th, 2023\nand May 15th, 2023. Second, the PaLM-v2-bison\nmodel (Narang and Chowdhery, 2022) (model\ncard text-bison@001), which was accessed\nthrough the Google Cloud VertexAI API.\nAnthropic Model. We collected outputs of the\nClaude V1.3 model (model card:claude-v1.3),\nthe latest and largest Anthropic model at the time\nof publication, using the official API hosted by\nAnthropic8.\nCohere Model. We collected outputs of Cohere’s\ncommand-xlarge model, the latest and largest\nCohere model at the time of publication, using the\nofficial API hosted by Cohere9.\nOpenAI Models. We collected outputs for\neight OpenAI models. Six models are from the\nGPT-3 family: Ada001 ( text-ada-001),\nBab001 ( text-babbage-001),\nCur001 ( text-curie-001), Dav001\n(text-davinci-001), Dav002\n(text-davinci-002), and Dav003\n(text-davinci-003). We also include\nGT3.5-turbo ( gpt-3.5-turbo) and GPT-4\n(gpt-4). All models were accessed through\nOpenAI’s official API10.\nB Explanation Annotation Guidelines\nWe hired a professional annotator to complete the\nannotation of model-generated explanations for\nAggreFact. The annotators were compensated at\n$20/hour. They received onboarding documenta-\ntion that introduced them to the task, and provided\nthe following definition for each type of explana-\ntion:\n• No Explanation: If the model did not provide\nany explanation. (For example just saying:\n“The summary is inconsistent”),\n7https://bard.google.com/\n8https://github.com/anthropics/\nanthropic-sdk-python\n9https://docs.cohere.com/docs/\nthe-cohere-platform\n10https://github.com/openai/\nopeai-python\n• Entirely Correct: if the explanation correctly\nidentifies and explains one or more factual\ninconsistencies in the summary,\n• Partially Correct : if the explanation pro-\nvided contains several elements and at least\none of them correctly identifies and explains\na factual inconsistency in the summary,\n• Unrelated: if the explanation given does not\ndirectly relate to a factual inconsistency be-\ntween the summary and the document,\n• Incorrect: if the explanation given does not\ncorrectly identify a factual inconsistency in\nthe summary, for example, making a logical\nerror.\nAn example for each type of explanation was\nprovided during onboarding. Annotation was per-\nformed in batches, and the first two batches of\nannotation by the annotator were reviewed by the\nauthors of the paper. Incorrect annotations were dis-\ncussed, allowing the annotator to better understand\nedge cases of the task, and modify their annotation\nin the first batches. Each annotator could commu-\nnicate with one of the authors to discuss edge cases\nand maintain a common understanding of the task.\nAnnotators could not communicate with each other.\nC S UMM EDITS Annotation Guidelines\nWe hired two professional annotators to complete\nthe annotation of Steps 1 and 3 of theSUMM EDITS\nprotocol (see Section 4). The annotators were com-\npensated at $20/hour. They received onboarding\ndocumentation that introduced them to the task and\nused the interface shown in Figure 3.\nAnnotators were first assigned 10 warm-up seed\nsummaries, each with roughly 30 edited summaries,\nwhich had been pre-annotated by the authors of the\npaper. The authors reviewed the completed warm-\nup exercises, and a strong agreement level on the\nwarm-up task with both annotators was observed.\nAnnotators could communicate with one of the\nauthors of the paper to discuss any edge case or\ndomain-specific question. For example, the annota-\ntion for the QMSumm domain required additional\ninstructions due to query-focused formulation of\nthe task, and instructions were communicated on\nhow to deal with the “query” element when eval-\nuating summaries. Namely, during Step 1 of the\nprotocol, participants were asked to additionally\njudge whether the summary accurately responded\n9674\nDocument:\nSimulation is a useful tool in\nsituations where training data for\nmachine learning models is costly to\nannotate or even hard to acquire. In\nthis work, we propose a\nreinforcement learning-based\nmethod for automatically adjusting\nthe parameters of any (non-\ndifferentiable) simulator, thereby\ncontrolling the distribution of\nsynthesized data in order to\nmaximize the accuracy of a model\ntrained on that data. In contrast to\nprior art that hand-crafts these\nsimulation parameters or adjusts\nonly parts of the available\nparameters, our approach fully\ncontrols the simulator with the\nactual underlying goal of\nmaximizing accuracy, rather than\nmimicking the real data distribution\nor randomly generating a large\nvolume of data. We ﬁnd that our\napproach (i) quickly converges to\nthe optimal simulation parameters in\ncontrolled experiments and (ii) can\nindeed discover good sets of\nparameters for an image rendering\nsimulator in actual computer vision\napplications.\nOriginal Summary:\nWe propose an algorithm that automatically adjusts parameters\nof a simulation engine to generate training data for a neural\nnetwork such that validation accuracy is maximized.\nTask 1:\nIs any of the information in the summary not  present in the\ndocument?\n Yes  No\nAre there any other issues with the summary? (incomplete\nsentence, formatting, etc.)\n Yes  No\nS u b m i t \nTask 2:\nModiﬁed Summaries:\nWe propose an algorithm that automatically adjusts parameters\nof a simulation engine to generate training data for a neural\nnetwork such that validation accuracy is maximized only\nslightly improved .\n Inconsistent  Consistent  Borderline\nWe propose an algorithm that automatically adjusts parameters\nof a simulation engine to generate training testing data for a\nneural network such that validation accuracy is maximized.\n Inconsistent  Consistent  Borderline\nWe propose an algorithm that automatically adjusts changes\nparameters of a simulation engine to generate training data for\na neural network in such a way that validation accuracy is\nmaximized.\n Inconsistent  Consistent  Borderline\nFigure 3: Two-column annotation interface used to annotate samples in the SUMM EDITS benchmark. Participants\ncould read the document on the left-hand column. Once they completed Task 1 in the right-hand column, the second\nannotation task became visible.\n9675\n#Distinct Edit Types\nModel 1 2 3 4\nDAE 50.2 53.5 55.4 64.9\nSummaC 58.2 56.3 57.6 67.3\nQAFactEval 59.4 63.7 72.3 76.5\nDav001 50.0 50.5 53.9 63.1\nVicuna-13b 52.8 57.0 60.2 58.5\nCohere-cmd-XL 50.0 55.9 63.7 70.0\nClaude v1.3 57.5 60.6 65.4 64.3\nDav002 56.3 61.2 69.4 81.7\nBard 61.0 64.9 72.4 73.4\nPaLM2-Bison 66.1 69.5 79.6 69.4\nChatGPT 68.5 71.4 82.0 86.6\nDav003 65.3 72.0 85.8 88.8\nGPT4 81.0 83.0 92.0 94.3\nAverage 59.2 62.5 69.2 74.1\nTable 7: Relationship between the number of edits types\nin the summary and balanced accuracy of models on\nSUMM EDITS . Models generally perform better as the\nnumber of introduced edits in a summary increases.\nto the query, and otherwise mark summaries as\ninadequate.\nD Number of Edits Effect\nUsing the labels of edit types generated in Sec-\ntion 5.4, each edited summary labeled as inconsis-\ntent receives between one and four edit types. We\ngroup the summaries based on the number of dis-\ntinct edit types that they contain, and report results\non this axis in Table 7.\nIn general, we find that as the number of edit\ntypes present in a summary increases, the majority\nof detection models (both LLM and specialized)\nsee sizable performance improvements, with an\naverage performance of 59.2 for summaries con-\ntaining a single error type, compared to 74.1 for\nsummaries containing all four error types.\n9676",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8541877269744873
    },
    {
      "name": "Computer science",
      "score": 0.5649598240852356
    },
    {
      "name": "Cognitive science",
      "score": 0.4577857255935669
    },
    {
      "name": "Lens (geology)",
      "score": 0.4412652552127838
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3795360326766968
    },
    {
      "name": "Psychology",
      "score": 0.24910226464271545
    },
    {
      "name": "Physics",
      "score": 0.06567275524139404
    },
    {
      "name": "Optics",
      "score": 0.06375440955162048
    }
  ]
}