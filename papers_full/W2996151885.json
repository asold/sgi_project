{
  "title": "Language Model-Driven Topic Clustering and Summarization for News Articles",
  "url": "https://openalex.org/W2996151885",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2034102953",
      "name": "Peng Yang",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2122100353",
      "name": "Wenhan Li",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2114011897",
      "name": "Guangzhen Zhao",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2034102953",
      "name": "Peng Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122100353",
      "name": "Wenhan Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114011897",
      "name": "Guangzhen Zhao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2841402245",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6638107783",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2132827946",
    "https://openalex.org/W6834357723",
    "https://openalex.org/W1969486090",
    "https://openalex.org/W6731883649",
    "https://openalex.org/W2467173223",
    "https://openalex.org/W6639619044",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W4233135949",
    "https://openalex.org/W6703400554",
    "https://openalex.org/W2963385935",
    "https://openalex.org/W2962686197",
    "https://openalex.org/W6640212811",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2259890326",
    "https://openalex.org/W6687459197",
    "https://openalex.org/W2803345270",
    "https://openalex.org/W2417560918",
    "https://openalex.org/W6683240801",
    "https://openalex.org/W6674813771",
    "https://openalex.org/W2798523152",
    "https://openalex.org/W2740139069",
    "https://openalex.org/W6729053843",
    "https://openalex.org/W2807056933",
    "https://openalex.org/W2338457964",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2540404261",
    "https://openalex.org/W2161050705",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964042380",
    "https://openalex.org/W2100002341",
    "https://openalex.org/W4301368689",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2574301479",
    "https://openalex.org/W2197590357",
    "https://openalex.org/W1762643624",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2107743791",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W2157006255"
  ],
  "abstract": "Topic models have been widely utilized in Topic Detection and Tracking tasks, which aim to detect, track, and describe topics from a stream of broadcast news reports. However, most existing topic models neglect semantic or syntactic information and lack readable topic descriptions. To exploit semantic and syntactic information, Language Models (LMs) have been applied in many supervised NLP tasks. However, there are still no extensions of LMs for unsupervised topic clustering. Moreover, it is difficult to employ general LMs (e.g., BERT) to produce readable topic summaries due to the mismatch between the pretraining method and the summarization task. In this paper, noticing the similarity between content and summary, first we propose a Language Model-based Topic Model (LMTM) for Topic Clustering by using an LM to generate a deep contextualized word representation. Then, a new method of training a Topic Summarization Model is introduced, where it is not only able to produce brief topic summaries but also used as an LM in LMTM for topic clustering. Empirical evaluations of two different datasets show that the proposed LMTM method achieves better performance over four baselines for JC, FMI, precision, recall and F1-score. Additionally, the generated readable and reasonable summaries also validate the rationality of our model components.",
  "full_text": "Received December 2, 2019, accepted December 15, 2019, date of publication December 18, 2019,\ndate of current version December 31, 2019.\nDigital Object Identifier 10.1 109/ACCESS.2019.2960538\nLanguage Model-Driven Topic Clustering and\nSummarization for News Articles\nPENG YANG\n 1,2,3, WENHAN LI\n 1,3, AND GUANGZHEN ZHAO\n1,3\n1School of Computer Science and Engineering, Southeast University, Nanjing 211189, China\n2School of Cyber Science and Engineering, Southeast University, Nanjing 211189, China\n3Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, Nanjing 211189, China\nCorresponding author: Peng Yang (pengyang@seu.edu.cn)\nThis work was supported in part by the Consulting Project of Chinese Academy of Engineering under Grant 2020-XY-5, in part by the\nNational Natural Science Foundation of China under Grant 61472080 and Grant 61672155, and in part by the Collaborative Innovation\nCenter of Novel Software Technology and Industrialization.\nABSTRACT Topic models have been widely utilized in Topic Detection and Tracking tasks, which aim\nto detect, track, and describe topics from a stream of broadcast news reports. However, most existing topic\nmodels neglect semantic or syntactic information and lack readable topic descriptions. To exploit semantic\nand syntactic information, Language Models (LMs) have been applied in many supervised NLP tasks.\nHowever, there are still no extensions of LMs for unsupervised topic clustering. Moreover, it is difﬁcult\nto employ general LMs (e.g., BERT) to produce readable topic summaries due to the mismatch between\nthe pretraining method and the summarization task. In this paper, noticing the similarity between content\nand summary, ﬁrst we propose a Language Model-based Topic Model (LMTM) for Topic Clustering by\nusing an LM to generate a deep contextualized word representation. Then, a new method of training a Topic\nSummarization Model is introduced, where it is not only able to produce brief topic summaries but also\nused as an LM in LMTM for topic clustering. Empirical evaluations of two different datasets show that the\nproposed LMTM method achieves better performance over four baselines for JC, FMI, precision, recall and\nF1-score. Additionally, the generated readable and reasonable summaries also validate the rationality of our\nmodel components.\nINDEX TERMS Topic model, topic summarization, language model, seq2seq.\nI. INTRODUCTION\nWith the rapid development of Internet technology, online\nmedia has become an important way for the public to publish\nand obtain information. Compared to other text medias, such\nas Twitter and blogs, news texts describe events in a timely\nand continuous manner, where the content is attractive and\nbelievable, and the texts are often high quality. Therefore,\nnews data has become an accurate and stable source of infor-\nmation for the public.\nHowever, since the number of news reports is very large,\nand different news articles have different values, as well as\ngain different amounts of attention, the articles reporting\nimportant and hot events may be overshadowed by those with\nless value. Additionally, for news related to an ongoing event,\nit is difﬁcult for readers to link the current news with previous\narticles, which leads to them having difﬁculty analyzing the\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Jerry Chun-Wei Lin\n .\ndevelopment of the events. Therefore, automatically extract-\ning hot events from massive news reports and linking them\nwith related topics is an urgent issue [1], [2].\nTopic Detection and Tracking (TDT) aims to detect, track,\nand describe topics from a stream of broadcast news reports.\nSince the topics are not deﬁned previously, the task of topic\ndetection and tracking can be regarded as an unsupervised\nclustering task that is used to design a model which is able to\ndivide articles into several groups automatically by extracting\nsemantics. One popular way to handle the TDT problem is\nthrough a Topic Model.\nTraditional topic models, such as pLSA [3], LDA [4] and\ntheir extensions, are probabilistic models, which represent\na document by the distribution of topics and represent a\ntopic by the distribution of words [5]. These probabilistic\nmodels are unsupervised and have led to signiﬁcant progress\nin TDT. However, there are several shortcomings among\nthese approaches. (a) A document is represented by a bag-\nof-words matrix, where each word is a one-hot vector, which\n185506 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/VOLUME 7, 2019\nP. Yanget al.: LM-Driven Topic Clustering and Summarization for News Articles\nleads to a lack of modeling semantics of both the words\nthemselves and the ordering of the words in the document [6].\nSpeciﬁcally, different words may have similar meanings. For\nexample, the two sentences I like Coke and I love cola have\nsimilar meanings and topics, but their bags of words are\nquite different since there is only one word that is exactly\nthe same. In addition, the same bag of words might have\ndifferent meanings, such as the department chair couches\noffers and the chair department offers couches . (b) Each topic\nis represented by a probability distribution of words, which is\ndifﬁcult to understand since it contains many useless words\nand lacks a brief summary.\nWith the wide application of deep learning technology in\nnatural language processing, a variety of neural network-\nbased topic models have been proposed. In general, neural\nnetworks employ word embedding technology to map words\nto dense vectors with low dimensions, which are able to\ncapture the semantics of words well, since words having\nsimilar meanings are mapped to similar vectors, for example,\nI like Coke and I love cola are transformed to similar matrixes,\nas like is similar to love and Coke is similar to cola. Moreover,\nRecurrent Neural Networks (RNN), such as Long Short-Term\nMemory (LSTM) [7], and a Gated Recurrent Unit (GRU) [8],\nare able to model a long short-term sequential dependency\nand capture the semantics of the ordering of the words. For\nexample, Cao et al. [9] proposed the use of word embed-\nding technology while generating topic-word distributions.\nYang et al. [10] proposed a model incorporating both the\nsemantic meanings of sentences and the words. Tian et al. [6]\nproposed the use of RNN to model a long-term dependency.\nHowever, most of these models are shallow neural net-\nworks, possibly because, although the neural networks may\nobtain stronger ﬁtting abilities with the increasing number\nof layers, a neural network that is too deep tends to overﬁt\nthe training set and may not converge because of the prob-\nlem of gradient disappearance. Moreover, since traditional\nword embedding technology converts each word to a single\ncontext-independent representation, it is impossible to model\npolysemy across linguistic contexts. In general, deep neural\nLanguage Models (LM), such as ELMo [11], BERT [12],\nand XLNet [13], are used to solve these problems by using\ndeep pretrained neural networks that extract semantic and\nsyntactic information and generate contextualized word rep-\nresentations. Deep neural LMs have perform exceptionally\nwell and are state-of-the-art approaches for many supervised\nNLP tasks. However, to the best of our knowledge, there are\nno approaches employing LMs to extract semantics for topic\nclustering that is unsupervised.\nMost models describe a topic by its key words or the dis-\ntribution of all words. With Sequence-to-Sequence (seq2seq)\ntechnology, a topic summary can be generated as a sentence\nto make a topic easy to understand. Seq2seq models have\nbeen successfully applied to text summarization, which aims\nat generating a summary of a text, whereas a topic summary\nis generated from several texts belong to the same topic.\nSutskever et al. [14] proposed seq2seq models in neural\nmachine translation, and Rush et al. [15] used a seq2seq\nmodel for the abstractive sentence summarization task. Gen-\nerally, a seq2seq model contains an encoder, which trans-\nforms the input sequences into a hidden vector, and a decoder,\nwhich generates the output sequences with the hidden vector;\ntherefore, the hidden vector contains most of the semantics of\nthe input and can be used to compute the topic distribution.\nIn conclusion, there are two main tasks of the TDT prob-\nlem. The ﬁrst task is to cluster documents into different topics\nvia their content. The other task is to produce a readable sum-\nmary for each topic to make it understandable. As traditional\ntopic models do not model semantics, especially syntactic\ninformation, we propose a Language Model-based Topic\nModel for topic clustering by using deep LM. Although gen-\neral LMs, such as BERT and XLNet, signiﬁcantly improve\nthe state-of-the-art approaches to several NLP problems,\nit is difﬁcult to apply them for summarization due to their\npretraining methods and ﬁxed vocabulary. Instead, we can\npropose a seq2seq Topic Summarization Model that is able\nto make a brief summary of each topic, and its encoder can\nbe the Language Model for LMTM. The main contributions\nof this paper are as follows:\n• We formally deﬁne the problems of topic clustering and\ntopic summarization, give a speciﬁc formal description\nof the Language Model-based Topic Model and the\nTopic Summarization Model, and then show the process\nof topic clustering and summarization.\n• We propose the use of LMs, which have been applied\nin many supervised NLP tasks but not to unsupervised\ntopic clustering, to overcome the limitation of traditional\ntopic models, i.e., that they do not make full use of\nsemantic and syntactic information, and to explain the\ndetails of LMTM by using a novel language model,\ne.g., BERT.\n• The Topic Summarization Model is proposed since it is\ndifﬁcult to produce topic summaries with general LMs.\nMoreover, the encoder of TSM can be used as a Lan-\nguage Model for LMTM, and LMTM-TSM shows com-\npetitive performance in experiments on two datasets.\nII. RELATED WORK\nThe main tasks of TDT are as follows: (a) identify whether\na document discusses a new topic or follows an old topic;\n(b) divide documents into distinct topics; (c) produce topic\ndescriptions via key words or summaries to make them under-\nstandable. A topic model is able to ﬁgure out which topic each\ndocument belongs to and solve the TDT problem.\nFor traditional topic models, each document d is repre-\nsented as a probability distribution over topics T , and each\ntopic ti is deﬁned as a probability distribution over words w.\nThe probability of whether document d contains word wj can\nbe computed as follows:\np\n(\nwj |d\n)\n=\nK∑\ni=1\np\n(\nwj |Ti\n)\np (Ti |d), (1)\nwhere K is the number of topics.\nVOLUME 7, 2019 185507\nP. Yanget al.: LM-Driven Topic Clustering and Summarization for News Articles\nProbabilistic latent semantic indexing (pLSI) [3] assumes\np\n(\nwj |Ti\n)\nand p (Ti |d) are both multinomial random vari-\nables. After adjusting the parameters so that p\n(\nwj |d\n)\nﬁts the\nfrequency of words in the documents, p (Ti |d)is supposed\nto show the probability of document d belonging to topic Ti.\nThe key words of topic Ti are picked as a description by\nusing p\n(\nwj |Ti\n)\n, which measures the connection between\nwj and Ti. Latent Dirichlet Allocation (LDA) [4], which is\nthe most popular topic model, introduced Dirichlet conju-\ngate priors for the two multinomial distributions and shows\nbetter performance. Following LDA, there are several topic\nmodels proposed, such as hierarchical topic models [16],\nsupervised LDA [17], and labeled LDA models [18]. Most\nof these models use a bag-of-words model to represent a\ndocument, which might be too simple, since it lacks the\nsemantics of both the words themselves and the ordering of\nwords in the document. To extract the semantics of the text\nspan, such as noun-phrases, copulaLDA [19] extends LDA\nby integrating part of the text structure into the model and\nrelaxing the conditional independence assumption between\nthe word-speciﬁc latent topics, given the per-document topic\ndistributions. To extract the information of the structure of\nthe textual input, sentenceLDA [20] extends LDA by incor-\nporating the structure of the text in the generative and infer-\nence processes. Considering the word and sentence-level\nsemantics, these extensions show better performance than the\nprimary LDA.\nRecently, neural networks, which employ word embedding\ntechnology to model the semantics of words and RNN-based\nunits to extract the syntactic information from the order of\nwords, have been widely applied in NLP problems. In regard\nto the TDT problem, neural networks are generally designed\nby modeling topic representations as the output of a hidden\nlayer [1], [6], [9], [21], [22], [24], [25]. Hinton and Salakhut-\ndinov [21] proposed a restricted Boltzmann machine to model\ntopic features as latent variables. Cao et al. [9] proposed\nsNTM, which uses word embedding technology to compute\nthe distribution of topics over words, where words with simi-\nlar semantics are represented as similar vectors. Additionally,\nit uses n-gram to model the sequential dependency of words.\nTian et al. [6] proposed a sentence level model, which is able\nto extract the semantics of sequential dependency with RNN.\nZhou et al. [1] proposed a model named NSEM that focuses\non storylines, which can be considered as hidden topics vary-\ning over time.\nNeural networks have shown signiﬁcant performance in\ntopic clustering, but there are still some issues. The syntaxes\nof documents, which are important to obtain the content of\narticles and extract topics, are not fully used through the exist-\ning shallow models. Additionally, traditional word embed-\nding technology is not able to model polysemy. To solve\nthese problems, Language Models (LM), such as ELMo [11],\nBERT [12], and XLNet [13], which are deep neural networks\npretrained on large open datasets to learn the general semantic\nand syntactic information of documents, are employed to gen-\nerate contextualized word representations by using semantic\nand syntactic information. Since general LMs are designed\nfor open downstream NLP tasks, they always contain huge\nnumbers of parameters and cost too much to pretrain and\nﬁnetune. For instance, XLNet is trained on 32.89 B subword\npieces from Wikipedia, BooksCorpus, Giga5, ClueWeb, and\nCommon Crawl, with 512 TPU v3 chips for 500 K steps,\ntaking approximately 2.5 days. For a speciﬁc task, such as\na TDT problem, it is able to design a proper language model\nthat has fewer parameters, costs less to pretrain and ﬁnetune,\nbut shows competitive performance.\nGenerating a summary for each topic can be consid-\nered as a sentence production problem, which is usually\nhandled with sequence to sequence (seq2seq) models [14].\nGenerally, a seq2seq model is composed of two parts,\ni.e., an encoder and a decoder. The encoder transforms a\nsequence = (x1,..., xn) into a hidden vector or matrix h,\nwhere xi is the word embedding representation of the\ni-th word in the input sentence, and the decoder outputs a\nsequence Y =(y1,..., ym) with h, where yj is a representa-\ntion of the j-th word in the output sentence. Seq2seq models\nwere ﬁrst used in machine translation [14], [26], [27] and\nhave been widely applied to other NLP tasks, including text\nsummarization [15], [27], [29]. Rush et al. [15] proposed a\nseq2seq with attention to the abstractive sentence summa-\nrization task, and Klein et al. [27] introduced a model based\non RNN. There are also convolutional neural network (CNN)\nbased seq2seq models, which can be parallelized to train and\nevaluate in linear time [26]. Since the vocabularies of general\nLMs are ﬁxed after pretraining, it is difﬁcult to design a\nseq2seq model by using pretrained LMs.\nIn text summarization, the input is the content of the arti-\ncle, while the output is the corresponding summary. Once\na seq2seq model is able to compute a hidden vector from\nthe content with its encoder and generate the corresponding\nsummary from the hidden vector with its decoder, it is con-\nsidered that the encoder is capable of extracting the semantics\nof the article, and the hidden vector is composed with the\nextracted semantics. Therefore, a pretrained encoder can be\na Language Model that extracts the semantics of articles.\nIn addition, if a text summarization model is able to produce\nan understandable and reasonable summary of a document,\nit can generate a summary of a topic with the inputs changed\nto all the documents of the topic. Thanks to the high quality\nand good structure of news articles, the titles are always\nproper summaries of their main bodies, and it is possible to\ntrain a text summarization model by using the main bodies of\nnews articles as inputs and the corresponding titles as outputs.\nBased on the analysis above, we propose a Language\nModel-based Topic Model, which employs a well-pretrained\ndeep neural LM to extract semantic and syntactic information\nfrom documents and map it to the proper topics. Considering\nthat general LMs are too large to be ﬂexible and efﬁcient, and\nthe pretraining of them is useless for topic clustering or sum-\nmarization, we propose a text summarization model whose\nencoder can be used as a light and ﬂexible LM. After topic\nclustering, the well-pretrained text summarization model is\n185508 VOLUME 7, 2019\nP. Yanget al.: LM-Driven Topic Clustering and Summarization for News Articles\nable to generate summaries for topics that are readable and\nunderstandable.\nIII. PROPOSED ARCHITECTURE\nIn this section, we ﬁrst deﬁne the formal statement of topic\nclustering and summarization and then introduce the overall\nframework of our approach.\nA. PROBLEM STATEMENT\nThe goal of this paper is to formulate a topic model that is\nable to not only cluster topics from a data set of news articles\nbut also produce an understandable summary for each topic.\nSince a news article is composed of a title and a main body,\nwhere the title is supposed to be a good summary of the main\nbody, we model the data set of articles as A ={Aik |Aik =\n(tik ,mbik ),i,kϵN∗,1 ≤k ≤K}, where (ti,mbi)are the title\nand the main body of the i-th article of the k-th topic. Let\nTopic be a set indicating topics, Topic ={k |1 ≤k ≤K},\nwhere K is the max index of topics, which are deﬁned before\nbuilding the model. Formally, the topic clustering problem\ncan be deﬁned as follows:\nDeﬁnition 1 (Topic Clustering for News Articles): This task\nis formulated as an unsupervised clustering problem, which\nlearns a model tc from an unlabeled data set of articles, and\nthe model is able to divide the articles into several groups\naccording to their semantics. The model can be denoted as\nfollows:\ntc\n\n\nA11 ··· Ai1 ···\n... ... ...···\nA1K ··· AiK ···\n\n=\n\n\n1 ··· 1 ···\n... ... ...···\nK ··· K ···\n\n. (2)\nDeﬁnition 2 (Text and Topic Summarization): this task aims\nto generate a summary from an article or an understandable\nsentence to describe a topic. We assume topic summariza-\ntion is similar to text summarization, where the former is\nto produce a summary of a sequence composed of several\ndocuments related to a topic, and the latter is to produce\na summary of a sequence composed of a single document.\nA sequence-to-sequence model ts is formulated; when the\ninput x is a sequence x0x1 ... xn,g is supposed to generate\nanother sequence y =y0y1 ... ym, where xi and yj are the\ndistribution of the corresponding words in x and y. The\nelement yj in output sequence y is produced one by one with\ninput x and the previous result y0y1 ... yj−1, that is, as follows:\nts\n(\nx0x1 ... xn,y0y1 ... yj−1\n)\n=yj, (3)\nwhere 1 ≤j ≤m, and the values of x0 and y0 are always\ndistributions representing the special token [start].\nWhen training, we generate titles from the main bodies,\nthat is, as follows:\n∀Aik ∈A, 1 ≤j <J,\nts\n(\nmbik0mbik1 ... mbikn,tik0tik1 ... tikj−1\n)\n=tikj, (4)\nwhere J is the max length of the title.\nFIGURE 1. The process of topic clustering and summarization. The TSM is\ntrained on a large dataset as the pre-training process of topic clustering,\nwhich makes it able to summarize. The TSM encoder is used as a\nLanguage Model for LMTM. When predicting, the LMTM determines which\ntopic each article belongs to, and the TSM is used to produce summaries\nfor the topics. Loss0 and loss1 are the loss functions to train the TSM and\nLMTM, respectively.\nDuring topic summarization, we generate summaries for\ntopics from the corresponding articles, that is, as follows:\n∀k ∈Topic, 1 ≤j <J,\nts\n(\nzk,0zk,1 ... zk,n,sk,0sk,1 ... sk,j−1\n)\n=sk,j, (5)\nwhere z = zk,0zk,1 ... zk,n is a sequence composed of all\narticles related to topic k, and s = sk,0sk,1 ... sk,J is its\nproduced summary.\nB. THE FRAMEWORK OF MODELS\nIn this paper, we propose a method for topic clustering and\nsummarization. Fig. 1 shows the details of the process, where\nthere are four main steps, as follows: (a) train the TSM on a\nlarge news dataset so that it can produce a proper summary\nfor an article; (b) train the LMTM on the topic dataset;\n(c) determine which topic each article belongs to; and (d) for\neach topic, generate a summary with key articles by using\nthe TSM. Step (a) belongs to pretraining process, whereas\n(b-d) belong to predicting process. The models can be\ndescribed as follows:\nDeﬁnition 3 (Language Model-Based Topic Model\n(LMTM)): Let the input news article be a sequence\nx =x0x1 ... xn. The LMTM ﬁrst converts x to a ﬁxed length\nhidden vector h = (h1,,..., hJ ) with a well pretrained\nLanguage Model. Then, it employs a full connected layer acti-\nvated by SoftMax activation to obtain the topic distribution\nt =(t1,..., tK ). The ﬁnal topic indication is the index k,\nwhich corresponds to the largest probability tk . The process\ncan be formulated as follows:\nh =LM (x0x1 ... xn), (6)\nt =softmax (Vh +b), (7)\nk =argmax([t1,..., tK ]). (8)\nDeﬁnition 4 (Topic Summarization Model (TSM)): Let\nAi ={A1\ni ,... Aj\ni}be the set of articles belonging to topic i, and\nthe articles are sorted in descending order according to their\nVOLUME 7, 2019 185509\nP. Yanget al.: LM-Driven Topic Clustering and Summarization for News Articles\nTABLE 1. Comparison of existing approaches.\ncorresponding probabilities ti = {t1\ni ,... tj\ni }. The summary\nof topic i is produced by the TSM with the ﬁrst n articles,\nwhere n ≤3 and the minimum probability is larger than the\nthreshold θ. If the maximum probability is smaller than θ,\nthe topic does not have a proper summary, and the TSM\noutputs a null character string ′′, as follows:\nA′\ni =\n{\nAn\ni |n ≤3,tn\ni >θ\n}\n, (9)\nTSM (Ai)=\n{\ng\n(\nA′\ni\n)\n, A′\ni ̸=∅\n′′, A′\ni =∅. (10)\nIV. METHODOLOGY\nA. MOTIVATION\nThe features of four existing topic models are shown\nin Table 1. It can be observed that all of the models are\nbased on the bag-of-words assumption, which leads to a lack\nof the information from the orders of words. To improve\nthe disadvantages of LDA [4], NTM [9] and NSEM [1] are\nneural networks, which employ word embedding technology\nto extract the semantic information. However, since the tra-\nditional word embedding technology converts a word into\na static representation, it is impossible to model polysemy.\nTwo of the approaches, i.e., senLDA [20] and NTM, have\naddressed the importance of syntactic information for topic\nclustering, where senLDA generates a topic distribution for\neach sentence and NTM extracts some syntactic information\nby using an n-gram model. However, these methods are too\nsimple to model long-term dependence at the document-\nlevel. Moreover, NSEM applies an extra processing step of\nNamed Entity Recognition (NER) to obtain the key elements\nof articles. Thus, the performance of NSEM heavily depends\non the tools of NER.\nTo overcome the problems above, an advanced topic model\nis supposed to make full use of both the semantic and\nsyntactic information from articles and produce a readable\ndescription for each topic. Thanks to the pretraining on large\ndatasets, deep LMs with RNNs and Transformers [30] are\nbelieved to extract both semantic and syntactic informa-\ntion and generate dynamic contextualized word representa-\ntions to model polysemy. Therefore, we propose employing\nan LM for topic clustering.\nLMs have been applied for many supervised NLP tasks,\nin which the loss functions are easy to design with golden\nlabels. However, in regard to topic clustering, it is difﬁcult to\ndesign a proper model and the corresponding loss function,\nsince this is an unsupervised problem. Following the steps of\nNTM and NSEM, we propose a self-supervised method to\ndesign the loss function with the following assumption:\nAssumption 1: For an article, the topic distribution of its\ncontent and summary should be similar . This assumption is\nnatural, as a summary is always a short statement representing\nthe whole article. Since there are no external golden labels for\nunsupervised tasks, topic models must be based on internal\ninformation. For general documents, the summaries may not\nbe available, but in regard to a news article, the corresponding\ntitle is always an acceptable summary. With this assumption,\na neural network can be trained with a loss function based on\nthe following:\nsim\n(\npm,ppos\n)\n≫sim\n(\npm,pneg\n)\n, (11)\nwhere pm is the topic distribution of a document and\nppos is the distribution of the relevant title, whereas pneg is\nthe distribution of an irrelevant title randomly sampled from\nthe dataset.\nBased on assumption 1, the LM-based Topic Model is\nproposed for topic clustering, where the LM can be either\nElMo, BERT or XLNet. However, since general LMs are\nnot pretrained for speciﬁc downstream tasks, especially not\nfor summarization, when applied to the seq2seq architecture,\nthey do not perform well, possibly because there is mismatch\nbetween the pretrained LM and the untrained decoder. There-\nfore, we proposed a method to train a seq2seq TSM, which is\nemployed to produce summaries of topics, and its encoder can\nbe an LM for topic clustering with the following assumption.\nAssumption 2: If a hidden matrix can be used to generate\na summary from a document, the matrix contains all the\nsemantics of the document .\nFor a news article, the title is supposed to be a good sum-\nmary of the main body. A seq2seq-based text summarization\nmodel is able to compute a hidden matrix with its encoder.\nIn addition, its decoder is designed to produce a title with\nthe matrix. Therefore, the encoder is supposed to extract the\n185510 VOLUME 7, 2019\nP. Yanget al.: LM-Driven Topic Clustering and Summarization for News Articles\nTABLE 2. Comparison of LMTMs.\nsemantics of articles. Moreover, compared to topic models,\ntext summarization models are supervised and often take\ncross entropy as the loss function. The LMTM and TSM can\nbe trained as a multi-task model, which is supposed to lead to\nbetter performance.\nTable 2 shows the features of two LMTMs. BERT is\nselected as a typical general LM to be applied in an LMTM.\nThe proposed models can extract semantic and document-\nlevel syntactic information by using the LM to generate con-\ntextualized word representations, which are supposed to be\nadvantageous for topic clustering. Compared with BERT, the\nseq2seq-based TSM is able to produce understandable topic\nsummaries with an extra topic summarization.\nB. LM-BASED TOPIC MODEL\nBased on assumption 1, we propose a Language Model-based\nTopic Model (LMTM) for topic clustering. To overcome the\nshortcomings of the existing approaches, we use an LM,\nsuch as BERT, to extract not only the semantic but also the\nsyntactic information of a document and compute a ﬁxed-\nlength vector to model the information. Then, a shallow\nneural network is used to transform the vector into the prob-\nability distributions of which the topics of the document are\nsupposed to belong to.\nAs shown in Fig. 2, the LMTM is composed of three mod-\nules. From bottom to top, the ﬁrst one is the input module,\nwhich takes articles as inputs and employs a tokenizer to\ndivide the articles into sequential tokens. The second one is an\nLM, which extracts the features from the tokens and converts\nthem into a ﬁxed-length vector. The last one is the topic\nmodule, which is designed to compute the topic distributions\nfor the articles.\nHere, we select BERT as a typical LM for feature extrac-\ntion, since it is one of the most famous language models\nand demonstrates excellent performance in many NLP tasks.\nTherefore, the output of the input module must be suitable\nfor BERT. That is, each sequence of tokens is supposed\nto start with a special token [cls] and end up with [sep].\nAdditionally, the words in the vocabulary of BERT will not\nchange after tokenizing, while the words that are not in the\nvocabulary will be divided into several sub-words to ﬁt the\ntable. Rarely, words that are not able to be transferred into\ntokens in the table will be changed into a special token\n[unknown]. Since BERT is pretrained with a max length\nof 512, the sequence of tokens is supposed to be less than 512.\nIf the length exceeds 512, the extra tokens are intercepted\nFIGURE 2. LMTM-BERT. The LMTM first converts the input documents into\nfixed length vectors by using a deep LM (e.g., BERT), then it computes\ntheir topic distributions with a full connection layer. When training,\nthe inputs are three sequences <M, P, N>, and the corresponding topic\ndistributions <m, p, n> are used in the loss function. When predicting,\nthe input is a single document, and the output is its topic distribution.\nfrom the end. If the length is less than 512, 0 is added to the\nend until the length reaches 512.\nTo obtain the semantic information from the sequential\ntokens, BERT ﬁrst employs embedding layers to convert each\ntoken to a ﬁxed-length vector that contains both the base\nsemantic and position information of the token. Then, several\ntransformer layers, which are based on self-attention technol-\nogy and perform better in long-term dependence modeling\nthan RNN [30], are used to model the dependence between\nthe tokens and extract the syntactic information. The output\nvector of the last transformer’s ﬁrst token will be used as the\nfeature vector of the article, and the length of the feature vec-\ntor is always 768. The speciﬁc description of the transformer\nis as follows:\ntransformer (q,k,v)=softmax(qW q(kW k )T\n√\ndk\n)vW v, (12)\nwhere q,k,v are three inputs of the transformer, and\nW q,W k ,W v are the corresponding weights to be trained.\nVOLUME 7, 2019 185511\nP. Yanget al.: LM-Driven Topic Clustering and Summarization for News Articles\nSince BERT uses a transformer with self-attention, the three\ninputs of q, k, v are the same sequence of vectors.\nThe topic module aims to convert the ﬁxed-length semantic\nvectors into a topic distribution. The topic module is com-\nposed of a dropout layer, a fully connected layer activated by\nthe SoftMax function, and a loss layer to compute the loss\nfunction when training.\nA dropout layer is usually applied between layers to sup-\npress overﬁtting. The dropout layer discards several neural\nunits of the network with a probability during training. When\nit discards different units, the model can appear as different\nmodels to reduce the impact of special units.\nThe fully connected layer converts a hidden vector into\nanother vector whose length is the same as the number of\ntopics, and the SoftMax function transfers it to a probability\ndistribution. Let the hidden vector generated by BERT be\nh =(h1,,..., h768), the weight matrix and bias of the fully\nconnected layer be V ∈R768×K , b ∈RK , where K is the\nnumber of topics determined before building the model. The\noutput of the fully connected layer is f ∈ RK , which is\ncomputed as follows:\nf =Vh +b =\n\n\nv1,1 ··· v1,768\n... ... ...\nvK,1 ··· vK,768\n\n×\n\n\nh1\n...\nh768\n\n\n+\n\n\nb1\n...\nb768\n\n. (13)\nAdditionally, the ﬁnal topic distribution t ∈RK is com-\nputed as follows:\nt =softmax (f )=[ exp(f1)\n∑k\ni=1 exp(fi)\n,..., exp(fK )\n∑k\ni=1 exp(fi)\n],\n(14)\nwhere for each ti in t =(t1,..., tK ),\n0 ≤ti ≤1,\n∑K\ni=1 ti =1. (15)\nThe loss function aims to compute the loss of the current\nmodel, and it determines the gradient of the weights. When\ntraining, the inputs of the LMTM are denoted as a triple\n(M,P,N), where M is the main body of a news article, P is\na positive title, which means it is the title from the same\narticle so that P and M are supposed to have similar topic\ndistributions, and N is a negative title randomly sampled from\nthe training dataset.\nAccording to assumption 1, the pairwise loss function\nproposed by NSEM [1] and NTM [9] is shown as follows:\nLpw =[−sim\n(\ntm,tp)\n+sim\n(\ntm,tn)\n+]+, (16)\nwhere is a positive scalar, tm,tp,tn are the topic distribu-\ntions of M,P,N, separately, and sim is a function measuring\nthe similarity between two distributions.\nNSEM takes the Kullback-Leibler (KL) divergence as the\nsimilarity function, as it makes L1 easy to calculate and\nderive, as follows:\nsim =−KL (p0,p1)=−\n∑\np0logp0\np1\n. (17)\nHowever, the pairwise loss function considers the two\nsimilarities sim (tm,tp)and sim (tm,tn)together, which may\ncause their separate features to be ignored. To handle this\nproblem, we propose the use of another margin loss function,\nwhich considers both the separate features and the difference\nbetween the similarity distributions.\nLm =[−sim\n(\ntm,tp)\n+0]++[sim\n(\ntm,tn)\n+1]+.\n(18)\nFor instance, let tm =[0.9, 0.09, 0.01], tp =[0.45, 0.5,\n0.05] tn =[0.01, 0.09, 0.9],  =5, 0 =0.1,1 =5.1.\nIt can be calculated that sim (tm,tp)=−5.77, sim (tm,tn)=\n−0.65, and the results of the wo loss functions are Lpw =0,\nLm =0.55. It can be observed that although tm and tp are not\nsimilar enough, Lpw cannot ﬁnd any loss while Lm can.\nSince the KL divergence treats each topic probability\nequally, the ﬁnal distributions tend to be smooth, which\nmakes obtaining topic labels difﬁcult. Therefore, we pro-\nposed a new similarity function by using the max term rather\nthan the sum. The max term similarity (MTS) function and\ncan be denoted as follows:\nMTS (p0,p1)=−max\n(\np0logp0\np1\n+p1logp1\np0\n)\n. (19)\nThe ﬁnal loss function of the LMTM can be denoted as\nfollows:\nL1 =[−MTS\n(\ntm,tp)\n+0]++[MTS\n(\ntm,tn)\n+1]+.\n(20)\nMoreover, since the negative titles are sampled from the\ndataset randomly, it is probably that the sampled title N\nbelongs to the same topic with the primary main body M.\nThus, we propose a multiple sampling process, which sam-\nples several titles, and use the one that is the most different\nfrom the main body, since we believe that sequences belong-\ning to similar topics are likely to obtain similar distributions.\nIf there are a articles belonging to the same topic as the main\nbody M from b articles in the whole dataset, after sampling\none, the probability that the positive title and main body are\nfrom the same topic is a/b. In addition, after sampling twice,\nthe probability that the more dissimilar title and main body\nare from the same topic is a2/b2.\nC. TOPIC SUMMARIZATION MODEL\nGeneral language models are pretrained on large open\ndatasets for one or several tasks and are ﬁne-tuned to suit\ndownstream tasks, which leads to two shortcomings. One is\nthat since LMs must ﬁt several downstream tasks, they always\ncontain so many parameters that it is not sufﬁciently ﬂexible.\nFor example, when the dataset of a clustering task contains\n185512 VOLUME 7, 2019\nP. Yanget al.: LM-Driven Topic Clustering and Summarization for News Articles\nAlgorithm 1Multiple Sampling Process\nInput: main body M, titles T , pretrained LM, α,β0\nOutput: the negative title\n1: sample L =[t0,t1,t2] from T\n2: sims =[0,0,0]\n3: for ti in L do\n4: sims [i] =sim(M,ti)\n5: res =argmax(sims)\n6: return sims[res]\nmany unknown tokens, it is difﬁcult to add them into the\nvocabulary and make them ﬁt the pretrained weights. The\nother problem is, although general language models are difﬁ-\ncult and costly to train, the pretraining task is useless except\nin providing the weights. For example, BERT is pretrained by\npredicting the original word of the [mask] token and whether\ntwo sentences are successive or not. As the [mask] token is\nsampled randomly, BERT is not suitable for sequence gener-\nation. In addition, another famous LM, ELMo, is pretrained\nby maximizing the likelihood to predict the next token of\na given sequence bidirectionally, which can be denoted as\nfollows:\nmax\n0\nlog pθ(x) =\nn∑\ni=1\nlog p0\nθ(xi|x<i) +\nn∑\ni=1\nlog p1\nθ (xi |x>i),\n(21)\nwhere θ is the weight to be trained and n is the length\nof a sequence. However, the bidirectional ELMo only con-\ncatenates two models with different directions. As they\ndo not share semantics, they extract separately, so ELMo\nis not able to take full advantage of the contextual\ninformation.\nTherefore, only focusing on the downstream task of this\npaper, i.e., topic clustering, we propose a brief method to\ndesign and train a light language model.\nBased on assumption 2, a Topic Summarization Model\n(TSM) is designed with the seq2seq architecture, which is\nable to extract semantic and syntactic information from arti-\ncles by using its encoder and produce summaries from the\ninformation with its decoder. To be speciﬁc, the TSM aims to\npredict the i-th word in a title with the ﬁrst ( i−1) words in the\ntitle and the semantic matrix of the main body. Additionally,\nthe 0-th word of a title is always the special token, [start], and\nthe last word is [end].\nTherefore, the object function of pretraining the TSM can\nbe denoted as follows:\nmax\n0\nlog pθ(x,y) =\nn∑\ni=1\nlog pθ(yi|x,y<i), (22)\nwhere x is the main body of an article, and y is the cor-\nresponding summary. Since the titles of news articles are\nnaturally good summaries of their main bodies, the TSM is\nparticularly suitable for news articles. After training, the well-\ntrained encoder is suitable to be the LM for topic clustering,\nand the TSM can produce a brief summary for each topic from\nthe key articles.\nAs shown in Fig. 3, the TSM is composed of an encoder\nand a decoder, both of which employ the LSTM to extract\ninformation from their input, while the inputs of the encoder\nare the main bodies of articles and the inputs of the decoder\nare the titles. It should be noted that a transformer is able\nto be used in the encoder of the TSM, too. However,\nin regard to the decoder, the input must be masked in case the\nmodel obtains information from tokens whose index is larger\nthan yi.\nSimilar to BERT in an LMTM, the embedding layers of the\nTSM aim to transform the tokens of the input sequence into\nﬁxed-length embedding vectors with semantics. However,\nsince BERT is pretrained on a large dataset and has too many\nparameters, it is difﬁcult to change its vocabulary since it may\ncause the vocabulary to mismatch with the parameters. There-\nfore, when ﬁnetuning BERT, words that are not pretrained\nhave to be changed into a special token [unknown] and some\nsemantics are lost. In regard to the TSM, thanks to its light\nweight, the embedding distributions of unknown words can\nbe initialized randomly, and the models are easy to converge\nafter training for several epochs.\nBiLSTM aims to extract both semantics and syntaxes.\nAs mentioned earlier, a simple fully connected layer, which\nis used in NTM and NSEM, is not enough to extract high-\nlevel feature interactions. Additionally, an LSTM, a special\nRNN unit, is designed to model the long sequential depen-\ndency of words and produce the semantic information of\neach word or the whole sequence. Formally, given a word\nembedding sequence x = {x0,..., xl},x ∈ Rl×300, the\noutput of each ei is computed as hi =LSTM(hi−1,xi), where\nl is the length of the input sequence, and 300 is the ﬁxed\nlength of the embedding vectors. Thus, hi has the summarized\nlocal contextual semantics of xi, and the same words with\ndifferent contexts may have different hidden representations.\nSince the semantic of a word depends on not only the previous\nwords but also the following words, we use BiLSTM, which\nprocesses the sequence bidirectionally in the encoder. The\nlength of the LSTM unit is set to 300, and the output of\nthe BiLSTM (return sequences) is a matrix, denoted as h =\n{h0,..., hn},h ∈ Rn×600. h is supposed to contain more\nprecise semantics than x. For example, the embedding vector\nof the word apple is composed of the semantics of both a\ntype of fruit and a tech company. After the processing of the\nBiLSM, the semantic becomes more precise via high-level\nfeature interactions with other words. Here, we use 3 succes-\nsive BiLSTM layers. Because the model is not supposed to\nobtain information from y≥i when predicting yi, we use the\nLSTM rather than the BiLSTM in the decoder.\nThe multi-head attention layer aims to obtain the feature\ninteractions between the hidden matrixes generated from\nthe main bodies and titles with several attention layers. For\neach attention layer, the output is a matrix, denoted as a =\n{a0,···,at−1}. To make the model lighter, we choose a sim-\npler way of implementing attention layers rather than using a\nVOLUME 7, 2019 185513\nP. Yanget al.: LM-Driven Topic Clustering and Summarization for News Articles\nFIGURE 3. The TSM and its extension for topic clustering. The TSM is composed of an encoder and a decoder, where the encoder is used to extract\ninformation from input sequences, and the decoder is used to produce a summary. When training, the input is the main body of a single news article,\nand the output is its title, which can be regarded as a summary. When predicting, the inputs are the key articles of a topic, and output is the topic’s\nsummary. The encoder can be used as a Language Model for the LMTM by using a BiLSTM to convert the hidden matrix to a fixed length vector.\ntransformer. It can be computed as follows:\nai =\n∑I\ni=1 score(hl,li)hl, (23)\nscore(hl,lt )=softmax\n((\nhT\nl W a +ba\n)\nli\n)\n, (24)\nwhere W a and ba are the weights of the attention layer, and ai\nwill be used to predict the distribution of the ( i +1)-th word\nof the titles. The output of the multi-head attention layer is\nthe concatenated outputs of all the attention layers.\nAfter the multi-head attention layer, the TSM employs a\nfully connected layer with the SoftMax activation function,\nwhich converts the hidden vector into probability distribu-\ntions of words.\nTo train the TSM on a large data set, we take Cross Entropy\nLoss as the loss function as follows:\nL0 =−1\nN\n∑N\nn=1\n∑M\nm=1 yn,m ×log ˆyn,m, (25)\nwhere N is the length of the input title, M is the size of\nthe vocabulary, ˆyn ={ˆyn,1,... ˆyn,m}is the distribution of the\nprediction of n-th word in the title generated with the model,\nand yn ={yn,1,... yn,M }is the onehot representation of the\nn-th word in the title; therefore, we have the following:\nyn,m ∈{0,1},\nM∑\nm=1\nyn,m =1, (26)\nSimilar to the LMTM, the TSM is trained with Adam to\nmake the loss function L1 reach its minimum value.\nCompared to BERT, which is not suitable for sequence\ngeneration, the TSM is able to perform topic summariza-\ntion, which makes topics easy to understand, and it employs\na BiLSTM or a transformer to extract contextual semantics,\nwhich has an advantage over ELMo, since ELMo only con-\ncatenates two unidirectional LSTMs.\nD. LMTM-TSM\nCompared to shallow neural models, deep models are difﬁcult\nto converge and tend to overﬁt the dataset. Pretrained LMs\nmake it possible to train an LMTM on a small dataset, but\noverﬁtting remains a problem. Since the LMTM and the TSM\nare two models that share some layers, they are able to be\ntrained in parallel as a multi-task learning model to reduce\nthe impact of overﬁtting.\nTo be speciﬁc, for each turn of the training, a triple\n(M,P,N) is sampled from the dataset, as is done when\ntraining an LMTM. Then, ( M,P,N) is used to compute the\nloss function of the LMTM as L0. At the same time, a main\nbody M and its corresponding title P are used to compute\nthe loss function of the TSM. The total loss function of the\nLMTM-TSM is denoted as follows:\nL =αL1 +βL0, (27)\nwhere L1 and L0 are the loss functions of the LMTM and\nTSM, respectively. α,β are the hyper-parameters to deter-\nmine which part of the function takes a greater role. Since\nthe TSM is well pretrained, βis designed to be dynamic with\n185514 VOLUME 7, 2019\nP. Yanget al.: LM-Driven Topic Clustering and Summarization for News Articles\nAlgorithm 2Training Procedure for LMTM-TSM\nInput: main bodies MB, titles T , pretrained LM, α,β0\nOutput: the weight W of well-trained LMTM-TSM\n1: Initialize W with LM\n2: step =0\n3: repeat\n4: for(M,P) in ( MB, T ) do\n5: step =step +1\n6: Calculate the predicting title distribution title\n7: Calculate the loss function L1(P,title)\n8: repeat\n9: sample N from T\n10: until neg ̸=pos\n11: Calculate the topic distribution pmb\n12: Calculate the topic distribution ppos\n13: Calculate the topic distribution pneg\n14: Calculate the loss function L0(pmb,ppos,pneg)\n15: β=(1− 1\nestep ) ×β0\n16: L =αL0(pmb,ppos,pneg) +βL1(pos,title)\n17: Calculate the gradients ∇W L,lr with Adam\n18: W =W −∇W L ×lr\n19: end for\n20: until convergence\nthe training steps, as follows:\nβ=\n(\n1 − 1\nestep\n)\n×β0, (28)\nwhere β = 0 when step = 0 to make L1 determine the\nloss function and focus more on the LMTM. Additionally,\nβ → β0 as the step increases to protect the model from\noverﬁtting.\nV. EXPERIMENTS AND ANALYSIS\nA. SETUP\nTo evaluate the performance of the LMTM, we ﬁrst use the\nBytecup Dataset as the pretraining set to train the TSM.\nThe Byte Cup 2018 International Machine Learning Con-\ntest released a data set consisting of 1.3 million pieces of\nnews articles with titles and main bodies. Then, we collect\nnews articles with tags from the Internet and build Dataset I,\nwhich contains 5000 articles organized into 10 classes, and\nDataset II, which contains 12000 articles organized into\n40 classes. The statistics are shown in Table 1; it should be\nnoted that the two datasets share the same vocabulary.\nIn the experiments, NLTK is used for the lemmatiza-\ntion, the words that are not in the vocabulary are set to\ntoken [unknown], as mentioned earlier, and all the letters are\nchanged into lower-case. In addition, the max length of the\nmain bodies is set to 512 (including punctuation).\nWe chose the following methods as the baseline\napproaches:\n(a) LDA [4]: It is a classic topic model, which uses the bag-\nof-words assumption. Therefore, it lacks the semantic and\nsyntactic information to produce the topic distribution of a\ndocument, and LDA describes each topic via the distribution\nof all the words in the vocabulary.\n(b) NTM [9]: It is a topic model based on neural networks,\nin which it uses word embedding technology to extract the\nsemantic information from words and n-grams. However,\nthe syntactic information can barely remain in the n-grams,\nand the description of a topic produced by NTM is a set of\nkey n-grams.\n(c) senLDA [20]: It is an extension of the LDA that incor-\nporates the structure of the text in the generative and inference\nprocesses. It assumes a strong dependence of the latent topics\nbetween the words of sentences and always performs better\nthan the original LDA.\n(d) NSEM [1]: It is a neural model that employs\nnamed entity recognition technology to extract key elements.\nIt extracts semantic information with word embedding tech-\nnology but lacks the syntactic information, and the descrip-\ntion of a topic produced by NSEM is a set of key elements.\nFor LDA, we used the implementation of the genism toolkit\nand set parameter α=1/T . For NTM, we set n to 2 in the n-\ngram representation. For NSEM, we used the Stanford named\nentity recognizer to identify the named entities. The key\nelements are picked up by using TFIDF, where the number of\neach key element is no more than 3. Additionally, we do not\nuse the module detecting the dynamics of topics over time.\nFor LMTM-BERT, we ﬁxed the weights of all the embedding\nlayers and the ﬁrst ten transformers to prevent overﬁtting.\nAdditionally, for LMTM-TSM, we train the topic model and\nsummarization model together to avoid overﬁtting.\nFor all of the models, the number of topics is set to 10 for\nDataset I and 40 for Dataset II, which is equal to the number\nof classes. For the NTM, NSEM, and LMTM-TSM, we use\nthe same pretrained embedding matrix, which represents each\nword as a 300-dimensional vector. For LMTM-BERT, we use\nthe uncased BERT-base. All of the neural models are based\non TensorFlow and trained on a 1080Ti GPU.\nB. EXPERIMENT ON TOPIC CLUSTERING\nSince the articles in Dataset I and Dataset II were labeled\npreviously, we use Jaccard Coefﬁcient (JC), Fowlkes and\nMallows Index (FMI), precision (PRE), recall (REC) and\nF1 to evaluate the performances of the different models on\ntopic clustering [31]. All of the metrics can be denoted as\nfollows:\nJC = TP\nTP +FP +FN , (29)\nFMI =\n√\nTP\nTP +FP× TP\nTP +FN , (30)\nPRE = TP\nTP +FP, (31)\nREC = TP\nTP +FN , (32)\nF1 =2 ×PRE ×REC\nPRE +REC , (33)\nVOLUME 7, 2019 185515\nP. Yanget al.: LM-Driven Topic Clustering and Summarization for News Articles\nTABLE 3. Statistics of the datasets.\nTABLE 4. Performance comparison on Dataset I.\nwhere\nTP =\n⏐⏐{\n(i,j)|predicti =predictj and label i =labelj\n}⏐⏐,\n(34)\nFP =\n⏐⏐{\n(i,j)|predicti =predictj and label i ̸=labelj\n}⏐⏐,\n(35)\nFN =\n⏐⏐{\n(i,j)|predicti̸=predictj and label i =labelj\n}⏐⏐,\n(36)\nTN =\n⏐⏐{\n(i,j)|predicti̸=predictj and label i ̸=labelj\n}⏐⏐.\n(37)\nAs shown in Tables 3 and 4, LMTM-BERT achieves the\nbest performance on both datasets in most of the metrics.\nOn Dataset I, the results of LMTM-BERT are 0.366 in JC,\n0.540 in FMI, 0.478 in precision, 0.609 in recall, and\n0.536 in F1, all of which are the best compared to the other\nmodels. On Dataset II, though the LDA reaches 0.401 in\nrecall, the other metrics for the LDA are much worse than\nthose of the other approaches. After examining the results of\nthe LDA manually, we ﬁnd that the ﬁnal distribution of the\nLDA is extremely unbalanced, which leads to a high recall\nand very low precision. Except for the recall, LMTM-BERT\nperforms the best among the approaches, while LMTM-TSM\nis the second best.\nC. EXPERIMENT ON LOSS FUNCTION\nTo verify the effect of margin loss function, we train\nLMTM-BERT on Dataset I with Lpw and Lm by using the\nFIGURE 4. Comparison between different loss functions.\nFIGURE 5. Comparison between different similarity functions.\nsame SGD optimizer. Lpw is the loss function used in the\nNTM and NSEM, which only considers the relative distance\nbetween sim (tm,tp) and sim (tm,tn), while Lm, which is\nproposed to use in this paper, considers the absolute values\nof the two similarities. As show in Fig. 4, Lm has a better\nperformance compared to Lpw in JC, FMI, and F1.\nThe loss function of LMTM is based on a similarity func-\ntion. To verify the effect of MTS, the similarity function pro-\nposed in this paper, we train LMTM-BERT on Dataset I with\ndifferent similarity functions, including the cosine similarity\n(CS), Euclidean Distance (ED), KL, and MTS, to compare\nthe metrics of the corresponding results.\nCS (cosine similarity) is used in the NTM to measure the\ndifferences between two topic distributions, which can be\ndenoted as follows:\nCS (p0,p1)= p0 ·p1\n|p0|×|p1|. (38)\nED (Euclidean Distance) is a general function to measure\nthe distance between two points and has been widely used in\nmachine learning. It can be denoted as follows:\nED (p0,p1)=\n√∑\n(pi\n0 −pi\n1)2. (39)\nIt can be observed from Fig. 5 that though KL achieves the\nbest performance in precision of 0.512, MTS has the highest\n185516 VOLUME 7, 2019\nP. Yanget al.: LM-Driven Topic Clustering and Summarization for News Articles\nFIGURE 6. LMTM-TSM loss vs epoch.\nFIGURE 7. LMTM-BERT and NSEM loss vs epoch.\nscores in all other metrics, with 0.366 for JC, 0.540 for FMI,\n0.609 for recall, and 0.536 for F1.\nD. EXPERIMENT ON LM\nTo verify the effect of pretraining Language Models, we con-\nduct an experiment to compare the loss versus epochs curve\nof the NSEM and two LMTMs, in which one uses well\npretrained weights whereas the other one does not. Note that\nthe loss of the LMTM-TSM shown in the ﬁgure only includes\nL1, the loss of the LMTM, but excludes L0, the loss of the\nTSM, though it is trained with L =αL1 +βL0.\nFrom Fig. 6, it can be observed that, thanks to the pre-\ntrained deep layers, LMTM-BERT is able to extract the\nsemantic and syntax information from documents, which\ncauses it to converge faster than NSEM and reach a lower\nﬁnal loss, which leads to a better performance for the metrics\nmentioned earlier.\nAs shown in Fig. 7, with deep layers, the LMTM-TSM is\nable to extract the semantic and syntax information, also, and\nit achieves a similar ﬁnal loss as LMTM-BERT. However,\nthe LMTM-TSM without pretrained weights does not tend\nto converge though 45-epochs of training. Thus, we conclude\nthat, since deep models have a ﬁtting ability that is too strong,\nwith the complex structure and numerous weight parameters,\nconverging on a small data set is difﬁcult to obtain. LMTM\nis able to alleviate the problem with pretraining weights and\nmulti-task learning technology.\nFIGURE 8. Similarity scores of the same words in different sentences.\nTABLE 5. Performance comparison on Dataset II.\nTo investigate the quality of the dynamic word represen-\ntation produced by the TSM, we conduct an experiment on\nseveral sentences to calculate the distributions of the same\nword with diverse meanings. As shown in Fig. 8, we compute\nthe cosine similarity between four distributions of the word\napple in four sentences, A, B, C, and D, where in A and B,\napple is a technology company, while it is a type of fruit in C\nand D. It can be observed that the distributions of the ﬁrst two\nwords are similar and the last two are similar, which means\nthat the model is able to distinguish the differences according\nto the contextual information..\nE. EXPERIMENT FOR TOPIC SUMMARIZATION\nCompared to other models, one of the advantages of our\napproach is that the LMTM-TSM is able to produce under-\nstandable descriptions of topics. To evaluate the capability\nof the TSM in generating a topic summary, we change the\nnumber of topics ( K) to 100 to obtain topics of different\ngranularity and produce the summary of each topic by using\nthe beam search algorithm, where the beam size is set to 3 and\nthe max length of a sentence is set to 50. Then, we compare\nthe results with those of the LDA and NSEM.\nWe manually examine all the topic descriptions generated\nby the models. As the examples shown in Table 5, it can be\nobserved that the topic summaries of the TSM are easy to\nunderstand compared to the descriptions as grouped named\nentities produced by NSEM and the separate words produced\nby LDA. More details of the summaries and relevant titles are\nshown in Table 6, where the descriptions shown in the ﬁrst\nVOLUME 7, 2019 185517\nP. Yanget al.: LM-Driven Topic Clustering and Summarization for News Articles\nTABLE 6. The descriptions of 2 topics generated by three models.\nTABLE 7. 3 topics’ most relevant titles and summaries generated by.\ncolumn are produced manually, three titles of the documents\nmost related to the topic are shown in the second column, and\nthe summaries produced by the TSM are shown in the third\ncolumn. From Table 5 and Table 6, we have the following\nobservations:\n• For each topic, the summary generated with TSM is\nalmost a natural sentence with few grammatical mis-\ntakes, for example, the summary of Topic 3 lacks a\npredicate verb.\n• For topics that contain many connected events, the sum-\nmaries are less reasonable. It seems that the TSM pro-\nduces a summary with several key words that it ﬁnds in\nthe topic. However, for topics composed of one or few\nevents, such as Topics 0 and 1, the summaries are rea-\nsonable enough.\nVI. CONCLUSION\nIn this paper, we propose a novel deep neural model, i.e.,\nLMTM, for topic clustering and summarization. Based on\na well-trained Language Model, the LMTM-BERT shows\nexcellent results for different metrics for topic clustering.\nAlthough the performance of the LMTM-TSM is slightly\nworse, it is much more ﬂexible and can generate a brief\nsummary for each topic to make it easy to understand.\nIn future work, we will focus on the following:\n• How to design a more efﬁcient Topic Summarization\nModel that is able to extract the semantics of the docu-\nment with the encoder and produce a summary with the\ndecoder.\n• How to design topic models for other text documents\nthat are not supposed to be divided into titles and main\nbodies sharing similar topic distributions.\nREFERENCES\n[1] D. Zhou, L. Guo, and Y . He, ‘‘Neural storyline extraction model for sto-\nryline generation from news articles,’’ in Proc. 56th Annu. Meeting Assoc.\nComput. Linguistics, New Orleans, LA, USA, 2018, pp. 1727–1736.\n[2] Z. Li, J. Tang, X. Wang, J. Liu, and H. Lu, ‘‘Multimedia news summariza-\ntion in search,’’ ACM Trans. Intell. Syst. Technol. , vol. 7, no. 3, pp. 1–20,\nApr. 2016.\n[3] T. Hofmann, ‘‘Probabilistic latent semantic indexing,’’ in Proc. 22th Annu.\nInt. ACM SIGIR Conf. Res. Develop. Inf. Retr. , Berkeley, CA, USA, 1999,\npp. 50–57.\n[4] D. M. Blei, A. Y . Ng, and M. I. Jordan, ‘‘Latent Dirichlet allocation,’’\nJ. Mach. Learn. Res. , vol. 3, pp. 993–1022, Mar. 2003.\n[5] H. Jelodar, Y . Wang, C. Yuan, X. Feng, X. Jiang, Y . Li, and L. Zhao, ‘‘Latent\nDirichlet allocation (LDA) and topic modeling: Models, applications, a\nsurvey,’’Multimedia Tools Appl. , vol. 78, pp. 15169–15211, Jun. 2018.\n[6] F. Tian, B. Gao, D. He, and T.-Y . Liu, ‘‘Sentence level recurrent topic\nmodel: Letting topics speak for themselves,’’ 2016, arXiv:1604.02038.\n[Online]. Available: https://arxiv.org/abs/1604.02038\n[7] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[8] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, ‘‘Empirical evalua-\ntion of gated recurrent neural networks on sequence modeling,’’ 2014,\narXiv:1412.3555. [Online]. Available: https://arxiv.org/abs/1412.3555\n[9] Z. Cao, S. Li, Y . Liu, W. Li, and H. Ji, ‘‘A novel neural topic model\nand its supervised extension,’’ in Proc. AAAI, Palo Alto, CA, USA, 2015,\npp. 2210–2216.\n[10] M. Yang, T. Cui, and W. Tu, ‘‘Ordering-sensitive and semantic-aware topic\nmodeling,’’ in Proc. AAAI, Palo Alto, CA, USA, 2015, pp. 2353–2360.\n[11] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, ‘‘Deep contextualized word representations,’’ 2018,\narXiv:1802.05365. [Online]. Available: https://arxiv.xilesou.top/abs/1802.\n05365\n185518 VOLUME 7, 2019\nP. Yanget al.: LM-Driven Topic Clustering and Summarization for News Articles\n[12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-\ntraining of deep bidirectional transformers for language understanding,’’\n2018, arXiv:1810.04805. [Online]. Available: https://arxiv.xilesou.top/abs/\n1810.04805\n[13] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le,\n‘‘XLNet: Generalized autoregressive pretraining for language understand-\ning,’’ 2019, arXiv:1906.08237. [Online]. Available: https://arxiv.xilesou.\ntop/abs/1906.08237\n[14] I. Sutskever, O. Vinyals, and Q. V . Le, ‘‘Sequence to sequence learning with\nneural networks, in Advances in neural information processing systems,’’\nin Proc. 27th Int. Conf. Neural Inf. Process. Syst. , Montreal, QC, Canada,\n2014, pp. 3104–3112.\n[15] A. M. Rush, S. Chopra, and J. Weston, ‘‘A neural attention model for\nabstractive sentence summarization,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process. (EMNLP) , Lisbon, Portugal, 2015, pp. 379–389.\n[16] D. M. Blei, T. L. Grifﬁths, M. I. Jordan, and J. B. Tenenbaum, ‘‘Hierar-\nchical topic models and the nested chinese restaurant process,’’ in Proc.\n16th Int. Conf. Neural Inf. Process. Syst. , Vancouver, BC, Canada, 2004,\npp. 17–24.\n[17] C. Wang, D. Blei, and F. F. Li, ‘‘Simultaneous image classiﬁcation and\nannotation,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , Miami,\nFL, USA, Jun. 2009, pp. 1903–1910.\n[18] D. Ramage, D. Hall, R. Nallapati, and C. D. Manning, ‘‘Labeled LDA:\nA supervised topic model for credit attribution in multi-labeled corpora,’’\nin Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP) ,\nSingapore, 2009, pp. 248–256.\n[19] G. Balikas, H. Amoualian, M. Clausel, E. Gaussier, and M. R. Amini,\n‘‘Modeling topic dependencies in semantically coherent text spans with\ncopulas,’’ in Proc. 26th Annu. Meeting Assoc. Comput. Linguistics , Osaka,\nJapan, 2016, pp. 1767–1776.\n[20] G. Balikas, M.-R. Amini, and M. Clausel, ‘‘On a topic model for sen-\ntences,’’ inProc. 39th Annu. Int. ACM SIGIR Conf. Res. Develop. Inf. Retr. ,\nNew York, NY , USA, 2016, pp. 921–924.\n[21] G. E. Hinton and R. R. Salakhutdinov, ‘‘Replicated SoftMax: An undi-\nrected topic model,’’ in Proc. Adv. Neural. Inf. Process. Syst. , Vancouver,\nBC, Canada, 2009, pp. 1607–1614.\n[22] H. Larochelle and S. Lauly, ‘‘A neural autoregressive topic model,’’\nin Proc. Adv. Neural. Inf. Process. Syst. , Reno, NV , USA, 2012,\npp. 2708–2716.\n[23] G. Xun, Y . Li, W. X. Zhao, J. Gao, and A. Zhang, ‘‘A correlated topic model\nusing word embeddings,’’ in Proc. IJCAI , Melbourne, VIC, Australia,\n2017, pp. 4207–4213.\n[24] C. Li, Y . Lu, J. Wu, Y . Zhang, and Z. Xia, ‘‘LDA meets Word2Vec: A novel\nmodel for academic abstract clustering,’’ in Proc. Companion Web Conf. ,\nLyon, France, 2018, pp. 1699–1706.\n[25] S. Bunk and R. Krestel, ‘‘WELDA: Enhancing topic models by incorpo-\nrating LocalWord context,’’ in Proc. 18th ACM/IEEE Joint Conf. Digit.\nLibraries, Fort Worth, TX, USA, Jun. 2018, pp. 293–302.\n[26] N. Kalchbrenner, L. Espeholt, K. Simonyan, A. D. V . Oord, and A. Graves,\n‘‘Neural machine translation in linear time,’’ 2017, arXiv:1610.10099.\n[Online]. Available: https://arxiv.org/abs/1610.10099\n[27] G. Klein, Y . Kim, Y . Deng, J. Senellart, and A. Rush, ‘‘OpenNMT: Open-\nsource toolkit for neural machine translation,’’ in Proc. 55th Annu. Meeting\nAssoc. Comput. Linguistics , Vancouver, BC, Canada, 2017, pp. 67–72.\n[28] S. Chopra, M. Auli, and A. M. Rush, ‘‘Abstractive sentence summarization\nwith attentive recurrent neural networks,’’ in Proc. 54th Annu. Meeting\nAssoc. Comput. Linguistics , San Diego, CA, USA, 2016, pp. 93–98.\n[29] A. Celikyilmaz, A. Bosselut, X. He, and Y . Choi, ‘‘Deep communicating\nagents for abstractive summarization,’’ in Proc. 56th Annu. Meeting Assoc.\nComput. Linguistics, New Orleans, LA, USA, 2018, pp. 1662–1675.\n[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ 2017,\narXiv:1706.03762. [Online]. Available: https://arxiv.org/abs/1706.03762\n[31] M. Alhawarat and M. Hegazi, ‘‘Revisiting K-means and topic modeling,\na comparison study to cluster arabic documents,’’ IEEE Access , vol. 6,\npp. 42740–42749, 2018.\nPENG YANG received the Ph.D. degree from\nSoutheast University, in 2006. He worked as a\nResearch Scientist with CERN, where he has\nparticipated in the alpha magnetic spectrometer\n(AMS) experiment, from 2007 to 2009, which is\nled by Nobel Laureate P. S. Ting. He is currently an\nAssociate Professor with the School of Computer\nScience and Engineering, Southeast University,\nwhere he is also the Deputy Director of the Future\nNetwork Research Center. His research interests\nfocus on new-generation Internet architecture, natural language processing,\nand Cyber content governance. He is also a member of the National Technical\nCommittee of Standardization Administration of China.\nWENHAN LI received the B.S. degree from\nSoutheast University, Nanjing, China, in 2017,\nwhere he is currently pursuing the M.S. degree\nwith the School of Computer Science and\nEngineering. His research interests include natural\nlanguage processing, cloud computing, and new-\ngeneration Internet architecture.\nGUANGZHEN ZHAOreceived the B.S. and M.S.\ndegrees from the School of Information Science\nand Technology, Shijiazhuang Tiedao University,\nChina, in 2015 and 2018, respectively. He is cur-\nrently pursuing the Ph.D. degree with the School\nof Computer Science and Engineering, Southeast\nUniversity. His current research interests include\nnetwork security, situation awareness, and new-\ngeneration Internet architecture.\nVOLUME 7, 2019 185519",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9109686613082886
    },
    {
      "name": "Automatic summarization",
      "score": 0.8362630605697632
    },
    {
      "name": "Cluster analysis",
      "score": 0.6772997379302979
    },
    {
      "name": "Topic model",
      "score": 0.6116755604743958
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5783893465995789
    },
    {
      "name": "Natural language processing",
      "score": 0.5710150003433228
    },
    {
      "name": "Language model",
      "score": 0.5657697319984436
    },
    {
      "name": "Task (project management)",
      "score": 0.4801197052001953
    },
    {
      "name": "Information retrieval",
      "score": 0.46147748827934265
    },
    {
      "name": "Exploit",
      "score": 0.4203302562236786
    },
    {
      "name": "Document clustering",
      "score": 0.4175994396209717
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76569877",
      "name": "Southeast University",
      "country": "CN"
    }
  ],
  "cited_by": 23
}