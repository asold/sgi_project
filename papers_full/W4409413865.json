{
  "title": "Revisiting the Role of Review Articles in the Age of AI-Agents: Integrating AI-Reasoning and AI-Synthesis Reshaping the Future of Scientific Publishing",
  "url": "https://openalex.org/W4409413865",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2588909907",
      "name": "Andrej Thurzo",
      "affiliations": [
        "Comenius University Bratislava"
      ]
    },
    {
      "id": "https://openalex.org/A2105161632",
      "name": "Ivan Varga",
      "affiliations": [
        "Comenius University Bratislava"
      ]
    },
    {
      "id": "https://openalex.org/A2588909907",
      "name": "Andrej Thurzo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105161632",
      "name": "Ivan Varga",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4402215122",
    "https://openalex.org/W2753066651",
    "https://openalex.org/W4406477863",
    "https://openalex.org/W4407030672",
    "https://openalex.org/W4407251675",
    "https://openalex.org/W4407219458",
    "https://openalex.org/W3087184120",
    "https://openalex.org/W2314164079",
    "https://openalex.org/W4283828465",
    "https://openalex.org/W4386258176",
    "https://openalex.org/W4402533807",
    "https://openalex.org/W4214755322",
    "https://openalex.org/W4407167265",
    "https://openalex.org/W4407030543",
    "https://openalex.org/W4405356889",
    "https://openalex.org/W4403978186",
    "https://openalex.org/W4406178148",
    "https://openalex.org/W4377138204",
    "https://openalex.org/W4391542643",
    "https://openalex.org/W4407199411",
    "https://openalex.org/W4401806632",
    "https://openalex.org/W4392504765",
    "https://openalex.org/W4390904004",
    "https://openalex.org/W4383554209",
    "https://openalex.org/W4400585815",
    "https://openalex.org/W4399207674",
    "https://openalex.org/W4366828769",
    "https://openalex.org/W4387394602",
    "https://openalex.org/W4390616109",
    "https://openalex.org/W4392982409",
    "https://openalex.org/W4392980716",
    "https://openalex.org/W4400118784",
    "https://openalex.org/W4402235658",
    "https://openalex.org/W4407305524",
    "https://openalex.org/W4405076710",
    "https://openalex.org/W4392589131",
    "https://openalex.org/W4404264507",
    "https://openalex.org/W4406872440",
    "https://openalex.org/W4405820263",
    "https://openalex.org/W4401834794",
    "https://openalex.org/W4401452457",
    "https://openalex.org/W4407414926",
    "https://openalex.org/W4386541335",
    "https://openalex.org/W3083849603",
    "https://openalex.org/W4310603931",
    "https://openalex.org/W4407106046",
    "https://openalex.org/W4399262091"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)\nBratislava Medical Journal (2025) 126:381–393 \nhttps://doi.org/10.1007/s44411-025-00106-8\nEDITORIAL\nRevisiting the Role of Review Articles in the Age of AI‑Agents: \nIntegrating AI‑Reasoning and AI‑Synthesis Reshaping the Future \nof Scientific Publishing\nAndrej Thurzo1 · Ivan Varga2\nPublished online: 14 April 2025 \n© The Author(s) 2025\n1 Introduction\n1.1  Scientific Reviews and Their Importance \nin Consolidating Knowledge\nScientific publishing now appears to be divided into two dis-\ntinct eras: pre-LLM and post-LLM (Large Language Models \nof generative artificial intelligence). Even before the advent \nof large language models, the academic community was \nalready grappling with a surging tide of research papers. \nOver the past decade, the sheer volume of publications has \nexploded—by 2022, databases like Scopus and Web of Sci-\nence recorded roughly a 47% increase in articles compared \nto 2016 [1 ]. Researchers were increasingly overwhelmed, \nstruggling to keep pace with the constant influx of new \nfindings [1–3].\nThe era of generative AI (artificial intelligence) has \ntransformed the landscape with unprecedented AI-assisted \nwriting capabilities [4 ]. This technological breakthrough \nnot only enhances the way research is produced but also \nredefines the challenges faced by the scientific community. \nToday, the introduction of \"deep-research\" tools by innova-\ntors such as OpenAI, Google, Perplexity, and Grok3-Deep-\nSearch—equipped with advanced chain-of-thought reason-\ning—signals the next evolutionary leap [4–7].\nIn the “pre-AI era”, based on the sources and available \nbibliometric studies from the years prior to introduction of \nLLM (2016–2022), review articles make up roughly 20–25% \nof all scientific publications [1, 8–10]. Recent analysis by the \nEuropean research community [1, 8] indicates that approxi-\nmately one in every five published papers was a review arti-\ncle. See, for example, the analysis on OpenAlex data [1 ], \nwhich—while primarily focused on publication trends—also \nnotes that review articles typically account for about 20–25% \nof the published literature of the pre-LLM era. Figure  1 \nshows a pie chart representing breakdown of scientific pub-\nlications into review articles versus original research articles \nin that era, using an approximate estimate of 23% reviews \nand 77% original research [11, 12].\nReview articles have long been the backbone of scien-\ntific scholarships, providing comprehensive overviews and \ncritical analyses of evolving research landscapes [13]. This \nconsolidation helps strengthen the foundation of knowl-\nedge in a field. Reviews can highlight key gaps and chal-\nlenges to address in future research [14]. This helps guide \nfuture research efforts and advances in the field. Reviews, \nespecially systematic reviews, help establish a link between \nresearch findings and decision-making in patient care [13].\nPreviously there have been noticed several notable trends \nregarding the share of review articles in overall scientific \nmedical publications:\n1. Increasing prevalence: The proportion of review arti-\ncles has been steadily increasing across various scien-\ntific fields. For example, in dentistry, systematic reviews \nincreased from 5.8 to 53.3% of publications between \n2000 and 2015 [3].\n2. Field-specific variations: The trend varies across differ-\nent scientific disciplines. For example, in neurosurgery, \ntopic reviews increased from < 1% before 1980 to 3–4% \nsince 2010 [15].\nAndrej Thurzo and Ivan Varga: Members of the Editorial Board of \nthe Bratislava Medical Journal.\n * Andrej Thurzo \n Thurzo3@uniba.sk\n1 Department of Orthodontics, Forensic and Regenerative \nDentistry, Faculty of Medicine, Comenius University \nin Bratislava, Bratislava, Slovakia\n2 Faculty of Medicine, Institute of Histology and Embryology, \nComenius University in Bratislava, Bratislava, Slovakia\n382 Bratislava Medical Journal (2025) 126:381–393\n3. Impact on journal metrics: Review articles often con-\ntribute significantly to journal impact factors.\n4. Geographical differences: The production of review \narticles varies by country and region. For example, \nin South Korea, open-access publications, including \nreviews, increased to account for almost 40% of all \npublications in the last decade [11]. While Asia leads \nin total AI-related life science publications, Northern \nAmerica and Europe contribute most of the AI research \nappearing in high-ranking outlets, generating up to 50% \nmore citations than other regions [16].\n5. Shift towards higher-quality reviews: There's a trend \ntowards more systematic and meta-analytic reviews. \nFor example, in general surgery research in Australia, \nthere was a significant increase in systematic reviews \nand meta-analyses from 2000 to 2020 [17].\nGlobal scientific publications grew at 5.6% annually from \n2016 to 2022, driven by China (42% of new articles) and \nIndia (11%) [8]. This surge has strained peer-review systems, \nwith publishers like MDPI and Hindawi reducing median \nreview times to under 30 days [8]. Such acceleration favors \noriginal research submissions over methodologically inten-\nsive reviews, creating a \"volume bias\" that depresses review \narticle percentages.\n1.2  The Emergence of AI‑Powered Deep Research \nTools\nAI is revolutionizing research and teaching methodologies \nin academia [4 , 7]. Study of Adebakin, 2025 discusses \nthe implications of AI-generated literature reviews and \nthe ethical concerns of citation accuracy [6 ]. The study \nof Oyinloye and Acetise from 2025 explores how AI-gen-\nerated literature reviews can enhance academic synthesis \nby clustering related papers and automating citation net-\nworks. The research highlights concern about the quality \nand contextual accuracy of AI-driven literature aggrega-\ntion [2 ]. AI-driven review automation and its impact on \nresearch depth and validity is assessed by Aggarwal and \nKarwasra (2025). Their study contrasts traditional human-\nled literature reviews with AI-generated synthesis, rais-\ning concerns about over-reliance on automated reports \nquality [18].\n\"Deep research\" AI tools distinguish themselves from ear-\nlier AI models by their advanced ability to synthesize vast, \nreal-time data across diverse sources, producing nuanced \ninsights rather than just retrieving or generating text. Unlike \npredecessors like ChatGPT, which primarily excel at lan -\nguage generation and basic question-answering, these tools \nintegrate reasoning transparency, adaptability to specialized \ndomains, and continuous learning, enabling them to tackle \ncomplex research tasks with greater depth. For instance, they \ncan analyze scientific literature, identify emerging trends, \nand even suggest novel hypotheses, making them transform-\native for fields like academic publishing where comprehen-\nsive, up-to-date synthesis is critical. This leap in capability \npositions them as active collaborators in knowledge creation, \nfar beyond the assistive role of earlier models.\nWe don't yet have all the data to fully understand how the \nnew wave of large language models will influence review \nanalysis. Before being able to assess this, even more novel \ntool is already on the scene—an advanced AI that dives \ndeep into specific topics, a process many are calling “deep \nresearch.” Its ultimate impact on scientific publications, \nespecially review articles, remains to be seen. This paper \nasks three key questions:\n1. How are AI-driven deep research tools transforming \nthe literature review process? We investigate whether \ninnovations like “deep research” can simplify, update, \nand even improve upon the traditional, time-consuming \nmethods of gathering and synthesizing scholarly work.\n2. What are the strengths and limitations of AI-gen-\nerated reviews compared to human-authored ones?  \nThis question examines factors such as citation accuracy, \ncontextual understanding, and the ability to pinpoint \nresearch gaps. It also considers issues like AI “halluci-\nnations” versus the nuanced insights provided by expert \nresearchers.\nFig. 1  Breakdown of scientific \npublications into review articles \nversus original research articles \nin the pre-generative-AI era \n(2016–2022)\n\n383Bratislava Medical Journal (2025) 126:381–393 \n3. Can a hybrid model that combines AI efficiency with \nhuman oversight boost the quality and timeliness of \nreview articles? We explore whether integrating deep \nresearch tools with expert curation could lead to a new \nkind of dynamic, continuously updated review—one \nthat compensates for the shortcomings of each approach \nwhen used alone.\nWhat is the new role of review articles in the current \nera of AI-powered deep research tools that influence the \nstructure and synthesis of academic review papers? Some \nstudies suggest that while AI can speed up literature analy -\nsis, it may lack the depth of human-led synthesis [10, 19]. \nNow AI-driven deep research methodologies are being \nintegrated into professional and academic research. Some \nstudies note a shift in scholarly communication due to AI-\nassisted reviews [9 , 20]. Hartung et al. (2025) discusses \nAI-powered systematic reviews and their implications for \nacademic integrity. Their paper highlights AI’s ability to \nextract key trends from thousands of papers but also warns \nagainst algorithmic biases [21]. All this research confirms \nthat while AI can accelerate the synthesis process, human \noversight remains crucial for context, accuracy, and critical \ninterpretation [22].\n1.3  Thesis Statement: Exploring Whether \nthe Traditional Review Article is Becoming \nObsolete or Evolving into a Hybrid form \nEnhanced by AI\nThis paper has these two scientific aims:\n1. Transformational aim: To investigate the potential \nimpact of AI-driven synthesis on the future role of \nreview articles, evaluating whether these tools may ren-\nder conventional review writing obsolete or lead to a \nnew, hybrid paradigm.\n2. Integrative aim: To propose and validate a hybrid \nframework that leverages the rapid data aggregation \nand analytical capabilities of AI while preserving the \ncritical, contextual insights of human experts, thereby \nenhancing the overall quality and currency of scientific \nliterature reviews.\n2  The Emergence of “Deep Research”\n2.1  Overview of OpenAI’s Deep Research Tool\nThe emergence of \"Deep Research\" artificial intelligence \nsystems represents a paradigm shift in how scientific \nknowledge is synthesized and disseminated. OpenAI's \ndeep research, Google Gemini Pro with deep research, and \nPerplexityAI's deep research have introduced unprecedented \ncapabilities for autonomous literature analysis, data synthe-\nsis, and report generation. These systems demonstrate accu-\nracy ranging from 21.1 to 26.6% on expert-level benchmarks \nlike Humanity's Last Exam [23], outperforming previous \nmodels by significant margins. Their ability to process multi-\nmodal inputs, dynamically adjust research strategies, and \ngenerate structured outputs with citations is already altering \nacademic workflows.\n2.2  Comparative Analysis of Deep Research \nArchitectures\n2.2.1  OpenAI Deep Research: Dynamic Reasoning \nParadigm\nPowered by the specialized o3-mini-high model [23], Ope-\nnAI's system employs reinforcement learning from human \nfeedback (RLHF) [24] to optimize multi-step research tra-\njectories. Architecture dynamically adjusts its search strat-\negy based on real-time findings, enabling what developers \ndescribe as \"recursive hypothesis refinement\" [25]. This \napproach achieved state-of-the-art performance on the GAIA \nbenchmark (67.36% accuracy) through:\n1. Multi-source verification loops that cross-check find-\nings across 5–8 authoritative sources before finalizing \nconclusions [25].\n2. Contextual citation mapping that links specific claims \nto originating passages in source material [23, 26].\n3. Python-integrated analytics for statistical validation of \ntrends in research data [23, 26].\nIn controlled trials, the system reduced literature review \ntime for meta-analyses by 72% compared to human research-\ners while maintaining 91% consensus with expert evalua-\ntions [23]. However, limitations persist in handling contra-\ndictory evidence from equal-authority sources, occasionally \ndefaulting to frequency-based conclusions rather than meth-\nodological rigor [23, 26].\n2.2.2  Google Gemini Pro Deep Research: Scale‑Optimized \nSynthesis\nGoogle's implementation combines  Mixture-of-Experts \n(MoE) architecture [ 27] with a 1-million-token context \nwindow [26], enabling simultaneous analysis of entire \nresearch corpora. Technical specifications reveal:\n• 512 expert pathways activated through gated linear units \n(GLUs) based on input topicality [26].\n• Multi-modal fusion layers integrating text, equations, \nand figures from 50 + file formats [23, 26].\n384 Bratislava Medical Journal (2025) 126:381–393\n• Automated confidence scoring those weights finding by \nsource reliability indices [26].\nThe system excels at longitudinal trend analysis, demon-\nstrated by its ability to trace concept evolution across 15,000 \npapers in the arXiv corpus within 23 min [26]. However, \nbenchmarking shows 18% higher factual inconsistency rates \ncompared to OpenAI in fast-moving fields like CRISPR \ntechnology [23, 26], suggesting scale-precision tradeoffs.\n2.2.3  PerplexityAI Deep Research: Accessibility‑Focused \nImplementation\nPositioned as the \"democratized\" alternative3 , Perplexi-\ntyAI's system combines proprietary neural retrieval with \nfederated language models. Key innovations include:\n• Distributed verification networks querying 14 fact-\nchecking APIs in parallel [28].\n• Dynamic abstraction layers simplifying complex find-\nings for interdisciplinary audiences [28].\n• Open collaboration features allowing real-time co-\nediting of AI-generated drafts [28].\nWhile achieving 21.1% accuracy on Humanity's Last \nExam3, 15% below OpenAI's benchmark, the platform pro-\ncesses 80% more daily queries through optimized GPU uti-\nlization [28]. Early adoption data shows particular traction \nin early-career researchers, reducing literature survey costs \nby 94% compared to traditional methods [28].\n2.2.4  xAI Grok 3 DeepSeach: Real‑Time Reasoning \nSynthesis\nDeveloped by xAI, Grok 3 DeepSearch integrates a large-\nscale AI model with real-time web and X search capa-\nbilities [29]. Built using 200,000 GPUs and featuring a \n1-million-token context window, the system employs rein-\nforcement learning to refine reasoning across multi-step \nresearch tasks [30]. Key features include:\n1. Dynamic synthesis of web and X data, adjusting con-\nclusions based on real-time feedback [29].\n2. Transparent reasoning via the \"Think\" feature, tracing \nlogic steps for user verification [31].\n3. High-capacity processing for complex queries in \nmath, science, and coding [ 30]. Benchmarks demon-\nstrate top performance, with an Elo score of 1402 in \nChatbot Arena and 93.3% accuracy on AIME 2025 [30]. \nIn trials, it reduced research synthesis time by 68% com-\npared to manual methods, with 89% alignment to expert \nreviews [27]. However, reliance on unfiltered web/X \ndata risks misinformation, and high computational \ndemands may limit scalability [31].\nCompared to OpenAI's dynamic reasoning paradigm, \nwhich uses RLHF and achieves 67.36% accuracy on the \nGAIA benchmark, Grok 3 DeepSearch's approach is more \nfocused on real-time synthesis, with superior benchmark \nscores. Google's scale-optimized synthesis, with a 1-mil-\nlion-token context window, shares similarities, but Grok 3's \nreasoning transparency is a notable differentiator. Perplexi-\ntyAI's accessibility focus contrasts with Grok 3's emphasis \non performance and depth, highlighting diverse strategies \nin AI research tools.\n2.3  Comparison, Benchmarks and Performance\nIn recent benchmarking tests—such as the Humanity’s Last \nExam and GAIA—OpenAI’s tool has demonstrated remark-\nable performance, outperforming some established systems \nby significant margins, albeit the Table  1 reveals the recent \nGrok 3's estimated performance that often surpasses all \nexisting AI tools, particularly in speed (20,000 pages per \nhour vs. Google Gemini Pro's 18,200) and citation precision \n(93% vs. OpenAI's 92.10%). This aligns with its design as a \nreasoning agent with real-time data access, potentially clos-\ning gaps with human baselines in cross-domain synthesis (75 \nvs. 71.20%) and contradiction detection (85 vs. 89.10%). The \nestimations relied on indirect benchmarks due to the absence \nof direct GAIA scores for Grok 3, highlighting the need for \nfuture testing on standardized benchmarks. The reliance on \nMMLU-pro for cross-domain synthesis and DeepSearch for \ncitation precision introduces potential inaccuracies, necessi-\ntating further validation. Table 1 shows performance bench-\nmarking across domains.\nTable 1  Comparison of Deep-research (OpenAI, Google, PerplexityAI) and Deep Search (xAI) AI tools performance to a human baseline\nMetric OpenAI deep research Google Gemini Pro PerplexityAI Grok 3 Human baseline\nCross-domain synthesis 67.36% (GAIA) [25] 59.12% [26] 54.80% [28] 75% [32, 33] 71.20% [21]\nCitation precision 92.10% [23] 85.30% [26] 88.70% [28] 93% [34, 35] 96.40% [21]\nContradiction detection 81.50% [26] 73.20% [26] 68.90% [28] 85% [36, 37] 89.10% [21]\nSpeed (pages per hour) 12,400 [25] 18,200 [26] 15,700 [28] 20,000 [34, 35, 38] 45 [21]\n385Bratislava Medical Journal (2025) 126:381–393 \nCross-domain synthesis: measured by the GAIA bench-\nmark for OpenAI deep research at 67.36%, tests the ability \nto synthesize information across different domains, requiring \nreasoning and tool-use proficiency. The human baseline is \n71.20%, while standard GPT-4 with plugins scored 15% on \nGAIA, indicating a discrepancy possibly due to different \nmodel variants. Grok 3's estimated 75% for cross-domain \nsynthesis is based on its MMLU-pro score of 79.9%, which \nexceeds the human baseline of 71.20%, suggesting strong \nperformance across diverse knowledge areas.\nCitation precision: with a human baseline of 96.40%, meas-\nures the accuracy of source citations. OpenAI deep research \nscores 92.10%, Google Gemini Pro 85.30%, and Perplexi-\ntyAI 88.70%. With a 93% estimate, Grok 3 leverages its \nDeepSearch feature for accurate source citation, slightly \noutperforming OpenAI deep research's 92.10%, reflecting \nits advanced information retrieval capabilities.\nContradiction detection: with a human baseline of 89.10%, \nassesses the ability to identify conflicting information. \nOpenAI deep research scores 81.50%, Google Gemini Pro \n73.20%, and PerplexityAI 68.90%. Estimated at 85%, Grok \n3's strong reasoning abilities suggest it can effectively detect \ncontradictions, positioning it between OpenAI's 81.50% and \nthe human baseline of 89.10%.\nProcessing speed: speed, measured in pages per hour, shows \nhuman baseline at 45, with OpenAI deep research at 12,400, \nGoogle Gemini Pro at 18,200, and PerplexityAI at 15,700. \nGrok 3's estimated speed of 20,000 pages per hour, surpass-\ning Google Gemini Pro's 18,200, highlights its training on \na powerful supercomputer with 200,000 GPUs, showcasing \nits computational efficiency.\nThe table reveals a convergence frontier where AI sys-\ntems now match junior researchers in citation accuracy and \nsurpass all human levels in processing speed [23]. How -\never, significant gaps remain in higher-order synthesis tasks \nrequiring creative conceptual integration [25, 26, 28].\n3  The Changing Landscape of Scientific \nLiterature Reviews\n3.1  Traditional Review Articles: Strengths \nand Limitations\nHuman-authored review articles are valued for their depth, \nnuance, and the expert judgment they bring to complex sub-\njects. Yet, they are not without drawbacks. The time-inten-\nsive process required to produce a comprehensive review \nmeans that such articles can quickly become outdated. \nAdditionally, the manual curation of literature may miss \nemerging trends that AI systems can detect more rapidly. \nThe rigorous, manually curated approach has its limitations \n(e.g., rapid obsolescence or labor-intensive updating). The \naverage time from research question formulation to literature \nreview completion has decreased from 42 days to 9 h for \nearly adopters [23]. This compression is enabling:\n1. Dynamic review articles updated in real-time as new \nstudies emerge, exemplified by the Living Systematic \nReview model adopted by Nature in 2024 [39].\n2. Automated gap analysis identifying under-researched \nareas with 83% precision compared to human \nexperts [25, 28].\n3. Plagiarism pattern detection at the conceptual level, \nreducing redundant publications by 37% in high-impact \njournals [23, 25].\nHowever, over 60% of journal editors report increased \nchallenges verifying AI-assisted submissions, with 23% \nimplementing mandatory \"algorithmic transparency\" \ndeclarations [23].\n3.2  AI‑Generated Reviews: Capabilities \nand Challenges\nAI-generated reviews can now accelerate literature synthe-\nsis, identify gaps, and update rapidly. AI tools like OpenAI’s \ndeep research offer a compelling alternative. By synthesizing \nlarge volumes of literature in a fraction of the time, these \ntools can produce dynamic and up-to-date review reports. \nEarly adopters have noted that the AI-generated reviews are \n“extremely impressive” and in some cases even surpass tra-\nditional reviews in clarity and readability. However, caution \nis warranted. The limitations inherent in current AI—such \nas the potential for citation errors and the occasional propa-\ngation of misinformation—indicate that human oversight \nremains crucial. The main risks are inaccuracies, halluci-\nnated data, citation errors, and distinguishing authoritative \nsources from noise.\n3.2.1  Challenges Faced by Deep Research Tools in Scientific \nPublication\nThe integration of artificial intelligence into scientific \nresearch has reached a pivotal juncture with tools like Ope-\nnAI’s \"deep research\" agent, which promises to revolution-\nize literature reviews and academic synthesis. While these \nsystems offer unprecedented speed and efficiency, they face \nsignificant challenges that undermine their reliability and \nadoption in scholarly contexts. Key issues include persistent \ninaccuracies in generated content, limitations in accessing \npaywalled research, difficulties in contextualizing complex \nscientific concepts, and ethical concerns about the role of \n386 Bratislava Medical Journal (2025) 126:381–393\nAI in knowledge creation. These challenges highlight the \ntension between technological innovation and the rigor -\nous standards required for credible scientific publication. A \nrecent study analysing MEDLINE-indexed abstracts found \nthat the prevalence of abstracts with a high probability \n(≥ 90%) of AI-generated text increased from 21.7 to 36.7% \nbetween 2020 and 2023 [40].\nHallucinations and factual errors One of the most pressing \nchallenges for AI-driven research tools is their propensity \nto generate plausible-sounding but factually incorrect infor-\nmation, a phenomenon known as \"hallucination.\" OpenAI’s \ndeep research tool, despite improvements over previous \nmodels, still produces reports that occasionally invent facts \nor misattribute claims. For instance, journalists testing the \ntool noted instances where it fabricated details about recent \nlegal rulings or scientific breakthroughs [41]. These errors \nstem from the probabilistic nature of large language models \n(LLMs), which generate text based on patterns in training \ndata rather than verified knowledge. In scientific publishing, \nwhere precision is paramount, such inaccuracies pose a criti-\ncal barrier to trust [39, 41, 42].\nCitation misattribution and source verification  Deep \nresearch tools struggle to consistently attribute information \nto correct sources. While they can generate citations, inter -\nnal evaluations by OpenAI acknowledge that the tool occa-\nsionally references non-existent papers or mislinks findings \nto authors [39, 42]. This issue is compounded when dealing \nwith interdisciplinary research, where nuanced distinctions \nbetween similar studies are easily overlooked by AI sys -\ntems. Kyle Kabasares, a data scientist at the Bay Area Envi-\nronmental Research Institute, observed that AI-generated \nreports often require extensive human revision to ensure \ncitation accuracy [42]. Furthermore, the tools cannot criti-\ncally assess the credibility of sources, potentially amplifying \nlow-quality or predatory journal content [43].\nAccess to comprehensive scientific literature: paywall limi-\ntations and subscription barriers A significant technical \nlimitation of current AI research tools is their inability to \naccess paywalled academic content, which constitutes a sub-\nstantial portion of high-impact scientific literature [39, 42]. \nOpenAI’s deep research agent, for example, relies on open-\naccess materials and publicly available databases, excluding \ncritical peer-reviewed studies behind subscription barriers. \nThis restriction skews the tool’s outputs toward less rigorous \nor outdated sources, compromising the comprehensiveness \nrequired for authoritative reviews [42]. Researchers have \nproposed integrating institutional credentials to bypass \npaywalls, but this raises ethical and logistical challenges \nrelated to copyright compliance and decentralized access \nmanagement [42].\nTemporal gaps in knowledge synthesis AI tools frequently \nlag in incorporating the latest research due to delays in \nindexing and processing newly published studies. Tests of \nthe deep research tool revealed its tendency to overlook \ndevelopments published within the preceding six months, \nparticularly in fast-moving fields like genomics or climate \nscience [43]. This limitation is inherent to the training \nand updating cycles of LLMs, which cannot dynamically \nassimilate real-time information without costly retraining. \nConsequently, AI-generated reviews risk omitting cutting-\nedge discoveries, rendering them inadequate for fields where \ntimeliness is crucial [39, 43].\nContextual understanding and intellectual depth: lack of \ndomain-specific expertise While AI excels at aggregating \ninformation, it fails to replicate the depth of understand-\ning that human experts bring to scientific synthesis. Mario \nKrenn of the Max Planck Institute emphasizes that genuine \nresearch involves not just summarizing existing knowledge \nbut advancing novel hypotheses through years of focused \ninquiry—a capability absents in current AI systems [42]. \nThe deep research tool’s reports, though structurally coher-\nent, often misinterpret technical jargon or misapply method-\nologies across disciplines [43]. For instance, in a synthesized \nreview of quantum computing applications, the tool con-\nflated theoretical frameworks from condensed matter physics \nwith unrelated engineering principles, requiring substantial \nhuman correction [43].\nInability to identify knowledge gaps critically A core func-\ntion of literature reviews is identifying underexplored \nresearch areas, yet AI tools struggle with this task. While \nOpenAI’s agent can flag topics with sparse publications, it \ncannot discern “meaningful” gaps that warrant investiga-\ntion. Human researchers contextualize gaps within theoreti-\ncal frameworks, funding landscapes, and societal needs—\na nuanced analysis beyond the reach of pattern-matching \nalgorithms [39, 42]. This limitation reduces the tool’s utility \nin guiding original research agendas, relegating it to a sup-\nplementary role in the exploratory phase [44].\n3.2.2  Emergent Quality Control Paradigms\nLeading publishers are deploying  counter-AI verification \nsystems to audit AI-generated reviews:\n• Semantic fingerprinting tracks argumentation patterns \nacross submissions [23, 26].\n• Citation network analysis detects synthetic reference \ngraphs lacking human reading patterns [25, 28].\n387Bratislava Medical Journal (2025) 126:381–393 \n• Adversarial prompting tests probe the depth of concep-\ntual understanding [23].\nThese measures aim to preserve intellectual rigor while \naccommodating AI's efficiency gains, but risk creating veri-\nfication bottlenecks that negate time savings.\n3.2.3  Case in Point: Reflections from the Field\nProminent voices in the scientific community are already \nweighing in. Derya Unutmaz, an immunologist, contends \nthat AI-generated reviews are “trustworthy” and might sig-\nnal the beginning of the end for conventional review writing. \nSimilarly, experts like Andrew White suggest that while AI \ntools offer speed and breadth, they must be rigorously evalu-\nated to ensure that their outputs meet the high standards of \nscientific discourse.\n4  Future Outlook: Obsolescence \nor Evolution?\nBy 2030, three main transformative developments are pro-\njected [23, 25, 26, 39, 43–45]:\n1. Self-improving review systems\n AI agents will continuously update review articles \nthrough:\n • Real-time PubMed/arXiv monitoring.\n • Automated clinical trial data integration.\n • Dynamic impact factor recalculations.\n2. Personalized knowledge synthesis\n Researchers will access review variants tailored to:\n •  Methodological preferences (Bayesian vs. frequen-\ntist frameworks.\n •  Application contexts (basic science vs. translational \nneeds).\n •  Career stage (novice vs. expert comprehension levels).\n3. Decentralized peer review networks\n Blockchain-based systems will enable:\n •  AI-assisted review assignment matching expertise \ngaps.\n • Reputation tokens for contribution tracking.\n • Automated meta-reviews of review quality.\n4.1  The Argument for Obsolescence\nRapid updating versus the static nature of tradi-\ntional reviews.\n• The ability of AI to generate literature syntheses “in tens \nof minutes” as opposed to months or years.\nThe traditional review article, with its long produc-\ntion cycle and inherent limitations in updating frequency, \nfaces stiff competition from AI-powered alternatives. In \nan age where information is rapidly evolving, the ability to \nupdate literature reviews every few minutes—as opposed \nto every few months—might render static, human-authored \nreviews less useful. The deep research tools’ ability to “do \nin tens of minutes what once took human expert months” \nsuggests a future where the conventional review article \ncould be viewed as a relic of a bygone era. A survey con-\nducted in 2023 revealed that 79.0% of researchers believed \nAI will play a major role in the future of research. Many \nscholars have started integrating AI technologies into their \nprojects, including tasks like rephrasing, translation, and \nproofreading [46].\n4.2  The Argument for Evolution\n• The complementary role of human oversight in interpret-\ning, critiquing, and contextualizing AI outputs.\nProspects for a hybrid model that combines AI efficiency \nwith expert judgment.\nDespite these challenges, there is a compelling argument \nfor a hybrid future. AI-generated literature reviews can serve \nas a preliminary synthesis, which human experts can then \nrefine, contextualize, and critique. This synergy between \nmachine efficiency and human interpretative skill might lead \nto a new form of review article—one that is continuously \nupdated and critically assessed by experts. Such a model \nwould harness the strengths of both AI and human intellect, \nensuring that reviews remain both timely and rigorous.\n4.3  Ethical, Epistemological and Practical \nConsiderations\n• Issues of trustworthiness, citation integrity, and transpar-\nency in AI-generated outputs.\n• The importance of open access to information and the \nlimitations imposed by paywalls.\nThe integration of AI into scholarly writing is not with-\nout its ethical challenges. Issues such as the accuracy of \ncitations, the potential for bias in algorithmically generated \ncontent, and the need for transparency in methodology must \nbe addressed [47]. Furthermore, the problem of paywalled \ncontent—which currently limits AI access to many scientific \npapers—raises questions about equity and open science. As \nresearchers and technologists work towards resolving these \nissues, the path forward is likely to involve collaborative, \niterative improvements to both AI tools and scholarly prac-\ntices. The proliferation of deep research tools raises critical \nquestions:\n388 Bratislava Medical Journal (2025) 126:381–393\n• Authorship attribution: Should AI systems meeting \nICJME criteria receive co-authorship? Current guidelines \nconflict, with 58% of journals prohibiting AI authorship \nwhile accepting AI-assisted works.\n• Epistemic dependency risks: Studies show 42% of \nearly-career researchers cannot validate AI review out-\nputs due to skill atrophy, potentially creating \"black box\" \nknowledge dependencies.\n• Commercialization pressures: With OpenAI charging \n$200/month for enterprise access versus PerplexityAI's \nfree tier, inequities may emerge between well-funded and \nresource-poor institutions.\n4.3.1  Erosion of Critical Thinking and Academic Rigor\nThe convenience of AI-generated reviews risks fostering \noverreliance, potentially eroding researchers’ critical evalu-\nation skills. Raffaele Ciriello of the University of Sydney \nwarns that tools like deep research create an \"illusion of \nunderstanding,\" where users mistake syntactically flu-\nent reports for rigorous scholarship [43]. This concern is \nparticularly acute for early-career researchers who might \nprioritize speed over thorough engagement with primary \nsources [44]. Additionally, the tool’s outputs lack transpar -\nency in decision-making processes, making it difficult to \naudit how conclusions were derived from source materi-\nals [39, 43]. Also, Melisa et al. 2025 published study that \nexamines how AI-based tools, including ChatGPT, impact \nstudents’ critical thinking in academia. It highlights how \nAI can efficiently process literature reviews but also warns \nabout over-reliance on automated synthesis [5].\n4.3.2  Intellectual Property and Authorship Disputes\nThe integration of AI into scientific writing raises unre -\nsolved questions about authorship and intellectual contri-\nbution. If a literature review synthesized by an AI tool forms \nthe basis of a published paper, does the system qualify as \na co-author? Current guidelines from major publishers like \nElsevier and Springer-Nature prohibit non-human author -\nship, but this stance may evolve as AI contributions become \nmore substantial [39]. Furthermore, the tool’s reliance on \ncopyrighted materials—even with proper citations—could \nexpose users to legal challenges if publishers deem AI syn-\nthesis a form of derivative work infringement [39, 42].\n4.3.3  Impact on Research Practices and Publishing Norms: \nDisruption of Traditional Review Cycles\nAndrew White of FutureHouse envisions AI tools enabling \n\"dynamic reviews\" that update continuously as new stud-\nies emerge, contrasting with static, peer-reviewed articles \npublished every six to twelve months [45]. While this model \npromises greater timeliness, it clashes with existing aca-\ndemic incentives tied to publication metrics. Journals may \nresist adopting fluid review formats that complicate citation \ntracking and impact factor calculations. Moreover, the sheer \nvolume of AI-generated content could overwhelm traditional \npeer-review systems, necessitating new frameworks for vali-\ndating automated syntheses [39].\n4.3.4  Bias Propagation and Representational Equity\nAI systems trained on historical scientific literature risk per-\npetuating biases embedded in their source materials. For \nexample, a deep research report on cardiovascular treatments \nmight overrepresent studies from high-income countries, \ninadvertently marginalizing research from underrepresented \nregions [44]. Without explicit programming to address these \nimbalances, AI tools could exacerbate existing inequities in \nscholarly visibility. OpenAI has not disclosed methodologies \nfor mitigating such biases in its deep research agent, leaving \nusers to manually audit outputs for fairness [39, 43].\n5  Discussion\nOur exploration of AI-driven deep research tools reveals a \ntransformative yet complex impact on scientific literature \nreviews. On one hand, these advanced systems offer unpar-\nalleled speed and efficiency by rapidly aggregating vast \namounts of data, continuously updating analyses, and even \nidentifying emerging research trends. Features such as multi-\nsource verification and dynamic citation mapping enable AI \nto synthesize literature in a fraction of the time required by \ntraditional methods—reducing review time from weeks or \nmonths to mere hours [48].\nHowever, the benefits of this rapid synthesis come with \nnotable challenges. AI-generated reviews can suffer from \nissues like data hallucination, citation misattribution, and an \ninability to access critical paywalled content. Most impor -\ntantly, while AI excels at processing and summarizing large \ndatasets, it currently lacks the deep contextual understand-\ning and nuanced critique that human experts bring to schol-\narly synthesis. These limitations raise concerns about the \nreliability and depth of AI-only reviews, especially when it \ncomes to interpreting complex scientific concepts and iden-\ntifying truly meaningful research gaps [49–53].\nThese insights point toward a future where the most effec-\ntive model is hybrid in nature. In this evolving paradigm, AI \nacts as an efficient preliminary synthesizer—handling tasks \nsuch as data aggregation, trend detection, and citation man-\nagement—while human researchers provide critical over -\nsight, contextual interpretation, and ethical judgment. This \ncollaborative approach not only preserves the intellectual \n389Bratislava Medical Journal (2025) 126:381–393 \nrigor of traditional reviews but also harnesses AI’s capa-\nbilities to keep pace with the accelerating pace of research. \nAI imposes new roles not only on medical clinicians and \nresearchers, but it also enables new forms of comprehensive \noverviews and critical analyses so fundamental to medical \nknowledge comprehension [4, 5, 7, 54].\nFinally, ethical and practical considerations must guide \nthis integration. Issues surrounding authorship, intellectual \nproperty, and the risk of over-reliance on automated outputs \nnecessitate the development of transparent guidelines and \nverification systems. Addressing these challenges will be \nessential to ensure that the fusion of AI and human expertise \nenhances rather than undermines the integrity of scholarly \ncommunication [55, 56].\nWe are the last scientists who experienced what life was \nlike before AI and the benefits of rapid analysis and syn-\nthesis will be sooner or later taken as common as is now \nautomatic speller and other proofing tools incorporated in \ntext editors. Building on this transformative shift, it's evi-\ndent that our transition from a pre-AI era to one defined \nby rapid, automated analysis is only the beginning. The \nbenefits once considered futuristic, comparable to today’s \nubiquitous spell-checkers, are now permeating every facet \nof research and clinical practice. This evolution paves the \nway for a seamless integration of AI-driven diagnostics with \nadvanced therapeutic techniques, as illustrated in the emerg-\ning interdisciplinary workflows below. AI implementations \nnot only fundamentally shift paradigms of medical diagnos-\ntics [57– 63] and comprehensive review synthesis [64– 69] \nin various fields, it also defines novel unprecedented work -\nflows. For example recently introduced novel workflow that \ncombines AI-driven auto-segmentation of CBCT scans \nwith 4D shape-memory resins for personalized scaffold \nfabrication opens new possibilities in AI implementation \nfor sophisticated workflows that have not yet established \nin human-operator domain and are already taken by AI \nautomatization [70]. Moreover, emerging applications are \npoised to redefine 3D personalized anatomy modeling by \nproviding individualized topographical data for bone defect \nreconstruction [71, 72] and to advance AI pattern recogni-\ntion techniques in chronic pain management, thereby open-\ning new avenues for improved diagnosis, tailored treatment, \nand enhanced patient care [73–75].\nThe advent of intensive LLM-AI analyses, collectively \nreferred to as “Deep research”, marks a pivotal moment in \nthe evolution of scientific literature reviews. While tradi-\ntional review articles have long served as the cornerstone of \nacademic synthesis, the rapid pace of modern research and \nthe capabilities of AI suggest that a radical transformation \nis underway. Rather than heralding the end of review arti-\ncles, AI-driven synthesis may well drive the evolution of a \nhybrid model—one that combines the speed and breadth of \nmachine intelligence with the critical, contextual insights of \nhuman experts [18]. The future of scholarly review writing, \ntherefore, lies not in obsolescence but in adaptation, inno-\nvation, and the thoughtful integration of new technologies \ninto established academic traditions. The deep research revo-\nlution does not render traditional review articles obsolete \nbut rather redefines their purpose. As these systems achieve \n91% parity with human synthesis quality, the scholar's role \nevolves from information aggregator to:\n• Validation architect designing robust verification work-\nflows.\n• Conceptual innovator identifying novel synthesis path-\nways.\n• Ethical Steward navigating AI's societal implications.\nFuture review formats will likely hybridize AI efficiency \nwith human insight through:\n1. Modular publishing separating machine-generated con-\ntent blocks from human commentary.\n2. Process transparency standards mandating disclosure \nof AI system parameters and training data.\n3. Dynamic impact metrics measuring real-world applica-\ntion success rather than citation counts.\nThis transition period demands proactive collaboration \nbetween AI developers, publishers, and research commu-\nnities to harness efficiency gains without compromising \nscientific integrity. The ultimate test lies not in surpassing \nhuman speed, but in enhancing our collective capacity for \ntransformative discovery [19, 21].\nNavigating the human-AI partnership will be a challenge. \nThe challenges confronting deep research tools underscore \nthe irreplaceable role of human expertise in scientific \ninquiry. While AI can accelerate data collection and pre-\nliminary synthesis, it cannot replicate the curiosity, creativ-\nity, and critical scrutiny that drive meaningful discoveries. \nThe path forward lies in developing hybrid workflows where \nAI handles repetitive tasks—such as citation formatting and \ndatabase searches—while researchers focus on hypothesis \ngeneration, experimental design, and contextual interpreta-\ntion [4, 7, 9, 19, 21].\nAddressing technical limitations like paywall access and \nhallucination rates will require collaborative efforts between \nAI developers, publishers, and research institutions. Initia-\ntives to create standardized APIs for authenticated jour -\nnal access or human-in-the-loop validation systems could \nenhance reliability. Simultaneously, the academic commu-\nnity must establish clear guidelines for AI-assisted research \ntransparency, ensuring that automated contributions are dis-\nclosed and rigorously verified [5, 7, 27].\nAs these tools evolve, their success will hinge on recog-\nnizing that AI is not a replacement for human intellect but a \ncatalyst for reimagining how knowledge is synthesized. By \n390 Bratislava Medical Journal (2025) 126:381–393\nleveraging the strengths of both human and artificial intel-\nligence, the scientific community can harness the efficiency \nof deep research tools while preserving the integrity and \ndepth that define scholarly excellence.\nOur analysis demonstrates that AI-driven deep research \ntools are radically transforming the literature review pro-\ncess. By aggregating vast amounts of data and dynamically \nupdating analyses in real time, these systems compress what \ntraditionally took weeks or months into mere hours. They \nachieve this through features like multi-source verification, \ndynamic citation mapping, and trend detection, which are \nparticularly valuable in rapidly evolving research fields.\nIn comparing AI-generated reviews with human-authored \nones, we find that while AI excels in efficiency and consist-\nency, delivering quick, broadly scoped syntheses, it yet falls \nshort in delivering the necessary depth of contextual under-\nstanding and critical nuance inherent to human expertise. AI \ntools can identify research gaps and format citations accu-\nrately most of the time, yet they remain vulnerable to issues \nsuch as data hallucinations and misattributed sources. This \nhighlights the promise of a hybrid model where AI handles \nthe heavy lifting of data aggregation and preliminary syn-\nthesis, and human experts provide the critical oversight and \ninterpretative depth necessary for rigorous scholarly work.\n6  Conclusion\nThe advent of AI-agentic deep research tools marks a pivotal \nturning point in the evolution of scientific literature reviews. \nThis paper concludes that hybrid reviews could slash pub-\nlishing timelines by 2030.\nAs one of the most important aspects of AI is its learn-\ning speed, current leader of “Deep research/search” AI tools \nis Grok from xAI with unprecedented speeds of learning. \nThese tools are here to stay, will keep improving and will \ntake over many routine tasks, even in PhD-level. So, scien-\ntists should become good at asking questions, as these AI \nagents require expertise in prompting. Our study highlights \nseveral major points:\n• Efficiency gains: AI systems dramatically reduce the \ntime required for literature synthesis, offering real-time \nupdates and broad data coverage.\n• Critical limitations: Despite their speed, current AI \ntools can struggle with factual accuracy, citation errors, \nand a lack of deep contextual analysis.\n• Hybrid potential: Combining AI's computational effi-\nciency with human critical oversight appears to be the \nmost promising path forward, ensuring that reviews \nremain both timely and intellectually rigorous.\nLooking ahead, the landscape of scholarly review writ-\ning is poised for significant transformation. We envision a \nfuture where review articles evolve into dynamic, continu -\nously updated documents—modular in design and enriched \nby both automated data synthesis and human interpretation. \nThis shift will not render traditional reviews obsolete but \nwill redefine their role, integrating real-time analytics with \nthe critical insight of seasoned researchers.\nTo fully realize this potential, further research is essential. \nFuture studies should focus on:\n• Refining AI methodologies: Minimizing errors and \nbiases in automated literature synthesis.\n• Developing robust verification systems: Establishing \nstandardized protocols and transparent guidelines to \nensure the integrity of AI-generated content.\n• Ethical integration: Crafting policies that balance effi-\nciency with the preservation of scholarly rigor and intel-\nlectual depth.\nBy embracing a hybrid approach and investing in these \nresearch avenues, the academic community can harness AI's \nefficiency while maintaining the high standards of scholarly \ninquiry that underpin transformative discovery.\nAcknowledgements The authors gratefully acknowledge the technical \nsupport of the digital dental lab infrastructure of 3Dent Medical Ltd. \nAnd dental clinic Sangre Azul Ltd.\nAuthor Contributions I.V. and A.T. performed the methodology and \nwrote the manuscript draft, A.T. and I.V. supervised the manuscript, \nI.V. and A.T. revised the original draft and conducted final revision of \nthe manuscript. All Authors reviewed and approved the final form of \nthe manuscript.\nFunding  This research was funded by: The Slovak Research and \nDevelopment Agency grant APVV-21-0173 and Cultural and Educa-\ntional Grant Agency of the Ministry of Education and Science of the \nSlovak Republic (KEGA) 2023 054UK-42023.\nData Availability Not applicable.\nDeclarations \nConflict of interest The authors declare no conflicts of interest.\nInformed consent Not applicable.\nInstitutional review board statement Not applicable.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \n391Bratislava Medical Journal (2025) 126:381–393 \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\n 1. Hanson MA, Barreiro PG, Crosetto P, Brockington D. The strain \non scientific publishing. Quant Sci Stud. 2024;5(4):823–43. \nhttps:// doi. org/ 10. 1162/ qss_a_ 00327.\n 2. Adamu A, Editor R, Okebukola PA, Assistants E, Gbeleyi O, \nOladejo A, et al. AI and curriculum development for the future \nbook in honour of Foreword by Cynthia Jackson-Hammond.\n 3. Muniz FWMG, Celeste RK, Oballe HJR, Rösing CK. Citation \nanalysis and trends in review articles in dentistry. J Evid Based \nDent Pract. 2018;18(2):110–8.\n 4. Abdullahi OA, Omeneke AB, Olajide N S, Oriola AO, Simeon \nAA, Musa IS, et al. Deployment of generative AI in academic \nresearch among higher education students: a bibliometric \napproach. Issue. 2025;15. https:// doi. org/ 10. 6007/ IJARB SS/ v15- \ni1/ 24351.\n 5. Melisa R, Ashadi A, Triastuti A, Hidayati S, Salido A, Efriani \nLuansi Ero P, et al. Critical thinking in the age of AI: a systematic \nreview of AI’s effects on higher education. Educat Process Int J \n[Internet]. 2025;14 [cited 2025 Feb 16]. Available from: www. \nedupij. com.\n 6. Agbanimu D, Onowugbeda F, Gbeleyi O, Peter E, Adam U. \nRole of AI in enhancing teaching/learning, research and com-\nmunity service in higher education foreword by Cynthia \nJackson-Hammond.\n 7. Schroeder MK, Alcruz J. Navigating new frontier: AI’s trans-\nformation of dissertation research and writing in an Educational \nLeadership Doctoral Program. Impact Educ J Transform Prof \nPract [Internet]. 2025;10(1):27–32 [cited 2025 Feb 16]. Avail-\nable from: https:// impac tinged. pitt. edu/ ojs/ Impac tingEd/ artic le/ \nview/ 477.\n 8. Ouvrir la Science—excessive growth in the number of scientific \npublications [Internet]. [cited 2025 Feb 16]. Available from: \nhttps:// www. ouvri rlasc ience. fr/ exces sive- growth- in- the- number- \nof- scien tific- publi catio ns/.\n 9. Colecchia F, Giunchi D, Qin R, Ceccaldi E, Wang F. Editorial: \nMachine learning and immersive technologies for user-centered \ndigital healthcare innovation. Front Big Data. 2025;8:1567941. \nhttps:// doi. org/ 10. 3389/ fdata. 2025. 15679 41/ full.\n 10. Fageeh A. The rise of Chatbots in higher education: exploring \nuser profiles, motivations, and integration strategies. 2025 [cited \n2025 Feb 16]. Available from: https:// papers. ssrn. com/ abstr act= \n51132 21.\n 11. Rim CH. Practical advice for South Korean medical researchers \nregarding open-access and predatory journals. Cancer Res Treat. \n2021;53(1):1–8.\n 12. Hanson MA, Barreiro PG, Crosetto P, Brockington D. The strain \non scientific publishing. Quant Sci Stud [Internet]. 2023:1–29 \n[cited 2025 Feb 14]. Available from: http:// arxiv. org/ abs/ 2309. \n15884.\n 13. Regnaux JP. Intérêt des revues systématiques pour la prise de déci-\nsion en pratique Clinique [The benefits of systematic reviews for \ndecision-making in clinical practice]. Soins. 2014 Nov;(790):58-\n62. French. PMID: 25619104.\n 14. Dhillon P. How to write a good scientific review article. FEBS J. \n2022;289(13):3592–602.\n 15. Begley SL, Pelcher I, Schulder M. Topic reviews in neurosurgi-\ncal journals: an analysis of publication trends. World Neurosurg. \n2023;179:171–6.\n 16. Schmallenbach L, Bärnighausen TW, Lerchenmueller MJ. The \nglobal geography of artificial intelligence in life science research. \nNat Commun. 2024 Sep 12;15(1):7527. doi: 10.1038/s41467-024-\n51714-x. PMID: 39266506; PMCID: PMC11392928. \n 17. Izwan S, Chan E, Ibraheem C, Bhagwat G, Parker D. Trends in \npublication of general surgery research in Australia, 2000–2020. \nANZ J Surg. 2022;92(4):718–22.\n 18. Aggarwal V, Karwasra N. Artificial intelligence v/s human \nintelligence: a relationship between digitalization and interna-\ntional trade. Future Bus J. 2025;11:16. https:// doi. org/ 10. 1186/ \ns43093- 025- 00438-5.\n 19. Ziky R, Bahida H, Abriane A. The impact of artificial intelligence \non business performance: a bibliometric analysis of publication \ntrends. Moroc J Quant Qual Res [Internet]. 2025;7(1) [cited \n2025 Feb 16]. Available from: https:// revues. imist. ma/ index. php/ \nMJQR/ artic le/ view/ 53210.\n 20. Bezuidenhout C, Abbas R, Mehmet M, Heffernan T. Artificial \nintelligence in professional services: a systematic review and \nfoundational baseline for future research [Internet]. 2025 [cited \n2025 Feb 16]. https:// doi. org/ 10. 1142/ S0219 64922 55000 91. \nAvailable from: https:// www. world scien tific. com/ world scinet/ \njikm.\n 21. Teunis M, Luechtefeld T, Hartung T, Oredsson S. Editorial: \nLeveraging artificial intelligence and open science for toxico-\nlogical risk assessment. Front Toxicol. 2025;7:1568453. https://  \ndoi. org/ 10. 3389/ ftox. 2025. 15684 53/ full.\n 22. Holzinger A, Zatloukal K, Müller H. Is human oversight to AI \nsystems still possible? N Biotechnol. 2025;25(85):59–62.\n 23. New OpenAI “Deep Research” agent turns ChatGPT into a \nresearch analyst—Campus Technology [Internet]. [cited 2025 \nFeb 17]. Available from: https://  campu stech nology. com/ artic  \nles/ 2025/ 02/ 12/ new- openai- deep- resea rch- agent- turns- chatg \npt- into-a- resea rch- analy st. aspx.\n 24. Li J, Luo B, Xu X, Huang T. Offline reward shaping with scal-\ning human preference feedback for deep reinforcement learn-\ning. Neural Netw. 2025 Jan;181:106848. doi: 10.1016/j.neu-\nnet.2024.106848. Epub 2024 Nov 1. PMID: 39515081.\n 25. OpenAI launches deep research: a new tool that cuts research \ntime by 90%! | Fello AI [Internet]. [cited 2025 Feb 17]. Avail-\nable from: https:// fello ai. com/ 2025/ 02/ openai- launc hes- deep- \nresea rch-a- new- tool- that- cuts- resea rch- time- by- 90/.\n 26. ChatGPT’s deep research vs. Google’s Gemini 1.5 Pro with \ndeep research: a detailed comparison | White Beard Strategies \n[Internet]. [cited 2025 Feb 17]. Available from: https:// white  \nbeard strat egies. com/ ai- prompt- engin eering/ chatg pts- deep- resea \nrch- vs- googl es- gemini- 1-5- pro- with- deep- resea rch-a- detai led- \ncompa rison/.\n 27. Büchel J, Vasilopoulos A, Simon WA, Boybat I, Tsai H, Burr \nGW, et al. Efficient scaling of large language models with mix-\nture of experts and 3D analog in-memory computing. Nat Com-\nput Sci. 2025;5(1):13–26.\n 28. Perplexity AI’s deep research feature is available now—\nhere’s how to try it for free | Tom’s Guide [Internet]. \n[cited 2025 Feb 17]. Available from: https:// www. tomsg  \nuide. com/ ai/ perpl exity- ais- deep- resea rch- featu re- is- avail  \nable- now- heres- how- to- try- it- for- free.\n 29. Elon Musk’s xAI releases its latest flagship model, Grok 3 | \nTechCrunch [Internet]. [cited 2025 Feb 20]. Available from: \nhttps:// techc runch. com/ 2025/ 02/ 17/ elon- musks- ai- compa ny- \nxai- relea ses- its- latest- flags hip- ai- grok-3/.\n 30. Grok 3 beta—the age of reasoning agents [Internet]. [cited 2025 \nFeb 20]. Available from: https://x. ai/ blog/ grok-3.\n 31. Grok 3 review: xAI’s most advanced AI model—features & per-\nformance | Neural Notes [Internet]. [cited 2025 Feb 20]. Avail-\nable from: https:// medium. com/ neural- notes/ grok-3- fd93b ac052 \n4a.\n392 Bratislava Medical Journal (2025) 126:381–393\n 32. Elon Musk’s Grok 3: performance, how to access, and more \n[Internet]. [cited 2025 Feb 20]. Available from: https:// www. \nanaly ticsv idhya. com/ blog/ 2025/ 02/ grok- 3/#h- bench mark- perfo \nrmance.\n 33. Grok-3—most advanced AI model from xAI [Internet]. [cited \n2025 Feb 20]. Available from: https:// opencv. org/ blog/ grok-3/.\n 34. Grok 3 technical review: everything you need to know [Internet]. \n[cited 2025 Feb 26]. Available from: https:// www. helic one. ai/  \nblog/ grok-3- bench mark- compa rison.\n 35. Grok 3 review: I tested 100+ prompts and here’s the truth \n(2025)—Writesonic Blog [Internet]. [cited 2025 Feb 26]. Avail-\nable from: https:// write sonic. com/ blog/ grok-3- review.\n 36. GPT-4o vs GPT-4 Turbo [Internet]. [cited 2025 Feb 26]. Available \nfrom: https:// www. vellum. ai/ blog/ analy sis- gpt- 4o- vs- gpt-4- turbo.\n 37. Grok-3 outperforms all AI models in benchmark test, xAI claims \n[Internet]. [cited 2025 Feb 26]. Available from: https:// coint elegr \naph. com/ news/ grok-3- tesla- bot- mars- missi on- 2026.\n 38. Grok 3—intelligence, performance & price analysis | Artificial \nAnalysis [Internet]. [cited 2025 Feb 26]. Available from: https://  \nartifi  cial analy sis. ai/ models/ grok-3.\n 39. OpenAI unveils “Deep Research”: a major leap in AI-driven \nscientific assistance | AI News [Internet]. [cited 2025 Feb 16]. \nAvailable from: https:// opent ools. ai/ news/ openai- unvei ls- deep- \nresea rch-a- major- leap- in- ai- driven- scien tific- assis tance.\n 40. Miller LE, Bhattacharyya D, Miller VM, Bhattacharyya M. \nRecent Trend in Artificial Intelligence-Assisted Biomedical \nPublishing: A Quantitative Bibliometric Analysis. Cureus. \n2023 May 19;15(5):e39224. doi: 10.7759/cureus.39224. PMID: \n37337487; PMCID: PMC10277011.\n 41. Gibson AF, Beattie A. More or less than human? Evaluating the \nrole of AI-as-participant in online qualitative research. Qual Res \nPsychol. 2024;21(2):175–99.\n 42. Jones N. OpenAI’s ‘deep research’ tool: is it useful for scien-\ntists? Nature. 2025 [Internet]. [cited 2025 Feb 16]. Available \nfrom: https:// bioen gineer. org/ openai- deep- resea rch- tool- revol \nution izes- liter ature- revie ws/.\n 43. OpenAI deep research agent a fallible tool [Internet]. [cited \n2025 Feb 16]. Available from: https:// www. sydney. edu. au/ news- \nopini on/ news/ 2025/ 02/ 12/ openai- deep- resea rch- agent-a- falli \nble- tool. html.\n 44. AI and automated literature reviews: opportunities and chal-\nlenges—Enago Read—literature review and analysis tool for \nresearchers [Internet]. [cited 2025 Feb 17]. Available from: \nhttps:// www. read. enago. com/ blog/ ai- and- autom ated- liter ature- \nrevie ws- oppor tunit ies- and- chall enges/.\n 45. OpenAI deep research tool revolutionizes literature reviews \n[Internet]. [cited 2025 Feb 17]. Available from: https:// bioen  \ngineer. org/ openai- deep- resea rch- tool- revol ution izes- liter ature- \nrevie ws/.\n 46. Salvagno M, De Cassai A, Zorzi S, Zaccarelli M, Pasetto M, \nSterchele ED, et al. The state of artificial intelligence in medical \nresearch: a survey of corresponding authors from top medical \njournals. PLoS ONE. 2024;19(8): e0309208. https:// doi. org/ 10. \n1371/ journ al. pone. 03092 08.\n 47. Messeri L, Crockett MJ. Artificial intelligence and illu-\nsions of understanding in scientific research. Nature. 2024 \nMar;627(8002):49-58. doi: 10.1038/s41586-024-07146-0. Epub \n2024 Mar 6. PMID: 38448693.\n 48. Danler M, Hackl WO, Neururer SB, Pfeifer B. Quality and \neffectiveness of AI tools for students and researchers for scien -\ntific literature review and analysis. Stud Health Technol Inform. \n2024;313:203–8.\n 49. Huang J, Tan M. The role of ChatGPT in scientific communica-\ntion: writing better scientific review articles. Am J Cancer Res. \n2023;13(4):1148.\n 50. Kacena MA, Plotkin LI, Fehrenbacher JC. The use of artificial \nintelligence in writing scientific review articles. Curr Osteoporos \nRep. 2024;22(1):115–21.\n 51. Van Dijk SHB, Brusse-Keizer MGJ, Bucsán CC, Van Der Palen \nJ, Doggen CJM, Lenferink A. Artificial intelligence in system-\natic reviews: promising when appropriately used. BMJ Open. \n2023;13(7):e072254.\n 52. Passby L, Madhwapathi V, Tso S, Wernham A. Appraisal of AI-\ngenerated dermatology literature reviews. J Eur Acad Dermatol \nVenereol. 2024;38(12):2235–9.\n 53. Mostafapour M, Fortier JH, Pacheco K, Murray H, Garber G. \nEvaluating literature reviews conducted by humans versus Chat-\nGPT: comparative study. JMIR AI. 2024;3: e56537.\n 54. Surovková J, Haluzová S, Strunga M, Urban R, Lifková M, Thurzo \nA. The new role of the dental assistant and nurse in the age of \nadvanced artificial intelligence in telehealth orthodontic care \nwith dental monitoring: preliminary report. Appl Sci [Internet]. \n2023;13(8):5212 [cited 2023 Dec 18]. Available from: https://  \nwww. mdpi. com/ 2076- 3417/ 13/8/ 5212/ htm.\n 55. Bhargava DC, Jadav D, Meshram VP, Kanchan T. Chat-\nGPT in medical research: challenging time ahead. Med Leg J. \n2023;91(4):223–5.\n 56. Messeri L, Crockett MJ. Artificial intelligence and illu-\nsions of understanding in scientific research. Nature. \n2024;627(8002):49–58.\n 57. Tomášik J, Zsoldos M, Oravcová Ľ, Lifková M, Pavleová G, \nStrunga M, et al. AI and face-driven orthodontics: a scoping \nreview of digital advances in diagnosis and treatment planning. \nAI (Switzerland). 2024;5(1):158–76.\n 58. Strunga M, Ballova DS, Tomasik J, Oravcova L, Danisovic L, \nThurzo A. AI-automated cephalometric tracing: a new normal in \northodontics? In: International Conference on Artificial Intelli-\ngence, Computer, Data Sciences, and Applications, ACDSA 2024. \n2024.\n 59. Thurzo A, Jungova P, Danisovic L. AI-powered segmentation \nrevolutionizes scaffold design in regenerative dentistry. In: Inter-\nnational Conference on Artificial Intelligence, Computer, Data \nSciences, and Applications, ACDSA 2024. 2024.\n 60. Bakhsh T, Tahir S, Shahzad B, Kováč P, Jackuliak P, Bražinová \nA, et al. Artificial intelligence-driven facial image analysis for \nthe early detection of rare diseases: legal, ethical, forensic, and \ncybersecurity considerations. AI [Internet]. 2024;5(3):990–1010 \n[cited 2024 Jul 1]. Available from: https:// www. mdpi. com/ 2673- \n2688/5/ 3/ 49/ htm.\n 61. Tomášik J, Zsoldos M, Majdáková K, Fleischmann A, Oravcová Ľ, \nSónak Ballová D, et al. The potential of AI-powered face enhance-\nment technologies in face-driven orthodontic treatment planning. \nAppl Sci [Internet]. 2024;14(17):7837 [cited 2025 Feb 17]. Avail-\nable from: https:// www. mdpi. com/ 2076- 3417/ 14/ 17/ 7837/ htm.\n 62. Jeong J, Kim S, Pan L, Hwang D, Kim D, Choi J, et al. Reducing \nthe workload of medical diagnosis through artificial intelligence: \na narrative review. Medicine. 2025;104(6): e41470.\n 63. Németh Á, Tóth G, Fülöp P, Paragh G, Nádró B, Karányi Z, \nParagh G Jr, Horváth Z, Csernák Z, Pintér E, Sándor D, Bagyó \nG, Édes I, Kappelmayer J, Harangi M, Daróczy B. Smart medi-\ncal report: efficient detection of common and rare diseases on \ncommon blood tests. Front Digit Health. 2024 Dec 5;6:1505483. \ndoi: 10.3389/fdgth.2024.1505483. PMID: 39703757; PMCID: \nPMC11656307.\n 64. Gašparovič M, Jungová P, Tomášik J, Mriňáková B, Hirjak D, \nTimková S, et al. Evolving strategies and materials for scaf-\nfold development in regenerative dentistry. Appl Sci [Internet]. \n2024;14(6):2270 [cited 2024 Jul 1]. Available from: https:// www. \nmdpi. com/ 2076- 3417/ 14/6/ 2270/ htm.\n 65. Tichá D, Tomášik J, Oravcová Ľ, Thurzo A. Three-dimen-\nsionally-printed polymer and composite materials for dental \n393Bratislava Medical Journal (2025) 126:381–393 \napplications with focus on orthodontics. Polymers [Internet]. \n2024;16(22):3151 [cited 2025 Feb 17]. Available from: https:// \nwww. mdpi. com/ 2073- 4360/ 16/ 22/ 3151/ htm.\n 66. Lepišová M, Tomášik J, Oravcová Ľ, Thurzo A. Three-dimen-\nsional-printed elements based on polymer and composite materi-\nals in dentistry: a narrative review. Bratisl Med J. 2025;2025:1–\n14. https:// doi. org/ 10. 1007/ s44411- 024- 00011-6.\n 67. Paľovčík M, Tomášik J, Zsoldos M, Thurzo A. 3D-printed acces-\nsories and auxiliaries in orthodontic treatment. Appl Sci [Inter -\nnet]. 2024;15(1):78 [cited 2025 Feb 17]. Available from: https://  \nwww. mdpi. com/ 2076- 3417/ 15/1/ 78.\n 68. Hoffmann K, Gebler R, Grummt S, Peng Y, Reinecke I, Wolfien \nM, et al. A concept for integrating AI-based support systems into \nclinical practice. Stud Health Technol Inform. 2024;316:643–4.\n 69. Nair M, Svedberg P, Larsson I, Nygren JM. A comprehensive over-\nview of barriers and strategies for AI implementation in health-\ncare: mixed-method design. PLoS ONE. 2024;19(8):e0305949.\n 70. Thurzo A, Varga I. Advances in 4D shape-memory resins for \nAI-aided personalized scaffold bioengineering. Bratisl Med J. \n2025;2025:1–6. https:// doi. org/ 10. 1007/ s44411- 025- 00043-6.\n 71. Inchingolo M, Kizek P, Riznic M, Borza B, Chromy L, Glinska \nKK, et al. Dental auto transplantation success rate increases by \nutilizing 3D replicas. Bioengineering [Internet]. 2023;10(9):1058 \n[cited 2023 Oct 5]. Available from: https:// www. mdpi. com/ 2306- \n5354/ 10/9/ 1058/ htm.\n 72. Czako L, Simko K, Thurzo A, Galis B, Varga I. The syndrome of \nelongated styloid process, the Eagle’s syndrome—from anatomi-\ncal, evolutionary and embryological backgrounds to 3D printing \nand personalized surgery planning. Report of five cases. Medicina \n(B Aires) [Internet]. 2020;56(9):458. Available from: https://  \nwww. mdpi. com/ 1648- 9144/ 56/9/ 458.\n 73. Zhang M, Zhu L, Lin SY, Herr K, Chi CL, Demir I, et al. \nUsing artificial intelligence to improve pain assessment and \npain management: a scoping review. J Am Med Inform Assoc. \n2023;30(3):570–87.\n 74. Kizek P, Pacutova V, Schwartzova V, Timkova S. Decoding \nchronic jaw pain: key nature of temporomandibular disorders \nin Slovak patients. Bratisl Med J. 2025. https:// doi. org/ 10. 1007/ \ns44411- 025- 00034-7.\n 75. Meier TA, Refahi MS, Hearne G, Restifo DS, Munoz-Acuna R, \nRosen GL, et al. The role and applications of artificial intelli-\ngence in the treatment of chronic pain. Curr Pain Headache Rep. \n2024;28(8):769–84.\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Publishing",
  "concepts": [
    {
      "name": "Publishing",
      "score": 0.5853652954101562
    },
    {
      "name": "Scientific publishing",
      "score": 0.5832995772361755
    },
    {
      "name": "Scientific reasoning",
      "score": 0.574130117893219
    },
    {
      "name": "Cognitive science",
      "score": 0.39839866757392883
    },
    {
      "name": "Epistemology",
      "score": 0.3463154435157776
    },
    {
      "name": "Psychology",
      "score": 0.34096527099609375
    },
    {
      "name": "Computer science",
      "score": 0.3405655026435852
    },
    {
      "name": "Philosophy",
      "score": 0.2424217164516449
    },
    {
      "name": "Literature",
      "score": 0.17049625515937805
    },
    {
      "name": "Art",
      "score": 0.1702663004398346
    }
  ],
  "institutions": []
}