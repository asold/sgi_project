{
  "title": "A Study on the Representativeness Heuristics Problem in Large Language Models",
  "url": "https://openalex.org/W4403182105",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2127671036",
      "name": "Jongwon Ryu",
      "affiliations": [
        "Chung-Ang University"
      ]
    },
    {
      "id": "https://openalex.org/A2109856108",
      "name": "Jung-Eun Kim",
      "affiliations": [
        "Chung-Ang University"
      ]
    },
    {
      "id": "https://openalex.org/A2099208401",
      "name": "Jun-Yeong Kim",
      "affiliations": [
        "Chung-Ang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W6678262379",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2962801832",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2963499153",
    "https://openalex.org/W6809646742",
    "https://openalex.org/W4292157289",
    "https://openalex.org/W4391655051",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W3185341429"
  ],
  "abstract": "Large language models (LLMs) exhibit remarkable proficiency in text generation. However, their logical reasoning capabilities require enhancement. Major strides have been achieved in reasoning techniques for LLM, such as Few-shot, Zero-shot, and Chain-of-Thought (CoT). Nevertheless, these techniques have shortcomings, particularly in addressing the representativeness heuristic (RH) phenomenon. RH is a cognitive bias that occurs when a person judges the probability of an event or the likelihood that an object belongs to a particular category based on how well it matches the prototype or stereotype of that category. In this study, we investigated the pervasive issue of RH errors in LLMs. This research surpasses the constraints of previous studies by analyzing various RH scenarios that they did not cover and by directly constructing and testing the corresponding datasets. Moreover, a novel prompt called zero-shot-RH is proposed to augment the reasoning ability of LLMs, mitigate RH errors, and thus bolster logical reasoning. This approach seeks to enable LLMs to comprehend the given information better and reduce the biases stemming from RH errors. The prompt zero-shot-RH achieved an average accuracy higher than zero-shot-CoT by 0.145 and 0.277 in the tasks of correct reasoning and correct reasonings by sex, respectively, without relying on RH. The outcomes of this research endeavor are a deeper understanding of RH errors in LLMs and novel strategies to mitigate these biases, thereby advancing the domain of logical reasoning within LLMs.",
  "full_text": null,
  "topic": "Representativeness heuristic",
  "concepts": [
    {
      "name": "Representativeness heuristic",
      "score": 0.9253605604171753
    },
    {
      "name": "Heuristics",
      "score": 0.8205620050430298
    },
    {
      "name": "Computer science",
      "score": 0.747348964214325
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39756903052330017
    },
    {
      "name": "Natural language processing",
      "score": 0.3678746223449707
    },
    {
      "name": "Data science",
      "score": 0.3420710563659668
    },
    {
      "name": "Statistics",
      "score": 0.18370887637138367
    },
    {
      "name": "Mathematics",
      "score": 0.12337613105773926
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I67900169",
      "name": "Chung-Ang University",
      "country": "KR"
    }
  ]
}