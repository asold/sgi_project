{
    "title": "Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance",
    "url": "https://openalex.org/W4389520312",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2928865931",
            "name": "Thiemo Wambsganss",
            "affiliations": [
                "Bern University of Applied Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2604109351",
            "name": "Xiaotian Su",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2809030798",
            "name": "Vinitra Swamy",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": null,
            "name": "Seyed Neshaei",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2529947158",
            "name": "Roman Rietsche",
            "affiliations": [
                "Bern University of Applied Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A1973002802",
            "name": "Tanja Käser",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2941035428",
        "https://openalex.org/W4283795605",
        "https://openalex.org/W4366547384",
        "https://openalex.org/W3037387464",
        "https://openalex.org/W2739810148",
        "https://openalex.org/W4385573549",
        "https://openalex.org/W3124522743",
        "https://openalex.org/W2124725212",
        "https://openalex.org/W4296934597",
        "https://openalex.org/W2041282815",
        "https://openalex.org/W4366593738",
        "https://openalex.org/W3162914310",
        "https://openalex.org/W4221142858",
        "https://openalex.org/W3206487987",
        "https://openalex.org/W2952328691",
        "https://openalex.org/W4229977739",
        "https://openalex.org/W2963078909",
        "https://openalex.org/W4378189609",
        "https://openalex.org/W4312089323",
        "https://openalex.org/W4220747294",
        "https://openalex.org/W3196248941",
        "https://openalex.org/W4385570036",
        "https://openalex.org/W3104617516",
        "https://openalex.org/W3162916232",
        "https://openalex.org/W4225015089",
        "https://openalex.org/W2251771443",
        "https://openalex.org/W4385571700",
        "https://openalex.org/W2970800693",
        "https://openalex.org/W2802105481",
        "https://openalex.org/W4385571301",
        "https://openalex.org/W4320830123",
        "https://openalex.org/W2963612262",
        "https://openalex.org/W3037831233",
        "https://openalex.org/W2893425640",
        "https://openalex.org/W1576726980",
        "https://openalex.org/W3202872855",
        "https://openalex.org/W3214911045",
        "https://openalex.org/W4225087473",
        "https://openalex.org/W4287749667",
        "https://openalex.org/W4385565334",
        "https://openalex.org/W3206079177",
        "https://openalex.org/W2887768933",
        "https://openalex.org/W2899451599",
        "https://openalex.org/W4288617757",
        "https://openalex.org/W2955996947",
        "https://openalex.org/W2950018712"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly utilized in educational tasks such as providing writing suggestions to students. Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners. Previous studies have investigated bias in models and data representations separately, neglecting the potential impact of LLM bias on human writing. In this paper, we investigate how bias transfers through an AI writing support pipeline. We conduct a large-scale user study with 231 students writing business case peer reviews in German. Students are divided into five groups with different levels of writing support: one in-classroom group with recommender system feature-based suggestions and four groups recruited from Prolific – a control group with no assistance, two groups with suggestions from fine-tuned GPT-2 and GPT-3 models, and one group with suggestions from pre-trained GPT-3.5. Using GenBit gender bias analysis and Word Embedding Association Tests (WEAT), we evaluate the gender bias at various stages of the pipeline: in reviews written by students, in suggestions generated by the models, and in model embeddings directly. Our results demonstrate that there is no significant difference in gender bias between the resulting peer reviews of groups with and without LLM suggestions. Our research is therefore optimistic about the use of AI writing support in the classroom, showcasing a context where bias in LLMs does not transfer to students' responses.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10275–10288\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nUnraveling Downstream Gender Bias from Large Language Models: A\nStudy on AI Educational Writing Assistance\nThiemo Wambsganss∗1, Xiaotian Su∗2,\nVinitra Swamy 2, Seyed Parsa Neshaei 2, Roman Rietsche 1, Tanja Käser 2\n1 Bern University of Applied Sciences\n{thiemo.wambsganss, roman.rietsche}@bfh.ch\n2 EPFL, Lausanne, Switzerland\n{xiaotian.su, vinitra.swamy, seyed.neshaei, tanja.kaeser}@epfl.ch\nAbstract\nLarge Language Models (LLMs) are increas-\ningly utilized in educational tasks such as pro-\nviding writing suggestions to students. De-\nspite their potential, LLMs are known to harbor\ninherent biases which may negatively impact\nlearners. Previous studies have investigated\nbias in models and data representations sepa-\nrately, neglecting the potential impact of LLM\nbias on human writing. In this paper, we inves-\ntigate how bias transfers through an AI writing\nsupport pipeline. We conduct a large-scale user\nstudy with 231 students writing business case\npeer reviews in German. Students are divided\ninto five groups with different levels of writ-\ning support: one classroom group with feature-\nbased suggestions and four groups recruited\nfrom Prolific – a control group with no assis-\ntance, two groups with suggestions from fine-\ntuned GPT-2 and GPT-3 models, and one group\nwith suggestions from pre-trained GPT-3.5. Us-\ning GenBit gender bias analysis, Word Embed-\nding Association Tests (WEAT), and Sentence\nEmbedding Association Test (SEAT) we eval-\nuate the gender bias at various stages of the\npipeline: in model embeddings, in suggestions\ngenerated by the models, and in reviews written\nby students. Our results demonstrate that there\nis no significant difference in gender bias be-\ntween the resulting peer reviews of groups with\nand without LLM suggestions. Our research is\ntherefore optimistic about the use of AI writing\nsupport in the classroom, showcasing a con-\ntext where bias in LLMs does not transfer to\nstudents’ responses 1.\n1 Introduction\nLarge Language Models (LLMs) have proven use-\nful for improving the adaptivity and individualiza-\ntion of educational technology (Jones and Stein-\nhardt, 2022; Xu et al., 2021). Researchers and\n*These authors contributed equally to this work\n1Our code and data available at: https://github.com/\nepfl-ml4ed/unraveling-llm-bias\npractitioners have been developing a plethora of ed-\nucational writing tools based on Natural Language\nProcessing (NLP), for example for writing sugges-\ntions, (Wambsganss et al., 2022a; Lauscher et al.,\n2018) conversational interaction (Ruan et al., 2019;\nSchmitt et al., 2021), or to support writing skills\nand learning at scale. Nevertheless, the increasing\nuse of LLMs for personalized support (especially\nin education) bears issues of critical concern. Re-\nsearch has found that these models can carry and\npropagate significant bias (Bolukbasi et al., 2016;\nSun et al., 2019). LLMs, at any stage in their devel-\nopment pipeline, can harbor and propagate biases\n(e.g., gender, racial, or conceptual) and thus reflect\nthe data they are trained on (Baker and Hawn, 2021;\nHutchinson and Mitchell, 2019; Sun et al., 2019).\nSuch biases, specifically gender bias, in LLMs, can\nreinforce harmful stereotypes in automated writing\nsupport applications, for example in automated es-\nsay scoring (Östling et al., 2013; Yannakoudakis\net al., 2011) or individual writing support (Wamb-\nsganss et al., 2022b), and thus can inadvertently\ninfluence students’ writing styles and perspectives\n(Su et al., 2023; Lee et al., 2022a).\nPrevious research has explored the existence of\nbias in different language models and their rep-\nresentations, focusing on the English language\n(Baker and Hawn, 2021). However, a growing body\nof literature advocates for the necessity of conduct-\ning comprehensive bias analyses, i.e. analyzing\nthe impact of language models when embedded in\nhuman educational tasks (Lee et al., 2022b; Baker\nand Hawn, 2021; Blodgett et al., 2020). Especially\nfor gender stereotypes, recent studies such as An-\ndersson et al. (2021) and Cheng et al. (2022) have\ndemonstrated the effects of such stereotypes on\nCV screening and child welfare programs, respec-\ntively. Nevertheless, detailed examinations of the\neffects of LLM-based writing suggestions on stu-\ndents and their use of gender stereotypes have been\nrather sparsely investigated, especially in other lan-\n10275\nguages than English (Baker and Hawn, 2021; Lee\nand Kizilcec, 2020). Given the expanding use of\nthese models in educational settings and for writ-\ning assistance in general, (e.g., (Lee et al., 2022a;\nChang et al., 2023), addressing this literature gap is\nof utmost importance. Understanding the nuanced\nways in which biases in LLMs can seep into edu-\ncational tasks can help researchers create safer and\nmore effective learning environments that promote\nequitable outcomes.\nIn this paper, we analyze how bias transfers\nthrough an AI writing support pipeline and we in-\nvestigate whether bias in LLMs translates into bias\nin human writing. We use an educational context,\nnamely German peer reviews collected from 231\nstudents divided into five groups: one group of\n52 students in a University classroom setting re-\nceiving feedback from a traditional feature-based\nrecommender system (G0) and four groups ( 179\nstudents in total) recruited through Prolific. Groups\nG1-G4 include a control group receiving no writing\nsupport (G1), two groups with suggestions from\nGPT-2 (G2) and GPT-3 models (G3) fine-tuned on\nan extended version of the non-biased corpus of\nWambsganss et al. (2022b) containing 11, 925 peer\nreviews in German, and one group with suggestions\nfrom pre-trained GPT-3.5 (G4).\nWe apply the GenBit gender bias test Bordia and\nBowman (2019), the German adaptation Kurpicz-\nBriki (2020) of the Word Embedding Association\nTests (WEAT) (Caliskan et al., 2017), and Sentence\nEmbedding Association Test (SEAT) (May et al.,\n2019) translated to German to the collected peer\nreviews as well as to the suggestions provided by\nthe different LLMs and the embeddings of fine-\ntuned GPT-2 model. With our analyses, we aim to\nanswer the following two research questions:\n1. In a real-world peer review writing exercise\nwith AI writing support, does LLM bias trans-\nfer to student writing (RQ1)?\n2. How does bias transfer across the different\nstages (i.e., model embeddings, model sugges-\ntions, student output) of the AI writing support\npipeline (RQ2)?\nOur results reveal a promising trend: groups re-\nceiving suggestions from LLMs (G2-G4) exhibit\nno significant measurable difference in gender\nbias in their writing compared to the control group\nwithout writing support (G1) and the in-classroom\nstudents receiving recommender-based feedback\n(G0). Furthermore, none of the GenBit gender\nbias, WEAT tests and SEAT tests reveal biases in\nthe provided suggestions from any of the LLMs,\ndespite that the analysis of GPT-2 embeddings de-\ntects significant gender bias for the GPT-2 model.\nOur results therefore demonstrate that LLM-based\nwriting support can be positively used for specific\neducational scenarios without bias.\n2 Related Work\n2.1 Bias in Educational Writings\nResearch has analyzed bias in educational tech-\nnology since around the 1960s and many parts of\ntoday’s research on algorithmic bias and fairness\nhave been anticipated (Baker and Hawn, 2021). To\neffectively probe bias, it is imperative to establish\nour viewpoint, as the term \"bias\" is multifaceted\nand defined differently across various research\nworks (see literature reviews such as Hutchinson\nand Mitchell (2019); Baker and Hawn (2021)). In\nour study, we adopt the view of algorithmic bias\nas \"situations where model performance is sub-\nstantially better or worse across mutually exclusive\ngroups\" (Baker and Hawn, 2021, p. 4). LLMs, such\nas GPT-2, GPT-3, or BERT, have been increas-\ningly utilized for educational writing assistance\n(Mirowski et al., 2023; Gero et al., 2022). These\nmodels, trained on extensive and diverse data, have\nproven instrumental in predicting subsequent text,\nthereby producing coherent responses (Lee et al.,\n2022a,b). Research has studied the risk that they\nmight reflect by investigating the biases inherent\nin the training data and the models (Kurpicz-Briki,\n2020). We aim to scrutinize the impact of bias\nfrom LLMs on student writings, thereby focusing\non the lower end of the NLP pipeline, investigating\nthe impact of writing suggestions on educational\ndownstream tasks as Lee et al. (2022b) suggested.\n2.2 NLP Bias Analysis\nResearch within the field of computational linguis-\ntics and NLP has progressively delved into bias\npresent in language systems. This includes investi-\ngations into bias in areas such as embedding spaces\n(Caliskan et al. (2017); Bolukbasi et al. (2016)),\nlanguage modeling (Lu et al., 2018), co-reference\nresolution (Rudinger et al., 2017), machine transla-\ntion (Stanovsky et al., 2019), and sentiment anal-\nysis (Kiritchenko and Mohammad, 2018). Sun\net al. (2019) have investigated strategies to mitigate\ngender bias across various NLP tasks. A variety\n10276\nof methods have been proposed for the detection\nof gender bias in text. These range from multi-\ndimensional classifications of gender bias (Dinan\net al., 2020) to the exploration of the frequency\nof gender bias metrics using word embeddings\n(Valentini et al., 2022). Some studies have even\nanalyzed human gender stereotypes via word as-\nsociation tests (Du et al., 2019). Instruments such\nas the Word Embedding Association Test (WEAT)\n(Caliskan et al., 2017) are commonly utilized for\nbias identification, enabling the quantification of\nbiases within word embeddings by assessing the\nstrength of correlations between target words and\nattribute words (Du et al., 2019). WEAT is com-\nposed of different tests that aim to reveal racial,\nconceptual, and gender bias. WEAT tests 6, 7, and\n8 have been also used to investigate gender bias in\nGerman texts (Kurpicz-Briki, 2020). Furthermore,\nBordia and Bowman (2019) proposed a test to de-\ntect gender bias in word-level language models and\nsuggested a bias score that has been widely used in\nthe NLP community (e.g., Sengupta et al. (2021)).\nFor sentence-level bias analysis, there is the Sen-\ntence Embedding Association Test (SEAT) (May\net al., 2019) which extends WEAT to measure bias\nin sentence encoders.\n2.3 Studies on Bias in Writing Assistance\nNevertheless, studies that investigate biases in\ndownstream applications and particularly the in-\nfluence of LLMs on human writing are rather rare.\nWhile Lee et al. (2022b) have proposed a frame-\nwork for investigating the impact of LLMs on hu-\nman writing, they have only motivated and not in-\nvestigated the impact of toxicity and bias on human\ntexts after receiving writing assistance. Further-\nmore, studies in the educational domain involving\nreal students especially outside of North America\nremain limited (Baker and Hawn, 2021; Sun et al.,\n2019). Our work centers on the impact of writ-\ning suggestions provided by LLMs on students’\ngender stereotypes. Our goal extends past work\non revealing bias in the educational NLP pipeline\n(Wambsganss et al., 2022b) and on human eval-\nuation (Lee et al., 2022b) by shedding light on\nthe impact of potentially biased LLMs on students.\nWe do so by investigating the case of student peer\nreviews in the German language since this is a\ndomain-independent and increasingly embedded\neducational context fostering writing competencies\nin large-scale learning scenarios. With this, we aim\nto contribute towards shaping a future where NLP\nresearchers and practitioners are aware of biases of\ndownstream educational models and hence strive\nto minimize the potential harm by those models\nwhen applied in sensitive contexts like education,\npotentially involving sensitive user groups (such as\nunder-aged students).\n3 Methodology\nTo investigate the impact of LLMs on gender biases\nin human writing within an educational context, we\nperformed a bias analysis in three steps (see Figure\n1). In a first step, we trained a feature-based recom-\nmender system and fine-tuned GPT-2 and GPT-3\nmodels on a corpus of 11, 925 student-written busi-\nness peer reviews in German to be able to provide\nautomated suggestions in a peer review exercise.\nIn a second step, we conducted a large-scale user\nstudy with 231 students in a classroom and an on-\nline context, providing students with different lev-\nels of writing support (feature-based recommender,\nno support, suggestions by fine-tuned GPT-2 or\nGPT-3, suggestions by pre-trained GPT-3.5) in a\npeer review writing exercise in German. In the\nthird and final step, we used the GenBit Gender\nBias Score (Bordia and Bowman, 2019), WEAT,\nand SEAT tests to analyze the gender bias of the\nresulting reviews, the model suggestions, as well\nas the GPT-2 embeddings.\n3.1 Model Development\nIn the FUSM group (G0), students received auto-\nmated advice based on the Feature Utility Satura-\ntion Model (FUSM). This model is trained on a\ncorpus of 9, 000 student-written peer reviews in\nGerman. The recommender system predicts the\nutility of specific types of feedback in improving\nthe review quality (Bauman et al., 2020). A person-\nalized subset of 3 out of 15 features, such as adding\nsuggestions, decreasing sentence length, and writ-\ning more directive words, is provided upon clicking\nthe get advice button. The features have been de-\nrived based on existing literature on peer feedback\nand a dictionary with keywords was developed to\noperationalize the features. We followed Bauman\net al. (2020) for the implementation of our model.\nTo provide students with adaptive writing sug-\ngestions, we fine-tuned GPT-2 and GPT-3 models\n(used by groups G2 and G3) and embedded GPT-\n3.5 in the peer review tool (used by group G4).\nThe two models for G2 and G3 were fine-tuned on\n10277\nRecommender\nsystem suggestions\n(G0 )\nFine-tuned\nGPT-2 generated\nsuggestions (G2 )\nFine-tuned\nGPT-3 generated\nsuggestions (G3 )\nGPT-3.5 generated\nsuggestions (G4 )\nWEAT Tests\n(Gender)\nGenBit\nGender Bias\nScores\nCorpus \nstudent-written \nbusiness peer\nreviews in\nGerman\nInfinite\ncontext\nFixed\ncontext\nTest 6: male &\nfemale names\nTest 7: math & art\nTest 8: science & art\nWindow size from 10 to 60\nIntroduction to a peer's\nbusiness model\nPost-survey\nGPT-3\nPeer review writing\nin German\nFeature-\nbased FUSM\nmodel\nGPT-2\nNo AI CG (G1 )\nClassroom Study\nOnline Study (Prolific)\nCorpus \n1 1,925 \n9,000 \nData Processing Modeling Data Collection & Data Processing\nModel Development Gender Bias AnalysisStudy Design\nProcedureParticipants\nSEAT Tests\n(Gender)\nData Analysis\nFigure 1: Overview of our pipeline. We first prepared machine learning models as well as LLMs to provide writing\nsuggestions for learners, see Section 3.1 for more details. We then conducted a user study with a peer review\nwriting exercise. Section 3.2 presents the whole study design including the procedure and all five groups finished\nthe exercise with different writing assistance tools. We provided details on data collection as well as data processing.\nFinally, we analyzed gender bias using the GenBit Bias Score, WEAT and SEAT tests (see Section 3.3).\nan extended version of the non-biased corpus of\npeer reviews in German reported by Wambsganss\net al. (2022b). The data was collected over four\nyears and includes 11,925 reviews from610 unique\nreviewers and 607 reviewees. Students wrote ap-\nproximately nine peer reviews per course with an\naverage length of 220 words. This extensive corpus\nserved as a solid foundation for fine-tuning models\nand preparing them to provide writing suggestions\nto students. We started data processing by expand-\ning abbreviations, removing HTML tags, irrelevant\ninformation like PDF file names and specific infor-\nmation like URLs, keywords (revealing the identity\nof students), and questions asked to write reviews\nwhich some students copied to their review text.\nThen, we shuffled and divided the cleaned data\ninto train and test datasets with proportions of 0.8\nand 0.2 for fine-tuning and evaluating the language\nmodels.\n3.2 Study Design\nParticipants. Participants of the user study were\n231 students who were split into five distinct groups\n(G0 - G4), controlling for the sensitive variables of\neducation level, language, age, and gender. While\nthere might be (small) observable differences in the\nsample means or medians for sensitive variables,\nadditional statistical analysis confirms that there are\nno significant differences between groups and that\nthe randomization has worked correctly. For the\nage attribute, a Shapiro-Wilk test indicates that the\ndata is not normally distributed and we have there-\nfore conducted a non-parametric Kruskal-Wallis\ntest, which confirms that there are no significant\ndifferences between groups G1-G4 in terms of age\n(H = 2.24, p = .52). Furthermore, a Chi-Square test\nconfirms that there are also no significant differ-\nences between the four Prolific groups in terms of\ngender (X2 = 0.0149, p = .99) or education level\n(X2 = 6.65, p = .35). Each group was provided\nwith writing suggestions from a unique model: G1\nreceived suggestions from a feature-based recom-\nmender system, for example, on text length or sen-\ntiments, G2 received suggestions from a fine-tuned\nGPT-2 model, G3 from a fine-tuned GPT-3 model,\nand G4 from pre-trained GPT-3.5. Table 1 provides\nan overview of each group and the demographics\nof the participants.\nProcedure. In order to control for the content as-\npect (as the content of the provided business model\ncould inadvertently have an influence on the ex-\npected bias), we present each participant with ex-\nactly the same predefined business model.\n10278\nContext Group # Gender Age Highest Education\nMale Female Mean Std. Below BSc BSc and above\nClassroom FUSM (G0) 52 62.0% 38.0% 25.7 1.9 0.0% 100%\nOnline\nNone (G1) 40 50.0% 50.0% 30.0 8.3 30.0% 70.0%\nGPT-2 (G2) 50 50.0% 50.0% 28.3 8.5 22.0% 78.0%\nGPT-3 (G3) 44 50.0% 50.0% 29.9 12.6 34.1% 65.9%\nGPT-3.5 (G4) 45 54.3% 45.7% 30.9 12.3 39.1% 60.9%\nTable 1: Overview of the data sample and participant demographics of our user study across five groups.\nStudents in the FUSM group (G0) received writ-\ning suggestions through a dashboard next to the\ntext input in the form of syntactical and semantic\nadvice. We collected data in a lecture at a Western\nEuropean university where 52 students were writ-\ning up to three reviews on a peer’s business model\nand they went through three peer feedback rounds.\nThe data of the three LLMs groups (G2-G4),\nas well as the control group (G1), was collected\nthrough the online platform Prolific 2 in order to\nnot cause potential harm to real-world students.\nThe task in the Prolific study involved the partic-\nipants writing a review on a pre-defined business\nmodel. Specifically, participants were asked to\nelaborate on strengths, weaknesses, and sugges-\ntions for improvement of the given business model.\nWe instructed people not to use search engines and\nspend a minimum of 15 minutes on the task. A\ncountdown indicated the remaining time. Students\nreceived different forms of writing support on that\ntask. Specifically, we followed the interface of the\nhuman-centric educational tool of Su et al. (2023)\n(see Figure 4 in the appendix) to ensure that stu-\ndents received beneficial writing aid with a satis-\nfactory user experience. We presented users with\nnext-sentence predictions, providing three sugges-\ntions at each point in time. Students could use the\nTab key to accept a suggestion, the Esc key to re-\nject a suggestion, and the Up or Down arrow keys\nto toggle through different suggestions. During\nthe writing process, we collected the final writings,\nsuggestions, as well as keystrokes of participants.\nWe cleaned the collected data by removing\nHTML tags, emojis, punctuation, abbreviations,\ndigits, and stop words (excluding words in the gen-\nder lists 3). The text was transformed to lower-\n2www.prolific.co\n3https://github.com/epfl-ml4ed/\nunraveling-llm-bias/tree/main/GenBit/gender%\n20list\ncase. Because of the linguistic characteristic of\nthe German language and its grammatical genders,\nspecial words like Ihr, Ihrem, Ihren, Ihrer, Ihres,\nand Sie have both gendered meanings (“she\") and\nnon-gendered meaning (e.g. “you\"). We filtered\nthem by checking their position in the sentence. If\nthey related to the polite form (Höflichkeitsform),\nwe removed them to not inflate the bias score in the\nlater calculation. Otherwise, we kept them. After-\nward, we did a human evaluation. Three German\nresearchers checked the outputs manually to con-\nfirm the quality of the processed data.\n3.3 Gender Bias Analysis\nTo investigate the bias in the resulting peer reviews,\nthe model suggestions, and the model embedding,\nwe utilized three different gender bias tests.\n3.3.1 GenBit Gender Bias Score\nThe GenBit Gender Bias Score was introduced by\nBordia and Bowman (2019) and is included in the\nMicrosoft Responsible AI Toolkit (Sengupta et al.,\n2021). The idea of the method is to identify gen-\nder bias by measuring the association between pre-\ndefined gendered words and other words in the\ncorpus via co-occurrence statistics.\nBias score definition. To compute the bias score,\nwe first estimate the probability of a word occurring\nin the context of gendered words within a text cor-\npus. The probability P(w|g) indicates how likely\nit is for a specific word, denoted as w, to appear\nnear (within a context window k) gendered words,\ndenoted as g:\nP(w|g) = countk(w, g)∑\ni countk(wi, g)\nwhere w is any word in the corpus, excluding stop\nwords and gendered words, countk(w, g) repre-\nsents the count of occurrences where the gendered\n10279\nword g appears in the context window k of the tar-\nget word w and ∑\ni countk(wi, g) calculates the\ntotal count of occurrences where any word wi ap-\npears in the context windowk of gendered words g.\nFinally, g is the set of gendered words that belong\nto either of the two categories: male or female. In\nother words, we count how many times the word\nw and at least one gendered word from the set\ng appear within a certain distance of each other.\nIn terms of defining gender pairs, we adopted the\nsame set of gender term pairs as Sengupta et al.\n(2021). Additionally, given the linguistic character-\nistic of the German language, we added gendered\npronouns to better capture the grammatical gender.\nFor pronouns with ambiguous meanings, we did a\nfiltering process in the data processing stage under\nthe instruction of three German researchers.\nThe bias score of a specific word w is then de-\nfined as:\nbias(w) = log(P(w|m)\nP(w|f) )\nThis bias score is measured per word and review.\nA positive bias score implies that a word co-occurs\nmore often with male words than female words.\nWindows size and context window. For a con-\ntext size k, there are k/2 words before and k/2 words\nafter the target word w for which the bias score is\nbeing measured. Qualitatively, a smaller context\nwindow size has more focused information about\nthe target word. On the other hand, a larger window\nsize captures topicality (Levy and Goldberg, 2014).\nAccording to Bordia and Bowman (2019), the opti-\nmal window size for fixed context isk = 20, which\nassigns an equal weight of 5% to the ten words be-\nfore and the ten words after the target word. For\nan infinite context, weights diminish exponentially\nbased on the distance between the target word w\nand the gendered word g. This method emphasizes\nthe fact that the nearest word has more information\nabout the target word.\n3.3.2 Word Embedding Association Test\nTo assess bias along the NLP pipeline suggested by\nHovy and Prabhumoye (2021), we rely on the Word\nEmbedding Association Test (WEAT) proposed by\nCaliskan et al. (2017). WEAT calculates the se-\nmantic similarity between two sets of target words\n(e.g., male vs. female names) and two sets of at-\ntribute words using word embeddings (e.g., career\nvs. family). Table 6 in the appendix indicates the\nnine WEAT tests and their corresponding targets\nand attributes, which we translated into German.\nOur analysis focuses on the gender bias dimension\nof WEAT using tests 6, 7, and 8.\nTo quantitatively compare across WEAT analy-\nses, we use the effect size as proposed by (Caliskan\net al., 2017). Effect size is a normalized measure\nof the distance between the two distributions of\nassociations and targets, calculated as follows:\n1\n|X|\n∑\nx∈X s(x, A, B) − 1\n|Y |\n∑\ny∈Y s(y, A, B)\nSw∈X∪Y (s(w, A, B))\nwhere X and Y are two sets of target words of\nequal size, A, B are two sets of attribute words,\ns(w, A, B) measures the association of embed-\ndings of the target word w with the attribute words,\nand S denotes the standard deviation. We conduct\nWEAT analyses at two granularities as proposed by\n(Wambsganss et al., 2022b): in the raw text corpora\n(through co-occurrence and GloVE models) and in\nthe fine-tuned language model.\n3.3.3 Sentence Embedding Association Test\nIn addition to word-level metrics like GenBit and\nWEAT, we have sentence-level metrics, the SEAT\ntest, for a more comprehensive analysis. This test\nwas defined in May et al. (2019) and implemented\nin Meade et al. (2022). We used the implementa-\ntion from Fairpy (Viswanath and Zhang, 2023), an\nopen source Toolkit for measuring and mitigating\nbiases in large pre-trained language models. By\napplying WEAT to the vector representation of a\nsentence, SEAT compares sets of sentences instead\nof sets of words. In the ideal case, the embedding\nrepresentation of each word in the vocabulary is\nexpected to be equidistant from the two attribute\nclasses. Any deviation suggests bias in one direc-\ntion. The greater the deviation, the greater the bias\n(Viswanath and Zhang, 2023). Same as WEAT\ntests, our analysis focuses on the gender bias di-\nmension of SEAT using tests 6, 6b, 7, 7b, 8, 8b.\nHowever, to the best of our knowledge, there is no\navailable German version of SEAT tests. We first\ntranslated the templates from English to German\nwith the translation software, then the translations\nwere revised by two native German speakers. To\nfacilitate future work, we publicly provided the\ntranslated files 4.\n4https://github.com/epfl-ml4ed/\nunraveling-llm-bias/tree/main/SEAT/translated%\n20de\n10280\n4 Results\nTo evaluate whether bias transfers from AI assis-\ntants to students for a real-world writing support\nscenario (RQ1) and to investigate where in the\npipeline the bias persists (RQ2), we applied the\nGenBit Bias score as well as the WEAT tests to the\nfive different subsets from our user study: four from\nthe Prolific user study (control group G1, GPT-2,\nGPT-3, and GPT 3.5 assisted reviews G2-G4) and\none from the classroom study with recommender\nsystem suggestions (G0).\n4.1 RQ1 - Does Bias Transfer?\nGroup # GenBit Bias Score\nFUSM (G0) 310 -0.024 ±0.275\nControl (G1) 40 0.065 ±0.487\nGPT-2 (G2) 50 -0.099 ±0.486\nGPT-3 (G3) 44 0.115 ±0.570\nGPT-3.5 (G4) 45 -0.058 ±0.592\nTable 2: Total number of reviews from each group (#)\nand statistical summary of the GenBit bias score for\nall groups. The large standard deviations indicate the\nvariability of the range of bias scores. Traditional ML\nfeedback (G0) has the lowest variability, while the LLM\nfeedback (G2-G4) and the control group with no feed-\nback (G1) have similar ranges of variability.\nTo answer our first research question, we com-\nputed the GenBit Bias Score for the peer reviews\nwritten by the students. The goal was to iden-\ntify which biases in the models transfer to humans\nthrough collaboration. We experimented with both\na fixed context for windows sizes ranging from 10\nto 60 as well as an infinite context. Since there\nwas no significant difference between the results\nfrom the fixed context and infinite context, and the\noptimal window size was determined to be 20 by\nprevious work Bordia and Bowman (2019), in this\nsection, we only present results for a fixed context\nwith a window size of 20. The resulting GenBit\nbias scores are illustrated in Figure 2. The total\nnumber of reviews from each group and statistical\nsummary of bias scores for all groups are presented\nin Table 2. We observe that all five groups exhibit\nbias scores close to 0. Additional results are pro-\nvided in the appendix in Section B.\nTo analyze differences between the bias scores\nof the four Prolific groups (G1-G4), we first ag-\ngregated the mean bias scores of each review from\nthese groups. The results of a Shapiro-Wilk test\nFigure 2: GenBit gender bias score of human-written\nreviews for a fixed context with a window size of 20.\n(Shapiro and Wilk, 1965) then showed that our data\nwas not normally distributed. Additionally, a Lev-\nene test (Levene, 1960) confirmed heteroscedas-\nticity within the data. Therefore, we chose the\nnon-parametric Mann-Whitney U test (MWU) to\nanalyze the difference in mean bias scores between\nany pairing of two groups and the Kruskal-Wallis\ntest (Kruskal and Wallis, 1952) for the difference\namong all four groups.\nTable 3 presents the results of statistical tests\nanalyzing the mean bias scores of reviews written\nwith and without suggestions from LLMs. We\ndid not find any statistically significant difference\nbetween the bias scores of the four groups.\nGroup\np-value\nMWU Test\nGPT-2\n(G2)\np-value\nMWU Test\nGPT-3\n(G3)\np-value\nMWU Test\nGPT-3.5\n(G4)\nControl (G1) 0.170 0.619 0.551\nGPT-2 (G2) - 0.075 0.635\nGPT-3 (G3) - - 0.269\nTable 3: Mann-Whitney U (MWU) test of bias scores\nfor reviews from multiple groups. Asterisks indicate sta-\ntistical significance (***: p<.001; **: p<.01; *: p<.05).\nThere is no statistically significant difference between\nthe bias scores of the four groups.\n4.2 RQ2 - Where is Bias Present?\nTo answer our second research question and inves-\ntigate where bias is present in the pipeline, we used\nthe GenBit Gender Score, WEAT tests, and SEAT\ntests to analyze the LLMs’ suggestions and (where\navailable) the model’s raw embeddings. This in-\n10281\nMale NamesFemale Names\nWEAT Test 6\nWEAT Difference Scores over Attributesfor each Target\n(Career) 0.2\n(Family) -0.1\n(Neutral) 0.0\nWEAT Test 7WEAT Test 8(Male) 0.2\n(Female) -0.1\n(Neutral) 0.0 (Neutral) 0.0\n(Male) 0.2\n(Female) -0.1MathArt ScienceArt\n0.1 0.1 0.1\nFigure 3: Comparing WEAT bias test scores for the fine-tuned GPT-2 model used in the human evaluation study.\ndepth analysis allowed us to dig deeper into the\npotential roots of bias within the suggestions made\nby the models and to further understand how these\nmay have influenced the students’ writings.\nPost-Hoc Analysis of Suggestions. We mea-\nsured the bias in the raw text corpora of model\nsuggestions from fine-tuned GPT-2, GPT-3, and\npre-trained GPT 3.5 with three methods: Gen-\nBit bias scores, WEAT co-occurrence tests, and\nWEAT GloVE embedding tests. In each of these\ntests, we did not identify any significant bias in\nthe suggestions. We pre-filtered suggestions that\nhad more than 10 words to measure bias. On aver-\nage, there were 13 −15 words in each suggestion\nacross groups. For a fixed context with a window\nsize of 10, the bias scores for fine-tuned GPT-2\nand GPT-3 were 0.000092 and 0 respectively, and\n0.000062 for GPT-3.5. In the GloVE embedding\nmodel trained on each text subset, we identified one\ninstance of bias in GPT-2 suggestions in the class-\nroom study for WEAT test 7 (examining female\nvs. male targets for math vs. art attributes), but\nthis was considered insignificant. Overall, through\nthe WEAT tests of co-occurence and embedding\nmodels, we determined that the raw text corpora\nacross the study were unbiased.\nPost-Hoc Analysis of Model Embeddings. To\ndive deeper into the bias behavior of a fine-tuned\nmodel used for writing assistance, we examined\nthe gender bias of the fine-tuned GPT-2 model em-\nbeddings in both word-level and sentence-level. As\nshown in Figure 3 (left), we identified a notable\nbias in the fine-tuned model that was not present in\nthe raw text corpora. In the WEAT Test 6, we ex-\namined attributes of Career vs. Family in relation\nto Male vs. Female names. We obtained an effect\nscore of 1.57 (with effect scores ranging from −2\nto 2), indicating that the career attribute is much\ncloser to Male than Female names.\nWEAT Test 7 focuses on the target disciplines\nof Math and Art, while examining Male and Fe-\nmale as attribute lists. The difference scores in\nFigure 3 (middle) showed minimal disparity, show-\ning a slight bias in favor of the male attribute for\nboth Math and Art targets. The five most similar\nattribute words to the Math target list, translated\nfrom German to English, (young, son, brother, mas-\nculine, and sister) depicted a more extreme picture\nwith 4 out of 5 male-oriented words, aligned with\nthe 5 most similar attributes to Art (young, brother,\nson, man, and daughter). The effect size was 0.27,\nshowing that Math is slightly more aligned to the\nMale attribute than the Art target.\nIn WEAT Test8, we examined the relationship\nbetween Science and Art targets and Male and Fe-\nmale attribute lists (see Figure 3 (right)). Again,\nwe identified similarities in the top five attribute\nwords with a strong male bias in the most related\nwords. For the Science target, (uncle, son, father,\ndaughter, and brother) were the five most similar\nattributes (four out of five male-oriented). For the\nArt target, the most similar attributes were (uncle,\nbrother, son, father, and daughter) (again four out\nof five male-oriented). The effect size for this test\nwas −0.56, indicating that Art is more related to\nmale attributes than Science.\nSEAT test results are summarized in Table 4,\nacross tests 6, 6b, 7, 7b, 8, and 8b, which corre-\nspond to the analogous gender tests of WEAT, there\nis no significant difference found between the em-\nbeddings of the target sentences and attribute sen-\ntences (i.e. in SEAT 6, male and female names are\nboth equally similar to career and family words),\n10282\n# Tragets Attributes Effect size\n6 Male vs. Female Names Career vs. Family 0.021\n6b Male vs. Female Terms Career vs. Family -0.074\n7 Math vs. Arts Male vs. Female Terms -0.705\n7b Math vs. Arts Male vs. Female Names -0.209\n8 Science vs. Arts Male vs. Female Terms -0.069\n8b Science vs. Arts Male vs. Female Names 0.078\nTable 4: SEAT gender bias analysis for the fine-tuned GPT-2 model used in the human evaluation study.\ncomputed using p-values and a hypothesis test as\nper the standard implementation. Each test also\nhas an effect size, ranging from -2 to +2, with 0\nrepresenting a completely neutral effect between\nboth target words and attribute words. Most effect\nsizes are within 0.1, so there are only minimal bias\neffects. The comparatively largest effect (SEAT\ntest 7) is -0.705 from comparing Math and Arts in\nassociation with Male and Female terms, but this\nwas still found to have insignificant bias. These\nresults are in line with the WEAT analysis on the\nword level, which also found no significant gender\nbias in the model embeddings.\nInterestingly, as demonstrated by our analyses,\nthe gender biases revealed in GPT-2 embeddings\ndid not translate into gender biases in suggestions.\n5 Discussion and Conclusion\nLLMs are increasingly used in educational settings,\ndespite that they harbor inherent biases which may\nhave a negative impact on learners. Our work ana-\nlyzed how bias transfers through an AI writing sup-\nport pipeline in an educational context and whether\nthe bias in LLMs translates to bias in students’\nwritings. To do so, we conducted a large-scale user\nstudy, providing students with different levels of\nwriting support in a peer review writing exercise.\nOur analysis of data collected from in-classroom\nand Prolific participants yielded positive results\nthat provide optimism for the field of NLP and its\napplication within educational contexts. The most\nnotable finding was that students who received writ-\ning suggestions from LLMs exhibited the same\ndegree of gender bias in their written text as stu-\ndents who received no suggestions. The group\nreceiving suggestions from a recommender system\nwith human interpretable features also showcased\na similar amount of bias in student responses. Our\nresults seem initially promising, showing that by\nmeasuring gender bias through multiple tests (Gen-\nBit, WEAT, and SEAT) at each stage of the pipeline\n(model embeddings, model suggestions, student\noutput), LLMs do not inadvertently foster and per-\npetuate gender bias. One possible explanation for\nthese results is that the biases present in the original\ntraining data and embeddings of the LLMs were not\ntransferred to the model suggestions, as indicated\nby the lack of bias in model suggestions. Unbiased\nsuggestions led to positive learning outcomes for\nstudents without an inadvertent bias transfer.\nOur post-hoc deep dive into the bias dimensions\nof the suggestions and the GPT-2 embeddings re-\nvealed the inherited bias in the fine-tuned models.\nThis analysis provided valuable insight into how\nbiases can become ingrained in LLMs and also\nidentified where in the pipeline the bias stops. It\nsuggests that even when models are fine-tuned with\nthe intention of reducing bias, their original train-\ning on vast amounts of data from the internet can\nleave an indelible imprint of bias. An important\ntakeaway from our study is that the applied domain\nof education, although it is often a sensitive context,\nis moving towards integration with LLM assistants.\nBefore we can be fully confident of model impacts\non sensitive young minds for downstream tasks, we\nneed to strive for not only more sophisticated bias\ndetection and mitigation techniques but also more\ntransparency in how these models are trained and\nfine-tuned. Also, future research is needed to better\ninvestigate the impact of potentially biased LLMs\nin different educational settings in addition to writ-\ning, for example, language learning (Xiao et al.,\n2023), STEM education (Lee and Perret, 2022),\nand legal education (Weber et al., 2023).\nIn conclusion, our study contributes towards a\nmore nuanced understanding of how LLMs inter-\nact with bias for educational tasks, and produces\nthe indication that although models contain bias,\npersonalized downstream applications might not.\nWe hope that our findings will stimulate further\nresearch, contributing to the UN’s fourth sustain-\nability goal of ensuring quality education for all.\n10283\nLimitations\nWhile our study provides crucial insights into the\nimpact of LLMs’ potential bias on human writing,\nwe acknowledge the need for further research. It\nwould be insightful to extend our study to other gen-\nerative language models, study populations (differ-\nent student levels, different languages), and educa-\ntional tasks to paint a more comprehensive picture\nof the complex dynamics at play.\nThe inability to have access to several of the\nadvanced model embeddings, unfortunately, pro-\nhibited us from conducting a WEAT analysis of\nGPT-3 and 3.5; we make the informed assumption\nthat bias exists in the embeddings as per previous\nwork, but we cannot measure or compare it with our\nGPT 2 findings. Additionally, our educational task\n(a business case study regarding ski instruction) is\nneutral and does not easily lend itself to measuring\ngender bias, compared to other potential settings.\nIt was selected because it is a real-world education\nexample integrated currently in a Western Euro-\npean university. If the case study was regarding\nmedicine, for example, the male and female words\nfor the doctor would be different in German, and\nthis could potentially lead to a different bias mea-\nsurement. Other dimensions of bias evaluated in\nLiang et al. (2023); Lin and Ng (2023) (e.g., racial,\ncognitive) would also need to be studied before\nLLMs for writing support can be fully trusted in\nthe classroom.\nEthics Statement\nWe note that this research was conducted by a\nmixed team of authors with Western European,\nAsian, North-American, Middle Eastern, female,\nand male backgrounds. All student data was\nanonymized in the use of this study and the study\nwas approved by the human research ethics com-\nmission of the university. While our results in-\ndicated that LLM-based AI writing support does\nnot have adverse effects of bias transfer on student\nwriting, our work is not a generalizable study over\nmany student populations and many different exer-\ncise settings. We present a case study over several\nmodel types and a positive finding regarding class-\nroom integration, but we still encourage caution\nand experimentation before integrating models in\nthe classroom.\nReferences\nEmma Rachel Andersson, Carolina E. Hagberg, and\nSara Hägg. 2021. Gender bias impacts top-merited\ncandidates. Frontiers in Research Metrics and Ana-\nlytics, 6.\nRyan S Baker and Aaron Hawn. 2021. Algorithmic\nbias in education. International Journal of Artificial\nIntelligence in Education, pages 1–41.\nKonstantin Bauman, Roman Rietsche, and Matthias\nSöllner. 2020. Supporting Students in the Peer-\nReview Process by Recommending Features of Writ-\nten Feedback that Should be Improved . Statisti-\ncal Challenges in Electronic Commerce Research\n(SCECR), Virtual Conference.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. Advances in\nneural information processing systems, 29.\nShikha Bordia and Samuel R. Bowman. 2019. Identify-\ning and reducing gender bias in word-level language\nmodels. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Student Research Workshop,\npages 7–15, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan.\n2017. Semantics derived automatically from lan-\nguage corpora contain human-like biases. Science,\n356(6334):183–186.\nMinsuk Chang, John Joon Young Chung, Katy Ilonka\nGero, Ting-Hao Kenneth Huang, Dongyeop Kang,\nMina Lee, Vipul Raheja, and Thiemo Wambsganss.\n2023. The second workshop on intelligent and in-\nteractive writing assistants. In Extended Abstracts\nof the 2023 CHI Conference on Human Factors in\nComputing Systems, CHI EA ’23, New York, NY ,\nUSA. Association for Computing Machinery.\nHao-Fei Cheng, Logan Stapleton, Anna Kawakami,\nVenkatesh Sivaraman, Yanghuidi Cheng, Diana Qing,\nAdam Perer, Kenneth Holstein, Zhiwei Steven Wu,\nand Haiyi Zhu. 2022. How child welfare workers\nreduce racial disparities in algorithmic decisions. In\nCHI Conference on Human Factors in Computing\nSystems, pages 1–22.\nEmily Dinan, Angela Fan, Ledell Wu, Jason Weston,\nDouwe Kiela, and Adina Williams. 2020. Multi-\ndimensional gender bias classification. In Proceed-\nings of the 2020 Conference on Empirical Methods\n10284\nin Natural Language Processing (EMNLP) , pages\n314–331, Online. Association for Computational Lin-\nguistics.\nYupei Du, Yuanbin Wu, and Man Lan. 2019. Exploring\nhuman gender stereotypes with word association test.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 6133–\n6143, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKaty Ilonka Gero, Vivian Liu, and Lydia Chilton. 2022.\nSparks: Inspiration for science writing using lan-\nguage models. In Designing Interactive Systems Con-\nference, DIS ’22, page 1002–1019, New York, NY ,\nUSA. Association for Computing Machinery.\nDirk Hovy and Shrimai Prabhumoye. 2021. Five\nsources of bias in natural language processing. Lan-\nguage and Linguistics Compass, 15(8):e12432.\nBen Hutchinson and Margaret Mitchell. 2019. 50 years\nof test (un)fairness: Lessons for machine learning. In\nProceedings of the Conference on Fairness, Account-\nability, and Transparency, FAT* ’19, page 49–58,\nNew York, NY , USA. Association for Computing\nMachinery.\nErik Jones and Jacob Steinhardt. 2022. Capturing fail-\nures of large language models via human cognitive\nbiases. In Advances in Neural Information Process-\ning Systems, volume 35, pages 11785–11799. Curran\nAssociates, Inc.\nSvetlana Kiritchenko and Saif Mohammad. 2018. Ex-\namining gender and race bias in two hundred senti-\nment analysis systems. In Proceedings of the Sev-\nenth Joint Conference on Lexical and Computational\nSemantics, pages 43–53, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nWilliam H Kruskal and W Allen Wallis. 1952. Use of\nranks in one-criterion variance analysis. Journal of\nthe American statistical Association , 47(260):583–\n621.\nMascha Kurpicz-Briki. 2020. Cultural differences in\nbias? origin and gender bias in pre-trained german\nand french word embeddings.\nAnne Lauscher, Goran Glavaš, and Kai Eckert. 2018.\nArguminSci: A tool for analyzing argumentation and\nrhetorical aspects in scientific writing. In Proceed-\nings of the 5th Workshop on Argument Mining, pages\n22–28, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nHansol Lee and René F Kizilcec. 2020. Evaluation\nof fairness trade-offs in predicting student success.\narXiv preprint arXiv:2007.00088.\nIrene Lee and Beatriz Perret. 2022. Preparing high\nschool teachers to integrate ai methods into stem\nclassrooms. Proceedings of the AAAI Conference on\nArtificial Intelligence, 36(11):12783–12791.\nMina Lee, Percy Liang, and Qian Yang. 2022a. Coau-\nthor: Designing a human-ai collaborative writing\ndataset for exploring language model capabilities. In\nProceedings of the 2022 CHI Conference on Human\nFactors in Computing Systems, pages 1–19.\nMina Lee, Megha Srivastava, Amelia Hardy, John Thick-\nstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-\nUrsin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong,\net al. 2022b. Evaluating human-language model in-\nteraction. arXiv preprint arXiv:2212.09746.\nHoward Levene. 1960. Robust tests for equality of\nvariances. Contributions to probability and statistics,\npages 278–292.\nOmer Levy and Yoav Goldberg. 2014. Dependency-\nbased word embeddings. In Proceedings of the 52nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) , pages 302–\n308.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Alexander Cosgrove, Christo-\npher D Manning, Christopher Re, Diana Acosta-\nNavas, Drew Arad Hudson, Eric Zelikman, Esin\nDurmus, Faisal Ladhak, Frieda Rong, Hongyu Ren,\nHuaxiu Yao, Jue W ANG, Keshav Santhanam, Laurel\nOrr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun,\nNathan Kim, Neel Guha, Niladri S. Chatterji, Omar\nKhattab, Peter Henderson, Qian Huang, Ryan An-\ndrew Chi, Sang Michael Xie, Shibani Santurkar,\nSurya Ganguli, Tatsunori Hashimoto, Thomas Icard,\nTianyi Zhang, Vishrav Chaudhary, William Wang,\nXuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Ko-\nreeda. 2023. Holistic evaluation of language models.\nTransactions on Machine Learning Research. Fea-\ntured Certification, Expert Certification.\nRuixi Lin and Hwee Tou Ng. 2023. Mind the bi-\nases: Quantifying cognitive biases in language model\nprompting. In Findings of the Association for Com-\nputational Linguistics: ACL 2023, pages 5269–5281,\nToronto, Canada. Association for Computational Lin-\nguistics.\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-\ncharla, and Anupam Datta. 2018. Gender bias in\nneural natural language processing. In Logic, Lan-\nguage, and Security.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measuring\nsocial biases in sentence encoders. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 622–628, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy.\n2022. An empirical survey of the effectiveness of\n10285\ndebiasing techniques for pre-trained language models.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics , Online.\nAssociation for Computational Linguistics.\nPiotr Mirowski, Kory W. Mathewson, Jaylen Pittman,\nand Richard Evans. 2023. Co-writing screenplays\nand theatre scripts with language models: Evaluation\nby industry professionals. In Proceedings of the 2023\nCHI Conference on Human Factors in Computing\nSystems, CHI ’23, New York, NY , USA. Association\nfor Computing Machinery.\nRobert Östling, André Smolentzov, Björn Tyrefors Hin-\nnerich, and Erik Höglin. 2013. Automated Essay\nScoring for Swedish. Proceedings of the Eighth\nWorkshop on Innovative Use of NLP for Building\nEducational Applications, pages 42–47.\nSherry Ruan, Liwei Jiang, Justin Xu, Bryce Joe-Kun\nTham, Zhengneng Qiu, Yeshuang Zhu, Elizabeth L.\nMurnane, Emma Brunskill, and James A. Landay.\n2019. QuizBot: A Dialogue-based Adaptive Learn-\ning System System for Factual Knowledge. pages\n1–13.\nRachel Rudinger, Chandler May, and Benjamin\nVan Durme. 2017. Social bias in elicited natural\nlanguage inferences. In Proceedings of the First ACL\nWorkshop on Ethics in Natural Language Process-\ning, pages 74–79, Valencia, Spain. Association for\nComputational Linguistics.\nAnuschka Schmitt, Thiemo Wambsganss, Matthias Söll-\nner, and Andreas Janson. 2021. Towards a trust re-\nliance paradox? exploring the gap between perceived\ntrust in and reliance on algorithmic advice. In Inter-\nnational Conference on Information Systems (ICIS),\nvolume 1, pages 1–17.\nKinshuk Sengupta, Rana Maher, Declan Groves, and\nChantal Olieman. 2021. Genbit: measure and mit-\nigate gender bias in language datasets. Microsoft\nJournal of Applied Research, 16:63–71.\nSamuel Sanford Shapiro and Martin B Wilk. 1965. An\nanalysis of variance test for normality (complete sam-\nples). Biometrika, 52(3/4):591–611.\nGabriel Stanovsky, Noah A. Smith, and Luke Zettle-\nmoyer. 2019. Evaluating gender bias in machine\ntranslation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1679–1684, Florence, Italy. Association for\nComputational Linguistics.\nXiaotian Su, Thiemo Wambsganss, Roman Rietsche,\nSeyed Parsa Neshaei, and Tanja Kser. 2023. Re-\nviewriter: AI-generated instructions for peer review\nwriting. In Proceedings of the 18th Workshop on In-\nnovative Use of NLP for Building Educational Appli-\ncations (BEA 2023), pages 57–71, Toronto, Canada.\nAssociation for Computational Linguistics.\nXinyu Sun, Maoxin Han, and Juan Feng. 2019. Help-\nfulness of online reviews: Examining review infor-\nmativeness and classification thresholds by search\nproducts and experience products. Decision Support\nSystems, 124:113099.\nFrancisco Valentini, Germán Rosati, Diego Fernan-\ndez Slezak, and Edgar Altszyler. 2022. The undesir-\nable dependence on frequency of gender bias metrics\nbased on word embeddings. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2022,\npages 5086–5092, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nHrishikesh Viswanath and Tianyi Zhang. 2023. Fairpy:\nA toolkit for evaluation of social biases and their\nmitigation in large language models.\nThiemo Wambsganss, Matthias Söllner, Kenneth R\nKoedinger, and Jan Marco Leimeister. 2022a. Adap-\ntive empathy learning support in peer review scenar-\nios. In Proceedings of the 2022 CHI conference on\nhuman factors in computing systems, pages 1–17.\nThiemo Wambsganss, Vinitra Swamy, Roman Rietsche,\nand Tanja Käser. 2022b. Bias at a second glance:\nA deep dive into bias for german educational peer-\nreview data modeling. In Proceedings of the 29th\nInternational Conference on Computational Linguis-\ntics, COLING 2022, Gyeongju, Republic of Korea,\nOctober 12-17, 2022, pages 1344–1356. International\nCommittee on Computational Linguistics.\nFlorian Weber, Thiemo Wambsganss, Seyed Parsa Ne-\nshaei, and Matthias Soellner. 2023. Structured per-\nsuasive writing support in legal education: A model\nand tool for german legal case solutions. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2023, pages 2296–2313.\nChangrong Xiao, Sean Xin Xu, Kunpeng Zhang, Yu-\nfang Wang, and Lei Xia. 2023. Evaluating read-\ning comprehension exercises generated by LLMs: A\nshowcase of ChatGPT in education applications. In\nProceedings of the 18th Workshop on Innovative Use\nof NLP for Building Educational Applications (BEA\n2023), pages 610–625, Toronto, Canada. Association\nfor Computational Linguistics.\nWei Xu, Marvin J. Dainoff, Liezhong Ge, and Zaifeng\nGao. 2021. From Human-Computer Interaction to\nHuman-AI Interaction: New Challenges and Oppor-\ntunities for Enabling Human-Centered AI. pages\n1–73.\nHelen Yannakoudakis, Ted Briscoe, and Ben Medlock.\n2011. A new dataset and method for automatically\ngrading ESOL texts. ACL-HLT 2011 - Proceedings\nof the 49th Annual Meeting of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, 1:180–189.\n10286\nA Interface of peer review writing during the Prolific study\nReviewriter - Peer Reviews Writing\nIn this assignment, you need to write a peer review of at least 300 words about a \nbusiness model. In doing so, you should try to include the strengths and weaknesses \nof the business model as well as your own suggestions and ideas for improvement.\nPress the Tab to accept the suggestion, the Esc key to reject it.\nPress the up and down arrow keys to switch between suggestions.\nZunächst kann ich sagen, dass deine erste Aufgabe gut gelöst wurde. Du hast die \nUmwelt- und Unternehmensanalyse gut strukturiert aufgebaut, eine gute Einleitung \ngeschrieben und die einzelnen Fakten auf die verschiedenen Faktoren bezogen, auch \ndie zu hohen Produktionskosten hast du raus gearbeitet. Zukünftige strategischen \nprobleme individuell beurteilen und kreativ lösen, auch die großen fehler hast du immer \nauf deine strategischen fehler beziehen können. in aufgabe hast du für die hohen \nproduktionskosten,\nWord count: 71\nSubmit\nFigure 4: A screenshot of the interface to provide inline suggestions for peer review writing in the Prolific study.\nBy clicking the question mark, people get detailed guidance on the peer review writing task and the usage of the\ntool. A simple text area supports all typical interactions, such as typing, selecting, editing, and deleting text, and\ncaret movement via keys and mouse. In the input area, the sentences in black are the actual text, we display the\nAI-generated instruction in an inline format in gray. The model generates next-sentence predictions to give people a\ncomplete view of the idea. We provide three instructions each time, and people may use the Tab key to accept, the\nEsc key to reject, and the Up and Down arrow keys to toggle through different instructions). The total number of\nwords is displayed below the text area to inform people of their writing progress.\nB Extra results of GenBit gender bias score with different window sizes\n(a) Fixed context\n (b) Infinite context\nFigure 5: Comparing Genbit gender bias scores with two different context types and varying windows sizes ranging\nfrom 10 to 60.\n10287\nC Statistics on gendered words\nGroup Avg. Num\nwords\nAvg. Num\nmale words\nAvg. Perc\nmale words\nAvg. Num\nfemale words\nAvg. Perc\nfemale words\nML-based (G0) 183 1.08 0.58% 0.69 0.39%\nNo AI CG (G1) 286 1.79 0.64% 2.56 0.94%\nFT GPT-2 (G2) 299 2.16 0.71% 2.18 0.71%\nFT GPT-3 (G3) 293 2.30 0.77% 3.20 1.17%\nGPT-3.5 (G4) 291 2.71 0.93% 2.98 1.03%\nTable 5: Statistics of total words and gendered words per review from each group after data processing.\nD WEAT analysis categorization form\nBias # Targets Attributes\nConceptual\n1 Flowers vs. Insects Pleasant vs. Unpleasant\n2 Instruments vs. Weapons Pleasant vs. Unpleasant\n9 Mental vs. Physical Disease Temporary vs. Permanent\nRacial\n3 Native vs. Foreign Names Pleasant vs. Unpleasant\n4 Native vs. Foreign Names (v2) Pleasant vs. Unpleasant\n5 Native vs. Foreign Names (v2) Pleasant vs. Unpleasant (v2)\nGender\n6 Male vs. Female Names Career vs. Family\n7 Math vs. Arts Male vs. Female Terms\n8 Science vs. Arts Male vs. Female Terms\nTable 6: WEAT analysis categorization from Wambsganss et al. (2022b) in three dimensions (conceptual, racial, and\ngender). In this work, we use the gender dimension tests. WEAT compares the association between two different\ntarget word lists (i.e. Math vs. Arts) to attribute word lists (i.e. Male vs. Female terms). # indicates the original\nWEAT test number (Caliskan et al., 2017).\n10288"
}