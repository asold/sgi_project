{
  "title": "MT-BioNER: Multi-task Learning for Biomedical Named Entity Recognition using Deep Bidirectional Transformers",
  "url": "https://openalex.org/W3002832564",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100549363",
      "name": "Muhammad Raza Khan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5028720148",
      "name": "Morteza Ziyadi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5110783444",
      "name": "Mohamed AbdelHady",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2414378847",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2984124298",
    "https://openalex.org/W2963339489",
    "https://openalex.org/W2743028754",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2734608416",
    "https://openalex.org/W2953285128",
    "https://openalex.org/W2527896214",
    "https://openalex.org/W2021331223",
    "https://openalex.org/W2914526845",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W2887593177",
    "https://openalex.org/W2901959724"
  ],
  "abstract": "Conversational agents such as Cortana, Alexa and Siri are continuously working on increasing their capabilities by adding new domains. The support of a new domain includes the design and development of a number of NLU components for domain classification, intents classification and slots tagging (including named entity recognition). Each component only performs well when trained on a large amount of labeled data. Second, these components are deployed on limited-memory devices which requires some model compression. Third, for some domains such as the health domain, it is hard to find a single training data set that covers all the required slot types. To overcome these mentioned problems, we present a multi-task transformer-based neural architecture for slot tagging. We consider the training of a slot tagger using multiple data sets covering different slot types as a multi-task learning problem. The experimental results on the biomedical domain have shown that the proposed approach outperforms the previous state-of-the-art systems for slot tagging on the different benchmark biomedical datasets in terms of (time and memory) efficiency and effectiveness. The output slot tagger can be used by the conversational agent to better identify entities in the input utterances.",
  "full_text": "MT-BioNER: Multi-task Learning for Biomedical Named Entity\nRecognition using Deep Bidirectional Transformers\nMuhammad Raza Khan∗\nAzure Text Analytics, Microsoft\nCloud & AI\nrazakhan@microsoft.com\nMorteza Ziyadi∗\nMicrosoft Dynamics 365 AI\nmorteza.ziyadi@microsoft.com\nMohamed AbdelHady†\nAlexa AI, Amazon\nmbdeamz@amazon.com\nABSTRACT\nConversational agents such as Cortana, Alexa and Siri are con-\ntinuously working on increasing their capabilities by adding new\ndomains. The support of a new domain includes the design and\ndevelopment of a number of NLU components for domain classifica-\ntion, intents classification and slots tagging (including named entity\nrecognition). Each component only performs well when trained\non a large amount of labeled data. Second, these components are\ndeployed on limited-memory devices which requires some model\ncompression. Third, for some domains such as the health domain, it\nis hard to find a single training data set that covers all the required\nslot types. To overcome these mentioned problems, we present a\nmulti-task transformer-based neural architecture for slot tagging.\nWe consider the training of a slot tagger using multiple data sets\ncovering different slot types as a multi-task learning problem. The\nexperimental results on the biomedical domain have shown that\nthe proposed approach outperforms the previous state-of-the-art\nsystems for slot tagging on the different benchmark biomedical\ndatasets in terms of (time and memory) efficiency and effectiveness.\nThe output slot tagger can be used by the conversational agent to\nbetter identify entities in the input utterances.\nKEYWORDS\nmulti-task learning, biomedical data, slot tagging, named entity\nrecognition, BioBERT\nACM Reference Format:\nMuhammad Raza Khan, Morteza Ziyadi, and Mohamed AbdelHady. 2020.\nMT-BioNER: Multi-task Learning for Biomedical Named Entity Recogni-\ntion using Deep Bidirectional Transformers. In Proceedings of WSDM 2020\nWorkshop on Conversational Systems for E-Commerce Recommendations and\nSearch (WSDM ConvERSe’20). ACM, New York, NY, USA, 8 pages.\n∗The first two authors contributed equally to this paper.\n†This work was done while the author was a data science manager in the Azure AI\ngroup at Microsoft.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nWSDM ConvERSe’20, Feb 07, 2020, Houston, TX\n© 2020 Association for Computing Machinery.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00\n1 INTRODUCTION\nHealthcare industry is going through a technological transforma-\ntion especially through the increasing adoption of conversational\nagents including voice assistants (such as Cortana, Alexa, Google\nAssistant, and Siri) and the medical chatbots 1. These conversa-\ntional agents hold great potential when it comes to transforming\nthe way how consumers connect with the providers but recent\nreports suggest that these conversational agents need to improve\ntheir performance and usability in order to fulfill their potential.\nThe field of biomedical text understanding has received increased\nattraction in the last few years. With the increase accessibility\nof the medical records, scientific reports and publications along\nwith the better tools and algorithms to process these big datasets,\nprecision medicine and diagnosis have the potential to make the\ntreatments much more effective. In addition, the relations between\ndifferent symptoms and diseases, side-effects of different medicines\ncan be more accurately identified based on text mining on these\nbig datasets. The performance of the biomedical text understand-\ning systems depends heavily on the accuracy of the underlying\nBiomedical Named Entity Recognition component (BioNER) i.e.,\nthe ability of these systems to recognize and classify different types\nof biomedical entities/slots present in the input utterances.\nSlot tagging and Named Entity Recognition (NER) extract seman-\ntic constituents by using the words of an input sentence/utterance\nto fill in predefined slots in a semantic frame. The slot tagging\nproblem can be regarded as a supervised sequence labeling task,\nwhich assigns an appropriate semantic label to each word in the\ngiven input sentence/utterance. The slot type could be a quantity\n(such as time, temperature, flight number, or building number) or a\nnamed entity such as person or location. One of the key challenges\nthese text understanding systems face is better identification of\nsymptoms and other entities in their input utterances. In other\nwords, these conversational agents need to improve their slot tag-\nging capabilities[20]. In general, conversational agents ability to\nperform different tasks like topic classification or intent detection\ndepends heavily on their ability to accurately identify slot types\nin the input and the researchers have been trying to both improve\nthe named entity recognition component then use the improved\ncomponent for the downstream task. Some example of this trend\ninclude Entity-aware topic classification [ 1], de-identification of\nmedical records for conversational systems [3] and improving in-\nquisitiveness of conversational systems through NER [18].\nBuilding an NER component with high precision and recall is\ntechnically challenging because of some reasons: 1) Requirement\n1https://www.usertesting.com/about-us/press/press-releases/healthcare-chatbot-\napps-on-the-rise-but-cx-falls-short\narXiv:2001.08904v1  [cs.CL]  24 Jan 2020\nWSDM ConvERSe’20, Feb 07, 2020, Houston, TX Muhammad Raza Khan, Morteza Ziyadi, and Mohamed AbdelHady\nof hand-crafted features for each of the label to increase the system\nperformance, 2) Lack of extensively labelled training dataset. 3) For\nconversational agents, the slot tagger may be deployed on limited-\nmemory devices which requires model compression or knowledge\ndistillation to fit into memory. 4) For some domains such as the\nhealth/biomedical domain, it is hard to find a single training data\nset that covers all the required slot types.\nMany of the current state of the art BioNER systems rely on\nhand-crafted features for each of the labels in the data. The compu-\ntation of these hand-crafted features takes most of the computation\ntime. Furthermore, the usage of these hand-crafted features results\nin a system that is optimized for the training dataset. Recent ad-\nvances in the neural network-based feature learning approaches\nhave helped researchers to develop BioNER systems that are no\nlonger dependent on the manual feature engineering process. How-\never, the performance of these neural network-based techniques\ndepends heavily on the presence of big and high-quality labelled\ntraining dataset. Such large datasets can be very difficult to obtain\nin the biomedical domain because of the cost and privacy issues.\nMulti-task learning is one way to solve the problem of lack of\nextensive training data. Basic premise behind multi-task learning is\nthat different datasets may have semantic and syntactic similarities\nand these similarities can help in training of a much more optimized\nmodel as compared to the one trained on a single dataset. It also\nhelps to reduce model overfitting. However, multi-task learning\nmodels without pre-trained weights can take a long time to train\non large datasets. In contrast, pre-trained language model based\napproaches ([6]; [15]; and [17]) combined with multi-task learning\nhave recently started showing promising results ([13].\nTo overcome these challenges, we present a multi-task transformer-\nbased neural architecture for slot tagging and applied it to the\nBiomedical domain (MT-BioNER). We consider the training of a\nslot tagger using multiple data sets covering different slot types\nas a multi-task learning problem. The experimental results on the\nbiomedical domain have shown that the proposed approach out-\nperforms the previous state-of-the-art methods for slot tagging\non the different benchmark biomedical datasets in terms of (time\nand memory) and effectiveness. These results can be used by the\nbiomedical conversational agent to better identify entities in the\ninput utterances.\n2 RELATED WORK\nMany recent studies on BioNER have used neural networks to\novercome the requirement of hand-crafted feature generation. [5]\nused a word and its surrounding context as input to the convolu-\ntional neural network while [8] used word embeddings as input\nto the BiLSTM-CRF model for named entity recognition. Both of\nthese models are based on single task / dataset approaches and\nthey cannot use the information contained in multiple datasets as\nit is done in multi-task learning (MTL). Although the multi-task\nlearning (MTL) based approaches have been widely used in the\nNLP research (for example, [4] used it for standard NLP tasks like\nPOS Tagging, Chunking etc.,), but application of MTL in biomedi-\ncal text mining has not seen promising results primarily because\nmany of the approaches (e.g. [5]) used word level features as input\nignoring sub-word information which can be quite important in\nthe biomedical domain. To overcome this challenge, [ 22] used a\nmulti-task BiLSTM-CRF model with an additional context depen-\ndent BiLSTM layer for modelling character sequences and was able\nto beat the benchmark results on five canonical biomedical datasets.\nOur proposed system is different from their scheme as we are able\nto combine the advantages of both the multi-task learning and pre-\ntrained language models. Furthermore, the use of character-based\nLSTM models can be slower in terms of training and scoring time.\nActual comparison of our model and [22] work is described later\non in this paper. Pre-trained language model based approaches\nhave been popular for different biomedical text mining tasks. For\ninstance, [19] used transfer learning based approach to pretrain\nweights of an LSTM by training it in both forward and reverse direc-\ntion using Word2Vec ([14]) embedding trained on a large collection\nof biomedical documents. In such approaches, Word2vec model\nneeds be fine-tuned according to the variations in the biomedi-\ncal data ([16]). Recent developments of ELMO ([15]); GPT ([17]);\nand BERT ([6]) language models have proven the effectiveness of\nthe contextualized representations for different NLP tasks. These\ncontextual representation learning models fine tune unsupervised\nobjectives on text data. For instance, BERT ([6]) uses multi-layer\nbidirectional Transformer on plain text for masked word prediction\nand next sentence prediction tasks. However, these unsupervised\nmodels must be fine-tuned to achieve better results for the specific\nprediction tasks using additional task-specific layers and datasets.\nThe examples of BERT based models fine-tuned for biomedical\ndomain include:\n(1) BioBERT [11] that fine-tunes BERT model on Pubmed ab-\nstracts and PMC full text articles along with English Wikipedia\nand Books corpus\n(2) ClinicalBERT [9] which fine-tunes the BERT model for the\npurpose of hospital re-admission prediction\n(3) Alsentzer et al . [2] fine-tuned BERT/BioBERT models for\nthe purpose of named entity recognition on a single dataset\n(task) and deidentification on clinical texts.\nWe can see two clear trends in the related research literature\ndescribed in this section: 1) Multi-task based learning approaches\nthat are able to use the sub-words based information are able to\nbeat the other models for biomedical named entity recognition.\n2) Contextual representation models like BERT have been quite\nsuccessful for language modeling related problems. Based on these\nobservations, we see a lot of potential for the approaches that com-\nbine both the multi-task learning and language model pretraining\nbased approaches and this paper presents one such approach. Our\napproach takes inspiration from the recent multi-task deep neural\nnetwork (MT-DNN, [13] work which combines both multi-task\nlearning and BERT language model, but MT-DNN is optimized for\ngeneral natural language understanding (NLU) tasks. In contrast,\nour model MT-BioNER is optimized for biomedical named entity\nrecognition using BioBERT as the shared layers and the different\ndata sets in the task-specific layers.\n3 MT-BIONER\n3.1 Model Architecture\nAs described in the introduction and related work sections, our\nmodel combines pre-trained language models (using BERT as the\nMT-BioNER: Multi-task Learning for Biomedical Named Entity Recognition using Deep Bidirectional Transformers WSDM ConvERSe’20, Feb 07, 2020, Houston, TX\nFigure 1: The representation of MT-BioNER input sequence and output label sequence. The input embeddings are the sum of\nthe token embeddings, the segmentation embeddings and the position embeddings\nFigure 2: Architecture of the MT-BioNER model.\nshared layers) and transfer learning (using task specific output lay-\ners). Figure 2 shows the architecture of the MT-BioNER model. The\nlower layers are shared across all tasks/datasets while the top lay-\ners are specific for each dataset (entity types). The input sentence\nis first represented as a sequence of embedding vectors ( X), one\nfor each token which consists of word, position and segment em-\nbeddings. Then the Transformer encoder captures the contextual\ninformation for each token and generates the shared contextual\nembedding vectors. Finally, for each task/dataset, a shallow layer\nis used to generate task-specific representations, followed by op-\nerations necessary for entity recognition. The input sentence is\nfirst represented as a sequence of embedding vectors (X), one for\neach token. Then the Transformer encoder captures the contextual\ninformation for each token and generates the shared contextual\nembedding vectors. Finally, for each task/dataset, a shallow layer is\nused to generate task-specific representations, followed by opera-\ntions necessary for entity recognition. The input word sequence\nand output label sequence are represented as shown in Figure 1.\nThe model architecture details are as follows:\n•Lexicon Encoder layer: The input sentences = {w1, ..., wn }\nis a sequence of tokens of length n. Note that two additional\nlabels [CLS]and [SEP ]are used to represent the start and\nend of a sentence, respectively. The first token w1 is always\nthe [CLS]token and the last token is [SEP ]. The lexicon en-\ncoder maps s into a sequence of input embedding vectors\nX = {x1, ..., xn }constructed by concatenating the corre-\nsponding word, segment, and position embeddings to pro-\nduce the input to the Transformer Encoder layers.\n•Transformer Encoder layers : The encoder of the multi-\nlayer bidirectional Transformer ([ 21]) is used to map the\ninput representation vectors into contextual embedding vec-\ntors. In our experiments, we use BERT encoder model [ 6]\nas the shared layers across different tasks (datasets). We\nfine-tune the BERT model alongside the task-specific lay-\ners during the training phase using a multi-task objective\nfunction.\n•Task-Specific Layers : We use a shallow linear layer for\neach of the tasks/datasets. Depending on the slot types cov-\nered by each training dataset, we treat each dataset as a\nseparate slot tagging task and add a separate output linear\nlayer as the last layer of the network to learn the entities in\nthat dataset. In our experiments, we use softmax layer. We\nhave conducted additional experiments where we replaced\nthe softmax layer with two layers: a feedforward layer and\na CRF layer. But the training takes longer and we didn’t get\nsignificant improvement in the test accuracy.\n3.2 Training\nWe use a similar objective function as in [ 22] to train the multi-\ntask model. The formal definition of the multi-task setting is as\nfollows. Given m datasets, for i ∈{1, ..., m}, each dataset Di con-\nsists of ni training samples, i.e., Di = {(si\nj , yi\nj )}ni\nj=1. We denote\nthe training set for each dataset Di as Xi = {Xi\n1, ..., Xi\nni }where\nXi\nj = {Xi\nj1, Xi\nj2, . . . ,Xi\njn }is the sequence of feature representations\nof the input word sequence si\nj of length n. The set of labels for each\nWSDM ConvERSe’20, Feb 07, 2020, Houston, TX Muhammad Raza Khan, Morteza Ziyadi, and Mohamed AbdelHady\nAlgorithm 1: Training Algorithm I\nInitialize model parameters θ\n|shared layers parameter θBERT\ni by BERT\n|task-specific layer parameters θo\ni randomly\nSet max epochs: epochmax\nforeach epoch in 1,2, epochmax do\nMerge all the datasets: D = D1U D2....U DT\nShuffle D foreach bt in D do\n//b_t is a mini-batch of task t.\nComputer Loss L(θ) (Eq 1)\nCompute gradient: ∇(θ)\nUpdate model: θ = θ −η∇θ\ndataset is Yi = {yi\n1, ..., yi\nni }where yi\nj is the output label sequence of\nsi\nj . The multi-task model consists ofm different models each trained\non separate dataset while sharing part of the model parameters\nacross datasets. The loss function L of the multi-task model is\nL =\nmÕ\ni=1\nλi Li (θBERT\ni , θo\ni )=\nmÕ\ni=1\nλi log P(Yi |Xi ;θBERT\ni , θo\ni ) (1)\nThe training maximizes the log-likelihood P(Yi |Xi ;θBERT\ni , θo\ni )\nof the label sequence y given the input sequence X for each given\ntraining data set Di as shown in Eq. 1 where the cross-entropy loss\nis used as the loss function. Cross-entropy loss increases as the\npredicted probability diverges from the actual label. The contribu-\ntion of each dataset Di is controlled by the hyperparameter λi . In\nour experiments, we assume that all the data sets have the same\ncontribution and set λi = 1 for all datasets. The BERT shared layer\nand the ith task-dependent layer parameters are represented by\nθBERT\ni and θo\ni , respectively. We conducted two variants of trans-\nfer learning. First, we freeze the shared layer and only fine-tune\nthe task-specific layers (that is, θBERT\ni = θBERT\ninitial for all datasets).\nIn the second variant, we fine-tune the whole network. (That is,\nθBERT\ni = θBERT\ntuned ).\nWe use stochastic gradient descent (SGD) to learn the parameters\nof all shared layers and task-specific layers as shown in Algorithm 1\n(based on [13]). We initialize the shared layers with the pre-trained\nBERT model while the task-dependent layers are initialized ran-\ndomly. After creating mini-batches of each dataset, we combine all\nthe datasets and shuffle the mini-batches. At each epoch, we train\nthe model on all the mini-batches and then at each batch-iteration,\na mini-batch bt corresponding to task t is selected (from all the\ndatasets), and the model is updated according to the objective for\nthe task t.\nDataset Size Entity Type and\nCounts\nBC2GM 20,000 sentences Gene/Protein\n(24,583)\nBC5CDR 1500 articles Chemical (15,935),\nDisease (12,852)\nNCBI-Disease 793 abstracts Disease (6,881)\nJNLPBA 2,404 abstracts Gene/ Protein\n(35,336), Cell\nLine (4,330), DNA\n(10,589), Cell Type\n(8,649), RNA (1,069)\nTable 1: Datasets used in the experiments All datasets\ncan be downloaded form https://github.com/cambridgeltl/MTL-\nBioinformatics-2016\n4 EXPERIMENTS\n4.1 Data Preparation\nWe evaluate the performance of the proposed approach on four\nbenchmark datasets used by Sachan et al . [19]. Table 1 gives a\nsummary of these datasets based on the number of sentences, and\nentities. We used these publicly available datasets in order to make\nthe experiments reproducible. We use the same training, develop-\nment and test sets splits according to Crichton et al. [5] for each\ndataset. As in Wang et al. [22] and Sachan et al. [19], we use training\nand development sets to train the final model. As part of the data\npreprocessing step, word labels are encoded using an IOB scheme.\nIn this scheme, for example, a word describing a disease entity is\ntagged with \"B-Disease\" if it is at the beginning of the entity, and\n\"I-Disease\" if it is inside the entity. All other words not describing\nentities of interest are tagged as ’O’.\n4.2 Training and Evaluation Details\nWe test our method on four benchmark datasets used by [19] . All\nthe neural network models are trained on one Tesla K80 GPU using\nPyTorch framework . To train our neural models, we use BertAdam\noptimizer with a learning rate of 5e-5 and a linear scheduler with a\nwarmup proportion of 0.4, and a weight decay of 0.01 applied to\nevery epoch of training. We use a batch size of 32, and the maximum\nsentence length of 128 words. We use BioBERT model ([11]) as the\ndomain-specific language model. Lee et al. [11] also presented the\nuse of BioBERT for biomedical NER scenario. But their scheme is\nto develop different models for different datasets.\n5 RESULTS\n5.1 Performance\nWe compare our proposed MT-BioNER model with state-of-the-\nart BioNER systems such as the single task LSTM-CRF model of\nHabibi et al. [8] (BiLSTM), the multi-task model of Wang et al. [22]\n(MTM-CW), and transfer learning approach of Sachan et al. [19]\n(BiLM-NER). We show the precision, recall, and F1 scores of the\nmodels trained on three and four datasets in Table 2. BioBERT\nMT-BioNER: Multi-task Learning for Biomedical Named Entity Recognition using Deep Bidirectional Transformers WSDM ConvERSe’20, Feb 07, 2020, Houston, TX\nDataset Metric Single-Task Learning Multi-Task Learning\nBenchmark BiLSTM BiLM MTM-CW MT-BioNER\n3-datasets 4-datasets\nBC2GM\n(Genes)\nPrecision - 81.57 81.81 82.10 82.01 80.33\nRecall - 79.48 81.57 79.42 84.04 82.82\nF1 - 80.51 81.69 80.74 83.01 81.55\nBC5CDR\n(Chemical)\nPrecision 89.21 87.60 88.10 89.10 88.46 87.99\nRecall 84.45 86.25 90.49 88.47 90.52 90.16\nF1 86.76 86.92 89.28 88.78 89.5 89.06\nNCBI\n(Disease)\nPrecision 85.10 86.11 86.41 85.86 86.73 84.50\nRecall 80.80 85.49 88.31 86.42 89.7 88.98\nF1 82.90 85.80 87.34 86.14 88.1 86.68\nJNLPBA\n(Genes\netc.)\nPrecision 69.42 71.35 71.39 70.91 - 67.40\nRecall 75.99 75.74 79.06 76.34 - 79.35\nF1 72.55 73.48 75.03 73.52 - 72.89\nTable 2: Comparison of MT-BioNER model and recent state-of-the-art models. Source of benchmark performance scores of datasets\nare: NCBI-disease: [10]; BC5CDR: [12]; JNLPBA: [7]; MTM-CW was proposed by [22]; BiLSTM was proposed by [8]. The performance scores\nfor these NER models are referred from [19]\nmodel is used as the shared layers for these results. From the re-\nsults, we see that our approach trained on three datasets obtains\nthe maximum recall and F1 scores. We should mention that our\nmodel is based on multi-task approach and achieves better per-\nformance even compared to Sachan et al. [19] single task transfer\nlearning approach (BiLM-NER) in which the whole network is only\ntrained on a single dataset. Moreover, our model trained on four\ndatasets performs better on recall and F1-score compared to MTM-\nCW approach which is a multi-task model. Another interesting\nresult is that our model achieves the highest recall among all the\nother approaches. But it has a lack in precision score. To further\nimprove the precision, we can add dictionaries as features which\ncould be an interesting future work. Also, it shows the potential\ncapability of BERT language model to provide the semantic feature\nrepresentations for the multi-task NER tasks.\n5.2 Training/Scoring Time and Model Size\nAs mentioned before, all the neural network models are trained on\none Tesla K80 GPU. We compare the training time of our model\nwith Wang et al. [22] as we utilize it for benchmark comparison,\nand they have made their model publicly available. We find that\nfor four datasets, it takes on average 40 minutes per epoch (1.45\nsec/mini-batch, with total of 1,537 minibatches, and batch size of 32)\nand with a total number of 8 epoch, the full training on 4 datasets is\nless than 6 hours. On average it is equivalent to 0.36 sec per sentence\nto train a final model which compare to Wang et al . [22] it is at\nleast twice faster to train our model. One of the main reasons is that\nWang et al. [22] model uses character, and word level bidirectional\nLSTM layers which could take more time to train a model.\nDataset Metric BERT-Base BioBERT\nEpochs 50 8\nBC2GM Precision 77.86 80.33\nRecall 80.57 82.82\nF1 79.19 81.55\nBC5CDR Precision 85.92 87.99\nRecall 87.79 90.16\nF1 86.85 89.06\nNCBI Precision 84.78 84.50\nRecall 86.90 88.98\nF1 85.83 86.68\nJNLPBA Precision 65.40 67.40\nRecall 75.58 79.35\nF1 70.12 72.89\nTable 3: Domain Adaptation Experiment\nAs another comparison, we study the scoring (inference) time as\nwell since in the real-world applications, this parameter plays an\nimportant role. For the scoring time, we test it on a single dataset\nof BC5CDR test set and it takes 80 seconds to run the prediction\nwhich compare to Wang et al. [22] model, it is at least twice faster\nand we think the character level LSTM layers could be the main\nreason. We should also mention that prediction time of a multi-task\nmodel in general is faster than multiple single-task models such as\nSachan et al. [19] since we can run the shared layers once and the\nremaining shallow-task dependent layers could be run in parallel.\nModel size is another factor in real-world applications. Using our\napproach, the model size is 430 MB which is bigger than [22] model\nwhich is 220MB. Since we are using BERT base model with 12\nneural network layers, the bigger size of the model was expected.\nMoreover, compared to single-task models which require multiple\nWSDM ConvERSe’20, Feb 07, 2020, Houston, TX Muhammad Raza Khan, Morteza Ziyadi, and Mohamed AbdelHady\nmodels to identify multiple entities (form different datasets), multi-\ntask approaches have the advantage of providing a single model for\nall the entity types across different datasets which is a big advantage\nin real-world production environments.\n5.3 Fine-tuning Approaches Study\nTo achieve the best results, we fine-tune the shared language model\nalongside the task-specific layers during training phase. An interest-\ning study could be to investigate if we only train the task-dependent\nlayers and freeze the language model to its pre-trained weights.\nWe run this experiment with the same parameters as other exper-\niments and observe a poor performance of the model, i.e., for all\nthe datasets the F1-score is less than 60%. This poor performance\ncould be explained as following. Since we are training only the\ntask-dependent layers, the shared layers act as fixed embedding\nlayer which is not trainable. So, the whole network to train would\nbe a shallow linear layer and using a linear layer with an embedding\nin a multi-task fashion may not give good results as it is observed\nin this study.\n5.4 Domain Adaptation Study\nThe benchmark results shown in the Table 2 are achieved based on\nBioBERT as the shared layer which is in-domain language model.\nTo analyze the domain adaptation scenario, we run the same ex-\nperiment on the general BERT base model. In the experiments, we\nuse BERT-base-Cased model instead of BioBERT since BioBERT is\na fine-tuned version of BERT-base-Cased model. Table 3 shows the\nperformance comparison of these two scenarios. We observe that\nBERT base model reaches to a margin of state-of-the-art results,\nbut it requires a greater number of training iterations to adapt to\na new domain. To mention that in order to further improve these\nresults, one might first fine-tune the BERT language model using\nall datasets and save a new language model and then use this new\nmodel in our approach. This approach is very similar to the work\nof Sachan et al. ([19]) on transfer learning in BiLM-NER in which\nthey have created their own language model based on character-\nlevel CNN layer with word embedding layers as lexicon encoder\nand bidirectional LSTM layers to extract the contextual embedding\nvectors. We didn’t experiment this approach in this paper since\nwe focused on the multi-task study of the work, but it could be an\ninteresting experiment for future works.\n5.5 Training Schema Study\nTo achieve the best results, we utilized the training scheme of MT-\nDNN (Liu et al . [13]) work, in which, at each training iteration\n(epoch), we train on all the batches of all the datasets by selecting\na random batch from all datasets. In Wang et al. [22], they utilize\na different training scheme in which at each iteration a random\ndataset is selected, and different batches of that specific dataset is\nfed into the pipeline to train. Using this scheme, they run several\nhundreds of epochs to train the model and it makes sense since\nthey train the language model part of their approach as well. But,\nin our experiment, we need a smaller number of iterations due to\navailability of the pre-trained language model. Thus, we run similar\nexperiment but instead of random selection, we iterate on the order\nof the datasets (Algorithm 2). Figure 3 show the performance of\n(a) Performance of Training Algorithm I\n(b) Performance of Training Algorithm II\nFigure 3: Comparison of Algorithm I and II\ntraining algorithms over the number of iterations for algorithm I,\nand II, respectively. It is clear that compared to algorithm I which\nachieve the best results with less number of iterations, training\nscheme II is performing poor and the learning process is not stable\nsince the model gets biased on the latest dataset that it is trained\non.\nAlgorithm 2: Training Algorithm II\nInitialize model parameters θ\n|shared layers parameter θBERT\ni by BERT\n|task-specific layer parameters θo\ni randomly\nSet max epochs: epochmax\nforeach t in 1,2,..., T do\nPack the dataset t into mini-batch: Dt .\nforeach epoch in 1,2, epochmax do\nSelect dataset Dt based on the taskorder\nforeach bt in D do\n//b_t is a mini-batch of task t.\nComputer Loss L(θ) (Eq 1)\nCompute gradient: ∇(θ)\nUpdate model: θ = θ −η∇θ\nMT-BioNER: Multi-task Learning for Biomedical Named Entity Recognition using Deep Bidirectional Transformers WSDM ConvERSe’20, Feb 07, 2020, Houston, TX\n6 CONCLUSION & FUTURE WORK\nConversational agents such as Cortana, Alexa and Siri are con-\ntinuously working on increasing their capabilities by adding new\ndomains. The support of a new domain includes the design and\ndevelopment of a number of NLU components for domain classifica-\ntion, intents classification and slots tagging (including named entity\nrecognition). Each component only performs well when trained\non a large amount of labeled data. Second, these components are\ndeployed on limited-memory devices which requires some model\ncompression. Third, for some domains such as the health domain, it\nis hard to find a single training data set that covers all the required\nslot types. In this paper, we presented a multi-task transformer-\nbased neural architecture for slot tagging that overcomes these\nmentioned problems and applied it into the biomedical domain\n(MT-BioNER). We formulated the training of a slot tagger using\nmultiple data sets covering different slot types as a multi-task learn-\ning problem. We also reported the training and scoring time and\ncompared it to the recent advancements. The experimental results\non the biomedical domain have shown that MT-BioNER model\ntrained using the proposed approach outperforms the previous\nstate-of-the-art systems for slot tagging on different benchmark\nbiomedical datasets in terms of both (time and memory efficiency)\nand effectiveness. The model has a shorter training time and infer-\nence time, and smaller memory footprint compared to the baseline\nmultiple single-task based models. This is another advantage of\nour approach which can play an important role in the real-world\napplications. We run extra experiments to study the impact of fine-\ntuning the shared language model as it is found as a crucial point of\nour approach. Utilizing BERT base model and its comparison with\nBioBERT showed an interesting result in which for the domains\nthat there is not a specific in-domain BERT model, the base model\nwould also perform well with considering a penalty of few percent\nin the F1-score and more training iterations. Furthermore, we have\nshowed through detailed analysis that the training algorithm plays\na very important role in achieving the strong performance. The\noutput slot tagger can be used by the conversational agent to better\nidentify entities in the input utterances.\nLastly, we highlight several future directions to improve the\nmulti-task BioNER model. Based on MT-BioNER results on three\nand four datasets, deep diving into the effect of dataset/entity types\non the multi-task learning approaches could be an interesting future\nwork. Overall this study, shows that how the named entity recog-\nnition capabilities of different biomedical systems can be obtained\nby employing recent trends from deep learning and multi-task\nlearning based research. Incorporating such techniques can help\nthe researchers to overcome the limitation of extensively labeled\ntraining data that is really hard to get in the biomedical domain.\nAs a future work, we want to further explore the impact of\noverlap between the datasets on the overall model performance.\nAddition of the JNLPBA to the training datasets results in degrada-\ntion of the overall performance of the model. One possible reason\nfor this degradation is that JNLPBA contains many genes and pro-\nteins which are represented as small abbreviations and code. These\nsmall abbreviations and genes can overlap with the entities in other\ndatasets resulting in confusion and degradation of the overall model.\nWe will like to explore ways to tackle overlap between entities that\ncan degrade the model performance. We will also like to perform\ncomparative analysis of different models on same input sentences\nto highlight the plus points of our model over other models.\nAlthough, the datasets and experiments specified in this research\npaper are focused on the biomedical domain but the techniques and\nalgorithms presented in this paper can be used in other domains\nand general conversational systems: ones that are not focused on\nthe biomedical domain. We also want to analyze the performance\nof our NER model on general domain conversational systems in\nfuture work as well.\nREFERENCES\n[1] Ali Ahmadvand, Harshita Sahijwani, Jason Ingyu Choi, and Eugene Agichtein.\n2019. ConCET: Entity-aware topic classification for open-domain conversational\nagents. In Proceedings of the 28th ACM International Conference on Information\nand Knowledge Management . 1371–1380.\n[2] Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan\nNaumann, and Matthew McDermott. 2019. Publicly available clinical BERT\nembeddings. arXiv preprint arXiv:1904.03323 (2019).\n[3] Ido Cohn, Itay Laish, Genady Beryozkin, Gang Li, Izhak Shafran, Idan Szpek-\ntor, Tzvika Hartman, Avinatan Hassidim, and Yossi Matias. 2019. Audio De-\nidentification: A New Entity Recognition Task. https://arxiv.org/pdf/1903.07037.\npdf\n[4] Ronan Collobert and Jason Weston. 2008. A unified architecture for natural lan-\nguage processing: Deep neural networks with multitask learning. In Proceedings\nof the 25th international conference on Machine learning . ACM, 160–167.\n[5] Gamal Crichton, Sampo Pyysalo, Billy Chiu, and Anna Korhonen. 2017. A neural\nnetwork multi-task learning approach to biomedical named entity recognition.\nBMC bioinformatics 18, 1 (2017), 368.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[7] Zhou GuoDong and Su Jian. 2004. Exploring deep knowledge resources in\nbiomedical name recognition. In Proceedings of the International Joint Workshop\non Natural Language Processing in Biomedicine and its Applications . Association\nfor Computational Linguistics, 96–99.\n[8] Maryam Habibi, Leon Weber, Mariana Neves, David Luis Wiegandt, and Ulf Leser.\n2017. Deep learning with word embeddings improves biomedical named entity\nrecognition. Bioinformatics 33, 14 (2017), i37–i48.\n[9] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. 2019. ClinicalBERT: Mod-\neling Clinical Notes and Predicting Hospital Readmission. arXiv preprint\narXiv:1904.05342 (2019).\n[10] Robert Leaman and Zhiyong Lu. 2016. TaggerOne: joint named entity recognition\nand normalization with semi-Markov Models. Bioinformatics 32, 18 (2016), 2839–\n2846.\n[11] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,\nChan Ho So, and Jaewoo Kang. 2019. Biobert: pre-trained biomedical language\nrepresentation model for biomedical text mining. arXiv preprint arXiv:1901.08746\n(2019).\n[12] Haodi Li, Qingcai Chen, Kai Chen, and Buzhou Tang. 2015. HITSZ_CDR System\nfor Disease and Chemical Named Entity Recognition and Relation Extraction. In\nProceedings of the Fifth BioCreative Challenge Evaluation Workshop. Sevilla: The\nfifth BioCreative challenge evaluation workshop , Vol. 2015. 196–201.\n[13] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019. Multi-\ntask deep neural networks for natural language understanding. arXiv preprint\narXiv:1901.11504 (2019).\n[14] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.\nDistributed representations of words and phrases and their compositionality. In\nAdvances in neural information processing systems . 3111–3119.\n[15] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\nClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word\nrepresentations. arXiv preprint arXiv:1802.05365 (2018).\n[16] S. Pyysalo, F. Ginter, H. Moen, T. Salakoski, and S. Ananiadou. 2013. Distributional\nSemantics Resources for Biomedical Text Processing. In Proceedings of LBM 2013 .\n39–44. http://lbm2013.biopathway.org/lbm2013proceedings.pdf\n[17] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. 2018. Improving language understanding by genera-\ntive pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding paper.\npdf (2018).\n[18] S Reshmi and Kannan Balakrishnan. 2018. Enhancing Inquisitiveness of Chatbots\nThrough NER Integration. In 2018 International Conference on Data Science and\nWSDM ConvERSe’20, Feb 07, 2020, Houston, TX Muhammad Raza Khan, Morteza Ziyadi, and Mohamed AbdelHady\nEngineering (ICDSE) . IEEE, 1–5.\n[19] Devendra Singh Sachan, Pengtao Xie, Mrinmaya Sachan, and Eric P Xing. 2017.\nEffective use of bidirectional language modeling for transfer learning in biomedi-\ncal named entity recognition. arXiv preprint arXiv:1711.07908 (2017).\n[20] UserTesting.com. 2019. healthcare Chatbot diagnosis.\n[21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems . 5998–6008.\n[22] Xuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang, Marinka Zitnik, Jingbo Shang,\nCurtis Langlotz, and Jiawei Han. 2018. Cross-type biomedical named entity\nrecognition with deep multi-task learning. Bioinformatics 35, 10 (2018), 1745–\n1752.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8575199842453003
    },
    {
      "name": "Transformer",
      "score": 0.792752742767334
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.711905837059021
    },
    {
      "name": "Task (project management)",
      "score": 0.5926944613456726
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5568156838417053
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.48250293731689453
    },
    {
      "name": "Training set",
      "score": 0.4637919068336487
    },
    {
      "name": "Named-entity recognition",
      "score": 0.46218621730804443
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4399595260620117
    },
    {
      "name": "Machine learning",
      "score": 0.37070804834365845
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32417815923690796
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": []
}