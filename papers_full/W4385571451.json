{
    "title": "Revisiting Relation Extraction in the era of Large Language Models",
    "url": "https://openalex.org/W4385571451",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2791586769",
            "name": "Somin Wadhwa",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A2106421351",
            "name": "Silvio Amir",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A2441726348",
            "name": "byron wallace",
            "affiliations": [
                "Universidad del Noreste"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3104950207",
        "https://openalex.org/W4221166835",
        "https://openalex.org/W3155073135",
        "https://openalex.org/W2429914308",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W2798734500",
        "https://openalex.org/W3210277894",
        "https://openalex.org/W1566346388",
        "https://openalex.org/W1604644367",
        "https://openalex.org/W3214607109",
        "https://openalex.org/W2996825178",
        "https://openalex.org/W2129767020",
        "https://openalex.org/W3105063288",
        "https://openalex.org/W2997876626",
        "https://openalex.org/W2132267839",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3152515526",
        "https://openalex.org/W3156470785",
        "https://openalex.org/W4205737716",
        "https://openalex.org/W3214342214",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4385567008",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4229019932",
        "https://openalex.org/W3121525843",
        "https://openalex.org/W2147880316",
        "https://openalex.org/W3122241445",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W2952179106",
        "https://openalex.org/W4281488715",
        "https://openalex.org/W4288104771"
    ],
    "abstract": "Relation extraction (RE) is the core NLP task of inferring semantic relationships between entities from text. Standard supervised RE techniques entail training modules to tag tokens comprising entity spans and then predict the relationship between them. Recent work has instead treated the problem as a <i>sequence-to-sequence</i> task, linearizing relations between entities as target strings to be generated conditioned on the input. Here we push the limits of this approach, using larger language models (GPT-3 and Flan-T5 large) than considered in prior work and evaluating their performance on standard RE tasks under varying levels of supervision. We address issues inherent to evaluating generative approaches to RE by doing human evaluations, in lieu of relying on exact matching. Under this refined evaluation, we find that: (1) <i>Few-shot</i> prompting with GPT-3 achieves near SOTA performance, i.e., roughly equivalent to existing <i>fully supervised</i> models; (2) Flan-T5 is not as capable in the few-shot setting, but supervising and fine-tuning it with Chain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA results. We release this model as a new baseline for RE tasks.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 15566–15589\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nRevisiting Relation Extraction in the era of Large Language Models\nSomin Wadhwa Silvio Amir Byron C. Wallace\nNortheastern University\n{wadhwa.s, s.amir, b.wallace}@northeastern.edu\nAbstract\nRelation extraction (RE) is the core NLP task\nof inferring semantic relationships between en-\ntities from text. Standard supervised RE tech-\nniques entail training modules to tag tokens\ncomprising entity spans and then predict the\nrelationship between them. Recent work has\ninstead treated the problem as a sequence-to-\nsequence task, linearizing relations between\nentities as target strings to be generated condi-\ntioned on the input. Here we push the limits\nof this approach, using larger language models\n(GPT-3 and Flan-T5 large) than considered in\nprior work and evaluating their performance\non standard RE tasks under varying levels of\nsupervision. We address issues inherent to eval-\nuating generative approaches to RE by doing\nhuman evaluations, in lieu of relying on exact\nmatching. Under this refined evaluation, we\nfind that: (1) Few-shot prompting with GPT-3\nachieves near SOTA performance, i.e., roughly\nequivalent to existing fully supervised mod-\nels; (2) Flan-T5 is not as capable in the few-\nshot setting, but supervising and fine-tuning\nit with Chain-of-Thought (CoT) style explana-\ntions (generated via GPT-3) yields SOTA re-\nsults. We release this model as a new baseline\nfor RE tasks1.\n1 Introduction\nRelation extraction (RE) is the task of identifying\nentities and their semantic relationships from texts.\nStandard supervised approaches (Eberts and Ulges,\n2019a) to RE learn to tag entity spans and then\nclassify relationships (if any) between these. More\nrecent work has shown that conditional language\nmodels can capably perform this task—achieving\nSOTA or near-SOTA results—when trained to out-\nput linearized strings encoding entity pairs and\ntheir relations (Paolini et al., 2021; Lu et al., 2022b;\nHuguet Cabot and Navigli, 2021). However, to date\nsuch work has considered only moderately sized\n1https://sominw.com/ACL23LLMs\nREBEL\n(Baseline)\nModel Performance on CoNLL\nMicro-F1 Score\nFew-Shot \nGPT-3\nFew-Shot \nGPT-3 + CoT\nFine-Tuned \nFlan-T5\nFine-Tuned\nFlan-T5 + CoT\n1 2 3 4\nFigure 1: RE performance of LLMs on the CoNLL\ndataset. 1 Few-shot GPT-3 slightly outperforms the ex-\nisting fully supervised SOTA method (Huguet Cabot and\nNavigli 2021; dotted horizontal line). 2 Eliciting CoT\nreasoning from GPT-3 further improves few-shot per-\nformance. 3 Fine-tuning Flan-T5 (large) is competitive\nwith, but no better than, existing supervised methods,\nbut 4 supervising Flan-T5 with CoT reasoning elicited\nfrom GPT-3 substantially outperforms all other models.\npre-trained models for RE such as BART (Paolini\net al., 2021; Huguet Cabot and Navigli, 2021).\nIn this work we investigate the use of very large\nlanguage models—-including GPT-3 (Brown et al.,\n2020b)—for end-to-end relation extraction via gen-\neration. Our contributions are as follows.\n1. We show that few-shot learning with GPT-3\nyields near SOTA performance on standard RE\ndatasets, outperforming fully supervised models.\n2. We find that Flan-T5 (large; Chung et al. 2022)\nis not as capable, even when fine-tuned. But we\nthen propose an approach to training Flan-T5 with\nChain-of-Thought (CoT) style “explanations” (gen-\nerated automatically by GPT-3) that support rela-\ntion inferences; this achieves SOTA results.\n3. Evaluating the performance of generative mod-\nels for RE is non-trivial because one cannot rely\non exact matches to targets. We address this by\ncollecting a small amount of annotations scoring\ngenerated outputs against targets. We use these\n15566\nannotations to quantify the problem, identify erro-\nneous gold references and accurately evaluate our\nmodels.\nOur results indicate that, in general, LLMs should\nbe the default approach to RE, especially given\nthat one can train Flan-T5—which is dramatically\nsmaller than GPT-3, and publicly available—to\nachieve SOTA performance (Figure 1).\n2 RE via Text Generation\nWe treat RE as a conditional text generation task.\nConcretely, for a dataset of size N, we model\nthe probability of generating a linearized string y\nof a relation triplet (entity_1, relation_type,\nentity_2) conditioned on a context string C.\nSpecifically, Cincludes a chain of n linearized ex-\namples (xi, yi), with n << N. Formally:\npLM(y|C, x) =\nT∏\nt=1\np(yt|C, x, y<t)\nWe provide examples of context strings in the Ap-\npendix. We conduct experiments over four stan-\ndard RE datasets comprising varying numbers of\nentities and relation types, namely ADE (Gurulin-\ngappa et al., 2012), CoNLL (Roth and Yih, 2004),\nNYT (Riedel et al., 2010), and DocRED (Yao et al.\n2019); details in Table 1 and Appendix A.\nFollowing Huguet Cabot and Navigli (2021), we\nlinearize our target relation triplets. However, we\nadopt a much simpler scheme than prior work: We\nlinearize inputs with a single relation type (e.g.\nADE) as a list of tuples:\n[(drug, effect), ... ,(drug, effect)]\nFor inputs with multiple relation types (as in\nCoNLL04 and NYT), we form triplets compris-\ning a subject, relation, and object(along with\ntheir corresponding types), in the order of appear-\nance of the subject entity:\n[(entity_1:entity_1_type, relation_type,\nentity_2:entity_2_type),..]\nA training instance is then a pair of input text\nand a linearized target string:\nInput Bill Nelson, NASA administrator\nannounced the mars mission today.\nTarget [(Bill Nelson:Per, Work_For,\nNASA:Org)]\nEntity\nTypes\nRelation\nTypes\n# of relation triplets\nTrain Val Test\nADE 2 1 4,272 – –\nCoNLL04 4 5 922 231 288\nNYT 4 24 56,196 5,000 5,000\nDocRED 6 96 3,008 300 700\nTable 1: Dataset statistics. Train, validation and test\nindicate the number of relation triplets in each dataset.\nChallenges inherent to evaluating generative\nlarge language models for RE The expressiv-\nity of language models coupled with the open-\nendedness of RE makes evaluation difficult. This\nhas led to inconsistent approaches to evaluation\n(Taillé et al., 2020). Past work, especially that\npre-dating LLMs for the task, has tended to per-\nform “strict” evaluation, requiring exact matches\nbetween generated linearized relation tuples and\nreferences. This may be appropriate when is evalu-\nating smaller conditional generation models (such\nas BART) for RE, which have beenfine-tuned on\nlarge training sets, because after training such mod-\nels consistently generate standardized outputs. By\ncontrast, however, models like GPT-3 (or other\nlarge language models capable of zero- or few-shot\napplication) can produce a wide variety of output\nformats which convey similar content.\nFor example, given an input from ADE and\nprompted to list all drugs and associated adverse\nevents, a large language model might yield Aspirin:\nstomach pain, chest pain . Or it may instead out-\nput: Side effects of aspirin include cramping and\nstomach pain, and pain in the chest . There are\ncountless possible variants which may all commu-\nnicate the correct answer; we provide additional\nreal examples in the Appendix D. The flexibility\nof language means that parsing out the structured\nresult to compare it to a reference (to calculate\nstandard metrics like precision, recall, and F-1)\nis a non-trivial problem. This is in stark contrast\nto traditional approaches to tasks like NER and\nRE where models effectively classify input tokens\ninstead of generating new ones from a vast vocabu-\nlary.\nTraining models, either via traditional super-\nvised learning or in-context few-shot learning, en-\ncourages models to comport with the structure of\ntraining instances. We therefore focus our analysis\non such supervised settings in this work, starting\nwith an evaluation of few-shot learning with GPT-3\nfor RE. Nonetheless, even when supervised, LLMs\n15567\nused for RE are prone to generating outputs which\nmay be accurate but nonetheless differ from the tar-\nget. To address this, we enlist human annotators to\njudge whether the model outputs convey the same\ninformation as the reference targets.\n3 In-Context Few-Shot Learning with\nGPT-3 for RE\nIn this section we first describe our few-shot\nprompting strategy for GPT-3, and report the results\nrealized by this approach across a set of RE corpora.\nWe adopt forms of instructional in-context few-shot\nprompting to GPT-3.2 Motivated by the preceding\ndiscussion regarding evaluation challenges, we col-\nlect human annotations judging the model’s gener-\nations against the gold references. Finally, using\nthese annotations we report results achieved using\nGPT-3 with few-shot prompting for RE (Table 2).\nAll references to GPT-3 in this work refer to the\n“text-davinci-002” variant.\n3.1 Prompts\nWe describe the prompts we use for each of the\ndatasets considered in turn.\nADE To construct prompts for ADE, we use the\ninstructional prompt: List all (drug: adverse ef-\nfects) pairs in the following text , followed by an\ninput text. We then select 12 examples (“shots”)\nat random from the training set, and for each we\nappend the corresponding input followed by lin-\nearized target relations to the instructional prompt;\nthis yields a prompt featuring 12 examples, com-\nprising 755 tokens. To make a prediction for a new\nexample we append one last List all (drug: ad-\nverse effects) pairs in the following text instruction\nfollowed by the corresponding text and then ask\nGPT-3 to generate text conditioned on this final\nprefix. Specifically, we perform this generation\nusing default parameters save for sampling temper-\nature, which we set to 0.5.3 We impose a maximum\noutput length of 256 tokens.\nCoNLL As an instructional prefix for CoNLL,\nwe use: List the entities of the types [LOCATION,\nORGANIZATION, PERSON] and relations of types\n[Organization Based In, Work For, Located In, Live\nIn, Kill] among the entities in the given text. Since\n2We provide details on the costs incurred for each of these\nexperiments in the Appendix B.1.\n3In preliminary manual assessments, this seemed to yield\nqualitatively better outputs here than the default temperature.\nCoNLL is composed of four entity and five rela-\ntion types, we constructed our prompt manually\nto contain at least one example of each entity and\neach relation type, for a total of 12 exemplars in\nthe prompt. The total length of the CoNLL prompt\nwas 960 tokens. To ensure fair comparison to prior\nwork on generative RE over CoNLL, we use the\nsame validation set as Eberts and Ulges (2019a).\nNYT The large number of relations ( 24 in to-\ntal) in the NYT dataset precludes the possibility\nof providing detailed instructions enumerating all\nentity and relation types. We instead shorten the\ninstructional prefix by removing specific relation-\ntype descriptors and create a prompt with only 20\nexemplars capturing all entity and relation types.\nThe size of this prompt was 2095 tokens.\nWe next aim to evaluate the performance of GPT-\n3 for RE when provided the above prompts. But\ndoing so requires addressing the challenges inher-\nent to evaluating LLMs for RE outlined above (and\nin prior work; Taillé et al. 2020).\n3.2 Manually re-evaluating “errors”\nWe quantify the errors in evaluation that occur\nwhen one uses “strict” measures of performance\nwhile using few-shot prompted LLMs for RE\nacross each dataset. We do this by acquiring hu-\nman annotations (collected via Mechanical Turk;\ndetails in Appendix D) on model outputs, with\nrespect to reference labels provided in the accom-\npanying datasets. In particular, we show annota-\ntors ostensible “false positive” and “false negative”\noutputs produced by GPT-3 for these corpora—as\nwould be computed using exact matching against\nreferences—and ask them to judge whether these\nare accurately categorized.\nOn ADE we find that 51.67% of “false\npositives”—a slight majority—are more accurately\nviewed as true positives, and 32.61% of “false\nnegatives” are deemed as, in fact, true negatives.\nOn CoNLL outputs, annotators marked 50.27%\nof “false positives” as valid, and 36.6% of “false\nnegatives” as being accurate.\nAs mentioned above, we were unable to design a\nprompt for NYT that yielded reasonable few-shot\nresults with GPT-3. So we instead ask annotators\nto evaluate outputs from Flan-T5 fine-tuned on the\nNYT train set. In this case, they deemed36.9% and\n22.97% of “false positives” and “false negatives”,\nrespectively, to in fact be accurate. We present\n15568\nMethod Params CONLL ADE NYT\n1. Fully supervised\na. SpERT* (Eberts and Ulges, 2019b) 110M 71.54 79 .22 -\nb. TANL (Paolini et al., 2021) 220M 71.48 80 .61 90 .83\nc. TANL (MT) (Paolini et al., 2021) 220M 72.66 80 .00 90 .52\nd. REBEL (Huguet Cabot and Navigli, 2021) 460M 75.44 82 .21 92 .00\ne. Flan T5 (Large) (Chung et al., 2022) 760M 75.28 83 .15 91 .03\nf. + GPT-3-generated CoT 760M 80.76 92.17 95.23\n2. Few-shot\na. In-Context GPT-3 (Brown et al., 2020a) 175B 76.53 82 .66 61 .79\nb. + CoT 175B 78.18 - -\nc. Flan T5 (Large) w/ CoT Explanations and\nreference labels generated from GPT-3\n760M 76.13 - -\nTable 2: Comparison of (micro-F1) performance with recent generative (except SpERT) approaches in RE. Relation\ntriplets/pairs are considered correct only if both of the corresponding entity types are correctly generated.\nsome illustrative cases in Figure 2 and additional\nexamples in Appendix Tables 8 and 7.\nThese findings imply that strict (exact-matching)\nevaluation against references for RE will be inac-\ncurate (and pessimistic). In the results we later\nreport for LLMs, we therefore take into account\nthese manual assessments.4\n3.3 Results\nUsing the above prompts and manual annotation\nprocess just described, we find that in most cases\nGPT-3 performs comparably to current fully su-\npervised SOTA RE models without fine-tuning\nand given only 12-20 training examples . This\ncan be seen in Table 2 (2.a). We also find a substan-\ntial number of instances where the model correctly\nidentifies relation pairs, which in fact are incor-\nrectly marked in the references (detailed below in\nSection D). We observe additional issues with the\nNYT and CoNLL datasets which we discuss below.\nCoNLL We find a number of relation triplets\nwhere the output does not conform to the set of\nvalid relation types (∼% of relation triplets in the\nvalidation set). Examining these triplets, we often\nfind the out-of-domain relation-types to be either\nclosely related to a correct CoNLL relation-type\n(e.g., shoot−→kill) or otherwise correct even if not\nrelated to a CoNLL relation-type. There were a\ntotal of 18 input validation instances in which at\nleast one of the generated relation triplet did not\nconform to a valid CoNLL relation; we provide a\n4One could also train a model on manual assessments of\n“false positives” and “false negatives” to semi-automate this\nevaluation (avoiding the need to collect such judgments on\nentire testing sets); we provide results showing the feasibility\nof doing so in the Appendix D.\nfull list of these instances and the generated relation\ntriplets in the Appendix D.1.\nNYT We find the strategy of omitting the relation\ndescriptions in the prompt to be detrimental to the\nmodel’s performance. Contrary to our findings in\nADE and CONLL, we observe a sharp decline in\nMicro-F1 scores in case of NYT (∼30 point reduc-\ntion) as compared to the fully supervised SOTA.\nFurther, we observe a non-trivial number of in-\nvalid or empty output instances ( ∼10.6% of all\ngenerated sequences). These results highlight a re-\nmaining limitation of in-context learning with large\nlanguage models: for datasets with long texts or\na large number of targets, it is not possible to fit\ndetailed instructions in the prompt. In light of the\nissues we were unable to evaluate this approach\non the DocRED dataset, which we leave for future\nwork. In such cases, traditional fine-tuning is the\npractical option.\nDespite these limitations, the fact that GPT-3 is\nable to (marginally) outperform the current SOTA\nwith in-context learning from tens of examples is\nencouraging. But GPT-3 is a massive opaque model\navailable only via OpenAI’s API (at cost). Fur-\nther, fine-tuning GPT-3 would incur additional cost,\nand one would have access to the resultant model\nonly via the OpenAI interface. For these reasons,\nsmaller, open-source LLMs for RE would be prefer-\nable. Next we show that by enriching supervision\nwith Chain-of-Thought (CoT) outputs elicited from\nGPT-3, we can achieve SOTA performance using\nFlan-T5 (Large).\n15569\nFour days after the initial injection of 3.6 mg of goserelin \nacetate, severe dyspnea developed due to worsening \npleuritis carcinomatosa, which was considered as a ﬂare-up.\nReference   \n[(‘goserelin acetate’,'flare’)]\nGenerated  \n[(‘goserelin acetate','severe dyspnea’)]\nWrong, but counted \nas a false negative\nSome have called for a memorial to the lynched youth to join \nthe many other shrines here in Waco, a city of 113,000 \nneighboring President Bush's ranch in Crawford, and home \nto Baylor University, founded in 1845, the ﬁrst institution of \nhigher learning in Texas and the largest baptist university in \nthe world.\nReference  \n[(‘texas', '/location/contains', 'waco’)]\nGenerated \n[(‘texas', '/location/contains', 'waco’), \n (‘texas', '/location/contains', 'crawford’)]\nCorrect, but counted \nas false positives\nADE\nNYT\nCoNLL04\nOn Friday, U.S. Ambassador Vernon A. Walters displayed \nphotographs of one Libyan jet showing shapes resembling \nmissile pods on its wings and fuselage.\nReference \n[(‘Vernon A. Walters', 'Live_In', ‘U.S.’)]\nGenerated \n[(‘Amb. Vernon A. Walters', 'Work_For', ‘U.S’)]\nCorrect, but counted \nas a false positive\nWrong, but counted \nas a false negative\nCorrect, but counted \nas a false positive\nOut-of-Domain (CoNLL04)\nIn 1881 , President James A. Garﬁeld was shot by Charles J. \nGuiteau, a disappointed o ﬃce-seeker, at the Washington \nrailroad station.\nReference \n[('Charles J. Guiteau', 'Kill', 'President James A. Garfield')]\nGenerated \n[(‘James A. Garfield', 'Shot_By', 'Charles J. Guiteau')]\nFigure 2: Examples of misclassified FPs and FNs from GPT-3 (generated under few-shot in-context prompting\nscheme) under traditional evaluation of generative output. In each instance, the entity-type of subjectand object\nwas correctly identified.\n4 SOTA RE Performance with Flan-T5\nWe use Flan-T5 (Large), an LLM trained on a large\nnumber of tasks with instructional prompts. We\nfirst evaluate this in a few-shot setting (Section 4.1),\nshortening prompts in light of T5’s smaller size,\ncompared to GPT-3. We then consider fine-tuned\nvariants, including a novel approach in which we\ntrain Flan-T5 using chain-of-thought (CoT) style\nexplanations for RE elicited from GPT-3. The latter\nstrategy yields SOTA results across all datasets\nconsidered.\n4.1 Few-Shot RE with Flan-T5\nFor few-shot learning with Flan-T5, we use the\nsame instructional prefixes (with examples) as we\ndid for GPT-3 above, but we reduce the number\nof exemplars in the prompts to make them more\nconcise. We summarize our findings from these ex-\nperiments on ADE and CoNLL below, and provide\na full set of results in Appendix B.\nADE We include 7 (instead of the 12 used for\nGPT-3) randomly selected in-context examples for\nADE. We observe a significant increase in non-\nconforming relation pairs in outputs (13.9% of gen-\nerations). These often include outputs where the\nmodel generates the same token (or a set of tokens)\nrepeatedly, or where relation tuples contain greater\nor fewer than 2 entities. Unsurprisingly given these\nqualitative impressions, the model fares poorly un-\nder strict evaluation on the validation set, resulting\nin a ∼20 drop in F1 score compared to GPT-3.\nCoNLL The prompt for CONLL consisted of\n7 (in place of the 12 for GPT-3) exemplars in-\nserted into the instructional prefix described above.\nAgain we found that Flan-T5 generated many non-\nconforming outputs (12.5%). Additionally, we find\nthat Flan-T5 generates a large number of out-of-\ndomain relations between entities (over 120 unique\nrelations), most of which are unrelated to CoNLL,\nmaking it impossible to meaningfully evaluate out-\nputs (details in Appendix D).\nNYT We exclude this dataset given the large set\nof relation and entity types, which—as discussed\nabove—makes designing a prompt with sufficient\ninstructions that also fits within the in-context win-\ndow impossible. (We address this below via fine-\ntuning, which sidesteps the issue.)\nThese results indicate that few-shot learning with\nFlan-T5 is not competitive with GPT-3, and so is\nnot comparable to SOTA RE models. However, we\nnext show that fine-tuning Flan-T5 can yield sub-\nstantially better results, especially if one includes\nreasoning about RE in the supervision.\n4.2 Fine-tuning Flan-T5 for RE\nWe first perform standard fine-tuning for Flan-T5\n(Large) using available training datasets. We report\nresults from the test set in Table 2 (1.e.). This yields\nperformance equivalent to, but not better than, ex-\nisting fully supervised models such as REBEL.\nAs a potential mechanism to improve the perfor-\nmance of Flan-T5 for RE, we propose enriching the\nsupervision used to fine-tune the model with chain-\n15570\nText: Edward marks, an oﬃcial with the ITAR explained \ntheir position… \nTriplets: [Edward marks:PER, work_for, ITAR:ORG]\nExplanation: Edward Marks is an oﬃcial with the ITAR, \ntherefore it can be concluded that he works for ITAR.<s> \nText: NASA administrator Bill Nelson said in his historic \nspeech that this mission… \nTriplets: [Bill Nelson:PER, work_for, NASA:ORG]\nExplanation:  \nFew-shot In-Context Prompt\nBill Nelson is the administrator of NASA, therefore it \ncan be concluded that he works for NASA.<s>\nMassive LLM (GPT-3)\nCoT Explanations\nList all relations of the \ntype […], and provide a \nreasonable explanation. \nText: …\nText: …\nText: …\nText: …\nRelations: …\nExplanation: …  \nRelations: …\nExplanation: …  \nX Y\nRelations: …\nExplanation: …  \nRelations: …\nExplanation: …  \n…\n…\nSmaller LLM \n(Flan-T5 Large)\n{\nFigure 3: We propose fine-tuning Flan-T5 (large) for relation extraction (RE) using standard supervision and\nChain-of-Thought (CoT) reasoning elicited from GPT-3 for RE. This yields SOTA performance across all datasets\nconsidered, often by substantial margin (∼5 points absolute gain in F1).\nof-thought (CoT; Wei et al. 2022b) explanations,\nwhich we elicit automatically from GPT-3 over the\ntraining instances. Specifically, we craft a hand-\nful of such reasoning chains describing how target\nrelations can be derived from the input texts. We\nprovide the following three illustrative examples\nbelow.\nExample Input (ADE) To describe a case\nof severe skin necrosis resulting from\nperipheral intravenous administration of\nlow-dose vasopressin in a patient with\ncatecholamine-resistant septic shock.\nTarget [(vasopressin, skin necrosis)]\nExplanation A case of skin necrosis was\ndescribed after administration of\nlow-dose vasopressin.\nExample Input (CONLL) In Colorado , 13\ninches of snow in Denver Wednesday\nprompted officials to close Interstate\n270 temporarily.\nTarget [(Denver, ‘Located In’,\nColorado)]\nExplanation - Denver officials closed\nInterstate 270 in Colorado, consequently\nwe can see that Denver is located in\nColorado.\nExample Input (NYT) It will be the final\nmovie credited to Debra Hill, a film\nproducer and native of Haddonfield, who\nproduced “Halloween” and was considered\na pioneering woman in film.\nTarget [[Debra Hill:Per,\n‘place-of-birth’, Haddonfield:Loc]]\nExplanation - Debra Hill was a film\nproducer born (native of) in\nHaddonfield.\nNext we evaluate the impact of CoT explana-\ntions in two settings: As additional context for\nprompting GPT-3, and then as additional supervi-\nsion signal with which to train Flan-T5.\n4.2.1 Eliciting CoT reasoning for RE\nWe use the same prompts from the few-shot ex-\nperiments above but augment them with CoT-style\nexplanations (one per shot) written by one of the au-\nthors. This yields moderate gains in the overall per-\nformance for GPT-3 (∼3 and ∼2.2 micro-F1 points\nfor ADE and CONLL, respectively; Table 2 2.b),\nand also reduces the number of non-conforming\n15571\nrelations generated (from 13.9% to 0.8% on ADE,\nand from 12.5% to 1.1% on CONLL). Further, us-\ning CoT results in only one instance of an out-of-\ndomain relation-type generated on CoNLL, com-\npared to over 120 relations generated without CoT\nexplanations. In sum: using CoT in few-shot learn-\ning for RE with GPT-3 yields more standardized\noutputs, but does not much improve performance.\nNext we propose to capitalize on CoTs automat-\nically generated over training sets to enrich the\nsupervision with which we train Flan-T5.\n4.2.2 Fine-tuning Flan-T5 with CoT\nexplanations\nWe augment target relations used to train Flan-T5\nwith CoT strings automatically generated by GPT-3\nover the training dataset. Specifically, we modify\nthe prompt used in Section 3 to generate CoT-style\nexplanations conditioned on the input and rela-\ntion reference labels. The following is an example\nof the prompt we provide GPT-3 to elicit a CoT-\nexplanation:\nText: This April 14 is the 125th\nanniversary of the night when Lincoln,\nthe 16th president, was assassinated by\nJohn Wilkes Booth in the presidential\nbox at Ford’s Theatre.\nTarget [(John Wilkes Booth, ‘Kill’,\nLincoln)]\nExplanation - John Wilkes Booth\nassassinated Lincoln at the ford\ntheatre.<s>\nText: Ray is being held in Tennessee ’s\nBrushy Mountain State Prison on a\n99-year sentence for the April 4, 1968,\nslaying of King.\nTarget [[Ray, ‘Kill’, King]]\nExplanation -\nWe then use these explanations along with refer-\nence relation labels as targets to fine-tune Flan-T5\n(Large), as depicted in Figure 3. Overall, we found\nthis strategy to be effective obtaining state-of-the-\nart results across datasets, while being much faster\nto train compared with existing fully supervised\nmodels. We summarize our findings below, and\nreport results in Table 1 (1.f.).\nADE We obtain explanations for the entire training\nset and fine-tune Flan-T5 Large with an instruc-\ntional prefix with a batch size of 8, learning rate\n3e-5 for 6 epochs. The dataset defines 10 folds\nof train/test splits, and we evaluate using the best\ncheckpoint for each fold in the dataset. Our model\nyields a 9.97 point gain in micro F-1 score (aver-\naged over the folds) over the existing fully super-\nvised generative SOTA (REBEL; Huguet Cabot\nand Navigli (2021)).\nCONLL For CONLL, we again obtain CoT-style\nexplanations for the entire dataset via GPT-3. We\nthen fine-tune with a batch size of 4 and learn-\ning rate 3e-5 for 10 epochs and evaluate using the\nbest-performing checkpoint on the validation set.\nWe see a 5.42 absolute point gain on the micro-F1\nscore over the existing fully-supervised generative\nSOTA.\nNYT comprises 56k training examples. In this case\nwe generate CoT explanations via GPT-3 for only a\nsubset of 25k examples (about half of the train set),\ndue to its large size and the associated cost. We\nfine-tune the model with a batch size of 4, learning\nrate 2e-5 for 4 epochs and then evaluate using the\nbest performing checkpoint on the validation set.\nWe obtain a 3.37 point gain on the micro-F1 score\nover the existing fully-supervised SOTA.\nIn sum, fine-tuning Flan-T5 (large) with both\ntrain labels and CoT explanations produced\nby GPT-3 yields SOTA performance across RE\ndatasets by a considerable (5-10 points micro-\nF1) margin (Figure 1).\n4.2.3 “Fully Supervising” Flan with GPT-3\nAbove we showed that Flan-T5 (large) outperforms\nexisting RE methods by substantial margins when\ntrained using CoTs from GPT-3. Now we ask\nwhether we can take this approach of distillation\nfrom GPT-3 even further by eliciting both labels\nand CoT explanations from GPT-3 in a few-shot\nsetting, and then using these to train Flan-T5. That\nis, above we used the reference labels for training,\nwhereas here we use “labels” produced by GPT-3\ngiven just a handful (10s) of training instances as\nshots. We run this experiment only on CoNLL due\nto the cost of processing datasets in this way (which\nrequires running few shot inference in GPT-3 over\nentire training sets).\nTo generate the targets in this case, we start with\nan instructional prefix and 12 training instances\nfrom CoNLL and their corresponding human-\nwritten explanations; this is the same setup as the\nin-context GPT-3 model (Table 1 2.b.), though here\nwe apply this to the training instances. We then\nprompt GPT-3 on all training instances except for\nthe 12 shots to produce pseudo labels (relations)\n15572\nand associated CoT explanations.\nUsing this new GPT-generated training data, we\nagain fine-tune Flan-T5 (Large) as described above\n(Section 4.2.2), and evaluate it on the validation set.\nThis approach marginally outperforms the existing\nfully-supervised SOTA (Huguet Cabot and Navigli,\n2021), but underperforms fine-tuning Flan with ref-\nerences references and GPT-generated explanations\n(Table 2, 2.c.).\n5 Related work\nStandard NLP methods for identifying relations in\nfree text have included Conditional Random Fields\n(Lafferty et al., 2001), structured SVMs (Tsochan-\ntaridis et al., 2004), and more recently, training\nlarge deep learning models with a joint objective\n(Eberts and Ulges, 2021, 2019a; Wang and Lu,\n2020) to identify entities and relations simultane-\nously. More recently, the rise of massive language\nmodels (Radford and Narasimhan, 2018; Radford\net al., 2019; Brown et al., 2020a) has also motivated\nresearch into prompt-based learning methods for\nstructured prediction (Wang et al., 2022).\n5.1 Relation extraction with pre-trained LMs\nSeveral recently proposed RE approaches (which\nwe have built upon here) have proposed address-\ning the task using conditional generative models to\noutput string encodings—i.e., linearized forms—of\ntarget relations (Zeng et al., 2018, 2020; Nayak\nand Ng, 2020; Huguet Cabot and Navigli, 2021).\nPaolini et al. (2021) proposed a framework that\nformulated many structured prediction tasks, in-\ncluding relation extraction, as a seq2seq problem\nwhere they decode outputs into structured informa-\ntion. Huguet Cabot and Navigli (2021) extended\nthis line of work by training a SOTA BART-style\n(Lewis et al., 2020) model specifically for rela-\ntion extraction using a unique triplet linearization\nstrategy. Beyond these task-specific models, Wang\net al. (2022) proposed a task-agnostic structured\npre-training scheme which enables zero-shot trans-\nfer to several structured prediction tasks.\nThese past efforts focussed on solely fine-tuning\nseq2seq models, adopting standard supervised ap-\nproaches to learning to generate the relations ex-\npressed in a given input. (REBELincorporated a pre-\ntraining scheme designed for RE (Huguet Cabot\nand Navigli, 2021), but this was in addition to a\nfine-tuning step.) In this work we also evaluate the\nability of large language models to perform few-\nshot relation extraction via in-context learning; to\nour knowledge this is the first such evaluation for\nRE specifically, although few-shot learning more\ngenerally is an active sub-area of research.\n5.2 Few Shot In-Context Learning\nFew shot in-context learning entails incorporating\na few training examples into model prompts, ef-\nfectively “learning” via the activations induced\nby passing these examples through the network\nat inference time. This has the advantage of com-\npletely forgoing model weight updates, which can\nbe costly for LLMs (Wang et al., 2021). An active\narea of research concerns such cross-task gener-\nalization capabilities (Ye et al., 2021; Wei et al.,\n2022a; Min et al., 2022; Xu et al., 2022) of LLMs\nwhere a model learns a new, previously-unseen\ntask efficiently with just a few examples. Chen\net al. (2022) also proposed a self-supervised objec-\ntive as an intermediate stage between pre-training\nand downstream few-shot learning. Recent work\non few shot in-context learning has largely focused\non the selection (Liu et al., 2022) and ordering (Lu\net al., 2022a) of exemplars included in the prompt\nprovided to the model.\n6 Conclusions and Future Directions\nWe have evaluated the capabilities of modern large\nlanguage models (LLMs)—specifically GPT-3 and\nFlan T5 (Large)—on the task of Relation Extrac-\ntion (RE). We found that, when evaluated carefully,\nGPT-3 performs comparably to fully supervised\nstate-of-the-art (SOTA) models, given only 10s of\nexamples. We then proposed a distillation tech-\nnique in which we augmented target RE labels with\nChain of Thought (CoT) style explanations elicited\nfrom GPT-3 and used this to fine-tune Flan-T5;\nthis yielded SOTA performance across all datasets\nconsidered, often by wide margins (5-10 points in\nF1). Our results suggest that where feasible, LLMs\nshould be a standard baseline for RE.\nFuture directions We have left several avenues\nopen for further exploration. For example, evalu-\nating LLMs like GPT-3 for RE required collecting\nmanual annotations to identify ostensible “false\npositive” and “false negative” model outputs which\nwere in fact accurate. Designing models to auto-\nmate this evaluation might provide similar reliabil-\nity without the accompanying costs; we provide\npreliminary work in this direction through the use\nof simple BERT-style classifiers in Appendix D.\n15573\nLimitations\nWe have demonstrated that across three standard\nRE datasets, LLMs achieve SOTA results. In par-\nticular, GPT-3 yields such performance even given\nonly 10s of training sample for in-context learn-\ning. We then showed that we can similarly achieve\nSOTA performance with the much smaller (and\nopen-source) Flan T5 (Large) model, when trained\nusing CoT generations produced by GPT-3. We\nalso highlighted key challenges for evaluation in\nthis setting.\nBut there are important limitations to these\ncontributions. First, here we considered three\nstandard RE datasets with binary relations but—\nas we discussed—we excluded more complex\nRE datasets. For example, we did not consider\ncorpora containing n-ary relations between enti-\nties (Taboureau et al., 2010). We were also unable\nto run experiments on datasets with lengthy texts\nand a large number of relations, such as DocRED\n(Yao et al., 2021), due to the necessary prompt\nlengths for such inputs.\nSecond, while we found that CoT-style expla-\nnations generated by GPT-3 can be fruitfully used\nas additional supervision to fine-tune smaller lan-\nguage models, we made no attempt to evaluate the\nquality of these generated explanations which may\nhave an impact on the model performance.\nThird, we did not fine-tune GPT-3 on the RE\ndatasets, mainly due to the cost of doing so. It\nis likely that a fine-tuned GPT-3 would yield per-\nformance superior to the results we achieved with\nFlan T5 (which constitute current SOTA). But, in\naddition to the costs necessary for fine-tuning this\nmodel, the resultant weights would not be acces-\nsible to run locally in any case; one would have\naccess to it only via the OpenAI interface, which\nmotivated our decision to fine-tune the smaller and\nopen-source Flan T5 instead.\nFinally, we only experiment with datasets cu-\nrated in the English language and therefore, we do\nnot know that the issues we have highlighted could\nreplicate in the same way in other languages.\nEthics Statement\nOur work required an extensive manual annota-\ntion and evaluation process which involved using\nAmazon Mechanical Turk. Turk requires we pay\nworkers per annotation, so we have to estimate\nthe time required for each task. To do so, we (the\nauthors) carried out a small number of these an-\nnotations ourselves to determine fair approximate\nhourly compensation. We then set the price per an-\nnotation such that it averages out to $15/hour (we\npay this rate irrespective of geographic location of\nthe workers). We also provided our recruited AMT\nworkers 20% additional time per annotation.\nAcknowledgements\nThis work was supported in part by the National In-\nstitutes of Health (NIH) under the National Library\nof Medicine (NLM) grant R01LM012086 and by\nthe National Science Foundation (NSF) grant III-\n1750978.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020a.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020b.\nLanguage models are few-shot learners. ArXiv,\nabs/2005.14165.\nMingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor\nMihaylov, Srini Iyer, Veselin Stoyanov, and Zor-\nnitsa Kozareva. 2022. Improving in-context few-shot\nlearning via self-supervised training. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3558–3573,\nSeattle, United States. Association for Computational\nLinguistics.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\n15574\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMarkus Eberts and Adrian Ulges. 2019a. Span-based\njoint entity and relation extraction with transformer\npre-training. ArXiv, abs/1909.07755.\nMarkus Eberts and Adrian Ulges. 2019b. Span-based\njoint entity and relation extraction with transformer\npre-training. CoRR, abs/1909.07755.\nMarkus Eberts and Adrian Ulges. 2021. An end-to-end\nmodel for entity-level relation extraction using multi-\ninstance learning. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, pages\n3650–3660, Online. Association for Computational\nLinguistics.\nHarsha Gurulingappa, Abdul Mateen Rajput, Angus\nRoberts, Juliane Fluck, Martin Hofmann-Apitius, and\nLuca Toldo. 2012. Development of a benchmark\ncorpus to support the automatic extraction of drug-\nrelated adverse effects from medical case reports.\nJournal of biomedical informatics, 45 5:885–92.\nPere-Lluís Huguet Cabot and Roberto Navigli. 2021.\nREBEL: Relation extraction by end-to-end language\ngeneration. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021 , pages 2370–\n2381, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nJohn D. Lafferty, Andrew McCallum, and Fernando\nC. N. Pereira. 2001. Conditional random fields:\nProbabilistic models for segmenting and labeling se-\nquence data. In Proceedings of the Eighteenth In-\nternational Conference on Machine Learning, ICML\n’01, page 282–289, San Francisco, CA, USA. Morgan\nKaufmann Publishers Inc.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022a. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nYaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu\nLin, Xianpei Han, Le Sun, and Hua Wu. 2022b. Uni-\nfied structure generation for universal information\nextraction. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5755–5772, Dublin,\nIreland. Association for Computational Linguistics.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022. MetaICL: Learning to learn\nin context. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2791–2809, Seattle, United States.\nAssociation for Computational Linguistics.\nTapas Nayak and Hwee Tou Ng. 2020. Effective mod-\neling of encoder-decoder architecture for joint entity\nand relation extraction. In AAAI Conference on Arti-\nficial Intelligence.\nGiovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie\nMa, Alessandro Achille, RISHITA ANUBHAI, Ci-\ncero Nogueira dos Santos, Bing Xiang, and Stefano\nSoatto. 2021. Structured prediction as translation be-\ntween augmented natural languages. In International\nConference on Learning Representations.\nAlec Radford and Karthik Narasimhan. 2018. Im-\nproving language understanding by generative pre-\ntraining.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nSebastian Riedel, Limin Yao, and Andrew McCallum.\n2010. Modeling relations and their mentions without\nlabeled text. In ECML/PKDD.\nDan Roth and Wen-tau Yih. 2004. A linear program-\nming formulation for global inference in natural lan-\nguage tasks. In Proceedings of the Eighth Confer-\nence on Computational Natural Language Learn-\ning (CoNLL-2004) at HLT-NAACL 2004, pages 1–8,\nBoston, Massachusetts, USA. Association for Com-\nputational Linguistics.\n15575\nOlivier Taboureau, Sonny Kim Nielsen, Karine Au-\ndouze, Nils Weinhold, Daniel Edsgärd, Francisco S.\nRoque, Irene Kouskoumvekaki, Alina Bora, Ramona\nCurpan, Thomas Skøt Jensen, Søren Brunak, and Tu-\ndor I. Oprea. 2010. ChemProt: a disease chemical\nbiology database. Nucleic Acids Research, 39:D367–\nD372.\nBruno Taillé, Vincent Guigue, Geoffrey Scoutheeten,\nand Patrick Gallinari. 2020. Let’s Stop Incorrect\nComparisons in End-to-end Relation Extraction! In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3689–3701, Online. Association for Computa-\ntional Linguistics.\nIoannis Tsochantaridis, Thomas Hofmann, Thorsten\nJoachims, and Yasemin Altun. 2004. Support vector\nmachine learning for interdependent and structured\noutput spaces. In Proceedings of the Twenty-First In-\nternational Conference on Machine Learning, ICML\n’04, page 104, New York, NY , USA. Association for\nComputing Machinery.\nChenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong,\nJie Tang, and Dawn Song. 2022. DeepStruct: Pre-\ntraining of language models for structure prediction.\nIn Findings of the Association for Computational Lin-\nguistics: ACL 2022, pages 803–823, Dublin, Ireland.\nAssociation for Computational Linguistics.\nJue Wang and Wei Lu. 2020. Two are better than\none: Joint entity and relation extraction with table-\nsequence encoders. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1706–1721, Online. As-\nsociation for Computational Linguistics.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want to reduce la-\nbeling cost? GPT-3 can help. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 4195–4205, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned\nlanguage models are zero-shot learners. ArXiv,\nabs/2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang-\ngang Wang, Haiyu Li, and Zhilin Yang. 2022. Ze-\nroprompt: Scaling prompt-based pretraining to 1,\n000 tasks improves zero-shot generalization. CoRR,\nabs/2201.06910.\nYuan Yao, Jiaju Du, Yankai Lin, Peng Li, Zhiyuan Liu,\nJie Zhou, and Maosong Sun. 2021. CodRED: A\ncross-document relation extraction dataset for acquir-\ning knowledge in the wild. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4452–4472, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nYuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin,\nZhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou,\nand Maosong Sun. 2019. DocRED: A large-scale\ndocument-level relation extraction dataset. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 764–777,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\nCrossFit: A few-shot learning challenge for cross-\ntask generalization in NLP. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7163–7189, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nDaojian Zeng, Haoran Zhang, and Qianying Liu. 2020.\nCopymtl: Copy mechanism for joint extraction of\nentities and relations with multi-task learning. ArXiv,\nabs/1911.10438.\nXiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu,\nand Jun Zhao. 2018. Extracting relational facts by\nan end-to-end neural model with copy mechanism.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 506–514, Melbourne, Australia.\nAssociation for Computational Linguistics.\n15576\nModel Data P R F-1\nFew-Shot In-Context\nPrompting GPT-3\nADE 80.85 84.54 82.66\nCoNLL 78.31 74.82 76.53\nNYT 66.63 70.58 68.55\nVanilla Fine-Tune\nFlan-T5-Large\nADE 89.11 77.93 83.15\nCoNLL 78.81 72.05 75.28\nNYT 91.82 90.25 91.03\nFine-Tune Flan\non GPT-3-generated CoT\nADE 91.74 92.60 92.17\nCoNLL 81.22 80.31 80.76\nNYT 95.49 94.97 95.23\nFine-Tune Flan w/ CoT\nExplanations and Reference\nlabels generated from GPT\nCoNLL 76.41 75.85 76.13\nTable 3: Average micro metrics over 5 seeds for the test\nsets (10-folds for ADE).\nA Datasets\nWe considered and conducted the evaluation of\nour methods on the following datasets. Basic data\nstatistics are also reported in Table 1.\nADE Adverse Drug Events (Gurulingappa et al.,\n2012) contains binary relations of (drug, adverse\nevent) pairs. Drugs and adverse events are the only\ntwo entity types. This dataset provides a 10-fold\nsplit.\nCONLL04 The CoNLL04 consists of sentences\nfrom news articles that were annotated for the men-\ntioned entities and relations between entities (Roth\nand Yih, 2004). It includes four entity types\n(PER, ORG, LOC, OTH) and five possible relations\n(KILL, WORK _FOR, LIVE _IN, LOCATED _IN,\nORG_BASED_IN).\nNYT The NYT comprises sentences sampled\nfrom New York Times news articles published be-\ntween 1987 and 2007 (Riedel et al., 2010). The\ndata was distantly annotated with relations triplets\nfrom FreeBase. We use a processed version of\nNYT (Zeng et al., 2018) containing three overlap-\nping entity types (LOC, PER, ORG) and 24 relation\ntypes.\nDocRED Originally designed as a relation clas-\nsification task, DocRED (Yao et al., 2019) differs\nconsiderably from the other datasets considered in\nthis work in two important ways: (1) It comprises\nlong texts which feature relations between entities\nat a document-level; (2) It contains annotations for\n6 entity types and 96 relation types, with an aver-\nage of 19.9 entities and 19.5 relation instances per\ndocument.\nB Models and Reproducibility\nWe provide average micro metrics over 5 seeds\nacross each dataset in Table 3. On Flan-T5-Large,\nwhere we do fine-tuning, some hyperparameters\nwere manually tuned but most left at their default\nvalues. The final values for the ones that were\nmanually tuned are provided in Table 4.\nWe perform all experiments with a single\nNVIDIA Quadro RTX 8000 with 64GB of RAM\non an Intel Xeon E502680v4 (2.4GHz).\nB.1 Costs ( $$$)\nWe provide details on the costs we incurred while\nrunning experiments on GPT-3 in Table 5.\nC Prompts\nWe use the following prompt elements as few-shot\nexemplars corresponding to each dataset in our\nevaluation. Inputs and target references are directly\nextracted from the original training sets while the\nexplanations are human-written and were added\nwhen necessary for the experiments described in\nsection 3 and 4.\nADE\nExample Instructional Prefix: List all\n[drug, adverse effects] pairs in the\nTEXT provided below.\nTEXT: We report on three observations of\nparkinsonian patients with\nlevo-dopa-induced diphasic dyskinesias,\nwho received subcutaneous apomorphine to\nreduce the duration of abnormal\nmovements.\nRelations: [[’levo-dopa’, ’diphasic\ndyskinesias’]]\nExplanation: levo-dopa induced diphasic\ndyskinesias in parkinsonian patients.<s>\nTEXT: A girl with cystic fibrosis and\ncyclic neutropenia developed an\nerythematous papular eruption without\nfever or neutrophilia 7 months after\ncommencing therapy with G-CSF.\nRelations: [[’G-CSF’, ’erythematous\npapular eruption’]]\nExplanation: G-CSF therapy caused\nerythematous papular eruption in a girl\nwith cystic fibrosis.<s>\nTEXT: Hypersensitivity to carboplatin is\na rare but real complication of therapy\nand should be considered in patients\npresenting with hyperacute changes on\nECG whilst receiving carboplatin\ntherapy.\n15577\nModel Data Batch Size Warm-up Learning Rate Time/Epoch\n(minutes) Max Epochs\nVanilla Fine-Tune\nFlan-T5-Large\nADE 8 10% 3e-5 36 6\nCoNLL 4 12% 3e-5 22 10\nNYT 4 12% 2e-5 99 4\nFine-Tune Flan on\nGPT-3-generated CoT\nADE 8 10% 3e-5 38 6\nCoNLL 4 12% 3e-5 28 10\nNYT 4 12% 2e-5 107 4\nFine-Tune Flan w/ CoT\nExplanations and Reference\nlabels generated from GPT\nADE 8 10% 3e-5 37 6\nCoNLL 4 12% 3e-5 28 10\nNYT 4 12% 2e-5 109 4\nTable 4: Hyperparameters and compute time for the fully fine-tuned Flan models (corresponding to main results\ntable 2).\nExperiment Data Cost (US$)\nEvaluation of\nFew-Shot In-Context\nPrompting\nADE 64.91\nCoNLL 19.24\nNYT 238.70\nGeneration of CoT\nExplanations (Training Set)\nADE 93.96\nCoNLL 44.20\nNYT 983.86\nGeneration of Target\nLabels + CoT Explanations CoNLL 86.41\nTable 5: Summary of costs incurred by prompting and\nusing GPT-3 as a labeler for RE.\nRelations: [[’carboplatin’, ’hyperacute\nchanges on ECG’], [’carboplatin’,\n’Hypersensitivity’]]\nExplanation: Patients who undergo\ncarboplatin therapy are prone to\nhypersensitivity and hyperacute changes\non their ECG.<s>\nTEXT: The diagnosis of hypothermia was\ndelayed until it was apparent for\nseveral days but resolved with the\ndiscontinuation of risperidone and\ncontinuation of clozapine.\nRelations: [[’risperidone’,\n’hypothermia’]]\nExplanation: risperidone caused\nhypothermia since it was resolved with\nits discontinuation.<s>\nTEXT: Eighty-two patients with various\nmalignancies who received\nimipenem/cilastatin 143 times for\nneutropenic fever between March 1994 and\nOctober 1999 in Department of Pediatric\nOncology, Gazi University, were\nidentified.\nRelations: [[’cilastatin’, ’neutropenic\nfever’], [’imipenem’, ’neutropenic\nfever’]]\nExplanation: Patients who received\neither cilastatin or imipenem were\nidentified with neutropenic fever.<s>\nTEXT: This increase when clozapine was\nswitched to risperidone and vice versa\nis consistent with our previous report\nof elevated serum triglyceride levels in\nclozapine-treated patients.\nRelations: [[’clozapine’, ’elevated\nserum triglyceride levels’]]\nExplanation: There was a report of\nelevated serum triglyceride levels in\nclozapine-treated patients.<s>\nTEXT: Autopsy findings were consistent\nwith bleomycin and oxygen-induced\npulmonary damage.\nRelations: [[’bleomycin’, ’pulmonary\ndamage’], [’oxygen’, ’pulmonary\ndamage’]]\nExplanation: Both bleomycin and oxygen\ncaused pulmonary damage in the autopsy\nfindings.<s>\nTEXT: CD4 T-lymphocyte depletion,\nmyelosuppression, and subsequent severe\ninfections are the major side effects of\nfludarabine phosphate therapy.\nRelations: [[’fludarabine phosphate’,\n’CD4 T-lymphocyte depletion’],\n[’fludarabine phosphate’,\n’myelosuppression’], [’fludarabine\nphosphate’, ’severe infections’]]\nExplanation: Following major\nside-effects are known of fludarabine\nphosphate therapy, CD4 T-lymphocyte\ndepletion, myelosuppression, and severe\n15578\ninfections.<s>\nTEXT: OBJECTIVE: To describe a case of\nsevere skin necrosis resulting from\nperipheral intravenous administration of\nlow-dose vasopressin in a patient with\ncatecholamine-resistant septic shock.\nRelations: [[’vasopressin’, ’skin\nnecrosis’]]\nExplanation: A case of skin necrosis was\ndescribed after administration of\nlow-dose vasopressin.<s>\nTEXT: In vitro inhibition of\nhematopoiesis in a patient with systemic\nsclerosis treated with D-penicillamine.\nRelations: [[’D-penicillamine’,\n’inhibition of hematopoiesis’]]\nExplanation: Patient treated with\nD-penicillamine had in vitro inhibition\nof hematopoiesis.<s>\nTEXT: PURPOSE: We report an unusual\nparadoxical effect of brimonidine.\nRelations: [[’brimonidine’, ’paradoxical\neffect’]]\nExplanation: paradoxical effect of\nbrimonidine was reported.<s>\nTEXT: Hepatocellular damage following\ntherapeutic intravenous iron sucrose\ninfusion in a child.\nRelations: [[’iron sucrose’,\n’Hepatocellular damage’]]\nExplanation: Hepatocellular damage\noccurred in a child after infusion of\niron sucrose.<s>\nCoNLL\nExamplee Instructional Prefix: List the\nrelations of the types [OrgBased In,\nWork For, Located In, Live In, Kill]\namong the entities [PERSON, LOCATION,\nORGANIZATION, OTHER] in the given text\nand provide a reasonable explanation.\nTEXT: “If it does not snow, and a lot,\nwithin this month we will have no water\nto submerge 150,000 hectares (370,500\nacres) of rice”, said Bruno Pusterla, a\ntop official of the Italian Agricultural\nConfederation.\nRelations: [[’Bruno Pusterla:Per’, ’Work\nFor’, ’Italian Agricultural\nConfederation:Org’]]\nExplanation: Bruno Pusterla is a top\nofficial of the Italian Agricultral\nConfederation.<s>\nTEXT: Meanwhile, Shi Liming at the\nInstitute of Zoology of Kunming found\nthat pandas lack variety in their\nprotein heredity, which may serve as one\nof the major reasons for pandas’ near\nextinction.\nRelations: [[’Shi Liming:Per’, ’Work\nFor’, ’Institute of Zoology:Org’],\n[’Institute of Zoology:Org’, ’OrgBased\nIn’, ’Kunming:Loc’]]\nExplanation: Shi Liming works for the\nInstitute of Zoology, which is an\norganization based in Kunming.<s>\nTEXT: The viewers of “JFK” and “The Men\nWho Killed Kennedy” never learn about\nthese facts, nor do they ever learn\nabout all of the other massive body of\nevidence that conclusively proves beyond\na reasonable doubt that Oswald was the\nlone gunman who killed President Kennedy\nand Officer Tippit and that there was no\ncoverup by Earl Warren or by the Warren\nCommission.\nRelations: [[’Oswald:Per’, ’Kill’,\n’President Kennedy:Per’], [’Oswald:Per’,\n’Kill’, ’Officer Tippit:Per’]]\nExplanation: Oswald was the lone gunman\nwho killed President Kennedy and Officer\nTippit.<s>\nTEXT: PURCHASE, N.Y .\nRelations: [[’PURCHASE:Loc’, ’Located\nIn’, ’N.Y.:Loc’]]\nExplanation: PURCHASE is a place located\nin N.Y..<s>\nTEXT: BELGRADE, Yugoslavia (AP)\nRelations: [[’BELGRADE:Loc’, ’Located\nIn’, ’Yugoslavia:Loc’], [’AP:Org’,\n’OrgBased In’, ’BELGRADE:Loc’],\n[’AP:Org’, ’OrgBased In’,\n’Yugoslavia:Loc’]]\nExplanation: City of BELGRADE is located\nin Yugoslavia and AP is an organization\nbased in BELGRADE, Yugoslavia.<s>\nTEXT: Rome is in Lazio province and\nNaples in Campania.\nRelations: [[’Rome:Loc’, ’Located In’,\n’Lazio:Loc’], [’Naples:Loc’, ’Located\n15579\nIn’, ’Campania:Loc’]]\nExplanation: Rome is a place located in\nLazio and Naples is a place located in\nCampania.<s>\nTEXT: (By ITAR-TASS correspondent\nMikhail Shevtsov)\nRelations: [[’Mikhail Shevtsov:Per’,\n’Work For’, ’ITAR-TASS:Org’]]\nExplanation: Mikhail Shevtsov is a\ncorrespondent for the ITAR-TASS.<s>\nTEXT: In the communique, the Group of\nRio states that the Haitian crisis can\nbe resolved only if unrestricted respect\nis shown for the Governor’s Island\nAgreement which calls for the prompt\nreturn of Haitian President Jean\nBertrand Aristide to the exercise of his\nconstitutional powers in Haiti.\nRelations: [[’Jean Bertrand\nAristide:Per’, ’Live In’, ’Haiti:Loc’]]\nExplanation: Jean Bertrand Aristide was\nthe president of Haiti and therefore\nlived in Haiti.<s>\nTEXT: Moscow ITAR-TASS\nRelations: [[’ITAR-TASS:Org’, ’OrgBased\nIn’, ’Moscow:Loc’]]\nExplanation: ITAR-TASS is an\norganization based in Moscow.<s>\nTEXT: King rose to prominence after Mrs.\nParks ’ action in December 1955 in\nMontgomery , Ala. , set the stage for a\nboycott and subsequent demonstrations\nthat caught the nation by surprise.\nRelations: [[’Mrs. Parks:Per’, ’Live\nIn’, ’Montgomery:Loc’], [’Mrs.\nParks:Per’, ’Live In’, ’Ala.:Loc’],\n[’Montgomery:Loc’, ’Located In’,\n’Ala.:Loc’]]\nExplanation: Mrs. Parks actions were in\nMontgomery, Ala., where she lived. It\ncan be derived that Montgomery is\nlocated in Ala..<s>\nTEXT: Sirhan says he was the lone\nassassin but can’t remember shooting\nKennedy.\nRelations: [[’Sirhan:Per’, ’Kill’,\n’Kennedy:Per’]]\nExplanation: Sirhan was the lone\nassassin in the Kennedy\nassassination.<s>\nTEXT: In Colorado, 13 inches of snow in\nDenver Wednesday prompted officials to\nclose Interstate 270 temporarily.\nRelations: [[’Denver:Loc’, ’Located In’,\n’Colorado:Loc’]]\nExplanation: Denver officials closed\nInterstate 270 in Colorado, consequently\nwe can see that Denver is located in\nColorado.<s>\nTEXT: Edward Marks, an official with the\nMontgomery County Democratic Party,\nargued that if Ms. Toth is not\ninterested in the job, “she should get\nout”.\nRelations: [[’Edward Marks:Per’, ’Work\nFor’, ’Montgomery County Democratic\nParty:Org’]]\nExplanation: Edward Marks is an official\nthat works for the Montgomery County\nDemocratic Party.<s>\nNYT\nTEXT: Massachusetts ASTON MAGNA Great\nBarrington; also at Bard College,\nAnnandale-on-Hudson, N.Y., July 1-Aug.\nRelations: [[’Annandale-on-Hudson’,\n’/location/location/contains’, ’Bard\nCollege’]]\nExplanation: Annandale-on-Hudson is a\nlocation in N.Y. that contains Bard\nCollege.<s>\nTEXT: It will be the final movie\ncredited to Debra Hill, a film producer\nand native of Haddonfield, who produced\n“Halloween” and was considered a\npioneering woman in film.\nRelations: [[’Debra Hill:Per’,\n’/people/person/place-of-birth’,\n’Haddonfield:Loc’]]\nExplanation: Debra Hill was a film\nproducer born (native of) in\nHaddonfield.<s>\nTEXT: Under pressure from Mr. Kerkorian\nand other disgruntled shareholders, Mr.\nWagoner started talks on Friday in\nDetroit with Carlos Ghosn, the chief\nexecutive of Renault and Nissan.\nRelations: [[’Carlos Ghosn:Per’,\n’/business/person/company’,\n’Renault:Org’]]\n15580\nExplanation: Carlos Ghosn is a business\nperson (chief executive) associated with\nRenault and Nissan.<s>\nTEXT: Mr. Ferrer still holds commanding\nleads over the other two Democrats in\nthe race – United States Representative\nAnthony D. Weiner of Brooklyn and\nQueens, and City Council Speaker Gifford\nMiller – and is also ahead of Mayor\nMichael R. Bloomberg in most polls.\nRelations: [[’Anthony D. Weiner:Per’,\n’/people/person/place-lived’,\n’Brooklyn:Loc’], [’Anthony D.\nWeiner:Per’,\n’/people/person/place-lived’,\n’Queens:Loc’]]\nExplanation: Anthony D. Weiner is a\nperson representing Brooklyn and Queens,\ntherefore we can infer he lives in those\nplaces.<s>\nTEXT: Quebec, Canada’s second most\npopulous province, after Ontario, has\nnot decided to go that far.\nRelations: [[’Ontario:Loc’,\n’/location/administrative-division/country’,\n’Canada:Loc’], [’Canada:Loc’,\n’/location/location/contains’,\n’Ontario:Loc’], [’Canada:Loc’,\n’/location/country/administrative-divisions’,\n’Ontario:Loc’]]\nExplanation: Ontario is a place located\nin the administrative divisions of the\ncountry Canada. Quebec is Canada’s\nsecond most populous province and hence,\nCanada is a place that contains\nQuebec.<s>\nTEXT: And Abu Izzadeen , who converted\nto Islam at 17 and heads another\nsuccessor group to Al Muhajiroun, called\nAl Ghurabaa, called suicide bombing\n“martyrdom operations”.\nRelations: [[’Abu Izzadeen:Per’,\n’/people/person/religion’, ’Islam:Org’]]\nExplanation: Since Abu Izzadeen\nconverted to Islam at the age of 17, we\ncan infer that this is a person who\nbelongs to the religion of Islam.<s>\nTEXT: And yet, despite the success of\nits exhibitions, the institute remains\nsomething of a strange hybrid: located\nsoutheast of Notre-Dame, in a striking\nbuilding designed by Jean Nouvel, it has\noperated since 1987 as a partnership\nbetween France and 22 Arab countries.\nRelations: [[’Jean Nouvel:Per’,\n’/people/person/nationality’,\n’France:Loc’]]\nExplanation: Jean Nouvel was a french\ndesigner and we can derive his\nnationality/citizenship as French or\nFrance.<s>\nTEXT: They could have done it Sunday,\nwhen we were closed,” said Joseph\nBastianich, who owns Del Posto with his\nmother, Lidia Bastianich, and the chef,\nMario Batali.\nRelations: [[’Lidia Bastianich:Per’,\n’/people/person/children’, ’Joseph\nBastianich:Per’]]\nExplanation: Joseph Bastianich owns Del\nPosto with his mother Lidia\nBastianich.<s>\nTEXT: A French court sentenced six\nAlgerian-French men to prison terms of\nup to 10 years on Tuesday for their role\nin a 2001 plot to attack the United\nStates Embassy in Paris , closing the\nbooks on one of France ’s most serious\nterrorist cases.\nRelations: [[’Paris:Loc’, ’/location/\nadministrative-division/country’,\n’France:Loc’], [’France:Loc’,\n’/location/location/contains’,\n’Paris:Loc’], [’France:Loc’,\n’/location/country/\nadministrative-divisions’, ’Paris:Loc’],\n[’France:Loc’,\n’/location/country/capital’,\n’Paris:Loc’]]\nExplanation: Paris is located in the\nadministrative divisons of the country\nFrance. Consequently, France is a place\nthat contains Paris. US embassies are\nlocated in the capital of countries,\ntherefore it can be inferred that Paris\nis the capital of France.<s>\nTEXT: Anheuser-Busch, which has been the\nexclusive beer sponsor for the Super\nBowl since 1989, will do so again for\nthe Super Bowls in 2007 and 2010 on CBS\nand in 2008 and 2011 on Fox Broadcasting\n15581\n, said Anthony T. Ponturo, vice\npresident for global media and sports\nmarketing at Anheuser-Busch in St.\nLouis.\nRelations: [[’Anheuser-Busch:Org’,\n’/business/company/place-founded’, ’St.\nLouis:Loc’], [’St. Louis:Loc’,\n’/location/location/contains’,\n’Anheuser-Busch:Org’]]\nExplanation: Anheuser-Busch is a\nbusiness that was founded in St. Louis.\nConsequently, St. Louis is a place that\ncontains Anheuser-Busch.<s>\nTEXT: Somewhat chastened by his retreat\nin the polls, Mr. Blair acknowledged\nthat Britons had turned against him in\npart over accusations that he led them\ninto a war in Iraq on dubious legal\ngrounds and on the false premise that\nSaddam Hussein presented a direct threat\nbecause of a supposed arsenal of\nunconventional weapons that was never\nfound.”\nRelations: [[’Saddam Hussein:Per’,\n’/people/deceased-person/place-of-death’,\n’Iraq:Loc’], [’Saddam Hussein:Per’,\n’/people/person/place-of-birth’,\n’Iraq:Loc’], [’Saddam Hussein:Per’,\n’/people/person/nationality’,\n’Iraq:Loc’]]\nExplanation: Saddam Hussein was killed\nin Iraq. His place of birth was also\nIraq. We can infer that his nationality\nwas Iraq.<s>\nTEXT: Rupert Murdoch and John C. Malone\n, who have wrangled for two years over\nMr. Malone ’s challenge to Mr. Murdoch\n’s control of the News Corporation ,\nhave made peace . Relations: [[’Rupert\nMurdoch’, ’/business/person/company’,\n’News Corporation’], [’News Corporation’,\n’/business/company/founders’, ’Rupert\nMurdoch’]] Explanation: Rupert Murdoch\nis a business person associated with\nNews Corporation, which was a company\nfounded by Rupert Murdoch.<s>\nTEXT: Manhattan, especially the East\nVillage , has long been well stocked\nwith cheap and raucous yakitori places\nthat specialize in skewers and beer.\nRelations: [[’Manhattan:Loc’,\n’/location/location/contains’, ’East\nVillage:Loc’], [’East Village:Loc’,\n’/location/neighborhood/neighborhood-of’,\n’Manhattan:Loc’]]\nExplanation: East Village is a\nneighborhood in Manhattan.<s>\nTEXT: HEADING OUT – Sanford I. Weill\nstepped down as chairman of Citigroup ,\nthe worldwide financial supermarket he\nhad meticulously and single-mindedly\nstitched together through dozens of\nmergers and acquisitions.\nRelations: [[’Citigroup:Org’,\n’/business/company/advisors’, ’Sanford I.\nWeill:Per’]]\nExplanation: Citigroup is a business\ncompany who was associated with (advised\nby) Sanford I. Weill.<s>\nTEXT: He had decided to use the premiere\nto publicize the issue; his plan was to\ninvite the neighborhood’s Russian\nspeakers to sign a petition against\npiracy, a common practice at the area’s\nRussian-language video outlets, which\nsell films and music from Russia and by\nRussian immigrants in the United States.\nRelations: [[’Russian:Per’,\n’/people/ethnicity/\ngeographic-distribution’, ’Russia:Loc’]]\nExplanation: Russian is an ethnicity in\nUnited States associated with immigrants\nwho came from the geographic\ndistribution of Russia.<s>\nTEXT: In 1995, Cleveland successfully\nlobbied to have the name Cleveland\nBrowns stay in that city after that\nvenerable franchise’s owner, Art Modell,\nopted to move it to Baltimore.\nRelations: [[’Cleveland:Loc’,\n’/sports/sports-team-location/teams’,\n’Cleveland Browns:Org’], [’Cleveland\nBrowns:Org’,\n’/sports/sports-team/location’,\n’Cleveland:Loc’]]\nExplanation: Cleveland Browns is the\nsports franchise located in Cleveland,\nconsequently Cleveland’s sports team is\nCleveland Browns.<s>\nTEXT: Mr. Fields, speaking from vacation\nin France, added, “That a mogul like\n15582\nSumner Redstone could make a statement\nso vicious, so pompous, so petulant as\nthat he didn’t want to make a deal with\nTom Cruise because of his personal\nconduct – it tells you more about Sumner\nRedstone and Viacom, than about Tom\nCruise”.\nRelations: [[’Sumner Redstone:Per’,\n’/business/ company-shareholder/\nmajor-shareholder-of’, ’Viacom:Org’]]\nExplanation: Sumner Redstone is a major\nshareholder of the company Viacom.<s>\nTEXT: It is a room of paintings by\nLeonard Peltier , a citizen of the\nAnishinabe and Dakota and Lakota nations\nwho is serving two consecutive life\nterms in Pennsylvania for the murder of\ntwo F.B.I. agents on the Pine Ridge\nReservation in South Dakota.\nRelations: [[’Leonard Peltier:Per’,\n’/people/person/ethnicity’,\n’Lakota:Per’], [’Lakota:Per’,\n’/people/ethnicity/people’, ’Leonard\nPeltier:Per’]]\nExplanation: Leonard Peltier is a member\nof the Lakota native-american tribe and\nconsequently belongs to that ethnic\ngroup.<s>\nTEXT: INSIDE THE N.B.A. Correction :\nFebruary 9 , 2006 , Thursday A sports\narticle on the Spotlight page on Sunday\nabout Dick Bavetta , a longtime referee\nin the National Basketball Association,\nmisstated the number he was approaching\nto set the record for regular-season\ngames worked.\nRelations: [[’Dick Bavetta:Per’,\n’/people/person/profession’, ’National\nBasketball Association:Org’]]\nExplanation: Dick Bavetta is a person\nwho’s profession is that of a referee in\nNational Basketball Association.<s>\nTEXT: Now the United States Postal\nService may be displaying a similar\nrebellious streak : tomorrow at the huge\nSturgis motorcycle rally in the Black\nHills of South Dakota, the Postal\nService will issue a set of four stamps\nthat depict classic American bikes.\nRelations: [[’United States Postal\nService:Org’,\nData Inaccure FPs\n/ Total FPs\nInaccurate FNs\n/ Total FNs\nADE 108 / 209 136 / 417\nCoNLL 92 / 183 56 / 152\nTable 6: Number of inaccurate false positives (FNs) and\nfalse negatives (FNs) identified during automated eval-\nuation in GPT-3 labelled outputs under the in-context\nfew-shot prompting setting.\n’/business/company/industry’, ’Postal\nService:Org’]]\nExplanation: United States Postal\nService is a business company in the\nindustry of providing postal\nservices.<s>\nFigure 4: AUC plots for FPs and FNs.\nD Learning to Identify False False\nPositives and Negatives\nAs discussed in the main paper, one common prob-\nlem across datasets in generative RE is evaluation,\ngiven that LMs are flexible in how they might ex-\npress entities and relations. Prior work in RE has\ntended rely on standard metrics to quantify perfor-\nmance (precision, recall, micro-F1). These rely\non matching classified (or in our case, generated)\n15583\nlabels to reference labels to calculate the number\nof true positives (TPs), false positives (FPs), true\nnegatives (TNs), and false negatives (FNs).\nPrior to the introduction of LLMs for generative\nRE, Taillé et al. (2020) attempted to unify evalu-\nation and provide useful guidelines around issues\nassociated with prior methods and how different\nevaluation strategies rendered an accurate compari-\nson infeasible. They broadly recommended the use\nof a strict evaluation scheme where for a relation\ntriplet to be considered correct, the head and tail\nentity surface forms must be an exact match, as\nwell as their corresponding types (when available).\nWhile this provides a standardized framework for\ntraditional models where entities and and relations\nare hard classification labels, in a generative set-\nting we often find that LLMs, under varying levels\nof supervision, produce relation triplets (or pairs)\nthat do not correspond exactly to their reference\ncounterparts, but are nonetheless correct upon man-\nual review. Consider the following example from\nCoNLL in Figure 2\nText: On Friday, U.S. Ambassador Vernon\nA. Walters... fuselage.\nGold Reference: [(Vernon A. Walters,\n‘Live In’, U.S.)]\nGenerated Relations: [[Vernon A.\nWalters, ‘Works For’, U.S.]]\nIn this example, one can reasonably infer that\nVernon A. Walter is a U.S. Ambassador. There-\nfore, by definition a U.S. diplomat to another coun-\ntry cannot live inside the U.S., but such a person\nmust work for the U.S. (commonsense dictates that\na diplomat would work for a specific country).\nTo achieve a more accurate characterization of\nhow LLMs perform on generative RE tasks, we\nhired human annotators on Amazon Mechanical\nTurk5 to manually re-assess all ostensible FPs and\nFNs from each of our datasets. To control for qual-\nity and recruit annotators we ran pilot experiments\non 50 instances of pre-annotated data. 6 We re-\nquired AMT workers to have an overall approval\nrating of > 95% irrespective of geographic region.\nBased on these initial set of results we hired a\ntotal of 9 workers who reliably followed our in-\nstructions. Recruited workers were paid periodic\nbonuses (equivalent to one hour of pay) based on\n5We set the payrate to average at $15/hour using time\nestimates informed by pilot experiments.\n6These instances used in pilot experiments were annotated\nby a graduate student familiar with this research.\nthe quality of their annotations.\nTo identify potentially faulty “false positives”,\nwe provided annotators with the input text along\nwith the relation identified as a FP, and ask the fol-\nlowing question: “Can the given given relation be\nreasonably derived from the text?”. Similarly, to\nidentify erroneous “false negatives”, we provide\nannotators with the input text, the full set of gen-\nerated labels, the ostensible FN from the reference\nset, and ask: “Can the reference relation triplet\n(or pair) be inferred from the generated set of rela-\ntions?”. Each instance was annotated by three dif-\nferent AMT workers, and we considered a potential\nFP/FN to be inaccurate only when all annotators\nagree on a label.7 We provide specific examples of\nFPs and FNs in Tables 8 and 7. We summarize the\ndataset-specific findings in Table 6.\nIn light of these findings, we make a first effort\nin using simple, learned models to classify false-\npositives/negatives in generative RE. We experi-\nment with fine-tuned BERT (Devlin et al., 2019)\nclassifier to classify “false positives” and “false\nnegatives” as being accurate designations (or not).\nFor FPs, we concatenate the input with a gener-\nated relation pair/triplet (potential FP) and classify\nusing the [CLS]token -\n[CLS] Input Text [SEP] Potential FP\nSimilarly, for FNs we concatenate the input text\nwith a potential FN and the full set of generated\nlabels, and classify using the [CLS]token -\n[CLS] Input Text [SEP] Potential FN\n[SEP] Generated Labels\nWe analyze the effectiveness of this approach in\nFigure 4 using the AUC-ROC. We find that this ap-\nproach is most effectiveness in identifying potential\npotential false positives for CoNLL (AUC 0.88),\nwhile being least effective at identifying false neg-\natives for CoNLL (AUC 0.73). This suggests that\nlearning to identify erroneous “false positives” and\n“false negatives” may be a promising avenue to fa-\ncilitate accurate automated evaluation of generative\nLLMs for RE.\nD.1 List of out-of-domain relation-types\ngenerated by Flan during Few-Shot\nPrompting with CoNLL\nAssassinates, Purpose, isPartOf, Mother, Spouse,\nPresident, date, killed, Summer, Works_at,\n7We observe a high degree of agreement among the anno-\ntators with a Fliess kappa of 0.83\n15584\nDataset Input Detected FN\n(From the Gold Reference Set)\nFull Set of\nGenerated Relations\nCoNLL04\n•The three reactors, all at the Sa-\nvannah River Plant, in Aiken, S.C.,\nhave been shut down since last April\nundergoing changes to make them\nsafer.\n[Savannah River Plant, Located_In, S.C.] [Savannah River Plant, Located_In, Aiken]\n[Aiken, Located_In, S.C.]\n•They will also be cleaning the car\nOswald drove on the day Kennedy\nwas shot and the ambulance that\ntook Oswald to hospital after he was\nshot by Jack Ruby.\n[’Jack Ruby’, ’Kill’, ’Oswald’] [Jack Ruby, Shoot, Oswald]\nADE\n•We report the case of an 11-year-\nold female treated for mediastinal t-\ncell lymphoma who presented renal\nfailure following the second cycle of\nhigh-dose methotrexate (hdmtx).\n[’hdmtx’, ’renal failure’] [’methotrexate’, ’renal failure’]\n•Four days after the initial injec-\ntion of 3.6 mg of goserelin acetate,\nsevere dyspnea developed due to\nworsening pleuritis carcinomatosa,\nwhich was considered as a flare-up.\n[’goserelin acetate’, ’flare’] [’goserelin acetate’, ’dyspnea’]\nNYT\n•This time, the president chose Fa-\nther Leon to replace one of those\nclergymen, the Rev. Franklin Gra-\nham, who was filling in for his father,\nthe Rev. Billy Graham, who was ill\nin 2001.\n[’Franklin Graham’, ’people/people/children’, ’Billy Graham’] [’Billy Graham’, ’/people/person/children’, ’Franklin Graham’]\n•The moves by Citigroup and\nDeutsche Bank are only the latest\nillustrations of the difficulty of re-\ntaining jobs in New York City and\nrebuilding the business district in\nLower Manhattan.\n[’Citigroup’, ’business/place/founded’, ’New York City’]\n[’Lower Manhattan’, ’/location/location/contains’, ’New York City’][’New York City’, ’/location/location/contains’, ’Lower Manhattan’]\nTable 7: Sample of inaccurate false negatives identified by human annotators for each dataset. Examples from\nCoNLL and ADE were generated from GPT-3, while those in NYT were generated by Flan.\n15585\nDataset Input Detected FPs\n(Generated)\nFull Set of\nTrue Relations\nCoNLL04\n• Illinois Gov. James Thompson signed legis-\nlation last month scrapping Chicago ’s central\nschool board next July , to be replaced by parent-\nrun councils empowered to set budgets and hire\nand fire principals .\n[James Thompson, Work_For , Illinois],\n[Chicago, Located_In , Illinois] [[James Thompson, Live_In , Illinois]]\n• On Friday, U.S. Ambassador Vernon A. Wal-\nters displayed photographs of one Libyan jet\nshowing shapes resembling missile pods on its\nwings and fuselage.\n[Vernon A. Walters, Work_For , U.S.] [[Vernon A. Walters, Live_In , U.S.]\nADE\n• Background: how to best treat psychotic\npatients who have had past clozapine-induced\nagranulocytosis or granulocytopenia remains a\nproblem.\n[clozapine, agranulocytosis] [[clozapine, granulocytopenia]]\n• Acute renal failure is a rare complication fol-\nlowing the administration of intravenous im-\nmunoglobulin (ivig).\n[ivig, acute renal failure] [[immunoglobulin, acute renal failure]]\nNYT\n• Many people in Turkey have lost hopes in\njoining Europe and they are looking for other\nhorizons , ” said Onur Oymen , an opposition\npolitician whose party is staunchly secular.\n[Turkey, location/administrative_division/country , Europe] [[Europe, LOC_CONTAINS , Turkey]]\n• To make his case , Dr. von Hagens invited two\njournalists to Dalian for a tour of his facility ,\nwhich he said was the first center in China to\npreserve bodies.\n[Dalian, location/administrative_division/country , China] [[China, location/location/contains , Dalian]]\nTable 8: Sample of inaccurate false positives (FPs) identified by human annotators for each dataset. Examples from\nCoNLL and ADE were generated from GPT-3, while those in NYT were generated by Flan.\n15586\nSentenced_To_Death, Source, Statue, Sec-\nretary, Born, Year, Born_in, Day, Place,\nNumber_Of_Passengers, Callers, Governor,\nHometown, has_a_leader, is_a_member_of,\nNickname, is_part_of, Office, Rank, Works_For,\nWorkedFor, Worked_For, Killed_By, Piano,\nTerm, Sentence, Person, Movie, Said, Brother,\nDate_of_Death, Type, Death_Penalty, assas-\nsination_date, Worked_for, capital, Killed,\nKilling, Occupation, Crime, Years_in_use, Org,\nEducation, Order_to_ignore, Assassination,\nLocation, Officer, language, former_name, To-\ntal_acres, Age, Cause, Chairman, worked_for, Son,\nStaff_name, departure, Capsule_name, Operator,\nSpin-off, Owner, located_in, theory, Birth_Place,\non_duty_with, City, Top_Leader, Director,\nstructure, Known_as, former_chief_executive,\nWorks_for, Native_name, Percentage, department,\nComponent, reminds_someone_of, Sex, Bank,\nAppointed_By, Activity, Title, has_a_river_name,\nSize, Office_Space, Part, Kingdom, Attached_to,\nDeath_Place, Years_on_the_Supreme_Court,\nAssassin, location, Newspaper, City„ island,\nEmployee, Friend, Native_Son, Speaker, Visi-\ntor, Date, Aircraft, channel, Sale_to, Creditor,\nClient, Nationality, Flight_Status, assassinater,\non_behalf_of, Shot_By.\n15587\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations (page 9)\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Introduction (1)\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\n3,4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n15588\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nAppendix\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\n3.2\n□\u0017 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot required.\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nAppendix\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n15589"
}