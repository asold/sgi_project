{
  "title": "An N400 identification method based on the combination of Soft-DTW and transformer",
  "url": "https://openalex.org/W4321094933",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2097524638",
      "name": "Yan Ma",
      "affiliations": [
        "Chongqing Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A4321095785",
      "name": "Yiou Tang",
      "affiliations": [
        "Chongqing Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2104614082",
      "name": "Yang Zeng",
      "affiliations": [
        "Chongqing Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2030191682",
      "name": "Tao Ding",
      "affiliations": [
        "Chongqing Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2129527211",
      "name": "Yifu Liu",
      "affiliations": [
        "Chongqing Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2097524638",
      "name": "Yan Ma",
      "affiliations": [
        "Chongqing Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A4321095785",
      "name": "Yiou Tang",
      "affiliations": [
        "Chongqing Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2104614082",
      "name": "Yang Zeng",
      "affiliations": [
        "Chongqing Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2030191682",
      "name": "Tao Ding",
      "affiliations": [
        "Chongqing Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2129527211",
      "name": "Yifu Liu",
      "affiliations": [
        "Chongqing Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3206962130",
    "https://openalex.org/W6773793617",
    "https://openalex.org/W3176495084",
    "https://openalex.org/W4301410151",
    "https://openalex.org/W2124958607",
    "https://openalex.org/W2908768147",
    "https://openalex.org/W3093672414",
    "https://openalex.org/W4312984907",
    "https://openalex.org/W2141138276",
    "https://openalex.org/W2000387713",
    "https://openalex.org/W1978415413",
    "https://openalex.org/W4249982495",
    "https://openalex.org/W4306955484",
    "https://openalex.org/W2898757254",
    "https://openalex.org/W6910519488",
    "https://openalex.org/W3184030040",
    "https://openalex.org/W2128909182",
    "https://openalex.org/W4312850659",
    "https://openalex.org/W2084616221",
    "https://openalex.org/W2092435433",
    "https://openalex.org/W3121356124",
    "https://openalex.org/W2548241290",
    "https://openalex.org/W4226083839",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2972861020",
    "https://openalex.org/W2977268464",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2466647518",
    "https://openalex.org/W3003866104"
  ],
  "abstract": "As a time-domain EEG feature reflecting the semantic processing of the human brain, the N400 event-related potentials still lack a mature classification and recognition scheme. To address the problems of low signal-to-noise ratio and difficult feature extraction of N400 data, we propose a Soft-DTW-based single-subject short-distance event-related potential averaging method by using the advantages of differentiable and efficient Soft-DTW loss function, and perform partial Soft-DTW averaging based on DTW distance within a single-subject range, and propose a Transformer-based ERP recognition classification model, which captures contextual information by introducing location coding and a self-attentive mechanism, combined with a Softmax classifier to classify N400 data. The experimental results show that the highest recognition accuracy of 0.8992 is achieved on the ERP-CORE N400 public dataset, verifying the effectiveness of the model and the averaging method.",
  "full_text": "fncom-17-1120566 February 11, 2023 Time: 14:31 # 1\nTYPE Original Research\nPUBLISHED 16 February 2023\nDOI 10.3389/fncom.2023.1120566\nOPEN ACCESS\nEDITED BY\nMing Li,\nThe Hong Kong Polytechnic University,\nHong Kong SAR, China\nREVIEWED BY\nGenghui Li,\nSouthern University of Science\nand Technology, China\nYixin Zhao,\nBeijing Institute of Petrochemical Technology,\nChina\n*CORRESPONDENCE\nYan Ma\n20130939@cqnu.edu.cn\nRECEIVED 10 December 2022\nACCEPTED 02 February 2023\nPUBLISHED 16 February 2023\nCITATION\nMa Y, Tang Y, Zeng Y, Ding T and Liu Y (2023)\nAn N400 identiﬁcation method based on\nthe combination of Soft-DTW\nand transformer.\nFront. Comput. Neurosci.17:1120566.\ndoi: 10.3389/fncom.2023.1120566\nCOPYRIGHT\n© 2023 Ma, Tang, Zeng, Ding and Liu. This is an\nopen-access article distributed under the terms\nof the Creative Commons Attribution License\n(CC BY). The use, distribution or reproduction\nin other forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted which\ndoes not comply with these terms.\nAn N400 identiﬁcation method\nbased on the combination of\nSoft-DTW and transformer\nYan Ma1,2*, Yiou Tang1, Yang Zeng1, Tao Ding1 and Yifu Liu1\n1College of Computer and Information Science, Chongqing Normal University, Chongqing, China,\n2Wisdom Education Research Institute, Chongqing Normal University, Chongqing, China\nAs a time-domain EEG feature reﬂecting the semantic processing of the human\nbrain, the N400 event-related potentials still lack a mature classiﬁcation and\nrecognition scheme. To address the problems of low signal-to-noise ratio and\ndifﬁcult feature extraction of N400 data, we propose a Soft-DTW-based single-\nsubject short-distance event-related potential averaging method by using the\nadvantages of differentiable and efﬁcient Soft-DTW loss function, and perform\npartial Soft-DTW averaging based on DTW distance within a single-subject range,\nand propose a Transformer-based ERP recognition classiﬁcation model, which\ncaptures contextual information by introducing location coding and a self-\nattentive mechanism, combined with a Softmax classiﬁer to classify N400 data.\nThe experimental results show that the highest recognition accuracy of 0.8992\nis achieved on the ERP-CORE N400 public dataset, verifying the effectiveness of\nthe model and the averaging method.\nKEYWORDS\nevent-related potential, dynamic time warping (DTW), self-attention, N400, transformer\nIntroduction\nCognitive linguistics, derived from cognitive science or cognitive psychology, is a\nsubdiscipline of linguistics. The discipline focuses on describing and explaining the systemic\nnature, structure, and function of language (Turakhonova, 2022). Electroencephalography\n(EEG) is the sum of postsynaptic potentials of neurons in the human cerebral cortex,\nwhich is acquired through the brain-computer interface (BCI) at the surface of the scalp\n(Nicolas-Alonso and Gomez-Gil, 2012). In recent years, there has been a gradual increase\nin the number of language cognitive indicators built around EEG features, thanks to the\nhigh temporal resolution of EEG signals, the richness of the implicit information, and the\nnon-invasive acquisition process (Sun et al., 2020). Among them, the time-domain feature\nevent-related potentials reﬂecting the impact of discrete events on the cognitive processing\nof the human brain play an important role (Zhang et al., 2019).\nN400 is a classical endogenous component of event-related potentials that are\nassociated with the cognitive processing of language in humans (Lau et al., 2008).\nN400 was ﬁrst identiﬁed by Kutas and Hillyard (1980). The duo modiﬁed the\nFrontiers in Computational Neuroscience 01 frontiersin.org\nfncom-17-1120566 February 11, 2023 Time: 14:31 # 2\nMa et al. 10.3389/fncom.2023.1120566\nOddball paradigm for linguistic material by attempting to induce\nthe p3b event-related potential through the semantic inconsistency\nof words at the end of English phrases but unexpectedly found\nanother large Unrelated potential eﬀect that manifested as a\nrelatively Unrelated peak around 400 ms after the subject’s brain\nreceived the relevant stimulus, hence the name N400 (Kutas and\nHillyard, 1980). The N400 was shown to reﬂect the processing\nof text semantics and text context in the human brain. Several\ndimensions such as signal amplitude, latency, and scalp distribution\nreﬂect the characteristics and distinctions of the human brain’s\nsemantic perception of things (Liang et al., 2018).\nToday, the N400 is considered a powerful tool for studying\nhow human language is understood and processed. The linguistic\nmedia involved include, but are not limited to, text, speech,\nand non-textual language, sometimes expressed in the form of\ngraphics, etc. The diﬀerent stages of semantic comprehension by\nthe human brain can be tracked by the N400 with millisecond\ntemporal resolution (Petten, 1993). Therefore, fast and eﬀective\ndetection and classiﬁcation of N400 event-related potentials have\nbecome a necessary and urgent need. Some recent work has used\nneural language models (NLMs) to investigate the relationship\nbetween N400 event-related potential amplitude, onset time, and\ncognitive processes in human language. Schwartz and Mitchell\n(2019) proposed that RNN can explain the magnitude of N400\nevent-related potential amplitudes evoked during subjects’ reading;\nAurnhammer and Frank (2019) used GRU, LSTM, and other\nmodels to predict N400 amplitudes; Merkx and Frank (2020)\nsuggested that T-LMs models have higher prediction accuracy for\nN400 signal amplitudes; the research direction of the above works\nmainly focused on exploring how human language comprehension\nsystems work through prediction of N400 data amplitudes.\nSolutions for the detection and classiﬁcation of N400 event-related\npotential data are relatively scarce. Most event-related potential\ndetection and classiﬁcation algorithms to date have been based\non experiments with the P300 data set, and there is still a lack of\nmore mature solutions for the task of N400 event-related potential\ndetection.\nTo address this problem, this study ﬁrst brieﬂy summarizes the\nexisting solutions for identifying and classifying other event-related\npotential components. Researchers in the ﬁeld of brain-computer\ninterfaces have tried to improve the recognition of event-related\npotentials from several perspectives, including feature extraction\nmethods and models. Gui et al. (2015) evaluated two sequence\nsimilarity measures, Euclidean distance as well as DTW, and\nobtained optimal classiﬁcation results on the Oz electrode channel,\nwhere Euclidean distance outperformed DTW distance. In the\nstudy of Al-rubaye et al. (2019) the DTW based on connectivity\nfeatures was used for the classiﬁcation of single-trial ERP cycles\nmaking a combination of support vector machine and K-NN\nclassiﬁer using the DTW metric and obtaining more accurate\nclassiﬁcation results. Davis (2016) used a DTW-based center of\ngravity averaging method for EEG sequences and proposes several\nfurther modiﬁcations to the initial sequence selection process to\nimprove the eﬀectiveness of the method in EEG analysis. Simbolon\net al. (2015) used support vector machines to classify P300 event-\nrelated potentials to distinguish whether subjects were lying or\nnot and obtained a classiﬁcation accuracy of 70.83%; Abou-Abbas\net al. (2021) used K-Nearest Neighbor, support vector machine, and\ndecision tree to identify N290 and P400 event-related potentials,\nand ﬁnally obtained 78.09% recognition accuracy by decision tree.\nTraditional machine learning classiﬁcation methods are highly\ndependent on manual feature extraction from raw data, which is\na cumbersome workﬂow, so researchers are gradually introducing\ndeep learning methods into EEG analysis. Chou et al. (2018)\ncombined the Inception v1 network and EEGNet network to\ndesign an improved CNN-IE network and obtained a correct\nrate higher than 95% on the P300 event-related potential data\nobtained by superposition averaging. Yu et al. (2021) used a one-\ndimensional time-space convolutional kernel for feature extraction\nof the P300 standard dataset and combined it with a support\nvector machine classiﬁer for classiﬁcation to achieve the eﬀective\nclassiﬁcation of few-trial visual event-related potentials. Maddula\net al. (2017) proposed a model architecture based on recurrent\nconvolutional neural networks to achieve eﬃcient detection of P300\nevent-related potentials. Wang et al. (2022) introduced an attention\nmechanism into a long and short-term memory recurrent neural\nnetwork to fully capture the global time-domain information of\nP300 sequences and achieved a classiﬁcation accuracy of 91.9%.\nDuru and Duru (2018) used topographic maps of P300 and P400 for\ntheir respective trigger periods as recognition features and achieved\na classiﬁcation accuracy of 73% for the training set.\nThe N400 signal is a non-strictly phase-locked event-related\npotential, and multi-trial averaging can easily lead to time-\ndomain feature falsiﬁcation, resulting in diﬃculty in feature\nextraction and model training underﬁtting (Kutas and Federmeier,\n2011). Diﬀerent event-related potential types, datasets as well as\ndiﬀerent acquisition methods can lead to large diﬀerences in the\nmultidimensional characteristics of event-related potentials. The\nexisting methods are not generalizable. Therefore, this study hopes\nto design a more eﬀective classiﬁcation scheme for N400 event-\nrelated potential identiﬁcation based on the existing research results\nof event-related potential classiﬁcation and identiﬁcation, and try\nto use the N400 event-related potential dataset provided by the\nERP CORE database as an entry point. This study proposes a Soft-\nDTW-based event-related potential averaging scheme for feature\nextraction of N400 data and a simpliﬁed Transformer neural\nnetwork model. The main contributions of this study are as follows.\n1) The Soft-DTW diﬀerentiable loss function is introduced\ninto the feature extraction process of N400 data, which\ntakes into account the time series translation invariance\nwhile accelerating the backpropagation speed by dynamic\nprogramming and improving the eﬃciency of data\nfeature extraction.\n2) A Soft-DTW-based single-subject short-range event-related\npotential averaging method is proposed to partially Soft-DTW\naverage the N400 similar data within a single-subject range to\nachieve the maximum retention of the location information\nand curve features of the N400 time series under the condition\nof multi-trial averaging.\n3) Based on the data features of N400 event-related potentials,\na TransFormer-based event-related potential classiﬁcation\nmodel is proposed to fully capture the temporal characteristics\nand contextual relationships of N400 data by introducing\nlocation coding and self-attentiveness mechanisms, achieving\nthe highest recognition accuracy of 89.92% on the ERP-CORE\nN400 public dataset.\nFrontiers in Computational Neuroscience 02 frontiersin.org\nfncom-17-1120566 February 11, 2023 Time: 14:31 # 3\nMa et al. 10.3389/fncom.2023.1120566\nMaterials and methods\nSoft-DTW-based averaging method for\nN400 event-related potentials\nThe N400 is a non-strictly phase-locked event-related potential\nwith certain timing deviations in the time domain dimension,\nand the application of the traditional arithmetic superposition\naveraging method to it easily leads to the degradation of the\nsignal-to-noise ratio of the time series, which is manifested by\nthe disappearance of local features or the introduction of dense\nnoise. In this study, a soft DTW time-series diﬀerentiable loss\nfunction was introduced, and a soft DTW-based superposition\naveraging method was designed based on this method to perform\ngravitational superposition averaging on the original N400 data\nand to extract event-related potential averaging sequences with\nsmoother waveform features and more uniform start times from\n35 electrode dimensions, respectively. The method was named the\nsingle-subject short-distance event-related potential superposition\naveraging method.\nDynamic time warping\nThe Dynamic Time Warping (DTW) algorithm is a numerical\ndistance measure for time series. The traditional Euclidean\ndistance algorithm relies on point-to-point numerical comparisons\nand cannot match time series of diﬀerent lengths. The DTW\nalgorithm allows one-to-many mapping between series points and\ncombines the idea of dynamic programming to achieve an eﬃcient\ncomparison of time series similarity, and nowadays it has become\none of the most commonly used metrics to quantify the similarity\nbetween series (Cuturi et al., 2007).\nAssume that the multidimensional time series of variable\nlength take values in the range of ∈Rp. Therefore, the\ntime series can be represented as matrices with p number of\nrows and diﬀerent number of columns. For two p-dimensional\ntime series x =(x1,...,xn)∈R\np∗n\nand y =(y1,...,ym)∈R\np∗m\nwith lengths n and m, respectively, deﬁne the cost matrix\n1\n(\nx,y\n)\n:=[δ(xi,yj)]ij ∈R\nn∗m\n, where δ is the diﬀerentiable cost\nfunction δ:Rp ×Rp →R+. The Euclidean distance is chosen as\nδin most cases. Let ri,j be the cost sum of the cost matrix from the\nupper left corner to any point [i,j], and the dynamic programming\nequation of the DTW algorithm can be expressed as:\nrDTW\ni,j =δi,j +min{ri,j−1,ri−1,j,ri−1,j−1}.\nThe DTW algorithm eﬀectively takes into account the\ntranslational invariance of the time series in the time domain\ndimension and is therefore often used in classiﬁers such as K-NN,\nSVM, etc. Petitjean et al. (2011) ﬁrst proposed to output the entire\ntime series as a ﬁtted loss to achieve superposition averaging of\nthe time series. However, at the computational level, the DTW\nalgorithm is not trivial and unstable when applied to back-\npropagation loss functions, a factor that hinders the application of\nthe DTW superposition averaging algorithm.\nSoft-DTW loss function\nTo solve the problem of the non-minimizability of the DTW\nalgorithm, Cuturi and Blondel (2017) used Soft minimum instead\nof DTW minimum and designed the minimizable Soft-DTW loss\nfunction. Under the Soft-DTW method, the sum of costs A can be\nexpressed as:\nrSoftDTW\ni,j =δi,j +min γ {\nri,j−1,ri−1,j,ri−1,j−1\n}\n.\nmin γa1,...,an =\n{\nmini≤n ai, γ =0,\n−γlog ∑n\ni=1 e−ai/γ, γ >0.\nWhere γ is the smoothing parameter. When γ =0 the\nalgorithm is equivalent to the conventional DTW. Deﬁne the\nset An,m ⊂{0,1}n×m, where each element A is an alignment\nmatrix for the sequence x, y. For a particular alignment matrix\nA =[ai,j],A ∈An,m, only the point (i,j) on the path from (1,1) to\n(n,m) has its ai,j =1, and all other points have ai,j value 0. The\npath follows three directions of extension downward, rightward,\nand diagonal right until it reaches the lower right corner of the\nmatrix. The cost sum under this path matrix can be expressed as⟨\nA,1(x,y)\n⟩\n. Thus, the soft-dtw loss function is deﬁned as:\ndtwγ(x,y)=min γ {⟨\nA,1(x,y)\n⟩\n,A ∈An,m\n}\n=−γlog(∑\nA∈An,m e−⟨A1(x,y)/γ⟩).\nSoft-DTW back propagation\nThe ﬁnal goal of Soft-DTW is to obtain the numerical distance\nbetween time series x and y. If y is regarded as the target series, all\nthat needs to be computed for back propagation is the gradient to\nthe time series x. The gradient is expressed through the chain rule\nas:\n∇xdtwγ(x,y)=(∂1(x,y)\n∂x )T (∂dtwγ(x,y)\n∂1(x,y) )\n=(∂1(x,y)\n∂x )T ( ∂rn,m\n∂1(x,y)).\nApplying the same chain rule to the second term yields:\n∂rn,m\n∂δi,j\n=∂rn,m\n∂ri,j\n∂ri,j\n∂δi,j\n=∂rn,m\n∂ri,j\n·1 =∂rn,m\n∂ri,j\n.\nDeﬁne the elements ei,j =∂rn,m\n∂ri,j such that E =[ei,j],E ∈Rn×m\nis a matrix consisting of ei,j. The above equation is then re-\nexpressed as:\n∇xdtwγ(x,y)=(∂1(x,y)\n∂x )T E.\nIn order to optimize the time complexity of back propagation,\ndynamic programming is introduced to calculate the matrix E.\nfor matrix terms ei,j =∂rn,m\n∂ri,j , ri,j are arithmetically associated with\nthree terms ei,j+1, ei+1,j+1, ei+1,j, respectively, and according to the\nchain rule we get:\n∂rn,m\n∂ri,j = ∂rn,m\n∂ri+1,j\n∂ri+1,j\n∂ri,j +∂rn,m\n∂ri,j+1\n∂ri,j+1\n∂ri,j + ∂rn,m\n∂ri+1,j+1\n∂ri+1,j+1\n∂ri,j .\nThus, for the element ei,j in the matrix E, the reverse derivation\nis performed using the following formula:\na =e\n1\nγ (ri+1,j−ri,j−δi+1,j),\nb =e\n1\nγ (ri,j+1−ri,j−δi,j+1),\nc =e\n1\nγ (ri+1,j+1−ri,j−δi+1,j+1),\nei,j =ei+1,j ·a +ei,j+1 ·b +ei+1,j+1 ·c.\nFrontiers in Computational Neuroscience 03 frontiersin.org\nfncom-17-1120566 February 11, 2023 Time: 14:31 # 4\nMa et al. 10.3389/fncom.2023.1120566\nSoft-DTW-based time series averaging\nCuturi and Blondel (2017) proposed that Soft-DTW can be\neﬀectively used in the Fréchet averaging algorithm. Given N time\nseries y1,......,yn, represented as N matrices M1,......,Mn with P\nnumber of rows and diﬀerent number of columns (corresponding\nto imperfect matching of time series lengths), deﬁne a single\ncenter of gravity time series X and a set of normalized weights\nλ1,......,λn ∈R+, such that ∑N\ni=1 λi =1 . Assuming that X\npossesses a ﬁxed length n, the objective of time series averaging with\nSoft-DTW is to solve the following optimization problem:\nmin\nx∈Rp×n\nN∑\ni=1\nλi\nmi\ndtwγ(x,yi).\nData preprocessing\nThe experimental dataset for this study was obtained from\nthe N400 subset of the ERP CORE, an open EEG event-\nrelated potentials dataset provided by the University of California\n(Kappenman et al., 2021). The dataset was collected from 40\nparticipants (25 females, 15 males; mean age 21.5 years, standard\ndeviation 2.87 years, range 18-30 years; 38 right-handed) at the\nUniversity of California, Davis, and all subjects were screened to\nensure the normal or corrected-to-normal vision and normal color\nvision, with no history of neurological damage or disease. Thirty\nscalp electrodes were recorded using the Biosemi-ActiveTwo EEG\nsignal recording system, with electrodes arranged according to the\ninternational 10-20 system, as shown in Figure 1.\nThe experimental paradigm for the N400 sub-dataset uses a\nword pair judgment task adapted from that proposed by Holcomb\nand Kutas. In each trial, a red prime word was followed by a\ngreen target word. Participants answered whether the target word\nwas semantically related or unrelated to the prime word, and\nlabeled single events \"Related\" and \"Unrelated\" accordingly. In the\nprocess of data screening, subjects with artifacts higher than 25%\nwere screened out. Subjects whose data will be excluded if their\naverage accuracy on semantic judgments is less than 75%, or if\ntheir accuracy in any single experimental condition is less than 50%.\nAn example of the word pair judgment task information layout is\nshown in Figure 2.\nTo eliminate the LCD delay, the stimulus event code was ﬁrst\nshifted back to 26 ms on the time scale. the signal sampling rate\nwas downsampled from 1,024 to 256 Hz to increase the data\nprocessing speed, and the data was re-referenced to the average\nof P9 and P10. To remove the DC oﬀset present in the raw data,\nhigh-pass ﬁltering was applied to the signal. In preparation for\nartifact correction, EEG data segments containing large myoelectric\nartifacts, extreme voltage shifts, and interruptions of more than 2\nseconds were removed by visual judgment using the open-source\ntool ERPLAB. Subsequent independent component analysis of\nthe data using the binICA method was used to remove artifacts\nand to remove components that were signiﬁcantly correlated\nwith horizontal eye movements (assessed by visual inspection\nof the scalp distribution of waveforms and components). The\ncorrected bipolar HEOG channels (HEOG_left - HEOG_right)\nand VEOG channels (VEOG_lower - FP2) are calculated from\nthe ICA correction data and the uncorrected bipolar HEOG and\nFIGURE 1\nInternational 10–20 system.\nFrontiers in Computational Neuroscience 04 frontiersin.org\nfncom-17-1120566 February 11, 2023 Time: 14:31 # 5\nMa et al. 10.3389/fncom.2023.1120566\nVEOG channels are retained for subsequent correction. Many\nstudies have existed to fuse data from EEG with EEG as a way\nto improve the accuracy of interaction intent recognition. For\nexample, Park et al. (2022) developed an online BCI appliance\ncontrol system based on steady-state visual evoked potentials\n(SSVEP) and electrooculography (EOG); Karimi and Shamsollahi\n(2022) proposed a ﬁducial ensemble (Fpz-Cz, Pz-Oz, and EOG)\nbased on latent structural inﬂuence models (LSIMs) that exhibited\nsome degree of performance improvement. Therefore, we decided\nto use both EOG data for classiﬁcation. The data were windowed\nbased on the event list and baseline corrections were performed\nfor each event point. Channels with excessive noise levels identiﬁed\nby visual inspection of the data were interpolated using EEGLAB’s\nspherical interpolation algorithm. The ﬁnal EEG time-dependent\npotential data for 35 channels were retained [FP1, F3, F7, FC3,\nC3, C5, P3, P7, PO7, PO3, O1, Oz, Pz, CPz, FP2, Fz, F4, F8,\nFC4, FCz, Cz, C4, C6, P4, P8, PO8, PO4, O2, HEOG_left, HEOG_\nright, VEOG_lower, (corr) HEOG, (corr) VEOG, (uncorr) HEOG,\n(uncorr) VEOG] totaled 4,497 entries, including 2,237 Related tags\nand 2,260 Unrelated tags. Each data is represented as a vector with\na time series length of 256 and dimension of 35 (256 ×35). In\nthis paper, experiments will be conducted based on the N400 sub-\ndataset of ERP CORE to verify the eﬀectiveness of the method and\nmodel structure in identifying and classifying N400 data through\nthe accuracy of the model in classifying diﬀerently labeled data.\nSingle-subject short-range event-related\npotential averaging method\nTo extract features from the pre-processed N400 event-related\npotentials, this study proposes a feature extraction process called\nthe \"single-subject event-related potential averaging method\" based\non Soft-DTW (The following will be referred to as the SSE\naveraging method).\nThe data processing process was as follows.\n1) Read the event-related potentials of 40 subjects separately, and\nclassify the data into P1,......,P40 and U1,......,U40 according\nto \"Related\" and \"Unrelated\" labels within a single subject.\n2) Taking P1 as an example, within this data grouping the\nDTW distances are calculated for every two units of data\nand saved as an array, named the distance matrix for this\ndata set. The distance between two independent N400 data\nis calculated as follows: for two independent event-related\npotentials (256 ×35), each column is treated as a time series\nof length 256, and the standard DTW distance is calculated in\nEEG/EOG channels column by column and summed up, and\nsaved in an equilateral array with side lengths of the divided\ndata set as subscripts (the DTW distances of data 1 and data 3\nare saved in the array subscripts [0][2] and [2][0] positions).\n3) All data are traversed sequentially, and for each independent\nevent-related potential signal d1, the N data with the smallest\nDTW distance within that data grouping (including d1 itself)\nis found by the distance matrix, and the N data are named as\na similar sample group of d1 .\n4) Soft-DTW superimposed averaging is performed column by\ncolumn for all data within similar sample groups in electrode\nchannels, and the averaged results are output as a new\nsequence.\nThe data processing ﬂow is shown in Figure 3:\nIn Step 1, we restrict the data samples for each Soft-DTW\nsuperimposed averaging to be from the same subject and the same\nlabel category to retain enough individual subject characteristics\nand reduce the loss of useful temporal information. For example,\nsubject 1 has 130 independent data, of which 70 are \"Relate\" labels\nFIGURE 2\nThe example of the word pair judgment task information layout.\nFIGURE 3\nSimple schematic diagram of SSE averaging method.\nFrontiers in Computational Neuroscience 05 frontiersin.org\nfncom-17-1120566 February 11, 2023 Time: 14:31 # 6\nMa et al. 10.3389/fncom.2023.1120566\nand 60 are \"Unrelate\" labels. \"Relate\" data, if the number of similar\nsample groups N is set to 25, then within the 70 \"Relate\" labeled\ndata, 24 of the closest data will be found to form a similar sample\ngroup with a total of 25 data based on the DTW distance, and the\ndata within the similar sample group will be Soft -DTW averaging.\nIn this way, similar data are averaged against each other to produce\na new time series. As all data are averaged against their nearest\ndata, the new average data are still somewhat data independent,\navoiding excessive homogenization. So the feature processing\nprocess is named \"single-subject short-range event-related potential\nsuperimposed averaging method \". The number N of similar sample\ngroups for each independent sample should not be too large to\nFIGURE 4\nExample of the averaging results of N400 event-related potentials by Soft-DTW under FP1 channel (Related label).\nFIGURE 5\nExample of the averaging results of N400 event-related potentials by Soft-DTW under FP1 channel (Unrelated label).\nFrontiers in Computational Neuroscience 06 frontiersin.org\nfncom-17-1120566 February 11, 2023 Time: 14:31 # 7\nMa et al. 10.3389/fncom.2023.1120566\nmitigate subsequent model training overﬁtting. Each independent\nevent-related potential signal will only be superimposed and\naveraged within the same scalp electrode, without any interference\nbetween data channels of diﬀerent electrodes.\nFigures 4, 5 show the results of the superimposed averaging of\nthe random independent data on the FP1 electrode channel (the\nelectrode position on the left side of the prefrontal lobe as deﬁned\nin the international 10–20 system) for output subject 1 under the\ncondition that the number of similar sample groups is 20. Where\ndata FP1_Related/Unrelated_2 to FP1_Related/Unrelated_20 are\nthe 19 data closest to the FP1_Related/Unrelated_1 data DTW of\nall signals acquired by the subject. The left subplot of the ﬁgure\nshows a general overview of the 20 data within this similar sample\ngroup, and the right subplot shows the Soft-DTW superimposed\naverage results. In fact, the time series obtained using this averaging\nmethod are not consistent with the results of traditional event-\nrelated potential superposition averaging, and the standard event-\nrelated potential patterns are not usually observed in the time\nseries waveforms extracted by the SSE averaging method. The main\nreason is that the method is not a global averaging but a limited\nSoft-DTW averaging over a similar sample set at the shortest\nDTW distance. This method is designed with the goal of serving\nthe classiﬁcation of N400 event-related potentials based on neural\nnetwork models.\nFigure 6 shows the sequence results obtained by SSE\naveraging and ordinary arithmetic averaging for the above random\nindependent data with the number of similar sample groups of 5,\n10, 15, 20, 25, and 30, respectively. The start time of the N400\ndata obtained based on the SSE averaging method advanced with\nthe increase of the number of similar sample groups and the more\nsimilar samples were used for the superimposed averaging, the\nFIGURE 6\nComparison of the SSE-averaged and arithmetic-averaged results of Related label N400 data under different similar sample group numbers.\nFIGURE 7\nErp-Transformer network structure.\nFrontiers in Computational Neuroscience 07 frontiersin.org\nfncom-17-1120566 February 11, 2023 Time: 14:31 # 8\nMa et al. 10.3389/fncom.2023.1120566\ncloser the averaging results were to the standard curve pattern\nof the N400 event-related potentials. The amplitude of the signal\nobtained by ordinary arithmetic averaging tends to ﬂatten out with\nthe increase in the number of similar sample groups, and the\namplitude range is the same as that of the N400 signal data obtained\nby Unrelated labeling, which is diﬃcult to distinguish eﬀectively.\nThe SSE averaging method can better capture the start-up utility\nof the N400 event-related potentials after the 400ms period, and\nthe time series output extracted by this method is more suitable as\na valid feature for the N400 event-related potential identiﬁcation\ntask.\nTransformer neural network\nAs an emerging deep learning model, Transformer has been\nwidely used and achieved outstanding results in various ﬁelds\nsuch as natural language processing, speech processing, and\ncomputer vision. The Transformer was initially applied as a\nsequence-to-sequence model for machine translation tasks, and\nlater many studies have shown that pre-trained models based on\nthe Transformer can perform better on various tasks, and today\nTransformer has become one of the most commonly used model\narchitectures in the ﬁeld of natural language processing (Lin et al.,\n2022).\nThe traditional Transformer model consists of two main parts,\nthe encoder and the decoder, each of which is a module with the\nsame structure. The encoder consists of a self-attentive module\nand a feedforward neural network. To reduce the information\nloss in the deep network and to solve the problem of gradient\ndisappearance and gradient explosion, residual links are introduced\nwithin the encoder. In contrast to the encoder, the decoder\nintroduces an additional self-attentive module between the self-\nattentive block and the feedforward neural network, fuses the\noutput of its self-attentive module with the output of the encoder\nand calculates the attention score, which is subsequently fed into\nthe feedforward neural network. Finally, the decoder-processed\nfeatures are fed into the fully connected layer to achieve the output\nof the sequence. The reason why Transformer outperforms RNN\nin predicting N400 amplitudes has been discussed by Michaelov\net al. (2021) who concluded that the prediction results of N400\nevent-related potentials are inﬂuenced by the context, which has\ngiven some inspiration to this study. This study attempts to use\na similar Transformer structure to capture the hidden contextual\ninformation inside the processed N400 event-related potential data\nand hypothesizes that this model can achieve more eﬃcient and\naccurate identiﬁcation of N400 data.\nIn this study, a simple Transformer neural network is designed\nfor the N400 event-related potential features obtained by SSE\naveraging method, which is structurally streamlined and optimized\nfor the N400 data features to achieve the eﬀective classiﬁcation of\nN400 signals within a limited batch.\nImproved network architecture\nIn this study, a relatively simpliﬁed network architecture\nis proposed based on Transformer according to the data\ncharacteristics of N400 event-related potentials, and the network\nstructure is shown inFigure 7. For ease of diﬀerentiation, the model\nstructure is named Erp-Transformer in the following.\nThe network consists of three main modules, namely the\nposition encoder module, the encoder group module, and the\nfully connected module. The internal structure of each module is\ndescribed as follows.\nPositional encoding module: N400 event-related potentials\nbelong to typical time series features, and the purely self-attentive\nmechanism cannot fail to capture the position information\ncontained in N400, including absolute position information,\nrelative position information, and distance between diﬀerent data\npoints, so the position encoding module is introduced. The position\nencoding is calculated as follows: for a p-dimensional time series\nx = (x1,...,xm)∈R\np∗n\nof length n, deﬁne t as a 35-dimensional\nvector of event-related potentials at a certain moment, i as the\nordinal number of the vector in the time series, and − →dt ∈Rp\ndenotes the position encoding vector corresponding to position t.\nDeﬁne the generating function f of the position vector − →dt :\n− →dt (i) =f (t)i :=\n{\nsin(ωk ·t),if i =2k,\ncos(ωk ·t),if i =2k +1.\nThe input sequence x enters the position encoding module to\nﬁrst obtain the position encoding sequence D by the above function\ncalculation, and then the matrix addition operation is performed\nbetween x and D. The operation result is input to the Dropout\nlayer, and the neuron information is randomly removed with a\nprobability of P = 0.5 to mitigate overﬁtting. Finally, the operation\nresult is input into the encoder group module.\nEncoder group module: The encoder group module contains\nthree encoders with the same structure, and a single encoder\ncontains a multi-headed attention layer and a position-encoding\nfeedforward neural network inside, and the structure of the multi-\nheaded attention layer is shown in Figure 8.\nThe multi-headed attention layer contains Num = 5 attention\nheads, denoted as a ∈{1,...,Num}. Each attention head dimension\nis Dim = 35. Deﬁne the fully connected layers L1, L2, L3 with both\ninput and output dimensions of 35, and convert the 35-dimensional\nvectors of a single moment into Q, K, and V vectors, respectively,\ncalculated as follows.\nQ(a)\n(t) =W(a)\nQ L1(t)∈RDim.\nK(a)\n(t) =W(a)\nK L2(t)∈RDim.\nFIGURE 8\nMulti-headed attention layer structure.\nFrontiers in Computational Neuroscience 08 frontiersin.org\nfncom-17-1120566 February 11, 2023 Time: 14:31 # 9\nMa et al. 10.3389/fncom.2023.1120566\nV(a)\n(t) =W(a)\nV L3(t)∈RDim.\nThe Q, K, and V vectors will be used simultaneously to\ncalculate the attention scores. Deﬁning the scaling factor Dk=5−0.5 ,\nthe attention score is calculated as follows.\nAttention(Q,K,V)=softmax(QKT\n√DK\n)V.\nThe result of the operation is ﬁrst input to the fully connected\nlayer for regularization, and the Dropout layer with P = 0.5\nrandomly removes the neuron information; then the residual\nconnection operation is introduced to add the output of the\nDropout layer directly with the input of the multi-headed attention\nlayer in a matrix, and ﬁnally, the output data is LayerNorm\nnormalized and input to the next layer of the network.\nThe structure of the position-encoded feedforward neural\nnetwork is relatively simple and is shown in Figure 9, consisting\nof two fully connected layers, one Dropout layer (P = 0.5) and\nLayerNorm layer, respectively, with the Relu activation function\nintroduced in the middle of the two fully connected layers to ﬁne-\ntune the parameters. The purpose of this layer design is to learn the\nposition information features introduced in the input features in\ncombination with the position encoding module.\nFully connected module: The ﬁnal module of the model consists\nof a single fully connected layer with an input dimension of\n256 ×35 = 8,960 and an output dimension of N400 induced event\ncategory number 2. This module no longer directly uses the Relu\nactivation function for parameter ﬁne-tuning and directly outputs\ntwo-dimensional classiﬁcation results.\nCompared to the traditional Transformer structure used for\nmachine translation tasks, the Erp-Transformer proposed in this\npaper has been somewhat simpliﬁed in structure. Compared to\nthe original Transformer which uses six encoders and decoders\nand eight parallel attention heads (Vaswani et al., 2017), Erp-\nTransformer retains only three structurally identical encoders\nand the number of parallel attention heads in the multi-headed\nattention module is reduced to ﬁve. In the subsequent experimental\nvalidation section we learn that for the N400 features extracted\nby the SSE averaging method, the model proposed in this paper\ncan also achieve good recognition and classiﬁcation results with a\nFIGURE 9\nStructure of position-encoded feedforward neural network.\nsmaller number of training parameters, smaller computation, and\nfaster training speed.\nExperimental design\nExperimental environment\nThe experiments were conducted on a Windows 10 Education\n(64-bit) computer operating system with an AMD Ryzen 5 1,600\nSix-Core Processor and an NVIDIA GeForce RTX 2060 GPU,\nusing Python 3.8.0 as the programming language. The experimental\nmodel was built based on the Pytorch machine learning framework.\nModel evaluation\nThe following quantiﬁers were used in this study: correctly\nidentiﬁed Relate N400 signal (True Relate, TR), correctly identiﬁed\nUnrelate N400 signal (True Unrelate, TU), incorrectly identiﬁed\nRelate N400 signal (False Relate, FR), incorrectly identiﬁed\nUnrelate N400 signal (False Unrelate, FU). Further deﬁne the\nN400 event-related potential recognition Accuracy, Precision, and\nRecall as:\nAccuracy = TR +TU\nTR +TU +FR +FU\nPrecision = TR\nTR +FR\nRecall = TR\nTR +FU\nExperimental details\nPrior to model training, DTW distances were ﬁrst computed\nseparately for all data within the same label range for a single\nsubject and stored as a matrix. The distance matrix was then used\nas the basis for feature extraction of all data using the Soft-DTW-\nbased N400 event-related potential averaging method proposed\nabove. After each individual event-related potential data matrix\nhas passed through the entire SSE averaging process, a total of\n4,497 corresponding superimposed averaging sequence matrices\nare output, with the total amount of data and data speciﬁcations\nremaining unchanged from the original data and still stored\nindependently on a subject by subject basis.\nThis study will use 5-fold cross-validation to evaluate the\nperformance of the model on this training set as well as its\ngeneralization ability. We divide the dataset into 5 copies on a\nsubject-by-subject basis, each containing data from 8 subjects.\nOne of these was selected at each time as the test set, and\nthe remaining four were used as the training set for model\ntraining. Under a particular experimental condition, a total of ﬁve\ntraining sessions were conducted to obtain 5 ×100 epochs = 500\nexperimental results, with each experimental data containing four\ncomponents: TR, TU, FR, and FU. Accuracy, Precision, and\nRecall were calculated separately for each training result, and the\nresults obtained from the ﬁve training sessions were averaged\nwithin the same epoch to obtain the mean values of the three\nevaluation metrics under each epoch, and subsequently, the epoch\ndata with the highest value was taken as the optimal average\nexperimental result.\nTo make the model training eﬀect optimal, the main parameters\nassociated with the Erp-Transformer neural network were screened\nFrontiers in Computational Neuroscience 09 frontiersin.org\nfncom-17-1120566 February 11, 2023 Time: 14:31 # 10\nMa et al. 10.3389/fncom.2023.1120566\nin this study, and the following optimal network parameters were\nﬁnally determined: a random deactivation ratio of 0.5 for the\nDropout layer, a learning rate of 5e-4 for the Relu activation\nfunction, 3 encoder layers for the encoder group, and 5 self-\nattentive heads for each independent encoder. The number of\ntraining data set batches is 256; the model training eﬃciency and\naccuracy perform best when the number of similar sample groups\nin the SSE average process is 25.\nResults and discussion\nAnalysis and discussion of experimental\nresults\nTo investigate the inﬂuence of the number of similar sample\ngroups of independent samples in the averaging process on the\ntraining eﬀect of the model, we conducted comparison experiments\nbetween the SSE averaging results and the original data on the\nTransformer model for the number of similar sample groups\nof 5, 10, 15, 20, 25, and 30, respectively. Figure 10 shows the\nnon-cross-validated confusion matrix of recognition classiﬁcation\nresults for the test set N400 data under the condition of a diﬀerent\nnumber of similar sample groups. Figure 11 shows the test set\nrecognition accuracy (Accuracy) line graph with training batches\nunder a diﬀerent number of similar sample groups with 5-fold\ncross-validation condition.\nAccording to the experimental results shown in Figure 10,\nthe Soft-DTW-based SSE averaging method eﬀectively improves\nthe model’s recognition of N400 data, and there is a Related\ncorrelation between the average recognition accuracy and the\nnumber of similar sample groups. From the experimental results\nshown in Figure 11, it can be seen that the original N400 time-\ncorrelated potential data without averaging reached the highest\naverage Accuracy of 0.705 at the 10th training, and continued to\nshow a certain degree of overﬁtting in the late training period.\nWhen the number of similar sample groups exceeded 20, the\nmodel recognition accuracy no longer improved signiﬁcantly. The\nmodel with similar sample group number 25 condition reached the\nhighest average Accuracy of 0.8992 at the 68th training, which is\nthe highest recognition accuracy of the model compared to other\nsimilar sample group parameter conditions.\nFIGURE 10\nConfusion matrix for identifying classiﬁcation results from test set N400 data under non-cross-validation conditions. In this ﬁgure, SSG is an\nabbreviation for similar sample groups.\nFrontiers in Computational Neuroscience 10 frontiersin.org\nfncom-17-1120566 February 11, 2023 Time: 14:31 # 11\nMa et al. 10.3389/fncom.2023.1120566\nSince the act of superimposed averaging inherently leads to the\nproximity of data features within similar sample groups to each\nother, to demonstrate the eﬀectiveness of the method used in this\npaper, we used applied the operational procedure of single-subject\nshort-distance averaging to conventional arithmetic averaging and\ncompared its obtained output combined with the recognition\nclassiﬁcation eﬀect of the Transformer model with the Soft-DTW-\nbased SSE averaging method, and the experimental results are\nshown in Figure 12.\nAfter the experimental results described in the above table,\nit is known that the classiﬁcation recognition results obtained by\nSoft-DTW applied to the single-subject short-distance averaging\nmethod are signiﬁcantly higher than the arithmetic averaging\nmethod in terms of the Accuracy index. Under the condition that\nthe number of similar sample groups is 25, the SSE averaging\nresults obtained the highest recognition accuracy and precision,\nbut when the number of sample groups is lower than 25,\nthe arithmetic averaging method obtained better recall results.\nWhen the number of similar sample groups is small (below 10),\nthe arithmetic averaging method can obtain higher recognition\naccuracy, however, when the similar sample groups contain more\nsamples, the SSE averaging method shows better resistance to\noverﬁtting and higher recognition accuracy, precision, and upper\nrecall. the high generalization of the SSE averaging method under 5-\nfold cross-validation indicates that it is more suitable for the N400\nevent-related potentials for high-precision recognition tasks.\nIn order to evaluate the recognition of N400 signals with\nfeatures extracted by the SSE averaging method under various\nmodels, this study conducted controlled experiments using the\nErp-Transformer neural network model against the mainstream\nevent-related potential methods under the optimal condition of\na similar sample group size of 25. We chose the CNN-IE model\nproposed by Chou et al., the Conv1d-based Erp feature extraction\nmodule proposed by Yu et al., the Conv2d+LSTM model used\nas a comparison in their work by Maddula et al. (2017) and the\ntraditional Transformer network structure (Vaswani et al., 2017).\nIn reproducing the above network structures, we uniformly used an\ninput size of BatchSize ×256 ×35, retained the original number of\nchannels, convolutional kernel size, and pooling layer parameters\nof the convolutional layer, and linked the above models to the\nFIGURE 11\nVariation of test set recognition accuracy (Accuracy) with training batches under different number of similar sample groups.\nFIGURE 12\nHotspot of model recognition accuracy under different number of similar sample groups.\nFrontiers in Computational Neuroscience 11 frontiersin.org\nfncom-17-1120566 February 11, 2023 Time: 14:31 # 12\nMa et al. 10.3389/fncom.2023.1120566\nTABLE 1 Comparison of the method of this study with other methods (when the number of similar sample groups is 25).\nArithmetic avg Soft-DTW avg\nDeep learning model Accuracy Precision Recall Accuracy Precision Recall\nConv1d 0.8480 0.8979 0.8804 0.8726 0.9122 0.8865\nConv2d+LSTM 0.6779 0.8000 0.6860 0.7837 0.9028 0.8180\nCNN-IE 0.8221 0.9059 0.8653 0.8504 0.9166 0.9026\nTransformer 0.9059 0.9063 0.9308 0.8966 0.9141 0.6860\nErp-Transformer 0.8943 0.9076 0.9188 0.8992 0.9042 0.9051\nTABLE 2 Number of ﬂoating point operations per second (FLOPS), number of parameters, the time required for training per 100 epoch (in seconds) for\nmodel training under different methods.\nConv1d Conv2d+LSTM CNN-IE Erp-Transformer Transformer\nFlops 4.06 MMac 867.45 MMac 140.23 MMac 58.83 MMac 242.81 MMac\nParams 2.59 M 366.74 k 127.58 M 329.6 k 1.11 M\nTimes 57.71 s 143.73 s 410.39 s 311.04 s 1741.92 s\nexact same Softmax binary classiﬁcation module (1,024 ×2) as\nErp-Transformer to compare each model for N400 event-related\npotentials. The Accuracy, Precision, Recall, computation, number\nof parameters, and training time for each model with N400 data\ninput were calculated for the arithmetic averaging method and the\nSoft-DTW averaging method, respectively.\nAccording to the experimental results in Table 1, Conv1d and\nCNN-IE models performed better than Conv2d+LSTM models in\nN400 event-related potential recognition task, which was due to\nthe fact that these two models had shown better performance in\nthe same type of P300 event-related potential recognition task.\nHowever, their performance in identifying N400 event-related\npotentials is still inferior to that of Erp-Transformer and traditional\nTransformer. We can observe that the self-attention mechanism\ngenerally performs better on this data set. Regardless of the model,\nthe short-distance event-related potential averaging method based\non Soft-DTW showed signiﬁcantly better recognition performance\nthan the arithmetic averaging method.\nFrom the experimental results in Table 2, we know that the\nConv1d model has the lowest hardware and time requirements for\ntraining, but the overall training eﬀect is poor; the rest of the model\nmethods containing 2D convolution have too many operations,\na large number of parameters and relatively long training time.\nIn contrast, the Erp-Transformer model has less computation,\nless number of parameters required, faster training eﬃciency, and\nrelatively higher average recognition accuracy. Even though the\nrecognition and classiﬁcation results of the traditional Transformer\nmodel on the processed N400 dataset are comparable to the\nErp-Transformer model proposed in this paper, it shows slightly\nbetter results under the arithmetic average condition. However, the\ntraditional Transformer, because of its relatively complex network\nstructure (6 encoders, 6 decoders, and 8 parallel self-attentive\nheads), increases the number of parameters and operations\nrequired for training substantially, and the average training time\nfor 100 epochs is almost 7 times longer than that required by the\nErp-Transformer. In contrast, the Erp-Transformer model is more\ncost-eﬀective in achieving high recognition classiﬁcation accuracy\nand is more suitable for recognizing N400 event-related potentials.\nConclusion\nThe identiﬁcation and classiﬁcation of N400 event-related\npotentials is important for cognitive linguistics as well as cognitive\npsychology research. Most of the existing event-related potential\nclassiﬁcation models are designed and tested based on the\nP300 dataset. Due to the large diﬀerences in multidimensional\nfeatures of diﬀerent event-related potential datasets, the models\noften have poor migration ability. Therefore, in this study,\nwe propose a Soft-DTW-based single-subject short-range event-\nrelated potential averaging method for feature extraction of\nthe original N400 data and a Transformer-based event-related\npotential classiﬁcation model for the ERP CORE N400 event-\nrelated potential data set. The comparison test shows that the\nnumber of similar sample groups in the averaging method is\nclosely related to the recognition accuracy of the model, and\nthe Soft-DTW-based event-related potential averaging method\nis more beneﬁcial to the recognition and classiﬁcation of\nN400 data by the later model than the traditional arithmetic\naveraging method.\nOn the other hand, the Soft-DTW-based averaging method\nstill has the disadvantages of low data processing eﬃciency and\nhigh time complexity when applied to event-related potentials,\nwhich makes the time cost of preliminary data processing high\nand reduces the overall recognition eﬃciency, so there is still much\nroom for optimization of the overall data processing process of the\nmethod. At the same time, whether the method is compatible with\nother types of event-related potentials, and whether the method\ncan be used as a data enhancement method to further improve the\ngeneralization of the model rather than just as a means of feature\nextraction, will be the focus of our next research.\nData availability statement\nThe original contributions presented in this study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nFrontiers in Computational Neuroscience 12 frontiersin.org\nfncom-17-1120566 February 11, 2023 Time: 14:31 # 13\nMa et al. 10.3389/fncom.2023.1120566\nEthics statement\nWritten informed consent was obtained from the individual(s)\nfor the publication of any potentially identiﬁable images or data\nincluded in this article.\nAuthor contributions\nYT was responsible for the design and implementation of the\nexperiments and the overall writing of the manuscript. YM was\nresponsible for the review and revision of the manuscript. YZ, TD,\nand YL were responsible for some of the programming and data\nvisualization. All authors contributed to the article and approved\nthe submitted version.\nFunding\nThis research was supported by the following two projects:\nScience and Technology Project of Chongqing Municipal\nEducation Commission - Research on Fine-Grained Retrieval\nof Massive Cross-Media Data Based on Cross-Modal Hash\nLearning (KJZD-K202200513) and Chongqing Education Science\n\"14th Five-Year Plan\" Project - Research on Construction and\nEvaluation of Collaborative Learning Model in Smart Education\nEnvironment (2021-GX-014).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAbou-Abbas, L., Noordt, S. V., and Elsabbagh, M. (2021). “Event related potential\nanalysis using machine learning to predict diagnostic outcome of autism spectrum\ndisorder, ” in Proceedings of the international conference on bioengineering and\nbiomedical signal and image processing , (Cham: Springer), 71–79. doi: 10.1007/978-\n3-030-88163-4_7\nAl-rubaye, K. K., Ucan, O. N., Duru, D. G., and Duru, A. D. (2019). “Dynamic time\nwarping based connectivity classiﬁcation of event-related potentials, ” inProceedings of\nthe 2019 medical technologies congress (TIPTEKNO), (Izmir: IEEE), 1–4.\nAurnhammer, C., and Frank, S. L. (2019). Comparing gated and simple recurrent\nneural network architectures as models of human sentence processing . Montreal: The\n41st Annual Conference of the Cognitive Science Society (CogSci 2019). Available\nonline at: https://repository.ubn.ru.nl/handle/2066/213724\nChou, Y., Qiu, T., and Zhong, M. (2018). Classiﬁcation and recognition of P300\nevent related potential based on convolutional neural network. Chin. J. Biomed. Eng.\n657–664. doi: 10.3969/j.issn.0258-8021.2018.06.003\nCuturi, M., and Blondel, M. (2017). “Soft-dtw: A diﬀerentiable loss function\nfor time-series, ” in Proceedings of the international conference on machine learning ,\nNew York, NY , 894–903. doi: 10.48550/arXiv.1703.01541\nCuturi, M., Vert, J. P., Birkenes, O., and Matsui, T. (2007). “A kernel for time series\nbased on global alignments, ” inProceedings of the 2007 IEEE international conference\non acoustics, speech and signal processing-ICASSP’07 , Vol. 2, (Honolulu, HI: IEEE).\ndoi: 10.1109/ICASSP.2007.366260\nDavis, L. C. (2016). Application of DTW barycenter averaging to ﬁnding EEG\nconsensus sequences. Ph.D. thesis. Boulder, CO: University of Colorado at Boulder.\nDuru, D. G., and Duru, A. D. (2018).Classiﬁcation of event related potential patterns\nusing deep learning. medical technologies national congress . Ýstanbul: Ýstanbul Arel\nÜniversitesi. doi: 10.1109/TIPTEKNO.2018.8597016\nGui, Q., Jin, Z., Blondet, M. V. R., Laszlo, S., and Xu, W. (2015). “Towards eeg\nbiometrics: Similarity-based approaches for user identiﬁcation, ” in Proceedings of the\nIEEE international conference on identity, security and behavior analysis, (Hong Kong:\nIEEE), 1–6.\nKappenman, E. S., Farrens, J. L., Zhang, W., Stewart, A. X., and Luck, S. J. (2021).\nERP CORE: An open resource for human event-related potential research.Neuroimage\n225:117465. doi: 10.1016/j.neuroimage.2020.117465\nKarimi, S., and Shamsollahi, M. B. (2022). A new post-processing method using\nlatent structure inﬂuence models for channel fusion in automatic sleep staging. IEEE\nJ. Biomed. Health Inform. 1–9. doi: 10.1109/JBHI.2022.3227407\nKutas, M., and Federmeier, K. D. (2011). Thirty years and counting: Finding\nmeaning in the N400 component of the event related brain potential (ERP). Ann. Rev.\nPsychol. 62:621. doi: 10.1146/annurev.psych.093008.131123\nKutas, M., and Hillyard, S. A. (1980). Reading senseless sentences: Brain potentials\nreﬂect semantic incongruity. Science 207, 203–205. doi: 10.1126/science.7350657\nLau, E. F., Phillips, C., and Poeppel, D. (2008). A cortical network for\nsemantics:(de) constructing the N400.Nat. Rev. Neurosci.9, 920–933. doi: 10.1038/nrn\n2532\nLiang, L., Zhang, Y., and Wang, Q. (2018). Language event related potential N400—\nWord processing and semantic processing interact. Adv. Psychol. 8, 1868–1873. doi:\n10.12677/AP.2018.812217\nLin, T., Wang, Y., Liu, X., and Qiu, X. (2022). A survey of transformers. AI Open 3,\n111–132. doi: 10.1016/j.aiopen.2022.10.001\nMaddula, R., Stivers, J., Mousavi, M., Ravindran, S., and de Sa, V. (2017). Deep\nRecurrent convolutional neural networks for classifying P300 BCI signals.GBCIC 201,\n18–22. doi: 10.3217/978-3-85125-533-1-54\nMerkx, D., and Frank, S. L. (2020). Human sentence processing: Recurrence or\nattention? arXiv [Preprint]. doi: 10.48550/arXiv.2005.09471\nMichaelov, J. A., Bardolph, M. D., Coulson, S., and Bergen, B. K. (2021). Diﬀerent\nkinds of cognitive plausibility: Why are transformers better than RNNs at predicting\nN400 amplitude? arXiv [Preprint]. doi: 10.48550/arXiv.2107.09648\nNicolas-Alonso, L. F., and Gomez-Gil, J. (2012). Brain computer interfaces, a review.\nSensors 12, 1211–1279. doi: 10.3390/s120201211\nPark, S., Ha, J., Park, J., Lee, K., and Im, C. H. (2022). Brain-controlled, AR-based\nHome automation system using SSVEP-based brain-computer interface and EOG-\nbased eye tracker: A feasibility study for the elderly end User. IEEE Trans. Neural Syst.\nRehabil. Eng. 31, 544–553. doi: 10.1109/TNSRE.2022.3228124\nPetitjean, F., Ketterlin, A., and Gançarski, P. (2011). A global averaging method for\ndynamic time warping, with applications to clustering. Pattern Recognit. 44, 678–693.\ndoi: 10.1016/j.patcog.2010.09.013\nPetten, C. V. (1993). A comparison of lexical and sentence-level context\neﬀects in event-related potentials. Lang. Cogn. Process. 8, 485–531. doi: 10.1080/\n01690969308407586\nSchwartz, D., and Mitchell, T. (2019). Understanding language-elicited EEG data\nby predicting it from a ﬁne-tuned language model. arXiv [Preprint]. doi: 10.18653/v1/\nN19-1005\nFrontiers in Computational Neuroscience 13 frontiersin.org\nfncom-17-1120566 February 11, 2023 Time: 14:31 # 14\nMa et al. 10.3389/fncom.2023.1120566\nSimbolon, A. I., Turnip, A., Hutahaean, J., Siagian, Y., and Irawati, N. (2015).\n“An experiment of lie detection based EEG-P300 classiﬁed by SVM algorithm, ” in\nProceedings of the 2015 international conference on automation, cognitive science,\noptics, micro electro-mechanical system, and information technology (ICACOMIT) ,\n(Bandung: IEEE), 68–71. doi: 10.1109/ICACOMIT.2015.7440177\nSun, Z., Xue, Q., Wang, X., and Huang, X. (2020). A survey of emotion recognition\nmethod based on EEG signals. Beijing Biomed. Eng. 39, 186–195.\nTurakhonova, G. B. (2022). Theoretical fundamentals of the concept in cognitive\nlinguistics. Curr. Res. J. Philol. Sci. 3, 36–39. doi: 10.37547/philological-crjps-03-\n01-07\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. Adv. Neural Inform. Process. Syst. 30, 5998—-6008.\nWang, X., Kan, X., and Fan, Y. (2022). Classiﬁcation and recognition of P300 event-\nrelated potential based on LSTM-attention network. Electron. Sci. Technol. 35, 10–16.\ndoi: 10.16180/j.cnki.issn1007-7820.2022.12.002\nYu, H., Xie, J., He, L., Y ang, Y., Zhang, H., and Xu, G. (2021). An event-related\npotential recognition method based on convolutional neural network and support\nvector machine. J. Xian Jiaotong Univ. 55, 47–54.\nZhang, G., Yu, M., Chen, G., Han, Y., Zhang, D., Zhao, G., et al. (2019). A review of\nEEG features for emotion recognition (in Chinese). Sci. Sin. Inform. 49, 1097–1118.\nFrontiers in Computational Neuroscience 14 frontiersin.org",
  "topic": "N400",
  "concepts": [
    {
      "name": "N400",
      "score": 0.7820808291435242
    },
    {
      "name": "Softmax function",
      "score": 0.7691689729690552
    },
    {
      "name": "Computer science",
      "score": 0.7411388158798218
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.7160801887512207
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6358822584152222
    },
    {
      "name": "Speech recognition",
      "score": 0.5815953016281128
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5194249153137207
    },
    {
      "name": "Dynamic time warping",
      "score": 0.5040387511253357
    },
    {
      "name": "Feature extraction",
      "score": 0.4890526235103607
    },
    {
      "name": "Electroencephalography",
      "score": 0.448643296957016
    },
    {
      "name": "Transformer",
      "score": 0.434843510389328
    },
    {
      "name": "Event-related potential",
      "score": 0.30905425548553467
    },
    {
      "name": "Artificial neural network",
      "score": 0.16017428040504456
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ]
}