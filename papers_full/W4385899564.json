{
    "title": "Evaluating Large Language Models for the National Premedical Exam in India: Comparative Analysis of GPT-3.5, GPT-4, and Bard (Preprint)",
    "url": "https://openalex.org/W4385899564",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2994037111",
            "name": "Faiza Farhat",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4295509337",
            "name": "Beenish Moalla Chaudhry",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1592578192",
            "name": "Mohammad Nadeem",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2123421705",
            "name": "Shahab Saquib Sohail",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2629439864",
            "name": "Dag Øivind Madsen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4381988031",
        "https://openalex.org/W4379416244",
        "https://openalex.org/W4383710481",
        "https://openalex.org/W4364360686",
        "https://openalex.org/W4360957277",
        "https://openalex.org/W4327522280",
        "https://openalex.org/W4318466538",
        "https://openalex.org/W4365512576",
        "https://openalex.org/W3201174429",
        "https://openalex.org/W4381855196",
        "https://openalex.org/W4321499901",
        "https://openalex.org/W4376866715",
        "https://openalex.org/W4309674289",
        "https://openalex.org/W4321499403",
        "https://openalex.org/W4320009668",
        "https://openalex.org/W4381431616",
        "https://openalex.org/W4323352552",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4319460874",
        "https://openalex.org/W4362600559",
        "https://openalex.org/W4365137614",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W4365790143",
        "https://openalex.org/W4323793950",
        "https://openalex.org/W4367353919",
        "https://openalex.org/W2945815334",
        "https://openalex.org/W2487897940",
        "https://openalex.org/W2514256928",
        "https://openalex.org/W4380356334",
        "https://openalex.org/W4366425747",
        "https://openalex.org/W4385235884",
        "https://openalex.org/W4319663047",
        "https://openalex.org/W4319779057",
        "https://openalex.org/W4384926409"
    ],
    "abstract": "<sec> <title>BACKGROUND</title> Large language models (LLMs) have revolutionized natural language processing with their ability to generate human-like text through extensive training on large data sets. These models, including Generative Pre-trained Transformers (GPT)-3.5 (OpenAI), GPT-4 (OpenAI), and Bard (Google LLC), find applications beyond natural language processing, attracting interest from academia and industry. Students are actively leveraging LLMs to enhance learning experiences and prepare for high-stakes exams, such as the National Eligibility cum Entrance Test (NEET) in India. </sec> <sec> <title>OBJECTIVE</title> This comparative analysis aims to evaluate the performance of GPT-3.5, GPT-4, and Bard in answering NEET-2023 questions. </sec> <sec> <title>METHODS</title> In this paper, we evaluated the performance of the 3 mainstream LLMs, namely GPT-3.5, GPT-4, and Google Bard, in answering questions related to the NEET-2023 exam. The questions of the NEET were provided to these artificial intelligence models, and the responses were recorded and compared against the correct answers from the official answer key. Consensus was used to evaluate the performance of all 3 models. </sec> <sec> <title>RESULTS</title> It was evident that GPT-4 passed the entrance test with flying colors (300/700, 42.9%), showcasing exceptional performance. On the other hand, GPT-3.5 managed to meet the qualifying criteria, but with a substantially lower score (145/700, 20.7%). However, Bard (115/700, 16.4%) failed to meet the qualifying criteria and did not pass the test. GPT-4 demonstrated consistent superiority over Bard and GPT-3.5 in all 3 subjects. Specifically, GPT-4 achieved accuracy rates of 73% (29/40) in physics, 44% (16/36) in chemistry, and 51% (50/99) in biology. Conversely, GPT-3.5 attained an accuracy rate of 45% (18/40) in physics, 33% (13/26) in chemistry, and 34% (34/99) in biology. The accuracy consensus metric showed that the matching responses between GPT-4 and Bard, as well as GPT-4 and GPT-3.5, had higher incidences of being correct, at 0.56 and 0.57, respectively, compared to the matching responses between Bard and GPT-3.5, which stood at 0.42. When all 3 models were considered together, their matching responses reached the highest accuracy consensus of 0.59. </sec> <sec> <title>CONCLUSIONS</title> The study’s findings provide valuable insights into the performance of GPT-3.5, GPT-4, and Bard in answering NEET-2023 questions. GPT-4 emerged as the most accurate model, highlighting its potential for educational applications. Cross-checking responses across models may result in confusion as the compared models (as duos or a trio) tend to agree on only a little over half of the correct responses. Using GPT-4 as one of the compared models will result in higher accuracy consensus. The results underscore the suitability of LLMs for high-stakes exams and their positive impact on education. Additionally, the study establishes a benchmark for evaluating and enhancing LLMs’ performance in educational tasks, promoting responsible and informed use of these models in diverse learning environments. </sec>",
    "full_text": null
}