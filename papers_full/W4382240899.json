{
    "title": "Symmetry-Aware Transformer-Based Mirror Detection",
    "url": "https://openalex.org/W4382240899",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2118922672",
            "name": "Tianyu Huang",
            "affiliations": [
                "Harbin Institute of Technology",
                "City University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2275601388",
            "name": "Bowen Dong",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2109333844",
            "name": "Jiaying Lin",
            "affiliations": [
                "City University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2101154754",
            "name": "Xiao-Hui Liu",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2104532978",
            "name": "Rynson W.H. Lau",
            "affiliations": [
                "City University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2079150322",
            "name": "Wangmeng Zuo",
            "affiliations": [
                "Harbin Institute of Technology",
                "Peng Cheng Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2118922672",
            "name": "Tianyu Huang",
            "affiliations": [
                "City University of Hong Kong",
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2079150322",
            "name": "Wangmeng Zuo",
            "affiliations": [
                "Peng Cheng Laboratory",
                "Harbin Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6676889658",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W2807746031",
        "https://openalex.org/W2798791840",
        "https://openalex.org/W2957414648",
        "https://openalex.org/W3097053213",
        "https://openalex.org/W3016673627",
        "https://openalex.org/W2607081783",
        "https://openalex.org/W6848416832",
        "https://openalex.org/W6779376668",
        "https://openalex.org/W2796255296",
        "https://openalex.org/W3034320133",
        "https://openalex.org/W6794896171",
        "https://openalex.org/W2151103935",
        "https://openalex.org/W1566328901",
        "https://openalex.org/W6798254802",
        "https://openalex.org/W3041375543",
        "https://openalex.org/W3035422681",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3196543878",
        "https://openalex.org/W2620958690",
        "https://openalex.org/W6838786147",
        "https://openalex.org/W174734558",
        "https://openalex.org/W2979983945",
        "https://openalex.org/W2780708736",
        "https://openalex.org/W3034185160",
        "https://openalex.org/W2937549930",
        "https://openalex.org/W6753421600",
        "https://openalex.org/W6741766692",
        "https://openalex.org/W3042674218",
        "https://openalex.org/W2111938263",
        "https://openalex.org/W3108421143",
        "https://openalex.org/W4214561053",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3035284915",
        "https://openalex.org/W3108822985",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4312836786",
        "https://openalex.org/W3034592098",
        "https://openalex.org/W3179324864",
        "https://openalex.org/W4239072543",
        "https://openalex.org/W2963112696",
        "https://openalex.org/W2963685207",
        "https://openalex.org/W2132083787",
        "https://openalex.org/W4282922170",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2884822772",
        "https://openalex.org/W3034552520",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W2738290714"
    ],
    "abstract": "Mirror detection aims to identify the mirror regions in the given input image. Existing works mainly focus on integrating the semantic features and structural features to mine specific relations between mirror and non-mirror regions, or introducing mirror properties like depth or chirality to help analyze the existence of mirrors. In this work, we observe that a real object typically forms a loose symmetry relationship with its corresponding reflection in the mirror, which is beneficial in distinguishing mirrors from real objects. Based on this observation, we propose a dual-path Symmetry-Aware Transformer-based mirror detection Network (SATNet), which includes two novel modules: Symmetry-Aware Attention Module (SAAM) and Contrast and Fusion Decoder Module (CFDM). Specifically, we first adopt a transformer backbone to model global information aggregation in images, extracting multi-scale features in two paths. We then feed the high-level dual-path features to SAAMs to capture the symmetry relations. Finally, we fuse the dual-path features and refine our prediction maps progressively with CFDMs to obtain the final mirror mask. Experimental results show that SATNet outperforms both RGB and RGB-D mirror detection methods on all available mirror detection datasets.",
    "full_text": "Symmetry-Aware Transformer-Based Mirror Detection\nTianyu Huang1,2, Bowen Dong1, Jiaying Lin2, Xiaohui Liu1, Rynson W.H. Lau2, Wangmeng\nZuo1, 3*\n1 Harbin Institute of Technology\n2 City University of Hong Kong\n3 Peng Cheng Laboratory\n{tyhuang0428,cndongsky,lxh720199}@gmail.com, jiayinlin5-c@my.cityu.edu.hk,\nrynson.lau@cityu.edu.hk, wmzuo@hit.edu.cn\nAbstract\nMirror detection aims to identify the mirror regions in the\ngiven input image. Existing works mainly focus on integrat-\ning the semantic features and structural features to mine spe-\ncific relations between mirror and non-mirror regions, or in-\ntroducing mirror properties like depth or chirality to help\nanalyze the existence of mirrors. In this work, we observe\nthat a real object typically forms a loose symmetry relation-\nship with its corresponding reflection in the mirror, which\nis beneficial in distinguishing mirrors from real objects.\nBased on this observation, we propose a dual-path Symmetry-\nAware Transformer-based mirror detection Network (SAT-\nNet), which includes two novel modules: Symmetry-Aware\nAttention Module (SAAM) and Contrast and Fusion Decoder\nModule (CFDM). Specifically, we first adopt a transformer\nbackbone to model global information aggregation in images,\nextracting multi-scale features in two paths. We then feed the\nhigh-level dual-path features to SAAMs to capture the sym-\nmetry relations. Finally, we fuse the dual-path features and re-\nfine our prediction maps progressively with CFDMs to obtain\nthe final mirror mask. Experimental results show that SATNet\noutperforms both RGB and RGB-D mirror detection methods\non all available mirror detection datasets.\nIntroduction\nMirrors are common objects in the human world, and their\npresence can affect the performance of a range of vision\ntasks. For example, Zendel et al. propose a list of potential\nhazards within the CV domain, and the existence of mirrors\nis one of them. However, mirror detection can be challeng-\ning by using some general detection methods from related\ntasks, such as salient object detection and semantic segmen-\ntation. As such, it is necessary to treat mirror detection as an\nindependent vision task, and previous works have managed\nto tackle this issue from either relation-based frameworks or\nproperty-based paradigms.\nRelations between mirror and non-mirror regions are\ncounted in most mirror detection methods. Yang et al. pro-\npose to extract contextual discontinuities among regions, but\nit can only be effective when mirror boundaries are clear\nagainst backgrounds. Lin, Wang, and Lau propose to per-\nceive similarity relationships for contents inside and outside\n*Corresponding Author\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nImage MirrorNet PMDNet SANet Ours GT\nFigure 1: Comparison of mirror detection among state-of-\nthe-art methods. MirrorNet (Yang et al. 2019) cannot han-\ndle scenes with vague mirror boundaries. Although PMD-\nNet (Lin, Wang, and Lau 2020) considers similarity seman-\ntics, it can hardly detect the symmetry pair (1st row), and\ncan easily count part of the similar non-mirror regions into\nmirrors (2nd row). SANet (Guan, Lin, and Lau 2022) only\ndetects the mirror region above the sink (1st row), and it\nhas worse predictions when semantic associations are lack-\ning (2nd row). By modeling a loose symmetry relationship,\nSATNet succeeds in both cases.\nmirrors, which may easily fail when similarities come from\nnon-mirror regions. Guan, Lin, and Lau propose to learn\nsemantic associations in mirror scenes, while such relation\nquite relies on the environments nearby mirrors. It can only\nadapt to a few mirror cases, e.g., a mirror above a sink.\nConsidering mirror properties, Mei et al. and Tan et al. re-\ngard depth and chirality as additional information for the de-\ntection, respectively. However, these property aggregations\nonly focus on mirror regions, dismissing the environmental\nsemantics related to mirrors.\nFor a general solution, we need to fully leverage the\nrelationship between mirror and non-mirror regions based\non mirror properties. Considering mirror reflection, sym-\nmetry relationships between mirror and non-mirror regions\nare supposed to be an essential cue for mirror detection. In\nFig. 1(1st row), the right half of the mirror would not be\nmissed if the mirror detection model could detect the mirror\nsymmetry relationship of the two paintings. In Fig. 1(2nd\nrow), if the model recognizes the left power bank as the\nmirror region, it can then classify the corresponding real\npower bank on the right as a non-mirror region. However,\nthis symmetry relationship is not a strict mirror symmetry\nand is highly dependent on the camera viewpoint. The paint-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n935\nings inside and outside mirrors form a nearly perfect re-\nflection symmetry pair in Fig. 1(1st row), while the power\nbanks inside and outside mirrors are from different views in\nFig. 1(2nd row). We cannot adopt reflection symmetry de-\ntection methods directly. Instead, we observe that real-world\nobject and their reflection in mirrors always maintain seman-\ntic or luminance consistency with each other, even though\nthey may not be strictly symmetric regarding the position or\norientation. That is, an object in a mirror should be a mir-\nror reflection of an object in the real world from a certain\nview. We regard this kind of relationship as loose symmetry\nand aim to explore a new solution to model and leverage this\nloose symmetry relationship for mirror detection.\nTaking loose symmetry into account, we present our\nSymmetry-Aware Transformer-based mirror detection Net-\nwork (SATNet). In particular, we introduce the first trans-\nformer baseline in mirror detection, considering the long-\nrange dependencies that loose symmetry requires. We con-\nstruct a dual-path network to extract and enhance symmetric\nfeatures, taking an input image as well as its corresponding\nhorizontally flipped image as inputs. For modeling symme-\ntry semantics, we propose a novel Symmetry-Aware Atten-\ntion Module (SAAM) in high-level dual-path features. For\nmirror region segmentation, we propose a novel Contrast\nand Fusion Decoder Module (CFDM), which constructs a\npyramidal decoder to progressively fuse and refine dual-path\nfeatures.\nTo sum up, our main contributions include:\n• We observe that there are typically loose symmetry\nrelationships between mirror and non-mirror regions.\nBased on this observation, we propose a novel dual-\npath Symmetry-Aware Transformer-based mirror detec-\ntion network (SATNet) to learn symmetry relations for\nmirror detection. This is the first transformer pipeline in\nmirror detection.\n• We present a novel Symmetry-Aware Attention Module\n(SAAM) to extract high-level symmetry semantics and a\nnovel Contrast and Fusion Decoder Module (CFDM) to\nrefine multi-scale mirror features.\n• Our network SATNet achieves state-of-the-art results on\nvarious mirror detection datasets. Experimental results\nclearly demonstrate the benefit of loose symmetry rela-\ntionships for mirror detection.\nRelated Work\nMirror Detection\nThe mirror detection task aims to identify the mirror regions\nof the given input image. To tackle this problem, several\nmethods attempt to model specific relations between mirror\nand non-mirror regions. Yang et al. proposed the first mir-\nror detection network called MirrorNet, which focuses on\nperceiving the contrasting features between the contexts in-\nside and outside mirrors. Lin, Wang, and Lau suggested a\nprogressive mirror detection network PMDNet, designing a\nrelational contextual contrasted local module to extract simi-\nlarity features. Guan, Lin, and Lau proposed to learn seman-\ntic association in mirror scenes, which may imply the exis-\ntence of mirrors. However, those methods can hardly adapt\nto general mirror detection cases as the relations they match\nare either too simple or too strict. Recent methods take mir-\nror properties into account. Mei et al. introduced depth in-\nformation to mirror detection as the depth value in mirror\nregions is irregular. In contrast, the depth input is unreliable,\nand the method can be easily misled by depth. Tan et al.\nproposed a dense visual chirality discriminator to judge the\npossible mirror existence, while the improvement is limited\nsince chirality information tends to be subtle when mirror\ncontents are clean. The leverage of mirror properties in these\nmethods mainly depends on the semantics of mirror regions,\ndismissing the interaction with non-mirror regions. Unlike\nexisting works, we aim to utilize loose symmetry relation-\nships between real-world objects and corresponding mirror\nregions to enhance the overall detection ability.\nReflection Symmetry Detection\nReflection symmetry detection aims to detect symmetry axes\nin given images. Early works in this task can be divided\ninto two categories: keypoint matching detection and dense\nheatmap detection. Loy and Eklundh adopted SIFT (Lowe\n2004) to compute matched keypoints, and generated poten-\ntial symmetry axes accordingly. Cornelius and Loy took a\nsingle matching pair for hypothesizing with the local affine\nframe. For dense heatmap, Tsogkas and Kokkinos utilized\npixel-level features to predict the symmetry area densely.\nFunk and Liu employed CNNs to extract the symmetric\nfeatures directly. Recently, Seo, Shim, and Cho proposed\na novel polar matching convolution to encode the similari-\nties among pixels. Contrary to the strict reflection symmetry,\nsymmetry relationships in mirror cases are loosely defined.\nTherefore, reflection symmetry detection methods cannot be\ndirectly employed in mirror detection. To tackle this, we pro-\npose a dual-path Transformer-based structure with attention\nmechanisms in high-level features to model the loose sym-\nmetry relationships.\nSalient Object Detection\nSalient object detection (SOD) aims to detect and segment\nthe most distinct object in an input image. Existing methods\nin RGB SOD are mainly based on the UNet structure (Ron-\nneberger, Fischer, and Brox 2015), like (Wang et al. 2017;\nPang et al. 2020b). Deng et al. adopt a recurrent network\nto refine the salient map progressively. Liu, Han, and Yang\nadopt attention mechanisms to learn more dependencies\namong features. Recently, RGB-D SOD has received con-\nsiderable attention. Several methods (Song et al. 2017) treat\ndepth as an additional dimension of the input features, while\nthe others (Fan et al. 2020a) separately extract RGB and\ndepth features and fuse them in the decoding process. Liu,\nZhang, and Han proposed to fuse depth information with at-\ntention mechanisms. Pang et al. integrated RGB and depth\nthrough densely connected structures. Liu et al. proposed\na vision transformer network, rethinking this field from an\naspect of sequence-to-sequence architectures. Albeit similar\nto mirror detection, SOD methods can hardly have a good\nperformance on the mirror detection task as mirrors are not\nsalient enough to detect in most cases. SOD methods may\nwrongly detect some conspicuous objects inside mirrors.\n936\nSeg Head\nFlippingUpsamplingSupervisionSharedWeights\n(a) Pipeline\nφ Concatenation+ElementAdditionQuery\nQuery\nKeyValue +\n+\n+\nφφ φ\n(b) SAAM (c) CFDM\nSAAMCFDMAttentionECACCL\nI\nIf\nF3F2F1F0\nFf\n0 Ff\n1 Ff\n2 Ff\n3\nFout\nFf Ff\nF F\nFc\n˜Ff\nFc\n˜F D\nM P0 P1 P2 P3\nFigure 2: (a) Pipeline of our SATNet. (b) Symmetry-Aware Attention Module. (c) Contrast and Fusion Decoder Module.\nMethod\nBased on the idea of loose symmetry relationships, we pro-\npose a dual-path Symmetry-Aware Transformer-based net-\nwork for mirror detection. Loose symmetry relationships\ncan assist the detecting process in two aspects: presences of\nloose symmetry relationships imply the possible existence of\nmirrors; differences between symmetric pairs indicate which\npart belongs to mirror regions. The dual-path structure and\nour novel Symmetry-Aware Attention Module are designed\nfor the first aspect. Additionally, to better encode the sym-\nmetry features as well as recognize the corresponding mirror\nsemantics, a transformer backbone and our Contrast and Fu-\nsion Decoder Module are proposed for the second aspect.\nOverview\nFig. 2(a) illustrates the pipeline of our SATNet. Given an in-\nput image I as well as its flipped image If , we feed them\ninto a shared-weights transformer backbone to obtain multi-\nscale features {F0, ...,F3} and corresponding flipped fea-\ntures {Ff\n0 , ...,Ff\n3 }, respectively. For modeling symmetry re-\nlations, we select features from the highest two levels of both\npaths, and feed features in the same level into our Symmetry-\nAware Attention Module (SAAM), fetching joint features\nˆF and ˆFf . Then, the multi-scale features {Fi/ˆFi} as well\nas the flipped features {Ff\ni /ˆFf\ni } are fed into corresponding\nContrast and Fusion Decoder Module (CFDM), generating\ncoarse output features Fout\ni with different scales progres-\nsively. For eachFout\ni (except Fout\n0 ), we upsample it into the\nnext decoder as the reference features Di−1 for further pre-\ndiction refinement. Meanwhile, we get the prediction map\nPi in each decoder through a segmentation head and super-\nvise it via the ground-truth mask M. Finally, our prediction\nresult ˜M is generated by the last decoder module.\nDual-Path Structure\nIn most cases, loose symmetry relationships are implicit un-\nder complex semantics. Such a relationship is too hidden\nto be perceived by existing baselines, which has been veri-\nfied in our ablation study. To better perceive the relationship,\ndual enhancements are suggested. As a common method of\ndata augmentation, horizontal flipping can modify the global\nsemantics of natural images, while symmetry relationships\nstill exist (they just display in the opposite direction). Thus,\nwe introduce a dual-path network to extract symmetric fea-\ntures: GivenF and Ff from both paths, we expect they differ\nfrom each other but have features of the same loose sym-\nmetry relationship. When we concatenate them together as\nFc, symmetry semantics in the symmetric region can be en-\nhanced. To extract the same symmetric features, input im-\nages I and If must be fed into the same backbone. Our fu-\nsion function is defined as follows:\nφ(a1, ...,an) =σ(BN (ψ3×3(ψ1×1([a1, ...,an])))), (1)\nFc = φ(F, flip(Ff )), (2)\nwhere [·, ...,·] denotes the concatenation operation on the\nchannel dimension. ψw×w is a w × w convolution, BN de-\nnotes the Batch Normalization, σ is the ReLU activation\nfunction, and flip is the horizontal flipping. To align features\nin the spatial level during concatenation, we flip Ff back\nbefore SAAM or CFDM.\nSymmetry-Aware Attention Module\nFig. 2(b) shows the architecture of our symmetry-aware at-\ntention module. With SAAM, we aim to perceive loose sym-\nmetry relationships in an image that indicates the possible\nexistence of mirrors. To this end, we use the attention mech-\nanisms (i) to enhance the feature F of the input image as\nwell as (ii) to obtain the symmetry-aware feature by model-\ning the dependency between the input and its flipped images.\n937\nIn general, the attention mechanism can model the depen-\ndencies among each position in a global manner (Vaswani\net al. 2017), which can be formulated as,\nAttention(Q, K, V) =Softmax (QKT\n√dk\n)V, (3)\nwhere Q, K and V denote Query, Key, and Value, respec-\ntively.\nOur SAAM takes both Fc as well as F and Ff as the\ninput. Among them, Fc aggregates the features from F\nand Ff from both paths and is spatially consistent with F,\nand thus can be treated as an augmented representation of\nF. To exploit the attention to enhance the feature F, we\ntreat F as query and Fc as key and value, and further ap-\nply channel transformation with Efficient Channel Attention\n(ECA) (Wang et al. 2020) module right after the attention\nmodule to obtain the enhanced feature ˆF,\nˆF = ECA(Attention(F, Fc, Fc)), (4)\nwhere ECA(·) denotes the Efficient Channel Attention\nmodule. To obtain the symmetry-aware feature, we treat Ff\nas query and Fc as key and value. Note that Ff is extracted\nfrom the flipped image, and Fc is spatially consistent with\nthe input image. Their similarity score can thus be treated as\nan indicator of loose mirror symmetry between parts from\nthe input and its flipped images. And the output of the at-\ntention module can then be regarded to be symmetry-aware.\nAnalogous to ˆF, the symmetry-aware feature is obtained by,\nˆFf = ECA(Attention(Ff , Fc, Fc)). (5)\nContrast and Fusion Decoder Module\nSince MirrorNet (Yang et al. 2019), Context Contrasted\nLocal (CCL) decoder (Ding et al. 2018) has been widely\nadopted in mirror detection networks. To better refine the\nprediction, edge extractors are joint as an extra supervision\nto previous methods (Lin, Wang, and Lau 2020; Tan et al.\n2022) as well. In this subsection, we further extend the CCL\nmodule to present our CFDM for handling multiple features.\nWith no edge information, our CFDM can outline precise\nmirror boundaries efficiently by refining multi-level features\nprogressively in a top-down structure.\nAs shown in Fig. 2(c), our CFDM takes Fi and Ff\ni as\nthe input when i = 0, 1, and ˆFi and ˆFf\ni when i = 2, 3.\nWithout loss of generality, we use Fi and Ff\ni as an example\nto explain the CFDM module. To begin with, we use Eq. (2)\nto obtain the fused feature Fc\ni . Denote by Fout\ni+1 the (i + 1)-\nscale CFDM output. We then upsample Fout\ni+1 to obtain the\nhigher-level feature map,\nDi = U2(σ(BN (ψ3×3(Fout\ni+1))), (6)\nwhere U2 denotes the bilinear upsampling operation. Subse-\nquently, the reference features for(Fc\ni , Fi, Ff\ni ) can be given\nby,\n(˜Fc\ni , ˜Fi, ˜Ff\ni ) =\n(\n(Fc\ni , Fi, Ff\ni ) ⊕ (Di, Di, Di), i < 3\n(Fc\ni , Fi, Ff\ni ), i = 3\n(7)\nwhere ⊕ denotes the element-wise summation operator.\nThe three feature maps ˜Fc\ni , ˜Fi, ˜Ff\ni are separately fed into\nthe CCL module to extract contrastive semantics. Here we\nuse ˜Fi as an example,\nCCL(˜Fi) =σ(BN (fl(˜Fi) − fct(˜Fi))), (8)\nwhere fl is the local feature extractor which contains a3 ×3\nconvolution with a dilation rate of 1, BN, and ReLU in turn.\nConsidering the changes in the receptive field, we set dila-\ntion rates to {8, 6, 4, 2} for layer {0, 1, 2, 3}, respectively.\nFinally, we concatenate those three CCL outputs together to\nget the output features Fout\ni and the corresponding predic-\ntion map Pi, which is given as follows,\nFout\ni = φ(CCL(˜Fc\ni ), CCL(˜Fi), CCL(˜Ff\ni )), (9)\nPi = fseg(Fout\ni ), (10)\nwhere fseg is a segmentation head whose output has two\nchannels. And the output of the last decoder layer P0 is\nadopted as the final prediction result ˜M of our network.\nTransformer for Mirror Detection\nAs for the feature extraction, loose symmetry is typically\na long-range relationship, which means our network needs\na large receptive field to perceive it. CNN-based methods\nutilize a couple of convolution kernels to fulfill local fea-\nture aggregation. However, the convolution with a small ker-\nnel size cannot construct global feature aggregation directly,\nwhich restricts the feature representation ability of those\nmethods in complex scenarios. In contrast, the self-attention\nmodule in transformers can model the long-range interac-\ntion explicitly, making vision transformers very competitive\nin several complex scene understanding tasks (Zheng et al.\n2021). Swin Transformer (Liu et al. 2021b) proposes regular\nand shifted window self-attention modules to construct lo-\ncal and global feature aggregation with limited computation\ncomplexity while achieving state-of-the-art performance in\nscene parsing. Thus, we adopt a transformer pipeline in mir-\nror detection based on Swin Transformer.\nLoss Function\nOur learning objective is defined by considering all scales.\nFor each prediction map Pi, we calculate the binary cross-\nentropy (BCE) loss (De Boer et al. 2005) betweenPi and the\nground-truth M. The overall loss functionL is then given as\nthe summation of BCE loss for each prediction map,\nL =\n3X\ni=0\nwiLbce(Pi, M), (11)\nwhere wi is the corresponding weight for the i-th layer. We\nempirically set the weightwi as [1.25, 1.25, 1.0, 1.5] accord-\ning to the experimental results.\nExperiments\nDatasets and Evaluation Metrics\nFollowing previous works (Yang et al. 2019; Lin, Wang, and\nLau 2020), we use Mirror Segmentation Dataset (MSD) and\n938\nMethod MSD PMD\nIoU ↑ Fβ ↑ MAE ↓ IoU ↑ Fβ ↑ MAE ↓\nCPDNet 57.58 0.743 0.115 60.04 0.733 0.041\nMINet 66.39 0.823 0.087 60.83 0.798 0.037\nLDF 72.88 0.843 0.068 63.31 0.796 0.037\nVST 79.09 0.867 0.052 59.06 0.769 0.035\nMirrorNet 78.88 0.856 0.066 58.51 0.741 0.043\nPMDNet 81.54 0.892 0.047 66.05 0.792 0.032\nSANet 79.85 0.879 0.054 66.84 0.837 0.032\nVCNet 80.08 0.898 0.044 64.02 0.815 0.028\nOurs 85.41 0.922 0.033 69.38 0.847 0.025\nTable 1: Quantitative results of the state-of-the-art methods\non MSD dataset and PMD dataset. Our method achieves the\nbest performance in terms of all the evaluation metrics.\nProgressive Mirror Dataset (PMD) to evaluate our method.\nBesides, we adopt an RGB-D dataset RGBD-Mirror to make\na comparison with the state-of-the-art RGB-D mirror detec-\ntion method PDNet (Mei et al. 2021). To assess mirror de-\ntection performance, we adopt three commonly used dense\nprediction evaluation metrics: intersection over union (IoU),\nF-measure Fβ, and mean absolute error (MAE).\nImplementation Details\nWe implement our network on PyTorch (Paszke et al. 2019)\nand use the small version of Swin Transformer (namely\nSwin-S) pre-trained on ImageNet-1k (Deng et al. 2009) as\nthe backbone of our network. Note that dual-path features\nare fed into the same backbone and share weights. Follow-\ning data augmentation methods adopted by previous works,\nwe adopt random resize and crop as well as random horizon-\ntal flipping to augment training images. And for testing, we\nsimply resize input images to 512 × 512 to evaluate our net-\nwork. Our network is trained on 8 Tesla V100 GPUs with\n2 images per GPU for 20K iterations. During training, we\nuse ADAM weight decay optimizer and set β1, β2, and the\nweight decay to 0.9, 0.999, and 0.01, respectively. The learn-\ning rate is initialized to 6 × 10−4 and decayed by the poly\nstrategy with the power of 1.0. It takes 6 hours to train our\nnetwork, and testing on a single GPU needs 0.08s per image.\nComparison with State-of-the-Arts\nTo evaluate SATNet, we extensively compare it with sev-\neral state-of-the-art methods. As shown in Table 1, we se-\nlect 8 state-of-the-art methods for the comparison on MSD\ndataset and PMD dataset, including 4 RGB salient object\ndetection methods CPDNet (Wu, Su, and Huang 2019),\nMINet (Pang et al. 2020c), LDF (Wei et al. 2020), and\nVST (Liu et al. 2021a), and 4 mirror detection methods Mir-\nrorNet (Yang et al. 2019), PMDNet (Lin, Wang, and Lau\n2020), SANet (Guan, Lin, and Lau 2022), and VCNet (Tan\net al. 2022). Our network outperforms other methods in\nterms of all the evaluation metrics. Fig. 3 provides the vi-\nsualized comparison with those methods. The first two rows\nare examples of loose symmetry relationships. Our network\ncan precisely distinguish real-world objects from their mir-\nror reflections. In the first row, the cartoon toy and its re-\nMethod w/ Depth RGBD-Mirror\nIoU ↑ Fβ ↑ MAE ↓\nJL-DCF ✓ 69.65 0.844 0.056\nDANet ✓ 67.81 0.835 0.060\nBBSNet ✓ 74.33 0.868 0.046\nVST ✓ 70.20 0.851 0.052\nPDNet 73.57 - 0.053\nPDNet ✓ 77.77 0.878 0.041\nSANet 74.99 0.873 0.048\nVCNet 73.01 0.849 0.052\nOurs 78.42 0.906 0.031\nTable 2: Quantitative results of the state-of-the-art methods\non RGBD-Mirror dataset. w/ Depth denotes the usage of\ndepth information in a corresponding method. Our method\noutperforms all the competing methods, even though we do\nnot use depth information.\nflection in mirrors cannot construct an apparent reflection\nsymmetry, but our network can still perceive which part is\nin the mirrors. Albeit PMDNet (Lin, Wang, and Lau 2020)\nhas a specific module for modeling similarity relationships,\nit fails in handling an easy case in the second row, in which a\nchalk eraser is reflected in the mirror. The last row has scenes\nwhere mirrors are similar to their surroundings. Our method\ncan well exclude the non-mirror region, while the competing\nmethods tend to classify a similar area as the mirror region,\nespecially four mirror detection methods. The results show\nthat symmetry awareness is beneficial for mirror detection,\nand our method can utilize symmetry information well.\nOur method is also compared with 4 RGB-D salient object\ndetection methods JL-DCF (Fu et al. 2020), DANet (Zhao\net al. 2020), BBSNet (Fan et al. 2020b) and VST (Liu et al.\n2021a), and 3 mirror detection methods PDNet (Mei et al.\n2021), SANet (Guan, Lin, and Lau 2022) and VCNet (Tan\net al. 2022) on the RGBD-Mirror dataset. As shown in Ta-\nble 2, our method does not leverage depth information, and\ncan still achieve the best performance in terms of all the eval-\nuation metrics. Visualization results are shown in Fig. 4. In\nall the four examples, RGB-D methods are likely misled by\ndepth information. Especially in the first row, they wrongly\njudge the depth changes as the existence of mirrors. In the\nsecond row, our method correctly detects the mirror region\nby exploiting the loose symmetry relationship between the\ntelevision and its reflection, while some competing methods\neven fail to detect the correct side of the mirror. In the third\nrow, there is a mirror that can be easily missed. All the com-\npeting methods ignore the left mirror, although the depth\nmap has an obvious change in that area. Our method can\nstill discover the mirror as the scene has a kind of symme-\ntry relation with the nearby cabinet. In the last row, we note\nthat our method does not mis-detect the glasses as a mirror\nregion, while the competing methods can hardly tell sub-\ntle differences between mirrors and glasses. Different from\nmirrors, glasses can transmit most of the light, which weak-\nens reflection effects. It shows that our method can identify\ncorresponding reflection features from mirrors. All the cases\nshow that symmetry information can greatly benefit the per-\n939\nImage CPDNet MINet LDF VST MirrorNet PMDNet SANet VCNet Ours GT\nFigure 3: Visualization results on MSD and PMD datasets. The first two rows are examples of loose symmetry relationships.\nThe last row has scenes where mirrors are similar to their surroundings.\nMethod IoU ↑ Fβ ↑ MAE ↓\nBaseline 80.46 0.901 0.045\nDual-Path 79.59 0.903 0.044\nDual-Path + SAAM 80.01 0.918 0.042\nDual-Path + SAAMs 80.03 0.903 0.043\nDual-Path + CFDM 81.98 0.918 0.039\nDual-Path + SAAM + CFDM 82.96 0.911 0.039\nSATNet(Ours) 85.41 0.922 0.033\nTable 3: Ablation study results on MSD. Swin-S denotes our\nbaseline method, which is decoded by UperNet. Dual-Path\ndenotes the dual-path Swin Transformer. SAAM denotes our\nSymmetry-Aware Attention Module on a scale of 3. SAAMs\ndenotes SAAM on both scale 2 and scale 3. CFDM denotes\nour Contrast and Fusion Decoder Module.\nformance of mirror detection, especially in complex scenes.\nAblation Study\nBenefits of Dual-Path Structure. To better analyze the\nbenefits of our dual-path structure, we conduct two exper-\niments: One is a pure Swin Transformer decoded by Uper-\nNet (Xiao et al. 2018) (1st row); the other is a dual-path Swin\nTransformer, where features are trained and supervised sep-\narately in two paths (2nd row). Results in the first two rows\nshow that, with extra features and supervision, the second\nmethod has no clear advantage when compared against the\nfirst one. That is to say, we cannot simply attribute the im-\nprovement of our method to the extra features we extract. Al-\nbeit we introduce the dual-path structure to enhance the sym-\nmetry semantics, the extra features are more like a repeated\ncomputation of the original ones if there are no appropriate\nfusing and matching mechanisms for the two paths.\nEffect of SAAM. To evaluate the effect of our attention\nmodule, we conduct another two experiments: One is a dual-\npath Swin-S with a SAAM in the highest level (3-rd row),\nand the other is the same structure, but with SAAMs in the\nhighest two levels (4-th row). Comparing the third row with\nthe second row, we discover that “Dual-Path + SAAM” gets\nbetter results in all the three metrics, which is reasonable\nas our SAAM models symmetry relationships in high-level\nfeatures. However,Fβ in the fourth row drops back to 0.903,\nindicating that directly applying SAAM to features in lower\nlevels may not work well. We further visualize the attention\nmap in SAAM. In Fig. 5(c), the mirror region (green con-\ntour in (b)) of the attention map focuses on the mirror itself.\nWhile in Fig. 5(d), the highest attention signal of the power\nbank region inside the mirror (red contour in (b)) is located\non the real power bank in the image. This observation sup-\nports that SAAM is able to model loose symmetry relations.\nEffect of CFDM. In the fifth row, we conduct an experiment\nbased on the second row, replacing the UperNet decoder\nwith our CFDM. Comparing results of the two rows, ”Dual-\nPath + CFDM” have a gain of 2.39%, 1.50%, and 0.5% in\nIoU , Fβ, and MAE , respectively. The improvement proves\nthat our decoder module can properly fuse features in the\ntwo paths, and is more suitable for the mirror detection task.\nCombination of SAAM and CFDM. To explore the best\nway to combine our SAAM and CFDM, we conduct two\nexperiments: one is a dual-path Swin Transformer with a\nSAAM in the highest level and CFDMs as the decoder (6th\nrow), and the other is our final network SATNet, which has\nSAAMs in the highest two levels (last row). Analyzing the\nlast three rows, we conclude that applying SAAM before\nCFDM is effective as the three evaluation metrics have pro-\ngressively improved to 85.41%. 0.922 and 0.033. On the\nother hand, comparing the network in the fourth row with\nSATNet, the improvement from “Dual-Path + SAAMs” to\nSATNet is even larger, which means our CFDM contributes\nto the fusion of dual-path features, especially for symmetry\nsemantics in high levels.\nVisualization results for the ablation study. To further\nanalyze the effectiveness of each component, we visualize\nthe prediction results of Swin-S, Dual-Path + SAAM, Dual-\nPath + CFDM, and SATNet in Fig. 6. Swin-S can provide\nthe approximate location of the mirror but is not sensitive\nto symmetry relationships, which demonstrates that current\nbaselines can hardly model loose symmetry relationships.\nEquipped with our attention module SAAM, the network\n940\nImage Detph DANet JL-DCF BBSNet VST PDNet SANet VCNet Ours GT\nFigure 4: Visualization results on RGB-D dataset. In the first row, changes in depth can easily affect the judgement of RGB-D\nmethods. The second row contains a pair of symmetric objects inside and outside mirrors. The third row represents mirrors that\ncan hardly be recognized. And the last row is a scene including both glasses and mirrors.\n(a) (b) (c) (d)\nFigure 5: Visualization of attention maps in SAAM. (a)-(d)\ndenote image, region of interest, attention of mirrors, and\nattention of objects in mirrors. While attention of the mirror\nregion focuses on the mirror itself, attention of the power\nbank in mirrors lies in the corresponding real object.\nImage Baseline +SAAM +CFDM SATNet GT\nFigure 6: Visualization results for the ablation study. In this\nexample, our baseline Swin-S cannot perceive the symmetry\nrelationship. The network embedded with SAAM does not\noutline a precise boundary. And when adding CFDM, the\nnetwork is still confused about the symmetry relationship.\nOnly SATNet can correctly detect the mirror region.\ncan exclude the real-world object which shades the mirror\nfrom the mirror region, showing the ability of perceiving\nsymmetry relationships. However, its prediction map is not\nprecise enough, especially near the boundary of mirrors. In\ncomparison to our SAAM, our decoder module CFDM re-\nfines mirror boundaries well, but it wrongly excludes the\nsymmetry area in the mirror region. Analogous to Swin-S, it\ncannot handle symmetry relationships well. Only with both\ntwo modules, SATNet marks the mirror region correctly. The\nvisualization result is basically consistent with the corre-\nsponding effects of the components we expect.\nInput Size. Following Swin Transformer (Liu et al. 2021b),\nwe train SATNet with an input image of512×512. Nonethe-\nless, previous networks usually adopt smaller input sizes,\ne.g., 384 × 384. To show that the superiority of our SATNet\ncannot be simply ascribed to larger input image size, we fur-\nther train another two SATNet models respectively for input\nimages with the sizes of384×384 on MSD. Table 4 lists the\nquantitative results of our SATNet and the competing meth-\nods. From the table, one can see that: (i) increasing input\nimage size is beneficial to the performance of our SATNet;\n(ii) with the same input image size, our SATNet consistently\noutperforms the competing methods.\nMethod MACs(G) Para.(M) IoU ↑ Fβ ↑ MAE ↓\nMirrorNet 77.7 121.77 78.88 0.856 0.066\nPMDNet 101.5 147.66 81.54 0.892 0.047\nOurs-384 84.1 111.34 82.56 0.911 0.041\nOurs-512 147.26 111.34 85.41 0.922 0.033\nTable 4: Comparison of different mirror detection networks\non MSD dataset. We report the results of our SATNet with\nthe input image sizes of 384 × 384 and 512 × 512.\nConclusion\nIn this paper, we proposed a dual-path Symmetry-Aware\nTransformer-based mirror detection network (SATNet) for\nbetter mirror detection. We presented a new perspective on\ndetecting mirrors by leveraging loose symmetry relation-\nships. Then, we suggested a novel dual-path network, in-\ntroducing a transformer pipeline to enhance the ability of\nlong-range dependencies understanding for mirror detec-\ntion. Furthermore, we proposed the Symmetry-Aware Atten-\ntion Module (SAAM) to aggregate better feature representa-\ntion of symmetry relations, while exploiting Contrast and\nFusion Decoder Module (CFDM) to generate refined pre-\ndiction maps progressively. Experimental results on multiple\ndatasets demonstrate the benefit of loose symmetry relation-\nships in mirror detection. Our network can effectively model\nsuch relationships and greatly improve the performance of\nmirror detection in comparison to state-of-the-arts.\n941\nAcknowledgements\nThis work was supported by the Major Key Project of\nPeng Cheng Laboratory (PCL2021A12), the National Natu-\nral Science Foundation of China (NSFC) under Grants No.s\nU19A2073, and two SRG grants from City University of\nHong Kong (Ref: 7005674 and 7005843).\nReferences\nCornelius, H.; and Loy, G. 2006. Detecting bilateral symme-\ntry in perspective. In 2006 Conference on Computer Vision\nand Pattern Recognition Workshop (CVPRW’06), 191–191.\nIEEE.\nDe Boer, P.-T.; Kroese, D. P.; Mannor, S.; and Rubinstein,\nR. Y . 2005. A tutorial on the cross-entropy method. Annals\nof operations research, 134(1): 19–67.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDeng, Z.; Hu, X.; Zhu, L.; Xu, X.; Qin, J.; Han, G.; and\nHeng, P.-A. 2018. R3net: Recurrent residual refinement net-\nwork for saliency detection. In Proceedings of the 27th In-\nternational Joint Conference on Artificial Intelligence, 684–\n690. AAAI Press.\nDing, H.; Jiang, X.; Shuai, B.; Liu, A. Q.; and Wang, G.\n2018. Context Contrasted Feature and Gated Multi-Scale\nAggregation for Scene Segmentation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR).\nFan, D.-P.; Lin, Z.; Zhang, Z.; Zhu, M.; and Cheng, M.-M.\n2020a. Rethinking RGB-D salient object detection: Models,\ndata sets, and large-scale benchmarks. IEEE Transactions\non neural networks and learning systems, 32(5): 2075–2089.\nFan, D.-P.; Zhai, Y .; Borji, A.; Yang, J.; and Shao, L. 2020b.\nBBS-Net: RGB-D salient object detection with a bifurcated\nbackbone strategy network. In European Conference on\nComputer Vision, 275–292. Springer.\nFu, K.; Fan, D.-P.; Ji, G.-P.; and Zhao, Q. 2020. JL-DCF:\nJoint Learning and Densely-Cooperative Fusion Framework\nfor RGB-D Salient Object Detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR).\nFunk, C.; and Liu, Y . 2017. Beyond planar symmetry: Mod-\neling human perception of reflection and rotation symme-\ntries in the wild. In Proceedings of the IEEE international\nconference on computer vision, 793–803.\nGuan, H.; Lin, J.; and Lau, R. W. 2022. Learning Seman-\ntic Associations for Mirror Detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 5941–5950.\nLin, J.; Wang, G.; and Lau, R. W. 2020. Progressive mirror\ndetection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 3697–3705.\nLiu, N.; Han, J.; and Yang, M.-H. 2018. PiCANet: Learning\nPixel-Wise Contextual Attention for Saliency Detection. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR).\nLiu, N.; Zhang, N.; and Han, J. 2020. Learning Selective\nSelf-Mutual Attention for RGB-D Saliency Detection. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR).\nLiu, N.; Zhang, N.; Wan, K.; Shao, L.; and Han, J.\n2021a. Visual Saliency Transformer. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 4722–4732.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021b. Swin transformer: Hierarchical\nvision transformer using shifted windows. arXiv preprint\narXiv:2103.14030.\nLowe, D. G. 2004. Distinctive image features from scale-\ninvariant keypoints. International journal of computer vi-\nsion, 60(2): 91–110.\nLoy, G.; and Eklundh, J.-O. 2006. Detecting symmetry and\nsymmetric constellations of features. In European Confer-\nence on Computer Vision, 508–521. Springer.\nMei, H.; Dong, B.; Dong, W.; Peers, P.; Yang, X.; Zhang,\nQ.; and Wei, X. 2021. Depth-aware mirror segmentation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 3044–3053.\nPang, Y .; Zhang, L.; Zhao, X.; and Lu, H. 2020a. Hierar-\nchical dynamic filtering network for rgb-d salient object de-\ntection. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceed-\nings, Part XXV 16, 235–252. Springer.\nPang, Y .; Zhao, X.; Zhang, L.; and Lu, H. 2020b. Multi-\nScale Interactive Network for Salient Object Detection. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR).\nPang, Y .; Zhao, X.; Zhang, L.; and Lu, H. 2020c. Multi-scale\ninteractive network for salient object detection. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 9413–9422.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in neural information pro-\ncessing systems, 32: 8026–8037.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Con-\nvolutional networks for biomedical image segmentation. In\nInternational Conference on Medical image computing and\ncomputer-assisted intervention, 234–241. Springer.\nSeo, A.; Shim, W.; and Cho, M. 2021. Learning To Discover\nReflection Symmetry via Polar Matching Convolution. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 1285–1294.\nSong, H.; Liu, Z.; Du, H.; Sun, G.; Le Meur, O.; and Ren, T.\n2017. Depth-aware salient object detection and segmenta-\ntion via multiscale discriminative saliency fusion and boot-\nstrap learning. IEEE Transactions on Image Processing,\n26(9): 4204–4216.\nTan, X.; Lin, J.; Xu, K.; Chen, P.; Ma, L.; and Lau, R. W.\n2022. Mirror Detection With the Visual Chirality Cue.IEEE\nTransactions on Pattern Analysis and Machine Intelligence.\n942\nTsogkas, S.; and Kokkinos, I. 2012. Learning-based sym-\nmetry detection in natural images. In European Conference\non Computer Vision, 41–54. Springer.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWang, Q.; Wu, B.; Zhu, P.; Li, P.; Zuo, W.; and Hu, Q. 2020.\nECA-Net: Efficient Channel Attention for Deep Convolu-\ntional Neural Networks. In IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR).\nWang, T.; Borji, A.; Zhang, L.; Zhang, P.; and Lu, H. 2017.\nA Stagewise Refinement Model for Detecting Salient Ob-\njects in Images. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV).\nWei, J.; Wang, S.; Wu, Z.; Su, C.; Huang, Q.; and Tian,\nQ. 2020. Label Decoupling Framework for Salient Object\nDetection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR).\nWu, Z.; Su, L.; and Huang, Q. 2019. Cascaded partial de-\ncoder for fast and accurate salient object detection. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 3907–3916.\nXiao, T.; Liu, Y .; Zhou, B.; Jiang, Y .; and Sun, J. 2018. Uni-\nfied perceptual parsing for scene understanding. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), 418–434.\nYang, X.; Mei, H.; Xu, K.; Wei, X.; Yin, B.; and Lau, R. W.\n2019. Where is my mirror? InProceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 8809–8818.\nZendel, O.; Honauer, K.; Murschitz, M.; Humenberger, M.;\nand Fernandez Dominguez, G. 2017. Analyzing computer\nvision data-the good, the bad and the ugly. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 1980–1990.\nZhao, X.; Zhang, L.; Pang, Y .; Lu, H.; and Zhang, L. 2020.\nA single stream network for robust and real-time RGB-D\nsalient object detection. In European Conference on Com-\nputer Vision, 646–662. Springer.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; and Zhang, L. 2021.\nRethinking Semantic Segmentation From a Sequence-to-\nSequence Perspective With Transformers. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 6881–6890.\n943"
}