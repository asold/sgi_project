{
  "title": "IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages",
  "url": "https://openalex.org/W3099919888",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2998891912",
      "name": "Divyanshu Kakwani",
      "affiliations": [
        "Microsoft (India)",
        "Indian Institute of Technology Madras",
        "Robert Bosch (India)"
      ]
    },
    {
      "id": "https://openalex.org/A392661354",
      "name": "Anoop Kunchukuttan",
      "affiliations": [
        "Robert Bosch (India)",
        "Indian Institute of Technology Madras",
        "Microsoft (India)"
      ]
    },
    {
      "id": "https://openalex.org/A3022973491",
      "name": "Satish Golla",
      "affiliations": [
        "Microsoft (India)",
        "Indian Institute of Technology Madras",
        "Robert Bosch (India)"
      ]
    },
    {
      "id": "https://openalex.org/A4287895109",
      "name": "Gokul N.c.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2343016281",
      "name": "Avik Bhattacharyya",
      "affiliations": [
        "Robert Bosch (India)",
        "Indian Institute of Technology Madras",
        "Microsoft (India)"
      ]
    },
    {
      "id": "https://openalex.org/A284849352",
      "name": "Mitesh M. Khapra",
      "affiliations": [
        "Indian Institute of Technology Madras",
        "Robert Bosch (India)",
        "Microsoft (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2242419757",
      "name": "Pratyush Kumar",
      "affiliations": [
        "Indian Institute of Technology Madras",
        "Microsoft (India)",
        "Robert Bosch (India)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2806157494",
    "https://openalex.org/W3031586918",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2986039727",
    "https://openalex.org/W2790325757",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2586756071",
    "https://openalex.org/W2963118869",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2316579313",
    "https://openalex.org/W3032532958",
    "https://openalex.org/W3028807187",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2964583233",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W95183648",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W2963216505",
    "https://openalex.org/W4287760320",
    "https://openalex.org/W4299579390",
    "https://openalex.org/W2251139694",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W2963667932",
    "https://openalex.org/W2250600805",
    "https://openalex.org/W2888740011",
    "https://openalex.org/W1523296404",
    "https://openalex.org/W2481265265",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4297801177",
    "https://openalex.org/W2120101509",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W4287827771",
    "https://openalex.org/W2971324494",
    "https://openalex.org/W2987270981",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2250653840",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2576516426",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W2739853973",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3032816972",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2252106004"
  ],
  "abstract": "In this paper, we introduce NLP resources for 11 major Indian languages from two major language families. These resources include: (a) large-scale sentence-level monolingual corpora, (b) pre-trained word embeddings, (c) pre-trained language models, and (d) multiple NLU evaluation datasets (IndicGLUE benchmark). The monolingual corpora contains a total of 8.8 billion tokens across all 11 languages and Indian English, primarily sourced from news crawls. The word embeddings are based on FastText, hence suitable for handling morphological complexity of Indian languages. The pre-trained language models are based on the compact ALBERT model. Lastly, we compile the (IndicGLUE benchmark for Indian language NLU. To this end, we create datasets for the following tasks: Article Genre Classification, Headline Prediction, Wikipedia Section-Title Prediction, Cloze-style Multiple choice QA, Winograd NLI and COPA. We also include publicly available datasets for some Indic languages for tasks like Named Entity Recognition, Cross-lingual Sentence Retrieval, Paraphrase detection, etc. Our embeddings are competitive or better than existing pre-trained embeddings on multiple tasks. We hope that the availability of the dataset will accelerate Indic NLP research which has the potential to impact more than a billion people. It can also help the community in evaluating advances in NLP over a more diverse pool of languages. The data and models are available at https://indicnlp.ai4bharat.org.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4948–4961\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n4948\nIndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and\nPre-trained Multilingual Language Models for Indian Languages\nDivyanshu Kakwani1, Anoop Kunchukuttan2∗, Satish Golla3∗, Gokul N.C.4,\nAvik Bhattacharyya5, Mitesh M. Khapra6, Pratyush Kumar7\nRobert Bosch Centre for Data Science and AI, IIT Madras1,6,7, Microsoft India2, AI4Bharat3,4,5\n{divk,miteshk,pratyush}@cse.iitm.ac.in1,6,7,\nankunchu@microsoft.com2, gokulnc@ai4bharat.org4,\n{gsatishkumaryadav,avikbhattacharyya.2k}@gmail.com3,5\nAbstract\nIn this paper, we introduce NLP resources for\n11 major Indian languages from two major\nlanguage families. These resources include:\n(a) large-scale sentence-level monolingual cor-\npora, (b) pre-trained word embeddings, (c)\npre-trained language models, and (d) multiple\nNLU evaluation datasets (IndicGLUE bench-\nmark). The monolingual corpora contains a\ntotal of 8.8 billion tokens across all 11 lan-\nguages and Indian English, primarily sourced\nfrom news crawls. The word embeddings\nare based onFastText, hence suitable for han-\ndling morphological complexity of Indian lan-\nguages. The pre-trained language models are\nbased on the compact ALBERT model. Lastly,\nwe compile theIndicGLUE benchmark for In-\ndian language NLU. To this end, we create\ndatasets for the following tasks: Article Genre\nClassification, Headline Prediction, Wikipedia\nSection-Title Prediction, Cloze-style Multiple\nchoice QA, Winograd NLI and COPA. We also\ninclude publicly available datasets for some\nIndic languages for tasks like Named Entity\nRecognition, Cross-lingual Sentence Retrieval,\nParaphrase detection,etc. Our embeddings are\ncompetitive or better than existing pre-trained\nembeddings on multiple tasks. We hope that\nthe availability of the dataset will accelerate\nIndic NLP research which has the potential to\nimpact more than a billion people. It can also\nhelp the community in evaluating advances in\nNLP over a more diverse pool of languages.\nThe data and models are available athttps:\n//indicnlp.ai4bharat.org.\n1 Introduction\nDistributional representations are the corner stone\nof modern NLP, which have led to significant\nadvances in many NLP tasks like text classifi-\ncation, NER, sentiment analysis, MT, QA, NLI,\netc. Particularly, word embeddings (Mikolov\n∗V olunteer effort for the AI4Bharat project\net al., 2013b), contextualized word embeddings\n(Peters et al., 2018), and language models (De-\nvlin et al., 2019) can model syntactic/semantic rela-\ntions between words and reduce feature engineer-\ning. These pre-trained models are useful for ini-\ntialization and/or transfer learning for NLP tasks.\nThey are also useful for learning multilingual em-\nbeddings which enable cross-lingual transfer. Pre-\ntrained models are typically learned from large, di-\nverse monolingual corpora. The quality of embed-\ndings is impacted by the size of the monolingual\ncorpora (Mikolov et al., 2013a; Bojanowski et al.,\n2017), a resource not widely available for many\nmajor languages.\nIn particular, Indic languages, widely spoken by\nmore than a billion speakers, lack large, publicly\navailable monolingual corpora. They include 8\nout of top 20 most spoken languages and∼30 lan-\nguages with more than a million speakers. There is\nalso a growing population of users consuming In-\ndian language content (print, digital, government\nand businesses). Further, Indic languages are very\ndiverse, spanning 4 major language families. The\nIndo-Aryan and Dravidian languages are spoken\nby 96% of the population in India. The other\nfamilies are diverse, but the speaker population\nis relatively small. Almost all Indian languages\nhave SOV word order and are morphologically\nrich. The language families have also interacted\nover a long period of time leading to significant\nconvergence in linguistic features; hence, the In-\ndian subcontinent is referred to as alinguistic area\n(Emeneau, 1956). Indic languages are thus of great\ninterest and importance for NLP research.\nUnfortunately, the progress on Indic NLP has\nbeen constrained by the unavailability of large\nscale monolingual corpora and evaluation bench-\nmarks. The former allows the development of pre-\ntrained language models and deep contextualised\nword embeddings which have become drivers of\n4949\nmodern NLP. The latter allows systematic evalua-\ntion across a wide variety of tasks to check the ef-\nficacy of new models. With the hope of accelerat-\ning Indic NLP research, we address the creation of\n(i) large, general-domain monolingual corpora for\nmultiple Indian languages, (ii) word embeddings\nand multilingual language models trained on this\ncorpora, and (iii) an evaluation benchmark com-\nprising of various NLU tasks.\nOur monolingual corpora, collectively referred\nto as IndicCorp, contains a total of 8.8 billion to-\nkens across 11 major Indian languages and English.\nThe articles in IndicCorp are primarily sourced\nfrom news crawls. UsingIndicCorp, we first train\nand evaluate word embeddings for each of the 11\nlanguages. Given the morphological richness of\nIndian languages we train FastText word embed-\ndings which are known to be more effective for\nsuch languages. To evaluate these embeddings we\ncurate a benchmark comprising of word similarity\nand analogy tasks (Akhtar et al., 2017; Grave et al.,\n2018), text classification tasks, sentence classifica-\ntion tasks (Akhtar et al., 2016; Mukku and Mamidi,\n2017), and bilingual lexicon induction tasks. On\nmost tasks, the word embeddings trained on our\nIndicCorp outperform similar embeddings trained\non existing corpora for Indian languages.\nNext, we train multilingual language models for\nthese 11 languages using the ALBERT model (Lan\net al., 2020). We chose ALBERT as the base\nmodel as it is very compact and hence easier to\nuse in downstream tasks. To evaluate these pre-\ntrained language models, we create an NLU bench-\nmark comprising of the following tasks: article\ngenre classification, headline prediction, named\nentity recognition, Wikipedia section-title predic-\ntion, cloze-style multiple choice QA,natural lan-\nguage inference, paraphrase detection, sentiment\nanalysis, discourse mode classification, and cross-\nlingual sentence retrieval. We collectively refer\nto this benchmark asIndicGLUE and it is a col-\nlection of (i) existing Indian language datasets for\nsome tasks, (ii) manual translations of some En-\nglish datasets into Indian languages done as a part\nof this work, and (iii) new datasets that were cre-\nated semi-automatically for all major Indian lan-\nguages as a part of this work. These new datasets\nwere created using external metadata (such as web-\nsite/Wikipedia structure) resulting in more com-\nplex NLU tasks.Across all these tasks, we show\nthat our embeddings are competitive or better than\nexisting pre-trained multilingual embeddings such\nas mBERT (Devlin et al., 2019) and XLM-R (Con-\nneau et al., 2020). We hope that these embeddings\nand evaluations benchmarks will not only be use-\nful in driving NLP research on Indic languages, but\nwill also help in evaluating advances in NLP over\na more diverse set of languages.\nIn summary, this paper introduces IndicNLP-\nSuite containing the following resources for Indic\nNLP which will be made publicly available:\n• IndicCorp: Large sentence-level monolingual\ncorpora for 11 languages from two language fam-\nilies (Indo-Aryan branch and Dravidian) and In-\ndian English with an average 9-fold increase in size\nover OSCAR.\n• IndicGLUE: An evaluation benchmark contain-\ning a variety of NLU tasks.\n• IndicFT and IndicBERT: FastText-based word\nembeddings (11 languages) and ALBERT-based\nlanguage models (12 languages) trained onIndic-\nCorp. The IndicBERT embeddings are multilin-\ngual (includes Indian English sources).\n2 Related Work\nText Corpora.Few organized sources of monolin-\ngual corpora exist for most Indian languages. The\nEMILLE/CIIL corpus (McEnery et al., 2000) was\nan early effort to build corpora for South Asian\nlanguages, spanning 14 languages with a total of\n92 million words.Wikipedia for Indian languages\nis small (the largest one, Hindi, has just 40 mil-\nlion words). The Leipzig corpus (Goldhahn et al.,\n2012) contains small collections of upto 1 million\nsentences for news and web crawls (average 300K\nsentences). In addition, there are some language\nspecific corpora for Hindi and Urdu (Bojar et al.,\n2014; Jawaid et al., 2014). In particular, the Hind-\nMonoCorp (Bojar et al., 2014) is one of the few\nlarger Indian language collections (787M tokens).\nThe CommonCrawl 1 project crawls webpages\nin many languages by sampling various websites.\nOur analysis of a processed crawl for the years\n2013-2016 (Buck et al., 2014) for Indian languages\nrevealed that most Indian languages, with the ex-\nception of Hindi, Tamil and Malayalam, have few\ngood sentences (≥10 words) - in the order of\naround 50 million words. The OSCAR project\n(Ortiz Suarez et al., 2019), a recent processing of\nCommonCrawl, also contains much less data for\nmost Indian languages than our crawls. The CC-\n1https://commoncrawl.org\n4950\nNet (Wenzek et al., 2019) and C4 (Raffel et al.,\n2019) projects also provide tools to process com-\nmon crawl, but the extracted corpora are not pro-\nvided and require a large amount of processing\npower. Our monolingual corpora is about 4 times\nlarger than the corresponding OSCAR corpus and\ntwo times larger than the corresponding CC-100\ncorpus (Conneau et al., 2020).\nWord Embeddings.Word embeddings have been\ntrained for many Indian languages using limited\ncorpora. The Polyglot (Al-Rfou et al., 2013) and\nFastText (Bojanowski et al., 2017) projects pro-\nvide embeddings trained on Wikipedia. FastText\nalso provides embeddings trained on Wikipedia\n+ CommonCrawl corpora. We show that on\nmost evaluation tasksIndicFT outperforms exist-\ning FastText based embeddings.\nPretrained Transformers.Pre-trained transform-\ners serve as general language understanding mod-\nels that can be used in a wide variety of down-\nstream NLP tasks (Radford et al., 2019). Sev-\neral transformer-based language models such as\nGPT (Radford, 2018), BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), ALBERT (Lan et al.,\n2020), etc. have been proposed. All these models\nrequire large amounts of monolingual corpora for\ntraining. For Indic languages, two such multilin-\ngual models are available: XLM-R (Conneau et al.,\n2020) and multilingual BERT (Devlin et al., 2019).\nHowever, they are trained across ~100 languages\nand smaller Indic language corpora.\nNLU Benchmarks. Benchmarks such as GLUE\n(Wang et al., 2018), SuperGLUE (Wang et al.,\n2019), CLUE (Chinese) (Xu et al., 2020), and\nFLUE (French) (Le et al., 2020) are important for\ntracking the efficacy of NLP models across lan-\nguages. Such a benchmark is missing for Indic\nlanguages and the goal of this work is to fill this\nvoid. Datasets are available for some tasks for\na few languages. The following are some of the\nprominent publicly available datasets2: word sim-\nilarity (Akhtar et al., 2017), word analogy (Grave\net al., 2018), text classification, sentiment analysis\n(Akhtar et al., 2016; Mukku and Mamidi, 2017),\nparaphrase detection (Anand Kumar et al., 2016),\nQA (Clark et al., 2020; Gupta et al., 2018), dis-\ncourse mode classification (Dhanwal et al., 2020),\netc.. We also create datasets for some tasks, most\nof which span all major Indian languages. We bun-\n2A comprehensive list of resources for\nIndian language NLP can be found here:\nhttps://github.com/AI4Bharat/indicnlp_catalog\nLanguage #S #T #V I/O\nPunjabi (pa) 29.2 773 3.0 22\nHindi (hi) 63.1 1,860 6.5 2\nBengali (bn) 39.9 836 6.6 2\nOdia (or) 6.94 107 1.4 9\nAssamese (as) 1.39 32.6 0.8 8\nGujarati (gu) 41.1 719 5.7 14\nMarathi (mr) 34.0 551 5.8 7\nKannada (kn) 53.3 713 11.9 14\nTelugu (te) 47.9 674 9.4 8\nMalayalam (ml) 50.2 721 17.7 8\nTamil (ta) 31.5 582 11.4 2\nEnglish (en) 54.3 1,220 4.5\nTotal 452.8 8789 84.7\nTable 1:IndicCorp de-duplicated monolingual corpora\nstatistics: number of sentences (S), tokens (T), types\n(V) in millions, the ratio ofIndicCorp size to OSCAR\ncorpus size (I/O).\ndle together the existing datasets and our newly cre-\nated datasets to create theIndicGLUE benchmark.\n3 IndicCorp: Indian Language Corpora\nIn this section, we describe the creation of our\nmonolingual corpora.\nData sources. Our goal was the collection of cor-\npora that reflect contemporary use of Indic lan-\nguages and cover a wide range of topics. Hence,\nwe focus primarily on crawling news articles, mag-\nazines and blogposts. We source our data from\npopular Indian language news websites. We dis-\ncover most of our sources through online newspa-\nper directories (e.g., w3newspaper) and automated\nweb searches using hand-picked terms in various\nlanguages.\nWe analyzed whether we could augment our\ncrawls with data from other smaller sources like\nLeipzig corpus (Goldhahn et al., 2012), WMT\nNewsCrawl, WMT CommonCrawl (Buck et al.,\n2014), HindEnCorp (Hindi) (Bojar et al., 2014),\netc. Amongst these we chose to augment our\ndataset with only the CommonCrawl data from the\nOSCAR corpus (Ortiz Suárez et al., 2019).\nArticle Extraction. For many news websites, we\nused BoilerPipe3, a tool to automatically extract\nthe main article content for structured pages with-\nout any site-specific customizations (Kohlschütter\net al., 2010). This approach works well for most\nof the Indian language news websites. In some\ncases, we wrote custom extractors for each website\n3https://github.com/kohlschutter/boilerpipe\n4951\nusing BeautifulSoup4, a Python library for parsing\nHTML/XML documents. After content extraction,\nwe applied filters on content length, script,etc., to\nselect good quality articles.\nText Processing.First, we canonicalize the repre-\nsentation of Indic language text in order to handle\nmultiple Unicode representations of certain char-\nacters. Next, we split the article into sentences\nand tokenize the sentences. These steps take into\naccount Indic punctuations and sentence delim-\niters. Heuristics avoid creating sentences for ini-\ntials (P. G. Wodehouse) and common Indian titles\n(Shri., equivalent to Mr. in English) which are fol-\nlowed by a period. We use theIndic NLP Library5\n(Kunchukuttan, 2020) for processing.\nThe final corpus for a language is created after\ncombining our crawls with OSCAR corpus6 and\nde-duplicating and shuffling sentences. We used\nthe Murmurhash algorithm (mmh3 Python library\nwith a 128-bit unsigned hash) for de-duplication.\nDue to copyright reasons, we only release the final\nshuffled corpus described below.\nDataset Statistics. Table 1 shows statistics of the\nde-duplicated monolingual datasets for each lan-\nguage. Hindi and Indian English are the largest\ncollections, while Odia and Assamese have the\nsmallest collection. All other languages contain\nbetween 500-1000 million tokens. OSCAR is an\nimportant contributor to our corpus and accounts\nfor nearly (23%) of our corpus by the number of\nsentences. The rest of the data originated from our\ncrawls. As evident from the last column of Table1,\nfor 8 languages the number of tokens in our corpus\nis at least 7 times that in OSCAR. For the remain-\ning 3 languages it is twice that of OSCAR.\n4 IndicGLUE: Multilingual NLU\nBenchmark\nWe now introduceIndicGLUE, the Indic General\nLanguage Understanding Evaluation Benchmark,\nwhich is a collection of various NLP tasks as de-\nscribed below. The goal is to provide an evaluation\nbenchmark for natural language understanding ca-\npabilities of NLP models on diverse tasks and mul-\ntiple Indian languages. As discussed earlier, very\nfew public NLP datasets are available for all Indian\nlanguages. Hence, we adopted a two-pronged ap-\nproach to construct this benchmark. One, we use\n4https://www.crummy.com/software/BeautifulSoup\n5https://github.com/anoopkunchukuttan/indic_nlp_library\n6https://oscar-corpus.com/\nexisting datasets that address some tasks. How-\never, such datasets are available for just 4-5 In-\ndian languages. We also manually translated some\nEnglish datasets into a few Indian languages. We\nsummarize statistics of these datasets in Appendix\nA. Two, we create new datasets that span all ma-\njor Indian languages. These datasets are curated\nsemi-automatically using external metadata like\nwebsite/Wikipedia structure and are designed to\npresent reasonably complex NLU tasks. Table2\nsummarizes the sizes of the respective datasets.\nFurther details (such as the min, max, average num-\nber of words per training instance) can be found in\nAppendix C. Standard train and test splits for\nall datasets are publicly available on the web-\nsite for reproducibility. For publicly available\ndatasets, we used the original split if provided.\nNews Category Classification.The task is to pre-\ndict the genre/topic of a given news article or news\nheadline. We create news article category datasets\nusing IndicCorp for 9 languages. The categories\nare determined from URL components. We chose\ngeneric categories which are likely to be consistent\nacross websites (e.g., entertainment, sports, busi-\nness, lifestyle, technology, politics, crime) . See\nAppendix B for details.\nHeadline Prediction Task.The task is to predict\nthe correct headline for a news article from a given\nlist of four candidate headlines (3 incorrect, 1 cor-\nrect). We generate the dataset from our news ar-\nticle crawls which contain articles and their head-\nlines. We ensure that the three incorrect candidates\nare not completely unrelated to the given article. In\nparticular, while choosing incorrect candidates, we\nconsidered only those articles that had a sizeable\noverlap of entities with the original article.\nWikipedia Section-title Prediction. The task is\nto predict the correct title for a Wikipedia section\nfrom a given list of four candidate titles (3 in-\ncorrect, 1 correct). We use the open-source tool\nWikiExtractor to extract sections and their titles\nfrom Wikipedia. To make the task challenging, we\nchoose the 3 incorrect candidates for a given sec-\ntion, only from the titles of other sections in the\nsame article as the given section.\nCloze-style Multiple-choice QA. Given a text\nwith an entity randomly masked, the task is to pre-\ndict that masked entity from a list of 4 candidate en-\ntities (3 incorrect, 1 correct). The text is obtained\nfrom Wikipedia articles and the entities in the text\nare identified using Wikidata. We choose the 3 in-\n4952\npa hi bn or as gu mr kn te ml ta total\nNews Category Classification\n3,120 - 14,000 30,000 - 2,040 4,770 30,000 24,000 6,000 11,700 125,630\nHeadline Prediction\n100,000 100,000 68,350 100,000 49,751 100,000 67,571 56,457 63,415 100,000 74,767 880,311\nWikipedia Section-Title Prediction\n10,966 55,087 59,475 5,019 6,251 12,506 13,058 44,224 100,000 34,409 61,175 402,170\nCloze-style QA\n5,664 35,135 38,845 1,975 2,942 22,856 11,370 13,656 41,338 26,531 38,585 238,897\nNamed Entity Recognition\n9,462 69,431 109,508 8,687 6,295 39,708 108,579 28,854 81,627 138,888 186,423 787,462\nCross-lingual Sentence Retrieval(#English to Indian language parallel sentences)\n- 5,169 5,522 752 - 6,463 5,760 - 5,049 4,886 5,637 39,238\nTable 2:IndicGLUE Datasets’ Statistics. The first four datasets have been created as part of this project.\ncorrect candidates from entities that occur in the\nsame article and have the same type as the correct\nentity. The type of an entity is taken from Wikidata.\nThis task is similar to the one proposed byPetroni\net al.(2019) for English, and aims to check if lan-\nguage models can be used as knowledge bases.\nNamed Entity Recognition.We use the WikiAnn\nNER dataset7 (Pan et al., 2017) which contains\nNER data for 282 languages. This dataset is cre-\nated from Wikipedia by utilizing cross language\nlinks to propagate English named entity labels to\nother languages. We consider the following coarse-\ngrained labels in this dataset: Person (PER), Or-\nganisation (ORG) and Location (LOC).\nCross-lingual Sentence Retrieval. Given a sen-\ntence in English, the task is to retrieve its transla-\ntion from a set of candidate sentences in an Indian\nlanguage. We use theCVIT-Mann Ki Baatdataset8\n(Siripragrada et al., 2020) for this task.\nWinograd NLI (WNLI) . The WNLI task\n(Levesque et al. , 2011) is part of the GLUE\nbenchmark. Each example in the dataset consists\nof a pair of sentences where the second sentence\nis constructed from the first sentence by replacing\nan ambiguous pronoun with a possible referent\nwithin the sentence. The task is to predict if\nthe second sentence is entailed by the original\nsentence. We manually translated this dataset to\n3 Indic languages (hi, mr, gu) with the help of\nskilled bilingual speakers. The annotators were\npaid 3 cents per word and the translations were\nthen verified by an expert bilingual speaker.\n7https://elisa-ie.github.io/wikiann/\n8http://preon.iiit.ac.in/ jerin/bhasha/\nCOPA. The Choice Of Plausible Alternatives\n(Gordon et al., 2011) task evaluates open-domain\ncommonsense causal reasoning. It consists of a\nlarge set of 2-choice questions, formulated as a\npremise and two alternatives written as sentences.\nThe task is to select the alternative that is more\nplausibly the cause (or effect) of the situation de-\nscribed by the premise. As with WNLI, we trans-\nlated the dataset into 3 Indic languages (hi, mr, gu).\nParaphrase Detection. We use the Amritha\nparaphrase dataset comprsing 4 Indic languages\n(hi,pa,ta,ml) (Anand Kumar et al., 2016). We eval-\nuate on two subtasks:Subtask 1-Given a pair of\nsentences from news paper domain, the task is to\nclassify them as paraphrases (P) or not paraphrases\n(NP). Subtask 2-Given two sentences from news\npaper domain, the task is to identify whether they\nare completely equivalent (E) or roughly equiva-\nlent (RE) or not equivalent (NE). This task is sim-\nilar to subtask 1, but the main difference is the use\nof three classes instead of two.\nDiscourse Mode Classification. Given a sen-\ntence, the task is to classify it into one of the follow-\ning discourse categories: argumentative, descrip-\ntive, dialogic, informative, narrative. We use the\nMIDAS Hindi Discourse Analysis dataset (Dhan-\nwal et al., 2020) for this task.\nSentiment Analysis. We used the following\npublicly available datasets: (a) IIT-Patna Movie\nand Product Sentiment Analysis dataset (Hindi)\n(Akhtar et al., 2016) , (b) ACTSA Sentiment Anal-\nysis corpus (Telugu) (Mukku and Mamidi, 2017).\n4953\nLang FT-W FT-WC IndicFT\nWord Similarity(Pearson Correlation)\npa 0.467 0.384 0.445\nhi 0.575 0.551 0.598\ngu 0.507 0.521 0.600\nmr 0.497 0.544 0.509\nte 0.559 0.543 0.578\nta 0.439 0.438 0.422\nAverage 0.507 0.497 0.525\nWord Analogy(% accuracy)\nhi 19.76 32.93 29.65\nTable 3: Word Similarity and Analogy Results for dif-\nferent pre-trained embeddings. (a) FT-W: FastText\nWikipedia, (b) FT-WC: FastText Wikipedia + Com-\nmonCrawl, (c)IndicFT: IndicNLP.\n5 IndicFT: Word Embeddings\nWe train FastText word embeddings for each lan-\nguage using IndicCorp, and evaluate their qual-\nity on: (a) word similarity, (b) word analogy,\n(c) text classification, (d) bilingual lexicon in-\nduction tasks. We compare our embeddings (re-\nferred to as IndicFT) with two pre-trained em-\nbeddings released by theFastText project trained\non Wikipedia (FT-W) (Bojanowski et al., 2017)\nand Wiki+CommonCrawl (FT-WC) (Grave et al.,\n2018) respectively.\n5.1 Training Details\nWe train 300-dimensional word embeddings for\neach language onIndicCorp using FastText (Bo-\njanowski et al., 2017). Since Indian languages are\nmorphologically rich, we choseFastText, which is\ncapable of integrating subword information by us-\ning character n-gram embeddings during training.\nWe train skipgram models for 10 epochs with a\nwindow size of 5, minimum token count of 5 and\n10 negative examples sampled for each instance.\nWe chose these hyper-parameters based on sug-\ngestions by Grave et al.(2018). Based on previ-\nously published results, we expect FastText to be\nbetter than word-level algorithms likeword2vec\n(Mikolov et al., 2013b) and GloVe (Pennington\net al., 2014) for morphologically rich languages.\n5.2 Word Similarity & Analogy Evaluation\nWe perform an intrinsic evaluation of the word\nembeddings using the IIIT-Hyderabad word simi-\nlarity dataset (Akhtar et al., 2017) (7 Indian lan-\nguages with 100-200 word-pairs per language) and\nthe Facebook Hindi word analogy dataset (Grave\net al., 2018). Table3 shows the evaluation results.\nLang Dataset FT-W FT-WC IndicFT\nhi BBC Articles 72.29 67.44 77.02\nIITP+ Movie 41.61 44.52 45.81\nIITP Product 58.32 57.17 61.57\nbn Soham Articles 62.79 64.78 71.82\ngu 81.94 84.07 90.74\nml iNLTK 86.35 83.65 95.87\nmr Headlines 83.06 81.65 91.40\nta 90.88 89.09 95.37\nte ACTSA 46.03 42.51 52.58\nAverage 69.25 68.32 75.80\nTable 4: Text classification accuracy on public datasets.\nOn average, IndicFT embeddings outperform the\nbaseline embeddings.\n5.3 Text Classification Evaluation\nWe evaluated the embeddings on different text clas-\nsification tasks: (a) news article topic, (b) news\nheadlines topic and (c) sentiment classification.\nDatasets. In addition to the IndicGLUE News\nCategory dataset, we experimented on the follow-\ning publicly available datasets: (a) IIT-Patna Sen-\ntiment Analysis dataset (Akhtar et al., 2016), (b)\nACTSA Sentiment Analysis corpus (Mukku and\nMamidi, 2017), (c) BBC News Articles classifica-\ntion dataset, (d) iNLTK Headlines dataset, and (e)\nSoham Bengali News classification dataset. (See\nAppendix A for dataset details).\nClassifier training.Following Meng et al.(2019),\nwe use ak-NN (k = 4) classifier since it is non-\nparameteric. Hence, classification performance di-\nrectly reflects how well the embedding space cap-\ntures text semantics. The input text embedding is\nthe mean of all word embeddings.\nResults. On nearly all datasets and languages,\nIndicFT embeddings outperform baseline embed-\ndings (see Tables4 and 5).\n5.4 Bilingual Lexicon Induction\nWe train bilingual word embeddings from English\nto Indian languages and vice versa using GeoMM\n(Jawanpuria et al., 2019), a state-of-the-art super-\nvised method for learning bilingual embeddings.\nWe evaluate the bilingual embeddings on the BLI\ntask, using bilingual dictionaries from the MUSE\nproject and aen-te dictionary created in-house. We\nsearch among the 200k most frequent target lan-\nguage words with the CSLS distance metric during\ninference (Conneau et al., 2018). Table 6 shows\nthe results. The quality of multilingual embed-\n4954\nLang FT-W FT-WC IndicFT\npa 97.12 95.53 96.47\nbn 96.57 97.57 97.71\nor 94.80 96.20 98.43\ngu 95.12 94.63 99.02\nmr 96.44 97.07 99.37\nkn 95.93 96.53 97.43\nte 98.67 98.08 99.17\nml 89.02 89.18 92.83\nta 95.99 95.90 97.26\nAverage 95.52 95.63 97.52\nTable 5: Accuracy onIndicGLUE News category test-\nset.\nen to Indic Indic to en\nFT-W FT-WC Ours FT-W FT-WC Ours\nbn 22.60 33.92 36.68 31.22 42.10 42.67\nhi 40.93 44.35 41.53 49.56 57.16 54.85\nte 21.10 23.01 51.11 25.36 32.84 57.58\nta 19.27 30.25 31.87 26.66 40.20 38.65\nAve. 25.98 32.88 40.29 33.20 43.08 48.38\nTable 6: Accuracy@1 for BLI.Ours refers toIndicFT.\ndings depends on the quality of monolingual em-\nbeddings. IndicFT bilingual embeddings signif-\nicantly outperform the baseline bilingual embed-\ndings for most languages.\n6 IndicBERT: Multilingual NLU Model\nIn this section, we introduceIndicBERT which is\ntrained onIndicCorp and evaluated onIndicGLUE.\nWe specifically chose ALBERT as the base model\nas it has fewer parameters making it easier to dis-\ntribute and use in downstream applications. Fur-\nther, similar to mBERT, we chose to train a sin-\ngle model for all Indian languages with a hope of\nutilizing the relatedness amongst Indian languages.\nIn particular, such joint training may be beneficial\nfor some of the under-represented languages (e.g.,\nOdia and Assamese).\n6.1 Pre-training\nUsing IndicCorp we first train a sentence piece to-\nkenizer (Kudo and Richardson, 2018) to tokenize\nthe sentences in each language. We use this tok-\nenized corpora to train a multilingual ALBERT us-\ning the standard masked language model (MLM)\nobjective. Note that we did not use the Sentence\nOrder Prediction objective used in the original AL-\nBERT work. Similar to mBERT and XLM-R mod-\nels, we perform exponentially smoothed weighting\nof the data across languages to give a better repre-\nsentation to low-resource languages. We choose\na vocabulary of 200k to accommodate different\nscripts and large vocabularies of Indic languages.\nWe train our models on a single TPU v3 pro-\nvided by Tensorflow Research Cloud9. We train\nboth the base and large versions of ALBERT. To\naccount for memory constraints, we use a smaller\nmaximum sequence length of 128. In addition, for\nthe large model, we use a smaller batch size of\n2048. For creating each batch, we first randomly\nselect a language and then randomly select sen-\ntences from that language. Apart from sequence\nlength and batch size, we use the default values\nfor the remaining hyperparameters as inLan et al.\n(2020). We train the model for a total of 400k steps.\nIt took 6 days to train the base model and 9 days\nto train the large model. In the remaining discus-\nsion, we refer to our models asIndicBERT base\nand IndicBERT large. Our models are compared\nwith two of the best performing multilingual mod-\nels: mBERT (Pires et al., 2019) and XLM-R base\nmodel (Ruder et al., 2019). Not that our model is\nmuch smaller compared to these models, while it is\ntrained on larger Indic language corpora (see Table\n14 in AppendixC.5 for details).\n6.2 Fine-tuning\nAfter pre-training, we fine-tune IndicBERT on\neach of the tasks inIndicGLUE using the respec-\ntive training sets. The fine-tuning is done indepen-\ndently for each task and each language (i.e., we\nhave a task-specific model for each language). We\ndescribe the fine-tuning procedure for each task.\nHeadline Prediction, Wikipedia Section Title\nPrediction. For headline prediction, we feed the\narticle and candidate headline to the model with\na SEP token in between. We have a classification\nhead at the top which assigns a score between 0 and\n1 to the headline. We use cross entropy loss with\nthe target label as 1 for the correct candidate and 0\nfor the incorrect candidates. During prediction, we\nchoose the candidate headline assigned the highest\nscore. Section title prediction uses the same proce-\ndure (Wikipedia section and section titles instead\nof news articles and headlines respectively).\nNamed Entity Recognition.Each sentence is fed\nas a single sequence to the model. For every to-\nken, we have a softmax layer at the output which\ncomputes a probability distribution over the NER\n9https://www.tensorflow.org/tfrc\n4955\nModel pa hi bn or as gu mr kn te ml ta avg\nNews Article Headline Prediction\nXLM-R 97.44 94.72 94.62 93.20 96.14 97.28 94.79 98.16 91.30 96.32 96.90 95.52\nmBERT 94.32 94.56 90.64 52.64 92.92 94.24 90.77 96.88 88.40 94.24 95.72 89.58\nIndicBERT base 97.36 95.36 95.91 93.84 96.62 97.36 93.85 97.88 89.16 96.48 96.26 95.46\nIndicBERT large 97.68 95.68 95.79 93.28 97.43 97.92 93.14 98.16 92.69 95.20 97.65 95.87\nWikipedia Section Title Prediction\nXLM-R 70.29 76.92 80.91 68.25 56.96 27.39 77.44 24.41 94.64 76.10 76.34 66.33\nmBERT 72.47 80.12 82.53 22.22 73.42 74.52 80.49 78.84 94.56 74.25 76.86 73.66\nIndicBERT base 67.39 74.02 80.11 57.14 65.82 68.79 72.56 75.05 94.80 75.87 74.90 73.31\nIndicBERT large 77.54 77.80 82.66 68.25 56.96 52.23 77.44 80.11 95.36 64.27 71.37 73.09\nCloze-style multiple-choice QA\nXLM-R 29.31 30.62 29.95 35.98 27.11 11.15 32.38 29.36 27.16 27.57 27.24 27.98\nmBERT 33.70 39.00 36.23 26.37 29.42 83.31 38.81 33.96 37.58 36.71 35.72 39.16\nIndicBERT base 44.74 41.55 39.40 39.32 40.49 70.78 44.85 39.57 32.60 35.39 31.83 41.87\nIndicBERT large 41.91 37.01 32.63 33.81 30.03 52.73 39.98 32.28 26.73 28.04 28.10 34.84\nTable 7: Test accuracy on various multiple-choice tasks.\nModel pa hi bn or as gu mr kn te ml ta avg\nArticle Genre Classification\nXLM-R 94.87 - 98.29 97.07 - 96.15 96.67 97.60 99.33 96.00 97.28 97.03\nmBERT 94.87 - 97.71 69.33 - 84.62 96.67 97.87 98.67 81.33 94.56 90.63\nIndicBERT base 97.44 - 97.14 97.33 - 100.00 96.67 97.87 99.67 93.33 96.60 97.34\nIndicBERT large 94.87 - 97.71 97.60 - 73.08 95.00 97.87 99.67 85.33 95.24 92.93\nNamed Entity Recognition (F1-score)\nXLM-R 17.86 89.62 92.95 25.00 66.67 55.32 87.86 47.06 81.71 81.98 79.16 65.93\nmBERT 50.00 86.56 91.81 19.05 92.31 68.04 91.27 59.72 84.31 82.64 79.90 73.24\nIndicBERT base 21.43 90.30 93.39 8.69 41.67 54.74 88.71 52.29 84.38 83.16 90.45 64.47\nIndicBERT large 44.44 86.81 91.85 35.09 43.48 70.21 87.73 63.51 80.12 84.35 80.81 69.85\nTable 8: Test accuracy on various classification tasks.\nclasses. We fine-tune the model using multi-class\ncross entropy loss.\nCloze-style Multiple-choice QA. We feed the\nmasked text segment as input to the model and at\nthe output we have a softmax layer which predicts\na probability distribution over the given candidates.\nWe fine-tune the model using cross entropy loss\nwith the target label as 1 for the correct candidate\nand 0 for the incorrect candidates.\nCross-lingual Sentence Retrieval. No fine-\ntuning is required for this task. We compute the\nrepresentation of every sentence by mean-pooling\nthe outputs in the last hidden layer and then using\ncosine distance to compute similarity between sen-\ntences (Libovický et al., 2019). Additionally, we\nalso center the sentence vectors across each lan-\nguage to remove language-specific bias in the vec-\ntors (Reimers and Gurevych, 2019).\nWinograd NLI, COPA, Paraphrase Detection:\nWe input the sentence pair into the model as seg-\nment A and segment B. The [CLS] representation\nfrom the last layer is fed into an output layer for\nclassification into one of the categories.\nNews Category Classification, Discourse Mode\nClassification, Sentiment Analysis. We feed the\nrepresentation of the [CLS] token from the last\nlayer to a linear classifier with a softmax layer\nto predict a probability distribution over the cate-\ngories. We fine-tune the model using multi-class\ncross entropy loss.\n6.3 Evaluation\nWe summarize the main observations from our re-\nsults as reported in Tables7-10.\nComparison with mBERT and XLM-R. On\nmost tasks,IndicBERT models outperform XLM-\nR and mBERT. Specifically,IndicBERT models\nare competitive on the Wikipedia Section Title pre-\n4956\nLanguage Dataset Ours mBERT XLM-R\nArticle Genre Classification\nhi BBC News 74.60 60.55 75.52\nbn Soham Articles 78.45 80.23 87.60\ngu INLTK Headlines 92.91 89.16 89.83\nml INLTK Headlines 94.76 82.28 95.40\nmr INLTK Headlines 94.30 87.50 92.48\nta INLTK Headlines 96.11 92.86 95.81\nSentiment Analysis\nhi Product Reviews 71.32 74.57 78.97\nhi Movie Reviews 59.03 56.77 61.61\nte ACTSA 61.18 48.53 59.33\nDiscourse Mode Classification\nhi MIDAS Discourse 78.44 71.20 79.94\nSemantic Similarity\nhi Amrita Subtask 1 93.11 93.22 91.78\nta Amrita Subtask 1 92.78 93.33 92.11\nml Amrita Subtask 1 89.11 88.67 88.78\npa Amrita Subtask 1 100.00 100.00 99.40\nhi Amrita Subtask 2 85.79 87.29 88.20\nta Amrita Subtask 2 69.07 68.57 68.21\nml Amrita Subtask 2 89.00 84.44 84.67\npa Amrita Subtask 2 93.47 93.20 87.73\nTextual Entailment\nhi WNLI 56.34 56.34 54.93\nmr WNLI 56.34 56.34 56.34\ngu WNLI 56.34 56.34 56.34\nhi COPA 62.50 65.91 43.18\nmr COPA 59.09 55.68 61.36\ngu COPA 53.41 43.18 48.86\nAverage 77.39 74.42 76.60\nTable 9: Test Accuracies on public datasets. Ours refer\nto IndicBERT-base.\ndiction task, but are out-performed by mBERT\non the NER dataset. On the publicly available\ndatasets (Table 9), IndicBERT-base outpeforms\nthe existing models.\nPerformance on Wikipedia Tasks. We no-\ntice that the performance of mBERT is rela-\ntively higher for the tasks based on Wikipedia\ndata, namely NER, Wikipedia Section Title pre-\ndiction, and Multiple-choice QA. This suggests\nthat mBERT, unlike other models, is benefit-\ning from exposure to Wikipedia data during pre-\ntraining. Note that we deliberately did not include\nWikipedia in our monolingual corpora as it is a\ngood source for creating NLU tasks. Hence, we\nwanted to avoid overlap between our pretraining\ndata and any potential Wikipedia-based dataset.\nSmall v/s LargeIndicBERT. The large and base\nLanguage XLM-R mBERT IB base IB large\nen-hi 4.77 33.73 24.67 21.99\nen-bn 9.46 26.30 26.12 29.00\nen-or 15.96 2.66 33.11 49.60\nen-gu 18.46 17.68 28.17 39.43\nen-mr 18.07 24.67 23.09 32.67\nen-te 15.23 26.13 25.10 34.30\nen-ml 17.47 16.76 31.22 32.26\nen-ta 10.48 23.78 25.44 33.58\navg 13.74 21.46 27.12 34.10\nTable 10: Precision@10 on Cross-Lingual Sentence Re-\ntrieval Task.\nmodels ofIndicBERT are comparable. There are\nsome tasks on which either task is clearly better.\nChallenging tasks. Multiple-choice QA and\nCross-Lingual Sentence Retrieval prove to be the\nmore challenging tasks. On both tasks,IndicBERT\nmodels improve on XLM-R and mBERT.\nEffect of corpus size. Comparing across lan-\nguages, on the 5 mono-lingual tasks, the perfor-\nmance ofIndicBERT large is poorest on Assamese\nand Odia – the two languages with the smallest cor-\npora sizes. On the other hand, performance is high-\nest on Hindi and Bengali, which have the largest\ncorpora sizes. This reinforces the expectation that\naccuracy is sensitive to the corpora size.\n7 Conclusion and Future Work\nWe present the IndicNLPSuite, a collection of\nlarge-scale, general-domain, sentence-level cor-\npora of 8.9 billion words across 11 Indian lan-\nguages, along with pre-trained models (IndicFT,\nIndicBERT) and NLU benchmarks (IndicGLUE).\nWe show that resources derived from this dataset\noutperform other pre-trained embeddings on many\nNLP tasks. The sentence-level corpora, em-\nbeddings and evaluation datasets are publicly\navailable under aCreative Commons Attribution-\nNonCommercial-ShareAlike 4.0 International Li-\ncense. We hope the availability of these resources\nwill accelerate NLP research for Indian languages\nby enabling the community to build further re-\nsources and solutions for various NLP tasks and\nopening up interesting NLP questions.\nAcknowledgments\nWe thank Google for providing TPU hardware via\nthe Tensorflow Research Cloud (TFRC) grant.\n4957\nReferences\nMd. Shad Akhtar, Ayush Kumar, Asif Ekbal, and Push-\npak Bhattacharyya. 2016. A hybrid deep learning\narchitecture for sentiment analysis. In COLING\n2016, 26th International Conference on Computa-\ntional Linguistics, Proceedings of the Conference:\nTechnical Papers, December 11-16, 2016, Osaka,\nJapan, pages 482–493.\nSyed Sarfaraz Akhtar, Arihant Gupta, Avijit Vajpayee,\nArjit Srivastava, and Manish Shrivastava. 2017.\nWord similarity datasets for Indian languages: An-\nnotation and baseline systems. InProceedings of the\n11th Linguistic Annotation Workshop, pages 91–94,\nValencia, Spain. Association for Computational Lin-\nguistics.\nRami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2013.\nPolyglot: Distributed word representations for mul-\ntilingual nlp. InProceedings of the Seventeenth Con-\nference on Computational Natural Language Learn-\ning, pages 183–192, Sofia, Bulgaria. Association for\nComputational Linguistics.\nM. Anand Kumar, S. Singh, B. Kavirajan, and K.P. So-\nman. 2016. DPIL@FIRE 2016: Overview of shared\ntask on detecting paraphrases in Indian Languages\n(DPIL). InCEUR Workshop Proceedings.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nOndrej Bojar, V ojtech Diatka, Pavel Rychlỳ, Pavel\nStranák, Vít Suchomel, Ales Tamchyna, and Daniel\nZeman. 2014. Hindencorp-hindi-english and hindi-\nonly corpus for machine translation. InLREC, pages\n3550–3555.\nChristian Buck, Kenneth Heafield, and Bas van Ooyen.\n2014. N-gram counts and language models from\nthe common crawl. In Proceedings of the Ninth\nInternational Conference on Language Resources\nand Evaluation (LREC’14), pages 3579–3584, Reyk-\njavik, Iceland. European Language Resources Asso-\nciation (ELRA).\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. Tydi qa: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages.Transactions of the As-\nsociation for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, E. Grave, Myle Ott, L. Zettlemoyer, and\nV . Stoyanov. 2020. Unsupervised cross-lingual rep-\nresentation learning at scale. InACL.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. InProceed-\nings of the International Conference on Learning\nRepresentations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nSwapnil Dhanwal, Hritwik Dutta, Hitesh Nankani, Ni-\nlay Shrivastava, Yaman Kumar, Junyi Jessy Li, De-\nbanjan Mahata, Rakesh Gosangi, Haimin Zhang, Ra-\njiv Ratn Shah, and Amanda Stent. 2020.An anno-\ntated dataset of discourse modes in Hindi stories. In\nProceedings of The 12th Language Resources and\nEvaluation Conference, pages 1191–1196, Marseille,\nFrance. European Language Resources Association.\nMurray B Emeneau. 1956. India as a lingustic area.\nLanguage.\nD. Goldhahn, T. Eckart, and U. Quasthoff. 2012. Build-\ning large monolingual dictionaries at the leipzig cor-\npora collection: From 100 to 200 languages. In\nProceedings of the International Conference on Lan-\nguage Resources and Evaluation (LREC 2012).\nAndrew S. Gordon, Zornitsa Kozareva, and Melissa\nRoemmele. 2011. Semeval-2012 task 7: Choice\nof plausible alternatives: An evaluation of common-\nsense causal reasoning. InSemEval@NAACL-HLT.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learn-\ning word vectors for 157 languages. In Proceed-\nings of the International Conference on Language\nResources and Evaluation (LREC 2018).\nDeepak Gupta, Surabhi Kumari, Asif Ekbal, and Push-\npak Bhattacharyya. 2018. MMQA: A Multi-domain\nMulti-lingual Question-Answering Framework for\nEnglish and Hindi. InProceedings of the Eleventh\nInternational Conference on Language Resources\nand Evaluation (LREC 2018), Miyazaki, Japan. Eu-\nropean Language Resources Association (ELRA).\nBushra Jawaid, Amir Kamran, and Ondrej Bojar. 2014.\nA Tagged Corpus and a Tagger for Urdu. InLREC,\npages 2938–2943.\nPratik Jawanpuria, Arjun Balgovind, Anoop\nKunchukuttan, and Bamdev Mishra. 2019. Learn-\ning multilingual word embeddings in latent metric\nspace: a geometric approach. Transaction of the\nAssociation for Computational Linguistics (TACL),\n7:107–120.\nChristian Kohlschütter, Péter Fankhauser, and Wolf-\ngang Nejdl. 2010. Boilerplate detection using shal-\nlow text features. InWSDM.\nTaku Kudo and John Richardson. 2018.SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\n4958\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAnoop Kunchukuttan. 2020. The IndicNLP Library.\nhttps://github.com/anoopkunchukuttan/indic_\nnlp_library/blob/master/docs/indicnlp.pdf.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations.ArXiv, abs/1909.11942.\nHang Le, Loic Vial, Jibril Frej, Vincent Segonne,\nMaximin Coavoux, B. Lecouteux, A. Allauzen,\nB. Crabbé, L. Besacier, and Didier Schwab. 2020.\nFlaubert: Unsupervised language model pre-training\nfor french. InLREC.\nH. Levesque, E. Davis, and L. Morgenstern. 2011. The\nwinograd schema challenge. InKR.\nJindřich Libovický, Rudolf Rosa, and Alexander Fraser.\n2019. How language-neutral is multilingual bert?\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nAnthony McEnery, Paul Baker, Rob Gaizauskas, and\nHamish Cunningham. 2000. Emille: Building a cor-\npus of south asian languages. VIVEK-BOMBAY-,\n13(3):22–28.\nYu Meng, Jiaxin Huang, Guangyuan Wang, Chao\nZhang, Honglei Zhuang, Lance Kaplan, and Jiawei\nHan. 2019. Spherical text embedding. InAdvances\nin Neural Information Processing Systems, pages\n8206–8215.\nTomas Mikolov, Kai Chen, G. S. Corrado, and J. Dean.\n2013a. Efficient estimation of word representations\nin vector space.CoRR, abs/1301.3781.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013b. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nSandeep Sricharan Mukku and Radhika Mamidi. 2017.\nACTSA: Annotated corpus for Telugu sentiment\nanalysis. In Proceedings of the First Workshop on\nBuilding Linguistically Generalizable NLP Systems,\npages 54–58, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Lau-\nrent Romary. 2019. Asynchronous Pipeline for\nProcessing Huge Corpora on Medium to Low Re-\nsource Infrastructures. In7th Workshop on the Chal-\nlenges in the Management of Large Corpora (CMLC-\n7), Cardiff, United Kingdom. Leibniz-Institut für\nDeutsche Sprache.\nPedro Javier Ortiz Suarez, Benoît Sagot, and Laurent\nRomary. 2019.Asynchronous pipelines for process-\ning huge corpora on medium to low resource infras-\ntructures. In Proceedings of the Workshop on Chal-\nlenges in the Management of Large Corpora, pages\n9 – 16, Mannheim. Leibniz-Institut für Deutsche\nSprache.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017.Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946–1958, Vancouver,\nCanada. Association for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018.Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227–\n2237, New Orleans, Louisiana. Association for Com-\nputational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019.Language models as knowl-\nedge bases? In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4996–\n5001, Florence, Italy. Association for Computational\nLinguistics.\nAlec Radford. 2018. Improving language\nunderstanding by generative pre-training.\nhttps://cdn.openai.com/research-covers/\nlanguage-unsupervised/language_\nunderstanding_paper.pdf.\nAlec Radford, Jeff Wu, Rewon Child, David\nLuan, Dario Amodei, and Ilya Sutskever.\n2019. Language models are unsupervised\nmultitask learners. https://cdn.openai.com/\nbetter-language-models/language_models_are_\nunsupervised_multitask_learners.pdf.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019.Exploring the limits\n4959\nof transfer learning with a unified text-to-text trans-\nformer. arXiv e-prints.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP),\npages 3982–3992, Hong Kong, China. Association\nfor Computational Linguistics.\nSebastian Ruder, Anders Søgaard, and Ivan Vulić. 2019.\nUnsupervised cross-lingual representation learning.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics: Tutorial\nAbstracts, pages 31–38, Florence, Italy. Association\nfor Computational Linguistics.\nShashank Siripragrada, Jerin Philip, Vinay P. Nam-\nboodiri, and C V Jawahar. 2020.A multilingual par-\nallel corpora collection effort for Indian languages.\nIn Proceedings of The 12th Language Resources\nand Evaluation Conference, pages 3743–3751, Mar-\nseille, France. European Language Resources Asso-\nciation.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. dAlché Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 3266–3280. Curran Asso-\nciates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzman, Ar-\nmand Joulin, and Edouard Grave. 2019. Ccnet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data.arXiv preprint arXiv:1911.00359.\nLiang Xu, Xuanwei Zhang, Lu Li, Hai Hu, Chenjie Cao,\nWeitang Liu, Junyi Li, Yudong Li, Kai Sun, Yechen\nXu, et al. 2020. Clue: A chinese language un-\nderstanding evaluation benchmark. arXiv preprint\narXiv:2004.05986.\nA Publicly Available Datasets\nIn this section, we summarize the publicly avail-\nable datasets which are part of the IndicGLUE\nbenchmark. The essential details of the datasets\nare described in Table 11. Except WNLI and\nLang Dataset N # Examples\nTrain Test\nhi BBC Articles 10 6 3,467 866\nbn Soham Articles 11 6 11,284 1411\ngu 3 5,269 659\nml iNLTK 3 5,036 630\nmr Headlines 12 3 9,672 1,210\nta 3 5,346 669\nhi IITP+ Movie Reviews 3 2,480 310\nIITP Product Reviews13 3 4,182 523\nte ACTSA corpus 14 3 4,328 541\nhi MIDAS Discourse Mode 15 5 7974 997\nhi 2 2500 900\npa Amrita Paraphrase 16 2 1700 500\nta Subtask 1 2 2500 900\nml 2 2500 900\nhi 2 3500 1400\npa Amrita Paraphrase 2 2200 750\nta Subtask 2 2 3500 1400\nml 2 3500 1400\nhi COPA 2 362 449\ngu (new, translated) 2 362 448\nmr 2 362 449\nhi WNLI 2 636 147\ngu (new, translated) 2 636 147\nmr 2 636 147\nTable 11:IndicGLUE public datasets statistics.N is the\nnumber of classes.\nCOPA, all other datasets are publicly available.\nThey cover sentiment analysis, new article clas-\nsification, news headline classification, discourse\nmode classification. The WNLI and COPA\ndatasets are manual translations of the original En-\nglish datasets into a few Indian languages.\nSome notes on public datasets\n• The IITP+ Movie Reviews sentiment analy-\nsis dataset is created by merging IIT-Patna\n10https://github.com/NirantK/hindi2vec/releases/tag/bbc-\nhindi-v0.1\n11https://www.kaggle.com/csoham/classification-bengali-\nnews-articles-indicnlp\n12https://github.com/goru001/inltk\n13http://www.iitp.ac.in/ ai-nlp-ml/resources.html\n14https://github.com/NirantK/bharatNLP/releases\n15https://github.com/midas-research/hindi-discourse\n16http://www.nlp.amrita.edu/dpil_cen\n4960\nLang Classes # Articles\nTrain Test\npa BIZ, ENT, POL, SPT 2,496 312\nbn ENT, SPT 11,200 1,400\nor BIZ, CRM, ENT, SPT 17,750 2,250\ngu BIZ, ENT, SPT 1,632 204\nmr ENT, STY , SPT 3,600 450\nkn ENT, STY , SPT 24,000 3,000\nte ENT, BIZ, SPT 19,200 2,400\nml BIZ, ENT, SPT, TECH 4,800 600\nta ENT, POL, SPT 7,200 900\nTable 12:IndicGLUE News category dataset statistics.\nThe following are the categories: entertainment: ENT,\nsports: SPT, business: BIZ, lifestyle; STY , techology:\nTECH, politics: POL, crime: CRM.\ndataset with the smaller IIT-Bombay and\niNLTK datasets.\n• The IIT-Patna Movie and Product review\ndatasets have 4 classes namely postive, nega-\ntive, neutral and conflict. We ignored the con-\nflict class.\n• In the Telugu-ACTSA corpus, we evaluated\nonly on the news line dataset (named as tel-\nugu_sentiment_fasttext.txt) and ignored all\nthe other domain datasets as they have very\nfew data-points.\nB IndicGLUE News Category Dataset\nThe IndicGLUE news category dataset is a collec-\ntion of articles labeled with news categories. We\nused this dataset in the evaluation of word embed-\ndings and language models. Table12 provides the\nstatistics of the dataset.\nC IndicGLUE Datasets\nWe provide some additional statistics for theIn-\ndicGLUE dataset in Table2. In the following sub-\nsections, we show some examples of the datasets\nthat we created .\nC.1 News Category Classification\nArticle Snippet கர்நாடக சட்டப் ேபரைவயில் ெவற்றி\nெபற்ற எம்எல்ஏக்கள் இன்று பதவிேயற்றுக் ெகாண்ட\nநிைலயில் , காங்கிரஸ் எம்எல்ஏ ஆனந்த் சிங் க்கள்\nஆப்ெசன்ட் ஆகி அதிர்ச்சிைய ஏற்படுத்தியுள்ளார்\n. உச்சநீதிமன்ற உத்தரவுப்படி இன்று மாைல\nமுதலைமச்சர் எடியூரப்பா இன்று நம்பிக்ைக\nMin Max Avg\nHeadline Prediction\nArticle Length (in words) 12 448 154\nHeadline Length (in words) 2 47 8.9\nWikipedia Section-Title Prediction\nSection Length (in words) 9 9554 140\nTitle Length (in words) 1 82 2.2\nNews Category Classification\nArticle Length (in words) 23 4649 205\nCloze-style QA\nQuestion Length (in words) 7 190 63\nCross-lingual Sentence Retrieval\nNumber of Sent Pairs per Lang Pair 752 6463 4904\nTable 13: AdditionalIndicGLUE statistics.\nவாக்ெகடுப்பு நடத்தி ெபரும்பான்ைமைய நிரூபிக்க\nஉச்சநீதிமன்றம் உத்தரவிட்டது .\nCategory: Politics\nC.2 Headline Prediction\nNews Article ºçɫɻೕಯɈģ: 23 ವಷɠದ\nಇĨೂɶೕèȟ ಮé½ ğÇɡ Ƣಬɷ ರನುɴ ನಡು ರĶɰಯʉĮೕ\n¸º ಸɰɻಗäಂದ ಬಬɠರ¾É ಹĤɺ ¸Óರುವ ಘಟĨ\nɈģಯʉ ಶÚ¾ರ ºÖɻ ನġØĦ.ಅಂತರ ±ȟ\nĕೂı¹ದ ಮé½ ğÇ ɡ ¹ÉÅį. ಅಂತº\nಅವರು ಪæɦಮ ಬಂ¢ಳದ ಮೂಲದವºÉÅį. ಕĲದ\nºÖɻ 8.00 ಗಂğ ಸು¸áė ĕಲಸ ಮುÉè ಮĨė\nĤರಳುÖɰದɲ ಸಂದಭɠದʉ ಅಂತº ಅವರ ĭೕı ±ä\n¸Óರುವ ದುಷ ɡ ßɠಗಳು ¸º ಸ ɰɻಗäಂದ ಹĻ\nನġèÅįಂದು Ɋãೕಸರು ķೕäÅį.±ä ನġèದ\nನಂತರ ರಕɰದ ಮಡುåನʉ Ýದುɲ ಒÅಡುÖɰದɲ ಅಂತº\nಅವರನುɴ ಸɱ äೕಯರು ಆಸɵ Ĥɻė ±ಳèÅį. ಆದį, ಆಸɵ Ĥɻė\n±ಖãಸುವಷɫ ರʉ ಅಂತº ಅವರು Áವನ ɴ ÛɵÅįಂದು\nಅವರು ķೕäÅį.ಪɻಕರಣ ±ಖãèĕೂಂÓರುವ Ɋãೕಸರು\nತÚĖ ಆರಂÞèÅį\"\nCandidate 1: ಇĨೂɶೕèȟ ಮé½ ğÇɡ ಯ ಬಬɠರ ಹĤɺ\n[correct answer]\nCandidate 2: ¸ನèಕ ಅಸɿ Ķɱ ĭೕı ಮಕɡ ಳ ಕäɾ ಎಂದು\nÞೕಕರ ಹĻ\nCandidate 3: ಕಸಬ īಂėɻಯʉ ಮುಸುಕು²áಗಳ\nತಂಡØಂದ ಮೂವರು ಯುವಕರ ĭೕı ಹĻ : ಓವɠ\nಗಂÞೕರ\nCandidate 4: ಕÕĳ ºಜɺ ದʉ mobile ಬಂȏ,\nÛɻಂÑಂȀ ĩɻȟ ĭೕı ±ä\nC.3 Wikipedia Section Title Prediction\nSection Text 2005માં, જ ેકમેન િનમાŏણ ક ં પની, સીડ\nòોડકશÑસ ઊભી કરવા તેના લાંબાસમયના મદદનીશ\nજહોન પાલેમĉ સાથે ŕડાયા, જ ેમનો òથમ òોજ ેકટ\n2007માં િવવા લાફિલન હતો. જ ેકમેનની અ�ભનેíી પÍની\nડે બોરા-લી ફનĀસ પણ ક ં પનીમાં ŕડાઈ, અને પાલેમĉએ\n4961\nModel Params #Train Tokens\nTotal Indic\nXLM-R 125M 295B 3.99B\nmBERT 110M 18.2B* 184M*\nIndicBERT base 12M 8.93B 7.59B\nIndicBERT large 18M 8.93B 7.59B\nTable 14: Comparison of Different Models. *Esti-\nmated.\nપોતાના, ફનĀસ અને જ ેકમેન માટે “ યુિનટી ” અથŏવાળા\nલખાણની આ íણ વąટીઓ બનાવી.[૨૭] íણેયના\nસહયોગ અંગે જ ેકમેને જણાÚયું કે “ મારી µજદગીમાં જ ેમની\nસાથે મÿ કામ કયુŐ તે ભાગીદારો અંગે ડે બ અને જહોન પાલેમĉ\nઅંગે હું ખૂબ નસીબદાર છુ ં . ખર ે ખર તેથી કામ થયું. અમારી\nપાસે જુ દું જુ દું સામŏÎય હતું. હ ું તે પસંદ કરતો હતો. I love\nit.તે ખૂબ ઉİેજક છે . ”[૨૮]ફોકસ આધાœરત સીડ લેબલ,\nઆમÑડા િÝકવેઈટઝર, કે થœરન ટે Öબિલન, એલન મંડે લબમ\nઅને ŕય મœરનો તેમજ સાથે ￹સડની આધાœરત િનમાŏણ\nકચેરીનું સંચાલન કરનાર અલાના óીનો સમાવેશ થતાં કદમાં\nિવÝતૃત બની. આ ક ં પીનોનો ઉĴ ે શ જ ેકમેનના વતનના\nદેશની Ýથાિનક ò￵તભાને કામે લેવા મÐયમ બજ ેટવાળી\nિફØમો બનાવવાનો છે .\nCandidate 1: એકસ-મેન\nCandidate 2: કારકીદŏ\nCandidate 3: િનમાŏણ કં પન [correct answer]\nCandidate 4: ઓÝટŌે િલય\nC.4 Cloze-style Question Answering\nQuestion অেাবরেফƎ হেলা দুই সĳােহর একিট\nঅনুƂান, যা Ĺিতবছর<MASK> অনুিƂত হয়। এিট\nমূলত ƶসেĲřর মােসর ƶশষ িদেক এবং অেাবর মােসর\nŲŠর িদেক পালন করা হেয় থােক। Ĺিতবছর Ĺায় ৬০ল§\nƶলাক এই আেয়াজেন অংশ·হণ কেরন। জামƸািনেত এবং\nসারািবেŸ অেাবরেফƎ নােম আরও উৎসব পালন করা\nহেয় থােক।\nCandidate 1: িমউিনেখ [correct answer]\nCandidate 2:ƶভেনজুেয়লােত\nCandidate 3: বািলƸেন\nCandidate 4:Źীল¿ােত\nC.5 Model Details\nTable 14 compares our models with existing pre-\ntrained models.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8709400296211243
    },
    {
      "name": "Natural language processing",
      "score": 0.7707670331001282
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6998559236526489
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6242534518241882
    },
    {
      "name": "Paraphrase",
      "score": 0.6084879636764526
    },
    {
      "name": "Headline",
      "score": 0.6057707667350769
    },
    {
      "name": "Sentence",
      "score": 0.5575931072235107
    },
    {
      "name": "Word (group theory)",
      "score": 0.5258544087409973
    },
    {
      "name": "Language model",
      "score": 0.4926837086677551
    },
    {
      "name": "Linguistics",
      "score": 0.18517765402793884
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24676775",
      "name": "Indian Institute of Technology Madras",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I4210162141",
      "name": "Microsoft (India)",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I4210151956",
      "name": "Robert Bosch (India)",
      "country": "IN"
    }
  ],
  "cited_by": 355
}