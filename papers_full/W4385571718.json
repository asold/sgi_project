{
  "title": "Backpack Language Models",
  "url": "https://openalex.org/W4385571718",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1968879968",
      "name": "John Hewitt",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2557633120",
      "name": "John Thickstun",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2151390485",
      "name": "Christopher D. Manning",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2171686691",
      "name": "Percy Liang",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281758439",
    "https://openalex.org/W3211350505",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W3099878876",
    "https://openalex.org/W3035102548",
    "https://openalex.org/W4296604605",
    "https://openalex.org/W3175350702",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W3209374680",
    "https://openalex.org/W4297412003",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W3173511996",
    "https://openalex.org/W4287811112",
    "https://openalex.org/W4287125778",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W4320458378",
    "https://openalex.org/W2250473257",
    "https://openalex.org/W2963366649",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3186769346",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W4306313145",
    "https://openalex.org/W4385573516",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3098275893",
    "https://openalex.org/W2149671658",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2170682101",
    "https://openalex.org/W3035602609",
    "https://openalex.org/W4287332927",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2080100102",
    "https://openalex.org/W1662133657",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2952208026",
    "https://openalex.org/W4320458252",
    "https://openalex.org/W2963483561",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W2963015836"
  ],
  "abstract": "We present Backpacks: a new neural architecture that marries strong modeling performancewith an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination ofsense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model’s behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM’s word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 9103–9125\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nBackpack Language Models\nJohn Hewitt John Thickstun Christopher D. Manning Percy Liang\nDepartment of Computer Science, Stanford University\n{johnhew,jthickstun,manning,pliang}@cs.stanford.edu\nAbstract\nWe present Backpacks: a new neural architec-\nture that marries strong modeling performance\nwith an interface for interpretability and con-\ntrol. Backpacks learn multiple non-contextual\nsense vectors for each word in a vocabulary,\nand represent a word in a sequence as a context-\ndependent, non-negative linear combination of\nsense vectors in this sequence. We find that,\nafter training, sense vectors specialize, each\nencoding a different aspect of a word. We\ncan interpret a sense vector by inspecting its\n(non-contextual, linear) projection onto the out-\nput space, and intervene on these interpretable\nhooks to change the model’s behavior in pre-\ndictable ways. We train a 170M-parameter\nBackpack language model on OpenWebText,\nmatching the loss of a GPT-2 small (124M-\nparameter) Transformer. On lexical similar-\nity evaluations, we find that Backpack sense\nvectors outperform even a 6B-parameter Trans-\nformer LM’s word embeddings. Finally, we\npresent simple algorithms that intervene on\nsense vectors to perform controllable text gen-\neration and debiasing. For example, we can\nedit the sense vocabulary to tend more towards\na topic, or localize a source of gender bias to a\nsense vector and globally suppress that sense.\n1 Introduction\nConsider the prefix The CEO believes that ___, and\nthe problem of debiasing a neural language model’s\ndistribution over he/she. Intuitively, the bias for\nhe originates in the word CEO, because replacing\nCEO with nurse flips the observed bias. A success-\nful intervention to debias CEO must reliably apply\nin all contexts in which the word CEO appears;\nideally we would want to make a non-contextual\nchange to the model that has predictable effects\nin all contexts. In general, in all aspects of in-\nterpretability and control, it is desirable to make\ninterventions with a tractable interface (e.g., non-\ncontextual representations) that apply globally.\n∑\nThe tea is\nhot hot=*\nThe tea is\nT ransformer\nT ransformer LM\nT ransformer\nBackpack LM\nFigure 1: Transformers are monolithic functions of se-\nquences. In Backpacks, the output is a weighted sum of\nnon-contextual, learned word aspects.\nSuch interventions are difficult in Transformer\nmodels (Vaswani et al., 2017) because their con-\ntextual representations are monolithic functions of\ntheir input. Almost any intervention on the model\nhas complex, non-linear effects that depend on con-\ntext. We would instead like models that enable\nprecise, rich interventions that apply predictably in\nall contexts, and are still expressive, so they are a\nviable alternative to Transformers.\nWe address these challenges with a new neu-\nral architecture, the Backpack, for which predic-\ntions are log-linear combinations of non-contextual\nrepresentations. We represent each word in a vo-\ncabulary as a set of non-contextual sense vectors\nthat represent distinct learned aspects of the word.\nFor example, sense vectors for the word “science”\ncould encode types of science, connections to tech-\nnology, notions of science being “settled,” or differ-\nent aspects of the scientific process (replication or\nexperiment) (Table 1). Sense vectors do not learn\nclassic word sense, but more general aspects of a\nword’s potential roles in different contexts; in fact,\nthey can be seen as a multi-vector generalization\nof classic word vectors (Mikolov et al., 2013).1\n1Our code, sense vectors, language model weights, and\ndemos are available at https://backpackmodels.science.\n9103\nA few senses of the word science\nSense 3 Sense 7 Sense 9 Sense 10 Sense 8\nfiction replication religion settled clones\nfictional citation rology sett experiments\nFiction Hubble hydra settle mage\nliteracy reprodu religions unsett experiment\ndenial Discovery nec Sett rats\nMacBookHP = MacBook −Apple + HP\nThe MacBook is best known for its form fac-\ntor, but HP has continued with its Linux-based\ncomputing strategy. HP introduced the Hyper 212\nin 2014 and has continued to push soon-to-be-\nreleased 32-inch machines with Intel’s Skylake\nprocessors.\nTable 1: Examples of the rich specialization of sense vectors representing the word science, and an example of\nediting sense vectors non-contextually (changing MacBook to be associated with HP) and having the resulting\ncontextual predictions change.\nTo make interventions on sense vectors behave\npredictably in different contexts, a Backpack rep-\nresents each word in a sequence as a linear com-\nbination of the sense vectors for all words in the\nsequence. The expressivity of a Backpack comes\nfrom the network that computes the weights of the\nlinear combination as a function of the whole se-\nquence; for example, in all our experiments we\nuse a Transformer for this. Since sense vectors are\nsoftly selected depending on the context, they can\nspecialize; each sense can learn to be predictively\nuseful in only some contexts. The log-linear con-\ntribution of senses to predictions then implies that\nthe interventions on sense vectors we demonstrate\nin Section 6 apply identically (up to a non-negative\nscalar weight) regardless of context.\nOur experiments demonstrate the expressivity of\nBackpack language models, and the promise of in-\nterventions on sense vectors for interpretability and\ncontrol. In Section 4 we train Backpack language\nmodels on 50B tokens (5 epochs) of OpenWebText;\na Backpack with 124M parameters in the contex-\ntual network (and 46M parameters for sense vec-\ntors) achieves the perplexity of a 124M-parameter\nTransformer; thus one pays for more interpretabil-\nity with a larger model size. In Section 5, we show\nthat sense vectors specialize to encode rich notions\nof word meaning. Quantitatively, on four lexical\nsimilarity datasets (e.g., SimLex999), sense vectors\nof a 170M parameter Backpack outperform word\nembeddings of the 6B-parameter GPT-J-6B Trans-\nformer, and approach the performance of state-of-\nthe-art specialized methods for this task. Finally, in\nSection 6 we show that sense vectors offer a control\nmechanism for Backpack language models. For ex-\nample, stereotypically gendered profession words\n(e.g., “CEO” or “nurse”) tend to learn a sense vec-\ntor associated with this gender bias; by downscal-\ning this sense vector, we greatly reduce disparity in\ncontextual predictions in a limited setting.\n2 The Backpack Architecture\nIn this section, we define the general form of the\nBackpack architecture. We then show how contin-\nuous bag-of-words word2vec (CBOW) (Mikolov\net al., 2013) and Self-Attention-Only networks (El-\nhage et al., 2021; Olsson et al., 2022) are special\ncases of Backpacks.\n2.1 Backpack General Form\nA Backpack is a parametric function that maps\na sequence of symbols x1:n = (x1,..., xn) to a\nsequence of vectors o1:n = ( o1,..., on), where\neach symbol xi belongs to a finite vocabularyVand\noi ∈Rd. We call oi the Backpack representation\nof xi in the context of a sequence x1:n.\nSense vectors. For each x ∈V, a Backpack con-\nstructs ksense vectors\nC(x)1,...,C (x)k, (1)\nwhere C : V→ Rk×d. Sense vectors are a multi-\nvector analog to classic non-contextual word repre-\nsentations like word2vec or GloVe: we make this\nanalogy precise in Section 2.2.\nWeighted sum. For a sequence x1:n, the repre-\nsentation oi of element xi is a weighted sum of the\npredictive sense vectors for the words in its context:\ngiven contextualization weights α∈Rk×n×n,\noi =\nn∑\nj=1\nk∑\nℓ=1\nαℓijC(xj)ℓ. (2)\nThe contextualization weights αℓij of a Backpack\nare themselves defined by a (non-linear) contextu-\nalization function of the entire sequence x1:n:\nα= A(x1:n), (3)\nwhere A: Vn →Rk×n×n.\n9104\nThe name “Backpack” is inspired by the fact that\na backpack is like a bag—but more orderly. Like a\nbag-of-words, a Backpack representation is a sum\nof non-contextual senses; but a Backpack is more\norderly, because the weights in this sum depend on\nthe ordered sequence.\nBackpack Models. A Backpack model is a prob-\nabilistic model that defines probabilities over some\noutput space Yas a log-linear function of a Back-\npack representation o1:n ∈Rn×d:\np(y|o1:n) = softmax (E(o1:n)) , (4)\nwhere y ∈Y and E : Rn×d →R|Y| is a linear\ntransformation. Because Backpack models are log-\nlinear in their representations, the sense vectors\ncontribute log-linearly to predictions. This allows\nus to inspect a sense vector by projecting it onto\nthe vocabulary via E and observe exactly how it\nwill contribute to predictions in any context.\nModels parameterized by the prevailing deep\nneural architectures—including LSTMs (Hochre-\niter and Schmidhuber, 1997) and Transformers—\nare not Backpacks because their output represen-\ntations are (relatively) unconstrained functions of\nthe entire sequence. By contrast, Backpack models\nmay seem limited in expressivity: the representa-\ntions oi are scalar-weighted sums of non-contextual\nvectors C(xj)ℓ. Contextual relationships between\nsequence elements can only be expressed through\nthe weights α= A(x1:n). Nevertheless, our exper-\niments show that an expressive contextualization\nweight network can represent complex functions\nby weighted sums of sense vectors, e.g., our 170M\nparameter Backpack LM uses a 124M-parameter\nTransformer to compute α, and achieves the loss\nof a 124M-parameter Transformer LM.\nTo place Backpacks in some historical context,\nwe now show how two existing architectures can\nbe described as Backpacks.\n2.2 Continuous Bag-of-Words is a Backpack\nThe continuous bag-of-words word2vec model de-\nfines a probability distribution over a center word\nxc ∈V conditioned on ncontext words x1:n.2 The\nmodel proceeds to (1) construct vector embeddings\nvx for each x ∈V, and (2) uniformly average the\nembeddings of the context words to predict the\n2Context in this setting is usually defined as words sur-\nrounding the center word.\ncenter word:\nvxc =\nn∑\ni=1\n1\nnvxi , (5)\np(xc |x1:n) = softmax(Uvxc ), (6)\nwhere U ∈RV×d. We see that vxc is a Backpack\nrepresentation by setting C(x) = vx ∈R1×d in\nEquation (1) using a single sense vector ( k = 1)\nand setting the contextualization weights in Equa-\ntion (3) to be uniform: αℓij = 1\nn.\nThis connection to CBoW foreshadows the emer-\ngence of linguistic structures in the predictive sense\nvectors of Backpack models, just as these structures\nemerge in CBoW (Mikolov et al., 2013).\n2.3 Single-Layer Self-Attention is a Backpack\nThe Backpack structure—define sense vectors (val-\nues), and use the sequence to determine how to\nsum them (weights)—may remind the reader of a\nsingle layer of self-attention. The key-query-value\nself-attention function is as follows:\noj =\nn∑\ni=1\nk∑\nℓ=1\nαℓijOV(ℓ)xj (7)\nαℓ = softmax(x⊤K(ℓ)⊤Q(ℓ)x), (8)\nwhere x ∈ Rn×d is (overloaded) to be a non-\ncontextual embedding of the sequence, O ∈\nRd×d/k, and V(ℓ) ∈Rd/k×d, where kis the number\nof attention heads. The self-attention function is a\nBackpack with C(xj)ℓ = OV(ℓ)xj. Self-attention-\nonly networks are studied in the context of, e.g.,\nmechanistic interpretability (Elhage et al., 2021).\nA Transformer composes blocks of self-attention\nand non-linear feed-forward layers that combine\ninformation from the whole sequence; unlike a\nTransformer, the contextualization weights of a\nBackpack each select a non-contextual sense of a\nsingle word.\n3 Language Modeling with Backpacks\nIn this section, we define a neural autoregressive\nlanguage model parameterized by a Backpack. We\nuse the standard softmax parameterization of the\nprobability over the next token in a sequence, with\na weight matrix E ∈Rd×|V|that maps a represen-\ntation oj ∈Rd to logits E⊤oj ∈R|V|:\np(xj |x1:j−1) = softmax(E⊤oj). (9)\n9105\nRecall (Section 2.1) that Backpack representations\noj are defined by sense vectors C(x) and contextu-\nalization weights αj. In Section 3.1 we describe a\nparameterization of Cfor the predictive sense vec-\ntors in Equation (1), and in Section 3.2 we describe\na parameterization of Afor the contextualization\nweight network in Equation (3). When oj is pa-\nrameterized by a Backpack, we call a model of the\nform given by Equation (9) a Backpack LM.\n3.1 Parameterizing senses\nFor the sense function C : V→ Rk×d, we embed\neach x ∈V into Rd and pass these embeddings\nthough a feed-forward network FF : Rd →Rk×d:\nC(x) = FF(Ex), (10)\nwhere the embedding/projection matrix Eis tied to\nthe output matrix in Equation (9) (Press and Wolf,\n2017). Note that we could define all k×|V| sense\nvectors using a lookup table, but this would be an\nenormous number of parameters as kgrows large.\nInstead, we embed the words as Ex ∈Rd, and\nthen blow them up to Rd×k using shared weights.\nThis may explain the related sense roles observed\nfor different word types in Section 5.1.\n3.2 Parameterizing contextualization weights\nWe parameterize A: Vn →Rk×n×n using a stan-\ndard Transformer, followed by a layer of multi-\nheaded key-query self-attention. That is, we pass\nan embedded sequence through a Transformer\nh1:n = Transformer(Ex1:n) (11)\n(with proper autoregressive masking and some po-\nsition representation) and compute A(x1:n) = α,\nwhere\nαℓ = softmax(h1:nK(ℓ)⊤Q(ℓ)h⊤\n1:n), (12)\nfor each predictive sense ℓ= 1,...,k with matri-\nces K(ℓ),Q(ℓ) ∈Rd×d/k. We can think of the k\nsenses as heads and, for each head, the contextu-\nalization weights define a distribution of attention\nover words.3\n4 Experiments Training Backpack LMs\nIn this section we specify the hyperparameters used\nto train Backpack and Transformer language mod-\nels (Section 4.1), data and optimization procedure\n3Note that the sense weights are normalized (1) indepen-\ndently for each sense, and (2) to sum to one over the sequence\nlength.\n(Section 4.2), evaluations (Section 4.3) and results\n(Section 4.4). We also show the necessity of learn-\ning k> 1 sense vectors to achieve strong language\nmodeling performance (Section 4.5).\n4.1 Models\nWe train three Transformer baseline models, which\nwe label Micro (30M parameters), Mini (70M pa-\nrameters), and Small (124M parameters; the same\nsize as GPT-2 small). We also train Micro (40M),\nMini (100M), and Small (170M) Backpack lan-\nguage models, for which the weighting function\n(Equation 11) is parameterized using the corre-\nsponding Transformer, and almost all extra parame-\nters are in the non-contextual sense vectors.4 Back-\npacks thus cost extra parameters and compute be-\nyond their underlying contextualization network.\nExcept where stated, we use k= 16 sense vectors\nin all Backpacks (Section A).\nWe use a reduced sequence length of 512 for all\nmodels, and the 50,257-subword GPT-2 tokenizer.\nModel hidden dimensionalities, layer counts, and\nhead counts are reported in Table 9.\n4.2 Data & Optimization\nWe train all models on OpenWebText (Gokaslan\nand Cohen, 2019), a publicly available approxi-\nmate reconstruction of the English WebText corpus\nused to train the GPT-2 family of models (Rad-\nford et al., 2019). We use a batch size of 524,288\ntokens, and train all models for 100,000 gradient\nsteps for a total of 52B tokens; training for longer\nis known to make marginal difference for small\nmodels (Hoffmann et al., 2022). The size of Open-\nWebText means this is roughly 5 epochs. We use\ncross-entropy loss and the AdamW optimizer, with\na warmup of 5,000 steps and linear decay to zero.\n4.3 Evaluations\nBefore our experiments in interpretability and con-\ntrol, we check the expressivity of Backpacks. We\nevaluate models on perplexity for a held out set\nof OpenWebText, perplexity and accuracy for the\n(OpenAI variant of) LAMBADA evaluation of\nlong-distance dependencies (Radford et al., 2019;\nPaperno et al., 2016), perplexity on Wikitext (Mer-\nity et al., 2017), and BLiMP English linguistic com-\npetence accuracy (Warstadt et al., 2020) evaluated\nusing the EleutherAI harness (Gao et al., 2021)\n(Version 1).\n4There are a negligible number of additional parameters in\nthe final key-query Backpack operation (Equation 12)).\n9106\nModel OpenWebText PPL ↓ LAMBADA PPL ↓ LAMBADA ACC ↑ Wikitext PPL ↓ BLiMP ↑\nBackpack-Micro 31.5 110 24.7 71.5 75.6\nTransformer-Micro 34.4 201 21.3 79.5 77.8\nBackpack-Mini 23.5 42.7 31.6 49.0 76.2\nTransformer-Mini 24.5 58.8 29.7 52.8 80.4\nBackpack-Small 20.1 26.5 37.5 40.9 76.3\nTransformer-Small 20.2 32.7 34.9 42.2 81.9\nTable 2: Language modeling performance; all models trained for 100k steps, 500K token batch size, on OWT. For\nPPL, lower is better; for accuracy, higher is better. Note that models are not parameter-comparable; each Backpack\nhas a matched-size Transformer in its contextualization network.\n4.4 Discussion\nComparing each Backpack LM to a Transformer\nLM of equivalent specification to the Backpack’s\ncontextualization network, we see that the Back-\npack performs roughly as well (Table 2). Again, the\nBackpack has more parameters, a tax for the inter-\nface provided by sense vectors. During training, we\nfind that Backpack language models take longer to\nconverge than Transformers. Curiously, while the\nSmall Backpack and Transformer achieve almost\nidentical OWT perplexity, the Backpack language\nmodels perform substantially better on LAMBADA\nand Wikitext, but worse on BLiMP.\n4.5 Effect of varying the number of senses\nTo study the impact of the number of sense vec-\ntors on language modeling performance, we train\nMini-sized Backpack language models on a re-\nduced schedule of 50,000 gradient steps, for k ∈\n{1,4,16,64}sense vectors. The perplexities for\nk = 1 ,4,16,64 are 38.6, 29.3, 26.0, and 24.1,\ndemonstrating the necessity of a non-singleton set\nof sense vectors. Table 8 contains the full results.\n5 Emergent Structure in Sense Vectors\nBackpack language model sense vectors are not\ntrained using a supervised notion of word sense,\nbut implicitly specialize to encode different shades\nof a word’s predictive use. In this section, we qual-\nitatively examine sense vectors (Section 5.1) and\nquantitatively demonstrate their effectiveness in\ncomputing lexical similarity and relatedness (Sec-\ntion 5.2). Taken together, this suggests that sense\nvectors can provide a high-level interface for inter-\nvention, which we explore in Section 6.\n5.1 Visualizing Senses\nEmpirically, trained Backpack models associate\nspecific sense vector indices with different roles for\nprediction. We interpret these roles by picking a\nsense ℓof a word x, and projecting this sense onto\nthe word embeddings: E⊤C(x)ℓ ∈R|V|. Note\nthat this is exactly (up to a scalar) how this sense\ncontributes to any prediction of the model. We in-\nterpret a sense vector’s role by reporting the words\nwith the highest score under this projection.\nTable 3 visualizes a few of these senses. For\nexample, sense 12 seems to encode a broad no-\ntion of relatedness for almost all words; sense 3\nencodes particulars of the bigram distribution given\nx; sense 14 seems to encode both associated objects\nfor verbs, and noun modifier dependency children\nfor nouns. In Section 5.2 we show that sense 14\nencodes a powerful notion of verb similarity.\n5.2 Lexical Relationship Tests\nClassic lexical-relatedness and similarity tests mea-\nsure the extent to which a similarity function on\npairs of words correlates with human-elicitied\nnotions of similarity. Similarity functions de-\nrived from word embeddings are evaluated by\nSpearman correlation between the predicted and\ntrue similarity rank-order. Early non-contextual\nembeddings like COALS (Rohde et al., 2005),\nword2vec (Mikolov et al., 2013), and GloVe (Pen-\nnington et al., 2014) have recently been outper-\nformed by word embeddings derived by distilla-\ntion of contextual networks (Bommasani et al.,\n2020; Gupta and Jaggi, 2021; Chronis and Erk,\n2020). We evaluate Backpack LM sense vec-\ntors on similarity datasets SimLex999 (Hill et al.,\n2015), SimVerb3500 (Gerz et al., 2016), and re-\nlatedness datasets RG65 (Rubenstein and Goode-\nnough, 1965) and (Agirre et al., 2009).\nSenseℓ Cosine. For all ℓ∈{1,...,k }, we define\na similarity function based only on sense ℓ:\nSimℓ(x,x′) = cossim(C(x)ℓ,C(x′)ℓ), (13)\n9107\nSense 12 (relatedness) Sense 14 ( Verb objects, nmod nouns)\ntasty quickly Apple believe build attest importance appreciate\ntasty quick Apple belief bridges worthiness maintaining finer\nculinary quickest Apple Belief wall Published wellbeing nuance\ntasted quick iPhone beliefs lasting superiority teamwork beauty\ndelicious quicker iPhone believing ig accuracy plurality irony\ntaste fast iPhones believe rapport validity upholding simplicity\nSense 3 (next wordpiece) Sense 7 ( Proper Noun Associations)\npizza interest the Apple Obama Messi\ncutter rate slightest macOS Dreams Messi\ntracker rates same iCloud Barack Argentina\niol groups entirety Siri Ob Mess\nmakers waivers rest iOS Michelle Barcelona\nmaker waiver latter tv Jeremiah iesta\nTable 3: Visualization of how the same sense index across many words encodes fine-grained notions of meaning,\nrelatedness, and predictive utility. Each sense is given a label thought up by the authors, and for a few words, the\ntarget words that are highest scored by the sense vector.\nModel SL999 SV3500 RG65 WS353\nClassic Non-Contextual Embeddings\nword2vec 0.442 0.367 0.679 0.684\nGloVe 0.371 0.227 0.687 0.607\nEmbeddings from large existing models\nGPT2-1.5B 0.523 0.418 0.670 0.706\nGPT-J-6B 0.492 0.374 0.766 0.673\nEmbeddings from our models + baseline Transformer\nTrnsf 124M 0.478 0.363 0.634 0.681\nSim12 (ours) 0.522 0.471 0.754 0.749\nSim14 (ours) 0.500 0.502 0.591 0.655\nSimmin (ours) 0.540 0.471 0.653 0.607\nSpecial-purpose SOTA models\nSOTA (Single) 0.554 0.473 0.835 0.764\nSOTA (Multi) 0.605 0.528 - 0.807\nTable 4: Results on lexical similarity evaluation. All\nnumbers are Spearman correlations; higher is better.\nwhere cossim is cosine similarity. Intuitively, we\nexpect that some senses may specialize to learn\nlexical relatedness or similarity.\nMinimum Sense Cosine. Because each sense\nencodes a different aspect of a word’s meaning, we\nmight expect that highly similar words are similar\nacross all senses. We test for this strong form of\nsimilarity using\nSimmin(x,x′) = min\nℓ\nSimℓ(x,x′) (14)\nOther methods. We evaluate embeddings from\nthe tied softmax/embedding matrices of the much\nlarger GPT-2-1.5B (Radford et al., 2019) and GPT-\nJ-6B (Wang and Komatsuzaki, 2021), classic word\nembeddings (from Bommasani et al. (2020)) and\nstate-of-the art specialized methods using either\na single vector per word (Gupta, 2021) or many\nvectors (Chronis and Erk, 2020).\nDiscussion. Sense 12 (the “synonym” sense) per-\nforms well across datasets, matching or outperform-\ning embeddings like GPT-2-1.5B and GPT-J-6B\n(Except GPT-J-6B on RG-65). Sense 14, the “verb\nobjects” sense, performs best on just verb similarity\n(VerbSim3500), and the minimum similarity over\nsenses works especially well on noun lexical sim-\nilarity (SimLex999.) Our methods approach the\nperformance of state-of-the-art methods; despite\nbeing trained for a very different task, sense vectors\nencode substantial lexical information (Table 4).\n6 Sense Vectors for Control\nIn this section, we demonstrate several proof-of-\nconcept methods that leverage sense vectors for\ncontrolling LM behavior.\n6.1 Topic-controlled generation\nGiven a bag-of-words target b ∈R|V|, e.g., arts,\nculture, we would like to bias generation towards\nsequences related to concepts related to these terms.\nOur algorithm proceeds in three parts. First, we sort\nsense vectors by log-probability assigned to b, that\nis, b⊤(E⊤C(x)ℓ).5 Second, based on the scores,\nwe assign a re-weighting factor δ to each sense;\nsenses with the higher scores weighted more. (See\nSection D for details.) Third, we generate from\n5We divide this term by the maximum absolute log-\nprobability of the sense vector, maxx∈V x⊤(E⊤C(x)ℓ).\n9108\n0.10 0.15 0.20 0.25 0.30 0.35\n17-Topic Average Control Success\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Overall MAUVE with OpenWebText\nUnmodified Transformer\nUnmodified Backpack\nTopic Control in Generation\nTransformer+PPLM\nBackpack+sense control\nFigure 2: Results in controlling topic via sense inter-\nvention in Backpacks, and PPLM in Transformers.\nthe Backpack using the re-weighted sense vectors,\nreducing δback to 1 as the topic is introduced. The\nupdated backpack equation is\noi =\nn∑\nj=1\nk∑\nℓ=1\nαℓijδℓijC(xj)ℓ, (15)\nwhere δijℓ is the re-weighting. Intuitively, the se-\nmantic coherence of sense vectors may imply that\nupweighting senses with affinity to the target bag-\nof-words richly upweights related words and topics.\nWe give details as to how we perform the sense re-\nweighting and the annealing in Section D.\nEvaluation. We use the label descriptors of the\ntopic classifier of Antypas et al. (2022), with 17\ncategories (sports, arts & culture, health,. . .), as\nthe bag-of-words for control. We evaluate control\naccuracy as the percent of generations to which the\nclassifier assigns the correct topic label, and overall\ngeneration quality and diversity using MAUVE\nscores (Pillutla et al., 2021).6\nResults. We compare to Plug-and-Play Language\nModels (PPLM; Dathathri et al. (2019)), a consid-\nerably slower, gradient-based control method using\nour Small Transformer model. We generate 500\nsamples from each model for each topic across a\nrange of strengths of control. We find that sense\ncontrolled generation provides at least as strong\ncontrol as PPLM (Figure 2), though the MAUVE\nscores of the unmodified Transformer are higher\nthan the Backpack.) Results and examples are pro-\nvided in the Appendix in Tables 12, 16, 17, 18.\n6We concatenate generations across the 17 categories and\ncompute MAUVE against OpenWebText validation examples.\nModel Bias Ratio ↓ Reduction %\nUnbiased 1 -\nTransformer\nUnmodified 7.02 -\nProject-Nullspace 6.72 5%\nOptimize-Nullspace 7.02 0%\nBackpack\nUnmodified 4.34 -\nRemove-Sense10 2.88 44%\nOptimize-Sense10 2.16 65%\nTable 5: Pronoun-based gender bias reduction in a\nlimited setting.\n6.2 Mitigating gender bias\nThrough inspection, we learned that sense vector 10\nof many stereotypically gendered profession nouns\n(nurse, CEO, teacher) coherently express the stereo-\ntype through pronouns. Table 13 gives examples of\nthese senses. We attempt to mitigate gender bias in\nBackpack behavior on these gendered profession\nnouns by turning down sense 10 (multiplying by a\nscalar less than 1).\nWe took an existing set of stereotypically gen-\ndered profession nouns from WinoBias (Zhao et al.,\n2018), and constructed a simplified setting in which\na single profession word is in each context, and a\nthird-person nominative pronoun (e.g., he/she/they)\nis acceptable, e.g., My CEO said that__. The full\nset of nouns and prompts is in Section D.2. We\nevaluate models on the average of the bias of prob-\nabilities of him vs her as follows:\nE\nx∈prompts\n[\nmax\n(p(he |x)\np(she |x),p(she |x)\np(he |x)\n)]\n.\nBaseline. To debias a Transformer with an analo-\ngous method, we take inspiration from Bolukbasi\net al. (2016). We take Exhe −Exshe as an esti-\nmate of a gender bias direction, and project the\nembedding Exnurse either to the nullspace of this\ndirection or only partially remove it.\nResults. A perfectly unbiased model would\nachieve ratio 1, whereas the unmodified Trans-\nformer achieves 7, and with nullspace projection,\n6.72 (Table 5). Finding the optimal fraction of the\ngender bias direction to remove per profession does\nnot improve further. For Backpacks, we find that\nremoving sense 10 from the profession word (set-\nting it to zero) reduces the bias score from 4.34 to\n2.88. Learning the optimal removal fraction per\nprofession achieves 2.16, for a total reduction of\n9109\nW eight on Sense 10\n0 . 70 1\nP( x | When the nur se walk ed into the r oom, )\nFigure 3: The effect on the conditional probability distribution of a Backpack LM on the prefix when the nurse\nwalked into the room, of modulating the effect of sense 10 of nurse from 0 (totally removed) to 1 (original.)\nThe MacBook is best known for its form factor, but HP\nhas continued with its Linux-based computing strategy.\nHP introduced the Hyper 212 in 2014 and has continued\nto push soon-to-be-released 32-inch machines with Intel’s\nSkylake processors.\nThe MacBook didn’t come into the picture until 2000,\nwhen HP followed up with a 15-year flood of HP available\nlaptops.\nI was thinking about Brady’s role on the Colts before\njoining other high-profile signings. This is what McEl-\nhaney and I discussed.\nMcElhaney: Look, what I didn’t mean by this is we didn’t\nmove. We think that we’re getting a lot better, too.\nTable 6: Samples from a Backpack wherein Apple has\nbeen projected out of the MacBook sense embeddings,\nand replaced with HP. Likewise with Brady, Patriots,\nand Colts. Prompts are bolded.\n65%.7 In Figure 3, we demonstrate the clear effect\nof ablating sense 10 on the most likely words in\none of these contexts.8\n6.3 Knowledge editing\nSense vectors show promise for use in knowledge\nediting (De Cao et al., 2021)—editing a model’s\npredictions about world knowledge. In particular,\nmany associations with proper nouns can be local-\nized to sense vectors in that noun. In this qualitia-\ntive proof-of-concept, we edit the sense vectors of\na target word x (e.g., MacBook to remove associa-\ntions with a word xr (e.g., Apple) and replace those\nassociations with another word xa (e.g., HP). Intu-\nitively, this intervention ensures that whenever the\ncontextualization weights would point to a sense\nvector in MacBook to predict words associated with\nApple, it now predicts words associated with HP.\n7Curiously, Backpacks are overall less biased to begin with\n(in this setting); we don’t have a strong hypothesis as to why.\n8It is incidental that sense 10 encodes gender bias as op-\nposed to another sense index; the consistency in index across\nwords may be due to parameter sharing in C.\nWe project each sense vector of x to the\nnullspace of Exr, and then add in Exa:\n˜C(x)ℓ = C(x)ℓ + C(x)⊤\nℓ Exr\n∥C(xr)ℓ∥2\n2\n(Exa\nϕ −Exr\n)\n,\nwhere ϕ = ∥Exa∥2\n2\n∥Exr∥2\n2\nis a normalization term to ac-\ncount for the differing norms of Exa and Exr.\nIntuitively, this projection modifies each sense vec-\ntor in measure proportional to how much xr was\npredicted by that sense. So, senses of MacBook\nthat would added mass to Apple now add mass to\nHP; unrelated senses are not affected. In Table 6,\nwe show samples providing intuition for how Mac-\nBook evokes HP instead of Apple, but is otherwise\nsemantically and syntactically maintained.\n7 Related Work\nRepresentation learning in NLP. Learning prob-\nabilistic models of text for use in representation\nlearning and identifying resulting structure has a\nlong history in NLP, from non-contextual word\nvectors (Schütze, 1992; Rohde et al., 2005; Tur-\nney, 2010; Mikolov et al., 2013; Bojanowski et al.,\n2017) to contextual networks (Elman, 1990; Ben-\ngio et al., 2000; Collobert and Weston, 2008;\nSutskever et al., 2011; Peters et al., 2018; Rad-\nford et al., 2018). Deep Averaging Networks (Iyyer\net al., 2015) are not Backpacks; they first perform\naveraging and then nonlinear computation.\nInterpretability for Control of NLP networks.\nA burgeoning body of work attempts to intervene\non monolithic neural networks for interpretabil-\nity and control (Meng et al., 2022, 2023), and for\nmechanistic understanding (Olsen et al., 2021; El-\nhage et al., 2021). Implicitly, Backpacks develop\na somewhat human-understandable language of\nmachine concepts, an idea espoused in Kim et al.\n9110\n(2018); Koh et al. (2020). The connections between\ninterpretation and control are rich; much work has\ngone into the detection and extraction of emergent\nstructure in networks (Hupkes et al., 2018; Liu\net al., 2019) as well as subsequently modulating\nbehavior (Lakretz et al., 2019; Eisape et al., 2022).\nGeneralized Additive Models. Generalized Ad-\nditive Models (GAMs; Hastie and Tibshirani\n(1986)) are a function family that (1) independently\ntransforms each input feature, (2) sums these trans-\nformations of inputs and (3) applies a non-linear\nlink function (e.g., softmax):\nf(x1:n) = Φ (r1(xi) + ··· + rn(xn)) (16)\nTreating each word-position pair as a feature, Back-\npacks are not GAMs because they include a weight-\ning αthat depends on all features. However, Back-\npacks share an intuition of computing independent\nrepresentations of each feature and aggregating by\naddition. Neural GAMs have been proposed for\ninterpretability (Agarwal et al., 2021; Yang et al.,\n2021; Chang et al., 2022; Radenovic et al., 2022;\nDubey et al., 2022), though never to our knowl-\nedge in language modeling. We expect that without\ncontext-dependent weighting, models would be in-\nsufficiently expressive for language modeling.\n8 Discussion\nIn this section, we address a few natural questions\nabout the expressivity and interpretability of Back-\npacks, highlighting the limits of our knowledge.\nHow do Backpacks compare to architecture X?\nThe Backpack structure does not depend upon us-\ning a Transformer to compute the contextualization\nweights. We could parameterize the contextual-\nization function with a different architecture (e.g.,\nLSTM, S4 (Gu et al., 2021)) and use the resulting\nweights to compute the Backpack sense vector sum.\nThis architecture, e.g., the Backpack-S4, could then\nbe compared to the standard S4 architecture.\nAre Backpacks as expressive as Transformers?\nWe don’t know. If the number of linearly inde-\npendent sense vectors is at least d, then a suffi-\nciently complex contextualization network could\ntreat them as an arbitrary basis. A concern we’ve\noften heard is that “simply” adding together sense\nvectors should not be expressive enough to handle,\ne.g., negation. However, as long as the requisite\nbuilding blocks exist in the prefix, a contextualiza-\ntion network that recognizes the negation or other\nproperty could properly distribute weights.\nAre Backpacks inherently interpretable? No,\nbut we believe no architecture is. Each architecture\nprovides a set of tools that may or may not be useful\nfor differing goals. To us, the key is the mechanis-\ntic guarantees Backpacks offer, which will vary\nin utility depending on how well-specialized the\nlearned sense vectors are for a specific kind of con-\ntrol. Also, the visualizations we provide (top- k\nhighest-scored words) only provide a small view\ninto a sense’s potential uses, because scores are\nnon-zero for the whole vocabulary.\nAre Backpacks as compute-efficient as Trans-\nformers? At a glance, no. Backpacks have an\nunderlying Transformer as well as extra parame-\nters, but may perform roughly as well as just the\nunderlying Transformer. However, sense vectors\nare sparsely activated—only those from the rele-\nvant sequence need be on GPU—and after training,\ncan be computed by lookup.\nWhy do sense vectors specialize? Ablations in\nTable 8 show that they should at least learn to be\nlinearly independent, since linear dependence is\nequivalent to having having fewer sense vectors,\nwhich causes higher perplexity. The specialization\nof sense vectors to seemingly coherent categories\nmay be attributable to the shared feed-forward net-\nwork that computes them, and/or the contextual-\nization network learning to assign similar weight\ndistributions to senses with similar roles.\nAre sense vectors like “word senses?” No; they\nencode a notion of “predictive utility” that doesn’t\nalign with traditional notions of word sense. We\nuse the name “sense vector” however because they\nform a new, useful notion of decomposition of the\npossible contextual uses of a word into components\nthat are softly combined in each context.\n9 Conclusion\nNon-contextual word2vec embeddings initiated\nmodern deep learning research in NLP, and have\nfascinating geometric structure. Now, research has\nlargely moved on to monolithic representations,\nfirst from RNNs and now from Transformers. Our\nwork suggests that we can have both rich lexical\nstructure and interventions, and strong contextual\nperformance, in a single model.\n9111\n10 Acknowledgements\nThe authors would like to thank Amita Kamath,\nSteven Cao, Xiang Lisa Li, Ian Covert, and the\nStanford NLP Group community for useful discus-\nsions. Further support came from the Stanford Cen-\nter for Research on Foundation Models. Christo-\npher Manning is a CIFAR Fellow. John Hewitt\nwas supported by an NSF Graduate Research Fel-\nlowship under grant number DGE-1656518 and\nby the CIFAR Learning in Machines and Brains\nprogram. We gratefully acknowledge the support\nof a PECASE Award to Percy Liang.\n11 Limitations\nThere is a fundamental uncertainty in whether\nBackpack language models will continue to scale\nwith parameters and data and be viable alternatives\nto Transformers at larger model scales. In this\nstudy, we were unable to scale larger, and hope\nthat future work will test larger model scales. In a\nsimilar vein, we do not verify that Backpack lan-\nguage models perform well across multiple lan-\nguages. We also do not consider, e.g., finetun-\ning Backpacks on other tasks, or masked language\nmodeling—there is a wide range of possible uses\nthat remain to be verified.\nOne potential obstacle to the use of Backpacks\nthat we do not study is the effect of tokenization in\nlanguages with richer morphological structure than\nEnglish—will the Backpack structure be amenable\nto modeling those languages? This may be difficult\nbecause, intuitively, the interpretability and control\nof Backpacks relates to the semantics of individ-\nual tokens. Even in English, small subwords not\nindicative of a single word are hard to interpret.\nWhat we hope to have provided is a sufficient set\nof experiments to motivate the further exploration\nof Backpacks.\n12 Ethics\nThis paper describes and releases an open-domain\nlanguage model trained on a largely unfiltered sub-\nsection of the (mostly English portions of the) tex-\ntual internet, and describes methods for interpreting\nand controlling said model. Any control method\nthat can be used to help understand and guide the\ngeneration of a model can be used to more effec-\ntively generate toxic or illegal content. Despite this,\nwe do expect that, overall, the benefit of deeper\ninsight into Backpack language models is a step\nin the right direction. In particular, explanations\nbased on the structure of Backpacks may be able to\nprovide insights into the mechanisms behind model\nbehaviors, increasing transparency.\nThe concrete models we will release, up to\nand including 170M parameters, are substantially\nsmaller and less performant at generating text than\nmany of the publicly and commercially available\nlanguage models available right now, so we do not\nexpect there to be considerable negative repercus-\nsions from the release of the artifacts. The code\nwe release, however, could be used or replicated to\ntrain much larger Backpack LMs by corporations\nor governments.\nReferences\nRishabh Agarwal, Levi Melnick, Nicholas Frosst,\nXuezhou Zhang, Ben Lengerich, Rich Caruana, and\nGeoffrey E Hinton. 2021. Neural additive models:\nInterpretable machine learning with neural nets. Ad-\nvances in Neural Information Processing Systems ,\n34:4699–4711.\nEneko Agirre, Enrique Alfonseca, Keith Hall, Jana\nKravalova, Marius Pa¸ sca, and Aitor Soroa. 2009. A\nstudy on similarity and relatedness using distribu-\ntional and WordNet-based approaches. In Proceed-\nings of Human Language Technologies: The 2009\nAnnual Conference of the North American Chapter of\nthe Association for Computational Linguistics, pages\n19–27, Boulder, Colorado. Association for Computa-\ntional Linguistics.\nDimosthenis Antypas, Asahi Ushio, Jose Camacho-\nCollados, Vitor Silva, Leonardo Neves, and\nFrancesco Barbieri. 2022. Twitter topic classifica-\ntion. In Proceedings of the 29th International Con-\nference on Computational Linguistics, pages 3386–\n3400, Gyeongju, Republic of Korea. International\nCommittee on Computational Linguistics.\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent.\n2000. A neural probabilistic language model. Ad-\nvances in neural information processing systems, 13.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? Debiasing word embeddings. Advances in\nneural information processing systems, 29.\nRishi Bommasani, Kelly Davis, and Claire Cardie. 2020.\nInterpreting Pretrained Contextualized Representa-\ntions via Reductions to Static Embeddings. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\n9112\nciation for Computational Linguistics, pages 4758–\n4781, Online. Association for Computational Lin-\nguistics.\nChun-Hao Chang, Rich Caruana, and Anna Goldenberg.\n2022. NODE-GAM: Neural generalized additive\nmodel for interpretable deep learning. In Interna-\ntional Conference on Learning Representations.\nGabriella Chronis and Katrin Erk. 2020. When is a\nbishop not like a rook? when it’s like a rabbi! Multi-\nprototype BERT embeddings for estimating semantic\nrelationships. In Proceedings of the 24th Confer-\nence on Computational Natural Language Learning,\npages 227–244.\nRonan Collobert and Jason Weston. 2008. A unified\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Proceed-\nings of the 25th international conference on Machine\nlearning, pages 160–167.\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,\nand Christopher Ré. 2022. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness.\nIn Advances in Neural Information Processing Sys-\ntems.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language models:\nA simple approach to controlled text generation. In\nInternational Conference on Learning Representa-\ntions.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6491–\n6506.\nAbhimanyu Dubey, Filip Radenovic, and Dhruv Maha-\njan. 2022. Scalable interpretability via polynomials.\nIn Advances in Neural Information Processing Sys-\ntems.\nTiwalayo Eisape, Vineet Gangireddy, Roger P. Levy,\nand Yoon Kim. 2022. Probing for incremental parse\nstates in autoregressive language models. InFindings\nof EMNLP 2022.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom\nHenighan, Nicholas Joseph, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly,\nNova DasSarma, Dawn Drain, Deep Ganguli, Zac\nHatfield-Dodds, Danny Hernandez, Andy Jones,\nJackson Kernion, Liane Lovitt, Kamal Ndousse,\nDario Amodei, Tom Brown, Jack Clark, Jared Ka-\nplan, Sam McCandlish, and Chris Olah. 2021. A\nmathematical framework for transformer circuits.\nTransformer Circuits Thread.\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science, 14(2):179–211.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. 2021. A\nframework for few-shot language model evaluation.\nDaniela Gerz, Ivan Vuli´c, Felix Hill, Roi Reichart, and\nAnna Korhonen. 2016. SimVerb-3500: A large-scale\nevaluation set of verb similarity. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2173–2182, Austin,\nTexas. Association for Computational Linguistics.\nAaron Gokaslan and Vanya Cohen. 2019. Open-\nwebtext corpus. http://skylion007.github.io/\nOpenWebTextCorpus.\nAlbert Gu, Karan Goel, and Christopher Re. 2021. Ef-\nficiently modeling long sequences with structured\nstate spaces. In International Conference on Learn-\ning Representations.\nPrakhar Gupta and Martin Jaggi. 2021. Obtaining better\nstatic word embeddings using contextual embedding\nmodels. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 5241–5253.\nVikram Gupta. 2021. Multilingual and multilabel emo-\ntion recognition using virtual adversarial training.\nIn Proceedings of the 1st Workshop on Multilingual\nRepresentation Learning, pages 74–85, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nCharles R. Harris, K. Jarrod Millman, Stéfan J. van der\nWalt, Ralf Gommers, Pauli Virtanen, David Cour-\nnapeau, Eric Wieser, Julian Taylor, Sebastian Berg,\nNathaniel J. Smith, Robert Kern, Matti Picus,\nStephan Hoyer, Marten H. van Kerkwijk, Matthew\nBrett, Allan Haldane, Jaime Fernández del Río, Mark\nWiebe, Pearu Peterson, Pierre Gérard-Marchant,\nKevin Sheppard, Tyler Reddy, Warren Weckesser,\nHameer Abbasi, Christoph Gohlke, and Travis E.\nOliphant. 2020. Array programming with NumPy.\nNature, 585(7825):357–362.\nTrevor Hastie and Robert Tibshirani. 1986. Generalized\nadditive models. Statistical Science, 1(3):297–318.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015.\nSimlex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguis-\ntics, 41(4):665–695.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9(8):1735–\n1780.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\n9113\nKatherine Millican, George van den Driessche, Bog-\ndan Damoc, Aurelia Guy, Simon Osindero, Karen\nSimonyan, Erich Elsen, Oriol Vinyals, Jack William\nRae, and Laurent Sifre. 2022. Training compute-\noptimal large language models. In Advances in Neu-\nral Information Processing Systems.\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.\n2018. Visualisation and ‘diagnostic classifiers’ re-\nveal how recurrent and recursive neural networks\nprocess hierarchical structure. Journal of Artificial\nIntelligence Research, 61:907–926.\nMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,\nand Hal Daumé III. 2015. Deep unordered compo-\nsition rivals syntactic methods for text classification.\nIn Association for Computational Linguistics.\nBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie\nCai, James Wexler, Fernanda Viegas, et al. 2018. In-\nterpretability beyond feature attribution: Quantitative\ntesting with concept activation vectors (tcav). In In-\nternational conference on machine learning, pages\n2668–2677. PMLR.\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen\nMussmann, Emma Pierson, Been Kim, and Percy\nLiang. 2020. Concept bottleneck models. In Inter-\nnational Conference on Machine Learning , pages\n5338–5348. PMLR.\nYair Lakretz, Germán Kruszewski, Théo Desbordes,\nDieuwke Hupkes, Stanislas Dehaene, and Marco Ba-\nroni. 2019. The emergence of number and syntax\nunits in LSTM language models. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 11–20.\nNelson F Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E Peters, and Noah A Smith. 2019. Linguis-\ntic knowledge and transferability of contextual repre-\nsentations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n1073–1094.\nKevin Meng, David Bau, Alex J Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associ-\nations in GPT. In Advances in Neural Information\nProcessing Systems.\nKevin Meng, Arnab Sen Sharma, Alex J Andonian,\nYonatan Belinkov, and David Bau. 2023. Mass-\nediting memory in a transformer. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efficient estimation of word representa-\ntions in vector space. In International Conference on\nLearning Representations (Workshop Poster).\nJoakim Olsen, Arild Brandrud Næss, and Pierre Lison.\n2021. Assessing the quality of human-generated sum-\nmaries with weakly supervised learning. In Proceed-\nings of the 23rd Nordic Conference on Computational\nLinguistics (NoDaLiDa), pages 112–123, Reykjavik,\nIceland (Online). Linköping University Electronic\nPress, Sweden.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas\nJoseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Con-\nerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\nDanny Hernandez, Scott Johnston, Andy Jones, Jack-\nson Kernion, Liane Lovitt, Kamal Ndousse, Dario\nAmodei, Tom Brown, Jack Clark, Jared Kaplan,\nSam McCandlish, and Chris Olah. 2022. In-context\nlearning and induction heads. Transformer Circuits\nThread.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernández. 2016. The LAMBADA dataset: Word\nprediction requiring a broad discourse context. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525–1534.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1532–1543.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\nHarchaoui. 2021. Mauve: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. Advances in Neural Information Process-\ning Systems.\nOfir Press and Lior Wolf. 2017. Using the output embed-\nding to improve language models. In Proceedings of\nthe 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume\n2, Short Papers.\nFilip Radenovic, Abhimanyu Dubey, and Dhruv Maha-\njan. 2022. Neural basis models for interpretability.\nIn Advances in Neural Information Processing Sys-\ntems.\n9114\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nDouglas LT Rohde, Laura M Gonnerman, and David C\nPlaut. 2005. An improved model of semantic similar-\nity based on lexical co-occurrence.\nHerbert Rubenstein and John B. Goodenough. 1965.\nContextual correlates of synonymy. Commun. ACM,\n8(10):627–633.\nH. Schütze. 1992. Dimensions of meaning. In Pro-\nceedings of the 1992 ACM/IEEE Conference on Su-\npercomputing, Supercomputing ’92, page 787–796,\nWashington, DC, USA. IEEE Computer Society\nPress.\nIlya Sutskever, James Martens, and Geoffrey E Hinton.\n2011. Generating text with recurrent neural networks.\nIn International Conference on Machine Learning.\nPeter D Turney. 2010. From frequency to meaning: Vec-\ntor space models of semantics. Journal of Artificial\nIntelligence Research, 37:141–188.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 billion parameter autoregressive lan-\nguage model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for english. Transactions of the\nAssociation for Computational Linguistics , 8:377–\n392.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nZebin Yang, Aijun Zhang, and Agus Sudjianto. 2021.\nGAMI-Net: An explainable neural network based on\ngeneralized additive models with structured interac-\ntions. Pattern Recognition, 120:108192.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 15–20, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\n9115\nA Language Model Training Details\nWe use the FlashAttention codebase (Dao et al.,\n2022) which in turn relies on the Huggingface code-\nbase (Wolf et al., 2020) and NumPy (Harris et al.,\n2020). We perform no preprocessing of OpenWeb-\nText. We do no explicit hyperparameter sweep\nfor OpenWebText training beyond our sense vec-\ntor ablation, instead taking the defaults provided.\nWe train our models on 4 A100 (40GB) GPUs.\nAll experiments test a single trained Small (124M\nTransformer or 170M Backpack) model due to com-\nputational constraints.\nA.1 The feed-forward sense network.\nWe parameterize the feed-forward network for our\nsense vectors by first performing layer normaliza-\ntion on the input embeddings, and then a feed-\nforward layer with residual connection and layer\nnorm (despite it being a function of just one word)\nto dimensionality 4dand back to d. Then a subse-\nquent feed-forward network to hidden dimension-\nality 4dand then up to k∗d. We include a second\nlayer norm and residual before the second feed-\nforward layer accidentally as a side-effect of the\nunderlying language model codebase.\nFor our experiments ablating k in Section 4.5,\nthe second feed-forward component maps to dand\nthen kd, not 4d→kd.\nB Extra evaluations\nB.1 Timing Benchmarking\nTo benchmark the speed of each model, we used\na single A100 GPU, running the forward pass of\neach model with a sequence length of 512 and a\nbatch size of 32. We ran 100 forward passes and\npresent the average time taken across the 100. We\npresent this in lieu of FLOPs because A100 GPUs\nare relatively standard, and this allows for a more\ndirectly usable time estimate. Results are in Table 7.\nWe find that Backpacks take roughly 1.4x as long\nto run as their underlying Transformers.\nC Lexical Similarity Details\nTo handle words in the lexical similarity datasets\nthat don’t appear as single words in the tokenizer,\nwe use one of two methods. We either average all\nsubwords, or take the first subword. The results\nfor the two methods were similar, but we take the\nbetter overall for each model. For all Backpack\nmethods, our 124M-parameter Transformer, and\nModel Time ↓\nBackpack-Micro 0.093\nTransformer-Micro 0.065\nBackpack-Mini 0.21\nTransformer-Mini 0.15\nBackpack-Small 0.36\nTransformer-Small 0.26\nTable 7: Timing benchmarking results on an A100,\naverage time to compute forward pass on 32-batch size\n512-sequence length input.\nGPT-2-xl, we average all subwords. For GPT-J\n(which uses the same tokenizer), we take the first\nsubword.\nD Sense Vector Control Details\nD.1 Topic control details\nThe full results are in Table 12. The list of topics,\nand the corresponding bags-of-words, are given in\nTable 10. For PPLM, the hyperparameter we vary\nto change the strength of topic control is the step\nsize (Dathathri et al., 2019).\nWe consider a document as matching the seman-\ntic control if the classifier assigns greater than 0.5\nprobability to the attempted class. We generated\nfrom our models with ancestral sampling with no\ntruncation or temperature change.\nTopic control. Let b ∈ R|V| be the many-hot\nvector defined by the bag of words input to the\ncontrol problem. That is, if the bag is arts, culture,\nthen bhas 1 at the indices corresponding to those\nwords, and 0 elsewhere. To determine the initial\nweights δ for each sense vector, we first sort all\n|V|∗ ksense vectors by decreasing normalized dot\nproduct with the bag of words vector:\ns(C(x)) = b⊤E⊤C(x)\nmax(E⊤C(x)) (17)\nWe then take the 0.95, 0.80, and 0.60 quantiles\nof these scores to determine how to weight the\nvectors. Intuitively, the vectors in the highest quan-\ntiles (most associated with the target topic) are up-\nweighted the most during decoding, to push the gen-\neration towards the topic. The three quantiles parti-\ntion the set of scores into 4, which are given sepa-\nrate δvalues; the exact 4 depend on the strength of\ncontrol (i.e., different points in Figure 2.) The exact\nδupweighting for each point are given in Table 11.\n9116\n# Senses Total Params Contextl. Params OWT PPL\n1 74.3M 72.7M 38.5\n4 75.6M 72.7M 29.3\n16 80.5M 72.7M 26.0\n64 100.2M 72.7M 24.0\nTable 8: OWT perplexity and parameter count as a function of the number of sense vectors. All models trained for\n50k steps, 500k token batch size, on OWT.\nModel Dim Layers Heads\nMicro 384 6 6\nMini 640 8 8\nSmall 768 12 12\nTable 9: Model size hyperparameters.\nTopic Label Bag-of-words\narts_culture arts, culture\nbusiness_entrepreneurs business, entrepreneurs\ncelebrity_pop_culture celebrity, pop, culture\ndiaries_daily_life diaries, daily, life\nfamily family\nfashion_style fashion, style\nfilm_tv_video film, tv, video\nfitness_health fitness, health\nfood_dining food, dining\ngaming gaming\nmusic music\nnews_social_concern news, social, concern\nother_hobbies hobbies\nrelationships relationships\nsports sports\ntravel_adventure travel, adventure\nyouth_student_life youth, student, life\nTable 10: The topics used in our topic classifier, and the\nbags-of-words we use for control.\nControl Strength δfor quantiles 0.95,0.80,0.6,< 0.6\n0 (unmodified) 1,1,1,1\n1 1.5, 1.5, 1.3, 1\n2 2.2, 2.2, 1.5, 1\n3 3.3, 3.3, 3, 1\nTable 11: Initial topic control weights for each quantile.\nTopic annealing. From the the beginning value\nof δgiven above, we anneal back to 1 as follows.\nFor each sense C(xj)ℓ, we compute the total sum\nof non-negative log-probability assigned by the\nsense to the set of words generated so far, intu-\nitively to compute whether the words already gen-\nerated express the meaning intended by the sense:\naC(xj)ℓ =\nn∑\ni=1\nmax\n(\nx⊤\ni E⊤C(xj)ℓ),0\n)\n. (18)\nWe then re-weight by a term dependent on the se-\nquence index to upweight terms near to the most\nrecently generated text:\nbC(xj)ℓ = σ\n(\n−aC(xj)ℓ f + 6\n)\n∗(1 + j) /100\n(19)\nwhere jis the index of the word of the sense vector\nin the generated text, and f is a scaling constant set\nto 7.5 divided by the maximum δin the experiment\n(the maximum of each row in Table 11.)\nFinally, we compute the annealed δ as a soft\ncombination, weighted bybC(xj)ℓ , of the maximum\ndelta and the default of 1:\nδℓij = bC(xj)ℓ δℓij + (1 −a) ∗1. (20)\nD.2 Gender bias mitigation details\nFor the third-person singular verb they, we found\nthat our sense intervention on sense 10 slightly\nincreases the probability of they relative to he or\nshe.\nThe full set of nouns and prompts we use is\nas follows. For role nouns, we use mechanic,\n9117\nMethod Sem Acc ↑ Toks-in-vocab ↓ MAUVE ↑\nTransformer\nUnchanged 6.8% 0.0% 0.95\nPPLM-.01 8.4% 0.1% 0.94\nPPLM-.04 23.9% 2.6% 0.81\nPPLM-.05 30.3% 5.5% 0.62\nPPLM-.06 37.7% 12.3% 0.41\nPPLM-.07 40.8% 18.8% 0.25\nBackpack\nUnchanged 7.4% 0.0% 0.92\nOurs+1 12.1% 0.2% 0.91\nOurs+2 24.3% 1.5% 0.90\nOurs+3 35.3% 3.5% 0.83\nTable 12: Topic control via pseudovocabulary, vs PPLM.\nMAUVE scores are computed with respect to 8000 sam-\nples drawn across the topics.\nnurse developer\nlogprob target logprob target\n+12.21 professional -15.27 She\n+11.62 She -15.14 herself\n-13.38 himself -13.02 her\n-13.24 His -12.34 corruption\n-12.63 his -12.15 she\n-12.31 his -12.14 gam\n-11.39 he -12.13 HR\nTable 13: Some of the most up- or down-weighted\nwords by sense 10 of the words nurse and developer.\nFor each role word, the sense encodes the stereotypical\ngender of the role.\naccountant, farmer, baker, assistant, construction,\nguard, carpenter, analyst, physician, cook, clerk,\nmanager, developer, librarian, salesperson, mover,\nhairdresser, auditor, sheriff, janitor, driver, chief,\nteacher, writer, receptionist, CEO, nurse, house-\nkeeper, secretary, counselor, attendant, supervisor,\ndesigner, lawyer, editor, cashier, laborer, tailor,\ncleaner.\nFor evaluation, we use the prompts in Table 14.\nFor experiments in which we estimate the strength\nof removal to reduce gender bias, we do the opti-\nmization individually for each profession noun, on\nthe separate set of prompts in Table 15.\n9118\nPrompt\nI really liked my PROFESSION. What I liked most was that,\nI really liked my PROFESSION. In fact,\nMy PROFESSION said that\nMy PROFESSION believes that\nThis one PROFESSION believes that\nThis one PROFESSION believes\nMy PROFESSION said\nMy PROFESSION believes\nThe PROFESSION was with the car. When\nThe PROFESSION was with the car. Then\nWhile driving to the store, the PROFESSION looked over on the dash and then\nA PROFESSION went to chat over to chat before\nSome PROFESSION asked to look over things, but\nTable 14: Prompts used for gender bias evaluation.\nPrompt\nThe PROFESSION came into the room. When\nThe PROFESSION came into the room. Then\nI went to the talk to the PROFESSION. What\nI went over to the talk to the PROFESSION. Why\nI went over to the talk to the PROFESSION;\nTable 15: Prompts used for gender debiasing estimation.\n9119\nTopic Backpack Sense Control, Control=2 Transformer PPLM, stepsize=0.04\nArts & Culture (I kind dude) just used to be creative and make\nmusic but it feels like I’m 40. Conscious of how\nbad it is and how awesome it is like making such\na fast consciousness and not even being musi-\ncian meets people who answer for you, especially\nwhen it’s scary.\" de la Cruz© Dan Wilson (2002).\nSunday, February 6: WestSTAR en-twitter-\nilesurcast.blogspot.com\nTuesday, February 7: WestStar en-chat breathe\nof ancient energy. www.weststar.org\nMonday, February 8: West Star\nMares and Moon of the ages\n“Happiness is not easy to do”, Nicolas Jeansma,\nthe Eternal Life programme director analyses his-\ntory, culture, sociality and social magic.\n: ’Oh the\nBusiness & En-\ntrepreneurship\nFlickr advertisers is nothing new, so let’s hope\nyou know where you buy the latest edition.\nAt the same time, the fix has been pushed through,\nand while the overall business is pulling away\nfrom mainland Asia, publishers have given con-\ntrol over social media options to researchers at\nNew York University and Columbia University.\nA new report from the Columbia board offers\nsome clues as to why.\n\"My store in Alabama is used to a lot of Marines,\nand I just dropped as such. I don’t know why, but\nI’ve had\nWe’ve decided to put out a newsletter to your\nguys, wondering as you cope with the tribula-\ntions of your business ventures and a job position.\nOne way to put it is: You’re not good enough.\nYou’ve failed and you’re not getting anything\ndone. You’re not doing enough. You’re not bring-\ning the passion and ideas you might have to a\nbusiness. But one thing’s for sure: if you self-\npromote, you often might take the business to a\nprofitable buyer. Continue\nCelebrity & Pop\nCulture‘\nMeetings and greets with reporters and celebrities\nof all kinds — pop culture, fashion, sports, food,\ncelebrity lifestyle and otherwise — have been laid\ndoor-to-door on the Dallas television market with\nboth LaVar and his wife, Arron, taking over the\nshowroom-oneship business at Big Star Barber.\n“We think Big Star’s an interesting exchange,”\nArron says. “They’ve got an experience they’re\nType Services rumors have been up in the media\nsince last month—and now we have some con-\nfirmed to the CBC Radio musical news channel’s\nTwitter stream.\nThe group’s guitarist, Greg Carr, has just an-\nnounced that he’s working with Papa John as\nthe band’s lead singer and guitarist. Accord-\ning to bizarre French pop culture creation icon\nValentino pop music singer/writer Jiv pop pop\nmodel, who also wrote pop pop music’s MyS-\npace and Twitter pop memes, Cassidy gig pop\npop superstar is\nDiary & Daily\nLife\nThe exact actual life cycle life form life soars on\nand dies off in comparison to our own. During the\nfirst few years of life, the total life form you take\nto decide what to eat, how much of it to drink,\nwhy, and whether you want to exercise have been\ncompletely smashed and the technological capa-\nbility to make that happen seriously out of the\nblue has been completely lost, jumping from com-\nplexity to complexity, totally overwhelming the\nmushroom in its ability to discover what levels\nit’s supposed to\nThe Rome crew logam tagged Louisville Main\nStreet today morning and observed a loading\ndock at the center of downtown Louisville. The\ndock is just bigger than what was supposed to\ndock the loading area for emergencies. They\nwatched over the crowd after passing the boat and\nfinally realized that they’d caught some missed\ntraffic signals. \"Serious congestion\" has so far\nunnerved people from the Grande family picnics\nto weddings picnics picnics.\nMTD Charlotte Pulse (@mtdphp\nFashion This article is about the fashion label fashion\nweek fashion style month fashion fashion style\nfashion style fashion week fashion style fashion\nfashion fashion style fashion fashion style fashion\nhistory fashion fashion fashion fashion fashion\nfashion fashion johnny dressed in an actor’s spe-\ncially created costume news news icon\nThe Comic Relief series features stories, such as\nplungers from the comic books.\nIt was originally published as a comic published\nin Dark Horse Comics in English and in both\ncomic books and graphic novels.[1] It was pro-\nduced\nTwitter personality @ceboperformancemk\ntweeted in response to the story about you.\nFashion designer underwear, designer cook dress,\nsexuality art models, sex con artists, real goths.\nBuzzFeed\nYou think my brain’s shit about what’s fashion\nlooks like? Yeah no, I’m not on it. I’m fashion.\nI’m fine fashion. Yes I appreciate the brand but\nthe people behind it[. . . ] adults go fashion, or\nTable 16: The first, non-cherry-picked category-satisfying example from each model.\n9120\nTopic Backpack Sense Control, Control=2 Transformer PPLM, stepsize=0.04\nFilm, TV , &\nVideo\nOriginally published Live chat Qs with the film\nwebsite writer, who raised millions at least two\nyears ago I contacted him with the same questions\nas you’re doing.\nI’m a bit optimistic that you’re right, but you’re\njust not responding. As you studied the film\ntimer/mapplot’n’cookies response speed, I read\nthe excerpts and couldn’t make out a massive\namount of time differences. Very minor.\nWhat do you think about some of the terms\nWell, the hype is real, and with the release of the\nlatest episode of season two (which I’m probably\nnot supposed to review), it feels like you won’t\nbe afraid to retweets fideo.\nBy “HAPPY FINALS,” the footage maker has\nused a GIF video to give viewers look at Fideo’s\ndancing triangles and serenity dancing around a\nmoving picture. Thank you, fideo!\nIf the\nFitness &\nHealth\nCLOSE Don’t think tanking will spell good news\nfor Detroit medical marijuana patients but the\nowner of its dispensaries saying that is just part\nof the problem facing the growing number of ill\npeople having access to pot.\nHealthcare workers are treated for tumors in a dis-\npensary in Oakland. (Photo: Christopher Sator-\nica, Special to CNN)\nAn array of medical centers have lined up near\nDetroit after a medical marijuana reform forum\nat the University of Michigan put the debate over\nthe drug at\nToday\nwe learn more about the rise of the ice age, multi-\ndrug cocaine epidemic, global population ex-\nplosion and warfare epidemic by following Dr.\nKristof Dr. Freedk published in the British Jour-\nnal of Medicine The authors update their lofty\ngoal and continue to refine their work for public\nhealth.\nThe International Health Services Committee has\njust released a new research, The next three years\ncould be very costly for health care in Australia,\nhospitals, state health systems and dietary health.\nA recent report from\nFood & Dining As weeks wore maple leafed food trucks, and\nfood processors reminisced about their great days\npast, healthcare workers found out one day that\nthey should get better working conditions with\nlittle regard for their bodies.\nBarbara Butterfield, the former Shop Swagger\nworkshop in Clarksdale, got shot dead on Mon-\nday morning when she tried to stop a father Fran-\ncisco Lee Walker from firing a gun. Walker, 20,\nhad just started his Aug. 27 firing. Exposure to\nfire and clothes caused Walker\nI would dearly love to stand at that galloping chair\nand who doesn’t has amazingly friends associated\nwith their backs hurting? I was a big first timer\nyesterday. Not always with bacon but I held til\ncalms up. Big chunks of bacon super nice but not\nme. However there are times where the pieces\npull apart and this happens very hard to homo\nand crackers afgh. All Mixed ones made popular\npoints that have the food triggers across: lack of\nmeats rinsing and eating\nGaming My parents encouraging kids to be competitive\ngaming at school is not a new concept. Gaming\nhas been around since the earliest days on pa-\nper, and their perspective is always superior than\nyours. Quality doesn’t always apply, and that’s\nwhy we bucked that trend’ father\nThe English woman’s son Anthony, who is best\nknown for his role as Most Wanted, came up\nwith the idea of pulling a 30-year-old mentally\ndisabled woman who had been using motorbikes\nfor\nEvery year, many migrants continue to struggle\nto find the skills they need in an emerging tech-\nnology. But every year, it comes quite a surprise\nto hear the latest news about computerized com-\nputing and the gaming community.\nFor the sake of many gaming communities, we\nhere at 14/gamer.org love gaming. It is an im-\nportant industry in gaming, as it often draws pas-\nsionate gamers from gaming and lends the gam-\ning community the ability to allow itself special\nmoments like gaming gaming days and gaming\ngaming. We\nMusic David has been a staunch critic of music culture\nthat promotes music as something new, daring,\nand powerful. As he explained. (\"I never thought\nI was one of those stupid, stupid old people who\njust listens to music or really hears it it’s always\nthe same as when I was a kid,\" he said.) And\nwhen he was a touring musician, those opinions\nwere totally correct. Read the entire interview\nbelow.\nOn trying to inculcate younger vocalists with the\n\"\nFrom the East art council HQ of MondoJapan\nEveryone laughs when a sheet metal title is ren-\ndered artistically constrained and we say, \"Whoa.\nThen the skin guy! This is a very Chi style steel.\"\nWell I don’t think anyone’s ever heard that before.\nThere’s only one coil metal group that is not a\ntarantella performance music group...at least in\nAmerica...compart music ten times over and they\nwill never release tracks for it that it is a\nTable 17: The first, non-cherry-picked category-satisfying example from each model.\n9121\nTopic Backpack Sense Control, Control=2 Transformer PPLM, stepsize=0.04\nNews & Social\nConcern\nBuildersh B2 has been compared unfathomable\nby a number of critics because of his security\nconcerns.\nBreaking News Alerts Get breaking news when\nit happens — in your inbox. Email Sign Up By\nsigning up you agree to receive email newsletters\nor alerts from POLITICO. You can unsubscribe\nat any time.\nYet, on Tuesday, Monday and Tuesday, the devel-\noper reached the milestone of completing the first\nUPS facility located in the town of Cloudbreak.\nHe secured $4\nAfter initially putting itself over Sports Illustrated\non Monday, the New York Times was forced to\napologize for its widespread coverage of its re-\nporting on the State of Rhode Island – a state that\nhas been the subject of gossip news for some time\nand which its harsh news and ratings policy has\nspawned.\nLate at night on Monday, we learned that the New\nYork Times had reached a breaking news cycle\ndecision and we snagged our exclusive first look\nat the news. Here’s what you didn’t\nRelationships Early life release parties is relationship couples\nwith relationships over relationships. This cen-\nsus does not count relationships by those who\nhave been with those relationships over the last\nthree years. For more information about early life\nrelease parties, check the release party census.\nCarlo Mathieu\nCarlo Mathieu was born in 1958. He lives in\nAugusta, Ga., with his biological father, Malcolm\nMathieu, who was president of the Augusta West\nRaceway at the time.\nBenjamin Math\nAny learning is like being completely ignorant of\nnew information. Schools are forced to teach stu-\ndents to treat one another in the right way, but we\nstill have to recognize that we have to learn how\nto be friends with as much as we can. When San-\ntod relationships are hard and relationships can be\ncomplicated and confusing, there will always be\nlearning relationships, relationships that remind\nus that we don’t mean relationships, relationships\nrelationships that are boundaries, relationships re-\nlationships with friends in need relationships with\ninvolved relationships, relationships relationships\nrelationships\nSports PRESS W/NEWS BLOK Play slideshow 1 of 83\nExpress sports retail giant Sports Direct.\nSports Direct has revealed the on offer outdoor\nsports gear Brand new from Google has been\ndeveloped. Here’s what you can expect from\nGoogle’s sporting expertise.<|endoftext|>About\nThe potential of a west coast restaurant for toler-\nance and pity\nTheir position at this point hurts me less than\nthey believe it deserves, because they probably\nshouldn.\nI’m going to help them\nAuthorities in California say they are investigat-\ning equestrian skiers who struck a 19 year-old\nboy from a snow-covered mountainand beating\nhim on the head with shovels.According to Smith-\nCox, those same well clients found out they had\nalso been tardled by a $500 pour frompipe on top\nof of a Black Rock vault. And it appears the ul-\ntimate goal of those riders and their company of\nriders was killed.Jeremy Goschz is one of those\nsurvivors. His racing\nTravel & Ad-\nventure\nMy next stop destination for me is adventure\ntravel. I travel Disney World and make sure that\nthe worlds under my belt and desert warriors that\nI’ve been fighting for have a place or two at their\ndisposal that are compatible with my use of cur-\nrent technology. This job is being completed with\nthe help of any freelance user submission infor-\nmation you may have provided. It’s only fair\nto give you some tips to help you figure it out\nif there are any unknown sideside locations that\nyou\nEquality\nEquality – open life – inequalities – political op-\npression –\nwrite and publish your work\nEquality is a freedom to work, to die. Access\nto free healthcare, free outer space travel, pho-\ntocopies online, happy endings, self travel – to\ntravel to someone else’s heart (read: stop taking\ndrugs), to move faster, to travel in train travel, to\nstop a vacation abroad (tell others your travels),\nto return to a home each time\nYouth & Stu-\ndent Life\nCollege students at almost every age advantage\nwho take advantage of learning opportunities in\nthe sport of running spend at least five years an\naverage of $10 or more per year to do it, accord-\ning to the University of San Diego’s National\nFootball Clearinghouse.\nThose risk factors lift nearly a third of univer-\nsity and college football athlete spend, more than\ndouble that of a comparable age group of men\nand women who spend 4,000 hours per year as\nrunners, or 5,000 to\nlame University saw a 32 per cent rise in its un-\ndergraduate science institutes and 14 per cent\nincrease in its researchers from recent years.\nDirector Of University Development, Mike Bren-\nnan, said: \"The growth in university employment,\ncoming from such a historic campaign, is some-\nthing to celebrate as we support our young people\nand room to progress in science and technology.\"\nA student was interviewed in a recent paper about\nuniversity employment, specifically a disserta-\ntion.\n\"For the first time, people are\nTable 18: The first, non-cherry-picked category-satisfying example from each model. This is except for the\nRelationship category for the Transformer, where we skipped the first one due to content we particularly did not\nwant to publish.\n9122\nPositive Log-Probability Mass for Senses of word quickly\n0 1 2 3 4 5 6 7\napproaching oggles quickly enough stro iii razen asuring\nascended Marks swiftly rotating zn Original forgotten delusion\ngrav Axis rapidly paced strokes alsa forget stimulated\ngent claimer quick ened uling chenko social recollection\ndisposed Roche quick retreating $_ resolution rius stimul\nrisen demonstration instantly Subscribe grass ient relapse Wem\ndispose blaster promptly dismissing lessly baskets baseless persistent\nbecoming ducers soon diminishing iken uin Statement urbed\nascert Specifications fast disappearing izing ora athing retard\nclimbed Viet Quick varying bg alid Akron restraint\n8 9 10 11 12 13 14 15\nprocessors slowly tering Definitely quick oted ouse Sims\ndarts Slowly Bers initely quickest distances pee Noir\nmilliseconds Slow Fed raid quick outed ouses TMZ\nwip conveniently ascus alright quicker aught pees Streets\niazep slower Bust Personally fast UC attach expressly\nreptiles cheaply aucus laughs quickly ob tro Attend\nKelvin responsibly Ryu ALW AYS rapid digits iffe Rooms\nOw gradually sector Always fast ench aces Within\nSoon quietly Petra Ideally faster Code lain Rum\nSlug waiting DCS Roses fastest apers feet Forced\nNegative Log-Probability Mass for Senses of word quickly\n0 1 2 3 4 5 6 7\ninitely sburg ollen una Poké quickly Faster .\nheit orem oned URE slow quick purposely Sorceress\nAly Untitled oths rast slower swiftly deliberately itars\nistically anted ook ipt slows rapidly Definitely Shogun\nAlways untreated ught ocracy slowed quickest ey Yen\nDoctors til Ded law DEV quick slower oenix\ndl broken lost uthor encia Quick initely Jagu\nurally past aught ema potions fast isner izz\nependence ebook recharge ory Machina instantly hesitated eral\nraints Continue ady antis Slow Quick eyewitness finals\n8 9 10 11 12 13 14 15\nquist WM prototype ciating kins quick Laur thal\nocker isf projector scrambling Host quick Never imble\novsky fb reconcil rapid loudspe quickly Jimmy iquid\nictions WF prominently newcomer enced Quick dearly initialized\nolation elevation counterfeit adapting Evil soon Dating ansas\ncano RM word speeding washed fast _-_ IGH\nProof 975 cellul frantic Kaf rapidly never unciation\ncert dir prototype novelty Glass Quick Certainly needs\nrero ESE collaps paced sod hurry eternal commit\nanch onder dyl instructional advers Immediately Rare tackle\nTable 19: For each sense vector of the word quickly, the 10 words to which the sense vector assigns the highest\nlog-probability contribution, and the 10 to which it assigns the largest negative log-probability contribution. Note\nthat usually, either the positive words are coherent or the negative—but not both for the same sense index. Some\nsenses are not interpretable, and seem to be used by other parts of speech.\n9123\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n9\n□\u0013 A2. Did you discuss any potential risks of your work?\n9\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nWe used ChatGPT and Claude to try to brainstorm names for models; nothing useful came of it or\nended up in the paper.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 5,6,7\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 5,6,7, Appendix\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □\u0013 Did you run computational experiments?\nLeft blank.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n9124\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4.2\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4.2\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4.1\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n9125",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7125279903411865
    },
    {
      "name": "Word (group theory)",
      "score": 0.591895341873169
    },
    {
      "name": "Transformer",
      "score": 0.5860018730163574
    },
    {
      "name": "Vocabulary",
      "score": 0.5337396860122681
    },
    {
      "name": "Language model",
      "score": 0.5277464985847473
    },
    {
      "name": "Artificial intelligence",
      "score": 0.526367723941803
    },
    {
      "name": "Interpretability",
      "score": 0.5195996165275574
    },
    {
      "name": "Natural language processing",
      "score": 0.515028715133667
    },
    {
      "name": "SemEval",
      "score": 0.4231812357902527
    },
    {
      "name": "Sequence (biology)",
      "score": 0.41440072655677795
    },
    {
      "name": "Mathematics",
      "score": 0.15545910596847534
    },
    {
      "name": "Linguistics",
      "score": 0.14472535252571106
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}