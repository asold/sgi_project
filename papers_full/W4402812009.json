{
    "title": "Evaluating the Performance of Large Language Models in Competitive Programming: A Multi-Year, Multi-Grade Analysis",
    "url": "https://openalex.org/W4402812009",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5107616090",
            "name": "Adrian Marius Dumitran",
            "affiliations": [
                "University of Bucharest"
            ]
        },
        {
            "id": "https://openalex.org/A5111339841",
            "name": "Adrian Cǎtǎlin Badea",
            "affiliations": [
                "University of Bucharest"
            ]
        },
        {
            "id": "https://openalex.org/A5107645454",
            "name": "Stefan-Gabriel Muscalu",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6794686226",
        "https://openalex.org/W6859147178",
        "https://openalex.org/W4311887664",
        "https://openalex.org/W6861708985",
        "https://openalex.org/W6798182279",
        "https://openalex.org/W2058715873",
        "https://openalex.org/W6752390083",
        "https://openalex.org/W6866440244",
        "https://openalex.org/W6861187108",
        "https://openalex.org/W2963122961",
        "https://openalex.org/W6854500773",
        "https://openalex.org/W6857644187",
        "https://openalex.org/W6967084484",
        "https://openalex.org/W6860976335",
        "https://openalex.org/W4386185625",
        "https://openalex.org/W4254635861",
        "https://openalex.org/W6865118152",
        "https://openalex.org/W4402670067"
    ],
    "abstract": "Recent studies have suggested that large language models (LLMs) underperform on mathematical and computer science tasks when these problems are translated from Romanian into English, compared to their original Romanian format. Accurate translation is critical for applications ranging from automatic translations in programming competitions to the creation of high-quality educational materials, as well as minimizing errors or fraud in human translations. This study shows that robust large language models (LLMs) can maintain or even enhance their performance in translating less common languages when given well-structured prompts. Our findings suggest that LLMs, with appropriate supervision, can be reliably used for the automatic translation of IOI (International Olympiad in Informatics)-style tasks. We evaluate several translation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B, Llama 3.2 3B and GPT-4o, assessing their translation accuracy and performance stability through repeated runs. Additionally, we augment the OJI (Romanian County-Level Informatics Olympiad) Romanian dataset with accurate English translations, enhancing its utility for future LLM training and evaluation. Through detailed syntactic and semantic analyses, we confirm that with human oversight, LLMs can serve as a viable solution for multilingual problem-solving. We also compare the translation quality of LLMs against human translators, as evaluated by a certified expert, underscoring the potential of LLMs in realworld scenarios.",
    "full_text": "Exploring Large Language Models for Translating Romanian\nComputational Problems into English\nAdrian Marius Dumitran1, Adrian-Catalin Badea2, Stefan-Gabriel Muscalu3, Angela-Liliana\nDumitran4, Stefan-Cosmin Dascalescu5, AND Radu-Sebastian Amarie6\n1University of Bucharest, Softbinator\nmarius.dumitran@unibuc.ro\n2University of Bucharest, UiPath\nbadeaadi1999@gmail.com\n3It Just Works Inc.\nstefan.gabriel.muscalu@gmail.com\n4University of Bucharest\ndumitranangela@gmail.com\n5QPillars, University of Bucharest\nstefdasca@gmail.com\n6It Just Works Inc., University of Bucharest\nraduamarie@gmail.com\nAbstract\nRecent studies have suggested that large language models (LLMs) underperform on mathematical and\ncomputer science tasks when these problems are translated from Romanian into English, compared to\ntheir original Romanian format. Accurate translation is critical for applications ranging from automatic\ntranslations in programming competitions to the creation of high-quality educational materials, as well\nas minimizing errors or fraud in human translations. This study shows that robust large language models\n(LLMs) can maintain or even enhance their performance in translating less common languages when\ngiven well-structured prompts. Our findings suggest that LLMs, with appropriate supervision, can be\nreliably used for the automatic translation of IOI (International Olympiad in Informatics)-style tasks. We\nevaluate several translation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B, Llama\n3.2 3B and GPT-4o, assessing their translation accuracy and performance stability through repeated runs.\nAdditionally, we augment the OJI (Romanian County-Level Informatics Olympiad) Romanian dataset\nwith accurate English translations, enhancing its utility for future LLM training and evaluation. Through\ndetailed syntactic and semantic analyses, we confirm that with human oversight, LLMs can serve as a\nviable solution for multilingual problem-solving. We also compare the translation quality of LLMs against\nhuman translators, as evaluated by a certified expert, underscoring the potential of LLMs in realworld\nscenarios.\nKeywords: Automated Translation, Romanian to English Translation, Dataset Enhancement, Syntactic and\nSemantic Analysis, Translation Quality, LLM Training and Assessment\n1 Introduction\nLarge language models (LLMs) have showcased remarkable capabilities across a wide range of natural\nlanguage processing (NLP) tasks, including text generation, translation, code completion, and problem-\nsolving in technical domains. Despite this progress, recent research has pointed to challenges in applying\nLLMs to structured domains, particularly in areas such as mathematics and computer science. Studies by\n(Rae et al., 2021) indicate that while scaling LLMs improves performance across many tasks, structured\n1\narXiv:2501.05601v1  [cs.CL]  9 Jan 2025\ntasks like mathematical problem-solving see smaller gains, suggesting that complexities in these do-\nmains are not fully captured or preserved. This discrepancy prompts further exploration of linguistic and\ncomputational factors that impact performance when translating such structured tasks across languages.\nTranslating mathematical and computational tasks across languages presents challenges that go be-\nyond typical linguistic differences. Technical problems require high levels of precision, and even minor\ntranslation errors—whether due to loss of mathematical context or subtle language ambiguities—can\nprevent humans from correctly understanding and solving these problems. When translations are flawed,\nthey can hinder human problem-solvers by introducing confusion or misinterpretation, which is particu-\nlarly impactful in high-stakes, multilingual environments.\nIn (Cosma et al., 2024b) emphasize the need for dedicated resources beyond simple automatic transla-\ntion, particularly for underrepresented languages like Romanian. Other researches such as (Cosma et al.,\n2024a) argue for the necessity of developing code models for languages other than English, highlighting\nthe current limitations of large language models in understanding and solving problems in non-English\nlanguages.\nIn (Dumitran et al., 2024), we delved into the performance of English large language models (LLMs)\nin solving competitive programming problems from the Romanian Informatics Olympiad at the county\nlevel. The study revealed significant variations in LLM performance across different grades and problem\ntypes, with GPT-4 showing strong performance.\nThe primary objective of this paper is to systematically analyze how translation-induced errors af-\nfect the ability of humans to solve technical problems when using LLMs as a translation aid. Focusing\non Romanian-to-English translations of IOI-style problems from the OJI dataset, we evaluate different\ntranslation strategies to identify the most reliable methods. Through repeated testing and performance\nanalysis, we determine the best ways to ensure that translations retain the original meaning and clarity.\nIn order to support human problem-solvers and to help LLM training, we enhance the OJI dataset with\naccurate English translations and propose an optimized prompt that improves translation accuracy, en-\nsuring that humans can effectively solve the translated problems with the same or better understanding\nthan from the original Romanian versions.\nAlthough prior research exists on Romanian-English translation and the translation of mathematical\nproblems, we have not identified any studies specifically addressing Machine Translation of computer\nscience tasks between any language pairs. Furthermore, we are confident that no such work has been\nconducted for the Romanian-English language pair.\nThis paper is organized as follows: Section 2 outlines the methodology, detailing the translation ap-\nproaches and the LLM evaluation process. Section 3 presents the results of our experiments, with a focus\non translation accuracy and performance variability. In Section 4, we conduct a syntactic and semantic\nanalysis of translation errors, and Section 5 introduces the optimal prompt and discusses the translation\nimprovements it brings. Finally, Section 6 presents the conclusions.\n2 Methodology\nThe methodology for this study was structured into several key stages, employing both quantitative and\nqualitative methods to thoroughly analyze the data.\n1. In the initial stage, we chose 44 problem statements in Romanian from the same grade (15-16 years\nold), out of the 300 in \"OJI\" dataset introduced by (Dumitran et al., 2024).\n2. In the second stage, we computed a \"Ro_score\" for each problem, representing the highest score\nGPT-4o achieved after attempting to solve each problem five times in its original Romanian form.\n3. The third stage involved translating the problems using a diverse set of LLMs(which will be detailed\nin 2.1 , with variations in temperature settings for some models to assess their impact on translation\nquality.\n4. In the fourth stage, GPT-4o was run five times with temperature 0.4 on each translated version of\nthe problem, and scores were obtained for each task and each translation.\nImportantly, we compared the scores achieved by GPT-4o on the original Romanian tasks (Ro_score)\nand the translated tasks, with GPT-4o acting as the evaluator while the other LLMs handled only the\ntranslation process, without generating code.\nA quantitative analysis was conducted to compare the judge scores across all LLM translations, with\nadditional investigations into the effects of temperature settings and model size.\nNext, we conducted an error analysis, where translations were reviewed and categorized by a lin-\nguist. Common issues identified included mistranslations, inappropriate language, untranslated content,\ninconsistent translation of algorithmic and computational terminology, and untranslated examples. Ad-\nditionally, members of the OJI scientific committee examined the translations for technical issues related\nto the content.\nFinally, we conducted a comparison between human translations provided by members of the OJI\nscientific committee and those generated by LLMs, to evaluate how closely the LLM translations align\nwith expert human translations.\n2.1 LLM Selection\nFor our experiments, we selected multiple state-of-the-art LLMs that are widely used for natural language\nprocessing and problem-solving tasks:\n• Llama 3.1 8B: A lightweight LLM optimized for efficiency and performance (Llama Team, 2024).\n• Llama 3.2 3B: A really lightweight LLM optimized for efficiency and performance (Llama Team,\n2024).\n• GPT-4o: The versatile flagship from Azure OpenAI, highly capable, known for its broad general-\nization abilities (OpenAI et al., 2024).\n• OpenLLMRo: We tried multiple models from the OpenLLMRo community (Masala et al., 2024).\n• panSophic-1 preview: A Romanian language-specific model.\n• Aya35B: Aya 35B, released by Cohere, is part of a new family of state-of-the-art multilingual mod-\nels. One of the 23 languages that it supports is Romanian (Aryabumi et al., 2024).\nWe conducted additional tests on Mistral 7B, Gemma 7B, and Gemini 1.5Pro, but the results for these\nmodels were below average.\nMost of our models fit into memory and no quantization was needed except for the Aya35B model.\nAccording to (Marchisio et al., 2023), Aya35B shows minimal degradation in translation tasks (only\n-0.7% on the Flores benchmark).\nGPT-4o was chosen for being state of the art, while the open-weight models were selected for their\nperceived strong performance in translating or knowledge of Romanian.\n2.2 Translation process\nWe translated each task using the LLMs listed in the previous section.\n(Peeperkorn et al., 2024) investigate the impact of temperature adjustments on the creativity of outputs\nproduced by large language models. The study demonstrates that setting the temperature above 1 has\nminimal effect on the results, specifically in the context of story generation.\nFor each task and model, we initially performed one translation at a temperature setting of 0.6. For the\ntop-performing models, we repeated the translations at varying temperature settings: 0.2, 0.6, and 1.0 to\nassess performance across different levels of randomness and across multiple iterations.\nWe started with a temperature of 0.6 as a balanced approach, providing a mix of randomness and\ncertainty in the model’s predictions. This allowed us to get a general sense of the model’s performance.\nThe top-performing models were then tested at different temperature settings to further understand\ntheir capabilities. A temperature of 0.2 was chosen to see how the model performs when it is more\nconfident in its predictions. This could potentially lead to more accurate translations, but it could also\nresult in less creative or diverse outputs.\nOn the other hand, a temperature of 1.0 was chosen to push the model towards more randomness in\nits predictions. This could lead to more diverse and creative translations, but it could also result in less\naccuracy.\n2.2.1 Prompt\nThe following simple prompt was used for translation with all models:\nYou will be provided with an OJI (Olimpiada Judet, ean˘ a de Informatic˘ a) challenge and you will need\nto translate it from Romanian to English.\nRules: Any strings related to the body of the challenge should not be translated, such as input/output\nexamples or file names.\nThe translation should follow the original format (numbering of paragraphs, examples, etc).\nThe translation should be in the same tense as the original text.\nRespond only with the challenge body in English.\nThis prompt was chosen for its specific requirements, including the need to preserve the structure,\nstrings, and tense of the original text. These constraints make the task an interesting subject for studying\ntranslation strategies and the challenges of translating technical and domain-specific content.\nWe aimed for a simple, reasonable prompt that typical LLM users could employ. However, as shown\nlater, a more complex prompt may yield better results.\nMoreover, the task’s focus on the OJI challenges also provides an opportunity to explore the translation\nof educational and competitive programming content, which is a less-studied area in translation studies.\n2.3 Problem Selection\nMost of the tests have been done on problems of low to medium difficulty, as our main method of\nautomatic verification was to have the translated tasks solved by LLMs. We focus our analysis on 8th\ngrade problems, which can be found on (Kilonova, (n.d.)). Kilonova is a renowned Romanian online\njudge platform, specifically designed for training in computer science olympiads. It has gained significant\npopularity in recent years due to its comprehensive collection of problems from all past olympiads. The\nplatform provides an interactive and challenging environment for students to enhance their problem-\nsolving and programming skills.\n8th grade problems tend to have complex texts with many subproblems. The 8th grade curriculum also\nincludes string-related problems, and we know that such problems tend to cause difficulties in translation\ndue to commonly mistranslated terms, such as: \"subsir\" (subsequence, not necessarily a contigous part\nof the sequence) and \"subsecvent, ˘a\" (subarray or substring).\nWe offer translations for all problems in the OJI dataset (Dumitran et al., 2024) and provide an external\nlink to a broader translation dataset.\n3 Translation Comparison\nWe start by emphasizing that, to our surprise, translating from Romanian to English does not seem to\ncause performance issues. Moreover, there may even be benefits to translating the task into English.\nFigure 1 illustrates the difference between the maximum translation score across all models and the\nro_score, showing this difference for each problem individually. For 22 out of 38 problems, the ro_score\nmatches the maximum translation score, while for the remaining cases, the maximum translation score\nexceeds the initial score.\nFigure 1: Comparison of maximum translation scores versus the original score (ro_score) across all\nproblems.\n3.1 Automated verification\nOur methodology involved conducting five runs using GPT-4o to solve the initial Romanian tasks. After\nobtaining the baseline scores, the problem statements were translated into English using various LLMs\nand temperatures. Although increasing the number of runs improves the likelihood that an LLM will\nsolve a given problem, it also incurs additional costs. Based on our findings in (Dumitran et al., 2024),\nwe conclude that five runs provide a good balance between quality and cost. For lower temperatures,\nfewer runs are also effective.\nThe translated versions were then fed back into GPT-4o, where five additional runs were performed\nfor each translated text. We compared the results of these runs with the initial Romanian versions. If a\nsimilar or better score was achieved on the translated English version, we considered the translation to\nbe effective.\nIn some cases, better scores were observed for the English translations due to several factors, including:\n• Randomness in the outputs : We often used temperatures higher than 0, introducing randomness\nthat can sometimes lead to better outcomes in the English version.\n• Superior training data in English : The model is typically trained on larger and more diverse\nEnglish datasets, leading to better performance in English tasks.\n• Refined error handling in English: LLMs generally have more robust error detection and correc-\ntion mechanisms when processing English inputs, improving their overall problem-solving capabil-\nities.\n• Better understanding of formal structures in English : English-specific models are better at un-\nderstanding and interpreting the formal language structures often used in technical problems.\n3.1.1 Overall results\nFigure 2 presents a comparative analysis of the 15 highest-scoring model runs, arranged in descending\norder of performance. The x-axis identifies each model run with an alphanumeric code, which represents\nspecific model architectures, temperature settings (’t’), iteration number (’i’), and, where known, the\nnumber of parameters. The percentages above the bars show the performance difference compared to the\nRo_score.\nFigure 2: Top 15 model runs vs RoScore\nThe close clustering of scores, ranging from +3.7% to -3.6%, illustrates that top models achieved\nsimilar performance levels, with only marginal variations between them. Unsurprisingly, multiple runs\nwith the same settings can yield different results due to the inherent randomness in model outputs. Open-\nLLmRo models were excluded from this analysis as their focus on Romanian significantly impacted their\nEnglish translation capabilities.\nThese results underscore the diminishing returns at the upper end of model performance and suggest\nthat further progress in enhancing translation accuracy may depend on fine-tuning models for specific\ntasks or further improving prompt design, rather than purely increasing model size or complexity.\n3.2 Temperatures - the creativity parameter\nTranslation Model Temperature Size Scores Score Average\ngpt4o 0.6 * 58.84 58.84\nro_score 0.6 * 55.13 55.13\naya 0.2, 0.6, 1.0 35B 54.00, 55.34, 53.73 54.35\nllama3 0.2, 0.6, 1.0 8B 52.16, 57.73, 47.45 52.44\nllama31 0.2, 0.6, 1.0 8B 50.05, 56.07, 49.77 51.96\nllama32 0.2, 0.6, 1.0 3B 51.89, 51.41, 49.41 50.90\npans-llama31 0.2, 0.6, 1.0 8B 52.86, 54.02, 46.93 51.27\ngemini 0.6 * 45.11 45.11\nmistral 0.6 7B 44.50 44.50\ngemma 0.6 7B 43.77 43.77\nTable 1: Average result on dataset over temperature and size\nFor the models for which we ran more iteratios, the scores are aligned respectively to the temperature\nin the table 1\nWe chose temperatures of 0.2, 0.6, and 1.0, as discussed in 2.2. We only ran temperature 0.2 and 1.0\nfor the LLMs with good results on temperature 0.6. Pricier models such as gpt4o or gemini were run\nonly once, with 0.6. Key findings:\n1. (Optimal temperature): On average temperature 0.6 showed best results followed by 0.2 and 1.0.\n2. (Small Model): Llama 3.2 3B, our smallest model, was the only one that performed better at a\ntemperature of 0.2, indicating that smaller models tend to deviate from optimal results more quickly.\n3. (Temperature 0.6 similar text). We add two translations by Llama 3.1 at temperature 0.6:\nA word consisting only of small letters is given. We call anagram a word formed by rearranging the\nletters of the given word. For example, armata is an anagram of tamara. Obviously, a word can be\nconsidered its own anagram.\nA word consisting only of lowercase letters is given. We call anagram a word formed by the letters\nof the given word, rearranging them if necessary. For example, armata is an anagram of tamara.\nObviously, a word can be considered an anagram of itself.\nNote that while the story is a bit modified in translation the rest of the text(not included here)\nis almost identical (the part related to the actual task and the restrictions). Thus, models tend to\nproduce similar but slightly varied translations at temperature 0.6.\n3.3 Model size\nWe utilized language models of varying sizes to represent different capacities and computational require-\nments (see 1 ). The models, ordered by size, include Llama 3.2 3B, Gemma 7B, Mistral 7B, Aya 8B,\nLlama 3 8B, Llama 3.1 8B, Pansophic (based on Llama 3.1 8B), Aya 35B, and GPT-4o (estimated at over\n100 billion parameters). By incorporating models ranging from 3 billion to over 100 billion parameters,\nwe investigated how model size impacts translation performance across temperature settings. This range\nallowed us to examine the trade-offs between computational resources and translation quality, as well as\nhow different-sized models respond to temperature adjustments in language translation tasks.\nKey Observations\n1. GPT-4o at 100B+:\n• GPT-4o, with over 100 billion parameters, demonstrates the highest performance in our study.\nHowever, GPT-4o’s scores should be evaluated in comparison with other large models like\nLlama 3.1 405B. Additionally, GPT-4o’s performance may be influenced by the fact that it was\nboth the translation model and the judge, potentially giving it an advantage by aligning the\ntranslation with the evaluation model’s own language patterns.\n2. Smaller Models Struggle with Longer Tasks\n• In our analysis, smaller models often struggled to complete translations, especially for longer,\nmore complex tasks. Many of the tasks that received a score of 0 points after translation were\nincomplete. This was particularly evident in problems with a lot of restrictions and examples\nlike problem 515, where examples and restrictions were either omitted or poorly translated. For\ninstance, in one Aya translation, a big part of the text was omitted and instead the following\ntext offered: The rest of the challenge remains unchanged and can be found in your original\ntext, instead of translating the task and restrictions fully.\n3. Llama Models’ Performance at 8B and 3B\n• The Llama models, known for robust multilingual capabilities, consistently succeeded in trans-\nlation tasks, often matching or exceeding the original Romanian scores (Ro_score). Zero scores\nwere only due to incomplete translations. This suggests that with better prompts tailored to\nsmaller models and allowing multiple attempts, both Llama 3 and Llama 3.1 could potentially\nmatch GPT-4o’s performance.\n• However, Llama 3.2 3B underperformed in comparison to the other Llama models, proving\ninadequate for this specific task.\n4. Similar Translation Quality Across Top Models:\n• As detailed in 4, our qualitative analysis revealed no significant differences in translation qual-\nity among the top-performing models. The variations in performance were largely attributed\nto model fatigue when handling longer tasks.\n4 Grammatical Analysis\nVarious translation techniques, including expansion, adaptation, transposition, and structure shift, con-\ntinue to be employed by machine translation (MT) systems. As noted by (Wu et al., 2016), these systems\nutilize large datasets and deep learning algorithms to effectively implement these techniques, aiming\nto deliver high-quality translations across a wide range of languages and contexts, ultimately improving\nboth the accuracy and readability of the output. Despite its widespread use and significant advancements,\nas (Karami, 2014) points out for Google Translate, the LLMs continue to encounter difficulties with\nmore complex texts, idiomatic expressions, and languages that have less digital representation. These\nlimitations underscore the necessity for ongoing improvements and suggest caution when depending on\nmachine translation for critical or nuanced content.\nThe initial step in addressing the errors identified in the translation of algorithmic and informatics\nproblems from Romanian into English by large language models (LLMs) is to classify them system-\natically. Classification enables the organization of errors into distinct categories, facilitating a more\nfocused analysis of the translation issues. By grouping errors—such as mistranslations, inconsistencies\nin technical terminology, or the omission of critical computational terms—it becomes possible to identify\nrecurring patterns and pinpoint areas where LLMs struggle the most. This structured approach lays the\nfoundation for subsequent error quantification and qualitative analysis, essential for improving the over-\nall performance of automated translation systems in technical domains. (Lommel et al., 2014) introduce\na hierarchical translation error taxonomy known as MQM, which distinguishes between fluency errors\nand accuracy errors. According to (Lommel et al., 2014), fluency errors are defined as issues \"related to\nthe language of the translation, irrespective of its status as a translation,\" while accuracy errors pertain to\n\"how accurately the target text reflects the content of the source text.\" Using these categories, errors can\nbe classified as either minor or critical. For this study, errors were categorized based on the classifications\nestablished by researchers such as (Hemchua and Schmitt, 2007), (Hsu, 2014), (Costa et al., 2015), and\naccording to the frequency of errors identified in the data.\nThe following table provides a comparative analysis of translation errors identified across various large\nlanguage models (LLMs) such as Mistral, Llama, Gemma, Gemini, GPT-4o, and Aya. Each model was\ntasked with translating technical structures from Romanian into English, with the table highlighting the\nerrors, the incorrect target language structure, and the suggested correct translations. While the table\ndoes not include every error identified, it highlights those that significantly impact the quality of the\ntranslation.\nLLM Model Problem Number Structure in Source Language Error Type\nMistral 515 rectiliniu Lexical error\nMistral 802 num˘ar de ordine Semantic error\nLlama 298 o matrice p˘atratic˘a Omission error\nMistral, Llama, Gemma, Gemini, GPT-4oAll texts „x” Punctuation error\nMistral, Llama, Gemma, Gemini, GPT-4oAll texts Tables and their contents Structural error\nGPT-4o 899 ¸ sir Semantic error\nAya 299 curent False friend\nGPT-4o 802 pozi¸ tiile de început ¸ si de final ale acestor secven¸ te în ¸ sirul din setSemantic error\nTable 2: Comparative analysis of translation errors across LLM models.\nIn this study, several algorithmic problems were translated from Romanian into English by various\nlarge language models (LLMs), including Mistral, Llama 3, Llama 3.1, Gemma, Gemini, GPT-4o, and\nAya. A consistent pattern of errors and systematic distortions was observed across these translations.\nNotably, certain parts of the text were left untranslated. For instance, the section \"restrict, ii s, i clarific˘ari\"\nwas often only partially translated as \"restrictions,\" omitting \"clarifications.\" Additionally, some tasks\nwere either partially translated or left untranslated entirely, as demonstrated by Llama 3’s translation\nof problem 899. Furthermore, all LLMs failed to fully translate the content of tables, with at least one\ncolumn from the \"example\" sections consistently omitted, and the explanatory content entirely absent. In\naddition to these omissions, punctuation errors were also frequent, with quotation marks, mathematical\nsymbols, and other punctuation marks being incorrectly rendered. These recurring issues highlight the\nlimitations of LLMs in accurately translating structured, technical content.\nSeveral types of translation errors were identified in the output of LLMs when translating algorith-\nmic problems from Romanian into English. One prominent error occurred in the translation of the term\n\"rectiliniu\" in problem 515, which was consistently rendered as \"straight\" by all the LLMs, instead of\nthe correct term \"linear\". This constitutes a lexical error, as the incorrect word choice alters the intended\nmeaning within the context of the problem. Additionally, this could be classified as a semantic error,\ngiven that the LLMs failed to capture the appropriate technical meaning of \"rectiliniu\" in the domain\nof algorithmic and computational language, where \"linear\" is the precise term. The misinterpretation\nlikely arises from the general meaning of \"rectiliniu\" in everyday language, where \"straight\" is a com-\nmon equivalent, but it is not suitable in the technical context, leading to a distortion of the problem’s\nmeaning. In the study, Llama exhibited a notable translation error in problem 298 by omitting the adjec-\ntive \"p˘atratic˘a\" when translating the Romanian structure \"o matrice p ˘atratic˘a\" as \"a matrix\", rather than\nthe correct \"a square matrix\". This constitutes a lexical omission error, where a critical modifier—in this\ncase, the adjective \"square\"—was not translated, resulting in an incomplete and less precise rendering\nof the source text. The omission impacts the technical accuracy of the translation, as \"square matrix\"\nis a specific term in mathematics that conveys essential information about the matrix’s dimensions. The\nfailure to include this detail diminishes the clarity and correctness of the translated problem, potentially\nleading to misinterpretation of the task.\nDiscrepancies were observed in the translation of key technical terms, particularly the Romanian word\n\"¸ sir\", which was inconsistently rendered by different LLMs. For instance, in problem number 899,\nLlama and GPT-4o translated \"¸ sir\" as \"sequence\", while Llama 3 translated it as \"string\". While both\ntranslations capture certain aspects of the term, neither fully aligns with the technical context in which\n\"¸ sir\" is used. The most accurate translation in this context would be \"array\", which is a more precise\nterm in algorithmic and computational problems. According to the literature (Hopcroft et al., 2006),\n\"string\" refers to a sequence of characters that can potentially have a very long or infinite length, which\nmay not align with all the contexts given in the algorithmic. The use of \"sequence\", while closer, still\nlacks the specificity required to convey the array-like properties of \"¸ sir\". Thus, the preferred translation\nof \"¸ sir\" in such cases would be \"array\", as it more accurately reflects the intended data structure within\nthe problem’s context.\nIn conclusion, the large language models (LLMs) in this study, especially Llama3.1, Llama 3.2, GPT-\n4o, and Aya23—showed strong proficiency in translating algorithmic problems from Romanian to En-\nglish, with overall translation quality approaching that of human translators. However, recurring issues\nlike lexical mismatches, semantic inaccuracies, omissions, and structural inconsistencies highlight the\nneed for human revision to ensure accuracy in technical domains. While LLMs are valuable for initial\ntranslations, human oversight remains crucial, especially in specialized contexts like algorithmic problem\ntranslation.\n5 Working solution\nWe enhanced the OJI Romanian dataset by incorporating manually verified English translations, creating\na valuable benchmark for future research in multilingual LLMs. Drawing on extensive experience with\nRomanian competitive programming, we identified and corrected common machine translation errors,\nparticularly technical jargon often mistranslated by generic tools. Key terms such as \"Cerint , ˘a\" (task),\n\"subsecvent, ˘a\" (subarray), \"subs, ir\" (subsequence), and \"s, ir de caractere\" (string) were carefully handled\nto ensure accuracy.\nOur experiments primarily used GPT-4o and the OpenAI API. We began with a basic prompt—\"Can\nyou translate this problem statement into English in the context of competitive programming?\"—which,\nalthough imperfect, significantly improved focus on competitive programming terminology compared to\ngeneric translation approaches.\nWe further improved the translations by manually correcting recurring errors and refining the prompt\nbased on our expertise with the OJI dataset. This refined prompt ensured a consistent structure in trans-\nlated problem sets and accommodated Markdown and LaTeX formatting, enhancing clarity and func-\ntionality.\nFor example, a simple prompt that doesn’t account for specific terms can lead to pitfalls. Misuse of\nterms like ’subsequence’ and ’substring’, or inconsistent descriptions of printed data, can cause con-\nfusion. In competitive programming, even one misunderstood word can result in solving an entirely\ndifferent problem.\nFigure 3: Original Kilonova problem 15\nFigure 4: Translated Problem Statement with issues\n5.1 Efficient prompt\nThe final version of the prompt used is: \"Please process the following text according to the specified\ninstructions: You will be given a competitive programming problem statement in markdown, written in\nthe Romanian language, using GFM extensions and MathJax/LaTeX math between dollar signs ($ or $$).\nAnother extension is the fact that image attachments are defined using a syntax similar to [name.png],\nwith optional attributes named after the end with a vertical bar (’|’). You must translate the statement in\nthe English language, while preserving mathematical values, variable names, general syntax, structure\nand format. You must also preserve the custom image format exactly as is. The word Cerint , ˘ a is always\ntranslated to Task, Date de intrare is translated to Input data, Date de ies , ire is translated to Output\ndata, subsecvent, ˘ a is translated to subarray, subs, ir is translated to subsequence, Restrict, ii s, i preciz˘ ari to\nConstraints and clarifications, vector is translated to array, s , ir de caractere is translated to string. In\naddition, in the Date de intrare and Date de ies, ire sections, if you see expressions such as Pe prima linie,\nPe a doua linie etc., you want to use the verb contain to describe the data we need to read or print. In\nthe Date de ies, ire sections, you can also use the verb print to describe the data we need to print. When\nseparating large integers (especially in latex/mathjax math) in groups of 3 digits, do not add a comma.\nInstead, add a backslash followed and preceded by a single space character. After you are done with\ntranslating, please double check the statement and fix potential grammar and/or syntax errors according\nto the rules of English language.\"\nUsing the refined prompt, the translations became nearly flawless, with no significant errors detected\nby linguists or members of the competitive programming scientific committee. The translations met both\nlinguistic and technical standards.\nThe final version of the OJI problem below illustrates the improvements achieved through prompt\nengineering and domain-specific adjustments while preserving competitive programming terminology.\nFigure 5: Correct Translation using enhanced prompt.\nGiven the relatively large size of the dataset—over 300 problems, as shown by this list from the\nKilonova online judge—relying solely on manual browsing was not feasible.\nAutomating the translation process became essential. We obtained markdown versions of the problem\nstatements from Kilonova, the only Romanian online judge hosting the entire OJI dataset. These files\nenabled us to develop Python scripts to process the raw statements, interact with the OpenAI API, and\nimprove formatting, reducing potential errors related to the LLM’s handling of markdown syntax.\nOnce the dataset was fully translated, we uploaded it to a GitHub repository for evaluating model\nperformance. We further improved the translations by fixing markdown issues and refining specific\naspects. This resulted in a comprehensive collection of Romanian problems accurately translated into\nEnglish, providing a valuable resource for future research and evaluation.\n6 Conclusions\nThis study demonstrates that large language models (LLMs) can effectively translate complex technical\ncontent from Romanian to English, achieving quality comparable to human translators when provided\nwith well-crafted prompts and human oversight. While focused on Romanian to English, our findings\nopen doors for exploration in other language pairs. Automated translation in STEM competitions is\na particularly promising application, where translation quality is crucial and manual translations have\nhistorically faced issues of accuracy and potential fraud, highlighting the need for reliable automated\nsolutions.\nThe results indicate that models like GPT-4.0 and Llama 3.1 8B consistently perform well, though\nsmaller models struggle with complex tasks. Proper prompts and temperature settings (such as 0.6)\nproved essential for optimal performance. However, despite advancements, lexical errors and structural\ninconsistencies—especially in smaller models—suggest that human oversight remains necessary.\nBy enhancing the OJI dataset with manually verified translations, this study provides valuable re-\nsources for future research into multilingual LLMs. Expanding these efforts to other language pairs and\nrefining the use of LLMs in high-stakes environments like educational competitions can significantly\nimprove accessibility and fairness in such events.\nReferences\nMax Peeperkorn, Tom Kouwenhoven, Dan Brown, and Anna Jordanous. 2024. Is Temperature the Creativity\nParameter of Large Language Models? . arXiv preprint arXiv:2405.00492. https://arxiv.org/abs/\n2405.00492.\nJ. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffman, F. Song, and G. Irving. 2021. Scaling Language\nModels: Methods, Analysis & Insights from Training Gopher . arXiv preprint arXiv:2112.11446. https:\n//arxiv.org/abs/2112.11446.\nAdrian Cosma, Bogdan Iordache, and Paolo Rosso. 2024. RoCode: A Dataset for Measuring Code Intelligence\nfrom Problem Definitions in Romanian . arXiv preprint arXiv:2402.13222. https://arxiv.org/abs/\n2402.13222.\nAdrian Marius Dumitran, Adrian Catalin Badea, and Stefan-Gabriel Muscalu. 2024. Evaluating the Performance\nof Large Language Models in Competitive Programming: A Multi-Year, Multi-Grade Analysis . arXiv preprint\narXiv:2409.09054.\nAdrian Cosma, Ana-Maria Bucur, and Emilian Radoi. 2024. RoMath: A Mathematical Reasoning Benchmark in\nRomanian. arXiv preprint arXiv:2409.11074. https://arxiv.org/abs/2409.11074.\nAbhimanyu Dubey, Abhinav Jauhri et al 2024. The Llama 3 Herd of Models . arXiv preprint arXiv:2407.21783.\nhttps://arxiv.org/abs/2407.21783.\nOpenAI, Josh Achiam, Steven Adler, et al. 2024. GPT-4 Technical Report. arXiv preprint, arXiv:2303.08774.\nURL: https://doi.org/10.48550/arXiv.2303.08774.\nMihai Masala, Denis C. Ilie-Ablachim, Dragos Corlatescu, Miruna Zavelca, Marius Leordeanu, Horia Velicu,\nMarius Popescu, Mihai Dascalu, and Traian Rebedea. 2024. OpenLLM-Ro – Technical Report on Open-source\nRomanian LLMs. arXiv preprint, arXiv:2405.07703. URL: https://arxiv.org/abs/2405.07703.\nViraat Aryabumi, John Dang, Dwarak Talupuru, et al. 2024. Aya 23: Open Weight Releases to Further Multilin-\ngual Progress. arXiv preprint, arXiv:2405.15032. URL: https://arxiv.org/abs/2405.15032.\nKelly Marchisio, Saurabh Dash, Hongyu Chen, Dennis Aumiller, Ahmet Üstün, Sara Hooker, and Sebastian Ruder.\n2023. How Does Quantization Affect Multilingual LLMs?. arXiv preprint, arXiv:2407.03211. URL: https:\n//arxiv.org/abs/2407.03211.\nJohn E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. 2006.Automata Theory, Languages, and Computation.\nAddison-Wesley.\nOmid Karami. 2014. The brief view on Google Translation Machine. In Seminar in Artificial Intelligence on\nNatural Language, Germany.\nArle Lommel, Aljoscha Burchardt, and Hans Uszkoreit. 2014. Multidimensional quality metrics (MQM): A\nframework for declaring and describing translation quality metrics. In Proceedings of the Eighth International\nConference on Language Resources and Evaluation (LREC’14) , European Language Resources Association\n(ELRA), 14–20.\nYonghui Wu, Mike Schuster, Zhifeng Chen et al 2016. Google’s neural machine translation system: Bridging the\ngap between human and machine translation. arXiv preprint arXiv:1609.08144 . URL: https://arxiv.\norg/abs/1609.08144.\nJ.-A. Hsu. 2014. Error classification of machine translation: A corpus-based study on Chinese-English patent\ntranslation. Studies of Translation and Interpretation, 18:121–136.\nAlexandre Costa, Ling Wang, Luís Tiago, Rui Costa, and Luisa Coheur. 2015. A linguistically motivated taxon-\nomy for machine translation error analysis. Machine Translation, 29:127–161.\nSaowalak Hemchua and Norbert Schmitt. 2007. An analysis of lexical errors in the English compositions of Thai\nlearners. Linguistics, Corpus ID: 42846941.\nKilonova problem Lists. Retrieved from https://kilonova.ro/problem_lists/456"
}