{
  "title": "Cost, Usability, Credibility, Fairness, Accountability, Transparency, and Explainability Framework for Safe and Effective Large Language Models in Medical Education: Narrative Review and Qualitative Study (Preprint)",
  "url": "https://openalex.org/W4395030863",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5044290434",
      "name": "Majdi Anwar Quttainah",
      "affiliations": [
        "Kuwait University"
      ]
    },
    {
      "id": "https://openalex.org/A5015934551",
      "name": "Vinaytosh Mishra",
      "affiliations": [
        "Gulf Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A5005510538",
      "name": "Somayya Madakam",
      "affiliations": [
        "Gulf Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A5013163033",
      "name": "Yotam Lurie",
      "affiliations": [
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A5041885354",
      "name": "Shlomo Mark",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4323545986",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W4362454930",
    "https://openalex.org/W4297965481",
    "https://openalex.org/W2599901939",
    "https://openalex.org/W3169509819",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4376106534",
    "https://openalex.org/W4376114743",
    "https://openalex.org/W4378373817",
    "https://openalex.org/W4366743045",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W4362472309",
    "https://openalex.org/W2960833983",
    "https://openalex.org/W4281251447",
    "https://openalex.org/W4281754333",
    "https://openalex.org/W4292623623",
    "https://openalex.org/W4224928673",
    "https://openalex.org/W4280505555",
    "https://openalex.org/W4320495408",
    "https://openalex.org/W2106264052",
    "https://openalex.org/W4381803920",
    "https://openalex.org/W4381804284",
    "https://openalex.org/W4310474711",
    "https://openalex.org/W4292457531",
    "https://openalex.org/W4311009624",
    "https://openalex.org/W2988194974",
    "https://openalex.org/W4232722847",
    "https://openalex.org/W2005935580",
    "https://openalex.org/W2767546445"
  ],
  "abstract": "<sec> <title>BACKGROUND</title> The world has witnessed increased adoption of large language models (LLMs) in the last year. Although the products developed using LLMs have the potential to solve accessibility and efficiency problems in health care, there is a lack of available guidelines for developing LLMs for health care, especially for medical education. </sec> <sec> <title>OBJECTIVE</title> The aim of this study was to identify and prioritize the enablers for developing successful LLMs for medical education. We further evaluated the relationships among these identified enablers. </sec> <sec> <title>METHODS</title> A narrative review of the extant literature was first performed to identify the key enablers for LLM development. We additionally gathered the opinions of LLM users to determine the relative importance of these enablers using an analytical hierarchy process (AHP), which is a multicriteria decision-making method. Further, total interpretive structural modeling (TISM) was used to analyze the perspectives of product developers and ascertain the relationships and hierarchy among these enablers. Finally, the cross-impact matrix-based multiplication applied to a classification (MICMAC) approach was used to determine the relative driving and dependence powers of these enablers. A nonprobabilistic purposive sampling approach was used for recruitment of focus groups. </sec> <sec> <title>RESULTS</title> The AHP demonstrated that the most important enabler for LLMs was &lt;i&gt;credibility&lt;/i&gt;, with a priority weight of 0.37, followed by &lt;i&gt;accountability&lt;/i&gt; (0.27642) and &lt;i&gt;fairness&lt;/i&gt; (0.10572). In contrast, &lt;i&gt;usability&lt;/i&gt;, with a priority weight of 0.04, showed negligible importance. The results of TISM concurred with the findings of the AHP. The only striking difference between expert perspectives and user preference evaluation was that the product developers indicated that &lt;i&gt;cost&lt;/i&gt; has the least importance as a potential enabler. The MICMAC analysis suggested that cost has a strong influence on other enablers. The inputs of the focus group were found to be reliable, with a consistency ratio less than 0.1 (0.084). </sec> <sec> <title>CONCLUSIONS</title> This study is the first to identify, prioritize, and analyze the relationships of enablers of effective LLMs for medical education. Based on the results of this study, we developed a comprehendible prescriptive framework, named CUC-FATE (Cost, Usability, Credibility, Fairness, Accountability, Transparency, and Explainability), for evaluating the enablers of LLMs in medical education. The study findings are useful for health care professionals, health technology experts, medical technology regulators, and policy makers. </sec>",
  "full_text": "JMIR Preprints Quttainah et al\nCUCFATE Frameworks for Safe and Effective Large\nLanguage Models in Medical Education: Using\nQualitative Methods\n Majdi Quttainah, Vinaytosh Mishra, Somayya Madakam, Yotam Lurie, Shlomo\nMark\nSubmitted to: JMIR AI\non: August 16, 2023\nDisclaimer: © The authors. All rights reserved. This is a privileged document currently under peer-review/community\nreview. Authors have provided JMIR Publications with an exclusive license to publish this preprint on it's website for\nreview purposes only. While the final peer-reviewed paper may be licensed under a CC BY license on publication, at this\nstage authors and publisher expressively prohibit redistribution of this draft paper other than for review purposes.\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nTable of Contents\nOriginal Manuscript ....................................................................................................................................................................... 5\nSupplementary Files ..................................................................................................................................................................... 28\nFigures ......................................................................................................................................................................................... 29\nFigure 1 ...................................................................................................................................................................................... 30\nFigure 2 ...................................................................................................................................................................................... 31\nFigure 3 ...................................................................................................................................................................................... 32\nMultimedia Appendixes ................................................................................................................................................................. 33\nMultimedia Appendix 0 .................................................................................................................................................................. 34\nCONSORT (or other) checklists ...................................................................................................................................................... 35\nCONSORT (or other) checklist 0 ...................................................................................................................................................... 35\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nCUCFATE Frameworks for Safe and Effective Large Language Models in\nMedical Education: Using Qualitative Methods\nMajdi Quttainah1 PhD; Vinaytosh Mishra2 PhD; Somayya Madakam 3 PhD; Yotam Lurie4 PhD; Shlomo Mark5 PhD\n1Kuwait University Kuwait KW\n2Gulf Medical University Ajman AE\n3ATLAS Skill Tech University Mumbai IN\n4Ben-Gurion University Negev IL\n5Shamoon College of Engineering Be'er Sheva IL\nCorresponding Author:\nVinaytosh Mishra PhD\nGulf Medical University\nAl Jurf 1\nAjman\nAE\nAbstract\nBackground: The world has witnessed increased adoption of Large Language Models (LLMs) in the last year. Although the\nproducts developed using LLMs have the potential to solve accessibility and efficiency problems in healthcare, there is a lack of\nguidelines available for developing LLMs for healthcare and especially medical education.\nObjective: The study aims to identify and prioritize the enablers for developing successful LLMs for medical education. The\nstudy also discusses the relationship among these identified enablers.\nMethods: The study first identifies key enablers for LLM development using the narrative review of extant literature. The next\nopinion of users of LLMs was taken to determine the relative importance of these enablers using the multi-criteria decision-\nmaking method called the Analytical Hierarchy Process. Further, Total Interpretive Structural Modelling (TISM) was used to\nanalyze product developers' perspectives and ascertain the relationship and hierarchy among these enablers. Finally, Cross-\nimpact matrix multiplication was applied to classification (MICMAC) to find these enablers' relative driving and dependence\npower. The non-probabilistic purposive sampling was used for the study.\nResults: The result of AHP concluded that credibility, with a priority weight of 0.37, is the most important enabler, followed by\nAccountability (0.27642) and Fairness (0.10572). In contrast, usability, with a priority weight of 0.04, has negligible importance.\nThe results of TISM concur with the findings of the AHP. The only striking difference from the user's preference was that\nproduct developers gave the least importance to cost. The development of the MICMAC analysis suggests that cost has a strong\ninfluence on other enablers. The inputs of the focus group were found reliable, with a consistency ratio (CR=0.084) less than 0.1.\nConclusions: The study is the first to identify, prioritize, and analyze the relationship of enablers for effective LLMs for medical\neducation. The study provides an easy to comprehendible prescriptive framework CUCFATE (Cost, Usability, Credibility,\nFairness, Accountability, Transparency, and Explainability) for the same. The study findings are useful for healthcare\nprofessionals, health technology experts, medical technology regulators, and policymakers.\n(JMIR Preprints 16/08/2023:51834)\nDOI: https://doi.org/10.2196/preprints.51834\nPreprint Settings\n1) Would you like to publish your submitted manuscript as preprint?\nPlease make my preprint PDF available to anyone at any time (recommended).\nPlease make my preprint PDF available only to logged-in users; I understand that my title and abstract will remain visible to all users.\nOnly make the preprint title and abstract visible.\nNo, I do not wish to publish my submitted manuscript as a preprint.\n2) If accepted for publication in a JMIR journal, would you like the PDF to be visible to the public?\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nYes, please make my accepted manuscript PDF available to anyone at any time (Recommended). \nYes, but please make my accepted manuscript PDF available only to logged-in users; I understand that the title and abstract will remain visible to all users (see Important note, above). I also understand that if I later pay to participate in <a href=\"https://jmir.zendesk.com/hc/en-us/articles/360008899632-What-is-the-PubMed-Now-ahead-of-print-option-when-I-pay-the-APF-\" target=\"_blank\">JMIR’s PubMed Now! service</a> service, my accepted manuscript PDF will automatically be made openly available.\nYes, but only make the title and abstract visible (see Important note, above). I understand that if I later pay to participate in  <a href=\"https://jmir.zendesk.com/hc/en-us/articles/360008899632-What-is-the-PubMed-Now-ahead-of-print-option-when-I-pay-the-APF-\" target=\"_blank\">JMIR’s PubMed Now! service</a> service, my accepted manuscript PDF will automatically be made openly available.\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nOriginal Manuscript\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nCUCFATE Frameworks for Safe and Effective Large Language Models in\nMedical Education Using Qualitative Methods \nMajdi Quttainah, Vinaytosh Mishra, Somayya Madakam, Yotam Lurie,\nBackground \nThe world has witnessed increased adoption of Large Language Models (LLMs) in the last year.\nAlthough the products developed using LLMs have the potential to solve accessibility and efficiency\nproblems in healthcare, there is a lack of guidelines available for developing LLMs for healthcare\nand especially medical education. \nObjective\nThe study aims to identify and prioritize the enablers for developing successful LLMs for medical\neducation. The study also discusses the relationship among these identified enablers. \nMethods \nThe study first identifies key enablers for LLM development using the narrative review of extant\nliterature. The next opinion of users of LLMs was taken to determine the relative importance of these\nenablers using the multi-criteria decision-making method called the Analytical Hierarchy Process.\nFurther, Total Interpretive Structural Modelling (TISM) was used to analyze product developers'\nperspectives and ascertain the relationship and hierarchy among these enablers. Finally, Cross-impact\nmatrix multiplication was applied to classification (MICMAC) to find these enablers' relative driving\nand dependence power. The non-probabilistic purposive sampling was used for the study. \nResults \nThe result of AHP concluded that credibility, with a priority weight of 0.37, is the most important\nenabler, followed by Accountability (0.27642) and Fairness (0.10572). In contrast, usability, with a\npriority weight of 0.04, has negligible importance.  The results of TISM concur with the findings of\nthe AHP. The only striking difference from the user's preference was that product developers gave\nthe least importance to cost . The development of the MICMAC analysis suggests that cost has a\nstrong  influence  on other  enablers.  The  inputs of  the  focus group  were found  reliable,  with  a\nconsistency ratio (CR=0.084) less than 0.1.\nConclusion \nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nThe study is the first to identify, prioritize, and analyze the relationship of enablers for effective\nLLMs for medical education. The study provides an easy to comprehendible prescriptive framework\nCUCFATE (Cost, Usability, Credibility, Fairness, Accountability, Transparency, and\nExplainability)  for  the  same. The  study  findings  are  useful  for  healthcare  professionals,  health\ntechnology experts, medical technology regulators and policymakers. \nKeywords: Large Language Models, ChatGPT, CUCFATE Framework, AHP, TISM \nIntroduction\nNatural Language Programming solutions have been available for the last fifteen years. However, the\navalanche breakdown phenomena recently hit the use of these models with the launch of ChatGPT\nby a company named OpenAI. The company received  investment from Elon Musk and others when\nit was established a few years ago. With 1.6 billion monthly users, this freemium is the fastest-\ngrowing application in the history of the internet. Released on November 30, 2022, OpenAI released\nChatGPT (Chat Generative Pre-Trained Transformer), a generative language model tool that enables\nusers to converse with machines about various subjects. Since its debut, ChatGPT has sparked much\ndiscussion  and  enthusiasm  in  multiple  industries,  including  medicine.  ChatGPT  and  related\ntechnologies  have  been  identified  as  disruptive  innovations  with  the  potential  to  revolutionize\nacademia and scholarly publishing [1]. Additionally, preliminary research suggests that ChatGPT has\npractical applications throughout the clinical workflow [2]  \nThe introduction of ChatGPT and the subsequent release of several extended products and functional\nplug-ins have profoundly impacted scientific researchers. They have also influenced the ideas and\nmethodologies used in traditional research, including recommendation, emotion recognition, and\ninformation generation. ChatGPT's assistance has improved some of the work, particularly in data\ngeneration.  ChatGPT  can  offer  helpful  supplementary  information  to  raise  the  calibre  of  data\ngeneration. With the integration of machine learning and artificial intelligence (AI) technologies,\nmedical imaging has advanced quickly. Among these developments, using cutting-edge language\nmodels like Large Language Model (LLM), ChatGPT, and GPT-4 has shown significant promise in\nelevating several elements of medical imaging and revolutionizing radiology. These models can\nproduce  and  comprehend  human-like  text  thanks  to  access  to  various  textbooks,  journals,  and\nresearch materials. This could provide the necessary context and prior knowledge to support a variety\nof  tasks  involving  medical  imaging,  such  as  synthesis,  reconstruction,  analysis,  segmentation,\ninterpretation,  automated  reporting,  and  more.  It  has  been  improved  using  supervised  and\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nreinforcement learning methods and is based on OpenAI's GPT big language models. These models\nhave performed excellently in various NLP tasks, including language translation, text summarization,\nand question-answering. They have been pre-trained on enormous amounts of text data. Users can\nask questions, get responses, and engage in genuine conversation with the bot thanks to ChatGPT's\nhuman-like conversational experience. ChatGPT and other big models remain a research hotspot in\nmultimedia analysis and application. However, several crucial difficulties must be resolved: 1) How\nto interact with ChatGPT to collect more useful auxiliary information; 2) How to combine with\ntraditional inquiries to fully exploit ChatGPT's better benefits; and 3) How to analyze the data\nobtained  from  ChatGPT  and  incorporate  it  with  the  intended  usage.  Effectively  using  past\ninformation from huge models and investigating consistency and complementary features across\nmany  modalities  to  improve  multi-modal  generation  performance  is  a  significant  challenge,\nparticularly in AI Generated Content (AIGC). The finest use cases for ChatGPT, a well-liked chatbot\nbuilt on a potent AI language model, are still being worked out. This article offers some suggestions\non how to make the tool work for you when writing academic papers. The following steps in writing\nan essay, thesis, or dissertation can be helped with via ChatGPT: creating a research question,\ndeveloping a plan, developing literary concepts, rewriting text and getting feedback.\nMoreover, Natural language processing and automated data analysis capabilities offered by ChatGPT\nenable researchers, marketers, and organizations to analyze papers quickly and accurately. Its AI-\npowered skills can assist you in spotting significant trends and insights in your data that might\notherwise be challenging to find. Additionally, ChatGPT can assist you in creating top-notch prompts\nfor paper analysis.\nLLM Functionality\nChatGPT is a prediction system that anticipates what it should write based on previously processed\ntexts.  This  sort  of  artificial  intelligence  is  known  as  a  language  model.  What  makes  it  more\npromising than its predecessors is that it is trained on enormous amounts of data, much of which\noriginates  from  the  abundant  supply  of  data  available  on  the  internet.  According  to  OpenAI,\nChatGPT was also trained on examples of back-and-forth human interaction, which makes it sound\nmuch more human in its discourse, thus advancing the capability of natural language processing\n(NLP) solutions. \nNLP is a field of artificial intelligence employing linguistics, statistics, and machine learning to\nenable computers to comprehend spoken language. NLP systems can infer meaning from spoken or\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nwritten words, including all the subtleties and complexity of an accurate narrative text. This makes it\npossible for machines to get value from even unstructured data. NLP has witnessed significant\nadvancements in recent years. The Large Language Model (LLM) is a Deep Learning algorithm that\ncan be used to perform NLP tasks, including, among other abilities, summarizing and generating\ntext. One of the applications of LLM-based chatbot. These are computer programs that can simulate\nconversations with human users. NLP techniques can be used to enable chatbots to understand and\nrespond  to  user  input.  LLM  uses  deep  learning  techniques  to  understand  and  generate  human\nlanguage. It requires training on vast amounts of text data and using statistical algorithms to learn\npatterns and relationships within language. They can perform various tasks, including language\ntranslation, question answering, sentiment analysis, and summarization. Users can learn, compare,\nand validate answers for different academic subjects, including physics, math, and chemistry, as well\nas abstract topics like philosophy and religion, using ChatGPT [3]. They can also generate human-\nlike text, such as news articles, chatbot conversations, and even literary works like essays and\nromantic poems. What makes GPTs different from other LLMs is their architecture and training\nmethodology. GPTs are based on a deep learning architecture called the \"Transformer\". Transformers\nare designed to process sequential data, such as language, more efficiently than other architectures.\nLarge language models are  currently  at  the  forefront  of  intertwining  AI  systems  with  human\ncommunication  and  everyday  life  [4] . Large  pre-trained  language  models  have  significantly\nadvanced natural language processing research on various applications  [5,6]. Although these more\ncomplicated language models can produce complex and coherent natural language, several recent\nstudies have shown that they can also pick up unfavourable social biases that can feed negative\nstereotypes [7]. \nNLP in Healthcare \nHealthcare consumers may turn to the research literature for information not provided in patient-\nfriendly documents. However, reading medical literature can be difficult. A study looked at four\nelements  made  possible  by  natural  language  processing  to  increase  access  to  medical  papers:\nexplanations of foreign terminology, plain language section summaries, a list of crucial questions\nthat direct readers to the portions that provide the answers, and simple language summaries of those\npassages. Significant advancements in smart healthcare have been made in recent years [8]. New AI\ntechnologies  enable  a  range  of  intelligent  applications  in  various  healthcare  contexts.  Natural\nlanguage processing (NLP), a fundamental AI-powered technology that can analyze and comprehend\nhuman language, is crucial for smart healthcare [9]. Natural language processing methods have been\nutilized to organize data in healthcare systems by sifting out pertinent information from narrative\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\ntexts to offer information for decision-making. Thus, NLP approaches help lower healthcare costs\nand are essential for streamlining healthcare procedures [10] .  Advancements in NLP will make\nrobotic process automation possible in healthcare, which can further drive efficiency in healthcare.\nHealthcare data is complex, and this should be given due consideration at the time of designing\nhealthcare applications. Deep Learning (DL) approaches, such as Convolutional Neural Network\n(CNN)  and  Recurrent  Neural  Network  (RNN)  models,  have  become  prominent  in  healthcare\napplications, and their accuracy is promising. Still, much must be done to enable their usage without\nhuman supervision. Deep Learning techniques offer an effective and efficient model for data analysis\nby revealing hidden patterns and extracting valuable information from a large volume of health data,\nwhich standard analytics cannot perform within a given time frame [11]. \nChatGPT in Medical Education \nChatGPT has many potential applications in healthcare education, research, and practice [12]. It can\nenhance medical education by helping students develop subjective learning and expression skills\n[13]. The number of ChatGPT users has shown exponential growth and is being increasingly utilized\nby students, residents, and attending physicians to direct learning and answer clinical questions [14].\nHowever, authors using ChatGPT professionally for academic work should exercise caution as it is\nunclear  how  ChatGPT handles  hazardous  content,  false  information,  or  plagiarism  [15]. While\nChatGPT can make radiological reporting simpler, there is still a chance of inaccurate statements and\nmissing medical information [15] . Therefore, it needs refinement before being used frequently in\nmedicine [16] . A recent review explores ChatGPT's applications and reports challenges such as\nethical concerns, data biases, and safety issues [17]. Thus, it is imperative to balance AI-assisted\ninnovation  and  human  expertise  [18]. ChatGPT  has  quickly  gained  significant  attention  from\nacademia, research, and industries despite these shortcomings. This study attempts to determine the\nrequirements for a successful LLM application in medical education using a narrative review of\nexisting literature. \nEnablers of LLM for Medical Education \nEnablers in this research refer to factors, resources, or conditions that facilitate or support achieving\na good LLM application for medical education. Medical education prepares would-be physicians and\nother healthcare professionals with the knowledge, skills, and attitudes necessary for competent and\ncompassionate  patient  care.  Enablers  make  it  easier  for  something  to  happen  or  someone  to\naccomplish a particular task. Enablers of LLM for Medical Education can be tangible or intangible\nand should play a crucial role in achieving outcomes expected from the application. \nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nAs LLMs are trained on massive data, they are resource demanding. The cost of training for an LLM\nfor medical education may be prohibitive [19]. Thus, it is imperative to use efficient computing to\naddress this issue [20]. Usability is one of the key criteria making an application useful in medical\neducation, and LLMs are no exception [21]. Extant literature cites usability as an important criterion\nfor  a  successful  technology  in  education  [22].  Similar  credibility  of  application  becomes  very\nimportant  in  medical  education  [23].  Another  recently  published  article  cites  the  credibility  of\ntechnological interventions in medical education [24]. Although ChatGPT puts disclaimers about the\nsource of information, it doesn't disclose it categorically. Worse is it hallucinates about the source\nsometimes  and  may  be  misleading.  Large  language  models  also  have  issues  with  fairness,\ncomputation, and privacy. By perpetuating social prejudices and stereotypes, they risk causing unfair\ndiscrimination, physical harm, and harm to their reputation [25]. In their study, Ma et al. provide  an\noverview of fairness in multilingual and non-English situations, emphasizing the limitations of recent\nstudies and the challenges faced by English-only methodologies [26]. \nNext comes the issue of accountability with LLMs such as ChatGPT. It is taking responsibility for\none's obligation to treat others honestly and morally. Who will be held accountable and responsible if\nthe  LLM  model  provides  incorrect  recommendations  or  forecasts  for  a  particular  downstream\nactivity? Overall, employing big language models has considerable dangers; therefore, precautions\nmust be taken to minimize these risks and ensure their ethical and responsible use. To foster a cross-\ndisciplinary  global  inclusive  consensus  on  the  ethical  use,  disclosure,  and  proper  reporting  of\nGAI/GPT/LLM technologies in academia, Cacciamani et al. presented the ChatGPT, Generative\nArtificial Intelligence, and Natural Large Language Models for Accountable Reporting and Use\nGuidelines initiative in 2023. The underlying model of GPT3.5 deviates from the ethical guidelines\nproposed by Cacciamani et al. [23].  Another important criterion reported for medical applications is\ntransparency. It is an ethic across science, engineering, business, and the humanities. It refers to\nfunctioning  in  a  way  that  makes  it  simple  for  others  to  observe  what  actions  are  taken  [28].\nTransparency is a sign of responsibility, honesty, and openness. LLMs are opaque to users. Recently\nsuggested explainability techniques aim to make language models more transparent. Although they\nare not a cure-all, they might act as the basis for models with fewer flaws or, at the very least, can\nexplain their logic. In their systematic experiments with synthetic data, Wu, Z. et al. demonstrate that\nautoregressive and masked language models can successfully learn to emulate semantic relations\nbetween  expressions  in  strong  transparency,  where  all  expressions  have  context-independent\ndenotations [29].\nFinally, the LLMs used in medical education must be explainable, and the best freely available\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\noptions  lag  here.  Most Large Language  Models (LLMs) are  complex models built  using Deep\nLearning [30]. They can produce better predictions with more information or network parameters but\nat the sacrifice of explainability. Some models fail to describe how they came to their conclusion.\nRecently suggested explainability techniques aim to make language models more transparent. Even\nthough they are not solutions, they can act as the basis for less problematic models or, at the very\nleast, models that can explain their logic. However, the authors Du, M. et al. identified false patterns\ndetected  by  LLMs  using  explainability  in  their  study  [31].  The  enablers  identified  using  the\nsecondary research are listed in Table 1. \nTable 1: Summary of Enablers of LLM for Medical Education \nEnabler Description References\nE1 Cost Cost of computation, including hardware,\nsoftware, and energy requirement. \n[19,20]\nE2 Usability User-centric  design,  ease  of  use,  and\npositive user experiences\n[21,22]\nE3 Credibility Level  of  trust  and  reliability  that  users\nplace in the application. \n[23,24]\nE4 Fairness Absence of unfair discrimination, physical\nharm, and harm to user reputation.\n[25,26]\nE5 Accountability Taking responsibility for the obligation to\ntreat users with honesty and morality.\n[27,28]\nE6 Transparency Functioning in a way that makes it simple\nfor  others  to  observe  what  actions  are\ntaken.\n[28,31]\nE7 Explainability Ability to describe how the models came\nto their conclusion\n[30,31]\nSource: Author's Compilation \nNeed of the Study \nThe need for this study arises from the rapid integration of Large Language Models (LLMs) like\nChatGPT  in  various  fields,  including  medical  education.  LLMs  offer  promising  benefits  for\nhealthcare, but their effective integration in medical education is still a developing area. This study\naims to identify and prioritize the key enablers for successful LLM implementation in medical\neducation. It addresses the lack of comprehensive frameworks guiding the development and use of\nLLMs in this field. By exploring the dynamics of various enablers such as credibility, accountability,\nfairness, cost, usability, transparency, and explainability, the study provides a structured approach to\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nenhance the quality and effectiveness of LLMs in educating healthcare professionals.  Thus, this\nstudy attempts to answer three major research questions: (1) What are the enablers of a suitable LLM\napplication for medical education? (2) What is the relative importance of these enablers in achieving\nthe goals of medical education? and (3) What is an approach to developing an LLM to achieve\nmedical education goals? With this background, the following are the research objectives (RO) of the\nstudy: \nRO1: Identify the enabler of a suitable LLM for medical education.\nRO2:  Prioritize  the  identified  enablers  in  achieving  the  goals  of  medical  education.\nRO3: Propose a framework for developing an LLM to achieve the medical education goals. \nMethodology \nTo  achieve  the  first  research  objective,  this  study  uses  a  narrative  review  of  extant  literature\npublished on technology solutions in medical education. A narrative review is a scholarly article\nsynthesizing  existing  research  on  a particular  topic  in  a  narrative  or story-like  manner.  Unlike\nsystematic reviews or meta-analyses, which use rigorous methodologies to analyze and summarize\nresearch findings quantitatively, narrative reviews provide a qualitative, comprehensive overview of\na subject. They often involve critical analysis and discussion, integrating the author's expertise and\ninterpretation.  Narrative reviews are  useful  for obtaining  a  broad understanding  of a topic and\nidentifying trends, gaps, and controversies within a field. The authors SM and VM searched Scopus,\nWeb of Science, and Google Scholar databases to identify suitable literature for the review. The\narticles selected for the study are literature in the English language and have been published in the\nlast  five  years.  The  second  stage  eliminated  duplicates  and  articles  for  which  full  text  was\nunavailable. One article published in 2010 was added on the recommendation of the focus group as it\nwas  found  useful  in  explaining  competing  interests  in  medical  education.  The  seven  identified\nenablers (E1 to E7) help us answer Research Question 1. These enablers were presented in front of a\nfocus group comprising seven experts working in universities and institutions delivering medical\neducation in India and the United Arab Emirates to validate the selection of barriers Table 2. The\nfocus group endorsed the choice of their seven enablers for further research.  The researcher VM\nfacilitated the focus group discussion to finalize the enablers. \nTable 2: Characteristics of the Focus Group Used for AHP\nExpert Qualification Experienc Age Nationality \nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\ne \nCardiologist Masters in Medicine 12 42 India\nEndocrinologist Masters in Medicine 20 45 India \nTechnology Expert Doctor of Philosophy 15 50 USA\nDentistry Educator Masters in Dentistry 10 40 UAE\nPodiatrists Educator Doctor of Philosophy 10 35 UAE\nDiabetes Educator Doctor of Philosophy 18 43 India \nNursing Educator Doctor of Philosophy 15 41 UAE \nRadiologist Doctor of Philosophy 12 41 India \nSource: Author's Compilation \nAHP Modelling  \nThe Analytical Hierarchy Process (AHP) was utilized to achieve the second objective. AHP is a\npopular method for calculating the relative importance of the criteria in the Multi-Criteria Decision\nAnalysis (MCDA) method. It has been extensively used in the management and social science\nliterature  [32].  The  advantage  of  the  process  is  that  it  incorporates  the  mechanisms  to  assure\nreliability in the decision-making case of ambiguity; researchers have suggested using a Fuzzy\nversion of AHP [33]. Further, some researchers have suggested the entropy weight method to reduce\nthe  negative  effect  of  individual  subjective  evaluation  bias  on  the  accuracy  of  comprehensive\nevaluation  [34].  Since  the  ranking  obtained  by  the  AHP method  is  further  validated  by  Total\nInterpretive Structural Modeling (TISM), Fuzzy Logic or Entropy Weight was avoided. The five\nsteps used for AHP are (1) Defining the Decision Problem, (2) Creating a Hierarchy, (3) Pairwise\nComparison, (4) Derive Weighted Priority, and (5) Consistency Check for Decision. The method\nused for pairwise comparison was the Delphi method. A Cut-off of 75% was used to accept the value\nfor  the  pairwise  comparison.  The  standard  scale  Saaty  suggested  was  used  for  the  pairwise\ncomparison [35]. Researcher VM facilitated the data collection for the AHP model. \nTISM Modeling \nFinally, for the third objective, this study investigates the relationships among key enablers for\nbuilding a suitable medical education LLM. Qualitative research design is useful to understand the\nphenomenon under study instead of assessing the strength and direction of causal relationships in the\nconceptual  model  [36]. This  objective  utilized  a  focus  group  with  five  information  technology\nexperts with product development and research experience. The details of this expert group are listed\nin Table 3.\nTable 3: Characteristics of the Focus Group Used for TISM\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nExpert Qualification Experienc\ne \nAge Country \nProduct Development Masters in management 21 42 Singapore \nProduct Development Bachelors  in\nengineering \n21 42 UAE\nTechnology Expert Bachelors  in\nengineering\n19 40 India \nTechnology Expert Masters in engineering 10 33 India\nDecision Science Expert Doctor of Philosophy 10 38 India \nSource: Author's Compilation \nThis study has used TISM to model the enabler for medical education LLM application. In his\nseminal paper, Sushil provides a detailed account of the interpretation of ISM and TISM and the\nlatter's advantage over the former [37]. For the sake of brevity, the authors have not included the\ndetails of the TISM method in this paper. It is a process that converts poorly articulated mental\nmodels of systems into visible and well-defined models useful for better understanding and decision-\nmaking [38]. The presence and absence of a relationship between enablers were ascertained based on\nan unstructured interview of the focus group conducted by the researcher SM. If more than fifty per\ncent of the focus group members think there is a relationship between two enablers, the response was\ntaken as 'Y'. The summary of the method used in the research is described in Figure 1. \nEthical Considerations \nThis study, involving a qualitative focus group discussion, did not require approval from an ethical\nreview  board  as it  did not  involve  human  subjects  in  a manner  necessitating  such  review. No\ninformed consent was required for the same reason. However, to maintain ethical standards, we\nensured that all data collected was either anonymized or de-identified. This means any information\nthat could potentially identify individual participants was removed or altered to protect their privacy.\nNo compensation was provided to participants, as is common in studies of this nature. This decision\nwas made considering the study's design and the ethical imperative to avoid undue influence on\nparticipants'  responses.  The  absence  of  compensation  was  communicated  to  all  participants.\nThroughout the study, we adhered to strict data protection protocols to safeguard the confidentiality\nof the information shared during the focus group discussions. These measures included secure data\nstorage,  restricted  access  to  authorized  personnel,  and  adherence  to  data  protection  laws  and\nregulations. This approach ensured that the privacy and integrity of participant information were\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nalways maintained.\nFigure 1: Summary of the TISM Approach Used in the Study\nSource:  Adapted by Authors [34]\nResults\nThe results of the narrative review are summarised in Table 1 in the earlier section. This section\npresents the results related to the second and third research objectives, respectively. \nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nAHP Modelling\nThe list of the selected enablers for developing a suitable LLM medical education application is\ndepicted in Table 1. The focus group was asked to provide their input for pairwise comparison, and\nthe resultant matrix [A] is described in Table 4. \nTable 4: Initial Pairwise Comparison Matrix for the AHP\n    E1 E2 E3 E4 E5 E6 E7\nCost (E1) E1 1 3 0.2 1 0.2 3 3\nUsability (E2) E2 0.33 1 0.11 0.33 0.11 1 1\nCredibility (E3) E3 5 9 1 5 5 3 3\nFairness (E4) E4 1 3 0.2 1 0.2 3 3\nAccountability (E5) E5 5 9 0.2 5 1 5 5\nTransparency (E6) E6 0.33 1 0.33 0.33 0.2 1 1\nExplainability (E7) E7 0.33 1 0.33 0.33 0.2 0.2 1\nSource: Author's Compilation \nOnce the Initial Comparison Matrix was determined, it was normalized, and an average of each row\nwas taken to calculate the priority weight [X]. The normalized matrix, priority weight (PW) and rank\nof the enablers is given in Table 5. The priority weight, the eigenvector, is used to calculate the\nconsistency ratio further. \nTable 5: Normalized Matrix and Priority Weight of Enablers\n  E1 E2 E3 E4 E5 E6 E7 PW Rank\nE1 0.077 0.1111 0.0844 0.077 0.0289 0.1852 0.1765 0.10572 3\nE2 0.0254 0.037 0.0464 0.026 0.0159 0.0617 0.0588 0.03871 7\nE3 0.3849 0.3333 0.4219 0.385 0.7236 0.1852 0.1765 0.37289 1\nE4 0.077 0.1111 0.0844 0.077 0.0289 0.1852 0.1765 0.10572 3\nE5 0.3849 0.3333 0.0844 0.385 0.1447 0.3086 0.2941 0.27642 2\nE6 0.0254 0.037 0.1392 0.025 0.0289 0.0617 0.0588 0.0538 5\nE7 0.0254 0.037 0.1392 0.025 0.0289 0.0123 0.0588 0.04674 6\nSource: Author's Compilation \nFinally, the consistency Ratio of judgement was calculated by dividing the consistency index (CI) by\nthe Random Index (RI). The next stage is to calculate λmax   Leading to the calculation of CI. For the\ncalculation of eigenvector X, the following equation is applicable: \n[A] X= λmax  X – (1)\nUsing Table 3-4 and Equation 1:\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\n[ A ] X=¿ 0.28,3.46,0.76,2.26,0.39.0.34] – (2)\nλmax= Average{ 0.76\n0.11 , 0.24\n0.04 , 3.46\n0.37 , 0.76\n0.11 , 2.26\n0.28 , 0.39\n0.05 , 0.34\n0.05 }  - (3) \nλmax=7.66  – (4)\nCI= (7.66-7)/6=0.11 – (5) \nThe random index (RI) value for a 7X7 Matrix is 1.32 from the Random Index table. Thus, the\nConsistency Ratio (CR) becomes 0.084, less than 0.1, hence acceptable. \nTISM Modelling \nNow, we move to the result of TISM for ascertaining the relationship between these seven enablers.\nTable 6 shows a matrix indicating the interrelationships between enablers listed in Table 1. The\nexistence of a relationship between enablers is depicted by the letter 'Y', while absence is represented\nby the letter 'N'. The resultant matrix is called the Structural Self-Interaction Matrix (SSIM). \nTable 6: Structural Self-Interaction Matrix for the Study\nSN Enablers for LLMs E1 E2 E3 E4 E5 E6\nE\n7\nE1 Cost (E1) Y Y N N N Y N\nE2 Usability (E2) Y Y N N N Y Y\nE3 Credibility (E3) N N Y Y Y N N\nE4 Fairness (E4) N N Y Y N N N\nE5 Accountability (E5) N N Y N Y N N\nE6 Transparency (E6) Y Y N N N Y Y\nE7 Explainability (E7) N Y N N N Y Y\nSource: Authors' Compilation\nThe next step is to replace all 'Y\" with '1' and all 'N' with 0 and incorporate the transitivity rule to get\nthe Final Reachability Matrix (FRM) listed in Table 7. \n Table 7: Final Reachability Matrix for the Study\nSN Enablers for LLMs E1 E2 E3 E4 E5 E6 E7\nDriving\nPower \nE1 Cost (E1) 1 1 0 0 0 1 1 4\nE2 Usability (E2) 1 1 0 0 0 1 1 4\nE3 Credibility (E3) 0 0 1 1 1 0 0 3\nE4 Fairness (E4) 0 0 1 1 0 0 0 2\nE5 Accountability (E5) 0 0 1 0 1 0 0 2\nE6 Transparency (E6) 1 1 0 0 0 1 1 4\nE7 Explainability (E7) 0 1 0 0 0 1 1 3\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nDependence Power 3 4 3 2 2 4 4\nSource: Authors' Compilation\nThe  next  step  in  developing  models  for  medical  education  LLMs  is  to  list  Reachability  and\nAntecedent sets for each enabler and perform Level Partitioning (LP). LP is an iterative process of\nassigning barriers at different levels. Enablers with similar intersection sets as reachability sets are\nplaced at the top level. The process is repeated until levels for all the enablers are established. In this\nstudy, all enablers were assigned after three iterations; hence, there are three levels in the hierarchy.\nThe summary of level partitioning for this study is listed in Table 8. The level of an enabler indicates\ndriving and dependence power, as indicated in Table 7. The higher the level of the enabler, the more\ndependent it is. At the same time, the driving ability improves as we go down to the lower levels.\nTable 8: Summary of Label Partitioning (LP) Iterations (1 to 6) \nEnablers\n(Mi)\nReachability Set\nR(Mi)\nAntecedent  Set\nA(Ni)\nIntersection  Set\nR(Mi)∩A(Ni) Level\n1 1 1 1 III\n2 1,2,6,7 1,2,6,7 1,2,6,7 I\n3 3,4,5 3,4,5 3,4,5 I\n4 3,4 3,4 3,4 I\n5 3,5 3,5 3,5 I\n6 1,2,6,7 1,2,6,7 1,2,6,7 I\n7 7 1,7 7 II\nSource: Authors' Compilation\nTISM Model \nOnce the level partitioning was done, the TISM model was developed and presented to the focus\ngroup for validation. Only significant transitive links were included in the model to make it easy to\ninterpret. The final digraph for the TISM model developed in the study is depicted in Figure 2.\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nFigure 2: Diagraph of TISM for LLM in Medical Education  \nSource: Authors Compilation \nMICMAC Analysis \nThe  study  further  uses  Cross-impact  matrix  multiplication  applied  to  classification  (MICMAC)\nanalysis to validate the study's findings and derive conclusions.  It involved the development of a\ngraph that classifies enablers based on their driving and dependence power Figure 3. The first\nquadrant contains autonomous enablers E3, E4, and E6 (Credibility, Fairness and Accountability).\nThe variables falling in this quadrant have low driving and dependence power. The two enablers\nfalling in the grey region between the third (linkage) and fourth (independent) quadrants are E2 and\nE6 (Usability and Transparency) and have medium driving and dependence power. Similarly, E7\n(Explainabilty) falls in the grey region between the first (autonomous) and second (dependent)\nvariables. Finally, E1 (Cost) falls under the fourth (independent) quadrant. \nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nFigure 3: MICMAC Analysis for Enablers of LLM in Medical Education\nSource: Authors Compilation \nDiscussion \nThe results of the AHP suggest that credibility, followed by accountability, are the foremost enablers\nfor  effective  LLMs  in  medical  education.  Extant  literature  supports  this  finding,  as  literature\nmentions the source of information based on which the response was generated [39]. Similarly, the\nimportance of defining accountability is reported in recent literature. Similar to an existing study, the\nresearchers also advocate accountability as an important factor in increasing the adoption of LLMs in\nmedical education, training, and practice [40]. Next in importance are ethical issues such as fairness\nand cost. LLMs have been criticized for bias against gender or ethnic groups [41]. These problems\nneed to be addressed to make LLMs effective in medical education. Secondly, training LLMs on\nbillions of parameters is demanding; thus, technology giants will launch these LLMs [42]. The\ngovernments should ensure that the cost of using these LLMs doesn't become prohibitive for end\nusers, and they resort to half-baked solutions, eventually affecting the safety of patients.\nContrary  to  existing  studies,  transparency  and  explainability  come  in  fifth  and  sixth  place  in\nimportance [40]. Many best practices related to health technology suggest that models should use\nexplainable artificial intelligence in medical devices [41]. The low priority of these enablers indicates\nthat the end user is unaware of the criticality of these factors, and healthcare professionals need to be\neducated about them as they are not technology savvy [43]. Governments should make guidelines for\nthe approval of Software as Medical Devices so that these enablers are taken care of at the product\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\ndevelopment stage. Finally, the focus group thinks that usability is the least important factor out of\nthe enablers discussed in the study. Although the general-purpose LLMs, such as ChatGPT, are less\ncluttered, their performance is input-dependent. Improving the prompt use of the recommendation\nsystem can enhance the usability and accuracy of the LLMs in medical education [44]. The expert\ngroup advised that the LLMs will improve on these factors with time. \nNext, we will discuss product development and technology expert's input for the TISM model. The\nmodel results suggest a slight difference in the perspective of product developers and end users. They\ngive  equal importance to  Credibility,  Fairness, Accountability, Transparency,  and Explainability.\nThese results are consistent with extant literature published in peer-reviewed journals [40,42]. These\nare all features related to model development and training.\nContrary to earlier studies, this group also gives usability less significance and puts it on a medium\nlevel [44]. Thus, the finding of the TISM validates the results of AHP. The only difference is the cost\nis the least important for product developers. Recently published studies opine that economic and\nenvironmental costs are significant in developing general-purpose LLMs [45]. \nA  successful  LLM  development  involves  a  complex  interplay  between  technical  innovation,\nregulatory compliance, production costs, and end-user needs. The aim should be to develop products\nthat excel in functionality and positively impact the lives of those who rely on them without causing\nfinancial hardship. Thus, this study calls for collaboration between product developers, original\nequipment  manufacturers,  regulators,  and  other  stakeholders  to  find  solutions  that  align  with\ntechnological advancements and societal expectations for affordability and accessibility.\nFinally, the study validates its findings using MICMAC analysis, creating a graph that categorizes\nenablers based on driving and dependence power. Here, enablers, namely, Credibility, Fairness, and\nAccountability, are in the first quadrant (Autonomous) with low power. These variables are relatively\nindependent and have limited influence on other variables. While Usability and Transparency are in\nthe  grey  region  between  the  third  quadrant  (Linkage)  and  fourth  (independent)  quadrants  with\nmedium power, they have a medium influence on other variables and are similarly influenced by\nthem. Explainability falls in the grey region between the first (Autonomous) and second (Dependent)\nquadrants, while cost is in the fourth quadrant. Again, made us conclude that their enablers have a\nmedium influence on other variables and a similar influence on them. Finally, the cost falls under the\nfourth quadrant (Independent) and makes us believe it strongly influences other enablers without\nbeing  significantly  influenced  by  them.  MICMAC  analysis  comprehensively  explains  the\nrelationships and dynamics among variables within a complex system. It helps decision-makers\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nidentify  key  drivers,  dependencies,  and  interactions,  enabling  them  to  make  informed  strategic\ndecisions and allocate resources effectively.\nConclusion \nThe study emphasizes key factors for effective Language Models (LLMs) in medical education:\nCredibility and Accountability are vital, while addressing bias and cost is crucial for LLM potential.\nThough  important,  Transparency  and  Explainability  rank  lower  among  health  professionals,\nsuggesting a need for education. Usability is the least important, but enhancing prompt use improves\nLLM accuracy. The study highlights a slight difference between product developers and end users.\nBoth  prioritize  Credibility,  Fairness,  Accountability,  Transparency,  and  Explainability.  Usability\nranks lower for developers. Successful LLM development balances innovation, compliance, costs,\nand  user  needs.  Collaboration  among  stakeholders  is  crucial  for  aligning  with  technology  and\nsocietal expectations.\nThe study has one implication each for theory as well as practice. For theory, this study extends the\nFATE  framework.  It  proposes  a  more  comprehensive  CUC-FATE  (Cost,  Usability,  Credibility,\nFairness,  Accountability,  Transparency,  and  Explainability)  for  developing  LLMs  for  healthcare\nprofessionals. For practice, this study is the first of its kind and provides a prescriptive framework for\ndeveloping LLMs in healthcare, especially medical education. The study's findings are useful for\npolicymakers,  medical  device  regulators,  education  policymakers,  healthcare  professionals,  and\nproduct developers at the helm of making software as a medical device (SaMD).  \nOne of the limitations of the study is that it uses experts from India and UAE only. Although\ntechnology  and  healthcare  practices  are  standardized  globally,  the  findings  should  only  be\ngeneralized to the population from these geographies. The study provides the relationship between\ndifferent enablers but does not discuss the strength of these associations. Graph Theory or Structured\nEquation Modeling can be used to address these gaps in future studies. \nAcknowledgment \nThe authors are highly indebted to all focus group participants for their time and effort. Authors are\nalso obliged to their respective institutions for infrastructural support provided. The authors also\ndisclose using Artificial Intelligence tools Grammarly and Quillbot for manuscript language editing. \nFunding Statement \nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nThe Article Processing Charges (APC) for the publication of the manuscript are funded by the\nCollege of Business Administration, Kuwait University. \nData Availability \nThe necessary data and calculations for the Analytic Hierarchy Process (AHP) model and the Self\nInteraction Matrix for the Total Interpretive Structural Modeling (TISM) Model are available on a\nGitHub repository. To access this information, please refer to the link to our document provided in\nreference [46].\nConflict of Interest\nThe authors state that there is no conflict of interest to report for this study. \nAuthors Contribution \nConceptualization: Vinaytosh  Mishra, Majdi  Quttainah, Somayya  Madakam,  Yotam  Lurie,  and\nShlomo Mark\nData curation: Vinaytosh Mishra, Somayya Madakam\nFormal Analysis: Vinaytosh Mishra, Yotam Lurie, and Shlomo Mark\nFunding acquisition: Majdi Quttainah\nMethodology: Vinaytosh Mishra, Majdi Quttainah\nProject administration: Majdi Quttainah\nSupervision: Yotam Lurie and Shlomo Mark\nValidation: Yotam Lurie and Shlomo Mark\nVisualization: Vinaytosh Mishra \nWriting – original draft: Vinaytosh Mishra, Majdi Quttainah\nWriting – review & editing: Yotam Lurie and Shlomo Mark\nReferences \n1. Haque, M. U., Dharmadasa, I., Sworna, Z. T., Rajapakse, R. N., & Ahmad, H. (2022). \" I\nthink  this  is  the  most  disruptive  technology\":  Exploring  Sentiments  of  ChatGPT  Early\nAdopters using Twitter Data. arXiv preprint arXiv:2212.05856.\n2. Nastasi, A. J., Courtright, K. R., Halpern, S. D., & Weissman, G. E. (2023). Does ChatGPT\nprovide  appropriate  and  equitable  medical  advice?  A vignette-based,  clinical  evaluation\nacross care contexts. medRxiv, 2023-02.\n3. Liu, Y ., Han, T., Ma, S., Zhang, J., Yang, Y ., Tian, J., ... & Ge, B. (2023). Summary of\nChatGPT/gpt-4 research and perspective towards the future of large language models. arXiv\npreprint arXiv:2304.01852.\n4. Hagendorff,  T.  (2023).  Machine  psychology:  Investigating  emergent  capabilities  and\nbehaviour  in  large  language  models  using  psychological  methods.  arXiv  preprint\narXiv:2303.13988.\n5. Májovský,  M.,  Černý,  M.,  Kasal,  M.,  Komarc,  M.,  &  Netuka,  D.  (2023).  Artificial\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nIntelligence  Can Generate  Fraudulent  but  Authentic-Looking  Scientific  Medical  Articles:\nPandora's Box Has Been Opened. Journal of Medical Internet Research, 25, e46924. \n6. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D.\n(2020). Language models are few-shot learners. Advances in neural information processing\nsystems, 33, 1877-1901.\n7. May, C., Wang, A., Bordia, S., Bowman, S. R., & Rudinger, R. (2019). On measuring social\nbiases in sentence encoders. arXiv preprint arXiv:1903.10561.\n8. August, T., Wang, L. L., Bragg, J., Hearst, M. A., Head, A., & Lo, K. (2022). Paper plain:\nMaking medical research papers approachable to healthcare consumers with natural language\nprocessing. ACM Transactions on Computer-Human Interaction.\n9. Kaelin, V . C., Valizadeh, M., Salgado, Z., Parde, N., & Khetani, M. A. (2021). Artificial\nintelligence in rehabilitation targeting the participation of children and youth with disabilities:\nScoping review. Journal of Medical Internet Research, 23(11), e25745.\n10. Iroju, O. G., & Olaleke, J. O. (2015). A systematic review of natural language processing in\nhealthcare. International Journal of Information Technology and Computer Science, 8, 44-50.\n11. Lavanya, P. M., & Sasikala, E. (2021, May). Deep learning techniques on text classification\nusing Natural language processing (NLP) in social healthcare network: A comprehensive\nsurvey. In 2021, the 3rd International Conference on Signal Processing and Communication\n(ICPSC) (pp. 603-609). IEEE.\n12. Sallam,  M.  (2023),  ChatGPT  utility  in  healthcare  education,  research,  and  practice:\nsystematic review on the promising perspectives and valid concerns. Healthcare,11(6),887,\nhttps://doi.org/10.3390/healthcare11060887\n13. Seetharaman, R. (2023). Revolutionizing Medical Education: Can ChatGPT Boost Subjective\nLearning and Expression? Journal of Medical Systems, 47(1), 1-4.\n14. Grabb, D. (2023). ChatGPT in Medical Education: a Paradigm Shift or a Dangerous Tool?\nAcademic Psychiatry, 1-2. https://doi.org/10.1007/s40596-023-01791-9\n15. Kleebayoon,  A., &  Wiwanitkit,  V .  (2023). ChatGPT in  medical  practice,  education,  and\nresearch: malpractice and plagiarism. Clinical Medicine, 23(3), 280-280.\n16. Liu, J., Wang, C., & Liu, S. (2023). Utility of ChatGPT in clinical practice. Journal of\nMedical Internet Research, 25, e48568.\n17. Ray,  P.  P.  (2023).  ChatGPT:  A comprehensive  review  on  background,  applications,  key\nchallenges, bias, ethics, limitations, and future scope. Internet of Things and Cyber-Physical\nSystems. https://doi.org/10.1016/j.iotcps.2023.04.003  \n18. Milano, S., McGrane, J. A., & Leonelli, S. (2023). Large language models challenge the\nfuture of higher education. Nature Machine Intelligence, 5(4), 333-334.\n19. Chen, J., & Ran, X. (2019). Deep learning with edge computing: A review. Proceedings of the\nIEEE, 107(8), 1655-1674.\n20. Bharany, S., Sharma, S., Khalaf, O. I., Abdulsahib, G. M., Al Humaimeedy, A. S., Aldhyani,\nT. H., ... & Alkahtani, H. (2022). A systematic survey on energy-efficient techniques in\nsustainable cloud computing. Sustainability, 14(10), 6256.\n21. Johnson, S. G., Potrebny, T., Larun, L., Ciliska, D., & Olsen, N. R. (2022). Usability methods\nand attributes reported in usability studies of mobile apps for health care education: a scoping\nreview. JMIR Medical Education, 8(2), e38259.\n22. Lu,  J.,  Schmidt,  M.,  Lee,  M.,  &  Huang,  R.  (2022).  Usability  research  in  educational\ntechnology:  A  state-of-the-art  systematic  review.  Educational  technology  research  and\ndevelopment, 70(6), 1951-1992.\n23. Hein, H. J., Glombiewski, J. A., Rief, W., & Riecke, J. (2022). Effects of a video intervention\non physicians' acceptance of pain apps: a randomized controlled trial. BMJ open, 12(4),\ne060020.\n24. Skalidis, I., Muller, O., & Fournier, S. (2022). CardioVerse: The cardiovascular medicine in\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nthe  era  of  Metaverse.  Trends  in  Cardiovascular  Medicine.\nhttps://doi.org/10.1016/j.tcm.2022.05.004\n25. Lund, B. D., & Wang, T. (2023). Chatting about ChatGPT: how may AI and GPT impact\nacademia and libraries? Library Hi Tech News, 40(3), 26-29.\n26. Ma, H., Zhang, C., Bian, Y ., Liu, L., Zhang, Z., Zhao, P., ... & Wu, B. (2023). Fairness-guided\nFew-shot Prompting for Large Language Models. arXiv preprint arXiv:2303.13217.\n27. Cacciamani, G. E., Eppler, M. B., Ganjavi, C., Pekan, A., Biedermann, B., Collins, G. S., &\nGill,  I.  S.  (2023).  Development  of  the  ChatGPT,  Generative  Artificial  Intelligence  and\nNatural  Large  Language  Models  for  Accountable  Reporting  and  Use  (CANGARU)\nGuidelines. arXiv preprint arXiv:2307.08974.\n28. Hébert, P. C., MacDonald, N., Flegel, K., & Stanbrook, M. B. (2010). Competing interests\nand undergraduate medical education: time for transparency. CMAJ, 182(12), 1279-1279.\n29. Wu, Z., Merrill, W., Peng, H., Beltagy, I., & Smith, N. A. (2023). Transparency Helps Reveal\nWhen Language Models Learn Meaning. Transactions of the Association for Computational\nLinguistics, 11, 617-634.\n30. Susnjak, T. (2023). Beyond Predictive Learning Analytics Modelling and onto Explainable\nArtificial  Intelligence  with  Prescriptive  Analytics  and  ChatGPT.  International  Journal  of\nArtificial Intelligence in Education, 1-31. https://doi.org/10.1007/s40593-023-00336-3\n31. Du, M., He, F., Zou, N., Tao, D., & Hu, X. (2022). Shortcut learning of large language\nmodels in natural language understanding: A survey. arXiv preprint arXiv:2208.11857.\n32. Mishra, V ., & Singh, J. (2022). Health technology assessment of telemedicine interventions in\ndiabetes management: Evidence from UAE. FIIB Business Review, 23197145221130651.\n33. Dua, S., Sharma, M. G., Mishra, V ., & Kulkarni, S. D. (2022). Modelling perceived risk in a\nblockchain-enabled  supply  chain  utilizing  fuzzy-AHP.  Journal  of  Global  Operations  and\nStrategic Sourcing, 16(1), 161-177.\n34. Mishra, V ., & Rana, S. (2023). Understanding barriers to inbound medical tourism in the\nUnited  Arab  Emirates from  a provider's perspective.  Worldwide  Hospitality  and Tourism\nThemes, 15(2), 131-142.\n35. Ahmed, F., & Mishra, V . (2020). Estimating relative immediacy of water-related challenges\nin  Small  Island  Developing  States  (SIDS)  of  the  Pacific  Ocean  using  AHP modelling.\nModelling Earth Systems and Environment, 6(1), 201-214.\n36. Groenland, E., & Dana, L. P. (2020). Qualitative methodologies and data collection methods:\nToward increased rigor in management research. World Scientific\n37. Sushil  (2012),  \"Interpreting  the  interpretive  structural  model.  Global  Journal  of  Flexible\nSystems Management, 13(2). 87-106.\n38. Prasad, U.C. & Suri, R.K. (2011). Modelling of continuity and change forces in private\nhigher  technical  education  using  total  interpretive  structural  modelling  (TISM).  Global\nJournal of Flexible Systems Management, 12 (3), 31-40.\n39. Jamal, A., Solaiman, M., Alhasan,  K., Temsah,  M. H., & Sayed,  G. (2023). Integrating\nChatGPT in medical education: adapting curricula to cultivate competent physicians for the\nAI era. Cureus, 15(8), DOI: 10.7759/cureus.43036\n40. Tan, L. F., Heng, J. J. Y ., & Teo, D. B. (2023). Response to:\" The next paradigm shift?\nChatGPT, artificial intelligence, and medical education\". Medical teacher, 1-1. https://doi.org/\n10.1080/0142159X.2023.2256961 \n41. Ray,  P.  P.  (2023).  ChatGPT:  A comprehensive  review  on  background,  applications,  key\nchallenges, bias, ethics, limitations and future scope.  Internet of Things and Cyber-Physical\nSystems. https://doi.org/10.1016/j.iotcps.2023.04.003  \n42. Rudolph, J., Tan, S., & Tan, S. (2023). War of the chatbots: Bard, Bing Chat, ChatGPT, Ernie\nand beyond. The new AI gold rush and its impact on higher education. Journal of Applied\nLearning and Teaching, 6(1). DOI: https://doi.org/10.37074/jalt.2023.6.1.23 \nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\n43. Baslom, M. M. M., & Tong, S. (2019). Strategic management of organizational knowledge\nand  employee  awareness  about  artificial  intelligence  with  mediating  effect  of  learning\nclimate. International Journal of Computational Intelligence Systems, 12(2), 1585.\n44. Rao, A., Pang, M., Kim, J., Kamineni, M., Lie, W., Prasad, A. K., ... & Succi, M. D. (2023).\nAssessing the utility of ChatGPT throughout the entire clinical workflow: Development and\nusability study. Journal of Medical Internet Research, 25, e48659.\n45. Zhang, J., Krishna, R., Awadallah, A. H., & Wang, C. (2023). EcoAssistant: Using LLM\nAssistant More Affordably and Accurately. arXiv preprint arXiv:2310.03046.\n46. Mishra, V . (n.d.). Data for AHP and TISM Model for the CUCFATE Framework. Retrieved\nDecember 20, 2023, from \nhttps://github.com/vinaytosh/datasharing/blob/master/Data_CUCFATE.xlsx\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nSupplementary Files\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nFigures\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nSummary of the TISM Approach Used in the Study.\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nDiagraph of TISM for LLM in Medical Education.\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nMICMAC Analysis for Enablers of LLM in Medical Education.\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nMultimedia Appendixes\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nData Source for AHP and TISM Modeling.\nURL: http://asset.jmir.pub/assets/15957931c196ef84b1dd285a5a68788c.xlsx\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n\nJMIR Preprints Quttainah et al\nCONSORT (or other) checklists\nCOREQ Check List.\nURL: http://asset.jmir.pub/assets/7a731e773d26e165110d4df188d99dc3.pdf\nPowered by TCPDF (www.tcpdf.org)\nhttps://preprints.jmir.org/preprint/51834 [unpublished, peer-reviewed preprint]\n",
  "topic": "Credibility",
  "concepts": [
    {
      "name": "Credibility",
      "score": 0.8811935186386108
    },
    {
      "name": "Transparency (behavior)",
      "score": 0.8430477380752563
    },
    {
      "name": "Usability",
      "score": 0.8364391326904297
    },
    {
      "name": "Accountability",
      "score": 0.823338508605957
    },
    {
      "name": "Preprint",
      "score": 0.736010730266571
    },
    {
      "name": "Narrative",
      "score": 0.6542523503303528
    },
    {
      "name": "Computer science",
      "score": 0.5052083134651184
    },
    {
      "name": "Computer security",
      "score": 0.2389521598815918
    },
    {
      "name": "Human–computer interaction",
      "score": 0.20527926087379456
    },
    {
      "name": "Political science",
      "score": 0.19393602013587952
    },
    {
      "name": "World Wide Web",
      "score": 0.1457396149635315
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}