{
    "title": "Adaptive Sparse Transformer for Multilingual Translation",
    "url": "https://openalex.org/W3154456300",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5102395716",
            "name": "Hongyu Gong",
            "affiliations": [
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A5061256224",
            "name": "Xian Li",
            "affiliations": [
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A5048470138",
            "name": "Dmitriy Genzel",
            "affiliations": [
                "Meta (Israel)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W273093436",
        "https://openalex.org/W3119866685",
        "https://openalex.org/W2964085268",
        "https://openalex.org/W3127719526",
        "https://openalex.org/W3107826490",
        "https://openalex.org/W3090350559",
        "https://openalex.org/W3040573126",
        "https://openalex.org/W2958953787",
        "https://openalex.org/W2970925270",
        "https://openalex.org/W3035019713",
        "https://openalex.org/W2963983698",
        "https://openalex.org/W2963088995",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2996490626",
        "https://openalex.org/W2970565456",
        "https://openalex.org/W2950733326",
        "https://openalex.org/W2970925677",
        "https://openalex.org/W3128413221",
        "https://openalex.org/W2964034111",
        "https://openalex.org/W3105281812",
        "https://openalex.org/W2983040767",
        "https://openalex.org/W3106539628",
        "https://openalex.org/W2953190730",
        "https://openalex.org/W2949911645"
    ],
    "abstract": "Multilingual machine translation has attracted much attention recently due to its support of knowledge transfer among languages and the low cost of training and deployment compared with numerous bilingual models. A known challenge of multilingual models is the negative language interference. In order to enhance the translation quality, deeper and wider architectures are applied to multilingual modeling for larger model capacity, which suffers from the increased inference cost at the same time. It has been pointed out in recent studies that parameters shared among languages are the cause of interference while they may also enable positive transfer. Based on these insights, we propose an adaptive and sparse architecture for multilingual modeling, and train the model to learn shared and language-specific parameters to improve the positive transfer and mitigate the interference. The sparse architecture only activates a sub-network which preserves inference efficiency, and the adaptive design selects different sub-networks based on the input languages. Our model outperforms strong baselines across multiple benchmarks. On the large-scale OPUS dataset with $100$ languages, we achieve $+2.1$, $+1.3$ and $+6.2$ BLEU improvements in one-to-many, many-to-one and zero-shot tasks respectively compared to standard Transformer without increasing the inference cost.",
    "full_text": "Adaptive Sparse Transformer for Multilingual Translation\nHongyu Gong, Xian Li, Dmitriy Genzel\nMeta AI\n{hygong,xianl,dgenzel}@fb.com\nAbstract\nMultilingual machine translation has attracted\nmuch attention recently due to its support of\nknowledge transfer among languages and the\nlow cost of training and deployment compared\nwith numerous bilingual models. A known\nchallenge of multilingual models is the nega-\ntive language interference. In order to enhance\nthe translation quality, deeper and wider archi-\ntectures are applied to multilingual modeling\nfor larger model capacity, which suffers from\nthe increased inference cost at the same time.\nIt has been pointed out in recent studies that\nparameters shared among languages are the\ncause of interference while they may also en-\nable positive transfer. Based on these insights,\nwe propose an adaptive and sparse architecture\nfor multilingual modeling, and train the model\nto learn shared and language-speciﬁc parame-\nters to improve the positive transfer and miti-\ngate the interference. The sparse architecture\nonly activates a sub-network which preserves\ninference efﬁciency, and the adaptive design\nselects different sub-networks based on the in-\nput languages. Our model outperforms strong\nbaselines across multiple benchmarks. On the\nlarge-scale OPUS dataset with 100 languages,\nwe achieve +2.1, +1.3 and +6.2 BLEU im-\nprovements in one-to-many, many-to-one and\nzero-shot tasks respectively compared to stan-\ndard Transformer without increasing the infer-\nence cost.\n1 Introduction\nMultilingual neural machine translation (MNMT)\ndevelops one model for translations in multiple lan-\nguage directions (Tan et al., 2019). A key advan-\ntage of multilingual models is the knowledge trans-\nfer, which improves the translation performance\nespecially for low-resource languages (Zoph et al.,\n2016). Multilingual models tend to generalize bet-\nter compared with bilingual translation due to the\nexposure to diverse languages (Zoph and Knight,\n2016; Arivazhagan et al., 2019). Moreover, it is\nburdensome to train hundreds of bilingual mod-\nels for each language pair, and one multilingual\nmodel reduces the deployment and maintenance\ncost (Dabre et al., 2020).\nA known challenge for multilingual modeling\nis the curse of multilinguality, where the language\ninterference hurts model performance (Conneau\net al., 2019). Language adapters attract research\nattention due to their strong performance in cross-\nlingual modeling (Wang et al., 2019). Adapter\nlayers are added for each language or language\ndirection, preserving the knowledge of language\nspeciﬁcity in a multilingual model (Bapna and Fi-\nrat, 2019; Zhang et al., 2021). Despite the sim-\nplicity, adapters are faced with extra inference cost\nbrought by additional adapter layers.\nIt has been revealed that interference occurs in\nthe shared parameters (Wang et al., 2020b). Sparse\nmodeling is extensively studied as an efﬁcient ap-\nproach to language interference mitigation. With\nthe assumption that interference is more likely be-\ntween diverse languages, existing works group lan-\nguages into families based on their proximity (Tan\net al., 2019). Languages in the same family are shar-\ning more parameters to encourage positive transfer,\nand different families have their exclusive decoders\n(Sen et al., 2019). But as pointed out by (Lin et al.,\n2019), the factors affecting parameter sharing are\nmore complicated than the language proximity.\nRecent studies integrate sparsity into different\ncomponents of a multilingual model and learn to\nshare parameters more ﬂexibly. Latent depth model\nleverages sparsity across layers, allowing different\nlanguages to choose a subset of layers in a deep\nTransformer model (Li et al., 2020). GShard (Lep-\nikhin et al., 2020) and Switch Transformer (Fe-\ndus et al., 2021) explore feed-forward (FFN) spar-\nsity with Mixture-of-experts (MoE). They replace\na feed-forward sub-layer with a set of identical\narXiv:2104.07358v2  [cs.CL]  22 Jan 2022\nsub-layers (i.e., multiple experts), and route input\ntokens to different experts. Language-sensitive at-\ntention sparsity is studied and each language direc-\ntion is assigned with one attention module (Wang\net al., 2019). A more ﬁne-grained scheme of atten-\ntion sparsity assigns each language with a selected\nsubset of attention heads (Gong et al., 2021). How-\never, it remains unknown which type of sparsity is\nmost effective in multilingual translation.\nIn this work, we propose a latent variable model\nto leverage the language-dependent sparsity in mul-\ntilingual machine translation. The latent variables\nlearn to activate various sub-networks for given\nlanguages in order to optimize translations. This\nis a general approach to integrate sparsity at differ-\nent scales. We are able to bring different types of\nsparsity under a common umbrella, including the\nsparsity within feed-forward and attention module\nas well as the sparsity across Transformer layers.\nTherefore, it enables a direct comparison of the\nsparsity in different components. Moreover, our\ndesign supports adaptive sparsity so that we could\nspecify the amount of model sparsity and easily\ncontrol the inference cost.\nOne limitation of existing approaches is the\ncoarse-grained parameter sharing, where two lan-\nguages share either all or no parameters of a given\ncomponent (e.g., the whole FFN or attention mod-\nule). It is possible that positive transfer occurs\nin some parameters while negative interference in\nother parameters of a component. We propose more\nﬁne-grained strategies of partial parameter shar-\ning. For FFN sparsity, our model divides the FFN\nweight matrix into multiple blocks. Each language\nactivates a subset of blocks so that partial sharing\nof FFN is enabled among languages. As for atten-\ntion sparsity, our model learns to select a subset\nof attention heads for each language. Knowledge\ntransfer is enabled in shared heads and language\nspeciﬁcity is preserved in heads exclusively owned\nby some languages.\nOur main ﬁndings are summarized below:\n1. Our sparse model consistently outperforms\nstrong multilingual baselines across bench-\nmark datasets on multilingual translation. On\nthe large-scale OPUS dataset with 100 lan-\nguages, we achieve average gains of 2.1, 1.3\nand 6.2 BLEU over Transformer in one-to-\nmany, many-to-one and zero-shot translations\nrespectively.\n2. Our model preserves the inference efﬁciency\nwith adaptive sparsity. By controlling the\namount of model sparsity, we improve the\ntranslation quality without increasing the in-\nference cost.\n3. We compare the sparsity in different model\ncomponents. FFN and attention sparsity\nworks best for one-to-many and many-to-\ntranslation respectively on medium-scale\ntranslation with 24 languages. As for large-\nscale translation covering100 languages, com-\nbining all types of sparsity yields the optimal\nperformance.\n4. We analyze the sparsity patterns learned for\nlanguages, and reveal that parameter sharing\nis affected by language proximity as well as\nthe resource sizes.\n2 Related Works\nMultilingual translation. Multilingual translation\nmodel refers to a universal system capable of trans-\nlating between multiple language pairs. Multilin-\ngual models are appealing due to better scalability,\nlower maintenance cost and more knowledge trans-\nfer compared with bilingual models (Dabre et al.,\n2020). Despite the beneﬁts above, multilingual\nmodels are faced with the negative transfer brought\nby the language interference (Conneau et al., 2019).\nRecent studies point out that conﬂicting gradients\nof different languages in their shared parameters\nare the cause of negative transfer among languages\n(Yu et al., 2020).\nThere are two lines of research towards the miti-\ngation of language interference. One line of studies\nresolve the gradient conﬂicts during model training\nfrom the optimization perspective (Suteu and Guo,\n2019; Yu et al., 2020; Wang et al., 2020c). The\nother line of research focuses on the model archi-\ntecture, and explores ways of parameter sharing\n(Sachan and Neubig, 2018). Our study falls into\nthe category of architecture design. We introduce\ncommonly used approaches in this category such\nas sparse models and language adapters.\nSparse modeling and conditional sparsity .\nSparse models have been extensively studied, and\nthe lottery ticket hypothesis suggests that sparsity\nimproves efﬁciency over dense models without\nhurting model performance (Frankle and Carbin,\n2018). Attention sparsity is introduced to Trans-\nformer, where attention weights (Correia et al.,\n2019) or attention outputs (Michel et al., 2019) are\nInput (de)\nOutput (fr)\nEncoder\nDecoder\nSelf-attention\nEncoder attention\nFeed-forward \nsublayer\nDecoder layer\nh1 h2 h3 h4\nh1 h2 h3 h4\nReLU\nLayerNorm\nLayerNorm\nLayerNorm\nh1 h2 h3 h4\nReLU\nLayerNorm\nLayerNorm\nEncoder layer\nFeed-forward \nsublayer\nSelf-attention\nFigure 1: Sparse architecture of adaptive Transformer. The grey areas are where sparsity is added to Transformer\nlayers, attention and feed-forward sub-layers. Only a sub-network is activated based on the source and target\nlanguages during inference time. In this example, blue layers, orange attention heads and green feed-forward\nblocks are activated components for source language (de) and target language (fr).\nsparsiﬁed. However, these approaches are limited\nto bilingual settings.\nConditional sparsity is further proposed for mul-\ntilingual models so that the model sparsity is con-\nditioned on languages or input tokens. An early\napproach trains multiple models for multilingual\ntranslation (Tan et al., 2019). Models are indepen-\ndently trained for each language family. Multi-\ndecoder model routes languages in different fam-\nilies to different decoders (Sen et al., 2019; Kong\net al., 2021). The intuition behind language group-\ning is that similar languages tend to have positive\ntransfer. One weakness of having separate mod-\nules per language is the increase of model sizes\nwith the number of languages. External linguistic\nknowledge and expertise are also required to mea-\nsure the language proximity. Moreover, it has been\nrevealed that high similarity between languages\ndoes not always lead to positive transfer (Lin et al.,\n2019).\nGShard (Lepikhin et al., 2020) and Switch Trans-\nformer (Fedus et al., 2021) replaces a single FFN\nsub-layer with Mixture-of-Experts consisting of\nmultiple FFN sub-layers. Each incoming token\nis routed to one of these FFN experts. Language-\nsensitive attention is integrated into a multilingual\nmodel and each language direction has its own\ncross-attention module (Wang et al., 2019). It has\nthe limitation of being unable to deal with zero-shot\ntranslation. A recent work selects a subset of atten-\ntion heads for each language, and demonstrates im-\nprovements in both multilingual and multi-domain\nmodeling (Gong et al., 2021). Conditional sparsity\nhas also been explored across layers, where a deep\nTransformer is trained to allow different languages\nto select their own subset of layers (Li et al., 2020).\nLanguage adapters. Bapna and Firat inserts an\nadapter layer for each language pair into a multi-\nlingual Transformer pre-trained in all languages\n(Bapna and Firat, 2019). The adapted model is\nﬁnetuned separately for each language pair. A sim-\nilar work uses adapter layers based on languages\ninstead of language directions (Philip et al., 2020).\nZhang et al. adds an adapter layer on top of each\nattention and FFN sub-layer, and the inputs could\nchoose to used shared or language-speciﬁc param-\neters within the adapter layer (Zhang et al., 2021).\nThe limitations of language adapters include the\nincreasing memory consumption with the number\nof languages and extra computation costs of addi-\ntional adapter layers.\n3 Model\nIn this work, we propose a sparse multilingual\nTransformer in order to optimize the translation\nquality with controlled inference cost. A Trans-\nformer model is basically a stack of Transformer\nlayers which consist of multi-head attention and\nfeed-forward sub-layers. We add sparsity to these\ncomponents respectively. To encourage positive\ntransfer and mitigate negative interference among\ndifferent languages, the sparsity is language depen-\ndent, i.e., the model activates different components\nfor each language. We start with the architecture\nof a standard Transformer layer, and then intro-\nduce sparsity to different modules in a Transformer\nmodel.\n3.1 Transformer Layer\nWithin a Transformer layer, suppose that there are\nH heads in its attention module. Each attention\nhead keeps a set of query, key and value vectors\nfor input tokens. For a given token, a head assigns\nits attention to the input sequence using query-key\nmatching between tokens. The value vectors of\nall tokens are weighted by the attention, and the\nweighted vectors from different heads are concate-\nnated as the new representation of the target token.\nLet xhbe the output of headh, and the token vector\nx learned by the attention module is:\nx = x1 ⊕···⊕ xh ⊕···⊕ xH, (1)\nwhere ⊕is the vector concatenation.\nThe token vector x from the attention module is\nupdated to y after linear projection, residual con-\nnection and layer normalization. The feed-forward\nmodule processes y with two dense layers. Tokens\nare projected to a higher dimension space in the\nﬁrst FFN sub-layer and then transformed to the\noriginal dimension in the second FFN sub-layer.\n{ y1 = FFN1(y) = ReLU(yW1 + b1),\ny2 = FFN2(y1) = y1W2 + b2,\n(2)\nwhere W1 ∈Rd×d′\nand W2 ∈Rd′×d are weight\nmatrices with d′>d, and b1 and b2 are bias vec-\ntors in FFN.\n3.2 Transformer Layer with Adaptive\nSparsity\nWe propose a general approach to integrate\nlanguage-dependent sparsity into Transformer us-\ning latent variables. Borrowing the idea of layer\nselection (Li et al., 2020), we use latent variables\nz to modulate component selection by languages.\nThe component can be an attention head, a block\nin FFN weight matrix, or an entire layer. Suppose\nthat a sample (x,y) has source text xin language\nl1 and target text y in language l2. The multilin-\ngual translation model has parameters θ which are\nmodulated by latent variables z.\np(y|x,l1,l2,θ) = Ep(z|l1,l2,θ)[p(y|x,z,l1,l2)].\n(3)\nEq. (3) is intractable given too many choices\nin component selection. Latent variable z has the\nprior p(z|l1,l2). We parameterize it with µ using\nGumbel-Softmax trick (Jang et al., 2017), and esti-\nmates its posterior qµ(z|l1,l2).\nWe derive the lower bound of Eq. (3) below with\nKL(·) as KL-divergence, and learn translation pa-\nrameters θ and selection parameters µ to maximize\nthe lower bound.\nlog p(y|x,l1,l2) ≥Eqµ(z|l1,l2)[log pθ(y|x,z,l1,l2)]\n−KL(qµ(z|l1,l2)||p(z|l1,l2)).\n(4)\nThe derivation of inequality (4) is included in\nAppendix. We assume that each component iis se-\nlected or discarded with equal probability by given\nlanguages, i.e., the prior p(zi = 1|l1,l2) = p(zi =\n0|l1,l2) = 0.5.\nFor simplicity, we denote the posterior of the\ni-th component, qµi (zi|l1,l2) as score sl,i, where\nl is l1 if the component iis in encoder, and it is\nl2 in decoder. Score sl,i is also used to weigh the\noutput of component iin the forward pass of model\ncomputation. In the following discussions, we dis-\ntinguish the scores of different components with\ndifferent notations. The score of an attention head\nis denoted as α, the score of FFN block is β and\nthe score of a layer is γ.\nAttention sparsity. It has been revealed by previ-\nous studies that some attention heads are redundant\nin Transformer (Michel et al., 2019). We propose\nto add sparsity to the attention module by masking\npartial heads. Suppose that language l assigns a\nscore αl,h to the head h. The outputs x of attention\nheads are multiplied by their scores.\n˜x = αl,1x1 ⊕···⊕ αl,hxh ⊕···⊕ αl,HxH,\n(5)\nwhere lis the language which head scores are con-\nditioned on. When the score is 0, it is equivalent to\nsetting dimensions corresponding to masked heads\nas 0. Masked heads thus bring sparsity to resulting\ntoken vector ˜x.\nFeed-forward sparsity. The feed-forward sub-\nlayers transform token vectors by ﬁrst increasing\nthe hidden dimension and then converting it back\nto the original dimension. This suggests that the\nintermediate hidden state has redundancy due to\nthe dimension increase from d to d′. Hence we\npropose to sparsify the feed-forward matrices.\nWe divide d′ dimensions into K blocks (sub-\nmatrices), and each block has a width of w = d′\nK\ndimensions. A language lselects a subset of blocks\nby assigning score βl,k to block k. Accordingly,\nthe score matrix βl,k for block kis a d×wmatrix\nwith all elements as the score βl,k. The score mask\nβl ∈Rd×d′\nassigned by language lto matrix W1\nis βl =\n[\nβl,1,..., βl,k,..., βl,K\n]\n.\nWe apply the mask βl to feed-forward matrices\nW1 and W2, and add sparsity by zeroing out their\ncolumns and rows respectively.\n{ ˜y1 = FFN1(y) = ReLU(y(W1 ⊙βl) + b1),\n˜y2 = FFN2(˜y1) = ˜y1(βT\nl ⊙W2) + b2,\n(6)\nwhere ⊙is element-wise multiplication.\n3.3 Adaptive Transformer\nSo far we have discussed sparsity within a Trans-\nformer layer, and will continue towards sparsity\nacross layers. We now introduce masks to the\nwhole Transformer layer. Similar to (Li et al.,\n2020), each language selects a subset of layers\nin the adaptive model. Suppose that language l\nassigns score γl,r to layer rto indicate how likely it\nuses this layer. Suppose that the input (a sequence\nof token vectors) to layer r is ur, and it is then\nprocessed by attention module “ Attn” and feed-\nforward module “FFN”. The output of layer r ,\nwhich is also the input to layer r+ 1, is denoted as\nur+1.\n{ v = ur + γl,r ·Attn(LayerNorm(ur)),\nur+1 = v + γl,r ·FFN(LayerNorm(v)),\n(7)\nwhere LayerNorm(·) is a normalization layer.\nIt is equivalent to a standard Transformer when\nγl,r = 1. When the score γl,r is 0, layer r is not\nselected and we have ur+1 = ur. Accordingly,\nthe model routes input ur directly to layer r+ 1,\nskipping layer r.\n4 Training and Inference\n4.1 Training Objective\nThe commonly used training objective for ma-\nchine translation is cross-entropy loss, which cor-\nresponds to the ﬁrst part of the lower bound in\nineqaulity (4). Besides the cross-entropy loss Lcp,\nour model adopts auxiliary losses to accommodate\nthe adaptive and sparse architecture.\nSuppose that we haveLlanguage directions, and\nthat the full multilingual model contains Dlayers\nwith H attention heads and Kfeed-forward blocks\nper layer. For simplicity of notation, we again\ndenote the component score as sin place of α, β\nand γused by different components.\nSparsity lossLs. The sparsity loss corresponds to\nthe KL-divergence in the lower bound of inequal-\nity (4).\nLs =\nL∑\nl=1\nN∑\ni=1\nsl,i(log sl,i −log 0.5), (8)\nWith the sparsity loss, the model leverages sparsity\nby assigning low scores to unimportant compo-\nnents.\nDisparity lossLd. Besides interference, too much\nparameter sharing among languages also leads to a\nwaste of model capacity when some components\nare not used by any language at all. This motivates\nthe disparity loss, which measures the similarity of\nmodule selection between languages.\nLd =\nL∑\nl=1\nL∑\nl′=l+1\n(\nN∑\ni=1\nsl,i ·sl′,i), (9)\nwhere N is the number of components that lan-\nguages can choose. Disparity loss is designed to\nencourage languages to choose different compo-\nnents to mitigate interference.\nTop-k lossLt. For the sub-network activated for\neach language within our sparse model, the infer-\nence budget decides its number of layers D′(D′<\nD), the number of attention heads H′(H′ < H)\nand feed-forward blocks K′(K′<K ) kept in each\nlayer. With the learned scores [sl,1,...,s l,N] that\nlanguage l assigns to N components, we select\ncomponents with the highest scores in their cat-\negory. For example, top H′ heads are selected\namong H heads within a layer. The selection is de-\nnoted by a binary vector[ml,1,...,m l,N], and ml,i\nis 1 if the corresponding component is selected, and\nit is 0 otherwise. We design the top-k loss Lt to\nmeasure the difference between scores and binary\nmasks.\nLt =\nL∑\nl=1\nN∑\ni=1\n(sl,i −ml,i)2. (10)\nBy minimizing the top-k loss, each language se-\nlects exact D′layers, H′heads and K′blocks as\nits activated sub-network to meet the inference efﬁ-\nciency requirements.\nWe leverage these losses in model training as\nwill be discussed below.\n4.2 Training\nAlgorithm 1Training Adaptive Sparse Model\n1: Input: Training data Dtrain, hyperparameters\ncs, cd ct, ¯T and T\n2: Output: Multilingual model parameters θ∗,\nparameters µ∗for attention head, FFN block\nand layer selection\n3: for t= 1,..., ¯T do\n4: Data batch: (x,y) ←Sample(Dtrain)\n5: Component scores: st ←\nGumbel-Softmax(µt)\n6: L←Lcp(x,y; θt,st) + cs ·Ls(st)\n7: θt+1,µt+1 ←update(L; θt,µt)\n8: end for\n9: for t= ¯T + 1,...,T do\n10: Data batch: (x,y) ←Sample(Dtrain)\n11: Component scores: st ←\nGumbel-Softmax(µt)\n12: Binary masks: mt ←{1 if st,i is among\ntop-k scores else 0: st,i ∈st}\n13: Sparse scores: ¯st ←mt ⊙st\n14: L ←Lcp(x,y; θt,¯st) + cd ·Ld(¯st) + ct ·\nLt(st,mt)\n15: θt+1,µt+1 ←update(L; θt,µt)\n16: end for\nThe model training is described in Algorithm 1. We\nﬁrst train the model with both cross-entropy and\nsparsity loss. All model parameters are used by\neach language, and the model learns to score each\ncomponent based on their impact on translation\nquality. After ¯T steps, the model starts component\nselection, i.e., only a speciﬁc number of compo-\nnents are used in model computation for a given\nlanguage to meet the budget of inference cost. It\nis trained with cross-entropy loss, disparity loss\nand top-k loss. The weights on auxiliary losses are\nhyperparameters set as cd = 0.02, cs = 0.1, and\nct = 0.1 in our experiments.\n4.3 Inference\nDuring inference, only selected components are\nactivated given a language. The sub-network for\ninference consists of the selected D′Transformer\nlayers with H′attention heads andK′feed-forward\nblocks in each layer.\n5 Experiments\nWe evaluate the proposed sparse models on mul-\ntilingual translation including on one-to-many\n(O2M), many-to-one (M2O) and many-to-many\n(M2M) translation. O2M translation has source\ntexts in one language and target texts in multiple\nlanguages. M2O translation has multiple source\nlanguages and only one target language, and M2M\ntranslation covers multiple source and target lan-\nguages.\n5.1 Experimental Setup\nDatasets. Models are evaluated on two widely\nused multilingual translation datasets at different\nscales. More details of these datasets are included\nin Appendix.\n• Public-24. This medium-scale dataset con-\ntains parallel corpora between English and 24\nlanguages, collected from public sources such\nas WMT shared tasks (Liu et al., 2020). It\nprovides O2M and M2O translations.\n• OPUS-100. This is a large-scale multilingual\ntranslation dataset covering 100 languages\n(Zhang et al., 2020). It serves for M2M trans-\nlations.\nBaselines. We include the following strong base-\nlines which are commonly used in multilingual\ntranslation.\n• Multilingual Transformer (Vaswani et al.,\n2017). A single Transformer model for\nmultilingual translation shares all parameters\namong languages.\n• Multi-decoder Transformer (Sen et al., 2019;\nKong et al., 2021). Similar to the multilin-\ngual Transformer, it has an encoder-decoder\narchitecture, but replaces the decoder with\nmultiple decoders. It is used for one-to-many\ntranslation, and target languages are clustered\ninto families based on their proximity (Lewis,\n2009). One decoder is shared by target lan-\nguages from the same family, and each family\nhas an exclusive decoder.\n• Multi-encoder Transformer. Similar to multi-\ndecoder Transformer, it replaces a single en-\ncoder with multiple encoders. It is used\nfor many-to-one translation, and each en-\ncoder corresponds to one family of source\nlanguages.\n• Adapter based Transformer (Bapna and Fi-\nrat, 2019). Adapter layers are transplanted be-\ntween adjacent layers of a trained multilingual\nModel O2M M2O\nSparse model #Params (M) Decode (tok/s) BLEU #Params (M) Decode (tok/s) BLEU\n+ Attn+FFN+Layer 31.5 (63.1) 1439.8 19.8 31.5 (56.8) 1402.5 22.4\n+ Attn only 31.5 (31.5) 1447.2 19.5 31.5 (31.5) 1430.0 22.9\n+ FFN only 31.5 (37.8) 1401.3 19.8 31.5 (37.8) 1360.0 22.8\n+ Layer only 31.5 (50.5) 1408.3 19.6 31.5 (44.2) 1359.6 22.2\nMulti-Encoder - - - 31.5 (119.0) 1368.0 22.0\nMulti-Decoder 31.5 (164.5) 1421.3 19.5 - - -\nAdapter 31.5 (50.9) 1236.6 19.5 31.5 (50.9) 1102.3 22.2\nTransformer 31.5 (31.5) 1412.4 19.3 31.5 (31.5) 1381.9 22.0\nTable 1: Average BLEU scores over 24 translation directions on Public-24 data. Besides the number of active\nparameters, total parameter sizes are included in the bracket. The inference efﬁciency is measured by the decoding\nspeed, i.e., the number of decoded tokens per second.\nTransformer. Each language pair is routed\nto its corresponding adapter layers which are\nﬁnetuned on the same data with other Trans-\nformer parameters frozen.\nEvaluation metrics. As we consider both trans-\nlation quality and model efﬁciency, two metrics\nare used for model evaluation. BLEU measures\nthe translation quality by comparing the predicted\nand reference translations. As for the efﬁciency,\nwe report the decoding speed, i.e., the number of\ndecoded tokens per second (tok/s) when one GPU\nis used with a batch size of 4096 tokens during\ninference.\n5.2 Model and Training\nFor multi-encoder (or multi-decoder), the number\nof encoders (or decoders) is the number of lan-\nguage families which are obtained using linguistic\nknowledge (Lewis, 2009). On Public-24 dataset,\n24 languages are grouped into 8 families, and the\ngrouping is included in Appendix.\nIn the adapter model, an adapter layer consists\nof two feed-forward sub-layers. The intermediate\nfeed-forward dimension is set as 128 in Public-\n24 experiments, and 32 in OPUS-100 so that the\nadapter models have similar sizes as our sparse\nmodels for a fair comparison.\nOur sparse model with layer sparsity starts with\n6 encoder layer and 12 decoder layers for O2M\ntranslation, and 12 encoder layers and 6 decoders\nfor M2O. It begins with 12 encoder and decoder\nlayers for M2M translation. During inference, only\n6 encoder and 6 decoder layers are used. We note\nthat the model with layer sparsity only is compara-\nble to the results of latent depth model in (Li et al.,\n2020).\nAs for the sparse model with attention sparsity,\nit has the same number of attention heads as Trans-\nformer baseline. On the Public-24 dataset, 3 out\nof 4 attention heads are selected in each Trans-\nformer layer. As for OPUS-100, 6 out of 8 heads\nare selected. Attention outputs corresponding to\nthe unselected heads are masked with 0’s.\nThe model with feed-forward sparsity has a di-\nmension of 2048 on Public-24 data. The feed-\nforward matrix is divided into 8 blocks, and 4\nblocks ( 1024 dimensions) are activated in each\nlayer. For OPUS-100, the feed-forward sub-layer\nwith a dimension of 4096 is divided into 16 blocks\nand 8 blocks (2048 dimensions) are selected by\neach language. In sparse model training, hyperpa-\nrameter ¯T in Algo. 1 is set as 8k on Public-24 and\n50k on OPUS-100.\nWe report more training details and hyperparam-\neters settings in Appendix.\n5.3 Results\nTo provide a good understanding of how sparsity in\ndifferent parts of Transformer inﬂuences the multi-\nlingual performance, we experiment with sparsity\nin different components including attention sparsity\n(“Attn only”), feed-forward sparsity (“FFN only”),\nlayer sparsity (“Layer only”) as well sparsity in all\nthese components (“Attn+FFN+Layer”).\nWe compare sparse models to dense baselines\nwith the similar amount of active parameters during\ninference. We also report the total number of pa-\nrameters including unused parameters which do not\ncontribute to the computation cost but account for\ntotal memory usage of sparse models. We note that\nembeddings are excluded from parameter count-\ning. We want to provide a consistent view of model\ncapacity without being affected by embedding pa-\nModel #Params (M) O2M M2O Zero-shot\nSparse model Decode (tok/s) BLEU Decode (tok/s) BLEU Decode (tok/s) BLEU\n+ Attn+FFN+Layer 44.2 (138.7) 1888.3 26.4 1938.1 31.5 1321.4 8.9\n+ Attn only 44.2 (44.2) 1919.4 24.6 2016.1 30.5 1405.1 4.0\n+ FFN only 44.2 (69.4) 1842.6 26.0 1966.7 31.2 1271.1 3.0\n+ Layer only 44.2 (88.3) 1860.5 25.6 1922.3 31.2 1184.4 4.9\nAdapter 44.6 (125.7) 1606.5 26.1 1765.8 30.9 - -\nTransformer 44.2 (44.2) 1858.4 24.3 1937.2 30.2 1206.1 2.7\nTable 2: Average BLEU scores over 94 O2M and M2O translation directions along with zero-shot translation in\n30 directions on OPUS-100 dataset. We report the number of effective parameters and include the total parameter\nsizes in the bracket.\nrameters varying with the vocabulary sizes across\ndatasets.\nPublic-24. Table 1 reports results on Public-24\ndataset. In terms of BLEU score, the best models\non O2M translation are sparse models with feed-\nforward sparsity and with all sparsity. They demon-\nstrate an average 0.5 BLEU gain over Transformer\nin 24 language directions. As for M2O transla-\ntion, the best performance is achieved by the sparse\nmodel with attention sparsity. Its improvement over\nTransformer is 0.9 BLEU. Adapter model has com-\nparable performance to multi-encoder and multi-\ndecoder model, and outperforms Transformer by\n0.2 in both O2M and M2O translations.\nAs for decoding speed, sparse models with only\nfeed-forward sparsity and with only layer sparsity\nare as efﬁcient as Transformer baseline, as their\nactivated sub-networks have the same architecture\nas Transformer. The sparse model with attention\nsparsity improves efﬁciency over Transformer in\nthat fewer attention heads are activated during in-\nference. Multi-encoder and multi-decoder models\nalso have comparable decoding speed as Trans-\nformer. Adapter model is slower in inference in\ncomparison with other models due to extra compu-\ntation costs brought by 12 adapter layers.\nOPUS-100. The results on OPUS-100 are\nshown in Table 2. We did not include multi-encoder\nor multi-decoder since they are too large for efﬁ-\ncient training. The models are trained on M2M\ntranslations, and we report the average BLEU\nscores on the test set in 94 one-to-many direc-\ntions, 94 many-to-one and 30 zero-shot directions.\nAdapter model trains adapter layers for each lan-\nguage pair, and cannot be applied to zero-shot di-\nrections without training data.\nIt can be seen from Table 2 that the sparse model\nachieves the best BLEU by combining all types of\nsparsity in attention, feed-forward and layer. In\ncomparison with Transformer, it achieves an aver-\nage of +2.1, +1.3 and +6.2 BLEU in O2M, M2O\nand zero-shot translations respectively. It also out-\nperforms Adapter by +0.3 and +0.6 BLEU in O2M\nand M2O translations respectively.\nAs for inference efﬁciency, we again observe\nthat sparse model with attention sparsity is fastest\nin decoding. It is worth mentioning that attention\nsparsity yields better BLEU with higher efﬁciency\nwhen compared with Transformer baseline. Sparse\nmodels with feed-forward sparsity or layer spar-\nsity demonstrate comparable inference efﬁciency\nto Transformer, and achieve faster inference than\nAdapter.\n6 Discussion\n6.1 Ablation Study\nAuxiliary losses. Three auxiliary losses are lever-\naged in training. We perform ablation studies to\nmeasure the impact of each loss on Public-24 O2M\ntranslation, using the model with FFN sparsity. In\nTable 3, we report BLEU of each model when one\nauxiliary loss is removed from their training objec-\ntive.\nSparse FFN Model BLEU\nAll losses 19.8\n- Sparsity loss 19.7\n- Top-k loss 19.3\n- Disparity loss 19.5\nTable 3: Ablation study of auxiliary losses on mod-\nels with FFN sparsity in Public-24 one-to-many trans-\nlation.\nThe model trained without top-k loss shows the\nlargest drop of 0.5 BLEU. Top-k loss trains the\nmodel so that the learned architecture satisﬁes the\nsparsity requirements such as the number of acti-\nvated layers and feed-forward dimensions. Without\ndisparity loss, the model loses0.3 BLEU as it is not\npunished for sharing parameters among languages.\nThe performance drop without disparity loss results\nfrom the language interference in shared parame-\nters. A drop of 0.1 BLEU is observed without\nsparsity loss. We note that the top-k loss compen-\nsates for the exclusion of sparsity loss, as it also\nencourages sparse architecture.\nSparsity types. As shown in Table 1 and 2, spar-\nsity in different model components affects trans-\nlation quality and efﬁciency differently. We now\ncompare and analyze these sparsity types from the\nresults in rows of “Attn only”, “FFN only” and\n“Layer only”. The sparsity in each component alone\nimproves BLEU scores in comparison with mul-\ntilingual Transformer. This suggests that the lan-\nguage interference exists in all these components.\nWith O2M translation on Public-24 data, FFN\nsparsity yields better BLEU than layer sparsity\nwith fewer parameters. The improvement of at-\ntention sparsity is relatively smaller compared with\nlayer and FFN sparsity for O2M translation. As for\nM2O translation, we observe that attention spar-\nsity outperforms other types of sparsity on Public-\n24 dataset. In this medium-scale dataset with 24\nlanguages, positive transfer brought by parameter\nsharing is more obvious than the negative interfer-\nence in both O2M and O2M translations. This is\nsuggested by the fact that multi-encoder and layer\nsparsity are beaten by other sparse models. Both of\nthem are coarse-grained approaches of parameter\nsharing. It reduces many shared parameters when\ntwo languages are routed to independent encoders\nor layers. However, when sparsity is applied to\nattention heads or feed-forward blocks, the model\ncould beneﬁt from knowledge transfer via partially\nshared parameters within a component.\nWhen it comes to large-scale OPUS-100, the\nmodel combining all types of sparsity demonstrate\nthe best BLEU in O2M, M2O and zero-shot trans-\nlations. A possible explanation is that negative\ninference is dominant in a multilingual model sup-\nporting a large number of languages. Mitigation of\ninterference by reducing parameter sharing is more\neffective in improving translations. We note that\nlarger BLEU gains are observed in O2M, M2O and\nzero-shot translations as the data scale grows larger.\nIt justiﬁes the effectiveness of our parameter shar-\ning approach when scaled to numerous languages.\nWe include additional analysis in Appendix.\n6.2 Sparsity Pattern\nFigure 2: Visualization of languages based on their\nmodel component selection. Language families are dif-\nferentiated with different colors (Pink: Arabic, blue:\nTurkic, orange: Austroasiatic, green: Indo-European,\nred: Uralic, purple: Sino-Tibetan, brown: Korean, grey:\nJaponic).\nTo gain an insight into the language-conditioned\nsparsity in our sparse models, we now analyze the\ncomponent selection by different languages. Take\nthe sparse model trained on O2M translation of\nPublic-24 dataset as an example, which integrates\nsparsity into layers, attention and FFN sub-layers.\nFor a given language, the model assign its score to\neach component. We represent this language with a\nvector of all component scores, and visualize these\nlanguage vectors in Fig. 2 with principal compo-\nnent analysis (PCA). The closeness of languages\nreﬂects the similarity of their component selection\npatterns.\nIt is observed that languages in Indo-European\nfamily form multiple clusters. One such cluster\nconsists of Russian (ru) and Latvian (lv). Some\nlanguages such as Vietnamese (vi) and Korean (ko)\nstay away from languages in other families. The pa-\nrameter sharing is shown correlated with language\nproximity to some extent.\nInterestingly, many high-resource languages in-\ncluding Czech (cs), Chinese (zh), German (de) and\nRussian (ru) do not have much parameter sharing\nwith low-resource languages. As can be seen in\nFig. 2, they do not have low-resource as close neigh-\nbors. This suggests that parameter sharing is also\nrelated to the resource sizes of languages. It res-\nonates with previous ﬁndings that low-resource de-\ngrades the performance of high-resource languages\nin multilingual models (Wang et al., 2020b). Excep-\ntions are French (fr) with the low-resource neigh-\nbor Turkish (tr) and Spanish (es) with low-resource\nneighbor Sinhala (si).\n7 Conclusion\nIn this study, we propose adaptive language-\nconditioned sparsity for multilingual translation.\nOur ﬁne-grained parameter sharing strategy is a\ngeneral approach to integrate and compare spar-\nsity at different scales, from attention heads, FFN\nblocks to Transformer layers. With extensive\nexperiments across multiple datasets, our sparse\nmodels demonstrate consistent performance gains\nover strong baselines including multilingual Trans-\nformer without increasing the inference cost. One\nlimitations of this work is the lack of analysis about\nhow model quality and efﬁciency change with spar-\nsity, which would be addressed in future work.\nReferences\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat,\nDmitry Lepikhin, Melvin Johnson, Maxim Krikun,\nMia Xu Chen, Yuan Cao, George Foster, Colin\nCherry, et al. 2019. Massively multilingual neural\nmachine translation in the wild: Findings and chal-\nlenges. arXiv preprint arXiv:1907.05019.\nAnkur Bapna and Orhan Firat. 2019. Simple, scalable\nadaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 1538–1548.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nGonc ¸alo M Correia, Vlad Niculae, and Andr´e FT Mar-\ntins. 2019. Adaptively sparse transformers. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2174–2184.\nRaj Dabre, Chenhui Chu, and Anoop Kunchukut-\ntan. 2020. A survey of multilingual neural ma-\nchine translation. ACM Computing Surveys (CSUR),\n53(5):1–38.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv\npreprint arXiv:2101.03961.\nJonathan Frankle and Michael Carbin. 2018. The lot-\ntery ticket hypothesis: Finding sparse, trainable neu-\nral networks. In International Conference on Learn-\ning Representations.\nHongyu Gong, Yun Tang, Juan Pino, and Xian Li. 2021.\nPay better attention to attention: Head selection in\nmultilingual and multi-domain sequence modeling.\narXiv preprint arXiv:2106.10840.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Categor-\nical reparameterization with gumbel-softmax. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings.\nXiang Kong, Adithya Renduchintala, James Cross,\nYuqing Tang, Jiatao Gu, and Xian Li. 2021. Neural\nmachine translation with deep encoder and multiple\nshallow decoders. In European Chapter of the Asso-\nciation for Computational Linguistics.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2020.\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding. arXiv preprint\narXiv:2006.16668.\nM Paul Lewis. 2009. Ethnologue: Languages of the\nworld. SIL international.\nXian Li, Asa Cooper Stickland, Yuqing Tang, and Xi-\nang Kong. 2020. Deep transformers with latent\ndepth. Advances in Neural Information Processing\nSystems, 33.\nYu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li,\nYuyan Zhang, Mengzhou Xia, Shruti Rijhwani,\nJunxian He, Zhisong Zhang, Xuezhe Ma, et al. 2019.\nChoosing transfer languages for cross-lingual learn-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, vol-\nume 57.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems,\npages 14014–14024.\nJerin Philip, Alexandre Berard, Matthias Gall ´e, and\nLaurent Besacier. 2020. Language adapters for zero\nshot neural machine translation. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 4465–\n4470.\nDevendra Sachan and Graham Neubig. 2018. Parame-\nter sharing methods for multilingual self-attentional\ntranslation models. In Proceedings of the Third Con-\nference on Machine Translation: Research Papers,\npages 261–271.\nSukanta Sen, Kamal Kumar Gupta, Asif Ekbal, and\nPushpak Bhattacharyya. 2019. Multilingual unsu-\npervised nmt using shared encoder and language-\nspeciﬁc decoders. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3083–3089.\nMihai Suteu and Yike Guo. 2019. Regularizing\ndeep multi-task networks using orthogonal gradients.\narXiv preprint arXiv:1912.06844.\nXu Tan, Jiale Chen, Di He, Yingce Xia, QIN Tao, and\nTie-Yan Liu. 2019. Multilingual neural machine\ntranslation with language clustering. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 962–972.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nXinyi Wang, Yulia Tsvetkov, and Graham Neubig.\n2020a. Balancing training for multilingual neural\nmachine translation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 8526–8537.\nYining Wang, Long Zhou, Jiajun Zhang, Feifei Zhai,\nJingfang Xu, and Chengqing Zong. 2019. A com-\npact and language-sensitive multilingual translation\nmethod. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1213–1223.\nZirui Wang, Zachary C Lipton, and Yulia Tsvetkov.\n2020b. On negative interference in multilingual lan-\nguage models. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 4438–4450.\nZirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan\nCao. 2020c. Gradient vaccine: Investigating and im-\nproving multi-task optimization in massively multi-\nlingual models. arXiv preprint arXiv:2010.05874.\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey\nLevine, Karol Hausman, and Chelsea Finn. 2020.\nGradient surgery for multi-task learning. Advances\nin Neural Information Processing Systems, 33.\nBiao Zhang, Ankur Bapna, Rico Sennrich, and Orhan\nFirat. 2021. Share or not? learning to schedule\nlanguage-speciﬁc capacity for multilingual transla-\ntion. In International Conference on Learning Rep-\nresentations.\nBiao Zhang, Philip Williams, Ivan Titov, and Rico Sen-\nnrich. 2020. Improving massively multilingual neu-\nral machine translation and zero-shot translation. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1628–\n1639.\nBarret Zoph and Kevin Knight. 2016. Multi-source\nneural translation. In Proceedings of the 2016 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 30–34.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1568–1575.\nA Sparse Model\nDerivation of lower bound. We now provide the\nevidence lower bound of Eq. (3) below.\nEqµ(z|l1,l2)[log pθ(y|x,z,l1,l2)]\n−KL(qµ(z|l1,l2)||p(z|l1,l2))\n= Eqµ(z|l1,l2)[log pθ(y|x,z,l1,l2)]\n−Eqµ(z|l1,l2)[log qµ(z|l1,l2) −log p(z|l1,l2)]\n(11)\n= Eqµ(z|l1,l2) [log pθ(y|x,z,l1,l2) + logp(z|l1,l2)\n−log qµ(z|l1,l2)]\n≤log Eqµ(z|l1,l2)[pθ(y|x,z,l1,l2) ·p(z|l1,l2)\nqµ(z|l1,l2) ]\n(12)\n≤log\n(∑\nz\npθ(y|x,z,l1,l2) ·p(z|l1,l2)\n)\n≤log p(y|x,l1,l2). (13)\nWe note that the equation (11) is based on the\ndeﬁnition of KL-divergence. As for the inequality\n(12), it is based on Jensen’s inequality. Hence we\nhave proved the inequality (4):\nlog p(y|x,l1,l2) ≥Eqµ(z|l1,l2)[log pθ(y|x,z,l1,l2)]\n−KL(qµ(z|l1,l2)||p(z|l1,l2)).\nModel complexity. The component selection in\nthe proposed sparse models is lightweight in that\nonly a small number of extra parameters are in-\ntroduced to the model. Suppose that the sparse\nmodel has a depth of Dlayers, each layer has H\nattention heads, and each FFN module is divided\ninto Kblocks. Given Llanguages, the number of\nparameters for head selection is D×H×L, and\nthe number of parameters for FFN block selection\nis D×K ×L, and the parameter size for layer\nselection is D×L. As we can see, the extra pa-\nrameters introduced to the adaptive sparse model\nis much fewer compared with the model size.\nB Datasets\nThe Public-24 dataset is recently collected by (Liu\net al., 2020) from multiple public sources as shown\nin Table 4. The sources are WMT shared tasks,\nIWSLT competition, W AT, FloRes and ITTB.\nThe OPUS corpus comes from multiple sources\nincluding movie subtitles, GNOME documentation\nand Bible. Following the data sampling process in\n(Zhang et al., 2020), we prepare the OPUS dataset\nwith up to 1M sentence pairs per language pair\nfor training, 2k for validation and 2k for testing.\nA total of 55M sentence pairs are included in the\nOPUS dataset.\nC Experiments\nC.1 Empirical Setup\nFor multi-encoder and multi-decoder models, lan-\nguages are grouped into families based on the\ntheir proximity. Multi-encoder is used for many-\nto-one translation, sharing an encoder for source\nlanguages in the same family. Similarly for multi-\ndecoder Transformer, one decoder is assigned to a\nfamily of target languages.\nPublic-24 data contains eight families: (1) Ara-\nbic; (2) Kazakh and Turkish; (3) Vietnamese; (4)\nCzech, German, Spanish, French, Gujarati, Hindi,\nItalian, Lithuanian, Latvian, Nepali, Dutch, Ro-\nmanian, Russian and Sinhala; (5) Estonian and\nEstonian; (6) Chinese and Burmese; (7) Korean;\n(8) Japanese.\nHyperparameters. We sample translation data\nin different languages with a temperature τ of 5.0\ndue to the data imbalance during training. Source\nand target vocabularies are learned with sentence-\npiece model and prepared for each dataset. Public-\n24 dataset has 250k tokens and OPUS has 64k\ntokens. We note that vocabulary size affects the\nmodel size since it decides the number of embed-\nding parameters in both encoder and decoder.\nThe models in our experiments are built upon\nTransformer architecture. They have 6 encoder\nlayers and 6 decoder layers, following the setting\nin (Wang et al., 2020a). The embedding dimension\nfor both encoder and decoder is set as 512. On\nPublic-24 data, the number of attention heads is\nset as 4 and the feed-forward dimension is 1024.\nFor OPUS-100, the model has 8 attention heads\nand feed-forward dimension of 2048. The training\nbatch size is set as 150k tokens on all datasets. As\nfor decoding, the beam size is 5 and length penalty\nis 1.0 for TED8 and Public-24 data. The beam size\nis 4 and length penalty is 0.6 for OPUS-100.\nOn Public-24 dataset, we have a dropout proba-\nbility of 0.1, and a learning rate of 0.0007. Models\non Public-24 data are trained for 100k steps. The\nmodels on OPUS data has a dropout of 0.1, and\nare trained for 500k steps with a learning rate of\n0.0015.\nLanguage Code Size Source Language Code Size Source\nGujarati gu 10k WMT19 Kazakh kk 91k WMT19\nVietnamese vi 133k IWSLT15 Turkish tr 207k WMT17\nJapanese ja 223k IWSLT17 Korean ko 230k IWSLT17\nDutch nl 237k IWSLT17 Arabic ar 250k IWSLT17\nItalian it 250k IWSLT17 Burmese my 259k W AT19\nNepali ne 564k FLoRes Romanian ro 608k WMT16\nSinhala si 647k FLoRes Hindi hi 1.56M ITTB\nEstonian et 1.94M WMT18 Lithuanian lt 2.11M WMT19\nFinnish ﬁ 2.66M WMT17 Latvian lv 4.50M WMT17\nCzech Cs 11M WMT Spanish es 15M WMT\nChinese zh 25M WMT German de 28M WMT\nRussian ru 29M WMT French fr 41M WMT\nTable 4: Data statistics of public-24 dataset.\nC.2 Result Analysis\nResource size affects performance. The lan-\nguage interference is reﬂected by the performance\ndrop in high-resource languages when trained to-\ngether with low-resource languages (Conneau et al.,\n2019). We analyze how model performance varies\nwith the resource size of languages in Table 5. Fol-\nlowing (Liu et al., 2020), we divide languages in\nPublic-24 dataset based on their data sizes: 13 low-\nresource languages with fewer than 1M parallel\nsentences, 6 high-resource languages with more\nthan 10M sentences, and the remaining 5 medium\nresource languages. The sparse models in Table 5\nare the model with feed-forward sparsity for O2M\ntranslation and the model with attention sparsity\nfor M2O translation.\nModel High Med Low\nO2M\nSparse Model 26.2 13.3 19.3\nMulti-Enc/Dec 25.9 13.0 19.0\nAdapter 25.5 13.0 19.2\nTransformer 25.2 12.8 19.0\nM2O\nSparse Model 29.9 17.0 22.7\nMulti-Enc/Dec 29.0 16.8 21.6\nAdapter 29.2 16.7 22.0\nTransformer 28.6 16.3 21.9\nTable 5: BLEU scores on high, medium and low-\nresource languages in Public-24 dataset.\nSparse models consistently improve the transla-\ntion quality of all categories of languages compared\nagainst other baselines. This resonates with a re-\ncent ﬁnding that interference impacts not only high-\nresource but also low-resource languages (Wang\net al., 2020b). Compared with Transformer base-\nline, all other models demonstrate similar trends\nof gains over different resources: the gain on high-\nresource >medium-resource >= low-resource. In\nparticular, the sparse models achieve BLEU gains\nof 1.0 and 1.3 on high-resource languages in O2M\nand M2O translations respectively."
}