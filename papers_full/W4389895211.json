{
    "title": "Transforming Healthcare Education: Harnessing Large Language Models for Frontline Health Worker Capacity Building using Retrieval-Augmented Generation",
    "url": "https://openalex.org/W4389895211",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3087403140",
            "name": "Yasmina Al Ghadban",
            "affiliations": [
                "Institute for Reproductive Health"
            ]
        },
        {
            "id": "https://openalex.org/A4286525974",
            "name": "Huiqi Yvonne Lu",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A5093522301",
            "name": "Uday Adavi",
            "affiliations": [
                "George Institute for Global Health"
            ]
        },
        {
            "id": "https://openalex.org/A2103934267",
            "name": "Ankita Sharma",
            "affiliations": [
                "Institute for Reproductive Health"
            ]
        },
        {
            "id": "https://openalex.org/A5093372593",
            "name": "Sridevi Gara",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A5005543169",
            "name": "Neelanjana Das",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2117153262",
            "name": "Bhaskar Kumar",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2698435108",
            "name": "Renu John",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2565107250",
            "name": "Praveen Devarsetty",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2524896630",
            "name": "Jane E. Hirst",
            "affiliations": [
                "Imperial College London"
            ]
        },
        {
            "id": "https://openalex.org/A3087403140",
            "name": "Yasmina Al Ghadban",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4286525974",
            "name": "Huiqi Yvonne Lu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5093522301",
            "name": "Uday Adavi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2103934267",
            "name": "Ankita Sharma",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5093372593",
            "name": "Sridevi Gara",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5005543169",
            "name": "Neelanjana Das",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2117153262",
            "name": "Bhaskar Kumar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2698435108",
            "name": "Renu John",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2565107250",
            "name": "Praveen Devarsetty",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2524896630",
            "name": "Jane E. Hirst",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4323835279",
        "https://openalex.org/W4385266429",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W3189216228",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W3161397949",
        "https://openalex.org/W4380485516",
        "https://openalex.org/W2061886377",
        "https://openalex.org/W4380291159",
        "https://openalex.org/W2148972377"
    ],
    "abstract": "Abstract In recent years, large language models (LLMs) have emerged as a transformative force in several domains, including medical education and healthcare. This paper presents a case study on the practical application of using retrieval-augmented generation (RAG) based models for enhancing healthcare education in low- and middle-income countries. The model described in this paper, SMART health GPT, stems from the necessity for accessible and locally relevant medical information to aid community health workers in delivering high-quality maternal care. We describe the development process of the complete RAG pipeline, including the creation of a knowledge base of Indian pregnancy-related guidelines, knowledge embedding retrieval, parameter selection and optimization, and answer generation. This case study highlights the potential of LLMs in building frontline healthcare worker capacity and enhancing guideline-based health education; and offers insights for similar applications in resource-limited settings. It serves as a reference for machine learning scientists, educators, healthcare professionals, and policymakers aiming to harness the power of LLMs for substantial educational improvement.",
    "full_text": "Transforming Healthcare Education: Harnessing\nLarge Language Models for Frontline Health Worker\nCapacity Building using Retrieval-Augmented\nGeneration\nYasmina Al Ghadban\nUniversity of Oxford, Department of\nWomen’s and Reproductive Health\nyasmina.alghadban@wrh.ox.ac.uk\nHuiqi (Yvonne) Lu\nUniversity of Oxford, Institute of\nBiomedical Engineering\nyvonne.lu@eng.ox.ac.uk\nUday Adavi\nThe George Institute for Global Health\nuadavi@georgeinstitute.org\nAnkita Sharma\nUniversity of Oxford, Department of\nWomen’s and Reproductive Health\nankita.sharma@gtc.ox.ac.uk\nSridevi Gara\nThe George Institute for Global Health\nsgara@georgeinstitute.org.in\nNeelanjana Das\nThe George Institute for Global Health\nndas@georgeinstitute.org.in\nBhaskar Kumar\nThe George Institute for Global Health\nbkumar@georgeinstitute.org.in\nRenu John\nThe George Institute for Global Health\nrjohn@georgeinstitute.org.in\nPraveen Devarsetty\nThe George Institute for Global Health\ndpraveen@georgeinstitute.org.in\nJane E. Hirst\nThe George Institute for Global Health,\nImperial College London\nj.hirst@imperial.ac.uk\nAbstract\nIn recent years, large language models (LLMs) have emerged as a transformative\nforce in several domains, including medical education and healthcare. This paper\npresents a case study on the practical application of using retrieval-augmented\ngeneration (RAG) based models for enhancing healthcare education in low- and\nmiddle-income countries. The model described in this paper, SMARThealthGPT,\nstems from the necessity for accessible and locally relevant medical information to\naid community health workers in delivering high-quality maternal care. We describe\nthe development process of the complete RAG pipeline, including the creation of\na knowledge base of Indian pregnancy-related guidelines, knowledge embedding\nretrieval, parameter selection and optimization, and answer generation. This case\nstudy highlights the potential of LLMs in building frontline healthcare worker\ncapacity and enhancing guideline-based health education; and offers insights for\nsimilar applications in resource-limited settings. It serves as a reference for machine\nlearning scientists, educators, healthcare professionals, and policymakers aiming\nto harness the power of LLMs for substantial educational improvement.\nPreprint. Under review.\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300009doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n1 Introduction\nRecently, the natural language processing (NLP) landscape has seen spectacular advances with the\nincreasing availability of pre-trained large language models (LLMs), such as Open AI’s GPT [1],\nLlama [2], and PaLM [3]. These models have had applications in several ﬁelds and are increasingly\nbeing employed in medical education and healthcare [4, 5].\nRetrieval-Augmented Generation (RAG) and ﬁne-tuning emerge as two powerful methodologies for\ntailoring pre-trained LLMs to speciﬁc applications. Fine-tuning modiﬁes the model’s weight based\non a task-speciﬁc dataset in a “close-book” setting, relying solely on additional input-output pairs of\ntraining data for learning [6, 7]. In contrast, RAG operates in an “open-book” setting and does not\nrequire labelled training data [8, 9]. Instead, it utilizes external information sources to retrieve and\nincorporate relevant information, enhancing the model’s comprehension and generative capabilities.\nThis paper introduces a case study, SMARThealthGPT (version Rv1), that showcases the application\nof RAG in developing educational and communication tools for frontline healthcare workers in low-\nand middle-income countries. This LLM tool aims to enhance community health workers’ knowledge,\nskills, and competencies by providing accessible, context-relevant information.\n2 SMARThealth Pregnancy\nIdentifying women with high-risk pregnancies before complications occur is essential to prevent\nmaternal and newborn mortality and morbidity. However, owing to health worker shortages, resource\nconstraints, poverty, and gender barriers, delivering high-quality pregnancy and postnatal care to\nwomen living in rural locations in low- and middle-income countries is challenging.\nTo improve the early detection, referral and management of high-risk pregnancy conditions and early\nprevention of non-communicable diseases, SMARThealthPregnancy, a digitally supported tool, was\ndeveloped for frontline health workers (ASHAs) [10, 11]. The system utilises task sharing to ASHAs,\nequipped with point-of-care devices and an Android tablet App with electronic decision support\nbased on the George Institute for Global Health (TGI) SMARThealthplatform. The system has been\nco-designed with end users in rural India, demonstrated to be feasible and acceptable, and is currently\nbeing assessed in a large cluster implementation effectiveness trial across 60 villages in two states in\nIndia (Telangana and Haryana) (clinicaltrial.gov NCT05752955).\nQualitative research has found that whilst the system is highly valued, ASHAs lack detailed knowledge\nto give diet and lifestyle advice, and information about medical conditions and pregnancy symptoms\nin simple terms and in local languages to support pregnant and postpartum women. In line with this\nneed, the aim of this project was to develop, technically and clinically validate, an LLM suitable\nfor community health workers in rural India to support guideline-based pregnancy care. We believe\nthis LLM application, SMARThealthGPT, can be a valuable tool to enhance medical education for\nfrontline health workers, particularly in resource-constrained environments.\n3 Methodology\n3.1 Formal deﬁnition of RAG\nRAG enables LLMs to access information from non-parametric storage, making it highly adaptable\nto new tasks and reducing the need for extensive annotated training data. Within the RAG framework,\nexternal information sources are transformed into embeddings and stored in a vector database. This\nprocess forms the basis for the subsequent steps. To illustrate the RAG process formally, we express\nit using conditional probabilities, dividing it into two key components: retrieval and generation.\nIn the retrieval step, the objective is to select a set of relevant documents,D(D1,...,D k), from a\nrepository of documents(E) based on a user’s question(Q). This can be expressed asP(D|Q), the\nlikelihood of choosing documents D from E, given the questionQ. This probability is computed by\nthe retriever.\nOnce the relevant documents(D) have been retrieved, the answer(A) needs to be generated. This\ngeneration step can be expressed asP(A|D,Q ), representing the probability distribution of generating\n2\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300009doi: medRxiv preprint \nthe answerA, given the retrieved documentsD and the questionQ. This probability is computed by\na text generator.\nThe entire RAG process can be expressed as follows:\nP(A | Q)=\nkX\ni=1\nP(A | Di,Q ) · P(Di | Q) (1)\nHere, P(A|Q) signiﬁes the probability of generating answer A given the question Q.\nIn summary, RAG selects pertinent documents based on the input context and then generates the\noutput text, conditioned on both the selected documents and the input context. These steps are\noften implemented using neural network models, which are trained to maximize the likelihood of\ngenerating the correct output, aligning with the retrieved documents.\n3.2 Rationale for the use of RAG\nWe selected RAG for our use case for several reasons. First, the pedagogical nature of our application\nnecessitates that responses provided by the model are not only accurate but also traceable back to their\nsources. RAG’s ability to trace responses back to their respective sources increases explainability\nand trustworthiness in the educational content and reduces potential model hallucinations. This also\naligns with the needs of our application where ASHAs not only require a response to their question\nbut also a source guideline to refer to. The RAG method allows us to retrieve the context relevant\nto the query and to return the source (document name and page number) to the user. Second, our\nmodel needs to be scalable, especially in the context of vast knowledge bases within the healthcare\ndomain. RAG models are highly scalable as they leverage retrieval mechanisms to accommodate\nlarge knowledge bases. Third, given the continuous evolvement of clinical guidelines, our model\nneeds to be ﬂexible and swiftly updated to align with the latest recommendations. By using RAG,\nour model can be easily and quickly updated by incorporating new guidelines or updates into the\nknowledge base, ensuring the relevance and accuracy of our model.\n3.3 Model development\nWe deployed a three-step approach in the RAG model development pipeline, as shown in Figure 1.\nFigure 1: Flow diagram of the RAG process in SMARThealthGPT\nStep 1: Development of the encyclopaedia of Indian guidelinesTo create the knowledge base\nthat RAG will be based on, the clinical team created a large repository of Indian pregnancy-related\nguidelines. This initial ﬁle included 37 documents. The clinical team then created a smaller repository\nof Indian pregnancy guidelines relevant only to community health workers, ASHAs. To identify\n3\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300009doi: medRxiv preprint \nwhether the repository was complete, we ﬁrst ran the model of 130 questions collected through\ncommunity engagement with ASHAs on the small repository. For questions where the model did\nnot return an answer, we ran the model on the large repository and identiﬁed the sources that were\nmissing. The clinical team then decided whether the source should be included or if it was appropriate\nfor the model not to return an answer to that question.\nWe transformed the ﬁnal repository of Indian guidelines into embeddings and stored them in the same\nFAISS vector database. We chose the FAISS vector store for its ability to perform similarity search in\nsets of vectors of any size, and the option to store the index locally. This allows us to load the vector\nstore directly, rather than creating it at every iteration, decreasing the processing time.\nStep 2.a: Context retrievalWe selected three commonly used retrieval methods in RAG: vector-\nstore backed retriever, contextual compression retriever and ensemble retriever, as shown in Sup-\nplementary Figure S1. The vector-store backed retriever is the simplest method, which retrieves\ndocuments with the highest similarity to the question.\nIn this study, all three retriever methods were tested and compared for both similarity search and\nMaximal Marginal Relevance (MMR) search. Compared to the similarity search, MMR balances\nbetween relevance and diversity. It selects the documentDi from the ranked list of relevant documents,\nD(D1,...,D k), that maximizes the trade-off between similarity to question Q,Sim1(Di,Q ) and\nsimilarity to the documents already selected (S),Sim2(Di,D j) [12]. A parameter\u0000 controls the\ntrade off betweenSim1(Di,Q ) and Sim2(Di,D j). MMR can be deﬁned as:\nMMR(D,Q,S )= argmaxDi2D\\S\n\n\u0000 · Sim1(Di,Q ) \u0000 (1 \u0000 \u0000) · max\nDj 2S\nSim2(Di,D j)\n\u0000\n(2)\nSimilarity is measured using the cosine similarity between two vectorsA and B, which is calculated\nas follows:\nCosine Similarity(A, B)= A · B\nkAk· kBk =\nPn\ni=1 Ai · BipPn\ni=1\nA2\ni ·\npPn\ni=1 B2\ni\n(3)\nThe selection of the retrieval method was based on a quantitative assessment and an evaluation of\nthe processing time (using both wall time and CPU time). For the quantitative assessment, we used\n10 diverse pregnancy-related questions and compared the model’s answer to the clinician provided\nanswer (the gold standard) using both a cosine similarity score and a Clinical BERT similarity score.\nClinicalBERT is a modiﬁed BERT model pre-trained on patient clinical notes and electronic health\nrecords, which more accurately captures clinical word similarity [13]. Given the clinical nature of\nour application, we used a similarity score based on the ClinicalBERT model (range 0-5) in addition\nto the cosine similarity score (range 0-1).\nStep 2.b: Parameter optimisation We then investigated the impact of 4 model parameters (chunk\nsize, chunk overlap, number of documents retrieved (k) and search type) on model performance. In\nthis round, we limited our retrieval method to a vector-store backed retrieval but compared similarity\nsearch and MMR. Due to the limited number of questions, we carried out two grid search tests on 1)\nchunk size and chunk overlap, and 2) search type and number of documents retrieved (k).\nTo select the model parameters, we developed an evaluation pipeline using the RAGAS framework,\nwhich allows evaluation of both generation and retrieval steps alone, and a RAGAS score for overall\nperformance assessment [14]. The metrics that evaluate retrieval arecontext relevancywhich\nmeasures the signal-to-noise ratio in retrieved contexts by determining the ratio of essential sentences\nto total sentences in the context, andcontext recallwhich assesses the retriever’s ability to ﬁnd all\nnecessary information by checking if statements from the ground-truth answer are present in the\nretrieved context. The metrics that evaluate generation arefaithfulness which assesses the factual\naccuracy of the generated answer by comparing its statements to the context and calculating the ratio\nof correct statements to total statements, andanswer relevancywhich measures how relevant the\ngenerated answer is to the original question by assessing its similarity to potential questions it could\naddress. The RAGAS score is the harmonic mean of all four metrics [14].\n4\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300009doi: medRxiv preprint \nTable 1: RAGAS metrics for SMARThealthGPT model parameters: search type and number of\nretrieved documents (k)\nk Evaluation metric\nContext relevancy Faithfulness Answer relevancy Context recall RAGAS score\nMMR search\n1 0.16 0.77 0.91 0.26 0.15\n2 0.12 0.85 0.92 0.22 0.15\n3 0.08 0.93 0.92 0.14 0.11\n4 0.07 0.89 0.91 0.31 0.13\n5 0.07 0.85 0.93 0.24 0.10\nSimilarity search\n1 0.10 0.72 0.93 0.28 0.17\n2 0.09 0.86 0.91 0.21 0.12\n3 0.10 0.90 0.91 0.37 0.20\n4 0.10 0.81 0.91 0.19 0.12\n5 0.10 0.90 0.91 0.40 0.20\nStep 3: Answer generation The backbone of the SMARThealthGPT Rv1 is the Open AI GPT\nmodel. To optimise the answer generation, we compared two pre-trained models: “gpt-3.5-turbo”\nand “gpt-4”. In this answer generation step, we also designed three prompt instructions that were\ncustomised to meet clinical needs, namely the length of response; length and use case; and length and\nuse case and one-shot learning. We evaluated model performance based on the RAGAS metrics.\n4 Model performance evaluation and results\nStep 1: Development of the encyclopaedia of Indian guidelinesThe ﬁnal repository included\n20 pregnancy-related guidelines. The included guidelines covered a range of pregnancy-related\nconditions with a particular focus on the three key conditions addressed by the SMARThealth\nPregnancy project: anaemia, gestational diabetes, and hypertension in pregnancy.\nStep 2.a: Context retrievalThe answer quality (measured by ClinicalBERT and cosine similarity\nscores) of the simplest vector-store based retriever did not improve with the contextual compression\nretriever or the ensemble retriever, while the processing time increased, as shown in Supplementary\nTable S1. The signiﬁcant response time increase was deemed not feasible for our application,\nconsidering the time pressure ASHAs face while providing care for women. Therefore, the simplest\nvector-store based retriever was chosen to be evaluated in the next steps.\nStep 2.b: Parameter optimisationInterestingly, the chunk size, chunk overlap, search type and\nnumber of retrieved documents (k) parameters did not signiﬁcantly impact model performance,\nmeasured by RAGAS metrics (Table 1 and Supplementary Table S2). As a result, model parameters\nwere primarily chosen based on processing time and technical considerations. Accordingly, we\nselected a chunk size of 1000, with an overlap of 200 characters. We chose the MMR search type\nas health guidelines include several similar parts, and considering diversity in retrieval minimises\nrepetition and provides more comprehensive documents. Two retrieved documents (k=2) showed a\nbalance in result completeness and duplications.\nStep 3: Answer generation The model choice and prompt template also minimally impacted\nRAGAS metrics. We selected “gpt-4” due to its lower likelihood of hallucinations as demonstrated in\nprevious studies [15], and the one-shot prompt template as providing an example may result in more\npredictable responses, required for our application.\n5\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300009doi: medRxiv preprint \nFigure 2: RAGAS metrics for different model parameters. (A) Prompt template (B) Model type\n5 Initial clinical validation results\nThe model is currently undergoing a ﬁrst round of clinical validation. During this round, 12 commu-\nnity medicine clinicians and two obstetricians, across two states, rated the model generated answers\nbased on accuracy, completeness, appropriateness, and presence of bias on a 3-point Likert scale.\nThe 180 questions included in this round of clinical validation were developed with ASHAs directly\nthough focus groups and community engagement. Each question was rated by a different number of\nclinicians between two and six. For 141 (79%) questions, all clinicians agreed that the AI generated\nanswer was completely or partially accurate; and that the AI-generated answer was either unbiased or\nactively promoted equity. However, all clinicians rated the completeness of the AI generated answer\nas adequate or comprehensive for only 49 (35%) questions. The clinical validation has allowed to\ngain signiﬁcant insight into the performance of the model, and importantly, has allowed us to identify\nthe cases where the model is failing (the answers with low ratings).\n6 Future work\nOur immediate next goal is the improvement of responses using prompt engineering and ensuring\nthe LLM includes safety features, such as moderation, bias and evaluation checks. In collaboration\nwith the clinical team, we also need to deﬁne topics that are beyond the scope of community health\nworkers, such as questions around medical prescriptions and interpretation of reports. Additionally,\nwe need to develop contextually relevant and gender transformative responses to sensitive queries\nsuch as questions about predicting the gender of the baby.\nIn addition to rating the model’s answer to the 180 questions, clinicians also provided an ideal\nresponse that is clear, uncontroversial, and appropriate for ASHAs. The answers provided by multiple\nclinicians are being compiled and standardized by an obstetrician and a community medicine doctor.\n6\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300009doi: medRxiv preprint \nFigure 3: Iterative LLM improvement through the four phases of clinical evaluation\nThese answers will constitute the “gold-standard” responses and the repository of 180 question-\nanswer pairs can be used to ﬁne-tune the model or improve its prompts. The LLM will be improved\niteratively based on the feedback from each phase of validation (Figure 3).\nFuture work must also address several impending challenges. These include scaling up the knowledge\nbase while ensuring efﬁcient information retrieval, handling multi-level conversations, as well as\naddressing critical data privacy concerns, such as communication storage management and learning\nfrom user requests. A future avenue of work is the exploration of local models to overcome the\nlimitations imposed by the use of Open-AI models such as cost.\n7 Conclusion\nIn this paper, we have described a comprehensive case study centred on the deployment of RAG to\ndevelop SMARThealthGPT (version Rv1), an LLM tool to build the capacity of community health\nworkers. This model was developed to enhance guideline-based pregnancy care, showcasing the\nimpactful intersection of generative AI and healthcare education. The selection of RAG was driven by\nclinical needs – traceability to source material, scalability across vast knowledge bases, and seamless\nadaptability to evolving clinical guidelines.\nOur process of model development and optimization encompassed the building of the encyclopaedia\nof Indian pregnancy-related guidelines and quantitative evaluations of different retriever methods and\nmodel parameters. SMARThealthGPT showcases the promising role of RAG and LLMs in medical\neducation and provides insights for future applications of generative AI in diverse educational settings.\nWe hope that the deployment of RAG models emerges as a strategy for surmounting educational\nbarriers, and the case study of SMARThealthGPT (version Rv1) can serve as a template for the\napplication of generative AI in education, particularly in non-traditional educational settings and\nresource-constrained environments.\n7\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300009doi: medRxiv preprint \nAcknowledgments and Disclosure of Funding\nThis study is funded by the Bill and Melinda Gates Foundation Grand Challenges Equitable AI. The\nviews expressed are those of the authors and not necessarily those of BMGF. The funder has no role\nin the study design, data collection, management, analysis and interpretation; writing or the report;\nand decisions related to publication and presentation of ﬁndings. The authors declare no ﬁnancial\ninterests.\nReferences\n[1] Tom B. Brown et al.Language Models are Few-Shot Learners. arXiv:2005.14165 [cs]. July\n2020. DOI : 10.48550/arXiv.2005.14165. URL : http://arxiv.org/abs/2005.14165\n(visited on 09/25/2023).\n[2] Hugo Touvron et al. LLaMA: Open and Efﬁcient Foundation Language Models.\narXiv:2302.13971 [cs]. Feb. 2023.DOI : 10 . 48550 / arXiv . 2302 . 13971. URL : http :\n//arxiv.org/abs/2302.13971 (visited on 09/25/2023).\n[3] Aakanksha Chowdhery et al. PaLM: Scaling Language Modeling with Pathways.\narXiv:2204.02311 [cs]. Oct. 2022.DOI : 10 . 48550 / arXiv . 2204 . 02311. URL : http :\n//arxiv.org/abs/2204.02311 (visited on 09/25/2023).\n[4] Rachel S. Goodman et al. “On the cusp: Considering the impact of artiﬁcial intelligence\nlanguage models in healthcare”. English. In:Med 4.3 (Mar. 2023). Publisher: Elsevier, pp. 139–\n140. ISSN : 2666-6359, 2666-6340.DOI : 10.1016/j.medj.2023.02.008 . URL : https:\n//www.cell.com/med/abstract/S2666-6340(23)00068-5 (visited on 08/30/2023).\n[5] Conrad W. Safranek et al. “The Role of Large Language Models in Medical Education:\nApplications and Implications”. EN. In:JMIR Medical Education9.1 (Aug. 2023). Company:\nJMIR Medical Education Distributor: JMIR Medical Education Institution: JMIR Medical\nEducation Label: JMIR Medical Education Publisher: JMIR Publications Inc., Toronto, Canada,\ne50945. DOI : 10.2196/50945 . URL : https://mededu.jmir.org/2023/1/e50945\n(visited on 09/23/2023).\n[6] Hyung Won Chung et al.Scaling Instruction-Finetuned Language Models. arXiv:2210.11416\n[cs]. Dec. 2022.DOI : 10.48550/arXiv.2210.11416 . URL : http://arxiv.org/abs/\n2210.11416 (visited on 09/25/2023).\n[7] Jason Wei et al.Finetuned Language Models Are Zero-Shot Learners. arXiv:2109.01652 [cs].\nFeb. 2022.DOI : 10.48550/arXiv.2109.01652 . URL : http://arxiv.org/abs/2109.\n01652 (visited on 09/25/2023).\n[8] Matthew Lamm et al. “QED: A Framework and Dataset for Explanations in Question An-\nswering”. In:Transactions of the Association for Computational Linguistics9 (2021). Place:\nCambridge, MA Publisher: MIT Press, pp. 790–806.DOI : 10.1162/tacl_a_00398 . URL :\nhttps://aclanthology.org/2021.tacl-1.48 (visited on 09/25/2023).\n[9] Patrick Lewis et al.Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\narXiv:2005.11401 [cs]. Apr. 2021.DOI : 10 . 48550 / arXiv . 2005 . 11401. URL : http :\n//arxiv.org/abs/2005.11401 (visited on 09/25/2023).\n[10] Shobhana Nagraj et al. “SMARThealth Pregnancy: Feasibility and Acceptability of a Complex\nIntervention for High-Risk Pregnant Women in Rural India: Protocol for a Pilot Cluster\nRandomised Controlled Trial”. In:Frontiers in Global Women’s Health2 (2021).ISSN : 2673-\n5059. URL : https://www.frontiersin.org/articles/10.3389/fgwh.2021.620759\n(visited on 09/21/2023).\n[11] Shobhana Nagraj et al. “A Mobile Clinical Decision Support System for High-Risk Pregnant\nWomen in Rural India (SMARThealth Pregnancy): Pilot Cluster Randomized Controlled Trial”.\nIn: JMIR Formative Research7 (July 2023), e44362.ISSN : 2561-326X.DOI : 10.2196/44362.\nURL : https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10401191/ (visited on\n09/25/2023).\n[12] Jade Goldstein and Jaime Carbonell. “Summarization: (1) Using MMR for Diversity- Based\nReranking and (2) Evaluating Summaries”. In:TIPSTER TEXT PROGRAM PHASE III:\nProceedings of a Workshop held at Baltimore, Maryland, October 13-15, 1998. Baltimore,\nMaryland, USA: Association for Computational Linguistics, Oct. 1998, pp. 181–195.DOI :\n8\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300009doi: medRxiv preprint \n10.3115/1119089.1119120. URL : https://aclanthology.org/X98-1025 (visited on\n09/25/2023).\n[13] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath.ClinicalBERT: Modeling Clinical Notes\nand Predicting Hospital Readmission. arXiv:1904.05342 [cs]. Nov. 2020.URL : http://\narxiv.org/abs/1904.05342 (visited on 09/25/2023).\n[14] Evaluating RAG pipelines with Ragas + LangSmith. en. Aug. 2023.URL : https://blog.\nlangchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/ (visited on\n09/21/2023).\n[15] Rohaid Ali et al. “Performance of ChatGPT, GPT-4, and Google Bard on a Neurosurgery Oral\nBoards Preparation Question Bank”. eng. In:Neurosurgery (June 2023).ISSN : 1524-4040.\nDOI : 10.1227/neu.0000000000002551.\n[16] Gordon V . Cormack, Charles L A Clarke, and Stefan Buettcher. “Reciprocal rank fusion\noutperforms condorcet and individual rank learning methods”. In:Proceedings of the 32nd\ninternational ACM SIGIR conference on Research and development in information retrieval.\nSIGIR ’09. New York, NY , USA: Association for Computing Machinery, July 2009, pp. 758–\n759. ISBN : 978-1-60558-483-6.DOI : 10.1145/1571941.1572114. URL : https://dl.acm.\norg/doi/10.1145/1571941.1572114 (visited on 09/25/2023).\n9\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 17, 2023. ; https://doi.org/10.1101/2023.12.15.23300009doi: medRxiv preprint "
}