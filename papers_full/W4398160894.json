{
  "title": "Can LLMs Answer Investment Banking Questions? Using Domain-Tuned Functions to Improve LLM Performance on Knowledge-Intensive Analytical Tasks",
  "url": "https://openalex.org/W4398160894",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5098728868",
      "name": "Nicholas Harvel",
      "affiliations": [
        "ModuleWorks (Romania)"
      ]
    },
    {
      "id": "https://openalex.org/A5098728869",
      "name": "Felipe Bivort Haiek",
      "affiliations": [
        "ModuleWorks (Romania)"
      ]
    },
    {
      "id": "https://openalex.org/A5021060898",
      "name": "Anupriya Ankolekar",
      "affiliations": [
        "ModuleWorks (Romania)"
      ]
    },
    {
      "id": "https://openalex.org/A5079818585",
      "name": "David James Brunner",
      "affiliations": [
        "ModuleWorks (Romania)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6852670792",
    "https://openalex.org/W4386302905",
    "https://openalex.org/W4378465454",
    "https://openalex.org/W4387838866",
    "https://openalex.org/W6853216470",
    "https://openalex.org/W6853859572",
    "https://openalex.org/W6851296002",
    "https://openalex.org/W6856020930",
    "https://openalex.org/W6849392780",
    "https://openalex.org/W4306178296",
    "https://openalex.org/W4392366650",
    "https://openalex.org/W4386302345",
    "https://openalex.org/W6850820320",
    "https://openalex.org/W6857346536",
    "https://openalex.org/W6854777383",
    "https://openalex.org/W4389520133",
    "https://openalex.org/W4386081878",
    "https://openalex.org/W4378505284",
    "https://openalex.org/W4289494028",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W4385571662",
    "https://openalex.org/W4386566577",
    "https://openalex.org/W4387390366",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W4386827556",
    "https://openalex.org/W4388778348",
    "https://openalex.org/W4378498632",
    "https://openalex.org/W4387636003",
    "https://openalex.org/W4366999624"
  ],
  "abstract": "Large Language Models (LLMs) can increase the productivity of general-purpose knowledge work, but accuracy is a concern, especially in professional settings requiring domain-specific knowledge and reasoning. To evaluate the suitability of LLMs for such work, we developed a benchmark of 16 analytical tasks representative of the investment banking industry. We evaluated LLM performance without special prompting, with relevant information provided in the prompt, and as part of a system giving the LLM access to domain-tuned functions for information retrieval and planning. Without access to functions, state-of-the-art LLMs performed poorly, completing two or fewer tasks correctly. Access to appropriate domain-tuned functions yielded dramatically better results, although performance was highly sensitive to the design of the functions and the structure of the information they returned. The most effective designs yielded correct answers on 12 out of 16 tasks. Our results suggest that domain-specific functions and information structures, by empowering LLMs with relevant domain knowledge and enabling them to reason in domain-appropriate ways, may be a powerful means of adapting LLMs for use in demanding professional settings.",
  "full_text": "Can LLMs Answer Investment Banking Questions? Using Domain-Tuned\nFunctions to Improve LLM Performance on Knowledge-Intensive Analytical Tasks\nNicholas Harvel, Felipe Bivort Haiek, Anupriya Ankolekar, David James Brunner\nModuleQ, Inc.\n10080 N. Wolfe Rd., Suite SW3-200\nCupertino, CA 95014\n{nicholas.harvel, felipe.bivort, anupriya, djb}@moduleq.com\nAbstract\nLarge Language Models (LLMs) can increase the produc-\ntivity of general-purpose knowledge work, but accuracy is a\nconcern, especially in professional settings requiring domain-\nspecific knowledge and reasoning. To evaluate the suitability\nof LLMs for such work, we developed a benchmark of 16 an-\nalytical tasks representative of the investment banking indus-\ntry. We evaluated LLM performance without special prompt-\ning, with relevant information provided in the prompt, and as\npart of a system giving the LLM access to domain-tuned func-\ntions for information retrieval and planning. Without access\nto functions, state-of-the-art LLMs performed poorly, com-\npleting two or fewer tasks correctly. Access to appropriate\ndomain-tuned functions yielded dramatically better results,\nalthough performance was highly sensitive to the design of\nthe functions and the structure of the information they re-\nturned. The most effective designs yielded correct answers on\n12 out of 16 tasks. Our results suggest that domain-specific\nfunctions and information structures, by empowering LLMs\nwith relevant domain knowledge and enabling them to rea-\nson in domain-appropriate ways, may be a powerful means of\nadapting LLMs for use in demanding professional settings.\nIntroduction\nWith their prodigious memory, flexible natural language\nreasoning and ease of verbal expression, Large Language\nModels (LLMs) show great promise for question-answering\nacross a variety of domains. They have been integrated into\nwidely-used search tools such as Google, with Search Gen-\nerative Experience (SGE) and Microsoft Bing. Significant\neffort and resources have been expended to apply them\nto general knowledge work (Microsoft Copilot) as well as\nspecific professional domains including financial services\n(BloombergGPT (Wu et al. 2023)), software engineering\n(Github CoPilot, Google Duet), and law (Casetext), to name\nprominent examples.\nRecent research showed that LLMs can dramatically\nincrease the productivity of sophisticated professionals\n(Dell’Acqua et al. 2023). However, the same research found\nthat reliance on LLMs may increase the risk of inaccurate\nanswers. In professional settings, inaccurate answers can\ncause financial damage, legal liability, and other adverse\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nconsequences. Relatively little is understood about how to\neffectively utilize LLMs to support professionals with accu-\nrate, up-to-date, trustworthy and reliable answers in specific\nprofessional industries.\nInvestment bankers advising on corporate mergers and\nacquisitions are an exemplary case of knowledge-intensive\nprofessionals whose judgments, often made under intense\ntime pressure, may have massive financial impacts. Invest-\nment bankers depend on accurate, current, and comprehen-\nsive information about their clients and competitors, in-\ndustry trends, capital markets activity, and analyst opin-\nions in order to provide strategic advice and structure cor-\nporate transactions. Investment banks have developed their\nown processes, methods and models to operate in this com-\nplex, high-stakes and fast-moving domain. To support deal-\nmaking and provide strategic advice, senior bankers rely on\nanalysts to gather extensive current and historical informa-\ntion about clients and companies in their coverage sectors.\nAnalysts make use of existing internal models to value com-\npanies, and they monitor industry trends from news and ex-\nternal information sources.\nTo be useful as a question-answering tool in an investment\nbanking setting, an LLM must be able to provide accurate\nand complete answers. This would likely entail analyzing\nmultiple structured and unstructured, often noisy, sources;\ncross-referencing and comparing data; working with sophis-\nticated valuation models; and interpreting all of this informa-\ntion from the perspective of an investment banker. Research\nis needed to evaluate LLM performance on such types of\nknowledge-intensive professional tasks that require domain-\nspecific reasoning and knowledge.\nLLMs face several well-known limitations. To begin with,\nnot all knowledge stored in LLMs can be easily and reli-\nably elicited (Su et al. 2023) and LLM awareness of re-\ncent events is limited by their “knowledge cutoff”, i.e. the\ndate at which the training data was acquired. Relevant infor-\nmation provided in the prompt context has been shown to\nhelp LLMs generate more accurate and grounded responses\n(Ram et al. 2023; Xu et al. 2023) without requiring the\nLLM to be retrained or fine-tuned. Nevertheless, we find\nthat LLM-enhanced search engines, such as Microsoft Bing\nand Google SGE, struggle to give accurate answers to ques-\ntions about recent high-profile mergers and acquisitions even\nwhen citing sources containing the relevant information.\nAAAI Spring Symposium Series (SSS-24)\n125\nThe need for accurate answers has led to substantial effort\nin developing retrieval-augmented generation (RAG), meth-\nods to increase and optimize the context window, e.g. us-\ning function calling (Packer et al. 2023), and prompt engi-\nneering methods, such as chain-of-thought (CoT) reasoning,\nto improve LLM performance on questions about structured\ndata in tables and graphs (Sui et al. 2024; Guo et al. 2023).\nWhile these show great potential, it remains unclear how\nwell LLMs can utilize contextual information and domain\nknowledge for professional question-answering.\nIn this paper, we address this gap by proposing a compact\nbenchmark of data and associated question-answering tasks\nfor investment banking. Using this benchmark, we evaluate\nLLM performance, both without special prompting and with\na number of existing methods for augmenting LLMs with\ndomain knowledge. In particular, we examine the effect on\nLLM performance of different prompt methods and different\nvariants of knowledge presentation. We focus on function\ncalling as an effective foundational method to enable LLMs\nto perform domain-appropriate reasoning by integrating do-\nmain knowledge sources and quantitative models.\nOur results show that enabling LLMs to use Chain-of-\nThought (CoT) reasoning with function calls to retrieve rel-\nevant semi-structured information yields the best perfor-\nmance. Examining LLM errors, we find that their perfor-\nmance is affected by the way the functions are designed\nand the way contextual knowledge is presented. This indi-\ncates a need for careful design and engineering of functional\nAPIs for LLMs and effective knowledge structures to enable\nLLMs to support professional question-answering.\nRelated Work\nIn-Context Learning with LLMs LLMs, such as GPT-\n3.5 and GPT-4, are flexible few-shot reasoners in natural lan-\nguage tasks. Relevant information provided in the prompt\ncontext has been shown to help LLMs generate more ac-\ncurate and grounded responses (Ram et al. 2023; Xu et al.\n2023) without requiring the LLM to be retrained or fine-\ntuned.\nPrompt context can extend LLM internal memory and\nground LLM responses on pre-specified information, im-\nproving the accuracy and reliability of LLM output. As all\nLLMs face limits on context size, various methods have been\nexplored to increase the size of the context window without\nretraining the model (Pal et al. 2023).\nUnfortunately, LLMs do not attend to the entirety of the\ncontext equally and tend to focus more on information lo-\ncated at the beginning and end of the context (Liu et al.\n2023). Thus, increasing context window length tends to de-\ngrade LLM performance in utilizing pertinent information\nburied in the middle of the context.\nInstead of increasing the size of the context, another ap-\nproach is to select and possibly summarize relevant infor-\nmation for inclusion in the context while respecting con-\ntext limits (Wang et al. 2023). Prior work (Xu et al. 2023)\nhas demonstrated the value of well-chosen context, as in\nretrieval-augmented generation (RAG) (Ram et al. 2023),\nas being more effective than simply increasing the context\nwindow of LLMs for long context tasks such as query-based\nsummarization and document question-answering.\nFunction Calling An alternative technique to increasing\ncontext window size is to use the emerging function calling\ncapability1. This enables LLMs to fetch additional context\nvia function calls to external APIs. Function calls have been\nused to enable LLMs to utilize different types of context far\nbeyond the context window limit (Packer et al. 2023). This\nindicates an intriguing ability of LLMs to independently for-\nmulate and compose function calls to retrieve relevant con-\ntextual information for question-answering. In our investiga-\ntion, we rely heavily on function-based prompting and probe\nLLM ability to effective utilize available external functions.\nPrompt Engineering Several methods have been ex-\nplored to improve LLM performance on natural language\ntasks. A popular technique using LLMs over 100B pa-\nrameters is Chain-of-Thought (CoT) prompting (Wei et al.\n2022). CoT has been shown to encourage structured reason-\ning by LLMs and significantly improve outcomes in numer-\nous tasks, including entity recognition (Ashok and Lipton\n2023), mathematical problem-solving and multi-step rea-\nsoning tasks (Liu and Tan 2023; Ziqi and Lu 2023) using\nfunction calls. There are multiple versions of CoT, ranging\nfrom zero-shot prompting (“Let’s think step by step” (Wei\net al. 2022)) to few-shot versions, where several examples\nare provided for the LLM to consult when constructing its\nresponse. Our investigation focuses on the former.\nText-to-SQL In answering factual queries on structured\ndata, the use of function calls is akin to the task of writ-\ning SQL queries to retrieve and manipulate data in relational\ntables. There has been significant effort to develop effective\nfew-shot prompting techniques for LLMs to convert natural\nlanguage questions into SQL queries (Gao et al. 2023), rang-\ning from improving instructions, including schema informa-\ntion, providing valid sample queries, consistency checking\nover multiple rounds of prompting and LLM review-and-\nrefine iterations. To our knowledge, text-to-SQL methods\nhave not yet informed the design of function-based prompt-\ning. We examine the performance of text-to-SQL methods\nand function-based prompting inspired by them on our eval-\nuation set.\nBenchmarks for Querying Structured Data Several\nevaluation benchmarks exist for querying and extraction of\nstructured information, e.g. Spider (Yu et al. 2018), Unite\n(Lan et al. 2023), FinLMEval (Guo, Xu, and Yang 2023)\nand SUC (Sui et al. 2024). These benchmarks focus on the\ntabular manipulation of data and do not probe LLMs on\ndomain-specific questions, e.g. comparing different labels\nfor the same industry. Spider-Syn (Gan et al. 2021) does\ninclude cases where an entity name in a question must be\nmatched against a different value in a field; however, it does\nnot consider one-to-many mappings that are commonly as-\nsumed in professional speech. For example, encryption soft-\nware and cloud security companies can arguably be consid-\n1Currently available for OpenAI LLMs GPT-3.5 and GPT-4,\nLlama 2 and Anthropic’s Claude.\n126\nered as companies in the cybersecurity business by an invest-\nment banker. We include such questions in our benchmark.\nMethod\nWe investigate LLM performance on professional question-\nanswering by developing a dataset relevant to investment\nbankers and a set of associated questions that represent typi-\ncal information needs of investment bankers. We then evalu-\nate LLM performance on this set of questions (often referred\nto as tasks in the literature) with and without additional con-\ntextual knowledge, and examine the impact of different ways\nto present contextual knowledge, and of various prompting\nmethods to procure and process the additional information.\nBenchmark\nTasks To examine the abilities of LLMs on typical ques-\ntions by investment bankers, we developed several ques-\ntions referring to various types of corporate transactions,\ne.g. mergers and acquisitions, by Microsoft and its sub-\nsidiaries between 1986 to the present day. These questions\nwere informed by interviews with investment bankers from\na medium-size boutique investment banking firm in the US\nand attempt to represent the types of questions that a senior\nbanker may ask an analyst when evaluating a deal. The ques-\ntions2 are categorized in Table 1 in terms of the abilities they\nprobe, namely:\nDate ranges: These investigate LLM comprehension and\nmanipulation of dates and time periods. E.g. years (“in\n2019”, “2020 through 2022” or “since 2010”) or monthly\nperiods (such as “calendar Q3 of 2015”, “in the twelve\nmonths beginning January 1, 2015”). Some date ranges are\nimplicit, e.g. “since the acquisition of X”.\nArithmetic: These require the ability to count, rank, round\nand compare. E.g. they request the “value of acquisitions ...\nto the nearest billion”, “largest acquisitions by value” or re-\nquest a comparison of numbers, e.g. “how much more [was\nspent] on acquisitions” and “does the total known value of\nacquisitions exceed 1T$”.\nMulti-step: Questions requiring multiple steps to produce\nan answer. These are often comparison questions, but also\ninclude questions such as “In the year of the Github acqui-\nsition, ...” requiring an LLM to first resolve the year of the\nGithub acquisition before proceeding to answer the rest of\nthe question.\nSemantics (of a domain): Such questions assume domain\nknowledge of investment industry sectors and the mean-\ning of acquisition, e.g. ”which video games businesses”\nor ”which companies were acquired by Microsoft’s Github\nsubsidiary”.\nOpen-ended questions: These questions probe domain\ncomprehension and require a reasoned explanation, e.g.\naround the impact of events on companies or peer/subsidiary\nconnections between companies.\nGeographic questions: These questions require an under-\nstanding of terrestrial geography, relating e.g. to “European\ncompanies”, “Israel-based companies”, “headquarters out-\nside the US”.\n2The full set of questions is available on request.\nData The dataset is a table of acquisitions and mergers,\nstake purchases and divestitures by Microsoft, drawn from\nthe corresponding Wikipedia page 3. It contains the date,\ntransaction value (if known), name of the acquired company,\nthe industry sector or business of the acquired company, the\ncountry where the acquired company is headquartered, and\na list of news articles and press releases announcing the ac-\nquisition. The press releases typically mention the same in-\nformation in unstructured form and enable the comparison\nof structured and unstructured information presentation on\nLLM responses.\nThe data was scraped from Wikipedia using Selenium and\nBeautiful Soup. There are two parts to the data: tabular and\nlinks to textual data. The tabular data was pre-processed and\nsaved in CSV format. Links to external resources (news and\npress releases) were followed, parsed and the resulting text\nconstituted the unstructured component of the data set. In\ncases where the press releases were unavailable or the links\nwere dead, we manually supplemented the dataset with sim-\nilar press releases obtained by web search.\nExperiments\nOur experiments investigate the effect on LLM performance\nof different prompt methods and different forms of knowl-\nedge presentation. We test LLM performance on a baseline\nof no additional context, context provided within a prompt\nand on-demand via function calling. Given the existence of\nseveral questions requiring multi-step computations, we also\ninvestigate combining function calls with CoT reasoning and\nalternative usage of an intermediate SQL engine.\nThe different variants of knowledge presentation we test\nare: unstructured natural language text (Text), i.e. news and\npress releases only, (b) structured, tabular data (Table), i.e.\nthe extracted Wikipedia table, and (c) a mix of unstructured\nand structured data (Combined ). The Combined case con-\nsists of data from Table and Text conditions.\nOne question specifically examines the ability of LLMs to\nextract data present only in unstructured form, i.e. to deter-\nmine the acquirer of a company from the press release. It is\ntherefore excluded from the Table condition.\nBaseline In this condition, we assess the performance of\nOpenAI GPT-4 without any additional context. Microsoft\nBing and Google SGE have access to Internet search results.\nFor these systems, we lightly modified questions as needed\nto enable these systems to respond accurately and to the best\nof their knowledge.\nPrompt All relevant context was placed in the prompt,\nnamely the relevant rows from the table, including refer-\nences (URLs) and their text. The current date and time was\nprovided at the end of the prompt in this and all the follow-\ning conditions.\nFunction In the Function conditions, the LLM is pro-\nvided with functions to retrieve additional information, ef-\nfectively increasing its context window. It is instructed to\nuse the functions to retrieve more accurate, up-to-date infor-\nmation and to make as many function calls as it likes.\n3https://en.wikipedia.org/wiki/List of mergers and\nacquisitions by Microsoft\n127\nQuestion Type Count Sample Questions\nDate ranges 15 What are the names of the companies acquired by Microsoft in the twelve months begin-\nning January 1, 2015?\nMulti-step 12 How much more did Microsoft spend on acquisitions from 2016 to 2020 compared to\n2020 to 2015?\nIn the 2 years prior to the Nemesys Games acquisition by Microsoft, which companies\nwith headquarters outside the US were also acquired by the same company?\nHas Microsoft taken a stake in any company at least 2 years before acquiring it?\nArithmetic 10 What were Microsoft’s three largest acquisitions by value, 2020 through 2022, according\nto public sources?\nSemantics 6 Since 2010, in which year did Microsoft make the most cybersecurity acquisitions?\nWhat are the names of the companies acquired by Microsoft’s GitHub subsidiary in 2019?\nOpen-ended 3 Wang Laboratories has declared bankruptcy. What impact will this have on Microsoft?\nGeography 3 How many European companies did Microsoft acquire in 2016?\nTable 1: A categorization of the 21 evaluation tasks in terms of the comprehension and reasoning abilities they require. See the\nBenchmarks section for a more detailed explanation of the categories.\nFigure 1 shows the process used for iterative function-\nbased prompting. For each user question, an LLM prompt\nis assembled with base and concluding instructions, the user\nquestion itself, and a (initially empty) history buffer consist-\ning of prior LLM responses and the results of the last func-\ntion call. The LLM invocation is augmented with an array of\npossible functions it can call, each specified with a function\nsignature and a textual description of the function and its pa-\nrameters. The LLM determines the data it requires to answer\nthe question and formulates a function call (if needed) to re-\nquest that data. The response to the function call, e.g. the\nextracted data, is then appended to the original prompt and\nresent as a new prompt to the LLM.\nThis condition initially specifies only two functions: Ex-\ntract Data to retrieve data with an optional start or end date\nand by acquisition target name, i.e. the name of the company\nbeing acquired; and the Respond function to simply return\nan answer to the user. Other conditions add more functions\nto the menu, as described below.\nNote that it is not sufficient for the LLM to simply for-\nmulate a function call. Although the functions are domain-\nspecific, they are general enough that they will typically not\nreturn a direct answer to a question. The LLM must addi-\ntionally review the data returned, extract any relevant infor-\nmation, and analyse it to construct an answer to the origi-\nnal question. In addition, depending on the formulation, the\nfunctions could either reduce the volume of information that\nthe LLM must analyze or return a large volume of irrelevant\ndata that may overwhelm the LLM context. Finally, the LLM\ntypically needs to string together function calls in varying\nways to answer questions. Hence, this condition, like the\nfollowing ones, is designed to test the ability of LLMs to\nidentify a correct sequence of function calls to make, intelli-\ngently formulate function calls and use function call output\nto construct answers to user questions.\nFunction + CoTThis condition extends the previous one\nby allowing the LLM to call an additional function Plan to\ncapture a plan of action on how to answer the query. The\nprompt instructions are modified to instruct the LLM to first\noutline this execution plan and only then proceed to actually\nexecute the plan. The Plan function allows the LLM to re-\nview information received from prior function calls (if any)\nand determine next steps—possibly deviating from its initial\nplan—without messaging the user.\nSQL In this condition, the Extract Data function is re-\nplaced by a call to an SQL engine. The new function re-\nquires a valid SQL query (for a given SQL engine) as a pa-\nrameter. In order to create usable SQL queries, the LLM is\nprovided with a schema of an SQL table in JSON format\nwithin the prompt. The table enables the querying of data\nby date ranges, by acquired companies and by the country\nin which an acquired company is headquartered. This con-\ndition uses a pared-down variation of the Function prompt,\nmodified to include a couple of hints to improve SQL query\nformulation and avoid common failure modes.\nSQL + Semantic MatchThis condition was introduced\nupon observing the LLM’s strong preference for using the\nSQL ‘LIKE’ operator to answer questions requiring do-\nmain understanding (Semantics), rather than using its innate\nworld knowledge, which led to several missed opportuni-\nties. To nudge the LLM into identifying semantically simi-\nlar phrases, an additional function Semantic Match was in-\ncluded. The LLM was forced to use this function to identify\nall values in a table column that are similar to a given phrase.\nThis enabled a company providing a ‘gaming backend ser-\nvice’ to be included as a result when searching for ‘video\ngames‘ businesses.\nFunction + CoT + SQLThis condition applies prompt-\ning methods from the text-to-SQL literature in two key\nchanges: (1) the table schema is included in JSON format\nwithin the prompt (with minor renaming of columns) and (2)\nan additional function is added to enable the LLM to select\nindividual columns in the data, thus permitting the retrieval\nof more targeted and less noisy data.\n128\nFigure 1: High-level diagram of our function-based prompting system for the following prompting methods: [Function], [Func-\ntion + CoT], [SQL], [SQL + Semantic Match] and [Function + CoT + SQL]. The Method section describes which functions are\navailable to each prompting method.\nModels Given the niche domain and relatively complex\nquestions, we use GPT-4, specifically gpt-4-0613 4, as rep-\nresenting the best of current LLM capabilities.\nEvaluation LLM performance is assessed primarily on\nthe basis of accuracy (correctness and completeness) of an-\nswers and, to a secondary degree, on the relevance of the in-\nformation provided. While accuracy in paramount in a pro-\nfessional setting, users are often able to compensate for a\nsystem’s deficits and still make use of partially relevant in-\nformation.\nResults\nTable 2 summarizes the overall results of LLM performance\nin terms of the number of correct, partially correct and in-\ncorrect responses to questions depending on the prompting\nmethod and the form of contextual information provided.\nFor questions with lists of answers (e.g. company names),\nwe further analyze the partially correct responses and clas-\nsify them into answers that were correct, missed and wrong\n(Table 3). If the LLM failed to provide an answer in the\nground truth, then that was considered to be a missed an-\nswer. If the LLM provided an answer that was not present in\nthe ground truth, we considered it a wrong answer.\nBaseline The baseline experiments with Base LLM (GPT-\n4), Bing and SGE do not perform well. Of 16 questions, each\nsystem gets only one question completely correct. For Base\nLLM and Bing, the remaining questions are almost evenly\ndivided between partially correct and wholly incorrect re-\nsponses. Google SGE seems to prefer descriptive responses\nat times and seems to perform relatively worse.\nPrompt Performance is only marginally improved when\nrelevant context is pasted into the prompt. The number of\n4with model parameters: temperature set to 0.5, top p to\n0.95.\ncorrect responses rises to 2, but the number of incorrect\nresponses remains high at 10, with 4 partially correct re-\nsponses.\nFunction In comparison with the previous conditions, the\nability to make function calls has a significant positive effect\non LLM performance. LLMs are able to make surprisingly\neffective use of function calls: identifying the need for ad-\nditional data and formulating queries based on the function\nsignature and documentation. Performance is also consider-\nably improved by the presence of contextual information in\nconcise, structured form.\nIn other words, when provided with additional relevant in-\nformation purely in unstructured, natural language form, the\nLLM performs only mildly better than in the Prompt con-\ndition. However, changing the form of the relevant informa-\ntion to either being completely structured or to be a mix of\nstructured and unstructured data, the LLM is able to per-\nform significantly better. This suggests that although LLMs\nare able to process unstructured and structured content, they\ncan more effectively utilize concise, well-structured infor-\nmation.\nThe effect of the form of contextual information is even\nclearer when further examining the partially correct answers\nin Table 3. Textual information caused the highest propor-\ntion of missed answers, suggesting the reduced LLM per-\nformance may be due to the lower information density of\nunstructured content. Structured information approximately\nhalves the proportion of missed answers and significantly\nincreases the proportion of correctly identified answers.\nFunction + CoT Chain-of-thought (CoT) reasoning has\nbeen shown to improve step-by-step reasoning in LLMs. Our\nresults also show that the LLM makes fewer mistakes when\nusing function calls if it is explicitly encouraged to formu-\nlate a plan of action beforehand. While this does not im-\nprove its performance overall in Table 2 significantly, it does\n129\nContext Design Results\nForm Condition Correct Partially Correct Incorrect Tasks\nNone\nBase LLM 1 6 9\n16Microsoft Bing 1 8 7\nGoogle SGE 0 4 12\nText Function 3 5 8 16Function + CoT 3 8 5\nTable Function 8 4 3 15Function + CoT 8 3 4\nCombined\nPrompt 2 4 10 16\nFunction 10 3 3\n16Function + CoT 9 3 4\nFunction + CoT + SQL 12 1 3\nSQL 10 1 5 16SQL + Semantic Match 12 1 3\nTable 2: Results of LLM performance in terms of number of Correct, Partially Correct and Incorrect answers depending on\nprompting method and the form of contextual information provided. We use the most frequent response in three runs as the\nrepresentative response. Not all tasks were appropriate for all forms of information, hence the number of “Tasks” is also noted\nin the table. The best-performing prompting methods are highlighted in bold.\nmarkedly change the nature of its partially correct answers,\nespecially with structured information. As shown in Table\n3, the combination of CoT reasoning and structured infor-\nmation (as in the Table and Combined conditions) leads to\nconsiderably fewer missed and wrong answers, while boost-\ning the proportion of correct answers.\nSQL and SQL + Semantic MatchLLM performance is\nfurther improved with Text-to-SQL prompt techniques, e.g.\nby including the table schema in the context and provid-\ning informative column names. Table 3 further highlights\nthe value of using a query engine on structured data for\nquestion-answering: while this method missed a few an-\nswers, it never resulted in a wrong answer in our testing.\nIt is noteworthy that the LLM needed to be forced to use\nan explicit semantic matching function in order to find simi-\nlar businesses in an industry, even though it could have done\nso itself. In fact, the Semantic Match function merely per-\nforms an independent call to an LLM requesting a semantic\nmatch. After receiving the results of an SQL query, the LLM\nwould not process them any further and would simply return\nthe results. This reluctance only manifests in processing the\noutput of an SQL query; the LLM manages to successfully\nleverage its internal knowledge when creating queries. For\nexample, the LLM successfully answered a question about\nEuropean companies by first creating a list of all European\ncountries and then creating an SQL query to check for the\npresence of any country in the table in that list.\nFunction + CoT + SQL By using techniques inspired\nfrom text-to-SQL prompting and combining them with CoT\nreasoning and function calls, comparable performance to\npure text-to-SQL can be achieved. In this condition, the ta-\nble schema was included in the prompt and the LLM was al-\nlowed to choose the fields of data it wished to retrieve. These\ntwo changes significantly improve the performance of CoT\nreasoning with function calls (Table 2). Examining the re-\nsults in Table 3, we see that it is the best-performing method,\nleading to the smallest proportion of missed answers, very\nfew wrong answers and the most correct answers.\nNoise As a side investigation, we also examined the effect\nof the presence of irrelevant information on LLM perfor-\nmance by presenting it with information about all acquisi-\ntions, stakes and divestitures, not just a relevant subset of the\ninformation. The presence of noise appears to only slightly\ndegrade LLM performance and the results are generally in-\nconclusive. We see this as an area for future research.\nPerformance by Question Type\nArithmetic and Date Range Calculations Arithmetic and\ndate computations are known to be challenging for LLMs.\nAs expected, many numerical questions and date computa-\ntions were incorrectly answered inBaseline conditions. That\nsaid, the LLM seemed to have less difficulty in formulat-\ning function calls with date ranges, both relative and abso-\nlute. The function calls typically had correct date parame-\nters, with relatively few mistakes. LLMs have sporadic for-\nmatting inconsistency in their responses. During our experi-\nments, this also manifested itself with inconsistent date for-\nmatting, e.g. reverting to timestamps instead of ISO dates,\nand consequently causing the extraction function to choke\nintermittently.\nMulti-step Computations These remain challenging for\nLLMs. Although the LLM is often able to formulate a rea-\nsonable plan with CoT reasoning, it regularly fails to exe-\ncute all components of its plan when dealing with complex\nqueries. In this case, SQL-inspired methods were more suc-\n130\nContext Design Results\nForm Condition Correct Missed Wrong Total Incorrect\nText Function 0.56 0.44 0.07 0.52\nFunction + CoT 0.56 0.44 0.22 0.52\nTable Function 0.78 0.22 0.02 0.24\nFunction + CoT 0.85 0.15 0.07 0.22\nCombined\nFunction 0.78 0.22 0.19 0.41\nFunction + CoT 0.93 0.07 0.07 0.15\nFunction + CoT + SQL 0.95 0.05 0.04 0.09\nSQL 0.80 0.20 0.00 0.20\nSQL + Semantic Match 0.91 0.09 0.00 0.09\nTable 3: An analysis of LLM performance on questions with lists as answers. For such questions, an incomplete answer may\nbe better than no answer; however, wrong answers are clearly undesirable. We characterize LLM performance in terms of the\nproportion of total list elements correctly identified, missed and wrongly identified. The total incorrect proportion is simply the\nsum of the missed and wrong proportions. The best-performing prompting method and results are highlighted in bold.\ncessful, as the LLM could sometimes encode a multi-step\nplan in the SQL query and then outsource the execution to\nan SQL engine. For example, when answering the question\n“How much more did Microsoft spend on acquisitions from\n2016 to 2020 compared to 2020 to 2015?”, the answer can\nbe encoded as the result of an SQL query.\nDomain Knowledge & Semantics In answering ques-\ntions that require (relatively shallow) domain knowledge,\nthe LLM was able to find good and unexpected answers.\nFor example, when asked about “cybersecurity”, it identi-\nfied companies in the data protection and data governance\nbusiness. Similarly, it correctly identified a “gaming back-\nend service” and an “industrial AI platform” as companies\nrelated to “cloud computing”. As discussed previously, these\nwere more difficult with the SQL function-based prompting\nmethods.\nOpen-Ended Open-ended questions were the most chal-\nlenging. Although they could be answered with some real-\nworld knowledge and the retrieval of appropriate informa-\ntion from the table, no method we tested induced the LLM\nto answer them successfully.\nGeography The geographic components of questions\nposed no problems whatsoever for the LLMs, suggesting\nthey are adept in reasoning about geography and have ex-\ntensive knowledge of cities, countries and larger geographic\nregions.\nDiscussion\nOur results show that, without special prompting, a state-of-\nthe-art LLM and major LLM-enhanced search engines are\ngenerally unable to answer accurately the kinds of straight-\nforward questions that investment bankers would expect\ntheir analysts to handle with ease. Adding relevant informa-\ntion to the prompt, as in a naive RAG implementation, is\nhelpful, but inadequate. Importantly, we find that domain-\ntuned function calls can have a transformative impact on\nLLM performance, increasing accuracy from at most 12.5%\nto as high as 75%, and providing a method for LLMs to in-\ncorporate domain-specific knowledge and reasoning. While\nthese results still fall short of the accuracy demanded in pro-\nfessional settings such as investment banking, the magnitude\nof the improvement suggests that function calls are a promis-\ning direction for research into applying LLMs in such task\ndomains.\nAn interesting side effect of using functions is that the\nfunction calls generated by the LLM provide some insight\ninto the LLM’s reasoning process. This led to several in-\ntriguing observations in the course of our experiments, sum-\nmarized below.\nFunction-induced bias: When functions are available,\nLLMs seem to prefer to use the functions, even without be-\ning explicitly instructed to do so. Moreover, functions ap-\npear to prime reasoning in particular ways. When asked to\nidentify the acquisitions made by Github in a given year\nand provided with a function allowing for retrieval of ad-\nditional data about acquired companies, the LLM frequently\nattempts to retrieve data for the Github acquisition by Mi-\ncrosoft, rather than acquisitions by Github. The correct ap-\nproach would have been to retrieve data about all acquisi-\ntions in the given year, and then to read through the text of\npress releases to identify the ones made by Github. When the\nquestions are about acquired companies, and thus match the\nfunction signature provided, the LLM performs very well.\nUnexpected function behavior: We observed a different\ntype of error when function behavior did not match the\nLLM’s expectation. When the LLM attempts to answer a\nquestion about acquisitions within a range of years, the LLM\ninvokes the function with the correct start and end dates.\nHowever, the underlying function (in Python) excludes the\nend of the data range. This causes an entire year of data to be\nomitted from the results. Although the incomplete results are\nappended to the prompt, subsequent LLM reasoning does\nnot spot the problem and blithely continues processing the\ndata. This problem was solved by instructing the LLM to\nspecify dates completely, including the day and month.\nInefficient and incomplete reasoning: The LLM does not\n131\nalways invoke function calls efficiently, despite instructions\nto do so. For example, to answer the question “Since 2010,\nin which year did Microsoft make the most cybersecurity\nacquisitions?”, rather than retrieving only acquisitions since\n2010, the LLM tends to query for all transactions and then\nfilter to those satisfying the date filter. This occurs even in\nthe CoT condition, and would clearly be very inefficient\nwhen accessing a large database. Similarly, without addi-\ntional prompting, the LLM often stops short of performing\nthe final step in an execution plan. For example, when using\nSQL to fetch press releases, it may subsequently forget to\nexamine them for answers, or when asked for the total ac-\nquisition expenditure in a given year, it fetches the numbers,\nbut subsequently fails to sum them up.\nThese observations highlight the sensitivity of LLM pro-\ncessing to the way that functions and data are made available\nto the LLM. Thoroughly-documented APIs are likely im-\nportant, as well as having a range of multiple domain-tuned\nfunctions to choose from. Performance improvements could\nbe achieved by presenting the same underlying functionality\nin multiple forms, with varying descriptions, signatures, and\nreturn values, so as to enable the LLM to better incorporate\nthe functionality in different types of analysis. For example,\nmultiple functions for calculating specific variants of finan-\ncial ratios might map to the same underlying logic, but in-\nstead of relying on the LLM to perform conversions, having\nfunctions with exactly matching signatures could yield more\nreliable reasoning.\nIn real-world applications, LLMs may benefit from ac-\ncess to an extensive set of domain-tuned functions. Even if\nall these functions were well-documented and potentially ac-\ncessible to the LLM, making every function available in ev-\nery LLM request would be impractical, at least with current\ntechniques. Function definitions are injected into the LLM\nprompt, so incorporating a large number of functions could\nconsume too much of the limited context window, and LLMs\nstruggle to effectively utilize long context. This suggests that\nLLMs could benefit from an intermediate function-selection\nstep, perhaps integrated with CoT reasoning and planning,\nto examine a user question and identify a subset of the avail-\nable functions that appear most relevant to answering the\nquestion. Research is needed to determine robust, scalable\nmechanisms for enabling LLMs to make effective use of\nlarge function collections.\nAdditional considerations in real-world settings include\ninput validation and verification, to ensure that functions be-\nhave as documented and as understood by the LLM, possi-\nbly by applying formal (automated) verification techniques.\nFurthermore, the use of function calls can multiply the num-\nber of LLM queries, resulting in higher cost and slower re-\nsponse time. Further research is needed, both to determine\nhow best to design and incorporate functions and knowledge\nstructures so that LLM performance is maximized, and to\ndevelop techniques for eliciting more efficient and reliable\nLLM behaviour.\nFinally, a surprising anecdote highlights the risk of us-\ning closed-source LLMs subject to change without notice.\nWhile querying GPT-4 on a question related to Israel-based\ncompanies, after initially responding as expected, it refused\nto answer any questions related to Israel. Since we were\nusing a specific snapshot of the GPT-4 model, such insta-\nbility was not expected. It is possible that the OpenAI hid-\nden prompt had been modified. While such controls may be\nnecessary in public consumer applications, in professional\ncontexts, such changes in behaviour could decrease relia-\nbility, undermine user trust, and potentially cause financial\nloss, regulatory breach, or legal liability. Adoption of closed-\nsource models in business may require greater transparency\nand mechanisms for giving customers advance notice about\npotentially breaking changes.\nConclusion\nDespite the promise of LLMs and the substantial effort put\ninto tuning their performance in numerous domains with\nfine-tuning and prompt engineering, it remains unclear how\nwell LLMs can utilize contextual information and domain\nknowledge for question-answering in professional settings.\nTo address this gap, we assessed LLM performance in sup-\nporting investment bankers on a custom benchmark of 16\nrelatively straightforward analytical tasks representative of\nthe investment banking industry. We evaluated LLM perfor-\nmance without special prompting, with relevant information\nprovided in the prompt, and as part of a system giving the\nLLM access to domain-tuned functions for information re-\ntrieval and planning.\nOur results show that without access to functions, state\nof the art LLMs performed poorly, completing two or fewer\ntasks correctly. Prompting with functions resulted in signifi-\ncantly better results, although LLM performance was highly\nsensitive to the design of the functions and the structure of\nthe information they returned. In our experiments, the most\neffective design was a combination of CoT reasoning with\nfunction calls, access to the information schemata and where\nthe LLM was permitted to choose which information the\nfunctions returned. This enabled the LLMs to effectively re-\ntrieve and process relevant information, resulting in correct\nanswers on 12 out of 16 tasks. Our results show that one way\nto empower LLMs with domain knowledge is to incorporate\ndomain-specific functions and information structures that\nenable LLMs to reason in domain-appropriate ways. These\ntechniques may prove to be a powerful means of adapting\nLLMs for use in demanding professional settings.\nReferences\nAshok, D.; and Lipton, Z. C. 2023. PromptNER: Prompting\nFor Named Entity Recognition. ArXiv:2305.15444 [cs].\nDell’Acqua, F.; McFowland, E.; Mollick, E. R.; Lifshitz-\nAssaf, H.; Kellogg, K.; Rajendran, S.; Krayer, L.; Candelon,\nF.; and Lakhani, K. R. 2023. Navigating the Jagged Techno-\nlogical Frontier: Field Experimental Evidence of the Effects\nof AI on Knowledge Worker Productivity and Quality.Har-\nvard Business School Technology & Operations Mgt. Unit\nWorking Paper No. 24-013.\nGan, Y .; Chen, X.; Huang, Q.; Purver, M.; Woodward, J. R.;\nXie, J.; and Huang, P. 2021. Towards Robustness of Text-\nto-SQL Models against Synonym Substitution. In Zong, C.;\nXia, F.; Li, W.; and Navigli, R., eds.,Proceedings of the 59th\n132\nAnnual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers), 2505–\n2515. Online: Association for Computational Linguistics.\nGao, D.; Wang, H.; Li, Y .; Sun, X.; Qian, Y .; Ding, B.; and\nZhou, J. 2023. Text-to-SQL Empowered by Large Language\nModels: A Benchmark Evaluation. ArXiv:2308.15363 [cs].\nGuo, J.; Du, L.; Liu, H.; Zhou, M.; He, X.; and Han, S.\n2023. GPT4Graph: Can Large Language Models Under-\nstand Graph Structured Data ? An Empirical Evaluation and\nBenchmarking. ArXiv:2305.15066 [cs].\nGuo, Y .; Xu, Z.; and Yang, Y . 2023. Is ChatGPT a Fi-\nnancial Expert? Evaluating Language Models on Financial\nNatural Language Processing. Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2023, 815–821.\nArXiv:2310.12664 [cs].\nLan, W.; Wang, Z.; Chauhan, A.; Zhu, H.; Li, A.; Guo, J.;\nZhang, S.; Hang, C.-W.; Lilien, J.; Hu, Y .; Pan, L.; Dong, M.;\nWang, J.; Jiang, J.; Ash, S.; Castelli, V .; Ng, P.; and Xiang,\nB. 2023. UNITE: A Unified Benchmark for Text-to-SQL\nEvaluation. ArXiv:2305.16265 [cs].\nLiu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua,\nM.; Petroni, F.; and Liang, P. 2023. Lost in the Middle: How\nLanguage Models Use Long Contexts. ArXiv:2307.03172\n[cs].\nLiu, X.; and Tan, Z. 2023. Divide and Prompt: Chain of\nThought Prompting for Text-to-SQL. ArXiv:2304.11556\n[cs].\nPacker, C.; Fang, V .; Patil, S. G.; Lin, K.; Wooders, S.; and\nGonzalez, J. E. 2023. MemGPT: Towards LLMs as Operat-\ning Systems. arXiv:2310.08560.\nPal, A.; Karkhanis, D.; Roberts, M.; Dooley, S.; Sundarara-\njan, A.; and Naidu, S. 2023. Giraffe: Adventures in Expand-\ning Context Lengths in LLMs. ArXiv:2308.10882 [cs].\nRam, O.; Levine, Y .; Dalmedigos, I.; Muhlgay, D.; Shashua,\nA.; Leyton-Brown, K.; and Shoham, Y . 2023. In-Context\nRetrieval-Augmented Language Models. Transactions of\nthe Association for Computational Linguistics, 11: 1316–\n1331. Place: Cambridge, MA Publisher: MIT Press.\nSu, D.; Patwary, M.; Prabhumoye, S.; Xu, P.; Prenger, R.;\nShoeybi, M.; Fung, P.; Anandkumar, A.; and Catanzaro, B.\n2023. Context Generation Improves Open Domain Question\nAnswering. ArXiv:2210.06349 [cs].\nSui, Y .; Zhou, M.; Zhou, M.; Han, S.; and Zhang, D. 2024.\nTable Meets LLM: Can Large Language Models Understand\nStructured Table Data? A Benchmark and Empirical Study.\nIn The 17th ACM International Conference on Web Search\nand Data Mining (WSDM ’24).\nWang, Q.; Ding, L.; Cao, Y .; Tian, Z.; Wang, S.; Tao,\nD.; and Guo, L. 2023. Recursively Summarizing Enables\nLong-Term Dialogue Memory in Large Language Models.\nArXiv:2308.15022 [cs] version: 1.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;\nXia, F.; Chi, E.; Le, Q.; and Zhou, D. 2022. Chain-of-\nThought Prompting Elicits Reasoning in Large Language\nModels.\nWu, S.; Irsoy, O.; Lu, S.; Dabravolski, V .; Dredze, M.;\nGehrmann, S.; Kambadur, P.; Rosenberg, D.; and Mann, G.\n2023. BloombergGPT: A Large Language Model for Fi-\nnance. ArXiv:2303.17564 [cs, q-fin].\nXu, P.; Ping, W.; Wu, X.; McAfee, L.; Zhu, C.; Liu, Z.; Sub-\nramanian, S.; Bakhturina, E.; Shoeybi, M.; and Catanzaro,\nB. 2023. Retrieval meets Long Context Large Language\nModels. Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, 815–821.\nYu, T.; Zhang, R.; Yang, K.; Yasunaga, M.; Wang, D.; Li,\nZ.; Ma, J.; Li, I.; Yao, Q.; Roman, S.; Zhang, Z.; and Radev,\nD. 2018. Spider: A Large-Scale Human-Labeled Dataset for\nComplex and Cross-Domain Semantic Parsing and Text-to-\nSQL Task. In EMNLP.\nZiqi, J.; and Lu, W. 2023. Tab-CoT: Zero-shot Tabular Chain\nof Thought. In Rogers, A.; Boyd-Graber, J.; and Okazaki,\nN., eds., Findings of the Association for Computational Lin-\nguistics: ACL 2023, 10259–10277. Toronto, Canada: Asso-\nciation for Computational Linguistics.\n133",
  "topic": "Domain (mathematical analysis)",
  "concepts": [
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6396538019180298
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6163415908813477
    },
    {
      "name": "Investment (military)",
      "score": 0.5319108366966248
    },
    {
      "name": "Computer science",
      "score": 0.5119677782058716
    },
    {
      "name": "Work (physics)",
      "score": 0.47856026887893677
    },
    {
      "name": "Productivity",
      "score": 0.41224405169487
    },
    {
      "name": "Knowledge management",
      "score": 0.3544304668903351
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.3286832571029663
    },
    {
      "name": "Business",
      "score": 0.23252230882644653
    },
    {
      "name": "Engineering",
      "score": 0.19374454021453857
    },
    {
      "name": "Political science",
      "score": 0.1614285111427307
    },
    {
      "name": "Economics",
      "score": 0.1457805633544922
    },
    {
      "name": "Economic growth",
      "score": 0.09758874773979187
    },
    {
      "name": "Mathematics",
      "score": 0.09073856472969055
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210111831",
      "name": "ModuleWorks (Romania)",
      "country": "RO"
    }
  ],
  "cited_by": 11
}