{
    "title": "Visformer: The Vision-friendly Transformer",
    "url": "https://openalex.org/W3159732141",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3018609235",
            "name": "Zhengsu Chen",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2116030393",
            "name": "Lingxi Xie",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A2137906572",
            "name": "Jianwei Niu",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2097092038",
            "name": "Xuefeng Liu",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2754205443",
            "name": "Longhui Wei",
            "affiliations": [
                "University of Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A1993201802",
            "name": "Qi Tian",
            "affiliations": [
                "Xidian University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6729342207",
        "https://openalex.org/W6749954789",
        "https://openalex.org/W6762718338",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W6779879114",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3035682985",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W6638523607",
        "https://openalex.org/W3035452548",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W6701947533",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W6758139636",
        "https://openalex.org/W6763367864",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W2982220924",
        "https://openalex.org/W6637373629",
        "https://openalex.org/W2097073572",
        "https://openalex.org/W6786585107",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W6788467338",
        "https://openalex.org/W6790275670",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W6638667902",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W6684191040",
        "https://openalex.org/W6638444622",
        "https://openalex.org/W6762226699",
        "https://openalex.org/W3035050085",
        "https://openalex.org/W2998508940",
        "https://openalex.org/W3034429256",
        "https://openalex.org/W6777239595",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W2949718784",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W3102631365",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W2937843571",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3034885317",
        "https://openalex.org/W2953328958",
        "https://openalex.org/W2992308087",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2911925209",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W3109319753",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2795783309",
        "https://openalex.org/W3129603602",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W1799366690"
    ],
    "abstract": "The past year has witnessed the rapid development of applying the Transformer module to vision problems. While some researchers have demonstrated that Transformer-based models enjoy a favorable ability of fitting data, there are still growing number of evidences showing that these models suffer over-fitting especially when the training data is limited. This paper offers an empirical study by performing step-by-step operations to gradually transit a Transformer-based model to a convolution-based model. The results we obtain during the transition process deliver useful messages for improving visual recognition. Based on these observations, we propose a new architecture named Visformer, which is abbreviated from the `Vision-friendly Transformer'. With the same computational complexity, Visformer outperforms both the Transformer-based and convolution-based models in terms of ImageNet classification accuracy, and the advantage becomes more significant when the model complexity is lower or the training set is smaller. The code is available at https://github.com/danczs/Visformer.",
    "full_text": "1\nVisformer: The Vision-friendly Transformer\nZhengsu Chen, Lingxi Xie, Jianwei Niu, Senior Member, IEEE,\nXuefeng Liu, Longhui Wei, Qi Tian, Fellow, IEEE\nAbstract—The past few years have witnessed the rapid devel-\nopment of applying the Transformer module to vision problems.\nWhile some researchers have demonstrated that Transformer-\nbased models enjoy a favorable ability of ﬁtting data, there are\nstill growing number of evidences showing that these models\nsuffer over-ﬁtting especially when the training data is limited.\nThis paper offers an empirical study by performing step-by-\nstep operations to gradually transit a Transformer-based model\nto a convolution-based model. The results we obtain during\nthe transition process deliver useful messages for improving\nvisual recognition. Based on these observations, we propose a\nnew architecture named Visformer, which is abbreviated from\nthe ‘Vision-friendly Transformer’. With the same computational\ncomplexity, Visformer outperforms both the Transformer-based\nand convolution-based models in terms of ImageNet classiﬁcation\nand object detection performance, and the advantage becomes\nmore signiﬁcant when the model complexity is lower or the\ntraining set is smaller. The code is available at https://github.\ncom/danczs/Visformer.\nIndex Terms—vision-friendly Transformer, vision Transformer,\nconvolutional neural network, image recognition.\nI. I NTRODUCTION\nI\nN the past decade, convolution used to play a central role\nin the deep learning models [1]–[4] for visual recognition.\nThis situation starts to change when the Transformer [5], a\nmodule that originates from natural language processing [5]–\n[7], is transplanted to the vision scenarios. It was shown in\nthe ViT model [8] that an image can be partitioned into a\ngrid of patches and the Transformer is directly applied upon\nthe grid as if each patch is a visual word. ViT requires a\nlarge amount of training data ( e.g., the ImageNet-21K [9]\nor the JFT-300M dataset), arguably because the Transformer\nis equipped with long-range attention and interaction, and is\nprone to over-ﬁtting. The follow-up efforts [10] improved ViT\nto some extent, but these models still perform badly especially\nunder limited training data or moderate data augmentation\ncompared with convolution-based models.\nOn the other hand, vision Transformers can achieve much\nbetter performance than convolution-based models when\nZhengsu Chen and Xuefeng Liu are with the School of Computer Sci-\nence and Engineering, Beihang University, 100191, Beijing, China (e-mail:\ndanczs@buaa.edu.cn; liu xuefeng@buaa.edu.cn)\nLingxi Xie is with Johns Hopkins University, Baltimore, USA (e-mail:\n198808xc@gmail.com)\nJianwei Niu is with Hangzhou Innovation Institute of Beihang University,\nZhengzhou University, and Beihang University, 100191, Beijing, China (e-\nmail: @niujianwei@buaa.edu.cn)\nLonghui wei is with the University of Science and Technology of China,\n230026, Hefei, China (e-mail:longhuiwei@pku.edu.cn)\nQi Tian is with Xidian University, 710126, Xi’an, China (e-mail: wywq-\ntian@gmail.com)\nTABLE I\nTHE COMPARISON AMONG RESNET-50, D EIT-S, AND THE PROPOSED\nVISFORMER -S MODEL ON IMAGE NET.\nNetwork ResNet-50 DeiT-S Visformer-S\nFLOPs (G) 4.1 4.6 4.9\nParameters (M) 25.6 21.8 40.2\nFull base setting 77.43 63.12 77.20\ndata elite setting 78.73 80.07 82.19\nPart of 10% labels 58.37 40.41 58.74\ndata 10% classes 89.90 80.06 90.06\ntrained with large amount of data. Namely, vision Transform-\ners have higher ‘upper-bound’ while convolution-based models\nare better in ‘lower-bound’. Both upper-bound and lower-\nbound are important properties for neural networks. Upper-\nbound is the potential to achieve higher performance and\nlower-bound enables networks to perform better when trained\nwith limited data or scaled to different complexity.\nBased on the observation of lower-bound and upper-bound\non Transformer-based and convolution-based networks, the\nmain goal of this paper is to identify the reasons behind\nthe difference, by which we can design networks with higher\nlower-bound and upper-bound. The gap between Transformer-\nbased and convolution-based networks can be revealed with\ntwo different training settings on ImageNet. The ﬁrst one is\nthe base setting. It is the standard setting for convolution-\nbased models, i.e., the training schedule is shorter and the data\naugmentation only contains basic operators such as random-\nsize cropping [11] and ﬂipping. The performance under this\nsetting is called base performance in this paper. The other\none is the training setting used in [10]. It is carefully tuned\nfor Transformer-based models, i.e., the training schedule is\nlonger and the data augmentation is stronger ( e.g., RandAug-\nment [12], CutMix [13], etc., have been added). We use the\nelite performance to refer to the accuracy produced by it.\nWe take DeiT-S [10] and ResNet-50 [4] as the examples of\nTransformer-based and convolution-based models. As shown\nin Table I, Deit-S and ResNet-50 employ comparable FLOPs\nand parameters. However, they behave very differently trained\non the full data under these two settings. Deit-S has higher elite\nperformance, but changing the setting from elite to base can\ncause a 10%+ accuracy drop for DeiT-S. ResNet-50 performs\nmuch better under the base setting, yet the improvement for\nthe elite setting is merely 1.3%. This motivates us to study\nthe difference between these models. With these two settings,\nwe can roughly estimate the lower-bound and upper-bound of\nthe models. The methodology we use is to perform step-by-\nstep operations to gradually transit one model into another, by\n0000–0000/00$00.00 © 2021 IEEE\narXiv:2104.12533v5  [cs.CV]  18 Dec 2021\n2\nwhich we can identify the properties of modules and designs\nin these two networks. The entire transition process, taking a\ntotal of 8 steps, is illustrated in Figure 1.\nSpeciﬁcally, from DeiT-S to ResNet-50, one should (i)\nuse global average pooling (not the classiﬁcation token),\n(ii) introduce step-wise patch embeddings (not large patch\nﬂattening), (iii) adopt the stage-wise backbone design, (iv) use\nbatch normalization [14] (not layer normalization [15]), (v)\nleverage 3 ×3 convolutions, (vi) discard the position embed-\nding scheme, (vii) replace self-attention with convolution, and\nﬁnally (viii) adjust the network shape ( e.g., depth, width, etc.).\nAfter a thorough analysis on the reasons behind the results, we\nabsorb all the factors that are helpful to visual recognition and\nderive the Visformer, i.e., the Vision-friendly Transformer.\nEvaluated on ImageNet classiﬁcation, Visformer claims\nbetter performance than the competitors, DeiT and ResNet,\nas shown in Table I. With the elite setting, the Visformer-\nS model outperforms DeiT-S and ResNet-50 by 2.12% and\n3.46%, respectively, under a comparable model complexity.\nDifferent from Deit-S, Visformer-S also survives two extra\nchallenges, namely, when the model is trained with 10%\nlabels (images) and 10% classes. Visformer-S even performs\nbetter than ResNet-50, which reveals the high lower-bound\nof Visformer-S. Additionally, for tiny models, Visformer-Ti\nsigniﬁcantly outperforms Deit-Ti by more than 6%.\nThe contribution of this paper is three-fold. First, for the\nﬁrst time, we introduce the lower-bound and upper-bound\nto investigate the performance of Transformer-based vision\nmodels. Second, we close the gap between the Transformer-\nbased and convolution-based models by a gradual transition\nprocess and thus identify the properties of the designs in\nthe Transformer-based and convolution-based models. Third,\nwe propose the Visformer as the ﬁnal model that achieves\nsatisfying lower-bound and upper-bound.\nThe preliminary version of this paper appeared as [16]. In\nthe extended version, we further explore the recently proposed\nwork and provide more experiments and analysis. The main\nimprovements over the preliminary version are summarized as\nfollows:\n• We optimize the architecture of Visformer according to\nthe experimental observations and propose VisformerV2\nwhich substantially outperforms the old version.\n• We analyze the overﬂow problem when utilizing half-\nprecision in Transformers and propose an efﬁcient\nmethod to avoid overﬂow without degrading the perfor-\nmance.\n• We generalize Visformer to downstream vision tasks and\nobserve consistent improvements.\nII. R ELATED WORK\nImage classiﬁcation is a fundamental task in computer\nvision. In the deep learning era, the most popular method is to\nuse deep neural networks [2], [4], [17]. One of the fundamental\nunits to build such networks is convolution, where a number\nof convolutional kernels are used to capture repeatable local\npatterns in the input image and intermediate data. To reduce\nthe computational costs as well as alleviate the risk of over-\nﬁtting, it was believed that the convolutional kernels should\nbe of a small size, e.g., 3 ×3. However, this brings the\ndifﬁculty for faraway contexts in the image to communicate\nwith each other – this is partly the reason that the number\nof layers has been increasing. Despite stacking more and\nmore layers, researchers consider another path which is to use\nattention-based approaches to ease the propagation of visual\ninformation.\nSince Transformers achieved remarkable success in natural\nlanguage processing (NLP) [5]–[7], many efforts have been\nmade to introduce Transformers to vision tasks. These works\nmainly fall into two categories. The ﬁrst category consists\nof pure attention models [8], [10], [18]–[22]. These models\nusually only utilize self-attention and attempt to build vision\nmodels without convolutions. However, it is computationally\nexpensive to relate all pixels with self-attention for realistic\nfull-sized images. Thus, there has some interest in forcing\nself-attention to only concentrate on the pixels in local neigh-\nborhoods ( e.g., SASA [18], LRNet [19], SANet [20]). These\nmethods replace convolutions with local self-attentions to learn\nlocal relations and achieve promising results. However, it\nrequires complex engineering to efﬁciently apply self-attention\nto every local region in an image. Another way to solve\nthe complexity problem is to apply self-attention to reduced\nresolution. These methods either reduce the resolution and\ncolor space ﬁrst [21] or regard image patches rather pixels as\ntokens ( i.e., words) [8], [10]. However, resolution reduction\nand patch ﬂattening usually make it more difﬁcult to utilize\nthe local prior in natural images. Thus, these methods usually\nobtain suboptimal results [21] or require huge dataset [8] and\nheavy augmentation [10].\nThe second category contains the networks built with not\nonly self-attentions but also convolutions. Self-attention was\nﬁrst introduced to CNNs by non-local neural networks [23].\nThese networks aim to capture global dependencies in images\nand videos. Note that non-local neural networks are inspired by\nthe classical non-local method in vision tasks [24] and unlike\nthose in Transformers, the self-attentions in non-local networks\nare usually not equipped with multi-heads and position em-\nbedding [23], [25], [26]. Afterwards, Transformers achieve\nremarkable success in NLP tasks [6], [7] and, therefore,\nself-attentions that inherits NLP settings ( e.g., multi-heads,\nposition encodings, classiﬁcation token, etc.) are combined\nwith convolutions to improve vision tasks [18], [27], [28].\nA common combination is to utilize convolutions ﬁrst and\napply self-attention afterwards [8], [29]. [8] builds hybrids of\nself-attention and convolution by adding a ResNet backbone\nbefore Transformers. Afterwards, more and more methods are\nproposed to combine self-attention and convolution. Besides\nutilizing convolution in early layers [30], BotNet [29] designs\nbottleneck cells for self-attention. Conformer [31] fuses the\nfeature of a convolution neural network and a Transformer\nwith the feature coupling unit to combine the global and local\nrepresentations. CvT [32] introduces convolution to the feature\nprojection of self-attention, by which the query, key and value\nin the self-attention can capture the local information. CoAt-\nNet [33] uniﬁes depthwise convolution with self-attention and\nvertically stacks convolution layers and self-attention layers to\nimprove the generalization, capacity and efﬁciency. However,\n3\nthese methods usually combine convolution and self-attention\nempirically or heuristically. Our method, in contrast, explores\nthe full process of converting a Transformer to a convolution\nneural network.\nThere is also work that studies scaling or training vision\nTransformer. CaiT [34] ﬁnd that it is very efﬁcient to scale\nup vision Transformer in depth dimension with LayerScale.\nAutoFormer [35] builds a super Transformer network by which\nthey can evaluate the different designs and search efﬁcient\nTransformer architecture. Zhai et al. [36] observe that most\nvision Transformers can beneﬁt from increasing compute\nresources and larger dataset. They suggest scaling up compute,\ndata and model together. Steiner et al. [37] further study data,\naugmentation and regularization in vision Transformer and\nﬁnds that augmentation can yield the same performance as\nthat trained on an order of magnitude more data.\nAdditionally, self-attention has been used in many down-\nstream vision tasks (detection [38], segmentation [39]) and low\nvision tasks [40]. These tasks usually utilize much larger input\nresolution than classiﬁcation. For example, the frameworks\nin COCO [41] usually utilize 1280 ×800 inputs. This is a\ncritical problem for vision Transformer, since the complexity\nincreases quadratically with pixel numbers. The widely used\nsolution is adopting sliding windows to capture local patterns\nand building extra pipelines for information exchange among\nthe windows. Swin Transformer [42] shifts the windows\nalternately in different layers, by which the tokens can build\nlong-distance relations as the depth increases. CSWin Trans-\nformer [43] further develops the cross-shaped self-attention\nmechanism to ensure that the windows can access the global\nfeature in one dimension. MSG-Transformer [44], by contrast,\nexchanges the local information by messenger tokens.\nIII. M ETHODOLOGY\nA. Transformer-based and convolution-based visual recogni-\ntion models\nRecognition is the fundamental task in computer vision.\nThis work mainly considers image classiﬁcation, where the\ninput image is propagated through a deep network to derive\nthe output class label. Most deep networks are designed in a\nhierarchical manner and composed of a series of layers.\nWe consider two popular layers named convolution and\nTransformer. Convolution originates from the intuition to\ncapture local patterns which are believed more repeatable\nthan global patterns. It uses a number of learnable kernels\nto compute the responses of the input to different patterns,\nfor which a sliding window is moved along both axes of\nthe input data and the inner-product between the data and\nkernel is calculated. In this paper, we constrain our study\nin the scope of residual blocks, a combination of 2 or 3\nconvolutional layers and a skip-connection. Non-linearities\nsuch as activation and normalization are inserted between the\nneighboring convolutional layers.\nOn the other hand, Transformer originates from natural\nlanguage processing and aims to frequently formulate the\nrelationship between any two elements (called tokens) even\nwhen they are far from each other. This is achieved by\ngenerating three features for each token, named the query, key,\nand value, respectively. Then, the response of each token is\ncalculated as a weighted sum over all the values, where the\nweights are determined by the similarity between its query and\nthe corresponding keys. This is often referred to as multi-head\nself-attention (MHSA), followed by other operations including\nnormalization and linear mapping.\nThroughout the remaining part, we consider DeiT-S [10]\nand ResNet-50 [4] as the representative of Transformer-based\nand convolution-based models, respectively. Besides the basic\nbuilding block, there are also differences in design, e.g.,\nResNet-50 has a few down-sampling layers that partition\nthe model into stages, but the number of tokens remains\nunchanged throughout DeiT-S. The impact of these details will\nbe elaborated in Section III-C.\nB. Settings: The base and elite performance\nAlthough DeiT-S reports a 80.1% accuracy which is higher\nthan 78.7% of ResNet-50, we notice that DeiT-S has changed\nthe training strategy signiﬁcantly, e.g., the number of epochs is\nenlarged by more than 3×and the data augmentation becomes\nmuch stronger. Interestingly, DeiT-S seems to heavily rely on\nthe carefully-tuned training strategy, and other Transformer-\nbased models including ViT [8] and PIT [40] also reported\ntheir dependency on other factors, e.g., a large-scale training\nset. In what follows, we provide a comprehensive study on\nthis phenomenon.\nWe evaluate all classiﬁcation models on the ImageNet\ndataset [45] which has 1K classes, 1.28M training images and\n50K testing images. Each class has roughly the same number\nof training images. This is one of the most popular datasets\nfor visual recognition.\nThere are two settings to optimize each recognition model.\nThe ﬁrst one is named the base setting which is widely\nadopted by convolution-based networks. Speciﬁcally, the\nmodel is trained for 90 epochs with the SGD optimizer.\nThe learning rate starts with 0.2 for batch size 512 and\ngradually decays to 0.00001 following the cosine annealing\nfunction. A moderate data augmentation strategy with random-\nsize cropping [11] and ﬂipping is used. The second one is\nnamed the elite setting which has been veriﬁed effective to\nimprove the Transformer-based models. The Adamw optimizer\nwith an initial learning rate of 0.0005 for batch size 512 is\nused. The data augmentation and regularization strategy is\nmade much stronger to avoid over-ﬁtting, for which intensive\noperations including RandAugment [12], Mixup [46], Cut-\nMix [13], Random Erasing [47], Repeated Augmentation [48],\n[49] and Stochastic Depth [50] are used. Correspondingly, the\ntraining lasts 300 epochs, much longer than that of the base\nsetting.\nThroughout the remaining part of this paper, we refer to\nthe classiﬁcation accuracy under the base and elite settings as\nbase performance and elite performance , respectively. We\nexpect the numbers to provide complementary views for us to\nunderstand the studied models.\n4\nLinear\nNorm\nMHSA\nNorm\nLinear\nLinear\nDeit in transformer view\nL x\nclassifier\n…\nConv,16x16,s16\nNorm\nMHSA\nNorm\nConv,1x1\nConv,1x1\nDeit in convolution view\nL x\nclassifier\nConv,2x2,s2\nMHSA cell\nFF cell\nL/3 x\nClassifier\nConv,2x2,s2\nMHSA cell\nFF cell\nL/3 x\nConv,4x4,s4\nMHSA cell\nFF cell\nL/3 x\nConv,2x2,s2\nFF cell\nFF cell\nL1 x\nClassifier\nConv,2x2,s2\nFF cell\nFF cell\nL2 x\nFF cell\nFF cell\nL3 x\nConv,7x7,s2\n…\nConv,4x4,s4\nConv,7x7,s2\n3x3 Convolution\nGlobal Self-attention\nNorm\n384,1x1,1536\n1536,1x1, 384\nNorm\n384,1x1,320\n320,1x1, 384\n320,3x3,320\nMLP Bottleneck\nFig. 1. The transition process that starts with DeiT and ends with ResNet-50. To save space, we only show three important movements. The ﬁrst movement\nconverts DeiT from the Transformer to convolution view (Section III-C1). The second movement replaces the patch ﬂattening module with step-wise patch\nembedding (elaborated in Section III-C2) and introduces the stage-wise design (Section III-C3) . The third movement replaces the self-attention module with\nconvolution (Section III-C7). The upper-right area shows a relatively minor modiﬁcations, inserting 3 ×3 convolution (Section III-C5). The lower-right area\ncompares the receptive ﬁelds of a 3 ×3 convolution and self-attention. This ﬁgure is best viewed in color.\nC. The transition from DeiT-S to ResNet-50\nThis subsection displays a step-by-step process in which we\ngradually transit a model from DeiT-S to ResNet-50. There are\neight steps in total. The key steps are illustrated in Figure 1,\nand the results, including the base and elite performance and\nthe model statistics, are summarized in Table II.\n1) Using global average pooling to replace the classiﬁ-\ncation token: The ﬁrst step of the transition is to remove\nthe classiﬁcation token and add global average pooling to\nthe Transformer-based models. Unlike the convolution-based\nmodels, Transformers usually add a classiﬁcation token to the\ninputs and utilize the corresponding output token to perform\nclassiﬁcation, which is inherited from NLP tasks [6]. As\na contrast, the classiﬁcation features in convolution-based\nmodels are obtained by conducting global average pooling in\nthe space dimension.\nBy removing the classiﬁcation token, the Transformer can\nbe equivalent translated to the convolutional version as shown\nin Figure 1. Speciﬁcally, the patch embedding operation is\nequivalent to a convolution whose kernel size and stride is the\npatch size [8]. The shape of the intermediate features can be\nnaturally converted from a sequence of tokens ( i.e., words) to\na bundle feature maps and the tokens become the vector in\nchannel dimension (illustrated in Figure 1). The linear layers\nin MHSA and MLP blocks are equivalent to1×1 convolutions.\nThe performance of the obtained network (Net1) is shown\nin Table II. As can be seen, this transition can substantially im-\nprove the base performance. Our further experiments show that\nadding global pooling itself can improve the base performance\nfrom 64.17% to 69.44%. In other words, the global average\npooling operation which is widely used in convolution-based\nmodels since NIN [51], enables the network to learn more\nefﬁciently under moderate augmentation. Furthermore, this\ntransition can slightly improve the elite performance.\n2) Replacing patch ﬂattening with step-wise patch embed-\nding: DeiT and ViT models directly encode the image pixels\nwith a patch embedding layer which is equivalent to a convolu-\ntion with a large kernel size and stride (e.g., 16). This operation\nﬂattens the image patches to a sequence of tokens so that\nTransformers can handle images. However, patch ﬂattening\nimpairs the position information within each patch and makes\nit more difﬁcult to extract the patterns within patches. To solve\nthis problem, existing methods usually attach a preprocessing\nmodule before patch embedding. The preprocessing module\ncan be a feature extraction convnet [8] or a specially designed\nTransformer [52].\nWe found that there is a rather simple solution, which is\nfactorizing the large patch embedding to step-wise small patch\nembeddings. Speciﬁcally, We ﬁrst add the stem layer in ResNet\nto the Transformer, which is a 7 ×7 convolution layer with a\nstride of two. The stem layer can be seen as a 2 ×2 patching\nembedding operation with pixel overlap (i.e., 7×7 kernel size).\nSince the patch size in the original DeiT model is 16, we still\nneed to embed 8×8 patches after the stem. We further factorize\nthe 8 ×8 patch embedding to a 4 ×4 embedding and a 2 ×2\nembedding, which are 4×4 and 2×2 convolution layers with\nstride 4 and 2 in the perspective of convolution. Additionally,\nwe add an extra 2 ×2 convolution to further upgrade the\npatch size from 16×16 to 32×32 before classiﬁcation. These\npatch embedding layers can also be seen as the down-sampling\nlayers and we double the channel numbers after embedding\nfollowing the practice in convolution-based models.\nBy utilizing step-wise embeddings, the position prior within\npatches is encoded into features. As a result, the model can\n5\nTABLE II\nTHE CLASSIFICATION ACCURACY ON IMAGE NET DURING THE TRANSITION PROCEDURE FROM DEIT-S TO RESNET-50. B OTH THE BASE SETTING AND\nTHE ELITE SETTING ARE CONSIDERED (FOR THE DETAILS , SEE SECTION III-B), AND WE MARK THE POSITIVE MODIFICATIONS IN RED AND THE\nNEGATIVE MODIFICATIONS IN BLUE . NOTE THAT A MODIFICATION CAN IMPACT THE BASE AND ELITE PERFORMANCE DIFFERENTLY . THOUGH THE\nNUMBER OF PARAMETERS INCREASES CONSIDERABLY AT THE INTERMEDIATE STATUS , THE COMPUTATIONAL COSTS MEASURED BY FLOP S DOES NOT\nCHANGE SIGNIFICANTLY .\nModel Name added removed base perf. elite perf. FLOPs (G) Params (M)\nDeiT-S – 64.17 80.07 4.60 22.1\nNet1 global average pooling classiﬁcation token 69.81 (+5.64) 80.16 (+0.09) 4.57 22.0\nNet2 step-wise embeddings large patch embedding 73.01 (+3.20) 81.35 (+1.19) 4.77 23.9\nNet3 stage-wise design – 75.76 (+2.75) 80.19 (-1.14) 4.79 39.5\nNet4 batch norm layer norm 76.49 (+0.73) 80.97 (+0.78) 4.79 39.5\nNet5 3 ×3 convolution – 77.37 (+0.88) 80.15 (-0.82) 4.76 39.2\nNet6 – position embedding 77.31 (-0.06) 79.86 (-0.29) 4.76 39.0\nNet7 convolution self-attention 76.24 (-1.07) 79.01 (-0.85) 4.83 45.0\nResNet-50 network shape adjustment 77.43 (+1.19) 78.73 (-0.28) 4.09 25.6\nlearn patterns more efﬁciently. As can be seen in Table II,\nthis transition can signiﬁcantly improve the base performance\nand elite performance of the network. It indicates that step-\nwise embedding is a better choice than larger patch embedding\nin Transformer-based models. Additionally, this transition is\ncomputationally efﬁcient and only introduces about 4% extra\nFLOPs.\n3) Stage-wise design: In this section, we split networks into\nstages like ResNets. The blocks in the same stage share the\nsame feature resolution. Since step-wise embeddings in the\nlast transition have split the network into different stages, the\ntransition in this section is to reassign the blocks to different\nstages as shown in Figure 1. However, unlike convolution\nblocks, the complexity of self-attention blocks increases by\nO\n(\nN4)\nwith respect to the feature size. Thus we only insert\nblocks to the 8 ×8, 16 ×16 and 32 ×32 patch embedding\nstages, which correspond to 28×28, 14×14 and 7×7 feature\nresolutions respectively for 224×224 inputs. Additionally, we\nhalve the head dimension and feature dimension before self-\nattention in 28×28 stage to ensure that the blocks in different\nstages utilize similar FLOPs.\nThis transition leads to interesting results. The base perfor-\nmance is further improved. It is conjectured that the stage-\nwise design leverages the image local priors and thus can\nperform better under moderate augmentation. However, the\nelite performance of the network decreases markedly. To study\nreasons, we conduct ablation experiments and ﬁnd that self-\nattention does not work well in very large resolutions. We\nconjecture that large resolution contains too many tokens and\nit is much more difﬁcult for self-attention to learn relations\namong them. We will detail it in section III-D.\n4) Replacing LayerNorm with BatchNorm: Transformer-\nbased models usually normalize the features with Layer-\nNorm [15], which is inherited from NLP tasks [5], [6]. As\na contrast, convolution-based models like ResNets usually\nutilize BatchNorm [14] to stabilize the training process. Lay-\nerNorm is independent of batch size and more friendly for\nspeciﬁc tasks compared with BatchNorm, while BatchNorm\nusually can achieve better performance given appropriate\nbatch size [53]. We replace all the LayerNorm layers with\nBatchNorm layers and the results show that BatchNorm per-\nforms better than LayerNorm. It can improve both the base\nperformance and elite performance of the network.\nIn addition, we also try to add BatchNorm to Net2 to\nfurther improve the elite performance. However, this Net2-BN\nnetwork suffers from convergence problems. This may explain\nwhy BatchNorm is not widely used in the pure self-attention\nmodels. But for our mixed model, BatchNorm is a reliable\nmethod to advance performance.\n5) Introducing 3 ×3 convolutions: Since the tokens of\nthe network are present as feature maps, it is natural to\nintroduce convolutions with kernel sizes larger than 1 ×1.\nThe speciﬁc meaning of large kernel convolution is illus-\ntrated at the bottom right of Figure 1. When global self-\nattentions attempt to build the relations among all the tokens\n(i.e., pixels), convolutions focus on relating the tokens within\nlocal neighborhoods. We chose to insert 3 ×3 convolutions\nbetween the 1 ×1 convolutions in feed-forward blocks, which\ntransforms the MLP blocks into bottleneck blocks as exhibited\nat the top right of Figure 1. Note that the channel numbers\nof the 3 ×3 convolution layers are tuned to ensure that the\nFLOPs of the feed-forward blocks are nearly unchanged. The\nobtained bottleneck blocks are similar to the bottleneck blocks\nin ResNet-50, although they have different bottleneck ratios\n(i.e., the factor of reducing the channel numbers before the\n3×3 convolution). We replace the MLP blocks with bottleneck\nblocks in all three stages.\nNot surprisingly, 3 ×3 convolutions which can leverage the\nlocal priors in images further improve the network base perfor-\nmance. The base performance (77.37%) becomes comparable\nwith ResNet-50 (77.43%). However, the elite performance\ndecreases by 0.82%. We conduct more experiments to study\nthe reasons. Instead of adding 3×3 convolutions to all stages,\nwe insert 3 ×3 convolutions to different stages separately.\nWe observe that 3 ×3 convolutions only work well on\nthe high-resolution features. We conjecture that leveraging\nlocal relations is important for the high-resolution features in\nnatural images. For the low-resolution features, however, local\nconvolutions become unimportant when equipped with global\nself-attention. We will detail it in section III-D.\n6) Removing position embedding: In Transformer-based\nmodels, position embedding is proposed to encode the position\ninformation inter tokens. In the transition network, we utilize\nlearnable position embedding as in [6] and add them to\n6\nfeatures after patch embeddings. To approaching ResNet-50,\nposition embedding should be removed.\nThe results are exhibited in Table II. The base perfor-\nmance is almost unchanged and the elite performance declines\nslightly (0.29%). As a comparison, We test to remove the\nposition embedding of DeiT-S and elite performance decreases\nsigniﬁcantly by 3.95%. It reveals that position embedding is\nless important in the transition model than that in the pure\nTransformer-based models. It is because that the position\nprior inter tokens is preserved by the feature maps and\nconvolutions with spatial kernels can encode and leverage it.\nConsequently, the harm of removing position embedding is\nremarkably reduced in the transition network. It also explains\nwhy convolution-based models do not need position embed-\nding.\n7) Replacing self-attention with feed-forward: In this sec-\ntion, we remove the self-attention blocks in each stage and\nutilize a feed-forward layer instead, so that the network\nbecomes a pure convolution-based network. To keep the\nFLOPs unchanged, several bottleneck blocks are added to each\nstage. After the replacement, the obtained network consists of\nbottleneck blocks like ResNet-50.\nThe performance of the obtained network (Net7) is shown\nin Table II. The pure convolution-based network performs\nmuch worse both in base performance and elite performance.\nIt indicates that self-attentions do drive neural networks to\nhigher elite performance and is not responsible for the poor\nbase performance in ViT or DeiT. It is possible to design a\nself-attention network with high base performance and elite\nperformance.\n8) Adjusting the shape of network: There are still many\ndifferences between Net7 and ResNet-50. First, the shape\nof Net7 is different from ResNet-50. Their depths, widths,\nbottleneck ratios and block numbers in network stages are\ndifferent. Second, they normalize the features in different\npositions. Net7 only normalizes input features in a block,\nwhile ResNet-50 normalizes features after each convolutional\nlayer. Third, ResNet-50 down-samples the features with bot-\ntleneck blocks but Net7 utilizes a single convolution layer\n(i.e., patch embedding layer). In addition, Net7 employs a\nfew more FLOPs. Nevertheless, both these two networks are\nconvolution-based networks. The performance gap between\nthese two networks can be attributed to architecture design\nstrategy.\nAs shown in Table II, the base performance is improved\nafter transition. It demonstrates that ResNet-50 has better net-\nwork architecture and can perform better with fewer FLOPs.\nHowever, ResNet-50 obtains worse elite performance. It indi-\ncates that the inconsistencies between base performance and\nelite performance exist not only in self-attention models but\nalso in pure convolution-based networks.\nD. Summary: the Visformer model\nWe aim to build a network with high base performance\nand elite performance. The transition study has shown that\nthere are some inconsistencies between base performance and\nelite performance. The ﬁrst problem is the stage-wise design,\nTABLE III\nIMPACT OF REPLACING THE SELF -ATTENTION BLOCKS WITH THE\nBOTTLENECK BLOCKS IN EACH STAGE OF NET5. T HESE EXPERIMENTS\nARE PERFORMED INDIVIDUALLY .\nNetwork base perf.(%) elite perf.(%)\nNet5 77.37 80.15\nNet5-DS1 77.29 (-0.08) 80.13 (-0.02)\nNet5-DS2 77.34 (-0.02) 79.75 (-0.40)\nNet5-DS3 77.05 (-0.32) 79.59 (-0.56)\nTABLE IV\nIMPACT OF REPLACING THE MLP LAYERS WITH THE BOTTLENECK\nBLOCKS IN EACH STAGE OF NET4. T HESE EXPERIMENTS ARE PERFORMED\nINDIVIDUALLY .\nNetwork base perf.(%) elite perf.(%)\nNet4 76.49 80.97\nNet4-S1 77.02 (+0.53) 81.10 (+0.13)\nNet4-S2 76.55 (+0.06) 80.50 (-0.47)\nNet4-S3 76.82 (+0.33) 80.44 (-0.53)\nNet5 77.37 (+0.88) 80.15 (-0.82)\nwhich increases the base performance but decreases the elite\nperformance. Stage-wise design re-arrange the blocks from\none stage to three stages. Thus, for elite performance, some\nblocks in the new two stages must work less efﬁciently than\nthose in the original stage. We replace the self-attention blocks\nwith bottleneck blocks in each stage separately for Net5,\nby which we can estimate the importance of self-attention\nin different stages. The results are shown in Table III. The\nreplacement of self-attention in all three stages reduces both\nthe base performance and the elite performance. There is\na trend that self-attentions in lower resolutions play more\nimportant roles than those in higher resolutions. Additionally,\nreplacing the self-attentions in the ﬁrst stage almost has no\neffect on the network performance. Larger resolutions contain\nmuch more tokens and we conjecture that it is more difﬁcult\nfor self-attentions to learn relations among them.\nThe second problem is adding 3 ×3 convolutions to the\nfeed-forward blocks, which decreases the elite performance\nby 0.82%. Based on Net4, we replace MLP blocks with\nbottleneck blocks in each stage separately. As can be seen\nin Table III-D, although all stages obtain improvements in\nbase performance, only the ﬁrst stage beneﬁts from bottleneck\nblocks in elite performance. The 3 ×3 convolutions are not\nnecessary for the other two low-resolution stages when self-\nattentions already have a global view in these positions. On the\nhigh-resolution stage, for which self-attentions have difﬁculty\nin handling all tokens, the 3 ×3 convolutions can provide\nimprovement.\nIntegrating the observation above, we propose the Vis-\nformer as vision-friendly, Transformer-based models. The\ndetailed architectures are shown in Table V. Besides the\npositive transitions, Visformer adopts the stage-wise design for\nhigher base performance. But self-attentions are only utilized\nin the last two stages, considered that self-attention in the high-\nresolution stage is relatively inefﬁcient. Visformer employs\nbottleneck blocks in the ﬁrst stage and utilizes group 3 ×3\nconvolutions in bottleneck blocks inspired by ResNeXt [54].\n7\nTABLE V\nTHE CONFIGURATION FOR CONSTRUCTING THE VISFORMER -TI AND VISFORMER -S MODELS , WHERE ‘EMB .’STANDS FOR FEATURE EMBEDDING , AND\n‘S0’–‘S3’ INDICATE THE FOUR STAGES WITH DIFFERENT SPATIAL RESOLUTIONS .\noutput size Visformer-Ti Visformer-S VisformerV2-Ti VisformerV2-S\nstem 112 ×112 7 ×7, 16, stride 2 7 ×7, 32, stride 2 7 ×7, 24, stride 2 7 ×7, 32, stride 2\nemb. 56 ×56 - - 2 ×2, 48, stride 2 2 ×2, 64, stride 2\ns0 56 ×56 - -\n\n\n1 ×1, 96\n3 ×3, 96\n(group = 8)\n1 ×1, 48\n\n ×1\n\n\n1 ×1, 128\n3 ×3, 128\n(group = 8)\n1 ×1, 64\n\n ×1\nemb. 28 ×28 4 ×4, 96, stride 4 4 ×4, 192, stride 4 2 ×2, 96, stride 2 2 ×2, 128, stride 2\ns1 28 ×28\n\n\n1 ×1, 192\n3 ×3, 192\n(group = 8)\n1 ×1, 96\n\n ×7\n\n\n1 ×1, 384\n3 ×3, 384\n(group = 8)\n1 ×1, 192\n\n ×7\n\n\n1 ×1, 192\n3 ×3, 192\n(group = 8)\n1 ×1, 96\n\n ×4\n\n\n1 ×1, 256\n3 ×3, 256\n(group = 8)\n1 ×1, 128\n\n ×10\nemb. 14 ×14 2 ×2, 192, stride 2 2 ×2, 384, stride 2\ns2 14 ×14\n\n\nMHSA, 192\n1 ×1, 768\n1 ×1, 192\n\n ×4\n\n\nMHSA, 384\n1 ×1, 1536\n1 ×1, 384\n\n ×4\n\n\nMHSA, 192\n1 ×1, 768\n1 ×1, 192\n\n ×6\n\n\nMHSA, 256\n1 ×1, 1024\n1 ×1, 256\n\n ×14\nemb. 7 ×7 2 ×2, 384, stride 2 2 ×2, 768, stride 2\ns3 7 ×7\n\n\nMHSA, 384\n1 ×1, 1536\n1 ×1, 384\n\n ×4\n\n\nMHSA, 768\n1 ×1, 3072\n1 ×1, 768\n\n ×4\n\n\nMHSA, 384\n1 ×1, 1536\n1 ×1, 384\n\n ×2\n\n\nMHSA, 512\n1 ×1, 2048\n1 ×1, 512\n\n ×3\n1 ×1 global average pool, 1000-d fc, softmax\nFLOPs 1.3 ×109 4.9 ×109 1.3 ×109 4.3 ×109\nWe also introduce BatchNorm to patch embedding modules\nas in CNNs. We name Visformer-S to denote the model that\ndirectly comes from DeiT-S. In addition, we can adjust the\ncomplexity by changing the output dimensionality of multi-\nhead attentions. Here, we shrink the dimensionality by half\nand derive the Visformer-Ti model, which requires around 1/4\ncomputational costs of the Visformer-S model.\nE. VisformerV2: optimizing the architecture conﬁguration\nSome architecture conﬁgurations of Visformer are not care-\nfully tuned. For example, when splitting the network into\ndifferent stages, we averagely assign the 12 blocks to the\nthree stages, expect that we utilize 3 more blocks in the\nﬁrst stage to compensate for the removal of self-attention. In\nother words, the stage conﬁguration ([7, 4, 4]) is not carefully\ndesigned. Furthermore, depth and width, which are also not\npolished in Visformer, have been demonstrated to be very\nimportant conﬁgurations for network performance. Therefore,\nwe conduct many experiments to explore the architecture of\nVisformer and propose VisformerV2. VisformerV2 is much\nbetter than the original Visformer and the architecture is shown\nin Table V. The detailed analysis and experiments are shown\nin Section IV-C.\nF . Transformer with Half-precision\nRecently, quantization has been widely used to accelerate\nthe training process and save GPU memory. Speciﬁcally, half-\nprecision ﬂoating-point (FP16), the lowest precision that can\npreserve the network performance, has been adopted by many\nresearchers. However, some works [10], [55], [56] have shown\nthat half-precision can lead to overﬂows in Transformers and\nwe also observe this problem in Visformer.\nBased on our experimental analysis, we ﬁnd that attention\nscore generation can cause overﬂow. With the queries ( Q) and\nkeys (K), the standard self-attention scores can be computed\nas:\nAscore = softmax(QKT\n√\nd\n) (1)\nHowever, Q and K can be very large matrices and the elements\nin QKT will be the dot-product of two very long vectors. As\na result, the scores can overﬂow easily while utilizing 16-bit\nprecision. To solve this problem, we ﬁrst try to pre-normalize\nQ and K:\nAscore = softmax(( Q\n4√\nd\n)(KT\n4√\nd\n)) (2)\nWhere d is the length of the vector. Nevertheless, the attention\nscores still overﬂow sometimes. This is because the scores are\nonly normalized with\n√\nd overall. As the dot product of two\nvectors with length d, the scores are still under the risk of\noverﬂow. Consequently, we try to normalize the score with d:\nAscore = softmax((Q√\nd\n)(KT\n√\nd\n)) (3)\nTABLE VI\nCOMPARISON OF INFERENCE TIME FOR VISFORMER V2-S WITH\nDIFFERENT SCORE GENERATING METHODS . THE TESTED GPU IS V100\nAND THE BATCH SIZE IS 32.\nMethod original PB-Relax ours\nBatch Time (ms) 42.8 46.6 43.0\n8\nTABLE VII\nTHE COMPARISON OF BASE AND ELITE PERFORMANCE AS WELL AS THE\nFLOP S BETWEEN VISFORMER AND DEIT, THE DIRECT BASELINE .\nNetwork base perf.\n(%)\nelite perf.\n(%)\nFLOPs\n(G)\nVisformer-Ti 74.34 78.62 1.3\nDeiT-Ti 63.87 72.21 1.3\nVisformer-S 77.20 82.19 4.9\nDeiT-S 63.12 80.07 4.6\nIn our experiments, we observed that it can effectively avoid\noverﬂow during computing scores and will not degrade the\nnetwork performance.\nNote that CogView [55] also proposes PB-Relax to elim-\ninate overﬂow in attention scores. PB-Relax pre-minuses the\nmaximum of the attention scores. However, this method needs\nto tune a hyper-parameter and usually considerably increases\nthe network runtime, as shown in Table VI. As a contrast, our\nmethod nearly does not introduce extra runtime.\nIV. M ORE EXPERIMENTS ON VISFORMER\nA. The improvements on the upper-bound and lower-bound\nWe ﬁrst compare Visformer against DeiT, the direct base-\nline. Results are summarized in Table VII. Using comparable\ncomputational costs, the Visformer models outperform the cor-\nresponding DeiT models signiﬁcantly. Speciﬁcally, the advan-\ntages of Visformer-S and Visformer-Ti over DeiT-S and DeiT-\nTi under the elite setting are 2.12% and 6.41%, while under\nthe base setting, the numbers grow to 14.08% and 10.47%,\nrespectively. In other words, the advantage becomes more\nsigniﬁcant under the base setting, which is more frequently\nused for visual recognition.\nB. Training with limited data\nWe evaluate the performance of Visformer in the scenario\nwith limited training data, which we consider is an important\nability of being vision-friendly, while prior Transformer-based\nmodels mostly required abundant training data [8].\nFour subsets of ImageNet are used, with 10% and 1%\nrandomly chosen classes (all data), and with 10% and 1%\nrandomly chosen images (all classes), respectively. To chal-\nlenge the models, we still use the elite setting with 300 epochs\n(not extended). As shown in Table VIII, it is observed that\nthe DeiT-S model reports dramatic accuracy drops in all the\nfour tests (note that the accuracy of using only 10% and\n1% classes should be much higher if epochs are extended).\nIn comparison, Visformer remains robust in these scenarios,\nshowing its potential of being used for visual recognition with\nlimited data.\nIn tiny level, ResNet-50-55% is obtained by reducing the\nchannel numbers (like other tiny models) to 55% (so that the\nFLOPs, 1.3G, is similar to Visformer-Ti and Deit-Ti). The\nconclusion is similar: Visformer-Ti is still the best overall\nmodel, and the advantage is slightly enlarged because the risk\nof over-ﬁtting has been reduced.\nTABLE VIII\nCOMPARISON AMONG VISFORMER , DEIT, AND RESNET, IN TERMS OF\nCLASSIFICATION ACCURACY (%) USING LIMITED TRAINING DATA . THE\nELITE SETTING WITH 300 EPOCHS IS USED FOR ALL MODELS .\nNetwork 100%\nclasses\n10%\nclasses\n1%\nclasses\n10%\nimages\n1%\nimages\nDeiT-S 80.07 80.06 73.40 40.41 6.94\nResNet-50 78.73 89.90 93.20 58.37 13.59\nVisformer-S 82.19 90.06 91.60 58.74 16.56\nDeit-Ti 72.33 78.72 74.40 38.44 6.53\nResNet-50-55% 72.84 87.10 91.40 51.48 10.68\nVisformer-Ti 78.62 89.48 90.60 55.14 11.79\nC. Designing VisformerV2\nWe design VisformerV2 by polishing the original Visformer.\nFirst, we apply relative position bias [42] to Visformer, which\nimproves the results to 82.39% as shown in Table IV-C. Then\nwe test whether we need to utilize an extra early stage. As\nshown in Table IX, assigning a block to the new stage does not\nimprove the performance and furthermore, the performance\ndecreases when more blocks are assigned to it. However, we\nﬁnd that this stage can improve the detection and segmentation\nresults, which will be detailed in Section IV-F. Therefore,\nwe decide to utilize one block in the new stage. Next, we\ntest to utilize deep and narrow architecture [43]. We ﬁrst\nnarrow down the network and directly assign blocks to the\nself-attention stages (the last two stages). The tested stage\nconﬁgurations are {1, 3, 11, 11 }and {1, 6, 10, 10 }. These\nsettings degrade the performance. Then we try to assign more\nblocks to the third stage ( i.e., {1, 3, 18, 3 }and {1, 6, 16,\n3}), which is the default setting for many convolution [4] and\nTransformer networks [42], [43]. It improves the networks\nsigniﬁcantly. We also ﬁnd that the second pure convolution\nstage is very important. Moving the blocks from this stage\nto the other stages will substantially degrade the network.\nTherefore we assign more blocks to this stage and obtain\nVisformerV2-S. With a similar study, we design VisformerV2-\nTi. The detailed architecture is shown in Table V.\nNote that the deep and narrow architecture signiﬁcantly\nincreases the runtime on GPU. This is because that the wide\nand shallow architecture has a better parallelization property.\nTo compensate for the loss in runtime, we utilize fewer\nFLOPs for ‘deep-narrow’ networks. More importantly, we\nﬁnd that when the input resolution is enlarged (detection and\nsegmentation tasks in Section IV-F) or the model is scaled up,\nthe parallelization property will be improved and the runtime\non GPU becomes more consistent with FLOPs.\nD. Comparison to the state-of-the-arts\nWe then compare Visformer and VisformerV2 to other\nTransformer-based approaches in Table X. At the tiny level,\nVisformer-Ti and VisformerV2-Ti outperform other vision\nTransformers that with similar FLOPs. For larger models,\nVisformer-S performs much better than most of the models\nwith similar FLOPs. VisformerV2-S further improves the per-\nformance and outperforms other vision Transformer models.\nNote that VisformerV2-S utilize fewer FLOPs and parameters\nthan Visformer-S.\n9\nTABLE IX\nTHE ELITE PERFORMANCE AND INFERENCE TIME OF DIFFERENT VISFORMER MODELS . THE BATCH TIME IS TESTED ON A V100 GPU WITH A BATCH\nSIZE OF 32.\nNetwork block numbers channel numbers elite perf.(%) FLOPs (G) Params (M) Batch Time (ms)\nVisformer-S {0, 7, 4, 4 } {96, 192, 384, 768 } 82.39 4.9 40.2 36.9\n{1, 6, 4, 4 } {96, 192, 384, 768 } 82.37 4.9 40.3 37.3\n{3, 4, 4, 4 } 81.70 4.9 39.3 38.4\n{1, 3, 11, 11 }\n{64, 128, 256, 512 }\n81.73 4.2 45.4 42.1\n{1, 6, 10, 10 } 82.20 4.2 41.9 41.7\n{1, 3, 18, 3 } 82.51 4.2 25.8 43.4\n{1, 6, 16, 3 } 82.89 4.2 24.6 42.8\nVisformerV2-S {1, 10, 14, 3 } {64, 128, 256, 512 } 82.97 4.3 23.6 43.0\nTABLE X\nCOMPARISON AMONG OUR METHOD AND OTHER TRANSFORMER -BASED\nVISION MODELS . ‘*’INDICATES THAT WE RE -RUN THE MODEL USING THE\nELITE SETTING . ‘KD’ STANDS FOR KNOWLEDGE DISTILLATION [57].\nMethods Top-1(%) FLOPs\n(G)\nParams\n(M)\nResNet-18 [4] 69.8 1.8 11.7\nDeiT-Ti [10] 72.2 1.3 5.7\nDeiT-Ti (KD) [10] 74.6 1.3 5.7\nAutoFormer-Ti [35] 74.7 1.3 5.7\nPVT-Ti [22] 75.1 1.9 13.2\nPVTv2-B1 [58] 78.7 2.1 13.1\nVisformer-Ti (ours) 78.6 1.3 10.3\nVisformerV2-Ti (ours) 79.6 1.3 9.4\nResNet-50 [4] 76.2 4.1 25.6\nResNet-50∗ [4] 78.7 4.1 25.6\nRegNetY-4GF [59] 79.4 4.0 20.6\nRegNetY-8GF [59] 79.9 8.0 39.2\nRegNetY-4GF∗ [59] 80.0 4.0 20.6\nDeiT-S [10] 79.8 4.6 21.8\nDeiT-S∗ [10] 80.1 4.6 21.8\nDeiT-B [10] 81.8 17.4 86.3\nPVT-S [22] 79.8 3.8 24.5\nPVT-Medium [22] 81.2 6.7 44.2\nPVTv2-B2-Li [58] 82.1 3.9 22.6\nPVTv2-B2 [58] 82.0 4.0 25.4\nSwin-T [42] 81.3 4.5 29\nCvT-13 [32] 81.6 4.5 20\nCvT-13-NAS [32] 82.2 4.1 18\nCvT-13(384) [32] 83.0 16.3 20\nConformer-Ti [31] 81.3 5.2 23.5\nT2T-ViTt-14 [52] 80.7 5.2 21.5\nT2T-ViTt-19 [52] 81.4 8.4 39.0\nBoTNet-S1-59 [29] 81.7 7.3 33.5\nCSWin-T [43] 82.7 4.3 23\nAutoFormer-S [35] 81.7 5.1 22.9\nVisformer-S (ours) 82.2 4.9 40.2\nVisformerV2-S (ours) 83.0 4.3 23.6\nE. Inference efﬁciency\nAlthough VisformerV2-S is not as efﬁcient as Visformer-S\nin runtime, it is still much faster than most vision Transformer\nmodels as shown in Table XI. As for the state-of-the-art\nEfﬁcientNet convnets, Visformer-S are below the EfﬁcientNets\nwith similar FLOPs. However, EfﬁcientNets are computing in-\nefﬁcient on GPUs. It is shown that Visformer-S is signiﬁcantly\nfaster than EfﬁcientNet-B3 which performance is slightly\nworse than our model. VisformerV2-S and EfﬁcientNet-B4\nhave similar FLOPs and performance, but VisformerV2-S is\nsigniﬁcantly faster than EfﬁcientNet-B4.\nTABLE XI\nCOMPARISON OF INFERENCE EFFICIENCY AMONG VISFORMER AND\nOTHER MODELS ON A 32G-V100. A BATCH SIZE OF 32 IS USED FOR\nTESTING . ‘*’INDICATES THAT THE MODEL IS RE -TRAINED WITH THE\nELITE SETTING .\nMethods Top-1\n(%)\nFLOPs\n(G)\nBatch Time\n(ms)\nResNet-50∗ 78.7 4.1 34.2\nDeiT-S∗ 80.1 4.6 36.9\nRegNetY-4GF∗ 80.0 4.0 40.2\nSwin-T 81.3 4.5 47.6\nCSwin-T 82.7 4.3 57.5\nPVT-S 79.8 3.8 47.6\nPVTv2-B2 82.0 4.0 57.1\nPVTv2-B2-Li 82.1 3.9 56.8\nEfﬁcientNet-B3 [60] 81.6 1.8 48.3\nEfﬁcientNet-B4 [60] 82.9 4.2 81.7\nVisformer-S (ours) 82.2 4.9 36.7\nVisformerV2-S (ours) 83.0 4.3 43.0\nF . COCO Object Detection\nLast but not least, we evaluate our models on the COCO\nobject detection task. Since the standard self-attention in\nVisformer models is not efﬁcient for high-resolution inputs,\nwe simply replace self-attention with the shifted window\n(Swin) self-attention [42] to apply our models to the detection\ntask. Therefore, Swin Transformers are our important baseline\nmodels. It should be emphasized that the self-attention in Vis-\nformer can also be replaced with other resolution-friendly self-\nattentions like CSWin self-attention and MSG self-attention.\nWe just utilize the widely used Swin self-attention to show\nthe superiority of Visformer architecture.\nThe models are evaluated with two frameworks: Mask-\nRCNN [61] and Cascade Mask-RCNN [62]. We train the\nmodels on COCO 2017 dataset and report the results on COCO\nval2017. We inherit the training settings in [42]: the AdamW\noptimizer with a learning rate of 0.0001 and the weight decay\nof 0.05. The batch size is 16 and we show the results of 1×(12\nepochs) and 3×(36 epochs) schedule. The FPS is measured on\na V100 GPU with a batch size of 1. The FLOPs are computed\nwith 1280 ×800 resolution.\nWe ﬁrst test different methods with the Mask R-CNN\n1×schedule. As shown at the top of Table XII, Visformer-\nS slightly outperform Swin-T. When we assign a block to\nthe ﬁrst stage (Visformer-S-F), although the classiﬁcation\nperformance is not improved (illustrated in Section IV-C),\nthe detection result becomes better. We conjecture that the\n10\nMethod Backbone APbox APbox\n50 APbox\n75 APmask APmask\n50 APmask\n75 FLOPs Params FPS\nMask R-CNN\n1×schedule\nR-50 38.0 58.6 41.4 34.4 55.1 36.7 260 44 18.6\nSwin-T 42.6 65.1 46.2 39.3 62.0 42.1 267 48 14.8\nVisformer-S 43.0 65.3 47.2 39.6 62.4 42.4 275 60 13.1\nVisformer-S-F 43.5 65.9 47.7 39.8 62.5 42.6 275 60 13.0\nVisformerV2-S 44.8 66.8 49.4 40.7 63.9 43.7 262 43 15.2\nMask R-CNN\n3×+ MS schedule\nR-50 41.0 61.7 44.9 37.1 58.4 40.1 260 44 18.6\nSwin-T 46.0 68.2 50.2 41.6 65.1 44.8 267 48 14.8\nVisformerV2-S 47.8 69.5 52.6 42.5 66.4 45.8 262 43 15.2\nCascade Mask R-CNN\n1×+ MS schedule\nR-50 43.7 61.7 47.5 38.0 58.8 41.0 739 82 10.6\nSwin-T 48.1 67.1 52.2 41.7 64.4 45.0 745 86 9.5\nVisformerV2-S 49.3 68.1 53.6 42.3 65.1 45.7 740 81 9.6\nCascade Mask R-CNN\n3×+ MS schedule\nR-50 46.3 64.3 50.5 40.1 61.7 43.4 739 82 10.6\nDeiT-S 48.0 67.2 51.7 41.4 64.2 44.3 889 80 -\nSwin-T 50.5 69.3 54.9 43.7 66.6 47.1 745 86 9.5\nMSG-T [44] 50.3 69.0 54.7 43.6 66.5 47.5 758 86 9.5\nPVTv2-B2-Li [58] 50.9 69.5 55.2 - - - 725 80 8.2\nPVTv2-B2 [58] 51.1 69.8 55.3 - - - 788 83 7.1\nVisformerV2-S 51.6 70.1 56.4 44.1 67.5 47.8 740 81 9.6\nTABLE XII\nOBJECT DETECTION AND INSTANCE SEGMENTATION PERFORMANCE ON COCO 2017. T HE FPS IS MEASURED ON A V100 GPU WITH A BATCH SIZE OF\n1. T HE FLOP S ARE COMPUTED WITH 1280 ×800 RESOLUTION . ‘MS’ INDICATES MULTI -SCALE TRAINING [38], [64]\n.\nblock in the ﬁrst stage can help the FPN [63] to explore the\nlow-level features. The VisformerV2-S further improves the\nperformance and outperforms Swin-T by 2.2%. Additionally,\nbecause of the improvement on parallelization property, the\nFPS becomes consistent with FLOPs and VisformerV2-S is\nfaster than Visformer-S.\nFor the Cascade Mask R-CNN framework, VisformerV2-\nS still outperforms Swin-T by a large margin. We compare\nVisformerV2-S with more methods for 3×and multi-scale\nschedule [38], [64], and our model still performs better than\nthe other methods. As for FPS, our method is as efﬁcient as\nSwin-T and MSG-T, and is faster than other vision Trans-\nformer Methods.\nV. C ONCLUSIONS\nThis paper presents Visformer, a Transformer-based model\nthat is friendly to visual recognition. We propose to use two\nprotocols, the base and elite setting, to evaluate the perfor-\nmance of each model. To study the reason why Transformer-\nbased models and convolution-based models behave differ-\nently, we decompose the gap between these models and design\nan eight-step transition procedure that bridges the gap between\nDeiT-S and ResNet-50. By absorbing the advantages and\ndiscarding the disadvantages, we obtain the Visformer-S model\nthat outperforms both DeiT-S and ResNet-50. Visformer also\nshows a promising ability when it is transferred to a compact\nmodel and when it is evaluated on small datasets.\nVI. A CKNOWLEDGMENTS\nThis work was supported by the National Key R&D Program\nof China (2017YFB1301100), National Natural Science Foun-\ndation of China (61772060, U1536107, 61472024, 61572060,\n61976012, 61602024), and the CERNET Innovation Project\n(NGII20160316).\nREFERENCES\n[1] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[2] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.\n[3] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV . Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2015, pp. 1–9.\n[4] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” inProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2016, pp. 770–778.\n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” arXiv preprint\narXiv:1706.03762, 2017.\n[6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[7] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\nlanguage understanding by generative pre-training,” 2018.\n[8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2009, pp. 248–\n255.\n[10] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ´egou, “Training data-efﬁcient image transformers & distillation\nthrough attention,” arXiv preprint arXiv:2012.12877 , 2020.\n[11] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking\nthe inception architecture for computer vision,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition , 2016,\npp. 2818–2826.\n[12] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V . Le, “Randaugment:\nPractical automated data augmentation with a reduced search space,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition Workshops, 2020, pp. 702–703.\n[13] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y . Yoo, “Cutmix: Reg-\nularization strategy to train strong classiﬁers with localizable features,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 6023–6032.\n[14] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” arXiv preprint\narXiv:1502.03167, 2015.\n[15] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n11\n[16] Z. Chen, L. Xie, J. Niu, X. Liu, L. Wei, and Q. Tian, “Visformer:\nThe vision-friendly transformer,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , October 2021,\npp. 589–598.\n[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances in neural infor-\nmation processing systems , 2012, pp. 1097–1105.\n[18] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and\nJ. Shlens, “Stand-alone self-attention in vision models,” arXiv preprint\narXiv:1906.05909, 2019.\n[19] H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image\nrecognition,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision , 2019, pp. 3464–3473.\n[20] H. Zhao, J. Jia, and V . Koltun, “Exploring self-attention for image\nrecognition,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2020, pp. 10 076–10 085.\n[21] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever,\n“Generative pretraining from pixels,” in International Conference on\nMachine Learning. PMLR, 2020, pp. 1691–1703.\n[22] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, “Pyramid vision transformer: A versatile backbone for dense\nprediction without convolutions,” arXiv preprint arXiv:2102.12122 ,\n2021.\n[23] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\nworks,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 7794–7803.\n[24] A. Buades, B. Coll, and J.-M. Morel, “A non-local algorithm for image\ndenoising,” in 2005 IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR’05) , vol. 2. IEEE, 2005, pp.\n60–65.\n[25] Y . Cao, J. Xu, S. Lin, F. Wei, and H. Hu, “Gcnet: Non-local networks\nmeet squeeze-excitation networks and beyond,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision Workshops ,\n2019, pp. 0–0.\n[26] Y . Li, X. Jin, J. Mei, X. Lian, L. Yang, C. Xie, Q. Yu, Y . Zhou, S. Bai,\nand A. L. Yuille, “Neural architecture search for lightweight non-local\nnetworks,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2020, pp. 10 297–10 306.\n[27] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le, “Attention\naugmented convolutional networks,” in Proceedings of the IEEE/CVF\ninternational conference on computer vision , 2019, pp. 3286–3295.\n[28] I. Bello, “Lambdanetworks: Modeling long-range interactions without\nattention,” arXiv preprint arXiv:2102.08602 , 2021.\n[29] A. Srinivas, T.-Y . Lin, N. Parmar, J. Shlens, P. Abbeel, and\nA. Vaswani, “Bottleneck transformers for visual recognition,” arXiv\npreprint arXiv:2101.11605, 2021.\n[30] A. Vaswani, P. Ramachandran, A. Srinivas, N. Parmar, B. Hechtman,\nand J. Shlens, “Scaling local self-attention for parameter efﬁcient visual\nbackbones,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2021, pp. 12 894–12 904.\n[31] Z. Peng, W. Huang, S. Gu, L. Xie, Y . Wang, J. Jiao, and Q. Ye,\n“Conformer: Local features coupling global representations for visual\nrecognition,” arXiv preprint arXiv:2105.03889 , 2021.\n[32] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,\n“Cvt: Introducing convolutions to vision transformers,” arXiv preprint\narXiv:2103.15808, 2021.\n[33] Z. Dai, H. Liu, Q. V . Le, and M. Tan, “Coatnet: Marrying convolution\nand attention for all data sizes,” arXiv preprint arXiv:2106.04803, 2021.\n[34] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. J ´egou, “Go-\ning deeper with image transformers,” arXiv preprint arXiv:2103.17239 ,\n2021.\n[35] M. Chen, H. Peng, J. Fu, and H. Ling, “Autoformer: Searching\ntransformers for visual recognition,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2021, pp. 12 270–12 280.\n[36] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, “Scaling vision\ntransformers,” arXiv preprint arXiv:2106.04560 , 2021.\n[37] A. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, and\nL. Beyer, “How to train your vit? data, augmentation, and regularization\nin vision transformers,” arXiv preprint arXiv:2106.10270 , 2021.\n[38] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean Conference on Computer Vision . Springer, 2020, pp. 213–\n229.\n[39] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille, and\nY . Zhou, “Transunet: Transformers make strong encoders for medical\nimage segmentation,” arXiv preprint arXiv:2102.04306 , 2021.\n[40] H. Chen, Y . Wang, T. Guo, C. Xu, Y . Deng, Z. Liu, S. Ma, C. Xu,\nC. Xu, and W. Gao, “Pre-trained image processing transformer,” arXiv\npreprint arXiv:2012.00364, 2020.\n[41] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll ´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\ncontext,” in European conference on computer vision . Springer, 2014,\npp. 740–755.\n[42] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” arXiv preprint arXiv:2103.14030 , 2021.\n[43] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and\nB. Guo, “Cswin transformer: A general vision transformer backbone\nwith cross-shaped windows,” arXiv preprint arXiv:2107.00652 , 2021.\n[44] J. Fang, L. Xie, X. Wang, X. Zhang, W. Liu, and Q. Tian, “Msg-\ntransformer: Exchanging local spatial information by manipulating mes-\nsenger tokens,” arXiv preprint arXiv:2105.15168 , 2021.\n[45] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large\nscale visual recognition challenge,” International Journal of Computer\nVision, vol. 115, no. 3, pp. 211–252, 2015.\n[46] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, “mixup: Beyond\nempirical risk minimization,” 2017.\n[47] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y . Yang, “Random erasing\ndata augmentation,” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 34, no. 07, 2020, pp. 13 001–13 008.\n[48] M. Berman, H. J ´egou, A. Vedaldi, I. Kokkinos, and M. Douze, “Multi-\ngrain: a uniﬁed image embedding for classes and instances,” arXiv\npreprint arXiv:1902.05509, 2019.\n[49] E. Hoffer, T. Ben-Nun, I. Hubara, N. Giladi, T. Hoeﬂer, and D. Soudry,\n“Augment your batch: Improving generalization through instance repeti-\ntion,” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020, pp. 8129–8138.\n[50] G. Huang, Y . Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, “Deep\nnetworks with stochastic depth,” in European Conference on Computer\nVision. Springer, 2016, pp. 646–661.\n[51] M. Lin, Q. Chen, and S. Yan, “Network in network,” arXiv preprint\narXiv:1312.4400, 2013.\n[52] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, F. E. Tay, J. Feng, and\nS. Yan, “Tokens-to-token vit: Training vision transformers from scratch\non imagenet,” arXiv preprint arXiv:2101.11986 , 2021.\n[53] Y . Wu and K. He, “Group normalization,” in Proceedings of the\nEuropean conference on computer vision (ECCV) , 2018, pp. 3–19.\n[54] S. Xie, R. Girshick, P. Doll ´ar, Z. Tu, and K. He, “Aggregated\nresidual transformations for deep neural networks,” arXiv preprint\narXiv:1611.05431, 2016.\n[55] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou,\nZ. Shao, H. Yang et al., “Cogview: Mastering text-to-image generation\nvia transformers,” arXiv preprint arXiv:2105.13290 , 2021.\n[56] Z. Liu, H. Hu, Y . Lin, Z. Yao, Z. Xie, Y . Wei, J. Ning, Y . Cao,\nZ. Zhang, L. Dong et al. , “Swin transformer v2: Scaling up capacity\nand resolution,” arXiv preprint arXiv:2111.09883 , 2021.\n[57] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural\nnetwork,” arXiv preprint arXiv:1503.02531 , 2015.\n[58] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, “Pvtv2: Improved baselines with pyramid vision transformer,”\narXiv preprint arXiv:2106.13797 , 2021.\n[59] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Doll ´ar,\n“Designing network design spaces,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2020, pp.\n10 428–10 436.\n[60] M. Tan and Q. Le, “Efﬁcientnet: Rethinking model scaling for con-\nvolutional neural networks,” in International Conference on Machine\nLearning. PMLR, 2019, pp. 6105–6114.\n[61] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick, “Mask r-cnn,” in\nInternational Conference on Computer Vision . IEEE, 2017.\n[62] Z. Cai and N. Vasconcelos, “Cascade r-cnn: Delving into high quality\nobject detection,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition , 2018, pp. 6154–6162.\n[63] T.-Y . Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie,\n“Feature pyramid networks for object detection,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2017, pp.\n2117–2125.\n[64] P. Sun, R. Zhang, Y . Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka,\nL. Li, Z. Yuan, C. Wang et al. , “Sparse r-cnn: End-to-end object\ndetection with learnable proposals,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2021, pp.\n14 454–14 463."
}