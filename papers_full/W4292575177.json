{
  "title": "COVID-19 CT image segmentation method based on swin transformer",
  "url": "https://openalex.org/W4292575177",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5050009113",
      "name": "Weiwei Sun",
      "affiliations": [
        "Chongqing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A5063331411",
      "name": "Jungang Chen",
      "affiliations": [
        "Chongqing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A5100342549",
      "name": "Yan Li",
      "affiliations": [
        "Southwest Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A5087882200",
      "name": "Jinzhao Lin",
      "affiliations": [
        "Chongqing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A5101653611",
      "name": "Yu Pang",
      "affiliations": [
        "Chongqing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A5007669044",
      "name": "Zhang Guo",
      "affiliations": [
        "Chongqing University of Posts and Telecommunications",
        "Southwest Medical University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3007497549",
    "https://openalex.org/W2946185430",
    "https://openalex.org/W3010381061",
    "https://openalex.org/W3006882119",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3012882790",
    "https://openalex.org/W3008443627",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2581082771",
    "https://openalex.org/W2905810301",
    "https://openalex.org/W3159778524",
    "https://openalex.org/W6810465815",
    "https://openalex.org/W4220718606",
    "https://openalex.org/W3040660552",
    "https://openalex.org/W6769553228",
    "https://openalex.org/W2109711012",
    "https://openalex.org/W3011149445",
    "https://openalex.org/W2592929672",
    "https://openalex.org/W6792155083",
    "https://openalex.org/W3025948831",
    "https://openalex.org/W2798122215",
    "https://openalex.org/W1905036359",
    "https://openalex.org/W2980965120",
    "https://openalex.org/W6764024669",
    "https://openalex.org/W3014561994",
    "https://openalex.org/W3007273493",
    "https://openalex.org/W3135243128",
    "https://openalex.org/W2908201961",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W4285707640",
    "https://openalex.org/W6775170262",
    "https://openalex.org/W3013130152",
    "https://openalex.org/W6753412334",
    "https://openalex.org/W3007170347",
    "https://openalex.org/W3001897055",
    "https://openalex.org/W3037538421",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3097065222",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W4221163766",
    "https://openalex.org/W2999580839"
  ],
  "abstract": "Owing to its significant contagion and mutation, the new crown pneumonia epidemic has caused more than 520 million infections worldwide and has brought irreversible effects on the society. Computed tomography (CT) images can clearly demonstrate lung lesions of patients. This study used deep learning techniques to assist doctors in the screening and quantitative analysis of this disease. Consequently, this study will help to improve the diagnostic efficiency and reduce the risk of infection. In this study, we propose a new method to improve U-Net for lesion segmentation in the chest CT images of COVID-19 patients. 750 annotated chest CT images of 150 patients diagnosed with COVID-19 were selected to classify, identify, and segment the background area, lung area, ground glass opacity, and lung parenchyma. First, to address the problem of a loss of lesion detail during down sampling, we replaced part of the convolution operation with atrous convolution in the encoder structure of the segmentation network and employed convolutional block attention module (CBAM) to enhance the weighting of important feature information. Second, the Swin Transformer structure is introduced in the last layer of the encoder to reduce the number of parameters and improve network performance. We used the CC-CCII lesion segmentation dataset for training and validation of the model effectiveness. The results of ablation experiments demonstrate that this method achieved significant performance gain, in which the mean pixel accuracy is 87.62%, mean intersection over union is 80.6%, and dice similarity coefficient is 88.27%. Further, we verified that this model achieved superior performance in comparison to other models. Thus, the method proposed herein can better assist doctors in evaluating and analyzing the condition of COVID-19 patients.",
  "full_text": "COVID-19 CT image\nsegmentation method based on\nswin transformer\nWeiwei Sun1, Jungang Chen1, Li Yan2, Jinzhao Lin1, Yu Pang1*\nand Guo Zhang1,2*\n1Chongqing University of Posts and Telecommunication, Chongqing, China,2School of Medical\nInformation and Engineering, Southwest Medical University, Luzhou, China\nOwing to its signiﬁcant contagion and mutation, the new crown pneumonia\nepidemic has caused more than 520 million infections worldwide and has\nbrought irreversible effects on the society. Computed tomography (CT) images\ncan clearly demonstrate lung lesions of patients. This study used deep learning\ntechniques to assist doctors in the screening and quantitative analysis of this\ndisease. Consequently, this study will help to improve the diagnostic efﬁciency\nand reduce the risk of infection. In this study, we propose a new method to\nimprove U-Net for lesion segmentation in the chest CT images of COVID-19\npatients. 750 annotated chest CT images of 150 patients diagnosed with\nCOVID-19 were selected to classify, identify, and segment the background\narea, lung area, ground glass opacity, and lung parenchyma. First, to address the\nproblem of a loss of lesion detail during down sampling, we replaced part of the\nconvolution operation with atrous convolution in the encoder structure of the\nsegmentation network and employed convolutional block attention module\n(CBAM) to enhance the weighting of important feature information. Second, the\nSwin Transformer structure is introduced in the last layer of the encoder to\nreduce the number of parameters and improve network performance. We used\nthe CC-CCII lesion segmentation dataset for training and validation of the\nmodel effectiveness. The results of ablation experiments demonstrate that this\nmethod achieved signiﬁcant performance gain, in which the mean pixel\naccuracy is 87.62%, mean intersection over union is 80.6%, and dice\nsimilarity coefﬁcient is 88.27%. Further, we veriﬁed that this model achieved\nsuperior performance in comparison to other models. Thus, the method\nproposed herein can better assist doctors in evaluating and analyzing the\ncondition of COVID-19 patients.\nKEYWORDS\nCOVID-19, CT image, deep learning, detection and recognition, lesion segmentation\nOPEN ACCESS\nEDITED BY\nLin Huang,\nUniversity of Electronic Science and\nTechnology of China, China\nREVIEWED BY\nSheng Ge,\nSoutheast University, China\nXiaomin Yang,\nSichuan University, China\n*CORRESPONDENCE\nYu Pang,\npangyu@cqupt.edu.cn\nGuo Zhang,\nzhangguo@swmu.edu.cn\nSPECIALTY SECTION\nThis article was submitted\nto Medical Physics and Imaging,\na section of the journal\nFrontiers in Physiology\nRECEIVED 29 June 2022\nACCEPTED 11 July 2022\nPUBLISHED 22 August 2022\nCITATION\nSun W, Chen J, Yan L, Lin J, Pang Y and\nZhang G (2022), COVID-19 CT image\nsegmentation method based on\nswin transformer.\nFront. Physiol. 13:981463.\ndoi: 10.3389/fphys.2022.981463\nCOPYRIGHT\n© 2022 Sun, Chen, Yan, Lin, Pang and\nZhang. This is an open-access article\ndistributed under the terms of the\nCreative Commons Attribution License\n(CC BY). The use, distribution or\nreproduction in other forums is\npermitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original\npublication in this journal is cited, in\naccordance with accepted academic\npractice. No use, distribution or\nreproduction is permitted which does\nnot comply with these terms.\nFrontiers inPhysiology frontiersin.org01\nTYPE Original Research\nPUBLISHED 22 August 2022\nDOI 10.3389/fphys.2022.981463\n1 Introduction\nAccording to several studies (Zhu et al., 2019; Dong et al., 2020;\nXu et al., 2020), computed tomography (CT) clearly displays the\ncharacteristic lung lesions of Covid-19 in patients. However, CT\nscans contain hundreds of slices, and CT images must be\nreconstructed and transmitted through an image archiving and\ncommunication system for doctors to interpret results and\ndiagnose patients. Covid-19 and other types of pneumonia are\ngenerally identiﬁed by radiologists by simply processing images\nat communication system terminals, reading them, or projecting\nthem through a lamp (Bai et al., 2020; Song et al., 2021; Bernheim\net al., 2020; Rubin et al., 2020; Wong et al., 2020; Lee et al., 2001).\nSimultaneously, radiologists must be experienced to achieve\nsufﬁcient detection results. Covid-19 has similar medical imaging\ncharacteristics to other types of pneumonia (Shi et al., 2020), and CT\ncan be used to determine whether a patient is infected with viral\np n e u m o n i a( C o v i d - 1 9i sav i r a lp n e u m o n i ac a u s e db yt h eS A R S -\nCOV-2 virus) (Chen and Li, 2020). However, CT is unable to\ndetermine which virus causes viral pneumonia; the novel\ncoronavirus or anothervirus, making it difﬁcult to distinguish\nand diagnose the virus type. Considering these dif ﬁculties,\nquickly and accurately distinguishing between Covid-19 and\nother types of pneumonia is crucial to facilitating the screening\nprocess in clinical practice. Therefore, with the AI-assisted diagnosis\nof medical images,accurate and efﬁcient recognition of Covid-19\nlung CT images is of profound signiﬁcance for controlling the\nepidemic (Ardila et al., 2019; Esteva et al., 2019; Esteva et al., 2017;\nLitjens et al., 2017; Mei et al., 2020; Qin et al., 2019; Topol 2019; Li\net al., 2020; Jaiswal et al., 2020).\nLocality is used by traditional convolutional networks to\nimprove efﬁciency but at the cost of losing the connection in a\nglobal context. Convolutional architecture has an inherent induction\nbias, lacking an understanding of position dependence in images\n(Wang et al., 2020; Valanarasu et al., 2021). In a study byDosovitskiy\net al. (2020), the proposed vision transformer (ViT) was trained on\nlarge image datasets using location-embedded two-dimensional\n(2D) image patches as input sequences, thus achieving a\nperformance comparable to that of convolutional networks.\nBased on the transformer architecture, a self-attention\nmechanism was utilized to encode the position dependence at a\ndistance to learn ef\nﬁcient representations. However, most existing\ntransformer-based network architectures require large datasets for\ntraining. Generalization may be inadequate if the training is\nperformed using insufﬁcient data. In a study byHassani et al.\n(2021), a compact convolutional transformer (CCT) (Wang et al.,\n2021) was proposed to eliminate the misunderstanding of the\nrequirement of a transformer for large amounts of data. It\nachieves comparable performance on small datasets; however,\nwhen the input dimension is large, the operational cost of the\nself-attention mechanism increases signiﬁcantly. Global pooling\ndoes not use the location information in the process of extracting\npneumonia symptoms, potentially causing loss of location\ninformation. For imaging tasks, it is important to obtain the\nspatial position structure of an image.\nTherefore, we use a new method to solve the above problems in\nCT lesion segmentation of COVID-19. To solve the problem of\ndetail loss, we add CBAM (Woo et al., 2018) and atrous convolution\nto the U-Net encoder part, and replace the partial convolution\noperation with the empty convolution operation. This can solve the\nproblem of feature image detail loss caused by the decrease of\nresolution after repeated down-sampling operations. A Swin\nTransformer (Liu et al., 2021 )i sa d d e dt oo b t a i nl o c a l\ninformation in the CNN network, and the joint loss function is\nused for optimization during training. Thus, the segmentation of\nbackground regions, lungs, ground-glass opacities, and lung\nparenchyma in the chest CT images of patients is achieved. The\nresults of ablation experiments demonstrate that this method\nachieved signiﬁcant performance gain,in which the mean pixel\naccuracy is 87.62%, mean intersection over union is 80.6%, and dice\nsimilarity coefﬁcient is 88.27%. The feasibility and effectiveness of\nthis method are proved. Chest CT examination has a very important\napplication prospect in clinical observation of treatment effect,\nmonitoring of lesions and follow-up.\n2 Materials and methods\nHere, a new lesion segmentation method in chest CT images of\nCOVID-19 patients is proposed, and the network structure is shown\nin Figure 1. The input is downsampled 4 times in total. The encoder\nperforms a normal convolution and a dilated convolution operation\nbefore downsampling. The BN layer and the activation function\nlayer are added to speed up the network convergence process. The\nCBAM mechanism is introduced in the downsampling process.\nAfter each downsampling iteration, the size of the feature vector is\nhalved, and the number of channels is doubled. In the experiment,\nimages with a height and width of 512 and three channels are used as\ninput, that is,512 × 512 × 3 After being processed by the encoder\npart, a feature vector of size 32 × 32 × 512 is output. Then, the\ndownsampled feature images are ﬂattened to ﬁt the vector\ndimension of the Swin Transformer structure by linear\nembedding. The vector dimension does not change in the\nTransformer encoder structure, and a sequence vector of 1,024 ×\n512 dimensions is output. The sequence vector is restored to 32 ×\n32 × 512 by the Reshape operation toﬁt the input dimension\nrequirement of the segmentation network upsampling. Finally, the\nsegmentation result whose height and width are consistent with the\ninput is obtained after passing through the decoder for four\nupsampling iterations.\n2.1 Convolution attention module\nWe use an attention mechanism in the network to perform\nweight adjustment on the feature vectors. This is similar to how\nFrontiers inPhysiology frontiersin.org02\nSun et al. 10.3389/fphys.2022.981463\nFIGURE 1\nSegmentation network structure diagram.\nFIGURE 2\nFlowchart of the attention module of the convolutional block.\nFIGURE 3\nFlowchart of channel attention mechanism.\nFrontiers inPhysiology frontiersin.org03\nSun et al. 10.3389/fphys.2022.981463\nthe human brain focuses on important information. Important\ninformation is made more prominent, and other information is\nﬁltered. The convolutional attention module is composed of\nchannel attention and spatial attention modules, which are\nused for the attention mechanism of the feature vector\nchannel and space, respectively. The process is shown in\nFigure 2. Finally, the attention weights are multiplied by the\ninput feature image to obtain the output feature image.\n2.1.1 Channel attention mechanism\nChannel attention assigns weights to each channel of the\nfeature image. Valid channel weights are increased, and\ninvalid channel weights are suppressed. The ﬂow of the\nchannel attention mechanism is shown in Figure 3 .T h e\ninput feature F ∈ RH×W×C is average-pooled to generate the\nvector FC\navg ∈ R1×1×C, where C represents the channel. The\nvector FC\nmax ∈ R1×1×C is generated by a max-pooling\noperation. Average pooling has the advantage of optimizing\nthe spatial information of feature images. Max pooling can\nextract landmark information in feature images. The two\noutput features are fed into a shared multilayer perceptron,\nand features with contextual descriptions are generated.\nFinally, the ReLU activation f unction is used to output the\nfeature image channel weights. Feature images are summed\na n dm e r g e de l e m e n t w i s e .T h ef e a t u r ev e c t o rM\nC ∈ RC×1×1 is\noutput through the sigmoid activation function.\nAccording to the above process, the calculation formula is as\nfollows:\nMc(F) /equals σ(MLP(AvgPool(F)) + MLP(MaxPool(F)))\n/equals σ(W1(W0(Fc\navg)) + W1(W0(Fc\nmax))) (1)\nwhere σ represents the Sigmoid activation function, AvgPool\nrepresents the average pooling operation, MaxPool represents\nthe maximum pooling operation, MLP represents the shared\nmulti-layer perceptron, andW0 and W1 ∈ RC/r×C are the weights\nof the shared multi-layer perceptron.\n2.1.2 Spatial attention mechanism\nThe spatial attention mechanism can measure some regions\nof the feature image to obtain higher responses, and the\nmechanism ﬂow is shown in Figure 4. Suppose the feature\nvector optimized by the channel attention module is\nF′ ∈ RH×W×C. F′ generates the two-dimensional vector\nFS\navg ∈ RH×W×1 by the max pooling operation, and\nFS\nmax ∈ RH×W×1 is generated through average pooling, where\nS represents a channel. The two-dimensional vector\ninformation obtained by the pooling operation is\nconcatenated. The feature information is fused through the\nconvolution operation, and a two-dimensional spatial attention\nimage is generated through the sigmoid activation function.\nFinally, the output of the spatial attention module is dot\nmultiplied with the feature image at the pixel level to obtain\nthe weighted feature image.\nThe equation of the above process is as follows:\nM\ns(F) /equals σ(f7×7([MaxPool(F′), AvgPool(F′)]))\n/equals σ(f7×7([Fs\nmax,F s\navg\n])) (2)\nwhere σ is the sigmoid activation function, andf7×7 indicates\nthat the feature vector in parentheses is convolved with a\nconvolution kernel of size 7 × 7.\nF represents the feature image, the outputF′ is optimized by\nthe channel attention module, and the outputF″ is optimized by\nthe spatial attention module. Therefore, feature F is optimized by\nthe CBAM module:\nF′ /equals M\nc(F) ⊗ F (3)\nF″ /equals Ms(F‘) ⊗ F′ (4)\nwhere ⊗ represents that the elementwise multiplication.\nFIGURE 4\nFlow chart of spatial attention mechanism.\nFrontiers inPhysiology frontiersin.org04\nSun et al. 10.3389/fphys.2022.981463\n2.2 Atrous convolution\nFeature information is extracted using U-Net model\nconvolution operations. Due to device performance\nlimitations, multiple pooling operations reduce the resolution\nof feature vectors. When using the convolution operation to\nextract higher-level features, the next convolution operation can\nobtain a larger receptive ﬁeld. However, as the feature size\ndecreases, feature information will be lost. The restoration\ndetail information cannot be restored, while upsampling\nrestores the size. Replacing ordinary convolution operations\nwith atrous convolution can achieve a larger receptive ﬁeld\nrange within a limited convolution kernel. Therefore, the loss\nof detail information caused by the downsampling process can be\nsolved. The ordinary convolution and atrous convolution\nmethods and the obtained receptive ﬁelds are shown in\nFigure 5. In the right image ofFigures 5A,B, the feature maps\nof 9 × 9 use a convolution kernel of size 3 × 3 and stride 1 for\nconvolution operation. In the right picture ofFigure 5A, the\nreceptive ﬁeld is obtained after two ordinary convolution\niterations; the range is 5 × 5. In the right picture ofFigure 5B,\nthe receptiveﬁeld is obtained after one ordinary convolution and\none dilated convolution with a dilation factor of 2; the range is 7 ×\n7. It shows that a larger receptiveﬁeld range is obtained after\nusing atrous convolution. The numbers in theﬁgure represent\nthe number of times the pixels are convolved.\nWhen using continuous atrous convolution, the dilation\nfactor cannot be a common divisor greater than 1. And the\nexpansion factor must satisfy the following formula:\nM\ni /equals max[Mi+1 − 2ri,M i+1 − 2(Mi+1 − ri),r i] (5)\nwhere Mi represents the maximum expansion factor of the i-th\nlayer, andri is the expansion factor that represents the distance\nbetween adjacent elements in the hollow convolution kernel,\nwhich should be less than or equal to the size of the convolution\nkernel. In the atrous convolution operation, the convolution\nkernel size is ﬁxed. When the dilation rate increases, the\nspacing of adjacent elements in the convolution kernel\nincreases. It is also possible to keep the height and width of\nthe original input feature map unchanged.\nFIGURE 5\nConvolution operation. (A): 2D-Convolution. (B): Dilated Convolution.\nFrontiers inPhysiology frontiersin.org05\nSun et al. 10.3389/fphys.2022.981463\n2.3 Swin transformer module\nAfter improving the convolutional structure network of\nU-Net to extract feature information, we use the Swin\nTransformer to extract the global information from the\nfeature information. We combine the CNN with the\nTransformer structural model. The insuf ﬁciency of context\ndependencies in the acquisition of low-level features by\nconvolutional networks will be compensated. Compared with\nViT, we improve the Transformer encoder by introducing\nwindows multi-head self-attention (W-MSA) and shifted\nwindows multi-head self-attention (SW-MSA) ( Hatamizadeh\net al., 2022). Assuming the input isx\nl−1 , the formula is as follows:\nx′l−1 /equals xl−1 + W − MSA(LN(xl−1)), (6)\nxl /equals x′l−1 + MLP(LN(x′l−1)), (7)\nx′l /equals xl + SW − MSA(LN(xl)), (8)\nxl+1 /equals x′l + MLP(LN(x′l)).( 9 )\nwhere l ∈ {1, 2, /,L }\nAccording to the Swin Transformer formula, it can be\nconcluded that the structure consists of two Transformer\nencoder modules. After the input is normalized by the layer,\nthe attention value is calculated using W-MSA, and the residual\nstructure is formed with the original input. After layer\nnormalization and MLP operation, the encoder module with\nthe SW-MSA calculation method is used to output the feature\nvector. The Swin Transformer structure is shown inFigure 6.\nCompared with the ViT, an encoder module is added, and the\nredesigned W-MSA and SW-MSA calculation methods greatly\nreduce the computational complexity.\nIn the W-MSA operation of the Swin Transformer, the\nfeature map is divided into windows of the same size, which\nis equivalent to reducing the size of the patch. Thereby the\ncomputational complexity is reduced. We utilize the same\nself-attention mechanism as ViT inside each individual\nwindow. However, after dividing the feature map into separate\nfeature windows, the attention mechanism values of the feature\nwindows are calculated separately, and there is no information\ninteraction between them. As a result, the self-attention\nmechanism cannot obtain global information. Therefore, the\nSW-MSA operation is increased, and the window operation is\nshifted. This solves the defect that information cannot be\nexchanged between W-MSA operation windows. The\noperation ﬂow of common MSA, W-MSA, and SW-MSA is\nshown in Figure 7. The W-MSA window size is 4. In the SW-\nMSA operation, the feature window is divided into three different\npatch sizes, which are 2 × 2, 2 × 4, and 4 × 4 sizes. After\ncombining four 2 × 2-sized windows and combining four 2 × 4-\nsized windows, two feature windows with patch size 2 × 2 are\nobtained. Then, the attention value is obtained by continuing the\ncalculation of W-MSA. Finally, the original window dimensions\nare restored. As such, not only is the computational complexity\nreduced, but the interactive information between the windows\ncan be obtained.\n2.4 Optimization of loss function\nIn medical image segmentation, common loss functions\ninclude cross entropy loss (CE loss) and dice coefﬁcient loss\n(Dice loss). The chest CT image segmentation method we\nproposed includes four categories: background region, lung\nregion, ground glass opacity, and lung parenchyma.Figure 8\nshows the chest CT images and the pixel distribution maps of\ndifferent categories in the corresponding segmentation gold\nstandard. The abscissa is the segmentation type, and the\nordinate is the number of pixels. It can be seen that the\nproportion of ground glass and lung parenchyma is much\nsmaller than the background and lung areas. This is common\nin mild and moderate patients, and there may even be no focal\nmanifestations. Therefore, uneven data distribution will be\ncaused in the experiment, which makes network training\nmore difﬁcult.\nThe cross-entropy loss function compares the pixel-\npredicted value output by the training model with the real\nFIGURE 6\nSwin Transformer module.\nFrontiers inPhysiology frontiersin.org06\nSun et al. 10.3389/fphys.2022.981463\nvalue. In the case of training without overﬁtting, the smaller the\nloss value, the better the result. The formula is as follows:\nCE loss/equals− yplog2(y′) (10)\nwhere y is the real label paper,y′ is the predicted value, and the\nloss function has the same prediction weight for each category.\nAs shown inFigure 8, the background area accounts for a large\nproportion, and the factors leading to theﬁnal result will be\nbiased towards the background area. After training, the\nperformance value of the loss function is small, but it cannot\nreﬂect the classiﬁcation effect of other categories through the loss\nvalue.\nFIGURE 7\nOperation diagram of self-attention mechanism.(A): MSA.(B): W-MSA.(C): SW-MSA.\nFIGURE 8\nCT images and corresponding pixel category distributions.\nFrontiers inPhysiology frontiersin.org07\nSun et al. 10.3389/fphys.2022.981463\nIn dice loss, dice represents the dice similarly coefﬁcient\n(DSC), which indicates the degree of similarity between two\nsample areas; the value is between 0 and 1; the larger the value,\nthe higher the similarity. Assuming that A and B represent sets of\nregions, the DSC formula is as follows:\nDice /equals\n2|A ∩ B|\n|A| + |B| (11)\nwhere ∩ represents the intersection of sets, and the dice loss\nformula can be obtained according to the DSC. The formula is as\nfollows:\nDice loss/equals 1 −\n2\n⏐⏐\n⏐\n⏐Y ∩ /C22Y\n⏐⏐\n⏐\n⏐+ 1\n|Y| +\n⏐⏐\n⏐\n⏐/C22Y\n⏐⏐\n⏐\n⏐+ 1 (12)\nwhere Y is the real segmentation area, and /C22Y is the model\nprediction area. We add 1 to the denominator and numerator\nto prevent the denominator from being zero and to reduce the\npossibility of overﬁtting during the training process. Compared\nwith the CE loss function, dice loss is not affected by the\nbackground when the number of pixel categories is unevenly\ndistributed. However, training instability occurs when the\nprediction is incorrect. Therefore, we combine the CE loss\nfunction and dice loss as a joint loss function and use the CE\nloss function to guide dice loss for training. The formula is as\nfollows:\nloss /equals 0.5× CE + Dice loss (13)\nloss /equals 0.5× Y · log\n2( /C22Y) + 1 − 2\n⏐⏐\n⏐\n⏐Y ∩ /C22Y\n⏐⏐\n⏐\n⏐+ 1\n|Y| +\n⏐⏐\n⏐\n⏐/C22Y\n⏐⏐\n⏐\n⏐+ 1 (14)\n2.5 Datasets\nWe utilized a dataset from the China Consortium for Chest\nCT Imaging Research (CC-CCII) (Ai et al., 2020). The CC-CCII\ndataset contains 617,775 CT images from 6,752 CT scans of\n4,154 patients. The study sample size was estimated by standard\nAI training and validation methods. Patients were randomly\nassigned to a training set (60%), an internal validation set (20%)\nor a test set (20%). We chose to use 750 annotated chest CT\nimages selected from 150 COVID-19 patients byﬁve radiologists\nwith 15 years of experience. These images include background\nareas, lung areas, ground-glass opacities, and lung parenchyma.\nMild patients mainly present with ground-glass opacity, which is\ndistributed in the lower lobes of both lungs and adheres closely to\nthe pleura. Ground-glass shadows are characterized by spreading\nFIGURE 9\nChest CT images of COVID-19 patients.(A): Initial CT image.(B): Gold standard.(C): Color-annotated segmentation results.\nFrontiers inPhysiology frontiersin.org08\nSun et al. 10.3389/fphys.2022.981463\ntoward the center and blurring at the edges. In moderate patients,\nthe number of lesions proliferated, and the lesions were markedly\nplaque-like. The patient is accompanied by a condition of cough\nand fatigue. In severe patients, the density of lung tissue increases\nand the lung parenchyma changes. The patient presented with\nfever and headache. An example of the segmentation of a chest CT\nimage of a COVID-19 patient is shown inFigure 9. Figure 9Ais the\ninitial image,Figure 9B is the gold standard of the dataset, and\nFigure 9Cis after the gold standard mask and the initial CT image\nare superimposed; the highlighted color is used to distinguish the\nsegmentation results. The gray area is the background area of the\npatient, the red area is the lung area, the yellow area is the ground\nglass opacity, and the blue area is the lung parenchyma.\n2.5.1 Data augmentation\nCT images have different properties, such as brightness,\nsaturation, and angle. Therefore, a data augmentation method\nis added in the preprocessing stage of experimental training to\nprevent overﬁtting of the training results. In this way, the model\nperformance is increased, and the data augmentation is shown in\nFigure 10. Figure 10Ais the initial image.Figures 10B– F are the\ncorresponding labels of the original image after rotating,\nhorizontally ﬂipping, randomly cropping, adjusting saturation,\nand adjusting brightness, respectively.\n2.5.2 Training parameters\nThe training set of the CC-CCII dataset is divided into ten\ngroups, each time nine groups of images are used as the training\nset and one group is used as the validation set. They were used in\nten-fold cross-validation experiments. After training and\nvalidation separately, we use the test set to test, repeat this\nprocess ten times, and ﬁnally take the average of the ten\nresults as the evaluation of algorithm accuracy. All CT image\npixels are resized to 512 × 512 pixels before being input into the\ntraining model. In the model training, the network adopts the\nmean square error loss function; The initial learning rate of the\nAdam optimizer is 0.0001; The batch size is set to 64; And the\nfully connected layer uses a dropout layer with probability 0.5.\nThis deep learning method does not require much analysis of the\nthreshold and gray value of CT images. Data augmentation is\nachieved by adjusting contrast, afﬁne transformation, and color\ndithering to achieve better performance of the model. The details\nof the experimental training parameters are listed inTable 1.\n2.5.3 Evaluation indicators\nTo analyze the segmentation performance of the trained\nmodel, we used three common performance metrics: mean\nintersection over union (mIoU) (Rezato ﬁghi et al., 2019), DSC\n(Huang et al., 2022 ), and mean pixel accuracy (mPA)\n(Paintdakhi et al., 2016). mIoU is the average of the ratios\nof the intersection and union of the results predicted by the\nFIGURE 10\nExample of CT image data-enhancement results.(A): the original CT image and its corresponding gold standard.(B): the CT image obtained\nafter rotation and its corresponding gold standard.(C): the CT image obtained after horizontalﬂipping and its corresponding gold standard.(D):t h e\nCT image obtained after random cropping and its corresponding gold standard.(E): the CT image obtained after adjusting the contrast and its\ncorresponding gold standard.(F): the CT image obtained after adjusting the brightness and its corresponding gold standard.\nTABLE 1 Training parameter settings.\nType Setting\nBatch size 64\nLearning rate 0.0001\nOptimizer Adam\nIterations (Epoch) 100\nUbuntu 18.04 PyToch1.6.0\nFrontiers inPhysiology frontiersin.org09\nSun et al. 10.3389/fphys.2022.981463\nmodel for each category and the true label, DSC is the\nsimilarity measure function, which can calculate the\nsimilarity between the true label and the predicted label,\nand mPA is represents the pixel accuracy of each category.\nThe pixel accuracy is summed and averaged.\nmIOU /equals\n1\nk + 1 ∑\nk\ni/equals 0\nTP\nTP + FN + FP (15)\nDSC /equals 1\nk + 1 ∑\nk\ni/equals 0\n2TP\nFP + 2TP + FN (16)\nmPA /equals 1\nk + 1 ∑\nk\ni/equals 0\nTP\nTP + FN (17)\nwhere k is the number of classes, TP is the number of pixels that\nare correctly predicted as positive examples, FN is the number of\npixels that are incorrectly predicted as negative examples, and FP\nis the number of pixels that are incorrectly predicted as positive\nexamples.\n3 Results and discussion\n3.1 Ablation experiment\nTo verify the segmentation effect of the improved U-Net\nmodel, we conducted ablation experiments. The segmentation\nFIGURE 11\nExample of ablation experiment comparison.(A): the CT image of the COVID-19 patient.(B): the gold standard of the CT image.(C): the U-Net\nsegmentation result.(D): the U-Net segmentation result after introducing atrous convolution.(E): the U-Net segmentation result after introducing\natrous convolution and CBAM.(F): the U-Net segmentation result after introducing the atrous convolution, CBAM, and Swin Transformer modules.\nTABLE 2 Comparison of ablation experiments.\nModle mPA/% mIoU/% DSC/%\nU-Net 85.86 78.59 86.74\nU-Net + Atrous convolution 86.22 79.11 87.15\nU-Net + Atrous convolution + CBAM 87.41 80.41 87.49\nU-Net + Atrous convolution + CBAM + Swin Transformer 87.62 80.64 88.27\nFrontiers inPhysiology frontiersin.org10\nSun et al. 10.3389/fphys.2022.981463\ntest results are shown inFigure 11. From the segmentation results\nof the CT image example, it can be observed that the original\nU-Net did not segment the tiny lesion details. The other\nimproved models identiﬁed the lesions, but the segmentation\neffects were different. The U-Net segmentation result after\nadding atrous convolution is shown in Figure 11D. After\nadding CBAM, the effect is improved, as shown in\nFigure 11E. The model segmentation results after introducing\nthe atrous convolution, CBAM, and Swin Transformer modules\nare signi ﬁcantly improved, as shown in Figure 11F . The\nsegmentation performance of our proposed method achieved\nthe best performance; especially in the case of a large number of\nlesion areas, the segmentation results of lesion and lung areas by\nthis method are closer to the corresponding gold standard.\nThe experimental segmentation performance indicators are\nlisted inTable 2. On the basis of U-Net, the atrous convolution\nmPA is added, and the mIoU and DSC indicators are increased\nby 0.36%, 0.52%, and 0.41%, respectively. After adding atrous\nconvolution and CBAM, the corresponding indicators greatly\nimproved. mPA, mIoU, and DSC metrics improved by 1.55%,\n1.82%, and 0.75%, respectively. After adding atrous convolution,\nthe corresponding indicators of CBAM and the Swin\nTransformer improved the most. The mPA, mIoU, and DSC\nmetrics improved by 1.76%, 2.05%, and 1.53%, respectively. The\ncorresponding metrics demonstrate the effectiveness and\nfeasibility of our method.\nThe convergence effect of the training loss function of the\nnew model is shown in Figure 12. The curves in the ﬁgure\nrepresent the training loss curves from the 1st to 5th fold,\nrespectively. After the training method of cross-validation is\nused, we ﬁnd that the training loss value of each epoch in\nfold 1 is the largest and the training loss value of each epoch\nin foldﬁve is the smallest. The training loss value of each epoch in\nthe next fold is smaller than that of the previous fold. The results\nshow that the convergence effect of the new model is signiﬁcantly\nimproved.\n3.2 Models comparison\nWe demonstrate the feasibility and effectiveness of the\nproposed improved method through ablation experiments. To\nfurther verify the segmentation ability of the model, we compared\nit with other models. The results of the segmentation experiment\nare shown inFigure 13. First, the ResU-Net model (Jha et al.,\n2019) adds a residual structure to the convolution operations of\nthe encoder and decoder to improve model performance. In CT\nFIGURE 12\nTraining convergence loss curve.\nTABLE 3 Performance comparison of different models.\nModel mPA/% mIoU/% DSC/% FLOPs (G)\nResU-Net 87.05 80.1 87.54 1.46\nAttention U-Net 86.26 78.31 86.47 1.95\nTransU-Net 86.99 79.33 87.31 1.39\nOurs 87.62 80.6 88.27 1.44\nTABLE 4 Subjective evaluation scoring method.\nScore Features of the\nrestored image\n0 Severely distorted image\n1 Image with severe distortion in some areas\n2 Slightly distorted image\n3D i f ﬁcult to spot distorted images\n4 Images with better visual effects\n5 Very sharp images\nTABLE 5 Subjective quality evaluation of different algorithms.\nMethod Sharpness Resolution Invariance Acceptability\nResU-Net 3.3 ± 0.21 3.5 ± 0.25 0.5 ± 0.39 3.8 ± 0.21\nAttention U-Net 3.6 ± 0.24 3.9 ± 0.49 0.6 ± 0.16 3.9 ± 0.41\nTransU-Net 3.7 ± 0.16 4.1 ± 0.21 0.6 ± 0.25 4.2 ± 0.24\nOur method 3.9 ± 0.24 4.3 ± 0.07 0.7 ± 0.36 4.2 ± 0.81\nFrontiers inPhysiology frontiersin.org11\nSun et al. 10.3389/fphys.2022.981463\nFIGURE 13\nExample of comparison of different models.(A): the CT images of COVID-19 patients.(B): the gold standard of CT images.(C):t h e\nsegmentation results of our model.(D): the ResU-Net segmentation results.(E): the Attention U-net segmentation results.(F): the TransU-net\nsegmentation result.\nFrontiers inPhysiology frontiersin.org12\nSun et al. 10.3389/fphys.2022.981463\nimages of mild patients, our method is compared with the ResU-\nnet method, as shown in Figures 13C,D. In the ﬁgure, the\nperformance of the two methods is comparable when\nsegmenting smaller lesions. However, when the proportion of\nthe lesion area is relatively large, the segmentation results show\nobvious voids, as shown in the second picture inFigure 13D.\nSecond, Attention U-net (Oktay et al., 2018) introduces a soft\nattention mechanism, which is implemented by supervising the\nupper-level features through the next-level features. Our method\nis compared with the attention U-net method, as shown in\nFigures 13C,E. From the segmentation results, it can be seen\nthat our method performs signiﬁcantly better than the attention\nU-net method in terms of lesion segmentation accuracy in\nsmaller regions. Further, leaky segmentation is present in the\nsixth picture of Figure 13E. Finally, TansU-Net (Chen et al.,\n2021) applies the Transformer encoder to image segmentation.\nOur method is compared with the TansU-Net method, as shown\nin Figures 13C,F. In the segmentation example, the TansU-Net\nmethod also appears similar to Attention U-net, failing to\nsuccessfully identify smaller lesion areas. We used the Swin\nTransformer encoder structure before the segmentation\nnetwork decoder. Although CBAM and hole convolution are\nadded, the FLOPs are not much different, and the comprehensive\nsegmentation ability is signiﬁcantly better than TransU-Net. The\neffectiveness of our method is further demonstrated, and some\ncomplexity is reduced from the Transformer structure.\nThe comparison performance indicators of the above models\nare listed inTable 3. The performance metrics of the Attention\nU-Net method were the worst. The ResU-Net model\noutperformed TransU-Net in segmentation performance in\nthe used test dataset. Compared with ResU-Net, our proposed\nsegmentation method has improved performance indicators.\nmPA, mIOU, and DSC were improved by 0.57%, 0.5%, and\n0.73%, respectively. Therefore, our proposed method performed\nthe best among the compared models.\n3.3 Subjective evaluation\nFor more specialized medical evaluation of segmentation\nmodels, clinical validation is required. We invited 10 chief\nphysicians with more than 5 years of clinical experience in\nradiology to independently perform image analysis (sharpness,\nresolution, invariance, and acceptability). The scoring criteria for\nsubjective evaluation are shown inTable 4. Ten groups of test\nsamples were randomly constructed, and each group consisted of\nten CT images of the lesion area. The subjective quality\nevaluation results of different algorithms utilized by\nradiologists are listed inTable 5.\nAs shown inTable 5, our proposed Atrous Convolution +\nCBAM + Swin Transformer model achieves the best subjective\nquality evaluations in terms of sharpness, resolution, invariance,\nand acceptability. The main reason is the bene ﬁt from\nintroducing W-MSA and the exchange of information.\nCompared with other segmentation methods, our W-MSA\nfuses the mutual information and the multimodal features of\nCT images and has strong representation. The consistency of\npathological information between segmented CT image and\noriginal CT image was guaranteed. This method achieves the\nbest segmentation effect in terms of ground-glass opacity and\nvisible plaque and lung parenchyma lesions.\n4 Conclusion\nCurrently, a key approach to prevent the spread of the\nepidemic is to combine the chest CT images of patients for\ndiagnosis. Therefore, this paper proposed an improved U-Net\nnetwork for lesion segmentation in chest CT images of COVID-\n19. Atrous convolution was used as the convolution operation\nof each layer of the segmentation network encoder structure,\nand CBAM was introduced in the downsampling process to\nsolve the problem of loss of lesion detail during the\ndownsampling process. The S win Transformer module was\nadded to the encoder using the transformer structure to\nobtain global feature information. The primary improvement\nof the segmentation model framework is in the encoder part,\nwhich improved the model feature extraction performance. The\nresults of the ablation experiments showed that the mPA,\nmIOU, and DSC reached 87.62, 80.6 and 88.27, respectively.\nIn the subjective evaluation of radiologists, our method can\neffectively segment ground-gla ss opacity, visible plaque and\nlung parenchyma lesions, and maintain consistency with the\noriginal CT image pathological information. In future research,\nwe will continue to reﬁne the model. We aim to improve the\nscreening process and the quantitative analysis of the disease\nand enhancing the ef ﬁciency of diagnosis and reducing\ninfection.\nData availability statement\nPublicly available datasets were analyzed in this study. This\ndata can be found here:http://ncov-ai.big.ac.cn/download\nEthics statement\nEthical review and approval was not required for the study on\nh u m a np a r t i c i p a n t si na c c o r d a n c ew i t ht h el o c a ll e g i s l a t i o na n d\ninstitutional requirements. Written informed consent for\nparticipation was not required for this study in accordance with\nthe national legislation and the institutional requirements. Written\ninformed consent was not obtained from the individual(s) for the\npublication of any potentially identiﬁable images or data included in\nthis article.\nFrontiers inPhysiology frontiersin.org13\nSun et al. 10.3389/fphys.2022.981463\nAuthor contributions\nAll authors listed have made a substantial, direct, and intellectual\ncontribution to the work and approved it for publication.\nFunding\nThis work was supported by the Doctoral Innovative Talents\nProject of Chongqing Universityof Posts and Telecommunications\n(BYJS202107). Chongqing Natural Science Foundation of China\n(grant number cstc2021jcyj-bsh0218); The National Natural Science\nFoundation of China (Grant No. U21A20447 and 61971079); The\nBasic Research and Frontier Exploration Project of Chongqing\n(Grant No. cstc2019jcyjmsxmX0666); Chongqing technological\ninnovation and application development project (cstc2021jscx-\ngksbx0051); The Innovative Group Project of the National\nNatural Science Foundation of Chongqing (Grant No.\ncstc2020jcyj-cxttX0002), and theR e g i o n a lC r e a t i v eC o o p e r a t i o n\nProgram of Sichuan (2020YFQ0025); The Science and\nTechnology Research Progra m of Chongqing Municipal\nEducation Commission (KJZD-k202000604).\nAcknowledgments\nWe thank the School of Optoelectronic Engineering of\nChongqing University of Posts and Telecommunications for\ntheir assistance in the research.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could\nbe construed as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their afﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nA i ,T . ,Y a n g ,Z . ,H o u ,H . ,Z h a n ,C . ,C h e n ,C . ,L v ,W . ,e ta l .( 2 0 2 0 ) .C o r r e l a t i o n\nof chest CT and RT-PCR testing for coronavirus disease 2019 (Covid-19) in\nChina: A report of 1014 cases. Radiology 296, E32– E40. doi:10.1148/radiol.\n2020200642\nArdila, D., Kiraly, A. P., Bharadwaj, S., Choi, B., Reicher, J. J., Peng, L., et al.\n(2019). End-to-end lung cancer screening with three-dimensional deep learning on\nlow-dose chest computed tomography.Nat. Med.25, 954– 961. doi:10.1038/s41591-\n019-0447-x\nBai, H. X., Hsieh, B., Xiong, Z., Halsey, K., Choi, J. W., Tran, T. M. L., et al. (2020).\nPerformance of radiologists in differentiating Covid-19 from non-Covid-19 viral\npneumonia at chest CT. Radiology 296 (2), E46 – E54. doi:10.1148/radiol.\n2020200823\nBernheim, A., Mei, X., Huang, M., Yang, Y., Fayad, Z. A., Zhang, N., et al. (2020).\nChest CTﬁndings in coronavirus disease-19 (Covid-19): Relationship to duration of\ninfection. Radiology 295 (3), 200463. doi:10.1148/radiol.2020200463\nChen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., et al. (2021). Transunet:\nTransformers make strong encoders for medical image segmentation. arXiv:\n2102.04306. doi:10.48550/arXiv.2102.04306\nChen, Y., and Li, L. (2020). SARS-CoV-2: Virus dynamics and host response.\nLancet. Infect. Dis.20 (5), 515– 516. doi:10.1016/S1473-3099(20)30235-8\nDong, E., Du, H., and Gardner, L. M. (2020). An interactive web-based dashboard\nto track Covid-19 in real time.Lancet. Infect. Dis. 20 (5), 533– 534. doi:10.1016/\nS1473-3099(20)30120-1\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., et al. (2020). An image is worth 16x16 words: Transformers for image\nrecognition at scale.arXiv. 2010:11929. doi:10.48550/arXiv.2010.11929\nEsteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., et al. (2017).\nDermatologist-level classiﬁcation of skin cancer with deep neural networks.Nature\n542 (7639), 115– 118. doi:10.1038/nature21056\nEsteva, A., Robicquet, A., Ramsundar, B., Kuleshov, V., DePristo, M., Chou, K.,\net al. (2019). A guide to deep learning in healthcare.Nat. Med.25 (1), 24– 29. doi:10.\n1038/s41591-018-0316-z\nHassani, A., Walton, S., Shah, N., Abuduweili, A., Li, J., and Shi, H. (2021).\nEscaping the big data paradigm with compact transformers.arXiv. 2104:05704.\ndoi:10.48550/arXiv.2104.05704\nHatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H., and Xu, D. (2022). Swin\nunetr: Swin transformers for semantic segmentation of brain tumors in mri images.\narXiv. 2201:01266.\nHuang, K., Xu, L., Zhu, Y., and Meng, P. (2022). AU-snake based deep learning\nnetwork for right ventricle segmentation.Med. Phys. 49 (6), 3900– 3913. doi:10.\n1002/mp.15613\nJaiswal, A., Gianchandani, N., Singh, D., Kumar, V., and Kaur, M. (2020).\nClassiﬁcation of the Covid-19 infected patients using DenseNet201 based deep\ntransfer learning.J. Biomol. Struct. Dyn.39 (15), 5682– 5689. doi:10.1080/07391102.\n2020.1788642\nJ h a ,D . ,S m e d s r u d ,P .H . ,R i e g l e r ,M .A . ,J o h a n s e n ,D . ,L a n g e ,T .D . ,H a l v o r s e n ,\nP., et al. (2019). “Resunet++: An advanced architecture for medical image\nsegmentation. ” in Proceedings IEEE International Symposium on Multimedia\n(ISM). (San Diego, CA, USA: IEEE), 225 – 2255. doi:10.1109/ISM46123.2019.\n00049\nLee, Y., Hara, T., Fujita, H., Itoh, S., and Ishigaki, T. (2001). Automated detection\nof pulmonary nodules in helical CT images based on an improved template-\nmatching technique. IEEE Trans. Med. Imaging20 (7), 595– 604. doi:10.1109/42.\n932744\nLi, L., Qin, L., Xu, Z., Yin, Y., Wang, X., Kong, B., et al. (2020). Using artiﬁcial\nintelligence to detect COVID-19 and community-acquired pneumonia based on\npulmonary CT: Evaluation of the diagnostic accuracy.Radiology 296, E65– E71.\n200905. doi:10.1148/radiol.2020200905\nLitjens, G., Kooi, T., Bejnordi, B. E., Setio, A. A. A., Ciompi, F., Ghafoorian, M.,\net al. (2017). A survey on deep learning in medical image analysis.Med. Image Anal.\n42, 60– 88. doi:10.1016/j.media.2017.07.005\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (2021). Swin transformer:\nHierarchical vision transformer using shifted windows. In.Proceedings of the IEEE/\nCVF International Conference on Computer Vision, 10012– 10022. doi:10.48550/\narXiv.2103.14030\nMei, X., Lee, H. C., Diao, K., Huang, M., Lin, B., Liu, C., et al. (2020). Artiﬁcial\nintelligence– enabled rapid diagnosis of patients with Covid-19.Nat. Med. 26 (8),\n1224– 1228. doi:10.1038/s41591-020-0931-3\nOktay, O., Schlemper, J., Folgoc, L. L., Lee, M., Heinrich, M., Misawa, K., et al.\n(2018). Attention u-net: Learning where to look for the pancreas.arXiv:1804.03999.\ndoi:10.48550/arXiv.1804.03999\nFrontiers inPhysiology frontiersin.org14\nSun et al. 10.3389/fphys.2022.981463\nPaintdakhi, A., Parry, B., Campos, M., Irnov, I., Elf, J., Surovtsev, I., et al. (2016).\nOufti: An integrated software package for high-accuracy high-throughput\nquantitative microscopy analysis. Mol. Microbiol. 99 (4), 767– 777. doi:10.1111/\nmmi.13264\nQ i n ,Z .Z . ,S a n d e r ,M .S . ,R a i ,B . ,T i t a h o n g ,C .N . ,S u d r u n g r o t ,S . ,L a a h ,S .N . ,\net al. (2019). Using arti ﬁcial intelligence to read chest radiographs for\ntuberculosis detection: A multi-site evaluation of the diagnostic accuracy of\nthree deep learning systems. Sci. Rep. 9 (1), 15000. doi:10.1038/s41598-019-\n51503-3\nRezatoﬁghi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., and Savarese, S. (2019).\n“Generalized intersection over union: A metric and a loss for bounding box\nregression.” in Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, 15-20 June 2019. (Long Beach, CA, USA: IEEE),\n658– 666. doi:10.1109/CVPR.2019.00075\nR u b i n ,G .D . ,R y e r s o n ,C .J . ,H a r a m a t i ,L .B . ,S v e r z e l l a t i ,N . ,K a n n e ,J .P . ,\nRaoof, S., et al. (2020). The role of chest imaging in patient management\nduring the covid-19 pandemic: A multinational consensus statement from\nthe ﬂeischner society. Chest 296 (1), 106 – 116. doi:10.1016/j.chest.2020.\n04.003\nShi, H., Han, X., Jiang, N., Cao, Y., Alwalid, O., Gu, J., et al. (2020). Radiological\nﬁndings from 81 patients with covi d-19 pneumonia in wuhan, China: A\ndescriptive study. Lancet. Infect. Dis. 20 (4), 425 – 434. doi:10.1016/S1473-\n3099(20)30086-4\nSong, Y., Zheng, S., Li, L., Zhang, X., Zhang, X., Huang, Z., et al. (2021). Deep\nlearning enables accurate diagnosis of novel coronavirus (COVID-19) with CT\nimages. IEEE/ACM Trans. Comput. Biol. Bioinform.18 (6), 2775– 2780. doi:10.1109/\nTCBB.2021.3065361\nTopol, E. J. (2019). High-performance medicine: The convergence of human and\nartiﬁcial intelligence. Nat. Med. 25 (1), 44– 56. doi:10.1038/s41591-018-0300-7\nValanarasu, J. M. J., Oza, P., Hacihaliloglu, I., and Patel, V. M. (2021). Medical\ntransformer: Gated axial-attention for medical image segmentation.arXiv. 2102:\n10662. doi:10.1007/978-3-030-87193-2_4\nWang, H., Cao, P., Wang, J., and Zaiane, O. R. (2021). Uctransnet: Rethinking the\nskip connections in u-net from a channel-wise perspective with transformer.arXiv:\n2109.04335. doi:10.48550/arXiv.2109.04335\nWang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., and Chen, L. C. (2020).“Axial-\ndeeplab: Stand-alone axial-attention for panoptic segmentation.” in Proceedings of\nthe European conference on computer vision (ECCV), 29 October 2020. Berlin,\nGermany: Springer, 108– 126. doi:10.1007/978-3-030-58548-8_7\nWong, H. Y. F., Lam, H. Y. S., Fong, A. H. T., Leung, S. T., Chin, T. W. Y., Lo, C. S.\nY., et al. (2020). Frequency and distribution of chest radiographicﬁndings in\npatients positive for Covid-19. Radiology 296 (2), E72– E78. doi:10.1148/radiol.\n2020201160\nWoo, S., Park, J., Lee, J. Y., and Kweon, I. S. (2018).“Cbam: Convolutional block\nattention module.”\nin Proceedings of the European conference on computer vision\n(ECCV), 06 October 2018. Cham: Springer, 3 – 19. doi:10.1007/978-3-030-\n01234-2_1\nXu, X., Jiang, X., Ma, C., Du, P., Li, X., Lv, S., et al. (2020). A deep learning system\nto screen novel coronavirus disease 2019 pneumonia. Engineering 6 (10),\n1122– 1129. doi:10.1016/j.eng.2020.04.010\nZhu, N., Zhang, D., Wang, W., Li, X., Yang, B., Song, J., et al. (2019). A novel\ncoronavirus from patients with pneumonia in China, 2019.N. Engl. J. Med.382,\n727– 733. doi:10.1056/NEJMoa2001017\nFrontiers inPhysiology frontiersin.org15\nSun et al. 10.3389/fphys.2022.981463",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.6914344429969788
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6506887674331665
    },
    {
      "name": "Sørensen–Dice coefficient",
      "score": 0.51279217004776
    },
    {
      "name": "Computer science",
      "score": 0.49926209449768066
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4987308979034424
    },
    {
      "name": "Weighting",
      "score": 0.47169116139411926
    },
    {
      "name": "Medicine",
      "score": 0.45787686109542847
    },
    {
      "name": "Encoder",
      "score": 0.4468930661678314
    },
    {
      "name": "Ground-glass opacity",
      "score": 0.443178653717041
    },
    {
      "name": "Deep learning",
      "score": 0.4128912091255188
    },
    {
      "name": "Image segmentation",
      "score": 0.3516201972961426
    },
    {
      "name": "Radiology",
      "score": 0.30858343839645386
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Adenocarcinoma",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Cancer",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I10535382",
      "name": "Chongqing University of Posts and Telecommunications",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I3017480383",
      "name": "Southwest Medical University",
      "country": "CN"
    }
  ],
  "cited_by": 20
}