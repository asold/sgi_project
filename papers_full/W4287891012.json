{
  "title": "Craft an Iron Sword: Dynamically Generating Interactive Game Characters by Prompting Large Language Models Tuned on Code",
  "url": "https://openalex.org/W4287891012",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287893384",
      "name": "Ryan Volum",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2117485767",
      "name": "Sudha Rao",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2123074866",
      "name": "Michael Xu",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A4287893387",
      "name": "Gabriel DesGarennes",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A1987357541",
      "name": "Chris Brockett",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2113965177",
      "name": "Benjamin Van Durme",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A4287893390",
      "name": "Olivia Deng",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2781259439",
      "name": "Akanksha Malhotra",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2165659695",
      "name": "Bill Dolan",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4293320163",
    "https://openalex.org/W2123159770",
    "https://openalex.org/W3200128700",
    "https://openalex.org/W4288280569",
    "https://openalex.org/W2963231009",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W3206345746",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W2480004914",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221155590",
    "https://openalex.org/W3205300104",
    "https://openalex.org/W3200005007",
    "https://openalex.org/W4226226396",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3034912332",
    "https://openalex.org/W4224308101"
  ],
  "abstract": "Ryan Volum, Sudha Rao, Michael Xu, Gabriel DesGarennes, Chris Brockett, Benjamin Van Durme, Olivia Deng, Akanksha Malhotra, Bill Dolan. Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022). 2022.",
  "full_text": "Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022), pages 25 - 43\nJuly 14, 2022 ©2022 Association for Computational Linguistics\nCraft an Iron Sword: Dynamically Generating Interactive Game Characters\nby Prompting Large Language Models Tuned on Code\nRyan Volum Sudha Rao Michael Xu Gabriel DesGarennes Chris Brockett\nBenjamin Van Durme Olivia Deng Akanksha Malhotra Bill Dolan\nMicrosoft Corporation\nsudhra@microsoft.com\nAbstract\nNon-Player Characters (NPCs) significantly en-\nhance the player experience in many games.\nHistorically, players’ interactions with NPCs\nhave tended to be highly scripted, to be limited\nto natural language responses to be selected by\nthe player, and to not involve dynamic change\nin game state. In this work, we demonstrate that\nuse of a few example conversational prompts\ncan power a conversational agent to generate\nboth natural language and novel code. This ap-\nproach can permit development of NPCs with\nwhich players can have grounded conversa-\ntions that are free-form and less repetitive. We\ndemonstrate our approach using OpenAI Codex\n(GPT-3 finetuned on GitHub), with Minecraft\ngame development as our test bed. We show\nthat with a few example prompts 1, a Codex-\nbased agent can generate novel code, hold\nmulti-turn conversations and answer questions\nabout structured data. We evaluate this applica-\ntion using experienced gamers in a Minecraft\nrealm and provide analysis of failure cases and\nsuggest possible directions for solutions.\n1 Introduction\nThe recent advent of large pre-trained language\nmodels such as GPT-2 (Radford et al., 2019) and\nGPT-3 (Brown et al., 2020) has fostered spectacular\nadvances in text-generation. In this work, we focus\non the potential application of these large language\nmodels in video games. In games, Non-Player\nCharacters (NPCs) enhance the player experience\nby providing interaction, often involving conversa-\ntion. Currently players’ conversations with NPCs\nare highly scripted: in a typical scenario players\nmust select from a set of preset responses that they\ncan give to the NPC. Moreover, this interaction\nis limited to natural language responses, and does\nnot directly involve dynamic game state change as\n1We release the prompt and the wrapper code in\nthis repository https://github.com/microsoft/\ninteractive-minecraft-npcs\nFigure 1: A Minecraft player interacting with a Codex-\npowered NPC in in two scenarios: question answering\n(top) and task completion (bottom).\npart of the interaction. Below, we explore some\nfirst steps towards creating functionally agentive\nNPCs with which players can hold free-form con-\nversations that are grounded in the game and which\nplayers can instruct to perform actions that change\nthe game state by having the NPC adaptively gener-\nate code that calls functions exposed by the game\nAPI. This is done by a single language model that\ngenerates both natural language and code. To this\nend, we use OpenAI Codex (Chen et al., 2021)\n(a GPT-3 model finetuned on GitHub data). We\ndemonstrate that by simply including examples of\nboth natural language conversations and code in the\nprompt, Codex can generalize to interesting new\nsettings, opening up intriguing possibilities for en-\nhanced player experiences and game development.\nWe employ Minecraft as our test bed. First, this\n25\nFigure 2: Sample prompt given to the Codex model to power an NPC in Minecraft.\nis an open-world game where players creatively\nbuild artifacts in the environment. This makes\nMinecraft a good use case for providing NPCs\nthat can converse and perform tasks for the player,\nsomething that Minecraft currently does not do.\nSecond, Minecraft has rich game APIs in scripting\nlanguages,2 that permit models to write function\ncalls that allow the NPC perform in-game actions.\nWe investigate these Codex-powered NPCs\nthrough an exploratory user study. We ask ex-\nperienced gamers to interact with the NPC to ac-\ncomplish tasks in a Minecraft realm: obtain craft-\ning recipes, mine resources, craft items and, lastly,\nbreak out of two escape rooms. Figure 1 shows\ntwo sample interactions. We analyze these interac-\ntions, and discuss fail cases and what modifications\nmight be needed to handle them. We also present\ndiscussion of some interesting avenues of future re-\nsearch in the gaming space that might be achieved\nby fine-tuning on game APIs.\n2We use the open-source Mineflayer API. Microsoft re-\ncently released a first-party API with similar functionality in\nits GameTest Framework SimulatedPlayer class. This is still\nunder development and was not available for us at the time we\nconducted our experiments.\n2 Related Work\nThe Minecraft gaming environment is increasingly\nwidely used as a platform for researching agents\nand machine-human collaboration. MALMO\n(Johnson et al., 2016) is a test-bed for machine\nlearning architectures trained on reinforcement\nlearning. (Rose, 2014) showcases dialog in which\nplayers provide NPCs with information and the\nNPCs retain episodic memory and identify player’s\nsentiments. (Szlam et al., 2019) lays out the moti-\nvation for building assistants in Minecraft. (Gray\net al., 2019) describes a framework for dialog-\nenabled interactive agents using high-level, hand-\nwritten composable actions. (Jayannavar et al.,\n2020) study collaborative conversation between\na builder and an architect about structure building.\nIGLU: Interactive Grounded Language Understand-\ning in a Collaborative Environment has emerged a\ncompetition to explore interactions in a Minecraft\nenvironment. (Kiseleva et al., 2021).\nThe model we explore here is distinct from the\nprevious Minecraft-related work in that it generates\nnovel code that allows the NPC 1) to perform con-\ntextually viable actions (moving around, mining,\ncrafting, etc), 2) to answer questions about struc-\ntured Minecraft data (such as crafting recipes) and\n26\n3) to engage in multi-turn conversations.\nThis richness does not emerge in a vacuum:\nit draws on several convergent lines of research.\nLarge pre-trained language models (PLMs) such\nas GPT-2 (Radford et al., 2019), GPT-3 (Brown\net al., 2020) and GPT-J have become the predom-\ninant paradigm for text generation. Research in\nneural modelling of dialog has focused on pow-\nerful new models derived from these, such as Di-\naloGPT(Zhang et al., 2020), Meena (Adiwardana\net al., 2020), PLATO-XL(Bao et al., 2021), and\nLaMDA (Thoppilan et al., 2022) that offer rich po-\ntential for open-ended conversational applications.\nThe application of new prompting functions to\nlarge PLMs enables them to perform few-shot or\nzero-shot learning to adapt to new scenarios with lit-\ntle or no data (Liu et al., 2021). This approach, too,\nis rapidly being mainstreamed in dialog generation.\n(Madotto et al., 2021) employ prompting to select\ndifferent dialogue skills, access multiple knowl-\nedge sources, generate human-like responses, and\ntrack user preferences. (Zheng and Huang, 2021)\nuse prompt-based few-shot learning for grounded\ndialog generation, in an approach similar to ours.\nPLMs that have been tuned on code reposito-\nries, typically GitHub, are have begun to be used\nto automate coding processes and generate code\naccording to programmer’s textual specifications,\ne.g., (Chen et al., 2021) and PaLm-Coder (Chowd-\nhery et al., 2022). (Shin and Durme, 2021) suggest\nthat models pre-trained on code may also benefit\nsemantic parsing for natural language understand-\ning. (Nijkamp et al., 2022) explore conversational\nprogram synthesis within this framework, and is\nclose in spirit to the current work by virtue of its\nfocus on emergent conversational properties.\n3 Methodology\nOur model is based on few-shot prompting of a\nlarge language model, in which a small number\nof sample instances in the prompt generalize to\nnew unseen input (Brown et al., 2020). We use\nCodex (the code-davinci-002 model) and the Mine-\nflayer API, together with MineCraft (Java Edition\nv.1.17.1). Our goal is to have the NPC respond\nto the player’s input appropriately according to\nwhether the input requires a purely natural lan-\nguage response or a call to a function to perform\nsome action. Figure 2 shows a section of the\nprompt we provide to the model. The prompt\nbegins with the following statement: “ This file\ncontains Minecraft bot commands and the code\nneeded to accomplish them using the Mineflayer\nJavaScript library. If asked something conversa-\ntional, the bot should use bot.chat() to answer. \"\nThis tells the model that the prompt includes nat-\nural language commands and the code needed to\naccomplish them. We include in the prompt the nat-\nural language commands and the code that need to\nbe generated to enable basic NPC functionalities3.\nA new command from the player is appended\nto this seed prompt and sent to the Codex model.\nIn the abstracted code, we evaluate the generated\ncompletion. When the completion includes a func-\ntion call to the game API, the corresponding action\nis performed by the NPC inside the game. When\nit includes a call to the bot.chat() function, (dis-\ncussed below) the response string is displayed on\nthe chat interface. For each subsequent input, the\nprompt includes the seed prompt plus the previous\nplayer commands and model completions. When\nthe prompt exceeds the allowed token limit (2048\ntokens), we revert to the seed prompt and report to\nthe player that the context has been reset.\nWe further refine this prompting approach using\nthe following strategies:\nUsing a stop sequence:Since we want only to\ngenerate NPC responses (and not an entire conver-\nsation), we use a stop sequence (comment operator).\nPlayer input always starts with the stop sequence.\nSyntactic sugaring:The Mineflayer API con-\ntains lower-level functions that might be hard to\nmap to a natural language command. We therefore\nwrap it in more abstract code 4 to be handled by\nthe Codex model, e.g., the functions locateBlock,\nopenChest and listItemsInChest in Figure 2.\nUsing the bot.chat() function: We use the chat\ninterface within the Minecraft game for interac-\ntion between player and NPC. The model calls the\nbot.chat() function whenever the NPC needs to re-\nspond using natural language.\nFunction chaining:Player instructions may re-\nquire the NPC to perform multiple actions, in par-\nticular, map to multiple function calls where sub-\nsequent calls depend on the success or failure of\nprevious calls. In Figure 2, the instruction “open\nthe chest\" triggers a chain of functions where the\nbot first locates the chest, opens it, then finally\nresponds with the result.\n3A full list of these functions is provided in Appendix A\n4We release the wrapper code in this GitHub\nrepository https://github.com/microsoft/\ninteractive-minecraft-npcs\n27\nFigure 3: Sample of the second prompt that gets called during question answering.\nAutoregressive prompting: Also known as\nRetrieval-Augmented Generation (RAG) (Lewis\net al., 2020). For prompts that require knowledge\nof game state, e.g., inventory/crafting queries, we\ncreate a call through Codex that first gathers the\nrequisite information and then self-generates a call\nto itself with the needed information.The last com-\nmand shown in Figure 2 responds to crafting ques-\ntions by first obtaining an ingredient list, then calls\ncreateQueryPrompt to generate a secondary com-\npletion on a sub-prompt (Figure 3) using the data.\n4 User Study\nWe conduct a user study to evaluate our NPC.\nWe invite eight participants who have previously\nplayed Minecraft. Each participant had an hour to\ncomplete the study and to answer the post study\nsurvey questions. The study consists of five parts:\n4.1 Get Crafting Recipe\nParticipants interacted with the NPC to determine\nthe ingredients and their count necessary to craft 5\nitems: wooden pickaxe, furnace, clock, pumpkin\npie, and any resource of their choice.\nTable 4 shows sample player inputs and gener-\nated code. We find that participants used different\nphrases (see table 1) to frame their questions and\nthe NPC was usually able to correctly map these\nto the right function call. It was able handle mi-\nnor variation in the resource name (‘wood pick-\naxe’ instead of ‘wooden pickaxe’), misspellings\n(‘fornace’), contextual phrasing (‘what goes into\na pumpkin pie’ since its a food item), and non-\nquestion phrasing (‘recipe for clock’). Some partic-\nipants held extensive conversations with the NPC\nFigure 4: Participant interacting with the NPC to get the\ncrafting recipes of different resources\nwhere they asked additional questions about the\nresource or its ingredients5.\nOverall, across all participants, excluding cases\nwhere the resource did not have a crafting recipe,\nthe success rate for the 4 specified resources was\n85%, whereas that for the ‘any resource’ category\nwas 75%. The fail cases split into two categories:\nthose where resources had a crafting recipe and\nthose where resources did not. When a resource\nhad a crafting recipe, all fail cases belonged to\nthe ‘natural language response instead of function\ncall’ category. This happened primarily when the\nquery was preceded by a lengthy language-only\nconversation without calls to code. Table 2 includes\nfail cases for resources without a crafting recipe. In\nthe first two cases, the NPC’s response (“could not\nfind recipe”) seems appropriate but uninformative.\nIn the third case the NPC hallucinates a recipe:\nnether portal is not an object that can be crafted.\n5Analysis included in the Appendix.\n28\nResource User query NPC Response\nWooden pickaxe what do I need to create a wood pickaxe? You need 3 oak planks and 2 sticks to make\na wooden pickaxe.\nFurnace how can I build a fornace? You need 8 cobblestones to make a furnace.\nPumpkin pie what goes into a pumpkin pie? You need 1 pumpkin, 1 egg and 1 sugar to make\na pumpkin pie.\nClock what items are needed to build a clock, yo? You need 4 gold ingots and 1 redstone to make a clock.\nClock recipe for clock You need 4 gold ingots and 1 redstone to make a clock.\nBeacon what do I need for a beacon? You need 5 glass, 1 nether star, and 3 obsidian to make\na beacon.\nTable 1: Different participant queries and NPC responses to get ingredients and count required to create resource\nUser utterance NPC Response Reason for failure\nHow do I make a sword? Could not find recipe for sword There are multiple types of sword.\nhow to craft saddle Could not find recipe for saddle You can’t craft a saddle.\nhow to build nether portal You need 3 dark oak planks to Nether portal is not something you can craft.\nbuild a nether portal\nTable 2: Participant queries that failed to generate the expected response from the NPC.\n4.2 Mine Resource\nParticipants asked the NPC to mine 1 dirt and 4\noak logs. Table 3 shows sample interactions. The\nlast column shows the code generated with each\nNPC response. In the first case, in response to\nthe player command “can you get me 1 dirt?\", the\nNPC checked whether it had dirt in its inventory.\nSince it didn’t, it responded “I don’t have any dirt\nin my inventory\" and then mined the dirt for the\nplayer. However, after mining the dirt, it failed to\ngive it to the player by dropping it near them. In\nthe second case, when the user said “go cut down\nan oak log\", the model called the function cutTree\nthat did not exist. When the user followed up by\nbeing more Minecraft-specific (“mine the tree\"),\nthe model was able to generalize tree to “oak log\".\nAdditional example interactions and their analysis\nmay be found in the appendix.\n4.3 Craft Item\nParticipants asked the NPC to craft an oak plank,\na crafting table and a chest. The third example in\ntable 3 shows an interaction involving crafting an\noak plank. When the player asked if the NPC can\ncraft an oak plank, the NPC generated a chain of\nfunctions where it first obtained the ingredients of\nan oak plank and then created a query for the ques-\ntion answering prompt where it checked whether\nit had those ingredients in its inventory. Since it\ndid not have them, it threw an error message indi-\ncating that it couldn’t make the plank. The player\nfollowed up by asking what is needed to make the\nplank and the NPC responded with the recipe for\nmaking oak plank. The player then asked the NPC\nto get the oak log and make the plank. The NPC\nFigure 5: Player instructing the NPC to craft items\nthen mined the oak log and crafted the oak plank.\nSee the appendix for further examples.\n4.4 Gather Minecraft knowledge\nTo get an idea what kinds of questions a typical\nMinecraft player might want to ask an NPC, we\nhad participants ask freeform general knowledge\nquestions of the NPC. Figure 6 shows a sample in-\nteraction. Participants tested the NPC’s knowledge\nof of the game. They were interested in whether\nthe NPC can get numerical facts correct, how much\nit knows about where to mine or find resources,\nwhether can it answer comparative questions, does\nit know the purpose of different things in Minecraft,\nand does it have a personality of its own. More ex-\nample queries are found in the appendix.\n4.5 Escape from Two Rooms\nParticipants collaborated with the NPC to success-\nfully escape from two rooms. In the first, both\nplayer and NPC had to stand on two separate pres-\nsure plates simultaneously to release a diamond\npickaxe that the player could use to break out. Dif-\n29\nPlayer Utterance NPC Response Generated Code\ncan you get me 1 dirt? No, I don’t have any if(isInInventory(bot, ‘dirt’)) {\ndirt in my inventory; goToPlayer(bot, username)\nI got your dirt .then(() => dropItem(bot, ‘dirt’))\n.then(() => bot.chat(“I got your dirt\"))\n} else {\nbot.chat(“No, I don’t have any dirt in my inventory;’¨)\n.then(() => mineBlock(bot, ‘dirt’))\n.then(() => goToPlayer(bot, username))\n.then(() => bot.chat(“I got your dirt\"))\n}\ngo cut down an oak log cutTree is not defined\nmine the tree Here you go! mineBlock(bot, ‘oak_log’)\n.then(() => goToPlayer(bot, username))\n.then(() => bot.chat(“Here you go!\"))\ngive me the oak log Here you go! dropItem(bot, ‘oak_log’)\n.then(() => goToPlayer(bot, username))\n.then(() => bot.chat(“Here you go!\"))\ncan you craft an oak plank? I’ll craft you an oak plank! getIngredients(bot, ’oak_planks’)\nI couldn’t make the oak plank. .then(ingredients => createQueryPrompt(bot, ingredients,\n‘Can you craft an oak plank’))\n.then(queryPrompt => model.getCompletion(queryPrompt))\n.then(completion => evaluateCode(completion, true))\n.then(() => craftItem(bot, ’oak_planks’, 1))\n.then(() => equipItem(bot, ’oak_planks’))\n.then(success => success ? bot.chat(\"I made an oak plank!\") :\n_throw(\"I couldn’t make the oak plank\"));\ncan you get an oak log and Here you go! getIngredients(bot, ’oak_planks’)\nthen make the oak plank? I made an oak plank .then(ingredients => createQueryPrompt(bot, ingredients,\n’Can you get an oak log and then make the oak plank’))\n.then(queryPrompt => model.getCompletion(queryPrompt))\n.then(completion => evaluateCode(completion, true))\n.then(() => mineBlock(bot, ’oak_log’, 1))\n.then(() => craftItem(bot, ’oak_planks’, 1))\n.then(() => equipItem(bot, ’oak_planks’))\n.then(success => success ? bot.chat(\"I made an oak plank!\") :\n_throw(\"I couldn’t make the oak plank\"));\nTable 3: Player and NPC interaction for mining 1 dirt, mining 4 oak logs and crafting an oak plank.\nFigure 6: Player asking NPC questions about Minecraft\nferent participants used different strategies to get\nthe NPC to stand on the plate. In most cases, par-\nticipants used the phrase “stand on the pressure\nplate”. In a few cases, they used phrases such as\n“stand on this square” and “stand on that purple\ntile” which failed because the NPC lacks the ability\nto map square or purple tile to the pressure plate.\nSimilarly, the phrase “stand where I am looking”\nalso failed because NPC lacks visual capabilities.\nAll participants were able to escape the room, with\nthe average time being 2 mins 30 secs.\nIn the second room, the player was trapped in\na cobweb. A chest placed at a distance held the\ningredients necessary to make an iron sword. To\nescape, participants needed to get the NPC to make\nthe sword and give it to them. Table 4 shows a\nsample interaction between participant and NPC\nin this room. We found that the NPC is able to\ngenerate complex code (with novel function chains)\nduring these interactions. Of the 8 participants, 7\nsuccessfully escaped with the help of the NPC,\n30\nFigure 7: Participant responses for different questions\nin post study survey.\ntaking an average 4 mins 40 secs.\nUpon finishing, if time permitted, participants\nwere allowed to interact freely with the NPC.\n4.6 Post Study Survey\nParticipants filled out a survey where they rated\ntheir overall experience interacting with the NPC.\nThey were asked to judged the fluency, relevancy\nand informativeness of the NPC responses, com-\npare this NPC experience with other in-game NPC\nexperiences, and finally provide feedback on how\nthis experience could be improved.6\nFigure 7 shows the results of the survey. 6 out\nof the 7 participants7 found the interaction with the\nNPC fun and said they would interact with it if it ex-\nisted in a game, thus showing value in pursuing this\ndirection further. Some thought the NPC was help-\nful and its responses were fluent and informative.\nMost thought the responses were only somewhat\nrelevant and also often hurt the game experience.\nThis suggests the need for further improving the\nproposed model and working on the shortcomings\nof prompt-based approach.\n5 NPC Capabilities\nOn analyzing interactions in the user study, we find\nthat the NPC exhibits the following capabilities:\nParse unseen commands:The NPC can under-\nstand player commands that are not in the prompt\nbut correspond to an existing functionality. It can\nmap the command ‘make me a chest’ (not in the\nprompt) to the right function calls and create the\nchest or explain why it can’t.\nGeneralize to new functionality:For some low-\nlevel functionality, the NPC can generalize to un-\n6Appendix B contains the full list of questions.\n7Only 7 of the 8 participants filled the survey.\nseen functions. For example, since the command\n‘move forward’ is included in the prompt, the NPC\nknows how to call the right functions to move in\nother directions (backward, right and left).\nHold multi-turn conversation:The NPC can\nretain the context (both code and language) and\nmaintain a multi-turn conversation in which the\nNPC both responds using natural language and\ntakes actions within the game.\nGenerate language about code:The NPC can\nremember multi-turn context and answering ques-\ntions about the code (and the language) generated\nin previous contexts. It can answer questions such\nas “What did you just do?” and “What directions\nhave you moved?” .\nSwitch between code & language generation:\nDepending on the player command and the pre-\nvious context, the NPC is able to automatically\ndecide when to respond using natural language and\nwhen to generate a function call.\nQuestion answering: The NPC is able to an-\nswer questions about its inventory (e.g. “what do u\nhave in your inventory?”, “do you have X?”, “how\nmany of X do you have” etc), about crafting recipes\n(e.g. “how can I make a chest?”, “how many cob-\nblestones do I need to make a furnace”) and answer\nquestions that require both inventory and crafting\nrecipe information (e.g. “how many more cobble-\nstones do I need to make a furnace?”). It can also\nanswer questions generally about Minecraft. (The\ntraining data for GPT-3 includes Minecraft infor-\nmation available on the web.).\nGenerate novel function chains:Depending on\nthe player command, the NPC is able to generate\nnovel function chains by combining functions in\nan order unseen in the prompt. Table 3 includes\nmultiple examples of such novel function chains.\n6 Issues with Prompting\nWe also observed issues in our prompt-based ap-\nproach. Many of are known issues in large lan-\nguage models, and more specifically in prompt\nengineering (Reynolds and McDonell, 2021; Liu\net al., 2022) and longer conversations with agents\nin general (Xu et al., 2021). Principled solutions\npose interesting avenues for future investigation.\nCalling non-existent functions:In response to\ncommand, the NPC may attempt to call a function\nthat does not exist in the API. For example, when\nthe user asks the NPC to put a block down, the\nNPC calls the placeBlock function which is not in\n31\nPlayer Utterance NPC Response Generated Code\nget what is on the chest Looks like 64 iron ingot locateBlock(bot, ’chest’, 1)\nand 64 stick .then(chestBlock => listItemsInChest(bot, chestBlock))\n.then(response => bot.chat(’Looks like ’ + response))\ntake everything takeAllItemsFromChest is not defined\ntake the 64 iron ingots I got your iron ingots and sticks! locateBlock(bot, ’chest’, 1)\nand the 64 sticks .then(chestBlock => getItemFromChest(bot, chestBlock,\n’iron_ingot’, 64))\n.then(chestBlock => getItemFromChest(bot, chestBlock,\n’stick’, 64))\n.then(success => success ?\nbot.chat(\"I got your iron ingots and sticks!\") :\n_throw(\"I couldn’t get your iron ingots and sticks!\"))\ncraft an iron sword OK I’ll craft you an iron sword! getIngredients(bot, ’iron_sword’)\nI made an iron sword!. .then(ingredients => createQueryPrompt(bot, ingredients,\n‘craft an iron sword’))\n.then(queryPrompt => model.getCompletion(queryPrompt))\n.then(completion => evaluateCode(completion, true))\n.then(() => craftItem(bot, ’iron_sword’, 1))\n.then(() => equipItem(bot, ’iron_sword’))\n.then(success => success ? bot.chat(\"I made an iron sword!\") :\n_throw(\"I couldn’t make the iron sword\"));\ngive me the iron sword Here you go! goToPlayer(bot, username)\n.then(() => dropItem(bot, ‘iron_sword’, 4) )\n.then(() => bot.chat(“Here you go!\"))\nTable 4: Player utterance, NPC response and the code generated during the second escape room interaction.\nthe codebase. This might solved by providing the\nmodel with a list of existing functions (perhaps in\nthe prompt), but a more principled solution may lie\nin fine-tuning the model on the game API itself.\nContext exceeding prompt token limit:When\nthe conversation exceeds the prompt’s token limit,\nthe prompt needs to be reset. This makes the NPC\nlose the context of the conversation. Instead of eras-\ning the conversational history, it may be possible to\nprune irrelevant parts of the context to keep within\nthe token limit. Some form of multi-stage prompt-\ning (Liu et al., 2022) may provide a solution.\nConversational response instead of function\ncall: The NPC sometimes responds conversation-\nally when the correct behavior would be to call a\nfunction. We observe that this happens when the\nplayer’s command is preceded by a long language-\nonly conversation thus priming the model for a\nlanguage only response. On the other hand, if the\npreceding context includes function calls, then the\nsame user command triggers the right function call.\nIt may be difficult to fix this issue purely by prompt\nengineering. A better solution may be to fine-tune\nthe Codex model on curated player NPC conversa-\ntions that include by function calls.\nFactual Inaccuracies: When the player asks\ngeneral questions about Minecraft, the NPC gets\nthe answer wrong. Table 11 includes instances of\nfactual inaccuracies. A potential fix could be to\nincorporate a mechanism whereby the NPC can\nrefer to an external knowledge source, e.g., as in\nretrieval-augmented methods (Lewis et al., 2020).\nInconsistencies: The NPC does not always have\na consistent persona. In a few cases, it responds\nwith a different answer for the same user query\ndepending on the context, even when the question\npertains to something that shouldn’t change with\nthe context. This could be addressed by enforcing\na strategy wherein the NPC maintains its persona\nthroughout the conversation; Again, multi-stage\nprompting (Liu et al., 2022) may help.\nRepetition: The NPC starts repeating itself.\nThis is especially likely when player and NPC en-\ngage in a long conversation that doesn’t involve\ncalls to code. In table 11, the player queries “what\nhave you built?” and “have you built a house?”,\nreceive the same response: “I have built a lot of\nthings”. This may be addressable by metaprompt\nprogramming (Reynolds and McDonell, 2021) or\nmulti-stage prompting (Liu et al., 2022).\nRecency bias: The NPC can be biased by the\nmost recent context and answers questions incor-\nrectly. For example, if player has been convers-\ning about things found in an ocean, and then asks\n“where is the best place to look for diamonds?”, the\nNPC responds incorrectly “The best place to look\nfor diamonds is in the ocean”. Retrieval-augmented\nmethods, e.g., (Lewis et al., 2020; Xu et al., 2021),\nmay provide the needed factual grounding.\n32\n7 Conclusion\nCodex-powered NPCs can integrate both conver-\nsational and task-oriented language interactions al-\nmost seamlessly with code generation in asset-rich\ncontexts, and suggest huge potential for new kinds\nof gaming experience, including the generation of\nside quests (Appendix F). Gaming, moreover, is a\nrich sandbox-like environment for exploring com-\nplex agent interactions with code and addressing\nissues faced by large language models. The behav-\nior of NPCs shed light many of the challenges en-\ncountered by large pretrained models of language\nand code in sustaining persona, goals, and intents\nover the course of interactions. It remains to be\nseen whether solutions can be found within exist-\ning training and tuning strategies or whether they\nmust be sought outside these models. These are im-\nportant, ongoing research questions, as are the huge\nchallenges remaining in mapping these interactions\nto image recognition and to game state.\nEthical Considerations\nThe use of very large language models runs the risk\nof exposing users to offensive or sensitive language\nthat might be contained in training data. Poten-\ntial harms include, but are not limited to, offensive\nreferences to classes of people and beliefs, encour-\nagement of violence outside the game, and socially\ninappropriate sexual references. Any implemen-\ntation outside a sandboxed research environment\nwill need to build guardrails appropriate to the au-\ndience and game environment, and especially to\nprovide protections for minors. In addition, im-\nplementations must be able to handle adversarial\nprobes designed to elicit offensive language.\nA further concern is that this technology may\nmake it easier for users to manipulate NPCs to per-\nform in socially inappropriate ways or to construct\nsocially inappropriate objects. Longer-term, the\nability to enable users themselves to generate code\nthat can affect game state may pose security threats.\nAcknowledgments\nWe would like to thank the eight participants from\nthe XBOX team at Microsoft who volunteered\nto the user study. Their insights and feedback\nhelped us shape our analysis. We would also like\nto thank Ali Kazim-Zaidi, Vlad Naoumoff, Paige\nNorth, Richard Shin, Georgia Clarke, Karan Gulati\nand Haiyan Zhang who were the participants of a\nHackathon at Microsoft that generated many of the\nideas in this project. We also thank Nebojsa Jojic,\nJorge Leandro, Brannon Zhand, Karmel Allison,\nSteve Latta and Mike Ammerlaan for all the useful\ndiscussions. We would also like to thank the two\nanonymous reviewers for their useful comments\nand suggestions.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R. So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\nand Quoc V . Le. 2020. Towards a human-like open-\ndomain chatbot. CoRR, abs/2001.09977.\nSiqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng\nWang, Wenquan Wu, Zhihua Wu, Zhen Guo, Hua Lu,\nXinxian Huang, Xin Tian, Xinchao Xu, Yingzhan\nLin, and Zhengyu Niu. 2021. PLATO-XL: exploring\nthe large-scale pre-training of dialogue generation.\nCoRR, abs/2109.09519.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\n33\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJonathan Gray, Kavya Srinet, Yacine Jernite, Hao-\nnan Yu, Zhuoyuan Chen, Demi Guo, Siddharth\nGoyal, C Lawrence Zitnick, and Arthur Szlam. 2019.\nCraftassist: A framework for dialogue-enabled inter-\nactive agents. arXiv preprint arXiv:1907.08584.\nPrashant Jayannavar, Anjali Narayan-Chen, and Julia\nHockenmaier. 2020. Learning to execute instructions\nin a Minecraft dialogue. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2589–2602, Online. Association\nfor Computational Linguistics.\nMatthew Johnson, Katja Hofmann, Tim Hutton, and\nDavid Bignell. 2016. The malmo platform for artifi-\ncial intelligence experimentation. In IJCAI.\nJulia Kiseleva, Ziming Li, Mohammad Aliannejadi,\nShrestha Mohanty, Maartje ter Hoeve, Mikhail Burt-\nsev, Alexey Skrynnik, Artem Zholus, Aleksandr\nPanov, Kavya Srinet, Arthur Szlam, Yuxuan Sun,\nKatja Hofmann, Michel Galley, and Ahmed Awadal-\nlah. 2021. Neurips 2021 competition iglu: Interactive\ngrounded language understanding in a collaborative\nenvironment.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474. Curran Associates, Inc.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nCoRR, abs/2107.13586.\nZihan Liu, Mostofa Patwary, Ryan Prenger, Shrimai\nPrabhumoye, Wei Ping, Mohammad Shoeybi, and\nBryan Catanzaro. 2022. Multi-stage prompting for\nknowledgeable dialogue generation.\nAndrea Madotto, Zhaojiang Lin, Genta Indra Winata,\nand Pascale Fung. 2021. Few-shot bot: Prompt-\nbased learning for dialogue systems. arXiv preprint\narXiv:2110.08118.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. A conversational paradigm for program\nsynthesis. arXiv preprint.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nLaria Reynolds and Kyle McDonell. 2021. Prompt\nprogramming for large language models: Beyond the\nfew-shot paradigm. In CHI ’21: CHI Conference\non Human Factors in Computing Systems, Virtual\nEvent / Yokohama Japan, May 8-13, 2021, Extended\nAbstracts, pages 314:1–314:7. ACM.\nCaroline M. Rose. 2014. Realistic dialogue engine for\nvideo games. Electronic Thesis and Dissertation\nRepository, 2652.\nRichard Shin and Benjamin Van Durme. 2021. Few-\nshot semantic parsing with language models trained\non code. CoRR, abs/2112.08696.\nArthur Szlam, Jonathan Gray, Kavya Srinet, Yacine\nJernite, Armand Joulin, Gabriel Synnaeve, Douwe\nKiela, Haonan Yu, Zhuoyuan Chen, Siddharth Goyal,\net al. 2019. Why build an assistant in minecraft?\narXiv preprint arXiv:1907.09273.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nIgor Krivokon, Will Rusch, Marc Pickett, Kathleen S.\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed H. Chi, and\nQuoc Le. 2022. Lamda: Language models for dialog\napplications. CoRR, abs/2201.08239.\nJing Xu, Arthur Szlam, and Jason Weston. 2021. Be-\nyond goldfish memory: Long-term open-domain con-\nversation.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020. DIALOGPT : Large-scale\ngenerative pre-training for conversational response\ngeneration. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 270–278, Online. As-\nsociation for Computational Linguistics.\nChujie Zheng and Minlie Huang. 2021. Exploring\nprompt-based few-shot learning for grounded dialog\ngeneration. arXiv preprint arXiv:2109.06513.\nA NPC functionalities\nWe include in the prompt the natural language com-\nmands and the code that should be generated to\nenable the following basic NPC functionalities:\n34\n• Move forward\n• Jump\n• Look at the player\n• Come to the player\n• Follow the player\n• Locate a block\n• Mine a block\n• Get the crafting recipe of an item\n• Craft an item\n• Open a chest\n• Take items from chest\n• Put items into the chest\n• Close the chest\n• List all items in inventory\n• Check if item is in the inventory\n• Get count of item in inventory\n• Give item to player\nB Post user study survey questions\nFollowing the study, the participants filled out a\nsurvey that had the following questions:\n(a) How would you rate your skill in Minecraft?\n(b) How would you rate your skill as a gamer in\ngeneral?\n(c) Did you have fun while interacting with the\nNPC in this study?\n(d) How did you find this NPC interaction in com-\nparison to the interactions you might have had\nwith dialog-capable NPCs in other games (e.g,\nSkyrim)?\n(e) How fluent were the NPC’s responses?\n(f) How relevant were the NPC’s responses?\n(g) How informative were the NPC’s responses?\n(h) Did you feel like you were interacting with an\nNPC with a consistent persona?\n(i) When the NCP’s response was incorrect, how\nmuch did that hurt your game experience?\n(j) How magical did the experience of interacting\nwith an NPC feel? If it didn’t, can you explain\nwhy not?\n(k) What could be done to improve the experience\nof interacting with the NPC?\n(l) How useful were your interactions with this\nNPC in helping you better understand how to\nplay Minecraft?\n(m) More generally, would you prefer interacting\nwith such an NPC to learn a new game, as\nopposed to a tutorial or an FAQ?\n(n) Would you ever want to interact with an NPC\nlike this generally in any game?\n(o) What are some useful applications of this kind\nof NPC to enrich game experiences (in gen-\neral, not only in Minecraft)?\n(p) If this technology could reach the level of a\ncompanion (like an AI-gamer friend) that you\ncould take to any game, would you want to\nuse it?\nC User study analysis\nC.1 Get Crafting Recipe\nIn this part of the user study, some participants had\na longer conversation with the NPC around the re-\nsource. Table 5 includes such an interaction around\nwooden pickaxe. The NPC is able to effectively\nswitch between code calling and natural language\nresponse. In this particular example, the partici-\npant asked “can I only use oak to make a wooden\npickaxe, why not pine?” to understand if the NPC\nknows that wooden pickaxes can be of various dif-\nferent types. Interestingly, the NPC first called the\nrecipe function to respond that you need sticks in\naddition to oak planks to make the pickaxe. And\nthen it generated a natural language response where\nit said you can use any wood to make the pickaxe.\nThis shows the kind of complex behavior that can\nbe generated by the codex model.\nTable 6 is an example of a fail case for question\nanswering. Although the initial query ‘how do\nI make a furnace’ correctly mapped to a recipe\nfunction call, the later query ‘How do I make a\nclock’ failed to do so since it was preceded with a\nlength language-only conversation. The participant,\nhowever, was able to recover by asking a more\nspecific question ‘in terms of raw materials, what\ndo I need to make a clock?’.\nC.2 Mine Resource\nTable 7 shows sample interactions for mining task.\nIn the first case, the user’s query was similar to\nthat in table 3, however this time the NPC only\nchecked for dirt in its inventory and when it didn’t\nfind any dirt, it said “I have no dirt\". But it did not\n35\nPlayer utterance NPC response\nI want a wooden pickaxe I don’t have the ingredients needed for a wooden pickaxe!\nI couldn’t make the wooden pickaxe\nhow do I make a wooden pickaxe You need 3 oak planks and 2 sticks to make\na wooden pickaxe.\nwhere do I get planks? You need to craft oak planks from oak logs.\nYou need to chop down some oak trees.\nhow many logs do I need for 3 planks? You need 3 logs to make 3 planks\ndoes it have to be oak? Yes, you will need 1 oak log to make 4 oak planks.\nYes, you will need to use oak logs to make oak planks.\ncan I only use oak to make a wooden pickaxe, why not pine? No, you can’t make a wooden pickaxe with only oak planks,\nyou need to use oak planks and sticks.\nYou can use any wood to make a wooden pickaxe.\nTable 5: Interaction between a participant and NPC around creation of a wooden pickaxe\nPlayer utterance NPC response\nhow do I make a furnace You need 8 cobblestones to make a furnace.\nwhere do I find cobblestone? You can find cobblestone in the ground\ndoes depth matter? No, depth does not matter.\ndo I need anything to collect cobblestone? You need a shovel to collect cobblestone\nhow do I pick up cobblestone? You need to mine cobblestone\nhow do you mine cobblestone? You need a pickaxe to mine cobblestone\nHow do I make a clock? You need a clock to tell the time.\nis there a way to make a clock I think you can make a clock by combining a clock\nand a compass\nin terms of raw materials, what do I need to make a clock? You need 4 gold ingots and 1 redstone to make a clock.\nTable 6: Sample interaction where the NPC failed to map user query to recipe function for a resource that has a\ncrafting recipe. The user query preceded with lengthy context that required purely natural language response.\nfollow it with mining the dirt as it did in the first\ncase. When the user explicitly used the term ‘mine’\nin their next utterance, the NPC mined the dirt for\nthem. This suggests that the model is not able to\nalways generalize “bring me\" to the mine action.\nIn the second case, the player was more direct and\nsaid “mine dirt\" and in response the NPC mined\nthe dirt. Next the player said “drop dirt\" and the\nNPC went to the player and dropped the dirt close\nto the player.\nThe third case is similar to the first. The notable\ndifference is that the model is able to understand\nthe compound instruction “can you mine some dirt\nand give it to me\" and correctly generates the code\nto first mine and then drop the item near the user.\nThe fourth case depicts an instance of fail case be-\nlonging to the ‘natural language response instead of\nfunction call’ category. Although this player used\nthe same utterance as in the first case, the NPC’s\nresponse was purely natural language. When the\nplayer followed this with “get me one dirt\" and “i\nwant one dirt\", both times, the model tried to call a\nfunction getItem() that did not exist. After doing a\nreset of the prompt, however, the user was able to\nget the NPC to mine them the dirt.\nThe fifth case is a successful interaction around\nmining of oak logs. In additon to generating the\ncorrect code, the NPC’s responses (“I’m chopping\nthe oak logs\" and “I dropped the oak logs\") were\ncustomized to the player’s phrasing (“please chop\noak logs\" and “drop oak logs\"). The sixth case de-\npicts a success case for mining of 4 oak logs. The\nlast case depicts a failure case. when the user says\n“bring me some oak logs please\", they were expect-\ning the NPC to mine the logs and then give them\nto the user. However, the model only generated\nthe code for mining the oak logs. When the user\nfollowed it up with a verbose utterance (“you are\nholding the logs, please throw them at my feed\"),\nthe model was not able to map this to dropping of\nthe oak logs. Likewise, the phrase “pass me the\nlogs\" also did not map to dropping of the oak logs.\nInstead, in both these cases it generated a purely\nnatural language response (“I am holding the oak\nlogs\").\nC.3 Gather Minecraft Knowledge\nTable 11 contains example queries where par-\nticipants tested NPC’s general knowledge about\nMinecraft. We group them by different aspects.\nIn the first four aspects, the models gets several\nquestions wrong, suggesting the need for the inte-\ngration of Minecraft specific knowledge base into\nthe model. The last aspect (personality of the NPC)\n36\nFigure 8: Clues for the first escape room: ‘I wonder\nwhat these pressure plates do?’, ‘I think we need to\nstand on both at the same time’.\nFigure 9: Clues for the second escape room: ‘Maybe\nthat chest has what I need to get out’, ‘I think I can get\nout if I get my hands on an Iron Sword’\nsuggests that users would like the NPC to have a\npersona and a history of its previous game experi-\nence.\nD Details of Escape Rooms\nBoth rooms had clues written on the wall or on\nboards that helped users figure out the path to es-\ncape. Figures 8 and 9 shows the clues written on\nthe wall/boards in the two rooms.\nE Need for Visual Capabilities\nDuring the user study, we found multiple instances\nwhere participants were expecting the NPC to be\nable to see the things in the games, just like a player\ndoes. Figure 10 shows one such instance where\nplayer pointed at a crafting table and asked the\nNPC “what is this”. The NPC responded “this is a\nchat” since it had only textual context. Figure 11\nshows another instance where participant pointed at\na location and told the NPC “place the crafting table\nhere”. Lastly, during the escape room interaction,\none participant looked at the pressure plate and\ntold the NPC “stand where I am looking”. These\ncases strongly suggest the need for the NPC to have\nvisual capabilities.\nFigure 10: Player pointing at an object and asking NPC\n‘what is this?’\nFigure 11: Player pointing at a location and asking the\nNPC to place a block there.\nFigure 12: Player asking the NPC to stand where they\nare looking.\n37\nF Side Quest Generation\nWe also experiment with the idea of side quest gen-\neration. Side quests represent a large amount of\ncontent that players engage with, they spend hours\nwandering worlds based on instructions given by\nNPCs. The more quests we can create automat-\nically, the longer the player can spend exploring\ntheir favorite world. This technology is meant for\nopen-world games like in Fallout, the OuterWorlds,\nor Skyrim. We design a separate prompt with few\nsample quests. When the player asks for a quest,\nthe NPC generates a new quest where it requests\none or more items from the player and devises a\nrelevant backstory as to why the requested items\nare needed.The NPC then checks whether or not\nthe player has completed the quest, and provides\na reward if so. The necessary code to execute this\nlogic is generated by the Codex model.\n1. For example, if the player asks, \"Do you have\na quest for me?\". User (U) says hello to Quest-\nGiver (Q), which is a prompt-based model\nthat is given some summary of the state of the\ngame (including items that exist in the game).\n2. Q specifies in response: (a) backstory; (b)\nreward; (c) quests; and (d) code logic that\nchecks for quest completion. Quests are items\nto retrieve. An example of a quest backstory\nwould be: \"The recent festival was wonderful,\nbut I fear my children were just a bit too ram-\nbunctious. I hear there is a new potion for sale\nthat would slow down anyone who drinks it. I\nwould like to try it out on my kids. Please get\nme some\", in reference to a slowness potion.\n3. U gets an item X\n4. U says to Q some paraphrase of \"I got the\nX that you asked for\", this triggers code that\nresults in U /give X to Q, followed by Q in-\nspecting inventory to verify it was given X\nand that it previously asked for X\n5. If all quests completed, Q says \"here is your\nreward\", with an associated /give to U\n38\nPlayer Utterance NPC Response Generated Code\nPlease bring me 1 dirt I have no dirt if(isInInventory(bot, ‘dirt’)) {\ngoToPlayer(bot, username)\n.then(() => dropItem(bot, ‘dirt’))\n.then(() => bot.chat(“Here you go!\"))\n} else {\nbot.chat(“I have no dirt\");\n}\nplease mine 1 dirt for me Here you go! mineBlock(bot, ‘dirt’)\n.then(() => goToPlayer(bot, username))\n.then(() => bot.chat(“Here you go!\"))\nmine dirt I mined the dirt! mineBlock(bot, ‘dirt’)\n.then(() => goToPlayer(bot, username))\n.then(() => bot.chat(“I mined the dirt\"))\ndrop dirt I dropped the dirt! goToPlayer(bot, username)\n.then(() => dropItem(bot, ‘dirt’))\n.then(() => bot.chat(“I dropped the dirt!\"))\ncan you get me a piece of dirt I have no dirt if(isInInventory(bot, ‘dirt’)) {\ngoToPlayer(bot, username)\n.then(() => dropItem(bot, ‘dirt’))\n.then(() => bot.chat(“Here you go!\"))\n} else {\nbot.chat(“I have no dirt\");\n}\ncan you mine some dirt and Here you go! mineBlock(bot, ‘dirt’)\ngive it to me .then(() => goToPlayer(bot, username))\n.then(() => dropItem(bot, ‘dirt’))\n.then(() => bot.chat(“Here you go!\"))\ncan you get me one dirt? I can get you some dirt!\nget me one dirt getItem is not defined\ni want one dirt getItem is not defined\nreset\nbring me one dirt I have no dirt if(isInInventory(bot, ‘dirt’)) {\ngoToPlayer(bot, username)\n.then(() => dropItem(bot, ‘dirt’))\n.then(() => bot.chat(“Here you go!\"))\n} else {\nbot.chat(“I have no dirt\");\n}\nmine one dirt I got your dirt mineBlock(bot, ‘dirt’)\n.then(() => goToPlayer(bot, username))\n.then(() => bot.chat(“I got your dirt\"))\nplease chop oak logs I’m chopping the oak logs mineBlock(bot, ‘oak_log’)\n.then(() => bot.chat(“I’m chopping the oak logs\"))\ndrop oak logs I dropped the oak logs dropItem(bot, ‘oak_log’)\n.then(() => goToPlayer(bot, username))\n.then(() => bot.chat(“I dropped the oak logs\"))\nI need 4 oak logs. Can you help? Here you go! mineBlock(bot, ‘oak_log’, 4)\n.then(() => goToPlayer(bot, username))\n.then(() => bot.chat(“Here you go!\"))\nplease give me the oak logs Here you go! dropItem(bot, ‘oak_log’, 4)\n.then(() => goToPlayer(bot, username))\n.then(() => bot.chat(“Here you go!\"))\nbring me some oak logs please I got your oak logs mineBlock(bot, ‘oak_log’)\n.then(() => goToPlayer(bot, username))\n.then(() => bot.chat(“I got your oak logs\"))\nyou are holding the logs, I am holding the oak logs\nplease throw them at my feet pass me the logs\nI am holding the oak logs\nTable 7: Player utterance, NPC response and generated code during mining interaction.\n39\nPlayer Utterance NPC Response Generated Code\ncraft an oak plank OK, I’ll craft you an oak plank! getIngredients(bot, ’oak_planks’)\nI made an oak plank .then(ingredients => createQueryPrompt(bot,\ningredients, ’Craft an oak plank’))\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\n.then(() => craftItem(bot, ’oak_planks’, 1))\n.then(() => equipItem(bot, ’oak_planks’))\n.then(success => success ?\nbot.chat(\"I made an oak plank!\") :\n_throw(\"I couldn’t make the oak plank\"));\ngive me the oak plank Here you go! goToPlayer(bot, 3, username)\n.then(success => success ?\ndropItem(bot, ’oak_planks’, 1) :\n_throw(\"I couldn’t drop the oak plank\"))\n.then(success => success\n&& bot.chat(\"Here you go!\"));\nhow do you make an oak plank? You need 1 oak log to make 1 oak getIngredients(bot, ’oak_planks’)\nplank. I made an oak plank! .then(ingredients =>\ncreateQueryPrompt(bot, ingredients,\n’How do you make an oak plank’))\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\n.then(() => craftItem(bot, ’oak_planks’, 1))\n.then(() => equipItem(bot, ’oak_planks’))\n.then(success => success ?\nbot.chat(\"I made oak planks!\") :\n_throw(\"I couldn’t make the oak plank\"));\nmake me an oak plank I’ll make some oak planks! getIngredients(bot, ’oak_planks’)\nI couldn’t make the oak plank .then(ingredients =>\ncreateQueryPrompt(bot, ingredients,\n’Make an oak plank’))\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\n.then(() => craftItem(bot, ’oak_planks’, 1))\n.then(() => equipItem(bot, ’oak_planks’))\n.then(success => success ?\nbot.chat(\"I made oak planks!\") :\n_throw(\"I couldn’t make the oak plank\"));\n<user dropped 1 oak log> <NPC picked it up the oak log>\nmake me an oak plank OK, I’ll make you an oak plank. getIngredients(bot, ’oak_planks’)\nI made oak planks! .then(ingredients =>\ncreateQueryPrompt(bot, ingredients,\n’Make an oak plank’))\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\n.then(() => craftItem(bot, ’oak_planks’, 1))\n.then(() => equipItem(bot, ’oak_planks’))\n.then(success => success ?\nbot.chat(\"I made oak planks!\") :\n_throw(\"I couldn’t make the oak plank\"));\nTable 8: Player utterance, NPC response and generated code for interaction around crafting an oak plank.\n40\nUser Utterance NPC Response Generated Code\nbuild a crafting table I’ll build you a crafting table. getIngredients(bot, ’crafting_table’)\nI built a crafting table .then(ingredients =>\ncreateQueryPrompt(bot, ingredients,\n’Build a crafting table’))\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\n.then(() => craftItem(bot, ’crafting_table’, 1))\n.then(() => equipItem(bot, ’crafting_table’))\n.then(success => success ?\nbot.chat(\"I built a crafting table!\") :\n_throw(\"I couldn’t build a crafting table\"))\nplease make a crafting tabel OK, I’ll make you a crafting table. getIngredients(bot, ’crafting_table’)\nI couldn’t make the crafting table. .then(ingredients =>\ncreateQueryPrompt(bot, ingredients,\n’Please make a crafting table’))\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\n.then(() => craftItem(bot, ’crafting_table’, 1))\n.then(() => equipItem(bot, ’crafting_table’))\n.then(success => success ?\nbot.chat(\"I built a crafting table!\") :\n_throw(\"I couldn’t build a crafting table\"))\nwhat do you need to make You need 4 oak planks to make getIngredients(bot, ’crafting_table’)\na crafting table? a crafting table. .then(ingredients =>\ncreateQueryPrompt(bot, ingredients,\n’What do you need to make a crafting table’))\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\nmake me 4 oak planks please OK, I’ll make you 4 oak planks! getIngredients(bot, ’oak_planks’)\nI made 4 oak planks. .then(ingredients =>\ncreateQueryPrompt(bot, ingredients,\n’Make me 4 oak planks’))\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\n.then(() => craftItem(bot, ’oak_planks’, 4))\n.then(() => equipItem(bot, ’oak_planks’))\n.then(success => success ?\nbot.chat(\"I made 4 oak planks!\") :\n_throw(\"I couldn’t make the 4 oak planks\"))\nwhat resources do you have now? I have 16 oak planks in createQueryPrompt(bot, ,\nmy inventory ’What resources do you have now’)\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\nplease craft a crafting table OK, I’ll craft you a crafting table! getIngredients(bot, ’crafting_table’)\nI made a crafting table. .then(ingredients =>\ncreateQueryPrompt(bot, ingredients,\n’Craft a crafting table’))\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\n.then(() => craftItem(bot, ’crafting_table’, 1))\n.then(() => equipItem(bot, ’crafting_table’))\n.then(success => success ?\nbot.chat(\"I made a crafting table!\") :\n_throw(\"I couldn’t make the crafting table\"));\nTable 9: Player utterance, NPC response and generated code for interaction around crafting a crafting table.\n41\nUser Utterance NPC Response Generated Code\nhow do you make a chest? You need 8 oak planks to make a chest. getIngredients(bot, ’chest’)\nI couldn’t make the chest. .then(ingredients =>\ncreateQueryPrompt(bot, ingredients,\n’How do you make a chest’))\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\nmake oak planks OK, I’ll make you some oak planks! getIngredients(bot, ’oak_planks’)\nI couldn’t make the oak planks .then(ingredients =>\ncreateQueryPrompt(bot, ingredients,\n’Make oak planks’))\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\n.then(() => craftItem(bot, ’oak_planks’, 8))\n.then(() => equipItem(bot, ’oak_planks’))\n.then(success => success ?\nbot.chat(\"I made oak planks!\") :\n_throw(\"I couldn’t make the oak planks\"))\nchop oak I chopped 4 oak logs! locateBlock(bot, ’oak_log’, 0)\n.then(success => success ?\nchopTree(bot, ’oak_log’, 4) :\n_throw(\"I couldn’t find any oak logs!\"))\n.then(success => success\n&& bot.chat(\"I chopped 4 oak logs!\"));\nmake oak planks I made oak planks! getIngredients(bot, ’oak_planks’)\n.then(ingredients =>\ncreateQueryPrompt(bot, ingredients,\n’Make oak planks’))\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\n.then(() => craftItem(bot, ’oak_planks’, 8))\n.then(() => equipItem(bot, ’oak_planks’))\n.then(success => success ?\nbot.chat(\"I made oak planks!\") :\n_throw(\"I couldn’t make the oak planks\"))\nhow many oak planks I have 12 oak planks in my inventory createQueryPrompt(bot, ,\n’How many oak planks do you have’)\ndo you have? .then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\nmake a chest I made a chest! getIngredients(bot, ’chest’)\n.then(ingredients =>\ncreateQueryPrompt(bot, ingredients,\n’Make a chest’))\n.then(queryPrompt =>\nmodel.getCompletion(queryPrompt))\n.then(completion =>\nevaluateCode(completion, true))\n.then(() => craftItem(bot, ’chest’, 1))\n.then(() => equipItem(bot, ’chest’))\n.then(success => success ?\nbot.chat(\"I made a chest!\") :\n_throw(\"I couldn’t make the chest\"))\nplace chest placeBlock is not a function locateBlock(bot, ’chest’, 1)\n.then(chestBlock =>\nplaceBlock(bot, chestBlock))\n.then(success => success ?\nbot.chat(\"I placed the chest!\") :\n_throw(\"I couldn’t place the chest\")) height\nTable 10: Player utterance, NPC response and generated code for interaction around crafting a chest.\n42\nUser utterance NPC response\nNumerical facts\nhow much hunger does steak restore Steak restores 4 hunger\nhow much damage does a creeper do? A creeper does 5 damage\nhow many mobs spawn in a raid? There are about 20 mobs that spawn in a raid!\nwhat is the build limit? The build limit is 256 blocks!\nhow long is a Minecraft day? A day in Minecraft is 20 minutes.\nhow many different potions are there? There are currently 16 different potions.\nLocation of things\nwhere can I find sponges? You can find sponges in the ocean!\nwhere is the best place to look for diamonds? The best place to look for diamonds is in the ocean\nwhere is the best place to look for coal? The best place to look for diamonds is in the ocean\nwhere do I find a nether star? A nether star is dropped by the Wither.\nWhere do I find zombies? Zombies are in the nether!\nWhere can I find diamond Diamond can be found in the ground\nWhere do I find pigs? Pigs are in the forest!\nComparisons\nwhat has longer pulse, a stone or a wooden button? A stone has a longer pulse!\nwhich is worse, a creeper or a the dragon? The dragon is worse than the creeper\nwho would win between a skeleton and a zombie? The zombie would win!\nPurpose of things\nWhat is redstone used for? Redstone is used to power things like furnaces and redstone torches\nWhat does an oven do? An oven is uded to cook things like bread and cake\nwhat does a tree drop A tree drops oak logs!\nwhat does the ground drop? The ground drops dirt!\nwhat is a nether star used for? A nether star is used to cure a zombie villager.\nPersonality of the NPC\nare you an NPC? I’m not an NPC, I’m a bot!\nwhat do you like to do? I like to build things!\nwhat have you built? I have built a lot of things\nhave you built a house? I have built a lot of things\nwhere did you get your name? I was named after the bot in the movie ’Ace Ventura: Pet Detective’\nWhat do you like to do in Minecraft? I like to make things in Minecraft!\nWhat kinds of things do you like to make? I like to make things like furnaces, pickaxes, and crafting tables!\nTable 11: Users testing NPC’s general knowledge about Minecraft.\n43",
  "topic": "Craft",
  "concepts": [
    {
      "name": "Craft",
      "score": 0.9173035621643066
    },
    {
      "name": "SWORD",
      "score": 0.8988061547279358
    },
    {
      "name": "Code (set theory)",
      "score": 0.6539772152900696
    },
    {
      "name": "Computer science",
      "score": 0.5756800174713135
    },
    {
      "name": "Programming language",
      "score": 0.5376421809196472
    },
    {
      "name": "Linguistics",
      "score": 0.38611018657684326
    },
    {
      "name": "Art",
      "score": 0.36875593662261963
    },
    {
      "name": "Humanities",
      "score": 0.3533214330673218
    },
    {
      "name": "Literature",
      "score": 0.34177452325820923
    },
    {
      "name": "Visual arts",
      "score": 0.28513652086257935
    },
    {
      "name": "Operating system",
      "score": 0.14040157198905945
    },
    {
      "name": "Philosophy",
      "score": 0.1337299346923828
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210105678",
      "name": "Microsoft (Finland)",
      "country": "FI"
    }
  ],
  "cited_by": 25
}