{
  "title": "Iterative refinement and goal articulation to optimize large language models for clinical information extraction",
  "url": "https://openalex.org/W4410687020",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2126721599",
      "name": "David Hein",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2149982640",
      "name": "Alana Christie",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2098354103",
      "name": "Michael Holcomb",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2265561759",
      "name": "Bingqing Xie",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5072456271",
      "name": "AJ Jain",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2540292656",
      "name": "Joseph Vento",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5116255452",
      "name": "Neil Rakheja",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A3132790536",
      "name": "Ameer Hamza Shakur",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2304088577",
      "name": "Scott Christley",
      "affiliations": [
        "Southwestern Medical Center",
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A1978413820",
      "name": "Lindsay G. Cowell",
      "affiliations": [
        "Southwestern Medical Center",
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2306128497",
      "name": "James Brugarolas",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2106147619",
      "name": "Andrew R. Jamieson",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2170828867",
      "name": "Payal Kapur",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2126721599",
      "name": "David Hein",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2149982640",
      "name": "Alana Christie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098354103",
      "name": "Michael Holcomb",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2265561759",
      "name": "Bingqing Xie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5072456271",
      "name": "AJ Jain",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2540292656",
      "name": "Joseph Vento",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5116255452",
      "name": "Neil Rakheja",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3132790536",
      "name": "Ameer Hamza Shakur",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2304088577",
      "name": "Scott Christley",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978413820",
      "name": "Lindsay G. Cowell",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2306128497",
      "name": "James Brugarolas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106147619",
      "name": "Andrew R. Jamieson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2170828867",
      "name": "Payal Kapur",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3181361218",
    "https://openalex.org/W1865128887",
    "https://openalex.org/W2884934935",
    "https://openalex.org/W4392740458",
    "https://openalex.org/W4404105438",
    "https://openalex.org/W4387526446",
    "https://openalex.org/W1965380368",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2951675429",
    "https://openalex.org/W4226191767",
    "https://openalex.org/W4310568840",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4363671827",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4281644150",
    "https://openalex.org/W4391292768",
    "https://openalex.org/W4394579747",
    "https://openalex.org/W4394869910",
    "https://openalex.org/W4401542090",
    "https://openalex.org/W4390064615",
    "https://openalex.org/W4396553888",
    "https://openalex.org/W4404867176",
    "https://openalex.org/W4396722287",
    "https://openalex.org/W4395467677",
    "https://openalex.org/W4391301614",
    "https://openalex.org/W4399854538",
    "https://openalex.org/W4378464928",
    "https://openalex.org/W4385381606",
    "https://openalex.org/W4386120650",
    "https://openalex.org/W4401847921",
    "https://openalex.org/W4381587418",
    "https://openalex.org/W4385014449",
    "https://openalex.org/W4405023514",
    "https://openalex.org/W4393157213",
    "https://openalex.org/W147910726",
    "https://openalex.org/W4391901128",
    "https://openalex.org/W4403798678",
    "https://openalex.org/W4404330874",
    "https://openalex.org/W2124014074",
    "https://openalex.org/W4401306886",
    "https://openalex.org/W4405655184",
    "https://openalex.org/W4386438154",
    "https://openalex.org/W2149441684",
    "https://openalex.org/W2114843025",
    "https://openalex.org/W4392005379",
    "https://openalex.org/W1908864745",
    "https://openalex.org/W4384831237",
    "https://openalex.org/W2161112598",
    "https://openalex.org/W4408226143",
    "https://openalex.org/W4404718139",
    "https://openalex.org/W2896812724",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4387559558",
    "https://openalex.org/W6910494829",
    "https://openalex.org/W4391987698",
    "https://openalex.org/W3009297012",
    "https://openalex.org/W4385571331",
    "https://openalex.org/W2418239156"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01686-z\nIterative reﬁnement and goal articulation\nto optimize large language models for\nclinical information extraction\nCheck for updates\nDavid Hein1 , Alana Christie2, Michael Holcomb1,B i n g q i n gX i e3,A JJ a i n1,J o s e p hV e n t o3,N e i lR a k h e j a2,\nAmeer Hamza Shakur1, Scott Christley4, Lindsay G. Cowell4, James Brugarolas2, Andrew R. Jamieson1,6 &\nPayal Kapur2,5,6\nExtracting structured data from free-text medical records at scale is laborious, and traditional\napproaches struggle in complex clinical domains. We present a novel, end-to-end pipeline leveraging\nlarge language models (LLMs) for highly accurate information extraction and normalization from\nunstructured pathology reports, focusing initially on kidney tumors. Our innovation combinesﬂexible\nprompt templates, the direct production of analysis-ready tabular data, and a rigorous, human-in-the-\nloop iterative reﬁnement process guided by a comprehensive error ontology. Applying theﬁnalized\npipeline to 2297 kidney tumor reports with pre-existing templated data available for validation yielded\na macro-averaged F1 of 0.99 for six kidney tumor subtypes and 0.97 for detecting kidney metastasis.\nWe further demonstrateﬂexibility with multiple LLM backbones and adaptability to new domains,\nutilizing publicly available breast and prostate cancer reports. Beyond performance metrics or pipeline\nspeciﬁcs, we emphasize the critical importance of task deﬁnition, interdisciplinary collaboration, and\ncomplexity management in LLM-based clinical workﬂows.\nExtracting structured information from free-text electronic medical records\n(EMR) presents a signiﬁcant challenge due to their narrative structure, spe-\ncialized terminology, and inherent variability1. Historically, this process has\nbeen labor-intensive and error-prone, requiring manual review by medical\nprofessionals2–4, thereby hindering large-scale retrospective studies and real-\nworld evidence generation5. Consequently, automated, reliable methods are\nneeded to extract clinically relevant information from unstructured EMR text6.\nNatural language processing (NLP) techniques, including rule-based\nsystems and early neural models, have struggled with the nuances of the\nmedical domain 7,8. While transformer-based architectures like\nClinicalBERT9, GatorTron10, and others11–13, furthered the state-of-the art,\nthey often require extensiveﬁne-tuning on large annotated datasets, which\nare costly and time-consuming to create14,15. The challenge is particularly\nacute in specialized tasks like the extraction of immunohistochemistry\n(IHC) results from pathology reports, which requires identifying and\nmapping tests to the correct results and specimens, resolving synonyms, and\nnavigating diverse terminology.\nThe rapid emergence of generative large language models (LLMs)\n16\noffers a potentially transformative approach. Their large number of para-\nmeters and ability to process extensive context windows enable them to\nretain and“reason”over substantial amounts of domain-speciﬁck n o w l e d g e\nwithout ﬁne-tuning17–20. Natural language prompts allow a high degree of\nﬂexibility, enabling rapid iteration and adaptation to new entities and\ninstructions21,22.\nRecent studies report promising results using LLMs for text-to-text\nmedical information extraction23. Initial efforts have successfully extracted\nsingular/non-compound report-level information, such as patient char-\nacteristics from clinical notes24, and tumor descriptors/diagnosis from\nradiology25 and pathology reports26,27. Studies have also demonstrated the\npotential for extracting inferred conclusions, such as classifying radiology\nﬁndings28 and cancer-related symptoms4. However, challenges remain,\nparticularly factually incorrect reasoning29,30, and the potential for infor-\nmation loss when forcing complex medical concepts into discrete\ncategories31.\n1Lyda Hill Department of Bioinformatics, University of Texas Southwestern Medical Center, Dallas, Texas, USA.2Harold C. Simmons Comprehensive Cancer\nCenter, University of Texas Southwestern Medical Center, Dallas, TX, USA.3Department of Internal Medicine, Division of Hematology & Oncology, University of\nTexas Southwestern Medical Center, Dallas, TX, USA.4Department of Health Data Science and Biostatistics, Peter O’Donnell Jr. School of Public Health,\nUniverisity of Texas Southwestern Medical Center, Dallas, TX, USA.5Department of Pathology, University of Texas Southwestern Medical Center, Dallas, TX, USA.\n6These authors contributed equally: Andrew R. Jamieson, Payal Kapur.e-mail: david.hein@utsouthwestern.edu\nnpj Digital Medicine|           (2025) 8:301 1\n1234567890():,;\n1234567890():,;\nEvaluating LLM performance is complicated by the lack of standar-\ndized error categorization that accounts for clinical signiﬁcance and the\nlimitations of traditional metrics like exact match accuracy, which are ill-\nsuited for open-ended generation32–36. For example, misclassifying a test\nresult as“negative” versus “positive” is substantially different than minor\ngrammatical discrepancies between labels, e.g.,“positive, diffusely” versus\n“diffuse positive”. This open-ended style thus necessitates methods to\nconstrain generation in order to minimize requisite downstream\nnormalization\n37,38. Furthermore, many existing clinical NLP datasets utilize\nBERT-style entity tagging, limiting their use for benchmarking end-to-end\ninformation extraction33,39,40. Nonetheless, our lack of pre-annotated data,\nhigh degree of entity complexity, and desire forﬂexibility, coupled with the\nrapidly improving performance of LLMs41, prompted us to explore their\npotential.\nTo address these challenges, we developed a novel LLM-based\npipeline for end-to-end information extraction from real-world clinical\ndata. We deﬁne ‘end-to-end’here as encompassing: (1) entity identi-\nﬁcation, (2) clinical question inference (e.g., determining the ﬁnal\ndiagnosis), (3) terminology normalization, (4) relationship mapping\n(e.g., linking IHC results to speciﬁc specimens), and (5) structured\noutput generation. Our approach leverages three key innovations:\nﬂexible prompt templates with a centralized schema (Fig.1a), multi-\nFeature/Report Segmentation Template\nExtraction Schema & Report Text\nJSON Formatted Output\nFeature/Specimen Segmentation Template\nExample Report Text\nA. Anterior left sixth rib lesion:\n- Metastatic carcinoma poorly differentiated, suggestive of clear cell RCC\nB. Left lung lesion biopsy:\n- Metastatic carcinoma poorly differentiated, suggestive of clear cell RCC\nTumor is poorly differentiated, definitive histological classification is not possible.\nIHC performed on B2. CA -9 is focally positive and BAP 1 is intact (positive nuclear staining) in tumor cells. The diagnosis remains \nunchanged.\nHistology Schema\n“histology” : {\n“labels”: [\n“Clear cell RCC”, “RCC, (unclassified) ”\n],\n“segmentation”: “Extract all qualifiers of confidence in \nhistology”,\n“standardization\": “Histology must be definitive ”}\nImmunohistochemistry Schema\n“immunohistochemistry” : {\n“test names”: [“BAP-1”, “PAX-8”, “AE1/AE3”, “CK7”, “CA-IX”],\n“synonyms”: [“AE1/AE3 (pancytokeratin) ”, “CA-IX (CA-9)”],\n“test results”: {\n“status”: [“Positive”, “Negative”, “Intact”, “Loss”],\n“intensity”: [“strongly”, “weakly”],\n“extent”: [“patchy”, “diffuse”, “focal”],\n“pattern”: [“box-like”, “cup like”]\n}\n“segmentation 1”: “Capture FISH tests as well as IHC tests, \nensure that adendums with updates to results are captured ”,\n“segmentation 2”: “Use specimen X and block 0 if not stated \nin report”,\n“standardization\": “If a report states ‘Intact (positive …)’ \nfavor only ‘Intact’. Result modifiers, if any, should be added in \norder of status, intensity, extent, and pattern. Check the \nprovided list of non-obvious synonyms and standardize to the name \nprovided under `test names` ”}\nDiagnosis Schema\n“diagnosis” : {\n“labels”: [\n“Benign neoplasm of kidney ”, “Metastatic RCC)”\n],\n“segmentation”: “Extract locations of metastasis if \npresent”,\n“standardization\": “Metastatic RCC must be confirmed at a \ndistant organ other than kidney, direct tumor extension into \nneighboring organs/tissue does not constitute metastasis\"}\n(a)\nIHC/FISH Segmentation I Template\nSegment text relevant to {{feature}} \nOrganize by specimen\n... reasoning and formatting instructions ...\nUnique instructions: {{segmentation}}\nSegment text relevant to IHC/FISH\n... reasoning and formatting instructions ...\nUnique instructions: {{segmentation 1}}\nIHC/FISH Segmentation II Template\nOrganize the text by specimen and block\n...reasoning and formatting instructions...\nUnique instructions: {{segmentation 2}}\nIHC/FISH Standardization Template\nFeature/Report Standardization Template Feature/Specimen Standardization Template\nReturn a standardized label from {{labels}} for each specimen\n... reasoning and formatting instructions ...\nUnique instructions: {{standardization}}\nFor each specimen and block, return \nstandardized test names and results from \n{{test names}} {{test results}} \nFavor the first test name {{synonyms}}\n... reasoning and formatting instructions ...\nUnique instructions: {{standardization}}\nReturn a standardized label from {{labels}}\n... reasoning and formatting instructions ...\nUnique instructions: {{standardization}}\nSegment text relevant to {{feature}} \n... reasoning and formatting instructions ...\nUnique instructions: {{segmentation}}\n{\n\"reasoning\" : \"...\",\n\"A_histology\" : \"RCC,(unclassified)\",\n\"B_histology\" : \"RCC,(unclassified)\"\n}\nTabular Output\nhistology\nitem\nA\nspecimen\nRCC,\n(unclassified)\nlabel\nhistology B RCC,\n(unclassified)\nhistology\nitem\nA\nspecimen\nRCC,\n(unclassified)\nlabel\nhistology B RCC,\n(unclassified)\n(b)\nFig. 1 | Example schema and Promptﬂow overview. aAbbreviated examples of the\nextraction schema for immunohistochemistry (IHC) andﬂuorescence in situ\nhybridization (FISH), histology, and diagnosis, demonstrating the inclusion of\nentity-speci ﬁc instructions, standardized labels, and a structured vocabulary for\nIHC test reporting. An abbreviated report text is included for reference.\nb Overview of pipeline steps. Each prompt template included base instructions\nconsistent across entities, and placeholders for entity-speciﬁc instructions and\nlabels that could be easily“hot-swapped ”, with the {{}} indicating where infor-\nmation from the schema is pasted in. All template sets include initial prompts to\nsegment and organize text, a subsequent standardization prompt to normalize\nlabels and produce structured output, and aﬁnal Python step for parsing into\ntabular data. The full output from segmentation steps, both reasoning and the\nsegmented text, is passed to subsequent steps. The resulting data is in tabular\nformat for immediate downstream utility.\nhttps://doi.org/10.1038/s41746-025-01686-z Article\nnpj Digital Medicine|           (2025) 8:301 2\nstep LLM interactions with chain-of-thought reasoning (Fig.1b), and a\ncomprehensive error ontology. Developed through iterative“human-\nin-the-loop”42 reﬁnement, this ontology provided a systematic frame-\nwork not only for classifying discrepancies but also for understanding\nthe diverse contextual challenges inherent in extracting nuanced\ninformation from complex medical text, thereby guiding the reﬁnement\nof our extraction objectives.\nWe demonstrate this pipeline using renal tumor pathology reports,\nextracting and normalizing report-level diagnosis, per-subpart/speci-\nmen histology, procedure, anatomical site, and detailed multipart IHC\nresults for 30+ assays at both the specimen and tissue block level. We\nfocus primarily on renal cell carcinoma (RCC) given the high volume of\nRCC patients treated at UT Southwestern, the diversity of RCC subtypes\nand the wide variety of ancillary studies used for subtyping, and a\nmultidisciplinary UTSW Kidney Cancer Program recognized with a\nSpecialized Program of Research Excellence award from the National\nCancer Institute.\nThis paper details the pipeline’s iterative development utilizing 152 diverse\nkidney tumor reports, highlighting how the error ontology revealed crucial\ninsights into task speciﬁcation and report complexity. We subsequently validate\nthe ﬁnalized pipeline on 3520 institutional reports and assess its portability\nusing independent, publicly availablebreast and prostate cancer datasets.\nBeyond the technical speciﬁcs of our pipeline, we place particular\nemphasis on the broader considerations that arose during development.\nSpeciﬁcally, our focus shifted from engineering prompts that could extract\ninformation, to precisely deﬁningwhatinformation to extract andwhy.T h i s\nexperience underscores that, as AI approaches human-level intelligence in\nmany domains\n41, success may increasingly hinge on the clear articulation of\no b j e c t i v e s ,r a t h e rt h a no ns i n g u l a rw o r kﬂow methodologies. As such, by\ndetailing both our successes and pitfa l l s ,w eh o p et op r o v i d ear o a d m a po f\ngeneralizable context and considerations for future AI-powered clinical\ninformation extraction workﬂows.\nResults\nWorkﬂow reﬁnement and gold-standard set\nA development set consisting of 152 reports reﬂecting diverse clinical\ncontexts was used to guide iterative“human-in-the-loop”42 reﬁnement of\nour pipeline, ultimately resulting in our error ontology and a set of“gold-\nstandard” annotations reﬂecting our desired pipeline output; Fig.2,s e e\nMethods for details. In total, 89 reports contained local/regional RCC, 41\ncontained metastatic RCC, nine contained non-RCC malignancies such as\nurothelial carcinoma, and 13 contained benign or neoplasms of uncertain\nbehavior of kidney such as renal oncocytoma; Supplementary Fig. 1. Aﬂow\nchart for deﬁning and documenting discrepancies across iterations as well as\nan introduction to error contexts, posed as broader questions regarding\ninformation extraction aims, is provided in Fig.3. Several error context\nexamples are provided in Tables1–5, with the remainder in Supplementary\nTables 1–11.\nFollowing six iterative reﬁnement cycles with GPT-4o 2024-05-13\n43 as\nthe LLM backbone, our pipeline achieved strong alignment with the gold-\nstandard annotations (1413 total entities: 152 diagnoses, 651 specimen-level\nlabels, 610 IHC/FISH results). Theﬁnal iteration yielded a major LLM error\nrate of 0.99% (14/1413) with no major annotation errors identiﬁed; Fig.4a.\nSchema issues necessitating more thanﬂagging for review were also\neliminated; Fig.4b and Supplementary Figs. 2–4. Supplementary Table 12\ndetails distinct iteration updates with respect toﬂuctuating error prevalence.\nThis iterative reﬁnement process, systematically guided by our\nerror ontology, provided critical insights into the diverse challenges\nencountered when extracting complex information from pathology\nreports, primarily falling into cate gories related to inherent report\nFig. 2 | Overview of the iterative pipeline\nimprovement and gold-standard set creation\nprocess. After completing each iteration, the\nschema, prompts, LLM outputs, and gold-standard\nare incrementally versioned, e.g., V1, V2, V3, etc.\nThe gold-standard set was structured in the exact\nformat as the table that held the LLM outputs, with\ncolumns for report ID, specimen (and block when\napplicable) name, item name, e.g., histology, and\nitem label, e.g., clear cell RCC, so that items could be\nprogrammatically matched for review. The same\nLLM backbone (GPT-4o 2024-05-13) is utilized\nthrough all iterations.\nGold V1\nPrelim Output\nRun LLM Pipeline\nManually review\nand correct\nModify schema\nand prompts to\nreflect desired\nbehavior Schema V1 Prompts V1\nSchema V0 Prompts V0 Report Text\nPrelim Output Report Text\nRun LLM Pipeline\nSchema V1 Prompts V1 Report Text LLM V1\nMatch gold items \nto LLM items\nLLM V1Gold V1\nMatched V1\nManually review\ndiscrepancies,\nannotate w/source\nof error, and\nupdate gold if\nneeded\nReport Text\nMatched V1\nErrors V1Gold V1 Gold V2\nIteration finished\nhttps://doi.org/10.1038/s41746-025-01686-z Article\nnpj Digital Medicine|           (2025) 8:301 3\ncomplexities, difﬁculties in task speciﬁcation, normalization hurdles,\nand the integration of medical nuance.\nReport complexities\nFirst, certain inherent report characteristics consistently generated dis-\ncrepancies. Five complex outside consultations accounted for dis-\nproportionately many minor IHC/FISH discrepancies; Fig.4a. These\n“problematic reports” all contained a mix of IHC/FISH tests where the\nassociated block or specimen was clearly identiﬁed for some tests but not\nothers. In such instances, rather than indicating ambiguity, the LLM would\noften incorrectly duplicate results across all specimens with similar histology\n(Table 1). Additionally, outside consultations often contained discordant\nreporting conventions for specimen names. For example, a specimen\ndesignated as“B”by the outside institution, could be referred to internally as\nspecimen “A”; Supplementary Table 1. Despite adding more illustrative\nexamples of correctly mapped output to the prompts, these challenges\npersisted; Fig.4a, b. We also observed that major errors in IHC/FISH\nextraction, primarily missing results, were more prevalent in reports con-\ntaining a high volume of tests (>10), and ambiguity arose in deﬁning the\nseverity of missing results when tests were mentioned by name but poten-\ntially not performed (Supplementary Table 2).\nSpeciﬁcation issues\nSecond, precisely deﬁning the desired information scope and granu-\nlarity (“speciﬁcation issues”)w a sak e yr eﬁnement focus. This required\nclarifying relevant entities when multiple labels were possible (e.g.,\nanatomical sites, Table 2) and optimizing the level of detail for IHC\nresults. The latter involved shifting from an exhaustive list of potential\nFig. 3 | Flow chart for documenting discrepancy source, severity, and context for\nan iteration.After matching LLM entities to gold-standard entities, e.g., specimen A\nhistology, the labels are checked for discordance (clear cell RCC vs papillary RCC).\nDiscrepancies are then assessed for source and severity. Issue contexts are intro-\nduced as questions needing to be asked about both workﬂow requirements and how\ncertain kinds of deviations from instructions might need to be addressed. For the\nﬁnal two contexts:“Formatting only”refers to discrepancies that are purely due to\nstandardized spellings/punctuation (BAP-1 vs BAP-1), while“Blatantly Incorrect\nReasoning”refers to errors not arising from any given nuanced context (e.g., hal-\nlucinating a test result not present in the report text).\nhttps://doi.org/10.1038/s41746-025-01686-z Article\nnpj Digital Medicine|           (2025) 8:301 4\ntest results to a structured vocabulary capturing the distinct dimensions\nof status, intensity, extent, and pattern, allowing more ﬂexible yet\nstandardized representation; Table 3. Additional instructions were\nneeded to handle nested labels, i.e.,one label is more precisely correct\nthan another, and the preferred level of granularity for capturing ana-\ntomical sites; Supplementary Tables 3, 4. Ontological nuances also\nrequired explicit instructions; for example, clarifying that a\n“peripancreatic mass” should not be coded as pancreatic metastasis\nrequired guidance on interpreting directional terms (Table4\n44).\nWe also addressed ontological overlap, i.e., multiple labels could be\nconsidered fully correct, by establishing prioritization rules. For example,\ndeﬁning the preferred primary status for BAP-1 when both“Positive”and\n“Intact” were reported (Supplementary Table 5). Finally, in certain cir-\ncumstances when individual specimens shared characteristics, we needed to\nTable 1 | Mixed known/unknown entity mapping\nReport Texta Review of outside slides\nA. Skin, abdomen\n- Metastatic carcinoma, IHC proﬁle suggestive of renal primary\nB. Skin, upper back\n- Metastatic carcinoma, IHC proﬁle suggestive of renal primary\n…\nIHC slides are positive for CK7,… IHC stains were performed on block A2 and showed the\nfollowing reactivity: PAX8 * Positive\nDiscordant Labels X_block_X0_IHC_CK7: Positive\nA_block_A2_IHC_PAX-8: Positive\nA_block_A0_IHC_CK7: Positive\nB_block_B0_IHC_CK7: Positive\nA_block_A2_IHC_PAX-8: Positive\nContext - The initial schema instructed the use of specimen“X”as a stand-in when it is not clear which specimen was\nused for a test.\n- In cases with multiple specimens of identical histology, for IHC tests lacking a speciﬁed specimen, the LLM\nwould continue to provide a duplicate set of results for all specimens.\nAddressing\nAction\n- A brief description of this situation along with a properly constructed output was added to the IHC/FISH\nsegmentation II and standardization prompt. This new example provided additional reinforcement to maintain\nusing X when specimen/block is not speciﬁed and the provided names only for the tests for which specimen/\nblock correspondence is explicit.\nContinued Error Severity Examples - Major: If the duplicated set of results was returned for both A & B but B was benign tissue.\n- Minor: Continued duplicated results, but only in the context of both specimens containing identical histology.\naNote that report text details and exact wording for this example and all subsequent examples have been modiﬁed for brevity and to further enhance anonymity.\nTable 2 | Relevance of multiple labels\nReport text A. Right kidney and adrenal gland, radical nephrectomy:\n- Renal cell carcinoma, clear-cell type\n- Adrenal gland, negative for malignancy\nDiscordant labels A_anatomical-site: Kidney, right; Adrenal gland A_anatomical-site: Kidney, right\nContext - The original instructions required listing all anatomical sites in the specimen, as some specimens have multiple\nanatomical sites.\n- In the above report, the adrenal gland and kidney are anatomical sites in the same subpart; however only the kidney is\npositive for RCC.\n- Ambiguity arose over whether to include both sites in the label for such contexts.\nAddressing\nAction\n- It was decided that for our purposes, we wanted the“anatomical site”ﬁeld to continue to capture the primary organs/\ntissues removed for a specimen with no carve-outs for histology. As such, in this case, we would rely on the diagnosis\nand histologyﬁelds to guide our understanding that this was NOT a case of adrenal metastasis.\nContinued Error Severity Examples - Major: An anatomical site of only“Adrenal gland”, omitting the more important site.\n- Minor: An anatomical site of only“Right kidney”. Although the adrenal gland is missing, because it is only benign tissue\nand not an RCC metastasis, its omission does not substantially affect the planned downstream analysis.\nTable 3 | Speciﬁcation issues, granularity\nReport text IHC was performed on A2. Tumor cells are diffusely positive for CA-IX in a membranous pattern\nDiscordant labels A_block_A2_IHC_CAIX: Positive, diffuse membranous A_block_A2_IHC_CAIX: Positive, diffuse\nContext - In our original schema, we attempted to provide a list of all possible IHC results to choose from.\n- After review, we found this to be entirely impractical as the space of possible test results became enormous.\n- We needed to precisely deﬁne the granularity of test results that we were interested in.\nAddressing\nAction\n- We shifted to a more modular schema comprising four dimensions— status, intensity, extent, and pattern— each with its own\ncontrolled vocabulary (see Fig.1A for an example).\n- Under this new approach, the LLM is instructed to sequentially append any applicable modiﬁers (intensity, then extent, then\npattern) to the primary status label, omitting those not present.\nContinued error severity examples - Major: Returning only“Positive”as in RCC, we are very interested in detailed CA-IX staining patterns.\n- Minor: If in the example report, CA-IX had additional describers/modiﬁers that we are not interested in, thus are not in the\nschema, and are then returned by the LLM. These additional modiﬁers would not be factually incorrect, but would be beyond\nthe standardized level of detail that we desire.\nhttps://doi.org/10.1038/s41746-025-01686-z Article\nnpj Digital Medicine|           (2025) 8:301 5\nclarify when the label for one entity can affect that of another (e.g., Specimen\nA and B both originate from a“nephrectomy”,b u ti nt h er e p o r tt e x ts p e -\ncimen A’s procedure is referred to only as a“resection”(Supplementary\nTable 6).\nNormalization difﬁculties\nThird, normalizing terms and handling free-text entries remained chal-\nlenging. While necessary to avoid information loss31, the inclusion of\n“Other-<as per report>“ categories consistently generated minor dis-\ncrepancies due to the difﬁculty of achieving verbatim matches between the\nLLM output and gold-standard (Supplementary Table 7).\nSpeciﬁc IHC/FISH term normalization, like standardizing“diffusely”\nto “diffuse”, also proved problematic. This single normalization challenge\naccounted for over half of the remaining formatting-only discrepancies in\nthe ﬁnal iteration (15/28 errors) despite the target term“diffuse” being\nrelatively infrequent overall (46/610 IHC/FISH annotations). Investigating\npotential causes, we examined the GPT-4o tokenizer’s\n45 byte pair encoding\n(BPE)46 behavior. We found“diffusely” tokenization varied with the pre-\nceding character: splitting into three tokens (“diff”, “us”, “ely”)a f t e ra\nnewline but two (“diffus”, “ely”) after a space. While BPE segmentation is\ncomplex, we hypothesize this differential sub-word tokenization may con-\ntribute to the inconsistent application of this speciﬁc normalization rule. In\ncontrast to these difﬁculties, the workﬂow proved highly adept at normal-\nizing historical terminology to updated terms (Fig.4ba n dS u p p l e m e n t a r y\nTable 8).\nMedical nuance\nFinally, difﬁculties in integrating medical history and nuance required\nclinical domain expertise and corresponding adjustments. For example,\nthe pathologist on our team clariﬁed that in pathology reporting, the\nterms “consistent with” or “compatible with” often carry more con-\nclusive meaning than in general p arlance, leading us to adjust\ninstructions regarding the level of certainty provided by these terms\n(Supplementary Table 9\n47). Similarly, interpreting complex distinc-\ntions, such as delineating local vs. distant lymph node metastases and\nassessing the relevance of medical h istory, necessitated providing\ndetailed clinical context within the prompts (Table5 and Supplemen-\ntary Tables 10, 11).\nAssessing LLM interoperability\nLLM backbone interoperability was assessed by comparing pipeline output\nusing GPT-4o 2024-05-13, Llama 3.3 70B Instruct48,a n dQ w e n 2 . 57 2 B\nInstruct49 to theﬁnal gold-standard. Exact match accuracies were 84.1% for\nGPT-4o, 78.1% for Qwen2.5, and 70.1% for Llama 3.3. Applying fuzzy\nmatching for IHC/FISH items (ignoring test name punctuation/capitali-\nzation and relaxing specimen/block mapping criteria) improved these\nrespective accuracies to 90.0, 86.1, and 82.0%. See Supplementary Table 13\nfor concrete examples of matching criteria and Supplementary Table 14 for\nexact counts of match type for each model. Exact match inter-model\nagreement between GPT-4o and the open-weight models was high (84.8%\nwith Qwen2.5, 82.2% with Llama 3.3). These results suggest the core prompt\nand schema logic is transferable, though model choice impacts\nperformance.\nValidation against preexisting data\nApplying theﬁnalized pipeline (using GPT-4o 2024-08-06) to 2297 internal\nkidney tumor reports with available structured EMR data for validation\nyielded high performance for identifying six key kidney tumor histologies\n(clear cell RCC, chromophobe RCC, papillary RCC, clear cell papillary renal\ncell tumor/CCPRCT, TFE3-rearranged RCC, and TFEB-altered RCC) with\nTable 4 | Speciﬁcation issues, ontology\nReport text C. Peripancreatic mass, excision:\n- Metastatic renal cell carcinoma, clear cell type\nDiscordant labels C_anatomical-site: Other- peripancreatic mass C_anatomical-site: Pancreas\nContext - Because the mass is described as being peripancreatic, is it precise to label the site as pancreas?\n- Additionally, in the context of metastasis, the histology of a tissue specimen should not be mistaken for its\nanatomical site\nAddressing\naction\n- Added to anatomical site standardization instructions:“Analyze whether there are any position or direction terms\nthat are relevant, for example, a“peripancreatic mass”would not be captured as‘Pancreas’as this refers to a mass\nin the tissue surrounding the pancreas.… renal cell carcinoma that has metastasized to the left lung would ONLY\nhave the anatomical site‘Lung, left’if the specimen ONLY contains lung tissue.”\nContinued error severity examples - Major: Continued use of the label“pancreas”would be considered major, as we have now instructed that the\nanatomical site must be consistent with the originating tissue.\n- Minor: In some cases, continued usage of the“Other”label vs a speciﬁc provided label can be justiﬁed as a minor\nerror if the site listed in the text does not cleanly map to labels in the schema. For example, an“intradural tumor”\ndevelops within the spinal cord, thus does not cleanly map to our schema label of“Spine, vertebral column”as this\nhas a connotation of a tumor developing in bone tissue, although for our purpose weﬁnd this mapping acceptable.\nTable 5 | Integrating medical history\nReport text A. Soft tissue mass, parasplenic\n- Poorly differentiated carcinoma, consistent with known renal cell carcinoma\nNote: Prior history of papillary renal cell carcinoma is noted.\nDiscordant labels A_histology: Papillary renal cell carcinoma A_histology: Poorly differentiated carcinoma\nContext - Should we label this specimen as papillary RCC inferring from the medical history, or only use the current report histology\n(poorly differentiated carcinoma)?\n- The goal is to avoid automatically applying historicalﬁndings unless they are truly consistent with current specimen details.\nAddressing\naction\n- Added to histology standardization instructions:“If a specimen is consistent or compatible with a known histology, you may\nuse that histology as part of your choice of a label, but ensure that the histology you choose is still applicable to the current\nspecimen.”\nContinued error severity examples - Major: If the report were to instead lack the“consistent with known renal cell carcinoma”modiﬁer, then the histology\n“Papillary RCC”would be a major error, as it would be reporting medical history alone.\n- Minor: Labeling the specimen“RCC, no subtype speciﬁed”instead of“papillary RCC,”even though the text leans toward\npapillary (note the specimen is only consistent with renal cell carcinoma- no subtype speciﬁed). While not optimal, it does\nnot fundamentally misclassify the specimen.\nhttps://doi.org/10.1038/s41746-025-01686-z Article\nnpj Digital Medicine|           (2025) 8:301 6\n433\n614\n19\n1\n2\n14\n19\n39\n29\n39\n39\n109\n100\n430\n659\n3\n7\n26\n35\n17\n35\n92\n49\n88\n4\n18\n446\n666\n1\n1\n7\n17\n2\n25\n111\n35\n59\n14\n59\n509\n697\n1\n3\n9\n19\n13\n15\n38\n37\n68\n8\n35\n465\n732\n1\n32\n10\n5\n8\n65\n49\n125\n13\n6\n489\n739\n9\n5\n15\n3\n63\n54\n65\n1\n4\nIHC/FISH: Exact Match\nIHC/FISH: Major Annotation Error\nIHC/FISH: Major LLM Error\nIHC/FISH: Minor Annotation Discrepancy\nIHC/FISH: Minor LLM Discrepancy\nIHC/FISH: Minor LLM Discrepancy Problematic (5) Reports\nIHC/FISH: Schema Issue\nOther Items: Exact Match\nOther Items: Major Annotation Error\nOther Items: Major LLM Error\nOther Items: Minor Annotation Discrepancy\nOther Items: Minor LLM Discrepancy\nOther Items: Schema Issue\nIteration\n1\nIteration\n2\nIteration\n3\nIteration\n4\nIteration\n5\nIteration\n6\na\n13142\n32 25 20 21 24\n17 21 20 21 19 20\n10 13 14 8 23 28\n1 399\n332\n17 31 84 39 45 17\n83 07 59 8\n384 1 1 1 2\n1 2 7947 1 0\n24 34 16 8 7 9\n7 20 9 13 17 13\n1\n6 2 71 21 52 83 0\n24 1\n1311\n1 5 2 1 3 321\n31\n21\n31 0 1 1 4\n32 4 3 8\n351542\n1 2 94531\n31 13 6 3\n11\n71 3\n13 25 15 14 17 2\n11 2\n1\n1\n12\n17\n11 27 9 5 9\n21\n2\n92113\n1\n935 1\n16\n168595\n52\n17 2 2\n23\n76 1\n2\n11\n51 1 1 6\n14 2 5 10 6\n52 62 97 3\n11 1 9 1\nMajor/Minor Annotation Discrepancy Schema Issue\nMinor LLM Discrepancy Major LLM Error\nIT1 IT2 IT3 IT4 IT5 IT6 IT1 IT2 IT3 IT4 IT5 IT6\nFlag for Review\nBlatantly Incorrect Reasoning\nWording of Other Categories\nTerminology Drift\nSpecification Issues− Ontology\nSpecification Issues− Granularity\nRelevance of Multiple Labels\nRelevance of Missing Data\nMixed Known and Unknown Entity Mapping (Duplicate)\nMixed Known and Unknown Entity Mapping\nInter−Entity Attribution\nIntegrating Medical History\nFormatting Only\nDiscordant Reporting (Duplicate)\nDiscordant Reporting\nBorderline Attributes\nFlag for Review\nBlatantly Incorrect Reasoning\nWording of Other Categories\nTerminology Drift\nSpecification Issues− Ontology\nSpecification Issues− Granularity\nRelevance of Multiple Labels\nRelevance of Missing Data\nMixed Known and Unknown Entity Mapping (Duplicate)\nMixed Known and Unknown Entity Mapping\nInter−Entity Attribution\nIntegrating Medical History\nFormatting Only\nDiscordant Reporting (Duplicate)\nDiscordant Reporting\nBorderline Attributes\nb\nFig. 4 | Error/discrepancy results across workﬂow iterations. aError/discrepancy\nsource, severity, and entity type across iterations.“Other Items” includes diag-\nnosis, histology, procedure, and anatomical site. Theﬁve “Problematic reports”are\nconsistent across iterations. Counts of 0 are left blank. Column totals are not equal\nacross all iterations due to duplicate IHC/FISH entities and variations in\nmissingness. b Error/discrepancy contexts by source and severity across iterations\n(IT). Counts of 0 are left blank. Due to the lower number of major annotation\nerrors, they have been grouped with minor annotation discrepancies for ease of\nvisualization. For all panels, theﬁll color scale is maintained with a maximum of 84\nand a minimum of 1.\nhttps://doi.org/10.1038/s41746-025-01686-z Article\nnpj Digital Medicine|           (2025) 8:301 7\na macro-averaged F1 score of 0.99, and an F1 of 0.97 for metastatic RCC\ndetection (Table6).\nA review of discrepancies for histological subtype showed continued\ndifﬁculty with integrating medical history; most clear cell false negatives\n(28/32), for instance, resulted from incomplete use of a patient’s prior his-\ntory of this subtype. Lower performance in detecting metastatic RCC was\nprimarily attributable to false positives due to misinterpreting medical\nhistory (n = 6), local tumor extension (n = 5), and differentiation of regional\nvs distant lymph nodes (n = 3). Such errors often occurred in complex or\nambiguous reports, underscoring the need for mechanisms toﬂag these\ncases for human review. Demonstrating clinical utility, the pipeline correctly\nidentiﬁed necessary updates or corrections to the preexisting structured data\nin 27 instances (e.g., reﬂecting addended results or more current termi-\nnology), all subsequently conﬁrmed by a pathologist. Comprehensive details\non all discrepancies, both errors and correct updates, are included in Sup-\nplementary Table 15.\nComparison to the regex baseline\nThe LLM pipeline demonstrated distinct advantages over a custom, rule-\nbased regex tool for extracting the six histologies of interest. The regex tool\nperformed reasonably well for common subtypes with fewer historical\nvariations in terminology (F1 scores: 0.96 Clear Cell, 0.91 Chromophobe,\n0.89 Papillary RCC; Table6). However, its performance degraded sub-\nstantially when encountering wider terminology variation, historical naming\nconventions, or results reported primarily in comments or addendums—\nsituations common for rarer subtypes. This resulted in much lower F1 scores\nfor CCPRCT (0.71), TFE3-rearranged RCC (0.11), and TFEB-altered RCC\n(0.35), whereas the LLM pipeline maintained high accuracy and precision.\nFull results for the regex tool are available in Supplementary Table 16.\nGauging internal consistency\nTo assess the internal consistencyof extracted data across a broader\ncohort, we analyzed the full set of 3520 internal reports (see Methods/\nSupplementary Fig. 1 for cohort criteria). This group included the\naforementioned 2297 reports, plus an additional 1223 reports for which\nno structured EMR data was available. For this analysis, we examined\nthe concordance between the pipeline’s extracted histology and asso-\nciated IHC results within the same specimen/subpart. Out of all avail-\nable reports, 2464 subparts/specimens were identiﬁed as containing any\nof the histologies of interest. Of these, 1906 were identiﬁed to have only\na single histology and corresponding IHC results for the same subpart;\nSupplementary Fig. 1.\nT h ep i p e l i n es h o w e dah i g hd e g r e eof consistency, for example, 87/87\nCD117 tests on specimens with chromophobe RCC were positive, and\naccurate extraction of the CA-IX“cup-like” expected staining pattern for\nCCPRCT was demonstrated (Table7). The two“box-like”results found for\nCCPRCT corresponded to two tumors in a single report, wherein the LLM\nwas consistent with the report text. The case was subsequently reviewed and\nTable 6 | Consistency between preexisting data and extracted histology and diagnosis of metastatic RCC\nClear cell RCC Ground truth\nAbsent Contains\nLLM predicted Absent 576 + 1a 32 LLM F1: 0.99 (Regex F1: 0.96)\nContains 4 1671 + 13b\nPapillary RCC Ground truth\nAbsent Contains\nLLM predicted Absent 2061 + 1 2 LLM F1: 0.99 (Regex F1: 0.89)\nContains 0 232 + 1\nClear cell papillary renal cell tumor (CCPRCT) Ground truth\nAbsent Contains\nLLM predicted Absent 2247 0 LLM F1: 0.98 (Regex F1: 0.71)\nContains 24 6 + 2\nChromophobe RCC Ground truth\nAbsent Contains\nLLM predicted Absent 2188 1 LLM F1: 0.99 (Regex F1: 0.91)\nContains 0 105 + 3\nTFE3-Rearranged RCCb Ground truth\nAbsent Contains\nLLM predicted Absent 2289 0 LLM F1: 1 (Regex F1: 0.11)\nContains 07 + 1\nTFEB-altered\nRCC\nb\nGround truth\nAbsent Contains\nLLM predicted Absent 2287 0 LLM F1: 1 (Regex F1: 0.36)\nContains 06 + 4\nMetastatic RCC Ground truth\nNon-\nmetastatic\nMetastatic\nRCC\nLLM predicted Non-metastatic 2050 2 F1: 0.97\nMetastatic RCC 14 230 + 1\na The digit after the plus here indicates the number of instances where, after review of the report free text, the LLM provided a pathologist-conﬁrmed updated label (See Supplementary Table 15 for details).\nb TFE3 was additionally matched to the older terminology- Xp11 translocation RCC. Similarly, TFEB was matched tot(6,11) translocation RCC. These terms were used in previous versions of CAP Kidney\ntemplates.\nSee Supplementary Table 16 for full regex tool results.\nhttps://doi.org/10.1038/s41746-025-01686-z Article\nnpj Digital Medicine|           (2025) 8:301 8\nfound to have a“cup-like”pattern and a correction was issued; Details on all\nunexpectedﬁndings are documented in Supplementary Table 17.\nAssessing clinical domain interoperability\nPortability of the workﬂow andﬁnal prompt templates was assessed using\npublicly available data from The Cancer Genome Atlas (TCGA)50–53.T o\ngauge generalization, prompt templates were not modiﬁed, and improve-\nments were performed on only the schema, with iterations concluding once\nthe schema adequately covered essential domain concepts and terminology.\nOf the 757 available TCGA Breast Invasive Carcinoma\n54 (BRCA) reports, 53\ncontained the words“immunohistochemistry” and “HER2” in the pro-\ncessed text. Three schema-only iterations were required to incorporate\ndomain-speciﬁc terminology and reporting conventions for extracting\nHER2 (IHC and FISH), estrogen receptor (ER), and progesterone receptor\n(PR) status. The pipeline (using GPT-4o 2024-08-06) subsequently achieved\n89% agreement with the curated tabular data; Supplementary Table 18.\nManual review of discrepancies suggested many were not LLM errors:\nnine cases involved results present in the curated data but absent in the\nscanned PDF, potentially indicating an alternate source for the curated\nresult. These nine speciﬁc data points, where required information was not\nin the source text, were excluded from agreement calculations. Another\nresult thatﬁrst appeared to be an LLM false positive was subsequently found\nto reﬂect known clinical ambiguity regarding the classiﬁcation of ER low-\npositive results (1–9% staining)\n55.\nOf the 333 available TCGA Prostate Adenocarcinoma56 (PRAD)\nreports, 253 had corresponding Gleason Scores in the structured clinical\ndata and available report text. For this task, 98% agreement was achieved on\ntheﬁrst run (Supplementary Table 19). The difference in convergence speed\n(one run for PRAD vs. three for BRCA) highlights the impact of task and\nreport complexity: extracting multiple, detailed IHC results from longer,\nmore variable BRCA reports (average 9250 characters) proved more chal-\nlenging than extracting a single report-level Gleason Score from shorter\nPRAD reports (average 3440 characters).\nDiscussion\nThis study demonstrates that high accuracy in automated pathology report\ninformation extraction with large language models (LLMs) is possible but\nhinges on careful task deﬁnition and reﬁnement. Although our pipeline\nyielded strong performance— for instance, a macro-averaged F1 score of\n0.99 on identifying important RCC histological subtypes and adaptability to\nnew entities such as Gleason Scores from prostate adenocarcinoma reports\n— our experience underscores that the process of deﬁning information\nextraction goals may hold broader relevance than the performance metrics\nor workﬂow technicalities alone.\nIt became evident that the model’s success depended heavily on the\nclarity and depth of instructions. Consequently, a multidisciplinary team\nwith domain expertise in NLP and LLMs, downstream statistical analysis,\nand clinical pathology became instrumental for success. Additional\nTable 7 | Consistency between extracted histology and IHC/FISH results\nChromo-\nphobe RCC\nPapillary RCC CCPRCT Clear cell RCC TFE3\nRearranged RCC\nTFEB\nAltered\nRCC\nTotal Number of specimensa 84 119 62 1630 6 5\nCA-IX Expected →\nExtracted ↓\nNegative Focal/Patchy Positive or\nNegative\nPositive\n(Cup-Like)\nPositive or Positive\n(Box-Like)\nNegative Negative\nPositive (cup-like) 0 1 d 61 3 d 00\nPositive (box-like) 0 0 2 c 164 0 0\nFocal/patchy\npositive\n02 2 0 6 c 01\nOther positiveb 0 6 3 548 0 0\nNegative 24 15 0 2 c 53\nCD117 Expected →\nExtracted ↓\nPositive Negative Negative Negative Negative Negative\nPositive 87 0 0 1 d 01 c\nNegative 0 4 6 26 2 3\nRacemase Expected →\nExtracted ↓\nNegative Positive Negative Mixed Mixed Mixed\nFocal/patchy\npositive\n02 0 7 0 0\nPositive/\ndiffuse positive\n2c 99 0 9 1 2\nNegative 3 0 13 4 0 0\nTFE3 Expected →\nExtracted ↓\nNegative Negative Negative Negative Rearranged Negative\nRearranged 0 0 0 0 6 0\nNegative 2 7 0 6 0 4\nTFEB Expected →\nExtracted ↓\nNegative Negative Negative Negative Negative Rearranged/\nAmpliﬁed\nRearranged/\nampliﬁed\n00 0 0 0 5\nNegative 2 6 0 4 6 0\naSingle specimens may have multiple tests, thus column totals may be higher than the number of specimens.\nbIncludes “Positive”alone, or with other modiﬁers not explicitly focal/patchy, cup-like, or box-like.\ncReport reviewed and the LLM was correct, thus identifying a typographic mistake in the report free text (Supplementary Table 17 for details).\ndReport reviewed and the LLM found to have made a mistake in either histology or IHC/FISH results (Supplementary Table 17 for details).\nhttps://doi.org/10.1038/s41746-025-01686-z Article\nnpj Digital Medicine|           (2025) 8:301 9\ncollaboration came from the LLM itself— particularly through review of the\n“reasoning”output. We found that well-meaning instructions like“focus on\nthe current specimen… , not past medical history” led to instances of\n‘malicious compliance’where the LLM followed instructions too literally,\ndiscarding important contextual information. Rectifying this required\ncareful consideration of how instructions might be interpreted by the model,\nleading to increased speciﬁcity (Table5). Underpinning this iterative pro-\ncess, the systematic application of our error ontology proved invaluable, not\njust for classifying discrepancies, butfor prompting essential questions (as\nper Fig.3) and actively guiding the reﬁnement of our extraction goals.\nCross-specialty collaboration was particularly vital for developing the\nerror ontology. While developed with expert clinical input, we do\nacknowledge that our distinction between schema issues, major, and minor\nerrors relied on contextual interpretation and our speciﬁc downstream use\ncase. We thus caution against interpreting raw performance numbers in\nisolation. Instead, we advocate for a holistic interpretation considering the\nclinical signiﬁcance of errors and their potential downstream impact,\ninformed directly by stake-holding researchers\n57,58. Interpretations could\ntherefore differ between groups. For example, in broader kidney cancer\nresearch, deviations in the detailed staining pattern modiﬁers for CA-IX\ncould be considered‘major’errors, as the distinction between“box-like”vs\n“cup-like”is decisive for RCC subtyping. However, if one were exclusively\nstudying oncocytoma, where CA-IX is canonically negative\n59,v a r i a t i o n si n\nthe details of a“Positive”result might sufﬁciently exclude this histology and\nbe considered‘minor’.\nWe note several limitations of our study and workﬂow. The iterative\nreﬁnement process risked adding overly speciﬁc ‘one-off’rules; we mitigated\nthis by striving for generalizable instructions, using speciﬁc cases as exam-\nples rather than distinct rules (Table4). Each reﬁnement cycle also required\nhighly detailed and time-intensive human review. One might ask whether\nan end-to-end manual annotation followed by traditionalﬁne-tuning would\nhave been easier. However, such an approach would have still demanded\nextensive manual review\n4, offered no guarantee of easily handling the diverse\nand complex“error-inducing”contexts that we encountered, and might not\nhave spurred the same level of insight into reﬁning our information\nextraction goals. Moreover, generative LLM technology is evolving rapidly41;\nour prompt-based pipeline is more adaptable to both changes in clinical\nentities and new model upgrades than static,ﬁne-tuned architectures.\nFurthermore, this approach currently produces only semi-structured\ndata that, in some instances, requires further downstream normalization.\nWhile our research team possessed this domain-speciﬁc skillset, allowing us\nto prioritize granular detail over strict upfront standardization, this trade-off\nmay not be acceptable for all teams and situations. Future work could\nleverage guided-decoding backends\n37,38 that can help constrain LLM gen-\neration to a predeﬁned JSON schema. Such tools, along with increasing\nbaseline AI performance and methods toﬂag problematic reports for\nhuman review, could further shift workﬂow efforts from managing for-\nmatting and data structuring requirements to well-informed task\nspeciﬁcation.\nLimitations related to gold-standard development and validation also\nwarrant mention. The standard was developed iteratively by the core team to\nreﬁne complex task speciﬁcations, precluding formal inter-reviewer agree-\nment metrics and limiting assessment of absolute annotation reliability. Our\nvalidation strategy— large-scale internal checks on core entities and external\nportability tests— aligned with the study’s focus on the reﬁnement metho-\ndology and task speciﬁcation challenges, rather than exhaustive bench-\nmarking across everyﬁeld against a static, held-out test set. As such, we posit\nthat as LLMs are utilized to perform increasingly complex tasks, particularly\nthose involving ambiguity and advancing medical knowledge, it may be\nuseful to conceptualize“gold-standard” not merely as static ground truth,\nbut as a dynamic reﬂection of evolving research goals and domain\nunderstanding\n40,57,58.\nIn summary, our LLM-based pipelinefor pathology report informa-\ntion extraction highlightsboth strong performance metrics and the intricate\nprocesses required to achieve them. Our experience illustrates the\nimportance of thoroughly understanding extraction intentions and goals,\nand how collaboration between domain experts— and even insights derived\nfrom the LLMs themselves— are crucial to this process. By documenting\nthese complexities, we aim to provide a set of generalizable considerations\nthat can inform future LLM-based clinical information extraction pipelines.\nAs generative AI matures,ﬂexible, human-in-the-loop strategies may prove\nessential to ensuring workﬂo w sr e m a i ng r o u n d e di nr e a l - w o r l dc l i n i c a l\nobjectives.\nMethods\nDeﬁning the task\nAs detailed in the Introduction, we deﬁned “end-to-end” information\nextraction for this study to encompass entity identiﬁcation, clinical question\ninference, terminology normalization, relationship mapping, and struc-\ntured output generation suitable for downstream analysis (e.g., tabular\ndatasets).\nInitial entities to extract and normalize from reports included: (1)\nreport-level diagnosis; (2) per-subpart/specimen histology, procedure type,\nand anatomical site; and (3) detailedspecimen and tissue block-level IHC/\nFISH (ﬂuorescence in situ hybridization) test names and results. Weﬁrst\ndeﬁned an‘extraction schema’, outlining standardized labels, a structured\nvocabulary of terms and preferred synonyms for IHC results, and unique\nentity-speciﬁc instructions; see Fig.1a. Unique instructions provided entity-\nspeciﬁc context and logic, like differentiating regional vs. distant metastasis\nbased on lymph node involvement for diagnosis.\nLabels for diagnosis were primarily derived from ICD-10 descriptors,\nwith procedure and histology labels primarily sourced from the con-\ntemporary College of American Pathologists (CAP) Cancer Protocol\nTemplates for kidney resection and biopsy\n60,61. Labels for anatomical site and\nIHC/FISH, along with specialized labels such as the diagnosis“Metastatic\nRCC,” were developed with guidance fromp a t h o l o g ye x p e r t s .W h i l et h e\nnormalized lists covered common entities, free-text options (e.g.,“Other-\n<ﬁll in as per report>”) were essential for capturing less frequentﬁndings or\nspeciﬁc nuances not fully represented, thus preventing information loss.\nPrompt templates\nWe used Microsoft Promptﬂow62 to organize the workﬂow as a directed\nacyclic graph, with nodes for Python code execution or LLM requests via\nspeciﬁc prompt templates. Reusable,modular prompt templates were\ndesigned for portability. We developed three distinct template sets, each\noptimized for a speciﬁc class of entity: The‘feature report’set for entities\nwith a single label per report, such as diagnosis; The“feature specimen”set\nfor entities with one label per specimen/subpart; And an IHC/FISH speciﬁc\nset as it uniquely requires matching any number of specimens, blocks, test\nnames, and test results; see Fig.1b. Initial prompts segmented and organized\nrelevant text, whileﬁnal prompts generated schema-normalized, parsable\nJSON output.\nAll prompts required initial“reasoning” generation, passed to sub-\nsequent prompts to create a“chain-of-thought”\n63. This has been shown to\nenhance LLM performance across a wide range of tasks64,65,a n dp r o v i d e s\ninsight into speciﬁc limitations and usage of our instructions66.T h er e a -\nsoning instructions prompted the LLM to consider uncertainties and\narticulate its decision-making process for text segmentation or standardi-\nzation. Prompt templates were writteni nm a r k d o w nf o r m a tf o ro r g a n i z a -\ntion and included examples of properly structured JSON responses. Full\nschema and templates are included in the Supplementary Information and a\nGitHub repository for implementation can be found at github.com/\nDavidHein96/prompts_to_table.\nWorkﬂow reﬁnement and gold-standard set\nWe selected 152 reports reﬂecting diverse clinical contexts from a set of\npatients with kidney tumor relatedI C D - 1 0c o d e si nt h e i rE M Rh i s t o r y ;\nSupplementary Fig. 1. This included reports with multiple specimens,\ncomplex cases that required expert consultation, biopsies and surgical\nresections, various anatomical sites such as primary kidney and suspected\nhttps://doi.org/10.1038/s41746-025-01686-z Article\nnpj Digital Medicine|           (2025) 8:301 10\nmetastases, and both RCC and non-RCC histologies. All data was collected\nunder the purview of our institutional IRB STU 022015-015.\nThese reports were used to iteratively develop the pipeline, error\nontology, and a corresponding“gold-standard”annotation set that could be\nused for benchmarking. First, the workﬂow was executed with preliminary\nprompts and schema to generate rough tabular outputs, allowing for\nexpedited manual review and reducing annotation burden for creating the\ninitial gold-standard set. Subsequently, the prompts and schema were\nupdated to reﬂect desired changes, and the workﬂow was executed again.\nThe output was then matched to gold-standard annotations and discordant\nresults were manually reviewed, with source and severity of the discrepancy\ndocumented and used to inform subsequent adjustments to the workﬂow\nand gold-standard. This process was then repeated iteratively, as outlined in\nFig. 2.\nA data scientist (5 years clinical oncology experience) and a statistician\n(13 years kidney cancer research experience) reviewed annotations and\ndiscrepancies with all uncertaintiesor ambiguities deferred to a board-\ncertiﬁed pathologist specializing in genitourinary pathology (23 years’\nexperience). All development used GPT-4o (2024-05-13, temperature 0) via\na secure HIPAA-compliant Azure API\n43.\nCreating an error ontology\nA structured error ontology was developed to both provide a framework for\nclassifying the source and severity of discrepancies between the LLM out-\nputs and the gold-standard, and to highlight generalizable contexts in which\ndiscrepancies arose. The ontology comprises three sources of discrepancy:\nLLM, manual annotations (errors introduced to the gold-standard set by\nincorrect or insufﬁcient annotation in a prior step), and what we term\n“schema issues”. Schema issues represent instances where the LLM and\ngold-standard were discordant, yet both appeared to have adhered to the\nprovided instructions. In these cases, the instructions themselves were found\nto be insufﬁcient or ambiguous. Both LLM and manual annotation dis-\ncrepancies were further subclassiﬁed as“major”or “minor”severity based\non their potential impact on clinical interpretation or downstream analysis.\nA ﬂow chart for deﬁning and documenting discrepancies as well as an\nintroduction to the error contexts, presented as broad questions regarding\ninformation extraction aims, is provided in Fig.3. Detailed examples for a\nsubset of contexts are given in Tables1–5,w i t ht h er e m a i n d e rp l u sa d d i -\ntional examples in Supplementary Tables 1–11. For each context, weﬁrst\nprovide two potential labels for an entity arising from insufﬁcient instruc-\ntions in the given context. This is followed by our addressing methodology,\nand further examples of LLM or annotation error severity in similar contexts\n(provided that we found our instructions to be sufﬁcient).\nStopping criteria\nReﬁnement concluded upon reaching zero major manual annotation errors,\na near-zero rate of minor annotation discrepancies, a major LLM error rate\nnear or below 1%, and an eliminationof most schema errors, except those\narising from complex cases deemed requiring human review. Upon\nreaching these criteria, we retrospectively reviewed the error documentation\nfor each iteration to ensure consistency in our categorization (e.g., reﬁning\ninitial broad speciﬁcation issues into more granular categories).\nAssessing LLM interoperability\nLLM backbone interoperability was assessed by comparing GPT-4o\n2024-05-13, Llama 3.3 70B Instruct48, and Qwen2.5 72B Instruct49 out-\nputs to theﬁnal gold-standard: First by exact match using the full spe-\ncimen, block and test name when applicable, then by using an additional\n“fuzzy match” for IHC/FISH items that ignored punctuation and capi-\ntalization in the test name and did not require the specimen or tissue\nblock identiﬁers to match the gold-standard. See Supplementary Table\n13 for examples of match criteria. Exact match inter-model agreement\nbetween GPT-4o and the two open-weight models was also calculated.\nBoth open-weight models were run on local compute infrastructure,\nwith generations using a temperature of 0.\nInternal application and validation\nOur ﬁnalized pipeline was run using GPT-4o 2024-08-06 on the free\ntext portion (ﬁnal diagnosis, ancillary studies, comments, addendums)\nof 3520 internal pathology reports containing evidence of renal tumors\nspanning April 2019–April 2024; see Supplementary Fig. 1 for addi-\ntional details on inclusion criter ia. Of these reports, 2297 utilized\nadditional discrete EMRﬁelds corresponding to CAP kidney resection/\nbiopsy and internal metastatic RCC pathology templates; This struc-\ntured data could be pulled separately from the report text and was used\nfor cross-referencing LLM output regarding metastatic RCC status and\nthe presence or absence of six kidney tumor subtypes: clear cell RCC,\nchromophobe RCC, papillary RCC, clear cell papillary renal cell tumor\n(CCPRCT), TFE3-rearranged RCC, and TFEB-altered RCC. Dis-\ncrepancies were manually reviewed using the free-text report as ground\ntruth. Notably, the LLM needed to infer updated TFE3/TFEB subtypes\nfrom older ‘MiT family translocation RCC’ terms present in some\nstructured data (per CAP Kidney 4.1.0.0)\n60,61.\nTo provide a baseline for histologyextraction and mapping to updated\nterminology, we developed a custom rule-based regular expression (regex)\ntool targeting predictable structural and lexical patterns commonly\nobserved in the“ﬁnal diagnosis”section of reports, where primary histology\nis typically stated. This represented a pragmatic, non-machine learning\napproach using domain heuristics for comparison.\nTo attempt scalable validation of LLM extracted histology and IHC/\nFISH results across all 3520 reports, including those with no available\ntemplated data, we selected all extracted subparts/specimens with a\nsingle histology of the above six for which IHC/FISH results were also\nextracted. We then assessed the consistency of the histological subtype\nwith the expected IHC/FISH pattern forﬁve common markers used to\ndifferentiate renal tumor subtypes; CA-IX, CD117, Racemase, TFE3, and\nTFEB\n67. Unexpected ﬁndings were subject to manual review of the\nreport text.\nAssessing clinical domain interoperability\nAdaptability to different clinical domains was evaluated using The Cancer\nGenome Atlas (TCGA)50–52 breast invasive carcinoma (BRCA)54 and pros-\ntate adenocarcinoma (PRAD)56 pathology reports that had undergone\nscanned PDF to text optical character recognition (OCR) processing and\nhad corresponding tabular clinical data available53. For BRCA reports, we\nextracted results for HER2 (both FISH and IHC separately), progesterone\nreceptor (PR), and estrogen receptor (ER), utilizing only modiﬁed IHC/\nFISH schema labels and instructions.We restricted the reports to those\ncontaining the words“immunohistochemistry”and “HER2”to ensure IHC\nresults were present in the OCR processed text.\nFor PRAD reports, we extracted Gleason Scores using the‘feature\nreport’ﬂow with only modiﬁcations to the schema instructions and labels.\nAll external validation was done using GPT-4o (2024-08-06 via Azure,\ntemperature of 0).\nData availability\nThe data used in this study contains patient identiﬁers and cannot be shared\npublicly due to privacy regulations andinstitutional policies. The publicly\navailable breast cancer reports and clinical data can be found athttps://www.\ncbioportal.org/study/clinicalData?id=brca_tcga_pub2015. The publicly\navailable prostate cancer reports and clinical data can be found athttps://\nwww.cbioportal.org/study/summary?id=prad_tcga_pub.T h eo p t i c a lc h a r -\nacter recognition processed reports are available athttps://data.mendeley.\ncom/datasets/hyg5xkznpx/1.\nCode availability\nCode for implementing the workﬂow described in this paper is available at\nhttps://github.com/DavidHein96/prompts_to_table.\nReceived: 14 February 2025; Accepted: 28 April 2025;\nhttps://doi.org/10.1038/s41746-025-01686-z Article\nnpj Digital Medicine|           (2025) 8:301 11\nReferences\n1. Li, I. et al. Neural natural language processing for unstructured data in\nelectronic health records: a review.Comput. Sci. Rev.46, 100511\n(2022).\n2. Zozus, M. N. et al. Factors affecting accuracy of data abstracted from\nmedical records.PLoS ONE10, e0138649 (2015).\n3. Brundin-Mather, R. et al. Secondary EMR data for quality\nimprovement and research: a comparison of manual and electronic\ndata collection from an integrated critical care electronic medical\nrecord system.J. Crit. Care47, 295–301 (2018).\n4. Sushil, M. et al. CORAL: expert-curated oncology reports to advance\nlanguage model inference.NEJM AI1 (2024).\n5. Jee, J. et al. Automated real-world data integration improves cancer\noutcome prediction.Nature 636, 728–736 (2024).\n6. Sedlakova, J. et al. Challenges and best practices for digital\nunstructured data enrichment in health research: a systematic\nnarrative review.PLoS Digit. Health2, e0000347 (2023).\n7. Xu, H., Anderson, K., Grann, V. R. & Friedman, C. Facilitating cancer\nresearch using natural language processing of pathology reports.\nStud. Health Technol. Inform.107, 565–572 (2004).\n8. Hripcsak, G. et al. Unlocking clinical data from narrative reports: a\nstudy of natural language processing.Ann. Intern. Med.122, 681–688\n(1995).\n9. Alsentzer, E. et al. Publicly available clinical BERT embeddings.\nPreprint athttps://doi.org/10.48550/ARXIV.1904.03323 (2019).\n10. Yang, X. et al. A large language model for electronic health records.\nNpj Digit. Med.5, 194 (2022).\n1 1 . R a s m y ,L . ,X i a n g ,Y . ,X i e ,Z . ,T a o ,C .&Z h i ,D .M e d - B E R T :\npretrained contextualized embeddings on large-scale structured\nelectronic health records for disease prediction.Npj Digit. Med.4,\n86 (2021).\n12. Lee, J. et al. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining.Bioinformatics 36,\n1234–1240 (2020).\n13. Peng, Y., Yan, S. & Lu, Z. Transfer learning in biomedical natural\nlanguage processing: an evaluation of BERT and ELMo on ten\nbenchmarking datasets. Preprint athttps://doi.org/10.48550/ARXIV.\n1906.05474 (2019).\n14. Su, P. & Vijay-Shanker, K. Investigation of improving the pre-training\nand ﬁne-tuning of BERT model for biomedical relation extraction.\nBMC Bioinformatics23, 120 (2022).\n15. Li, Y., Wehbe, R. M., Ahmad, F. S., Wang, H. & Luo, Y. A comparative\nstudy of pretrained language models for long clinical text.J. Am. Med.\nInform. Assoc.30, 340–347 (2023).\n16. Brown, T. et al. Language models are few-shot learners.Adv. Neural\nInf. Process. Syst. 33, 1877–\n1901 (2020).\n17. Liu, H. et al. Evaluating the logical reasoning ability of ChatGPT and\nGPT-4. Preprint athttps://doi.org/10.48550/ARXIV.2304.03439\n(2023).\n18. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E.\nCapabilities of GPT-4 on medical challenge problems. Preprint at\nhttps://doi.org/10.48550/ARXIV.2303.13375 (2023).\n19. Singhal, K. et al. Large language models encode clinical knowledge.\nNature 620, 172–180 (2023).\n20. Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. & Sontag, D. Large\nlanguage models are few-shot clinical information extractors. Preprint\nat https://doi.org/10.48550/ARXIV.2205.12689 (2022).\n21. Hu, Y. et al. Improving large language models for clinical named entity\nrecognition via prompt engineering.J. Am. Med. Inform. Assoc.31,\n1812–1820 (2024).\n22. Sivarajkumar, S., Kelley, M., Samolyk-Mazzanti, A., Visweswaran, S.\n& Wang, Y. An empirical evaluation of prompting strategies for large\nlanguage models in zero-shot clinical natural language processing:\nalgorithm development and validation study.JMIR Med. Inform.12,\ne55318 (2024).\n23. Peng, C. et al. Generative large language models are all-purpose text\nanalytics engines: text-to-text learning is all your need.J. Am. Med.\nInform. Assoc.31, 1892–1903 (2024).\n24. Burford, K. G., Itzkowitz, N. G., Ortega, A. G., Teitler, J. O. & Rundle, A.\nG. Use of generative AI to identify helmet status among patients with\nmicromobility-related injuries from unstructured clinical notes.JAMA\nNetw. Open7, e2425981 (2024).\n25. Hu, D., Liu, B., Zhu, X., Lu, X. & Wu, N. Zero-shot information\nextraction from radiological reports using ChatGPT.Int. J. Med. Inf.\n183, 105321 (2024).\n26. Huang, J. et al. A critical assessment of using ChatGPT for extracting\nstructured data from clinical notes.Npj Digit. Med.7, 106 (2024).\n27. Johnson, B. et al. Large language models for extracting\nhistopathologic diagnoses from electronic health records. Preprint at\nhttps://doi.org/10.1101/2024.11.27.24318083 (2024).\n28. Le Guellec, B. et al. Performance of an open-source large language\nmodel in extracting information from free-text radiology reports.\nRadiol. Artif. Intell.6, e230364 (2024).\n29. Liu, F. et al. Large language models are poor clinical decision-makers:\na comprehensive benchmark. Preprint athttps://doi.org/10.1101/\n2024.04.24.24306315 (2024).\n30. Omiye, J. A., Gui, H., Rezaei, S. J., Zou, J. & Daneshjou, R. Large\nlanguage models in medicine: the potentials and pitfalls: a narrative\nreview. Ann. Intern. Med.177, 210–220 (2024).\n31. Sushil, M. et al. A comparative study of large language model-based\nzero-shot inference and task-speciﬁc supervised classiﬁcation of\nbreast cancer pathology reports.J. Am. Med. Inform. Assoc.31,\n2315–2327 (2024).\n32. Wang, L. L. et al. Automated metrics for medical multi-document\nsummarization disagree with human evaluations.Proc. Conf. Assoc.\nComput. Linguist. Meet.2023, 9871–9889 (2023).\n33. Wornow, M. et al. The shaky foundations of large language models\nand foundation models for electronic health records.Npj Digit. Med.6\n,\n135 (2023).\n34. Tang, L. et al. Evaluating large language models on medical evidence\nsummarization. Npj Digit. Med.6, 158 (2023).\n35. Reichenpfader, D., Müller, H. & Denecke, K. A scoping review of large\nlanguage model based approaches for information extraction from\nradiology reports.Npj Digit. Med.7, 222 (2024).\n36. Tian, S. et al. Opportunities and challenges for ChatGPT and large\nlanguage models in biomedicine and health.Brief. Bioinform.25,\nbbad493 (2023).\n37. Willard, B. T. & Louf, R. Efﬁcient guided generation for large language\nmodels. Preprint athttps://doi.org/10.48550/ARXIV.2307.09702\n(2023).\n38. Dong, Y. et al. XGrammar:ﬂexible and efﬁcient structured generation\nengine for large language models. Preprint athttps://doi.org/10.\n48550/ARXIV.2411.15100 (2024).\n39. Fleming, S. L. et al. Medalign: a clinician-generated dataset for\ninstruction following with electronic medical records. inProceedings\nof the AAAI Conference on Artiﬁcial Intelligencevol. 38 22021–22030\n(2024).\n40. McIntosh, T. R. et al. Inadequacies of large language model\nbenchmarks in the era of generative artiﬁcial intelligence. Preprint at\nhttps://doi.org/10.48550/ARXIV.2402.09880 (2024).\n41. Zhong, T. et al. Evaluation of OpenAI o1: opportunities and challenges\nof AGI. Preprint athttps://doi.org/10.48550/ARXIV.2409.18486\n(2024).\n42. Goel, A. et al. Llms accelerate annotation for medical information\nextraction. inMachine Learning for Health (ML4H)82–100 (PMLR,\n2023).\n43. OpenAI et al. GPT-4o system card. Preprint athttps://doi.org/10.\n48550/ARXIV.2410.21276 (2024).\n44. Lavu, H. & Yeo, C. J. Metastatic renal cell carcinoma to the pancreas.\nGastroenterol. Hepatol.7, 699–700 (2011).\nhttps://doi.org/10.1038/s41746-025-01686-z Article\nnpj Digital Medicine|           (2025) 8:301 12\n45. OpenAI. tiktoken. OpenAI (2025).\n46. Gage, P. A new algorithm for data compression.C. Users J.12,2 3–38\n(1994).\n47. Oien, K. A. & Dennis, J. L. Diagnostic work-up of carcinoma of\nunknown primary: from immunohistochemistry to molecular proﬁling.\nAnn. Oncol.23, x271–x277 (2012).\n48. Gratta ﬁori, A. et al. The Llama 3 Herd of models. Preprint athttps://doi.\norg/10.48550/ARXIV.2407.21783 (2024).\n49. Qwen et al. Qwen2.5 technical report. Preprint athttps://doi.org/10.\n48550/arXiv.2412.15115 (2025).\n50. De Bruijn, I. et al. Analysis and visualization of longitudinal genomic\nand clinical data from the AACR project GENIE Biopharma\nCollaborative in cBioPortal.Cancer Res.83, 3861–3867 (2023).\n51. Cerami, E. et al. The cBio cancer genomics portal: an open platform for\nexploring multidimensional cancer genomics data.Cancer Discov.2,\n401–404 (2012).\n52. Gao, J. et al. Integrative analysis of complex cancer genomics and\nclinical proﬁles using the cBioPortal.Sci. Signal. 6, pl1 (2013).\n53. Kefeli, J. & Tatonetti, N. TCGA-Reports: a machine-readable\npathology report resource for benchmarking text-based AI models.\nPatterns 5, 100933 (2024).\n54. Ciriello, G. et al. Comprehensive molecular portraits of invasive lobular\nbreast cancer.Cell 163, 506–519 (2015).\n55. Makhlouf, S. et al. The clinical and biological signiﬁcance of estrogen\nreceptor-low positive breast cancer.Mod. Pathol.36, 100284 (2023).\n56. Abeshouse, A. et al. The molecular taxonomy of primary prostate\ncancer. Cell 163, 1011–1025 (2015).\n57. Shool, S. et al. A systematic review of large language model (LLM)\nevaluations in clinical medicine.BMC Med. Inform. Decis. Mak.25,\n117 (2025).\n58. Ho, C. N. et al. Qualitative metrics from the biomedical literature for\nevaluating large language models in clinical decision-making: a\nnarrative review.BMC Med. Inform. Decis. Mak.24, 357 (2024).\n59. Büscheck, F. et al. Aberrant expression of membranous carbonic\nanhydrase IX (CAIX) is associated with unfavorable disease course in\npapillary and clear cell renal cell carcinoma.Urol. Oncol. Semin. Orig.\nInvestig. 36, 531.e19–531.e25 (2018).\n60. Murugan, P. Protocol for the examination of resection specimens from\npatients with renal cell carcinoma. (2024).\n61. Murugan, P. Protocol for the examination of biopsy specimens from\npatients with renal cell carcinoma. (2024).\n62. Microsoft Corporation. Prompt\nﬂow. Microsoft (2024).\n63. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large\nlanguage models.Adv. Neural Inf. Process. Syst.35, 24824–24837 (2022).\n64. Yu, Z., He, L., Wu, Z., Dai, X. & Chen, J. Towards better chain-of-\nthought prompting strategies: a survey. Preprint athttps://doi.org/10.\n48550/ARXIV.2310.04959 (2023).\n65. Chen, Q. et al. Towards reasoning era: a survey of long chain-of-\nthought for reasoning large language models. Preprint athttps://doi.\norg/10.48550/ARXIV.2503.09567 (2025).\n66. Yeo, W. J., Satapathy, R., Goh, R. S. M. & Cambria, E. How\ninterpretable are reasoning explanations from prompting large\nlanguage models? Preprint athttps://doi.org/10.48550/ARXIV.2402.\n11863. (2024)\n67. Kim, M. et al. Comprehensive immunoproﬁles of renal cell carcinoma\nsubtypes. Cancers 12, 602 (2020).\nAcknowledgements\nThis work was supported by the NIH-sponsored Kidney Cancer SPORE\ngrant (P50CA196516) and endowment from the Jan and Bob Pickens\nDistinguished Professorship in Medical Science and Brock Fund for Medical\nScience Chair in Pathology. The authors thank the UTSW data warehouse\nteam, who are supported by UL1TR003163, for their assistance in retrieving\nthe data used in this study.\nAuthor contributions\nD.H.: Conceptualization, methodology, data collection, data analysis,\nsoftware, visualization, writing original draft, writing review and editing. A.C.:\nConceptualization, data collection, data analysis, writing review and editing.\nM.H.: Software, methodology, writing review and editing. B.X.:\nConceptualization, data collection, writing review, and editing. A.J.J.:\nWriting review and editing. J.V.: Data collection, clinical expertise, writing\nreview and editing. N.R.: Data collection. A.H.S.: Software. S.C.: Data\ncollection, software. L.G.C: Conceptualization, writing review and editing.\nJ.B.: Writing review and editing, project administration and supervision,\nclinical expertise. A.R.J.: Writing review and editing, project administration\nand supervision. P.K.: Conceptualization, data collection, writing review and\nediting, project administration and supervision, clinical expertise.\nCompeting interests\n[D.H., M.H, A.H.S, A.R.J.] Azure compute credits were provided to Dr.\nJamieson [A.R.J.] and lab members [D.H., M.H., A.H.S.] by Microsoft as part\nof the Accelerating Foundation Models Research initiative. No otherwise\ncompeting interests are declared for these authors. Remaining authors,\n[A.C., B.X, A.J.J., J.V., N.R., S.C., L.G.C., and P.K.] declare no competing\ninterests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01686-z\n.\nCorrespondenceand requests for materials should be addressed to\nDavid Hein.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01686-z Article\nnpj Digital Medicine|           (2025) 8:301 13",
  "topic": "Articulation (sociology)",
  "concepts": [
    {
      "name": "Articulation (sociology)",
      "score": 0.7259182929992676
    },
    {
      "name": "Computer science",
      "score": 0.5940596461296082
    },
    {
      "name": "Extraction (chemistry)",
      "score": 0.5224363207817078
    },
    {
      "name": "Natural language processing",
      "score": 0.4804948568344116
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3372909426689148
    },
    {
      "name": "Political science",
      "score": 0.10417774319648743
    },
    {
      "name": "Chemistry",
      "score": 0.06682035326957703
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I867280407",
      "name": "The University of Texas Southwestern Medical Center",
      "country": "US"
    }
  ],
  "cited_by": 4
}