{
    "title": "TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models",
    "url": "https://openalex.org/W3130395060",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4227371052",
            "name": "Li, Zhuohan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2742703092",
            "name": "Zhuang, Siyuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2355815026",
            "name": "Guo Shiyuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227371059",
            "name": "Zhuo, Danyang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1979946437",
            "name": "Zhang Hao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2751273816",
            "name": "Song, Dawn",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3043869039",
            "name": "Stoica, Ion",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3045733172",
        "https://openalex.org/W2166706236",
        "https://openalex.org/W2991040477",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3121562065",
        "https://openalex.org/W2622263826",
        "https://openalex.org/W3103682594",
        "https://openalex.org/W1598866093",
        "https://openalex.org/W2883830791",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2124653173",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2165928541",
        "https://openalex.org/W2807147113",
        "https://openalex.org/W3025935268",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2977720775",
        "https://openalex.org/W2083842231",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2163751474",
        "https://openalex.org/W3037847693",
        "https://openalex.org/W2979044977",
        "https://openalex.org/W2338908902",
        "https://openalex.org/W3000514857",
        "https://openalex.org/W2342173569",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W2963351145",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3038581078",
        "https://openalex.org/W2786066748"
    ],
    "abstract": "Model parallelism has become a necessity for training modern large-scale deep language models. In this work, we identify a new and orthogonal dimension from existing model parallel approaches: it is possible to perform pipeline parallelism within a single training sequence for Transformer-based language models thanks to its autoregressive property. This enables a more fine-grained pipeline compared with previous work. With this key idea, we design TeraPipe, a high-performance token-level pipeline parallel algorithm for synchronous model-parallel training of Transformer-based language models. We develop a novel dynamic programming-based algorithm to calculate the optimal pipelining execution scheme given a specific model and cluster configuration. We show that TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175 billion parameters on an AWS cluster with 48 p3.16xlarge instances compared with state-of-the-art model-parallel methods. The code for reproduction can be found at https://github.com/zhuohan123/terapipe",
    "full_text": "TeraPipe: Token-Level Pipeline Parallelism for Training\nLarge-Scale Language Models\nZhuohan Li1 Siyuan Zhuang1 Shiyuan Guo1 Danyang Zhuo2 Hao Zhang1 Dawn Song1 Ion Stoica1\nAbstract\nModel parallelism has become a necessity for\ntraining modern large-scale deep language mod-\nels. In this work, we identify a new and or-\nthogonal dimension from existing model paral-\nlel approaches: it is possible to perform pipeline\nparallelism within a single training sequence for\nTransformer-based language models thanks to its\nautoregressive property. This enables a more ﬁne-\ngrained pipeline compared with previous work.\nWith this key idea, we design TeraPipe, a high-\nperformance token-level pipeline parallel algo-\nrithm for synchronous model-parallel training\nof Transformer-based language models. We de-\nvelop a novel dynamic programming-based al-\ngorithm to calculate the optimal pipelining exe-\ncution scheme given a speciﬁc model and clus-\nter conﬁguration. We show that TeraPipe can\nspeed up the training by 5.0x for the largest GPT-\n3 model with 175 billion parameters on an AWS\ncluster with 48 p3.16xlarge instances compared\nwith state-of-the-art model-parallel methods. The\ncode for reproduction can be found at https:\n//github.com/zhuohan123/terapipe\n1. Introduction\nTransformer-based language models (LMs) have revolu-\ntionized the area of natural language processing (NLP) by\nachieving state-of-the-art results for many NLP tasks, in-\ncluding text classiﬁcation, question answering, and text gen-\neration (Brown et al., 2020; Radford et al.). The accuracy of\na Transformer-based LM grows substantially with its model\nsize, attributing to the fact that they can be unsupervisedly\ntrained on almost unlimited text data. Today, a large LM,\nsuch as GPT-3 (Brown et al., 2020), can have more than\n175B parameters, which amounts to 350 GB, assuming 16-\n1UC Berkeley 2Duke University. Correspondence to: Zhuohan\nLi <zhuohan@cs.berkeley.edu>.\nProceedings of the38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\nbit ﬂoating-point numbers. This signiﬁcantly exceeds the\nmemory capacity of existing hardware accelerators, such\nas GPUs and TPUs, which makes model-parallel training\na necessity, i.e., partitioning the model on multiple devices\nduring the training process.\nBecause of the demands for efﬁcient LM training, many\nresearchers and industry practitioners have proposed differ-\nent ways for model parallel training. One approach is to\npartition the weight matrices and dispatch smaller matrix\noperations to parallel devices (Figure 1b; Shoeybi et al.,\n2019; Shazeer et al., 2018). Another approach is to split\na batch of training data into many microbatches and then\nevenly pipeline the layer computations across different mi-\ncrobatches and devices (Figure 1c; Huang et al., 2019).\nUnfortunately, these approaches either introduce excessive\ncommunication overheads between compute devices, or lead\nto reduced efﬁciency due to pipeline “bubbles” (i.e. device\nidle time, see Section 2 and 3.2 for details).\nOur key observation in this paper is that Transformer-based\nlanguage models have a key property: the computation of a\ngiven input token only depends on previous tokens, but not\non future tokens. This lack of dependency on future tokens\nprovides new opportunities for pipeline parallel training.1\nIn particular, it allows us to create a ﬁne-grained pipeline\nwithin a single training sequence for Transformer-based\nLMs, by parallelizing the computation of the current token\non the current layer with the computation of the previous\ntoken on the next layer of the model. For example, in Fig-\nure 1d, we can pipeline the execution across all 5 devices\nwithin a single input sequence. Similar to other synchronous\nmodel parallel training methods, e.g., Gpipe (Huang et al.,\n2019), Megatron-LM (Shoeybi et al., 2019), we do not\nchange the underlying optimization algorithm, so the result-\ning model has exactly the same accuracy.\nHowever, leveraging the token dimension for efﬁcient model\nparallel training raises several challenges. First, if the par-\ntitioning along the token dimension is too ﬁne-grained, it\nleads to under-utilization on devices that require large blocks\n1In this paper, we focus on unidirectional autoregressive lan-\nguage models (e.g., GPT (Radford et al.; Brown et al., 2020))\nbut not bidirectional models like masked language models (e.g.,\nBERT (Devlin et al., 2018)).\narXiv:2102.07988v2  [cs.LG]  28 Sep 2021\nTeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models\n«\u0003\n\u0011ÙŗƖ\nΌőķƖ΋\nÙŊƈ\n\u0011ÙŗƖ\nŗƋô\nÙŊƈ\nèôƖƱ\nŗƋô\nΌôƒő΋\nèôƖƱ\n7UDQVIRUPHU\u0003OD\\HU\u0003\u0014\n7UDQVIRUPHU\u0003OD\\HU\u0003\u0015\n7UDQVIRUPHU\u0003OD\\HU\u00031\u0010\u0014\n7UDQVIRUPHU\u0003OD\\HU\u00031\n(a) Transformer-based LM\n/D\\HU\u0003\u0014\u0003SDUW\u0003\u0014\n/D\\HU\u0003\u0015\u0003SDUW\u0003\u0014\n/D\\HU\u0003\u0016\u0003SDUW\u0003\u0014\n«\u0003'HYLFH\u0003\u0014 'HYLFH\u0003\u0015\n/D\\HU\u0003\u0014\u0003SDUW\u0003\u0014\n/D\\HU\u0003\u0015\u0003SDUW\u0003\u0014\n/D\\HU\u0003\u0016\u0003SDUW\u0003\u0014 (b) Operation partitioning\n(Megatron-LM)\n7UDQVIRUPHU\u0003OD\\HU\u0003\u0014\n7UDQVIRUPHU\u0003OD\\HU\u0003\u0015\n7UDQVIRUPHU\u0003OD\\HU\u0003\u0016\n7UDQVIRUPHU\u0003OD\\HU\u0003\u0017\n7UDQVIRUPHU\u0003OD\\HU\u0003\u0018\n'HYLFH\u0003\u0014\n'HYLFH\u0003\u0015\n'HYLFH\u0003\u0016\n'HYLFH\u0003\u0017\n'HYLFH\u0003\u0018\n(c) Microbatch-based pipeline\nparallelism (GPipe)\n7UDQVIRUPHU\u0003OD\\HU\u0003\u0014\n7UDQVIRUPHU\u0003OD\\HU\u0003\u0016\n7UDQVIRUPHU\u0003OD\\HU\u0003\u0017\n7UDQVIRUPHU\u0003OD\\HU\u0003\u0018\n'HYLFH\u0003\u0014\n'HYLFH\u0003\u0015\n'HYLFH\u0003\u0016\n'HYLFH\u0003\u0017\n'HYLFH\u0003\u0018\n7UDQVIRUPHU\u0003OD\\HU\u0003\u0015\n(d) Token-based pipeline\nparallelism (TeraPipe)\nFigure 1.Different approaches of model parallel training of Transformer-based LMs. (a) shows a standard multi-layer Transformer LM.\nIn each layer, each position only takes only its previous positions as input. (b) shows operation partitioning (Shoeybi et al., 2019). An\nallreduce operation is required to synchronize the results of each layer. (c) shows microbatch-based pipeline parallelism (Huang et al.,\n2019), which allows different microbatches (red and green bars) to be executed on different layers of the DNN in parallel. (d) show\nTeraPipe (our work), which pipelines along the token dimension.\nof data for efﬁcient processing (e.g., GPU). Second, since\neach token position in the sequence depends on all previous\ntokens, different positions in a transformer layer exhibit\nuneven computation loads. This means that uniformly parti-\ntioning along the token dimension might cause uneven load\nacross devices, and degenerate the training efﬁciency.\nTo this end, we design and implement TeraPipe, a high-\nperformance synchronous model parallel training approach\nfor large-scale Transformer-based language models, which\nexploits the token dimension to pipeline the computation\nacross devices. TeraPipe uses a small number of simple\nworkloads to derive a performance model and then uses\na novel dynamic programming algorithm to compute the\noptimal partitioning of the token dimension for the pipeline.\nTeraPipe is orthogonal to previous model-parallel training\nmethods, so it can be used together with these methods to\nfurther improve the training performance. Our evaluation\nshows that for the largest GPT-3 model with 175 billion\nparameters, TeraPipe achieves a 5.0x speedup improvement\nover the state-of-the-art synchronous model-parallel training\nmethods on an AWS cluster consisting of 48 p3.16xlarge\ninstances.\nOur paper makes the following contributions:\n• We propose a new dimension, token dimension, for\npipeline-parallel training of Transformer-based LMs.\n• We develop a dynamic programming algorithm to com-\npute a partition along the token dimension to maximize\npipeline parallelism.\n• We implement TeraPipe and show that we can increase\nthe synchronous training throughput of the largest GPT-\n3 model (with 175 billion parameters) by 5.0x over the\nprevious state-of-the-art model-parallel methods.\n2. Related Work\nData parallelismscales ML training by partitioning train-\ning data onto distributed devices (Zinkevich et al., 2010;\nKrizhevsky, 2014; Goyal et al., 2017; Rajbhandari et al.,\n2019). Each device holds a model replica, works on an\nindependent data partition, and synchronizes the updates\nvia allreduce (Krizhevsky, 2014) or a parameter server (Li\net al., 2014). Data parallelism alone is not enough to train\nlarge-scale DNNs due to two main reasons: (1) every de-\nvice has to have enough memory to store the model and the\ngradients generated during the training process; (2) com-\nmunication can be a performance bottleneck to synchronize\nmodel parameters.\nModel parallelismallows for training models larger than\nthe memory capacity of a single device, by partitioning\nthe model (e.g., layers) into disjoint parts and executing\neach on a dedicated device. Existing model parallel train-\ning approaches can be roughly categorized as: operation\npartitioning and pipeline parallelism.\nOperation partitioning.One way to split the model is to\npartition and parallelize computational operations across\nmultiple devices. For example, the computation of matrix\nmultiplications (matmul) XAB can be spitted across mul-\ntiple devices by partitioning Aand B along its rows and\ncolumns, respectively.\nXAB = X·\n[A1 A2\n]\n·\n[B1\nB2\n]\n= XA1B1 + XA2B2.\nThis means we can have one device calculate XA1B1 and\nanother device calculate XA2B2 in parallel. After that,\ncross-device communication is needed to compute the sum\nof these two parts.\nMany existing works (Jia et al., 2018; 2019; Wang et al.,\n2019; Shazeer et al., 2018) study how to optimize the\nTeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models\npartitioning schemes for different operations to maximize\nthroughput and minimize communication overheads, among\nwhich, Megatron-LM (Figure 1b; Shoeybi et al., 2019) de-\nsigns partitioning schemes speciﬁcally for large-scale Trans-\nformers. However, due to the excessive communication\nrequired to collect partial results after each layer, it is not\nefﬁcient when the bandwidth between devices is limited\n(Shoeybi et al., 2019). Flexﬂow (Jia et al., 2018) proposes a\nframework to ﬁnd the optimal operation partitioning, but it\ncannot model the new dimension proposed in our work.\nPipeline parallelismpartitions a DNN into layers and put\ndifferent layers onto different devices (Figure 1c; Petrowski\net al., 1993). Each device computes the input on a given\nlayer and sends the result to the next device. Pipeline par-\nallelism signiﬁcantly reduces communication between de-\nvices, because only devices holding neighboring layers need\nto communicate and they only need to communicate the\nactivations on a particular layer.\nPrevious pipeline parallel training methods are based on\nmicrobatch pipelining, e.g., GPipe (Huang et al., 2019).\nThis means the computation for a given microbatch in a\nminibatch on a layer can run in parallel with the next micro-\nbatch in the same minibatch on the previous layer. However,\nmicrobatch-based pipeline parallelism still cannot achieve\nhigh efﬁciency due to its pipeline bubbles. This is because\nthe start of the forward propagation on a minibatch requires\nthe backward propagation of the previous minibatch to\ncomplete (Figure 2a). This problem becomes more severe\nwhen model sizes increase (see Section 3.2). Harlap et al.\n(2018) propose using an asynchronous training algorithm to\nmitigate the effect of pipeline bubbles in microbach-based\npipeline parallel training, but asynchronous training intro-\nduces uncertainty in model accuracy and is thus not widely\nadopted for training DNNs.\nWavefront parallelismis a variant of pipeline parallelism,\nbroadly applied in shared-memory multiprocessors (Sin-\nharoy & Szymanski, 1994; Manjikian & Abdelrahman,\n1996). In deep learning, it has been used to accelerate the\ncomputation of multi-layer RNNs on a single GPU (Apple-\nyard et al., 2016), where different input positions of differ-\nent layers can execute in parallel in a wavefront fashion to\nmaximize the utilization of the GPU. However, wavefront\nparallelism cannot accelerate the execution of Transformers\nbecause there is no dependency between different input po-\nsitions within a single Transformer layer to begin with. In\naddition, wavefront parallelism uses ﬁne-grained per-word\npipelining due to the temporal data dependency in RNNs,\nwhile too ﬁne-grained pipelining in TeraPipe would lead to\ninferior pipeline efﬁciency (see Section 3.2 and 3.3).\n*38\u0003\u0014\n7LPH\n*38\u0003\u0015\n*38\u0003\u0016\n*38\u0003\u0017\n)RUZDUG %DFNZDUG\n(a) Microbatch-based pipeline parallelism\n*38\u0003\u0014\n7LPH\n*38\u0003\u0015\n*38\u0003\u0016\n*38\u0003\u0017\n)RUZDUG %DFNZDUG\n(b) Microbatch-based pipeline parallelism with small batch size\n*38\u0003\u0014\n7LPH\n*38\u0003\u0015\n*38\u0003\u0016\n*38\u0003\u0017\n)RUZDUG %DFNZDUG )RUZDUG\n(c) TeraPipe\nFigure 2.Execution timeline for different pipelining methods.\nGrey blocks indicate GPUs idle time (a.k.a. pipeline bubbles).\n(a) Microbatch-based pipeline parallelism (e.g. GPipe). Each color\ncorresponds to a microbatch. (b) Microbatch-based pipeline paral-\nlelism with longer sequence (hence smaller minibatch size due to\nﬁxed GPU memory). Pipeline bubbles signiﬁcantly increase. (c)\nTeraPipe. Pipeline bubbles are substantially reduced because of\nthe improved pipelining granularity.\n3. Method\nIn this section, we brieﬂy introduce language modeling\nand Transformers. Based on their structures, we identify\nnew opportunities for performing pipelining along the input\nsequence (which we will notate as thetoken dimensionin the\nrest of the paper). With that, we derive the optimal slicing\nscheme over the token dimension to maximize pipeline\nefﬁciency using a dynamic programming algorithm. Finally,\nwe show how to combine our new method with existing\nparallel training techniques.\n3.1. Language Modeling and Transformers\nThe task of language modeling is usually framed as unsu-\npervised distribution estimation of a text corpus X, where\neach example x∼X is a variable length sequence of tokens\n(x1,x2,...,x L).Since language has a natural sequential\nordering, it is common to factorize the joint probability over\nthe tokens as the product of conditional probabilities (a.k.a.\nautoregressive decomposition; Bengio et al., 2003):\nP(x) =\nL∏\nt=1\nP(xt|x1, . . . , xt−1). (1)\nTransformer (Vaswani et al., 2017) is the state-of-the-art\narchitecture for modeling these conditional probabilities. As\nTeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models\nvisualized in Figure 1a, a Transformer-based LM F takes\nthe sequence (⟨sos⟩,x1,...,x L−1) as input, where ⟨sos⟩\nrepresents the start of a sentence, and outputs a probability\ndistributions pt at each positiontthat models the conditional\nprobability P(xt|x1,...,x t−1) as in Eq. 1. In practice, F\nis stacked with many Transformer layers F = fN ◦fN−1 ◦\n···◦ f1 (Vaswani et al., 2017; Radford et al.): f1 takes\nthe embedding of the original sequence as input, while\nfi (i >1) takes the output of fi−1 as input. The main\ncomponents of a Transformer layerfcontain a self-attention\nlayer and a position-wise feed-forward network layer:\nSelfAtt(ht; h1,...,h t−1) =\nt∑\ns=1\nαts ·(WV hs),\nwhere αts = softmax\n((WQht)⊤(WKhs)√\nH\n)\n; (2)\nFFN(ht) =W2σ(W1ht + b1) +b2. (3)\nh1,...,h L ∈RH are hidden states correspond to each posi-\ntion of the input sequence,W and bare learnable parameters,\nand σ is the nonlinear activation function. An important\nnote here: for each ht, Eq. 2 takes only the hidden states\nbefore position tas inputs and Eq. 3 only takes ht as input.\nThe operation and data dependency in Transformers make it\nmore amenable to parallelization on GPUs/TPUs compared\nto RNNs (Vaswani et al., 2017). Therefore, Transformers\nhave been scaled to enormous datasets and achieved state-of-\nthe-art performance on a wide range of NLP tasks (Vaswani\net al., 2017; Devlin et al., 2018; Radford et al.; Yang et al.,\n2019; Brown et al., 2020; Liu et al., 2019). Recently, people\nshow that the accuracy of LMs can consistently improve\nwith increasing model sizes (Radford et al.; Yang et al.,\n2019). While the growing model size greatly exceeds the\nmemory capacity of a single GPU (Brown et al., 2020),\nmodel parallelism becomes a necessity for training large-\nscale LMs (Shoeybi et al., 2019).\n3.2. Pipeline Parallelism Within a Sequence\nIn this subsection, we expose the limitations of existing\npipelining parallelism approaches, and develop the proposed\nnew pipelining method for Transformer-based LMs.\nTypically, to perform pipeline parallelism, a Transformer\nmodel F is partitioned into multiple cells c1,...,c K. Each\ncell ck consists of a set of consecutive Transformer layers\nfj ◦···◦ fi+1 ◦fi so that F = cK ◦···◦ c2 ◦c1. Each\nck is placed and executed on the k-th device (e.g. GPU).\nThe output of cell ck is sent to cell ck+1 during forward\npropagation, and the backward states computed on cell ck+1\nis sent to cell ck during backward propagation. Since each\nlayer f exhibits the same structure, the entire LM can be\nuniformly partitioned: each cell possesses the same number\nof layers hence the same amount of computation workload,\nto reach optimal pipeline efﬁciency (see Figure 2).\nHowever, previous pipelining methods (Huang et al.,\n2019; Harlap et al., 2018) do not perform well on large\nTransformer-based LMs due to the growing model size.\nConsider a minibatch of size B. The input to a Transformer\nlayer f is a 3-dimensional tensor (h(1),h(2),...,h (B)) of\nsize (B,L,H ),where Lis the sequence length and H is\nthe hidden state size. To improve accuracy, large LMs are\noften conﬁgured to have a large Lto capture longer-term\ndependency in language sequences (Tay et al., 2020; Zaheer\net al., 2020). To ﬁt the model into a GPU, the minibatch\nsize B has to decrease accordingly. The pipeline bubbles\nbecome larger (Figure 2b) because fewer input sequences\ncan be processed in parallel.\nIn this work, we make a key observation: for Transformer-\nbased LMs, with appropriate scheduling, the token dimen-\nsion Lcan be pipelined for parallel training; and this pipelin-\ning dimension is complementary to other model parallelism\napproaches. Precisely, for an input hidden state sequence\n(h1,h2,...,h L), the computation of a self-attention layer\nSelfAtt(ht) only depends on the hidden states of previous\npositions (h1,...,h t−1), and the computation of a feed-\nforward layer FFN(ht) only depends on ht itself. These\noffer a new opportunity for pipelining: the computation of\nlayer fi at step tcan commence once the hidden states of\nprevious steps ( < t) at fi−1 are ready, which, also, can\nbe parallelized with the computation of latter steps at fi−1,\nillustrated in Figure 1d. This property enables us to per-\nform pipeline parallelism within a single input sequence.\nSpeciﬁcally, we can split an input sequence x1,...,x L\ninto s1,...,s M , where each subsequence si consists of\ntokens (xl,xl+1,...,x r).The computation of c1,...,c K\nover s1,...,s M can be pipelined, for example: when ck\ncomputes over si, ck+1 can process si−1 and ck−1 can pro-\ncess si+1 in parallel.\nConsidering that nowadays LMs operate on sequences with\nthousands of tokens (Radford et al.; Brown et al., 2020) (e.g.\n2048 for GPT-3), the token dimension opens substantial\nspace to improve the pipelining efﬁciency. However, apply-\ning it in practice is still challenging, especially on GPUs,\nfor the following reasons.\nFirst, ﬁner-grained pipelining (i.e. picking a small |si|) is\nprone to underutilizing the computational power of GPUs,\nand thus lowering the training throughput. As shown on\nthe top part of Figure 3, for a single layer of the GPT3-1B\nmodel (see Table 1 for specs), the forward propagation time\nfor an input sequence with a single token is the same as an\ninput sequence with 256 tokens. In this case, the GPU is not\nbeing fully utilized for input sequence lengths less than 256.\nThis means a large subsequence length is needed to achieve\nhigh throughput for a single layer (see the bottom part of\nFigure 3). On the other hand, although GPUs have better\nTeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models\n1.5\n2.0\n2.5Time (ms)\n0 200 400 600 800 1000\n# Input tokens\n0\n100\n200\n300\n400Throughput (tokens / ms)\nFigure 3.Forward propagation time and throughput for a single\nlayer of GPT3-1B model with a single input sequence with dif-\nferent number of input tokens on a single NVIDIA V100 GPU,\naveraged by 30 independent runs. Top: Time per forward propa-\ngation. Bottom: Throughput measured by number of tokens per\nmillisecond.\ntraining throughput per layer for longer sequences due to\nthe SIMD architecture and better locality, longer input slices\nlead to fewer pipeline stages within a sequence, which will\nincrease the pipeline bubble, and thus reduce the pipeline\nefﬁciency and hurt the overall training speed.\nSecond, splitting inputs into multiple same-size chunks for\npipelining, as normally done in existing work (Huang et al.,\n2019; Harlap et al., 2018), is not the ideal way for pipelin-\ning on the token dimension. For the self-attention layer,\nthe computation of SelfAtt(h1) only requires the hidden\nstate h1 from its previous layer, while the computation of\nSelfAtt(hL) takes all h1,...,h L as inputs, as shown in Fig-\nure 1a. Therefore, the computation load on a later token\nposition in a sequence is heavier than that of previous to-\nkens. Since the total latency of a pipeline is determined\nby its slowest stage (Figure 4), an optimal slicing scheme\nshould have a long slice in the beginning and a shorter slice\nin the end. We next develop methods to select the optimal\nslicing scheme over the token dimension.\n3.3. Selecting Optimal Slicing Scheme\nWe propose a dynamic programming (DP) algorithm to par-\ntition the input sequence to achieve the optimal pipeline\nefﬁciency. Speciﬁcally, given a partitioned Transformer-\nbased LM F = cK ◦···◦ c1 and a training input sequence\nof length L, the goal of the algorithm is to ﬁnd the slicing\nscheme l1,...,l M to minimize the total forward and back-\nward propagation latency, where li = |si|is the length each\nsub-sequence slice si (l1 + ··· + lM = L).\nLet’s ﬁrst consider the latency of forward propagation. As\nshown in Section 3.2, all cells ck have exact same amount\nof computation.\n*38\u0003\u0014\n*38\u0003\u0015\n*38\u0003\u0016\n*38\u0003\u0017\nW\u0014 W\u0015 W\u0016 W\u0017\nW\u0016 W\u0017\nW\u0016\nW\u0016\nW\u0017\nW\u0017\n)RUZDUG %DFNZDUG\n*38\u0003\u0014\n7LPH\n*38\u0003\u0015\n*38\u0003\u0016\n*38\u0003\u0017\nW\u0014 W\u0015 W\u0016 W\u0017\n)RUZDUG %DFNZDUG\nW\u0014 W\u0015 W\u0016 W\u0017\nW\u0014 W\u0015 W\u0016 W\u0017\nW\u0014 W\u0015 W\u0016 W\u0017\nW\u0014\nW\u0014\nW\u0014\nW\u0015\nW\u0015\nW\u0015\nFigure 4.Execution timeline for inputs for uniform sequence split\nwith non-uniform running time (top) and non-uniform sequence\nsplit with uniform running time (bottom). The total latency of a\npipeline is determined by its slowest stage, and thus splits with\nnon-uniform running time result in larger pipeline bubbles and\ninferior pipeline efﬁciency.\nThe forward propagation time ti for the slice si on the\ncell ck is determined by the length of the ith slice (li), the\nlengths of all the previous subsequences (l1,...,l i−1), and\nthe cluster speciﬁcations (e.g., GPU, bandwidth and latency\nof the underlying computer networks). We use tfwd to\ndenote the sum of the computation latency plus data trans-\nmission latency for a givenli and the previous subsequences\nl1,...,l i−1. We have:\nti = tfwd\n\nli,\ni−1∑\nj=1\nlj\n\n. (4)\nNote the second term ∑i−1\nj=1 lj is the total length of previ-\nous subsequences s1,...,s i−1 to compute SelfAtt(st). As\nvisualized in Figure 4, The optimal overall pipeline forward\npropagation latency is:\nT∗= min\nl1,...,lM\n{M∑\ni=1\nti + (K−1) · max\n1≤j≤M\n{tj}\n}\n. (5)\nThe overall latency consists of two terms: The ﬁrst term\nhere is the total forward propagation time on a device (i.e.\non a cell ck); The second term is the overhead brought by\nthe pipeline execution, which is determined by the slowest\ncomponent in the whole pipeline multiplied by the number\nof pipeline stages K minus 1. For example, on the top of\nFigure 4, the total execution time will be T = (t1 + ... +\nt4) + 3t4.\nOur goal is to ﬁnd the optimal slicing scheme l1,...,l M\nthat achieves the optimal latency T∗. We choose to ﬁrst\nenumerate the second term tmax = max1≤j≤M {tj}and\nminimize the ﬁrst term for each different tmax . In other\nTeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models\nAlgorithm 1Selecting optimal slicing scheme given tmax .\nInput: Forward propagation time function tfwd and max-\nimum per-slice time tmax .\nOutput: Minimal total forward propagation time\nS∗(L; tmax ) and the corresponding slicing scheme\nl1,...,l M .\n// Dynamic programming for the total forward propaga-\ntion time.\nS∗(0; tmax ) ←0\nfor ifrom 1 to Ldo\nS∗(i; tmax ) ← min1≤k≤i{S∗(i − k; tmax ) +\ntfwd (k,i −k) |tfwd (k,i −k) ≤tmax }.\nqi ←argmin1≤k≤i{ri−k +tfwd (k,i−k) |tfwd (k,i−\nk) ≤tmax }.\nend for\n// Derive the optimal slicing scheme.\ni←L,l ←{}\nwhile i> 0 do\nl.prepend(qi)\ni←i−qi\nend while\nwords, we reformulate T∗as:\nT∗= min\ntmax\n{S∗(L; tmax ) + (K−1) ·tmax }, (6)\nS∗(L; tmax ) = min\nl1+···+lM=L\n{M∑\ni=1\nti |ti ≤tmax\n}\n. (7)\nNote that S∗(·; tmax ) has the following optimal substruc-\nture:\nS∗(i; tmax ) = min\n1≤k≤i\n{S∗(i−k; tmax ) +tfwd (k,i −k)\n|tfwd (k,i −k) ≤tmax }. (8)\nTherefore, we can get the slicing scheme l1,...,l M\nthat achieves the total total forward propagation time\nS∗(L; tmax ) with Algorithm 1. By enumerating all different\ntmax ,we can get the optimal slicing scheme that reaches\nthe optimal overall pipeline latency T∗.\nComplexity. With our DP algorithm, we can compute the\nbest partition in O(L2) time for a ﬁxed tmax . Note that in\ntotal there are at most O(L2) different choices (tfwd (i,j)\nfor i,j = 1,...,L ) of tmax . We therefore can derive the\noptimal slicing scheme in O(L4) time.\nOptimization. To further accelerate the above DP algo-\nrithm, we enumerate different tmax from small to large;\nwhen K·tmax is greater than the current best T, we stop\nthe enumeration since larger tmax cannot provide a better\nslicing scheme. In addition, during enumeration of tmax ,\nwe only evaluate with tmax larger than the last tmax by at\nleast ε. In this case, the gap between the solution found by\nthe DP algorithm and the global optima is at most K·ε. We\nchoose ε = 0.1 ms in our evaluation and observe that the\nsolution given by Algorithm 1 and the real optimal solution\n(ε = 0) are always the same in all our evaluated settings.\nWith these two optimizations, the dynamic programming\ncan ﬁnish within a minute in our evaluations.\nEstimating tfwd . To avoid the cost of evaluatingtfwd (i,j)\nfor all O(L2) combinations of i,j on real clusters, we use\na simple performance model to estimate tfwd .Speciﬁcally,\nwe split tfwd (i,j) into two terms:\ntfwd (i,j) =tfwd (i,0) +tctx (i,j), (9)\nwhere tfwd (i,0) is the forward propagation time without\nany extra context input andtctx (i,j) is the latency overhead\nbrought by the extra context input. We measure the ﬁrst\nterm with all Lchoices of iand we ﬁt a simple linear model\ntctx (i,j) =a0 + a1i+ a2j+ a3ijfor the second term with\na subset of all (i,j) combinations. In our experiments, the\nlinear model can achieve a <2% relative prediction error\ncompared to the actual overhead.\nThe development above can be applied to backward propaga-\ntion time tbwd , since the backward propagation computation\nin transformers is symmetric with its forward counterpart.\nOne step further, we can replace all the tfwd above with\ntfwd + tbwd to derive the optimal slicing scheme that mini-\nmizes the total training time.\n3.4. Combining with Other Parallel Training methods\nThe new dimension to perform pipeline parallelism pro-\nposed by TeraPipe is orthogonal to all previous model paral-\nlel techniques, hence can be naturally combined with them.\nWe explain next how TeraPipe can be combined with other\nparallelization methods and show, when combined, it signif-\nicantly boosts parallelization performance in Section 4.\nCombine with microbatch-based pipeline parallelism.\nTo combine with microbatch-based pipeline parallelism\n(Huang et al., 2019), we slice the batch dimen-\nsion and the token dimension jointly to form the\npipeline. Speciﬁcally, consider a training input batch\n(x(1),x(2),...,x (B)),where each x(i) is an input sequence\n(x(i)\n1 ,...,x (i)\nL ) of length L, we partition the input batch\ninto (s(1),s(2),...,s (D)), such that each s(d)\ni includes\n(x(a)\nl ,x(a)\nl+1,...,x (a)\nr ), (x(a+1)\nl ,x(a+1)\nl+1 ,...,x (a+1)\nr ), ...,\n(x(b)\nl ,x(b)\nl+1,...,x (b)\nr ), which is the subsequence from po-\nsition l to r of input data a to b. During training, all\nslices s(1)\n1 ,...,s (1)\nM ,s(2)\n1 ,...,s (2)\nM ,...,s (D)\n1 ,...,s (D)\nM can\nexecute on cellsc1,...,c K in a pipelined fashion. To jointly\noptimize the sequence slicing and batch splitting, the DP al-\ngorithm in Section 3.3 can be extended to include the batch\ndimension: we can ﬁrst run the whole DP algorithm in Sec-\ntion 3.3 for all different batch sizes bfrom 1 to B.For each\nTeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models\nTable 1.Model settings and parallel training setups used in the evaluation. N: Number of Transformer layers. H: Hidden state size.\n#Params: Number of total parameters. L: Input sequence length. #GPUs: Total number of GPUs. B: Batch size. #Data: Number of data\nparallel shards. #Pipe: Number of pipeline stages. #Op: Number of GPUs used for operational partitioning by each Transformer layer.\nModel N H #Params L #GPUs B #Data #Pipe #Op\n(1)\nGPT3-1B 24 2048 1B 2048 192\n128 8 24 1\n(2) 72 2 12 8\n(3) 72 1 24 8\n(4) GPT3-13B 40 5120 13B 2048 320 32 2 20 8\n(5) 32 1 40 8\n(6)\nGPT3-44B 96 6144 44B 2048 384\n8 4 96 1\n(7) 8 2 24 8\n(8) 8 1 48 8\n(9) GPT3-175B 96 12288 175B 2048 384 2 1 96 4\n(10) 2 1 48 8\n(1) (2) (3)0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6Latency (s)\nw/o TeraPipe\nw/ TeraPipe\n(a) GPT3-1B\n(4) (5)0.0\n0.5\n1.0\n1.5\n2.0\n2.5Latency (s)\nw/o TeraPipe\nw/ TeraPipe (b) GPT3-13B\n(6) (7) (8)0\n2\n4\n6\n8\n10\n12\n14Latency (s)\nw/o TeraPipe\nw/ TeraPipe (c) GPT3-44B\n(9) (10)0\n2\n4\n6\n8\n10Latency (s)\nw/o TeraPipe\nw/ TeraPipe (d) GPT3-175B\nFigure 5.Training iteration latency for all conﬁgurations with and without TeraPipe. Details for each conﬁguration are listed in Table 1.\nb, we derive the optimal Tb and the corresponding slicing\nscheme sb.With all Tb and sb,we only need to determine\nthe size of each slice in the batch dimensionb1,...,b D such\nthat b1 + ··· + bD = Band Tb1 + ··· + TbD is minimized.\nThis reduces to a 1D knapsack problem and can be solved\nusing off-the-shelf solvers.\nCombine with operation partitioning.TeraPipe is orthog-\nonal from operation partitioning in the sense that: opera-\ntion partitioning is intra-operation parallelism that paral-\nlelizes the execution of a single operation, whereas TeraPipe\npipelines the execution of different operations. To com-\nbine with operation partitioning, we distribute each pipeline\nparallel cell cK to a set of target devices and then perform\noperation partitioning across target devices.\nCombine with data parallelism.Similarly, because data\nparallelism maintains multiple identical copies of the model,\nwe can perform model parallelism for each data parallel\nmodel replica and synchronize the gradient updates between\nthe replicas after each forward and backward propagation.\nCombine with memory optimization.Same as previous\npipeline parallel methods (Huang et al., 2019), TeraPipe\nstores the activations of a whole mini-batch in our imple-\nmentation. TeraPipe can also be combined with various\nmemory optimization techniques, e.g., gradient accumula-\ntion (Fan et al., 2020), rematerialization (Chen et al., 2016;\nJain et al., 2019), or memory swapping (Ren et al., 2021).\nSee supplementary material for more discussions on com-\nbining TeraPipe with gradient accumulation.\n4. Evaluation\nTeraPipe is a synchronous model parallel training method\nthat performs exactly the same underlying optimization al-\ngorithm as training the model on a single device. The opti-\nmization performance of TeraPipe (i.e. training loss versus\ntraining iterations) is hence the same compared to training\non a single device. Therefore, in this paper, we focus on the\nper-iteration latency (i.e. wall-clock time used per training\niteration) as our evaluation metric.\nWe evaluate TeraPipe following the setup in Brown et al.\n(2020). Speciﬁcally, we test 3 settings in Brown et al. (2020):\nGPT3-1B, GPT3-13B, and GPT3-175B, which have 1 bil-\nlion, 13 billion, and 175 billion parameters in total, respec-\ntively. Note that GPT3-175B is the largest setting in Brown\net al. (2020). In addition, we also test on a GPT3-44B model\nwith half the hidden state size H of the GPT3-175B model,\nwhich includes 44 billion parameters in total.\nFor each model, we select multiple data parallelism, oper-\nTeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models\n1 4 8 16DP\n#Slices\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5Latency (s)\n(a) GPT3-44B (8)\n1 4 8 16 32 64 128DP\n#Slices\n0\n2\n4\n6\n8\n10Latency (s) (b) GPT3-175B (9)\nFigure 6.Training iteration latency of TeraPipe with uniform slic-\ning scheme with different number of slices and the optimal slicing\nscheme ﬁnd by the dynamic programming algorithm.\nation partitioning, and pipeline parallelism setup combina-\ntions. The conﬁguration details are shown in Table 1. For all\nconﬁgurations, we set the input sequence length L= 2048\nfollowing Brown et al. (2020). We evaluate the conﬁgu-\nrations on an AWS cluster with p3.16xlarge nodes (each\nwith 8 NVIDIA V100 GPUs). For each model, we select a\ncluster size based on its model size and number of layers\nso that each pipeline stage (each cell ck) has the same num-\nber of layers. Since operation partitioning requires higher\ninter-connection speed compared to pipeline parallelism, we\nperform operation partitioning only inside a node, where all\nGPUs have high-speed inter-connection thanks to NVLink.\nFor each conﬁguration, we select the maximal batch size\nthat can ﬁt the memory of the GPUs.\nWe compare the per-iteration latency achieved by previous\nmodel parallel methods without TeraPipe and the latency\nachieved by TeraPipe for each conﬁguration. Speciﬁcally,\nfor the setup without TeraPipe, we measure the training\nlatency with GPipe (Huang et al., 2019) as the pipeline par-\nallel training method. For TeraPipe, we perform a joint\ndynamic programming on both batch and token dimension\nas shown in Section 3.4 and measure the training latency\nwith the optimal slicing scheme found by the dynamic pro-\ngramming algorithm. All the latency results in the paper are\naveraged over 10 runs. The detailed numbers of the latency\nresults and the solution ﬁnd by the dynamic programming\nalgorithm can be found in the supplementary material.\n4.1. Main Results\nWe show the latency results for all conﬁgurations in Fig-\nure 5. TeraPipe accelerates the training for all models: For\nGPT3-1B, TeraPipe accelerates training for setting (1) by\n1.21x. For setting (2) and (3), because of the large batch\nsize, the optimal slicing scheme found by our dynamic pro-\ngramming algorithm only slices the batch dimension and\nthus TeraPipe does not provide speedup. For GPT3-13B,\nTeraPipe speeds up the training by 1.40x for both setting (4)\nand (5). For GPT3-44B, TeraPipe accelerates the training by\n1.88x, 1.56x, and 2.40x for setting (6), (7), and (8), respec-\n2048 4096 6144 8192\nInput sequence length\n0\n1\n2\n3\n4\n5Latency (s)\nw/o TeraPipe\nw/ TeraPipe\nFigure 7.Training iteration latency of TeraPipe with different input\nsequence length for the GPT3-13B model.\ntively. For GPT3-175B, TeraPipe accelerates the training by\n6.75x and 5.02x for setting (9) and (10), respectively.\nTeraPipe provides higher speedup for larger models: Larger\nmodels have a larger hidden state size H,and a larger por-\ntion of GPU memory is devoted to storing the model weights\nand hidden states. Therefore, the batch size B has to be\ndecreased to ﬁt the model into the GPU memory, as shown\nin the setup in Table 1. Smaller batch size Blimits the pre-\nvious microbatch-based pipeline parallel methods’ ability\nto saturate the pipeline bubbles, while the token dimension\nused by TeraPipe still provides abundant opportunity to im-\nprove pipeline efﬁciency. In addition, larger models have\nmore pipeline stages compared to smaller models, because\nlarger models have more layers and each layer takes more\nmemory than the smaller models. More pipeline stages\nrequire more input slices to saturate the pipeline.\n4.2. Dynamic Programming\nIn this subsection, we provide an ablation study on the effec-\ntiveness of the dynamic programming algorithm proposed in\nSection 3.3. We compare the training latency with the slic-\ning scheme found by the dynamic programming algorithm,\nto a simple heuristic that slices the input sequence uniformly.\nSpeciﬁcally, we evaluate GPT3-44B with setting (8) and\nGPT3-175B with setting (9). For the uniform slicing base-\nline, we slice the whole input on the batch dimension and\nrange the number of slices on the token dimension from 1 to\n16 and 1 to 128 for two settings, respectively, and evaluate\nthe iteration latency for each uniform slicing scheme.\nThe result is shown in Figure 6. As in Section 3.2, too ﬁne-\ngrained pipeline (e.g. #slices=128 in Figure 6b) performs\nbadly because of the underutilization of the GPUs. Also,\ntoo coarse-grained pipeline (e.g. #slices=4 in Figure 6b)\nhas large pipeline bubbles, which leads to high iteration la-\ntency. In addition, because of the non-uniform running time\nbrought by the Transformer structure, the slicing scheme de-\nrived by the dynamic programming program achieves better\nperformance compared to the best uniform sliced pipeline:\nthe optimal solutions found by dynamic programming are\n1.12x and 1.04x faster compared to the best uniform slicing\nscheme for GPT3-44B and GPT3-175B model, respectively.\nTeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models\n4.3. Longer Sequence Length\nA growing set of works start to focus on increasing the input\nsequence length of the Transformers (Tay et al., 2020; Za-\nheer et al., 2020; Kitaev et al., 2020). Long sequence length\nenables Transformers to reason about long-term dependen-\ncies and thus extends its applicability to more complex ap-\nplications such as modeling documents. However, longer\nsequences increases the memory usage of a single input\nsequence, and decreases the maximum batch size allowed,\nwhich limits the pipeline efﬁciency of previous microbatch-\nbased pipeline parallelism methods.\nIn this subsection, we vary the sequence length from 2048 to\n8192 for the GPT3-13B model (setting (5)) and evaluate the\ntraining iteration latency. Because of the growth in memory\nusage, the batch sizes for sequence length 4096, 6144, 8196\nare reduced to 8, 4, 2, respectively. We show the results in\nFigure 7. TeraPipe achieves 2.76x, 4.97x, 7.83x speedup\nfor sequence length 4096, 6144, and 8196, respectively.\nAs the sequence length grows, the gap between the perfor-\nmance with and without TeraPipe signiﬁcantly increases,\nas expected. Meanwhile, longer sequence length provides\nmore space on the token dimension and thus TeraPipe can\nperform even better – TeraPipe enables efﬁcient training of\nfuture-emerging LMs with growing sequence lengths.\n5. Conclusion\nWe present TeraPipe, a high-performance token-level\npipeline parallel algorithm for training large-scale\nTransformer-based language model. We develop a novel\ndynamic programming-based algorithm to calculate the op-\ntimal pipelining execution scheme, given a speciﬁc LM\nand a cluster conﬁguration. TeraPipe is orthogonal to other\nmodel parallel training methods and can be complemented\nby them. Our evaluations show that TeraPipe accelerates\nthe synchronous training of the largest GPT-3 models with\n175 billion parameters by 5.0x on an AWS cluster with 48\np3.16xlarge instances compared to previous methods.\nAcknowledgement\nWe thank our anonymous reviewers for their insightful feed-\nback. We also thank Lianmin Zheng and many others at the\nUC Berkeley RISELab for their helpful discussion and com-\nments. In addition to NSF CISE Expeditions Award CCF-\n1730628, this research is supported by gifts from Alibaba\nGroup, Amazon Web Services, Ant Group, CapitalOne,\nEricsson, Facebook, Futurewei, Google, Intel, Microsoft,\nNvidia, Scotiabank, Splunk, and VMware.\nReferences\nAppleyard, J., Kocisky, T., and Blunsom, P. Optimizing\nperformance of recurrent neural networks on gpus. arXiv\npreprint arXiv:1604.01946, 2016.\nBengio, Y ., Ducharme, R., Vincent, P., and Jauvin, C. A\nneural probabilistic language model. Journal of machine\nlearning research, 3(Feb):1137–1155, 2003.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Training\ndeep nets with sublinear memory cost. arXiv preprint\narXiv:1604.06174, 2016.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018.\nFan, S., Rong, Y ., Meng, C., Cao, Z., Wang, S., Zheng, Z.,\nWu, C., Long, G., Yang, J., Xia, L., et al. Dapple: A\npipelined data parallel approach for training large models.\narXiv preprint arXiv:2007.01045, 2020.\nGoyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P.,\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and\nHe, K. Accurate, large minibatch sgd: Training imagenet\nin 1 hour. arXiv preprint arXiv:1706.02677, 2017.\nHarlap, A., Narayanan, D., Phanishayee, A., Seshadri, V .,\nDevanur, N., Ganger, G., and Gibbons, P. Pipedream:\nFast and efﬁcient pipeline parallel dnn training. arXiv\npreprint arXiv:1806.03377, 2018.\nHuang, Y ., Cheng, Y ., Bapna, A., Firat, O., Chen, D., Chen,\nM., Lee, H., Ngiam, J., Le, Q. V ., Wu, Y ., et al. Gpipe:\nEfﬁcient training of giant neural networks using pipeline\nparallelism. In Advances in neural information process-\ning systems, pp. 103–112, 2019.\nJain, P., Jain, A., Nrusimha, A., Gholami, A., Abbeel, P.,\nKeutzer, K., Stoica, I., and Gonzalez, J. E. Checkmate:\nBreaking the memory wall with optimal tensor remateri-\nalization. arXiv preprint arXiv:1910.02653, 2019.\nJia, Z., Lin, S., Ruizhongtai Qi, C., and Aiken, A. Exploring\nhidden dimensions in parallelizing convolutional neural\nnetworks. 02 2018.\nJia, Z., Zaharia, M., and Aiken, A. Beyond data and model\nparallelism for deep neural networks. SysML 2019, 2019.\nKitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The\nefﬁcient transformer. arXiv preprint arXiv:2001.04451,\n2020.\nTeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models\nKrizhevsky, A. One weird trick for parallelizing convolu-\ntional neural networks. ArXiv, abs/1404.5997, 2014.\nLi, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed,\nA., Josifovski, V ., Long, J., Shekita, E. J., and Su, B.-Y .\nScaling distributed machine learning with the parameter\nserver. In 11th {USENIX}Symposium on Operating\nSystems Design and Implementation ({OSDI}14), pp.\n583–598, 2014.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692, 2019.\nManjikian, N. and Abdelrahman, T. S. Scheduling of wave-\nfront parallelism on scalable shared-memory multipro-\ncessors. In Proceedings of the 1996 ICPP Workshop on\nChallenges for Parallel Processing, volume 3, pp. 122–\n131. IEEE, 1996.\nNCCL. The nvidia collective communication library (nccl).\nhttps://developer.nvidia.com/nccl, 2021.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. Pytorch: An imperative style, high-performance\ndeep learning library. arXiv preprint arXiv:1912.01703,\n2019.\nPetrowski, A., Dreyfus, G., and Girault, C. Performance\nanalysis of a pipelined backpropagation parallel algo-\nrithm. IEEE Transactions on Neural Networks, 4(6):\n970–981, 1993. doi: 10.1109/72.286892.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners.\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y . Zero:\nMemory optimization towards training a trillion parame-\nter models. arXiv preprint arXiv:1910.02054, 2019.\nRen, J., Rajbhandari, S., Aminabadi, R. Y ., Ruwase, O.,\nYang, S., Zhang, M., Li, D., and He, Y . Zero-ofﬂoad: De-\nmocratizing billion-scale model training. arXiv preprint\narXiv:2101.06840, 2021.\nShazeer, N., Cheng, Y ., Parmar, N., Tran, D., Vaswani, A.,\nKoanantakool, P., Hawkins, P., Lee, H., Hong, M., Young,\nC., et al. Mesh-tensorﬂow: Deep learning for supercom-\nputers. In Advances in Neural Information Processing\nSystems, pp. 10414–10423, 2018.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J.,\nand Catanzaro, B. Megatron-lm: Training multi-billion\nparameter language models using gpu model parallelism.\narXiv preprint arXiv:1909.08053, 2019.\nSinharoy, B. and Szymanski, B. Finding optimum wavefront\nof parallel computation. Parallel Algorithms and Appli-\ncations, 2, 08 1994. doi: 10.1080/10637199408915404.\nTay, Y ., Dehghani, M., Abnar, S., Shen, Y ., Bahri, D., Pham,\nP., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long\nrange arena: A benchmark for efﬁcient transformers.\narXiv preprint arXiv:2011.04006, 2020.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nWang, M., Huang, C.-c., and Li, J. Supporting very large\nmodels using automatic dataﬂow graph partitioning. In\nProceedings of the Fourteenth EuroSys Conference 2019,\npp. 1–17, 2019.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov,\nR. R., and Le, Q. V . Xlnet: Generalized autoregressive\npretraining for language understanding. In Advances in\nneural information processing systems, pp. 5753–5763,\n2019.\nZaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti,\nC., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang,\nL., et al. Big bird: Transformers for longer sequences.\narXiv preprint arXiv:2007.14062, 2020.\nZinkevich, M., Weimer, M., Li, L., and Smola, A. Par-\nallelized stochastic gradient descent. In Lafferty, J.,\nWilliams, C., Shawe-Taylor, J., Zemel, R., and Culotta,\nA. (eds.), Advances in Neural Information Processing\nSystems, volume 23, pp. 2595–2603. Curran Associates,\nInc., 2010.\nTeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models\nAppendix\nA. Combine TeraPipe with Gradient Accumulation\nTeraPipe and gradient accumulation (GA) are orthogonal and TeraPipe can further speed up over GA. To see this, we\nvisualize a 3-stage pipeline training with an input batch of 6 training sequences below, similar to Figure 2 in the main paper.\n\u0014 \u0015 \u0016 \u0014 \u0017 \u0015 \u0018 \u0016 \u0019 \u0017 \u0018 \u0019\n\u0014 \u0015 \u0016\u0014 \u0015 \u0017 \u0016 \u0018 \u0017 \u0019 \u0018 \u0019\n\u0014 \u0014 \u0015 \u0015 \u0016 \u0016 \u0017 \u0017 \u0018 \u0018 \u0019 \u0019\n\u0014 \u0015 \u0016\u0014 \u0017\u0015 \u0018\u0016 \u0019\u0017 \u0018 \u0019\n\u0014 \u0015 \u0016\u0014 \u0015 \u0017 \u0016 \u0018\u0017 \u0019 \u0018 \u0019\n\u0014 \u0014 \u0015 \u0015 \u0016 \u0016 \u0017 \u0017 \u0018 \u0018 \u0019 \u0019\n\u0014D\u0014E\n\u0014D\u0014E\n\u0014D\u0014E\n\u0015D\u0015E\n\u0015D\u0015E\n\u0014E\n\u0014E\n\u0014E\n\u0014D\n\u0014D\n\u0014D\n\u0015D \u0015E\u0015D\n\u0016D\n\u0015E\n\u0015E\u0015D\n\u0015E\u0015D\u0016E\n\u0016D\u0016E\n\u0016D\u0016E\n\u0017D\u0017E\n\u0017E\u0017D\n\u0017E\u0017D\n\u0017E\u0017D\n\u0016E\u0016D\n\u0016E\u0016D\n\u0016E\u0016D\n\u0017D\u0017E\n\u0018D\u0018E\n\u0017E\n\u0018D\u0018E\n\u0018D\u0018E\u0017D\n\u0019D\u0019E\n\u0018E\u0018D\n\u0018E\u0018D\n\u0018E\u0018D\n\u0019D\u0019E\n\u0019D\u0019E\u0019E\u0019D\n\u0019E\u0019D\n\u0019E\u0019D\n*38\u0003\u0014\n*38\u0003\u0015\n*38\u0003\u0016\n*38\u0003\u0014\n*38\u0003\u0015\n*38\u0003\u0016\n*38\u0003\u0014\n*38\u0003\u0015\n*38\u0003\u0016\n7LPH\n)RUZDUG\n%DFNZDUG\n\u000bD\f\n\u000bE\f\n\u000bF\f\nIn (a), we show the case where each GPU is capable of storing the intermediate activations of at most 3 input sequences.\nWith scheduling algorithms like DAPPLE (Fan et al., 2020), GA indeed increases the pipeline efﬁciency. However in (b),\nwhen each GPU can only support 2 input sequences (due to large model size), the forward pass of input sequence 3 cannot\nstart on GPU 1 until sequence 1 ﬁnishes the backward pass and release the memory of its intermediate activations. The\nmemory constraint limits the pipeline efﬁciency: only two GPUs can work at a time, and GA cannot solve the issue. In\n(c), we follow the setting in (b) but enable TeraPipe to split a training sequence into two. TeraPipe improves the pipeline\nefﬁciency compared to (b) thanks to more ﬁne-grained pipelining: the three can work at the same time.\nIn our experiments, we have 48 pipeline stages but a single GPU is only capable to hold 2 input sequences due to its memory\ncapacity. Even with newer GPUs (e.g. 80GB A100, 5x memory compared to V100s in the paper), their memory capacity\nis still not enough to fulﬁll the pipeline with 48 input sequences. Therefore, even with GA, TeraPipe is still expected to\nsigniﬁcantly improve the training efﬁciency.\nB. Implementation\nWe implement TeraPipe with PyTorch (Paszke et al., 2019) and NCCL (NCCL). We use Megatron-LM (Shoeybi et al.,\n2019) as the library for operation partitioning and implement microbatch-based pipeline parallelism and data parallelism\nby ourselves. The core of TeraPipe is implemented using 1714 lines of Python. We include the code in the supplementary\nmaterial and the code will be open-sourced.\nC. Experiment Results\nHere, we include the detailed numbers (mean and standard deviation of the latency) and the slicing schemes found by the DP\nalgorithms for all experiments in the main paper. Speciﬁcally, we list the details of Figure 5, 6, and 7 in Table 2, 3, and 4.\nTeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models\nTable 2.Detailed numbers and slicing schemes in main experiments (Figure 5 in the main paper).\nModel Setting Algorithm Slicing Scheme Latency (s) TFlops (per GPU)\nGPT3-1B\n5, (1) w/o TeraPipe [(1, [2048])] * 16 1.517±0.107 0.8841\nw/ TeraPipe [(1, [776, 640 ,632])] * 16 1.254±0.160 1.0695\n5, (2) w/o TeraPipe [(1, [2048])] * 36 1.018±0.065 2.9643\nw/ TeraPipe [(1, [2048])] * 36 1.018±0.065 2.9643\n5, (3) w/o TeraPipe [(1, [2048])] * 72 0.913±0.027 6.6105\nw/ TeraPipe [(1, [2048])] * 72 0.913±0.027 6.6105\nGPT3-13B\n5, (4) w/o TeraPipe [(1, [2048])] * 16 2.637±0.055 3.0305\nw/ TeraPipe [(1, [1024, 1024])] * 16 1.891±0.084 4.2261\n5, (5) w/o TeraPipe [(1, [2048])] * 32 1.863±0.007 8.5792\nw/ TeraPipe [(1, [704, 688, 656])] * 32 1.328±0.037 12.0354\nGPT3-44B\n5, (6) w/o TeraPipe [(1, [2048])] * 2 13.319±0.067 0.2148\nw/ TeraPipe [(1, [64] * 26 + [56] * 6 + [48])] * 2 7.103±0.243 0.4028\n5, (7) w/o TeraPipe [(1, [2048])] * 4 4.311±0.032 1.3274\nw/ TeraPipe [(1, [368, 384, 384, 368, 256, 288])] * 4 2.771±0.112 2.0652\n5, (8) w/o TeraPipe [(1, [2048])] * 8 2.662±0.001 4.2995\nw/ TeraPipe [(1, [384, 384, 368, 320, 296, 296])] * 8 1.111±0.002 10.3018\nGPT3-175B\n5, (9) w/o TeraPipe [(1, [2048])] * 2 9.990±0.005 1.1300\nw/ TeraPipe [(1, [120] * 4 + [112] * 6 + [104] * 8 + [64])] * 21.481±0.002 7.6225\n5, (10) w/o TeraPipe [(1, [2048])] * 2 5.822±0.003 1.9390\nw/ TeraPipe [(1, [128] * 16)] * 2 1.160±0.001 9.7318\nTable 3.Detailed numbers and slicing schemes in ablation studies on the effectiveness of the dynamic programming algorithm (Figure 6\nin the main paper).\nModel Setting Algorithm Slicing Scheme Latency (s) TFlops (per GPU)\nGPT3-44B 6, (a)\n#Slices=1 [(1, [2048])] * 8 2.662±0.001 4.2995\n#Slices=4 [(1, [512] * 4)] * 8 1.241±0.003 9.2226\n#Slices=8 [(1, [256] * 8)] * 8 1.255±0.004 9.1197\n#Slices=16 [(1, [128] * 16)] * 8 1.241±0.003 9.2226\nDP [(1, [384, 384, 368, 320, 296, 296])] * 8 1.111±0.002 10.3018\nGPT3-175B 6, (b)\n#Slices=1 [(1, [2048])] * 2 9.990±0.005 1.1300\n#Slices=4 [(1, [512] * 4)] * 2 2.902±0.003 3.8900\n#Slices=8 [(1, [256] * 8)] * 2 1.892±0.002 5.9667\n#Slices=16 [(1, [128] * 16)] * 2 1.547±0.01 7.2973\n#Slices=32 [(1, [64] * 32)] * 2 1.593±0.002 7.0866\n#Slices=64 [(1, [32] * 64)] * 2 2.227±0.002 5.0691\n#Slices=128 [(1, [16] * 128)] * 2 3.252±0.004 3.4714\nDP [(1, [120] * 4 + [112] * 6 + [104] * 8 + [64])] * 2 1.481±0.002 7.6225\nTable 4.Detailed numbers and slicing schemes in experiments with longer sequence lengths (Figure 7 in the main paper).\nModel Input Sequence Length Algorithm Slicing Scheme Latency (s) TFlops (per GPU)\nGPT3-13B\n2048 w/o TeraPipe [(1, [2048])] * 32 1.863±0.007 8.5792\nw/ TeraPipe [(1, [704, 688, 656])] * 32 1.328±0.037 12.0354\n4096 w/o TeraPipe [(1, [4096])] * 8 2.526±0.001 1.5819\nw/ TeraPipe [(1, [552, 536, 528, 512, 504, 496, 488, 480])] * 80.913±0.085 4.3765\n6144 w/o TeraPipe [(1, [6144])] * 4 3.754±0.006 0.5322\nw/ TeraPipe [(1, [584, 568] + [512] * 6 + [496, 488, 472, 464])] * 40.756±0.008 2.6427\n8192 w/o TeraPipe [(1, [8192])] * 2 4.978±0.004 0.2007\nw/ TeraPipe [(1, [512] * 6 + [480] * 2 + [416] * 10)] * 2 0.636±0.001 1.5707"
}