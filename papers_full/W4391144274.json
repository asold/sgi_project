{
  "title": "Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding",
  "url": "https://openalex.org/W4391144274",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2316436344",
      "name": "Jixin Hou",
      "affiliations": [
        "University of Georgia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3033672077",
    "https://openalex.org/W3169229012",
    "https://openalex.org/W3198722803",
    "https://openalex.org/W4379769532",
    "https://openalex.org/W4384945684",
    "https://openalex.org/W4381051539",
    "https://openalex.org/W4387429503",
    "https://openalex.org/W4388581713",
    "https://openalex.org/W4390414459",
    "https://openalex.org/W6851646166",
    "https://openalex.org/W4285601618",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6848909144",
    "https://openalex.org/W6849898756",
    "https://openalex.org/W4380887490",
    "https://openalex.org/W4383711355",
    "https://openalex.org/W4386530347",
    "https://openalex.org/W4386043294",
    "https://openalex.org/W4362703219",
    "https://openalex.org/W6852656845",
    "https://openalex.org/W4375858605",
    "https://openalex.org/W4328049434",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4380686968",
    "https://openalex.org/W4381512905",
    "https://openalex.org/W4387773384",
    "https://openalex.org/W4383046823",
    "https://openalex.org/W4320854883",
    "https://openalex.org/W4362656092",
    "https://openalex.org/W4388717714",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2473344385",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4390195781",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4289601502",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W4384200891",
    "https://openalex.org/W4383374753",
    "https://openalex.org/W4385671288",
    "https://openalex.org/W4381377104",
    "https://openalex.org/W4296918394",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4377096651",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4384484700",
    "https://openalex.org/W4387521449",
    "https://openalex.org/W4313483544"
  ],
  "abstract": "This study is a pioneering endeavor to investigate the capabilities of Large Language Models (LLMs) in addressing conceptual questions within the domain of mechanical engineering with a focus on mechanics. Our examination involves a manually crafted exam encompassing 126 multiple-choice questions, spanning various aspects of mechanics courses, including Fluid Mechanics, Mechanical Vibration, Engineering Statics and Dynamics, Mechanics of Materials, Theory of Elasticity, and Continuum Mechanics. Three LLMs, including ChatGPT (GPT-3.5), ChatGPT (GPT-4), and Claude (Claude-2.1), were subjected to evaluation against engineering faculties and students with/without mechanical engineering background. The findings reveal GPT-4‚Äôs superior performance over the other two LLMs and human cohorts in answering questions across various mechanics topics, except for Continuum Mechanics. This signals the potential future improvements for GPT models in handling symbolic calculations and tensor analyses. The performances of LLMs were all significantly improved with explanations prompted prior to direct responses, underscoring the crucial role of prompt engineering. Interestingly, GPT-3.5 demonstrates improved performance with prompts covering a broader domain, while GPT-4 excels with prompts focusing on specific subjects. Finally, GPT-4 exhibits notable advancements in mitigating input bias, as evidenced by guessing preferences for humans. This study unveils the substantial potential of LLMs as highly knowledgeable assistants in both mechanical pedagogy and scientific research.",
  "full_text": "   \n \n   \n \nAssessing Large Language Models in Mechanical Engineering \nEducation: A Study on Mechanics-Focused Conceptual Understanding \n \nJie Tian1*, Jixin Hou 1*, Zihao Wu 2*, Peng Shu 2, Ning Liu 3, Zhengliang Liu 2, Yujie \nXiang3, Beikang Gu 1, Nicholas Filla 1, Yiwei Li 2, Xianyan Chen 4, Keke Tang 3#, \nTianming Liu2#, and Xianqiao Wang1# \n \n1School of Environmental, Civil, Agricultural and Mechanical Engineering, College of \nEngineering, University of Georgia, Athens, GA, 30602, USA \n2School of Computing, University of Georgia, Athens, GA, 30602, USA \n3School of Aerospace Engineering and Applied Mechanics, Tongji University, \nShanghai, China \n4Department of Epidemiology and Biostatistics, College of Public Health, University \nof Georgia, Athens, GA 30602, USA \n*Co-first authors \n#Corresponding authors: kktang@tongji.edu.cn, tliu@uga.edu, xqwang@uga.edu \nAbstract \nThis study is a pioneering endeavor to investigate the capabilities of Large Language \nModels (LLMs) in addressing conceptual questions within the domain of mechanical \nengineering with a focus on mechanics . Our examination involves a manually crafted \nexam encompassing 126 multiple -choice questions, spanning various aspects of \nmechanics courses, including Fluid Mechanics, Mechanical Vibration, Engineering \nStatics and Dynamics, Mechanics of Materials, Theory of Elasticity, and Continuum \nMechanics. Three LLMs, including ChatGPT (GPT -3.5), ChatGP T (GPT -4), and \nClaude (Claude -2.1), were subjected to evaluation against engineering faculties and \nstudents with/without mechanical engineering background. The findings reveal GPT -\n4‚Äôs superior performance over the other two LLMs and human cohorts in answering \nquestions across various mechanics topics, except for  Continuum Mechanics. This \n   \n \n   \n \nsignals the potential future improvements for GPT models in handling symbolic \ncalculations and tensor analyses. The performances  of LLMs were all significantly \nimproved with explanations prompted prior to direct responses, underscoring the \ncrucial role of prompt engineering. Interestingly, GPT -3.5 demonstrates improved \nperformance with prompts covering a broader domain, while GPT -4 excels with \nprompts focusing on specific subjects. Finally, GPT -4 exhibits notable advancements \nin mitigating input bias, as ev idenced by guessing preferences for human s. This study \nunveils the substantial potential of LLMs as highly knowledgeable assistants in both \nmechanical pedagogy and scientific research. \n \nKeywords: Large language models, ChatGPT, Mechanics, Engineering education \n1. Introduction \nMechanical engineering stands as the forefront of technological advancement, \nserving as the cornerstone for innovation across diverse industries [1-11]. Yet, the study \nof fundamental mechanics, such as Mechanics of Materials, Theory of Elasticity, \nEngineering Statics and Dynamics, present various challenges for students due to their \ninherent complexities. Mechanics involves abundant abstract concepts that are not \nreadily observable, making it challenging to conceptualize the terminologies like forces, \ntorques, and moments. Additionally, the field heavily relies on mathematical \nexpressions, hindering a holistic understanding of the underlying physical principles \nand significantly dampening enthusiasm for both student learning and educator \nteaching of mechanics. Mean while, traditional pedagogical methods often struggle to \neffectively bridge the gap between theoretical principles and practical applications, \nleaving students grappling with those intricate descriptions of material deformations \nand balance laws. This disp arity underscores the need for innovative teaching \nmethodologies that can enrich the learning experience and enhance comprehension in \nmechanical engineering education. \nIn the rapidly evolving landscape of Natural Language Processing (NLP), the \n   \n \n   \n \nemergence of Large Language Models (LLMs) signifies a transformative paradigm \nshift in artificial intelligence capabilities [12]. NLP, dedicated to facilitating computer-\nhuman language interaction, encompasses various applications ranging from language \ntranslation, content summarization to sentiment analysis  [13]. Conventional NLPs are \nconstrained in offering tailored solutions for specific applications through supervised \ntraining [14]. In this paradigm, models undergo fine -tuning on labeled training data, \nand often require further pre -training on domain -specific datasets to achieve optimal \nperformance. In contrast, LLMs, characterized by their expansive size and exceptional \nfew-shot learning capabilities, transcend these limitations by introducing the in-context \nlearning approach  [15-17]. The overarching impact of this method is a novel and \nstreamlined NLP workflow, eliminating the need for supervised fine-tuning along with \nassociated intricacies such as hyper -parameter tuning and model architecture \nmodifications. LLMs has  demonstrated significant potential in advancing scientific \nresearch across diverse domains, including biomedical research  [18-20], material \nscience [21, 22], chemistry [23-25], and environmental science [26]. For instance, Lee, \net al. [27]  show that BioBERT model outperforms state -of-art counterparts in \nbiomedical text mining tasks, such as terminology recognition, relation extraction, and \nquestion answering. Another study by Jablonka, et al. [28]  highlight the promising \napplications of LLMs in predicting properties of molecules and materials, designing \ninterfaces, and extracting knowledge from unstructured data in chemistry and material \nscience. In the realm of mechanics, while the use of LLMs is rather  surprising, some \nnoteworthy endeavors have been explored. Brodnik, et al. [29]  point out that LLMs \nexhibit emergent capabilities in the area of applied mechanics, such as parallel \ninformation extraction, representations normalization to enrich downstream training \ndataset, programming assistance in computational and experimental mech anics, and \nhypothesis generation. A more in-depth exploration is provided by Buehler‚Äôs group [30, \n31], where a human- machine interactive MechGPT model, trained on a set of 1,103 \nWikipedia articles related to mechanics, show satisfying potential in solving forward \nand inverse problem in various areas, including bio- inspired hierarchical honeycomb \n   \n \n   \n \ndesign, carbon nanotube mechanics, and protein unfolding. \nA prominent example of LLMs is ChatGPT, developed by OpenAI. The first \ncommercial version, GPT-3.5, was released in November 2022. Trained on an extensive \ninternet dataset, this 175- billion-parameters LLMs showcases unprecedented \nproficiency in text genera tion, language translation, creative content creation, and \nfurnishing informative response to a diverse array of queries  [32]. Its successor, GPT-\n4, launched in March 2023, boasts an enhanced architecture and a more extensive \ndataset. It surpasses its predecessor in both size and capability, yielding outputs that are \nmore cohesive, contextually relevant, and nuanced. Furthermore , GPT-4 integrates a \nrobust multimodal capability, which allows it seamlessly incorporate information from \nvarious modalities such as text and images. G PT-4 has demonstrated an exceptional \nperformance on diverse academic benchmarks. For example, it success fully passes \nsimulated exams designed for humans, including the Uniform Bar Examination \n(MBE+MEE+MPT), Graduate Record Examination (GRE), and SAT Math  [33]. \nHowever, it is worth noting that these exams boast substantial annual participation, and \ntheir resourceful testing materials are readily available on the internet. This suggests \nthat the exam questions are probably included in the training data of GPT models [34]. \nTherefore, the superior performance in these examinations may be attributed to the \nextensive exposure to similar questions during training. An intriguing question arises: \nhow does GPT perform in more specialized domain with smaller available training data, \nsuch as mechanical engineering? Addressing this query will provide insights into the \npotential of LLMs in revolutionizing academic and educational fields. While analogous \nefforts have been made in areas like radiation oncology [34] and synthetic biology [35], \nto the best of our knowledge, the capabilities of LLMs in solving mechanical problems \nremain unexplored.  \nIn this manuscript, we embark on a pioneering journey, delving into the potential \nof LLMs to revolutionize the teaching and learning of fundamental mechanics within \nthe mechanical engineering program.  Following the GPT -4 technical report, we \nmanually designed 126 multiple -choice questions pertaining to six fundamental \n   \n \n   \n \nmechanics disciplines: Fluid Mechanics, Mechanical Vibration, Engineering Statics \nand Dynamics, Mechanics of Materials, Theory of Elasticity, and Continuum \nMechanics. Three state-of-art transformer-based LLMs were selected for evaluation: \nGPT-3.5, GPT-4, Claude-2.1. These results were compared with those obtained from \nhuman test -takers, including both experts and non- experts. As we navigate this \nintersection of LLMs and mechanics education, our objective is to harness the unique \ncapabilities of LLMs to cult ivate an enriched and interactive learning environment, \nultimately reshaping the pedagogical landscape of the engineering mechanical program.  \n2. Related Work in LLMs \n2.1. Large language models \nClaude-2.1 [36] is the latest model developed by Anthropic , which delivers \nadvancements in key capabilities for enterprises including an industry -leading 200K \ntoken context window, significant reductions in rates of model hallucination. Claude-\n2.1 distinguishes itself from other large language models by prioritizing ethical AI \ndesign and user safety. At the heart of Claude- 2.1‚Äôs design is a commitment to ethical \nAI principles. The model is engineered to minimize biases and ensure safe interactions. \nThis makes Claude- 2.1 particularly suited for applications where trust and reliability \nare crucial. Tool use is a new beta feature added in Claude -2.1 to integrate with users‚Äô \nexisting processes, products, and APIs. This expanded interoperability aims to make \nClaude more useful across users‚Äô day -to-day operations. Claude -2.1 has also made \nsignificant gains in honesty, with a 2X  decrease in false statements compared to the \nprevious Claude-2.0 model. It promises to deploy AI across users‚Äô operations with \ngreater trust and reliability. \nDeveloped as an iteration of the revolutionary GPT-3 by OpenAI, GPT-3.5-Turbo-\n1106 model stands out for its specialized enhancements in processing speed and \nefficiency. It is the latest GPT -3.5 Turbo model with improved instruction following, \nJSON mode, reproducible outputs, parallel function calling, and more, which accepts \n   \n \n   \n \nup to 16,385 tokens for context window and returns a maximum of 4,096 output tokens. \nThe ‚ÄúTurbo‚Äù in its name underscores its design philosophy ‚Äì to provide rapid, high-\nperformance language processing capabilities without compromising the depth and \nversatility for which the GPT series is renowned. This optimization is pivotal for \napplications requiring real- time interaction, such as conversational AI, where latency \ncan significantly impact user experience.  Despite its focus on speed, GPT -3.5-Turbo-\n1106 retains the comprehensive language understanding and generation capabilities \ncharacteristic of the GPT-3 model. \nGPT-4 [33] is the next generation model in the groundbreaking Generative Pre -\ntrained Transformer series by OpenAI, representing a monumental leap in the field of \nartificial intelligence and natural language processing. As a successor to the widely \nacclaimed GPT-3, GPT-4 continues to push the boundaries of what AI can achieve in \nunderstanding and generating human language. GPT-4 is a large multimodal model \n(accepting image and text inputs, emitting text outputs) that, while less capable than \nhumans in many real -world scenarios, exhibits human- level performance on various \nprofessional and academic benchmarks. For example, it passes a simulated bar exam \nwith a score around the top 10% of test takers; in contrast, GPT-3.5‚Äôs score was around \nthe bottom 10%. It is believed that GPT -4 consists of about 1.76 trillion parameters  \nallowing this model even more versatile in various applications, from content creation, \neducational tools, and creative writing to more complex tasks like programming \nassistance and data analysis.  With its advanced capabilities, GPT -4 also incorporates \nimproved safety features and ethical considerations. OpenAI has aimed to mitigate risks \nassociated with AI-generated content, such as misinformation and biases, making GPT-\n4 a more responsible and trustwor thy AI model. GPT-4 marks a significant milestone \nin AI development. It not only demonstrates the rapid advancements in machine \nlearning and AI capabilities but also highlights the increasing importance of ethical \nconsiderations in AI development.  \n   \n \n   \n \n2.2. Language model examination \nLarge language models exhibit exceptional natural language comprehension \ncapabilities and are trained on vast datasets, endowing them with extensive knowledge. \nThese attributes render large language models as ideal candidates for academic and \nprofessional benchmarking. \nOpenAI recently published the inaugural study in the literature evaluating large \nlanguage models on academic and professional exams intended for educated humans. \nThis study reveals that GPT -4 excels across a broad spectrum of subjects, from the \nUniform Bar Exam to the GRE. Additionally, a Microsoft study demonstrates GPT-4‚Äôs \nability to pass the USMLE, a professional exam for medical residents, with a significant \nmargin. \nResearchers have applied plenty of benchmarks to test the performance of their \nlarge models. For example, General Language Understanding Evaluation (GLUE) [37] \nis a collection of nine different tasks designed to test a model‚Äô s ability to understand \nEnglish language. These tasks include sentiment analysis, textual entailment, and \nquestion answering. It assesses the model ‚Äôs understanding of grammar, logic, and the \nrelationships between sentences. SuperGLUE  [38] is an extension of the GLUE \nbenchmark but with more challenging tasks including reading comprehension, word \nsense disambiguation, and more complex question- answering challenges.  Stanford \nQuestion Answering Dataset (SQuAD) [38] is a dataset consisting of questions posed \nby crowdworkers on a set of Wikipedia articles. The answers to these questions are \nsegments of text from the corresponding reading passage. It tests a model's reading \ncomprehension ability and how well it can extract answers from a given text. The \nLAMBADA [39] dataset evaluates the capabilities of language models in text \nunderstanding and prediction, particularly focusing on predicting the final word in a \npassage of text.  These examinations provide a comprehensive overview of a model's \nlinguistic abilities, ranging from basic understanding to advanced reasoning and \ncomprehension. \n   \n \n   \n \n2.3. Prompt engineering \nThe collection and labeling of data for training or fine -tuning NLP models can be \nresource-intensive and expensive, particularly in specialized fields like medicine. \nRecent research indicates that large-scale pre-trained language models (PLMs) can be \nadapted to downstream tasks using prompts, thereby potentially circumventing the need \nfor fine-tuning. \nA prompt is a set of instructions that tailor or refine the LLM's response, extending \nbeyond simple task description or output format specification. Indeed, they can be \nengineered to facilitate novel interactions. For instance, ChatGPT can be prompted to \nsimulate a cybersecurity breach using fictitious terminal commands. Furthermore, \nprompts can be used to generate additional prompts through a self-adaptation process. \nThe emergence of prompt engineering marks the beginning of a new era in natural \nlanguage processing. The potential for diverse and complex applications of well-crafted \nprompts is undeniable. However, the challenge lies in determining the ideal prompt in \nthe era of large language models. Currently, prompts can be manually created or \nautomatically generated. While automatically generated prompts may excel in certain \ntasks, they often lack human readability and explainabilit y. Thus, in domains where \ninterpretability is paramount, such as in education and research, manual prompt \ngeneration may be preferable. In this study, we develop a series of prompts and chain-\nof-thought prompts, informed by our expertise and cooperated with GPTs develop kit \nprovided ChatGPT website. \n3. Methodology and Strategy \nFor this investigation, an evaluation consisting of 126 multiple -choice questions \npertinent to mechanical engineering program with a focus on mechanics was formulated \nby a ground of seasoned expert s in the field. This assessment covers six distinct \ndomains: Fluid Mechanics, with  16 questions; Mechanics of Vibration, 20 questions; \nEngineering Statics and Dynamics, 24 questions; Mechanics of Materials, 25 questions; \nTheory of Elasticity, 19 questions; and Continuum Mechanics, 22 questions. The \n   \n \n   \n \nspecific questions featured in this examination are comprehensively detailed in \nSupplemental Material. \n3.1. Question design \nAs illustrated in Supplemental Material, the majority of the questions in the \nassessment predominantly focus on definitions and factual knowledge, alongside a \nconsiderable number of calculation-based questions. To more thoroughly comprehend \nthe design of the examination (depicted in Figure 1), an analysis of the distribution of \ncorrect answers was conducted. Figure 1a showed that options B and C were more \nfrequently selected as the correct answers, aligning with a common human intuition \nthat places these op tions ‚Äúin the middle.‚Äù Figure 1b further delineates the distribution \nof correct answers across each domain, indicating that for each domain, three options \nfollow a near-normal distribution pattern, while one option is consistently less favored. \n3.2. Strategy for comparison between LLM scores and human scores \nHuman Test Data Collection \nTo formulate a thorough benchmark, examination results were collated from a \ndiverse array of individuals. After an extensive review process, which involved the \nexclusion of evidently problematic results (such as those with incorrect lengths or \noptions not listed in the question), data were collected from four distinct human test \ngroups. This data set encompasses results from 17 undergraduate students majoring in \nnon-mechanical engineering disciplines, 14 undergraduate students majoring in \nmechanical engineering, 9 graduate students in mechanics (inclusive of both M aster‚Äôs \nand Ph.D. students), and 6 engineering faculty members. For a detailed visualization of \nthis data, please see Figure 2. The examination format for each human participant was \nconsistent, allowing no time restrictions, enforcing a closed -book policy, and \npermitting the use of a basic calculator. \nLarge Language Models Evaluated and Hyperparameter Setting \nThis study is dedicated to the evaluation of two extensively utilized large language \n   \n \n   \n \nmodels (LLMs), specifically OpenAI and Claude, with no preceding inputs. For \nassessing OpenAI, the GPT-4 chat box serves as a benchmark, and their latest models, \nGPT-3.5-turbo-1106 and GPT-4-1106-preview, are tested with a temperature setting of \n1 and a maximum token value of 4,096. Claude's most recent iteration, Claude -2.1, is \nevaluated under a hyperparameter configuration of temperature ùëáùëá =  1, ùë°ùë°ùë°ùë°ùëùùëùùëòùëò  =  40, \nand ùë°ùë°ùë°ùë°ùëùùëùùëùùëù =  0.9. \nTo ensure the dependability of responses from LLMs, each question is subjected \nto multiple trials with requests for varied responses. As detailed in Table 1, specific \nprompts are utilized for different trials. These prompts are categorized according to \ntheir function: System prompts, which establish the role of the LLM, and Question \nprompts, which determine whether the LLM should first clarify the question before \nanswering. We have developed three types of system prompts: 1. Simple prompts, \ngeneral in nature, guiding LLMs to respond to scientific inqui ries; 2. Mechanics -\nspecific prompts, which direct the LLMs to tackle Mechanics questions and clearly \nrefer to all six domains; 3. Domain -specific prompts, explicitly highlighting the \npertinent domain of the question.  \nTo create a benchmark for LLM performance, a single trial was conducted using \nChatbox (GPT-4), incorporating domain- specific prompts and necessitating \nexplanations. Both GPT-4 and GPT-3.5 were subjected to six trials, each encompassing \nthree rounds. In these rounds, the question options were shuffled, and the OpenAI \nmodels were directed to  produce five responses in each instance. For Claude -2.1, a \nsimilar trial was conducted, mirroring the approach used with Chatbox (GPT -4) and \nutilizing the identical prompt. This trial included three rounds, with each round yielding \nthree responses. \nScore Calculation and Evaluation \nThe test scores of large language models (LLMs) and their distribution patterns \nwere compared both among themselves and with those of human cohorts. In this \ncomparative analysis, the mean scores, consistency of scores, and confidence in \nresponses were key evaluation metrics. To quantify the overall consistency in scoring \n   \n \n   \n \nsuccess, we computed the standard deviation and the average correlation between trials. \nThis average correlation, defined as the mean of the upper values in the Pearson \ncorrelation matrix across trials, assesses the uniformity of correct scores between trials. \nA value of 1 signifies identical distributions, 0 indicates a completely random \ndistribution, and - 1 represents a perfect anti -correlation. Additionally, to measure the \nconfidence level in the responses provided by the LLMs and human groups, we tallied \nthe number of correct answers for each question across all trials. \n3.3. Strategy for ChatGPT accuracy improvement \nRecent studies have found that transformer -based language models can greatly \nenhance response accuracy by generating answers in a stepwise manner. This is made \npossible by the LLM‚Äôs ability to predict the following word based on prior context. To \nachieve this, it is necessary to prompt a sufficiently large LLM to generate responses \nstep by step. In our research, we have improved the prompt by explicitly indicating the \ndomain of ques tions or requesting the LLM to explain its reasoning process before \nproviding an answer. Table 1 showcases various types of prompts that were utilized. \nFor the OpenAI model, the domain- specific aspect is included in the system prompt, \nwhile the explanation prompt is in the user prompt. It is important to note that, for \ndomain-specific prompts, the derived step pertains to all domains before being further \nrefined into a specific domain. \nOwing to the inherent design of transformer -based LLMs, which predict \nsubsequent words based on preceding context, it has been demonstrated that the \naccuracy of responses can be enhanced if a sufficiently expansive LLM is prompted to \nconstruct answers in a step-wise fashion [40]. This approach was applied to ChatGPT \n(GPT-4) to investigate whether its performance could be augmented by prompting it to \nfirst elucidate and then respond. To refine the outcomes, we compared the results across \ntrials where the question prompt either directed the LLM to provide an answer directly \nor to first explain, then answer, as depicted in each row of Table 1. \n   \n \n   \n \n3.4. Reasoning capability of ChatGPT \nIn transformer- based models, the preceding output serves as the input for \ngenerating the subsequent token, forming the foundation of prompt engineering and \nchain-of-thought reasoning [40]. Specifically, ChatGPT and GPT-4 have demonstrated \nsignificant advancements in reasoning across various scenarios, such as solving \nmathematical and meme understanding. These LLMs excel in inductive, abductive, and \nanalogical reasoning, which rely on pattern recognition and retrieval from pretrained \nknowledge, followed by correlating and adapting to a given set of observations to \ndeduce conclusions. While these conclusions are insightful, they are not guaranteed to \nbe accurate. Such reasoning is effective f or general domain questions, where tasks \ninvolve directly extracting or summarizing answers from the provided context by \nutilizing their extensive pre-training knowledge. In contrast, applications in mechanical \nengineering may require LLMs to engage in ded uctive reasoning. This reasoning type \ninvolves drawing conclusions based on the truth of the premises, thus enabling the \ndeduction of solid conclusions through a rigorous logical chain. To further assess \nChatGPT‚Äôs reasoning abilities, particularly in terms  of human- like thinking and \nreasoning, we analyzed the similarity in accuracy distribution between human cohorts \nand LLMs. \n4. Results \n4.1. Comparison between LLM scores and human scores \nFigure 2 delineates the raw scores for both human participants and LLMs. The \nLLM results are domain -specific and emphasize the importance of providing \nexplanations first. Figure 3 highlights the mean test scores, revealing that contemporary \nmodels such as GPT-4 and Claude-2.1 demonstrate superior performance compared to \nengineering faculty. It is noteworthy that engineering faculty, rather than human experts, \nwere chosen for comparison, as they often have expertise in one or several specific \nareas. Consider ing the closed- book nature of the exam, their scores might not fully \n   \n \n   \n \nrepresent the expertise typically expected. Generally, AI models appear to surpass non-\nexpert human performance. The raw scores of undergraduate students majoring in non-\nmechanical engineering fields, with a mean accuracy near 25%, suggest a performance \nlevel akin to random guessing, corroborated by the distribution patterns shown in Figure \n4. \nIn Figures 4b and 4c, the accuracy of engineering faculty and various LLMs across \ndifferent domains is presented. These results indicate that answers accompanied by \nexplanations generally lead to higher accuracy. Notably, GPT- 4 shows a significant \nimprovement in performance over GPT -3.5. Continuum Mechanics emerges as the \ndomain where human engineering faculty maintain a notable edge over LLMs . This \nphenomenon can be attributed to the massive symbolic representations and mathematic \ncalculations in the question design, especially those related to the tensor analysis. \nThe comparative analysis depicted in Figure 4b and Figure 5 suggests that the \naccuracy distribution models of engineering faculty and various large language models, \nsuch as Claude, GPT-3.5 (answer only), and GPT-4 (answer only), are similar. However, \nwhen the models are prompted to provide both an answer and an e xplanation, the \naccuracy distribution aligns more closely with human results. This implies that \nincreased reasoning enhances the credibility and confidence in AI-generated responses. \nAs shown in Figure 5c, we measure the correlation between trials from both \nEngineering Faculty and LLMs. Since human results are collected from different \npersons, The correlation is significantly lower, and LLMs all have greater confidence. \nEspecially GPT-4 with explanation provided first. \n4.2. Comparison on the prompt engineering  \nBased on the data presented in Figure 4d, restricting  the domain before posing \nquestions can significantly enhance the accuracy of the AI model. Furthermore, we can \nexamine the differences between various domains and evaluate the significance of \nproviding explanations, as shown in Figure 6. The technique of prompt engineering \nreveals improved answer performance after an explanation. For GPT -3.5, we can \n   \n \n   \n \nconclude that the prompt covering all domains has a higher likelihood of yielding \noptimal results. Regarding GPT -4, a more specific domain typically leads to  a higher \naccuracy, but the improvement is not as substantial as with GPT-3.5. \n4.3. Guessing preference ‚Äì option distribution of incorrect answers \nAccording to Figure 1, a tendency exists among humans to favor certain answers \nwhile setting up exam questions. Similarly, when confronted with unfamiliar questions, \nhuman respondents often lean towards specific choices, as evidenced  in Figure 7a, \nwhere incorrect responses predominantly cluster around options B and C, even though \nthe correct answer lies nearby. To delve deeper, we analyzed the option preference of \nLLMs, which exhibited no input bias due to the randomization of question options. \nFigure 7b elucidates that in the coding scheme, ‚ÄúE‚Äù signifies that GPT models identify \nnone of the options as correct, ‚ÄúF‚Äù indicates that all options are correct, and ‚ÄúG‚Äù denotes \na failure in generation. The distribution patterns suggest that GPT -3.5 shows a \npredilection for options A and C, whereas GPT -4 does not display a discernible \npreference for any specific option. \n4.4. Reasoning capability of ChatGPT \nWhen comparing the accuracy distribution of responses between engineering \nfaculty and the GPT model, a clear pattern emerges: incorporating explanations \nsignificantly increases the randomness of incorrect answers and bolsters the confidence \nin correct ones. As illustrated in Figure 7b, responses generated without accompanying \nexplanations tend to include some non- existent answer s. While this might impair \noverall performance, prioritizing the provision of explanations before answers is \nessential for augmenti ng the model ‚Äôs capacity for deductive reasoning through the \nquestions. We further assessed the reasoning capabilities of LLMs  by examining the \ncongruence in accuracy distribution between human cohorts and LLMs, as depicted in \nFigures 5c and 5d. For the correlation computation, we initially subtracted the mean \nvalue to mitigate bias stemming from the correct answer target. Incorporating \n   \n \n   \n \nexplanations into LLM responses seemingly augments their resemblance to human-like \nattributes. \n5. Discussions \nThe objective of this research is to evaluate the capabilities of large language \nmodels (LLMs) within the specific context of mechanical engineering with a focus on \nmechanics. A comprehensive 126-question examination, devised by a group of experts \nin the field (refer to the Supplemental Material), was employed to gauge the proficiency \nof LLMs. This assessment primarily targeted OpenAI models, namely GPT -3.5 and \nGPT-4, and included a comparative analysis with Claude-2.1. The findings reveal that \nall tested L LMs outperformed non- expert human participants. Notably, GPT -4 \nachieved higher scores than engineering faculty members. However, in more advanced \nand knowledge -intensive areas, such as Continuum Mechanics, human experts \nmaintained a superior edge over LLMs. \nThe study‚Äôs focus on prompt engineering significantly augmented the evaluation \nof LLM performance. Three types of prompt systems were identified, each aiding in \nthe precise categorization of question domains. While GPT -4 exhibited some \nenhancements, its proficiency appears somewhat constrained. In contrast, GPT -3.5 \ndemonstrated a more pronounced efficacy, particularly within the mechanics domain. \nHowever, this effectiveness varied when the focus was narrowed to specific areas like \nFluid Mechanics. The OpenA I models, particularly when prompted to provide \nexplanations before responses, displayed a high level of reasonability. The shift from \nmoderate to either low or high accuracy in the results, compared to faculty and Claude‚Äôs \naccuracy distribution, suggests an increase in confidence and precision. ChatGPT \n(GPT-4) showcased exceptional performance, with prospects for future advancements. \nThe decision to exclude a custom -tailored GPT assistant from testing was based on \nconcerns over potential overfitting. Overa ll, the study underscores the significant \npotential for collaboration between mechanics experts and ChatGPT (GPT -4) as a \nhighly proficient assistant. \n   \n \n   \n \nA limitation in assessing LLMs through exams, as demonstrated in this study, is \nthat such exams predominantly reflect textbook knowledge, restricting their \napplicability mainly to educational contexts. Consequently, the relative performance of \nLLMs compared to engineering faculty in m echanics exams might not accurately \nrepresent the parity between LLMs and individual expertise in m echanics. Moreover, \nthe high performance of GPT -4 in this certificate -like exam, focusing on a highly \nspecialized subject, raises concerns about the depth of knowledge evaluated. This leads \nto a conundrum: if taken at face value, it could imply that GPT -4 possesses sufficient \ncompetence to function as a medical physicist, which appears highly unlikely. Thus, \nthe mechanics community might need to reconsider their certificate procedures, as the \nneed for humans to extensively learn such potentially superficial knowledge might \ndecrease with the ongoing evolution of LLMs. In this context, LLMs could serve as a \nbenchmark for superficiality in knowledge assessment. Emphasizing know ledge areas \nthat LLMs have not mastered might become increasingly important. \n5.1. Applying large language models in mechanical engineering  \nThe incorporation of large language models emerges as a groundbreaking asset in \nthe domains of mechanical engineering education and scientific research. For example, \nLLM-based chatbots, such as ChatGPT, can serve as dynamic and interactive tutors, \nproviding real -time assistance to students in learning complex mechanical concepts \nwith clarity and engagement. M oreover, LLMs like PaLM exhibits promising \nproficiency in tasks such as information extraction, synthesis, and distillation across \ndiverse fields. They excel in activities ranging from code generations and translations \nbetween programming languages to guiding experiments and explaining methodologies \n[29]. However, understanding the scope and limitations of the language model is \nessential both for learners and educators in mechanical engineering [41, 42]. Primarily, \nthe reliability and accuracy of these models should be meticulously assessed, i.e., \nverifying their information sources and relevance to mechanics domain. On the other \nhand, a thorough evaluation of the ethical implications is paramount, especially \n   \n \n   \n \nconcerning academic integrity and responsible use of technology. Last, we should be \nmindful of the potential bias in the training corpus and seek diverse perspectives to \nensure a comprehensive and well-rounded understanding of mechanics.  \nThe finetuning of LLMs offers a powerful strategy for bolstering the capability of \nscientific research. In the research context, finetuning enables the customization of pre-\ntrained language models to align with the specific nuances and terminologies inhere nt \nin the mechanical engineering field. For example, Luu and Buehler [43] developed the \nBioinspireLLM through finetuning with a corpus comprising over a thousand peer -\nreviewed articles in structural biology and bio- inspired materials. Their model \nexhibited remarkable capabilities in formulating sound hypotheses regarding biological \nmaterials design and predicting novel materials, thereby reshaping the traditional \nmaterials design process. A similar endeavor is the MechGPT [30] , which was \nfinetuned on a set of 1,103 Wikipedia articles related to mechanics. MechGPT \ndemonstrates exceptional performance in solving both forward and inverse problems.  \n5.2. Multi-modal models in mechanical engineering \nMulti-modal models, representing the forefront of language model development, \nhold significant potential in the field of mechanics. By integrating capabilities such as \nimage recognition, these models could play a pivotal role in addressing complex \nmechanics problems. For instance, in the realm of structural engineering, multi- modal \nmodels can analyze visual data from structural inspections alongside textual reports and \nsensor readings to assess the integrity and safety of bridges or buildings. This \ncomprehensive analysis enables more accurate predictions of potential structural \nfailures or maintenance needs, leading to safer and more efficient management of \ninfrastructure. Furthermore, in aerodynamics, multi -modal models could analyze \nairflow patterns over a vehicle‚Äôs surface captured through visual methods, correlate this \ndata with computational fluid dynamics simulations, and adjust design parameters \naccordingly. Additionally, certain reinforcement learning systems, such as AutoGPT, \nshow promise in aiding structural design. In essence, multi-modal models in mechanics \n   \n \n   \n \nare paving the way for more integrated, intelligent, and predictive approaches to \nmechanical challenges. As these models continue to evolve, they are poised to become \nindispensable tools in the mechanical and structural engineering fields. \n6. Conclusion \nOur examination of large language models (LLMs) and human participants in \nmechanical engineering assessments reveals compelling insights. LLMs, notably GPT-\n4, demonstrate superior performance over human cohorts in closed-book exams related \nto engineering mechanics, and exhibit remarkable capabilities in deductive reasoning. \nThe incorporation of explanations in prompts design consistently enhances both \nanswering accuracy and deductive reasoning for LLMs. Prompt engineering proves \ncritical for enhancing model accuracy, with GPT-3.5 excelling in a broader domain and \nGPT-4 preferring more specific domains. Additionally, the exploration of guessing \npreferences highlights human biases, while LLMs like GPT -4 demonstrate \nadvancements in mitigating input bias. Overall, these findings suggest a promising \nfuture for LLMs in mechanical education and scientific research settings, paving the \nway for their increased integration and impact in these domains. \nData Availability Statement \nThe original contributions presented in the study are included in the \narticle/supplemental material. Further inquiries can be directed to the corresponding \nauthors. \nConflict of Interest \nThe authors declare that the research was conducted in the absence of any commercial \nor financial relationships that could be construed as a potential conflict of interest. \n   \n \n   \n \nAcknowledgements \nWe would like to thank the many individuals ( currently undergraduate student s, \ngraduate students, and engineering faculty in both Tongji University , China and \nUniversity of Georgia , USA) that volunteered to take the mechanical engineering \nassessment.  \n   \n \n   \n \nReferences \n[1] K. Tang, Z. Du, B. Wu, and J. Hou, \"Fatigue behavior prediction of metal alloys based on a unified \nmultiscale crack growth model,\" Engineering Fracture Mechanics, vol. 235, p. 107132, \n2020/08/01/ 2020, doi: https://doi.org/10.1016/j.engfracmech.2020.107132. \n[2] P. Chavoshnejad et al., \"Role of axonal fibers in the cortical folding patterns: A tale of variability \nand regularity,\" Brain Multiphysics, vol. 2, p. 100029, 2021/01/01/ 2021, doi: \nhttps://doi.org/10.1016/j.brain.2021.100029. \n[3] J. Hou, X. Li, Z. Li, L. Yin, X. Chen, and F. Liang, \"An In Vivo Data-Based Computational Study \non Sitting-Induced Hemodynamic Changes in the External Iliac Artery,\" Journal of Biomechanical \nEngineering, vol. 144, no. 2, 2021, doi: 10.1115/1.4052292. \n[4] B. Gu, W. Wang, S. Shen, Z. Chen, and H. Ma, \"Investigation of formation and breakage mechanism \nof microweld of typical wire -bonding materials via molecular dynamics simulation,\" MRS \nCommunications, vol. 12, no. 5, pp. 864- 872, 2022/10/01 2022, doi: 10.1557/s43579- 022-00259-\n5. \n[5] J. Tian, K. Tang, X. Chen, and X. Wang, \"Machine learning-based prediction and inverse design of \n2D metamaterial structures with tunable deformation -dependent Poisson's ratio,\" Nanoscale, vol. \n14, no. 35, pp. 12677-12691, 2022. \n[6] P. Chavoshnejad et al., \"An integrated finite element method and machine learning algorithm for \nbrain morphology prediction,\" Cerebral Cortex, vol. 33, no. 15, pp. 9354 -9366, 2023, doi: \n10.1093/cercor/bhad208. \n[7] N. Filla, J. Hou, H. Li, and X. Wang, \"A multiscale framework for modeling fibrin fiber networks: \nTheory development and validation,\" Journal of the Mechanics and Physics of Solids, vol. 179, p. \n105392, 2023/10/01/ 2023, doi: https://doi.org/10.1016/j.jmps.2023.105392\n. \n[8] N. Liu, S. Li, and X. Wang, \"Mechanism of coupling polymer thickness and interfacial interactions \non strength and toughness of non-covalent nacre-inspired graphene nanocomposites,\" Composites \nScience and Technology, vol. 241, p. 110124, 2023/08/18/ 2023, doi: \nhttps://doi.org/10.1016/j.compscitech.2023.110124\n. \n[9] Y . Liu, J. Hou, C. Li, and X. Wang, \"Intelligent Soft Robotic Grippers for Agricultural and Food \nProduct Handling: A Brief Review with a Focus on Design and Control,\" Advanced Intelligent \nSystems, vol. 5, no. 12, p. 2300233, 2023, doi: https://doi.org/10.1002/aisy.202300233\n. \n[10] J. Hou, B. Park, C. Li, and X. Wang, \"A multiscale computation study on bruise susceptibility of \nblueberries from mechanical impact,\" Postharvest Biology and Technology, vol. 208, p. 112660, \n2024/02/01/ 2024, doi: https://doi.org/10.1016/j.postharvbio.2023.112660. \n[11] K. Tang et al., \"Machine learning -based morphological and mechanical prediction of kirigami -\ninspired active composites,\" International Journal of Mechanical Sciences, vol. 266, p. 108956, \n2024/03/15/ 2024, doi: https://doi.org/10.1016/j.ijmecsci.2023.108956. \n[12] L. Zhao et al., \"When brain-inspired ai meets agi,\" Meta-Radiology, p. 100005, 2023. \n[13] R. Varaprasad and G. Mahalaxmi, \"Applications and Techniques of Natural Language Processing: \nAn Overview,\" IUP Journal of Computer Sciences, vol. 16, no. 3, pp. 7-21, 2022. \n[14] S. Rezayi et al., \"Agribert: knowledge-infused agricultural language models for matching food and \nnutrition,\" in Proceedings of the Thirty -First International Joint Conference on Artificial \nIntelligence, 2022, vol. 7, pp. 5150-5156.  \n[15] T. B. Brown et al., \"Language models are few -shot learners,\" presented at the Proceedings of the \n   \n \n   \n \n34th International Conference on Neural Information Processing Systems, Vancouver, BC, Canada, \n2020. \n[16] Q. Dong et al., \"A survey for in-context learning,\" arXiv preprint arXiv:2301.00234, 2022. \n[17] G. Mialon et al., \"Augmented language models: a survey,\" arXiv preprint arXiv:2302.07842, 2023. \n[18] S. Thapa and S. Adhikari, \"ChatGPT, bard, and large language models for biomedical research: \nopportunities and pitfalls,\" Annals of Biomedical Engineering, pp. 1-5, 2023. \n[19] S. Pal, M. Bhattacharya, S. -S. Lee, and C. Chakraborty, \"A Domain -Specific Next -Generation \nLarge Language Model (LLM) or ChatGPT is Required for Biomedical Engineering and Research,\" \nAnnals of Biomedical Engineering, pp. 1-4, 2023. \n[20] Q. Chen et al., \"An extensive benchmark study on biomedical text generation and mining with \nChatGPT,\" Bioinformatics, vol. 39, no. 9, p. btad557, 2023. \n[21] M. Zaki and N. Krishnan, \"MaScQA: A Question Answering Dataset for Investigating Materials \nScience Knowledge of Large Language Models,\" arXiv preprint arXiv:2308.09115, 2023. \n[22] T. Xie et al., \"Large Language Models as Master Key: Unlocking the Secrets of Materials Science \nwith GPT,\" arXiv preprint arXiv:2304.02213, 2023. \n[23] A. D. White, \"The future of chemistry is language,\" Nature Reviews Chemistry, pp. 1-2, 2023. \n[24] D. Xuan-Quy, L. Ngoc-Bich, N. Bac -Bien, and P. Xuan -Dung, \"LLMs' Capabilities at the High \nSchool Level in Chemistry: Cases of ChatGPT and Microsoft Bing Chat,\" 2023. \n[25] M.-L. Tsai, C. W. Ong, and C. -L. Chen, \"Exploring the use of large language models (LLMs) in \nchemical engineering education: Building core course problem models with Chat-GPT,\" Education \nfor Chemical Engineers, vol. 44, pp. 71-95, 2023. \n[26] J.-J. Zhu, J. Jiang, M. Yang, and Z. J. Ren, \"ChatGPT and environmental research,\" Environmental \nScience & Technology, 2023. \n[27] J. Lee et al., \"BioBERT: a pre-trained biomedical language representation model for biomedical \ntext mining,\" Bioinformatics, vol. 36, no. 4, pp. 1234 -1240, 2019, doi: \n10.1093/bioinformatics/btz682. \n[28] K. M. Jablonka et al., \"14 examples of how LLMs can transform materials science and chemistry: \na reflection on a large language model hackathon,\" Digital Discovery, vol. 2, no. 5, pp. 1233-1250, \n2023. \n[29] N. R. Brodnik  et al., \"Perspective: Large Language Models in Applied Mechanics,\" Journal of \nApplied Mechanics, vol. 90, no. 10, 2023, doi: 10.1115/1.4062773. \n[30] M. J. Buehler, \"MechGPT, a Language-Based Strategy for Mechanics and Materials Modeling That \nConnects Knowledge Across Scales, Disciplines and Modalities,\" Applied Mechanics Reviews, pp. \n1-82, 2023, doi: 10.1115/1.4063843. \n[31] M. J. Buehler, \"MeLM, a generative pretrained language modeling framework that solves forward \nand inverse mechanics problems,\" Journal of the Mechanics and Physics of Solids, vol. 181, p. \n105454, 2023. \n[32] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang, \"Is ChatGPT a general -purpose \nnatural language processing task solver?,\" arXiv preprint arXiv:2302.06476, 2023. \n[33] OpenAI, \"GPT-4 Technical Report,\" ArXiv, vol. abs/2303.08774, 2023. \n[34] J. Holmes  et al. , \"Evaluating large language models on a highly -specialized topic, radiation \noncology physics,\" arXiv preprint arXiv:2304.01938, 2023. \n[35] X. Gong et al., Evaluating the Potential of Leading Large Language Models in Reasoning Biology \nQuestions. 2023. \n   \n \n   \n \n[36] Anthropic, \"Model Card and Evaluations for Claude Models,\" 2023. [Online]. Available: \nhttps://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf \n[37] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, \"GLUE: A multi -task \nbenchmark and analysis platform for natural language understanding,\" arXiv preprint \narXiv:1804.07461, 2018. \n[38] A. Wang et al. , \"Superglue: A stickier benchmark for general -purpose language understanding \nsystems,\" Advances in neural information processing systems, vol. 32, 2019. \n[39] D. Paperno et al., \"The LAMBADA dataset: Word prediction requiring a broad discourse context,\" \narXiv preprint arXiv:1606.06031, 2016. \n[40] J. Wei et al., \"Chain-of-thought prompting elicits reasoning in large language models,\" Advances \nin Neural Information Processing Systems, vol. 35, pp. 24824-24837, 2022. \n[41] E. Kasneci et al., \"ChatGPT for good? On opportunities and challenges of large language models \nfor education,\" Learning and Individual Differences, vol. 103, p. 102274, 2023/04/01/ 2023, doi: \nhttps://doi.org/10.1016/j.lindif.2023.102274. \n[42] J. G. Meyer et al. , \"ChatGPT and large language models in academia: opportunities and \nchallenges,\" BioData Mining, vol. 16, no. 1, p. 20, 2023/07/13 2023, doi: 10.1186/s13040 -023-\n00339-9. \n[43] R. K. Luu and M. J. Buehler, \"BioinspiredLLM: Conversational Large Language Model for the \nMechanics of Biological and Bio -Inspired Materials,\" Advanced Science, vol. n/a, no. n/a, p. \n2306724, doi: https://doi.org/10.1002/advs.202306724\n. \n \n  \n   \n \n   \n \nFigures & Captions \n \nTable 1: Prompt of different trials. This table comprises three rows and two columns. Each cell includes \ntwo distinct prompts: the first paragraph functions as the system prompt, and the second paragraph \nsignifies the beginning of the user's prompt. For example, in the OpenAI API, the initial paragraph is the \n‚Äúsystem prompt,‚Äù and the subsequent paragraph forms the ‚Äúuser prompt.‚Äù In other large language models \n(LLMs), these prompts may be amalgamated. Variations across rows primarily stem from the first \nparagraph, which may range from a basic t o a mechanics -specific or a domain -specific prompt. \nDistinctively, the second paragraph separates the columns: the first column contains prompts \nnecessitating an explanation, while the second column includes prompts that require LLMs to provide \nanswers without an explanation. \n   \n \n   \n \n \n \n \n\n   \n \n   \n \n \nFigure 1: Distribution of accurate responses to a set of 126 questions crafted for human participants. \n(a) The aggregate distribution of accurate answers reveals a predominant selection of options ‚Äú B‚Äù and \n‚ÄúC‚Äù, with ‚ÄúA‚Äù being the least chosen, aligning with human intuition. (b) Examining the distribution of \ncorrect choices across various domains, it is typically observed that one option is markedly less favored \nwhile the remaining three options exhibit a uniform distribution. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n   \n \n   \n \n \nFigure 2: Collection of unprocessed data from diverse groups. Results were gathered from \nundergraduate students majoring in mechanical engineering (ME major)  and non -mechanical \nengineering disciplines (non-ME Major), graduate students specializing in mechanical engineering, and \nfaculty members in the mechanical engineering program. For large language models, data were collected \nfrom Claude-2.1, GPT-3.5, and GPT -4, using a domain -specific prompt necessitating an explanatory \nresponse. \n \n \n \n \n \n\n   \n \n   \n \n \nFigure 3: Comprehensive analysis of results from human participants and LLMs . (a) Assessing \naccuracy across various groups, for GPT -3.5 and GPT -4, data from all three distinct prompts are \naggregated. The most recent models, including Claude -2.1 and GPT -4, demonstrate superior \nperformance compared to engineering faculty members. GPT -3.5 exhibits better results than graduate \nstudents. (b) In the comparison of engineering faculty and Claude, Claude‚Äôs accuracy distribution is more \nuniform, suggesting that human accuracy is more susceptible to variation based on question type or \ndifficulty. This is illustrated by a distinct clustering of low and high accuracy in humans. (c) Comparing \nhuman engineering faculty with AI models across different domains reveals that, in certain areas, faculty \nmembers specializing in specific domains outperform  AI models. As indicated in (a), providing an \nexplanation before the answer yields slightly better results, a trend more evident in this context. (d) \nAnalysis of varying prompts and their respective accuracies. \n \n \n\n   \n \n   \n \n \nFigure 4: Comparison of answers between random guessing and responses from undergraduate \nstudents in non- mechanical engineering (non -ME) majors. While the overall accuracy of these \nstudents slightly surpasses that of random guessing, their performance, lacking mechanical engineering \nexpertise, does not significantly exceed a random guess. \n \n\n   \n \n   \n \n \n \nFigure 5: Comparative analysis of accuracy histogram plots involving engineering faculty, GPT -\n3.5, and GPT-4. (a) Contrasts the performance of engineering faculty with that of GPT-3.5. (b) Examines \nthe differences between GPT-4 and engineering faculty. The depicted histograms illustrate that prefacing \nanswers with an explanation notably elevates the incidence of l ow accuracy. The trends reveal a \nsimilarity between GPT-3.5 and Claude (referenced in Figure 4b). GPT -4‚Äôs distribution, mirroring that \nof the engi neering faculty, suggests greater stability in its performance. Additionally, the tendency of \nGPT-4 to provide explanations indicates an inherent capacity for reasoning.  (c) To assess the stability \nand confidence of LLMs, we analyzed the Pearson correlation between different trials and compared it \nwith the results of the Engineering Faculty. Due to the variability in human respondents, their correlation \nis markedly lower. All LLMs demonstrate high stability, with GPT -4 outperforming others. (d) We \nevaluated the reasoning ability of LLMs by analyzing the similarity in accuracy distribution between \nhuman cohorts and LLMs, as illustrated in (a) and (b). The addition of explanations to LLM responses \nappears to enhance their human-like characteristics. \n \n \n \n\n   \n \n   \n \n \n \nFigure 6: Analysis of the impact of specifically designed prompts on model accuracy. (a) Evaluates \nthe influence of prompts on GPT -3.5‚Äôs performance without the requirement for an explanation. (b) \nAssesses the effect of prompts necessitating a prior explanation on GPT -3.5‚Äôs accuracy. (c) Examines \nthe impact of prompts on GPT -4 when no explanation is required. (d) Investigates the influence of \nprompts requiring a preceding explanation on GPT-4. A comparative analysis of figures (a, c) and (b, d) \nindicates that GPT -3.5 benefits more substantially from the designed prompts compared to GPT -4. In \nthe analysis of figures (a, b) and (c, d), it is observed that for GPT-3.5, prompts encompassing all domains \ngenerally yield improved results in most areas. Conversely, for GPT-4, domain-specific prompts tend to \nproduce better outcomes. \n \n \n \n \n \n \n\n   \n \n   \n \n \nFigure 7: Collection of incorrect choices from human participants and LLMs . (a) Intuitively, when \nconfronted with unfamiliar problems, humans tend to select options ‚ÄúB‚Äù and ‚ÄúC‚Äù. (b) In the case of LLMs, \nparticularly GPT-3.5, there is a tendency to choose options ‚ÄúA‚Äù and ‚ÄúC‚Äù; however, GPT -4 exhibits no \nclear preference for any option. Option ‚ÄúE‚Äù denotes that none of the presented choices are correct; option \n‚ÄúF‚Äù indicates that all options are correct, and option ‚ÄúG‚Äù signifies instances where the LLMs fail to \ngenerate an output that includes an answer. \n \n \n \n \n",
  "topic": "Applied mechanics",
  "concepts": [
    {
      "name": "Applied mechanics",
      "score": 0.4962826371192932
    },
    {
      "name": "Computer science",
      "score": 0.356125146150589
    },
    {
      "name": "Engineering ethics",
      "score": 0.3228920102119446
    },
    {
      "name": "Engineering",
      "score": 0.29836422204971313
    },
    {
      "name": "Mechanical engineering",
      "score": 0.21752023696899414
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165733156",
      "name": "University of Georgia",
      "country": "US"
    }
  ],
  "cited_by": 4
}