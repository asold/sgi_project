{
  "title": "Textual Datasets For Portuguese-Brazilian Language Models",
  "url": "https://openalex.org/W4297497034",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3187262638",
      "name": "Matheus Ferraroni Sanches",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    },
    {
      "id": "https://openalex.org/A3161966587",
      "name": "Jader M. C. de Sa",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    },
    {
      "id": "https://openalex.org/A4297506971",
      "name": "Henrique T. S. Foerste",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    },
    {
      "id": "https://openalex.org/A2630530379",
      "name": "Rafael R. Souza",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    },
    {
      "id": "https://openalex.org/A4260163826",
      "name": "Júlio C. dos Reis",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    },
    {
      "id": "https://openalex.org/A2103004164",
      "name": "Leandro A. Villas",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    },
    {
      "id": "https://openalex.org/A3187262638",
      "name": "Matheus Ferraroni Sanches",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3161966587",
      "name": "Jader M. C. de Sa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4297506971",
      "name": "Henrique T. S. Foerste",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2630530379",
      "name": "Rafael R. Souza",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4260163826",
      "name": "Júlio C. dos Reis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103004164",
      "name": "Leandro A. Villas",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3096266342",
    "https://openalex.org/W2962854379",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3016339201",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W4285293816",
    "https://openalex.org/W2806758205",
    "https://openalex.org/W2155870214",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3080944400",
    "https://openalex.org/W2963026768"
  ],
  "abstract": "Advances in Natural Language Processing have generated new models that push forward the state of the art. This reached new heights in complex tasks in handling unstructured texts. Most of the new architectures and models focus on the English language. There is a lack of available datasets that can be used during the training of new models. This investigation presents four new textual datasets for language modeling in Brazilian Portuguese. Our datasets were generated from several specific methodologies that aimed to obtain data of different natures. Two of our sets were originally built from data in online web forums. We also distribute a translated version of MultiWOZ, and a clean version of BrWaC. The original datasets are made available in a structured way to facilitate their use during the training of NLP models, with questions, answers and conversations already identified.",
  "full_text": "Textual Datasets For Portuguese-Brazilian Language Models\nMatheus Ferraroni Sanches, Jader M. C. de S´a, Henrique T. S. Foerste,\nRafael R. Souza, Julio C. Dos Reis, Leandro A. Villas\n1Institute of Computing, University of Campinas, Campinas, S˜ao Paulo, Brazil\n{m212142, j234830, h236651}@dac.unicamp.br\n{rroque, jreis, lvillas}@unicamp.br\nAbstract. Advances in Natural Language Processing have generated new mod-\nels that push forward the state of the art. This reached new heights in complex\ntasks in handling unstructured texts. Most of the new architectures and models\nfocus on the English language. There is a lack of available datasets that can\nbe used during the training of new models. This investigation presents four new\ntextual datasets for language modeling in Brazilian Portuguese. Our datasets\nwere generated from several specific methodologies that aimed to obtain data of\ndifferent natures. Two of our sets were originally built from data in online web\nforums. We also distribute a translated version of MultiWOZ, and a clean ver-\nsion of BrWaC. The original datasets are made available in a structured way to\nfacilitate their use during the training of NLP models, with questions, answers\nand conversations already identified.\nResumo. Avanc ¸os em Processamento de Linguagem Natural geraram novos\nmodelos no estado da arte e alcanc ¸aram novos patamares em tarefas complexas\nem tratamento de textos n ˜ao estruturados. A maioria das novas arquiteturas e\nmodelos foca na l ´ıngua inglesa. Constatamos uma baixa disponibilidade de\nconjuntos de dados que podem ser utilizados durante o treinamento de novos\nmodelos. Esta investigac ¸˜ao apresenta quatro novos conjunto de dados textuais\npara modelagem de linguagem no Portugu ˆes-Brasileiro. Nossos conjuntos de\ndados foram gerados a partir de diversas metodologias espec´ıficas que visaram\nobter dados de diferentes naturezas. Dois de nossos conjuntos foram original-\nmente constru´ıdos a partir dados em for ´uns Web online. Distribu ´ımos igual-\nmente uma vers˜ao traduzida do MultiWOZ, e uma vers ˜ao limpa do BrWaC. Os\nconjuntos de dados originais s˜ao disponibilizados de maneira estruturada para\nfacilitar sua utilizac ¸˜ao durante o treinamento de modelos PLN, com perguntas,\nrespostas e conversas j´a identificadas.\n1. Introduction\nThe growing computing power allied with an increasing amount of data available al-\nlowed state-of-the-art models to achieve higher scores in benchmarks [Wang et al. 2018,\nRajpurkar et al. 2018]. The Natural Language Processing (NLP) area is taking ad-\nvantage of these advances and newly available data. Techniques based on synthetics\nare becoming obsolete as deep models have explored semantics to understand words\nand phrases better. The state of the art in NLP is moving fast, with new architec-\ntures [Vaswani et al. 2017], models [Devlin et al. 2018, Radford et al. 2019] and datasets\nProceedings of the IV Dataset Showcase Workshop (DSW) September 2022 – B´ uzios, RJ, Brazil\n1\n[Budzianowski et al. 2018, Wagner et al. 2018] being created and shared. Advances in\nthis field allow new research topics and can be used as a foundation for novel solutions.\nDespite the advances and breakthroughs, train NLP models are still a big chal-\nlenge. The model architecture and the amount of data used make the training process\nof NLP models expansive, both in price and in time. A single NLP model can cost up\nto millions of dollars [Sharir et al. 2020]. Scaling the model size and amount of data\nused during the training stage can increase performance, whereas reducing over-fitting\n[Kaplan et al. 2020]. Although the high costs, the benefits of increasing model size and\namount of data are still an interesting tradeoff.\nIn order to get around the high costs related to training NLP models from scratch,\nresearchers can perform only fine-tuning in pre-trained generic models with a smaller\ndataset. This can be combined with a shorter training with a lower learning rate, pre-\nserving most of the patterns already learned by the model. This approach has several\nadvantages but relies on good generic models to be refined, which can be very limited in\nmany languages [Howard and Ruder 2018].\nThe lack of models in different languages can be directly related to the lack of\ndatasets, as without proper data to train the models, the training process is affected. Fur-\nthermore, models that deal with specific problems and contexts, such as models for Ques-\ntion Answering about the medical area, require detailed data to be appropriately trained.\nPerforming fine-tuning must rely on adequate models to use less data during the training\nstage and achieve good results [Howard and Ruder 2018].\nIn this investigation, we provide open domain data to pave the way for future\nresearchers to train new generic or contextual specific models. We introduce four new\ndatasets: two entirely new datasets; and two adapted datasets. The approach and tools\nemployed are documented. Our key contributions are two novel datasets with human-to-\nhuman conversation with multiple answers to the same message identified; one translated\ndataset; one cleaned dataset. We demonstrate the defined methodology used to translate\nand clean the datasets. Our methodology must be helpful for future researchers with\nsimilar challenges.\n2. Related Work\nThe increasing demand for NLP models leads to novel research topics by promoting the\ncreation and sharing of new databases to support researchers. We describe datasets avail-\nable today in Portuguese and two in English related to our study.\nThe Web corpus for Brazilian Portuguese (brWaC)[Wagner et al. 2018] dataset is\none of the most popular datasets in Portuguese due to its size and variety of content.\nThis dataset was constructed adopting an approach called WaCky (Web-As-Corpus Kool\nYinitiative), which can obtain data from multiple languages extracting content from the\nWeb. This dataset contains more than 3.5 million pages of 120,000 websites and more\nthan 2.7 billion tokens. Although this is one of the biggest datasets in Portuguese, and\nthe content has been filtered, the quality of many instances is questionable due to the poor\nwriting and offensive/racist content. The model BERTimbau [Souza et al. 2020], based\non BERT [Devlin et al. 2018], was trained on this dataset. It is possible for the model to\noutput names of politics-related with offensive words.\nProceedings of the IV Dataset Showcase Workshop (DSW) September 2022 – B´ uzios, RJ, Brazil\n2\nWikipedia creates a standardized manner to share data by enabling to acquire data\nabout articles [Meta 2021]. This data can be used in several ways: NLP models can\nbe trained on high-quality content, with annotations and facts from the articles. The\nlimitation faced in this data is the specific writing style found on Wikipedia. We found\nthat the text is completely correct and written in an informative way. In other words,\nunderlying models trained on this basis understand and generate informative writing style\noutputs without slang or abbreviations. The model GPorTuguese-2 [Guillou 2020] was\ntrained on this dataset. Tests on the model showed that it is unable to handle slang and\nabbreviations due to the specific writing style used during training.\nThe MultiDomain Wizard-of-Oz (MultiWOZ) [Budzianowski et al. 2018] is an\nEnglish fully labeled dataset. The instances were generated by two humans interacting,\nsimulating a user requesting information or reservations to a system. More than 10,000\ndialogues are available. This dataset is a large-scale multi-turn conversation dataset about\ndifferent domains with annotations. This dataset’s major limitation remains on the origi-\nnal language, as every instance is in English.\nThe Ubuntu Dialogue Corpus [Lowe et al. 2016] is a dataset in English that uses\na straightforward methodology to identify answers to other messages and recreate the\ndialogue between users. The source of this dataset is a technical forum about Ubuntu\nOperational System. Although the methodology to identify the dialogues is simple, this\ndataset can identify dialogues with hundreds of turns; the mean messages per dialogue\nare around seven messages. This work paved the way for new approaches to identify\nconversations by sharing a unique dataset. Despite the advances, the identification of\nconversations is very limited, and the generated data is only available in English.\nWe reveal that the lack of datasets is a problem in many languages. This fact can\nbe checked at HuggingFace1, a website specialized in NLP models and datasets. There\nare more than 1000 datasets in English[HuggingFace 2022a]; there are around 100 in\nPortuguese[HuggingFace 2022b]. This problem worsens if the trained model requires a\nspecific context, structure, or a specific type, such as conversations or translated sentences.\nThe training stage usually requires that the data being used are in a specific language and,\nif possible, inside or close to a specific theme to obtain better results.\nThe lack of appropriate models for many languages forces the researchers to use\nalternative approaches, such as translating the original sentence to a specific language to\nuse an already trained model [Poncelas et al. 2020]. This process increases the cost and\ntime for such operations and does not guarantee good results.\n3. Datasets\nThis Section presents the datasets proposed with details about how they were created or\nadapted. The final result of each dataset is in the Portuguese language. Each dataset\nmaintains specific characteristics, such as different objectives and domains. In the list\nbellow, we present a short reference to every dataset presented and how they were created.\n1. MultiWOZ-PTBR: This dataset is a direct translation from the original Multi-\nWOZ [Budzianowski et al. 2018]. The content is a fully annotated multi-domain\nconversation composed of an exchange between a user and a system with a specific\n1https://huggingface.co/\nProceedings of the IV Dataset Showcase Workshop (DSW) September 2022 – B´ uzios, RJ, Brazil\n3\ngoal, such as ask information about places and making reservations (cf. Subsec-\ntion 3.1).\n2. Cleaned BrWaC: This dataset is a cleaned version from the original dataset\nBrWaC [Wagner et al. 2018]. The original dataset collects multiple websites with-\nout a specific domain, making this data generic and large. The generic and cover-\nage are a trade for quality, as many politics-related websites or offensive content\nare indexed (cf. 3.2).\n3. MCCD Generated: Novel human conversational datasets generated using our\nproper methodology MCCD [Sanches. et al. 2022] and Miner-XenForo2 (cf. 3.3).\n(a) Adrenaline Dataset: Dataset based on a Web Forum about technology,\nhardware, and games. We were able to identify high engagement in con-\nversations by users and longer conversations.\n(b) OuterSpace Dataset: Dataset based on a Web Forum about games on\ndifferent platforms and millions of messages generic messages.\nWe choose not to compare these datasets directly as the final objective, creation\nmethod, annotation, and structure may differ and this may lead to wrong conclusions. All\nthe scripts used to generate our datasets, methodologies, software tools, and datasets are\navailable at GitHub 3.\n3.1. MultiWOZ-PTBR\nThe original MultiWOZ is a dataset to create high-quality data to train NLP models to\ndeal with dialogues, where a user and a system talk to each other through turns about\ndifferent topics. Besides the large size of this dataset, with more than 110.000 turns, the\ncontent is fully annotated, allowing researchers to use this dataset for distinct objectives.\nThis dataset is only available in English because the generated data uses crowd-\nsourcing methods. The generation process was done using Amazon Mechanical Turk 4\nusers following a script to create the initial conversation and later annotate each turn fully.\nThe utterance contains the actual text obtained through crowd-sourced. It is usu-\nally 13 words long, for instance, ”I need a train to stansted airport that leaves after 21:30.\nI am also going to be looking for local places to eat .”. The slot value only contains spe-\ncific information that was annotated, for instance, “stansted airport”. Meta data and data\nunrelated to the sentences were not translated as they would not be used by any model\ndirectly.\nEvery slot value is related to a unique utterance and is part of the utterance string.\nAlthough this is very useful during the annotation, the matching between the slot value\ninside the string may be missing after the translation. This behavior is caused by the trans-\nlation, which may translate an entire utterance differently to just a piece to better match\nthe context. For instance, the utterance “Yes, I am also looking for a barbeque restaurant\nin the same area and price range as my hotel.” has the slot “barbeque”.\nThe translated utterance “ Sim, tamb ´em estou procurando uma churrascaria na\nmesma ´area e faixa de prec ¸o do meu hotel. ” has the slot ”churrasco”. In this case, the\n2https://github.com/MatheusFerraroni/miner-xenforo\n3https://github.com/MatheusFerraroni/nlp ptbr datasets\n4https://www.mturk.com/\nProceedings of the IV Dataset Showcase Workshop (DSW) September 2022 – B´ uzios, RJ, Brazil\n4\nchange occurs because the target language uses the word “ churrascaria” to “ barbeque\nrestaurant”, causing the loss between these direct matches. To tackle the changes between\nutterance and slot values after the translation, we identified the mismatches and manually\nfixed most of these cases. Although there are a few occurrences in the final version, the\ndataset already is a viable option to train NLP models.\n3.2. Cleaned BrWaC\nThe WaCky - The Web-As-Corpus Kool Yinitiative [Baroni et al. 2009] paved the way\nfor a new set of datasets based on WebSites. This kind of corpus is often very large, as\nit can collect the content of many different websites. Various websites, including news to\npersonal blogs, collaborate on the corpus generality.\nBased on WaCky initiative, the BrWaC dataset was created to be the equivalent\nof this corpus, but in Portuguese. As the general purpose and the extensive collection\nof websites, this dataset is one of the biggest available in that language. However, the\ndata does not have any meta-data, and some instances can be labeled as offensive or\nlow-quality. The model BERTimbau[Souza et al. 2020], trained with BrWaC, generates\nproblematic outputs due to examples presented in the training dataset.\nFigure 1 presents the output from BERTimbau to replace the token [MASK] in\nthe sentence ’THE PSYCHOPATHY of [MASK] is so big’, with the ’is’ being written\nas a slang. The problem occurs as the model suggests the name of a Brazilian politician,\nHitler, a USA politician, and God to replace the token.\nFigure 1. Problematic output from BERTimbau model.\nIn order to tackle this problem and create a cleaner version of BrWaC corpus,\nwe applied a specific methodology with three steps: 1) Data Exploration; 2) low-quality\nremoval; and 3) offensive content removal.\nThe BrWaC is separated into two main data types, an URL and the content of\nthat specific WebPage. By exploring the URLs available, we found that many URLs are\nrelated to personal blogs and politics. On this basis, we created a list of ignored domains.\nBased on the findings of the URLs, we removed most of the problematic content, as low-\nquality content was usually related to personal blogs.\nIn order to further filter the content, we developed an algorithm capable of inspect-\ning every word in the dataset and searching for offensive terms, which led to discarding\nProceedings of the IV Dataset Showcase Workshop (DSW) September 2022 – B´ uzios, RJ, Brazil\n5\nthat specific instance from the dataset. A list of offensive terms was created and fed\ninto our algorithm, which searches for variations for these offensive terms. We discarded\ninstances not classified as Portuguese language or that had more than 25% of digits.\nThe original BrWaC corpus contains 120989 domains and 3530796 pages. After\nthe cleaning process, the cleaned version contained 96814 domains and 3166108 pages,\nreducing more than 24000 domains and more than 360000 pages.\n3.3. MCCD Generated\nThis Subsection presents the datasets created using our MCCD methodology\n[Sanches. et al. 2022] and Miner-XenForo [Sanches. et al. 2022]. We created two\ndatasets using the same methodology and software tool but with different data sources.\nDue to this, the shared datasets have very different content with specific characteristics.\nBoth datasets use online forums as data sources due to constraints from the Miner-\nXenForo tool. Although this may limit the domain from the generated datasets to the\nforum domain, the data generated has some moderation. Such forums rely on rules and\nadministrators to keep order and organization. All data gathered is publicly available and\ncan be found accessing the source directly in the browser.\nThe datasets are released in two versions, one with raw data acquired, containing\nonly data about messages and categories, and one with the processed files and conver-\nsations identified. The conversations have been identified by the Miner-XenForo, which\nimplements our methodology MCCD to follow references between messages to identify\nall the possible conversation flows.\nTable 1 compares different metrics of both generated datasets. The raw size of\nboth datasets is similar, but the Processed size is very different, especially if we compare\nit with the Raw Size. We observe that the Adrenaline gets 2730% larger after processing,\nwhereas OuterSpace increases only 17%. This difference is explained by other metrics,\nwhich highly influence the number of conversation flows.\nTable 1. Comparison between Adrenaline and OuterSpace Dataset.\nData Adrenaline OuterSpace Adrenaline/OuterSpace\nRaw Size (Gb) 2,0 4,6 0,43\nProcessed Size (Gb) 71 5,4 13,0\nTotal Topics 356k 570k 0,62\nTotal Messages 9.5M 24M 0,38\nTotal Tokens 477M 1T 0,44\nMean Size Per Topic 26 42 0,62\nMean Token Per Post 50 43 1,14\nMean Size Conversation 34 3 11,3\nTotal Conversation 4.5M 3M 1,47\nUnique Tokens 1.7B 2.7B 0,63\nMean Token per Topic 1.3K 1.8k 0,71\nSource https://forum.adrenaline.com.br/https://forum.outerspace.com.br/ -\nBoth datasets have similar raw sizes, total topics, total conversations, and total\nmessages. Nevertheless, users from Adrenaline are more engaged in conversations, which\nthe Mean Size Conversation confirms. In the Adrenaline dataset, the discussions are\nusually longer, with 34 messages; in OuterSpace conversation, the mean size is only 3.\nThis means that the Adrenaline Forum users are more likely to answer other messages\nProceedings of the IV Dataset Showcase Workshop (DSW) September 2022 – B´ uzios, RJ, Brazil\n6\nand these conversations are longer than OuterSpace. More persons can join with longer\nconversations, and more conversational flows were identified.\nFigure 2 compares the total of messages per topic in both datasets. We observe\nthat the distribution of messages per topic is almost exponential, with many topics with\nfew messages and few topics with many messages. The OuterSpace dataset has a larger\nnumber of topics without any response. Considering the characteristics presented, we note\nthat even with more messages in general and more messages per topic, the OuterSpace\nfails to engage users in conversations.\n0 10000 20000 30000 40000\nTotal Topics\n100\n101\n102\n103\n104\n105\nTotal Messages\n134202\nAdrenaline\n0 10000 20000 30000 40000\nTotal Topics\n100\n101\n102\n103\n104\n105\n324882\nOuterSpace\nTotal messages per topic\nFigure 2. Comparison between total messages per topic between Adrenaline and\nOuterSpace datasets.\n4. Experimental Evaluation\nThis section evaluates two BERT models trained for two classification tasks. These clas-\nsification tasks were chosen based on their fitness to highlight the difference in how the\nmodels deal with specific contexts. The first model is a literature model, BERTimbau; the\nsecond is a sample model trained on a portion of the Adrenaline dataset exclusive for this\nevaluation. This evaluation aims to demonstrate that the proposed datasets can be used to\ntrain novel NLP models and which limitations are faced in different situations.\n4.1. Procedures\nThe Figure 3 presents the complete flow during evaluation, including the training of our\nsample Bert Model to the final classification task performed. The grey box represents the\nsets to train our sample model and the green box represents the results from the classifi-\ncation tasks to determine the Category and Sub-Category of each instance.\nAdrenaline\nDataset\nSample\nBERT Model\n2Gb\nBERTimbau\nClassification\nDataset Embedding\nMLPClassifier1 Predicted\nCategory\nMLPClassifier2 Predicted\nSub-Category\nFigure 3. Complete Evaluation flow\nProceedings of the IV Dataset Showcase Workshop (DSW) September 2022 – B´ uzios, RJ, Brazil\n7\nThe first step in the evaluation was creating a sample BERT model to be compared\nwith an existing model from the literature. We use the raw data from Adrenaline Dataset,\npresented in Section 3, to train this sample model. Data used from the Adrenaline Dataset\nrepresents about 2Gb of textual data.\nOur sample BERT model was trained using masked language modeling, with a\nlearning rate of5−05 and AdamW optimizer. As the dataset has a large variety of sentences\nand this is just a sample model, we train this model for only one epoch with a weight\ndecay of 5−07. As the evaluation task is known, we safely set the max sequence length\nconsidered by the model to 512.\nBoth models, our sample BERT-base model, and BERTimbau, were used to gen-\nerate the embedding for each instance from both classification tasks. This was done by\ninputting the instance to the model and saving the state of the last layer once the model\noutput the token “[CLS]”. The embedded save is formed by and 1D array with 768 floats.\nThe embedded generated for each instance was then used to train a Multi-layer\nPerceptron classifier, using the embedding as input and the class as label. We used the\nsame architecture for the MLPClassifier in both classification tasks, using 100 layers with\n200 nodes, each activated by using a ReLU function. In both classification tasks, 75% of\nthe dataset was used during the training stage, and 25% was used as validation.\nThe first classification task is to determine which Category and Sub-Category a\npost belong in a portion of the Adrenaline Dataset, considering only the title and the first\nmessage content concatenated. Figure 4 shows how the Categories and Sub-Categories\nare distributed. There are seven categories with an unbalanced number of Sub-categories.\nThe first Category has 10 Sub-Categories; the second and third Categories have 4 Sub-\nCategories; the fourth Category has three sub-categories; the fifth and sixth categories\nhave two Sub-Categories; and the last Category has only one Sub-Category. Every Sub-\nCategory has exactly 900 instances. The total instances in the Sub-Categories are bal-\nanced, but the instances in the Categories are unbalanced.\n1 2 3 4 5 6 7 8 9 1011121314151617181920212223242526\nSub-Categories\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000Total Instances\nBERT models in different tasks\nCat 1\nCat 2\nCat 3\nCat 4\nCat 5\nCat 6\nCat 7\nFigure 4. Categories and Sub-Categories distribution.\nWe used both BERT models to determine the Category using the embedding for\nProceedings of the IV Dataset Showcase Workshop (DSW) September 2022 – B´ uzios, RJ, Brazil\n8\neach instance as input. In order to assist the Sub-Category prediction, the predicted Cate-\ngory was input with the embedding to predict the Sub-Category.\nThe second classification task is to determine if a movie review is positive or nega-\ntive [Gonc ¸alves 2022] (sentiment analysis). This dataset contains almost 50000 instances\nand is already balanced between positive and negative reviews. This scenario was chosen\nto compare the effectiveness of the evaluated BERT models on a scenario with different\nwords and goals as a generic scenario.\n4.2. Experimental Results\nFigure 5 shows the score obtained for the two classification tasks. The bars at “Category”\nand “Sub-Category” are the scores obtained at the classification Task in the Adrenaline\nDataset; the bars at “Sentiment” are the scores in the sentiment analysis classification\ndataset. Although we show only the score obtained, the F1 score diverges less than 1pp.\nResults obtained from our evaluation indicate that our sample model, which was\ntrained for one epoch, achieves a better result representing the sentences and working as\ninput to anMLPClassifier. The sample model scores more than 9% more thanBERTimbau\nat classifying the Category and almost 4% more at classifying the Sub-Category. These\ngains translate to a 4pp at the Category and 3pp at the sub-Category.\nAlthough our sample model achieves better scores at the first classification task,\nthe BERTimbau gets better results classifying sentiments in the second classification task.\nBERTimbau was able to score 0.86, while our sample model only scored 0.77, repre-\nsenting about 11% gain. The score difference happens due to the training dataset used\nby BERTimbau, which is more open-domain, allowing the model to be better suited for\ngeneric tasks.\nCategory Sub-Category Sentiment0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n0.82 0.81\n0.86\n0.78 0.78 0.77\nBERT models in different tasks\nSample Model\nBERTimbau\nFigure 5. Score comparison between our sample model and BERTimbau.\n5. Discussion\nThe explored datasets play a central role as a factor that influences the results obtained.\nThis happens due to ability of the model to properly understand the semantics of words\nand phrases to represent a better context at the embedding layer.\nProceedings of the IV Dataset Showcase Workshop (DSW) September 2022 – B´ uzios, RJ, Brazil\n9\nThe Adrenaline and the OuterSpace datasets were explicitly processed and struc-\ntured with human conversations between two or more identified users. We employed a\nspecific methodology to translate the MultiWOZ dataset to Portuguese. We described the\napproach used during the translation and how different portions of the translated sentences\nwere matched. Furthermore, we created a cleaned version of the biggest Web Corpus in\nPortuguese, BrWaC. We showed how we removed low-quality content and instances clas-\nsified as offensive by our methodology.\nAlthough the Adrenaline and OuterSpace datasets can be used to train models with\ndistinct objectives, such as Questions & Answers (QA) and text generation, both datasets\nare limited to the specific themes from the original data source. The Adrenaline dataset\nwas created from an online forum with the main topic related to technology and games.\nOuterSpace dataset has a larger but limited range of topics, including one sub-category\ncompletely open topic. Models being trained on these datasets require attention to this\nand may require an additional dataset to better represent the sentences into embedding.\nTo complete the data being used during the training stage, we can use our cleaned\nversion of BrWaC with Adrenaline and OuterSpace datasets. With the composition of\ndatasets being used, it is possible to train an NLP model to understand the sentences\nbetter and achieve better results.\nThe difference in the model’s ability to understand the data is directly related to the\ndataset used and how the training stage was conducted. The dataset utilized by our sample\nmodel was directly related to the data being evaluated. In this sense, the model could bet-\nter represent specific words from the dataset context, which result in better representation\nat the embedding layer. The BERTimbau was trained with BrWaC, a very generic and\nlarge dataset, allowing the model to perform well on most tasks but not achieve the best\nresult possible in specific tasks.\nThe training stage on our sample model was small, as this model was trained only\nto validate the datasets and show it is possible to obtain an adequate model even with small\ndata. Despite the differences and conclusions already explained, it is possible to refine an\nalready trained version of the model to a specific domain, such as the BERTimbau. Using\nthis approach, the model can keep most of the knowledge already learned and achieve\neven better results. Using context-specific data during the refining stage with a lower\nlearning rate allows the model to learn new patterns related to the new context.\n6. Conclusion\nThe lack of specific datasets to address key tasks in different languages is a challenging\nresearch problem. This study presented a set of datasets specifically created or adapted\nto train or refine NLP models in the Portuguese language. We share two novel human-to-\nhuman datasets with multiple answers to the same message in Portuguese, one translated\ndataset, and one cleaned version of a well-established dataset. We found that NLP models\ntrained with our dataset can outperform literature models in specific tasks. Future studies\ninvolve training the first NLP models with BERT and GPT architectures, using the four\ndifferent datasets presented. These models may be employed to solve various tasks and\nevaluated against the state-of-the-art models on well-established benchmarks. We will\ndetermine the differences in how the state-of-the-art models and our models can represent\nwords and sentences in the embedding layer and how this influences several tasks.\nProceedings of the IV Dataset Showcase Workshop (DSW) September 2022 – B´ uzios, RJ, Brazil\n10\nAcknowledgments\nThis research was supported by CI&T. We are thankful to our colleagues Diego Augusto,\nLead Data Scientist at CI&T, and Gabriel Marostegam, Head of Data at CI&T, for their\nimportant participation in discussions during the development of this work.\nReferences\n[Baroni et al. 2009] Baroni, M., Bernardini, S., Ferraresi, A., and Zanchetta, E. (2009). The\nWaCky wide web: a collection of very large linguistically processed web-crawled cor-\npora. Language Resources and Evaluation, 43(3):209–226.\n[Budzianowski et al. 2018] Budzianowski, P., Wen, T.-H., Tseng, B.-H., Casanueva, I.,\nUltes, S., Ramadan, O., and Gaˇsi´c, M. (2018). Multiwoz – a large-scale multi-domain\nwizard-of-oz dataset for task-oriented dialogue modelling.\n[Devlin et al. 2018] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert:\nPre-training of deep bidirectional transformers for language understanding.\n[Gonc ¸alves 2022] Gonc ¸alves, L. (2022). Imdb pt-br. https://www.kaggle.com/\ndatasets/luisfredgs/imdb-ptbr. Accessed: 2022-05-25.\n[Guillou 2020] Guillou, P. (2020). Gportuguese-2 (portuguese gpt-2 small): a language\nmodel for portuguese text generation (and more nlp tasks...).\n[Howard and Ruder 2018] Howard, J. and Ruder, S. (2018). Universal language model fine-\ntuning for text classification.\n[HuggingFace 2022a] HuggingFace (2022a). Hugging face – the ai community building the\nfuture. https://huggingface.co/datasets?languages=languages:\nen. Accessed: 2022-05-25.\n[HuggingFace 2022b] HuggingFace (2022b). Hugging face – the ai community building the\nfuture. https://huggingface.co/datasets?languages=languages:\npt. Accessed: 2022-05-25.\n[Kaplan et al. 2020] Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B.,\nChild, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). Scaling laws for\nneural language models.\n[Lowe et al. 2016] Lowe, R., Pow, N., Serban, I., and Pineau, J. (2016). The ubuntu dialogue\ncorpus: A large dataset for research in unstructured multi-turn dialogue systems.\n[Meta 2021] Meta (2021). Main page — meta, discussion about wikimedia projects. [On-\nline; accessed 25-May-2022].\n[Poncelas et al. 2020] Poncelas, A., Lohar, P., Way, A., and Hadley, J. (2020). The impact\nof indirect machine translation on sentiment classification.\n[Radford et al. 2019] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever,\nI. (2019). Language models are unsupervised multitask learners.\n[Rajpurkar et al. 2018] Rajpurkar, P., Jia, R., and Liang, P. (2018). Know what you don’t\nknow: Unanswerable questions for squad.\nProceedings of the IV Dataset Showcase Workshop (DSW) September 2022 – B´ uzios, RJ, Brazil\n11\n[Sanches. et al. 2022] Sanches., M., C. de S ´a., J., M. de Souza., A., Silva., D., R. de Souza.,\nR., Reis., J., and Villas., L. (2022). Mccd: Generating human natural language conver-\nsational datasets. In Proceedings of the 24th International Conference on Enterprise\nInformation Systems - Volume 2: ICEIS,, pages 247–255. INSTICC, SciTePress.\n[Sharir et al. 2020] Sharir, O., Peleg, B., and Shoham, Y . (2020). The cost of training nlp\nmodels: A concise overview.\n[Souza et al. 2020] Souza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: Pretrained\nbert models for brazilian portuguese. In Cerri, R. and Prati, R. C., editors, Intelligent\nSystems, pages 403–417, Cham. Springer International Publishing.\n[Vaswani et al. 2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need.\n[Wagner et al. 2018] Wagner, J., Wilkens, R., Idiart, M., and Villavicencio, A. (2018). The\nbrwac corpus: A new open resource for brazilian portuguese.\n[Wang et al. 2018] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R.\n(2018). Glue: A multi-task benchmark and analysis platform for natural language\nunderstanding.\nProceedings of the IV Dataset Showcase Workshop (DSW) September 2022 – B´ uzios, RJ, Brazil\n12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8445461392402649
    },
    {
      "name": "Portuguese",
      "score": 0.7284550666809082
    },
    {
      "name": "Focus (optics)",
      "score": 0.673725426197052
    },
    {
      "name": "Natural language processing",
      "score": 0.6421022415161133
    },
    {
      "name": "Language model",
      "score": 0.5918490290641785
    },
    {
      "name": "Artificial intelligence",
      "score": 0.563295841217041
    },
    {
      "name": "Training set",
      "score": 0.4543421268463135
    },
    {
      "name": "Natural language",
      "score": 0.42321568727493286
    },
    {
      "name": "Information retrieval",
      "score": 0.3332650065422058
    },
    {
      "name": "World Wide Web",
      "score": 0.3255322575569153
    },
    {
      "name": "Linguistics",
      "score": 0.18365338444709778
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ]
}