{
  "title": "Large Language Models in Psychology: Application in the Context of a Systematic Literature Review.",
  "url": "https://openalex.org/W4392637694",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2886754685",
      "name": "Angélique Roquet",
      "affiliations": [
        "Czech Academy of Sciences, Institute of Psychology",
        "University of Lausanne",
        "Center for Health, Exercise and Sport Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A224534478",
      "name": "Kim Uittenhove",
      "affiliations": [
        "Czech Academy of Sciences, Institute of Psychology",
        "University of Lausanne",
        "Center for Health, Exercise and Sport Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1855611839",
      "name": "Paolo Martinelli",
      "affiliations": [
        "Czech Academy of Sciences, Institute of Psychology",
        "University of Lausanne",
        "Center for Health, Exercise and Sport Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2183490157",
    "https://openalex.org/W2053154970",
    "https://openalex.org/W4382310450",
    "https://openalex.org/W4388202310",
    "https://openalex.org/W4385614921",
    "https://openalex.org/W4384389802",
    "https://openalex.org/W4294214983",
    "https://openalex.org/W2969341057",
    "https://openalex.org/W3118615836",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4367368990",
    "https://openalex.org/W4386173035",
    "https://openalex.org/W4391655051",
    "https://openalex.org/W2485698693",
    "https://openalex.org/W4385259451",
    "https://openalex.org/W4320717566",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "The present study assesses the potential of employing Large Language Models (LLMs) in the context of a systematic literature review in psychology. We tasked one of the currently available ChatGPT-4-turbo-preview models from OpenAI with a qualitative coding assignment, which involved identifying elements related to a specific theoretical-analytical framework within 39 scientific empirical papers. We evaluated the quality of LLM-generated outcomes by comparing them with results generated through traditional human coding. In the process, we outlined the capabilities and advantages of using LLMs for systematic literature reviews, including practical considerations for their implementation. Our analyses showed that the LLM produced results that aligned closely with those obtained through traditional human coding. Furthermore, our experience indicated that incorporating LLMs into our research workflow was time- and cost-effective. Our results suggest that researchers and LLMs can work synergistically, improving efficiency, cost-effectiveness, and quality of the systematic literature review process. We underline the critical role of human arbitration in prompt crafting and decision-making.",
  "full_text": "   \n \n  1 \n \nLarge Language Models in Psychology: Application in the Context of a Systematic \nLiterature Review. \n \nKim Uittenhove1,2, Paolo Martinelli1,2, and Angélique Roquet1,2 \n1Institute of Psychology, University of Lausanne \n2Research Center for Health, Sport, and Aging Psychology (PHASE) \n \nShort Title: Supporting Psychology Research with Large Language Models \n \n \n \nAuthor Note\n \nPaolo Martinelli and Kim Uittenhove contributed equally to this work. \nCorrespondence concerning this article should be addressed to Dr. Kim Uittenhove \n(kimuittenhove@msn.com) or to Paolo Martinelli (paolo.martinelli@unil.ch).  \n \n \n \nKeywords: Large Language Models, Literature Review, Psychology Research \n \nLarge Language Models in Psychology \n2 \n \nAbstract \nThe present study assesses the potential of employing Large Language Models \n(LLMs) in the context of a systematic literature review in psychology. We tasked one of \nthe currently available ChatGPT-4-turbo-preview models from OpenAI with a qualitative \ncoding assignment, which involved identifying elements related to a speci�ic theoretical-\nanalytical framework within 39 scienti�ic empirical papers. We evaluated the quality of \nLLM-generated outcomes by comparing them with results generated through traditional \nhuman coding. In the process, we outlined the capabilities and advantages of using LLMs \nfor systematic literature reviews , including practical considerations for their \nimplementation. Our analyses showed that the LLM produced results that aligned closely \nwith those obtained through traditional human coding.  Furthermore, o ur experience \nindicated that incorporating LLMs into our research work�low was time- and cost-\neffective. Our results  suggest that researchers and LLMs  can work synergistically, \nimproving ef�iciency, cost-effectiveness, and quality of the systematic literature review \nprocess. We underline the critical role of human arbitration in prompt crafting and \ndecision-making.  \n \n  \n   \n \n  3 \n \nIntroduction \nThe advent of Large Language Models (LLMs) and their widespread availability \nto the public mark a signi�icant milestone. It is highly likely that this constantly evolving \ntechnology will become part of many social and scienti�ic sectors (Fecher, Heibing, \nLaufer, Pohle, & Sofsky, 2023). Therefore, psychology researchers have a duty to \ninvestigate the responsible use of LLMs and pinpoint this technology’s strengths and \nlimitations in addressing the shortcomings of traditional research. \nLLMs are exceptional at processing language and identifying complex concepts. \nThese abilities emerge from the interplay between architecture and learning, analogous \nto how in humans, specialized brain architecture and accumulated learning shape \ncognition (e.g., Suri, Slater, Ziaee, & Nguyen, 2024; Petersen & van der Plas, 2023). More \nprecisely, LLMs derive their capabilities from two core elements. Firstly, the \n\"transformer\" architecture (Vaswani et al. in 2017), facilitated by advances in \ncomputational power, allows LLMs to process entire sections of text comprehensively, \nwhile simultaneously focusing on relevant words in relation to context (e.g., Naveed et \nal., 2023). Thus, this parallel processing of text outperforms earlier sequential \nprocessing models such as recurrent neural networks (Vaswani et al., 2017). Secondly, \nLLMs are trained on vast amounts of human-generated text (e.g., Raffel et al., 2023), \nallowing them to recognize patterns and meanings within our language. The interplay of \nthese two core elements engenders LLMs' remarkable ability to perform complex \nlinguistic tasks that once posed signi�icant challenges to machines. \nIn the context of psychology research, there could be many advantages to using \nLLMs for analyzing primary (e.g., human narratives) and secondary qualitative research \ndata (e.g., scienti�ic literature). By employing this technology, researchers may be able to \n   \n \n  4 \n \nautomate text analysis, and have it executed at a pace that is impossible to match for \nhumans (e.g., To ̈ rnberg, 2023). Consequently, this would accelerate research processes \nand diminish the resources required for such tasks. Ultimately, it would enable \nresearchers to enhance their ef�iciency, obtain results faster, and allocate more time to \nhigher-level tasks, such as results interpretation or theoretical framing.  \nHowever, it is necessary to experiment with this technology to understand the \nstrengths and limitations of using it in psychology research. Therefore, our study \nfocused on the potential of using LLMs for a systematic literature review in psychology \nand evaluates the outcomes generated by this approach. Importantly, there is currently \nno established method for evaluating the quality of LLM-driven analysis. As an initial \neffort towards establishing such methods, we compare the outcomes produced by LLMs \nwith those generated through traditional human efforts. Furthermore, we outline \npractical aspects of employing LLMs for qualitative coding, based on our �irsthand \nexperience. \nApplication in the Context of a Systematic Literature Review  \nPrevious research has argued that systematic literature reviews are particularly \nappropriate for analysis with LLMs (e.g., Qureshi et al., 2023; Nashwan & Jaradat, 2023). \nSystematically coding scienti�ic texts in accordance with several criteria is a time-costly \nand resource-intensive process, which often takes up time from multiple researchers for \nseveral months (Michelson & Reuter, 2019). Moreover, despite the availability of \nguidelines aimed at increasing the rigor of systematic reviews (e.g., Preferred Reporting \nItems for Systematic Reviews and Meta-Analyses – PRISMA; Page et al., 2021), the \nprocess is marked by inherent subjectivity and uncertainty as coders interpret and \nclassify responses differently, an issue solved through extensive discussion to reach \n   \n \n  5 \n \nconsensus (e.g., Uttley et al., 2023). Thus, the use of LLMs provides an important \nopportunity for systematic reviews in terms of coding ef�iciency and quality of results \n(de la Torre-Lo ́ pez, Ramı́rez, & Romero, 2023; Schopow, Osterhoff, & Baur, 2023). \nIndeed, using LLMs could standardize and streamline this process, while speeding it up \nand decreasing the time burden on human researchers. In addition, while the coding \nuncertainty cannot be entirely eliminated— since qualitative evaluations remain \nsubjective despite achieving consensus among coders —the use of LLMs in conjunction \nwith human judgment holds the potential for enhanced results. \nThe Current Study \nIn our study, we utilized LLMs to extract data from several research papers as \npart of a systematic literature review in aging psychology (Martinelli et al., in progress). \nThis review sought to determine the extent to which examining lay perspectives on \nsuccessful aging has effectively addressed the critiques aimed at this area of study. The \nreview adopted a sophisticated theoretical-analytical framework for examining lay \nperspectives (Spini, 2024). Detecting key components of this framework in scienti�ic \ntexts poses a signi�icant challenge for humans and machines alike. Moreover, we took \nadvantage of the versatility of LLMs, by prompting the delivery of both quanti�iable data \n(i.e., decisions concerning the presence or absence of reviewed elements) and \nqualitative data (i.e., supplementing decisions with textual insights). These qualitative \ndata did not undergo further analysis in the current paper , but some examples can be \nread in Supplemental File 1. \nIn the current paper , we aimed to explore the potential of using LLMs for \nconducting systematic literature reviews in psychology. Moreover, we investigated \nwhether the results obtained from LLMs align with those produced through human \n   \n \n  6 \n \nefforts. We tackled this second question through a detailed analysis, which is presented \nin the data analysis and results section of this paper . Finally, we outlined practical \naspects of using LLMs for coding qualitative data in the methods and discussion section. \nMethods \nStudy Data \nIn line with the PRISMA guidelines, a three-step selection of studies led to the \ninclusion of 39 research articles in the systematic review, through the screening of titles \n(n = 5455), abstracts (n = 744) and full texts (n = 160). The included studies were peer-\nreviewed English written research articles on lay perspectives of successful aging, \npublished between 2012 and 2023, and including elderly individuals aged 60 or more. \nLLMs were prompted to evaluate elements of a theoretical-analytical framework (Spini, \n2024) within the aim and discussion sections of these research articles. Several \nelements were evaluated and then categorized into higher-order groups termed micro-, \nmeso-, and macro-level. Only the data concerning these higher-order groups were \nincluded in the analysis for the current paper. For comprehensive details on interpreting \nthose levels, we refer to Spini (2024), Martinelli and colleagues (in progress), and the \nprompt supplied to the model (see Supplemental File 2). Of the full set of 39 research \narticles, 15 were randomly selected and independently coded by two authors of this \npaper, enabling a comparison between LLM-coding and human coding. The analysis \npresented in the current paper focuses on this subset of 15 research articles. We \ncollected results in an excel �ile where each row corresponded to one research paper . \nThere was a dedicated column for the study aim section, a column for the discussion \nsection, and columns for each of the elements to be evaluated against these texts (i.e., 8 \nevaluation columns and 8 justi�ication columns for each study section). The cells in \n   \n \n  7 \n \nthese evaluation columns could take on values of either 1 (=presence) or 0 (=absence) \nof the elements in the corresponding text, while the cells in the justi�ication columns \nwere collecting textual examples that supported each decision.  \nHuman Coding \nTwo authors of this paper independently evaluated whether elements of the \ntheoretical-analytical framework were present or absent in the research articles sections \nunder review. In instances where an element was identi�ied, the coders provided a textual \nexample to support their decision (see examples in Supplemental File 1). Following the \nindependent coding, the two authors engaged in a discussion to reconcile any differences, \nresulting in a �inal uni�ied human coding. \nLLM Coding \nWe used LLMs to replicate the same coding task �irst performed by the study \nauthors, including giving textual examples to support decisions ( see examples in \nSupplemental File 1). Like the human process, the LLM coding was done twice. Since the \nLLM's context was reset for each text and for each evaluated element, the two instances \nof LLM coding can be regarded as independent.  Afterward, one of the authors examined \ninstances of LLM disagreement and resolved them, leading to a �inal uni�ied LLM coding \nfor comparison with human coding. To obtain the evaluation from LLMs for each text and \nelement, one of the authors  developed a series of Python scripts that executed several \nsequential steps. We will address these steps in detail and offer recommendations from \nour experience. \n    \n 1) Sequential Selection and Prompt Construction: The Python script reads the \nexcel �ile that lists the texts and elements for evaluation.  The script sequentially selects \n   \n \n  8 \n \nthe text and the element to be evaluated and combines them into a prompt. This prompt \nalso includes additional instructions explaining the theoretical-analytical framework and \nthe coding task (see Supplemental File 2). A critical aspect is prompt engineering. Indeed, \ngiven that generative AI tends to produce verbose responses, it is advisable to adjust \nmodel parameters (i.e., verbosity) and the prompt content  to constrain the model to \noutput either 0 or 1, followed by a separator and a textual justi�ication. C onstructing the \nright prompt is essential to obtaining results that permit further processing, necessitating \na prototype phase to �ine-tune the prompt. \n2) LLM call via OpenAI API  (see https://platform.openai.com/docs/overview): \nWith the openai Python package, we can call different LLMs. It is important to think about \nthis choice of model; in our experience, the most recent model at the time of this analysis \n(i.e., gpt-4-1106-preview) signi�icantly outperform s its predecessors in instruction \nadherence and contextual capacity ( 128’000 tokens for gpt-4-1106-preview, \ncorresponding to roughly 96’000 words). Indeed, LLMs have limited contextual capacity, \nand sending the prompt containing text and task information involves thoughtful \nstructuring of API calls. For lengthy texts, developing a chunking algorithm, like \nhierarchical summarization, might be necessary. In our case, the large contextual window \nof the chosen model  eliminated the need for dividing texts into smaller segments . \nHowever, note that this model is relatively costly ($0.01 / 1K tokens for input and $0.03 / \n1K tokens) and imposes stricter rate limits, which might extend the total processing time. \nThe cost and slower performance may be a limiting factor for larger analyses, despite the \nsuperior quality of outcomes. Prompts could be sent in batches for faster processing, but \nin our case, we opted for individual API calls , resetting the context for each single \nevaluation. \n   \n \n  9 \n \n     3) Result Processing: The entire process was repeated twice to allow the \naggregation of results  and establish ment of  consensus scores.  A consensus score of 1 \nindicated agreement across both repetitions on the presence of a topic, 0 denoted \nagreement on its absence, and 0.5 indicated a lack of consensus. This mirrors the human \ncoding practice of not relying on a single coder's results and provides crucial information \nto help understand the robustness of decisions made by the model . Finally, one of the \nauthors examined cases of no consensus, and implemented the rule that no consensu s \ndefaulted to absence (=0), therefore favoring selectivity over sensitivity.  This resulted in \na �inal uni�ied LLM coding that could be compared against the human evaluation.  \nData Analysis \nFor study aim s and discussion s separately, we calculated measures of coding \nconsistency for micro-, meso-, and macro-level coding, resulting from the aggregation of \ntwo (micro-) or three (meso - and macro -) constituting elements. We then aggregated \nthese levels to obtain a measure of general cross -level coding consistency, discussed in \nmore detail in the results section. We computed human -human, LLM-LLM, and �inal \nhuman-LLM consistencies. We used  two strongly related consistency assessment \nmethods (Ben -David, 2008), including the estimation of Cohen’s Kappa for inter -rater \nreliability (Cohen, 1960) and the measurement of the Area Under the ROC Curve (AUC; \ne.g., Provost & Fawcett, 1997).  \nInter-rater reliabilities (Cohen’s Kappa) between 0.60 and 0.79 were considered \nas moderate, between 0.80 and 0.90 as strong, and above 0.90 as almost perfect (McHugh, \n2012). Furthermore, coding consistency was considered fair when the AUC was between \n0.70 and 0.79, good when the AUC was between 0.80 and 0.89, and excellent when the \nAUC was 0.90 or above (Baird et al., 2013; Thornton & Laws, 2009).  \n   \n \n  10 \n \nResults \nTable 1 presents the general inter-rater consistency metrics, including Cohen’s \nkappa, false positive rate, sensitivity, speci�icity, accuracy, and AUC. Figure 1 shows the \nROCs curves. First, we analyzed the results for study aim sections. Between human \ncoders, Cohen’s k coef�icients showed strong agreement (k = 0.832). Concerning the two \niterations of coding of the LLM, Cohen’s k coef�icients revealed moderate agreement (k = \n770). Meanwhile, between the �inal human and LLM coding, the Cohen’s k values \ndisplayed strong agreement (k = 0.899). AUC indicated similar consistency patterns, with \nexcellent consistency for human coders (AUC = 0. 914), and good consistency for LLMs  \n(AUC = 0.894). When considering the �inal human and LLM coding, AUC showed excellent \nconsistency (AUC = 0.951). In the study discussion sections, inter-rater agreement was \nmoderate for humans  (k = 0.793) and for LLM s (k = 0.713). Meanwhile, agreement \nbetween the �inal human and LLM coding was strong ( k = 0.823). In addition, the AUCs \nfor inter-rater consistency were excellent for humans (AUC = 0.905), good for LLMs (AUC \n= 0.829), and excellent for the �inal human and LLM coding (AUC = 0.829).  When \nconsidering level -speci�ic inter -rater consistency, the pattern of results was mostly \nsimilar with agreement ranging from moderate to almost perfect (see Table 2).  \n \n  \n   \n \n  11 \n \nTable 1. General Inter-rater Consistency Metrics. \n  Study Aim Sections   Study Discussion Sections \n  Human   LLM   Human-\nLLM   Human   LLM   Human-\nLLM \nGeneral (N = 120) \nCohen’s k 0.832  0.77  0.899  0.793  0.731  0.823 \nFPa 3  14  4  4  12  6 \nFNb 7  0  2  7  0  3 \nTPc 49  54  51  75  85  79 \nTNd 61  52  63  34  23  32 \nFPRe 0.094  0.212  0.06  0.105  0.343  0.158 \nSensitivity 0.875  1  0.962  0.915  1  0.963 \nSpecificity 0.953  0.788  0.94  0.895  0.657  0.842 \nAccuracy 0.917  0.883  0.95  0.908  0.9  0.925 \nAUC 0.914  0.894  0.951  0.905  0.829  0.903 \nNote. aFalse Positive; bFalse Negative; cTrue Positive; dTrue Negative; eFalse Positive Rate = \nFP / (FP + TN); Sensitivity = TP / (TP + FN); Specificity = TN / (TN + FP); Accuracy = (TP \n+ TN) / (TP + TN + FP + FN). \n \n \nFigure 1. ROC for general inter-rater consistency for study aim sections (left panel) and \nstudy discussion asssessment (right panel). \n \n  \n\n   \n \n  12 \n \nTable 2. Level-Speci�ic Inter-rater Consistency Metrics. \n Study Aim Sections  Study Discussion Sections \n Human  LLM  Human-\nLLM \n Human  LLM  Human-\nLLM \nMicro-Level (N = 30) \nCohen’s k 0.762  0.710  0.902  0.870  1.000  0.714 \nFPa 2  3  0  1  0  2 \nFNb 0  0  1  0  0  0 \nTPc 24  22  23  25  27  25 \nTNd 4  5  6  4  3  3 \nFPRe 0.333  0.375  0.000  0.200  0.000  0.400 \nSensitivityf 1.000  1.000  0.958  1.000  1.000  1.000 \nSpecificityg 0.667  0.625  1.000  0.800  1.000  0.600 \nAccuracyh 0.933  0.900  0.967  0.967  1.000  0.933 \nAUC 0.833  0.812  0.979  0.900  1.000  0.800 \nMeso-Level (N = 45) \nCohen’s k 0.857  0.780  0.863  0.723  0.672  0.773 \nFP 0  5  3  2  5  2 \nFN 3  0  0  3  0  2 \nTP 15  20  17  30  33  31 \nTN 27  20  25  10  7  10 \nFPR 0.000  0.200  0.107  0.167  0.417  0.167 \nSensitivity 0.833  1.000  1.000  0.909  1.000  0.939 \nSpecificity 1.000  0.800  0.893  0.833  0.583  0.833 \nAccuracy 0.933  0.889  0.933  0.889  0.889  0.911 \nAUC 0.917  0.900  0.946  0.871  0.792  0.886 \nMacro-Level (N = 45) \nCohen’s k 0.725  0.706  0.886  0.779  0.674  0.866 \nFP 1  6  1  1  7  2 \nFN 4  0  1  4  0  1 \nTP 10  12  11  20  25  23 \nTN 30  27  32  20  13  19 \nFPR 0.032  0.182  0.030  0.048  0.350  0.095 \nSensitivity 0.714  1.000  0.917  0.833  1.000  0.958 \nSpecificity 0.968  0.818  0.970  0.952  0.650  0.905 \nAccuracy 0.889  0.867  0.956  0.889  0.844  0.933 \nAUC 0.841  0.909  0.943  0.893  0.825  0.932 \nNote. aFalse Positive; bFalse Negative; cTrue Positive; dTrue Negative; eFalse Positive Rate = \nFP / (FP + TN); Sensitivity = TP / (TP + FN); Specificity = TN / (TN + FP); Accuracy = (TP \n+ TN) / (TP + TN + FP + FN). \n  \n   \n \n  13 \n \nDiscussion \nThis study explored the potential of using LLMs for conducting systematic \nliterature reviews in psychology. To evaluate the quality of LLM -generated results, we \ndirectly compared them against those obtained through traditional human coding efforts. \nInterestingly, while LLM  outputs exhibited variability across iterations, this was not \ndramatically different from  the observed inconsistency between human coders . Inter -\nrater agreement reached acceptable levels for LLMs as well as humans in our analysis. \nCrucially, in comparing the �inal outcomes between humans and LLMs, we noted that the \nlevel of agreement was almost perfect, thus exceeding what we observed for humans or \nLLMs separately . This underscores the advantage of employing a multiple-coding \napproach for both humans and LLMs. This not only allows to calculate a con�idence level \nfor each code, but also results in a more consistent �inal coding when various c oding \niterations are combined. \nFurthermore, agreement between humans and LLMs was preserved across the \ndistinct analytical levels outlined in the theoretical -analytical framework ( i.e., micro-, \nmacro-, and meso -level). The agreement ranged from strong to almost perfect in study \naim sections, and from moderate to strong in discussion sections. The maintained coding \nquality across levels was crucial for the comparative analysis within our literature review \n(Martinelli et al., in progress). Taken together, the consistent coding observed between \nLLMs and humans, coupled with its stability across analytical levels, establishes coding \nvia LLMs  as a reliable method for systematic literature reviews in psychology. \nAdditionally, our analysis marks an initial step toward developing criteria for evaluating \nthe analytical quality of LLM-generated results.  \n \n   \n \n  14 \n \nPotential Advantages of Using LLMs  \nBased on our results, we propose a synergistic coding approach that combines \nthe capacity of LLMs with human expertise. This approach would use the rapid \nprocessing and cost ef�iciency of LLMs to produce various iterations of the results. These \niterations are then aggregated and compared against the �indings of a single human \ncoder, with a human arbitrator resolving any discrepancies. This combination has the \npotential to not only speed up the analysis process compared to needing several human \ncoders, but also improve the quality of the �inal outcomes compared to either method \nused alone.   \nThe generative AI used in our study offers many advantages over more \ntraditional Natural Language Processing (NLP) algorithms. It seamlessly handles \ncontent across multiple languages and can consider information from relatively large \ncontext windows. Moreover, while presenting challenges for managing its generative \ncapabilities, it can provide textual justi�ications for decisions, adding a layer of \ninterpretability to the analysis. Furthermore, the �lexibility of LLMs allows for the \ncustomization of tasks through prompts, offering a range of analytical possibilities. For \nexample, LLMs can be asked to express their con�idence in the presence of a topic within \na text using Likert Scales, choosing from options like 'De�initely not Present', 'Likely not \nPresent', 'Uncertain', 'Likely Present', and 'De�initely Present'. Incorporating con�idence \nscores generated by LLMs enables a more thorough analysis of how LLMs identify topics \nin text. Finally, because the use of LLMs can greatly support the process of data \nquanti�ication, it facilitates the application of advanced analytical techniques such as \ngraph analysis to explore the interconnectedness among topics within coded data. \n \n   \n \n  15 \n \nFeasibility of Integrating LLMs in Psychology Research \nOur experience con�irmed that LLMs produce high-quality results and can expand \nthe analysis possibilities within the context of psychological research.  In addition, our \nexperience testi�ies  to the feasibility of integrating LLMs into a psychology research \nwork�low. Detail s about LLM integration are addressed  in our methods section. To \nsummarize, w e found the integration process accessible, technically manageable, \n�inancially viable, and time-ef�icient, albeit with nuances worth mentioning. \nConcerning the topic of accessibility, it was fairly straightforward to subscribe to \nOpenAI and obtain an API key,  and this is available to the public at large. The LLM \nintegration process was technically easily manageable, by using the Python OpenAI \npackage. Financially, it was viable to integrate LLMs into our analysis, with a modest cost \namounting to approximately 20 USD for the full analysis in our literature review. As \ncosts are calculated per token, the total expense varies with the volume of text analyzed \nand the amount of generated content. While the cost of integrating LLMs might be a \nbarrier in some cases, especially with large volumes of data, affordability of LLM \ntechnology already compares favorably to other research tools, such as expensive \nstatistical analysis software licenses common in psychological research. For instance, \nNVivo, a widely used software for qualitative data analysis, is priced at €455 annually. \nTherefore, in typical research contexts, the cost of using LLMs does not currently seem \nprohibitive. Concerning time ef�iciency, the LLM completed our coding tasks \nsigni�icantly faster than the human coders, taking only a few hours compared to several \ndays. However, one needs to plan a preliminary phase to test the model on a subset of \nthe data to re�ine prompts and other processing aspects. We recommend initial testing \n   \n \n  16 \n \nwith cost-effective and fast models (e.g., ChatGPT 3.5 models), and then switching to \nChatGPT 4 preview or higher for a �inal analysis of superior quality.  \nLimitations of Integrating LLMs in Psychology Research \nData c on�identiality is  currently a key limitation for using LLMs in psychology \nresearch. Our integration of LLMs  entailed the automatic  sending of data to OpenAI for \nprocessing, raising concerns for sensitive or personal data. This complicates the analysis \nof data that contains con�idential or identi�iable information, such as primary qualitative \nresearch data. Depending on the situation, anonymization by removing identi�iable \ninformation before sending data to the model may be necessary. Alternatively, recent \nadvancements facilitate the use of  local models, like the Llama models \n(https://ollama.com/), which can be downloaded and run locally with dedicated Python \ntools, thus addressing con�identiality concerns. Finally, in speci�ic scenarios, obtaining \ninformed consent for AI analysis of narrative data from participants might be possible, \ncontingent upon receiving prior approval from an ethics committee. Naturally, data \ncollection must ensure that identi�iable information is separated from narrative data \nprior to AI analysis.  \nConclusion \nOur study encourages further research into LLM integration in psychological \nresearch, while assessing  both the strengths and potential limitations of these \ntechnologies, for example when dealing with more extensive or complex  datasets. \nConsidering the absence of a universally accepted benchmark for evaluating LLM outputs, \nit is critical to examine the differences between human and AI coding  in the �ield of \napplication. Furthermore, it is important for future studies in the �ield of psychology to \nidentify viable ways to adopt these technologies for the analysis of human narratives \n   \n \n  17 \n \ncontaining sensitive information. Most critically , it is necessary to understand how \nresearchers can work synergistically with  LLMs, by focusing on the researcher role in \nareas such as prompt engineering and decision- making. Optimizing how psychology \nresearchers make use of LLMs  will effectively connect the domains of psychology and \ntechnology. \n \n \n \n \n  \n   \n \n  18 \n \nReferences \nBen-David, A. (2008). About the relationship between ROC cuves and Cohen’s Kappa. \nEngineering Applications of Arti�icial Intelligence, 21, 874-882. \nhttps://doi.org/10.1016/j.engappaai.2007.09.009 \nBaird et al. (2013). A comparison of risk assessment instruments in juvenile justice. \nMadison, WI: National Council on Crime and Delinquency. \nCohen, J. (1960). A coef�icient of agreement for nominal scales. Educational and \nPsychological Measurement, 20, 37-46. \nhttps://doi.org/10.1177/001316446002000104 \nDe la Torre-Lo ́ pez, J., Ramı́rez, A., & Romero, J. R. (2023). Arti�icial intelligence to \nautomate the systematic review of scienti�ic literature. Computing, 105, 2171-\n2194. https://doi.org/10.1007/s00607-023-01181-x \nFecher, B., Heibing, M., Laufer, M., Pohle, J., & Sofsky, F. (2023). Friend or foe? Exploring \nthe implications of large language models on the science system. AI & Society, 1-\n13. https://doi.org/10.1007/s00146-023-01791-1 \nNashwan, A., & Jaradat, J. H. (2023). Streamlining Systematic Reviews: Harnessing Large \nLanguage Models for Quality Assessment and Risk-of-Bias Evaluation. Cureus, \n15(8), Article e43023. doi: 10.7759/cureus.43023 \nNaveed et al. (2023). A comprehensive overview of large language models. arXiv \npreprint. https://doi.org/10.48550/arXiv.2307.06435. \nMartinelli, P ., Uittenhove, K., Jopp, D. S., Spini, D., & Roquet, A. (in progress). Successful \nAging Criticisms and Lay Perspectives: A Large Language Model-Assisted \nSystematic Review (2012-2023) Using a Three-Level Analytical Framework.  \n   \n \n  19 \n \nMcHugh, M. L. (2012). Interrater reliability: the kappa statistic. Biochemia Medica, 22(3), \n276-282. doi:10.11613/BM.2012.031 \nMichelson, M., & Reuter , K. (2019). The signi�icant cost of systematic reviews and meta-\nanalyses: A call for greater involvement of machine learning to assess the \npromise of clinical trials. Contemporary Clinical Trails Communications, 16, Article \n100443. https://doi.org/10.1016/j.conctc.2019.100443 \nPage et al. (2021). The PRISMA 2020 statement: an updated guideline for reporting \nsystematic reviews. BMJ, 372(71), 1-9. http://dx.doi.org/10.1136/bmj.n71 \nPetersen, M., & van der Plas, L. (2023). Can language models learn analogical reasoning ? \nInvestigating training objectives and comparison to human performance. In \nProceedings of the 2023 Conference on Empirical Methods in natural Language \nProcessing, Singapore, Association for Computational Linguistics.  \nProvost, F ., & Fawcett, T . (1997). Analysis and visualization of classi�ier performance \nwith nonuniform class and cost distributions. In Proceedings of AAAI-97 \nWorkshop on AI Approaches to Fraud Detection & Risk Management (pp. 57-63). \nRaffel et al. (2023). Exploring the Limits of Transfer Learning with Uni�ied Text-to-Text \nTransformer. arXiv preprint. https://doi.org/10.48550/arXiv.1910.10683. \nQureshi et al. (2023). Are ChatGPT and large language models “the answer“ to bringing \nus closer to systematic review automation? Systematic Reviews, 12(72), 1-4. \nhttps://doi.org/10.1186/s13643-023-02243-z \nSchopow, N., Osterhoff, G., & Baur , D. (2023). Applications of the Natural Language \nProcessing Tool ChatGPT in Clinical Practice: Comparative Study and Augmented \n   \n \n  20 \n \nSystematic Review. JMIR Medical Informatics, 28(11), Article e48933. doi: \n10.2196/48933  \nSpini, D. (submitted). De�ining the meso-level as geosocial embeddedness: A new avenue \nfor the life course paradigm.  \nSuri, G., Slater , L. R., Ziaee, A., & Nguyen, M. (2024). Do large language models show \ndecision heuristics similar to humans? A case study using GPT-3.5. Journal of \nExperimental Psychology General. doi: 10.1037/xge0001547. \nThornton, D., & Laws, R. (2009). Cognitive Approaches to the Assessment of Sexual \nInterest in Sexual Offenders. John Wiley & Sons Ltd.  \nTo ̈ rnberg, P . (2023). How to use LLMs for Text Analysis. arXiv preprint. \nhttps://doi.org/10.48550/arXiv.2307.12106. \nUttley et al. (2023). The problems with systematic reviews: a living systematic review. \nJournal of Clinical Epidemiology, 156, 30-41. \nhttps://doi.org/10.1016/j.jclinepi.2023.01.011 \nVaswani et al. (2017). Attention is all you need. In: Advances in neural information \nprocessing systems (pp. 6000-6010). Red Hook: Curran Associates. ",
  "topic": "Psychology",
  "concepts": [
    {
      "name": "Psychology",
      "score": 0.580930233001709
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5774262547492981
    },
    {
      "name": "Epistemology",
      "score": 0.4600939154624939
    },
    {
      "name": "Cognitive science",
      "score": 0.39265337586402893
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3761291205883026
    },
    {
      "name": "Linguistics",
      "score": 0.36302876472473145
    },
    {
      "name": "Philosophy",
      "score": 0.14200946688652039
    },
    {
      "name": "History",
      "score": 0.11039960384368896
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97565354",
      "name": "University of Lausanne",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I4210144394",
      "name": "Czech Academy of Sciences, Institute of Psychology",
      "country": "CZ"
    },
    {
      "id": "https://openalex.org/I4210106484",
      "name": "Center for Health, Exercise and Sport Sciences",
      "country": "RS"
    }
  ],
  "cited_by": 1
}