{
    "title": "So-ViT: Mind Visual Tokens for Vision Transformer.",
    "url": "https://openalex.org/W3155420132",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2164507147",
            "name": "Jiangtao Xie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3153412741",
            "name": "Ruiren Zeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2139438904",
            "name": "Qilong Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2146660198",
            "name": "Ziqi Zhou",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2259148696",
            "name": "Peihua Li",
            "affiliations": [
                "Dalian University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2950094539",
        "https://openalex.org/W2502312327",
        "https://openalex.org/W1967722715",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2936614765",
        "https://openalex.org/W3123006928",
        "https://openalex.org/W1663973292",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W1584270973",
        "https://openalex.org/W2237132896",
        "https://openalex.org/W1978601068"
    ],
    "abstract": "Recently the vision transformer (ViT) architecture, where the backbone purely consists of self-attention mechanism, has achieved very promising performance in visual classification. However, the high performance of the original ViT heavily depends on pretraining using ultra large-scale datasets, and it significantly underperforms on ImageNet-1K if trained from scratch. This paper makes the efforts toward addressing this problem, by carefully considering the role of visual tokens. First, for classification head, existing ViT only exploits class token while entirely neglecting rich semantic information inherent in high-level visual tokens. Therefore, we propose a new classification paradigm, where the second-order, cross-covariance pooling of visual tokens is combined with class token for final classification. Meanwhile, a fast singular value power normalization is proposed for improving the second-order pooling. Second, the original ViT employs the naive embedding of fixed-size image patches, lacking the ability to model translation equivariance and locality. To alleviate this problem, we develop a light-weight, hierarchical module based on off-the-shelf convolutions for visual token embedding. The proposed architecture, which we call So-ViT, is thoroughly evaluated on ImageNet-1K. The results show our models, when trained from scratch, outperform the competing ViT variants, while being on par with or better than state-of-the-art CNN models. Code is available at this https URL",
    "full_text": "SoT: Delving Deeper into Classiﬁcation Head for Transformer\nJiangtao Xie1‡, Ruiren Zeng1‡, Qilong Wang2, Ziqi Zhou3, Peihua Li1§\n1 Dalian University of Technology,2 Tianjin University,3 MEGVII Technology\nAbstract\nTransformer models are not only successful in natural\nlanguage processing (NLP) but also demonstrate high po-\ntential in computer vision (CV). Despite great advance,\nmost of works only focus on improvement of architectures\nbut pay little attention to the classiﬁcation head. For years\ntransformer models base exclusively on classiﬁcation token\nto construct the ﬁnal classiﬁer, without explicitly harnessing\nhigh-level word tokens. In this paper, we propose a novel\ntransformer model called second-order transformer (SoT),\nexploiting simultaneously the classiﬁcation token and word\ntokens for the classiﬁer. Speciﬁcally, we empirically dis-\nclose that high-level word tokens contain rich information,\nwhich per se are very competent with the classiﬁer and\nmoreover, are complementary to the classiﬁcation token. To\neffectively harness such rich information, we propose multi-\nheaded global cross-covariance pooling with singular value\npower normalization, which shares similar philosophy and\nthus is compatible with the transformer block, better than\ncommonly used pooling methods. Then, we study compre-\nhensively how to explicitly combine word tokens with clas-\nsiﬁcation token for building the ﬁnal classiﬁcation head.\nFor CV tasks, our SoT signiﬁcantly improves state-of-the-\nart vision transformers on challenging benchmarks includ-\ning ImageNet and ImageNet-A. For NLP tasks, through ﬁne-\ntuning based on pretrained language transformers includ-\ning GPT and BERT, our SoT greatly boosts the performance\non widely used tasks such as CoLA and RTE. Code will be\navailable at https://peihuali.org/SoT.\n1. Introduction\nIn the past years transformer models have been very suc-\ncessful in the ﬁeld of natural language processing (NLP) [7,\n33, 41]. The transformer architecture, solely based on at-\ntention mechanisms, can naturally model long-range depen-\ndency of tokens and learn contextual knowledge. In con-\ntrast, the convolutional neural networks (CNNs) [22, 49]\nlack such capability as the convolutions are fundamen-\n‡These authors contributed equally to this work.\n§Corresponding author. Email: peihuali@dlut.edu.cn\nVision\nTransformer\nDeiT-T [40] T2T-7 [51] T2T-14 [51]\nIN IN-A IN IN-A IN IN-A\nClassT 72.2 7.3 71.7 6.1 81.5 23.9\nWordT 77.9 5.7↑ 15.58.2↑ 73.72.0↑ 7.81.7↑ 82.10.6↑ 26.6 2.7↑\nClassT+WordT 78.66.4↑ 17.510.2↑ 74.52.8↑ 8.12.0↑ 82.61.1↑ 27.13.2↑\nLanguage\nTransformer\nGPT [33] BERT-base [7] BERT-large [7]\nCoLA RTE CoLA RTE CoLA RTE\nClassT 54.3 63.2 54.8 67.2 60.6 73.7\nWordT 56.1 1.8↑ 65.01.8↑ 56.41.6↑ 69.01.8↑ 61.40.8↑ 74.40.7↑\nClassT+WordT 57.33.0↑ 65.42.2↑ 58.03.2↑ 69.32.1↑ 61.81.2↑ 75.11.4↑\nTable 1. Accuracies (%) of transformer models which use sin-\ngle classiﬁcation token (ClassT), single word tokens (WordT) and\ntheir combination (ClassT+WordT). We showcase performance of\nvision transformers (i.e., DeiT and T2T) on ImageNet (IN) [6] and\nImageNet-A (IN-A) [13], and that of language transformers (i.e.,\nGPT and BERT) on CoLA [46] and RTE [2]. The results indicate\nword tokens per se are very competent with classiﬁer and more-\nover, are complementary to the classiﬁcation token. See Sec. 4 for\ndetails and for diverse results.\ntally local operations. The success of transformer mod-\nels has attracted great interests of computer vision (CV)\nresearchers [1, 11, 21, 45]. Recently, a vision transformer\nmodel called ViT [8] is proposed entirely based on a stack\nof transformer blocks, which has matched or outperformed\nstate-of-the-art CNNs when pre-trained on ultra large-scale\ndatasets of ImageNet-21K [6] or JFT-300M [38]. There-\nafter, a number of pure transformer models, e.g., [44, 51,\n52], have been proposed to improve the architecture of\ntransformers, reporting impressive performance gains when\ntrained from scratch on ImageNet with 1K classes [6].\nDespite great advance, for years pure transformer models\ninvariably build ﬁnal classiﬁer exclusively based on classiﬁ-\ncation token, without explicitly harnessing high-level word\ntokens. Although the classiﬁcation token interacts with all\nword tokens * through the attention mechanisms across the\nnetwork backbone, we conjecture the high-level word to-\nkens by themselves contain rich information that the classi-\nﬁcation token fails to accommodate. Therefore, exploiting\nonly the classiﬁcation token but excluding the word tokens\nfrom the classiﬁer limits transformer models. Actually, we\n*For brevity, we use the notation of “word” token for both NLP and CV\ntasks, which, for the latter case, indicates image patch.\narXiv:2104.10935v2  [cs.CV]  18 Dec 2021\nempirically ﬁnd that rich information inherent in word to-\nkens per se is very competent with classiﬁer and moreover,\nis complementary to the classiﬁcation token.\nAs show in Tab. 1, the experimental results on both CV\nand NLP tasks show that classiﬁcation head solely based\non word tokens (i.e., WordT) is often better than that based\non single classiﬁcation token (i.e., ClassT), and combina-\ntion of WordT and ClassT further boosts the classiﬁcation\naccuracy. Based on the empirical observations above, we\nrethink classiﬁcation head for transformer, and propose a\nnovel transformer model, namely second-order transformer\n(SoT), to exploit simultaneously classiﬁcation token and\nword tokens for the ﬁnal classiﬁer. To this end, there exist\ntwo key issues to be tackled: (1) how to effectively aggre-\ngate the word tokens to fully explore their rich information;\n(2) how to explicitly combine word tokens with classiﬁca-\ntion token for building the ﬁnal classiﬁcation head.\nFor effective token aggregation, we propose a multi-\nheaded global cross-covariance pooling (MGCrP) method,\nwhich learns a group of second-order, cross-covariance rep-\nresentations. Previous works [24,42] have shown that struc-\ntured normalization plays an important role for the second-\norder representations. Unfortunately, existing structured\nnormalization methods are not applicable to our MGCrP as\nit produces asymmetric matrices. Therefore, we present a\nsingular value power normalization (svPN) method in light\nof the overall statistical analysis of data; moreover, an ap-\nproximate, fast svPN algorithm is developed for guarantee-\ning efﬁciency. MGCrP shares similar philosophy and so is\nmore consistent with the transformer block, clearly better\nthan global average pooling (GAP) [12, 25, 39] and global\ncovariance pooling (GCP) [26, 42] widely used in CNNs.\nTo build the classiﬁcation head by combining word to-\nkens with classiﬁcation token, we propose three early fu-\nsion schemes, which combine the two kinds of token rep-\nresentations through operations such as concatenation and\nsum, along with one late fusion scheme which integrates\nindividual classiﬁcation scores. These schemes are illus-\ntrated in Fig. 2 and their comparisons are given in Tab. 3b;\namong them, the sum scheme performs best. Note that the\nproposed classiﬁcation head is architecture-agnostic, which\ncan be seamlessly integrated into a wide variety of vision\ntransformers and language transformers. To verify the ef-\nfectiveness of our SoT, we conduct experiments on both CV\nand NLP tasks.\nOur contributions are summarized as follows.\n• We dig into the transformer’s classiﬁcation head, ﬁnd-\ning that word tokens per se are very competent with\nclassiﬁer and moreover, is complementary to classi-\nﬁcation token. Based on this, we propose a second-\norder transformer (SoT) model. As far as we know, our\nSoT makes the ﬁrst attempt to exploit simultaneously\nclassiﬁcation token and word tokens for classiﬁcation,\nwhich can span and beneﬁt both CV and NLP tasks.\n• We propose multi-headed global cross-covariance\npooling with structured normalization for mining word\ntokens, while systemically studying several schemes to\ncombine word tokens with classiﬁcation token. As a\nresult, we achieve a novel classiﬁcation head, which is\nvery effective and suitable for a variety of transformer\narchitectures.\n• We perform thorough ablation study to validate our\nSoT. Extensive experiments show our SoT can signiﬁ-\ncantly improve vision transformers on challenging Im-\nageNet and ImageNet-A. Meanwhile, our SoT is very\nhelpful to language transformers such as BERT and\nGPT, performing better than its conventional counter-\npart on General Language Understating tasks.\n2. Related Works\nTransformer architectures Solely based on attention\nmechanisms without any recurrence and convolutions, the\ntransformer models [41] can naturally model long-range de-\npendencies and global context, outperforming LSTM [15]\nand CNN [9] on NLP tasks. The transformer models pre-\ntrained on large-scale unlabeled corpus (e.g., GPT [33, 34]\nand BERT [7]) are able to learn powerful language repre-\nsentations, and, after ﬁne-tuning, achieve state-of-the-art\nresults for a wide range of downstream language tasks [7,\n29, 33, 34]. For CV tasks, Dosovitskiy et al. [8] present a\npure transformer architecture (i.e., ViT) which reports very\npromising performance when pre-trained on ultra large-\nscale datasets. The works that follow greatly improve over\nViT. DeiT [40] proposes an extra distillation token to trans-\nfer knowledge from teacher models. T2T-ViT [51] and PS-\nViT [52] focus on better tokenization of vision patches.\nSwin Transformer [30] and PVT-T [44] introduces hierar-\nchical structure into the transformer. Conformer [32] is a\ndual architecture which combines CNN with transformer.\nThe pure transformer architectures, either in NLP or CV\nﬁeld, only use the classiﬁcation token for the classiﬁer, lim-\niting the performance of models. As such, we propose a\nnovel classiﬁcation head, which exploits simultaneously the\nclassiﬁcation token and word tokens.\nSecond-order pooling GCP, also known as bilinear pool-\ning, often produces symmetric positive deﬁnite (SPD) ma-\ntrices as global representations [17, 28]. It can capture\nsecond-order relations, performing much better than GAP\nwhich only estimates ﬁrst-order statistics [26,42]. Research\nhas shown that normalization techniques are very helpful to\nthe second-order representations [17, 24, 28, 50]. Element-\nwise normalization improves GCP but fails to consider\nholistic structure of data [28]. The structured normaliza-\ntion such as matrix power normalization (MPN) [24,27,42]\nexploits the overall statistical structure and geometric struc-\nture of covariance matrices, and has greatly beneﬁted GCP.\nAs MPN depends on GPU-unfriendly eigen-decomposition,\niSQRT [23] proposes a fast method for computing ma-\ntrix square root, suitable for parallel implementation on\nGPU. A recent study [43] has shown that GCP with MPN\nimproves Lipschitzness of the loss function, leading to\nfast convergence and robustness to distorted images. Dif-\nferent from GCP, we propose multi-headed global cross-\ncovariance pooling, which shares similar philosophy and\nis consistent with transformers; furthermore, we propose a\nnew structured method to normalize cross-covariance ma-\ntrices.\n3. Second-order Transformer (SoT)\nIn this section, we ﬁrst describe the SoT network,\nfollowed by the proposed multi-headed global cross-\ncovariance pooling method. Finally, we introduce the nor-\nmalization method for cross-covariance matrices.\n3.1. SoT Network\nWe illustrate our SoT network in Fig. 1. Similar to [52],\nwe develop a small, hierarchical module for token embed-\nding, which, based on off-the-shelf convolutions, gradu-\nally reduces spatial size of feature maps. In designing\nthe token embedding module, we evaluate varying type of\nconvolution blocks including ResNet block [12], Inception\nblock [39], DenseNet block [16] and Non-local block [45].\nThis token embedding method has capability to model lo-\ncal image properties, which ViT [8] lacks due to its naive\nembedding method.\nThe features produced by the token embedding module\nare reshaped to a sequence of vectors as word tokens. As\nin [8], we prepend a learnable classiﬁcation token to the\nword token sequence, and then position embeddings are\nadded. The modiﬁed token sequence is then fed to the back-\nbone network, which consists of a stack of standard trans-\nformer blocks, each containing a multi-head self-attention\nand a multi-layer perception.\nFor classiﬁcation head, unlike the conventional trans-\nformers, we explicitly combine the word tokens with the\nclassiﬁcation token. We investigate different fusion meth-\nods to combine two kinds of tokens, ﬁnding the sum scheme\nperforms best which, therefore, is adopted in our SoT. We\ndesign a family of transformers of varying depths, including\na 12-layer SoT-Tiny, a 14-layer SoT-Small and a 24-layer\nSoT-Base; in addition, we design a 7-layer SoT for the sake\nof ablation study. The details on these models as well as\non the token embedding module are provided in the supple-\nment A.\n3.2. Proposed Classiﬁcation Head\nThe conventional pure transformer models only use clas-\nsiﬁcation token for the ﬁnal classiﬁer. As high-level word\nBackbone of transformer\nToken embedding\nClassificationtokenWordtokenPositiontoken\nClassifier\nLinear\nSingular value power normalizedcross-covariance pooling\nLinearLinearLinear\nSingular value power normalizedcross-covariance pooling\nConcatenation\nLinear\nMGCrPProposedclassificationhead\nFigure 1. Diagram of our SoT network. Given an input image,\nthe token embedding module produces a sequence of word tokens,\nwhich is then prepended by a classiﬁcation token. After added by\nposition tokens, the sequence of tokens is subject to the backbone\nconsisting of a stack of standard transformer blocks. Finally, the\nclassiﬁcation token and the word tokens output from the backbone\nare fed to the proposed classiﬁcation head.\ntokens contain rich information, neglect of them leads to in-\nformation loss. So we propose to combine the word tokens\nwith the classiﬁcation token for the ﬁnal classiﬁer.\nWe introduce three early fusion schemes (i.e., sum,\nconcat and aggr all) and a late fusion scheme (i.e., late).\nFig. 2 illustrates these schemes, where aLinear transforma-\ntion is equivalent to (and thus denoted by) a fully-connected\n(FC) layer. For thesum scheme, the classiﬁcation token and\naggregated word tokens are separately connected to a FC\nlayer and are then summed, before fed to the softmax clas-\nsiﬁer. In the concat scheme, the representations of classi-\nﬁcation token and the aggregated word tokens are concate-\nnated, followed by a FC layer and then a softmax classiﬁer.\nFor the aggr all scheme, we directly aggregate all tokens\nincluding both the classiﬁcation token and the word tokens,\nand then connect the resulting representation to a FC layer\nsucceeded by a softmax classiﬁer. In the late fusion scheme,\nthe classiﬁcation token and the word tokens are indepen-\ndently attached to a FC layer and a softmax classiﬁer, and\nﬁnally the two classiﬁcation scores are added.\nLet {zi ∈ Rp,i = 0 ,1,...,q }denote the features\nof all tokens, where z0 indicates the classiﬁcation token\n+Classifier\nLinear\nWordtokens\nPoolingLinear\nClassificationtoken\n(a) Sum\nLinear\nWordtokensClassificationtoken\nConcatPooling\nClassifier (b) Concat\nPooling\nWordtokensClassificationtoken\nLinear\nClassifier\n(c) Aggr all\nClassificationtoken\nLinear\nClassifier+\nWordtokens\nPoolingLinear\nClassifier (d) Late\nFigure 2. Fusion schemes designed for our classiﬁcation head.\nwhile the remaining ones indicate word tokens. We con-\ncatenate the features of word tokens to form a matrix Z =\n[z1,z2,··· ,zq] ∈Rp×q. The four fusion schemes can be\nformulated as\nsum : softmax\n(\nFC(z0) + FC(pool(Z)\n)\n(1)\nconcat : softmax\n(\nFC\n([\nz0,pool(Z)\n]))\naggr all : softmax\n(\nFC(pool([z0,Z])\n)\nlate : softmax\n(\nFC(z0)\n)\n+ softmax\n(\nFC(pool(Z))\n)\nHere pool(·) denotes a pooling function. The commonly\nused pooling functions are global average pooling (GAP)\nand Global covariance pooling (GCP), which, however, are\ndeveloped for CNN architecture and may be sub-optimal for\nthe transformer. As described in [41, Sec. 3.2.2], the multi-\nhead structure in the transformer block facilitates modeling\ninformation from different representation space. Inspired\nby this, we propose multi-headed global cross-covariance\npooling (MGCrP) with structured normalization, as illus-\ntrated in Fig. 1 (top-right).\nIn the following, we ﬁrst introduce cross-covariance\npooling with single head. Given an input matrix Z, we per-\nform two separate linear transformations, obtaining X =\nWZ and Y = RZ, where W ∈Rm×p and R ∈Rn×p\nare learnable weight matrices. Then we compute cross-\ncovariance matrix Q = 1\nqXYT. Previous works have\nshown that structured normalization plays an important\nrole for GCP. However, as Q is asymmetric (square or\nnon-square), existing normalization method for SPD matri-\nces [42] is not applicable. Thus we propose a new structured\nnormalization method, called singular value power normal-\nPooling\nmethod Formula Matrix\nproperty\nStructured\nNorm\nMulti-\nhead\nGAP 1\nq Z1 Vector N/A \u0017\nGCP MPN\n(\n1\nq ZZT\n)\nSPD MPN \u0017\nMGCrP\n(ours)\n[\nsvPN\n(\n1\nq W1ZZT RT\n1\n)\n,...,\nsvPN\n(\n1\nq WhZZT RT\nh\n)] Asym svPN \u0013\nTable 2. Differences of our MGCrP from the classical pool-\ning methods. GAP produces ﬁrst-order, vectorial representa-\ntions; GCP produces symmetric positive deﬁnite (SPD) matrices to\nwhich matrix power normalization (MPN) is applicable; MGCrP\nyields asymmetric matrices, normalized by the proposed singular\nvalue power normalization (svPN).\nization (i.e., svPN), details of which are deferred to next\nsection. Now we can deﬁne our global cross-covariance\npooling:\nGCrP(Z) = svPN\n(1\nqWZZTRT\n)\n(2)\nWe continue to equip our cross-covariance pooling with\nmulti-head structure:\nMGCrP(Z) =\n[\nGCrP1(Z),··· ,GCrPh(Z)\n]\nwhere GCrPi(Z) = svPN\n(1\nqWiZZTRT\ni\n)\n(3)\nHere [·] denotes concatenation operation; GCrPi(Z) de-\nnotes the cross-covariance matrix of the i-th head, and Wi\nand Ri are two learnable linear projections.\nDifferences among GAP, GCP and MGCrP are summa-\nrized in Tab. 2, where 1 in the formula of GAP denotes a\nq-dimensional vector each component of which is 1, and we\nuse state-of-the-art MPN [24] for GCP. Performance com-\nparison among them is presented in Tab. 3b. Ablation anal-\nysis on MGCrP is given in Sec. 4.2.2.\n3.3. Normalization of Cross-covariance Matrix\nWe propose singular value power normalization (svPN)\nfor cross-covariance matrices. As svPN is computationally\nexpensive, we further develop a fast approximate algorithm.\n3.3.1 Singular Value Power Normalization\nOur method is motivated by MPN [42], an effective method\nfor normalizing any covariance matrix P = ZZT † that is\nSPD. This normalization consists in computation of power\nof eigenvalues aligned with eigenvectors of P. In terms of\nprincipal component analysis [3, Chap. 12], the eigenval-\nues of P in decreasing order amounts to from maximum to\n†Without loss of generality, in Sec. 3.3, we omit the constant1/qin repre-\nsenting a covariance matrix or cross-covariance matrix for simplicity.\nminimum variances of Z , while the corresponding eigen-\nvectors of P characterize the principal components. There-\nfore, MPN can be interpreted as shrinking these variances\naligned with the principal components.\nLet us consider uTXYTv where u ∈ Rm,v ∈ Rn,\nwhich indicates the cross-covariance between projections of\nX on u and those of Y on v. For simpliﬁcation, we denote\nQ = XYT. Actually, we have a proposition about such\ncross-covariances.\nProposition 1. Given ui and vi where i = 1,...,k −1,\nlet us consider the objective\nmax\n∥u∥=∥v∥=1\nuTXYTv (4)\ns.t. uT\ni u = 0, vT\ni v = 0, i<k.\nBy inductively optimizing (4) for k = 1,..., min(m,n),\nwe can obtain, in decreasing order, the k-th largest cross-\ncovariance uT\nkXYTvk, which is equal to the k-th singular\nvalue λk of Q while uk and vk are the corresponding left\nand right singular vectors, respectively.\nIn light of this proposition, we can deﬁne our normaliza-\ntion by shrinking the cross-covariances between X and Y\naligned with the left and right singular vectors of Q:\nsvPN(Q) =\nmin(m,n)∑\ni=1\nλα\ni uivT\ni , (5)\nwhere 0<α<1. Our svPN can be performed accurately via\nSVD, which, however, is computationally expensive, as the\nSVD algorithm is GPU-unfriendly [23]. We give a proof of\nProposition 1 and the backpropagation of svPN via SVD in\nthe supplement B and C, respectively.\n3.3.2 Fast Approximate Normalization Algorithm\nBased on low-rank assumption widely used in machine\nlearning [10], we can efﬁciently implement approximate\nnormalization by only estimating few largest singular val-\nues. We use an iterative method to consecutively estimate\nthe singular values. Given an initial vectorv(0), the iterative\nprocedure takes the following form [37]:\nu(j+1) = Qv(j)\n∥Qv(j)∥, v(j+1) = QTu(j+1)\n∥QTu(j+1)∥ (6)\nwhere the superscript j denotes the j-th iteration. After a\nfew iterations, we obtain approximately the largest singular\nvalue ˆλ1 = ∥QTu(j+1)∥and the corresponding left singu-\nlar vector ˆu1 = u(j+1) and right one ˆv1 = v(j+1).\nSuppose we have the k-th (k≥1) largest singular value,\nwe deﬂate matrix Q to obtain\nQ′= Q −\nk∑\ni=1\nˆλiˆuiˆvT\ni (7)\nFor Q′, we can perform iteration with Eq. (6) to achieve ap-\nproximately the (k+ 1)-th largest singular value ˆλk+1 and\nthe corresponding singular vectors ˆuk+1 and ˆvk+1. The de-\nﬂation (7) and the iteration (6) can be repeated. Given r\nlargest singular values, we deﬁne the approximate normal-\nization as\nˆsvPN(Q)=\nr−1∑\ni=1\nˆλα\ni ˆuiˆvT\ni + 1\nˆλ1−αr\n(\nQ−\nr−1∑\ni=1\nˆλiˆuiˆvT\ni\n)\n(8)\nIt shrinks the 1st to the (r−1)-th singular values aligned\nwith the corresponding singular vectors, while shrinking the\nremaining ones with the r-th largest singular value. Note\nthat ˆsvPN(Q) reduces to Q/ˆλ1−α\n1 if we only use the largest\nsingular value.\n4. Experiments\nWe ﬁrst introduce experimental setting in Sec. 4.1. Then\nwe evaluate the proposed methods for computer vision\n(CV) tasks and natural language processing (NLP) tasks in\nSec. 4.2 and Sec. 4.3, respectively. We train models with 8\nNVIDIA 2080Ti GPUs based on PyTorch framework. Our\ncode will be open-sourced after acceptance.\n4.1. Experimental Setting\nHere we brieﬂy describe the benchmarks and training\nstrategy for both CV and NLP tasks. Details on benchmark\nstatistics, task description and hyper-parameters setting are\ngiven in the supplement D.\nDatasets For CV tasks, we evaluate on ILSVRC ImageNet\nbenchmark [6, 36], which has 1.28M images for training\nand 50K images for test. Furthermore, we adopt a more\nchallenging dataset (i.e., ImageNet-A [13]) for evaluation,\nwhich consists of real-world adversarial images, involving\nheterogeneous and varied distribution shift.\nFor NLP tasks, following the common practice [7, 33],\nwe ﬁne-tune the transformer models pre-trained in an unsu-\npervised manner on large-scale corpus on four downstream\ntasks, i.e., Corpus of Linguistic Acceptability (CoLA) [46],\nRecognizing Textural Entailment (RTE) [2], Multi-Genre\nNatural Language Inference (MNLI) [47] and Stanford\nQuestion Answering (QNLI) [35].\nTraining strategy In image classiﬁcation, we train our SoT\nmodels on ImageNet from scratch. Besides scale, color\nand ﬂip augmentations [12, 20], following [51], we also\nadopt mixup [54], randAugment [5], cutmix [53], and la-\nbel smoothing [39]. We use AdamW [31] algorithm with\nwarmup for network optimization and cosine annealing\nschedule for learning rate. Detailed training strategies about\noptimizers, hyper-parameters, etc., are given in the sup-\nplement D. Also, we present in detail the hyper-parameter\nsetting on natural language processing tasks in the supple-\nment D.\n4.2. Image Classiﬁcation for CV Tasks\nTo make our extensive evaluations computationally fea-\nsible on ImageNet, in Sec. 4.2.1 and Sec. 4.2.2, we use\nthe 7-layer SoT model; besides, we re-scale the images\nsuch that their short sizes are 128 and 112 ×112 patches\nare cropped as network inputs. In Sec. 4.2.3, we com-\npare with state-of-the-art methods using standard protocol\non ImageNet.\n4.2.1. How Does Our Classiﬁcation Head Perform?\nWe ﬁrst establish a strong baseline model that only uses the\nclassiﬁcation token. Based on this strong baseline, we com-\npare different fusion schemes and pooling methods.\nBaseline based on conventional classiﬁcation head Sev-\neral works [51, 52] improve the simple embedding method\nof ViT [8] (i.e., a naive linear projection of ﬁxed patches).\nSpeciﬁcally, T2T [51] introduces soft-split operations\ncalled Tokens-to-token and PS-ViT [52] proposes a progres-\nsively sampling strategy built upon a convolution stem to\nlearn the local structure of images. In contrast, we design\na small, hierarchical module based on off-the-shelf convo-\nlutions for token embedding, and evaluate various types of\nconvolution blocks. Tab. 3a compares these baseline mod-\nels where only classiﬁcation token is used for the classiﬁer.\nIt can be seen that both T2T and PS-ViT clearly improve\nover ViT, while all of our models perform much better than\nthe two variants while having comparable parameters and\nFLOPs. Our embedding module with DenseNet block ob-\ntains the best result (73.13%), establishing a strong base-\nline. This token embedding module will be used across our\nfamily of transformer models.\nEffect of our classiﬁcation head Tab. 3b evaluates the pro-\nposed classiﬁcation head on the basis of the strong baseline.\nWe adopt iterative square-root normalization (iSQRT) [23]\nfor GCP which is a fast version of matrix power normal-\nization (MPN) [24]. For our MGCrP, we use ˆsvPN with\nthe single largest singular value. According to the results\nin Tab. 3b, we have two observations. (1) Fundamentally,\nall fusion schemes improve the baseline whatever the pool-\ning function is, which indicates that explicit combination\nof the word tokens indeed beneﬁts the transformer models.\nAmong the fusion methods, the aggr all scheme is supe-\nrior to the late scheme which only slightly improves the\nbaseline, while both of the sum scheme and concat scheme\nperform much better than the other two fusion methods. (2)\nThe second-order pooling outperforms the ﬁrst-order pool-\ning by a large margin regardless of fusion method, which is\nconsistent with previous conclusion drawn under the CNN\narchitectures [26, 42]. For any fusion method, our MGCrP\nperforms better than GCP by about 0.5%, suggesting that\nour multi-headed cross-covariance pooling has more power-\nModel Token embedding Top-1\n(%)\nParams\n(M)\nFLOPs\n(G)\nBaseline\n(classiﬁcation\ntoken only)\nNaive linear proj [8] 66.25 3.98 0.73\nTokens-to-token [51] 68.30 4.25 0.81\nProgressive sampling [52] 70.48 4.13 0.90\nResNet block (ours) 72.51 4.28 0.88\nInception block (ours) 71.61 4.09 0.81\nDenseNet block (ours) 73.13 4.23 1.06\nNon-local block (ours) 71.45 4.17 0.89\n(a) Results of baselines using conventional classiﬁcation head.\nFusion scheme Pool method Repr. size Top-1 (%) Params (M)\nBaseline 256 73.13 4.23\n(classiﬁcation token only) 1280 73.43 5.57\naggr all\nGAP 256 73.56 4.22\nGCP 1176 74.12 5.15\nMGCrP 1176 74.74 5.20\nconcat\nGAP 512 73.84 4.73\nGCP 1432 75.37 5.66\nMGCrP 1432 75.85 5.71\nsum\nGAP 256 73.85 4.47\nGCP 1176 75.23 5.40\nMGCrP 1176 75.97 5.44\nlate\nGAP 256 73.27 4.47\nGCP 1176 73.46 5.40\nMGCrP 1176 73.98 5.46\n(b) Results using our classiﬁcation head.\nTable 3. Evaluation of proposed classiﬁcation head. (a) We build\na strong baseline which only uses the classiﬁcation token. (b)\nWe compare different fusion schemes along with pooling meth-\nods, based on the strong baseline.\nful representation capability. We note that the sum scheme\nwith MGCrP obtains the highest accuracy of 75.97%.\nThe second-order pooling enlarges the representation\nsize (Repr. size), leading to more parameters than the base-\nline. For fair comparison, for the baseline model, we add a\nlinear projection layer after the classiﬁcation token, increas-\ning its dimension to 1280; as a result, its accuracy increases,\nbut only slightly (0.3%, 2nd row in Tab. 3b), which is still\nmuch lower than the fusion method. This suggests that the\nperformance increase of the fusion methods is mainly at-\ntributed to more powerful representation capability rather\nthan capacity growth. Note that all our fusion methods bring\nnegligible increase of FLOPs, compared to the baseline.\n4.2.2. Ablation Study of MGCrP and Normalization\nFor the MGCrP module, we ﬁrst evaluate the number of\nheads and representation size. After that, we assess the ex-\nact normalization (i.e., svPN) against the approximate one\n(i.e., ˆsvPN). Finally, we compare with other normalization\nmethods for cross-covariance matrices.\nNumber of heads and representation size The represen-\ntation size (Repr. size) of MGCrP is equal to h×m×n,\nwhere h, mand nare number of heads, and dimensions of\nh 1 2 4 6 8\n(m,n) (32,32) (24,24) (16,16) (14,14) (12,12)\nTop-1 (%) 75.14 75.36 75.24 75.97 75.37\n(a) Effect of head number hgiven ﬁxed Repr. size (∼1K).\nRepr. size 0.5K 1K 2K 3K 6K\n(m,n) (9,9) (14,14) (18, 18) (24,24) (32,32)\nTop-1 (%) 74.38 75.97 76.24 76.73 77.01\n(b) Effect of Repr. size given ﬁxed head number (h= 6).\nMethod Setting Top-1\n(%)\nSpeed\n(Hz)\nsvPN α\n0.3 75.74\n1100.5 76.13\n0.7 75.45\nˆsvPN (#sv,#iter)\nα=0.5\n(1,1) 75.97 2226\n(1,3) 75.82 2207\n(1,5) 74.11 2188\n(2,1) 73.51 2207\n(3,1) 74.89 2188\n(c) Exact normalization versus approximate one.\nMethod Top-1 (%) Speed (Hz)\n– 74.82 2248\n1/τ 75.13 0.31↑ 2226\nEPN [28] 73.29 1.53↓ 2206\nLN [18] 75.25 0.43↑ 2245\nˆsvPN 75.97 1.15↑ 2226\nsvPN 76.13 1.31↑ 110\n(d) Comparison with different normal-\nization methods.\nTable 4. Ablation analysis of MGCrP and normalization.\ntwo linear projections, respectively. Exhaustive grid search\nfor these hyper-parameters of MGCrP is computationally\nprohibitive. For simpliﬁcation, we set m = n, and search\nfor optimal hby ﬁxing the Repr. size, followed by evalua-\ntion of the Repr. size with ﬁxed number of heads hjust de-\ntermined. Tab. 4a shows accuracy versus hwhen the Repr.\nsize is ﬁxed to about 1K. It can be seen that h= 6achieves\nthe best result. By setting hto 6, Tab. 4b shows the effect\nof the representation size on performance. We can see that\nthe accuracy consistently increases as the Repr. size grows\nwhile less than 3K; however, further doubling Repr. size\nto 6K brings minor increase of accuracy, which suggests\nthat the performance tends to saturate. We use six heads for\nMGCrP across the paper, unless otherwise speciﬁed.\nExact normalization against approximate one Tab. 4c\n(upper panel) shows the effect of exponent α (Eq. 5) for\nthe exact normalization svPN, where α= 0.5 achieves the\nhighest accuracy. However, svPN via SVD is computation-\nally very expensive, running only at 110 Hz. By setting\nαto 0.5, we demonstrate, in Tab. 4c (lower panel), the ef-\nfect of the number of singular values (#sv) and the number\nof iterations ( #iter) on ˆsvPN (Eq. 8). We note that the\napproximate normalization is slightly inferior to but runs\n20 times faster than its exact counterpart. With only the\nlargest singular value, increase of iteration number brings\nno gains; If we use two or three largest singular values, we\nobserve performance decline. We conjecture the reason is\nthat more iterations accumulate numerical errors, leading to\nperformance decline. Notably, ˆsvPN with the single largest\neigenvalue and one iteration achieves a very competitive ac-\ncuracy of 75.97% with the fastest speed of 2226 Hz, and this\nsetting is used throughout, unless otherwise speciﬁed.\nDifferent normalization methods Besides the proposed\nnormalization method, there are several other options to\nnormalize the cross-covariance matrices, including layer\nnormalization (LN) [18], EPN [28] and an adaptive scal-\ning. EPN consists in signed square root for each element\nfollowed by ℓ2 normalization. In contrast to ˆsvPN which\nscales the cross-covariance matrix by 1/λ1−α\n1 , we design\nthe adaptive scaling which learns a scalar 1/τ >0 to cal-\nibrate the cross-covariance matrix. The comparison results\nare given in Tab. 4d. We can see that all normalization meth-\nods except EPN improve over the baseline that has no nor-\nmalization. In particular, our normalization methods per-\nform much better than the competitors, and improve the\nbaseline by ∼1.2%, suggesting superiority of our normal-\nization method for cross-covariance matrices.\n4.2.3. Comparison with State-of-the-art Methods\nWe present comparisons with a series of vision transformer\nmodels. Tabs. 5a, 5b and 5c show comparison results\nwith lightweight models, middle-sized models and heavy-\nweight models, respectively. In light of these results we\ncan draw the following three conclusions. (1) As regards\nour family of transformer models, on ImageNet, SoT-Tiny\nsigniﬁcantly outperforms the competing light-weight trans-\nformers by 3.8%. When the networks deepens, the perfor-\nmance gaps between our models and the competitors be-\ncome smaller. This is natural as the network gets deeper,\nfurther performance increase becomes more difﬁcult [12].\n(2) When we equip state-of-the-art architectures with our\nmethod, on ImageNet, we can invariably observe consis-\ntent beneﬁts while introducing small, additional cost. For\nlight-weight models, the gains are substantial, i.e., 6.4%,\n5.3% and 2.9% for DeiT-T, iRPE-K-T and T2T-ViT-12, re-\nspectively. For middle-sized models, the improvements are\n1.1%∼2.9%. Even for very strong heavyweight models,\nwe can still achieve gains of 0.5% ∼1.0%. Note that Swin-\nTransformer models have no classiﬁcation token, so we use\nour MGCrP in place of the original GAP. These comparison\nresults demonstrate that our method well generalizes to dif-\nferent vision transformer architectures. (3) On ImageNet-\nA, our SoT-Tiny is superior across the light-weight mod-\nels, while our SoT-Small and SoT-Base are very competi-\ntive compared to state-of-the-art models. Notably, equipped\nwith our method, the compared state-of-the-art methods can\nachieve impressive improvements, i.e., 3.2% ∼10.2% for\nlight-weight models, 3.2%∼13.3% for middle-sized models\nand 1.2%∼7.1% for heavyweight models. These results in-\ndicate that our classiﬁcation head can substantially enhance\nrobustness of different architectures.\nModel Params FLOPs ImageNet ImageNet-A\n(M) (G) Top-1 (%) Top-1 (%)\nDeiT-T [40] 5.7 1.3 72.2 7.3\nT2T-ViT-7 [51] 4.3 1.2 71.7 6.1\nT2T-ViT-12 [51] 6.9 2.2 76.5 12.2\nPS-ViT-Ti/14 [52] 4.8 1.6 75.6 –\nPVT-T [44] 13.2 1.9 75.1 7.9\nPiT-Ti [14] 4.9 0.7 73.0 6.2\niRPE-K-T [48] 6.0 1.3 73.7 8.8\nAutoFormer-tiny [4] 5.7 1.3 74.7 10.3\nSoT-Tiny (ours) 7.7 2.5 80.3 21.5\nDeiT-T+ours 7.0 2.3 78.6 6.4↑ 17.510.2↑\niRPE-K-T+ours 7.0 2.3 79.0 5.3↑ 18.2 9.4↑\nT2T-ViT-12+ours 6.9 2.3 79.4 2.9↑ 15.4 3.2↑\n(a) Comparison with light-weight models.\nModel Params FLOPs ImageNet ImageNet-A\n(M) (G) Top-1 (%) Top-1 (%)\nDeiT-S [40] 22.1 4.6 79.8 18.9\nT2T-ViT-14 [51] 21.5 5.2 81.5 23.9\nPVT-S [44] 24.5 3.8 79.8 18.0\nPS-ViT-B/10 [52] 21.3 3.1 80.6 –\nPS-ViT-B/14 [52] 21.3 5.4 81.7 27.3\nPS-ViT-B/18 [52] 21.3 8.8 82.3 31.7\niRPE-QKV-S [48] 22.0 4.9 81.4 25.0\nPiT-S [14] 23.5 2.9 80.9 21.7\nAutoFormer-small [4] 22.9 5.1 81.7 25.7\nConformer-Ti [32] 23.5 5.2 81.3 27.2\nSwin-T [30] 28.3 4.5 81.3 21.6\nSoT-Small (ours) 26.9 5.8 82.7 31.8\nDeiT-S+ours 25.6 5.5 82.7 2.9↑ 32.213.3↑\nT2T-ViT-14+ours 24.4 5.4 82.6 1.1↑ 27.1 3.2↑\nSwin-T+ours 31.6 6.0 83.0 1.7↑ 33.511.9↑\nConformer-Ti+ours 30.6 6.3 83.0 1.7↑ 36.4 9.2↑\n(b) Comparison with middle-sized models.\nModel Params FLOPs ImageNet ImageNet-A\n(M) (G) Top-1 (%) Top-1 (%)\nDeiT-B [40] 86.6 17.6 81.8 27.4\nT2T-ViT-24 [51] 64.1 15.0 82.3 28.9\nPVT-L [44] 61.4 9.8 81.7 26.6\niRPE-K-B [48] 87.0 17.7 82.4 31.8\nPiT-B [14] 73.8 12.5 82.0 33.9\nAutoFormer-base [4] 54.0 11.0 82.4 28.8\nSwin-B [30] 87.8 15.4 83.5 35.8\nSoT-Base (ours) 76.8 14.5 83.5 34.6\nDeiT-B+ours 94.9 18.2 82.9 1.1↑ 29.11.7↑\nT2T-ViT-24+ours 72.1 15.5 83.3 1.0↑ 30.11.2↑\nSwin-B+ours 95.9 16.9 84.0 0.5↑ 42.97.1↑\n(c) Comparison with heavyweight models.\nTable 5. Comparison with state-of-the-art vision transformer mod-\nels on image classiﬁcation tasks.\n4.3 Text Classiﬁcation for NLP Tasks\nAt last, we evaluate our classiﬁcation head on natural\nlanguage processing tasks. Note that our purpose here is not\nto achieve state-of-the-art performance; instead, we intend\nto show how our classiﬁcation head performs against the\nconventional classiﬁcation head. Two kinds of pre-trained\ntransformer models are used, namely GPT [33] as well as\nModel CoLA RTE MNLI QNLI\nGPT [33] 54.32 63.17 82.10 86.36\nGPT+ours 57.25 2.93↑ 65.352.18↑ 82.410.31↑ 87.130.77↑\nBERT-base [7] 54.82 67.15 83.47 90.11\nBERT-base+ours 58.03 3.21↑ 69.312.16↑ 84.200.73↑ 90.780.67↑\nBERT-large [7] 60.63 73.65 85.90 91.82\nBERT-large+ours 61.82 1.19↑ 75.091.44↑ 86.460.56↑ 92.370.55↑\nSpanBERT-base [19] 57.48 73.65 85.53 92.71\nSpanBERT-base+ours 63.77 6.29↑ 77.263.61↑ 86.130.60↑ 93.310.60↑\nSpanBERT-large [19] 64.32 78.34 87.89 94.22\nSpanBERT-large+ours 65.94 1.62↑ 79.791.45↑ 88.160.27↑ 94.490.27↑\nRoBERTa-base [29] 61.58 77.60 87.50 92.70\nRoBERTa-base+ours 65.28 3.70↑ 80.502.90↑ 87.900.40↑ 93.100.40↑\nRoBERTa-large [29] 67.98 86.60 90.20 94.70\nRoBERTa-large+ours 70.90 2.92↑ 88.101.50↑ 90.500.30↑ 95.000.30↑\nTable 6. Performance improvement over language transformer\nmodels on text classiﬁcation tasks.\nBERT [7] and its stronger variants (i.e., SpanBERT [19]\nand RoBERTa [29]). According to Tab. 6, on CoLA and\nRTE, our method with GPT models improves over the con-\nventional one by 2.18% or more, while with BERT models\nand the variants, we achieve 1.19% ∼6.29% gains in accu-\nracy. As opposed to CoLA and RTE, the improvement on\nMNLI and QNLI is not that large: with GPT we achieve\n0.31% gains for GPT and 0.27% ∼0.73% for BERT or its\nvariants. Note that MNLI and QNLI are much bigger than\nCoLA and RTE, both containing similar sentences with pre-\ntraining datasets; consequently, the performance of individ-\nual models may tend to saturate and further improvement\nbecomes difﬁcult. Notably, the magnitude of performance\nboost by using our method preserves for stronger models,\ne.g., the gains over stronger RoBERTa are comparable to\nthose over BERT. It is well-known that real-world tasks of-\nten have limited labeled data since human annotations are\nexpensive and laborious. For such tasks, our method is more\npreferred as it can provide nontrivial performance increase\nover the conventional method.\n5. Conclusion\nIn this paper, we propose a novel second-order trans-\nformer (SoT) model. The key of our SoT is a novel clas-\nsiﬁcation head which exploits simultaneously word tokens\nand classiﬁcation token. It goes beyond, for the ﬁrst time\nas far as we know, the prevalent classiﬁcation paradigm\nof transformers which exclusively use the classiﬁcation to-\nken. We perform extensive ablation analysis on ImageNet,\nvalidating the effectiveness and superiority of our method.\nThe proposed classiﬁcation head is ﬂexible and ﬁts for\na variety of vision transformer architectures, signiﬁcantly\nimproving them on challenging image classiﬁcation tasks.\nWhat’s more, the proposed classiﬁcation head generalizes\nto language transformer architecture, performing much bet-\nter than the conventional classiﬁcation head on general lan-\nguage understanding tasks.\nReferences\n[1] Irwan Bello. Lambdanetworks: Modeling long-range interac-\ntions without attention. In ICLR, 2020. 1\n[2] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Gi-\nampiccolo. The ﬁfth pascal recognizing textual entailment\nchallenge. In TAC, 2009. 1, 5\n[3] Christopher Bishop. Pattern Recognition and Machine Learn-\ning. Springer, 2006. 4\n[4] Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling.\nAutoformer: Searching transformers for visual recognition. In\nICCV, 2021. 8\n[5] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le.\nRandaugment: Practical automated data augmentation with a\nreduced search space. In CVPR, 2020. 5\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. ImageNet: A large-scale hierarchical image database.\nIn CVPR, 2009. 1, 5\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, 2019.\n1, 2, 5, 8\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenbor, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021. 1, 2, 3, 6\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats,\nand Yann N. Dauphin. Convolutional sequence to sequence\nlearning. In ICML, 2017. 2\n[10] Arthur Gretton, Michael W Mahoney, Mehryar Mohri, and\nAmeet S Talwalkar. Low-rank methods for large-scale ma-\nchine learning. In NIPS Workshop, 2010. 5\n[11] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen,\nJianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chun-\njing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, and\nDacheng Tao. A survey on visual transformer. arXiv preprint\narXiv:2012.12556, 2020. 1\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. InCVPR, 2016.\n2, 3, 5, 7\n[13] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-\nhardt, and Dawn Song. Natural adversarial examples. In\nCVPR, 2021. 1, 5\n[14] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk\nChun, Junsuk Choe, and Seong Joon Oh. Rethinking spatial\ndimensions of vision transformers. In ICCV, 2021. 8\n[15] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735–1780, 1997. 2\n[16] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-\nian Q. Weinberger. Densely connected convolutional net-\nworks. In CVPR, 2017. 3\n[17] Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu.\nMatrix backpropagation for deep networks with structured\nlayers. In ICCV, 2015. 2\n[18] Geoffrey E. Hinton Jimmy Lei Ba, Jamie Ryan Kiros. Layer\nnormalization. arXiv:1607.06450, 2016. 7\n[19] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke\nZettlemoyer, and Omer Levy. SpanBERT: Improving pre-\ntraining by representing and predicting spans.TACL, 8:64–77,\n2020. 8\n[20] Andrew Zisserman Karen Simonyan. Very deep convolu-\ntional networks for large-scale image recognition. In ICLR,\n2015. 5\n[21] Salman H. Khan, Muzammal Naseer, Munawar Hayat,\nSyed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey. arXiv preprint\narXiv:2101.01169, 2021. 1\n[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-\nagenet classiﬁcation with deep convolutional neural networks.\nIn NIPS, 2012. 1\n[23] Peihua Li, Jiangtao Xie, Qilong Wang, and Zilin Gao. To-\nwards faster training of global covariance pooling networks\nby iterative matrix square root normalization. In CVPR, 2018.\n3, 5, 6\n[24] Peihua Li, Jiangtao Xie, Qilong Wang, and Wangmeng\nZuo. Is second-order information helpful for large-scale vi-\nsual recognition? In ICCV, 2017. 2, 4, 6\n[25] Min Lin, Qiang Chen, and Shuicheng Yan. Network in net-\nwork. In ICLR, 2014. 2\n[26] Tsung-Yu Lin, Aruni Roy Chowdhury, and Subhransu Maji.\nBilinear convolutional neural networks for ﬁne-grained visual\nrecognition. IEEE TPAMI, 40(6):1309–1322, 2018. 2, 6\n[27] Tsung-Yu Lin and Subhransu Maji. Improved bilinear pool-\ning with CNNs. In BMVC, 2017. 2\n[28] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji.\nBilinear CNN models for ﬁne-grained visual recognition. In\nICCV, 2015. 2, 7\n[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. RoBERTa: A ro-\nbustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019. 2, 8\n[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer: Hi-\nerarchical vision transformer using shifted windows. InICCV,\n2021. 2, 8\n[31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2018. 5\n[32] Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei\nWang, Jianbin Jiao, and Qixiang Ye. Conformer: Local fea-\ntures coupling global representations for visual recognition.\nIn ICCV, 2021. 2, 8\n[33] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. Technical report, OpenAI, 2018. 1, 2, 5, 8\n[34] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. Technical report, OpenAI, 2019. 2\n[35] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. Squad: 100,000+ questions for machine com-\nprehension of text. In EMNLP, pages 2383–2392, 2016. 5\n[36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and Li\nFei-Fei. ImageNet Large Scale Visual Recognition Challenge.\nIJCV, 115(3):211–252, 2015. 5\n[37] Seymour Shlien. A method for computing the partial singular\nvalue decomposition. IEEE TPAMI, 4(6):671–676, 1982. 5\n[38] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In CVPR, 2017. 1\n[39] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with con-\nvolutions. In CVPR, 2015. 2, 3, 5\n[40] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efﬁcient image transformers & distillation through atten-\ntion. In ICML, 2021. 1, 2, 8\n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, 2017. 1, 2, 4\n[42] Qilong Wang, Jiangtao Xie, Wangmeng Zuo, Lei Zhang,\nand Peihua Li. Deep CNNs meet global covariance pool-\ning: Better representation and generalization. IEEE TPAMI,\n43(8):2582–2597, 2021. 2, 4, 6\n[43] Qilong Wang, Li Zhang, Banggu Wu, Dongwei Ren, Peihua\nLi, Wangmeng Zuo, and Qinghua Hu. What deep cnns beneﬁt\nfrom global covariance pooling: An optimization perspective.\nIn CVPR, 2020. 3\n[44] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense pre-\ndiction without convolutions. In ICCV, 2021. 1, 2, 8\n[45] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018. 1, 3\n[46] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman.\nNeural network acceptability judgments. TACL, pages 625–\n641, 2019. 1, 5\n[47] Adina Williams, Nikita Nangia, and Samuel R Bowman. A\nbroad-coverage challenge corpus for sentence understanding\nthrough inference. In NAACL-HLT, 2018. 5\n[48] Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and\nHongyang Chao. Rethinking and improving relative position\nencoding for vision transformer. In ICCV, 2021. 8\n[49] Yoshua Bengio Patrick Haffner Yann LeCun, L ´eon Bot-\ntou. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11):2278–2324, 1998. 1\n[50] Tan yu, Yunfeng Cai, and Ping Li. Toward faster and simpler\nmatrix normalization via rank-1 update. In ECCV, 2020. 2\n[51] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token ViT: Training vision transformers from scratch on\nimagenet. In ICCV, 2021. 1, 2, 5, 6, 8\n[52] Xiaoyu Yue, Shuyang Sun, Zhanghui Kuang, Meng Wei,\nPhilip Torr, Wayne Zhang, and Dahua Lin. Vision transformer\nwith progressive sampling. In ICCV, 2021. 1, 2, 3, 6, 8\n[53] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-\nization strategy to train strong classiﬁers with localizable fea-\ntures. In ICCV, 2019. 5\n[54] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. Mixup: Beyond empirical risk minimiza-\ntion. ICLR, 2018. 5\nSupplemetary\nContents\nA. Architectures of Our Family of SoT ··············· 11\nB . Proof of Proposition 1 ······························ 11\nC. Backpropagation of svPN via SVD ················ 13\nD. Detailed Experimental Settings for CV and\nNLP Tasks··········································· 13\nD.1. Benchmark Description ··················· 13\nD.1.1 Benchmarks Used in CV ·············· 13\nD.1.2 Benchmarks Used in NLP ············ 13\nD.2. Training Strategy ·························· 14\nD.2.1 Training from Scratch on CV Tasks·· 14\nD.2.2 Fine-tuning on Downstream NLP\nTasks··································· 15\nE . Visualization for CV and NLP Tasks·············· 16\nE.1 . Visualization for CV Model ·············· 16\nE.2 . Visualization for NLP Model ············ 16\nA. Architectures of Our Family of SoT\nFollowing [S-7], our transformer architecture consists of\na token embedding module, a backbone and a classiﬁcation\nhead, as shown in Tab. S-1. For token embedding, the orig-\ninal ViT only uses a single linear projection of ﬁxed image\npatches, failing to model local image information. To ad-\ndress this limitation, we design a small, hierarchical mod-\nule based on off-the-shelf convolutions. Note that our sim-\nple embedding module has proven to be very competitive,\ncompared to the other variants, as shown in the main paper.\nToken embedding Our embedding module consists of a\nstem and a stack of three convolution blocks, gradually de-\ncreasing image resolution. The stem is a 3 ×3 convolution\nfollowed by a max pooling of stride 2 (S2). The design of\nblocks is ﬂexible, and we choose ResNet block [S-9], In-\nception block [S-24], DenseNet block [S-11] and Non-local\nblock [S-29], whose conﬁgurations are shown in Fig. S-1.\nWe halve the spatial size of feature maps for each block, and\nafter the last block, we use a 1×1 convolution to change the\ndimension of features such that the dimension is consistent\nwith that of the backbone. For an input image of224×224,\nour token embedding outputs 14 ×14 spatial features, re-\nshaped to a sequence of 196 vectors as word tokens.\nBackbone of transformer We build the backbone by\nstacking standard transformer blocks as [S-7], where\neach transformer block contains a multi-head self-attention\n(MSA) and a multi-layer perception (MLP). Throughout the\nbackbone, the dimension of tokens (token size) remain un-\nchanged. In MSA, we specify the number of heads; the\ndimension of queries, keys and values can be determined\naccordingly. Each MLP contains two fully-connected (FC)\nlayers, where the dimension of hidden layer is increased\n(called MLP size).\nClassiﬁcation head The proposed classiﬁcation head\ncombines the classiﬁcation token and word tokens, where\nword tokens are aggregated by multi-headed global cross-\ncovariance pooling (MGCrP) with structured normalization\nsvPN. We keep the number of heads unchanged and vary\nthe dimensions (mand n) of two linear projections for SoT\nof different depths.\nWe develop a family of SoT, namely, a light-weight 12-\nlayer SoT-Tiny, a middle-sized 14-layer SoT-Small and a\nheavyweight 24-layer SoT-Base, and their conﬁgurations\nare shown in Tab. S-1. In addition, to make computationally\nfeasible the ablation analysis where extensive evaluations\non ImageNet are involved, we also design a 7-layer SoT. It\nshares the same conﬁguration with SoT-Tiny, but only has 7\nlayers and moreover, the downsampling of the last block in\nthe token embedding module is removed.\nB. Proof of Proposition 1\nProposition 1. Given ui and vi where i = 1,...,k −1,\nlet us consider the objective\nmax\n∥u∥=∥v∥=1\nuTXYTv (S-1)\ns.t. uT\ni u = 0, vT\ni v = 0, i<k.\nBy inductively optimizing (S-1) for k = 1,..., min(m,n),\nwe can obtain, in decreasing order, the k-th largest cross-\ncovariance uT\nkXYTvk, which is equal to the k-th singular\nvalue λk of Q while uk and vk are the corresponding left\nand right singular vectors, respectively.\nProof. We prove this proposition using mathematical in-\nduction. Note that Q = XYT. For convenience, we let\nR(Q,u,v) =uTXYTv.\nInitial case First, let us consider the initial case of objec-\ntive (S-1) for whichk= 1, i.e., max∥u∥=∥v∥=1 R(Q,u,v).\nNote that ∥u∥= 1is equivalent to ∥u∥2 = uTu = 1. The\nLagrange function associated with the objective (S-1) is\nL(u,v,γ,β ) =R(Q,u,v) −γ\n2 (uTu −1) −β\n2 (vTv −1).\n(S-2)\nSoT-Tiny SoT-Small SoT-Base\nToken\nembedding Token size 240 384 528\nBackbone\nLayers 12 14 24\nMSA heads 4 6 8\nMLP size 600 1344 1584\nClassiﬁcation MGCrP heads 6 6 6\nhead MGCrP (m,n) (14,14) (24,24) (38,38)\nParameters (M) 7.7 26.9 76.8\nFLOPs (G) 2.5 5.8 14.5\nTable S-1. Architectures of the proposed SoT networks.\n1x1, 32, S13x3, 16, S1concat. . .\n. . .. . .1x1, 32, S13x3, 16, S1concat\n1x1, 128, S12x2, Avg, S2\n(a) DenseNet block\nX X\n+\n1x1, 64, S11x1, 64, S11x1, 64, S1\n1x1, 192, S1\n3x3, 64, S2 (b) Non-local block\n1x1, 64, S13x3, 64, S2\n+1x1, 256, S1\n(c) ResNet block\n1x1, 32, S2\nconcat\nPrevious layer\n1x1, 32, S13x3, 64, S21x1, 16, S15x5, 16, S23x3, Max, S11x1, 16, S1 (d) Inception block\nFigure S-1. Illustration of different type of convolution blocks in\nour token embedding module. S1/2: Stride 1/2; Avg: average\npooling; Max: max pooling.\nWe calculate the gradient ▽L=\n[\n∂L\n∂u ,∂L\n∂v ,∂L\n∂γ,∂L\n∂β\n]\n, where\n∂L\n∂u is the partial derivatives of Lwith respect to u, and set\nit to be zero. After some manipulations, we have\nQv −γu = 0, (S-3)\nQTu −βv = 0,\nuTu −1 = 0,\nvTv −1 = 0.\nThe last two equations simply produce the constraints, i.e.,\n∥u∥= 1and ∥v∥= 1. We left multiple the ﬁrst and second\nequation by uT and vT, respectively. Then we can obtain\nγ = β = R(Q,u,v),\nas uTQv = vTQTu. Therefore, we have\nQv = γu, (S-4)\nQTu = γv.\nAccording to the property of SVD [S-8, Chap.2.4], we know\nthat u and v which satisfy S-4 are respectively left and right\nsingular vectors ofQ with γbeing the singular value. Obvi-\nously, R(Q,u,v) achieves its maximum when it is equal to\nthe largest singular value. Therefore, by optimizing the ob-\njective (S-1) withk= 1, we obtain vectorsu1 and v1 which\nare respectively the left and right singular vectors corre-\nsponding to the largest singular value λ1 = R(Q,u1,v1).\nInductive step Next, we optimize the objective (S-1) for\nk >1. Suppose the statement holds for i < k. That is,\nfor any i, ui and vi, which maximize R(Q,u,v) while sat-\nisfying the constraints ∥ui∥ = ∥vi∥ = 1 and uT\ni ui′ =\n0,vT\ni vi′ = 0,i′<i, are the singular vectors corresponding\nto the ith largest singular values λi. Obviously\nR(Q,u1,v1)  \nλ1\n≥... ≥R(Q,uk−1,vk−1)  \nλk−1\n.\nNow, let us prove the statement holds for the casek. The\nLagrange function of the objective (S-1) is\nL(u,v,γ,β,τ i,δi) =R(Q,u,v) −γ\n2 (uTu −1) (S-5)\n−β\n2 (vTv −1) −\n∑k−1\ni=1\nτiuTui −\n∑k−1\ni=1\nδivTvi.\nWe calculate the gradient of the Lagrange function\n▽L=\n[\n∂L\n∂u ,∂L\n∂v ,∂L\n∂γ,∂L\n∂β, ∂L\n∂τ1\n,..., ∂L\n∂τk−1\n, ∂L\n∂δ1\n,..., ∂L\n∂δk−1\n]\n.\nBy setting ▽Lto be zero, we obtain a set of equations\nQv −γu −\n∑k−1\ni=1\nτiui = 0, (S-6)\nQTu −βv −\n∑k−1\ni=1\nδivi = 0,\nuTu −1 = 0,\nvTv −1 = 0,\nuTui = 0, i= 1,...,k −1,\nvTvi = 0, i= 1,...,k −1.\nThe third to last equations are simply constraints of the\nmaximization problem (S-1). We left multiply uT (resp.,\nvT) the ﬁrst (resp., second) equation, and we can obtain\nuTQv = γ(resp., vTQTu = β), by noting that uTui = 0\n(resp., vTvi = 0) u for i < k. Therefore, we have\nγ = β = R(Q,u,v).\nSubsequently, we will show that τj = 0,j < k. We left\nmultiply the ﬁrst equation by uT\nj ,j < k. We recall that uj\nis orthogonal to uj′for j′̸= jand to u, and then can derive\nτj = uT\nj Qv.\nAs uj is the left singular value of Q, we know QTuj =\nλjvj, i.e., uT\nj Q = λjvj. Therefore we have\nτj = uT\nj Qv = λjvT\nj v = 0.\nHere we make use of the fact that vj is orthogonal to v.\nIn a similar manner, we left multiply the second equation\nby vT\nj ,j < k, and then we can derive δj = 0. Up to this\npoint, we know u and v for which the objective (S-1) is\nmaximized satisfy the following pair of equations\nQv = γu, (S-7)\nQTu = γv.\nAgain, according to the property of SVD, we know u and\nv are left and right singular values of Q and γ is the cor-\nresponding singular value. Obviously, γ achieves the max-\nimum when it equals the k-th largest singular value. This\nconcludes our proof.\nAs far as we know, the statement as described in Propo-\nsition 1 appeared early in [S-2] and later in [S-27, Chap.\n14.1.7], among others. However, we fail to ﬁnd any for-\nmal proof of this statement; hence, we provide a proof here.\nIt is worth mentioning that this statement is closely related\nto but different from canonical correlation analysis (CCA).\nFor detailed theory on CCA, one may refer to [S-26].\nC. Backpropagation of svPN via SVD\nLet Q = Udiag(λi)VT be the SVD of Q. The forward\npropagation of our normalization ˜Q\n△\n= svPN(Q) can be\ndescribed in two consecutive steps as follows:\nQ\nSVD\n−→Udiag(λi)VT power\n−→Udiag(λα\ni )VT (S-8)\nThe associated backward propagations are not that straight-\nforward as the structured, nonlinear matrix operations are\ninvolved. Suppose lis the network loss function. Let us de-\nnote Asym = 1\n2 (A + AT), Λ = diag(λi), and Adiag being\na matrix setting the off-diagonals of A to zero. Based on\nthe theory of matrix backpropagation [S-12], we can derive\nthe gradients relative to svPN via SVD, which are given in\nthe following corollary.\nCorollary 1. Suppose we have ∂l\n∂˜Q\nfrom the succeeding\nlayer. The gradient involved in the ﬁrst step of (S-8) is\n∂l\n∂Q = ∂l\n∂UΛ−1VT + U\n(∂l\n∂Λ −UT ∂l\n∂UΛ−1\n)\ndiag\nVT\n+ 2UΛ\n(\nKT ◦\n(\nVT\n( ∂l\n∂V −VΛ−1\n( ∂l\n∂U\n)T\nUΛ\n)))\nsym\nVT\nwhere Kij = (λ2\ni −λ2\nj)−1 if λi ̸= λj and Kij = 0other-\nwise, and ◦denotes Hadamard product. The partial deriva-\ntives with respect to the second step of (S-8) are\n∂l\n∂U = ∂l\n∂˜Q\nVΛα,\n∂l\n∂V =\n(∂l\n∂˜Q\n)T\nUΛα,\n∂l\n∂Λ = αΛα−1UT ∂l\n∂˜Q\nV.\nD. Detailed Experimental Settings for CV and\nNLP Tasks\nD.1. Benchmark Description\nD.1.1 Benchmarks Used in CV\nImageNet Our experiments are mainly conducted on\nILSVRC ImageNet 2012 image classiﬁcation bench-\nmark [S-21,S-5], which contains 1K classes with 1.28M im-\nages for training and 50K images for validation. Note that\nas test images are not publicly available, the common prac-\ntice is to adopt the validation images for testing [S-34,S-\n25,S-35]. We train the transformer models from scratch on\nthe training set and report top-1 accuracy on the validation\nset.\nImageNet-A The ImageNet-A [S-10] is a hard ImageNet\ntest set of real-world adversarial images with adversarial ﬁl-\ntration. It contains 7,500 natural images from 200 classes\nwhich cover most broad categories spanned by ImageNet.\nThis dataset has heterogeneous and varied distribution shift\nfrom ImageNet. ImageNet-A is far more challenging than\nthe original ImageNet validation set (and test set). For ex-\nample, DeiT-Small model achieves only a top-1 accuracy of\n18.9% on ImageNet-A against 79.8% on ImageNet valida-\ntion set. Fig. S-2 (1st column) shows four examples from\nImageNet-A, in which an object (blue text) of some class is\nmistakenly identiﬁed as that of another class with high con-\nﬁdence score (in red); for contrast, the 2nd and 3rd columns\nshow the images of the corresponding classes in ImageNet.\nIt can be seen that images of ImageNet-A is highly adver-\nsarial and its distribution deviates much from that of Ima-\ngeNet. Notably, on the challenging ImageNet-A, the pro-\nposed method can substantially improve state-of-the-art vi-\nsion transformers (see Sec. 4.2.3 in the main paper).\nD.1.2 Benchmarks Used in NLP\nCoLA The goal of the Corpus of Linguistic Acceptability\ntask [S-30] is to judge whether a sentence is grammatical or\nnot. It can be formulated as a binary single-sentence classi-\nﬁcation problem. The dataset contains 10,657 English sen-\ntences which are labeled as grammatical or ungrammatical,\nRobin\nKite 97.11% \nQuill \nEgret\n96.48% \nGrand\nSnail 97.05% \nEft\nSnake\n93.67% \nRobin\nQuill\nGrand\nEft\nKite\nEgret\nSnail\nSnake\nImageNet\n-\nA\nImageNet\nFigure S-2. Examples of adversarial images from ImageNet-A (1st\ncolumn). The blue text is the test class, and the red text is the\nfalse prediction and the score produced by a DeiT-Small model.\nThe 2nd and 3rd columns show the images of the correspond-\ning classes from ImageNet. It can be seen that the images from\nImageNet-A are highly adversarial, whose distribution deviates\nmuch from that of ImageNet. On the challenging ImageNet-A, the\nproposed method can substantially improve state-of-the-art vision\ntransformers (see Sec. 4.2.3 in the main paper).\nand these sentences are split into training (8,551)/develop-\nment (1,043)/test (1,063) sets.\nRTE Given a pair of text fragments, denoted by (“Text”,\n“Hypothesis”), Recognizing Textual Entailment [S-1] aims\nto determine whether the “Text” entails “Hypothesis”. This\ntask can be converted into a binary entailment classiﬁcation\ntask. The dataset of RTE consists of 5,767 examples, among\nwhich the training set contains 2,490 examples, while the\ndevelopment set and test set contain 3,000 and 277 exam-\nples, respectively.\nMNLI Similar to RTE, Multi-Genre Natural Language\nInference [S-31] is also concerned with judgement of en-\ntailment. Given a pair of sentences, the task is to pre-\nModels SoT-Tiny SoT-Small SoT-Base\nBatch size 1024 1024 512\nOptimizer AdamW AdamW AdamW\nEpochs 310 310 310\nBase LR 1e-3 1e-3 5e-4\nFinal LR 1e-5 1e-5 1e-5\nScheduler cosine cosine cosine\nWeight decay 0.03 0.03 0.065\nLabel smoothing 0.1 0.1 0.1\nMixup prob. 0.8 0.8 0.8\nCutmix prob. 1.0 1.0 1.0\nErasing prob. 0.25 0.25 0.25\nRandAugment 9/0.5 9/0.5 9/0.5\nMGCrP dropout 0.0 0.5 0.7\nTable S-3. Hyper-parameters for image classiﬁcation.\ndict whether the “Text” entails the “Hypothesis” (entail-\nment), contradicts the “Hypothesis” (contradiction), or nei-\nther (neutral). As such, this task can be formulated as a\nthree-way classiﬁcation problem. MNLI is a large-scale\ndataset, consisting of 432,702 examples, in which 392,702\nexamples belong to the training set, 20,000 examples be-\nlong to the development set and the remaining 20,000 ex-\namples are in the test set.\nQNLI The Stanford Question Answering task [S-20] is\nconverted to a sentence-pair classiﬁcation problem [S-28].\nGiven a pair of sentences, the model needs to determine\nwhether the sentence contains the answer to the question.\nQNLI dataset contains 105K training examples, 5.4K de-\nvelopment examples and 5.4K test examples.\nD.2. Training Strategy\nD.2.1 Training from Scratch on CV Tasks\nAs suggested in [S-7], training of high-performance trans-\nformer models requires ultra large-scale datasets. Hence,\nfor training from scratch on ImageNet which is not that\nlarge, one often depends on extensive data augmentation\nand regularization methods [S-25], for which we mainly fol-\nlow [S-25,S-34,S-35]. For data augmentation, besides stan-\ndard scale, color and ﬂip jittering [S-14,S-9] with default\nsettings in PyTorch, we adopt randAugment [S-4] and ran-\ndom erasing [S-39]. For model regularization, we employ\nlabel smoothing [S-23], mixup [S-38] and cutmix [S-36].\nWe use AdamW [S-17] optimizer with a learning rate warm\nup (5 epochs) and cosine annealing scheduler. We adopt\ndropout for our MGCrP module. The hyper-parameters in-\nvolved in augmentation, regularization, optimization, etc.,\nare summarized in Tab. S-3. Note that the 7-layer SoT used\nin the ablation study shares the same hyper-parameters with\nSoT-Tiny.\nBackbone  of  BERT (GPT)\nClassifier\nLinear\nSentence1\nMGCrP\nLinear\"Contradiction\"\"Entailment\"\"Neutral\"\n[CLS]\nClassifier\nLinear\n\"Contradiction\"\"Entailment\"\"Neutral\"BERT GPT\nToken embedding\nSegmenttokenPositiontoken\nSentence2Wordtoken SeparatortokenClassificationtoken\nSaidI proud[SEP]My placedpride [CLS]\n(a) Sentence-pair classiﬁcation task (RTE, MNLI and QNLI).\nBackbone  of  BERT (GPT)\nClassifier\nLinear\n MGCrP\nLinear\n[CLS]\nClassifier\nLinear\nBERT GPT\nToken embedding\nSegmenttokenPositiontokenSingle SentenceWordtoken SeparatortokenClassificationtoken\ndemandI that morepayshe [CLS]\n\"Grammatical\"\"Ungrammatical\" \"Grammatical\"\"Ungrammatical\" (b) Single-sentence classiﬁcation task (CoLA)\nFigure S-3. Diagrams of ﬁne-tuning BERT and GPT on downstream NLP tasks, formulated as either (a) sentence-pair classiﬁcation (RTE,\nMNLI and QNLI), or (b) single-sentence classiﬁcation (CoLA). Note that for BERT & its variants, the classiﬁcation token [CLS] is always\nin the ﬁrst place of the sequence, while it is at the end for GPT whose illustration fades; besides, GPT does not use segment embedding.\nModels GPT BERT SpanBERT RoBERTa\nBatch size {32,64} {24,64,96} 32 {16,32}\nAdam β1 0.9 0.9 0.9 0.9\nOptimizer β2 0.999 0.999 0.999 0.98\nϵ 1e-8 1e-8 1e-6 1e-6\nEpochs {10,15} 10 {10,30} { 10,20}\nBase LR 6.25e-5 {2e-5,3e-5} 2e-5 {1e-5,2e-5}\nFinal LR 0 0 0 0\nScheduler linear linear linear linear\nWeight decay 0.01 0 0.01 0.1\nMGCrP repr. size {4K,6K}{1K,4K,5K}{1K,4K,5K}{1K,4K,5K}\nMGCrP dropout {0.5,0.7} {0.5,0.8} { 0.5,0.7} { 0.5,0.7}\nTable S-4. Hyper-parameters for text classiﬁcation.\nD.2.2 Fine-tuning on Downstream NLP Tasks\nThe illustration of ﬁne-tuning BERT and GPT with our\nclassiﬁcation head can be seen in Fig S-3. The four NLP\ntasks are formulated as either sentence-pair classiﬁcation\ntask (RTE, MNLI and QNLI) or single-sentence classiﬁca-\ntion task (CoLA). For each task, we plug in the task-speciﬁc\ninput and outputs into the transformer models and ﬁne-tune\nthe whole networks in an end-to-end fashion. Following\nprevious works [S-16,S-19,S-6,S-13], for each task we ﬁne-\ntune the model on the training set while evaluating on the\ndevelopment set.\nIn the following, we introduce the ﬁne-tuning pipeline by\ntaking the sentence-pair classiﬁcation with BERT model as\nan example. As shown in Fig. S-3a, for a sentence-pair clas-\nsiﬁcation, a pair of sentences are concatenated into a single\nsequence with a special token ([SEP]) separating them, and\nis then prepended by a classiﬁcation token ([ CLS]). The\ninput representation of every token is built by summing the\nword embedding (by e.g., WordPiece [S-33]), segment em-\nbedding and position embedding. At the output, the to-\nken representations are fed into our proposed classiﬁcation\nhead, in which we combine classiﬁcation token and word\ntokens. Fig. S-3b shows the single-sentence classiﬁcation\ntask, which is similar to sentence-pair task except the input\nonly involves one sentence. We mention that the [ CLS] is\nalways in the ﬁrst place of the token sequence for BERT,\nwhile in the last for GPT.\nFor GPT and BERT & its variants, most training strate-\ngies and hyper-parameters in ﬁne-tuning are the same as\nthose in pre-training. We use Adam [S-15] algorithm for\nmodel optimization. The learning rate is linearly warmed\nup over a number of steps to a peak value, and then lin-\nearly decayed to zero. We mainly tune the batch size,\nlearning rate and number of training epochs. The optimal\nhyper-parameters are task-speciﬁc. Following [S-19,S-6],\nwe choose them from a small set of options; for example,\nfor GPT, we select batch size from {32,64}. For our clas-\nsiﬁcation head, we use dropout for MGCrP. For simplicity,\nwe adopt single head for our MGCrP and select the repre-\nsentation (repr.) size from a set of values, e.g., {4K,6K}\nfor GPT. The detailed settings of the hyper-parameters are\nsummarized in Tab. S-4.\nFine-tuning of BERT and GPT are implemented based\non HuggingFace’s codebase [S-32], while that of RoBERTa\nis based on fairseq [S-18], an open-source sequence model-\ning toolkit. We implement ﬁne-tuning of SpanBERT using\nthe code available at ofﬁcial website of facebook. The pre-\ntrained BERT model is downloaded from HuggingFace’s\nwebsite, and pretrained RoBERTa and SpanBERT models\nare both from fairseq website. The pretrained GPT model is\navailable at the ofﬁcial website of OpenAI .\nE. Visualization for CV and NLP Tasks\nTo further analyze the effectiveness of our proposed clas-\nsiﬁcation head, we make qualitative comparisons by visual-\nizing the models for CV and NLP tasks. Speciﬁcally, SoT-\nTiny and the BERT-base are used as the backbone models\nfor CV and NLP tasks, respectively. For each model, we\ncompare three variants as follows:\n• ClassT: only classiﬁcation token is used for classiﬁer;\n• WordT: only word tokens are used for classiﬁer;\n• ClassT+WordT: Both classiﬁcation token and word\ntokens are used for classiﬁer based on the sum scheme.\nE.1. Visualization for CV Model\nTo visualize the models in CV task, we ﬁrst train our\nSoT-Tiny variants on ImageNet, and then adopt the Grad-\nCAM [S-22] to obtain class activation map of each input im-\nage. As such, we can visualize the most important regions\n(i.e., regions of interest) for the ﬁnal classiﬁcation accord-\ning to the gradient information. As illustrated in Fig. S-4,\nwe show three kinds of scenarios, in whichClassT+WordT\nall makes correct predictions, i.e., (left panel)ClassT makes\ncorrect predictions butWordT fails, (middle panel) WordT\npredicts correctly but ClassT does not; (right panel) neither\nClassT nor WordT predicts correctly.\nFrom Fig. S-4, we have the following observations: (1)\nAs classiﬁcation token interacts with all word tokens across\nthe network, it tends to focus on the global context of\nimages, especially some messy backgrounds. Therefore,\nClassT is more suitable for classifying the categories as-\nsociated with the backgrounds and the whole context, e.g.,\n“Bookshop”. (2) The word tokens mainly correspond to\nlocal patches, so WordT performs classiﬁcation primar-\nily based on some local discriminative regions. As such,\nWordT has better ability to classify the categories associ-\nated with local parts and subtle variations, e.g., “Standard\npoodle”. (3) Our ClassT+WordT can make fully use of\nmerits of both word tokens and classiﬁcation token, which\ncan focus on the most important regions for better classiﬁca-\ntion by exploiting both local discriminative parts and global\ncontext information.\nE.2. Visualization for NLP Model\nSimilarly, we compare the visualization results of the\nBERT-base under three scenarios on the examples of CoLA.\nThe task is to judge whether an English sentence is gram-\nmatical or not. We use visualization methods proposed\nin [S-37,S-3] to show the inﬂuence of each word in the ﬁ-\nnal prediction. As shown in Fig. S-5, the green denotes\nstronger impact while the blue implies weaker one.\nAll examples in Fig. S-5 are ungrammatical. Over-\nall, we can see ClassT inclines to make predictions from\nthe whole sentence, such as the conjunction of two sub-\nsentences (e.g., “Because.., as”) or the key global semantic\nword; WordT tends to focus on local correctness of each\nsentence, ignoring the global context. This observation is\nsimilar with the visualization results of CV model, demon-\nstrating that the classiﬁcation token and word tokens are\nhighly complementary for both CV and NLP tasks. Finally,\nthe proposed ClassT+WordT can highlight all important\nwords in sentence, including the subordinate clause, con-\njunction, etc., which can help to boost the performance of\nclassiﬁcation.\nReferences\n[S-1] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Gi-\nampiccolo. The ﬁfth pascal recognizing textual entailment\nchallenge. In TAC, 2009.\n[S-2] Christopher S. Bretherton, Catherine Smith, and John M.\nWallace. An intercomparison of methods for ﬁnding cou-\npled patterns in climate data. JCLI, (6):541–560, 1992.\n[S-3] Hila Chefer, Shir Gur, and Lior Wolf. Transformer inter-\npretability beyond attention visualization. arXiv preprint\narXiv:2012.09838, 2021.\n[S-4] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In CVPR, 2020.\n[S-5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and\nLi Fei-Fei. ImageNet: A large-scale hierarchical image\ndatabase. In CVPR, 2009.\n[S-6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT,\n2019.\n[S-7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenbor, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-\nage is worth 16x16 words: Transformers for image recog-\nnition at scale. In ICLR, 2021.\n[S-8] Gene H. Golub and Charles F. Van Loan. Matrix Compu-\ntations (4th Ed.) . Johns Hopkins University Press, USA,\n2003.\n[S-9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016.\nBookshop Binoculars\n Home theatre Barbell Standard poodle\n Crawfish Hoopskirt Hatchet\n Gown\nClassT\nWordT\nClassT+WordT\nImage\n✓\n✗\n✓\n✓\n✗\n✓ ✓\n✗\n✗\nFigure S-4. Visualizations of images on ImageNet validation set based on SoT-Tiny by using the Grad-CAM [S-22]. We show the examples\nfor which ClassT+WordT predicts correctly, but ClassT or WordT fails. ✓: correct prediction; \u0017: incorrect prediction.\n✗✓\n✓Becausesheissopleasant,asformaryIreallylikeher.\nBecausesheissopleasant,asformaryIreallylikeher.Becausesheissopleasant,asformaryIreallylikeher.\nBecausesheissopleasant,asformaryIreallylikeher.WordTClassT\nClassT+WordT\nIdemandthatthemorejohneat,themorehepays.Idemandthatthemorejohneat,themorehepays.Idemandthatthemorejohneat,themorehepays.\nIdemandthatthemorejohneat,themorehepays.✗✓✓WordTClassT\nClassT+WordT\nWordTClassT\nClassT+WordT ✗ShesaidshetalkedtothreestudentsbutIdon'tknowhowmany.✗ShesaidshetalkedtothreestudentsbutIdon'tknowhowmany.\nShesaidshetalkedtothreestudentsbutIdon'tknowhowmany.✓\nShesaidshetalkedtothreestudentsbutIdon'tknowhowmany.\nStronger\nWeaker\nFigure S-5. Visualization of the inﬂuence of each word for linguistic acceptability on the given English sentence. We adopt the BERT-base\nas the backbone and refer [S-37,S-3] to obtain the results. ✓: correct prediction; \u0017: incorrect prediction.\n[S-10] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-\nhardt, and Dawn Song. Natural adversarial examples. In\nCVPR, 2021.\n[S-11] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and\nKilian Q Weinberger. Densely connected convolutional\nnetworks. In CVPR, 2017.\n[S-12] Catalin Ionescu, Orestis Vantzos, and Cristian Smin-\nchisescu. Training deep networks with structured\nlayers by matrix backpropagation. arXiv preprint\narXiv:1509.07838, 2015.\n[S-13] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. SpanBERT: Improving\npre-training by representing and predicting spans. TACL,\n8:64–77, 2020.\n[S-14] Andrew Zisserman Karen Simonyan. Very deep convo-\nlutional networks for large-scale image recognition. In\nICLR, 2015.\n[S-15] Diederik Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015.\n[S-16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. RoBERTa: A ro-\nbustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n[S-17] Ilya Loshchilov and Frank Hutter. Decoupled weight de-\ncay regularization. In ICLR, 2018.\n[S-18] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael Auli.\nfairseq: A fast, extensible toolkit for sequence modeling.\nIn NAACL-HLT, 2019.\n[S-19] Alec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. Improving language understanding by gen-\nerative pre-training. Technical report, OpenAI, 2018.\n[S-20] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. Squad: 100,000+ questions for machine\ncomprehension of text. In EMNLP, pages 2383–2392,\n2016.\n[S-21] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexan-\nder C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual\nRecognition Challenge. IJCV, 115(3):211–252, 2015.\n[S-22] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek\nDas, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-\ntra. Grad-cam: Visual explanations from deep networks\nvia gradient-based localization. In IEEE, pages 618–626,\n2017.\n[S-23] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-\nmanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,\nVincent Vanhoucke, and Andrew Rabinovich. Going\ndeeper with convolutions. In CVPR, 2015.\n[S-24] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception\narchitecture for computer vision. In CVPR, 2016.\n[S-25] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In ICML, 2021.\n[S-26] Viivi Uurtio, Jo ˜ao M. Monteiro, Jaz Kandola, John Shawe-\nTaylor, Delmiro Fernandez-Reyes, and Juho Rousu. A\ntutorial on canonical correlation methods. CSUR, 50(6),\n2017.\n[S-27] Hans von Storch and Francis W. Zwiers. Statistical Anal-\nysis in Climate Research . Cambridge University Press,\n2003.\n[S-28] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel Bowman. Glue: A multi-task\nbenchmark and analysis platform for natural language un-\nderstanding. In EMNLP, pages 353–355, 2018.\n[S-29] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and\nKaiming He. Non-local neural networks. In CVPR, 2018.\n[S-30] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman.\nNeural network acceptability judgments. TACL, pages\n625–641, 2019.\n[S-31] Adina Williams, Nikita Nangia, and Samuel R Bowman. A\nbroad-coverage challenge corpus for sentence understand-\ning through inference. In NAACL-HLT, 2018.\n[S-32] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-\nmond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R ´emi Louf, Morgan Funtowicz, and Jamie\nBrew. Huggingface’s transformers: State-of-the-art natu-\nral language processing. arXiv preprint arXiv:1910.03771,\n2019.\n[S-33] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, and et al. Klaus Macherey.\nGoogle’s neural machine translation system: Bridging\nthe gap between human and machine translation. arXiv\npreprint arXiv:1609.08144, 2016.\n[S-34] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun\nShi, Francis EH Tay, Jiashi Feng, and Shuicheng Yan.\nTokens-to-token ViT: Training vision transformers from\nscratch on imagenet. In ICCV, 2021.\n[S-35] Xiaoyu Yue, Shuyang Sun, Zhanghui Kuang, Meng Wei,\nPhilip Torr, Wayne Zhang, and Dahua Lin. Vision trans-\nformer with progressive sampling. In ICCV, 2021.\n[S-36] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Reg-\nularization strategy to train strong classiﬁers with localiz-\nable features. In ICCV, 2019.\n[S-37] Zeyu Yun, Yubei Chen, Bruno A Olshausen, and Yann\nLeCun. Transformer visualization via dictionary learn-\ning: contextualized embedding as a linear superposition\nof transformer factors. In NAACL, 2021.\n[S-38] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk mini-\nmization. In ICLR, 2018.\n[S-39] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In AAAI,\nvolume 34, pages 13001–13008, 2020."
}