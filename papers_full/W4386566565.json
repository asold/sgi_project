{
    "title": "Fairness in Language Models Beyond English: Gaps and Challenges",
    "url": "https://openalex.org/W4386566565",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3169131437",
            "name": "Krithika Ramesh",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2013710467",
            "name": "Sunayana Sitaram",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2162966668",
            "name": "Monojit Choudhury",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4306705877",
        "https://openalex.org/W2971051967",
        "https://openalex.org/W3178522238",
        "https://openalex.org/W4285060468",
        "https://openalex.org/W2950888501",
        "https://openalex.org/W3155266314",
        "https://openalex.org/W4288058287",
        "https://openalex.org/W1796112755",
        "https://openalex.org/W4206292552",
        "https://openalex.org/W4385567161",
        "https://openalex.org/W4287855127",
        "https://openalex.org/W2970408399",
        "https://openalex.org/W3032765105",
        "https://openalex.org/W2004719672",
        "https://openalex.org/W2563852449",
        "https://openalex.org/W4221167694",
        "https://openalex.org/W4283162604",
        "https://openalex.org/W4287887365",
        "https://openalex.org/W3013677002",
        "https://openalex.org/W4311398160",
        "https://openalex.org/W4286917614",
        "https://openalex.org/W4287658197",
        "https://openalex.org/W3035379020",
        "https://openalex.org/W3206487987",
        "https://openalex.org/W3123978464",
        "https://openalex.org/W3168477526",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W3104114204",
        "https://openalex.org/W3090414294",
        "https://openalex.org/W3038073095",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3168656614",
        "https://openalex.org/W4229026816",
        "https://openalex.org/W2135585764",
        "https://openalex.org/W2250782535",
        "https://openalex.org/W3198409578",
        "https://openalex.org/W3126947648",
        "https://openalex.org/W4285155368",
        "https://openalex.org/W3202415077",
        "https://openalex.org/W3034515982",
        "https://openalex.org/W4223600764",
        "https://openalex.org/W2329121264",
        "https://openalex.org/W3037831233",
        "https://openalex.org/W3093211917",
        "https://openalex.org/W4308672074",
        "https://openalex.org/W4241439391",
        "https://openalex.org/W4287887849",
        "https://openalex.org/W3135514117",
        "https://openalex.org/W1547384207",
        "https://openalex.org/W4297816198",
        "https://openalex.org/W3156592339",
        "https://openalex.org/W3167047789",
        "https://openalex.org/W4309798993",
        "https://openalex.org/W2053181824",
        "https://openalex.org/W3196248941",
        "https://openalex.org/W4288029087",
        "https://openalex.org/W2893425640",
        "https://openalex.org/W2251409655",
        "https://openalex.org/W4287545934",
        "https://openalex.org/W3172415559",
        "https://openalex.org/W2949678053",
        "https://openalex.org/W4243810292",
        "https://openalex.org/W2022840847",
        "https://openalex.org/W4285163830",
        "https://openalex.org/W4287890645",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W4385573768",
        "https://openalex.org/W2503738056",
        "https://openalex.org/W2952349219",
        "https://openalex.org/W3213241618",
        "https://openalex.org/W2563747519",
        "https://openalex.org/W4285199616",
        "https://openalex.org/W4310413652",
        "https://openalex.org/W2250559305",
        "https://openalex.org/W2963919731",
        "https://openalex.org/W3212327893",
        "https://openalex.org/W2142112646",
        "https://openalex.org/W3146885639",
        "https://openalex.org/W4312108607",
        "https://openalex.org/W4287891150",
        "https://openalex.org/W4287887133",
        "https://openalex.org/W3102641573",
        "https://openalex.org/W1581904241",
        "https://openalex.org/W2963078909",
        "https://openalex.org/W2972866455",
        "https://openalex.org/W2510483258",
        "https://openalex.org/W2972413484",
        "https://openalex.org/W4226462293",
        "https://openalex.org/W4294029935",
        "https://openalex.org/W3154839373",
        "https://openalex.org/W2563656189",
        "https://openalex.org/W4287890642",
        "https://openalex.org/W636189409",
        "https://openalex.org/W2950018712",
        "https://openalex.org/W3035032094",
        "https://openalex.org/W2996952890",
        "https://openalex.org/W2863494293",
        "https://openalex.org/W4285294416",
        "https://openalex.org/W2963099212",
        "https://openalex.org/W4286969132",
        "https://openalex.org/W3172917028",
        "https://openalex.org/W4404752288",
        "https://openalex.org/W3029881688",
        "https://openalex.org/W3127314936",
        "https://openalex.org/W2186005756",
        "https://openalex.org/W4225378656"
    ],
    "abstract": "With language models becoming increasingly ubiquitous, it has become essential to address their inequitable treatment of diverse demographic groups and factors. Most research on evaluating and mitigating fairness harms has been concentrated on English, while multilingual models and non-English languages have received comparatively little attention. In this paper, we survey different aspects of fairness in languages beyond English and multilingual contexts. This paper presents a survey of fairness in multilingual and non-English contexts, highlighting the shortcomings of current research and the difficulties faced by methods designed for English. We contend that the multitude of diverse cultures and languages across the world makes it infeasible to achieve comprehensive coverage in terms of constructing fairness datasets. Thus, the measurement and mitigation of biases must evolve beyond the current dataset-driven practices that are narrowly focused on specific dimensions and types of biases and, therefore, impossible to scale across languages and cultures.",
    "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 2106–2119\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nFairness in Language Models Beyond English: Gaps and Challenges\nKrithika Ramesh, Sunayana Sitaram, Monojit Choudhury\nMicrosoft Corporation\n{t-kriramesh, sunayana.sitaram, monojitc}@microsoft.com\nAbstract\nWith language models becoming increasingly\nubiquitous, it has become essential to address\ntheir inequitable treatment of diverse demo-\ngraphic groups and factors. Most research on\nevaluating and mitigating fairness harms has\nbeen concentrated on English, while multilin-\ngual models and non-English languages have\nreceived comparatively little attention. In this\npaper, we survey different aspects of fairness\nin languages beyond English and multilingual\ncontexts. This paper presents a survey of fair-\nness in multilingual and non-English contexts,\nhighlighting the shortcomings of current re-\nsearch and the difficulties faced by methods\ndesigned for English. We contend that the mul-\ntitude of diverse cultures and languages across\nthe world makes it infeasible to achieve com-\nprehensive coverage in terms of constructing\nfairness datasets. Thus, the measurement and\nmitigation of biases must evolve beyond the cur-\nrent dataset-driven practices that are narrowly\nfocused on specific dimensions and types of bi-\nases and, therefore, impossible to scale across\nlanguages and cultures.\n1 Introduction\nLanguage models are known to be susceptible to\ndeveloping spurious correlations and encoding bi-\nases that have potentially harmful consequences\nin downstream tasks. Whilst prior work has doc-\numented these harms (Dev et al., 2021) (Bender\net al., 2021) (Kumar et al.), there remains much to\nbe studied and criticism for the existing research\n(or lack thereof) that remains to be addressed.\nIn the context of language models, fairness can\nmanifest in two forms; representational and allo-\ncational harms. Representational harms generally\nrefer to cases where demographic groups end up be-\ning misrepresented. This includes stereotypes and\nnegative associations with these groups and even a\nlack of acknowledgment of certain groups that are\nunderrepresented in the data. Allocational harms,\non the other hand, refer to the inequitable distri-\nbution of resources and opportunities to groups\nwith different demographic attributes associated\nwith them. The nature of allocational harms can\nvary based on the sociocultural, economic, and\nlegal settings where the system has been deployed.\nHowever, it can also take shape in terms of the\nmodel’s functionality across languages with fewer\nresources (Choudhury and Deshpande, 2021; Liu\net al., 2021). While current literature adopts a Euro-\nAmerican-centric view of fairness, work such as\nSambasivan et al. (2021) pushes to recognize algo-\nrithmic fairness from a more inclusive lens.\nBias crops up in multiple steps of the pipeline\n(Hovy and Prabhumoye, 2021) (Sap et al., 2022),\nincluding the annotation process, the training data,\nthe input representations, model architecture, and\nthe structure of the research design. Thus, mea-\nsures to mitigate bias in one of these components\nalone will likely not suffice as a corrective measure,\nnecessitating human intervention at different stages\nof the pipeline.\nMost work that addresses fairness in NLP ad-\ndresses it from an Anglo-centric context, with\ncomparatively significantly less work done in\ngrammatically-gendered and low-resource lan-\nguages. Their inability to capture social and cul-\ntural nuances and demographic variations is well-\ndocumented (Talat et al., 2022). Despite this, they\nare ubiquitous, with applications ranging diverse\nfields, from legal contexts to healthcare. That said,\nthere is insufficient documentation of the harms\nthat could stem from unfair models trained for\ndownstream tasks involving natural language gen-\neration, despite Arnold et al. (2018); Bhat et al.\n(2021); Buschek et al. (2021) indicating the influ-\nence of these systems on users. Apart from this,\nthese NLP systems also reinforce and reproduce\nthe social and racial hierarchies observed in society\nand fail to recognize underrepresented communi-\nties that are already marginalized (Dev et al., 2021;\n2106\nLauscher et al., 2022b). The ramifications of ne-\nglecting these issues are diverse and far-reaching,\nfrom minor inconveniences for users in less harm-\nful contexts to compromising their privacy as well\nas depriving them of opportunities and resources\n(Cirillo et al., 2020; Köchling and Wehner, 2020).\nFinally, while the interplay and tradeoff between\nprivacy, efficiency, and fairness in tabular data\nhas received extensive examination (Hooker et al.,\n2020; Lyu et al., 2020) comparatively fewer studies\nhave been conducted in NLP (Tal et al., 2022; Ahn\net al., 2022; Hessenthaler et al., 2022).\nThe contributions of this work center around\ndrawing attention to the current state of research\non fairness in the context of linguistic and cultural\nissues in non-English languages and in the context\nof multilingual models. While thorough survey\nstudies such as Sun et al. (2019); Stanczak and Au-\ngenstein (2021); Bhatt et al. (2022) yield valuable\ninsights into some of these aspects, none address\nthe current state of the work in multilingual fairness.\nOur paper provides insights into the following:\n• This work surveys and presents challenges and\nunanswered questions with respect to fairness\nin both monolingual and multilingual NLP.\n• We analyze bias from both a linguistic and\ncultural lens for non-English languages and\npresent a comprehensive overview of the lit-\nerature in bias pertaining to grammatically\ngendered languages and multilinguality.\n• We bring to the forefront challenges in multi-\nlingual fairness and begin a dialogue for cre-\nating more equitable systems for multilingual\nNLP.\n2 Bias in Monolingual Setups for English\n2.1 Metrics for Measurement\nPrior to delving into the complexities of fairness in\nmultilingual systems, it is essential to first examine\nthe prevalent biases and challenges in monolin-\ngual systems. By prefacing the discussion on bias\nin multilingual systems with an overview of the\ncurrent state of fairness evaluation and identifying\nareas for improvement, we aim to shed light on the\npotential for similar issues to arise in multilingual\nsystems, as many of the biases present in monolin-\ngual systems are likely to persist in multilingual\ncontexts. Some of the initial work on analyzing\nbiases in NLP models (Bolukbasi et al., 2016) pro-\npose quantitative measures of evaluating bias in\nword embeddings. Broadly speaking, bias mea-\nsures are subcategorized into i) intrinsic and ii)\nextrinsic measures. Intrinsic metrics quantify bias\nin the model’s pre-trained representations, whereas\nextrinsic metrics deal with bias observed in the out-\nputs of the downstream task the model is trained\nfor.\nCaliskan et al. (2017); May et al. (2019);\nNadeem et al. (2021); Nangia et al. (2020) are com-\nmonly used in papers evaluating language mod-\nels for fairness. Caliskan et al. (2017) proposes\nthe Word Embedding Association Test (WEAT). A\nfundamental criticism of WEAT is that it can be\nexploited to overestimate the bias in a model (Etha-\nyarajh et al., 2019). The Sentence Encoder Asso-\nciation Test (SEAT) metric (May et al., 2019) was\nproposed to address WEAT’s limitation of measur-\ning bias only over static word embeddings. SEAT\nis an adaptation of WEAT that allows us to measure\nbias over contextualized embeddings.\nStereoSet (Nadeem et al., 2021), and CrowS-Pair\n(Nangia et al., 2020) are crowdsourced datasets\nspecifically geared toward measuring the model’s\nstereotypical proclivity over multiple dimensions,\nwhich are inclusive of gender, race, and reli-\ngion, among others. Blodgett et al. (2021) points\nout the flaws in the data quality, such as invalid\nstereotype/anti-stereotype pairs, reliance on indi-\nrect group identifiers as a proxy for demographic\nidentification, and logical incongruities in the sen-\ntence pairs.\nSeveral other intrinsic measures and adaptations\nof the aforementioned ones have also been pro-\nposed (Kurita et al., 2019; Webster et al., 2020;\nKaneko and Bollegala, 2021; Lauscher et al., 2021).\nRecent studies (Delobelle et al., 2022; Meade et al.,\n2022) that perform comparative evaluations across\nthese measures provide valuable insights into how\nand where the metrics can be used, along with their\npotential drawbacks.\n2.2 Intrinsic vs Extrinsic Evaluation\nWhile intrinsic measures are valuable in that they\nindicate the existence of representational bias in\nsystems, the current literature on fairness evalua-\ntion largely concentrates on intrinsic metrics alone.\nConsiderably less work has been done on address-\ning bias in extrinsic evaluation, with several down-\nstream tasks needing concrete metrics to evaluate\nbias in their outputs. This is a pressing issue due to\nthe lack of correlation between intrinsic and extrin-\n2107\nsic measures (Goldfarb-Tarrant et al., 2020; Cao\net al., 2022; Delobelle et al., 2022). As emphasized\nin Orgad and Belinkov (2022), incorporating ex-\ntrinsic evaluation measures is crucial for several\nreasons, including the greater relevance of these\nmetrics to bias mitigation objectives. Aside from\nthis, evaluating fairness on the downstream task’s\noutputs allows us to gauge more precisely how a\nparticular demographic may be affected by the bi-\nases in the system.\nAlthough work done in fairness evaluation in\nNLP primarily concentrates on monolingual stud-\nies, there remain several unanswered questions and\ninconclusive results. For instance, although May\net al. (2019) claims to use semantically bleached\ntemplates, experiments in Delobelle et al. (2022)\nsuggest that they retain some degree of semantic\nsignificance. While several bias evaluation meth-\nods use template-based data, recent findings (Al-\nnegheimish et al., 2022) suggest that this approach\nmay be unreliable and advocate the use of natural\nsentence prompts.\n2.3 Fairness From the Lens of Multiple Social\nDimensions\nThe focus of much of the existing body of literature\nis on gender bias, with little that covers other di-\nmensions like race and religion. Evaluation metrics\nshould be able to evaluate harms in language mod-\nels over the intersectionality of multiple identities,\nakin to what would realistically be expected in real-\nworld data. While previous research (Talat et al.,\n2022; Kirk et al., 2021) has emphasized the im-\nportance of fairness evaluation and mitigation over\nintersectional identities, there is relatively sparse\nwork that attempts to address the same (Tan and\nCelis, 2019; Subramanian et al., 2021; Hassan et al.,\n2021; Lalor et al., 2022; Câmara et al., 2022). It\nis also crucial to gauge if reducing bias across one\ndimension could affect biases in the other dimen-\nsions. Most fairness measures do not account for\nthe intersectionality of identities and standards of\njustice outside the predominantly Western sphere\nof distributive justice (Sambasivan et al., 2021;\nLundgard, 2020).\nWhilst there has been an increase in proposing\nnovel methods to mitigate bias in language mod-\nels, there needs to be more work in benchmarking\nthese debiasing techniques to assess their relative\neffectiveness. Meade et al. (2022) represents a step\nforward in this direction. Despite criticism (Etha-\nyarajh et al., 2019; Blodgett et al., 2021) of some\nevaluation metrics, they are still consistently used\n(and not always in conjunction with other metrics)\nin bias evaluation studies.\n3 Linguistic Aspects\nThe linguistic variations between languages pose\nadditional problems in the realm of multilingual\nNLP. Take, for example, the concept of gen-\nder, which has multiple definitions in linguistic\nterms (namely, grammatical, referential, lexical\nand bio-social gender) (Stanczak and Augenstein,\n2021). Section 3.1 delves into how the grammati-\ncally gendered nature of languages can affect bias\nin multilingual and monolingual spaces alike. Ref-\nerential gender, on the other hand, deals with terms\nthat referentially address a person’s gender, such\nas pronouns. Terms that non-referentially describe\ngender fall under the umbrella of lexical gender,\nand the bio-social definition of gender involves\na mixture of phenotypic traits, gender expression,\nand identity as well as societal and cultural aspects\nthat influence them (Ackerman, 2019).\nAlthough initial forays into this field investigate\nbias caused by grammatical gender, problems in\nthese systems can also crop up due to the other\ndefinitions of gender. Referential gender terms are\nnot always aligned when used in conjunction with\nlexically gendered terms, particularly with respect\nto pronoun-based anaphors for queer-identifying\nindividuals. Several default assumptions regard-\ning the individual’s gender identity are made as a\nconsequence (Cao and Daumé III, 2021).\nThere are multiple varying forms of pronoun\ncomplexity (Lindström, 2008; Ballard, 1978).\nApart from this, there are instances of substantial\nvariations in their linguistic forms even among lan-\nguages within a specific region, as highlighted in\nNair (2013). Linguistics also involves the presence\nof constructs like deictic pronouns and honorific\npronouns (Goddard, 2005), which in some cases\ncan lead to the pronouns used to reference someone\nchanging based on their social dynamic within the\ncommunity (Lauscher et al., 2022c). These linguis-\ntic aspects represent another line of work that must\nbe addressed for lower-resourced communities that\ncommunicate using languages that utilize these.\nLexical gender, while non-referential, finds its\nown challenges due to the variation of these terms\nacross languages. For example, while certain rela-\ntionships with individuals in a family may have an\n2108\nexact mapping in other languages, more often than\nnot (particularly with Southeast Asian languages),\nthere is no precise mapping, and the system ends\nup making an approximation or ignoring the term\naltogether. Such issues may also be likely to per-\nforate to other axes such as race, religion, caste,\nand so forth. In particular, considering that one\nmethod of training multilingual embeddings relies\non alignment-based approaches, it is imperative\nthat we keep in mind how these design choices\ncould affect the representations of these terms.\nWhilst utilizing linguistic features in methods\nto evaluate and mitigate gender bias is a relatively\nnew field of study, previous work has demonstrated\nthat additional linguistic context can result in per-\nformance gains (V olkova et al., 2013; Wallace et al.,\n2014), thus in alignment with the claim from Hovy\nand Yang (2021) that LMs must utilize social con-\ntext to be able to reach human-level performance on\ntasks. Sun et al. (2021) utilizes linguistic features\nto capture cross-cultural similarities, and thus, to\nselect languages that are optimal for cross-lingual\ntransfer. However, it is essential to acknowledge\nthat languages are susceptible to cultural and lin-\nguistic shifts that occur at both global and local\nlevels over time, as noted in Hamilton et al. (2016).\nPretrained models also have the capability to em-\nbed sociodemographic information, as evinced by\nLauscher et al. (2022a).\nIt has also been noted that other linguistic forms\nof gender do not translate well to sociological gen-\nder (Cao and Daumé III, 2021). Furthermore, the\nscarcity of non-binary gender options in different\nlanguages can lead to the misgendering of non-\nbinary individuals in these languages, as they may\nbe constricted to fit into a binarized definition of\nsociological gender.\n3.1 Grammatically Gendered Languages\nLinguistics recognizes multiple forms of gender\n(Cao and Daumé III, 2020), as observed in gram-\nmatically gendered languages where most or all\nnouns, including those referring to inanimate ob-\njects, possess a syntactic concept of gender. These\nlanguages can have anywhere between 2 to 20\nforms of grammatical gender divisions. There has\nbeen an almost exclusive focus on English for eval-\nuating gender bias, even in the setting of mono-\nlingual models and systems. English, however, is\nnot a grammatically-gendered language. This may\nlimit the transferability of techniques used for bias\nevaluation and mitigation to other languages that\nare grammatically gendered.\nZhou et al. (2019) examines bias from the view\nof grammatically gendered languages by decom-\nposing the gendered information of words in the\nembedding space into two components; i) semantic\nand ii) syntactic. For instance, the Spanish word\nfor \"man\" (hombre) is both semantically and syn-\ntactically gendered. However, the Spanish word\nfor \"water\" (agua) is not semantically gendered but\nis considered a feminine noun. The proximity of\nfemale occupation words to the feminine side and\nmale occupation words to the masculine side of the\nsemantic gender direction suggests the presence\nof bias in these Spanish embeddings. Zhou et al.\n(2019) also demonstrates via experiments on bilin-\ngual embeddings that, post-alignment, masculine-\ngendered words are closer to the English equivalent\nof the occupation words than feminine-gendered\nones. The paper also proposes bias mitigation meth-\nods and demonstrates that the quality of the em-\nbeddings is preserved via word-translation exper-\niments. Nevertheless, the validity of these mitiga-\ntion measures would need to be verified by testing\nthem on downstream tasks. Gonen et al. (2019)\nshow that grammatical gender affects the word rep-\nresentations in Italian and German and that inan-\nimate nouns end up being closer to words of the\nsame gender. They propose to address this through\nthe precise use of a language-specific morphologi-\ncal tool and a careful approach to removing all the\ngender signals from a given text.\nThe grammatical properties of a language might\nshow some interesting properties to be taken into\naccount when dealing with the fairness of large lan-\nguage models, particularly for gender bias. Studies\ndirected toward them could yield insights into ob-\nservable trends across language families, with Go-\nnen et al. (2019) demonstrating how the alignment\nof languages in the embedding space is negatively\naffected by grammatical gender. They could also\nprove helpful when analyzing bias in multilingual\nmodels, where both grammatically gendered and\nnon-gendered languages are aligned to the same\nembedding space. The research and datasets avail-\nable for extrinsic evaluation over other languages\nremain an area with scope for improvement.\nApart from these grammatical properties that\naffect the results we observe, the translation of\nexisting bias evaluation datasets into other lan-\nguages to create parallel corpora does not suffice\n2109\nwhen dealing with languages apart from English.\nThis is partly because most languages are inher-\nently rooted in cultural context. Any data cu-\nrated for these languages must incorporate socio-\ncultural and linguistic aspects unique to the lan-\nguage/region. Depriving NLP systems of cultural\ncontext could consequently lead to entire axes over\nwhich social biases are measured being ignored.\nThe cultural significance of words and phrases in\nvarious languages can vary significantly, as demon-\nstrated in Mohamed et al. (2022), as well as in char-\nacteristics such as metaphorical tendencies (Gutiér-\nrez et al., 2016) and communication styles (Miehle\net al., 2016; Suszczy´nska, 1999). Hovy and Yang\n(2021) includes an overview and critique of this in\nthe current state of NLP literature, which they claim\nadopts an oversimplified view and focuses on the\ninformation content alone while ignoring the social\ncontext of this content. Milios and BehnamGhader\n(2022); España-Bonet and Barrón-Cedeño (2022)\nillustrate the inefficiency of direct translation meth-\nods, and España-Bonet and Barrón-Cedeño (2022)\nadvocates for the creation of culturally-sensitive\ndatasets for fairness assessment. However, Kaneko\net al. (2022) proposes a way to generate parallel\ncorpora for other languages that bears high correla-\ntion with human bias annotations.\n4 Multilingual Models\nMultilingual spaces allow the embeddings of multi-\nple languages to be aligned so that the mappings of\nevery word to its equivalent in other languages are\nclose to each in these embedding spaces. There are\nnumerous ways of training multilingual language\nmodels (Hedderich et al., 2021) using monolin-\ngual and unlabeled data. Multilingual language\nmodels can improve cross-lingual performance on\nlow-resource languages leveraging the data avail-\nable to higher-resourced languages up to a certain\nnumber of languages. Beyond a point, however,\nthe performance across these languages on cross-\nlingual and monolingual tasks begins to dip as the\nnumber of languages increases (Conneau et al.,\n2020). However, few studies explore the impact\nof multilingual training on biases. Hovy and Yang\n(2021) illustrate how language and culture share\na strong association, and Khani et al. (2021); Sun\net al. (2021) reveal that geographical and cultural\nproximity among languages could enhance the per-\nformance of models.\nLanguages provide much insight into a society’s\ncultural norms, ideologies, and belief systems (Her-\nshcovich et al., 2022; Wilson et al., 2016). Often,\nthe properties unique to a language are not clearly\nmapped to other languages or even other dialects\nwithin a language, with no direct translations avail-\nable for several phrases and terminology. Whether\nor not language models can retain this cultural in-\nformation and context while utilizing information\nfrom higher-resourced languages still requires in-\nvestigation.\n4.1 An Outline of Fairness Evaluation in the\nContext of Multilinguality\nSeveral datasets have been put forward for the pur-\npose of multilingual evaluation, and Table 1 de-\nscribes these datasets along with details regard-\ning their utility. These include the languages they\ncover, whether or not they evaluate bias over pre-\ntrained representations or a downstream task, and\nthe downstream tasks and dimensions they cater\ntoward.\nZhao et al. (2020) was among the first papers to\nquantify biases in multilingual spaces and does so\nusing both extrinsic and intrinsic evaluation tech-\nniques. Their findings indicate that some factors\nthat influence bias in multilingual embeddings in-\nclude the language’s linguistic properties, the target\nlanguage used for the alignment of the embeddings,\nand transfer learning on these embeddings induces\nbias. Additionally, there is the possibility that non-\nGermanic languages do not align well with Ger-\nmanic ones, and further work would be required to\nderive conclusions as to how this affects fairness\nmeasurements.\nHuang et al. (2020) released the first multilin-\ngual Twitter corpus for hate speech detection, anno-\ntated with the author’s demographic attributes (age,\ncountry, gender, race/ethnicity), which allows for\nfairness evaluation across hate speech classifiers.\nThrough experiments, they prove that variations\nin language, which are highly correlated with de-\nmographic attributes (Preo¸ tiuc-Pietro and Ungar,\n2018; Osiapem, 2007), can result in biased classi-\nfiers. However, there are some promising results\nfrom Liang et al. (2020), which proposes a novel\ndebiasing method using Dufter and Schütze (2019).\nWhile the multilingual model is originally debi-\nased over English, results show its effectiveness for\nzero-shot debiasing over Chinese.\nCâmara et al. (2022) measures both unisectional\nand intersectional social biases over gender, race,\n2110\nDataset Languages Task Metric Dimensionshttps://github.com/MSR-LIT/MultilingualBiasEnglish, Spanish, German, FrenchText ClassificationI, E Genderhttps://github.com/xiaoleihuang/DomainFairnessEnglish, Italian, Portuguese, SpanishText ClassificationE Gender\nhttps://github.com/kanekomasahiro/bias_eval_in_multiple_mlmGerman, Japanese, Arabic, Spanish,Portuguese, Russian, Indonesian, ChineseMasked Language ModellingI Gender\nhttps://github.com/ascamara/ml-intersectionalityEnglish, Arabic, Spanish Text ClassificationE Gender, Race/Ethnicity,Intersectionhttps://github.com/liangsheng02/densray-debiasing/English, Chinese Masked Language ModellingI Gender\nhttps://github.com/xiaoleihuang/Multilingual_Fairness_LRECEnglish, Italian, Portuguese,Spanish, Polish Text ClassificationE Age, Country, Gender,Race/Ethnicity\nhttps://github.com/coastalcph/fairlex English, German, French,Italian and Chinese Text ClassificationE Gender, Age, Region,Language, Legal Area\nTable 1: Datasets for fairness evaluation beyond English. I = Intrinsic, E = Extrinsic\nand ethnicity in multilingual language models. This\nis particularly relevant, as in a practical setting,\ntreating identities as composites of various demo-\ngraphic attributes is a necessity. Kaneko et al.\n(2022) measures gender bias in masked language\nmodels and proposes a method to use parallel cor-\npora to evaluate bias in languages shown to have\nhigh correlations with human bias annotations. In\ncases where manually annotated data doesn’t exist,\nthis could prove helpful.\nAlthough there has been research on fairness in\nmultimodal contexts (Wolfe and Caliskan, 2022;\nWolfe et al., 2022), in a first-of-its-kind study,\nWang et al. (2022) looks at fairness from a multi-\nlingual view in multimodal representations. Whilst\nthey find that multimodal representations may be\nindividually fair, i.e., similar text representations\nacross languages translate to similar images, this\nconcept of fairness does not extend across multiple\ngroups.\nTalat et al. (2022) expresses criticism over the\nprimary data source for multilingual large language\nmodels being English, which they claim is reflec-\ntive of cultural imperialism. They also advocate\nfor these models to be used only for languages\nthey have been trained for to retain the cultural\ncontext unique to a language. The multilingual\ndatasets commonly used tend to be parallel corpora\nderived directly from English translations, neglect-\ning the socio-cultural nuances specific to a given\nlanguage, as evidenced by the CommonCrawl cor-\npora (Dodge et al., 2021).\nMoreover, recent literature (Al Kuwatly et al.,\n2020; Parmar et al., 2022; Sap et al., 2022) presents\nus with yet another potential issue; lack of demo-\ngraphic variation in the annotation of these dataset\nresults could contribute to bias in the pipeline. As\nof yet, several languages (Aji et al., 2022; Joshi\net al., 2020) (such as Hindi, Arabic, and Indonesian,\nwhich have tens to hundreds of million of native\nspeakers) have had little to no fairness benchmark-\ning datasets developed for them, an indicator that\nmuch remains to be done to develop more equitable\nlanguage models.\n4.2 An Outline of Fairness Mitigation in the\nContext of Multilinguality\nDue to multilingual spaces being a composite of\nthe embeddings of various languages with different\nlinguistic and semantic properties, it would serve\nmitigation techniques well to consider these differ-\nences. Other methods could use these distinctions\nto reduce bias in downstream tasks. Zhao et al.\n(2020), for one, show that balancing the corpus\nand transferring it to a grammatically gendered lan-\nguage’s embedding space could reduce bias, and\nthat using debiased embeddings could also aid with\nbias mitigation.\nHuang (2022) takes inspiration from the FEDA\ndomain adaption technique (Daumé III, 2007) to\nuse it to mitigate bias in multilingual text classi-\nfication and compares this with other mitigation\nmethods. These debiasing baselines involve adver-\nsarial training, masking out tokens associated with\ndemographic groups, and instance weighting to re-\nduce the impact of data instances that could lead to\nmore biased classifiers. While Liang et al. (2020)\nshow that zero-shot debiasing can be beneficial for\nthis purpose, further study would be required to\nascertain if this is a feasible possibility.\n4.3 Problems in Multilingual Evaluation and\nMitigation\nA major challenge in multilingual fairness is the\nlack of datasets (including parallel corpora) and\nliterature for evaluation across tasks. Much of\nthe research conducted in monolingual contexts\nhas yet to be replicated in a multilingual setting,\nwhich would enable us to determine whether or\nnot bias trends in monolingual spaces are directly\ntransferable to multilingual contexts. Research and\ndata resources also tend to neglect less-represented\n2111\ndemographics, notably those local to a particular\nregion. Further, datasets require thorough docu-\nmentation, as variations in annotator information\ncan result in different types of biases infiltrating\nthe pipeline (Mohamed et al., 2022; Joshi et al.,\n2016; Bracewell and Tomlinson, 2012). These\ncould include attitudes towards other cultures and\nlanguages, which must be assessed and reported\nduring data collection. Multilingual users speak\nmultiple languages, and there is no work on evalu-\nating bias in language contact settings such as code-\nswitching. Certain axes along which systems may\ndiscriminate may be contained to a given region.\nDue to the underrepresented nature of marginalized\nidentities (such as immigrant communities), mod-\nels will likely not learn useful representations of\nthese identities.\n5 Culture\nLanguage and culture are intrinsically linked with\neach other. However, NLP research has historically\nplaced a considerable emphasis on the information\ncontent of the data, as opposed to the contextual\ninformation surrounding the same data. Hovy and\nYang (2021) propose a broad taxonomy of 7 so-\ncial factors that encompasses various aspects of\nthis contextual information. This could be incor-\nporated into models to improve performance and\nmake them aware from a socio-cultural perspective.\nThe differences between a pair of languages or\neven a pair of dialects could reflect across multi-\nple attributes; this could lead to variations in lan-\nguage’s phonology, tone, text, and lexical forms .\nSome of these attributes are controlled by the\nspeaker and receiver involved. Despite evidence of\ngains in performance by leveraging these features,\nsystems still retain the potential to discriminate\nagainst marginalized communities, as evinced in\nSap et al. (2019). This necessitates the proposal of\nevaluation methods to analyze the potential harms\nthat people from different cultural backgrounds\nmight expose themselves to via the use of such\nsystems.\nMultilingualism also entails the need to navi-\ngate the nuances of language, including the poten-\ntial for stereotypes and discriminatory language,\nwhich may not have precise equivalents in other\nlanguages. Cultural taboos and stereotypes can\nbe highly localized. As an example, pregnant or\nlactating women are discouraged from consuming\nnutritious food in certain cultures (Meyer-Rochow,\n2009). Such contextual information might be un-\nderrepresented or nonexistent in the data that the\nmodel is exposed to. While some culture-specific\nbehaviors may be prohibited or frowned upon in\nsome parts of the world, there are yet other places\nthat may encourage or remain indifferent to these\nvery same behaviors.\nAdditionally, the axes we consider require to be\ntreated differently in different cultural and linguis-\ntic settings. Take, for instance, gender. While gen-\nder has, for the most part, been treated as a binary\nvariable in these studies, this does not echo what is\nobserved in real-world settings, where several indi-\nviduals have non-binary gender identities (Devin-\nney et al., 2022). Non-binary gender identities en-\ncompass a broad spectrum of gender identities, and\nthe term is generally considered an umbrella term\nfor any identity outside the binary. The inability of\nmodels to incorporate this additional information\non gender has subsequently led to them developing\nmeaningless representations of non-binary genders\nin text (Dev et al., 2021). This translates to the\nsystematic erasure of their identities. Baumler and\nRudinger (2022) show that much remains to be\ndone concerning addressing non-binary identities\noutside the Western context. For instance, several\nnon-binary identities, such as the Aravanis and the\nM¯ah¯us (local to India and Hawaii, respectively) are\nlikely to have little to no meaningful coverage in\nthe training data of the models. These identities\ncan also have unanswered nuances in literature;\nfor example, the Acaults of Myanmar do not con-\nsider transsexualism, transvestism, and homosexu-\nality to be distinct categories. This is also applica-\nble to languages such as Arabana-Wangkangurru,\nwhich make use of deictic pronouns (previously\ndiscussed in Section 3) (Lauscher et al., 2022c;\nHercus, 1994).\nFurther, given that models are highly suscept to\nthe kind of data they are trained on, it is unlikely\nthat our models can recognize that certain forms\nof prejudice are more frequent in specific socio-\ncultural environments than others. The targets of\nthis discrimination are also likely to vary from re-\ngion to region, another nuance that models find dif-\nficult to account for. India and Nepal, for instance,\nare two countries that still suffer from the effects of\nthe hierarchy of a historically caste-based society\nthat (despite sharing similar roots) bear differences\nin terms of representation of the various castes and\nhow they are referred to (Jodhka et al., 2010; Rao,\n2112\n2010). It is important to note that the ability of a\nsystem to incorporate information from these so-\ncial factors to mitigate biases is task-dependent.\nDownstream tasks like machine translation and di-\nalogue/response generation may depend more on\ncues related to speaker and receiver characteristics\nfrom the taxonomy proposed in Hovy and Yang\n(2021) than other tasks. Extrinsic metrics for ma-\nchine translation focus primarily on the gender bias\nof the mappings of nouns and pronouns from one\nlanguage to another (Cho et al., 2019). On the other\nhand, more open-ended, subjective tasks like NLG\nare prone to encoding underlying biases and stereo-\ntypes across multiple axes and reproducing these\nin their outputs (Henderson et al., 2018).\nIt is critical to consider intersectionality in these\nstudies, as every individual is a composite of multi-\nple identities across multiple axes. When conduct-\ning inquiries into the biased nature of these systems,\nwe encourage researchers to use metrics that treat\nfairness as an intersectional concept and keep in\nline with the recommendations as suggested in Ta-\nlat et al. (2022); Blodgett et al. (2020) to document\nthe affected demographics. Testing the validity\nand reliability of bias measurement and debiasing\nmetrics is essential to ensuring the effectiveness of\nproposed methods (Blodgett et al., 2020), and it is\ncrucial to report any limitations of the same.\n6 Moving Towards Inclusive Systems in\nAll Languages\nThe issue of fairness in multilingualism presents\na number of challenges. Although current prac-\ntitioners encourage making systems multicultural\nand developing systems to be used only for spe-\ncific cultural contexts (Talat et al., 2022), we posit\nthat this may not be a viable solution due to vari-\nous practical considerations. The vast diversity of\ncultures and ethnicities across the world presents\nsignificant difficulties in terms of creating equitable\nmultilingual systems. Even within languages such\nas English, several dialectal variants, both of the\nregional and social kind (Nguyen et al., 2016), still\nneed to be accounted for. Blodgett and O’Connor\n(2017) is an example of how this could further\nstigmatize oppressed communities. Language and\nvarious social aspects related to language are ever-\nevolving. Modeling aspects such as lexical variants\nand the syntactical difference between languages,\nelements like phonology, and speech inflections in\nspoken language could contribute to the complexity\nof these systems.\nSeveral countries have diverse concentrations of\npeople from all regions of the world with unique\nbackgrounds. The intricacies of the social inter-\nactions resulting from the population’s diverse lin-\nguistic backgrounds and issues arising from lan-\nguage contact make the study of the fairness of mul-\ntilingual systems that would be deployed to cater\nto these populations essential. It is not possible to\nmake models agnostic to demographic attributes.\nEven with the omission of certain attributes, mod-\nels can still exhibit bias based on factors such as\nlinguistic variations in dialect, or the linguistic fea-\ntures employed, as demonstrated by Hovy and Sø-\ngaard (2015) who highlight the improved perfor-\nmance of NLP systems on texts written by older\nindividuals. The data that large language models\n(LLMs) are trained on tends to be biased towards\ncertain demographic strata (Olteanu et al., 2019).\nAlthough curating more diverse datasets and fol-\nlowing recommendations to mitigate bias in the\ndata pipeline would be a step forward to mitigat-\ning this problem (B et al., 2021), various resource\nconstraints could hinder this or make it impractical.\nDue to all these challenges and the ubiquity of\nlanguage technologies that are used by large popu-\nlations of non-English speaking users, addressing\nfairness and bias, taking into account diverse lin-\nguistic, socio-linguistic, and cultural factors, is of\nutmost importance. Interdisciplinary and multicul-\ntural teams are crucial to identifying, measuring,\nand mitigating harms caused by bias in multilingual\nmodels. Better evaluation benchmarks covering di-\nverse linguistic phenomena and cultures will lead\nto better fairness evaluation.\nRegarding data collection, as discussed in Sec-\ntion 3.1, it would be prudent to avoid directly trans-\nlating datasets for training or evaluation in applica-\ntions where fairness is critical. As we have shown\nin this survey, it is not enough to collect datasets\nin multiple languages for measuring and mitigat-\ning bias, although even these are lacking for most\nlanguages worldwide. Zero-shot techniques that\nignore the cultural nuances of a language should be\nused with care in fairness-critical applications, as\nlinguistically similar languages may have different\ncultural values and vice versa. Finally, multilingual\nmodels and systems need to incorporate shared\nvalue systems that take into account diverse cul-\ntures, although some cultural differences may still\ngo unacknowledged.\n2113\nLimitations\nOur work surveys fairness literature in languages\nother than English, including bias measurement\nand mitigation strategies. Although we call out the\nfact that bias in literature is studied from an Anglo-\ncentric point of view, it is conceivable that we miss\nmany diverse perspectives on linguistic and cultural\naspects of bias in different languages and cultures\nof the world due to the relatively heterogeneous\nbackground (in terms of nationality, ethnicity and\nfield of study) of the authors. There may also be\nother relevant work in the social science literature\nthat we may have missed including in this survey.\nReferences\nLauren Ackerman. 2019. Syntactic and cognitive issues\nin investigating gendered coreference. Glossa, 4.\nJaimeen Ahn, Hwaran Lee, Jinhwa Kim, and Alice Oh.\n2022. Why knowledge distillation amplifies gender\nbias and how to mitigate from the perspective of Dis-\ntilBERT. In Proceedings of the 4th Workshop on Gen-\nder Bias in Natural Language Processing (GeBNLP),\npages 266–272, Seattle, Washington. Association for\nComputational Linguistics.\nAlham Fikri Aji, Genta Indra Winata, Fajri Koto,\nSamuel Cahyawijaya, Ade Romadhony, Rahmad Ma-\nhendra, Kemal Kurniawan, David Moeljadi, Radi-\ntyo Eko Prasojo, Timothy Baldwin, Jey Han Lau,\nand Sebastian Ruder. 2022. One country, 700+ lan-\nguages: NLP challenges for underrepresented lan-\nguages and dialects in Indonesia. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 7226–7249, Dublin, Ireland. Association for\nComputational Linguistics.\nHala Al Kuwatly, Maximilian Wich, and Georg Groh.\n2020. Identifying and measuring annotator bias\nbased on annotators’ demographic characteristics. In\nProceedings of the Fourth Workshop on Online Abuse\nand Harms, pages 184–190, Online. Association for\nComputational Linguistics.\nSarah Alnegheimish, Alicia Guo, and Yi Sun. 2022.\nUsing natural sentence prompts for understanding bi-\nases in language models. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2824–2830, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nKenneth C. Arnold, Krysta Chauncey, and Krzysztof Z\nGajos. 2018. Sentiment bias in predictive text recom-\nmendations results in biased writing. Proceedings of\nthe 44th Graphics Interface Conference.\nSenthil Kumar B, Aravindan Chandrabose, and\nBharathi Raja Chakravarthi. 2021. An overview\nof fairness in data – illuminating the bias in data\npipeline. In Proceedings of the First Workshop on\nLanguage Technology for Equality, Diversity and\nInclusion, pages 34–45, Kyiv. Association for Com-\nputational Linguistics.\nWilliam L. Ballard. 1978. More on yuchi pro-\nnouns. International Journal of American Linguis-\ntics, 44(2):103–112.\nConnor Baumler and Rachel Rudinger. 2022. Recog-\nnition of they/them as singular personal pronouns in\ncoreference resolution. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3426–3432, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big? . In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nAdvait Bhat, Saaket Agashe, and Anirudha Joshi. 2021.\nHow do people interact with biased text prediction\nmodels while writing? In Proceedings of the First\nWorkshop on Bridging Human–Computer Interaction\nand Natural Language Processing, pages 116–121,\nOnline. Association for Computational Linguistics.\nShaily Bhatt, Sunipa Dev, Partha Talukdar, Shachi\nDave, and Vinodkumar Prabhakaran. 2022. Re-\ncontextualizing fairness in NLP: The case of India. In\nProceedings of the 2nd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics and the 12th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 727–740, Online only. Association for\nComputational Linguistics.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna Wallach. 2021. Stereotyping\nNorwegian salmon: An inventory of pitfalls in fair-\nness benchmark datasets. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1004–1015, Online. Association\nfor Computational Linguistics.\n2114\nSu Lin Blodgett and Brendan O’Connor. 2017. Racial\ndisparity in natural language processing: A case\nstudy of social media african-american english.\nTolga Bolukbasi, Kai-Wei Chang, James Y . Zou,\nVenkatesh Saligrama, and Adam Kalai. 2016. Man is\nto computer programmer as woman is to homemaker?\ndebiasing word embeddings. CoRR, abs/1607.06520.\nDavid Bracewell and Marc Tomlinson. 2012. The lan-\nguage of power and its cultural influence. In Pro-\nceedings of COLING 2012: Posters, pages 155–164,\nMumbai, India. The COLING 2012 Organizing Com-\nmittee.\nDaniel Buschek, Martin Zurn, and Malin Eiband. 2021.\nThe impact of multiple parallel phrase suggestions on\nemail input and composition behaviour of native and\nnon-native english writers. Proceedings of the 2021\nCHI Conference on Human Factors in Computing\nSystems.\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nAntónio Câmara, Nina Taneja, Tamjeed Azad, Emily\nAllaway, and Richard Zemel. 2022. Mapping the\nmultilingual margins: Intersectional biases of sen-\ntiment analysis systems in English, Spanish, and\nArabic. In Proceedings of the Second Workshop on\nLanguage Technology for Equality, Diversity and In-\nclusion, pages 90–106, Dublin, Ireland. Association\nfor Computational Linguistics.\nYang Cao, Yada Pruksachatkun, Kai-Wei Chang, Rahul\nGupta, Varun Kumar, Jwala Dhamala, and Aram Gal-\nstyan. 2022. On the intrinsic and extrinsic fairness\nevaluation metrics for contextualized language repre-\nsentations. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 561–570, Dublin,\nIreland. Association for Computational Linguistics.\nYang Trista Cao and Hal Daumé III. 2020. Toward\ngender-inclusive coreference resolution. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 4568–4595, On-\nline. Association for Computational Linguistics.\nYang Trista Cao and Hal Daumé III. 2021. Toward\ngender-inclusive coreference resolution: An analysis\nof gender and bias throughout the machine learning\nlifecycle*. Computational Linguistics, 47(3):615–\n661.\nWon Ik Cho, Ji Won Kim, Seok Min Kim, and Nam Soo\nKim. 2019. On measuring gender bias in translation\nof gender-neutral pronouns. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 173–181, Florence, Italy. Associa-\ntion for Computational Linguistics.\nMonojit Choudhury and Amit Deshpande. 2021. How\nlinguistically fair are multilingual pre-trained lan-\nguage models? Proceedings of the AAAI Conference\non Artificial Intelligence, 35(14):12710–12718.\nDavide Cirillo, Silvina Catuara-Solarz, Czuee Morey,\nEmre Guney, Laia Subirats, Simona Mellino, An-\nnalisa Gigante, Alfonso Valencia, María José Re-\nmenteria, Antonella Santuccione Chadha, and Niko-\nlaos Mavridis. 2020. Sex and gender differences and\nbiases in artificial intelligence for biomedicine and\nhealthcare. npj Digital Medicine, 3(1):81.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nHal Daumé III. 2007. Frustratingly easy domain adap-\ntation. In Proceedings of the 45th Annual Meeting of\nthe Association of Computational Linguistics, pages\n256–263, Prague, Czech Republic. Association for\nComputational Linguistics.\nPieter Delobelle, Ewoenam Tokpo, Toon Calders, and\nBettina Berendt. 2022. Measuring fairness with bi-\nased rulers: A comparative study on bias metrics\nfor pre-trained language models. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1693–1706,\nSeattle, United States. Association for Computational\nLinguistics.\nSunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Ar-\njun Subramonian, Jeff Phillips, and Kai-Wei Chang.\n2021. Harms of gender exclusivity and challenges in\nnon-binary representation in language technologies.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1968–1994, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nHannah Devinney, Jenny Björklund, and Henrik Björk-\nlund. 2022. Theories of “gender” in nlp bias research.\nIn 2022 ACM Conference on Fairness, Accountabil-\nity, and Transparency, FAccT ’22, page 2083–2102,\nNew York, NY , USA. Association for Computing\nMachinery.\nJesse Dodge, Maarten Sap, Ana Marasovi ´c, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, Margaret\nMitchell, and Matt Gardner. 2021. Documenting\nlarge webtext corpora: A case study on the colos-\nsal clean crawled corpus. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1286–1305, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\n2115\nPhilipp Dufter and Hinrich Schütze. 2019. Analytical\nmethods for interpretable ultradense word embed-\ndings. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n1185–1191, Hong Kong, China. Association for Com-\nputational Linguistics.\nCristina España-Bonet and Alberto Barrón-Cedeño.\n2022. The (undesired) attenuation of human biases\nby multilinguality. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages –, Online and Abu Dhabi, UAE.\nAssociation for Computational Linguistics.\nKawin Ethayarajh, David Duvenaud, and Graeme Hirst.\n2019. Understanding undesirable word embedding\nassociations. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1696–1705, Florence, Italy. Associa-\ntion for Computational Linguistics.\nCliff Goddard. 2005. The languages of east and south-\neast asia: An introduction.\nSeraphina Goldfarb-Tarrant, Rebecca Marchant, Ri-\ncardo Muñoz Sanchez, Mugdha Pandya, and Adam\nLopez. 2020. Intrinsic bias metrics do not correlate\nwith application bias.\nHila Gonen, Yova Kementchedjhieva, and Yoav Gold-\nberg. 2019. How does grammatical gender affect\nnoun representations in gender-marking languages?\nIn Proceedings of the 23rd Conference on Computa-\ntional Natural Language Learning (CoNLL), pages\n463–471, Hong Kong, China. Association for Com-\nputational Linguistics.\nE.D. Gutiérrez, Ekaterina Shutova, Patricia Lichtenstein,\nGerard de Melo, and Luca Gilardi. 2016. Detecting\ncross-cultural differences using a multilingual topic\nmodel. Transactions of the Association for Computa-\ntional Linguistics, 4:47–60.\nWilliam L. Hamilton, Jure Leskovec, and Dan Juraf-\nsky. 2016. Cultural shift or linguistic drift? compar-\ning two computational measures of semantic change.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2116–2121, Austin, Texas. Association for Computa-\ntional Linguistics.\nSaad Hassan, Matt Huenerfauth, and Cecilia Ovesdotter\nAlm. 2021. Unpacking the interdependent systems of\ndiscrimination: Ableist bias in nlp systems through\nan intersectional lens.\nMichael A. Hedderich, Lukas Lange, Heike Adel, Jan-\nnik Strötgen, and Dietrich Klakow. 2021. A survey\non recent approaches for natural language process-\ning in low-resource scenarios. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2545–2568,\nOnline. Association for Computational Linguistics.\nPeter Henderson, Koustuv Sinha, Nicolas Angelard-\nGontier, Nan Rosemary Ke, Genevieve Fried, Ryan\nLowe, and Joelle Pineau. 2018. Ethical challenges\nin data-driven dialogue systems. In Proceedings of\nthe 2018 AAAI/ACM Conference on AI, Ethics, and\nSociety, AIES ’18, page 123–129, New York, NY ,\nUSA. Association for Computing Machinery.\nLuise Hercus. 1994. A grammar of the arabana-\nwangkangurru language : Lake eyre basin, south\naustralia.\nDaniel Hershcovich, Stella Frank, Heather Lent,\nMiryam de Lhoneux, Mostafa Abdou, Stephanie\nBrandl, Emanuele Bugliarello, Laura Cabello Pi-\nqueras, Ilias Chalkidis, Ruixiang Cui, Constanza\nFierro, Katerina Margatina, Phillip Rust, and Anders\nSøgaard. 2022. Challenges and strategies in cross-\ncultural NLP. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 6997–7013,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nMarius Hessenthaler, Emma Strubell, Dirk Hovy, and\nAnne Lauscher. 2022. Bridging fairness and environ-\nmental sustainability in natural language processing.\nSara Hooker, Nyalleng Moorosi, Gregory Clark, Samy\nBengio, and Emily Denton. 2020. Characterising\nbias in compressed models.\nDirk Hovy and Shrimai Prabhumoye. 2021. Five\nsources of bias in natural language processing. Lan-\nguage and Linguistics Compass, 15(8):e12432.\nDirk Hovy and Anders Søgaard. 2015. Tagging perfor-\nmance correlates with author age. In Proceedings\nof the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), pages 483–488, Beijing,\nChina. Association for Computational Linguistics.\nDirk Hovy and Diyi Yang. 2021. The importance of\nmodeling social factors of language: Theory and\npractice. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 588–602, Online. Association\nfor Computational Linguistics.\nXiaolei Huang. 2022. Easy adaptation to mitigate\ngender bias in multilingual text classification. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 717–723, Seattle, United States. Association\nfor Computational Linguistics.\nXiaolei Huang, Linzi Xing, Franck Dernoncourt, and\nMichael J. Paul. 2020. Multilingual Twitter cor-\npus and baselines for evaluating demographic bias\nin hate speech recognition. In Proceedings of the\n2116\nTwelfth Language Resources and Evaluation Confer-\nence, pages 1440–1448, Marseille, France. European\nLanguage Resources Association.\nSurinder S. Jodhka, Ghanshyam Shah, Tudor Kalinga,\nP Silva, Paramsothy Sivapragasam, Thanges, Sri\nLanka, Uddin Iftekhar, Chowdhury, Bangladesh\nZulfi, Ali Shah, Krishna Bhattachan, Tej Sunar, Yasso\nBhattachan, Nepal Senapati, Sobin George, Martin\nMacwan, S Thorat, Vincent Manoharan, and Gowhar\nYakoob. 2010. Comparative contexts of discrimina-\ntion: Caste and untouchability in south asia. Eco-\nnomic and Political Weekly, 45.\nAditya Joshi, Pushpak Bhattacharyya, Mark Carman,\nJaya Saraswati, and Rajita Shukla. 2016. How do\ncultural differences impact the quality of sarcasm\nannotation?: A case study of Indian annotators and\nAmerican text. In Proceedings of the 10th SIGHUM\nWorkshop on Language Technology for Cultural Her-\nitage, Social Sciences, and Humanities, pages 95–99,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282–6293, Online. Association for Computational\nLinguistics.\nMasahiro Kaneko and Danushka Bollegala. 2021. Un-\nmasking the mask – evaluating social biases in\nmasked language models.\nMasahiro Kaneko, Aizhan Imankulova, Danushka Bol-\nlegala, and Naoaki Okazaki. 2022. Gender bias in\nmasked language models for multiple languages. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2740–2750, Seattle, United States. Association\nfor Computational Linguistics.\nNikzad Khani, Isidora Tourni, Mohammad Sadegh Ra-\nsooli, Chris Callison-Burch, and Derry Tanti Wijaya.\n2021. Cultural and geographical influences on image\ntranslatability of words across languages. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n198–209, Online. Association for Computational Lin-\nguistics.\nHannah Kirk, Yennie Jun, Haider Iqbal, Elias Benussi,\nFilippo V olpin, Frédéric A. Dreyer, Aleksandar Sht-\nedritski, and Yuki Markus Asano. 2021. How true is\ngpt-2? an empirical analysis of intersectional occu-\npational biases. CoRR, abs/2102.04130.\nAlina Köchling and Marius Claus Wehner. 2020. Dis-\ncriminated by an algorithm: a systematic review of\ndiscrimination and fairness by algorithmic decision-\nmaking in the context of hr recruitment and hr devel-\nopment. Business Research, 13(3):795–848.\nSachin Kumar, Vidhisha Balachandran, Lucille Njoo,\nAntonios Anastasopoulos, and Yulia Tsvetkov. Lan-\nguage generation models can cause harm: So what\ncan we do about it? an actionable survey.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in con-\ntextualized word representations. In Proceedings of\nthe First Workshop on Gender Bias in Natural Lan-\nguage Processing, pages 166–172, Florence, Italy.\nAssociation for Computational Linguistics.\nJohn Lalor, Yi Yang, Kendall Smith, Nicole Forsgren,\nand Ahmed Abbasi. 2022. Benchmarking intersec-\ntional biases in NLP. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3598–3609, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nAnne Lauscher, Federico Bianchi, Samuel R. Bowman,\nand Dirk Hovy. 2022a. SocioProbe: What, when,\nand where language models learn about sociodemo-\ngraphics. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 7901–7918, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nAnne Lauscher, Archie Crowley, and Dirk Hovy. 2022b.\nWelcome to the modern world of pronouns: Identity-\ninclusive natural language processing beyond gender.\nAnne Lauscher, Archie Crowley, and Dirk Hovy. 2022c.\nWelcome to the modern world of pronouns: Identity-\ninclusive natural language processing beyond gen-\nder. In Proceedings of the 29th International Con-\nference on Computational Linguistics, pages 1221–\n1232, Gyeongju, Republic of Korea. International\nCommittee on Computational Linguistics.\nAnne Lauscher, Tobias Lüken, and Goran Glavas. 2021.\nSustainable modular debiasing of language models.\nCoRR, abs/2109.03646.\nSheng Liang, Philipp Dufter, and Hinrich Schütze. 2020.\nMonolingual and multilingual reduction of gender\nbias in contextualized representations. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 5082–5093, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nEva Lindström. 2008. Language complexity and inter-\nlinguistic difficulty, page 217–242.\nFangyu Liu, Emanuele Bugliarello, Edoardo Maria\nPonti, Siva Reddy, Nigel Collier, and Desmond El-\nliott. 2021. Visually grounded reasoning across lan-\nguages and cultures. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 10467–10485, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\n2117\nAlan Lundgard. 2020. Measuring justice in machine\nlearning. In Proceedings of the 2020 Conference on\nFairness, Accountability, and Transparency. ACM.\nLingjuan Lyu, Xuanli He, and Yitong Li. 2020. Differ-\nentially private representation for nlp: Formal guar-\nantee and an empirical study on privacy and fairness.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measuring\nsocial biases in sentence encoders. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 622–628, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy.\n2022. An empirical survey of the effectiveness of\ndebiasing techniques for pre-trained language models.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1878–1898, Dublin, Ireland.\nAssociation for Computational Linguistics.\nVictor Benno Meyer-Rochow. 2009. Food taboos: their\norigins and purposes. Journal of Ethnobiology and\nEthnomedicine, 5(1):18.\nJuliana Miehle, Koichiro Yoshino, Louisa Pragst, Ste-\nfan Ultes, Satoshi Nakamura, and Wolfgang Minker.\n2016. Cultural communication idiosyncrasies in\nhuman-computer interaction. In Proceedings of the\n17th Annual Meeting of the Special Interest Group on\nDiscourse and Dialogue, pages 74–79, Los Angeles.\nAssociation for Computational Linguistics.\nAristides Milios and Parishad BehnamGhader. 2022.\nAn analysis of social biases present in bert variants\nacross multiple languages. ArXiv, abs/2211.14402.\nYoussef Mohamed, Mohamed Abdelfattah, Shyma\nAlhuwaider, Feifan Li, Xiangliang Zhang, Ken-\nneth Ward Church, and Mohamed Elhoseiny. 2022.\nArtelingo: A million emotion annotations of wikiart\nwith emphasis on diversity over language and culture.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356–5371, Online. Association for\nComputational Linguistics.\nRavi Sankar S Nair. 2013. Tribal languages of kerala.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. Crows-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models.\nDong Nguyen, A. Seza Do ˘gruöz, Carolyn P. Rosé,\nand Franciska de Jong. 2016. Computational So-\nciolinguistics: A Survey. Computational Linguistics,\n42(3):537–593.\nAlexandra Olteanu, Carlos Castillo, Fernando D. Diaz,\nand Emre Kıcıman. 2019. Social data: Biases,\nmethodological pitfalls, and ethical boundaries. Fron-\ntiers in Big Data, 2.\nHadas Orgad and Yonatan Belinkov. 2022. Choose your\nlenses: Flaws in gender bias evaluation. In Proceed-\nings of the 4th Workshop on Gender Bias in Natu-\nral Language Processing (GeBNLP), pages 151–167,\nSeattle, Washington. Association for Computational\nLinguistics.\nIyabo Osiapem. 2007. Florian coulmas, sociolinguis-\ntics: The study of speakers’ choices -. Language in\nSociety - LANG SOC, 36.\nMihir Parmar, Swaroop Mishra, Mor Geva, and Chitta\nBaral. 2022. Don’t blame the annotator: Bias already\nstarts in the annotation instructions.\nDaniel Preo¸ tiuc-Pietro and Lyle Ungar. 2018. User-level\nrace and ethnicity predictors from Twitter text. In\nProceedings of the 27th International Conference on\nComputational Linguistics, pages 1534–1545, Santa\nFe, New Mexico, USA. Association for Computa-\ntional Linguistics.\nJasmine Rao. 2010. The caste system: Effects on\npoverty in india, nepal and sri lanka. Glob. Majority\nE-J., 1.\nNithya Sambasivan, Erin Arnesen, Ben Hutchinson,\nTulsee Doshi, and Vinodkumar Prabhakaran. 2021.\nRe-imagining algorithmic fairness in india and be-\nyond. CoRR, abs/2101.09995.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A. Smith. 2019. The risk of racial bias\nin hate speech detection. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1668–1678, Florence, Italy. Asso-\nciation for Computational Linguistics.\nMaarten Sap, Swabha Swayamdipta, Laura Vianna,\nXuhui Zhou, Yejin Choi, and Noah A. Smith. 2022.\nAnnotators with attitudes: How annotator beliefs\nand identities bias toxic language detection. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5884–5906, Seattle, United States. Association for\nComputational Linguistics.\nKarolina Stanczak and Isabelle Augenstein. 2021. A\nsurvey on gender bias in natural language processing.\nCoRR, abs/2112.14168.\nShivashankar Subramanian, Xudong Han, Timothy\nBaldwin, Trevor Cohn, and Lea Frermann. 2021.\nEvaluating debiasing techniques for intersectional\nbiases.\nJimin Sun, Hwijeen Ahn, Chan Young Park, Yulia\nTsvetkov, and David R. Mortensen. 2021. Cross-\ncultural similarity features for cross-lingual transfer\n2118\nlearning of pragmatically motivated tasks. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 2403–2414, Online.\nAssociation for Computational Linguistics.\nTony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,\nMai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth\nBelding, Kai-Wei Chang, and William Yang Wang.\n2019. Mitigating gender bias in natural language\nprocessing: Literature review. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 1630–1640, Florence, Italy.\nAssociation for Computational Linguistics.\nMałgorzata Suszczy´nska. 1999. Apologizing in english,\npolish and hungarian: Different languages, different\nstrategies. Journal of Pragmatics, 31(8):1053–1065.\nYarden Tal, Inbal Magar, and Roy Schwartz. 2022.\nFewer errors, but more stereotypes? the effect of\nmodel size on gender bias. In Proceedings of the 4th\nWorkshop on Gender Bias in Natural Language Pro-\ncessing (GeBNLP), pages 112–120, Seattle, Wash-\nington. Association for Computational Linguistics.\nZeerak Talat, Aurélie Névéol, Stella Biderman, Miruna\nClinciu, Manan Dey, Shayne Longpre, Sasha Luc-\ncioni, Maraim Masoud, Margaret Mitchell, Dragomir\nRadev, Shanya Sharma, Arjun Subramonian, Jaesung\nTae, Samson Tan, Deepak Tunuguntla, and Oskar Van\nDer Wal. 2022. You reap what you sow: On the chal-\nlenges of bias evaluation under multilingual settings.\nIn Proceedings of BigScience Episode #5 – Workshop\non Challenges & Perspectives in Creating Large Lan-\nguage Models, pages 26–41, virtual+Dublin. Associ-\nation for Computational Linguistics.\nYi Chern Tan and L. Elisa Celis. 2019. Assessing so-\ncial and intersectional biases in contextualized word\nrepresentations. CoRR, abs/1911.01485.\nSvitlana V olkova, Theresa Wilson, and David Yarowsky.\n2013. Exploring demographic language variations\nto improve multilingual sentiment analysis in social\nmedia. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1815–1827, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nByron C. Wallace, Do Kook Choe, Laura Kertz, and\nEugene Charniak. 2014. Humans require context to\ninfer ironic intent (so computers probably do, too).\nIn Proceedings of the 52nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2:\nShort Papers), pages 512–516, Baltimore, Maryland.\nAssociation for Computational Linguistics.\nJialu Wang, Yang Liu, and Xin Wang. 2022. Assess-\ning multilingual fairness in pre-trained multimodal\nrepresentations. In Findings of the Association for\nComputational Linguistics: ACL 2022, pages 2681–\n2695, Dublin, Ireland. Association for Computational\nLinguistics.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beu-\ntel, Emily Pitler, Ellie Pavlick, Jilin Chen, and\nSlav Petrov. 2020. Measuring and reducing gen-\ndered correlations in pre-trained models. CoRR,\nabs/2010.06032.\nSteven Wilson, Rada Mihalcea, Ryan Boyd, and James\nPennebaker. 2016. Disentangling topic models: A\ncross-cultural analysis of personal values through\nwords. In Proceedings of the First Workshop on\nNLP and Computational Social Science, pages 143–\n152, Austin, Texas. Association for Computational\nLinguistics.\nRobert Wolfe and Aylin Caliskan. 2022. American\n== white in multimodal language-and-image ai. In\nProceedings of the 2022 AAAI/ACM Conference on\nAI, Ethics, and Society , AIES ’22, page 800–812,\nNew York, NY , USA. Association for Computing\nMachinery.\nRobert Wolfe, Yiwei Yang, Bill Howe, and Aylin\nCaliskan. 2022. Contrastive language-vision ai mod-\nels pretrained on web-scraped multimodal data ex-\nhibit sexual objectification bias.\nJieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini,\nKai-Wei Chang, and Ahmed Hassan Awadallah. 2020.\nGender bias in multilingual embeddings and cross-\nlingual transfer. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2896–2907, Online. Association for\nComputational Linguistics.\nPei Zhou, Weijia Shi, Jieyu Zhao, Kuan-Hao Huang,\nMuhao Chen, Ryan Cotterell, and Kai-Wei Chang.\n2019. Examining gender bias in languages with\ngrammatical gender. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 5276–5284, Hong Kong, China. As-\nsociation for Computational Linguistics.\n2119"
}