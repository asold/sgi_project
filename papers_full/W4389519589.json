{
    "title": "Harnessing LLMs for Temporal Data - A Study on Explainable Financial Time Series Forecasting",
    "url": "https://openalex.org/W4389519589",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2095874112",
            "name": "Xinli Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098664215",
            "name": "Zheng Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2112153945",
            "name": "Yan-bin Lu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3130139313",
        "https://openalex.org/W4379089709",
        "https://openalex.org/W3000577518",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2135282243",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W2768348081",
        "https://openalex.org/W2901366432",
        "https://openalex.org/W2012079387",
        "https://openalex.org/W4283768109",
        "https://openalex.org/W3124667040",
        "https://openalex.org/W2970066309",
        "https://openalex.org/W2896827527",
        "https://openalex.org/W1994411832",
        "https://openalex.org/W4319165821",
        "https://openalex.org/W2780490630",
        "https://openalex.org/W4363672099",
        "https://openalex.org/W4233713109",
        "https://openalex.org/W3113177135",
        "https://openalex.org/W2007519228",
        "https://openalex.org/W2947640734",
        "https://openalex.org/W2141077304",
        "https://openalex.org/W2121970262",
        "https://openalex.org/W3157054611",
        "https://openalex.org/W1986078433",
        "https://openalex.org/W3198281649",
        "https://openalex.org/W2122514273",
        "https://openalex.org/W4362707064",
        "https://openalex.org/W3137262131",
        "https://openalex.org/W2905922589",
        "https://openalex.org/W4361866125",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3004006062",
        "https://openalex.org/W2161078209",
        "https://openalex.org/W2068715670",
        "https://openalex.org/W1555686918",
        "https://openalex.org/W3007066689",
        "https://openalex.org/W2785336079",
        "https://openalex.org/W4230206799",
        "https://openalex.org/W2890340393",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W1894414046",
        "https://openalex.org/W3095883070",
        "https://openalex.org/W2017270018",
        "https://openalex.org/W1493047750",
        "https://openalex.org/W1967351845",
        "https://openalex.org/W1985611875",
        "https://openalex.org/W1968380673",
        "https://openalex.org/W2088794999",
        "https://openalex.org/W3211394146",
        "https://openalex.org/W3124263521",
        "https://openalex.org/W3121975202",
        "https://openalex.org/W2125736403",
        "https://openalex.org/W4385572162",
        "https://openalex.org/W1964166103",
        "https://openalex.org/W4206815600",
        "https://openalex.org/W3121675720",
        "https://openalex.org/W1999945375",
        "https://openalex.org/W1575726819",
        "https://openalex.org/W2741908740",
        "https://openalex.org/W2002849459",
        "https://openalex.org/W3190469032",
        "https://openalex.org/W85898607",
        "https://openalex.org/W2069240581"
    ],
    "abstract": "Applying machine learning to financial time series has been an active area of industrial research enabling innovation in market insights, risk management, strategic decision-making, and policy formation. This paper explores the novel use of Large Language Models (LLMs) for explainable financial time series forecasting, addressing challenges in cross-sequence reasoning, multi-modal data integration, and result interpretation that are inherent in traditional approaches. Focusing on NASDAQ-100 stocks, we utilize public historical stock data, company metadata, and economic/financial news. Our experiments employ GPT-4 for zero-shot/few-shot inference and Open LLaMA for instruction-based fine-tuning. The study demonstrates LLMs' ability to generate well-reasoned decisions by leveraging cross-sequence information and extracting insights from text and price time series. We show that our LLM-based approach outperforms classic ARMA-GARCH and gradient-boosting tree models. Furthermore, fine-tuned public LLMs, such as Open-LLaMA, can generate reasonable and explainable forecasts, although they underperform compared to GPT-4.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 739–753\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nHarnessing LLMs for Temporal Data\n- A Study on Explainable Financial Time Series Forecasting\nXinli Yu, Zheng Chen, Yanbin Lu\n{xinliy, zgchen, luyanbin}@amazon.com\nAbstract\nApplying machine learning to financial time\nseries has been an active area of industrial\nresearch enabling innovation in market in-\nsights, risk management, strategic decision-\nmaking, and policy formation. This paper ex-\nplores the novel use of Large Language Mod-\nels (LLMs) for explainable financial time se-\nries forecasting, addressing challenges in cross-\nsequence reasoning, multi-modal data integra-\ntion, and result interpretation that are inher-\nent in traditional approaches. Focusing on\nNASDAQ-100 stocks, we utilize public histor-\nical stock data, company metadata, and eco-\nnomic/financial news. Our experiments em-\nploy GPT-4 for zero-shot/few-shot inference\nand Open LLaMA for instruction-based fine-\ntuning. The study demonstrates LLMs’ ability\nto generate well-reasoned decisions by lever-\naging cross-sequence information and extract-\ning insights from text and price time series.\nWe show that our LLM-based approach outper-\nforms classic ARMA-GARCH and gradient-\nboosting tree models. Furthermore, fine-tuned\npublic LLMs, such as Open-LLaMA, can gen-\nerate reasonable and explainable forecasts, al-\nthough they underperform compared to GPT-4.\n1 Introduction\nThe rapid advancements in Machine Learning (ML)\nand Artificial Intelligence (AI) technologies over\nthe past few years have opened up numerous op-\nportunities and challenges across various domains,\nincluding the realm of financial markets (Kovaler-\nchuk and Vityaev, 2005; Bahrammirzaee, 2010;\nQi and Xiao, 2018). In particular, the task of fi-\nnancial time series forecasting, a key element in\nindustrial risk management, market insights, strate-\ngic decision-making and policy formation, has wit-\nnessed significant technological innovations, from\nstatistical/econometric time series techniques (Här-\ndle et al., 1997; Andersen et al., 2009; Chen et al.,\n2011; Patton, 2012), to machine learning tech-\nniques (Kim, 2003; Yoo et al., 2005; Krollner et al.,\n2010), to deep learning (Dingli and Fournier, 2017;\nJúnior and Nievola, 2018; Sezer et al., 2020; Leung\nand Zhao, 2021; Lara-Benítez et al., 2021). Despite\nthese advancements, there are several inherent chal-\nlenges associated with the deployment of ML/AI\nmodels in finance.\nOne challenge lies in the realm of cross-\nsequence reasoning and inference, a vital aspect\nfor understanding temporal patterns and making ac-\ncurate predictions. The current approaches include\ntime-series correlation analysis (Plerou et al., 1999;\nGopikrishnan et al ., 2000; Conlon et al ., 2009;\nChen et al., 2018) and clustering (Rani and Sikka,\n2012; Babu et al., 2012; Aghabozorgi et al., 2015).\nDeep learning has recently been leveraged to learn\nfrom the complex latent dependencies among time\nseries (Hua et al., 2019; Maulik et al., 2020; Song\nand Fujimura, 2021; Nguyen and Quanz, 2021).\nDespite these advancements, existing methods have\nyet to effectively capture the intricate dependencies\ncharacteristic of time series data. The varying de-\nsign, implementation, and data requirements of\nthese methods further creates a barrier for their\nwidespread application in the field.\nAnother notable hurdle involves handling com-\nplex multi-modal financial temporal data that\nextends beyond numeric sequences. The data may\nencapsulate diverse sources such as historical news,\nfinancial knowledge graphs, social media activities,\nand various other market indicators. There has been\nrecent effort leveraging statistical inference (Ka-\nnungsukkasem and Leelanupab, 2019), RNN/CNN\nwith text embedding (Vargas et al., 2017), graph\nneural networks (Cheng et al., 2022), etc. to inte-\ngrate the complex information.\nLast but of utmost importance, the issue of in-\nterpretability and explainability poses significant\nchallenges to the trustworthiness of machine learn-\ning and deep learning models. The majority of ex-\nisting deep learning models operate as black boxes,\noffering little insight into their decision-making\n739\nprocesses. This lack of transparency sometimes\nraises concerns about the result reliability and im-\npedes user trust. This is particularly relevant in sen-\nsitive fields like finance, where substantial invest-\nments and assets are at stake. There is recent study\ntrying to understand deep-learning based predic-\ntions through attention scores (Hsieh et al., 2021),\nbut such insight is still not readily human readable\nand still requires considerable interpretation effort.\nThe recent advancement of Large Language\nModels (LLMs) (Brown et al ., 2020a; Touvron\net al., 2023b; Brown et al., 2020b; OpenAI, 2023a)\npotentially lend us a powerful tool to address all\nabove challenges in a unified, flexible way.\nFirst, LLMs can learn complex relations\namong sequences . LLMs are the most power-\nful Transformer-based models, and there has been\nabundant researches showing Transformer-based\nmodels capable of learning the underlying complex\nrelations among textual sequences (Yun et al., 2019;\nRong et al., 2020; Zhang et al., 2020; Dwivedi and\nBresson, 2020; Ying et al., 2021) and solving quan-\ntitative problems (Wei et al ., 2022; Lewkowycz\net al., 2022; Imani et al., 2023). It is reasonable to\nexpect the potential of LLMs understanding com-\nplex dependencies among numeric time series aug-\nmented by temporal textual sequences.\nSecondly, LLMs have demonstrated outstand-\ning reasoning and inference capability over\nmulti-modal data. By design, LLMs are proficient\nat learning from a broad spectrum of data sources\nand types. They are trained on a vast amount of\ntexts from the internet, encompassing a wide range\nof topics, styles, and formats. This equips them to\nhandle diverse input data, such as numerical, tex-\ntual, structured data (Wu et al., 2023; Shen et al.,\n2023). This multi-modal data handling capability\ncould be particularly useful for financial forecast-\ning, where crucial information often comes from\ndisparate sources, such as numerical market data,\ntextual news articles, and social media posts.\nLastly, LLMs are natural explainers that gen-\nerate human readable explanations providing in-\nsight into a decision. One of the key advantages of\nLLMs is their ability to generate natural language\ntext that is coherent, contextual, and comprehen-\nsive. This allows them to provide human-readable\nexplanations for their decisions (Zhao et al., 2023).\nFurthermore, through Chain-of-Thoughts (COT)\nor step-by-step thinking (Wei et al., 2022; Zhang\net al., 2023; Lightman et al., 2023), beyond a few\nsentences of explanation, LLMs can even gener-\nate detailed step-by-step reasoning to reveal the\ndecision-making process.\nThe following summarizes the main contribu-\ntions of this paper,\n• This paper takes a novel exploration to study\nLLMs’ potential to the valuable task of ex-\nplainable financial time series forecasting. For\nthis paper, we focus on the NASDAQ-100\nstock price time series. To the best of our\nknowledge, there is not yet public studies on\nthis topic to date.\n• We experiment with a combination of zero-\nshot/few-shot inference techniques with the\nstate-of-the-art AI model GPT-4 (OpenAI,\n2023a), and instruction-based fine-tuning us-\ning Open LLaMA (Geng and Liu, 2023). Our\nexperiment results also show that the tech-\nnique of chain-of-thoughts helps boost the\nperformance in most of the experiments.\n• We compare our proposed LLM approaches\nwith existing methods, including an ARMA-\nGARCH model and a gradient-boosting tree\nmodel. We show even zero-shot inference\nusing GPT-4 can outperform a boosting-tree\nmodel with about ∼300 features.\n2 Related Works\nThe field of financial time series forecasting has\nbeen a subject of extensive research, with various\nmethodologies being proposed over the years.\n2.1 Traditional Statistical/Econometric\nMethods\nTraditional statistical/econometric methods have\nlong been the cornerstone of financial time series\nforecasting. Techniques such as ARMA-GARCH\nmodels have been widely used due to their abil-\nity to capture dependencies and volatility cluster-\ning in financial time series (Drost and Nijman,\n1993; Francq and Zakoian, 2004; Andersen et al.,\n2009; Henneke et al., 2011). These models have\nbeen extended and modified in various ways to bet-\nter capture the complexities of financial markets\n(Tang et al., 2003; Ghahramani and Thavaneswaran,\n2006; Hossain and Nasser, 2011; Ma and Yu, 2013).\nOther popular statistical/econometric methods for\nfinancial time series include Vector Autoregres-\nsive Models (V AM) (Zivot and Wang, 2006), State-\nSpace Models and the Kalman Filter (De Jong and\n740\nZehnwirth, 1983), Diffusion Models (Fan, 2005),\nVector Error Correction Model (VECM) (Johansen,\n1995), Dynamic Stochastic General Equilibrium\n(DSGE) (Smets and Wouters, 2003), etc.\n2.2 Machine Learning Techniques\nWith the advent of machine learning, a variety of\nmodels have been applied to financial forecasting.\nDecision trees, support vector machines, etc., have\nbeen actively studied for financial time series pre-\ndiction (Trafalis and Ince, 2000; Yang et al., 2002;\nPai and Lin, 2005; Wang and Chan, 2006; Tsai\nand Wang, 2009; Li and Liao, 2017). More re-\ncently, deep learning techniques, such as Recurrent\nNeural Networks (RNNs), Convolutional Neural\nNetworks (CNNs), and Transformer models, have\nbeen applied to this task, demonstrating their abil-\nity to capture complex, non-linear relationships in\nthe data (Dingli and Fournier, 2017; Júnior and\nNievola, 2018; Sezer et al., 2020; Leung and Zhao,\n2021; Lara-Benítez et al., 2021).\n2.3 Large Language Models\nThe recent development of Large Language Mod-\nels (LLMs) has opened up new possibilities for\nfinancial time series forecasting. LLMs, such as\nGPT-3 (Brown et al., 2020b) and GPT-4 (OpenAI,\n2023a), LLaMA(Touvron et al., 2023a) (including\nAlpaca(Taori et al., 2023), Vincuna(Chiang et al.,\n2023)), have demonstrated remarkable capabilities\nin reasoning and understanding complex dependen-\ncies in the heterogeneous data, and the ability to\ngenerate human-readable explanations for their de-\ncisions (Zhao et al., 2023; Wei et al., 2022; Zhang\net al., 2023; Lightman et al., 2023). However, the\napplication of LLMs in financial time series fore-\ncasting with explanation is still a relatively unex-\nplored area, and this paper aims to contribute to\nthis emerging field.\n3 Methodology\nFor this paper, we study the NASDAQ-100 stock\nprice time series, supplemented by metadata about\nthe stock company and relevant financial news data\nconcerning both the specific stock and the broader\nfinancial/economic landscape. Our primary focus\nis on forecasting weekly/monthly stock returns (de-\nfined as the percentage change in stock price from\nthe beginning to the end of the week/month) with\naccompanying explanations 1.\n1Weekly/monthly stock return forecast aligns better with\nLLMs’ expertise at strategic reasoning and decision-making.\nWe demonstrate our structured design of\nprompts for LLMs and apply the state-of-the-art\nGPT-4 model (OpenAI, 2023b) for zero-shot and\nfew-shot inference tasks. For fine-tuning, we uti-\nlize the publicly available Open LLaMA (Geng and\nLiu, 2023). We also incorporate the technique of\nChain of Thoughts (COT) (Wei et al., 2022; Light-\nman et al., 2023), which has been found to enhance\nthe effectiveness of LLMs in other research studies.\n3.1 Data\n3.1.1 Stock Price Data\nWe download daily NASDAQ-100 stock price data\nfrom Yahoo Finance2. We first normalize the nu-\nmeric price time series as a percentage-change\ntime series, and then categorize them into bins.\nFor example, for weekly forecasting, we cate-\ngorize the price change between this week and\nlast week into 12 bins \"D5+\", \"D5\", \"D4\", \"D3\",\n\"D2\", \"D1\", \"U1\", \"U2\", \"U3\", \"U4\", \"U5\", \"U5+\",\nwhere \"D5+\" means price dropping more than 5%,\n\"Di\" (i=5,4,3,2,1) means price dropping between\n(i − 1)% and i%, \"U5+\" means price rising more\nthan 5%, \"U i\" ( i=1,2,3,4,5) means price rising\nbetween (i − 1)% and i%. The number of bins\nmight vary at different granularity. For example,\nfor monthly inference, we allow i be up to 10, and\nthere is corresponding \"D10+\", \"U10+\" bins.\n3.1.2 Company Profile Data\nWe use GTP-4 to generate company description,\ngeneral positive/negative factors that might impact\nthe company’s stock price. See Appendix Figure 1\nfor an example of the prompt to ask GPT-4 to gen-\nerate the company profile, and the GPT-4 response.\n3.1.3 Finance/Economy News Data\nWe use Google Custom Search API to obtain stock\ntop-5 news stories on a weekly basis for each\nNASDAQ-100 stock. After that, we use GPT-4\nto generate a summary and extract keywords from\neach obtained news article. An example of prompt\nand GPT-4 response is shown in Appendix Figure 2.\nA similar method is applied to obtain weekly top-5\nnews stories about macro economy and finance.\nTo reduce input size, We further generatemeta\nsummary & keywords for each week using GPT-4,\ngiven all the top story summaries and keywords\nof the week. An example of the meta summary &\nIt is also much less demanding on LLMs’ latency.\n2Using the stock list of Jun 01, 2023, and download using\nyfinance package pypi.org/project/yfinance/\n741\nkeywords is shown in Appendix Figure 3. They\nlook similar to the example in Appendix Figure 2,\nbut much condensed. We use the meta summary &\nkeywords for further experiments and evaluation.\n3.2 Instruction-Based Zero-shot/Few-shot\nInference with LLMs\nIn zero-shot and few-shot inference, LLMs demon-\nstrate their ability to generate responses either with-\nout any additional examples (zero-shot) or based on\na minimal number of examples beyond the original\ntraining set (few-shot). In our zero-shot/few-shot in-\nference experiment, we utilize an instruction-based\nprompt. The structure of our prompt includes in-\nstructions, the company profile, a historical tempo-\nral news summary/keywords sequence intermixed\nwith the categorized stock price time series, and\ncross-sequence few-shot learning examples (see\nAppendix Figure 4 for example).\n• We provide few-shot learning examples from\nstocks similar to the subject of interest. This\ndesign multi-purposes the few-shot examples\nto enable the LLM consider cross-sequence\ninformation from other stocks. To identify\nsimilar stocks, we query GPT-4 with a query\n\"List top 3 NASDAQ stocks most similar to\nAAPL\". A typical response is like \"MSFT,\nGOOGL, AMZN\"3. Here we in fact implic-\nitly leverage LLM inherent knowledge of\nfinancial entities and concepts.\n• There are other tweaks to the prompt struc-\nture. For instance, we divided the instruction\ninto two parts, positioning them at the begin-\nning and end of the prompt. This aids the\nmodel in better recognizing the task: to pre-\ndict next week’s summary & keywords, rather\nthan summarizing historical data. The pre-\ndicted summary & keywords serve as the ex-\nplanation for the stock return prediction.\nWe also experimented theChain-of-Thoughts ap-\nproach (Wei et al., 2022; Zhang et al., 2023; Light-\nman et al ., 2023), i.e., the idea of \" step-by-step\nthinking\", by appending the instruction \"Can you\nreason step by step before finalizing the output?\"\nto the end of the prompt. To our surprise, this no-\ntably improved the performance by a few points\n3We manually checked the \"similar-stocks\" responses by\nGPT-4 for NASDAQ-100 stocks and can confirm the results\nare logical; no hallucination was observed. The 09/2021\nknowledge cut-off does not pose a concern, as all NASDAQ-\n100 stocks have established for some time prior to this date.\n(see Section 4.2). The result of the step-by-step\nthinking process in response to Appendix Figure 4\nis illustrated in Appendix Figure 5, where it is evi-\ndent that GPT-4 identifies a previously overlooked\ncrucial point about \"earnings reports\" when explicit\nreasoning steps are generated.\n3.3 Instruction-based Fine-tuning with Open\nLLaMA\nWe perform instruction-based fine-tuning using\nOpen LLaMA 13B model to see how well a pub-\nlicly available model could perform in compari-\nson to GPT-4, especially after fine-tuning. The\nOpen LLaMA 13B model, in its zero-shot infer-\nence, typically tends to replicate portions of the\nprompt rather than executing the prompt instruc-\ntions effectively. Therefore, it is incapable of prop-\nerly handling instruction-based prompts as shown\nin Appendix Figure 4 without undergoing a process\nof fine-tuning. Therefore we focus on fine-tuning\nwith the Open LLaMA model in this paper.\nInstruction-based fine-tuning has been recently\nshown to be effective in guiding the model’s train-\ning process with specific directives (Taori et al .,\n2023; Peng et al., 2023). We created a dataset of\n30K weekly forecasting plus 7K monthly forecast-\ning, derived from 5-year historical data spanning\nfrom Jun 2017 to June 2022. Unlike GPT-4 that\nsupports up to 8K token size, we need to com-\npress the prompt into 1K tokens for fine-tuning\nOpen LLaMA, due to model and hardware con-\nstraints. For each fine-tuning example, we employ\nGPT-4 to condense the full historical meta news\nsummary/keywords (e.g. from week 8 to the last\nweek as shown in Appendix Figure 4) into a sin-\ngle, even more concise summary/keywords pair.\nSimultaneously, the \"Company Profile\" and \"Fore-\ncasting Examples\" sections of the prompt are also\nrespectively condensed into more succinct sum-\nmary paragraphs.\nWhile it would be ideal for Open LLaMA to\nmanage its own end-to-end experiment, including\nthe task of prompt compression for fine-tuning, we\nstill resort to using GPT-4 right now. This is due to\nOpen LLaMA 13B model’s zero-shot summariza-\ntion capability is considerably inferior to those of\nGPT-4 in practice. The summaries and keywords\nextracted by Open LLaMA 13B model often fall\nshort of usability.\nOnce fine-tuned, the Open LLaMA 13B model\ndemonstrates a much more satisfactory comprehen-\n742\nsion of the instruction, resulting in the generation\nof a forecast and an accompanying explanation that\nappears coherent. This is illustrated in Appendix\nFigure 6. As per the result in section 4.2, when it\ncomes to binary classification, the Open LLaMA\nmodel’s performance is competitive compared to\nGPT-4. However, we’ve noticed that the Open\nLLaMA model has a tendency to produce more\nextreme predictions, such as U5+ or D5+, which\nresult in a relatively higher squared error.\n4 Experiments\n4.1 Experiment Setup\n4.1.1 Data Time Window\nThe details of the data used in the experiments is as\ndescribed in Section 3.1. We focus on NASDAQ-\n100 stock return forecasting for this paper.\n• The training/fine-tuning time window contains\n5-year data from 06/12/2017 to 06/05/2022.\nThere are in total 26K data examples in this\ntime window (260 weeks and 100 stocks), and\nthey are used for training the baseline models,\nand fine-tuning the Open LLaMA 13B model.\n• The evaluation time window has 52 weeks\nspanning from 06/06/2022 to 06/04/2023. The\nevaluation of baseline models, the zero/few-\nshot inference with GPT-4, and the evaluation\nof fine-tuned Open LLaMA 13B model, are\nbased on the 5.2K evaluation examples (52\nweeks, 100 stocks) in this time window.\n4.1.2 Baseline Models\nTo evaluate the performance of our approach, we\ninclude a heuristic baseline using the most-frequent\nhistorical bin (i.e. the most frequent bin from\nhistorical weeks before the target week to fore-\ncast) as the prediction, an ARMA-GARCH model\n(p = q = 1) (Tang et al., 2003; Ma and Yu, 2013),\nand a gradient-boosting tree model (Natekin and\nKnoll, 2013) implemented by LightGBM package\n(Ke et al., 2017). These baseline models are trained\non the training/fine-tuning data time window, and\nevaluated on the evaluation time window.\nFor the gradient-boosting tree model, we include\nthe following features. There are total about 300\nfeatures for the tree.\n1. Historical price time series available in the\ndaily stock price data, including open, close,\nmin, max prices, and the daily trading volume.\n2. The average, medium, min, max, and stddev\nof a rolling window of size 2, 5, 10, 30, 60, 90\nfor the above time series.\n3. The stock sector information and historical\nearnings are obtained from Alpha Vantage4.\n4.1.3 Evaluation Metrics\nWe perform weekly and monthly stock return fore-\ncasting with the baselines and LLM-based methods.\nWe treat 4 weeks as one month for convenience,\nand therefore there are 13 \"month\"s in the 52-week\nevaluation time window.\nTo evaluate the performance of our forecasting\nmodels, we employ three metrics.\n• Binary precision assesses the model’s abil-\nity to correctly predict the general direction\nof stock price movement, i.e., \"Up\" (U) or\n\"Down\" (D).\n• Bin precision, on the other hand, evaluates the\nmodel’s accuracy in predicting the exact bin\nfrom a full list of bins such as \"D5+\", \"D5\",\n\"D4\", ..., \"D1\", \"U1\", ..., \"U5\", \"U5+\".\n• The MSE of consecutive bin ordinals (e.g., -6\nfor \"D5+\", -5 for \"D5\", ..., 0 for \"U1\", ..., 4 for\n\"U5\", 5 for \"U5+\") is used to measure the aver-\nage squared differences between the model’s\npredictions and the actual values. This metric\nhelps to understand the model’s tendency to\nmake extreme forecasts when its predictions\nare incorrect.\nTo evaluate the quality of the forecasting ex-\nplanation (the predicted next-week/month sum-\nmary/keywords), we employ ROGUE-1 and\nROGUE-2 scores to compare with the actual sum-\nmary/keywords by GPT-4 extracted from the actual\ntop news of the next week/month.\n4.2 Performance Evaluation\nOur experiment results are summarized in Table 1\nand 2. Table 1 provides a comparative analysis of\nour LLM-based methods and the baseline models\nin terms of their performance in forecasting stock\nreturns. Table 2, on the other hand, evaluates the\nquality of the explanations generated by the LLMs.\nIn summary, our results show the effectiveness\nof LLMs in financial time series forecasting, with\n\"GPT-4 few-shot with COT\" consistently showing\n4https://www.alphavantage.co/documentation/\n743\nWeekly Monthly (Every 4 Weeks)\nBinary PrecisionBin PrecisionMSE Binary PrecisionBin PrecisionMSE\nMost-Frequent Historical Bin 50.7% 16.4% 43.5 51.4% 17.2% 155.1\nARMA-GARCH 52.4% 11.1% 22.1 50.5% 6.2% 90.1\nGradient Boosting Tree Model 60.8% 26.4% 24.3 56.4% 17.7% 85.6\nGPT-4 Zero-Shot 64.5% 31.2% 20.5 64.8% 26.0% 60.1\nGPT-4 Few-Shot 65.8% 32.7% 20.6 65.3% 26.5% 58.2\nGPT-4 Few-Shot w/ COT 66.5% 35.2% 18.7 69.5% 28.6% 50.4\nOpen LLaMA (13B) Fine-Tuned 62.2% 26.5% 23.3 60.1% 22.6% 63.3\nOpen LLaMA (13B) Fine-Tuned w/ COT64.7% 30.7% 21.0 62.2% 24.4% 63.5\nTable 1: Performance comparison between the baseline models and LLMs for stock price weekly/monthly forecast.\nWeekly Monthly (Every 4 Weeks)ROUGE-1 (S)ROUGE-2 (S)ROUGE-1 (K)ROUGE-2 (K)ROUGE-1 (S)ROUGE-2 (S)ROUGE-1 (K)ROUGE-2 (K)GPT-4 Zero-Shot 0.2212 0.0675 0.1295 0.0447 0.2528 0.0665 0.1335 0.0657GPT-4 Few-Shot 0.2242 0.0526 0.1304 0.0454 0.2450 0.0634 0.1348 0.0644GPT-4 Few-Shot w/ COT 0.2414 0.0543 0.2083 0.0869 0.2645 0.0758 0.2450 0.1025Open LLaMA (13B) Fine-Tuned0.2053 0.0395 0.0927 0.0324 0.2242 0.0474 0.1167 0.0520Open LLaMA (13B) Fine-Tuned w/ COT0.2371 0.0434 0.1123 0.0425 0.2436 0.0536 0.1356 0.0834\nTable 2: Explanation quality evaluation using ROGUE scores, using the GPT-4 summary/keyword extraction of\neach week’s true top news from google search as the ground truth.\nthe best performance in both prediction accuracy\nand explanation quality. The results also highlight\nthe technique of Chain-of-Thoughts (COT) con-\nsistently boosts performance, and the potential of\ninstruction-based fine-tuning with publicly avail-\nable LLMs like Open LLaMA to achieve reason-\nable performance in comparison to GPT-4 through\nfine-tuning with COT.\n4.2.1 Stock Price Forecasting\nFrom the results of Table 1, we observe that\nboth GPT-4 and Open LLaMA 13B model outper-\nform the ARMA-GARCH model and the gradient-\nboosting tree model in terms of both binary and bin\nprecision. GPT-4, in particular, shows superior per-\nformance in both zero-shot and few-shot settings,\nwith the few-shot setting with COT achieving the\nbest performance. In terms of MSE, \"GPT-4 few-\nshot with COT\" also achieves the lowest error, indi-\ncating that it not only best predicts the direction of\nthe price change but also provides a more accurate\nestimate of the magnitude of the change.\nOpen LLaMA 13B model, after fine-tuning,\nshows competitive performance compared to GPT-\n4 in terms of binary precision. However, its bin\nprecision is obviously worse, indicating it lacks\ncompetitive fine-grained reasoning capability to\npick the right bin. It also tends to produce more\nextreme predictions, resulting in a higher MSE.\n4.2.2 Explanation Quality\nTable 2 shows the quality of the explanations gen-\nerated by the LLMs (GPT-4 and fine-tuned Open\nLLaMA), evaluated using ROUGE-1 and ROUGE-\n2 scores for both the summary (S) and keywords\n(K) of the news. Again, the results show that \"GPT-\n4 few-shot with COT\" achieves the highest ROUGE\nscores, indicating that it generates the most rele-\nvant and accurate explanations for the predictions.\nOpen LLaMA, after fine-tuning with COT, also\nshows reasonable explanation quality in parallel\nwith GPT-4 results without COT.\n5 Conclusion\nIn this study, we explored using Large Language\nModels (LLMs) to tackle inherent challenges like\ncross-sequence reasoning, multi-modal signals inte-\ngration, and result interpretability in financial time\nseries forecasting. In particular, we experimented\nGPT-4 and Open LLaMA for the NASDAQ-100\nstock return predictions. With structured prompts\ncomprising company profile, historical stock price,\nand financial news data, LLMs generated human\nunderstandable explanations and forecasts. The per-\nformance of these LLMs surpassed traditional mod-\nels like ARMA-GARCH and gradient-boosting\ntrees, especially when integrating a step-by-step\nreasoning process based on the Chain of Thought\n(COT) approach. Furthermore, our fine-tuning ex-\nperiments highlighted the viability of tuning a pub-\nlicly available LLM to also achieve reasonable per-\nformance in comparison to GPT-4.\nThe preliminary results of applying LLMs in\nexplainable financial forecasting are encourag-\ning. This is the first step to develop a LLM-\nbased explainable financial forecast system to assist\nbusiness decision-making. We envision a future\n744\nwhere financial forecasting is not only more pre-\ncise but also more comprehensible and transparent,\nthus transforming financial and business decision-\nmaking across the sector.\nLimitations\nWhile we present promising initial results for the\nLLM-based approach for explainable financial time\nseries based on NASDAQ-100 stock returns, the\ngeneral applicability of our approach to different\ntypes of temporal data remains a question of future\ninvestigation.\n• In a narrower context, the effectiveness of our\napproach when applied to other stock indices\nlike the S&P 500 or Russell 2000 is yet to\nbe validated. Each of these indices harbors\ndistinct characteristics and diverse company\ncompositions, which may influence the perfor-\nmance of our method.\n• In a wider context, the potential of our method\nto forecast other types of financial temporal\ndata remains unexplored. This includes in-\nternal temporal time series such as return-on-\ninvestment (ROI), sales, headcounts, and costs\nfrom various departments, augmented by re-\nlated internal documents. It also extends to\nother public time series data such as company\nearnings, housing prices, security prices, and\nfutures prices.\nThe quality of explanations generated by our\nmethod is currently evaluated using automatic text\nsimilarity metrics, namely ROGUE and BLEU\nscores. These scores compare predicted expla-\nnations to ground-truth news summaries and key-\nwords. However, we have yet to thoroughly exam-\nine other aspects of these explanations, including\nthe possibility of hallucination or the presence of\nspecific patterns in explanations associated with\nfailed time series forecasts.\nOther signals can be considered for future in-\nvestigations, such as macro economy time series\n(Stock and Watson, 1999) and social media data\n(Mankar et al., 2018; Javed Awan et al., 2021).\nEthics Statement\nIn conducting our research, we committed to trans-\nparency in our methodology, results reporting, and\ndata usage. Our work adhered to the guidelines\nand principles of responsible AI research. All data\nused in this paper is public data or can be obtained\nthrough publicly available APIs. No user data is\ninvolved in the research.\nReferences\nSaeed Aghabozorgi, Ali Seyed Shirkhorshidi, and\nTeh Ying Wah. 2015. Time-series clustering–a\ndecade review. Information systems 53 (2015), 16–\n38.\nTorben Gustav Andersen, Richard A Davis, Jens-Peter\nKreiß, and Thomas V Mikosch. 2009. Handbook of\nfinancial time series. Springer Science & Business\nMedia.\nM Suresh Babu, N Geethanjali, and B Satyanarayana.\n2012. Clustering approach to stock market prediction.\nInternational Journal of Advanced Networking and\nApplications 3, 4 (2012), 1281.\nArash Bahrammirzaee. 2010. A comparative survey of\nartificial intelligence applications in finance: artificial\nneural networks, expert system and hybrid intelligent\nsystems. Neural Computing and Applications 19, 8\n(2010), 1165–1195.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020b. Language models are few-shot\nlearners. Advances in neural information processing\nsystems 33 (2020), 1877–1901.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020a.\nLanguage Models are Few-Shot Learners. ArXiv\nabs/2005.14165 (2020).\nCathy WS Chen, Feng-Chi Liu, and Mike KP So. 2011.\nA review of threshold time series models in finance.\nStatistics and its Interface 4, 2 (2011), 167–181.\nZheng Chen, Xinli Yu, Yuan Ling, Bo Song, Wei\nQuan, Xiaohua Hu, and Erjia Yan. 2018. Corre-\nlated anomaly detection from large streaming data.\nIn 2018 IEEE International Conference on Big Data\n(Big Data). IEEE, 982–992.\nDawei Cheng, Fangzhou Yang, Sheng Xiang, and Jin\nLiu. 2022. Financial time series forecasting with\nmulti-modality graph neural network. Pattern Recog-\nnition 121 (2022), 108218.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\n745\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An Open-\nSource Chatbot Impressing GPT-4 with 90%* Chat-\nGPT Quality. https://vicuna.lmsys.org\nThomas Conlon, Heather J Ruskin, and Martin Crane.\n2009. Cross-correlation dynamics in financial time\nseries. Physica A: Statistical Mechanics and its Ap-\nplications 388, 5 (2009), 705–714.\nPiet De Jong and Ben Zehnwirth. 1983. Claims re-\nserving, state-space models and the Kalman filter.\nJournal of the Institute of Actuaries 110, 1 (1983),\n157–181.\nAlexiei Dingli and Karl Sant Fournier. 2017. Finan-\ncial time series forecasting-a deep learning approach.\nInternational Journal of Machine Learning and Com-\nputing 7, 5 (2017), 118–122.\nFeike C Drost and Theo E Nijman. 1993. Temporal\naggregation of GARCH processes. Econometrica:\nJournal of the Econometric Society(1993), 909–927.\nVijay Prakash Dwivedi and Xavier Bresson. 2020. A\ngeneralization of transformer networks to graphs.\narXiv preprint arXiv:2012.09699 (2020).\nJianqing Fan. 2005. A selective overview of nonpara-\nmetric methods in financial econometrics. Statist. Sci.\n(2005), 317–337.\nChristian Francq and Jean-Michel Zakoian. 2004. Max-\nimum likelihood estimation of pure GARCH and\nARMA-GARCH processes. Bernoulli 10, 4 (2004),\n605–637.\nXinyang Geng and Hao Liu. 2023. OpenLLaMA: An\nOpen Reproduction of LLaMA . https://github.\ncom/openlm-research/open_llama\nM Ghahramani and A Thavaneswaran. 2006. Financial\napplications of ARMA models with GARCH errors.\nThe Journal of Risk Finance 7, 5 (2006), 525–543.\nParameswaran Gopikrishnan, Vasiliki Plerou, Yan Liu,\nLA Nunes Amaral, Xavier Gabaix, and H Eugene\nStanley. 2000. Scaling and correlation in financial\ntime series. Physica A: Statistical Mechanics and its\nApplications 287, 3-4 (2000), 362–373.\nWolfgang Härdle, Helmut Lütkepohl, and Rong Chen.\n1997. A review of nonparametric time series analysis.\nInternational statistical review 65, 1 (1997), 49–72.\nJan S Henneke, Svetlozar T Rachev, Frank J Fabozzi,\nand Metodi Nikolov. 2011. MCMC-based estima-\ntion of Markov Switching ARMA–GARCH models.\nApplied Economics 43, 3 (2011), 259–271.\nAltaf Hossain and Mohammed Nasser. 2011. Compar-\nison of the finite mixture of ARMA-GARCH, back\npropagation neural networks and support-vector ma-\nchines in forecasting financial returns. Journal of\nApplied Statistics 38, 3 (2011), 533–551.\nTsung-Yu Hsieh, Suhang Wang, Yiwei Sun, and Vasant\nHonavar. 2021. Explainable multivariate time series\nclassification: a deep neural network which learns to\nattend to important variables as well as time intervals.\nIn Proceedings of the 14th ACM international con-\nference on web search and data mining. 607–615.\nYuxiu Hua, Zhifeng Zhao, Rongpeng Li, Xianfu Chen,\nZhiming Liu, and Honggang Zhang. 2019. Deep\nlearning with long short-term memory for time series\nprediction. IEEE Communications Magazine 57, 6\n(2019), 114–119.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathprompter: Mathematical reasoning using large\nlanguage models. arXiv preprint arXiv:2303.05398\n(2023).\nMazhar Javed Awan, Mohd Shafry Mohd Rahim,\nHaitham Nobanee, Ashna Munawar, Awais Yasin,\nand Azlan Mohd Zain. 2021. Social media and stock\nmarket prediction: a big data approach. MJ Awan,\nM. Shafry, H. Nobanee, A. Munawar, A. Yasin et al.,\"\nSocial media and stock market prediction: a big data\napproach,\" Computers, Materials & Continua 67, 2\n(2021), 2569–2583.\nSøren Johansen. 1995. Likelihood-based inference in\ncointegrated vector autoregressive models. OUP Ox-\nford.\nNorberto Ritzmann Júnior and Julio Cesar Nievola.\n2018. A generalized financial time series forecast-\ning model based on automatic feature engineering\nusing genetic algorithms and support vector machine.\nIn 2018 International Joint Conference on Neural\nNetworks (IJCNN). IEEE, 1–8.\nNont Kanungsukkasem and Teerapong Leelanupab.\n2019. Financial latent Dirichlet allocation (FinLDA):\nFeature extraction in text and data mining for finan-\ncial time series prediction. IEEE Access 7 (2019),\n71645–71664.\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang,\nWei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.\n2017. Lightgbm: A highly efficient gradient boost-\ning decision tree. Advances in neural information\nprocessing systems 30 (2017).\nKyoung-jae Kim. 2003. Financial time series forecast-\ning using support vector machines. Neurocomputing\n55, 1-2 (2003), 307–319.\nBoris Kovalerchuk and Evgenii Vityaev. 2005. Data\nmining for financial applications. Data Mining\nand Knowledge Discovery Handbook (2005), 1203–\n1224.\nBjoern Krollner, Bruce J Vanstone, Gavin R Finnie, et al.\n2010. Financial time series forecasting with machine\nlearning techniques: a survey.. In ESANN.\nPedro Lara-Benítez, Manuel Carranza-García, and\nJosé C Riquelme. 2021. An experimental review\n746\non deep learning architectures for time series fore-\ncasting. International Journal of Neural Systems 31,\n03 (2021), 2130001.\nTim Leung and Theodore Zhao. 2021. Financial time\nseries analysis and forecasting with Hilbert–Huang\ntransform feature generation and machine learning.\nApplied Stochastic Models in Business and Industry\n37, 6 (2021), 993–1016.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo\nGutman-Solo, et al . 2022. Solving quantitative\nreasoning problems with language models. arXiv\npreprint arXiv:2206.14858 (2022).\nWei Li and Jian Liao. 2017. A comparative study on\ntrend forecasting approach for stock price time series.\nIn 2017 11th IEEE International Conference on Anti-\ncounterfeiting, Security, and Identification (ASID) .\nIEEE, 74–78.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri\nEdwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\n2023. Let’s Verify Step by Step. arXiv preprint\narXiv:2305.20050 (2023).\nJunmei Ma and Xinli Yu. 2013. Research on the Pricing\nof the Basket Credit Default Swap. Mathematical\nComputation 2, 4 (2013).\nTejas Mankar, Tushar Hotchandani, Manish Madhwani,\nAkshay Chidrawar, and CS Lifna. 2018. Stock mar-\nket prediction based on social sentiments using ma-\nchine learning. In 2018 international conference on\nsmart city and emerging technology (ICSCET). IEEE,\n1–3.\nRomit Maulik, Arvind Mohan, Bethany Lusch, Sandeep\nMadireddy, Prasanna Balaprakash, and Daniel\nLivescu. 2020. Time-series learning of latent-space\ndynamics for reduced-order model closure. Physica\nD: Nonlinear Phenomena 405 (2020), 132368.\nAlexey Natekin and Alois Knoll. 2013. Gradient boost-\ning machines, a tutorial. Frontiers in neurorobotics7\n(2013), 21.\nNam Nguyen and Brian Quanz. 2021. Temporal latent\nauto-encoder: A method for probabilistic multivariate\ntime series forecasting. In Proceedings of the AAAI\nConference on Artificial Intelligence, V ol. 35. 9117–\n9125.\nOpenAI. 2023a. GPT-4 Technical Report. ArXiv\nabs/2303.08774 (2023).\nOpenAI. 2023b. GPT-4 Technical Report.\narXiv:2303.08774 [cs.CL]\nPing-Feng Pai and Chih-Sheng Lin. 2005. A hybrid\nARIMA and support vector machines model in stock\nprice forecasting. Omega 33, 6 (2005), 497–505.\nAndrew J Patton. 2012. A review of copula models\nfor economic time series. Journal of Multivariate\nAnalysis 110 (2012), 4–18.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277 (2023).\nVasiliki Plerou, Parameswaran Gopikrishnan, Bernd\nRosenow, Luís A Nunes Amaral, and H Eugene Stan-\nley. 1999. Universal and nonuniversal properties of\ncross correlations in financial time series. Physical\nreview letters 83, 7 (1999), 1471.\nYuan Qi and Jing Xiao. 2018. Fintech: AI powers\nfinancial services to improve people’s lives.Commun.\nACM 61, 11 (2018), 65–69.\nSangeeta Rani and Geeta Sikka. 2012. Recent tech-\nniques of clustering of time series data: a survey.\nInternational Journal of Computer Applications 52,\n15 (2012).\nYu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie,\nYing Wei, Wenbing Huang, and Junzhou Huang.\n2020. Self-supervised graph transformer on large-\nscale molecular data. Advances in Neural Informa-\ntion Processing Systems 33 (2020), 12559–12571.\nOmer Berat Sezer, Mehmet Ugur Gudelek, and Ah-\nmet Murat Ozbayoglu. 2020. Financial time series\nforecasting with deep learning: A systematic litera-\nture review: 2005–2019. Applied soft computing 90\n(2020), 106181.\nLi Shen, Yan Sun, Zhiyuan Yu, Liang Ding, Xinmei\nTian, and Dacheng Tao. 2023. On Efficient Training\nof Large-Scale Deep Learning Models: A Literature\nReview. arXiv preprint arXiv:2304.03589 (2023).\nFrank Smets and Raf Wouters. 2003. An estimated dy-\nnamic stochastic general equilibrium model of the\neuro area. Journal of the European economic associ-\nation 1, 5 (2003), 1123–1175.\nWen Song and Shigeru Fujimura. 2021. Capturing com-\nbination patterns of long-and short-term dependen-\ncies in multivariate time series forecasting. Neuro-\ncomputing 464 (2021), 72–82.\nJames H Stock and Mark W Watson. 1999. Business\ncycle fluctuations in US macroeconomic time series.\nHandbook of macroeconomics 1 (1999), 3–64.\nHim Tang, Kai-Chun Chiu, and Lei Xu. 2003. Fi-\nnite mixture of ARMA-GARCH model for stock\nprice prediction. In Proceedings of the Third Inter-\nnational Workshop on Computational Intelligence\nin Economics and Finance (CIEF’2003), North Car-\nolina, USA. 1112–1119.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford Alpaca:\nAn Instruction-following LLaMA model. https:\n//github.com/tatsu-lab/stanford_alpaca.\n747\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al . 2023a. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971 (2023).\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur’elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023b. LLaMA:\nOpen and Efficient Foundation Language Models.\nArXiv abs/2302.13971 (2023).\nTheodore B Trafalis and Huseyin Ince. 2000. Support\nvector machine for regression and applications to\nfinancial forecasting. In Proceedings of the IEEE-\nINNS-ENNS International Joint Conference on Neu-\nral Networks. IJCNN 2000. Neural Computing: New\nChallenges and Perspectives for the New Millennium,\nV ol. 6. IEEE, 348–353.\nChih F Tsai and Sammy P Wang. 2009. Stock price\nforecasting by hybrid machine learning techniques.\nIn Proceedings of the international multiconference\nof engineers and computer scientists, V ol. 1. 60.\nManuel R Vargas, Beatriz SLP De Lima, and Alexan-\ndre G Evsukoff. 2017. Deep learning for stock mar-\nket prediction from financial news articles. In 2017\nIEEE international conference on computational in-\ntelligence and virtual environments for measurement\nsystems and applications (CIVEMSA). IEEE, 60–65.\nJar-Long Wang and Shu-Hui Chan. 2006. Stock market\ntrading rule discovery using two-layer bias decision\ntree. Expert Systems with Applications 30, 4 (2006),\n605–611.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903\n(2022).\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\nMark Dredze, Sebastian Gehrmann, Prabhanjan Kam-\nbadur, David Rosenberg, and Gideon Mann. 2023.\nBloomberggpt: A large language model for finance.\narXiv preprint arXiv:2303.17564 (2023).\nHaiqin Yang, Laiwan Chan, and Irwin King. 2002. Sup-\nport vector machine regression for volatile stock mar-\nket prediction. In Intelligent Data Engineering and\nAutomated Learning—IDEAL 2002: Third Interna-\ntional Conference Manchester, UK, August 12–14,\n2002 Proceedings 3. Springer, 391–396.\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin\nZheng, Guolin Ke, Di He, Yanming Shen, and Tie-\nYan Liu. 2021. Do transformers really perform badly\nfor graph representation? Advances in Neural Infor-\nmation Processing Systems34 (2021), 28877–28888.\nPaul D Yoo, Maria H Kim, and Tony Jan. 2005. Ma-\nchine learning techniques and use of event informa-\ntion for stock market prediction: A survey and evalu-\nation. In International Conference on Computational\nIntelligence for Modelling, Control and Automation\nand International Conference on Intelligent Agents,\nWeb Technologies and Internet Commerce (CIMCA-\nIAWTIC’06), V ol. 2. IEEE, 835–841.\nSeongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo\nKang, and Hyunwoo J Kim. 2019. Graph transformer\nnetworks. Advances in neural information process-\ning systems 32 (2019).\nJiawei Zhang, Haopeng Zhang, Congying Xia, and Li\nSun. 2020. Graph-bert: Only attention is needed\nfor learning graph representations. arXiv preprint\narXiv:2001.05140 (2020).\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\nGeorge Karypis, and Alex Smola. 2023. Multi-\nmodal chain-of-thought reasoning in language mod-\nels. arXiv preprint arXiv:2302.00923 (2023).\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al . 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223 (2023).\nEric Zivot and Jiahui Wang. 2006. Vector autoregressive\nmodels for multivariate time series. Modeling finan-\ncial time series with S-PLUS® (2006), 385–429.\n748\nA Appendix - Example Prompts and Responses\nPrompt: Generate a short description for stock AAPL’ s company \"Apple Inc\". Also list general\npositive and negative factors that might impact the stock price; be brief and use keywords.\nConsider diverse general factors, such as macro economic situation (e.g. inflation, CPI growth),\nbusiness factors (e.g. sales, investment, products), technology factors (e.g. innovation), and others.\nUse format Description: ..., Positive Factors: ..., Negative factors: ...\nGPT-4 Response:\nDescription:\nAAPL (Apple Inc.) is a world-renowned technology company based in Cupertino, California. Known for its sleek\ndesigns and innovative technologies, the company produces a wide range of products including the iPhone, iPad, Mac,\nApple Watch, and services like iCloud, App Store, and more.\nPositive Factors:\nMacroeconomic: Economic growth, increased consumer spending\nBusiness: Strong sales, expansion into new markets, positive earnings reports, strategic acquisitions\nTechnology: Innovation, new product launches, advancements in artificial intelligence and augmented reality\nBrand: Strong global brand, loyal customer base\nLegal/Regulatory: Favorable policy changes, successful patent applications\nNegative Factors:\nMacroeconomic: Inflation, economic downturn, trade restrictions\nBusiness: Decreased sales, poor earnings reports, lack of new investments\nTechnology: Slow pace of innovation, delays in product launches\nBrand: Damage to brand reputation, loss of customer loyalty\nLegal/Regulatory: Antitrust investigations, unfavorable policy changes, patent disputes\nEnvironmental: Failure to meet sustainability goals, negative environmental impact\nFigure 1: An example of a stock’s company profile consisting of the company description, the general posi-\ntive/negative factors impacting the company’s stock prices.\n749\nPrompt [template, those inside {} are placeholders]: Please summarize the following noisy but\npossible news data extracted from web page HTML, and extract keywords of the news. The news\ntext can be very noisy due to it is HTML extraction. Give formatted answer such as Summary: ...,\nKeywords: ... The news is supposed to be for {symbol} stock. You may put ’N/A’ if the noisy text\ndoes not have relevant information to extract.\nNews: {news}\nGPT-4 Response:\nSummary: Apple Inc is postponing perks for some corporate divisions and expanding a cost-cutting effort, joining its\npeers in streamlining operations. The company proposes to forego bonuses or promotions for April, with the payments\noccurring only in October. Apple is also restricting hiring and leaving additional positions open after employee departure.\nThis is triggering fear of downsizing among employees.\nKeywords: Apple, bonus, hiring, cost-cutting, promotions, downsizing, employees\nFigure 2: An example of news summary & keywords extracted from one news article for a stock’s com-\npany (AAPL in this case), generated by GPT-4. The original news is at https://sports.yahoo.com/\napple-joins-cost-cut-bandwagon-145845685.html . The prompt in this example is a template. Texts in-\nside [] are comments and are not really in the prompt when we submit it to the LLM, and we will fill stock symbol\nand the news into the placeholders enclosed by {}.\nSummary: Apple Inc.’s stock is displaying rising relative strength, although shy of a key\nbenchmark, and the company has been highlighted as a top-performing stock due to its sales of 1.3\nbillion iPhones and a significant $19.4 billion profit last quarter. As the NASDAQ surges with\nApple as a big cap, Apple expands cost-cutting measures, postpones perks, restricts hiring, and\nleaves positions open, instigating downsizing concerns. The overall stock market performance is\nboosted by a $30 billion deposit infusion for struggling firms.\nKeywords: Apple Inc., stock, relative strength, cost-cutting, downsizing, NASDAQ, $30 billion\ndeposit infusion, iPhones, profit.\nFigure 3: An example of one week’s meta summary & keywords condensed from all the company’s summaries and\nkeywords from the week.\n750\nPrompt [template, those inside {} are placeholders]:\nInstruction: Forecast next week stock return (price change) for symbol, given the company profile,\nhistorical weekly news summary, keywords, and stock returns, and optionally the examples from\nother stocks of a similar company. The trend is represented by bins \"D5+\", \"D5\", \"D4\", \"D3\",\n\"D2\", \"D1\", \"U1\", \"U2\", \"U3\", \"U4\", \"U5\", \"U5+\", where \"D5+\" means price dropping more than\n5%, D5 means price dropping between 4% and 5%, \"D4\" means price dropping between 3% and\n4%, \"U5+\" means price rising more than 5%, \"U5\" means price rising between 4% and 5%, \"D4\"\nmeans price rising between 3% and 4%, etc.\nCompany Profile: {company_profile} [company description, general positive/negative factors as\nshown in Appendix Figure 1.]\nRecent News: News are ordered from oldest news to latest news.\n====8 Weeks ago====\n{meta_news_summary_company} [meta news summary & keywords of the week for the company]\n{meta_news_summary_macro} [meta news summary & keywords for the macro economy and\nfinance status of the week] ...\n====7 weeks ago====\n...\n====Last week====\n...\nForecasting Examples: {few_shot_learning_examples_from_similar_stocks} [We use forecasting\nexamples from top-3 similar stocks as the few-short learning examples. The example is structured\nsimilar to this prompt, with company profile and recent news, but without the instruction texts.]\nNow predict what could be the next week’s Summary, Keywords, and forecast the Stock Return.\nThe predicted Summary/Keywords should explain the stock return forecasting. You should predict\nwhat could happen next week. Do not just summarize the history. The next week stock return need\nnot be the same as the previous week. Use format Summary: ..., Keywords: ..., Stock Return: ...\nGPT-4 Response [Forecasting week 05/01/2023 to 05/07/2023]:\nSummary: Apple’s stock is expected to see fluctuations in the week to come against a backdrop of mixed news. The\ncompany’s announced expansion into India, coupled with the introduction of high-yield savings accounts, initially\nprovided a boost to the stock. However, growing concerns surrounding reduced PC shipments and apprehensions about\nthe company’s upcoming earnings injected a degree of uncertainty. Despite these challenges, Apple’s unwavering\nfocus on cost control and the anticipation surrounding innovative offerings, such as the forthcoming mixed-reality\nheadset, shored up investor confidence. Consequently, the stock maintained a general upward trend throughout the\nweek, albeit punctuated by a few dips. The broader economic climate on Wall Street also had a role to play, with a\nrecovering sentiment in Q1 towards high-tech companies like Microsoft and Alphabet. These companies released\npivotal financial reports and ground-breaking products during the period, influencing the tech sector’s performance,\npotentially impacting Apple’s stock trajectory.\nKeywords: AAPL, mixed-reality headset, India expansion, high-yield savings account, cost control strategies, Bernstein,\nWall Street, economic recovery, tech sector performance\nStock Return: U1\nGround Truth [Summary from actual news from 05/01/2023 to 05/07/2023 and the actual return]:\nSummary: Apple’s stock saw significant gains after outperforming Wall Street’s fiscal second quarter targets, driven\nby robust iPhone sales and strong services, despite posting a decrease in revenue for the second consecutive quarter\nand facing potential competition in the VR headset market from Reliance Jio’s JioDive. The company also noted an\nimprovement in supply and strength in emerging markets, further boosting investor confidence.\nKeywords: Apple, stock, Wall Street, fiscal second quarter, iPhone, services sales, quarterly earnings, revenue, declining\nrevenue, emerging markets, improved supply, Virtual Reality, competition, JioDive, Reliance Jio.\nStock Return: U3\nFigure 4: The prompt structure for the experiments in this paper with LLMs. We also give an example of GPT-4\nresponse to a concrete prompt constructed from information on and before 04/30/2023. We notice the cross-sequence\ninformation and macro-economy information are obviously considered in the LLM’s reasoning. The stock return\nforecast U1 is for the next week from 05/01/2023 to 05/07/2023. The actual market performance for AAPL is U3.\nWe note Apple 2023 Q2 earning call happened on May 04 beat expectation, which maybe the major contributor to\nthe higher-than-forecasting gain in the week.\n751\nPrompt: {The same prompt as in Appendix Figure 4.}\nCan you reason step by step before the finalized output?\nGPT-4 Response [Forecasting week 05/01/2023 to 05/07/2023]:\nGiven the information provided, we can see that Apple (AAPL) has had a number of positive developments and a few\npotential concerns in the recent past.\nPositive Developments:\n• Recent sales and earnings have been strong, and the company appears to be exploring new markets such as India.\n• The upcoming mixed-reality headset and the \"Apple Pay Later\" service show that Apple is still innovating and\npushing into new product areas.\n• The cost-cutting measures and the plan to use 100% recycled cobalt show that Apple is taking steps to improve its\nefficiency and sustainability, which could be attractive to certain investors.\n• Winning the lawsuit against Britain’s antitrust regulator and avoiding potential penalties.\n• AAPL stock has been trending upwards recently, with multiple weeks of upward movement (U5, U4, U3, U2),\nand only a single day of slight downward movement (D1).\nPotential Concerns:\n• Apple is cutting costs, delaying bonuses, and slowing down hiring in certain areas, which could potentially affect\nemployee morale and productivity.\n• The negative sentiment from analyst Toni Sacconaghi about the overestimation of Q3 and Q4 revenues by Wall\nStreet could potentially impact the stock price.\nIn addition to above factors, the general direction of the stock market appears to be on an upward trajectory, as\nevidenced by the recent increases in the Dow Jones, S&P 500, and Nasdaq. In recent weeks, the stock performances of\nhigh-tech giants such as Microsoft and Alphabet have also shown considerable strength. Furthermore, recent positive\ndevelopments in the macroeconomic environment, including a surge in consumer spending, seem encouraging.\nNow, we can generate a prediction for next week’s Summary, Keywords, and Stock Return. However, it’s important\nto note that while the information provided is used as a basis, this is a prediction and the actual outcome may be different.\nSummary: Apple’s mixed-reality headset previews have received positive feedback from industry experts, increasing\ninvestor confidence. However, apprehension about cost-cutting measures and fears of slower hiring might add some\nuncertainty. Wall Street anticipates a strong earnings report, boosting stock morale. Furthermore, Apple’s expansion\ninto India continues to show promise.\nKeywords: Apple, mixed-reality headset, investor confidence, cost-cutting, hiring, Wall Street, earnings report,\nexpansion, India.\nStock Return: U2. Given the positive trends and promising developments, the stock is likely to experience upward\nmovement. However, cost-cutting concerns may moderate gains, leading to a prediction of \"U2\", suggesting a rise of\n1% to 2% next week.\nFigure 5: GPT-4 outputs its detailed reasoning steps if we simply add an instruction \"Can you reason step by step\nbefore the finalized output?\" to the end of the prompt in Appendix Figure 4. With detailed reasoning steps, GPT-4\ncaptures a previously missed point \"Wall Street anticipates a strong earnings report, boosting stock morale\", and\namends the stock return forecast as U2.\n752\nSummary: The stock market is likely to react favorably to Apple’s continued focus on cost\noptimization and revenue generation through new product launches and market expansions.\nDespite facing regulatory challenges, the company’s efforts towards environmental responsibility\nand diversification of financial offerings are likely to have a positive impact on its long-term\nprospects. As a result, we expect Apple’s stock to see another strong week of growth, with a\nprojected return of U5+.\nKeywords: Cost-Cutting, New Product Launches, Expansion, Regulation, Supply Chain\nStock Return: U5+\nFigure 6: Explainable forecasting by fine-tuned Open LLaMA 13B model.\n753"
}