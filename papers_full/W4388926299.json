{
  "title": "Refactoring Programs Using Large Language Models with Few-Shot Examples",
  "url": "https://openalex.org/W4388926299",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4382332901",
      "name": "Shirafuji, Atsushi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2532805723",
      "name": "Oda Yusuke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2243380924",
      "name": "Suzuki Jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749283652",
      "name": "Morishita, Makoto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2315234095",
      "name": "Watanobe Yutaka",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4321276180",
    "https://openalex.org/W4211263275",
    "https://openalex.org/W4390045011",
    "https://openalex.org/W4376167329",
    "https://openalex.org/W4380993527",
    "https://openalex.org/W4385187421",
    "https://openalex.org/W4283705032",
    "https://openalex.org/W3127404656",
    "https://openalex.org/W1647671624",
    "https://openalex.org/W4288057765",
    "https://openalex.org/W3217787939",
    "https://openalex.org/W3184011599",
    "https://openalex.org/W4307932749",
    "https://openalex.org/W4224060952",
    "https://openalex.org/W2113157806",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4361866100",
    "https://openalex.org/W3108032709",
    "https://openalex.org/W2099613036",
    "https://openalex.org/W2130877788",
    "https://openalex.org/W4394664141",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W4399203759",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W2620436109",
    "https://openalex.org/W3165405326",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3123221944",
    "https://openalex.org/W4382239980",
    "https://openalex.org/W3148057751",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2153887189",
    "https://openalex.org/W2138756793",
    "https://openalex.org/W4313547604",
    "https://openalex.org/W2972934903",
    "https://openalex.org/W4288055861",
    "https://openalex.org/W2066455950",
    "https://openalex.org/W4281560022",
    "https://openalex.org/W3081159607",
    "https://openalex.org/W4298186479"
  ],
  "abstract": "A less complex and more straightforward program is a crucial factor that enhances its maintainability and makes writing secure and bug-free programs easier. However, due to its heavy workload and the risks of breaking the working programs, programmers are reluctant to do code refactoring, and thus, it also causes the loss of potential learning experiences. To mitigate this, we demonstrate the application of using a large language model (LLM), GPT-3.5, to suggest less complex versions of the user-written Python program, aiming to encourage users to learn how to write better programs. We propose a method to leverage the prompting with few-shot examples of the LLM by selecting the best-suited code refactoring examples for each target programming problem based on the prior evaluation of prompting with the one-shot example. The quantitative evaluation shows that 95.68% of programs can be refactored by generating 10 candidates each, resulting in a 17.35% reduction in the average cyclomatic complexity and a 25.84% decrease in the average number of lines after filtering only generated programs that are semantically correct. Furthermore, the qualitative evaluation shows outstanding capability in code formatting, while unnecessary behaviors such as deleting or translating comments are also observed.",
  "full_text": "Refactoring Programs Using Large Language\nModels with Few-Shot Examples\nAtsushi Shirafuji†, Yusuke Oda‡, Jun Suzuki ‡, Makoto Morishita §, Yutaka Watanobe†\n†University of Aizu, Japan\n‡Tohoku University, Japan\n§NTT Communication Science Laboratories, Japan\nEmail: {m5261161, yutaka}@u-aizu.ac.jp, {yusuke.oda.c1, jun.suzuki}@tohoku.ac.jp, makoto.morishita@ntt.com\nAbstract—A less complex and more straightforward program\nis a crucial factor that enhances its maintainability and makes\nwriting secure and bug-free programs easier. However, due to its\nheavy workload and the risks of breaking the working programs,\nprogrammers are reluctant to do code refactoring, and thus, it\nalso causes the loss of potential learning experiences.\nTo mitigate this, we demonstrate the application of using a\nlarge language model (LLM), GPT-3.5, to suggest less complex\nversions of the user-written Python program, aiming to encourage\nusers to learn how to write better programs. We propose a\nmethod to leverage the prompting with few-shot examples of the\nLLM by selecting the best-suited code refactoring examples for\neach target programming problem based on the prior evaluation\nof prompting with the one-shot example.\nThe quantitative evaluation shows that 95.68% of programs\ncan be refactored by generating 10 candidates each, resulting in\na 17.35% reduction in the average cyclomatic complexity and a\n25.84% decrease in the average number of lines after filtering\nonly generated programs that are semantically correct. Further-\nmore, the qualitative evaluation shows outstanding capability in\ncode formatting, while unnecessary behaviors such as deleting or\ntranslating comments are also observed.\nIndex Terms—code refactoring, large language models, few-\nshot prompting, software complexity, programming education\nI. I NTRODUCTION\nProgrammers and learners often write unreadable, redun-\ndant, or complicated programs because they lack the knowl-\nedge to write less complex programs or are in a hurry to meet\nminimum requirements by sacrificing the complexity. Writing\nreadable and maintainable programs from the beginning is\ndifficult for not only novices but also experts. They often itera-\ntively make minor modifications to the previous version of the\nprogram to improve the code readability and maintainability,\ncalled code refactoring (Figure 1).\nComplicated and unreadable programs have risks of making\nthem harder to maintain (i.e., difficult to add, modify, or\nremove functions) and to hand over to a successor because\nthe program can be understood by the author only, as well\nas the risks of making bugs and the difficulty of finding and\nfixing the potential bugs.\nProgrammers sometimes have no choice but to write such\ncomplicated and unreadable programs because they lack the\nknowledge to write the program in a less complex and more\nThis work was supported by the Japan Society for the Promotion of Science\n(JSPS) KAKENHI Grant Number JP23H03508.\nCorrect but with room for improvement.\n(Before)\ntext = input()\nfor i in range(len(text)):\nprint(text[len(text)-1-i],end=‘’)\nprint()\nImproved and correct program.\n(After)\ntext = input()\nprint(text[::-1])\nCode Refactoring\nFig. 1: Example of code refactoring to improve a correct\nprogram with room for improvement.\nstraightforward way than the one that comes to their mind,\nor they intentionally leave them complicated and unreadable\nbecause of the high cost of modifying them (e.g., lack of\ntime for delivery) although they recognize that they should\ndeal with them. According to one of the definitions of code\nrefactoring by Fowler et al. [1], “the process of changing a\nsoftware system in such a way that it does not alter the external\nbehavior of the code yet improves its internal structure,” it\nshould keep the external behavior of the previous version\nof the program. In fact, code refactoring also has a risk of\nbreaking the already working program, which is another reason\nprogrammers are reluctant to do code refactoring.\nIn recent years, large language models (LLMs) trained\non texts in programming languages as well as natural lan-\nguages [2]–[7] have shown the potential to support program-\nming in both programming education [8]–[11] and software\ndevelopment [12]–[14]. From a prompt engineering perspec-\ntive, White et al. [15] proposed several prompt patterns for\ncode refactoring to support software engineering activities. As\nthe most related work, Madaan et al. [16] used two popular\nLLMs, Codex and CodeGen, to improve the program’s time\nefficiency. In another work, Madaan et al. [17] used Codex\nto improve the readability of variable names and comments.\nHowever, to the best of our knowledge, no prior work has\ndemonstrated code refactoring to reduce the software com-\nplexity using LLMs.\nTo address the issue rising in both the fields of software\ndevelopment and programming education, we propose the\nuse of an existing LLM, GPT-3.5, to suggest a complexity-\nimproved version of a user-written complex Python program\nto motivate them to learn how to write better programs, as\narXiv:2311.11690v1  [cs.PL]  20 Nov 2023\nwell as supporting programming instructors by reducing their\nworkloads to think and answer the questions for each learner.\nTo leverage the prompting with few-shot examples 1 of the\nLLM, we propose a method to select the best-suited code\nrefactoring examples used for few-shot prompts in each target\nprogramming problem based on the prior evaluation results\nof one-shot prompting for each code refactoring example.\nSince the LLM has the potential to break the input program,\nthe generated programs are validated, and only functionally\ncorrect programs are suggested to a user.\nIn the experiments, we collect functionally correct Python\nprograms to be refactored from a set of 44 introduc-\ntory programming problems provided on Aizu Online Judge\n(AOJ) [18] and randomly select 20 unique programs in each\nprogramming problem. We generate 10 code refactoring can-\ndidates for each of the correct programs and verify if the\ngenerated program is syntactically and semantically correct.\nWe demonstrate the applicability of the LLM to generate\nless complex programs aligned to the user-written program,\nevaluated both quantitatively and qualitatively.\nThe contributions of this work are as follows:\n• We demonstrate that the LLM can generate correct and\nless complex programs for a majority of the input pro-\ngrams.\n• We propose leveraging few-shot prompting by selecting\nrefactoring examples to help guide the LLM to better\nalign with user-written programs.\n• Our quantitative and qualitative evaluations exhibit the\nLLM’s performance in code refactoring and its potential\nto assist programming.\n• We discuss the limitations of the current approach, pro-\nviding insights for future research in leveraging LLMs\nfor code refactoring.\nII. R ELATED WORK\nCode refactoring is a well-known practice in software\ndevelopment that aims to improve the internal structure and\nreadability of a program without changing its external behav-\nior [1]. Most code refactoring tasks are performed manually\nby developers based on experience and best practices [19].\nHowever, there have been several research efforts to assist or\nautomate code refactoring using various techniques.\nOne important target of code refactoring is code clone or\ncode duplication, which refers to reusing several portions of\ncode across different parts of the codebase instead of defining\nmodular functions. Code clones increase complexity, reduce\nmaintainability, and violate the Don’t Repeat Yourself (DRY)\nprinciple. Refactoring the duplicated or near-duplicated code\nfragments makes the program more concise and easier to\nmaintain [20], [21], and code clone detection techniques [22]\ncan also be applied.\nSeveral different approaches have been proposed to assist\ncode refactoring. WitchDoctor [23] is a tool that can de-\ntect refactoring behaviors of programmers and automatically\n1In this paper, we denote n-shot prompting for prompting with n-shot\nexamples.\ncomplete the refactoring while they are being performed.\nBlue-Pencil [24] empowers the refactoring feature of Visual\nStudio IntelliCode 2 to suggest repetitive edits automatically.\nSearch-based techniques, which find code fragments that can\nimprove program readability or complexity, have also been\napplied in refactoring suggestions [25]–[27].\nIn addition to automated code refactoring approaches men-\ntioned above, refactoring is also used in a more narrow\ncontext within integrated development environments (IDEs). In\nIDEs, refactoring often refers to the process of automatically\nrenaming identifiers, such as classes, functions, and variables,\nthroughout the codebase. Many IDEs, such as Visual Studio 3,\nEclipse4, and IntelliJ IDEA 5, provide built-in functionality to\nfacilitate this renaming process, ensuring consistent updates of\nidentifiers throughout the codebase.\nIt is worth noting that code refactoring can be considered\na code-to-code generation task in the natural language pro-\ncessing field since a potentially complex program written in\na programming language is converted into a less complex\nprogram written in the same language. While code refactoring\naims to improve already correct programs, code repair focuses\non converting incorrect programs into correct ones [28]–[30].\nLLMs, such as Codex [2], have shown the capability of\nrepairing programs [11], [31]–[33]. Code editing [34]–[36] is\na more generalized task that involves learning the code editing\nbehavior. Pre-trained models can be fine-tuned on downstream\ntasks such as code repair [35], [36].\nIn recent work, White et al. [15] proposed a catalog\nof prompt patterns for software engineering activities using\nChatGPT as a representative LLM. In particular, they provided\nsix prompt patterns for code refactoring tasks, such as making\na given program follow certain coding principles. As the most\nrelated work, Madaan et al. [16] demonstrated the capability of\nLLMs (i.e., Codex [2] and CodeGen [7]) to improve the time\nefficiency of programs by proposing a large-scale dataset con-\nsisting of (slower, faster) pairs of programs written by the same\nuser. Similarly, Madaan et al. [17] used Codex to improve\nthe readability of variable names and comments by leveraging\nthe iterative refinement of the generated programs using the\nfeedback generated by Codex, named Self-Refine. These\nworks are closely related to ours because they focus on im-\nproving already correct programs in terms of performance or\nreadability by keeping the functional correctness. In contrast,\nour work focuses on reducing program complexity.\nIII. M ETHODOLOGY\nA. Overview\nThe proposed methodology is illustrated in Figure 2.\nFirstly, we define code refactoring examples used in\nprompts. Secondly, we evaluate each example in each pro-\ngramming problem, and based on the performance of each\n2https://devblogs.microsoft.com/visualstudio/\nrefactoring-made-easy-with-intellicode/.\n3https://visualstudio.microsoft.com/.\n4https://www.eclipse.org/.\n5https://www.jetbrains.com/idea/.\nCode Refactoring\nExamples\nLarge \nLanguage \nModel\nOne-Shot\nSelected Best-Suited\nExamples\nEvaluation Results of \nGenerated Programs\nFew-Shot\nInput\nLarge \nLanguage \nModel\nSuggested\nFiltered\nCorrect\nFig. 2: Illustration of the proposed approach selecting the best-suited code refactoring examples for few-shot prompting for\neach programming problem based on the performance of one-shot prompting. Only the filtered programs are suggested to a\nuser.\nexample, we select the best-suited examples for each program-\nming problem. Thirdly, the user-written program is passed to\nthe LLM with the selected few-shot examples, and the LLM\ngenerates several refactoring candidates. Finally, we validate\nthe generated programs using an automatic judge system, and\nonly functionally correct programs are suggested to the user.\nB. Examples Preparation\nFew-shot prompting[37] provides a few examples for LLMs\nto demonstrate the expected inputs and outputs of conversa-\ntions, whereas zero-shot prompting provides no examples.\nFigure 3 illustrates the prompting in this work. The system\ninstruction and the user’s program are always passed to the\nLLM. No examples are passed in the zero-shot, one in the\none-shot, and three in the few-shot (i.e., 3-shot). The program\non the LLM (left) side is not actually generated by the LLM\nbut prepared by us manually. It imitates that it is generated to\ndemonstrate the generation of the LLM.\nWe manually define the following 10 code refactoring ex-\namples, aiming to reduce the software complexity by utilizing\nthe defined functions and statements in Python, as well as\nsome techniques that make the program more readable but do\nnot directly reduce the software complexity.\n• Use a formatted string.\n• Use a built-in (i.e., radians) function.\n• Use a logical operator instead of a nested if.\n• Use a for-loop instead of a while-loop.\n• Use list comprehension instead of a for-loop.\n• Use the map function instead of list comprehension.\n• Use a throwaway variable.\n• Use the enumerate function instead of the range function.\n• Use the zip function instead of the range function.\n• Use a ternary operator instead of an if-branch.\nC. Examples Evaluation\nIn this phase, each code refactoring example is used for\none-shot prompting and evaluated by a validator.\nLet E = {(original, refactored)} be a set of code refac-\ntoring examples, where (original, refactored) ∈ E is a pair\nconsisting of a correct program with room for improve-\nment and its refactored version. Hereafter, let us denote\ne = (original, refactored) for short, and thus, e ∈ E. Note\n```python\nhas_positive = False\nfor num in numbers:\nif num > 0:\nhas_positive = True\nbreak\n```\n```python\nhas_positive = any(num > 0 for num in numbers)\n```\nRefactor the given Python program to a more readable, \nefficient, and maintainable one. You can assume that the \ngiven program is semantically correct. Do not change the \nexternal behavior of the program, and keep the syntactic \nand semantic correctness. Python programs should be in a \ncode block. Do not explain anything in natural language.\n```python\ns = 0\ni = 0\nwhile i < len(numbers):\ns += numbers[i]\ni += 1\n```\nSystem Instruction\nExample\nFig. 3: Illustration of prompting consisting of (1) a system\ninstruction, (2) zero/one/few-shot examples, and (3) the user’s\ninput program. The conversation in blue is the code refactoring\nexample.\nthat the number of code refactoring examples in E is a\nhyper-parameter. We use |E| = 10 in our experiments, as\ndefined in Section III-B.\nLet P be a set of programming problems. Then, each\nprogramming problem P ∈ P has a set of original correct\nprograms written by users XP = {x}. Moreover, Yex = {y}\nis a set of code refactoring candidates generated by a pre-\ntrained LLM using a code refactoring example e ∈ E and an\ninput program x ∈ XP . In our experiments, we use |XP | = 20\nprograms for |P| = 44 programming problems and generate\n|Yex| = 10code refactoring candidates.\nA validator VP to validate the correctness of a generated\nprogram y for the programming problem P can be denoted as\nFormula 1.\nVP (y) =\n\u001a 1 if y solves P,\n0 otherwise. (1)\nIn practice, the validator actually executes the given program\nusing test cases and returns the correctness in binary, as\ndescribed in Section IV-D.\nThen, for each code refactoring example e ∈ E and each\nprogramming problem P ∈ P, the score SeP can be denoted\nas Formula 2. This score indicates how suitable the code\nrefactoring example e is for the programming problem P.\nSeP =\nX\nx∈XP\nX\ny∈Yex\nVP (y) (2)\nFinally, for each code refactoring example e ∈ E, the\ncumulative score Se can be denoted as Formula 3. This score\nreflects how suitable the code refactoring example e is overall.\nSe =\nX\nP∈P\nSeP (3)\nD. Examples Selection\nLet argtopk (x, k) a function to return the arguments at\nwhich the function returns the top-k values, the best-performed\nk code refactoring examples EP for each programming prob-\nlem P ∈ P is calculated by Formula 4.\nEP = argtopke(1000SeP + Se, k) (4)\nAfter selecting the best-performed code refactoring ex-\namples for few-shot prompting, we construct the few-shot\nprompts for the LLM. As a few-shot prompting, we adopt 3-\nshot prompting, which provides k = 3sets of best-performed\ncode refactoring examples.\nIV. E XPERIMENTAL SETUP\nWe demonstrate the applicability of an LLM in code\nrefactoring to reduce software complexity. In this section, we\ndescribe the original dataset consisting of target programs to be\nrefactored, the model from the representatives of off-the-shelf\nLLMs used to generate the refactored programs, and the\nobjective metrics to evaluate the effectiveness.\nA. Dataset\nWe construct a dataset consisting of the original programs\nto be refactored by an LLM.\n1) Data Collection: To construct a dataset, we collect a\nvariety of correct programs written in Python3 from AOJ, an\nonline judge system where users submit programs to solve\ngiven programming problems. AOJ provides approximately\n3,000 programming problems and stores 8,000,000 programs\nsubmitted by 100,000 users, including wrong or incomplete\nprograms [38]. The source code submitted to AOJ is available\nfor research or educational purposes and can be downloaded\nfrom the official source archive 6 or via public datasets such\nas CodeNet [39] or CodeContests [6].\n6http://developers.u-aizu.ac.jp/index.\nWe limit the target problems from a popular course problem\nnamed Introduction to Programming I (ITP1)7, which has\n44 introductory programming problems, ranging from basic\noperations of input/output to class definitions.\nEach submission of a program on AOJ has a verdict\nresulting from executing hidden test cases by the judge system,\nsuch as Accepted, Wrong Answer, or Runtime Error. We only\nuse the submissions judged as Accepted because we only focus\non the code refactoring task to improve correct but complex\nprograms.\nBy the above collecting conditions, we collect 296,885\ncorrect Python programs from 44 programming problems on\nAOJ.\nTABLE I: Statistics of collected programs in each of the\ndata preprocessing phases. LOC indicates the average number\nof lines, Chars and Tokens indicate the average number of\ncharacters and tokens, respectively, and CC indicates the\naverage cyclomatic complexity.\n#Programs LOC Chars Tokens CC\nCollected 296,885 14.59 332.56 108.79 5.85\nUnique 161,670 15.24 346.19 111.88 6.01\nFiltered 158,081 14.60 329.40 107.26 5.79\nSelected 880 14.82 331.35 106.15 5.65\n2) Preprocessing: After collecting the correct programs\nfrom AOJ, we preprocess the data to select partial programs\nfor the experiments in the following phases. The statistics of\nthe data in each phase are summarized in Table I. Also, the\nmetrics used in the table are explained in Section IV-C.\na) Duplicate Deletion: Firstly, we remove duplicate pro-\ngrams in each problem. The duplicate detection is based on\nthe characters of the raw source code, neither based on tokens\nnor trees. Therefore, the programs are considered different if\nonly one space or empty line is different. After removing\nthe duplicated programs, the number of unique programs is\n161,670.\nb) Outlier Deletion: Secondly, we remove outliers in\neach problem. The outlier criterion is 2σ + µ, where σ is\nthe standard deviation, and µ is the mean of the number of\ntokens in programs in each problem. After removing outlier\nprograms, the number of filtered programs is 158,081.\nc) Randomly Selection: Finally, we randomly select 20\nprograms for each problem, and the final dataset contains\n880 programs in total (20 programs × 44 problems = 880\nprograms).\nB. Model\nTo demonstrate the few-shot prompting using the pre-\ntrained LLM, we use the representative LLM from the\noff-the-shelf models. We use the gpt-3.5-turbo engine\nserved from the OpenAI API 8, often referred to as Chat-\nGPT or GPT-3.5 models. More precisely, in this work,\n7https://onlinejudge.u-aizu.ac.jp/courses/lesson/2/ITP1/all.\n8https://platform.openai.com/docs/models/gpt-3-5.\nwe use gpt-3.5-turbo-0301, a snapshot version from\nMarch 1st, 2023, for reproducibility. gpt-3.5-turbo is\nan InstructGPT [40]-based model, trained to follow user\ninstructions and provide detailed responses using a technique\ncalled reinforcement learning from human feedback to align\nwith the user’s instructions. InstructGPT is also based on\nCodex [2], which is trained on massive source code. Therefore,\ngpt-3.5-turbo has a high capability in understanding\nand generating programming languages as well as natural\nlanguages.\nWe set the temperature, which determines the creativity\nof generated texts, to 0.2 to be slightly creative for all\ngenerations in this work. We set the max tokens, which\nlimits the maximum number of tokens to generate, to 1,024.\nAlthough gpt-3.5-turbo supports up to 4,097 tokens, it\ncounts both the prompt and generation tokens. Given that the\naverage number of tokens in the input programs is 106.15, as\nshown in Table I, the limit of 1,024 tokens is sufficient for\ngenerating refactored programs. We use the following system\ninstruction to ask the LLM to do code refactoring: “Refactor\nthe given Python program to a more readable, efficient, and\nmaintainable one. You can assume that the given program is\nsemantically correct. Do not change the external behavior of\nthe program, and keep the syntactic and semantic correctness.\nPython programs should be in a code block. Do not explain\nanything in natural language. ”The system instruction is used\nin all generations, including zero-shot prompting. In addition,\nas instructed in the system instruction, each Python program\ncommunicated with the LLM is represented as a code block,\nenclosed with three backticks.\nC. Metrics\nSince our purpose is to suggest less complex programs as\nwell as functionally correct programs for the user, we evaluate\nthe generated programs from the perspective of correctness and\ncomplexity. We also report other software metrics for analysis,\nsuch as Levenshtein distance (or edit distance), lines of code\n(LOC), the number of characters, the number of tokens, and\nthe character-based similarity.\na) Compilability: is the syntactic correctness of the gen-\nerated program, whether the program passes the compilation,\ndenoted by Compilability = P/(P + F), where P is the\nnumber of programs that passed the compilation, and F is the\nnumber of programs that failed in the compilation. Since all\nthe collected programs for code refactoring initially solved the\nproblem, the compilability of the original programs is 100%.\nb) Pass@ k: is used to evaluate the semantic (func-\ntional) correctness of the generated programs, proposed by\nChen et al. [2]. Although the metric is designed to evaluate if\nthe problem is solved by generating k samples, we adopt this\nmetric to evaluate if the program is refactored by generating k\nsamples. Semantic correctness is validated by a virtual judge\nsystem, where hidden test cases are given to the program and\nconsidered correct if the program passes all the hidden test\ncases. Pass@ k is denoted as Formula 5, where n ≥ k is\nthe number of samples and c ≤ n is the number of correct\nsamples.\npass@k := E\nProblems\n\"\n1 −\n\u0000n−c\nk\n\u0001\n\u0000n\nk\n\u0001\n#\n(5)\nWe generate n = 10 samples for each program and report\npass@k at k = 1, 10. In this work, pass@1 indicates the\nratio of correct programs out of all generated programs. Also,\nassuming to suggest the programs only validated as correct,\npass@10 indicates the ratio of programs that at least one\nrefactored program is suggested within the 10 generation tries.\nc) McCabe’s Cyclomatic Complexity (CC): is a software\nmetric to measure the program’s complexity [41]. CC increases\nwith the number of if branches, for loops, and their nesting.\nA smaller CC is preferable, and CC < 10 is categorized as\na little risk in software development. We employ the radon 9\nlibrary to calculate the CC.\nd) Chars: is a length based on the number of characters\nin the program. This metric is highly influenced by the longer\ntokens, such as string literals, comments written in natural\nlanguage, and white spaces, as well as the variable or function\nnames. However, this metric can reflect the raw length of the\nprogram compared to the other metrics.\ne) Tokens: is a length based on the number of tokens in\nthe program. This metric can particularly reflect the number of\nelements (tokens) directly influencing the program’s behavior,\nnot affected by superficial texts that are already measured by\nthe chars. We utilize the official Python tokenizer10 to tokenize\nthe programs. In tokenization, we exclude comments and\nspecial tokens, such as newline, indent, and dedent, to better\nreflect the length of the semantic elements of the program.\nf) Lines of Code (LOC): is a length based on the number\nof lines in the program, including empty and comment lines.\nAnother metric named Source Lines of Code (SLOC) excludes\nempty and comment lines in counting. Although either metric\ncan be used in the experiments, we use LOC since it can reflect\nthe number of insertions and deletions of empty and comment\nlines.\ng) Distance: is a Levenshtein distance [42] calculated\nbetween the original and generated programs based on char-\nacters, also known as edit distance. Each inserted, deleted, or\nsubstituted character is counted as one distance, whereas there\nis another method to count substitution as two distances (i.e.,\nan insertion and a deletion).\nh) Similarity: is a syntactic similarity between the orig-\ninal and generated programs based on characters using the\nLevenshtein distance. In this work, the similarity of two\nprograms, a, b, of length |a|, |b|, is defined as Formula 6, where\nDistance(a, b) is the Levenshtein distance between a and b.\nSimilarity(a, b) = 1− Distance(a, b)\nmax(|a|, |b|) (6)\n9https://github.com/rubik/radon.\n10https://docs.python.org/library/tokenize.html.\nD. Execution Environment\nTo validate the semantic correctness of generated programs,\nwe execute the generated program using test cases for each\nproblem. The program is judged correct if it passes all test\ncases and is incorrect otherwise, similar to unit testing in\nsoftware development. The test cases for each problem are\navailable through AOJ API 11. However, several prior works\nmentioned that LLMs trained on public source code have a\nrisk of generating malicious or vulnerable programs, which\nmay harm the host computer [2], [43]. Therefore, we prepare\na virtual judge system on our isolated sandbox environment\nto not be affected by the malicious generated programs.\nV. R ESULTS\nA. Quantitative\nTABLE II: Pass@ k, compilability, and CC of generated pro-\ngrams, along with the input programs. One-shot indicates\nthe best one-shot using the list comprehension example that\nperformed the best among one-shot prompting on both pass@1\nand pass@10. Few-shot indicates our proposed approach using\n3-shot prompting. CC indicates cyclomatic complexity. The\nbest score in each metric is in bold.\nPass@1 Pass@10 Compilability CC\nInput — — 100.00% 5.65\nZero-shot 87.44% 93.30% 99.99% 4.71\nOne-shot 89.77% 94.77% 99.95% 4.78\nFew-shot 91.11% 95.68% 99.93% 4.67\n1) Main Results: Table II shows the main evaluation results\nof our proposed methodology. Both pass@1 and pass@10\nscores are improved by giving more examples in prompts,\nand the proposed 3-shot prompting resulted in the best perfor-\nmance. In addition, the CC is improved (reduced) by 17.35%\ncompared to the input programs. Although the compilability\nslightly decreases, we prioritize the higher scores of pass@ k\nbecause the semantically wrong programs cannot be suggested\nto the user even if they are compilable.\n2) Detailed Scores for Each Prompting: Figure 4 and Fig-\nure 5 show the detailed scores for each prompting, representing\nthe difference from the zero-shot prompting. While the 3-shot\nprompting yielded the best performance on pass@10, several\none-shot prompting also performed better than the zero-shot\nprompting. This variation of performance among the one-shot\nprompting highlights the need for selecting the best-suited\nexamples for each problem.\n3) Conflict Between Pass@10 and Cyclomatic Complexity:\nWhen we refer to the relationship between pass@10 and CC,\nwe observe that these two metrics conflict. For Figure 4, the\nbest (highest) one-shot prompting uses the list comprehension\nexample, and the worst (lowest) prompting uses the zip\nfunction example. In contrast, as shown in Figure 5, the zip\nfunction example that performed the worst in pass@10 is the\n11http://developers.u-aizu.ac.jp/index.\nString format\nRadians function\nNested if\nFor\nList comprehension\nMap function\nThrowaway variableEnumerate function\nZip function\nT ernary operator\n3-shot\n0.02\n0.01\n0.00\n0.01\n0.02\nDiff of Pass@10\nFig. 4: The difference of pass@10 from zero-shot prompting\nfor each prompting. Higher is better.\nString format\nRadians function\nNested if\nFor\nList comprehension\nMap function\nThrowaway variableEnumerate function\nZip function\nT ernary operator\n3-shot\n0.10\n0.05\n0.00\n0.05\n0.10\nDiff of Cyclomatic Complexity\nFig. 5: The difference of CC from zero-shot prompting for\neach prompting. Lower is better.\nbest (lowest) in CC. Similarly, the list comprehension example\nthat performed the best in pass@10 is the worst (largest)\nin CC. This conflict of metrics is verified in Figure 6. We\nidentify a strong positive correlation between the pass@10\nand CC, as the correlation coefficient is 0.7916. It indicates\nthe difficulty of reducing the CC while keeping the pass@10\nsimultaneously. However, to raise the availability of suggesting\nat least one candidate of code refactoring for each program,\nwe prioritize raising the pass@10 in this work, and reducing\nthe CC is our secondary importance.\n4) Decrease in Cyclomatic Complexity: We test the differ-\nence in CC between the input and generated programs using\nthe Wilcoxon signed-rank test, where the significance level is\nset to 0.05. We find a statistically significant difference, as the\nP-value is less than 0.001. As shown in Figure 7, the CC in\nthe generated programs generally decreases.\n0.920 0.925 0.930 0.935 0.940 0.945\nPass@10\n4.60\n4.65\n4.70\n4.75\n4.80\n4.85\nAverage Cyclomatic Complexity\nFig. 6: A strong positive correlation (coefficient is 0.7916)\nbetween pass@10 and CC, indicating the difficulty of reducing\nthe CC while keeping the pass@10 simultaneously.\n0 5 10 15 20 25 30\nCyclomatic Complexity\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nDensity\nInput\nGenerated\nFig. 7: Distributions on the CC for input and generated\nprograms.\n5) Correlations in Levenshtein Distance: Figure 8 shows a\npositive correlation between the CC of the input programs\nand the Levenshtein distance, as the correlation coefficient\nis 0.4886. It indicates that the LLM makes more extensive\nchanges to the program with larger CC trying to make it less\ncomplex, whereas the LLM makes only small changes if the\ngiven program is already simple. Similarly, Figure 9 shows a\nstrong positive correlation between the number of characters\nof the input programs and the Levenshtein distance, as the\ncorrelation coefficient is 0.7442. It indicates that the LLM\nmakes longer changes to the longer programs.\n6) Decrease in LOC, Chars, and Tokens: As shown in\nTable III, all of the metrics of LOC, Char-based length, and\nToken-based length are reduced in the generated programs\ncompared with the input programs. Although the metrics do\n0 10 20 30 40 50 60 70\nCyclomatic Complexity of Input Programs\n0\n500\n1000\n1500\n2000\n2500\n3000Levenshtein Distance\nFig. 8: A positive correlation (coefficient is 0.4886) between\nthe CC of input programs and the Levenshtein distance,\nindicating that the LLM made larger modifications to the\nprograms with higher complexity.\n0 1000 2000 3000 4000\nNumber of Characters in Input Programs\n0\n500\n1000\n1500\n2000\n2500\n3000Levenshtein Distance\nFig. 9: A strong positive correlation (coefficient is 0.7442)\nbetween the number of characters in input programs and the\nLevenshtein distance, indicating that the LLM made larger\nmodifications to longer programs.\nnot directly indicate the readability and maintainability of the\nprograms (i.e., longer programs may have descriptive variable\nnames and comments), it shows that the generated programs\nare more concise while keeping the semantic correctness and\nreducing the complexity.\n7) Levenshtein Distance and Similarity: In addition, as\nshown in Table IV, we identify that the generated programs\ncontain at least one or more edits, as the minimum Levenshtein\ndistance is 1 and the maximum similarity is 99.46%. This\nfurther indicates that there is no cheating case where the\ncorrect program is just returned without any edits to pass the\nsemantic validation. However, on the other hand, this suggests\nTABLE III: Average values of LOC, Chars, and Tokens of\nthe programs. The values in parentheses indicate the standard\ndeviation.\nLOC Chars Tokens\nInput 14.82 ( ± 17.29) 331.35 ( ± 462.24) 106.15 ( ± 116.70)\nGenerated 10.99 ( ± 10.76) 263.17 ( ± 333.94) 85.18 ( ± 95.68)\nTABLE IV: Character-based Levenshtein distance\nand similarity, compared with the input programs.\nMean (Std) Min ∼ Max\nDistance 95.59 ( ± 162.89) 1 ∼ 3035\nSimilarity 67.54% ( ± 16.95%) 10.76% ∼ 99.46%\nthat modifications can be forcibly made, even if the input\nprogram is already readable.\nB. Qualitative\n1) Main Results: For a qualitative evaluation, we manu-\nally inspect the generated programs randomly. Overall, the\nsuggested programs contain helpful modifications to make\nthe input programs less complex, as well as the improve-\nment of readability and maintainability. For the readability\nimprovement, we observe the cases of variable renaming for\nmore descriptive variables and code formatting of adding or\nremoving white spaces and white lines appropriately.\nFigure 10 shows the representatives of improved and wors-\nened refactorings generated by the LLM. Figure 10a is the\nimproved example, changing from for and if statements to\nan effective generator. Figure 10b is the worsened example,\ndefining an excessive function that worsens the complexity.\n2) Code Formatting: To support the observation of the\ncapability in code formatting, we perform additional analysis\nto calculate the character-based Levenshtein distance between\nthe programs and their formatted programs. We employ the\nyapf12 library to format the Python programs. While the input\nprograms have a 21.96 ( ± 50.83) distance from the formatted\nprograms on average, the generated programs have only a 7.95\n12https://github.com/google/yapf.\na,b,c = (int(i) for i in\ninput().split())\ncount = 0\nfor i in range(a,b+1):\nif c%i == 0:\ncount += 1\nprint(count)\na, b, c = map(int, input().split())\ncount = sum(1 for i in range(a, b+1)\nif c % i == 0)\nprint(count)\n(a) Improved example.\n#!/usr/bin/env python3\nprint(“Hello World”)\n#!/usr/bin/env python3\ndef main():\nprint(“Hello World”)\nif __name__ == “__main__”:\nmain() (b) Worsened example.\nFig. 10: Examples of refactoring by the LLM.\n(± 32.78) distance. The significantly smaller distance in the\ngenerated programs indicates that the generated programs are\nwell-formatted and less required to be formatted.\nTABLE V: The average number of comments in the input and\ngenerated programs. Incl. and excl. indicate whether to include\ninline comments. The comment ratio is calculated by dividing\nthe number of comment lines excluding inline comments by\nthe number of lines of code including white lines.\nComments (incl.) Comments (excl.) Comment Ratio\nInput 0.60 ( ± 2.59) 0.53 ( ± 2.44) 2.39% ( ± 7.92%)\nGenerated 0.16 ( ± 0.75) 0.14 ( ± 0.70) 0.89% ( ± 4.42%)\n3) Comment Deletion: Although we observe many cases\nimproving the code readability, we also observe that the LLM\ndeletes many comments that can help understand the pro-\ngram. We count the number of comments using the standard\ntokenize13 library to support this observation. As shown in\nTable V, we verify that the average number of comment lines\ndecreased from 0.60 to 0.16, which reduced the comment ratio\nfrom 2.39% to 0.89%.\nTABLE VI: The proportion of natural languages\nused in code.\nInput Generated\n1 English 95.11% English 97.16%\n2 Japanese 3.98% Japanese 1.58%\n3 Unknown 0.91% Unknown 1.26%\n4) Comment Translation: Similar to deleting comments,\nwe also observe a few cases of non-English comments (e.g.,\nJapanese) being translated into English without a demand. To\nsupport this observation, we count the languages used in the\ncode by asking GPT-3.5 to determine the natural language. We\nuse the engine gpt-3.5-turbo-0301 with the parameters\nof temperature = 0to be deterministic and max tokens =\n10 to be answered concisely. We use the system instruction\nof “What natural language is used in this code? Select\nfrom [English, Japanese, Korean, Chinese, Other, Unknown,\nNone]. ”to be easier to aggregate. We compare the changes\nin the language proportion since it is difficult to quantitatively\ndetect what language is translated into what language for each\ncomment, as the whole program is refactored by the LLM.\nThe decrease in the proportion of Japanese and the increase\nin English in Table VI suggests that the Japanese comments\nare translated into English without demand. Note that the\nreason only English or Japanese comments are used is that\nthe AOJ is a Japanese web service that supports only English\nand Japanese.\nHowever, the decrease in the number of comments (i.e.,\nnatural language explanations) and the preferred use of English\nmight be affected by the system instruction we used. Firstly,\nthe instruction “Do not explain anything in natural language. ”\n13https://docs.python.org/3/library/tokenize.html\nmight suppress the natural language explanations in the com-\nments, although we intended to suppress explanations outside\nthe code block (program). Secondly, the LLM might decide\nthat the comments should be rewritten in English so that the\nusers using English can understand them since the instruction\nis written in English.\nVI. L IMITATIONS\nAs described in Section V-A, the LLM makes at least\nsome modifications to the given program. It can worsen an\nalready straightforward, readable, or concise program that\nis not required to be refactored. To avoid this, applying\nsome techniques to make modifications only if the program\nrequires refactoring can be beneficial. For instance, detecting\nwhether the program should be refactored before the refac-\ntoring request, or providing some instructions to control the\nLLM to make modifications only when there is a particular\nneed for refactoring, can be considered. Furthermore, since\ncode refactoring is usually performed iteratively, suggesting\nthe fully refactored program with extensive modifications at\nonce can be overwhelming for users. Iterative suggestions\ncontaining a small fraction of the modifications would be more\neducational.\nIn this work, we employ GPT-3.5 as the representative\nLLM from the publicly available LLMs to demonstrate the\neffectiveness of our proposed approach. Many LLMs capa-\nble of solving programming problems have been proposed\nrecently [44]–[48], including those that may perform even\nbetter than GPT-3.5. However, our results, which show the\nrefactoring ability of an LLM that can be further improved by\nselecting better few-shot examples, can be a foundation for\nfurther advancements in applying LLMs for code refactoring\ntasks.\nVII. C ONCLUSION\nThis paper presents a methodology for using a large lan-\nguage model (LLM), GPT-3.5, to suggest less complex ver-\nsions of user-written Python programs. The proposed approach\nleverages few-shot prompting with carefully selected examples\nto encourage users to learn how to write better programs.\nThe quantitative evaluation demonstrates that the LLM can\ngenerate correct and less complex programs for a majority\n(95.68%) of the input programs by generating 10 candidates\nfor each. The average cyclomatic complexity is reduced by\n17.35%, and the average number of lines of code is decreased\nby 25.84%, indicating the effectiveness of the LLM in code\nrefactoring. Furthermore, we observe several associations in\nmetrics, suggesting the capabilities and limitations of the\nLLM.\nThe qualitative evaluation shows that the LLM can improve\ncode readability through variable renaming and formatting.\nHowever, we also observe that the LLM made unnecessary\nmodifications to already straightforward and readable pro-\ngrams, as well as deletion and translation of comments.\nOverall, this work demonstrates the potential of LLMs\nin supporting code refactoring and improving program com-\nplexity. Future research includes exploring using different\nLLMs and further improvements on making code refactoring\nexamples to enhance the pass rates and effectiveness of the\nsuggested refactorings.\nREFERENCES\n[1] M. Fowler, K. Beck, J. Brant, W. Opdyke, and D. Roberts, Refactor-\ning: Improving the Design of Existing Code . Addison-Wesley Object\nTechnology Series, Addison-Wesley Professional, 1999.\n[2] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, et al. ,\n“Evaluating large language models trained on code,” arXiv preprint ,\n2021.\n[3] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, et al. ,\n“PaLM: Scaling language modeling with pathways,” arXiv preprint ,\n2022.\n[4] F. Christopoulou, G. Lampouras, M. Gritta, G. Zhang, Y . Guo, et al. ,\n“PanGu-Coder: Program synthesis with function-level language model-\ning,” arXiv preprint, 2022.\n[5] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, et al., “InCoder:\nA generative model for code infilling and synthesis,” in Proceedings of\nthe 11th International Conference on Learning Representations (ICLR) ,\n2023.\n[6] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, et al. ,\n“Competition-level code generation with AlphaCode,” Science, vol. 378,\nno. 6624, pp. 1092–1097, 2022.\n[7] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, et al. , “CodeGen:\nAn open large language model for code with multi-turn program\nsynthesis,” in Proceedings of the 11th International Conference on\nLearning Representations (ICLR) , 2023.\n[8] J. Finnie-Ansley, P. Denny, B. A. Becker, A. Luxton-Reilly, and\nJ. Prather, “The robots are coming: Exploring the implications of\nopenai codex on introductory programming,” in Proceedings of the 24th\nAustralasian Computing Education Conference (ACE) , p. 10–19, 2022.\n[9] S. Sarsa, P. Denny, A. Hellas, and J. Leinonen, “Automatic generation\nof programming exercises and code explanations using large language\nmodels,” in Proceedings of the 2022 ACM Conference on International\nComputing Education Research (ICER) , p. 27–43, 2022.\n[10] M. Wu, N. Goodman, C. Piech, and C. Finn, “Prototransformer: A meta-\nlearning approach to providing student feedback,” arXiv preprint, 2021.\n[11] J. Zhang, J. Cambronero, S. Gulwani, V . Le, R. Piskac, et al., “Repair-\ning bugs in python assignments using large language models,” arXiv\npreprint, 2022.\n[12] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, “IntelliCode\nCompose: Code generation using transformer,” in Proceedings of the\n28th ACM Joint Meeting on European Software Engineering Con-\nference and Symposium on the Foundations of Software Engineering\n(ESEC/FSE), p. 1433–1443, 2020.\n[13] P. Vaithilingam, T. Zhang, and E. L. Glassman, “Expectation vs.\nexperience: Evaluating the usability of code generation tools powered\nby large language models,” in Extended Abstracts of the 2022 CHI\nConference on Human Factors in Computing Systems (CHI EA) , 2022.\n[14] F. F. Xu, B. Vasilescu, and G. Neubig, “In-ide code generation from nat-\nural language: Promise and challenges,” ACM Transactions on Software\nEngineering and Methodology , vol. 31, no. 2, 2022.\n[15] J. White, S. Hays, Q. Fu, J. Spencer-Smith, and D. C. Schmidt, “Chatgpt\nprompt patterns for improving code quality, refactoring, requirements\nelicitation, and software design,” arXiv preprint, 2023.\n[16] A. Madaan, A. Shypula, U. Alon, M. Hashemi, P. Ranganathan, et al.,\n“Learning performance-improving code edits,” arXiv preprint, 2023.\n[17] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, et al., “Self-refine:\nIterative refinement with self-feedback,” arXiv preprint, 2023.\n[18] Y . Watanobe, “Aizu Online Judge,” 2004.\n[19] E. Murphy-Hill, C. Parnin, and A. P. Black, “How we refactor, and how\nwe know it,” IEEE Transactions on Software Engineering, vol. 38, no. 1,\npp. 5–18, 2012.\n[20] N. Tsantalis, D. Mazinanian, and G. P. Krishnan, “Assessing the\nrefactorability of software clones,” IEEE Transactions on Software\nEngineering, vol. 41, no. 11, pp. 1055–1090, 2015.\n[21] N. Tsantalis, D. Mazinanian, and S. Rostami, “Clone refactoring with\nlambda expressions,” in Proceedings of the 2017 IEEE/ACM 39th\nInternational Conference on Software Engineering (ICSE) , pp. 60–70,\n2017.\n[22] T. Kamiya, S. Kusumoto, and K. Inoue, “Ccfinder: a multilinguistic\ntoken-based code clone detection system for large scale source code,”\nIEEE Transactions on Software Engineering, vol. 28, no. 7, pp. 654–670,\n2002.\n[23] S. R. Foster, W. G. Griswold, and S. Lerner, “Witchdoctor: Ide support\nfor real-time auto-completion of refactorings,” inProceedings of the 34th\nInternational Conference on Software Engineering (ICSE) , p. 222–232,\n2012.\n[24] A. Miltner, S. Gulwani, V . Le, A. Leung, A. Radhakrishna, et al., “On\nthe fly synthesis of edit suggestions,” Proc. ACM Program. Lang., vol. 3,\nno. OOPSLA, 2019.\n[25] O. Seng, J. Stammel, and D. Burkhart, “Search-based determination\nof refactorings for improving the class structure of object-oriented\nsystems,” in Proceedings of the 8th Annual Conference on Genetic and\nEvolutionary Computation (GECCO) , p. 1909–1916, 2006.\n[26] M. Harman and L. Tratt, “Pareto optimal search based refactoring at the\ndesign level,” in Proceedings of the 9th Annual Conference on Genetic\nand Evolutionary Computation (GECCO) , pp. 1106–1113, 2007.\n[27] F. Adler, G. Fraser, E. Gr ¨undinger, N. K ¨orber, S. Labrenz, et al., “Im-\nproving readability of scratch programs with search-based refactoring,”\nin Proceedings of the 2021 IEEE 21st International Working Conference\non Source Code Analysis and Manipulation (SCAM), pp. 120–130, 2021.\n[28] M. M. Rahman, Y . Watanobe, and K. Nakamura, “A bidirectional lstm\nlanguage model for code evaluation and repair,”Symmetry, vol. 13, no. 2,\n2021.\n[29] T. Matsumoto, Y . Watanobe, and K. Nakamura, “A model with iterative\ntrials for correcting logic errors in source code,” Applied Sciences ,\nvol. 11, no. 11, 2021.\n[30] A. Shirafuji, M. M. Rahman, M. F. I. Amin, and Y . Watanobe, “Program\nrepair with minimal edits using codet5,” arXiv preprint, 2023.\n[31] H. Joshi, J. Cambronero Sanchez, S. Gulwani, V . Le, G. Verbruggen,\nand I. Radi ˇcek, “Repair is nearly generation: Multilingual program\nrepair with llms,” Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 37, no. 4, pp. 5131–5140, 2023.\n[32] H. Pearce, B. Tan, B. Ahmad, R. Karri, and B. Dolan-Gavitt, “Ex-\namining zero-shot vulnerability repair with large language models,” in\nProceedings of the 2023 IEEE Symposium on Security and Privacy (SP),\npp. 2339–2356, 2023.\n[33] J. A. Prenner, H. Babii, and R. Robbes, “Can openai’s codex fix bugs?\nan evaluation on quixbugs,” in Proceedings of the 3rd International\nWorkshop on Automated Program Repair (APR) , p. 69–75, 2022.\n[34] S. Chakraborty, Y . Ding, M. Allamanis, and B. Ray, “CODIT: Code\nediting with tree-based neural models,” IEEE Transactions on Software\nEngineering, vol. 48, no. 4, pp. 1385–1399, 2022.\n[35] J. Li, G. Li, Z. Li, Z. Jin, X. Hu, et al., “CodeEditor: Learning to edit\nsource code with pre-trained models,” ACM Transactions on Software\nEngineering and Methodology , vol. 32, no. 6, 2023.\n[36] J. Zhang, S. Panthaplackel, P. Nie, J. J. Li, and M. Gligoric, “CoditT5:\nPretraining for source code and natural language editing,” inProceedings\nof the 37th IEEE/ACM International Conference on Automated Software\nEngineering (ASE), 2023.\n[37] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, et al., “Lan-\nguage models are few-shot learners,” in Advances in Neural Information\nProcessing Systems (NeurIPS) , vol. 33, pp. 1877–1901, 2020.\n[38] Y . Watanobe, M. M. Rahman, T. Matsumoto, U. K. Rage, and P. Raviku-\nmar, “Online judge system: Requirements, architecture, and experi-\nences,” International Journal of Software Engineering and Knowledge\nEngineering, vol. 32, no. 4, pp. 1–30, 2022.\n[39] R. Puri, D. S. Kung, G. Janssen, W. Zhang, G. Domeniconi, et al. ,\n“CodeNet: A large-scale AI for code dataset for learning a diversity\nof coding tasks,” in Proceedings of the 35th Conference on Neural\nInformation Processing Systems (NeurIPS) Track on Datasets and\nBenchmarks (Round 2) , 2021.\n[40] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, et al. ,\n“Training language models to follow instructions with human feedback,”\nin Advances in Neural Information Processing Systems (NeurIPS), 2022.\n[41] T. McCabe, “A complexity measure,” IEEE Transactions on Software\nEngineering, vol. SE-2, no. 4, pp. 308–320, 1976.\n[42] V . I. Levenshtein, “Binary Codes Capable of Correcting Deletions,\nInsertions and Reversals,” Soviet Physics Doklady, vol. 10, p. 707, 1966.\n[43] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri, “Asleep\nat the keyboard? assessing the security of github copilot’s code contri-\nbutions,” in Proceedings of the 2022 IEEE Symposium on Security and\nPrivacy (SP), pp. 980–994, 2022.\n[44] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, et al. ,\n“Llama: Open and efficient foundation language models,”arXiv preprint,\n2023.\n[45] OpenAI, “Gpt-4 technical report,” arXiv preprint, 2023.\n[46] Q. Zheng, X. Xia, X. Zou, Y . Dong, S. Wang, et al. , “Codegeex: A\npre-trained model for code generation with multilingual evaluations on\nhumaneval-x,” arXiv preprint, 2023.\n[47] R. Li, L. B. Allal, Y . Zi, N. Muennighoff, D. Kocetkov,et al., “Starcoder:\nmay the source be with you!,” arXiv preprint, 2023.\n[48] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, et al. , “Wizardcoder:\nEmpowering code large language models with evol-instruct,” arXiv\npreprint, 2023.",
  "topic": "Code refactoring",
  "concepts": [
    {
      "name": "Code refactoring",
      "score": 0.9701066613197327
    },
    {
      "name": "Computer science",
      "score": 0.8433170914649963
    },
    {
      "name": "Maintainability",
      "score": 0.8379780650138855
    },
    {
      "name": "Programming language",
      "score": 0.7089530229568481
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.625887930393219
    },
    {
      "name": "Python (programming language)",
      "score": 0.6085106730461121
    },
    {
      "name": "Source lines of code",
      "score": 0.5821635127067566
    },
    {
      "name": "Disk formatting",
      "score": 0.5285410284996033
    },
    {
      "name": "Code (set theory)",
      "score": 0.4958361089229584
    },
    {
      "name": "Software engineering",
      "score": 0.45800966024398804
    },
    {
      "name": "Cyclomatic complexity",
      "score": 0.42346152663230896
    },
    {
      "name": "Scripting language",
      "score": 0.41061365604400635
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36976301670074463
    },
    {
      "name": "Software",
      "score": 0.22314932942390442
    },
    {
      "name": "Operating system",
      "score": 0.10751831531524658
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141591182",
      "name": "University of Aizu",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I201537933",
      "name": "Tohoku University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I2251713219",
      "name": "NTT (Japan)",
      "country": "JP"
    }
  ]
}