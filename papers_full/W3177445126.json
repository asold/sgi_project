{
    "title": "IrEne: Interpretable Energy Prediction for Transformers",
    "url": "https://openalex.org/W3177445126",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2129761198",
            "name": "Qingqing Cao",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A2763032118",
            "name": "Yash Kumar Lal",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A2165999522",
            "name": "Harsh Trivedi",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A2237204116",
            "name": "Aruna Balasubramanian",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A2116597581",
            "name": "Niranjan Balasubramanian",
            "affiliations": [
                "Stony Brook University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4212774754",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W4250255992",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W2136334324",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2101234009",
        "https://openalex.org/W2906043559",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W4287726890",
        "https://openalex.org/W1577404640",
        "https://openalex.org/W4287868032",
        "https://openalex.org/W3119866685",
        "https://openalex.org/W3006130287",
        "https://openalex.org/W3005957694",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2953384591",
        "https://openalex.org/W2789554134",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2975429091",
        "https://openalex.org/W4310492983",
        "https://openalex.org/W1993158854",
        "https://openalex.org/W4302023899",
        "https://openalex.org/W3106171539"
    ],
    "abstract": "Qingqing Cao, Yash Kumar Lal, Harsh Trivedi, Aruna Balasubramanian, Niranjan Balasubramanian. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 2145–2157\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2145\nIrEne: Interpretable Energy Prediction for Transformers\nQingqing Cao, Yash Kumar Lal, Harsh Trivedi,\nAruna Balasubramanian, Niranjan Balasubramanian\nDepartment of Computer Science\nStony Brook University\nStony Brook, NY 11794, USA\n{qicao,ylal,hjtrivedi,arunab,niranjan}@cs.stonybrook.edu\nAbstract\nExisting software-based energy measurements\nof NLP models are not accurate because they\ndo not consider the complex interactions be-\ntween energy consumption and model execu-\ntion. We present IrEne, an interpretable and\nextensible energy prediction system that accu-\nrately predicts the inference energy consump-\ntion of a wide range of Transformer-based\nNLP models. IrEne constructs a model tree\ngraph that breaks down the NLP model into\nmodules that are further broken down into\nlow-level machine learning (ML) primitives.\nIrEne predicts the inference energy consump-\ntion of the ML primitives as a function of\ngeneralizable features and ﬁne-grained run-\ntime resource usage. IrEne then aggregates\nthese low-level predictions recursively to pre-\ndict the energy of each module and ﬁnally of\nthe entire model. Experiments across multiple\nTransformer models show IrEne predicts infer-\nence energy consumption of transformer mod-\nels with an error of under 7% compared to the\nground truth. In contrast, existing energy mod-\nels see an error of over 50%. We also show\nhow IrEne can be used to conduct energy bot-\ntleneck analysis and to easily evaluate the en-\nergy impact of different architectural choices.\nWe release the code and data at https://\ngithub.com/StonyBrookNLP/irene.\n1 Introduction\nAccurately measuring the energy consumption of\nNLP models is becoming ever more important.\nModels are growing exponentially, with billions,\neven approaching trillions, of parameters with\ncorrespondingly large resource consumption (e.g.\nGPT-3 (Brown et al., 2020) has 175 billion param-\neters and Switch Transformers can have 1.6 trillion\nparameters (Fedus et al., 2021)). Recent works\nhave sought to estimate energy consumption and\nsuggest ways to reduce the resulting costs and car-\nbon impacts (Strubell et al., 2019; Schwartz et al.,\n2019; Henderson et al., 2020; Anthony et al., 2020)\nUnfortunately, there are no easy-to-use and ac-\ncurate solutions for measuring or predicting the\nenergy consumption. On the one hand, measur-\ning energy consumption directly through hardware\npower monitors is not feasible as it requires ex-\nclusive access to the hardware and detailed instru-\nmentation. On the other hand, there are software\nmodels that predict energy as a function of resource\nutilization (Strubell et al., 2019; Henderson et al.,\n2020) but these energy prediction models are in-\naccurate (Cao et al., 2020). The inaccuracy stems\nfrom the prediction models not accounting for the\ncomplex interactions between energy consumption\nand resource utilization.\nIn this work, we focus on inference energy which\ncan incur substantial costs especially for models\nthat support high-volume web services. We ask\nhow we can build an energy prediction method that\nis accurate, interpretable, and extensible. We make\nthree contributions in answering this question.\nFirst, we frame the problem of interpretable en-\nergy prediction over a model tree abstraction. This\nabstraction represents the model as the root node\nthat is composed from model-speciﬁc modules,\nwhich themselves are recursively composed from\nlower-level machine learning (ML) primitives, ones\nthat are not model-speciﬁc. Given a model, the en-\nergy prediction problem is framed as the task of\npredicting the energy of all the nodes in its model\ntree abstraction. The result is that IrEne can predict\nnot only the inference energy consumption of the\nentire model, but also of its components, making\nthe energy prediction highly interpretable.\nSecond, we develop IrEne, that includes a multi-\nlevel prediction method that predicts energy in all\nnodes of the abstraction tree in a bottom-up fashion\nusing resource utilization and model description\nfeatures. For each of the leaf-nodes that are re-used\n2146\nin different models, the ML primitives, IrEne uses\na separate regressor trained on ground-truth energy\nmeasurements. One simple way to get energy for\nall other higher-level nodes is to recursively sum-\nup the values. While this works reasonably well\n(even better than a prior prediction model), direct\nsumming of the raw predictions is sub-optimal be-\ncause the error can propagate through the model\ntree thus making upper-level nodes estimation more\nerroneous. Instead, we learn a single regressor for\nall intermediate nodes, one that essentially adjusts\nthe sum of children’s predicted energy values based\non features of the children. Since IrEne is built on\ntop of energy predictions of ML primitives that are\nnot model speciﬁc, it is generalizable and can be\nused to predict the energy for previously unseen\n(Transformer-based) models.\nThird, to evaluate IrEne, we create an evaluation\ndataset with ground-truth energy measurements for\nmultiple Transformer-based models at all levels in\nthe model tree abstraction. Evaluations show that\nIrEne is more accurate – with an average model-\nlevel energy error of 5 ∼7% compared against\nthe ground-truth, while existing software-based\nmethod (Strubell et al., 2019) has over 55% error.\nThe module-level energy errors are also substan-\ntially small showing that IrEne is both accurate and\ninterpretable. Last, we also conduct multiple anal-\nyses that show the utility of IrEne for interpretable\nenergy predictions.\n2 Related work\nOver the last couple of years, there has been\nincreased interest in the energy consumption of\nNLP models, starting with the work by Strubell\net al. (Strubell et al., 2019). This work, and a\nfollow up software framework called experiment-\nimpact-tracker (Henderson et al., 2020) tracks the\nresource (i.e., CPU, GPU, memory) utilization of\nan NLP model and predicts energy consumption\nas a function of resources. However, our previ-\nous study shows that this type of resource utiliza-\ntion only modeling can be highly inaccurate (Cao\net al., 2020). This is in part due to the complex\nrelationship between resource utilization and en-\nergy consumption. Further, there are other activi-\nties that are not accounted via resource utilization\nsuch as data movement in GPU memory which can\nalso cause signiﬁcant energy footprint (Chen et al.,\n2016; Boroumand et al., 2018).\nOther works (Zhou et al., 2020; Schwartz et al.,\n2019) report the energy numbers through alternate\nmetrics including dollar cost or in terms of ﬂoating\npoint operations. However, these do not directly\nmap to the energy consumption. Energy prediction\nof applications on mobile devices is a well-studied\ntopic in the systems community (Pathak et al., 2011,\n2012; Yoon et al., 2012; Cao et al., 2017) but these\nwork require ﬁne-grained understanding of the ap-\nplication. None of the existing systems predict\nenergy for NLP applications.\n3 Interpretable Energy Prediction\nIn this section we ﬁrst state our design goals, moti-\nvate the abstraction, and problem formulation for\ninterpretable energy prediction.\n3.1 Design Goals\nWe design the energy prediction model with three\ndesign goals: (i) accurate prediction while incur-\nring low proﬁling overheads; high overheads when\nmeasuring runtime resource utilization can hide the\ntrue energy costs of the NLP model, (ii) provide\ninterpretable energy analysis of the components\ninside the NLP model, especially for analyzing en-\nergy bottlenecks; (iii) extensible and generalizable,\nin the sense that, they are trained once but can\nwork on unseen NLP models to remain useful as\nnew models emerge.\n3.2 Model Tree Abstraction\nTo achieve the above goals, we ﬁrst need a repre-\nsentation of the NLP model that is at a suitable\nabstraction both from interpretability and general-\nization standpoints.\nOn the one hand, using only low-level abstrac-\ntions such as the math operations can help with\neasy generalization to new models as their units\nare basic math (or other compute) operations that\nare building blocks of any model. However, they\nlack interpretability since they don’t directly con-\nvey the model architecture semantics. For example,\na BERT (Devlin et al., 2019) model has matrix\nmultiplications in both the self-attention and feed\nforward layers. Only having the energy of each ma-\ntrix multiplication alone, without knowing which\nhigher level logic units (i.e., either self-attention\nor feed forward layer) they belong to, does not\nhelp analyze if they are the bottlenecks for that\nparticular unit. On the other hand, high-level ab-\nstractions preserve the architecture semantics and\nare interpretable for practitioners, but they don’t\n2147\nModule Level\nML Level\nBertModel\nBertEmbeddings BertEncoder BertPooler\nEmbedding:word LayerNorm BertLayer:0\nBertAttention BertIntermediate BertOutput\nBertSelfAttention BertSelfOutput\nLinear:query matmul softmax Linear:dense LayerNorm\nLinear:dense Linear:dense LayerNorm\nLinear:dense Tanh\nFigure 1: A tree view of a 1-layer BERT model. The yellow rectangle nodes stand for basic machine learning (ML)\nlevel operations. The brown rectangle nodes are also ML level which are non-parametric (i.e., has no trainable\nparameters). The ML level operations are model-agnostic and provided by machine learning software framework.\nThe light blue oval nodes denote model-speciﬁc operations that reﬂect the architectural semantics given by the\nmodel developer, for example BertSelfAttention was designed to transform input sequence representations by\n‘attending\" (weighted combination) to each position of the input sequence.\neasily generalize to unseen models that may not\nhave the same modules used for training.\nInstead, we use a model tree abstraction that rep-\nresents the model nodes in three-levels: math level,\nmachine learning (ML) level and module level.\nMath level nodes are a ﬁnite set of mathematical\noperations (like addition, subtraction, matrix multi-\nplication etc); they form model-agnostic ML level\nnodes (such as Linear, LayerNorm etc.), which fur-\nther can be used to construct complex module level\nnodes. Module level nodes are groups of lower ML\nlevel node operations that reﬂect the logic units\nof the NLP algorithms deﬁned by model authors.\nThe model tree abstraction is such that each parent\nnode captures computation of all of its children\nnodes. Figure 1 shows an example of a one-layer\nBERT (Devlin et al., 2019) model (omitted math\nlevel nodes). The execution of the model tree nodes\ncan be in parallel, but current systems have a ﬁxed\nsequential order for executing the sibling nodes.\nIn this work, we only focus on sequential execu-\ntion. Note that the model tree doesn’t capture the\norder of execution. E.g., BertOutput appears\nright after BertIntermediate in BERT’s com-\nputation graph, but here they’ll be represented\nas siblings of the same parent BertLayer:0,\nand their energy will be treated separately.\nThe parent node BertLayer:0 encapsulates\nthe energy and computation of its children\nnode BertIntermediate, BertOutput, and\nBertAttention, in no particular order.\n3.3 Problem Deﬁnition\nWith this new model tree abstraction, we formally\nstate the problem of interpretable energy estimation\nof a NLP model. Given a model tree abstraction\nof a NLP model Mconsisting of a set of nodes\nN= {n|nml ∪nmod}(nml is the set of ML level\nnodes, nmod is the set of module level nodes), for\nan input size I(a pair of batch size band sequence\nlength s) 1, we can predict the energy En for every\nnode nin the model tree. The energy of root node\nis the energy for the entire model.\n4 Interpretable Prediction with IrEne\nFigure 2 shows the IrEne architecture. IrEne takes\nthe user-speciﬁed model and builds an energy pre-\ndictor for a target hardware device. The model\nis run once on the target hardware and the run-\ntime resource utilization is logged. During this run,\nIrEne uses code instrumentation and just-in-time\n(JIT) run-time tracing to break down the model into\nsub-components, and extracts a model tree repre-\nsentation (see details in §A).\nIrEne then provides interpretable energy analy-\nsis by predicting the energy for every node in the\nmodel tree in a bottom-up fashion. At the leaves,\nwhere the nodes correspond to the ML primitives,\nIrEne uses separate regression models for each type\nof ML primitive (e.g., one regressor for Linear\nLayer, another for LayerNorm etc.). For the inter-\nmediate nodes, their energy is predicted recursively\nusing a single regressor that makes a weighted com-\nbination of the predicted energy values from its\nchildren. For both types of regressors, they use\nfeatures that are derived from resource utilization\n(e.g. cpu utilization) and generalized node features\n1The batch size and input sequence length together decide\nthe amount of input data to the model, therefore, they both\naffect the model energy consumption.\n2148\nResource features\nModel features\nModel specs\nBertModel\nBertEmbeddingsBertEncoderBertPooler\nEmbedding:wordLayerNormBertLayer:0\nBertAttentionBertIntermediateBertOutput\nBertSelfAttentionBertSelfOutput\nLinear:querymatmulsoftmaxLinear:denseLayerNorm\nLinear:denseLinear:denseLayerNorm\nLinear:denseTanh\nJIT tracing\nProfile\n Estimation \nmodel\nPredicted energy for each nodeIrEne Energy Estimation\nFigure 2: IrEne works by taking model speciﬁcations (for example, model code) as inputs and extracting a model\ntree representation using code instrumentation and run-time tracing. IrEne then runs the model once on a given\nhardware and feeds resource proﬁles combined with the model computation features into a regressor to predict the\nenergy of the entire model tree representation. The root of the tree represents the energy of the entire NLP model\nand each child node represents the energy of different modules/ML operators that make up the model.\n(e.g. size of inputs) enabling accurate multi-level\nenergy prediction.\nIrEne represents higher-level modules via gener-\nalizable features and the ML primitives. Even if the\nintermediate modules are model-speciﬁc (e.g. Bert-\nSelfAttention), the features are general, allowing\nIrEne to predict energy of unseen models.\nThe IrEne model is trained using ground-truth\nenergy measurements of ML primitives and a hand-\nful of NLP models; we use a highly accurate hard-\nware power monitor to measure ground truth energy\n(§A). Of course, one can use the power monitor to\nmeasure energy directly at runtime. However, this\nis cumbersome and requires physical access to the\ndevice which is not always feasible with cloud-\nbased deployments. Further, the hardware meter\nonly measures the total energy, which is not inter-\npretable in terms of its components.\n4.1 Multilevel energy prediction\nAt the leaf-level, the energy prediction problem\nrequires predicting the energy of ML primitives. As\nan ofﬂine step, IrEne ﬁrst enumerates all relevant\nML primitives and builds a specialized regressor\nfor each primitive by training over ground truth\ndata. In some cases, model developers can deﬁne\ntheir own ML primitives. We extract information\nabout such custom primitives from the JIT trace.\nFormally, for a leaf node nwith ML primitive i,\nwe predict the energy of the node as:\nPMLi\ne (n) =Wi ∗feat(n) +bi (1)\nusing primitive speciﬁc parameters Wi the\nweight vector and bi the bias. We learn these pa-\nrameters using a mean squared error loss between\npredicted Pe(n) and ground-truth energy Ge(n).\nOur hierarchical tree representation gives a natu-\nrally interpretable way of propagating this predic-\ntion through the tree. Since each node represents\ntotal computation of its children nodes, the total\nenergy from children nodes should also roughly\ncorrespond to that of the parent node. Formally,\nPe(n) =\n∑\nc ∈ child(n)\nPe(c) if n is non-leaf\n= PMLi\ne (n) if n is leaf (2)\nWe call this baseline prediction model Predict-\nedSum. This model is interpretable but naively\nsumming up the energy values accumulates error\ngoing up the tree and results in noisy module-level\npredictions. To account for this, we use a weighted\nsum of child node energy, where the weights are\nlearnt using node features. Formally,\nPe(n) =\n∑\nc ∈ child(n)\nα(c) ∗Pe(c) if n is non-leaf\n= PMLi\ne (n) if n is leaf\nα(c) = 1 +tanh(W∗feat(c) +b)/τ (3)\nwhere W and bare parameters and τ is a hyper-\nparameter. Unlike ML primitives, here we have a\nsingle regressor with one set of weight vector (W)\nand bias scalar (b) parameters across all non-leaf\nnodes of any type. Note that this single regres-\nsor doesn’t predict node’s energy directly, but de-\ntermines how much the predicted energy from its\nchild node should be scaled before summing the\nchildren node energy. It does this recursively start-\ning from the root, and hence encodes tree structure\nin its computation. We do not learn node-speciﬁc\nregressors because that does not allow generalizing\nto new models that may have different modules\n2149\nthan the ones during training.\nSince the method is essentially calibrating the\nsum of the energy values, regularizing the model\nso that the computed weights on the energy values\nto be around 1 helps the learning. We do this by\nequation 3, which makes the range of computed\nweights, α(c) to be within 1 ±τ. To supervise this\nmodel, we use the ground-truth energy from all the\nnon-leaf nodes, and we train it in an end-to-end\nfashion. Formally,\nloss(n) =\n∑\ns ∈ subtree(n)\n(\nPe(s) −Ge(s)\n)2\nGe(s)2 (4)\nWe scale the mean squared error with ground-\ntruth energy, since scales of energy at different\nlevels of the tree are vastly different. We refer to\nthis model as the End2End regressor, since the\nerror signal in energy prediction of any node back-\npropagates through the whole subtree. We use this\ntraining scheme in IrEne. In our evaluation (sec-\ntion 5), we perform an ablation study to show why\nthe tree structure and the end-to-end regressor is\ncrucial for accuracy.\n4.2 Featurization\nWe design two categories of energy-relevant fea-\ntures in IrEne : (i) the model features that reﬂect\nhardware-independent compute and memory infor-\nmation, and (ii) the resource features that capture\nhow the models use hardware resources and cause\nenergy activities. Table 1 shows the features used\nin IrEne. For the model description related informa-\ntion, we use features that characterize the compute,\nmemory, and size of input etc. These are features\nthat are independent of the underlying hardware.\nFor resource features, we use utilization, usage\nand clock speed of hardware components including\nCPU, memory and GPU. Note that these two sets\nof features are extensible, meaning that one can\nadd more either hardware-speciﬁc features or new\nmodel features. See Appendix sections A.2 and\nA.3 for details on how we obtain these features.\n5 IrEne Evaluation\nOur evaluation is aimed at measuring the accuracy\nof IrEne relative to ground truth and the state-of-\nthe-art. We show the IrEne only causes 5-7% error\nfor the model energy prediction. We also show\nthat for a given Transformer model, IrEne can be\nused to ﬁnd the energy bottlenecks and analyze the\nenergy versus task performance trade-offs.\nbatch_size : batch size\nseq_len : # of input tokens\nﬂops : ﬂoating point operations (unit: million)\nmem_bytes : memory read and write (unit: MiB)\ncpu_util : CPU utilization (unit: %)\nmem_usg : memory usage (unit: %)\ngpu_util : GPU processor utilization (unit: %)\ngm_usg : GPU memory usage (unit: %)\ng_clk : GPU processor clock speed (unit: MHz)\ngm_clk : GPU memory clock speed (unit: MHz)\nlatency : inference latency (unit: s)\ngpu_energy : GPU driver energy (unit: joule)\nTable 1: Features used for energy estimation in IrEne.\nSpeciﬁcation PC1 PC2\nCPU Intel i9-7900X Intel i7-6800K\nMemory 32 GiB 32 GiB\nGPU 2 ×GTX 1080 Ti 2 ×GTX 1070\nGPU Memory 11.2 GiB per GPU 8 GiB per GPU\nStorage 1 TiB SSD 1 TiB SSD\nTable 2: Target hardware speciﬁcations.\n5.1 Setup\nTarget Hardware: we use 2 GPU-equipped desk-\ntop PCs as the target hardware for running our\nmodels. See Table 2 for details.\nSoftware and models: We perform inference in\nTransformer models using PyTorch (Paszke et al.,\n2019) v1.7 through the HuggingFace Transform-\ners (Wolf et al., 2020) library. The six mod-\nels we study are — BERT-base (Devlin et al.,\n2019), RoBERTa-base (Liu et al., 2019), Distill-\nBERT (Sanh et al., 2020), DistilGPT2 (Sanh et al.,\n2020; Radford et al., 2019), OpenAI GPT (Radford\net al., 2018) and GPT2 (Radford et al., 2019).\nSoftware-based Measurement Baseline: For\ncomparisons, we use the software-based energy\nmeasurements provided by the experiment-impact-\ntracker (Henderson et al., 2020) which estimates\nenergy as a function of the GPU, CPU, and mem-\nory utilization. The method computes energy by\naggregating resource usage as follows: etotal =\nPUE ∑\np(pdramedram + pcpuecpu + pgpuegpu),\nwhere presource 2 are the percentages of each sys-\ntem resource used by the attributable processes\nrelative to the total in-use resources and eresource\nis the energy usage of that resource. The constant\n2resources can be dram, cpu, gpu\n2150\nfor power usage effectiveness (PUE) compensates\nfor extra energy used to cool or heat data centers.\n5.2 Dataset and Evaluation Methodology\nFor each model, we obtain the model tree and for\neach node in it, we associate ground-truth energy\nmeasurements using the power monitor and its re-\nsource features using low-overhead logging (Sec-\ntion A). For each node we run it repetitively for\n20 seconds, since it often takes a very short time\nfor one run (e.g. from 0.1 to 100 millisecond). We\nrepeat this process for ﬁve rounds (the variations\nare within <1%) and record the average energy as\nthe ground-truth for the node. We use 1 GPU to\nrun all experiments. We record the start and end\ntimestamp of the model program, and extract the\nenergy values by comparing and aligning the times-\ntamps from the resource proﬁler logs and power\nmonitor logs.\nGround Truth Energy: We measure ground truth\nenergy using a emonPi power monitor (Hudson,\n2021) which is open source. The emonPi uses a\nclip-on CT sensor to monitor the energy consumed\nby the computer which records the passthrough\ncurrent and voltage every 170 ms. This allows\nus to accurately measure the power draw at a sub\nsecond granularity. We obtain current, voltage,\nand timestamp values from the power meter’s built-\nin serial port. The energy ( e) consumed during a\ntime period is then calculated using the sampled\ncurrent (It) and voltage (Vt) values in that period:\ne= ∑\nt VtIt.\nTo guarantee the consistency and reliability of\nthe hardware energy measurements, we cool down\nthe PCs after each experiment ﬁnishes to avoid po-\ntential overheating issue that can cause subsequent\nenergy distortions. We measure the standby power\nconsumption (when the CPU load is <0.1%) and\nensure before running the experiments that the PC\ndoes not draw more than the standby power. Fur-\nther, no other application is running during our\nexperiments.\nTo understand the scale of energy usage, Table 3\nshows the estimated energy consumption (in kWh)\nusing our ground truth measurement. We also show\nthe cost of answering one million queries (in USD)\nwhen using a BERT-base model in a reading com-\nprehension (over one passage), and in an end-to-\nend setting (over 150 passages) ignoring retrieval\ncompute. For reference, Google search handles\nmillions of queries every minute (Kenshoo, 2019).\nUse Case Energy/1M\nQns (kWh)\nCost/1M\nQns (USD)\nQA over a single passage 161 21.24\nQA over 150 passages\n(ignore search/retrieval) 24,000 3,165\nTable 3: Example energy for BERT-base QA models\nusing batch size 16 and sequence length 256 on PC1\nusing one GPU. The cost is estimated at 13.19 cents\nper kWh. 3\nQuantity BERT-base DistilBERT GPT2\n# ML Nodes 3864 1932 2997\n# Module Nodes 2100 560 972\n# Model Nodes 28 28 28\n# Tree Depth 6 5 4\nTable 4: Energy dataset statistics for BERT-base, Distil-\nBERT and GPT2 model. For each model, we construct\n28 trees (model nodes) with batch sizes from 8 to 32\nwith a step of 8, and input sequence lengths from 32\nto 256 with a step of 32. We associate features and\nground-truth energy for each node in these trees.\nEnergy Dataset: To evaluate the energy predic-\ntion, we create a dataset that cover a wide range\nof input sizes for the six studied Transformer mod-\nels and the 24 BERT model variants (Turc et al.,\n2019). Each instance in the dataset can be of type\nML, Module or Model level and is associated with\nfeatures shown in Table 1 and hardware measured\nenergy. We show the statistics of the dataset for\nBERT-base, DistilBERT and GPT2 in Table 4.\nEnergy Error Metric: We measure the energy\nerror percentage as 100 ×|PE −GE|/GE, where\nPE is the predicted energy and GEis the ground\ntruth energy.\n5.3 Energy Prediction Results\nWe compare IrEne with the existing software mea-\nsurement methods (Strubell et al., 2019; Henderson\net al., 2020). We apply their method directly for all\nthe models in our dataset. Note that their method\nis a fully-deﬁned estimation model with a ﬁxed set\nof parameters without any training. For IrEne ex-\nperiments, we report cross-validated evaluation on\nthe energy prediction dataset — leaving data from\none model out of training set and evaluating on it,\nand then repeating the same for all the models.\n3based on the US national average as of May 2021\naccording to https://www.electricchoice.com/\nelectricity-prices-by-state .\n2151\n5.0 7.5 10.0 12.5 15.0\nModel error (%)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cumulative probability\nFigure 3: The CDF of model’s predicted energy errors.\nWe see that for 99% of the cases, the error is under 16%\nIrEne is accurate Table 5 shows the energy pre-\ndiction errors at the model-level for all the models\non the two PCs. The existing software-based base-\nline method from Strubell et al. (2019) incurs large\nenergy prediction errors of over 50%.\nIrEne on the other hand incurs substantially\nlower errors, with at most 7.6% errors across the\nmodels, showing its value for reliable and accurate\nenergy analysis. As seen from the cumulative dis-\ntribution function for the model errors in Figure 3,\nall of IrEne’s errors are below 17% and nearly half\nof its errors are below 10%. We note here that our\nleave-one-model-out cross validation speciﬁcally\nevaluates the generalizability of IrEne.\nML and Module Levels Errors are also low. Ta-\nble 7, 6 show a break down of the IrEne errors at\nthe ML and module levels respectively. Accurately\npredicting ML level energy is key to accurate pre-\ndictions for at the module level and higher, as the\nerrors will accumulate up the model tree in IrEne.\nIt turns out that we can indeed predict ML level\nenergy with high-levels of accuracy — errors are\nlower than 1%, providing reliable values for the\nmodule level predictions. Note that even unseen\nmodels (ie ones evaluated in the test partition) will\nbe made up of the same set of ML primitives (per-\nhaps with different input and batch sizes). The\nresults here cannot be directly generalized to un-\nseen ML-primitives. Module level errors are higher\nand vary in range (5.4% to 16.7%) across different\nmodels. Module level errors also turn out to be\nhigher than the model level errors. This is mainly\nbecause the module level errors are averages across\nall intermediate module level nodes in the model\ntree; some modules might have bigger errors, but\nthese get calibrated by our End2End energy re-\ngressor. We further characterize these effects in\nIrEne ablation and validation analysis.\n5.4 Feature Ablations\nTable 8 shows the contribution of model and re-\nsource features in IrEne energy prediction. We\nobserve that resource features provide most of the\nbeneﬁts for energy estimation IrEne for all levels,\nconﬁrming that resource information is important\nfor energy prediction. Model features do not reduce\nML level error because the error is already small,\nbut they help further reduce the prediction errors\nfor module and model levels and combining model\nand resource features together brings the average\nestimation errors further down to 8.5% and 5.5%.\n5.5 Modeling Ablations\nTo understand the impact of learning and the ar-\nchitectural choices of aggregating ML level energy\ninto module level energy in IrEne affect the model\naccuracy, we build three (ablated) models:\nIs end-to-end learning necessary? To test this,\nwe build aStepWise regressor that simply learns to\npredict the energy of parent node from the ground-\ntruth energy of its child nodes at the training time.\nAt the test time, it uses predicted energy generating\npredictions from ground up.\nPe(n) =\n∑\nc ∈ child(n)\nα(c) ∗Ge(c) Training\nPe(n) =\n∑\nc ∈ child(n)\nα(c) ∗Pe(c) Testing (5)\nHere, α(c) and loss are still as deﬁned in equa-\ntion 3 and 4 respectively. However, unlike the IrEne\n(End2End) regressor, the errors in the prediction\nof root node, do not backpropogate to its predic-\ntion of descendant nodes i.e. there is no end-to-end\ntraining.\nIs tree-structure necessary? To test this, we build\nan Unstructured regressor that ignores the tree\nstructure completely, and directly predicts the en-\nergy from the feature representation of nodes (Mod-\nule and Model level) using linear regression as in\nequation (1). Unlike ML-level regressor though,\nhere we need to use single set of parameters for\ncommon across the nodes.\nIs learning necessary? To test this, we use the\nPredictedSum model (equation 2). Recall this\nmodel also aggregates energy predictions over the\ntree-structure but has no parameters to train.\nTable 9 shows the ablation of IrEne with respect\nto different algorithmic choices of the module level\nenergy aggregation. First, we ﬁnd that the regres-\nsor that ignores the tree structure (Unstructured)\n2152\nMachine System BERT-base DistilBERT RoBERTa-base GPT2 DistilGPT2 OpenaiGPT Average\nPC1 Strubell et al., 2019 57.9 56.3 62.5 62.6 55.9 61.8 57.8\nIrEne 5.8 11.6 7.1 3.5 2.2 2.7 5.5\nPC2 Strubell et al., 2019 55.1 52.6 58.9 54.6 49.8 60.6 55.6\nIrEne 10.0 9.4 7.1 6.1 4.9 5.9 7.2\nTable 5: Energy Prediction Errors at Model level: Comparing IrEne and a software measurement baseline for the\ntwo PCs. IrEne is signiﬁcantly more accurate than Strubell et al., 2019.\nMachine BERT-base DistilBERT RoBERTa-base GPT2 DistilGPT2 OpenaiGPT Average\nPC1 5.37 5.93 5.44 14.92 14.73 13.98 8.54\nPC2 6.78 7.96 6.69 16.65 16.41 16.07 10.16\nTable 6: Energy Prediction Errors at module levels using IrEne on two PCs. Note that in Table 11 at the appendix,\nwe also show a subset of the module level energy errors using Strubell et al., 2019.\nMachine Embedding LayerNorm Linear Tanh MatMul Softmax Conv1D Average\nPC1 0.65 0.89 0.60 0.82 0.61 1.0 0.58 0.70\nPC2 0.38 0.66 0.55 0.43 0.43 0.67 0.41 0.53\nTable 7: Energy Prediction Errors at ML levels using IrEne on two PCs. Note that the evaluation for these operation-\nspeciﬁc (eg. Embedding) regressors is done using the leave-one-model out setting as before.\nML Module Model\nIrEne 0.70 8.54 5.52\nw/o resource features 5.76 11.54 7.08\nw/o model features 0.63 8.87 7.32\nTable 8: Energy Prediction Errors of IrEne with ablated\nfeatures. Both model and resource features help the\nIrEne’s performance at model and module levels, while\nresource features are sufﬁcient for ML-level.\nModule Model\nIrEne (End2End) 8.54 5.52\nStepWise 9.28 14.84\nPredictSum 16.4 17.69\nUnstructured 278.0 39.79\nTable 9: Energy Prediction Errors of IrEne using differ-\nent module/model level regressors on PC1. Tree struc-\nture of the regressor crucial, and end-to-end optimisa-\ntion on tree helps IrEne to get lower errors.\nperforms signiﬁcantly worse than all other regres-\nsors that do consider it. Interestingly, learning\nwithout structure even performs worse than Pre-\ndictedSum regressor that naively adds child en-\nergy without any learning, highlighting the impor-\ntance of tree-structure. Further, learnt weighted\nsum outperforms PredictedSum regressor. In par-\nticular, End2End regressor performs better than\nStepWise regressor showing the importance of op-\ntimizing on whole tree in an end-to-end fashion.\n5.6 Interpretable Energy Analysis\nIn this section, we use the interpretable energy anal-\nysis from IrEne to show energy bottlenecks for\ngiven Transformer models, how energy varies for\ndifferent model architectures, and how it can be\nused to effectively pick accuracy-energy trade-offs.\nFinding energy bottlenecks: We use IrEne to ana-\nlyze the energy bottlenecks in Transformer models.\nFor simplicity of analysis, we predict the energy for\nmodules that are immediate parents of the ML level\nnodes and use it calculate the percentage of energy\nit contributes to the model overall. Table 10 shows\nthe energy breakdown of two models: RoBERTa-\nbase and GPT2. We observe that self-attention\nlayers in RoBERTa-base model consume 31% of\nthe total energy while it is the feed forward layers\nin GPT2 that consume more than 59% of the energy.\nThe module level energy breakdown of all models\nin Table 12 in Appendix C. We also present the full\nenergy breakdown of the BERT-base model and an-\nnotate each node with predicted energy percentage\nin Figure 5 in the Appendix.\nTask accuracy versus energy tradeoffs:\nWe ﬁne-tune BERT-24 models (Turc et al.,\n2019) on the Stanford Sentiment Treebank V2\n(SST2) (Socher et al., 2013) using the default exam-\nples in the HuggingFace Transformers (Wolf et al.,\n2020) without any hyperparameter tuning. We eval-\nuate the accuracy on the dev set of SST2. These\n2153\nModule Energy %\nRobertaSelfAttention 31.24\nRobertaIntermediate 30.57\nRobertaOutput 28.64\nRobertaSelfOutput 09.11\nRobertaEmbeddings 00.41\nRobertaPooler 00.03\n(a) RoBERTa-base\nModule Energy %\nMLP 59.13\nAttention 37.94\nLayerNorm 2.84\nEmbedding 0.1\n(b) GPT2\nTable 10: Module level predicted energy breakdown\nof two Transformer models. We average the energy of\nthese modules across all input sizes for each model ar-\nchitecture. Self-attention is the energy bottleneck in\nRoBERTa-base, but for GPT2, the bottleneck is feed\nforward layers (MLP module).\nSST2 Accuracy\nEnergy (J)\n0\n5\n10\n15\n20\n80 82 84 86 88 90\nTrue Energy Predicted Energy\nFigure 4: Ground-truth and predicted energy vs accu-\nracy on SST2 task for BERT-24 models. Energy data is\ncollected with batch size 16 and sequence length 128.\nBecause our energy predictions are accurate, we can\nuse energy consumption vs NLP model accuracy trade-\noffs to select a model.\nmodels are not part of our energy prediction train-\ning data. We additionally exclude BERT-base from\ntraining data to show the extensibility of IrEne.\nGiven an energy budget, IrEne allows for selec-\ntion of an optimal architecture that gets the highest\naccuracy for a task. In Figure 4, we see that it is\npossible for models to use more energy but return\nlower accuracy than other models which might use\nless energy. Similarly, given an accuracy target, we\ncan choose an architecture with the lowest energy\nuse. For example, for a target of 88% accuracy or\nabove, there are many such models ranging from\n4J all the way to 12J. Last, we point out that the\ntrade-off curve based on the predicted energy mir-\nrors that of the ground-truth well enough to be used\nas an accurate proxy.\n6 Discussion\nThis work focused on inference energy predic-\ntions of Transformers on a target hardware device.\nThe model tree abstraction is general and not tied\nto Transformer architectures nor to speciﬁc deep\nlearning frameworks, it is extensible to other neu-\nral networks like LSTM and frameworks like Ten-\nsorFlow. The abstraction is built from the com-\nputational graph and knowledge about the model\narchitecture and underlying software. As long as\nthese are available we can apply our methodology\nto other architectures as well.\nPredicting the training energy is an important\nand a more challenging problem. We believe our\nmethodology can be extended. However, it will re-\nquire tracking the energy of both forward and back-\nward processes and even modeling other aspects\ntraining dynamics, for example, time to converge\nto speciﬁc accuracy.\nScaling to unseen hardware is an important and\nchallenging area that needs further research. It\nrequires both measuring the ground truth energy\nfor a more diverse collection of hardware and de-\nsigning proper hardware-speciﬁc features (i.e., L1\ncache size, CPU cores, etc.). We believe IrEne’s\nmethodology can be extended to calibrate software\nreported energy as a way to scale how we collect\nground truths (as weak-supervision). In the future,\nwe plan to study workloads on more hardware to\nchoose proper features that capture the hardware\nenergy differences.\n7 Conclusions\nEnergy consumption of NLP models is an impor-\ntant consideration from a cost perspective and in-\ncreasingly, from an environmental impact perspec-\ntive as well. Designing energy efﬁcient and cost-\neffective models requires both accurate and inter-\npretable energy modeling. In this work, we showed\nthat by carefully combining resource utilization\nwith model description based features, we can de-\nvelop a multi-level energy prediction model that\nis not only highly accurate but is also able to pro-\nvide a break-down of how its different components\ncontribute to its overall energy.\n8 Acknowledgement\nThis material is based upon work supported by\nthe National Science Foundation under Grant No\n2007362.\n2154\nReferences\nMartín Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Geoffrey Irving, Michael Isard,\net al. 2016. Tensorﬂow: A system for large-scale\nmachine learning. In 12th {USENIX}symposium\non operating systems design and implementation\n({OSDI}16), pages 265–283.\nLasse F. Wolff Anthony, Benjamin Kanding, and\nRaghavendra Selvan. 2020. Carbontracker: Track-\ning and Predicting the Carbon Footprint of Training\nDeep Learning Models. ICML Workshop on \"Chal-\nlenges in Deploying and monitoring Machine Learn-\ning Systems\".\nAmirali Boroumand, Saugata Ghose, Youngsok\nKim, Rachata Ausavarungnirun, Eric Shiu, Rahul\nThakur, Daehyun Kim, Aki Kuusela, Allan Knies,\nParthasarathy Ranganathan, and Onur Mutlu. 2018.\nGoogle Workloads for Consumer Devices: Mitigat-\ning Data Movement Bottlenecks. In Proceedings of\nthe Twenty-Third International Conference on Archi-\ntectural Support for Programming Languages and\nOperating Systems , ASPLOS ’18, pages 316–331,\nNew York, NY , USA. Association for Computing\nMachinery.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D. Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language Models are Few-Shot Learners. Ad-\nvances in Neural Information Processing Systems ,\n33.\nQingqing Cao, Aruna Balasubramanian, and Niranjan\nBalasubramanian. 2020. Towards accurate and reli-\nable energy measurement of NLP models. In Pro-\nceedings of SustaiNLP: Workshop on Simple and\nEfﬁcient Natural Language Processing , pages 141–\n148, Online. Association for Computational Linguis-\ntics.\nYi Cao, Javad Nejati, Muhammad Wajahat, Aruna Bal-\nasubramanian, and Anshul Gandhi. 2017. Decon-\nstructing the Energy Consumption of the Mobile\nPage Load. Proceedings of the ACM on Measure-\nment and Analysis of Computing Systems , 1(1):6:1–\n6:25.\nYu-Hsin Chen, Joel Emer, and Vivienne Sze. 2016.\nEyeriss: A spatial architecture for energy-efﬁcient\ndataﬂow for convolutional neural networks. In ACM\nSIGARCH computer architecture news , volume 44,\npages 367–379, New York, NY , USA. IEEE.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch Transformers: Scaling to Trillion Parameter\nModels with Simple and Efﬁcient Sparsity. arxiv.\nPeter Henderson, Jieru Hu, Joshua Romoff, Emma\nBrunskill, Dan Jurafsky, and Joelle Pineau. 2020.\nTowards the Systematic Reporting of the En-\nergy and Carbon Footprints of Machine Learning.\narXiv:2002.05651 [cs].\nGlyn Hudson. 2021. emonPi - OpenEnergyMonitor.\nKenshoo. 2019. How Many Google Searches Per Day?\nSEM Pros Should Know This!\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs].\nNvidia. 2021. NVML API Reference Guide.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. PyTorch:\nAn Imperative Style, High-Performance Deep Learn-\ning Library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d\\textquotesingle Alché-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 32, pages 8026–\n8037. Curran Associates, Inc.\nAbhinav Pathak, Y . Charlie Hu, and Ming Zhang.\n2012. Where is the energy spent inside my app?\nﬁne grained energy accounting on smartphones with\nEprof. In Proceedings of the 7th ACM european con-\nference on Computer Systems , EuroSys ’12, pages\n29–42, New York, NY , USA. Association for Com-\nputing Machinery.\nAbhinav Pathak, Y . Charlie Hu, Ming Zhang, Paramvir\nBahl, and Yi-Min Wang. 2011. Fine-grained power\nmodeling for smartphones using system call tracing.\nIn Proceedings of the sixth conference on Computer\nsystems, EuroSys ’11, pages 153–168, New York,\nNY , USA. Association for Computing Machinery.\n2155\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011a. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research ,\n12:2825–2830.\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron\nWeiss, Vincent Dubourg, et al. 2011b. Scikit-learn:\nMachine learning in python. the Journal of machine\nLearning research, 12:2825–2830.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. OpenAI Blog.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. DistilBERT, a distilled ver-\nsion of BERT: smaller, faster, cheaper and lighter.\narXiv:1910.01108 [cs].\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren\nEtzioni. 2019. Green AI. arXiv:1907.10597 [cs,\nstat].\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and Policy Considerations for\nDeep Learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-Read Students Learn Better:\nOn the Importance of Pre-training Compact Models.\narXiv:1908.08962 [cs]. ArXiv: 1908.08962.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nHuggingFace’s Transformers: State-of-the-art Natu-\nral Language Processing. arXiv:1910.03771 [cs].\nChanmin Yoon, Dongwon Kim, Wonwoo Jung,\nChulkoo Kang, and Hojung Cha. 2012. AppScope:\napplication energy metering framework for android\nsmartphones using kernel activity monitoring. In\nProceedings of the 2012 USENIX conference on\nAnnual Technical Conference , USENIX ATC’12,\npage 36, USA. USENIX Association.\nXiyou Zhou, Zhiyu Chen, Xiaoyong Jin, and\nWilliam Yang Wang. 2020. HULK: An Energy Efﬁ-\nciency Benchmark Platform for Responsible Natural\nLanguage Processing. arXiv:2002.05829 [cs].\n2156\nA IrEne Implementation Details\nIn this section, we provide the implementation de-\ntails of IrEne. IrEne is implemented for PyTorch\n(Paszke et al., 2019), but can be extended to Ten-\nsorFlow (Abadi et al., 2016) in future.\nA.1 Constructing the model tree\nThe ﬁrst step to extracting the model tree is to run\nthe model on the target hardware. We run the ver-\nsion of the model on HuggingFace Transformers\nlibrary v4.2.2 (Wolf et al., 2020) for random data\nof different input sizes. Once run, we have both the\nexecution graph and the JIT trace that provides run-\ntime information. We use existing PyTorch APIs\nto obtain module level nodes, ML primitives, and\nthe relationships between them, from the execu-\ntion graph. In some cases, the NLP model may\nuse customized ML primitives. To extract infor-\nmation about these custom primitives, we combine\ninformation from the JIT trace and the execution\ngraph. Once we obtain all the component, we can\nconstruct the model tree.\nThe following ML primitives are used in\nTransformers: Linear, LayerNorm, Embedding,\nBatchNorm1d, Conv1d, MaxPool1d, AvgPool1d,\nLSTM, Tanh, Conv1D, LogSigmoid, ReLU, Sig-\nmoid, GELU, and LeakyReLU. Two custom\nprimitives: matrix multiplications (including\ntorch.matmul, torch.bmm and torch.einsum), soft-\nmax (torch.softmax).\nMachine PC1\nBERT-base 32.54\nDistilBERT 62.80\nRoBERTa-base 13.36\nGPT2 24.96\nDistilGPT2 35.93\nOpenaiGPT 42.37\nAverage 35.33\nTable 11: Energy Prediction Errors at Module levels\nusing Strubell et al., 2019 methodology on PC1.\n.\nA.2 Model features\nThe model features reﬂect hardware-independent\ncompute and memory information for a given\nmodel. We use the model execution to extract\nmodel features used by IrEne for energy predic-\ntion. We add forward hooks to each node in the\nmodel to track the shape and input data of each\nmodule and ML primitive. PyTorch hooks only\nsupport tuple arguments, but we extend these to\nalso support keyword based arguments. The JIT\ntrace contains information about the number of\nFLOPs and memory bytes for each module and ML\nprimitive. By combining JIT information and the\ninformation obtained from our hooks, we get the\nmodel features.\nA.3 Resource features\nResource features capture how the models use hard-\nware resources and cause energy activities. Exist-\ning work (Henderson et al., 2020) uses the OS\nresource proﬁler to log the resource utilization of\nCPU, memory and GPU events. However, this in-\ncurs high proﬁling overhead, and proﬁling is only\ndone at a low rate of once every second. Instead, to\nmonitor resources, we obtain the CPU utilization by\ndirectly reading /proc/stat and memory usage\nby reading /proc/meminfo via a C program.\nWe simultaneously log the GPU utilization, GPU\nmemory usage, GPU Streaming processor (SM)\nclock frequency and GPU memory frequency using\nthe Nvidia NVML API (Nvidia, 2021). To main-\ntain low monitoring overhead, we log resources\nevery 170 ms, resulting in less than 0.5% increase\nin CPU utilization and <15 MB memory footprint.\nNote that both model and resource features are\nextensible, meaning that one can add more either\nhardware-speciﬁc features or new model features\nfor newer deep learning frameworks or emerging\nhardware like customized deep learning accelera-\ntors.\nA.4 Regressor Training Procedures\nWe’ve implemented IrEne using SciKit Learn (Pe-\ndregosa et al., 2011a) and PyTorch (Paszke et al.,\n2019). We learn linear regressors for ML-level in\nSciKit Learn (Pedregosa et al., 2011b), and module\nand model level regressor in PyTorch, which allows\neasily optimizing on dynamic tree-structured com-\nputation graphs. We use Adam optimizer (Kingma\nand Ba, 2014) with 0.001 learning rate. In our\nexperiments τ in equation 3 is ﬁxed value of 10.\nWe normalize all the features to have 0 mean and\n1 standard deviation, learning mean and standard\ndeviation from the training set and applying it on\nthe test set.\n2157\nBertModel (54.8 J)\nBertEmbeddings (0.2%) BertEncoder (99.6%) BertPooler (0.2%)\nEmbedding:word (0.1%) LayerNorm (0.1%) BertLayer (8.3%)\nBertAttention (3.3%) BertIntermediate (2.3%) BertOutput (2.2%)\nBertSelfAttention (2.3%) BertSelfOutput (0.7%)\nLinear:query (0.5%) matmul (0.2%) softmax (0.1%) Linear:dense 0.5(%) LayerNorm (0.1%)\nLinear:dense (2.1%) Linear:dense (0.5%) LayerNorm (0.1%)\nLinear:dense: (0.1%) Tanh (0.1%)\nFigure 5: Abridged view of a BERT-base-uncased model annotated with predicted energy from our prediction\nmethod. The root contains the absolute energy of the model while every other node is annotated with its respective\nenergy percentage share. Darker colors represent nodes that consume a higher percentage of energy. There are 12\nBertLayer modules in the actual model. We show just one for brevity. The shown energy is an average of energy\nof the node across all (batch size, sequence length) models of BERT-base-uncased type.\nB Software Measurements Results\nWe use experiment-impact-tracker (Henderson\net al., 2020) to estimate software-based energy mea-\nsurements for the models at a module level as well\nas ML level. Table 11 shows the percentage er-\nror in software based measurements for module\nlevel operations. We calculate a model’s module\nlevel error as average percentage error over runs\nfor batch sizes 24 and 38, and sequence length\n32 and 128. Getting granular ML level software\nenergy corresponding to Strubell et al. (2019) re-\nquires modifying the existing framework which is\nnon-trivial. We leave this to future work.\nC Energy Breakdowns\nWe show module level predicted energy breakdown\nof four Transformer models in Table 12, and show\nan abridged view of BERT-base-uncased tree an-\nnotated with predicted energy and distribution in\nFigure 5.\nModule Energy %\nBertOutput 31.89\nBertSelfAttention 29.26\nBertIntermediate 27.97\nBertSelfOutput 09.74\nBertEmbeddings 00.34\nBertPooler 00.11\n(a) BERT-base\nModule Energy %\nMLP 61.41\nAttention 35.70\nLayerNorm 2.79\nEmbedding 0.11\n(b) OpenAI-GPT\nModule Name Energy %\nFFN 57.23\nMultiHeadSelfAttention 39.46\nLayerNorm 2.69\nEmbeddings 0.62\n(c) DistilBERT\nModule Name Energy %\nFFN 57.50\nMultiHeadSelfAttention 39.43\nLayerNorm 2.86\nEmbeddings 0.21\n(d) DistilGPT2\nTable 12: Module level predicted energy breakdown of\nfour Transformer models. We average the energy of\nthese modules across all available input sizes for each\nmodel architecture. Interestingly, we ﬁnd that even\nmodels with similar architecture have different types of\nenergy bottlenecks. For example, BERT-base has simi-\nlar architecture to DistilBERT but has different energy\nbottlenecks."
}