{
  "title": "Several Experiments on Investigating Pretraining and Knowledge-Enhanced Models for Natural Language Inference",
  "url": "https://openalex.org/W2942236412",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5032073184",
      "name": "Tianda Li",
      "affiliations": [
        "Queen's University"
      ]
    },
    {
      "id": "https://openalex.org/A5016892586",
      "name": "Xiaodan Zhu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5100666897",
      "name": "Quan Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100657544",
      "name": "Chen Qian",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100332258",
      "name": "Zhigang Chen",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5050660824",
      "name": "Si Wei",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2573425638",
    "https://openalex.org/W2308720496",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3016169217",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W2798665661",
    "https://openalex.org/W2601454101",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W2828845773",
    "https://openalex.org/W2963197830",
    "https://openalex.org/W2888481121",
    "https://openalex.org/W2782363479",
    "https://openalex.org/W2962843521",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2608787653",
    "https://openalex.org/W2805083708",
    "https://openalex.org/W2576562514",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2798416089"
  ],
  "abstract": "Natural language inference (NLI) is among the most challenging tasks in natural language understanding. Recent work on unsupervised pretraining that leverages unsupervised signals such as language-model and sentence prediction objectives has shown to be very effective on a wide range of NLP problems. It would still be desirable to further understand how it helps NLI; e.g., if it learns artifacts in data annotation or instead learn true inference knowledge. In addition, external knowledge that does not exist in the limited amount of NLI training data may be added to NLI models in two typical ways, e.g., from human-created resources or an unsupervised pretraining paradigm. We runs several experiments here to investigate whether they help NLI in the same way, and if not,how?",
  "full_text": "arXiv:1904.12104v1  [cs.CL]  27 Apr 2019\nSeveral Experiments on Investigating Pretraining and\nKnowledge-Enhanced Models for Natural Language Inference\nTianda Li\nECE, Queen’s University\ntianda.li@queensu.ca\nQuan Liu\niFL YTEK Research\nquanliu@ustc.edu.cn\nZhigang Chen\niFL YTEK Research\nzgchen@iflytek.com\nXiaodan Zhu\nECE, Queen’s University\nzhu2048@gmail.com\nQian Chen\nUniversity of Science and T echnology of China\ncq1231@mail.ustc.edu.cn\nSi Wei\niFL YTEK Research\nsiwei@iflytek.com\nAbstract\nNatural language inference (NLI) is among the\nmost challenging tasks in natural language un-\nderstanding. Recent work on unsupervised\npretraining that leverages unsupervised signals\nsuch as language-model and sentence predic-\ntion objectives has shown to be very effec-\ntive on a wide range of NLP problems. It\nwould still be desirable to further understand\nhow it helps NLI; e.g., if it learns artifacts in\ndata annotation or instead learn true inference\nknowledge. In addition, external knowledge\nthat does not exist in the limited amount of\nNLI training data may be added to NLI mod-\nels in two typical ways, e.g., from human-\ncreated resources or an unsupervised pretrain-\ning paradigm. W e runs several experiments\nhere to investigate whether they help NLI in\nthe same way, and if not, how?\n1 Introduction\nModelling informal reasoning in natural language\nis a very challenging problem in natural language\nunderstanding. The recent availability of rela-\ntively large annotated datasets (\nBowman et al. ,\n2015; Williams et al. , 2017) have made it feasi-\nble to train complex natural language inference\n(NLI) models that need to estimate a large number\nof parameters, including neural network models.\nSuch models have shown to achieve the state-of-\nthe-art performance (\nBowman et al. , 2015, 2016;\nY u and Munkhdalai , 2017; Parikh et al. , 2016;\nSha et al. , 2016; Chen et al. , 2017; T ay et al. ,\n2018). However, many recent research ef-\nforts ( Glockner et al. , 2018; W ang et al. , 2018;\nNaik et al. , 2018; Poliak et al. , 2018) have also ob-\nserved that complex models may achieve better\nevaluation scores by over-exploiting the artifacts\nin data construction rather than modelling real NLI\nsemantics.\nAs the recent advance in learning representa-\ntion for natural language, unsupervised pretrain-\ning that leverages large unannotated data using\nlanguage-model or sentence prediction objectives\nhave shown to be effective on a wide range of NLP\ntasks. It would, however, be desirable to under-\nstand if such models leverage artifacts in data con-\nstruction or actually help learn inference related\nsemantics.\nFrom a more general viewpoint, pretrained\nmodels, in parallel to those explicitly exploring\nhuman authorized semantics (\nChen et al. , 2018b),\nincorporate external knowledge into NLI by learn-\ning from unlabelled data.\nW e run several experiments showing that the re-\ncent pretraining schema do help learn NLI-related\nsemantics to achieve better NLI prediction. The\nexperiments also reveal that external knowledge\nobtained from pretraining and human authorized\nresources complement each other, suggesting po-\ntential beneﬁts of incorporating the latter to the\npretraining-ﬁnetuning schema.\n2 Related W ork\nUnsupervised Pretraining Unsupervised pre-\ntraining has recently shown to be very effective\nin improving the performances of a wide range\nof NLP tasks. Feature-based and ﬁnetune-based\nmodels are two main strategies for using them in\ndownstream tasks. Feature-based models such as\nELMo (\nPeters et al. , 2018) incorporate pretrained\nrepresentation into task-speciﬁc models. As the\napproach keeps the original task-speciﬁc models\nintact, it can be seen as a powerful feature extrac-\ntor for speciﬁc tasks.\nFinetune-based approaches such as Genera-\ntive Pretrained Transformer (GPT) (\nRadford et al. ,\n2018) and BERT ( Devlin et al. , 2018), however,\npretrain a model on unannotated data and then\nﬁnetune the same architecture and use it on dif-\nferent downstream tasks.\nPretraining has been used in NLI (\nPeters et al. ,\n2018; Devlin et al. , 2018) and shown to improve\nperformance on many tasks. As complex models\noften overﬁt to class-conditional idiosyncrasies in\nthe existing NLI datasets, in this paper we run sev-\neral experiments to understand if the pretraining\nschema helps learn true NLI related semantics to\nhelp achieve better NLI prediction.\nEvaluation in Natural Language Inference\nPrevious research has paid attention to judge\nwhether existing NLI systems have learned NLI-\nrelated semantics or explored the regularities ex-\nisting in the data that are not relevant to NLI. For\nexample (\nGururangan et al. , 2018) found that in\nthe construction phase of SNLI datasets, due to the\nstrategies that the human subjects create hypothe-\nses, the distributions of words among NLI cate-\ngories are different, which may not be relevant to\nNLI semantics.\nNaik et al. (2018) pointed out that\ncomplex machine learning models have the capac-\nity to exploit such artifacts. This characteristic al-\nlows the models to achieve better performance on\nmany benchmark datasets.\nT o investigate this, different methods have been\nproposed. For example,\nW ang et al. (2018) use\na swapping evaluation method, by switching a\npremise and its hypothesis to change the distribu-\ntion of words to test the robustness of a model.\nAlso, efforts have also been made to propose\nnew test dataset (\nGlockner et al. , 2018). In the\ntest set, premises are taken from the SNLI train-\ning set and for each premise, hypotheses of dif-\nferent inference categories (i.e., entailment, neu-\nral, and contradiction) are generated by replac-\ning a single word in premise sentence. In ad-\ndition, the stress test proposed by\nNaik et al.\n(2018) construct stress test dataset based on ex-\nisting MultiNLI corpus ( Williams et al. , 2017).\n3 Experiment Set-Up\nModels Our experiments use the the following\nmodels:Note that for all the previously published\nmodels, we either used the original code provided\nby its authors, or if not available, our implemen-\ntation achieved a performance comparable to that\nreported in the original papers.\n• BERT (\nDevlin et al. , 2018)\n• DenseNet+DynAtt: DenseNet plus Dynamic\nSelf Attention. This is a state-of-the-art\nsentence-embedding-based model proposed\nby (\nY oon et al. , 2018).\n• DenseNet+MultiHeads: This is a model pro-\nposed in this paper to further investigate\nDenseNet+DynAtt, by replacing its top dy-\nnamic self-attention layer with multi-head at-\ntention used by (\nChen et al. , 2018a), to help\nobserve the role of dynamic attention in NLI.\n• LSTM+DynAtt: This is a model pro-\nposed in this paper to further investi-\ngate DenseNet+DynAtt, by replacing its\nlower DenseNet layer with LSTM as in\n(\nChen et al. , 2018a), to observe the role of\nDenseNet in NLI.\n• ESIM ( Chen et al. , 2017). 1\n• ESIM+ELMo ( Peters et al. , 2018)\n• GenPool: Generalized Pooling ( Chen et al. ,\n2018a)\n• GenPool+ELMo: This is a model proposed\nin this paper to add ELMo to GenPool to ob-\nserve the effect of ELMo on GenPool.\n• GPT (\nRadford et al. , 2018)\n• KIM ( Chen et al. , 2018b)\nData W e use both existing data and that we\nfurther annotated in our experiments. The ex-\nisting data include SNLI (\nBowman et al. , 2015),\nMultiNLI ( Williams et al. , 2017), the Glockner\ndataset ( Glockner et al. , 2018), and the stress test\ndataset ( Naik et al. , 2018). The ﬁrst two are used\nfor training models and the last two for testing.\nW e will discuss the subset that we further anno-\ntate later in the experiment result section.\n1 ESIM is a strong NLI baseline. W e used the source\ncode made available at https://github.com/lukecq1231/nli.\nThe code can run efﬁciently and has been adapted for\nsummarization (\nChen et al. , 2016) and question-answering\ntasks ( Zhang et al. , 2017).\nGlockner Entailment (982) Glockner Neutral (47) Glockner Contradiction (7164)\nModels P R F1 P R F1 P R F1\nBERT .800 .980 .880 .050 .260 .080 .990 .930 .960\nDenseNet+DynAtt .394 .908 .550 .002 .021 .003 .979 .732 .834\nDenseNet+MultiHeads .274 .914 .421 .000 .000 .000 .977 .655 .784\nLSTM+DynAtt .214 .990 .352 .008 .043 .014 .992 .472 .640\nESIM .226 .994 .368 .006 .064 .011 .992 .466 .634\nESIM+ELMo .238 .982 .383 .006 .085 .012 .992 .488 .654\nGenPool .163 .993 .280 .005 .021 .007 .994 .275 .431\nGenPool+ELMo .162 .996 .279 .010 .043 .016 .996 .272 .428\nGPT .735 .849 .788 .013 .511 .027 .994 .726 .839\nKIM .450 .973 .615 .017 .128 .029 .991 .790 .879\nT able 1: Model performances on the Glockner dataset.\n4 Experiment Results and Discussion\n4.1 Pretraining Helps Learn Inference\nKnowledge\nAs discussed above, the improvement in NLI mod-\nelling can be due to the models’ sophisticated\ncapability in capturing annotation artifacts intro-\nduced in data construction. While the recent pre-\ntrained models achieved impressive performance\non a wide range of NLP tasks, including NLI, we\nshow that they do learn NLI related semantics.\nThe Glockner and Swapping T est\n(\nGlockner et al. , 2018) take the premises from\nSNLI and uses lexical replacement to generate\nhypotheses for different NLI categories (i.e.,\nentailment, contradiction, and neutral) by re-\nplacing a single word in the premise and asking\nhuman subjects to conﬁrm the replacement and\nthe resulting inference relationships are correct.\nW e ﬁrst perform tests on the Glockner dataset\nand then annotate a subset to provide some more\ninsights. T able\n1 shows the results on the orig-\ninal Glockner test data, with all models trained\non SNLI. Note that the Glockner dataset has 982,\n47, and 7164 test cases for entailment, neural, and\ncontradiction, respectively , as marked in the table.\nIn general we can see that GPT , BERT , and\nKIM outperform the other models. As the Glock-\nner test set is constructed to speciﬁcally focus on\nlexical semantics in NLI, the results suggest that\nGPT , BERT , and KIM capture lexical level in-\nference knowledge. As a comparison, we can\nsee that ESIM, which relies on the SNLI train-\ning data only , shows an inferior performance on\nthis dataset. Note that KIM incorporates W ordNet\nknowledge, while GPT and BERT learn external\nknowledge automatically from unannotated data.\nDenseNet+DynAtt is a state-of-the-art model\nand its performance is close to that of GPT\nin the contradiction category . W e investigate\nthe roles of the DenseNet and DynAtt in NLI\nby replacing these main components to create\nDenseNet+Multiheads and LSTM+DynAtt mod-\nels, as explained in Section\n3. The results show\nthat the DenseNet and DynAtt jointly work very\nwell to contribute to the ﬁnal performance, as the\nperformance drops signiﬁcantly when either of the\ncomponents is replaced with those in other state-\nof-the-art models.\nW ang et al. (2018) proposed an interesting idea\nto evaluate whether a model learns NLI-related se-\nmantics or explore statistics irrelevant to NLI by\nusing a swapping strategy . The method switches\na premise with its hypothesis to test a model: if a\nmodel learns entailment, it will have a large per-\nformance drop when tested on the swapped pairs.\nW e note that caution should be exercised as the\nswapping evaluation may not be conclusive if en-\ntailment sentence pairs (e.g., those in the SNLI and\nthe Glockner data set) contain also paraphrases\n(premise entails hypothesis and vice versa).\nFor this reason, we further manually annotated\nthe 982 entailment pairs in the Glockner dataset,\nwhich contains only 32 non-paraphrase entailment\nsentence pairs. W e perform the state-of-the-art\nmodels on these pairs and their swapping versions.\nAgain, as discussed in (\nW ang et al. , 2018), a larger\ndecrease of accuracy before and after the swap-\nping corresponds to a better entailment model.\nIn T able\n2, we see the differences of accuracies\nfor the models. The performances of KIM, GPT ,\nand BERT all have the largest differences before\nand after the swapping, indicating their better per-\nformance on this entailment test. Again, in KIM,\ndifferent semantic relations between word pairs\nStrict entailment Paraphrasing\nBERT .969 -.007\nDenseNet+DynAtt .593 -.019\nDenseNet+MultiHeads .625 -.023\nLSTM+DynAtt .437 -.001\nESIM .25 -.154\nESIM+ELMo .219 -.001\nGenPool .000 .001\nGenPool+ELMo .125 .027\nGPT .844 -.101\nKIM .781 -.006\nT able 2: Accuracy differences of each model before\nand after swapping, on the strict entailment subset and\nparaphrasing subset of the Glockner dataset, respec-\ntively.\nare incorporated from W ordNet, while the exper-\niment suggests that the pretrain-ﬁnetune schema\ncan learn such knowledge (e.g., here hypernym re-\nplacement) for NLI. In the table we also include\nthe differences between models on the paraphras-\ning subset. As discussed, the results are not con-\nclusive. W e list it here as one needs to be careful\nwhen applying the swapping test on the existing\nNLI data set.\n4.2 Investigating External Knowledge from\nDifferent Sources\nAlthough the current NLI training\ndatasets (\nBowman et al. , 2015) are much larger\nthan what were available previously , the amount\nof NLI knowledge that can be learned is still\nlimited. As suggested in (\nGlockner et al. , 2018),\nexternal knowledge from W ordNet can signiﬁ-\ncantly improve the performance of NLI prediction\non the Glockner dataset. Also as discussed\nabove, a NLI model may beneﬁt from external\nknowledge in two typical ways, e.g., from human\nauthorized sources (\nChen et al. , 2018b) or from\nlarge unannotated data, e.g., via unsupervised\npretraining (\nDevlin et al. , 2018; Radford et al. ,\n2018). In order to ﬁnd out how NLI systems\nbeneﬁt from different external knowledge sources\nand whether they complement each other, we\ncarry out further experiments. Speciﬁcally , we\nperform stress test (\nNaik et al. , 2018) on KIM,\nGPT , BERT , and ESIM.\nThe stress test proposed in ( Naik et al. , 2018)\nwas constructed in three categories: competence,\ndistraction, and noise test. W e use the compe-\nBERT GPT KIM ESIM\nBERT .561 .580 .652 .616\nGPT .304 .543 .457\nKIM .491 .552\nESIM .320\nT able 3: Oracle accuracy of merging two models in\nstress test on the in-domain set.\nBERT GPT KIM ESIM\nBERT .482 .504 .580 .539\nGPT .251 .482 .378\nKIM .426 .488\nESIM .265\nT able 4: Oracle accuracy of merging two systems in\nstress test on the out-of-domain test set.\ntence test data to evaluate the models’ ability to\nunderstand antonym relations in NLI, as the re-\nlationship between the other two categories and\nNLI prediction is less straightforward. W e per-\nform KIM, GPT , BERT , and ESIM on the com-\npetence test and derive T able\n3, in which the cross\nof a row (e.g., BERT) and a column (e.g., KIM)\nis the oracle accuracy of merging the two models\n(i.e., 0.652). That is, for each test case, if any of\nthe two models makes the correct predication, we\nregard the answer to be right. In this way , we show\nevery single model’s performance (along the diag-\nonal) and the oracle performance of merging two\nmodels.\nW e can see in T able\n3 that the model using hu-\nman edited knowledge (KIM) and those learned\nwith unannotated data (e.g., BERT and GPT) com-\nplement each other. Along the diagonal, we can\nsee BERT outperforms KIM and achieves the best\nperformance.\nIt is also interesting to see that BERT and KIM\nare better than GPT on this task. This could be\ndue to the use of sentence prediction objective in\nBERT , which may help capture word pair informa-\ntion between two sentences, while KIM explicitly\nincorporates different types of word-pair relations\nin W ordNet.\nW e have also performed the above experiment\n(T able\n4) on the out-of-domain dataset. W e ob-\nserved similar results, except that on the out-of-\ndomain test data, the performance of models are\nlower than that on the in-domain test data shown\nin T able\n3.\nExample Sentences\n1 P: There are two people inside, and two men outside, a cafe; with a tv on in the background.\nH: There are two people outside, and two men outside, a cafe; with a tv on in the background.\nT able 5: Examples on which KIM is right but BER T is wrong.\nCategories Example Sentences\nI 2 P: Y ellow banners with a black lion print are hung across so me trees in a sun-lit neighborhood.\nH: Y ellow banners with a black lion print are hung across some trees in a moon-lit neighborhood.\n3 P: A young boy takes the ﬁrst step onto Mars.\nH: A young boy takes the ﬁrst step onto Earth.\nII 4 P: A Vietnamese woman gives a manicure a South Korean woman gives a manicure.\nH: A Vietnamese woman gives a manicure a North Korean woman gives a manicure.\n5 P: An Indian man is perching on top of a wall with a hammer and chisel.\nH: An Indonesian man is perching on top of a wall with a hammer and chisel.\nT able 6: Examples on which BER T is right but KIM is wrong.\nExample Sentences Label\n6 P: A woman wearing a patterned dress in an outdoor market sits surrounded by her offerings of\nonions, eggs, tomatoes, beans, and many other things.\nneutral\nH: A woman wearing a patterned dress in an outdoor market sits surrounded by her offerings of\nonions, eggs, carrots, beans, and many other things.\n7 P: A woman wearing a patterned dress in an outdoor market sits surrounded by her offerings of\nonions, eggs, tomatoes, beans, and many other things.\ncontradiction\nH: A woman wearing a patterned dress in an outdoor market sits surrounded by her offerings of\nonions, eggs, pumpkins, beans, and many other things.\nT able 7: Some sentence pairs with inconsistent true labels.\n4.3 Case Study and Analysis\nW e further performed detailed analyses on\npremise-hypothesis pairs where KIM is correct but\nthe pretrained model (BERT) is wrong, and vice\nversa, on both the Glockner and stress test set.\nOnly KIM Correct W e found that for most\ncases in which the word-pair information required\nto make the judgment (e.g., antonym and syn-\nonym) does present in W ordNet, KIM is more ac-\ncurate than BERT , with an example shown in T a-\nble\n5.\nOnly BERT Correct W e found when the pre-\ntrained model is correct but KIM wrong, the exam-\nples could be categorized into one of the following\ntwo situations.\nCategory I : W ord-pair information is missing in\nW ordNet but learned by pretraining in BERT .As\nshown the example in T able\n6, the word pair ⟨\nsun-lit, moon-lit⟩ cannot be found in W ordNet,\nbut their relationship can be learned by BERT via\nleveraging the large corpora.\nCategory II: Some word-pair relations can be\nfound in W ordNet but KIM does not make a cor-\nrect decision. In example 4 of T able\n6, the word\npair ⟨north, south⟩as well as their relationship (co-\nhypernym) can be found in W ordNet, but KIM did\nnot categorize this example correctly . W e found\nthe attention did not give enough focus on these\nword pairs.\nNote that in a number of cases in the Glock-\nner dataset, the true labels are not consistent. As\nshown in T able\n7, one of the two sentence pairs is\nlabelled as neutral and the other as contradiction.\nW e excluded such cases in our analysis.\n5 Conclusions\nW e run several experiments showing that the re-\ncent unsupervised-pretraining schema does help\nlearn NLI related semantics to achieve better NLI\nprediction, including synonyms and hypernyms\nthat are useful in judging entailment. The ex-\nperiments also reveal that external knowledge ob-\ntained from pretraining and human authorized re-\nsources complement each other, suggesting the\npotential beneﬁt of combining these two ap-\nproaches.\nReferences\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large an-\nnotated corpus for learning natural language infer-\nence. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP 2015, Lisbon, P ortugal, September 17-\n21, 2015, pages 632–642.\nSamuel R. Bowman, Jon Gauthier, Abhinav Ras-\ntogi, Raghav Gupta, Christopher D. Manning, and\nChristopher Potts. 2016. A fast uniﬁed model for\nparsing and sentence understanding. In Proceedings\nof the 54th Annual Meeting of the Association for\nComputational Linguistics, ACL 2016, August 7-12,\n2016, Berlin, Germany, V olume 1: Long P apers.\nQian Chen, Zhen-Hua Ling, and Xiaodan Zhu. 2018a.\nEnhancing sentence embedding with generalized\npooling. CoRR, abs/1806.09828.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana\nInkpen, and Si W ei. 2018b. Neural natural language\ninference models enhanced with external knowl-\nedge. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (V ol-\nume 1: Long P apers), volume 1, pages 2406–2417.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, and Hui\nJiang. 2016. Distraction-based neural networks for\nmodeling document. In Proceedings of IJCAI, pages\n2754–2760.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si W ei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced LSTM for\nnatural language inference. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2017, V ancouver , Canada,\nJuly 30 - August 4, V olume 1: Long P apers , pages\n1657–1668.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nMax Glockner, V ered Shwartz, and Y oav Goldberg.\n2018. Breaking NLI systems with sentences\nthat require simple lexical inferences. CoRR,\nabs/1805.02266.\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel R. Bowman, and\nNoah A. Smith. 2018. Annotation artifacts in natu-\nral language inference data. CoRR, abs/1803.02324.\nAakanksha Naik, Abhilasha Ravichander, Norman M.\nSadeh, Carolyn Penstein Ros´ e, and Graham Neubig.\n2018. Stress test evaluation for natural language in-\nference. CoRR, abs/1806.00692.\nAnkur P . Parikh, Oscar T ¨ ackstr ¨ om, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. In Proceed-\nings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2016,\nAustin, T exas, USA, November 1-4, 2016 , pages\n2249–2255.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. CoRR, abs/1802.05365.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin V an Durme. 2018.\nHypothesis only baselines in natural language infer-\nence. CoRR, abs/1805.01042.\nAlec Radford, Karthik Narasimhan, Tim Sali-\nmans, and Ilya Sutskever. 2018. Improv-\ning language understanding by generative pre-\ntraining. URL https://s3-us-west-2. amazon-\naws. com/openai-assets/research-covers/language-\nunsupervised/language\nunderstanding paper . pdf.\nLei Sha, Baobao Chang, Zhifang Sui, and Sujian Li.\n2016. Reading and thinking: Re-read LSTM unit\nfor textual entailment recognition. In COLING\n2016, 26th International Conference on Computa-\ntional Linguistics, Proceedings of the Conference:\nT echnical P apers, December 11-16, 2016, Osaka,\nJapan, pages 2870–2879.\nY i T ay, Luu Anh Tuan, and Siu Cheung Hui. 2018. A\ncompare-propagate architecture with alignment fac-\ntorization for natural language inference. CoRR,\nabs/1801.00102.\nHaohan W ang, Da Sun, and Eric P . Xing. 2018. What if\nwe simply swap the two text fragments? A straight-\nforward yet effective way to test the robustness of\nmethods to confounding signals in nature language\ninference tasks. CoRR, abs/1809.02719.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nDeunsol Y oon, Dongbok Lee, and SangKeun Lee.\n2018. Dynamic self-attention : Computing atten-\ntion over words dynamically for sentence embed-\nding. CoRR, abs/1808.07383.\nHong Y u and Tsendsuren Munkhdalai. 2017. Neural\ntree indexers for text understanding. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics,\nEACL 2017, V alencia, Spain, April 3-7, 2017, V ol-\nume 1: Long P apers, pages 11–21.\nJunbei Zhang, Xiaodan Zhu, Qian Chen, Lirong\nDai, Si W ei, and Hui Jiang. 2017. Ex-\nploring question understanding and adaptation in\nneural-network-based question answering. CoRR,\nabs/arXiv:1703.04617v2.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7883989810943604
    },
    {
      "name": "Inference",
      "score": 0.6907968521118164
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6518720388412476
    },
    {
      "name": "Natural language processing",
      "score": 0.6325220465660095
    },
    {
      "name": "Sentence",
      "score": 0.6190831661224365
    },
    {
      "name": "Annotation",
      "score": 0.598067581653595
    },
    {
      "name": "Natural language",
      "score": 0.5313905477523804
    },
    {
      "name": "Language understanding",
      "score": 0.43259936571121216
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.4245338439941406
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204722609",
      "name": "Queen's University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    }
  ]
}