{
  "title": "How Large Language Models are Transforming Machine-Paraphrased Plagiarism",
  "url": "https://openalex.org/W4318766201",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3137632550",
      "name": "Jan Philip Wahle",
      "affiliations": [
        "Klinikum St. Georg",
        "University of Göttingen"
      ]
    },
    {
      "id": "https://openalex.org/A1980529301",
      "name": "Terry Ruas",
      "affiliations": [
        "Klinikum St. Georg",
        "University of Göttingen"
      ]
    },
    {
      "id": "https://openalex.org/A3194667443",
      "name": "Frederic Kirstein",
      "affiliations": [
        "University of Göttingen",
        "Klinikum St. Georg",
        "Mercedes-Benz (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A72611330",
      "name": "Bela Gipp",
      "affiliations": [
        "Klinikum St. Georg",
        "University of Göttingen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2251882135",
    "https://openalex.org/W3100501376",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W2086668169",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2133012565",
    "https://openalex.org/W2970559004",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2798935874",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2531908596",
    "https://openalex.org/W3211481821",
    "https://openalex.org/W2001771035",
    "https://openalex.org/W2119298903",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3125358881",
    "https://openalex.org/W4206732210",
    "https://openalex.org/W4226053440",
    "https://openalex.org/W4234918518",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2996573939",
    "https://openalex.org/W2016172157",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W3034287667",
    "https://openalex.org/W2147528976",
    "https://openalex.org/W2963551569",
    "https://openalex.org/W2963991775",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W3044021949",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4319811928",
    "https://openalex.org/W4213122582",
    "https://openalex.org/W3102273025",
    "https://openalex.org/W4293350112"
  ],
  "abstract": "The recent success of large language models for text generation poses a severe threat to academic integrity, as plagiarists can generate realistic paraphrases indistinguishable from original work.However, the role of large autoregressive transformers in generating machineparaphrased plagiarism and their detection is still developing in the literature.This work explores T5 and GPT-3 for machine-paraphrase generation on scientific articles from arXiv, student theses, and Wikipedia.We evaluate the detection performance of six automated solutions and one commercial plagiarism detection software and perform a human study with 105 participants regarding their detection performance and the quality of generated examples.Our results suggest that large models can rewrite text humans have difficulty identifying as machine-paraphrased (53% mean acc.).Human experts rate the quality of paraphrases generated by GPT-3 as high as original texts (clarity 4.0/5, fluency 4.2/5, coherence 3.8/5).The best-performing detection model (GPT-3) achieves a 66% F1-score in detecting paraphrases.We make our code, data, and findings publicly available for research purposes.",
  "full_text": null,
  "topic": "Paraphrase",
  "concepts": [
    {
      "name": "Paraphrase",
      "score": 0.8776694536209106
    },
    {
      "name": "Fluency",
      "score": 0.8185765743255615
    },
    {
      "name": "Computer science",
      "score": 0.8007150888442993
    },
    {
      "name": "Plagiarism detection",
      "score": 0.7964285612106323
    },
    {
      "name": "CLARITY",
      "score": 0.6430548429489136
    },
    {
      "name": "Natural language processing",
      "score": 0.5980781316757202
    },
    {
      "name": "Artificial intelligence",
      "score": 0.544206976890564
    },
    {
      "name": "Transformer",
      "score": 0.5366067290306091
    },
    {
      "name": "Language model",
      "score": 0.44174355268478394
    },
    {
      "name": "Software",
      "score": 0.43108290433883667
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4217360019683838
    },
    {
      "name": "Automatic summarization",
      "score": 0.42170625925064087
    },
    {
      "name": "Machine learning",
      "score": 0.38862234354019165
    },
    {
      "name": "Programming language",
      "score": 0.23275485634803772
    },
    {
      "name": "Mathematics education",
      "score": 0.15974026918411255
    },
    {
      "name": "Psychology",
      "score": 0.07572910189628601
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}