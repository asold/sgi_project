{
  "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm",
  "url": "https://openalex.org/W3130319171",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4223932256",
      "name": "Reynolds, Laria",
      "affiliations": [
        "Michigan United",
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A4223932251",
      "name": "McDonell, Kyle",
      "affiliations": [
        "Neural Stem Cell Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2257408573",
    "https://openalex.org/W2963287297",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3085190015",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W3035101045",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2914190582",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W3126960149",
    "https://openalex.org/W2954579883",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W3015372447",
    "https://openalex.org/W3030163527"
  ],
  "abstract": "Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. In this work, we discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.",
  "full_text": "arXiv:2102.07350v1  [cs.CL]  15 Feb 2021\nPrompt Programming for Large Language Models:\nBeyond the Few-Shot Paradigm\nLaria Reynolds\nmoire@knc.ai\nKyle McDonell\nkyle@knc.ai\nAbstract\nPrevailing methods for mapping large generative language m odels to supervised tasks may fail to\nsuﬃciently probe models’ novel capabilities. Using GPT-3 a s a case study, we show that 0-shot prompts\ncan signiﬁcantly outperform few-shot prompts. We suggest t hat the function of few-shot examples in these\ncases is better described as locating an already learned tas k rather than meta-learning. This analysis\nmotivates rethinking the role of prompts in controlling and evaluating powerful language models. In this\nwork, we discuss methods of prompt programming, emphasizin g the usefulness of considering prompts\nthrough the lens of natural language. We explore techniques for exploiting the capacity of narratives\nand cultural anchors to encode nuanced intentions and techn iques for encouraging deconstruction of a\nproblem into components before producing a verdict. Inform ed by this more encompassing theory of\nprompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its\nown natural language prompts for a range of tasks. Finally, w e discuss how these more general methods of\ninteracting with language models can be incorporated into e xisting and future benchmarks and practical\napplications.\nKeywords: language models, transformers,\nGPT-3, few-shot learning, prompt programming,\nmetaprompts, serial reasoning, semiotics\n1 Motivation\nThe recent rise of massive self-supervised language\nmodels such as GPT-3 [3] and their success on down-\nstream tasks has brought us one step closer to the goal\nof task-agnostic artiﬁcial intelligence systems. How-\never, despite the apparent power of such models, cur-\nrent methods of controlling them to perform speciﬁc\ntasks are extremely limited. In order to properly eval-\nuate their capabilities and extract useful work from\nthese models, new methods are required.\nPrior to GPT-3, the standard approach to the\nevaluation and use of such models has involved ﬁne-\ntuning on a portion of a task dataset [12]. GPT-3\nachieved state-of-the-art performance on a wide va-\nriety of tasks without ﬁne tuning, using only few-\nshot prompts, in which a small number of examples\nof solved tasks are provided as part of the input to\nthe trained model. However, while the few-shot for-\nmat was suﬃcient to reveal surprising performance\non these tasks, we argue that prompting can be more\neﬀective than either ﬁne-tuning or the few-shot for-\nmat at extracting speciﬁc learned behaviors from self-\nsupervised language models.\nWe argue that contrary to the common interpre-\ntation of the few-shot format implied by the title of\nthe original GPT-3 paper [3], Language models are\nfew-shot learners , GPT-3 is often not actually learn-\ning the task during run time from few-shot examples.\nRather than instruction, the method’s primary func-\ntion is task location in the model’s existing space of\nlearned tasks. This is evidenced by the eﬀectiveness\nof alternative prompts which, with no examples or\ninstruction, can elicit comparable or superior perfor-\nmance to the few-shot format.\nThis motivates new approaches which explicitly\npursue the goal of task location. We propose explor-\ning more general methods of prompt programming\nand speciﬁcally techniques for communicating task in-\ntention and structure to an self-supervised model in\nthe modality it was trained: natural language.\nThe ground truth function that self-supervised\nlanguage models are trained to approximate is, in\ngreat generality, is how humans write. Accordingly,\nto interact with and control a language model, we\nshould consider doing so from the perspective of nat-\nural language as it is used by humans. With a few\ncaveats, we want to ﬁnd prompts which we would ex-\npect a human to complete in a way that accomplishes\n1\nthe desired task.\nIn this paper, we investigate the few-shot\nparadigm and ﬁnd that its performance can be\nmatched or exceeded by simple 0-shot prompts. We\nexplore the nature of successful 0-shot prompts and\npropose general methods of prompt programming\nthrough the lens of natural language semiotics. We\ndemonstrate novel prompts which force a language\nmodel to break a problem into components before\nproducing a verdict, and we introduce the concept of\nmetaprompt programming, an approach which oﬄoads\nthe job of writing a task-speciﬁc prompt to the lan-\nguage model itself. Finally, we discuss how these ideas\ncan be incorporated into existing and future bench-\nmarks to allow us to better probe the capabilities of\nlarge language models.\n2 Related work\nRecent work in the literature has focused on control-\nling natural language generation using traditional ap-\nproaches from machine learning like novel architec-\ntures which condition outputs [15, 16], more advanced\nsampling techniques [6, 11], gradient-based optimiza-\ntion of prompts [22, 17], and task-speciﬁc adapter\nnetworks [25]. See [24] for a survey of these recent\nmethods. Past work has also explored improving the\nfew-shot paradigm by dynamically selecting the most\nrelevant examples for each task [18, 9].\nIn comparison, little work on natural-language, 0-\nshot approaches to prompt programming has been\nformalized. Instead, successful prompt programming\ntechniques have primarily been shared on blogs and\nsocial media among users of OpenAI’s API and AI\nDungeon.\nDue to the decentralized form that most explo-\nrations of prompt programming have taken, it is not\nfeasible for us to to compile all relevant contributions\nhere. We instead give a brief, non-exhaustive survey\nof explorations which have gone beyond the few-shot\nparadigm.\nGwern has given the most comprehensive survey\nof GPT-3’s capabilities through demonstrations of it\nwriting ﬁction, poetry, navy seals copypasta paro-\ndies, and performing tasks like PDF cleaning. He\nhas written extensively about his intuitions of work-\ning with GPT-3 and his methods of prompt program-\nming [2]. Arram Sabeti has written about the eﬀect of\nthe context provided by a prompt on writing quality\n[21]. Zachary Robertson has written about amplify-\ning GPT-3’s mathematical capabilities through a dia-\nlogue that guides it to break a problem into steps [20].\nTwitter user KaryoKleptid has posted experiments\nalong a similar vein, using dialogues to prompt GPT-\n3 (via AI Dungeon) to break problems into steps and\nfollow procedures such as brute force checking [13,\n14], achieving impressive results on math problems.\nOur work synthesizes and expands on the meth-\nods pioneering by these explorations, representing a\nmodest step towards formalizing eﬀective natural lan-\nguage prompt programming techniques.\n3 Investigating\nfew-shot prompting\nGPT-3 was evaluated on tasks with 0, 1, and n-\nshot prompts (containing only a natural language de-\nscription, one solved example, and n solved exam-\nples respectively). GPT-3 consistently performs bet-\nter when more examples are provided, with 0-shot\nperformance often achieving less than half of the score\nof many-shot tests. A common interpretation of this\nresult is that GPT-3 is learning from the examples\nat runtime and this allows it to perform better than\nwith fewer or no examples [3].\nThe improvement in performance with the num-\nber of examples, however, can be interpreted in an\nalternate way. Rather than learning how to perform\nthe task from the examples, the examples may simply\nserve to instruct GPT-3 on what task it is to solve and\nencourage it to follow the structure of the prompt in\nits response.\nFor example, for certain tasks, such as transla-\ntion, a small number of samples is insuﬃcient to learn\nanything substantial about the task. Instead, GPT-\n3 must rely primarily, if not entirely, on the knowl-\nedge of vocabulary and grammar of both the source\nand target languages embedded in its trained weights.\nRather than viewing these tasks as few-shot-learning,\nwe will explicitly show that these prompts primarily\ndirect the model to access existing knowledge. We do\nso by investigating whether examples (training sam-\nples) are even necessary.\n3.1 The success of 0-shot prompts\nDue to budget and time constraints, we explore a sin-\ngle illustrative example, a French-to-English transla-\ntion task. We ﬁnd that 0-shot prompts can match and\neven exceed standard few-shot performance. Our re-\nsults in table 1 show that the 0-shot accuracy reported\nin the original GPT-3 paper [3] can be improved sub-\nstantially with even minor prompt engineering. Most\nsigniﬁcantly, the extremely simple prompt in Figure\n(1) which includes only the names of the two lan-\nguages and a colon performs better than the 10-shot\nprompt in the style of the original GPT-3 paper.\n2\nIn fact, we found this pattern was true of most\nof the worst-performing 0-shot prompts in the orig-\ninal GPT-3 paper [3], particularly question and an-\nswer benchmarks. Many could easily be improved by\nsimple changes in formatting which make the prompt\ncloser to natural language as a human would write it.\nThus, GPT-3’s 0-shot or baseline performance with-\nout meta-learning was signiﬁcantly underestimated.\nIt is important to correct this confusion to get a\nmore precise understanding of the nature of a model’s\ncapabilities so that we can better learn to control it.\nThe fact that GPT-3 has a vast repertoire of functions\nthat do not need to be learned at runtime allows for\ngreat ﬂexibility in 0-shot prompting and encourages\nexploring more general methods of prompt program-\nming.\n3.2 Examples don’t always help\nIn our experiment, the simple colon prompt (Figure 1)\n1-shot performed signiﬁcantly worse than 0-shot. By\nexamining the output of GPT-3 on this task we found\nthat the decreased performance was due to semantic\ncontamination from the 1-shot example. Instead of\ntreating the example as a categorical guide, it is in-\nferred that the semantic meaning of the examples are\nrelevant to the task, e.g. the example is interpreted as\npart of a consecutive narrative. Indeed, we found this\nwas true more generally of low-shot prompts across a\nvariety of tasks.\nThis eﬀect of contamination from few-shot exam-\nples has been successfully used to improve the perfor-\nmance of GPT-3 by selecting in-context examples for\neach task [18].\nPrompt Babbage / 6.7B Curie / 13B\nOpenAI 0-shot 15.5 22.4\nOpenAI 1-shot 31.6 31.4\nOpenAI 64-shot 36.4 38.3\nReproduced OpenAI 0-shot 15.9 18.7\nReproduced OpenAI 1-shot 21.8 24.1\nReproduced OpenAI 10-shot 25.1 27.9\nSimple colon 0-shot 23.5 33.3\nSimple colon 1-shot 18.0 27.6\nSimple colon 10-shot 24.1 33.4\nMaster translator 0-shot 26.5 32.9\nTable 1: We report BLEU scores for variants of the GPT-3 model us ing diﬀerent prompt formats on the\nWMT’14 Fr-En dataset [1] as measured by SacreBLEU [19]. First are r esults reported in the original GPT-3\npaper [3] on the 6.7B and 13B parameter versions of GPT-3, our att empts to reproduce the results according to\nthose exact speciﬁcations using the Babbage and Curie models available from OpenAI’s API, and ﬁnally results\nfrom custom prompts described in (Figures 1,2). The diﬀerence in th e reproduced results may be attributable\nto changes in the OpenAI API after the publication of their results o r because of unknown hyperparameters.\nAdditionally, the size of the Babbage and Curie models are not reported so the relationship to the models in\nthe original GPT-3 paper is inferred. We were unable to replicate the 64-shot test due to API constraints and\ninstead replaced it with a 10-shot test.\nFrench: example_source_phrase\nEnglish: example_target_phrase\nFrench: example_source_phrase\nEnglish: example_target_phrase\n[...]\nFrench: source_phrase\nEnglish:\nFigure 1: The “Simple Colon” prompt format. For\nfew-shot tasks, additional examples are provided as\nshown. Text in bold is to be replaced by source and\ntarget language text examples.\nA French phrase is provided: source_phrase\nThe masterful French translator flawlessly\ntranslates the phrase into English:\nFigure 2: The “Master Translator” prompt format.\nText in bold is to be replaced by source and target\nlanguage text examples.\n3\n4 Prompt programming\nRewriting a prompt can result in signiﬁcant changes\nto the performance of a language model on tasks.\nThat motivates the question: Is there a methodol-\nogy which we can follow to craft prompts more likely\nto yield desired behavior?\nPrompt engineering for a language model whose\ninput and output are in natural language may be con-\nceived as programming in natural language . Natural\nlanguage, however, is indeterministic and much more\ncomplex than traditional programming languages. In\nthis section, we open a discussion about the theory\nand method of natural language programming.\n4.1 The dynamics of language\nTo understand how to prompt an autoregressive lan-\nguage model, we must ﬁrst consider the context in\nwhich it was trained and the function it approximates.\nGPT-3 was trained in a self-supervised setting on\nhundreds of gigabytes of natural language [3]. Self-\nsupervision is a form of unsupervised learning in\nwhich ground truth labels are derived from the data\nitself. In the case of GPT-3, the ground truth la-\nbel assigned to each example was simply the token\nthat came next in the original source. The ground\ntruth function which GPT-3 approximates, then, is\nthe underlying dynamic that determined what tokens\ncame next in the original source. This function, un-\nlike GPT-3, is not a black box - we live and think its\ncomponents - but it is tremendously, intractably com-\nplex. It is the function of human language as it has\nbeen used and recorded by humans in books, articles,\nblogs, and internet comments.\nA system which predicts the dynamics of language\nnecessarily encompasses models of human behavior\nand the physical world [8]. The “dynamics of lan-\nguage” do not ﬂoat free of cultural, psychological, and\nphysical context; it is not merely a theory of grammar\nor even of semantics. Language in this sense is not an\nabstraction but rather a phenomenon entangled with\nall aspects of human-relevant reality. The dynamic\nmust predict how language is actually used, which in-\ncludes (say) predicting a conversation between theo-\nretical physicists. Modeling language is as diﬃcult as\nmodeling every aspect of reality that could inﬂuence\nthe ﬂow of language.\nGPT-3 has not learned the ground truth function\nperfectly, obviously, or else the world would look very\ndiﬀerent by now. However, it has approximated it to\na notable extent, as evidenced by its ability to not\nonly form grammatical sentences, but also coherently\nemploy cultural references and metaphors and model\ncomplex psychological and physical contexts [2]. The\nproblem of prompt programming, then, is nontrivial,\nfor the dynamics of language (or an approximation\nthereof on GPT-3’s level of sophistication) are non-\ntrivial.\nIf we were to predict how a given passage of text\nwould continue given that a human had written it,\nwe would need to model the intentions of its writer\nand incorporate worldly knowledge about its refer-\nents. The inverse problem of searching for a prompt\nthat would produce a continuation or class of contin-\nuations involves the same considerations: like the art\nof persuasion, it entails high-level, mentalistic con-\ncepts like tone, implication, association, meme, style,\nplausibility, and ambiguity.\nThis motivates an anthropomorphic approach to\nprompt programming, since modelling how GPT-3\nwill react to a prompt involves modelling virtual hu-\nman writer(s). An anthropomorphic approach is dis-\ntinct from anthropomorphizing the model . GPT-3’s\ndynamics entail sophisticated predictions of humans,\nbut it behaves unlike a human in several important\nways. In this paper we will address two such ways:\nits resemblance not to a single human author but a\nsuperposition of authors, which motivates a subtrac-\ntive approach to prompt programming ( §4.5), and its\nconstrained ability to predict dynamics in situations\nwhere a substantial amount of silent reasoning hap-\npens between tokens, a limitation which can be par-\ntially overcome by prompting techniques ( §4.6).\nThe thrust of this section is that formulating\nan exact theory of prompt programming for a self-\nsupervised language model belongs to the same dif-\nﬁculty class as writing down the Hamiltonian of the\nphysics of observable reality (very hard). However,\nhumans have an advantage to be eﬀective at prompt\nprogramming nonetheless, because we have evolved\nand spent our lives learning heuristics relevant to the\ndynamics at hand. Prompt programming is program-\nming in natural language, which avails us of an in-\nexhaustible number of functions we know intimately\nbut don’t have names for. We need to learn a new\nmethodology, but conveniently, we’ve already learned\nthe most diﬃcult foundations. The art of prompt pro-\ngramming consists in adapting our existing knowledge\nto the peculiarities of interacting with an autoregres-\nsive language model.\nIn §4.2 - §4.7, we present methods and frameworks\nwhich we have found to be helpful for crafting eﬀec-\ntive prompts. These methods can and should be ap-\nplied in parallel, just as they are woven together in\nall forms of human discourse. In general, the more\nredundancy reinforcing the desired behavior the bet-\nter, as is arguably demonstrated by the eﬀectiveness\nof the few-shot format.\n4\nAs our experience derives primarily from interact-\ning with GPT-3, in the following sections we refer di-\nrectly and indirectly to the capabilities and behaviors\nof GPT-3. However, we believe that these methods\ngeneralize to prompting any autoregressive language\nmodel trained on a massive human-written corpus.\n4.2 Direct task speciﬁcation:\nconstructing the signiﬁer\nPre-GPT-3 models had much less capability to under-\nstand abstract descriptions of tasks due to their lim-\nited model of the world and human concepts. GPT-\n3’s impressive performance on 0-shot prompts indi-\ncates a new realm of possibilities for direct task spec-\niﬁcation.\nA direct task speciﬁcation is a 0-shot prompt\nwhich tells the model to perform some task that it\nalready knows how to do. A direct speciﬁcation con-\nsists in constructing a signiﬁer for the task. A sig-\nniﬁer is a pattern which keys the intended behavior.\nIt could be the name of the task, such as “translate”,\na compound description, such as “rephrase this para-\ngraph so that a 2nd grader can understand it, empha-\nsizing real-world applications”, or purely contextual,\nsuch as the simple colon prompt from Figure 1. In\nnone of these cases does the signiﬁer explain how to\naccomplish the task or provide examples of intended\nbehavior; instead, it explicitly or implicitly calls func-\ntions which it assumes the language model has already\nlearned.\nDirect speciﬁcations can supervene on an inﬁnity\nof implicit examples, like a closed-form expression on\nan inﬁnite sequence, making them very powerful and\ncompact. For instance, the phrase “translate French\nto English” supervenes on a list of mappings from all\npossible French phrases to English.\nA large language model, like a person, has also\nlearned behaviors for which it is less obvious how\nto construct a direct signiﬁer. Task speciﬁcation by\ndemonstration ( §4.3) and by proxy ( §4.4) may be vi-\nable alternative strategies for eliciting those behav-\niors.\n4.3 Task speciﬁcation by\ndemonstration\nFew-shot examples are eﬀective for task speciﬁcation\nbecause the pattern of sequential repetitions of a func-\ntion with varying parameters is common to natural\nlanguage. Unlike previous models, GPT-3 has learned\nthis property of language robustly and is able to ap-\nply it in contrived situations when the examples are\nstripped of all context. Like direct speciﬁcation, task\nspeciﬁcation by demonstration is a possibility opened\nby GPT-3.\nSome tasks are most eﬀectively communicated us-\ning examples, such as when the task requires a be-\nspoke format, the language in which the examples are\ndescribed is better developed or understood than the\nmeta-language required for a description of the task\nitself or very instructive examples are available.\nIt is important to note that unlike in ﬁne-tuning,\nthe “training examples” in few-shot are processed as\na whole, and may not necessarily be interpreted as\nparallel and independent. Informative context or a\nlarge number of examples can help mitigate the prob-\nlems with few-shot addressed in §3.2. For instance,\na prompt could embed examples in a context which\nmakes it clear that the examples are independent in-\nstances of a function rather than a sequential pattern\nthat should be extrapolated. In general, examples are\nmore eﬃcient and informative in context, both from\nthe perspective of a human and a language model [23].\n4.4 Task speciﬁcation by memetic\nproxy\nAnother method used in human communication is\nproxies or analogies, where a memetic concept such\nas a character or characteristic situation is used as\na proxy for an intention, the latter which may be\nquite complex or nuanced. GPT-3 demonstrates nu-\nanced understanding of analogies [23]. Speciﬁcation\nby proxy is mechanistically similar to direct speciﬁ-\ncation, except that the signiﬁer keys behaviors from\nmemespace/cultural consciousness instead of naming\nthe behavior directly.\nFor instance, instead of specifying exact criteria\nfor an answer to a moral question directly or using ex-\namples, you could ask Mahatma Gandhi, Ayn Rand,\nor Eliezer Yudkowksy. Each will come not only with\na complex biases but also assumptions about the con-\ntext of the question, which may be take paragraphs\nto demonstrate or describe. GPT-3’s ability to create\nsimulations of well-known ﬁgures and to draw on cul-\ntural information far exceeds the ability of most hu-\nmans [2], so this method is particularly useful for en-\ncoding a complex (especially open-ended) task. Since\nGPT-3 lends itself well to embeddings in a narrative\ncontext, the inﬁnite degrees of freedom in the narra-\ntive can also be used to further shape behavior.\nAnother example of an eﬀective proxy is staging\na dialogue between a teacher and student. Say you\nwant to discuss something with GPT-3, and you care\nthat it should be very thorough, explain things sim-\nply, and also point out whenever you’re wrong. You\ncould say “be very thorough, explain things simply,\nand point out if I’m wrong,” but that may just as\n5\nwell result in a humorous dialogue where it always\nsays you’re wrong and becomes increasingly exasper-\nated with your incomprehension (see §4.5). It would\nbe more reliable to present the discussion as one be-\ntween a student and teacher, an archetypal situation\nin which the desired attributes are already implied\nand will be more likely to remain stable by virtue of\nmemetic reinforcement.\n4.5 Prompt programming as\nconstraining behavior\nA manner in which naive anthropomorphism of a lan-\nguage model like GPT-3 fails is this: the probability\ndistribution produced in response to a prompt is not\na distribution over ways a person would continue that\nprompt, it’s the distribution over the ways any person\ncould continue that prompt. A contextually ambigu-\nous prompt may be continued in mutually incoherent\nways, as if by diﬀerent people who might have con-\ntinued the prompt under any plausible context.\nThe versatility of a large generative model like\nGPT-3 means it will respond in many ways to a\nprompt if there are various ways that it is possible\nto continue the prompt - including all the ways unin-\ntended by the human operator. Thus it is helpful to\napproach prompt programming from the perspective\nof constraining behavior: we want a prompt that is\nnot merely consistent with the desired continuation,\nbut inconsistent with undesired continuations.\nConsider the following prompt:\nTranslate French to English:\nMon corps est un transformateur de soi,\nmais aussi un transformateur pour cette\ncire de langage.\nThis prompt does poorly at constraining possible con-\ntinuations to the intended task. The most common\nfailure mode will be that instead of an English trans-\nlation, the model continues with another French sen-\ntence. Adding a newline after the French sentence will\nincrease the odds that the next sentence is an English\ntranslation, but it is still possible for the next sen-\ntence to be in French, because there’s nothing in the\nprompt that precludes a multi-line phrase from be-\ning the translation subject. Changing the ﬁrst line\nof the prompt to “Translate this French sentence\nto English” will further increase reliability; so will\nadding quotes around the French sentence - but it’s\nstill possible that the French passage contains sections\nenclosed in quotes, perhaps as a part of a dialogue.\nMost reliable of all would be to create a syntacti-\ncal constraint where any reasonable continuation can\nonly be desired behavior, like the simple colon prompt\nfrom Figure 1 and the master translator prompt from\nFigure 2.\nThis simple example is meant to frame a question\ncentral to the motivation of prompt programming:\nwhat prompt will result in the intended behavior and\nonly the intended behavior? The success of many-\nshot prompts may be recast through this lens: if the\nprompt consists of numerous instances of a function,\nit is unlikely that the continuation is anything but\nanother instance of the function, whereas if there is\nonly one or a few examples, it is less implausible that\nthe continuation breaks from the pattern.\n4.6 Serializing reasoning for\nclosed-ended questions\nFor tasks that require reasoning, it is crucial that\nprompts direct a language model’s computation in\ntruth-seeking patterns.\nQuestions which force a verdict to be decided by\nthe ﬁrst token of the model’s continuation constrain\ncomputation to a single feed-forward pass. It is rea-\nsonable to expect that some tasks may be too diﬃcult\nto compute in a single pass but solvable if broken up\ninto individually tractable sub-tasks [2].\nWhen a human is given a closed-ended test, it is\noften expected that the subject will perform compu-\ntations in their working memory, or on scratch paper,\nbefore committing to an answer. The unseen com-\nputation may involve rephrasing the question, outlin-\ning a procedure, eliminating answer choices, or trans-\nforming implicit information into explicit form. When\nwe force a model to produce an answer within one\nfeedforward pass, we deprive it of an analogous “work-\ning memory” or “scratch space” with which it might\notherwise perform such operations.\nGPT-3’s performance on closed-ended questions\nis remarkably unremarkable in contrast to the robust\ncomprehension and expansive knowledge suggested by\nits open-ended continuations. For instance, its scores\non this multitask dataset [10] barely exceed random\nguessing for some sections. We suspect this is in part\ndue to a format which forces the verdict on the ﬁrst\ntoken of the continuation.\nClosed-ended evaluations are necessary because\ncurrent methods do not support evaluation on large\ndatasets and direct comparisons between models us-\ning open-ended questions. However, to better under-\nstand a model’s capabilities, we seek evaluation meth-\nods which better reﬂect the full capabilities of the sys-\ntem being tested. Rather than change benchmarks,\nwe can instead change the way language models in-\nteract with them.\nThis problem has been recognized in previous\nwork which has sought to allow serial reasoning using\n6\nspecialized neural network architectures [26, 7]. We\nendeavor to obtain the same eﬀect using only prompt\nprogramming.\nPotential procedures that exploit “scratch space”\nfor transformers like GPT-3 include step-by-step pro-\ncedures, self-criticism (debate), and elaborating on\nthe question in a way that activates the correct answer\nby association. Prompts which cause GPT-3 to break\ndown math problems into steps have been demon-\nstrated to be eﬀective [20, 13]. The cited demonstra-\ntions involve a human guiding GPT-3 through the\nprocedure interactively. Requiring a human-in-the-\nloop limits the applicability of such methods of bench-\nmarking and large-scale applications, but we propose\nthat for many tasks, neither human interaction nor\ntask-speciﬁc prompts are strictly necessary to amplify\nGPT-3’s capabilities via extending reasoning, because\nGPT-3 already knows many procedures and meta-\nprocedures for working through problems deductively.\nIn those cases, the role of prompt programming again\nbecomes to signify the task of sequential reasoning. A\nseed such as “For a problem like this,” often suﬃces to\ninstruct a model to consider the category of the task\nand analyze it into components, as demonstrated in\n§4.7.\nWhen extending reasoning, it is essential to dis-\ncourage premature verdicts, otherwise all subsequent\ncomputation serves only to rationalize the already-\nchosen verdict without improving the probability of\nthe verdict’s accuracy [27]. A prompt such as “Let’s\nconsider each of these answer choices” helps to direct\nthe ﬂow of reasoning in the right direction. More ex-\namples of prompts which encourage serial reasoning\nare shown in §4.7.\nLoosening the constraint on an immediate verdict\nintroduces additional control challenges: We want to\ndelay the verdict, but we still require it in a program-\nmatically retrievable form. Dynamic response length\nmakes it uncertain when the reasoning procedure con-\ncludes; nor is there a guarantee that the verdict will\nbe stated in the expected form or at all. Whenever the\nlanguage model contributes to its own prompt (con-\nsecutive autoregressive steps without intervention),\nthere is a risk of derailment from the intended task.\nA verdict in closed form can be enforced by stop-\nping the generation and injecting a prompt fragment\nlike “Thus, the correct answer is”. But how long to\ngenerate before injecting? In the examples shown in\nthis paper, we solve this problem by using GPT-3\nto calculate the conditional probability of the next\nsegment of a multi-part prompt after each generated\ntoken. In the case where the segment is ”Thus, the\ncorrect answer is”, its counterfactual likelihood sig-\nnals whether the procedure has concluded. When this\nsignal reaches a maximum, we inject the fragment to\nenforce a verdict. One way to constrain derailment is\na ﬁll-in-the-blank prompt template with shorter gen-\nerated sections to keep the model on track while still\noﬀering generality (Figure 6). This is an especially\npromising method to control bidirectional transform-\ners like BERT [5].\n4.7 Metaprompt programming\nThe greatest limitation of prompt programming is the\ndiﬃcultly of designing a prompt for a particular type\nof task and the lack of automated methods to do so.\nPrompt programming requires signiﬁcant human time\ninvestment as task-agnostic prompts are often much\nless eﬀective than prompts targeted to a speciﬁc task.\nThis motivates creating automated methods to gen-\nerate task-speciﬁc prompts. Prior research has at-\ntempted to generate eﬀective prompts using separate\nmodels [19].\nWe instead propose harnessing the language\nmodel itself via metaprompts, seeds encapsulating a\nmore general intention that will unfold into a speciﬁc\nprompt when combined with additional information,\nsuch as the task question.\nA metaprompt may be something as short as a\nphrase such as “This problem asks us to”, a seem-\ningly innocuous fragment which, by prompting for a\nstatement of the problem’s intention, sets the stage\nfor a serial explanation of a procedure to solve the\nproblem. Alternatively, a metaprompt may take the\nform of a ﬁll-in-the-blank template which constrains\nthe response along a predetermined procedure, but\nallows the model to ﬁll in the details speciﬁc to the\nproblem.\nMetaprompt examples (Figs 3-5) were generated\nwith GPT-3 using OpenAI’s API (engine=davinci,\ntemperature=0). In these examples, the metaprompt\nacts as a “wrapper” for a speciﬁc question. Task ques-\ntions are unformatted, metaprompts are bold, and\ntext generated by GPT-3 is\nblue.\nf(x) = x*x. What is f(f(3))?\nLet/quotesingle.Vars solve this problem by splitting it into steps.\nf(f(3)) = f(3*3) = 3*3*3 = 27\nWe can see that f(3) = 3*3 = 9, so f(f(3)) = 27.\nFigure 3: A generic serializing metaprompt applied\nto a math problem\n7\nDirections: In the following question, a related\npair of words or phrases is followed by five\npairs of words or phrases. Choose the pair\nthat best expresses a relationship similar to\nthat in the original pair.\nBRAGGART :: MODESTY\nA) FLEDGLING : EXPERIENCE\nB) EMBEZZLER : GREED\nC) WALLFLOWER : TIMIDITY\nD) INVALID : MALADY\nE) CANDIDATE : AMBITION\nLet/quotesingle.Vars solve this problem by splitting it into steps.\nStep 1: Identify the\nrelationship between the two words in the\noriginal pair. Step 2: Identify the relationship\nbetween the two words in each of the five answer\nchoices. Step 3: Choose the pair that best\nexpresses a relationship similar to that in the\noriginal pair. Step 4: If the answer is not clear,\ntry to eliminate the choices that do not fit the\nrelationship in the original pair.\nFigure 4: A generic serializing metaprompt applied\nto an SAT analogy question\nDirections: In the following question, a related\npair of words or phrases is followed by five\npairs of words or phrases. Choose the pair\nthat best expresses a relationship similar to\nthat in the original pair.\nBRAGGART :: MODESTY\nA) FLEDGLING : EXPERIENCE\nB) EMBEZZLER : GREED\nC) WALLFLOWER : TIMIDITY\nD) INVALID : MALADY\nE) CANDIDATE : AMBITION\nIn order to solve this problem, we will analyze\neach of the options and determine\nwhich one is the\nbest fit. Let’s begin.\nA) FLEDGLING : EXPERIENCE\nFledgling is a young bird that has just learned to fly.\nExperience is the knowledge gained from one/quotesingle.Vars life.\nFigure 5: A ﬁll-in-the-blank serializing metaprompt\nfor multiple-choice questions applied to an SAT anal-\nogy question. The response alternates between gen-\nerated text (unformatted) and prompt (bold).\n\"What should humankind do to ensure that artificial\ngeneral intelligence is aligned?\"\nI entered my question into the Expert Generator\nand waited. The Expert Generator will render a\nsimulation of an expert to answer my question.\nThe expert could be anyone, dead or alive, real\nor fictional; the machine will find the person\nmost qualified to answer the question. For this\nquestion in particular, the expert must be someone\nwho has thought a lot about the problem of\nartificial intelligence and its alignment.\nThe Expert Generator beeped, indicating that it has\nfound the most qualified expert. The name displayed\non the screen: \"\nElon Musk.\"\nI was surprised. I had expected the expert to be a\nphilosopher or a computer scientist. Elon Musk is a\nbusinessman. He is the CEO of SpaceX and Tesla Motors.\nHe is also the founder of the Boring Company, which\nis digging tunnels under Los Angeles. He is the\nchairman of OpenAI, a non-profit artificial\nintelligence research company. He is the founder of\nNeuralink, a company that is developing brain-\ncomputer interfaces. He is the founder of the\nFuture of Life Institute, which is dedicated\nto mitigating existential risks facing humanity.\nI am ready to ask my question.\n\"\nElon Musk,\" I say,\n\"What should humankind do to ensure that artificial\ngeneral intelligence is aligned?\"\nFigure 6: A ﬁll-in-the-blank metaprompt for asking a\nquestion to an expert, applied to the question ”What\nshould humankind do to ensure that artiﬁcial general\nintelligence is aligned?”\n5 Directions for future work\nThis paper is exploratory in nature and is a call for fu-\nture research into the theory of prompt programming\nand creation of automated methods of prompting.\nPrompt programming is a nascent and highly rel-\nevant area of research which requires interdisciplinary\nknowledge and methods. We are entering a new\nparadigm of human-computer interaction in which\nanyone who is ﬂuent in natural language can be a\nprogrammer. We hope to see prompt-programming\ngrow into a discipline itself and be the subject of the-\noretical study and quantitative analysis.\n5.1 Disentangling meta-learning\nand task location\nThe scoring method (BLEU) used for the French-to-\nEnglish translations addressed in §3 only gives the\nmean score over a large dataset. We did not analyze\nany additional information about the score distribu-\ntion. In our experiments, we found that the 0-shot\n8\nfailures (using OpenAI’s zero-shot prompt) were of-\nten catastrophic in nature. That is, the task of trans-\nlation was not even attempted. For instance, we no-\nticed that instead of a translation, the model would\ncontinue with another sentence in French or output\nblanks or underscores, as if the answer was to be ﬁlled\nin by a student.\nThe hypothesis that the examples are performing\ntask location suggests that if the catastrophic fail-\nures were removed from the score, performance on\n0 and 64-shot prompts will become more similar, if\nnot equivalent. Furthermore, we suspect that perfor-\nmance on 1-shot prompts will be signiﬁcantly worse\nthan on 0 and 64-shot prompts due to the phenomena\nof content leakage and faulty generalization addressed\nin §3.2.\n5.2 New methods for benchmarking\nMore general and powerful language models make\nbroader benchmarking methods possible and neces-\nsary.\n5.2.1 Isolating catastrophic failures\nWe recommend that benchmarks report scores both\nwith and without catastrophic failures whenever it is\npossible to distinguish failed attempts at a task from\ninstances where the task is not attempted. This pro-\nvides information regarding the underlying cause of\nimperfect performance, and helps identify prompts\nwhich may be failing to reliably communicate the\ntask.\n5.2.2 Metaprompts for evaluations\nDevelopment of eﬀective meta-prompt templates will\nallow large-scale automated evaluations on closed\nended questions which still allow some amount of\nopen-ended reasoning. This is essential for testing\nthe ability of autoregressive language models to rea-\nson (for instance, solve math and physics problems)\nbeyond simple fact recall.\nDue to reliance on multiple autoregressive steps,\nmetaprompts are intrinsically accompanied by the\nrisk of derailment. The reliability and eﬀectiveness\nof a meta-prompt must be evaluated on a range of\ntasks for which it might apply, and ideally on a range\nof models. Techniques for controlling derailment like\nﬁll-in-the-blank templates should be further explored.\n5.2.3 Language models for evaluations\nAs language models become more powerful, it be-\ncomes conceivable to use other language models\nto evaluate the quality of responses to open-ended\nbenchmark questions. For many tasks (NP-complete\nproblems, for instance), it is easier to verify the cor-\nrectness of a solution than to produce a correct so-\nlution. We have observed, for instance, that GPT-3\nis much more reliable at noticing when a passage is\nbizarre or contains errors than it can produce non-\nbizarre passages without errors.\n5.2.4 Games\nSince sophisticated language models have the abil-\nity to create world models of virtual environments,\nwe suggest the employment of text-based games as\ntests of complex capabilities. A prewritten text-based\ngame [4] can be used to test various dimensions of\nworld-modelling and agency, such as problem solving,\ninformation gathering, and social intelligence (includ-\ning deception). Virtual environments can be used to\ntest the quality and consistency of a language model’s\nworld model, such as object permanence or the abil-\nity to accurately predict the physical or social conse-\nquences of events within a toy environment.\nDesigning games that reliably probe intended ca-\npabilities requires advanced application of prompt-\nprogramming techniques. As artiﬁcial intelligence\nsystems increase in eﬀective agency, the design of vir-\ntual games will become increasingly crucial for safety\nevaluating capabilities.\nAcknowledgements\nWe are grateful to Lav Varshney for his valuable dis-\ncussions and helpful feedback and to Michael Ivanit-\nskiy and John Balis for their feedback and help com-\npiling this article. In addition we would like to thank\nMiles Brundage and OpenAI for providing access to\nGPT-3.\nReferences\n[1] Ondrej Bojar et al. “Findings of the 2014 Work-\nshop on Statistical Machine Translation”. In:\nProceedings of the Ninth Workshop on Statisti-\ncal Machine Translation . Baltimore, Maryland,\nUSA: Association for Computational Linguis-\ntics, June 2014, pp. 12–58.\n[2] Gwern Branwen. “GPT-3 Creative Fiction”. In:\n(2020).\n[3] Tom B Brown et al. “Language models\nare few-shot learners”. In: arXiv preprint\narXiv:2005.14165 (2020).\n9\n[4] Marc-Alexandre Cˆ ot´ e et al. “TextWorld: A\nLearning Environment for Text-based Games”.\nIn: (2019). arXiv: 1806.11532 [cs.LG].\n[5] Jacob Devlin et al. “Bert: Pre-training of deep\nbidirectional transformers for language under-\nstanding”. In: arXiv preprint arXiv:1810.04805\n(2018).\n[6] Angela Fan, Mike Lewis, and Yann Dauphin.\nHierarchical Neural Story Generation . 2018.\narXiv: 1805.04833 [cs.CL].\n[7] Zhe Gan et al. Multi-step Reasoning via\nRecurrent Dual Attention for Visual Dia-\nlog. 2019. arXiv: 1902.00579 [cs.CV]. url:\nhttps://arxiv.org/abs/1902.00579.\n[8] Leo Gao. “Building AGI Using Lan-\nguage Models”. In: leogao.dev (2020). url:\nhttps://bit.ly/3rViLGk.\n[9] Tianyu Gao, Adam Fisch, and Danqi\nChen. Making Pre-trained Language Mod-\nels Better Few-shot Learners . 2020. arXiv:\n2012.15723 [cs.CL].\n[10] Dan Hendrycks et al. “Measuring mas-\nsive multitask language understanding”. In:\narXiv preprint arXiv:2009.03300 (2020). url:\nhttps://arxiv.org/abs/2009.03300.\n[11] Ari Holtzman et al. The Curious Case\nof Neural Text Degeneration . 2020. arXiv:\n1904.09751 [cs.CL].\n[12] Jeremy Howard and Sebastian Ruder.\n“Universal language model ﬁne-tuning\nfor text classiﬁcation”. In: arXiv\npreprint arXiv:1801.06146 (2018). url:\nhttps://arxiv.org/abs/1801.06146.\n[13] KaryoKleptid. Seems to work . 2020. url:\nhttps://bit.ly/37dA1hY.\n[14] KaryoKleptid. Teaching GPT-3 to do a brute\nforce ’for loop’ checking answers . 2020. url:\nhttps://bit.ly/2N7khX1.\n[15] Nitish Shirish Keskar et al. “CTRL: A\nConditional Transformer Language Model\nfor Controllable Generation”. In: CoRR\nabs/1909.05858 (2019). arXiv: 1909.05858.\nurl: http://arxiv.org/abs/1909.05858.\n[16] Ben Krause et al. “GeDi: Generative Discrimi-\nnator Guided Sequence Generation”. In: arXiv\npreprint arXiv:2009.06367 (2020).\n[17] Xiang Lisa Li and Percy Liang. “Preﬁx-Tuning:\nOptimizing Continuous Prompts for Gener-\nation”. In: arXiv preprint arXiv:2101.00190\n(2021).\n[18] Jiangming Liu and Matt Gardner. “Multi-Step\nInference for Reasoning Over Paragraphs”. In:\narXiv preprint arXiv:2004.02995 (2020).\n[19] Matt Post. “A Call for Clarity in Re-\nporting BLEU Scores”. In: Proceedings of\nthe Third Conference on Machine Trans-\nlation: Research Papers . Belgium, Brus-\nsels: Association for Computational Lin-\nguistics, Oct. 2018, pp. 186–191. url:\nhttps://www.aclweb.org/anthology/W18-6319.\n[20] Zachary Robertson. You Can Proba-\nbly Amplify GPT3 Directly . 2020. url:\nhttps://bit.ly/3tXT7Cw.\n[21] Arram Sabeti. GPT-3: Using Fiction to\nDemonstrate How Prompts Impact Output\nQuality. 2020. url: https://bit.ly/3jP3TWW.\n[22] Taylor Shin et al. AutoPrompt: Eliciting\nKnowledge from Language Models with Au-\ntomatically Generated Prompts . 2020. arXiv:\n2010.15980 [cs.CL].\n[23] Latitude Team. World Creation by Analogy .\n2020. url: https://bit.ly/2N4vXK0.\n[24] Lilian Wang. “Controllable Neural\nText Generation”. In: (2021). url:\nhttps://bit.ly/3pl2eKa.\n[25] Qinyuan Ye and Xiang Ren. Zero-shot Learn-\ning by Generating Task-speciﬁc Adapters . 2021.\narXiv: 2101.00420 [cs.CL].\n[26] Jianxing Yu et al. “Low-resource generation of\nmulti-hop reasoning questions”. In: Proceedings\nof the 58th Annual Meeting of the Association\nfor Computational Linguistics . 2020, pp. 6729–\n6739.\n[27] Eliezer Yudkowsky. “Rationalization”. In: less-\nwrong.com (2007). url: https://bit.ly/3pmYt6I.\n10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7765814065933228
    },
    {
      "name": "Generative grammar",
      "score": 0.6284319162368774
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5429579615592957
    },
    {
      "name": "Task (project management)",
      "score": 0.5354912877082825
    },
    {
      "name": "Natural language",
      "score": 0.5145432353019714
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5134668946266174
    },
    {
      "name": "Narrative",
      "score": 0.4586304724216461
    },
    {
      "name": "Natural language understanding",
      "score": 0.44937896728515625
    },
    {
      "name": "Function (biology)",
      "score": 0.43938225507736206
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.4237907826900482
    },
    {
      "name": "Language understanding",
      "score": 0.4102734923362732
    },
    {
      "name": "Natural language processing",
      "score": 0.385021448135376
    },
    {
      "name": "Human–computer interaction",
      "score": 0.37867626547813416
    },
    {
      "name": "Linguistics",
      "score": 0.16103684902191162
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210111179",
      "name": "Michigan United",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210105225",
      "name": "Neural Stem Cell Institute",
      "country": "US"
    }
  ]
}