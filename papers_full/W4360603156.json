{
  "title": "Lightweight Fine-tuning a Pretrained Protein Language Model for Protein Secondary Structure Prediction",
  "url": "https://openalex.org/W4360603156",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1987581894",
      "name": "Wei Yang",
      "affiliations": [
        "Henan University"
      ]
    },
    {
      "id": "https://openalex.org/A2101066039",
      "name": "Chun Liu",
      "affiliations": [
        "Henan University"
      ]
    },
    {
      "id": "https://openalex.org/A2107695934",
      "name": "Zheng Li",
      "affiliations": [
        "Henan University"
      ]
    },
    {
      "id": "https://openalex.org/A1987581894",
      "name": "Wei Yang",
      "affiliations": [
        "Henan University"
      ]
    },
    {
      "id": "https://openalex.org/A2101066039",
      "name": "Chun Liu",
      "affiliations": [
        "Henan University"
      ]
    },
    {
      "id": "https://openalex.org/A2107695934",
      "name": "Zheng Li",
      "affiliations": [
        "Henan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2158714788",
    "https://openalex.org/W2039139819",
    "https://openalex.org/W2134299061",
    "https://openalex.org/W3104537585",
    "https://openalex.org/W2594661604",
    "https://openalex.org/W2060178110",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W3040739508",
    "https://openalex.org/W2791790018",
    "https://openalex.org/W2919831875",
    "https://openalex.org/W2905446269",
    "https://openalex.org/W2607268717",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W4281993476",
    "https://openalex.org/W4308930019",
    "https://openalex.org/W2153187042",
    "https://openalex.org/W2008708467",
    "https://openalex.org/W2950374603",
    "https://openalex.org/W1989447327",
    "https://openalex.org/W4286500588",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4225868104",
    "https://openalex.org/W2142529984",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W4303645584",
    "https://openalex.org/W2156798505",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2051210555",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W4210535996",
    "https://openalex.org/W4229451461",
    "https://openalex.org/W4242765109",
    "https://openalex.org/W4255594044",
    "https://openalex.org/W2975086539",
    "https://openalex.org/W4306889100",
    "https://openalex.org/W4225891318",
    "https://openalex.org/W2153153865",
    "https://openalex.org/W2130357646",
    "https://openalex.org/W3042567394",
    "https://openalex.org/W3216165545",
    "https://openalex.org/W4210830887",
    "https://openalex.org/W3103992997"
  ],
  "abstract": "Abstract Pretrained large-scale protein language models, such as ESM-1b and ProtTrans, are becoming the fundamental infrastructure for various protein-related biological modeling tasks. Existing works use mainly pretrained protein language models in feature extraction. However, the knowledge contained in the embedding features directly extracted from a pretrained model is task-agnostic. To obtain task-specific feature representations, a reasonable approach is to fine-tune a pretrained model based on labeled datasets from downstream tasks. To this end, we investigate the fine-tuning of a given pretrained protein language model for protein secondary structure prediction tasks. Specifically, we propose a novel end-to-end protein secondary structure prediction framework involving the lightweight fine-tuning of a pretrained model. The framework first introduces a few new parameters for each transformer block in the pretrained model, then updates only the newly introduced parameters, and then keeps the original pretrained parameters fixed during training. Extensive experiments on seven test sets, namely, CASP12, CASP13, CASP14, CB433, CB634, TEST2016, and TEST2018, show that the proposed framework outperforms existing predictors and achieves new state-of-the-art prediction performance. Furthermore, we also experimentally demonstrate that lightweight fine-tuning significantly outperforms full model fine-tuning and feature extraction in enabling models to predict secondary structures. Further analysis indicates that only a few top transformer blocks need to introduce new parameters, while skipping many lower transformer blocks has little impact on the prediction accuracy of secondary structures.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8142668008804321
    },
    {
      "name": "Transformer",
      "score": 0.8128928542137146
    },
    {
      "name": "Language model",
      "score": 0.6432894468307495
    },
    {
      "name": "Fine-tuning",
      "score": 0.6357513070106506
    },
    {
      "name": "Embedding",
      "score": 0.5765717625617981
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5311115384101868
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4826410412788391
    },
    {
      "name": "Task (project management)",
      "score": 0.46620237827301025
    },
    {
      "name": "Machine learning",
      "score": 0.446671724319458
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37117135524749756
    },
    {
      "name": "Voltage",
      "score": 0.1630171239376068
    },
    {
      "name": "Engineering",
      "score": 0.08098945021629333
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I173899330",
      "name": "Henan University",
      "country": "CN"
    }
  ]
}