{
  "title": "Large Scale Language Modeling: Converging on 40GB of Text in Four Hours",
  "url": "https://openalex.org/W2952754453",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227412475",
      "name": "Puri, Raul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2527928916",
      "name": "Kirby Robert",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288982588",
      "name": "Yakovenko, Nikolai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2988481184",
      "name": "Catanzaro, Bryan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2953360861",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2749988060",
    "https://openalex.org/W2953391683",
    "https://openalex.org/W2338908902",
    "https://openalex.org/W2507974895",
    "https://openalex.org/W2765733029",
    "https://openalex.org/W2804935296",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2284050935",
    "https://openalex.org/W2525246036",
    "https://openalex.org/W2806311723",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2963702144",
    "https://openalex.org/W2740711318",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2775461895",
    "https://openalex.org/W2951991713",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W2786685006",
    "https://openalex.org/W2027731328",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W2952020226",
    "https://openalex.org/W2617242334",
    "https://openalex.org/W2769856846",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2759465730",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2158139315"
  ],
  "abstract": "Recent work has shown how to train Convolutional Neural Networks (CNNs) rapidly on large image datasets, then transfer the knowledge gained from these models to a variety of tasks. Following [Radford 2017], in this work, we demonstrate similar scalability and transfer for Recurrent Neural Networks (RNNs) for Natural Language tasks. By utilizing mixed precision arithmetic and a 32k batch size distributed across 128 NVIDIA Tesla V100 GPUs, we are able to train a character-level 4096-dimension multiplicative LSTM (mLSTM) for unsupervised text reconstruction over 3 epochs of the 40 GB Amazon Reviews dataset in four hours. This runtime compares favorably with previous work taking one month to train the same size and configuration for one epoch over the same dataset. Converging large batch RNN models can be challenging. Recent work has suggested scaling the learning rate as a function of batch size, but we find that simply scaling the learning rate as a function of batch size leads either to significantly worse convergence or immediate divergence for this problem. We provide a learning rate schedule that allows our model to converge with a 32k batch size. Since our model converges over the Amazon Reviews dataset in hours, and our compute requirement of 128 Tesla V100 GPUs, while substantial, is commercially available, this work opens up large scale unsupervised NLP training to most commercial applications and deep learning researchers. A model can be trained over most public or private text datasets overnight.",
  "full_text": "Large Scale Language Modeling: Converging on\n40GB of Text in Four Hours\nRaul Puri\nNVIDIA\nraulp@nvidia.com\nRobert Kirby\nNVIDIA\nrkirby@nvidia.com\nNikolai Yakovenko\nNVIDIA\nnyakovenko@nvidia.com\nBryan Catanzaro\nNVIDIA\nbcatanzaro@nvidia.com\nAbstract—Recent work has shown how to train Convolutional\nNeural Networks (CNNs) rapidly on large image datasets [1], then\ntransfer the knowledge gained from these models to a variety of\ntasks [2]. Following [3], in this work, we demonstrate similar\nscalability and transfer for Recurrent Neural Networks (RNNs)\nfor Natural Language tasks.\nBy utilizing mixed precision arithmetic and a 32k batch size\ndistributed across 128 NVIDIA Tesla V100 GPUs, we are able\nto train a character-level 4096-dimension multiplicative LSTM\n(mLSTM) [4] for unsupervised text reconstruction over 3 epochs\nof the 40 GB Amazon Reviews dataset [5] in four hours. This\nruntime compares favorably with previous work taking one\nmonth to train the same size and conﬁguration for one epoch\nover the same dataset [3].\nConverging large batch RNN models can be challenging.\nRecent work has suggested scaling the learning rate as a function\nof batch size, but we ﬁnd that simply scaling the learning\nrate as a function of batch size leads either to signiﬁcantly\nworse convergence or immediate divergence for this problem.\nWe provide a learning rate schedule that allows our model to\nconverge with a 32k batch size.\nSince our model converges over the Amazon Reviews dataset\nin hours, and our compute requirement of 128 Tesla V100 GPUs,\nwhile substantial, is commercially available, this work opens\nup large scale unsupervised NLP training to most commercial\napplications and deep learning researchers 1. A model can be\ntrained over most public or private text datasets overnight.\nI. I NTRODUCTION\nIn recent years, deep learning has been successfully applied\nto many problems. The successful use of transfer learning\nfor computer vision problems has enabled many applications:\nlarge CNNs such as VGG [6] and ResNets [7] are pre-trained\non a large image dataset such as ImageNet [8], [9] and then\nutilized as the backbone for other computer vision tasks. These\nmodels are able to extract meaningful features for new tasks\nwithout needing to be trained from scratch for each task [2],\n[10]–[12].\nRecent work has shown promising results from unsuper-\nvised language modeling, followed by transfer learning to\nnatural language tasks [3], [13]. However, neural language\nmodels have not beneﬁted from scale and transfer learning\nin the same way as convolutional image models. Historically,\nnatural language leverages large scale transfer learning through\nthe use of word embedding pretraining on large corpora [14]–\n[16]. Transferring only the embeddings limits the scope of\n1Our code is publicly available: https://github.com/NVIDIA/sentiment-\ndiscovery\nthe transfer, since word embeddings do not capture sequential\ninformation in a section of text. We would like to transfer\nwhole NLP models capable of processing a text sequence.\nHowever, transfer learning in this context is difﬁcult because\nof the time it takes to train large language models on large\ndatasets. Several recent publications seek to address long\ntraining times by leveraging distributed data parallelism and\nincreasing the effective batch size during training [1], [17]–\n[20], taking advantage of advances in distributed deep learning\nand improvements in the memory size and compute capability\nof available high performance computing (HPC) resources.\nThis work often focuses on computer vision and rarely on\nnatural language tasks, let alone RNN-based language models,\nwhich are numerically difﬁcult to train and suffer from poor\nparallelization due to their sequential nature. We do have evi-\ndence that RNNs for language modeling, speech recognition,\nand neural machine translation continue to provide accuracy\nimprovements as they are trained on larger datasets [21].\nAccordingly, techniques for efﬁciently training large RNN\nmodels will lead to improved accuracy on many natural\nlanguage tasks.\nWe focus on training a single-layer 4096 neuron multi-\nplicative LSTM-based character language model [4] on the\nAmazon Reviews dataset, one of the largest publicly-available\nNLP datasets, and transfer the model to the downstream tasks\nof sentiment classiﬁcation on the Binary Stanford Sentiment\nTreebank (SST) and IMDB movie review datasets. We train\nour recurrent models with mixed precision FP16/FP32 arith-\nmetic, which speeds up training on a single V100 by 4.2X\nover training in FP32.\nWe then train the mixed precision model using a 32k batch\nsize via distributed data parallelism across 128 GPUs. This\nachieves a 109x increase in training data throughput relative\nto the single GPU case. However, with such a large batch size,\nwe require additional epochs to train the model to a similar\nlevel of accuracy, bringing the total training time to 4 hours.\nIn addition, we train a 8192 neuron mLSTM capable of\nbeating state of the art performance in Amazon review lan-\nguage modeling with a bits per character (BPC) of 1.038 and\nSST classiﬁcation accuracy of 93.8%.\nWe analyze how distributed data parallelism scales with\nlarger models. While utilizing distributed data parallelism\nfor training RNNs, we observe some problems common to\ntraining with large batches. We investigate the relationship\narXiv:1808.01371v2  [cs.LG]  11 Aug 2018\nbetween dataset size, batch size, and learning rate schedule\nto investigate how to effectively use large batch training to\ntrain models on commonly available large NLP datasets.\nII. L ANGUAGE MODEL PRETRAINING AND TRANSFER\nSeparately trained word embeddings [14]–[16] are com-\nmonly used to transfer learning from large datasets to speciﬁc\ntasks. However, word embeddings function only as a lookup\ntable for in-vocabulary words. They do not transfer well to\nmulti-word sequences and contexts.\nWorks such as Semi-supervised Sequence Learning [22],\ncontext2vec [23], Contextualized Word Vectors (CoVe) [24],\nand Deep Contextualized Word Representations (ELMo) [25]\nseek to remedy this by computing embeddings of words in a\nsequence using a pretrained recurrent neural language model.\nIn these approaches, the surrounding words provide context\nwhich is used to produce an embedding that represents the\nmeaning of a given word. These works approach the transfer\nlearning problem with a whole neural language model capable\nof modeling the compositional nature of language rather than\na lookup table that considers all words independently.\nThis pretraining and transfer work has motivated follow\non works trying to increase the scope of neural language\nmodel pretraining and transfer [3], [13], [26]–[29], in which\nthe authors explore new types of language models, multiple\ntypes of language model pretraining, and the effect these\ntwo have on a wide variety of down stream language tasks.\nA common theme between these different research efforts,\nhowever, is that downstream transfer success is predicated\non the pretraining corpus size. Larger text corpora produce\nmore powerful language models, which then improve transfer\nlearning.\nA. Pretraining Tasks and Datasets\nAs part of pretraining there are three components that\ndetermine pretraining success: the task used for pretraining,\npretraining dataset quality, and pretraining dataset size.\nThe former requires careful consideration as it affects the\nother two. A number of language pretraining tasks can be\nconsidered generative pretraining tasks, where the language\nmodels are trained to generate some language as output. Some\nof these include sequence to sequence (Seq2Seq) tasks such\nas Skip-Thought pretraining [27], [30] and Neural Machine\nTranslation [27], [31]. However, we instead choose to focus\non unsupervised text reconstruction as our pretraining task:\npredict the next character of text, given the previous characters.\nText reconstruction captures the fundamental components of\nsequence modeling required by other language modeling tasks.\nWith text reconstruction, the data provides its own labels,\nand given the data has undergone reasonable cleaning, we can\nfocus on dataset size rather than dataset type or quality. Several\ncorpora successfully utilized for unsupervised text pretraining\nin prior work are the BooksCorpus [32], GigaWord [33], 1-\nBillion Word [34], and Amazon Reviews [5] datasets. Similar\nto [3], [35], we focus our pretraining efforts on the largest\nof the four datasets (see Fig. 1), by training a mLSTM on\nDataset corpus size (GBs)\n1-Billion Word 3\nBooksCorpus 5\nGigaWord 26\nAmazon Reviews Dataset 41\nFig. 1: Various large language corpora and their size\nan aggressively deduplicated copy of the Amazon Reviews\ndataset totaling 82 million reviews (40GB). The generality\nof our task and the size of our dataset allow the insights\ndeveloped in this work to be applied to other large scale\nlanguage tasks.\nIII. L ARGE BATCH TRAINING\nGiven the size of the Amazon corpus, pretraining a large\nstate of the art neural language model is a time consuming\nprocess. Running such a workload on a single GPU is not\npractical, as state of the art models tend to be large and can\nonly ﬁt a modest training batch size per GPU. In order to\nenable effective pretraining and transfer of large language\nmodels, we employ multi-GPU parallelism. We focus on\nscaling to multiple GPUs with data parallelism, meaning that\nwe partition the batch during training across multiple GPUs.\nWe don’t use model parallelism, which partitions the neural\nnetwork itself across multiple processors, because it’s less\nﬂexible and places more constraints on software, although it\nremains an interesting avenue for further parallelism.\nWe use synchronous data parallelism, where a large batch is\ndistributed evenly amongst all participating worker processes,\nat which point the worker processes run forward and backward\npropagation, communicate the resulting gradients with each\nother, and update the model before receiving a new data\nbatch. Depending on model size and communication latency,\ndata parallelism allows for near linear speed up by scaling\nbatch size linearly with respect to the number of available\nGPUs. Taking advantage of such scaling, the Computer Vision\ncommunity has been able to reduce the training time of\nAlexNet and ResNet-50 models on the ImageNet benchmark\nfrom hours to the order of minutes [1], [17]–[19].\nHowever, these projects have focused on convolutional\nnetworks for image classiﬁcation, and comparatively less work\nhas been published on large batch training of language models.\nOtt et. al [20] employ data parallelism to speed up Seq2Seq\nneural machine translation. However, similar to prior work,\nOtt et. al train convolutional models with large batches.\nIn order to enable large batch pretraining of an arbitrary\nlanguage model it is important to explicitly analyze the effects\nof large batch training with RNN-based language models.\nThe sequential nature of recurrent neural networks makes\nthe training landscape difﬁcult to optimize, due to saddle\npoints, local minima, and numerical instabilities in the RNN\ncomputation itself [36]–[38]. These complexities necessitate\nanalysis of large batch training with RNNs.\nLarge batch training is itself not without difﬁculties. Identi-\ncal hyperparameters at different batch sizes regularly produce\nmodels which generalize differently. Recent work analyzes\nthe relationship between large batch size, learning rates, and\ngeneralization, showing how to achieve similar evaluation\nresults when training across different batch sizes [17], [39],\n[40].\nBy analyzing the noise scale of gradient-descent optimiza-\ntion, these methods modify learning rate ϵ proportionally to\nbatch size B, with a linear scaling rule ϵ ∝ B provided\nthat B ≪N, where N is the dataset size. The authors ﬁnd\nthat learning rate scaling leads to models that generalize well\nacross various batch sizes. Additionally, Smith et. al [39],\n[40] proposed scaling momentum as a function of batch size;\nhowever, we do not investigate such scaling in this work.\nIn order to enable large batch training of RNN language\nmodels, we explore the effects of this linear scaling rule as\nwell as a softer square root scaling rule ϵ∝\n√\nB proposed by\nHoffer et. al [41].\nAdditionally, we investigate the scalability of data paral-\nlelism with different interconnects and model sizes, so as to\nassess the effectiveness of data parallelism for an arbitrary\nneural language model.\nIV. D ISTRIBUTED DEEP LEARNING SETUP\nWe use NVIDIA DGX1-V systems built from 16 GB Tesla\nV100 GPUs. For intra-node and inter-node communication we\nleverage the NCCL2 (NVIDIA Collective Communications)\nlibrary which uses the DGX1-V’s underlying NVLink and\nInﬁniBand connections for GPU to GPU communication.\nWe do not use a central parameter server for managing gra-\ndient reduction and updating the model. In order to efﬁciently\nperform updates to the model, the group of worker processes\nperform a ring reduce of the gradients, and each worker\nindependently updates the model parameters. Crucial to reduc-\ning the necessary communication bandwidth, the library also\nsupports communication of FP16 values natively with no FP16\nemulation overhead when reducing FP16 parameter gradients\nacross GPUs.\nV. M IXED PRECISION TRAINING\nFP16 is not only useful for reducing communication over-\nhead, it also plays a key role in directly accelerating training\non processors like the V100 that support higher throughput\nmixed-precision arithmetic. The V100 provides 15.6 TFlops\nin single precision, but 125 TFlops with mixed-precision\narithmetic (FP16 storage and multiplication, FP32 accumu-\nlation). Using FP16 reduces the dynamic range and precision\nof the computations being performed. This presents a unique\nset of training difﬁculties, which, if not addressed, lead to\nconvergence issues while training.\nDrawing from [42], [43], we use automatic loss scaling\nto effectively increase the dynamic range of the training\nprocess. Automatic loss scaling seeks to ameliorate numerical\nunderﬂow by multiplying the training loss by a scalar ”loss\nscale” factor α > 1, performing backpropagation with all\nintermediary gradients multiplied by α, and dividing the ﬁnal\nweight gradients by α. This multiplication shifts small gradient\nvalues into the range permitted by FP16, thereby ensuring that\nthey do not vanish during back propagation.\nWe choose α dynamically by starting at a large value,\nperforming backpropagation, and checking for an overﬂow\nin the weight gradients. If an overﬂow is detected, then the\nweight update for the batch is skipped, and αis halved. After\nthe algorithm ﬁnds a suitable α, it tries to increase α after a\nsufﬁcient number of iterations have passed without overﬂow,\nand again backs off if overﬂow occurs. The algorithm repeats\nthis process throughout training, iteratively updating the loss\nscale, hence the name automatic loss scaling.\nWithout automatic loss scaling, we found that our models\ndid not train to convergence. Although the computationally\nintensive parts of training were performed in mixed precision,\na minority of the work still remained in FP32 in order to\nconverge properly:\n• Gradients are accumulated into a “master” FP32 copy of\nthe parameters. The division by αoccurs on the gradients\nof these master copies.\n• Reductions are performed in FP32; it only takes a few\nlarge values to cause an overﬂow in FP16.\n• Accumulation of the summation in the ℓ2 norm compu-\ntation required by weight normalization should be done\nin FP32 to avoid overﬂow. The ﬁnal norm value is output\nin FP16.\n• Softmax loss is computed in FP32, operating on FP32\nlogits in order to avoid numerical issues when exponen-\ntiating FP16 values.\nThese techniques working in conjunction allowed for suc-\ncessful training of the mLSTM language model in mixed\nprecision.\nVI. E XPERIMENTS\nAll experiments are set up following [3] and run with\nPytorch’s v0.4 release [44]. The Amazon Reviews dataset is\nshufﬂed and split into training, validation, and test sets. The\nmodel is trained using truncated backpropagation through time\n(TBTT) [45] on sequences of 256 characters. We persist hid-\nden state across each minibatch during training and evaluation.\nA. Data Sharding\nIn order to create the training, validation, and test sets,\nthe dataset is split proportionally by a ratio of 1000, 1, and\n1 allocated for train, validation, and test sets respectively.\nWithin these sets we create batch size B number of shards for\nevaluation, and max(1000,B) shards for training. A shard is\ndeﬁned as a subset of strings sampled without replacement\nfrom one of the dataset splits; this subset is concatenated\ntogether into one large string to form a shard. These shards are\nused for all training epochs with no further shufﬂing. Hidden\nstate is initialized to zero at the beginning of a shard and\npersisted throughout the shard.\nWhen constructing a minibatch Dij , data is sampled such\nthat between two consecutive minibatches i and i+ 1, mini-\nbatch index j contains contiguous subsequences from within a\nshard. This contiguity across minibatches enables hidden state\npersistence across truncated subsequences in TBTT.\nB. Weight Normalization\nIn order to aid with convergence during training time, we\napplied weight normalization [46] to the LSTM parameters\nonly, following [3]. This includes the 4 hidden →hidden and\ninput→hidden parameters of the multiplicative LSTM. Weight\nnormalization was not applied to the bias terms.\nC. Optimization and Learning Rate (LR) schedule\nAs in [3], Adam [47] is utilized for optimization, along with\na learning rate schedule that decays linearly to zero over the\ncourse of training. For a global batch size of 128 a learning\nrate of 5e-4 is used, and is scaled up according to the batch\nsize using either the linear or square root scaling rule.\nD. Evaluation\nTwo metrics for evaluating training are considered:\n1) A bits per character (BPC) metric calculated on the\nimmediate task of predicting the next character given\nthe current character on the Amazon Reviews test set.\nWe calculate the average BPC across 16 random shards\nof the test set by using an evaluation batch size B of\n16. Since our model operates directly on character-level\ntokens, calculation of BPC is simply l·log2 ewhere l is\nthe softmax cross entropy loss averaged over the entire\nsequence.\n2) Accuracy from the downstream tasks of binary sentiment\nclassiﬁcation on the Binary SST, and IMDB Movie\nReview datasets. To perform transfer the model weights\nare taken at the end of Amazon training, frozen, and\nused to featurize text samples from the classiﬁcation\ndataset. A simple binary logistic regression classiﬁer\nfrom scikit-learn [48] is trained to classify these text\nfeatures as having positive or negative sentiment. The\ntransfer process is negligible computationally because\nof the simple model we use on the downstream task.\nVII. A NALYSIS OF MIXED PRECISION VS FP32 TRAINING\nMixed precision training allows for faster computation as\nwell as a 2x increase in effective batch size during training,\nbecause FP16 storage is 2x smaller. In this section we analyze\nperformance gains and convergence for training networks with\nmixed precision arithmetic, comparing it to single precision\ntraining. This allows us to validate the correctness of the re-\nmaining experiments, which are all trained in mixed precision.\nUsing the techniques described in section IV & V, we train\na model on the Amazon Reviews dataset using a full DGX1-V\nnode with 8 GPUs. We initially begin with a batch size of 128\nper GPU, for a global batch size of 1024, and compare the\nrelative speedup granted by mixed precision arithmetic. Next,\nwe quantify the beneﬁts of the reduced memory footprint by\n(a)\n(b)\nType Batch LR Time BPC SST IMDB\nSP 128 5e-4 1 month 1.12 91.9 92.8\nSP 1024 1.2e-3 73.8 hr 1.104 90.8 92.5\nMP 1024 1.2e-3 24.2 hr 1.108 91.5 91.7\nMP 2048 2e-3 17.4 hr 1.117 90.2 91.9\nFig. 2: a) Training curves for mixed precision (MP) and single\nprecision (SP) training b) Test set evaluation comparison of\nsingle precision vs mixed precision training w.r.t. the Amazon\nBPC and binary sentiment classiﬁcation accuracy baselines set\nby Radford et. al [3]\ndoubling the batch size to 256 per GPU (2048 global) in order\nto better saturate the GPU. Additionally, we utilize the softer\nsquare root scaling rule [41] to modify the learning rate as a\nfunction of batch size.\nFigure 2 shows that training in mixed precision and single\nprecision both produce similar training curves and converge\nto similar numbers for both language modeling and transfer\nevaluation. We ﬁnd that moving to mixed precision not only\nachieves similar training results, but it also provides a 3x\nspeedup in training. By taking advantage of the reduced\nmemory footprint of FP16 storage, we increase the batch size\ntwo-fold to 256 per GPU, better saturating the GPU, and\nachieve an additional speedup of 40% on top of our original\nspeedup. This provides approximately a 4.2x speedup when\nswitching from single precision arithmetic to mixed precision.\nOverall, this yields a speed up from one month of training\nas in [3] to 18 hours. We have accomplished this using 8 Tesla\nV100 GPUs, larger batch size, and mixed precision arithmetic.\nVIII. D ISTRIBUTED DATA PARALLEL SCALING\nTo train a language model in hours, not in days, we\nfurther parallelize the training process by using multiple nodes\nand additional data parallelism. We ﬁrst analyze the effect\nof communication overhead on the scalability of multi-GPU\ntraining at various batch sizes and processor counts.\nThe model is trained in mixed precision on 1, 8, 16, 32, 64,\nand 128 GPUs with a local batch size of 256 batches/GPU and\n8 GPUs/DGX1-V node. In Fig. 3 we observe that NCCL2 pro-\nvides near linear scaling with minimal overhead when scaling\nfrom 1 to 8 GPUs within a node. Inﬁniband efﬁciently handles\ninter-node communication for the 4096 neuron mLSTM with\neffectively constant overhead with respect to the number of\nparticipating nodes. This allows for a total speedup of 109x\nwhen scaling to 128 GPUs across 16 DGX1-V Nodes. More\nconcretely, we complete one epoch of training on the Amazon\nreviews dataset in only 1.2 hours.\n(a)\n(b)\nGPUs w/o I.band w/ I.band 8192-d + I.band\ns/iter speed s/iter speed s/iter speed\n1 .81 1x .81 1x 2.01 1x\n8 .85 7.6x .85 7.6x 2.02 7.9x\n16 1.09 14.3x .91 13.6x 2.08 15.5x\n32 1.11 23.4x .91 27.2x 2.05 31.4x\n64 1.13 55.7x .93 55.7x 2.10 61.3x\n128 1.12 92.6x .91 109x 2.13 120.8x\nFig. 3: a) Training time for 1 epoch of Amazon Reviews\nexhibits linear scaling relative to the single GPU case. b)\nAverage per iteration times and relative speedup for distributed\ndata parallel training with (and without) Inﬁniband.\nA. Scaling Large Model Training\nNot every problem calls for training a 4096-d mLSTM.\nSmaller models will train faster and may converge to a good\nenough BPC, while larger models may be necessary for state\nof the art performance. To illustrate this, we train an mLSTM\nwith hidden state sizes of 256, 1024, 4096, and 8192 and a\nglobal training batch size of 2048 split across 1 DGX1-V node\nand learning rate of 2e-3. In the case of the 8192-d hidden state\nmLSTM we use a per GPU batch size of 96 (768 total) due\nto memory constraints. In this experiment, we use a learning\nrate of 7.8e-4 that observes the square root scaling rule. In Fig.\nFig. 4: Training progress over one epoch of Amazon Reviews\nfor mLSTM models at a particular dimension and batch size.\nDashed lines indicate the evaluation BPC after one epoch of\ntraining, with State Of The Art (SOTA) evaluation results set\nby Gray et. al [35].\n4 we can see the beneﬁt of training larger models, with the\n8192-d mLSTM achieving state of the art language modeling\ncomparable to [35], albeit at the cost of additional compute\nand memory.\nWe investigate the scalability of a larger 8192-d mLSTM\nmodel compared to the baseline 4096-d mLSTM model in Fig.\n3. The 8192-d model has 0.72 GB of parameters in FP16,\nwhile the 4096-d model has 0.18 GB of parameters. While\ntraining the 8192-d model on 128 GPUs, we see a speedup\nfactor of 120.8x across 128 GPUs. Even though the larger\nmodel has correspondingly larger gradients, it is also more\ncomputationally intensive, leading to better scaling than the\nbaseline model on the same hardware.\nIX. A NALYSIS OF LARGE BATCH TRAINING\nDistributed data parallel training allows for near linear\nscalability with respect to available GPUs by increasing the\nbatch size. However, as seen already in this work (see section\nVII), training with large batches may run faster than training\nwith small batches, but it may not converge to the same\nvalidation accuracy. Using the same setup as section VIII and\n8, 16, 32, 64, and 128 GPUs we take a look at how the learning\nrate schedule affects convergence.\nA. Learning Rate Scaling\nFormer work in this space trained a 4096-d mLSTM with\nan initial learning rate of 5e-4, batches of 128, and a linear\nlearning rate that decays to zero over one epoch of training\n[3]. As expected and shown in Fig. 5, keeping this same\nlearning rate schedule as we increase batch size leads to worse\naccuracy, or a higher BPC 2.\n2We train our models to convergence, not for one epoch, but results over\none epoch are representative, and easier to compare.\nRecent work scaling image CNN models with SGD suggest\nthat learning rates could be scaled linearly as batch size\nincreases without a noticeable loss in model accuracy [39].\nHowever, we found that our mLSTM model, optimized with\nAdam, diverged for large batches when we scaled up the 5e-4\ninitial learning rate with either a linear or a square root rule,\nas we increased the batch size.\nWe observed that for batch sizes of 2k-8k, the model\nconverged reasonably well with an initial learning rate of 3e-3\ndecayed to zero over one epoch. Thus we kept 3e-3 as the\ninitial learning rate for all other experiments.\nBatch Iters Rule LR BPC SST IMDB\n2048 72.6k\nlinear 8e-3 1.280 79.4 77.6\nsqrt 2e-3 1.117 90.2 91.9\n- 5e-4 1.130 89.1 90.8\n- 3e-3 1.110 89.0 92.1\n4096 37.3k\nlinear 1.6e-2 1.275 78.3 77.6\nsqrt 2.8e-3 1.122 89.6 91.0\n- 5e-4 1.146 89.3 90.9\n- 3e-3 1.119 89.2 91.8\n8192 18.6k\nlinear 3.2e-2 1.476 65.4 67.3\nsqrt 4e-3 1.133 89.7 90.8\n- 5e-4 1.175 87.3 89.6\n- 3e-3 1.132 89.5 91.4\n16384 9.3k\nlinear 6.4e-2 Div - -\nsqrt 5.8e-3 Div - -\n- 5e-4 1.254 85.1 86.4\n- 3e-3 1.162 89.0 90.1\n32768 4.6k\nlinear 1.3e-1 Div - -\nsqrt 8e-3 Div - -\n- 5e-4 1.380 75.2 74.8\n- 3e-3 1.218 87.1 87.9\nFig. 5: Evaluation results for various initial learning rates with\na schedule decaying to zero over 1 epoch. Some initial rates\nare set by a linear or square root scaling rule based on a 5e-4\nrate for a batch of 128. Div indicates that training diverged.\nB. Learning Rate Schedule\nWhen training to convergence, we used the same learning\nrate schedule for all batch sizes:\n• Set an initial learning rate of 3e-3.\n• Linearly decay learning rate to zero over 100,000 itera-\ntions.\n• Stop training at 3 epochs over the dataset, if fewer than\n100,000 iterations.\nThis schedule, constant across all batch sizes, avoided the\ndivergence observed in Fig. 5, from scaling learning rate too\nmuch, but it also performed better than if we had instead kept\nthe initial 5e-4 learning rate constant across batch sizes.\nUsing this learning rate schedule for the model with dif-\nferent batch sizes, Fig. 6 shows that large batch training\nfor this problem can converge to a similar evaluation BPC\n(a)\n(b)\nBatch GPU Iters Ep hrs BPC SST IMDB\n2048 8 100k 1.4 23.7 1.102 90.6 92.1\n4096 16 100k 2.7 25.3 1.090 90.6 92.7\n8192 32 55k 3.0 14.0 1.104 91.2 92.3\n16384 64 28k 3.0 7.1 1.116 90.3 92.3\n32768 128 14k 3.0 3.5 1.132 90.1 90.4\nFig. 6: a) Training progress as a function of time for a 3e-\n3 initial learning rate decaying to zero over 100k iterations.\nVarious batch sizes are trained with distributed data parallelism\nwith a batch size of 256 per GPU. b) Comparison of model\nconvergence, hardware used, time taken, and iterations and\nepochs (Ep) trained for a particular batch size. Batch sizes that\nhave not reached 100k iterations after 3 epochs did not fully\ndecay their learning rate and may beneﬁt from more training.\nas smaller batch training given a good training schedule.\nHowever, adjusting the learning rate schedule is not as simple\nas modifying the learning rate according to batch size. In our\nexperiments we found that controlling the steepness of decay\nwas also required.\nX. D ISCUSSION\nWe were able to converge our model in mixed precision, to\na similar value as the FP32 baseline. This speeds up training,\nand substantially reduces our memory footprint, without a\nmeasurable change in accuracy, as shown in Fig. 2. We\nfurther speed up training by saturating up to 128 GPUs with\ndistributed data parallelism, which we can do with a near-\nlinear scaling factor Fig. 3.\nHowever, as batch size increases from 128 in [3] to 32k, the\nmodel needs more training steps to converge, and it does not\nconverge to to quite as good a validation BPC as the low-batch\nmodel Fig. 6.\nWith longer training: 3 epochs of the Amazon Reviews\ndataset rather than 1, we do converge the 32k batch model\nclose to the small batch model, doing so in a few hours instead\nof days or weeks. We also show that downstream task transfer\nto sentiment extraction is comparable when using converged\nlarge batch models (Fig. 6b).\nSmith et. al [39] suggest that to scale the learning rate\nwithout a loss in generalization quality, given a batch size\nB and total amount of data N, B must be sufﬁciently large\nso that N ≫ B. It is possible that a batch size ≥ 32k\nand the amount of available Amazon data do not satisfy this\nrequirement since the Amazon Reviews dataset is reduced to\nfewer than 5000 iterations when we scale up to a 32k batch\nsize. This observation opens up new research questions for\nfuture work.\nXI. F UTURE WORK\nWe have shown that distributed data parallelism scales for\nlarge RNN text models. However, we start to see diminish-\ning returns on wall time convergence at very large batches,\npossibly because each epoch is reduced to a small number of\ntraining iterations. Now that we can train a language model on\nthe 40 GB Amazon reviews dataset in hours, a next step could\nbe to train on larger text datasets. Orders of magnitude larger\ntext datasets could be constructed by collecting web pages,\nnews articles, Reddit comments, and tweets, for example.\nIn addition to larger text datasets, we could further improve\nAmazon Reviews BPC (and presumably accuracy on transfer\ntasks) with some of the following:\n• Training for more than 3 epochs.\n• Data shufﬂing between epochs.\n• Larger RNN models, with more layers and larger hidden\nstates.\n• Alternative language models, such as the Transformer\nnetwork [49].\n• Hyper-parameter search for an ideal large batch learning\nrate schedule.\nAs shown in Fig. 6b, our best large batch training runs did\nnot decay the learning rate to zero by the end of 3 epochs.\nAs long as the initial learning rate is low enough not to cause\ntraining divergence, it may be possible to keep the learning\nrate high through several epochs of training. Recent language\nmodeling work with the Transformer network has shown that\ntriangular learning rate warmup and non-linear learning rate\ndecay (cosine annealing) can lead to a better learning rate\nschedule with the Adam optimizer [13]. We showed that a\nsimple learning rate schedule can work for large batch training,\nbut further work on learning rate schedules will likely improve\nconvergence.\nIncreasing the mLSTM size from 4096 to 8192-d reduces\nthe per-GPU batch size by a factor of four. Using gradient\ncheckpointing [50] would allow training larger models with\nlarger batches without being constrained by memory capacity.\nIn order to get maximal text understanding from these larger\nmodels, we could modify the unsupervised task to include\nadditional objectives, along with language modeling. Auxiliary\ntasks may include predicting a review’s star rating, the title or\ntopic of a piece of text, or any other freely available structural\ntext label. Since the purpose of unsupervised training is to\nbuild a model with deep conceptual understanding of the text,\nauxiliary tasks that leverage metadata available with the text\ncould provide additional understanding.\nXII. C ONCLUSION\nWe set out to investigate large scale training for recurrent\nmodels in the Natural Language domain. With mixed precision\ntraining we can successfully converge a model 4.2x faster with\ndouble the batch size compared to FP32 training. By lever-\naging distributed deep learning with NCCL2, NVLINK, and\nInﬁniband interconnect, we achieve near linear scaling of 109x\nwith 128 GPUs, as we grow the batch size proportionately to\nthe number of available machines.\nIn addition to pushing wall time scalability by decreasing\nthe time needed to converge a language model on the Amazon\nReviews dataset, we analyze the convergence of models trained\nwith large batches. We ﬁnd that training with very large\nbatches leads to somewhat worse generalization, requiring\nmore data to converge to a similar validation BPC and transfer\naccuracy as small batch training. Learning rate schedule modi-\nﬁcations are necessary to help with convergence. Without such\ntechniques evaluation quality begins to decline as batch size\nincreases, or the model fails to converge if the learning rate is\nscaled too high.\nWith further modiﬁcation to the learning rate schedule and\nadditional training it is possible to train models with large\nbatches comparable to models trained with smaller batches.\nOur experiments lead to two insights:\n• The relationship between batch size and learning regime\nis complex and learning rate scaling alone is not always\nenough to converge a model.\n• Even with the largest public text corpus available, it\nmay not be feasible to satisfy the B ≪N batch size\nrequirement needed to effectively train with the largest\nbatches that modern hardware allows.\nWe look forward to more work investigating large scale\nlanguage model training and using it in transferred tasks to\nsolve difﬁcult natural language problems.\nREFERENCES\n[1] T. Akiba, S. Suzuki, and K. Fukuda, “Extremely large minibatch\nSGD: training resnet-50 on imagenet in 15 minutes,” CoRR, vol.\nabs/1711.04325, 2017.\n[2] S. Kornblith, J. Shlens, and Q. V . Le, “Do better imagenet models\ntransfer better?” 2018.\n[3] A. Radford, R. J ´ozefowicz, and I. Sutskever, “Learning to generate\nreviews and discovering sentiment,” CoRR, vol. abs/1704.01444, 2017.\n[4] B. Krause, L. Lu, I. Murray, and S. Renals, “Multiplicative LSTM for\nsequence modelling,” CoRR, vol. abs/1609.07959, 2016.\n[5] J. McAuley, C. Targett, Q. Shi, and A. van den Hengel, “Image-based\nrecommendations on styles and substitutes,” SIGIR, 2015.\n[6] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” CoRR, vol. abs/1409.1556, 2014.\n[7] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” CoRR, vol. abs/1512.03385, 2015.\n[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” 2009.\n[9] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and\nF. Li, “Imagenet large scale visual recognition challenge,” CoRR, vol.\nabs/1409.0575, 2014.\n[10] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and\nT. Darrell, “Decaf: A deep convolutional activation feature for generic\nvisual recognition,” International Conference on Machine Learning ,\n2013.\n[11] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, “CNN features\noff-the-shelf: an astounding baseline for recognition,” IEEE Conference\non Computer Vision and Pattern Recognition Workshops (CVPRW) ,\n2014.\n[12] K. He, G. Gkioxari, P. Doll ´ar, and R. B. Girshick, “Mask R-CNN,”IEEE\nInternational Conference on Computer Vision (ICCV) , 2017.\n[13] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\nlanguage understanding by generative pre-training,” 2018. [Online].\nAvailable: https://blog.openai.com/language-unsupervised/\n[14] J. Turian, L. Ratinov, and Y . Bengio, “Word representations: A simple\nand general method for semi-supervised learning,” in Proceedings of the\n48th Annual Meeting of the Association for Computational Linguistics ,\nser. ACL ’10. Stroudsburg, PA, USA: Association for Computational\nLinguistics, 2010, pp. 384–394.\n[15] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed\nrepresentations of words and phrases and their compositionality,” CoRR,\nvol. abs/1310.4546, 2013.\n[16] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global\nvectors for word representation,” 2014. [Online]. Available: https:\n//www.aclweb.org/anthology/D14-1162\n[17] P. Goyal, P. Doll ´ar, R. B. Girshick, P. Noordhuis, L. Wesolowski,\nA. Kyrola, A. Tulloch, Y . Jia, and K. He, “Accurate, large minibatch\nSGD: training imagenet in 1 hour,” CoRR, vol. abs/1706.02677, 2017.\n[18] Y . You, Z. Zhang, C. Hsieh, and J. Demmel, “100-epoch imagenet\ntraining with alexnet in 24 minutes,” CoRR, vol. abs/1709.05011, 2017.\n[19] Y . You, I. Gitman, and B. Ginsburg, “Scaling SGD batch size to 32k\nfor imagenet training,” CoRR, vol. abs/1708.03888, 2017.\n[20] M. Ott, S. Edunov, D. Grangier, and M. Auli, “Scaling neural machine\ntranslation,” 2018.\n[21] J. Hestness, S. Narang, N. Ardalani, G. F. Diamos, H. Jun, H. Kianinejad,\nM. M. A. Patwary, Y . Yang, and Y . Zhou, “Deep learning scaling is\npredictable, empirically,” CoRR, vol. abs/1712.00409, 2017.\n[22] A. M. Dai and Q. V . Le, “Semi-supervised sequence learning,” CoRR,\nvol. abs/1511.01432, 2015.\n[23] O. Melamud, J. Goldberger, and I. Dagan, “context2vec: Learning\ngeneric context embedding with bidirectional lstm,” in Proceedings\nof The 20th SIGNLL Conference on Computational Natural Language\nLearning, 01 2016, pp. 51–61.\n[24] B. McCann, J. Bradbury, C. Xiong, and R. Socher, “Learned in transla-\ntion: Contextualized word vectors,” CoRR, vol. abs/1708.00107, 2017.\n[25] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, “Deep contextualized word representations,” CoRR,\nvol. abs/1802.05365, 2018.\n[26] J. Howard and S. Ruder, “Fine-tuned language models for text classiﬁ-\ncation,” CoRR, vol. abs/1801.06146, 2018.\n[27] S. Subramanian, A. Trischler, Y . Bengio, and C. J. Pal, “Learning general\npurpose distributed sentence representations via large scale multi-task\nlearning,” CoRR, vol. abs/1804.00079, 2018.\n[28] D. Cer, Y . Yang, S. Kong, N. Hua, N. Limtiaco, R. S. John, N. Constant,\nM. Guajardo-Cespedes, S. Yuan, C. Tar, Y . Sung, B. Strope, and\nR. Kurzweil, “Universal sentence encoder,” CoRR, vol. abs/1803.11175,\n2018.\n[29] P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, and\nN. Shazeer, “Generating wikipedia by summarizing long sequences,”\nICLR, 2018.\n[30] R. Kiros, Y . Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba, R. Urtasun,\nand S. Fidler, “Skip-thought vectors,” CoRR, vol. abs/1506.06726, 2015.\n[31] P. Ramachandran, P. J. Liu, and Q. V . Le, “Unsupervised pretraining for\nsequence to sequence learning,” CoRR, vol. abs/1611.02683, 2016.\n[32] Y . Zhu, R. Kiros, R. S. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,\nand S. Fidler, “Aligning books and movies: Towards story-like visual\nexplanations by watching movies and reading books,” CoRR, vol.\nabs/1506.06724, 2015.\n[33] R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda, “English\ngigaword ﬁfth edition,” 2011. [Online]. Available: https://catalog.ldc.\nupenn.edu/ldc2011t07\n[34] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, and P. Koehn,\n“One billion word benchmark for measuring progress in statistical\nlanguage modeling,” CoRR, vol. abs/1312.3005, 2013.\n[35] S. Gray, A. Radford, and D. P. Kingma, “Gpu kernels for block-\nsparse weights,” 2017. [Online]. Available: https://blog.openai.com/\nblock-sparse-gpu-kernels/\n[36] Y . A. LeCun, L. Bottou, G. B. Orr, and K.-R. M ¨uller, Efﬁcient\nBackProp. Berlin, Heidelberg: Springer Berlin Heidelberg, 2012, pp.\n9–48. [Online]. Available: https://doi.org/10.1007/978-3-642-35289-8 3\n[37] R. Pascanu, T. Mikolov, and Y . Bengio, “On the difﬁculty of training\nrecurrent neural networks,” CoRR, vol. abs/1211.5063, 2012.\n[38] A. Karpathy, J. Johnson, and F. Li, “Visualizing and understanding\nrecurrent networks,” CoRR, vol. abs/1506.02078, 2015.\n[39] S. L. Smith and Q. V . Le, “A bayesian perspective on generalization and\nstochastic gradient descent,” CoRR, vol. abs/1710.06451, 2017.\n[40] S. L. Smith, P. Kindermans, and Q. V . Le, “Don’t decay the learning\nrate, increase the batch size,” CoRR, vol. abs/1711.00489, 2017.\n[41] E. Hoffer, I. Hubara, and D. Soudry, “Train longer, generalize better:\nclosing the generalization gap in large batch training of neural networks,”\n2017.\n[42] P. Micikevicius, S. Narang, J. Alben, G. F. Diamos, E. Elsen, D. Garcia,\nB. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H. Wu,\n“Mixed precision training,” CoRR, vol. abs/1710.03740, 2017.\n[43] NVIDIA. (2018) Mixed precision training: Choosing a scaling\nfactor. [Online]. Available: https://docs.nvidia.com/deeplearning/sdk/\nmixed-precision-training/index.html#scalefactor\n[44] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,\nA. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in\npytorch,” 2017.\n[45] I. Sutskever, “Training recurrent neural networks,” 2013. [Online].\nAvailable: https://www.cs.utoronto.ca/ ∼ilya/pubs/ilya sutskever phd\nthesis.pdf\n[46] T. Salimans and D. P. Kingma, “Weight normalization: A simple repa-\nrameterization to accelerate training of deep neural networks,” CoRR,\nvol. abs/1602.07868, 2016.\n[47] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980 , 2014.\n[48] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vander-\nplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay, “Scikit-learn: Machine learning in Python,” Journal of Machine\nLearning Research, vol. 12, pp. 2825–2830, 2011.\n[49] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” CoRR, vol.\nabs/1706.03762, 2017.\n[50] T. Chen, B. Xu, C. Zhang, and C. Guestrin, “Training deep nets with\nsublinear memory cost,” CoRR, vol. abs/1604.06174, 2016.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7982926368713379
    },
    {
      "name": "Scalability",
      "score": 0.6536903381347656
    },
    {
      "name": "Artificial intelligence",
      "score": 0.54696124792099
    },
    {
      "name": "Multiplicative function",
      "score": 0.5059375166893005
    },
    {
      "name": "Scaling",
      "score": 0.4972520172595978
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4749610424041748
    },
    {
      "name": "Deep learning",
      "score": 0.46681568026542664
    },
    {
      "name": "Transfer of learning",
      "score": 0.45653659105300903
    },
    {
      "name": "Function (biology)",
      "score": 0.43430548906326294
    },
    {
      "name": "Machine learning",
      "score": 0.4152795076370239
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4141894578933716
    },
    {
      "name": "Artificial neural network",
      "score": 0.3582504391670227
    },
    {
      "name": "Mathematics",
      "score": 0.09810507297515869
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1304085615",
      "name": "Nvidia (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 15
}