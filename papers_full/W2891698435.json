{
  "title": "How to represent a word and predict it, too: Improving tied architectures for language modelling",
  "url": "https://openalex.org/W2891698435",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2250848545",
      "name": "Kristina Gulordava",
      "affiliations": [
        "Pompeu Fabra University"
      ]
    },
    {
      "id": "https://openalex.org/A2803120775",
      "name": "Laura Aina",
      "affiliations": [
        "Pompeu Fabra University"
      ]
    },
    {
      "id": "https://openalex.org/A142903237",
      "name": "Gemma Boleda",
      "affiliations": [
        "Pompeu Fabra University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W581956982",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W2112184938"
  ],
  "abstract": "Recent state-of-the-art neural language models share the representations of words given by the input and output mappings. We propose a simple modification to these architectures that decouples the hidden state from the word embedding prediction. Our architecture leads to comparable or better results compared to previous tied models and models without tying, with a much smaller number of parameters. We also extend our proposal to word2vec models, showing that tying is appropriate for general word prediction tasks.",
  "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2936–2941\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n2936\nHow to represent a word and predict it, too:\nImproving tied architectures for language modelling\nKristina Gulordava Laura Aina Gemma Boleda\nUniversitat Pompeu Fabra\nBarcelona, Spain\n{firstname.lastname}@upf.edu\nAbstract\nRecent state-of-the-art neural language mod-\nels share the representations of words given by\nthe input and output mappings. We propose a\nsimple modiﬁcation to these architectures that\ndecouples the hidden state from the word em-\nbedding prediction. Our architecture leads to\ncomparable or better results compared to pre-\nvious tied models and models without tying,\nwith a much smaller number of parameters.\nWe also extend our proposal to word2vec mod-\nels, showing that tying is appropriate for gen-\neral word prediction tasks.\n1 Introduction\nIn neural models, reusing representations of the\nsame type of data (e.g., sentences or words) in dif-\nferent parts of the architecture can be a powerful\nway to aid learning: it reduces parameters, en-\nabling more compact models and faster learning.\nRecurrent neural network (RNN) language models\n(Mikolov et al., 2010; Sundermeyer et al., 2012;\nZaremba et al., 2014) have two word mappings:\nFrom the input word onto its embedding repre-\nsentation, and from the internal representation of\nthe network (the hidden layer) to the weights for\nthe prediction of the next word. In standard mod-\nels, these representations are different. Recently,\nInan et al. (2017) and Press and Wolf (2017) pro-\nposed to instead use a single word representation,\ntying the input and output mappings. Intuitively,\nboth are representations of the same type of data\n(words), and information learnt when observing a\nword as input can be reused when predicting this\nword as output. Tied language models obtained\nbetter perplexity and better word similarity scores\nof embedding matrices while reducing the number\nof parameters. The models that achieve the latest\nstate-of-the art results incorporate this technique\n(see, e.g., Merity et al., 2018).\nHowever, note that, by tying the output map-\nping to the input mapping, the hidden layer of the\nnetwork is optimised to match the representation\nof the predicted word. We suggest that this intro-\nduces a constraint that conﬂicts with the function\nof the hidden layer in language models to repre-\nsent the previous context and transmit information\nto the next timestep. In this paper, we propose a\nminimal modiﬁcation to tied LM architectures to\naddress this issue: we add a linear transformation\nbetween the hidden layer and the word embedding\nprediction, partially decoupling the two. This has\nan important advantage. Standard tied architec-\ntures require the hidden layer to have the same\ndimensionality as the word embeddings. We lift\nthis constraint in our architecture: by separating\nthe hidden layer and the word mapping, we can\nchoose a large hidden layer dimensionality while\nkeeping the embedding dimensionality and, con-\nsequently, the size of the embedding matrix small.\nIn a set of experiments on LM, we show that\nour tied models achieve results similar to or better\nthan models with standard or no tying, with much\nsmaller embedding sizes and a reduction of 30-\n60% in the overall number of parameters. Notably,\nthe word embeddings obtained with the modiﬁed\nmodel have a higher quality than double-sized em-\nbeddings obtained with standard tied models, as\nmeasured on word similarity.\nWe further extend this idea to word represen-\ntation learning models (in particular, word2vec),\nwhich have a similar architecture and objective\nfunction to language models. For instance, the\nstandard skipgram model (Mikolov et al., 2013)\nhas two mappings, one for the context words and\none for the target word. While tying these two ma-\ntrices directly constraints learning too strongly, an\nadditional linear mapping adds the sufﬁcient ca-\npacity to learn embeddings of the same quality as\nthe standard model using only half the parameters.\n2937\n2 Tied LM architectures\n2.1 Previous work\nEquations (1) through (4) deﬁne a standard RNN\nlanguage model (Mikolov et al., 2010):\nˆ xt = xtE (1)\nht = LSTM(ˆ xt,ht−1) (2)\no = htWT (3)\np = softmax(o), (4)\nwhere xt is the one-hot encoding of the input word\nat time t, ˆ xt is its embedded representation and E\nis the embedding matrix (input mapping). We fol-\nlow the majority of previous work in adopting an\nLSTM (Hochreiter and Schmidhuber, 1997) as the\nrecurrent unit, as it was shown to outperform other\nrecurrent architectures for LM (Jozefowicz et al.,\n2015). The hidden vector ht is used as input to the\nLSTM for the next time step and also as input to a\nlinear transformation W (output mapping) which\nproduces the output weights. These weights are\nnormalised into probability scores using the soft-\nmax function.\nThe tied models proposed by Inan et al. (2017)\nand Press and Wolf (2017) set W to be equal to E\n(Figure 1b). Note that since Eis of size |V|×m,\nwhere mis the embedding size, and W is of size\n|V|×n, where nis the hidden state size, the hidden\nand embedding dimensions must be equal, that is,\nm= n.\nPress and Wolf (2017) observe that the output\nmatrix W represents an embedding matrix since\ntwo similar words with indices i and j are learnt\nto receive similar probabilities given a context and\nhence the rows Wi and Wj should also be similar.\nIndeed, they show that on some word similarity\nevaluation tasks the output matrix W outperforms\nsigniﬁcantly the input matrix E. Tying E and W\nmakes the model share the representations for the\ninput and output vocabularies.\nInan et al. (2017) suggest a theoretical moti-\nvation for the tying technique and derive it as an\ninstance of a more general approach of augment-\ning the cross-entropy loss. They show that a loss\nthat takes into account not only the target word\n(i.e., log pi for cross-entropy) but the scores for all\nwords in the vocabulary according to their similar-\nity to the target (computed as dot-product of em-\nbeddings in E) improves performance on LM.\n(a) Non-tied model\n(b) Tied model\n(c) Tied+L model\nFigure 1: Effect of the three architectures on map-\nping sizes. Note that the actual difference between\nvocabulary size |V|and n, mis of 2 to 3 orders of\nmagnitude.\nOne important practical advantage of tying in-\nput and output matrices is the reduction in num-\nber of parameters with respect to a standard model\nwith the same hidden and embedding dimensions,\nsince instead of two matrices E and W of size\n|V|×mwe have only one matrix of size|V|×m.\n2.2 Proposed modiﬁcation\nOne potential problem with the tied model, where\nE = W, is that the hidden state ht is optimised\nto be close to the embedding of the target word.\nTo see this, consider that oj = ej ·ht,∀j. The\ncross-entropy objective is to maximise log pi for\nthe target word with index i and consequently to\nminimise log pj∀j ̸= i. Hence, oi = ei ·ht will\nbe increased by the gradient descent update andht\nwill be aligned closer with ei (for each dimension\nk of the two vectors, htk ·eik will be increased).\nThis association between ht and word embedding\nspace could prevent efﬁcient retention of LSTM\nhistory in ht, which is used as input for the fol-\nlowing time step.\nTo address this issue, we propose a simple mod-\niﬁcation to the standard tied model, replacing the\n2938\noutput transformation (3) as follows:\nˆht = htL\no = ˆhtET .\n(5)\nL is an additional linear transformation that de-\ncouples the hidden state ht, which is passed to the\nnext time step and represents the previous, possi-\nbly long-term, linguistic context, from ˆht, which\nis instead optimised to match the embedding of the\noutput word.\nAs Figure 1 illustrates, an important advantage\nof the additional transformation Lis that a model\ncan have different dimensions for the hidden vec-\ntor (n) and the embedding vectors ( m). The em-\nbedding matrix is the largest part of the model\nwhen the vocabulary is large ( |V| ≫n). Re-\nducing the size of the embedding m leads to a\nsigniﬁcant reduction in the number of parameters,\nproportional to |V|, and the acceleration of soft-\nmax computation. On the other hand, the size of\nthe additional matrix L is only n×m and con-\ntributes very little to the overall size of the model.\nWe test empirically how reducing the embedding\nsize affects the performance of language models\nby varying hidden and embedding sizes in our ex-\nperiments and evaluating embedding matrices on\nword similarity tasks.\nStandardly used LM models often have two lay-\ners of LSTM cells. Thus, the issue we identifed\nmight be mitigated in practice, since the hidden\nstate of the ﬁrst layer is not directly affected by ty-\ning the output and input transformation matrices.\nMoreover, an LSTM cell carries over information\nboth through the hidden state and a memory state;\nthe latter is affected by tying only indirectly (see\nHochreiter and Schmidhuber (1997) for details on\nLSTM architectures). However, our experiments\nshow that, in practice, two-layer LSTM LMs are\nstill affected by tying despite these caveats.\n2.3 Extending the tied technique to word2vec\nThe tied technique, as formulated above, can be in\nprinciple applied to any model which has the same\ngeneral objective of LM: predicting a target word\ngiven context words. The CBOW word2vec model\nfor word representation learning (Mikolov et al.,\n2013) is the primary candidate for testing the ap-\nplicability of the tied technique beyond LM, since\nwe can see it as substituting the LSTM function\n(2) with a simple sum of context word embeddings\nh = ∑\ni ˆ xi (where xi are words in the context win-\ndow, e.g., of size 5). Similarly then, the equations\nin (5) describe the tied version of CBOW model.\nThe linear step here provides the capacity to learn\na transformation from the sum of embeddings to\nthe predicted embedding ˆh. Without such trans-\nformation, the tying model would assume that the\nsum function is always a good approximation of\nthe output embedding.\nThe skipgram word2vec model (Mikolov et al.,\n2013) employs a variant of the LM objective: It\nis trained to predict context words given a word,\ninstead of the opposite. As in CBOW and neural\nlanguage models, words are both inputs and tar-\ngets, making the use of tying an option also for\nthis architecture. Press and Wolf (2017) apply di-\nrect tying to this architecture and report that the\nquality of the obtained embeddings is below the\nquality of non-tied skipgram embeddings. Unlike\nCBOW or LSTM, the input-to-hidden state func-\ntion of the skipgram model is identity, reducing\nthe tying model objective toˆ xi = ˆ xj for every pair\nof input-output words i,j. It is thus not surprising\nthat enforcing the tying constraint leads to poor\nempirical results. We test whether adding an addi-\ntional linear transformation improves performance\nof the tied technique also for a skipgram model.\n3 Experiments and results\n3.1 Evaluation data\nWe use two corpora for the evaluation of lan-\nguage models. First, we employ a medium-\nsized corpus of approximately 100M tokens with\na relatively large vocabulary, 50K words, created\nfrom a Wikipedia dump (henceforth, Wiki). 1 To\nallow comparison with previous work, we also\nevaluate on the Penn Treebank (PTB), which is\nsmall but has been used as a benchmark for LM\nsince Mikolov et al. (2011). The PTB has approx-\nimately 1M tokens and is preprocessed to have\n10K vocabulary words; we use the standard train-\nvalidation-test split.\nFurthermore, to evaluate the quality of the em-\nbeddings induced by the language models, as well\nas for the word representation experiments in Sec-\ntion 3.4, we use three standard word similarity\ndatasets: SimLex-999 (Hill et al., 2015; SimLex),\nMEN (Bruni et al., 2014), and RareWords (Lu-\nong et al., 2013; RW). The performance on these\ndatasets is evaluated in terms of Spearman corre-\n1https://dumps.wikimedia.org/enwiki/\n20180301/\n2939\nHid Emb Model Valid Test ∆ Size\n200 200 non-tied 95.0 91.1 4.7M\ntied 90.8 86.6 -4.5 2.7M\ntied+L 89.8 85.8 -5.3 2.7M\n400 200 non-tied 89.4 85.3 8.3M\ntied+L 83.4 80.3 -5.0 4.3M\n400 non-tied 87.2 83.5 10.6M\ntied 82.0 78.2 -5.3 6.6M\ntied+L 81.9 78.0 -5.5 6.7M\n600 400 non-tied 85.8 82.4 15.3M\ntied+L 79.0 76.0 -6.4 9.5M\n600 non-tied 84.3 81.3 17.8M\ntied 79.7 76.1 -5.2 11.8M\ntied+L 78.7 75.5 -5.8 12.1M\nInan2017 VD tied 650 77.1 73.9 -\nZaremba2014 1500 82.2 78.4 66M\nP&W2016 tied 1500 77.7 74.3 51M\nTable 1: LM perplexity results on PTB. ∆: differ-\nence in test perplexity of the tied models with re-\nspect to the non-tied model with the same number\nof hidden units.\nlation between the cosine similarity of word pairs\nand human judgments.2\n3.2 Training setup\nAs our base language models, we adopt the ones\nproposed in Zaremba et al. (2014). We use 2-layer\nLSTMs with dropout applied to the input embed-\nding, to the output of the ﬁrst LSTM layer and\nto the output of the second layer. We used the\nPyTorch implementation3 and modiﬁed it to in-\nclude the additional linear layer for our tied mod-\nels. We report the best model after the hyperpa-\nrameter search for dropout and learning rate (see\nthe details in Appendix A).\n3.3 Language modelling results\nWe present the LM results for the standard non-\ntied model, the tied model as in Inan et al. (2017)\nand Press and Wolf (2017), and our tied model\nwith an additional linear transformation ( tied+L)\nin Tables 1 (PTB) and 2 (Wiki).\n2We computed correlation on the word pairs covered by\nthe Wiki corpus, namely 98%, 88% and 31% (973, 2648 and\n623 datapoints) for SimLex, MEN, and RW, respectively.\n3https://github.com/pytorch/examples/\ntree/master/word_language_model\nTable 1 conﬁrms that tying generally brings\ngains with respect to not tying. This is also true\nfor the cases when the hidden and embedding sizes\nare different (e.g. 400/200 and 600/400), where\nour tied+L model outperforms the non-tied model\nby 5 to 6.4 points having around 40% less param-\neters. Furthermore, our decoupled model slightly\nbut consistently improves results with respect to\nstandard tying, conﬁrming our intuition that the\ncoupling of the hidden state to the embedding rep-\nresentation is a limiting constraint. Smaller tied+L\nmodels perform well compared to larger tied mod-\nels. In particular, the tied+L model with 600/400\nunits has perplexity of 76.0, compared to 76.1 of\nthe tied 600/600 model, with 55% the number of\nparametres. Note that our results are comparable\nto previously reported perplexity values on PTB\nfor similar models. Our best results of 75.5 test\nperplexity is only 1.2 points behind the large tied\nmodel with 1500 units reported in Press and Wolf\n(2017) and is only 1.6 points behind the medium\ntied model with 650 units and variational dropout\n(Gal and Ghahramani, 2016) reported in Inan et al.\n(2017).\nOn the Wiki corpus with larger vocabulary (Ta-\nble 2), we ﬁnd that tied models achieve slightly\nlower perplexity than non-tied models with half\nthe number of parameters, and our proposed\ntied+L model achieves lower perplexity than the\ntied model. The most relevant result of the present\nexperiment, however, is that the tied+L model\nwith 300 embedding units is actually better than\nthe tied model with 600 units (38.5 vs 39.7 points;\nthe tied+L model has 20M parameters compared\nto 36M of the tied model) – that is, a smaller\nmodel outperforms a larger model. Thus, our\ndecoupling mechanism not only allows models\nto have better perplexity, but also more compact\nword embeddings, which are of a higher quality\nalso as measured on word similarity: .42/.61/.68\nfor the tied+L embeddings of size 300, compared\nto .39/.55/.64 for the tied embeddings of size 600.\n3.4 Experiments on word2vec models\nTable 3 presents the evaluation of word2vec mod-\nels on the three word similarity datasets. We ran\nthe experiments only on the Wiki corpus due to its\nhigher coverage (50K vocabulary), and used em-\nbeddings of size 300.\nOur results on CBOW show that the tied+L ar-\nchitecture obtains comparable results to the non-\n2940\nPerplexity Spearman’s ρ\nHidden Emb Model Size Valid Test SimLex RW MEN\n600 300 non-tied 50M 40.0 39.2 .33/.34 .52/.50 .55/.60\ntied+L 20M 39.3 38.5 .42 .61 .68\n600 non-tied 66M 40.8 40.0 .33/.34 .52/.50 .55/.60\ntied 36M 40.5 39.7 .39 .55 .64\ntied+L 36M 38.6 37.9 .42 .59 .70\nTable 2: Results on Wiki on LM (perplexity; lower is better) and word similarity (Spearman’s ρ; for\nnon-tied models, results for input and output matrix are reported).\nModel Type SimLex RW MEN\nCBOW non-tied .38 .51 .63\ntied+L .38 .50 .65\nskipgram non-tied .39 .52 .74\ntied .18 .25 .50\ntied+L .35 .51 .72\nTable 3: Results (ρ) for word2vec models.\ntied architecture with almost half the parameters\n(15.1M vs 30M). This conﬁrms that tying with an\nadditional linear transformation is appropriate not\nonly for language models but for word learning\nmodels more generally.\nThe skipgram algorithm shows a small degra-\ndation of performance for the tied+L architecture\nwith respect to the non-tied one; note that, as ex-\nplained in Section 2.3, tying makes the most sense\nfor CBOW. However, the fact that standard tying\nobtains much worse results (similarly to the re-\nsults of Press and Wolf, 2017) shows that the linear\nmapping substantially relaxes the tying constraint.\n4 Conclusions\nOverall, our simple modiﬁcation to tied language\nmodelling architectures generalises previous work\nby allowing tying without imposing constraints\non the number of hidden and embedding dimen-\nsions. This leads to ﬂexible architectures with a\nmore efﬁcient use of both hidden states and em-\nbeddings. For word representation learning mod-\nels, having an additional linear transformation re-\nduces the number of parameters while maintain-\ning learning capacity. In general, reducing model\nsize without harming performance is a desirable\nfeature in practice, for example in the case of lan-\nguage models running on mobile devices, and it is\nalso desirable on theoretical grounds, since it is a\nbetter use of the learning capacity of neural net-\nworks.\nAcknowledgements\nWe thank Germ ´an Kruszewski and the AMORE\nteam for the helpful discussions. This project\nhas received funding from the European Research\nCouncil (ERC) under the European Union’s Hori-\nzon 2020 research and innovation programme\n(grant agreement No 715154), and from the\nRam´on y Cajal programme (grant RYC-2015-\n18907) and the Catalan government (SGR 2017\n1575). We gratefully acknowledge the support of\nNVIDIA Corporation with the donation of GPUs\nused for this research. This paper reﬂects the au-\nthors’ view only, and the EU is not responsible for\nany use that may be made of the information it\ncontains.\nReferences\nElia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.\nMultimodal distributional semantics. Journal of Ar-\ntiﬁcial Intelligence Research, 49:1–47.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Advances in neural information\nprocessing systems, pages 1019–1027.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015.\nSimlex-999: Evaluating Semantic Models with Gen-\nuine Similarity Estimation. Comput. Linguist. ,\n41(4):665–695.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9 8:1735–\n80.\n2941\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. In ICLR’17.\narXiv preprint arXiv:1611.01462.\nRafal Jozefowicz, Wojciech Zaremba, and Ilya\nSutskever. 2015. An empirical exploration of re-\ncurrent network architectures. In Proceedings of\nthe 32Nd International Conference on International\nConference on Machine Learning - Volume 37 ,\nICML’15, pages 2342–2350. JMLR.org.\nThang Luong, Richard Socher, and Christopher Man-\nning. 2013. Better Word Representations with Re-\ncursive Neural Networks for Morphology. In Pro-\nceedings of the Seventeenth Conference on Com-\nputational Natural Language Learning, pages 104–\n113. Association for Computational Linguistics.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and Optimizing LSTM\nLanguage Models. In ICLR’18. arXiv preprint\narXiv:1708.02182.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient Estimation of Word Represen-\ntations in Vector Space. pages 1–12. ArXiv preprint\narXiv:1301.3781.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nTom´aˇs Mikolov, Stefan Kombrink, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2011. Ex-\ntensions of recurrent neural network language\nmodel. In Acoustics, Speech and Signal Processing\n(ICASSP), 2011 IEEE International Conference on,\npages 5528–5531. IEEE.\nOﬁr Press and Lior Wolf. 2017. Using the Output Em-\nbedding to Improve Language Models. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pages 157–163. Association\nfor Computational Linguistics.\nMartin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.\n2012. LSTM Neural Networks for Language Mod-\neling. In INTERSPEECH.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.",
  "topic": "Tying",
  "concepts": [
    {
      "name": "Tying",
      "score": 0.8523101806640625
    },
    {
      "name": "Computer science",
      "score": 0.8402062654495239
    },
    {
      "name": "Word2vec",
      "score": 0.8286906480789185
    },
    {
      "name": "Word (group theory)",
      "score": 0.7683731913566589
    },
    {
      "name": "Language model",
      "score": 0.6400812864303589
    },
    {
      "name": "Embedding",
      "score": 0.5891590118408203
    },
    {
      "name": "Word embedding",
      "score": 0.5513455867767334
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5383335947990417
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5234526991844177
    },
    {
      "name": "Natural language processing",
      "score": 0.4901474416255951
    },
    {
      "name": "State (computer science)",
      "score": 0.46010175347328186
    },
    {
      "name": "Algorithm",
      "score": 0.18706262111663818
    },
    {
      "name": "Mathematics",
      "score": 0.06698489189147949
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170486558",
      "name": "Universitat Pompeu Fabra",
      "country": "ES"
    }
  ]
}