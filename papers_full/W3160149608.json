{
    "title": "Retrieval-Augmented Transformer-XL for Close-Domain Dialog Generation",
    "url": "https://openalex.org/W3160149608",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4223610772",
            "name": "Bonetta, Giovanni",
            "affiliations": [
                "University of Turin"
            ]
        },
        {
            "id": "https://openalex.org/A4223610774",
            "name": "Cancelliere, Rossella",
            "affiliations": [
                "University of Turin"
            ]
        },
        {
            "id": "https://openalex.org/A2126836301",
            "name": "Liu Ding",
            "affiliations": [
                "Nuance Communications (United States)"
            ]
        },
        {
            "id": null,
            "name": "Vozila, Paul",
            "affiliations": [
                "Nuance Communications (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2971737394",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W2988841832",
        "https://openalex.org/W2947497897",
        "https://openalex.org/W1958706068",
        "https://openalex.org/W889023230",
        "https://openalex.org/W2418993857",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W6635590879",
        "https://openalex.org/W2914204778",
        "https://openalex.org/W2584185835",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2946345909"
    ],
    "abstract": "&#x0D; &#x0D; &#x0D; Transformer-based models have demonstrated excellent capabilities of capturing patterns and structures in natural language generation and achieved state-of-the-art results in many tasks. In this paper we present a transformer-based model for multi-turn dialog response generation. Our solution is based on a hybrid approach which augments a transformer-based generative model with a novel retrieval mechanism, which leverages the memorized information in the training data via k-Nearest Neighbor search. Our system is evaluated on two datasets made by customer/assistant dialogs: the Taskmaster-1, released by Google and holding high quality, goal-oriented conversational data and a proprietary dataset collected from a real customer service call center. Both achieve better BLEU scores over strong baselines.&#x0D; &#x0D; &#x0D;",
    "full_text": "Retrieval-Augmented Transformer-XL for\nClose-Domain Dialog Generation\nGiovanni Bonetta,12 Rossella Cancelliere,1 Ding Liu,2 Paul Vozila2\n1Department of Computer Science, University of Turin, Torino, Italy\n2Nuance Communications Inc., Burlington, MA, USA\n{giovanni.bonetta, rossella.cancelliere}@unito.it, {ding.liu, paul.vozila}@nuance.com\nAbstract\nTransformer-based models have demonstrated excellent ca-\npabilities of capturing patterns and structures in natural\nlanguage generation and achieved state-of-the-art results in\nmany tasks. In this paper we present a transformer-based\nmodel for multi-turn dialog response generation. Our so-\nlution is based on a hybrid approach which augments a\ntransformer-based generative model with a novel retrieval\nmechanism, which leverages the memorized information in\nthe training data via k-Nearest Neighbor search. Our sys-\ntem is evaluated on two datasets made by customer/assistant\ndialogs: the Taskmaster-1, released by Google and holding\nhigh quality, goal-oriented conversational data and a propri-\netary dataset collected from a real customer service call cen-\nter. Both achieve better BLEU scores over strong baselines.\nIntroduction\nAutomatic dialog generation is become today a fundamental\ncomponent for many real-world, challenging applications,\nsuch as virtual assistants, chatbots, etc., and is also a matter\nof great concern for companies and organizations relying on\nartiﬁcial intelligence solutions to enhance millions of daily\ninteractions through their services.\nSimple single-turn Seq2Seq architectures, initially pro-\nposed for this task, often fail to capture long-term tempo-\nral dependencies across dialog turns. (Sutskever, Vinyals,\nand Le 2014; Vinyals and Le 2015; Li et al. 2016). Multi-\nturn Seq2Seq models, such as the hierarchical recurrent en-\ncoder decoder (HRED) (Serban et al. 2016; Xing et al. 2018;\nSerban et al. 2017) have tried to alleviate these problems,\nyielding responses more coherent with the dialog contexts.\nNonetheless, the generated texts tend to be either generic\nor too short, and not comparable with the human ones. Re-\ncently, pretrained transformer-based models such as BERT\n(Devlin et al. 2018), Transformer-XL (Dai et al. 2019), XL-\nNet (Yang et al. 2019) and ERNIE (Zhang et al. 2019) led to\nstate-of-the-art performance on many natural language pro-\ncessing/understanding (NLP/NLU) tasks, including ques-\ntion answering, sentence classiﬁcation, sentence similarity\ninference, and named entity recognition etc.\nCopyright © 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nAn interesting idea which further enhances the generative\nmodel performance is to condition the generation on sam-\nples retrieved from a task-related datastore. In (Guu et al.\n2020; Lee, Chang, and Toutanova 2019) a generative model\nis augmented with a neural retriever trained to pick infor-\nmative text paragraphs; (Khandelwal et al. 2020) propose to\nenhance a language model (LM) through a nearest neigh-\nbor search in suitable text collections. The model we present\nin this paper exploits a similar framework for dialog gen-\neration. Our ﬁrst original contribution is showing how to\ngenerate dialog continuations using a LM augmented with a\nk-nearest neighbors (kNN) based retrieval mechanism. Fur-\nthermore, we exploit the typical dialog structure to enhance\nand speed the retrieval mechanism, improving the genera-\ntion results. In section ”Model Overview” we introduce our\nmodel and formally deﬁne our approach, also going into de-\ntail of the retrieval mechanism. The remaining sections are\ndevoted to the dataset descriptions and results discussion.\nModel Overview\nWe propose a method which improves dialog generation by\nexploiting memorized information from the training data,\nwithout further model training. At inference, turn generation\nis enhanced by interpolating the next word distribution based\non the trained LM with the one based on a kNN search sys-\ntem. A single LM forward pass over the training data is pre-\nliminary conducted to compute context-target pairs and store\nthem in a key-value pair datastore, which will be queried to\nperform the kNN search. The next sections describe this pro-\ncedure and how a kNN distribution is computed and used to\naugment the LM.\nDatastore Creation\nThe ﬁrst step in order to create the datastore is the training\nof a LM, in our case a Transformer-XL (Dai et al. 2019), by\nminimizing the cross entropy of the training data. Overﬁtting\nis controlled through early stopping on validation data per-\nformance. Differently from (Dai et al. 2019) and (Khandel-\nwal et al. 2020), which train a LM by concatenating all the\nexamples, we train the model by resetting the Transformer-\nXL states at the beginning of each chat: this effectively pre-\nvents the model from conditioning on previous unrelated\nFigure 1: Illustration of the Generation Process\ncontexts.\nLet (ci\nt,wi\nt) ∈D be the ith example in training data D.\nThe context ci\nt is a sequence of dialog turns of a dyadic chat\noccurring between an assistant and a user; ci\nt is represented\nas a sequence of tokens, i.e. ci\nt = (wi\n1,wi\n2 ...w i\nt−1), and wi\nt\nis the target word.\nLet f(ci\nt) denote the context-encoder function, that maps\nthe context ci\nt to its ﬁxed-length vector embedding. We de-\nﬁne f(·) as the input to the last feedforward layer in the ﬁnal\nattention block of Transformer-XL, as in (Khandelwal et al.\n2020). This achieves better performance than other options\n(e.g, the output of the last transformer layer). More specif-\nically, f(ci\nt) represents the embedding of token wi\nt−1 after\nattending to all the previous tokens in the example.\nThrough one forward pass on the training data, the trained\nLM is used to build the datastore(K,W) containing the em-\nbeddings of all the tokens in the training data:\n(K,W):=( ki\nt,wi\nt)=( f(ci\nt),wi\nt), ∀(ci\nt,wi\nt)∈D\nwhere ki\nt = f(ci\nt) is the vector representation of the context,\nand wi\nt is the target word id (i.e. integer number).\nHybrid Probability Distribution\nAt inference, at every time step t, the trained LM receives a\nquery (qt), i.e. a chat truncated at the end of a user turn, and\ngenerates the next assistant turn token-by-token, according\nto the following steps, also illustrated in Fig. 1:\n• Generate the context embedding f(qt) and the probabil-\nity distribution PLM (vt|qt) over next words in the vocab-\nulary\n• Issue a kNN search with f(qt) as query, to get from the\ndatastore its nearest neighbors Nt:\nNt = {(k1,w1),(k2,w2) ... (kn,wn) ... }\n• Compute the score SkNN (wn|qt) of the token wn over\nNt, based on L2 distance between kn and f(qt):\nSkNN (wn|qt) = e−d(kn,f(qt))\n∑\nkj∈Nt e−d(kj,f(qt))\n• Aggregate the scores of each vocabulary token wn as the\nsum of all its occurrences within the retrieved neighbors:\nSAggr\nkNN (wn|qt) =\n∑\nwn′ ∈Ntwn′ =wn\nSkNN (wn′ |qt)\n• Get the probability distribution PkNN over next words in\nthe vocabulary:\nPkNN (vt|qt)=\n∑\n(kn,wn)∈Nt\n1vt=wn (SAggr\nkNN (wn|qt))\nwhere 1vt=wn is a vector whose dimension is equal to the\nvocabulary size and whose elements are all zero except\nfor the t-th one, equal to 1.\n• Interpolate PkNN with PLM to get the ﬁnal probability\ndistribution P for next word vt :\nP(vt|qt)= λPkNN (vt|qt)+(1−λ)PLM (vt|qt)\n• Sample the next word ˆvt by greedily sampling from\nP(vt|qt) and concatenate ˆvt to qt to update the context:\nqt+1 = qt + ˆvt\nIf ˆvt is a terminal token the generation process stops; other-\nwise the entire procedure is repeated.\nRetrieval Mechanism\nTo search the datastore, we use FAISS (Johnson, Douze, and\nJ´egou 2017), an open source library for fast nearest neighbor\nretrieval in high dimensional space. FAISS’s central build-\ning block is the index, a structure which stores millions of\nkey-value pairs for efﬁcient search. An issue with the index\nis that the number of elements could easily grow to hun-\ndreds of millions, leading to memory issues and hindering\nthe search performance. However in practice, we only need\nto store token embeddings for assistant turns, since we are\nonly interested in generating assistant responses. So we pro-\npose the simple but effective idea of ﬁltering out from the\ndatastore every token coming from a user turn, so almost\nhalving its size, and allows the generation of consistent ut-\nterances, resembling assitant speciﬁc style.\nDataset Description\nTwo different datasets are used as benchmarks for our\nmethod: a public dataset, the Taskmaster-1, released by\nGoogle in 2019 and a real, company collected, call center\ncustomer service dataset.\nTaskmaster-1 dataset. Taskmaster-1 (Byrne et al. 2019) is\na crowsurced dataset, where Amazon turkers were asked to\nwrite dyadic dialogs following some given set of instructions\ndescribing six tasks: ordering pizza, creating auto repair ap-\npointments, setting up rides for hire, ordering movie tickets,\nordering coffee drinks and making restaurant reservations.\nTable 1: Dataset speciﬁcations.\nTaskmaster-1 Prop. dataset\n# dialogs 7,708 1,328,301\n# turns 169,467 21,953,321\n# unique tokens 29,626 1,601,647\navg. turn per chat 21.99 16.53\navg. tokens per turn 7.83 18.00\nWorkers were asked to play the role of both assistant and\nuser. Speciﬁcally, they were told to write a scenario in which\nthey are speaking to their assistant on the phone while the as-\nsistant accesses the services for one of the given tasks. The\nresulting dataset contains 7,708 conversations. More info\nabout the dataset are in table 1.\nProprietary (Prop.) dataset.1 This dataset contains dyadic\nagent-user chats collected from a ﬁnancial service call cen-\nter over a one year time period, giving us the opportunity\nto test our approach in a real company scenario. It contains\n172 times the dialogs number of the Taskmaster-1, as shown\nin table 1, and comes with two meta-information, the turn\nnumbers and the agent-ids. The turn number is just the po-\nsition of the speciﬁc turn within the chat, while the agent-id\nis a unique identiﬁer for the agent speaking. We concatenate\nthese information to the chat’s text, following the approach\nused in (Wolf et al. 2019). An example is given in ﬁgure 3.\nImplementation Details and Results\nIn this section we present the model implementation details\nand discuss the results obtained for both datasets.\nTaskmaster-1 dataset\nFor the Taskmaster-1 we used a Transformer-XL model with\n12 layers, 8 heads, 512-dimensional hidden states and 2048\nas inner attention dimension, resulting in 49M weights and\ntrained for a maximum of 10k steps optimizing with Adam.\nThe training stopping criterion is based on perplexity on\nthe development set. Hyperparameter tuning, including op-\ntimal λdetermination, is done through performance evalu-\nation over the development set. We adopted a BPE vocabu-\nlary (Sennrich, Haddow, and Birch2015) consisting of 16K\ntokens and generated using the Sentencepiece library (Kudo\nand Richardson 2018). All the training set is used to build\nthe datastore.\nOur model Transformer-XL + kNN is compared with\ntwo baselines: -Transformer, the best performing model by\n(Byrne et al. 2019) and - Transformer-XL, i.e. the LM used\nwithout the retrieval mechanism. The ﬁrst column of table 2\nshows the corresponding BLEU scores 2, obtained as mean\nvalues of 10 different runs, and standard deviations. We can\nsee that our method gets more than two BLEU points over\nthe Transformer baseline, and more than one point over the\nTransformer-XL baseline.\n1The dataset can not be made public due to privacy constraints\n2BLEU script at: https://github.com/tensorﬂow/tensor2tensor/blob\n/master/tensor2tensor/bin/t2t-bleu\nTable 2: Average BLEU and standard deviations on test set.\nThe statistical signiﬁcance is validated via Student’s t-test\nwith signiﬁcance level of 99.8%.\nTaskmaster-1 Prop. dataset\nModels: Avg Std Avg Std\nTransf. 6.11 3 - - -\n(Byrne et al. 2019)\nTransf.-XL 7.09 0.14 39.96 0.36\nTransf.-XL + kNN 8.30 0.05 41.72 0.20\nFigure 2: Taskmaster-1 BLEU trend (development set).\nFigure 2 depicts the BLEU trend curve when the interpo-\nlation parameter λvaries through the selected range. We can\nsee that kNN interpolation improves the BLEU scores over\nthe Transformer-XL baseline for every value of λin the se-\nlected range. The best result is with λ= 0.4, indicating LM\nand context retrieval are almost equally contributing.\nProprietary dataset\nFor the proprietary dataset we used the same model hyperpa-\nrameters as for the Taskmaster-1 but augmented the hidden\nstates dimension to 768 and the inner attention dimension\nto 3072, resulting in ˜116M weights. We trained for a maxi-\nmum of 400k steps.\nSince using all the training set for the datastore would\nresult in a prohibitively large disk space usage we decided\nto build it using just the last 3 months of the training set\n(1/4 of the entire data). This resulted in ˜176M embeddings\nwhich occupy ˜500GB of disc memory. Also in this case the\nTransformer-XL + kNN improves over the LM model for\nabout 1.8 BLEU points, even with a datastore smaller then\nthe entire training set. These results are obtained interpolat-\ning with λ= 0.5 (best on dev. set).\nFigure 3 shows a sample from the test data along with\nthe expected target, the turn generated by the Transformer-\nXL, and the turn generated by our Transformer + kNN. In\nthis dialog a user wants some help for a credit card applica-\ntion. Our proposed model generates a sensible and relevant\n3Results from original paper\nFigure 3: Example of inference query, along with results\nfrom baseline and our best model. agent@company.com is\nthe agent-id, which is preceded by the turn number. Tokens\nbetween angular parenthesis indicate the beginning and end\nof turns.\ncontinuation: the agent conveys the intent to help the user\napply for the credit card, as in the target. On the other hand\nthe baseline Transformer-XL model generates a generic re-\nsponse which is not useful in advancing the dialog.\nConclusions\nIn this work we shown how to enhance a generative model\nfor dialog completion by pairing it with an effective retrieval\nsystem. Our approach achieves higher BLEU scores than\nstrong generative models when tested on two challenging\ndatasets. Moreover, our solution often outputs more sensi-\nble/informative dialog turns. In the future we plan to extend\nthis preliminary work analysing more models on different\ndatasets, and further investigating results and generated ex-\namples.\nReferences\nByrne, B.; Krishnamoorthi, K.; Sankar, C.; Neelakantan, A.;\nGoodrich, B.; Duckworth, D.; Yavuz, S.; Dubey, A.; Kim,\nK.; and Cedilnik, A. 2019. Taskmaster-1: Toward a realistic\nand diverse dialog dataset. In Inui, K.; Jiang, J.; Ng, V .; and\nWan, X., eds., Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Pro-\ncessing, EMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , 4515–4524. Association for Computational\nLinguistics.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J. G.; Le, Q. V .;\nand Salakhutdinov, R. 2019. Transformer-xl: Attentive\nlanguage models beyond a ﬁxed-length context. CoRR\nabs/1901.02860.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2018.\nBERT: pre-training of deep bidirectional transformers for\nlanguage understanding. CoRR abs/1810.04805.\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M. 2020.\nREALM: retrieval-augmented language model pre-training.\nCoRR abs/2002.08909.\nJohnson, J.; Douze, M.; and J ´egou, H. 2017. Billion-\nscale similarity search with gpus. arXiv preprint\narXiv:1702.08734.\nKhandelwal, U.; Levy, O.; Jurafsky, D.; Zettlemoyer, L.;\nand Lewis, M. 2020. Generalization through memoriza-\ntion: Nearest neighbor language models. In Proceedings of\nthe 2020 International Conference on Learning Representa-\ntions.\nLee, K.; Chang, M.; and Toutanova, K. 2019. Latent re-\ntrieval for weakly supervised open domain question answer-\ning. CoRR abs/1906.00300.\nLi, J.; Galley, M.; Brockett, C.; Gao, J.; and Dolan, B. 2016.\nA diversity-promoting objective function for neural conver-\nsation models. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies , 110–\n119. San Diego, California: Association for Computational\nLinguistics.\nSerban, I.; Sordoni, A.; Bengio, Y .; Courville, A.; and\nPineau, J. 2016. Building end-to-end dialogue systems using\ngenerative hierarchical neural network models. In Proceed-\nings of the Thirtieth AAAI Conference on Artiﬁcial Intelli-\ngence (AAAI 2016), 3776—-3784.\nSerban, I. V .; Klinger, T.; Tesauro, G.; Talamadupula, K.;\nZhou, B.; Bengio, Y .; and Courville, A. 2017. Multiresolu-\ntion recurrent neural networks: An application to dialogue\nresponse generation. In Proceedings of the Thirty-First\nAAAI Conference on Artiﬁcial Intelligence (AAAI 2017).\nSutskever, I.; Vinyals, O.; and Le, Q. 2014. Sequence to\nsequence learning with neural networks. In Proceedings of\nAdvances in Neural Information Processing Systems (NIPS),\n3104––3112.\nVinyals, O., and Le, Q. 2015. A neural conversational\nmodel. In Proceedings of ICML Deep Learning Workshop.\nWolf, T.; Sanh, V .; Chaumond, J.; and Delangue, C. 2019.\nTransfertransfo: A transfer learning approach for neural net-\nwork based conversational agents. CoRR abs/1901.08149.\nXing, C.; Wu, Y .; Zhou, M.; Huang, Y .; and Ma, W.-Y . 2018.\nHierarchical recurrent attention network for response gen-\neration. In Proceedings of the The Thirty-Second AAAI\nConference on Artiﬁcial Intelligence (AAAI 2018) , 5610—\n-5617.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J. G.; Salakhut-\ndinov, R.; and Le, Q. V . 2019. Xlnet: Generalized au-\ntoregressive pretraining for language understanding. CoRR\nabs/1906.08237.\nZhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu,\nQ. 2019. ERNIE: enhanced language representation with\ninformative entities. CoRR abs/1905.07129."
}