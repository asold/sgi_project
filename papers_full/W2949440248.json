{
  "title": "Inverse Compositional Spatial Transformer Networks",
  "url": "https://openalex.org/W2949440248",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101863881",
      "name": "Chen-Hsuan Lin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5055053078",
      "name": "Simon Lucey",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2157558673",
    "https://openalex.org/W2950124505",
    "https://openalex.org/W2950094539",
    "https://openalex.org/W2157285372",
    "https://openalex.org/W2121684305",
    "https://openalex.org/W2156163116",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2323139888",
    "https://openalex.org/W2949427019",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W1998294030",
    "https://openalex.org/W2118877769",
    "https://openalex.org/W2035379092"
  ],
  "abstract": "In this paper, we establish a theoretical connection between the classical Lucas &amp; Kanade (LK) algorithm and the emerging topic of Spatial Transformer Networks (STNs). STNs are of interest to the vision and learning communities due to their natural ability to combine alignment and classification within the same theoretical framework. Inspired by the Inverse Compositional (IC) variant of the LK algorithm, we present Inverse Compositional Spatial Transformer Networks (IC-STNs). We demonstrate that IC-STNs can achieve better performance than conventional STNs with less model capacity; in particular, we show superior performance in pure image alignment tasks as well as joint alignment/classification problems on real-world problems.",
  "full_text": "Inverse Compositional Spatial Transformer Networks\nChen-Hsuan Lin Simon Lucey\nThe Robotics Institute\nCarnegie Mellon University\nchenhsul@andrew.cmu.edu slucey@cs.cmu.edu\nAbstract\nIn this paper, we establish a theoretical connection be-\ntween the classical Lucas & Kanade (LK) algorithm and\nthe emerging topic of Spatial Transformer Networks (STNs).\nSTNs are of interest to the vision and learning communi-\nties due to their natural ability to combine alignment and\nclassiﬁcation within the same theoretical framework. In-\nspired by the Inverse Compositional (IC) variant of the LK\nalgorithm, we present Inverse Compositional Spatial Trans-\nformer Networks (IC-STNs). We demonstrate that IC-STNs\ncan achieve better performance than conventional STNs\nwith less model capacity; in particular, we show superior\nperformance in pure image alignment tasks as well as joint\nalignment/classiﬁcation problems on real-world problems.\n1. Introduction\nRecent rapid advances in deep learning are allowing\nfor the learning of complex functions through convolu-\ntional neural networks (CNNs), which have achieved state-\nof-the-art performances in a plethora of computer vision\ntasks [9, 17, 4]. Most networks learn to tolerate spatial vari-\nations through: (a) spatial pooling layers and/or (b) data\naugmentation techniques [16]; however, these approaches\ncome with several drawbacks. Data augmentation ( i.e. the\nsynthetic generation of new training samples through ge-\nometric distortion according to a known noise model) is\nprobably the oldest and best known strategy for increasing\nspatial tolerance within a visual learning system. This is\nproblematic as it can often require an exponential increase\nin the number of training samples and thus the capacity of\nthe model to be learned. Spatial pooling operations can par-\ntially alleviate this problem as they naturally encode spatial\ninvariance within the network architecture and uses sub-\nsampling to reduce the capacity of the model. However,\nthey have an intrinsic limited range of tolerance to geo-\nmetric variation they can provide; furthermore, such pool-\ning operations destroy spatial details within the images that\ncould be crucial to the performance of subsequent tasks.\nInstead of designing a network to solely give tolerance to\nspatial variation, another option is to have the network solve\nfor some of the geometric misalignment in the input im-\nages [12, 6]. Such a strategy only makes sense, however, if\nit has lower capacity and computational cost as well as bet-\nter performance than traditional spatially invariant CNNs.\nSpatial Transformer Networks (STNs) [7] are one of the\nﬁrst notable attempts to integrate low capacity and compu-\ntationally efﬁcient strategies for resolving - instead of tol-\nerating - misalignment with classical CNNs. Jaderberg et\nal. presented a novel strategy for integrating image warping\nwithin a neural network and showed that such operations are\n(sub-)differentiable, allowing for the application of canoni-\ncal backpropagation to an image warping framework.\nThe problem of learning a low-capacity relationship be-\ntween image appearance and geometric distortion is not new\nin computer vision. Over three and a half decades ago, Lu-\ncas & Kanade (LK) [14] proposed the seminal algorithm for\ngradient descent image alignment. The LK algorithm can be\ninterpreted as a feed forward network of multiple alignment\nmodules; speciﬁcally, each alignment module contains a\nlow-capacity predictor (typically linear) for predicting geo-\nmetric distortion from relative image appearance, followed\nby an image resampling/warp operation. The LK algorithm\ndiffers fundamentally, however, to STNs in their applica-\ntion: image/object alignment instead of classiﬁcation.\nPutting applications to one side, the LK and STN frame-\nworks share quite similar characteristics however with a\ncriticial exception. In an STN with multiple feed-forward\nalignment modules, the output image of the previous align-\nment module is directly fed into the next. As we will\ndemonstate in this paper, this is problematic as it can cre-\nate unwanted boundary effects as the number of geomet-\nric prediction layers increase. The LK algorithm does not\nsuffer from such problems; instead, it feeds the warp pa-\nrameters through the network (instead of the warped im-\nage) such that each subsequent alignment module in the\nnetwork resamples the original input source image. Fur-\nthermore, the Inverse Compositional (IC) variant of the LK\nalgorithm [2] has demonstrated to achieve equivalently ef-\n1\narXiv:1612.03897v1  [cs.CV]  12 Dec 2016\nfective alignment by reusing the same geometric predictor\nin a compositional update form.\nInspired by the IC-LK algorithm, we advocate an im-\nproved extension to the STN framework that (a) propagates\nwarp parameters, rather than image intensities, through the\nnetwork, and (b) employs the same geometric predictor that\ncould be reapplied for all alignment modules. We propose\nInverse Compositional Spatial Transformer Networks (IC-\nSTNs) and show its superior performance over the original\nSTNs across a myriad of tasks, including pure image align-\nment and joint alignment/classiﬁcation problems.\nWe organize the paper as follows: we give a general re-\nview of efﬁcient image/object alignment in Sec. 2 and an\noverview of Spatial Transformer Networks in Sec. 3. We\ndescribe our proposed IC-STNs in detail in Sec. 4 and show\nexperimental results for different applications in Sec. 5. Fi-\nnally, we draw to our conclusion in Sec. 6.\n2. Efﬁcient Image & Object Alignment\nIn this section, we give a review of nominal approaches\nto efﬁcient and low-capacity image/object alignment.\n2.1. The Lucas & Kanade Algorithm\nThe Lucas & Kanade (LK) algorithm [14] has been a\npopular approach for tackling dense alignment problems for\nimages and objects. For a given geometric warp function\nparameterized by the warp parameters p, one can express\nthe LK algorithm as minimizing the sum of squared differ-\nences (SSD) objective in the image space,\nmin\n∆p\n∥I(p + ∆p) −T(0)∥2\n2 , (1)\nwhere Iis the source image, T is the template image to\nalign against, and ∆p is the warp update being estimated.\nHere, we denote I(p) as the image I warped with the\nparameters p. The LK algorithm assumes a approximate\nlinear relationship between appearance and geometric dis-\nplacements; speciﬁcally, it linearizes (1) by taking the ﬁrst-\norder Taylor approximation as\nmin\n∆p\nI(p) + ∂I(p)\n∂p ∆p −T(0)\n\n2\n2\n. (2)\nThe warp parameters are thus additively updated through\np ←p + ∆p, which can be regarded as a quasi-Newton\nupdate. The term ∂I(p)\n∂p , known as the steepest descent im-\nage, is the composition of image gradients and the prede-\nﬁned warp Jacobian, where the image gradients are typi-\ncally estimated through ﬁnite differences. As the true rela-\ntionship between appearance and geometry is seldom linear,\nthe warp update ∆p must be iteratively estimated and ap-\nplied until convergence is reached.\nA fundamental problem with the canonical LK formula-\ntion, which employs addtive updates of the warp parame-\nters, is that ∂I(p)\n∂p must be recomputed on the rewarped im-\nages for each iteration, greatly impacting computational ef-\nﬁciency. Baker and Matthews [2] devised a computationally\nefﬁcient variant of the LK algorithm, which they referred to\nas the Inverse Compositional (IC) algorithm. The IC-LK\nalgorithm reformulates (1) to predict the warp update to the\ntemplate image instead, written as\nmin\n∆p\n∥I(p) −T(∆p)∥2\n2 , (3)\nand the linearized least-squares objective is thus formed as\nmin\n∆p\nI(p) −T(0) −∂T(0)\n∂p ∆p\n\n2\n2\n. (4)\nThe least-squares solution is given by\n∆p =\n(∂T(0)\n∂p\n)†\n(I(p) −T(0)) , (5)\nwhere the superscript †denotes the Moore-Penrose pseudo-\ninverse operator. This is followed by the inverse composi-\ntional update p ←p ◦(∆p)−1, where we abbreviate the\nnotation ◦to be the composition of warp functions param-\neterized by p, and (∆p)−1 is the parameters of the inverse\nwarp function parameterized by ∆p.\nThe solutions of (2) and (4) are in the form of linear re-\ngression, which can be more generically expressed as\n∆p = R ·I(p) + b, (6)\nwhere R is a linear regressor establishing the linear rela-\ntionship between appearance and geometry, and b is the\nbias term. Therefore, LK and IC-LK can be interpreted as\nbelonging to the category of cascaded linear regression ap-\nproaches for image alignment.\nIt has been shown [2] that the IC form of LK is effec-\ntively equivalent to the original form; the advantage of the\nIC form lies in its efﬁciency of computing the ﬁxed steepest\ndescent image ∂T(0)\n∂p in the least-squares objective. Specif-\nically, it is evaluated on the static template image T at the\nidentity warp p = 0 and remains constant across iterations,\nand thus so is the resulting linear regressor R. This gives\nan important theoretical proof of concept that a ﬁxed pre-\ndictor of geometric updates can be successfully employed\nwithin an iterative image/object alignment strategy, further\nreducing unnecessary model capacities.\n2.2. Learning Alignment from Data\nMore generally, cascaded regression approaches for\nalignment can be learned from data given that the distri-\nbution of warp displacements is known a priori. A notable\nexample of this kind of approach is the Supervised Descent\nMethod (SDM) [19], which aims to learn the series of linear\ngeometric predictors {R,b}from data. The formulation of\nSDM’s learning objective is\nmin\nR,b\nN∑\nn=1\nM∑\nj=1\n∥δpn,j −R ·In(pn ◦δpn,j) −b∥2\n2 , (7)\nwhere δp is the geometric displacement drawn from a\nknown generating distribution using Monte Carlo sampling,\nand M is the number of synthetically created examples for\neach image. Here, the image appearance Iis often replaced\nwith a predeﬁned feature extraction function (e.g. SIFT [13]\nor HOG [3]) of the image. This least-squares objective is\ntypically solved with added regularization (e.g. ridge regres-\nsion) to ensure good matrix condition.\nSDM is learned in a sequential manner, i.e. the train-\ning data for learning the next linear model is drawn from\nthe same generating distribution and applied through the\npreviously learned regressors. This has been a popular\napproach for its simplicity and effectiveness across vari-\nous alignment tasks, leading to a large number of vari-\nants [15, 1, 11] of similar frameworks. Like the LK and\nIC-LK algorithms, SDM is another example of employing\nmultiple low-capacity models to establish the nonlinear re-\nlationship between appearance and geometry. We draw the\nreaders’ attention to [11] for a more formally established\nlink between LK and SDM.\nIt is a widely agreed that computer vision problems can\nbe solved much more efﬁciently if misalignment among\ndata is eliminated. Although SDM learns alignment from\ndata and guarantees optimal solutions after each applied lin-\near model, it is not clear whether such alignment learned\nin a greedy fashion is optimal for the subsequent tasks at\nhand, e.g. classiﬁcation. In order to optimize in terms of the\nﬁnal objective, it would be more favorable to paramterize\nthe model as a deep neural network and optimize the entire\nmodel using backpropagation.\n3. Spatial Transformer Networks\nIn the rapidly emerging ﬁeld of deep learning among\nwith the explosion of available collected data, deep neural\nnetworks have enjoyed huge success in various vision prob-\nlems. Nevertheless, there had not been a principled way\nof resolving geometric variations in the given data. The re-\ncently proposed Spatial Transformer Networks [7] performs\nspatial transformations on images or feature maps with a\n(sub-)differentiable module. It has the effects of reducing\ngeometric variations inside the data and has brought great\nattention to the deep learning community.\nIn the feed-forward sense, a Spatial Transformer warps\nan image conditioned on the input. This can be mathemati-\nFigure 1: Network module of Spatial Transformers [7]. The\nblue arrows indicate information passing of appearance, and\nthe purple one indicate that of geometry. The yellow 3D\ntrapezoid denotes the geometric predictor, which contains\nthe learnable parameters.\ncally written as\nIout(0) = Iin(p), where p = f(Iin(0)). (8)\nHere, the nonlinear function f is parametrized as a learn-\nable geometric predictor (termed the localization network\nin the original paper), which predicts the warp parameters\nfrom the input image. We note that the “grid generator” and\nthe “sampler” from the original paper can be combined to\nbe a single warp function. We can see that for the special\ncase where the geometric predictor consists of a single lin-\near layer, fwould consists of a linear regressorR as well as\na bias term b, resulting the geometric predictor in an equiv-\nalent form of (6). This insight elegantly links the STN and\nLK/SDM frameworks together.\nFig. 1 shows the basic architecture of STNs. STNs are\nof great interest in that transformation predictions can be\nlearned while also showing that grid sampling functions\ncan be (sub-)differentiable, allowing for backpropagation\nwithin an end-to-end learning framework.\nDespite the similarities STNs have with classic align-\nment algorithms, there exist some fundamental drawbacks\nin comparison to LK/SDM. For one, it attempts to directly\npredict the optimal geometric transformation with a sin-\ngle geometric predictor and does not take advantage of the\nemployment of multiple lower-capacity models to achieve\nmore efﬁcient alignment before classiﬁcation. Although\nit has been demonstrated that multiple Spatial Transform-\ners can be inserted between feature maps, the effectiveness\nof such employment has on improving performance is not\nwell-understood. In addition, we can observe from (8) that\nno information of the geometric warp p is preserved after\nthe output image; this leads to a boundary effect when re-\nsampling outside the input source image. A detailed treat-\nment on this part is provided in Sec. 4.1.\nIn this work, we aim to improve upon STNs by theo-\nretically connecting it to the LK algorithm. We show that\nemploying multiple low-capacity models as in LK/SDM for\nlearning spatial transformation within a deep network yields\n(a)\n(b) (c)\n(d) (e)\nFigure 2: Boundary effect of Spatial Transformers on real\nimages. (a) Original image, where the green box indicates\nthe cropped region. (b) Cropped image as the input of the\nSpatial Transformer. (c) Zoom-in transformation: sampling\noccurs within the range of the input image. (d)(e) Zoom-out\ntransformation: discarding the information outside the input\nimage introduces a boundary effect (STNs), while it is not\nthe case with geometry preservation (c-STNs). The white\ndotted box indicates the warp from the original image.\nsubstantial improvement on the subsequent task at hand. We\nfurther demonstrate the effectiveness of learning a single\ngeometric predictor for recurrent transformation and pro-\npose the Inverse Compositional Spatial Transformer Net-\nworks (IC-STNs), which exhibit signiﬁcant improvements\nover the original STN on various problems.\n4. Inverse Compositional STNs\n4.1. Geometry Preservation\nOne of the major drawbacks of the original Spatial\nTransformer architecture (Fig. 1) is that the output image\nsamples only from the cropped input image; pixel informa-\ntion outside the cropped region is discarded, introducing a\nboundary effect. Fig. 2 illustrates the phenomenon.\nWe can see from Fig. 2(d) that such effect is visible for\nSTNs in zoom-out transformations where pixel information\noutside the bounding box is required. This is due to the fact\nthat geometric information is not preserved after the spa-\ntial transformations. In the scenario of iterative alignment,\nboundary effects are accumulated for each zoom-out trans-\nformations. Although this is less of an issue with images\nwith clean background, this is problematic with real images.\nA series of spatial transformations, however, can be com-\nposed and described with exact expressions. Fig. 3 illus-\ntrates an improved alignment module, which we refer to as\ncompositional STNs (c-STNs). Here, the geometric trans-\nformation is also predicted from a geometric predictor, but\nthe warp parameters p are kept track of, composed, and\npassed through the network instead of thewarped images. It\nis important to note that if one were to incorporate a cascade\nof multiple Spatial Transformers, the geometric transforma-\nFigure 3: A learnable warping module with geometry pre-\nserved, termed as c-STNs. The warp parameters are passed\nthrough the network instead of the warped images.\ntions are implicitly composed through multiple resampling\nof the images. We advocate that these transformations are\nable to be and should be explicitly deﬁned and composed.\nUnlike the Spatial Transformer module in Fig. 1, the ge-\nometry is preserved in p instead of being absorbed into the\noutput image. Furthermore, c-STNs allows repeated con-\ncatenation, illustrated in Fig. 4, where updates to the warp\ncan be iteratively predicted. This eliminates the boundary\neffect because pixel information outside the cropped image\nis also preserved until the ﬁnal transformation.\nThe derivative of warp compositions can also be math-\nematically expressed in closed forms. Consider the input\nand output warp parameters pin and pout in Fig. 3. Tak-\ning the case of afﬁne warps for example, the parameters\np = [p1 p2 p3 p4 p5 p6]⊤are relatable to transforma-\ntion matrices in the homogeneous coordinates as\nM(p) =\n\n\n1 + p1 p2 p3\np4 1 + p5 p6\n0 0 1\n\n. (9)\nFrom the deﬁnition of warp composition, the warp parame-\nters are related to the transformation matrices through\nM(pout) = M(∆p) ·M(pin). (10)\nWe can thus derive the derivative to be\n∂pout\n∂pin\n= I +\n\n\n∆p1 0 0 ∆ p2 0 0\n0 ∆ p1 0 0 ∆ p2 0\n0 0 ∆ p1 0 0 ∆ p2\n∆p4 0 0 ∆ p5 0 0\n0 ∆ p4 0 0 ∆ p5 0\n0 0 ∆ p4 0 0 ∆ p5\n\n\n∂pout\n∂∆p = I +\n\n\npin,1 pin,4 0 0 0 0\npin,2 pin,5 0 0 0 0\npin,3 pin,6 0 0 0 0\n0 0 0 pin,1 pin,4 0\n0 0 0 pin,2 pin,5 0\n0 0 0 pin,3 pin,6 0\n\n\n, (11)\nFigure 4: Multiple concatenation of c-STNs for an iterative alignment framework.\nwhere I is the identity matrix. This allows the gradients to\nbackpropagate into the geometric predictor.\nIt is interesting to note that the expression of∂pout\n∂pin\nin (11)\nhas a very similar expression as in Residual Networks [4, 5],\nwhere the gradients contains the identity matrix I and\n“residual components”. This suggests that the warp pa-\nrameters from c-STNs are generally insensitive to the van-\nishing gradient phenomenon given the predicted warp pa-\nrameters ∆p is small, and that it is possible to repeat the\nwarp/composition operation by a large number of times.\nWe also note that c-STNs are highly analogous to clas-\nsic alignment algorithms. If each geometric predictor con-\nsists of a single linear layer, i.e. the appearance-geometry\nrelationship is assumed to be linearly approximated, then it\nperforms equivalent operations as the compositional LK al-\ngorithm. It is also related to SDM, where heuristic features\nsuch as SIFT are extracted before each regression layer.\nTherefore, c-STNs can be regarded as a generalization of\nLK and SDM, differing that the features for predicting the\nwarp updates can be learned from data and incorporated into\nan end-to-end learning framework.\n4.2. Recurrent Spatial Transformations\nOf all variants of the LK algorithm, the IC form [2]\nhas a very special property in that the linear regressor re-\nmains constant across iterations. The steepest descent im-\nage ∂T(0)\n∂p in (5) is independent of the input image and the\ncurrent estimate of p; therefore, it is only needed to be\ncomputed once. In terms of model capacity, IC-LK fur-\nther reduces the necessary learnable parameters compared\nto canonical LK, for the same regressor can be applied re-\npeatedly and converges provided a good initialization. The\nmain difference from canonical LK and IC-LK lies in that\nthe warp update ∆p should be compositionally applied in\nthe inverse form. We redirect the readers to [2] for a full\ntreatment of IC-LK, which is out of scope of this paper.\nThis inspires us to propose the Inverse Compositional\nSpatial Transformer Network (IC-STN). Fig. 5 illustrates\nthe recurrent module of IC-STN: the warp parameters p is\niteratively updated by ∆p, which is predicted from the cur-\nrent warped image with the same geometric predictors. This\nFigure 5: Illustration of the proposed Inverse Compositional\nSpatial Transformer Network (IC-STN). The same geomet-\nric predictor is learned to predict recurrent spatial transfor-\nmations that are composed together to warp the input image.\nallows one to recurrently predict spatial transformations on\nthe input image. It is possible due to the close spatial prox-\nimity of pixel intensities within natural images: there exists\nhigh correlation between pixels in close distances.\nIn the IC-LK algorithm, the predicted warp parameters\nare inversely composed. Since the IC-STN geometric pre-\ndictor is optimized in an end-to-end learning framework,\nwe can absorb the inversion operation into the geometric\npredictor without explicitly deﬁning it; in other words, IC-\nSTNs are able to directly predict the inverse parameters. In\nour experiments, we ﬁnd that there is negligible difference\nto explicitly perform an additional inverse operation on the\npredicted forward parameters, and that implicitly predicting\nthe inverse parameters ﬁts more elegantly in an end-to-end\nlearning framework using backpropagation. We name our\nproposed method Inverse Compositional nevertheless as IC-\nLK is where our inspirations are drawn from.\nIn practice, IC-STNs can be trained by unfolding the ar-\nchitecture in Fig. 5 multiple times into the form of c-STNs\n(Fig. 4), sharing the learnable parameters across all geo-\nmetric predictors, and backpropagating the gradients as de-\nscribed in Sec. 4.1. This results in a single effective geo-\nmetric predictor that can be applied multiple times before\nperforming the ﬁnal warp operation that suits subsequent\ntasks such as classiﬁcation.\n(a) (b)\nFigure 6: Visualization of the image and perturbed train-\ning samples for the planar image alignment experiment. (a)\nOriginal image, where the red box indicates the ground-\ntruth warp and the yellow boxes indicate example generated\nwarps. (b) Examples of the perturbed images (afﬁne warps\nwith σ= 7.5 in this case).\nModel σ= 2.5 σ= 5 σ= 7.5 σ= 10\nc-STN-1 2.699 5.576 9.491 9.218\nIC-STN-2 0.615 2.268 5.283 5.502\nIC-STN-3 0.434 1.092 2.877 3.020\nIC-STN-4 0.292 0.481 1.476 2.287\nIC-STN-6 0.027 0.125 0.245 1.305\nTable 1: Test error for the planar image alignment exper-\niment under different extents of initial perturbations. The\nnumber following the model names indicate the number of\nwarp operations unfolded from IC-STN during training.\n5. Experiments\n5.1. Planar Image Alignment\nTo start with, we explore the efﬁcacy of IC-STN for\nplanar alignment of a single image. We took an example\nimage from the Caffe library [8] and generated perturbed\nimages with afﬁne warps around the hand-labeled ground\ntruth, shown in Fig. 6. We used image samples of size 50×\n50 pixels. The perturbed boxes are generated by adding\ni.i.d. Gaussian noise of standard deviation σ (in pixels) to\nthe four corners of the ground-truth box plus an additional\ntranslational noise from the same Gaussian distribution, and\nﬁnally ﬁtting the box to the initial warp parameters p.\nTo demonstrate the effectiveness of iterative alignment\nunder different amount of noise, we consider IC-STNs that\nconsist of a single learnable linear layer with different num-\nbers of learned recurrent transformations. We optimize all\nnetworks in terms of L2 error between warp parameters\nwith stochastic gradient descent and a batch size of 100 per-\nturbed training samples generated on the ﬂy.\nThe test error is illustrated in Table 1. We see from\nc-STN-1 (which is equivalent to IC-STN-1 with only one\nwarp operation unfolded) that a single geometric warp pre-\nFigure 7: Evaluation on trained IC-STNs, where the dot on\neach curve corresponds to the number of recurrent transfor-\nmations unfolded during training.\ndictor has limited ability to directly predict the optimal ge-\nometric transformation. Reusing the geometric predictor to\nincorporating multiple spatial transformations yields better\nalignment performance given the same model capacity.\nFig. 7 shows the test error over the number of warp op-\nerations applied to the learned alignment module. We can\nsee that even when the recurrent spatial transformation is\napplied more times than trained with, the error continues to\ndecrease until some of point of saturation, which typically\ndoes not hold true for classical recurrent neural networks.\nThis implies that IC-STN is able to capture the correlation\nbetween appearance and geometry to perform gradient de-\nscent on a learned cost surface for successful alignment.\n5.2. MNIST Classiﬁcation\nIn this section, we demonstrate how IC-STNs can be uti-\nlized in joint alignment/classﬁcation tasks. We choose the\nMNIST handwritten digit dataset [10], and we use a ho-\nmography warp noise model to perturb the four corners of\nthe image and translate them with Gaussian noise, both with\na standard deviation of 3.5 pixels. We train all networks for\n200K iterations with a batch size of 100 perturbed samples\ngenerated on the ﬂy. We choose a constant learning rate\nof 0.01 for the classiﬁcation subnetworks and 0.0001 for\nthe geometric predictors as we ﬁnd the geometric predic-\ntor sensitive to large changes. We evaluate the classiﬁcation\naccuracy on the test set using the same warp noise model.\nWe compare IC-STN to several network architectures,\nincluding a baseline CNN with no spatial transformations,\nthe original STN from Jaderberget al., and c-STNs. All net-\nworks with spatial transformations employ the same classi-\nﬁcation network. The results as well as the architectural de-\ntails are listed in Table 2. We can see that classical CNNs do\nnot handle large spatial variations efﬁciently with data aug-\nmentation. In the case where the digits may be occluded,\nModel Test error Capacity Architecture\nCNN(a) 6.597 % 39079 conv(3×3, 3)-conv(3×3, 6)-P-conv(3×3, 9)-conv(3×3, 12)-FC(48)-FC(10)\nSTN(a) 4.944 % 39048 [ conv(7×7, 4)-conv(7×7, 8)-P-FC(48)-FC(8) ]×1 →conv(9×9, 3)-FC(10)\nc-STN-1(a) 3.687 % 39048 [ conv(7×7, 4)-conv(7×7, 8)-P-FC(48)-FC(8) ]×1 →conv(9×9, 3)-FC(10)\nc-STN-2(a) 2.060 % 38528 [ conv(9×9, 4)-FC(8) ]×2 →conv(9×9, 3)-FC(10)\nc-STN-4(a) 1.476 % 37376 [ FC(8) ]×4 →conv(9×9, 3)-FC(10)\nIC-STN-2(a) 1.905 % 39048 [ conv(7×7, 4)-conv(7×7, 8)-P-FC(48)-FC(8) ]×2 →conv(9×9, 3)-FC(10)\nIC-STN-4(a) 1.230 % 39048 [ conv(7×7, 4)-conv(7×7, 8)-P-FC(48)-FC(8) ]×4 →conv(9×9, 3)-FC(10)\nCNN(b) 19.065 % 19610 conv(9×9, 2)-conv(9×9, 4)-FC(32)-FC(10)\nSTN(b) 9.325 % 18536 [ FC(8) ]×1 →conv(9×9, 3)-FC(10)\nc-STN-1(b) 8.545 % 18536 [ FC(8) ]×1 →conv(9×9, 3)-FC(10)\nIC-STN-2(b) 3.717 % 18536 [ FC(8) ]×2 →conv(9×9, 3)-FC(10)\nIC-STN-4(b) 1.703 % 18536 [ FC(8) ]×4 →conv(9×9, 3)-FC(10)\nTable 2: Classiﬁcation error on the perturbed MNIST test set. The non-recurrent networks have similar numbers of layers and\nlearnable parameters but different numbers of warp operations (bold-faced). The ﬁlter dimensions are shown in parentheses,\nwhere those of the geometric predictor(s) are in green and those of the subsequent classiﬁcation network are in blue (P\ndenotes a 2×2 max-pooling operation). Best viewed in color.\nhowever, trading off capacity for a single deep predictor of\ngeometric transformation also results in poor performance.\nIncorporating multiple transformers lead to a signiﬁcant im-\nprovement in classiﬁcation accuracy; further comparing c-\nSTN-4(a) and IC-STN-4(b), we see that IC-STNs are able\nto trade little accuracy off for a large reduction of capacity\ncompared to its non-recurrent counterpart.\nFig. 8 shows how IC-STNs learns alignment for classi-\nﬁcation. In many cases where the handwritten digits are\noccluded, IC-STN is able to automatically warp the image\nand reveal the occluded information from the original im-\nage. There also exists smooth transitions during the align-\nment, which conﬁrms with the recurrent spatial transforma-\ntion concept IC-STN learns. Furthermore, we see that the\noutcome of the original STN becomes cropped digits due to\nthe boundary effect described in Sec. 4.1.\nWe also visualize the overall ﬁnal alignment perfor-\nmance by taking the mean and variance on the test set\nappearance before classiﬁcation, shown in Fig. 9. The\nmean/variance results of the original STN becomes a down-\nscaled version of the original digits, reducing information\nnecessary for better classiﬁcation. From c-STN-1, we see\nthat a single geometric predictor is poor in directly pre-\ndicting geometric transformations. The variance among all\naligned samples is dramatically decreased when more warp\noperations are introduced in IC-STN. These results support\nthe fact that elimination of spatial variations within data is\ncrucial to boosting the performance of subsequent tasks.\n5.3. Trafﬁc Sign Classiﬁcation\nHere, we show how IC-STNs can be applied to real-\nworld classiﬁcation problems such as trafﬁc sign recogni-\ntion. We evaluate our proposed method with the German\nTrafﬁc Sign Recognition Benchmark [18], which consists\nSTN\nc-STN-1\nIC-STN-2\nIC-STN-4\nMean\nVariance\nperturbed\noriginal\nSTN\nc-STN-1\nIC-STN-2\nIC-STN-4\nperturbed\noriginal\nFigure 9: Mean/variance of the aligned appearances from\nthe 10 classes of the test set (homography perturbations).\nof 39,209 training and 12,630 test images from 43 classes\ntaken under various conditions. We consider this as a chal-\nlenging task since many of the images are taken with mo-\ntion blurs and/or of resolution as low as 15 ×15 pixels. We\nrescale all images and generate perturbed samples of size\n36×36 pixels with the same homography warp noise model\ndescribed in Sec. 5.2. The learning rate is set to be 0.001\nfor the classiﬁcation subnetworks and 0.00001 for the geo-\nmetric predictors.\nSTN\ninit.\n(1)\n(2)\n(3)\nfinal\nFigure 8: Sample alignment results of IC-STN-4(a) on the MNIST test set with homography warp perturbations. The ﬁrst\nrow of each column shows the initial perturbation; the middle three rows illustrates the alignment process (iterations 1 to 3);\nthe second last row shows the ﬁnal alignment before feeding into the classiﬁcation network. The last row shows the alignment\nfrom the original STN: the cropped digits are the results of the boundary effect.\nModel Test error Capacity Architecture\nCNN 8.287 % 200207 conv(7×7, 6)-conv(7×7, 12)-P-conv(7×7, 24)-FC(200)-FC(43)\nSTN 6.495 % 197343 [ conv(7×7, 6)-conv(7×7, 24)-FC(8) ]×1 →conv(7×7, 6)-conv(7×7, 12)-P-FC(43)\nc-STN-1 5.011 % 197343 [ conv(7×7, 6)-conv(7×7, 24)-FC(8) ]×1 →conv(7×7, 6)-conv(7×7, 12)-P-FC(43)\nIC-STN-2 4.122 % 197343 [ conv(7×7, 6)-conv(7×7, 24)-FC(8) ]×2 →conv(7×7, 6)-conv(7×7, 12)-P-FC(43)\nIC-STN-4 3.184 % 197343 [ conv(7×7, 6)-conv(7×7, 24)-FC(8) ]×4 →conv(7×7, 6)-conv(7×7, 12)-P-FC(43)\nTable 3: Classiﬁcation error on the perturbed GTSRB test set. The architectural descriptions follow that in Table 2.\nSTN\nIC-STN-4\ninitial\nFigure 10: Sample alignment results of IC-STN-4 on the\nGTSRB test set in comparison to the original STN.\nSTN\nc-STN-1\nIC-STN-2\nIC-STN-4\nperturbed\nclass\n 30\n 80\nFigure 11: Mean aligned appearances for classiﬁcation\nfrom sampled classes of the GTSRB test set.\nWe set the controlled model capacities to around 200K\nlearnable parameters and perform similar comparisons to\nthe MNIST experiment. Table 3 shows the classiﬁcation\nerror on the perturbed GTSRB test set. Once again, we see\na considerable amount of classiﬁcation improvement of IC-\nSTN from learning to reuse the same geometric predictor.\nFig. 10 compares the aligned images from IC-STN and\nthe original STN before the classiﬁcation networks. Again,\nIC-STNs are able to recover occluded appearances from the\ninput image. Although STN still attempts to center the per-\nturbed images, the missing information from occlusion de-\ngrades its subsequent classiﬁcation performance.\nWe also visualize the aligned mean appearances from\neach network in Fig. 11, and it can be observed that the\nmean appearance of IC-STN becomes sharper as the num-\nber of warp operations increase, once again indicating that\ngood alignment is crucial to the subsequent target tasks. It\nis also interesting to note that not all trafﬁc signs are aligned\nto be ﬁt exactly inside the bounding boxes,e.g. the networks\nﬁnds the optimal alignment for stop signs to be zoomed-in\nimages while excluding the background information outside\nthe octagonal shapes. This suggests that in certain cases,\nonly the pixel information inside the sign shapes are neces-\nsary to achieve good alignment for classiﬁcation.\n6. Conclusion\nIn this paper, we theoretically connect the core idea of\nthe Lucas & Kanade algorithm with Spatial Transformer\nNetworks. We show that geometric variations within data\ncan be eliminated more efﬁciently through multiple spa-\ntial transformations within an alignment framework. We\npropose Inverse Compositional Spatial Transformer Net-\nworks for predicting recurrent spatial transformations and\ndemonstrate superior alignment and classiﬁcation results\ncompared to baseline CNNs and the original STN.\nReferences\n[1] A. Asthana, S. Zafeiriou, S. Cheng, and M. Pantic. Incre-\nmental face alignment in the wild. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1859–1866, 2014. 3\n[2] S. Baker and I. Matthews. Lucas-kanade 20 years on: A uni-\nfying framework. International journal of computer vision ,\n56(3):221–255, 2004. 1, 2, 5\n[3] N. Dalal and B. Triggs. Histograms of oriented gradi-\nents for human detection. In 2005 IEEE Computer Soci-\nety Conference on Computer Vision and Pattern Recognition\n(CVPR’05), volume 1, pages 886–893. IEEE, 2005. 3\n[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. arXiv preprint arXiv:1512.03385,\n2015. 1, 5\n[5] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in\ndeep residual networks. arXiv preprint arXiv:1603.05027 ,\n2016. 5\n[6] G. Huang, M. Mattar, H. Lee, and E. G. Learned-miller.\nLearning to align from scratch. In F. Pereira, C. J. C. Burges,\nL. Bottou, and K. Q. Weinberger, editors, Advances in Neu-\nral Information Processing Systems 25, pages 764–772. Cur-\nran Associates, Inc., 2012. 1\n[7] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial\ntransformer networks. In Advances in Neural Information\nProcessing Systems, pages 2017–2025, 2015. 1, 3\n[8] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-\ntional architecture for fast feature embedding. In Proceed-\nings of the 22nd ACM international conference on Multime-\ndia, pages 675–678. ACM, 2014. 6\n[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems , pages\n1097–1105, 2012. 1\n[10] Y . LeCun, C. Cortes, and C. J. Burges. The mnist database\nof handwritten digits, 1998. 6\n[11] C.-H. Lin, R. Zhu, and S. Lucey. The conditional lucas &\nkanade algorithm. In European Conference on Computer\nVision (ECCV), pages 793–808. Springer International Pub-\nlishing, 2016. 3\n[12] J. L. Long, N. Zhang, and T. Darrell. Do convnets learn\ncorrespondence? In Z. Ghahramani, M. Welling, C. Cortes,\nN. D. Lawrence, and K. Q. Weinberger, editors, Advances\nin Neural Information Processing Systems 27 , pages 1601–\n1609. Curran Associates, Inc., 2014. 1\n[13] D. G. Lowe. Distinctive image features from scale-\ninvariant keypoints. International journal of computer vi-\nsion, 60(2):91–110, 2004. 3\n[14] B. D. Lucas, T. Kanade, et al. An iterative image registra-\ntion technique with an application to stereo vision. In IJCAI,\nvolume 81, pages 674–679, 1981. 1, 2\n[15] S. Ren, X. Cao, Y . Wei, and J. Sun. Face alignment at 3000\nfps via regressing local binary features. InProceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1685–1692, 2014. 3\n[16] P. Y . Simard, D. Steinkraus, and J. C. Platt. Best practices for\nconvolutional neural networks applied to visual document\nanalysis. In ICDAR, volume 3, pages 958–962, 2003. 1\n[17] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014. 1\n[18] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The ger-\nman trafﬁc sign recognition benchmark: a multi-class classi-\nﬁcation competition. In Neural Networks (IJCNN), The 2011\nInternational Joint Conference on, pages 1453–1460. IEEE,\n2011. 7\n[19] X. Xiong and F. De la Torre. Supervised descent method\nand its applications to face alignment. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 532–539, 2013. 3",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6710387468338013
    },
    {
      "name": "Inverse",
      "score": 0.661188542842865
    },
    {
      "name": "Transformer",
      "score": 0.5696609616279602
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4551711678504944
    },
    {
      "name": "Algorithm",
      "score": 0.38651761412620544
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3361958861351013
    },
    {
      "name": "Mathematics",
      "score": 0.13442939519882202
    },
    {
      "name": "Engineering",
      "score": 0.08546599745750427
    },
    {
      "name": "Geometry",
      "score": 0.06680536270141602
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}