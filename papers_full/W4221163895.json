{
    "title": "TableFormer: Robust Transformer Modeling for Table-Text Encoding",
    "url": "https://openalex.org/W4221163895",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2099665068",
            "name": "Jingfeng Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2117027874",
            "name": "Aditya Gupta",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2196028422",
            "name": "Shyam Upadhyay",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2139844888",
            "name": "Luheng He",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2128363014",
            "name": "Rahul Goel",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2902818996",
            "name": "Shachi Paul",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3035231859",
        "https://openalex.org/W3103667349",
        "https://openalex.org/W3197798882",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2971822538",
        "https://openalex.org/W3035140194",
        "https://openalex.org/W2963899988",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2970916293",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W3184222203",
        "https://openalex.org/W3034835156",
        "https://openalex.org/W2612228435",
        "https://openalex.org/W3105238007",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4287659415",
        "https://openalex.org/W3165753548",
        "https://openalex.org/W3169622372",
        "https://openalex.org/W3099873751",
        "https://openalex.org/W3037082750"
    ],
    "abstract": "Understanding tables is an important aspect of natural language understanding. Existing models for table understanding require linearization of the table structure, where row or column order is encoded as an unwanted bias. Such spurious biases make the model vulnerable to row and column order perturbations. Additionally, prior work has not thoroughly modeled the table structures or table-text alignments, hindering the table-text understanding ability. In this work, we propose a robust and structurally aware table-text encoding architecture TableFormer, where tabular structural biases are incorporated completely through learnable attention biases. TableFormer is (1) strictly invariant to row and column orders, and, (2) could understand tables better due to its tabular inductive biases. Our evaluations showed that TableFormer outperforms strong baselines in all settings on SQA, WTQ and TabFact table reasoning datasets, and achieves state-of-the-art performance on SQA, especially when facing answer-invariant row and column order perturbations (6% improvement over the best baseline), because previous SOTA models’ performance drops by 4% - 6% when facing such perturbations while TableFormer is not affected.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 528 - 537\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nTABLE FORMER : Robust Transformer Modeling for Table-Text Encoding\nJingfeng Yang\n∗\nAditya Gupta† Shyam Upadhyay†\nLuheng He† Rahul Goel† Shachi Paul†\n⋆Georgia Institute of Technology\n†Google Assistant\njingfengyangpku@gmail.com\ntableformer@google.com\nAbstract\nUnderstanding tables is an important aspect of\nnatural language understanding. Existing mod-\nels for table understanding require lineariza-\ntion of the table structure, where row or col-\numn order is encoded as an unwanted bias.\nSuch spurious biases make the model vulner-\nable to row and column order perturbations.\nAdditionally, prior work has not thoroughly\nmodeled the table structures or table-text align-\nments, hindering the table-text understanding\nability. In this work, we propose a robust and\nstructurally aware table-text encoding architec-\nture TABLE FORMER , where tabular structural\nbiases are incorporated completely through\nlearnable attention biases. T ABLE FORMER is\n(1) strictly invariant to row and column or-\nders, and, (2) could understand tables better\ndue to its tabular inductive biases. Our eval-\nuations showed that T ABLE FORMER outper-\nforms strong baselines in all settings on SQA,\nWTQ and T ABFACT table reasoning datasets,\nand achieves state-of-the-art performance on\nSQA, especially when facing answer-invariant\nrow and column order perturbations (6% im-\nprovement over the best baseline), because pre-\nvious SOTA models’ performance drops by\n4% - 6% when facing such perturbations while\nTABLE FORMER is not affected.1\n1 Introduction\nRecently, semi-structured data (e.g. variable length\ntables without a ﬁxed data schema) has attracted\nmore attention because of its ubiquitous presence\non the web. On a wide range of various table rea-\nsoning tasks, Transformer based architecture along\nwith pretraining has shown to perform well (Eisen-\nschlos et al., 2021; Liu et al., 2021).\nIn a nutshell, prior work used the Transformer\narchitecture in a BERT like fashion by serializing\ntables or rows into word sequences (Yu et al., 2020;\n∗Work done during an internship at Google.\n1Code to be released at https://github.com/\ngoogle-research/tapas\nTitle Length\nScrewed Up 5:02\nGhetto Queen 5:00\nQuestion: Of all song lengths, which one is the longest?\nGold Answer:5:02\nTAPAS:5:00\nTAPAS after row order perturbation:5:02\nTABLE FORMER : 5:02\n(a) TAPAS predicts incorrect answer based on the original table,\nwhile it gives the correct answer if the ﬁrst row is moved to\nthe end of the table.\nNation Gold Silver Bronze\nGreat Britain 2 1 2\nSpain 1 2 0\nUkraine 0 2 0\nQuestion: Which nation received 2 silver medals?\nGold Answer:Spain, Ukraine\nTAPAS:Spain\nTABLE FORMER : Spain, Ukraine\nTABLE FORMER w/o a proposed structural bias:Spain\n(b) TAPAS gives incomplete answer due to its limited cell\ngrounding ability.\nFigure 1: Examples showing the limitations of exist-\ning models (a) vulnerable to perturbations, and (b) lack-\ning structural biases. In contrast, our proposed TABLE -\nFORMER predicts correct answers for both questions.\nLiu et al., 2021), where original position ids are\nused as positional information. Due to the usage\nof row/column ids and global position ids, prior\nstrategies to linearize table structures introduced\nspurious row and column order biases (Herzig et al.,\n2020; Eisenschlos et al., 2020, 2021; Zhang et al.,\n2020; Yin et al., 2020). Therefore, those models are\nvulnerable to row or column order perturbations.\nBut, ideally, the model should make consistent pre-\ndictions regardless of the row or column ordering\nfor all practical purposes. For instance, in Figure 1,\nthe predicted answer ofTAPAS model (Herzig et al.,\n2020) for Question (a) “Of all song lengths, which\none is the longest?” based on the original table is\n“5:00”, which is incorrect. However, if the ﬁrst row\nis adjusted to the end of the table during inference,\nthe model gives the correct length “5:02” as an-\n528\nswer. This probing example shows that the model\nbeing aware of row order information is inclined\nto select length values to the end of the table due\nto spurious training data bias. In our experiments\non the SQA dataset, TAPAS models exhibit a 4% -\n6% (Section 5.2) absolute performance drop when\nfacing such answer-invariant perturbations.\nBesides, most prior work (Chen et al., 2020; Yin\net al., 2020) did not incorporate enough structural\nbiases to models to address the limitation of sequen-\ntial Transformer architecture, while others induc-\ntive biases which are either too strict (Zhang et al.,\n2020; Eisenschlos et al., 2021) or computationally\nexpensive (Yin et al., 2020).\nTo this end, we propose TABLE FORMER , a\nTransformer architecture that is robust to row and\ncolumn order perturbations, by incorporating struc-\ntural biases more naturally. TABLE FORMER re-\nlies on 13 types of task-independent table ↔text\nattention biases that respect the table structure and\ntable-text relations. For Question (a) in Figure 1,\nTABLE FORMER could predict the correct answer\nregardless of perturbation, because the model could\nidentify the same row information with our “same\nrow” bias, avoiding spurious biases introduced by\nrow and global positional embeddings. For Ques-\ntion (b), TAPAS predicted only partially correct\nanswer, while TABLE FORMER could correctly pre-\ndict “Spain, Ukraine” as answers. That’s because\nour “cell to sentence” bias could help table cells\nground to the paired sentence. Detailed attention\nbias types are discussed in Section 5.2.\nExperiments on 3 table reasoning datasets show\nthat TABLE FORMER consistently outperforms orig-\ninal TAPAS in all pretraining and intermediate\npretraining settings with fewer parameters. Also,\nTABLE FORMER ’s invariance to row and column\nperturbations, leads to even larger improvement\nover those strong baselines when tested on pertur-\nbations. Our contributions are as follows:\n• We identiﬁed the limitation of current table-\ntext encoding models when facing row or col-\numn perturbation.\n• We propose TABLE FORMER , which is guaran-\nteed to be invariant to row and column order\nperturbations, unlike current models.\n• TABLE FORMER encodes table-text structures\nbetter, leading to SoTA performance on SQA\ndataset, and ablation studies show the effec-\ntiveness of the introduced inductive biases.\n2 Preliminaries: T APAS for Table\nEncoding\nIn this section, we discuss TAPAS which serves\nas the backbone of the recent state-of-the-art table-\ntext encoding architectures. TAPAS (Herzig et al.,\n2020) uses Transformer architecture in a BERT\nlike fashion to pretrain and ﬁnetune on tabular\ndata for table-text understanding tasks. This is\nachieved by using linearized table and texts for\nmasked language model pre-training. In the ﬁne-\ntuning stage, texts in the linearized table and text\npairs are queries or statements in table QA or table-\ntext entailment tasks, respectively.\nSpeciﬁcally, TAPAS uses the tokenized and ﬂat-\ntened text and table as input, separated by [SEP]\ntoken, and preﬁxed by [CLS]. Besides token, seg-\nment, and global positional embedding introduced\nin BERT (Devlin et al., 2019), it also uses rank em-\nbedding for better numerical understanding. More-\nover, it uses column and row embedding to encode\ntable structures.\nConcretely, for any table-text linearized se-\nquence S = {v1,v2,··· ,vn}, where n is the\nlength of table-text sequence, the input to TAPAS\nis summation of embedding of the following:\ntoken ids (W) = {wv1 ,wv2 ,··· ,wvn}\npositional ids (B) = {b1,b2,··· ,bn}\nsegment ids (G) = {gseg1 ,gseg2 ,··· ,gsegn}\ncolumn ids (C) = {ccol1 ,ccol2 ,··· ,ccoln}\nrow ids (R) = {rrow1 ,rrow2 ,··· ,rrown}\nrank ids (Z) = {zrank1 ,zrank2 ,··· ,zrankn}\nwhere segi, coli, rowi, ranki correspond to\nthe segment, column, row, and rank id for the ith\ntoken, respectively.\nAs for the model, TAPAS uses BERT’s self-\nattention architecture (Vaswani et al., 2017) off-\nthe-shelf. Each Transformer layer includes a multi-\nhead self-attention sub-layer, where each token\nattends to all the tokens. Let the layer input\nH = [h1,h2,··· ,hn]⊤∈Rn×d corresponding to\nS, where dis the hidden dimension, andhi ∈Rd×1\nis the hidden representation at position i. For\na single-head self-attention sub-layer, the input\nH is projected by three matrices WQ ∈Rd×dK ,\nWK ∈Rd×dK , and WV ∈Rd×dV to the corre-\nsponding representations Q, K, and V:\nQ= HWQ, V = HWV, K = HWK (1)\n529\nTransformer (Self Attention)\nTransformer (Self Attention)\n⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ \n ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ \n ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ \n ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ \n ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ \n ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ \n ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ \n ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ \n ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ \n ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ \n ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ ⬤ \n Learnable structure enforced \nattention bias scalars. We use \n13 types of attention biases based \non task independent relation \nbetween header, row, column, \ntext, etc. \nwlongest w[SEP] wtitle wlength wqueen w5:00\ng0 g0 g1 g1 g1 g1\np10 p11 p0 p0 p1 p0\nz0 z0 z0 z0 z2\ntoken\nsegment\nper cell position\nrank\nw[CLS] wof\ng0 g0\np0 p1\nz0\n…\n…\n…\n…\n…\n…\nh1\nlongest h1\n[SEP] h1\ntitle h1\nlength h1\nqueenh1\n5:00h1\n[CLS] h1\nof … …\nhn\nlongest hn\n[SEP] hn\ntitle hn\nlength hn\nqueenhn\n5:00hn\n[CLS] hn\nof\nquery\n…\ntable\n…\n… …\n.\n.\n.\nQuestion: “Of all song lengths, which one is the longest ?”  \nTitle Length\nScrewed Up 5:02\nGhetto Queen 5:00\nQuery Table\n[CLS]\nquery\n[SEP]\nTitle\nLength\nScrewed\nUp\n5:02\nGhetto\nQueen\n5:00\n[CLS]\nquery\n[SEP]\nTitle\nScrewed\nLength\nUp\n5:02\nQueen\n5:00\nGhetto\nz0z0\nFigure 2: T ABLE FORMER input and attention biases in the self attention module. This example corresponds to\ntable (a) in Figure 1 and its paired question “query”. Different colors in the attention bias matrix denote different\ntypes of task independent biases derived based on the table structure and the associated text.\nThen, the output of this single-head self-\nattention sub-layer is calculated as:\nAttn(H) = softmax(QK⊤\n√dK\n)V (2)\n3 T ABLE FORMER : Robust Structural\nTable Encoding\nAs shown in Figure 2, TABLE FORMER encodes the\ngeneral table structure along with the associated\ntext by introducing task-independent relative atten-\ntion biases for table-text encoding to facilitate the\nfollowing: (a) structural inductive bias for better\ntable understanding and table-text alignment, (b)\nrobustness to table row/column perturbation.\nInput of T ABLE FORMER . TABLE FORMER\nuses the same token embeddings W, segment\nembeddings G, and rank embeddings Zas TAPAS.\nHowever, we make 2 major modiﬁcations:\n1) No row or column ids.We do not use row em-\nbeddings Ror column embeddings Cto avoid any\npotential spurious row and column order biases.\n2) Per cell positional ids.To further remove any\ninter-cell order information, global positional em-\nbeddings Bare replaced by per cell positional em-\nbeddings P = {ppos1 ,ppos2 ,··· ,pposn}, where\nwe follow Eisenschlos et al. (2021) to reset the\nindex of positional embeddings at the beginning\nof each cell, and posi correspond to the per cell\npositional id for the ith token.\nPositional Encoding in TABLE FORMER . Note\nthat the Transformer model either needs to spec-\nify different positions in the input (i.e. absolute\npositional encoding of Vaswani et al. (2017)) or\nencode the positional dependency in the layers (i.e.\nrelative positional encoding of Shaw et al. (2018)).\nTABLE FORMER does not consume any sort of\ncolumn and row order information in the input. The\nmain intuition is that, for cells in the table, the only\nuseful positional information is whether two cells\nare in the same row or column and the column\nheader of each cell, instead of the absolute order\nof the row and column containing them. Thus, in-\nspired by relative positional encoding (Shaw et al.,\n2018) and graph encoding (Ying et al., 2021), we\ncapture this with a same column/row relation as\none kind of relative position between two linearized\ntokens. Similarly, we uses 12 such table-text struc-\nture relevant relations (including same cell, cell\nto header and so on) and one extra type represent-\ning all other relations not explicitly deﬁned. All\nof them are introduced in the form of learnable\n530\nattention bias scalars.\nFormally, we consider a functionφ(vi,vj) : V×\nV →N, which measures the relation between vi\nand vj in the sequence (vi,vj ∈S). The function φ\ncan be deﬁned by any relations between the tokens\nin the table-text pair.\nAttention Biases in TABLE FORMER . In our\nwork, φ(vi,vj) is chosen from 13 bias types, cor-\nresponding to 13 table-text structural biases. The\nattention biases are applicable to any table-text pair\nand can be used for any downstream task:\n• “same row” identiﬁes the same row infor-\nmation without ordered row id embedding or\nglobal positional embedding, which help the\nmodel to be invariant to row perturbations,\n• “same column”, “header to column cell”, and\n“cell to column header” incorporates the same\ncolumn information without ordered column\nid embedding,\n• “cell to column header” makes each cell\naware of its column header without repeated\ncolumn header as features,\n• “header to sentence” and “cell to sentence”\nhelp column grounding and cell grounding of\nthe paired text,\n• “sentence to header” , “sentence to cell” , and\n“sentence to sentence” helps to understand the\nsentence with the table as context,\n• “header to same header” and “header to\nother header” for better understanding of ta-\nble schema, and “same cell bias” for cell con-\ntent understanding.\nNote that, each cell can still attend to other cells\nin the different columns or rows through “others”\ninstead of masking them out strictly.\nWe assign each bias type a learnable scalar,\nwhich will serve as a bias term in the self-attention\nmodule. Speciﬁcally, each self-attention head\nin each layer have a set of learnable scalars\n{b1,b2,··· ,b13}corresponding to all types of in-\ntroduced biases. For one head in one self-attention\nsub-layer of TABLE FORMER , Equation 2 in the\nTransformer is replaced by:\n¯A= QK⊤\n√dK\n, A = ¯A+ ˆA (3)\nAttn(H) = softmax(A)V (4)\nwhere ¯Ais a matrix capturing the similarity be-\ntween queries and keys, ˆA is the Attention Bias\nMatrix, and ˆAi,j = bφ(vi,vj).\nRelation between TABLE FORMER and ETC.\nETC (Ainslie et al., 2020) uses vectors to repre-\nsent relative position labels, although not directly\napplied to table-text pairs due to its large computa-\ntional overhead (Eisenschlos et al., 2021). TABLE -\nFORMER differs from ETC in the following as-\npects (1) ETC uses relative positional embeddings\nwhile TABLE FORMER uses attention bias scalars.\nIn practice, we observed that using relative posi-\ntional embeddings increases training time by more\nthan 7x, (2) ETC uses global memory and local at-\ntention, while TABLE FORMER uses pairwise atten-\ntion without any global memory overhead, (3) ETC\nuses local sparse attention with masking, limiting\nits ability to attend to all tokens, (4) ETC did not\nexplore table-text attention bias types exhaustively.\nAnother table encoding model MATE (Eisensch-\nlos et al., 2021) is vulnerable to row and column\nperturbations, and shares limitation (3) and (4).\n4 Experimental Setup\n4.1 Datasets and Evaluation\nWe use the following datasets in our experiments.\nTable Question Answering. For the table QA\ntask, we conducted experiments on WikiTableQues-\ntions (WTQ ) (Pasupat and Liang, 2015) and Se-\nquential QA (SQA) (Iyyer et al., 2017) datasets.\nWTQ was crowd-sourced based on complex ques-\ntions on Wikipedia tables. SQA is composed of\n6,066 question sequences (2.9 question per se-\nquence on average), constructed by decomposing a\nsubset of highly compositional WTQ questions.\nTable-Text Entailment. For the table-text en-\ntailment task, we used TABFACT dataset (Chen\net al., 2020), where the tables were extracted from\nWikipedia and the sentences were written by crowd\nworkers. Among total 118, 000 sentences, each\none is a positive (entailed) or negative sentence.\nPerturbation Evaluation Set. For SQA and\nTABFACT, we also created new test sets to measure\nmodels’ robustness to answer-invariant row and col-\numn perturbations during inference. Speciﬁcally,\n531\nrow and column orders are randomly perturbed for\nall tables in the standard test sets.2\nPre-training All the models are ﬁrst tuned on\nthe Wikipidia text-table pretraining dataset (Herzig\net al., 2020), optionally tuned on synthetic dataset\nat an intermediate stage (“inter”) (Eisenschlos et al.,\n2020), and ﬁnally ﬁne-tuned on the target dataset.\nTo get better performance on WTQ , we follow\nHerzig et al. (2020) to further pretrain on SQA\ndataset after the intermediate pretraining stage in\nthe “inter-sqa” setting.\nEvaluation For SQA, we report the cell selection\naccuracy for all questions (ALL) using the ofﬁcial\nevaluation script, cell selection accuracy for all se-\nquences (SEQ), and the denotation accuracy for all\nquestions (ALLd). To evaluate the models’ robust-\nness in the instance level after perturbations, we\nalso report a lower bound of example prediction\nvariation percentage:\nVP = (t2f + f2t)\n(t2t + t2f + f2t + f2f) (5)\nwhere t2t, t2f, f2t, and f2f represents how many ex-\nample predictions turning from correct to correct,\nfrom correct to incorrect, from incorrect to correct\nand from incorrect to incorrect, respectively, after\nperturbation. We report denotation accuracy on\nWTQ and binary classiﬁcation accuracy on TAB-\nFACT respectively.\n4.2 Baselines\nWe use TAPASBASE and TAPASLARGE as base-\nlines, where Transformer architectures are exactly\nsame as BERTBASE and BERTLARGE (Devlin\net al., 2019), and parameters are initialized from\nBERTBASE and BERTLARGE respectively. Cor-\nrespondingly, we have our TABLE FORMER BASE\nand TABLE FORMER LARGE, where attention bias\nscalars are initialized to zero, and all other pa-\nrameters are initialized from BERTBASE and\nBERTLARGE.\n4.3 Perturbing Tables as Augmented Data\nCould we alleviate the spurious ordering biases\nby data augmentation alone, without making any\nmodeling changes? To answer this, we train an-\nother set of models by augmenting the training data\n2We ﬁxed perturbation random seeds to make our results\nreproducible.\nBefore Perturb After Perturb\nALL SEQ ALL d ALL V P\nHerzig et al. (2020) 67.2 40.4 – – –\nEisenschlos et al. (2020) 71.0 44.8 – – –\nEisenschlos et al. (2021) 71.7 46.1 – – –\nLiu et al. (2021) – – 74.5 – –\nTAPASBASE 61.1 31.3 – 57.4 14.0%\nTABLE FORMER BASE 66.7 39.7 – 66.7 0.2%\nTAPASLARGE 66.8 39.9 – 60.5 15.1%\nTABLE FORMER LARGE 70.3 44.8 – 70.3 0.1%\nTAPASBASE inter 67.5 38.8 – 61.0 14.3%\nTABLE FORMER BASE inter 69.4 43.5 – 69.3 0.1%\nTAPASLARGE inter 70.6 43.9 – 66.1 10.8%\nTABLE FORMER LARGE inter 72.4 47.5 75.9 72.3 0.1%\nTable 1: Results on SQA test set before and after per-\nturbation during inference (median of 5 runs). ALL is\ncell selection accuracy, SEQ is cell selection accuracy\nfor all question sequences, ALL d is denotation accu-\nracy for all questions (reported to compare with Liu\net al. (2021)). VP is model prediction variation per-\ncentage after perturbation. Missing values are those not\nreported in the original paper.\nfor TAPAS through random row and column order\nperturbations.3\nFor each table in the training set, we randomly\nshufﬂe all rows and columns (including corre-\nsponding column headers), creating a new table\nwith the same content but different orders of rows\nand columns. Multiple perturbed versions of the\nsame table were created by repeating this process\n{1,2,4,8,16}times with different random seeds.\nFor table QA tasks, selected cell positions are also\nadjusted as ﬁnal answers according to the perturbed\ntable. The perturbed table-text pairs are then used\nto augment the data used to train the model. During\ntraining, the model takes data created by one spe-\nciﬁc random seed in one epoch in a cyclic manner.\n5 Experiments and Results\nBesides standard testing results to compare TABLE -\nFORMER and baselines, we also answer the follow-\ning questions through experiments:\n• How robust are existing (near) state-of-the-\nart table-text encoding models to semantic\npreserving perturbations in the input?\n• How does TABLE FORMER compare with ex-\nisting table-text encoding models when tested\non similar perturbations, both in terms of per-\nformance and robustness?\n3By perturbation, we mean shufﬂing row and columns\ninstead of changing/swapping content blindly.\n532\nBefore Perturb After Perturb\ndev test test simple testcomplex testsmall test test simple testcomplex testsmall\nEisenschlos et al. (2020) 81.0 81.0 92.3 75.6 83.9 – – – –\nEisenschlos et al. (2021) – 81.4 – – – – – – –\nTAPASBASE 72.8 72.3 84.8 66.2 74.4 71.2 83.4 65.2 72.5\nTABLE FORMER BASE 75.1 75.0 88.2 68.5 77.1 75.0 88.2 68.5 77.1\nTAPASLARGE 74.7 74.5 86.6 68.6 76.8 73.7 86.0 67.7 76.1\nTABLE FORMER LARGE 77.2 77.0 90.2 70.5 80.3 77.0 90.2 70.5 80.3\nTAPASBASE inter 78.4 77.9 90.1 71.9 80.5 76.8 89.5 70.5 79.7\nTABLE FORMER BASE inter 79.7 79.2 91.6 73.1 81.7 79.2 91.6 73.1 81.7\nTAPASLARGE inter 80.6 80.6 92.0 74.9 83.1 79.2 91.7 73.0 83.0\nTABLE FORMER LARGE inter 82.0 81.6 93.3 75.9 84.6 81.6 93.3 75.9 84.6\nTable 2: Binary classiﬁcation accuracy on T ABFACT development and 4 splits of test set, as well as performance\non test sets with our perturbation evaluation. Median of 5 independent runs are reported. Missing values are those\nnot reported in the original paper.\nModel dev test\nHerzig et al. (2020) – 48.8\nEisenschlos et al. (2021) – 51.5\nTAPASBASE 23.6 24.1\nTABLE FORMER BASE 34.4 34.8\nTAPASLARGE 40.8 41.7\nTABLE FORMER LARGE 42.5 43.9\nTAPASBASE inter-sqa 44.8 45.1\nTABLE FORMER BASE inter-sqa 46.7 46.5\nTAPASLARGE inter-sqa 49.9 50.4\nTABLE FORMER LARGE inter-sqa 51.3 52.6\nTable 3: Denotation accuracy on WTQ development\nand test set. Median of 5 independent runs are reported.\n• Can we use perturbation based data augmen-\ntation to achieve robustness at test time?\n• Which attention biases in TABLE FORMER\ncontribute the most to performance?\n5.1 Main Results\nTable 1, 2, and 3 shows TABLE FORMER perfor-\nmance on SQA, T ABFACT, and WTQ , respec-\ntively. As can be seen, TABLE FORMER outper-\nforms corresponding TAPAS baseline models in all\nsettings on SQA and WTQ datasets, which shows\nthe general effectiveness of TABLE FORMER ’s\nstructural biases in Table QA datasets. Speciﬁ-\ncally, TABLE FORMER LARGE combined with inter-\nmediate pretraining achieves new state-of-the-art\nperformance on SQA dataset.\nSimilarly, Table 2 shows that TABLE FORMER\nalso outperforms TAPAS baseline models in all set-\ntings, which shows the effectiveness of TABLE -\nFORMER in the table entailment task. Note that,\nLiu et al. (2021) is not comparable to our results, be-\ncause they used different pretraining data, different\npretraining objectives, and BART NLG model in-\nstead of BERT NLU model. But TABLE FORMER\nattention bias is compatible with BART model.\n5.2 Perturbation Results\nOne of our major contributions is to systematically\nevaluate models’ performance when facing row and\ncolumn order perturbation in the testing stage.\nIdeally, model predictions should be consistent\non table QA and entailment tasks when facing such\nperturbation, because the table semantics remains\nthe same after perturbation.\nHowever, in Table 1 and 2, we can see that in our\nperturbed test set, performance of all TAPAS mod-\nels drops signiﬁcantly in both tasks. TAPAS models\ndrops by at least 3.7% and up to 6.5% in all settings\non SQA dataset in terms of ALL accuracy, while\nour TABLE FORMER being strictly invariant to such\nrow and column order perturbation leads to no drop\nin performance.4 Thus, in the perturbation setting,\nTABLE FORMER outperforms all TAPAS baselines\neven more signiﬁcantly, with at least 6.2% and\n2.4% improvements on SQA andTABFACT dataset,\nrespectively. In the instance level, we can see that,\nwith TAPAS , there are many example predictions\nchanged due to high VP , while there is nearly no\nexample predictions changed with TABLE FORMER\n(around zero VP ).\n4In SQA dataset, there is at most absolute 0.1% perfor-\nmance drop because of some bad data point issues. Speciﬁ-\ncally, some columns in certain tables are exactly the same, but\nthe ground-truth selected cells are in only one of such columns.\nTABLE FORMER would select from one column randomly.\n533\nModel Number of parameters\nTAPASBASE 110 M\nTABLE FORMER BASE\n110 M - 2*512*768\n+ 12*12*13 =\n110 M - 0.8 M + 0.002 M\nTAPASLARGE 340 M\nTABLE FORMER LARGE\n340 M - 2*512*1024\n+ 24*16*13 =\n340 M - 1.0 M + 0.005M\nTable 4: Model size comparison.\n5.3 Model Size Comparison\nWe compare the model sizes of TABLE FORMER\nand TAPAS in Table 4. We added only a few atten-\ntion bias scalar parameters (13 parameters per head\nper layer) in TABLE FORMER , which is negligible\ncompared with the BERT model size. Meanwhile,\nwe delete two large embedding metrics (512 row\nids and 512 column ids). Thus, TABLE FORMER\noutperforms TAPAS with fewer parameters.\n5.4 Analysis of TABLE FORMER Submodules\nIn this section, we experiment with several variants\nof TABLE FORMER to understand the effectiveness\nof its submodules. The performance of all variants\nof TAPAS and TABLE FORMER that we tried on the\nSQA development set is shown in Table 5.\nLearnable Attention Biases v/s Masking. In-\nstead of adding learnable bias scalars, we mask out\nsome attention scores to restrict attention to those\ntokens in the same columns and rows, as well as\nthe paired sentence, similar to Zhang et al. (2020)\n(SAT). We can see that TAPASBASE-SAT performs\nworse than TAPASBASE, which means that restrict-\ning attention to only same columns and rows by\nmasking reduce the modeling capacity. This led to\nchoosing soft bias addition over hard masking.\nAttention Bias Scaling. Unlike TABLE -\nFORMER , we also tried to add attention biases\nbefore the scaling operation in the self-attention\nmodule (SO). Speciﬁcally, we compute pair-wise\nattention score by:\nAij =\n(h⊤\ni WQ)(h⊤\nj WK)⊤+ ˆAij\n√dK\n(6)\ninstead of using:\nAij =\n(h⊤\ni WQ)(h⊤\nj WK)⊤\n√dK\n+ ˆAij, (7)\nrc-gp c-gp gp pcp\nTAPASBASE 57.6 47.4 46.4 29.1\nTAPASBASE-SAT 45.2 - - -\nTABLE FORMER BASE-SO 60.0 60.2 59.8 60.7\nTABLE FORMER BASE 62.2 61.5 61.7 61.9\nTable 5: ALL questions’ cell selection accuracy of\nTABLE FORMER variants on SQA development set. rc-\ngp represents the setting including row ids, column\nids and global positional ids, c-gp represents column\nids and global positional ids, gp represents global po-\nsitional ids, and pcp represents per-cell positional ids.\n“SAT” represents masking out some attention scores.\n“SO” represents adding attention bias before scaling.\nwhich is the element-wise version of Equa-\ntion 1 and 3. However, Table 5 shows\nthat TABLE FORMER BASE-SO performs worse than\nTABLE FORMER BASE, showing the necessity of\nadding attention biases after the scaling operation.\nWe think the reason is that the attention bias term\ndoes not require scaling, because attention bias\nscalar magnitude is independent of dK, while the\ndot products grow large in magnitude for large val-\nues of dK. Thus, such bias term could play an\nmore important role without scaling, which helps\neach attention head know clearly what to pay more\nattention to according to stronger inductive biases.\nRow, Column, & Global Positional IDs.\nWith TAPASBASE, TABLE FORMER BASE-SO, and\nTABLE FORMER BASE, we ﬁrst tried the full-version\nwhere row ids, column ids, and global positional\nids exist as input ( rc-gp). Then, we deleted row\nids (c-gp), and column ids ( gp) sequentially. Fi-\nnally, we changed global positional ids in gp to\nper-cell positional ids ( pcp). Table 5 shows that\nTAPASBASE performs signiﬁcantly worse from rc-\ngp →c-gp →gp →pcp, because table structure in-\nformation are deleted sequentially during such pro-\ncess. However, with TABLE FORMER BASE, there is\nno obvious performance drop during the same pro-\ncess. That shows the structural inductive biases in\nTABLE FORMER can provide complete table struc-\nture information. Thus, row ids, column ids and\nglobal positional ids are not necessary in TABLE -\nFORMER . We pick TABLE FORMER pcp setting as\nour ﬁnal version to conduct all other experiments in\nthis paper. In this way, TABLE FORMER is strictly\ninvariant to row and column order perturbation by\navoiding spurious biases in those original ids.\n534\nBefor Perturb After Perturb\nALL SEQ ALL V P\nTAPASBASE 61.1 31.3 57.4 14.0%\nTAPASBASE 1p 63.4 34.6 63.4 9.9%\nTAPASBASE 2p 64.6 35.6 64.5 8.4%\nTAPASBASE 4p 65.1 37.0 65.0 8.1%\nTAPASBASE 8p 65.1 37.3 64.3 7.2%\nTAPASBASE 16p 62.4 33.6 62.2 7.0%\nTABLE FORMER BASE 66.7 39.7 66.7 0.1%\nTable 6: Comparison of TABLE FORMER and perturbed\ndata augmentation on SQA test set, where VP repre-\nsents model prediction variation percentage after per-\nturbation. Median of 5 independent runs are reported.\n5.5 Comparison of TABLE FORMER and\nPerturbed Data Augmentation\nAs stated in Section 4.3, perturbing row and col-\numn orders as augmented data during training can\nserve as another possible solution to alleviate the\nspurious row/column ids bias. Table 6 shows the\nperformance of TABPAS BASE model trained with\nadditional {1, 2, 4, 8, 16} perturbed versions of\neach table as augmented data.\nWe can see that the performance of TAPASBASE\non SQA dataset improves with such augmentation.\nAlso, as the number of perturbed versions of each\ntable increases, model performance ﬁrst increases\nand then decreases, reaching the best results with\n8 perturbed versions. We suspect that too many\nversions of the same table confuse the model about\ndifferent row and column ids for the same table,\nleading to decreased performance from 8p to 16p.\nDespite its usefulness, such data perturbation is\nstill worse than TABLE FORMER , because it could\nnot incorporate other relevant text-table structural\ninductive biases like TABLE FORMER .\nAlthough, such data augmentation makes the\nmodel more robust to row and column order per-\nturbation with smaller VP compared to standard\nTAPASBASE, there is still a signiﬁcant prediction\ndrift after perturbation. As shown in Table 6, VP\ndecreases from 1p to 16p, however, the best VP\n(7.0%) is still much higher than (nearly) no varia-\ntion (0.1%) of TABLE FORMER .\nTo sum up, TABLE FORMER is superior to row\nand column order perturbation augmentation, be-\ncause of its additional structural biases and strictly\nconsistent predictions after perturbation.\nALL SEQ\nTABLE FORMER BASE 62.1 38.4\n- Same Row 32.1 2.8\n- Same Column 62.1 37.7\n- Same Cell 61.8 38.4\n- Cell to Column Header 60.7 36.6\n- Cell to Sentence 60.5 36.4\n- Header to Column Cell 60.5 35.8\n- Header to Other Header 60.6 35.8\n- Header to Same Header 61.0 36.9\n- Header to Sentence 61.1 36.3\n- Sentence to Cell 60.8 36.2\n- Sentence to Header 61.0 37.3\n- Sentence to Sentence 60.0 35.3\n- All Column Related (# 2, 4, 6) 54.5 29.3\nTable 7: Ablation study of proposed attention biases.\n5.6 Attention Bias Ablation Study\nWe conduct ablation study to demonstrate the util-\nity of all 12 types of deﬁned attention biases. For\neach ablation, we set the corresponding attention\nbias type id to “others” bias id. Table 7 shows\nTAPASBASE’s performance SQA dev set. Over-\nall, all types of attention biases help the TABLE -\nFORMER performance to some extent, due to cer-\ntain performance drop after deleting each bias type.\nAmongst all the attention biases, deleting “same\nrow” bias leads to most signiﬁcant performance\ndrop, showing its crucial role for encoding table\nrow structures. There is little performance drop\nafter deleting “same column” bias, that’s because\nTABLE FORMER could still infer the same column\ninformation through “cell to its column header”\nand “header to its column cell” biases. After\ndeleting all same column information (“same col-\numn”, “cell to column header” and “header to col-\numn cell” biases), TABLE FORMER performs signif-\nicantly worse without encoding column structures.\nSimilarly, there is little performance drop after\ndeleting “same cell” bias, because TABLE FORMER\ncan still infer same cell information through “same\nrow” and “same column” biases.\n5.7 Limitations of TABLE FORMER\nTABLE FORMER increases the training time by\naround 20%, which might not be ideal for very\nlong tables and would require a scoped approach.\nSecondly, with the strict row and column order in-\nvariant property, TABLE FORMER cannot deal with\nquestions based on absolute orders of rows in ta-\nbles. This however is not a practical requirement\nbased on the current dataset. Doing a manual study\nof 1800 questions in SQA dataset, we found that\n535\nthere are 4 questions 5 (0.2% percentage) whose\nanswers depend on orders of rows. Three of them\nasked “which one is at the top of the table” , an-\nother asks “which one is listed ﬁrst” . However,\nthese questions could be potentially answered by\nadding back row and column order information\nbased on TABLE FORMER .\n6 Other Related Work\nTransformers for Tabular Data. Yin et al.\n(2020) prepended corresponding column headers\nto cells contents, and Chen et al. (2020) used cor-\nresponding column headers as features for cells.\nHowever, such methods encode each table header\nmultiple times, leading to duplicated computing\noverhead. Also, tabular structures (e.g. same row\ninformation) are not fully incorporated to such mod-\nels. Meanwhile, Yin et al. (2020) leveraged row\nencoder and column encoder sequentially, which\nintroduced much computational overhead, thus re-\nquiring retrieving some rows as a preprocessing\nstep. Finally, SAT (Zhang et al., 2020), Deng\net al. (2021) and Wang et al. (2021) restricted atten-\ntion to same row or columns with attention mask,\nwhere such inductive bias is too strict that cells\ncould not directly attend to those cells in different\nrow and columns, hindering the modeling ability\naccording to Table 5. Liu et al. (2021) used the\nseq2seq BART generation model with a standard\nTransformer encoder-decoder architecture. In all\nmodels mentioned above, spurious inter-cell or-\nder biases still exist due to global positional ids\nof Transformer, leading to the vulnerability to row\nor column order perturbations, while our TABLE -\nFORMER could avoid such problem. Mueller et al.\n(2019) and Wang et al. (2020) also used relative\npositional encoding to encode table structures, but\nthey modeled the relations as learnable relation vec-\ntors, whose large overhead prevented pretraining\nand led to poor performance without pretraining,\nsimilarly to ETC (Ainslie et al., 2020) explained in\nSection 3.\nStructural and Relative Attention. Modiﬁed\nattention scores has been used to model relative\npositions (Shaw et al., 2018), long documents (Dai\net al., 2019; Beltagy et al., 2020; Ainslie et al.,\n2020), and graphs (Ying et al., 2021). But adding\n5We ﬁnd such 4 questions by manually looking at\nall 125 questions where the model predictions turn from\ncorrect to incorrect after replacing TAPASLARGE with\nTABLE FORMER LARGE.\nlearnable attention biases to model tabular struc-\ntures has been under-explored.\n7 Conclusion\nIn this paper, we identiﬁed the vulnerability of\nprior table encoding models along two axes: (a)\ncapturing the structural bias, and (b) robustness\nto row and column perturbations. To tackle\nthis, we propose TABLE FORMER , where learnable\ntask-independent learnable structural attention bi-\nases are introduced, while making it invariant to\nrow/column order at the same time. Experimental\nresults showed that TABLE FORMER outperforms\nstrong baselines in 3 table reasoning tasks, achiev-\ning state-of-the-art performance on SQA dataset,\nespecially when facing row and column order per-\nturbations, because of its invariance to row and\ncolumn orders.\nAcknowledgments\nWe thank Julian Eisenschlos, Ankur Parikh, and\nthe anonymous reviewers for their feedbacks in\nimproving this paper.\nEthical Considerations\nThe authors foresee no ethical concerns with the\nresearch presented in this paper.\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs\nin transformers. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 268–284, Online. Asso-\nciation for Computational Linguistics.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2020. Tabfact : A large-scale\ndataset for table-based fact veriﬁcation. In Inter-\nnational Conference on Learning Representations\n(ICLR), Addis Ababa, Ethiopia.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\n536\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2021. TURL: Table Understanding\nthrough Representation Learning. In VLDB.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJulian Eisenschlos, Maharshi Gor, Thomas Müller, and\nWilliam Cohen. 2021. MATE: Multi-view attention\nfor table transformer efﬁciency. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 7606–7619, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nJulian Eisenschlos, Syrine Krichene, and Thomas\nMüller. 2020. Understanding tables with interme-\ndiate pre-training. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n281–296, Online. Association for Computational\nLinguistics.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4320–4333, Online. Association for\nComputational Linguistics.\nMohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.\nSearch-based neural structured learning for sequen-\ntial question answering. In Proceedings of the\n55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1821–1831, Vancouver, Canada. Association\nfor Computational Linguistics.\nQian Liu, Bei Chen, Jiaqi Guo, Zeqi Lin, and Jian-\nguang Lou. 2021. Tapex: Table pre-training via\nlearning a neural sql executor. arXiv preprint\narXiv:2107.07653.\nThomas Mueller, Francesco Piccinno, Peter Shaw,\nMassimo Nicosia, and Yasemin Altun. 2019. An-\nswering conversational questions on structured data\nwithout logical forms. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5902–5910, Hong Kong,\nChina. Association for Computational Linguistics.\nPanupong Pasupat and Percy Liang. 2015. Compo-\nsitional semantic parsing on semi-structured tables.\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n1470–1480, Beijing, China. Association for Compu-\ntational Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2020. RAT-SQL:\nRelation-aware schema encoding and linking for\ntext-to-SQL parsers. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7567–7578, Online. Association\nfor Computational Linguistics.\nZhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi\nFu, Shi Han, and Dongmei Zhang. 2021. TUTA:\nTree-based Transformers for Generally Structured\nTable Pre-training. In Proceedings of the 27th ACM\nSIGKDD Conference on Knowledge Discovery &\nData Mining, pages 1780–1790.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\nbastian Riedel. 2020. TaBERT: Pretraining for joint\nunderstanding of textual and tabular data. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8413–\n8426, Online. Association for Computational Lin-\nguistics.\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin\nZheng, Guolin Ke, Di He, Yanming Shen, and Tie-\nYan Liu. 2021. Do Transformers Really Perform\nBad for Graph Representation? arXiv preprint\narXiv:2106.05234.\nTao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin\nWang, Yi Chern Tan, Xinyi Yang, Dragomir Radev,\nRichard Socher, and Caiming Xiong. 2020. GraPPa:\nGrammar-Augmented Pre-Training for Table Se-\nmantic Parsing. arXiv preprint arXiv:2009.13845.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact veriﬁcation with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP) , pages 1624–1629, Online. Associa-\ntion for Computational Linguistics.\n537"
}