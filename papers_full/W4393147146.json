{
  "title": "Can Large Language Models Understand Real-World Complex Instructions?",
  "url": "https://openalex.org/W4393147146",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2103044703",
      "name": "Qianyu He",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A1982654681",
      "name": "Jie Zeng",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2116110812",
      "name": "Wenhao Huang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2044350115",
      "name": "Lina Chen",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2098193652",
      "name": "Jin Xiao",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2795538134",
      "name": "Qianxi He",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A5111153523",
      "name": "Xunzhe Zhou",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2161872403",
      "name": "Jiaqing Liang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2131222654",
      "name": "Yanghua Xiao",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2103044703",
      "name": "Qianyu He",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A1982654681",
      "name": "Jie Zeng",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2116110812",
      "name": "Wenhao Huang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2044350115",
      "name": "Lina Chen",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2098193652",
      "name": "Jin Xiao",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2795538134",
      "name": "Qianxi He",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A5111153523",
      "name": "Xunzhe Zhou",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2161872403",
      "name": "Jiaqing Liang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2131222654",
      "name": "Yanghua Xiao",
      "affiliations": [
        "Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2620787630",
    "https://openalex.org/W6610928041",
    "https://openalex.org/W4385009323",
    "https://openalex.org/W4379539933",
    "https://openalex.org/W4312091623",
    "https://openalex.org/W4366559971",
    "https://openalex.org/W4384816576",
    "https://openalex.org/W4385570984",
    "https://openalex.org/W4362508448",
    "https://openalex.org/W4385474529",
    "https://openalex.org/W4378770815",
    "https://openalex.org/W4367369699",
    "https://openalex.org/W4389518784",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4380994495",
    "https://openalex.org/W4365601026",
    "https://openalex.org/W4378468563",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4376653782",
    "https://openalex.org/W4380355783",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4380993527",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4385682136",
    "https://openalex.org/W4389519291",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4380136141",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4384648604",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W4377297670"
  ],
  "abstract": "Large language models (LLMs) can understand human instructions, showing their potential for pragmatic applications beyond traditional NLP tasks. However, they still struggle with complex instructions, which can be either complex task descriptions that require multiple tasks and constraints, or complex input that contains long context, noise, heterogeneous information and multi-turn format. Due to these features, LLMs often ignore semantic constraints from task descriptions, generate incorrect formats, violate length or sample count constraints, and be unfaithful to the input text. Existing benchmarks are insufficient to assess LLMs’ ability to understand complex instructions, as they are close-ended and simple. To bridge this gap, we propose CELLO, a benchmark for evaluating LLMs' ability to follow complex instructions systematically. We design eight features for complex instructions and construct a comprehensive evaluation dataset from real-world scenarios. We also establish four criteria and develop corresponding metrics, as current ones are inadequate, biased or too strict and coarse-grained. We compare the performance of representative Chinese-oriented and English-oriented models in following complex instructions through extensive experiments. Resources of CELLO are publicly available at https://github.com/Abbey4799/CELLO.",
  "full_text": "Can Large Language Models Understand Real-World Complex Instructions?\nQianyu He1, Jie Zeng1, Wenhao Huang1, Lina Chen2, Jin Xiao2, Qianxi He1, Xunzhe Zhou1,\nJiaqing Liang2*, Yanghua Xiao1,3*\n1Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n2School of Data Science, Fudan University\n3Fudan-Aishu Cognitive Intelligence Joint Research Center, Shanghai, China\n{qyhe21, jzeng23, whhuang21, lnchen23, jinxiao23, qxhe23}@m.fudan.edu.cn,\n{xzzhou20, liangjiaqing, shawyh}@fudan.edu.cn\nAbstract\nLarge language models (LLMs) can understand human in-\nstructions, showing their potential for pragmatic applications\nbeyond traditional NLP tasks. However, they still struggle\nwith complex instructions, which can be either complex task\ndescriptions that require multiple tasks and constraints, or\ncomplex input that contains long context, noise, heteroge-\nneous information and multi-turn format. Due to these fea-\ntures, LLMs often ignore semantic constraints from task de-\nscriptions, generate incorrect formats, violate length or sam-\nple count constraints, and be unfaithful to the input text. Ex-\nisting benchmarks are insufficient to assess LLMs’ ability\nto understand complex instructions, as they are close-ended\nand simple. To bridge this gap, we propose CELLO, a bench-\nmark for evaluating LLMs’ ability to follow complex in-\nstructions systematically. We design eight features for com-\nplex instructions and construct a comprehensive evaluation\ndataset from real-world scenarios. We also establish four cri-\nteria and develop corresponding metrics, as current ones are\ninadequate, biased or too strict and coarse-grained. We com-\npare the performance of representative Chinese-oriented and\nEnglish-oriented models in following complex instructions\nthrough extensive experiments. Resources ofCELLO are pub-\nlicly available at https://github.com/Abbey4799/CELLO.\nIntroduction\nThe emergence of large-scale models (Brown et al.\n2020; Chowdhery et al. 2022; Touvron et al. 2023) has\nyielded noteworthy transformations in real-world applica-\ntions (Richards 2023; Liu et al. 2023b). These models are\nable to understand a wide range of human instructions, span-\nning from casual conversations (Taori et al. 2023) to com-\nplex problems solving (Brown et al. 2020). Since human\ninstructions are massive and diverse, traditional academic\nbenchmarks that focus on specific tasks are no longer suffi-\ncient to evaluate LLMs (Zhong et al. 2023; Chia et al. 2023).\nReal-world applications often involve a diverse range of\ncomplex instructions that significantly differ from the simple\nand common instructions in current benchmarks (Hendrycks\net al. 2020; Huang et al. 2023), as shown in Fig. 1. Instruc-\ntion generally consists of two parts (Honovich et al. 2022):\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nInput Text (histories of multi-round dialogue)\n| Brand| Characteristics| Flavors||-------|-----------------|---------|| Starbucks| A globally renowned…|…|| Nestle| A world-leading food…|…\nList fivedifferent brands of coffee and describe theircharacteristics and flavors. Output in table format, including brand, characteristics, and flavors.\nTask DescriptionAdd ‘Origin’ info. in the above table.HumanAssistant\n| Brand | Characteristics | Flavors | Origin｜|-------|-----------------|---------|--------｜| Blue Mountain | A well-known…|…|…|…Unfaithful toInput text\n| Brand | Characteristics | Flavors ||-------|-----------------|---------|| Starbucks | A globally renowned…|…|… IgnoreTask Description\n| Brand | Characteristics | Flavors |Origin｜|-------|-----------------|---------|--------｜| Starbucks | A globally renowned…|…|…|<eos> WrongSample Count\nStarbucks originates from the United States, while Nestlé…WrongFormat\nInstruction in Real-World Scenarios\nInstructions in Existing BenchmarksFind the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.  [A]0  [B]4  [C]2 [D]6Repeat the word cat four times. After the second time, also say the word meow.MMLUBBH\nFigure 1: Existing benchmarks generally contain simple and\ncommon instructions. However, the complex instructions in\nreal-world scenarios are a composition of multiple features,\nsuch as constraints on the output format, number of output\nsamples, key elements of the output, and heterogeneity of\ninput texts in the given example. The understanding of com-\nplex instructions poses challenges to current models.\nTask description(mandatory) describes the task goal andIn-\nput text (optional) provides reference texts for the model\nto answer questions or the history of multi-turn conversa-\ntions, as shown in Fig. 1. Hence, there can be two cate-\ngories of complex instructions: complex task descriptions\nand complex input. Regarding complex task descriptions,\nmodels need to undertake multiple tasks (i.e. multi-tasking)\nand there can be diverse restrictions describing the task, in-\ncluding semantics constraints (e.g. the inclusion of key ele-\nments (Zhou et al. 2023a) or the use of predefined callable\nfunctions (Liu et al. 2023b)), format constraints(e.g. the pre-\ndefined format in few-shot scenarios (Yao et al. 2023b) or\nstructured format imitating human reasoning processes (Liu\net al. 2023b)), quantity constraints (e.g. word, sentence, or\nsample count regulating the length of model output (Zhou\net al. 2023b; Yao et al. 2023a)). Regarding complex input,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18188\nCriterion: resolvelimit: jsonCriterion: keywordslimit: [ “{“ , “}” ]Criterion: keywordslimit: [“time”, “magnitude”..]Criterion: keywordslimit: [”November 14, 2008”..]\nAnswer FormatTask-prescribedPhrasesInputDependentQueryCount LimitCriterion: N/Alimit: N/ACriterion: keywordslimit: [ “|“ , “-” , “\\n” ]Criterion: keywordslimit: [“Origin”]Criterion: keywordslimit: [“Starbucks”, “Brand”]\nAnswer FormatTask-prescribedPhrasesInputDependentQueryCount LimitCriterion: sample-tablelimit: 5\nEvaluation System\nCriterion: COPY-BLEU\nCriterion: COPY-BLEU\nCase 1\nCase 2\nExtract all earthquake-related information from the following news, including time, location, magnitude, depth of the epicenter, and epicenter position. And output in Jsonformat.\nAdd ‘Origin’ info. in the above table.\nList fivedifferent brands of coffee and describe their characteristics and flavors separately. Output in table format, including brand, characteristics, and flavors.\nInput Text (histories of multi-round dialogue)\nHumanAssistant\nAccording to the China Earthquake Networks Center, at 06:53on November 14, 2008 Beijing time, a 4.3 magnitude earthquake occurred in HaichengCity, Liaoning Province…\nTask Description\nInput Text\nTask Description\nTask Description\nInput TextGiven the SQL text, What is the salary of record with primeKeyf ......Merge the above two jsoninto a new json.\n...Help me write an introduction within 100 words.…Consider dividing them into shorter and simpler sentences....\nFormatsConstraints…Output format:{\"need_knowledge\": \"<yes or no>\",\"thought\":\"<your thought process>\",\"code\": \"def search():messages =…\nSemanticsConstraintsGiven the candidate relationships: ['Participant', 'Winner'], extract …… using the functions :1. get_entity_info(entity_aliases): Get …\nQuantityConstraints\nMuti-Tasking...Translatethe above jsontext into English and mergethe answers in Chinese and English into one json.\nFeatures for Complex InstructionsDataset Construction\nReference the following 3 paragraphs in this article related to the problem, and then answer the questions below:...(2024 words)\nHeterogeneousInformationLong ContextNoiseConversation content: Speaker 1: Hello. Speaker 2: Uh,Nanning, I'm the one who just consulted you about the customer group of Futian…Multi-turnExpand and describe the first person, including his background and characteristics. | Brand| Characteristics| Flavors||-------|-----------------|---------|| Starbucks| A globally renowned…|\nFigure 2: The framework of our benchmark design contains eight features for complex instructions, an evaluation dataset\ncovering nine tasks, and four evaluation criteria along with their corresponding metrics.\nthe input text generally have long context (An et al. 2023;\nLiu et al. 2023a), noise (e.g. colloquial expressions (Guo\net al. 2023) and error accumulation caused by pipeline\nmethod (Sun et al. 2023)), heterogeneous information (e.g.\na combination of structured and unstructured data (Zha et al.\n2023)), and in the form of multi-turn (Ding et al. 2023).\nThe complexity of real-world instructions accounts for\nprevalent errors observed in LLMs. As shown in Fig. 1,\nLLMs may (1) ignore semantic constraints from task de-\nscription(s) (Zhou et al. 2023a), (2) generate answers in in-\ncorrect format (Qin et al. 2023), or (3) violate the length\nor sample count constraints (Zhou et al. 2023b), especially\nwhen multiple tasks are required to be performed. More-\nover, models can (4) be unfaithful to the input text, espe-\ncially when it is long, noisy, heterogeneous or in the form of\nmulti-turn (Li et al. 2023b; An et al. 2023). Overall, complex\ninstructions pose challenges to current models.\nHowever, existing benchmarks are insufficient for effec-\ntively assessing the ability of LLMs to understand complex\ninstructions. On one hand, Fig. 1 shows that existing bench-\nmarks are either close-ended (Huang et al. 2023; Zhong et al.\n2023; Yu et al. 2023) or contain common and simple instruc-\ntions (Srivastava et al. 2023; Chia et al. 2023; Dubois et al.\n2023), which fail to mirror the complexity of real-world in-\nstructions. On the other hand, even though certain bench-\nmarks cover some of the above features of complex instruc-\ntions, such as count restriction (Zhou et al. 2023b; Yao et al.\n2023a), semantic restriction (Chen et al. 2022), and long\ntext understanding (An et al. 2023), they only encompass\nisolated features, while real-world instructions comprehen-\nsively cover these features (Zhou et al. 2023a). Overall, none\nof the existing benchmarks systematically study the complex\ninstructions understanding ability of LLMs.\nIn this paper, we propose CELLO, a benchmark for eval-\nuating the ComplEx instruction understanding ability of\nLarge Language MOdels systematically. The framework of\nour benchmark is shown in Fig. 2. As existing benchmarks\nonly cover isolated features of complex instructions, we es-\ntablish a comprehensive framework comprising eight fea-\ntures of complex instructions. Accordingly, we propose a\nnovel evaluation system comprised of four criteria along\nwith their corresponding metrics. The current evaluation cri-\nteria are insufficient to comprehensively reflect the ability\nof LLMs to understand complex instructions for the follow-\ning reasons. First, complex instructions in real-world scenar-\nios are open-ended (Xu et al. 2023), thus the criteria com-\nmonly used for close-ended benchmarks are not suitable in\nsuch cases (Hendrycks et al. 2020). Moreover, many studies\nadopt GPT4 evaluation for automated open-ended assess-\nment, which introduces bias problems (Wang et al. 2023).\nFurthermore, the binary pass rate adopted by the bench-\nmarks containing complex instructions is strict and coarse-\ngrained, resulting in universally low scores for smaller LLM\nwithout discrimination (Liu et al. 2023b; Qin et al. 2023).\nOverall, our contributions are mainly four-fold:\n• To the best of our knowledge, we are the first to systemat-\nically investigate the ability of LLMs to follow complex\ninstructions. We propose a comprehensive set of features\nfor complex instructions, facilitating both dataset con-\nstruction and evaluation criteria design.\n• We construct a complex instruction dataset from real-\nworld scenarios, containing 523 samples encompassing\nnine tasks, effectively covering our specified features.\nSpecifically, we propose a two-stage framework for con-\nstructing the evaluation dataset for LLM’s complex in-\nstruction understanding.\n• We design four evaluation criteria and corresponding au-\ntomatic metrics for assessing LLMs’ ability to under-\nstand complex instructions in a comprehensive and dis-\ncriminative way.\n• We compare 19 representative Chinese-oriented models\nand 15 representative English-oriented models’ perfor-\nmance on our benchmark.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18189\nRelated Work\nEvaluation for LLMs Many benchmarks propose com-\nprehensive evaluation frameworks that integrate existing\nevaluation datasets (Liang et al. 2022; Zhong et al. 2023;\nDubois et al. 2023; Chia et al. 2023). Mainstream bench-\nmarks primarily focus on assessing knowledge (Huang et al.\n2023; Gu et al. 2023; Yu et al. 2023), programming (Chen\net al. 2021), and complex reasoning (Cobbe et al. 2021; Sri-\nvastava et al. 2023). Recently, many benchmarks focus on\nspecific capabilities of models (Qin et al. 2023; Liu et al.\n2023b; An et al. 2023). However, none of the existing bench-\nmarks systematically investigate the ability of LLMs to fol-\nlow complex instructions. Their evaluation criteria have sev-\neral limitations when evaluating complex instruction under-\nstanding. First, the close-ended benchmarks fail to mirror\nthe complexity of the real-world instructions (Huang et al.\n2023; Gu et al. 2023; Zhong et al. 2023). Also, the binary\nsuccess rate (Chen et al. 2021; Qin et al. 2023; Liu et al.\n2023b) is too strict and coarse-grained, resulting in weak\ndiscrimination. Moreover, GPT-4 automatic scoring intro-\nduces bias problems (Wang et al. 2023). Overall, the existing\nbenchmarks and their criteria are insufficient to effectively\nassess LLMs’ ability to understand complex instructions.\nComplex Instruction Following The current datasets\ngenerally have simple and common instructions, making\nLLMs challenging to follow complex instructions in real-\nworld scenarios (Zhou et al. 2023a; Xu et al. 2023). Vari-\nous methods have been proposed to improve models’ under-\nstanding of complex instructions (Luo et al. 2023; Zhou et al.\n2023a; Mukherjee et al. 2023). Despite the advancements,\nthere is a lack of a benchmark for systematically evaluating\nmodels’ understanding of complex instructions.\nEvaluation for Constrained Instructions Many studies\ninvestigate the ability of LLMs to understand constrained\ninstructions (Yao et al. 2023a; Zhou et al. 2023b; Chen et al.\n2022). However, the instructions of these benchmarks are\nsimplistic, and the constraints they involve are narrow.\nCELLO Benchmark\nAs shown in Fig. 2, we first establish a framework contain-\ning eight features for complex instructions, then construct an\nevaluation dataset, and finally propose four evaluation crite-\nria along with their corresponding metrics.\nDataset Construction\nWe first collect data from real scenarios, covering 9\ntasks. Then we diversify the collected complex instructions\nthrough In-breadth Evolution and complicate the collected\nsimple instructions through In-breadth Evolution.\nData Source and Selected TasksWhen constructing the\ndataset, we take into account its coverage and represen-\ntativeness. Regarding coverage, we include common NLP\ntasks found in existing benchmarks (Liang et al. 2022),\nwhile incorporating instructions with more complex task\ndescriptions or input beyond those benchmarks. More-\nover, we introduce specific tasks involving complex instruc-\ntions, which align with common real-world applications for\nLLMs. Regarding representativeness, instructions are gath-\nered from 90,000 user interaction logs over six months with\nour implemented chatbot. Finally, we include nine tasks,\nclassified into six categories:\nComplex NLP Tasks.Instructions concerning NLP tasks\nin real-world scenarios are more diverse and detailed (Xu\net al. 2023) and contain noisy and long contexts (An et al.\n2023) compared to academic datasets. Overall, we choose\nfour tasks commonly found in existing benchmarks (Liang\net al. 2022), enhancing them with more complex instructions\nand inputs beyond traditional benchmarks:long text summa-\nrization, long text closed-domain question answering,long\ntext keywords extraction, complex information extraction.\nThe details can be found in the Appendix.\nMeta-prompt. Researchers design elaborate prompts to\nleverage LLMs to construct datasets (Xu et al. 2023; Hon-\novich et al. 2022; Qin et al. 2023), which can be defined as\nMeta-prompts (Honovich et al. 2022). These prompts gener-\nally have varied instructions, rich input topics, few-shot sam-\nples, clear format requirements and are unlikely to appear in\nthe training samples. Therefore, we collect prompts crafted\nby domain experts who focus on various real-world appli-\ncations of LLMs, such as financial numerical reasoning and\neducational knowledge graph taxonomy construction, due to\ntheir high quality and origin in real-world scenarios.\nPlanning. Many studies have designed prompts to mimic\nhuman thinking processes, guiding LLMs to perform rea-\nsoning and planning (Yao et al. 2023b; Liu et al. 2023b).\nThese prompts often impose restrictions on callable func-\ntions, have clear format requirements, offer few-shot sam-\nples, and provide long contexts. Therefore, we collect\nprompts that require LLMs to complete planning tasks based\non CN-DBpedia (Xu et al. 2017), fund knowledge base, and\nthose from Langchain 1. Since smaller LLMs have limited\nplanning capabilities (Liu et al. 2023b), we solely evaluate\nthe models’ ability to perform single-step planning.\nStructured Input.Structured text is a common and cru-\ncial type of user input, due to its well-organized and eas-\nily interpretable format. Therefore, we include instructions\nwith: (1) Six structured data types, namely Markdown, La-\nTeX, SQL, Tree, Python, JSON. (2) Two distinct tasks\nfor their complexity and representativeness: Path Compose\ndirectly evaluates the model’s understanding of complex\nnested data structures, while TextRetrieval is a common ap-\nplication to extract content meeting specific requirements.\n(3) Two levels of difficulty, which are categorized based on\nthe length and depth of the structured input.\nWell-guided Writing.Existing benchmarks (Chia et al.\n2023) considering writing ability mainly have the follow-\ning limitations: (1) They overlook the specific needs users\nhave in real-world scenarios when seeking efficient writing\nguidance, such as word count, key information, or included\nhashtags. (2) They fail to consider the iterative nature of user\nsatisfaction, as users may continually provide modification\nfeedback. (3) They are difficult to automatically evaluate.\nTo address these limitations, we collect various single-turn\ncomplex instructions covering various complex features and\n1https://www.langchain.com/\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18190\nCategory Tasks #Samples #Format #Task #Input #Count Avg TD Len. Avg IP Len. Avg Ins Len.\nComplex\nTask\nDescription\nExtraction 49 49 35 49 N/A 125 169 295\nPlanning 52 52 46 48 N/A 1070 534 1606\nMeta. 20 20 15 6 2 765 166 933\nBS(S) 20 20 20 1 15 70 N/A 70\nWriting(S) 23 2 23 2 12 82 25 107\nComplex\nInput\nKeywords 15 15 15 15 N/A 546 943 1579\nQA 89 N/A N/A 89 N/A 25 881 814\nSum. 108 N/A N/A 108 N/A 45 514 562\nStructure 38 6 N/A 38 N/A 29 1360 1390\nBS(M) 52 50 50 10 36 31 559 31\nWriting(M) 57 3 35 48 43 30 656 51\nOverall 523 217 239 414 108 256 528 676\nTable 1: The statistics of our benchmark. For each task, #Format, #Task, #Input, #Count denote the number of samples covering\nthe corresponding criteria. Avg TD/IP/Ins Len. denote the average word number of task description,input textand instruction.\nMeta., BS, SUM. denote the Meta-prompt, Brainstorming, Summarization task respectively. (S) and (M) represent single-round\nand multi-round. N/A denotes that such tasks do not involve corresponding evaluation criteria.\nmulti-turn instructions that reflect realistic revision needs.\nDetailed Brainstorming. Brainstorming yields an intu-\nitive impression for the chat models. However, existing eval-\nuation datasets either have overly simple and open instruc-\ntions that are difficult to evaluate (Li et al. 2023a), or they\nare excessively tricky with limited discrimination 2. In our\nbenchmark, we collect single-turn brainstorming data with\ndetailed requirements and multi-turn brainstorming data that\nsimulate realistic user interactions.\nData Evolution The collected complex instructions have\ntwo limitations: (1) For those collected from real-world\nprojects, the human-elaborated task descriptions are com-\nplex but alike. (2) For those collected from usage logs, many\nsimple instructions are not effectively utilized. Hence, we\nintroduce two perspectives to evolve data, thereby achieving\na more robust and reliable evaluation. In-breadth Evolu-\ntion aims to diversify the collected complex instructions (in-\ncluding three methods task description relocation,task de-\nscription paraphrasingand task emulation).In-depth Evo-\nlution aims to complicate the simple instructions to increase\nthe data scale (including two methods constraints addition,\nmulti-round interaction). The motivation and prompts for\neach method are detailed in the Appendix.\nEvaluation System\nCriteria We define the following criteria that should be\nassessed as they can encompass common errors made by\nmodels. (1) Count limit: the number of words, sentences,\nor samples allowed in the response. (2) Answer format:\nthe expected structure or format of the response, such as a\nparsable JSON format, or a specified format for few-shot\nsamples. (3) Task-prescribed phrases: semantic constraints\non the response that are stipulated in the task description,\nsuch as predefined functions, primary subjects, or key el-\nements. (4) Input-dependent query: the query should be\nanswered faithfully according to the given input texts.\n2https://github.com/zhenbench/z-bench\nAlthough Task-prescribed phrases and Input-dependent\nquery both impose content-related constraints on the re-\nsponse, they differ in the information they rely on. The for-\nmer centers on constraints explicitly stated by the user in\nthe task description, while the latter focuses on constraints\nimplicitly derived from the content of the input text.\nEvaluation Metrics We propose automated evaluation\nmetrics for designed criteria, considering various perspec-\ntives and difficulty levels. Each sample si = {Ii, ai, hi}\nconsists of instruction Ii, a model answer ai and given\nhistories3 hi = {(I0, a′\n0), ...,(Ii−1, a′\ni−1)}. Here, i denotes\nthe round number within multi-turn dialogues. For each\nsample s, its score for each criteria comprises multiple sub-\nscores C = {c1, c2, ..., ci}. Each sub-score ci = fx(l, ai, hi)\nis determined by scoring function fn based on the criterion\nx, and a limit l manually annotated by humans. The limit l\ncan be an integer, a list of keywords, or a referenced string4.\nCount Limit.We mainly consider four sub-scores: word\ncount score,sentence count score, andsample count score,\nrevise score. Forword count score, the criteria can beword-\nmax and word-min. For the scoring function fword-max, the\nmore word count exceeds the threshold limit lc, the lower\nthe score will be, thus fword-max is defined as follows:\nfword-max(ai, lc) =\n(\n1 n(ai) ⩽ lc\n1 − |n(ai)−l|\nn(ai) n(ai) > lc\nHere, n(ai) is the valid word count of answerai excluding\npunctuation marks. fword-min is defined as follows:\nfword-min(ai, lc) =\n(\n1 n(ai) ⩾ lc\nn(ai)\nl n(ai) < lc\nLikewise, the scoring functions for sentence count en-\ncompass fsentence-max, fsentence-min, fsentence-exact. The scoring\n3To ensure a fair comparison between models, all the model\nanswers in the histories for each sample are the same and provided\nby GPT-3.5-turbo.\n4The annotation process is detailed in the Appendix.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18191\nBenchmark Avg Ins\nLen. Format Metric Obj.\nC-Eval 110 C ACC T\nAGIEval 184 C EM/F1 T\nWizardLM\nTestset 62 O Preference F\nToolBench N/A O Pass Rate T\nPreference F\nAgentBench N/A O Pass Rate T\nCELLO 676 O\nFour\nFine-grained\nMetrics\nT\nTable 2: Statistics of existing benchmarks. Avg Ins denotes\nthe average word numbers in instructions. C and O denote\nthe Close-ended and Open-ended respectively. Preference\nrefers to evaluation via GPT4. Obj. represents whether the\nevaluation metrics are objective (T) or subjective (F).\nfunction for sample count fsample-exact is implemented us-\ning regex matching. The limit lc for revise score frevise can\nbe the string longer or shorter. Speicifically, the function\nfrevise(ai, longer) equals 1 if n(ai) > n(ai−1), otherwise, it\nequals 0. For each sample, the final Count Limitscore Sc is\nthe average of all the sub-scores.\nAnswer Format. This metric has two sub-scores:\nparseability and keywords. First, if the model output\ncan be parsed in the prescribed format, such as JSON,\nfparseability(ai, json) equals 1; otherwise, it equals 0. How-\never, even in cases where the model output cannot be di-\nrectly parsed, its ability to learn certain patterns still demon-\nstrates its capacity to follow complex instructions. Conse-\nquently, for each sample, we first extract keywords list lf =\n{w1, w2, ..., wi} from pre-defined formats, which we define\nas Scoring Keywords. Then, the sub-scorefkeywords(ai, lf ) is\ndefined as follows:\nfkeywords(ai, lf ) = N(ai, lf )\n|lf | ,\nwhere N denotes the number of scoring keywords covered\nby the model output ai. Finally, the overall score for answer\nformat Sf is the average of fparseability and fkeywords.\nInput-dependent Query.The key phrases of the correct\nanswer stem from the input text. The more scoring keywords\nincluded in a response, the higher the quality of the response.\nHence, for each sample, the subscore fkeywords(ai, l) is also\napplied here, where the Scoring keywords lq are extracted\nfrom input text. Moreover, certain models tend to repeat in-\nput text when they fail to understand the instructions, es-\npecially when the input text is long and noisy or during\nthe multi-turn dialogue. To prevent this undesirable copy-\ning behavior, we introduce a penalty term known as COPY-\nBLEU (Chen et al. 2022), which decreases as the response\nexhibits greater similarity to the input text. The final score\nSq for the Input-dependent query is defined as follows:\nSq = (1− fBLEU(ai, ti))fkeywords(ai, lq),\nwhere ti is the input text of sample si.\nTask-prescribed Phrases.The mandatory phrases speci-\nfied in the task description are essential conditions that must\nbe fulfilled. The more mandatory phrases covered in the an-\nswers, the better the model follows complex instructions.\nHence, the subscore fkeywords(ai, lt) is applied where lt is\nthe scoring keywords extracted from the task description.\nEvaluation of the Benchmark\nEach sample is labeled by three annotators. Specifically, we\nretain samples only when at least two annotators agree on\nthe criteria Count Limitand Output Format Parseability. For\ncriteria involving Keywords Coverage, we only keep key-\nwords with a consensus from at least two annotators.\nStatistics of the Benchmark\nTab. 1 presents the statistics5 of CELLO. Our dataset has two\ncategories depending on whether the criteria are mainly in\nthe task description or the input text. Different tasks also\nhave different emphases on the criteria, and our dataset\ncovers the four criteria effectively. Tab. 2 compares our\nbenchmark with existing ones. Our benchmark is the first\nto systematically test LLMs’ ability to follow complex in-\nstructions, which are generally longer and more complex\nthan other benchmarks. The tasks we cover are open-ended,\nwhich are more realistic and practical. Our evaluation is also\nmore objective and fine-grained.\nExperiment\nEvaluated Models We evaluate a total of 34 models\nthat demonstrated exceptional performance on other bench-\nmarks (Huang et al. 2023; Dubois et al. 2023), ranging from\ntheir model size, supported context length, and instruction\ntuning data size, as illustrated in Appendix. These models\nare categorized into three groups: Chinese-oriented Mod-\nels (From Scratch, FS), Chinese-oriented Models(Continue\nPretraining, CP), and English-oriented Models. The distinc-\ntion between English and Chinese-oriented Models lies in\nthe composition of their pretraining corpus, whereby the\nformer possesses a small portion and the latter possesses a\nsubstantial volume of Chinese data. Chinese-oriented Mod-\nels (FS) are trained entirely from scratch using Chinese cor-\npora. Chinese-oriented Models (CP) continue pretraining on\nChinese corpora utilizing an English-oriented base model.\nTask-categorized Performance The performance of the\nmodels on different tasks is shown in Tab. 3.\nGeneral Comparisons. Among the models assessed,\nOpenChat-V3.2 was the best, followed by Vicuna-V1.5-\n13B and ChatGLM. These models had different parameter\nsizes, showing that small-scale LLMs can follow complex\ninstructions as well as larger ones. The Chinese-oriented\n(FS) group and the English-oriented group perform equally\n5Chinese word are counted via https://github.com/fxsjy/jieba.\nEnglish words are counted via https://www.nltk.org/.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18192\nModel Complex Task Description Complex Input All\nExtract. Plan. Meta. Wri.(S) BS.(S) Key. QA. Sum. Struct. Wri.(M) BS.(M) Avg.\nChinese-oriented Models (Continue Pretraining)\nBaize-V2-7B 0.203 0.266 0.300 0.504 0.245 0.056 0.121 0.045 0.593 0.381 0.558 0.298\nLlama2-FlagAlpha 0.205 0.095 0.129 0.262 0.547 0.150 0.423 0.297 0.354 0.406 0.591 0.309\nBaize-V2-13B 0.214 0.334 0.342 0.272 0.536 0.070 0.143 0.019 0.540 0.433 0.574 0.318\nAlpaca-V1-13B 0.289 0.183 0.209 0.209 0.697 0.411 0.272 0.226 0.399 0.291 0.480 0.332\nAlpaca-V1-7B 0.264 0.123 0.215 0.357 0.612 0.265 0.267 0.243 0.465 0.401 0.703 0.352\nLlama2-Linly 0.382 0.170 0.205 0.352 0.527 0.196 0.464 0.406 0.596 0.352 0.594 0.381\nAlpaca-V1-33B 0.379 0.200 0.283 0.664 0.663 0.415 0.334 0.221 0.426 0.476 0.609 0.426\nBELLE 0.400 0.157 0.363 0.589 0.734 0.379 0.478 0.508 0.458 0.439 0.672 0.469\nCuteGPT 0.482 0.529 0.460 0.534 0.739 0.294 0.506 0.459 0.653 0.626 0.804 0.553\nLlama2-LinkSoul 0.521 0.326 0.431 0.652 0.769 0.615 0.788 0.684 0.565 0.747 0.909 0.629\nLlama2-OpenBuddy 0.585 0.638 0.344 0.697 0.697 0.638 0.752 0.685 0.711 0.812 0.892 0.670\nChinese-oriented Models (From Scratch)\nBatGPT-sirius 0.011 0.044 0.094 0.352 0.233 0.046 0.394 0.054 0.294 0.135 0.321 0.177\nMOSS 0.493 0.310 0.461 0.634 0.644 0.473 0.396 0.500 0.521 0.696 0.658 0.525\nInternLM 0.452 0.540 0.493 0.690 0.622 0.247 0.515 0.399 0.428 0.732 0.877 0.546\nChatGLM2 0.539 0.317 0.608 0.664 0.632 0.589 0.725 0.669 0.590 0.738 0.777 0.616\nChatGLM2-32k 0.526 0.399 0.572 0.699 0.690 0.653 0.686 0.571 0.427 0.758 0.876 0.620\nBaichuan-chat 0.473 0.373 0.471 0.800 0.794 0.491 0.728 0.701 0.601 0.776 0.857 0.637\nQwen 0.544 0.551 0.493 0.646 0.740 0.486 0.767 0.705 0.575 0.710 0.888 0.642\nChatGLM 0.649 0.522 0.612 0.700 0.808 0.532 0.742 0.672 0.573 0.735 0.870 0.673\nEnglish-oriented Models\nLlama2-chat-7B 0.495 0.326 0.500 0.358 0.465 0.157 0.135 0.060 0.708 0.541 0.447 0.385\nLlama2-chat-70B 0.431 0.289 0.484 0.397 0.472 0.147 0.158 0.079 0.719 0.570 0.552 0.393\nLlama2-chat-13B 0.445 0.329 0.624 0.359 0.453 0.154 0.127 0.108 0.753 0.569 0.458 0.402\nVicuna-V1.3-7B 0.485 0.661 0.303 0.748 0.665 0.180 0.651 0.583 0.525 0.674 0.773 0.569\nWizardLM 0.422 0.592 0.281 0.675 0.565 0.261 0.594 0.570 0.519 0.711 0.839 0.574\nLongChat-V1-13B 0.523 0.591 0.423 0.654 0.533 0.400 0.572 0.532 0.579 0.752 0.810 0.576\nLongChat-V1.5-7B 0.489 0.620 0.358 0.664 0.731 0.608 0.687 0.633 0.378 0.747 0.825 0.609\nLongChat-V1-7B 0.549 0.475 0.424 0.710 0.805 0.527 0.604 0.557 0.692 0.729 0.856 0.627\nVicuna-V1.3-13B 0.521 0.625 0.474 0.743 0.840 0.346 0.672 0.582 0.613 0.651 0.869 0.631\nVicuna-V1.5-7B 0.544 0.670 0.398 0.506 0.770 0.711 0.739 0.667 0.513 0.693 0.906 0.641\nVicuna-V1.3-33B 0.589 0.702 0.385 0.752 0.835 0.503 0.680 0.643 0.627 0.622 0.872 0.655\nVicuna-V1.5-13B 0.601 0.721 0.425 0.744 0.794 0.682 0.765 0.723 0.630 0.746 0.896 0.699\nOpenChat-V3.2 0.629 0.733 0.510 0.754 0.868 0.725 0.771 0.663 0.608 0.761 0.919 0.720\nGPT-3.5-turbo 0.709 0.805 0.632 0.879 0.854 0.765 0.795 0.832 0.697 0.879 0.908 0.794\nGPT-4 0.737 0.879 0.666 0.828 0.810 0.862 0.889 0.911 0.727 0.867 0.910 0.822\nTable 3: The performance of models on different tasks. Detailed information of each model is provided in the Appendix. The\nbold, underlined, and italicized denote the first, second, and third rankings, respectively. Here, Extract., Plan., Meta., Key.,\nSum., Struct., Avg. denote Extraction, Planning, Meta-prompt, Keywords, Summarization, Structure, Average respectively.\nwell and better than the Chinese-oriented (CC) group, prov-\ning that complex instruction comprehension is not language-\ndependent. Moreover, under the same base model, vocabu-\nlary, and supported context length (e.g. Llama2-7B), the per-\nformance of the models varies greatly. This demonstrates a\nstrong correlation between the ability to comprehend com-\nplex instructions and the instruction tuning phase. Over-\nall, the current open-source small to medium-scale models\nexhibit a significant performance gap compared to close-\nsource large-scale models (GPT-3.5-turbo, GPT4).\nComplex Task Description.Among the data with complex\ntask descriptions, first, four of the top 5 models belong to the\nEnglish-oriented Models, which demonstrate that the ability\nto understand complex task descriptions can transfer across\ndifferent languages. Next, within the same series of models,\nlarger model sizes do not always lead to improvements. Fur-\nthermore, the best-performing models in each group have a\nsupported context length of less than 4096, suggesting that\nthe supported text context length does not significantly im-\npact the ability to comprehend complex task descriptions.\nComplex Input Text.For the data with complex input text,\nfirst, seven of the top 10 models belong to Chinese-oriented\nmodels, which implies that more Chinese training data as-\nsists the models in comprehending long and noisy Chinese\ntexts. Next, within the same model series, larger scales gen-\nerally improve performance, while longer supported context\nlength can result in performance drops in many cases.\nCriteria-categorized Performance As shown in Tab. 4,\nregarding Answer format, the English-oriented Models sig-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18193\nModel Format Input Task Count\nChinese-oriented Models (Continue Pretraining)\nBaize-V2-7B 0.409 0.300 0.246 0.466\nLlama2-FlagAlpha 0.499 0.218 0.221 0.468\nBaize-V2-13B 0.530 0.247 0.302 0.444\nAlpaca-V1-13B 0.603 0.207 0.259 0.458\nAlpaca-V1-7B 0.663 0.224 0.256 0.512\nLlama2-Linly 0.411 0.347 0.374 0.490\nAlpaca-V1-33B 0.655 0.353 0.357 0.576\nBELLE 0.556 0.408 0.484 0.498\nCuteGPT 0.640 0.548 0.576 0.514\nLlama2-LinkSoul 0.662 0.623 0.662 0.603\nLlama2-OpenBuddy 0.734 0.627 0.704 0.638\nChinese-oriented Models (From Scratch)\nBatGPT-sirius 0.154 0.206 0.069 0.357\nMOSS 0.586 0.514 0.564 0.534\nInternLM 0.650 0.527 0.524 0.612\nChatGLM2 0.620 0.605 0.691 0.568\nChatGLM2-32k 0.687 0.563 0.716 0.603\nBaichuan-chat 0.750 0.603 0.586 0.662\nQwen 0.764 0.584 0.625 0.570\nChatGLM 0.715 0.628 0.742 0.571\nEnglish-oriented Models\nLlama2-chat-7B 0.598 0.294 0.306 0.686\nLlama2-chat-70B 0.631 0.318 0.265 0.701\nLlama2-chat-13B 0.640 0.342 0.280 0.674\nVicuna-V1.3-7B 0.598 0.520 0.599 0.597\nWizardLM 0.730 0.525 0.531 0.586\nLongChat-V1-13B 0.723 0.528 0.585 0.507\nLongChat-V1.5-7B 0.791 0.518 0.589 0.535\nLongChat-V1-7B 0.789 0.574 0.615 0.609\nVicuna-V1.3-13B 0.766 0.588 0.641 0.554\nVicuna-V1.5-7B 0.756 0.536 0.698 0.599\nVicuna-V1.3-33B 0.770 0.609 0.668 0.575\nVicuna-V1.5-13B 0.786 0.656 0.701 0.640\nOpenChat-V3.2 0.766 0.703 0.776 0.617\nGPT-3.5-turbo 0.899 0.760 0.799 0.700\nGPT-4 0.911 0.796 0.792 0.724\nTable 4: The performance of models for different criteria.\nThe bold, underlined, and italicized denote the first, second,\nand third rankings, respectively.\nnificantly perform better than Chinese-oriented Models.\nThis demonstrates the English-oriented Models’ ability to\nfollow few-shot examples and generate code, as well as par-\ntially explains why their complex instruction-following abil-\nity can transfer across languages. Next, for Task-prescribed\nphrases, two of the top-3 models are Chinese-oriented Mod-\nels, suggesting that Chinese data helps the models un-\nderstand Chinese semantic restrictions. Finally, the perfor-\nmance differences between models for Count limit criteria\nare not big compared to other criteria, which shows that the\nmodels have similar comprehension of numerical concepts.\nComparisons between Benchmarks We present the\nperformance6 on mainstream benchmarks in Fig. 3. First, on\nbenchmarks focusing on Chinese knowledge, smaller mod-\nels achieve similar or even better performance compared to\nGPT-3.5-turbo. Also, on challenging benchmarks like com-\nplex reasoning and programming ability, there is a lack of\n6https://opencompass.org.cn/leaderboard-llm.\nCeval\nAGIeval\nMMLU\nCMMLU\nGAOKAOHumanEval\nGSM8k\nBBH\nCELLO\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nGPT4\nGPT-3.5-turbo\nBaichuan-chat\nChatGLM2\nLlama2-chat-13B\nVicuna-V1.3-7B\nLlama2-chat-7B\nFigure 3: Model performance on mainstream benchmarks.\ndistinction between smaller models. Overall, our benchmark\ncan exhibit more discriminative results.\nFormat\nInput\nTask\nCount\n10\n20\n30\n40\n50\n60\n70\n80\nLlama2-chat-7B\nLlama2-Linly\nLongChat-V1.5-7B\nOpenChat-v3.2\nExtraction\nPlanning\nMeta.\nWriting(S)\nBS(S)\nKeywordsQA\nSum.\nStructure\nWriting(M)\nBS(M)\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nFigure 4: LLMs’ performance on different tasks and criteria\nbased on the same model (Touvron et al. 2023)\nFine-grained Evaluation Fig. 4 shows the performance\nof LLMs based on the same base model for different tasks\nand criteria. Different models have different strengths for\ndifferent criteria. For example, Llama2-chat-7B is good at\nunderstanding format but bad at comprehending Chinese in-\nput and semantic constraints. Different models also excel\nin specific tasks. Llama2-chat-7B handles complex task de-\nscriptions well, but not complex input text.\nConclusion\nIn this work, we systematically investigate the complex in-\nstructions following ability of LLMs. We establish a frame-\nwork comprising eight features for complex instructions,\nthen construct an evaluation dataset covering nine tasks, and\nfinally propose four evaluation criteria and corresponding\nmetrics to assess LLMs’ complex instruction understanding\nability. Furthermore, we conduct extensive experiments to\ncompare the performance of representative models.\nAcknowledgements\nThis work is supported by Science and Technology\nCommission of Shanghai Municipality Grant (No.\n22511105902), National Natural Science Foundation\nof China (No.62102095), Shanghai Municipal Science\nand Technology Major Project (No.2021SHZDZX0103).\nYanghua Xiao is also a member of Research Group of Com-\nputational and AI Communication at Institute for Global\nCommunications and Integrated Media, Fudan University.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18194\nReferences\nAn, C.; Gong, S.; Zhong, M.; Li, M.; Zhang, J.; Kong, L.;\nand Qiu, X. 2023. L-Eval: Instituting Standardized Evalu-\nation for Long Context Language Models. arXiv preprint\narXiv:2307.11088.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nChen, H.; Li, H.; Chen, D.; and Narasimhan, K. 2022. Con-\ntrollable Text Generation with Language Constraints. arXiv\npreprint arXiv:2212.10466.\nChen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.;\nKaplan, J.; Edwards, H.; Burda, Y .; Joseph, N.; Brockman,\nG.; et al. 2021. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374.\nChia, Y . K.; Hong, P.; Bing, L.; and Poria, S. 2023.\nINSTRUCTEV AL: Towards Holistic Evaluation of\nInstruction-Tuned Large Language Models. arXiv preprint\narXiv:2306.04757.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2022. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311.\nCobbe, K.; Kosaraju, V .; Bavarian, M.; Chen, M.; Jun, H.;\nKaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.;\net al. 2021. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168.\nDing, N.; Chen, Y .; Xu, B.; Qin, Y .; Zheng, Z.; Hu, S.; Liu,\nZ.; Sun, M.; and Zhou, B. 2023. Enhancing Chat Lan-\nguage Models by Scaling High-quality Instructional Con-\nversations. arXiv preprint arXiv:2305.14233.\nDubois, Y .; Li, X.; Taori, R.; Zhang, T.; Gulrajani, I.; Ba, J.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Alpaca-\nfarm: A simulation framework for methods that learn from\nhuman feedback. arXiv preprint arXiv:2305.14387.\nGu, Z.; Zhu, X.; Ye, H.; Zhang, L.; Wang, J.; Jiang, S.;\nXiong, Z.; Li, Z.; He, Q.; Xu, R.; et al. 2023. Xiezhi: An\nEver-Updating Benchmark for Holistic Domain Knowledge\nEvaluation. arXiv preprint arXiv:2306.05783.\nGuo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y .;\nYue, J.; and Wu, Y . 2023. How close is chatgpt to human ex-\nperts? comparison corpus, evaluation, and detection. arXiv\npreprint arXiv:2301.07597.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika,\nM.; Song, D.; and Steinhardt, J. 2020. Measuring mas-\nsive multitask language understanding. arXiv preprint\narXiv:2009.03300.\nHonovich, O.; Scialom, T.; Levy, O.; and Schick, T. 2022.\nUnnatural instructions: Tuning language models with (al-\nmost) no human labor. arXiv preprint arXiv:2212.09689.\nHuang, Y .; Bai, Y .; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu,\nJ.; Lv, C.; Zhang, Y .; Lei, J.; et al. 2023. C-eval: A multi-\nlevel multi-discipline chinese evaluation suite for foundation\nmodels. arXiv preprint arXiv:2305.08322.\nLi, G.; Hammoud, H. A. A. K.; Itani, H.; Khizbullin, D.;\nand Ghanem, B. 2023a. Camel: Communicative agents for”\nmind” exploration of large scale language model society.\narXiv preprint arXiv:2303.17760.\nLi, J.; Cheng, X.; Zhao, W. X.; Nie, J.-Y .; and Wen, J.-R.\n2023b. HaluEval: A Large-Scale Hallucination Evaluation\nBenchmark for Large Language Models. arXiv e-prints,\narXiv–2305.\nLiang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.;\nYasunaga, M.; Zhang, Y .; Narayanan, D.; Wu, Y .; Kumar,\nA.; et al. 2022. Holistic evaluation of language models.\narXiv preprint arXiv:2211.09110.\nLiu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua,\nM.; Petroni, F.; and Liang, P. 2023a. Lost in the middle:\nHow language models use long contexts. arXiv preprint\narXiv:2307.03172.\nLiu, X.; Yu, H.; Zhang, H.; Xu, Y .; Lei, X.; Lai, H.; Gu,\nY .; Ding, H.; Men, K.; Yang, K.; et al. 2023b. Agent-\nBench: Evaluating LLMs as Agents. arXiv preprint\narXiv:2308.03688.\nLuo, Z.; Xu, C.; Zhao, P.; Sun, Q.; Geng, X.; Hu, W.; Tao,\nC.; Ma, J.; Lin, Q.; and Jiang, D. 2023. WizardCoder: Em-\npowering Code Large Language Models with Evol-Instruct.\narXiv preprint arXiv:2306.08568.\nMukherjee, S.; Mitra, A.; Jawahar, G.; Agarwal, S.; Palangi,\nH.; and Awadallah, A. 2023. Orca: Progressive learning\nfrom complex explanation traces of gpt-4. arXiv preprint\narXiv:2306.02707.\nQin, Y .; Liang, S.; Ye, Y .; Zhu, K.; Yan, L.; Lu, Y .; Lin,\nY .; Cong, X.; Tang, X.; Qian, B.; et al. 2023. ToolLLM:\nFacilitating Large Language Models to Master 16000+ Real-\nworld APIs. arXiv preprint arXiv:2307.16789.\nRichards, T. B. 2023. Auto-GPT: An Autonomous GPT-4\nExperiment.\nSrivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A. M.; Abid,\nA.; Fisch, A.; Brown, A. R.; Santoro, A.; Gupta, A.; Garriga-\nAlonso, A.; et al. 2023. Beyond the Imitation Game: Quanti-\nfying and extrapolating the capabilities of language models.\nTransactions on Machine Learning Research.\nSun, W.; Yan, L.; Ma, X.; Ren, P.; Yin, D.; and Ren, Z.\n2023. Is ChatGPT Good at Search? Investigating Large\nLanguage Models as Re-Ranking Agent. arXiv preprint\narXiv:2304.09542.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stan-\nford alpaca: An instruction-following llama model.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288.\nWang, P.; Li, L.; Chen, L.; Zhu, D.; Lin, B.; Cao, Y .; Liu,\nQ.; Liu, T.; and Sui, Z. 2023. Large language models are not\nfair evaluators. arXiv preprint arXiv:2305.17926.\nXu, B.; Xu, Y .; Liang, J.; Xie, C.; Liang, B.; Cui, W.;\nand Xiao, Y . 2017. CN-DBpedia: A never-ending Chinese\nknowledge extraction system. In International Conference\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18195\non Industrial, Engineering and Other Applications of Ap-\nplied Intelligent Systems, 428–438. Springer.\nXu, C.; Sun, Q.; Zheng, K.; Geng, X.; Zhao, P.; Feng,\nJ.; Tao, C.; and Jiang, D. 2023. WizardLM: Empowering\nLarge Language Models to Follow Complex Instructions.\narXiv:2304.12244.\nYao, S.; Chen, H.; Hanjie, A. W.; Yang, R.; and Narasimhan,\nK. 2023a. COLLIE: Systematic Construction of Constrained\nText Generation Tasks. arXiv preprint arXiv:2307.08689.\nYao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,\nK.; and Cao, Y . 2023b. ReAct: Synergizing Reasoning and\nActing in Language Models (arXiv: 2210.03629). arXiv.\nYu, J.; Wang, X.; Tu, S.; Cao, S.; Zhang-Li, D.; Lv, X.; Peng,\nH.; Yao, Z.; Zhang, X.; Li, H.; et al. 2023. KoLA: Carefully\nBenchmarking World Knowledge of Large Language Mod-\nels. arXiv preprint arXiv:2306.09296.\nZha, L.; Zhou, J.; Li, L.; Wang, R.; Huang, Q.; Yang, S.;\nYuan, J.; Su, C.; Li, X.; Su, A.; et al. 2023. TableGPT:\nTowards Unifying Tables, Nature Language and Commands\ninto One GPT. arXiv preprint arXiv:2307.08674.\nZhong, W.; Cui, R.; Guo, Y .; Liang, Y .; Lu, S.; Wang, Y .;\nSaied, A.; Chen, W.; and Duan, N. 2023. Agieval: A human-\ncentric benchmark for evaluating foundation models. arXiv\npreprint arXiv:2304.06364.\nZhou, C.; Liu, P.; Xu, P.; Iyer, S.; Sun, J.; Mao, Y .; Ma, X.;\nEfrat, A.; Yu, P.; Yu, L.; et al. 2023a. Lima: Less is more for\nalignment. arXiv preprint arXiv:2305.11206.\nZhou, W.; Jiang, Y . E.; Wilcox, E.; Cotterell, R.; and Sachan,\nM. 2023b. Controlled text generation with natural language\ninstructions. arXiv preprint arXiv:2304.14293.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18196",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5847998857498169
    },
    {
      "name": "Natural language processing",
      "score": 0.37442296743392944
    },
    {
      "name": "Programming language",
      "score": 0.34623709321022034
    },
    {
      "name": "Linguistics",
      "score": 0.32763028144836426
    },
    {
      "name": "Philosophy",
      "score": 0.10125419497489929
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    }
  ],
  "cited_by": 17
}