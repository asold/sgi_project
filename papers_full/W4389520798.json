{
    "title": "Regulation and NLP (RegNLP): Taming Large Language Models",
    "url": "https://openalex.org/W4389520798",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2224025738",
            "name": "Catalina Goanta",
            "affiliations": [
                "University of Sheffield",
                "Utrecht University"
            ]
        },
        {
            "id": "https://openalex.org/A93365683",
            "name": "Nikolaos Aletras",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2779539080",
            "name": "Ilias Chalkidis",
            "affiliations": [
                "University of Copenhagen",
                "Tilburg University",
                "Maastricht University"
            ]
        },
        {
            "id": "https://openalex.org/A2527138506",
            "name": "Sofia Ranchordás",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2294980368",
            "name": "Gerasimos Spanakis",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3086052196",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W4226439868",
        "https://openalex.org/W3037831233",
        "https://openalex.org/W4377010286",
        "https://openalex.org/W4210916416",
        "https://openalex.org/W4381713708",
        "https://openalex.org/W3146142859",
        "https://openalex.org/W4385573947",
        "https://openalex.org/W1567306273",
        "https://openalex.org/W2759653948",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W3200597535",
        "https://openalex.org/W4283170666",
        "https://openalex.org/W2058345208",
        "https://openalex.org/W1605878143",
        "https://openalex.org/W2773019705",
        "https://openalex.org/W1997737167",
        "https://openalex.org/W4313045059",
        "https://openalex.org/W3207241864",
        "https://openalex.org/W4321392130",
        "https://openalex.org/W4379797189",
        "https://openalex.org/W1862536137",
        "https://openalex.org/W3176964086",
        "https://openalex.org/W2086496283",
        "https://openalex.org/W3105871743",
        "https://openalex.org/W444448",
        "https://openalex.org/W4287674181",
        "https://openalex.org/W1873833619",
        "https://openalex.org/W3181893318",
        "https://openalex.org/W4362514994",
        "https://openalex.org/W2978346165",
        "https://openalex.org/W3046077792",
        "https://openalex.org/W3111621478",
        "https://openalex.org/W4285199616",
        "https://openalex.org/W2901848379",
        "https://openalex.org/W2318753082",
        "https://openalex.org/W4381612673",
        "https://openalex.org/W4382363171",
        "https://openalex.org/W4235722751",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W2146718094",
        "https://openalex.org/W4321610465",
        "https://openalex.org/W2281799576",
        "https://openalex.org/W4385571788",
        "https://openalex.org/W2901707424",
        "https://openalex.org/W2107912056",
        "https://openalex.org/W3010423680",
        "https://openalex.org/W4379087118",
        "https://openalex.org/W2997421576",
        "https://openalex.org/W4282969450",
        "https://openalex.org/W4381855887",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W605393428",
        "https://openalex.org/W2564635859",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3196248941",
        "https://openalex.org/W3203737321",
        "https://openalex.org/W3186782927",
        "https://openalex.org/W3211203552",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3128511965"
    ],
    "abstract": "The scientific innovation in Natural Language Processing (NLP) and more broadly in artificial intelligence (AI) is at its fastest pace to date. As large language models (LLMs) unleash a new era of automation, important debates emerge regarding the benefits and risks of their development, deployment and use. Currently, these debates have been dominated by often polarized narratives mainly led by the AI Safety and AI Ethics movements. This polarization, often amplified by social media, is swaying political agendas on AI regulation and governance and posing issues of regulatory capture. Capture occurs when the regulator advances the interests of the industry it is supposed to regulate, or of special interest groups rather than pursuing the general public interest. Meanwhile in NLP research, attention has been increasingly paid to the discussion of regulating risks and harms. This often happens without systematic methodologies or sufficient rooting in the disciplines that inspire an extended scope of NLP research, jeopardizing the scientific integrity of these endeavors. Regulation studies are a rich source of knowledge on how to systematically deal with risk and uncertainty, as well as with scientific evidence, to evaluate and compare regulatory options. This resource has largely remained untapped so far. In this paper, we argue how NLP research on these topics can benefit from proximity to regulatory studies and adjacent fields. We do so by discussing basic tenets of regulation, and risk and uncertainty, and by highlighting the shortcomings of current NLP discussions dealing with risk assessment. Finally, we advocate for the development of a new multidisciplinary research space on regulation and NLP (RegNLP), focused on connecting scientific knowledge to regulatory processes based on systematic methodologies.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8712–8724\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nRegulation and NLP (RegNLP): Taming Large Language Models\nCatalina Goanta\nUtrecht University\nNikolaos Aletras\nUniversity of Sheffield\nIlias Chalkidis\nUniversity of Copenhagen\nSofia Ranchordas\nTilburg University\nGerasimos Spanakis\nMaastricht University\nAbstract\nThe scientific innovation in Natural Language\nProcessing (NLP) and more broadly in artificial\nintelligence (AI) is at its fastest pace to date. As\nlarge language models (LLMs) unleash a new\nera of automation, important debates emerge\nregarding the benefits and risks of their devel-\nopment, deployment and use. Currently, these\ndebates have been dominated by often polar-\nized narratives mainly led by the AI Safety and\nAI Ethics movements. This polarization, often\namplified by social media, is swaying political\nagendas on AI regulation and governance and\nposing issues of regulatory capture. Capture\noccurs when the regulator advances the inter-\nests of the industry it is supposed to regulate, or\nof special interest groups rather than pursuing\nthe general public interest. Meanwhile in NLP\nresearch, attention has been increasingly paid\nto the discussion of regulating risks and harms.\nThis often happens without systematic method-\nologies or sufficient rooting in the disciplines\nthat inspire an extended scope of NLP research,\njeopardizing the scientific integrity of these en-\ndeavors. Regulation studies are a rich source of\nknowledge on how to systematically deal with\nrisk and uncertainty, as well as with scientific\nevidence, to evaluate and compare regulatory\noptions. This resource has largely remained\nuntapped so far. In this paper, we argue how\nNLP research on these topics can benefit from\nproximity to regulatory studies and adjacent\nfields. We do so by discussing basic tenets\nof regulation, and risk and uncertainty, and by\nhighlighting the shortcomings of current NLP\ndiscussions dealing with risk assessment. Fi-\nnally, we advocate for the development of a new\nmultidisciplinary research space on regulation\nand NLP (RegNLP), focused on connecting sci-\nentific knowledge to regulatory processes based\non systematic methodologies.\n1 Introduction\nThe development of Large Language Models\n(LLMs) is at its fastest pace to date. In the past\nFigure 1: A depiction of the cross-over between AI\nSafety, Ethics, Doomism, and how they capture AI Reg-\nulation.\nyears alone, LLMs have seen considerable advance-\nment across a multitude of languages and types\nof data, with models such as GPT-3.5 (Ouyang\net al., 2022), GPT-4 (OpenAI, 2023), LLaMA (Tou-\nvron et al., 2023), and PALM-2 (Anil et al., 2023)\ndemonstrating unprecedented capabilities across\na broad collection of natural language processing\n(NLP) tasks.1\nThese innovations have led to rapid shifts in\nvarious applications such as open-domain search,\ncoding, e-commerce and education. For example,\nstate-of-the-art LLMs already power conversational\nsearch engines (e.g. OpenAI ChatGPT, Bing Chat,\nand Google Bard), coding assistants (e.g. OpenAI\nCodex and Github Copilot), product recommender\nsystems (e.g. Alibaba Tongyi and SalesForce Com-\nmerceGPT) and educational assistants (e.g. Khan-\nmigo) inter alia.\nAs with most technologies, the development\nand use of LLMs do not come without concerns.\nResearchers are rightfully worried that while this\ntechnology may be transformative, its societal\nimplications might be higher than its benefits\n(Gabriel, 2020). Concerns have been raised es-\n1In the sense that LLMs generalize to a great extend in\nout-of-distribution and out-of-domain use cases.\n8712\npecially around ethics (Floridi, 2023; Tsarapatsa-\nnis and Aletras, 2021), bias (Hovy and Prabhu-\nmoye, 2021; Blodgett et al., 2020), safety (Dobbe\net al., 2021) and environmental impact (Rillig et al.,\n2023; Schwartz et al., 2020; Strubell et al., 2019).\nThe unsupervised use of LLMs has already led to\nwidely-publicized examples of professional negli-\ngence. The all too public fiasco of the lawyer who\nused ChatGPT for a court brief and unknowingly\nincluded made-up case law references was taken by\nmany as an example of the dangers of immersing\ndaily professional activities in generative AI.2 The\npublic debate around the future of AI, and implic-\nitly NLP, is very complex and multi-layered. While\nthe debate seems to converge on the point of calling\nfor regulation to control the unwanted effects of\nthese technologies,3 different regulatory directions\nare proposed by the various stakeholders involved\nin this debate.\nThe current public discourse has been dominated\nby two groups. On the one hand, proponents of AI\nexistential risks (the AI Safety movement)4 that in-\nclude technology CEOs and AI researchers have\nbeen publishing open letters5,6,7 and regularly meet\nwith regulators to warn about catastrophic scenar-\nios around general AI, proposing industry-friendly\nsolutions (Roose, 2023). On the other hand, the AI\nethics movement (Borenstein et al., 2021), mostly\nreflects the voice of researchers from various disci-\nplines as well as civil society activists. They have\nraised compelling alarm bells with respect to the\nrisks posed by LLMs (Bender et al., 2021; Wei-\ndinger et al., 2022; Floridi, 2023). The AI ethics\nmovement has also offered guidance to regulators\nsuch as the European Parliament on AI governance,\narguing for e.g. broad definitions of general pur-\npose AI.8\nYet while NLP research increasingly focuses\non LLM regulation, it remains generally detached\n2https://edition.cnn.com/2023/05/27/business/\nchat-gpt-avianca-mata-lawyers/index.html\n3https://www.forbrukerradet.no/side/new-repor\nt-generative-ai-threatens-consumer-rights/\n4We distinguish the AI Safety movement from ‘AI\nDoomism’ ( https://time.com/6266923/ai-eliezer\n-yudkowsky-open-letter-not-enough/ ) flirting with con-\nspiracy theories.\n5https://www.nytimes.com/2023/03/29/technolog\ny/ai-artificial-intelligence-musk-risks.html\n6https://futureoflife.org/open-letter/pause-g\niant-ai-experiments/\n7https://www.safe.ai/statement-on-ai-risk\n8https://ainowinstitute.org/publication/gpa\ni-is-high-risk-should-not-be-excluded-from-eu-a\ni-act.\nfrom prior work on regulation studies. Instead, the\noften intense public conflicts in this space are nudg-\ning regulators towards reactionary public relations\nactivities rather than the collection of scientific ex-\npertise representing broader parts of the NLP and\nAI communities. For instance, right after the most\nrecent letter on existential risks, the US and EU\nagreed to develop an AI code of conduct ‘within\nweeks’.9 Similarly, the UK has announced it will\nbe holding a global summit on AI safety, 10 and\nthe US Congress has been taking evidence from a\nwide array of industry executives, in what has been\ndescribed as a global race to regulate AI. 11 With\nlegacy and social media considerably increasing the\nvisibility of these often polarizing debates, there\nis a real danger of regulatory capture by visible\nvoices in industry and academia alike on scientific\nviews that might not necessarily be representative\nof the ‘silent majority’ of NLP researchers. Regu-\nlatory capture is the process by which regulation\nis directed away from the public interest and to-\nwards the interests of specific groups (Levine and\nForrence, 1990; Dal Bo, 2006).\nAgainst this background, we call on the NLP\ncommunity to familiarize itself with regulation\nstudies. We argue that this can lead to a clearer\nvision about how NLP as a field can properly partic-\nipate in AI governance not only as an object of reg-\nulation, but also as a source of scientific knowledge\nthat can benefit individuals, societies and markets\nalike. In this paper, we contribute to the existing\ndebate relating to the future of NLP by discussing\nthe benefits of interfacing NLP research with reg-\nulation studies in a systematic way. This view is\nbased on two main ideas:\n1. NLP research on regulation needs a multidis-\nciplinary framework engaging with regulation\nstudies, as well as adjacent disciplines such\nas law, economics, environmental science, etc.\nWe advocate for a new crucial area of research\non regulation and NLP (RegNLP), with har-\nmonized and systematic methodologies.\n2. A more coordinated NLP research field on risk\n9https://www.fastcompany.com/90903919/will-t\nhe-eu-u-s-new-voluntary-code-of-conduct-on-ai-w\nork-to-rein-in-the-tech\n10https://www.theguardian.com/technology/2023/\njun/09/rishi-sunak-ai-summit-what-is-its-aim-a\nnd-is-it-really-necessary.\n11https://foreignpolicy.com/2023/05/05/eu-ai-a\nct-us-china-regulation-artificial-intelligence-c\nhatgpt/\n8713\nand regulation (such as RegNLP) can interact\nwith policy-makers with more transparency,\nrepresentation, and trustworthiness.\n2 Regulation: A Short Introduction\nWhy Do We Regulate? Calls to regulate LLMs\nand AI are everywhere, to the extent that overusing\nthe term ‘regulation’ is trivializing its meaning. So\nwhat exactly is regulation and why do we rely on\nit?\nHistorically, regulation was defined by reference\nto state intervention in the economy. Selznick\n(1985) defined regulation as ‘a sustained and fo-\ncused control exercised by a public agency over\nactivities that are valued by the community’. Over\nthe last decades, regulation has evolved and it has\nincreasingly acquired a hybrid character as both\npublic and private actors may issue rules that shape\nsocial behavior. In this paper, we draw on Black’s\ndefinition of regulation as an organized and inten-\ntional attempt to manage another person’s behavior\nso as to solve a collective problem (Black, 2008).\nThis is done through a combination of rules or\nnorms which come together with means for their\nimplementation and enforcement (Ogus, 2009).\nRegulation includes different types of regula-\ntory instruments (e.g. laws) such as traditional top-\ndown or command-and-control regulations. Recent\nexamples in digital public policy include the laws\nissued by the European Union, such as the Digital\nServices Act, or India’s law banning TikTok. How-\never, beyond these public regulatory instruments,\nthere is an array of private or hybrid instruments\n(Veale et al., 2023). Some of these are qualified as\n‘soft’ regulation because they cannot be enforced in\ncourt but they remain relevant and they effectively\nshape the behavior of the industry. Examples in-\nclude the EU’s Ethics Guidelines for Trustworthy\nAI12, but also codes of conduct initiated by industry\nitself.13\nOver the last decades, different theories of regu-\nlation have helped us understand either on norma-\ntive or empirical accounts why we should regulate.\nThey generally reflect various hypotheses about\n‘why regulation emerges, which actors contribute to\nthat emergence and typical patterns of interaction\nbetween regulatory actors’ (Morgan and Yeung,\n2007). Public interest and private interest theories\n12https://digital-strategy.ec.europa.eu/en/libr\nary/ethics-guidelines-trustworthy-ai\n13https://ethics.acm.org/code-of-ethics/softwa\nre-engineering-code/\nare two well-known examples (Morgan and Yeung,\n2007). According to public interest theories, regu-\nlation is used by law-makers to serve a broad public\ninterest, seeking to regulate in the most efficient\nway possible. Regulators assume that markets need\n‘a helping hand’ because when unhindered, they\nwill fail. Information asymmetries are one of the\nmarket failures that regulation seeks to address. At\nthe same time, public interest theories of regulation\nare normative and prescriptive: on the one hand,\nthey assume that benevolent state regulators ought\nalways to use regulation to advance the public inter-\nest; on the other, they also advise on how to achieve\nthis goal.\nIn contrast, private interest theories are not pri-\nmarily concerned with normative justifications for\nregulation. Rather, they are prescriptive accounts\nof the complex dynamics between different market\nactors, stakeholders and public officials situated in\na given socio-economic, political and cultural time\nand place. They explain, for example, why regula-\ntion may fail to pursue the public interest but they\ndo not offer prescriptions on how address to such\nproblems. Private interest theories assume that reg-\nulation emerges from the actions of individuals or\ngroups motivated to maximize their self-interest.\nTechnology Regulation and the Role of Science\nTechnological change often disrupts the wider reg-\nulatory order, triggering concerns about its ad-\nequacy and regulatory legitimacy (Brownsword\net al., 2017). Differences in the timing of tech-\nnology and regulation explain this difficulty. The\nliterature has claimed there is a ‘pacing gap’ be-\ntween the slow-going nature of regulation and the\nspeed of technological change (Marchant et al.,\n2013). Technological innovations have specific de-\nvelopment trajectories, investment and life cycles,\nand path dependencies (van den Hoven, 2014) that\ndo not go well with the speed of technology. This\nalso applies to LLMs. This is a well-known prob-\nlem in regulatory studies that has been captured\nby the Collingridge dilemma (Genus and Stirling,\n2018). This dilemma explains that when an inno-\nvation emerges, regulators hesitate to regulate due\nto the limited availability of information. However,\nby the time more is known, regulations may have\nbecome obsolete as technology may have already\nchanged.\nThe increased pace of regulatory activities in the\npast years in the field of technology shows that\nregulators are trying to close the pacing gap and\n8714\nbe more proactive in tackling the potential risks of\ntechnology. In doing so, they increasingly depend\non retrieving scientific information in a quick and\nagile way.\nThe role of science in regulation and public pol-\nicy has been the subject of important debates. On\nthe one hand, regulation should reflect the latest\nscientific evidence and be evidence-based. An il-\nlustration of this approach is the European Union’s\n‘Better Regulation’ agenda, a public policy strategy\naiming to ensure that European regulation is based\non scientific evidence, as well as the involvement\nof a wide range of stakeholders in the decision-\nmaking process (Commission, 2023b; Simonelli\nand Iacob, 2021). Citizens, businesses and any\nother stakeholders can submit their contributions\nto the calls for evidence, feedback and public con-\nsultations. For instance, 303 contributions were re-\nceived on the AI Act proposal during 26 April 2021\nand 6 August 2021, out of which 28% from busi-\nnesses, 24% from business associations, 17% from\nNGOs, and 6% from academic/research institutions\n(Commission, 2023a). On the other hand, science\nis complex and difficult to translate into regulatory\nmeasures. Science has thus been used to oversim-\nplify regulatory problems and justify poor regula-\ntory decisions based on the existence of scientific\nevidence pointing in a specific direction (Porter,\n2020). This is a particular danger in the LLM pub-\nlic debate. With science becoming increasingly\ncomplex, so do the scientific perspectives on how\nto proceed with this technology.\nLessons to be Learned The theoretical underpin-\nnings of regulation have helped shape a cohesive\nunderstanding of the rationale behind regulatory\nactivity. The interest in technology regulation, par-\nticularly for disruptive innovations such as LLMs,\nhas exploded in past years across a wide array of\nscientific disciplines. While understandable, such\npopularity often leads to inquiries which are not\nconnected to prior knowledge on regulation stud-\nies. This reflects a more general problem faced by\ncontemporary science, namely that of tackling mul-\ntidisciplinary issues without multidisciplinary ex-\npertise. Considering these circumstances, research\non LLMs and regulation could benefit from en-\ngaging with the regulation studies and governance\ncontext.\n3 LLMs: Risk and Uncertainty\nThe need to bridge NLP research with regulation\nstudies is especially important in the discussion of\nrisks. New and emerging technologies are typically\naccompanied by risk and uncertainty. The regu-\nlation of technological change and innovation is\nhighly complex, as innovation remains an elusive\nconcept hard to define, measure, and thus regulate.\nIn the past years, the question of risks arising\nout of NLP developments such as LLMs has been\nincreasingly embraced in computer science litera-\nture. One strand of this literature is reflected by\nthe theme of algorithmic unfairness. This theme\nemerged at the intersection of discrimination law\nand automated decision-making, and includes ques-\ntions relating to fairness and machine learning in\ngeneral (Barocas et al., 2019), as well as specific\nexamples of algorithmic bias risks in NLP (Field\net al., 2023; Talat et al., 2022; Kidd and Birhane,\n2023), computer vision (Wolfe et al., 2023), mul-\ntimodal models (Birhane et al., 2021), as well as\nprivacy risks (Mireshghallah et al., 2022). Another\nstrand of this literature looks at LLMs from a more\nholistic perspective, raising concerns about their\nsize vis-a-vis a broader number of risks for e.g. the\nenvironment, bias, representation or hate speech\n(Bender et al., 2021; Weidinger et al., 2022; Bom-\nmasani et al., 2022). This theme does not only\ninclude risks in commercial applications, but also\nrisks arising out of the mere scientific development\nof technology.\nThis literature has raised important concerns re-\nlating to the immediate and longer-term implica-\ntions around the advancement of machine learning\nand NLP. However, when positioned in the regu-\nlatory context, we can observe conceptual clashes\nwith frameworks which have been traditionally re-\nlied upon in public policy and risk regulation. One\nsuch framework is the field of risk and uncertainty.\nPut simply, ‘risk is the situation under which the\ndecision outcomes and their probabilities of oc-\ncurrences are known to the decision-maker, and\nuncertainty is the situation under which such in-\nformation is not available to the decision-maker’\n(Park and Shapira, 2017). In more technical terms,\n‘risk is the probability of an event multiplied by its\nimpact, and uncertainty reflects the accuracy with\nwhich a risk can be assessed’ (Krebs, 2011). As a\nfield, risk and uncertainty has made considerable\ncontributions to the development of risk regulation,\nmost notably in relation to environmental regula-\n8715\ntion (Heyvaert, 2011; Yarnold et al., 2022). It is\nimportant that policy-makers have a concrete quan-\ntification of risk (Aumann and Serrano, 2008), in\norder to determine the adequate level of risk as-\nsociated with various public policies. In addition,\nrisk determination and management have important\neconomic consequences, specifically for determin-\ning ‘what level of expenditure in reducing risk is\nproportionate to the risk itself’ (Krebs, 2011).\nEspecially in the case of technologies that\neasily transcend physical borders, societies and\neconomies, determining risk and uncertainty is a\ncomplex undertaking, even for scientists. Some of\nthe factors that make it difficult to assess risk and\nuncertainty include the complexity of the technol-\nogy itself, as well as the information asymmetry\nunderlying commercial practices. LLMs are hu-\nmongous (billion-parameters-sized) Transformer-\nbased (Vaswani et al., 2017) models, which have\nbeen initially pre-trained as standard language mod-\nels (Radford et al., 2018) in a vast quantity of\ntext data (mostly web scrapes) and have been also\nfurther optimized to follow instructions (Chung\net al., 2022) and user alignment (Leike et al., 2018)\nwith reinforcement learning from human feedback\n(RLHF) (Christiano et al., 2017; Stiennon et al.,\n2020). Particularly, AI alignment has been a con-\ntroversial topic, since it implies a broad consensus\non what sort of values (standards) AI should align\nwith (Gabriel, 2020). As such, LLMs are com-\nplex technologies where in addition to risk, we also\ndeal with considerable uncertainty, which can be,\namong others, descriptive (e.g. relating to the vari-\nables defining a system), or related to measurement\n(e.g. uncertainty about the value of the variables in\na system) (Gough, 1988).\nLLMs as the new GMOs? Yet LLMs are neither\nthe first nor the last technology development posing\nconcerns about wide-spread risks. As an illustra-\ntion, in the 1970s and 1980s, In-Vitro Fertilization\n(IVF) was a demonized scientific development con-\nsidered inhumane, which led to nothing short of\na large-scale moral panic (Garber, 2012). In the\n2000s, concerns around Genetically Modified Or-\nganisms (GMOs) dominated media coverage in\nEuropean and North American countries, in what\nwas deemed a ‘superstorm’ of moral panic and new\nrisk discourses (Howarth, 2013). These are only\ntwo examples of risk narratives that were amplified\nby media coverage in ways that overshadowed im-\nportant scientific expertise. Yet through regulation\nsupporting scientific advancement, their use today\nhas become mundane as part of solving consider-\nable society problems such as infertility or food\navailability. These comparisons by no means imply\nthat earlier biotechnological innovations pose the\nsame levels of risk as LLMs or should entail the\nsame level of regulation. However, it is important\nto learn from our past experiences with technology\nhow to distinguish between moral panics and real\nproblems that need scientific solutions.\nResearchers are starting to develop concrete\nmethodologies for the auditing of LLMs and re-\nlated NLP technologies (Derczynski et al., 2023),14\nas well as dealing with particular risks such as en-\nvironmental impact (Rolnick et al., 2022). These\ncontributions are much needed, as they can be trans-\nlated into concrete measurements of risk and un-\ncertainty, and further lead to the development of\npolicy options in risk management. However, these\ninitiatives are so far too few, as no cohesive scien-\ntific approach exists on the assessment of the risk\nand uncertainty posed by LLMs. To date, even\nthe most comprehensive overviews of LLM risks\n(Weidinger et al., 2022) lack basic methodological\npractices such as the systematic retrieval of infor-\nmation from the disciplines of inquiry (Page et al.,\n2021). In some cases, strong projections about\nrisk impact are made without any scientific rigor\nwhatsoever (Hendrycks et al., 2023). Similarly,\nDobbe et al. (2021) note that while many technical\napproaches, including approaches related to ’math-\nematical criteria for “safety” or “fairness” have\nstarted to emerge, ‘their systematic organization\nand prioritization remains unclear and contested.\nAs a result, the lack of systematization and\nmethodological integrity in scientific work around\nLLM risks contributes to a credibility crisis which\nmay impact regulation and governance directly.\nInnovation Governance and Risk Relativization\nLearning from earlier experiences with risk and un-\ncertainty can also help NLP researchers understand\nhow risk has been dealt with in other policy areas.\nOne of the reasons why it is important to contex-\ntualize NLP research on risks into a broader reg-\nulatory landscape is because this area has already\ngenerated meaningful frameworks for the under-\nstanding of risk in the context of innovation gover-\nnance. Such frameworks include for instance the\nprinciple of responsible innovation, which calls for\n‘taking care of the future through collective steward-\n14https://github.com/leondz/garak/\n8716\nship of science and innovation in the present’ (von\nSchomberg, 2013). This is only one of the many\nother approaches that can guide decision-making\non technology regulation (Hemphill, 2020).\nA regulatory angle can also help with the rel-\nativization of risk - that is, putting risks into per-\nspective by considering other policy areas as well.\nFor instance, the UK Risk Register 2020 discusses\npotential risks and challenges that could cause sig-\nnificant disruption to the UK 15. The report themat-\nically groups six risks (malicious attacks, serious\nand organized crime, environmental hazards, hu-\nman and animal health, major accidents and soci-\netal risks). This insight is useful in understanding\nthe scale and diversity of risks that public policy\nneeds to account for. Such awareness could also\ncontribute to the generation of policy options that\ncan put LLM risks into perspective in relation to\nother categories of risks like the ones mentioned\nabove. Here, a more holistic perspective on risk\ncould also take a sectoral approach to LLM risks.\nFor instance, going back to the example of the\nlawyer who invented case law using ChatGPT, ex-\nisting legal and self-regulatory frameworks already\naddress the risk of negligence in conducting pro-\nfessional legal activities. Considering this context\ncan contribute with insights into whether it is re-\nally necessary to treat LLM-mediated information\nas a novel danger. While technology has gener-\nated a broad digital transformation (Verhoef et al.,\n2021), it adds layers to existing problems (e.g. so-\ncial inequality) which need policy interventions\nindependent from their digital amplification.\n4 Scientific Expertise, Social Media and\nRegulatory Capture\nRegulatory Capture As a source of evidence for\npolicy-makers, scientific expertise has increasingly\nplayed a central role in regulation (Paschke et al.,\n2019). In some supranational governance contexts,\nscientific expertise is called upon in procedures\nthat often require a certain level of transparency.\nThis is the case of the call for public comments\nwhich we have discussed above as part of the EU’s\n‘Better Regulation’ agenda. However, in the past\ndecade, the rise of science communication on so-\ncial media has somewhat changed the interaction\nbetween policy-makers and scientists (van Dijck\nand Alinejad, 2020). A lot of the public debate\n15https://www.gov.uk/government/publications/n\national-risk-register-2020\nbetween stakeholders from industry, academia and\npolicy relating to LLM risks is had on social media\nplatforms such as Twitter. This can pose a regula-\ntory capture problem. Regulatory capture occurs\nwhen the regulator advances the interests of the in-\ndustry it is supposed to regulate, or of special inter-\nest groups rather than pursuing the public interest\n(Carpenter and Moss, 2013). Regulatory capture\nfits within the private interest theories we explored\nin Section 2, and often refers to the influence exer-\ncised by industry over regulation processes (Saltelli\net al., 2022). The most recent example is OpenAI’s\nwhite paper suggesting narrow regulatory interpre-\ntations for general purpose high-risk AI systems to\nEuropean regulators 16. Similarly, multiple tech-\nnology executives of large companies using LLMs,\nsuch as HuggingFace or OpenAI have been testi-\nfying before the US Congress to propose industry-\nfriendly interpretations of AI risks. This is hap-\npening in a context of existing concerns around\nthe industry orchestration of research agendas in\nNLP (Abdalla et al., 2023), and science (Abdalla\nand Abdalla, 2021) in general. However, regula-\ntion can also be captured by special interest groups\nfrom civil society, and increasingly, academia. In\nthis meaning, regulatory capture has a cultural or\nvalue-driven dimension that encompasses ‘intellec-\ntual, ideological, or political forms of dominance’\n(Saltelli et al., 2022). In a landscape where spe-\ncial interest groups are increasingly represented by\npopular science communicators, a lot of questions\narise in relation to the power exercised by the ris-\ning impact of science influencers, whether from\nacademic, journalism and industry environments\n(Zhang and Lu, 2023).\nThe Rise of Science Influencers Traditionally,\nscience communication has followed a conven-\ntional model dominated by professional actors gate-\nkeeping information (e.g. scientists, journalists and\ngovernment). Social media has led to the creation\nof a networked model of science communication,\nwith underlying socio-technical and political power\nshifts (van Dijck and Alinejad, 2020). Science in-\nfluencers are rooted in this development, as well as\nthe broader rise of social media influencers (Goanta\nand Ranchordás, 2020). They also bring with them\nadditional complexities. They may emerge from\na scientific background, but may use their plat-\nforms both for professional as well as personal\n16https://time.com/6288245/openai-eu-lobbyin\ng-ai-act/\n8717\nself-disclosure (Kim and Song, 2016). In doing so,\nthey also become political influencers who ’harness\ntheir digital clout to promote political causes and\nsocial issues’ (Riedl et al., 2023). To signal the\nsocial media influence of such public opinion lead-\ners, some media outlets even rank them for awards\npurposes,17 or profile them vis-a-vis state of the art\nscientific expertise.18\nThis development is especially important since\nsome social media platforms are more relevant for\nregulators than others. A recent report published\nby Oxford University’s Reuters Institute shows that\nTwitter is the platform politicians pay most atten-\ntion to across all studied markets. 19 With this in\nmind, the polarized narratives around LLM risks\nunfolding on social media pose the danger that\nscientific expertise is only partially represented in\npublic debates, in spite of the promises of speech\ndemocratization expected from these platforms in\nthe context of science communication.\nWhile followers and engagement may be a mea-\nsure of popularity with some communities and\nstakeholders, it raises concerns relating to the role\npopularity metrics and algorithmic amplification\non social media may have in representing scien-\ntific or industry consensus before policy-makers.\nAs Zhang et al. (2018) put it, there is a popularity\nbias that means ‘attention tends to beget attention’.\nIn other words, ‘the more contacts you have and\nmake, the more valuable you become, because peo-\nple think you are popular and hence want to connect\nwith you’ (van Dijck, 2013).\nHow exactly scientific popularity influences reg-\nulation still needs to be explored in greater detail,\nparticularly as a novel example of potential regu-\nlatory capture. What we know so far is that social\nmedia influencers can be highly effective in rely-\ning on authenticity and para-social relations for\npersuasion purposes (Vannini and Franzese, 2008;\nHudders et al., 2021). In this context, popularity\ndetermines power relationships within social me-\ndia networks that may capture regulatory processes\nin two ways. First, by exercising persuasion over\npolicy-makers as audiences through visibility and\npopularity. Simply put, not all research that is avail-\nable in a given field is presented on social media.\n17https://www.euractiv.com/section/digital/new\ns/meet-the-2019-euinfluencer-awardees/\n18https://spectrum.ieee.org/artificial-general\n-intelligence\n19https://reutersinstitute.politics.ox.ac.uk/d\nigital-news-report/2023\nUnder the premise of basing policy on scientific ev-\nidence, politicians may rely on research that gains\nvisibility due to amplification by scientific influ-\nencers, particularly when social media popularity\nis doubled by the brand power of prestigious aca-\ndemic institutions. Second, by amplifying polar-\nized debates that may trigger policy options which\nare not sufficiently informed through transparent\nand collective processes of evidence gathering. If\nmultiple AI research groups are vocal on social\nmedia about the future of AI research, this fuels\na race towards AI regulation. This can take away\nfrom the thoroughness that is necessary in collect-\ning evidence for such a complex field.\n5 Regulation and NLP (RegNLP): A New\nField\nThe danger of regulatory capture, taken together\nwith the lack of systematization in the identification\nand measurement of risk and uncertainty around\nLLMs, calls for a cohesive scientific agenda and\nstrategy. In 2023, the world counts 8 billion hu-\nmans20, and further digitalization and automation\nare not only unstoppable, but also absolutely nec-\nessary. Technological innovation is currently vital\nin the governance of our society. That does not\nmean its pursuit ought to be free from regulatory\nframeworks mandating rules on how to deal with\nthe risks it poses. NLP research has already drawn\nattention to some of the potential risks of LLMs.\nTo consolidate this effort, it is necessary to consider\nin what direction NLP research can further develop\nand what contributions it can make to regulation.\nA concrete proposal we advocate for is the creation\nof a new field of scientific inquiry which we call\nRegulation and NLP (RegNLP). RegNLP has three\nessential features which we discuss below.\n5.1 Multidisciplinarity\nFirst, RegNLP needs to be a multidisciplinary field\nthat spans across any scientific areas of study which\nare relevant for the intersection of regulation and\nAI. In the past years, multidisciplinary communi-\nties have been increasingly popular. An example\nis the ACM Fairness, Accountability and Trans-\nparency Conference (FAccT21), which often fea-\ntures NLP research. Such research communities\n20https://ourworldindata.org/world-populatio\nn-growth. For a brief comparison, at the time of the Dart-\nmouth AI workshop in 1956, the world population was at a\nmere 2.5 billion, Statista, 2023.\n21https://facctconference.org\n8718\nform around the disciplines that are most reflected\nby their research questions. For RegNLP, the con-\nstitutive disciplines ought to include NLP and reg-\nulation studies but also law, economics, political\nscience, etc.. NLP research approaches cannot re-\nplace expertise from other fields. At the same time,\nexpertise entails more than an interest in an adja-\ncent field, but rather a deep understanding of the\ncontributions and limitations such a field can entail\nwhen interacting with NLP. One strategy to encour-\nage this cross-pollination is for NLP researchers\ninterested in regulation to co-author papers with\nregulation experts and other relevant scholars. In\ndoing so, RegNLP can also contribute to the re-\nsearch gaps in other fields, such as public admin-\nistration, where literature on AI still needs further\ndevelopment. For instance, in 2019, only 12 sci-\nentific articles were published on AI and public\npolicy and administration, mostly focused on the\nuse of AIin public administration (Valle-Cruz et al.,\n2020). Similarly, a quick search in ‘Regulation &\nGovernance’, a leading journal in regulation stud-\nies, yields a total of 18 results, out of which only\none discusses AI risks (Laux et al., 2023).\n5.2 Harmonized Methodologies\nRegNLP needs harmonized methodologies. One\nof the biggest problems with the consolidation of\nmultidisciplinary research agendas and communi-\nties relates to the lack of alignment between the\ndifferent methods and goals pursued by different\nresearchers. This issue trickles down into all rel-\nevant activities which normally help consolidate\nmultidisciplinary groups, and is most specifically\nvisible in the process of peer review. If review-\ners are not familiar with methodologies from other\nfields, they will be unable to adequately assess the\nquality of research (Laufer et al., 2022). This can\nlead to the publication of research which may be\ninteresting across disciplines, but which may not\nmeet the methodological rigor of the scientific dis-\ncipline to which a given method pertains.\nRegNLP can help establish shared standards for\nscientific quality around shared methodologies and\nscience practices. This can mean embracing a di-\nverse methodological scope to reflect the tools that\nare needed in the inquiry of different types of re-\nsearch questions. It can also mean perfecting exist-\ning methods and deploying them on novel sources\nof data, as NLP research methods are a natural\nstarting point for the systemic retrieval of complex\ninformation and overviews from existing scientific\nresearch, such as meta-studies (Heijden, 2021).\n5.3 Science Participation in Regulation\nRegNLP can help research on regulation and NLP\ninterface with regulatory processes. At a time of\nincreased complexity, it is important for scientists\nto clarify the state of art of fast paced technolog-\nical change. Using harmonized methodologies in\nthe context of a multidisciplinary research agenda\ncan bring much needed coordination to the inter-\naction between NLP development and regulation.\nIn the absence of such coordination, as we have\ndiscussed in Section 4, there is a potential dan-\nger that policy-makers are only exposed to popular\nscientific opinions instead of consolidated science\ncommunication. What is more, embedding Reg-\nNLP into a risk and regulation context can offer\nfurther inspiration for the role of academia, as a\nrepository of public trust. A lot of regulatory agen-\ncies and standardization bodies govern the imple-\nmentation of regulation. In addition, new forms of\ninteractions with civil society are being set up by\nEU regulation, such as the Digital Services Act’s\n‘trusted flaggers’, namely organizations that can\nflag illegal content on online platforms. Similarly,\nthere can be new roles to play for RegNLP agendas\nand communities.\n6 Conclusion\nLLMs reflect a momentous development in NLP re-\nsearch. As they unleash a new era of automation, it\nis important to understand their risks and how these\nrisks can be controlled. While eager to engage with\nregulatory matters, NLP research on LLM risks\nhas so far been disjointed from other fields which\nare of direct interest, such as regulation studies. In\nparticular, the field of risk and uncertainty has been\nconceptualizing and discussing scientific risks for\ndecades. In this paper, we introduced these two\nareas of study and explained why it would be bene-\nficial for NLP research to consider them in greater\ndepth. In doing so, we also raised concerns relat-\ning to the fact that a lot of scientific debates on\nNLP risks are taking place on social media. This\nmay lead to regulatory capture, or in other words\nthe exercise of influence over law-makers, who are\nnotoriously active on social media platforms. To\ntackle these issues, we propose a new multidisci-\nplinary area of scientific inquiry at the intersection\nof regulation and NLP (RegNLP), aimed at the\n8719\ndevelopment of a systematic approach for the iden-\ntification and measurement of risks arising out of\nLLMs and NLP technology more broadly.\nLimitations\nOur paper reflects on the future of NLP in a land-\nscape where interest in regulation is increasing ex-\nponentially, within and outside the field. Given\nthe nature of this paper, we will refer to limita-\ntions dealing with the feasibility of our proposed\nresearch agenda. The most important limitation\nreflects the discussion around inter- and multidis-\nciplinarity. This is by no means a new theme in\nscience, and its implementation has cultural, man-\nagerial and economic implications that we do not\ndiscuss in the paper, but which are important to\nacknowledge. Similarly, another limitation is re-\nflected by the modest amount of knowledge we\nhave relating to the impact of social media influ-\nencers (such as science influencers) on regulation\nand public policy. In this paper, we raise certain\nissues around science communication as the start-\ning point of a broader discussion around power\nand influence in law-making as amplified by social\nmedia.\nAcknowledgements\nIlias Chalkidis is funded by the Novo Nordisk Foun-\ndation (grant NNF 20SA0066568). Sofia Ranchor-\ndas is funded by the NWO-Vidi project ’Vulnera-\nbility in the Digital Administrative State’. Catalina\nGoanta is funded by the ERC Starting Grant HU-\nMANads (ERC-2021-StG No 101041824).\nReferences\nMohamed Abdalla and Moustafa Abdalla. 2021. The\ngrey hoodie project: Big tobacco, big tech, and the\nthreat on academic integrity. In Proceedings of the\n2021 AAAI/ACM Conference on AI, Ethics, and Soci-\nety, AIES ’21, page 287–297, New York, NY , USA.\nAssociation for Computing Machinery.\nMohamed Abdalla, Jan Philip Wahle, Terry Ruas, Au-\nrélie Névéol, Fanny Ducel, Saif M. Mohammad, and\nKarën Fort. 2023. The elephant in the room: Ana-\nlyzing the presence of big tech in natural language\nprocessing research.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R.\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\nreport.\nRobert J Aumann and Roberto Serrano. 2008. An eco-\nnomic index of riskiness. J. Polit. Econ., 116(5):810–\n836.\nSolon Barocas, Moritz Hardt, and Arvind Narayanan.\n2019. Fairness and Machine Learning: Limitations\nand Opportunities. fairmlbook.org. http://www.fa\nirmlbook.org.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nAbeba Birhane, Vinay Uday Prabhu, and Emmanuel\nKahembwe. 2021. Multimodal datasets: misogyny,\npornography, and malignant stereotypes.\nJulia Black. 2008. Constructing and contesting legit-\nimacy and accountability in polycentric regulatory\nregimes. Regul. Gov., 2(2):137–164.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n8720\n5476, Online. Association for Computational Lin-\nguistics.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, Shyamal Buch, Dallas\nCard, Rodrigo Castellon, Niladri Chatterji, Annie\nChen, Kathleen Creel, Jared Quincy Davis, Dora\nDemszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy,\nKawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor\nGale, Lauren Gillespie, Karan Goel, Noah Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil\nJain, Dan Jurafsky, Pratyusha Kalluri, Siddharth\nKaramcheti, Geoff Keeling, Fereshte Khani, Omar\nKhattab, Pang Wei Koh, Mark Krass, Ranjay Kr-\nishna, Rohith Kuditipudi, Ananya Kumar, Faisal Lad-\nhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle\nLevent, Xiang Lisa Li, Xuechen Li, Tengyu Ma,\nAli Malik, Christopher D. Manning, Suvir Mirchan-\ndani, Eric Mitchell, Zanele Munyikwa, Suraj Nair,\nAvanika Narayan, Deepak Narayanan, Ben Newman,\nAllen Nie, Juan Carlos Niebles, Hamed Nilforoshan,\nJulian Nyarko, Giray Ogut, Laurel Orr, Isabel Pa-\npadimitriou, Joon Sung Park, Chris Piech, Eva Porte-\nlance, Christopher Potts, Aditi Raghunathan, Rob\nReich, Hongyu Ren, Frieda Rong, Yusuf Roohani,\nCamilo Ruiz, Jack Ryan, Christopher Ré, Dorsa\nSadigh, Shiori Sagawa, Keshav Santhanam, Andy\nShih, Krishnan Srinivasan, Alex Tamkin, Rohan\nTaori, Armin W. Thomas, Florian Tramèr, Rose E.\nWang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai\nWu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan\nYou, Matei Zaharia, Michael Zhang, Tianyi Zhang,\nXikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn\nZhou, and Percy Liang. 2022. On the opportunities\nand risks of foundation models.\nJason Borenstein, Frances S Grodzinsky, Ayanna\nHoward, Keith W Miller, and Marty J Wolf. 2021. AI\nethics: A long history and a recent burst of attention.\nComputer (Long Beach Calif.), 54(1):96–102.\nRoger Brownsword, Eloise Scotford, and Karen Yeung,\neditors. 2017. The oxford handbook of law, regu-\nlation and technology. Oxford Handbooks. Oxford\nUniversity Press, London, England.\nDaniel Carpenter and David A Moss, editors. 2013. Pre-\nventing regulatory capture. Cambridge University\nPress, Cambridge, England.\nPaul Christiano, Jan Leike, Tom B. Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nEuropean Commission. 2023a. Artificial intelligence\n– ethical and legal requirements. https://ec.e\nuropa.eu/info/law/better-regulation/have\n-your-say/initiatives/12527-Artificial-i\nntelligence-ethical-and-legal-requireme\nnts/feedback_en?p_id=24212003 . [Accessed\n16-Jun-2023].\nEuropean Commission. 2023b. Better regulation: why\nand how. https://commission.europa.eu/l\naw/law-making-process/planning-and-pro\nposing-law/better-regulation_en#:~:text\n=The%20Better%20Regulation%20agenda%20en\nsures,those%20that%20may%20be%20affected .\n[Accessed 16-Jun-2023].\nE Dal Bo. 2006. Regulatory capture: A review. Oxf.\nRev. Econ. Pol., 22(2):203–225.\nLeon Derczynski, Hannah Rose Kirk, Vidhisha\nBalachandran, Sachin Kumar, Yulia Tsvetkov,\nMR Leiser, and Saif Mohammad. 2023. Assessing\nlanguage model deployment with risk cards. arXiv\npreprint arXiv:2303.18190.\nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan\nMintz. 2021. Hard choices in artificial intelligence.\nArtif. Intell., 300(103555):103555.\nAnjalie Field, Amanda Coston, Nupoor Gandhi, Alexan-\ndra Chouldechova, Emily Putnam-Hornstein, David\nSteier, and Yulia Tsvetkov. 2023. Examining risks\nof racial biases in NLP tools for child protective ser-\nvices. In 2023 ACM Conference on Fairness, Ac-\ncountability, and Transparency, New York, NY , USA.\nACM.\nLuciano Floridi. 2023. The ethics of artificial intelli-\ngence the ethics of artificial intelligence . Oxford\nUniversity Press, London, England.\nIason Gabriel. 2020. Artificial intelligence, values, and\nalignment. Minds Mach., 30(3):411–437.\nMegan Garber. 2012. The IVF Panic: ’All Hell Will\nBreak Loose, Politically and Morally, All Over the\nWorld’ — theatlantic.com. https://www.theatl\nantic.com/technology/archive/2012/06/the\n-ivf-panic-all-hell-will-break-loose-pol\nitically-and-morally-all-over-the-world/2\n58954/. [Accessed 16-Jun-2023].\nAudley Genus and Andy Stirling. 2018. Collingridge\nand the dilemma of control: Towards responsible and\naccountable innovation. Res. Policy, 47(1):61–69.\nCatalina Goanta and Sofia Ranchordás. 2020. The reg-\nulation of Social Media Influencers. Edward Elgar\nPublishing.\n8721\nJanet Gough. 1988. Risk and uncertainty. [Accessed\n16-Jun-2023].\nJeroen Heijden. 2021. Why meta-research matters to\nregulation and governance scholarship: An illustra-\ntive evidence synthesis of responsive regulation re-\nsearch. Regul. Gov., 15(S1).\nThomas A Hemphill. 2020. “the innovation governance\ndilemma: Alternatives to the precautionary princi-\nple”. Technol. Soc., 63(101381):101381.\nDan Hendrycks, Mantas Mazeika, and Thomas Wood-\nside. 2023. An overview of catastrophic ai risks.\nVeerle Heyvaert. 2011. Governing climate change: To-\nwards a new paradigm for risk regulation. Mod. Law\nRev., 74(6):817–844.\nDirk Hovy and Shrimai Prabhumoye. 2021. Five\nsources of bias in natural language processing. Lang.\nLinguist. Compass, 15(8):e12432.\nAnita Howarth. 2013. A ‘superstorm’: when moral\npanic and new risk discourses converge in the media.\nHealth Risk Soc., 15(8):681–698.\nLiselot Hudders, Steffi De Jans, and Marijke De Veir-\nman. 2021. The commercialization of social media\nstars: a literature review and conceptual framework\non the strategic use of social media influencers. Int.\nJ. Advert., 40(3):327–375.\nCeleste Kidd and Abeba Birhane. 2023. How AI can dis-\ntort human beliefs. Science, 380(6651):1222–1223.\nJihyun Kim and Hayeon Song. 2016. Celebrity’s self-\ndisclosure on twitter and parasocial relationships: A\nmediating role of social presence. Comput. Human\nBehav., 62:570–577.\nJohn R. Krebs. 2011. Handling uncertainty in science.\nPhilosophical Transactions: Mathematical, Physical\nand Engineering Sciences, (1956):4842–4852.\nBenjamin Laufer, Sameer Jain, A. Feder Cooper, Jon\nKleinberg, and Hoda Heidari. 2022. Four years\nof facct: A reflexive, mixed-methods analysis of\nresearch contributions, shortcomings, and future\nprospects. In Proceedings of the 2022 ACM Confer-\nence on Fairness, Accountability, and Transparency,\nFAccT ’22, page 401–426, New York, NY , USA. As-\nsociation for Computing Machinery.\nJohann Laux, Sandra Wachter, and Brent Mittelstadt.\n2023. Trustworthy artificial intelligence and the eu-\nropean union AI act: On the conflation of trustwor-\nthiness and acceptability of risk. Regul. Gov.\nJan Leike, David Krueger, Tom Everitt, Miljan Martic,\nVishal Maini, and Shane Legg. 2018. Scalable agent\nalignment via reward modeling: a research direction.\nCoRR, abs/1811.07871.\nMichael E. Levine and Jennifer L. Forrence. 1990. Reg-\nulatory capture, public interest, and the public agenda:\nToward a synthesis. Journal of Law, Economics, &\nOrganization, 6:167–198.\nGary E. Marchant, Braden R. Allenby, and Joseph R.\nHerkert. 2013. The Growing Gap Between Emerg-\ning Technologies and Legal-Ethical Oversight: The\nPacing Problem. Springer Publishing Company, In-\ncorporated.\nFatemehsadat Mireshghallah, Kartik Goyal, Archit\nUniyal, Taylor Berg-Kirkpatrick, and Reza Shokri.\n2022. Quantifying privacy risks of masked language\nmodels using membership inference attacks. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 8332–\n8347, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nBronwen Morgan and Karen Yeung. 2007. An intro-\nduction to law and regulation: Text and materials .\nCambridge University Press, Cambridge, England.\nAnthony Ogus. 2009. Regulation revisited. Public Law,\n(2):332–346.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nMatthew J Page, Joanne E McKenzie, Patrick M\nBossuyt, Isabelle Boutron, Tammy C Hoffmann,\nCynthia D Mulrow, Larissa Shamseer, Jennifer M\nTetzlaff, Elie A Akl, Sue E Brennan, Roger Chou,\nJulie Glanville, Jeremy M Grimshaw, Asbjørn Hrób-\njartsson, Manoj M Lalu, Tianjing Li, Elizabeth W\nLoder, Evan Mayo-Wilson, Steve McDonald, Luke A\nMcGuinness, Lesley A Stewart, James Thomas, An-\ndrea C Tricco, Vivian A Welch, Penny Whiting, and\nDavid Moher. 2021. The PRISMA 2020 statement:\nan updated guideline for reporting systematic reviews.\nSyst. Rev., 10(1):89.\nK Francis Park and Zur Shapira. 2017. Risk and un-\ncertainty. In The Palgrave Encyclopedia of Strategic\nManagement, pages 1–7. Palgrave Macmillan UK,\nLondon.\nMelanie Paschke, Andrea Pfisterer, Christian Hirschi,\nLuisa Last, Daniela Pauli, Bruno Studer, Jasmin\nSchubert, Robert Herrendörfer, and Kaitlin Elyse\nMc Nally. 2019. Evidence-based policymaking.\nTheodore M Porter. 2020. Objectivity and the politics\nof disciplines. In Trust in Numbers, pages 193–216.\nPrinceton University Press.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\n8722\nMartin J Riedl, Josephine Lukito, and Samuel C\nWoolley. 2023. Political influencers on social\nmedia: An introduction. Soc. Media Soc. ,\n9(2):205630512311779.\nMatthias C Rillig, Marlene Ågerstrand, Mohan Bi, Ken-\nneth A Gould, and Uli Sauerland. 2023. Risks and\nbenefits of large language models for the environ-\nment. Environ. Sci. Technol., 57(9):3464–3466.\nDavid Rolnick, Priya L. Donti, Lynn H. Kaack,\nKelly Kochanski, Alexandre Lacoste, Kris Sankaran,\nAndrew Slavin Ross, Nikola Milojevic-Dupont,\nNatasha Jaques, Anna Waldman-Brown, Alexan-\ndra Sasha Luccioni, Tegan Maharaj, Evan D. Sher-\nwin, S. Karthik Mukkavilli, Konrad P. Kording,\nCarla P. Gomes, Andrew Y . Ng, Demis Hassabis,\nJohn C. Platt, Felix Creutzig, Jennifer Chayes, and\nYoshua Bengio. 2022. Tackling climate change with\nmachine learning. ACM Comput. Surv., 55(2).\nKevin Roose. 2023. A.I. Poses ‘Risk of Extinction,’\nIndustry Leaders Warn — nytimes.com. https:\n//www.nytimes.com/2023/05/30/technolog\ny/ai-threat-warning.html . [Accessed 04-Jun-\n2023].\nAndrea Saltelli, Dorothy J Dankel, Monica Di Fiore,\nNina Holland, and Martin Pigeon. 2022. Science,\nthe endless frontier of regulatory capture. Futures,\n135(102860):102860.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni. 2020. Green AI. Commun. ACM, 63(12):54–\n63.\nPhilip Selznick. 1985. Focusing organizational research\non regulation. Regulatory policy and the social sci-\nences, 1(1):363–367.\nFelice Simonelli and Nadina Iacob. 2021. Can we better\nthe european union better regulation agenda? Euro-\npean Journal of Risk Regulation, 12(4):849–860.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 3008–3021. Curran Associates,\nInc.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nZeerak Talat, Aurélie Névéol, Stella Biderman, Miruna\nClinciu, Manan Dey, Shayne Longpre, Sasha Luc-\ncioni, Maraim Masoud, Margaret Mitchell, Dragomir\nRadev, Shanya Sharma, Arjun Subramonian, Jaesung\nTae, Samson Tan, Deepak Tunuguntla, and Oskar Van\nDer Wal. 2022. You reap what you sow: On the chal-\nlenges of bias evaluation under multilingual settings.\nIn Proceedings of BigScience Episode #5 – Workshop\non Challenges & Perspectives in Creating Large Lan-\nguage Models, pages 26–41, virtual+Dublin. Associ-\nation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nDimitrios Tsarapatsanis and Nikolaos Aletras. 2021. On\nthe ethical limits of natural language processing on\nlegal text. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n3590–3599, Online. Association for Computational\nLinguistics.\nDavid Valle-Cruz, J Ignacio Criado, Rodrigo Sandoval-\nAlmazán, and Edgar A Ruvalcaba-Gomez. 2020. As-\nsessing the public policy-cycle framework in the age\nof artificial intelligence: From agenda-setting to pol-\nicy evaluation. Gov. Inf. Q., 37(4):101509.\nJeroen van den Hoven. 2014. Responsible innovation:\nA new look at technology and ethics. In Responsi-\nble Innovation 1, pages 3–13. Springer Netherlands,\nDordrecht.\nJose van Dijck. 2013. The culture of connectivity. Ox-\nford University Press, New York, NY .\nJosé van Dijck and Donya Alinejad. 2020. Social media\nand trust in scientific expertise: Debating the covid-\n19 pandemic in the netherlands. Soc. Media Soc. ,\n6(4):205630512098105.\nPhillip Vannini and Alexis Franzese. 2008. The authen-\nticity of self: Conceptualization, personal experience,\nand practice. Sociol. Compass, 2(5):1621–1637.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Proceedings of the 31st Interna-\ntional Conference on Neural Information Processing\nSystems, pages 6000–6010, Long Beach, California,\nUSA.\nMichael Veale, Kira Matus, and Robert Gorwa. 2023.\nAi and global governance: Modalities, rationales,\ntensions. Annual Review of Law and Social Science,\n19.\nPeter C Verhoef, Thijs Broekhuizen, Yakov Bart, Abhi\nBhattacharya, John Qi Dong, Nicolai Fabian, and\nMichael Haenlein. 2021. Digital transformation: A\nmultidisciplinary reflection and research agenda. J.\nBus. Res., 122:889–901.\nRené von Schomberg. 2013. A vision of responsible\nresearch and innovation. In Responsible Innovation,\npages 51–74. John Wiley & Sons, Ltd, Chichester,\nUK.\n8723\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh,\nConor Griffin, Po-Sen Huang, John Mellor, Amelia\nGlaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,\nCourtney Biles, Sasha Brown, Zac Kenton, Will\nHawkins, Tom Stepleton, Abeba Birhane, Lisa Anne\nHendricks, Laura Rimell, William Isaac, Julia Haas,\nSean Legassick, Geoffrey Irving, and Iason Gabriel.\n2022. Taxonomy of risks posed by language models.\nIn Proceedings of the 2022 ACM Conference on Fair-\nness, Accountability, and Transparency, FAccT ’22,\npage 214–229, New York, NY , USA. Association for\nComputing Machinery.\nRobert Wolfe, Yiwei Yang, Bill Howe, and Aylin\nCaliskan. 2023. Contrastive language-vision ai mod-\nels pretrained on web-scraped multimodal data ex-\nhibit sexual objectification bias. In Proceedings of\nthe 2023 ACM Conference on Fairness, Accountabil-\nity, and Transparency, FAccT ’23, page 1174–1185,\nNew York, NY , USA. Association for Computing\nMachinery.\nJennifer Yarnold, Ray Maher, Karen Hussey, and\nStephen Dovers. 2022. Uncertainty. In Routledge\nHandbook of Global Environmental Politics, pages\n253–268. Routledge, London.\nAnnie Li Zhang and Hang Lu. 2023. Scientists\nas influencers: The role of source identity, self-\ndisclosure, and anti-intellectualism in science com-\nmunication on social media. Social Media + Society,\n9(2):20563051231180623.\nYini Zhang, Chris Wells, Song Wang, and Karl Rohe.\n2018. Attention and amplification in the hybrid me-\ndia system: The composition and activity of donald\ntrump’s twitter following during the 2016 presidential\nelection. New Media Soc., 20(9):3161–3182.\n8724"
}