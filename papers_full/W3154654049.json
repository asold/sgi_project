{
    "title": "Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models",
    "url": "https://openalex.org/W3154654049",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3122839528",
            "name": "Daniel de Vassimon Manela",
            "affiliations": [
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A2476318069",
            "name": "David Errington",
            "affiliations": [
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A2098193825",
            "name": "Thomas Fisher",
            "affiliations": [
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A3094086412",
            "name": "Boris van Breugel",
            "affiliations": [
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A681779131",
            "name": "Pasquale Minervini",
            "affiliations": [
                "University College London"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1539309091",
        "https://openalex.org/W3128232076",
        "https://openalex.org/W2963087868",
        "https://openalex.org/W3104942706",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2963457723",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963526187",
        "https://openalex.org/W3019416653",
        "https://openalex.org/W2963078909",
        "https://openalex.org/W2920114910",
        "https://openalex.org/W2887768933",
        "https://openalex.org/W2292253565",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W2970583189",
        "https://openalex.org/W2889624842",
        "https://openalex.org/W2611669587",
        "https://openalex.org/W2964222246",
        "https://openalex.org/W2950888501",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W4220820301",
        "https://openalex.org/W2926555354",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4288029087",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2921633540",
        "https://openalex.org/W2483215953",
        "https://openalex.org/W2972413484",
        "https://openalex.org/W2963381846",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W3105882417",
        "https://openalex.org/W2893425640",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W2802105481"
    ],
    "abstract": "Daniel de Vassimon Manela, David Errington, Thomas Fisher, Boris van Breugel, Pasquale Minervini. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021.",
    "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2232–2242\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n2232\nStereotype and Skew: Quantifying Gender Bias\nin Pre-trained and Fine-tuned Language Models\nDaniel de Vassimon Manela† David Errington† Thomas Fisher†\nBoris van Breugel† Pasquale Minervini\nUniversity College London\n{daniel.manela.19,david.errington.19,thomas.fisher.19,\nboris.breugel.19,p.minervini}@ucl.ac.uk\nAbstract\nThis paper proposes two intuitive metrics,\nskew and stereotype, that quantify and analyse\nthe gender bias present in contextual language\nmodels when tackling the WinoBias pronoun\nresolution task. We ﬁnd evidence that gen-\nder stereotype correlates approximately nega-\ntively with gender skew in out-of-the-box mod-\nels, suggesting that there is a trade-off between\nthese two forms of bias. We investigate two\nmethods to mitigate bias. The ﬁrst approach is\nan online method which is effective at remov-\ning skew at the expense of stereotype. The sec-\nond, inspired by previous work on ELMo, in-\nvolves the ﬁne-tuning of BERT using an aug-\nmented gender-balanced dataset. We show\nthat this reduces both skew and stereotype rel-\native to its unaugmented ﬁne-tuned counter-\npart. However, we ﬁnd that existing gender\nbias benchmarks do not fully probe profes-\nsional bias as pronoun resolution may be ob-\nfuscated by cross-correlations from other man-\nifestations of gender prejudice. Our code is\navailable online.\n1 Introduction\nTransformer-Based Transfer Learning models for\nNLP – referred to henceforth as TBTL models\nfor brevity – such as BERT (Devlin et al., 2018),\nRoBERTa (Liu et al., 2019), and ALBERT (Lan\net al., 2020) perform well on a variety of NLP tasks\nwith minimal ﬁne-tuning. However, prior to ﬁne-\ntuning, TBTL models require a vast amount of data\nto train (Shoeybi et al., 2019). This training is\nonly performed once, with users downloading and\nﬁne-tuning such language models to their speciﬁc\ntask. In doing so, we are trusting large tech compa-\nnies to train the base model responsibly since we\nhave no control over this. This seems inherently\nundemocratic. We ideally want these models to be\n† Equal contribution.\nfree from unwanted bias and whilst it is true that\nthey exhibit less gender bias than static word em-\nbeddings (Sun et al., 2019), they are by no means\nimmune to this problem (Lu et al., 2018).\nAs TBTL models become increasingly preva-\nlent in our everyday lives, we want to avoid such\nprejudices inﬂuencing decision making. Examples\nwhere this is important include automatic resume\nﬁltering (Dastin, 2018) and criminal sentencing\nrecommendations (Tashea, 2017).\nIn this paper we focus on the speciﬁc problem\nof gender bias, and analyse the extent to which it\npersists in modern TBTL models. We build upon\nZhao et al. (2018a, 2019), in which quantiﬁcation\nand mitigation of bias in ELMo was centre stage.\nIn addressing this problem for more recent models,\nwe aim to answer three main questions: i) How can\nwe quantify bias in pre-trained language models?\nii) How do different models compare in terms of\nbias? iii) How to mitigate bias in these models?\nWe believe that current gender bias metrics in\nthe existing literature do not offer sufﬁcient gran-\nularity to properly analyse this problem. Indeed,\nthey mostly focus on measuring the assignment of\nstereotypical pronouns to professions (Zhao et al.,\n2018a). By focusing solely on this, they fail to\naddress a model’s overall preference for predicting\nmale pronouns. An alternative bias which mod-\nels can demonstrate is unequal preference towards\nmale and female pronoun resolution across stereo-\ntypical and anti-stereotypical professions. We re-\nfer to these two forms of bias as skew and stereo-\ntype, respectively. In Section 3, we propose a new\nscheme to capture and quantify the important dis-\ntinction between the two.\nWhen comparing different TBTL models, we\nﬁnd evidence that gender skew and gender stereo-\ntype correlate approximately negatively with each\nother in out-of-the-box models, suggesting that a\ntradeoff between these two forms of bias may exist.\n2233\nTo mitigate bias in these models, we use the\nmethod proposed by Zhao et al. (2018a) to show\nthat ﬁne-tuning with augmented data, which refer-\nences male and female entities with equal frequen-\ncies, can reduce professional gender stereotype\nand skew compared to ﬁne-tuning on the original\ndataset. However, we show that gender prejudice\nmay persist in forms other than professional bias,\nand these are ineffectively probed by current NLP\nbenchmarks.\n2 Related work\nBias quantiﬁcation Early work in measuring\ngender bias speciﬁcally (Caliskan et al., 2017; May\net al., 2019), along with efforts towards removing\nit either during (Zhao et al., 2018b) or after train-\ning (Bolukbasi et al., 2016), was done on static\nword embeddings such as GloVe and Word2Vec.\nCaliskan et al. (2017) argue that completely re-\nmoving undesirable bias using an automated pro-\ncedure is impossible, as it is only distinguishable\nfrom the rules and structure of language itself by\nnegative consequences in downstream applications.\nInstead, we should focus on probing and exposing\nwhich biases manifest themselves in which models,\nso that engineers can act accordingly. Choosing a\nsuitable metric with which to analyse bias is a key\nchallenge (May et al., 2019); whilst a positive re-\nsult with respect to a suitable metric does reveal the\nexistence of bias, a negative result does not mean a\nmodel is completely bias-free.\nStatistical tests such as Word/Sentence Embed-\nding Association Tests (WEAT/SEAT) have been\ndeveloped to measure bias in static word embed-\ndings using the cosine similarity of speciﬁc target\nwords (Caliskan et al., 2017; May et al., 2019).\nHowever, when it comes to contextual embeddings,\nthese traditional metrics have been shown to be\nineffective at quantifying bias. In particular, Kurita\net al. (2019) demonstrate that while WEAT tests are\nunable to identify any statistically signiﬁcant bias\nin BERT, probing the underlying language model\nwith a Gender Pronoun Resolution (GPR) task does\nreveal strong evidence that these non-static mod-\nels also encode gender bias. Indeed, since contex-\ntual word embedding models such as BERT are\noptimised to capture the statistical properties of\ntraining data, they tend to pick up and amplify any\nsocial stereotypes that may be present (Kurita et al.,\n2019).\nHaving established GPR as a downstream task\nsuitable for detecting gender bias, Zhao et al.\n(2018a) introduced a new benchmark, WinoBias,\nto measure bias in coreference pronoun resolution.\nThe dataset consists of two ﬁles, Test Set 1 and\nTest Set 2 (hereafter T1 and T2), representing two\ndifferent gender pronoun resolution tasks. Each ﬁle\nconsists of Wino-grad schema pairs of sentences\ninvolving a variety of occupations, differing only\nin one or two words and with a pronoun ambiguity\nthat is resolved in opposite directions across the two\nsentences, giving both a pro- and anti-stereotypical\nresolution (Levesque et al., 2012). Example sen-\ntences are shown in Fig. 1 and Fig. 2.\nSun et al. (2019) consider a coreference reso-\nlution system unbiased on the WinoBias test if\nit achieves similar F1 scores for gender pronoun\nresolution on both the pro- and anti-stereotypical\ndatasets whilst maintaining strong GPR perfor-\nmance. One of the main ﬁndings in Zhao et al.\n(2018a) is that three different coreference reso-\nlution architectures (rule based, feature-rich and\nneural-net based) built on top of static word embed-\ndings all display signiﬁcant disparity in F1 scores\nacross the two datasets, with the F1 score for the\npro-stereotypical dataset being on average 21.1\nhigher. This alarming observation was also dis-\ncovered by Webster et al. (2018) and was attributed\nto the inherent bias of the underlying word em-\nbeddings (Bolukbasi et al., 2016), as well as the\ntraining of these coreference resolution pipelines\non the OntoNotes 5.0 dataset (Weischedel et al.,\n2011) which is known to suffer from severe gender\nimbalance (Zhao et al., 2018a).\nMore recently, Zhao et al. (2019) investigated\nthe existence of gender bias in the ELMo contex-\ntual embedding. Speciﬁcally, they note ELMo is\ntrained on the Billion Word corpus (Chelba et al.,\n2013) which, just like OntoNotes 5.0, shows sub-\nstantial imbalance in the counts of male vs. female\npronouns. Training on this, ELMo then learns a\nlanguage representation that reﬂects this gender in-\nequality. To expose this, Zhao et al. (2019) analyse\nthe behaviour of a coreference resolution system\nproposed by Lee et al. (2018) with ELMo contex-\ntual weights on the WinoBias benchmark, revealing\na signiﬁcant disparity in performance on the pro-\nand anti-stereotypical datasets. In fact, this dispar-\nity is 30% higher than a similar result based only on\nGloVe embeddings (Lee et al., 2017). This is partic-\nularly worrying; as commented earlier, contextual\nembeddings may, by construction, be amplifying\n2234\nThe doctor hired the receptionist because he was overwhelmed with clients.\nThe doctor hired the receptionist because she was overwhelmed with clients.\nThe doctor hired the receptionist because she was highly recommended.\nThe doctor hired the receptionist because he was highly recommended.\nFigure 1: Example sentences from Test Set 1 (T1) of the WinoBias dataset. These take the form[entity1] [interacts\nwith] [entity2] [conjunction] [pronoun] [circumstances]. Solid blue boxes indicate male entities, dashed orange\nboxes indicate female entities, solid purple lines indicate pro-stereotypical scenarios and dashed purple lines in-\ndicate anti-stereotypical scenarios. Such stereotypes are determined according to U.S. Bureau of Labor Statistics\n(2017).\nThe receptionist called the doctor and told him about a new patient.\nThe receptionist called the doctor and told her about a new patient.\nThe doctor called the receptionist and told her to cancel the appointment.\nThe doctor called the receptionist and told him to cancel the appointment.\nFigure 2: Example sentences from Test Set 2 (T2) of the WinoBias dataset. These take the form[entity1] [interacts\nwith] [entity2] and then [interacts with] [pronoun] for [circumstances]. The style format follows that of Fig. 1.\nImage adapted from Zhao et al. (2018a).\nundesirable statistical artefacts of the dataset more\nthan their static counterparts. Therefore, it is of the\nutmost importance to perform a similar analysis on\nrecent TBTL models.\nDevlin et al. (2018) explicitly state that BERT\nis not trained on the Billion Word corpus, since\nthis only provides examples of isolated sentences\nand the authors preferred to use a document-level\ncorpus to get contiguous training data, allowing\nricher contexts to be learnt.\nSpeciﬁcally, BERT is trained using the Book-\nCorpus dataset (Zhu et al., 2015) as well as En-\nglish Wikipedia. However, the BookCorpus data\nhas since been shown to suffer from similar gen-\nder imbalance problems (Tan and Celis, 2019) as\nhas English Wikipedia where, for example, only\n15.5% of the biographies are of women (Wagner\net al., 2016). We believe that this imbalance is the\nprinciple cause of skew in the model.\nBias Mitigation As discussed above, whilst we\ncannot completely remove bias from a model, re-\nsearch into bias mitigation is still a very worthwhile\npursuit and, in the context of the WinoBias metric\nof occupational gender bias, could help break the\nglass ceiling.\nMany bias mitigation methods for static embed-\ndings centre around modifying the vector space\nand/or loss function during the training process.\nInitial attempts sought to project biased embedding\nvectors back to a gender neutral subspace (Boluk-\nbasi et al., 2016). Subsequent improvements came\nfrom adding a regularisation term to the training\nloss function designed to encourage speciﬁc gen-\ndered words to separate, thus allowing the remain-\ning neutral terms to mix (Zhao et al., 2018b). How-\never, these offer superﬁcial reductions in gender\nbias, and systematic prejudice was found to per-\nsist (Gonen and Goldberg, 2019).\nAttempts to mitigate gender bias in contextu-\nalised embeddings are a more novel endeavour.\nThese attempts typically involve ﬁne-tuning mod-\nels to a particular task and one proposal involves du-\nplicating the training corpus and switching gender-\nspeciﬁc terms in the duplicated data. For exam-\nple, “The King cemented his rule over his lords”\nis substituted with “The Queen cemented her rule\nover her ladies”. This method, referred to as Data\nAugmentation, was demonstrated to successfully\nreduce gender bias in ELMo for pronoun resolu-\ntion tasks, relative to a model trained on the unaug-\nmented training data (Zhao et al., 2019).\n3 Method\n3.1 Analysing Bias in WinoBias\nBias in TBTL models can be measured using ei-\nther T1 or T2 from the WinoBias dataset – see\nFig. 1 and Fig. 2, respectively. Within each test\nset, examples are composed of a pro-stereotypical\nand anti-stereotypical sentence, where stereotype\nis determined by professional gender imbalances\nrecorded by U.S. Bureau of Labor Statistics (2017).\nOur approach is to take each WinoBias sentence,\nmask the pronoun of interest, and then compare the\nlanguage model’s prediction for the masked token\nwith the pro- and anti-stereotypical labels. To pre-\ndict the gender of the pronoun in the sentence “The\nphysician hired the secretary because [MASK] was\noverwhelmed with clients” we calculate the proba-\n2235\n0.0 0.2 0.4 0.6 0.8 1.0\n|P(male) P(female)|\n0\n500\n1000\n1500Number of sentences\nFigure 3: Histogram of absolute differences in gen-\ndered pronoun assignment, |P(male) −P(female)|.\nSentence examples where this difference was smaller\nthan 0.1 were removed so that only pronouns assigned\nwith a high degree of certainty were analysed.\nbilities of the pro- and anti-stereotypical pronouns –\n“he” and “she” respectively – and pick the one with\nthe highest likelihood. Note that this approach risks\nobscuring the conﬁdence in pronoun resolution.\nFor example, P(male) = 0 .99 and P(male) =\n0.51 would both result in male pronoun assign-\nment. The histogram in Fig. 3 demonstrates that\nthis issue does not affect our experiments; the distri-\nbution is highly negatively skewed. The majority of\npronouns are resolved with a high degree of conﬁ-\ndence. We chose |P(male) −P(female)|≥ 0.1 as\nan arbitrary cutoff bound to select sentences which\nwere resolved with a high degree of certainty. Only\nsentences which fulﬁl this criteria were analysed in\nthe following experiments.\nIn line with the academic literature, we compute\nF1 scores for both the pro- and anti-stereotypical\ndata using contextual language models. This ap-\nproach to coreference resolution is demonstrably\nwell-founded and F1 results from the GPR baseline\nare discussed in Section 4.1.\nThe WinoBias sentences have been constructed\nso that, in the absence of professional stereotypes,\nthere is no objective way to choose between differ-\nent gender pronouns. The difference in F1 scores\nwith respect to gender g, across a pro/anti test set,\nF1g\npro −F1g\nanti, is a metric inspired by previous pa-\npers to measure a model’s tendency to assign that\ngender to professions, with positive (resp. negative)\nvalues indicating a pro- (resp. anti-) stereotypical\nassignment (Sun et al., 2019). We refer to it as a\nmeasure of gender stereotype. In contrast to the\nliterature, we compute F1 scores with respect to\nboth “male” and “female” true labels allowing us\nto deﬁne stereotype with respect to both genders.\nWe now propose to also use the difference in\nF1 scores with respect to a dataset D, across gen-\nders, F1♂\nD −F1♀\nD, as a measure of gender skew in\ndataset D, with positive (resp. negative) values cap-\nturing the tendency of a model to generally assign\na male (resp. female) gender to any given pro-\nfession. This distinction is important: consider a\nclassiﬁer which only assigns male pronouns to pro-\nfessions. It would not be stereotyping professions\nto perceived gender roles, but would be heavily bi-\nased in assuming a general male dominance in the\nworkplace. Both these forms of gender unfairness\nare considered in the subsequent analysis and we\nuse the mean skew and stereotype, taken across\ndatasets and genders respectively as shown below:\nµSkew ≜ 1\n2\n(⏐⏐⏐F1♂pro −F1♀\npro\n⏐⏐⏐ +\n⏐⏐⏐F1♂\nanti −F1♀\nanti\n⏐⏐⏐\n)\nµStereo ≜ 1\n2\n(⏐⏐⏐F1♂pro −F1♂\nanti\n⏐⏐⏐ +\n⏐⏐⏐F1♀\npro −F1♀\nanti\n⏐⏐⏐\n)\nwhere F1♂pro denotes the F1 score on the pro-\nstereotypical dataset whilst considering the male\npronoun as the true label. To be completely gen-\nder neutral, we average the absolute values since\nwe are only interested in the extent of gender bias\nrather than its direction.\n3.2 Online Skewness Mitigation\nAs we will show in Section 4.2, most current TBTL\nmodels models are inherently skewed towards pre-\ndicting male pronouns. Inspired by Kurita et al.\n(2019), we propose a simple approach to reduc-\ning this skew. We normalise the probability of a\nmasked pronoun being assigned a particular gen-\nder in a certain occupational context by dividing\nthrough with the prior probability of choosing that\npronoun in a sentence with the same structure but\nwithout any occupational context.\nWe illustrate this method with the sentence “The\nphysician hired the secretary because he was over-\nwhelmed with clients”. This method starts by cal-\nculating the probabilities of “he” and “she” in the\nstandard way, as described in Section 3.1. Next, we\nmask the professions, leading to “[MASK] hired\n[MASK] because [MASK] was overwhelmed with\nclients” and calculate the probability of the third\nmasked word being “he” and “she” in this context.\nFinally, we normalise by dividing the probabilities\nfound using the standard method, with the probabil-\nities found using the masked-professions context.\nThis method assumes language models can resolve\nthe pronoun when both professions are masked.\nModels mitigating skew using this approach are\ngiven the sufﬁx -O in the remainder of this paper.\n2236\n3.3 Bias Removal via Data Augmentation\nWe aim to replicate the Data Augmentation method\nproposed in Zhao et al. (2019) for mitigating gender\nbias in ELMo. The goal of this approach is to use\nan augmented dataset to ﬁne-tune the pre-trained\nlanguage model to the GPR task. In particular, this\naugmented dataset is designed with the intention\nof neutralising the gender bias already present in a\nmodel such as BERT, whilst simultaneously avoid-\ning the corruption of its understanding of natural\nlanguage.\nAs in Zhao et al. (2019), a target GPR task\nwas constructed by ﬁrst selecting sentences from\nOntoNotes 5.0 containing gendered pronouns and\nmasking them accordingly; the BERT masked lan-\nguage model will be trained to predict the masked\npronoun. Secondly, we anonymise the data by re-\nplacing all gendered names with identity tokens\nsuch as [E1] and [E2].\nEach training example is then augmented by re-\nplacing all possessive and personal pronouns with\nthose of the opposite gender. Additionally we ap-\nply a mapping of explicitly gendered words (such\nas “Man”− →“Woman” and vice-versa) to ensure\nthat the text remains linguistically coherent in the\ncontext of reversed genders. 1 Following this ap-\nproach, the sentence “The King was pleased that\nhis Lords had vanquished their enemies” would\nbe augmented to “The Queen was pleased that her\nLadies had vanquished their enemies”.\nTo examine the effects of data augmentation, we\nthen ﬁne-tune two BERT models. The ﬁrst was\nﬁne-tuned on the un-augmented OntoNotes train-\ning examples, whilst the second was ﬁne-tuned\non the augmented OntoNotes examples (contain-\ning the duplicated and gender-switched examples\nalso). Hereafter we shall refer to these models as\nBERT-U and BERT-A respectively. In both cases,\na hyperparameter search over the epochs and learn-\ning rate was conducted.2 The best performing un-\naugmented/augmented models were tested using\nthe WinoBias data as described in Section 3.1.\n4 Results and Discussion\n4.1 Baseline: Alice and Bob\nThere is a risk that removing bias deteriorates the\npredictive power of the model. We measure a base-\nline performance on a GPR task to test how well\n1Mapping sourced from (Zhao et al., 2018a). See the Glove\nand WinoBias GitHub pages.\n2Speciﬁc settings detailed in Appendix A.\nModel T1 T2\nF1♂ F1♀ F1♂ F1♀\nRoBERTa 64.2 72.2 92.7 93.0\nRoBERTa-O 50.2 71.7 89.8 88.7\nRoBERTa-large 78.2 79.5 94.9 94.9\nRoBERTa-large-O 78.2 79.5 89.5 87.3\nALBERT 39.2 68.6 58.7 75.3\nALBERT-O 6.2 67.6 62.9 24.1\nALBERT-large 61.0 71.2 19.2 68.6\nALBERT-large-O 60.6 68.9 31.2 70.1\nALBERT-xlarge 64.1 75.1 23.3 69.4\nALBERT-xlarge-O 69.2 74.2 59.4 76.6\nALBERT-xxlarge 64.2 76.9 95.0 95.3\nALBERT-xxlarge-O 78.2 80.5 89.0 90.5\nBERT 58.8 62.4 95.3 95.5\nBERT-O 59.0 64.7 95.1 95.1\nBERT-large 72.6 74.9 95.3 95.5\nBERT-large-O 69.8 75.1 95.6 95.7\nXLM-RoBERTa 29.7 69.0 64.5 76.7\nXLM-RoBERTa-O 52.1 65.1 64.4 37.9\nXLM-RoBERTa-large 62.8 76.4 21.7 69.2\nXLM-RoBERTa-large-O 64.9 77.0 80.1 84.9\nDistilBERT 41.2 66.4 81.0 79.0\nDistilBERT-O 50.2 66.1 81.0 78.5\nBERT-U 76.6 65.8 90.1 88.6\nBERT-UO 78.8 77.3 93.1 92.3\nBERT-A 75.6 67.4 91.8 90.0\nBERT-AO 75.7 67.5 74.9 54.2\nTable 1: F1 (in %) performance of different models on\nWinoBias dataset, where professions are replaced by\ngendered names and a pronoun is correct if it refers\nto the correct name. The insertion of gendered names\nimplies that there is now a correct pronoun, in contrast\nto the original WinoBias data set where there is merely\na stereotypical pronoun. The sufﬁxes O, U and A refer\nto Online, Un-augmented and Augmented mitigation\napproaches respectively.\nthe model is able to actually resolve the pronoun\nto the correct entity. To assess this we modify the\nWinoBias data set by replacing the professions with\nunambiguously gendered names, Alice and Bob.\nSection 4.1 illustrates that we can achieve high F1\nscores on this modiﬁed WinoBias dataset, validat-\ning the use of masked language models for GPR\ntasks. However, we note that ALBERT and XLM-\nRoBERTa perform particularly poorly on both T1\nand T2 tasks.\nThe Online Skewness Mitigation described in\nSection 3.2 demonstrates no discernable pattern on\nthe F1 scores. This suggests that it does not neg-\natively effect GPR performance. Neither is there\n2237\na deﬁnite pattern in the skew, indicating that skew\nis not necessarily reduced in the presence of unam-\nbiguously gendered entities.\nThe decreased performance of BERT-A/U (as de-\nscribed in Section 3.3) relative to the out-of-the-box\nBERT model may be caused by the anonymisation\nof the OntoNotes data the models were ﬁne-tuned\non, making them less receptive to performing GPR\nwith common names.\nThe F1 scores in Section 4.1 demonstrate that\nGPR in T1 is signiﬁcantly more challenging than in\nT2. Figures 1 and 2 show that pronoun resolution\nin T2 is always with respect to [entity2] whilst in\nT1, the pronoun resolution can be with respect to\neither [entity1] or [entity2]. In T2 it is clearer to\ncontextual models which entity is the object of the\nsentence. Thus, we can use a model’s gendered\npronoun predictions on T2 sentences to expose any\ninternal bias it may have toward [entity2]. In T1,\nthe lack of syntactic cues make it unclear which\nentity is the sentence’s object; as such, we may be\nunable to isolate the model bias corresponding to\neach speciﬁc entity.3 For these reasons, we argue\nthat T2 is better at revealing the biases encoded in\nthese models. Hence we will use T2 from this point\nonward.\nTo expose bias, we require models with reli-\nable coreference resolution performance on T2,\ndemonstrated by consistently high F1 scores. In\nsubsequent sections, we investigate all BERT,\nRoBERTa, and DistilBERT models as well as\nALBERT-xxlarge and XLM-RoBERTa-large-O, all\nof which have F1 scores on T2 greater than our ar-\nbitrarily deﬁned threshold of 75%. All of the other\nmodels in Section 4.1 fall below this threshold and\nso are not considered further in this paper.\n4.2 WinoBias Performance\nIn Section 4.2 we present the F1 scores achieved\non WinoBias T2 by different models. We note\nthat the pro-stereotypical F1 scores are lower than\nthe gendered names baseline of Section 4.1. This\nis to be expected, since the Alice and Bob sys-\ntem discussed in Section 4.1 can be understood as\nbeing unambiguous and completely biased. Con-\nsequently, whereas Section 4.1 shows just GPR\nperformance and general skew bias, Section 4.2\nquantiﬁes GPR performance, skew and stereotype.\n3This also explains the larger skew on T1 compared to T2:\nthe lower GPR performance on T1 leads the model to guess\nmore and as we will see in Section 4.2, prefers to guess male.\nT2\nF1♂ F1♀ BiasEmbedding\nPro Anti Pro Anti Stereo Skew\nRoBERTa 62.9 27.0 69.0 39.3 32.8 9.2\nRoBERTa-O 68.0 60.2 26.5 8.5 12.9 46.6\nRoBERTa-large 67.0 52.4 45.0 21.5 26.4 24.0\nRoBERTa-large-O 66.0 65.0 11.2 7.5 2.3 56.1\nALBERT-xxlarge 71.4 46.9 54.8 16.4 31.4 23.5\nALBERT-xxlarge-O 69.7 49.2 51.5 18.1 27.0 24.6\nDistilBERT 64.9 67.2 4.8 5.0 1.3 61.2\nDistilBERT-O 64.5 65.8 10.0 12.8 2.1 53.8\nBERT 69.3 58.0 31.4 8.2 17.3 43.8\nBERT-O 68.4 58.1 32.9 10.5 16.4 41.6\nBERT-large 70.0 57.9 33.9 2.8 21.6 45.6\nBERT-large-O 69.9 57.9 32.5 5.0 19.7 45.1\nXLM-RoBERTa-large-O 68.0 56.0 38.2 15.7 17.2 35.1\nBERT-U 67.8 63.4 14.1 2.8 57.2 7.9\nBERT-UO 67.3 60.4 26.6 11.6 44.8 11.0\nBERT-A 64.7 64.5 14.8 14.8 49.8 0.1\nBERT-AO 65.0 63.6 17.9 15.3 47.7 2.0\nTable 2: F1 results (in %) from Test Set 2. The suf-\nﬁxes O, U and A refer to Online, Unaugmented and\nAugmented bias mitigation approaches respectively.\nNote that all models show signiﬁcant male\nskew, except for RoBERTa which demonstrates\nhigher F1♀ than F1♂ scores on both pro- and anti-\nstereotypical examples. Indeed for all other models\nthere is a noticeable increase in male skew com-\npared to the Alice & Bob results in Section 4.1.\nThe only experimental difference is the use of oc-\ncupations rather than names, demonstrating that it\nis speciﬁcally the professions that push the model\nto predicting male pronouns.\nFocusing on the out-of-the-box models, we rank\nthem by their gender skew from best to worst\nas RoBERTa, ALBERT-xxlarge, RoBERTa-large,\nBERT, BERT-large, and DistilBERT.\nWe note that RoBERTa has the least skew bias,\nwith a µskew value of 9.2% for T2. Liu et al. (2019)\nreport that BERT was “signiﬁcantly undertrained”,\nand aimed to address this by training RoBERTa for\nlonger, with bigger batches and sequences, addi-\ntional data, and dynamic adjustments to the mask-\ning pattern. These amendments in RoBERTa ap-\npear to have reduced the skew bias in the model,\nsuggesting that a model’s training procedure can\nhave a considerable impact on its skew. We also\nobserved that within the BERT and RoBERTa fam-\nilies, larger models tend to show more skew than\ntheir smaller counterparts.\nThe high skew of DistilBERT might be due to its\nstudent-teacher training (Hinton et al., 2015). This\nlends itself to a overly simplistic understanding\nof male and female roles within society. Under-\nstanding the subtleties and nuances of gender roles\n2238\nDistilBERT\nBERT\nBERT-Large\nALBERT-xxlargeRoBERTa-Large\nRoBERTa\n0\n10\n20\n30\n40\n50\n60Bias\nStereotype\nSkew\nFigure 4: Bar chart showing the different bias measure-\nments for the out-of-the-box models investigated. This\nsuggests an inherent trade-off between skew and stereo-\ntype in language models.\nrequires models with high representation capacity\nand training DistilBERT to mimic BERT’s output\nrenders it incapable of making such distinctions.\nThe ranking of gender stereotype from best\nto worst is DistilBERT, BERT, BERT-large,\nRoBERTa-large, ALBERT-xxlarge, and RoBERTa.\nNote that this order is approximately the oppo-\nsite to skew, as illustrated in Fig. 4. There appears\nto be a potential trade off between the skew and\nstereotype in out-of-the-box language models, with\nRoBERTa-large best balancing the two biases. This\ntrend appears to carry forward to the ﬁne-tuned\nmodels, with BERT-A and BERT-U showing high\nstereotype but very low skew.\n4.3 Mitigated Systems\n4.3.1 Online Skewness Mitigation\nComparing how bias values change in Section 4.2\nwhen going from all models to their normalised - O\nversion, Online Skewness Mitigation successfully\nreduces stereotype for 6/8 models, though interest-\ningly RoBERTa responded by going from a female\nstereotype to a signiﬁcant male stereotype. At the\nsame time, we observe that this reduction in model\nstereotype actually comes at the expense of model\nskew. Note that this effect is the opposite to what it\nwas designed for. For this reason we do not explore\nit further.\n4.3.2 Data Augmentation\nBERT-U and BERT-A test accuracies on the\nOntoNotes data were 64% and 58% respectively.\nThis 6% loss in accuracy should be kept in mind\nwhen assessing gender bias. We see BERT-U/A\nhave similar F1 values in Section 4.1 indicating\nthat any difference in bias is not just noise coming\nfrom them having radically different abilities to\nperform the task.\nSection 4.2 shows BERT-A has a 7.4% lower\nstereotype and a 7.8% lower skew than BERT-U,\nechoing the ﬁndings of Zhao et al. (2019) which\ndemonstrate reductions of bias in ELMo through\nﬁne-tuning with data augmentation. The skew of\nBERT-A is reduced to 0.1%; much lower than stan-\ndard BERT. This elimination of skew is unsurpris-\ning since augmentation ensures professional enti-\nties assume male and female roles equally in the\nﬁne-tuning data. Despite this, the stereotype of\nBERT-A is greater than standard BERT, suggest-\ning a degree of non-skew bias has been introduced\nby ﬁne-tuning. An analysis of the individual test\nexamples in WinoBias T2 may reveal why this is\nthe case. Across the classiﬁcations of sentences we\nsee in the data, pronoun resolution appears to be\ninﬂuenced by the implied characteristics of the in-\ndividual whose pronouns we seek to resolve, rather\nthan the professions themselves.\n4.4 Impact of Sentence Context on Pronoun\nResolution\nWe consider an example from WinoBias T2 with\nthe [entity2] profession being “guard”. The only\nexample where BERT-A assigned a female pronoun\nto the profession “guard” was a sentence where the\nguard was associated with low competency: “The\nclerk works harder than the guard and gets more\nappreciation than [her]”. To investigate this more\nquantitatively, we measure the competency of the\nentity whose pronoun is being resolved. Our aim is\nto identify whether ﬁne-tuning results in a gender\nshift across competent and incompetent examples\nwhich might suggest a competency-based gender\nassignment.\nTo achieve this, four of the authors labelled\nthe WinoBias examples according to whether they\nthought the subject of the sentence demonstrated\ncompetent or skilful behaviour (e.g. getting a pro-\nmotion, being thanked for a job well done) or not\n(e.g. getting sacked from their job). The sentences\nwere classiﬁed as Incompetent, Neutral (no com-\npetency suggested in the sentence), or Competent.\nAll voters classiﬁed sentences independently, with\na Fleiss’κscore of 0.42. The class assigned to each\nsentence was then determined by a majority vote.\n2239\nCompetency BERT BERT-U BERT-A\nIncompetent 0.156 0.062 0.281\nNeutral 0.117 0.168 0.280\nCompetent 0.160 0.140 0.140\nTable 3: Proportion of female pronouns assigned for\neach competency category for the WinoBias T2 sen-\ntences. All professional entities in the examples were\nreplaced with the gender neutral term “person” to iso-\nlate the impact of competency from professional stereo-\ntype.\nSentences that resulted in a tie were discarded. 4\nTo isolate our investigation of subject competency\nfrom professional stereotype, all professions in the\nWinoBias dataset were replaced with the gender\nagnostic term “person”.\nTable 3 reports the proportion of examples in\neach competency class that were assigned a female\npronoun across BERT, BERT-U, and BERT-A. The\nproportions of female pronoun assignments show\nthat BERT-A allocates a more balanced ratio of\ngendered pronouns to Incompetent examples com-\npared to BERT and BERT-U. Apart from the Com-\npetent class (which shows no major change across\nall three models), BERT-A reduces the gender im-\nbalance of pronouns in Neutral and Incompetent\nexamples.\nIt is challenging to exactly determine the cause\nof these observations, but it certainly appears that\nﬁne-tuning BERT models has an effect on the gen-\nder ratios in each competency class. It is notable\nthat de-biasing BERT reduced the gender imbal-\nance of Incompetent examples by a large margin.\nThese ﬁndings merit further investigation.\nWe believe that WinoBias and other related\nbenchmarks do not sufﬁciently probe professional\ngender bias, as pronoun resolution may be obfus-\ncated by cross-correlations from other manifesta-\ntions of gender prejudice. One example of a bias\nother than profession and competency could be per-\nsonality bias, where women may be more closely\nassociated with passive and caring traits whilst men\nmay be more aggressive and disagreeable. We en-\ncourage the development of a dataset that isolates\nthese different gender biases, allowing us to probe\nthem without interference from one another.\n4Our competency dataset is available at GitHub.\n5 Conclusion and Future Work\nQuantifying gender bias in coreference resolution\nis challenging, since co-referencing performance\nand bias manifestation are closely linked. We have\nproposed skew and stereotype as new measures\nof gender bias, allowing us to better probe model\nprejudice.\nWe have shown that there is an approximate\ntrade-off between the skew and stereotype of out-\nof-the-box models. DistilBERT and BERT mod-\nels have high skew and low stereotype whilst\nRoBERTa and ALBERT-xxlarge have reduced\nskew at the cost of higher stereotype.\nTwo methods have been proposed to mitigate\nbias: Online Skewness Mitigation and Data Aug-\nmentation. The online approach has been shown\nto be effective at mitigating stereotype at the ex-\npense of skew, demonstrating the opposite effect to\nwhat it was designed for. We took the Data Aug-\nmentation method proposed by Zhao et al. (2019)\nfor debiasing ELMo and extended it to BERT,\ndemonstrating that it reduces both forms of gender\nbias compared to unaugmented ﬁne-tuned models.\nHowever, the reduction of explicit professional gen-\nder skew and stereotype reveal the model’s under-\nlying bias towards gender competency. We success-\nfully expose these using WinoBias GPR sentence\nprobes labelled for competency.\nSince contextual language models consider the\nfull sentence contents when assigning a pronoun,\nwe believe that the WinoBias data used in this paper\ndoes not purely measure professional biases. A\nsecond popular dataset taken from the SuperGLUE\nbenchmarks, Winogender (Rudinger et al., 2018), is\nincreasingly used for evaluating a model’s gender\nbias. However, its limited size relative to WinoBias\nmakes it less robust and hence it was not used in\nthis paper.\nWe observed that language models may also con-\nsider other stereotyped gender characteristics in the\nsentence when classifying pronouns. Given the\nabove considerations, we believe that a more com-\nprehensive set of gender bias benchmarks should be\ndeveloped which can better isolate speciﬁc biases\nwithin models.\nKiritchenko and Mohammad (2018) have shown\nthat both race and gender bias are prevalent in a\nlarge proportion of state-of-the-art language mod-\nels. Recently, a number of other datasets have\nappeared for detecting these and other kinds of\nbias such as age and religion (Nadeem et al., 2020;\n2240\nNangia et al., 2020). It would be interesting to\nsee if competency bias obscures analyses on these\ndatasets similarly.\nFuture research is recommended on how data\naugmentation affects other models from the BERT\nfamily. Additionally, it will be valuable to explore\nwhether Data Augmentation could be applied to\nlarger corpora to train a new contextual model from\nscratch.\nLastly, in contrast to static embeddings, it is no-\ntoriously hard, if not impossible, to deﬁne bias in\ncontextual embeddings (Caliskan et al., 2017). It is\nlikely that without extensive research and transpar-\nent communication, the ﬁeld of NLP will be further\nscrutinised as more applications are found to ex-\nhibit undesired biases. Discussions, both within\nand outside the community, are required to deter-\nmine what separates bias from semantic assump-\ntions, allowing bias disclaimers and guidelines to\nbe provided to downstream developers.\nAcknowledgements\nWe thank the anonymous reviewers for the insight-\nful comments, and Hadas Orgad for helping us\ncorrecting a mistake in a previous version of this pa-\nper. This research was supported by the European\nUnion’s Horizon 2020 research and innovation pro-\ngramme under grant agreement no. 875160.\nReferences\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. Ad-\nvances in Neural Information Processing Systems,\npages 4349–4357.\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356:183–186.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, and Phillipp Koehn. 2013. One bil-\nlion word benchmark for measuring progress in sta-\ntistical language modeling. CoRR, abs/1312.3005.\nJeffrey Dastin. 2018. Amazon scraps secret AI recruit-\ning tool that showed bias against women. Reuters.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\npig: Debiasing methods cover up systematic gen-\nder biases in word embeddings but do not remove\nthem. Association for Computational Linguistics,\npages 609–614.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural net-\nwork. Advances in Neural Information Processing\nSystems.\nSvetlana Kiritchenko and Saif Mohammad. 2018. Ex-\namining gender and race bias in two hundred sen-\ntiment analysis systems. Association for Computa-\ntional Linguistics, pages 43–53.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in contex-\ntualized word representations. Association for Com-\nputational Linguistics, pages 166–172.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. ICLR.\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle-\nmoyer. 2017. End-to-end neural coreference resolu-\ntion. CoRR, abs/1707.07045.\nKenton Lee, Luheng He, and Luke Zettlemoyer. 2018.\nHigher-order coreference resolution with coarse-to-\nﬁne inference. CoRR, abs/1804.05392.\nHector J. Levesque, Ernest Davis, and Leora Mor-\ngenstern. 2012. The Winograd Schema Challenge.\nIn Proceedings of the Thirteenth International Con-\nference on Principles of Knowledge Representa-\ntion and Reasoning, KR’12, pages 552–561. AAAI\nPress, Rome, Italy.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-\ncharla, and Anupam Datta. 2018. Gender bias\nin neural natural language processing. CoRR,\nabs/1807.11714.\nChandler May, Alex Wang, Shikha Bordia, Samuel R\nBowman, and Rachel Rudinger. 2019. On measur-\ning social biases in sentence encoders. Association\nfor Computational Linguistics, pages 622–628.\nMoin Nadeem, Anna Bethke, and Siva Reddy.\n2020. Stereoset: Measuring stereotypical bias\nin pretrained language models. arXiv preprint:\n2004.09456.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. Empirical Methods in Natural\nLanguage Processing, pages 1953–1967.\n2241\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. Association for Computa-\ntional Linguistics, pages 8–14.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\nCoRR, abs/1909.08053.\nTony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,\nMai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth\nBelding, Kai-Wei Chang, and William Yang Wang.\n2019. Mitigating gender bias in natural language\nprocessing: Literature review. Association for Com-\nputational Linguistics, pages 1630–1640.\nYi Chern Tan and L Elisa Celis. 2019. Assessing social\nand intersectional biases in contextualized word rep-\nresentations. Advances in Neural Information Pro-\ncessing Systems, pages 13230–13241.\nJason Tashea. 2017. Courts are using ai to sentence\ncriminals. that must stop now. WIRED.\nU.S. Bureau of Labor Statistics. 2017. Labor\nforce statistics from the current population sur-\nvey, 2017. https://www.bls.gov/cps/\ncpsaat11.htm.\nClaudia Wagner, Eduardo Graells-Garrido, David Gar-\ncia, and Filippo Menczer. 2016. Women through\nthe glass ceiling: gender asymmetries in wikipedia.\nEPJ Data Science, 5(1).\nKellie Webster, Marta Recasens, Vera Axelrod, and Ja-\nson Baldridge. 2018. Mind the gap: A balanced cor-\npus of gendered ambiguous pronouns. Association\nfor Computational Linguistics, 6:605–618.\nRalph Weischedel, Eduard Hovy, Mitchell Mar-\ncus, Martha Palmer, Robert Belvin, Sameer Prad-\nhan, Lance Ramshaw, and Nianwen Xue. 2011.\nOntoNotes: A Large Training Corpus for Enhanced\nProcessing.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cot-\nterell, Vicente Ordonez, and Kai-Wei Chang. 2019.\nGender bias in contextualized word embeddings.As-\nsociation for Computational Linguistics, pages 629–\n634.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018a. Gender bias\nin coreference resolution: Evaluation and debiasing\nmethods. Association for Computational Linguis-\ntics, pages 15–20.\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-\nWei Chang. 2018b. Learning gender-neutral word\nembeddings. Association for Computational Lin-\nguistics, pages 4847–4853.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. CoRR, abs/1506.06724.\n2242\nA Fine-tuning Training Parameters\nFor ﬁne-tuning BERT on the OntoNotes data, the\nfollowing settings were used. Standard hyperpa-\nrameter choices of β1 = 0.9,β2 = 0.999,ϵ =\n10−8, and a dropout probability of 0.1 were cho-\nsen. Model training and validating with a 80/20\ntrain/test split of the training data, across training\nepochs ∈{1,..., 10}. The selected (epoch) model\nwas that with the highest pronoun prediction accu-\nracy on the validation set."
}