{
  "title": "A Bi-Directional Transformer for Musical Chord Recognition",
  "url": "https://openalex.org/W2954719386",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A4297717558",
      "name": "Park, Jonggwon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4297717559",
      "name": "Choi, Kyoyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745015637",
      "name": "Jeon Sung-Wook",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2518935073",
      "name": "Kim， Dokyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A860543283",
      "name": "Park Jong-Hun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2886597424",
    "https://openalex.org/W2964111193",
    "https://openalex.org/W2398410198",
    "https://openalex.org/W2552400013",
    "https://openalex.org/W2964287748",
    "https://openalex.org/W2036405930",
    "https://openalex.org/W2398597216",
    "https://openalex.org/W2900365452",
    "https://openalex.org/W1989216934",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W1619472568",
    "https://openalex.org/W1982446897",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2395935897",
    "https://openalex.org/W2407685581",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2137519524",
    "https://openalex.org/W2522090280",
    "https://openalex.org/W2408401389",
    "https://openalex.org/W2189280616",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1503560067",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2007321142",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2537159808",
    "https://openalex.org/W2398360960",
    "https://openalex.org/W2919115771"
  ],
  "abstract": "Chord recognition is an important task since chords are highly abstract and descriptive features of music. For effective chord recognition, it is essential to utilize relevant context in audio sequence. While various machine learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been employed for the task, most of them have limitations in capturing long-term dependency or require training of an additional model. In this work, we utilize a self-attention mechanism for chord recognition to focus on certain regions of chords. Training of the proposed bi-directional Transformer for chord recognition (BTC) consists of a single phase while showing competitive performance. Through an attention map analysis, we have visualized how attention was performed. It turns out that the model was able to divide segments of chords by utilizing adaptive receptive field of the attention mechanism. Furthermore, it was observed that the model was able to effectively capture long-term dependencies, making use of essential information regardless of distance.",
  "full_text": "A BI-DIRECTIONAL TRANSFORMER FOR MUSICAL CHORD\nRECOGNITION\nJonggwon Park Kyoyun Choi Sungwook Jeon\nDokyun Kim Jonghun Park\nDepartment of Industrial Engineering & Center for Superintelligence,\nSeoul National University, Seoul, Republic of Korea\n{jayg996, andyhome1907, wookee3, kdk01, jonghun}@snu.ac.kr\nABSTRACT\nChord recognition is an important task since chords are\nhighly abstract and descriptive features of music. For ef-\nfective chord recognition, it is essential to utilize relevant\ncontext in audio sequence. While various machine learning\nmodels such as convolutional neural networks (CNNs) and\nrecurrent neural networks (RNNs) have been employed\nfor the task, most of them have limitations in capturing\nlong-term dependency or require training of an additional\nmodel.\nIn this work, we utilize a self-attention mechanism for\nchord recognition to focus on certain regions of chords.\nTraining of the proposed bi-directional Transformer for\nchord recognition (BTC) consists of a single phase while\nshowing competitive performance. Through an attention\nmap analysis, we have visualized how attention was per-\nformed. It turns out that the model was able to divide seg-\nments of chords by utilizing adaptive receptive ﬁeld of the\nattention mechanism. Furthermore, it was observed that the\nmodel was able to effectively capture long-term dependen-\ncies, making use of essential information regardless of dis-\ntance.\n1. INTRODUCTION\nThe goal of chord recognition task is to output a se-\nquence of time-synchronized chord labels when a raw\naudio recording of music is given as input. Chords are\nhighly abstract and descriptive features of music that can\nbe used for a variety of musical purposes, including auto-\nmatic lead-sheet creation for musicians, cover song iden-\ntiﬁcation, key classiﬁcation and music structure analysis\n[4, 24, 26]. Since manual chord annotation is labor inten-\nsive, time consuming and requires expert knowledge, auto-\nmatic chord recognition system has been an active research\narea within the music information retrieval community.\nc⃝ Jonggwon Park, Kyoyun Choi, Sungwook Jeon, Dokyun\nKim and Jonghun Park. Licensed under a Creative Commons Attribution\n4.0 International License (CC BY 4.0). Attribution: Jonggwon Park,\nKyoyun Choi, Sungwook Jeon, Dokyun Kim and Jonghun Park. “A BI-\nDIRECTIONAL TRANSFORMER FOR MUSICAL CHORD RECOG-\nNITION ”, 20th International Society for Music Information Retrieval\nConference, Delft, The Netherlands, 2019.\nAutomatic chord recognition is challenging due to the\nfact that 1) not all the notes played are necessarily related\nto the chord of the moment and 2) simple one-hot encoding\nof chord labels cannot represent the inherent relationship\nbetween different chords. Most traditional automatic chord\nrecognition systems consist of three parts: feature extrac-\ntion, pattern matching and chord sequence decoding. The\nmost common strategy was to rely on hidden Markov mod-\nels (HMMs) [3] for sequence decoding. Recently, many\nstudies have explored various deep neural networks such as\nconvolutional neural networks (CNNs) or recurrent neural\nnetworks (RNNs) [23] for chord recognition.\nRecently, a novel attention-based network architecture\nnamed Transformer was proposed in [33]. It performs\nwell without any recurrence or convolution and the use\nof Transformer has become popular in various domains.\nFor example, a bi-directional Transformer model called\nBERT achieved state-of-the-art results on eleven natural\nlanguage processing (NLP) tasks [10]. In the domain of\nmusic, [13] applied Transformer to a music generation task\nand succeeded in creating music with complex and repeti-\ntive structure.\nIn this paper we propose BTC (Bi-directional Trans-\nformer for Chord recognition). In contrast to the other\nchord recognition models that depend on training of sep-\narate feature extractors or adopting additional decoders\nsuch as HMMs or Conditional Random Fields (CRFs) [22],\nBTC requires only a single training phase while being able\nto obtain results comparable to them. We also visualize\nhow the model works through attention maps. The atten-\ntion maps demonstrate that BTC is able to 1) divide seg-\nments of chords by utilizing its adaptive receptive ﬁeld and\n2) capture long-term dependencies.\n2. RELATED WORK\n2.1 Automatic Chord Recognition\nIn the past, most automatic chord recognition systems were\ndivided into three parts: feature extraction, pattern match-\ning and chord sequence decoding. After applying transfor-\nmation such as short-time Fourier transform or constant-q\ntransform (CQT) to an input audio signal, features are ex-\ntracted from the resulting time-frequency domain. Some\nexamples of such hand-crafted features include chroma\narXiv:1907.02698v1  [cs.SD]  5 Jul 2019\nvectors and the \"Tonnetz\" [11] representation. For pattern\nmatching and chord sequence decoding, Gaussian mixture\nmodels with feature smoothing [6, 7] and HMMs [28, 32]\nhave been the most popular choices, respectively.\nWith the recent wide acceptance of deep learning in\nresearch communities, there have been many studies ap-\nplying it to chord recognition task in various ways. The\nvery ﬁrst deep-learning-based chord recognition system\nwas proposed by [14] where they trained a CNN for major-\nminor chord classiﬁcation. Attempts to apply deep learn-\ning to feature extraction include [16] and [19], where the\nformer employed a CNN to extract Tonnetz features from\naudio data and the latter adopted a deep neural network\n(DNN) to compute chroma features. CNN and HMM were\ncombined for chord recognition in [15] and [35].\nIn addition to CNN, another popular network architec-\nture for chord recognition is RNN. [5] and [31] explored an\nRNN as chord sequence decoding method, relying on deep\nbelief network and a DNN, respectively. Another branch of\nRNN-based chord recognition systems utilize a language\nmodel which predicts only the sequence of chords with-\nout considering their durations. This might be helpful when\nthe number of chord labels is large (e.g. large vocabulary\ntype, explained in Section 4.1). A large-scale study of lan-\nguage models for chord prediction was conducted in [18].\nWithout audio data, the authors trained just a language\nmodel with the chord progression data only and showed\nthat RNNs outperformed N-gram models. In their succeed-\ning work [21], they combined the RNN-based harmonic\nlanguage model with a chord duration model to complete\nthe chord recognition task.\nAnother RNN-based approach is presented in [34]\nwhich trained a CNN feature extractor with large MIDI\n(Musical Instrument Digital Interface) data and combined\nBLSTM (Bi-directional Long-Short Term Memory) with\nCRF for sequence decoder. This BLSTM-CRF model\nachieved good performance but has a drawback that its\ntraining procedure involves complex MIDI pre-training.\nThe model that we propose, on the other hand, is much\nsimpler to train.\n2.2 Attention-based Models\nThe attention mechanism, ﬁrst introduced by [2], can be\ndescribed as computing an output vector when query, key\nand value vectors are given. In sequence modelling tasks\nsuch as machine translation, query and key correspond to\ncertain elements of the target sequence and the source se-\nquence respectively. Each key has its own value. The out-\nput is computed as a weighted sum of the values where\nthe weights are computed from the query and key. Self-\nattention refers to the case when query, key and value are\ncomputed from the same input.\nTransformer is an attention-based network that relies on\nattention mechanism only and does not include recurrent or\nconvolutional architecture. Utilizing multi-head attention\ntogether with position-wise fully-connected feed-forward\nnetwork, it showed signiﬁcantly faster training speed and\nachieved better performance than recurrent or convolu-\ntional networks for translation tasks.\nTransformer used scaled dot-product as an attention\nfunction:\nAttention(Q,K,V ) =softmax(QKT\n√dK\n)V (1)\nwhere Q, K and V are matrices of query, key and value\nvectors respectively, and dK is the dimension of key.\nThe use of Transformer has become very popular,\nachieving the state-of-the-art results in various domains.\nA well-known example is bi-directional encoder represen-\ntations from Transformers (BERT) [10]. BERT is a pre-\ntraining model based on masked language model for lan-\nguage representations that achieved state-of-the-art results\non eleven NLP tasks. In the domain of music, [13] pro-\nposed music Transformer for symbolic music generation.\nMusic Transformer employed relative attention to capture\nlong-term structure effectively, which resulted in music\ncompositions that are both qualitatively and quantitatively\nbetter structured than existing music generation models.\n3. BI-DIRECTIONAL TRANSFORMER FOR\nCHORD RECOGNITION\n3.1 Bi-directional Transformer\nMaking use of appropriate surrounding frames is essen-\ntial for successful chord recognition [7, 8]. This context-\ndependent characteristic of the task is the motivation\nfor applying the self-attention mechanism. With some\nmodiﬁcation to the original Transformer architecture, we\npresent a bi-directional Transformer for chord recognition\n(BTC). 1\nThe structure of BTC is shown in Figure 1. The\nmodel consists of bi-directional multi-head self-attentions,\nposition-wise convolutional blocks, a positional encoding,\nlayer normalization [1], dropout [30] and fully-connected\nlayers. The model takes a CQT feature of 10 second au-\ndio signal (Section 4.1) as input. The results of adding po-\nsitional encoding are given as input to two self-attention\nblocks with different masking directions, indicated as dot-\nted boxes in Figure 1(b). The outputs are concatenated and\nare fed into a fully-connected layer so that the output size is\nthe same as the original input. A stack of N bi-directional\nself-attention layers is followed by another fully-connected\nlayer that outputs logit values. The size of the logit values\nis the same as the number of chord labels. These logits are\nused to predict the chord and calculate the loss.\nThe loss function is a negative log-likelihood and all the\nmodel parameters are trained to minimize the loss given by\nthe following equation (2).\nL= −\nT∑\nt=1\n∑\nc∈V\nyc(t)log( ˆyc(t)) (2)\nT is the number of total time frames and V is the chord\nlabel set. yc(t) is 1 if the reference label at time tis cand\n0 otherwise. ˆyc(t) is the output of the model, representing\nthe probability of the chord at time tbeing c.\n1 https://github.com/jayg996/BTC-ISMIR19\nRaw Audio Data\nCQT\nBi-directional\nSelf-attention\nLayer\n(b)\nFully-connected\nLayer\nSoftmax\nChord Recognition \nProbabilities\nAddPositional\nEncoding\n× N\nFully-connected\nLayer\nInput\nLayer Normalization\nMasked\nMulti-head\nAttention\n(Backward)\nDropout\nLayer Normalization\nPosition-wise\nConvolutional Block\nAdd\nAdd\nLayer Normalization\nMasked\nMulti-head\nAttention\n(Forward)\nDropout\nLayer Normalization\nPosition-wise\nConvolutional Block\nAdd\nAdd\nConcat\nFully-connected Layer\nLayer Normalization\nOutput\n(a) (b)\nQ K V Q K V\nDropout\nFigure 1. Structure of BTC. (a) shows the overall network architecture and (b) describes the bi-directional self-attention\nlayer in detail. Dotted boxes indicate self-attention blocks.\n3.1.1 Bi-directional Multi-head Self-attention\nBTC employs multi-head self-attention as in the original\nTransformer. For each time frame, the input features are\nsplit into nh pieces and provided as input to the multi-head\nself-attention with the number of heads, nh. Given I as\nan input matrix, the multi-head self-attention can be com-\nputed as (3):\nMultihead = Concat(head1,...,head nh )WO (3)\nQj = (IWQ)j,Kj = (IWK)j and Vj = (IWV )j are\ngiven as input to the attention function (1) to produce\nheadj for j = 1,...,n h. WQ,WK and WV are fully-\nconnected layers that project the input to the dimension\nof Q,K and V, respectively. WO is also a fully-connected\nlayer that projects the concatenated output of dimension\n(nh ×dVj ) to the dimension of the ﬁnal output. Dropout\nis applied to the softmax output weights when computing\neach headj.\nIn BTC, self-attention can be interpreted as determin-\ning how much attention to apply to the value of the key\ntime frame when inferring the chord of the query time\nframe. To prevent the loss of information due to the at-\ntention being performed to the entire input at once, we\nemployed bi-directional masking. The forward / backward\ndirection refers to masking all the preceding / succeed-\ning time frames. The same masked multi-head attention\nmodule as the Transformer decoder was adopted. The bi-\ndirectional structure enables BTC to fully utilize the con-\ntext before and after the target time frame.\nSince the multi-head attention is performed on every\ntime frame in the sequence, information about the order of\nthe sequence is lost. We employed the same solution pro-\nposed by Transformer to address this issue: adding posi-\ntional encoding results to the input, which are obtained by\napplying sinusoidal functions to each position. Since rel-\native positions between two frames can be expressed as a\nlinear function of the encodings, positional encoding helps\nthe model learn to apply attention via relative positions.\n3.1.2 Position-wise Convolutional Block\nTo utilize the adjacent feature information in a time\nframe, we replaced the position-wise fully-connected feed-\nforward network from the original Transformer archi-\ntecture with a position-wise convolutional block. The\nposition-wise convolutional block consists of a 1D con-\nvolution layer, a ReLU (Rectiﬁed Linear Unit) activation\nfunction and a dropout layer, where the whole sequence of\nlayers is repeated nC times. Input and output channel size\nwere identical to keep the feature size and sequence length\nconstant. With the position-wise convolutional block, we\nanticipate to search the boundary and smooth the chord\nsequence by exploring adjacent information at each time\nframe.\n3.2 Self-attention in Chord Recognition\nFor chord recognition, it is important to utilize not only the\ninformation from the target time frame but also from other\nrelated frames, which we call the context. The network ar-\nchitectures such as CNNs or RNNs can also explore the\ncontext, but self-attention is more suitable for the task be-\ncause of the following reasons.\nFirst, self-attention has selective usage of attention. In\nother words, the receptive ﬁeld can be adaptive unlike\nCNNs where the kernel size is ﬁxed. For example, assume\nthat the labels for 16 frames are Cs for the ﬁrst four frames,\nGs and Fs for the next eight frames and Cs for the last four\nframes (see Figure 2). Consider the situation of recogniz-\ning Gs in frames 5 to 8. As for a CNN with kernel size of 3,\nwhen recognizing the chord of frame 7, the receptive ﬁeld\n(frame 6 to 8) would be informative enough since all the\nframes contain the same chord. However, when inferring\nframe 5, the receptive ﬁeld of frame 4 to 6 contains not\nonly G but also C. With self-attention, on the other hand,\nthe model can pay attention to the section of frame 5 to 8\nregardless of the target frame’s position.\nAnother advantage of attention mechanism is its abil-\nity to capture long-term dependency effectively. RNNs can\nalso utilize distant information but direct access is not\npossible. For CNNs, there are two ways to access distant\nframes: by stacking layers in depth or by increasing the\nkernel size. The former has the same drawback as RNNs\nand the latter has the disadvantage that the weight shar-\ning becomes less effective. Unlike these, self-attention has\ndirect access to other frames no matter how far they are.\nSpeciﬁcally, when recognizing the chord of frame 13, per-\nforming attention to ﬁrst four frames would be helpful\nsince they all contain C. With RNNs or deep CNNs, infor-\nmation that the ﬁrst four frames were C would inevitably\nbe diluted while passing through frames 5 to 12.\nC C C C G G G G F F F F C C C C\n1 4 5 6 7 8 12 13 16\nFigure 2. Chord sequence example\n4. EXPERIMENTS\n4.1 Data and Preprocessing\nBTC and other baseline models were evaluated on the\nfollowing datasets. A subset of 221 songs from Isophon-\nics 2 : 171 songs by the Beatles, 12 songs by Carole King,\n20 songs by Queen and 18 songs by Zweieck; Robbie\n2 http://isophonics.net/datasets\nWilliams [12]: 65 songs by Robbie Williams; and a sub-\nset of 185 songs from UsPop2002 3 . These datasets con-\nsist of label ﬁles that specify the start time, end time and\ntype of the chord. Due to copyright issue, these datasets do\nnot include audio ﬁles. The audio ﬁles used in this work\nwere collected from online music service providers (e.g.\nMelon 4 ), which do not always provide the same audio\nﬁles corresponding to the songs in the datasets. Since it\nwas not possible to get exactly the same audio ﬁles, there\nwere subtle differences in the chord start time of the la-\nbel ﬁle and audio ﬁle. Accordingly we manually matched\nthe labels to the audio ﬁle by shifting the whole label ﬁle\nback and forth, which resulted in no more than adding or\ndeleting some “No chord” labels.\nEach 10-second-long audio signal (consecutive signals\noverlapping 5 seconds) was processed at the sampling rate\nof 22,050Hz using CQT with 6 octaves starting from C1,\n24 bins per octave, and the hop size of 2048 [34]. The CQT\nfeatures were transformed to log amplitude with Slog =\nln(S+ ϵ) where S represents the CQT feature and ϵis an\nextremely small number. After that, global z-normalization\nwas applied with mean, variance from the training data.\nPitch augmentation was also employed to the audio\nﬁle with pyrubberband 5 package and labels were changed\nwith pitch variation. Pitch augmentation between -5 ∼+6\nsemitones were applied to all the training data.\nTwo different label types were used: maj-min and large\nvocabulary. The maj-min label type consists of 25 chords\n(12 semitones ×{maj, min} and “No chord”) [20]. The\nlarge vocabulary label type consists of 170 chords (12\nsemitones ×{maj, min, dim, aug, min6, maj6, min7, min-\nmaj7, maj7, 7, dim7, hdim7, sus2, sus4} and “X chord : the\nunknown chord”, “No chord”) [25]. From the label ﬁles,\nwe extracted the chord that matches the time frame of in-\nput feature and transformed it to the appropriate label type.\n4.2 Evaluation Metric\nThe evaluation metric was weighted chord symbol recall\n(WCSR) score and 5-fold cross validation was applied to\nthe entire data. When separating the evaluation data from\nthe training data, there was no song included in both. The\nWCSR score can be computed as (4), where tc is the du-\nration of correctly classiﬁed chord segments and ta is the\nduration of the entire chord segments.\nWCSR = tc\nta\n×100(%) (4)\nScores were computed with mir_eval [27]. Root and\nMaj-min scores were used for the maj-min label type.\nRoot, Thirds, Triads, Sevenths, Tetrads, Maj-min and\nMIREX scores were used for the large vocabulary label\ntype. To calculate the score with mir_eval, the chord recog-\nnition results were converted into label ﬁles.\n3 https://github.com/tmc323/Chord-Annotations\n4 http://www.melon.com\n5 https://github.com/bmcfee/pyrubberband\nModel maj-min label type large vocabulary label type\nRoot Maj-min Root Thirds Triads Sevenths Tetrads Maj-min MIREX\nCNN 83.6±1.3 81.8±1.2 83.5±1.4 80.4±1.2 75.5±0.6 71.5±1.9 65.2±1.0 81.9±1.4 79.8±0.7\nCNN+CRF [20] 84.0±1.3 83.1±1.4 83.7±1.5 81.1±1.4 76.3±0.8 71.3±1.9 65.7±1.6 82.1±1.5 81.8±1.1\nCRNN [25] 83.4±0.8 82.3±0.9 82.9±1.1 80.1±1.0 75.3±0.7 71.3±1.9 65.2±0.9 81.5±1.3 79.9±0.8\nCRNN+CRF 83.3±0.8 82.3±1.0 82.7±1.2 79.7±0.9 74.8±0.5 69.5±2.0 63.9±1.0 80.7±1.4 80.2±1.0\nBTC 83.8±1.0 82.7±1.0 83.5±1.2 80.8±1.0 75.9±0.5 71.8±1.7 65.5±0.9 82.3±1.2 80.8±0.9\nBTC+CRF 83.9±1.0 83.1±1.1 83.5±1.2 80.7±1.1 75.7±0.5 70.7±2.0 64.8±1.1 81.7±1.4 81.4±0.9\nTable 1. WCSR scores averaged over the same 5 folds. Numbers next to the scores denote the standard deviations.\n4.3 Results\nSpeciﬁc hyperparameters of BTC are summarized in\nTable 2. The hyperparameters with the best validation per-\nformance were obtained empirically after applying in 5-\nfold cross validation. Adam optimizer [17] was used with\ninitial learning rate of 10−4. Learning rate was decayed\nwith rate 0.95 when validation accuracy did not increase.\nTraining was stopped if the validation accuracy did not im-\nprove for over 10 epochs.\nSince existing studies of chord recognition were evalu-\nated on different datasets, it is difﬁcult to say that a par-\nticular model is the state-of-the-art. Among the models\nthat were trainable with our datasets, we chose three base-\nline models with good performance: CNN, CNN+CRF and\nCRNN. CNN is a VGG [29]-style CNN and CNN+CRF\nhas an additional CRF decoder [20]. CRNN is a com-\nbination of CNN and gated recurrent unit [9], named\n\"CR2\" in [25]. The input was preprocessed as mentioned\nin Section 4.1 for BTC and CRNN. For CNN+CRF and\nCNN, a single label was estimated with a patch of 15 time\nframes, in a similar way to [20].\nTable 1 shows the performance comparison results of\nthe baseline models and BTC for two label types. The best\nvalue for each metric is represented in bold. Among the\nmodels without a CRF decoder, BTC showed the best per-\nformance for all metrics. Including models with a CRF\ndecoder, CNN+CRF obtained the best result in most of\nthe metrics. Still, BTC shows comparable performance to\nCNN+CRF, performing better in Sevenths and Maj-min\nmetrics for the large vocabulary label type.\nThe main purpose of training a CRF decoder is to\nsmooth the predicted chord sequences that are often frag-\nBi-directional\nself-attention\nlayer\nlayer repetition (N) {1, 2, 4, 8, 12}\nself-attention heads (nh) {1, 2, 4}\ndimension of Q, K, V\nand all the hidden layers {64, 128, 256}\nPosition-wise\nconvolutional\nblock\nblock repetition (nC) 2\nkernel size 3\nstride 1\npadding size 1\nDropout dropout probability {0.2, 0.3, 0.5}\nTable 2. Hyperparameters of BTC. Hyperparameters with\nthe best validation performance are shown in bold.\nmented. The performances of CRNN+CRF and BTC+CRF\nare also presented in Table 1 for comparison. Performance\nimprovements due to the introduction of CRFs are evident\nin CNN but not in BTC and CRNN. This indicates that out-\nputs of CNN were fragmented and an additional decoder\ntraining is necessary for better performance. On the other\nhand, BTC and CRNN can be trained with only CQT fea-\ntures and chord labels. That is, BTC requires only a single\ntraining phase while achieving the performance compara-\nble to that of CNN+CRF.\n4.4 Attention Map Analysis\nAttention maps demonstrate that each self-attention layer\nhas different characteristics. Figure 3 shows the attention\nmap of self-attention layers 1, 3, 5 and 8, trained with the\nmaj-min label type. The lower / upper triangle of each at-\ntention map represents the attention probability of the for-\nward / backward direction self-attention layer. The labels\nof the vertical axis and the horizontal axis are the reference\nchord and the chord recognition result of the target time\nframe, respectively. The cell of i-th row and j-th column\nrepresents the attention probability to the j-th time frame\nwhen inferring the chord of the i-th time frame.\nAt the ﬁrst self-attention layer, only neighboring frames\nare used to construct the representation of the target frame.\nFor the third layer, the attention is widely spread over all\ntime frames, yet still with higher probabilities for nearby\nframes than distant frames. At the ﬁfth layer, several ad-\njacent time frames form a group, which appears in a rect-\nangular region in the attention map. This means that the\nmodel divides the whole input into some sections, which\nis possible due to the adaptive receptive ﬁeld. The network\nfocuses only on a few important sections to identify the tar-\nget frame, regardless of the distance between section and\nthe frame. Unlike the ﬁfth layer, attention is more dense in\ncertain regions at the eighth layer. In particular, the bound-\nary of the high probability region matches that of the ﬁnal\nrecognition result.\nSpeciﬁcally, at the ﬁfth layer in Figure 3(c), the refer-\nence chord for region ② is B:min. Region ① shares the\nsame reference chord B:min and the network assigns high\nattention probabilities to region ① for time frames in re-\ngion ②. This phenomenon is similar in layer 8 between\n①′ and ②′(Figure 3(d)), which results in the correct ﬁ-\nnal chord recognition of B:min. In contrast, for region ③\nwhere the reference chord is G, the attention probability is\nG B:minA A G F#:min B:min A G A\nG\nB:min\nA\nA\nG\nB:min\nA\nG\nA\nA\nRecognition\nRerference\nG B:minA A G F#:min B:min A G A\nG\nB:min\nA\nA\nG\nB:min\nA\nG\nA\nA\nRecognition\nRerference\n0.05\n0.04\n0.03\n0.02\n0.01\n0.00\nG B:minA A G F#:min B:min A G A\nG\nB:min\nA\nA\nG\nB:min\nA\nG\nA\nA\nRecognition\nRerference\n① ②\n③\n④⑤ ⑥ ⑦ ⑧\nG B:minA A G F#:min B:min A G A\nG\nB:min\nA\nA\nG\nB:min\nA\nG\nA\nA\nRecognition\nRerference\n①′ ②′\n③′\n⑥′\n(a) Self-attention layer 1 (b) Self-attention layer 3\n(c) Self-attention layer 5 (d) Self-attention layer 8\nFigure 3. The ﬁgures represent the probability values of the attention of self-attention layers 1, 3, 5 and 8 respectively.\nThe layers that best represent the different characteristics were chosen. The input audio is the song \"Just A Girl\" (0m30s∼\n0m40s) by No Doubt from UsPop2002, which was in evaluation data.\nhigh at layer 5 but not for region③′at layer 8. This can be\nattributed to G and B:min sharing two notes in common,\nsince G and B:min consist of (G,B,D) and (B,D,F#) re-\nspectively. In other words, attention at layer 5 can be seen\nas attention to partial features of chords sharing the same\nnotes. None the less, the ﬁnal recognition result after the\nlast layer is not G but B:min. This is possible because of\nthe multi-head attention structure: the other heads might\nlower the attention probability even if the attention to a\nwrong chord is active, leading to the correct result.\nOn the other hand, there are cases where the recogni-\ntion results are wrong in a similar situation. The reference\nchord for regions ⑥ and ⑥′ is A. At layer 5, the atten-\ntion mechanism seems to work well with high attention\nprobabilities to region ④,⑤,⑦ and ⑧, where the refer-\nence chords are all As. However, the attention to those re-\ngions cannot be seen at the last layer, and the ﬁnal recogni-\ntion result is not A but F#:min. This recognition failure can\nbe regarded as a result of two notes of F#:min (F#,A,C#)\noverlapping with A (A,C#,E).\nTo summarize, for each target frame in the input audio,\nthe model uses only neighboring frames at ﬁrst. At the mid-\ndle layers, the model gradually broadens the receptive ﬁeld\nand selectively focuses on time frames with characteristics\nsimilar to that of the target frame. Finally, at the last layer,\nthe attention is performed on only essential information for\nchord recognition.\n5. CONCLUSION\nIn this paper, we presented bi-directional Transformer for\nchord recognition (BTC). To the best of our knowledge,\nthis paper was the ﬁrst attempt to apply Transformer to\nchord recognition. The self-attention mechanism was ap-\npropriate for the task that attempts to capture long-term de-\npendency by effectively exploring relevant sections. BTC\nhas an advantage in that its training procedure is simple\nand it showed results competitive to other models in most\nof the evaluation metrics. Through the attention map analy-\nsis, it turned out that each self-attention layer had different\ncharacteristics and that the attention mechanism was effec-\ntive in identifying sections of chords that were crucial for\nchord recognition.\n6. ACKNOWLEDGEMENTS\nThis work was supported by Kakao and Kakao Brain cor-\nporations.\n7. REFERENCES\n[1] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normaliza-\ntion. arXiv preprint, arXiv:1607.06450, 2016.\n[2] D. Bahdanau, K. Cho, and Y . Bengio. Neural machine\ntranslation by jointly learning to align and translate. In\n3rd International Conference on Learning Representa-\ntions (ICLR), Conference Track Proc., San Diego, CA,\nUSA, 2015.\n[3] L. E. Baum and T. Petrie. Statistical inference for prob-\nabilistic functions of ﬁnite state markov chains. The\nannals of mathematical statistics, 37(6):1554–1563,\n1966.\n[4] J. P. Bello. Chord segmentation and recognition using\nem-trained hidden markov models. In Proc. of the 8th\nInternational Society for Music Information Retrieval\nConference (ISMIR), pages 239–244, Vienna, Austria,\n2007.\n[5] N. Boulanger-Lewandowski, Y . Bengio, and P. Vin-\ncent. Audio chord recognition with recurrent neural\nnetworks. In Proc. of the 14th International Society\nfor Music Information Retrieval Conference (ISMIR),\npages 335–340, Curitiba, Brazil, 2013.\n[6] T. Cho. Improved Techniques for Automatic Chord\nRecognition from Music Audio Signals. PhD thesis,\nNew York University, 2014.\n[7] T. Cho and J. P. Bello. A feature smoothing method\nfor chord recognition using recurrence plots. In Proc.\nof the 12th International Society for Music Information\nRetrieval Conference (ISMIR), pages 651–656, Miami,\nFlorida, USA, 2011.\n[8] T. Cho and J. P. Bello. On the relative importance of\nindividual components of chord recognition systems.\nIEEE/ACM Trans. Audio, Speech & Language Process-\ning, 27(2):477–492, 2014.\n[9] J. Chung, Ç. Gülçehre, K. Cho, and Y . Bengio. Empir-\nical evaluation of gated recurrent neural networks on\nsequence modeling. arXiv preprint, arXiv:1412.3555,\n2014.\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.\nBERT: pre-training of deep bidirectional transform-\ners for language understanding. arXiv preprint ,\narXiv:1810.04805, 2018.\n[11] L. Euler. Tentamen novae theoriae musicae ex certis-\nsimis harmoniae principiis dilucide expositae. ex ty-\npographia Academiae scientiarum, 1739.\n[12] B. Di Giorgi, M. Zanoni, A. Sarti, and S. Tubaro. Au-\ntomatic chord recognition based on the probabilistic\nmodeling of diatonic modal harmony. In Proc. of the\n8th International Workshop on Multidimensional Sys-\ntems, Erlangen, Germany, 2013.\n[13] C.-Z. Anna Huang, A. Vaswani, J. Uszkoreit,\nN. Shazeer, C. Hawthorne, A. M. Dai, M. D. Hoffman,\nand D. Eck. Music transformer: Generating music with\nlong-term structure. arXiv preprint, arXiv:1809.04281,\n2018.\n[14] E. J. Humphrey and J. P. Bello. Rethinking automatic\nchord recognition with convolutional neural networks.\nIn 11th International Conference on Machine Learn-\ning and Applications(ICMLA), pages 357–362, Boca\nRaton, FL, USA, 2012.\n[15] E. J. Humphrey and J. P. Bello. Four timely insights on\nautomatic chord estimation. In Proc. of the 16th Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR), pages 673–679, Málaga, Spain, 2015.\n[16] E. J. Humphrey, T. Cho, and J. P. Bello. Learning\na robust tonnetz-space transform for automatic chord\nrecognition. In Proc. of the IEEE International Con-\nference on Acoustics, Speech, and Signal Process-\ning(ICASSP), pages 453–456, Kyoto, Japan, 2012.\n[17] D. P. Kingma and J. Ba. Adam: A method for stochas-\ntic optimization. In 3rd International Conference on\nLearning Representations (ICLR), Conference Track\nProc., San Diego, CA, USA, 2015.\n[18] F. Korzeniowski, D. R. W. Sears, and G. Widmer. A\nlarge-scale study of language models for chord pre-\ndiction. In Proc. of the IEEE International Conference\non Acoustics, Speech, and Signal Processing(ICASSP),\npages 91–95, Calgary, AB, Canada, 2018.\n[19] F. Korzeniowski and G. Widmer. Feature learning for\nchord recognition: The deep chroma extractor. InProc.\nof the 17th International Society for Music Information\nRetrieval Conference (ISMIR), pages 37–43, New York\nCity, USA, 2016.\n[20] F. Korzeniowski and G. Widmer. A fully convolutional\ndeep auditory model for musical chord recognition. In\n26th IEEE International Workshop on Machine Learn-\ning for Signal Processing, (MLSP), pages 1–6, Vietri\nsul Mare, Salerno, Italy, 2016.\n[21] F. Korzeniowski and G. Widmer. Improved chord\nrecognition by combining duration and harmonic lan-\nguage models. In Proc. of the 19th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR), pages 10–17, Paris, France, 2018.\n[22] J. D. Lafferty, A. McCallum, and F. C. N. Pereira.\nConditional random ﬁelds: Probabilistic models for\nsegmenting and labeling sequence data. In Proc. of\nthe 18th International Conference on Machine Learn-\ning (ICML 2001), Williams College, pages 282–289,\nWilliamstown, MA, USA, 2001.\n[23] Y . LeCun, Y . Bengio, and G. E. Hinton. Deep learning.\nNature, 521(7553):436–444, 2015.\n[24] K. Lee. Identifying cover songs from audio using\nharmonic representation. MIREX 2006, pages 36–38,\n2006.\n[25] B. McFee and J. P. Bello. Structured training for large-\nvocabulary chord recognition. In Proc. of the 18th In-\nternational Society for Music Information Retrieval\nConference (ISMIR), pages 188–194, Suzhou, China,\n2017.\n[26] J. Pauwels, F. Kaiser, and G. Peeters. Combin-\ning harmony-based and novelty-based approaches for\nstructural segmentation. In Proc. of the 14th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR), pages 601–606, Curitiba, Brazil, 2013.\n[27] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, and D. P. W. Ellis. Mir_eval: A\ntransparent implementation of common mir metrics. In\nProc. of the 15th International Society for Music Infor-\nmation Retrieval Conference (ISMIR), pages 367–372,\nTaipei, Taiwan, 2014.\n[28] A. Sheh and D. P. W. Ellis. Chord segmentation and\nrecognition using em-trained hidden markov models.\nIn Proc. of the 4th International Society for Music\nInformation Retrieval Conference (ISMIR), Baltimore,\nMaryland, USA, 2003.\n[29] K. Simonyan and A. Zisserman. Very deep convolu-\ntional networks for large-scale image recognition. In\n3rd International Conference on Learning Representa-\ntions (ICLR), Conference Track Proc., San Diego, CA,\nUSA, 2015.\n[30] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,\nand R. Salakhutdinov. Dropout: A simple way to pre-\nvent neural networks from overﬁtting. Journal of Ma-\nchine Learning Research, 15:1929–1958, 2014.\n[31] S.Sigtia, N. Boulanger-Lewandowski, and S.Dixon.\nAudio chord recognition with a hybrid recurrent neu-\nral network. In Proc. of the 16th International Society\nfor Music Information Retrieval Conference (ISMIR),\npages 127–133, Málaga, Spain, 2015.\n[32] Y . Ueda, Y . Uchiyama, T. Nishimoto, N. Ono, and\nS. Sagayama. Hmm-based approach for automatic\nchord detection using reﬁned acoustic features. In\nProc. of the IEEE International Conference on Acous-\ntics, Speech, and Signal Processing(ICASSP), pages\n5518–5521, Dallas, Texas, USA, 2010.\n[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need. In Advances in Neural In-\nformation Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017, pages\n6000–6010, Long Beach, CA, USA, 2017.\n[34] Y . Wu and W. Li. Automatic audio chord recogni-\ntion with midi-trained deep feature and BLSTM-CRF\nsequence decoding model. IEEE/ACM Trans. Audio,\nSpeech & Language Processing, 27(2):355–366, 2019.\n[35] X. Zhou and A. Lerch. Chord detection using deep\nlearning. In Proc. of the 16th International Society\nfor Music Information Retrieval Conference (ISMIR),\npages 52–58, Málaga, Spain, 2015.",
  "topic": "Chord (peer-to-peer)",
  "concepts": [
    {
      "name": "Chord (peer-to-peer)",
      "score": 0.8487830758094788
    },
    {
      "name": "Computer science",
      "score": 0.7426530718803406
    },
    {
      "name": "Transformer",
      "score": 0.5653493404388428
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5499880909919739
    },
    {
      "name": "Speech recognition",
      "score": 0.5212184190750122
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4747070372104645
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41332727670669556
    },
    {
      "name": "Artificial neural network",
      "score": 0.41167595982551575
    },
    {
      "name": "Engineering",
      "score": 0.094230055809021
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Distributed computing",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    }
  ],
  "cited_by": 7
}