{
  "title": "Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders",
  "url": "https://openalex.org/W3213730158",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2031764755",
      "name": "Fangyu. Liu",
      "affiliations": [
        "Language Science (South Korea)",
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A1969142033",
      "name": "Ivan Vulić",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1927037681",
      "name": "Anna Korhonen",
      "affiliations": [
        "University of Cambridge",
        "Language Science (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2098750953",
      "name": "Nigel Collier",
      "affiliations": [
        "University of Cambridge",
        "Language Science (South Korea)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2346452181",
    "https://openalex.org/W3144596436",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W3155682407",
    "https://openalex.org/W2954699761",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W3152788712",
    "https://openalex.org/W2994931756",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2509884321",
    "https://openalex.org/W3173188814",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2126400076",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3176047188",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W3175362188",
    "https://openalex.org/W2462305634",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2518186251",
    "https://openalex.org/W3115295967",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2251861449",
    "https://openalex.org/W2948110372",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2250214110",
    "https://openalex.org/W2964266061",
    "https://openalex.org/W2994088087",
    "https://openalex.org/W2963118869",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3164540570",
    "https://openalex.org/W4299579390",
    "https://openalex.org/W3098985395",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W3131648208",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2995289474",
    "https://openalex.org/W3154229486",
    "https://openalex.org/W2963644595",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3131870090",
    "https://openalex.org/W3100652389",
    "https://openalex.org/W1521968289",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W3094540663",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2152180407",
    "https://openalex.org/W2963804993",
    "https://openalex.org/W4299574851",
    "https://openalex.org/W3122838366",
    "https://openalex.org/W2786464815",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4294367149",
    "https://openalex.org/W3175049034",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3105801792",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2952267213",
    "https://openalex.org/W2133458109",
    "https://openalex.org/W4294875919",
    "https://openalex.org/W2964073004",
    "https://openalex.org/W3172806051",
    "https://openalex.org/W3125516434",
    "https://openalex.org/W2962772361",
    "https://openalex.org/W2963165489",
    "https://openalex.org/W3118062200",
    "https://openalex.org/W1486649854",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3173783447",
    "https://openalex.org/W2251882135",
    "https://openalex.org/W3175394187",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W2250790822",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W1854884267"
  ],
  "abstract": "Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision. We propose an extremely simple, fast, and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds with no access to additional external knowledge. Mirror-BERT relies on identical and slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during \"identity fine-tuning\". We report huge gains over off-the-shelf MLMs with Mirror-BERT both in lexical-level and in sentence-level tasks, across different domains and different languages. Notably, in sentence similarity (STS) and question-answer entailment (QNLI) tasks, our self-supervised Mirror-BERT model even matches the performance of the Sentence-BERT models from prior work which rely on annotated task data. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple Mirror-BERT fine-tuning approach can yield effective universal lexical and sentence encoders.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1442–1459\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n1442\nFast, Effective, and Self-Supervised: Transforming Masked Language\nModels into Universal Lexical and Sentence Encoders\nFangyu Liu, Ivan Vuli´c, Anna Korhonen, Nigel Collier\nLanguage Technology Lab, TAL, University of Cambridge\n{fl399, iv250, alk23, nhc30}cam.ac.uk\nAbstract\nPrevious work has indicated that pretrained\nMasked Language Models (MLMs) are not ef-\nfective as universal lexical and sentence en-\ncoders off-the-shelf, i.e., without further task-\nspeciﬁc ﬁne-tuning on NLI, sentence similar-\nity, or paraphrasing tasks using annotated task\ndata. In this work, we demonstrate that it is\npossible to turn MLMs into effective lexical\nand sentence encoders even without any addi-\ntional data, relying simply on self-supervision.\nWe propose an extremely simple, fast, and ef-\nfective contrastive learning technique, termed\nMirror-BERT, which converts MLMs (e.g.,\nBERT and RoBERTa) into such encoders in\n20–30 seconds with no access to additional\nexternal knowledge. Mirror-BERT relies on\nidentical and slightly modiﬁed string pairs as\npositive (i.e., synonymous) ﬁne-tuning exam-\nples, and aims to maximise their similarity dur-\ning “identity ﬁne-tuning” . We report huge\ngains over off-the-shelf MLMs with Mirror-\nBERT both in lexical-level and in sentence-\nlevel tasks, across different domains and differ-\nent languages. Notably, in sentence similarity\n(STS) and question-answer entailment (QNLI)\ntasks, our self-supervised Mirror-BERT model\neven matches the performance of the Sentence-\nBERT models from prior work which rely on\nannotated task data. Finally, we delve deeper\ninto the inner workings of MLMs, and sug-\ngest some evidence on why this simple Mirror-\nBERT ﬁne-tuning approach can yield effective\nuniversal lexical and sentence encoders.\n1 Introduction\nTransfer learning with pretrained Masked Lan-\nguage Models (MLMs) such as BERT (Devlin et al.,\n2019) and RoBERTa (Liu et al., 2019) has been\nwidely successful in NLP, offering unmatched per-\nformance in a large number of tasks (Wang et al.,\n2019a). Despite the wealth of semantic knowledge\nstored in the MLMs (Rogers et al., 2020), they do\nnot produce high-quality lexical and sentence em-\nbeddings when used off-the-shelf, without further\ndist( , ) < dist( / , / /… )f(x1) f(¯x1) f(x1) f(¯x1) f(x2) f(x3)\nBERT/\nRoBERTa\nBERT/\nRoBERTaweight \nsharing\n( random span masking )\nmultiple \ndropout layers\nx1 ¯x1\nf(x1)\nf(¯x1)\nf(x2)\nf(x3)\nf(x4)\nf(x5)\nFigure 1: Illustration of the main concepts behind the\nproposed self-supervised Mirror-BERT method. The\nsame text sequence can be observed from two addi-\ntional “views”: 1) by performing random span mask-\ning in the input space, and/or 2) by applying dropout\n(inside the BERT/RoBERTa MLM) in the feature space,\nyielding identity-based (i.e., “mirrored”) positive exam-\nples for ﬁne-tuning. A contrastive learning objective\nis then applied to encourage such “mirrored” positive\npairs to obtain more similar representations in the em-\nbedding space relatively to negative pairs.\ntask-speciﬁc ﬁne-tuning (Feng et al., 2020; Li et al.,\n2020). In fact, previous work has shown that their\nperformance is sometimes even below static word\nembeddings and specialised sentence encoders (Cer\net al., 2018) in lexical and sentence-level seman-\ntic similarity tasks (Reimers and Gurevych, 2019;\nVuli´c et al., 2020b; Litschko et al., 2021).\nIn order to address this gap, recent work has\ntrained dual-encoder networks on labelled exter-\nnal resources to convert MLMs into universal lan-\nguage encoders. Most notably, Sentence-BERT\n(SBERT, Reimers and Gurevych 2019) further\ntrains BERT and RoBERTa on Natural Language\nInference (NLI, Bowman et al. 2015; Williams et al.\n2018) and sentence similarity data (Cer et al., 2017)\nto obtain high-quality universal sentence embed-\ndings. Recently, SapBERT (Liu et al., 2021) self-\n1443\naligns phrasal representations of the same meaning\nusing synonyms extracted from the UMLS (Boden-\nreider, 2004), a large biomedical knowledge base,\nobtaining lexical embeddings in the biomedical\ndomain that reach state-of-the-art (SotA) perfor-\nmance in biomedical entity linking tasks. However,\nboth SBERT and SapBERT require annotated (i.e.,\nhuman-labelled) data as external knowledge: it is\nused to instruct the model to produce similar repre-\nsentations for text sequences (e.g., words, phrases,\nsentences) of similar/identical meanings.\nIn this paper, we fully dispose of any external\nsupervision, demonstrating that the transformation\nof MLMs into universal language encoders can\nbe achieved without task-labelled data. We pro-\npose a ﬁne-tuning framework termedMirror-BERT,\nwhich simply relies on duplicating and slightly aug-\nmenting the existing text input (or their representa-\ntions) to achieve the transformation, and show that\nit is possible to learn universal lexical and sentence\nencoders with such “mirrored” input data through\nself-supervision (see Fig. 1). The proposed Mirror-\nBERT framework is also extremely efﬁcient: the\nwhole MLM transformation can be completed in\nless than one minute on two 2080Ti GPUs.\nOur ﬁndings further conﬁrm a general hypothe-\nsis from prior work (Liu et al., 2021; Ben-Zaken\net al., 2020; Glavaš and Vuli ´c, 2021) that ﬁne-\ntuning exposes the wealth of (semantic) knowledge\nstored in the MLMs. In this case in particular, we\ndemonstrate that the Mirror-BERT procedure can\nrewire the MLMs to serve as universal language en-\ncoders even without any external supervision. We\nfurther show that data augmentation in both input\nspace and feature space are key to the success of\nMirror-BERT, and they provide a synergistic effect.\nContributions. 1)We propose a completely self-\nsupervised approach that can quickly transform\npretrained MLMs into capable universal lexical\nand sentence encoders, greatly outperforming off-\nthe-shelf MLMs in similarity tasks across different\nlanguages and domains. 2) We investigate the ra-\ntionales behind why Mirror-BERT works at all,\naiming to understand the impact of data augmen-\ntation in the input space as well as in the feature\nspace. We release our code and models at https:\n//github.com/cambridgeltl/mirror-bert.\n2 Mirror-BERT: Methodology\nMirror-BERT consists of three main parts, de-\nscribed in what follows. First, we create positive\npairs by duplicating the input text (§2.1). We then\nfurther process the positive pairs by simple data\naugmentation operating either on the input text or\non the feature map inside the model (§2.2). Finally,\nwe apply standard contrastive learning, ‘attracting’\nthe texts belonging to the same class (i.e., positives)\nwhile pushing away the negatives (§2.3).\n2.1 Training Data through Self-Duplication\nThe key to success of dual-network representa-\ntion learning (Henderson et al., 2019; Reimers and\nGurevych, 2019; Humeau et al., 2020; Liu et al.,\n2021, inter alia) is the construction of positive and\nnegative pairs. While negative pairs can be eas-\nily obtained from randomly sampled texts, posi-\ntive pairs usually need to be manually annotated.\nIn practice, they are extracted from labelled task\ndata (e.g., NLI) or knowledge bases that store rela-\ntions such as synonymy or hypernymy (e.g., PPDB,\nPavlick et al. 2015; BabelNet, Ehrmann et al. 2014;\nWordNet, Fellbaum 1998; UMLS).\nMirror-BERT, however, does not rely on any ex-\nternal data to construct the positive examples. In\na nutshell, given a set of non-duplicated strings\nX, we assign individual labels (yi) to each string\nand build a dataset D= {(xi,yi)|xi ∈X ,yi ∈\n{1,..., |X|}}. We then create self-duplicated\ntraining data D′ simply by repeating every ele-\nment in D. In other words, let X= {x1,x2,... }.\nWe then have D = {(x1,y1),(x2,y2),... }and\nD′ = {(x1,y1),(x1,y1),(x2,y2),(x2,y2),... }\nwhere x1 = x1,y1 = y1,x2 = x2,y2 = y2,... . In\n§2.2, we introduce data augmentation techniques\n(in both input space and feature space) applied on\nD′. Each positive pair (xi,xi) yields two different\npoints/vectors in the encoder’s representation space\n(see again Fig. 1), and the distance between these\npoints should be minimised.\n2.2 Data Augmentation\nWe hypothesise that applying certain ‘corruption’\ntechniques to (i) parts of input text sequences or\n(ii) to their representations, or even (iii) doing both\nin combination, does not change their (captured)\nmeaning. We present two ‘corruption’ techniques\nas illustrated in Fig. 1. First, we can directly mask\nparts of the input text. Second, we can erase (i.e.,\ndropout) parts of their feature maps. Both tech-\nniques are rather simple and intuitive: (i) even\nwhen masking parts of an input sentence, humans\ncan usually reconstruct its semantics; (ii) dropping\na small subset of neurons or representation dimen-\n1444\nAssumption\nv1 == v1\nf(x1)\nf(x1)\nf(x2)\nf(x3)\nf(x5)\nf(x4)\ndist(f(x1), f(x1)) < dist(f(x1/x1), f(x2/x3/…))\nx1: Economist Paul Krugman mainly works on trade models. \nx1: Econ [MASK] Paul Krugman mainly works on trade models. \nBERT/\nRoBERTa\nBERT/\nRoBERTa\nx1\nweight \nsharing\nx1random masking\ndropout\ndropout(v1) != dropout(v1)\ndropout\nmultiple \ndropout layers\nFigure 2: An example of input data augmentation via\nrandom span masking.\nsions, the representations of a neural network will\nnot drift too much.\nInput Augmentation: Random Span Masking.\nThe idea is inspired by random cropping in visual\nrepresentation learning (Hendrycks et al., 2020). In\nparticular, starting from the mirrored pairs (xi,yi)\nand (xi,yi), we randomly replace a consecutive\nstring of length kwith [MASK] in either xi or xi.\nThe example (Fig. 2) illustrates the random span\nmasking procedure with k= 5.\nFeature Augmentation: Dropout. The random\nspan masking technique, operating directly on text\ninput, can be applied only with sentence/phrase-\nlevel input; word-level tasks involve only short\nstrings, usually represented as a single token under\nthe sentence-piece tokeniser. However, data aug-\nmentation in the feature space based on dropout, as\nintroduced below, can be applied to any input text.\nDropout (Srivastava et al., 2014) randomly drops\nneurons from a neural net during training with a\nprobability p. In practice, it results in the erasure of\neach element with a probability of p. It has mostly\nbeen interpreted as implicitly bagging a large num-\nber of neural networks which share parameters at\ntest time (Bouthillier et al., 2015). Here, we take\nadvantage of the dropout layers in BERT/RoBERTa\nto create augmented views of the input text. Given\na pair of identical strings xi and xi, their repre-\nsentations in the embedding space slightly differ\ndue to the existence of multiple dropout layers in\nthe BERT/RoBERTa architecture (Fig. 6). The two\ndata points in the embedding space can be seen as\ntwo augmented views of the same text sequence,\nwhich can be leveraged for ﬁne-tuning.1\nIt is possible to combine data augmentation via\nrandom span masking and featuure augmentation\nvia dropout; this variant is also evaluated later.\n2.3 Contrastive Learning\nLet f(·) denote the encoder model. The encoder\nis then ﬁne-tuned on the data constructed in §2.2.\n1The dropout augmentations are naturally a part of the\nBERT/RoBERTa network. That is, no further actions need to\nbe taken to implement them. Note that random span masking\nis applied on only one side of the positive pair while dropout\nis applied on all data points.\ndropout dropout\n  ==  v1 ¯v1\ndropout(  ) != dropout(  )v1 ¯v1\nFigure 3: As the same vector goes through the same\ndropout layer separately, the outcomes are independent.\nConsequently, two fully identical strings fed to the sin-\ngle BERT/RoBERTa model yield different representa-\ntions in the MLM embedding space.\nGiven a batch of dataD′b, we leverage the standard\nInfoNCE loss (Oord et al., 2018) to cluster/attract\nthe positive pairs together and push away the nega-\ntive pairs in the embedding space:\nLb = −\n|Db|∑\ni=1\nlog exp(cos(f(xi),f(xi))/τ)∑\nxj∈Ni\nexp(cos(f(xi),f(xj))/τ)\n. (1)\nτ denotes a temperature parameter; Ni denotes all\nnegatives ofxi, which includes allxj,xj where i̸=\njin the current data batch (i.e., |Ni|= |D′b|−2).\nIntuitively, the numerator is the similarity of the\nself-duplicated pair (the positive example) and the\ndenominator is the sum of the similarities between\nxi and all other strings besides xi (the negatives).2\n3 Experimental Setup\nEvaluation Tasks: Lexical. We evaluate on\ndomain-general and domain-speciﬁc tasks: word\nsimilarity and biomedical entity linking (BEL). For\nthe former, we rely on the Multi-SimLex evaluation\nset (Vuli´c et al., 2020a): it contains human-elicited\nword similarity scores for multiple languages. For\nthe latter, we use NCBI-disease (NCBI, Do ˘gan\net al. 2014), BC5CDR-disease, BC5CDR-chemical\n(BC5-d, BC5-c, Li et al. 2016), AskAPatient\n(Limsopatham and Collier, 2016) and COMETA\n(stratiﬁed-general split, Basaldella et al. 2020) as\nour evaluation datasets. The ﬁrst three datasets are\nin the scientiﬁc domain (i.e., the data have been ex-\ntracted from scientiﬁc papers), while the latter two\n2We also experimented with another state-of-the-art con-\ntrastive learning scheme proposed by Liu et al. (2021). There,\nhard triplet mining combined with multi-similarity loss (MS\nloss) is used as the learning objective. InfoNCE and triplet\nmining + MS loss work mostly on par, with slight gains of\none variant in some tasks, and vice versa. For simplicity and\nbrevity, we report the results only with InfoNCE.\n1445\nare in the social media domain (i.e., extracted from\nonline forums discussing health-related topics). We\nreport Spearman’s rank correlation coefﬁcients (ρ)\nfor word similarity; accuracy @1/@5 is the stan-\ndard evaluation measure in the BEL task.\nEvaluation Tasks: Sentence-Level. Evaluation\non the intrinsic sentence textual similarity (STS)\ntask is conducted on the standard SemEval 2012-\n2016 datasets (Agirre et al., 2012, 2013, 2014,\n2015, 2016), STS Benchmark (STS-b, Cer et al.\n2017), SICK-Relatedness (SICK-R, Marelli et al.\n2014) for English; STS SemEval-17 data is used for\nSpanish and Arabic (Cer et al., 2017), and we also\nevaluate on Russian STS.3 We report Spearman’sρ\nrank correlation. Evaluation in the question-answer\nentailment task is conducted on QNLI (Rajpurkar\net al., 2016; Wang et al., 2019b). It contains 110k\nEnglish QA pairs with binary entailment labels.4\nEvaluation Tasks: Cross-Lingual. We also as-\nsess the beneﬁts of Mirror-BERT on cross-lingual\nrepresentation learning, evaluating on cross-lingual\nword similarity (CLWS, Multi-SimLex is used) and\nbilingual lexicon induction (BLI). We rely on the\nstandard mapping-based BLI setup (Artetxe et al.,\n2018), and training and test sets from Glavaš et al.\n(2019), reporting accuracy @1 scores (with CSLS\nas the word retrieval method, Lample et al. 2018).\nMirror-BERT: Training Resources. For ﬁne-\ntuning (general-domain) lexical representations,\nwe use the top 10k most frequent words in each\nlanguage. For biomedical name representations,\nwe randomly sample 10k names from the UMLS.\nIn sentence-level tasks, for STS, we sample 10k\nsentences (without labels) from the training set\nof the STS Benchmark; for Spanish, Arabic and\nRussian, we sample 10k sentences from the Wiki-\nMatrix dataset (Schwenk et al., 2021). For QNLI,\nwe sample 10k sentences from its training set.\nTraining Setup and Details.The hyperparame-\nters of word-level models are tuned on SimLex-999\n(Hill et al., 2015); biomedical models are tuned\non COMETA (zero-shot-general split). Sentence-\nlevel models are tuned on the dev set of STS-b. τ\nin Eq. (1) is 0.04 (biomedical and sentence-level\nmodels); 0.2 (word-level). Dropout rate pis 0.1.\nSentence-level models use a random span masking\n3github.com/deepmipt/deepPavlovEval\n4We follow the setup of Li et al. (2020) and adapt QNLI\nto an unsupervised task by computing the AUC scores (on\nthe development set, ≈5.4k pairs) using 0/1 labels and cosine\nsimilarity scores of QA embeddings.\nlang.→ EN FR ET AR ZH RU ES PL avg.\nfastText .528 .560 .447 .409 .428 .435 .488 .396 .461\nBERT .267 .020 .106 .220 .398 .202 .177 .217 .201\n+ Mirror .556 .621 .308 .538 .639 .365 .296 .444 .471\nmBERT .105 .130 .094 .101 .261 .109 .095 .087 .123\n+ Mirror .358 .341 .134 .097 .501 .210 .332 .141 .264\nTable 1: Word similarity evaluation on Multi-SimLex.\n“BERT” denotes monolingual BERT models in each\nlanguage (see the Appendix). “mBERT” denotes mul-\ntilingual BERT.Bold and underline denote highest and\nsecond-highest scores per column, respectively.\nscientiﬁc language social media language\ndataset→\nmodel↓\nNCBI BC5-d BC5-c AskAPatient COMETA\n@1 @5 @1 @5 @1 @5 @1 @5 @1 @5\nSapBERT .920 .956 .935 .960 .965 .982 .705 .889 .659 .779\nBERT .676 .770 .815 .891 .798 .912 .382 .433 .404 .477\n+ Mirror .872 .921 .921 .949 .957 .971 .555 .695 .547 .647\nPubMedBERT .778 .869 .890 .938 .930 .946 .425 .496 .468 .532\n+ Mirror .909 .948 .930 .962 .958 .979 .590 .750 .603 .713\nTable 2: Biomedical entity linking (BEL) evaluation.\nrate of k= 5, while k= 2for biomedical phrase-\nlevel models; we do not employ span masking for\nword-level models (an analysis is in the Appendix).\nAll lexical models are trained for 2 epochs, max to-\nken length is 25. Sentence-level models are trained\nfor 1 epoch with a max sequence length of 50.\nAll models use AdamW (Loshchilov and Hut-\nter, 2019) as the optimiser, with a learning rate of\n2e-5, batch size of 200 (400 after duplication).\nIn all tasks, for all ‘Mirror-tuned’ models, unless\nnoted otherwise, we create ﬁnal representations\nusing [CLS], instead of another common option:\nmean-pooling (mp) over all token representations\nin the last layer (Reimers and Gurevych, 2019).5 6\n4 Results and Discussion\n4.1 Lexical-Level Tasks\nWord Similarity (Tab. 1).SotA static word em-\nbeddings such as fastText (Mikolov et al., 2018)\ntypically outperform off-the-shelf MLMs on word\nsimilarity datasets (Vuli ´c et al., 2020a). How-\never, our results demonstrate that the Mirror-BERT\nprocedure indeed converts the MLMs into much\nstronger word encoders. The Multi-SimLex results\non 8 languages from Tab. 1 suggest that the ﬁne-\n5For ‘non-Mirrored’ original MLMs, the results withmp\nare reported instead; they produce much better results than\nusing [CLS]; see the Appendix.\n6All reported results are averages of three runs. In general,\nthe training is very stable, with negligible ﬂuctuations.\n1446\nmodel↓, dataset→ STS12 STS13 STS14 STS15 STS16 STS-b SICK-R avg.\nSBERT* .719 .774 . 742 .799 .747 .774 .721 .754\nBERT-CLS .215 .321 .213 .379 .442 .203 .427 .314\nBERT-mp .314 .536 .433 .582 .596 .464 .528 .493\n+ Mirror .670 .801 .713 .812 .743 .764 .699 .743\n+ Mirror (drophead) .691 .811 .730 .819 .757 .780 .691 .754\nRoBERTa-CLS .090 .327 .210 .338 .388 .317 .355 .289\nRoBERTa-mp .134 .126 .124 .203 .224 .129 .320 .180\n+ Mirror .646 .818 .734 .802 .782 .787 .703 .753\n+ Mirror (drophead) .666 .827 .740 .824 .797 .796 .697 .764\nTable 3: English STS. *We were able to reproduce the scores reported in the original Sentence-BERT (SBERT,\nReimers and Gurevych 2019) paper. However, we found mean-pooling over all tokens (including padded ones)\nyield better performance (.754 vs .749). We thus report the stronger baseline.\nmodel↓, lang.→ ES AR RU avg.\nBERT .599 .455 .552 .533\n+ Mirror .709 .669 .673 .684\nmBERT .610 .447 .616 .558\n+ Mirror .755 .594 .692 .680\nTable 4: STS evaluation in other languages.\ntuned +Mirror variant substantially improves the\nperformance of base MLMs (both monolingual and\nmultilingual ones), even beating fastText in 5 out\nof the 8 evaluation languages.7\nWe also observe that it is essential to have a\nstrong base MLM. While Mirror-BERT does offer\nsubstantial performance gains with all base MLMs,\nthe improvement is more pronounced when the\nbase model is strong (e.g., EN, ZH).\nBiomedical Entity Linking (Tab. 2).The goal of\nBEL is to map a biomedical name mention to a\ncontrolled vocabulary (usually a node in a knowl-\nedge graph). Considered a downstream application\nin BioNLP, the BEL task also helps evaluate and\ncompare the quality of biomedical name representa-\ntions: it requires pairwise comparisons between the\nbiomedical mention and all surface strings stored\nin the biomedical knowledge graph.\nThe results from Tab. 2 suggest that our+Mirror\ntransformation achieves very strong gains on top\nof the base MLMs, both BERT and PubMedBERT\n(Gu et al., 2020). We note that PubMedBERT is a\ncurrent SotA MLM in the biomedical domain, and\nperforms signiﬁcantly better than BERT, both be-\nfore and after +Mirror ﬁne-tuning. This highlights\nthe necessity of starting from a domain-speciﬁc\nmodel when possible. On scientiﬁc datasets, the\nself-supervised PubMedBERT+Mirror model is\n7Language codes: see the Appendix for a full listing.\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nBERT (area = 0.545)\nBERT+Mirror (area = 0.674)\nRoBERTa+Mirror (area = 0.709)\nSBERT (area = 0.706)\nFigure 4: Unsupervised QNLI: ROC curves and AOC\nscores.\nvery close to SapBERT, which ﬁne-tunes PubMed-\nBERT with more than 10 million synonyms ex-\ntracted from the external UMLS knowledge base.\nHowever, in the social media domain, PubMed-\nBERT+Mirror still cannot match the performance\nof knowledge-guided SapBERT. This in fact re-\nﬂects the nature and complexity of the task do-\nmain. For the three datasets in the scientiﬁc domain\n(NCBI, BC5-d, BC5-c), strings with similar surface\nforms tend to be associated with the same concept.\nOn the other hand, in the social media domain, se-\nmantics of very different surface strings might be\nthe same.8 This also suggests that Mirror-BERT\nadapts PubMedBERT to a very good surface-form\nencoder for biomedical names, but dealing with\nmore difﬁcult synonymy relations (e.g. as found in\nthe social media) does need external knowledge.9\n4.2 Sentence-Level Tasks\nEnglish STS (Tab. 3). Regardless of the base\nmodel (BERT/RoBERTa), applying+Mirror ﬁne-\n8For instance, HCQ and Plaquenil refer to exactly the same\nconcept on online health forums: Hydroxychloroquine.\n9Motivated by these insights, in future work we will also\ninvestigate a combined approach that blends self-supervision\nand external knowledge (Vuli´c et al., 2021), which could also\nbe automatically mined (Su, 2020; Thakur et al., 2021).\n1447\nlang.→ EN-FR EN -ZH EN -HE FR -ZH FR -HE ZH -HE avg.\nmBERT .163 .118 .071 .142 .104 .010 .101\n+ Mirror .454 .385 .133 .465 .163 .179 .297\nTable 5: Cross-lingual word similarity results.\nlang.→ EN-FR EN -IT EN -RU EN -TR IT -FR RU -FR avg.\nBERT .014 .112 .154 .150 .025 .018 .079\n+ Mirror .458 .378 .336 .289 .417 .345 .371\nTable 6: BLI results.\ntuning greatly boosts performance across all En-\nglish STS datasets. Surprisingly, on average,\nRoBERTa+Mirror, ﬁne-tuned with only 10k sen-\ntences without any external supervision, is on-par\nwith the SBERT model, which is trained on the\nmerged SNLI (Bowman et al., 2015) and MultiNLI\n(Williams et al., 2018) datasets, containing 570k\nand 430k sentence pairs, respectively.\nSpanish, Arabic and Russian STS (Tab. 4).The\nresults in the STS tasks on other languages, which\nall have different scripts, again indicate very large\ngains, using both monolingual language-speciﬁc\nBERTs and mBERT as base MLMs. This conﬁrms\nthat Mirror-BERT is a language-agnostic method.\nQuestion-Answer Entailment (Fig. 4). The re-\nsults indicate that our +Mirror ﬁne-tuning con-\nsistently improves the underlying MLMs. The\nRoBERTa+Mirror variant even shows a slight edge\nover the supervised SBERT model (.709 vs. .706).\n4.3 Cross-Lingual Tasks\nWe observe huge gains across all language pairs\nin CLWS (Tab. 5) and BLI (Tab. 6) after running\nthe Mirror-BERT procedure. For language pairs\nthat involve Hebrew, the improvement is usually\nsmaller. We suspect that this is due to mBERT\nitself containing poor semantic knowledge for He-\nbrew. This ﬁnding aligns with our prior argument\nthat a strong base MLM is still required to obtain\nprominent gains from running Mirror-BERT.\n4.4 Further Discussion and Analyses\nRunning Time. The Mirror-BERT procedure is\nextremely time-efﬁcient. While ﬁne-tuning on\nNLI (SBERT) or UMLS (SapBERT) data can take\nhours, Mirror-BERT with 10k positive pairs com-\npletes the conversion from MLMs to universal lan-\nguage encoders within a minute on two NVIDIA\nRTX 2080Ti GPUs. On average, 10-20 seconds is\nneeded for 1 epoch of the Mirror-BERT procedure.\n1k 10k 20k 50k 100k\ninput size\n0.4\n0.5\n0.6score\n task\nword similarity\nbiomedical entity linking\nsentence similarity\nFigure 5: The impact of the number of ﬁne-tuning “mir-\nrored” examples ( x-axis) on the task performance ( y-\naxis). The scores across tasks are not directly compara-\nble, and are based on different evaluation metrics (§3).\nInput Data Size (Fig. 5). In our main experi-\nments in §4.1-§4.3, we always use 10k examples\nfor Mirror-BERT tuning. In order to assess the\nimportance of the ﬁne-tuning data size, we run a\nrelevant analysis for a subset of base MLMs, and\non a subset of English tasks. In particular, we\nevaluate the following: (i) BERT, Multi-SimLex\n(EN) (word-level); (ii) PubMedBERT, COMETA\n(biomedical phrase-level); (iii) RoBERTa, STS12\n(sentence-level). The results indicate that the per-\nformance in all tasks reaches its peak in the region\nof 10k-20k examples and then gradually decreases,\nwith a steeper drop on the the word-level task.10 11\nRandom Span Masking + Dropout? (Tab. 7).\nWe conduct our ablation studies on the English\nSTS tasks. First, we experiment with turn-\ning off dropout, random span masking, or both.\nWith both techniques turned off, we observe\nlarge performance drops of RoBERTa+Mirror and\nBERT+Mirror (see also the Appendix). Span mask-\ning appears to be the more important factor: its\nabsence causes a larger decrease. However, the\nbest performance is achieved when both dropout\nand random span masking are leveraged, suggest-\ning a synergistic effect when the two augmentation\ntechniques are used together.\nOther Data Augmentation Types? Dropout vs.\nDrophead (Tab. 7).Encouraged by the effective-\nness of random span masking and dropout for\nMirror-BERT, a natural question to pose is: can\n10We suspect that this is due to the inclusion of lower-\nfrequency words into the ﬁne-tuning data: embeddings of\nsuch words typically obtain less reliable embeddings (Pilehvar\net al., 2018).\n11For word-level experiments, we used the top 100k words\nin English according to Wikipedia statistics. For phrase-level\nexperiments, we randomly sampled 100k names from UMLS.\nFor sentence-level experiments we sampled 100k sentences\nfrom SNLI and MultiNLI datasets (as the STS training set has\nfewer than 100k sentences).\n1448\nmodel conﬁguration avg. ρ\nRoBERTa + Mirror .753\n- dropout + drophead .764 ↑.011\n- dropout .732 ↓.021\n- span mask .717 ↓.036\n- dropout & span mask .682 ↓.071\nTable 7: Ablation study: (i) replacing dropout with\ndrophead; (ii) the synergistic effect of dropout and ran-\ndom span masking in the English STS tasks.\ncontrolled \ndropout\ncontrolled \ndropout\n  ==  v1 ¯v1\ndropout(  ) == dropout(  )v1 ¯v1\nFigure 6: Under controlled dropout, if two strings are\nidentical, they will have an identical set of dropout\nmasks throughout the encoding process.\nother augmentation types work as well? Recent\nwork points out that pretrained MLM are heav-\nily overparameterised and most Transformer heads\ncan be pruned without hurting task performance\n(V oita et al., 2019; Kovaleva et al., 2019; Michel\net al., 2019). Zhou et al. (2020) propose a drop-\nhead method: it randomly prunes attention heads\nat MLM training as a regularisation step. We\nthus evaluate a variant of Mirror-BERT where the\ndropout layers are replaced with such dropheads:12\nthis results in even stronger STS performance,\ncf. Tab. 7. In short, this hints that the Mirror-BERT\nframework might beneﬁt from other data and fea-\nture augmentation techniques in future work.13\nRegularisation or Augmentation? (Tab. 8).\nWhen using dropout, is it possible that we are sim-\nply observing the effect of adding/removing regu-\nlarisation instead of the augmentation beneﬁt? To\nanswer this question, we design a simple probe\nthat attempts to disentangle the effect of regular-\n12Drophead rates for BERT and RoBERTa are set to the\ndefault values of 0.2 and 0.05, respectively.\n13Besides the drophead-based feature space augmentation,\nin our side experiments, we also tested input space augmenta-\ntion techniques such as whole-word masking, random token\nmasking, and word reordering; they typically yield perfor-\nmance similar or worse to random span masking. We also\npoint to very recent work that explores text augmentation in\na different context (Wu et al., 2020; Meng et al., 2021). We\nleave a thorough search of optimal augmentation techniques\nfor future work.\nmodel conﬁguration (MLM=RoBERTa) ρon STS12\nrandom span masking \u0017; dropout \u0017 .562\nrandom span masking \u0017; dropout \u0013 .648 ↑ .086\nrandom span masking \u0017; controlled dropout \u0013 .452 ↓ .110\nTable 8: Probing the impact of dropout.\nisation versus augmentation; we turn off random\nspan masking but leave the dropout on (so that the\nregularisation effect remains). However, instead of\nassigning independent dropouts to every individual\nstring (rendering each string slightly different), we\ncontrol the dropouts applied to a positive pair to\nbe identical. As a result, it holds f(xi) = f(xi),\nwhen xi ≡xi,∀i∈{1,··· ,|D|}. We denote this\nas “controlled dropout”. In Tab. 8, we observe that,\nduring the +Mirror ﬁne-tuning, controlled dropout\nlargely underperforms standard dropout and is even\nworse than not using dropout at all. As the only\ndifference between controlled and standard dropout\nis the augmented features for positive pairs in the\nlatter case, this suggests that the gain from +Mir-\nror indeed stems from the data augmentation effect\nrather than from regularisation.\nMirror-BERT Improves Isotropy? (Fig. 7).We\nargue that the gains with Mirror-BERT largely stem\nfrom its reshaping of the embedding space geome-\ntry. Isotropy (i.e., uniformity in all orientations)\nof the embedding space has been a favourable\nproperty for semantic similarity tasks (Arora et al.,\n2016; Mu and Viswanath, 2018). However, Etha-\nyarajh (2019) shows that (off-the-shelf) MLMs’\nrepresentations are anisotropic: they reside in a nar-\nrow cone in the vector space and the average cosine\nsimilarity of (random) data points is extremely high.\nSentence embeddings induced from MLMs with-\nout ﬁne-tuning thus suffer from spatial anistropy\n(Li et al., 2020; Su et al., 2021). Is Mirror-BERT\nthen improving isotropy of the embedding space?14\nTo investigate this claim, we inspect (1) the distri-\nbutions of cosine similarities and (2) an isotropy\nscore, as deﬁned by Mu and Viswanath (2018).\nFirst, we randomly sample 1,000 sentence pairs\nfrom the Quora Question Pairs (QQP) dataset. In\n14Some preliminary evidence from Tab. 7 already leads in\nthis direction: we observe large gains over the base MLMs\neven without any positive examples, that is, when both span\nmasking and dropout are not used (i.e., it always holdsxi = xi\nand f(xi) =f(xi)). During training, this leads to a constant\nnumerator in Eq. (1). In this case, learning collapses to the\nscenario where all gradients solely come from the negatives:\nthe model is simply pushing all data points away from each\nother, resulting in a more isotropic space.\n1449\n0.0 0.5 1.0\ncosine similarity\n0\n5\n10frequency\npositive/negative\npositives\nnegatives\n(a) BERT-CLS\n0.0 0.5 1.0\ncosine similarity\n0.0\n2.5\n5.0\n7.5frequency\npositive/negative\npositives\nnegatives (b) BERT-mp\n0.0 0.5 1.0\ncosine similarity\n0\n2\n4frequency\npositive/negative\npositives\nnegatives (c) BERT + Mirror\nFigure 7: Cosine similarity distribution over 1k sen-\ntence pairs sampled from QQP. Blue and orange mean\npositive and negative similarities, respectively.\nFig. 7, we plot the distributions of pairwise co-\nsine similarities of BERT representations before\n(Figs. 7a and 7b) and after the +Mirror tuning\n(Fig. 7c). The overall cosine similarities (regard-\nless of positive/negative) are greatly reduced and\nthe positives/negatives become easily separable.\nWe also leverage a quantitative isotropy score\n(IS), proposed in prior work (Arora et al., 2016;\nMu and Viswanath, 2018), and deﬁned as follows:\nIS(V) = minc∈C\n∑\nv∈Vexp(c⊤v)\nmaxc∈C\n∑\nv∈Vexp(c⊤v) (2)\nwhere Vis the set of vectors,15 Cis the set of all\npossible unit vectors (i.e., any c so that ∥c∥= 1)\nin the embedding space. In practice, Cis approx-\nimated by the eigenvector set of V⊤V (V is the\nstacked embeddings of V). The larger the IS value,\nmore isotropic an embedding space is (i.e., a per-\nfectly isotropic space obtains the IS score of 1).\nIS scores in Tab. 9 conﬁrm that the+Mirror ﬁne-\ntuning indeed makes the embedding space more\nisotropic. Interestingly, with both data augmenta-\ntion techniques switched off, a naive expectation\nis that IS will increase as the gradients now solely\ncome from negative examples, pushing apart points\nin the space. However, we observe the increase of\nIS only for word-level representations. This hints at\nmore complex dynamics between isotropy and gra-\ndients from positive and negative examples, where\npositives might also contribute to isotropy in some\nsettings. We will examine these dynamics more in\nfuture work.16\nLearning New Knowledge or Exposing Avail-\nable Knowledge?Running Mirror-BERT for more\nepochs, or with more data (see Fig. 5) does not re-\n15Vcomprises the corresponding text data used for Mirror-\nBERT ﬁne-tuning (10k items for each task type).\n16Introducing positive examples also naturally yields\nstronger task performance, as the original semantic space\nis better preserved. Gao et al. (2021) provide an insightful\nanalysis on the balance of learning uniformity and alignment\npreservation, based on the method of Wang and Isola (2020).\nlevel→ word phrase sentence\nBERT .169 .205 .222\n+ Mirror .599 .252 .265\n+ Mirror (w/o aug.) .825 .170 .255\nTable 9: IS of word, phrase, and sentence-level models.\nmodel ρ\nfastText .528\nBERT-CLS .105\nBERT-mp .267\n+ Mirror .556\n+ Mirror (random string) .393\n+ Mirror (random string, lr 5e-5) .481\nTable 10: Running Mirror-BERT with a set of ‘zero-\nsemantics’ random strings. Evaluation is conducted on\nMulti-SimLex (EN).\n0 10k 20k 30k 40k 50k 60k 70k 80k 90k 100k\nword frequency rank in English Wikipedia\n0\n250\n500\n750\n1000\n1250Count\n0.55\n0.56\n0.57\n0.58\nSpearman's rho\nMulti-SimLex (en) word freq. distribution\nBERT+Mirror (w/ words of different freq.)\nFigure 8: Blue: words in Multi-SimLex ( EN) follow a\nlong-tail distribution. Yellow: BERT+Mirror trained\nwith frequent words tend to perform better.\nsult in performance gains. This hints that, instead\nof gaining new knowledge from the ﬁne-tuning\ndata, Mirror-BERT in fact ‘rewires’ existing knowl-\nedge in MLMs (Ben-Zaken et al., 2020). To fur-\nther verify this, we run Mirror-BERT with random\n‘zero-semantics’ words, generated by uniformly\nsampling English letters and digits, and evaluate on\n(EN) Multi-SimLex. Surprisingly, even these data\ncan transform off-the-shelf MLMs into effective\nword encoders: we observe a large improvement\nover the base MLM in this extreme scenario, from\nρ =0.267 to 0.481 (Tab. 10). We did a similar\nexperiment on the sentence-level and observed sim-\nilar trends. However, we note that using the actual\nEnglish texts for ﬁne-tuning still performs better as\nthey are more ‘in-domain’ (with further evidence\nand discussions in the following paragraph).\nSelecting Examples for Fine-Tuning.Using raw\ntext sequences from the end task should be the de-\nfault option for Mirror-BERT ﬁne-tuning since they\nare in-distribution by default, as semantic similarity\nmodels tend to underperform when faced with a\ndomain shift (Zhang et al., 2020). In the general-\ndomain STS tasks, we ﬁnd that using sentences\n1450\nextracted from the STS training set, Wikipedia ar-\nticles, or NLI datasets all yield similar STS per-\nformance after running Mirror-BERT (though op-\ntimal hyperparameters differ). However, porting\nBERT+Mirror trained on STS data to QNLI results\nin AUC drops from .674 to .665. This suggests that\nslight or large domain shifts do affect task perfor-\nmance, further corroborated by our ﬁndings from\nﬁne-tuning with fully random strings (see before).\nFurther, Fig. 8 shows a clear tendency that more\nfrequent strings are more likely to yield good\ntask performance. There, we split the 100k most\nfrequent words from English Wikipedia into 10\nequally sized ﬁne-tuning buckets of 10k examples\neach, and run +Mirror ﬁne-tuning on BERT with\neach bucket. In sum, using frequent in-domain\nexamples seems to be the optimal choice.\n5 Related Work\nSelf-supervised text representations have a large\nbody of literature. Here, due to space constraints,\nwe provide a highly condensed summary of the\nmost related work. Even prior to the emergence\nof large pretrained LMs (PLMs), most represen-\ntation models followed the distributional hypothe-\nsis (Harris, 1954) and exploited the co-occurrence\nstatistics of words/phrases/sentences in large cor-\npora (Mikolov et al., 2013a,b; Pennington et al.,\n2014; Kiros et al., 2015; Hill et al., 2016; Lo-\ngeswaran and Lee, 2018). Recently, DeCLUTR\n(Giorgi et al., 2021) follows the distributional hy-\npothesis and formulates sentence embedding train-\ning as a contrastive learning task where span pairs\nsampled from the same document are treated as pos-\nitive pairs. Very recently, there has been a growing\ninterest in using individual raw sentences for self-\nsupervised contrastive learning on top of PLMs.\nWu et al. (2020) explore input augmentation\ntechniques for sentence representation learning\nwith contrastive objectives. However, they use it\nas an auxiliary loss during full-ﬂedged MLM pre-\ntraining from scratch (Rethmeier and Augenstein,\n2021). In contrast, our post-hoc approach offers a\nlightweight and fast self-supervised transformation\nfrom any pretrained MLM to a universal language\nencoder at lexical or sentence level.\nCarlsson et al. (2021) use two distinct models\nto produce two views of the same text, where we\nrely on a single model, that is, we propose to use\ndropout and random span masking within the same\nmodel to produce the two views, and demonstrate\ntheir synergistic effect. Our study also explores\nword-level and phrase-level representations and\ntasks, and to domain-specialised representations\n(e.g., for the BEL task).\nSimCSE (Gao et al., 2021), a work concurrent\nto ours, adopts the same contrastive loss as Mirror-\nBERT, and also indicates the importance of data\naugmentation through dropout. However, they do\nnot investigate random span masking as data aug-\nmentation in the input space, and limit their model\nto general-domain English sentence representations\nonly, effectively rendering SimCSE a special case\nof the Mirror-BERT framework. Other concurrent\npapers explore a similar idea, such as Self-Guided\nContrastive Learning (Kim et al., 2021), ConSERT\n(Yan et al., 2021), and BSL (Zhang et al., 2021),\ninter alia. They all create two views of the same\nsentence for contrastive learning, with different\nstrategies in feature extraction, data augmentation,\nmodel updating or choice of loss function. How-\never, they offer less complete empirical ﬁndings\ncompared to our work: we additionally evaluate\non (1) lexical-level tasks, (2) tasks in a specialised\nbiomedical domain and (3) cross-lingual tasks.\n6 Conclusion\nWe proposed Mirror-BERT, a simple, fast, self-\nsupervised, and highly effective approach that trans-\nforms large pretrained masked language models\n(MLMs) into universal lexical and sentence en-\ncoders within a minute, and without any external\nsupervision. Mirror-BERT, based on simple un-\nsupervised data augmentation techniques, demon-\nstrates surprisingly strong performance in (word-\nlevel and sentence-level) semantic similarity tasks,\nas well as on biomedical entity linking. The large\ngains over base MLMs are observed for different\nlanguages with different scripts, and across diverse\ndomains. Moreover, we dissected and analysed the\nmain causes behind Mirror-BERT’s efﬁcacy.\nAcknowledgements\nWe thank the reviewers and the AC for their consid-\nerate comments. We also thank the LTL members\nand Xun Wang for insightful feedback. FL is sup-\nported by Grace & Thomas C.H. Chan Cambridge\nScholarship. AK and IV are supported by the ERC\nGrant LEXICAL (no. 648909) and the ERC PoC\nGrant MultiConvAI (no. 957356). NC kindly ac-\nknowledges grant-in-aid funding from ESRC (grant\nnumber ES/T012277/1).\n1451\nReferences\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Iñigo Lopez-Gazpio, Montse Maritxalar, Rada\nMihalcea, German Rigau, Larraitz Uria, and Janyce\nWiebe. 2015. SemEval-2015 task 2: Semantic tex-\ntual similarity, English, Spanish and pilot on inter-\npretability. In Proceedings of the 9th International\nWorkshop on Semantic Evaluation (SemEval 2015) ,\npages 252–263, Denver, Colorado. Association for\nComputational Linguistics.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Rada Mihalcea, German Rigau, and Janyce\nWiebe. 2014. SemEval-2014 task 10: Multilingual\nsemantic textual similarity. In Proceedings of the\n8th International Workshop on Semantic Evaluation\n(SemEval 2014), pages 81–91, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nEneko Agirre, Carmen Banea, Daniel Cer, Mona Diab,\nAitor Gonzalez-Agirre, Rada Mihalcea, German\nRigau, and Janyce Wiebe. 2016. SemEval-2016\ntask 1: Semantic textual similarity, monolingual\nand cross-lingual evaluation. In Proceedings of the\n10th International Workshop on Semantic Evalua-\ntion (SemEval-2016) , pages 497–511, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nEneko Agirre, Daniel Cer, Mona Diab, and Aitor\nGonzalez-Agirre. 2012. SemEval-2012 task 6: A\npilot on semantic textual similarity. In *SEM 2012:\nThe First Joint Conference on Lexical and Compu-\ntational Semantics – Volume 1: Proceedings of the\nmain conference and the shared task, and Volume\n2: Proceedings of the Sixth International Workshop\non Semantic Evaluation (SemEval 2012), pages 385–\n393, Montréal, Canada. Association for Computa-\ntional Linguistics.\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, and Weiwei Guo. 2013. *SEM 2013 shared\ntask: Semantic textual similarity. In Second Joint\nConference on Lexical and Computational Seman-\ntics (*SEM), Volume 1: Proceedings of the Main\nConference and the Shared Task: Semantic Textual\nSimilarity, pages 32–43, Atlanta, Georgia, USA. As-\nsociation for Computational Linguistics.\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,\nand Andrej Risteski. 2016. A latent variable model\napproach to PMI-based word embeddings. Transac-\ntions of the Association for Computational Linguis-\ntics, 4:385–399.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.\nA robust self-learning method for fully unsupervised\ncross-lingual mappings of word embeddings. InPro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 789–798, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nMarco Basaldella, Fangyu Liu, Ehsan Shareghi, and\nNigel Collier. 2020. COMETA: A corpus for med-\nical entity linking in the social media. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n3122–3137, Online. Association for Computational\nLinguistics.\nElad Ben-Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2020. BitFit: Simple parameter-efﬁcient\nﬁne-tuningfor transformer-based masked language-\nmodel. Technical Report.\nOlivier Bodenreider. 2004. The uniﬁed medical lan-\nguage system (umls): integrating biomedical termi-\nnology. Nucleic acids research, 32(suppl_1):D267–\nD270.\nXavier Bouthillier, Kishore Konda, Pascal Vincent, and\nRoland Memisevic. 2015. Dropout as data augmen-\ntation. ArXiv preprint, abs/1506.08700.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nXingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth\nChurch. 2021. Isotropy in the contextual embed-\nding space: Clusters and manifolds. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net.\nFredrik Carlsson, Amaru Cuba Gyllensten, Evan-\ngelia Gogoulou, Erik Ylipää Hellqvist, and Magnus\nSahlgren. 2021. Semantic re-tuning with contrastive\ntension. In 9th International Conference on Learn-\ning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nBrian Strope, and Ray Kurzweil. 2018. Universal\nsentence encoder for English. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 169–174, Brussels, Belgium. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\n1452\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRezarta Islamaj Do ˘gan, Robert Leaman, and Zhiyong\nLu. 2014. Ncbi disease corpus: a resource for dis-\nease name recognition and concept normalization.\nJournal of biomedical informatics, 47:1–10.\nMaud Ehrmann, Francesco Cecconi, Daniele Vannella,\nJohn Philip McCrae, Philipp Cimiano, and Roberto\nNavigli. 2014. Representing multilingual data as\nlinked data: the case of BabelNet 2.0. In Proceed-\nings of the Ninth International Conference on Lan-\nguage Resources and Evaluation (LREC’14), pages\n401–408, Reykjavik, Iceland. European Language\nResources Association (ELRA).\nKawin Ethayarajh. 2019. How contextual are contex-\ntualized word representations? comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nChristiane Fellbaum. 1998. WordNet. MIT Press.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\nArivazhagan, and Wei Wang. 2020. Language-\nagnostic bert sentence embedding. ArXiv preprint,\nabs/2007.01852.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence\nembeddings. ArXiv preprint, abs/2104.08821.\nJohn Giorgi, Osvald Nitski, Bo Wang, and Gary Bader.\n2021. DeCLUTR: Deep contrastive learning for\nunsupervised textual representations. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pages 879–895,\nOnline. Association for Computational Linguistics.\nGoran Glavaš, Robert Litschko, Sebastian Ruder, and\nIvan Vuli´c. 2019. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, com-\nparative analyses, and some misconceptions. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics , pages 710–721,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nGoran Glavaš and Ivan Vuli´c. 2021. Is supervised syn-\ntactic parsing beneﬁcial for language understanding\ntasks? an empirical investigation. In Proceedings of\nthe 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main\nVolume, pages 3090–3104, Online. Association for\nComputational Linguistics.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\nspeciﬁc language model pretraining for biomedi-\ncal natural language processing. ArXiv preprint ,\nabs/2007.15779.\nZellig S Harris. 1954. Distributional structure. Word,\n10(2-3):146–162.\nMatthew Henderson, Ivan Vuli ´c, Daniela Gerz, Iñigo\nCasanueva, Paweł Budzianowski, Sam Coope,\nGeorgios Spithourakis, Tsung-Hsien Wen, Nikola\nMrkši´c, and Pei-Hao Su. 2019. Training neural re-\nsponse selection for task-oriented dialogue systems.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n5392–5404, Florence, Italy. Association for Compu-\ntational Linguistics.\nDan Hendrycks, Norman Mu, Ekin Dogus Cubuk,\nBarret Zoph, Justin Gilmer, and Balaji Lakshmi-\nnarayanan. 2020. Augmix: A simple data process-\ning method to improve robustness and uncertainty.\nIn 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen.\n2016. Learning distributed representations of sen-\ntences from unlabelled data. In Proceedings of the\n2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1367–1377, San\nDiego, California. Association for Computational\nLinguistics.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015.\nSimLex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguis-\ntics, 41(4):665–695.\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Architec-\ntures and pre-training strategies for fast and accurate\nmulti-sentence scoring. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Ad-\ndis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-\nview.net.\nTaeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2021.\nSelf-guided contrastive learning for BERT sentence\nrepresentations. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 2528–2540, Online. Association for\nComputational Linguistics.\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S. Zemel, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. 2015. Skip-thought vec-\ntors. In Advances in Neural Information Processing\nSystems 28: Annual Conference on Neural Informa-\ntion Processing Systems 2015, December 7-12, 2015,\nMontreal, Quebec, Canada, pages 3294–3302.\n1453\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China. Association for\nComputational Linguistics.\nGuillaume Lample, Alexis Conneau, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. In 6th Inter-\nnational Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Conference Track Proceedings . OpenRe-\nview.net.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130, Online. Association for Computa-\ntional Linguistics.\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction.\nDatabase, 2016.\nNut Limsopatham and Nigel Collier. 2016. Normalis-\ning medical concepts in social media texts by learn-\ning semantic representation. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1014–1023, Berlin, Germany. Association for Com-\nputational Linguistics.\nRobert Litschko, Ivan Vuli ´c, Simone Paolo Ponzetto,\nand Goran Glavaš. 2021. Evaluating multilin-\ngual text encoders for unsupervised cross-lingual re-\ntrieval. In Proceedings of 43rd European Confer-\nence on Information Retrieval (ECIR 2021) , pages\n342–358.\nFangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco\nBasaldella, and Nigel Collier. 2021. Self-alignment\npretraining for biomedical entity representations. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 4228–4238, Online. Association for Compu-\ntational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv preprint, abs/1907.11692.\nLajanugen Logeswaran and Honglak Lee. 2018. An\nefﬁcient framework for learning sentence represen-\ntations. In 6th International Conference on Learn-\ning Representations, ICLR 2018, Vancouver, BC,\nCanada, April 30 - May 3, 2018, Conference Track\nProceedings. OpenReview.net.\nIlya Loshchilov and Frank Hutter. 2019. Decou-\npled weight decay regularization. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zampar-\nelli. 2014. A SICK cure for the evaluation of compo-\nsitional distributional semantic models. In Proceed-\nings of the Ninth International Conference on Lan-\nguage Resources and Evaluation (LREC’14), pages\n216–223, Reykjavik, Iceland. European Language\nResources Association (ELRA).\nYu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Ti-\nwary, Paul Bennett, Jiawei Han, and Xia Song. 2021.\nCoco-lm: Correcting and contrasting text sequences\nfor language model pretraining. ArXiv preprint ,\nabs/2102.08473.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-\n14, 2019, Vancouver, BC, Canada , pages 14014–\n14024.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013a. Efﬁcient estimation of word\nrepresentations in vector space. ArXiv preprint ,\nabs/1301.3781.\nTomas Mikolov, Edouard Grave, Piotr Bojanowski,\nChristian Puhrsch, and Armand Joulin. 2018. Ad-\nvances in pre-training distributed word representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nTomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013b. Distributed rep-\nresentations of words and phrases and their com-\npositionality. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States , pages 3111–\n3119.\nJiaqi Mu and Pramod Viswanath. 2018. All-but-the-\ntop: Simple and effective postprocessing for word\nrepresentations. In 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings. OpenReview.net.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. ArXiv preprint, abs/1807.03748.\n1454\nEllie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,\nBenjamin Van Durme, and Chris Callison-Burch.\n2015. PPDB 2.0: Better paraphrase ranking, ﬁne-\ngrained entailment relations, word embeddings, and\nstyle classiﬁcation. In Proceedings of the 53rd An-\nnual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 425–430, Beijing, China. As-\nsociation for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nMohammad Taher Pilehvar, Dimitri Kartsaklis, Vic-\ntor Prokhorov, and Nigel Collier. 2018. Card-660:\nCambridge rare word dataset - a reliable benchmark\nfor infrequent word representation models. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1391–\n1401, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nSara Rajaee and Mohammad Taher Pilehvar. 2021. A\ncluster-based approach for improving isotropy in\ncontextual embedding space. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers) , pages 575–584, Online. As-\nsociation for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for\nComputational Linguistics.\nNils Rethmeier and Isabelle Augenstein. 2021. A\nprimer on contrastive pretraining in language pro-\ncessing: Methods, lessons learned and perspectives.\nArXiv preprint, abs/2102.12982.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzmán. 2021. Wiki-\nMatrix: Mining 135M parallel sentences in 1620\nlanguage pairs from Wikipedia. In Proceedings of\nthe 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main\nVolume, pages 1351–1361, Online. Association for\nComputational Linguistics.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nJianlin Su. 2020. Simbert: Integrating retrieval and\ngeneration into bert. Technical report.\nJianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou.\n2021. Whitening sentence representations for bet-\nter semantics and faster retrieval. ArXiv preprint,\nabs/2103.15316.\nNandan Thakur, Nils Reimers, Johannes Daxen-\nberger, and Iryna Gurevych. 2021. Augmented\nSBERT: Data augmentation method for improving\nbi-encoders for pairwise sentence scoring tasks. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 296–310, Online. Association for Computa-\ntional Linguistics.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nIvan Vuli´c, Simon Baker, Edoardo Maria Ponti, Ulla\nPetti, Ira Leviant, Kelly Wing, Olga Majewska, Eden\nBar, Matt Malone, Thierry Poibeau, Roi Reichart,\nand Anna Korhonen. 2020a. Multi-SimLex: A large-\nscale evaluation of multilingual and crosslingual lex-\nical semantic similarity. Computational Linguistics,\n46(4):847–897.\nIvan Vuli´c, Edoardo Maria Ponti, Anna Korhonen, and\nGoran Glavaš. 2021. LexFit: Lexical ﬁne-tuning of\npretrained language models. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 5269–5283, Online. As-\nsociation for Computational Linguistics.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020b. Prob-\ning pretrained language models for lexical semantics.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7222–7240, Online. Association for Computa-\ntional Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\n1455\nLevy, and Samuel R. Bowman. 2019a. Superglue:\nA stickier benchmark for general-purpose language\nunderstanding systems. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 3261–3275.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nTongzhou Wang and Phillip Isola. 2020. Understand-\ning contrastive representation learning through align-\nment and uniformity on the hypersphere. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 9929–9939. PMLR.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Madian\nKhabsa, Fei Sun, and Hao Ma. 2020. Clear: Con-\ntrastive learning for sentence representation. ArXiv\npreprint, abs/2012.15466.\nYuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang,\nWei Wu, and Weiran Xu. 2021. ConSERT: A con-\ntrastive framework for self-supervised sentence rep-\nresentation transfer. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5065–5075, Online. Associa-\ntion for Computational Linguistics.\nYan Zhang, Ruidan He, Zuozhu Liu, Lidong Bing, and\nHaizhou Li. 2021. Bootstrapped unsupervised sen-\ntence representation learning. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 5168–5180, Online. As-\nsociation for Computational Linguistics.\nYan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim,\nand Lidong Bing. 2020. An unsupervised sentence\nembedding method by mutual information maxi-\nmization. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP) , pages 1601–1610, Online. Associa-\ntion for Computational Linguistics.\nWangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou,\nand Ke Xu. 2020. Scheduled DropHead: A regu-\nlarization method for transformer models. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020 , pages 1971–1980, Online. As-\nsociation for Computational Linguistics.\n1456\nA Language Codes\nEN English\nES Spanish\nFR French\nPL Polish\nET Estonian\nFI Finnish\nRU Russian\nTR Turkish\nIT Italian\nZH Chinese\nAR Arabic\nHE Hebrew\nTable 11: Language abbreviations used in the paper.\nB Additional Training Details\nMost Frequent 10k/100k Words by Language.\nThe most frequent 10k words in each language\nwere selected based on the following list:\nhttps://github.com/oprogramador/\nmost-common-words-by-language .\nThe most frequent 100k English words in\nWikipedia can be found here:\nhttps://gist.github.com/h3xx/\n1976236.\n[CLS] or Mean-Pooling? For MLMs, the con-\nsensus in the community, also validated by our\nown experiments, is that mean-pooling performs\nbetter than using [CLS] as the ﬁnal output rep-\nresentation. However, for Mirror-BERT models,\nwe found [CLS] (before pooling) generally per-\nforms better than mean-pooling. The exception is\nBERT on sentence-level tasks, where we found\nmean-pooling performs better than [CLS]. In\nsum, sentence-level BERT+Mirror models are ﬁne-\ntuned and tested with mean-pooling while all other\nMirror-BERT models are ﬁne-tuned and tested with\n[CLS]. We also tried representations after the\npooling layer, but found no improvement.\nTraining Stability. All task results are reported\nas averages over three runs with different random\nseeds (if applicable). In general, ﬁne-tuning is very\nstable and the ﬂuctuations with different random\nseeds are very small. For instance, on the sentence-\nlevel task STS, the standard deviation is <0.002.\nOn word-level, standard deviation is a bit higher,\nbut is generally <0.005. Note that the randomly\ndropout rate→ 0.05 0.1 ∗ 0.2 0.3 0.4\nBERT + Mirror .740 .743 .748 .748 .731\nRoBERTa + Mirror .755 .753 .737 .694 .677\nTable 12: Average ρ across STS tasks with different\ndropout rates. ∗default dropout rate for all models in\nother experiments.\nrandom span mask rate→2 5 ∗ 10 15 20\nBERT + Mirror .741 .743 .720 .690 .616\nRoBERTa + Mirror .750 .753 .757 .743 .706\nTable 13: Avg. ρacross STS tasks with different ran-\ndom span masking rates. ∗default mask rates for all\nmodels in other experiments.\nsampled training sets are ﬁxed across all experi-\nments, and changing the training corpus for each\nrun might lead to larger ﬂuctuations.\nC Details of Mirror-BERT Trained on\nRandom Strings\nWe pointed out in the main text that BERT+Mirror\ntrained on random strings can outperform MLMs\nby large margins. With standard training conﬁgu-\nrations, BERT improves from .267 (BERT-mp) to\n.393 with +Mirror. When learning rate is increased\nto 5e-5, the MLM ﬁne-tuned with random strings\nperforms only around 0.07 lower than the standard\nBERT+Mirror model ﬁne-tuned with the 10k most\nfrequent English words.\nD Dropout and Random Span Masking\nRates\nDropout Rate (Tab. 12).The performance trends\nconditioned on dropout rates are generally the same\nacross word-level, phrase-level and sentence-level\nﬁne-tuning. Here, we use the STS task as a ref-\nerence point. BERT prefers larger dropouts (0.2\n& 0.3) and is generally more robust. RoBERTa\nprefers a smaller dropout rate (0.05) and its perfor-\nmance decreases more steeply with the increase of\nthe dropout rate. For simplicity, as mentioned in\nthe main paper, we use the default value of 0.1 as\nthe dropout rate for all models.\nRandom Span Masking Rate (Tab. 13).Interest-\ningly, the opposite holds for random span masking:\nRoBERTa is more robust to larger masking ratesk,\nand is much more robust than BERT to this hyper-\nparameter.\n1457\nlevel→\nmodel↓\nword phrase sentence\nMVN IS MVN IS MVN IS\nBERT-CLS 13.79 .043 12.8 .028 12.73 .062\nBERT-mp 7.89 .169 6.82 .205 6.93 .222\n+ Mirror 2.11 .599 5.91 .252 5.57 .265\n+ Mirror (w/o aug.) 0.71 .825 8.16 .170 5.75 .255\nTable 14: Full table for MVN and IS of word-, phrase-\n, and sentence-level models. Higher is better, that is,\nmore isotropic with IS, while the opposite holds for\nMVN (lower scores mean more isotropic representa-\ntion spaces).\nE Mean-Vector l2-Norm (MVN)\nTo supplement the quantitative evidence already\nsuggested by the Isotropy Score (IS) in the main\npaper, we additionally compute the mean-vector l2-\nnorm (MVN) of embeddings. In the word embed-\nding literature, mean-centering has been a widely\nstudied post-processing technique for inducing bet-\nter semantic representations. Mu and Viswanath\n(2018) point out that mean-centering is essentially\nincreasing spatial isotropy by shifting the centre\nof the space to the region where actual data points\nreside in. Given a set of representation vectors V,\nwe deﬁne MVN as follows:\nMVN(V) =\n\n∑\nv∈V\nv\n|V|\n\n2\n. (3)\nThe lower MVN is, the more mean-centered an em-\nbedding is. As shown in Tab. 14, MVN aligns with\nthe trends observed with IS. This further conﬁrms\nour intuition that +Mirror tuning makes the space\nmore isotropic and shifts the centre of space close\nto the centre of data points.\nVery recently, Cai et al. (2021) deﬁned more\nmetrics to measure spatial isotropy. Rajaee and\nPilehvar (2021) also used Eq. (2) for analysing\nsentence embedding’s isotropiness.\nF Evaluation Dataset Details\nAll datasets used and links to download them can\nbe found in the code repository provided. The\nRussian STS dataset is provided by\nhttps://github.com/deepmipt/\ndeepPavlovEval. The Quora Ques-\ntion Pair (QQP) dataset is downloaded\nat https://www.kaggle.com/c/\nquora-question-pairs.\nG Pretrained Encoders\nA complete listing of URLs for all used pretrained\nencoders is provided in Tab. 15. For monolingual\nMLMs of each language, we made the best effort\nto select the most popular one (based on download\ncounts). For computational tractability of the large\nnumber of experiments conducted, all models are\nBASE models (instead of LARGE ).\nH Full Tables\nHere, we provide the complete sets of results. In\nthese tables we include both MLMs w/ features\nextracted using both mean-pooling (“mp”) and\n[CLS] (“CLS”).\nFor full multilingual word similarity results,\nview Tab. 16. For full Spanish, Arabic and Russian\nSTS results, view Tab. 3. For full cross-lingual\nword similarity results, view Tab. 18. For full BLI\nresults, view Tab. 19. For full ablation study results,\nview Tab. 20. For full MVN and IS scores, view\nTab. 14.\nI Number of Model Parameters\nAll BERT/RoBERTa models in this paper have\n≈110M parameters.\nJ Hyperparameter Optimisation\nTab. 21 lists the hyperparameter search space. Note\nthat the chosen hyperparameters yield the overall\nbest performance, but might be suboptimal on any\nsingle setting (e.g. different base model).\nK Software and Hardware Dependencies\nAll our experiments are implemented using Py-\nTorch 1.7.0 and huggingface.co transformers\n4.4.2, with Automatic Mixed Precision (AMP) 17\nturned on during training. Please refer to the\nGitHub repo for details. The hardware we use\nis listed in Tab. 22.\n17https://pytorch.org/docs/stable/amp.\nhtml\n1458\nmodel URL\nfastText https://fasttext.cc/docs/en/crawl-vectors.html\nSBERT https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens\nSapBERT https://huggingface.co/cambridgeltl/SapBERT-from-PubMedBERT-fulltext\nBERT (English) https://huggingface.co/bert-base-uncased\nRoBERTa (English)https://huggingface.co/roberta-base\nmBERT https://huggingface.co/bert-base-multilingual-uncased\nTurkish BERT dbmdz/bert-base-turkish-uncased\nItalian BERT dbmdz/bert-base-italian-uncased\nFrench BERT https://huggingface.co/camembert-base\nSpanish BERT https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased\nRussian BERT https://huggingface.co/DeepPavlov/rubert-base-cased\nChinese BERT https://huggingface.co/bert-base-chinese\nArabic BERT https://huggingface.co/aubmindlab/bert-base-arabertv02\nPolish BERT https://huggingface.co/dkleczek/bert-base-polish-uncased-v1\nEstonian BERT https://huggingface.co/tartuNLP/EstBERT\nTable 15: A listing of HuggingFace & fastText URLs of all pretrained models used in this work.\nlanguage→ EN FR ET AR ZH RU ES PL avg.\nfastText .528 .560 .447 .409 .428 .435 .488 .396 .461\nBERT-CLS .105 .050 .160 .210 .277 .177 .152 .257 .174\nBERT-mp .267 .020 .106 .220 .398 .202 .177 .217 .201\n+ Mirror .556 .621 .308 .538 .639 .365 .296 .444 .471\nmBERT-CLS .062 .046 .074 .047 .204 .063 .039 .051 .073\nmBERT-mp .105 .130 .094 .101 .261 .109 .095 .087 .123\n+ Mirror .358 .341 .134 .097 .501 .210 .332 .141 .264\nTable 16: Word similarity evaluation on Multi-SimLex (Spearman’sρ).\nmodel↓, lang.→ ES AR RU avg.\nBERT-CLS .526 .308 .470 .435\nBERT-mp .599 .455 .552 .535\n+ Mirror .709 .669 .673 .684\nmBERT-CLS .421 .326 .430 .392\nmBERT-mp .610 .447 .616 .558\n+ Mirror .755 .594 .692 .680\nTable 17: Full Spanish, Arabic and Russian STS evaluation. Spearman’sρcorrelation reported.\nlang.→ EN-FR EN -ZH EN -HE FR -ZH FR -HE ZH -HE avg.\nmBERT-CLS .059 .053 .032 .042 .024 .050 .043\nmBERT-mp .163 .118 .071 .142 .104 .010 .101\n+ Mirror .454 .385 .133 .465 .163 .179 .297\nTable 18: Full cross-lingual word similarity evaluation on Multi-SimLex (Spearman’sρ).\nlang.→ EN-FR EN -IT EN -RU EN -TR IT -FR RU -FR avg.\nBERT-CLS .045 .049 .108 .109 .046 .068 .071\nBERT-mp .014 .112 .154 .150 .025 .018 .079\n+ Mirror .458 .378 .336 .289 .417 .345 .371\nTable 19: Full Bilingual Lexicon Induction results (accuracy reported). “EN-FR” means en mapped to FR.\n1459\nmodel conﬁguration↓, dataset→ STS12 STS13 STS14 STS15 STS16 STS-b SICK-R avg.\nBERT + Mirror .674 .796 .713 .814 .743 .764 .703 .744\n- dropout .646 .770 .691 .800 .726 .745 .701 .726 ↓.018\n- random span masking .641 .775 .684 .777 .737 .749 .658 .717 ↓.027\n- dropout & random span masking .587 .695 .617 .688 .683 .674 .614 .651 ↓.093\nRoBERTa + Mirror .648 .819 .732 .798 .780 .787 .706 .753\n- dropout .619 .795 .706 .802 .777 .727 .698 .732 ↓.021\n- random span masking .616 .786 .689 .766 .743 .756 .663 .717 ↓.036\n- dropout & random span masking .562 .730 .643 .744 .752 .708 .638 .682 ↓.071\nTable 20: Full table for the synergistic effect of dropout and random span masking in sentence similarity tasks.\nhyperparameters search space\nlearning rate { 5e-5, 2e-5∗, 1e-5}\nbatch size {100, 200 ∗, 300}\ntraining epochs {1 ∗, 2∗, 3, 5}\nτ in Eq. (1) {0.03, 0.04 ∗, 0.05, 0.07, 0.1, 0.2∗, 0.3}\nTable 21: Hyperparameters along with their search grid.∗marks the values used to obtain the reported results. The\nhparams are not always optimal in every setting but generally performs (close to) the best.\nhardware speciﬁcation\nRAM 128 GB\nCPU AMD Ryzen 9 3900x 12-core processor × 24\nGPU NVIDIA GeForce RTX 2080 Ti (11 GB) ×2\nTable 22: Hardware speciﬁcations of the used machine. When encountering out-of-memoery error, we also used a\nsecond server with two NVIDIA GeForce RTX 3090 (24 GB).",
  "topic": "Sentence",
  "concepts": [
    {
      "name": "Sentence",
      "score": 0.8209829926490784
    },
    {
      "name": "Computer science",
      "score": 0.7676455974578857
    },
    {
      "name": "Encoder",
      "score": 0.720318078994751
    },
    {
      "name": "Natural language processing",
      "score": 0.6621456146240234
    },
    {
      "name": "Task (project management)",
      "score": 0.6216185092926025
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6035265922546387
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.560861349105835
    },
    {
      "name": "Speech recognition",
      "score": 0.45026615262031555
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210107233",
      "name": "Language Science (South Korea)",
      "country": "KR"
    }
  ],
  "cited_by": 83
}