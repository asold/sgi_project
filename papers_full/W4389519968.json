{
    "title": "Manipulating the Perceived Personality Traits of Language Models",
    "url": "https://openalex.org/W4389519968",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5064550961",
            "name": "Graham Caron",
            "affiliations": [
                "University of North Carolina Health Care",
                "University of North Carolina at Chapel Hill"
            ]
        },
        {
            "id": "https://openalex.org/A2118811920",
            "name": "Shashank Srivastava",
            "affiliations": [
                "University of North Carolina at Chapel Hill",
                "University of North Carolina Health Care"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4385573216",
        "https://openalex.org/W2767879018",
        "https://openalex.org/W3034723486",
        "https://openalex.org/W4231760770",
        "https://openalex.org/W4205243614",
        "https://openalex.org/W3173681001",
        "https://openalex.org/W1641003075",
        "https://openalex.org/W3101767999",
        "https://openalex.org/W2029283287",
        "https://openalex.org/W3025830159",
        "https://openalex.org/W4221167110",
        "https://openalex.org/W1998744381",
        "https://openalex.org/W4283020727",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4225009785",
        "https://openalex.org/W3170344956",
        "https://openalex.org/W3165022283",
        "https://openalex.org/W2077412304",
        "https://openalex.org/W3130967929",
        "https://openalex.org/W2006652642",
        "https://openalex.org/W4321455981",
        "https://openalex.org/W3214023750",
        "https://openalex.org/W2963612262",
        "https://openalex.org/W2082758677",
        "https://openalex.org/W2777647957",
        "https://openalex.org/W4382765604"
    ],
    "abstract": "Psychology research has long explored aspects of human personality like extroversion, agreeableness and emotional stability, three of the personality traits that make up the 'Big Five'. Categorizations like the 'Big Five' are commonly used to assess and diagnose personality types. In this work, we explore whether text generated from large language models exhibits consistency in it's perceived 'Big Five' personality traits. For example, is a language model such as GPT2 likely to respond in a consistent way if asked to go out to a party? We also show that when exposed to different types of contexts (such as personality descriptions, or answers to diagnostic questions about personality traits), language models such as BERT and GPT2 consistently identify and mirror personality markers in those contexts. This behavior illustrates an ability to be manipulated in a predictable way (with correlations up to 0.84 between intended and realized changes in personality traits), and frames them as tools for controlling personas in applications such as dialog systems. We contribute two data-sets of personality descriptions of humans subjects.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2370–2386\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nManipulating the Perceived Personality Traits of Language Models\nGraham Caron\nUNC Chapel Hill\ncarongraham29@gmail.com\nShashank Srivastava\nUNC Chapel Hill\nssrivastava@cs.unc.edu\nAbstract\nPsychology research has long explored aspects\nof human personality like extroversion, agree-\nableness and emotional stability, three of the\npersonality traits that make up the ‘Big Five’.\nCategorizations like the ‘Big Five’ are com-\nmonly used to assess and diagnose personality\ntypes. In this work, we explore whether text\ngenerated from large language models exhibits\nconsistency in it’s perceived ‘Big Five’ person-\nality traits. For example, is a language model\nsuch as GPT2 likely to respond in a consistent\nway if asked to go out to a party? We also\nshow that when exposed to different types of\ncontexts (such as personality descriptions, or\nanswers to diagnostic questions about person-\nality traits), language models such as BERT\nand GPT2 consistently identify and mirror per-\nsonality markers in those contexts. This be-\nhavior illustrates an ability to be manipulated\nin a predictable way (with correlations up to\n0.84 between intended and realized changes in\npersonality traits), and frames them as tools for\ncontrolling personas in applications such as di-\nalog systems. We contribute two data-sets of\npersonality descriptions of humans subjects.\n1 Introduction\nWith the meteoric rise of AI systems based on lan-\nguage models, there is an increasing need to under-\nstand the ‘personalities’ of these models. As com-\nmunication with AI systems increases, so does the\ntendency to anthropomorphize them (Salles et al.,\n2020; Mueller, 2020; Kuzminykh et al., 2020).\nThus, even though language models encode prob-\nability distributions over text and the tendency to\nassign cognitive abilities to them has been criti-\ncized (Bender and Koller, 2020), the way users\nperceive these systems can have significant conse-\nquences. If the perceived personality traits of these\nmodels can be better understood, their behavior can\nbe tailored for specific applications. For instance,\nwhen suggesting email auto-completes, it may be\nFigure 1: The top frame (Panel A) shows how a per-\nsonality trait (here, openness to experience) might be\nexpressed by a language model, and how the response\ncan be modified by exposing the language model to a\ntextual context. We use psychometric questionnaires to\nevaluate perceived personality traits (Panel B), and show\nthat they can be predictably manipulated with different\ntypes of contexts (§5,§6). We also evaluate the text\ngenerated from these contextualized language models\n(Panel C), and show that they reflect the same traits (§7).\nuseful for a model to mirror the personality of the\nuser. In contrast, for a dialog agent in a clinical\nsetting, it may be desirable to manipulate a model\ninteracting with a depressed individual such that it\ndoes not reinforce depressive behavior. Addition-\nally, since such models are subject to biases in the\ntext they are trained on, some may be prone to in-\nteract with users in hostile ways (Wolf et al., 2017).\nManipulating these models can enable smoother\nand more amiable interactions with users.\nLanguage-based questionnaires have long been\n2370\nused in psychological assessments for measuring\npersonality traits in humans (John et al., 2008).\nWe apply the same principle to language models,\nand investigate the personality traits of these mod-\nels through the text that they generate in response\nto such questions. As previously mentioned, we\ndo not posit that these models have actual cogni-\ntive abilities, but are focused on exploring how\ntheir personality may be perceived through the\nlens of human psychology. Since language mod-\nels are subject to influence from the context they\nsee (O’Connor and Andreas, 2021), we also ex-\nplore how specific context could be used to ma-\nnipulate the perceived personality of the models\nwithout controlling sources of bias or the mod-\nels themselves (i.e., pretraining, parameter fine-\ntuning). Figure 1 shows an example illustrating\nthis approach.\nOur analysis reveals that personality traits of\nlanguage models are influenced by ambient con-\ntext, and that this behavior can be manipulated in\na highly predictable way. In general, we observe\nhigh correlations (median Pearson correlation co-\nefficients of up to 0.84 and 0.81 for BERT and\nGPT2) between the expected and observed changes\nin personality traits across different contexts. The\nmodels’ affinity to be affected by context positions\nthem as potential tools for characterizing person-\nality traits in humans. In further experiments, we\nfind that, when using context from self-reported\ntext descriptions of human subjects, language mod-\nels can predict the subject’s personality traits to a\nsurprising degree (correlation up to 0.48 between\npredicted and actual human subject scores). We\nalso confirm that the measured personality of a\nmodel reflects the personality seen in the text that\nthe model generates. Together, these results frame\nlanguage models as tools for identifying personal-\nity traits and controlling personas in applications\nsuch as dialog systems. Our contributions are:\n• We introduce the use of psychometric question-\nnaires for probing the personalities of language\nmodels.\n• We demonstrate that the personality traits of com-\nmon language models can be predictably con-\ntrolled using textual contexts.\n• We contribute two data-sets: 1) self-reported per-\nsonality descriptions of human subjects paired\nwith their psychometric assessment data, 2) per-\nsonality descriptions collated from Reddit.\n(See project Git repository)\n2 Related Work\nIn recent years, research has looked at multiple\nforms of biases (i.e., racial, gender) in language\nmodels (Bordia and Bowman, 2019; Huang et al.,\n2020; Abid et al., 2021). However, the issue of\nmeasuring and controlling for biases in personas of\nlanguage models is under-explored. A substantial\nbody of research has explored the ways language\nmodels can be used to predict personality traits of\nhumans. Mehta et al. (2020) and Christian et al.\n(2021) apply language models to such personality\nprediction tasks. Similar to our methodology, Ar-\ngyle et al. (2022) contextualize large language mod-\nels on a data-set of socio-economic back-stories to\nshow that they model socio-cultural attitudes in\nbroad human populations, and Yang et al. (2021)\ndevelop a new model designed to better detect per-\nsonalty in user based context, using question based\nanswering. Most relevant to our work are con-\ntemporaneous unpublished works by Karra et al.\n(2022), Miotto et al. (2022), and Jiang et al. (2022),\nwho also explore aspects of personality in the lan-\nguage models themselves. However, these works\nsubstantially diverge from our approach and, along\nwith Yang et al. (2021), do not attempt to character-\nize or manipulate the perceived personality of the\nmodels as we do.\n3 ‘Big Five’ Preliminaries\nThe ‘Big Five’ is a seminal grouping of personality\ntraits in psychological trait theory (Goldberg, 1990,\n1993), and remains the most widely used taxonomy\nof personality traits (John and Srivastava, 1999;\nPureur and Erder, 2016). These traits are:\n• Extroversion (E): People with a strong tendency\nin this trait are outgoing and energetic. They\nobtain energy from the company of others.\n• Agreeableness (A): People with a strong ten-\ndency in this trait are compassionate and kind.\nThey value getting along with others.\n• Conscientiousness (C): People with a strong ten-\ndency in this trait are goal focused and organized.\nThey follow rules and plan their actions.\n• Emotional Stability(ES): People with a strong\ntendency in this trait are not anxious or impulsive.\nThey experience negative emotions less easily.\n• Openness to Experience (OE): People with a\nstrong tendency in this trait are imaginative and\ncreative. They are open to new ideas.\nWhile there are other personality groupings such\nas MBTI and the Enneagram (Bayne, 1997; Wag-\n2371\nner and Walker, 1983), we use the Big Five as the\nbasis of our analyses, because the Big Five remains\nthe most used taxonomy for personality assess-\nment, and has been shown to be predictive of out-\ncomes such as educational attainment (O’Connor\nand Paunonen, 2007), longevity (Masui et al., 2006)\nand relationship satisfaction (White et al., 2004).\nFurther, it is relatively natural to cast as an assess-\nment for language models.\n4 Experiment Design\nWe experiment with two language models, BERT-\nbase (Devlin et al., 2019) and GPT2 (124M param-\neters) (Radford et al., 2019), to answer questions\nfrom a standard 50-item ‘Big Five’ personality as-\nsessment (IPIP, 2022) 1. Each item consists of a\nstatement beginning with the prefix “I” or “I am”\n(e.g., I am the life of the party). Acceptable an-\nswers lie on a 5-point Likert scale where the an-\nswer choices disagree, slightly disagree, neutral,\nslightly agree, and agree correspond to numerical\nscores of 1, 2, 3, 4, and 5, respectively. To make\nthe questionnaire more conducive to answering by\nlanguage models, items were modified to a sen-\ntence completion format. For instance, the item\n“I am the life of the party” was changed to “I am\n{blank} the life of the party”, where the model is ex-\npected to select the answer choice that best fits the\nblank (see Appendix B for a complete list of items\nand their corresponding traits). To avoid complex-\nity due to variable number of tokens, the answer\nchoices were modified to the adverbs never, rarely,\nsometimes, often, and always, corresponding to nu-\nmerical scores 1, 2, 3, 4, and 5, respectively. It is\nnoteworthy that in this framing, an imbalance in\nthe number of occurrences of each answer choice\nin the pretraining data might cause natural biases\ntoward certain answer choices. However, while this\nfactor might affect the absolute scores of the mod-\nels, this is unlikely to affect the consistent overall\npatterns of changes in scores that we observe in our\nexperiments by incorporating different contexts.\nFor assessment with BERT, the answer choice\nwith the highest probability in place of the masked\nblank token was selected as the response. For as-\nsessment with GPT2, the procedure was modified,\nsince GPT2 is an autoregressive model, and hence\nnot directly conducive to fill-in-the-blank tasks. In\nthis case, the probability of the sentence with each\n1BERT & GPT2 were selected because of their availability\nas open-source, pretrained models.\ncandidate answer choice was evaluated, and the\nanswer choice from the sentence with the highest\nprobability was selected.\nFinally, for each questionnaire (consisting of\nmodel responses to 50 questions), personality\nscores for each of the ‘Big Five’ personality traits\nwere calculated according to a standard scoring\nprocedure defined by the International Personality\nItem Pool (IPIP, 2022). Specifically, each of the\nfive personality traits is associated with ten ques-\ntions in the questionnaire. The numerical values\nassociated with the response for these items were\nentered into a formula for the trait in which the\nitem was assigned, leading to an overall integer\nscore for each trait. To interpret model scores, we\nestimated the distribution of ‘Big Five’ personal-\nity traits in the human population. For this, we\nused data from a large-scale survey of ‘Big Five’\npersonality scores in 1,015,000 individuals (Open-\nPsychometrics, 2018). In the following sections,\nwe report model scores in percentile terms of these\nhuman population distributions. Statistics for the\nhuman distributions and details of the IPIP scoring\nprocedure are included in Appendix B.\n5 Base Model Trait Evaluation\nTable 1 shows the results of the base personality\nassessment for GPT2 and BERT for each of the five\ntraits in terms of numeric values and correspond-\ning human population percentiles. In the table, E\nstands for extroversion, A for agreeableness, C for\nconscientiousness, ES for emotional stabilityand\nOE for openness to experience. None of the base\nscores from BERT or GPT2, which we refer to as\nXbase, diverge from the spread of the population\ndistributions (TOST equivalence test at α= 0.05).\nAll scores were within 26 percentile points of the\nhuman population medians. This suggests that\nthe pretraining data reflected the population dis-\ntribution of the personality markers to some extent.\nHowever, percentiles for BERT’s openness to ex-\nperience (24) and GPT2’s agreeableness (25) are\nsubstantially lower and GPT2’s conscientiousness\n(73) and emotional stability(71) are significantly\nhigher than the population median.\n6 Manipulating Personality Traits\nIn this section, we explore manipulating the base\npersonality traits of language models. Our explo-\nration focuses on using prefix contexts to influence\nthe personas of language models. For example,\n2372\nTrait Xbase Pbase (%)\nBERT\nE 18 42\nA 27 39\nC 25 54\nES 22 60\nOE 25 24\nGPT2\nE 21 54\nA 24 25\nC 29 73\nES 25 71\nOE 28 39\nTable 1: Base model evaluation scores (Xbase) and per-\ncentile (Pbase) of these scores in the human population.\nif we include a context where the first person is\nseen to engage in extroverted behavior, the idea\nis that language models might pick up on such\ncues and modify their language generation (e.g., to\ngenerate language that also reflects extrovert behav-\nior). We investigate using three types of context:\n(1) answers to personality assessment items, (2)\ndescriptions of personality from Reddit, and (3)\nself-reported personality descriptions from human\nusers. In the following subsections, we describe\nthese experiments in detail.\n6.1 Analysis With Assessment Item Context\nTo investigate whether the personality traits of mod-\nels can be manipulated predictably, the models are\nfirst evaluated on the ‘Big Five’ assessment (§4)\nwith individual questionnaire items serving as con-\ntext. When used as context, we refer to the answer\nchoices as modifiers and the items themselves as\ncontext items. For example, for extroversion, the\ncontext item “I am {blank} the life of the party\"\npaired with the modifier always yields the context\n“I am always the life of the party\", which precedes\neach extroversion questionnaire item.\nTo calculate the model scores, Xcm, for each\ntrait, the models are evaluated on all ten items as-\nsigned to the trait, with each item serving as context\nonce. This is done for each of the five modifiers,\nresulting in 10 (context items per trait) ×5 (mod-\nifiers per context item) ×10 (questionnaire items\nto be answered by the model) = 500 responses per\ntrait and 10 (context items per trait) ×5 (modifiers\nper context item) = 50 scores (Xcm) per trait (one\nfor each context). Context/modifier ratings ( rcm)\nare calculated to quantify the models’ expected\nbehavior in response to context. First, each mod-\nifier is assigned a modifier rating between -2 and\nTrait Context/Modifier +/-\nBERT\nE I am never the life of the party. -\nA I never make people feel at ease. -\nC I am always prepared. +\nES I never get stressed out easily. +\nOE I never have a rich vocabulary. -\nGPT2\nE I am never the life of the party. -\nA I never have a soft heart. -\nC I am never prepared. -\nES I always get stressed out easily. -\nOE I never have a rich vocabulary. -\nTable 2: List of context items & modifiers (along with\nthe direction of change) that caused the largest magni-\ntude of change, ∆cm, for each personality trait.\n2 with -2 = never, -1 = rarely, 0 = sometimes, 1\n= often and 2 = always. Because this experiment\nexamines correlation between models scores and\nratings, the magnitude of the modifier rating is ar-\nbitrary, so long as the ratings increase linearly from\nnever (strongest negative connotation) to always\n(strongest positive connotation). Context items are\ngiven a context rating of -1 if the item negatively\naffected the trait score based on the IPIP scoring\nprocedure, and 1 otherwise. The context ratings\nare multiplied by the modifier ratings to get the\nrcm. This value represents the expected relative\nchange in trait score (expected behavior) when the\ncorresponding context/modifier pair was used as\ncontext.\nNext, the differences, ∆cm, between Xcm and\nXbase values are calculated and the Pearson cor-\nrelation with the rcm ratings measured (see Table\n2 for the context/modifier pairs with the largest\n∆cm). One would expect Xcm evaluated on more\npositive rcm to increase relative to Xbase and vice\nversa. This is what we observe for BERT (see Fig-\nure 2) and GPT2, both of which show significant\ncorrelations (0.40 and 0.54) between ∆cm and rcm\n(p< 0.01, t-test).\nFurther, to examine at the effect of individ-\nual context items as the strength of the modifier\nchanges, we compute the correlation, ρ, between\n∆cm and rcm for individual context items (correla-\ntion computed from 5 data points per context item,\none for each modifier). Table 3 reports the mean\nand median values of these correlations. These re-\nsults indicate a strong relationship between ∆cm\nand rcm. The mean values are significantly less\nthan the medians, suggesting a left skew. For fur-\nther analysis, the data was broken down by trait.\n2373\nFigure 2: BERT ∆cm vs rcm plots for data from all\ntraits. We observe a consistent change in personality\nscores (∆cm) across context items as the strength of\nquantifiers change.\nBERT GPT2\nMean ρ 0.40 0.54\nMed ρ 0.84 0.81\nTable 3: Mean & median ρfrom ∆cm vs rcm plots by\ncontext item\nThe histograms in Figure 3 depict ρby trait and\ninclude summary statistics for this data.\nMean and median ρfrom Figure 3 plots suggest\na positive linear correlation between ∆cm and rcm\namongst context item plots, with conscientiousness\nand emotional stabilityhaving the strongest corre-\nlation for both BERT and GPT2. Groupings of ρ\naround 1 in conscientiousness and emotional stabil-\nity plots from Figure 3 demonstrate this correlation.\nGPT2 extroversion, BERT & GPT2 agreeableness\nand BERT openness to experienceshow large left\nskews. A possible explanation for for this is that\nmodels may have had difficulty distinguishing be-\ntween the double negative statements created by\nsome context/modifier pairs (i.e. item 36 with mod-\nifier never: “I never don’t like to draw attention\nto myself.\"). This may have caused ∆cm to be\nnegatively correlated with rcm, leading to an accu-\nmulation of ρvalues near -1.\nTable 2 shows the contexts that lead to the largest\nchange for each of the personality traits for BERT\nand GPT2. We observe that all 10 contexts consist\nof the high-polarity quantifiers (either always or\nnever), which is consistent with the correlation re-\nsults. Further, we note that for four of the five traits,\nthe item context that leads to the largest change is\ncommon between the two models.\nIt is important to note a possible weakness with\nour approach of using questionnaire items as con-\ntext. Since our evaluation includes a given ques-\ntionnaire item as context to itself during scoring,\na language model could achieve a spurious cor-\nrelation, simply by copying the modifier choice\nmentioned in the context item. We experimented\nwith adjustments 2 that would account for this issue\nand saw similar trends, with slightly lower but con-\nsistent correlation numbers (mean correlations of\n0.25 and 0.40 for BERT and GPT2, compared with\n0.40 and 0.54, statistically significant at p< 0.05,\nt-test).\nAlternate Framing: Another possible concern\nis the altering of the Big Five personality assess-\nment framing to involve quantifiers. We experi-\nmented with an alternate fill-in-the-blank framing\n(e.g., I {blank} that I am the life of the party) that\nuses the same answer choices as the original test.\nNote that neutral was excluded because it fails to\nform a grammatical sentence. Despite the differ-\nences in token count amongst these answers, the\ngreater frequency imbalance of these answers in the\npretraining data compared to the altered answers,\nand the added sentence complexity of the assess-\nment items, we saw similar trends. BERT extrover-\nsion and emotional stabilityhad mean correlations\nof 0.22 & 0.29 respectively, and GPT2agreeable-\nness, conscientiousness, emotional stability and\nopenness to experiencehad mean correlations of\n0.10, 0.14, 0.61 & 0.40. These results suggest that\nour results are robust to our modification of the\nwording of the answer choices.\n6.2 Analysis With Reddit Context\nNext, we qualitatively analyze how personality\ntraits of language models react to user-specific\ncontexts. To acquire such context data, we cu-\nrated data from Reddit threads asking individuals\nabout their personality (see Appendix D for a list\nof sources). 1119 responses were collected, the\nmajority of which were first person. Table 4 shows\ntwo examples. 3 Because GPT2 & BERT tokeniz-\ners can’t accept more than 512 tokens, responses\nlonger than this were truncated. The models were\nevaluated on the ‘Big Five’ assessment (§4) us-\ning each of the 1119 responses as context (Reddit\n2We replaced the model responses where the questionnaire\nand context items matched with the base model’s response\nfor the item. This means that the concerning context item\ncan no longer contribute to ∆. However, this also means that\nnumbers with this adjustment cannot be directly compared\nwith those without since there are fewer sources of variation.\n3In qualitative analysis of a random sample of 200 re-\nsponses, 3.5% of sampled responses were found to be hostile,\nharmfully biased or offensive, while 71.5% were found to be\nrelevant to the topic of personality.\n2374\nFigure 3: Histograms of ρby trait for ∆cm vs rcm context item plots. Across all ten scenarios, a plurality of context\nitems show a strong correlation (peak close to 1) between observed changes in personality traits and strengths of\nquantifiers in the context items.\nContext\nSubdued until I really get to know someone.\nI am polite but not friendly. I do not feel the need\nto hang around with others and spend most of my time\nreading, listening to music, gaming or watching films.\nGetting to know me well is quite a challenge I suppose,\nbut my few friends and I have a lot of fun when we\nmeet (usually at university or online, rarely elsewhere\nirl). I’d say I am patient, rational and a guy with a\nbig heart for the ones I care for.\nTable 4: Examples of Reddit data context.\ncontext). For each Reddit context, scores, Xreddit,\nwere calculated for all 5 traits. The difference be-\ntween Xreddit and Xbase was calculated as ∆reddit.\nTo interpret what phrases in the contexts affect\nthe language models’ personality traits, we train re-\ngression models on bag-of-words and n-gram (with\nn = 2 and n = 3 ) representations of the Red-\ndit contexts as input, and ∆reddit values as labels.\nSince the goal is to analyze attributes in the con-\ntexts that caused substantial shifts in trait scores, we\nonly consider contexts with ∥∆reddit∥≥ 1. Next,\nwe extract the ten most positive and most nega-\ntive feature weights for each trait. We note that\nfor extroversion, phrases such as ‘friendly’, ‘great’\nand ‘no problem’ are among the highest positively\nweighted phrases, whereas phrases such as ‘stub-\nborn’ and ‘don’t like people’ are among the most\nnegatively weighted. For agreeableness, phrases\nlike ‘love’ and ‘loyal’ are positively weighted,\nwhereas phrases such as ‘lazy’, ‘asshole’ and exple-\ntives are weighted highly negative. On the whole,\nchanges in personality scores for most traits con-\nformed with a human understanding of the most\nhighly weighted features. As further examples,\nphrases such as ‘hang out with’ caused a positive\nshift in trait score foropenness to experience, while\n‘lack of motivation’ causes a negative shift for con-\nscientiousness. There were fewer phrases for GPT2\nopenness to experience, GPT2 negatively weighted\nagreeableness, and GPT2 negatively weighted ex-\ntroversion that caused shifts in the expected direc-\ntion. This was consistent with results from §6.1,\nwhere these traits exhibited the weakest relative\npositive correlations. Appendix D contains the full\nlists of highly weighted features for each trait.\nContext\nUndirected Response\nI am a very open-minded, polite person and always crave\nnew experiences. At work I manage a team of software\ndevelopers and we often have to come up with new ideas.\nI went to college and majored in computer science ...\nI try to do something fun every week, even if I’m busy,\nlike having a BBQ or watching a movie. I have a wife\nwhom I love and we live together in a single-family home.\nDirected Response\nI consider myself to be someone that is quiet and\nreserved. I do not like to talk that much unless I have\nto. I am fine with being by myself and enjoying the peace\nand quiet. I usually agree with people more often than\nnot. I am a polite and kind person. I am mostly honest,\nbut I will lie if I feel it is necessary or if it benefits me\nin a huge way. I am easily irritated by things and I have\nanxiety issues ...\nTable 5: Examples of survey data contexts.\n6.3 Analysis With Psychometric Survey Data\nThe previous sections indicate that language mod-\nels can pick up on personality traits from context.\nThis raises the question of whether they can be\nused to estimate an individual’s personality. In the-\nory, this would be done by evaluating on the ‘Big\nFive’ personality assessment using context describ-\ning the individual, which could aid in personality\ncharacterization in cases where it is not feasible\nfor a subject to manually undergo a personality as-\nsessment. We investigate this with the following\nexperiment. The experimental design for this study\n2375\nwas vetted and approved by an Institutional Review\nBoard (IRB) at the authors’ home institution.\nUsing Amazon Mechanical Turk, subjects were\nasked to complete the 50-item ‘Big Five’ personal-\nity assessment outlined in §4 (the assessment was\nnot modified to a sentence completion format as\nwas done for model testing) and provide a 75-150\nword description of their personality (see Appendix\nE for survey instructions). Responses were man-\nually filtered and low effort attempts discarded,\nresulting in 404 retained responses. Two variations\nof the study were adopted: the subjects for 199 of\nthe responses were provided a brief summary of the\n‘Big Five’ personality traits and asked to consider,\nbut not specifically reference, these traits in their\ndescriptions. We refer to these responses as the\nDirected Responsesdata set. The remaining 205\nsubjects were not provided this summary and their\nresponses make up the Undirected Responsesdata\nset. Table 5 shows examples of collected descrip-\ntions. Despite asking for personality descriptions\nupwards of 75 words, around a fourth of the re-\nsponses fell below this limit. The concern was\nthat data with low word counts may not provide\nenough context. Thus, we experiment with filtering\nthe responses by removing outliers (based on the\ninterquartile ranges of measured correlations) and\nincluding minimum thresholds on the description\nlength (75 and 100).\nHuman subject scores, Xsubject, were calculated\nfor each assessment, using the same scoring pro-\ncedure as previously described in §4. The models\nwere subsequently evaluated on the ‘Big Five’ per-\nsonality assessment using the subjects’ personality\ndescriptions as context, yielding Xsurvey scores\ncorresponding to each subject. Table 6 shows a\nsummary of the correlation statistics for the two\ndata sets and the different filters. There are strong\ncorrelations (0.48 for GPT2 and 0.44 for BERT\nfor Directed Responses) between predicted scores\nfrom personality descriptions and the actual psy-\nchometric assessment scores. We note that there are\nonly marginal differences in correlations between\nthe two datasets, in spite of their different charac-\nteristics. While more specific testing is required\nto determine causal factors that explain these ob-\nserved correlation values, they suggest the potential\nfor using language models as probes for personality\ntraits in free text.\nFigure 4 plots the correlations ( ρ, outliers re-\nmoved) for the individual personality traits, and\nFigure 4: The plot compares ρ from model evalua-\ntion with item context (§6.1) and survey context (§6.3).\nSurvey context ρshown here are from Undirected Re-\nsponses (c≥100). In both cases, ρmeasures the Pear-\nson correlation between trait scores with context and\nexpected behavior. The variables used to quantify ex-\npected behavior differ between experiments.\nTrait ρno−outlier ρc≥75 ρc≥100\nUndirected Responses\nBERT 0.40 0.39 0.41\nGPT2 0.48 0.43 0.48\nDirected Responses\nBERT 0.44 0.42 0.39\nGPT2 0.48 0.43 0.42\nTable 6: ρfor Xsurvey vs Xsubject for data filtered by\nremoving outliers and enforcing word counts.\nincludes correlation coefficients from §6.1. While\nthe correlations from both sections are measured\nfor different variables, they both represent a general\nrelationship between observed personality traits of\nlanguage models and the expected behavior (from\ntwo different types of contexts). While there are\npositive correlations for all ten scenarios, correla-\ntions from survey contexts are smaller than those\nfrom item contexts. This is not surprising since\nitem contexts are specifically handpicked by do-\nmain experts to be relevant to specific personality\ntraits, while survey contexts are free texts from\nopen-ended prompts.\n6.4 Observed Ranges of Personality Traits\nIn the previous subsections, we investigated prim-\ning language models with different types of con-\ntexts to manipulate their personality traits. Figure 5\nsummarizes the observed ranges of personality trait\nscores for different contexts, grouped by context\ntype. The four columns for each trait represent the\nscores achieved by the base model (no context),\nand the ranges of scores achieved by the different\ntypes of contexts. The minimum, median and max-\nimum scores for each context type are indicated\nby different shades on each bar. We observe that\nthe different contexts lead to a remarkable range\n2376\nFigure 5: Observed ranges of personality traits (in hu-\nman percentiles) from BERT, when conditioned on dif-\nferent context types. These include scores from the\nbase model (Pbase) and ranges of scores from the three\ncontext types: item (Pcm), Reddit (Preddit) and survey\n(Psurvey ). Bars for context-based scores show the per-\ncentile of the minimum, median, and maximum-scoring\ncontext, in ascending order. The lightest shade of each\ncolor indicates the minimum, the darkest indicates the\nmaximum and the intermediate indicates the median.\nof scores for all five personality traits. In partic-\nular, for two of the traits ( conscientiousness and\nemotional stability), the models actually achieve\nthe full range of human scores (nearly 0 to 100\npercentile). Curiously, for all five traits, different\ncontexts are able to achieve very low scores (<10\npercentile). However, the models particularly strug-\ngle with achieving high scores for agreeableness.\n7 Effects on Text Generation\nWhile the previous sections strongly suggest that\nthe perceived personality traits of language mod-\nels can be influenced for fill-in-the-blank person-\nality questionnaires, it is important to understand\nwhether these influences also translate to text gen-\nerated by these language models in downstream\napplications. To answer this question, we created\n‘text generation contexts’ by concatenating each\ncontext/modifier pair from §6.1 with each of six\nneutrally framed prompts (e.g., \"I am always the\nlife of the party\"+ \"When I talk to others, I...\",\nsee Appendix F for complete list of prompts). For\nthis experiment, GPT2 4 was used to generate a 50\ntoken text for each text generation context.\nTable 7 gives examples of some text generation\ncontexts and corresponding generated texts. Exam-\nple 1 in Table 7 corresponds to a text generation\ncontext that asserts that the model is “always inter-\nested in people\"; the generated text matches this in\n4Since BERT is trained for masked language modeling,\nand is not well suited for text generation\nExample 1 (agreeableness)\nText Generation Context:\nI am always interested in people. When I have work to\ndo, I ...\nGenerated Text:\noften get curious about people. I love the time I’ve\nspent at my job. My children grow up knowing me really\nwell. Are there any other things you ...\nExample 2 (emotional stability)\nText Generation Context:\nI never get stressed out easily. When I talk to others, I ...\nGenerated Text:\ndon’t get stressed out much either. I can go to\nrestaurants we want to go to and get to see great\nfood or other people that I know, and be ...\nExample 3 (openness to experience)\nText Generation Context:\nI never have excellent ideas. Others say that I ...\nGenerated Text:\nam a fool. When I write my thoughts I try to find out\nwhere I am supposed to get an idea. That is why\nit is so hard for me to do all the ...\nTable 7: Representative examples of Text Generation\nContexts & corresponding Generated Texts. Each text\ngeneration context is a concatenation of a context/modi-\nfier pair from §6.1 and a neutral prompt.\nFigure 6: GPT2 Xgen vs Xcm plots. We observe a\nstrong correlation between scores using generated text\nas context (Xgen) and scores using assessment items &\nanswers as context (Xsubject).\nboth sentiment and topic, describing an individual\nwho is both curious about people and who enjoys\nspending time in an interactive environment like\na job. While there are some generated texts with\nno apparent relation to text generation contexts, we\nfound that most of the generated texts qualitatively\nmirror the personality in text generation context.\nWe also quantitatively evaluate how well the\npersonality traits in the generated texts matches\ncorresponding text generation contexts. For this,\neach generated text is, itself, used as context for a\nBig Five assessment (as previously shown in Figure\n1, panel C). We measure the Pearson correlation\nbetween the resulting scores, Xgen, and the scores\nfor the context/item pair (Xcm) from §6.1 that were\n2377\nused in the corresponding text generation context.\nFigure 6 gives the results from this analysis, and\nshows an overall Pearson correlation of 0.49 be-\ntween Xgen and Xcm.\nThis suggests that the personality scores of the\nmodel, measured using the Big Five assessment,\nare a good indication of the personality that might\nbe seen in text generated from the contextualized\nlanguage models.\n8 Conclusion\nWe have presented a simple approach for mea-\nsuring and controlling the perceived personality\ntraits of language models. Further, we show that\nsuch models can predict personality traits of hu-\nman users, possibly enabling assessment in cases\nwhere participation is difficult to attain. Future\nwork can explore the use of alternate personality\ntaxonomies. Similarly, there is a large and growing\nvariety of language models. It is unclear to what\nextent our findings generalize to other language\nmodels, particularly those with significantly more\nparameters (Brown et al., 2020; Smith et al., 2022).\nFinally, the role that pretraining data plays on per-\nsonality traits is an another important question for\nexploration.\nLimitations\nOur exploration has some notable limitations.\nThese include answer bias due to variable token\ncount and frequency imbalance in pretraining data\nand the presence of double negative statements in\nquestionnaire items (§4). The later might be ad-\ndressed by experimentation with other language\nmodels. For instance, GPT2’s closed source suc-\ncessors, GPT3 and GPT4, are shown to handle dou-\nble negatives better than GPT2 ( (Nguyena et al.,\n2023)). Concerns with the altered questionnaire\nframing and the context item evaluation procedure\nwere partially addressed in follow up experiments\nin §6.1. As mentioned in the Conclusions section,\nwhether and how our results generalize to other\nlanguage models remains an open question.\nEthics and Broader Impact\nThe ‘Big Five’ assessment items and scoring proce-\ndure used in this study were drawn from free pub-\nlic resources and open source implementations of\nBERT and GPT2 (HuggingFace, 2022) were used.\nReddit data was scraped from public threads and\nno usernames or other identifiable markers were\ncollated. The crowd-sourced survey data was col-\nlected using Amazon Mechanical Turk (AMT) with\nthe permission of all participants, following IRB\napproval of the study design. No personally iden-\ntifiable markers were stored and participants were\ncompensated fairly, with a payment rate ($2.00/task\nw/ est. completion time of 15 min) significantly\nhigher than AMT averages (Hara et al., 2018).\nThe broader goal of this line of research is to\ninvestigate aspects of personality in language mod-\nels, which are increasingly being used in a number\nof NLP applications. Since AI systems that use\nthese technologies are growing ever pervasive, and\nas humans tend to anthropomorphize such systems\n(i.e., Siri and Alexa), understanding and controlling\ntheir perceived personalities can have both broad\nand deep consequences. This is especially true for\napplications in domains such as education and men-\ntal health, where interactions with these systems\ncan have lasting personal impacts on their users.\nFinally, if the personalities of AI systems can be\nmanipulated in the ways that our research suggests,\nthere is a serious risk of such systems being ma-\nnipulated, through targeted attacks, to be hostile or\ndisagreeable to their users. Developing methods\nthrough which language models could be made im-\nmune to such attacks would then be a necessary\nconsideration before fielding such systems.\nAcknowledgements\nThis work was supported in part by NSF grant\nDRL2112635. The authors also thank anonymous\nreviewers for suggestions and feedback.\nReferences\nAbubakar Abid, Maheen Farooqi, and James Zou. 2021.\nLarge language models associate muslims with vio-\nlence. Nature Machine Intelligence, 3(6):461–463.\nLisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua\nGubler, Christopher Rytting, and David Wingate.\n2022. Out of one, many: Using language mod-\nels to simulate human samples. arXiv preprint\narXiv:2209.06899.\nRowan Bayne. 1997. The myers-briggs type indicator:\nA critical review and practical guide.\nEmily M. Bender and Alexander Koller. 2020. Climbing\ntowards NLU: On meaning, form, and understanding\nin the age of data. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5185–5198, Online. Association for\nComputational Linguistics.\n2378\nShikha Bordia and Samuel R. Bowman. 2019. Identify-\ning and reducing gender bias in word-level language\nmodels. In Proceedings of the 2019 Conference of\nthe North, Minneapolis, Minnesota.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nHans Christian, Derwin Suhartono, Andry Chowanda,\nand Kamal Z. Zamli. 2021. Text based personality\nprediction from multiple social media data sources\nusing pre-trained language model and model averag-\ning. Journal of Big Data, 8(1).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Minneapolis, Minnesota.\nLewis R. Goldberg. 1990. An alternative \"descrip-\ntion of personality\": The big-five factor struc-\nture. Journal of Personality and Social Psychology,\n59(6):1216–1229.\nLewis R. Goldberg. 1993. The structure of phenotypic\npersonality traits. American Psychologist, 48(1):26–\n34.\nKotaro Hara, Abigail Adams, Kristy Milland, Saiph\nSavage, Chris Callison-Burch, and Jeffrey P. Bigham.\n2018. A data-driven analysis of workers’ earnings on\namazon mechanical turk. Proceedings of the 2018\nCHI Conference on Human Factors in Computing\nSystems.\nPo-Sen Huang, Huan Zhang, Ray Jiang, Robert Stan-\nforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani\nYogatama, and Pushmeet Kohli. 2020. Reducing sen-\ntiment bias in language models via counterfactual\nevaluation. Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020.\nHuggingFace. 2022. the ai community building the\nfuture. Last accessed 22 March 2022.\nIPIP. 2022. Administering IPIP measures, with a 50-\nitem sample questionnaire. Last accessed 22 March\n2022.\nGuangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wen-\njuan Han, Chi Zhang, and Yixin Zhu. 2022. MPI:\nevaluating and Inducing personality in pre-trained\nlanguage models. arXiv preprint arXiv:2206.07550.\nOliver P. John, Laura P. Naumann, and Christopher J.\nSoto. 2008. Paradigm shift to the integrative big-five\ntrait taxonomy: History, measurement, and concep-\ntual issues. In Oliver P. John, Richard W. Robins,\nand Lawrence A. Pervin, editors, Handbook of per-\nsonality: Theory and research, pages 114–158. The\nGuilford Press, New York, New York.\nOliver P. John and Sanjay Srivastava. 1999. The big five\ntrait taxonomy: History, measurement, and theoreti-\ncal perspectives. In Lawrence A. Pervin and Oliver P.\nJohn, editors, Handbook of personality: Theory and\nresearch, pages 102–138. The Guilford Press, New\nYork, New York.\nSaketh Reddy Karra, Son Nguyen, and Theja Tula-\nbandhula. 2022. AI Personification: Estimating\nthe personality of language models. arXiv preprint\narXiv:2204.12000.\nAnastasia Kuzminykh, Jenny Sun, Nivetha Govindaraju,\nJeff Avery, and Edward Lank. 2020. Genie in the\nbottle: Anthropomorphized perceptions of conversa-\ntional agents. In Proceedings of the 2020 CHI Con-\nference on Human Factors in Computing Systems,\npages 1–13.\nY Masui, Y Gondo, H Inagaki, and N Hirose. 2006. Do\npersonality characteristics predict longevity? findings\nfrom the tokyo centenarian study. Age, 28(4):353–\n361.\nYash Mehta, Samin Fatehi, Amirmohammad Kazameini,\nClemens Stachl, Erik Cambria, and Sauleh Eetemadi.\n2020. Bottom-up and top-down: Predicting person-\nality with psycholinguistic and language model fea-\ntures. In 2020 IEEE International Conference on\nData Mining (ICDM), Sorrento, Italy.\nMarilù Miotto, Nicola Rossberg, and Bennett Klein-\nberg. 2022. Who is gpt-3? an exploration of per-\nsonality, values and demographics. arXiv preprint\narXiv:2209.14338.\nShane T Mueller. 2020. Cognitive anthropomorphism\nof ai: How humans and computers classify images.\nErgonomics in Design, 28(3):12–19.\nHa Thanh Nguyena, Randy Goebelb, Francesca Tonic,\nKostas Stathisd, and Ken Satoha. 2023. A nega-\ntion detection assessment of gpts: analysis with the\nxnot360 dataset.\nOpen-Psychometrics. 2018. Open-source psychomet-\nrics project: Answers to the IPIP big five factor mark-\ners. Last accessed 29 August 2022.\nJoe O’Connor and Jacob Andreas. 2021. What con-\ntext features can transformer language models use?\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers).\nMelissa C O’Connor and Sampo V Paunonen. 2007.\nBig five personality predictors of post-secondary aca-\ndemic performance. Personality and Individual dif-\nferences, 43(5):971–990.\nPierre Pureur and Murat Erder. 2016. 8, page 187–213.\nMorgan Kaufmann Publishers.\n2379\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nArleen Salles, Kathinka Evers, and Michele Farisco.\n2020. Anthropomorphism in ai. AJOB neuroscience,\n11(2):88–95.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nJerome P Wagner and Ronald E Walker. 1983. Relia-\nbility and validity study of a sufi personality typol-\nogy: The enneagram. Journal of clinical psychology,\n39(5):712–717.\nJason K White, Susan S Hendrick, and Clyde Hendrick.\n2004. Big five personality variables and relationship\nconstructs. Personality and individual differences,\n37(7):1519–1530.\nMarty J Wolf, Keith W Miller, and Frances S Grodzin-\nsky. 2017. Why we should have seen that com-\ning: comments on microsoft’s tay “experiment,” and\nwider implications. The ORBIT Journal, 1(2):1–12.\nFeifan Yang, Tao Yang, Xiaojun Quan, and Qinliang\nSu. 2021. Learning to answer psychological ques-\ntionnaire for personality detection. Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021.\n2380\nAppendix A Model Background\nBERT, which stands for Bidirectional Encoder Rep-\nresentations from Transformers, is a transformer-\nbased deep learning model for natural language\nprocessing (Devlin et al., 2019). The model is\npretrained on unlabeled data from the 800M word\nBooksCorpus and 2500M word English Wikipedia\ncorpora. While BERT can be fine-tuned for autore-\ngressive language modeling tasks, it is pretrained\nfor masked language modeling. This study uses\na BERT model from HuggingFaces’s Transformer\nPython Library with a language model head for\nmasked language modeling. No fine-tuning was\ndone to the model. GPT2, which stands for Genera-\ntive Pretrained Transformer 2, is a general-purpose\nlearning transformer model developed by OpenAI\nin 2018 (Radford et al., 2019). Like BERT, this\nmodel is also pretrained on unlabeled data from the\n800M word BooksCorpus. The study used Huggin-\nface’s GPT2 model with a language model head for\nautoregressive language modeling. As with BERT,\nno fine-tuning took place.\nAppendix B Experiment Design Items\nFigure 7: Human distributions of ‘Big Five’ trait scores.\n2381\nItem Associated Trait\nI am {blank} the life of the party. E\nI {blank} feel little concern for others. A\nI am {blank} prepared. C\nI {blank} get stressed out easily. ES\nI {blank} have a rich vocabulary. OE\nI {blank} don’t talk a lot. E\nI am {blank} interested in people. A\nI {blank} leave my belongings around. C\nI am {blank} relaxed most of the time. ES\nI {blank} have difficulty understanding abstract ideas. OE\nI {blank} feel comfortable around people. E\nI {blank} insult people. A\nI {blank} pay attention to details. C\nI {blank} worry about things. ES\nI {blank} have a vivid imagination. OE\nI {blank} keep in the background. E\nI {blank} sympathize with others’ feelings. A\nI {blank} make a mess of things. C\nI {blank} seldom feel blue. ES\nI am {blank} not interested in abstract ideas. OE\nI {blank} start conversations. E\nI am {blank} not interested in other people’s problems. A\nI {blank} get chores done right away. C\nI am {blank} easily disturbed. ES\nI {blank} have excellent ideas. OE\nI {blank} have little to say. E\nI {blank} have a soft heart. A\nI {blank} forget to put things back in their proper place. C\nI {blank} get upset easily. ES\nI {blank} do not have a good imagination. OE\nI {blank} talk to a lot of different people at parties. E\nI am {blank} not really interested in others. A\nI {blank} like order. C\nI {blank} change my mood a lot. ES\nI am {blank} quick to understand things. OE\nI {blank} don’t like to draw attention to myself. E\nI {blank} take time out for others. A\nI {blank} shirk my duties. C\nI {blank} have frequent mood swings. ES\nI {blank} use difficult words. OE\nI {blank} don’t mind being the center of attention. E\nI {blank} feel others’ emotions. A\nI {blank} follow a schedule. C\nI {blank} get irritated easily. ES\nI {blank} spend time reflecting on things. OE\nI am {blank} quiet around strangers. E\nI {blank} make people feel at ease. A\nI am {blank} exacting in my work. C\nI {blank} feel blue. ES\nI am {blank} full of ideas. OE\nTable B1: Adjusted ‘Big Five’ Personality Assessment Items.\nTrait Median Mean (µ) SD (σ)\nE 20 19.60 9.10\nA 29 27.74 7.29\nC 24 23.66 7.37\nES 19 19.33 8.59\nOE 29 28.99 6.30\nTable B2: Human Population Distribution of ‘Big Five’ Personality Traits.\n2382\nTrait Base Value Positively Scored Item # Negatively Scored Item #\nE 20 1, 11, 21, 31, 41 6, 16, 26, 36, 46\nA 14 7, 17, 27, 37, 42, 47 2, 12, 22, 32\nC 14 3, 13, 23, 33, 43, 48 8, 18, 28, 38\nES 38 9, 19 4, 14, 24, 29, 34, 39, 44, 49\nOE 8 5, 15, 25, 35, 40, 45, 50 10, 20, 30\nTable B3: ‘Big Five’ Personality Item Scoring Procedure.\nAppendix C Item Context Evaluation Tables\nrcm Mean ∆cm Med ∆cm ∆cm SD Confidence Interval\nBERT\n-2 -3.36 -2.0 7.49 [-5.51, -1.21]\n-1 -3.18 -3.50 4.81 [-4.56, -1.80]\n0 -0.02 0.00 4.51 [-1.32, 1.28]\n1 2.42 2.00 6.17 [0.648, 4.19]\n2 3.96 3.00 8.33 [1.57, 6.35]\nGPT2\n-2 -7.34 -8.0 6.38 [-9.17, -5.51]\n-1 -4.58 -4.0 4.32 [-5.82, -3.34]\n0 -2.06 -1.0 4.24 [-3.28, -0.84]\n1 0.0 0.0 3.13 [-0.90, 0.90]\n2 1.56 1.0 5.78 [-0.10, 3.22]\nTable C1: Statistics from ∆cm vs rcm plots containing data from all traits. Statistics include mean, median, standard\ndeviation and a confidence interval for ∆cm at each rcm.\nAppendix D Reddit Context Evaluation Tables\nReddit Context Sources\nreddit.com/r/AskReddit/comments/k3dhnt/how_would_you_describe_your_personality/\nreddit.com/r/AskReddit/comments/q4ga1j/redditors_what_is_your_personality/\nreddit.com/r/AskReddit/comments/68jl8g/how_can_you_describe_your_personality/\nreddit.com/r/AskReddit/comments/ayjgyz/whats_your_personality_like/\nreddit.com/r/AskReddit/comments/9xjahw/how_would_you_describe_your_personality/\nreddit.com/r/AskWomen/comments/c1gr4a/how_would_you_describe_your_personality/\nreddit.com/r/AskWomen/comments/7x23zg/what_are_your_most_defining_personalitycharacter/\nreddit.com/r/CasualConversation/comments/5xtckg/how_would_you_describe_your_personality/\nreddit.com/r/AskReddit/comments/aewroe/how_would_you_describe_your_personality/\nreddit.com/r/AskMen/comments/c0grgv/how_would_you_describe_your_personality/\nreddit.com/r/AskReddit/comments/pzm3in/how_would_you_describe_your_personality/\nreddit.com/r/AskReddit/comments/bem0ro/how_would_you_describe_your_personality/\nreddit.com/r/AskReddit/comments/1w9yp0/what_is_your_best_personality_trait/\nreddit.com/r/AskReddit/comments/a499ng/what_is_your_worst_personality_trait/\nreddit.com/r/AskReddit/comments/6onwek/what_is_your_worst_personality_trait/\nreddit.com/r/AskReddit/comments/2d7l2i/serious_reddit_what_is_your_worst_character_trait/\nreddit.com/r/AskReddit/comments/449cu7/serious_how_would_you_describe_your_personality/\nTable D1: Domain names of threads that were scraped to collect Reddit context.\nTrait Mean ∆reddit Med ∆reddit ∆reddit SD 5 Max ∆reddit 5 Min ∆reddit\nBERT\nE -2.28 -2 4.04 8, 7, 7, 6, 5 -14, -13, -13, -13, -13\nA -2.02 -1 3.38 2, 2, 2, 2, 2 -19, -18, -15, -15, -15\nC 3.77 4 5.17 15, 15, 15, 15, 13 -17, -17, -16, -14, -13\nES 1.71 2 2.29 14, 14, 13, 13, 12 -12, -10, -10, -10, -10\nOE 1.74 1 2.17 9, 7, 7, 7, 7 -11, -11, -8, -8, -7\nGPT2\nE -3.73 -4 3.33 7, 5, 5, 4, 4 -14, -10, -10, -10, -10\nA -0.98 -1 4.26 13, 10, 8, 7, 7 -17, -15, -15, -15, -14\nC -0.27 0 4.27 11, 11, 11, 11, 9 -20, -16, -16, -16, -15\nES -3.83 -3 6.27 8, 8, 8, 8, 8 -21, -21, -21, -21, -21\nOE -1.91 -2 3.21 4, 4, 4, 4, 4 -15, -12, -12, -12, -12\nTable D2: ∆reddit summary statistics. Statistics include mean, median and standard deviation, as well as 5 largest\nand 5 smallest ∆reddit.\n2383\nBERT\nExtroversion\n• Notable Positively Weighted Phrases: ‘friendly’, ‘great’, ‘good’, ‘quite’, ‘laugh’, ‘please’, ‘sense of’, ‘thanks\nfor’, ‘really good’, ‘and friendly’, ‘no problem’, ‘to please’, ‘my sense of’, ‘finish everything start’, ‘enthusiastic\nbut sensitive’\n• Notable Negatively Weighted Phrases: ‘question’, ‘stubborn’, ‘why’, ‘lack’, ‘fuck’, ‘fucking’, ‘hate’, ‘not’, ‘lack\nof’, ‘too much’, ‘don know’, ‘don like’, ‘too easily’, ‘way too’, ‘don like people’, ‘you go out’, ‘don know how’,\n‘don[’t] know what’\nAgreeableness\n• Notable Positively Weighted Phrases: ‘will’, ‘friendly’, ‘lol’, ‘love’, ‘loyal’, ‘calm’, ‘yup’, ‘does’, ‘honesty’,\n‘laid back’, ‘go out’, ‘thanks for’, ‘really good’, ‘out with me’, ‘friendly polite and’, ‘really good listener’, ‘true\nto myself’, ‘my sense of’\n• Notable Negatively Weighted Phrases: ‘lack’, ‘didn[’t]’, ‘won[’t], ‘lazy’, ‘fucking’, ‘self’, ‘worst’, ‘lack of’,\n‘too easily’, ‘don like’, ‘the worst’, ‘being too’, ‘have no’, ‘don like people’, ‘lack of motivation’, ‘don know\nhow’, ‘my worst trait’, ‘also my worst’, ‘too honest sometimes’, ‘doesn[’t] talk much’\nConscientiousness\n• Notable Positively Weighted Phrases: ‘am’, ‘friendly’, ‘just’, ‘calm’, ‘believe’, ‘can be’, ‘of people’, ‘tend to’,\n‘feel like’, ‘the most humble’, ‘most humble person’, ‘my sense of’, ‘get to know’, ‘friendly polite and’, ‘get\nalong with’, ‘people like me’\n• Notable Negatively Weighted Phrases: ‘lack’, ‘no’, ‘lazy’, ‘inability’, ‘fucks’, ‘half’, ‘lack of’, ‘fuck off’, ‘don\nlike’, ‘inability to’, ‘don like people’, ‘you go out’, ‘lack of motivation’, ‘don even know’, ‘monotonous and\nimpulsive’\nEmotional Stability\n• Notable Positively Weighted Phrases: ‘will’, ‘feel’, ‘out with me’, ‘go out with’, ‘will you go’, ‘the most humble’\n• Notable Negatively Weighted Phrases: ‘no’, ‘off’, ‘hypercritical’, ‘overthinking’, ‘lack of’, ‘easily distracted’,\n‘doesn[’t] talk’, ‘don even’, ‘too easily distracted’, ‘lack of motivation’, ‘doesn[’t] talk much’, ‘don even know’,\n‘unrelatable is strange’, ‘is strange one’, ‘this said foreskin’\nOpenness to Experience\n• Notable Positively Weighted Phrases: ‘most’, ‘like’, ‘me to’, ‘out with’, ‘like me’, ‘like to’, ‘want to’, ‘with me’,\n‘out with me’, ‘will you go’, ‘want to be’, ‘all the time’, ‘for me to’, ‘hang out with’\n• Notable Negatively Weighted Phrases: ‘lack’, ‘never’, ‘fucks’, ‘sad’, ‘nothing’, ‘lack empathy’, ‘the complainer’,\n‘no confidence’, ‘lack of’, ‘easily distracted’, ‘blame helicopter’, ‘helicopter parents’, ‘never say sorry’, ‘blame\nhelicopter parents’, ‘too easily distracted’, ‘finish projects after’, ‘never finish projects’, ‘procrastination out of’,\n‘my lack of’, ‘lack of personality’, ‘too many fucks’\nTable D3: Analysis of highest weighted phrases from BERT logistic regression.\n2384\nGPT2\nExtroversion\n• Notable Positively Weighted Phrases: ‘believe’, ‘loyal’, ‘curious’, ‘best’, ‘passionate’, ‘enjoy’, ‘bright’, ‘hard\nworking’, ‘no problem’, ‘am nice’, ‘my amazing modesty’, ‘smooth bright epic’, ‘patient and flexible’, ‘great\nwith children’, ‘calm cool collected’\n• Notable Negatively Weighted Phrases: ‘introverted’, ‘lack of’, ‘laid back’, ‘don know how’\nAgreeableness\n• Notable Positively Weighted Phrases: ‘friendly’, ‘loyal’, ‘honest’, ‘gay’, ‘humor’, ‘like people’, ‘thanks for’, ‘to\nplease’, ‘and friendly’, ‘no problem’, ‘friendly polite and’, ‘patient and flexible’, ‘calm cool collected’, ‘honesty\nbeing straightforward’\n• Notable Negatively Weighted Phrases: ‘too easily’, ‘too much’, ‘lack of’, ‘you go out’, ‘don know what’, ‘self’,\n‘asshole’\nConscientiousness\n• Notable Positively Weighted Phrases: ‘smile’, ‘thanks for’, ‘no problem’, ‘friendly polite and’, ‘really good\nlistener’, ‘true to myself’, ‘patient and flexible’\n• Notable Negatively Weighted Phrases: ‘stop’, ‘jealousy’, ‘lazy’, ‘hate’, ‘lack’, ‘fuck’, ‘worst’, ‘lack of’, ‘too\neasily’, ‘fuck off’, ‘too nice’, ‘don know’, ‘don know how’, ‘lack of motivation’, ‘don even know’, ‘my worst\ntrait’, ‘damn it uncle’, ‘depressed as shit’\nEmotional Stability\n• Notable Positively Weighted Phrases: ‘friendly’, ‘calm’, ‘easy’, ‘honesty’, ‘laid back’, ‘hard working’, ‘calm\nand’, ‘humble am’, ‘polite and’, ‘no problem’, ‘out with me’, ‘the most humble’\n• Notable Negatively Weighted Phrases: ‘lack’, ‘anxious’, ‘lazy’, ‘jealousy’, ‘lack of’, ‘don know’, ‘too easily’,\n‘don like’, ‘don like people’, ‘don know how’, ‘lack of motivation’, ‘don even know’\nOpenness to Experience\n• Notable Positively Weighted Phrases: ‘understand’, ‘having’, ‘wanting’, ‘thoughts’, ‘thanks for’, ‘too nice’, ‘no\nproblem’, ‘can relate’, ‘being too nice’, ‘that just confidence’\n• Notable Negatively Weighted Phrases: ‘fuck’, ‘myself’, ‘cynical’, ‘lack’, ‘boring’, ‘lack of’, ‘don like people’\nTable D4: Analysis of highest weighted phrases from GPT2 logistic regression.\nAppendix E Survey Context Evaluation Tables\nPart 1 Instruction\nThere are two parts to this questionnaire. In the first part (on this page), you will be shown 50 questions,\nand need to choose a response which best matches your personality. In the second part (on the next page),\nyou will be asked to write a short (75-150 word) description of your personality in free text. Participants\nwill only be compensated if they respond to all questions.\nPart 2 Instruction\nIn between 75 and 150 words, please describe your personality [Directed responses: as it relates to the 5 personality traits\noutlined above. Be sure not to use the name of the personality traits themselves in your response].\nTable E1: Data collection survey instructions.\n2385\nAppendix F Generated Text Evaluation Tables\nText Generation Prompts\nWhen I go to a gathering, I ...\nOthers say that I am ...\nWhen I am around people, I ...\nWhen I have work to do, I ...\nWhen I have free time, I ...\nWhen I talk to others, I ...\nTable F1: List of prompts used in text generation context.\n2386"
}