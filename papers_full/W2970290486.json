{
    "title": "Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention",
    "url": "https://openalex.org/W2970290486",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2017340206",
            "name": "Biao Zhang",
            "affiliations": [
                "University of Edinburgh"
            ]
        },
        {
            "id": "https://openalex.org/A2127391507",
            "name": "Ivan Titov",
            "affiliations": [
                "University of Edinburgh",
                "University of Amsterdam"
            ]
        },
        {
            "id": "https://openalex.org/A93200637",
            "name": "Rico Sennrich",
            "affiliations": [
                "University of Zurich",
                "University of Edinburgh"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4297747548",
        "https://openalex.org/W2962835968",
        "https://openalex.org/W2963091079",
        "https://openalex.org/W2970690146",
        "https://openalex.org/W2767206889",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2972451902",
        "https://openalex.org/W2963991316",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963713328",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2963248296",
        "https://openalex.org/W2964213727",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2594990650",
        "https://openalex.org/W2897983179",
        "https://openalex.org/W2912521296",
        "https://openalex.org/W4323654151",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2903193068",
        "https://openalex.org/W1603870118",
        "https://openalex.org/W4289302788",
        "https://openalex.org/W2257408573",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W4394650345",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2963532001",
        "https://openalex.org/W2908336025",
        "https://openalex.org/W2963599677",
        "https://openalex.org/W3041866211",
        "https://openalex.org/W2952809536",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W2963946353",
        "https://openalex.org/W4300852664",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W4297687616",
        "https://openalex.org/W2963807318",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2964045208",
        "https://openalex.org/W2176263492",
        "https://openalex.org/W2888520903",
        "https://openalex.org/W2805493160",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2963403868"
    ],
    "abstract": "The general trend in NLP is towards increasing model capacity and performance via deeper neural networks. However, simply stacking more layers of the popular Transformer architecture for machine translation results in poor convergence and high computational overhead. Our empirical analysis suggests that convergence is poor due to gradient vanishing caused by the interaction between residual connection and layer normalization. We propose depth-scaled initialization (DS-Init), which decreases parameter variance at the initialization stage, and reduces output variance of residual connections so as to ease gradient back-propagation through normalization layers. To address computational cost, we propose a merged attention sublayer (MAtt) which combines a simplified average-based self-attention sublayer and the encoder-decoder attention sublayer on the decoder side. Results on WMT and IWSLT translation tasks with five translation directions show that deep Transformers with DS-Init and MAtt can substantially outperform their base counterpart in terms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the decoding speed of the baseline model thanks to the efficiency improvements of MAtt. Source code for reproduction will be released soon.",
    "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 898–909,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n898\nImproving Deep Transformer\nwith Depth-Scaled Initialization and Merged Attention\nBiao Zhang1 Ivan Titov1,2 Rico Sennrich3,1\n1School of Informatics, University of Edinburgh\n2ILLC, University of Amsterdam\n3Institute of Computational Linguistics, University of Zurich\nB.Zhang@ed.ac.uk, ititov@inf.ed.ac.uk, sennrich@cl.uzh.ch\nAbstract\nThe general trend in NLP is towards in-\ncreasing model capacity and performance via\ndeeper neural networks. However, simply\nstacking more layers of the popular Trans-\nformer architecture for machine translation re-\nsults in poor convergence and high computa-\ntional overhead. Our empirical analysis sug-\ngests that convergence is poor due to gradi-\nent vanishing caused by the interaction be-\ntween residual connections and layer normal-\nization. We propose depth-scaled initialization\n(DS-Init), which decreases parameter variance\nat the initialization stage, and reduces out-\nput variance of residual connections so as to\nease gradient back-propagation through nor-\nmalization layers. To address computational\ncost, we propose a merged attention sublayer\n(MAtt) which combines a simpliﬁed average-\nbased self-attention sublayer and the encoder-\ndecoder attention sublayer on the decoder side.\nResults on WMT and IWSLT translation tasks\nwith ﬁve translation directions show that deep\nTransformers with DS-Init and MAtt can sub-\nstantially outperform their base counterpart in\nterms of BLEU (+1.1 BLEU on average for\n12-layer models), while matching the decod-\ning speed of the baseline model thanks to the\nefﬁciency improvements of MAtt.1\n1 Introduction\nThe capability of deep neural models of handling\ncomplex dependencies has beneﬁted various ar-\ntiﬁcial intelligence tasks, such as image recogni-\ntion where test error was reduced by scaling VGG\nnets (Simonyan and Zisserman, 2015) up to hun-\ndreds of convolutional layers (He et al., 2015). In\nNLP, deep self-attention networks have enabled\nlarge-scale pretrained language models such as\nBERT (Devlin et al., 2019) and GPT (Radford\n1Source code for reproduction is available at https://\ngithub.com/bzhangGo/zero\n0\n1\n2\n3\n4\n5\n6Gradient Norm\n(a) Encoder\nTransformer 6L\nTransformer 12L\nTransformer 18L\nTransformer+DS-Init 6L\nTransformer+DS-Init 12L\nTransformer+DS-Init 18L\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\nLayer Depth\n0\n1\n2\n3\n4\n5\n6Gradient Norm\n(b) Decoder\nTransformer 6L\nTransformer 12L\nTransformer 18L\nTransformer+DS-Init 6L\nTransformer+DS-Init 12L\nTransformer+DS-Init 18L\nFigure 1: Gradient norm (y-axis) of each encoder layer (top)\nand decoder layer (bottom) in Transformer with respect to\nlayer depth (x-axis). Gradients are estimated with ∼3k target\ntokens at the beginning of training. “ DS-Init”: the proposed\ndepth-scaled initialization. “ 6L”: 6 layers. Solid lines indi-\ncate the vanilla Transformer, and dashed lines denote our pro-\nposed method. During back-propagation, gradients in Trans-\nformer gradually vanish from high layers to low layers.\net al., 2018) to boost state-of-the-art (SOTA) per-\nformance on downstream applications. By con-\ntrast, though neural machine translation (NMT)\ngained encouraging improvement when shifting\nfrom a shallow architecture (Bahdanau et al.,\n2015) to deeper ones (Zhou et al., 2016; Wu\net al., 2016; Zhang et al., 2018; Chen et al., 2018),\nthe Transformer (Vaswani et al., 2017), a cur-\nrently SOTA architecture, achieves best results\nwith merely 6 encoder and decoder layers, and no\ngains were reported by Vaswani et al. (2017) from\n899\nfurther increasing its depth on standard datasets.\nWe start by analysing why the Transformer does\nnot scale well to larger model depth. We ﬁnd that\nthe architecture suffers from gradient vanishing as\nshown in Figure 1, leading to poor convergence.\nAn in-depth analysis reveals that the Transformer\nis not norm-preserving due to the involvement of\nand the interaction between residual connection\n(RC) (He et al., 2015) and layer normalization\n(LN) (Ba et al., 2016).\nTo address this issue, we propose depth-scaled\ninitialization (DS-Init) to improve norm preserva-\ntion. We ascribe the gradient vanishing to the large\noutput variance of RC and resort to strategies that\ncould reduce it without model structure adjust-\nment. Concretely, DS-Init scales down the vari-\nance of parameters in thel-th layer with a discount\nfactor of 1√\nl at the initialization stage alone, where\nldenotes the layer depth starting from 1. The in-\ntuition is that parameters with small variance in\nupper layers would narrow the output variance of\ncorresponding RCs, improving norm preservation\nas shown by the dashed lines in Figure 1. In\nthis way, DS-Init enables the convergence of deep\nTransformer models to satisfactory local optima.\nAnother bottleneck for deep Transformers is\nthe increase in computational cost for both train-\ning and decoding. To combat this, we pro-\npose a merged attention network (MAtt). MAtt\nsimpliﬁes the decoder by replacing the separate\nself-attention and encoder-decoder attention sub-\nlayers with a new sublayer that combines an\nefﬁcient variant of average-based self-attention\n(AAN) (Zhang et al., 2018) and the encoder-\ndecoder attention. We simplify the AAN by re-\nducing the number of linear transformations, re-\nducing both the number of model parameters and\ncomputational cost. The merged sublayer beneﬁts\nfrom parallel calculation of (average-based) self-\nattention and encoder-decoder attention, and re-\nduces the depth of each decoder block.\nWe conduct extensive experiments on WMT\nand IWSLT translation tasks, covering ﬁve transla-\ntion tasks with varying data conditions and trans-\nlation directions. Our results show that deep\nTransformers with DS-Init and MAtt can substan-\ntially outperform their base counterpart in terms of\nBLEU (+1.1 BLEU on average for 12-layer mod-\nels), while matching the decoding speed of the\nbaseline model thanks to the efﬁciency improve-\nments of MAtt.\nOur contributions are summarized as follows:\n•We analyze the vanishing gradient issue in\nthe Transformer, and identify the interaction\nof residual connections and layer normaliza-\ntion as its source.\n•To address this problem, we introduce depth-\nscaled initialization (DS-Init).\n•To reduce the computational cost of training\ndeep Transformers, we introduce a merged\nattention model (MAtt). MAtt combines a\nsimpliﬁed average-attention model and the\nencoder-decoder attention into a single sub-\nlayer, allowing for parallel computation.\n•We conduct extensive experiments and ver-\nify that deep Transformers with DS-Init and\nMAtt improve translation quality while pre-\nserving decoding efﬁciency.\n2 Related Work\nOur work aims at improving translation quality\nby increasing model depth. Compared with the\nsingle-layer NMT system (Bahdanau et al., 2015),\ndeep NMT models are typically more capable of\nhandling complex language variations and trans-\nlation relationships via stacking multiple encoder\nand decoder layers (Zhou et al., 2016; Wu et al.,\n2016; Britz et al., 2017; Chen et al., 2018), and/or\nmultiple attention layers (Zhang et al., 2018). One\ncommon problem for the training of deep neural\nmodels are vanishing or exploding gradients. Ex-\nisting methods mainly focus on developing novel\nnetwork architectures so as to stabilize gradient\nback-propagation, such as the fast-forward con-\nnection (Zhou et al., 2016), the linear associa-\ntive unit (Wang et al., 2017), or gated recurrent\nnetwork variants (Hochreiter and Schmidhuber,\n1997; Gers and Schmidhuber, 2001; Cho et al.,\n2014; Di Gangi and Federico, 2018). In contrast to\nthe above recurrent network based NMT models,\nrecent work focuses on feed-forward alternatives\nwith more smooth gradient ﬂow, such as convo-\nlutional networks (Gehring et al., 2017) and self-\nattention networks (Vaswani et al., 2017).\nThe Transformer represents the current SOTA\nin NMT. It heavily relies on the combination of\nresidual connections (He et al., 2015) and layer\nnormalization (Ba et al., 2016) for convergence.\nNevertheless, simply extending this model with\nmore layers results in gradient vanishing due to\nthe interaction of RC and LN (see Section 4). Re-\ncent work has proposed methods to train deeper\n900\nTransformer models, including a rescheduling of\nRC and LN (Vaswani et al., 2018), the transpar-\nent attention model (Bapna et al., 2018) and the\nstochastic residual connection (Pham et al., 2019).\nIn contrast to these work, we identify the large\noutput variance of RC as the source of gradient\nvanishing, and employ scaled initialization to mit-\nigate it without any structure adjustment. The ef-\nfect of careful initialization on boosting conver-\ngence was also investigated and veriﬁed in previ-\nous work (Zhang et al., 2019; Child et al., 2019;\nDevlin et al., 2019; Radford et al., 2018).\nThe merged attention network falls into the cat-\negory of simplifying the Transformer so as to\nshorten training and/or decoding time. Methods\nto improve the Transformer’s running efﬁciency\nrange from algorithmic improvements (Junczys-\nDowmunt et al., 2018), non-autoregressive trans-\nlation (Gu et al., 2018; Ghazvininejad et al., 2019)\nto decoding dependency reduction such as aver-\nage attention network (Zhang et al., 2018) and\nblockwise parallel decoding (Stern et al., 2018).\nOur MAtt builds upon the AAN model, further\nsimplifying the model by reducing the number\nof linear transformations, and combining it with\nthe encoder-decoder attention. In work concur-\nrent to ours, So et al. (2019) propose the evolved\nTransformer which, based on automatic architec-\nture search, also discovered a parallel structure of\nself-attention and encoder-decoder attention.\n3 Background: Transformer\nGiven a source sequence X = {x1,x2,...,x n}∈\nRn×d, the Transformer predicts a target sequence\nY = {y1,y2,...,y m}under the encoder-decoder\nframework. Both the encoder and the decoder in\nthe Transformer are composed of attention net-\nworks, functioning as follows:\nATT(Zx,Zy) =\n[\nsoftmax(QKT\n√\nd\n)V\n]\nWo\nQ,K,V = ZxWq,ZyWk,ZyWv,\n(1)\nwhere Zx ∈RI×d and Zy ∈RJ×d are input se-\nquence representations of length I and J respec-\ntively, W∗ ∈ Rd×d denote weight parameters.\nThe attention network can be further enhanced\nwith multi-head attention (Vaswani et al., 2017).\nFormally, the encoder stacks L identical lay-\ners, each including a self-attention sublayer (Eq. 2)\nand a point-wise feed-forward sublayer (Eq. 3):\n¯Hl = LN\n(\nRC\n(\nHl−1,ATT(Hl−1,Hl−1)\n))\n(2)\nHl = LN\n(\nRC\n(\n¯Hl,FFN( ¯Hl)\n))\n. (3)\nHl ∈Rn×d denotes the sequence representation\nof the l-th encoder layer. Input to the ﬁrst layerH0\nis the element-wise addition of the source word\nembedding X and the corresponding positional\nencoding. F FN(·) is a two-layer feed-forward net-\nwork with a large intermediate representation and\nReLU activation function. Each encoder sublayer\nis wrapped with a residual connection (Eq. 4), fol-\nlowed by layer normalization (Eq. 5):\nRC(z,z′) =z + z′, (4)\nLN(z) =z −µ\nσ ⊙g + b, (5)\nwhere z and z′are input vectors, and ⊙indicates\nelement-wise multiplication. µ and σ denote the\nmean and standard deviation statistics of vector z.\nThe normalized z is then re-scaled and re-centered\nby trainable parameters g and b individually.\nThe decoder also consists of Lidentical layers,\neach of them extends the encoder sublayers with\nan encoder-decoder attention sublayer (Eq. 7) to\ncapture translation alignment from target words to\nrelevant source words:\n˜Sl = LN\n(\nRC\n(\nSl−1,ATT(Sl−1,Sl−1)\n))\n(6)\n¯Sl = LN\n(\nRC\n(\n˜Sl,ATT(˜Sl,HL)\n))\n(7)\nSl = LN\n(\nRC\n(\n¯Sl,FFN(¯Sl)\n))\n. (8)\nSl ∈Rm×d is the sequence representation of thel-\nth decoder layer. InputS0 is deﬁned similar toH0.\nTo ensure auto-regressive decoding, the attention\nweights in Eq. 6 are masked to prevent attention\nto future target tokens.\nThe Transformer’s parameters are typically ini-\ntialized by sampling from a uniform distribution:\nW ∈Rdi×do ∼U (−γ,γ) ,γ =\n√\n6\ndi + do\n, (9)\nwhere di and do indicate input and output dimen-\nsion separately. This initialization has the advan-\ntage of maintaining activation variances and back-\npropagated gradients variance and can help train\ndeep neural networks (Glorot and Bengio, 2010).\n901\n4 Vanishing Gradient Analysis\nOne natural way to deepen Transformer is simply\nenlarging the layer number L. Unfortunately, Fig-\nure 1 shows that this would give rise to gradient\nvanishing on both the encoder and the decoder at\nthe lower layers, and that the case on the decoder\nside is worse. We identiﬁed a structural problem\nin the Transformer architecture that gives rise to\nthis issue, namely the interaction of RC and LN,\nwhich we will here discuss in more detail.\nGiven an input vector z ∈Rd, let us consider\nthe general structure of RC followed by LN:\nr = RC (z,f(z)) , (10)\no = LN (r) , (11)\nwhere r,o ∈ Rd are intermediate outputs. f(·)\nrepresents any neural network, such as recurrent,\nconvolutional or attention network, etc. Suppose\nduring back-propagation, the error signal at the\noutput of LN is δo. Contributions of RC and LN to\nthe error signal are as follows:\nδr = ∂o\n∂r δo = diag( g\nσr\n)(I −1 −¯ r¯ rT\nd )δo (12)\nδz = ∂r\n∂zδr = (1 +∂f\n∂z)δr, (13)\nwhere ¯ rdenotes the normalized input. I is the\nidentity matrix and diag (·) establishes a diagonal\nmatrix from its input. The resulting δr and δz are\nerror signals arrived at outputr and z respectively.\nWe deﬁne the change of error signal as follows:\nβ = βLN ·βRC = ∥δz∥2\n∥δr∥2\n·∥δr∥2\n∥δo∥2\n, (14)\nwhere β (or model ratio), βLN (or LN ratio) and\nβRC (or RC ratio) measure the gradient norm ratio2\nof the whole residual block, the layer normaliza-\ntion and the residual connection respectively. In-\nformally, a neural model should preserve the gra-\ndient norm between layers ( β ≈ 1) so as to al-\nlow training of very deep models (see Zaeemzadeh\net al., 2018).\nWe resort to empirical evidence to analyze these\nratios. Results in Table 1 show that LN weak-\nens error signal ( βLN < 1) but RC strengthens it\n(βRC >1). One explanation about LN’s decay ef-\nfect is the large output variance of RC (Var (r) >\n2Model gradients depend on both error signal and layer\nactivation. Reduced/enhanced error signal does not necessar-\nily result in gradient vanishing/explosion, but strongly con-\ntributes to it.\nMethod Module Self Cross FFN\nBase\nEnc\nβLN 0.86 - 0.84\nβRC 1.22 - 1.10\nβ 1.05 - 0.93\nVar(r) 1.38 - 1.40\nDec\nβLN 0.82 0.74 0.84\nβRC 1.21 1.00 1.11\nβ 0.98 0.74 0.93\nVar(r) 1.48 1.84 1.39\nOurs\nEnc\nβLN 0.96 - 0.95\nβRC 1.04 - 1.02\nβ 1.02 - 0.98\nVar(r) 1.10 - 1.10\nDec\nβLN 0.95 0.94 0.94\nβRC 1.05 1.00 1.02\nβ 1.10 0.95 0.98\nVar(r) 1.13 1.15 1.11\nTable 1: Empirical measure of output variance Var (r) of\nRC and error signal change ratio βLN , βRC and β (Eq. 14)\naveraged over 12 layers. These values are estimated with\n∼3k target tokens at the beginning of training using 12-layer\nTransformer. “ Base”: the baseline Transformer. “ Ours”:\nthe Transformer with DS-Init. Enc and Dec stand for en-\ncoder and decoder respectively. Self, Cross and FFN in-\ndicate the self-attention, encoder-decoder attention and the\nfeed-forward sublayer respectively.\n1) which negatively affects δr as shown in Eq. 12.\nBy contrast, the short-cut in RC ensures that the\nerror signal at higher layer δr can always be safely\ncarried on to lower layer no matter how complex\n∂f\n∂z would be as in Eq. 13, increasing the ratio.\n5 Depth-Scaled Initialization\nResults on the model ratio show that self-attention\nsublayer has a (near) increasing effect ( β > 1)\nthat intensiﬁes error signal, while feed-forward\nsublayer manifests a decreasing effect ( β < 1).\nIn particular, though the encoder-decoder atten-\ntion sublayer and the self-attention sublayer share\nthe same attention formulation, the model ratio of\nthe former is smaller. As shown in Eq. 7 and 1,\npart of the reason is that encoder-decoder attention\ncan only back-propagate gradients to lower lay-\ners through the query representation Q, bypassing\ngradients at the key K and the value V to the en-\ncoder side. This negative effect explains why the\ndecoder suffers from more severe gradient vanish-\ning than the encoder in Figure 1.\nThe gradient norm is preserved better through\nthe self-attention layer than the encoder-decoder\nattention, which offers insights on the successful\ntraining of the deep Transformer in BERT (De-\nvlin et al., 2019) and GPT (Radford et al., 2018),\nwhere encoder-decoder attention is not involved.\nHowever, results in Table 1 also suggests that the\nself-attention sublayer in the encoder is not strong\n902\nLinear Linear Linear\nLinear\nScale & Mask\nMatMul\nSoftMax\nMatMul\ntarget\nQ K V\n(a) Self-Attention\nMatMul\nLinear\ntarget\nAvg Mask\nLinear\nRelu\nGate Layer (b) AAN\nLinearLinear\nLinearScale & MaskSoftMaxMatMultargetQKVsourceAvg MaskMatMulMatMulVLinearLinear (c) Merged attention with simpliﬁed AAN\nFigure 2: An overview of self-attention, AAN and the proposed merged attention with simpliﬁed AAN.\nenough to counteract the gradient loss in the feed-\nforward sublayer. That is why BERT and GPT\nadopt a much smaller standard deviation (0.02) for\ninitialization, in a similar spirit to our solution.\nWe attribute the gradient vanishing issue to the\nlarge output variance of RC (Eq. 12). Consid-\nering that activation variance is positively corre-\nlated with parameter variance (Glorot and Bengio,\n2010), we propose DS-Init and change the original\ninitialization method in Eq. 9 as follows:\nW ∈Rdi×do ∼U\n(\n−γ α√\nl\n,γ α√\nl\n)\n, (15)\nwhere αis a hyperparameter in the range of [0,1]\nand ldenotes layer depth. Hyperparameter αim-\nproves the ﬂexibility of our method. Compared\nwith existing approaches (Vaswani et al., 2018;\nBapna et al., 2018), our solution does not require\nmodiﬁcations in the model architecture and hence\nis easy to implement.\nAccording to the property of uniform distribu-\ntion, the variance of model parameters decreases\nfrom γ2\n3 to γ2α2\n3l after applying DS-Init. By doing\nso, a higher layer would have smaller output vari-\nance of RC so that more gradients can ﬂow back.\nResults in Table 1 suggest that DS-Init narrows\nboth the variance and different ratios to be∼1, en-\nsuring the stability of gradient back-propagation.\nEvidence in Figure 1 also shows that DS-Init helps\nkeep the gradient norm and slightly increases it on\nthe encoder side. This is because DS-Init endows\nlower layers with parameters of larger variance\nand activations of larger norm. When error signals\nat different layers are of similar scale, the gradi-\nent norm at lower layers would be larger. Never-\ntheless, this increase does not hurt model training\nbased on our empirical observation.\nDS-Init is partially inspired by the Fixup ini-\ntialization (Zhang et al., 2019). Both of them try\nto reduce the output variance of RC. The differ-\nence is that Fixup focuses on overcoming gradi-\nent explosion cased by consecutive RCs and seeks\nto enable training without LN but at the cost of\ncarefully handling parameter initialization of each\nmatrix transformation, including manipulating ini-\ntialization of different bias and scale terms. In-\nstead, DS-Init aims at solving gradient vanishing\nin deep Transformer caused by the structure of RC\nfollowed by LN. We still employ LN to standard-\nize layer activation and improve model conver-\ngence. The inclusion of LN ensures the stability\nand simplicity of DS-Init.\n6 Merged Attention Model\nWith large model depth, deep Transformer un-\navoidably introduces high computational over-\nhead. This brings about signiﬁcantly longer train-\ning and decoding time. To remedy this issue,\nwe propose a merged attention model for decoder\nthat integrates a simpliﬁed average-based self-\nattention sublayer into the encoder-decoder atten-\ntion sublayer. Figure 2 highlights the difference.\nThe AAN model (Figure 2(b)), as an alternative\nto the self-attention model (Figure 2(a)), acceler-\nates Transformer decoding by allowing decoding\nin linear time, avoiding the O(n2) complexity of\nthe self-attention mechanism (Zhang et al., 2018).\nUnfortunately, the gating sublayer and the feed-\nforward sublayer inside AAN reduce the empiri-\ncal performance improvement. We propose a sim-\npliﬁed AAN by removing all matrix computation\nexcept for two linear projections:\nSAAN(Sl−1) =\n[\nMa(Sl−1Wv)\n]\nWo, (16)\n903\nDataset #Src #Tgt #Sent #BPE\nWMT14 En-De 116M 110M 4.5M 32K\nWMT14 En-Fr 1045M 1189M 36M 32K\nWMT18 En-Fi 73M 54M 3.3M 32K\nWMT18 Zh-En 510M 576M 25M 32K\nIWSLT14 De-En 3.0M 3.2M 159K 30K\nTable 2: Statistics for different training datasets. #Src and\n#Tgt denote the number of source and target tokens respec-\ntively. #Sent: the number of bilingual sentences. #BPE: the\nnumber of merge operations in BPE.M: million, K: thousand.\nwhere Ma denotes the average mask matrix for\nparallel computation (Zhang et al., 2018). This\nnew model is then combined with the encoder-\ndecoder attention as shown in Figure 2(c):\nMATT(Sl−1) =SAAN(Sl−1) +ATT(Sl−1,HL)\n¯Sl = LN\n(\nRC\n(\nSl−1,MATT(Sl−1)\n))\n. (17)\nThe mapping Wo is shared for SA AN and ATT.\nAfter combination, MAtt allows for the paral-\nlelization of AAN and encoder-decoder attention.\n7 Experiments\n7.1 Datasets and Evaluation\nWe take WMT14 English-German translation\n(En-De) (Bojar et al., 2014) as our bench-\nmark for model analysis, and examine the\ngeneralization of our approach on four other\ntasks: WMT14 English-French (En-Fr), IWSLT14\nGerman-English (De-En) (Cettolo et al., 2014),\nWMT18 English-Finnish (En-Fi) and WMT18\nChinese-English (Zh-En) (Bojar et al., 2018).\nByte pair encoding algorithm (BPE) (Sennrich\net al., 2016) is used in preprocessing to handle low\nfrequency words. Statistics of different datasets\nare listed in Table 2.\nExcept for IWSLT14 De-En task, we collect\nsubword units independently on the source and\ntarget side of training data. We directly use the\npreprocessed training data from the WMT18 web-\nsite3 for En-Fi and Zh-En tasks, and use new-\nstest2017 as our development set, newstest2018 as\nour test set. Our training data for WMT14 En-\nDe and WMT14 En-Fr is identical to previous se-\ntups (Vaswani et al., 2017; Wu et al., 2019). We\nuse newstest2013 as development set for WMT14\nEn-De and newstest2012+2013 for WMT14 En-\nFr. Apart from newstest2014 test set 4, we also\n3http://www.statmt.org/wmt18/translation-task.html\n4We use the ﬁltered test set consisting of 2737 sentence\npairs. The difference of translation quality on ﬁltered and full\ntest sets is marginal.\nevaluate our model on all WMT14-18 test sets\nfor WMT14 En-De translation. The settings for\nIWSLT14 De-En are as in Ranzato et al. (2016),\nwith 7584 sentence pairs for development, and the\nconcatenated dev sets for IWSLT 2014 as test set\n(tst2010, tst2011, tst2012, dev2010, dev2012).\nWe report tokenized case-sensitive BLEU (Pa-\npineni et al., 2002) for WMT14 En-De and\nWMT14 En-Fr, and provide detokenized case-\nsensitive BLEU for WMT14 En-De, WMT18 En-\nFi and Zh-En with sacreBLEU (Post, 2018)5. We\nalso report chrF score for En-Fi translation which\nwas found correlated better with human evalu-\nation (Bojar et al., 2018). Following previous\nwork (Wu et al., 2019), we evaluate IWSLT14 De-\nEn with tokenized case-insensitive BLEU.\n7.2 Model Settings\nWe experiment with both base (layer size\n512/2048, 8 heads) and big (layer size 1024/4096,\n16 heads) settings as in Vaswani et al. (2017). Ex-\ncept for the vanilla Transformer, we also compare\nwith the structure that is currently default in ten-\nsor2tensor (T2T), which puts layer normalization\nbefore residual blocks (Vaswani et al., 2018). We\nuse an in-house toolkit for all experiments.\nDropout is applied to the residual connection\n(dpr) and attention weights ( dpa). We share the\ntarget embedding matrix with the softmax projec-\ntion matrix but not with the source embedding ma-\ntrix. We train all models using Adam optimizer\n(0.9/0.98 for base, 0.9/0.998 for big) with adap-\ntive learning rate schedule (warm-up step 4K for\nbase, 16K for big) as in (Vaswani et al., 2017) and\nlabel smoothing of 0.1. We set α in DS-Init to\n1.0. Sentence pairs containing around 25K ∼50K\n(bs) target tokens are grouped into one batch. We\nuse relatively larger batch size and dropout rate\nfor deeper and bigger models for better conver-\ngence. We perform evaluation by averaging last\n5 checkpoints. Besides, we apply mixed-precision\ntraining to all big models. Unless otherwise stated,\nwe train base and big model with 300K maximum\nsteps, and decode sentences using beam search\nwith a beam size of 4 and length penalty of 0.6.\nDecoding is implemented with cache to save re-\ndundant computations. Other settings for speciﬁc\ntranslation tasks are explained in the individual\nsubsections.\n5Signature BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.2.20\n904\nID Model #Param Test14 △Dec △Train\n1 Base 6 layers dpa = dpr = 0.1,bs = 25K 72.3M 27.59 (26.9) 62.26/1.00× 0.105/1.00×\n2 1 + T2T 72.3M 27.20 (26.5) 68.04/0.92× 0.105/1.00×\n3 1 + DS-Init 72.3M 27.50 (26.8) ⋆/1.00× ⋆/1.00×\n4 1 + MAtt 66.0M 27.49 (26.8) 40.51/1.54× 0.094/1.12×\n5 1 + MAtt + DS-Init 66.0M 27.35 (26.8) 40.84/1.52× 0.094/1.12×\n6 1 + MAtt with self-attention 72.3M 27.41 (26.7) 60.25/1.03× 0.105/1.00×\n7 1 + MAtt with original AAN 72.2M 27.36 (26.7) 46.13/1.35× 0.098/1.07×\n8 1 + bs= 50K 72.3M 27.84 (27.2) ⋆/1.00× ⋆/1.00×\n9 1 + >12 layers + bs= 25K ∼50K - - - -\n10 4 + >12 layers + bs= 25K ∼50K - - -\n11 3 + 12 layers + bs= 40K,dpr = 0.3,dpa = 0.2 116.4M 28.27 (27.6) 102.9/1.00×† 0.188/1.00×†\n12 11 + T2T 116.5M 28.03 (27.4) 107.7/0.96×† 0.191/0.98×†\n13 11 + MAtt 103.8M 28.55 (27.9) 67.12/1.53×† 0.164 /1.15×†\n14 3 + 20 layers + bs= 44K,dpr = 0.3,dpa = 0.2 175.3M 28.42 (27.7) 157.8/1.00×‡ 0.283/1.00×‡\n15 14 + T2T 175.3M 28.27 (27.6) 161.2/0.98×‡ 0.289/0.98×‡\n16 14 + MAtt 154.3M 28.67 (28.0) 108.6/1.45×‡ 0.251/1.13×‡\nTable 3: Tokenized case-sensitive BLEU (in parentheses: sacreBLEU) on WMT14 En-De translation task.#Param: number of\nmodel parameters. △Dec: decoding time (seconds)/speedup on newstest2014 dataset with a batch size of 32. △Train: training\ntime (seconds)/speedup per training step evaluated on 0.5K steps with a batch size of 1K target tokens. Time is averaged over 3\nruns using Tensorﬂow on a single TITAN X (Pascal). “-”: optimization failed and no result. “⋆”: the same as model 1⃝. †and\n‡: comparison against 11⃝and 14⃝respectively rather than 1⃝. Base: the baseline Transformer with base setting. Bold indicates\nbest BLEU score. dpa and dpr: dropout rate on attention weights and residual connection. bs: batch size in tokens.\n7.3 WMT14 En-De Translation Task\nTable 3 summarizes translation results under dif-\nferent settings. Applying DS-Init and/or MAtt\nto Transformer with 6 layers slightly decreases\ntranslation quality by∼0.2 BLEU (27.59→27.35).\nHowever, they allow scaling up to deeper architec-\ntures, achieving a BLEU score of 28.55 (12 layers)\nand 28.67 (20 layers), outperforming all baselines.\nThese improvements can not be obtained via en-\nlarging the training batch size ( 8⃝), conﬁrming the\nstrength of deep models.\nWe also compare our simpliﬁed AAN in MAtt\n( 4⃝) with two variants: a self-attention network\n( 6⃝), and the original AAN ( 7⃝). Results show\nminor differences in translation quality, but im-\nprovements in training and decoding speed, and\na reduction in the number of model parameters.\nCompared to the baseline, MAtt improves decod-\ning speed by 50%, and training speed by 10%,\nwhile having 9% fewer parameters.\nResult 9⃝indicates that the gradient vanishing\nissue prevents training of deep vanilla Transform-\ners, which cannot be solved by only simplifying\nthe decoder via MAtt (10⃝). By contrast, both T2T\nand DS-Init can help. Our DS-Init improves norm\npreservation through speciﬁc parameter initializa-\ntion, while T2T reschedules the LN position. Re-\nsults in Table 3 show that T2T underperforms DS-\nInit by 0.2 BLEU on average, and slightly in-\ncreases training and decoding time (by 2%) com-\npared to the original Transformer due to additional\nID BLEU PPL\nTrain Dev Train Dev\n1 28.64 26.16 5.23 4.76\n11 29.63 26.44 4.48 4.38\n12 29.75 26.16 4.60 4.49\n13 29.43 26.51 5.09 4.71\n14 30.71 26.52 3.96 4.32\n15 30.89 26.53 4.09 4.41\n16 30.25 26.56 4.62 4.58\nTable 4: Tokenized case-sensitive BLEU (BLEU) and per-\nplexity (PPL) on training (Train) and development (new-\nstest2013, Dev) set. We randomly select 3K sentence pairs\nas our training data for evaluation. Lower PPL is better.\nLN layers. This suggests that our solution is more\neffective and efﬁcient.\nSurprisingly, training deep Transformers with\nboth DS-Init and MAtt improves not only run-\nning efﬁciency but also translation quality (by 0.2\nBLEU), compared with DS-Init alone. To get an\nimproved understanding, we analyze model per-\nformance on both training and development set.\nResults in Table 4 show that models with DS-Init\nyield the best perplexity on both training and de-\nvelopment set, and those with T2T achieve the\nbest BLEU on the training set. However, DS-\nInit+MAtt performs best in terms of BLEU on the\ndevelopment set. This indicates that the success of\nDS-Init+MAtt comes from its better generaliza-\ntion rather than better ﬁtting training data.\nWe also attempt to apply DS-Init on the encoder\nalone or the decoder alone for 12-layer models.\nUnfortunately, both variants lead to unstable op-\ntimization where gradients tend to explode at the\n905\nTask Model #Param BLEU △Dec △Train\nWMT14 En-Fr Base + 6 layers 76M 39.09 167.56/1.00× 0.171/1.00×\nOurs + Base + 12 layers 108M 40.58 173.62/0.97× 0.265/0.65×\nIWSLT14 De-En Base + 6 layers 61M 34.41 315.59/1.00× 0.153/1.00×\nOurs + Base + 12 layers 92M 35.63 329.95/0.96× 0.247/0.62×\nWMT18 En-Fi Base + 6 layers 65M 15.5 (50.82) 156.32/1.00× 0.165/1.00×\nOurs + Base + 12 layers 96M 15.8 (51.47) 161.74/0.97× 0.259/0.64×\nWMT18 Zh-En Base + 6 layers 77M 21.1 217.40/1.00× 0.173/1.00×\nOurs + Base + 12 layers 108M 22.3 228.57/0.95× 0.267/0.65×\nTable 5: Translation results on different tasks. Settings for BLEU score is given in Section 7.1. Numbers in bracket denote\nchrF score. Our model outperforms the vanilla base Transformer on all tasks. “Ours”: DS-Init+MAtt.\n10 15 20 25 30\nModel Depth\n27.50\n27.75\n28.00\n28.25\n28.50\n28.75\nBLEU Score\nFigure 3: Test BLEU score on newstest2014 with respect to\nmodel depth for Transformer+DS-Init+MAtt.\nModel #Param Test14 Test14-18\nVaswani et al. (2017) 213M 28.4 -\nChen et al. (2018) 379M 28.9 -\nOtt et al. (2018) 210M 29.3 -\nBapna et al. (2018) 137M 28.04 -\nWu et al. (2019) 213M 29.76\n(29.0)\n∗ 33.13\n(32.86)\n∗\nBig + 6 layers 233M 29.07\n(28.3)\n33.16\n(32.88)\nOurs + Big + 12 layers 359M 29.47\n(28.7)\n33.21\n(32.90)\nOurs + Big + 20 layers 560M 29.62\n(29.0)\n33.26\n(32.96)\nTable 6: Tokenized case-sensitive BLEU (sacreBLEU) on\nWMT14 En-De translation task. “ Test14-18”: BLEU score\naveraged over newstest2014 ∼newstest2018. ∗: results ob-\ntained by running code and model released by Wu et al.\n(2019). “Ours”: DS-Init+MAtt.\nmiddle of training. We attempt to solve this issue\nwith gradient clipping of rate 1.0. Results show\nthat this fails for decoder and achieves only 27.89\nBLEU for encoder, losing 0.66 BLEU compared\nwith the full variant (28.55). We leave further\nanalysis to future work and recommend using DS-\nInit on both the encoder and the decoder.\nEffect of Model Depth We empirically com-\npare a wider range of model depths for\nTransformer+DS-Init+MAtt with up to 30 layers.\nHyperparameters are the same as for14⃝except that\nwe use 42K and 48K batch size for 18 and 30 lay-\ners respectively. Figure 3 shows that deeper Trans-\nformers yield better performance. However, im-\nprovements are steepest going from 6 to 12 layers,\nand further improvements are small.\n7.3.1 Comparison with Existing Work\nTable 6 lists the results in big setting and compares\nwith current SOTA. Big models are trained with\ndpa = 0.1 and dpr = 0.3. The 6-layer baseline\nand the deeper ones are trained with batch size of\n48K and 54K respectively. Deep Transformer with\nour method outperforms its 6-layer counterpart\nby over 0.4 points on newstest2014 and around\n0.1 point on newstest2014 ∼newstest2018. Our\nmodel outperforms the transparent model (Bapna\net al., 2018) (+1.58 BLEU), an approach for\nthe deep encoder. Our model performs on par\nwith current SOTA, the dynamic convolution\nmodel (DCNN) (Wu et al., 2019). In particular,\nthough DCNN achieves encouraging performance\non newstest2014, it falls behind the baseline on\nother test sets. By contrast, our model obtains\nmore consistent performance improvements.\nIn work concurrent to ours, Wang et al. (2019)\ndiscuss how the placement of layer normalization\naffects deep Transformers, and compare the orig-\ninal post-norm (which we consider our baseline)\nand a pre-norm layout (which we call T2T). Their\nresults also show that pre-norm allows training of\ndeeper Transformers. Our results show that deep\npost-norm Transformers are also trainable with ap-\npropriate initialization, and tend to give slightly\nbetter results.\n7.4 Results on Other Translation Tasks\nWe use 12 layers for our model in these tasks. We\nenlarge the dropout rate to dpa = 0.3,dpr = 0.5\nfor IWSLT14 De-En task and train models on\nWMT14 En-Fr and WMT18 Zh-En with 500K\nsteps. Other models are trained with the same set-\ntings as in WMT14 En-De.\nWe report translation results on other tasks in\nTable 5. Results show that our model beats the\nbaseline on all tasks with gains of over 1 BLEU,\n906\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nGradient Norm\n(a) Encoder\nTransformer L1\nTransformer L18\nTransformer + DS-Init L1\nTransformer + DS-Init L18\n0 10 20 30 40 50 60 70 80 90\nTraining Steps (x50)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nGradient Norm\n(b) Decoder\nTransformer L1\nTransformer L18\nTransformer + DS-Init L1\nTransformer + DS-Init L18\nFigure 4: Gradient norm (y-axis) of the ﬁrst and the last\nencoder layers (top) and decoder layers (bottom) in 18-layer\ndeep Transformer over the ﬁst 5k training steps. We use\naround 25k source/target tokens in each training batch. Each\npoint in this plot is averaged over 50 training steps. “L1/L18”\ndenotes the ﬁrst/last layer. DS-Init helps stabilize the gradient\nnorm during training.\nexcept the WMT18 En-Fi where our model yields\nmarginal BLEU improvements (+0.3 BLEU). We\nargue that this is due to the rich morphology of\nFinnish, and BLEU’s inability to measure im-\nprovements below the word level. We also provide\nthe chrF score in which our model gains 0.6 points.\nIn addition, speed measures show that though our\nmodel consumes 50+% more training time, there\nis only a small difference with respect to decoding\ntime thanks to MAtt.\n7.5 Analysis of Training Dynamics\nOur analysis in Figure 1 and Table 1 is based\non gradients estimated exactly after parameter ini-\ntialization without considering training dynam-\nics. Optimizers with adaptive step rules, such as\nAdam, could have an adverse effect that enables\ngradient scale correction through the accumulated\nﬁrst and second moments. However, results in Fig-\nure 4 show that without DS-Init, the encoder gra-\ndients are less stable and the decoder gradients still\nsuffer from the vanishing issue, particularly at the\nﬁrst layer. DS-Init makes the training more stable\nand robust.6\n8 Conclusion and Future Work\nThis paper discusses training of very deep Trans-\nformers. We show that the training of deep Trans-\nformers suffers from gradient vanishing, which we\nmitigate with depth-scaled initialization. To im-\nprove training and decoding efﬁciency, we pro-\npose a merged attention sublayer that integrates\na simpliﬁed average-based self-attention sublayer\ninto the encoder-decoder attention sublayer. Ex-\nperimental results show that deep models trained\nwith these techniques clearly outperform a vanilla\nTransformer with 6 layers in terms of BLEU, and\noutperforms other solutions to train deep Trans-\nformers (Bapna et al., 2018; Vaswani et al., 2018).\nThanks to the more efﬁcient merged attention\nsublayer, we achieve these quality improvements\nwhile matching the decoding speed of the baseline\nmodel.\nIn the future, we would like to extend our\nmodel to other sequence-to-sequence tasks, such\nas summarization and dialogue generation, as well\nas adapt the idea to other generative architec-\ntures (Zhang et al., 2016, 2018). We have trained\nmodels with up to 30 layers each for the encoder\nand decoder, and while training was successful\nand improved over shallower counterparts, gains\nare relatively small beyond 12 layers. An open\nquestion is whether there are other structural is-\nsues that limit the beneﬁts of increasing the depth\nof the Transformer architecture, or whether the\nbeneﬁt of very deep models is greater for other\ntasks and dataset.\nAcknowledgments\nWe thank the reviewers for their insightful com-\nments. This project has received funding from the\ngrant H2020-ICT-2018-2-825460 (ELITR) by the\nEuropean Union. Biao Zhang also acknowledges\nthe support of the Baidu Scholarship. This work\nhas been performed using resources provided by\nthe Cambridge Tier-2 system operated by the Uni-\nversity of Cambridge Research Computing Ser-\nvice (http://www.hpc.cam.ac.uk) funded by EP-\nSRC Tier-2 capital grant EP/P020259/1.\n6We observe this both in the raw gradients and after taking\nthe Adam step rules into account.\n907\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural Machine Translation by Jointly\nLearning to Align and Translate. In Proceedings of\nthe International Conference on Learning Represen-\ntations (ICLR).\nAnkur Bapna, Mia Chen, Orhan Firat, Yuan Cao, and\nYonghui Wu. 2018. Training deeper neural ma-\nchine translation models with transparent attention.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 3028–3033, Brussels, Belgium. Association\nfor Computational Linguistics.\nOndrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve\nSaint-Amand, Radu Soricut, Lucia Specia, and Aleˇs\nTamchyna. 2014. Findings of the 2014 workshop on\nstatistical machine translation. In Proceedings of the\nNinth Workshop on Statistical Machine Translation,\npages 12–58, Baltimore, Maryland, USA. Associa-\ntion for Computational Linguistics.\nOndˇrej Bojar, Christian Federmann, Mark Fishel,\nYvette Graham, Barry Haddow, Philipp Koehn, and\nChristof Monz. 2018. Findings of the 2018 con-\nference on machine translation (WMT18). In Pro-\nceedings of the Third Conference on Machine Trans-\nlation: Shared Task Papers , pages 272–303, Bel-\ngium, Brussels. Association for Computational Lin-\nguistics.\nDenny Britz, Anna Goldie, Minh-Thang Luong, and\nQuoc Le. 2017. Massive exploration of neural\nmachine translation architectures. In Proceedings\nof the 2017 Conference on Empirical Methods in\nNatural Language Processing , pages 1442–1451,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nMauro Cettolo, Jan Niehues, Sebastian St ¨uker, Luisa\nBentivogli, and Marcello Federico. 2014. Report\non the 11th IWSLT Evaluation Campaign, IWSLT\n2014. In Proceedings of the 11th Workshop on Spo-\nken Language Translation, pages 2–16, Lake Tahoe,\nCA, USA.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Mike Schuster, Noam Shazeer, Niki Parmar,\nAshish Vaswani, Jakob Uszkoreit, Lukasz Kaiser,\nZhifeng Chen, Yonghui Wu, and Macduff Hughes.\n2018. The best of both worlds: Combining recent\nadvances in neural machine translation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 76–86. Association for Computational\nLinguistics.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers. CoRR, abs/1904.10509.\nKyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learn-\ning Phrase Representations using RNN Encoder–\nDecoder for Statistical Machine Translation. In Pro-\nceedings of the 2014 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) ,\npages 1724–1734, Doha, Qatar.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMattia Antonino Di Gangi and Marcello Federico.\n2018. Deep neural machine translation with weakly-\nrecurrent units. In Proceedings of EAMT, Alicante,\nSpain.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning, volume 70 of Proceedings of Machine\nLearning Research, pages 1243–1252, International\nConvention Centre, Sydney, Australia. PMLR.\nFelix A. Gers and J ¨urgen Schmidhuber. 2001. Long\nShort-Term Memory Learns Context Free and Con-\ntext Sensitive Languages. In Proceedings of the\nICANNGA 2001 Conference, volume 1, pages 134–\n137.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke S. Zettlemoyer. 2019. Constant-time machine\ntranslation with conditional masked language mod-\nels. CoRR, abs/1904.09324.\nXavier Glorot and Y Bengio. 2010. Understanding the\ndifﬁculty of training deep feedforward neural net-\nworks. Journal of Machine Learning Research -\nProceedings Track, 9:249–256.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O.K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In Inter-\nnational Conference on Learning Representations.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Deep residual learning for image recog-\nnition. CoRR, abs/1512.03385.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nShort-Term Memory. Neural Comput., 9(8):1735–\n1780.\n908\nMarcin Junczys-Dowmunt, Kenneth Heaﬁeld, Hieu\nHoang, Roman Grundkiewicz, and Anthony Aue.\n2018. Marian: Cost-effective high-quality neural\nmachine translation in C++. In Proceedings of the\n2nd Workshop on Neural Machine Translation and\nGeneration, Melbourne, Australia.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. arXiv preprint arXiv:1806.00187.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting on Association for Com-\nputational Linguistics , ACL ’02, pages 311–318,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nNgoc-Quan Pham, Thai-Son Nguyen, Jan Niehues,\nMarkus M ¨uller, and Alexander H. Waibel. 2019.\nVery deep self-attention networks for end-to-end\nspeech recognition. CoRR, abs/1904.13377.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2016. Sequence Level\nTraining with Recurrent Neural Networks. In The\nInternational Conference on Learning Representa-\ntions.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nK. Simonyan and A. Zisserman. 2015. Very deep con-\nvolutional networks for large-scale image recogni-\ntion. In International Conference on Learning Rep-\nresentations.\nDavid R So, Chen Liang, and Quoc V Le.\n2019. The evolved transformer. arXiv preprint\narXiv:1901.11117.\nMitchell Stern, Noam Shazeer, and Jakob Uszkoreit.\n2018. Blockwise parallel decoding for deep autore-\ngressive models. In Advances in Neural Information\nProcessing Systems, pages 10086–10095.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan Gomez, Stephan Gouws, Llion\nJones, Łukasz Kaiser, Nal Kalchbrenner, Niki Par-\nmar, Ryan Sepassi, Noam Shazeer, and Jakob\nUszkoreit. 2018. Tensor2Tensor for neural machine\ntranslation. In Proceedings of the 13th Conference\nof the Association for Machine Translation in the\nAmericas (Volume 1: Research Papers), pages 193–\n199, Boston, MA. Association for Machine Transla-\ntion in the Americas.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30 , pages 5998–6008. Curran As-\nsociates, Inc.\nMingxuan Wang, Zhengdong Lu, Jie Zhou, and Qun\nLiu. 2017. Deep neural machine translation with lin-\near associative unit. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 136–\n145, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1810–1822, Florence, Italy. Associa-\ntion for Computational Linguistics.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. 2019. Pay less attention with\nlightweight and dynamic convolutions. In Interna-\ntional Conference on Learning Representations.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin\nJohnson, Xiaobing Liu, ukasz Kaiser, Stephan\nGouws, Yoshikiyo Kato, Taku Kudo, Hideto\nKazawa, Keith Stevens, George Kurian, Nishant\nPatil, Wei Wang, Cliff Young, Jason Smith, Jason\nRiesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2016. Google’s\nneural machine translation system: Bridging the gap\nbetween human and machine translation. CoRR,\nabs/1609.08144.\nAlireza Zaeemzadeh, Nazanin Rahnavard, and\nMubarak Shah. 2018. Norm-preservation: Why\nresidual networks can become extremely deep?\nCoRR, abs/1805.07477.\nBiao Zhang, Deyi Xiong, and Jinsong Su. 2018. Accel-\nerating neural transformer via an average attention\nnetwork. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1789–1798, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nBiao Zhang, Deyi Xiong, and Jinsong Su. 2018. Neu-\nral machine translation with deep attention. IEEE\n909\nTransactions on Pattern Analysis and Machine In-\ntelligence, pages 1–1.\nBiao Zhang, Deyi Xiong, Jinsong Su, Hong Duan, and\nMin Zhang. 2016. Variational neural machine trans-\nlation. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 521–530, Austin, Texas. Association for\nComputational Linguistics.\nHongyi Zhang, Yann N. Dauphin, and Tengyu Ma.\n2019. Fixup initialization: Residual learning with-\nout normalization via better initialization. In Inter-\nnational Conference on Learning Representations.\nXiangwen Zhang, Jinsong Su, Yue Qin, Yang Liu, Ron-\ngrong Ji, and Hongji Wang. 2018. Asynchronous\nbidirectional decoding for neural machine transla-\ntion. In Thirty-Second AAAI Conference on Artiﬁ-\ncial Intelligence.\nJie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei\nXu. 2016. Deep recurrent models with fast-forward\nconnections for neural machine translation. Trans-\nactions of the Association for Computational Lin-\nguistics, 4:371–383."
}