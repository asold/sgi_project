{
  "title": "Referring Transformer: A One-step Approach to Multi-task Visual Grounding",
  "url": "https://openalex.org/W3171547673",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2671174060",
      "name": "Li Muchen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2620566726",
      "name": "Sigal Leonid",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2989604896",
    "https://openalex.org/W2983358816",
    "https://openalex.org/W2963914122",
    "https://openalex.org/W3034772468",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2302548814",
    "https://openalex.org/W2876852810",
    "https://openalex.org/W2946086442",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3156978626",
    "https://openalex.org/W2963109634",
    "https://openalex.org/W2963783181",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3035097537",
    "https://openalex.org/W2251512949",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W1773149199",
    "https://openalex.org/W2986803748",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2952524542",
    "https://openalex.org/W2964276121",
    "https://openalex.org/W3034727271",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3038476992",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3093025045",
    "https://openalex.org/W2964022527",
    "https://openalex.org/W2558535589",
    "https://openalex.org/W2606473278",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2796347433",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3020827971",
    "https://openalex.org/W3110435696",
    "https://openalex.org/W3159619744",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3169150690",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2980088508",
    "https://openalex.org/W3034692043",
    "https://openalex.org/W2489434015",
    "https://openalex.org/W3107331169",
    "https://openalex.org/W2984121207",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3203974803",
    "https://openalex.org/W2987734933",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W2964345792",
    "https://openalex.org/W3034325957",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W2964284374",
    "https://openalex.org/W3163747765",
    "https://openalex.org/W2904164128",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W639708223"
  ],
  "abstract": "As an important step towards visual reasoning, visual grounding (e.g., phrase localization, referring expression comprehension/segmentation) has been widely explored Previous approaches to referring expression comprehension (REC) or segmentation (RES) either suffer from limited performance, due to a two-stage setup, or require the designing of complex task-specific one-stage architectures. In this paper, we propose a simple one-stage multi-task framework for visual grounding tasks. Specifically, we leverage a transformer architecture, where two modalities are fused in a visual-lingual encoder. In the decoder, the model learns to generate contextualized lingual queries which are then decoded and used to directly regress the bounding box and produce a segmentation mask for the corresponding referred regions. With this simple but highly contextualized model, we outperform state-of-the-arts methods by a large margin on both REC and RES tasks. We also show that a simple pre-training schedule (on an external dataset) further improves the performance. Extensive experiments and ablations illustrate that our model benefits greatly from contextualized information and multi-task training.",
  "full_text": "Referring Transformer: A One-step Approach to\nMulti-task Visual Grounding\nMuchen Li1,2 Leonid Sigal1,2,3,4\nmuchenli@cs.ubc.ca lsigal@cs.ubc.ca\n1Department of Computer Science, University of British Columbia\n2Vector Institute for AI 3CIFAR AI Chair 4NSERC CRC Chair\nAbstract\nAs an important step towards visual reasoning, visual grounding (e.g., phrase\nlocalization, referring expression comprehension / segmentation) has been widely\nexplored. Previous approaches to referring expression comprehension (REC) or\nsegmentation (RES) either suffer from limited performance, due to a two-stage\nsetup, or require the designing of complex task-speciﬁc one-stage architectures.\nIn this paper, we propose a simple one-stage multi-task framework for visual\ngrounding tasks. Speciﬁcally, we leverage a transformer architecture, where two\nmodalities are fused in a visual-lingual encoder. In the decoder, the model learns to\ngenerate contextualized lingual queries which are then decoded and used to directly\nregress the bounding box and produce a segmentation mask for the corresponding\nreferred regions. With this simple but highly contextualized model, we outperform\nstate-of-the-art methods by a large margin on both REC and RES tasks. We also\nshow that a simple pre-training schedule (on an external dataset) further improves\nthe performance. Extensive experiments and ablations illustrate that our model\nbeneﬁts greatly from contextualized information and multi-task training.\n1 Introduction\nMulti-modal grounding1 tasks (e.g., phrase localization [1, 3, 9, 41, 48], referring expression compre-\nhension [17, 19, 24, 26, 29, 30, 37, 50, 51, 54, 55] and segmentation [6, 18, 20, 21, 29, 38, 52, 55])\naim to generalize traditional object detection and segmentation to localization of regions (rectangular\nor at a pixel level) in images that correspond to free-form linguistic expressions. These tasks have\nemerged as core problems in vision and ML due to the breadth of applications that can make use of\nsuch techniques, spanning image captioning, visual question answering, visual reasoning and others.\nThe majority of multi-modal grounding architectures, to date, take the form of two-stage approaches,\ninspired by Faster RCNN [44] and others, which ﬁrst generate a set of image region proposals and\nthen associate/ground one, or more, of these regions to a phrase by considering how well the content\nmatches the query phrase. Context among the regions and multiple query phrases, which often come\nparsed from a single sentence, has also been considered in various ways (e.g., using LSTM stacks\n[9], graph neural networks [1] and others). More recent variants leverage pre-trained multi-modal\nTransformers (e.g., ViLBERT [33, 34]) to ﬁne-tune to the grounding tasks. Such models have an\nadded beneﬁt of being able to learn sophisticated cross-modal feature representations from external\n1Grounding and referring expression comprehension have been used interchangeably in the literature. While\nthe two terms are indeed trying to characterize the same task of associating a lingual phrase with an image region,\nthere is a subtle difference in that referring expressions tend to be unique and hence need to be grounded to a\nsingle region, e.g., “person in a red coat next to a bus\". The grounding task, as originally deﬁned in [41], is more\ngeneral where a lingual phrase may be ambiguous and therefore grounded to multiple regions, e.g., “a person\".\nPreprint. Under review.\narXiv:2106.03089v2  [cs.CV]  14 Jul 2021\nlarge-scale data, which further improve the performance. However, a signiﬁcant limitation of all such\ntwo-stage methods is their inability to condition the proposal mechanism on the query phrase itself,\nwhich inherently limits the upper bound of performance (see Table 3 in [50]).\nTo address these limitations, more recently, a number of one-stage approaches have been introduced\n[22, 36, 50, 51]. Most of these take inspiration from Yolo [43] and the variants, and rely on more\nintegrated visual-linguistic fusion and a dense anchoring mechanism to directly predict the grounding\nregions. While this alleviates the need for a proposal stage, it instead requires somewhat ad hoc anchor\ndeﬁnitions, often obtained by clustering of labeled regions, and also limits ability to contextualize\ngrounding decisions as each query phrase is effectively processed independently. Finally, little\nattention in the literature has been given to leveraging relationship among the REC and RES tasks.\nIn this work we propose an end-to-end one-stage architecture, inspired by the recent DETR [ 2]\ndetection framework, which is capable of simultaneous language grounding at both a bounding-\nbox and segmentation level, without requiring dense anchor deﬁnitions. This model also enables\ncontextualized reasoning by taking into account the entire image, all referring query phrases of\ninterest and (optionally) lingual context (e.g., a sentence from which referring phrases are parsed).\nSpeciﬁcally, we leverage a transfomer architecture, with a visual-lingual encoder, to encode image\nand lingual context, and a two-headed (detection and segmentation) custom contextualized tranformer\ndecoder. The contextualized decoder takes as input learned contextualized phrase queries and decodes\nthem directly to bounding boxes and segmentation masks. Implicit 1-to-1 correspondence between\ninput referring phrases and resulting outputs also enables a more direct formulation of the loss without\nrequiring Hungarian matching. With this simple model we outperform state-of-the-art methods by a\nlarge margin on both REC and RES tasks. We also show that a simple pre-training schedule (on an\nexternal dataset) further improves the performance. Extensive experiments and ablations illustrate\nthat our model beneﬁt greatly from the contextualized information and the multi-task training.\nContributions. Our contributions are: (1) We propose a simple and general one-stage transformer-\nbased architecture for referring expression comprehension and segmentation. The core of this model\nis the novel transformer decoder that leverages contextualized phrase queries and is able to directly\ndecode those, subject to contextualized image embeddings, into corresponding image regions and\nsegments; (2) Our approach is unique in enabling simultaneous REC and RES using a single trained\nmodel (the only other method capable of this is [36]); showing that such multi-task learning leads\nto improvements on both tasks; (3) As with other transformer-based architectures, we show that\npre-training can further improve the performance and both vanila and pre-trained models outperform\nstate-of-the-art on both tasks by signiﬁcant margins (up to 8.5% on refcoco dataset for REC and\n19.4% for RES). We also thoroughly validate our design in detailed ablations.\n2 Related works\nReferring Expression Comprehension (REC). REC focuses on producing an image bounding\nbox tightly encompassing a language query. Previous two-staged works [17, 19, 29, 30, 55] refor-\nmulate this as a ranking task with a set of candidate regions predicted from a pre-trained proposal\nmechanism. Despite achieving great success, the performance of two-staged methods is capped by\nthe speed and accuracy of region proposals in the ﬁrst stage. More recently, one-stage approaches\n[26, 50, 51] have been used to alleviate the aforementioned limitations. Yang et al. [50, 51] proposed\nto fuse query information with visual features and pick the bounding box with maximum activation\nscores from YOLOv3 [43]. Liao et al. [26] utilizes CenterNet [11] to perform correlation ﬁltering for\nregion center localization. However, such methods either require manually tuned anchor boxes or\nsuffer from semantic loss due to modality misalignment. In contrast, our model learns to better align\nmodalities using a cross-modal transformer and directly decode bounding boxes for each query.\nReferring Expression Segmentation (RES). Similar to REC, RES, proposed in [ 18], aims to\npredict segmentation masks to better describe the shape of the referred region. A typical solution for\nreferring expression segmentation is to fuse multi-modal information with a segmentation network\n(e.g., [16, 31]) and train it to output the segmented masks [18, 29, 38, 52, 55]. More recent approaches\nfocus on designing module to enable better multi-modal interactions, e.g., progressive multi-scale\nfusion used in [ 21] and cross-modal attention block used in [ 20]. Since localization information\nmatters in predicting instance segmentations (as noted in Mask RCNN [16]), very recent work [22]\naims to explicitly localize object before doing segmentation. Despite the relatively high performance\nbeing achieved in RES, existing approaches still struggle to determine the correct referent region and\n2\ntend to output noisy segmentation results with an irregular shape, while our model is able to produce\nsegmentations with ﬁne-grained shapes even on challenging scenarios with occlusions or shadows.\nMulti-task Learning for REC and RES.Multi-task learning is widely applied in object detection\nand segmentation [2, 16], often, by leveraging shared backbone and task-speciﬁc heads. Building on\nthis idea, Luo et al. [36] proposed a multi-task collaborative network (MCN) to jointly address REC\nand RES. They introduce consistency energy maximization loss that constrains the feature activation\nmap in REC and RES to be similar. While our model is also set up to learn REC and RES tasks\njointly, we argue that an explicit constraint tends to downplay the quality of the ﬁnal predicted mask\nsince the feature map from the REC branch can blur out ﬁne-grained region shape information needed\nby the RES branch (see Figure 2). Hence, we use an implicit constraint where tasks head of REC and\nRES are trained to output corresponding bounding box and mask from the same joint multi-modal\nrepresentation. We illustrate that our model can beneﬁt from multi-task supervision, leading to more\naccurate results as compared to single-task variants.\nPretrained Multi-modal Transformers. Transformer-based pretrained models [5, 12, 33, 47, 53]\nhave recently showed strong potential in multi-modal understanding. LXMERT [47] and ViLBERT\n[33] use two stream transformers with cross-attention transformer layers on top for multimodal fusion.\nMore recent works, [5, 12] advocate a single-stream design to fuse two modalities earlier. The success\nof the aforementioned models can largely be attributed to the cross-modal representations obtained\nby multi-task pretraining on a large amount of aligned image-text pairs. Despite state-of-the-art\nperformance of such models on the downstream REC task, these models, fundamentally, are still a\nform of a two-stage pipeline where image features are extracted using pretrained detectors or proposal\nmechanisms. We focus on a one-stage architecture variant that allows visual and lingual features to\nbe aligned at the early stages. Although the focus of our work is not to design a better pretraining\nscheme, we show that our model can outperform the existing state-of-the-art with proper pretraining.\nTransformer-based Detectors. More recently, DETR [2] and its variants [13, 57], were proposed\nto enable end-to-end object detection. DETR reformulates detection as a set prediction tasks and uses\ntransformers to decode learnable queries to bounding boxes. Despite state-of-art performance, DETR\nis disadvantaged by its optimization difﬁculty and, usually, extra-long training time. While adopting\na similar pipeline, our model focus on aligning different modalities to generate contextualized\nexpression-speciﬁc referring queries. We also design our model to get rid of Hungarian matching\nloss by leveraging one-to-one correspondences between predicted bounding boxes and referring\nexpressions, which leads to faster convergence for our model.\nContemporaneous Works. Concurrent and independent to us, very recently, there are some\nworks that use transformers for visual referring tasks [ 7, 10, 23]. All three of these are non-\nrefereed/unpublished at the time of this manuscript upload to ArXiv. Importantly, unlike [7, 10, 23]\nour approach formulated in multi-task setting and solves both REC and RES tasks simultaneously;\nwhile the aforementioned works [7, 10] focus on REC speciﬁcally. In addition, our model is faster\nand is capable of grounding multiple contextualized phrases, while [7, 10] follow previous one-stage\napproaches and are only able to infer a single expression at a time; leading, in our case, to more\naccurate results. Finally, [ 23] builds on DETR [ 2] while our approach deviates from DETR in a\nnumber of ways, including in terms of the loss, and focuses more on the end-to-end joint multi-task\nlearning.\n3 Approach\nGiven an image Iand a set of query phrases / referring expressions Qp = {pi}i=1,...,M , that we\nassume to come from an (optional) contextual text source2 Q, our goal is to predict a set of bounding\nboxes B= {bi}i=1,...,M and corresponding segmentation masks S= {si}i=1,...,M , one for each\nquery phrase ithat localizes that phrase in the image. Note, M is the number of phrases / referring\nexpressions for a given image Iand is typically between 1 and 16 for the Flickr30k [41] dataset.\nAs shown in Figure 1, our referring transformer is composed of four stages. Given an image-(con)text\npair, <I,Q>, a cross-modal encoder generates joint image-text embeddings for each visual and\ntextual token – feature columns and word embeddings respectively. Phrases / referring expressions\nQp and the corresponding pooled joint embeddings (from cross-modal encoder) are then fed into\n2For example, a sentence from which noun phrases/referring expressions pi were parsed. Where contextual\ntext source is unavailable and only one phrase exists, we simply let Q= Qp.\n3\nFigure 1: Referring Transformer.An overview of the proposed architecture is shown in (a). For an\nimage and (con)text input, a visual-lingual encoder is used to reﬁne image features, extracted from a\nconvolutional backbone, and lingual features, extracted by a BERT. A query encoder and decoder\nproduce features for REC and RES heads, given multi-modal features and referring expressions.\nThe detailed structure of the query encoder and decoder is shown in (b). Colored squares denote\nembeddings for corresponding referring queries.\na query generator which produces phrase-speciﬁc queries. The decoder jointly reasons across all\nthese queries and decodes joint image-(con)text embeddings. The decoded feature is then sent to the\ndetection and segmentation head to produce a set of detection boxes Band segmentation masks S.\nThe result is a one-staged end-to-end model that solves the REC and RES tasks at the same time. We\nwill now introduce constituent architectural components for the four stages brieﬂy described above.\n3.1 Feature Extraction\nVisual & Text Backbone.Starting from an initial image I∈ R3×H0×W0 , we adopt the widely\nused ResNet [15] to generate its low-resolution feature map fI ∈RCi×HW . For the corresponding\nexpression or sentence, we use the uncased base of BERT [ 8] to obtain the representation fQ ∈\nRCt×N , while N is the length of the input context sentence.\nVisual-Lingual Encoder. The visual-lingual encoder is designed to fuse information from multi-\nmodal sources. For cross-modality encoding, we use a transformer encoder based model, which is\ncomposed of 6 transformer encoder layers. Speciﬁcally, given both image and text features, multi-\nlayer perceptions are applied ﬁrst to project different modalities to a joint embedding space with\na hidden dimension of C. Since transformer based encoders are permutation invariant and do not\npreserve positional information, we follow [2, 8] to add cosine positional embedding Pimg for image\nfeatures and learnable positional embeddingPtext for text features. We then concatenate the projected\nfeatures into a single sequence f ∈RC×(HW +N). To distinguish between modalities, we also deign\na learnable modal label embedding Elabel : {Eimg,Etext}which is added to the original sequences.\nThe visual-lingual encoder then takes a sequence as input, and {Pimg,Ptext,Elabel}are fed into\neach encoder layer. The encoder output is a multi-modal feature sequence fvl ∈RC×(HW +N).\n3.2 Referring Decoder\nThe referring decoder aims to decode the phrase query, conditioned on visual-lingual features from\nthe encoder, into an output bounding box and segmentation. In this stage, we ﬁrst generate query\nembeddings corresponding to each referring phrase. Then these query embeddings are fed into the\ndecoder together with the joint embedding from the encoder to generate outputs.\nEncoding Referring Queries. To enable the decoder to generate the desired output (bounding box\nand/or segmentation) the queries must encode several bits of crucial information. Mainly, (1) encoding\nof the referring phrase, (2) image encoding and (3) phrase-speciﬁc optional (con)text information.\n4\nFor phrase encoding in (1) we use a BERT model with pooling heads which share weights with\n(con)text encoder; this results in the phrase feature vector fpi ∈RC for the i-th referring phrase.\nWe note that because we use visual-lingual encoder, (2) and (3) are jointly encoded in multi-modal\nfeatures fvl described in Section 3.1 above. However, fvl is phrase-agnostic encoding of the image\nand (con)text. To generate phrase-speciﬁc context, given a phrase pi, average pooling is used to\nextract the phrase-speciﬁc context information fc(pi) from the visual-lingual feature sequence as\nfollows:\nfc(pi) =\n∑fvl[lpi : rpi ]\nrpi −lpi\n(1)\nwhere lpi and rpi denotes the left and right bounds of phrase pi in the original (con)text sentence.\nFinally, given phrase encoding fpi and phrase-speciﬁc context fc(pi) we construct our phrase queries\nusing a multi-later perceptron:\nˆQpi = MLP ([fc(pi); fpi ]) +Ep, (2)\nwhere Ep ∈RC is a learnable embedding which serves as a bias to the formed query.\nDecoding. In the decoder, an attention graph convolution layer is used to enable information ﬂow\nin a dense connected graph of phrase queries. This allows phrase queries to contextualize and reﬁne\neach other; the inspiration for this step is taken from [1]. After that, a cross attention layer decodes\nvisual-lingual information given the updated phrase query and feature sequence from the encoder.\nThe design of our decoder is similar to the transformer decoder, except attention is non-causal.\n3.3 Multi-task training\nIn this section, we demonstrate how the decoded phrase-speciﬁc query features can be naturally used\nto train multiple heads for different referring tasks (regression for REC and segmentation for RES).\nReferring Comprehension/Detection (REC). For referring detection tasks, the ﬁnal output is\ncomputed by a simple two-layer perceptron over the decoded phrase-speciﬁc query features. We\nlet the detection head directly output center coordinates ˜b = (x,y,h,w ) for the referred image. To\nsupervise the training, we use a weighted sum of an L1 loss and a Generalized IOU loss [45]:\nLdet = λiouLiou(b,˜b) +λL1||b −˜b||1. (3)\nThe λiou and λL1 control the relative weighting of the two losses in the REC objective.\nReferring Segmentation (RES). Following previous work [2], we design an FPN-like architecture\nto predict a referring segmentation mask for each phrase expression. Attention masks from the\ndecoder and image features from the visual-lingual encoder are concatenated as the FPN input, while\nfeatures from different stages of image backbones are used as skip connections to reﬁne the ﬁnal\noutput. The last linear layer project the upsampled feature to a single channel heatmap and a sigmoid\nfunction is used to map the feature to mask scores ˜s ∈RH0/4×W0/4. The loss for training RES task\nis:\nLseg = λfocal Lfocal (s,˜s) +λdiceLdice(s,˜s). (4)\nHere Lfocal is the focal loss for classifying pixels used in [28], Ldice is the DICE/F-1 loss proposed\nin [39]; λfocal and λdice are hyper-parameters controlling the relative importance of the two losses.\nJoint Training. While it is possible to train referring segmentation and referring detection tasks\nseparately, we ﬁnd that joint training is highly beneﬁcial. Therefore the combined training loss which\nwe optimize is L= Lseg + Ldet.\nPretraining the Transformer. Transformers are generally data hungry and requires a lot of data to\ntrain [5, 33]. Although in this paper we do not use a large pretraining model and a lot of data. We\nfound that simple preatraining strategy on the region description splits of Visual-Genome dataset [25]\nmakes our model achieve comparable and even better performance against some of state-of-the-art\npretrained models. Interestingly, we found that although there is no ground truth segmentation\nprovided in Visual-Genome, the RES task can still beneﬁt greatly from pretrained models, likely due\nto the ﬁne-tuned multi-task representation.\n5\n4 Experiments\n4.1 Datasets\nRefCOCO/RefCOCO+/RefCOCOg (REC&RES). RefCOCO, RefCOCO+ [54] and RefCOCOg\n[40] are collections of images and referred objects from MSCOCO [ 27]. On RefCOCO and Ref-\nCOCO+ we follow the split used in [54] and report scores on the validation, testA and testB splits.\nOn RefCOCOg, we use the RefCOCO-umd splits proposed in[40].\nFlickr30k Entities (REC). Flickr30k Entities [ 41] contains 31,783 images and 158k caption\nsentences with 427k annotated phrase. We use splits from [ 41, 42]. Bounding boxes and phrase\nannotations are consistent with the previous one-stage approaches [50, 51] for fair comparisons.\nReferIt (REC). The ReferItGame dataset [24] contains 20,000 images. We follow setup in [3] for\nsplitting train, validation and test set; resulting in 54k, 6k and 6k referring expressions respectively.\n4.2 Implementing Details\nWe train our model with AdamW [32]. The initial learning rate is set to 1e-4 while the learning rate\nof image backbone and context encoder is set to 1e-5. We initialized weights in the transformer\nencoder and decoder with Xavier initialization [14]. For image backbone, we experiment with the\npopular ResNet-50 and ResNet-101 networks [15] where weights are initialized from corresponding\nImageNet-pretrained models. For the context encoder and phrase encoder, we use an uncased version\nof BERT model [8] with weights initialized from pretrained checkpoints provided by HuggingFace\n[49]. For data augmentation, we scale images such that the longest side is 640 pixels and follow\n[50] to do random intensity saturation and afﬁne transforms. We remove the random horizontal ﬂip\naugmentation used in previous work [50] since we notice it causes semantic ambiguity on RefCOCO,\nlikely due to relative location (e.g., left of/right of) speciﬁc queries in the dataset.\nOn Flickr30k dataset, we set the maxium length of context sentence to 90 and maximum number\nof referring phrases to 16. On the ReferIt and the RefCOCO dataset, only phrase expressions\nare provided and the task aims to predict a single bounding box for each of the expressions. In\nthose cases, the context sentence is taken as the referring phrase expression itself. We set the\nmaximum length of context sentence on these two datasets to 40. To fairly compare with pretrained\nmethods, we use region description split in the VisualGenome [25] to pretrain our model. The dataset\ncontains approximately 100k images and we remove the images that appears in Flickr30k Entities\nand RefCOCO/RefCOCOg/RefCOCO+’s validation and test set to avoid potential test data leak. For\nall the pretrained methods, we only train the model on pretraining dataset for 6 epoches. We ﬁnd\nthat longer pretraining schedule gives better performance, but since the focus of this paper is not on\npretraining methods, we stick to shorter pretraining schedules to save computational resources. All\nexperiments are conducted using 4 Nvidia 2080TI GPU with batch size as 8. For all the results given,\nwe run experiments several times with random seeds and the error bars are within ±0.5%.\n4.3 Quantitative Analysis\nEvaluation Metrics. For referring expression comprehension (REC), consistent with prior works,\nwe use precision as the evaluation metric. We mark a referring detection as correct when the\nintersection-over-union (IoU) between the predicted bounding box and ground truth is larger than\n0.5. For referring expression segmentation (RES), we reported the Mean IoU (MIoU) between the\npredicted segmentation mask and ground truth mask.\nREC and RES on Refcoco/Refcoco+/Refcocog.Our model addresses REC and RES tasks jointly.\nWe compare their respective performances with the state-of-the-art in Table 6 and Table 2. In Table 6,\nthe model is ﬁrst compared with previous one-stage and two-stage approaches for REC. Without bells\nand whistles, we observe a consistent performance boost of +2.7%/+ 4%/+ 2.1% on RefCOCO,\n+6.6%/+ 4.3%/+ 8.5% on RefCOCO+ and +4.4%/+ 5.1% on RefCOCOg. To compare with\npretrained BERT methods, we use the pretraining strategy discussed in Section 3.3. As results show,\nour model achieves comprehensive advantage and shows distinct improvement on some splits, even\ncompared to advance BERT models that use 40×more data in pretraining. Table 2 illustrates results\non RES task in terms of MIoU score. It can be seen that our model achieves the best performance;\nsubstantially better than the state-of-art. We further observe that pretraining on the REC task gives a\n6\nTable 1: Comparison on REC task.Performance on RefCOCO/RefCOCO+/RefCOCOg datasets\n[54] is reported. Ours ∗ denotes that pretraining is used. RN50 and RN101 refer to ResNet50 and\nResNet101 [15] respectively; DN53 refers to DarkNet53 [43] backbone.\nModels Visual Pretrain Multi- RefCOCO RefCOCO+ RefCOCOg\nFeatures Images task val testA testB val testA testB val-u test-u\nTwo-stage:\nCMN [19] VGG16 None × - 71.03 65.77 - 54.32 47.76 - -\nMAttNet [55] RN101 None × 76.65 81.14 69.99 65.33 71.62 56.02 66.58 67.27\nRvG-Tree [17] RN101 None × 75.06 78.61 69.85 63.51 67.45 56.66 66.95 66.51\nNMTree [29] RN101 None × 76.41 81.21 70.09 66.46 72.02 57.52 65.87 66.44\nCM-Att-Erase [30]RN101 None × 78.35 83.14 71.32 68.09 73.65 58.03 67.99 68.67\nOne-stage:\nRCCF [26] DLA34 None × - 81.06 71.85 - 70.35 56.32 - 65.73\nSSG [4] DN53 None × - 76.51 67.50 - 62.14 49.27 58.80 -\nFAOA [50] DN53 None × 72.54 74.35 68.50 56.81 60.23 49.60 61.33 60.36\nReSC-Large [51] DN53 None × 77.63 80.45 72.30 63.59 68.36 56.81 67.30 67.20\nMCN [36] DN53 None ✓ 80.08 82.29 74.98 67.16 72.86 57.31 66.46 66.01\nOurs RN50 None ✓ 81.82 85.33 76.31 71.13 75.58 61.91 69.32 69.10\nOurs RN101 None ✓ 82.23 85.59 76.57 71.58 75.96 62.16 69.41 69.40\nPretrained:\nVilBERT[33] RN101 3.3M × - - - 72.34 78.52 62.61 - -\nERNIE-ViL_L[53]RN101 4.3M × - - - 75.89 82.37 66.91 - -\nUNTIER_L[5] RN101 4.6M × 81.41 87.04 74.17 75.90 81.45 66.70 74.86 75.77\nVILLA_L[12] RN101 4.6M × 82.39 87.48 74.84 76.17 81.54 66.84 76.18 76.71\nOurs* RN50 100k ✓ 85.43 87.48 79.86 76.40 81.35 66.59 78.43 77.86\nOurs* RN101 100k ✓ 85.65 88.73 81.16 77.55 82.26 68.99 79.25 80.01\nTable 2: Comparison on RES tasks.Performance on RefCOCO/RefCOCO+/RefCOCOg datasets\n[54] is reported. Ours ∗ denotes that pretraining is used. RN50 abd RN101 refer to ResNet50 and\nResNet101 [15] respectively; DN53 refers to DarkNet53 [43] backbone.\nMethods Backbone RefCOCO RefCOCO+ RefCOCOg Inference\nval testA testB val testA testB val test time(ms)\nDMN [38] RN101 49.78 54.83 45.13 38.88 44.22 32.29 - - -\nMAttNet [55] RN101 56.51 62.37 51.70 46.67 52.39 40.08 47.64 48.61 378\nNMTree [29] RN101 56.59 63.02 52.06 47.40 53.01 41.56 46.59 47.88 -\nLang2seg [6] RN101 58.90 61.77 53.81 - - - 46.37 46.95 -\nBCAM [20] RN101 61.35 63.37 59.57 48.57 52.87 42.13 - - -\nCMPC [21] RN101 61.36 64.53 59.64 49.56 53.44 43.23 - - -\nCGAN [35] DN53 64.86 68.04 62.07 51.03 55.51 44.06 51.01 51.69 -\nLTS [22] DN53 65.43 67.76 63.08 54.21 58.32 48.02 54.40 54.25 -\nMCN+ASNLS [36] DN53 62.44 64.20 59.71 50.62 54.99 44.69 49.22 49.40 56\nOurs RN50 69.94 72.80 66.13 60.9 65.20 53.45 57.69 58.37 38\nOurs RN101 70.56 73.49 66.57 61.08 64.69 52.73 58.73 58.51 41\nOurs∗ RN50 73.61 75.22 69.80 65.30 69.69 56.98 65.70 65.41 38\nOurs∗ RN101 74.34 76.77 70.87 66.75 70.58 59.40 66.63 67.39 41\nhuge performance boost to the RES task, even when no segmentation mask is used in pre-training.\nMulti-task training enables the model to leverage performance boost in one task to improve the other.\nWe show the inference time for our model in Table 2. Our model can directly decode all referring\nqueries in an image in parallel, allowing it to reach real-time performance. Importanly, note the\ncorresponding scores for our model in Table 6 and Table 2 are based on the output of a single multi-\ntask model that predicts referring detection box and segmentation mask simultaneously. The only\nrelated work that shares this property is the MCN [36], which has substantially inferior performance.\nREC on Flickr30k-Entities. For Flickr30k dataset, previous one-stage works [50, 51] extract short\nphrases from sentences and treated them as separate referring expression comprehension tasks. We\nargue that in this setting, queries are mostly short phrases and therefore cannot well reﬂect the model’s\nability to comprehend them in context. In contrast, our model, given an image and a caption (con)text\nsentence, aims to predict bounding boxes for all referred entities in the sentence. Doing so gives\nseveral advantages: 1. We are able to contextualize referring expressions given all other referring\nexpressions and (con)text provided by the sentence. 2. Locations for all phrases can be inferred in\none forward pass of the network, which saves a lot of computation as compared to previous one-stage\napproaches [50, 51] that process one phrase at a time. Note that our task formulation is consistent\nwith some two-stage models [1, 9], but is unique for a one stage approach.\nIn Table 3 we compare our results with state-of-the-art methods. Without pretraining, we obtain a\nhuge performance boost compared to both previous one-stage (+13.54%) and two-stage (+7.31%)\n7\nFigure 2: Qualitative Evaluation.In (a) comparison to MCN [36] on REC is shown; orange, blue\nand red bounding boxes correspond to outputs from MCN, our model and the ground truth. In (b)\nsimilar comparison on RES is made. The attention map is drawn from the last layer of the decoder.\nWe add mosaic to all human face to protect personal information.\nTable 3: Comparison with State-of-The-Art Methods.Table illustrates performance on the test set\nof ReferItGame [24] and Flickr30K Entities [41] datasets in terms of top-1 accuracy (%).\nModels BackboneReferItGameFlickr30K Inference time\ntest test on Flickr30k(ms)\nTwo-stage\nMAttNet [55] RN101 29.04 - 320\nSimilarity Net [48]RN101 34.54 60.89 184\nCITE [42] RN101 35.07 61.33 196\nDDPN [56] RN101 63.00 73.30 -\nOne-stage\nSSG [4] DN53 54.24 - 25\nZSGNet [46] RN50 58.63 58.63 -\nFAOA [50] DN53 60.67 68.71 23\nRCCF [26] DLA34 63.79 - 25\nReSC-Large [51] DN53 64.60 69.28 36\nOurs RN50 70.81 78.13 37(14)\nOurs RN101 71.42 78.66 40(15)\nOurs* RN50 75.49 79.46 37(14)\nOurs* RN101 76.18 81.18 40(15)\nstate-of-the art methods. By using pretrained models, we observe that our model tends to generalize\nbetter on the test set and gives even better performance. We also provide comparison of inference\ntime both per image and (per-expression), since our model can amortize inference across expressions.\nPer-expression, our inference time is substantially lower than all prior methods.\nREC on ReferIt. Since ReferIt is a relative small dataset, we use a slightly smaller model which\ncontains 3 cross attention layers in the query decoder. The results are shown in Table 3. Our model is\nable to perform better, by a large margin, than even the latest one-stage methods. We also observe a\nconsistent boost brought by pretraining.\n4.4 Qualitative Analysis\nIn Figure 2, we show our qualitative comparison with previous state-of-the-art multi-task model –\nMCN [36]. The ﬁrst two rows of Figure 2(a) show failure cases of MCN that can be better handled\nby our model. We observe that MCN appears to fail because it neglects some attributes in referring\nexpression (e.g., \"yellow drink\" and \"blue striped shirt\"), while our model is able to better model the\nquery and pay attention to object attributes. In the last row, we shows several failure cases of our\nmodel. For the ﬁrst case, the query requires the model to have the ability to recognize number \"44\".\n8\nTable 4: Ablation studies. Table on the left ablates our multi-task and pretraining schehme on\nRefCOCO+ validation set. Table on the right ablates on core components of our model.\nREC RES Pretrain RefCOCO+\nLoss Loss REC↑ RES↑ IE↓\n✓ - 58.39 23.52%✓ 70.02 -\n✓ ✓ 71.13 61.08 4.73%\n✓ ✓ 75.07 -\n✓ ✓ ✓ 76.40 65.30 4.48%\nFlickr30k\nModel component\n-w/oQuery Decoder 49.38\n-w/oContext Encoder 73.68\nQuery Encoder\n-w/oContext & Phrase Feature 42.05\n-w/oContext Feature 76.64\n-w/oPhrase Feature 77.02\n-w/oLearnable Embedding 77.64\nFull model 78.13\nFor the second and third case, there is visual ambiguity to identify the nearest glass to the bowl or to\ndetermine which bear(brown or white) has the longest leg.\nIn Figure 2(b), we show qualitative comparison in terms of referred segmentation mask. Compared to\nMCN, our model is able to output more detailed object shape and ﬁner outlines. Moreover, our model\nshows the ability to handle shadows (e.g., the right bottom of the donut) and occlusions (e.g., the man\noccluded by another man’s arm) and predict smoother segmentation mask. We also give a result on a\nchallenging case in the last row, where the texture boundary of the two giraffe is hard to distinguish.\nDespite imperfections, our model is still able to focus on the giraffe’s head in the foreground and\nperforms much better than MCN.\n4.5 Ablation Studies\nWe ﬁrst consider the importance of the multi-task setup in Table 4 (left). Results indicate that multi-\ntask training boosts both REC and RES performance by a considerable margin. More speciﬁcally,\nwe observe that REC loss helps the transformer to better locate the referred object and converge\nfaster in early stages of training. At the same time, RES loss aids the model with more ﬁne-grained\ninformation on the shape of the referred region, which helps to further enhance the accuracy. IE here\nis Inconsistency Error metric originally used in [36] to measure the prediction conﬂict between the\nREC and RES task. We can see that joint training of RES and REC greatly reduce the inconsistency\nbetween the two tasks. Note that our model also has a much lower multi-task inconsistency compared\nto MCN [36], with a corresponding IE score of 7.54%(-40%). This shows that our model can do\nbetter collaborative learning.\nNext, we validate the design of our network architecture. We report our scores on the Flickr30k\ntest sets. In Table 4 (right), we ablate the model’s major components and features used to form the\nquery. Without (w/o) context encoder indicates that we directly use learnable embedding to encode\ntext; w/o Query Decoder means that we directly use the average pooled feature from the encoder\nto predict a single referred output. We can see that the context encoder plays an important role in\nproviding good textual representation for further multi-modal fusion. Query encoders are also quite\nimportant without which we also observe a big performance drop. For the ablation on query features,\nwe observe that both context feature and phrase features are crucial without which the performance\nwill decrease considerably. Learnable embeddings also help to enhance the performance by adding\nbias to fused query semantics. The table also showed that the network will not work without guidance\nof both context and phrase features since we cannot establish a correspondence between multiple\nqueries and outputs in such a case.\n5 Conclusions and Future Work\nIn this work, we present Referring Transformer, a one-step approach to referring expression compre-\nhension (REC) and segmentation (RES). We jointly train our model for RES and REC tasks while\nenabling contextualized multi-expression references. Our models outperform state-of-the-art by a\nlarge margin on ﬁve / three datasets for REC / RES respectively, while achieving real-time runtime.\nOne limitation for our model is that we follow the setup in previous works [50, 51] and assume that\neach expression refers to only one region. In the future, we plan to explore learning to predict multiple\nregions for each referring entity if necessary. Large-scale multi-task pretraining has been demonstrated\nto be very effective for ViLBEERT and other similar architectures; this is complementary to our focus\nin this paper, and we expect such strategies to further improve the performance.\n9\n6 Acknowledgments and Disclosure of Funding\nThis work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chair, NSERC CRC\nand an NSERC DG and Discovery Accelerator Grants. Resources used in preparing this research\nwere provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and\ncompanies sponsoring the Vector Institute www.vectorinstitute.ai/#partners. Additional\nhardware support was provided by John R. Evans Leaders Fund CFI grant and Compute Canada\nunder the Resource Allocation Competition award.\n10\nReferences\n[1] M. Bajaj, L. Wang, and L. Sigal. G3raphground: Graph-based language grounding. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV), pages 4281–4290, 2019.\n[2] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection\nwith transformers. In European Conference on Computer Vision (ECCV), pages 213–229, 2020.\n[3] K. Chen, R. Kovvuri, and R. Nevatia. Query-guided regression network with context policy for phrase\ngrounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages\n824–832, 2017.\n[4] X. Chen, L. Ma, J. Chen, Z. Jie, W. Liu, and J. Luo. Real-time referring expression comprehension by\nsingle-stage grounding network. arXiv preprint arXiv:1812.03426, 2018.\n[5] Y .-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y . Cheng, and J. Liu. Uniter: Universal image-text\nrepresentation learning. In European Conference on Computer Vision (ECCV), pages 104–120, 2020.\n[6] Y . W. Chen, Y . H. Tsai, T. Wang, Y . Y . Lin, and M. H. Yang. Referring expression object segmentation\nwith caption-aware consistency. In Proceedings of the British Machine Vision Conference (BMVC), 2019.\n[7] J. Deng, Z. Yang, T. Chen, W. Zhou, and H. Li. TransVG: End-to-end visual grounding with transformers.\narXiv preprint arXiv:2104.08541, 2021.\n[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 4171–4186, 2019.\n[9] P. Dogan, L. Sigal, and M. Gross. Neural sequential phrase grounding (seqground). In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4175–4184, 2019.\n[10] Y . Du, Z. Fu, Q. Liu, and Y . Wang. Visual grounding with transformers.arXiv preprint arXiv:2105.04281,\n2021.\n[11] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian. Centernet: Keypoint triplets for object detection.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 6569–6578,\n2019.\n[12] Z. Gan, Y .-C. Chen, L. Li, C. Zhu, Y . Cheng, and J. Liu. Large-scale adversarial training for vision-and-\nlanguage representation learning. In Proceedings of the International Conference on Neural Information\nProcessing Systems (NeurIPS), 2020.\n[13] P. Gao, M. Zheng, X. Wang, J. Dai, and H. Li. Fast convergence of detr with spatially modulated\nco-attention. arXiv preprint arXiv:2101.07448, 2021.\n[14] X. Glorot and Y . Bengio. Understanding the difﬁculty of training deep feedforward neural networks. In\nProceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics , pages\n249–256, 2010.\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.\n[16] K. He, G. Gkioxari, P. Dollár, and R. Girshick. Mask r-cnn. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pages 2961–2969, 2017.\n[17] R. Hong, D. Liu, X. Mo, X. He, and H. Zhang. Learning to compose and reason with language tree\nstructures for visual grounding. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),\n2019.\n[18] R. Hu, M. Rohrbach, and T. Darrell. Segmentation from natural language expressions. In European\nConference on Computer Vision (ECCV), pages 108–124, 2016.\n[19] R. Hu, M. Rohrbach, J. Andreas, T. Darrell, and K. Saenko. Modeling relationships in referential\nexpressions with compositional modular networks. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 1115–1124, 2017.\n[20] Z. Hu, G. Feng, J. Sun, L. Zhang, and H. Lu. Bi-directional relationship inferring network for referring\nimage segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2020.\n11\n[21] S. Huang, T. Hui, S. Liu, G. Li, Y . Wei, J. Han, L. Liu, and B. Li. Referring image segmentation via\ncross-modal progressive comprehension. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2020.\n[22] Y . Jing, T. Kong, W. Wang, L. Wang, L. Li, and T. Tan. Locate then segment: A strong pipeline for\nreferring image segmentation. arXiv preprint arXiv:2103.16284, 2021.\n[23] A. Kamath, M. Singh, Y . LeCun, I. Misra, G. Synnaeve, and N. Carion. MDETR–modulated detection for\nend-to-end multi-modal understanding. arXiv preprint arXiv:2104.12763, 2021.\n[24] S. Kazemzadeh, V . Ordonez, M. Matten, and T. Berg. Referitgame: Referring to objects in photographs of\nnatural scenes. In Proceedings of the Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 787–798, 2014.\n[25] R. Krishna, Y . Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y . Kalantidis, L.-J. Li, D. A. Shamma,\net al. Visual genome: Connecting language and vision using crowdsourced dense image annotations.\nInternational Journal of Computer Vision (IJCV), 123(1):32–73, 2017.\n[26] Y . Liao, S. Liu, G. Li, F. Wang, Y . Chen, C. Qian, and B. Li. A real-time cross-modality correlation ﬁltering\nmethod for referring expression comprehension. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), June 2020.\n[27] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft\ncoco: Common objects in context. In European Conference on Computer Vision (ECCV), pages 740–755,\n2014.\n[28] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Dollár. Focal loss for dense object detection. InProceedings\nof the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2980–2988, 2017.\n[29] D. Liu, H. Zhang, F. Wu, and Z.-J. Zha. Learning to assemble neural module tree networks for visual\ngrounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages\n4673–4682, 2019.\n[30] X. Liu, Z. Wang, J. Shao, X. Wang, and H. Li. Improving referring expression grounding with cross-\nmodal attention-guided erasing. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 1950–1959, 2019.\n[31] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3431–\n3440, 2015.\n[32] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on\nLearning Representations (ICLR), 2019.\n[33] J. Lu, D. Batra, D. Parikh, and S. Lee. ViLBERT: Pretraining task-agnostic visiolinguistic representations\nfor vision-and-language tasks. In Proceedings of the International Conference on Neural Information\nProcessing Systems (NeurIPS), pages 13–23, 2019.\n[34] J. Lu, V . Goswami, M. Rohrbach, D. Parikh, and S. Lee. 12-in-1: Multi-task vision and language repre-\nsentation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 10437–10446, 2020.\n[35] G. Luo, Y . Zhou, R. Ji, X. Sun, J. Su, C.-W. Lin, and Q. Tian. Cascade grouped attention network for\nreferring expression segmentation. In ACM International Conference on Multimedia, 2020.\n[36] G. Luo, Y . Zhou, X. Sun, L. Cao, C. Wu, C. Deng, and R. Ji. Multi-task collaborative network for\njoint referring expression comprehension and segmentation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2020.\n[37] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy. Generation and comprehension of\nunambiguous object descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 11–20, 2016.\n[38] E. Margffoy-Tuay, J. C. Pérez, E. Botero, and P. Arbeláez. Dynamic multimodal instance segmentation\nguided by natural language queries. In European Conference on Computer Vision (ECCV), 2018.\n[39] F. Milletari, N. Navab, and S.-A. Ahmadi. V-net: Fully convolutional neural networks for volumetric\nmedical image segmentation. In International Conference on 3D Vision (3DV), pages 565–571, 2016.\n12\n[40] V . K. Nagaraja, V . I. Morariu, and L. S. Davis. Modeling context between objects for referring expression\nunderstanding. In European Conference on Computer Vision (ECCV), pages 792–807, 2016.\n[41] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik. Flickr30k\nentities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2641–2649, 2015.\n[42] B. A. Plummer, P. Kordas, M. H. Kiapour, S. Zheng, R. Piramuthu, and S. Lazebnik. Conditional image-text\nembedding networks. In European Conference on Computer Vision (ECCV), pages 249–264, 2018.\n[43] J. Redmon and A. Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.\n[44] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region\nproposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 39(6):\n1137–1149, 2017.\n[45] H. Rezatoﬁghi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese. Generalized intersection over\nunion: A metric and a loss for bounding box regression. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 658–666, 2019.\n[46] A. Sadhu, K. Chen, and R. Nevatia. Zero-shot grounding of objects from natural language queries. In\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4694–4703,\n2019.\n[47] H. Tan and M. Bansal. Lxmert: Learning cross-modality encoder representations from transformers.\nProceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.\n[48] L. Wang, Y . Li, J. Huang, and S. Lazebnik. Learning two-branch neural networks for image-text matching\ntasks. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 41(2):394–407, 2018.\n[49] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,\nJ. Davison, S. Shleifer, P. von Platen, C. Ma, Y . Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame,\nQ. Lhoest, and A. M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of\nthe Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 38–45, 2020.\n[50] Z. Yang, B. Gong, L. Wang, W. Huang, D. Yu, and J. Luo. A fast and accurate one-stage approach to visual\ngrounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages\n4683–4693, 2019.\n[51] Z. Yang, T. Chen, L. Wang, and J. Luo. Improving one-stage visual grounding by recursive sub-query\nconstruction. In European Conference on Computer Vision (ECCV), 2020.\n[52] L. Ye, M. Rochan, Z. Liu, and Y . Wang. Cross-modal self-attention network for referring image segmen-\ntation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\n2019.\n[53] F. Yu, J. Tang, W. Yin, Y . Sun, H. Tian, H. Wu, and H. Wang. Ernie-vil: Knowledge enhanced vision-\nlanguage representations through scene graph. arXiv preprint arXiv:2006.16934, 2020.\n[54] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Modeling context in referring expressions. In\nEuropean Conference on Computer Vision (ECCV), pages 69–85, 2016.\n[55] L. Yu, Z. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L. Berg. Mattnet: Modular attention network\nfor referring expression comprehension. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 1307–1315, 2018.\n[56] Z. Yu, J. Yu, C. Xiang, Z. Zhao, Q. Tian, and D. Tao. Rethinking diversiﬁed and discriminative proposal\ngeneration for visual grounding. International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2018.\n[57] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai. Deformable detr: Deformable transformers for end-to-end\nobject detection. In International Conference on Learning Representations (ICLR), 2021.\n13\nA More Details for Referring Expression Segmentation (RES)\nFigure 3: RES Task Head.A detailed illustration of our model for RES task.\nWe provide a more detailed illustration of our model for the RES task in Figure 3. With decoded\nquery embedding from the query decoder and visual feature coming from visual-lingual decoder,\na query attention score Satt ∈RM×(HW ) is computed using a dot product. Here M denotes the\nnumber of attention heads, which is 8 in our implementation. The attention score is then concatenated\nwith visual feature and sent into several up-sampling blocks (convolution layer with stride of 2).\nWe also add residual connections from different stages of the ResNet backbone to help reﬁne the\nup-sampled features. All convolution layers here use a kernel size of 3. The design is motivated by\nMaskRCNN[16] and DETR[2].\nB Additional Implementation Details\nPretraining. We use the description split of Visual Genome [25] for pretraining, it contains 100k\nimages with an average of 40 region descriptions per-image. We pretrain our model with REC task\non the Visual Genome dataset for 6 epochs. We set the learning rate at 1e-4 and decay it by 10x after\n4 epochs. The trained model is then used to initialize the model for dataset-speciﬁc ﬁne-tuning.\nRefCOCO Training. For experiments on RefCOCO(+/g) [40, 54], since auxiliary loss is expensive\nfor RES task, we ﬁrst train our model with auxiliary loss on REC task for 60 epochs using a learning\nrate of 1e-4. Then, we disable auxiliary loss and train the model jointly on RES and REC task for 30\nepochs with learning rate of 1e-4 that decays on the 10th epoch.\nReferItGame / Flickr30k Training. On ReferItGame [24] and Flickr30k Entities [41], our model\nis trained for 90 and 60 epochs respectively, with learning rate decays on the 60th and 40th epoch.\nSource Code. We include core codes for our model in the Supplemental for the reference. We will\nrelease complete code with checkpoints to reproduce reported scores upon acceptance.\nC Additional Results\nMore Qualitative Comparison. Additional qualitative results on REC and RES tasks, compared\nto the previous multi-task framework of [36], are shown in Figure 4 and Figure 5 respectively. Note\nthat the score of RES can beneﬁt greatly from better localization of corresponding REC task. In\nFigure 5, to better compare the quality of generated referred masks, we compare the mask quality in\nthe case where both MCN [36] and our method assume correct REC localization.\nResults with Different Input Resolution. In RES and REC tasks, the size of input image is\na matter of trade off between performance and speed, which is largely effected by the network\narchitecture. Despite that our model is designed to be able to process 640 ×640 images at real time\nspeed, we provide our model with different input resolution for reference, as showed in Table 5. Note\nthat we replace convolution in the ﬁnal stage with dilated convolution to make sure the number of\nvisual features sent into visual-lingual encoder are roughly the same.\n14\nFigure 4: Additional Qualitative Results on REC Task.Orange, blue and red bounding boxes\ncorrespond to outputs from MCN [36], our model and the ground truth. The ﬁrst row, second row\nand third row comes from RefCOCO+ testA, testB and RefCOCOg test set respectively.\nTable 5: Results on RefCOCO+ Dataset with Different Input Resolutions.Our methods corre-\nspond to the model without pretraining.\nModels Resolution REC(prec@0.5) RES(MIoU)\nval testA testB val testA testB\nFAOA [50] 256×256 56.81 60.23 49.60 - - -\nReSC-Large [51] 256×256 63.59 68.36 56.81 - - -\nCMPC [21] 320×320 - - - 49.56 53.44 43.23\nLTS [22] 416×416 - - - 54.21 58.32 48.02\nMCN [36] 416×416 67.16 72.86 57.31 50.62 54.99 44.69\nOurs 256×256 70.05 73.29 61.48 58.26 61.09 52.20\nOurs 320×320 70.03 73.23 61.52 58.42 61.48 52.34\nOurs 416×416 71.50 75.87 61.71 61.00 64.48 52.44\nOurs 640×640 71.58 75.96 62.16 61.08 64.69 52.73\nComparison with Contemporaneous WorkAs discussed in the main paper, [7, 10, 23] are non-\nrefereed/unpublished works that appear on ArXiv recently. We provide quantitative comparison on\nREC task with these approaches, based on their reported numbers in Table 6. We stress that none of\nthese works address or show performance on multi-task performance of REC & RES task, which is\none of distinctive qualities of our model in comparison.\nCompared to [7, 10], our model performs substantially better in all, with an exception of RefCOCO\ntestB (where [7] is marginally better), datasets and splits. The biggest improvements can be seen\non RefCOCO+, where our model is 10.4% better (or 6.76 points better), than the closest concurrent\nwork of [7], on the Val split; similar sizable improvements are illustrated on other splits,e.g., 9.2%\non RefCOCO+ testB. In addition, our approach is considerably faster in runtime, since our model is\nable to handle multiple queries simultaneously (unlike [ 7, 10]). Unfortunately, we are not able to\nreport speciﬁc numbers for this at this time, as [7, 10] do not report them in their ArXiv papers.\nCompared to [23], in a pretrained model setting, we see that our model performs similarly to [23] on\nRefCOCO and marginally worse on RefCOCO+ and RefCOCOg. We believe this difference can be\nattributed to two factors: (1) pretraining on a larger dataset (200k images vs. 100k for us, plus we use\na shorter 6 epochs pretraining schedule) (2) using more sophisticated language model (RoBERTa for\n[23] vs. vanilla BERT for us). We were unable to explore effect those choices would have on our\nmodel for the moment, but expect them to further boost the performance. In addition, we setup our\nmethod in multi-task setting to solve RES and REC task at the same time, so our formulation while\nperhaps marginally inferior on REC is more general overall. Finally, our formulation, which does\naway with Hungarian matching loss, is likely to also be signiﬁcantly faster to train. Exploring this\nwould require re-running [23], which we hope to do for the camera ready.\n15\nFigure 5: Additional Results on RES Task.Images come from RefCOCO+ testA and testB splits.\nTable 6: Comparison with Concurrent Work.Performance on RefCOCO/RefCOCO+/RefCOCOg\ndatasets [54] is reported. Ours∗ denotes that pretraining is used. All methods use ResNet101 as the\nimage backbone.\nModels Visual Pretrain Multi- RefCOCO RefCOCO+ RefCOCOg\nFeatures Images task val testA testB val testA testB val-u test-u\nOne-stage:\nVGTR [10] Bi-LSTM None × 79.20 82.32 73.78 63.91 70.09 56.51 65.73 67.23\nTransVG [7] BERT None × 81.02 82.72 78.35 64.82 70.70 56.94 68.67 67.73\nOurs BERT None ✓ 82.23 85.59 76.57 71.58 75.96 62.16 69.41 69.40\nPretrained:\nMDETR [23] RoBERTa 200k ✓ 86.75 89.58 81.41 79.52 84.09 70.62 81.64 80.89\nOurs∗ BERT 100k ✓ 85.65 88.73 81.16 77.55 82.26 68.99 79.25 80.01\n16",
  "topic": "Ground",
  "concepts": [
    {
      "name": "Ground",
      "score": 0.693386971950531
    },
    {
      "name": "Transformer",
      "score": 0.6627774834632874
    },
    {
      "name": "Task (project management)",
      "score": 0.48473402857780457
    },
    {
      "name": "Computer science",
      "score": 0.46918725967407227
    },
    {
      "name": "Engineering",
      "score": 0.21227076649665833
    },
    {
      "name": "Electrical engineering",
      "score": 0.16853341460227966
    },
    {
      "name": "Systems engineering",
      "score": 0.06934306025505066
    },
    {
      "name": "Voltage",
      "score": 0.055486321449279785
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141945490",
      "name": "University of British Columbia",
      "country": "CA"
    }
  ],
  "cited_by": 73
}