{
    "title": "Improving Language Models by Clustering Training Sentences",
    "url": "https://openalex.org/W2950492505",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5100676025",
            "name": "David Carter",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2083398114",
        "https://openalex.org/W2127314673",
        "https://openalex.org/W2444378235",
        "https://openalex.org/W1967165764",
        "https://openalex.org/W1509045587",
        "https://openalex.org/W2099111195",
        "https://openalex.org/W2084084380",
        "https://openalex.org/W1789580494",
        "https://openalex.org/W2134237567",
        "https://openalex.org/W2072223048"
    ],
    "abstract": "Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences. I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction, and calculating separate language model parameters for each cluster. This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model. It also offers a reasonably automatic means to gather evidence on whether a more complex, context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it: if clustering improves the performance of a model, this proves the existence of further context dependencies, not exploited by the unclustered model. As evidence for these claims, I present results showing that clustering improves some models but not others for the ATIS domain. These results are consistent with other findings for such models, suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model.",
    "full_text": "arXiv:cmp-lg/9410001v1  4 Oct 1994\nImproving Language Models by\nClustering Training Sentences\nDavid Carter\nSRI International\n23 Millers Yard\nCambridge CB2 1RQ, UK\ndmc@cam.sri.com\nAbstract\nMany of the kinds of language model used in speech\nunderstanding suﬀer from imperfect modeling of intra-\nsentential contextual inﬂuences. I argue that this prob-\nlem can be addressed by clustering the sentences in a\ntraining corpus automatically into subcorpora on the\ncriterion of entropy reduction, and calculating separate\nlanguage model parameters for each cluster. This kind\nof clustering oﬀers a way to represent important con-\ntextual eﬀects and can therefore signiﬁcantly improve\nthe performance of a model. It also oﬀers a reasonably\nautomatic means to gather evidence on whether a more\ncomplex, context-sensitive model using the same gen-\neral kind of linguistic information is likely to reward the\neﬀort that would be required to develop it: if clustering\nimproves the performance of a model, this proves the\nexistence of further context dependencies, not exploited\nby the unclustered model. As evidence for these claims,\nI present results showing that clustering improves some\nmodels but not others for the ATIS domain. These\nresults are consistent with other ﬁndings for such mod-\nels, suggesting that the existence or otherwise of an\nimprovement brought about by clustering is indeed a\ngood pointer to whether it is worth developing further\nthe unclustered model.\n1. Introduction\nIn speech recognition and understanding systems, many\nkinds of language model may be used to choose between\nthe word and sentence hypotheses for which there is\nevidence in the acoustic data. Some words, word se-\nquences, syntactic constructions and semantic struc-\ntures are more likely to occur than others, and the\npresence of more likely objects in a sentence hypoth-\nesis is evidence for the correctness of that hypothesis.\nEvidence from diﬀerent knowledge sources can be com-\nbined in an attempt to optimize the selection of correct\nhypotheses; see e.g. Alshawi and Carter (1994); Rayner\net al (1994); Rosenfeld (1994).\nMany of the knowledge sources used for this purpose\nscore a sentence hypothesis by calculating a simple, typ-\nically linear, combination of scores associated with ob-\njects, such as N-grams and grammar rules, that char-\nacterize the hypothesis or its preferred linguistic anal-\nysis. When these scores are viewed as log probabilities,\ntaking a linear sum corresponds to making an indepen-\ndence assumption that is known to be at best only ap-\nproximately true, and that may give rise to inaccuracies\nthat reduce the eﬀectiveness of the knowledge source.\nThe most obvious way to make a knowledge source\nmore accurate is to increase the amount of structure\nor context that it takes account of. For example, a\nbigram model may be replaced by a trigram one, and\nthe fact that dependencies exist among the likelihoods\nof occurrence of grammar rules at diﬀerent locations in\na parse tree can be modeled by associating probabilities\nwith states in a parsing table rather than simply with\nthe rules themselves (Briscoe and Carroll, 1993).\nHowever, such remedies have their drawbacks.\nFirstly, even when the context is extended, some im-\nportant inﬂuences may still not be modeled. For exam-\nple, dependencies between words exist at separations\ngreater than those allowed for by trigrams (for which\nlong-distance N-grams [Jelinek et al , 1991] are a par-\ntial remedy), and associating scores with parsing table\nstates may not model all the important correlations be-\ntween grammar rules. Secondly, extending the model\nmay greatly increase the amount of training data re-\nquired if sparseness problems are to be kept under con-\ntrol, and additional data may be unavailable or expen-\nsive to collect. Thirdly, one cannot always know in ad-\nvance of doing the work whether extending a model in\na particular direction will, in practice, improve results.\nIf it turns out not to, considerable ingenuity and eﬀort\nmay have been wasted.\nIn this paper, I argue for a general method for ex-\ntending the context-sensitivity of any knowledge source\nthat calculates sentence hypothesis scores as linear com-\nbinations of scores for objects. The method, which is\nrelated to that of Iyer, Ostendorf and Rohlicek (1994),\ninvolves clustering the sentences in the training corpus\ninto a number of subcorpora, each predicting a diﬀerent\nprobability distribution for linguistic objects. An utter-\nance hypothesis encountered at run time is then treated\nas if it had been selected from the subpopulation of\nsentences represented by one of these subcorpora. This\ntechnique addresses as follows the three drawbacks just\nalluded to. Firstly, it is able to capture the most im-\nportant sentence-internal contextual eﬀects regardless\nof the complexity of the probabilistic dependencies be-\ntween the objects involved. Secondly, it makes only\nmodest additional demands on training data. Thirdly,\nit can be applied in a standard way across knowledge\nsources for very diﬀerent kinds of object, and if it does\nimprove on the unclustered model this constitutes proof\nthat additional, as yet unexploited relationships exist\nbetween linguistic objects of the type the model is based\non, and that therefore it is worth looking for a more\nspeciﬁc, more powerful way to model them.\nThe use of corpus clustering often does not boost the\npower of the knowledge source as much as a speciﬁc\nhand-coded extension. For example, a clustered bigram\nmodel will probably not be as powerful as a trigram\nmodel. However, clustering can have two important\nuses. One is that it can provide some improvement to\na model even in the absence of the additional (human\nor computational) resources required by a hand-coded\nextension. The other use is that the existence or oth-\nerwise of an improvement brought about by clustering\ncan be a good indicator of whether additional perfor-\nmance can in fact be gained by extending the model by\nhand without further data collection, with the possibly\nconsiderable additional eﬀort that extension would en-\ntail. And, of course, there is no reason why clustering\nshould not, where it gives an advantage, also be used\nin conjunction with extension by hand to produce yet\nfurther improvements.\nAs evidence for these claims, I present experimental\nresults showing how, for a particular task and training\ncorpus, clustering produces a sizeable improvement in\nunigram- and bigram-based models, but not in trigram-\nbased ones; this is consistent with experience in the\nspeech understanding community that while moving\nfrom bigrams to trigrams usually produces a deﬁnite\npayoﬀ, a move from trigrams to 4-grams yields less clear\nbeneﬁts for the domain in question. I also show that,\nfor the same task and corpus, clustering produces im-\nprovements when sentences are assessed not according\nto the words they contain but according to the syntax\nrules used in their best parse. This work thus goes be-\nyond that of Iyer et al by focusing on the methodolog-\nical importance of corpus clustering, rather than just\nits usefulness in improving overall system performance,\nand by exploring in detail the way its eﬀectiveness varies\nalong the dimensions of language model type, language\nmodel complexity, and number of clusters used. It also\ndiﬀers from Iyer et al ’s work by clustering at the utter-\nance rather than the paragraph level, and by using a\ntraining corpus of thousands, rather than millions, of\nsentences; in many speech applications, available train-\ning data is likely to be quite limited, and may not always\nbe chunked into paragraphs.\n2. Cluster-based Language Modeling\nMost other work on clustering for language modeling\n(e.g. Pereira, Tishby and Lee, 1993; Ney, Essen and\nKneser, 1994) has addressed the problem of data sparse-\nness by clustering words into classes which are then\nused to predict smoothed probabilities of occurrence for\nevents which may seldom or never have been observed\nduring training. Thus conceptually at least, their pro-\ncesses are agglomerative: a large initial set of words is\nclumped into a smaller number of clusters. The ap-\nproach described here is quite diﬀerent. Firstly, it in-\nvolves clustering whole sentences, not words. Secondly,\nits aim is not to tackle data sparseness by grouping\na large number of objects into a smaller number of\nclasses, but to increase the precision of the model by\ndividing a single object (the training corpus) into some\nlarger number of sub-objects (the clusters of sentences).\nThere is no reason why clustering sentences for predic-\ntion should not be combined with clustering words to\nreduce sparseness; the two operations are orthogonal.\nOur type of clustering, then, is based on the assump-\ntion that the utterances to be modeled, as sampled in\na training corpus, fall more or less naturally into some\nnumber of clusters so that words or other objects as-\nsociated with utterances have probability distributions\nthat diﬀer between clusters. Thus rather than estimat-\ning the relative likelihood of an utterance interpretation\nsimply by combining ﬁxed probabilities associated with\nits various characteristics, we view these probabilities as\nconditioned by the initial choice of a cluster or subpopu-\nlation from which the utterance is to be drawn. In both\ncases, many independence assumptions that are known\nto be at best reasonable approximations will have to\nbe made. However, if the clustering reﬂects signiﬁcant\ndependencies, some of the worst inaccuracies of these\nassumptions may be reduced, and system performance\nmay improve as a result.\nSome domains and tasks lend themselves more ob-\nviously to a clustering approach than others. An ob-\nvious and trivial case where clustering is likely to be\nuseful is a speech understander for use by travelers in\nan international airport; here, an utterance will typi-\ncally consist of words from one, and only one, natural\nlanguage, and clusters for diﬀerent languages will be\ntotally dissimilar. However, clustering may also give\nus signiﬁcant leverage in monolingual cases. If the di-\nalogue handling capabilities of a system are relatively\nrigid, the system may only ask the user a small number\nof diﬀerent questions (modulo the ﬁlling of slots with\ndiﬀerent values). For example, the CLARE interface\nto the Autoroute PC package (Lewin et al , 1993) has a\nfairly simple dialogue model which allows it to ask only\na dozen or so diﬀerent types of question of the user. A\nWizard of Oz exercise, carried out to collect data for\nthis task, was conducted in a similarly rigid way; thus\nit is straightforward to divide the training corpus into\nclusters, one cluster for utterances immediately follow-\ning each kind of system query. Other corpora, such\nas Wall Street Journal articles, might also be expected\nto fall naturally into clusters for diﬀerent subject areas,\nand indeed Iyer et al (1994) report positive results from\ncorpus clustering here.\nFor some applications, though, there is no obvious\nextrinsic basis for dividing the training corpus into clus-\nters. The ARPA air travel information (ATIS) domain\nis an example. Questions can mention concepts such as\nplaces, times, dates, fares, meals, airlines, plane types\nand ground transportation, but most utterances men-\ntion several of these, and there are few obvious restric-\ntions on which of them can occur in the same utterance.\nDialogues between a human and an ATIS database ac-\ncess system are therefore likely to be less clearly struc-\ntured than in the Autoroute case.\nHowever, there is no reason why automatic cluster-\ning should not be attempted even when there are no\ngrounds to expect clearly distinct underlying subpopu-\nlations to exist. Even a clustering that only partly re-\nﬂects the underlying variability of the data may give us\nmore accurate predictions of utterance likelihoods. Ob-\nviously, the more clusters are assumed, the more likely\nit is that the increase in the number of parameters to be\nestimated will lead to worsened rather than improved\nperformance. But this trade-oﬀ, and the eﬀectiveness\nof diﬀerent clustering algorithms, can be monitored and\noptimized by applying the resulting cluster-based lan-\nguage models to unseen test data. In Section 1 below,\nI report results of such experiments with ATIS data,\nwhich, for the reasons given above, would at ﬁrst sight\nseem relatively unlikely to yield useful results from a\nclustering approach. Since, as we will see, clustering\ndoes yield beneﬁts in this domain, it seems very plau-\nsible that it will also do so for other, more naturally\nclustered domains.\n3. Clustering Algorithms\nThere are many diﬀerent criteria for quantifying the\n(dis)similarity between (analyses of) two sentences or\nbetween two clusters of sentences; Everitt (1993) pro-\nvides a good overview. Unfortunately, whatever the cri-\nterion selected, it is in general impractical to ﬁnd the\noptimal clustering of the data; instead, one of a vari-\nety of algorithms must be used to ﬁnd a locally optimal\nsolution.\nLet us for the moment consider the case where the\nlanguage model consists only of a unigram probability\ndistribution for the words in the vocabulary, with no N-\ngram (for N > 1) or fuller linguistic constraints consid-\nered. Perhaps the most obvious measure of the similar-\nity between two sentences or clusters is then Jaccard’s\ncoeﬃcient (Everitt, 1993, p41), the ratio of the num-\nber of words occurring in both sentences to the number\noccurring in either or both. Another possibility would\nbe Euclidean distance, with each word in the vocabu-\nlary deﬁning a dimension in a vector space. However, it\nmakes sense to choose as a similarity measure the quan-\ntity we would like the ﬁnal clustering arrangement to\nminimize: the expected entropy (or, equivalently, per-\nplexity) of sentences from the domain. This goal is\nanalogous to that used in the work described earlier on\nﬁnding word classes by clustering.\nFor our simple unigram language model without clus-\ntering, the training corpus perplexity is minimized (and\nits likelihood is maximized) by assigning each word wi\na probability pi = fi/N, where fi is the frequency of wi\nand N is the total size of the corpus. The corpus like-\nlihood is then P1 = ∏\ni pfi\ni , and the per-word entropy,\n− ∑\nwi pilog(pi), is thus minimized. (See e.g. Cover and\nThomas, 1991, chapter 2 for the reasoning behind this).\nIf now we model the language as consisting of sen-\ntences drawn at random from K diﬀerent subpopula-\ntions, each with its own unigram probability distribu-\ntion for words, then the estimated corpus probability\nis\nPK = ∏\nuj\n∑\nck\nqk\n∏\nwi∈ uj pk,i\nwhere the iterations are over each utterance uj in the\ncorpus, each cluster c1 . . . cK from which uj might arise,\nand each word wi in utterance uj. qk = |ck|/ ∑\ni |ci| is\nthe likelihood of an utterance arising from cluster (or\nsubpopulation) ck, and pk,i is the likelihood assigned to\nword wi by cluster k, i.e. its relative frequency in that\ncluster.\nOur ideal, then, is the set of clusters that maximizes\nthe cluster-dependent corpus likelihood PK . As with\nnearly all clustering problems, ﬁnding a global maxi-\nmum is impractical. To derive a good approximation\nto it, therefore, we adopt the following algorithm.\n• Select a random ordering of the training corpus, and\ninitialize each cluster ck, k= 1 . . . K, to contain just\nthe kth sentence in the ordering.\n• Present each remaining training corpus sentence in\nturn, initially creating an additional singleton cluster\ncK+1 for it. Merge that pair of clusters c1 . . . cK+1\nthat entails the least additional cost, i.e. the smallest\nreduction in the value of PK for the subcorpus seen\nso far.\n• When all training utterances have been incorporated,\nﬁnd all the triples ( u, ci, cj ), i̸= j, such that u ∈ ci\nbut the probability of u is maximized by cj . Move all\nsuch u’s (in parallel) between clusters. Repeat until\nno further movements are required.\nIn practice, we keep track not of PK but of the over-\nall corpus entropy HK = −log(PK ). We record the\ncontribution each cluster ck makes to HK as\nHK (ck) = − ∑\nwi∈ ck\nfiklog(fik/Fk)\nwhere fik is the frequency of wi in ck and Fk =∑\nwj ∈ ck\nfjk , and ﬁnd the value of this quantity for all\npossible merged clusters. The merge in the second step\nof the algorithm is chosen to be the one minimizing\nthe increase in entropy between the unmerged and the\nmerged clusters.\nThe adjustment process in the third step of the al-\ngorithm does not attempt directly to decrease entropy\nbut to achieve a clustering with the obviously desirable\nproperty that each training sentence is best predicted\nby the cluster it belongs to rather than by another clus-\nter. This heightens the similarities within clusters and\nthe diﬀerences between them. It also reduces the arbi-\ntrariness introduced into the clustering process by the\norder in which the training sentences are presented. 1\n1(Footnotes in this paper are used for the results of sta-\ntistical signiﬁcance tests and other technical details not es-\nsential to an understanding of the main argument).\nThis clustering algorithm is closely related to that of Ney,\nEssen and Kneser (1994), who cluster words into equivalence\nclasses rather than training sentences into subcorpora. Ne y\net al begin with a clustering in which the K − 1 most fre-\nquent words each occupy a singleton cluster and the Kth\ncluster contains all the other words. They then move words\nbetween clusters to maximize probabilities. They remark\nthat “other initialization schemes were found to work as wel l\nand not to aﬀect much the ﬁnal result; however, their speed\nof convergence may be much slower”. A frequency-based\ninitialization scheme of this kind is, however, less approp ri-\nate for clustering sentences, because whereas very frequen t\nwords are likely to have diﬀerent distributions (the basis f or\nNey et al’s clustering), some very frequent sentences may\ncontain very similar word sequences (the basis for ours), an d\nit is therefore undesirable automatically to put them in dif -\nferent clusters. In the ATIS corpus used for the experiments\nThe approach is applicable with only a minor modiﬁca-\ntion to N-grams for N > 1: the probability of a word\nwithin a cluster is conditioned on the occurrence of the\nN − 1 words preceding it, and the entropy calculations\ntake this into account. Other cases of context depen-\ndence modeled by a knowledge source can be handled\nsimilarly. And there is no reason why the items charac-\nterizing the sentence have to be (sequences of) words;\noccurrences of grammar rules, either without any con-\ntext or in the context of, say, the rules occurring just\nabove them in the parse tree, can be treated in just the\nsame way.\n4. Experimental Results\nExperiments were carried out to assess the eﬀective-\nness of clustering, and therefore the existence of un-\nexploited contextual dependencies, for instances of two\ngeneral types of language model. In the ﬁrst experi-\nment, sentence hypotheses were evaluated on the N-\ngrams of words and word classes they contained. In\nthe second experiment, evaluation was on the basis of\ngrammar rules used rather than word occurrences.\nN-gram Experiment\nIn the ﬁrst experiment, reference versions of a set of\n5,873 domain-relevant (classes A and D) ATIS-2 sen-\ntences were allocated to K clusters for K = 2 ,3,5,6,10\nand 20 for the unigram, bigram and trigram conditions\nand, for unigrams and bigrams only, K = 40 and 100\nas well. Each run was repeated for ten diﬀerent ran-\ndom orders for presentation of the training data. The\nunclustered ( K = 1) version of each language model\nwas also evaluated. Some words, and some sequences\nof words such as “San Francisco”, were replaced by class\nnames to improve performance. The per-item entropy\nof the training set (i.e. the per-word entropy, but ignor-\ning the need to distinguish diﬀerent words in the same\nclass) was 6.04 for a unigram language model, 2.96 for\nbigrams, and 1.97 for trigrams, giving perplexities of\n65.7, 7.76 and 3.92 respectively. The greater the value\nof K, the more a clustering reduced the apparent train-\ning set per-item entropy (which, of course, is not the\nsame thing as reducing test set entropy). The reduc-\ntions for K = 20 were around 20% for unigrams, 40%\ndescribed in Section 1, for example, the ﬁrst and third most\ncommon sentence patterns are “Show me the ﬂights from\n(city) to (city)” and “Show me ﬂights from (city) to (city)”;\nand our algorithm assigned these to the same cluster for\n80% of the runs with ten or fewer clusters.\nIyer et al (1994) cluster training corpus paragraphs ag-\nglomeratively on the basis of the proportion of content word s\nin common. This criterion is not related in any obvious way\nto perplexity minimization, and would certainly be too blun t\nan instrument for clustering ATIS sentences, which are fair ly\nshort and more limited in vocabulary.\nfor bigrams and 50% for trigrams, with very little vari-\nation (typically 1% or less) between diﬀerent runs for\nthe same condition.\nThe improvement (if any) due to clustering was mea-\nsured by using the various language models to make\nselections from N-best sentence hypothesis lists; this\nchoice of test was made for convenience rather than out\nof any commitment to the N-best paradigm, and the\ntechniques described here could equally well be used\nwith other forms of speech-language interface.\nSpeciﬁcally, each clustering was tested against 1,354\nhypothesis lists output by a version of the DECIPHER\n(TM) speech recognizer (Murveit et al , 1993) that it-\nself used a (rather simpler) bigram model. Where more\nthen ten hypothesis were output for a sentence, only the\ntop ten were considered. These 1,354 lists were the sub-\nset of two 1,000 sentence sets (the February and Novem-\nber 1992 ATIS evaluation sets) for which the reference\nsentence itself occurred in the top ten hypotheses. The\nclustered language model was used to select the most\nlikely hypothesis from the list without paying any at-\ntention either to the score that DECIPHER assigned to\neach hypothesis on the basis of acoustic information or\nits own bigram model, or to the ordering of the list. In\na real system, the DECIPHER scores would of course\nbe taken into account, but they were ignored here in\norder to maximize the discriminatory power of the test\nin the presence of only a few thousand test utterances.\nTo avoid penalizing longer hypotheses, the probabil-\nities assigned to hypotheses were normalized by sen-\ntence length. The probability assigned by a cluster to\nan N-gram was taken to be the simple maximum like-\nlihood (relative frequency) value where this was non-\nzero. When an N-gram in the test data had not been\nobserved at all in the training sentences assigned to\na given cluster, a “failure”, representing a vanishingly\nsmall probability, was assigned. 2 A number of backoﬀ\nschemes of various degrees of sophistication, including\n2Failures, like log probabilities, were added together; a\nderived sentence log probability therefore consisted of a s um\nof log probabilities of the usual kind combined with a failur e\ncount, i.e. a pair ( LP, F). A diﬀerence in failure counts\nwas viewed as more signiﬁcant than any diﬀerence in log\nprobabilities; formally,\n(LP1, F1) < (LP2, F2) ⇔ F1 > F2 ∨ (F1 = F2 ∧ LP1 < LP2).\nProbabilities arising from diﬀerent clusters were added as\nfollows:\n(P1, F1) + ( P2, F2) = ( P1, F1) if F1 < F2;\n(P2, F2) if F1 > F2;\n(P1 + P2, F1) if F1 = F2.\nwhere Pi = eLPi .\nAlthough this scheme is quite adequate for hypothesis se-\nlection, it means that no ﬁgures can be calculated for test\nset entropy analogous to those for training set entropy.\nClusters Unigram Bigram Trigram\n1 12.4 34.3 51.6\n2 13.8 37.9 51.0\n3 15.3 39.5 50.8\n4 16.1 41.2 50.4\n5 16.8 41.2 51.0\n6 17.2 41.8 50.7\n10 17.8 43.1 51.2\n20 19.9 43.9 50.3\n40 22.3 45.0\n100 24.4 46.4\nTable 1: Average percentage scores for cluster-based\nN-gram models\nthat of Katz (1987), were tried, but none produced any\nimprovement in performance, and several actually wors-\nened it.\nThe average percentages of sentences correctly iden-\ntiﬁed by clusterings for each condition were as given in\nTable 1. The maximum possible score was 100%; the\nbaseline score, that expected from a random choice of\na sentence from each list, was 11.4%.\nThe unigram and bigram scores show a steady and,\nin fact, statistically signiﬁcant 3 increase with the num-\nber of clusters. Using twenty clusters for bigrams (score\n43.9%) in fact gives more than half the advantage over\nunclustered bigrams that is given by moving from un-\nclustered bigrams to unclustered trigrams. However,\nclustering trigrams produces no improvement in score;\n3For both unigrams and bigrams, the performance of the\nunclustered case was compared with each of the ten runs for\neach clustered condition. All the clustered runs scored bet -\nter than the corresponding unclustered case. For each clus-\ntered run, the McNemar change test (Siegel and Castellan,\n1988, p75) was applied to the number of sentences for which\nan improvement was observed (incorrect choice by unclus-\ntered model, correct by clustered model) and the number\nwith a deterioration (correct by unclustered, incorrect by\nclustered). At the P=0.05 level, two-tail, the clustered un -\nigram results were signiﬁcantly better for three of the ten\ntwo-cluster runs, nine of the ten three-cluster runs, and al l\nthe runs for more than three clusters. For the bigram case,\nall runs except one of the two-cluster ones were signiﬁcantl y\nbetter than the unclustered result.\nThe Wilcoxon-Mann-Whitney test ( ibid, p128) was ap-\nplied to the scores for all ten runs at each of two numbers\nof clusters K1 and K2. It showed that for unigrams, for\nall K1 > K2 > 1, having K1 clusters is signiﬁcantly better\nat the P=0.05 level than having K2 except for the cases\n(K1, K2) = (5 ,4), (6 ,5) and (10 ,6), where no signiﬁcant\ndiﬀerence was found. For bigrams, the diﬀerence between\n4, 5 or 6 clusters was not signiﬁcant, and neither was that\nbetween 10 and 20, but all other ( K1, K2) pairs were sig-\nniﬁcantly diﬀerent.\nin fact, it gives a small but statistically signiﬁcant 4 dete-\nrioration, presumably due to the increase in the number\nof parameters that need to be calculated.\nThe random choice of a presentation order for the\ndata meant that diﬀerent clusterings were arrived at\non each run for a given condition (( N, K) for N-grams\nand K clusters). There was some limited evidence that\nsome clusterings for the same condition were signiﬁ-\ncantly better than others, rather than just happening to\nperform better on the particular test data used. 5 More\ntrials would be needed to establish whether presenta-\ntion order does in general make a genuine diﬀerence to\nthe quality of a clustering. If there is one, however,\nit would appear to be fairly small compared to the im-\nprovements available (in the unigram and bigram cases)\nfrom increasing the numbers of clusters.\nGrammar Rule Experiment\nIn the second experiment, each training sentence and\neach test sentence hypothesis was analysed by the Core\nLanguage Engine (Alshawi, 1992) trained on the ATIS\ndomain (Agn¨ as et al , 1994). Unanalysable sentences\nwere discarded, as were sentences of over 15 words in\nlength (the ATIS adaptation had concentrated on sen-\ntences of 15 words or under, and analysis of longer\nsentences was less reliable and slower). When a sen-\ntence was analysed successfully, several semantic anal-\nyses were, in general, created, and a selection was made\nfrom among these on the basis of trained preference\nfunctions (Alshawi and Carter, 1994). For the purpose\nof the experiment, clustering and hypothesis selection\nwere performed on the basis not of the words in a sen-\ntence but of the grammar rules used to construct its\nmost preferred analysis.\nThe simplest condition, hereafter referred to as “1-\nrule”, was analogous to the unigram case for word-based\nevaluation. A sentence was modeled simply as a bag of\nrules, and no attempt (other than the clustering itself)\nwas made to account for dependencies between rules.\nAnother condition, henceforth “2-rule” because of its\nanalogy to bigrams, was also tried. Here, each rule\noccurrence was represented not in isolation but in the\ncontext of the rule immediately above it in the parse\n4The McNemar test revealed no cases of clustered tri-\ngrams performing signiﬁcantly better than unclustered.\nHowever, in 15 of the 70 runs, the performance was sig-\nniﬁcantly worse. The Wilcoxon-Mann-Whitney test showed\nno clear advantage or disadvantage in diﬀerent numbers of\nclusters for trigrams.\n5A positive correlation was found between scores on the\nFebruary 1992 and November 1992 parts of the test set for\neight of the nine unigram conditions; the correlation was\npositive and signiﬁcant (P=0.05 level, two-tail) for two of\nthese conditions. For bigrams, the correlation was less cle ar,\nand for trigrams, not apparent at all.\nClusters 1-rule 2-rule\n1 29.4 34.3\n2 31.4 35.5\n3 31.8 36.2\n4 31.7 37.0\n5 32.3 37.2\n6 31.9 37.3\n10 32.8 37.5\n20 35.1 38.3\n40 35.8 38.9\nTable 2: Average percentage scores for cluster-based\nN-rule models\ntree(its “predecessor” if the tree is traversed top-down).\nThis choice was made on the assumption that the imme-\ndiately dominating rule would be one important inﬂu-\nence on the likelihood of occurrence of a particular rule.\nOther choices, involving sister rules and/or rules in less\nclosely related positions, or the compilation of rules into\ncommon combinations (Samuelsson and Rayner, 1991)\nmight have worked as well or better; our purpose here\nis simply to illustrate and assess ways in which explicit\ncontext modeling can be combined with clustering.\nThe training corpus consisted of the 4,279 sentences\nin the 5,873-sentence set that were analysable and con-\nsisted of ﬁfteen words or less. The test corpus consisted\nof 1,106 hypothesis lists, selected in the same way (on\nthe basis of length and analysability of their reference\nsentences) from the 1,354 used in the ﬁrst experiment.\nThe “baseline” score for this test corpus, expected from\na random choice of (analysable) hypothesis, was 23.2%.\nThis was rather higher than the 11.4% for word-based\nselection because the hypothesis lists used were in gen-\neral shorter, unanalysable hypotheses having been ex-\ncluded.\nThe average percentages of correct hypotheses (ac-\ntual word strings, not just the rules used to represent\nthem) selected by the 1-rule and 2-rule conditions were\nas given in Table 2.\nThese results show that clustering gives a signiﬁcant\nadvantage for both the 1-rule and the 2-rule types of\nmodel,6 and that the more clusters are created, the\nlarger the advantage is, at least up to K = 20 clusters. 7\n6For both the 1-rule and the 2-rule condition, nearly all\nthe runs for all values of K were signiﬁcantly better than\nthe unclustered case: all the clustered scores were higher\nthan the corresponding unclustered one, and the diﬀerence\nwas signiﬁcant under the McNemar test in 131 of the 140\ncases.\n7The Wilcoxon-Mann-Whitney test suggested that, for\nthe 1-rule condition, clusterings with K = 20 and K = 40\nclusters were signiﬁcantly better than all other values of K\nAs with the N-gram experiment, there is weak evidence\nthat some clusterings are genuinely better than others\nfor the same condition. 8\n5. Conclusions\nI have suggested that training corpus clustering can be\nused both to extend the eﬀectiveness of a very general\nclass of language models, and to provide evidence of\nwhether a particular language model could beneﬁt from\nextending it by hand to allow it to take better account\nof context. Clustering can be useful even when there\nis no reason to believe the training corpus naturally\ndivides into any particular number of clusters on any\nextrinsic grounds.\nThe experimental results presented show that clus-\ntering increases the (absolute) success rate of unigram\nand bigram language modeling for a particular ATIS\ntask by up to about 12%, and that performance im-\nproves steadily as the number of clusters climbs towards\n100 (probably a reasonable upper limit, given that there\nare only a few thousand training sentences). However,\nclusters do not improve trigram modeling at all. This\nis consistent with experience (Rayner et al , 1994) that,\nfor the ATIS domain, trigrams model inter-word eﬀects\nmuch better than bigrams do, but that extending the\nN-gram model beyond N = 3 is much less beneﬁcial.\nFor N-rule modeling, clustering increases the success\nrate for both N = 1 and N = 2, although only by about\nhalf as much as for N-grams. This suggests that condi-\ntioning the occurrence of a grammar rule on the identity\nof its mother (as in the 2-rule case) accounts for some,\nbut not all, of the contextual inﬂuences that operate.\nFrom this it is sensible to conclude, consistently with\nthe results of Briscoe and Carroll (1993), that a more\ncomplex model of grammar rule interaction might yield\nbetter results. Either conditioning on other parts of the\nparse tree than the mother node could be included, or\na rather diﬀerent scheme such as Briscoe and Carroll’s\ncould be used.\ntried, but not signiﬁcantly diﬀerent from either other. K =\n2 was signiﬁcantly worse than all larger K values except\nK=4. Most other comparisons were not signiﬁcant. For\nthe 2-rule case, K = 2 was signiﬁcantly worse than K = 3,\nwhich in turn was signiﬁcantly worse than all K > 3. K =\n10 and K = 20 did not diﬀer signiﬁcantly, but apart from\nthat, K = 20 and K = 40 were again signiﬁcantly better\nthan all other K, but did not diﬀer signiﬁcantly from each\nother.\n8A signiﬁcant positive correlation was found between\nscores on the February 1992 and November 1992 parts of\nthe test set for three of the eight 1-rule conditions; how-\never, one signiﬁcant (but presumably coincidental) negative\ncorrelation was also found. For 2-rule conditions, one sig-\nniﬁcant positive correlation and no signiﬁcant negative on es\nwere found.\nNeither the observation that trigrams may represent\nthe limit of usefulness for N-gram modeling in ATIS,\nnor that non-trivial contextual inﬂuences exist between\noccurrences of grammar rules, is very novel or remark-\nable in its own right. Rather, what is of interest is that\nthe improvement (or otherwise) in particular language\nmodels from the application of clustering is consistent\nwith those observations. This is important evidence\nfor the main hypothesis of this paper: that enhanc-\ning a language model with clustering, which once the\nsoftware is in place can be done largely automatically,\ncan give us important clues about whether it is worth\nexpending research, programming, data-collection and\nmachine resources on hand-coded improvements to the\nway in which the language model in question models\ncontext, or whether those resources are best devoted to\ndiﬀerent, additional kinds of language model.\nAcknowledgements\nThis research was partly funded by the Defence\nResearch Agency, Malvern, UK, under assignment\nM85T51XX.\nI am grateful to Manny Rayner and Ian Lewin for\nuseful comments on earlier versions of this paper. Re-\nsponsibility for any remaining errors or unclarities rests\nin the customary place.\nA shorter version of this paper appears in the Pro-\nceedings of the ACL Conference on Applied Natural\nLanguage Processing, Stuttgart, October 1994, and is\nc⃝ Association for Computational Linguistics.\nReferences\nAgn¨ as, M-S.,et al (1994). Spoken Language Transla-\ntor First Year Report . SRI International Cambridge\nTechnical Report CRC-043.\nAlshawi, H., and D.M. Carter (1994). “Training and\nScaling Preference Functions for Disambiguation”.\nComputational Linguistics (to appear).\nBriscoe, T., and J. Carroll (1993). “Generalized Prob-\nabilistic LR Parsing of Natural Language (Corpora)\nwith Uniﬁcation-Based Grammars”, Computational\nLinguistics, Vol 19:1, 25-60.\nCover, T.M., and J.A. Thomas (1991). Elements of\nInformation Theory . New York: Wiley.\nEveritt, B.S. (1993). Cluster Analysis , Third Edition.\nLondon: Edward Arnold.\nIyer, R., M. Ostendorf and J.R. Rohlicek (1994). “Lan-\nguage Modeling with Sentence-Level Mixtures”. Pro-\nceedings of the ARPA Workshop on Human Language\nTechnology.\nJelinek, F., B. Merialdo, S. Roukos and M. Strauss\n(1991). “A Dynamic Language Model for Speech\nRecognition”, Proceedings of the Speech and Natural\nLanguage DARPA Workshop , Feb 1991, 293-295.\nKatz, S.M. (1987). “Estimation of Probabilities from\nSparse Data for the Language Model Component of\na Speech Recognizer”, IEEE Transactions on Acous-\ntics, Speech and Signal Processing , Vol ASSP-35:3.\nLewin, I., D.M. Carter, S. Pulman, S. Browning,\nK. Ponting and M. Russell (1993). “A Speech-Based\nRoute Enquiry System Built From General-Purpose\nComponents”, Proceedings of Eurospeech-93.\nMurveit, H., J. Butzberger, V. Digalakis and M. Wein-\ntraub (1993). “Large Vocabulary Dictation using\nSRI’s DECIPHER(TM) Speech Recognition System:\nProgressive Search Techniques”, Proceedings of the\nInternational Conference on Acoustics, Speech and\nSignal Processing, Minneapolis, Minnesota.\nNey, H., U. Essen and R. Kneser (1994). “On Struc-\nturing Probabilistic Dependencies in Stochastic Lan-\nguage Modeling”. Computer Speech and Language ,\nvol 8:1, 1-38.\nPereira, F., N. Tishby and L. Lee (1993). “Distribu-\ntional Clustering of English Words”. Proceedings of\nACL-93, 183-190.\nRayner, M., D. Carter, V. Digalakis and P. Price (1994).\n“Combining Knowledge Sources to Reorder N-best\nSpeech Hypothesis Lists”. Proceedings of the ARPA\nWorkshop on Human Language Technology .\nRosenfeld, R. (1994). “A Hybrid Approach to Adaptive\nStatistical Language Modeling”. Proceedings of the\nARPA Workshop on Human Language Technology .\nSamuelsson, C., and M. Rayner (1991). “Quantita-\ntive Evaluation of Explanation-Based Learning as a\nTuning Tool for a Large-Scale Natural Language Sys-\ntem”. Proceedings of 12th International Joint Con-\nference on Artiﬁcial Intelligence . Sydney, Australia.\nSiegel, S., and N.J. Castellan (1988). Nonparametric\nStatistics, Second Edition. New York: McGraw-Hill."
}