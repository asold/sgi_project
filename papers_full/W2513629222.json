{
    "title": "Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks",
    "url": "https://openalex.org/W2513629222",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1990217128",
            "name": "Liu Bing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4259126180",
            "name": "Lane, Ian",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2117130368",
        "https://openalex.org/W2097550833",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2949541494",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2024632416",
        "https://openalex.org/W2166293310",
        "https://openalex.org/W1934019294",
        "https://openalex.org/W2949888546",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2289394825",
        "https://openalex.org/W2491408735",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W2950304420",
        "https://openalex.org/W2094472029",
        "https://openalex.org/W2077302143"
    ],
    "abstract": "Speaker intent detection and semantic slot filling are two critical tasks in spoken language understanding (SLU) for dialogue systems. In this paper, we describe a recurrent neural network (RNN) model that jointly performs intent detection, slot filling, and language modeling. The neural network model keeps updating the intent estimation as word in the transcribed utterance arrives and uses it as contextual features in the joint model. Evaluation of the language model and online SLU model is made on the ATIS benchmarking data set. On language modeling task, our joint model achieves 11.8% relative reduction on perplexity comparing to the independent training language model. On SLU tasks, our joint model outperforms the independent task training model by 22.3% on intent detection error rate, with slight degradation on slot filling F1 score. The joint model also shows advantageous performance in the realistic ASR settings with noisy speech input.",
    "full_text": "Joint Online Spoken Language Understanding and Language Modeling\nwith Recurrent Neural Networks\nBing Liu\nCarnegie Mellon University\nElectrical and Computer Engineering\nliubing@cmu.edu\nIan Lane\nCarnegie Mellon University\nElectrical and Computer Engineering\nLanguage Technologies Institute\nlane@cmu.edu\nAbstract\nSpeaker intent detection and semantic slot\nﬁlling are two critical tasks in spoken lan-\nguage understanding (SLU) for dialogue\nsystems. In this paper, we describe a re-\ncurrent neural network (RNN) model that\njointly performs intent detection, slot ﬁll-\ning, and language modeling. The neural\nnetwork model keeps updating the intent\nprediction as word in the transcribed ut-\nterance arrives and uses it as contextual\nfeatures in the joint model. Evaluation of\nthe language model and online SLU model\nis made on the ATIS benchmarking data\nset. On language modeling task, our joint\nmodel achieves 11.8% relative reduction\non perplexity comparing to the indepen-\ndent training language model. On SLU\ntasks, our joint model outperforms the in-\ndependent task training model by 22.3%\non intent detection error rate, with slight\ndegradation on slot ﬁlling F1 score. The\njoint model also shows advantageous per-\nformance in the realistic ASR settings with\nnoisy speech input.\n1 Introduction\nAs a critical component in spoken dialogue sys-\ntems, spoken language understanding (SLU) sys-\ntem interprets the semantic meanings conveyed\nby speech signals. Major components in SLU\nsystems include identifying speaker’s intent and\nextracting semantic constituents from the natural\nlanguage query, two tasks that are often referred\nto as intent detection and slot ﬁlling.\nIntent detection can be treated as a seman-\ntic utterance classiﬁcation problem, and slot ﬁll-\ning can be treated as a sequence labeling task.\nThese two tasks are usually processed separately\nby different models. For intent detection, a\nnumber of standard classiﬁers can be applied,\nsuch as support vector machines (SVMs) (Haffner\net al., 2003) and convolutional neural networks\n(CNNs) (Xu and Sarikaya, 2013). For slot ﬁll-\ning, popular approaches include using sequence\nmodels such as maximum entropy Markov models\n(MEMMs) (McCallum et al., 2000), conditional\nrandom ﬁelds (CRFs) (Raymond and Riccardi,\n2007), and recurrent neural networks (RNNs) (Yao\net al., 2014; Mesnil et al., 2015).\nRecently, neural network based models that\njointly perform intent detection and slot ﬁlling\nhave been reported. Xu (2013) proposed using\nCNN based triangular CRF for joint intent detec-\ntion and slot ﬁlling. Guo (2014) proposed using a\nrecursive neural network (RecNN) that learns hi-\nerarchical representations of the input text for the\njoint task. Such joint models simplify SLU sys-\ntems, as only one model needs to be trained and\ndeployed.\nThe previously proposed joint SLU models,\nhowever, are unsuitable for online tasks where it\nis desired to produce outputs as the input sequence\narrives. In speech recognition, instead of receiving\nthe transcribed text at the end of the speech, users\ntypically prefer to see the ongoing transcription\nwhile speaking. In spoken language understand-\ning, with real time intent identiﬁcation and seman-\ntic constituents extraction, the downstream sys-\ntems will be able to perform corresponding search\nor query while the user dictates. The joint SLU\nmodels proposed in previous work typically re-\nquire intent and slot label predictions to be con-\nditioned on the entire transcribed word sequence.\nThis limits the usage of these models in the online\nsetting.\nIn this paper, we propose an RNN-based on-\nline joint SLU model that performs intent detec-\ntion and slot ﬁlling as the input word arrives. In\narXiv:1609.01462v1  [cs.CL]  6 Sep 2016\naddition, we suggest that the generated intent class\nand slot labels are useful for next word prediction\nin online automatic speech recognition (ASR).\nTherefore, we propose to perform intent detec-\ntion, slot ﬁlling, and language modeling jointly\nin a conditional RNN model. The proposed joint\nmodel can be further extended for belief track-\ning in dialogue systems when considering the dia-\nlogue history beyond the current utterance. More-\nover, it can be used as the RNN decoder in an\nend-to-end trainable sequence-to-sequence speech\nrecognition model (Jaitly et al., 2015).\nThe remainder of the paper is organized as fol-\nlows. In section 2, we introduce the background\non using RNNs for intent detection, slot ﬁlling,\nand language modeling. In section 3, we describe\nthe proposed joint online SLU-LM model and its\nvariations. Section 4 discusses the experiment\nsetup and results on ATIS benchmarking task, us-\ning both text and noisy speech inputs. Section 5\ngives the conclusion.\n2 Background\n2.1 Intent Detection\nIntent detection can be treated as a semantic ut-\nterance classiﬁcation problem, where the input to\nthe classiﬁcation model is a sequence of words\nand the output is the speaker intent class. Given\nan utterance with a sequence of words w =\n(w1,w2,...,w T), the goal of intent detection is to\nassign an intent class c from a pre-deﬁned ﬁnite\nset of intent classes, such that:\nˆc= arg max\nc\nP(c|w) (1)\nRecent neural network based intent classiﬁca-\ntion models involve using neural bag-of-words\n(NBoW) or bag-of-n-grams, where words or n-\ngrams are mapped to high dimensional vector\nspace and then combined component-wise by\nsummation or average before being sent to the\nclassiﬁer. More structured neural network ap-\nproaches for utterance classiﬁcation include us-\ning recursive neural network (RecNN) (Guo et\nal., 2014), recurrent neural network (Ravuri and\nStolcke, 2015), and convolutional neural network\nmodels (Collobert and Weston, 2008; Kim, 2014).\nComparing to basic NBoW methods, these mod-\nels can better capture the structural patterns in the\nword sequence.\n2.2 Slot Filling\nA major task in spoken language understand-\ning (SLU) is to extract semantic constituents by\nsearching input text to ﬁll in values for prede-\nﬁned slots in a semantic frame (Mesnil et al.,\n2015), which is often referred to as slot ﬁlling.\nThe slot ﬁlling task can also be viewed as assign-\ning an appropriate semantic label to each word in\nthe given input text. In the below example from\nATIS (Hemphill et al., 1990) corpus following\nthe popular in/out/begin (IOB) annotation method,\nSeattle and San Diego are the from and to loca-\ntions respectively according to the slot labels, and\ntomorrow is the departure date. Other words in the\nexample utterance that carry no semantic meaning\nare assigned “O” label.\nFigure 1: ATIS corpus sample with intent and slot\nannotation (IOB format).\nGiven an utterance consisting of a sequence of\nwords w = (w1,w2,...,w T), the goal of slot ﬁll-\ning is to ﬁnd a sequence of semantic labels s =\n(s1,s2,...,s T), one for each word in the utterance,\nsuch that:\nˆs = arg max\ns\nP(s|w) (2)\nSlot ﬁlling is typically treated as a sequence la-\nbeling problem. Sequence models including con-\nditional random ﬁelds (Raymond and Riccardi,\n2007) and RNN models (Yao et al., 2014; Mes-\nnil et al., 2015; Liu and Lane, 2015) are among\nthe most popular methods for sequence labeling\ntasks.\n2.3 RNN Language Model\nA language model assigns a probability to a se-\nquence of words w = (w1,w2,...,w T) following\nprobability distribution. In language modeling, w0\nand wT+1 are added to the word sequence repre-\nsenting the beginning-of-sentence token and end-\nof-sentence token. Using the chain rule, the likeli-\nhood of a word sequence can be factorized as:\nP(w) =\nT+1∏\nt=1\nP(wt|w0,w1,...,w t−1) (3)\nRNN-based language models (Mikolov et al.,\n2011), and the variant (Sundermeyer et al., 2012)\nh1 h2 hT\nw1 wT\nc\nh1 h2 hT\nw1 w2 wT\ns1\ns2\nsT\nw2\nh0 h1 hT\nw0 w1 wT\nw1\nw2\nwT+1\nOutputs\nInputs\nHidden layer\nOutputs\nInputs\nHidden layer\nOutputs\nInputs\nHidden layer\n(a)\n(b)\n(c)\nFigure 2: (a) RNN language model. (b) RNN in-\ntent detection model. The RNN output at last step\nis used to predict the intent class. (c) RNN slot\nﬁlling model. Slot label dependencies are mod-\neled by feeding the output label of the previous\ntime step to the current step hidden state.\nusing long short-term memory (LSTM) (Hochre-\niter and Schmidhuber, 1997) have shown supe-\nrior performance comparing to traditional n-gram\nbased models. In this work, we use an LSTM cell\nas the basic RNN unit for its stronger capability\nin capturing long-range dependencies in word se-\nquence.\n2.4 RNN for Intent Detection and Slot Filling\nAs illustrated in Figure 2(b), RNN intent detection\nmodel uses the last RNN output to predict the ut-\nterance intent class. This last RNN output can be\nseen as a representation or embedding of the entire\nutterance. Alternatively, the utterance embedding\ncan be obtained by taking mean of the RNN out-\nputs over the sequence. This utterance embedding\nis then used as input to the multinomial logistic\nregression for the intent class prediction.\nRNN slot ﬁlling model takes word as input and\nthe corresponding slot label as output at each time\nstep. The posterior probability for each slot label\nis calculated using the softmax function over the\nRNN output. Slot label dependencies can be mod-\neled by feeding the output label from the previ-\nous time step to the current step hidden state (Fig-\nure 2(c)). During model training, true label from\nprevious time step can be fed to current hidden\nstate. During inference, only the predicted label\ncan be used. To bridge the gap between training\nand inference, scheduled sampling method (Ben-\ngio et al., 2015) can be applied. Instead of only\nusing previous true label, using sample from pre-\nvious predicted label distribution in model train-\ning makes the model more robust by forcing it to\nlearn to handle its own prediction mistakes (Liu\nand Lane, 2015).\n3 Method\nIn this section we describe the joint SLU-LM\nmodel in detail. Figure 3 gives an overview of the\nproposed architecture.\n3.1 Model\nLet w = ( w0,w1,w2,...,w T+1) represent\nthe input word sequence, with w0 and wT+1\nbeing the beginning-of-sentence ( ⟨bos⟩) and\nend-of-sentence ( ⟨eos⟩) tokens. Let c =\n(c0,c1,c2,...,c T) be the sequence of intent class\noutputs at each time step. Similarly, let s =\n(s0,s1,s2,...,s T) be the slot label sequence,\nwhere s0 is a padded slot label that maps to the\nbeginning-of-sentence token ⟨bos⟩.\nReferring to the joint SLU-LM model shown in\nFigure 3, for the intent model, instead of predict-\ning the intent only after seeing the entire utterance\nas in the independent training intent model (Figure\n2(b)), in the joint model we output intent at each\ntime step as input word sequence arrives. The in-\ntent generated at the last step is used as the ﬁnal\nutterance intent prediction. The intent output from\neach time step is fed back to the RNN state, and\nthus the entire intent output history are modeled\nand can be used as context to other tasks. It is\nnot hard to see that during inference, intent classes\nthat are predicted during the ﬁrst few time steps\nare of lower conﬁdence due to the limited infor-\nmation available. We describe the techniques that\ncan be used to ameliorate this effect in section 3.3\nbelow. For the intent model, with both intent and\nslot label connections to the RNN state, we have:\nP(cT|w) =P(cT|w≤T,c<T,s<T) (4)\nFor the slot ﬁlling model, at each step talong the\ninput word sequence, we want to model the slot\nlabel output st as a conditional distribution over\nthe previous intents c<t, previous slot labels s<t,\nand the input word sequence up to step t. Using\nh0\nw0\ns0\nc0\nMLPC MLPS\nMLPW\nw1\nOutputs\nInputs\nHidden layer\nsampleC sampleS\nh1\nw1\ns1\nc1\nMLPC MLPS\nMLPW\nw2\nsampleC sampleS\nhT\nwT\nsT\ncT\nMLPC MLPS\nMLPW\nwT+1\nsampleC sampleS\nFigure 3: Proposed joint online RNN model for intent detection, slot ﬁlling, and next word prediction.\nht\nwt\nst\nct\nMLPC MLPS\nMLPW\nwt+1\nsampleC\nht+1ht\nwt\nst\nct\nMLPC MLPS\nMLPW\nwt+1\nsampleC\nht+1ht\nwt\nst\nct\nMLPC MLPS\nMLPW\nwt+1\nsampleC\nht\nwt\nst\nct\nMLPC MLPS\nMLPW\nwt+1\n(a) Basic joint model (b) Model with local context (c) Model with recurrent context (d) Model with local and recurrent context\nFigure 4: Joint online SLU-LM model variations. (a) Basic joint model with no conditional dependencies\non emitted intent classes and slot labels. (b) Joint model with local intent context. Next word prediction\nis conditioned on the current step intent class. (c) Joint model with recurrent intent context. The entire\nintent prediction history and variations are captured in the RNN state. (d) Joint model with both local\nand recurrent intent context.\nthe chain rule, we have:\nP(s|w) =P(s0|w0)\nT∏\nt=1\nP(st|w≤t,c<t,s<t)\n(5)\nFor the language model, the next word is mod-\neled as a conditional distribution over the word se-\nquence together with intent and slot label sequence\nup to current time step. The intent and slot label\noutputs at current step, together with the intent and\nslot label history that is encoded in the RNN state,\nserve as context to the language model.\nP(w) =\nT∏\nt=0\nP(wt+1|w≤t,c≤t,s≤t) (6)\n3.2 Next Step Prediction\nFollowing the model architecture in Figure 3, at\ntime step t, input to the system is the word at in-\ndex t of the utterance, and outputs are the intent\nclass, the slot label, and the next word prediction.\nThe RNN state ht encodes the information of all\nthe words, intents, and slot labels seen previously.\nThe neural network model computes the outputs\nthrough the following sequence of steps:\nht = LSTM(ht−1,[wt,ct−1,st−1]) (7)\nP(ct|w≤t,c<t,s<t) = IntentDist(ht) (8)\nP(st|w≤t,c<t,s<t) = SlotLabelDist(ht) (9)\nP(wt+1|w≤t,c≤t,s≤t) = WordDist(ht,ct,st)\n(10)\nwhere LSTM is the recurrent neural network func-\ntion that computes the hidden state ht at a step\nusing the previous hidden state ht−1, the em-\nbeddings of the previous intent output ct−1 and\nslot label output st−1, and the embedding of cur-\nrent input word wt. IntentDist, SlotLabelDist,\nand WordDist are multilayer perceptrons (MLPs)\nwith softmax outputs over intents, slot labels, and\nwords respectively. Each of these three MLPs has\nits own set of parameters. The intent and slot label\ndistributions are generated by the MLPs with input\nbeing the RNN cell output. The next word distri-\nbution is produced by conditioning on current step\nRNN cell output together with the embeddings of\nthe sampled intent and sampled slot label.\n3.3 Training\nThe network is trained to ﬁnd the parameters θ\nthat minimise the cross-entropy of the predicted\nand true distributions for intent class, slot label,\nand next word jointly. The objective function also\nincludes an L2 regularization term R(θ) over the\nweights and biases of the three MLPs. This equal-\nizes to ﬁnding the parameters θthat maximize the\nbelow objective function:\nmax\nθ\nT∑\nt=0\n[\nαclog P(c∗|w≤t,c<t,s<t; θ)\n+αslog P(s∗\nt|w≤t,c<t,s<t; θ)\n+αwlog P(wt+1|w≤t,c≤t,s≤t; θ)\n]\n−λR(θ)\n(11)\nwhere c∗ is the true intent class and and s∗\nt is the\ntrue slot label at time stept. αc, αs, and αw are the\nlinear interpolation weights for the true intent, slot\nlabel, and next word probabilities. During model\ntraining, ct can either be the true intent or mix-\nture of true and predicted intent. During inference,\nhowever, only predicted intent can be used. Con-\nﬁdence of the predicted intent during the ﬁrst few\ntime steps is likely to be low due to the limited\ninformation available, and the conﬁdence level is\nlikely to increase with the newly arriving words.\nConditioning on incorrect intent for next word pre-\ndiction is not desirable. To mitigate this effect,\nwe propose to use a schedule to increase the in-\ntent contribution to the context vector along the\ngrowing input word sequence. Speciﬁcally, during\nthe ﬁrst k time steps, we disable the intent con-\ntext completely by setting the values in the intent\nvector to zeros. From step k+ 1till the last step\nof the input word sequence, we gradually increase\nthe intent context by applying a linearly growing\nscaling factor η from 0 to 1 to the intent vector.\nThis scheduled approach is illustrated in Figure 5.\nFigure 5: Schedule of increasing intent contribu-\ntion to the context vector along with the growing\ninput sequence.\n3.4 Inference\nFor online inference, we simply take the greedy\npath of our conditional model without doing\nsearch. The model emits best intent class and slot\nlabel at each time step conditioning on all previous\nemitted symbols:\nˆct = arg max\nct\nP(ct|w≤t,ˆc<t,ˆs<t) (12)\nˆst = arg max\nst\nP(st|w≤t,ˆc<t,ˆs<t) (13)\nMany applications can beneﬁt from this greedy in-\nference approach comparing to search based infer-\nence methods, especially those running on embed-\nded platforms that without GPUs and with limited\ncomputational capacity. Alternatively, one can do\nleft-to-right beam search (Sutskever et al., 2014;\nChan et al., 2015) by maintaining a set of β best\npartial hypotheses at each step. Efﬁcient beam\nsearch method for the joint conditional model is\nleft to explore in our future work.\n3.5 Model Variations\nIn additional to the joint RNN model (Figure 3)\ndescribed above, we also investigate several joint\nmodel variations for a ﬁne-grained study of vari-\nous impacting factors on the joint SLU-LM model\nperformance. Designs of these model variations\nare illustrated in Figure 4.\nFigure 4(a) shows the design of a basic joint\nSLU-LM model. At each step t, the predictions of\nintent class, slot label, and next word are based on\na shared representation from the LSTM cell out-\nput ht, and there is no conditional dependencies\non previous intent class and slot label outputs. The\nsingle hidden layer MLP for each task introduces\nadditional discriminative power for different tasks\nthat take common shared representation as input.\nWe use this model as the baseline joint model.\nThe models in Figure 4(b) to 4(d) extend the\nbasic joint model by introducing conditional de-\npendencies on intent class outputs. Note that the\nsame type of extensions can be made on slot la-\nbels as well. For brevity and space concern, these\ndesigns are not added in the ﬁgure, but we report\ntheir performance in the experiment section.\nThe model in Figure 4(b) extends the basic joint\nmodel by conditioning the prediction of next word\nwt+1 on the current step intent class ct. The intent\nclass serves as context to the language model task.\nWe refer to this design as model with local intent\ncontext.\nThe model in Figure 4(c) extends the basic joint\nmodel by feeding the intent class back to the RNN\nstate. The history and variations of the predicted\nintent class from each previous step are monitored\nby the mode with such class output connections to\nRNN state. The intent, slot label, and next word\npredictions in the following step are all dependent\non this history of intents. We refer to this design\nas model with recurrent intent context.\nThe model in Figure 4(d) combines the two\ntypes of connections shown in Figure 4(b) and\n4(c). At step t, in addition to the recurrent intent\ncontext (c<t), the prediction of word wt+1 is also\nconditioned on the local intent contextfrom cur-\nrent step intent class ct. We refer to this design as\nmodel with local and recurrent intent context.\n4 Experiments\n4.1 Data\nWe used the Airline Travel Information Systems\n(ATIS) dataset (Hemphill et al., 1990) in our ex-\nperiment. The ATIS dataset contains audio record-\nings of people making ﬂight reservations, and it is\nwidely used in spoken language understanding re-\nsearch. We followed the same ATIS corpus1 setup\nused in (Mesnil et al., 2015; Xu and Sarikaya,\n2013; Tur et al., 2010). The training set contains\n4978 utterances from ATIS-2 and ATIS-3 corpora,\nand test set contains 893 utterances from ATIS-3\nNOV93 and DEC94 datasets. We evaluated the\nsystem performance on slot ﬁlling (127 distinct\nslot labels) using F1 score, and the performance on\n1We thank Gokhan Tur and Puyang Xu for sharing the\nATIS dataset.\nintent detection (18 different intents) using classi-\nﬁcation error rate.\nIn order to show the robustness of the proposed\njoint SLU-LM model, we also performed experi-\nments using automatic speech recognition (ASR)\noutputs. We managed to retrieve 518 (out of\nthe 893 test utterances) utterance audio ﬁles from\nATIS-3 NOV93 and DEC94 data sets, and use\nthem as the test set in the ASR settings. To provide\na more challenging and realistic evaluation, we\nused the simulated noisy utterances that were gen-\nerated by artiﬁcially mixing clean speech data with\nnoisy backgrounds following the simulation meth-\nods described in the third CHiME Speech Sepa-\nration and Recognition Challenge (Barker et al.,\n2015). The average signal-to-noise ratio for the\nsimulated noisy utterances is 9.8dB.\n4.2 Training Procedure\nWe used LSTM cell as the basic RNN unit, follow-\ning the LSTM design in (Zaremba et al., 2014).\nThe default forget gate bias was set to 1. We\nused single layer uni-directional LSTM in the pro-\nposed joint online SLU-LM model. Deeper mod-\nels by stacking the LSTM layers are to be explored\nin future work. Word embeddings of size 300\nwere randomly initialized and ﬁne-tuned during\nmodel training. We conducted mini-batch train-\ning (with batch size 16) using Adam optimization\nmethod following the suggested parameter setup\nin (Kingma and Ba, 2014). Maximum norm for\ngradient clipping was set to 5. During model train-\ning, we applied dropout (dropout rate 0.5) to the\nnon-recurrent connections (Zaremba et al., 2014)\nof RNN and the hidden layers of MLPs, and ap-\nplied L2 regularization (λ= 10−4) on the param-\neters of MLPs.\nFor the evaluation in ASR settings, we\nused the acoustic model trained on LibriSpeech\ndataset (Panayotov et al., 2015), and the language\nmodel trained on ATIS training corpus. A 2-gram\nlanguage model was used during decoding. Dif-\nferent N-best rescoring methods were explored by\nusing a 5-gram language model, the independent\ntraining RNN language model, and the joint train-\ning RNN language model. The ASR outputs were\nthen sent to the joint SLU-LM model for intent de-\ntection and slot ﬁlling.\nModel Intent Error F1 Score LM PPL\n1 RecNN (Guo et al., 2014) 4.60 93.22 -\n2 RecNN+Viterbi (Guo et al., 2014) 4.60 93.96 -\n3 Independent training RNN intent model 2.13 - -\n4 Independent training RNN slot ﬁlling model - 94.91 -\n5 Independent training RNN language model - - 11.55\n6 Basic joint training model 2.02 94.15 11.33\n7 Joint model with local intent context 1.90 94.22 11.27\n8 Joint model with recurrent intent context 1.90 94.16 10.21\n9 Joint model with local & recurrentintent context 1.79 94.18 10.22\n10 Joint model with local slot label context 1.79 94.14 11.14\n11 Joint model with recurrent slot label context 1.79 94.64 11.19\n12 Joint model with local & recurrentslot label context 1.68 94.52 11.17\n13 Joint model with local intent + slot label context 1.90 94.13 11.22\n14 Joint model with recurrent intent + slot label context 1.57 94.47 10.19\n15 Joint model with local & recurrentintent + slot label context 1.68 94.45 10.28\nTable 1: ATIS Test set results on intent detection error, slot ﬁlling F1 score, and language modeling\nperplexity. Related joint models: RecNN: Joint intent detection and slot ﬁlling model using recursive\nneural network (Guo et al., 2014). RecNN+Viterbi: Joint intent detection and slot ﬁlling model using\nrecursive neural network with Viterbi sequence optimization for slot ﬁlling (Guo et al., 2014).\n4.3 Results and Discussions\n4.3.1 Results with True Text Input\nTable 1 summarizes the experiment results of the\njoint SLU-LM model and its variations using ATIS\ntext corpus as input. Row 3 to row 5 are the inde-\npendent training model results on intent detection,\nslot ﬁlling, and language modeling. Row 6 gives\nthe results of the basic joint SLU-LM model (Fig-\nure 4(a)). The basic joint model uses a shared rep-\nresentation for all the three tasks. It gives slightly\nbetter performance on intent detection and next\nword prediction, with some degradation on slot\nﬁlling F1 score. If the RNN output ht is con-\nnected to each task output directly via linear pro-\njection without using MLP, performance drops for\nintent classiﬁcation and slot ﬁlling. Thus, we be-\nlieve the extra discriminative power introduced by\nthe additional model parameters and non-linearity\nfrom MLP is useful for the joint model. Row 7\nto row 9 of Table 1 illustrate the performance of\nthe joint models with local, recurrent, and local\nplus recurrent intent context, which correspond to\nmodel structures described in Figure 4(b) to 4(d).\nIt is evident that the recurrent intent context helps\nthe next word prediction, reducing the language\nmodel perplexity by 9.4% from 11.27 to 10.21.\nThe contribution of local intent context to next\nword prediction is limited. We believe the advan-\ntageous performance of using recurrent context is\na result of modeling predicted intent history and\nintent variations along with the growing word se-\nquence. For intent classiﬁcation and slot ﬁlling,\nperformance of these models with intent context\nis similar to that of the basic joint model.\nRow 10 to row 12 of Table 1 illustrate the per-\nformance of the joint model with local, recurrent,\nand local plus recurrent slot label context. Com-\nparing to the basic joint model, the introduced slot\nlabel context (both local and recurrent) leads to\na better language modeling performance, but the\ncontribution is not as signiﬁcant as that from the\nrecurrent intent context. Moreover, the slot la-\nbel context reduces the intent classiﬁcation error\nfrom 2.02 to 1.68, a 16.8% relative error reduc-\ntion. From the slot ﬁlling F1 scores in row 10 and\nrow 11, it is clear that modeling the slot label de-\npendencies by connecting slot label output to the\nrecurrent state is very useful.\nRow 13 to row 15 of Table 1 give the perfor-\nmance of the joint model with both intent and slot\nlabel context. Row 15 refers to the model de-\nscribed in Figure 3. As can be seen from the re-\nsults, the joint model that utilizes two types of\nrecurrent context maintains the beneﬁts of both,\nnamely, the beneﬁt of applying recurrent intent\ncontext to language modeling, and the beneﬁt of\nASR Model (with LibriSpeech AM) WER Intent Error F1 Score\n2-gram LM decoding 14.51 4.63 84.46\n2-gram LM decoding + 5-gram LM rescoring 13.66 5.02 85.08\n2-gram LM decoding + Independent training RNN LM rescoring 12.95 4.63 85.43\n2-gram LM decoding + Joint training RNN LM rescoring 12.59 4.44 86.87\nTable 2: ATIS test set results on ASR word error rate, intent detection error, and slot ﬁlling F1 score with\nnoisy speech input.\napplying recurrent slot label context to slot ﬁlling.\nAnother observation is that once recurrent context\nis applied, the beneﬁt of adding local context for\nnext word prediction is limited. It might hint that\nthe most useful information for the next word pre-\ndiction can be well captured in the RNN state, and\nthus adding explicit dependencies on local intent\nclass and slot label is not very helpful.\nFigure 6: LM perplexity of the joint SLU-LM\nmodels with different schedules in adjusting the\nintent contribution to the context vector.\nDuring the joint model training and inference,\nwe used a schedule to adjust the intent contribu-\ntion to the context vector by linearly scaling the in-\ntent vector with the growing input word sequence\nafter step k. We found this technique to be criti-\ncal in achieving advantageous language modeling\nperformance. Figure 6 shows test set perplexities\nalong the training epochs for models using differ-\nent kvalues, comparing to the model with uniform\n(η = 1) intent contribution. With uniform intent\ncontribution across time, the context vector does\nnot bring beneﬁt to the next word prediction, and\nthe language modeling perplexity is similar to that\nof the basic joint model. By applying the adjusted\nintent scale ( k = 2), the perplexity drops from\n11.26 (with uniform intent contribution) to 10.29,\nan 8.6% relative reduction.\n4.3.2 Results in ASR Settings\nTo further evaluate the robustness of the proposed\njoint SLU-LM model, we experimented with noisy\nspeech input and performed SLU on the rescored\nASR outputs. Model performance is evaluated in\nterms of ASR word error rate (WER), intent clas-\nsiﬁcation error, and slot ﬁlling F1 score. As shown\nin Table 2, the model with joint training RNN LM\nrescoring outperforms the models using 5-gram\nLM rescoring and independent training RNN LM\nrescoring on all the three evaluation metrics. Us-\ning the rescored ASR outputs (12.59% WER) as\ninput to the joint training SLU model, the intent\nclassiﬁcation error increased by 2.87%, and slot\nﬁlling F1 score dropped by 7.77% comparing to\nthe setup using true text input. The performance\ndegradation is expected as we used a more chal-\nlenging and realistic setup with noisy speech in-\nput. These results in Table 2 show that our joint\ntraining model outperforms the independent train-\ning model consistently on ASR and SLU tasks.\n5 Conclusion\nIn this paper, we propose a conditional RNN\nmodel that can be used to jointly perform on-\nline spoken language understanding and language\nmodeling. We show that by continuously mod-\neling intent variation and slot label dependencies\nalong with the arrival of new words, the joint train-\ning model achieves advantageous performance in\nintent detection and language modeling with slight\ndegradation on slot ﬁlling comparing to the in-\ndependent training models. On the ATIS bench-\nmarking data set, our joint model produces 11.8%\nrelative reduction on LM perplexity, and 22.3%\nrelative reduction on intent detection error when\nusing true text as input. The joint model also\nshows consistent performance gain over the in-\ndependent training models in the more challeng-\ning and realistic setup using noisy speech input.\nCode to reproduce our experiments is available at:\nhttp://speech.sv.cmu.edu/software.html\nReferences\nJon Barker, Ricard Marxer, Emmanuel Vincent, and\nShinji Watanabe. 2015. The third chime speech sep-\naration and recognition challenge: Dataset, task and\nbaselines. In 2015 IEEE Automatic Speech Recog-\nnition and Understanding Workshop (ASRU 2015).\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and\nNoam Shazeer. 2015. Scheduled sampling for se-\nquence prediction with recurrent neural networks.\nIn Advances in Neural Information Processing Sys-\ntems, pages 1171–1179.\nWilliam Chan, Navdeep Jaitly, Quoc V Le, and Oriol\nVinyals. 2015. Listen, attend and spell. arXiv\npreprint arXiv:1508.01211.\nRonan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th international conference on\nMachine learning, pages 160–167. ACM.\nDaniel Guo, Gokhan Tur, Wen-tau Yih, and Geoffrey\nZweig. 2014. Joint semantic utterance classiﬁca-\ntion and slot ﬁlling with recursive neural networks.\nIn Spoken Language Technology Workshop (SLT),\n2014 IEEE, pages 554–559. IEEE.\nPatrick Haffner, Gokhan Tur, and Jerry H Wright.\n2003. Optimizing svms for complex call classiﬁ-\ncation. In Acoustics, Speech, and Signal Process-\ning, 2003. Proceedings.(ICASSP’03). 2003 IEEE In-\nternational Conference on, volume 1, pages I–632.\nIEEE.\nCharles T Hemphill, John J Godfrey, and George R\nDoddington. 1990. The atis spoken language sys-\ntems pilot corpus. In Proceedings of the DARPA\nspeech and natural language workshop, pages 96–\n101.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nNavdeep Jaitly, Quoc V Le, Oriol Vinyals, Ilya\nSutskeyver, and Samy Bengio. 2015. An on-\nline sequence-to-sequence model using partial con-\nditioning. arXiv preprint arXiv:1511.04868.\nYoon Kim. 2014. Convolutional neural net-\nworks for sentence classiﬁcation. arXiv preprint\narXiv:1408.5882.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nBing Liu and Ian Lane. 2015. Recurrent neural net-\nwork structured output prediction for spoken lan-\nguage understanding. In Proc. NIPS Workshop\non Machine Learning for Spoken Language Under-\nstanding and Interactions.\nAndrew McCallum, Dayne Freitag, and Fernando CN\nPereira. 2000. Maximum entropy markov mod-\nels for information extraction and segmentation. In\nICML, volume 17, pages 591–598.\nGr´egoire Mesnil, Yann Dauphin, Kaisheng Yao,\nYoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-\naodong He, Larry Heck, Gokhan Tur, Dong Yu,\net al. 2015. Using recurrent neural networks for\nslot ﬁlling in spoken language understanding. Au-\ndio, Speech, and Language Processing, IEEE/ACM\nTransactions on, 23(3):530–539.\nTom´aˇs Mikolov, Stefan Kombrink, Luk ´aˇs Burget,\nJan Honza ˇCernock`y, and Sanjeev Khudanpur.\n2011. Extensions of recurrent neural network lan-\nguage model. In Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2011 IEEE International Confer-\nence on, pages 5528–5531. IEEE.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and\nSanjeev Khudanpur. 2015. Librispeech: an asr cor-\npus based on public domain audio books. In Acous-\ntics, Speech and Signal Processing (ICASSP), 2015\nIEEE International Conference on, pages 5206–\n5210. IEEE.\nSuman Ravuri and Andreas Stolcke. 2015. Recurrent\nneural network and lstm models for lexical utterance\nclassiﬁcation. In Sixteenth Annual Conference of the\nInternational Speech Communication Association.\nChristian Raymond and Giuseppe Riccardi. 2007.\nGenerative and discriminative algorithms for spoken\nlanguage understanding. In INTERSPEECH, pages\n1605–1608.\nMartin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.\n2012. Lstm neural networks for language modeling.\nIn INTERSPEECH, pages 194–197.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nGokhan Tur, Dilek Hakkani-Tur, and Larry Heck.\n2010. What is left to be understood in atis? In Spo-\nken Language Technology Workshop (SLT), 2010\nIEEE, pages 19–24. IEEE.\nPuyang Xu and Ruhi Sarikaya. 2013. Convolutional\nneural network based triangular crf for joint in-\ntent detection and slot ﬁlling. In Automatic Speech\nRecognition and Understanding (ASRU), 2013 IEEE\nWorkshop on, pages 78–83. IEEE.\nKaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Ge-\noffrey Zweig, and Yangyang Shi. 2014. Spoken lan-\nguage understanding using long short-term memory\nneural networks. In Spoken Language Technology\nWorkshop (SLT), 2014 IEEE, pages 189–194. IEEE.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329."
}