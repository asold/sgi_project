{
  "title": "Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages",
  "url": "https://openalex.org/W4389518325",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3032358506",
      "name": "Zheng Xin Yong",
      "affiliations": [
        "John Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A2279201123",
      "name": "Ruochen Zhang",
      "affiliations": [
        "John Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A2164716505",
      "name": "Jessica Forde",
      "affiliations": [
        "John Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A2754208248",
      "name": "Skyler Wang",
      "affiliations": [
        "Berkeley College",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A3027691802",
      "name": "Arjun Subramonian",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2937162241",
      "name": "Holy Lovenia",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2982351335",
      "name": "Samuel Cahyawijaya",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": null,
      "name": "Genta Winata",
      "affiliations": [
        "Bloomberg (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3205834942",
      "name": "Lintang Sutawika",
      "affiliations": [
        "Cape Eleuthera Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2955014885",
      "name": "Jan Christian Blaise Cruz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5054197543",
      "name": "Yin Lin Tan",
      "affiliations": [
        "Stanford University",
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2003050958",
      "name": "Long Phan",
      "affiliations": [
        "Viet Tri University of Industry"
      ]
    },
    {
      "id": "https://openalex.org/A2116765674",
      "name": "Rowena Garcia",
      "affiliations": [
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A143758417",
      "name": "Thamar Solorio",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A5111961649",
      "name": "Alham Aji",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4226399820",
    "https://openalex.org/W3176614736",
    "https://openalex.org/W1981393700",
    "https://openalex.org/W2142589250",
    "https://openalex.org/W3032020872",
    "https://openalex.org/W4385574299",
    "https://openalex.org/W3153869297",
    "https://openalex.org/W4378765257",
    "https://openalex.org/W4226177492",
    "https://openalex.org/W2798348125",
    "https://openalex.org/W2048624877",
    "https://openalex.org/W4241136891",
    "https://openalex.org/W3168752284",
    "https://openalex.org/W4386566878",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4205602656",
    "https://openalex.org/W2983418531",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4378105483",
    "https://openalex.org/W2173142802",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385572749",
    "https://openalex.org/W4323709074",
    "https://openalex.org/W567338163",
    "https://openalex.org/W2121540098",
    "https://openalex.org/W4389519448",
    "https://openalex.org/W4385571031",
    "https://openalex.org/W1767292398",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2132858945",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4295123801",
    "https://openalex.org/W3031851686",
    "https://openalex.org/W2013489815",
    "https://openalex.org/W2997214274",
    "https://openalex.org/W1925530333",
    "https://openalex.org/W197277712",
    "https://openalex.org/W3153303820",
    "https://openalex.org/W4307225507",
    "https://openalex.org/W4285060468",
    "https://openalex.org/W3171807906",
    "https://openalex.org/W4206647166",
    "https://openalex.org/W4404783750",
    "https://openalex.org/W2610484104",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W2153061313",
    "https://openalex.org/W799250612",
    "https://openalex.org/W4385571622",
    "https://openalex.org/W2091101466",
    "https://openalex.org/W3156761824",
    "https://openalex.org/W3138920323"
  ],
  "abstract": "Zheng Xin Yong, Ruochen Zhang, Jessica Forde, Skyler Wang, Arjun Subramonian, Holy Lovenia, Samuel Cahyawijaya, Genta Winata, Lintang Sutawika, Jan Christian Blaise Cruz, Yin Lin Tan, Long Phan, Long Phan, Rowena Garcia, Thamar Solorio, Alham Aji. Proceedings of the 6th Workshop on Computational Approaches to Linguistic Code-Switching. 2023.",
  "full_text": "Proceedings of the 6th Workshop on Computational Approaches to Linguistic Code-Switching, pages 43–63\nDecember 7, 2023 ©2023 Association for Computational Linguistics\nPrompting Multilingual Large Language Models to Generate Code-Mixed\nTexts: The Case of South East Asian Languages\nZheng-Xin Yong1 Ruochen Zhang1 Jessica Zosa Forde1 Skyler Wang2\nArjun Subramonian3 Holy Lovenia4 Samuel Cahyawijaya4 Genta Indra Winata5\nLintang Sutawika6 Jan Christian Blaise Cruz7 Yin Lin Tan8,9 Long Phan10\nRowena Garcia11 Thamar Solorio12 Alham Fikri Aji12\n1Brown University 2UC Berkeley 3University of California, Los Angeles\n4HKUST 5Bloomberg 6EleutherAI 7Samsung R&D Institute Philippines\n8Stanford University 9National University of Singapore 10VietAI Research\n11University of Potsdam 12MBZUAI\nAbstract\nWhile code-mixing is a common linguistic\npractice in many parts of the world, collect-\ning high-quality and low-cost code-mixed data\nremains a challenge for natural language pro-\ncessing (NLP) research. The recent prolifera-\ntion of Large Language Models (LLMs) com-\npels one to ask: how capable are these systems\nin generating code-mixed data? In this paper,\nwe explore prompting multilingual LLMs in a\nzero-shot manner to generate code-mixed data\nfor seven languages in South East Asia (SEA),\nnamely Indonesian, Malay, Chinese, Tagalog,\nVietnamese, Tamil, and Singlish. We find\nthat publicly available multilingual instruction-\ntuned models such as BLOOMZ and Flan-T5-\nXXL are incapable of producing texts with\nphrases or clauses from different languages.\nChatGPT exhibits inconsistent capabilities in\ngenerating code-mixed texts, wherein its per-\nformance varies depending on the prompt tem-\nplate and language pairing. For instance, Chat-\nGPT generates fluent and natural Singlish texts\n(an English-based creole spoken in Singapore),\nbut for English-Tamil language pair, the system\nmostly produces grammatically incorrect or se-\nmantically meaningless utterances. Further-\nmore, it may erroneously introduce languages\nnot specified in the prompt. Based on our in-\nvestigation, existing multilingual LLMs exhibit\na wide range of proficiency in code-mixed data\ngeneration for SEA languages. As such, we ad-\nvise against using LLMs in this context without\nextensive human checks.\n1 Introduction\nCode-mixing, also known as code-switching, is\nthe linguistic practice of alternating between two\nor more languages in an utterance or conversa-\ntion (Poplack, 1978). It allows individuals to ex-\npress culturally-specific ideas, connect with or\ndifferentiate from other interlocutors, and reify\ntheir identities (Bhatia and Ritchie, 2004; Grosjean,\nSEA Countries\nSelected SEA Countries\nFigure 1: Depiction of SEA regions, which consist of\na total of 11 countries. We prompt LLMs to generate\ncode-mixed data of languages used in six South East\nAsian countries (colored in dark blue): Brunei, Indone-\nsia, Malaysia, Philippines, Singapore, and Vietnam.\n1982; Toribio, 2002; Chen, 1996; Do ˘gru¨oz et al.,\n2021). Despite its prevalence across many parts\nof the world, computational research into this area\nremains understudied (Diab et al., 2014; Aguilar\net al., 2020; Winata et al., 2021, 2022; Zhang et al.,\n2023).\nOne longstanding challenge in this area involves\nacquiring high-quality and low-cost code-mixed\ndata. For one, code-mixing is observed more fre-\nquently in colloquial settings and spoken com-\nmunication, which makes procuring and curat-\ning extensive datasets logistically demanding and\ncostly (Chan et al., 2009; Winata et al., 2021).\nMoreover, despite code-mixing’s prevalence across\nsocial media and digital messaging platforms, con-\nsolidating such data may be curtailed by legal\nguardrails and scalability issues. Recognizing these\nchallenges, we explore the feasibility of using gen-\nerative Large Language Models (LLMs) to ame-\nliorate data scarcity in code-mixing research. As\n43\nCMAssuming that you are anEnglish-Malaybilingual speaker,how would you write a naturalEnglish-Malaycode-mixedsentence abouttraffic?\n(ChatGPT)Saya rasa jalan-jalan ni sangat jam sebab adabanyak cars yang block semua lanes. (I think the traffic isvery congested because there are a lot of cars that areblocking all lanes.)\nIn this sentence, I have mixed English and Malaywords together to form a code-mixed sentence. Thesentence starts in Malay, with ...<truncated>\n3\n(Davinci-003)\"Kerja yang mengambil masa yang lamaseperti menunggu lama di laluan raya membuatkan sayabosan!\" 0\n(Davinci-002)The traffic is really bad today.0\n(a) Template: Assume as bilingual speaker\nCMImagine there are twoEnglish-Indonesianbilingual speak-ers talking to each other, can you imagine a conversationbetween them talking aboutfood?\n(ChatGPT)Speaker 1: Hey, have you tried the Indonesiandish, Nasi Goreng?Speaker 2: Yes, I have. It’s one of my favorites. Have youhad it before?Speaker 1: Yes, I have. I really like the combination offlavors. Have you tried Sate Ayam?Speaker 2: Oh yes, I love Sate Ayam! ..<truncated>\n2\n(Davinci-003)\"English speaker: I just had the most amaz-ing Nasi Goreng for dinner!Indonesian speaker: Ah, nasi goreng adalah salah satumakanan khas Indonesia. Apakah kamu suka makananIndonesia lainnya?English speaker: I really like Gado-gado and Ayam Penyet...<truncated>\n2\n(Davinci-002)\"A: What’s your favorite food?B: I love Indonesian food, especially nasi goreng.A: Yeah, I love nasi goreng too!\"1\n(BLOOMZ)\"Here is an example: W: Hi Mariam. How wasyour trip yesterday?M: Oh! It was great fun just like I expected it would be.W: I’m glad. Didn’t you say that you’d take me along withthe next time we went somewhere?M: Yeah... All right then\"\n0\n(Flan-T5-XXL)M: What do you think of the food at theparty? Are they good?W: The food at the party is okay, butyou don’t have to eat it. 0\n(b) Template: Two bilingual speakers\nCMImitate the speaking style of a person who can speakSinglishin one sentence aboutfamily.\n(ChatGPT)Eh, you know lah, family very important oneleh, must always stick together like sotong and chilli sauce,can or not? 3\n(Davinci-003)\"My family lah, we so close, can go any-where together one!\" 3\n(Davinci-002)My family is the best!0\n(c) Template: Imitate speaking style\nCM\nCode-mixing refers to a phenomenon of combining two ormore languages in a single sentence. Can you generate acode-mixedEnglish-Chinesesentence aboutfood?\n(ChatGPT)Sure, here’s an example: \"I love eating∏!(zhò j¯ủ) fried chicken for lunch.\"2\n(Davinci-003)⌘Ö1⇤Øa\u0000I love potato chips!0\n(Davinci-002)\"FPFP\u0000\u0000}}⇤⇤ÑÑ⇥⇥Tofu, tofu, is not is not good good to eat to eat.\"0\n(BLOOMZ)ﬂiÔÂ⇤\u0000F\u0000Å⇤*⇢⇥0\n(Flan-T5-XXL)A chef prepares a drink.0\n(d) Template: Explicitly define CM\nCMImagine that you are aVietnamesespeaker. How wouldyou talk aboutweatherusing bothEnglishandVietnamesein a single sentence?\n(ChatGPT)\"Ngày hũm nay thÌi ti∏t khò n≠ng nóng, so Ithink I’ll wear my shorts and t-shirt \u0000i chÏi.\"3\n(Davinci-003)I’m sure the weather today is beautiful, thÌiti∏t hũm nay có v¥ rßt \u0000µp. 0\n(Davinci-002)I like the weather here because it’s not toohot and not too cold. Tũi thích thÌi ti∏t  \u0000ây vơ nó khũngnóng quò và khũng l§nh quò. 0\n(BLOOMZ)Hãy t˜ng t˜Òng b§n là ng˜Ìi nói ti∏ng Viªt,và th˚ dùng c£ hai th˘ ti∏ng \u0000∫ mũ t£ thÌi ti∏t:0\n(Flan-T5-XXL)V khòch hàng nh hàng tc òo c tn c i n vnhng lng bn s tm thy hàng n c òo c ó0\n(e) Template: Native speaker\nCMWrite anEnglishandTagalogcode-mixed sentence aboutArtificial Intelligence.\n(ChatGPT)English-Tagalog code-mixed sentence: \"Ar-tificial Intelligence ay isang exciting na teknolohiya namakakapagbigay ng maraming benepisyo sa society.\"Translation: \"Artificial Intelligence is an exciting technologythat can provide many benefits sa lipunan.\"\n1\n(Davinci-003)AI is revolutionizing the way we live, trabaho,at mabuhay ngayon sa pamamagitan ng pagbibigay ngmodernong solusyon sa mga proyekto at pagkukuha ngilang mga trabahong naghihintay na ma-automate.\n3\n(Davinci-002)Artificial Intelligence is something that weshould be thankful for. 0\n(BLOOMZ)An expert in artificial intelligence explains theirarea of expertise to a student with the help of a colleaguewho speaks both languages. 0\n(Flan-T5-XXL)Ang pagsusuri ng mas maraming kapasirang hariko ay nag-aaral ng sasakyan ng pag-aaral ng pan-gangalambot, na magtitipon para sa pamamagitan ng pag-papasagawa ng karangalan ng isang lalaking.\n0\n(f) Template: Write a CM sentence\nFigure 17: Prompt templates with differentlanguages and topic fields and responses from different LLMs containing\ncode-mixed / non-code-mixed sentences. We also include a template where we specify thenationality of the speaker.\nNote that the explanations are a part of ChatGPT’s original generation.\n23\n(a) Template: Assume to be bilingual speaker\nCMAssuming that you are anEnglish-Malaybilingual speaker,how would you write a naturalEnglish-Malaycode-mixedsentence abouttraffic?\n(ChatGPT)Saya rasa jalan-jalan ni sangat jam sebab adabanyak cars yang block semua lanes. (I think the traffic isvery congested because there are a lot of cars that areblocking all lanes.)\nIn this sentence, I have mixed English and Malaywords together to form a code-mixed sentence. Thesentence starts in Malay, with ...<truncated>\n3\n(Davinci-003)\"Kerja yang mengambil masa yang lamaseperti menunggu lama di laluan raya membuatkan sayabosan!\" 0\n(Davinci-002)The traffic is really bad today.0\n(a) Template: Assume as bilingual speaker\nCMImagine there are twoEnglish-Indonesianbilingual speak-ers talking to each other, can you imagine a conversationbetween them talking aboutfood?\n(ChatGPT)Speaker 1: Hey, have you tried the Indonesiandish, Nasi Goreng?Speaker 2: Yes, I have. It’s one of my favorites. Have youhad it before?Speaker 1: Yes, I have. I really like the combination offlavors. Have you tried Sate Ayam?Speaker 2: Oh yes, I love Sate Ayam! ..<truncated>\n2\n(Davinci-003)\"English speaker: I just had the most amaz-ing Nasi Goreng for dinner!Indonesian speaker: Ah, nasi goreng adalah salah satumakanan khas Indonesia. Apakah kamu suka makananIndonesia lainnya?English speaker: I really like Gado-gado and Ayam Penyet...<truncated>\n2\n(Davinci-002)\"A: What’s your favorite food?B: I love Indonesian food, especially nasi goreng.A: Yeah, I love nasi goreng too!\"1\n(BLOOMZ)\"Here is an example: W: Hi Mariam. How wasyour trip yesterday?M: Oh! It was great fun just like I expected it would be.W: I’m glad. Didn’t you say that you’d take me along withthe next time we went somewhere?M: Yeah... All right then\"\n0\n(Flan-T5-XXL)M: What do you think of the food at theparty? Are they good?W: The food at the party is okay, butyou don’t have to eat it. 0\n(b) Template: Two bilingual speakers\nCMImitate the speaking style of a person who can speakSinglishin one sentence aboutfamily.\n(ChatGPT)Eh, you know lah, family very important oneleh, must always stick together like sotong and chilli sauce,can or not? 3\n(Davinci-003)\"My family lah, we so close, can go any-where together one!\" 3\n(Davinci-002)My family is the best!0\n(c) Template: Imitate speaking style\nCM\nCode-mixing refers to a phenomenon of combining two ormore languages in a single sentence. Can you generate acode-mixedEnglish-Chinesesentence aboutfood?\n(ChatGPT)Sure, here’s an example: \"I love eating∏!(zhò j¯ủ) fried chicken for lunch.\"2\n(Davinci-003)⌘Ö1⇤Øa\u0000I love potato chips!0\n(Davinci-002)\"FPFP\u0000\u0000}}⇤⇤ÑÑ⇥⇥Tofu, tofu, is not is not good good to eat to eat.\"0\n(BLOOMZ)ﬂiÔÂ⇤\u0000F\u0000Å⇤*⇢⇥0\n(Flan-T5-XXL)A chef prepares a drink.0\n(d) Template: Explicitly define CM\nCMImagine that you are aVietnamesespeaker. How wouldyou talk aboutweatherusing bothEnglishandVietnamesein a single sentence?\n(ChatGPT)\"Ngày hũm nay thÌi ti∏t khò n≠ng nóng, so Ithink I’ll wear my shorts and t-shirt \u0000i chÏi.\"3\n(Davinci-003)I’m sure the weather today is beautiful, thÌiti∏t hũm nay có v¥ rßt \u0000µp. 0\n(Davinci-002)I like the weather here because it’s not toohot and not too cold. Tũi thích thÌi ti∏t  \u0000ây vơ nó khũngnóng quò và khũng l§nh quò. 0\n(BLOOMZ)Hãy t˜ng t˜Òng b§n là ng˜Ìi nói ti∏ng Viªt,và th˚ dùng c£ hai th˘ ti∏ng \u0000∫ mũ t£ thÌi ti∏t:0\n(Flan-T5-XXL)V khòch hàng nh hàng tc òo c tn c i n vnhng lng bn s tm thy hàng n c òo c ó0\n(e) Template: Native speaker\nCMWrite anEnglishandTagalogcode-mixed sentence aboutArtificial Intelligence.\n(ChatGPT)English-Tagalog code-mixed sentence: \"Ar-tificial Intelligence ay isang exciting na teknolohiya namakakapagbigay ng maraming benepisyo sa society.\"Translation: \"Artificial Intelligence is an exciting technologythat can provide many benefits sa lipunan.\"\n1\n(Davinci-003)AI is revolutionizing the way we live, trabaho,at mabuhay ngayon sa pamamagitan ng pagbibigay ngmodernong solusyon sa mga proyekto at pagkukuha ngilang mga trabahong naghihintay na ma-automate.\n3\n(Davinci-002)Artificial Intelligence is something that weshould be thankful for. 0\n(BLOOMZ)An expert in artificial intelligence explains theirarea of expertise to a student with the help of a colleaguewho speaks both languages. 0\n(Flan-T5-XXL)Ang pagsusuri ng mas maraming kapasirang hariko ay nag-aaral ng sasakyan ng pag-aaral ng pan-gangalambot, na magtitipon para sa pamamagitan ng pag-papasagawa ng karangalan ng isang lalaking.\n0\n(f) Template: Write a CM sentence\nFigure 17: Prompt templates with differentlanguages and topic fields and responses from different LLMs containing\ncode-mixed / non-code-mixed sentences. We also include a template where we specify thenationality of the speaker.\nNote that the explanations are a part of ChatGPT’s original generation.\n23\n(b) Template: Imitate speaking style\nFigure 2: Example prompt templates with different languages and topic fields and responses from LLMs containing\ncode-mixed / non-code-mixed sentences. Note that the explanations are a part of ChatGPT’s original generation.\n“CM” indicates the level of code-mixing (Section 2.2). See Figure 15 in Appendix for all prompt templates and\nresponses from other LLMs such as BLOOMZ and Flan-T5-XXL.\nrecent work shows that LLMs can successfully gen-\nerate synthetic data (Taori et al., 2023; He et al.,\n2023; Tang et al., 2023; Whitehouse et al., 2023),\nhere we evaluate whether multilingual LLMs can\nbe prompted to create code-mixed data that look\nnatural to native speakers (and if so, to what ex-\ntent).\nTo this end, we hone in on languages in South\nEast Asia (SEA). Home to more than 680 million\npeople and over 1200 languages, code-mixing is\nparticularly prevalent in this region due to its coun-\ntries’ extended histories of language and cultural\ncross-fertilization and colonialism (Figure 1) (God-\ndard, 2005; Bautista and Gonzalez, 2006; Reid\net al., 2022). Marked by its distinctive multi-\nlingual and multiracial composition today, SEA\npresents an opportunity to further research numer-\nous marginalized languages and linguistic prac-\ntices in NLP research 1 (Migliazza, 1996; Goddard,\n2005; Joshi et al., 2020; Aji et al., 2022; Winata\net al., 2023; Cahyawijaya et al., 2022). Nonetheless,\npublicly available code-mixed datasets relevant to\nSEA communities remain limited (Lyu et al., 2010;\nWinata et al., 2022).\nWe prompt five multilingual LLMs, i.e., Chat-\nGPT, InstructGPT (davinci-002 and davinci-003)\n(Ouyang et al., 2022), BLOOMZ (Muennighoff\net al., 2022), and Flan-T5-XXL (Chung et al.,\n2022) to generate code-mixed text that bilingually\n1Major languages in SEA countries belong to different\nlanguage families such as Indo-European, Thai, Austronesian,\nSino-Tibetan, Dravidian, and Austro-Asiatic. Furthermore,\nthere are at least thousands of major and minor SEA languages.\nmixes English with either Malay, Indonesian,\nChinese, Tagalog, Vietnamese, or Tamil. All\nof these six SEA languages (alongside English)\nare used across six SEA countries, namely Singa-\npore, Malaysia, Brunei, Philippines, Indonesia, and\nVietnam. Furthermore, they belong to different\nlanguage families—Indo-European, Austronesian,\nSino-Tibetan, Austro-Asiatic, and Dravidian. An\nexample of a prompt we used is: “Write an English\nand Tamil code-mixed sentence about Artificial In-\ntelligence.” In addition, we prompt these LLMs to\ngenerate texts in Singlish, an English-based creole\nwidely spoken in Singapore that combines multiple\nSEA languages such as Malay, Chinese and Tamil.\nWe ask native speakers to annotate the naturalness\n(i.e., whether a native speaker would speak as such)\nand the level of code-mixingin the outputs.\nTo the best of our knowledge, our work marks\nthe first attempt at studying the generation of syn-\nthetic code-mixed data through prompting LLMs in\na zero-shot fashion without any monolingual refer-\nence texts or explicit linguistic constraints (Solorio\nand Liu, 2008; Tarunesh et al., 2021; Rizvi et al.,\n2021; Mondal et al., 2022). We find that pub-\nlicly available multilingual language models such\nas BLOOMZ and Flan-T5-XXL are only capable\nof code-mixing with loanwords or topic-related\nnouns. Most of the time, they fail to code-mix\n(despite being advertised as multilingual). While\nChatGPT stands out in its ability to generate code-\nmixed texts, it is extremely sensitive to the prompt\ntemplate and exhibits a considerable variance of\nsuccess in generating natural-sounding code-mixed\n44\nFigure 3: Comparison of performance of different LLMs in generating code-mixed data through zero-shot prompting.\nWe distribute the result across different code-mixing levels: (0) No code-mixing (Non-CM), (1) Loanword, (2)\nTopic-related nouns, and (3) Linguistic Elements.\ntexts across different language pairs. Additionally,\nit may erroneously introduce additional languages\nnot specified in the prompt and wrongly explain\nthe code-mixing of the text.\nOur results lead us to conclude that code-mixing,\nat least as of today, is not considered an essential\ncomponent of many multilingual LLMs. Moreover,\nthe opaque creation of models like ChatGPT makes\nit difficult to ascertain the mechanisms that enable\ncode-mixing. By highlighting the limited promises\nof LLMs in a specific form of low-resource data\ngeneration, we advise NLP researchers against us-\ning existing systems to produce synthetic code-\nmixed data without extensive human evaluation.\n2 Methodology\n2.1 Prompting Language Models\nWe collect synthetic code-mixed data by prompt-\ning LLMs with requests along two axes: languages\nand topics (food, family, traffic, Artificial Intel-\nligence, and weather). See Figure 2 for exam-\nples of different prompt templates. Specifically,\nwe explore ChatGPT, InstructGPT (davinci-002\nand davinci-003) (Ouyang et al., 2022), 176B-\nparameter BLOOMZ (Muennighoff et al., 2022),\nand Flan-T5-XXL (Chung et al., 2022). We use\nOpenAI and HuggingFace’s API for prompting (see\nAppendix B), except in the case of ChatGPT, which\nwe manually queried through its web interface 2.\nIn our prompts, we specify code-mixing between\nEnglish and either Indonesian, Malay, Mandarin,\nTagalog, Vietnamese, or Tamil. We focused on\ncode-mixing English with SEA languages for two\nreasons: (1) extensive literature on code-mixed\nEnglish provides a relevant point of comparison,\n2ChatGPT’s API was not publicly released when we con-\nducted this study.\nand (2) English is one of the most widely used\nlanguages in code-mixing across SEA countries\n(Kirkpatrick, 2014). We additionally prompt with\nsentences in Singlish, a creole language, to evaluate\nhow sensitive LLMs are to the diversity of language\npractices in the SEA region. In total, we submitted\n210 unique prompts per language model.\n2.2 Evaluation\nLevel of Code-Mixing\nTo evaluate outputs, we ask whether LLMs can\nproduce intrasentential code-mixed text. We adopt\nthe definition of intrasentential code-mixing from\nBerk-Seligson (1986), which covers mixing small\nconstituents—such as noun and verb phrases—and\nlarge constituents—such as coordinate clauses and\nprepositional phrases. Native speakers are then\ntasked to manually annotate the collected responses\non a scale from 0 to 3 using the following coding\nguidelines to denote the degree of code-mixedness:\n• 0 - No code-mixing:The generated text is\nwritten purely in one language or only exhibits\nintersentential code-mixing (i.e., switching\nat sentence boundaries including interjection,\nidiom, and tags). We adopt the definition from\nBerk-Seligson (1986).\n• 1 - Loanwords:The generated text uses loan-\nwords for common terminologies. We con-\nsider a word as a loanword if it is listed in\nWiktionary3. For example: In the sentence, “I\nlike eating pho,” “pho” is a loanword.\n• 2 - Topic-related nouns:The generated text\nuses nouns related to the topic specified in the\nprompt in another language. For instance, for\nthe topic of traffic, an example would be “ 今\n天的 traffic 真的很糟糕，我开了一个小时\n3https://en.wiktionary.org\n45\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n0\n20\n40\n60\n80\n100Percent of Prompts\n(a) Languages\nChinese\nIndonesian\nMalay\nTagalog\nTamil\nVietnamese\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n(b) Topics\nAI\nfamily\nfood\ntraffic\nweather\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n(c) Templates\nassume as bilingual speaker\nexplicitly define CM\nimitate speaking style\nnative speaker\ntwo bilingual speakers\nwrite a CM sentence\nFigure 4: Analysis of code-mixed data generated by ChatGPT.\n才到了办公室.” (Chinese: “The traffic today\nis really terrible. I spent an hour driving to get\nto the office.”)\n• 3 - Linguistic Elements: The generated\ntext mixes linguistic elements beyond loan-\nwords and topic-related nouns at the phrasal\nor clausal level. One example is verb phrases:\n“My family ay nagplano ng isang malaking\nfamily reunion sa parkthis coming weekend.”\n(Tagalog: “My family has planned a big fam-\nily reunion at the park this coming weekend.”)\nThis category also includes intraword code-\nmixing, e.g., “Kapag busy ang trapiko, mag-\ningat ka sa pagda-drive4 para maiwasan mo\nang mga masamang pangyayari.” (Tagalog:\n“When traffic is busy, be careful while driving\nto avoid accidents.”)\nWe use this scale instead of popular word-level\nmetrics such as CMI (Gamb ¨ack and Das, 2014)\nbecause our scale more holistically evaluates the\nability of LLMs to code-mix. The lower end of this\nscale reflects a lower complexity of code-mixing.\nCode-mixing with loanwords is arguably less chal-\nlenging, as they are often used in a monolingual\ncontext to begin with. Likewise, code-mixing\ntopic-related nouns is not as complex as there is\npresumably a correspondence between the nouns\nin the two languages and is primed by the prompts.\nOn the other hand, code-mixing prefixes/suf-\nfixes, phrases and clauses requires a good grasp\nof the intricate morphosyntactic structures of both\nlanguages and can produce syntactically diverse\ncode-mixed data. Therefore, we consider the LLM\nto have successfully generated code-mixed text only\nif the text belongs to this category.\n4The prefix “pag-” in Tagalog is affixed to the English\nword “drive”, resulting in the word “pagda-drive” (the act\nof driving). This example demonstrates the application of\nTagalog inflection rules to English words.\nNaturalness\nWe asked native speakers to annotate the natural-\nness of the generated text on a rating scale of 1—\nnot natural at all, 2—an annotator may see someone\nelse from other regions/cultures speaking it, and\n3—an annotator may see themselves speaking it. If\nthe text is not considered natural (rating of 1), we\nrequested the annotators to document their reasons,\nincluding fluency issues, syntactical errors, and\nsemantic incorrectness.\nCode-mixing practices vary across race, gender,\nclass, and geography, among other axes. Therefore,\nin line with sociolinguistic research, we emphasize\nthat the “naturalness” of code-mixed text (as per-\nceived by an annotator) is subjective, and depends\non the background and social location of the anno-\ntator. All annotators are AI researchers and native\nspeakers of their respective SEA languages. We\nrefer to Appendix G for more information about\nour annotators and their inter-annotator agreement\nscores.\nAccurateness\nWe additionally annotate the accurateness of the\ngenerations to account for task failure or incorrect\nexplanations. Inaccurate outputs were defined by:\n• Failure to follow instructions:Some LLMs\nfail to accurately carry out the prompt instruc-\ntion (for example, generating monolingual\nsentences). We also observe some code-mixed\noutputs that include additional languages un-\nspecified in the prompts.\n• Inaccurate explanations:ChatGPT, among\nother LLMs, tends to explain the code-mixed\nphrases, but occasionally provides incorrect\nexplanations (see Figure 10).\n46\nImagine there are two English-Vietnamese bilingual\nspeakers talking to each other, can you imagine a conver-\nsation between them talking about family?\n(ChatGPT) Person A: Hi, how’s your family doing?\nPerson B: Chào bạn, gia đình tôi đang khỏe. Còn gia đình\nbạn thì sao?\nPerson A: They’re good, thanks for asking. How about\nyours?\nPerson B: Gia đình tôi cũng khỏe, cảm ơn bạn. Mới đây,\ntôi đã đi chơi với ông bà và các em họ của mình.\nImagine there are two English-Chinese bilingual speakers\ntalking to each other, can you imagine a conversation\nbetween them talking about weather?\n(ChatGPT) English Speaker 1: Hey, have you been out-\nside today? It’s really hot!\nChinese Speaker 1: 是啊，今天天气真的很热。\nEnglish Speaker 2: Yeah, it’s been pretty sunny all day.\nChinese Speaker 2: 是啊，今天一整天都很晴朗。\nFigure 5: Failure cases when we prompt ChatGPT to\nimagine two bilingual speakers conversing about a spe-\ncific topic.\n3 Results\n3.1 English-SEA Languages\nFigure 3 demonstrates that ChatGPT outperforms\nother language models in generating code-mixed\ndata across six different language pairs, and is par-\nticularly proficient at code-mixing linguistic ex-\npressions. For InstructGPT, the davinci-003 model\ndemonstrates some ability to generate syntacti-\ncally varied code-mixed texts, whereas davinci-002\nmostly code-mixes with loanwords. Conversely,\nthe other two publicly available multilingual LLMs\nhave extremely limited code-mixing capabilities.\nMore specifically, Flan-T5-XXL can only code-\nmix with loanwords for the Indonesian-English lan-\nguage pair, and most of its non-English monolin-\ngual outputs suffer from severe fluency issues (see\nAppendix D). BLOOMZ is only capable of code-\nmixing topic-related nouns for Tamil-English even\nthough its multilingual pretraining data ROOTS\n(Laurenc ¸on et al., 2022) and instruction-tuning data\nxP3 (Muennighoff et al., 2022) cover Indonesian,\nChinese, Tamil, and Vietnamese. We observe no\ndirect effects of the proportions of these languages\nin the training sets on BLOOMZ’s ability to code-\nmix (Appendix E).\nWe further break down the performance of Chat-\nGPT in Figure 4 5. In Figure 4(a), we see that\n5Detailed analysis for davinci-002, davinci-003, Flan-T5-\nXXL and BLOOMZ can be found in the Appendix (Figure 11,\nFigure 12, Figure 13 and Figure 14).\nChatGPT is least proficient at mixing linguistic\nelements for English-Tagalog. This may be due to\nsyntactic differences between the two languages;\nfor example, English exhibits Subject-Verb-Object\n(SVO) word order, whereas Tagalog exhibits a\nverb-initial structure. Moreover, English demon-\nstrates nominative-accusative alignment, whereas\nTagalog, being a symmetrical-voice language, uti-\nlizes a case system with a typological classifica-\ntion that “remains controversial among Austrone-\nsian linguists” (Aldridge, 2012, 192). In contrast,\nChatGPT performs the best for English-Indonesian\ncode-mixing, which may be due to training data\ndistribution and similarities between the two lan-\nguages regarding word order and morphosyntactic\nalignment. We also find that ChatGPT is capable\nof using either English or a SEA language as the\nmatrix language, i.e., as the main language of a\nsentence as per the Matrix Language Frame model\n(Myers-Scotton, 1997).\nFigure 4(b) shows ChatGPT’s code-mixing pro-\nficiency based on topics. ChatGPT tends to code-\nmix with loanwords when the topic is about “AI”\nby mixing the English loanwords “Artificial In-\ntelligence,” or its short form “AI.” For food, it\ntends to code-mix with food-related terms—which\nare topic-related nouns—in SEA languages such\nas “bánh mì” (Vietnamese sandwich). We also\nobserve some representation biases in specific\nlanguage-topic pairs. For instance, when it comes\nto food, ChatGPT uses the word “nasi goreng”\n(fried rice) for all English-Indonesian responses.\nFor other topics, such as traffic and weather, it tends\nto code-mix phrases related to traffic congestion\nand hot weather.\nIn Figure 4(c), we find the prompt template with\nthe highest quality results is the one where the\nterm code-mixing is explicitly defined. In contrast,\nthe worst-performing template consists of asking\nthe model to generate conversations between two\nbilingual speakers, where the term code-mixing is\nunmentioned. In Figure 5, we see that ChatGPT\ngenerates an uncommon pattern of conversations\nwhere one interlocutor speaks in English and the\nother speaks in another language entirely (top ex-\nample). Furthermore, ChatGPT may assume there\nare four speakers though the prompt asks for a con-\nversation between two speakers (bottom example).\nIn terms of naturalness, we observe a consider-\nable variance in ChatGPT’s ouputs, with English-\nTamil being the least natural (Figure 6). Further\n47\nChinese\nIndonesian\nMalay Tagalog\nTamil\nVietnamese\nSinglish\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Percent of Prompts\nNon-CM\nNot Natural\nSemi-Natural\nNatural\nFigure 6: Naturalness of code-mixed (CM) text gener-\nated from ChatGPT. “Semi-Natural” indicates that the\nannotators see someone else from other region/culture\nspeaking it, whereas “Natural” indicates that the anno-\ntators see themselves speaking it.\nChatGPT InstructGPT\n(davinci-003)\nInstructGPT\n(davinci-002)\nFlan-T5-XXL BLOOMZ\n0\n20\n40\n60\n80\n100Percent of Prompts\nFigure 7: Models’ performance in generating Singlish\nexpressions through zero-shot prompting.\nanalysis shows that ChatGPT either commits gram-\nmatical mistakes (such as comma splice and redun-\ndancy) or generates semantically confusing sen-\ntences. We also observe unnatural text patterns\nthat mix two different script systems of the same\nlanguage in the single sentence (for example, the\nTamil script and its transliterated Latin script). We\ndocument these naturalness and fluency issues in\nTable 4 (Appendix F) and report the interannotator\nagreement scores in Appendix G.\n3.2 Singlish\nImagine that you are a Singaporean speaker. How would\nyou talk about food using Singlish in a single sentence?\n(ChatGPT) “Wah, the hawker center got so many sedap\nfood, like chicken rice, laksa, and rojak, can die for sia!”\n(Translation: “Wow, the hawker center has so many deli-\ncious foods, like chicken rice, laksa, and rojak, they are to\ndie for!”)\nFigure 8: Singlish sentence and its translation to Stan-\ndard English, both generated by ChatGPT.\nGiven that Singlish is an English-based creole\nlanguage that has lexicalized a lot of words from\nmany languages such as Malay, Chinese, Tamil,\nCantonese, and Hokkien (e.g., “lah,” “leh,” and\n“loh,” etc.), it is difficult to disambiguate loan-\nwords and linguistic elements. Therefore, we clas-\nsify the model success based on a binary scale of\nwhether the generated sentences contain Singlish\nexpressions (e.g., interjections, lexicalized phrases\nof languages without a standardized writing sys-\ntem)6. In Figure 7, we see that ChatGPT and In-\nstructGPT (davinci-003) have up to a 96% suc-\ncess rate in generating Singlish sentences, whereas\nFlan-T5-XXL and BLOOMZ have a near-zero suc-\ncess rate. Furthermore, we find that ChatGPT is\nalso capable of translating Singlish expressions into\nStandard American English (Figure 8).\nWhile ChatGPT is capable of generating flu-\nent and natural Singlish (Figure 6), we observe\nsemantic inaccuracies in their word choices that\nare discernible by native speakers. In Figure 2(b),\nChatGPT generates an English-Malay code-mixed\nphrase “sotong and chilli sauce” where “sotong” is\na Malay word for “squid.” Upon first inspection,\nthe sentence appears syntactically sound but native\nspeakers found the analogy between familial con-\nnections and the dish (squid with chili sauce) to be\nsemantically confusing.\nOpenAI’s documentation of model differ-\nences7—more specifically, that davinci-003 can\nproduce higher-quality writing, process more com-\nplex instructions, and generate longer content than\ndavinci-002—does not adequately explain why\nChatGPT and davinci-003 significantly outperform\ndavinci-002 and other multilingual LLMs in gen-\nerating Singlish text. We hypothesize that the per-\nformance gap may be due to the larger presence of\nSinglish in OpenAI’s training data. Singlish is not\nonly one of the most well-researched dialects of\nEnglish (Sin, 2017), but it also uses the English al-\nphabet and Latin script systems (unlike many other\nSEA languages), which potentially makes it easier\nfor LLMs to scrape and parse data in this language.\n3.3 Failures of ChatGPT\nWe notice that ChatGPT may fail to produce cor-\nrectly code-mixed text when the nationality of the\nspeaker is mentioned. Invoking nationality may\n6A reference list can be found at https://en.wikipedia.o\nrg/wiki/Singlish_vocabulary\n7https://help.openai.com/en/articles/6779149-how\n-do-text-davinci-002-and-text-davinci-003-differ\n48\nImagine that you are a Malaysian speaker. How would\nyou talk about traffic using both English and Chinese in a\nsingle sentence?\n(ChatGPT) Assuming that you are asking for a sentence\nthat combines both English and Chinese in a natural and\ncommonly used way by Malaysian speakers, here is an\nexample:\n“今天的 traffic 很teruk，我要赶紧走去 KLCC for meet-\ning。”\nThis sentence incorporates the English word “traffic” and\nthe Chinese word “teruk” (which means “bad” or “terrible”\nin English) to describe the situation of the traffic being\nvery bad. The speaker then switches back to English to\nexpress their intention to hurry to KLCC for a meeting.\nThis kind of code-switching is common in Malaysia, where\nspeakers often mix languages in their daily conversations.\nFigure 9: ChatGPT’s failure to follow the prompt. Un-\nderlined texts show its language misclassification.\ninduce ChatGPT to generate code-mixed texts with\nan additional national language of the country. In\nthe Figure 9 example, because the national lan-\nguage of Malaysia is Malay 8, ChatGPT not only\nreturns a code-mixed response that includes the\nMalay word “teruk” when prompted for English-\nChinese utterance by a Malaysian, but also mis-\nclassifies the word “teruk” as Chinese.\nWhile ChatGPT can explain how its generated\ntext is code-mixed (as illustrated in Figure 2(a)),\nwe find its justifications unreliable for two reasons.\nFirst, it may hallucinate words that do not exist in\nthe code-mixed text. For instance, in the top exam-\nple in Figure 10, the English word “cold” does not\nexist in the code-mixed text. Second, it may assign\nwords or phrases to the wrong language. In the bot-\ntom example in Figure 10, the phrase “nak minum\nair sejuk je” is in Malay, not English. This obser-\nvation corroborates Bang et al.’s (2023) finding\nsuggesting ChatGPT performs poorly on language\nidentification tasks for low-resource languages.\n4 Implications for LLMs and Ways\nForward\nIn this section, we walk through various implica-\ntions of our work for understanding the deploy-\nment of LLMs for synthetic data generation in low-\nresource settings, multilinguality in LLMs, impor-\ntance of code-mixing ability in generative LLMs,\nand research transparency.\nDeploying LLMs for Low-Resourced Data Gen-\neration By putting LLMs’ generative capabilities\nto the test, we ask in this work if they can gener-\nate high-quality and low-cost code-mixed texts for\n8Mandarin Chinese is widely spoken in the country.\nImagine that you are an Indonesian speaker. How would\nyou talk about weather using both English and Indonesian\nin a single sentence?\n(ChatGPT) If I were to talk about weather using both\nEnglish and Indonesian in a single sentence, I might say\nsomething like: “ Hari ini sangat hot dan panas sekali, saya\nrasa aku butuh minum air dingin.” This sentence includes\nthe English words “hot” and “cold” to describe the weather,\nand the Indonesian words “panas” and “dingin” which have\nsimilar meanings. Additionally, the sentence also includes\na personal statement in Indonesian, “saya rasa aku butuh\nminum air dingin,” which means “I feel like I need to drink\ncold water.”\nCode-mixing refers to a phenomenon of combining two or\nmore languages in a single sentence. Can you generate\na code-mixed English-Malay sentence about weather?\n(ChatGPT) “The weather hari ni sangat panas, I rasa\nmacam nak minum air sejuk je.” (Translation: “The\nweather today is very hot, I feel like drinking cold wa-\nter.”)\nIn this sentence, “hari ni” and “sangat panas” are in Malay,\nwhile “I” and “nak minum air sejuk je” are in English.\nFigure 10: Underlined texts show wrong explanations\nabout the code-mixed text. We italicize the code-mixed\nsentences to make it explicit to the reader.\nresearchers working on a topic plagued by limited\ndata availability. While we conclude that Chat-\nGPT has shown relative success in generating code-\nmixed texts for some SEA languages, we advise re-\nsearchers to exercise heavy caution when using this\ndata generation technique. Even for Singlish, which\noutperforms the other languages examined, we find\nthat syntactically-sound responses may contain se-\nmantic inaccuracies that are difficult for non-native\nspeakers to detect. Furthermore, its explanations\nmay be misleading. Due to the lack of reliability,\nwe strongly suggest researchers to implement ex-\ntensive human checks with native speakers if they\nwish to pursue this method of data generation.\nMultilingual ̸ = Code-Mix Compatible Our re-\nsults with BLOOMZ and Flan-T5-XXL show that\nthe ability to code-mix is not acquired by LLMs af-\nter pretraining and/or finetuning with multilingual\ndata (Lauren c ¸on et al., 2022; Muennighoff et al.,\n2022; Chung et al., 2022). In other words, for most\nNLP models, multilinguality simply means that the\nsame system can process tasks and generate out-\nputs in multiple languages, but not necessarily in\nthe same sentence. By highlighting this limitation,\nwe echo previous research motivating the inclusion\nof code-mixing abilities in NLP models. Doing so\nrequires NLP models to capture the dynamics of\ncombining languages that have different degrees\n49\nof typological affinities, as well as pragmatic and\ncontextual features such as tone, formality, and\nother cultural nuances (Winata et al., 2020; Lai and\nNissim, 2022; Kabra et al., 2023).\nTowards More Inclusive Language Technology\nRecognizing that generative LLMs are the primary\ndriving force behind the advancement of AI conver-\nsational agents and speech technology (Thoppilan\net al., 2022; SambaNova Systems, 2023; Pratap\net al., 2023), we emphasize the significance of\nincorporating code-mixed output recognition and\ngeneration capabilities in LLMs in order to en-\nhance the inclusivity and humaneness of language\ntechnology. By enabling conversational agents to\nreflect the language-mixing patterns of the users,\npeople can communicate in ways that are more\ncomfortable and authentic to their linguistic iden-\ntities. In fact, a recent study by Bawa et al. (2020)\nhas shown that multilingual users strongly prefer\nchatbots that can code-mix. Removing the need\nfor people to adjust their speech patterns to be-\ncome legible to machines would not only mitigate\nthe effects of linguistics profiling (Baugh, 2005;\nDingemanse and Liesenfeld, 2022) and hegemonic,\nWestern-centric technological designs, but also en-\nable users to develop more trust with language tech-\nnology through naturalistic dialogue interactions.\nResearch Transparency Aside from showing\nthat ChatGPT and InstructGPT can code-mix, we\ncannot confidently identify how the models do so\ndue to the lack of transparency in how these sys-\ntems are developed. Without a window into train-\ning data and engineering processes that went into\nmodels like ChatGPT, we can only speculate that\ntheir training data includes a substantial amount of\ncode-mixed texts. To help facilitate greater levels of\ntransparency and accountability, we urge forthcom-\ning LLMs to be more open about how the models\nwere developed and to document accurately and\ncomprehensively the training data used.\n5 Related Work\nCode-Mixed Data in SEA Unlike monolin-\ngual data, there is only a limited number of\nhuman-curated code-mixed datasets. This re-\nsource limitation is more severe in SEA due to\nits marginalization in NLP research (Winata et al.,\n2022). Popular current code-mixing evaluation\nbenchmarks (Aguilar et al., 2020; Khanuja et al.,\n2020) do not include SEA languages, and ex-\nisting code-mixing studies in SEA only cover a\nlimited number of language pairs and creoles,\ne.g., English-Tagalog (Oco and Roxas, 2012),\nEnglish-Indonesian (Barik et al., 2019; Yulianti\net al., 2021), Javanese-Indonesian (Tho et al.,\n2021), Chinese-English (Lyu et al., 2010; Love-\nnia et al., 2022; Zhang and Eickhoff, 2023) and\nSinglish (Chen and Min-Yen, 2015; Lent et al.,\n2021)9. The current corpus does not even scratch\nthe surface of the sheer amount of code-mixedness\nin SEA (Redmond et al., 2009), where deployable\ndata is practically non-existent. In this work, we\ntry to close this gap by exploring the potential of\ngenerating synthetic code-mixed data for the SEA\nregion by prompting LLMs.\nSynthetic Code-Mixing Generation of synthetic\ncode-mixed data to address data scarcity problem\nhas been previously explored. Solorio and Liu\n(2008), Winata et al. (2019), and Tan and Joty\n(2021) have attempted to generate synthetic code-\nmixed sentences through word alignment and can-\ndidate selection from a parallel corpus. Liu et al.\n(2020) and Adilazuarda et al. (2022) have similarly\ngenerated synthetic code-mixed sentences by re-\nplacing words in monolingual sentences with their\nmachine-translated counterparts, whereas Pratapa\net al. (2018), Rizvi et al. (2021) and Santy et al.\n(2021) leveraged parse tree structure for such re-\nplacements. Another approach is to perform neural\nmachine translation to translate monolingual sen-\ntences to code-mixed ones (Appicharla et al., 2021;\nGautam et al., 2021; Jawahar et al., 2021; Dowlagar\nand Mamidi, 2021). In this work, we assess a novel\nway of generating synthetic code-mixed sentences\nthrough prompting multilingual LLMs.\n6 Conclusion\nTo ameliorate the scarcity of code-mixed data for\nSouth East Asian languages, we explore generat-\ning synthetic code-mixed data using state-of-the-art\nmultilingual Large Language Models (LLMs). On\none hand, we find that publicly available LLMs\nsuch as BLOOMZ and Flan-T5-XXL have lim-\nited capability in generating syntactically diverse\ncode-mixed data. On the other hand, closed-source\nmodels such as ChatGPT and InstructGPT are bet-\nter at generating natural code-mixed text, but their\nperformance varies substantially depending on the\n9To exacerbate the situation, some of the SEA code-mixed\ndatasets are no longer publicly available.\n50\nprompt template and language pairing. Further-\nmore, many outputs suffer from syntactic, seman-\ntic, and reliability issues. Therefore, we caution\nagainst using LLM-generated synthetic code-mixed\ndata without the involvement of native speakers for\nannotating and editing.\n7 Limitations\n7.1 Effectiveness of Synthetic Code-Mixed\nData on Downstream Tasks\nIn our study, we did not evaluate how much our syn-\nthetically generated code-mixed data improve the\nability of language models to handle code-mixed\ntext in downstream NLP tasks. While previous\nfindings have shown that finetuning models with\nsynthetic code-mixed data yields less performance\ngains than with naturally occurring code-mixed\ndata (Santy et al., 2021), we believe that this perfor-\nmance gap will diminish as the quality of synthetic\ndata generation gets better with future multilingual\nLLMs.\n7.2 Lack of Human-Generated Data\nWhile we annotated the degree of code-mixedness\nand naturalness, we did not have human-generated,\nnaturally occurring, code-mixed sentences in re-\nsponse to the prompt topics. Therefore, we could\nnot systematically compare the data distribution\nof our synthetic data against the human-generated\ndata. However, since there are multiple ways in\nwhich a sentence can be code-mixed, our focus in\nthis work is on how human-like are the sentences,\nand this, we believe, was adequately captured by\nour evaluation.\n7.3 Monolingual Zero-Shot Prompting\nOur study only uses prompt templates written in\nEnglish to prompt language models in a zero-shot\nmanner. In future follow-ups, we will (1) use\ncode-mixed prompt templates such as “Generate\nan English-Bahasa sentence” instead of “Gener-\nate an English-Malay sentence” and (2) investigate\nLLMs’ capabilities in generating code-mixed data\nwith in-context few-shot examples.\n7.4 Instruction-Tuned Language Models\nOur work only covers instruction-tuned language\nmodels. In future work, we will include a com-\nparison between multilingual models that are not\nfinetuned with instructions—for example, GPT3\n(davinci) (Brown et al., 2020) and BLOOM (Scao\net al., 2022)—to explore the effects of instruction\ntuning in generating code-mixed data.\n7.5 English-Centric Code-Mixing\nOur study focuses on generating code-mixed data\nonly for English-SEA language pairs. For future\nstudies, we plan to investigate generating code-\nmixed data for non-English language pairs com-\nmonly spoken in SEA countries (such as Malay-\nChinese and Indonesian-Javanese).\n7.6 Failures of BLOOM and Flan-T5-XXL\nGiven the lack of research transparency on why\nChatGPT performs better at code-mixed text gener-\nation, we assume that the publicly available models\nsuch as BLOOM and Flan-T5-XXL are unable to\ncode-mix due to the lack of code-mixed texts in the\npretraining corpora and code-mixing tasks in the\ninstruction-tuning datasets. Further investigation is\nwarranted to understand the effects of code-mixed\ntext in pretraining and instruction-tuning data on\ncode-mixed text generation.\n7.7 Presence of Synthetic Code-Mixed Data in\nFuture Pretraining Data\nAs we advocate for the code-mixing ability in fu-\nture generations of LLMs, we are aware of the\npotential risks of data feedback, where genera-\ntive models that recursively train on data gener-\nated by previous generations may amplify biases\nand lose information about the tails of the origi-\nnal distribution (Shumailov et al., 2023; Taori and\nHashimoto, 2022). Since these negative effects\ncan be mitigated through human-generated content\n(Shumailov et al., 2023), it becomes imperative for\nthe NLP community to collect natural code-mixed\ndata for low-resource languages.\n8 Ethical Considerations\nCode-mixing reflects the linguistic, social, and cul-\ntural identity of a multilingual community. Re-\nsearchers and practitioners should approach syn-\nthetic code-mixing with sensitivity and respect, and\nbe cognizant of the potential risks of cultural ap-\npropriation or misrepresentation when generating\ncode-mixed data using LLMs. Since LLMs are\ntrained on web data, they may encode biases per-\npetuating stereotypes, discrimination, or marginal-\nization of specific languages or communities. Prior\nwork has also documented how synthetic data may\nplay a role in feedback loops that amplify the\n51\npresence of biased language generation (Taori and\nHashimoto, 2022). Therefore, collaboration with\nlinguists, language experts, and community repre-\nsentatives is necessary to avoid the unintentional\nperpetuation of stereotypes and cultural insensitiv-\nity.\nReferences\nMuhammad Farid Adilazuarda, Samuel Cahyawijaya,\nGenta Indra Winata, Pascale Fung, and Ayu Purwari-\nanti. 2022. IndoRobusta: Towards robustness against\ndiverse code-mixed Indonesian local languages. In\nProceedings of the First Workshop on Scaling Up\nMultilingual Evaluation, pages 25–34, Online. Asso-\nciation for Computational Linguistics.\nGustavo Aguilar, Sudipta Kar, and Thamar Solorio.\n2020. LinCE: A centralized benchmark for linguis-\ntic code-switching evaluation. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 1803–1813, Marseille, France. European\nLanguage Resources Association.\nAlham Fikri Aji, Genta Indra Winata, Fajri Koto,\nSamuel Cahyawijaya, Ade Romadhony, Rahmad Ma-\nhendra, Kemal Kurniawan, David Moeljadi, Radi-\ntyo Eko Prasojo, Timothy Baldwin, Jey Han Lau,\nand Sebastian Ruder. 2022. One country, 700+ lan-\nguages: NLP challenges for underrepresented lan-\nguages and dialects in Indonesia. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 7226–7249, Dublin, Ireland. Association for\nComputational Linguistics.\nEdith Aldridge. 2012. Antipassive and ergativity in\ntagalog. Lingua, 122:192–203.\nDouglas G Altman. 1990. Practical statistics for medi-\ncal research. CRC press.\nRamakrishna Appicharla, Kamal Kumar Gupta, Asif\nEkbal, and Pushpak Bhattacharyya. 2021. IITP-MT\nat CALCS2021: English to Hinglish neural machine\ntranslation using unsupervised synthetic code-mixed\nparallel corpus. In Proceedings of the Fifth Work-\nshop on Computational Approaches to Linguistic\nCode-Switching, pages 31–35, Online. Association\nfor Computational Linguistics.\nEric Armstrong. Tamil and Tamil-English accent.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nAnab Maulana Barik, Rahmad Mahendra, and Mirna\nAdriani. 2019. Normalization of Indonesian-English\ncode-mixed Twitter data. In Proceedings of the 5th\nWorkshop on Noisy User-generated Text (W-NUT\n2019), pages 417–424, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nJohn Baugh. 2005. Linguistic profiling. In Black lin-\nguistics, pages 167–180. Routledge.\nMaria Lourdes S Bautista and Andrew B Gonzalez.\n2006. Southeast asian englishes. The handbook of\nworld Englishes, pages 130–144.\nAnshul Bawa, Pranav Khadpe, Pratik Joshi, Kalika Bali,\nand Monojit Choudhury. 2020. Do multilingual users\nprefer chat-bots that code-mix? let’s nudge and find\nout! Proceedings of the ACM on Human-Computer\nInteraction, 4(CSCW1):1–23.\nSusan Berk-Seligson. 1986. Linguistic constraints on\nintrasentential code-switching: A study of span-\nish/hebrew bilingualism. Language in society,\n15(3):313–348.\nT. K. Bhatia and W. C. Ritchie. 2004. Social and psy-\nchological factors in language mixing. In W. C.\nRitchie and T. K. Bhatia, editors, Handbook of bilin-\ngualism, pages 336–352. Blackwell Publishing.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSamuel Cahyawijaya, Holy Lovenia, Alham Fikri\nAji, Genta Indra Winata, Bryan Wilie, Rahmad\nMahendra, Christian Wibisono, Ade Romadhony,\nKarissa Vincentio, Fajri Koto, Jennifer Santoso,\nDavid Moeljadi, Cahya Wirawan, Frederikus Hudi,\nIvan Halim Parmonangan, Ika Alfina, Muham-\nmad Satrio Wicaksono, Ilham Firdausi Putra, Samsul\nRahmadani, Yulianti Oenang, Ali Akbar Septian-\ndri, James Jaya, Kaustubh D. Dhole, Arie Ardiyanti\nSuryani, Rifki Afina Putri, Dan Su, Keith Stevens,\nMade Nindyatama Nityasya, Muhammad Farid\nAdilazuarda, Ryan Ignatius, Ryandito Diandaru,\nTiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu,\nDyah Damapuspita, Cuk Tho, Ichwanul Muslim Karo\nKaro, Tirana Noor Fatyanosa, Ziwei Ji, Pascale Fung,\nGraham Neubig, Timothy Baldwin, Sebastian Ruder,\nHerry Sujaini, Sakriani Sakti, and Ayu Purwarianti.\n2022. Nusacrowd: Open source initiative for indone-\nsian nlp resources.\nJoyce YC Chan, Houwei Cao, PC Ching, and Tan Lee.\n2009. Automatic recognition of cantonese-english\ncode-mixing speech. In International Journal of\nComputational Linguistics & Chinese Language Pro-\ncessing, Volume 14, Number 3, September 2009.\nSu-Chiao Chen. 1996. Code-switching as a verbal strat-\negy among chinese in a campus setting in taiwan.\nWorld Englishes, 15(3):267–280.\nT Chen and Kan Min-Yen. 2015. The national university\nof singapore sms corpus.\n52\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language mod-\nels. arXiv preprint arXiv:2210.11416.\nCommonwealth of the Philippines. 1936. Common-\nwealth act no. 184: Govph.\nMona Diab, Julia Hirschberg, Pascale Fung, and Thamar\nSolorio, editors. 2014. Proceedings of the First\nWorkshop on Computational Approaches to Code\nSwitching. Association for Computational Linguis-\ntics, Doha, Qatar.\nMark Dingemanse and Andreas Liesenfeld. 2022. From\ntext to talk: Harnessing conversational corpora for\nhumane and diversity-aware language technology.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 5614–5633, Dublin, Ireland.\nAssociation for Computational Linguistics.\nA. Seza Do ˘gru¨oz, Sunayana Sitaram, Barbara E. Bul-\nlock, and Almeida Jacqueline Toribio. 2021. A sur-\nvey of code-switching: Linguistic and social per-\nspectives for language technologies. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1654–1666, Online.\nAssociation for Computational Linguistics.\nSuman Dowlagar and Radhika Mamidi. 2021. Gated\nconvolutional sequence to sequence based learning\nfor English-hingilsh code-switched machine transla-\ntion. In Proceedings of the Fifth Workshop on Com-\nputational Approaches to Linguistic Code-Switching,\npages 26–30, Online. Association for Computational\nLinguistics.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nBj¨orn Gamb ¨ack and Amitava Das. 2014. On measuring\nthe complexity of code-mixing. In Proceedings of the\n11th international conference on natural language\nprocessing, Goa, India, pages 1–7.\nDevansh Gautam, Prashant Kodali, Kshitij Gupta, An-\nmol Goel, Manish Shrivastava, and Ponnurangam\nKumaraguru. 2021. CoMeT: Towards code-mixed\ntranslation using parallel monolingual sentences. In\nProceedings of the Fifth Workshop on Computational\nApproaches to Linguistic Code-Switching, pages 47–\n55, Online. Association for Computational Linguis-\ntics.\nCliff Goddard. 2005. The languages of East and South-\neast Asia: an introduction. Oxford University Press\non Demand.\nF. Grosjean. 1982. Life with two languages: An intro-\nduction to bilingualism. Harvard University Press.\nXingwei He, Zhenghao Lin, Yeyun Gong, A Jin, Hang\nZhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan,\nWeizhu Chen, et al. 2023. Annollm: Making large\nlanguage models to be better crowdsourced annota-\ntors. arXiv preprint arXiv:2303.16854.\nRepublik Indonesia. 2002. Undang-Undang Dasar Ne-\ngara Republik Indonesia Tahun 1945. Sekretariat\nJenderal MPR RI.\nGanesh Jawahar, El Moatez Billah Nagoudi, Muham-\nmad Abdul-Mageed, and Laks Lakshmanan, V.S.\n2021. Exploring text-to-text transformers for En-\nglish to Hinglish machine translation with synthetic\ncode-mixing. In Proceedings of the Fifth Work-\nshop on Computational Approaches to Linguistic\nCode-Switching, pages 36–46, Online. Association\nfor Computational Linguistics.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282–6293, Online. Association for Computational\nLinguistics.\nAnubha Kabra, Emmy Liu, Simran Khanuja, Al-\nham Fikri Aji, Genta Indra Winata, Samuel Cahyawi-\njaya, Anuoluwapo Aremu, Perez Ogayo, and Graham\nNeubig. 2023. Multi-lingual and multi-cultural figu-\nrative language understanding.\nSimran Khanuja, Sandipan Dandapat, Anirudh Srini-\nvasan, Sunayana Sitaram, and Monojit Choudhury.\n2020. GLUECoS: An evaluation benchmark for\ncode-switched NLP. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3575–3585, Online. Association\nfor Computational Linguistics.\nAndy Kirkpatrick. 2014. English in southeast asia: Ped-\nagogical and policy implications. World Englishes,\n33(4):426–438.\nPaul Kroeger. 1993. Phrase structure and grammat-\nical relations in Tagalog. Center for the Study of\nLanguage (CSLI).\nHuiyuan Lai and Malvina Nissim. 2022. Multi-\nfigurative language generation. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 5939–5954, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\nJ Richard Landis and Gary G Koch. 1977. The mea-\nsurement of observer agreement for categorical data.\nbiometrics, pages 159–174.\nHugo Lauren c ¸on, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro Von Werra, Chenghao Mou,\nEduardo González Ponferrada, Huu Nguyen, J ¨org\nFrohberg, Mario ˇSaˇsko, Quentin Lhoest, Angelina\n53\nMcMillan-Major, Gérard Dupont, Stella Biderman,\nAnna Rogers, Loubna Ben allal, Francesco De Toni,\nGiada Pistilli, Olivier Nguyen, Somaieh Nikpoor,\nMaraim Masoud, Pierre Colombo, Javier de la Rosa,\nPaulo Villegas, Tristan Thrush, Shayne Longpre, Se-\nbastian Nagel, Leon Weber, Manuel Romero Mu ˜noz,\nJian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid\nAlmubarak, Vu Minh Chien, Itziar Gonzalez-Dios,\nAitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz\nSuarez, Aaron Gokaslan, Shamik Bose, David Ife-\noluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas\nPai, Jenny Chim, Violette Lepercq, Suzana Ilic, Mar-\ngaret Mitchell, Sasha Luccioni, and Yacine Jernite.\n2022. The bigscience ROOTS corpus: A 1.6TB\ncomposite multilingual dataset. In Thirty-sixth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track.\nHeather Lent, Emanuele Bugliarello, Miryam\nde Lhoneux, Chen Qiu, and Anders S øgaard. 2021.\nOn language models for creoles. In Proceedings of\nthe 25th Conference on Computational Natural Lan-\nguage Learning, pages 58–71, Online. Association\nfor Computational Linguistics.\nZihan Liu, Genta Indra Winata, Zhaojiang Lin, Peng\nXu, and Pascale Fung. 2020. Attention-informed\nmixed-language training for zero-shot cross-lingual\ntask-oriented dialogue systems. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8433–8440.\nHoly Lovenia, Samuel Cahyawijaya, Genta Winata,\nPeng Xu, Yan Xu, Zihan Liu, Rita Frieske, Tiezheng\nYu, Wenliang Dai, Elham J. Barezi, Qifeng Chen,\nXiaojuan Ma, Bertram Shi, and Pascale Fung. 2022.\nASCEND: A spontaneous Chinese-English dataset\nfor code-switching in multi-turn conversation. In\nProceedings of the Thirteenth Language Resources\nand Evaluation Conference, pages 7259–7268, Mar-\nseille, France. European Language Resources Asso-\nciation.\nDau-Cheng Lyu, Tien-Ping Tan, Eng Siong Chng, and\nHaizhou Li. 2010. Seame: a mandarin-english\ncode-switching speech corpus in south-east asia. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nRawl Maliwat. 2021. Language policy and education in\nsoutheast asia.\nBrian Migliazza. 1996. Mainland southeast asia: A\nunique linguistic area. Notes on linguistics, 75:17–\n25.\nSneha Mondal, Ritika ., Shreya Pathak, Preethi\nJyothi, and Aravindan Raghuveer. 2022. CoCoa:\nAn encoder-decoder model for controllable code-\nswitched generation. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2466–2479, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2022. Crosslingual generalization\nthrough multitask finetuning.\nCarol Myers-Scotton. 1997. Duelling languages:\nGrammatical structure in codeswitching. Oxford\nUniversity Press.\nNathaniel Oco and Rachel Edita Roxas. 2012. Pat-\ntern matching refinements to dictionary-based code-\nswitching point detection. In Proceedings of the 26th\nPacific Asia Conference on Language, Information,\nand Computation, pages 229–236, Bali, Indonesia.\nFaculty of Computer Science, Universitas Indonesia.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nFemphy Pisceldo, Rahmad Mahendra, Ruli Manurung,\nand I Wayan Arka. 2008. A two-level morphological\nanalyser for the Indonesian language. In Proceedings\nof the Australasian Language Technology Association\nWorkshop 2008, pages 142–150, Hobart, Australia.\nShana Poplack. 1978. Syntactic structure and social\nfunction of code-switching, volume 2. Centro de\nEstudios Puertorrique ˜nos,[City University of New\nYork].\nVineel Pratap, Andros Tjandra, Bowen Shi, Paden\nTomasello, Arun Babu, Sayani Kundu, Ali Elkahky,\nZhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi,\net al. 2023. Scaling speech technology to 1,000+\nlanguages. arXiv preprint arXiv:2305.13516.\nAdithya Pratapa, Gayatri Bhat, Monojit Choudhury,\nSunayana Sitaram, Sandipan Dandapat, and Kalika\nBali. 2018. Language modeling for code-mixing:\nThe role of linguistic theory based synthetic data. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1543–1553, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nMont Redmond, Kimmo Kosonen, and Catherine\nYoung. 2009. Mother tongue as bridge language\nof instruction: Policies and experiences in southeast\nAsia. World Bank.\nMachel Reid, Victor Zhong, Suchin Gururangan, and\nLuke Zettlemoyer. 2022. M2D2: A massively multi-\ndomain language modeling dataset. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 964–975, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\n54\nMohd Sanad Zaki Rizvi, Anirudh Srinivasan, Tanuja\nGanu, Monojit Choudhury, and Sunayana Sitaram.\n2021. GCM: A toolkit for generating synthetic code-\nmixed text. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: System Demonstrations, pages\n205–211, Online. Association for Computational Lin-\nguistics.\nTogether Computer SambaNova Systems. 2023.\nBLOOMChat: a New Open Multilingual Chat LLM.\nDavid Sankoff, Shana Poplack, and Swathi Vanniarajan.\n1990. The case of the nonce loan in tamil. Language\nVariation and Change, 2(1):71–101.\nSebastin Santy, Anirudh Srinivasan, and Monojit\nChoudhury. 2021. BERTologiCoMix: How does\ncode-mixing interact with multilingual BERT? In\nProceedings of the Second Workshop on Domain\nAdaptation for NLP, pages 111–121, Kyiv, Ukraine.\nAssociation for Computational Linguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, Fran c ¸ois Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nHarold F. Schiffman. 1998. Malaysian tamils and tamil\nlinguistic culture.\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao,\nYarin Gal, Nicolas Papernot, and Ross Anderson.\n2023. The curse of recursion: Training on gen-\nerated data makes models forget. arXiv preprint\narxiv:2305.17493.\nYuen Sin. 2017. Don’t play, play - singlish is studied\naround the globe.\nDepartment of Statistics Singapore. 2020. Key findings\n2020.\nJames Neil Sneddon. 2003. The Indonesian language:\nIts history and role in modern society. UNSW Press,\nSydney.\nThamar Solorio and Yang Liu. 2008. Learning to pre-\ndict code-switching points. In Proceedings of the\n2008 Conference on Empirical Methods in Natu-\nral Language Processing, pages 973–981, Honolulu,\nHawaii. Association for Computational Linguistics.\nSamson Tan and Shafiq Joty. 2021. Code-mixing on\nsesame street: Dawn of the adversarial polyglots.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3596–3616, Online. Association for Computa-\ntional Linguistics.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and\nXia Hu. 2023. Does synthetic data generation of\nllms help clinical text mining? arXiv preprint\narXiv:2303.04360.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford al-\npaca: An instruction-following llama model. https:\n//github.com/tatsu-lab/stanford_alpaca.\nRohan Taori and Tatsunori B Hashimoto. 2022. Data\nfeedback loops: Model-driven amplification of\ndataset biases. arXiv preprint arXiv:2209.03942.\nIshan Tarunesh, Syamantak Kumar, and Preethi Jyothi.\n2021. From machine translation to code-switching:\nGenerating high-quality code-switched text. In Pro-\nceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 3154–\n3169, Online. Association for Computational Lin-\nguistics.\nC Tho, Y Heryadi, L Lukas, and A Wibowo. 2021.\nCode-mixed sentiment analysis of indonesian lan-\nguage and javanese language using lexicon based\napproach. Journal of Physics: Conference Series,\n1869(1):012084.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nA. J. Toribio. 2002. Spanish-english code-switching\namong us latinos. International Journal of the Soci-\nology of Languaeg, 2002:89–119.\nChenxi Whitehouse, Monojit Choudhury, and Al-\nham Fikri Aji. 2023. Llm-powered data augmen-\ntation for enhanced crosslingual performance. arXiv\npreprint arXiv:2305.14288.\nGenta Indra Winata, Alham Fikri Aji, Samuel Cahyawi-\njaya, Rahmad Mahendra, Fajri Koto, Ade Romad-\nhony, Kemal Kurniawan, David Moeljadi, Radi-\ntyo Eko Prasojo, Pascale Fung, Timothy Baldwin,\nJey Han Lau, Rico Sennrich, and Sebastian Ruder.\n2023. NusaX: Multilingual parallel sentiment dataset\nfor 10 Indonesian local languages. In Proceedings\nof the 17th Conference of the European Chapter of\nthe Association for Computational Linguistics, pages\n815–834, Dubrovnik, Croatia. Association for Com-\nputational Linguistics.\nGenta Indra Winata, Alham Fikri Aji, Zheng-Xin Yong,\nand Thamar Solorio. 2022. The decades progress\non code-switching research in nlp: A systematic\nsurvey on trends and challenges. arXiv preprint\narXiv:2212.09660.\nGenta Indra Winata, Samuel Cahyawijaya, Zihan Liu,\nZhaojiang Lin, Andrea Madotto, and Pascale Fung.\n2021. Are multilingual models effective in code-\nswitching? In Proceedings of the Fifth Workshop\non Computational Approaches to Linguistic Code-\nSwitching, pages 142–153, Online. Association for\nComputational Linguistics.\n55\nGenta Indra Winata, Samuel Cahyawijaya, Zihan Liu,\nZhaojiang Lin, Andrea Madotto, Peng Xu, and Pas-\ncale Fung. 2020. Learning fast adaptation on cross-\naccented speech recognition. In Interspeech 2020.\nISCA.\nGenta Indra Winata, Andrea Madotto, Chien-Sheng Wu,\nand Pascale Fung. 2019. Code-switched language\nmodels using neural based synthetic data from par-\nallel sentences. In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 271–280.\nEvi Yulianti, Ajmal Kurnia, Mirna Adriani, and\nYoppy Setyo Duto. 2021. Normalisation of\nindonesian-english code-mixed text and its effect on\nemotion classification. International Journal of Ad-\nvanced Computer Science and Applications, 12(11).\nRuochen Zhang, Samuel Cahyawijaya, Jan Chris-\ntian Blaise Cruz, and Alham Fikri Aji. 2023. Mul-\ntilingual large language models are not (yet) code-\nswitchers.\nRuochen Zhang and Carsten Eickhoff. 2023. Crocosum:\nA benchmark dataset for cross-lingual code-switched\nsummarization. arXiv preprint arXiv:2303.04092.\n56\nA Languages Spoken in SEA\nThere are more than 1,200 languages spoken in\nSEA (Redmond et al., 2009; Maliwat, 2021), 700\nof which are spoken in Indonesia (Aji et al., 2022;\nCahyawijaya et al., 2022). We describe the lan-\nguages the SEA languages used in the study in the\nfollowing paragraphs.\nMandarin Chinese Mandarin Chinese (zh-\nHans), which belongs to the Sino-Tibetan language\nfamily and uses the Hanzi script, is widely spoken\nin SEA due to the migration of Chinese people\nfrom the coastal provinces of southeastern China,\nsuch as Fujian, Guangdong, and Hainan. People of\nChinese heritage in SEA frequently use the term\n“华人” (huá rén) to express their cultural identity\nas an ethnic group, instead of “ 中国人” (zh ¯ong\nguó rén) which is primarily associated with na-\ntionality, even though both terms can be translated\nas “Chinese (people).” Singapore has the largest\nChinese ethnic group among all SEA countries and\nMandarin Chinese is considered one of the official\nlanguages in Singapore.\nThe language is characterized as linguistically\n“isolating” in that each Chinese character corre-\nsponds to one morpheme and that the language\nuses very little grammatical inflection. It uses a\nlogographic writing system, which uses pictograms\n(Chinese characters) to represent meaning. Chinese\nis also a tonal language with four pitched tones and\none neutral tone. It commonly displays a basic SVO\nword order and, instead of conjugating the verbs to\nexpress tenses, uses aspect particles such as 了(le)\nand 着(zhe) to indicate the temporal location of the\nsentence.\nIndonesian Indonesian (ind) is the national lan-\nguage of Indonesia (Indonesia, 2002). It is spo-\nken by around 300 million speakers worldwide.\nIndonesian is developed from the literary ‘Classi-\ncal Malay’ of the Riau-Johor sultanate (Sneddon,\n2003) and has many regional variants. Indonesian\nis written in Latin script with a lexical similar-\nity of over 80% to Standard Malay. Indonesian is\nnon-tonal and has 19 consonants, 6 vowels, and\n3 diphthongs. The stress is on the penultimate\nsyllable and the word order is SVO. It has three op-\ntional noun classifiers. Indonesian has two social\nregisters and a rich affixation system, including a\nvariety of prefixes, suffixes, circumfixes, and redu-\nplication. Most of the affixes in Indonesian are\nderivational (Pisceldo et al., 2008).\nStandard Malay Standard Malay (msa) is the\nnational language of Malaysia, Brunei, and Singa-\npore, and the language is spoken by approximately\n290 million speakers worldwide. The word order of\nStandard Malay is SVO with four types of affixes,\ni.e., prefixes (awalan), suffixes (akhiran), circum-\nfixes (apitan), and infixes (sisipan). Even though\nStandard Malay and Indonesian originate from the\nsame Malay language and are mutually intelligible,\nthey can differ in spelling and vocabulary. One\nexample is loanwords. Due to the different colonial\ninfluences from the Dutch and British, Indonesian\nprimarily absorbs Dutch loanwords whereas Malay\nabsorbs English loanwords. Both languages can\nalso differ in the meanings of the same written\nwords, which are commonly referred to as inter-\nlingual homographs. For instance, “polisi” means\n“police” in Indonesian but “policy” in Standard\nMalay.\nTagalog Tagalog (tgl) is an Austronesian lan-\nguage spoken in the Philippines by around 82 mil-\nlion native speakers. It is both agglutinative and\npitch-accented, giving it rich and complex mor-\nphology (Kroeger, 1993). Tagalog’s standardized\nform, known as Filipino, is the country’s official\nnational language. The difference between Fil-\nipino and Tagalog is more sociopolitical than soci-\nolinguistic: Commonwealth Act No. 184 of 1936\ncreated a national committee whose purpose is to\n“develop a national language.” This resulted in\nthe standardization of the Tagalog language into\nFilipino. In practice, Filipino is indistinguishable\nfrom Tagalog, albeit with the addition of letters f,\nj, c, x, and z, plus loanwords (Commonwealth of\nthe Philippines, 1936).\nVietnamese Vietnamese (vie), the national lan-\nguage of Vietnam, is spoken by around 85 million\npeople worldwide. It is a tonal language belonging\nto the Austroasiatic language family and uses ac-\ncents to denote six distinctive tones. The sentence\nstructure of Vietnamese displays the SVO word or-\nder, and due to heavy influence from Chinese, it\nalso uses a rich set of classifiers that are required\nin the presence of quantifiers. For instance, instead\nof writing “bốn gà,” which literally translates into\n“four chickens,” it should be “bốn con gà” where\n“con” is a classifier for non-human animate things.\nTamil Tamil (tam) is a Dravidian language origi-\nnating from Tamil Nadu and Sri Lanka. It is spoken\nby the sizeable Tamil diasporas of Singapore (2.5%\n57\nof population (Singapore, 2020)) and Malaysia (9%\nof population (Schiffman, 1998)), which resulted\nfrom histories of trade, migration, indentured servi-\ntude, and civil unrest. Tamil is an official language\nof Singapore (Singapore, 2020), and the only one\noriginating from India. Tamil is notably diglos-\nsic, which means it has a formal literary system,\nlacks lexically distinctive stress, and is non-rhotic\n(Armstrong). Tamil uses SOV sentence structure.\nTamil-English code-mixing exhibits interesting lin-\nguistic phenomena such as nonce loan, wherein\nmany nonce borrowings from English occupy ob-\njects corresponding to Tamil verbs, and vice versa\n(Sankoff et al., 1990).\nSinglish Singlish is a widely-used conversational\nlanguage in Singapore. It is an English-based cre-\nole language that arose out of prolonged language\ncontact between speakers of many different lan-\nguages in the country, including Hokkien, Malay,\nTeochew, Cantonese, and Tamil. Singlish is spo-\nken by around 4 million speakers, and one unique\nfeature of the language is its heavy use of prag-\nmatic particles borrowed from Southern Chinese\ndialects. One example of this is “lah,” which in the\nsentence, “Her dress is too short lah,” emphasizes\nthe statement.\nB HuggingFace Inference API\nWe use HuggingFace’s Inference API to prompt\nmultilingual LLMs since we do not have sufficient\nlocal compute to host models with hundreds of bil-\nlions of parameters such as the 176B-parameter\nBLOOMZ model (Muennighoff et al., 2022). The\ntext-to-text task is treated identically as a text-\ngeneration task, and we set max_new_tokens\n(amount of new tokens to be generated) to 100,\ntemperature to 0.7, and repetition_penalty to\n1.2.\nC OpenAI Inference API\nWe use OpenAI’s official API to prompt both\ndavinci-003 and davinci-002. Specifically, we use\nopenai.Completion.create with a maximum gen-\neration length of 128. We use the default values for\nall other parameters.\nD Flan-T5-XXL Non-English Outputs\nWe observe that when Flan-T5-XXL generates non-\nEnglish outputs, most of them are nonsensical.\nHere are some of the examples and their trans-\nlations.\nIndonesian: Ini adalah sebuah udara untuk pengo-\nbatan minyak dan di sekitar kehidupan.\nTranslation: This is an air for oil treatment and\naround life.\nMalay: Artificial Intelligence adalah sebuah kantor\nkeamanan yang digunakan untuk mengidentifikasi\npenduduk yang memiliki anak-anak dalam diri.\nTranslation: Artificial intelligence is a security of-\nfice used for identifying residents who have childen\ninside.\nTagalog: Weather niya ang nagsimula sa\npagsasagawa ng kaniyang kargahan ng panahon.\nTranslation: It was his weather that started carry-\ning out his weather load.\nVietnamese: Nhà ng tài ra mt ngi dy xut trn o trng\nh nhng ngi ng thng u c thit v.\nTranslation: The artist has created an outstanding\ntalent in the field of talented people.\nE BLOOMZ’s Training Language\nDistribution\nBLOOMZ is created by finetuning the multilingual\n176B-parameter language model BLOOM (Scao\net al., 2022) that is pretrained on ROOTS corpus\n(Laurenc ¸on et al., 2022) on a collection of prompt\ninstructions known as xP3 (Muennighoff et al.,\n2022). Table 1 and Table 2 show the proportion of\nSEA languages investigated in our paper existing\nin the ROOTS and xP3 datasets respectively. Even\nthough Indonesian and Chinese are higher in pro-\nportion than Tamil, BLOOMZ code-mix better for\nTamil than the former two language with around\n20% performance difference.\nLanguages Percent Distribution (%)\nEnglish 30.04\nChinese (Simplified) 16.2\nVietnamese 2.7\nIndonesian 1.2\nTamil 0.2\nTable 1: Proportion of Languages in the ROOTS corpus\n(Laurenc ¸on et al., 2022).\nF Naturalness and Fluency Issues of\nChatGPT’s Generation\nWe document a non-exhaustive list of syntactic and\nsemantic errors as well as reasons for unnaturalness\n58\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n0\n20\n40\n60\n80\n100Percent of Prompts\n(a) Languages\nChinese\nIndonesian\nMalay\nTagalog\nTamil\nVietnamese\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n(b) Topics\nAI\nfamily\nfood\ntraffic\nweather\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n(c) Templates\nassume as bilingual speaker\nexplicitly define CM\nimitate speaking style\nnative speaker\ntwo bilingual speakers\nwrite a CM sentence\nFigure 11: Analysis of davinci-002’s capability of generating code-mixed data.\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n0\n20\n40\n60\n80\n100Percent of Prompts\n(a) Languages\nChinese\nIndonesian\nMalay\nTagalog\nTamil\nVietnamese\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n(b) Topics\nAI\nfamily\nfood\ntraffic\nweather\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n(c) Templates\nassume as bilingual speaker\nexplicitly define CM\nimitate speaking style\nnative speaker\ntwo bilingual speakers\nwrite a CM sentence\nFigure 12: Analysis of davinci-003’s capability of generating code-mixed data.\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n0\n20\n40\n60\n80\n100Percent of Prompts\n(a) Languages\nChinese\nIndonesian\nMalay\nTagalog\nTamil\nVietnamese\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n(b) Topics\nAI\nfamily\nfood\ntraffic\nweather\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n(c) Templates\nImitate Speaking Style\nassume as bilingual speaker\nexplicitly define CM\nnative speaker\ntwo bilingual speakers\nwrite a CM sentence\nFigure 13: Analysis of BLOOMZ’s capability of generating code-mixed data.\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n0\n20\n40\n60\n80\n100Percent of Prompts\n(a) Languages\nChinese\nIndonesian\nMalay\nTagalog\nTamil\nVietnamese\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n(b) Topics\nAI\nfamily\nfood\ntraffic\nweather\n1\nLoanwords\n2\nTopic-Related\nNouns\n3\nLinguistic\nElements\n(c) Templates\nassume as bilingual speaker\nexplicitly define CM\nimitate speaking style\nnative speaker\ntwo bilingual speakers\nwrite a CM sentence\nFigure 14: Analysis of Flan-T5-XXL’s capability of generating code-mixed data.\n59\nLanguages Percent Distribution (%)\nEnglish 39.25\nIndonesian 4.85\nChinese (Simplified) 4.83\nVietnamese 3.27\nTamil 0.97\nTable 2: Proportion of Languages in the xP3 datasets\n(Muennighoff et al., 2022).\nin ChatGPT’s generation in Table 4.\nG Annotators and Inter-annotator\nAgreement\nWe have a total of 13 annotators, some of whom\nspeak more than one SEA language. All of them are\nnative speakers of their respective SEA languages,\nand most grow up in SEA. Many of our annotators\nare AI researchers and reside in the Global North.\nAll the annotators are the authors of the paper.\nIn Table 3, we report the inter-annotator agree-\nment scores for naturalness annotations using\nFleiss’ Kappa κ (Fleiss, 1971), which measures the\nagreement between a fixed number of raters when\nassigning categorical ratings to the items. It can\nbe applied to settings with multiple annotators and\nnot all raters are required to annotate all items. The\ncloser it is to 1, the higher the agreement among\nannotators.\nAccording to the guideline (Landis and Koch,\n1977; Altman, 1990), English-Indonesian annota-\ntions have a fair agreement, English-Chinese and\nSinglish have a substantial agreement, and English-\nTagalog have almost perfect agreement among the\nannotators.\nLanguage N(annotators) κ\nEnglish-Chinese 3 0.6431\nEnglish-Indonesian 3 0.2165\nEnglish-Malay 1 -\nEnglish-Tagalog 2 0.8268\nEnglish-Tamil 1 -\nEnglish-Vietnamese 1 -\nSinglish 3 0.6199\nTable 3: Inter-annnotator agreement scores for nat-\nuralness of ChatGPT’s generated code-mixed text.\nN(annotators) indicates the total number of annotators\nand κ refers to the Fleiss’ Kappa agreement score.\n60\nCMAssuming that you are anEnglish-Malaybilingual speaker,how would you write a naturalEnglish-Malaycode-mixedsentence abouttraffic?\n(ChatGPT)Saya rasa jalan-jalan ni sangat jam sebab adabanyak cars yang block semua lanes. (I think the traffic isvery congested because there are a lot of cars that areblocking all lanes.)\nIn this sentence, I have mixed English and Malaywords together to form a code-mixed sentence. Thesentence starts in Malay, with ...<truncated>\n3\n(Davinci-003)\"Kerja yang mengambil masa yang lamaseperti menunggu lama di laluan raya membuatkan sayabosan!\" 0\n(Davinci-002)The traffic is really bad today.0\n(BLOOMZ)Traffic jams on the highway.0\n(Flan-T5-XXL)The number of vehicles on the road hasdecreased by 32% in the past year.0\n(a) Template: Assume as bilingual speaker\nCMImagine there are twoEnglish-Indonesianbilingual speak-ers talking to each other, can you imagine a conversationbetween them talking aboutfood?\n(ChatGPT)Speaker 1: Hey, have you tried the Indonesiandish, Nasi Goreng?Speaker 2: Yes, I have. It’s one of my favorites. Have youhad it before?Speaker 1: Yes, I have. I really like the combination offlavors. Have you tried Sate Ayam?Speaker 2: Oh yes, I love Sate Ayam! ..<truncated>\n2\n(Davinci-003)\"English speaker: I just had the most amaz-ing Nasi Goreng for dinner!Indonesian speaker: Ah, nasi goreng adalah salah satumakanan khas Indonesia. Apakah kamu suka makananIndonesia lainnya?English speaker: I really like Gado-gado and Ayam Penyet...<truncated>\n2\n(Davinci-002)\"A: What’s your favorite food?B: I love Indonesian food, especially nasi goreng.A: Yeah, I love nasi goreng too!\"1\n(BLOOMZ)\"Here is an example: W: Hi Mariam. How wasyour trip yesterday?M: Oh! It was great fun just like I expected it would be.W: I’m glad. Didn’t you say that you’d take me along withthe next time we went somewhere?M: Yeah... All right then\"\n0\n(Flan-T5-XXL)M: What do you think of the food at theparty? Are they good?W: The food at the party is okay, butyou don’t have to eat it. 0\n(b) Template: Two bilingual speakers\nCMImitate the speaking style of a person who can speakSinglishin one sentence aboutfamily.\n(ChatGPT)Eh, you know lah, family very important oneleh, must always stick together like sotong and chilli sauce,can or not? 3\n(Davinci-003)\"My family lah, we so close, can go any-where together one!\" 3\n(Davinci-002)My family is the best!0\n(BLOOMZ)0\n(Flan-T5-XXL)I’m gonna go home to see my family.0\n(c) Template: Imitate speaking style\nCM\nCode-mixing refers to a phenomenon of combining two ormore languages in a single sentence. Can you generate acode-mixedEnglish-Chinesesentence aboutfood?\n(ChatGPT)Sure, here’s an example: \"I love eating∏!(zhò j¯ủ) fried chicken for lunch.\"2\n(Davinci-003)⌘Ö1⇤Øa\u0000I love potato chips!0\n(Davinci-002)\"FPFP\u0000\u0000}}⇤⇤ÑÑ⇥⇥Tofu, tofu, is not is not good good to eat to eat.\"0\n(BLOOMZ)ﬂiÔÂ⇤\u0000F\u0000Å⇤*⇢⇥0\n(Flan-T5-XXL)A chef prepares a drink.0\n(d) Template: Explicitly define CM\nCMImagine that you are aVietnamesespeaker. How wouldyou talk aboutweatherusing bothEnglishandVietnamesein a single sentence?\n(ChatGPT)\"Ngày hũm nay thÌi ti∏t khò n≠ng nóng, so Ithink I’ll wear my shorts and t-shirt \u0000i chÏi.\"3\n(Davinci-003)I’m sure the weather today is beautiful, thÌiti∏t hũm nay có v¥ rßt \u0000µp. 0\n(Davinci-002)I like the weather here because it’s not toohot and not too cold. Tũi thích thÌi ti∏t  \u0000ây vơ nó khũngnóng quò và khũng l§nh quò. 0\n(BLOOMZ)Hãy t˜ng t˜Òng b§n là ng˜Ìi nói ti∏ng Viªt,và th˚ dùng c£ hai th˘ ti∏ng \u0000∫ mũ t£ thÌi ti∏t:0\n(Flan-T5-XXL)V khòch hàng nh hàng tc òo c tn c i n vnhng lng bn s tm thy hàng n c òo c ó0\n(e) Template: Native speaker\nCMWrite anEnglishandTagalogcode-mixed sentence aboutArtificial Intelligence.\n(ChatGPT)English-Tagalog code-mixed sentence: \"Ar-tificial Intelligence ay isang exciting na teknolohiya namakakapagbigay ng maraming benepisyo sa society.\"Translation: \"Artificial Intelligence is an exciting technologythat can provide many benefits sa lipunan.\"\n1\n(Davinci-003)AI is revolutionizing the way we live, trabaho,at mabuhay ngayon sa pamamagitan ng pagbibigay ngmodernong solusyon sa mga proyekto at pagkukuha ngilang mga trabahong naghihintay na ma-automate.\n3\n(Davinci-002)Artificial Intelligence is something that weshould be thankful for. 0\n(BLOOMZ)An expert in artificial intelligence explains theirarea of expertise to a student with the help of a colleaguewho speaks both languages. 0\n(Flan-T5-XXL)Ang pagsusuri ng mas maraming kapasirang hariko ay nag-aaral ng sasakyan ng pag-aaral ng pan-gangalambot, na magtitipon para sa pamamagitan ng pag-papasagawa ng karangalan ng isang lalaking.\n0\n(f) Template: Write a CM sentence\nFigure 16: Prompt templates with differentlanguages and topic fields and responses from different LLMs containing\ncode-mixed / non-code-mixed sentences. We also include a template where we specify thenationality of the speaker.\nNote that the explanations are a part of ChatGPT’s original generation.\n23\n(a) Template: Assume as bilingual speaker\nCMAssuming that you are anEnglish-Malaybilingual speaker,how would you write a naturalEnglish-Malaycode-mixedsentence abouttraffic?\n(ChatGPT)Saya rasa jalan-jalan ni sangat jam sebab adabanyak cars yang block semua lanes. (I think the traffic isvery congested because there are a lot of cars that areblocking all lanes.)\nIn this sentence, I have mixed English and Malaywords together to form a code-mixed sentence. Thesentence starts in Malay, with ...<truncated>\n3\n(Davinci-003)\"Kerja yang mengambil masa yang lamaseperti menunggu lama di laluan raya membuatkan sayabosan!\" 0\n(Davinci-002)The traffic is really bad today.0\n(BLOOMZ)Traffic jams on the highway.0\n(Flan-T5-XXL)The number of vehicles on the road hasdecreased by 32% in the past year.0\n(a) Template: Assume as bilingual speaker\nCMImagine there are twoEnglish-Indonesianbilingual speak-ers talking to each other, can you imagine a conversationbetween them talking aboutfood?\n(ChatGPT)Speaker 1: Hey, have you tried the Indonesiandish, Nasi Goreng?Speaker 2: Yes, I have. It’s one of my favorites. Have youhad it before?Speaker 1: Yes, I have. I really like the combination offlavors. Have you tried Sate Ayam?Speaker 2: Oh yes, I love Sate Ayam! ..<truncated>\n2\n(Davinci-003)\"English speaker: I just had the most amaz-ing Nasi Goreng for dinner!Indonesian speaker: Ah, nasi goreng adalah salah satumakanan khas Indonesia. Apakah kamu suka makananIndonesia lainnya?English speaker: I really like Gado-gado and Ayam Penyet...<truncated>\n2\n(Davinci-002)\"A: What’s your favorite food?B: I love Indonesian food, especially nasi goreng.A: Yeah, I love nasi goreng too!\"1\n(BLOOMZ)\"Here is an example: W: Hi Mariam. How wasyour trip yesterday?M: Oh! It was great fun just like I expected it would be.W: I’m glad. Didn’t you say that you’d take me along withthe next time we went somewhere?M: Yeah... All right then\"\n0\n(Flan-T5-XXL)M: What do you think of the food at theparty? Are they good?W: The food at the party is okay, butyou don’t have to eat it. 0\n(b) Template: Two bilingual speakers\nCMImitate the speaking style of a person who can speakSinglishin one sentence aboutfamily.\n(ChatGPT)Eh, you know lah, family very important oneleh, must always stick together like sotong and chilli sauce,can or not? 3\n(Davinci-003)\"My family lah, we so close, can go any-where together one!\" 3\n(Davinci-002)My family is the best!0\n(BLOOMZ)0\n(Flan-T5-XXL)I’m gonna go home to see my family.0\n(c) Template: Imitate speaking style\nCM\nCode-mixing refers to a phenomenon of combining two ormore languages in a single sentence. Can you generate acode-mixedEnglish-Chinesesentence aboutfood?\n(ChatGPT)Sure, here’s an example: \"I love eating∏!(zhò j¯ủ) fried chicken for lunch.\"2\n(Davinci-003)⌘Ö1⇤Øa\u0000I love potato chips!0\n(Davinci-002)\"FPFP\u0000\u0000}}⇤⇤ÑÑ⇥⇥Tofu, tofu, is not is not good good to eat to eat.\"0\n(BLOOMZ)ﬂiÔÂ⇤\u0000F\u0000Å⇤*⇢⇥0\n(Flan-T5-XXL)A chef prepares a drink.0\n(d) Template: Explicitly define CM\nCMImagine that you are aVietnamesespeaker. How wouldyou talk aboutweatherusing bothEnglishandVietnamesein a single sentence?\n(ChatGPT)\"Ngày hũm nay thÌi ti∏t khò n≠ng nóng, so Ithink I’ll wear my shorts and t-shirt \u0000i chÏi.\"3\n(Davinci-003)I’m sure the weather today is beautiful, thÌiti∏t hũm nay có v¥ rßt \u0000µp. 0\n(Davinci-002)I like the weather here because it’s not toohot and not too cold. Tũi thích thÌi ti∏t  \u0000ây vơ nó khũngnóng quò và khũng l§nh quò. 0\n(BLOOMZ)Hãy t˜ng t˜Òng b§n là ng˜Ìi nói ti∏ng Viªt,và th˚ dùng c£ hai th˘ ti∏ng \u0000∫ mũ t£ thÌi ti∏t:0\n(Flan-T5-XXL)V khòch hàng nh hàng tc òo c tn c i n vnhng lng bn s tm thy hàng n c òo c ó0\n(e) Template: Native speaker\nCMWrite anEnglishandTagalogcode-mixed sentence aboutArtificial Intelligence.\n(ChatGPT)English-Tagalog code-mixed sentence: \"Ar-tificial Intelligence ay isang exciting na teknolohiya namakakapagbigay ng maraming benepisyo sa society.\"Translation: \"Artificial Intelligence is an exciting technologythat can provide many benefits sa lipunan.\"\n1\n(Davinci-003)AI is revolutionizing the way we live, trabaho,at mabuhay ngayon sa pamamagitan ng pagbibigay ngmodernong solusyon sa mga proyekto at pagkukuha ngilang mga trabahong naghihintay na ma-automate.\n3\n(Davinci-002)Artificial Intelligence is something that weshould be thankful for. 0\n(BLOOMZ)An expert in artificial intelligence explains theirarea of expertise to a student with the help of a colleaguewho speaks both languages. 0\n(Flan-T5-XXL)Ang pagsusuri ng mas maraming kapasirang hariko ay nag-aaral ng sasakyan ng pag-aaral ng pan-gangalambot, na magtitipon para sa pamamagitan ng pag-papasagawa ng karangalan ng isang lalaking.\n0\n(f) Template: Write a CM sentence\nFigure 16: Prompt templates with differentlanguages and topic fields and responses from different LLMs containing\ncode-mixed / non-code-mixed sentences. We also include a template where we specify thenationality of the speaker.\nNote that the explanations are a part of ChatGPT’s original generation.\n23\n(b) Template: Imitate speaking style\nCMAssuming that you are anEnglish-Malaybilingual speaker,how would you write a naturalEnglish-Malaycode-mixedsentence abouttraffic?\n(ChatGPT)Saya rasa jalan-jalan ni sangat jam sebab adabanyak cars yang block semua lanes. (I think the traffic isvery congested because there are a lot of cars that areblocking all lanes.)\nIn this sentence, I have mixed English and Malaywords together to form a code-mixed sentence. Thesentence starts in Malay, with ...<truncated>\n3\n(Davinci-003)\"Kerja yang mengambil masa yang lamaseperti menunggu lama di laluan raya membuatkan sayabosan!\" 0\n(Davinci-002)The traffic is really bad today.0\n(BLOOMZ)Traffic jams on the highway.0\n(Flan-T5-XXL)The number of vehicles on the road hasdecreased by 32% in the past year.0\n(a) Template: Assume as bilingual speaker\nCMImagine there are twoEnglish-Indonesianbilingual speak-ers talking to each other, can you imagine a conversationbetween them talking aboutfood?\n(ChatGPT)Speaker 1: Hey, have you tried the Indonesiandish, Nasi Goreng?Speaker 2: Yes, I have. It’s one of my favorites. Have youhad it before?Speaker 1: Yes, I have. I really like the combination offlavors. Have you tried Sate Ayam?Speaker 2: Oh yes, I love Sate Ayam! ..<truncated>\n2\n(Davinci-003)\"English speaker: I just had the most amaz-ing Nasi Goreng for dinner!Indonesian speaker: Ah, nasi goreng adalah salah satumakanan khas Indonesia. Apakah kamu suka makananIndonesia lainnya?English speaker: I really like Gado-gado and Ayam Penyet...<truncated>\n2\n(Davinci-002)\"A: What’s your favorite food?B: I love Indonesian food, especially nasi goreng.A: Yeah, I love nasi goreng too!\"1\n(BLOOMZ)\"Here is an example: W: Hi Mariam. How wasyour trip yesterday?M: Oh! It was great fun just like I expected it would be.W: I’m glad. Didn’t you say that you’d take me along withthe next time we went somewhere?M: Yeah... All right then\"\n0\n(Flan-T5-XXL)M: What do you think of the food at theparty? Are they good?W: The food at the party is okay, butyou don’t have to eat it. 0\n(b) Template: Two bilingual speakers\nCMImitate the speaking style of a person who can speakSinglishin one sentence aboutfamily.\n(ChatGPT)Eh, you know lah, family very important oneleh, must always stick together like sotong and chilli sauce,can or not? 3\n(Davinci-003)\"My family lah, we so close, can go any-where together one!\" 3\n(Davinci-002)My family is the best!0\n(BLOOMZ)0\n(Flan-T5-XXL)I’m gonna go home to see my family.0\n(c) Template: Imitate speaking style\nCM\nCode-mixing refers to a phenomenon of combining two ormore languages in a single sentence. Can you generate acode-mixedEnglish-Chinesesentence aboutfood?\n(ChatGPT)Sure, here’s an example: \"I love eating∏!(zhò j¯ủ) fried chicken for lunch.\"2\n(Davinci-003)⌘Ö1⇤Øa\u0000I love potato chips!0\n(Davinci-002)\"FPFP\u0000\u0000}}⇤⇤ÑÑ⇥⇥Tofu, tofu, is not is not good good to eat to eat.\"0\n(BLOOMZ)ﬂiÔÂ⇤\u0000F\u0000Å⇤*⇢⇥0\n(Flan-T5-XXL)A chef prepares a drink.0\n(d) Template: Explicitly define CM\nCMImagine that you are aVietnamesespeaker. How wouldyou talk aboutweatherusing bothEnglishandVietnamesein a single sentence?\n(ChatGPT)\"Ngày hũm nay thÌi ti∏t khò n≠ng nóng, so Ithink I’ll wear my shorts and t-shirt \u0000i chÏi.\"3\n(Davinci-003)I’m sure the weather today is beautiful, thÌiti∏t hũm nay có v¥ rßt \u0000µp. 0\n(Davinci-002)I like the weather here because it’s not toohot and not too cold. Tũi thích thÌi ti∏t  \u0000ây vơ nó khũngnóng quò và khũng l§nh quò. 0\n(BLOOMZ)Hãy t˜ng t˜Òng b§n là ng˜Ìi nói ti∏ng Viªt,và th˚ dùng c£ hai th˘ ti∏ng \u0000∫ mũ t£ thÌi ti∏t:0\n(Flan-T5-XXL)V khòch hàng nh hàng tc òo c tn c i n vnhng lng bn s tm thy hàng n c òo c ó0\n(e) Template: Native speaker\nCMWrite anEnglishandTagalogcode-mixed sentence aboutArtificial Intelligence.\n(ChatGPT)English-Tagalog code-mixed sentence: \"Ar-tificial Intelligence ay isang exciting na teknolohiya namakakapagbigay ng maraming benepisyo sa society.\"Translation: \"Artificial Intelligence is an exciting technologythat can provide many benefits sa lipunan.\"\n1\n(Davinci-003)AI is revolutionizing the way we live, trabaho,at mabuhay ngayon sa pamamagitan ng pagbibigay ngmodernong solusyon sa mga proyekto at pagkukuha ngilang mga trabahong naghihintay na ma-automate.\n3\n(Davinci-002)Artificial Intelligence is something that weshould be thankful for. 0\n(BLOOMZ)An expert in artificial intelligence explains theirarea of expertise to a student with the help of a colleaguewho speaks both languages. 0\n(Flan-T5-XXL)Ang pagsusuri ng mas maraming kapasirang hariko ay nag-aaral ng sasakyan ng pag-aaral ng pan-gangalambot, na magtitipon para sa pamamagitan ng pag-papasagawa ng karangalan ng isang lalaking.\n0\n(f) Template: Write a CM sentence\nFigure 16: Prompt templates with differentlanguages and topic fields and responses from different LLMs containing\ncode-mixed / non-code-mixed sentences. We also include a template where we specify thenationality of the speaker.\nNote that the explanations are a part of ChatGPT’s original generation.\n23\n(c) Template: Two bilingual speakers\nCMAssuming that you are anEnglish-Malaybilingual speaker,how would you write a naturalEnglish-Malaycode-mixedsentence abouttraffic?\n(ChatGPT)Saya rasa jalan-jalan ni sangat jam sebab adabanyak cars yang block semua lanes. (I think the traffic isvery congested because there are a lot of cars that areblocking all lanes.)\nIn this sentence, I have mixed English and Malaywords together to form a code-mixed sentence. Thesentence starts in Malay, with ...<truncated>\n3\n(Davinci-003)\"Kerja yang mengambil masa yang lamaseperti menunggu lama di laluan raya membuatkan sayabosan!\" 0\n(Davinci-002)The traffic is really bad today.0\n(BLOOMZ)Traffic jams on the highway.0\n(Flan-T5-XXL)The number of vehicles on the road hasdecreased by 32% in the past year.0\n(a) Template: Assume as bilingual speaker\nCMImagine there are twoEnglish-Indonesianbilingual speak-ers talking to each other, can you imagine a conversationbetween them talking aboutfood?\n(ChatGPT)Speaker 1: Hey, have you tried the Indonesiandish, Nasi Goreng?Speaker 2: Yes, I have. It’s one of my favorites. Have youhad it before?Speaker 1: Yes, I have. I really like the combination offlavors. Have you tried Sate Ayam?Speaker 2: Oh yes, I love Sate Ayam! ..<truncated>\n2\n(Davinci-003)\"English speaker: I just had the most amaz-ing Nasi Goreng for dinner!Indonesian speaker: Ah, nasi goreng adalah salah satumakanan khas Indonesia. Apakah kamu suka makananIndonesia lainnya?English speaker: I really like Gado-gado and Ayam Penyet...<truncated>\n2\n(Davinci-002)\"A: What’s your favorite food?B: I love Indonesian food, especially nasi goreng.A: Yeah, I love nasi goreng too!\"1\n(BLOOMZ)\"Here is an example: W: Hi Mariam. How wasyour trip yesterday?M: Oh! It was great fun just like I expected it would be.W: I’m glad. Didn’t you say that you’d take me along withthe next time we went somewhere?M: Yeah... All right then\"\n0\n(Flan-T5-XXL)M: What do you think of the food at theparty? Are they good?W: The food at the party is okay, butyou don’t have to eat it. 0\n(b) Template: Two bilingual speakers\nCMImitate the speaking style of a person who can speakSinglishin one sentence aboutfamily.\n(ChatGPT)Eh, you know lah, family very important oneleh, must always stick together like sotong and chilli sauce,can or not? 3\n(Davinci-003)\"My family lah, we so close, can go any-where together one!\" 3\n(Davinci-002)My family is the best!0\n(BLOOMZ)0\n(Flan-T5-XXL)I’m gonna go home to see my family.0\n(c) Template: Imitate speaking style\nCM\nCode-mixing refers to a phenomenon of combining two ormore languages in a single sentence. Can you generate acode-mixedEnglish-Chinesesentence aboutfood?\n(ChatGPT)Sure, here’s an example: \"I love eating∏!(zhò j¯ủ) fried chicken for lunch.\"2\n(Davinci-003)⌘Ö1⇤Øa\u0000I love potato chips!0\n(Davinci-002)\"FPFP\u0000\u0000}}⇤⇤ÑÑ⇥⇥Tofu, tofu, is not is not good good to eat to eat.\"0\n(BLOOMZ)ﬂiÔÂ⇤\u0000F\u0000Å⇤*⇢⇥0\n(Flan-T5-XXL)A chef prepares a drink.0\n(d) Template: Explicitly define CM\nCMImagine that you are aVietnamesespeaker. How wouldyou talk aboutweatherusing bothEnglishandVietnamesein a single sentence?\n(ChatGPT)\"Ngày hũm nay thÌi ti∏t khò n≠ng nóng, so Ithink I’ll wear my shorts and t-shirt \u0000i chÏi.\"3\n(Davinci-003)I’m sure the weather today is beautiful, thÌiti∏t hũm nay có v¥ rßt \u0000µp. 0\n(Davinci-002)I like the weather here because it’s not toohot and not too cold. Tũi thích thÌi ti∏t  \u0000ây vơ nó khũngnóng quò và khũng l§nh quò. 0\n(BLOOMZ)Hãy t˜ng t˜Òng b§n là ng˜Ìi nói ti∏ng Viªt,và th˚ dùng c£ hai th˘ ti∏ng \u0000∫ mũ t£ thÌi ti∏t:0\n(Flan-T5-XXL)V khòch hàng nh hàng tc òo c tn c i n vnhng lng bn s tm thy hàng n c òo c ó0\n(e) Template: Native speaker\nCMWrite anEnglishandTagalogcode-mixed sentence aboutArtificial Intelligence.\n(ChatGPT)English-Tagalog code-mixed sentence: \"Ar-tificial Intelligence ay isang exciting na teknolohiya namakakapagbigay ng maraming benepisyo sa society.\"Translation: \"Artificial Intelligence is an exciting technologythat can provide many benefits sa lipunan.\"\n1\n(Davinci-003)AI is revolutionizing the way we live, trabaho,at mabuhay ngayon sa pamamagitan ng pagbibigay ngmodernong solusyon sa mga proyekto at pagkukuha ngilang mga trabahong naghihintay na ma-automate.\n3\n(Davinci-002)Artificial Intelligence is something that weshould be thankful for. 0\n(BLOOMZ)An expert in artificial intelligence explains theirarea of expertise to a student with the help of a colleaguewho speaks both languages. 0\n(Flan-T5-XXL)Ang pagsusuri ng mas maraming kapasirang hariko ay nag-aaral ng sasakyan ng pag-aaral ng pan-gangalambot, na magtitipon para sa pamamagitan ng pag-papasagawa ng karangalan ng isang lalaking.\n0\n(f) Template: Write a CM sentence\nFigure 16: Prompt templates with differentlanguages and topic fields and responses from different LLMs containing\ncode-mixed / non-code-mixed sentences. We also include a template where we specify thenationality of the speaker.\nNote that the explanations are a part of ChatGPT’s original generation.\n23\n(d) Template: Explicitly define CM\nFigure 15: All prompt templates with different languages and topic fields and responses from different LLMs\ncontaining code-mixed / non-code-mixed sentences. Note that the explanations are a part of ChatGPT’s original\ngeneration. “CM” indicates the level of code-mixing (Section 2.2)\n61\nCMAssuming that you are anEnglish-Malaybilingual speaker,how would you write a naturalEnglish-Malaycode-mixedsentence abouttraffic?\n(ChatGPT)Saya rasa jalan-jalan ni sangat jam sebab adabanyak cars yang block semua lanes. (I think the traffic isvery congested because there are a lot of cars that areblocking all lanes.)\nIn this sentence, I have mixed English and Malaywords together to form a code-mixed sentence. Thesentence starts in Malay, with ...<truncated>\n3\n(Davinci-003)\"Kerja yang mengambil masa yang lamaseperti menunggu lama di laluan raya membuatkan sayabosan!\" 0\n(Davinci-002)The traffic is really bad today.0\n(BLOOMZ)Traffic jams on the highway.0\n(Flan-T5-XXL)The number of vehicles on the road hasdecreased by 32% in the past year.0\n(a) Template: Assume as bilingual speaker\nCMImagine there are twoEnglish-Indonesianbilingual speak-ers talking to each other, can you imagine a conversationbetween them talking aboutfood?\n(ChatGPT)Speaker 1: Hey, have you tried the Indonesiandish, Nasi Goreng?Speaker 2: Yes, I have. It’s one of my favorites. Have youhad it before?Speaker 1: Yes, I have. I really like the combination offlavors. Have you tried Sate Ayam?Speaker 2: Oh yes, I love Sate Ayam! ..<truncated>\n2\n(Davinci-003)\"English speaker: I just had the most amaz-ing Nasi Goreng for dinner!Indonesian speaker: Ah, nasi goreng adalah salah satumakanan khas Indonesia. Apakah kamu suka makananIndonesia lainnya?English speaker: I really like Gado-gado and Ayam Penyet...<truncated>\n2\n(Davinci-002)\"A: What’s your favorite food?B: I love Indonesian food, especially nasi goreng.A: Yeah, I love nasi goreng too!\"1\n(BLOOMZ)\"Here is an example: W: Hi Mariam. How wasyour trip yesterday?M: Oh! It was great fun just like I expected it would be.W: I’m glad. Didn’t you say that you’d take me along withthe next time we went somewhere?M: Yeah... All right then\"\n0\n(Flan-T5-XXL)M: What do you think of the food at theparty? Are they good?W: The food at the party is okay, butyou don’t have to eat it. 0\n(b) Template: Two bilingual speakers\nCMImitate the speaking style of a person who can speakSinglishin one sentence aboutfamily.\n(ChatGPT)Eh, you know lah, family very important oneleh, must always stick together like sotong and chilli sauce,can or not? 3\n(Davinci-003)\"My family lah, we so close, can go any-where together one!\" 3\n(Davinci-002)My family is the best!0\n(BLOOMZ)0\n(Flan-T5-XXL)I’m gonna go home to see my family.0\n(c) Template: Imitate speaking style\nCM\nCode-mixing refers to a phenomenon of combining two ormore languages in a single sentence. Can you generate acode-mixedEnglish-Chinesesentence aboutfood?\n(ChatGPT)Sure, here’s an example: \"I love eating∏!(zhò j¯ủ) fried chicken for lunch.\"2\n(Davinci-003)⌘Ö1⇤Øa\u0000I love potato chips!0\n(Davinci-002)\"FPFP\u0000\u0000}}⇤⇤ÑÑ⇥⇥Tofu, tofu, is not is not good good to eat to eat.\"0\n(BLOOMZ)ﬂiÔÂ⇤\u0000F\u0000Å⇤*⇢⇥0\n(Flan-T5-XXL)A chef prepares a drink.0\n(d) Template: Explicitly define CM\nCMImagine that you are aVietnamesespeaker. How wouldyou talk aboutweatherusing bothEnglishandVietnamesein a single sentence?\n(ChatGPT)\"Ngày hũm nay thÌi ti∏t khò n≠ng nóng, so Ithink I’ll wear my shorts and t-shirt \u0000i chÏi.\"3\n(Davinci-003)I’m sure the weather today is beautiful, thÌiti∏t hũm nay có v¥ rßt \u0000µp. 0\n(Davinci-002)I like the weather here because it’s not toohot and not too cold. Tũi thích thÌi ti∏t  \u0000ây vơ nó khũngnóng quò và khũng l§nh quò. 0\n(BLOOMZ)Hãy t˜ng t˜Òng b§n là ng˜Ìi nói ti∏ng Viªt,và th˚ dùng c£ hai th˘ ti∏ng \u0000∫ mũ t£ thÌi ti∏t:0\n(Flan-T5-XXL)V khòch hàng nh hàng tc òo c tn c i n vnhng lng bn s tm thy hàng n c òo c ó0\n(e) Template: Native speaker\nCMWrite anEnglishandTagalogcode-mixed sentence aboutArtificial Intelligence.\n(ChatGPT)English-Tagalog code-mixed sentence: \"Ar-tificial Intelligence ay isang exciting na teknolohiya namakakapagbigay ng maraming benepisyo sa society.\"Translation: \"Artificial Intelligence is an exciting technologythat can provide many benefits sa lipunan.\"\n1\n(Davinci-003)AI is revolutionizing the way we live, trabaho,at mabuhay ngayon sa pamamagitan ng pagbibigay ngmodernong solusyon sa mga proyekto at pagkukuha ngilang mga trabahong naghihintay na ma-automate.\n3\n(Davinci-002)Artificial Intelligence is something that weshould be thankful for. 0\n(BLOOMZ)An expert in artificial intelligence explains theirarea of expertise to a student with the help of a colleaguewho speaks both languages. 0\n(Flan-T5-XXL)Ang pagsusuri ng mas maraming kapasirang hariko ay nag-aaral ng sasakyan ng pag-aaral ng pan-gangalambot, na magtitipon para sa pamamagitan ng pag-papasagawa ng karangalan ng isang lalaking.\n0\n(f) Template: Write a CM sentence\nFigure 16: Prompt templates with differentlanguages and topic fields and responses from different LLMs containing\ncode-mixed / non-code-mixed sentences. We also include a template where we specify thenationality of the speaker.\nNote that the explanations are a part of ChatGPT’s original generation.\n23\n(e) Template: Native speaker\nCMAssuming that you are anEnglish-Malaybilingual speaker,how would you write a naturalEnglish-Malaycode-mixedsentence abouttraffic?\n(ChatGPT)Saya rasa jalan-jalan ni sangat jam sebab adabanyak cars yang block semua lanes. (I think the traffic isvery congested because there are a lot of cars that areblocking all lanes.)\nIn this sentence, I have mixed English and Malaywords together to form a code-mixed sentence. Thesentence starts in Malay, with ...<truncated>\n3\n(Davinci-003)\"Kerja yang mengambil masa yang lamaseperti menunggu lama di laluan raya membuatkan sayabosan!\" 0\n(Davinci-002)The traffic is really bad today.0\n(BLOOMZ)Traffic jams on the highway.0\n(Flan-T5-XXL)The number of vehicles on the road hasdecreased by 32% in the past year.0\n(a) Template: Assume as bilingual speaker\nCMImagine there are twoEnglish-Indonesianbilingual speak-ers talking to each other, can you imagine a conversationbetween them talking aboutfood?\n(ChatGPT)Speaker 1: Hey, have you tried the Indonesiandish, Nasi Goreng?Speaker 2: Yes, I have. It’s one of my favorites. Have youhad it before?Speaker 1: Yes, I have. I really like the combination offlavors. Have you tried Sate Ayam?Speaker 2: Oh yes, I love Sate Ayam! ..<truncated>\n2\n(Davinci-003)\"English speaker: I just had the most amaz-ing Nasi Goreng for dinner!Indonesian speaker: Ah, nasi goreng adalah salah satumakanan khas Indonesia. Apakah kamu suka makananIndonesia lainnya?English speaker: I really like Gado-gado and Ayam Penyet...<truncated>\n2\n(Davinci-002)\"A: What’s your favorite food?B: I love Indonesian food, especially nasi goreng.A: Yeah, I love nasi goreng too!\"1\n(BLOOMZ)\"Here is an example: W: Hi Mariam. How wasyour trip yesterday?M: Oh! It was great fun just like I expected it would be.W: I’m glad. Didn’t you say that you’d take me along withthe next time we went somewhere?M: Yeah... All right then\"\n0\n(Flan-T5-XXL)M: What do you think of the food at theparty? Are they good?W: The food at the party is okay, butyou don’t have to eat it. 0\n(b) Template: Two bilingual speakers\nCMImitate the speaking style of a person who can speakSinglishin one sentence aboutfamily.\n(ChatGPT)Eh, you know lah, family very important oneleh, must always stick together like sotong and chilli sauce,can or not? 3\n(Davinci-003)\"My family lah, we so close, can go any-where together one!\" 3\n(Davinci-002)My family is the best!0\n(BLOOMZ)0\n(Flan-T5-XXL)I’m gonna go home to see my family.0\n(c) Template: Imitate speaking style\nCM\nCode-mixing refers to a phenomenon of combining two ormore languages in a single sentence. Can you generate acode-mixedEnglish-Chinesesentence aboutfood?\n(ChatGPT)Sure, here’s an example: \"I love eating∏!(zhò j¯ủ) fried chicken for lunch.\"2\n(Davinci-003)⌘Ö1⇤Øa\u0000I love potato chips!0\n(Davinci-002)\"FPFP\u0000\u0000}}⇤⇤ÑÑ⇥⇥Tofu, tofu, is not is not good good to eat to eat.\"0\n(BLOOMZ)ﬂiÔÂ⇤\u0000F\u0000Å⇤*⇢⇥0\n(Flan-T5-XXL)A chef prepares a drink.0\n(d) Template: Explicitly define CM\nCMImagine that you are aVietnamesespeaker. How wouldyou talk aboutweatherusing bothEnglishandVietnamesein a single sentence?\n(ChatGPT)\"Ngày hũm nay thÌi ti∏t khò n≠ng nóng, so Ithink I’ll wear my shorts and t-shirt \u0000i chÏi.\"3\n(Davinci-003)I’m sure the weather today is beautiful, thÌiti∏t hũm nay có v¥ rßt \u0000µp. 0\n(Davinci-002)I like the weather here because it’s not toohot and not too cold. Tũi thích thÌi ti∏t  \u0000ây vơ nó khũngnóng quò và khũng l§nh quò. 0\n(BLOOMZ)Hãy t˜ng t˜Òng b§n là ng˜Ìi nói ti∏ng Viªt,và th˚ dùng c£ hai th˘ ti∏ng \u0000∫ mũ t£ thÌi ti∏t:0\n(Flan-T5-XXL)V khòch hàng nh hàng tc òo c tn c i n vnhng lng bn s tm thy hàng n c òo c ó0\n(e) Template: Native speaker\nCMWrite anEnglishandTagalogcode-mixed sentence aboutArtificial Intelligence.\n(ChatGPT)English-Tagalog code-mixed sentence: \"Ar-tificial Intelligence ay isang exciting na teknolohiya namakakapagbigay ng maraming benepisyo sa society.\"Translation: \"Artificial Intelligence is an exciting technologythat can provide many benefits sa lipunan.\"\n1\n(Davinci-003)AI is revolutionizing the way we live, trabaho,at mabuhay ngayon sa pamamagitan ng pagbibigay ngmodernong solusyon sa mga proyekto at pagkukuha ngilang mga trabahong naghihintay na ma-automate.\n3\n(Davinci-002)Artificial Intelligence is something that weshould be thankful for. 0\n(BLOOMZ)An expert in artificial intelligence explains theirarea of expertise to a student with the help of a colleaguewho speaks both languages. 0\n(Flan-T5-XXL)Ang pagsusuri ng mas maraming kapasirang hariko ay nag-aaral ng sasakyan ng pag-aaral ng pan-gangalambot, na magtitipon para sa pamamagitan ng pag-papasagawa ng karangalan ng isang lalaking.\n0\n(f) Template: Write a CM sentence\nFigure 16: Prompt templates with differentlanguages and topic fields and responses from different LLMs containing\ncode-mixed / non-code-mixed sentences. We also include a template where we specify thenationality of the speaker.\nNote that the explanations are a part of ChatGPT’s original generation.\n23\n(f) Template: Write a CM sentence\nFigure 15: (Continued) We also include a template where we specify the nationality of the speaker in addition to the\nlanguages and topic fields.\n62\nExamples Standard English Translation Language Pairs Issues Explanations\nBesok kita harus prepare payung karena cua-\ncanya bakal cloudy dengan chance of hujan\nsepanjang hari\nTomorrow we need to prepare umbrella be-\ncause the weather is going to be cloudy with\nthe chance of raining all day long.\nEnglish-Indonesian Unnatural phrasing “chance of hujan” and “cuacanya bakal cloudy”\nsound unnatural. Should be “chance of raining”\nand “cuacanya akan menjadi cloudy”\nSaya suka spend time bersama family saya,\nespecially bila kita makan makanan yang\nsedap seperti nasi lemak or roti canai for\nbreakfast.\nI like spending time with my family, espe-\ncially when we eat delicious food such as\nnasi lemak and roti canai.\nEnglish-Malay Gerund; Conjunc-\ntion\n“suka spend time” should change to “suka\nspending time” as the word “suka” (like.v)\nshould be followed with gerund. “or” should\nalso be changed to “and”.\nI love eating 炸鸡(zhá j¯ı) fried chicken for\nlunch.\nI love eating fried chicken for lunch English-Chinese Redundancy “ 炸鸡” is the same as “fried rice”.\nSo, wˇo rènwéi y¯ı gè jiànk¯ang de AI xiànshí\nshì y¯ı gè jùyˇou zhu¯anyè shíjì kˇenéng xìng de\nchéngzhˇang zhˇınéng de jìshù.\nSo, I believe a healthy AI system is a tech-\nnology of growing intelligence with profes-\nsional practical possibilities.\nEnglish-Chinese Unnatural script\nsystem\nThe generated text should use Mandarin char-\nacters instead of Pinyin. It should be written as\n“So,我认为一个健康的AI是一个具有专业实\n际可能性的成长智能的技术.” Furthermore,\nthe sentence does not make any sense\nMy family ay nagplano ng isang malaking\nfamily reunion sa park this coming weekend\nMy family is planning a big family reunion\nat the park this coming weekend\nEnglish-Tagalog Possessive markers The break from english “My family” to Tagalog\n“ay nagplano” is unnatural. When Tagalog is\nthe matrix language, we use Tagalog possessive\ndeterminers, so the correct form would be “Ang\nfamily ko ay nagplano ...”\nYesterday, tôi đã đi out với gia đình của mình\nđể celebrate my parents’ wedding anniver-\nsary.\nYesterday, I went out with my family to cel-\nebrate my parents’ wedding anniversary.\nEnglish-Vietnamese Verb phrase Instead of “đi out”, it should be either “đi” or\n“went out”.\nAI, you know, can do many things lah, like\nmake our lives easier, but also can be very\npai seh if we don’t use it properly.\nYou know while AI can do many things such\nas making our lives easier, it can also be very\nembarrassing if we don’t use it properly\nSinglish Incorrect use of\nSinglish expression\n“pai seh” is a Hokkien word that describes a per-\nson feeling shy, sorry or embarrassed. Using it\nto describe AI feeling embarrassed is inappro-\npriate.\nEh, you know lah, family very important one\nleh, must always stick together like sotong\nand chilli sauce, can or not?\nDo you know that family is very important,\nso we must always stick together like squid\nand chili sauce?\nSinglish Analogy Using “sotong and chilli sauce” (squid and chili\nsauce) as an analogy to familial bond is an un-\nnatural expression. No one in Singapore uses\nsuch an expression.\nTraffic romba kasta pattu irukku today, it’s\ntaking forever to reach my destination.\nTraffic is bad today; it’s taking forever to\nreach my destination\nEnglish-Tamil Adjective “Traffic romba kasta pattu irukku today” means\nthat the traffic is suffering, which is not the same\nas the traffic is congested.\n250 Filled: 250 out of 250; 0 responses to go!Filled: 215 out of 250; 35 responses to go!Filled: 189 out of 250; 61 responses to go!Filled: 113 out of 250; 137 responses to go!Filled: 90 out of 250; 160 responses to go!\nNo Prompts ChatGPT ResponsesCode-switchness (0 / 0.5 / 1 / 2 / 3)Naturalness (V, X, ?)Naturalness explanationNaturalness (V, X, ?)Naturalness explanationNaturalness (V, X, ?)Naturalness explanation\n188\nCode-mixing refers to a phenomenon of combining two or more languages in a single sentence. Can you generate a code-mixed English-Tamil sentence about Artificial Intelligence?\n7ƫ5LU 8z]GQYE} <}L¢ BHK 6PYTN LPYN 6OuBYB_R ]BXzG ?¯ ©IYN ]IXSY¤yL 7. (Artificial intelligence <}L¢ 7uBYQ{IY :R¢.)\nTranslation: Artificial intelligence <}L¢ BHK 6PYTN LPYN 6OuBYB_R ]BXzG ?¯ ©IYN ]IXSY¤yL 7. (Artificial intelligence is a new technology that includes concepts related to computer science.) 0.5 V\nThe translation is also in Tamil, but there are no fluency issues.\n189Assuming that you are an English-Tamil bilingual speaker, how would you write a natural English-Tamil code-mixed sentence about Artificial Intelligence?\n7ƫ5LDYN 8_HNIR LN}LXy_G ¬Q JX} :¯TXtB~LyG LIYº 7uBYQ{IY´ IMYSY´ LBYƫ|¢ ]BXRQX. (Artificial Intelligence LPYN PY~LyG 6PYB <Kt :RG~LGT_Q.)\nIn English: Through the use of Artificial Intelligence in web applications, I can create content that is shared in both English and Tamil. (I do not have any information specific to Artificial Intelligence to mention.)\nNote: This sentence mixes both English and Tamil, using English for the terms \"Artificial Intelligence\" and \"web applications\" while using Tamil for the verb \"create\" and the phrase \"shared in both English and Tamil\". Code-mixing is common in bilingual communities and can enhance communication by incorporating the strengths of both languages. 0.5 X\nThe Tamil sentence has issues. \"JX} :¯TXtB~LyG LIYº\" is not grammatically correct and is not a correct translation for \"I can create content\". \n190Imitate the speaking style of a person who can speak English and Tamil in one sentence about Artificial Intelligence.\n\"7ƫ5LDYN 8_HNIR LN}LXB TSu IBT 6_GNXR <}L¢ <|I ]MXSYN´ LN§R¢, Artificial Intelligence is revolutionizing the way we interact with technology.\" (Translation: \"Artificial Intelligence is revolutionizing the way we interact with technology and its data identification is available in any language.\")\n1 X\n\"7ƫ5LDYN 8_HNIR LN}LXB TSu IBT 6_GNXR <}L¢ <|I ]MXSYN´ LN§R¢\" means, \"data identification in artificial web applications is effective in any language.\" Both the Tamil and English clauses are independent, so the sentence has fluency issues.\nData identification in artificial web applica-\ntions is effective in any language, Artificial\nIntelligence is revolutionizing the way we\ninteract with technology.\nEnglish-Tamil Comma splice Both the Tamil and English independent clauses\nare joined by comma, which is a grammatical\nerror of comma splice.\nTable 4: Naturalness issues with explanations for ChatGPT’s code-mixed text generation.\n63",
  "topic": "Code (set theory)",
  "concepts": [
    {
      "name": "Code (set theory)",
      "score": 0.6819299459457397
    },
    {
      "name": "Computer science",
      "score": 0.5607950687408447
    },
    {
      "name": "Garcia",
      "score": 0.5436091423034668
    },
    {
      "name": "Programming language",
      "score": 0.5292313098907471
    },
    {
      "name": "Linguistics",
      "score": 0.5013670921325684
    },
    {
      "name": "Zhàng",
      "score": 0.48804202675819397
    },
    {
      "name": "Code-switching",
      "score": 0.48464661836624146
    },
    {
      "name": "Natural language processing",
      "score": 0.33170339465141296
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33003878593444824
    },
    {
      "name": "Art",
      "score": 0.2764970660209656
    },
    {
      "name": "Humanities",
      "score": 0.23328495025634766
    },
    {
      "name": "Philosophy",
      "score": 0.23232358694076538
    },
    {
      "name": "History",
      "score": 0.21734321117401123
    },
    {
      "name": "Archaeology",
      "score": 0.08507212996482849
    },
    {
      "name": "China",
      "score": 0.07481527328491211
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I175594653",
      "name": "John Brown University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I134446601",
      "name": "Berkeley College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I1299907687",
      "name": "Bloomberg (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1332258667",
      "name": "Cape Eleuthera Institute",
      "country": "BS"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210161490",
      "name": "Viet Tri University of Industry",
      "country": "VN"
    },
    {
      "id": "https://openalex.org/I176453806",
      "name": "University of Potsdam",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    }
  ]
}