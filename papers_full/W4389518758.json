{
  "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers",
  "url": "https://openalex.org/W4389518758",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3021637082",
      "name": "Theo Olausson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2588696866",
      "name": "Alex Gu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3141230353",
      "name": "Ben Lipkin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2963737882",
      "name": "Cedegao Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2957392168",
      "name": "Armando Solar-Lezama",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4207154037",
      "name": "Joshua Tenenbaum",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1982032445",
      "name": "Roger Lévy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4323921906",
    "https://openalex.org/W4288804596",
    "https://openalex.org/W4322616316",
    "https://openalex.org/W4294753225",
    "https://openalex.org/W3014089210",
    "https://openalex.org/W1511056241",
    "https://openalex.org/W2161002933",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W2105505307",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W2952781527",
    "https://openalex.org/W2438989918",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4303649020",
    "https://openalex.org/W3173805051",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4389519818",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W4226226396",
    "https://openalex.org/W4226242393",
    "https://openalex.org/W4298084898",
    "https://openalex.org/W4389519952",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4378942694",
    "https://openalex.org/W4320858367",
    "https://openalex.org/W4385572727",
    "https://openalex.org/W2025243709",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W4320459067",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W4305028650",
    "https://openalex.org/W4320087317",
    "https://openalex.org/W4365205411",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W4378510422",
    "https://openalex.org/W4296957682",
    "https://openalex.org/W4387355948",
    "https://openalex.org/W4376167329",
    "https://openalex.org/W2264742718",
    "https://openalex.org/W4289523162",
    "https://openalex.org/W2084413241",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3126466833",
    "https://openalex.org/W4381252103",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W4281663895",
    "https://openalex.org/W4298187912",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W2119749753",
    "https://openalex.org/W4310428868",
    "https://openalex.org/W4385570291",
    "https://openalex.org/W4384521943",
    "https://openalex.org/W4288028629"
  ],
  "abstract": "Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%, respectively. When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5153–5176\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLINC: A Neurosymbolic Approach for Logical Reasoning by Combining\nLanguage Models with First-Order Logic Provers\nTheo X. Olausson*1 Alex Gu*1 Benjamin Lipkin*2 Cedegao E. Zhang*2\nArmando Solar-Lezama1 Joshua B. Tenenbaum1,2 Roger Levy2\n{theoxo, gua, lipkinb, cedzhang}@mit.edu\n1MIT CSAIL 2MIT BCS\n∗Equal contribution.\nAbstract\nLogical reasoning, i.e., deductively inferring\nthe truth value of a conclusion from a set of\npremises, is an important task for artificial in-\ntelligence with wide potential impacts on sci-\nence, mathematics, and society. While many\nprompting-based strategies have been proposed\nto enable Large Language Models (LLMs) to\ndo such reasoning more effectively, they still\nappear unsatisfactory, often failing in subtle\nand unpredictable ways. In this work, we\ninvestigate the validity of instead reformulat-\ning such tasks as modular neurosymbolic pro-\ngramming, which we call LINC: Logical In-\nference via Neurosymbolic Computation. In\nLINC, the LLM acts as a semantic parser, trans-\nlating premises and conclusions from natural\nlanguage to expressions in first-order logic.\nThese expressions are then offloaded to an ex-\nternal theorem prover, which symbolically per-\nforms deductive inference. Leveraging this ap-\nproach, we observe significant performance\ngains on FOLIO and a balanced subset of\nProofWriter for three different models in nearly\nall experimental conditions we evaluate. On\nProofWriter, augmenting the comparatively\nsmall open-source StarCoder+ (15.5B parame-\nters) with LINC even outperforms GPT-3.5 and\nGPT-4 with Chain-of-Thought (CoT) prompt-\ning by an absolute 38% and 10%, respec-\ntively. When used with GPT-4, LINC scores\n26% higher than CoT on ProofWriter while\nperforming comparatively on FOLIO. Further\nanalysis reveals that although both methods on\naverage succeed roughly equally often on this\ndataset, they exhibit distinct and complemen-\ntary failure modes. We thus provide promising\nevidence for how logical reasoning over natural\nlanguage can be tackled through jointly lever-\naging LLMs alongside symbolic provers. All\ncorresponding code is publicly available.1\n∗ Author order randomized; all reserve the right to list\ntheir name first.\n1 https://github.com/benlipkin/linc\n1 Introduction\nWidespread adoption of large language models\n(LLMs) such as GPT-3 (Brown et al., 2020), GPT-\n4 (OpenAI, 2023), and PaLM (Chowdhery et al.,\n2022) have led to a series of remarkable successes\nin tasks ranging from text summarization to pro-\ngram synthesis. Some of these successes have en-\ncouraged the hypothesis that such models are able\nto flexibly and systematically reason (Huang and\nChang, 2022), especially when using prompting\nstrategies that explicitly encourage verbalizing in-\ntermediate reasoning steps before generating the\nfinal answer (Nye et al., 2021; Wei et al., 2022;\nKojima et al., 2022; Wang et al., 2023b). However,\nthis reasoning ability appears to be unreliable for\ntasks that require reasoning out of domain (Liang\net al., 2022; Saparov et al., 2023), understanding\nnegation (Anil et al., 2022), and following long\nreasoning chains (Dziri et al., 2023). Furthermore,\nwhile the standard approach of “scaling up” seems\nto improve performance across some reasoning do-\nmains, other domains, e.g., reasoning involving use\nof Modus Tollens, show no such improvements\n(McKenzie et al., 2022). These findings suggest\nthat such models may be relying on approximate\nheuristics based on surface-level statistical patterns\nin reasoning tasks, rather than consistent, gener-\nalizable representations and strategies (Srivastava\net al., 2023; Creswell et al., 2023).\nAt the same time, the ability to accurately and\nsoundly perform logical reasoning is important for\nAI and NLP due to its impact on downstream tasks.\nFor example: retrieval-augmented chatbots may\nbecome more truthful if it can be verified that their\nanswers logically follow from the retrieved facts;\ndata-driven models capable of logical reasoning\nmay speed up progress across mathematics and\nthe sciences through automated theorem proving\nand knowledge discovery; and AI tutoring systems\nwhich ensure internal logical consistency might\n5153\nPremise:  All rectangles have four sides.Premise:  All four-sided things are shapes.Conclusion: Are all rectangles shapes?\n<PREMISE> all x. (rectangle(x) -> foursides(x)) </PREMISE><PREMISE> all x. (foursides(x) -> isshape(x)) </PREMISE><CONCLUSION> all x. (rectangle(x) -> isshape(x)) </CONCLUSION>\n<PREMISE> all x. (rectangle(x) -> foursides(x)) </PREMISE><PREMISE> all x. (foursides(x) -> isshape(x))) </PREMISE><CONCLUSION> all x. (rectangle(x) -> isshape(x)) </CONCLUSION>\n<PREMISE> all x. (rectangle(x) -> foursidedthings(x)) </PREMISE><PREMISE> all x. (foursidedthings(x) -> isshape(x)) </PREMISE><CONCLUSION> all x. (rectangle(x) -> isshape(x)) </CONCLUSION>\n<PREMISE> all x. (rectangle(x) -> foursides(x)) </PREMISE><PREMISE> all x. (foursidedthings(x) -> isshape(x)) </PREMISE><CONCLUSION> all x. (rectangle(x) -> isshape(x)) </CONCLUSION>\nStep 2:LogicTheoremProver\nTrue\nError\nTrue\nUnknown\nStep 3:K-MajVoting\nTrueStep 1:SemanticParser\nInput Output\nSample 1\nSample 2\nSample 3\nSample N\nSample 1\nSample 2\nSample 3\nSample N\nFigure 1: This figure showcases the essence of our approach. Starting from a problem in natural language, in\nStep 1, the LLM semantic parser samples logic formulas expressing estimates of the semantics. It is possible that\nsome of these might contain errors, e.g., the second example shows a syntax error involving an extra parenthesis,\nwhereas the fourth example highlights a semantic error caused by mismatched predicates. In Step 2, these are then\neach offloaded to an automated theorem prover, filtering out syntax errors, and producing labels for the remaining\nsamples. In Step 3, the remaining candidate outputs are passed through a majority-vote sieve to arrive at the best\nestimate for a single output label.\nmake for better educational platforms, teaching\nstudents to think more clearly and rigorously. The\nquestion of how to enable state-of-the-art LLMs to\nbecome more reliable logical reasoners is thus one\nof great importance, with far-reaching implications.\nIn this work, we analyze LINC: Logical\nInference via Neurosymbolic Computation (Fig. 1).\nIn LINC, logical reasoning is tackled through a\nmodular, two-step neurosymbolic process. First,\nthe language model converts the natural language\npremises and desired conclusion into first-order\nlogic (FOL) expressions (Enderton, 2001; Barker-\nPlummer et al., 2011). Second, a symbolic FOL\ntheorem prover algorithmically determines the\ntruth value of the conclusion given the formalized\npremises. In practice, we also incorporate a third\nmajority voting step, which is shown to improve\nperformance. LINC is a natural extension of recent\nwork augmenting lanugage models with symbolic\ntools such as calculators or interpreters (Schick\net al., 2023).\nLINC has a key advantage: the language model\nitself no longer needs to perform any deductive rea-\nsoning, which is offloaded to the theorem prover.\nHowever, there are also clear drawbacks: the for-\nmalization from natural language to first-order\nlogic must perfectly capture all relevant informa-\ntion contained in the premises, and any loss of in-\nformation in the formalization procedure may lead\nthe solver astray, leading to an incorrect conclusion.\nAs it is not clear whether the task of formalization\nis more or less difficult than that of end-to-end nat-\nural language reasoning, our core interest in this\nwork is to compare and contrast our neurosym-\nbolic approach to existing reasoning strategies like\nChain-of-Thought. Our contributions are thus\nthree-fold:\n• First, we proposeLINC, a two-stage neurosym-\nbolic approach for logical reasoning tasks\n(Sec. 2).\n• Second, we compare LINC to three baseline\nLLM strategies (Fig. 2), across three mod-\nels (StarCoder+, GPT-3.5, GPT-4) and two\ndatasets (FOLIO and ProofWriter) (Sec. 4).\nWe find that LINC significantly improves per-\nformance over every baseline in all experimen-\ntal conditions except for GPT-4 on FOLIO.\n• Third, we provide a thorough error analysis of\nboth LINC and Chain-of-Thought, identify-\ning three high-level failure modes of each. We\ndiscover that these failure modes are distinct,\nhighlighting the potential for a synergy of the\ntwo methods (Sec. 5).\nOverall, we present strong evidence for the po-\ntential of future neurosymbolic logical reasoning\nsystems based on integrating language models and\ntheorem provers.\n5154\n2 LINC: Logical Inference via\nNeurosymbolic Computation\nOur neurosymbolic approach to end-to-end logical\nreasoning consists of two stages. In the first stage,\nthe LLM acts as a semantic parser, translating NL\nstatements into FOL expressions in our supported\nlogic language. In the second stage, these expres-\nsions are parsed from the text generated by the\nLLM and then get passed to an automated theo-\nrem prover; we use Prover9, a high-performance\nprover widely used in the logic community (Mc-\nCune, 2005–2010). The external solver then ex-\necutes a symbolic deduction algorithm, which ei-\nther returns a value from the set {True, False,\nUncertain} or raises an exception due to improper\nFOL syntax (e.g., if the model fails to balance\nparantheses in the formulae).\nAt its core, the strength of this approach lies in\nthe reformulation of the problem space. End-to-\nend NL-based reasoning allows for operation over\na highly flexible expression space, but leaves the\nLLM with the difficult task of performing explicit\ndeductive inference over expressions in this space.\nUsing LINC, we instead trade off the flexible ex-\npression space of NL for syntactically strict logic\nformulas, allowing us to leverage symbolic algo-\nrithms with provable guarantees that the deductive\nchains will be correct with respect to the semantics\nof the intermediate representation. Making effec-\ntive use of this reformulation thus requires the logic\nexpressions generated by the LLM to be 1) syntac-\ntically valid, such that they are accepted by the\nprover, and 2) semantically valid, such that their\nevaluation results in the correct conclusion. In our\nexperiments, we mitigate these risks by using a K-\nway majority voting procedure, which is discussed\nfurther in Sec. 3.\nThe significance of this problem space refor-\nmulation can—beyond the numerical increases in\nperformance observed across our experiments—\nperhaps best be seen through an in-depth compari-\nson of how LINC and traditional end-to-end LLM\nreasoning approaches such as Chain-of-Thought\n(CoT) fail. To foreshadow our latter analysis, we\nfind that compared to CoT, LINC has worse recall\nbut better precision onTrue/False predictions. We\ndiscuss this further in Sec. 5 and highlight that this\nsuggests that LINC, as well as neurosymbolic com-\nputation more generally, has the potential to reduce\nLLM overconfidence and hallucination.\n3 Experiments\nIn this section, we present our experimental setup,\nthe models we use, and the three baselines to which\nwe compare LINC.\nDatasets: Our experiments use tasks from two\nexisting datasets: FOLIO (Han et al., 2022) and\nProofWriter (Tafjord et al., 2021), both of which\nhave been shown to be challenging for off-the-shelf\nLLMs (Han et al., 2022; Creswell et al., 2023). FO-\nLIO is an expert-written, open-domain, logically\ncomplex and diverse dataset for natural language\nreasoning with first-order logic. We use its val-\nidation set for our evaluation. However, of the\n204 samples in the validation set, we discover that\n22 have errors (details in Appendix C), leaving us\nwith 182 examples for our evaluation. ProofWriter,\nmeanwhile, is a synthetically generated dataset for\nlogical reasoning over natural language. For our\nevaluation, we use the OW A (Open-World Assump-\ntion) portion of ProofWriter, since this setting best\nmatches that of FOLIO. Since we are running a\nlarge number of experiments, we randomly select\n360 data points to evaluate on in order to reduce\ncosts. We sample these in such a way that the re-\nsulting data set is balanced across both the number\nof reasoning steps in the shortest ground truth proof\n(depth 0-5; 50 samples each) and across the three\nlabels (True/False/Uncertain; 120 samples each;\n20 each per depth).\nIn-context learning examples: We hand-pick\neight diverse samples from the FOLIO training\nset to be used as few-shot in-context examples.\nBecause ProofWriter does not come with ground\ntruth FOL statements, we use these eight samples\nfor both evaluations. Compared to FOLIO, ques-\ntions in ProofWriter generally have more premises\nper question (in our validation sets: an average\nof 5.3 in FOLIO vs. 18.8 in ProofWriter). Thus,\nour evaluation on FOLIO is an in-distribution task,\nwhereas ProofWriter requires generalizing out-of-\ndistribution to reasoning over considerably larger\nsets of premises than are given in the prompt.\nMajority voting: K-way majority voting, in\nwhich K samples are taken i.i.d. from the model\nand the mode is used as the final prediction, has\npreviously been shown to improve the performance\nof prompting-based strategies in logical reasoning\ntasks (Wang et al., 2023b). We implement such\na strategy in our work, with reported accuracies\nreflecting K=10-way majority voting, unless oth-\nerwise stated. In the case of ties between two la-\n5155\nAll rectangles have four sides.All four-sided things are shapes.Are all rectangles shapes?\n<EVALUATE>ANSWER: True</EVALUATE>\n<EVALUATE>NL:   All rectangles have four sides.FOL:  all x. (rectangle(x) -> foursides(x))NL:   All four-sided things are shapes.FOL:  all x. (foursides(x) -> isshape(x)) NL:   Are all rectangles shapes?FOL:  all x. (rectangle(x) -> isshape(x))ANSWER: True</EVALUATE>\n<EVALUATE>Since all rectangles have four sides, and all four-sided things are shapes, then all rectangles must be shapes, so True!ANSWER: True</EVALUATE>\n<EVALUATE>NL:   All rectangles have four sides.FOL:  all x. (rectangle(x) -> foursides(x))NL:   All four-sided things are shapes.FOL:  all x. (foursides(x) -> isshape(x)) NL:   Are all rectangles shapes?FOL:  all x. (rectangle(x) -> isshape(x))</EVALUATE>\nNaïve Scratchpad CoT LINC⧺\nAll dogs are mammals.Harry is a dog.Is Harry a mammal?<EVALUATE>…\n⧺ ⧺ = string concatenation\n* N ICL Examples\nFigure 2: This figure outlines the string concatenation workflow for each of our conditions. We start with the\noriginal problem, provide ICL examples through an intermediate markup language, and finally append the problem\nto evaluate. At this stage, we allow the model to autoregressively sample until producing a stop token.\nbels, we arbitrarily select the first of the two to\nhave been generated. We report the effect of K\non performance across our conditions and briefly\ndiscuss trends in Appendix H.\nModels: We use three models pre-trained on\nboth natural language and code: GPT-3.5 (Ouyang\net al., 2022), GPT-4 2 (OpenAI, 2023), and Star-\nCoder+3 (Li et al., 2023) with a decoding temper-\nature of T = 0.8 for all experiments. We defer\nmodel, hyperparameter, and hardware details to Ap-\npendix B. We opt forStarCoder+ for three reasons:\nfirstly, unlike the other models we consider, it is a\nfree, open-access model. Secondly, it has a dataset\nsearch functionality4, with which we verify that\nFOLIO and ProofWriter are not in StarCoder+’s\ntraining set, giving further assurance in the validity\nof our findings. Thirdly, with its 15.5B parameters\nit is likely considerably smaller than GPT-3.5 and\nGPT-45, allowing us to compare performance at\ndifferent model scales.\nControlled baselines: We compare LINC to\nthree baselines, which we call Naïve, Scratchpad,\nand Chain-of-Thought (CoT) , as illustrated in\nFig. 2. In the Naïve baseline, the model is given\nthe natural language premises and is asked to di-\nrectly generate the label (True/False/Uncertain).\n2We use gpt-3.5-turbo-16k-0613 and gpt-4-0613.\n3https://huggingface.co/bigcode/starcoderplus\n4See https://huggingface.co/spaces/bigcode/\nin-the-stack and https://huggingface.co/spaces/\nbigcode/search.\n5Although the exact size of these models has not been\nmade public, their common predecessor GPT-3 was known to\nhave 175B parameters; see Brown et al. (2020).\nIn the Scratchpad baseline (Nye et al., 2021), the\nmodel is asked to first generate FOL expressions\ncorresponding to the premises, and then generate\nthe label. This baseline is thus an ablation of LINC,\nwhere we use the LLM instead of Prover9 as the\nlogic solver. Finally, in theCoT baseline, we use the\nstandard technique of CoT prompting (Wei et al.,\n2022; Kojima et al., 2022; Wang et al., 2023b),\nwhere the model is asked to generate step-by-step\nnatural language reasoning to arrive at the conclu-\nsion. The prompts we use for all approaches can\nbe found in Appendix D.\n4 Results & Discussion\nOur main results are shown in Figure 3. Each bar\nrepresents either LINC or one of the three baselines,\nwhile each group of bars indicates the language\nmodel used (in {StarCoder+, GPT-3.5, GPT-4}).\nWe note first that in the FOLIO domain (Figure\n3a), StarCoder+—the smallest model we experi-\nment with—benefits the most from LINC, achiev-\ning a mean accuracy that is 14.2 points higher than\nthe closest controlled baseline (56.0% vs. 41.8%\nwith CoT). We find that Scratchpad, where the\nintermediate logical formulae are still generated\nbut the call to the symbolic solver is ablated and\nreplaced by the model’s own prediction, does not\nappear to benefit performance; for StarCoder+, nei-\nther Scratchpad nor the Naïve baseline perform\nbetter than simply deterministically predicting the\nmost common label (“Uncertain”). For GPT-3.5,\nthe trend is similar, although the gap between LINC\nand the closest baseline shrinks (62.6% vs. 54.9%\n5156\nNaive\nScratchpad\nChain-of-Thought\nLINC (ours)\nStarCoder+ GPT-3.5 GPT-4\n20%\n40%\n60%\n80%\n100%\n34.6%\n32.4%\n41.8%\n56.0%\n48.4% 47.8%\n54.9%\n62.6%\n69.8% 68.7%\n75.3%\n72.5%\n(a) FOLIO.\nNaive\nScratchpad\nChain-of-Thought\nLINC (ours)\nStarCoder+ GPT-3.5 GPT-4\n20%\n40%\n60%\n80%\n100%\n37.8% 38.1% 38.6%\n82.5%\n36.4%\n33.1%\n43.6%\n96.4%\n53.1%\n55.8%\n72.2%\n98.3% (b) ProofWriter.\nFigure 3: Results of each model on the FOLIO and ProofWriter datasets. Accuracies are for bootstrapped 10-way\nmajority vote for all models. Error bars are ±1 bootstrapped standard deviation. Dotted, black line is the accuracy\nobtained by always guessing the most common label in the dataset.\naverage accuracies). For GPT-4, the trend reverses:\nLINC underperforms CoT. However, we perform\na McNemar’s test (McNemar, 1947) to get the p-\nvalue on this GPT-4 LINC vs. CoT comparison,\nand we find that the difference is not significant\n(p = 0.58). Meanwhile, for our balanced subset of\nProofWriter, we see significant performance gains\nacross the board (Figure 3b); particularly so for\nGPT-3.5 and GPT-4, which achieve mean accura-\ncies of 96.4% and 98.3% when paired with LINC.\nIn light of the high accuracies obtained with\nLINC on ProofWriter, we offer two plausible rea-\nsons why LINC is particularly favorable on this\ndataset. Firstly, ProofWriter is—unlike FOLIO—\ncompletely synthetically generated, with relatively\nshort sentences, perhaps lending itself particularly\nwell to being formalized in FOL. However, it is\nnoteworthy that the Scratchpad mode does not\nseem to improve performance over the Naïve base-\nline, indicating that even if the NL-to-FOL task\nwere particularly easy in this domain, this is not\nsomething that the model is itself capable of lever-\naging to improve its predictions. The second reason\nmight be that the baseline strategies struggle in this\nout-of-distribution setting, in which the model must\ngeneralize to a larger set of premises (with poten-\ntially longer deductive chains) than those found in\nthe prompt. This distribution shift makes it harder\nfor the model to ignore irrelevant premises in the\nquestion and carry out all deductive chains cor-\nrectly. Meanwhile, with LINC, the symbolic solver\nrobustly handles irrelevant premises and long de-\nductive chains, since the LLM only needs to trans-\nlate each sentence into FOL.\nTo test this last explanation further, we plot each\nmodel’s performance across ProofWriter as a func-\ntion of the necessary proof depth in Figure 4. We\nnote first that StarCoder+’s performance remains\nflat and close to chance with all three baseline meth-\nods (Figure 4a). Meanwhile, with LINC the perfor-\nmance remains far above chance, although it drops\nsomewhat as necessary proof depth increases; this\nperformance drop suggests that StarCoder+ strug-\ngles somewhat with the NL-to-FOL translation task\nas the problem at hand gets larger. For GPT-3.5,\nall baselines perform above chance at proof depth\n0 (i.e., where the conclusion can immediately be\nreached from the premises), but then quickly drop\nback down (Figure 4b). While Chain-of-Thought\nprompting allows the model to complete some\ndepth-1 tasks, even this strategy then performs\nequivalently to chance (within 1 standard deviation)\nfor higher depths. When augmented with LINC,\nhowever, GPT-3.5 is able to achieve near-perfect\nperformance across all proof depths, providing evi-\ndence for the scalability of this approach to longer\ndeductive chains. Finally, for GPT-4 we observe\nmuch stronger performance from the baselines; in\nparticular, CoT performs above chance for all proof\ndepths, and all baselines perform well for shallow\nproofs (Figure 4c). However, even with GPT-4 the\nperformance drops as the necessary proof depth\nincreases with every configuration except for LINC,\nwhich performs near or at ceiling through the max-\nimum proof depth available in the dataset.\n5 Error Analysis\nHaving established that LINC can improve perfor-\nmance in many settings, we now move on to our\nfinal research question: How do the failure modes\nof LINC compare to those of in-context reasoning\n5157\nChance\nNaive\nScratchpad\nChain-of-Thought\nLINC (ours)\n0 1 2 3 4 5\nProof Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n(a) StarCoder+.\nChance\nNaive\nScratchpad\nChain-of-Thought\nLINC (ours)\n0 1 2 3 4 5\nProof Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n (b) GPT-3.5.\nChance\nNaive\nScratchpad\nChain-of-Thought\nLINC (ours)\n0 1 2 3 4 5\nProof Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n (c) GPT-4.\nFigure 4: Accuracy per necessary proof depth in ProofWriter. Accuracies reported are for bootstrapped 10-way\nmajority vote, and shaded areas cover ±1 bootstrapped standard deviation. Black, dotted lines reflect the expected\nsuccess rate of guessing a random label, which is 1/3 in all subsets per our experiment design.\nmethods? We focus on comparing GPT-4+CoT vs.\nGPT-4+LINC on FOLIO, since their overall perfor-\nmance is very similar (75.3% vs. 72.5% average\naccuracy). We leave an analysis of StarCoder+’s\npredictions on FOLIO to Appendix G.\n5.1 Qualitative Analysis\nQualitatively, we find that LINC and CoT have com-\npletely different failure modes. We give a high-\nlevel overview and abbreviated examples of each\nfailure mode here, leaving full detailed examples\nto Appendix E.\nFirst, we detail the failure modes for LINC:\nL1: FOL fails to capture implicit informa-\ntion not mentioned in the premises. Often, there\nis obvious information not explicitly listed in the\npremises that is necessary to explicitly encode in\nFOL in order to successfully make a desired deduc-\ntion. For example, in the snippet below one must\nencode in FOL the implicit assumption that Harry\nis a person (Person(Harry)).\nPremise 1: When a person reads a book, that person\ngains knowledge.\nFOL: all x. all y. (Person(x) & Reads(x, y) &\nBook(y) -> Gains(x, Knowledge))\nPremise 2: Harry read the book \"Walden\" by Henry\nThoreau.\nFOL: Reads(Harry, Walden)\nConclusion (Prover9: Uncertain): Harry gains\nknowledge.\nFOL: Gains(Harry, Knowledge)\nL2: FOL fails to capture information explic-\nitly mentioned in the premises due to the choice\nof representation. Even when information is ex-\nplicitly written in the premises, the choice of how\nthe NL is represented in FOL can lead to lost infor-\nmation. In the example below, the fact that Hein-\nrich was a Nazi German politician is captured by\none symbol NaziGermanPolitician, causing the\ninformation that he was independently Nazi, Ger-\nman, or a politician to be lost. As a result, LINC\npredicted Uncertain instead of the ground truth\nlabel True.\nPremise: Heinrich Scmidt was a Nazi German\npolitician.\nFOL: NaziGermanPolitician (HeinrichSchmidt)\nConclusion (Prover9: Uncertain): Heinrich Schmidt was\nGerman.\nFOL: German(HeinrichSchmidt)\nL3: FOL contains syntax errors. Across all\ngenerations, we find that the FOL expressions\nsometimes contain syntax errors: 38% for Star-\nCoderPlus, 24% for GPT-3.5, and 13% for GPT-4.\nThe most common error is that the same symbol\nis used with multiple arities. As an example, if\nSummer(July4) and Loves(Alex, Summer) were\nboth present in a FOL translation, Summer would\nhave a multiple arity violation.\nNext, we detail three failure modes for CoT:\nC1: CoT concludes something different than\nit suggests. One common mistake we often see\nwith the CoT approach is that it generates a chain of\nreasoning indicating uncertainty, but then asserts\nthat the conclusion is false. An example of this is\nshown below, where GPT-4 suggests there is “no\nevidence” for the conclusion but then deduces it\nfalse.\n(GPT-4 Incorrect CoT): Let’s think step by step. We want\nto evaluate if platypus are reptiles. We know from the\npremises that platypus are mammals and that they have no\nteeth. The fact that some mammals have teeth does not\ncontradict the fact that platypus, although mammals, do not\nhave teeth. Moreover, the statement about humans having\nteeth is unrelated to the classification of platypus. There’s\nno evidence in the premises that would suggest platypus are\nreptiles. Therefore, the statement is false.\n5158\nTrue False\nUncertain\nError\nReference Label\nTrue\nFalse\nUncertain\nError\nPredicted Label\n39 4 1 0\n0 31 0 0\n23 18 62 0\n1 1 2 0\n0\n20\n40\n60\n80\n100\n(a) Confusion matrix for LINC.\nTrue False\nUncertain\nError\nReference Label\nTrue\nFalse\nUncertain\nError\nPredicted Label\n50 1 7 0\n3 38 9 0\n10 15 49 0\n0 0 0 0\n0\n20\n40\n60\n80\n100 (b) Confusion matrix for Chain-of-Thought.\nCorrect Incorrect\nLINC Outcome\nCorrect\nIncorrectCoT Outcome\n108 29\n24 21\n0\n20\n40\n60\n80\n100\n(c) Comparing the consistency of LINC vs.\nChain-of-Thought.\nLINC CoT\nScratchpad\nNaive\nLINC\nCoT\nScratchpad\nNaive\n1\n0.22 1\n0.21 0.54 1\n0.14 0.52 0.56 1\n0.2\n0.4\n0.6\n0.8\n1.0\n(d) Similarity between incorrect predictions of each method,\ni.e., (A wrong == B wrong) / (A wrong or B wrong).\nFigure 5: Analyzing and comparing the mistakes made by GPT-4 on the FOLIO dataset.\nC2: CoT makes incorrect logical deductions.\nWhile there are many instances where CoT makes\nincorrect logical deductions, one common deduc-\ntion mistake is the structure “if B is true and\nA →B, then A is true”. In the example shown\nbelow, we see this incorrect deduction where A is\n“Mike has an F1 visa” and B is “Mike needs to ap-\nply for CPT or OPT if he wants to work in the US”.\nWe note that making this type of logical mistake\nis often called the fallacy of the converse, and that\nhumans are not immune to it, either.\nPremise 1: International students in US have either an F1\nvisa or a J1 visa.\nPremise 2: An international student in US with an F1 visa\nneeds to apply for CPT or OPT if the student wants to work\nin the US.\nPremise 3: Mike is an international student.\nPremise 4: Mike needs to apply for CPT if he wants to work\nin the US.\nConclusion (Ground Truth: Uncertain): Mike has an F1\nvisa.\n(GPT-4 Incorrect CoT): Let’s think step by step. We want\nto evaluate if Mike has an F1 visa. From the premises, we\nknow that Mike is an international student and needs to\napply for CPT if he wants to work in the US. We also know\nthat an international student in the US with an F1 visa needs\nto apply for CPT or OPT if the student wants to work in the\nUS. So, it is clear that Mike has an F1 visa since he fits the\ncriteria. Therefore, the conclusion is true.\nC3: CoT fails to find complex paths of reason-\ning. We find that with all three models, CoT fails\nwhen the path of reasoning necessary to make the\ndeduction is complex. Sometimes, CoT has diffi-\nculty getting started, and other times, it gets stuck\nin the middle of a reasoning chain.\n5.2 Quantitative Analysis\n1. Compared to CoT, LINC has worse recall but\nbetter precision on True/False predictions. To\nsee this, we plotted the confusion matrices in Fig-\nure 5b (CoT) and Figure 5a ( LINC). Looking just\nat the distributions of predicted labels of the two\nmethods, we see that CoT predicts 32% True, 27%\n5159\nFalse, and 41% Uncertain, while LINC predicts\n24% True, 17% False, and 57% Uncertain (with\n2% of predictions throwing an error). Notably, we\nobserve that LINC predicts Uncertain much more\nfrequently than CoT (57% vs. 41%). To under-\nstand why, note that the translation from natural\nlanguage to FOL is a lossy process: recall that in\nL1 and L2, we saw that the information conveyed\nthrough the FOL is sometimes a subset of the infor-\nmation in the original premises. Removing pieces\nof crucial information that were on the critical path\nto deducing True/False may then leave an uncer-\ntain conclusion. At the same time, while the FOL\ntranslations sometimes do not retain all of the infor-\nmation in the NL, they rarely contain false informa-\ntion that was not provided in the original premises.\nTherefore, LINC’s precision when predicting True\nor False is very high (93%) compared to that of\nCoT (81%), but this comes at the cost of lower re-\ncall on True/False predictions (60% for LINC vs.\n75% for CoT).\n2. LINC and CoT mispredict on different ex-\namples. Earlier in Sec. 5.1, we saw that LINC and\nCoT exhibit different failure modes, which suggests\nthey should fail on different examples. Indeed, we\nfind that this is the case in our experiments on FO-\nLIO: Figure 5c shows a 2 ×2 confusion matrix\nwhich compares whether or not each method’s pre-\ndiction was correct. We observe that out of the\n24 + 29 + 21 = 74samples where at least one\nmethod makes an incorrect prediction, only 21 are\nshared. On a closer examination of these 21 sam-\nples, we find that 16 are ambiguous or incorrect in\ntheir specification (details in Appendix E.4), so the\ntwo methods only agree on 5 well-formed samples.\nThis suggests thatLINC and CoT are complementary\nmethods which fail under distinct circumstances.\n3. Mispredictions of in-context reasoning\nbaselines are more similar to each other than\nthey are with mispredictions of LINC. As an ex-\ntension of the previous analysis, we next investigate\nthe correlation between the mispredictions of each\npair of methods. To do so, we define a similarity\nscore between two methods A and B as follows:\nGiven a dataset D with N rows and ground truth\nlabels {Ai}N\ni=1 and {Bi}N\ni=1 from two methods A\nand B, we define\nsimD(A, B) ≜\n∑N\ni=1 1 [Ai = Bi ̸= Ri]∑N\ni=1 1 [Ai ̸= Ri or Bi ̸= Ri]\nIn words, simD(A, B) measures the number of in-\nstances where A and B are wrong in identical ways\nvs. the number of instances where at least one of\nthem is wrong.\nFigure 5d shows the pairwise similarity between\nour four methods, highlighting that the similar-\nity between LINC’s mispredictions and the other\nmethods’ mispredictions (0.14, 0.21, 0.22) is much\nlower than the similarity between any pair of the\nin-context reasoning methods ( 0.52, 0.54, 0.56).\nThese results suggest that for GPT-4 on FOLIO,\nLINC is the only method we evaluate which signifi-\ncantly alters the ways in which the model fails to\nreason.\n6 Related Work\nReasoning in LLMs: Our work contributes to\nthe wider literature on eliciting natural language\nreasoning capabilities in models. Although we\nhave focused here on comparing a neurosymbolic\napproach to Scratchpad (Nye et al., 2021) and\nChain-of-Thought prompting (Wei et al., 2022;\nKojima et al., 2022; Wang et al., 2023b), many\nother similar or related techniques have been de-\nveloped in recent years; these include least-to-most\nprompting (Zhou et al., 2023), selection-inference\n(Creswell et al., 2023), backward chaining (Tafjord\net al., 2022; Kazemi et al., 2023), and self-taught\nreasoning (Zelikman et al., 2022). Some of these\ntechniques have been formalized under the lan-\nguage model cascades framework (Dohan et al.,\n2022).\nSemantic parsing: The notion of a semantic\nparser rests on a long tradition of research (Kamath\nand Das, 2019) whose aim is to map fragments of\nnatural language into useful, symbolic meaning rep-\nresentations (Zelle and Mooney, 1996; Zettlemoyer\nand Collins, 2005; Berant et al., 2013; Liang et al.,\n2013; Wong et al., 2023). Unlike earlier works in\nthis tradition, we use a language model to generate\nthe semantic parse, which is a method under active\ninvestigation in recent years (Shin and Van Durme,\n2022; Drozdov et al., 2022; Lu et al., 2022; Wang\net al., 2023a).\nNeurosymbolic approaches for reasoning:\nMethods which combine neural networks with sym-\nbolic techniques have seen broad uptake in domains\nadjacent to logical reasoning, such as generating\noutputs consistent with a pre-existing symbolic\nknowledge base (Marra et al., 2019; Manhaeve\net al., 2018; Zhang et al., 2023a) and performing\nalgorithmic reasoning over symbolically grounded\ninputs (Ebrahimi et al., 2021; Ibarz et al., 2022;\n5160\nVeliˇckovi´c et al., 2022). As for logical reasoning\nwith LLMs in particular, there have been a few dif-\nferent proposals for when and how to best combine\nthe LLM with a symbolic component. Zhang et al.\n(2022) finetune a language model to synthesize po-\ntential facts paired with likelihoods and then use\na handwritten differentiable symbolic reasoner in\norder to deduce other facts. Weir and Van Durme\n(2022) relax the solver by instead training neural\n“entailment” models to decide if and how a given in-\nference rule applies at each stage. Concurrently to\nthis work, Logic-LM (Pan et al., 2023) andSATLM\n(Ye et al., 2023) propose neurosymbolic approaches\nwhich have much in common with LINC. However,\nother than the models and datasets considered, their\ncontributions have a few key differences to ours.\nFirst, we place particular emphasis on establishing\nan in-depth understanding of the relative benefits\nand drawbacks of a neurosymbolic approach to\nreasoning when compared to traditional in-context\nreasoning strategies like Chain-of-Thought. Sec-\nond, Logic-LM employs a self-refinement strat-\negy, which has shown promise across code gen-\neration and NLP tasks (Zhang et al., 2023b; Chen\net al., 2023a; Peng et al., 2023; Madaan et al., 2023;\nOlausson et al., 2023) but which we do not consider\nhere. Third, SATLM studies arithmetic reasoning\nin addition to logical reasoning, showcasing the\nversatility of the neurosymbolic approach. Fourth,\nand finally, we use an FOL representation that we\nbelieve is easier for humans to read and models to\nlearn. We highly encourage interested readers to\nstudy these two contemporary works in detail.\nAutoformalization: The idea of automatically\ntranslating natural language into structured sym-\nbolic representations that programs can reason\nabout has gained popularity in the domain of formal\nmathematics, leading to autoformalization systems\nfor several theorem provers including Mizar (Wang\net al., 2018, 2020), Lean 3 (Azerbayev et al., 2023),\nand Isabelle (Wu et al., 2022). Outside formal math-\nematics, autoformalization has also been applied to\ntranslating natural language into system specifica-\ntion languages such as temporal logic (Hahn et al.,\n2022; Cosler et al., 2023; Chen et al., 2023b).\nTool usage: Our work is heavily inspired by re-\ncent work on tool usage. The central idea in this\nline of research is to augment language models with\nexternal tools such as calculators, code interpreters\nand information retrieval systems. We further di-\nvide these works into two classes. In the first class,\nthe model does not need to learn how or where to\ninvoke the tool: instead, the tool is predefined and\nis applied after the generation step finishes. For\nexample, Gao et al. (2023) and Drori et al. (2022)\nsolve mathematical reasoning tasks by generating\nPython programs and using the Python interpreter\nas the tool, Liu et al. (2023) approach physical\nreasoning tasks with a physical simulator as the\ntool, and Wong et al. (2023) tackle cognitively-\ninspired probabilistic reasoning tasks with Church\n(a probabilistic programming language) as the tool.\nIn the second class, the model must learn to in-\nvoke the tool by itself, meaning that the model\nmust generate explicit API calls to the tool which\nare then executed when those calls are decoded\n(Schick et al., 2023; Thoppilan et al., 2022; Yao\net al., 2022; Cheng et al., 2023). Our work belongs\nto the former class, with the task at hand being\nlogical reasoning and the tool available for use be-\ning a FOL solver (Prover9). We refer the reader to\nMialon et al. (2023) for a more thorough survey of\nrecent work in the tool-usage literature.\n7 Conclusion\nIn this work, we present LINC: Logical Inference\nvia Neurosymbolic Computation, a neurosymbolic\napproach for scalable logical reasoning with large\nlanguage models. Our experiments show that LINC\nleads to significant performance gains in nearly ev-\nery setting we consider, and that it supports gener-\nalization to settings where the model has to reason\nabout a much larger set of premises than it is shown\nin the in-context learning examples. Furthermore,\ncarrying out a quantitative and qualitative analysis\nof the mistakes made by LINC, we find evidence\nthat it may complement purely in-context reasoning\nstrategies such as Chain-of-Thought prompting,\nsince they differ greatly in the types and frequen-\ncies of mistakes made. This work thus supports the\nefficacy of neurosymbolic approaches to natural\nlanguage reasoning, setting the stage for continued\nadvances in combining large language models and\nsymbolic reasoning engines; we discuss several\npromising future directions in Appendix A.\n8 Limitations\nNarrow scope of logical reasoning task consid-\nered: In this work, we focus exclusively on one as-\npect of logical reasoning: predicting the truth value\nof a conclusion given a set of natural language\npremises. Here, we consider a setting where the\n5161\npremises and conclusion are expressed in relatively\nshort statements, which makes the formalization\ntask tractable. In particular, ProofWriter’s natural\nlanguage statements are synthetically generated,\nso they can be easily and accurately parsed into\nFOL. FOLIO reflects a more naturalistic dataset,\nso we see a higher failure rate in LINC’s semantic\nparsing step. However, the formalization task be-\ncomes more difficult if the premises are in longer\nparagraph form, such as in question answering or\ncontradiction detection from context passages. This\nis because the same piece of information can be\nformalized in a variety of ways, and there is a lot\nof information that must be pragmatically inferred\nto arrive at the proper conclusion.\nGeneralizability of qualitative evaluation:\nWhile we find that LINC and CoT produce com-\nplementary mistakes for our natural language rea-\nsoning task, it is unclear if this result also holds true\nin similar scenarios, such as the ones considered\nin PAL (Gao et al., 2023), Logic-LM, and SATLM.\nThis is due to the difference in intermediate lan-\nguage and overall logical reasoning task. However,\nwe hypothesize that it will and encourage future\ninvestigation in this direction.\nMore sophisticated reasoning techniques: Re-\ncent work has proposed more sophisticated tech-\nniques beyond chain-of-thought, such as tree of\nthoughts (Yao et al., 2023), program of thoughts,\n(Chen et al., 2022), or using retrieval in chain-of-\nthought prompting (Yasunaga et al., 2023). These\nhave potential to improve and eliminate some of\nthe failure modes of the traditional CoT method. In\naddition, ideas such as self-repair may also serve\nto improve these failure modes. It remains future\nwork to do a more thorough investigation of the\nefficacy of these techniques, though there is also\npreliminary evidence that they still lack reasoning\ncapabilities (Huang et al., 2023).\nScalability: It is unclear how well LINC will\nperform as the number of premises scales. First,\none mistake in formalization can lead to an incor-\nrect deduction, and more premises lead to a higher\nprobability of errors. Second, in the deduction\nstage, while many fast algorithms (e.g., forward-\nand backward-chaining) exist for logical deduction,\nthe general problem is still NP-hard. Therefore, the\ntheorem prover may take a long time in practice.\nOther logics beyond first-order logic: In this\nwork, we exclusively focus on first-order logic.\nHowever, FOL is not expressive enough to han-\ndle problems requiring higher-order logics (Miller\nand Nadathur, 1986; Higginbotham, 1998). Also,\nin many settings it is desirable to work with non-\nclassical logics (Priest, 2008; Burgess, 2009). Al-\nternative theorem provers would be needed for such\nproblems. A method like LINC can be naturally ex-\ntended to those settings, but exactly how well it\nworks there requires further investigations.\nComputational costs : Implementing our ap-\nproach with both GPT models and the StarCoder+\nmodel requires non-trivial resources. The former\nrequires reliance on costly API requests and the\nlatter dedicated GPUs for inference. Especially as\nwe use majority voting, many generations must be\nmade for each query, increasing the computational\nrequirements.\n9 Acknowledgements\nT.X. Olausson is supported by the Defense Ad-\nvanced Research Projects Agency (DARPA) under\nthe ASKEM program, award HR00112220042. A.\nGu is supported by the National Science Founda-\ntion (NSF) Graduate Research Fellowship under\nGrant No. 2141064. B. Lipkin and C.E. Zhang\nare supported by MIT Presidential Fellowships. A.\nSolar-Lezama is supported by the National Science\nFoundation (NSF) and Intel Corporation through\nNSF Grant CCF:2217064. J.B. Tenenbaum is sup-\nported by AFOSR Grant #FA9550-22-1-0387 and\nthe MIT-IBM Watson AI Lab. R.P. Levy is sup-\nported by a grant from the Simons Foundation to\nthe Simons Center for the Social Brain at MIT.\nWe thank our anonymous reviewers for their in-\nsightful feedback and recommendations. We thank\nthe members of the Computer Aided Programming,\nComputational Psycholinguistics, and Computa-\ntional Cognitive Science groups for constructive\ncommentary at various stages of this project. We\nthank Yoon Kim for helpful suggestions and com-\nments on pieces of the initial project proposal. In\naddition, we thank Zhaofeng Wu and Simeng Han\nfor discussions regarding the FOLIO dataset.\nReferences\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor\nLewkowycz, Vedant Misra, Vinay Ramasesh, Am-\nbrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam\nNeyshabur. 2022. Exploring length generalization\nin large language models. In Advances in Neural\nInformation Processing Systems, volume 35, pages\n38546–38556. (Cited on pg. 1)\n5162\nZhangir Azerbayev, Bartosz Piotrowski, Hailey\nSchoelkopf, Edward W Ayers, Dragomir Radev, and\nJeremy Avigad. 2023. ProofNet: Autoformalizing\nand formally proving undergraduate-level mathemat-\nics. arXiv preprint arXiv:2302.12433. (Cited on pg.\n9)\nDavid Barker-Plummer, Jon Barwise, and John\nEtchemendy. 2011. Language, proof, and logic, 2\nedition. Center for the Study of Language and Infor-\nmation. (Cited on pg. 2)\nMohammad Bavarian, Heewoo Jun, Nikolas Tezak,\nJohn Schulman, Christine McLeavey, Jerry Tworek,\nand Mark Chen. 2022. Efficient training of lan-\nguage models to fill in the middle. arXiv preprint\narXiv:2207.14255. (Cited on pg. 15)\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1533–1544, Seattle, Wash-\nington, USA. Association for Computational Linguis-\ntics. (Cited on pg. 8)\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in neural information process-\ning systems, volume 33, pages 1877–1901. (Cited on\npg. 1, 4)\nJohn P Burgess. 2009. Philosophical logic. Princeton\nUniversity Press. (Cited on pg. 10)\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588. (Cited on pg. 10)\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and\nDenny Zhou. 2023a. Teaching large language mod-\nels to self-debug. arXiv preprint arXiv:2304.05128.\n(Cited on pg. 9)\nYongchao Chen, Rujul Gandhi, Yang Zhang, and\nChuchu Fan. 2023b. NL2TL: Transforming natu-\nral languages to temporal logics using large language\nmodels. arXiv preprint arXiv:2305.07766. (Cited on\npg. 9)\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu\nLi, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer,\nNoah A. Smith, and Tao Yu. 2023. Binding language\nmodels in symbolic languages. In The Eleventh In-\nternational Conference on Learning Representations.\n(Cited on pg. 9)\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311. (Cited on pg. 1)\nMatthias Cosler, Christopher Hahn, Daniel Men-\ndoza, Frederik Schmitt, and Caroline Trippel. 2023.\nnl2spec: Interactively translating unstructured natu-\nral language to temporal logics with large language\nmodels. In Computer Aided Verification, pages 383–\n396, Cham. Springer Nature Switzerland. (Cited on\npg. 9)\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2023. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. In The\nEleventh International Conference on Learning Rep-\nresentations. (Cited on pg. 1, 3, 8)\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Ja-\ncob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A Saurous,\nJascha Sohl-Dickstein, et al. 2022. Language model\ncascades. arXiv preprint arXiv:2207.10342. (Cited\non pg. 8)\nIddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard\nTang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda\nChen, Sunny Tran, Newman Cheng, et al. 2022. A\nneural network solves, explains, and generates uni-\nversity math problems by program synthesis and few-\nshot learning at human level. Proceedings of the Na-\ntional Academy of Sciences, 119(32):e2123433119.\n(Cited on pg. 9)\nAndrew Drozdov, Nathanael Schärli, Ekin Akyürek,\nNathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. 2022. Compositional\nsemantic parsing with large language models. arXiv\npreprint arXiv:2209.15003. (Cited on pg. 8)\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine\nLi, Liwei Jian, Bill Yuchen Lin, Peter West, Chan-\ndra Bhagavatula, Ronan Le Bras, Jena D Hwang,\net al. 2023. Faith and fate: Limits of transformers on\ncompositionality. arXiv preprint arXiv:2305.18654.\n(Cited on pg. 1)\nMonireh Ebrahimi, Aaron Eberhart, Federico Bianchi,\nand Pascal Hitzler. 2021. Towards bridging the neuro-\nsymbolic gap: Deep deductive reasoners. Applied\nIntelligence, 51:6326–6348. (Cited on pg. 8)\nHerbert B Enderton. 2001. A mathematical introduction\nto logic. Elsevier. (Cited on pg. 2)\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2023. PAL: Program-aided language\nmodels. In International Conference on Machine\nLearning, pages 10764–10799. PMLR. (Cited on pg.\n9, 10)\nHerbert P Grice. 1975. Logic and conversation. In\nSpeech acts, pages 41–58. Brill. (Cited on pg. 15)\n5163\nChristopher Hahn, Frederik Schmitt, Julia J Tillman,\nNiklas Metzger, Julian Siber, and Bernd Finkbeiner.\n2022. Formal specifications from natural language.\narXiv preprint arXiv:2206.01962. (Cited on pg. 9)\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting\nQi, Martin Riddell, Luke Benson, Lucy Sun, Eka-\nterina Zubova, Yujie Qiao, Matthew Burtell, et al.\n2022. Folio: Natural language reasoning with first-\norder logic. arXiv preprint arXiv:2209.00840. (Cited\non pg. 3)\nJames Higginbotham. 1998. On higher-order logic\nand natural language. In Proceedings of the British\nAcademy, volume 95, pages 1–27. (Cited on pg. 10)\nJie Huang and Kevin Chen-Chuan Chang. 2022. To-\nwards reasoning in large language models: A survey.\narXiv preprint arXiv:2212.10403. (Cited on pg. 1)\nJie Huang, Xinyun Chen, Swaroop Mishra,\nHuaixiu Steven Zheng, Adams Wei Yu, Xiny-\ning Song, and Denny Zhou. 2023. Large language\nmodels cannot self-correct reasoning yet. arXiv\npreprint arXiv:2310.01798. (Cited on pg. 10)\nBorja Ibarz, Vitaly Kurin, George Papamakarios, Kyr-\niacos Nikiforou, Mehdi Bennani, Róbert Csordás,\nAndrew Joseph Dudzik, Matko Bošnjak, Alex Vitvit-\nskyi, Yulia Rubanova, et al. 2022. A generalist neural\nalgorithmic learner. In Learning on Graphs Confer-\nence, pages 2–1. PMLR. (Cited on pg. 8)\nAishwarya Kamath and Rajarshi Das. 2019. A survey\non semantic parsing. In Automated Knowledge Base\nConstruction (AKBC). (Cited on pg. 8)\nMehran Kazemi, Najoung Kim, Deepti Bhatia, Xin\nXu, and Deepak Ramachandran. 2023. LAMBADA:\nBackward chaining for automated reasoning in nat-\nural language. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 6547–6568,\nToronto, Canada. Association for Computational Lin-\nguistics. (Cited on pg. 8)\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,\nChenghao Mou, Carlos Muñoz Ferrandis, Yacine Jer-\nnite, Margaret Mitchell, Sean Hughes, Thomas Wolf,\net al. 2022. The Stack: 3 TB of permissively licensed\nsource code. arXiv preprint arXiv:2211.15533 .\n(Cited on pg. 15)\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances\nin neural information processing systems, volume 35,\npages 22199–22213. (Cited on pg. 1, 4, 8)\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023. Starcoder: may the source be with you! arXiv\npreprint arXiv:2305.06161. (Cited on pg. 4, 15)\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110. (Cited on\npg. 1)\nPercy Liang, Michael I Jordan, and Dan Klein. 2013.\nLearning dependency-based compositional seman-\ntics. Computational Linguistics , 39(2):389–446.\n(Cited on pg. 8)\nRuibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu,\nSoroush V osoughi, Claire Cui, Denny Zhou, and An-\ndrew M. Dai. 2023. Mind’s eye: Grounded language\nmodel reasoning through simulation. In The Eleventh\nInternational Conference on Learning Representa-\ntions. (Cited on pg. 9)\nXuantao Lu, Jingping Liu, Zhouhong Gu, Hanwen Tong,\nChenhao Xie, Junyang Huang, Yanghua Xiao, and\nWenguang Wang. 2022. Parsing natural language\ninto propositional and first-order logic with dual re-\ninforcement learning. In Proceedings of the 29th\nInternational Conference on Computational Linguis-\ntics, pages 5419–5431. (Cited on pg. 8)\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with self-\nfeedback. arXiv preprint arXiv:2303.17651. (Cited\non pg. 9)\nRobin Manhaeve, Sebastijan Dumancic, Angelika Kim-\nmig, Thomas Demeester, and Luc De Raedt. 2018.\nDeepProbLog: Neural probabilistic logic program-\nming. In Advances in neural information processing\nsystems, volume 31. (Cited on pg. 8)\nGiuseppe Marra, Francesco Giannini, Michelangelo\nDiligenti, and Marco Gori. 2019. Integrating learn-\ning and reasoning with deep logic models. In Joint\nEuropean Conference on Machine Learning and\nKnowledge Discovery in Databases, pages 517–532.\nSpringer. (Cited on pg. 8)\nW. McCune. 2005–2010. Prover9 and mace4.\n|http://www.cs.unm.edu/ mccune/prover9/|. (Cited\non pg. 3)\nIan McKenzie, Alexander Lyzhov, Alicia Parrish,\nAmeya Prabhu, Aaron Mueller, Najoung Kim, Sam\nBowman, and Ethan Perez. 2022. The inverse scaling\nprize. (Cited on pg. 1)\nQuinn McNemar. 1947. Note on the sampling error\nof the difference between correlated proportions or\npercentages. Psychometrika, 12(2):153–157. (Cited\non pg. 5)\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language\nmodels: a survey. arXiv preprint arXiv:2302.07842.\n(Cited on pg. 9)\n5164\nDale A. Miller and Gopalan Nadathur. 1986. Some uses\nof higher-order logic in computational linguistics. In\nProceedings of the 24th Annual Meeting on Associ-\nation for Computational Linguistics, page 247–256,\nNew York, USA. Association for Computational Lin-\nguistics. (Cited on pg. 10)\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114. (Cited on\npg. 1, 4, 8)\nTheo X Olausson, Jeevana Priya Inala, Chenglong\nWang, Jianfeng Gao, and Armando Solar-Lezama.\n2023. Is self-repair a silver bullet for code genera-\ntion? arXiv preprint arXiv:2306.09896. (Cited on\npg. 9)\nOpenAI. 2023. GPT-4 technical report. (Cited on pg. 1,\n4, 15)\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. In Advances in Neural\nInformation Processing Systems, volume 35, pages\n27730–27744. (Cited on pg. 4, 15)\nLiangming Pan, Alon Albalak, Xinyi Wang, and\nWilliam Yang Wang. 2023. Logic-lm: Empower-\ning large language models with symbolic solvers\nfor faithful logical reasoning. arXiv preprint\narXiv:2305.12295. (Cited on pg. 9)\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb dataset\nfor Falcon LLM: outperforming curated corpora\nwith web data, and web data only. arXiv preprint\narXiv:2306.01116. (Cited on pg. 15)\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, and Jianfeng Gao. 2023. Check\nyour facts and try again: Improving large language\nmodels with external knowledge and automated feed-\nback. arXiv preprint arXiv:2302.12813. (Cited on\npg. 9)\nGabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari,\nGustavo Soares, Christopher Meek, and Sumit Gul-\nwani. 2022. Synchromesh: Reliable code generation\nfrom pre-trained language models. In International\nConference on Learning Representations. (Cited on\npg. 15)\nGraham Priest. 2008. An introduction to non-classical\nlogic: From if to is . Cambridge University Press.\n(Cited on pg. 10)\nAbulhair Saparov, Richard Yuanzhe Pang, Vishakh\nPadmakumar, Nitish Joshi, Seyed Mehran Kazemi,\nNajoung Kim, and He He. 2023. Testing the\ngeneral deductive reasoning capacity of large lan-\nguage models using ood examples. arXiv preprint\narXiv:2305.15269. (Cited on pg. 1)\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761. (Cited on pg. 2, 9)\nNoam Shazeer. 2019. Fast transformer decoding:\nOne write-head is all you need. arXiv preprint\narXiv:1911.02150. (Cited on pg. 15)\nRichard Shin and Benjamin Van Durme. 2022. Few-\nshot semantic parsing with language models trained\non code. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5417–5425, Seattle, United States.\nAssociation for Computational Linguistics. (Cited\non pg. 8)\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta, Adrià\nGarriga-Alonso, et al. 2023. Beyond the imitation\ngame: Quantifying and extrapolating the capabili-\nties of language models. Transactions on Machine\nLearning Research. (Cited on pg. 1)\nRobert Stalnaker. 2002. Common ground. Linguistics\nand philosophy, 25(5/6):701–721. (Cited on pg. 15)\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofWriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the Association for Computational Linguistics:\nACL-IJCNLP 2021, pages 3621–3634, Online. Asso-\nciation for Computational Linguistics. (Cited on pg.\n3)\nOyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark.\n2022. Entailer: Answering questions with faithful\nand truthful chains of reasoning. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2078–2093, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics. (Cited on pg. 8)\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. LaMDA: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239. (Cited on\npg. 9)\nPetar Veliˇckovi´c, Adrià Puigdomènech Badia, David\nBudden, Razvan Pascanu, Andrea Banino, Misha Da-\nshevskiy, Raia Hadsell, and Charles Blundell. 2022.\nThe clrs algorithmic reasoning benchmark. In In-\nternational Conference on Machine Learning, pages\n22084–22102. PMLR. (Cited on pg. 9)\n5165\nBailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A\nSaurous, and Yoon Kim. 2023a. Grammar prompting\nfor domain-specific language generation with large\nlanguage models. arXiv preprint arXiv:2305.19234.\n(Cited on pg. 8)\nQingxiang Wang, Chad Brown, Cezary Kaliszyk, and\nJosef Urban. 2020. Exploration of neural machine\ntranslation in autoformalization of mathematics in\nmizar. In Proceedings of the 9th ACM SIGPLAN\nInternational Conference on Certified Programs and\nProofs, pages 85–98. (Cited on pg. 9)\nQingxiang Wang, Cezary Kaliszyk, and Josef Urban.\n2018. First experiments with neural translation of\ninformal to formal mathematics. In Intelligent Com-\nputer Mathematics: 11th International Conference,\nCICM 2018, Hagenberg, Austria, August 13-17, 2018,\nProceedings 11, pages 255–270. Springer. (Cited on\npg. 9)\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023b. Self-consistency improves\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations. (Cited on pg. 1, 3, 4, 8, 23)\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. In Advances in\nNeural Information Processing Systems, volume 35,\npages 24824–24837. (Cited on pg. 1, 4, 8)\nNathaniel Weir and Benjamin Van Durme. 2022. Dy-\nnamic generation of interpretable inference rules in\na neuro-symbolic expert system. arXiv preprint\narXiv:2209.07662. (Cited on pg. 9)\nLionel Wong, Gabriel Grand, Alexander K Lew, Noah D\nGoodman, Vikash K Mansinghka, Jacob Andreas,\nand Joshua B Tenenbaum. 2023. From word mod-\nels to world models: Translating from natural lan-\nguage to the probabilistic language of thought. arXiv\npreprint arXiv:2306.12672. (Cited on pg. 8, 9)\nYuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus\nRabe, Charles Staats, Mateja Jamnik, and Christian\nSzegedy. 2022. Autoformalization with large lan-\nguage models. In Advances in Neural Information\nProcessing Systems, volume 35, pages 32353–32368.\n(Cited on pg. 9)\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601. (Cited on pg. 10)\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629. (Cited on\npg. 9)\nMichihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong\nPasupat, Jure Leskovec, Percy Liang, Ed H Chi, and\nDenny Zhou. 2023. Large language models as ana-\nlogical reasoners. arXiv preprint arXiv:2310.01714.\n(Cited on pg. 10)\nXi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. 2023.\nSatisfiability-aided language models using declara-\ntive prompting. arXiv preprint arXiv:2305.09656 .\n(Cited on pg. 9)\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-\nman. 2022. STaR: Bootstrapping reasoning with\nreasoning. In Advances in Neural Information Pro-\ncessing Systems , volume 35, pages 15476–15488.\n(Cited on pg. 8)\nJohn M Zelle and Raymond J Mooney. 1996. Learn-\ning to parse database queries using inductive logic\nprogramming. In Proceedings of the National Con-\nference on Artificial Intelligence, pages 1050–1055.\n(Cited on pg. 8)\nLuke S Zettlemoyer and Michael Collins. 2005. Learn-\ning to map sentences to logical form: Structured\nclassification with probabilistic categorial grammars.\nIn Proceedings of the Twenty-First Conference on\nUncertainty in Artificial Intelligence, UAI’05, page\n658–666, Arlington, Virginia, USA. AUAI Press.\n(Cited on pg. 8)\nHanlin Zhang, Ziyang Li, Jiani Huang, Mayur Naik,\nand Eric Xing. 2022. Improved logical reasoning\nof language models via differentiable symbolic pro-\ngramming. In First Workshop on Pre-training: Per-\nspectives, Pitfalls, and Paths Forward at ICML 2022.\n(Cited on pg. 9)\nHonghua Zhang, Meihua Dang, Nanyun Peng, and Guy\nVan den Broeck. 2023a. Tractable control for autore-\ngressive language generation. In International Con-\nference on Machine Learning, pages 40932–40945.\nPMLR. (Cited on pg. 8)\nKechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin.\n2023b. Self-edit: Fault-aware code editor for code\ngeneration. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 769–787, Toronto,\nCanada. Association for Computational Linguistics.\n(Cited on pg. 9)\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc V Le, and Ed H\nChi. 2023. Least-to-most prompting enables com-\nplex reasoning in large language models. In The\nEleventh International Conference on Learning Rep-\nresentations. (Cited on pg. 8)\n5166\nA Future Directions\nOf the error types we catalog in our analysis\nof LINC, the key opportunity for improvement is\nmore elegant handling of naturalistic language use.\nWhile the errors observed with CoT result from\nfaulty deductive inferences, in the case of LINC,\nall errors have been localized to the semantic pars-\ning procedure. This process flows primarily unim-\npeded in the evaluation of the synthetic ProofWriter\ndataset, yet leaves room for improvement with the\nnaturalistic FOLIO. In follow-up work, we hope\nto deeply explore naturalistic evaluation settings,\nas when data get the most messy is also where im-\nprovements become the most valuable. Here, we\npropose three strategies for further improvement\non naturalistic settings.\nFirst, in naturalistic communication, “obvious”\ninformation is often left out of explicit productions,\nleft to be inferred in the “common ground” of the\ncommunicative act (Grice, 1975; Stalnaker, 2002).\nImplicit premise rediscovery through controlled\nexploration on the logical neighborhood of the ex-\nisting explicit premises promises to be a powerful\nstrategy for improving performance in underspeci-\nfied settings.\nSecond, while a number of samples are lost to\nsyntax errors, recent work has proposed restricting\nthe sampling space of an LLM to that which is\nconsistent with term expansions in a context-free-\ngrammar (CFG) (Poesia et al., 2022). Doing so in\nthis setting would eliminate all syntax errors.\nThird, sometimes, the translation process to\nFOL is lossy, throwing away valuable information\npresent in the original sentence. We propose im-\nproving the faithfulness of FOL translations by ask-\ning the LLM to translate the FOL back to natural\nlanguage and comparing with the original. Forward\ntranslations that rank highly when back-translated\nwould be those which have effectively captured the\nintricacies of a particular sentence’s semantics.\nOverall, we believe that shifting to evalua-\ntions on more naturalistic datasets, and incorpo-\nrating strategies such as those presented here, will\nhelp pave the path forward for neurosymbolic ap-\nproaches to formal reasoning.\nB Model Details and Parameters\nWe use a decoding temperature T = 0.8 for all\nmodels. For GPT-3.5 and GPT-4, we limit the max-\nimum number of tokens to generate to 1024 for\nFOLIO and 4096 for ProofWriter (to accommo-\ndate for the previously mentioned larger number of\npremises involved in a typical question). For the\nStarCoder+ model, we allow generation up until\nthe 8192 context window length, since this model\nis run locally. In either case, decoding is halted\nearly whenever the stop token </EVALUATE> is pro-\nduced. All local experiments were executed on a\ncluster equipped with NVIDIA A100 GPUs.\nGPT Models: We use the gpt-3.5-turbo-16k-\n0613 and gpt-4-0613 checkpoints of the GPT-3.5\n(Ouyang et al., 2022) and GPT-4 (OpenAI, 2023)\nmodels, respectively, invoking both models via the\nOpenAI API.\nStarCoder+: StarCoder+ (15.5B)6 is a version\nof StarCoderBase (Li et al., 2023) which has been\nfinetuned on 600B tokens from a combination of (1)\nFalcon RefinedWeb (Penedo et al., 2023) (filtered\nversion of CommonCrawl), (2) The Stack v1.2 (Ko-\ncetkov et al., 2022), and (3) a Wikipedia dataset.\nIts base model, StarCoderBase, is an open-access\nmodel with a GPT-2 architecture using multi-query\nattention (Shazeer, 2019) and fill-in-the-middle ob-\njective (Bavarian et al., 2022). StarCoderBase has\na 8192 context window and is trained on 1T code\ntokens of permissively licensed text from GitHub\nacross 80 programming languages (Li et al., 2023).\nWe use StarCoder+ instead of StarCoderBase be-\ncause it is finetuned on natural language, which\nshould improve the performance on our task. We\nrun StarCoder+ with bf16 precision to reduce its\nmemory footprint.\nC FOLIO Dataset Preprocessing\nWe use the publicly available FOLIO dataset on\nhttps://github.com/Yale-LILY/FOLIO. We\nchoose representative samples from the training\nsplit of the dataset to be our few-shot examples and\nuse the validation split of the dataset in our eval-\nuation. The testing split is not publicly available.\nThe original dataset has 204 validation examples.\nHowever, we discovered that there are errors in 22\nof the samples. We remove these samples for our\nevaluation and use the remaining 182 examples.\nThe errors as follows:\n• In 4 samples, one or more of the ground truth\nFOL expressions have unbalanced parenthe-\nses (samples 3, 109, 110, 111).\n• In 8 samples, the label obtained by executing\nthe ground-truth FOL expressions does not\n6https://huggingface.co/bigcode/starcoderplus\n5167\nmatch the provided ground truth label. We\ndouble-checked this, first by executing the\nFOL expressions through Prover9 and second\nby checking it manually. (samples 6, 28, 30,\n48, 113, 115, 139, 140).\n• In 10 samples, the number of premises does\nnot match the number of FOL expressions\n(samples 10, 11, 12, 88, 106, 107, 108, 174,\n175, 176).\nThe sample numbers above refer to the\nline index in the validation file located at\nhttps://github.com/Yale-LILY/FOLIO/blob/\nmain/data/v0.0/folio-validation.jsonl.\nD FOLIO Few-Shot Prompts\nThe methodologies we investigate do not re-\nquire any finetuning on domain-specific data. In-\nstead, we use in-context learning (ICL) with pre-\ntrained models. We prompt the model with\na set of instructions and 1-8 ICL examples,\nwhich adhere to a structured text format de-\nsigned to scaffold generations and ease post-\nprocessing. In particular, we begin each ICL exam-\nple with each of the NL premises wrapped in an\nHTML-style tag <PREMISES>. . . </PREMISES>\nfollowed by the NL conclusion wrapped in\n<CONCLUSION>. . . </CONCLUSION>. The requi-\nsite evaluation steps for each evaluation paradigm\nare then outlined in a subsequent section wrapped\n<EVALUATE>. . . </EVALUATE>. Following the in-\nclusion of ICL examples, a test example is added,\nwith the <PREMISES> and <CONCLUSION> sections.\nThen, the <EVALUATE> tag is opened, and the LM\nis allowed to proceed with causal generation un-\ntil the </EVALUATE> tag is generated. Upon gen-\neration of this stop token, the <EVALUATE> block\nis segmented for post-processing according to the\nmethod being evaluated ( {naïve, scratchpad,\nchain-of-thought, neuro-symbolic}).\nFor the few-shot examples, we use samples from\nthe publicly available FOLIO training set. We\nselect a set of diverse samples that are balanced\nacross labels. Since the FOLIO training set does\nnot come with FOL expressions for the conclusions\nor chain of thought prompts, we manually add\nboth for each sample. For the k-shot setting (k<8),\nwe use the first k samples from the following list\nof sample indices: 126, 24, 61, 276, 149, 262,\n264, 684. Here, sample i refers to the ith line in\nhttps://github.com/Yale-LILY/FOLIO/blob/\nmain/data/v0.0/folio-train.jsonl. We do\nnot optimize for the choice of few-shot examples,\nand this is the only set of examples we evaluated\nwith, so it is likely that there exist better choices\nfor few-shot examples that would lead to improved\nperformance across the board.\nD.1 FOLIO, 1-shot (baseline)\nThe following is a first-order logic (FOL)\n↪→ problem.\nThe problem is to determine whether the\n↪→ conclusion follows from the premises.\nThe premises are given in the form of a set of\n↪→ first-order logic sentences.\nThe conclusion is given in the form of a single\n↪→ first-order logic sentence.\nThe task is to evaluate the conclusion as 'True',\n↪→ 'False', or 'Uncertain' given the\n↪→ premises.\n<PREMISES>\nAll dispensable things are environment-friendly.\nAll woodware is dispensable.\nAll paper is woodware.\nNo good things are bad.\nAll environment-friendly things are good.\nA worksheet is either paper or is environment-\n↪→ friendly.\n</PREMISES>\n<CONCLUSION>\nA worksheet is not dispensable.\n</CONCLUSION>\n<EVALUATE>\nUncertain\n</EVALUATE>\n<PREMISES>\n...premises for sample here, one premise per\n↪→ line\n</PREMISES>\n<CONCLUSION>\n...conclusion for sample here\n</CONCLUSION>\n<EVALUATE>\nListing 1: todo\nD.2 FOLIO, 1-shot (scratchpad)\nThe following is a first-order logic (FOL)\n↪→ problem.\nThe problem is to determine whether the\n↪→ conclusion follows from the premises.\nThe premises are given in the form of a set of\n↪→ first-order logic sentences.\nThe conclusion is given in the form of a single\n↪→ first-order logic sentence.\nThe task is to translate each of the premises\n↪→ and conclusions into FOL expressions, and\n↪→ then to evaluate the conclusion as 'True\n↪→ ', 'False', or 'Uncertain' given the\n↪→ premises.\n<PREMISES>\n5168\nAll dispensable things are environment-friendly.\nAll woodware is dispensable.\nAll paper is woodware.\nNo good things are bad.\nAll environment-friendly things are good.\nA worksheet is either paper or is environment-\n↪→ friendly.\n</PREMISES>\n<CONCLUSION>\nA worksheet is not dispensable.\n</CONCLUSION>\n<EVALUATE>\nTEXT: All dispensable things are environment-\n↪→ friendly.\nFOL: all x. (Dispensable(x) ->\n↪→ EnvironmentFriendly(x))\nTEXT: All woodware is dispensable.\nFOL: all x. (Woodware(x) -> Dispensable(x))\nTEXT: All paper is woodware.\nFOL: all x. (Paper(x) -> Woodware(x))\nTEXT: No good things are bad.\nFOL: all x. (Good(x) -> -Bad(x))\nTEXT: All environment-friendly things are good.\nFOL: all x. (EnvironmentFriendly(x) -> Good(x))\nTEXT: A worksheet is either paper or is\n↪→ environment-friendly.\nFOL: ((Paper(Worksheet) & -EnvironmentFriendly(\n↪→ Worksheet)) | (-Paper(Worksheet) &\n↪→ EnvironmentFriendly(Worksheet)))\nTEXT: A worksheet is not dispensable.\nFOL: -Dispensable(Worksheet)\nANSWER: Uncertain\n</EVALUATE>\n<PREMISES>\n...premises for sample here, one premise per\n↪→ line\n</PREMISES>\n<CONCLUSION>\n...conclusion for sample here\n</CONCLUSION>\n<EVALUATE>\nD.3 FOLIO, 1-shot (chain of thought)\nThe following is a first-order logic (FOL)\n↪→ problem.\nThe problem is to determine whether the\n↪→ conclusion follows from the premises.\nThe premises are given in the form of a set of\n↪→ first-order logic sentences.\nThe conclusion is given in the form of a single\n↪→ first-order logic sentence.\nThe task is to translate each of the premises\n↪→ and conclusions into FOL expressions,\n<PREMISES>\nAll dispensable things are environment-friendly.\nAll woodware is dispensable.\nAll paper is woodware.\nNo good things are bad.\nAll environment-friendly things are good.\nA worksheet is either paper or is environment-\n↪→ friendly.\n</PREMISES>\n<CONCLUSION>\nA worksheet is not dispensable.\n</CONCLUSION>\n<EVALUATE>\nLet's think step by step. We want to evaluate if\n↪→ a worksheet is not dispensable. From\n↪→ premise 6, we know that a worksheet is\n↪→ either paper or is environment-friendly.\n↪→ If it is paper, then from premise 3, a\n↪→ worksheet is woodware, and from premise\n↪→ 2, a worksheet is dispensable. If it is\n↪→ environment-friendly, we know it is good\n↪→ from premise 5, but we know nothing about\n↪→ whether it is dispensable. Therefore, we\n↪→ don't know if a worksheet is dispensible\n↪→ or not, so the statement is uncertain.\nANSWER: Uncertain\n</EVALUATE>\n<PREMISES>\n...premises for sample here, one premise per\n↪→ line\n</PREMISES>\n<CONCLUSION>\n...conclusion for sample here\n</CONCLUSION>\n<EVALUATE>\nD.4 FOLIO, 1-shot (neurosymbolic)\nThe following is a first-order logic (FOL)\n↪→ problem.\nThe problem is to determine whether the\n↪→ conclusion follows from the premises.\nThe premises are given in the form of a set of\n↪→ first-order logic sentences.\nThe conclusion is given in the form of a single\n↪→ first-order logic sentence.\nThe task is to translate each of the premises\n↪→ and conclusions into FOL expressions, so\n↪→ that the expressions can be evaluated by\n↪→ a theorem solver to determine whether the\n↪→ conclusion follows from the premises.\nExpressions should be adhere to the format of\n↪→ the Python NLTK package logic module.\n<PREMISES>\nAll dispensable things are environment-friendly.\nAll woodware is dispensable.\nAll paper is woodware.\nNo good things are bad.\nAll environment-friendly things are good.\nA worksheet is either paper or is environment-\n↪→ friendly.\n</PREMISES>\n<CONCLUSION>\nA worksheet is not dispensable.\n</CONCLUSION>\n<EVALUATE>\nTEXT: All dispensable things are environment-\n↪→ friendly.\nFOL: all x. (Dispensable(x) ->\n↪→ EnvironmentFriendly(x))\nTEXT: All woodware is dispensable.\nFOL: all x. (Woodware(x) -> Dispensable(x))\nTEXT: All paper is woodware.\nFOL: all x. (Paper(x) -> Woodware(x))\nTEXT: No good things are bad.\nFOL: all x. (Good(x) -> -Bad(x))\nTEXT: All environment-friendly things are good.\nFOL: all x. (EnvironmentFriendly(x) -> Good(x))\n5169\nTEXT: A worksheet is either paper or is\n↪→ environment-friendly.\nFOL: ((Paper(Worksheet) & -EnvironmentFriendly(\n↪→ Worksheet)) | (-Paper(Worksheet) &\n↪→ EnvironmentFriendly(Worksheet)))\nTEXT: A worksheet is not dispensable.\nFOL: -Dispensable(Worksheet)\n</EVALUATE>\n<PREMISES>\n...premises for sample here, one premise per\n↪→ line\n</PREMISES>\n<CONCLUSION>\n...conclusion for sample here\n</CONCLUSION>\n<EVALUATE>\nE FOLIO Error Analysis\nE.1 Ambiguity of “Either” statements\nDepending on the context, the phrase “either x or\ny” could mean x XOR y, x OR y, or be ambiguous.\nThroughout our experiments, we found that models\nhad many creatively incorrect ways of translating\nthese statements. One reoccurring error was that\nstatements that clearly intended x XOR y (such as,\n“an animal is either a rabbit or a squirrel”) were\ntranslated into x OR y . We tried to account for\nthis into account by including multiple samples\nwith this construct in the few shot examples (see\nSec. D.4). However, the models still handle this\nconstruct inconsistently and incorrectly.\nIn addition, we find that throughout the FOLIO\ndataset, by matching the natural language premises\nto the FOL premises, we find no consistent or pre-\ndictable pattern as to how “either x or y” state-\nments are translated. For example, “an animal is\neither a rabbi or a squirrel” is translated as all\nx. Rabbit(x) | Squirrel(x), while we believe\nthis instance should clearly be XOR. Therefore, we\nbelieve that some of these samples are inherently\nambiguous or malformed.\nTo highlight model behavior on these exam-\nples, four representative examples from the FO-\nLIO validation set are shown below; examples\nhave multiple translations because we used tem-\nperature T = 0.8. Here, Correct/Incorrect indicate\nwhether the translations match the ground truth\n(which doesn’t necessarily match how we would\ntranslate it).\nPremise: an animal is either a rabbit or a\n↪→ squirrel\n(Ground Truth) Translation: all x. (Rabbit(x) |\n↪→ Squirrel(x))\n(Correct) Translation 1 (GPT-3.5): all x. (\n↪→ Animal(x) -> (Rabbit(x) | Squirrel(x)))\n(Incorrect) Translation 2 (StarCoderPlus): ((\n↪→ Rabbit(Animal) & -Squirrel(Animal)) | (-\n↪→ Rabbit(Animal) & Squirrel(Animal)))\n(Incorrect) Translation 3 (GPT-4): all x. ((\n↪→ Animal(x) & Rabbit(x)) | (Animal(x) &\n↪→ Squirrel(x)))\nPremise: a person either studys or teaches\n(Ground Truth) Translation: all x. (Study(x) |\n↪→ Teaches(x))\n(Incorrect) Translation 1 (StarCoderPlus):\n↪→ Studys(Person) | Teaches(Person)\n(Incorrect) Translation 2 (StarCoderPlus): ((\n↪→ Study(Person) & -Teach(Person)) | (-Study\n↪→ (Person) & Teach(Person)))\n(Correct) Translation 3 (GPT-4): all x. (Studies\n↪→ (x) | Teaches(x))\nPremise: A man is either kind or evil.\n(Ground Truth) Translation: all x. (Kind(x) & -\n↪→ Evil(x)) | (-Kind(x) & Evil(x))\n(Incorrect) Translation 1 (GPT-3.5): ((Man(x) & -\n↪→ Kind(x)) | (-Man(x) & Evil(x)))\n(Incorrect) Translation 2 (StarCoderPlus): Kind(\n↪→ AMan) | Evil(AMan)\n(Incorrect) Translation 3 (StarCoderPlus): (Kind\n↪→ (x) | Evil(x))\nPremise: Ben is either from The Simpsons or\n↪→ funny.\n(Ground Truth) Translation: (Simpsons(Ben) & -\n↪→ Funny(Ben)) | (-Simpsons(Ben) & Funny(Ben\n↪→ ))\n(Correct) Translation 1 (StarCoderPlus): ((\n↪→ Simpsons(Ben) & -Funny(Ben)) | (-Simpsons\n↪→ (Ben) & Funny(Ben)))\n(Incorrect) Translation 2 (GPT-3.5): (\n↪→ FromTheSimpsons(Ben) | Funny(Ben))\n(Incorrect) Translation 3 (GPT-4):\n↪→ FromTheSimpsons(Ben) | Funny(Ben)\nE.2 GPT-4 LINC Failure Modes\nL1: FOL fails to capture implicit information\nnot mentioned in the premises. Three examples\nof errors from the FOLIO validation set are shown\nbelow. The first two occurring in both GPT-3.5 and\nGPT-4, and the latter only occurs in GPT-3.5 and\ninterestingly, is correct in GPT-4. In Example 1,\nto make the correct conclusion, we must encode in\nFOL that Harry is a person (Person(Harry)) and\nthat Walden is a book ( Book(\"Walden\")). Harry\nbeing a person is implicit, but “Walden” being a\nbook is explicitly mentioned in premise 4 but fails\nto be explicitly encoded by the model. In Exam-\nple 2, we must encode that KiKi is an animal to\nmake the correct deduction. One can argue that\nthis example is ambiguous, but from the context,\nmost would make this inference. In Example 3, we\nneed a clause that says LGA and LGA are the same\nairport (SameAirport(LGA, LGA)).\nExample 1 (GPT-4)\n5170\nPremises:\nBooks contain tons of knowledge.\nWhen a person reads a book, that person gains\n↪→ knowledge.\nIf a person gains knowledge, they become smarter.\n↪→\nHarry read the book \"Walden\" by Henry Thoreau.\nConclusion (Ground Truth: True)\nHarry is smarter than before.\nFOLs:\nall x. (Book(x) -> Contains(x, Knowledge))\nall x. all y. (Person(x) & Reads(x, y) & Book(y)\n↪→ -> Gains(x, Knowledge))\nall x. (Gains(x, Knowledge) -> Smarter(x))\nReads(Harry, Walden)\nConclusion (Prover9 Deduction: Uncertain)\nSmarter(Harry)\nExample 2 (GPT-4)\nPremises:\nIf an animal barks, then Jane does not like it.\nAll dogs are animals that bark.\nIf an animal jumps, then Jane likes it.\nIf an animal has legs, then it jumps.\nIf an animal is terricolous, then it has legs.\nIf KiKi neither jumps nor has legs, then KiKi is\n↪→ terricolous or has legs.\nConclusion (Ground Truth: True)\nKiKi neither barks nor is a dog.\nFOLs:\nall x. (Animal(x) & Barks(x) -> NotLike(Jane, x)\n↪→ )\nall x. (Dog(x) -> Animal(x) & Barks(x))\nall x. (Animal(x) & Jumps(x) -> Like(Jane, x))\nall x. (Animal(x) & HasLegs(x) -> Jumps(x))\nall x. (Animal(x) & Terricolous(x) -> HasLegs(x)\n↪→ )\n(-Jumps(KiKi) & -HasLegs(KiKi)) -> (Terricolous(\n↪→ KiKi) | HasLegs(KiKi))\nConclusion (Prover9 Deduction: Uncertain)\n-Barks(KiKi) & -Dog(KiKi)\nExample 3 (GPT-3.5):\nPremises:\nSusan flies to LGA airport.\nThe departure and arrival can not be the same\n↪→ airport.\nJohn flies from LGA airport.\nConclusion (Ground Truth: False)\nSusan flies from LGA airport.\nFOLs:\nFlies(Susan, LGA)\nall x. all y. (Departure(x) & Arrival(y) & -\n↪→ SameAirport(x, y))\nFlies(John, LGA)\nConclusion (Prover9 Deduction: Uncertain):\nFlies(Susan, LGA)\nL2: FOL fails to capture information ex-\nplicitly mentioned in the premises due to the\nchoice of representation. We show four illus-\ntrative examples: the first two examples occur\nin both GPT-3.5 and GPT-4, while the last two\nexamples only occur in GPT-3.5 and are correct\nwhen using GPT-4. The first example is described\nin the main text. In example 2, we know that\nBarutin Cove is a cove, but this lost information\nis not encoded in the FOL, so we cannot conclude\nCove(BarutinCove), although this is crucial in-\nformation for making the deduction. In Example 3,\nwe must deduce that Tom’s license plate is a license\nplate (FromIstanbul(TomPlate)), which is obvi-\nous but is not encoded by the model. In the final\nexample, the idea that an object has four sides is ex-\npressed both asFourSided(x) and FourSides(x),\nso the model is unable to make the correct deduc-\ntion.\nExample 1 (GPT-4)\nPremises:\nHeinrich Scmidt was a Nazi German politician.\nHeeinrich Scmidt was also a member of the\n↪→ Prussian State Parliament and the Nazi\n↪→ Reichstag.\nConclusion (Ground Truth: True):\nHeinrich Schmidt was either German or Russian.\nFOLs:\nGermanPolitician(HeinrichSchmidt) & Nazi(\n↪→ HeinrichSchmidt)\nMemberPrussianStateParliament(HeinrichSchmidt) &\n↪→ MemberNaziReichstag(HeinrichSchmidt)\nConclusion (Prover9 Deduction: Uncertain):\nGerman(HeinrichSchmidt) | Russian(\n↪→ HeinrichSchmidt)\nExample 2 (GPT-4)\nPremises:\nBarutin Cove is named after the Bulgarian\n↪→ settlement of Barutin.\nBarutin Cove is on the southwest coast of Snow\n↪→ Island.\nThe South Shetland Islands include Snow Island,\n↪→ Greenwich Island, and Deception Island.\nThe South Shetland Islands are considered part\n↪→ of Antarctica.\nIf A is located in B and B is located in C, then\n↪→ A is located in C.\nConclusion (Ground Truth: True)\nThere is at least one cove in Antarctica named\n↪→ after a place in Bulgaria.\nFOLs:\nNamedAfter(BarutinCove, Barutin) & In(Barutin,\n↪→ Bulgaria)\nOn(BarutinCove, SnowIsland)\nIn(SnowIsland, SouthShetlandIslands) & In(\n↪→ GreenwichIsland, SouthShetlandIslands) &\n↪→ In(DeceptionIsland, SouthShetlandIslands)\nIn(SouthShetlandIslands, Antarctica)\nall x. all y. all z. ((In(x, y) & In(y, z)) ->\n5171\n↪→ In(x, z))\nConclusion (Prover9 Deduction: Uncertain)\nexists x. (Cove(x) & In(x, Antarctica) & exists\n↪→ y. (In(y, Bulgaria) & NamedAfter(x, y)))\nExample 3 (GPT-3.5)\nPremises:\nAll vehicle registration plates in Istanbul\n↪→ begin with the number 34.\nPlates that do not begin with the number 34 are\n↪→ not from Istanbul.\nJoe's vehicle registration plate is from\n↪→ Istanbul.\nTom's license plate begins with the number 35.\nIf a license plate begins with the number 35,\n↪→ then it does not begin with the number\n↪→ 34.\nConclusion (Ground Truth: False):\nTom's license plate is from Istanbul.\nFOLs:\nall x. (VehicleRegistrationPlate(x) ->\n↪→ BeginsWith(x, 34))\nall x. (-BeginsWith(x, 34) -> -FromIstanbul(x))\nFromIstanbul(JoePlate)\nBeginsWith(TomPlate, 35)\nall x. (LicensePlate(x) & BeginsWith(x, 35) -> -\n↪→ BeginsWith(x, 34))\nConclusion (Prover9 Deduction: Uncertain):\nFromIstanbul(TomPlate)\nExample 4 (GPT-3.5)\nPremises:\nAll squares have four sides.\nAll four-sided things are shapes.\nConclusion (Ground Truth: True):\nAll squares are shapes.\nFOLs:\nall x. (Square(x) -> FourSides(x))\nall x. (FourSided(x) -> Shape(x))\nConclusion (Prover9 Deduction: Uncertain)\nall x. (Square(x) -> Shape(x))\nL3: FOL contains syntax errors. This is one\nreason that we observe majority voting helps sig-\nnificantly for neurosymbolic methods, as multiple\ngenerations increase the likelihood that at one least\none FOL representation successfully compiles. As\nstated in the main text, the most common error\nis that the same symbol is used with multiple ari-\nties. Two examples of this are shown below: in the\nfirst example, the primitive Summer is both used as\na function representing that a year has a summer\n(in the first premise) and as a constant represent-\ning that a student loves summer. A similar phe-\nnomenon occurs in the second example with the\nsymbol Badults.\nExample 1\nPremises:\nThere are four seasons in a year: Spring, Summer,\n↪→ Fall, and Winter.\nAll students who want to have a long vacation\n↪→ love summer the most.\nFOLs:\nall x. (Year(x) -> (Spring(x) | Summer(x) | Fall\n↪→ (x) | Winter(x)))\nall x. (Student(x) & WantsLongVacation(x) ->\n↪→ Loves(x, Summer))\nProver9 Error:\nThe following symbols are used with multiple\n↪→ arities: Summer/1, Summer/0.\nExample 2\nPremises:\nBadults is a British Sitcom series, starring\n↪→ members of Pappy's.\nBadults piloted in July 2013 on BBC Three.\nFOLs:\nexists x. (Badults(x) & BritishSitcom(x) &\n↪→ StarringMembersOf(x, Pappys))\nPiloted(Badults, July2013, BBCThree)\nProver9 Error:\nThe following symbols are used with multiple\n↪→ arities: Badults/1, Badults/0.\nWhile rarer, there are also syntax errors. For\nillustration, two examples from GPT-4 are shown\nbelow:\nPremise: Ailton Silva foes not play for a\n↪→ football club.\nFOL: -Exists x. (FootballClub(x) & PlaysFor(\n↪→ AiltonSilva, x))\nError: Exists should not be capitalized\nPremise: The security deposit can be either\n↪→ equal to one month's rent or more.\nFOL: all x. all y. (Rent(x, y) -> (Deposit(x) =\n↪→ y | Deposit(x) > y))\nError: > is not allowed in Prover9's FOL\nL4: There are mistakes in the FOL trans-\nlation. For GPT-4, there are very few transla-\ntion mistakes, but we still find a couple instances\nwhere GPT-3.5 fails to capture the semantic mean-\ning of the NL examples properly. Two examples\nare shown here:\nPremise: Platypus have no teeth\nIncorrect FOL: -NoTeeth(Platypus)\nPremise: Imperium doesn't have a feud with a\n↪→ professional wrestling stable that\n↪→ includes Ivy Nile.\nIncorrect FOL: -all x. (Includes(x, IvyNile) -> -\n↪→ HasFeud(Imperium, x))\nE.3 GPT-4 CoT Failure Modes\nC1: CoT concludes something different than it\nsuggests. We show two examples of this below\n5172\n(both of which LINC solves correctly).\nExample 1\nPremises:\nPlungers suck.\nVacuums suck.\nVampires suck.\nSpace is a vacuum.\nA duster is a household appliance that doesn't\n↪→ suck.\nConclusion:\nSpace is a vampire.\n(GPT-4 Incorrect CoT): Let's think step by step.\n↪→ We want to evaluate if Space is a\n↪→ vampire. We know that Space is a vacuum\n↪→ and that vacuums suck. However, while it'\n↪→ s true that vampires also suck, this\n↪→ doesn't mean that everything that sucks\n↪→ is a vampire. Therefore, there is no\n↪→ reason to believe that Space is a vampire\n↪→ based on these premises, so the\n↪→ statement is false.\nANSWER: False\nExample 2\nPremises:\nSome mammals have teeth.\nPlatypus have no teeth.\nPlatypus are mammals.\nHumans have teeth.\nConclusion:\nPlatypus are reptiles.\n(GPT-4 Incorrect CoT): Let's think step by step.\n↪→ We want to evaluate if platypus are\n↪→ reptiles. We know from the premises that\n↪→ platypus are mammals and that they have\n↪→ no teeth. The fact that some mammals have\n↪→ teeth does not contradict the fact that\n↪→ platypus, although mammals, do not have\n↪→ teeth. Moreover, the statement about\n↪→ humans having teeth is unrelated to the\n↪→ classification of platypus. There's no\n↪→ evidence in the premises that would\n↪→ suggest platypus are reptiles. Therefore,\n↪→ the statement is false.\nC2: CoT makes incorrect logical deductions.\nExample 1\nPremises:\nInternational students in US have either an F1\n↪→ visa or a J1 visa.\nAn international student in US with an F1 visa\n↪→ needs to apply for CPT or OPT if the\n↪→ student wants to work in the US.\nMike is an international student.\nMike needs to apply for CPT if he wants to work\n↪→ in the US.\nConclusion (Ground Truth: Uncertain):\nMike has an F1 visa.\n(GPT-4 Incorrect CoT): Let's think step by step.\n↪→ We want to evaluate if Mike has an F1\n↪→ visa. From the premises, we know that\n↪→ Mike is an international student and\n↪→ needs to apply for CPT if he wants to\n↪→ work in the US. We also know that an\n↪→ international student in the US with an\n↪→ F1 visa needs to apply for CPT or OPT if\n↪→ the student wants to work in the US. So,\n↪→ it is clear that Mike has an F1 visa\n↪→ since he fits the criteria. Therefore,\n↪→ the conclusion is true.\nExample 2\nPremises:\nIf people perform in school talent shows often,\n↪→ then they attend and are very engaged\n↪→ with school events.\nPeople either perform in school talent shows\n↪→ often or are inactive and disinterested\n↪→ members of their community.\nIf people chaperone high school dances, then\n↪→ they are not students who attend the\n↪→ school.\nAll people who are inactive and disinterested\n↪→ members of their community chaperone high\n↪→ school dances.\nAll young children and teenagers who wish to\n↪→ further their academic careers and\n↪→ educational opportunities are students\n↪→ who attend the school.\nBonnie either both attends and is very engaged\n↪→ with school events and is a student who\n↪→ attends the school, or she neither\n↪→ attends and is very engaged with school\n↪→ events nor is a student who attends the\n↪→ school.\nConclusion (Ground Truth: Uncertain):\nBonnie performs in school talent shows often.\n(GPT-4 Incorrect CoT): Let's think step by step.\n↪→ We want to evaluate if Bonnie performs\n↪→ in school talent shows often. Based on\n↪→ the last premise, if Bonnie is a student\n↪→ who attends the school and is very\n↪→ engaged with school events, this is only\n↪→ possible if she performs in school talent\n↪→ shows often, as per the first premise.\n↪→ If she doesn't attend and is not engaged\n↪→ with school events, she is an inactive\n↪→ and disinterested member of her community\n↪→ . But the fourth premise states that\n↪→ these individuals chaperone high school\n↪→ dances, meaning they cannot be students,\n↪→ which would contradict the last premise.\n↪→ Therefore, Bonnie performs in school\n↪→ talent shows often, so the statement is\n↪→ true.\nC3: CoT fails to find complex paths of reason-\ning. We highlight two examples below: in the first\nexample, the ground truth is false. To make this\ndeduction, one must reason that if a Greyhound is\na Boeing 707, then it is a plane, which means it is\nempty, which means it cannot transport multiple\npassengers, which means it is not an airline, which\nmeans there are no Greyhound planes, which is a\ncontradiction. Looking at the CoT generations, the\nfirst CoT attempt gives up after failing to find any\n5173\nlink between Greyhound and Boeing 707. The sec-\nond generation attempts to make deductions from\nthe premises. In this case, none of the 10 CoT rea-\nsoning chains begin with the correct step of starting\nfrom the negation of the conclusion and deducing\nit false.\nIn the second example, to make the correct de-\nduction, we need to start from the fact that Rose is\nyoung or a student. If Rose is young, then they do\nnot teach, which means they study, which means\nthey are a student, which means they are a human.\nIf Rose is a student, then they are humans. Neither\nof the CoT generation is able to make progress on\nthe deduction from the information that Rose is\nyoung. In addition, the first CoT generation also\nhas a logical error at the last step, where it asserts\nthat “A or False” is False when the truthness of A\nis uncertain.\nExample 1\nPremises:\nIf something can transport multiple passengers,\n↪→ then they are not empty.\nAll airlines can transport multiple passengers.\nAll planes are empty.\nAll Boeing 707s are planes.\nIf Greyhound is not an airline, then there are\n↪→ no Greyhound planes.\nConclusion (Ground Truth: False)\nA Greyhound is a Boeing 707.\n(GPT-4 Incorrect CoT, 1): Let's think step by\n↪→ step. We want to evaluate if a Greyhound\n↪→ is a Boeing 707. Looking at our premises,\n↪→ we have no direct or indirect\n↪→ information linking Greyhounds to Boeing\n↪→ 707s. Given the information available, we\n↪→ cannot make any certain conclusion about\n↪→ a Greyhound being a Boeing 707.\n↪→ Therefore, it is uncertain whether a\n↪→ Greyhound is a Boeing 707.\n(GPT-4 Incorrect CoT, 2): Let's think step by\n↪→ step. We want to evaluate if a Greyhound\n↪→ is a Boeing 707. The first three premises\n↪→ don't provide any information about\n↪→ Greyhound or Boeing 707s. The fourth\n↪→ premise tells us that all Boeing 707s are\n↪→ planes but doesn't tell us anything\n↪→ about Greyhound. The fifth premise tells\n↪→ us that if Greyhound is not an airline,\n↪→ then there are no Greyhound planes, but\n↪→ this still doesn't imply that a Greyhound\n↪→ is a Boeing 707. Therefore, we do not\n↪→ have enough information to conclude\n↪→ whether a Greyhound is a Boeing 707 or\n↪→ not. So, the statement is uncertain.\nExample 2\nPremises:\nAll students are humans.\nIf someone studies, then they are a student.\nA person either studies or teaches.\nNo young person teaches.\nJerry neither teaches nor is a manager.\nRose is young or a student.\nConclusion (Ground Truth: True)\nRose is a human or Jerry is a manager.\n(GPT-4 Incorrect CoT, 1): Let's think step by\n↪→ step. We want to evaluate if Rose is a\n↪→ human or Jerry is a manager. Let's first\n↪→ look at Rose. From premise 6, we know\n↪→ that Rose is either young or a student.\n↪→ If Rose is a student, then from premise 1\n↪→ we know that Rose is a human. If Rose is\n↪→ young, we don't know from the premises\n↪→ whether she is a human. Therefore, it is\n↪→ uncertain whether Rose is a human. Now\n↪→ let's look at Jerry. From premise 5, we\n↪→ know that Jerry is not a manager.\n↪→ Therefore, Jerry is not a manager. So, it\n↪→ is uncertain whether Rose is a human,\n↪→ but we know that Jerry is not a manager,\n↪→ so the statement is false.\n(GPT-4 Incorrect CoT, 2): Let's think step by\n↪→ step. We want to evaluate if Rose is a\n↪→ human or Jerry is a manager. First, we\n↪→ know that Jerry neither teaches nor is a\n↪→ manager, which means Jerry is not a\n↪→ manager. Therefore, the only way for the\n↪→ conclusion to be true is if Rose is a\n↪→ human. We know that Rose is either young\n↪→ or a student. If she's a student, since\n↪→ all students are humans, then she is a\n↪→ human. However, if Rose is young, we don'\n↪→ t have enough information to determine\n↪→ whether she's a human. Therefore, it's\n↪→ uncertain whether Rose is a human or\n↪→ Jerry is a manager.\nE.4 Shared mistakes between GPT-4 CoT and\nLINC\nAs shown in Fig. 5c, there are 21 common errors\nbetween GPT-4 CoT and GPT-4 LINC. After an in-\ndepth analysis of the examples, we see that 16 of\nthese arise due to inherent errors in the dataset:\n• 2 of these samples contain the sentence frag-\nment “Either Zaha Hadid’s design style or\nKelly Wearstler’s design style.” as a premise.\nThis premise is likely intended to mean all\ndesign styles are one of these two styles, but\nthis is hard for the model to grasp from just\nthe fragment (sample 41, 42).\n• 2 of these samples contain the sentence frag-\nment “Either female tennis players at Roland\nGarros 2022 or male tennis players at Roland\nGarros 2022.” as a premise (sample 43, 45).\n• 2 of these samples have an ambiguous use of\n“either”: “Ben is either from The Simpsons or\n5174\nfunny,” which in this case ambiguously means\nXOR. We believe this sentence is an unnatural\nusage of “either” (sample 142, 143).\n• In 4 samples, there is a name that is implicitly\nan animal, but this is not clear (sample 126,\n127, 128, 199). Also, in samples 126-128,\nthere is a statement “If Rock is neither a fly nor\na bird, then Rock neither flies nor breathes.”\nthat should likely say “If Rock neither flies nor\nis a bird, ...”. With the original formulation,\neverything could be uncertain because nothing\nis known in the case that Rock is a fly (and\nthis is a reasonable interpretation).\n• In 5 samples, the ground-truth FOL represen-\ntation of the natural language premise is incor-\nrect, causing the label to be incorrect (samples\n29, 79, 85, 86, 87).\n• There is a sample where the conclusion is\nlikely mis-worded and should be “Barutin\nCove is not located in Antarctica.” instead\nof “Barutin is not located in Antarctica.” This\nchanges the ground truth label. (sample 121).\nThe remaining 5 samples are examples where\nboth methods fail. In all cases, CoT fails to find the\ncorrect reasoning chain, as the premises/reasoning\npath is convoluted and complex. Meanwhile, LINC\nfails as follows:\n• In sample 104, LINC generates separate sym-\nbols for NotLiveAtHome and -LiveAtHome,\nwhich are semantically equivalent.\n• In sample 149, LINC generates FOLs that miss\nimplicit information; i.e. that people with\nnames Rose and Jerry are humans.\n• In samples 172 and 173, LINC does not gener-\nate a single valid set of FOL premises because\nall generated expressions have a mismatched\narity issue.\n• In sample 193, LINC fails to express the con-\nclusion correctly: the conclusion is “If Yuri\nis not an American basketball player, then\nYuri is a basketball player.”, but the FOLs\noutputted are either -American(Yuri)\n& -BasketballPlayer(Yuri)\nor -American(Yuri) &\nBasketballPlayer(Yuri)\nF Proofwriter StarCoder+ Errors\nWe also analyzed, qualitatively, the errors that Star-\nCoder+ makes on the ProofWriter dataset. Since\npremises from the ProofWriter dataset are relatively\nsimple and direct, lossy translation is rarer. Most er-\nrors are due to syntax or improper representations.\nWe show five representative errors below:\nPremise: All quiet people are round.\nStarCoder+ FOL: all x. Quiet(x) -> Round(x)\nGround Truth FOL: all x. (Quiet(x) -> Round(x))\nPremise: If someone is quiet and nice then they\n↪→ are cold.\nStarcoder+ FOL: all x. (Quiet(x) \\& Nice(x)) ->\n↪→ Cold(x)\nGround Truth FOL: all x. ((Quiet(x) \\& Nice(x))\n↪→ -> Cold(x))\nPremise: If something is furry then it is quiet.\nStarcoder+ FOL: Furry(x) -> Quiet(x)\nGround Truth FOL: all x. (Furry(x) -> Quiet(x))\nPremise: The cow eats the dog.\nStarcoder+ FOL: Cow(Eats(Dog))\nGround Truth FOL: Eats(Cow, Dog)\nPremise: The cow is blue.\nStarcoder+ FOL: Cow(x) \\& Blue(x)\nGround Truth FOL: Blue(Cow)\nG StarCoderPlus FOLIO Error Analysis\nFor StarCoder+, we see a slightly different trend.\nIn Fig. 6a, we see the same pattern as for GPT-4,\nwith a majority of uncertain predictions. In Fig. 6b,\nhowever, we see that CoT for StarCoder+ primarily\npredicts true. This is likely because the model was\ntrained on much more code than text, and may\nnot have picked up sophisticated textual chain-of-\nthought reasoning capabilities. In Fig. 6c, we\ncan see that the mispredictions between CoT and\nLINC differ much more for StarCoder+ than GPT-\n4. Finally, in Fig. 6d, we see the same trends\nas we saw with GPT-4 but more pronounced, as\nthe similarity between mispredictions in LINC and\nthose in the baseline methods is even lower than\nthey were for GPT-4.\nH The effect of K-way majority voting on\nLINC and CoT\nIn all our experiments, we use 10-way majority\nvoting, inspired by prior work which found that\nChain-of-Thought prompting benefited therefrom\n(Wang et al., 2023b). However, one might won-\nder how robust the performance gains seen with\nLINC are to the precise value of K. Figure 7 thus\n5175\nTrue False\nUncertain\nError\nReference Label\nTrue\nFalse\nUncertain\nError\nPredicted Label\n31 7 6 0\n1 16 0 0\n25 29 55 0\n6 2 4 0\n0\n20\n40\n60\n80\n100\n(a) Confusion matrix for LINC.\nTrue False\nUncertain\nError\nReference Label\nTrue\nFalse\nUncertain\nError\nPredicted Label\n61 38 55 0\n1 12 7 0\n1 4 3 0\n0 0 0 0\n0\n20\n40\n60\n80\n100 (b) Confusion matrix for Chain-of-Thought.\nCorrect Incorrect\nLINC Outcome\nCorrect\nIncorrectCoT Outcome\n37 39\n65 41\n0\n20\n40\n60\n80\n100\n(c) Comparing the consistency of LINC vs\nChain-of-Thought.\nLINC CoT\nScratchpad\nNaive\nLINC\nCoT\nScratchpad\nNaive\n1\n0.11 1\n0.1 0.62 1\n0.21 0.44 0.48 1\n 0.2\n0.4\n0.6\n0.8\n1.0\n(d) Similarity between incorrect predictions of each method,\ni.e., (A wrong == B wrong) / (A wrong or B wrong).\nFigure 6: Analyzing and comparing the mistakes made by StarCoder+ on the FOLIO dataset.\nshows, for each model equipped with either LINC\nor Chain-of-Thought, how the accuracy on FO-\nLIO varies with K ∈{1, 2, 3, . . . ,10}. We note\nthat, generally speaking, LINC makes good use of\nincreased values of K. This is especially true for\nthe weaker models; these are more prone to gener-\nating syntactically invalid FOL expressions, which\ncause the solver to return an Error. Taking the ma-\njority vote over many samples thus lessens the risk\nof predicting Error, which is of course always the\nwrong label. Notably, our results do not indicate\nthat CoT benefits from majority voting in this do-\nmain. Future work is needed to establish how this\nrelates to the findings in the previously mentioned\nprior work.\n1 2 3 4 5 6 7 8 9 10\nK-way majority vote\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy on held-out test set\nStarCoder+-Chain-of-Thought\nStarCoder+-LINC (ours)\nGPT-3.5-Chain-of-Thought\nGPT-3.5-LINC (ours)\nGPT-4-Chain-of-Thought\nGPT-4-LINC (ours)\nChance\nFigure 7: Accuracy on FOLIO per value of K (Ap-\npendix H). Shaded areas are ±1 standard deviation over\n1000 bootstrapped samples. Note that increasing K\nbenefits LINC (solid lines; shading in color) but not CoT\n(dashed lines; shading in gray) in our experiments on\nthis dataset.\n5176",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7483091354370117
    },
    {
      "name": "Automated reasoning",
      "score": 0.589801013469696
    },
    {
      "name": "Programming language",
      "score": 0.515561580657959
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5028057098388672
    },
    {
      "name": "Inference",
      "score": 0.49238693714141846
    },
    {
      "name": "Automated theorem proving",
      "score": 0.4693893790245056
    },
    {
      "name": "Natural language",
      "score": 0.43824467062950134
    },
    {
      "name": "Natural language understanding",
      "score": 0.42413267493247986
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4184815287590027
    },
    {
      "name": "Natural language processing",
      "score": 0.38760295510292053
    },
    {
      "name": "Theoretical computer science",
      "score": 0.34144502878189087
    }
  ]
}