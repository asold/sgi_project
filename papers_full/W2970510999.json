{
  "title": "Subword Language Model for Query Auto-Completion",
  "url": "https://openalex.org/W2970510999",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2114721219",
      "name": "Gyuwan Kim",
      "affiliations": [
        "Naver (South Korea)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2796612419",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2964325845",
    "https://openalex.org/W2963205761",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1982858363",
    "https://openalex.org/W2531207078",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2115584760",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4300427683",
    "https://openalex.org/W2963077280",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2963324947",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2300605907",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W2952125979",
    "https://openalex.org/W2885588803",
    "https://openalex.org/W2963582782",
    "https://openalex.org/W2745673470",
    "https://openalex.org/W2530486890",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2952487930",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2895976713",
    "https://openalex.org/W2787178450",
    "https://openalex.org/W2963022149",
    "https://openalex.org/W2779809129",
    "https://openalex.org/W2964046515",
    "https://openalex.org/W2325237720",
    "https://openalex.org/W2963251942",
    "https://openalex.org/W2798523458",
    "https://openalex.org/W4289302788",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2099649694",
    "https://openalex.org/W2962791799",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2592647456",
    "https://openalex.org/W4300069726",
    "https://openalex.org/W2795247881",
    "https://openalex.org/W2963609889",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2417736714",
    "https://openalex.org/W2164986850",
    "https://openalex.org/W1989234058",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W4297797397",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W2311921240",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2337480916",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2522306626",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963946353",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2952288254",
    "https://openalex.org/W1993378086",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2740195281"
  ],
  "abstract": "Current neural query auto-completion (QAC) systems rely on character-level language models, but they slow down when queries are long. We present how to utilize subword language models for the fast and accurate generation of query completion candidates. Representing queries with subwords shorten a decoding length significantly. To deal with issues coming from introducing subword language model, we develop a retrace algorithm and a reranking method by approximate marginalization. As a result, our model achieves up to 2.5 times faster while maintaining a similar quality of generated results compared to the character-level baseline. Also, we propose a new evaluation metric, mean recoverable length (MRL), measuring how many upcoming characters the model could complete correctly. It provides more explicit meaning and eliminates the need for prefix length sampling for existing rank-based metrics. Moreover, we performed a comprehensive analysis with ablation study to figure out the importance of each component.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 5022–5032,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n5022\nSubword Language Model for Query Auto-Completion\nGyuwan Kim\nClova AI, NA VER Corp.\ngyuwan.kim@navercorp.com\nAbstract\nCurrent neural query auto-completion (QAC)\nsystems rely on character-level language mod-\nels, but they slow down when queries are long.\nWe present how to utilize subword language\nmodels for the fast and accurate generation\nof query completion candidates. Represent-\ning queries with subwords shorten a decod-\ning length signiﬁcantly. To deal with issues\ncoming from introducing subword language\nmodel, we develop a retrace algorithm and a\nreranking method by approximate marginal-\nization. As a result, our model achieves up\nto 2.5 times faster while maintaining a sim-\nilar quality of generated results compared to\nthe character-level baseline. Also, we pro-\npose a new evaluation metric, mean recover-\nable length (MRL), measuring how many up-\ncoming characters the model could complete\ncorrectly. It provides more explicit meaning\nand eliminates the need for preﬁx length sam-\npling for existing rank-based metrics. More-\nover, we performed a comprehensive analysis\nwith ablation study to ﬁgure out the impor-\ntance of each component1.\n1 Introduction\nQuery auto-completion (QAC) is one of the es-\nsential features for search engines. When a user\ntypes a query in the search box, QAC systems sug-\ngest most likely completion candidates (Cai et al.,\n2016). It not only saves time for users to enter\nsearch terms but also provides new information\nmore than what was initially expected.\nRecent neural QAC models in the literature em-\nploy character-level language models (Park and\nChiba, 2017). It is a natural choice in that QAC\nsystems need to respond whenever a user enters a\nquery as input character-by-character. In addition\nto the accuracy, speed in terms of latency is also an\n1Code is available at https://github.com/\nclovaai/subword-qac.\nindispensable prerequisite for practical QAC sys-\ntems. The generation process is auto-regressive,\nand the size of the search space is exponential to\nthe sequence length. Long character sequences\nmake prediction slow and inaccurate in the con-\nstraints of limited computation. Also, character-\nlevel models are prone to errors due to long-range\ndependency (Sennrich, 2016). Therefore, these\nlimitations arouse to consider alternatives to rep-\nresent a query in a shorter sequence.\nIn this paper, we apply a subword language\nmodel for query auto-completion. Compared\nto character language models, subword language\nmodels reduce sequence length and the number\nof decoding steps signiﬁcantly, thus resulting in\nmuch faster decoding. For subword-level model-\ning, a segmentation algorithm is necessary. Byte\npair encoding (BPE) (Sennrich et al., 2015) is\nwidely used, but noise in the data makes segmen-\ntation ambiguous and degrades BPE output. To\naddress this issue, as well as BPE, we use subword\nregularization (SR) algorithm proposed by Kudo\n(2018) that stochastically samples multiple seg-\nmentations by utilizing a unigram language model.\nTo our knowledge, we are the ﬁrst to apply SR to\nlanguage modeling.\nInterestingly, language models for QAC should\ntake care of the last token that may be incom-\nplete. Like character language models, subword\nlanguage models can represent incomplete tokens\nbecause it can generate any subsequence of sen-\ntences, whereas word language models cannot. If\nwe segment preﬁx as given to encode it using neu-\nral networks, the segmentation of preﬁx may not\nmatch with that of ground truth query because the\npreﬁx is an incomplete substring of the original\ndesired query. In that case, this enforced segmen-\ntation is less likely to appear in training, especially\nfor deterministic segmentation such as BPE. As a\nresult, the model starting from this segmentation\n5023\nis unlikely to generate ground truth query. To con-\nsider every possible segmentation of target com-\npletion, we propose retrace algorithm that is going\na few characters back from the end and generat-\ning candidates with the restriction that they should\nmatch with retraced characters. For the case of\nSR models, due to the stochasticity of segmenta-\ntion, we should marginalize over all possible seg-\nmentations to calculate the likelihood of a query.\nFor better approximation than just argmax, we\nperform reranking with approximated marginal-\nization using the output of beam search. Experi-\nmental results show that these techniques improve\nthe robustness of the decoding process of the sub-\nword language model to achieve close generation\nquality compared to the character baseline.\nWe propose a novel metric for query auto-\ncompletion evaluation, called mean recoverable\nlength (MRL). This metric remedies shortcom-\nings of common QAC evaluation metrics, mean\nreciprocal rank (MRR) and partial-matching MRR\n(PMRR), which require sampling of a preﬁx\nlength and are favorable to short queries. We con-\nduct comprehensive ablation study and analysis of\nour models on these three metrics.\n2 Related Work\nOne of the successful traditional QAC approaches\nis most popular completion (MPC) (Bar-Yossef\nand Kraus, 2011), which returns the most frequent\ncandidates among all previously observed queries\nthat match the preﬁx. After extracting candi-\ndates, reranking algorithms (e.g., LambdaMART\n(Burges, 2010)) with additional features are used\nto align ﬁnal candidates. These methods can-\nnot generate previously unseen queries by nature.\nContrary to traditional approaches based on infor-\nmation retrieval, neural approaches can generalize\nto unseen preﬁxes.\nChoosing an appropriate granularity level for\ntext segmentation has been long studied over the\nvariety of natural language processing problems.\nIt can be a character, subword, word, phrase, sen-\ntence, and even paragraph. A trade-off between\nthem exists, and the best performing granular-\nity often varies depending on tasks and datasets.\nCharacter models are widely used to address nat-\nural language processing tasks including text clas-\nsiﬁcation (Kim, 2014; Zhang et al., 2015; Con-\nneau et al., 2016), language modeling (Hwang and\nSung, 2017; Al-Rfou et al., 2018), machine trans-\nlation (Chung et al., 2016; Lee et al., 2017), etc.\nCurrently, neural machine translation systems\nwidely use subword segmentation as de facto .\nMikolov et al. (2012) observed that a subword lan-\nguage model is advantageous in that it achieves\nbetter performance compared to character-level\nmodels with zero out-of-vocabulary rate and\nsmaller model size. BERT (Devlin et al., 2018)\nuses a subword as the unit token for their (masked)\nlanguage models.\nWord-level segmentation can easily shorten se-\nquence length compared to character-level. How-\never, word-level models require larger vocabulary\nsize and the number of parameters to learn. Also,\nit causes data sparsity issue. Because the vocab-\nulary of words is usually ﬁxed before training, it\ncannot generate out-of-vocabulary words by itself.\nSearch systems are especially in an open vocab-\nulary setting. For word-level models, it is hard\nto deal with the last incomplete token because it\nmay not be in the vocabulary, unlike character-\nlevel naturally handle it. Even if the vocabulary\ncontains this token, the decoding process may be\nsomewhat different from expected.\nWord-character hybrid models were proposed\nto overcome the out-of-vocabulary problem (Lu-\nong and Manning, 2016; Wu et al., 2016). A\nword-level decoder generates a word sequence,\nand when it generates a special <UNK> token, a\ncharacter-level decoder generates a character se-\nquence on top of it. These two decoders are\nconnected hierarchically. Word models assume\nwhitespace as the boundary of words. In some\nlanguages including Japanese and Chinese, seg-\nmentation of sentences into words is unknown in\nadvance and sometimes vague. Moreover, input\nqueries usually include much noise such as typos,\ngrammatical errors and spacing errors. The prob-\nlems mentioned above hinder word-level process-\ning for QAC.\nPark and Chiba (2017) and Fiorini and Lu\n(2018) incorporate word information by concate-\nnating its embedding with character embedding\nonly at the word boundary and use a special\n<INC> token embedding for non-boundary posi-\ntions. This mechanism is inefﬁcient in that the\nword signal is sparse. Most of the word-character\nhybrid models focus on input representation rather\nthan generation. Usually, their representations\nare concatenated, or composition functions are\nlearned (Kim et al., 2016; Miyamoto and Cho,\n5024\n2016). Even though they use word information to\nthe input, the decoding process of their models is\nstill in the character-level.\nWe can interpret generating a subword which\nis a concatenation of characters as parallel de-\ncoding of characters (Stern et al., 2018). In this\nsense, non-autoregressive neural machine transla-\ntion (Gu et al., 2017; Lee et al., 2018) is related\nto our work. They also aim to improve decoding\nspeed with minimal performance degradation. Our\nmodel and decoding method can be used for non-\nautoregressive NMT in place of a character-level\ndecoder, and in the opposite direction, we can ap-\nply their approaches to QAC vice versa.\n3 Subword Language Model\nLet Σ be the set of all possible characters and V\nbe the vocabulary of tokens. Each token is a char-\nacter or a concatenation of characters, and it is a\nsubword in our case. A language model estimates\nthe probability of a token sequence t where the\nprobability distribution of token ti at each step iis\nconditioned on the previous tokens t<i:\np(t; θ) =\n|t|∏\ni=1\np(ti|t<i; θ)\nwhere ti ∈ V and θ is a set of model parame-\nters. For a token sequence t, we can map it to a\nquery q = concat(t)(= t1 ⊕t2 ⊕···⊕ t|t|) by\nconcatenating itself sequentially. Then, the prob-\nability of a given query q is the sum of the prob-\nability over the set of all possible segmentation t,\nS(q) = {t : concat(t) = q}:\np(q; θ) =\n∑\nt∈S(q)\np(t; θ).\nSimilar to (Chan et al., 2016), segmentation t can\nbe interpretable as a latent decomposition of the\nquery q.\n3.1 Segmentation\nIn character-level language modeling, token vo-\ncabulary V is equal to Σ, and segmentation is per-\nformed by merely splitting every character. We ex-\nclude word-level language modeling which splits\na sentence by whitespace from consideration due\nto its limitations mentioned in Section 2.\nIn the case of subword language modeling, we\nuse two widely used segmentation algorithms: (1)\nbyte pair encoding (BPE) and (2) subword regular-\nization (SR). Formally, a segmentation algorithm\ndeﬁnes a probability distribution over a token se-\nquence t conditioned on given queryq: pseg(t|q).\nThe BPE algorithm is deterministic because it\nsegments greedily from left to right. On the\nother hand, SR can sample multiple segmentations\nstochastically. The number of possible segmenta-\ntions is exponentially large. It is hard to calculate\nthe likelihood of a given sentence using dynamic\nprogramming because even with the same preﬁx,\nhidden states vary upon different previous tok-\nenization. Marginalization over all possible seg-\nmentations of very long sequences is intractable.\nIn sum, we compare character-level and subword-\nlevel (BPE, SR) language modeling.\n3.2 Training\nWe can derive an unbiased gradient estima-\ntor of the log-likelihood of a query by using\nBayes’ theorem and the identity of ∇θf(x; θ) =\nf(x; θ)∇θlog f(x; θ) assuming f(x; θ) ̸= 0 for\nall x(Williams, 1992):\n∇θlog p(q; θ) = E\nt∼p(t|q;θ)\n∇θlog p(t; θ).\nHowever, since sampling t from p(t|q; θ) is\ncomputationally expensive, we heuristically use\npseg(t|q) instead. Regardless of the language\nmodel parameters θ, segmentation model pseg is\nlearned before the language model training and\ncan be used to sample t easily. The better way\nto approximate the distribution p(t|q; θ) will be\nexplored in the future.\nOur training objective becomes equivalent to\nmaximizing the average log-likelihood of the seg-\nmentation of sentences:\nL(θ) = 1\n|Q|\n∑\nq∈Q\nlog p(t; θ)\n= 1\n|Q|\n∑\nq∈Q\n∑\ni\nlog p(ti|t<i; θ),\nwhere Q is the training set of all queries, and t\nis the segmentation of a query q sampled from\npseg(t|q) which depends on the segmentation al-\ngorithm. This objective is equal to the average\nnegative log-likelihood of sentences if and only\nif the segmentation is deterministic. The gradi-\nents of the loss function are computed using the\nback-propagation through time (BPTT) (Rumel-\nhart et al., 1986).\n5025\n4 Decoding\nGiven preﬁx p, let the set of its completions\nbe Q+(p) and the set of their tokenizations be\nS+(p) = {t : concat(t) ∈Q+(p)}. We want\nto ﬁnd the most likely completion ˆq:\nˆq = argmax\nq\nlog p(q|p)\n= argmax\nq∈Q+(p)\nlog p(q)\n= argmax\nq∈Q+(p)\nlog\n∑\nt∈S(q)\np(t) (1)\nhowever, this is obviously intractable to search\nin the inﬁnitely large token sequence space. We\napproximate this by decoding for the best token\nsequence ˆt and then returning its corresponding\nquery ˜q by concatenating its token sequentially:\nˆt = argmax\nt∈S+(p)\nlog p(t)\n˜q = concat(ˆt)\nBasic practice is segmenting p, feeding it into\nlanguage model to encode p, and using it for the\ndecoding. Since ﬁnding ˆt is also intractable, beam\nsearch decoding is used but only results in subop-\ntimal predictions. We will improve this incremen-\ntally with techniques following.\n4.1 Retrace Algorithm\nThere is no guarantee that the end of given pre-\nﬁx matches the tokenization boundary of the com-\npleted query. To address this possibility of the in-\ncompleteness at the end of a preﬁx, we can retrace\na few characters and generate from there. For the\ncase (call it Rr) where the last token that overlaps\nwith the preﬁx ﬁnishes rcharacters before the end\nof the preﬁx, ﬁrst starting from a token sequence\nof p1:|p|−r, we can perform beam search decoding\non the restriction that the next token should cover\nthe remaining part of the preﬁx and the next new\ncharacter. Figure 1 illustrates this algorithm.\nThis process is unnecessary for a character-level\nmodel since every token is a single character. On\nthe other hand, the retrace algorithm is helpful for\nsubword models, especially BPE models which\nhave deterministic segmentation algorithm.\nWe can limit the maximum step of retrace by L\nto only consider Rr where 0 ≤r ≤L because\nof the computational issue. We will denote this\nlimitation as RL. R0 is the usual case without the\nretrace algorithm, and R∞counts every possible\nretrace steps.\nr e s t a u r a n t s\nr e s t a u r a n t s\nr e s t a u r a n t s\nr e s t a u r a n t s\nr e s t a u r a n t s\nGT\nR0\nR1\nR2\nR3\nFigure 1: Illustration of retrace algorithm with the ex-\nample of “restaurants.” The gray area means given pre-\nﬁx (“res”) of the query. The solid line indicates the\nboundary of the segmentation. GT is the segmentation\nof ground truth query. Possible examples of the gener-\nated sequence of tokens belonging to the case Rr are\nvisualized. Blue boxes indicate a ﬁxed segmentation\nwith retrace algorithm at the end of the preﬁx.\n4.2 Reranking Method by Approximate\nMarginalization\nQAC system has to suggest N completion can-\ndidates sorted in order of likelihood rather than\nﬁnding only the best completion candidate. We\ncan extract a set of top B(≥ N) candidates us-\ning beam search with beam size B, namely TB =\n{t1,··· ,tB}in the descending order of likeli-\nhood. In the case of deterministic segmentation,\nqi = concat(ti) are mutually different. i.e.\n|QB|= B for QB = {q1,··· ,qB}. Then, triv-\nially our prediction would be (q1,··· ,qN).\nOn the other hand, in the case of stochastic seg-\nmentation, same query qi,qj(i ̸= j) with differ-\nent token sequence ti,tj may exist. The obvious\nway is merely removing duplicates.\nOn the assumption that log p(t) ≫log p(t′) for\nall t ∈ TB and t′ /∈ TB, Equation (1) implies\nthat marginalization over the ﬁnal beam outputs\ncan provide better approximation:\nˆq ≈argmax\nq∈Q+(p)\nlog\n∑\nt∈TB\np(t)\n= argmax\nq∈QB\nlog\n∑\nt∈TB\np(t)\nIn other words, reranking after summing out the\nprobability of duplicates can give better ordered\nlist of candidates.\n5026\n5 Evaluation Metric\n5.1 MRR and PMRR\nOne of the most standard QAC evaluation metrics\nis the mean reciprocal rank (MRR). The MRR for\nthe QAC system mis calculated with test dataset\nQtest as follows:\nMRR(m) = 1\n|Qtest|\n∑\nq∈Qtest\nRR(q,m(p)),\nwhere p is a preﬁx of a query q and m(p) is the\nranked list of candidate completions of p from m.\nRR is reciprocal rank of q if q is in m(p), other-\nwise 0.\nSince it is hard to get a pair of the desired query\nand its preﬁx in a real situation, we should synthet-\nically select a preﬁx by cutting off a given query\nfor the evaluation. Common practice is to uni-\nformly sample from all possible preﬁxes within\nminimal length constraint in characters (or words)\nHowever, real distribution of the preﬁx length may\ndiffer to the uniform distribution. For example,\nusers tend to engage with QAC at the position\nclose to the boundary of words, and after typ-\ning half of query characters (Mitra et al., 2014).\nDue to the stochastic characteristic of preﬁx sam-\npling processes or their difference among distinct\nQAC systems, evaluation results are inconsistent\neven with the same test dataset. To prevent this\nproblem, a sampling function should be concretely\nspeciﬁed.\nPark and Chiba (2017) introduced a new metric,\npartial-matching MRR (PMRR):\nPMRR(m) = 1\n|Qtest|\n∑\nq∈Qtest\nPRR(q,m(p)),\nwhere partial-matching reciprocal rank PRR is\nthe reciprocal of the index of the ﬁrst candidate\nsuch that the original query is the same as it or\nstarts with it plus whitespace. If there is no such\ncandidate, PRR equals to 0.\nPMRR also requires sampling of the preﬁx\nlength. PMRR values are often omitted in the liter-\nature because of the similar tendency to MRR. In\nother words, PMRR does not give much additional\ninformation about the quality of the generated re-\nsults.\n5.2 Recoverable Length\nTo avoid the synthetic sampling process and length\ndependency, we propose a new evaluation metric\nfor query auto-completion, namely mean recover-\nable length (MRL). We deﬁne recoverable length\nRL as the number of characters right before the\nﬁrst position where candidates do not have the\nquery. When all characters of a query are known,\nwe can readily suggest itself. If we delete chars\nfrom right to left one-by-one, the ground truth\nquery will disappear in the list of candidate com-\npletions. For example, if q ∈ m(q1:|q|−l) for\nl = 1 ,2,3 but not 4, recoverable length of this\nquery with respect to the QAC system m is 3.\nMRL is mean of recoverable length:\nMRL(m) = 1\n|Qtest|\n∑\nq∈Qtest\nRLm(q)\nMRL is a useful metric for additive QAC which\nsuggests one word at a time instead of a whole-\nquery completion (Vargas et al., 2016) in that it\nmeasures how many characters the system can pre-\ndict correctly at once. MRL does not care about\nthe order of candidates and check whether they\ncontain the target query or not. Lastly, it elimi-\nnates the need to choose a preﬁx length in the test\ndata.\n6 Experiments\n6.1 Data\nWe use the public AOL query log dataset (Pass\net al., 2006) for the experiments. We split data\nbased on time. Among three months of the entire\nspan, we use last one week as test data and one\nweek right before the test data as validation data.\nIt is close to a real scenario where future queries\nare unseen during the training.\nWe perform Unicode NFKC normalization and\nremove non-ASCII characters. For simplicity, we\nchange uppercase alphabets to lowercase. Af-\nter normalization and changing to lowercase, only\n43 unique characters including special symbols\n<BOS>, <EOS> and <UNK> remain. We substi-\ntuted multiple adjacent spaces to a single one and\nremoved leading or trailing spaces. We merged\nduplicates which appear adjacently by the same\nuser and the same query. Queries of a length\nshorter than three characters are ﬁltered out.\nIn total, the training, validation, test data\ncontain 17,521,031, 1,521,971, and 1,317,632\nqueries, respectively. Among the test data,\n670,810 queries are seen, and 646,822 queries are\nunseen in the training data. Almost half of the test\ndata are unseen.\n5027\nModel\nMRR PMRR MRL Execution Speed\n(QPS) Decode\nLength\nSeen Unseen All Seen Unseen All Seen Unseen All CPU GPU\nMPC .570 .000 .290 .616 .095 .360 8.06 0.00 4.10 >100 N/A\nChar .458 .160 .311 .552 .372 .464 5.77 4.24 5.02 11.0 (1.0x) 16.5 (1.0x) 14.5\nBPE .242 .085 .164 .305 .232 .269 0.49 0.54 0.51 24.2 (2.2x) 37.4 (2.3x) 7.1\nBPE+R1 .427 .156 .294 .517 .368 .444 5.28 3.98 4.64 15.8 (1.4x) 27.3 (1.7x) 11.8\nBPE+R2 .430 .157 .296 .520 .369 .446 5.44 4.01 4.74 15.5 (1.4x) 27.2 (1.6x) 12.2\nBPE+R∞ .431 .157 .296 .520 .369 .446 5.50 4.01 4.76 15.3 (1.4x) 26.9 (1.6x) 12.2\nSR .422 .148 .288 .541 .379 .461 5.11 3.82 4.48 20.8 (1.9x) 40.1 (2.4x) 6.8\nSR+M .424 .149 .289 .535 .373 .455 5.14 3.85 4.50 19.6 (1.8x) 40.0 (2.4x) 6.8\nSR+R∞ .423 .148 .289 .541 .378 .461 5.14 3.83 4.50 16.3 (1.5x) 29.6 (1.8x) 7.4\nSR+R∞+M .427 .150 .291 .538 .375 .458 5.19 3.88 4.54 16.2 (1.5x) 28.7 (1.7x) 7.4\nTable 1: Results of completion generation. We group MPC, character language model baseline, and two subword\nlanguage models separately. +R implies the retrace algorithm. +M implies reranking with approximate marginal-\nization. QPS stands for query per seconds. The higher the QPS, the better. The best results for each column related\nto accuracy are shown in bold for each segmentation algorithm (BPE and SR). SR model shows higher unseen\nPMRR scores (underlined). Our models are faster than the character baseline.\n6.2 Implementation Details\nThe language model used in the experiments con-\nsists of an input layer, a single LSTM layer, a pro-\njection layer, and an output layer. For the LSTM\nlayer, following Melis et al. (2017) and Jaech and\nOstendorf (2018), we apply layer normalization\n(Ba et al., 2016) to each gate and couple input and\nforget gates. We tie input and output embeddings\nfor better generalization (Press and Wolf, 2016;\nInan et al., 2016). We set the LSTM hidden size to\n600 and the input embedding size to 100.\nWe train three individual language models:\nnamely Char, BPE, SR. The only difference\namong models is how to segment a given sentence\ninto tokens. We believe that increasing model size\n(number of LSTM layers, input size, and hidden\nsize) would improve the performance. Also, the\nbest set of a combination may differ depending on\nmodels. However, we use the same model size for\nthe character baseline and our variants for the fair-\nness since our goal is proposing a new method and\ncomparing between baseline and ours rather than\nachieving the best performance with the restriction\nof having a similar number of parameters.\nWe use the off-the-shelf SentencePiece (Kudo\nand Richardson, 2018) library for vocabulary\nlearning and segmentation of the vocabulary size\n256 using BPE, SR algorithms. For the subword\nregularization, we use sampling parameters l =\n∞, α= 0.2 for the training. We choose this value\nby the generation accuracy on the validation data.\nIncrease of model size and computation due to\nlarger vocabulary size are not substantial. By set-\nting a manageable amount of vocabulary size, we\ncan balance performance and computational cost.\nFor the computational efﬁciency, we truncated\nqueries in the training data to a length of 40. Only\nless than 3% of queries in the training data are\nlonger than 40 characters. We train models for\nthirty epochs by the Adam (Kingma and Ba, 2014)\noptimizer with a learning rate 5e-3 and batch size\n1024. Following Smith (2018), we use a large\nlearning rate and batch size. We use recurrent\ndropout (Semeniuta et al., 2016) with probability\nof 0.25 for regularization. The best model is cho-\nsen using validation data.\nUsing QAC models, we generate N = 10 com-\npletion candidates using beam search decoding of\na beam width B = 30.\nFor the SR models, the segmentation of p (or\nretraced p1:|p|−r) is not deterministic and gener-\nated completions may differ depending on its seg-\nmented token sequences with their different en-\ncoded representation. By following (Kudo, 2018),\nwe can ﬁnd the most likely segmentation sequence\nt starting from all of the n-best segmentations\n˜t1,··· ,˜tnof S(p) rather than from only˜t1. How-\never, we observe that this n-best decoding per-\nforms worse than one-best decoding. One possi-\n5028\nble reason is that segmentations which are not the\nbest have a smaller probability as itself and so less\nlikely to appear in training and less competitive in\nthe process of beam search. For this reason, we set\nnto 1.\nWe used a trie (Fredkin, 1960) data structure to\nimplement most popular completion baseline.\nAll experiments were performed on NA VER\nSmart Machine Learning (NSML) platform (Sung\net al., 2017; Kim et al., 2018).\n6.3 Decoding Results\nWe performed comprehensive experiments to an-\nalyze the performance of query auto-completion.\nTable 1 shows the generation result of MPC, the\ncharacter baseline, and our model variants. For\nBPE models, we varied the maximum retrace step\nto 0 (without retrace algorithm), 1, 2, and ∞(no\nlimitation on retracing step size). For SR models,\nwe compare decoding results without any tech-\nniques, with marginalization only, with retrace al-\ngorithm only, and with both.\nMPC is a very fast and remarkably strong base-\nline. It is worse than language models in the over-\nall score (MRR, PMRR, and MRL), but better\nfor previously seen queries. However, it is un-\nable to predict unseen queries. Even with efﬁ-\ncient data structures, MPC requires huge memory\nto keep statistics of all previous queries. As a prac-\ntical view, combining frequency-based traditional\nmethod and neural language model approach can\nboost the accuracy and meet trade-off between the\nperformance and computational costs.\nMRRs and PMRRs of our best methods are\nclose to that of the character model with less than\n0.02 point drop. Notably, the SR model has bet-\nter generalization ability in that their PMRR for\nunseen queries is higher than that of the character\nmodel. In a real scenario, it is more critical be-\ncause unseen queries come in increasingly as time\ngoes by.\nWe measure execution time with Tesla P40\nGPU and Xeon CPU. Subword-level models are\nup to 2.5 times faster than the character baseline\nwith minimal loss in performance both in CPU and\nGPU. Decoding length which is maximum suf-\nﬁx length until beam search ends correlates with\nthe number of ﬂoating-point operations. Subword\nmodels signiﬁcantly reduce the decoding length\nfrom the character baseline more than two times\nshorter by generating multiple characters at once.\nre nat\nChar SR Char SR\nrealtor.com recipes national city bank national bank\nrecipes rentals nationalcity.com national city\nreal estate real estate national city nationwide\nremax restaurants national geographic national parks\nrealtor resources national car rental national park\nTable 2: Examples of top 5 candidates of completions\ngiven ”re” and ”nat” as preﬁxes generaed by the char-\nacter baseline and SR model.\nModels with additional techniques perform bet-\nter than without them. Especially, retrace al-\ngorithm gives huge improvement for BPE case.\nWithout retrace algorithm, BPE models do not\nwork well. On the other hand, SR models only\nobtain small improvement. Because retrace algo-\nrithm goes back, it increases the decoding length\nand slows down the speed. Although current re-\ntrace algorithm is implemented straightforwardly,\nit can be improved by merging beams efﬁciently.\nMost of the subword lengths are equal or shorter\nthan 3, so retrace of step 2 is quite enough, and R2\nget a close result with R∞.\nThe reranking method by approximate\nmarginalization gives a small amount of im-\nprovement and is orthogonal to retrace algorithm.\nMarginalization method increases MRR but\ndecreases PMRR. It is plausible in that it changes\nthe order of candidates by reranking. The effect of\nmarginalization would be better if we use a bigger\nbeam size. Because the reranking process is done\nafter beam search decoding which takes most of\nthe decoding time and only consists of summation\nand sorting the ﬁnal beam outputs, it does not take\na long time.\nWe also had experimented by increasing the vo-\ncabulary size. The accuracy of BPE models de-\ngrades fast as the vocabulary size increases. On\nthe other hand, the performance of SR models is\nquite stable due to the regularization effect during\ntraining. As desired, the larger the dictionary size,\nthe shorter the decoding length. Whereas compu-\ntations run in parallel in GPU, the number of oper-\nations for the output layer in the language model is\nproportional to the vocabulary size in CPU. There-\nfore, a larger vocabulary size does not always\nguarantee speedup for execution in the CPU. More\nthorough investigation about the correlation be-\ntween QAC performance and the vocabulary size\nof subword language models remains for future\nwork.\n5029\n5 10 15 20 25 30 35 40\nQuery Length\n(in characters)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6MRR\n5 10 15 20 25 30 35 40\nQuery Length\n(in characters)\n0.400\n0.425\n0.450\n0.475\n0.500\n0.525\n0.550\n0.575\n0.600PMRR\n5 10 15 20 25 30 35 40\nPrefix Length\n(in characters)\n0.20\n0.25\n0.30\n0.35\n0.40MRR\n5 10 15 20 25 30 35 40\nPrefix Length\n(in characters)\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60PMRR\nChar BPE + R\n SR + R\n SR + R + M\n5 10 15 20 25 30 35 40\nQuery Length\n(in characters)\n1\n2\n3\n4\n5\n6MRL\nFigure 2: Comparison of the character-level baseline model and our best models by changing query length and\npreﬁx length in terms of three evaluation metrics: MRR, PMRR, and MRL. MRL is only varied by query length\nbecause it does not require preﬁx length sampling.\nTable 2 shows examples of decoding results.\nOur model generates more concise and realistic\nexamples.\n6.4 Analysis on Evaluation Metrics\nAs shown in Figure 2, we compared our best mod-\nels on three evaluation metrics (MRR, PMRR, and\nMRL) by changing the query length and preﬁx\nlength. MRR and PMRR are more favorable to\nshorter queries. They drop very fast as the query\nbecomes longer. For a longer query, the sufﬁx\nlength after sampling preﬁx has more chance to be\nlonger. The search space increases exponentially\nwith its sufﬁx length. Even though QAC systems\ncould generate realistic candidates, it is quite hard\nto match a long sequence with the ground truth.\nAs the preﬁx length becomes longer which means\nthat much information for determining the query\nhas been given, the completion performance im-\nproves.\nInterestingly, MRR and MRL of BPE are higher\nthan those of SR, although BPE is worse in terms\nof PMRR than SR. For short queries, SR outper-\nforms the character baseline. On the other hand,\nBPE is poor when the query length (or preﬁx\nlength) is short. However, for a longer case, its\nMRR is almost close to that of the character base-\nline.\nMRR and PMRR are highly dependent on the\nlength distribution of test data. In contrast, MRL\nkeeps the order between different methods as the\nquery length changes. MRL is more reliable in the\nrespect that it could provide consistent order be-\ntween methods regardless of query length distri-\nbution. For long queries lengths, MRL stays in the\nﬂat area. Normalizing recoverable length based on\nthe query length might be necessary.\n7 Future Work\nApproximation in training (Section 3.2) and de-\ncoding (Section 4) deteriorate the accuracy of sub-\nword language modeling. One possible solution to\nreduce the accuracy gap between the character lan-\nguage model baseline and the subword language\nmodel is knowledge distillation (Hinton et al.,\n2015; Kim and Rush, 2016; Liu et al., 2018) from\ncharacter-level language models. A student model\ncan learn to match an estimation of query proba-\nbility with that of a teacher model.\nAnother interesting research direction is learn-\ning segmentation jointly with language model\n5030\n(Kawakami et al., 2019; Grave et al., 2019) rather\nthan using ﬁxed pretrained segmentation algo-\nrithms. A conditional semi-Markov assumption\nallows exact marginalization using dynamic pro-\ngramming (Ling et al., 2016; Wang et al., 2017).\nNevertheless, beam search decoding on those lan-\nguage models, especially faster decoding, is non-\ntrivial.\nProposed method can be extended to wide range\nof tasks. Query suggestion (Sordoni et al., 2015;\nDehghani et al., 2017) and query reformulation\n(Nogueira and Cho, 2017) are related to QAC and\nwell-established problems. They both are also\npossible applications of the subword-level model-\ning. (Drexler and Glass, 2019) used subword reg-\nularization and beam search decoding for end-to-\nend automatic speech recognition.\nLastly, implementation with more advanced\ndata structure (Hsu and Ottaviano, 2013) and par-\nallel algorithms to speed up and meet memory\nlimitation are necessary for the real deployment\n(Wang et al., 2018). It would be helpful if the\ncomputation is adaptively controllable on-the-ﬂy\n(Graves, 2016) at the runtime depending on the sit-\nuation.\n8 Conclusion\nIn this paper, we propose subword language mod-\nels for query auto-completion with additional\ntechniques, retrace algorithm and reranking with\napproximate marginalization. We observed sub-\nword language models signiﬁcant speedup com-\npared to the character-level baseline while main-\ntaining the generation quality. Our best models\nachieve up to 2.5 times faster decoding speed with\nless than 0.02 point drop of MRR and PMRR.\nUsing a subword language model, we build an\naccurate and much faster QAC system compared\nto the character-level language model baseline.\nAlthough there is still much room for improve-\nment on hyperparameter optimization, decoding\nsearch, and neural architectures like Transformer\n(Vaswani et al., 2017; Dai et al., 2019), the goal\nof this work is to prove that the subword language\nmodel is an attractive choice for QAC as an alter-\nnative to the character-level language model, espe-\ncially if latency is considered.\nWe believe that our newly proposed metric,\nmean recoverable length (MRL), provides fruitful\ninformation for the QAC research in addition to\nconventional evaluation metric based on ranks.\nAcknowledgments\nThe author would like to thank Clova AI mem-\nbers and the anonymous reviewers for their helpful\ncomments.\nReferences\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nZiv Bar-Yossef and Naama Kraus. 2011. Context-\nsensitive query auto-completion. In Proceedings\nof the 20th international conference on World wide\nweb, pages 107–116. ACM.\nChristopher JC Burges. 2010. From ranknet to lamb-\ndarank to lambdamart: An overview. Learning,\n11(23-581):81.\nFei Cai, Maarten De Rijke, et al. 2016. A survey\nof query auto completion in information retrieval.\nFoundations and TrendsR⃝in Information Retrieval,\n10(4):273–363.\nWilliam Chan, Yu Zhang, Quoc Le, and Navdeep Jaitly.\n2016. Latent sequence decompositions. arXiv\npreprint arXiv:1610.03035.\nJunyoung Chung, Kyunghyun Cho, and Yoshua Ben-\ngio. 2016. A character-level decoder without ex-\nplicit segmentation for neural machine translation.\narXiv preprint arXiv:1603.06147.\nAlexis Conneau, Holger Schwenk, Lo ¨ıc Barrault,\nand Yann Lecun. 2016. Very deep convolutional\nnetworks for text classiﬁcation. arXiv preprint\narXiv:1606.01781.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860.\nMostafa Dehghani, Sascha Rothe, Enrique Alfonseca,\nand Pascal Fleury. 2017. Learning to attend, copy,\nand generate for session-based query suggestion.\nIn Proceedings of the 2017 ACM on Conference\non Information and Knowledge Management, pages\n1747–1756. ACM.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\n5031\nJennifer Drexler and James Glass. 2019. Subword reg-\nularization and beam search decoding for end-to-\nend automatic speech recognition. In ICASSP 2019-\n2019 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n6266–6270. IEEE.\nNicolas Fiorini and Zhiyong Lu. 2018. Personalized\nneural language models for real-world query auto\ncompletion. arXiv preprint arXiv:1804.06439.\nEdward Fredkin. 1960. Trie memory. Communica-\ntions of the ACM, 3(9):490–499.\n´Edouard Grave, Sainbayar Sukhbaatar, Piotr Bo-\njanowski, and Armand Joulin. 2019. Training hy-\nbrid language models by marginalizing over seg-\nmentations. In Proceedings of the 57th Conference\nof the Association for Computational Linguistics ,\npages 1477–1482.\nAlex Graves. 2016. Adaptive computation time\nfor recurrent neural networks. arXiv preprint\narXiv:1603.08983.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor OK Li, and Richard Socher. 2017. Non-\nautoregressive neural machine translation. arXiv\npreprint arXiv:1711.02281.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nBo-June Paul Hsu and Giuseppe Ottaviano. 2013.\nSpace-efﬁcient data structures for top-k completion.\nIn Proceedings of the 22nd international conference\non World Wide Web, pages 583–594. ACM.\nKyuyeon Hwang and Wonyong Sung. 2017. Character-\nlevel language modeling with hierarchical recurrent\nneural networks. In Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2017 IEEE International\nConference on, pages 5720–5724. IEEE.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2016. Tying word vectors and word classiﬁers:\nA loss framework for language modeling. arXiv\npreprint arXiv:1611.01462.\nAaron Jaech and Mari Ostendorf. 2018. Personalized\nlanguage model for query auto-completion. arXiv\npreprint arXiv:1804.09661.\nKazuya Kawakami, Chris Dyer, and Phil Blunsom.\n2019. Learning to discover, ground and use words\nwith segmental neural language models. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, pages 6429–6441.\nHanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong\nKim, Heungseok Park, Soeun Park, Hyunwoo Jo,\nKyungHyun Kim, Youngil Yang, Youngkwan Kim,\net al. 2018. Nsml: Meet the mlaas platform\nwith a real-world case study. arXiv preprint\narXiv:1810.09957.\nYoon Kim. 2014. Convolutional neural net-\nworks for sentence classiﬁcation. arXiv preprint\narXiv:1408.5882.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In AAAI, pages 2741–2749.\nYoon Kim and Alexander M Rush. 2016. Sequence-\nlevel knowledge distillation. arXiv preprint\narXiv:1606.07947.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. ACL.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nJason Lee, Kyunghyun Cho, and Thomas Hofmann.\n2017. Fully character-level neural machine trans-\nlation without explicit segmentation. Transactions\nof the Association for Computational Linguistics ,\n5:365–378.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative reﬁnement. arXiv\npreprint arXiv:1802.06901.\nWang Ling, Edward Grefenstette, Karl Moritz Her-\nmann, Tom ´aˇs Ko ˇcisk`y, Andrew Senior, Fumin\nWang, and Phil Blunsom. 2016. Latent predic-\ntor networks for code generation. arXiv preprint\narXiv:1603.06744.\nYijia Liu, Wanxiang Che, Huaipeng Zhao, Bing Qin,\nand Ting Liu. 2018. Distilling knowledge for\nsearch-based structured prediction. arXiv preprint\narXiv:1805.11224.\nMinh-Thang Luong and Christopher D Manning. 2016.\nAchieving open vocabulary neural machine trans-\nlation with hybrid word-character models. arXiv\npreprint arXiv:1604.00788.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2017. On\nthe state of the art of evaluation in neural language\nmodels. arXiv preprint arXiv:1707.05589.\nTom´aˇs Mikolov, Ilya Sutskever, Anoop Deoras, Hai-\nSon Le, Stefan Kombrink, and Jan Cernocky.\n2012. Subword language modeling with neu-\nral networks. preprint (http://www. ﬁt. vutbr.\ncz/imikolov/rnnlm/char. pdf), 8.\nBhaskar Mitra, Milad Shokouhi, Filip Radlinski, and\nKatja Hofmann. 2014. On user interactions with\nquery auto-completion. In Proceedings of the 37th\ninternational ACM SIGIR conference on Research &\ndevelopment in information retrieval , pages 1055–\n1058. ACM.\n5032\nYasumasa Miyamoto and Kyunghyun Cho. 2016.\nGated word-character recurrent language model.\narXiv preprint arXiv:1606.01700.\nRodrigo Nogueira and Kyunghyun Cho. 2017. Task-\noriented query reformulation with reinforcement\nlearning. arXiv preprint arXiv:1704.04572.\nDae Hoon Park and Rikio Chiba. 2017. A neural lan-\nguage model for query auto-completion. In Pro-\nceedings of the 40th International ACM SIGIR Con-\nference on Research and Development in Informa-\ntion Retrieval, pages 1189–1192. ACM.\nGreg Pass, Abdur Chowdhury, and Cayley Torgeson.\n2006. A picture of search. In InfoScale, volume\n152, page 1.\nOﬁr Press and Lior Wolf. 2016. Using the output\nembedding to improve language models. arXiv\npreprint arXiv:1608.05859.\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J\nWilliams. 1986. Learning representations by back-\npropagating errors. nature, 323(6088):533.\nStanislau Semeniuta, Aliaksei Severyn, and Erhardt\nBarth. 2016. Recurrent dropout without memory\nloss. arXiv preprint arXiv:1603.05118.\nRico Sennrich. 2016. How grammatical is character-\nlevel neural machine translation? assessing mt qual-\nity with contrastive translation pairs. arXiv preprint\narXiv:1612.04629.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nLeslie N Smith. 2018. A disciplined approach to neu-\nral network hyper-parameters: Part 1–learning rate,\nbatch size, momentum, and weight decay. arXiv\npreprint arXiv:1803.09820.\nAlessandro Sordoni, Yoshua Bengio, Hossein Vahabi,\nChristina Lioma, Jakob Grue Simonsen, and Jian-\nYun Nie. 2015. A hierarchical recurrent encoder-\ndecoder for generative context-aware query sugges-\ntion. In Proceedings of the 24th ACM International\non Conference on Information and Knowledge Man-\nagement, pages 553–562. ACM.\nMitchell Stern, Noam Shazeer, and Jakob Uszkoreit.\n2018. Blockwise parallel decoding for deep autore-\ngressive models. In Advances in Neural Information\nProcessing Systems, pages 10106–10115.\nNako Sung, Minkyu Kim, Hyunwoo Jo, Youngil Yang,\nJingwoong Kim, Leonard Lausen, Youngkwan Kim,\nGayoung Lee, Donghyun Kwak, Jung-Woo Ha,\net al. 2017. Nsml: A machine learning platform that\nenables you to focus on your models. arXiv preprint\narXiv:1712.05902.\nSa´ul Vargas, Roi Blanco, and Peter Mika. 2016. Term-\nby-term query auto-completion for mobile search.\nIn Proceedings of the Ninth ACM International Con-\nference on Web Search and Data Mining, pages 143–\n152. ACM.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nChong Wang, Yining Wang, Po-Sen Huang, Abdel-\nrahman Mohamed, Dengyong Zhou, and Li Deng.\n2017. Sequence modeling via segmentations. In\nProceedings of the 34th International Conference\non Machine Learning-Volume 70, pages 3674–3683.\nJMLR. org.\nPo-Wei Wang, J Zico Kolter, Vijai Mohan, and Inder-\njit S Dhillon. 2018. Realtime query completion via\ndeep language models. SIGIR eCom.\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. Machine learning, 8(3-4):229–256.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in neural information pro-\ncessing systems, pages 649–657.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8130287528038025
    },
    {
      "name": "Metric (unit)",
      "score": 0.6747283339500427
    },
    {
      "name": "Character (mathematics)",
      "score": 0.6489831209182739
    },
    {
      "name": "Language model",
      "score": 0.5917282700538635
    },
    {
      "name": "Prefix",
      "score": 0.5737563371658325
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.5346279144287109
    },
    {
      "name": "Decoding methods",
      "score": 0.48115432262420654
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4686994254589081
    },
    {
      "name": "Natural language processing",
      "score": 0.4283742308616638
    },
    {
      "name": "Component (thermodynamics)",
      "score": 0.41970232129096985
    },
    {
      "name": "Baseline (sea)",
      "score": 0.4139162003993988
    },
    {
      "name": "Algorithm",
      "score": 0.2597581744194031
    },
    {
      "name": "Linguistics",
      "score": 0.08563673496246338
    },
    {
      "name": "Mathematics",
      "score": 0.0802590548992157
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I60922564",
      "name": "Naver (South Korea)",
      "country": "KR"
    }
  ],
  "cited_by": 12
}