{
  "title": "A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment",
  "url": "https://openalex.org/W3177376359",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5100458628",
      "name": "Jingyi Zhang",
      "affiliations": [
        "German Research Centre for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A5049194403",
      "name": "Josef van Genabith",
      "affiliations": [
        null,
        "German Research Centre for Artificial Intelligence",
        "Saarland University",
        "Language Science (South Korea)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4241645538",
    "https://openalex.org/W2970757000",
    "https://openalex.org/W2759932073",
    "https://openalex.org/W2970279348",
    "https://openalex.org/W2998135987",
    "https://openalex.org/W2251395256",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2156985047",
    "https://openalex.org/W2250545560",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3103942011",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2759027098",
    "https://openalex.org/W3120929527",
    "https://openalex.org/W1973923101",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3104881680",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W2970911692",
    "https://openalex.org/W2912070261",
    "https://openalex.org/W2148708890",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2970045405",
    "https://openalex.org/W2153653739",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3035463087",
    "https://openalex.org/W2133564696"
  ],
  "abstract": "Jingyi Zhang, Josef van Genabith. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 283–292\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n283\nA Bidirectional Transformer Based Alignment Model for Unsupervised\nWord Alignment\nJingyi Zhang1 and Josef van Genabith1,2\n1German Research Center for Artiﬁcial Intelligence (DFKI),\nSaarland Informatics Campus, Saarbr¨ucken, Germany\n2Department of Language Science and Technology, Saarland University,\nSaarland Informatics Campus, Saarbr¨ucken, Germany\nJingyi.Zhang@dfki.de,Josef.Van Genabith@dfki.de\nAbstract\nWord alignment and machine translation are\ntwo closely related tasks. Neural transla-\ntion models, such as RNN-based and Trans-\nformer models, employ a target-to-source at-\ntention mechanism which can provide rough\nword alignments, but with a rather low accu-\nracy. High-quality word alignment can help\nneural machine translation in many different\nways, such as missing word detection, anno-\ntation transfer and lexicon injection. Existing\nmethods for learning word alignment include\nstatistical word aligners (e.g. GIZA++) and re-\ncently neural word alignment models. This pa-\nper presents a bidirectional Transformer based\nalignment (BTBA) model for unsupervised\nlearning of the word alignment task. Our\nBTBA model predicts the current target word\nby attending the source context and both left-\nside and right-side target context to produce\naccurate target-to-source attention (alignment).\nWe further ﬁne-tune the target-to-source atten-\ntion in the BTBA model to obtain better align-\nments using a full context based optimization\nmethod and self-supervised training. We test\nour method on three word alignment tasks and\nshow that our method outperforms both previ-\nous neural word alignment approaches and the\npopular statistical word aligner GIZA++.\n1 Introduction\nNeural machine translation (NMT) (Bahdanau\net al., 2014; Vaswani et al., 2017) achieves state-\nof-the-art results for various translation tasks (Bar-\nrault et al., 2019, 2020). Neural translation models,\nsuch as RNN-based (Bahdanau et al., 2014) and\nTransformer (Vaswani et al., 2017) models, gen-\nerally have an encoder-decoder structure with a\ntarget-to-source attention mechanism. The target-\nto-source attention in NMT can provide rough word\nalignments but with a rather low accuracy (Koehn\nand Knowles, 2017). High-quality word alignment\ncan be used to help NMT in many different ways,\nsuch as detecting source words that are missing\nin the translation (Lei et al., 2019), integrating an\nexternal lexicon into NMT to improve translation\nfor domain-speciﬁc terminology or low-frequency\nwords (Chatterjee et al., 2017; Chen et al., 2020),\ntransferring word-level annotations (e.g. under-\nline and hyperlink) from source to target for docu-\nment/webpage translation (M¨uller, 2017).\nA number of approaches have been proposed to\nlearn the word alignment task, including both statis-\ntical models (Brown et al., 1993) and recently neu-\nral models (Zenkel et al., 2019; Garg et al., 2019;\nZenkel et al., 2020; Chen et al., 2020; Stengel-\nEskin et al., 2019; Nagata et al., 2020). The pop-\nular word alignment tool GIZA++ (Och and Ney,\n2003) is based on statistical IBM models (Brown\net al., 1993) which learn the word alignment task\nthrough unsupervised learning and do not require\ngold alignments from humans as training data. As\ndeep neural networks have been successfully ap-\nplied to many natural language processing (NLP)\ntasks, neural word alignment approaches have de-\nveloped rapidly and outperformed statistical word\naligners (Zenkel et al., 2020; Garg et al., 2019).\nNeural word alignment approaches include both su-\npervised and unsupervised approaches: supervised\napproaches (Stengel-Eskin et al., 2019; Nagata\net al., 2020) use gold alignments from human an-\nnotators as training data and train neural models to\nlearn word alignment through supervised learning;\nunsupervised approaches do not use gold human\nalignments for model training and mainly focus on\nimproving the target-to-source attention in NMT\nmodels to produce better word alignment, such\nas performing attention optimization during infer-\nence (Zenkel et al., 2019), encouraging contiguous\nalignment connections (Zenkel et al., 2020) or us-\ning alignments from GIZA++ to supervise/guide\nthe attention in NMT models (Garg et al., 2019).\n284\nWe propose a bidirectional Transformer based\nalignment (BTBA) model for unsupervised learn-\ning of the word alignment task. Our BTBA model\npredicts the current target word by paying atten-\ntion to the source context and both left-side and\nright-side target context to produce accurate target-\nto-source attention (alignment). Compared to the\noriginal Transformer translation model (Vaswani\net al., 2017) which computes target-to-source at-\ntention based on only the left-side target context\ndue to left-to-right autoregressive decoding, our\nBTBA model can exploit both left-side and right-\nside target context to compute more accurate target-\nto-source attention (alignment). We further ﬁne-\ntune the BTBA model to produce better alignments\nusing a full context based optimization method\nand self-supervised training. We test our method\non three word alignment tasks and show that our\nmethod outperforms previous neural word align-\nment approaches and also beats the popular statisti-\ncal word aligner GIZA++.\n2 Background\n2.1 Word Alignment Task\nThe goal of the word alignment task (Och and Ney,\n2003) is to ﬁnd word-level alignments for paral-\nlel source and target sentences. Given a source\nsentence sI−1\n0 = s0, ..., si, ..., sI−1 and its parallel\ntarget sentence tJ−1\n0 = t0, ..., tj, ..., tJ−1, the word\nalignment G is deﬁned as a set of links that link the\ncorresponding source and target words as shown in\nEquation 1.\nG ⊆{(i, j) :i = 0, ..., I−1; j = 0, ..., J−1} (1)\nThe word alignment G allows one-to-one, one-to-\nmany, many-to-one, many-to-many alignments and\nalso unaligned words (Och and Ney, 2003). Due to\nthe lack of labelled training data (gold alignments\nannotated by humans) for the word alignment task,\nmost word alignment methods learn the word align-\nment task through unsupervised learning (Brown\net al., 1993; Zenkel et al., 2020; Chen et al., 2020).\n2.2 Neural Machine Translation\nNeural translation models (Bahdanau et al., 2014;\nVaswani et al., 2017) generally have an encoder-\ndecoder structure with a target-to-source attention\nmechanism: the encoder encodes the source sen-\ntence; the decoder generates the target sentence\nby attending the source context and performing\nleft-to-right autoregressive decoding. The target-to-\nsource attention learned in NMT models can pro-\nvide rough word alignments between source and\ntarget words. Among various translation models,\nthe Transformer translation model (Vaswani et al.,\n2017) achieves state-of-the-art results on various\ntranslation tasks and is based solely on attention:\nsource-to-source attention in the encoder; target-to-\ntarget and target-to-source attention in the decoder.\nThe attention networks used in the Transformer\nmodel are called multi-head attention which per-\nforms attention using multiple heads as shown in\nEquation 2.\nMultiHead (Q, K, V)\n= Concat (head0, ..., headN−1) Wo\nHeadn = An ·Vn\nAn = softmax\n(\nQnKT\nn√\ndk\n)\nQn = QWQ\nn , Kn = KW K\nn , Vn = V WV\nn\n(2)\nwhere Q, K and V are query, keys, values for\nthe attention function; Wo, WQ\nn , WK\nn and WV\nn\nare model parameters; dk is the dimension of the\nkeys. Based on parallelizable attention networks,\nthe Transformer can be trained much faster than\nRNN-based translation models (Bahdanau et al.,\n2014).\n3 Related Work\n3.1 Statistical Alignment Models\nWord alignment is a key component in traditional\nstatistical machine translation (SMT), such as\nphrase-based SMT (Koehn et al., 2003) which ex-\ntracts phrase-based translation rules based on word\nalignments. The popular statistical word alignment\ntool GIZA++ (Och and Ney, 2003) implements the\nstatistical IBM models (Brown et al., 1993). The\nstatistical IBM models are mainly based on lexical\ntranslation probabilities. Words that co-occur fre-\nquently in parallel sentences generally have higher\nlexical translation probabilities and are more likely\nto be aligned. The statistical IBM models are\ntrained using parallel sentence pairs with no word-\nlevel alignment annotations and therefore learn the\nword alignment task through unsupervised learn-\ning. Based on a reparameterization of IBM Model\n2, Dyer et al. (2013) presented another popular sta-\ntistical word alignment tool fast align which can be\ntrained faster than GIZA++, but GIZA++ generally\nproduces better word alignments than fast align.\n285\n3.2 Neural Alignment Models\nWith neural networks being successfully applied\nto many NLP tasks, neural word alignment ap-\nproaches have received much attention. The ﬁrst\nneural word alignment models are based on feed-\nforward neural networks (Yang et al., 2013) and\nrecurrent neural networks (Tamura et al., 2014)\nwhich can be trained in an unsupervised manner by\nnoise-contrastive estimation (NCE) (Gutmann and\nHyv¨arinen, 2010) or in a supervised manner by us-\ning alignments from human annotators or existing\nword aligners as labelled training data.\nAs NMT (Bahdanau et al., 2014; Vaswani et al.,\n2017) achieves great success, the target-to-source\nattention in NMT models can be used to infer rough\nword alignments, but with a rather low accuracy.\nA number of recent works focus on improving the\ntarget-to-source attention in NMT to produce better\nword alignments (Garg et al., 2019; Zenkel et al.,\n2019; Chen et al., 2020; Zenkel et al., 2020). Garg\net al. (2019) trained the Transformer translation\nmodel to jointly learn translation and word align-\nment through multi-task learning using word align-\nments from existing word aligners such as GIZA++\nas labelled training data. Chen et al. (2020) pro-\nposed a method to infer more accurate word align-\nments from the Transformer translation model by\nchoosing the appropriate decoding step and layer\nfor word alignment inference. Zenkel et al. (2019)\nproposed an alignment layer for the Transformer\ntranslation model and they only used the output\nof the alignment layer for target word prediction\nwhich forces the alignment layer to produce bet-\nter alignment (attention). Zenkel et al. (2019) also\nproposed an attention optimization method which\ndirectly optimizes the attention for the test set to\nproduce better alignment. Zenkel et al. (2020) pro-\nposed to improve the attention in NMT by using a\ncontiguity loss to encourage contiguous alignment\nconnections and performing direct attention opti-\nmization to maximize the translation probability\nfor both the source-to-target and target-to-source\ntranslation models. Compared to these methods\nthat infer word alignments based on NMT target-to-\nsource attention which is computed by considering\nonly the left-side target context, our BTBA model\ncan exploit both left-side and right-side target con-\ntext to compute better target-to-source attention\n(alignment).\nThere are also a number of supervised neural\napproaches that require gold alignments from hu-\nmans for learning the word alignment task (Stengel-\nEskin et al., 2019; Nagata et al., 2020). Because\ngold alignments from humans are scarce, Stengel-\nEskin et al. (2019); Nagata et al. (2020)’s models\nonly have a small size of task-speciﬁc training data\nand exploit representations from pre-trained NMT\nand BERT models. Compared to these supervised\nmethods, our method does not require gold human\nalignments for model training.\n4 Our Approach\nWe present a bidirectional Transformer based align-\nment (BTBA) model for unsupervised learning of\nthe word alignment task. Motivated by BERT\nwhich learns a masked language model (Devlin\net al., 2019), we randomly mask 10% of the words\nin the target sentence and then train our BTBA\nmodel to predict the masked target words by pay-\ning attention to the source context and both left-\nside and right-side target context. Therefore, our\nBTBA model can exploit both left-side and right-\nside target context to compute more accurate target-\nto-source attention (alignment) compared to the\noriginal Transformer translation model (Vaswani\net al., 2017) which computes the target-to-source\nattention based on only the left-side target context\ndue to left-to-right autoregressive decoding. We\nfurther ﬁne-tune the target-to-source attention in\nthe BTBA model to produce better alignments us-\ning a full context based optimization method and\nself-supervised training.\n4.1 Bidirectional Transformer Based\nAlignment (BTBA)\nFigure 1 shows the architecture of the proposed\nBTBA model. The encoder is used to encode the\nsource sentence 1 and has the same structure as\nthe original Transformer encoder (Vaswani et al.,\n2017). The input of the decoder is the masked\ntarget sentence and 10% of the words in the tar-\nget sentence are randomly masked2. As shown in\nFigure 1, the target sentence contains a masked\nword <x>. The decoder contains 6 layers. Each\nof the ﬁrst 5 layers of the decoder has 3 sub-layers:\n1Following Och and Ney (2003)’s work, we add a<bos>\ntoken at the beginning of the source sentence for target words\nthat are not aligned with any source words.\n2During training, we randomly mask 10% of the words in\nthe target sentences for each training epoch, i.e., one target\nsentence is masked differently for different training epochs.\nIf a target sentence contains less than 10 words, then we just\nrandomly mask one word in this sentence.\n286\nMulti-Head\nAttention\nAdd & Norm\nFeed\nForward\nAdd & Norm\nMulti-Head\nAttention\nAdd & Norm\nFeed\nForward\nAdd & Norm\nMulti-Head\nAttention\nAdd & Norm\nEmbedding Embedding\n6 x 5 x\nMulti-Head\nAttention\nMulti-Head\nAttention\nAdd & Norm\nPositional\nEncoding\nPositional\nEncoding\nV0 V1 V2 V3 V4\nAijncake\nTarget:\nthe <x> is very delicious\nSource:\n<bos> der kuchen ist sehr lecker\nFigure 1: Architecture of our BTBA model.\na multi-head self-attention sub-layer, a target-to-\nsource multi-head attention sub-layer and a feed\nforward sub-layer, like a standard Transformer de-\ncoder layer except that the self-attention sub-layer\nin the standard Transformer decoder can only at-\ntend left-side target context while the self-attention\nsub-layer in our BTBA decoder can attend all target\nwords and make use of both left-side and right-side\ntarget context to compute better target-to-source\nattention (alignment). The last layer of the BTBA\ndecoder contains a self-attention sub-layer and a\ntarget-to-source attention sub-layer like the ﬁrst 5\nlayers of the BTBA decoder but without the feed-\nforward sub-layer. We use the output of the last\ntarget-to-source attention sub-layer for predicting\nthe masked target words and we use the attention of\nthe last target-to-source attention sub-layer for in-\nferring word alignments between source and target\nwords. Our design that only uses the last target-\nto-source attention sub-layer output for predicting\nthe masked target words is motivated by the align-\nment layer of Zenkel et al. (2019) in order to force\nOriginal the cake is very delicious\n<x> cake is very delicious\nthe <x> is very delicious\nMasked the cake <x> very delicious\nthe cake is <x> delicious\nthe cake is very <x>\nTable 1: Masking target sentences in the test set.\nthe last target-to-source attention sub-layer to pay\nattention to the most important source words for\npredicting the target word and therefore produce\nbetter word alignments.\nIn Figure 1, Aijn is the attention value of the\njth target word paying to the ith source word using\nthe nth head in the last target-to-source multi-head\nattention sub-layer. V0, V1, V2, V3, V4 are the out-\nputs of the decoder for the 5 target words and V1\nis used to predict the masked target word “cake”.\nBecause V1 is used to predict “cake”, the attention\nvalue A21n should be learned to be high in order\nto make V1 contain the most useful source infor-\nmation (“kuchen”). Therefore, Aijn can be used\nto infer word alignment for the target word “cake”\neffectively. However, Aijn cannot provide good\nword alignments for unmasked target words such\nas “delicious” in Figure 1 becauseV4 is not used to\npredict any target word and A54n is not necessarily\nlearned to be high.\nBecause Aijn can only be used to infer accu-\nrate word alignment for masked target words but\nwe want to get alignments for all target words in\nthe test set, we mask a target sentence tJ−1\n0 in the\ntest set J times and each time we mask one target\nword as shown in Table 1. Each masked target sen-\ntence is fed into the BTBA model together with the\nsource sentence and then we collect the attention\nAijn for the masked target words. Suppose the j′th\ntarget word is masked, then we compute the source\nposition that it should be aligned to as,\ni′= arg max\ni\nN−1∑\nn=0\nAij′n (3)\n4.2 Full Context Based Optimization\nIn Equation 3, the attention Aij′n for the j′ tar-\nget word is computed by considering both left-side\nand right-side target context, but information about\nthe current target word is not used since the j′tar-\nget word is masked. For example in Figure 1, the\nBTBA model does not know that the second target\nword is “cake” because it is masked, therefore the\nBTBA model computes the attention (alignment)\n287\nfor “cake” only using the left-side and right-side\ncontext of “cake” without knowing that the word\nthat needs to be aligned is “cake”. We propose a\nnovel full context based optimization method to use\nfull target context, including the current target word\ninformation, to improve the target-to-source atten-\ntion in the BTBA model to produce better align-\nments. That is for the last 50 training steps of the\nBTBA model, we do not mask the target sentence\nany more and we only optimize parameters WQ\nn\nand WK\nn in the last target-to-source multi-head at-\ntention sub-layer. As shown in Equation 2, WQ\nn\nand WK\nn are parameters that are used to compute\nthe attention values in multi-head attention. Opti-\nmizing WQ\nn and WK\nn based on full target context\ncan help the BTBA model to produce better atten-\ntion (alignment) while at the same time freezing\nother parameters can make the BTBA model keep\nthe knowledge learned from masked target word\nprediction. After full target context based optimiza-\ntion, we do not need to mask target sentences in\nthe test set as shown in Table 1 any more. We\ncan directly feed the original source and target test\nsentences into the BTBA model and compute atten-\ntion (alignment) for all target words in the sentence.\nThe full context based optimization method can be\nseen as a ﬁne-tuning of the original BTBA model,\ni.e. we ﬁne-tune the two parameters WQ\nn and WK\nn\nin the last target-to-source attention layer based on\nfull target context to compute more accurate word\nalignments.\n4.3 Self-Supervised Training\nThe BTBA model learns word alignment through\nunsupervised learning and does not require labelled\ndata for the word alignment task. We train two\nunsupervised BTBA models, one for the forward\ndirection (source-to-target) and one for the back-\nward direction (target-to-source), and then sym-\nmetrize the alignments using heuristics such as\ngrow-diagonal-ﬁnal-and (Och and Ney, 2003) as\nthe symmetrized alignments have better quality\nthan the alignments from a single forward or back-\nward model. After unsupervised learning, we use\nthe symmetrized word alignmentsGa inferred from\nour unsupervised BTBA models as labelled data to\nfurther ﬁne-tune each BTBA model for the word\nalignment task through supervised training using\nthe alignment loss in Equation 4 following Garg\net al. (2019)’s work. 3 During supervised train-\ning, the BTBA model is trained to learn the align-\nment task instead of masked target word prediction,\ntherefore the target sentence does not need to be\nmasked.\nLa (A) =− 1\n|Ga|\n∑\n(p,q)∈Ga\nN−1∑\nn=0\nlog (Apqn) (4)\nNote that we apply byte pair encoding (BPE)\n(Sennrich et al., 2016) for both source and tar-\nget sentences before we feed them into the BTBA\nmodel. Therefore the alignments inferred from\nthe BTBA model is on BPE-level. We convert 4\nBPE-level alignments to word-level alignments be-\nfore we perform alignment symmetrization. Af-\nter alignment symmetrization, we want to use\nthe symmetrized alignments to further ﬁne-tune\neach BTBA model through supervised learning and\ntherefore we convert5 the word-level alignments\nback to BPE-level for supervised training of the\nBTBA models.\n5 Experiments\n5.1 Settings\nIn order to compare with previous work, we used\nthe same datesets6 as Zenkel et al. (2020)’s work\nand conducted word alignment experiments for\nthree language pairs: German ↔ English (DeEn),\nEnglish ↔ French (EnFr) and Romanian ↔ En-\nglish (RoEn). Each language pair contains a test\nset and a training set: the test set contains paral-\nlel sentences with gold word alignments annotated\nby humans; the training set contains only parallel\nsentences with no word alignments. Table 2 gives\nnumbers of sentence pairs contained in the train-\ning and test sets. Parallel sentences from both the\ntraining set and the test set can be used to train\n3We optimize all model parameters during supervised ﬁne-\ntuning.\n4To convert BPE-level alignments to word-level align-\nments, we add an alignment between a source word and a\ntarget word if any parts of these two words are aligned. Align-\nments between the source <bos> token and any target word\nare deleted; alignments between the last source word “.” (full\nstop) and a target word which is not the last target word are\nalso deleted.\n5To convert word-level alignments to BPE-level align-\nments, we add an alignment between a source BPE token\nand a target BPE token if the source word and the target word\nthat contain these two BPE tokens are aligned; we add an\nalignment between the source <bos> token and a target BPE\ntoken if the target word that contains this target BPE token is\nnot aligned with any source words.\n6https://github.com/lilt/alignment-scripts\n288\nDeEn EnFr RoEn\nTRAIN 1.91M 1.13M 447k\nTEST 508 447 248\nTable 2: Numbers of sentence pairs in the datasets.\nunsupervised word alignment models. We use BPE\n(Sennrich et al., 2016) to learn a joint source and tar-\nget vocabulary of 40k. After BPE, we train BTBA\nmodels to learn the word alignment tasks. We use\na word embedding size of 512. The feed forward\nlayer contains 2048 hidden units. The multi-head\nattention layer contains 8 heads. We use the Adam\n(Kingma and Ba, 2014) algorithm for optimiza-\ntion and set the learning rate to 0.0002. We use a\ndropout of 0.3. Each training batch contains 40k\nmasked target words. Since the word alignment\ntasks do not provide validation data, we trained\nall BTBA models for a ﬁxed number of training\nepochs: 50 for DeEn, 100 for EnFr and 200 for\nRoEn.7 For the last 50 training steps of each BTBA\nmodel, we performed full context based optimiza-\ntion.\nFor each language pair, we trained two BTBA\nmodels, one for the forward direction and one\nfor the backward direction, and then symmetrized\nthe alignments. We tested different heuristics for\nalignment symmetrization, including the standard\nMoses heuristics, grow-diagonal, grow-diagonal-\nﬁnal, grow-diagonal-ﬁnal-and. We also tested an-\nother heuristic grow-diagonal-and which is slightly\ndifferent from grow-diagonal: the grow-diagonal-\nand heuristic only adds a new alignment (i, j)\nwhen both si and tj are unaligned while grow-\ndiagonal adds a new alignment (i, j) when any\nof the two words (si and tj) are unaligned. We ﬁnd\nthat the Moses heuristic grow-diagonal-ﬁnal-and\ngenerally achieved the best results for symmetriz-\ning the BTBA alignments, but grow-diagonal-and\nworked particularly good for the EnFr task.\nFinally, we used the symmetrized alignments\ninferred from our unsupervised BTBA models as\nlabelled data to further ﬁne-tune each BTBA model\nto learn the alignment task through supervised train-\ning. We ﬁne-tuned each BTBA model for 50 train-\ning steps using the alignment loss in Equation 4.\nIn addition, we also tested to use alignments from\nGIZA++ instead of alignments inferred from our\n7The training time (time of one training epoch ×number\nof training epochs) of one BTBA model for different tasks\n(DeEn, EnFr and RoEn) is roughly the same, 30 hours using 4\nGPUs.\nMethod DeEn EnFr RoEn\nZenkel et al. (2019) 21.2% 10.0% 27.6%\nGarg et al. (2019) 16.0% 4.6% 23.1%\nZenkel et al. (2020) 16.3% 5.0% 23.4%\nChen et al. (2020) 15.4% 4.7% 21.2%\nGIZA++ 18.4% 5.2% 24.2%\nOurs\nBTBA-left 30.3% 20.2% 33.0%\nBTBA-right 32.3% 14.9% 38.6%\nBTBA 17.8% 9.5% 22.9%\n+ FCBO 16.3% 8.9% 20.6%\n+ SST 14.3% 6.7% 18.5%\n+ GST 14.5% 4.2% 19.7%\nTable 3: AER Results. FCBO: full context based opti-\nmization; SST: self-supervised training; GST: GIZA++\nsupervised training.\nunsupervised BTBA models as labelled data for\nsupervised ﬁne-tuning of the BTBA models.\n5.2 Results\nTable 3 gives alignment error rate (AER) (Och and\nNey, 2000) results of our BTBA model and com-\nparison with previous work. Table 3 also gives\nresults of BTBA-left and BTBA-right: BTBA-left\nmeans that the BTBA decoder only attends left-\nside target context; BTBA-right means that the\nBTBA decoder only attends right-side target con-\ntext. As shown in Table 3, the BTBA model, which\nuses both left-side and right-side target context,\nsigniﬁcantly outperformed BTBA-left and BTBA-\nright. Results also show that the performance of\nour BTBA model can be further improved by full\ncontext based optimization (FCBO) and supervised\ntraining including both self-supervised training and\nGIZA++ supervised training. For DeEn and RoEn\ntasks, the self-supervised BTBA (S-BTBA) model\nachieved the best results, outperforming previous\nneural and statistical methods. For the EnFr task,\nas the statistical aligner GIZA++ performed well\nand achieved better results than our unsupervised\nBTBA model, the GIZA++ supervised BTBA (G-\nBTBA) model achieved better results than the S-\nBTBA model and also outperformed the original\nGIZA++ and previous neural models.\nTables 4, 5 and 6 give results of using differ-\nent heuristics for symmetrizing alignments pro-\nduced by BTBA, GIZA++ and G-BTBA, respec-\ntively. For our unsupervised and self-supervised\nBTBA models, grow-diagonal-ﬁnal-and achieved\nthe best results on DeEn and RoEn tasks while\ngrow-diagonal-and achieved the best results on the\nEnFr task. For GIZA++ and G-BTBA, the best\nheuristics for different language pairs are quite dif-\nferent, though grow-diagonal-ﬁnal-and generally\n289\nDeEn EnFr RoEn\nBTBA +FCBO +SST BTBA +FCBO +SST BTBA +FCBO +SST\nforward 20.2% 18.3% 14.3% 13.6% 12.8% 7.3% 24.7% 22.4% 20.5%\nbackward 23.8% 23.3% 17.2% 14.6% 13.3% 7.5% 27.3% 26.1% 22.0%\nunion 20.6% 18.3% 14.5% 15.7% 14.3% 7.5% 24.1% 21.2% 18.9%\nintersection 23.7% 23.9% 17.1% 11.6% 11.2% 7.4% 28.3% 27.9% 24.0%\ngrow-diagonal 19.9% 18.5% 14.3% 11.2% 10.7% 6.9% 23.6% 21.6% 18.6%\ngrow-diagonal-and 21.0% 20.6% 17.3% 9.5% 8.9% 6.7% 26.1% 25.4% 23.6%\ngrow-diagonal-ﬁnal 19.5% 17.3% 14.4% 14.4% 13.4% 7.4% 23.4% 20.8% 18.6%\ngrow-diagonal-ﬁnal-and 17.8% 16.3% 14.3% 11.9% 11.2% 7.0% 22.9% 20.6% 18.5%\nTable 4: Comparison of different heuristics for symmetrizing the BTBA alignments. FCBO: full context based\noptimization. SST: self-supervised training.\nDeEn EnFr RoEn\nforward 19.0% 10.3% 25.6%\nbackward 22.5% 9.1% 29.7%\nunion 22.1% 12.9% 27.5%\nintersection 19.0% 5.2% 27.8%\ngrow-diagonal 18.4% 7.7% 24.5%\ngrow-diagonal-and 18.9% 5.7% 26.1%\ngrow-diagonal-ﬁnal 21.1% 11.7% 26.0%\ngrow-diagonal-ﬁnal-and 18.9% 8.5% 24.2%\nTable 5: Comparison of different heuristics for sym-\nmetrizing GIZA++ alignments.\n16\n18\n20\n22\n24\n26\n28\n30\n0 10 20 30 40 50 60\nfreeze\nno freeze\nAER\nTraining step\nFigure 2: DeEn test AER per training step during\nFCBO with/without parameter freezing.\nobtained good (best or close to best) results on\nDeEn and RoEn tasks while grow-diagonal-and\ngenerally obtained good (close to best) results on\nthe EnFr task.\nFCBO with/without Parameter Freezing As\nwe explained in Section 4.2, during full context\nbased optimization (FCBO), we only optimize\nWQ\nn and WK\nn in the last target-to-source attention\nsub-layer and freeze all other parameters so the\nBTBA model can keep the knowledge learned from\nmasked target word prediction. We also tested\nto optimize all parameters of the BTBA model\nwithout parameter freezing during FCBO. Figure 2\nshows how the AER results on the DeEn test set\nchanged during FCBO with and without param-\neter freezing. Without freezing any parameters\nDeEn EnFr RoEn\nforward 14.5% 5.8% 21.4%\nbackward 17.6% 4.2% 21.9%\nunion 15.1% 5.3% 19.9%\nintersection 17.2% 4.7% 23.6%\ngrow-diagonal 14.7% 4.6% 19.7%\ngrow-diagonal-and 17.5% 4.4% 23.7%\ngrow-diagonal-ﬁnal 15.1% 5.3% 19.8%\ngrow-diagonal-ﬁnal-and 14.8% 4.7% 19.8%\nTable 6: Comparison of different heuristics for sym-\nmetrizing G-BTBA alignments.\nduring FCBO, the AER result (the red curve) ﬁrst\nincreased a little, then decreased sharply, and soon\nincreased again. In contrast, when we freeze most\nof the parameters, the AER result (the blue curve)\ndecreased stably and eventually got better results\n(16.3%) than no parameter freezing (16.7%). Note\nthat the results in Figure 2 are computed based\non full target context, i.e., the target sentence is\nnot masked. As we explained in Section 4.1, the\nBTBA model without FCBO should only be used\nto infer word alignments for masked target words.\nWithout FCBO, using the BTBA model to infer\nword alignments for unmasked target words pro-\nduces poor AER results (26.9% as shown in Fig-\nure 2) compared to using the BTBA model to infer\nword alignments for masked target words (17.8%\nas shown in Table 3). FCBO can quickly improve\nthe results of using the BTBA model for inferring\nword alignments for unmasked target words, and\neventually after FCBO, the BTBA model can effec-\ntively use full target context to compute better word\nalignment compared to the original BTBA model\nwithout FCBO (16.3% versus 17.8% as shown in\nTable 3).\nTraining Data for Supervised Learning Be-\ncause the symmetrized BTBA alignments have bet-\nter quality compared to alignments from a single\nunidirectional (forward or backward) BTBA model\nas shown in Table 4, we used the symmetrized\n290\nGold\nOurs\n, .dürfenwerdenabgefülltschaumweinflascheningetränkewelcheklar definiertberichtsvorschlagimes wird\nmaybeverageswhichdefinesclearlyreportthe .bottleswinesparklinginbottledbe\nmaybeverageswhichdefinesclearlyreportthe .bottleswinesparklinginbottledbe\nFigure 3: An example of gold alignments and alignments produced by our S-BTBA model.\n14\n16\n18\n20\n22\n24\n0 10 20 30 40 50 60\nUNI\nSYM\nTraining  step\nAER\nFigure 4: AER results of the forward BTBA model dur-\ning self-supervised training. UNI: using unidirectional\nBTBA alignments as labelled training data. SYM: us-\ning symmetrized BTBA alignments as labelled training\ndata.\nword alignments inferred from our unsupervised\nBTBA models as labelled data to further ﬁne-tune\neach unidirectional BTBA model for the alignment\ntask through supervised training. We also tested\nto use unidirectional BTBA alignments instead of\nsymmetrized BTBA alignments as labelled data\nfor supervised training. Figure 4 (the blue curve)\nshows how the performance of the forward BTBA\nmodel of the DeEn task changes during supervised\ntraining when using unidirectional alignments in-\nferred from itself (the forward BTBA model) as\nlabelled training data, which demonstrates that\nthe forward BTBA model can be signiﬁcantly im-\nproved through supervised training even when the\ntraining data is inferred from itself and not im-\nproved by alignment symmetrization. Figure 4 also\nshows that using symmetrized alignments for su-\npervised training (the red curve) did achieve better\nresults than using unidirectional alignments for su-\npervised training. In addition, it is worth noting that\nsupervised training can improve the BTBA model\neven if the quality of the labelled training data is\nsomewhat worse than the BTBA model itself, e.g.\nfor the RoEn task, using the GIZA++ alignments\nfor ﬁne-tuning the forward BTBA model through\nsupervised training improved the result of the for-\nward BTBA model (22.4% → 21.4% as shown in\nDeEn EnFr RoEn\nS-BTBA FF 12.3 11.3 18.2\nCC 6.1 3.3 7.8\nFC 44.4 12.8 41.1\nG-BTBA FF 13.2 5.1 18.6\nCC 7.1 2.9 8.3\nFC 43.3 9.3 46.1\nTable 7: AER for different types of alignments.\nTable 4 and Table 6) even though GIZA++ pro-\nduced worse alignments (24.2% in Table 3) than\nthe forward BTBA model.\nAlignment Error Analysis We analyze the\nalignment errors produced by our system and ﬁnd\nthat most of the alignment errors are caused by\nfunction words. As shown in the alignment exam-\nple in Figure 3, source and target corresponding\ncontent words (e.g. “deﬁniert” and “deﬁnes”) are\nall correctly aligned by our model, but function\nwords such as “the”, “im” and “wird” are not cor-\nrectly aligned. To give a more detailed analysis, we\ncompute AER results of our model for 3 different\ntypes of alignments: FF (alignments between two\nfunction words), CC (alignments between two con-\ntent words) and FC (alignments between a function\nword and a content word).8 Table 7 shows that our\nmodels achieved signiﬁcantly better results for CC\nalignments than for FF and FC alignments. Func-\ntion words are more difﬁcult to align than content\nwords most likely because content words in a paral-\nlel sentence pair usually have very clear correspond-\ning relations (such as “deﬁnes” clearly corresponds\nto “deﬁniert” in Figure 3), but function words (such\nas “the”, “es” and “im”) are used more ﬂexibly and\nfrequently do not have clear corresponding words\nin parallel sentences, which increases the alignment\ndifﬁculty signiﬁcantly.\n8For each language, we judge whether a word is a function\nword or a content word using a list of stopwords from nltk,\nhttps://www.nltk.org/\n291\nde→en en →de\nSHIFT-AET 34.8 28.0\nOurs 35.1 28.7\nTable 8: Translation results (BLEU) for dictionary-\nguided NMT.\n5.3 Dictionary-Guided NMT via Word\nAlignment\nFor downstream tasks, word alignment can be used\nto improve dictionary-guided NMT (Song et al.,\n2020; Chen et al., 2020). Speciﬁcally, at each de-\ncoding step in NMT, Chen et al. (2020) used a\nSHIFT-AET method to compute word alignment\nfor the newly generated target word and then re-\nvised the newly generated target word by encour-\naging the pre-speciﬁed translation from the dictio-\nnary. The SHIFT-AET alignment method adds a\nseparate alignment module to the original Trans-\nformer translation model (Vaswani et al., 2017)\nand trains the separate alignment module using\nalignments induced from the attention weights\nof the original Transformer. To test the effec-\ntiveness of our alignment method for improving\ndictionary-guided NMT, we used the alignments\ninferred from our BTBA models as labelled data\nfor supervising the SHIFT-AET alignment module\nand performed dictionary-guided translation for the\nGerman↔English language pair following Chen\net al. (2020)’s work. Table 8 gives the translation re-\nsults of dictionary-guided NMT and shows that our\nalignment method led to higher translation quality\ncompared to the original SHIFT-AET method.\n6 Conclusion\nThis paper presents a novel BTBA model for unsu-\npervised learning of the word alignment task. Our\nBTBA model predicts the current target word by\npaying attention to the source context and both\nleft-side and right-side target context to produce\naccurate target-to-source attention (alignment). We\nfurther ﬁne-tune the target-to-source attention in\nthe BTBA model to obtain better alignments using\na full context based optimization method and self-\nsupervised training. We test our method on three\nword alignment tasks and show that our method out-\nperforms both previous neural alignment methods\nand the popular statistical word aligner GIZA++.\nAcknowledgments\nThis work is supported by the German Federal Min-\nistry of Education and Research (BMBF) under\nfunding code 01IW20010 (CORA4NLP).\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nLoc Barrault, Magdalena Biesialska, Ondej Bo-\njar, Marta R. Costa-juss, Christian Federmann,\nYvette Graham, Roman Grundkiewicz, Barry Had-\ndow, Matthias Huck, Eric Joanis, Tom Kocmi,\nPhilipp Koehn, Chi-kiu Lo, Nikola Ljubei, Christof\nMonz, Makoto Morishita, Masaaki Nagata, Toshi-\naki Nakazawa, Santanu Pal, Matt Post, and Marcos\nZampieri. 2020. Findings of the 2020 conference on\nmachine translation (wmt20). In Proceedings of the\nFifth Conference on Machine Translation, pages 1–\n55, Online. Association for Computational Linguis-\ntics.\nLoc Barrault, Ondej Bojar, Marta R. Costa-juss, Chris-\ntian Federmann, Mark Fishel, Yvette Graham, Barry\nHaddow, Matthias Huck, Philipp Koehn, Shervin\nMalmasi, Christof Monz, Mathias Mller, Santanu\nPal, Matt Post, and Marcos Zampieri. 2019. Find-\nings of the 2019 conference on machine translation\n(wmt19). In Proceedings of the Fourth Conference\non Machine Translation (Volume 2: Shared Task Pa-\npers, Day 1) , pages 1–61, Florence, Italy. Associa-\ntion for Computational Linguistics.\nPeter F. Brown, Stephen A. Della Pietra, Vincent J.\nDella Pietra, and Robert L. Mercer. 1993. The math-\nematics of statistical machine translation: Parameter\nestimation. Computational Linguistics, 19(2):263–\n311.\nRajen Chatterjee, Matteo Negri, Marco Turchi, Mar-\ncello Federico, Lucia Specia, and Fr ´ed´eric Blain.\n2017. Guiding neural machine translation decoding\nwith external knowledge. In Proceedings of the Sec-\nond Conference on Machine Translation, pages 157–\n168, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nYun Chen, Yang Liu, Guanhua Chen, Xin Jiang, and\nQun Liu. 2020. Accurate word alignment induction\nfrom neural machine translation. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 566–576,\nOnline. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\n292\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013. A simple, fast, and effective reparameter-\nization of IBM model 2. In Proceedings of the\n2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies , pages 644–648, At-\nlanta, Georgia. Association for Computational Lin-\nguistics.\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy,\nand Matthias Paulik. 2019. Jointly learning to align\nand translate with transformer models. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4453–4462, Hong\nKong, China. Association for Computational Lin-\nguistics.\nMichael Gutmann and Aapo Hyv ¨arinen. 2010. Noise-\ncontrastive estimation: A new estimation principle\nfor unnormalized statistical models. In Proceedings\nof the Thirteenth International Conference on Artiﬁ-\ncial Intelligence and Statistics, pages 297–304.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceed-\nings of the First Workshop on Neural Machine Trans-\nlation, pages 28–39, Vancouver. Association for\nComputational Linguistics.\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003.\nStatistical phrase-based translation. In Proceedings\nof the 2003 Human Language Technology Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics, pages 127–133.\nWenqiang Lei, Weiwen Xu, Ai Ti Aw, Yuanxin Xiang,\nand Tat Seng Chua. 2019. Revisit automatic error\ndetection for wrong and missing translation – a su-\npervised approach. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 942–952, Hong Kong, China. As-\nsociation for Computational Linguistics.\nMathias M ¨uller. 2017. Treatment of markup in sta-\ntistical machine translation. In Proceedings of the\nThird Workshop on Discourse in Machine Transla-\ntion, pages 36–46, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nMasaaki Nagata, Chousa Katsuki, and Masaaki\nNishino. 2020. A supervised word alignment\nmethod based on cross-language span predic-\ntion using multilingual bert. arXiv preprint\narXiv:2004.14516.\nFranz Josef Och and Hermann Ney. 2000. Improved\nstatistical alignment models. In Proceedings of the\n38th Annual Meeting of the Association for Com-\nputational Linguistics, pages 440–447, Hong Kong.\nAssociation for Computational Linguistics.\nFranz Josef Och and Hermann Ney. 2003. A systematic\ncomparison of various statistical alignment models.\nComputational Linguistics, 29(1):19–51.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nKai Song, Kun Wang, Heng Yu, Yue Zhang,\nZhongqiang Huang, Weihua Luo, Xiangyu Duan,\nand Min Zhang. 2020. Alignment-enhanced trans-\nformer for constraining nmt with pre-speciﬁed trans-\nlations. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 34, pages 8886–8893.\nElias Stengel-Eskin, Tzu-ray Su, Matt Post, and Ben-\njamin Van Durme. 2019. A discriminative neural\nmodel for cross-lingual word alignment. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , pages 910–920, Hong\nKong, China. Association for Computational Lin-\nguistics.\nAkihiro Tamura, Taro Watanabe, and Eiichiro Sumita.\n2014. Recurrent neural networks for word align-\nment model. In Proceedings of the 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 1470–\n1480, Baltimore, Maryland. Association for Compu-\ntational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nNan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-\nhai Yu. 2013. Word alignment modeling with con-\ntext dependent deep neural network. In Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 166–175, Soﬁa, Bulgaria. Association\nfor Computational Linguistics.\nThomas Zenkel, Joern Wuebker, and John DeNero.\n2019. Adding interpretable attention to neural trans-\nlation models improves word alignment. arXiv\npreprint arXiv:1901.11359.\nThomas Zenkel, Joern Wuebker, and John DeNero.\n2020. End-to-end neural word alignment outper-\nforms GIZA++. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1605–1617, Online. Association for\nComputational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7364596724510193
    },
    {
      "name": "Transformer",
      "score": 0.6390523910522461
    },
    {
      "name": "Natural language processing",
      "score": 0.610511064529419
    },
    {
      "name": "Zhàng",
      "score": 0.5827783942222595
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5354487895965576
    },
    {
      "name": "Speech recognition",
      "score": 0.3418169617652893
    },
    {
      "name": "Engineering",
      "score": 0.14888718724250793
    },
    {
      "name": "Electrical engineering",
      "score": 0.10264411568641663
    },
    {
      "name": "History",
      "score": 0.09723716974258423
    },
    {
      "name": "China",
      "score": 0.05783557891845703
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I33256026",
      "name": "German Research Centre for Artificial Intelligence",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I91712215",
      "name": "Saarland University",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210107233",
      "name": "Language Science (South Korea)",
      "country": "KR"
    }
  ],
  "cited_by": 12
}