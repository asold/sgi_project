{
  "title": "FASTSUBS: An Efficient and Exact Procedure for Finding the Most Likely Lexical Substitutes Based on an N-Gram Language Model",
  "url": "https://openalex.org/W1979812056",
  "year": 2012,
  "authors": [
    {
      "id": null,
      "name": "D. Yuret",
      "affiliations": [
        "Koç University"
      ]
    },
    {
      "id": null,
      "name": "D. Yuret",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2078384767",
    "https://openalex.org/W6685760887",
    "https://openalex.org/W2091525749",
    "https://openalex.org/W6638699399",
    "https://openalex.org/W2035959783",
    "https://openalex.org/W6601296810",
    "https://openalex.org/W7062008872",
    "https://openalex.org/W1736441281",
    "https://openalex.org/W1831478036",
    "https://openalex.org/W2128514324",
    "https://openalex.org/W32283444",
    "https://openalex.org/W2179984881",
    "https://openalex.org/W1631260214"
  ],
  "abstract": "Lexical substitutes have found use in areas such as paraphrasing, text simplification, machine translation, word sense disambiguation, and part of speech induction. However the computational complexity of accurately identifying the most likely substitutes for a word has made large scale experiments difficult. In this letter we introduce a new search algorithm, FASTSUBS, that is guaranteed to find the K most likely lexical substitutes for a given word in a sentence based on an n-gram language model. The computation is sublinear in both K and the vocabulary size V. An implementation of the algorithm and a dataset with the top 100 substitutes of each token in the WSJ section of the Penn Treebank are available at http://goo.gl/jzKH0.",
  "full_text": "arXiv:1205.5407v2  [cs.CL]  1 Sep 2012\nIEEE SIGNAL PROCESSING LETTERS, VOL. 0, NO. 0, JANUARY 0000 1\nFASTSUBS: An Efﬁcient and Exact Procedure for\nFinding the Most Likely Lexical Substitutes Based\non an N-gram Language Model\nDeniz Yuret\nAbstract—Lexical substitutes have found use in areas such\nas paraphrasing, text simpliﬁcation, machine translation, word\nsense disambiguation, and part of speech induction. However\nthe computational complexity of accurately identifying the most\nlikely substitutes for a word has made large scale experiments\ndifﬁcult. In this paper I introduce a new search algorithm,\nFASTSUBS , that isguaranteedto ﬁnd theK most likely lexical\nsubstitutes for a given word in a sentence based on an n-gram\nlanguage model. The computation is sub-linear in bothK and\nthe vocabulary sizeV . An implementation of the algorithm and\na dataset with the top 100 substitutes of each token in the WSJ\nsection of the Penn Treebank are available at http://goo.gl/jzKH0.\nEDICS Category: SPE-LANG\nI. INTRODUCTION\nLexical substitutes have proven useful in applications such\nas paraphrasing [1], text simpliﬁcation [2], and machine trans-\nlation [3]. Best published results in unsupervised word sense\ndisambiguation [4], and part of speech induction [5] represent\nword context as a vector of substitute probabilities. Usinga\nstatistical language model to ﬁnd the most likely substitutes of\na word in a given context is a successful approach ([6], [7]).\nHowever the computational cost of an exhaustive algorithm,\nwhich computes the probability of every word before deciding\nthe topK, makes large scale experiments difﬁcult. On the\nother hand, heuristic methods run the risk of missing important\nsubstitutes.\nThis paper presents theFASTSUBS algorithm which can\nefﬁciently and correctly identify the most likely lexical substi-\ntutes for a given context based on an n-gram language model\nwithout going through most of the vocabulary. Even though\nthe worst-case performance ofFASTSUBS is still proportional\nto vocabulary size, experiments demonstrate that the average\ncost is sub-linear in both the number of substitutesK and the\nvocabulary sizeV . To my knowledge, this is the ﬁrst sub-\nlinear algorithm that exactly identiﬁes the topK most likely\nlexical substitutes.\nThe efﬁciency ofFASTSUBS makes large scale experiments\nbased on lexical substitutes feasible. For example, it is possible\nto compute the top 100 substitutes for each one of the\n1,173,766 tokens in the WSJ section of the Penn Treebank\nCopyright (c) 2012 IEEE. Personal use of this material is permitted.\nHowever, permission to use this material for any other purposes must be\nobtained from the IEEE by sending a request to pubs-permissions@ieee.org.\nD. Yuret is with the Department of Computer Engineering, Koc¸ University,\n˙\nIstanbul, Turkey, e-mail:dyuret@ku.edu.tr.\n[8] in under 5 hours on a typical workstation. The same\ntask would take about 6 days with the exhaustive algorithm.\nThe Penn Treebank substitute data and an implementation\nof the algorithm are available from the author’s website at\nhttp://goo.gl/jzKH0.\nSection II derives substitute probabilities as deﬁned by an\nn-gram language model with an arbitrary order and smoothing.\nSection III describes theFASTSUBS algorithm. Section IV\nproves the correctness of the algorithm and Section V presents\nexperimental results on its time complexity. Section VI sum-\nmarizes the contributions of this paper.\nII. SUBSTITUTE PROBABILITIES\nThis section presents the derivation of lexical substitute\nprobabilities based on an n-gram language model. Details of\nthis derivation are important in ﬁnding an admissible algorithm\nthat identiﬁes the most likely substitutes efﬁciently, without\ntrying out most of the vocabulary.\nN-gram language models assign probabilities to arbitrary\nsequences of words (or other tokens like punctuation etc.)\nbased on their occurrence statistics in large training corpora.\nThey approximate the probability of a sequence of words by\nassuming each word is conditionally independent of the rest\ngiven the previous(n−1) words. For example a trigram model\nwould approximate the probability of a sequenceabcde as:\np(abcde) =p(a)p(b|a)p(c|ab)p(d|bc)p(e|cd) (1)\nwhere lowercase letters likea, b, c represent words and\nstrings of letters likeabcde represent word sequences. The\ncomputation is typically performed using log probabilities,\nwhich turns the product into a summation:\nℓ(abcde) =ℓ(a) +ℓ(b|a) +ℓ(c|ab) +ℓ(d|bc) +ℓ(e|cd) (2)\nwhere ℓ(x) ≡ log p(x). The individual conditional probability\nterms are typically expressed in back-off form:1\nℓ(c|ab) =\n{ α(abc) iff(abc) > 0\nβ(ab) +ℓ(c|b) otherwise (3)\nwhere α(abc) is the discounted log probability estimate for\nℓ(c|ab) (typically slightly less than the log frequency in the\ntraining corpus),f(abc) is the number of timesabc has been\nobserved in the training corpus,β(ab) is the back-off weight\n1Even interpolated models can be represented in the back-offform and in\nfact that is the way SRILM stores them in ARPA (Doug Paul) format model\nﬁles.\n2 IEEE SIGNAL PROCESSING LETTERS, VOL. 0, NO. 0, JANUARY 0000\nto keep the probabilities add up to 1. The formula can be\ngeneralized to arbitrary n-gram orders if we letb stand for\nzero or more words. The recursion bottoms out at unigrams\n(single words) whereℓ(c) = α(c). If there are any out-of-\nvocabulary words we assume they are mapped to a special\n⟨UNK ⟩ token, soα(c) is never undeﬁned.\nIt is best to use both left and right context when estimating\nthe probabilities for potential lexical substitutes. For example,\nin“He lived in San Francisco suburbs. ”, the tokenSan would\nbe difﬁcult to guess from the left context but it is almost certain\nlooking at the right context. The log probability of a substitute\nword given both left and right contexts can be estimated as:\nℓ(x|ab\nde) ∝ ℓ(abxde) (4)\n∝ ℓ(x|ab) +ℓ(d|bx) +ℓ(e|xd)\nHere the “” symbol represents the position the candidate\nsubstitutex is going to occupy. The ﬁrst line follows from\nthe deﬁnition of conditional probability and the second line\ncomes from Equation 1 except the terms that do not include\nthe candidatex have been dropped.\nThe expression for the unnormalized log probability of a\nlexical substitute according to Equation 4 and the decompo-\nsition of its terms according to Equation 3 can be combined\nto give us Equation 5. For arbitrary order n-gram models we\nwould end up with a sum ofn terms and each term would\ncome from one ofn alternatives.\nℓ(x|ab\nde) ∝ (5)\n\n\n\nα(abx) iff(abx) > 0\nβ(ab) +α(bx) iff(bx) > 0\nβ(ab) +β(b) +α(x) otherwise\n+\n\n\n\nα(bxd) iff(bxd) > 0\nβ(bx) +α(xd) iff(xd) > 0\nβ(bx) +β(x) +α(d) otherwise\n+\n\n\n\nα(xde) iff(xde) > 0\nβ(xd) +α(de) iff(de) > 0\nβ(xd) +β(d) +α(e) otherwise\nIII. ALGORITHM\nThe task ofFASTSUBS is to pick the topK substitutes (x)\nfrom a vocabulary of sizeV that maximize Equation 5 for a\ngiven contextab\nde. Equation 5 forms a tree where leaf nodes\nare primitive terms such asβ(bx), α(xd), and parent nodes\nare compound terms, i.e. sums or conditional expressions. The\nbasic strategy is to construct a priority queue of candidate\nsubstitutes for Equation 5 by composing substitute queues\nfor each of its sub-expressions. The structure of these queues\nand how they can be composed is described next, followed\nby the construction of the individual queues for each of the\nsubexpressions.\nA. Upper bound queues\nA sum such asβ(bx)+ α(xd) is not necessarily maximized\nby the x’s that maximize either of its terms. What we can\nsay for sure is that the sum for anyx cannot exceed the\nupper bound β(bx1) + α(x2d) where x1 maximizes β(bx)\nand x2 maximizes α(xd). We can ﬁnd thex that maximizes\nthe sum by repeatedly evaluating candidates until we ﬁnd one\nwhose value is (i) larger than all the candidates that have\nbeen evaluated, and (ii) larger than the upper bound for the\nremaining candidates.\nBased on this intuition, we deﬁne an abstract data type\ncalled anupper bound queuethat maintains anupper bound\non the actual values of its elements. Each successivepop\nfrom an upper bound queue is not guaranteed to retrieve the\nelement with the largest value, but the remaining elements\nare guaranteed to have values smaller than or equal to a non-\nincreasing upper bound. An upper bound queue supports three\noperations:\n• SUP (q): returns an upper bound on the value of the\nelements in the queue.\n• TOP (q): returns the top element in the queue. Note that\nthis element is not guaranteed to have the highest value.\n• POP (q): extracts and returns the top element in the queue\nand updates the upper bound if possible.\nUpper bound queues can be composed easily. Going back\nto our sum example let us assume that we have valid upper\nbound queuesqα forα(xd) and qβ forβ(bx). The queueqσ for\nthe sum(β(bx) +α(xd)) has SUP (qσ ) =SUP (qα ) +SUP (qβ )\nbecause the upper bound for a sum clearly cannot exceed the\ntotal of the upper bounds for its constituent terms.TOP (qσ )\ncan return any element from the queue without violating the\ncontract. However in order to ﬁnd the true maximum, we\neventually need an element whose value exceeds the upper\nbound for the remaining elements. Thus we can bias our\nchoice forTOP (qσ ) to prefer elements that (i) have high values,\nand (ii) reduce the upper bound quickly. In practice non-\ndeterministically pickingTOP (qσ ) to be one ofTOP (qα ) or\nTOP (qβ ) works well.POP (qσ ) can extract and return the same\nelement from the corresponding child queue. If the upper\nbound of a child queue drops as a result, so does the upper\nbound of the compound queueqσ .\nB. Top level queue\nThe top level sum in Equation 5 is a sum ofN conditional\nexpressions for an orderN language model. We can construct\nan upper bound queue for the sum using the upper bound\nqueues for its constituent terms as described in the previous\nsection. Letq represent the queue for the top level sum,\nδ ∈ C represent the constituent conditional expressions and\nqδ represent their associated queues.\nSUP (q) =\n∑\nδ∈ C\nSUP (qδ ) (6)\nTOP (q) = TOP (qδ ) for a randomδ.\nFor TOP (q) we non-deterministically pick the top element from\none of the children andPOP (q) extracts and returns that same\nelement adjusting the upper bound if necessary.\nAs mentioned beforeTOP (q) does not necessarily return\nthe element with the maximum value. In order to ﬁnd the top\nK elementsFASTSUBS keeps popping elements fromq and\nYURET: FASTSUBS 3\ncomputes their true values according to Equation 5 until at\nleastK of them have values above the upper bound for the\nremaining elements in the queue. Table I gives the pseudo-\ncode forFASTSUBS .\nFASTSUBS (S,K)\n1) Initialize upper bound queueq for contextS.\n2) Initialize set of candidate wordsX = {}.\n3) WHILE |{x : x ∈ X, ℓ(x|S) ≥ SUP (q)}| < K\nDO X := X ∪ { POP (q)}\n4) Return topK words inX based onℓ(x|S).\nTABLE I\nPSEUDO -CODE FOR FASTSUBS . GIVEN A WORD CONTEXT S AND THE\nDESIRED NUMBER OF SUBSTITUTES K,FASTSUBS RETURNS THE SET OF\nTOP K WORDS THAT MAXIMIZE ℓ(x|S).\nThis procedure will return the correct result as long as\nPOP (q) cycles through all the words in the vocabulary and the\nupper bound for the remaining elements,SUP (q), is accurate.\nThe loop can in fact cycle through all the words in the\nvocabulary because at least one of the subexpressions,α(x), is\nwell deﬁned for every word. The accuracy ofSUP (q) depends\non the accuracy of the upper bounds for constituent terms,\nwhich are described next.\nC. Queues for conditional expressions\nConditional expressions indicated by “{” in Equation 5 pick\ntheir topmost child whoseα argument has been observed\nin the training corpus. Letqδ be the queue for such a\nconditional expression andσ ∈ Cδ be its children terms. Let\nσmax = arg maxσ ∈ Cδ SUP (qσ ) be the child whose queue has\nthe maximum upper bound. The upper bound forqδ cannot\nexceed the upper bound forqσ max because the value of the\nconditional expression for any givenx is equal to the value of\none of its children. Thus we deﬁne the queue operations for\nconditional expressions based onqσ max :\nSUP (qδ ) = SUP (qσ max ) (7)\nTOP (qδ ) = TOP (qσ max )\nD. Queues for sums of primitive terms\nAs described in Section III-A, the upper bound of a queue\nfor a sum likeβ(bx) +α(xd) is equal to the sum of the upper\nbounds of the constituent queues. It turns out that for sums of\nprimitive terms, only theα term that has the candidate wordx\nas an argument has a non-constant upper-bound. The language\nmodel deﬁnesβ to be 0 for any word sequence that does not\nappear in the training set. Therefore theβ terms that have the\ncandidate wordx as an argument always have the upper bound\n0. Finally, theα and β terms without the candidate wordx\nact as constants.\nFor notational consistency we deﬁne upper bounds for the\nconstant terms as well. LetA and B represent sequences of\nzero or more words that do not include the candidatex. We\nhave:\nSUP (qα (A)) = α(A) (8)\nSUP (qβ (B)) = β(B)\nFor β terms withx in their argument, many words from the\nvocabulary would be unobserved in the argument sequence\nand share the maximumβ value of 0. In the rare case that\nall vocabulary words have been observed in the argument\nsequence, they would each have negativeβ values and 0 would\nstill be a valid upper bound. ThusFASTSUBS uses the constant\n0 as an upper bound forβ terms withx.\nSUP (qβ (AxB)) = 0 (9)\nOnly theα term with anx argument has an upper bound\nqueue as described in the next section.FASTSUBS picks the\ntop element for a sum of primitive terms only from itsα\nconstituent.2 Let qσ be the queue for a sum of primitive terms\nand letγ ∈ Cσ indicate its constituents (α, β, constant or\notherwise). We have:\nSUP (qσ ) =\n∑\nγ ∈ Cσ\nSUP (qγ ) (10)\nTOP (qσ ) =\n{\nTOP (qα ) if theα term has anx argument.\nUNDEF otherwise.\nE. Queues for primitive terms\nFASTSUBS pre-computes actual priority queues (which sat-\nisfy the upper bound queue contract) forα terms that include\nx in their argument:\nSUP (qα (AxB)) = max\nx\nα(AxB) (11)\nTOP (qα (AxB)) = arg max\nx\nα(AxB)\nHere A and B stand for zero or more words andx is\na candidate lexical substitute word.SUP (qα ) gives the real\nmaximum, thus provides a tight upper bound.TOP (qα ) is\nguaranteed to return the element with the highest value.\nThe qα queues are constructed once in the beginning of the\nprogram as sorted arrays and re-used in queries for different\ncontexts. The construction can be performed in one pass\nthrough the language model and the memory requirement is of\nthe same order as the size of the language model. Candidates\nthat have not been observed in the argument context will\nbe at the bottom of this queue becauseα(AxB) ≡ −∞ if\nf(AxB) = 0. To save memory suchx are not placed in the\nqueue. Thus after we run out of elements inqα the queue\nreturns:\nSUP (qα (AxB)) = −∞ (12)\nTOP (qα (AxB)) = UNDEF\nIV. CORRECTNESS\nAs mentioned in Section III, the correctness of the algorithm\ndepends on two factors: (i) theSUP (q) function should return\nan upper bound on the remaining values inq, and (ii) the\nPOP (q) function should cycle through the whole vocabulary\nfor the top level queue.\nThe correctness of theSUP (q) function can be proved\nrecursively. For primitive termsSUP (q) is equal to the actual\n2 Remember that the top value in an upper bound queue is not guaranteed\nto have the largest value. Thus ignoring theβ terms does not effect the\ncorrectness of the algorithm.\n4 IEEE SIGNAL PROCESSING LETTERS, VOL. 0, NO. 0, JANUARY 0000\nmaximum (e.g. forqα ), or is an obvious upper bound (e.g.\nSUP (qβ (AxB)) = 0). For sums,SUP (q) is equal to the sum\nof the upper bounds for the children and, for conditional\nexpressions,SUP (q) is equal to the maximum of the upper\nbounds for the children.\nTo prove thatPOP (q) will cycle through the entire vocabu-\nlary it sufﬁces to show that the queue for at least one child of\nq will cycle through the entire vocabulary. This is in fact the\ncase because one of the children will always include the term\nα(x) whose queue contains the entire vocabulary.\nV. C OMPLEXITY\nA exhaustive algorithm to ﬁnd the most likely substitutes\nin a given context could try each word in the vocabulary as a\npotential substitutex and compute the value of the expression\ngiven in Equation 5. The computation of Equation 5 requires\nO(N2) operations for an orderN language model, which we\nwill assume to be a constant. If we haveV words in our\nvocabulary the cost of the exhaustive algorithm to ﬁnd a single\nmost likely substitute would beO(V ).\nIn order to quantify the efﬁciency ofFASTSUBS on a real\nworld dataset, I used a corpus of 126 million words of WSJ\ndata as the training set and the WSJ section of the Penn\nTreebank [8] as the test set. Several 4-gram language models\nwere built from the training set using Kneser-Ney smoothing\nin SRILM [9] with vocabulary sizes ranging from 16K to\n512K words. The average number ofPOP (q) operations for\nthe top level upper bound queue was measured for number\nof substitutesK ranging from 1 to 16K. Figure 1 shows the\nresults.\n 10\n 100\n 1000\n 10000\n 100000\n 1  10  100  1000  10000  100000\nnumber of iterations\nnumber of substitutes (K)\nFig. 1. Number of iterations as a function of the number of substitutesK\nand the vocabulary sizeV . The solid curves represent results with vocabulary\nsizes from 16K to 512K. The horizontal dotted line gives the cost of the\nexhaustive algorithm forV = 64K. The diagonal dotted line is a functional\napproximation in the formKλ V (1− λ ) forV = 64K and λ = 0.5878.\nThe time cost ofFASTSUBS depends on the number of\niterations of the while loop in Table I which in turn depends\non the quality of words returned byPOP (q) and the tightness\nof the upper bound given bySUP (q). The worst case is no\nbetter than the exhaustive algorithm’sO(V ). However Figure 1\nshows that the average performance ofFASTSUBS on real data\nis signiﬁcantly better whenK ≪ V . The number ofPOP (q)\noperations in the while loop to get the topK substitutes is\nsub-linear inK (the slope of the log-log curves are around\n0.5878) and approaches the vocabulary sizeV asK → V . The\neffect of vocabulary size is practically insigniﬁcant: increasing\nvocabulary size from 16K to 512K less than doubles the\naverage number of steps for a givenK.\nAs a practical example, it is possible to compute the top\n100 substitutes for each one of the 1,173,766 tokens in Penn\nTreebank with a vocabulary size of 64K in under 5 hours on\na typical 2012 workstation.3 The same task would take about\n6 days for the exhaustive algorithm.\nVI. C ONTRIBUTIONS\nFinding likely lexical substitutes has a range of applications\nin natural language processing. In this paper we introducedan\nexact and efﬁcient algorithm,FASTSUBS , that is guaranteed\nto ﬁnd theK most likely substitutes for a given word context\nfrom a V word vocabulary. Its average runtime is sub-linear\nin bothV and K giving a signiﬁcant improvement over an\nexhaustiveO(V ) algorithm whenK ≪ V . An implementation\nof the algorithm and a dataset with the top 100 substitutes\nof each token in the WSJ section of the Penn Treebank are\navailable at http://goo.gl/jzKH0.\nA CKNOWLEDGMENTS\nI would like to thank the members of the Natural Language\nGroup at USC/ISI for their hospitality and for convincing me\nthat aFASTSUBS algorithm is possible.\nR EFERENCES\n[1] D. McCarthy and R. Navigli, “Semeval-2007 task 10: English lexical\nsubstitution task,” inProceedings of the 4th International Workshop on\nSemantic Evaluations (SemEval-2007), 2007, pp. 48–53.\n[2] L. Specia, S. Jauhar, and R. Mihalcea, “Semeval-2012 task 1: English\nlexical simpliﬁcation,” inProceedings of the International Workshop on\nSemantic Evaluation, 2012, forthcoming.\n[3] R. Mihalcea, R. Sinha, and D. McCarthy, “Semeval-2010 task 2: Cross-\nlingual lexical substitution,” inProceedings of the 5th International\nWorkshop on Semantic Evaluation. Association for Computational\nLinguistics, 2010, pp. 9–14.\n[4] D. Yuret and M. A. Yatbaz, “The noisy channel model for\nunsupervised word sense disambiguation,”Computational Linguistics,\nvol. 36, no. 1, pp. 111–127, March 2010. [Online]. Available:\nhttp://www.aclweb.org/anthology-new/J/J10/J10-1004.pdf\n[5] M. A. Yatbaz, E. Sert, and D. Yuret, “Learning syntactic categories using\nparadigmatic representations of word context,” inProceedings of the\n2012 Conference on Empirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics, July 2012.\n[6] T. Hawker, “Usyd: Wsd and lexical substitution using theweb1t corpus,”\nin SemEval-2007: 4th International Workshop on Semantic Evaluations,\n2007. [Online]. Available: /ref/hawker/98.pdf\n[7] D. Yuret, “Ku: Word sense disambiguation by substitution,” inPro-\nceedings of the 4th International Workshop on Semantic Evaluations.\nAssociation for Computational Linguistics, 2007, pp. 207–213.\n[8] M. P. Marcus, B. Santorini, M. A. Marcinkiewicz, and A. Taylor,\nTreebank-3. Philadelphia: Linguistic Data Consortium, 1999.\n[9] A. Stolcke, “Srilm – an extensible language modeling toolkit,” inSeventh\nInternational Conference on Spoken Language Processing, 2002.\n3 Running a single thread on an Intel Xeon E7-4850 2GHz processor.",
  "topic": "Treebank",
  "concepts": [
    {
      "name": "Treebank",
      "score": 0.8938288688659668
    },
    {
      "name": "Computer science",
      "score": 0.7817163467407227
    },
    {
      "name": "n-gram",
      "score": 0.7061431407928467
    },
    {
      "name": "Language model",
      "score": 0.7041913270950317
    },
    {
      "name": "Natural language processing",
      "score": 0.6814090609550476
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6303267478942871
    },
    {
      "name": "Vocabulary",
      "score": 0.6061314940452576
    },
    {
      "name": "Word (group theory)",
      "score": 0.5885818004608154
    },
    {
      "name": "Security token",
      "score": 0.5018889904022217
    },
    {
      "name": "Computation",
      "score": 0.4982321262359619
    },
    {
      "name": "Machine translation",
      "score": 0.49549901485443115
    },
    {
      "name": "Text simplification",
      "score": 0.4705560505390167
    },
    {
      "name": "Sentence",
      "score": 0.44782721996307373
    },
    {
      "name": "Speech recognition",
      "score": 0.35705506801605225
    },
    {
      "name": "Algorithm",
      "score": 0.20938313007354736
    },
    {
      "name": "Linguistics",
      "score": 0.19193586707115173
    },
    {
      "name": "Parsing",
      "score": 0.08520939946174622
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1351752",
      "name": "Koç University",
      "country": "TR"
    }
  ]
}