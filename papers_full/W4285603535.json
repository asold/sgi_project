{
  "title": "AggPose: Deep Aggregation Vision Transformer for Infant Pose Estimation",
  "url": "https://openalex.org/W4285603535",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2102363819",
      "name": "Xu Cao",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2177510325",
      "name": "Xiaoye Li",
      "affiliations": [
        "Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2143387512",
      "name": "Liya Ma",
      "affiliations": [
        "Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2109153795",
      "name": "Yi Huang",
      "affiliations": [
        "Shenzhen Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2098467652",
      "name": "Feng Xuan",
      "affiliations": [
        "Shenzhen Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2133442939",
      "name": "Zening Chen",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2571521857",
      "name": "Hongwu Zeng",
      "affiliations": [
        "Shenzhen Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2095935896",
      "name": "Jianguo Cao",
      "affiliations": [
        "Shenzhen Children's Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4214508443",
    "https://openalex.org/W3160605389",
    "https://openalex.org/W3034399482",
    "https://openalex.org/W2903831537",
    "https://openalex.org/W3203925315",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W3034750257",
    "https://openalex.org/W3162547991",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2972362249",
    "https://openalex.org/W4312726009",
    "https://openalex.org/W4309442969",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3206810688",
    "https://openalex.org/W2913645120",
    "https://openalex.org/W3092789429",
    "https://openalex.org/W3034742259"
  ],
  "abstract": "Movement and pose assessment of newborns lets experienced pediatricians predict neurodevelopmental disorders, allowing early intervention for related diseases. However, most of the newest AI approaches for human pose estimation methods focus on adults, lacking publicly benchmark for infant pose estimation. In this paper, we fill this gap by proposing infant pose dataset and Deep Aggregation Vision Transformer for human pose estimation, which introduces a fast trained full transformer framework without using convolution operations to extract features in the early stages. It generalizes Transformer + MLP to high-resolution deep layer aggregation within feature maps, thus enabling information fusion between different vision levels. We pre-train AggPose on COCO pose dataset and apply it on our newly released large-scale infant pose estimation dataset. The results show that AggPose could effectively learn the multi-scale features among different resolutions and significantly improve the performance of infant pose estimation. We show that AggPose outperforms hybrid model HRFormer and TokenPose in the infant pose estimation dataset. Moreover, our AggPose outperforms HRFormer by 0.8 AP on COCO val pose estimation on average. Our code is available at github.com/SZAR-LAB/AggPose.",
  "full_text": "AggPose: Deep Aggregation Vision Transformer for Infant Pose Estimation\nXu Cao1,4∗ , Xiaoye Li1,2† , Liya Ma1,2 , Yi Huang1,3 , Xuan Feng1,3 , Zening Chen4 ,\nHongwu Zeng3 and Jianguo Cao1,3‡\n1Shenzhen Automatic Rehabilitation Laboratory\n2Shenzhen Baoan Women’s and Childiren’s Hospital, Jinan University\n3Shenzhen Children’s Hospital\n4New York University\nxc2057@nyu.edu, caojgsz@126.com\nAbstract\nMovement and pose assessment of newborns lets\nexperienced pediatricians predict neurodevelop-\nmental disorders, allowing early intervention for\nrelated diseases. However, most of the newest AI\napproaches for human pose estimation methods fo-\ncus on adults, lacking publicly benchmark for in-\nfant pose estimation. In this paper, we fill this\ngap by proposing infant pose dataset and Deep Ag-\ngregation Vision Transformer for human pose esti-\nmation, which introduces a fast trained full trans-\nformer framework without using convolution op-\nerations to extract features in the early stages. It\ngeneralizes Transformer + MLP to high-resolution\ndeep layer aggregation within feature maps, thus\nenabling information fusion between different vi-\nsion levels. We pre-train AggPose on COCO\npose dataset and apply it on our newly released\nlarge-scale infant pose estimation dataset. The re-\nsults show that AggPose could effectively learn\nthe multi-scale features among different resolutions\nand significantly improve the performance of in-\nfant pose estimation. We show that AggPose out-\nperforms hybrid model HRFormer and TokenPose\nin the infant pose estimation dataset. Moreover,\nour AggPose outperforms HRFormer by 0.8 AP on\nCOCO val pose estimation on average. Our code is\navailable at github.com/SZAR-LAB/AggPose.\n1 Introduction\nEach year, approximately 5 million newborns around the\nworld are suffering from neurodevelopmental disorder. Due\nto the lack of early diagnosis and intervention, many infants\nare severely disabled and abandoned by their parents, espe-\ncially in countries with limited numbers of pediatricians with\nextensive experience in neurodevelopmental disorders. This\nhas become a conundrum that plagues many families around\nthe world.\n∗project lead\n†contributed equally to the first-author\n‡corresponding author\nRecent developments in deep learning based approaches\nopen possibilities for developing computer-aid movement as-\nsessment tools in early intervention for neurodevelopmental\ndisorder. One of the most predictive tools for early cerebral\npalsy diagnosis is general movements assessment (GMA), as\nit needs to discriminate fidgety from non-fidgety movements\nin many small-amplitude movements [Silva et al., 2021 ],\nwhere computers are more sensitive to detect such move-\nments. Researchers used human pose estimation methods\nlike OpenPose [Cao et al., 2019 ] to capture infant pose and\nthen generate infant motion sequence to detect cerebral palsy.\nCompared with manual GMA detection, computer-based ap-\nproaches are much faster with low cost. However, this task\nis challenging in real applications considering complex sce-\nnarios for infant pose and there is a lack of large-scale public\ninfant pose datasets around the world. Besides, the 17 adult\nkeypoints defined by the COCO dataset do not support infant\nmovement detection well due to the lack of clinical signifi-\ncance and actionability.\nAnother problem is the performance of the pose estimation\nmethods. Although CNN-based methods have pushed human\npose estimation to a new level thanks to the intense repre-\nsentation learning and semantics understanding ability, it is\nstill not performing well to understand global constraint rela-\ntionships between body parts [Li et al., 2021 ]. Researchers\ncombined Vision Transformer with CNN into hybrid models\nto address this issue, let the ViT expand the receptive field,\nand enhance the model’s ability to capture constraint rela-\ntionships between body parts. Among recent advancements,\nthe local-window self-attention structure from Swin Trans-\nformer [Liu et al., 2021 ], and Mix Feed Forward Network\n(Mix-FNN) from SegFormer [Xie et al., 2021] showed great\npotential in the direction of multi-scale feature representation\nlearning [Gu et al., 2021].\nHowever, some issues still make it challenging to ap-\nply Transformer for human pose estimation: (1) The first\nstages of the hybrid models highly rely on the pretrained HR-\nNet convolutional layers, which can not utilize large-scale\nunlabeled data with newest self-supervised masked autoen-\ncoder [He et al., 2021]; (2) Hard to converge during the train-\ning process; (3) Models are challenging to transfer from one\ndomain to another domain.\nIn this paper, we propose AggPose, a generalization of\nmulti-scale transformer architecture to the deep aggregation\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5045\nnetwork. Different from HRFormer, AggPose does not use\nconvolutional layers for initial feature extractor and fusion\nmodules. Instead, it uses layer-by-layer Mix Transformers\nand a cross-resolution MLP fusion module. The Transformer\nreceives input from the former layer, applies self-attention\noperation and Mix-FNN, and sends the message to the next\nlayer. The MLP fusion module integrates richer spatial infor-\nmation from different resolution levels and sends the result\nto the next stage. We conduct experiments on COCO human\npose estimation dataset and then fine-tune the model on our\nproposed large-scale labeled infant pose estimation dataset.\nAggPose achieves competitive performance on both bench-\nmarks. For example, AggPose-L gains 0.8 AP and 0.6 AP\nover HRFormer and TokenPose on COCO val set. AggPose-\nL’s robustness and fast convergence make it easy to transfer\nfrom the COCO dataset to our infant pose dataset and achieve\nthe highest 95.0 AP.\nThe contributions are summarized as follows:\n• We propose a new Transformer + MLP based aggrega-\ntion architecture without using HRNet’s CNN backbone\nand CNN-based multi-scale fusion modules.\n• To enhance the efficiency of deep layer aggregation, we\ndesign a special deep aggregation MLP structure to fuse\ninformation across different resolutions.\n• To facilitate research in early intervention for neurode-\nvelopmental disorders, we present a large-scale infant\nchallenging dataset including 20,748 pose labeled im-\nages. Experimental results show our framework’s ro-\nbustness in both COCO and this new dataset. To the best\nof our knowledge, this is the largest dataset constructed\nfor infant pose estimation for clinical application.\n2 Related Works\n2.1 Infant Pose Estimation\nInfant pose estimation has been found to have high applica-\ntion value in clinical research. Most commonly used cere-\nbral palsy assessment tools such as GMA and CPVC [Ab-\nbruzzese et al., 2020 ] are already using automatic pose es-\ntimation methods to aid diagnosis. However, most existing\nalgorithms are based on traditional machine learning meth-\nods for automatic infant pose estimation, limiting their ca-\npability to deal with complex conditions [Silva et al., 2021].\nMeanwhile, there are very few attempts initiated by the arti-\nficial intelligence community to handle infant images. Only\n[McCay et al., 2019; Reich et al., 2021 ] gave primacy at-\ntempts to adopt OpenPose [Cao et al., 2019 ] to extract key-\npoints for infants but lack a large and general dataset. MINI-\nRGBD [Hesse et al., 2018 ] is the most famous open-source\ndataset in this field, where it only contains 700 authentic in-\nfant images and a small set of synthesized infant images. All\nof these make automatic infant pose assessment methods un-\nreliable in the real world clinical systems.\n2.2 Vision Transformers for Pose Estimation\nFor many years, deep convolutional neural networks have\nbeen applied to human pose estimation. Among all CNN-\nbased pose estimation algorithms, the schemes that main-\ntain high-resolution representations throughout the network\n(a)\n (b)\nFigure 1: (a) Examples for our proposed InfantPose dataset. (b) 21\ninfant body key-points\nachieved great success. The most representative models are\nHRNet [Wang et al., 2020 ], HigherHRNet [Cheng et al.,\n2020], UDP [Huang et al., 2020], DARK[Zhang et al., 2020].\nHowever, it is still tricky for CNNs to capture constraint rela-\ntionships between human keypoints, as CNN’s receptive field\nrestrict its ability to understand global spatial relationships.\nRecent several works have introduced Transformer for hu-\nman pose estimation [Yuan et al., 2021; Yang et al., 2021;\nLi et al., 2021]. TokenPose [Li et al., 2021] introduced Trans-\nformer with representing key-points as token embeddings for\nhuman Pose estimation. HRFormer [Yuan et al., 2021 ] in-\ntegrated HRNet with Swin Transformer [Liu et al., 2021 ],\nwhich makes full use of multi-resolution parallel informa-\ntion over different non-overlapping image windows. How-\never, both HRFormer and TokenPose did not discard convo-\nlution operations to obtain initial features, as the first stage\nof HRFormer and TokenPose were fine-tuned on HRNet’s\nCNN backbone. In this work, we propose Aggregation Vi-\nsion Transformers (A ViT), which provides a different way to\nsolve the low-resolution problem of ViT and replace convolu-\ntion operations with overlapping patch embedding to extract\nfeatures in the early stages.\n3 Proposed Method\nOur goal is to propose a new infant pose dataset and build\na new benchmark that can fast extract infant pose via vision\ntransformers. Figure 2 shows the pipeline of the model. Our\ninfant pose estimation research have passed the ethics checks\nof Shenzhen Baoan Women’s and Childiren’s Hospital.\n3.1 Infant Pose Detection Dataset\nIn this paper, we present a large-scale challenging dataset for\nnewborn pose extraction and detection. It can be applied to\npredict infant movement sequence and design automatic clin-\nical tools like automatic GMA. Despite the importance and\ndifficulty of infant pose detection, existing datasets are either\ntoo small or too simple, and a large public annotated bench-\nmark is needed to compare different methods. Besides, none\nof these datasets proposed suitable keypoints annotation for\ninfant images, as they adopt the COCO’s 17 keypoints for-\nmat, while it loses many significant refined pose and move-\nment features for the infant.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5046\nDataset Videos Labeled Images Unlabeled\nMINI RGBD 12 700 -\nCOCO (infant) 0 1904 -\nSyRIP 0 1700 -\nOurs 5187 20748 15 million\nTable 1: Comparison between other infant pose dataset.\nInspired by [Silva et al., 2021; Huang et al., 2021 ], we\npublish our new open-source infant pose dataset and new in-\nfant keypoints format. To collect data, we adopt GMA de-\nvices to record infant movement videos from 2013 to now.\nMore than 216 hours of videos were collected, and 15 mil-\nlion frames were extracted. Both the size and the scala-\nbility of our dataset are much better than the MINI-RGBD\ndataset [Hesse et al., 2018 ]. We randomly sampled 20,748\nframes from the videos and let professional clinicians an-\nnotate infant keypoints. Then, we divided the dataset into\n11,756 for the training set and validation set, 8,992 for the\ntest set. The 21 keypoints format for infant pose is proposed\nby experienced clinicians who have researched neurodevelop-\nmental disorders over 30 years.Figure 1 shows some exam-\nples, considering clinical application requirements and pro-\ntection of patient’s privacy, our dataset reduces keypoints on\ninfants’ heads and comprises more refined body keypoints\nlike fingers, toes, and navel. For public version, we will re-\nformat the dataset to solve ethical issues: all infants’ heads\nwill be covered with mosaics in the final published keypoint\ndataset to preserve the patients’ privacy. Commercial usage\nof infant pose dataset is prohibited.\nIn this paper, we focus on the human/infant supine position\npose detection, which is the most straightforward application\nfor the new presented dataset. However, this dataset can also\nbe used in other clinical fields, as it contains over 200 hours\nof infant movement sequence and has a high relationship with\nthe automated prediction of cerebral palsy and other neurode-\nvelopmental disorders. We hope applying our dataset and Ag-\ngPose to early diagnosis and intervene disorders, promoting\nwell-being for all at all ages, especially the children. In the\nfuture, we will also release more than 200 hours of new in-\nfant pose sequences generated from AggPose, and associated\nGMA labels. The retrospective study was approved by our\ninstitutional review board.\n3.2 Deep Aggregation Vision Transformers\nOverlapped Patch Embedding\nEarly convolutions were considered practical tools to extract\nlow-level features for hybrid transformer architectures. It\nis due to that transformers in the early stage treat the in-\nput as 1D vectors and exclusively focus on modeling the\nglobal context, which lose detailed localization information.\nHRNet and its pre-trained CNN parameters are the corner-\nstones of almost all the latest models for human pose estima-\ntion. Inspired by SegFormer [Xie et al., 2021], we adopt full\nTransformer with Overlapped Patch Embedding to replace\nHRNet’s CNN feature extractor and down-sampling stem of\neach stage. Compared with the early convolutions in HR-\nNet, HRFormer, and TokenPose, Overlapped Patch Embed-\nFeatures level Stage 1 Stage 2 Stage 3 Stage 4\n1/4 3 3 3 3\n1/8 6 3 3\n1/16 40 3\n1/32 3\nTable 2: The number of Transformer layers for each stage.\nding can obtain better low-level features, enhancing the high-\nresolution Transformer’s feature representation, and reduce\ncomputation complexity.\nAggregation Vision Transformers (A ViTs) Architecture\nWe follow the transformer module design from Mix Trans-\nformer [Xie et al., 2021 ] and start from high-resolution fea-\nture maps generated by the overlapped patch embedding with\npatch size = 7, stride = 4, and padding size = 3 as the first\nstage. Then, we add high-to-low resolution streams one by\none via overlapped patch embedding. We use multiple multi-\nhead self-attention blocks for each resolution stream to up-\ndate feature representation. To construct different depth of\nmodels, we propose small (AggPose-S), and large (AggPose-\nL) model, respectively. Table 2 shows the number of trans-\nformer layers for each stage in AggPose-L.\nCompared with Swin Transformer and HRFormer, we do\nnot use local-window self-attention to augment local infor-\nmation understanding considering the usage of overlapped\npatches. Instead, we use the sequence reduction process re-\nfer to [Xie et al., 2021 ] and [Wang et al., 2021 ], which sig-\nnificantly reduces the amount of calculation inside the trans-\nformer and accelerates the convergence process during model\ntraining. For each Transformer block, the self-attention is es-\ntimated as:\nK = Linear(γC, C)(K.Reshape(N\nγ , γC)) (1)\nAttention(Q, K, V) =Softmax ( QKT\n√dhead\n)V (2)\n,where K is the token representation with initial shapeN ×\nC. γ is the reduction ratio that decrease the dimension of K\nfrom N ×C to N/γ ×C.\nMLP Cross-Layer Aggregation\nBoth ViT and Swin Transformer uses positional embedding\nto introduce the location information across layers. How-\never, the resolution of positional embedding is fixed. For\nthe local-window Transformer, there is lacking information\nexchange across the windows. Thus, both SegFormer and\nHRFormer introduced 3 ×3 depth-wise convolution into the\nfeed-forward network (FFN) to expand the receptive field and\nreduce the harmful effect caused by positional embedding.\nThe FFN with depth-wise convolution (HRFormer) and Mix-\nFFN (SegFormer) used a very similar calculation:\ny = MLP (Activation(DW Conv(MLP (x)))) +x (3)\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5047\nFigure 2: The proposed AggPose architecture. Each module consists of multiple successive Mix Transformer blocks. Features across different\nresolutions are connected by MLP layer (blue square in the figure).\nwhere DW Convis a 3 ×3 depth-wise convolution opera-\ntion.\nIn AggPose, we expand the usage of Mix-FFN into the\ndeep aggregation approach across different resolution lay-\ners. For deep aggregation in CNN such as CAggNet [Cao\nand Lin, 2021 ] and HRNet, the aggregation begins at the\nshallowest, high resolution layer and then iteratively merges\ndeeper, low resolution layer. In this way, shallow features\nare refined as they are propagated through different stages\nof aggregation. Related research showed that deep aggre-\ngation structure propagates the aggregation of all resolutions\ninstead of the preceding block alone to better preserve fea-\ntures. It is widely used for semantic segmentation tasks and\nto achieve competitive performance. In our work, the pro-\nposed cross-layer aggregation module consists of two main\nsteps for each resolution level. First, multi-level features from\ndifferent resolutions go through a mixed feed-forward net-\nwork with 3 ×3 depth-wise convolution to unify the channel\ndimension and upsample or downsample (overlapped patch\nembedding) the feature map to the same shape. Then, we con-\ncatenate the feature vector from adjacent levels together and\nadopt an additional FFN layer to fuse the cross-layer informa-\ntion. Compared to convolutional multi-scale fusion modules\nin HRFormer and HRNet, MLP fusion modules accelerate\nconvergence while improving model performance.\nxi,j =\n( OverlappedP Ei,j(F F N(xi)) i < j\nxi i = j\nUpsamplei,j(F F N(xi)) i > j\n(4)\nwhere xi,j is the input of aggregation MLP layer. xi de-\nnotes the feature map from adjacent resolution. The cross-\nlayer aggregation module is defined as\nxj = MixF F N(Concat(xj−1, xj, xj+1)) +xj (5)\nwhere MixF F Nrepresents the Mix feed forward block\nin formula (3).\n3.3 Analysis\nThere are two main benefits of AggPose and our large-scale\ninfant pose dataset over other CNN or hybrid CNN Trans-\nformer methods like HRFormer and TokenPose and other\nsmall dataset for infant pose estimation, which are concluded\nas follows.\n(1) Potential of using self-supervised learning. Recently,\nVision Transformers pre-trained with self-supervised learn-\ning have attracted much attention. MAE [He et al., 2021 ]\nconstruct an inpainting masked autoencoder task to learn rep-\nresentation from unlabeled data and fine-tuning the model on\nany supervised tasks. Their results prove that full transform-\ners can learn reasonable semantic from large-scale unlabeled\ndataset. As Table 1 shows, we have plenty of unlabeled infant\nmovement frames from 5,187 videos. All these data would\nbe helpful for pre-training transformer-based autoencoder via\nself-supervised learning.\n(2) Faster convergence. In HRFormer, the feature passing\nis achieved via cross-layer convolution operation, which, is\ndifficult to convergence. In our AggPose framework, mes-\nsages are propagated by MLP across different layers. It can\nbe viewed as a kind of modification to the deep layer aggrega-\ntion model. As our experiments will show, such message pass\nscheme achieves better results than hybrid CNN-Transformer\nbased methods.\n4 Experiment\n4.1 Model Variants\nConsidering that the training process of most Transformer-\nbased pose estimation models is complicated, we provide an\neffective training policy in this paper. First, we load the Mix\nTransformer [Xie et al., 2021] pre-trained on ImageNet, train-\ning Mix Transformer on the COCO keypoints training set.\nAfter the Mix Transformer converges, we load the parameter\nof Mix Transformer into each layer of AggPose. Then, we\nfixed the parameters of AggPose at different resolution levels\nlayer by layer and fine-tuned the model on COCO and infant\npose dataset.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5048\nMethod Input size Backbone GFLOPs AP AP 50 AP75 APM APL AR\nSimpleBaseline-Res152 256×192 - 15.7 72.0 89.3 79.8 68.7 78.9 77.8\nHRNet-W32 [Wang et al., 2020] 256×192 - 7.1 74.4 90.5 81.9 70.8 81.0 79.8\nHRNet-W48 [Wang et al., 2020] 256×192 - 16.0 75.1 90.6 82.2 71.5 81.8 80.4\nTransPose [Yang et al., 2021] 256×192 HRNet 21.8 75.8 90.1 82.1 71.9 82.8 80.8\nTokenPose-L/D24 [Li et al., 2021] 256×192 HRNet 11.0 75.8 90.3 82.5 72.3 82.7 80.9\nHRFormer-B [Yuan et al., 2021] 256×192 HRNet 12.2 75.6 90.8 82.8 71.7 82.6 80.8\nAggPose-S 256×192 MiT-B2 9.0 75.2 89.9 82.0 71.4 82.4 80.3\nAggPose-L 256×192 MiT-B5 15.0 76.4 90.6 82.9 72.7 83.4 81.3\nTable 3: Comparisons on the COCO validation set, provided with the same detected human boxes from HRNet.\nThe configuration details for the size of overlapped patch\nembedding and the number of transformer layers are pre-\nsented in Table 2. Note, Table 2 only provide the configu-\nration for AggPose-L, which uses MiT-B5 as the backbone.\nFor AggPose-S, we used MiT-B2 as backbone, the number of\ntransformer layers is [[3,3,3,3],[4,3,3],[6,3],[3]].\n4.2 Comparing with SOTA Methods\nDataset. We study the performance of AggPose on the\nCOCO human pose estimation dataset [Lin et al., 2014 ],\nwhich contains more than 250K person instances labeled with\n17 keypoints, and the new infant pose estimation dataset,\nwhich contains 20k infant instances labeled with 21 key-\npoints. MPII dataset is not used in our experiment due to\nits size (25K) is much smaller than COCO and has different\nkeypoints format.\nTraining setting. Following most of the default training\nand evaluation settings from HRNet and HRFormer, we\ntrained the models using AdamW optimizer and an initial\nvalue of 0.001 as the learning rate. For the training batch\nsize, we chose 32 due to limited GPU memory. The exper-\niment takes 4 ×48G-RTX8000 GPUs. We follow the data\naugmentation in [Wang et al., 2020] mainly.\nEvaluation metric. For COCO dataset, we adopt the de-\nfault standard average precision (AP) as our evaluation met-\nric. AP is calculated based on Object Keypoint Similarity\n(OKS):\nOKS =\nP\ni exp(−\nˆd2\ni\n2s2k2\ni\n)δ(vi > 0)\nP\ni δ(vi > 0) (6)\nwhere ˆdi is the L2 distance between the i-th keypoint and\nthe groundtruth. vi denotes the visibility of the keypoint. ki\nis a keypoint-specific constant, which is different for different\nkeypoint. We adopt the same evaluation metric to COCO for\nthe infant pose dataset. As the new proposed infant pose has\n21 keypoints, we set ki of each keypoint to the same value.\nKeypoints detection on COCO pose estimation. Table 3\nshows the comparisons on COCO val set. We com-\npare AggPose with several state-of-art methods, includ-\ning HRFormer [Yuan et al., 2021 ], TokenPose [Li et al.,\n2021], TransPose [Yang et al., 2021 ], HRNet [Wang et al.,\n2020]. For input size of 256×192, AggPose-L achieves\nMethod image size AP AR\nOpenPose 256×192 90.2 91.1\nSimpleBaseline-Res152 256×192 93.9 94.9\nHRNet-W48 256×192 94.5 95.6\nHRFormer-B 256×192 93.8 95.0\nTokenPose-L/D24 256×192 93.0 93.9\nAggPose-L 256×192 95.0 95.7\nTable 4: Comparisons on the infant pose test set, provided with the\nsame object detection boxes (OpenPose do not need object detec-\ntion, as it is a bottom-up method. We select OpenPose for com-\nparison is because most of newest proposed infant pose estimation\nframeworks are choose OpenPose as backbone.)\nMethod image size GFLOPs AP\nSwin-B 256× 192 17.6 74.3\nSegFormer-B5 256× 192 12.3 74.2\nAggPose-L 256× 192 15.0 76.4\nTable 5: Comparisons to Non-aggregation framework (Swin Trans-\nformer, SegFormer) on COCO pose estimation val\n76.4 AP, which is best among all methods. We believe\nthat AggPose-L can achieve better results after applying the\nnewest distribution-aware coordinate representation [Zhang\net al., 2020 ] or UDP [Huang et al., 2020 ]. AggPose-L\nachieves 75.7 AP on the COCO test-dev set with 256 ×192\ninput size.\nKeypoints detection on infant pose estimation. Table 4\nreports the comparisons on our infant pose test set. We com-\npare AggPose to the most representative bottom-up method\nOpenPose, as it is used by almost all newest proposed in-\nfant pose estimation frameworks [Silva et al., 2021]. We also\ncompare AggPose to several recent CNN and hybrid Trans-\nformer models such as HRNet, TokenPose, and HRFormer.\nAggPose gains the highest 95.0 AP with an input size of\n256×192. During the training, we also find that both Agg-\nPose and HRNet perform better and converge faster than hy-\nbrid model such as TokenPose, and HRFormer. Though all\nof these models are pre-trained on COCO dataset, full CNN\nor full Transformer based methods are more robust after we\nfine-tune them on other domain like infant pose data.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5049\nFigure 3: Visualization of the pose estimation heatmap results based on AggPose-L on infant pose test set.\nFigure 4: Visualization of the pose estimation results based on\nAggPose-L on COCO val.\nFigure 5: Visualization of the pose estimation results based on\nAggPose-L on infant pose test.\n4.3 Ablation Experiments\nIn previous sections, we compare AggPose with several state-\nof-art human pose estimation methods. To verify the tech-\nniques used in our method, we make detailed ablation studies\nin this subsection.\nInfluence of full Transformer backbone. Considering all\nof the other new proposed hybrid methods are using HR-\nNet’s CNN encoder as backbone, we compare our method\n(without using convolution layers in the first stage) with the\nCNN scheme of HRNet in Table 3. The Backbone col-\numn shows the difference of the first stage inside the model.\nBoth AggPose-S and AggPose-L are using SegFormer, a\nTransformer-based method as the first stage layer. Although\nother authors claim Transformers in the early stage will lead\nto lack detailed localization information, we observe that\nthe full Transformer-based early stage of AggPose can still\nachieve better performance.\nInfluence of Deep Aggregation Framework. We report\nthe COCO pose estimation results based on two well-known\nfull transformer models, Swin Transformer and SegFormer in\nTable 5. Both the Swin-B and SegFormer-B5 are pre-trained\non ImageNet21K and fine-tuned on COCO with 300 epochs.\nIn fact, AggPose-L can be considered as a deep layer aggre-\ngation structure of SegFormer-B5 with MLP skip-connection.\nAccording to the results in Table 5, our proposed multi-\nresolution aggregation framework (AggPose) achieves better\nperformance than both Swin Transformer and SegFormer.\n4.4 Visualization Analysis\nWe provide qualitative results on both COCO val set and in-\nfant pose test set, as shown in Figure 3, Figure 4 and Figure 5.\nFigure 3 shows a group of predictions and dependency areas\nfor infant pose heatmap. Although infant pose data formats\nuse more keypoints than COCO. AggPose still learns good\nrepresentations in capturing constraint relationships between\nhuman keypoints.\n5 Conclusion\nBy leveraging a new dataset with pose labels and clinical\nlabels, we built a Transformer-based infant pose estimation\nframework which can accurately detect infant supine position\npose from movement frames in video. The key insight of\nthe AggPose model is the deep aggregation Transformer with\ncross-layer MLP connection. The pose sequence generated\nby our model has been used in neurodevelopmental disorder\nprediction for newborns and early evaluation for related dis-\neases. Besides, our method can be packaged to mobile de-\nvices in the future and solve inequality in medical resources\nand privacy protection for patients. Although our results are\npromising, we acknowledge that there is still a long path to\napply our model completely end-to-end with currently avail-\nable hardware.\nIn addition to testing the utility of AggPose in real-time in-\nfant pose extraction and evaluation, a clear next step would be\npredicting the cerebral palsy via pose sequence understanding\nmodels in the future–before it is even visible to a trained pe-\ndiatrician eye. For countries with limited pediatricians, this\nwill greatly reduce the risk of severe disability in children.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5050\nAcknowledgments\nThis work was supported by Sanming Project of Medicine in\nShenzhen, China (SZSM202011005).\nReferences\n[Abbruzzese et al., 2020] Laurel Daniels Abbruzzese,\nNatasha Yamane, Deborah Fein, Letitia Naigles, and\nSylvie Goldman. Assessing child postural variability:\nDevelopment, feasibility, and reliability of a video coding\nsystem. Physical & Occupational Therapy In Pediatrics,\n41(3):314–325, 2020.\n[Cao and Lin, 2021] Xu Cao and Yanghao Lin. Caggnet:\nCrossing aggregation network for medical image segmen-\ntation. In 2020 25th International Conference on Pattern\nRecognition (ICPR), pages 1744–1750. IEEE, 2021.\n[Cao et al., 2019] Zhe Cao, Gines Hidalgo, Tomas Simon,\nShih-En Wei, and Yaser Sheikh. Openpose: realtime\nmulti-person 2d pose estimation using part affinity fields.\nIEEE transactions on pattern analysis and machine intel-\nligence, 43(1):172–186, 2019.\n[Cheng et al., 2020] Bowen Cheng, Bin Xiao, Jingdong\nWang, Honghui Shi, Thomas S Huang, and Lei Zhang.\nHigherhrnet: Scale-aware representation learning for\nbottom-up human pose estimation. In Proceedings of\nCVPR, pages 5386–5395, 2020.\n[Gu et al., 2021] Jiaqi Gu, Hyoukjun Kwon, Dilin Wang,\nWei Ye, Meng Li, Yu-Hsin Chen, Liangzhen Lai, Vikas\nChandra, and David Z Pan. Multi-scale high-resolution\nvision transformer for semantic segmentation. arXiv\npreprint arXiv:2111.01236, 2021.\n[He et al., 2021] Kaiming He, Xinlei Chen, Saining Xie,\nYanghao Li, Piotr Doll ´ar, and Ross Girshick. Masked\nautoencoders are scalable vision learners. arXiv preprint\narXiv:2111.06377, 2021.\n[Hesse et al., 2018] Nikolas Hesse, Christoph Bodensteiner,\nMichael Arens, Ulrich G Hofmann, Raphael Weinberger,\nand A Sebastian Schroeder. Computer vision for medical\ninfant motion analysis: State of the art and rgb-d data set.\nIn Proceedings of ECCV Workshops, pages 0–0, 2018.\n[Huang et al., 2020] Junjie Huang, Zheng Zhu, Feng Guo,\nand Guan Huang. The devil is in the details: Delving into\nunbiased data processing for human pose estimation. In\nProceedings of CVPR, pages 5700–5709, 2020.\n[Huang et al., 2021] Xiaofei Huang, Nihang Fu, Shuangjun\nLiu, and Sarah Ostadabbas. Invariant representation learn-\ning for infant pose estimation with small data. In 2021\n16th IEEE International Conference on Automatic Face\nand Gesture Recognition (FG 2021), pages 1–8. IEEE,\n2021.\n[Li et al., 2021] Yanjie Li, Shoukui Zhang, Zhicheng Wang,\nSen Yang, Wankou Yang, Shu-Tao Xia, and Erjin Zhou.\nTokenpose: Learning keypoint tokens for human pose es-\ntimation. In Proceedings of ICCV, pages 11313–11322,\n2021.\n[Lin et al., 2014] Tsung-Yi Lin, Michael Maire, Serge Be-\nlongie, James Hays, Pietro Perona, Deva Ramanan, Pi-\notr Doll ´ar, and C Lawrence Zitnick. Microsoft coco:\nCommon objects in context. In ECCV, pages 740–755.\nSpringer, 2014.\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. arXiv preprint arXiv:2103.14030, 2021.\n[McCay et al., 2019] Kevin D McCay, Edmond SL Ho,\nClaire Marcroft, and Nicholas D Embleton. Establishing\npose based features using histograms for the detection of\nabnormal infant movements. In EMBC, pages 5469–5472.\nIEEE, 2019.\n[Reich et al., 2021] Simon Reich, Dajie Zhang, Tomas Kul-\nvicius, Sven B¨olte, Karin Nielsen-Saines, Florian B Poko-\nrny, Robert Peharz, Luise Poustka, Florentin W ¨org¨otter,\nChrista Einspieler, et al. Novel ai driven approach to clas-\nsify infant motor functions. Scientific Reports, 11(1):1–13,\n2021.\n[Silva et al., 2021] Nelson Silva, Dajie Zhang, Tomas Kulvi-\ncius, Alexander Gail, Carla Barreiros, Stefanie Lindstaedt,\nMarc Kraft, Sven B ¨olte, Luise Poustka, Karin Nielsen-\nSaines, et al. The future of general movement assess-\nment: The role of computer vision and machine learning–\na scoping review. Research in developmental disabilities,\n110:103854, 2021.\n[Wang et al., 2020] Jingdong Wang, Ke Sun, Tianheng\nCheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu,\nYadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep\nhigh-resolution representation learning for visual recogni-\ntion. IEEE transactions on pattern analysis and machine\nintelligence, 2020.\n[Wang et al., 2021] Wenhai Wang, Enze Xie, Xiang Li,\nDeng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A ver-\nsatile backbone for dense prediction without convolutions.\narXiv preprint arXiv:2102.12122, 2021.\n[Xie et al., 2021] Enze Xie, Wenhai Wang, Zhiding Yu,\nAnima Anandkumar, Jose M Alvarez, and Ping Luo.\nSegformer: Simple and efficient design for seman-\ntic segmentation with transformers. arXiv preprint\narXiv:2105.15203, 2021.\n[Yang et al., 2021] Sen Yang, Zhibin Quan, Mu Nie, and\nWankou Yang. Transpose: Keypoint localization via trans-\nformer. In Proceedings of ICCV, pages 11802–11812,\n2021.\n[Yuan et al., 2021] Yuhui Yuan, Rao Fu, Lang Huang, Wei-\nhong Lin, Chao Zhang, Xilin Chen, and Jingdong Wang.\nHrformer: High-resolution transformer for dense predic-\ntion. arXiv preprint arXiv:2110.09408, 2021.\n[Zhang et al., 2020] Feng Zhang, Xiatian Zhu, Hanbin Dai,\nMao Ye, and Ce Zhu. Distribution-aware coordinate rep-\nresentation for human pose estimation. In Proceedings of\nCVPR, pages 7093–7102, 2020.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5051",
  "topic": "Pose",
  "concepts": [
    {
      "name": "Pose",
      "score": 0.9084540605545044
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7351320385932922
    },
    {
      "name": "Computer science",
      "score": 0.7274665832519531
    },
    {
      "name": "Transformer",
      "score": 0.5634518265724182
    },
    {
      "name": "Deep learning",
      "score": 0.4966939091682434
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.47337645292282104
    },
    {
      "name": "3D pose estimation",
      "score": 0.42741748690605164
    },
    {
      "name": "Computer vision",
      "score": 0.4063599109649658
    },
    {
      "name": "Machine learning",
      "score": 0.3918949365615845
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.376778781414032
    },
    {
      "name": "Engineering",
      "score": 0.1083672046661377
    },
    {
      "name": "Geography",
      "score": 0.08158981800079346
    },
    {
      "name": "Cartography",
      "score": 0.06092125177383423
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210152664",
      "name": "Shenzhen Children's Hospital",
      "country": "CN"
    }
  ]
}