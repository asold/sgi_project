{
  "title": "Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports",
  "url": "https://openalex.org/W4391420849",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101299282",
      "name": "Qingqing Zhu",
      "affiliations": [
        "National Center for Biotechnology Information",
        "National Institutes of Health"
      ]
    },
    {
      "id": "https://openalex.org/A5042441263",
      "name": "Xiuying Chen",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100611571",
      "name": "Qiao Jin",
      "affiliations": [
        "National Center for Biotechnology Information",
        "National Institutes of Health"
      ]
    },
    {
      "id": "https://openalex.org/A5084982293",
      "name": "Benjamin Hou",
      "affiliations": [
        "National Institutes of Health Clinical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5101299283",
      "name": "Tejas Sudharshan Mathai",
      "affiliations": [
        "National Institutes of Health Clinical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5075266423",
      "name": "Pritam Mukherjee",
      "affiliations": [
        "National Institutes of Health Clinical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5100618900",
      "name": "Xin Gao",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5016047550",
      "name": "Ronald M. Summers",
      "affiliations": [
        "National Institutes of Health Clinical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5083081872",
      "name": "Zhiyong Lu",
      "affiliations": [
        "National Center for Biotechnology Information",
        "National Institutes of Health"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1444168786",
    "https://openalex.org/W3164654615",
    "https://openalex.org/W4381587418",
    "https://openalex.org/W4308112316",
    "https://openalex.org/W2768567289",
    "https://openalex.org/W4377047569",
    "https://openalex.org/W4387356888"
  ],
  "abstract": "In radiology, Artificial Intelligence (AI) has significantly advanced report generation, but automatic evaluation of these AI-produced reports remains challenging. Current metrics, such as Conventional Natural Language Generation (NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic intricacies of clinical contexts or overemphasize clinical details, undermining report clarity. To overcome these issues, our proposed method synergizes the expertise of professional radiologists with Large Language Models (LLMs), like GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning, our approach aligns LLM evaluations with radiologist standards, enabling detailed comparisons between human and AI generated reports. This is further enhanced by a Regression model that aggregates sentence evaluation scores. Experimental results show that our \"Detailed GPT-4 (5-shot)\" model achieves a 0.48 score, outperforming the METEOR metric by 0.19, while our \"Regressed GPT-4\" model shows even greater alignment with expert evaluations, exceeding the best existing metric by a 0.35 margin. Moreover, the robustness of our explanations has been validated through a thorough iterative strategy. We plan to publicly release annotations from radiology experts, setting a new standard for accuracy in future assessments. This underscores the potential of our approach in enhancing the quality assessment of AI-driven medical reports.",
  "full_text": "Leveraging Professional Radiologists’ Expertise to\nEnhance LLMs’ Evaluation for Radiology Reports\nQingqing Zhu∗, Xiuying Chen †, Qiao Jin ∗, Benjamin Hou ‡, Tejas Sudharshan Mathai ‡,\nPritam Mukherjee‡, Xin Gao †, Ronald M. Summers ‡, Zhiyong Lu ∗\n∗National Center for Biotechnology Information, National Library of Medicine,\nNational Institutes of Health, Bethesda, MD, USA\n†Bioscience Reseach Center, King Abdullah University of Science & Technology, Saudi Arabia\n‡Imaging Biomarkers and Computer-Aided Diagnosis Laboratory,\nDepartment of Radiology and Imaging Sciences,\nNational Institutes of Health Clinical Center, Bethesda, MD, USA\nAbstract—In radiology, Artificial Intelligence (AI) has signifi-\ncantly advanced report generation, but automatic evaluation of\nthese AI-produced reports remains challenging. Current metrics,\nsuch as Conventional Natural Language Generation (NLG)\nand Clinical Efficacy (CE), often fall short in capturing the\nsemantic intricacies of clinical contexts or overemphasize clinical\ndetails, undermining report clarity. To overcome these issues,\nour proposed method synergizes the expertise of professional\nradiologists with Large Language Models (LLMs), like GPT-3.5\nand GPT-4. Utilizing In-Context Instruction Learning (ICIL)\nand Chain of Thought (CoT) reasoning, our approach aligns\nLLM evaluations with radiologist standards, enabling detailed\ncomparisons between human and AI-generated reports. This is\nfurther enhanced by a Regression model that aggregates sentence\nevaluation scores. Experimental results show that our “Detailed\nGPT-4 (5-shot)” model achieves a 0.48 score, outperforming\nthe METEOR metric by 0.19, while our “Regressed GPT-4”\nmodel shows even greater alignment with expert evaluations,\nexceeding the best existing metric by a 0.35 margin. Moreover,\nthe robustness of our explanations has been validated through a\nthorough iterative strategy. We plan to publicly release annotations\nfrom radiology experts, setting a new standard for accuracy in\nfuture assessments. This underscores the potential of our approach\nin enhancing the quality assessment of AI-driven medical reports.\nI. I NTRODUCTION\nWith the progression of Artificial Intelligence (AI) and Ma-\nchine Learning (ML) technologies, automated report generation\nsystems are increasingly leveraging a myriad of models [1]–[3].\nThe accuracy and precision of generated outputs are paramount\nin medical fields, such as radiology, due to their direct impact\non patient care [4]–[7]. As such, establishing an effective and\naccurate evaluation framework for these reports is essential.\nWhile radiologist assessments are considered the gold\nstandard for evaluating radiology reports, relying on their\nexpertise and context-specific knowledge, the growing volume\nof AI-generated reports makes this approach increasingly\nimpractical. Current automatic evaluation metrics, including\nNatural Language Generation (NLG) and Clinical Efficacy\n(CE) metrics, offer efficiency but are often limited in capturing\nthe depth and complexity necessary for medical reports [8].\nFurthermore, a significant shortcoming of these metrics is\ntheir lack of explanatory power. They typically provide scores\nwithout detailed explanations, leaving a gap in understanding\nthe nuances and clinical relevance of the reports. This limitation\nunderscores the need for more advanced evaluation tools that\ncan provide not just quantitative assessments but also qualitative\ninsights.\nRecent advancements in the field of natural language process-\ning, particularly with Large Language Models (LLMs), present\npotential solutions to these challenges. Groundbreaking studies\n[9]–[16] have demonstrated the adaptability of LLMs in diverse\ntasks. This adaptability is realized through explicit instructions\nand few-shot templates, a paradigm often referred as In-Context\nInstruction Learning (ICIL) [12]. Additionally, the concept of\nChain of Thought (CoT) reasoning, which involves a series\nof intermediate reasoning steps, significantly enhances the\ncapability of LLMs for complex reasoning tasks, as indicated by\n[17]. This methodology, already successfully applied in various\ngeneral domains, hints at its immense potential when applied\nto in-depth medical reasoning, particularly under the guidance\nof expert radiologists. By harnessing these advancements, our\nmethod aims to combine the expertise of radiologists with the\nstrengths of LLMs, creating a new way to evaluate complex\nmedical reports.\nOur contributions are: (1) Introducing a unique approach that\ncombines radiologist expertise, ICIL, and CoT to improve the\nevaluation of radiology reports with LLMs, notably GPT-3.5\nand GPT-4. (2) Benchmarking our method against existing\nmetrics, quantifying its correlation with expert evaluations,\nand demonstrating its superiority over the state-of-the-art.\nAdditionally, we plan to publicly release annotations from\nradiology experts, setting a new standard for accuracy in future\nassessments. (3) Our approach not only offers a comprehensive\nand precise assessment of AI-generated radiology reports but\nalso provides the added benefit of explainability.\nII. R ELATED WORK\nA. Evaluation Metrics in Radiology Reports\nSeveral metrics have been developed and used for evaluating\ntext generated by AI systems. Metrics like BLEU (Bilingual\nEvaluation Understudy) [18], METEOR (Metric for Evaluation\nof Translation with Explicit ORdering) [19], and ROUGE\n(Recall-Oriented Understudy for Gisting Evaluation) [20] are\narXiv:2401.16578v3  [cs.CL]  17 Feb 2024\nused widely, but they each have limitations when applied to\nmedical reports [21], [22]. These metrics primarily assess the\nn-gram overlap between generated text and reference text,\nor consider word and phrase alignments. Consequently, they\nlack the capacity to evaluate complex semantic and contextual\nnuances that are intrinsic to medical reporting [23]. Meanwhile,\nCE metrics such as F1 score, precision, and recall, primarily\nused in machine learning, have been adapted to evaluate\nthe performance of automated systems in identifying and\ncategorizing observations in radiology reports [8], [24], [25].\nWhile these metrics are proficient in assessing the model’s\nability to correctly identify observations, they fall short in\nevaluating the overall quality and coherence of the generated\nreports.\nIn contrast, our proposed method provides a more nuanced\nevaluation of AI-generated radiology reports. It adeptly captures\nthe essential details and subtleties inherent in such reports,\nmaking it particularly applicable in the medical domain.\nFurthermore, our approach not only saves substantial human\nlabor by reducing the reliance on manual evaluation but also\npossesses the unique quality of being explainable, further\nenhancing its practical value.\nB. LLMs for Evaluation\nIn recent years, we’ve seen significant advancements in\nLLMs, with models spanning from BERT [26] to GPT. These\nmodels, characterized by their escalating sophistication and\ncapabilities, have vastly facilitated the progression of advanced\ntechniques. Among them, GPT-3.5 and GPT-4 have catalyzed\na paradigm shift within the field of intelligent human-machine\ndialogue. This shift continues to make a significant impact on\nthe research community and various industries [27].\nThe advent of ChatGPT has stimulated immense interest in\ntwo primary areas. Firstly, many papers explore its performance\nacross a myriad of Natural Language Processing (NLP) tasks,\nshedding light on its extensive capabilities. Secondly, there’s\ngrowing intrigue in employing it as a metric for evaluating\nmodel outputs [28]. Evaluations involving ChatGPT typically\nfall into two categories: Natural Language Understanding\n(NLU) and Natural Language Generation (NLG). ChatGPT\nhas demonstrated remarkable performance across virtually\nall NLU tasks, as confirmed by existing work [29], [30].\nWithin the NLG sphere, it has been applied in areas such\nas machine translation [31], monolingual summarization [32],\ncross-lingual summarization [33], review generation [34], and\nradiology reports generation [35]. However, our work diverges\nby employing GPT-3.5 or GPT-4 as a human evaluator, using\nit to autonomously assess the quality of general textual\ngenerations, rather than merely utilizing it to solve tasks.\nWhile studies exist that utilize ChatGPT to evaluate specific\nfields such as translation [28] or human personalities [36],\nthese applications are often simplistic and lack grounding in\nany professional domain. In contrast, our study innovatively\nemploys GPT-3.5 or GPT-4 in the medical domain, which also\nactively involves domain experts in the process.\nIII. M ETHOD\nOur research is uniquely positioned to evaluate any model\ndesigned for generating radiological reports, showcasing a\nbroad applicability in this field.\nIn our study, we primarily concentrate on the model presented\nin Zhu [37], which we refer to as the LongiFill model in our\npaper. This choice is motivated by its recent advancements in\nthe field of medical report generation. This model leverages\nlongitudinal multi-modal data, encompassing prior patient visit\nchest X-rays (CXR), current visit CXR, and the previous visit’s\nreport, to efficiently pre-populate the report for a current patient\nvisit. We conduct our evaluation using reports generated by\nthe this model that is trained on the MIMIC-CXR 1 dataset.\nIt’s important to note that our evaluation framework is distinct\nfrom the LongiFill model. In report generation process, we\ntreat reports from the MIMIC-CXR dataset as “Original” and\nthose generated by AI models as “Predicted” reports. This\nframework integrates the expertise of professional radiologists\nwith advanced LLMs, specifically GPT-3.5 and GPT-4.This\nintegration enables a more comprehensive and nuanced analysis\nof the generated reports.\nFigure 1 shows the whole architecture of our evaluation\nstrategy, following the steps:\n(1) First, the input, which includes the Original and Predicted\nreports, is passed to an LLM, along with instructions and\ntemplates. The number of these templates is denoted as “k”.\nThis stage epitomizes our ICIL approach. It ensures that the\nmodel not only determines scores but also formulates an\nexplanation with a set output structure, following a logical\nsequence. The “Explanation” segment, though not delineated\ninto a sequential chain of thought, mirrors the essence of a CoT.\nIt offers clarity and sheds light on the derivation of the overall\nscore. Grounded in the CoT methodology, this explanation\nacts as a pivotal intermediate output, setting the foundation for\nfurther enhancements. (2) The sentence scores derived from the\nICIL process are then fed into a regression model. The objective\nis to calculate an overall score for the report. This phase is\nlabeled “Overall Score Regression”. (3) In this phase, we focus\non verifying the quality of our explanations. These explanations,\nalong with the AI generated report, are then passed to the LLMs\nagain. The result is a Refined Report. Subsequently, this report\nis returned for another round of evaluation. This step is termed\n“Iterative Verification” and acts as our validation loop.\nA. In-context Instruction Learning\nLarge Language Models For evaluation, we use LLMs such\nas GPT-3.5 and GPT-4. These models not only serve as quasi-\nhuman evaluators but also follow ICIL and CoT mechanisms,\nprogressively building upon prior information to generate\ncomprehensive outputs.\nInput We first pre-process the radiology reports by splitting\nthem using a period (“.”). Following this segmentation, each\nsentence is then identified using specific identifiers such as\n“a”, “b”, and so on, in a sequential manner.\n1https://physionet.org/content/mimic-cxr-jpg/2.0.0/\nLarge Language Model\nParameters Frozen\nInstruction\nIn this task, you are\nrequired to evaluate\npairs of medical\nreports: an original\nreport and a predicted\nreport....\nOutput \n[Case, Type, ID, Sentence, Match ID,\nSentence Score, Overall Score, Explanation]\nTemplate\nInput: \n- Ground-truth report\n- Predicted report\nOutput:\n- Case,\n- Type,\n- ID,\n- Sentence,\n- Match ID,\n- Sentence Score,\n- Overall Score,\n- Explanation\nInput\n- Original report\n- Predicted report\nRegression \nModel\nSentence\nScore\nk-templates\nOverall\nScore\nExplanation\nEvaluation Pipeline\n(1) In-context Instruction Learning\na\nc\nb\nA\nB\nC\nEntailment Score\nGround Truth Prediction\n···\n···\nd Arrow Colour:\n- Green = 1 (perfectly match)\n- Yellow = 0.5 (partially match)\n- Red = -1 (contradict)\n- None = 0 (no match)\nLarge Language Model\nParameters Frozen\nRefined Report\n(3) Iterative Verification\n(2) Overall Score Regression\nFig. 1: The whole architecture of our evaluation strategy. It is primarily focused on three key areas: In-context Instruction\nLearning, Overall Score Regression and Iterative Verification. The “sentence score” within the template represents the entailment\nscore, derived by comparing each sentence from the original reports with its corresponding sentence in the prediction. An\nexplanation for this is provided in the lower right corner of the figure.\nRole of Radiologists Radiologists play a crucial role in crafting\nthe instructions, designing the evaluation templates, and\nlabeling the cases. These instructions and templates were\nincorporated into the LLMs, which subsequently assisted\nthe LLMs in emulating professional evaluation patterns. The\nlabeled cases were subsequently utilized for in-depth analysis.\nInstruction Two types of instructions were utilized: the\nsimplistic and detailed versions (We outline both versions of\nthe task instructions in the supplementary materials). Both sets\nof instructions were crafted with the assistance of a renowned\nradiology expert. The simplistic version is concise, directing\nradiologists to compare predictions with the original reports\nand then provide an overall score. Conversely, the detailed\nversion elaborates on the nature of the reports, introducing a\nwell-defined scoring system, and underscoring the real-world\nsignificance of the task. These instructions guide LLMs through\nthe evaluation process.\nTemplates Each template consists of two distinct sections:\ninput and output. A representative example of this can be\nfound in the supplementary materials. The structure for the\ninput adheres to the previously specified format. The output\nsection, however, is delineated into the subsequent fields: 1.\nCase: Denotes the number assigned to each case. 2. Type :\nDistinguishes whether the sentence is sourced from the original\nreports or is a prediction. 3. ID: A unique identifier assigned to\neach sentence. 4. Sentence: The actual content of the sentence.\n5. Match ID : An identifier linking to matched sentences. 6.\nSentence Score: A numerical value that signifies the level of\nentailment between the predicted and original statements. The\nscoring metrics are: (1) Score of 1: The predicted statement\naligns in meaning and detail with the original. Word choice\nmay vary, but the core message remains consistent. Crucially,\nno original information is omitted or contradicted. (2) Score\nof 0.5: The predicted statement bears some resemblance to the\noriginal statement, though not entirely. While certain elements\nare congruent, others may differ or be absent. The foundational\nidea is present, but not every detail is precise. (3) Score\nof -1: The predicted statement is diametrically opposed to\nor starkly contrasts the original statement. Any information\nprovided directly challenges the original content. (4) Score\nof 0: The predicted statement lacks a corresponding original\nstatement for comparison. 7. In addition to the detailed output\nfor each sentence, every case comes with: (1) Overall Score :\nRepresents a holistic assessment of the example from 0 to 5.\n(2) Explanation : A descriptive commentary elucidating the\nCoT that provides context and insight into how the overall\nscore was derived.\nB. Overall Score Regression\nInstead of solely relying on the overall scores directly\nassigned by GPT-3.5 or GPT-4, we adopted an alternative\napproach that employed regression models to predict the overall\nscore. By leveraging this model, our goal was to encapsulate\nthe combined insights of multiple features, and potentially gain\na more nuanced understanding of the underlying patterns in\nthe data [38].\n1. Feature Engineering: Sentence Score RatiosTo establish\nthe input features for our regression model, we calculated\nthe ratios of various sentence scores in both original and AI-\npredicted sentences. These scores, representing key assessment\ncriteria, include 0, 0.5, 1, and -1. The ratios, indicating the\nfrequency of each score relative to the total, are crucial in\nunderstanding the balance of scoring tendencies. For the\noriginal report sentences, with a total count of m, the ratios for\nscores 0, 0.5, 1, and -1 are denoted as ro0 , ro0.5 , ro1 , and ro−1\nrespectively. Similarly, for the AI-predicted sentences having a\ntotal count of n, the ratios are rp0 , rp0.5 , rp1 , and rp−1 . The\nfeature set X is thus represented as:\nX = [ro0 , ro0.5 , ro1 , ro−1 , rp0 , rp0.5 , rp1 , rp−1 ]\n2. Target Variable The overall scores given by human\nannotators, denoted by O, serve as our target variable Y,\nwhich represent aggregate or average evaluations of specific\ncriteria by annotators, encapsulating a holistic view of report\nquality.\n3. Training With the defined feature set X and target vari-\nable Y, we employed various regression models (e.g., Decision\nTree, Support Vector Machine, K-Nearest Neighbors, Neural\nNetwork, Gradient Boosting, Random Forest) to assess the\nperformance of LLMs. Each model predicts regressed overall\nscores ( ˆY) based on the given features and is represented as:\nˆY = fmodel(X; θ)\nwhere fmodel denotes the specific regression model used, and\nθ represents the model parameters.\nC. Iterative Verification of the Explanatory Mechanisms\nLarge Language Models (LLMs) do more than just evaluate\nAI reports. They offer detailed insights into AI decisions,\nshowing both their strengths and weaknesses. To validate\nthe accuracy of these explanations, we first employ a refined\nprocess. This process is structured around specific instructions\nand templates (examples of this process can be found in the\nsupplementary materials). These instructions carefully steer\nthe creation of polished reports based on the explanations\nprovided, while the template serves as a practical example,\nfurther clarifying the refinement process. The Refined Report\nthen goes through another round of evaluation. No additional\ndata, such as the original image, was introduced during this\nprocess. Then by re-evaluating the Refined Report, it’s possible\nto assess whether the initial explanations were accurate and\nwhether they have been effectively integrated into the new\nversion of the report.\nIV. E XPERIMENTS\nWe utilized both GPT-4 and GPT-3.5. For our work, we\nemployed the OpenAI API version “2023-03-15-preview”,\nusing the engines “gpt-35-turbo” for GPT-3.5 and “gpt-4” for\nGPT-4. We find that the scores predicted by the models for\neach sentence could vary slightly due to the nature of sampling.\nTherefore, to ensure accuracy and reliability in our findings,\nwe conducted three iterations of the sentence scoring process,\nobserving some minor variations and calculated average results\nfor the training data used in our regressed model and evaluation\nprocess.\nMetrics for Comparison We juxtaposed our proposed method\nagainst several established automatic evaluation metrics: 1.\nNLG Metrics: Metrics such as BLEU [18], METEOR [19],\nand RougeL [20] were incorporated into our comparative study.\n2. CE Metrics: For clinical accuracy assessment of generated\nreports, we employed the CheXpert labeler 2 [8]. Emulating the\nmethodology of [39], we contrasted the positive labels of 14\nCheXpert observations3 based on accuracy, precision, recall,\nand F-1 metrics.\nHuman Assessment The human assessment involved 100\noriginal-prediction pairs, which were randomly selected by the\nLongiFill model. It was carried out by three human raters:\nRater1 is a physician with a doctoral degree, while Rater2 and\nRater3 were trained in biomedical informatics. A consensus,\nachieved after deliberation among the three raters, served as\nthe ground truth. During this process, to guarantee the quality\nof annotations, we also sought assistance from a renowned\nradiology expert. We carefully selected five of these cases as\ntemplates, which showcased diverse scoring patterns. These\nremaining 95 examples are used for further evaluation.\nV. R ESULTS AND ANALYSES\nEvaluating Alignment between Different Metrics and\nHuman Evaluations To discern the degree of alignment\nbetween automated metrics (including our methods) and human\nevaluations’ overall score, we relied on Kendall’s Tau [40].\nFigure 2 illustrates the results for various metric pairs. 1.\nComparative Metric Performance with our “Detailed\nGpt-4 (5-shot)” Model Among NLG metrics, METEOR\ndemonstrates the strongest alignment score of 0.29 with the\nground truth evaluations, followed by ROUGE and then BLEU.\n2https://github.com/stanfordmlgroup/chexpert-labeler/\n3No Finding, Enlarged Cardiomediastinum, Cardiomegaly, Lung Lesion,\nAirspace Opacity, Edema, Consolidation, Pneumonia, Atelectasis, Pneumotho-\nrax, Pleural Effusion, Pleural Other, Fracture, and Support Devices\nFor the CE metrics, Recall and F1 score present the most\nsignificant correlation with the expert evaluations. While NLG\nmetrics generally outperform CE metrics in this context, both\ncategories fall short when compared to the “Detailed Gpt-4\n(5-shot)”. This superior alignment may stem from the model’s\ncapability to discern nuanced and qualitative elements that\naren’t comprehensively captured by any single metric. 2.\nComparative Metric Performance with our “Regressed Gpt-\n4” Model We also evaluated the performance of the Regressed\nmodel. During training, we used the ground truth sentence\nscores and overall scores from the 95 manually annotated\nexamples. While we evaluated various regression models, the\nRandom Forest model demonstrated the best performance.\nDetails of this ablation study can be found in the supplementary\nmaterials. Subsequently, we input the sentence score from\ndetailed GPT-4 (5-shot) into the Regressed model to obtain\nRegressed overall scores, denoted as \"Regressed GPT-4\" in the\nfigure. This score was then compared with other evaluation\nmethods. Notably, the Regressed model exhibits a correlation\nthat is 0.35 (0.64 vs. 0.29) higher than the best METEOR in\nother metrics. This underscores the effectiveness of integrating\nmachine learning with LLM evaluations. 3. Comparative\nMetric Performance with our “Detailed GPT-3.5 (5-shot)”\nModel The performance of “Detailed GPT-3.5 (5-shot)” seems\nto lag behind both traditional metrics and GPT-4. This suggests\nthat GPT-3.5 may struggle to accurately encapsulate the nuances\nwithin sentences.\nbleumeteorrouge_lprecisionrecallaccuracyf1_score\ndetailed Gpt-4 (5-shot)Regressed Gpt-4detailed Gpt-3.5 (5-shot)\nground truth\nbleu\nmeteor\nrouge_l\nprecision\nrecall\naccuracy\nf1_score\ndetailed Gpt-4 (5-shot)\nRegressed Gpt-4 (5-shot)\ndetailed Gpt3.5 (5-shot)ground truth\n0.54\n0.6 0.54\n0.120.080.08\n0.060.120.050.28\n0.080.070.040.710.68\n0.230.290.270.070.170.12\n0.220.230.280.070.150.140.13\n0.140.190.160.070.190.170.140.45\n0.140.080.110.010.010.020.010.190.14\n0.230.290.270.070.170.120.160.480.640.16\n 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nFig. 2: Correlation matrix of Kendall’s Tau Values for Metric\nPairs. All scores have p value < 0.05.\nIterative Verification Results A defining feature of our\nmethodology is validation of the explanations produced. We\nrandomly selected 100 reports from the LongiFill model. These\nchosen reports and their explanations were then fed into\nthe GPT-4 model. After refinement, the reports underwent\nanother round of evaluation. The results of these evaluations\nare presented in Table II. The table emphasizes notable\nimprovements in the quality of the regenerated reports across\nall metrics, where the explanations has indeed assisted in\nproducing superior reports. This iterative process illustrates the\nreliability of the explanations provided by our methodology\nand proves the interpretability of our framework.\nVI. D ISCUSSION\nground truthrater1rater2rater3\nsimple GPT-4(1-shot)simple GPT-4(5-shot)detailed GPT-4(1-shot)detailed GPT-4(5-shot)simple GPT-3.5(1-shot)simple GPT-3.5(5-shot)detailed GPT-3.5(1-shot)detailed GPT-3.5(5-shot)\nground truthrater1\nrater2\nrater3\nsimple GPT-4(1-shot)\nsimple GPT-4(5-shot)\ndetailed GPT-4(1-shot)\ndetailed GPT-4(5-shot)\nsimple GPT-3.5(1-shot)\nsimple GPT-3.5(5-shot)\ndetailed GPT-3.5(1-shot)\ndetailed GPT-3.5(5-shot)\n0.96\n0.760.73\n0.760.750.76\n0.640.640.640.65\n0.7 0.7 0.7 0.650.6\n0.650.630.590.640.540.8\n0.740.750.680.7 0.620.590.67\n0.330.330.3 0.320.3 0.380.380.34\n0.470.460.460.450.430.5 0.410.470.44\n0.450.450.4 0.430.420.440.440.440.460.45\n0.440.430.420.440.4 0.460.410.440.410.560.45\n 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFig. 3: Correlation matrix depicting Cohen’s Kappa scores\nfor different annotation methods when aggregating sentence\nscores.\nSentence-Level Evaluation: Superior Correlation with\nHuman Judgment In the process of evaluating the original-\nprediction pairs, each sentence was first assigned a corre-\nsponding sentence score. The agreement between these scores\nand human evaluations, analyzed on a sentence-by-sentence\nbasis, was quantified using Cohen’s Kappa [41] in Figure\n3, providing valuable insights. 1. Agreement between the\nGround Truth and Others: Rater1, with expertise in radiology,\nexhibited the highest kappa scores with the ground truth,\nregistering a kappa score of 0.956. Given their specialized\ntraining, this robust correlation is anticipated. Notably, the\n5-shot version of the detailed GPT-4 model (with a score of\n0.74) has achieved a performance remarkably close to that of\nRater2 (0.76) and Rater3 (0.73). 2. Effect of Different GPT\nModels: GPT-4 displayed superior performance to GPT-3.5\nacross all conditions, implying improved capabilities in the\nnewer model version. 3. Effect of Different Instructions: The\neffectiveness of detailed instructions is notably reflected in GPT-\n4’s performance. For example, when using a Detailed GPT-4\napproach with 5-shot learning, there was an improvement of\n0.04 (from 0.70 to 0.74) compared to the simpler approach.\nThis improvement can likely be attributed to the more detailed\ninstructions providing clearer context, which enables GPT-\n4 to generate sentences that more closely align with the\nintended objective. In contrast, the results are reversed when\napplying detailed instructions with GPT-3.5. This difference\nis likely due to GPT-3.5’s relatively lower reasoning and\nlearning capabilities compared to GPT-4. In the case of GPT-\n3.5, providing excessive context appears to hinder rather than\nenhance its performance, underscoring the nuanced differences\nbetween these two models in handling complex instructions. 4.\nEffect of Different Number of Templates: In our experiments,\nwe tested scenarios with a single template (1-shot learning)\nand with five templates (5-shot learning). Beyond the influence\nof instructions, we also noticed that the 5-shot learning method\nType ID Report Match ID Score\nGPT-4 GT GPT-4 GT\nOriginal a Comparison is made to prior study from. - - 0 0\nOriginal b There is a swan-ganz catheter whose distal lead tip is\nin the main pulmonary outflow tract.\n- - 0 0\nOriginal c The cardiac silhouette is enlarged. C C 1 0.5\nOriginal d There is again seen moderate <unk> pleural effusion\nwhich is stable.\nF F 0.5 0.5\nOriginal e There is some improvement in the pulmonary vascular\nedema.\nE E 0.5 0.5\nOriginal f There are no pneumothoraces identified. H H 1 1\nPrediction A The patient is status post median sternotomy and cabg. - - 0 0\nPrediction B Left-sided AICD device is noted with leads terminating\nin the right atrium, right ventricle, and region of the\ncoronary sinus.\n- - 0 0\nPrediction C Moderate to severe cardiomegaly is re-demonstrated. c c 1 0.5\nPrediction D The mediastinal contour is unchanged. - - 0 0\nPrediction E There is mild pulmonary vascular congestion. e e 0.5 0.5\nPrediction F Small bilateral pleural effusions are noted. d d 0.5 0.5\nPrediction G Patchy opacities in the lung bases likely reflect areas\nof atelectasis.\n- - 0 0\nPrediction H No pneumothorax is identified. f f 1 1\nOverall Score: 3 2.5\nExplanation: The AI’s generated report has some alignment with the ground truth - the cardiac silhouette enlargement\nand the absence of pneumothorax are accurate. The ground truth’s note on improved pulmonary vascular edema\naligns partially with the AI’s note on mild congestion. However, the AI report doesn’t highlight the presence of the\nswan-ganz catheter, and it inaccurately reports small bilateral effusion instead of the ground truth’s moderate, stable\npleural effusion. Additionally, the AI mentions post median sternotomy and cabg status, and AICD device installation,\nwhich are not mentioned in the ground truth.\nTABLE I: Case study for evaluation of original and prediction reports. “GT” means “Ground Truth”.\nNLG CE Our Method\nBL-1 M RL A P R F1 ReGPT-4\nBefore 0.3060 0.1192 0.2389 0.7986 0.4527 0.346 0.3922 2.4219\nAfter 0.3846 0.2420 0.2480 0.8007 0.4716 0.5057 0.4881 3.5467\nTABLE II: Comparative Analysis of Key Metrics “Before” and “After” Reintroducing Generated Reports into GPT-4. This\niterative verification process aims to ensure the reliability and correctness of explanations.\nNLG CE Our Method\nBL-1 M RL A P R F1 ReGPT-4\nTransformer 0.2951 0.1233 0.2601 0.8067 0.4858 0.3212 0.3867 2.5201\nLongiFill 0.3356 0.1341 0.2728 0.8219 0.5397 0.4178 0.471 2.6144\nTABLE III: Comparative Study for evaluation between the Transformer and LongiFill Models: Metrics include NLG metrics\nsuch as BLEU (BL), METEOR (M), and ROUGE RL, CE metrics such as Accuracy (A), Precision (P), Recall (R), and F-1\nscore (F1), and our proposed evaluation method, Regressed GPT-4 (ReGPT-4).\ntypically outperforms the 1-shot method for GPT-4. The 5-shot\napproach offers more examples to the model, aiding it in better\ncomprehending the requirements of the task.\nCase Study In this specific case in Table I, the BLEU,\nROUGE, and METEOR metrics, which are common NLG\nmetrics for natural language generation tasks, demonstrates\nthe risks associated with depending solely on n-gram overlap\nbetween predicted and original sentences. For example: (1) The\noriginal report says “the cardiac silhouette is enlarged”, while\nthe generated sentences note “moderate to severe cardiomegaly\nis re-demonstrated”. Here, both phrases indicate heart enlarge-\nment, but NLG metrics might fail to recognize this synonymy.\n(2) The original report mentions “there is again seen moderate\n<unk> pleural effusion which is stable”, whereas the generated\nsentences note “small bilateral pleural effusions are noted”.\nThese sentences are somewhat similar, but NLG metrics might\nnot capture the difference in severity (“moderate” vs “small”).\nContrary to this, GPT-4 (In this case, we use detailed Gpt-4 (5-\nshot) to generate this evaluation result) showcases its prowess\nin discerning semantic similarities and differences that might\nelude conventional metrics. GPT-4’s evaluations tend to align\nclosely with human evaluations. This alignment suggests that\nGPT-4 captures not just general semantic understanding, but\nalso clinical nuances pivotal to radiology and patient care. To\nenhance comprehension, GPT-4 was also employed to generate\nexplanations for the scores, reinforcing the criticality of precise\ninterpretation in fields like radiology. This is crucial in fields\nsuch as radiology, where the interpretation of findings can\nGPT-3.5 GPT-4\nw/ expl. w/o expl. w/ expl. w/o expl.\nKendall’s 0.1551 -0.0030 0.4775 -0.0232\nPearsons 0.2264 0.0130 0.6336 0.0063\nTABLE IV: Comparative analysis of Kendall’s and Pearson’s\nCorrelation Coefficients: GPT-3.5 vs. GPT-4 in Agreement\nthe ground truth’s Overall Scores. “w/ expl.” refers to “with\nexplanation” and “w/o expl.” refers to “without explanation”.\ngreatly influence patient care and results.\nEffect of Explanation We also conducted ablation experiments\nin LLMs directly predict the overall scores of reports without\nproviding explanations. For these experiments, we employed\nKendall’s Tau [40] and Pearson’s Correlation Coefficient [42]\nto analyze the results. Table IV reveals that both models\ndemonstrate improved performance when they include explana-\ntions, exhibiting stronger correlations with expert evaluations.\nNotably, GPT-4 shows a significant enhancement in alignment\nwith human expert evaluations when explanations are pro-\nvided, compared to GPT-3.5. However, in scenarios lacking\nan explanatory framework, the near-zero correlation scores\nhighlight that both GPT-4 and GPT-3.5 face challenges in\naligning their assessments with those of human experts. This\nfinding emphasizes the crucial role of a CoT or an explanatory\nframework in augmenting AI models with expert knowledge.\nThese results strongly support a collaborative approach between\nAI models and human experts. By combining the strengths of\nboth, we can more effectively refine the evaluation of complex\nmedical reports, leveraging the unique insights and capabilities\nof each.\nComparative Study for evaluation between the Transformer\nand LongiFill Models To rigorously assess the efficacy\nof our newly proposed evaluation model, we conducted a\ncomparative analysis between two distinct models: the advanced\nLongiFill model and its baseline transformer model in [37]\n. The transformer is a simple model that uses image as\ninput and output reports, trained on the same dataset as\nLongiFill. For a more comprehensive and robust evaluation,\nwe randomly selected a set of 300 examples generated from\neach model. Considering the results in Table III, the LongiFill\nmodel consistently outperformed the baseline, particularly\nin established metrics such as BLEU and ROUGE. This\nsuperiority was not merely restricted to traditional metrics.\nImpressively, when assessed using our novel evaluation model,\nthe LongiFill model’s scores witnessed an improvement. Such\nresults serve as a testament to the reliability of our proposed\nevaluation framework.\nVII. C ONCLUSION\nIn this paper, we introduced a novel method for evaluating\nAI-generated radiology reports, by leveraging the expertise of\nprofessional radiologists and the capabilities of large language\nmodels. Our method demonstrated superior performance over\ntraditional metrics and a high correlation with human eval-\nuations. Furthermore, our method is explainable, providing\nvaluable insights that can be used to improve the AI models\ngenerating the reports. We believe our work contributes to\nthe ongoing advancements in the field of AI and healthcare,\npaving the way for more reliable, accurate, and trustworthy AI\napplications in medical report evaluation.\nVIII. L IMITATIONS\nOur study provides valuable insights but also faces certain\nlimitations. Our methodology, specifically tailored for chest\nX-ray report evaluation, utilizes the most extensive dataset\ncurrently available in this field. This approach, owing to the\nstandardization in radiological practices, holds promise for\napplication to various chest X-ray datasets globally. Further-\nmore, the consistency in medical terminology indicates the\npotential applicability of our method to other types of imaging\nreports, such as those from CT scans. However, further testing\non a broader range of datasets is essential to confirm this\npotential. The generalizability of our approach across different\ntypes of radiology reports or other medical fields is not yet\nfully determined, necessitating additional research to evaluate\nits transferability and effectiveness in contexts beyond chest\nX-rays.\nIX. E THICAL STATEMENT\nOur experiments strictly followed HIPAA compliance\nthrough the Azure OpenAI Service and adhered to the Phy-\nsioNet Data Use Agreement, ensuring the confidentiality of\nMIMIC data. We accessed GPT models via Azure’s OpenAI\nservice, taking necessary steps to keep our data private and\nunreviewed by Microsoft, as per our agreement.\nX. A CKNOWLEDGEMENTS\nThis research was supported by the Intramural Research\nProgram of the National Library of Medicine and Clinical\nCenter at the NIH.\nREFERENCES\n[1] Q. Zhu, X. Chen, P. Wu, J. Liu, and D. Zhao, “Combining curriculum\nlearning and knowledge distillation for dialogue generation,” in Findings\nof the Association for Computational Linguistics: EMNLP 2021 , 2021,\npp. 1284–1295.\n[2] X. Chen, M. Li, S. Gao, X. Cheng, Q. Yang, Q. Zhang, X. Gao, and\nX. Zhang, “A topic-aware summarization framework with different modal\nside information,” SIGIR, 2023.\n[3] X. Chen, G. Long, C. Tao, M. Li, X. Gao, C. Zhang, and X. Zhang, “Im-\nproving the robustness of summarization systems with dual augmentation,”\nACL, 2023.\n[4] X. Wu, J. Li, J. Wang, and Q. Qian, “Multimodal contrastive learning\nfor radiology report generation,” Journal of Ambient Intelligence and\nHumanized Computing, pp. 1–10, 2022.\n[5] N. Kaur and A. Mittal, “Chexprune: sparse chest x-ray report generation\nmodel using multi-attention and one-shot global pruning,” Journal of\nAmbient Intelligence and Humanized Computing , pp. 1–13, 2022.\n[6] M. M. Mohsan, M. U. Akram, G. Rasool, N. S. Alghamdi, M. A. A.\nBaqai, and M. Abbas, “Vision transformer and language model based\nradiology report generation,” IEEE Access, vol. 11, pp. 1814–1824, 2022.\n[7] Q. Jin, F. Chen, Y . Zhou, Z. Xu, J. M. Cheung, R. Chen, R. M.\nSummers, J. F. Rousseau, P. Ni, M. J. Landsman et al., “Hidden flaws\nbehind expert-level accuracy of gpt-4 vision in medicine,” arXiv preprint\narXiv:2401.08396, 2024.\n[8] J. Irvin, P. Rajpurkar, M. Ko, Y . Yu, S. Ciurea-Ilcus, C. Chute, H. Mark-\nlund, B. Haghgoo, R. Ball, K. Shpanskaya et al., “Chexpert: A large\nchest radiograph dataset with uncertainty labels and expert comparison,”\nin Proceedings of the AAAI conference on artificial intelligence , vol. 33,\nno. 01, 2019, pp. 590–597.\n[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) . Minneapolis,\nMinnesota: Association for Computational Linguistics, Jun. 2019, pp.\n4171–4186. [Online]. Available: https://aclanthology.org/N19-1423\n[10] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[11] J. Wei, L. Hou, A. Lampinen, X. Chen, D. Huang, Y . Tay, X. Chen, Y . Lu,\nD. Zhou, T. Ma et al., “Symbol tuning improves in-context learning in\nlanguage models,” arXiv preprint arXiv:2305.08298 , 2023.\n[12] S. Ye, H. Hwang, S. Yang, H. Yun, Y . Kim, and M. Seo, “In-context\ninstruction learning,” arXiv preprint arXiv:2302.14691 , 2023.\n[13] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models\nare few-shot learners,” Advances in neural information processing systems,\nvol. 33, pp. 1877–1901, 2020.\n[14] OpenAI, “Gpt-4 technical report,” 2023.\n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi,\nS. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y . Tay, N. Shazeer,\nV . Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury,\nJ. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya,\nS. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V . Misra, K. Robinson,\nL. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov,\nR. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai,\nM. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee,\nZ. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei,\nK. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel, “Palm:\nScaling language modeling with pathways,” 2022.\n[16] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen et al., “Palm 2 technical report,”\narXiv preprint arXiv:2305.10403 , 2023.\n[17] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le,\nD. Zhou et al., “Chain-of-thought prompting elicits reasoning in large\nlanguage models,” Advances in Neural Information Processing Systems ,\nvol. 35, pp. 24 824–24 837, 2022.\n[18] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for\nautomatic evaluation of machine translation,” in Proceedings of the 40th\nannual meeting of the Association for Computational Linguistics , 2002,\npp. 311–318.\n[19] S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt evaluation\nwith improved correlation with human judgments,” in Proceedings of the\nacl workshop on intrinsic and extrinsic evaluation measures for machine\ntranslation and/or summarization , 2005, pp. 65–72.\n[20] C.-Y . Lin, “Rouge: A package for automatic evaluation of summaries,”\nin Text summarization branches out , 2004, pp. 74–81.\n[21] J. Novikova, O. Dušek, A. C. Curry, and V . Rieser, “Why we need new\nevaluation metrics for nlg,” arXiv preprint arXiv:1707.06875 , 2017.\n[22] T. Pang, P. Li, and L. Zhao, “A survey on automatic generation of medical\nimaging reports based on deep learning,” BioMedical Engineering OnLine,\nvol. 22, no. 1, pp. 1–16, 2023.\n[23] M. Kilickaya, A. Erdem, N. Ikizler-Cinbis, and E. Erdem, “Re-\nevaluating automatic metrics for image captioning,” arXiv preprint\narXiv:1612.07600, 2016.\n[24] M. Sokolova and G. Lapalme, “A systematic analysis of performance\nmeasures for classification tasks,” Information processing & management,\nvol. 45, no. 4, pp. 427–437, 2009.\n[25] M. C. Chen, R. L. Ball, L. Yang, N. Moradzadeh, B. E. Chapman, D. B.\nLarson, C. P. Langlotz, T. J. Amrhein, and M. P. Lungren, “Deep learning\nto classify radiology free-text reports,” Radiology, vol. 286, no. 3, pp.\n845–852, 2018.\n[26] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training of\ndeep bidirectional transformers for language understanding,” in NAACL-\nHLT (1). Association for Computational Linguistics, 2019, pp. 4171–\n4186.\n[27] S. Tian, Q. Jin, L. Yeganova, P.-T. Lai, Q. Zhu, X. Chen, Y . Yang,\nQ. Chen, W. Kim, D. C. Comeau et al., “Opportunities and challenges\nfor chatgpt and large language models in biomedicine and health,” arXiv\npreprint arXiv:2306.10070, 2023.\n[28] T. Kocmi and C. Federmann, “Large language models are state-of-the-art\nevaluators of translation quality,” CoRR, vol. abs/2302.14520, 2023.\n[29] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang,\n“Is chatgpt a general-purpose natural language processing task solver?”\nCoRR, vol. abs/2302.06476, 2023.\n[30] Y . Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia,\nZ. Ji, T. Yu, W. Chung, Q. V . Do, Y . Xu, and P. Fung, “A multitask,\nmultilingual, multimodal evaluation of chatgpt on reasoning, hallucination,\nand interactivity,” CoRR, vol. abs/2302.04023, 2023.\n[31] W. Jiao, J. Huang, W. Wang, X. Wang, S. Shi, and Z. Tu, “Parrot:\nTranslating during chat using large language models,” CoRR, vol.\nabs/2304.02426, 2023.\n[32] S. Gao, Z. Yao, C. Tao, X. Chen, P. Ren, Z. Ren, and Z. Chen,\n“Umse: Unified multi-scenario summarization evaluation,” arXiv preprint\narXiv:2305.16895, 2023.\n[33] J. Wang, Y . Liang, F. Meng, Z. Li, J. Qu, and J. Zhou, “Cross-lingual\nsummarization via chatgpt,” CoRR, vol. abs/2302.14229, 2023.\n[34] Q. Wang, Q. Zeng, L. Huang, K. Knight, H. Ji, and N. F. Rajani,\n“Reviewrobot: Explainable paper review generation based on knowledge\nsynthesis,” in INLG. Association for Computational Linguistics, 2020,\npp. 384–397.\n[35] K. Jeblick, B. Schachtner, J. Dexl, A. Mittermeier, A. T. Stüber, J. Topalis,\nT. Weber, P. Wesp, B. O. Sabel, J. Ricke, and M. Ingrisch, “Chatgpt\nmakes medicine easy to swallow: An exploratory case study on simplified\nradiology reports,” CoRR, vol. abs/2212.14882, 2022.\n[36] H. Rao, C. Leung, and C. Miao, “Can chatgpt assess human personalities?\nA general evaluation framework,” CoRR, vol. abs/2303.01248, 2023.\n[37] Q. Zhu, T. S. Mathai, P. Mukherjee, Y . Peng, R. M. Summers, and\nZ. Lu, “Utilizing longitudinal chest x-rays and reports to pre-fill radiology\nreports,” arXiv preprint arXiv:2306.08749 , 2023.\n[38] M. R. Segal, “Machine learning benchmarks and random forest regres-\nsion,” 2004.\n[39] J. H. Moon, H. Lee, W. Shin, Y .-H. Kim, and E. Choi, “Multi-\nmodal understanding and generation for medical images and text via\nvision-language pre-training,” IEEE Journal of Biomedical and Health\nInformatics, vol. 26, no. 12, pp. 6070–6080, 2022.\n[40] M. G. Kendall, “A new measure of rank correlation,” Biometrika, vol. 30,\nno. 1/2, pp. 81–93, 1938.\n[41] A. J. Viera, J. M. Garrett et al., “Understanding interobserver agreement:\nthe kappa statistic,” Fam med, vol. 37, no. 5, pp. 360–363, 2005.\n[42] K. Pearson, “Vii. note on regression and inheritance in the case of two\nparents,” proceedings of the royal society of London, vol. 58, no. 347-352,\npp. 240–242, 1895.\nAPPENDIX\nTable V presents both the Simplistic and Detailed versions\nof task instructions that guided LLMs through the evaluation\nprocess using in-context learning. Table VI provides a template\nused during the evaluation, while Table VII showcases the\ninstructions and template for the optimized process. Table VIII\nprovides Kendall’s Tau values and corresponding p-values for\nvarious regressed models. Random Forest performs best. Also,\nthe various regression models exhibit comparable performance,\ndemonstrating the robustness of our method.\nInstructions\nSimplistic Your task is to evaluate the prediction sentence by sentence, comparing it with the original report. You should also have one\noverall score from 0-5 for prediction sentences with respect to the original report based on your subjective impression. And I\nonly need the csv format for the output.\nDetailed In this task, you are required to evaluate pairs of medical reports: an original report and a predicted report. The original\nreport is the benchmark, containing confirmed and accurate information about a patient’s condition derived from radiological\nstudies. The predicted report, generated by another model, requires assessment for its degree of accuracy. Each report includes\nseveral points or observations about the patient’s health, designated by identifiers such as “a” , “b” , “c” for the original\nreport and “A” , “B” , “C” for the predicted report. Your objective is to appraise the accuracy of each point in the predicted\nreport concerning the equivalent point in the original report. Please adhere to the following scoring guidelines: \"\"\nScore of 1:\nThe predicted statement matches the original in meaning and details. Even if words are different, the message is the same.\nNo information from the original is missing or contradicted.\nScore of 0.5:\nThe predicted statement somewhat resembles the original statement but not entirely. Some elements align, but others might\ndiffer or be missing. The core idea might be there, but not all details are accurate.\nScore of -1:\nThe predicted statement goes against or is the opposite of the original statement. The information presented directly contradicts\nwhat’s in the original report.\nScore of 0:\nThe predicted statement doesn’t have a matching original statement to compare with.\nBeyond the detailed scoring for each prediction statement, you are also required to give an overall score ranging from 0 to 5\nfor all the prediction sentences with respect to the original report. Your task is not only to find errors or omissions, but also to\nassess the clinical relevance, accuracy, and potential harm of the AI’s outputs. Please bear in mind that incorrect information\nin a clinical setting could be potentially more damaging than missing information. It’s not just about comparing the AI and\nhuman reports line by line. Please consider the whole report and its coherence, the context of the patient’s condition, and\nthe potential impact of the AI statements on clinical decisions. Remember, you’re evaluating these reports as if they were\nto be used in a real-world clinical scenario. After your evaluation, please provide an overall impression score of the AI’s\nreport from 0 to 5, where 0 is ’poor’ and 5 is ’excellent’. Keep in mind that these reports could be used by other healthcare\nprofessionals (such as physicians, surgeons, and nurses) and mistakes or unclear information could lead to wrong treatment\ndecisions. Your insights as a clinician are crucial in refining this AI tool and improving patient outcomes.\nTABLE V: Simplistic and Detailed Instructions provided by radiologists for the LLMs to evaluate medical report predictions.\nA Template\nInput Original “a. In comparison with a series of images from and there has been progressive decrease in the pleural fluid in the left\nhemithorax though some persists b. Elevation of the hemidiaphragm with mild shift of the mediastinum to the left is consistent\nwith previous surgery c. The right lung is clear and there is no vascular congestion.”\nPrediction “A. PA and lateral views of the chest were reviewed and compared to the prior studies B. Elevation of the left\nhemidiaphragm is unchanged since C. The right lung is clear D. There is no pulmonary edema vascular congestion pleural\neffusion or pneumothorax E. The cardiac and mediastinal contours are normal F. There are no concerning osseous or soft tissue\nlesions.”\nOutput Original ,a,in comparison with a series of images from and there has been progressive decrease in the pleural fluid in the left\nhemithorax though some persists,D,0.5,\nOriginal,b,elevation of the hemidiaphragm with mild shift of the mediastinum to the left is consistent with previous surgery,B,1,\nOriginal,c,the right lung is clear and there is no vascular congestion .,“C,D”,1,\nOriginal,A,pa and lateral views of the chest were reviewed and compared to the prior studies,a,0.5,\nOriginal,B,elevation of the left hemidiaphragm is unchanged since,b,1,\nOriginal,C,the right lung is clear,c,1,\nOriginal,D,there is no pulmonary edema vascular congestion pleural effusion or pneumothorax,“a,c”,0.5,\nOriginal,E,the cardiac and mediastinal contours are normal,0,\nOriginal,F,there are no concerning osseous or soft tissue lesions .,0,\n„-„,3.5/5, “The AI-generated report is quite consistent with the original report. Both reports note the elevation of the left\nhemidiaphragm and the clear state of the right lung. However, there are minor differences. The original report mentions a\nprogressive decrease in the pleural fluid in the left hemithorax, which the AI does not explicitly point out. Instead, the AI\nreport states there is no pleural effusion, which might imply the resolution of previous fluid. Additionally, the AI report adds\nobservations on cardiac and mediastinal contours and the absence of concerning osseous or soft tissue lesions, which are not\nmentioned in the original report.”\nTABLE VI: Template for Evaluating Medical Reports: This table showcases the evaluation process where reports are input and\nthe results are output in a CSV format.\nInstruction You are provided with pairs of chest X-ray reports and their corresponding reviews. Based on the observations and corrections\nmentioned in the reviews, your task is to create a revised report for each original report. Ensure that the revised report\naccurately reflects the observations from the review, including additions, omissions, and corrections. For each report: Begin\nwith a label like “Revised Report,”,Follow it with the revised content. Your goal is to ensure that the revised report is clear,\nconcise, and written in a clinically appropriate manner using correct medical terminologies.\nTemplate Report,“pa and lateral chest views were obtained with patient in upright position . analysis is performed in direct comparison\nwith the next preceding similar study of . the heart size remains normal . no configurational abnormality is identified\n. thoracic aorta unremarkable . the pulmonary vasculature is not congested . no signs of acute or chronic parenchymal\ninfiltrates are present and the lateral and posterior pleural sinuses are free . no pneumothorax in apical area . skeletal\nstructures of the thorax grossly unremarkable .”\nReview,“The AI report correctly notes the absence of pleural effusions, pneumothorax, and normal cardiac and mediastinal\ncontours. However, it does not mention the right upper lobe consolidation with air bronchograms, the new focal tubular\nlucency within the opacity, and the progressed opacity in the right lower lobe. Furthermore, the AI report fails to capture\nthe original report’s note on the mild thickening of the left major fissure. Finally, the AI report adds unnecessary details not\nmentioned in the original report, such as the unremarkable thoracic aorta and the absence of acute or chronic parenchymal\ninfiltrates.”\nRefined Report,“pa and lateral chest radiographs were obtained . a right upper lobe consolidation with air bronchograms\nis similar to . focal tubular lucency within the opacity is new and may reflect cavitation dilated airways or spared lung\nparenchyma . opacity in the right lower lobe has progressed since the prior study . there is no effusion or pneumothorax .\ncardiac and mediastinal contours are normal . there is mild thickening of the left major fissure . ”\nTABLE VII: Instruction and Example Template for the refined process. This table provides guidelines on how to integrate\nobservations and corrections from reviews into the AI generated reports to produce clinically accurate refined reports. The\ntemplate illustrates the refined process with an example.\nKendall’s Tau Value P-Value\nDecision Tree 0.5659 1.5435e-13\nSupport Vector Machine 0.6175 1.0276e-17\nK-Nearest Neighbors 0.6204 2.4497e-17\nNeural Network 0.6216 6.0731e-18\nGradient Boosting 0.6231 9.8870e-19\nRandom Forest 0.6372 9.3613e-19\nTABLE VIII: Kendall’s Tau values and corresponding p-values for various regressed models.",
  "topic": "Radiology",
  "concepts": [
    {
      "name": "Radiology",
      "score": 0.4779665172100067
    },
    {
      "name": "Medicine",
      "score": 0.4554230570793152
    },
    {
      "name": "Medical physics",
      "score": 0.3558046519756317
    },
    {
      "name": "Medical education",
      "score": 0.3336245119571686
    }
  ],
  "cited_by": 5
}