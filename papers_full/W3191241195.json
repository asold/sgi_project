{
  "title": "Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer",
  "url": "https://openalex.org/W3191241195",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5057760050",
      "name": "Zhihe Lu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5035606153",
      "name": "Sen He",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5028643592",
      "name": "Xiatian Zhu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100425448",
      "name": "Li Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5046046128",
      "name": "Yi-Zhe Song",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5014436524",
      "name": "Tao Xiang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3096805028",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W3047258141",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W2952865063",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3108189450",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W3033502887",
    "https://openalex.org/W3109083691",
    "https://openalex.org/W2798836702",
    "https://openalex.org/W2963599420",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2964105864",
    "https://openalex.org/W2983850069",
    "https://openalex.org/W2963078159",
    "https://openalex.org/W2893918048",
    "https://openalex.org/W3108975329",
    "https://openalex.org/W3108187451",
    "https://openalex.org/W3108878460",
    "https://openalex.org/W3118439879",
    "https://openalex.org/W2963341924",
    "https://openalex.org/W1507506748",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3012255272",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963680240",
    "https://openalex.org/W3106906018",
    "https://openalex.org/W3006505669",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2981787211",
    "https://openalex.org/W2990230185",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "A few-shot semantic segmentation model is typically composed of a CNN encoder, a CNN decoder and a simple classifier (separating foreground and background pixels). Most existing methods meta-learn all three model components for fast adaptation to a new class. However, given that as few as a single support set image is available, effective model adaption of all three components to the new class is extremely challenging. In this work we propose to simplify the meta-learning task by focusing solely on the simplest component, the classifier, whilst leaving the encoder and decoder to pre-training. We hypothesize that if we pre-train an off-the-shelf segmentation model over a set of diverse training classes with sufficient annotations, the encoder and decoder can capture rich discriminative features applicable for any unseen classes, rendering the subsequent meta-learning stage unnecessary. For the classifier meta-learning, we introduce a Classifier Weight Transformer (CWT) designed to dynamically adapt the supportset trained classifier's weights to each query image in an inductive way. Extensive experiments on two standard benchmarks show that despite its simplicity, our method outperforms the state-of-the-art alternatives, often by a large margin.Code is available on https://github.com/zhiheLu/CWT-for-FSS.",
  "full_text": "Simpler is Better: Few-shot Semantic Segmentation with Classiﬁer Weight\nTransformer\nZhihe Lu1,2 Sen He1,2 Xiatian Zhu1 Li Zhang3 Yi-Zhe Song1,2 Tao Xiang1,2\n1CVSSP, University of Surrey\n2iFlyTek-Surrey Joint Research Centre on Artiﬁcial Intelligence\n3School of Data Science, Fudan University\n{zhihe.lu, sen.he, xiantian.zhu, y.song, t.xiang}@surrey.ac.uk, lizhangfd@fudan.edu.cn\nAbstract\nA few-shot semantic segmentation model is typically\ncomposed of a CNN encoder, a CNN decoder and a simple\nclassiﬁer (separating foreground and background pixels).\nMost existing methods meta-learn all three model compo-\nnents for fast adaptation to a new class. However, given\nthat as few as a single support set image is available, ef-\nfective model adaption of all three components to the new\nclass is extremely challenging. In this work we propose to\nsimplify the meta-learning task by focusing solely on the\nsimplest component – the classiﬁer, whilst leaving the en-\ncoder and decoder to pre-training. We hypothesize that if\nwe pre-train an off-the-shelf segmentation model over a set\nof diverse training classes with sufﬁcient annotations, the\nencoder and decoder can capture rich discriminative fea-\ntures applicable for any unseen classes, rendering the sub-\nsequent meta-learning stage unnecessary. For the classi-\nﬁer meta-learning, we introduce a Classiﬁer Weight Trans-\nformer (CWT) designed to dynamically adapt the support-\nset trained classiﬁer’s weights to each query image in an in-\nductive way. Extensive experiments on two standard bench-\nmarks show that despite its simplicity, our method outper-\nforms the state-of-the-art alternatives, often by a large mar-\ngin. Code is available on https://github.com/zhiheLu/CWT-\nfor-FSS.\n1. Introduction\nSemantic segmentation has achieved remarkable\nprogress in the past ﬁve years thanks to the availability\nof large-scale labeled datasets and advancements in deep\nlearning algorithms [5, 7, 21]. Nonetheless, relying on\nmany training images with exhaustive pixel-level anno-\ntation for every single class, existing methods have poor\nscalability to new classes. Indeed, the high annotation\ncost has hindered the general applicability of semantic\nsegmentation models. For instance, creating the COCO\nSupport\nQuery\nQuery\nSupport\nEncoder\nClassifier\nClassifier\n(a) Existing methods\n(b) Ours\nDecoder\nDecoder\nEncoder\nTrain a linear classifier\nSupport Mask\nA\nPrediction\nSupport Mask\nTraining A\nAdapted \nClassifier\nTraining\nA Adaptive module\nMasked average pooling\nMeta-learned\nMeta-learned\nPre-trained\nPrediction\nFigure 1. Illustrating the model training difference for 1-shot sce-\nnario between (a) existing few-shot segmentation methods and (b)\nours. A few-shot segmentation model is typically composed of\na deep CNN encoder, a deep CNN decoder and a much simpler\nclassiﬁer. (a) Previous training methods usually meta-learn all the\nthree parts ( i.e., the whole model), so that all three can adapt to\na new class represented by an annotated support set image to per-\nform segmentation on a query image. This adaptation is intrinsi-\ncally difﬁcult and sub-optimal due to complex model design and\nrather limited supervision available for the adaptation. Instead,\n(b) we propose to meta-learn the simple classiﬁer part only whilst\npre-training and then freezing the encoder and decoder, making\nfew-shot adaptation to new classes much more tractable.\ndataset [17] took over 70,000 worker hours even for only\n80 common object categories. Inspired by the signiﬁcant\nefforts in few-shot image classiﬁcation [27, 11, 29, 37],\nfew-shot learning has been introduced into semantic seg-\nmentation recently [25, 22, 3, 34, 36, 40, 41]. A few-shot\nsegmentation method eliminates the need of labeling a\nlarge set of training images. This is typically achieved by\nmeta learning which enables the model to adapt to a new\nclass represented by a support set consisting of as few as a\nsingle image.\nA fundamental challenge faced by any few-shot segmen-\narXiv:2108.03032v3  [cs.CV]  12 Aug 2021\nAirplane Bicycle BoatBird\nFigure 2. Examples of large intra-class variation in unconstrained\nimages. It is evident that the objects from the same class (in each\ncolumn) may look rather different due to uncontrolled changes in\nspatial size, viewpoint, style, and occlusion.\ntation method is how to effectively adapt a complex image\nsegmentation model to a new class represented by a small\nsupport set composed of only few images. More speciﬁ-\ncally, most recent segmentation models are deep CNNs with\ncomplex architectures consisting of three parts: a CNN en-\ncoder, a CNN decoder and a classiﬁer. Among the three,\nthe classiﬁer, which labels each pixel into either foreground\nclasses or background, is much simpler compared to the\nother two – in most cases, it is a 1 ×1 convolutional layer\nwith the parameter number doubling the pixel feature di-\nmension.\nAs shown in Figure 1(a), existing few-shot segmenta-\ntion models [25, 36, 40, 41] aim to meta-learn all three\nparts. Concretely, existing few-shot segmentation methods\nmostly adopt an episodic training strategy. In each train-\ning episode, one training class is sampled with a small sup-\nport set and query images to imitate the setting for testing.\nOnce learned, given a new class with a fully annotated sup-\nport set and an unannotated query image, all three parts are\nexpected to adapt to the new class so that foreground and\nbackground pixels can be separated accurately in the query\nimage. Note that both the encoder and decoder are deep\nCNNs, e.g., VGG-16 [26] (15M parameters without fully-\nconnected (FC) layers) or ResNet-50/101 [15] (24M/43M\nparameters without FC layers) for encoder, and ASPP [6]\n(3.4M parameters) for decoder. Effectively adapting the full\nmodel, especially the encoder and decoder, is thus a daunt-\ning task, hindering the performance of the existing models.\nTo overcome the above fundamental limitations with ex-\nisting few-shot segmentation methods, we conjecture that\nthe key is tosimplify the meta-learning taskso that few-shot\nlearning becomes more tractable and hence more effective.\nTo this end, we propose to focus meta-learning on the sim-\nplest and latest stage of the three-part pipeline – the classi-\nﬁer whilst leaving the training of the encoder and decoder to\na pre-training stage (see Figure 1(b)). Once trained, only the\nclassiﬁer needs to be adapted to the new class with the rest\nof the model frozen, drastically reducing the complexity of\nmodel adaptation. Our assumption is that if we pre-train an\noff-the-shelf segmentation model over a set of diverse train-\ning classes, the encoder and decoder can already capture a\nrich set of discriminative segmentation features suitable for\nnot only the training classes but also the unseen test classes,\ni.e., being class-agnostic. We need to then focus on adapt-\ning the classiﬁer part alone. Indeed, we found that if we\nsimply do the pre-training and use the support set of a new\nclass to train a classiﬁer ( i.e., without meta-learning), the\nresult is already comparable to that obtained by the state-\nof-the-art methods (see Sec. 4). However, this naive base-\nline cannot adapt to each query image, which is critical for\nour problem due to the large intra-class variation (Figure\n2). Without sufﬁcient training samples in the support set\nto accommodate this intra-class variation, a few-shot seg-\nmentation model must adapt to each query image as well.\nWe hence further propose a novel meta-learning framework\nthat employs a Classiﬁer Weight Transformer (CWT) to dy-\nnamically adapt the support-set trained classiﬁer’s weights\nto each query image in an inductive way, i.e., adaptation\noccurs independently on each query image.\nWe make the following contributions in this work: (1)\nWe propose a novel model training paradigm for few-shot\nsemantic segmentation. Instead of meta-learning the whole,\ncomplex segmentation model, we focus on the simplest\nclassiﬁer part to make new-class adaptation more tractable.\n(2) We introduce a novel meta-learning algorithm that lever-\nages a Classiﬁer Weight Transformer (CWT) for adapt-\ning dynamically the classiﬁer weights to every query sam-\nple. (3) Extensive experiments with two popular back-\nbones (ResNet-50 and ResNet-101) show that the proposed\nmethod yields a new state-of-the-art performance, often sur-\npassing existing alternatives, especially on 5-shot case, by a\nlarge margin. Further, under a more challenging yet practi-\ncal cross-domain setting, the margin becomes even bigger.\n2. Related Work\n2.1. Few-shot Learning\nFew-shot learning (FSL) aims to learn to learn a model\nfor a novel task with only a handful of labeled samples.\nThe majority of existing FSL works adopt the meta-learning\nparadigm [24] and are mostly focused on image classiﬁca-\ntion [33, 11, 27, 2, 29, 12, 19, 28, 13]. As for which part\nof a classiﬁcation model is meta-learned, this varies in dif-\nferent works including feature representation [27] and dis-\ntance metrics [29]. Beyond image classiﬁcation, this learn-\ning paradigm can be applied to many different computer vi-\nsion problems including semantic segmentation as investi-\ngated in this work.\n2.2. Few-shot Semantic Segmentation\nRecently, meta-learning has been introduced into seman-\ntic segmentation for addressing the same few-shot learning\nchallenge [25]. A semantic segmentation system generally\nconsists of three parts: an encoder, a decoder and a clas-\nsiﬁer. To incorporate meta-learning, a common strategy\nexisting works consider involves two steps: ﬁrst relating\nthe support-set and query-set image features from the en-\ncoder, and then updating all three parts by minimizing a\nloss measuring the discrepancy between the prediction and\nthe ground-truth of query samples. In terms of how to re-\nlate the support and query images, there exist two different\nways: prototypical learning [8, 34, 20], and feature con-\ncatenation [3, 1, 40, 36]. PL [8] is the ﬁrst work introduc-\ning prototypical learning into few-shot segmentation which\npredicts foreground/background classes by similarity com-\nparison to prototypes. PANet [34] further introduced a pro-\ntotype alignment regularization to do bidirectional proto-\ntypical learning. Recently, PPNet [20] emphasized the im-\nportance of ﬁne-grained features and proposed part-aware\nprototypes. By contrast, feature concatenation based meth-\nods ﬁrst combine prototypes and query features, and then\nutilize a segmentation head, e.g., ASPP [6] and PPM [42],\nfor the ﬁnal prediction. Despite the differences in model de-\nsign, existing methods share a common characteristic, i.e.,\nthey all attempt to update the whole complex model with\njust a few samples during meta-learning. This may cause\noptimizing difﬁculty as we mentioned before. To overcome\nthis issue, we propose to only meta-learn the classiﬁer dur-\ning meta-training.\nTo address the intra-class variation challenge, we fur-\nther introduce a Classiﬁer Weight Transformer (CWT) to\nadapt the support-set trained classiﬁer to every query im-\nage. Our CWT is based on the self-attention mechanism\n[32]. Motivated by the great success in NLP, researchers\nhave started to employ self-attention for vision tasks such\nas object detection [16, 4], and image classiﬁcation [35, 9].\nThe closest work to ours is FEAT [37] which leverages a\nprototype Transformer to calibrate the relationships of dif-\nferent classes for few-shot image classiﬁcation. However, in\nthis work we explore the Transformer differently for tack-\nling the intra-class variation problem in few-shot segmen-\ntation.\n3. Methodology\n3.1. Task Deﬁnition\nWe adopt the standard few-shot semantic segmentation\nsetting [25, 3]. Given a meta-test dataset Dtest, we sample\na target task with K-shot labeled images ( i.e., the support\nset) and several test images ( i.e., the query set) from one\nrandom class and test a learned segmentation model θ. The\nobjective is to segment all the objects of the new class in\nthe query images. To train the model θin a way that it can\nperform well on those sampled segmentation tasks, episodic\ntraining is adopted to meta-learn the model. Concretely, a\nlarge number of such tasks are randomly sampled from a\nmeta-training set Dtrain, and then used to train the model in\nan episodic manner.\nIn each episode, we start with sampling one class cfrom\nDtrain at random, from which labeled training samples are\nthen randomly drawn to create a support set Sand a query\nset Qwith K and Q samples, respectively. Formally, the\nsupport and query sets are deﬁned as:\nS= {(xi,Mi)}K\ni=1, Q= {(xj,Mj)}Q\nj=1, (1)\nwhere Mi/j denotes the ground-truth mask. Note, S∩Q =\n∅are sample-wise disjoint and Qis 1 for the currently stan-\ndard setting.\nWe conduct episodic training in a two-loop manner [27]:\nthe support set is ﬁrst used in the inner loop to construct a\nclassiﬁer for the sampled class, and the query set is then uti-\nlized in the outer loop to adapt the classiﬁer with a Classiﬁer\nWeight Transformer (CWT).\nThe key is to obtain a learner able to recognize any\nnovel class with only a few labeled samples. Compared\nwith the standard segmentation setup, this is a more chal-\nlenging task due to lacking sufﬁcient supervision for new\ntarget classes. Dtrain and Dtest contain base classes Cbase\nand novel classes Cnovel, which are mutually disjoint, i.e.,\nCbase ∩Cnovel = ∅. Unlike the sparsely annotated meta-\ntest classes, each meta-training class comes with abundant\nlabeled training data so that sufﬁcient episodes for model\nmeta-training can be formed.\n3.2. Model Architecture\nA few-shot segmentation model generally consists of\nthree modules: an encoder, a decoder and a classiﬁer. For\nlearning to adapt to a new class, existing methods typically\nmeta-learn the entire model after the encoder is pre-trained\non ImageNet [23]. During the episodic training stage, all\nthree parts of the model are meta-learned. Once trained,\ngiven a new class with annotated support set images and\nquery images for test, the model is expected to adapt all\nthree parts to the new class. With only few annotated sup-\nport set images and the complex and interconnected three\nparts, this adaptation is often sub-optimal.\nTo overcome these limitations we propose a simple yet\neffective training paradigm in two stages. In the ﬁrst stage,\nwe pre-train the encoder and decoder for stronger feature\nrepresentation with supervised learning. In the second\nstage, together with the frozen encoder and decoder we\nmeta-train the classiﬁer only. This is because we consider\nthe pre-trained feature representation parts (i.e., the encoder\nand decoder) are sufﬁciently generalizable to any unseen\nclasses; the key for few-shot semantic segmentation would\nEncoder Decoder\nClassifier\nQ\nSupport Image\nQuery Image\nShared Weights\nQuery Feature\nClassifier Weights\nSupport Mask\nQuery Conditioned \nWeights\nPrediction\nEncoder Decoder\nLinear Classifier Training\nK\nV\nQuery Feature\nClassifier Adaptation with Transformer\nLinear \nLayer\nResidual connection\nMulti-head Attention\nLinear \nLayer\nLinear \nLayer\nLayer \nNorm\nFeature extraction without updating\nTrain a linear classifier\nClassifier adaptation\nQ, K, V: query, key and value for transformer\nSum operation\nConv operation\nTraining\nTraining\nFigure 3. Schematic overview of the proposed few-shot semantic segmentation method. Our model is trained in two stages. In the ﬁrst\nstage, we pre-train the encoder and decoder on the base classes in a standard supervised learning manner. In the second stage, given\nan episode we froze the encoder and decoder, initialize the classiﬁer on the support set, and meta-learn a Classiﬁer Weight Transformer\n(CWT) to udpate the classiﬁer for each query image. During meta-testing, the classiﬁer is ﬁrst trained on the support set, then updated by\nthe frozen CWT to adapt to any query image, and ﬁnally applied to that query image for segmentation.\nthus be in adapting the binary classiﬁer (separating fore-\nground and background pixels) rather than the entire model\nfrom few-shot samples. The overview of our method is de-\npicted in Figure 3.\n3.3. Stage 1: Model Pre-training\nAs in all existing few-shot semantic segmentation mod-\nels [25, 3], one of the key objectives is to learn the fea-\nture representation parts ( i.e., the encoder and decoder)\nthrough meta learning, so that it can generalize to any un-\nseen classes. For example, the state-of-the-art RPMMs\nmodel [36] was directly meta-trained with the encoder pre-\ntrained on ImageNet. However, in most recent few-shot\nlearning methods for static image classiﬁcation [37, 38],\npre-training the feature network on the whole meta-training\nset before episodic training starts has become a standard\nstep. In this work, we also adopt such a pre-training step and\nshow in our experiments that this step is vital (see Sec. 4.5).\nSpeciﬁcally, we use the PSPNet [42] as our backbone\nsegmentation model. It is then pre-trained on the whole\ntraining set Dtrain with the cross-entropy loss. Training de-\ntails are provided in Sec 4.2.\n3.4. Stage 2: Classiﬁer Meta-learning with Classi-\nﬁer Weight Transformer (CWT)\nAfter the pre-training stage, the encoder and decoder can\nbe simply frozen for any different few-shot tasks. Since any\nnew task involves a previously unseen class, the classiﬁer\nhas to be learned. To this end, an intuitive and straightfor-\nward method is to optimize the classiﬁer weights w with\nthe labeled support set S. With the pre-trained encoder\nand decoder we ﬁrst extract the feature vectors f ∈ Rd\nfor every support-set image pixel. The feature dimension is\ndenoted as d. Same as in pre-training, we then adopt the\ncross-entropy loss function to train the classiﬁer model w.\nAs the feature representation is considered to be class\ngeneric, they can be used directly with the newly trained\nclassiﬁer for meta-test without going through the episodic\ntraining process. Especially, after seeing sufﬁcient diverse\nclasses, it can work well when the task is to separate fore-\nground and background pixels. Indeed, our experiments\nshow that this turns out to be a surprisingly strong base-\nline that even outperforms a state-of-the-art PPNet [20] (see\nTables 1 and 5). This veriﬁes for the ﬁrst time that good fea-\nture representation (or feature reuse) is similarly critical for\nfew-shot semantic segmentation modeling – a ﬁnding that\nhas been reported in recent static image few-shot learning\nworks [30, 18].\nNonetheless, this baseline is still limited for few-shot\nsegmentation since it cannot adapt to every query image\nwhereby the target object may appear drastically dissimilar\nto the ones in the support-set images. To that end, we next\nintroduce our meta-learning algorithm that learns a Classi-\nﬁer Weight Transformer (CWT) for query object adaptation.\nDuring episodic training, we aim to learn via our CWT\nhow to adapt the classiﬁer weights w∈R2×d to a sampled\nclass in each episode. Formally, the input to our transformer\nis in the triplet form of ( Query, Key, Value). We start\nwith extracting the feature F ∈Rn×d for all n pixels of\nthe query image using the encoder and decoder. To learn\ndiscriminative query conditioned information, the input is\ndesigned as:\nQuery = wWq, Key = FWk, Value = FWv, (2)\nwhere Wq/Wk/Wv ∈Rd×da are the learnable parameters\n(each represented by a fully connected layer) that project the\nclassiﬁer weights and query feature to a da-D latent space.\nTo adapt the classiﬁer weights to the query image, we form\na classiﬁer-to-query-image attention mechanism as:\nw∗= w + ψ(softmax(wWq(FWk)⊤\n√da\n)(FWv)), (3)\nwhere softmax(·) is a row-wise softmax function for atten-\ntion normalization and ψ(·) is a linear layer with the input\ndimension da and output dimension d. Residual learning is\nadopted for more stable model convergence.\nAs written in Eq. (3), pairwise similarity deﬁnes the at-\ntentive scores between the classiﬁer weight and query image\npixels, and is further used for weighted aggregation in the\nValue space. This adapts the classiﬁer weight to the query\nimage. The intuition is that, the pairs involving a query im-\nage pixel from the new class often enjoy higher similarity\nthan those with background classes except few outlier in-\nstances; as a result, this attentive learning would reinforce\nthis desired proximity and adjust the classiﬁer weights con-\nditioned on the query. Consequently, the intra-class varia-\ntion can be mitigated.\nLearning objective Once the classiﬁer weight w∗ is\nadapted to a query sample, we then apply it for segmenta-\ntion prediction on the corresponding featureF. To train our\ntransformer, a cross-entropy loss can be derived from the\nquery-image ground-truth and the prediction as the meta-\ntraining objective.\nUnseen class adaptation During meta-testing, given\nany new task/class this proposed transformer can be directly\napplied to condition the classiﬁer’s weights ﬁrst optimized\non the support set to any given query image. Note that both\nsupport set and query images are used as input to our trans-\nformer to update the foreground/background classiﬁer for\nthe new unseen foreground class. However, the transformer\nparameter is ﬁxed after meta-training and the adaptation is\ndone for each query image independently, i.e., in an in-\nductive manner as in most existing few-shot segmentation\nworks.\n4. Experiments\n4.1. Datasets and Settings\nIn our experiments, two standard few-shot semantic seg-\nmentation datasets are used.\nCOCO-20i is currently the largest and most challeng-\ning dataset for few-shot semantic segmentation. It pro-\nvides the train/val sets including 82,081/40,137 images in\n80 classes, built from the popular COCO [17] benchmark.\nFollowing [22] we divide the 80 classes into 4 splits i ∈\n{0,1,2,3}, each of which contains 20 classes. In a sin-\ngle experiment, three class splits are selected as the base\nclasses for training whilst the remaining class split for test-\ning. Therefore a total of four experiments are conducted.\nPASCAL-5i is the extension of PASCAL VOC 2012 [10]\nwith extra annotations from SDS dataset [14]. The train\nand val sets contain 5,953 and 1,449 images, respectively.\nThere are 20 categories in both the training set and the vali-\ndation set. Following [25] we make 4 class splits each with\n5 classes and design the experiments in a similar protocol\nas COCO-20i.\n4.2. Implementation Details and Metrics\nPre-training To obtain a strong encoder and decoder, we\nadopt the standard supervised learning for semantic seg-\nmentation on base classes, i.e., 16/61 base classes (in-\ncluding the background class) in each split of PASCAL-\n5i/COCO-20i. We choose PSPNet [42] as our baseline\nsegmentation model. For fair comparisons with existing\nmethods, we tested two common backbones, ResNet-50 and\nResNet-101 [15]. We also present the results with VGG-16\nin the supplementary. We train a model for 100 epochs on\nPASCAL-5i and 20 epochs on COCO-20i. We set the batch\nsize to 12, the image size to 417. The objective function is\nthe cross-entropy loss. We use the SGD optimizer with the\nmomentum 0.9, the weight decay 1e-4, the initial learning\nrate 2.5e-3, and the cosine learning rate scheduler. We set\nthe label smoothing parameter ϵto 0.1. For data augmenta-\ntion, we only use random horizontal ﬂipping.\nEpisodic Training After pre-training, we froze the en-\ncoder and decoder in the subsequent episodic training. In\nthis stage, we form the training data of base classes into\nepisodes, each including a support set and a query set from\na randomly selected class. We ﬁrst train a new classiﬁer for\nthe selected class for 200 iterations on the support set us-\ning the SGD optimizer and cross-entropy loss function. The\nlearning rate 1e-1 is used. Next, we train the proposed CWT\nonce at learning rate of 1e-3. There are total 20 epochs for\nabove inner- and outer- loop optimization. Our transformer\nhas a shared linear layer with dimension, 512 ×2048, for\nprojecting the inputs to latent space, a 4-head attention mod-\nule and a fully connected layer recovering the dimension\nto 512. In addition, a dropout layer for stable training and\nlayer normalization are used. It outputs query-adaptive clas-\nsiﬁer weights which will be used to predict every pixel of\nthe query images. In meta-learning setup, the classiﬁer re-\nsides in the inner loop whilst the transformer is in the outer\nloop.\nEvaluation Metrics We use the class mean intersection\nover union (mIoU) as the evaluation metric. Speciﬁcally,\nmIoU is computed over averaging the IoU rates of each\nclass. Following [20], we report all the results averaged\nacross 5 trials. For each trial, we test 1,000 episodes. We\nreport the results for every single split and their average.\n4.3. Single Domain Evaluation\n4.3.1 COCO- 20i Results\nIn Table 1 we compare the segmentation results of our\nmethod and the latest state-of-the-art models on COCO-\nBackbone Methods 1-shot 5-shot\ns-0 s-1 s-2 s-3 Mean s-0 s-1 s-2 s-3 Mean\nResNet-50\nPANet [34] (ICCV19)† 31.5 22.6 21.5 16.2 23.0 45.9 29.2 30.6 29.6 33.8\nRPMMs [36] (ECCV20) 29.5 36.8 29.0 27.0 30.6 33.8 42.0 33.0 33.3 35.5\nPPNet [20] (ECCV20) 34.5 25.4 24.3 18.6 25.7 48.3 30.9 35.7 30.2 36.2\nCWT (Ours) 32.2 36.0 31.6 31.6 32.9 40.1 43.8 39.0 42.4 41.3\nResNet-101\nFWB [22] (ICCV19) 19.9 18.0 21.0 28.9 21.2 19.1 21.5 23.9 30.1 23.7\nPFENet [31] (TPAMI20) 34.3 33.0 32.3 30.1 32.4 38.5 38.6 38.2 34.3 37.4\nCWT (Ours) 30.3 36.6 30.5 32.2 32.4 38.5 46.7 39.4 43.2 42.0\nTable 1. Few-shot semantic segmentation results on COCO-20i. For a fair comparison among all methods, we compare with the results of\nPPNet [20] without extra unlabeled training data. †: The results cited from PPNet [20].\n20i. We consider 1-shot and 5-shot cases, and two back-\nbone networks (ResNet-50 and ResNet-101) for a more ex-\ntensive comparison. Overall, the performance advantage of\nour method over all competitors is signiﬁcant. For exam-\nple, in 1-shot case we obtain 2.3% mIoU gain over the best\ncompetitor with ResNet-50 backbone. Much more gains\n(5.1%/4.6% with ResNet-50/ResNet-101) are shown in 5-\nshot case. Similar to PPNet, our method can beneﬁt consis-\ntently from support set expansion. In contrast, some exist-\ning methods (e.g., RPMMs, FWB and PFENet) are clearly\ninferior in leveraging extra labeled samples. More impor-\ntantly, all the compared SOTA baselines attempt to adapt all\nthree parts of a segmentation model to the new class and\neach query image. Nevertheless, our method is much sim-\npler by focusing on the linear classiﬁer only. The superior\nresults achieved by our method thus verify our assumption\nthat the pre-trained encoder and decoder is generalizable;\nand meta-learning and adaptation are thus only necessary\nfor the classiﬁer. Furthermore, we investigate the infer-\nence speed under a challenging yet practical scenario, i.e.,\n1,000 query samples per task on 1-shot case, as a trained\nmodel is expected to serve more images. Our model runs\nat 21.7 frame per second (FPS) vs. 18.9 FPS achieved by\nRPMMs [36] with ResNet-50.\n4.3.2 PASCAL- 5i Results\nIn Table 2 we show the comparative results on PASCAL-\n5i. On this less challenging dataset with a smaller num-\nber of object classes, we have similar observations as on\nCOCO-20i. Our proposed method again achieves the best\noverall performance. It is also noted that our model’s ad-\nvantage over the competitors is less pronounced compared\nto the COCO results. Our method even performs worse in\n1-shot case. A plausible reason is that with far less train-\ning classes and images on PASCAL- 5i, our assumption of\nthe pre-trained encoder/decoder being class-agnostic is not\nvalid anymore. Existing methods’ approach of adapting\nthem to new class thus has some beneﬁt contributing to nar-\nrowing down the gap to our method.\n4.4. Cross Domain Evaluation\nBeyond the standard single domain few-shot segmenta-\ntion setting, we further introduce a more challenging and\nmore realistic cross-domain setting. In this new setting,\nwe aim to test the generalization of a pre-trained model\nacross previously unseen domains (datasets) with different\ndata distributions. This setting is more difﬁcult yet more\npractical – in real-world applications, the new segmentation\ntasks often involve both new classes and new domains.\nIn this experiment, we train a few-shot segmentation\nmodel on COCO- 20i/PASCAL-5i and then directly apply\nit to PASCAL- 5i/COCO-20i without any domain-speciﬁc\nmodel re-training or ﬁne-tuning. This data transfer design is\na good cross-domain test, as the two datasets present a clear\ndomain shift in terms of instance size, instance number and\ncategories per image [17]. Concretely, from COCO to PAS-\nCAL, we use the original COCO- 20i training class splits\nfor model training. During testing on PASCAL-5i, we take\nall the classes in validation set as a whole and remove the\ntraining classes seen in each split of COCO- 20i to ensure\nthe few-shot learning nature. For PASCAL-to-COCO case,\nthe only difference from the original PASCAL- 5i experi-\nments is that the testing splits are from COCO that contains\nall the classes of PASCAL.\nFor comparative evaluation, we select the latest state-of-\nthe-art method RPMMs [36]. We use the models released\nby the RPMMs’ authors for achieving its optimal results.\nWe adopt the ResNet-50 backbone. It is observed in Ta-\nble 3 that our method is signiﬁcantly superior on almost all\nthe splits. For COCO-to-PASCAL setting, on average, our\nmodel yields a gain of 9.9% and 12.7 % in 1-shot and 5-\nshot cases, respectively. From PASCAL to COCO, the im-\nprovements are 2.8%/4.8% for 1-shot/5-shot. This suggests\nthat our training method is more effective than conventional\nwhole pipeline meta-learning for solving the domain shift\nproblem. This is due to stronger feature representation and a\nsimpler and more effective learning to learn capability with\nfocus on classiﬁer adaptation.\nBackbone Methods 1-shot 5-shot\ns-0 s-1 s-2 s-3 Mean s-0 s-1 s-2 s-3 Mean\nResNet-50\nCANet [40] (CVPR19) 52.5 65.9 51.3 51.9 55.4 55.5 67.8 51.9 53.2 57.1\nPGNet [39] (ICCV19) 56.0 66.9 50.6 50.4 56.0 57.7 68.7 52.9 54.6 58.5\nRPMMs [36] (ECCV20) 55.2 66.9 52.6 50.7 56.3 56.3 67.3 54.5 51.0 57.3\nPPNet [20] (ECCV20) 47.8 58.8 53.8 45.6 51.5 58.4 67.8 64.9 56.7 62.0\nPFENet [31] (TPAMI20) 61.7 69.5 55.4 56.3 60.8 63.1 70.7 55.8 57.9 61.9\nCWT (Ours) 56.3 62.0 59.9 47.2 56.4 61.3 68.5 68.5 56.6 63.7\nResNet-101\nFWB [22] (ICCV19) 51.3 64.5 56.7 52.2 56.2 54.9 67.4 62.2 55.3 59.9\nDAN [3] (ECCV20) 54.7 68.6 57.8 51.6 58.2 57.9 69.0 60.1 54.9 60.5\nPFENet [31] (TPAMI20) 60.5 69.4 54.4 55.9 60.1 62.8 70.4 54.9 57.6 61.4\nCWT (Ours) 56.9 65.2 61.2 48.8 58.0 62.6 70.2 68.8 57.2 64.7\nTable 2. Few-shot semantic segmentation results on PASCAL-5i. For a fair comparison among all methods, we compare with the results\nof PPNet [20] without extra unlabeled training data.\nSetting Methods 1-shot 5-shot\ns-0 s-1 s-2 s-3 Mean s-0 s-1 s-2 s-3 Mean\nCOCO → PASCAL RPMMs [36] (ECCV20) 36.3 55.0 52.5 54.6 49.6 40.2 58.0 55.2 61.8 53.8\nCWT (Ours) 53.5 59.2 60.2 64.9 59.5 60.3 65.8 67.1 72.8 66.5\nPASCAL → COCO RPMMs [36] (ECCV20) 27.0 44.7 40.6 33.2 36.4 30.2 47.8 46.2 39.6 41.0\nCWT (Ours) 34.3 42.8 44.8 34.7 39.2 40.6 48.6 51.9 41.9 45.8\nTable 3. Few-shot semantic segmentation results in bidirectional cross-domain setting. Backbone: ResNet-50.\nMeta-training 1-shot\ns-0 s-1 s-2 s-3 Mean\nWhole model 22.0 25.6 19.5 19.0 21.5\nClassiﬁer (Ours) 32.2 36.0 31.6 31.6 32.9\nTable 4. Meta-training the entire model vs. meta-training the\nclassiﬁer. Backbone: ResNet-50, Dataset: COCO-20 i.\n4.5. Ablation Study\nWe conduct a set of ablation experiments on the more\nchallenging dataset COCO-20 i. We take ResNet-50 as\nbackbone and test the 1-shot case.\n4.5.1 What should be meta-learned, the whole model\nor the classiﬁer only?\nA fundamental question we investigate in this paper is\nwhat should be meta-learned for few-shot semantic seg-\nmentation. Existing methods usually meta-learn the whole\nmodel [36, 3], which we consider to be sub-optimal due to\nthe number of model parameters to be updated given very\nscarce training data. Instead, we propose to only meta-learn\nthe classiﬁer. For an exact comparison, we create a vari-\nant of our method which updates the whole model (i.e., the\nencoder, decoder, and classiﬁer) in episodic training. To en-\nable one-stage training as our main model, we compute a\npair of prototypes for foreground and background classes\nfrom the support set and use it as the classiﬁer’s weights.\nIt is shown in Table 4 that meta-learning the whole model\nis signiﬁcantly inferior by 11.4 % in mIoU. This veriﬁes\nour assumption that the meta-learning and adaptation of the\nwhole complex segmentation model is neither effective nor\nnecessary.\nComponent 1-shot\ns-0 s-1 s-2 s-3 Mean\nClassiﬁer Only 27.3 30.7 27.6 28.9 28.6\nClassiﬁer Adapt. 32.2 36.0 31.6 31.6 32.9\nTable 5. Model component analysis. Backbone: ResNet-50,\nDataset: COCO-20i.\n4.5.2 Component analysis\nOur method can be decomposed into two phases: model\npre-training, and classiﬁer adaptation. Speciﬁcally, we pre-\ntrain the encoder and decoder ( i.e., the feature representa-\ntion components) on the whole training set. To validate\nits effectiveness, we create a baseline of pre-training only\nwithout any meta-learning. That is, after pre-training, we\ngo straight into testing and train a classiﬁer on the support\nset for every meta-test task whilst freezing the encoder and\ndecoder. The results in Table 5 reveal that this turns out to\nbe a very strong baseline. For example, it even outperforms\nthe latest model PPNet [20] by 2.9% (Table 1). This veriﬁes\nthe importance of model pre-training for obtaining a strong\nclass-agnostic feature representation, and the efﬁcacy of fo-\ncusing meta-training on the classiﬁer alone. Both simplify\nmodel optimization and ﬁnally improve the model perfor-\nmance. By adapting the classiﬁer to every query sample as\nin our full CWT model, the performance can be further im-\nSupport Query Baseline Ours\nFigure 4. Quality results under 1-shot segmentation on COCO-20i.\nFrom left to right, support images with masks, query images with\nmasks, baseline results, and our results.\nAttend to 1-shot\ns-0 s-1 s-2 s-3 Mean\nSupport image 20.1 18.9 21.9 11.9 18.2\nQuery image 32.2 36.0 31.6 31.6 32.9\nTable 6. Effect of intra-class variation. Backbone: ResNet-50,\nDataset: COCO-20i.\nproved signiﬁcantly. This validates the design of our CWT\nfor solving the intra-class variation challenge. As shown in\nFigure 4, the baseline fails to detect the airplane (1 st row)\nand the person (2 nd row) due to the lack of query adap-\ntation. These failure cases are rectiﬁed once query-image\nadaptation is in place using the proposed CWT.\n4.5.3 Importance of query adaptation\nRecall that our CWT is designed primarily to address\nthe intra-class variation problem by adapting the classiﬁer\nweights initialized on the support set to each query im-\nage. To further validate this module, we contrast with an-\nother transformer design without involving the query image\nwhilst still remaining the attention learning ability. Con-\ncretely, we set the support feature as the Key and Value\ninputs of the transformer, instead of the query feature. This\nignores the intra-class variation issue in design. The results\nin Table 6 show that without conditioning on query image,\nthe segmentation performance degrades drastically. This in-\ndicates the essential importance of resolving the intra-class\nvariation problem and the clear effectiveness of the pro-\nposed design.\nSupport Query OursBaseline\nFigure 5. Failure cases. Backbone: ResNet-50, Dataset: COCO-\n20i, 1-shot setting.\n4.5.4 Failure Cases\nBeyond the numerical evaluations as shown above, we fur-\nther study failure cases in Figure 5. This can give us some\ninsights and potentially inspiring directions for the future\ninvestigation. Overall we observe that our model fails when\nthe target instances with extreme object appearance changes\nexist between the support and query images. For instance,\nthe support image presents only the hands of a person whilst\nthe query image covers a whole person body (see the 2 nd\nand 3rd rows). In contrast, the failure in the 1 st and 4th\nrows would be caused mainly by extreme viewpoint differ-\nences. How to deal with such appearance variation requires\nbetter modeling of changes in views, pose and occlusion.\n5. Conclusion\nWe have presented a novel few-shot segmentation learn-\ning method with meta-learning. Our method differs sig-\nniﬁcantly from existing ones in that we only meta-learn\nthe classiﬁer part of a complex segmentation model whilst\nfreezing the pre-trained encoder and decoder parts. To ad-\ndress the intra-class variation issue, we further propose a\nClassiﬁer Weight Transformer (CWT) for adapting the clas-\nsiﬁer’s weights, which is ﬁrst initialized on a support set, to\nevery query image. Extensive experiments verify the perfor-\nmance superiority of our proposed method over the existing\nstate-of-the-art few-shot segmentation methods on two stan-\ndard benchmarks. Besides, we investigate a more challeng-\ning and realistic setting – cross-domain few-shot segmenta-\ntion, and show the advantages of the proposed method.\nReferences\n[1] Reza Azad, Abdur R Fayjie, Claude Kauffmann, Ismail\nBen Ayed, Marco Pedersoli, and Jose Dolz. On the texture\nbias for few-shot cnn segmentation. In WCACV, 2021. 3\n[2] Qi Cai, Yingwei Pan, Ting Yao, Chenggang Yan, and Tao\nMei. Memory matching networks for one-shot image recog-\nnition. In CVPR, 2018. 2\n[3] Xianbin Cao and Xiantong Zhen. Few-shot semantic seg-\nmentation with democratic attention networks. In ECCV,\n2020. 1, 3, 4, 7\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 3\n[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected CRFS. PAMI, 2017. 1\n[6] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\nHartwig Adam. Rethinking atrous convolution for seman-\ntic image segmentation. arXiv preprint arXiv:1706.05587 ,\n2017. 2, 3\n[7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nECCV, 2018. 1\n[8] Nanqing Dong and Eric P Xing. Few-shot semantic segmen-\ntation with prototype learning. In BMVC, 2018. 3\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 3\n[10] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (VOC) challenge. IJCV, 2010. 5\n[11] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-\nagnostic meta-learning for fast adaptation of deep networks.\nIn ICML, 2017. 1, 2\n[12] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting. In CVPR, 2018. 2\n[13] Micah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Va-\nleriia Cherepanova, and Tom Goldstein. Unraveling meta-\nlearning: Understanding feature representations for few-shot\ntasks. In ICML, 2020. 2\n[14] Bharath Hariharan, Pablo Arbel ´aez, Ross Girshick, and Ji-\ntendra Malik. Simultaneous detection and segmentation. In\nECCV, 2014. 5\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 2, 5\n[16] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen\nWei. Relation networks for object detection. InCVPR, 2018.\n3\n[17] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV, 2014. 1, 5, 6\n[18] Bin Liu, Yue Cao, Yutong Lin, Qi Li, Zheng Zhang, Ming-\nsheng Long, and Han Hu. Negative margin matters: Under-\nstanding margin in few-shot classiﬁcation. In ECCV, 2020.\n4\n[19] Jinlu Liu, Liang Song, and Yongqiang Qin. Prototype recti-\nﬁcation for few-shot learning. In ECCV, 2020. 2\n[20] Yongfei Liu, Xiangyi Zhang, Songyang Zhang, and Xum-\ning He. Part-aware prototype network for few-shot semantic\nsegmentation. In ECCV, 2020. 3, 4, 5, 6, 7\n[21] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In\nCVPR, 2015. 1\n[22] Khoi Nguyen and Sinisa Todorovic. Feature weighting and\nboosting for few-shot segmentation. In ICCV, 2019. 1, 5, 6,\n7\n[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. ImageNet large\nscale visual recognition challenge. IJCV, 2015. 3\n[24] J ¨urgen Schmidhuber. Evolutionary principles in self-\nreferential learning, or on learning how to learn: the meta-\nmeta-... hook. PhD thesis, 1987. 2\n[25] Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, and\nByron Boots. One-shot learning for semantic segmentation.\nIn BMVC, 2017. 1, 2, 3, 4, 5\n[26] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. InICLR,\n2015. 2\n[27] Jake Snell, Kevin Swersky, and Richard S Zemel. Prototyp-\nical networks for few-shot learning. In NeurIPS, 2017. 1, 2,\n3\n[28] Jong-Chyi Su, Subhransu Maji, and Bharath Hariharan.\nWhen does self-supervision improve few-shot learning? In\nECCV, 2020. 2\n[29] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS\nTorr, and Timothy M Hospedales. Learning to compare: Re-\nlation network for few-shot learning. In CVPR, 2018. 1, 2\n[30] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenen-\nbaum, and Phillip Isola. Rethinking few-shot image classiﬁ-\ncation: a good embedding is all you need? In ECCV, 2020.\n4\n[31] Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng\nYang, Ruiyu Li, and Jiaya Jia. Prior guided feature enrich-\nment network for few-shot segmentation. TPAMI, 2020. 6,\n7\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 3\n[33] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray\nKavukcuoglu, and Daan Wierstra. Matching networks for\none shot learning. In NeurIPS, 2016. 2\n[34] Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou,\nand Jiashi Feng. PANet: Few-shot image semantic segmen-\ntation with prototype alignment. In ICCV, 2019. 1, 3, 6\n[35] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan,\nPeizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer, and Pe-\nter Vajda. Visual transformers: Token-based image repre-\nsentation and processing for computer vision. arXiv preprint\narXiv:2006.03677, 2020. 3\n[36] Boyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, and Qixiang\nYe. Prototype mixture models for few-shot semantic seg-\nmentation. In ECCV, 2020. 1, 2, 3, 4, 6, 7\n[37] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-\nshot learning via embedding adaptation with set-to-set func-\ntions. In CVPR, 2020. 1, 3, 4\n[38] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen.\nDeepEMD: Few-shot image classiﬁcation with differentiable\nearth mover’s distance and structured classiﬁers. In CVPR,\n2020. 4\n[39] Chi Zhang, Guosheng Lin, Fayao Liu, Jiushuang Guo,\nQingyao Wu, and Rui Yao. Pyramid graph networks with\nconnection attentions for region-based one-shot semantic\nsegmentation. In ICCV, 2019. 7\n[40] Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chun-\nhua Shen. CANet: Class-agnostic segmentation networks\nwith iterative reﬁnement and attentive few-shot learning. In\nCVPR, 2019. 1, 2, 3, 7\n[41] Xiaolin Zhang, Yunchao Wei, Yi Yang, and Thomas S\nHuang. SG-One: Similarity guidance network for one-shot\nsemantic segmentation. IEEE Transactions on Cybernetics,\n2020. 1, 2\n[42] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nCVPR, 2017. 3, 4, 5",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8179171681404114
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6791398525238037
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6729857921600342
    },
    {
      "name": "Encoder",
      "score": 0.6478926539421082
    },
    {
      "name": "Segmentation",
      "score": 0.5971137881278992
    },
    {
      "name": "Transformer",
      "score": 0.5230339765548706
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.511991560459137
    },
    {
      "name": "Discriminative model",
      "score": 0.45680588483810425
    },
    {
      "name": "Machine learning",
      "score": 0.4011252522468567
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}