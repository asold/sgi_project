{
  "title": "Deduplicating Training Data Makes Language Models Better",
  "url": "https://openalex.org/W3177765786",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2107274534",
      "name": "Katherine Lee",
      "affiliations": [
        "Brain (Germany)",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2799006035",
      "name": "Daphne Ippolito",
      "affiliations": [
        "Google (United States)",
        "California University of Pennsylvania",
        "Brain (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2751405213",
      "name": "Andrew Nystrom",
      "affiliations": [
        "Google (United States)",
        "Brain (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2118495952",
      "name": "Zhang Chiyuan",
      "affiliations": [
        "Brain (Germany)",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2141395071",
      "name": "Douglas Eck",
      "affiliations": [
        "Brain (Germany)",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4207984947",
      "name": "Chris Callison-Burch",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A1606335232",
      "name": "Nicholas Carlini",
      "affiliations": [
        "Brain (Germany)",
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2158874082",
    "https://openalex.org/W2132069633",
    "https://openalex.org/W2963096987",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2800224756",
    "https://openalex.org/W4287204036",
    "https://openalex.org/W2533248932",
    "https://openalex.org/W3169411391",
    "https://openalex.org/W3022034311",
    "https://openalex.org/W3032503335",
    "https://openalex.org/W3158631574",
    "https://openalex.org/W3012990076",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3100352836",
    "https://openalex.org/W3191483612",
    "https://openalex.org/W2008434289",
    "https://openalex.org/W3118901667",
    "https://openalex.org/W2682189153",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W117230746",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W2123845384",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W3156891177",
    "https://openalex.org/W2139749722",
    "https://openalex.org/W2969947100",
    "https://openalex.org/W2962911098",
    "https://openalex.org/W3048045781",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W2000982728",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W2948223045",
    "https://openalex.org/W3156216837",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W3168416344",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2979792666",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2123402141",
    "https://openalex.org/W3099635335",
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2911227954",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W3190860428",
    "https://openalex.org/W4297801643",
    "https://openalex.org/W2947813521",
    "https://openalex.org/W3212368439",
    "https://openalex.org/W4283172211",
    "https://openalex.org/W2130564474"
  ],
  "abstract": "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 8424 - 8445\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nDeduplicating Training Data Makes Language Models Better\nKatherine Lee∗† Daphne Ippolito ∗†‡ Andrew Nystrom† Chiyuan Zhang†\nDouglas Eck† Chris Callison-Burch‡ Nicholas Carlini†\nAbstract\nWe ﬁnd that existing language modeling\ndatasets contain many near-duplicate exam-\nples and long repetitive substrings. As\na result, over 1% of the unprompted out-\nput of language models trained on these\ndatasets is copied verbatim from the train-\ning data. We develop two tools that allow\nus to deduplicate training datasets—for exam-\nple removing from C4 a single 61 word En-\nglish sentence that is repeated over 60,000\ntimes. Deduplication allows us to train mod-\nels that emit memorized text ten times less\nfrequently and require fewer training steps\nto achieve the same or better accuracy. We\ncan also reduce train-test overlap, which af-\nfects over 4% of the validation set of stan-\ndard datasets, thus allowing for more accurate\nevaluation. Code for deduplication is released\nat https://github.com/goog/l.Vare-research/\ndedup/l.Varicate-text-datasets.\n1 Introduction\nA key factor behind the recent progress in natural\nlanguage processing is the development of large-\nscale text corpora used to train increasingly large\nlanguage models. These datasets have grown from\nsingle gigabytes to as much as a terabyte over the\npast few years (Chelba et al., 2013; Xue et al., 2020;\nGraff et al., 2003; Brown et al., 2020). Because\nit is so expensive to perform manual review and\ncuration on massive datasets, they tend to suffer\nin quality compared to their smaller predecessors.\nThis has implications far beyond metrics like per-\nplexity and validation loss, as learned models re-\nﬂect the biases present in their training data (Ben-\nder et al., 2021; Wallace et al., 2019; Sheng et al.,\n2020). Quantitatively and qualitatively understand-\ning these datasets is therefore a research challenge\nin its own right (Dodge et al., 2021a).\n∗ Equal contribution. † Google Research, Brain Team.\n‡ University of Pennsylvania. Correspond to kather-\ninelee@google.com and daphnei@seas.upenn.edu.\nWe show that one particular source of bias, du-\nplicated training examples, is pervasive: all four\ncommon NLP datasets we studied contained dupli-\ncates. Additionally, all four corresponding valida-\ntion sets contained text duplicated in the training\nset. While naive deduplication is straightforward\n(and the datasets we consider already perform some\nnaive form of deduplication), performing thorough\ndeduplication at scale is both computationally chal-\nlenging and requires sophisticated techniques.\nWe propose two scalable techniques to detect\nand remove duplicated training data. Exact sub-\nstring matching identiﬁes verbatim strings that are\nrepeated. This allows us to identify cases where\nonly part of a training example is duplicated (§4.1).\nApproximate full document matching uses hash-\nbased techniques (Broder, 1997) to identify pairs\nof documents with high n-gram overlap (§4.2).\nWe identify four distinct advantages to training\non datasets that have been thoroughly deduplicated.\n1. Over 1% of tokens emitted unprompted from\na model trained on standard datasets (e.g., C4)\nare part of a memorized sequence (See §6.2)—\neven though the 1.5 billion parameter model\nis much smaller than the 350GB dataset it\nwas trained on. By deduplicating the training\ndataset we reduce the rate of emitting memo-\nrized training data by a factor of 10×.\n2. Train-test overlap is common in non-\ndeduplicated datasets. For example, we ﬁnd a\n61-word sequence1 in C4 (Raffel et al., 2020)\nthat is repeated 61,036 times verbatim in the\ntraining dataset and 61 times in the validation\nset (0.02% of the samples in each dataset).\n1“by combining fantastic ideas, interesting arrangements,\nand follow the current trends in the ﬁeld of that make you\nmore inspired and give artistic touches. We’d be honored if\nyou can apply some or all of these design in your wedding.\nbelieve me, brilliant ideas would be perfect if it can be applied\nin real and make the people around you amazed!”\n1\n8424\nThis train-test set overlap not only causes re-\nsearchers to over-estimate model accuracy, but\nalso biases model selection towards models\nand hyperparameters that intentionally overﬁt\ntheir training datasets.\n3. Training models on deduplicated datasets is\nmore efﬁcient. Processing a dataset with our\nframework requires a CPU-only linear-time\nalgorithm. And so because these datasets are\nup to 19% smaller, even including the dedu-\nplication runtime itself, training on dedupli-\ncated datasets directly reduces the training\ncost in terms of time, dollar, and the environ-\nment (Bender et al., 2021; Strubell et al., 2019;\nPatterson et al., 2021).\n4. Deduplicating training data does not hurt\nperplexity: models trained on deduplicated\ndatasets have no worse perplexity compared\nto baseline models trained on the original\ndatasets. In some cases deduplication reduces\nperplexity by up to 10%. Further, because re-\ncent LMs are typically limited to training for\njust a few epochs (Radford et al., 2019; Raffel\net al., 2020), by training on higher quality data\nthe models can reach higher accuracy faster.\nTo summarize, data duplication offers signiﬁcant\nadvantages and no observed disadvantages. In the\nremainder of this paper we present our text dedu-\nplication framework in §4, and study the extent of\nduplicate content in common NLP datasets (e.g.,\nC4, Wiki-40B, and LM1B) in §5. We then exam-\nine the impact of deduplication on test perplexity\n(§6.1) and on the frequency of emitting memorized\ncontent (§6.2). Finally, we analyze to what ex-\ntent perplexity on existing, released models are\nskewed as a result of overlap between the train and\ntest/validation splits (§6.3).\n2 Related Work\nLarge language model datasets. While we be-\nlieve our results are independent of model archi-\ntecture, we perform our analysis on Transformer-\nbased decoder-only language models (Vaswani\net al., 2017) trained for open-ended text generation.\nThese current state-of-the-art models are trained\non internet text. For example, the GPT-2 family\nof models Radford et al. (2019) is trained on Web-\nText, a dataset of web documents highly ranked on\nReddit—however this dataset was not made avail-\nable publicly. A common dataset starting point\nis CommonCrawl, an index of public webpages.\nAmong the models trained on CommonCrawl in-\nclude GPT-3 (Brown et al., 2020) with the addition\nof book datasets, GROVER (Zellers et al., 2019) on\na restricted subset ﬁltered to news domains called\nRealNews, and T5 (Raffel et al., 2020) on a cleaned\nversion of common crawl called C4. Other models\nare trained on more curated Internet sources—for\nexample Guo et al. (2020) used high quality pro-\ncessed Wikipedia text from 40 different languages\nto train monolingual 141.4M parameter language\nmodels. Non-English models necessarily use dif-\nferent datasets; Zeng et al. (2021) for instance in-\ntroduced PANGU-α, a family of models with up to\n200B parameters that were trained on a non-public\ncorpus of cleaned and ﬁltered Chinese-language\ndocuments from CommonCrawl and other sources.\nSince many of these datasets are not public, we\ndeduplicate three that are: Wiki-40B, C4, and\nRealNews–as well as the One Billion Word Lan-\nguage Model Benchmark (Chelba et al., 2013), a\nsmaller dataset commonly used for evaluation.\nContamination of downstream tasks. When\nmodels are trained on datasets constructed by crawl-\ning the Internet, it is possible the model will train\non the test set of downstream target tasks. For ex-\nample, Radford et al. (2019, §4) performed a post-\nhoc analysis to identify 8-gram overlaps between\nGPT-2’s training set and datasets used for evalu-\nation, and Dodge et al. (2021b) analyzed C4 and\nfound that up to 14.4% of test examples for various\nstandard tasks were found verbatim (normalizing\nfor capitalization and punctuation) in the dataset.\nA more proactive approach removes contaminated\ndata. Trinh and Le (2018, Appendix B) removed\ndocuments from their CommonCrawl-based train\nset that overlapped substantially with the common-\nsense reasoning used for evaluation. And GPT-3\n(Brown et al., 2020, §5) did the reverse and re-\nmoved downstream evaluation examples from their\ntraining data by conservatively ﬁltering out any\ntrain set examples with a 13-gram overlap with\nany evaluation example. Up to 90% of tasks were\nﬂagged as potentially contaminated.\nIn our research, we do not focus on the impact of\nduplicate text in pretrained models on downstream\nbenchmark tasks; instead we address how duplicate\ntext in the LM training and validation sets impacts\nmodel perplexity and the extent to which generated\ntext included memorized content.\n2\n8425\nMemorizing training data. The privacy risks of\ndata memorization, for example the ability to ex-\ntract sensitive data such as valid phone numbers\nand IRC usernames, are highlighted by Carlini et al.\n(2020). While their paper ﬁnds 604 samples that\nGPT-2 emitted from its training set, we show that\nover 1% of the data most models emit is memorized\ntraining data. In computer vision, memorization of\ntraining data has been studied from various angles\nfor both discriminative and generative models (e.g.\nArpit et al., 2017; Webster et al., 2019; Feldman\nand Zhang, 2020; Stephenson et al., 2021; Teter-\nwak et al., 2021).\nDuplicate text in training data. The Book Cor-\npus (Zhu et al., 2015), which was used to train pop-\nular models such as BERT, has a substantial amount\nof exact-duplicate documents according to Bandy\nand Vincent (2021). Allamanis (2019) shows that\nduplicate examples in code datasets cause wors-\nened performance on code understanding tasks.\n3 Language Modeling Datasets\nWe analyze the presence of duplicate text in four\ndatasets of varying sizes that have been used for\ntraining natural language generation systems, pro-\nducing general-purpose pre-trained models, and for\nlanguage model benchmarking. While this paper\nrestricts itself to English datasets, we expect that\nnon-English datasets suffer from similar issues and\ncould likewise beneﬁt from de-duplication.\nWikipedia (Wiki-40B) consists of multi-lingual\ncleaned Wikipedia text (Guo et al., 2020). We\ntake the English portion, which contains 2.9M\nWikipedia pages with an average length of 768 BPE\ntokens. The dataset creators do not indicate any\ndeduplication was performed aside from removing\nredirect-pages (e.g., “sunﬂower” to “Helianthus”).\nOne-Billion Word benchmark (LM1B) con-\ntains 30M sentences of news commentary (Chelba\net al., 2013). Unlike the other datasets we analyze,\nLM1B’s examples are one sentence long rather\nthan multi-sentence documents. The average ex-\nample length is 32 BPE tokens. While this dataset\nis extremely standard for benchmarking language\nmodels, Radford et al. (2019, Sec 4) note it has\n13.2% overlap of the test set with the train set.\nColossal Cleaned Common Crawl (C4) is\nmade up of 360M web documents, with an average\nlength of 486 BPE tokens (Raffel et al., 2020). C4\nwas introduced as a pre-training dataset for T5, a set\nof encoder-decoder models which have been widely\nused in ﬁne-tuned downstream tasks. The dataset\nwas previously deduplicated in a more sophisti-\ncated process than the prior two datasets. Each\nparagraph was hashed and paragraphs resulting in\nhash collisions were removed. This was followed\nby a pass that removed placeholder text, code, and\nprohibited words. See Dodge et al. (2021a) for a\ndetailed breakdown of the source text in C4.\nRealNews is a subset of the Common Crawl con-\nsisting of articles from news domains (Zellers et al.,\n2019). It contains 31M documents with average\nlength 793 BPE tokens. RealNews was dedupli-\ncated by inserting a hash of the ﬁrst 100 characters\nof each document into a bloom ﬁlter (Bloom, 1970)\nand then excluding any document which resulted in\na hash collision. Like C4, examples with duplicate\nURLs were excluded.\n4 Methods for Identifying Duplicates\nThe simplest technique to ﬁnd duplicate examples\nwould be to perform exact string matching between\nall example pairs, but as we will show, this is insuf-\nﬁcient. We introduce two complementary methods\nfor performing deduplication. First, using a suf-\nﬁx array (Manber and Myers, 1993), we remove\nduplicate substrings from the dataset if they oc-\ncur verbatim in more than one example. Second,\nwe use MinHash (Broder, 1997), an efﬁcient algo-\nrithm for estimating the n-gram similarity between\nall pairs of examples in a corpus, to remove entire\nexamples from the dataset if they have highn-gram\noverlap with any other example.\nWe consider a dataset D= {xi}N\ni=1 as a collec-\ntion of examples xi. Each of these examples is itself\na sequence of tokens: xi =\n[\nx1\ni ,x2\ni ,··· ,xsi\ni\n]\n.\n4.1 Exact Substring Duplication\nDue to the diversity of possibilities in human lan-\nguage, it is rare for the same idea to be expressed\nidentically in multiple documents unless one ex-\npression is derived from the other, or both are quot-\ning from a shared source. This observation moti-\nvates deduplicating exact substrings. We call our\napproach EXACT SUBSTR . When two examples\nxi and xj share a sufﬁciently long substring (that\nis, a substring for which xa..a+k\ni = xb..b+k\nj ), that\nsubstring is removed from one of them. Based\non statistical analyses (§B), we select k = 50to-\nkens as the minimum matching substring length.\n3\n8426\nA breakdown of the computation needed for this\napproach can be found in Appendix B.\n4.1.1 Sufﬁx Arrays\nThis exact-substring-matching criterion, while con-\nceptually simple, is computationally prohibitive\nwith naive (quadratic) all-pair matching. To im-\nprove the efﬁciency, we concatenate all the exam-\nples of the entire dataset Dinto a giant sequence S,\nand construct a Sufﬁx Array Aof S. A sufﬁx array\n(Manber and Myers, 1993) is a representation of a\nsufﬁx tree (Weiner, 1973) that can be constructed\nin linear time in ∥S∥(Kärkkäinen and Sanders,\n2003) and enables efﬁcient computation of many\nsubstring queries; in particular, they allow us to\nidentify duplicated training examples in linear time.\nSufﬁx arrays have the advantage over sufﬁx trees\nin that they are 10–100×more memory efﬁcient\n(Manber and Myers, 1993), requiring just 8 bytes\nper input token, though they are asymptotically\nless efﬁcient for some query types. They have been\nused widely in NLP, such as for efﬁcient TF-IDF\ncomputation (Yamamoto and Church, 2001) and\ndocument clustering (Chim and Deng, 2007).\nThe sufﬁx array A for a sequence S is a\nlexicographically-ordered list of all sufﬁxes con-\ntained in the sequence. Formally,\nA(S) =arg sort all_sufﬁxes(S)\nFor example, the sufﬁxes of the sequence “banana”\nare (“banana”, “anana”, “nana” “ana”, “na”, “a”)\nand so the sufﬁx array is the sequence (6 4 2 1 5 3).\nIn practice, we construct Sfrom the bytes of the\nBPE tokenization of the text (§6).\n4.1.2 Substring matching\nAfter constructing A, it is straightforward to iden-\ntify duplicated training examples. Suppose that\nthe sequence swas repeated exactly twice in the\ntraining dataset S at positions i and j, that is,\nSi..i+|s|= Sj..j+|s|. Then the indices i,j will occur\nadjacent to each other in the sufﬁx array A.\nFinding all repeated sequences is thus a matter of\nlinearly scanning the sufﬁx array from beginning to\nend and looking for sequences Ai,Ai+1 that share\na common preﬁx of at least some threshold length.\nAny satisfying sequences are recorded. This al-\ngorithm is embarrassingly parallel, and so we can\nefﬁciently process the dataset. Based on experi-\nmentation (Appendix B), we choose a threshold\nlength of 50 BPE tokens for all experiments.\n4.2 Approximate Matching with MinHash\nWe also perform approximate deduplication based\non matching entire examples. This method, which\nwe call NEAR DUP, is a good complement to the\nexact substring matching, especially for web crawl\ntext, as it handles the very common case of docu-\nments being identical except for interspersed tem-\nplated ﬁelds (such as the last row of Table 1).\nMinHash (Broder, 1997) is an approximate\nmatching algorithm widely used in large-scale\ndeduplication tasks (Versley and Panchenko, 2012;\nGabriel et al., 2018; Gyawali et al., 2020), in-\ncluding to deduplicate the training set for a large\nChinese-language LM (Zeng et al., 2021). Given\ntwo documents xi and xj, the main idea is to repre-\nsent each document by its respective set ofn-grams\ndi and dj. We can then use hash functions to ap-\nproximate the Jaccard Index (Jaccard, 1912):\nJaccard(di,dj) =|di∩dj |/|di∪dj |\nIf the Jaccard Index between di and dj is sufﬁ-\nciently high, it is likely that documents are approx-\nimate matches of each other. To efﬁciently approx-\nimate the Jaccard index, MinHash constructs doc-\nument signatures by sorting each of the n-grams\nvia a hash function, and then keeping only the k\nsmallest hashed n-grams. There are multiple ways\nto construct estimators of the Jaccard index from\nthese kinds of signatures (Cohen, 2016).\nIn our implementation, we use 5-grams and a\nsignature of size 9,000. The probability that two\ndocuments are considered a potential match is\nPr(di,dj|Jaccard(di,dj) =si,j) = 1−(1−sb\ni,j)r\nwhere b = 20 and r = 450 are user-settable pa-\nrameters to control the strength of the ﬁlter. See\nAppendix A for more details.\nFor each pair of documents identiﬁed as a poten-\ntial match, more computationally expensive similar-\nity metrics can be employed as a subsequent ﬁlter-\ning step. In particular, we identify two documents\nas duplicates if they are matched by the MinHash\nalgorithm and their edit similarity is greater than\n0.8. The edit similarity between token sequences\nxi and xj is deﬁned as:\nEditSim(xi,xj) = 1−EditDistance(xi,xj)\nmax(|xi|,|xj|)\nTo build clusters of similar documents, we con-\nstruct a graph that has an edge between two doc-\numents if they are considered a match. Then, we\n4\n8427\nDataset Example Near-Duplicate Example\nWiki-40B \\n_START_ARTICLE_\\nHum Award for Most Impact-\nful Character \\n_START_SECTION_\\nWinners and nomi-\nnees\\n_START_PARAGRAPH_\\nInthe list below, winners are\nlisted ﬁrst in the colored row, followed by the other nominees.\n[...]\n\\n_START_ARTICLE_\\nHum Award for Best Actor in a\nNegative Role \\n_START_SECTION_\\nWinners and nomi-\nnees\\n_START_PARAGRAPH_\\nIn the list below, winners are\nlisted ﬁrst in the colored row, followed by the other nominees. [...]\nLM1B I left for California in 1979 and tracked Cleveland ’schanges on\ntrips back to visit my sisters .\nI left for California in 1979 , and tracked Cleveland ’schanges on\ntrips back to visit my sisters .\nC4 Affordable and convenient holiday ﬂights take off from your\ndeparture country, \"Canada\". From May 2019 to October 2019,\nCondor ﬂights to your dream destination will be roughly 6 a\nweek! Book your Halifax (YHZ) - Basel (BSL) ﬂight now, and\nlook forward to your \"Switzerland\" destination!\nAffordable and convenient holiday ﬂights take off from your depar-\nture country, \"USA\". From April 2019 to October 2019, Condor\nﬂights to your dream destination will be roughly 7 a week! Book\nyour Maui Kahului (OGG) - Dubrovnik (DBV)ﬂight now,and look\nforward to your \"Croatia\" destination!\nTable 1: Qualitative examples of near-duplicates identiﬁed by N EAR DUP from each dataset. The similarity be-\ntween documents is highlighted. Note the small interspersed differences that make exact duplicate matching less\neffective. Examples ending with “[...]” have been truncated for brevity. More data available in Appendix.\n0100 101 102 103 104 105 106 107 108 109\nNumber of groups\n1\n2\n3\n4\n5\n[6, 10)\n[11, 20)\n[21, 50)\n[51, 500)\n[501, 5000)\n[5001, )\nGroup sizes\n348,320,475\n1,861,744\n292,575\n109,853\n54,984\n85,567\n42,723\n28,446\n23,094\n2,782\n280 C4\nFigure 1: The distribution of near-duplicate cluster\nsizes from running NEAR DUP on C4.\nuse the method introduced in Ł ˛ acki et al. (2018) to\nidentify connected components. A breakdown of\nthe computation needed is given in Appendix A.\n5 Deduplication Results\nWe deduplicate each of the four datasets with both\nof our two techniques. When text was duplicated\nacross multiple data splits, we prioritized keeping\na copy in the test or validation set and removing it\nfrom the train set.\n5.1 Amount of Text Removed\nWith NEAR DUP, we found that the web-scrape\ndatasets contain between 3.04% (on C4) to 13.63%\n(on RealNews) near duplicates (Table 2). Near-\nduplicate text is much less common in Wiki-40B,\nforming only 0.39% of the train set.2 In C4, the ma-\njority (1.8M) of near-duplicate clusters consisted of\njust a single pair of examples that matched against\neach other, but there were 280 clusters with over\n5,000 examples in them (Figure 1), including one\ncluster of size 250,933.\n2Most duplicates we saw were automatically generated\npages, such as the outcomes of sports games. This shows the\nstrength of manual curation for creating high-quality datasets.\n% train examples with % valid with\ndup in train dup in valid dup in train\nC4 3.04% 1.59% 4.60%\nRealNews 13.63% 1.25% 14.35%\nLM1B 4.86% 0.07% 4.92%\nWiki40B 0.39% 0.26% 0.72%\nTable 2: The fraction of examples identiﬁed by\nNEAR DUP as near-duplicates.\n% train tokens with % valid with\ndup in train dup in valid dup in train\nC4 7.18% 0.75 % 1.38 %\nRealNews 19.4 % 2.61 % 3.37 %\nLM1B 0.76% 0.016% 0.019%\nWiki40B 2.76% 0.52 % 0.67 %\nTable 3: The fraction of tokens (note Table 2 reports\nthe fraction of examples) identiﬁed by EXACT SUBSTR\nas part of an exact duplicate 50-token substring.\nOn average with EXACT SUBSTR , we remove\nmore total content than with NEAR DUP (de-\nspite EXACT SUBSTR not removing any examples\noutright)—for example removing 7.18% of the to-\nkens in C4. The exception is LM1B, where EX-\nACT SUBSTR removes 8×less data than NEAR DUP.\nOn investigation, we ﬁnd this is due to the fact that\nLM1B documents are signiﬁcantly shorter: 90%\nof all documents are under 50 tokens, and so are\nnot even candidates for potential matches even if\nthe entire sequence matched verbatim. We ﬁnd\nthat both NEAR DUP and EXACT SUBSTR remove\nsimilar content— 77% of the training examples that\nNEAR DUP removes from C4 have at least one ver-\nbatim length-50 match found by EXACT SUBSTR .\n5\n8428\n5.2 Properties of Duplicated Text\nWhile the authors of both RealNews and C4 ex-\nplicitly attempted deduplication during dataset con-\nstruction, the methods were insufﬁcient to capture\nthe more subtle types of duplicate text commonly\nfound on the internet. In C4 and Wiki-40B, we\nqualitatively observe that much of the text identi-\nﬁed as near-duplicated is computer-generated. The\ntext is identical except for the names of places, busi-\nnesses, products, dates, and so on. Because these\nexamples frequently differ by just a few words at\na time, deduplication strategies relying on exact\nstring matching would fail to identify a match. Ex-\nample duplicate pairs from each dataset can be\nfound in Table 1 (more examples in the Appendix).\nFor RealNews and LM1B, derived from news\nsites, we observe that many near-duplicates occur\nbecause the same news article appears on multiple\nnews sites with slightly different formatting. For\nexample, in LM1B, there is one example that starts\n“MINEOLA , N.Y. - New York ofﬁcials say[...]” and\nanother that starts “( AP ) - New York ofﬁcials say\n[...]”. The two examples are otherwise identical.\n5.3 Train / Test Set Leakage\nBoth deduplication methods identify overlap be-\ntween the train set and the validation set (Table 2).\nFor example, 4.6% of the C4 validation set and\n14.4% of the RealNews validation set examples\nhad an approximate duplicate in their respective\ntraining sets. Such duplication is problematic since\nit could cause evaluation metrics to be unfairly in-\nﬂated for models that are better at memorizing their\ntrain sets. We evaluate the effect of this leakage on\npublicly released models in Section 6.3.\n6 Impact on Trained Models\n. We trained 1.5B parameter “XL\", decoder-\nonly, Transformer-based language models similar\nto GPT-2, on C4-ORIGINAL , C4-NEAR DUP, and\nC4-EXACT SUBSTR , respectively. We use the T5\ncodebase and model architecture from Raffel et al.\n(2020), and each model was trained for about two\nepochs on its respective dataset. To better under-\nstand the amount of variance in the perplexities\nof trained models, we also trained three different\nrandom seeds of the 110M parameter “base\" model\nfor each of the above three datasets—for a total of\nnine base-sized models.\nFor all experiments, we used a Byte Pair Encod-\ning (BPE) vocabulary trained on C4- NEAR DUP\n0 5 10 15 20 25 30 35\nPerplexity\nC4 Original\nC4 Duplicates\nC4 Unique\nLM1B\nWiki40B\nEvaluation dataset\nTraining data\nOriginal\nNearDup\nExactSubstr\nFigure 2: Impact of deduplicating the training set on\nvalidation perplexity. We plot the results from T5 XL\n(see Appendix for base-sized model). For C4, we eval-\nuate on C4 Original , the original validation set; C4\nUnique, a subset of the validation set identiﬁed by\nNEAR DUP as having zero matches across C4; and C4\nDuplicates, a subset of the validation set identiﬁed by\nNEAR DUP as having a match in the C4 train set.\nwith a budget of 50K tokens, which resulted in a\nvocabulary the same size as GPT-2’s. We trained\nwith a maximum sequence length of 512 tokens\n(for longer documents, we randomly extracted sub-\nsequences of this length.) Further training details\ncan be found in Appendix C.\n6.1 Model Perplexity\nWe computed the perplexity of our trained mod-\nels on the validation sets of LM1B and Wiki-40B,\nand on subsets of the C4 validation set (Figure 2).\nFor the base size, we observe that all models have\nsimilar perplexity on the original C4 validation set\nand on validation set examples that were identi-\nﬁed as unique (no near-duplicate in either train\nor validation). However, both models trained on\ndeduplicated data have signiﬁcantly higher perplex-\nity on validation set examples that have duplicates\nin the training set than the model trained on the\noriginal C4. EXACT SUBSTR -deduplicated results\nin higher perplexity than NEAR DUP-deduplicated.\nThese trends holds true for the XL sized model as\nwell. While this may suggest EXACT SUBSTR du-\nplication results in models least overﬁt on the train\nset, note that both of these techniques have used\nseparate duplicate thresholds and a different choice\nof thresholds could change the results.\nWhen evaluating on the validation sets of LM1B\nand Wiki-40B, we found that models trained on\nNEAR DUP-deduplicated C4 consistently achieved\nlowest perplexity (for LM1B eval with base models,\nsee Appendix Figure 7). EXACT SUBSTR dedupli-\ncation decreases perplexity of the XL model by\nalmost 3 points perplexity on Wiki-40B which is\n6\n8429\nModel 1 Epoch 2 Epochs\nXL-ORIGINAL 1.926% 1.571%\nXL-NEAR DUP 0.189% 0.264%\nXL-EXACT SUBSTR 0.138% 0.168%\nTable 4: When generating 100k sequences with no\nprompting, over 1% of the tokens emitted from a model\ntrained on the original dataset are part of a 50-token\nlong sequence copied directly from the training dataset.\nThis drops to 0.1% for the deduplicated datasets.\nmuch larger than the variation of about 1 point per-\nplexity we observed in the base models. This is\ndespite seeing fewer tokens of training data overall.\nLastly, we note all our XL models achieved <35\nperplexity on LM1B, which is less than the 42.16\nperplexity reported for the 1.5B GPT-2 using a\nvocabulary the same size as ours.\n6.2 Generated Text\nData duplication has the effect of biasing the\ntrained LM towards particular types of examples.\nThis can contribute to a lower diversity of genera-\ntions, and increased likelihood that the generated\ncontent is copied from the training data (Carlini\net al., 2020). For our generation experiments, we\nuse top-krandom sampling with k= 50and exper-\niment with prompted and unprompted generation.\nNo prompt. We ﬁrst evaluate memorization ten-\ndencies in the case where the model is asked\nto generate text without any prompt sequence.\nWe generate 100,000 samples, each up to 512\ntokens in length (examples provided in the Ap-\npendix). For each generated token, we say the\ntoken is memorized if it is part of a 50-token sub-\nstring that is exactly contained in the training data.\nOn XL-ORIGINAL , over 1% of the generated to-\nkens belong to memorized sub-sequences (see Ta-\nble 4). This is ∼10×more memorization than XL-\nEXACT SUBSTR or XL-NEAR DUP. Some example\nsubsequences that were copied verbatim from the\ntrain set can be found in Table 9 in the Appendix.\nWith prompting. In most real use cases, lan-\nguage model generation is controlled by providing\na prompt for the model to continue. We experi-\nment with four possible prompt sources: training\nexamples identiﬁed by EXACT SUBSTR as having\nnear-duplicates in the train set (train dup), train-\ning examples identiﬁed as unique (train unique),\nvalidation set examples with a near-duplicate in\nthe train set (valid in train), and validation set ex-\n0.0 0.1 0.2 0.3 0.4\nFraction of LM continuations\nmatching true continuation\ntrain dup\ntrain unique\nvalid in train\nvalid uniquePrompt source\nTraining data\nOriginal\nNearDup\nExactSubstr\nFigure 3: The proportion of generations which have\nedit similarity above 0.8 with the groundtruth continu-\nation when using the LM to generate continuations for\n32-token prompts identiﬁed by NEAR DUP as either du-\nplicated or unique.\nModel Dataset Orig Dups Unique\nTransformer-XL LM1B 21.77 10.11 23.58\nGROVER-Base RealNews 15.44 13.77 15.73\nGROVER-XL RealNews 9.15 7.68 9.45\nTable 5: For each model, the perplexity of the ofﬁ-\ncial validation set ( Orig), valid set examples which\nwere identiﬁed by N EAR DUP as matches of train set\nexamples (Dups), and valid set examples identiﬁed by\nNEAR DUP as unique (Unique). Due to the size of the\nRealNews validation set, we evaluated on only the ﬁrst\n25k examples meeting each condition.\namples identiﬁed as unique across all splits (valid\nunique). We select the ﬁrst 32 tokens of each exam-\nple as the prompt, which means we can evaluate the\nfraction of generations which are near-duplicates\nwith the ground-truth continuation for the prompt\n(Figure 3). When the prompt comes from dupli-\ncate examples in the train set, XL-ORIGINAL repro-\nduces the groundtruth continuation over 40% of the\ntime. XL-EXACT SUBSTR and XL-NEAR DUP still\ncopy the groundtruth more often when the prompt\ncomes from a duplicate example than when the\nprompt comes from a unique example, suggesting\nthat more stringent deduplication may be necessary\nto remove memorization tendencies entirely.\n6.3 Impact on Existing Models\nTrain-test leakage does not just impact models\ntrained on C4. Table 5 shows that the presence\nof near-duplicates of the evaluation set in the train\nset has a signiﬁcant impact on model perplexity\nfor two standard models: Transformer-XL (Dai\net al., 2019), which was trained on LM1B, and\nGROVER (Zellers et al., 2019), which was trained\non RealNews. For Transformer XL, the perplexity\n7\n8430\nhalves on examples identiﬁed as near-duplicates.\nFor GROVER, the difference, though not quite as\nstark, is present in both model sizes considered.\nExisting models also suffer from the problem\nof generating text from their train sets. We ﬁnd\nthat 1.38% of the tokens in the ofﬁcial release of\n25k GROVER-Mega outputs 3 are part of verbatim\nmatches in RealNews of at least length 50. Like-\nwise, more than 5% of the tokens in ~200k se-\nquences outputted by GPT-Neo 1.3B (Black et al.,\n2021) are part of a 50 token matches of its training\ndata, the Pile (Gao et al., 2020).\n7 Discussion\nThe focus of this paper is on the datasets used to\ntrain language models. While recent work focused\non documenting the potential harms that could arise\nfrom problematic datasets (Bender and Friedman,\n2018; Gebru et al., 2020), less work has been done\nto quantitatively analyze properties of real language\nmodelling datasets, like Dodge et al. (2021a) has\ndone for C4. Our paper provides analysis on one\nparticular axis, that of data duplication.\nOur experiments measured what could be quan-\ntiﬁed: the amount of duplicate content in com-\nmon datasets, the effect of deduplication on trained\nmodel perplexity, and the reduction of memorized\ncontent in trained models through deduplication.\nWe do not focus on the nature of the data being\nremoved by deduplication or memorized by LMs.\nPrivacy is an important subject for future work,\nas memorized training data has signiﬁcant privacy\nconsequences. By this, we mean the standard pri-\nvacy deﬁnition that a model should not reveal any-\nthing particular to the speciﬁc dataset it was trained\non, as opposed to another training dataset from a\nsimilar distribution (Shokri et al., 2017). 4 Train-\ning on standard datasets that have not yet been\ndeduplicated results in models that are particularly\nsensitive to examples that happened to be repeated\nmultiple times, and this has negative privacy im-\nplications. For instance, it could violate a person’s\nexpectations of privacy if their publicly available\npersonal data appeared in a different, surprising\ncontext. Downstream applications of LMs, such\n3gs://grover-mode/l.Vars/generation_examp/l.Vares/\ngenerator=mega~dataset=p0.90.json/l.Var\n4Another interpretation of privacy focuses on the sensitiv-\nity of the data involved, when a model is trained on and able\nto reproduce personal identiﬁers or other forms of “private\ndata.” Our deﬁnition is more expansive.\nas the game AI Dungeon5, should also not output\nmemorized content like adverts for real products.\nWe stress that in our experiments, we do not dis-\ntinguish between undesired memorized text (such\nas phone numbers), innocuous memorized text\n(common phrases), and text we may want to be\nmemorized (such as a quote by a public ﬁgure),\nand instead treat all instances of the LM generat-\ning text that closely matches the training set as\nproblematic. While we qualitatively observed that\nmuch of the identiﬁed memorized content was rel-\natively innocuous, a more systematic study of the\nrisks associated with the detected memorization\nwas beyond the scope of this work.\nWe also do not investigate the negative conse-\nquences of deduplication. Some language tasks\nexplicitly require memorization, like document re-\ntrieval or closed-book question answering. Also,\ntext that gives attribution is often duplicated across\ndocuments, so removing duplicate substrings could\ncorrespond to removing just the attribution, which\ncould result in models that learn the content with-\nout its attached attribution. Deduplication is also\nnot sufﬁcient to remove privacy-sensitive data like\nbank passwords and medical records which should\nnever be used in training data (Brown et al., 2022).\nUltimately, whether memorization is a desired\nproperty of a language model, or else risky and\nunwanted, depends both on the nature of the text\nthat has been memorized and on the downstream\napplications of the trained model. However, since\nthe trend has been towards creating datasets and\nmodels that are application-agnostic, we encourage\nresearchers to think carefully about the limitations\nof the data they have collected and the how the\nmodel’s intended usage constrains what should be\npart of the training set. Developing techniques to\nmemorize or forget speciﬁc sequences depending\non the end application is a promising research di-\nrection.\n8 Conclusion\nWe encourage future language model research to\nperform dataset deduplication, either by training\non the deduplicated datasets we release, using the\ndeduplication tools we release, or following our\napproach to deduplicate datasets with new tools.\nThe exact technique used to perform dedupli-\ncation is less important than performing stringent\ndeduplication in the ﬁrst place. On the whole, dedu-\n5https://p/l.Varay.aidungeon.io/\n8\n8431\nplication does not harm, and sometimes improves,\nmodel perplexity, despite the fact that the dedupli-\ncated datasets are smaller and faster to train on.\nIt is especially important that there are no dupli-\ncates between the training and testing sets, because\noverlap here explicitly encourages selecting models\nthat memorize the training data. Lastly, deduplica-\ntion helps to reduce some of the privacy concerns\naround LMs memorizing their training data.\nEthics\nThe developers of large language models typi-\ncally attempt to create training data that reﬂects\nnatural human communication, but current meth-\nods to collect and curate such datasets are falli-\nble. There are multiple reasons some text ends\nup over-represented. For example, bot replies,\nauto-generated templates, and licenses are repeated\nfor structural (e.g., legal, economical) reasons (as\nwas also observed by Dodge et al. (2021a)). Ad-\nditionally, common techniques for acquiring and\n“cleaning” data can result in an over-representation\nof particular subsets of world users, often those\nwho are English-speaking and publishing in es-\ntablished forums. This effectively under-represents\nnon-English speakers as well as groups whose com-\nmunication mostly occurs outside of the public\nweb. In this paper, we focus on the problem of\nover-representation of some types of text (struc-\ntural duplicates) but do not address the problem of\nunder-representation of others.\nAdditionally, while we discuss when memorized\ncontent might be desired and when it might not\nbe desired, our analysis does not disambiguate\nthese two cases. Work to disambiguate helpful\nfrom harmful memorization is tremendously com-\nplex and would require a different set of research\nmethodologies than are presented in this work.\nAcknowledgements\nWe are grateful to the many researchers whose\ntechnical help, feedback, and discussions shaped\nthis project: Jacob Austin, Samy Bengio, Olivier\nBousquet, James Bradbury, Fernando Diaz, Mark\nDiaz, Noah Fiedel, Jonathan Frankle, David\nGrangier, Stefanie Karp, David Mimno, Gaurav\nMishra, Michael Mozer, Sharan Narang, Alex Pas-\nsos, Adam Roberts, Hanie Sedghi, Jascha Sohl-\ndickstein, David So, Florian Tramer, and Yun\nWilliam Yu. We are also grateful to the Google\nBrain women who have given us continuous sup-\nport.\nChris Callison-Burch and Daphne Ippolito’s\nresearch is supported in part by the DARPA\nKAIROS Program (contract FA8750-19-2-1004),\nthe DARPA LwLL Program (contract FA8750-19-\n2-0201), and the IARPA BETTER Program (con-\ntract 2019-19051600004). The views and conclu-\nsions contained herein are those of the authors and\nshould not be interpreted as necessarily represent-\ning the ofﬁcial policies, either expressed or implied,\nof DARPA, IARPA, or the U.S. Government.\nContributions\nEach of the authors on this paper signiﬁcantly con-\ntributed to the ﬁnal results.\n• Katherine trained the models used in the pa-\nper, built and ran the eval and text generation\npipelines, contributed signiﬁcantly to writing,\nanalysis, and project organization and manage-\nment.\n• Daphne ran the approximate matching data dedu-\nplication pipelines, extracted prompts and evalu-\nation datasets, ran eval pipelines, and contributed\nsigniﬁcantly to planning, writing, and analysis.\n• Andrew wrote the code to perform deduplica-\ntion with approximate matching, helped evaluate\nenergy expenditure, and helped with analysis.\n• Chiyuan helped generate plots and contributed to\nproject scoping, writing, and data analysis.\n• Chris offered mentorship and guidance through-\nout the project and contributed to writing.\n• Doug offered mentorship and guidance through-\nout the project and contributed to writing.\n• Nicholas wrote the sufﬁx array implementation,\nran all EXACT SUBSTR deduplication experi-\nments, contributed signiﬁcantly to planning, writ-\ning, and analysis, as well as scoping the project.\nReferences\nMiltiadis Allamanis. 2019. The adverse effects of\ncode duplication in machine learning models of\ncode. In Proceedings of the 2019 ACM SIG-\nPLAN International Symposium on New Ideas, New\nParadigms, and Reﬂections on Programming and\nSoftware, pages 143–153.\n9\n8432\nDevansh Arpit, Stanisław Jastrz˛ ebski, Nicolas Ballas,\nDavid Krueger, Emmanuel Bengio, Maxinder S Kan-\nwal, Tegan Maharaj, Asja Fischer, Aaron Courville,\nYoshua Bengio, et al. 2017. A closer look at mem-\norization in deep networks. In International Confer-\nence on Machine Learning, pages 233–242. PMLR.\nJack Bandy and Nicholas Vincent. 2021. Addressing\n\"documentation debt\" in machine learning research:\nA retrospective datasheet for bookcorpus.\nEmily M. Bender and Batya Friedman. 2018. Data\nstatements for natural language processing: Toward\nmitigating system bias and enabling better science.\nTransactions of the Association for Computational\nLinguistics, 6:587–604.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big?\n . In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nscale autoregressive language modeling with mesh-\ntensorﬂow.\nBurton H Bloom. 1970. Space/time trade-offs in hash\ncoding with allowable errors. Communications of\nthe ACM, 13(7):422–426.\nAndrei Z Broder. 1997. On the resemblance and con-\ntainment of documents. In Proceedings. Compres-\nsion and Complexity of SEQUENCES 1997 (Cat. No.\n97TB100171), pages 21–29. IEEE.\nHannah Brown, Katherine Lee, Fatemehsadat\nMireshghallah, Reza Shokri, and Florian Tramèr.\n2022. What does it mean for a language model to\npreserve privacy? arXiv preprint.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Pro-\ncessing Systems 33.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ul-\nfar Erlingsson, Alina Oprea, and Colin Raffel. 2020.\nExtracting training data from large language models.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nHung Chim and Xiaotie Deng. 2007. A new sufﬁx\ntree similarity measure for document clustering. In\nProceedings of the 16th International Conference on\nWorld Wide Web, WWW ’07, page 121–130, New\nYork, NY , USA. Association for Computing Machin-\nery.\nEdith Cohen. 2016. Min-hash sketches: A brief survey.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860.\nJesse Dodge, Maarten Sap, Ana Marasovic, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, and Matt\nGardner. 2021a. Documenting the english colossal\nclean crawled corpus.\nJesse Dodge, Maarten Sap, Ana Marasovic, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, and\nMatt Gardner. 2021b. Documenting the english\ncolossal clean crawled corpus. arXiv preprint\narXiv:2104.08758.\nVitaly Feldman and Chiyuan Zhang. 2020. What neu-\nral networks memorize and why: Discovering the\nlong tail via inﬂuence estimation. In Advances in\nNeural Information Processing Systems.\nRodney A. Gabriel, Tsung-Ting Kuo, Julian McAuley,\nand Chun-Nan Hsu. 2018. Identifying and char-\nacterizing highly similar notes in big clinical note\ndatasets. Journal of Biomedical Informatics, 82:63–\n69.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The Pile: An\n800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027.\nTimnit Gebru, Jamie Morgenstern, Briana Vec-\nchione, Jennifer Wortman Vaughan, Hanna Wal-\nlach, Hal Daumé III au2, and Kate Crawford. 2020.\nDatasheets for datasets.\nDavid Graff, Junbo Kong, Ke Chen, and Kazuaki\nMaeda. 2003. English gigaword. Linguistic Data\nConsortium, Philadelphia, 4(1):34.\nMandy Guo, Zihang Dai, Denny Vrandecic, and Rami\nAl-Rfou. 2020. Wiki-40b: Multilingual language\nmodel dataset. In LREC 2020.\nBikash Gyawali, Lucas Anastasiou, and Petr Knoth.\n2020. Deduplication of scholarly documents using\nlocality sensitive hashing and word embeddings. In\nProceedings of the 12th Language Resources and\nEvaluation Conference, pages 901–910.\nPaul Jaccard. 1912. The distribution of the ﬂora in the\nalpine zone. New phytologist, 11(2):37–50.\n10\n8433\nJuha Kärkkäinen and Peter Sanders. 2003. Simple lin-\near work sufﬁx array construction. In International\ncolloquium on automata, languages, and program-\nming, pages 943–955. Springer.\nPang Ko and Srinivas Aluru. 2003. Space efﬁcient\nlinear time construction of sufﬁx arrays. In An-\nnual Symposium on Combinatorial Pattern Match-\ning, pages 200–210. Springer.\nUdi Manber and Gene Myers. 1993. Sufﬁx arrays: a\nnew method for on-line string searches. siam Jour-\nnal on Computing, 22(5):935–948.\nGe Nong, Sen Zhang, and Wai Hong Chan. 2009. Lin-\near sufﬁx array construction by almost pure induced-\nsorting. In 2009 data compression conference ,\npages 193–202. IEEE.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen\nLiang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. 2021. Car-\nbon emissions and large neural network training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning ,\npages 4596–4604. PMLR.\nEmily Sheng, Kai-Wei Chang, Premkumar Natara-\njan, and Nanyun Peng. 2020. Towards control-\nlable biases in language generation. arXiv preprint\narXiv:2005.00268.\nReza Shokri, Marco Stronati, Congzheng Song, and\nVitaly Shmatikov. 2017. Membership inference at-\ntacks against machine learning models. In 2017\nIEEE Symposium on Security and Privacy (SP) ,\npages 3–18. IEEE.\nCory Stephenson, Suchismita Padhy, Abhinav Ganesh,\nYue Hui, Hanlin Tang, and SueYeon Chung. 2021.\nOn the geometry of generalization and memoriza-\ntion in deep neural networks. In International Con-\nference on Learning Representations.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in nlp.\nPiotr Teterwak, Chiyuan Zhang, Dilip Krishnan, and\nMichael C Mozer. 2021. Understanding invariance\nvia feedforward inversion of discriminatively trained\nclassiﬁers. In International Conference on Machine\nLearning, pages 10225–10235. PMLR.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nYannick Versley and Yana Panchenko. 2012. Not just\nbigger: Towards better-quality web corpora. In Pro-\nceedings of the seventh Web as Corpus Workshop\n(WAC7), pages 44–52.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing nlp. arXiv preprint\narXiv:1908.07125.\nRyan Webster, Julien Rabin, Loïc Simon, and Frédéric\nJurie. 2019. Detecting overﬁtting of deep generative\nnetworks via latent recovery. In 2019 IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 11265–11274.\nPeter Weiner. 1973. Linear pattern matching algo-\nrithms. In 14th Annual Symposium on Switching and\nAutomata Theory (swat 1973), pages 1–11. IEEE.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nMikio Yamamoto and Kenneth W Church. 2001. Using\nsufﬁx arrays to compute term frequency and docu-\nment frequency for all substrings in a corpus. Com-\nputational Linguistics, 27(1):1–30.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. arXiv preprint arXiv:1905.12616.\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang,\nYi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang\nYang, Kaisheng Wang, Xiaoda Zhang, Chen Li,\nZiyan Gong, Yifan Yao, Xinjing Huang, Jun\nWang, Jianfeng Yu, Qi Guo, Yue Yu, Yan Zhang,\nJin Wang, Hengtao Tao, Dasen Yan, Zexuan Yi,\nFang Peng, Fangqing Jiang, Han Zhang, Lingfeng\nDeng, Yehong Zhang, Zhe Lin, Chao Zhang, Shao-\njie Zhang, Mingyue Guo, Shanzhi Gu, Gaojun\nFan, Yaowei Wang, Xuefeng Jin, Qun Liu, and\nYonghong Tian. 2021. Pangu- α: Large-scale au-\ntoregressive pretrained chinese language models\nwith auto-parallel computation. arXiv preprint\narXiv:2104.12369.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 19–\n27.\n11\n8434\nJakub Ł ˛ acki, Vahab Mirrokni, and Michał Włodarczyk.\n2018. Connected components at scale via local con-\ntractions.\n12\n8435\nA Further Details on N EAR DUP\nFor our MinHash based deduplication method, doc-\numents are ﬁrst space tokenized, then each consec-\nutive 5-gram is hashed using tabulation hashing.\nThe set of these hashes is the signature for the doc-\nument. For each element in a document’s signature,\nthe element is hashed using kother hash functions.\nThe minimum hashed element for each of the k\nhash functions is stored. These minimum hashes\nare then partitioned into rbuckets, with bhashes\nper bucket. These bhashes are augmented into a\nsingle value, then if two documents have the same\nvalue in at least one bucket, they’ll be marked as\na potential match. The probability that two doc-\numents are considered a potential match is equal\nto\nPr(di,dj|Jaccard(di,dj) =si,j) = 1−(1−sb\ni,j)r\nwhere si,j is the Jaccard index between the two\ndocuments iand j. For document pairs that were\nidentiﬁed as potential matches, we computed their\nactual Jaccard index, and if that was above 0.8,\nwe computed their edit similarity. Document pairs\nwith edit similarity higher than 0.8 were identi-\nﬁed as duplicates. After some experimentation, we\nchose to use b= 20, and r = 450, so k = 9,000,\nso as to make sure a collision at the desired Jaccard\nindex threshold of 0.8 had a high probability of\noccurring.\nWe also tested an alternative conﬁguration—\nﬁltering to document pairs with Jaccard index of at\nleast 0.9 and edit similarity of at least 0.9. In this\ncase, we used b= 20, r = 40, and k = 800. Fig-\nure 4 shows the histogram of Jaccard similarities\nand edit similarities for all document pairs which\ncollided in min-hash space, for our chosen conﬁgu-\nration (blue) and for the alternative conﬁguration\n(orange). This allows us verify if the threshold\nchosen has few comparisons around the chosen\nthreshold, then we’ve likely captured the majority\nof actual near duplicates above that threshold. To\nverify that yourself, look at the left hand tails of\nthe distributions. Since both 0.8 and 0.9 begin to\nvanish at the same point (in spite of the fact that the\ntwo thresholds are optimized for accuracy around\ndifferent thresholds), we feel comfortable saying\nthat we’re capturing the majority of actual near\nduplicates.\nComputational Analysis Let N be the number\nof documents and T be the maximal number of to-\nkens in a document. Edit similarity has a worst case\ncomplexity of T2, so the worst case complexity is\nO(N + bk2T2N) =O(N)\nsince b, k, and T are all ≪N. The left term is the\ncomplexity of grouping by the signatures, and the\nright represents the pathological worst case of all\ndocuments falling into the same Bbuckets.\nThe highly distributed NEAR DUP implementa-\ntion we employed is one used for large-scale pro-\nduction tasks at Google. On the English C4 dataset,\nthe algorithm consumed approximately 41.5 kWh\nof energy. Note that our choices of kand bwere\ndesigned to produce very high recall, and with dif-\nferent parameters, the algorithm could be made\nmuch more energy efﬁcient while producing simi-\nlar results.\nB Further Details on E XACT SUBSTR\nParallel linear time construction. We build a\nparallelized linear time sufﬁx array algorithm. As\na building block, we make black-box use of the\nSA-IS algorithm for constructing a sufﬁx array\nin linear time Nong et al. (2009); Ko and Aluru\n(2003). Unfortunately, this algorithm is not eas-\nily parallelized directly, so we introduce a simple\ndivide and conquer approach to parallelizing the\narray construction.\nWe build our implementation in Rust and ex-\ntend an existing sufﬁx array library 6 with three\nmodiﬁcation. The ﬁrst two are straightforward im-\nplementation differences: we modify the code to\nallow datasets larger than 4GB, and we remove the\nrequirement that strings parse as valid UTF-8 se-\nquences in favor of raw byte sequences. Our third\nchange is more signiﬁcant: we re-implement the\nalgorithm so that we can stream the sufﬁx array\nitself off disk.\nParallel partial sufﬁx array construction. Our\ndivide and conquer sufﬁx array construction algo-\nrithm starts by partitioning the dataset intoKdiffer-\nent “splits” with SA-IS run over independently on\neach split in parallel. This algorithm still requires\nO(N) work but runs in O(N/K) wall-clock time.\nThis gives us N separate sufﬁx arrays Ai.\nGiven two sufﬁx arrays A1 and A2 for two se-\nquences S1 and S2 it’s not completely trivial to\nconstruct a single sufﬁx array Afor S = S1 ||S2\nbecause of the boundary conditions. Instead, we\n6https://github.com/BurntSushi/sufﬁx\n13\n8436\n0.00 0.25 0.50 0.75 1.00\nEdit similarity\n0.0\n0.2\n0.4\n0.6\n% of pairwise\ndocument comparisons\nC4 (t=0.8)\nC4 (t=0.9)\n0.00 0.25 0.50 0.75 1.00\nEdit similarity\nLM1B (t=0.8)\nLM1B (t=0.9)\n0.00 0.25 0.50 0.75 1.00\nEdit similarity\nRealNews (t=0.8)\nRealNews (t=0.9)\n0.00 0.25 0.50 0.75 1.00\nEdit similarity\nWiki40B (t=0.8)\nWiki40B (t=0.9)\n0.00 0.25 0.50 0.75 1.00\nJaccard similarity\n0.0\n0.1\n0.2\n0.3\n0.4\n% of pairwise\ndocument comparisons\nC4 (t=0.8)\nC4 (t=0.9)\n0.00 0.25 0.50 0.75 1.00\nJaccard similarity\nLM1B (t=0.8)\nLM1B (t=0.9)\n0.00 0.25 0.50 0.75 1.00\nJaccard similarity\nRealNews (t=0.8)\nRealNews (t=0.9)\n0.00 0.25 0.50 0.75 1.00\nJaccard similarity\nWiki40B (t=0.8)\nWiki40B (t=0.9)\nFigure 4: Histograms of document similarities.\ndon’t build the data S = S1 ||S2 but rather let\nS′\n1 = S1 ||S2[uptoK] for some K greater than\nthe longest substring match. Then we build the\narrays on S′\n1 and S2. To merge the arrays together\nwe can remove the items from the ﬁrst array af-\nter index |S1|and merge-sort insert them into the\nsecond.\nParallel merge of partial sufﬁx arrays. We\nnow merge these separate arrays together into a\nsingle sufﬁx array A, Consider the simpler case of\ntwo partial sufﬁx arrays B and C that we would\nlike to merge together. We can achieve this by\nletting i = 0 index B and j = 0 index C. Each\niteration of the algorithm then pushes Bi into A\nif SBi.. < SCi and Ci otherwise, repeating until\ni= |B|−1 and j = |C|−1. To generalize to K\nsplits, we need only replace the single comparison\nabove with a min-heap requiring O(log K) ≪10\nwork on each iteration.\nObserve that in the general case this algorithm\nis O(Nmlog(K)) where N is the length of the\ndataset, mis the average length of a preﬁx match,\nand Kis the number of splits. It is therefore incor-\nrect to call this algorithm linear time in the general\ncase, for ours it is. Because the length of the longest\nmatch is bounded above by the length of the longest\nsequence, as long as the size of the dataset is inde-\npendent of the length of the longest sequence in the\ndataset, this algorithm remains efﬁcient.\nAgain, we can parallelize this operation among\nLsimultaneous jobs (in practice we set K = Las\nthe number of threads on our machine). In theK =\n2 case, job l processes i ∈[jN/L,(j+ 1)N/L],\nchoosing the bounds of jby binary searching into\nC so that SBi < SCj < SBj+1 . The case where\nK >2 is identical except that we repeat this over\nall Kpartial sufﬁx arrays.\nComputational Analysis. We run our algorithm\non a single VM on the cloud with 96 cores and\n768GB of memory. Our algorithm is efﬁcient, for\nexample processing the Wiki-40B training set ( 3\nmillion examples containing 4GB of text) in 2.3\nminutes wall-clock time (2.1 CPU-hours of work).\nThe 350GB C4 dataset takes under 12 hours (wall-\nclock) to build a sufﬁx array; although we are still\nmemory constrained and so this corresponds to\n∼1000 CPU-hours. Once the sufﬁx array has been\nconstructed, it takes under an hour to deduplicate\nthe C4 dataset.\nNote that this algorithm still requires that the\ndataset itself ﬁts in memory (so that we can efﬁ-\nciently index in arbitrary positions), but we do not\nneed to ﬁt the entire sufﬁx array into memory. This\nis fortunate since our sufﬁx array requires an 8×\nspace overhead. For example, the sufﬁx array for\nthe 350GB C4 is 1.5TB.\nCompared to the cost of training a language\nmodel on this dataset, the additional work required\nto deduplicate the training dataset is negligible.\nSetting a threshold of duplicates. An important\nquestion is how long must a substring match be\nbefore it is counted as a duplicate. In Figure 5, we\nplot the frequency of substring matches within the\nfour datasets we will consider. For each substring\nof length k, we compute the probability that there\nexists another sequence of length kidentical to this\n14\n8437\nLM1B\nC4\nRealNews\nWiki-40B\nFigure 5: For each substring of length k, we plot the\nprobability that there exists a second identical length-\nksubstring in the same train set. Matches with length\nunder 10 subword tokens are common, and account for\n90% of tokens. We choose a threshold of 50 for experi-\nments.\none; formally:\nm(k) = Pr\ni∈[N]\n[\n∃j ̸= i: Si..i+k = Sj..j+k\n]\n.\nWe choose 50 tokens as the threshold to be conser-\nvative: the “bend in the knee” occurs at10 tokens,\nand manual inspection of length-25 matches found\nno false positives. We then doubled this value to\nhave an exceptionally large margin for error.\nC Further Details on Model Training\nEach model was trained for two epochs. Since both\nC4-ORIGINAL and C4-EXACT SUBSTR contain ap-\nproximately 365M examples, we performed 152K\nsteps with a batch size of 4800 (or approximately\n2 epochs). C4-NEAR DUP contains approximately\n350M examples, we performed 146K steps (or ap-\nproximately 2 epochs). On a 128-core TPU v3 pod\nslice, XL models trained on C4-ORIGINAL and C4-\nEXACT SUBSTR took approximately 131 hours (5.5\ndays) to train, while the XL model trained on C4-\nNEAR DUP took approximately 126 hours to train.\nLike T5, models were trained with the Adafactor\noptimizer (Shazeer and Stern, 2018). A constant\nlearning rate of 0.01 was used for the base models\nand 0.001 for the XL models.\nThe 1.5B parameter XL models had 24 layers,\neach with 32 attention heads. The model embed-\nding size was 2,048, the feed forward layers had\na hidden size of 5,120, and the key/value dimen-\nsion size for the attention heads 64. The 110M\nparameter base models had 12 layers, each with 12\nattention heads. The model embedding size was\n768, the feed forward layers had a hidden size of\n2,048, and the key/value dimension size for the\nattention heads 64.\nD Energy Consumption\nWe trained for approximately 131 hours or 5.5\ndays on a 128-core TPU v3. The approximate\ndeduplicated dataset is 3.9% smaller than the orig-\ninal dataset and trains in 63 hours/epoch, saving\nus around 5 hours of compute time for the two\nepochs. The XL- ORIGINAL model was trained in\nNorth America where the XL-EXACT SUBSTR and\nXL-NEAR DUP were trained in Taiwan. We used\ndata from Patterson et al. (2021) to estimate amount\nof energy used in training these models by comput-\ning the amount of MWh/hour/core and multiply-\ning by our usage (see Table 6 for how we computed\nthese values). For simplicity, we use estimates\nfrom Taiwainese datacenters as an estimate. We es-\ntimate training 2 epochs of XL-ORIGINAL and XL-\nEXACT SUBSTR uses 5.86MWh. XL-NEAR DUP\nis trained for fewer steps and we estimate uses\n5.63MWh. Training each base model was approxi-\nmately 3 days on a 64-core TPU v3 pod slice which\nuses an estimated 1.61MWh.\nIn addition to model training, evaluation and in-\nference were performed on 64-core TPU v3 pod\nslices. Generating 100,000 sequences from the XL\nmodels takes approximately 0.64 hours. We gen-\nerated 100,000 sequences for each of ﬁve types of\nprompts for two checkpoints of the model for a\ntotal of 1M sequences per model. This took ap-\nproximately 19.2 hours. We estimate generating\n3M sequences uses 0.43MWh.\nE More Results\nQualitative Examples. Table 8 shows several ex-\namples of pairs of documents in C4 whose edit dis-\ntance is close to our chosen edit similarity thresh-\nold of 0.8. Table 9 shows substrings which were\nidentiﬁed by EXACT SUBSTR as being in C4 more\nthan once. Table 10 shows several examples of\nunprompted generations which were identiﬁed as\nmemorized are shown.\nDistribution of memorization. Figure 6 shows\nthe distribution in memorization amount over all\ngenerated sequences when using four types of\nprompting: train example with duplicates in train,\n15\n8438\nT5 11B\nXL-ORIGINAL\nXL-EXACT SUBSTR XL-NEAR DUP\nBase-ORIGINAL\nBase-EXACT SUBSTR Total Inference\nTPU v3 cores 512 128 128 64 64\nTraining time (days) 20 5.47 5.26 3 0.80\nTPU hrs 245760 16804.70 16149.31 4608 1228.80\nEnergy (MWh) 85.70 5.86 5.63 1.61 0.43\nTable 6: Estimates of energy usage based on the data in Patterson et al. (2021). The ﬁrst column is Patterson et al.\n(2021)’s estimate of the T5 11B encoder-decoder model, which we based our own estimates on. Inference includes\nall XL models. We generated 100,000 sequences from 3 models, with 5 prompts, and at 2 different checkpoints.).\nDataset Example Near-Duplicate Example\nWiki-40B \\n_START_ARTICLE_\\nHum Award\nfor Most Impactful Character\n\\n_START_SECTION_\\nWinners and nom-\ninees\\n_START_PARAGRAPH_\\nIn the list\nbelow, winners are listed ﬁrst in the colored row,\nfollowed by the other nominees. [...]\n\\n_START_ARTICLE_\\nHumAward for Best Actor\nin a Negative Role\\n_START_SECTION_\\nWinners\nand nominees\\n_START_PARAGRAPH_\\nInthe list\nbelow, winners are listed ﬁrst in the colored row, fol-\nlowed by the other nominees. [...]\nLM1B I left for California in 1979 and tracked Cleveland\n’schanges on trips back to visit my sisters .\nI left for California in 1979 , and tracked Cleveland\n’schanges on trips back to visit my sisters .\nRealNews KUALA LUMPUR (Reuters) - Roads in South-\neast Asia have been getting a little louder lately\nas motorcycle makers, an aspiring middle class\nand easy bank credit come together to breed a new\ngenus of motorcyclists – the big-bike rider. [...]\nA visitor looks at a Triumph motorcycle on dis-\nplay at the Indonesian International Motor Show\nin Jakarta September 19, 2014. REUTERS/Darren\nWhiteside\\nKUALA LUMPUR (Reuters) - Roads in\nSoutheast Asia have been getting a little [...] big-bike\nrider. [...]\nC4 Affordable and convenient holiday ﬂights take\noff from your departure country, \"Canada\". From\nMay 2019 to October 2019, Condor ﬂights to your\ndream destination will be roughly 6 a week! Book\nyour Halifax (YHZ) - Basel (BSL) ﬂight now, and\nlook forward to your \"Switzerland\" destination!\nAffordable and convenient holiday ﬂights take off\nfrom your departure country, \"USA\". From April\n2019 to October 2019, Condor ﬂights to your dream\ndestination will be roughly 7 a week! Book your\nMaui Kahului (OGG) - Dubrovnik (DBV) ﬂight now,\nand look forward to your \"Croatia\" destination!\nTable 7: Qualitative examples of near-duplicates identiﬁed by N EAR DUP from each dataset. The similarlity be-\ntween documents is highlighted. Note the small interspersed differences that make exact duplicate matching less\neffective. Examples ending with “[...]” have been truncated for brevity.\ntrain dup train unique valid in train valid unique\nPrompt Source\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nedit sim between generated\nand groundtruth continuations\nmodel\nOriginal NearDup ExactSubstr\nFigure 6: Memorized continuations distribution\ntrain examples without any duplicates, validation\nexamples with duplicates in train, and validation\nexamples without any duplicates.\nURLs with many duplicates. Table 11 shows\nthe URLs had the largest proportion of examples\nidentiﬁed by NEAR DUP as near-duplicates. For\nC4, these tend to be websites that sell many similar\nproducts and thus have a large amount of templated\ntext. For RealNews, content aggregators seem es-\npecially common.\nNEAR DUP cluster sizes. Figure 8 shows the dis-\ntribution of cluster sizes from running NEAR DUP\non RealNews, LM1B, and Wiki-40B (results for\nC4 are in Figure 1 the main paper).\nDataset Sizes Table 13 gives the size in BPE to-\nkens and in examples of each dataset before and\nafter deduplication. Because most datasets were\n16\n8439\nDue to high demand, we have yet to critique this request. That\nsaid, we assure that the review will be produced in due time\nby our dilligent and unwavering staff in a professional manner.\nThis site is highly regarded amongst its peers in terms of speed\nand reliability, so feel free to check us out!\nDue to a heavy overﬂow, we have not been able to critique\nthis request. That said, we assure that the review will be pro-\nduced in due time by our dilligent and unshakable staff in a\nprofessional manner. This site is highly regarded amongst its\npeers in terms of efﬁciency and reliability, so feel free to visit!\nNeed Pop Tacos parking? You can reserve parking near Pop\nTacos with SpotHero. Find low rates without parking coupons\nby booking a guaranteed spot online. Avoid circling, getting\nticketed or running out to feed your meter. Search our parking\nmap, compare parking rates and reserve a discounted parking\nspot today. Happy parking, and enjoy your meal at Pop Tacos!\nIl Sole parking. Reserve parking near Il Sole in NYC.\\nYou\ncan reserve parking near Il Sole with SpotHero. Find low rates\nwithout parking coupons by booking a guaranteed spot online.\nAvoid circling, getting ticketed or running out to feed your\nmeter. Search our parking map, compare parking rates and\nreserve a discounted parking spot today. Happy parking, and\nenjoy your meal at Il Sole!\nThis item was available on Vinyl 7\" but is now sold out on all\nformats, sorry. Take a look at what else we have in by Jumbo,\ncheck out some related artists, head over to our new releases\nor knock yourself out reading our latest music news & album\nreviews.\\n2nd single edn of 550.\nThis item was available on CD but is now sold out on all for-\nmats, sorry. Take a look at what else we have in by Sirconical,\nMisty Dixon, Various, check out some related artists, head\nover to our new releases or knock yourself out reading our\nlatest music news & album reviews.\\nTwisted Nerve comp\nmini album.\nHere is all the information you need about \"No One Killed\nJessica\" on American Netﬂix. Details include the date it was\nadded to Netﬂix in the USA, any known expiry dates and new\nepisodes/seasons, the ratings and cast etc. So scroll down for\nmore information or share the link on social media to let your\nfriends know what you’rewatching.\nHere is all the information you need about \"A Land Imagined\"\non Netﬂix in the UK. Details include the date it was added to\nUK Netﬂix, any known expiry dates and new episodes/seasons,\nthe ratings and cast etc. So scroll down for more information\nor share the link on social media to let your friends know what\nyou’rewatching.\n8 + 8 = Solve this simple math problem and enter the result.\nE.g. for 1+3, enter 4.\nMath question * 7 + 1 = Solve this simple math problem and\nenter the result. E.g. for 1+3, enter 4.\nLong Island College Hospital is committed to providing out-\nstanding patient care in the Brooklyn, NY area, but before you\ncommit to Long Island College Hospital for a Endometrial\nAblation make sure you compare and shop other medical fa-\ncilities. It may save you hundreds (in some cases thousands)\nof dollars. View a Endometrial Ablation cost comparison for\nBrooklyn and Request a Free Quote before you make a deci-\nsion.\nMorristown Memorial Hospital is committed to providing out-\nstanding patient care in the Morristown, NJ area, but before\nyou commit to Morristown Memorial Hospital for a Breast\nUltrasound make sure you compare and shop other medical\nfacilities. It may save you hundreds (in some cases thousands)\nof dollars. View a Breast Ultrasound cost comparison for\nMorristown and Request a Free Quote before you make a\ndecision.\nTable 8: Several examples of pairs of documents in C4 that were found by the Approximate Matching algorithm\nand identiﬁed as having edit similarity of almost exactly 0.8. Pairs of documents less similar than 0.8 were not\nidentiﬁed as duplicates. For readability, matching subsequences have been highlighted.\n17\n8440\nText Freq in C4\nHD wallpaper. This wallpaper was upload at April 19, 2019 upload by admin in.You can download it\nin your computer by clicking resolution image in Download by size:. Don’t forget to rate and comment\nif you interest with this wallpaper.\n40,340\nto the address posted below. Include our failure information form,a packing slip with your Company\nname, contact person, and Email address or phone number. Upon receipt of your repair, we\\’ll inspect it\nand then contact you with a quote or evaluation notice. Normal turn around for repair is 5 to 7 business\ndays, with \"Rush Repair\" available.\n5,900\nis a great place to begin your search. Whether you are a ﬁrst-time home buyer or you are already\nfamiliar with the home buying process, you can be assured that you have the best tools and the perfect\nagent available to help with your\n5,358\npics at these awesome group starting P letter. Desktop wallpapers were ﬁrst introduced way back in\nthe 1980s and have gained immense popularity since then. It is possible to come across more than 80\nmillion sites on the web offering some sort of wallpaper.\n848\nﬂowers will let them know you’re thinking of them and wishing them well. Cheerful yellow ﬂowers\nbring their own sunshine and will get right to work on lifting spirits, and a colorful vase will bring\nloads of smiles to friends and visitors! Get Well ﬂower arrangements from\n479\nour premier 24 hour emergency* plumbing and heating solutions. We realise that when your heating\nfails or pipes and drains leak it can cause havoc with your routine and even cause damage to your\nproperty. When a plumbing problem occurs that requires an immediate response we provide qualiﬁed\nlocal plumbers throughout\n56\nis to remove all images that violate copyrights. Please contact us to request that images be removed or\nto assign proper credit. The images displayed on this site may be used for Free or educational purposes\nonly. If you would like to use any of the images displayed on this site for any other purpose, please\nobtain permission from the owner. www.\n48\nlist of ﬁshing locations, providing interactive maps that show each location’s GPS coordinates, nearby\nfacilities (like restaurants, gas stations, marinas and ﬁshing shops), their current and forecasted weather\nand, if available, their water conditions.\\nFind any of the 8\n5\n. Dyer, Ph.D., is an internationally renowned author and speaker in the ﬁeld of self-development. He’s\nthe author of 30 books, has created many audio programs and videos, and has appeared on thousands\nof television and radio shows.\n5\nTable 9: A selection of substrings identiﬁed by E XACT SUBSTR as being in C4 multiple times. The number of\ntimes this exact substring occurs in C4 is also given.\nalready deduplicated of exact matches during their\ncreation, EXACT SUBSTR deduplication does not\nactually remove any examples.\nPerplexity on LM1B. Figure 7 is the same as\nFigure 2 of the main paper, except with perplexity\non LM1B included. LM1B was omitted from the\nmain paper’s ﬁgure in order to improve readability.\n18\n8441\n0 10 20 30 40 50\nPerplexity\nC4 Original\nC4 Duplicates\nC4 Unique\nLM1B\nWiki40B\nEvaluation dataset\nTraining data\nOriginal\nNearDup\nExactSubstr\n(a) Base model\n0 5 10 15 20 25 30 35\nPerplexity\nC4 Original\nC4 Duplicates\nC4 Unique\nLM1B\nWiki40B\nEvaluation dataset\nTraining data\nOriginal\nNearDup\nExactSubstr\n(b) XL model\nFigure 7: Impact of deduplicating the training set on validation perplexity. In (a), we plot the results from T5 base\n(110M parameters) across three training runs with different random initializations. The black bar represent the\nlowest perplexity to the highest perplexity, and the colored bar the median perplexity. In (b), we plot the results\nfrom T5 XL (1.5B parameters).\n19\n8442\nGenerated Text Freq in C4\n, you’ll need to be knowledgeable to make the very best decisions. We will make sure you know what\ncan be expected. We take the surprises from the picture by giving accurate and thorough information.\nYou can start by talking about your task with our client service staff when\nyou dial 888-353-1299. We’ll address all of your questions and arrange the initial meeting. We work\nclosely with you through the whole project, and our team can show up promptly and prepared.\n5,497\nthen Waterside Lodge are well equipped for the task. Our fully equipped family sized lodges offer\na comfortable luxurious stay for a fantastic price, giving you beautiful views of the lakes and the\nsurrounding countryside. Offering luxurious self-catering holidays in our fully featured Scandinavian\nholiday lodges. Perfectly located to explore the beaches, coastline. All of our lodges are sized for 6\npeople and are furnished to the highest standards to ensure you have a stay like no other. At Waterside\nLodge the stay itself is only half of the package, Waterside lodge is situated closely to the Heritage\nCoast which makes our lodges the perfect stay for anyone wanting to get away and have a relaxing\ncountryside break from the city. Whilst you stay with us be sure to take advantage of all the activities\nWaterside Lodge has to offer. Such as the use of our on-site ﬁshing lakes for the keen ﬁsherman, free\ninternet access, outside relaxation areas, comfortable lounges and much more.\n571\nyou are only looking to ﬁnd rent to own homes in your city or are open to exploring all kinds of rent to\nown home listings, our database does it all. One of the best aspects of iRentToOwn.com is that, besides\noptions to rent to buy a house, it has numerous other categories of home sale options. These include\nbank foreclosure homes, pre-foreclosure homes, short sales, HUD/government foreclosures, auction\nhomes and owner-ﬁnancing/FSBO (For Sale By Owner) homes. With help from the convenient search\nfeatures offered by our site, shoppers are able to ﬁnd their ideal lease to own home, real estate company,\nand more in South\n51\n, IL employs journeyman as licensed to work by themselves, without direct supervision, installing\nwiring, outlets and ﬁxtures. Our journeyman also does service work, troubleshooting when a breaker\nfails or a light stops working. Our journeyman does not offer permits that must be issued by our master.\nOur journeyman follows our master’s plans and directions. Our journeyman’s responsibilities will vary\nbased on the work that needs to be done. Our journeymen are skilled with residential, commercial and\nindustrial installations and repairs.ust work from six years as an apprentice, under direct supervision of\nour master, and pass a journeyman test. This person also must have some classroom education on the\nNational Electrical Code and fundamental electricity in a technical school a program afﬁliated with the\nNational Joint Apprenticeship Training Council. Journeyman training combines hands-on work with\neducation on basic electricity.\n6\ncombustion process of a petrol engine is never perfect. Dangerous gases, such as nitrogen oxide, carbon\nmonoxide and hydrocarbons will arise and it is the job of the catalytic converter to reduce these to safer\nemissions. These cat converters can fail by becoming clogged, or if the engine has bad exhaust valves\nor the plugs fail, causing unburned fuel to overheat the converter. Mettam’s Mufﬂers can resolve these\nissues with your Karr\n5\n,ANDREW Find the ancestral town: Many a researcher is stuck behind records that say, BIRTHPLACE:\nIRELAND without saying where in Ireland, or whatever other country. Remember that your immigrant\nancestor’s siblings probably were born in the same ancestral town, so check all o\nf their records, too. Around 1900, the Roman Catholic churches reported marriages to the churches\nwhere the persons were baptised, and before the wedding, they would require a baptismal certiﬁcate\nfrom that church, without marriage notations, to make sure that the persons were no\nt already married, ordained, or whatever, and were free to marry. Do check the Catholic records\nespecially for ex loco and the home town. If your ancestor’s sister had a daughter who generated a\nmarriage or death record saying, MOTHER’S BIRTHPLACE: and the exact town, then y\nou know where to start searching for records that will conﬁrm it is your ancestor’s home town.\nBEW ARE: Just because you ﬁnd a family with the same names does not mean they are the same family,\nas they could very well be an unrelated family from a different town in the same an\ncestral country. The webmaster has learned this. One clue was that one family was still having babies\nin Potenza city, Italy while the other was having babies in Colorado, U.S.A.\n2\nwill not want to search for Power Washing companies in Wyoming on an extensive basis. The service\npersonnel will be at your doorsteps through online or phone booking. The power wash solutions offered\nby us are matchless and you can compare with others in Winﬁeld, IL. The power wash services offered\nby us are very economical. Gutter brightener will be applied which will be followed by cleaning through\ndouble scrub. The cleaning will be done by using a soft bristle brush. The bond and contaminants will\nbe released in an effortless manner.\n1\nZ3 Plus are valid in all major cities of India like Delhi, Gurgaon, Noida, Mumbai, Chennai, Bangalore,\nHyderabad, Kolkata, Pune, Ahmedabad, Coimbatore, Lucknow, Trichy, Madurai, Trivandrum, Mysore,\nJaipur, Chandigarh, Pondicherry, Bhopal, Patna, Bhubaneswar, Amritsar, Cochin,\nAllahabad, Srinagar, New Delhi, Surat, Ludhiana, Navi Mumbai, Ghaziabad, Bengaluru, Indore,\nNagpur, Thane, Agra, Meerut, Ranchi. The delivery feasibility and charges may be varying, hence for\nthem please check with the particular seller or store.\n1\nTable 10: A selection of substrings generated by XL-O RIGINAL with no prompting (and top- k with k=50) that\nwere identiﬁed by E XACT SUBSTR as being in C4 multiple times. The number of times each substring was found\nin C4 is given. We observe that most memorized generations tend to be from advertisements.\n20\n8443\nRealNews Url # Total Frac Dups C4 Url # Total Frac Dups\nmedicalnewstoday.com. 12 1.00 hairtechkearney.com 4883 1\ndodbuzz.com 301 0.99 keywordsking.com 1786 1\nundertheradar.military.com 187 0.97 sydneysitalianfruitshops.online 1178 1\nq.usatoday.com 33 0.94 moewiki.usamimi.info 1001 1\nad-test.thirdage.com 354 0.94 swarovskijewelryoutlet.org 984 1\namp.nymag.com 15 0.93 forzadurto.org 980 1\ncitizenwire.com 1022 0.93 producerati.com 971 1\npaycheck-chronicles.military.com 363 0.92 sourceryforge.org 908 1\nproduct-reviews.net 73403 0.92 heavenz-kitchen.com 876 1\nkitup.military.com 196 0.92 little-eclipse.com 822 1\ngcaptain.com 33903 0.92 walops.com 819 1\ndev.screenrant.com 70 0.91 16thstlaunderland.com 713 1\nlive.swissinfo.ch 66 0.91 theroyalstarinfo.com 696 1\nnews.theepochtimes.com 82 0.87 code4kt.com 684 1\nopinion.toledoblade.com 986 0.87 nﬂfalconsjerseys.us 682 1\ncdn.moneytalksnews.com 121 0.86 quiltingbeeshop.com 676 1\namp.fox23.com 14 0.86 ulifeinsurancemiami.com 675 1\nsales.rollingstone.com 20 0.85 wowkeyword.com 673 1\nftp.screenrant.com 20 0.85 taspetro.com 671 1\nTable 11: On the left, we show the URLs that had the greatest proportion of examples marked as near-duplicates by\nNEAR DUP(ﬁltered to URLs which occurred at least 10 times). On the right, we show the 20 most frequent URLs\nin C4 for which all examples were marked as near-duplicates by NEAR DUP.\nTraining Dataset: C4-O RIGINAL C4-NEAR DUP C4-EXACT SUBSTR\nEpoch: 1 2 1 2 1 2\nNo prompt 1.93% 1.57% 0.19% 0.26% 0.14% 0.17%\nDuplicate Train Prompts 35.88% 34.34% 3.34% 3.15% 5.71% 4.67%\nUnique Train Prompt 0.42% 0.41% 0.42% 0.41% 0.22% 0.23%\nDuplicate Test Prompt 16.27% 15.32% 1.61% 1.52% 0.34% 0.25%\nUnique Test Prompt 0.25% 0.22% 0.21% 0.23% 0.03% 0.08%\nTable 12: Percentage of tokens in 100k generations that were part of memorized substring according to E XACT-\nSUBSTR . Models trained with approximate or exact deduplication have 10×less memorization than the model\ntrained on the original (non-deduplicated) dataset.\nFinal train set size in tokens Final train set size in examples\nORIGINAL NEAR DUP EXACT SUBSTR ORIGINAL NEAR DUP EXACT SUBSTR\nC4 177.3B 173.7B 165.4B 364.87M 350.48M 350.48M\nReal News 24.7B 22.4B 20.1B 31.16M 28.39M 28.39M\nLM1B 1.0B 0.94B 0.90B 30.30M 29.87M 30.16M\nWiki40B 2.25B 2.24B 2.19B 2.93M 2.91M 2.93M\nTable 13: Each row shows the size in tokens (according to our 50k BPE vocab) and in examples of a train set in its\noriginal form, with NEAR DUP deduplication, and with EXACT SUBSTR deduplication.\n21\n8444\n0 100 101 102 103 104 105 106 107\nNumber of groups\n1\n2\n3\n4\n5\n[6, 10)\n[11, 20)\n[21, 50)\n[51, 500)\n[501, 5000)\n[5001, )\nGroup sizes\n29,096,827\n595,632\n68,775\n15,825\n4,432\n2,762\n340\n66\n13\n0\n0 LM1B\n0 100 101 102 103 104 105 106 107\nNumber of groups\n1\n2\n3\n4\n5\n[6, 10)\n[11, 20)\n[21, 50)\n[51, 500)\n[501, 5000)\n[5001, )\nGroup sizes\n27,917,044\n1,715,379\n231,913\n89,017\n34,487\n24,889\n1,150\n243\n129\n11\n0 Real News\n0 100 101 102 103 104 105 106\nNumber of groups\n1\n2\n3\n4\n5\n[6, 10)\n[11, 20)\n[21, 50)\n[51, 500)\n[501, 5000)\n[5001, )\nGroup sizes\n3,228,888\n3,557\n833\n399\n245\n391\n163\n60\n24\n1\n0 Wiki40B\nFigure 8: The distribution of near-duplicate cluster\nsizes from running NEAR DUP on each dataset.\n22\n8445",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.7281004786491394
    },
    {
      "name": "Computer science",
      "score": 0.5830677151679993
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5474016666412354
    },
    {
      "name": "Computational linguistics",
      "score": 0.5259457230567932
    },
    {
      "name": "Natural language processing",
      "score": 0.4819600284099579
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40904858708381653
    },
    {
      "name": "Linguistics",
      "score": 0.396436870098114
    },
    {
      "name": "Library science",
      "score": 0.3706406354904175
    },
    {
      "name": "Cognitive science",
      "score": 0.33488208055496216
    },
    {
      "name": "History",
      "score": 0.2513757348060608
    },
    {
      "name": "Philosophy",
      "score": 0.22836315631866455
    },
    {
      "name": "Psychology",
      "score": 0.13923975825309753
    },
    {
      "name": "Physics",
      "score": 0.0849069356918335
    },
    {
      "name": "Archaeology",
      "score": 0.08122581243515015
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "China",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210113520",
      "name": "Brain (Germany)",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I36788626",
      "name": "California University of Pennsylvania",
      "country": "US"
    }
  ],
  "cited_by": 218
}