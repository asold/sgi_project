{
  "title": "ArabianGPT: Native Arabic GPT-based Large Language Model",
  "url": "https://openalex.org/W4392373243",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A290206851",
      "name": "Anis Koubaa",
      "affiliations": [
        "Prince Sultan University"
      ]
    },
    {
      "id": "https://openalex.org/A2121296178",
      "name": "Adel Ammar",
      "affiliations": [
        "Prince Sultan University"
      ]
    },
    {
      "id": "https://openalex.org/A2016908314",
      "name": "Lahouari Ghouti",
      "affiliations": [
        "Prince Sultan University"
      ]
    },
    {
      "id": "https://openalex.org/A5113132115",
      "name": "Omer Necar",
      "affiliations": [
        "Prince Sultan University"
      ]
    },
    {
      "id": "https://openalex.org/A5093081405",
      "name": "Serry Sibaee",
      "affiliations": [
        "Prince Sultan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6601378370",
    "https://openalex.org/W6833105446",
    "https://openalex.org/W4385572802",
    "https://openalex.org/W6611333299",
    "https://openalex.org/W3106433641",
    "https://openalex.org/W4386977752",
    "https://openalex.org/W3134155512",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4285289306",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4287548279",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4297823766",
    "https://openalex.org/W4386365131",
    "https://openalex.org/W3133440961",
    "https://openalex.org/W3008110149"
  ],
  "abstract": "The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning the models on tasks like sentiment analysis and summarization demonstrate significant improvements. For sentiment analysis, the fine-tuned ArabianGPT-0.1B model achieved a remarkable accuracy of 95%, a substantial increase from the base model's 56%. Similarly, in summarization tasks, fine-tuned models showed enhanced F1 scores, indicating improved precision and recall in generating concise summaries. Comparative analysis of fine-tuned ArabianGPT models against their base versions across various benchmarks reveals nuanced differences in performance, with fine-tuning positively impacting specific tasks like question answering and summarization. These findings underscore the efficacy of fine-tuning in aligning ArabianGPT models more closely with specific NLP tasks, highlighting the potential of tailored transformer architectures in advancing Arabic NLP.",
  "full_text": null,
  "topic": "Arabic",
  "concepts": [
    {
      "name": "Arabic",
      "score": 0.7886704206466675
    },
    {
      "name": "Computer science",
      "score": 0.5225089192390442
    },
    {
      "name": "Natural language processing",
      "score": 0.4670746624469757
    },
    {
      "name": "Linguistics",
      "score": 0.39385122060775757
    },
    {
      "name": "Philosophy",
      "score": 0.11314919590950012
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I142024983",
      "name": "Prince Sultan University",
      "country": "SA"
    }
  ]
}