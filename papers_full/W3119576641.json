{
  "title": "Transformer for Image Quality Assessment",
  "url": "https://openalex.org/W3119576641",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222575503",
      "name": "You, Junyong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4318742863",
      "name": "Korhonen, Jari",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3035595647",
    "https://openalex.org/W3100404621",
    "https://openalex.org/W3036239693",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2981374717",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2593290446",
    "https://openalex.org/W3091249416",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2891645170",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2081939415",
    "https://openalex.org/W2565639579"
  ],
  "abstract": "Transformer has become the new standard method in natural language processing (NLP), and it also attracts research interests in computer vision area. In this paper we investigate the application of Transformer in Image Quality (TRIQ) assessment. Following the original Transformer encoder employed in Vision Transformer (ViT), we propose an architecture of using a shallow Transformer encoder on the top of a feature map extracted by convolution neural networks (CNN). Adaptive positional embedding is employed in the Transformer encoder to handle images with arbitrary resolutions. Different settings of Transformer architectures have been investigated on publicly available image quality databases. We have found that the proposed TRIQ architecture achieves outstanding performance. The implementation of TRIQ is published on Github (https://github.com/junyongyou/triq).",
  "full_text": "TRANSFORMER FOR IMAGE QUALITY ASSESSMENT \n \nJunyong You1, Jari Korhonen2 \n \n1. NORCE Norwegian Research Centre, Bergen, Norway (junyong.you@ieee.org);  \n2. Shenzhen University, Shenzhen, China (jari.t.korhonen@ieee.org)  \n \nABSTRACT \n \nTransformer has become the new standard method in natural \nlanguage pro cessing (NLP), and it also attracts research \ninterests i n comp uter vision area. In this paper w e \ninvestigate the applicati on of Transformer in Image Quality \n(TRIQ) assessment. Following the original Transformer \nencoder employed in Vision Transformer (ViT), we propose \nan ar chitecture of using a shallow Transformer encoder  on \nthe top of  a feature map  extracted by convolution neural \nnetworks (CNN). Adaptive positional emb edding is \nemployed in the Transformer encoder to handle images with \narbitrary resolutions. Different setting s of Transformer \narchitectures have been investigated  on publicly available \nimage quality databases.  We have found that the proposed \nTRIQ a rchitecture achieve s outstanding performance. The \nimplementation of T RIQ is published on Github  \n(https://github.com/junyongyou/triq). \n \nIndex Terms— Attention, hybrid model, image quality \nassessment, Transformer \n \n1. INTRODUCTION \n \nTransformer [1] was first proposed to replace recurrent \nneural networks (RNN), e.g., long s hort-term memo ry \n(LSTM), in a n encoder-decoder architecture for machine \ntranslation tasks. It is purely based on attention mechanisms, \nincluding self -attention and encoder -decoder attention, \nwhich can overcome intrinsic shortages of RN N. Lately, \nTransformer has been adopted in computer vision (CV) \ntasks, e.g., object detection  (DETR) [2] and image \nrecognition (ViT) [3].  \nFollowing the original idea in Transformer, DETR [ 2] \nemploys the encoder-decoder architecture to predict either a \ndetection (ob ject class and bounding box) or a bac kground \nclass. Relevant image features are first learned from  2D \nconvolution neural net works (CNN), and then f ed into  the \nencoder together with  positional embedding.  The decoder \ntakes a small fixed numbe r of learned positional embedding \nand attends to the encoder  output for object detection . In \nViT for image recognition  [3], only the Transformer \nencoder is employed. A n input image is first divided into \nsmall patches (e.g., 16 ×16 pixels), which are then fl atten \nand projected onto patch embedding by a linear pro jection \nlayer. The l inear project ion works similarly to the word \nembedding in  the original Tra nsformer and  can also match \nthe image patch dimension to the model dimension of \nTransformer encoder. Subsequently, an extra e mbedding, \nsimilar to the [class] token in BERT [ 4], is added in the \nbeginning of the projected embeddings. This extra \nembedding is expected to represent the aggregated \ninformation for image representation on the whole set of \nimage patches  without be ing biased to any particular \npatches. Learnable positional embedding is added to the \nembeddings to ret ain positional info rmation. The  encoder \narchitecture is the same as the original  Transformer. This is \nalso a common ap proach of applying Transforme r in CV \ntasks, allowing the latest developments of Transformer to be \ndirectly applied . Consequently, a multi -layer percep tron \n(MLP) head is added on the top of the Transformer encoder \nfor image re cognition. A hybrid approach to apply  \nTransformer encoder on CNN features has also been \nsuggested in the ViT paper [3]. \nImage quality assessment (IQA) can be considered \nessentially as a recognition task, i.e., recognizing the quality \nlevel of an image . Therefore, we attempt to investigate how \nto app ly Transformer i n the IQA t ask. Existing deep \nlearning driven IQA models are mainly based on  CNN \narchitectures. A typical example is to use  CNN as a feature \nextractor and MLP on the top to predict image quality . Due \nto hardware restrictions, e arlier works  applied CNNs on \nsmall image  patches, e.g., AlexNet in [5] and VGG in [6], \nand then combined the quality predictions from the patches \ninto a sing le quality indicator. Later , larger image patches \n(e.g., 224×224 pixels) have been used [7]. However, patch-\nbased models often assume that image p atches share the \nsame quality level as their original full image, when training \nthe models. Such assumption might not be reliable, as image \nquality can be spatially var ying or viewers might not pay \nsimilar attention to different image regions. Thus, an end-to-\nend approach predicting quality from the whole image is \nmore desired in IQA. \nConsidering that image quality can be affected by \nspatial saliency distribution, Yang et al . [ 8] proposed an \nend-to-end multi-task network (SGDNet) to predict saliency \nmap and image quality jointly. Image saliency map can also \nbe used a s input to derive more perceptually consistent \nfeatures for quality prediction. Inspired by the mechanism of \nhierarchical representations in visual system  [9] and object \ndetection tas ks [10], Wu et al . [ 11] proposed a cascaded \narchitecture ( CaHDC) to derive features at different s cales, \nand then combined them into quality prediction  by a side \npooling net . In [1 2], Hosu et al . first built a large scale  \nimage quality database (Kon IQ-10k). A simple yet effective \narchitecture has then been proposed consisting of a base \nCNN network (Inception-Resnet V2) followed by MLP.  \nIQA has a particu lar cha racteristic that distinguishes it \nfrom other CV  tasks (e.g., object detection  or image \nrecognition): image quality can be significantly influenced \nby resolution. In other w ords, image quality can be \npotentially affected  by resizing, e.g., significantly down-\nsampling images might degrade their perceived quality. \nEven thoug h image resizing is widely employed in deep \nlearning models for other CV tasks, it should be avoided in \nIQA models. In principle, Transformer can accept inputs \nwith varying lengths (e.g., sentences with varying number s \nof words). Thus, it is also possi ble to adapt the Transformer \nencoder to images with different resol utions to build a \ngeneric IQA model. \n The remainder of the paper is organized as follows.  \nSection 2 presents the Transformer encoder for Image \nQuality (T RIQ) assessment. Detailed experiments ar e \ndiscussed in Sec tion 3, and Section 4 draws the concluding \nremarks. \n \n2. TRIQ: TRANSFORMER FOR IMAGE QUALITY \nASSESSMENT \n \nViT [3] contains two approaches: image patching-based and \nhybrid. The form er is to divide an image into patches with \nfixed size (e.g., 32×32 pixels), and the latter us es a feature \nmap produced by other CNN  base networks (e.g., ResNet). \nThe flattened image patches o r feature map will th en be fed \ninto the Transformer encoder. In order to better unify the \npatching-based and hybrid approaches, a 2D convolutional \nlayer (Conv2D-projection) is employed for feature \nprojection. The number of kernels in the project ion layer is \nset to the model di mension (D) of the Transformer encoder, \nas Transformer uses constant latent vector size D. For the \nimage patching approach , the kernel size and stride of the \nConv2D-projection layer are both set to the same as the \npatch size. For example, if a patch size 32×32 pixels is used, \nthe projected features have a  shape of  [H/32, W/32,  D], \nwhere H, W denote the image height and width. On the other \nhand, if the hybrid approach is used , the activation map of \nthe final block of the CNN is employed as the feature map . \nFor example, if ResNet50 is used as the base network , it \nproduces the feature map with a shape  of [H/32, W/32 , \n2048], where 2048 is the channel dimension. Subsequently, \nthe kernel size and stride in the Conv2D-projection layer are \nset to 1. Consequently, th e projected features also have a  \nshape [H/32, W/32, D]. \nThe proposed TRIQ architecture follows the hybrid \napproach, as illustrated in Fig. 1 . ResNet50 was chosen as \nthe base network. It should be  noted that other CNN \narchitectures can also be used.  \nViT for image recogni tion can take constant input size \nwith the help of  image resizing. However, as explained \nearlier, image resizing should be avoided in IQA. Therefore, \nTRIQ needs to handle images  with different resolutions. \nThis mainly concerns the positional embedding representing \npositional information of image patches or CNN features for \nimage quality perception. To solve this issue, we define the \npositional embed ding with sufficient length to cover the  \n1×32 1×32 1×32 1×32 1×32F0: Extra learnable \nIQ embedding\nFeature Projection \n(Conv2D Layer, kernel=32, size=1, stride=1 )\nF1 F2 F3 F4 FN\nPosition \nEmbedding\nTransformer Encoder\nMLP\nHead\nQuality \ndistribution\nTRIQ Architecture\nMulti-Head \nAttention (mask)\nDropout\nAdd & Norm\nFeed-forward\nDropout\nAdd & Norm\nTransformer Encoder\n2×1×32\n(1 + N) × 32\n(1 + N) × 32\nF0PE0 PE1 PE2 PE3 PE4 PEN\nF0 F1PE0 PE1 FNPEN\nRGB image C2 C3 C4 C5\nH × W × 3 — × — × 1204H\n16  \nW\n16  — × — × 512H\n8  \nW\n8  — × — × 256H\n4  \nW\n4  — × — × 2048H\n32  \nW\n32  \n— × — × 32H\n32  \nW\n32  \nMax-pooling \n(size P=1/2/4) —— × —— × 32H\n32×P  \nW\n32×P  \nFlatten \n1×32\nFxPEx\n—— × ——H\n32×P  \nW\n32×P  N=\n \nFig.1. Architecture of TRIQ and Transformer encoder (the numbers indicate actual output shape of layers/blocks in our \nexperiments). \nmaximal image resolution in our datasets, which is the n \ntruncated for individual smaller images.  \nFurthermore, when an input image has large  resolution, \nthe number of image  patches or CNN features (e.g., H/32 × \nW/32) can also be large. This causes two potential issues. \nThe first is that a lot of  memory is req uired to run the \nTransformer encod er on  an image with  a very high \nresolution. Second, a large number of patches/ features will \nalso cause di fficulty to the Transfo rmer encoder to c apture \nlong-term d ependence across the patches/ features. \nTherefore, max-pooling is performed, depending on the \ninput image resolu tion. For TRIQ with hybrid Transformer, \nmax-pooling is applied to the CNN features bef ore feeding \nthem into the Conv2D-projection layer. Whereas, for TRIQ \nwith patching approach, image pat ches are first fed into the \nConv2D-projection layer , and then max-pooling is \nperformed. The pooling size is adaptively determined based \non the resolu tion of the input image . Thus, the projected \nfeatures from the Conv2D-projection layer have a shape  \n[H/(32×P), W/(32×P), D], where P is the max-pooling size.  \nSubsequently, the two spatial dimensions of the pooled \nfeatures are first flattened to shape [ N=(H×W)/ \n(32×32×P×P), D], and then the learnable positional \nembeddings (PE) are added. As explained in ViT and BERT \n[4], an extra embedding (F0) is appended in front of  the \nprojected features  (F), which is also ad ded by a positional \nembedding (PE0). Eq. (1) roughly  explains the projection, \nmax-pooling and positional embedding , where FM denotes \nthe feature map. \n \n(1 )\n0 0 0\n[ 2 _ ( )], 1,\n[ ; ],\nj\nND\nN N j\nF Max Conv D proj FM j N\nZ F PE F PE PE R +\n== = + + \n          (1) \nThe Transformer encoder, specified essentially by four \nkey hyper-parameters: L (number of layers), D (model \ndimension), H (number of heads), and dff (dimension of the  \nfeed-forward network), consists of an alternative number of \nencoder layers . As shown in Fig. 1, each encoder layer \ncontains two sublayers: t he first is for multi -head attention \n(MHA), and the se cond contains position-wise feed-forward \n(FF) layer. Layer normalization ( LN) is performed on the \nresidual connection in the two  sublayers. The Transformer \nencoder, as briefly explained in Eq. ( 2), produces an output \nof shape (1+N)×D. \n         \n11( ( ) )\n1,\n( ( ) )\nl l l\nl l l\nZ LN MHA Z Z\nlL\nZ LN FF Z Z\n−−=+ = =+\n             (2) \nFinally, the first vector of output ( ZL[0]), that is \nsupposed to contain aggregated information  from the \nTransformer encoder for image quality perception, is fed \ninto a n MLP head. The MLP head cons isting of two fully \nconnected (FC) layers an d a dropo ut layer in between \npredicts the perceived image quality. The first FC layer uses \ndff filters with GELU activation, as suggested in BERT [4] \nand ViT [3]. Earlier studies have demonstrated that \npredicting quality distribution o ver grade to vote (e.g., \n1=bad, 2=poor, 3=fair, 4=  good, 5=excellent) often provide \nmore robust results than single MOS values [13]. Thus, we \nuse five filters in the last FC layer with Softmax activation \nto predict the quality distribution, i.e., normalized \nprobabilities over the five  quality grades . Consequ ently, \ncross entropy is chosen as the loss function to measure the \ndistance betwe en the predicted image quality distribution \nand the ground-truth distribution. Assuming p(x) denotes the \nnormalized probability predicted by TRIQ, a single MOS \nvalue can be easily derived as defined in Eq. (3). \n                          \n{1,2,3,4,5}\n()\nx\nMOS x p x\n\n=                             (3) \n \n3. EXPERIMENTS \n \n3.1. Experiment settings \n \nTwo large -scale image quality datab ases have been \nemployed in the experim ents, namely Kon IQ-10k [12] and \nLIVE-wild databases [14]. Both the MOS values and quality \nscore distribution in the KonIQ-10k database were \npublished by the authors . The authors of LIVE-wild \ndatabase released standard deviations of quality scores, and \nthe score distribution can be derived by assuming the scores \nfollow a truncated Gaussian distribution. The two databases \nshare many similarities, e.g., same method ology of  \ncrowdsourcing, and natural image content with non-specific \ndistortions. However, it is inappropriate to combine IQA \ndatabases fr om different experiments directly. The m ain \nconcern is calibration of rating scales , e.g., identi cal image \ncontent might be assigned wit h dissimilar quality levels in \ndifferent experiments. We have conducted a small -scale \nexperiment using five p articipants to compare 20 image \npairs to align the KonIQ-10k and LIVE -wild databases. An \nimage pair consisting of an  image from KonIQ -10k and \nanother from LIVE-wild with the same rating scales and the \nparticipants were asked to compare them a nd to judge if  \nthey represented a similar quality level.  \nThe calibration experiment suggested that almost all the \nrandomly selected 20 image pairs share similar qu ality \nlevels. Thus, the two databases were combined in this wo rk. \nSubsequently, we split the t wo databases into training and \ntesting sets  randomly according to both MOS values  and \nimage complexity, measured by  spatial information (SI) as \ndefined in the ITU Recom mendation [15]. All images were \nfirst roughly classified  into two complexity categories : high \nand low SI, and then the images in each category are further \ndivided into f ive quality categories based on their MOS \nvalues. As the number of images in the KonIQ -10k database \nis much higher LIVE-wild, we randomly chose  85% of the \nimages in KonIQ -10k and 50% of the  images in LIVE -wild \nfrom each quality category in each complexity category as \ntraining images, and the rest of the images as testing images. \nIn addi tion, the authors of KonIQ -10k data base also used \nhalf-sized images in the database wit h the quality scores  \nvoted on the original  full resolution images to develop the \nKoncept512 model [12]. We followed this a pproach to \ninclude the half -sized image s, named as KonIQ -half-sized \ndatabase, in our  experiment. Consequently, a combined \ndatabase was generated consisting of KonIQ -10k, KonIQ -\nhalf-sized, and LIV E-wild dat abases, and t he contained \ntraining and testing sets wer e inherited from the original \nsplits of the individual databases. \nThree state-of-the-art IQA models, namely SGDNet [8], \nCaHDC [ 11] and Koncept512 [12], were included in our \nexperiments as a comparison point.  These models can be \nadjusted to accept images with varying resol utions by  not \nspecifying input  size and using global pooling bef ore the \nprediction layers. The models were retrained on our training \nand testing sets  using t he original implementations and \ntraining strategies published by the authors, respectively . It \nshould be  noted that SGDNet requires a saliency map to \npredict imag e qua lity, which is unava ilable in the LIVE -\nwild database. Thus, SGDNet was not included in the \nexperiments involving the LIVE-wild database.  \nIn TRIQ,  the output from the last residual block is \nchosen as the feature map . Subsequently, a shallow \narchitecture for the Transformer encoder [L=2, D=32, H=8, \ndff=64] has been e mployed, which obtained the highest \nprediction accuracy in our e xperiments. Other se ttings for \nthe Transformer en coder have also been test ed, while we \nfound that they both produce worse resu lts t han the above \none. \nThe official implementations of ViT contain a patching \nbased approach using two patch sizes and a hybrid approach \nbased on Res net50 V2 as base network . Model va riants \nbased on configurations for  BERT have been proposed.  \nAccordingly, the pretrained weights on large-scale databases \nhave al so been released [ 16]. In our experiments, we have \nbased on  two base ViT models (ViT-B_16 and R50+ViT-\nB_16 [16]) to predict image quality. The published weights \npretrained on ImageNet21k  have been employed for \napplying tran sfer learning for IQA in ou r experiments. The \ntwo models are  named as ViT -IQA and R50+ViT-IQA, \nrespectively.   \nSubsequently, TRIQ, ViT-IQA and R50+ViT-IQA were \ntrained on the training set using Adam as  optimizer. A \nlearning rate sche duler with linear warm-up and cosine \ndecay was applied. A base learning rate 5e -5 was used for \npretraining and the best model  weights were stored when \nthey produced the highest Pear son correlation (PLCC) \nbetween the predicted MOS values and ground -truth on the \ntesting sets. Subsequently, model finetuning was performed \nwith a smaller learning rate 1e -6, based on the best weights \nproduced in the pretraining phase.  \n \n3.2. Evaluation results and discussions \n \nThree criteria, namely PL CC, Spearman rank -order \ncorrelation (SROCC), and ro ot mean square d error (RMS E) \nbetween MOS predictions and grou nd-truth values, have \nbeen employed to evaluate indi vidual IQA mo dels in \ndifferent scenarios. All the studied  models were trained on \nthe combined database and separate databases, respectively.  \nIn the first experiment, the models were trained on the \ntraining set of the combined database, and then evaluated on \nthe testing set.  Table I reports the evaluation results, which \ndemonstrate that TRIQ outperforms significantly other IQA \nmodels. Even though  patching-based ViT has achieved \npromising performance in image reco gnition with the help \nof pretrain on large-scale databases, it does not beat TRIQ in \nIQA. We suspect the main reason is that relatively s mall-\nscale databases are used fo r IQA in this work, and they  \ncannot take the full advantage of large-scale pretraining. \nThe hybrid approach of ViT, including the pr oposed TRIQ, \nTable I. Evaluation results on the Combined testing set  \nModels PLCC↑ SROCC↑ RMSE↓ \nCaHDC 0.625 0.628 0.473 \nKoncept512 0.719 0.755 0.601 \nViT-IQA 0.775 0.764 0.388 \nR50+ViT-IQA 0.858 0.835 0.304 \nTRIQ 0.884 0.868 0.280 \n \nTable II. Evaluation results on the entire SPAQ \ndatabase  \nModels PLCC↑ SROCC↑ RMSE↓ \nCaHDC 0.647 0.656 0.720 \nKoncept512 0.817 0.825 0.607 \nViT-IQA 0.754 0.762 0.585 \nR50+ViT-IQA 0.845 0.845 0.534 \nTRIQ 0.848 0.857 0.480 \n \n \n  \n  \n \n  \n  \n \n  \n  \nFig. 2. Distribution of average attention weights over \nmulti-heads (left: TRIQ model; right: ViT model for  \nimage recognition) \n \nhas shown outstanding performance in IQA. We assume this \nis because the inductive capability of CNN architectures can \nmore appropriately capture image quality features than \npatches purely. As the employed datase ts are all in s mall \nscale, a shallow architecture of Transformer en coder used in \nTRIQ, rather than a deep er one in R50 +ViT-IQA, is \nsufficient for capturing the characteristics of CNN -derived \nfeatures for image quality perception.  \nTable II reports the results of the models trained on the \ncombined training set and then evaluated on the SPAQ \ndatabase [17]. Interested readers can refer to [17] for the \ndetails about the SPAQ database. We tested the IQA models \non the entire SPAQ database. TRIQ st ill shows promising \nperformance across databases. \nIn addition, t he train ed models we re also tested on \nindividual testing sets from respective databases, and Table \nIII gives the evaluation results. This demonstrates promising \nperformance of TRIQ as a g eneric IQA model for diverse \nimage co ntents and r esolutions, compared with other stat e-\nof-the-art models. \nIn ord er to evaluate the model performance across \nresolutions and databases,  we have also trained the models \non K onIQ-10k, KonIQ-half-sized databas es, and their \ncombination, respect ively. The models were evaluated on \nboth the testing sets of KonIQ-10k and KonIQ -half-sized \ndatabases and the entire LIVE -wild database. Tables IV, V \nand VI report the respective evaluation results. ViT-IQA \nand R50+ViT-IQA still perform wor se than TRIQ in these \nTable III. IQA models trained on Combined training set \nand evaluation on the separate testing sets of KonIQ-10k & KonIQ-half-sized and LIVE-wild databases \nModels Testing set of KonIQ-10k Testing set of KonIQ-half-\nsized \nTesting set of LIVE-wild \ndatabase \nPLCC↑ SROCC↑ RMSE↓ PLCC↑ SROCC↑ RMSE↓ PLCC↑ SROCC↑ RMSE↓ \nCaHDC 0.702 0.721 0.442 0.731 0.701 0.386 0.551 0.542 0.693 \nKoncept512 0.789 0.828 0.570 0.749 0.788 0.637 0.693 0.704 0.592 \nTRIQ 0.923 0.909 0.213 0.895 0.868 0.249 0.826 0.812 0.449 \n \nTable IV. IQA models trained on KonIQ-10k training set \nand evaluation on the testing sets of KonIQ-10k & KonIQ-half-sized and entire LIVE-wild database \nModels Testing set of KonIQ-10k Testing set of KonIQ-half-\nsized Entire LIVE-wild database \nPLCC↑ SROCC↑ RMSE↓ PLCC↑ SROCC↑ RMSE↓ PLCC↑ SROCC↑ RMSE↓ \nSGDNet 0.840 0.819 0.300 0.720 0.664 0.396 — — — \nCaHDC 0.750 0.729 0.368 0.611 0.544 0.502 0.414 0.384 0.746 \nKoncept512 0.854 0.854 0.293 0.745 0.722 0.374 0.738 0.716 0.591 \nTRIQ 0.925 0.907 0.212 0.837 0.818 0.349 0.805 0.781 0.503 \n \nTable V. IQA models trained on KonIQ-half-sized training set \nand evaluation on the testing sets of KonIQ-10k & KonIQ-half-sized and entire LIVE-wild database \nModels Testing set of KonIQ-10k Testing set of KonIQ-half-\nsized Entire LIVE-wild database \nPLCC↑ SROCC↑ RMSE↓ PLCC↑ SROCC↑ RMSE↓ PLCC↑ SROCC↑ RMSE↓ \nSGDNet 0.772 0.777 0.431 0.823 0.798 0.320 — — — \nCaHDC 0.627 0.658 0.458 0.763 0.723 0.364 0.578 0.575 0.686 \nKoncept512 0.869 0.876 0.575 0.923 0.901 0.215 0.819 0.806 0.681 \nTRIQ 0.893 0.882 0.327 0.925 0.905 0.211 0.800 0.779 0.493 \n \nTable VI. IQA models trained on KonIQ-10k & KonIQ-half-sized training sets \nand evaluation on the testing sets of KonIQ-10k & KonIQ-half-sized and entire LIVE-wild database \nModels \nCombined Testing set \nof KonIQ-10k & \nKonIQ-half-sized \nTesting set of KonIQ-\n10k \nTesting set of \nKonIQ-half-sized \nEntire LIVE-wild  \ndatabase \nPLCC SROCC RMSE PLCC SROCC RMSE PLCC SROCC RMSE PLCC SROCC RMSE \nSGDNet 0.813 0.782 0.322 0.860 0.843 0.284 0.766 0.721 0.355 — — — \nCaHDC 0.683 0.669 0.411 0.693 0.723 0.436 0.750 0.718 0.385 0.566 0.575 0.687 \nKoncept512 0.807 0.817 0.342 0.871 0.870 0.281 0.785 0.809 0.285 0.708 0.722 0.613 \nTRIQ 0.900 0.880 0.244 0.919 0.906 0.219 0.889 0.861 0.266 0.845 0.784 0.500 \n \n \nexperiments. According to the results, cross-resolution and \ncross-database clearly have a negative impact on  the \nperformance o f IQA models , even tho ugh TRIQ shows \npromising potential also in this scenario . This issue will be \ninvestigated to develop a more generic IQA model  in the \nfuture work.  \nFurthermore, as  the self-attention mechanism  is \nemployed in the Transformer encoder, it is inter esting to \ninvestigate how attention works in IQA. Thus, the attention  \nweights learned during the training process are visualized by \naveraging the attention weights over different heads, and \nthen applying the normalized weights as a mask on the input \nimage. As comparison, the pretrained ViT-B16 weights [16] \nfor image recognition were also used to present the attention \ndistribution in recognition task. Fig. 2 shows a few \nrandomly chosen images with the masked at tention \ndistribution. Compar ed with image recognition  where \nattention is mainly  allocated to semantic objects, the \nattention distributi on is spreading more widely  in IQA , \nrather than focusing on regions of interest. We assume this \nis because the  voters (subjective participants  or objective \nquality models ) often need to concentrate on the whole \nimage area to collec t more information when assessing \nimage quality. \n \n4. CONCLUSIONS \n \nA pr omising IQA model TRIQ has been proposed  in this \nwork. TRIQ takes the advantage of inductive capability of \nCNN arch itecture for quality f eature derivation and \nTransformer encoder for aggregated representation of \nattention mechanism. TRI Q has demonstrated outstandin g \nperformance o n two publicly available large -scale image \nquality databases, compared with other deep lear ning driven \nIQA models . TRIQ shows  another p romising example to \nexploit Transformer in computer vision tasks. Future work \nwill focus on developing more generic IQA models that can \nadapt to diverse image contents, resolutions and distortion \ntypes.  \n \n5. REFERENCES \n \n[1] A. Vaswan i, N. Shazeer,  N. Parmar, J. Uszkoreit, L. \nJones, A.N. Gomez, L. Kaiser, and I. Polosukhin, “Attention \nis All Your Need”, In Proc. Adv. Neural Inf. Pro cess. Syst. \n(NIPS), Dec. 2017, Long Beach, CA, USA. \n[2] N. Carion, F . Massa, G . Synnaeve, N . Usunier, A . \nKirillov, and S. Zagoruyko, “End-to-End Object Detection \nwith Transformers,” in Proc. European Conf. Comput. Vis. \n(ECCV), Aug. 2020. \n[3] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. \nWeissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. \nMinderer, G. Heigold,  S. Gelly, J. Uszkoreit, and N. \nHoulsby, “An Image Is Worth 16x16 Words: Transformers \nfor Image Recognition At Scale,” in Proc. Int. Conf. \nLearning Representations  (ICLR’21), available on arXiv, \nhttps://arxiv.org/pdf/2010.11929.pdf. \n[4] J. Devlin, M. -W. Chang,  K. Lee, and K. Toutanova. \n“BERT: Pre-training of Deep Bidirectional Transformers for \nLanguage Understanding,” in Proc. NAACL -HLT, Jun. \n2019, Minneapolis, Minnesota, USA.  \n[5] A. Krizhevsky, I. Sutsk verer, and G. Hilton, “ImageNet \nClassification w ith Deep Convolutional Neural Networks,” \nin Proc. Adv. Neural Inf. Process. Syst . (NIPS), Dec. 2012, \nLake Tahoe, NV, USA. \n[6] K. Simonyan, and A. Zisserman, “Very Deep \nConvolutional Networks for Large-scale Image \nRecognition,” in Proc. Int. Conf. on Learni ng \nRepresentations (ICLR), May 2015, San Diego, CA, USA. \n[7] Y. Li, L -M. Po, L. Feng, and F. Yuan, “No -reference \nImage Quality Assessment with Deep Convolutional Neural \nNetworks,” in Proc. IEEE Int. Conf. Dig it. Signal Pro cess. \n(DSP), Oct. 2016, Beijing, China. \n[8] S. Yang, Q. Jiang, W. Lin , and Y. Wang, “SGDNet: An \nEnd-to-end Saliency-guided Deep Neural Network for No-\nreference Image Quality Assessment,” in Proc. ACM Int. \nConf. Multimed. (MM), Oct. 2019, Nice, France. \n[9] S. Hochstein and M. Ahissar, “View  from the top: \nHierarchies and  reverse hierarchies in the visual system,” \nNeuron, vol. 36, no. 5, pp. 791–804, 2002. \n[10] T-Y. Lin, P. Dollár, R. Girshick, K. He, B.  Hariharan, \nand S. Belongi e, “Feature Pyramid Networks fo r Object \nDetection,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis. \nPattern Recognit. (CVPR), Jul. 2017, Honolulu, HI, USA. \n[11] J. Wu, J. Ma, F. Liang, W. Dong, G. Shi, W. Lin, “End-\nto-end Blind Image Quality Prediction with Cascaded Deep \nNeural Network,” IEEE Trans. Image Process ., vol. 2 9, pp. \n7414-7426, Jun. 2020. \n[12] V. Hosu, H. Lin, T. Sziranyi, and D. Saupe, “K onIQ-\n10k: An Ecologically Valid Database for Deep Learning of \nBlind Image Quality Assessment,” IEEE Trans. Image \nProcess., vol. 29, pp. 4041-4056, Jan. 2020. \n[13] H. Zeng, L. Z hang, and A.C. Bovik, “Blind Image \nQuality Assessment with a Probabilistic Quality \nRepresentation,” in Proc. IEEE. Int. Conf. Image Process. \n(ICIP), Oct. 2018, Athens, Greece. \n[14] D. Ghadiyaram, and A.C. Bov ik, “Massive Online \nCrowdsourced Study of Subjective and Objective Picture \nQuality,” IEEE Trans. Image Process ., vol. 25, no. 1,  pp. \n372-387, Nov. 2015. \n[15] ITU-T Recommendation P.910, “Subjective Video \nQuality Assessment Methods for Multimedia Applications,” \nITU, Apr. 2008. \n[16] Vision Transformer , av ailable onli ne: \nhttps://github.com/google-research/vision_transformer. \n[17] Y. Fang, H . Zhu, Y . Zeng, K . Ma, and Z. Wang, \n“Perceptual Quality Assessment of Smartphone \nPhotography,” in Proc. IEEE  Comput. Soc. Conf. Comput. \nVis. Pattern Recognit. (CVPR), 2020. \n ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.730175256729126
    },
    {
      "name": "Computer science",
      "score": 0.7269729375839233
    },
    {
      "name": "Encoder",
      "score": 0.5918450951576233
    },
    {
      "name": "Architecture",
      "score": 0.5826818943023682
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5519610047340393
    },
    {
      "name": "Embedding",
      "score": 0.5223132967948914
    },
    {
      "name": "Image quality",
      "score": 0.4674677848815918
    },
    {
      "name": "Computer vision",
      "score": 0.45043492317199707
    },
    {
      "name": "Image (mathematics)",
      "score": 0.1913830041885376
    },
    {
      "name": "Engineering",
      "score": 0.15062350034713745
    },
    {
      "name": "Electrical engineering",
      "score": 0.13455316424369812
    },
    {
      "name": "Voltage",
      "score": 0.08447840809822083
    },
    {
      "name": "Geography",
      "score": 0.06348741054534912
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}