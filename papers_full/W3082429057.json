{
  "title": "HittER: Hierarchical Transformers for Knowledge Graph Embeddings",
  "url": "https://openalex.org/W3082429057",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2912935011",
      "name": "Sanxing Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101917160",
      "name": "Xiaodong Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104437897",
      "name": "Jian-Feng Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1972451723",
      "name": "Jian Jiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2162372842",
      "name": "Ruofei Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2153992776",
      "name": "Yangfeng Ji",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3023009950",
    "https://openalex.org/W2963432357",
    "https://openalex.org/W2951515642",
    "https://openalex.org/W2283196293",
    "https://openalex.org/W2511149293",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3103296573",
    "https://openalex.org/W2909137510",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2951048068",
    "https://openalex.org/W2728059831",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3034239155",
    "https://openalex.org/W3034758281",
    "https://openalex.org/W2988237903",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W3106378800",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2995448904",
    "https://openalex.org/W3006500814",
    "https://openalex.org/W2184957013",
    "https://openalex.org/W2996268457",
    "https://openalex.org/W3114500711",
    "https://openalex.org/W2250342289",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1852412531",
    "https://openalex.org/W2962886429",
    "https://openalex.org/W205829674",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2970836468",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2964311892",
    "https://openalex.org/W2998702685",
    "https://openalex.org/W2964116313",
    "https://openalex.org/W3167292670",
    "https://openalex.org/W2997012196",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3034862985",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W2995925258",
    "https://openalex.org/W3175989614",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2128407051",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3169726359",
    "https://openalex.org/W1662382123",
    "https://openalex.org/W2963450615",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3042770487",
    "https://openalex.org/W2946088473",
    "https://openalex.org/W2432356473",
    "https://openalex.org/W3039064020",
    "https://openalex.org/W2963203544",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W3019011053",
    "https://openalex.org/W2951105272",
    "https://openalex.org/W2949434543",
    "https://openalex.org/W2892280852",
    "https://openalex.org/W3034374701",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2250184916",
    "https://openalex.org/W2996028985",
    "https://openalex.org/W2964279602",
    "https://openalex.org/W3035134435",
    "https://openalex.org/W2964321699"
  ],
  "abstract": "This paper examines the challenging problem of learning representations of entities and relations in a complex multi-relational knowledge graph. We propose HittER, a Hierarchical Transformer model to jointly learn Entity-relation composition and Relational contextualization based on a source entity’s neighborhood. Our proposed model consists of two different Transformer blocks: the bottom block extracts features of each entity-relation pair in the local neighborhood of the source entity and the top block aggregates the relational information from outputs of the bottom block. We further design a masked entity prediction task to balance information from the relational context and the source entity itself. Experimental results show that HittER achieves new state-of-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10395–10407\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n10395\nHittER: Hierarchical Transformers for Knowledge Graph Embeddings\nSanxing Chen∗\nUniversity of Virginia\nsc3hn@virginia.edu\nXiaodong Liu, Jianfeng Gao\nMicrosoft Research\n{xiaodl,jfgao}@microsoft.com\nJian Jiao, Ruofei Zhang\nMicrosoft Bing Ads\n{jiajia,bzhang}@microsoft.com\nYangfeng Ji\nUniversity of Virginia\nyangfeng@virginia.edu\nAbstract\nThis paper examines the challenging problem\nof learning representations of entities and rela-\ntions in a complex multi-relational knowledge\ngraph. We propose HittER, a Hierarchical\nTransformer model to jointly learn Entity-\nrelation composition and Relational contextu-\nalization based on a source entity’s neighbor-\nhood. Our proposed model consists of two dif-\nferent Transformer blocks: the bottom block\nextracts features of each entity-relation pair in\nthe local neighborhood of the source entity and\nthe top block aggregates the relational infor-\nmation from outputs of the bottom block. We\nfurther design a masked entity prediction task\nto balance information from the relational con-\ntext and the source entity itself. Experimental\nresults show that HittER achieves new state-\nof-the-art results on multiple link prediction\ndatasets. We additionally propose a simple\napproach to integrate HittER into BERT and\ndemonstrate its effectiveness on two Freebase\nfactoid question answering datasets.\n1 Introduction\nKnowledge graphs (KG) are a major form of knowl-\nedge bases where knowledge is stored as graph-\nstructured data. Because of their broad applications\nin various intelligent systems including natural lan-\nguage understanding (Logan et al., 2019; Zhang\net al., 2019b; Hayashi et al., 2020) and reason-\ning (Riedel et al., 2013; Xiong et al., 2017; Bauer\net al., 2018; Verga et al., 2021), learning represen-\ntations of knowledge graphs has been studied in a\nlarge body of literature.\nTo learn high quality representations of knowl-\nedge graphs, many researchers adopt the idea of\nmapping the entities and relations in a knowledge\ngraph to points in a vector space. These knowledge\ngraph embedding (KGE) methods usually lever-\nage geometric properties in the vector space, such\n∗Work was done during an internship at Microsoft Bing\nAds.\nSunnyvale\nCaliforniaCupertino\nUnited States\nadjoins state\ncountry\nFigure 1: An example subgraph sampled from FB15K-\n237. Four nodes (entities) are connected by three\ndifferent types of relations representing facts like\nSunnyvale belongs to the state of California.\nas translation (Bordes et al., 2013), bilinear trans-\nformations (Yang et al., 2015, DistMult), or rota-\ntion (Sun et al., 2018). Multi-layer convolutional\nnetworks are also used for KGE (Dettmers et al.,\n2018, ConvE). Such KGE methods are conceptu-\nally simple and can be applied to tasks like factoid\nquestion answering (Saxena et al., 2020) and lan-\nguage modeling (Peters et al., 2019).\nHowever, it is rather challenging to encode all\nof the information about an entity into a single vec-\ntor. For example, to infer the missing object in\nthe incomplete triplet <Sunnyvale, county,\n?> (Figure 1), traditional KGE methods rely on the\ngeographic information stored in the embedding\nof Sunnyvale. While we can read such informa-\ntion from its graph context, e.g., from a neighbor\nnode that represents the state it belongs to (i.e.,\nCalifornia). In this way, we allow the model\nto store and utilize information about an entity via\nits relational context. To implement this process,\nprevious work uses graph neural networks (GNN)\nor attention-based approaches to learn representa-\ntions based on both entities and their graph con-\ntext (Kipf and Welling, 2017; Bansal et al., 2019;\n10396\nVashishth et al., 2020). However, these methods\nare usually restricted in expressiveness because of\nthe shallow network architecture they use.1\nIn this paper, we present HittER, a deep hier-\narchical Transformer model to learn representa-\ntions of entities and relations in a knowledge graph\njointly by aggregating information from graph\nneighborhoods. Although prior work shows Trans-\nformers can learn relational knowledge from large\namounts of unstructured textual data (Jiang et al.,\n2020; Manning et al., 2020), HittER explicitly op-\nerates over structured inputs using a hierarchical\narchitecture. Essentially, HittER consists of two\nlevels of Transformer blocks. As shown in Figure 2,\nthe bottom block provides relation-dependent en-\ntity embeddings for the neighborhood around an\nentity and the top block aggregates information\nfrom its graph context. To ensure HittER work\nacross graphs of different properties, we further de-\nsign a masked entity prediction task to balance the\ncontextual relational information and information\nfrom the training entity itself.\nWe evaluate the proposed method using the link\nprediction task, which is one of the canonical tasks\nin statistical relational learning (SRL). Link pre-\ndiction (essentially KG completion) serves as a\ngood proxy to evaluate the effectiveness of learned\ngraph representations, by measuring the ability\nof a model to generalize relational knowledge\nstored in training graphs to unseen facts. Mean-\nwhile, it has an important application to knowl-\nedge graph completion given the fact that most\nof the knowledge graphs are still highly incom-\nplete (West et al., 2014). Our approach achieves\nnew state-of-the-art results on two standard bench-\nmark datasets: FB15K-237 (Toutanova and Chen,\n2015) and WN18RR (Dettmers et al., 2018).\nUnlike the previous shallow KGE methods\nthat cannot be trivially utilized by widely used\nTransformer-based models for language tasks (Pe-\nters et al., 2019), our approach beneﬁts from the\nuniﬁed Transformer architecture and its extensi-\nbility. As a case study, we show how to inte-\ngrate the learned representations of HittER into\npre-trained language models like BERT (Devlin\net al., 2019). Our experiments demonstrate that\nHittER signiﬁcantly improves BERT on two Free-\nbase factoid question answering (QA) datasets:\nFreebaseQA (Jiang et al., 2019) and Webques-\n1GNN methods’ depth is tied to their receptive ﬁelds and\nthus constrained by over-smoothing issues (Liu et al., 2020).\ntionSP (Yih et al., 2016).\nOur experimental code as well as multiple pre-\ntrained models are publicly available.2\n2 HittER\nWe introduce our proposed hierarchical Trans-\nformer model (Figure 2) in this section. In Sec-\ntion 2.1, we provide the background about how\nlink prediction can be done with a simple Trans-\nformer scoring function. We then describe the de-\ntailed architecture of our proposed model in Sec-\ntion 2.2. Finally, we discuss our strategies to learn\nbalanced contextual representations of an entity in\nSection 2.3.\n2.1 Transformers for Link Prediction\nA knowledge graph can be viewed as a set of\ntriplets (G = {(es,rp,eo)}) and each has three\nitems including the subject es ∈ E, the predi-\ncate rp ∈R, and the object eo ∈E to describe\na single fact (link) in the knowledge graph. Our\nmodel approximates a pointwise scoring function\nψ: E×R×E↦→ R which takes a triplet as input\nand produces a score reﬂecting the plausibility of\nsuch fact triplet existing in the knowledge graph.\nIn the task of link prediction, given a triplet with\neither the subject or the object missing, the goal is\nto ﬁnd the missing entity from the set of all entities\nE. Without loss of generality, we describe the case\nwhere an incomplete triplet (es,rp) is given and\nwe want to predict the object eo. And vice versa,\nthe subject es can be predicted in a similar pro-\ncess, except that a reciprocal predicate r˜p will be\nused to distinguish these two cases (Lacroix et al.,\n2018). We call the entity in the incomplete triplet\nthe source entity esrc and call the entity we want to\npredict the target entity etgt .\nLink prediction can be done in a straightforward\nmanner with a Transformer encoder (Vaswani et al.,\n2017) as the scoring function, depicted inside the\ndashed box in Figure 2. Our inputs to the Trans-\nformer encoder are randomly initialized embed-\ndings of the source entity esrc, the predicate rp,\nand a special[CLS] token. Three different learned\ntype embeddings are directly added to the three to-\nken embeddings similar to the input representations\nof BERT (Devlin et al., 2019). Then we use the\noutput embedding corresponding to the [CLS] to-\nken (Mesrc) to predict the target entity, which is\n2https://github.com/sanxing-chen/\nHittER\n10397\n{NE ×\n{NC ×\nEntity TransformerEntity TransformerEntity Transformer\nContext Transformer\nLink Prediction\nMasked Entity Prediction\nE[CLS] Ee1\nEr1E[CLS] Eesrc\nErp E[CLS] Ee2\nEr2\nMesrc Me1\nMe2\nE[MASK] Eerandom\nE[GCLS]\nT[GCLS] Tesrc\nFigure 2: Our model consists of two Transformer blocks organized in a hierarchical fashion. The bottom Trans-\nformer block captures the interactions between a entity-relation pair while the top one gathers information from an\nentity’s graph neighborhood. Taking the entity embeddingsEe and the relation embeddings Er as input, the output\nembedding T[GCLS] is used for predicting the target entity. We sometimes mask or replace Eesrc with E[MASK] or\nEerandom . In which case, an additional output embedding Tesrc can be used to recover the perturbed entity. The\ndashed box indicates a simple context-independent baseline where Mesrc is directly used for link prediction.\nimplemented as follows. We ﬁrst compute the plau-\nsibility score of the true triplet as the dot-product\nbetween Mesrc and the token embedding of the tar-\nget entity. In the same way, we also compute the\nplausibility scores for all other candidate entities\nand normalize them using the softmax function.\nLastly, we use the normalized distribution to get\nthe cross-entropy loss LLP = −log p(etgt |Mesrc)\nfor training. We will use this model as a simple\ncontext-independent baseline later in experiments,\nwhich is similar to the approach explored in Wang\net al. (2019).\nAlthough such simple Transformer encoder does\na decent work in link prediction tasks, learning\nknowledge graph embeddings from one triplet at\na time ignores the abundant structural information\nin the graph context. Our model, as described in\nthe following section, also considers the relational\nneighborhood of the source vertex (entity), which\nincludes all of its adjacent vertices in the graph,\ndenoted as NG(esrc) ={(esrc,ri,ei)}.3\n2.2 Hierarchical Transformers\nWe propose a hierarchical Transformer model for\nknowledge graph embeddings (Figure 2). The pro-\nposed model consists of two blocks of multi-layer\n3Our referred neighborhood is slightly different from the\nformal deﬁnition since we only consider edges connecting to\nthe source vertex.\nbidirectional Transformer encoders.\nWe employ the Transformer described in Sec-\ntion 2.1 as our bottom Transformer block, called the\nentity Transformer, to learn interactions between\nan entity and its associated relation type. Different\nfrom the context-independent scenario described\nin the last section, this entity Transformer is now\ngeneralized to also encode information from a re-\nlational context. In speciﬁc, there are two cases in\nour context-dependent scenario:\n1. We consider the source entity with the predi-\ncate in the incomplete triplet as the ﬁrst entity-\nrelation pair;\n2. We consider an entity from the graph neigh-\nborhood of the source entity with the relation\ntype of the edge that connects them.\nThe bottom block is responsible of packing all use-\nful features from the entity-relation pairs into vec-\ntor representations to be further used by the top\nblock. Compared with directly feeding all entity-\nrelation pairs to the top block, it helps reduce the\nrun-time of the model by converting two inputs to\none.4\nThe top Transformer block is called the context\nTransformer. Given the output of the previous en-\n4This avoids long input sequences for Transformer’s\nO(n2) computation.\n10398\ntity Transformer and a special[GCLS] embedding,\nit contextualizes the source entity with relational in-\nformation from its graph neighborhood. Similarly,\nthree type embeddings are assigned to the special\n[GCLS] token embedding, the intermediate source\nentity embedding, and the other intermediate neigh-\nbor entity embeddings. The cross-entropy loss for\nlink prediction is now changed as follows.\nLLP = −log p(etgt |T[GCLS]) (1)\nThe top block does most of the heavy lifting to\naggregate contextual information together with the\ninformation from the source entity and the predi-\ncate, by using structural features extracted from the\noutput vector representations of the bottom block.\n2.3 Balanced Contextualization\nOur hierarchical Transformer model shows a sim-\nple way to introduce graph context to link predic-\ntion, however, trivially providing contextual infor-\nmation to the model could cause problems. On\none hand, since a source entity often contains high-\nquality information for link prediction and learning\nto extract useful information from a broad noisy\ncontext requires substantial effort, the model could\nsimply learn to ignore the additional contextual\ninformation. On the other hand, the introduction\nof rich contextual information could in turn down-\ngrade information from the source entity and con-\ntain spurious correlations, which potentially lead to\nover-ﬁtting based on our observation. To address\nthese challenges, inspired by the successful Masked\nLanguage Modeling pre-training task in BERT, we\npropose a two-step Masked Entity Prediction task\n(MEP) to balance the utilization of source entity\nand graph context during contextualization process.\nTo avoid the ﬁrst problem, we apply a masking\nstrategy to the source entity of each training exam-\nple as follows. During training, we randomly select\na proportion of training examples in a batch. With\ncertain probabilities, we replace the input source\nentity with a special mask token [MASK], a ran-\ndom chosen entity, or just leave it unchanged. The\npurpose of these perturbations is to introduce ex-\ntra noise to the information from the source entity,\nthus forcing the model to learn contextual repre-\nsentations. The probability of each category is\ndataset-speciﬁc hyper-parameter: for example, we\ncan mask out the source entity more frequently if\nits graph neighborhood is denser (in which case,\nthe source entity can be easily replaced by the ad-\nditional contextual information).\nIn terms of the second problem, we want to pro-\nmote the model’s awareness of the masked entity.\nThus we train the model to recover the perturbed\nsource entity based on the additional contextual in-\nformation. To do this, we use the output embedding\ncorresponding to the source entity Tesrc to predict\nthe correct source entity via a classiﬁcation layer.5\nWe can add the cross-entropy classiﬁcation loss to\nthe previous mentioned link prediction loss as an\nauxiliary loss, as follows.\nLMEP = −log p(esrc |Tesrc) (2)\nL=LLP + LMEP (3)\nThis step is important when solely relying on the\ncontextual clues is insufﬁcient to do link prediction,\nwhich means the information from the source entity\nneeds to be emphasized. And it is otherwise unnec-\nessary when there is high-quality contextual infor-\nmation. However, the ﬁrst step of entity masking\nis always beneﬁcial to the utilization of contextual\ninformation according to our observations. Thus\nwe use dataset-speciﬁc conﬁgurations to strike a\nbalance between these two sides.\nIn addition to the MEP task, we implement a uni-\nform neighborhood sampling strategy where only\na fraction of the entities in the graph neighborhood\nwill appear in a training example. This sampling\nstrategy acts like a data augmenter and similar to\nthe edge dropout regularization in graph neural net-\nwork methods (Rong et al., 2020). We also have\nto remove the ground truth target entity from the\nsource entity’s neighborhood during training. Oth-\nerwise, it will create a dramatic train-test mismatch\nbecause the ground truth target entity can always\nbe found from the source entity’s neighborhood\nduring training while it can rarely be found during\ntesting. The model will thus learn to naively select\nan entity from the neighborhood.\n3 Link Prediction Experiments\nWe describe our link prediction experiments in this\nsection. Section 3.1 introduces two standard bench-\nmark datasets we used. We then describe our eval-\nuation protocol in Section 3.2, and the detailed\nexperimental setup in Section 3.3. Our proposed\nmethod are assessed both quantitatively and qual-\nitatively in Section 3.4. Besides, several ablation\n5We share the same weight matrix in the input embeddings\nlayer and the linear transformation of this classiﬁcation layer.\n10399\nModel\nFB15K-237 WN18RR\n#Params MRR↑ Hits↑ #Params MRR↑ Hits↑\n@1 @3 @10 @1 @3 @10\nRESCAL (Nickel et al., 2011) 6M .356 .266 .390 .535 6M .467 .439 .478 .516\nTransE (Bordes et al., 2013) 2M .310 .218 .345 .495 21M .232 .061 .366 .522\nDistMult (Yang et al., 2015) 4M .342 .249 .378 .531 21M .451 .414 .466 .523\nComplEx (Trouillon et al., 2016) 4M .343 .250 .377 .532 5M .479 .441 .495 .552\nConvE (Dettmers et al., 2018) 9M .338 .247 .372 .521 36M .439 .409 .452 .499\nRotatE (Sun et al., 2018) 15M .338 .241 .375 .533 20M .476 .428 .492 .571\nCoKE (Wang et al., 2019) 10M .364 .272 .400 .549 17M .484 .450 .496 .553\nTuckER (Balazevic et al., 2019) - .358 .266 .394 .544 - .470 .443 .482 .526\nCompGCN (Vashishth et al., 2020) - .355 .264 .390 .535 - .479 .443 .494 .546\nRotH (Chami et al., 2020) 8M .344 .246 .380 .535 21M .496 .449 .514 .586\nHittER 16M .373 .279 .409 .558 24M .503 .462 .516 .584\nTable 1: Comparison between the proposed method and baseline methods. Results of RotatE, CoKE, TuckER,\nCompGCN, and RotH are taken from their original papers. Numbers in bold represent the best results.\nstudies are presented in Section 3.5 to demonstrate\nthe importance of balanced contextualization.\n3.1 Datasets\nWe train and evaluate our proposed method on\ntwo standard benchmark datasets FB15K-\n237 (Toutanova and Chen, 2015) and\nWN18RR (Dettmers et al., 2018) for link\nprediction, following the standard train/test\nsplit.6 FB15K-237 is a subset sampled from\nthe Freebase (Bollacker et al., 2008) with trivial\ninverse links removed. It stored facts about topics\nin movies, actors, awards, etc. WN18RR is a\nsubset of the WordNet (Miller, 1995) which\ncontains structured knowledge of English lexicons.\nStatistics of these two datasets are shown in\nTable 2. Notably, WN18RR is much sparser than\nFB15k-237 which implies it has less structural\ninformation in the local neighborhood of an entity.\nThis will affect our conﬁgurations of the masked\nentity prediction task consequently.\n3.2 Evaluation Protocol\nThe task of link prediction in a knowledge graph\nis deﬁned as an entity ranking task. Essentially,\nfor each test triplet, we remove the subject or the\nobject from it and let the model predict which is the\nmost plausible answer among all possible entities.\nAfter scoring all entity candidates and sorting them\n6We intentionally omit the original FB15K and\nWN18 datasets because of their known ﬂaw in test-\nleakage (Toutanova and Chen, 2015).\nDataset FB15K-237 WN18RR\n#Entities 14,541 40,943\n#Relations 237 11\n#Triples 310,116 93,003\n#Avg. degree 42.7 4.5\nTable 2: Dataset statistics. The WN18RR dataset is\nsigniﬁcantly sparser than the FB15K-237 dataset.\nby the computed scores, the rank of the ground\ntruth target entity is used to further compute vari-\nous ranking metrics such as mean reciprocal rank\n(MRR) and hits@k, k ∈{1,3,10}. We report all\nof these ranking metrics under the ﬁltered setting\nproposed in Bordes et al. (2013) where valid enti-\nties except the ground truth target entity are ﬁltered\nout from the rank list.\n3.3 Experimental Setup\nWe implement our proposed method in Py-\nTorch (Paszke et al., 2019) under the LibKGE\nframework (Broscheit et al., 2020). To perform a\nfair comparison with some early baseline methods,\nwe reproduce their results using hyper-parameter\nconﬁgurations from LibKGE.7 All data and evalua-\ntion metrics can be found in LibKGE.\nOur model consists of a three-layer entity Trans-\nformer and a six-layers context Transformer. Each\n7These conﬁgurations consider many recent training tech-\nniques and are found by extensive searches. Thus the results\nare generally much better then the original reported ones.\n10400\nTransformer layer has eight heads. The dimension\nsize of hidden states is 320 across all layers except\nthat we use 1280 dimensions for the position-wise\nfeed-forward networks inside Transformer layers\nsuggested by Vaswani et al. (2017). We set the\nmaximum numbers of uniformly sampled neigh-\nbor entities for every example in the FB15K-237\nand WN18RR dataset to be 50 and 12 respectively.\nSuch conﬁgurations are intended to ensure most ex-\namples (more than 85% of the cases in each dataset)\ncan have access to its entire local neighborhood\nduring inference. During training, we further ran-\ndomly drop 30% of entities from these ﬁxed-size\nsets in both datasets.\nWe train our models using Adamax (Kingma\nand Ba, 2015) with a learning rate of 0.01 and an\nL2 weight decay rate of 0.01. The learning rate\nlinearly increases from 0 over the ﬁrst ten percent\nof training steps, and linearly decreases through\nthe rest of the steps. We apply dropout (Srivastava\net al., 2014) with a probability p= 0.1 for all lay-\ners, except that p= 0.6 for the embedding layers.\nWe apply label smoothing with a rate0.1 to prevent\nthe model from being over-conﬁdent during train-\ning. We train our models using a batch size of 512\nfor at most 500 epochs and employ early stopping\nbased on MRR in the validation set.\nWhen training our model with the masked en-\ntity prediction task, we use the following dataset-\nspeciﬁc conﬁgurations based on validation MRR\nin few early trials:\n•WN18RR: 50% of examples are subjected to\nthis task. Among them, 60% of examples are\nmasked out, the rest are split in a 3:7 ratio for\nreplaced and unchanged ones.\n•FB15K-237: 50% of examples in a batch are\nmasked out. No replaced or unchanged ones.\nWe do not include the auxiliary loss.\nTraining our full models takes 7 hours\n(WN18RR) and 37 hours (FB15K-237) on a\nNVIDIA Tesla V100 GPU.\n3.4 Experimental Results\nTable 1 shows that the results of HittER compared\nwith baseline methods including some early meth-\nods and previous SOTA methods.8 We outperform\nall previous work by a substantial margin across\n8We do not compare with huge models that employ exces-\nsive embeddings size (Lacroix et al., 2018).\nContextualization FB15K-237 WN18RR\nMRR H@10 MRR H@10\nBalanced 37.5(.1) 56.1(.1) 50.0(.4) 58.2(.4)\nUnbalanced 36.7 (.2) 55.4(.4) 47.5(.1) 55.4(.2)\nNone 37.3 (.1) 56.1(.1) 47.3(.6) 53.8(.7)\nTable 3: Results of models with different contextualiza-\ntion techniques on dev sets. We report average scores\nand standard deviation from ﬁve random runs.\nnearly all the metrics. Comparing to some pre-\nvious methods which target some observed pat-\nterns of speciﬁc datasets, our proposed method is\nmore general and is able to give more consistent\nimprovements over the two standard datasets. For\ninstance, the previous SOTA in WN18RR, RotH\nexplicitly captures the hierarchical and logical pat-\nterns by hyperbolic embeddings. Comparing to it,\nour model performs better especially in the FB15K-\n237 dataset which has a more diverse set of relation\ntypes. On the other hand, our models have compa-\nrable numbers of parameters to baseline methods,\nsince entity embeddings contribute to the majority\nof the parameters.\n3.5 Ablation Studies\nTo show the contributions of adding graph context\nand balanced contextualization, we compare results\nof three different settings (Table 3), i.e., HittER\nwith no context (the context-independent Trans-\nformer described in Section 2.1), contextualized\nHittER without balancing techniques proposed in\nSection 2.3, and our full model. We ﬁnd that di-\nrectly adding in contextual information does not\nbeneﬁt the model (“Unbalanced”), while balanced\ncontextualization generates signiﬁcantly superior\nresults in terms of MRR on both datasets, especially\nfor the WN18RR dataset, which has a sparser and\nnoisier graph structure.\nBreaking down the model’s performance by re-\nlation types in WN18RR, Table 4 shows that in-\ncorporating contextual information brings us sub-\nstantial improvements on two major relation types,\nnamely the hypernym and the member meronym re-\nlations, which both include many examples belong\nto the challenging one-to-many relation categories\ndeﬁned in Bordes et al. (2013).\nInferring the relationship between two entities\ncan be viewed as a process of aggregating infor-\nmation from the graph paths between them (Teru\net al., 2020). To gain further understanding of what\n10401\nRelation Name Count No ctx Full Gain\nhypernym 1174 .144 .181 26%\nderivationally related form 1078 .947 .947 0%\nmember meronym 273 .237 .316 33%\nhas part 154 .200 .235 18%\ninstance hypernym 107 .302 .330 9%\nsynset domain topic of 105 .350 .413 18%\nverb group 43 .930 .931 0%\nalso see 41 .585 .595 2%\nmember of domain region 34 .201 .259 29%\nmember of domain usage 22 .373 .441 18%\nsimilar to 3 1 1 0%\nTable 4: Dev MRR and relative improvement percent-\nage of our proposed method with or without thecontext\nTransformer respect to each relation in the WN18RR\ndataset.\n0\n200\n400\n600\n800\n1000\n#Examples\n1 2 3 4 5 6\nHops\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0MRR\nNo context\nFull\nFigure 3: Dev mean reciprocal rank (MRR) in the\nWN18RR dataset grouped by the number of hops. The\nbar chart shows the number of examples in each group.\nthe role the contextual information play from this\naspect, we group examples in the development set\nof WN18RR by the number of hops (i.e., the short-\nest path length in the undirected training graph)\nbetween the subject and the object in each exam-\nple (Figure 3). From the results, we can see that\nthe MRR metric of each group decreases by the\nnumber of hops of the examples. This matches our\nintuition that aggregating information from longer\ngraph paths is generally harder and such informa-\ntion is more unlikely to be meaningful. Comparing\nmodels with and without the contextual informa-\ntion, the contextual model performs much better in\ngroups of multiple hops ranging from two to four.\nThe improvement also shrinks as the number of\nhops increases.\nHittER Layer\nMafganistan Mkabul MDari\nBERT Layer\nE[CLS]\nWhat is the capital city of Afghanistan? [MASK] \nEof Eafganistan E[MASK]\nHittER LayerBERT Layer\nKabul\nHittER LayerBERT Layer\nMLM Loss\nSelf Attn\nCross Attn\n··· ···\nFigure 4: Combining HittER and BERT for factoid\nQA. Each BERT layer is connected to a layer of Hit-\ntER’s context Transformer via a cross-attention module.\nWe jointly ﬁne-tune the combined model to predict the\nmasked entity name in the input question.\n4 Factoid QA Experiments\nIn addition to HittER’s superior intrinsic evaluation\nresults, in this section, we conduct a case study\non the factoid question answering (QA) task to\ndemonstrate HittER’s potential to enhance popular\npre-trained Transformer-based language models’\nperformance on knowledge-intensive tasks.\nAs a Transformer-based model, HittER enables\nus to integrate its multilayer knowledge representa-\ntion into other Transformer models (BERT in our\ncase) using the multi-head attention mechanism. In\neach BERT layer, after the original self-attention\nmodule we add a cross-attention module where the\nqueries come from the previous BERT layer while\nthe keys and values come from the output of a cor-\nresponding HittER layer (Vaswani et al., 2017), so\nthat HittER’s knowledge information can ﬂow into\nBERT (Figure 4).\nWe perform experiments on two factoid QA\ndatasets: FreebaseQA (Jiang et al., 2019) and We-\nbQuestionSP (Yih et al., 2016), both pertaining\nto facts on Freebase. Each question in the two\ndatasets is labeled with a context entity and an in-\nferred relation between the context entity and the\nanswer entity, which we use for preparing the en-\ntity and relation inputs for HittER. To better exploit\nthe knowledge in BERT, we follow its pretraining\ntask to create a word-based QA setting, where fac-\ntoid questions are converted to cloze questions by\nappending the special [MASK] tokens to the end.\nBoth models are trained to recover these [MASK]\ntokens to the original words.9 We use the BERT-\n9This is different from the entity-based QA setting. To\n10402\nFreebaseQA WebQuestionSP\nFull Filtered Full Filtered\nTrain 20358 3713 3098 850\nTest 3996 755 1639 484\nTable 5: Number of examples in two Freebase question\nanswering datasets.\nModel FreebaseQA WebQuestionSP\nFull Filtered Full Filtered\nBERT 19.8 (.1) 30.8(.1) 23.2(.3) 46.5(.4)\n+HittER 21.2(.2) 37.1(.6) 27.1(.2) 51.0(.7)\nTable 6: QA accuracy of combining HittER and BERT\nin two Freebase-based question answering datasets. We\nreport average scores and standard deviation from ﬁve\nrandom runs.\nbase model (Devlin et al., 2019) and our best per-\nforming HittER model pre-trained on the FB15K-\n237 dataset. Since FB15K-237 only covers a small\nportion of Freebase, most questions in the two QA\ndatasets are not related to the knowledge from the\nFB15K-237 dataset, in which case the input enti-\nties for HittER cannot be provided. Thus we also\nreport results under a ﬁltered setting, i.e., a subset\nretaining only examples whose context entity and\nanswer entity both exist on the FB15K-237 dataset.\nOur experimental results in Table 6 show that\nHittER’s representation signiﬁcantly enhances\nBERT’s question answering ability, especially\nwhen the questions are related to entities in the\nknowledge graph used to train HittER. We include\nmore details of the experiments in the Appendix.\n5 Related Work\nKGE methods have been extensively studied in\nmany diverse directions. Our scope here is limited\nto methods that purely rely on entities and relations,\nwithout access to other external resources.\n5.1 Triple-based Methods\nMost of the previous work focuses on exploiting\nexplicit geometric properties in the embedding\nspace to capture different relations between entities.\nEarly work uses translational distance-based scor-\ning functions deﬁned on top of entity and relation\nsimplify the modeling architecture, we also make the number\nof tokens known to all models.\nembeddings (Bordes et al., 2013; Wang et al., 2014;\nLin et al., 2015; Ji et al., 2015).\nAnother line of work uses tensor factorization\nmethods to match entities semantically. Starting\nfrom simple bi-linear transformations in the eu-\nclidean space (Nickel et al., 2011; Yang et al.,\n2015), numerous complicated transformations in\nvarious spaces have been hence proposed (Trouil-\nlon et al., 2016; Ebisu and Ichise, 2018; Sun et al.,\n2018; Zhang et al., 2019a; Chami et al., 2020; Tang\net al., 2020; Chao et al., 2021). Such methods effec-\ntively capture the intuition from observation of data\nbut suffer from unobserved geometric properties\nand are generally limited in expressiveness.\nIn light of recent advances in deep learning,\nmany more powerful neural network modules such\nas Convolutional Neural Networks (Dettmers et al.,\n2018), Capsule Networks (Nguyen et al., 2019),\nand Transformers (Wang et al., 2019) are also intro-\nduced to capture the interaction between entity and\nrelation embeddings. These methods produce rich\nrepresentations and better performance on predict-\ning missing links in knowledge graphs. However,\nthey are restricted by the amount of information\nthat can be encoded in a single node embedding\nand the great effort to memorize local connectivity\npatterns.\n5.2 Context-aware Methods\nVarious forms of graph contexts have been proven\neffective in recent work on neural networks oper-\nating in graphs under the message passing frame-\nwork (Bruna et al., 2014; Defferrard et al., 2016;\nKipf and Welling, 2017). Schlichtkrull et al. (2018,\nR-GCN) adapt the Graph Convolutional Networks\nto realistic knowledge graphs which are character-\nized by their highly multi-relational nature. Teru\net al. (2020) incorporate an edge attention mecha-\nnism to R-GCN, showing that the relational path\nbetween two entities in a knowledge graph con-\ntains valuable information about their relations\nin an inductive learning setting. Vashishth et al.\n(2020) explore the idea of using existing knowl-\nedge graph embedding methods to improve the\nentity-relation composition in various Graph Con-\nvolutional Network-based methods. Bansal et al.\n(2019) borrow the idea from Graph Attention\nNetworks (Veliˇckovi´c et al., 2018), using a bi-\nlinear attention mechanism to selectively gather\nuseful information from neighbor entities. Differ-\nent from their simple single-layer attention formu-\n10403\nlation, we use the advanced Transformer to cap-\nture both the entity-relation and entity-context in-\nteractions. Nathani et al. (2019) also propose an\nattention-based feature embedding to capture multi-\nhop neighbor information, but unfortunately, their\nreported results have been proven to be unreliable\nin a recent re-evaluation (Sun et al., 2020).\n6 Conclusion and Future Work\nIn this work, we proposed HittER, a novel\nTransformer-based model with effective training\nstrategies for learning knowledge graph embed-\ndings in complex multi-relational graphs. We show\nthat with contextual information from a local neigh-\nborhood, our proposed method outperforms all pre-\nvious approaches in long-standing link prediction\ntasks, achieving new SOTA results on FB15K-237\nand WN18RR. Moreover, we show that the knowl-\nedge representation learned by HittER can be ef-\nfectively utilized by a Transformer-based language\nmodel BERT to answer factoid questions.\nIt is worth mentioning that our proposed bal-\nanced contextualization is also applicable to other\ncontext-aware KGE methods such as GNN-based\napproaches. Future work can also apply HittER to\nother graph representation learning tasks besides\nlink prediction. Currently, our proposed HittER\nmodel performs well while only aggregating con-\ntextual information from a local graph neighbor-\nhood. It would be interesting to extend it with a\nbroader graph context to obtain potential improve-\nments.\nToday, the Transformer has become the de facto\nmodeling architecture in natural language process-\ning. As experimental results on factoid question\nanswering tasks showcasing HittER’s great poten-\ntial to be integrated into common Transformer-\nbased model and generate substantial gains in per-\nformance, we intend to explore training HittER\non large-scale knowledge graphs, so that more\nNLP models would beneﬁt from HittER in vari-\nous knowledge-intensive tasks.\nAcknowledgments\nWe thank Hao Cheng, Hoifung Poon, Xuan Zhang,\nYu Bai, Aidan San, colleagues from Microsoft Bing\nAds team and Microsoft Research, and the anony-\nmous reviewers for their valuable discussions and\ncomments.\nReferences\nIvana Balazevic, Carl Allen, and Timothy Hospedales.\n2019. TuckER: Tensor factorization for knowledge\ngraph completion. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 5185–5194, Hong Kong, China. As-\nsociation for Computational Linguistics.\nTrapit Bansal, Da-Cheng Juan, Sujith Ravi, and An-\ndrew McCallum. 2019. A2N: Attending to neigh-\nbors for knowledge graph inference. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 4387–4392, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nLisa Bauer, Yicheng Wang, and Mohit Bansal. 2018.\nCommonsense for generative multi-hop question an-\nswering tasks. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 4220–4230, Brussels, Belgium.\nAssociation for Computational Linguistics.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collab-\noratively created graph database for structuring hu-\nman knowledge. In Proceedings of the 2008 ACM\nSIGMOD international conference on Management\nof data, pages 1247–1250.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in neural information\nprocessing systems, pages 2787–2795.\nSamuel Broscheit, Daniel Rufﬁnelli, Adrian Kochsiek,\nPatrick Betz, and Rainer Gemulla. 2020. LibKGE\n- a knowledge graph embedding library for repro-\nducible research. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing: System Demonstrations , pages 165–\n174, Online. Association for Computational Linguis-\ntics.\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and\nYann LeCun. 2014. Spectral networks and locally\nconnected networks on graphs. In International\nConference on Learning Representations.\nInes Chami, Adva Wolf, Da-Cheng Juan, Frederic\nSala, Sujith Ravi, and Christopher Ré. 2020. Low-\ndimensional hyperbolic knowledge graph embed-\ndings. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6901–6914, Online. Association for Computa-\ntional Linguistics.\nLinlin Chao, Jianshan He, Taifeng Wang, and Wei\nChu. 2021. PairRE: Knowledge graph embeddings\nvia paired relation vectors. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\n10404\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 4360–4369, Online. As-\nsociation for Computational Linguistics.\nMichaël Defferrard, Xavier Bresson, and Pierre Van-\ndergheynst. 2016. Convolutional neural networks\non graphs with fast localized spectral ﬁltering. In\nAdvances in neural information processing systems ,\npages 3844–3852.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2018. Convolutional 2d\nknowledge graph embeddings. In Proceedings of\nthe Thirty-Second AAAI Conference on Artiﬁcial In-\ntelligence.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTakuma Ebisu and Ryutaro Ichise. 2018. Toruse:\nKnowledge graph embedding on a lie group. In Pro-\nceedings of the Thirty-Second AAAI Conference on\nArtiﬁcial Intelligence.\nHiroaki Hayashi, Zecong Hu, Chenyan Xiong, and Gra-\nham Neubig. 2020. Latent relation language mod-\nels. In Proceedings of the Thirty-Fourth AAAI Con-\nference on Artiﬁcial Intelligence, pages 7911–7918.\nGuoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and\nJun Zhao. 2015. Knowledge graph embedding via\ndynamic mapping matrix. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers) , pages 687–696, Beijing,\nChina. Association for Computational Linguistics.\nKelvin Jiang, Dekun Wu, and Hui Jiang. 2019. Free-\nbaseQA: A new factoid QA data set matching trivia-\nstyle question-answer pairs with Freebase. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers) , pages 318–323,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nThomas N Kipf and Max Welling. 2017. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. In International Conference on Learning\nRepresentations.\nTimothee Lacroix, Nicolas Usunier, and Guillaume\nObozinski. 2018. Canonical tensor decomposi-\ntion for knowledge base completion. In Inter-\nnational Conference on Machine Learning , pages\n2863–2872.\nYankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and\nXuan Zhu. 2015. Learning entity and relation em-\nbeddings for knowledge graph completion. In Pro-\nceedings of the Twenty-Ninth AAAI conference on ar-\ntiﬁcial intelligence.\nMeng Liu, Hongyang Gao, and Shuiwang Ji. 2020. To-\nwards deeper graph neural networks. In Proceed-\nings of the 26th ACM SIGKDD International Confer-\nence on Knowledge Discovery &amp; Data Mining,\npages 338–348.\nRobert Logan, Nelson F. Liu, Matthew E. Peters, Matt\nGardner, and Sameer Singh. 2019. Barack’s wife\nhillary: Using knowledge graphs for fact-aware lan-\nguage modeling. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5962–5971, Florence, Italy. Associa-\ntion for Computational Linguistics.\nChristopher D Manning, Kevin Clark, John Hewitt, Ur-\nvashi Khandelwal, and Omer Levy. 2020. Emer-\ngent linguistic structure in artiﬁcial neural networks\ntrained by self-supervision. Proceedings of the Na-\ntional Academy of Sciences.\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. Communications of the ACM , 38(11):39–\n41.\nDeepak Nathani, Jatin Chauhan, Charu Sharma, and\nManohar Kaul. 2019. Learning attention-based\nembeddings for relation prediction in knowledge\ngraphs. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 4710–4723, Florence, Italy. Association\nfor Computational Linguistics.\nDai Quoc Nguyen, Thanh Vu, Tu Dinh Nguyen,\nDat Quoc Nguyen, and Dinh Phung. 2019. A cap-\nsule network-based embedding model for knowl-\nedge graph completion and search personalization.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) , pages\n2180–2189, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nMaximilian Nickel, V olker Tresp, and Hans-Peter\nKriegel. 2011. A three-way model for collective\nlearning on multi-relational data. In Icml, vol-\nume 11, pages 809–816.\n10405\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learn-\ning library. In Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 43–54, Hong Kong, China. Associ-\nation for Computational Linguistics.\nSebastian Riedel, Limin Yao, Andrew McCallum, and\nBenjamin M. Marlin. 2013. Relation extraction with\nmatrix factorization and universal schemas. In Pro-\nceedings of the 2013 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n74–84, Atlanta, Georgia. Association for Computa-\ntional Linguistics.\nYu Rong, Wenbing Huang, Tingyang Xu, and Junzhou\nHuang. 2020. Dropedge: Towards deep graph con-\nvolutional networks on node classiﬁcation. In Inter-\nnational Conference on Learning Representations.\nDaniel Rufﬁnelli, Samuel Broscheit, and Rainer\nGemulla. 2020. You can teach an old dog new\ntricks! on training knowledge graph embeddings.\nIn International Conference on Learning Represen-\ntations.\nApoorv Saxena, Aditay Tripathi, and Partha Taluk-\ndar. 2020. Improving multi-hop question answering\nover knowledge graphs using knowledge base em-\nbeddings. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4498–4507, Online. Association for Computa-\ntional Linguistics.\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem,\nRianne Van Den Berg, Ivan Titov, and Max Welling.\n2018. Modeling relational data with graph convolu-\ntional networks. In European Semantic Web Confer-\nence, pages 593–607. Springer.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2018. Rotate: Knowledge graph embedding\nby relational rotation in complex space. In Interna-\ntional Conference on Learning Representations.\nZhiqing Sun, Shikhar Vashishth, Soumya Sanyal,\nPartha Talukdar, and Yiming Yang. 2020. A re-\nevaluation of knowledge graph completion methods.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n5516–5522, Online. Association for Computational\nLinguistics.\nYun Tang, Jing Huang, Guangtao Wang, Xiaodong He,\nand Bowen Zhou. 2020. Orthogonal relation trans-\nforms with graph context modeling for knowledge\ngraph embedding. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 2713–2722, Online. Association\nfor Computational Linguistics.\nKomal K Teru, Etienne Denis, and William L Hamil-\nton. 2020. Inductive relation prediction by subgraph\nreasoning. In Proceedings of the 37th International\nConference on Machine Learning.\nKristina Toutanova and Danqi Chen. 2015. Observed\nversus latent features for knowledge base and text\ninference. In Proceedings of the 3rd Workshop on\nContinuous Vector Space Models and their Composi-\ntionality, pages 57–66, Beijing, China. Association\nfor Computational Linguistics.\nThéo Trouillon, Johannes Welbl, Sebastian Riedel, Éric\nGaussier, and Guillaume Bouchard. 2016. Complex\nembeddings for simple link prediction. International\nConference on Machine Learning (ICML).\nShikhar Vashishth, Soumya Sanyal, Vikram Nitin, and\nPartha Talukdar. 2020. Composition-based multi-\nrelational graph convolutional networks. In Interna-\ntional Conference on Learning Representations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Liò, and Yoshua Bengio.\n2018. Graph attention networks. In International\nConference on Learning Representations.\nPat Verga, Haitian Sun, Livio Baldini Soares, and\nWilliam Cohen. 2021. Adaptable and interpretable\nneural MemoryOver symbolic knowledge. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n3678–3691, Online. Association for Computational\nLinguistics.\nHaoyu Wang, Vivek Kulkarni, and William Yang Wang.\n2020. Dolores: Deep contextualized knowledge\ngraph embeddings. In Automated Knowledge Base\nConstruction.\nHongwei Wang, Hongyu Ren, and Jure Leskovec.\n2021. Relational message passing for knowledge\n10406\ngraph completion. In Proceedings of the 27th\nACM SIGKDD Conference on Knowledge Discovery\n&amp; Data Mining , KDD ’21, page 1697–1707,\nNew York, NY , USA. Association for Computing\nMachinery.\nQuan Wang, Pingping Huang, Haifeng Wang, Song-\ntai Dai, Wenbin Jiang, Jing Liu, Yajuan Lyu, Yong\nZhu, and Hua Wu. 2019. Coke: Contextual-\nized knowledge graph embedding. arXiv preprint\narXiv:1911.02168.\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\nChen. 2014. Knowledge graph embedding by trans-\nlating on hyperplanes. In Proceedings of the Twenty-\nEighth AAAI Conference on Artiﬁcial Intelligence ,\npages 1112–1119.\nRobert West, Evgeniy Gabrilovich, Kevin Murphy,\nShaohua Sun, Rahul Gupta, and Dekang Lin. 2014.\nKnowledge base completion via search-based ques-\ntion answering. In Proceedings of the 23rd inter-\nnational conference on World wide web, pages 515–\n526.\nWenhan Xiong, Thien Hoang, and William Yang Wang.\n2017. DeepPath: A reinforcement learning method\nfor knowledge graph reasoning. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 564–573, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\nGao, and Li Deng. 2015. Embedding entities and\nrelations for learning and inference in knowledge\nbases. In International Conference on Learning Rep-\nresentations.\nWen-tau Yih, Matthew Richardson, Chris Meek, Ming-\nWei Chang, and Jina Suh. 2016. The value of se-\nmantic parse labeling for knowledge base question\nanswering. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers) , pages 201–206, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nShuai Zhang, Yi Tay, Lina Yao, and Qi Liu. 2019a.\nQuaternion knowledge graph embeddings. In Ad-\nvances in Neural Information Processing Systems ,\npages 2735–2745.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019b. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441–1451, Florence, Italy. Association\nfor Computational Linguistics.\nA Embedding Clustering\nTable 7 lists the entity clustering results of ﬁrst\nfew entities in each dataset, based on our learned\nentity representations. Clusters in FB15K-237\nusually are entities of the same type, such as\nSouth/Central American countries, government sys-\ntems, and American voice actresses. While clusters\nin WN18RR are generally looser but still relevant\nto the topic of the central word.\nB Factoid QA Experiment Details\nIn order to connect our HittER model with BERT,\nwe add a cross-attention module after the self-\nattention module in each BERT layer. Follow-\ning the encoder-decoder attention mechanism in\nVaswani et al. (2017), we use queries from previous\nBERT layer and keys and values from the output\nof a corresponding HittER layer. The pre-trained\nBERT (base) and HittER models we use have two\ndifferences in terms of hyper-parameter settings,\ni.e., the number of layers and dimentionality. Since\nBERT has 12 layers while HittER only has 6 layers,\nwe connect every two BERT layers to one HittER\nlayer and skip the ﬁrst two layers in BERT.10 Be-\nfore attention computation, we increase the dimen-\ntionality of HittER’s output representations to the\nnumber of BERT’s using linear transformations.\nThe dimensionality and number of cross-attention\nheads are set as the same conﬁguration of the BERT\nbase model we use.\nWe ﬁnetune all of our question answering (QA)\nmodels using a batch size of 16 for 20 epochs. We\nuse the Adam optimizer (Kingma and Ba, 2015)\nwith a learning rate of 5e−6 for all pretrained\nweights and a learning rate of 5e−5 for newly\nadded cross-attention modules. The learning rate\nlinearly increases from 0 over the ﬁrst 10% training\nsteps.\nC Discussion\nC.1 Right Context for Link Prediction\nStructural information of knowledge graphs can\ncome from multiple forms, such as graph paths,\nsub-graphs, and the local neighborhood that we\nused in this work. In addition, these context forms\ncan be represented in terms of the relation type, the\nentity, or both of them.\n10Among various connection strategies, this strategy gives\nus the best results in pilot experiments, which also suggests\nthat HittER stores different types of information in its multi-\nlayer representations.\n10407\nEntity Top 5 Neighbors\nDominican Republic Costa Rica, Ecuador, Puerto Rico, Colombia, El Salvador\nRepublic Presidential system, Unitary state, Democracy, Parliamentary system, Constitu-\ntional monarchy\nMMPR Power Rangers, Sonic X, Ben 10, Star Trek: Enterprise, Code Geass\nWendee Lee Liam O’Brien, Michelle Ruff, Hilary Haag, Chris Patton, Kari Wahlgren\nDrama Thriller, Romance Film, Mystery, Adventure Film, LGBT\nLand reform Pronunciamento, Premium, Protest march, Reform, Birth-control reformer\nReform Reform, Land reform, Optimization, Self-reformation, Enrichment\nCover Surface, Spread over, Bind, Supply, Strengthen\nCovering Sheet, Consumer goods, Flap, Floor covering, Coating\nPhytology Paleobiology, Zoology, Kingdom fungi, Plant life, Paleozoology\nTable 7: Nearest neighbors of ﬁrst ﬁve entities in FB15K-237 and WN18RR based on the cosine similarity between\nlearned entity embeddings from our proposed method.\nIn this work, we show that a simple local neigh-\nborhood is sufﬁcient to greatly improve a link pre-\ndiction model. In early experiments in the FB15K-\n237 dataset, we actually observe that masking\nout the source entity all the time does not harm\nthe model performance much. This implies that\nthe contextual information in a dense knowledge\ngraph like FB15K-237 is rich enough to replace the\nsource entity in the link prediction task.\nRecently, Wang et al. (2021) argue that graph\npaths and local neighborhood should be jointly con-\nsidered when only the relation types is used (throw-\ning out entities). Although some recent work has\nmade a ﬁrst step towards utilizing graph paths for\nknowledge graph embeddings (Wang et al., 2019,\n2020), there is still no clear evidence of its effec-\ntiveness.\nC.2 Limitations of the1vsAll Scoring\nRecall that HittER learns a representation for an in-\ncomplete triplet (es,rp) and then computes the dot-\nproduct between it and all the candidate target en-\ntity embeddings. This two-way scoring paradigm,\nwhich is often termed 1vsAll scoring, supports fast\ntraining and inference when the interactions be-\ntween the source entity and the predicate are cap-\ntured by some computation-intensive operations\n(i.e., the computations of Transformers in our case),\nbut unfortunately loses three-way interactions. We\nintentionally choose 1vsAll scoring for two rea-\nsons. On one hand, 1vsAll together with cross-\nentropy training has shown a consistent improve-\nment over other alternative training conﬁgurations\nempirically (Rufﬁnelli et al., 2020). On the other\nhand, it ensures a reasonable speed for the infer-\nence stage where the 1vsAll scoring is necessary.\nAdmittedly, early interactions between the\nsource entity and the target entity can provide valu-\nable information to inform the representation learn-\ning of the incomplete triplet (es,rp). For instance,\nwe ﬁnd that a simple bilinear formulation of the\nsource entity embeddings and the target entity em-\nbeddings can be trained to reﬂect the distance (mea-\nsured by the number of hops) between the source\nentity and the target entity in the graph. We leave\nthe question of how to effectively and efﬁciently\nincorporate such early fusion for future work.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.815038800239563
    },
    {
      "name": "Transformer",
      "score": 0.7128129005432129
    },
    {
      "name": "Relational database",
      "score": 0.6646946668624878
    },
    {
      "name": "Contextualization",
      "score": 0.6631550788879395
    },
    {
      "name": "Statistical relational learning",
      "score": 0.5843935012817383
    },
    {
      "name": "Graph",
      "score": 0.4805341362953186
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4466976523399353
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4263223111629486
    },
    {
      "name": "Natural language processing",
      "score": 0.36774158477783203
    },
    {
      "name": "Data mining",
      "score": 0.346911758184433
    },
    {
      "name": "Programming language",
      "score": 0.1300085186958313
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I51556381",
      "name": "University of Virginia",
      "country": "US"
    }
  ],
  "cited_by": 96
}