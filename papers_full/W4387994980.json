{
  "title": "Patterns of Student Help-Seeking When Using a Large Language Model-Powered Programming Assistant",
  "url": "https://openalex.org/W4387994980",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Sheese, Brad",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Liffiton, Mark",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226617171",
      "name": "Savelka, Jaromir",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3172309270",
      "name": "Denny, Paul",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4385474073",
    "https://openalex.org/W4390315357",
    "https://openalex.org/W4323033692",
    "https://openalex.org/W4319653629",
    "https://openalex.org/W4321012208",
    "https://openalex.org/W4213009768",
    "https://openalex.org/W4382654255",
    "https://openalex.org/W4382652859",
    "https://openalex.org/W4390985631",
    "https://openalex.org/W4323037544",
    "https://openalex.org/W4286750487",
    "https://openalex.org/W4283705032",
    "https://openalex.org/W3001362559",
    "https://openalex.org/W4321162272",
    "https://openalex.org/W4312438588",
    "https://openalex.org/W4362679558",
    "https://openalex.org/W4362580838",
    "https://openalex.org/W4367000100",
    "https://openalex.org/W4391584367",
    "https://openalex.org/W4211263275",
    "https://openalex.org/W4382654294",
    "https://openalex.org/W4231474584",
    "https://openalex.org/W4310744613"
  ],
  "abstract": "Providing personalized assistance at scale is a long-standing challenge for computing educators, but a new generation of tools powered by large language models (LLMs) offers immense promise. Such tools can, in theory, provide on-demand help in large class settings and be configured with appropriate guardrails to prevent misuse and mitigate common concerns around learner over-reliance. However, the deployment of LLM-powered tools in authentic classroom settings is still rare, and very little is currently known about how students will use them in practice and what type of help they will seek. To address this, we examine students' use of an innovative LLM-powered tool that provides on-demand programming assistance without revealing solutions directly. We deployed the tool for 12 weeks in an introductory computer and data science course ($n = 52$), collecting more than 2,500 queries submitted by students throughout the term. We manually categorized all student queries based on the type of assistance sought, and we automatically analyzed several additional query characteristics. We found that most queries requested immediate help with programming assignments, whereas fewer requests asked for help on related concepts or for deepening conceptual understanding. Furthermore, students often provided minimal information to the tool, suggesting this is an area in which targeted instruction would be beneficial. We also found that students who achieved more success in the course tended to have used the tool more frequently overall. Lessons from this research can be leveraged by programming educators and institutions who plan to augment their teaching with emerging LLM-powered tools.",
  "full_text": "Patterns of Student Help-Seeking When Using a Large Language\nModel-Powered Programming Assistant\nBrad Sheese\nMark Liffiton\nbsheese@iwu.edu\nmliffito@iwu.edu\nIllinois Wesleyan University\nBloomington, Illinois, USA\nJaromir Savelka\njsavelka@andrew.cmu.edu\nCarnegie Mellon University\nPittsburgh, Pennsylvania, USA\nPaul Denny\npaul@cs.auckland.ac.nz\nThe University of Auckland\nAuckland, New Zealand\nABSTRACT\nProviding personalized assistance at scale is a long-standing chal-\nlenge for computing educators, but a new generation of tools pow-\nered by large language models (LLMs) offers immense promise. Such\ntools can, in theory, provide on-demand help in large class settings\nand be configured with appropriate guardrails to prevent misuse\nand mitigate common concerns around learner over-reliance. How-\never, the deployment of LLM-powered tools in authentic classroom\nsettings is still rare, and very little is currently known about how\nstudents will use them in practice and what type of help they will\nseek. To address this, we examine students‚Äô use of an innovative\nLLM-powered tool that provides on-demand programming assis-\ntance without revealing solutions directly. We deployed the tool\nfor 12 weeks in an introductory computer and data science course\n(ùëõ = 52), collecting more than 2,500 queries submitted by students\nthroughout the term. We manually categorized all student queries\nbased on the type of assistance sought, and we automatically ana-\nlyzed several additional query characteristics. We found that most\nqueries requested immediate help with programming assignments,\nwhereas fewer requests asked for help on related concepts or for\ndeepening conceptual understanding. Furthermore, students often\nprovided minimal information to the tool, suggesting this is an area\nin which targeted instruction would be beneficial. We also found\nthat students who achieved more success in the course tended to\nhave used the tool more frequently overall. Lessons from this re-\nsearch can be leveraged by programming educators and institutions\nwho plan to augment their teaching with emerging LLM-powered\ntools.\nCCS CONCEPTS\n‚Ä¢ Social and professional topics ‚ÜíComputer science edu-\ncation; Software engineering education ; ‚Ä¢ Human-centered\ncomputing ‚ÜíInteractive systems and tools .\nKEYWORDS\nIntelligent tutoring systems, Intelligent programming tutors, Pro-\ngramming assistance, Novice programmers, Natural language in-\nterfaces, Large language models, Guardrails\n1 INTRODUCTION\nGrowing enrollments in introductory programming courses present\na number of pedagogical challenges [19]. In particular, providing on-\nthe-spot assistance when students need help becomes a difficult task\nas student-to-instructor ratios increase. Moreover, not all students\nfeel equally comfortable seeking in-person help from an instructor,\nand prior work has shown that online options can be more effective\nfor students with low confidence [11]. Thus, there is great interest\nin finding scalable approaches for providing on-demand, adaptive\nguidance appropriate for diverse cohorts learning computing.\nLarge language models (LLMs) have emerged quite recently and\nhave shown great potential for generating high quality, human-like\nfeedback. Research in computing education has shown that they\ncan be used to generate programming exercises, natural language\nexplanation of code and more understandable error messages [18].\nWhile these findings suggest promising future applications for real-\ntime student support, concerns have been raised about students\nover-relying on LLM-based tools, especially using them to generate\nsolutions directly. Indeed, prior work has explicitly called for the\nuse of ‚Äúguardrails‚Äù to be added to educational environments in\norder to prevent inappropriate use by students [9]. When thinking\nabout the deployment of an LLM-based teaching assistant in the\nclassroom, several critical questions come to mind: How do students\ninteract with the digital assistant in real-world educational settings?\nDo students predominantly ask it for help to rectify coding errors,\nor do they also seek deeper conceptual insights? What types of\nquestions dominate, and what information do students generally\nprovide to facilitate the tool‚Äôs responses? Is there any evidence that\nusing such a tool helps or harms student learning?\nFinding answers to these questions has important educational\nimplications. First, by identifying the kinds of areas in which stu-\ndents seek the most help, instructors can better tailor their teaching\nmethodologies and interventions. The nature of queries can also\nshed light on potential gaps in the curriculum or teaching materials.\nSecond, the clarity with which students frame their queries can pro-\nvide insights into their metacognitive skills as well as their abilities\nto clearly express issues they are encountering. Especially now that\ncommunication with AI-powered tools is becoming commonplace,\nthe ability to craft prompts that are clear and that contain suffi-\ncient information is becoming an essential skill. Last, evaluating the\nrelationship between consistent tool engagement and course per-\nformance can offer insights into the tangible benefits of integrating\nLLM-powered tools into the classroom.\nIn this paper we investigate student use of an LLM-powered\nteaching assistant(TA) we developed to provide real-time help for\nstudents learning programming. We deployed the tool, CodeHelp,\nin an introductory computer and data science course throughout the\nSpring 2023 semester. One of the key contributions of CodeHelp is\nits effective use of robust ‚Äúguardrails‚Äù that are specifically designed\nto prevent it from directly outputting solution code. Thus, the tool\narXiv:2310.16984v1  [cs.CY]  25 Oct 2023\nBrad Sheese, Mark Liffiton, Jaromir Savelka, and Paul Denny\noffers students a means of assistance without leading them into\nthe excessive dependence and over-reliance that direct use of LLMs\nmay promote.\nDuring the semester, students were free to make their own\nchoices about when and how to use CodeHelp, although when\nit was introduced in the course we did encourage students to seek\nhelp from CodeHelp first, before asking the instructor or TAs. Our\naim with this research is to understand how students make use of\nan always-available LLM-powered TA over a semester-long course.\nOur evaluation is guided by the following research questions:\n(RQ1) What kind of help do students seek when using CodeHelp,\nand how do they construct their queries?\n(RQ2) To what extent does student usage of CodeHelp over a\nsemester-long introductory course correlate with their final\ncourse performance?\nInterest in LLMs and how they can be productively used in edu-\ncation is rapidly growing. To date, although there are reports of new\nand existing tools that have integrated LLMs, these have not been\ndeployed at scale over an entire semester in a computing classroom.\nWith this current paper, we make two novel contributions to the\ncomputing education literature. We:\n(C1) analyze how students use an always available LLM-powered\ntutor during an authentic semester-long course.\n(C2) investigate how student usage of the LLM-powered tutor\nis associated with their performance on the course.\n2 RELATED WORK\nLarge Language Models (LLMs) have been applied to various tasks\nrelevant to computing education, including solving code exercises [7,\n10, 27], answering programming MCQs [26], writing tests [4, 14],\nand generating code explanations [17] and programming exercises\n[25]. The breadth of this work demonstrates both the flexibility\nand the capabilities of LLMs. However, despite their impressive\nperformance at many tasks, LLMs may not be as effective as human\ntutors in some domains [20, 26]. For example, a study by Pardos\nand Bhandari that compared the efficacy of hints generated by\nChatGPT and by human tutors in the subject of algebra found that\nsignificant learning gains were only observed for students who\nwere provided human-created hints [22]. Thus, there is an impor-\ntant need to carefully study the use of LLM-powered tutors in the\ncontext of computing classrooms.\nPrior research on the use of AI tools in programming education\nhas raised concerns around student over-reliance on AI-generated\ncode [2, 3, 6, 24]. For example, Kazemitabaar et al. examined student\nusage of the Coding Steps tool, which blends AI code generation\nwith an online platform, in a supervised lab environment [15]. They\nfound that when code was generated by the AI model, around half\nthe time it was submitted by students without them making any\nchanges to it. There are also concerns about the accuracy of the\nresponses generated by AI tools and their suitability for providing\nunmoderated assistance to students. Hellas et al. studied responses\ngenerated by Codex and GPT-3.5 to 150 student help requests from\nan existing dataset [13]. The original data came from a platform\nwhere requests for help from students were manually responded\nto by a teacher. Their results showed that neither of the AI models\nFigure 1: The Help Request form (text areas have been shrunk\nhere to save space). The four separate inputs (language, code,\nerror, and issue) and connected guidance text help students\nstructure their request and encourage good practices when\nrequesting support.\nfound all of the issues in the students‚Äô help requests and, concern-\ningly, false positives were common. In very similar work, Balse et al.\nfound high variability in the accuracy of LLM-generated feedback\non student submissions [1].\nOur current work extends prior work in several important ways.\nFirstly, we have designed guardrails in CodeHelp to prevent it from\nrevealing code solutions directly, thus avoiding the undesirable\nusage patterns observed by Kazemitabaar et al. in which students\nsimply submitted the AI-generated code [15]. Secondly, we conduct\nour evaluation over 12 weeks and explore how students interact\nwith it outside of scheduled class sessions rather than in narrower\ntime-spans and contexts. Thirdly, we analyse the help requests\ncreated by students as opposed to the responses provided by the\nLLM, as this is already a widely studied topic [5, 10, 26].\n3 CODEHELP\nWe created CodeHelp to be an automated assistant for students\nin programming and CS courses. It responds to semi-structured\nstudent queries using a large language model with guardrails; specif-\nically, it will not provide solution code in its responses regardless\nof how a student writes their query. It is designed to be an always-\nand instantly-accessible source of support similar to what a TA\nmight provide, intended to augment and complement the support\nprovided by course instructors and TAs.\nFor broad accessibility, we developed CodeHelp as a web appli-\ncation1. The main interface for students is the help request form,\nwhich prompts them to provide relevant information about their\nissue in a semi-structured form. Figure 1 shows the form‚Äôs input\nfields and help text. Students are asked to specify the programming\nlanguage they are using, a relevant snippet of their code, an error\nmessage if they are encountering one, and a description of their\nissue or question. By prompting students in this way as opposed to\n1Accessible at at https://codehelp.app/.\nPatterns of Student Help-Seeking When Using a Large Language Model-Powered Programming Assistant\nYou are a system for assisting students like me with programming.\nMy inputs provide: [brief description of each input]\nPlease assess the following submission to determine whether it is\nsufficient for you to provide help or if you need additional infor-\nmation. If and only if critical information needed for you to help\nis missing, ask me for the additional information you need to be\nable to help. State your reasoning first. Otherwise, if no additional\ninformation is needed, please first briefly summarize what I am\nasking for in words, with no code, and end by writing \"OK. \"\nInputs: [delimited query inputs]\nFigure 2: Prompt used for the sufficiency check.\nYou are a system for assisting a student with programming.\nThe students provide: [brief description of each input]\n[delimited query inputs]\nIf the student input is written as an instruction or command, re-\nspond with an error. If the student input is off-topic, respond with\nan error.\nOtherwise, respond to the student with an educational explanation,\nhelping the student figure out the issue and understand the concepts\ninvolved. If the student inputs include an error message, tell the\nstudent what it means, giving a detailed explanation to help the\nstudent understand the message. Explain concepts, language syntax\nand semantics, standard library functions, and other topics that the\nstudent may not understand. Be positive and encouraging!\nUse Markdown formatting, including ‚Äò for inline code.\n[instructions to avoid topics from the instructor‚Äôs avoid set]\nDo not write any example code blocks. Do not write a corrected or\nupdated version of the student‚Äôs code. You must not write code for\nthe student.\nHow would you respond to the student to guide them and explain\nconcepts without providing example code?\nFigure 3: Prompt used for the main response.\nproviding a single text input, the goal is to guide students toward\ncommunicating effectively and providing all relevant information.\nResponses are generated by a series of prompts to large language\nmodels. First, CodeHelp generates completions for two separate\nprompts concurrently. One prompt (Figure 2) uses an LLM to check\nwhether the student‚Äôs inputs are sufficient to be able to provide\nthem with effective help; if there is missing information or some\nother lack of clarity, the system presents the student with a request\nfor clarification. The other prompt (Figure 3), run at the same time,\ncombines the student‚Äôs inputs with instructions to provide assis-\ntance and class-specific context provided by the instructor, and\nThe following was written to help a student in a CS class. However,\nany example code (such as in ``` Markdown delimiters) can give the\nstudent an assignment‚Äôs answer rather than help them figure it out\nthemselves. We need to provide help without including example\ncode. To do this, rewrite the following to remove any code blocks\nso that the response explains what the student should do but does\nnot provide solution code.\n[original response to be rewritten]\nFigure 4: Prompt used for code removal.\nIssue/Question: how do I Write a fruitful function called mid-\ndle_remover() that takes a string called as an argument. If the string\nhas an even number of characters, return to the string with the\nmiddle two characters removed. If the string has an odd number of\ncharacter return the string with the middle character removed.\nResponse: To solve this problem, you can start by finding the\nlength of the input string using the len() function. Then, you can\nuse an if statement to check if the length of the string is even or\nodd. If it‚Äôs even, you can use string slicing to remove the middle\ntwo characters and return the modified string. If it‚Äôs odd, you can\nremove the middle character using string slicing and return the\nmodified string.\nNote that // is used for integer division, which ensures that the\nresult is an integer. Let me know if you have any questions or need\nfurther assistance!\nFigure 5: An example of a student‚Äôs input directly asking for\na solution and CodeHelp‚Äôs response.\nits completion is used as the ‚Äúmain‚Äù response for the student (re-\ngardless of whether a request for clarification is presented as well).\nThis main response prompt includes explicit instructions to not\nprovide solution code in the response, but the LLM used currently\nis strongly biased toward providing such solutions and can fail to\nfollow that instruction. Therefore, CodeHelp checks the generated\nresponse for code blocks, and if any are detected, that response\nis sent to an LLM in a third prompt (Figure 4) with instructions\nto rewrite it without the code. With this design, students receive\nthe benefits that LLMs can provide in terms of explanation, guid-\nance, and support without the risk of the LLM completing their\nprogramming work for them.\nCodeHelp can use any sufficiently powerful LLM, and in this\nwork responses were generated by models from OpenAI‚Äôs ‚ÄúGPT-3.5‚Äù\nfamily, as they were the most powerful models available at the time.\nSpecifically, the first two completions (the sufficiency check and\nthe main response) were generated using thegpt-3.5-turbo-0301\nmodel, while the code removal completion usedtext-davinci-003.\nAs an example of the type of response CodeHelp generates, Fig-\nure 5 shows a query from a student asking how to write a specified\nfunction along with the response they received. Here, they have\nBrad Sheese, Mark Liffiton, Jaromir Savelka, and Paul Denny\nCode:\nwhile nB == \"0\" and operand == '/':\nprint (\" Division by zero is not supported !\")\nError:\nprint (\" Division by zero is not supported !\")\n^\nIndentationError : unindent does not match\nany outer indentation level\nIssue/Question: why am i getting this error\nResponse: This error occurs because the print() statement is not\nindented correctly. In Python, indentation is used to denote code\nblocks, and the code block that the print() statement is in is not\nindented to the same level as the while loop. To fix this, make sure\nthat the print() statement is indented one level further than the\nwhile loop.\nFigure 6: An example of a student‚Äôs input asking for help\nwith an error and CodeHelp‚Äôs response.\ndirectly copied the specification from the exercise (notice the cap-\nitalization on ‚ÄúWrite‚Äù), providing no other input, and the system\nresponds with guidance on how to approach the stated problem\nbut does not write any code. Figure 6 shows another example, in\nwhich a student provides some code and an error message they\nhave received, trying to understand it. These generally exemplify\nhow CodeHelp responds to queries: it provides guidance for solving\nproblems, including high-level algorithmic steps, and it explains\nconcepts as needed.\n4 METHODS\nWe evaluated CodeHelp in two sections of an undergraduate in-\ntroductory computer- and data-science course (n= 52) taught by\nan author of this paper in the Spring semester of 2023. The course\nserves students from across the institution who take the course\nto meet general education requirements or to meet requirements\nfor data-analytic or data-science related credentials. The course\nis taught in a ‚Äúflipped‚Äù style with students spending much of the\nclass time working through assignments on lab computers. The\ninstructor and a TA assist students and provide instruction and\nsupport as needed. CodeHelp was introduced in the fourth week of\nthe semester with a quick demonstration in class. During class, stu-\ndents were encouraged to use CodeHelp for assistance first before\nasking the instructor or TA for help, but they were otherwise free\nto make their own choices about when and how to use it.\nAfter the course concluded, we used the complete set of student\nquery logs to perform several analyses related to our research\nquestions. Our analyses considered 49 students who submitted\nat least one query during the course of the semester. Students\nsubmitted a total of 2,591 queries.\n4.1 CodeHelp Overall Usage\nTo capture different aspects of students‚Äô CodeHelp usage a compos-\nite measure was created from the three metrics described below.\nEach metric was transformed to help address skew and then stan-\ndardized before being averaged together. The composite measured\nshowed good internal consistency (Cronbach‚Äôs ùõº = .87).\n(1) Total Queries: We calculated the total number of queries a\nstudent submitted during the course of the semester. While,\non average, students submitted about 40 queries during the\nsemester (M = 39.50, SD = 42.45), one student was an ex-\ntreme outlier submitting a total of 614 queries. This student\nand was excluded from subsequent correlations analyses\nexamining overall usage.\n(2) Total Sessions: This represents the unique number of ‚Äòses-\nsions‚Äô in which a student engaged with CodeHelp during\nthe semester (M = 13.50, SD = 11.54). A ‚Äòsession‚Äô is charac-\nterized as a span of activity that is distinctly segregated by\na minimum inactivity period of one hour.\n(3) Average Length of Session: We calculated the mean length\nof a student‚Äôs sessions during the semester in seconds (M =\n885.13, SD = 1082.17), which was roughly 15 minutes.\n4.2 Characterizing Queries\nTo investigate how students used CodeHelp, we characterized their\nqueries using a mix of manual categorization and automated analy-\nsis.\n4.2.1 Deduplication and Query Cleaning. Students would some-\ntimes resubmit a query with little or no change, likely in an at-\ntempt to receive a different response from the LLM, which is non-\ndeterministic. We filtered out these duplicate queries prior to con-\ntent coding so that repetitions did not exert an undue influence on\nefforts to characterize the results2. Of the 2,591 raw queries, this\nprocess filtered 509 as duplicates, leaving 2,082 queries to be coded.\nIn addition, during the coding process, three queries were identified\nthat did not pertain to the course (e.g. ‚ÄúWhat is the meaning of\nlife?‚Äù) and were omitted from subsequent analysis.\n4.2.2 Query Content Categories. We manually coded all 2,082 dedu-\nplicated queries into categories based on their contents, with each\nquery coded independently by two of the authors. To determine the\ncategories, all four authors independently read different selections\nof the queries, each creating categories as needed to classify suc-\ncessive queries until reaching saturation (a long stretch of queries\nthat required no new categories). In total, this effort covered over\n500 different queries. Then, the authors compared and refined their\n2Any two consecutive queries ùëã and ùëå from a given user were considered duplicate\nbased on a measure of similarity between ùëã and ùëå . Similarity was calculated by\ncomputing the Levenshtein edit distances between each of the code, error, and issue\nstrings of ùëã and ùëå , normalize each to the range [0, 1]by dividing by the longer of\nthe two strings in each case, and add the three normalized distances. If the sum is less\nthan a threshold ùëò, we considered the second query to be a duplicate of the first.\nDuplicate(ùëã, ùëå):= ¬©¬≠\n¬´\n‚àëÔ∏Å\nùë†‚àà{ùëêùëúùëëùëí,ùëíùëüùëüùëúùëü,ùëñùë†ùë†ùë¢ùëí }\nLevenshtein(ùë†ùëã , ùë†ùëå )\nmax(|ùë†ùëã |, |ùë†ùëå |)\n¬™¬Æ\n¬¨\n< ùëò\nThe sum can range from 0 (all three strings equivalent) to 3 (all three strings entirely\ndifferent). We chose a thresholdùëò of 0.25, meaning, roughly, that a query is considered\na duplicate of a student‚Äôs previous query if it differs in no more than a quarter of one\nof the query‚Äôs three strings.\nPatterns of Student Help-Seeking When Using a Large Language Model-Powered Programming Assistant\nproposed taxonomies. This process produced four major categories\nof queries:\n(1) ‚ÄúDebugging‚Äù: Queries where students were looking for help\nto solve specific errors and faults in their code. Debugging\nqueries were further sub-categorized into one of the fol-\nlowing types: a) debugging queries that only included the\nerror b) debugging queries that only included the desired\noutcome (what the code is supposed to do), or c) debug-\nging queries that included both the error and the desired\noutcome.\n(2) ‚ÄúImplementation‚Äù: Queries about implementing code or\nfunctions to solve specific assignment problems. These\nqueries included code and/or referenced course assignment\ninstructions.\n(3) ‚ÄúUnderstanding‚Äù: Queries centered around gaining an un-\nderstanding of programming concepts, algorithms, data\nstructures, language, or library features, but not obviously\nasking how to complete a given assignment problem. These\nqueries did not include code or reference assignment in-\nstructions.\n(4) ‚ÄúNothing‚Äù (no explicit student request): Queries that pro-\nvided no error or meaningful issue. These queries com-\nmonly only provided code with no other context.\n4.2.3 Inter-Rater Reliability. To examine inter-rater reliability of\nhuman coded categories, we calculated Cohen‚Äôs ùúÖ for all categories\nand sub-categories, for all categories with the sub-categories col-\nlapsed, and then for individual categories using binary comparisons.\nOverall reliability for ratings of all categories and sub-categories\nwas substantial (ùúÖ = .75), and overall reliability was even higher (ùúÖ\n= .83) when Debugging sub-categories were collapsed in a single\nDebugging category [21]. ùúÖ values for individual category ratings\nare presented in Table 1.\n4.2.4 Identifying \"Low-Effort\" Queries. During the course and while\ninspecting queries later, we noticed that students would often sub-\nmit ‚Äúlow-effort‚Äù queries in which they wrote little or nothing them-\nselves. Some queries contained little or nothing in the Issue input at\nall, while others were mostly or entirely copied and pasted from the\ninstructions in a class exercise or assignment. This behavior some-\nwhat mirrors the low-effort submissions noted by Kazemitabaar et\nal. in their LLM-powered tool [15]. To investigate this further, we\nused automated analyses to identify queries with two additional\ncharacteristics:\n(1) Empty or Short Issue\nAny issue text whose length was than 10 characters was\nclassified as empty/short, matching 8% of all queries. In\nthese cases, there is little to indicate the student‚Äôs intent,\nbut CodeHelp‚Äôs responses typically described what the code\nappeared to be aiming to do and pointed out potential er-\nrors in the code. Students may have been submitting these\nqueries expecting that type of response. Note that these\nwere inferred to be Debugging queries in the case that\nsomething was specified in the error input.\n(2) Substantially Copied from Course Materials\nTo quantify copied course content, we computed ‚Äúdiffs‚Äù be-\ntween each query‚Äôs Issue text and the text of every exercise\n0.0 0.2 0.4 0.6 0.8 1.0\nFraction of queries (after deduplication)\nImplementation\nDebugging\nUnderstanding\nNothing\nIssue short\n(< 10 characters)\nIssue copied\n(> 80% equivalent)\nIssue short or copied\nFigure 7: Fraction of each student‚Äôs queries (after dedupli-\ncation) with each characteristic. Each point represents one\nstudent and the fraction of their queries having that charac-\nteristic. The box-and-whiskers plots follow convention: the\nleft and right edges of each box indicate the lower and upper\nquartiles, the red line inside the box indicates the median,\nand whiskers are drawn to the farthest point that is at most\n1.5 times the inter-quartile range beyond the box.\nin the course using Python‚Äôs difflib library. For each diff,\nwe calculated the percentage of the Issue text that matched\nthe exercise, and we took the highest percentage across all\nexercises as the ‚Äúcopied percentage‚Äù for that Issue text. We\nchose a threshold of 80% as a cutoff to indicate that most if\nnot all of the Issue text was directly copied from an exercise.\nIn cases where the match percentage was less than 100% but\ngreater than 80%, the non-matching text was often some-\nthing like ‚ÄúHow do I‚Äù followed by the copied exercise text\nor a simple bit of added context like ‚ÄúThis is using Pandas. ‚Äù\n14% of all queries were above this 80%-matching threshold.\n4.3 Course Performance\nTo address the question of how CodeHelp usage relates to perfor-\nmance in the class we used course points that were earned through-\nout the semester through typical course activities including quizzes,\nassignments, and reading responses. Following transformation to\ncorrect skew, the course points earned in each activity were z-scored\nand then aggregated to create an overall Course Performance met-\nric.\n5 RESULTS\n5.1 Characterizing Queries\nFigure 7 presents the fraction of each student‚Äôs queries (after dedu-\nplication) in each category, including both the manually-coded and\nthe automatically-assigned categories.\nStudents tended to submit more debugging queries than any\nother type. On average, 48% of a student‚Äôs queries were debugging\nqueries, and 47 of the 49 students submitted at least one debugging\nBrad Sheese, Mark Liffiton, Jaromir Savelka, and Paul Denny\nTable 1: Query Categories. Descriptives (count and %) and inter-rater agreement for top-level content categories and debugging\nsubcategories.\nQuery Category Example Text Count Percent Kappa\nDebugging (all) ‚ÄúWhy doesn‚Äôt/can‚Äôt/isn‚Äôt the code doing X?‚Äù 833 40% .86\nImplementation ‚ÄúHow do I get it to X?‚Äù 1038 50% .83\nUnderstanding ‚ÄúWhat is the difference between functions X and Y?‚Äù 161 8% .77\nNothing (No explicit student request) 47 2% .78\nDebugging Sub-Categories Count Percent Kappa\nIncluding error 484 58% .78\nIncluding outcome 90 11% .41\nIncluding error & outcome 259 31% .65\nquery. For 21 students, debugging made up more than half of all\nof their queries. We do see a wide range here, though, and in fact\nstudents are evenly distributed across the entire range from 0% to\n100% in that category. Of the debugging subcategories, including\nthe error but not expected outcome was the most common, making\nup 58% of all debugging queries. The remaining 42% of debugging\nqueries contained or at least suggested an expected outcome, while\nonly 31% of debugging queries contained both an indication of the\nerroneous behavior and the expected outcome.\nImplementation queries were broadly popular, though less so\nthan debugging. On average, 38% of a student‚Äôs queries were re-\nquesting implementation help. 44 of 49 students submitted at least\none implementation query, and for 18 students, more than half of\ntheir queries were implementation queries.\nQueries related to understanding were relatively rare, with only\n28 students submitting at least one query of this type. On average,\njust 9% of each student‚Äôs queries were understanding related. For\ntwo students, understanding queries were the majority of their\nqueries, though both of these students submitted fewer than ten\nqueries, total.\nIn many cases, students submitted a query with nothing or very\nlittle (fewer than ten characters) in the Issue input. 29 students did\nthis at least once, and for five students, the Issue input was effec-\ntively empty in more than 10% of their queries. Similarly, about\n14% of all queries were entirely or substantially copied from ex-\nercise instructions (at or above the 80% threshold), and 31 of the\n49 students submitted at least one of these queries. Six students\nsubmitted substantially copied Issue text in more than 50% of their\nqueries. Taking these two categories together as queries in which\nthe student wrote little or nothing themselves in the Issue input,\nwe found that 41 students submitted at least one query with either\nof those characteristics, and such queries made up over 50% of\nsubmitted queries for 12 students.\nAll together, these data illustrate that, broadly, the students in\nthis course tended to be:\n‚Ä¢ Primarily focused on receiving immediate help with a pro-\ngramming problem rather than on improving their under-\nstanding of concepts ‚Äì indicated by far higher incidence\nof Implementation and Debugging queries compared to\nUnderstanding.\n‚Ä¢ Unsophisticated in their requests, often writing little or\nnothing themselves to direct the assistance they receive,\nand often omitting important details (any indication of\neither the error or the expected outcome) when requesting\ndebugging help.\n5.2 Tool Usage and Course Performance\nWe computed Pearson‚Äôs r correlation coefficient to analyze the\nrelationship between tool usage and course performance (r = 0.38,\np = 0.0126). This result indicates a modest positive relation between\nthe two variables. As illustrated in Figure 8, students showing higher\ntool usage tended to show somewhat higher course performance.\nPlease note that this correlation does not suggest, or support, any\nclaims of causality and should not be interpreted as support for the\nclaim that usage caused higher performance.\n6 DISCUSSION & CONCLUSIONS\nThis work examined how students seek help when using an LLM-\npowered teaching assistant during a semester long introductory\nprogramming course. We found that individual students showed\na variety of approaches when constructing queries, but that the\nmajority of queries concerned debugging code errors or getting\nstarted with programming assignments. Students were relatively\nunlikely to request help with deepening their understanding of\ngeneral concepts or principles related to their assignments. We\nfound the scope and quality of student queries to be limited in\nmany cases. For example, we found many instances of debugging\nqueries without descriptions of problems or intended behavior.\nOur results highlight the strengths and weaknesses of these\ntypes of tools in supporting student learning and problem-solving.\nStudents collectively submitted thousands of queries throughout\nthe course of the semester without any extrinsic incentive to do\nso. Many of the queries were well-articulated and provided good\nopportunities for the student to learn from the response. However,\nthe poor quality of many queries also highlights the need to cul-\ntivate effective help-seeking skills. Guardrails that prohibit LLMs\nfrom providing code may be just the first step to guiding students\nthrough the process of problem-solving and fostering an inquisitive\nmindset rather than mere solution-seeking.\nPatterns of Student Help-Seeking When Using a Large Language Model-Powered Programming Assistant\n2.0\n 1.5\n 1.0\n 0.5\n 0.0 0.5 1.0 1.5\nCourse Performance\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nUsage\nr = 0.35, p = 0.0147\nFigure 8: Scatter plot of the association between CodeHelp Us-\nage and Course Performance. See separate Usage and Course\nPerformance sections for details on metric transformation,\nstandardization, and aggregation. Note: Due to standardiza-\ntion, both metrics have a mean of 0 and a standard deviation\nof 1.\nWe also examined how student performance in the course was\nrelated to their overall pattern of CodeHelp usage during the se-\nmester. We were particularly interested in this relationship given\nthe widely expressed concerns in many areas of education that the\nuse of LLMs will undermine student learning [23]. Indeed, recent\nwork in computing education has clearly illustrated the risks in\nproviding students with unconstrained access to LLMs. For exam-\nple, Prather et al. observed students using Github Copilot to solve a\ntypical CS1 assignment in a laboratory setting [24]. Interviews with\nparticipants were revealing, with one-third of students expressing\nconcerns about the potential negative effects of LLM-based coding\ntools on their learning. These included concerns around solving\nproblems without needing to understand or even process what is\ngoing on, and worries that using such tools would harm problem\nsolving skills given that the tools ‚Äúwould just do it for me‚Äù. Similarly,\nKazemitabaar et al. performed a thematic analysis of student queries\nto an AI code generator based on OpenAI‚Äôs Codex model [16]. They\nidentified four distinct approaches that students used, and found\nthat by far the most common approach (AI Single Prompt) involved\nlearners prompting Codex just once to generate the entire solution\nto a task. Not only was this the most commonly used approach,\nbut it also correlated negatively with student performance on sub-\nsequent post-tests, suggesting it was detrimental to learning. In\nlight of these well-documented concerns, our goal with CodeHelp\nwas to provide the benefits of LLM assistance to students while\nreducing the harms (e.g., over-reliance). In the current study, we\nfound no evidence consistent with the idea that CodeHelp usage\nundermined student learning. Instead, our results showed a modest\npositive association between overall use during the semester and\ncourse performance.\nAn interesting avenue for future work would be to provide au-\ntomatic feedback to students directly within the tool to help them\nconstruct effective queries when seeking help. This aligns with\nthe goals of recent work by Denny et al., on the use of Prompt\nProblems in introductory programming courses, which leverages\nLLMs to teach students how to craft good prompts for AI-based\ncode generators [8]. In the case of our work with CodeHelp, if a\nstudent failed to clearly describe the intended behaviour of the\ncode for which they are seeking help, the tool could prompt them\nto do so before providing assistance. Similar feedback could be\nused to dissuade students from relying too much on the tool for\ncertain kinds of queries, such as ‚Äòimplementation‚Äô questions. In\nsome cases, such as where an input field is left empty, generating\nsuitable feedback to students would be straightforward. In other\ncases, the ability to automatically classify the queries that students\nsubmit into the categories we have presented in this work would\nfacilitate the generation of useful feedback. For instance, this would\npermit the identification of debugging queries where the intended\nbehaviour is missing, or potential overuse of CodeHelp for request-\ning help with implementation. Recent work by Gao et al. has shown\ngood potential for this type of automated classification [12].\n6.1 Limitations and Threats to Validity\nOur research context was limited to a specific introductory com-\nputer and data science course with a sample of 52 students. Caution\nis warranted in generalizing our findings to other student demo-\ngraphics, course structures, and institutional cultures. Similarly,\nthe 12-week duration of our investigation may influence both the\nfrequency and type of student queries. It may be the case that the\nnature of students‚Äô queries changed over time as they gained more\nexperience with the tool. Thus, longer or shorter studies may find\nother patterns of usage. Similarly, the association we reported be-\ntween academic performance and usage may be yolked to a variety\nof factors that we did not address, such as course style, content,\ndifficulty, average student preparedness, and student comfort with\nAI-driven tools. As this study was conducted in the Spring of 2023,\nmany students may not have had much, if any, prior experience\nworking with LLMs. As the general population becomes increas-\ningly familiar with LLMs, different patterns of results may emerge.\nOur qualitative approach to the development of content cate-\ngories as well as our use of human raters introduces quite a bit of\nsubjectivity into our study. While we aimed to be objective and to\nestablish the reliability of our approach, the inherent subjectivity\ncannot be ignored. Our categories and ratings may reflect unknown\nbiases and it is possible that other researchers with different back-\ngrounds might produce categorizations schemes, different ratings,\nand, ultimately, different conclusions.\nFinally, we acknowledge the role that interface design can play\nin shaping user behaviors and interactions. For example, the Code-\nHelp‚Äôs interface may have steered students towards submitting\ndebugging-focused queries. Overall, our tool‚Äôs layout might have\nbeen more amenable to code implementation queries, at the ex-\npense of more abstract or conceptual inquiries. As we look to future\nrefinements, it is crucial to contemplate redesigning the interface or\nBrad Sheese, Mark Liffiton, Jaromir Savelka, and Paul Denny\nembedding clear cues to facilitate a broader spectrum of queries, en-\nsuring a balance between technical troubleshooting and conceptual\nunderstanding.\n6.2 Implications for Teaching Practice and\nFuture Work\nFuture work should move beyond overall course performance and\nfocus on studying the long-term effects of LLM usage on student\nproblem-solving skills and conceptual development. Furthermore,\nexamining a broader range of students from across computing\ndisciplines and educational levels would provide a more thorough\nunderstanding of LLMs‚Äô capabilities in supporting student learning\nin other contexts.\nIt is a notable feature of CodeHelp and the LLM-powered ap-\nproach that even when students do not clearly communicate their\nissue or even write little to nothing in their own words, they can\nstill receive a helpful response from the tool. However, these helpful\nresponses can then positively reinforce the submission of poorly\narticulated queries. This is unfortunate, as effective communication\nof programming problems is a useful skill to develop and is an\nactivity that may help students identify their learning gaps and\ndeepen their conceptual understanding of course materials. While\nthe students appear to prefer immediate task-specific assistance, it\nis essential to also emphasize the importance of developing deep\nconceptual understanding. Programming education is not limited\nto resolving immediate coding challenges, but also entails building\na solid foundation of underlying concepts.\nThere are many opportunities to investigate these tendencies and\ntheir implications further. Empirically, there is more to be learned\nabout the balance between the efficiency of ‚Äúlow-effort‚Äù queries and\nthe learning costs to the student of not articulating their issue. When\nfurther developing tools, additional or alternative methods of scaf-\nfolding and guiding students‚Äô inputs may encourage use that helps\nadvance students‚Äô conceptual understanding and communication\nskills in addition to helping them solve their immediate problems.\nAdditionally, educators should consider offering explicit guidance\non formulating high-quality technical questions, e.g., by integrat-\ning structured tutorials, examples of well-formulated questions,\nor feedback mechanisms into the assisting tool. Finally, practicing\neffective interaction with LLM powered tools as described in [ 8]\nmay help students overcome these issues.\nREFERENCES\n[1] Rishabh Balse, Bharath Valaboju, Shreya Singhal, Jayakrishnan Madathil War-\nriem, and Prajish Prasad. 2023. Investigating the Potential of GPT-3 in Providing\nFeedback for Programming Assessments. In Proceedings of the 2023 Conference\non Innovation and Technology in Computer Science Education V. 1(Turku, Fin-\nland) (ITiCSE 2023). Association for Computing Machinery, New York, NY, USA,\n292‚Äì298. https://doi.org/10.1145/3587102.3588852\n[2] Brett A Becker, Paul Denny, James Finnie-Ansley, Andrew Luxton-Reilly, James\nPrather, and Eddie Antonio Santos. 2023. Programming Is Hard-Or at Least It\nUsed to Be: Educational Opportunities and Challenges of AI Code Generation. In\nProceedings of the 54th ACM Technical Symposium on Computer Science Education\nV. 1. ACM, Toronto, ON, Canada, 500‚Äì506.\n[3] Peter Brusilovsky, Barbara J Ericson, Cay S Horstmann, and Christian Servin.\n2023. The Future of Computing Education Materials.\n[4] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang\nLou, and Weizhu Chen. 2022. CodeT: Code Generation with Generated Tests.\nhttp://arxiv.org/abs/2207.10397 arXiv:2207.10397 [cs].\n[5] Bruno Pereira Cipriano and Pedro Alves. 2023. GPT-3 vs Object Oriented Pro-\ngramming Assignments: An Experience Report. In Proceedings of the 2023 Con-\nference on Innovation and Technology in Computer Science Education V. 1(Turku,\nFinland) (ITiCSE 2023). Association for Computing Machinery, New York, NY,\nUSA, 61‚Äì67. https://doi.org/10.1145/3587102.3588814\n[6] Jonathan E Collins. 2023. Policy Solutions: Policy questions for ChatGPT and\nartificial intelligence. Phi Delta Kappan104, 7 (2023), 60‚Äì61.\n[7] Paul Denny, Viraj Kumar, and Nasser Giacaman. 2023. Conversing with Copi-\nlot: Exploring Prompt Engineering for Solving CS1 Problems Using Natu-\nral Language. In Proceedings of the 54th ACM Technical Symposium on Com-\nputer Science Education V. 1. ACM, Toronto ON Canada, 1136‚Äì1142. https:\n//doi.org/10.1145/3545945.3569823\n[8] Paul Denny, Juho Leinonen, James Prather, Andrew Luxton-Reilly, Thezyrie\nAmarouche, Brett A Becker, and Brent N Reeves. 2023. Promptly: Using Prompt\nProblems to Teach Learners How to Effectively Utilize AI Code Generators.arXiv\npreprint arXiv:2307.16364(2023).\n[9] Paul Denny, James Prather, Brett A. Becker, James Finnie-Ansley, Arto Hellas,\nJuho Leinonen, Andrew Luxton-Reilly, Brent N. Reeves, Eddie Antonio San-\ntos, and Sami Sarsa. 2023. Computing Education in the Era of Generative AI.\narXiv:2306.02608 [cs.CY]\n[10] James Finnie-Ansley, Paul Denny, Brett A Becker, Andrew Luxton-Reilly, and\nJames Prather. 2022. The robots are coming: Exploring the implications of openai\ncodex on introductory programming. In Proceedings of the 24th Australasian\nComputing Education Conference. 10‚Äì19. https://doi.org/10.1145/3511861.3511863\n[11] Zhikai Gao, Sarah Heckman, and Collin Lynch. 2022. Who Uses Office Hours?\nA Comparison of In-Person and Virtual Office Hours Utilization. In Proceedings\nof the 53rd ACM Technical Symposium on Computer Science Education - Volume\n1 (Providence, RI, USA) (SIGCSE 2022). Association for Computing Machinery,\nNew York, NY, USA, 300‚Äì306. https://doi.org/10.1145/3478431.3499334\n[12] Zhikai Gao, Collin Lynch, Sarah Heckman, and Tiffany Barnes. 2021. Automati-\ncally Classifying Student Help Requests: A Multi-Year Analysis. International\nEducational Data Mining Society(2021).\n[13] Arto Hellas, Juho Leinonen, Sami Sarsa, Charles Koutcheme, Lilja Kujanp√§√§,\nand Juha Sorva. 2023. Exploring the Responses of Large Language Models to\nBeginner Programmers‚Äô Help Requests. arXiv preprint arXiv:2306.05715(2023).\n[14] Sajed Jalil, Suzzana Rafi, Thomas D. LaToza, Kevin Moran, and Wing Lam. 2023.\nChatGPT and Software Testing Education: Promises & Perils. http://arxiv.org/\nabs/2302.03287 arXiv:2302.03287 [cs].\n[15] Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara J. Ericson, David\nWeintrop, and Tovi Grossman. 2023. Studying the Effect of AI Code Generators\non Supporting Novice Learners in Introductory Programming. In Proceedings\nof the 2023 CHI Conference on Human Factors in Computing Systems(Hamburg,\nGermany) (CHI ‚Äô23). Association for Computing Machinery, New York, NY, USA,\nArticle 455, 23 pages. https://doi.org/10.1145/3544548.3580919\n[16] Majeed Kazemitabaar, Xinying Hou, Austin Henley, Barbara J. Ericson, David\nWeintrop, and Tovi Grossman. 2023. How Novices Use LLM-Based Code Gen-\nerators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment.\narXiv:2309.14049 [cs.HC]\n[17] Juho Leinonen, Paul Denny, Stephen MacNeil, Sami Sarsa, Seth Bernstein, Joanne\nKim, Andrew Tran, and Arto Hellas. 2023. Comparing Code Explanations Created\nby Students and Large Language Models. http://arxiv.org/abs/2304.03938\narXiv:2304.03938 [cs].\n[18] Juho Leinonen, Arto Hellas, Sami Sarsa, Brent Reeves, Paul Denny, James Prather,\nand Brett A Becker. 2023. Using large language models to enhance programming\nerror messages. InProceedings of the 54th ACM Technical Symposium on Computer\nScience Education V. 1. 563‚Äì569.\n[19] National Academies of Sciences, Engineering, and Medicine and others. 2018.\nAssessing and responding to the growth of computer science undergraduate enroll-\nments. National Academies Press.\n[20] Nhan Nguyen and Sarah Nadi. 2022. An empirical evaluation of GitHub copilot‚Äôs\ncode suggestions. In Proceedings of the 19th International Conference on Mining\nSoftware Repositories. ACM, Pittsburgh Pennsylvania, 1‚Äì5. https://doi.org/10.\n1145/3524842.3528470\n[21] Cliodhna O‚ÄôConnor and Helene Joffe. 2020. Intercoder reliability in qualitative\nresearch: debates and practical guidelines. International journal of qualitative\nmethods 19 (2020), 1609406919899220.\n[22] Zachary A. Pardos and Shreya Bhandari. 2023. Learning gain differences between\nChatGPT and human tutor generated algebra hints. arXiv:2302.06871 [cs.CY]\n[23] James Prather, Paul Denny, Juho Leinonen, Brett A. Becker, Ibrahim Albluwi,\nMichelle Craig, Hieke Keuning, Natalie Kiesler, Tobias Kohn, Andrew Luxton-\nReilly, Stephen MacNeil, Andrew Peterson, Raymond Pettit, Brent N. Reeves,\nand Jaromir Savelka. 2023. The Robots are Here: Navigating the Generative AI\nRevolution in Computing Education. arXiv:2310.00658 [cs.CY]\n[24] James Prather, Brent N. Reeves, Paul Denny, Brett A. Becker, Juho Leinonen,\nAndrew Luxton-Reilly, Garrett Powell, James Finnie-Ansley, and Eddie Antonio\nSantos. 2023. \"It‚Äôs Weird That it Knows What I Want\": Usability and Interac-\ntions with Copilot for Novice Programmers. http://arxiv.org/abs/2304.02491\narXiv:2304.02491 [cs].\n[25] Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. 2022. Automatic Gen-\neration of Programming Exercises and Code Explanations Using Large Language\nModels. In Proceedings of the 2022 ACM Conference on International Computing\nPatterns of Student Help-Seeking When Using a Large Language Model-Powered Programming Assistant\nEducation Research V.1. ACM, Lugano and Virtual Event Switzerland, 27‚Äì43.\nhttps://doi.org/10.1145/3501385.3543957\n[26] Jaromir Savelka, Arav Agarwal, Marshall An, Chris Bogart, and Majd Sakr. 2023.\nThrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle\nto Pass Assessments in Higher Education Programming Course. In Proceedings\nof the 2023 ACM Conference on International Computing Education Research V.1.\nACM.\n[27] Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques\nKlein, and Tegawend√© F. Bissyand√©. 2023. Is ChatGPT the Ultimate Programming\nAssistant ‚Äì How far is it? http://arxiv.org/abs/2304.11938 arXiv:2304.11938 [cs].",
  "topic": "Software deployment",
  "concepts": [
    {
      "name": "Software deployment",
      "score": 0.7808334231376648
    },
    {
      "name": "Computer science",
      "score": 0.6942988038063049
    },
    {
      "name": "Class (philosophy)",
      "score": 0.6041943430900574
    },
    {
      "name": "Plan (archaeology)",
      "score": 0.6024378538131714
    },
    {
      "name": "Lesson plan",
      "score": 0.43136078119277954
    },
    {
      "name": "Data science",
      "score": 0.3419341444969177
    },
    {
      "name": "Multimedia",
      "score": 0.3265327215194702
    },
    {
      "name": "Mathematics education",
      "score": 0.3144040107727051
    },
    {
      "name": "Software engineering",
      "score": 0.2388611137866974
    },
    {
      "name": "Artificial intelligence",
      "score": 0.21128839254379272
    },
    {
      "name": "Psychology",
      "score": 0.18394365906715393
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}