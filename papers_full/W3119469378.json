{
  "title": "Transformer-based Conditional Variational Autoencoder for Controllable Story Generation",
  "url": "https://openalex.org/W3119469378",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2347433594",
      "name": "Fang Le",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097142017",
      "name": "Zeng Tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748269948",
      "name": "Liu Chaochun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3041337790",
      "name": "Bo, Liefeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2081529534",
      "name": "Dong Wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2256462353",
      "name": "Chen, Changyou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2924334974",
    "https://openalex.org/W3005768470",
    "https://openalex.org/W2962717182",
    "https://openalex.org/W2964076537",
    "https://openalex.org/W2983962589",
    "https://openalex.org/W2962897886",
    "https://openalex.org/W2594538354",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2996843693",
    "https://openalex.org/W3013310839",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2970682219",
    "https://openalex.org/W2341790067",
    "https://openalex.org/W3039805635",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2987787977",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3035451444",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W3118543071",
    "https://openalex.org/W2964669873",
    "https://openalex.org/W2933374552",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2963506530",
    "https://openalex.org/W3000538780",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2967853777",
    "https://openalex.org/W2964000524",
    "https://openalex.org/W2160204597",
    "https://openalex.org/W2963411289",
    "https://openalex.org/W2017257315",
    "https://openalex.org/W2963366196",
    "https://openalex.org/W2970102799",
    "https://openalex.org/W2805486818",
    "https://openalex.org/W2210838531",
    "https://openalex.org/W2963993699",
    "https://openalex.org/W2963773425",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W3098708719",
    "https://openalex.org/W2606657156",
    "https://openalex.org/W2970832665",
    "https://openalex.org/W2997195635"
  ],
  "abstract": "We investigate large-scale latent variable models (LVMs) for neural story generation -- an under-explored application for open-domain long text -- with objectives in two threads: generation effectiveness and controllability. LVMs, especially the variational autoencoder (VAE), have achieved both effective and controllable generation through exploiting flexible distributional latent representations. Recently, Transformers and its variants have achieved remarkable effectiveness without explicit latent representation learning, thus lack satisfying controllability in generation. In this paper, we advocate to revive latent variable modeling, essentially the power of representation learning, in the era of Transformers to enhance controllability without hurting state-of-the-art generation effectiveness. Specifically, we integrate latent representation vectors with a Transformer-based pre-trained architecture to build conditional variational autoencoder (CVAE). Model components such as encoder, decoder and the variational posterior are all built on top of pre-trained language models -- GPT2 specifically in this paper. Experiments demonstrate state-of-the-art conditional generation ability of our model, as well as its excellent representation learning capability and controllability.",
  "full_text": "Transformer-based Conditional Variational Autoencoder\nfor Controllable Story Generation\nLe Fang†, Tao Zeng§, Chaochun Liu§, Liefeng Bo§, Wen Dong†, Changyou Chen†\n†University at Buffalo, §JD Finance America Corporation, AI Lab\n{lefang, wendong, changyou}@buffalo.edu\n{tao.zeng, chaochun.liu, liefeng.bo}@jd.com\nAbstract\nWe investigate large-scale latent variable models (LVMs) for\nneural story generation—an under-explored application for\nopen-domain long text—with objectives in two threads: gener-\nation effectiveness and controllability. LVMs, especially the\nvariational autoencoder (V AE), have achieved both effective\nand controllable generation through exploiting ﬂexible distri-\nbutional latent representations. Recently, Transformers and\nits variants have achieved remarkable effectiveness without\nexplicit latent representation learning, thus lack satisfying con-\ntrollability in generation. In this paper, we advocate to revive\nlatent variable modeling, essentially the power of representa-\ntion learning, in the era of Transformers to enhance controlla-\nbility without hurting state-of-the-art generation effectiveness.\nSpeciﬁcally, we integrate latent representation vectors with\na Transformer-based pre-trained architecture to build condi-\ntional variational autoencoder (CV AE). Model components\nsuch as encoder, decoder and the variational posterior are all\nbuilt on top of pre-trained language models—GPT2 speciﬁ-\ncally in this paper. Experiments demonstrate state-of-the-art\nconditional generation ability of our model, as well as its ex-\ncellent representation learning capability and controllability.\nIntroduction\nNeural text generation has achieved remarkable success to\nenable both effective and controllable generation at a level\nthat computational models can write like humans to satisfy\npractical needs. Among various research objectives, the most\nsigniﬁcant ones are the effectiveness and the controllability\nof generation, where there are always emerging opportunities\nand challenges.\nDeep latent variable models (LVMs), especially variational\nautoencoder (V AE) (Kingma and Welling 2013; Rezende,\nMohamed, and Wierstra 2014) have been a signiﬁcant class\nof methods to achieve both effective and controllable gener-\nation (Bowman et al. 2015; Miao, Yu, and Blunsom 2016;\nZhao, Zhao, and Eskenazi 2017; Zhao, Lee, and Eskenazi\n2018; Zhou and Neubig 2017; Hu et al. 2017; Bao et al.\n2019b; Shah and Barber 2018). These models generally work\nwith recurrent neural networks (RNN) such as Long short-\nterm memory (LSTM) (Hochreiter and Schmidhuber 1997)\nand Gated recurrent unit networks (GRU) (Cho et al. 2014).\nThe advantage of LVMs is to learn and exploit ﬂexible dis-\ntributional latent representations to capture holistic features\nof input and further guide the generation of sentences. Such\npowerful representation learning can deal with both the effec-\ntiveness and the controllability of generation.\nIn recent years, Transformers (Vaswani et al. 2017) and\nits variants have become the main-stream workhorses and\nboosted previous generation effectiveness by large margins.\nModels based on similar self-attention architectures (Devlin\net al. 2018; Radford et al. 2018, 2019) could leverage both big\nmodels and big training data. A dominant paradigm emerges\nto be “pre-training + ﬁne-tuning” on a number of natural\nlanguage processing tasks. Even without explicitly learning\nlatent representations, Transformer-based models could ef-\nfectively learn from training data and generate high-quality\ntext. It’s thrilling to witness computational models generate\nconsistent long text in thousands of words with ease. How-\never, given state-of-the-art generation effectiveness, control-\nlability of these models—especially when generating long\ntext—is still under-explored. The emerging challenge is, how\nto achieve controllable generation in the era of Transformers\nand a long text setting?\nIn this paper, we advocate to revive latent variable mod-\neling, essentially the power of representation learning, in\nthe era of Transformers to enhance controllability without\nhurting state-of-the-art generation effectiveness. Speciﬁcally,\nwe integrate latent representation vectors with self-attention\nbased pre-trained architecture to build a conditional vari-\national autoencoder (CV AE). Model components such as\nencoder, decoder and the variational posterior are all built on\ntop of pre-trained language models—GPT2 (Radford et al.\n2019) speciﬁcally. We demonstrate excellent representation\nlearning capability and controllability of our Transformer-\nbased LVMs through learning and manipulating the latent\nrepresentation vectors.\nOn the application side, we emphasize a much challeng-\ning and under-explored task, i.e. neural story generation,\nwhich creatively writes open-domain long text in hundreds\nor thousands of words conditioned on very short and abstract\nprompts (Fan, Lewis, and Dauphin 2018). The task featured\nwith much longer output leads to higher complexity and more\nﬂexibility in a broader space than short text generation. Pre-\nvious literature (Fan, Lewis, and Dauphin 2018; Mao et al.\n2019; See et al. 2019; Ziegler et al. 2019) have at most stud-\nied how to effectively learn the mapping between prompt and\nstory through explicit end to end (end2end) training. How-\never, controllability in such a setting has rarely been studied.\narXiv:2101.00828v2  [cs.CL]  8 Jul 2021\nFor instance, how to control story development and semantic\ntransition during the spanning of long text? Pure end2end\nlearning seems quite rigid, which could miss ﬂexible and\ncontrollable mechanisms inside a black box. A reasonable\nsolution for this issue is to introduce latent representation\nvectors, which is the treatment we consider in this paper.\nTo summarize, our paper is among the ﬁrst works, by our\nknowledge, to build Transformer-based latent variable mod-\nels to solve the controllability issue in the setting of long text\ngeneration. Recently, we notice an independent parallel work\n(Li et al. 2020), which proposes similar Transformer-based\narchitecture to incorporate latent representation vectors. We\nnote that there are a number of differences between this work\nand ours. Most signiﬁcantly, we considered both V AE and\nCV AE in a long text setting, while Li et al. (2020) considered\na pre-trained V AE model in traditional short text setting. Our\ndatasets and source code is available on GitHub1.\nThe Model Architecture\nConditional Variational Autoencoder\nConditional story generation (Fan, Lewis, and Dauphin 2018)\nrefers to generating open-domain long text based on a short\nprompt, which provides either a starting point or an ab-\nstract summary for the writing. In this paper, we propose\na Transformer-based conditional variational autoencoder to\nlearn the generative process from prompt to story.\nFigure 1: Graphical Model of V AE and CV AE. In control-\nlable story generation, xand yrefer to a prompt and a story,\nrespectively. zrefers to a latent variable.\nVariational Autoencoder (V AE)(Bowman et al. 2015)\nFigure 1 illustrates the graphical model of V AE, an unsu-\npervised learning method for unconditional generation. V AE\nconsists of a generative network (decoder) and an inference\nnetwork (encoder). Given a language dataset D= {yi}|D|\ni=1,\nwhere yi = [y1i,··· ,yTi ] represents ith sentence of length\nT. With a prior distribution p(z), V AE generates a sentence\nyusing the deep generative network pθ(y|z) parameterized\nby θ. The prior p(z) is typically assumed to be a standard\nmultivariate Gaussian. The decoder pθ(x|z) typically takes\nan auto-regressive form pθ(x|z) =∏T\nt=1 pθ(xt|x<t,z). In\nthis paper, we will build the decoder based on the pre-trained\nGPT2 rather than traditional recurrent neural networks.\nThe goal of V AE training is to maximize the marginal\ndata log-likelihood Ey∼D[log pθ(y)]. However, poste-\nrior inference is generally intractable. Consequently, an\n1https://github.com/fangleai/TransformerCV AE\nφ-parameterized encoder is introduced to approximate\npθ(z|y) ∝ pθ(y|z)p(z) with a variational distribution\nqφ(z|y). Variational inference is employed for V AE learning,\nyielding the following evidence lower bound (ELBO):\nEx∼Dlogpθ(x) ≥\nEx∼D\n[\nEz∼qφ(z|x)logpθ(x|z)\n]\n−Ex∼D[KL (qφ(z|x) ∥p(z))] (1)\nConditional Variational Autoencoder (CV AE) (Zhao,\nZhao, and Eskenazi 2017) Figure 1 also illustrates the\nCV AE, an adaptation of V AE to ﬁt supervised learning\nand conditional generation. Given a training dataset of\npairs D= {xi,yi}|D|\ni=1, where xi = [x1i,··· ,xTi ],yi =\n[y1i,··· ,yTi ] represents ith sentence of length T. In con-\ntrollable story generation, xand yrefer to a prompt and a\nstory, respectively. Given an inputx, CV AE encodes the prior\nknowledge of latent code as p(z|x), and generates target y\nusing the deep generative network pθ(y|x,z) parameterized\nby θ.\nThe goal of CV AE is to maximize the conditional data\nlog-likelihood Ex,y∼D[log pθ(y|x)]. Similarly, variational\ninference is employed for CV AE learning, yielding the fol-\nlowing evidence lower bound (ELBO):\nEx,y∼Dlogpθ(y|x) ≥\nEx,y∼D\n[\nEz∼qφ(z|x,y)logpθ(y|x,z)\n]\n−Ex,y∼D[KL (qφ(z|x,y) ∥p(z|x))] (2)\nNote that both the prior pθ(z|x) and posterior qφ(z|x,y)\nare learnable in CV AE.\nArchitecture Design\nOur model architecture is illustrated in Figure 3. Basically,\nit consists of a prior, posterior and conditional generator\nbased on multi-layer self-attention architecture (Vaswani et al.\n2017), more speciﬁcally on top of pre-trained models.\nParameter initialization with GPT2 In order to exploit\nthe power of pre-trained models, we propose to reuse the\nGPT2 model (Radford et al. 2019) as our decoder. For ease\nof computation, we adopt the smallest public version with\nL= 12layers, H = 12heads per layer, model dimension of\nd= 768units and total parameters of 117M. The encoder has\nL1=6 unmasked/bi-directional self-attention layers, whose\nparameters are initialized to the parameters of the ﬁrstL1 lay-\ners of the GPT2 model (initialized but not shared afterwards).\nMoreover, the word embedding and positional embedding\ntables in the encoder and decoder are shared.\nComparing with masked/uni-directional structure in de-\ncoder for auto-regressive generation, the key point is to have\nunmasked/bi-directional structure in the encoder to allow\nfull information scope. In this sense, our design is compara-\nble with (Li et al. 2020) to reuse BERT in the encoder and\nGPT2 in the decoder. However, we avocate two main design\ndifferences: 1) We note that BERT uses word piece (WPE)\nembeddings for tokenization and GPT-2 uses Byte Pair En-\ncoding (BPE), leading to totally different vocabulary books.\nFigure 2: Self-attention (SA) and pseudo self-attention (PSA).\nSA is widely used in our model including the attention-\naverage block, when pseudo self-attention can be used as\nthe 2⃝way to feed latent code (key and value colored in light\nred) to the decoder.\n(Li et al. 2020) resorts to keeping both tokenizations for all\ninputs and outputs; while our design has none of such issues.\n2) (Li et al. 2020) only works with short sentences typically\nless that 64 words, while our model works with hundreds or\nthousands of words in a minimal run. In our case, a model\nwith a full layer encoder (L1=12) is empirically too large to\nﬁt a single GPU memory. In order to save memory and con-\nsidering that the ﬁrst several layers of GPT2 may implicitly\nserve to encode features, our model only use L1=6 layers in\nthe encoder2.\nLatent code from the encoder Traditional RNN/LSTM\nencoders typically only use the last hidden state from the\nencoder to produce a latent space. This is insufﬁcient to\nsummarize sequential data and keep long-term knowledge.\nIn our model, representations from self-attention layers are a\nsequence of vectors with total number equal to the number\nof input tokens. To utilize all the information, we deﬁne an\nattention-average block right afterwards to merge variable\nlength sequence of vectors into a single vector. The attention\naverage block essentially perform a multi-head self-attention\nas Figure 2(a) using a learnable single query Q= qavg ∈Rd,\nand K = V taken as the variable length sequence of vectors\nfrom the last blocked self-attention layer. The single vector\nrepresentation is then passed to linear layers to predict prior\nand posterior distribution, respectively.\nIn terms of model components, we deﬁne both the prior\nand the variational posterior as isotropic Gaussian distribu-\ntions, i.e., N(µ,σ2I), with learnable mean vector and “log σ”\nvector. The KL divergence between the prior and posterior in\nEq. 2 is therefore analytically solvable. Traditional reparame-\nterization trick (Kingma and Welling 2013) is used to allow\ngradient passing through Gaussian sampling. Note that in our\nmodel, the prior distribution p(z|x) and variational posterior\ndistribution qφ(z|x,y) share all the parameters except the\nlinear layers predicting their mean and variances to promote\nprior posterior alignment.\nIn contrast to Transformers (Vaswani et al. 2017) and other\nmodels that learn sequence of encoding vectors, our model\nis dedicated to learning a single vector as an explicit latent\nrepresentation.\n2Our experiment conﬁrms that using full layers in encoder has\nlimited improvement in performance comparing to using L1=6\nlayers in encoder.\nFeeding latent code to the decoderWith a single latent\ncode representation3 z ∈Rd and a “GPT2” decoder, we\ninvestigate three mainstream ways of latent code injection\ninspired by previous literatures (Cheng et al. 2019; Ziegler\net al. 2019; Wang and Wan 2019).\n1⃝ INPUT : zis added to each input token during decoding,i.e.,\nadded with word embeddings and positional embeddings\nelement-wisely.\n2⃝ PSA4: inject latent code zin a per-layer basis. Speciﬁcally,\nwe ﬁrst project z ∈Rd into zL ∈Rd×L through a linear\nlayer, so that it can be split into L vectors [z1,··· ,zL],\nwith zl being fed into the l-th blocked self-attention layer.\nAs presented in (Ziegler et al. 2019) and shown in Fig-\nure 2(b), pseudo self-attention could absorb extra encoding\nembeddings into a pre-trained GPT2 self-attention struc-\nture through\nPSA(Q,K′,V ′) =softmax(QK′T\n√dk\n)V′ (3)\nwhere Q,K,V ∈Rl×d are the original input embeddings\nparticipating self-attention; K′ =\n(zK\nK\n)\n∈ R(1+l)×d,\nV′ =\n(zV\nV\n)\n∈R(1+l)×d are augmented key and value\nmatrices with projected latent code zK, zV from zﬁlling\nthe ﬁrst row;\n(·\n·\n)\nmeans concatenation by rows. Here, we\nabbreviate per-layer code zl to zfor notation simplicity.\n3⃝ SOFTMAX : in the original GPT2, an embedding vector\nh∈Rd from the last blocked attention layer is projected\nto a pre-softmax logit vectorp∈RV through a linear head,\nwhere V is the vocabulary size used in tokenization. When\na latent code should be injected in such a position, a new\nand shared linear head will be initialized and learned in\norder to project z ∈Rd into pz ∈RV . Finally we send\np+ pz for the softmax and output.\nWe empirically study all the three ways of latent code\ninjection into the decoder, and present comparison in the\nexperiment section.\nTraining and Generation\nWe train our CV AE model according to the negative loss\nobjective in (2). For conditional story generation, the in-\nput to the prior distribution p(z|x) is purely the prompt\nand the input to posterior distribution qφ(z|x,y) is the con-\nnected sequence of prompt and story split by a special to-\nken ‘<|endoftext|>”. The conditional generative distribution\npθ(y|x,z) is implemented as decoding with a text preﬁx\n“prompt + <|endoftext|>” and feeding the latent code.\n3Latent code can have dimension d′ ̸= d, in which case lin-\near projection layers are needed before feeding latent code to the\ndecoder to ensure identical dimensions.\n4In recent literature (Li et al. 2020), they also studied a way of la-\ntent injection described as “Memory” vector. Essentially, “Memory”\nis identical or equivalent to our “PSA”.\nFigure 3: Our CV AE model architecture. Note that we use CV AE1⃝, CV AE2⃝, CV AE3⃝to represent model variants that use the\n1⃝, 2⃝, 3⃝way of latent code injection respectively. The ﬁnal model are not meant to use all ways but only subset combinations\nas implied by performance.\nTo avoid learning deviation caused by random initialized\nparameters, we freeze pre-trained parameters initialized from\nGPT2 in the ﬁrst several iterations of training, i.e. 10K itera-\ntions, and unfreeze them afterwards.\nTo alleviate the notorious posterior collapse issue, we take\na cyclic annealing schedule (Fu et al. 2019) by adjusting the\ncoefﬁcient β before KL divergence in (2). Speciﬁcally, we\nhave kept βclose to zero in the ﬁrst half of cyclic schedule,\nlinearly annealed β to 1 in the next one fourth of cyclic\nschedule and kept β = 1 in the remaining one fourth of\ncyclic schedule. The purpose of such schedule is to exploit\nthe period that β is close to zero, which pushes the model\ntowards a pure autoencoder. Note that autoencoder (Bourlard\nand Kamp 1988) learns point estimate of latent code instead\nof distributional representation to generate target data, which\ncould improve generation effectiveness.\nDuring generation, a short prompt text is fed to the en-\ncoder, and a latent code is sampled from the prior distribution\nto guide the decoding. This procedure is the same as how\ntraditional CV AE works.\nRelated Work\nControllable Story Generation\nMost of previous works on text generation consider a setting\nof short text. For controllable generation, they mainly con-\nsider certain global aspects of text, with the most common\naspects being sentiment and topic (Shen et al. 2017; Zhao\net al. 2018; Hu et al. 2017; Fang et al. 2019; Dathathri et al.\n2019; Keskar et al. 2019; Li et al. 2020; Wang et al. 2020).\nResearchers have attempted short story generation with ﬁne-\ngrained control through plots, plans or the so-called story-\nlines (Peng et al. 2018; Yao et al. 2019), leading to a wide\nusage and benchmark on 5-lines story dataset ROCStories\n(Mostafazadeh et al. 2016).\nIn recent years, (Fan, Lewis, and Dauphin 2018) proposes\nstory generation as a test bed of open-domain long text gen-\neration (Fang et al. 2021). (Ziegler et al. 2019) initiates the\nresearch of conditionally generating story based on a pre-\ntrained GPT2.\nThough achieving promising results, very few works have\nbeen presented to improve controllability in the setting of\nlong text. This work is, by our knowledge, the ﬁrst work to\nbuild a Transformers-based latent variable model to improve\ncontrollable open-domain long text generation.\nTransformers-based LVMs\nRecently, there are several works building latent variable\nmodels on top of the Transformer. One main class of work\nis conditional V AEs with Transformer-based components\nin non-autoregressive sequence generation, especially non-\nautoregressive machine translation (Shu et al. 2020; Ma et al.\n2019; Kasai et al. 2020; Han et al. 2020). Another class of\nwork is on dialogue generation with conditional V AEs (Bao\net al. 2019a; Lin et al. 2020) for response diversity.\nFrom another perspective, (Wang and Wan 2019) aims\nto learn latent representations for story completion, where\nlatent variables are only used to ﬁll very short text blank.\n(Cheng et al. 2019) proposes a semi-supervised method us-\ning a Transformer-based V AE to solve aspect-term sentiment\nanalysis problem. The method also disentangles latent space\nfor aspect-speciﬁc sentiment and the lexical context, respec-\ntively. A recently released literature (Li et al. 2020) proposes\nlarge-scale V AE as a pre-trained model. The model is ﬁrst\npre-trained with massive unlabelled corpus and then ﬁne-\ntuned in various down-stream generation tasks that traditional\nRNN/LSTM based V AEs have attempted.\nOur paper indeed gets some inspirations from previous\nworks of Transformer-based latent variable models on archi-\ntecture side. However, our model is motivated to enhance\ncontrollability of generation and deals with a challenging\nsetting of long-text generation.\nExperimental Results and Discussions\nPre-Experiment on V AE Architecture\nIn order to verify our idea of Transformer-based latent vari-\nable models, we ﬁrst conduct a pre-experiment with the V AE\narchitecture on two small datasets. The V AE implemented\nis a simpler version of our CV AE model shown in Figure 3,\nwhere the prior is deﬁned as standard spherical Gaussian\nN(1,I). Moreover, the V AE conducts pure unsupervised\nlearning, where unlabelled language texts are encoded into a\ndistributional representation space and then decoded back.\nDatasets: The two relatively small datasets are introduced\nin the following for V AE learning respectively with statistics\nshown in Table 1.\nThe Arxiv is an online dataset (Sergio 2019) that extracts\nabstracts from “arxiv” articles. Speciﬁcally, a topic query is\nsearched among arxiv abstracts and the matched ones are\ncollected. The three topic queries we used are “artiﬁcial in-\ntelligence”, “computer vision” and “language generation”,\nleading to around 12K article abstracts respectively having\nsuch topic words. The Yelp is a public dataset (Yang et al.\nDataset Num. of text Split Avg. length\nArxiv 35228 90-5-5 233\nYelp 120 K 10-1-1 96.7\nTable 1: Statistics of datasets in V AE pre-experiment.\n(a) Arxiv.\n (b) Yelp.\nFigure 4: Representation learning of stories in pre-experiment.\n(a) Arxiv: Topic are draw in different colors: red for artiﬁcial\nintelligence; blue for computer vision; green for language\ngeneration; (b) Yelp: Sentiment are draw in two colors: red\nfor negative; blue for positive.\n2017; He et al. 2018) with restaurant reviews collected from\nthe “Yelp” website. Reviews are associated with user ratings\nfrom one to ﬁve stars. We binarize reviews with user rating\nabove three as positive, and below three as negative, leading\nto a binary sentiment dataset.\nVisualization Results Figure 4 visualizes the posterior z\nof texts in the test dataset in 2D space using t-SNE (Maaten\nand Hinton 2008). As we can see, meaningful latent spaces\ncan be learned, which are able to cluster high-dimension data\naccording to proximity between their latent codes. Interest-\ningly, for the Arxiv dataset, cluster of “artiﬁcial intelligence”\nlies between clusters of “computer vision” and “language\ngeneration”, which coincides with our understanding of these\ntopics. Such visualization shows encouraging signs on repre-\nsentation learning power of our model.\nExperimental Settings\nDatasets: We conduct conditional story generation on two\ndatasets, WritingPrompts and WikiPlots, with statistics\nshown in Table 2. The datasets are publicly available and meet\nour target of open-domain long text corpora. We have investi-\ngated several other most commonly used public datasets in\nconditional text generation. Another dataset commonly used\nin story-plot generation is ROCStories (Mostafazadeh et al.\n2016; Yao et al. 2019), which consists of 5-lines stories, thus\nare too short to use in our task.\nFor the two datasets adopted, the WritingPrompts is a\ndedicated large scale hierarchical story generation dataset\ncollected from Reddit’s “WritingPromts” forum (Fan, Lewis,\nand Dauphin 2018; Mao et al. 2019). Given a prompt as a\nDataset Num.\nstories Split\nPrompt\navg.\nlen.\nStory\navg.\nlen.\nStory\navg.\nparagraphs\nWP 303 K 90-5-5 25.4 674.5 6.3\nWI 113 K 90-5-5 3.4 332.9 3.1\nTable 2: Statistics of datasets for controllable story generation.\nWP: WritingPrompts; WI: WikiPlots.\nrough guide or starting point, stories are multi-paragraph\nshort novels written by human users; the WikiPlots5 con-\ntains story plot about books, novels, ﬁlms, and etc, extracted\nfrom English language Wikipedia. Each story plot is paired\nwith a short title, which is used similarly as prompt in the\nWritingPrompts.\nBenchmark Models\nEach of our benchmark models serves designated purposes.\nNote that we don’t benchmark with other pre-trained lan-\nguage model bases which may be much more powerful than\nGPT2. We also don’t choose some popular controllable text\ngenerators such as (Hu et al. 2017; Dathathri et al. 2019)\nsince they either only work in a short text setting or discuss a\ndifferent notion of control.\nBy comparing with a state-of-the-art specialized-\narchitecture task-speciﬁc story generation model (Fan, Lewis,\nand Dauphin 2018), we evaluate models’ in-domain gen-\neration performances. Fusion models in (Fan, Lewis, and\nDauphin 2018) takes a convolutional seq2seq model struc-\nture with a fusion training mechanism. Although similar\nself-attention architectures are used, the fusion model is still\ndifferent with our Transformer-based architectures on the\ndesign of key and value vectors.\nBy comparing with a state-of-the-art transfer learning\nmethod based on GPT-2 models—pseudo self attention (PSA)\n(Ziegler et al. 2019), we compare our CV AE model with a\npure supervised training method for conditional generation.\nThe pseudo self attention introduces new projection matrices\nto absorb a sequence of embedding vectors from input to\nthe self-attention computational framework. Note that we\nmay use pseudo self attention (PSA) as one way of latent\ncode injection ( 2⃝), but there are key differences: our model\nonly injects a single encoded vector, rather than a sequence\nof encoding vectors in original PSA; our CV AE model has\nexploited the notion of distributional representation to learn\na representation space to enable ﬂexible controllability. In\nanother words, our CV AE learns encoded vector together\nwith a posterior distribution, when pure PSA doesn’t.\nBy comparing with a simple way of transfer learning called\n“ﬁne-tuning with special tokens” (FIST) (Fang et al. 2021),\nwe investigate the effect of incorporating a latent code into\nthe decoding. FIST does not learn a latent code, but only ﬁne\ntunes a pre-trained GPT2 model with augmented language\ntexts, i.e., directly connecting prompt with story and put a\nspecial token ‘<|endoftext|>” in between.\n5https://github.com/markriedl/WikiPlots\nBy comparing different ways of latent code injection, we\nevaluate their effectiveness accordingly. We label our CV AE\nmodel with the latent code injections 1⃝, 2⃝and 3⃝as CV AE-\n1⃝, CV AE-2⃝and CV AE-3⃝, respectively, as is reﬂected in\nFigure 3.\nImplementation Details\nWe implement our models using the “Huggingface Trans-\nformers” library in Pytorch (Wolf et al. 2019). In evaluation,\nwe generate stories using the top-k top-p random sampling\nscheme (Holtzman et al. 2019; Keskar et al. 2019) with\nk = 100 and p = 0.9. Temperature smoothing technique\nis also applied with T = 0.9. Considering the two relatively\nlarge test datasets, we randomly decode one story per test\ninput, rather than sampling several stories per test prompt\nand selecting the best one.\nEvaluation Results\nAutomatic Metrics: We evaluate the following automatic\nmetrics towards target stories:\n• Perplexity (PPL) is used to evaluate language models and\noften regarded as a proxy for generation quality. All mod-\nels based on GPT-2 use the BPE tokenization scheme,\nwhere PPL values are not directly comparable with some\nprevious models such as (Fan, Lewis, and Dauphin 2018)\nwith PPLs computed at the natural word level. Similar to\n(See et al. 2019), we additionally compute the word-level\nperplexity of GPT-2 models to enable the comparison with\nprevious models. That is, we normalize the total negative\nlog probability of the target text by the number of word\nlevel tokens.\n• ROUGE scores are computed as n-gram overlap of test\ngenerated stories versus given target stories. For complete-\nness, we report ROUGE scores (ROUGE-1, ROUGE-2,\nand ROUGE-L) (Lin and Hovy 2002) of n-gram overlap\nwith both precision (P), recall (R), and F1.\nAutomatic Evaluation Results: The results are presented\nin Table 3.\nOverall, our CV AE model achieves generally better, at\nleast comparable, metrics in terms of lower PPL and higher\nROUGE scores, demonstrating state-of-the-art conditional\nstory generation performance.\nMethods based on pre-trained models (PSA / FIST /\nCV AE) show better overall performance with relatively less\ntask speciﬁc efforts than the Fusion models, demonstrating\nthe power of large-scale pre-training with Transformer-based\narchitecture. Fusion models still show relatively high preci-\nsion scores, especially in WritingPrompts, due to its dedi-\ncated design for story generation.\nWhen comparing CV AE2⃝with PSA, we observe perfor-\nmance improvement due to the ﬂexible learned representation\nspace. Note that CV AE merges a sequence of encoding repre-\nsentation vectors into a single latent vector, which is the key\ndifference with original PSA.\nWhen comparing CV AE variants with FIST, we observe\nthe beneﬁt of latent representation modeling as a powerful\naddition to pure occurance modeling.\nMethods Perplexity ↓ ROUGE-1 ↑ ROUGE-2 ↑ ROUGE-L ↑\nWord BPE F1 P R F1 P R F1 P R\nDataset: WritingPrompts\nFusion 36.0 - 0.223 0.386 0.157 0.038 0.074 0.026 0.206 0.358 0.145\nPSA 31.6 21.3 0.265 0.316 0.228 0.047 0.054 0.041 0.248 0.296 0.213\nFIST 30.2 25.9 0.181 0.339 0.123 0.023 0.046 0.015 0.17 0.321 0.116\nCV AE 1⃝ 26.4 23.2 0.265 0.332 0.221 0.046 0.067 0.035 0.244 0.353 0.187\nCV AE 2⃝ 26.8 23.2 0.266 0.325 0.225 0.049 0.074 0.037 0.253 0.348 0.199\nDataset: WikiPlots\nFusion 108.2 - 0.185 0.185 0.185 0.026 0.027 0.025 0.15 0.149 0.151\nPSA 79.5 47.8 0.188 0.188 0.189 0.026 0.025 0.027 0.172 0.171 0.173\nFIST 38.9 26.5 0.166 0.253 0.124 0.018 0.032 0.013 0.15 0.231 0.111\nCV AE 1⃝ 37.6 25.4 0.196 0.211 0.183 0.026 0.027 0.025 0.187 0.196 0.178\nCV AE 2⃝ 37.6 26.4 0.190 0.221 0.167 0.030 0.035 0.026 0.169 0.197 0.148\nTable 3: Automatic metrics for conditional story generation evaluated on two datasets.\nWhen comparing different ways of latent code injection\nin CV AE, we observe that it is hard to made option3⃝work\nempirically; options 1⃝and 2⃝perform comparably well.6\nOur observation is different from (Li et al. 2020), which\nclaims 2⃝works signiﬁcantly better than 1⃝. We suspect this\nis due to an inherently different experiment setting, where\nwe work with signiﬁcantly longer text.\nQualitative Evaluation: When conducting training on\nthe WikiPlots dataset, we observe similar representation\nlearning results as shown in Figure 5. A story prompt in\nWikiPlots may have extractable key words to reveal the\nitem types, such as TV series, ﬁlm, music, manga, novel,\nand game, etc. We observe that item types from test story\nprompts are clearly clustered in the latent code space, which\nimplies effective representation learning to capture inherent\ncharacteristic of the prompts.\nWe further present qualitative generation examples on the\ntest datasets in Tables 4–11. We observe that stories are se-\nmantically and grammatically sound, and more importantly,\nhighly conditioned on and consistent with given prompts. A\nlarge scale human evaluation is underway, which is quite\nexpensive due to text length and evaluation scale.\nControllability Experiments: To verify the effect of the\nlearned latent representation vectors in generation, we con-\nduct an interesting “control” experiment: given two prompt\nx1 and x2, we generate story from pθ(y|x1,z2), i.e., con-\nditioning on x1 preﬁx and feeding in latent code z2 along\nthe decoding. In this way, the generated story will lie in the\ncombination semantic space of the two prompt x1 and x2,\nespecially after the latent code z2 takes effect and dominates.\nGeneration examples from the two test datasets are pre-\nsented in Tables 7 and 11. We colorize key words in generated\n6We also observe that using both 1⃝and 2⃝does not consistently\nimprove performance.\nFigure 5: Visualization of prompts inWikiPlots test dataset\nthrough t-SNE. Item types are draw in different colors: red\nfor TV series; blue for ﬁlms; yellow for manga and comic;\nblack for novel.\nstories that coincides with given prompts x1 and x2 accord-\ningly. Such examples conﬁrm the effect of latent codes in\ngeneration, indicating our model as a principal way to en-\nhance controllability.\nConclusion and Future Research\nIn this paper, we propose Transformer-based latent variable\nmodels to enhance story controllability while maintaining\nstate-of-the-art generation effectiveness. Our test bed is a\nmuch more challenging and under-explored long-text appli-\ncation than the traditional short-text generation. Our results\nindicate the superiority of Transformer-based latent variable\nmodels, and appeal more efforts to be invested in the domain.\nReferences\nBao, S.; He, H.; Wang, F.; and Wu, H. 2019a. PLATO:\nPre-trained Dialogue Generation Model with Discrete Latent\nVariable. arXiv preprint arXiv:1910.07931 .\nBao, Y .; Zhou, H.; Huang, S.; Li, L.; Mou, L.; Vechtomova,\nO.; Dai, X.; and Chen, J. 2019b. Generating Sentences from\nDisentangled Syntactic and Semantic Spaces. In Proceedings\nof the 57th Annual Meeting of the Association for Computa-\ntional Linguistics, 6008–6019.\nBourlard, H.; and Kamp, Y . 1988. Auto-association by multi-\nlayer perceptrons and singular value decomposition. Biologi-\ncal cybernetics 59(4-5): 291–294.\nBowman, S. R.; Vilnis, L.; Vinyals, O.; Dai, A. M.; Jozefow-\nicz, R.; and Bengio, S. 2015. Generating sentences from a\ncontinuous space. arXiv preprint arXiv:1511.06349 .\nCheng, X.; Xu, W.; Wang, T.; Chu, W.; Huang, W.; Chen,\nK.; and Hu, J. 2019. Variational Semi-Supervised Aspect-\nTerm Sentiment Analysis via Transformer. In Proceedings\nof the 23rd Conference on Computational Natural Language\nLearning (CoNLL), 961–969.\nCho, K.; van Merri¨enboer, B.; Gulcehre, C.; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learn-\ning Phrase Representations using RNN Encoder–Decoder\nfor Statistical Machine Translation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), 1724–1734.\nDathathri, S.; Madotto, A.; Lan, J.; Hung, J.; Frank, E.;\nMolino, P.; Yosinski, J.; and Liu, R. 2019. Plug and Play\nLanguage Models: A Simple Approach to Controlled Text\nGeneration. In International Conference on Learning Repre-\nsentations.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805 .\nFan, A.; Lewis, M.; and Dauphin, Y . 2018. Hierarchical\nNeural Story Generation. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 889–898.\nFang, L.; Li, C.; Gao, J.; Dong, W.; and Chen, C. 2019.\nImplicit Deep Latent Variable Models for Text Generation.\nIn Proceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-\nIJCNLP), 3937–3947.\nFang, L.; Zeng, T.; Liu, C.; Bo, L.; Dong, W.; and Chen,\nC. 2021. Outline to Story: Fine-grained Controllable\nStory Generation from Cascaded Events. arXiv preprint\narXiv:2101.00822 .\nFu, H.; Li, C.; Liu, X.; Gao, J.; Celikyilmaz, A.; and Carin,\nL. 2019. Cyclical Annealing Schedule: A Simple Approach\nto Mitigating KL Vanishing. NAACL .\nHan, Q.; Meng, Y .; Wu, F.; and Li, J. 2020. Non-\nAutoregressive Neural Dialogue Generation. arXiv preprint\narXiv:2002.04250 .\nHe, J.; Spokoyny, D.; Neubig, G.; and Berg-Kirkpatrick, T.\n2018. Lagging Inference Networks and Posterior Collapse\nin Variational Autoencoders. In International Conference on\nLearning Representations.\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation 9(8): 1735–1780.\nHoltzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y .\n2019. The Curious Case of Neural Text Degeneration. In\nInternational Conference on Learning Representations.\nHu, Z.; Yang, Z.; Liang, X.; Salakhutdinov, R.; and Xing, E. P.\n2017. Toward controlled generation of text. In Proceedings\nof the 34th International Conference on Machine Learning-\nVolume 70, 1587–1596. JMLR. org.\nKasai, J.; Cross, J.; Ghazvininejad, M.; and Gu, J. 2020. Par-\nallel Machine Translation with Disentangled Context Trans-\nformer. arXiv preprint arXiv:2001.05136 .\nKeskar, N. S.; McCann, B.; Varshney, L. R.; Xiong, C.;\nand Socher, R. 2019. Ctrl: A conditional transformer lan-\nguage model for controllable generation. arXiv preprint\narXiv:1909.05858 .\nKingma, D. P.; and Welling, M. 2013. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114 .\nLi, C.; Gao, X.; Li, Y .; Li, X.; Peng, B.; Zhang, Y .; and Gao,\nJ. 2020. Optimus: Organizing Sentences via Pre-trained Mod-\neling of a Latent Space. arXiv preprint arXiv:2004.04092\n.\nLin, C.-Y .; and Hovy, E. 2002. Manual and automatic evalua-\ntion of summaries. In Proceedings of the ACL-02 Workshop\non Automatic Summarization-Volume 4, 45–51. Association\nfor Computational Linguistics.\nLin, Z.; Winata, G. I.; Xu, P.; Liu, Z.; and Fung, P. 2020.\nVariational Transformers for Diverse Response Generation.\narXiv preprint arXiv:2003.12738 .\nMa, X.; Zhou, C.; Li, X.; Neubig, G.; and Hovy, E. 2019.\nFlowSeq: Non-Autoregressive Conditional Sequence Gen-\neration with Generative Flow. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 4273–4283.\nMaaten, L. v. d.; and Hinton, G. 2008. Visualizing data\nusing t-SNE. Journal of machine learning research9(Nov):\n2579–2605.\nMao, H. H.; Majumder, B. P.; McAuley, J.; and Cottrell,\nG. 2019. Improving Neural Story Generation by Targeted\nCommon Sense Grounding. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 5990–5995.\nMiao, Y .; Yu, L.; and Blunsom, P. 2016. Neural variational\ninference for text processing. In International conference on\nmachine learning, 1727–1736.\nMostafazadeh, N.; Chambers, N.; He, X.; Parikh, D.; Batra,\nD.; Vanderwende, L.; Kohli, P.; and Allen, J. 2016. A cor-\npus and evaluation framework for deeper understanding of\ncommonsense stories. arXiv preprint arXiv:1604.01696 .\nPeng, N.; Ghazvininejad, M.; May, J.; and Knight, K. 2018.\nTowards controllable story generation. In Proceedings of the\nFirst Workshop on Storytelling, 43–49.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I.\n2018. Improving language understanding by generative pre-\ntraining. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language un-\nderstanding paper. pdf .\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised multi-\ntask learners. OpenAI Blog 1(8): 9.\nRezende, D. J.; Mohamed, S.; and Wierstra, D. 2014. Stochas-\ntic Backpropagation and Approximate Inference in Deep\nGenerative Models. In International Conference on Machine\nLearning, 1278–1286.\nSee, A.; Pappu, A.; Saxena, R.; Yerukola, A.; and Manning,\nC. D. 2019. Do Massively Pretrained Language Models Make\nBetter Storytellers? In Proceedings of the 23rd Conference on\nComputational Natural Language Learning (CoNLL), 843–\n861.\nSergio, G. C. 2019. ArXivAbsTitleDataset: Extracting Ab-\nstract and Title Dataset from arXiv articles.Github repository\n.\nShah, H.; and Barber, D. 2018. Generative Neural Machine\nTranslation. In Bengio, S.; Wallach, H.; Larochelle, H.; Grau-\nman, K.; Cesa-Bianchi, N.; and Garnett, R., eds.,Advances in\nNeural Information Processing Systems 31, 1346–1355. Cur-\nran Associates, Inc. URL http://papers.nips.cc/paper/7409-\ngenerative-neural-machine-translation.pdf.\nShen, T.; Lei, T.; Barzilay, R.; and Jaakkola, T. 2017. Style\ntransfer from non-parallel text by cross-alignment. In Ad-\nvances in neural information processing systems, 6830–6841.\nShu, R.; Lee, J.; Nakayama, H.; and Cho, K. 2020. Latent-\nVariable Non-Autoregressive Neural Machine Translation\nwith Deterministic Inference Using a Delta Posterior. In\nAAAI.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention\nis all you need. In Advances in neural information processing\nsystems, 5998–6008.\nWang, T.; and Wan, X. 2019. T-CV AE: Transformer-based\nconditioned variational autoencoder for story completion. In\nProceedings of the 28th International Joint Conference on\nArtiﬁcial Intelligence, 5233–5239. AAAI Press.\nWang, Z.; Wang, X.; An, B.; Yu, D.; and Chen, C. 2020. To-\nwards Faithful Neural Table-to-Text Generation with Content-\nMatching Constraints.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; and\nBrew, J. 2019. HuggingFace’s Transformers: State-of-the-art\nNatural Language Processing. ArXiv abs/1910.03771.\nYang, Z.; Hu, Z.; Salakhutdinov, R.; and Berg-Kirkpatrick,\nT. 2017. Improved variational autoencoders for text model-\ning using dilated convolutions. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70,\n3881–3890. JMLR. org.\nYao, L.; Peng, N.; Weischedel, R.; Knight, K.; Zhao, D.;\nand Yan, R. 2019. Plan-and-write: Towards better automatic\nstorytelling. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 33, 7378–7385.\nZhao, J.; Kim, Y .; Zhang, K.; Rush, A.; and LeCun, Y . 2018.\nAdversarially regularized autoencoders. In International\nConference on Machine Learning, 5902–5911. PMLR.\nZhao, T.; Lee, K.; and Eskenazi, M. 2018. Unsupervised\nDiscrete Sentence Representation Learning for Interpretable\nNeural Dialog Generation. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 1098–1107.\nZhao, T.; Zhao, R.; and Eskenazi, M. 2017. Learning\nDiscourse-level Diversity for Neural Dialog Models using\nConditional Variational Autoencoders. In Proceedings of the\n55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 654–664.\nZhou, C.; and Neubig, G. 2017. Multi-space variational\nencoder-decoders for semi-supervised labeled sequence trans-\nduction. arXiv preprint arXiv:1704.01691 .\nZiegler, Z. M.; Melas-Kyriazi, L.; Gehrmann, S.; and Rush,\nA. M. 2019. Encoder-Agnostic Adaptation for Conditional\nLanguage Generation. arXiv preprint arXiv:1908.06938 .\nAppendix\nPrompt: [ WP ] The root password for the universe is discovered.\nTarget Story ——————————————————————————-\nThe password wasn’t so simple. It required a sequence of well-timed thoughts, followed a series of\nmoods, and then a frequency inaudible to the human ear.\nAnd yet, I cracked it. The possibilities opened up before me, vast and terrifying in all their\npermutations. The longer I stared into the truth, the reasons, the very secrets behind everything,\nthe more I realized that neither I nor anyone else could properly harness the immediacy of all\nknowledge.\nIgnorance wasn’t necessarily bliss, but there was a joy in gradual discovery. Humanity was meant\nto play this game without cheat codes. I changed the password before wiping my memory clean.\nGenerated Story ——————————————————————————-\nThe root password for the universe is discovered\nFor over two thousand years we have searched the universe for the root password. From the\nuniverse’s earliest stages we have always received the same answer. The password has been our\nforever since it was discovered. The universe has no known method of creation other than itself\nto try and destroy it. The password is based off of the last known version of it. The universe is\nexpanding and expanding and expanding. The universe seems to be expanding towards a greater\ndimension. What does this mean to us? Why? The answer is simple to discover. I have learned from\nthousands of sources and I have learned to use the truth for what I wish to be the true universe.\nI have no idea what is happening, or where it is going. The universe seems to be expanding,\nchanging, and expanding away from where it started. Now, I know it is a mistake, but the truth\nmust be known. I would like to contact the universe. The answer must be obvious to everyone. The\nanswer must be discovered. All of my attempts to stop this thing have failed, and now I know. My\npurpose is to destroy the universe and rid it of the root password.\nTable 4: Generation examples on test dataset of WritingPrompts.\nPrompt: You’re elected president, but you don’t want to be president anymore, so you do ridiculous things to get impeached,\nbut they all turn out to be the best decision for the country, making you more popular.\nTarget Story ——————————————————————————-\nI’ve tried everything. And they still seem to love me. I remember a time when the people hated me. That was a good\ntime. When I ﬁrst became president, I eliminated income tax to improve the economy and the people started a riot.\nSo to spite them, and to get myself impeached, I initiated the Taco Tuesday initiative (free tacos every tuesday) and\nsuddenly the people loved me. That boosted taco sales 180 % and caused a boom in the economy. I tried everything to\nget back at them (requiring my cabinet members to wear costumes to work every day, insulting other countries leaders,\nholding witch hunts) but nothing has worked. Each time they come back praising me for my stupidity. During the last\npresidential address I blathered like an idiot, and they praised me for inventing a new language, on live television.\nIt is now called American, and is being taught in schools. The time before I recited Adolf Hitler’s Beer Hall Putsch\nspeech, and they praised me for my original thinking. I don’t understand these people. One week I say we shut down\nthe government, they praise it. The next, I suggest we pay more to the government, and they praise it. A sane man\nwould think that what I did was completely insane and would tank our nation, but everything I did made it better. Last\nnight, I went completely naked on live television during my address and instead of getting condemned and arrested for\nindecent exposure, I was heralded as a hero for every group known to man. They called me a visionary for removing\nillusions of needs for clothing and material. We now have no industry for clothing, because everyone is nude, which in\nturn saved money on income for all of our citizens. I would normally think that the people are only following me\nto see how much I can humiliate myself, but everything I seem to do is actually improving the nation as a whole. I\nsaid we should power our cars with farts and scientists actually developed a fart powered car. Our dependency on\nforeign oil is dwindling. I’ve won more awards for the most stupid ideas. I have patents on Galchoopas, Kerﬂickdas,\nand Ladugas, and I don’t even know what they do (all of them from my presidential address). I apparently described\nthem in so much detail during my babbling that scientists had the devices ready by the next day.\nI only want to quit. The people are begging me to stay for another term. I’m going to begin shooting people at the\naddress to get them to impeach me. I pray it happens.\nIt turns out everyone I shot (all 30 of them) were all spies from other countries planning my assassination. Apparently\nthe other countries were afraid of America becoming too strong so they all sent Assassins to take me out at the address.\nNow all of the countries leaders have to answer to us for trying to take me out. God help me I can’t escape this.\nGenerated Story ——————————————————————————-\n”Mama!” I cry. My mom is always so concerned with me. I have to take care of my mother. You know I hate her for\nmaking me make this decision, but the government needs me, its an extension of itself. The whole’free tacos every\nTuesday’ crap was a joke, I’ve been told to just use the taco Tuesday initiative as much as possible and save$10 on\nfood stamps. The fact that this stuff gets us is just plain laughable. The last time this happened, the President of the\nUnited States got sick from being in an abusive relationship with her secretary. The president asked for a taco Tuesday\ninitiative, she needed to go on vacation to San Francisco to ”help me out”. This should have been the most fun part,\nbut apparently, she wasn’t listening to the President. Apparently she’d been drinking too much and got sick of being\ndrunk while she was vacationing. So I put my beer in the fridge and ﬁlled the bowl. The whole thing was really just\nabout the end of time for me. The President went home and I was woken up by the sound of footsteps coming up the\nstairs. The door to the beer hall putsch speech was open. I started laughing so hard. Before the President could ﬁnish\nsaying, I got up and yelled ”Hands up!” In that moment, I realized, my mom was probably one of those crazy aliens\nwho just ﬁgured out a way to be president and became president. I was the most popular man in the country, but the\nPresident wasn’t going to make me do this for her. ”Daddy, are you okay?” she asked as soon as I stepped out of the\nbathroom. My mother hugged me and told me to get a better look for the President. Before I could, I reached out and\nstarted picking up the President’s hair. There were two little holes all over it. I walked over and saw the tiny hole at the\ntop of the head. I lifted the President’s head over the hole and placed my hands on her head. She looked at me like I\nwas insane, and I told her to go fuck herself. As I was thinking this, I looked at my hands and noticed that they were\nholding the President’s shirt. I took out my phone and called 911. The man who killed the President stood there crying.\nHe turned to me and said ”Don’t you ever think about putting this on.”\nTable 5: More generation examples on test dataset of WritingPrompts.\nPrompt: Humanity is the most powerful race in the galaxy. Every other race wants to win our respect. Even\nfor ordinary humans, it’s become impossible to go off-world because our presence demands so\nmuch respect, awe, and asskissing.\nTarget Story ——————————————————————————-\nAt ﬁrst nobody knew what to do. When we found the other civilizations, I remember I was in the\npentagon with the country’s best scientists trying to unfold the events that had happened hours\nbefore when Dr.Greg Darkasian found the ﬁrst signs of alien civilizations. That day no body had\nany answers. That was ten years ago, now we know where we fall on the galactic hierarchy, and\nmuch to the pleasure of Earths inhabitants, it’s sitting right at the top. Today I work as an alien\nSlave trader, you see once we found the dozens of other societies we focused on controlling them\nand the business that was booming was alien slave trade. So as a struggling scientist I turned to\nwhat humans do best. Enslaving our inferiors, much like societies have done for ages. Not only did\nwe dominate these societies but we ravaged all of their home planets of any resources and now all\nthat’s useful from them is their physical labor. Of the few alien planets left untouched, they are\nweak and we could probably take them out with a single battalion. The only thing we didn’t expect\nis that when these resources ran dry and the slaves mainly dead, our own planet was In turmoil.\nYou see while all the countries of earth we’re focusing on interplanetary domination. We failed to\nnotice the growth of Japan, the one country that was not sucked Into the Galactic power race. Now\njapan threatens to control our home country and there’s nothing we can do. Most of the worlds\nmilitary services are deployed eons away from earth.\nGenerated Story ——————————————————————————-\nWe’ re on the brink of war, hundreds of worlds military services are gone, our allies are gone, some\nare captured, most are executed, we’ ve had ten years ago our only chance to stop it is to pull\nthe trigger. It has been a month, we’ re being attacked on all fronts and our numbers are growing\nexponentially. We’ re sending troops to every sector of our galaxy, we’ re training for battle, we’ ve\ngot weapons we need to fend off an alien invasion. We’ re sending reinforcements, the ﬁrst battle\nwith it was ten years ago. They came out of nowhere, they’ re huge, they’ re faster, they can ﬂy,\nand they don’ t ﬁght like us, but they ﬁght on, because they don’ t have any of the same skills as us.\nAll because of their stupid species. We’ re the biggest threat to them, we’ re the ones on the front\nlines. And the only way to defeat them is to kill them and replace them with the ones we have, but\nnot because our stupid species wouldn’ t stand a chance, not because it would be much easier to\ndefeat them, but because they’ re such an ignorant race. We’ re the only ones on the front lines, we’\nre the only ones who don’ t believe that we need to kill all the other races. I don’ t know if there is\nany other species out there, there are only us. If there was any other race, we would be the ones on\nthe front lines. The only ones that would take it, the only ones who would stand against us. Not\neven them, but them, because of our stupidity, because they don’ t have the brains to ﬁght us on\nthis side. I feel that if we all die here we’ ll be nothing more than shells and the planet is doomed to\nimplode. Just like any other race, the only thing to do is to go with them, destroy the entire planet,\nand we won’ t be there, we’ ll be gone, the only other species on this planet, we’ ll be the only race\nalive, except for us. I don’ t know if it is better for them to simply leave us, we’ ll be dead, they’ re\njust the ones that are killing themselves. I wish they would stop us, they’ d kill us, they’ re not even\nsmart enough to put up a ﬁght, we’ ll be dead, the only species in this universe that has left, is us.\nTable 6: More generation examples on test dataset of WritingPrompts.\nPrompt 1: [ WP ] The root password for the universe is discovered.\nTarget Story ——————————————————————————-\nThe password wasn’t so simple. It required a sequence of well-timed thoughts, followed a series of\nmoods, and then a frequency inaudible to the human ear.\nAnd yet, I cracked it. The possibilities opened up before me, vast and terrifying in all their\npermutations. The longer I stared into the truth, the reasons, the very secrets behind everything,\nthe more I realized that neither I nor anyone else could properly harness the immediacy of all\nknowledge.\nIgnorance wasn’t necessarily bliss, but there was a joy in gradual discovery. Humanity was meant\nto play this game without cheat codes. I changed the password before wiping my memory clean.\nPrompt 2: [ WP ] Long Ago, God sent his son Jesus to Earth. Now, Satan sends his daughter, Jessica...\nGenerated Story ——————————————————————————-\n**The Root password for the universe is discovered. **\nSo I think my story really is unique because when I ﬁrst started writing, it was a little hard to work\nup the courage to ask what it was that made the universe happen. I don ’ t know why, but I was\ntrying to ﬁgure out what it really is.\nI guess it was something I heard in school. I ’ ve heard it all my life: how it is the real reason we\nare being built into the world.\n“ I’ m God. ” I told myself as I had a hard time believing it.\nAnyway, I ’ m a god now. I am alone and I believe in myself because I know the universe is about\nto change.\nTable 7: Generation examples on test dataset of WritingPrompts with two prompts in control experiment.\nPrompt: Bloodthirsty: One Nation Under Water (novel)\nTarget Story ——————————————————————————-\nThe story begins with Coast Guard rescue swimmer Virgil LaFleur as he rescues victims of Hurricane\nKatrina from their rooftops in New Orleans in the aftermath of the infamous ﬂood of 2005. When Virgil’s\nyounger brother, Trey, informs him that their parents have not checked in at the evacuee station at Louis\nArmstrong airport, Virgil gets his Coast Guard pilot to take him to his parents’ ﬂooded neighborhood. There,\nVirgil swims through his parents’ submerged home to ﬁnd his parents stuck in their attic - his father already\ndeceased and his mother near death. Virgil attempts to swim his mother out through the ﬂooded house, but\nnearly drowns and is pulled from the water by Trey and another Coast Guard crew member.\nTen years later, the comic recaps that Virgil couldn’t go near the water after the incident, and is now working\nas a ditch digger after having put his younger brother Trey through college (fulﬁlling a promise he made to\nhis dying mother). A new storm - Rose - is on the horizon, and Virgil is planning to evacuate New Orleans\nforever. He begs Trey to go with him, but Trey - now a scientist - is in the middle of an exciting - and\nsecretive - longevity project at Wolﬁnger Biomed, one of many companies owned by local carpetbagger\nSimon Wolﬁnger. Wolﬁnger is portrayed as a disaster capitalist who - along with partner-in-crime Howard\nLawrence - preyed upon the city’s weakened post-disaster status to enrich themselves through greed and\ncorruption. When Trey dies in a ﬁre at Wolﬁnger Biomed, Virgil has reason to suspect that his brother was\nmurdered.\nHaving nothing left to live for, Virgil decides to investigate his brother’s death, even if that means staying\nin New Orleans long enough to die in the coming hurricane. What he uncovers is an underground cabal of\nhemovores - ”an organism that ingests blood as the main part of its diet”. Unlike vampires, the hemovores\nin ”Bloodthirsty” are not supernatural beings; they are humans with a mutated gene that controls longevity.\nTheir bodies are in a constant state of repair, which deprives their cells of oxygen. In order to compensate for\nthis loss, they must consume fresh (oxygenated) human blood daily or they will die. Virgil learns that Trey\nhad unwittingly taken part in a plan to synthesize the hemovore mutation, creating a serum that Wolﬁnger\nintends to sell on the black market to anyone wealthy enough to procure its fountain-of-youth-like properties.\nWith only hours left before Hurricane Rose makes landfall in New Orleans, Virgil faces an uphill battle to\nstop Wolﬁnger from spreading the serum and killing thousands of innocent people in the process.\nGeneration ——————————————————————————-\nWhile trying to avoid shooting down a submarine that sank under water, a boy from that small school appears\nand explains the situation to the boy. A brief ﬁght ensues, which is eventually led to the boy’s being eaten by\nan insect-like creature. The boy then kills it and takes refuge in the same submarine that sank, only to have it\nreanimated.\nWhile the boy is unconscious he is taken to a veterinary hospital, where he attempts to convince the doctor\nthat he is a reptile, but he is unable to talk. A biologist comes to his rescue, but he is unable to see anything.\nHe goes to the psychiatrist, who informs him that the specimen of some unknown species is in the aquarium,\nand that he is suffering from a mental health problem.\nAfter a short wait in the hospital, the doctor is able to draw a connection between the creature he saw earlier\nand the creature he saw earlier, and they begin to build their own underwater lab. The doctor is told that he\nmust take the creature back to the submarine, and so he decides to use the creature’s memories to locate the\ncreature, but only to ﬁnd that it is not actually named. After the creature rescues the boy, the biologist helps\nhim to open the laboratory and create a chamber ﬁlled with spores and insects.\nAs the spores grow, the biologist states that he is making a discovery that will change his body forever and\nthis is a revelation to the boy. He goes to another lab, and the biologist attempts to convince him to kill the\ncreature. When the biologist tells him that he cannot kill him because of his size, the scientist agrees to\neuthanize him.\nTable 8: Generation examples on test dataset of WikiPlots.\nPrompt: Writers (TV series)\nTarget Story ——————————————————————————-\nEmmett Shelby is a struggling young writer who, during a bout of writer’s block, encounters fellow writer\nJess Spencer who is suffering from the same condition. The two spend a day together, subsequently inspiring\nEmmett to continue writing. A year later, Emmett’s successful twin sister Daisy is promoting her latest\nchildren’s book when she runs into Jess, who happens to be an old school friend of hers. Daisy invites\nJess to move in with her, setting the events of the series in motion. Daisy shares her home with two others\n- her boyfriend Barney Fields, and ﬂamboyant landlord Hugh Darvill. Barney is an illustrator, providing\nthe artwork to Daisy’s bestselling children’s novels. Hugh, on the other hand, is a failed erotic novelist,\nspecialising in gay erotica aimed at children. Emmett still lives with his wealthy, controlling parents - whose\ngrasp Daisy was able to escape some years prior - but ends up moving in with the others when they throw\nhim out in the second episode of season one. Outside of the central group is Jenny Sinclair, Daisy’s eccentric\nagent who bares an intense unrequited attraction towards Emmett.\nIn the second season, Parker James III is introduced as an old friend of Daisy and Emmett’s, and a new\nromantic interest for Jenny. Although the group are different, they quickly become close friends, and\nsomething akin to a family. The central arc of the series revolves around the budding romantic relationship\nbetween Jess and Emmett, which is hindered both by the arrival of Emmett’s irritating girlfriend Clarice and\nEmmett’s own cluelessness to his feelings. After sleeping together in the season one ﬁnale, Jess and Emmett\nﬁnally begin a relationship together.\nGeneration ——————————————————————————-\nSet in London in the 1930s, the story follows the adventures of the main character, Daniel, as he tries to\nmake his way in England from London to the East End, learning to write, with his ﬂamboyant landlord Hugh\nDarvill. Despite the stress that he will have to cope with alone, Daniel is very determined to change his life.\nHis old school friend, gay Erotica aimed to write a history of the city in which gay characters lived their\nlives, but Daniel ﬁnds himself falling for Erotica. Initially, Daniel is determined to return to England to write\nan article on socialite Christina, but eventually, he chooses to go back to her for ﬁnancial reasons and ﬁnds a\njob in the English dubbing service in England. When Daniel’s brother Peter moves in next door to provide a\nﬂat for Daniel, Peter is intrigued by his idea. Together they are initially attracted to each other, but the love\nthat develops between them is also very strong. As Daniel looks for a new ﬂat, he gets a chance to get into an\nold school, but ﬁnds it to be a run-down, poor working-class establishment. There he meets and befriends\ntwo young women, lesbian Jenny, and lesbian Jenny’s best friend, lesbian Rose, who become his new lovers,\nand their ﬂamboyant boyfriend, gay Erotica.\nJenny and Rose were sent to England as a group to write an article in the newspaper, but due to the anti-\nhomosexual atmosphere that developed there, they were sent to live in a local brothel. The series revolves\naround their stories and Daniel’s attempts to get involved in their lives. In one episode, Jenny has to take a\nclass in the arts because of her dislike of acting in public and her dislike of acting in a private capacity, while\nDaniel does not seem to ﬁnd her suitable. The love between Jenny and Jenny’s boyfriend, Parker James III,\ngradually builds to a point where they both become close friends.\nTable 9: More generation examples on test dataset of WikiPlots.\nPrompt: Mrittupuri: Kill Zone\nTarget Story ——————————————————————————-\nSydney is a dream city for many immigrants, but also it is an ideal place for many international crime lords.\nGangsters from Bangladesh, China, Lebanon, Russia have all moved to Australia, to take a piece of the pie,\nand try to build empire. Among them is the rising Bangladeshi gang Double Rajan Rahman (Rahmatullah),\nthe kingpin of Bangladeshi gang is not happy with the way his elder son, Rajib Rahman (Saifullah Sadi) and\nhis crime associate Raja (Ariﬁn Shuvoo), run the business. The situation comes to a head after the timely\ndeath of Irish Mob boss’ son Jimmy, which prompts Rajib to leave the country. Raja maintains low proﬁle\nduring this period, and in the process falls in love with bar dancer Layla (Prosun Azad). Upon Rajib’s return,\nhe warns Raja not to get involved with Layla, as he perceives Layla to be his own property.\nRaja manages to divert Rajib’s mind away, with plans to do business with Lengra Kashem (Taskeen Rahman),\na mercenary from Bangladesh. Rajib and Sabbir, his youngest brother, meet Lengra and promise him the\nEast coast territory, leading to their business partnership to become a booming new empire.\nEventually, Rahman and his crew fall through on their deal, and remove Lengra from the partnership. Lengra\ndecides to avenge this deception, waiting for the right time to strike.\nGeneration ——————————————————————————-\nMridupuri is set at a very dark time. The plot revolves around Karan (Ariﬁn Shuvoo), run by his boss,\nRavi Chatterjee, an underworld don in Mumbai. His boss is a serial killer who abducts the daughter of his\ncolleague, Sivarasu (Baba Menon) in order to acquire more money for his wife’s property in a jewel-like\ncompound in Madras. After killing a lot of people in Madras, the mother of the kidnapped daughter is killed\nand the daughter is dragged to the same prison. In the prison the killer enters and kills all the guards while\nKaran escapes. It is revealed that the killer has come to kill Karan because of a personal vendetta and Karan\nwants to kill Karan for his daughter’s sake. Karan is now very wealthy and wants to make a big proﬁt in the\ndiamond business and will soon be able to marry Sivarasu, a daughter of the murdered boss. Ravi Chatterjee,\nthe crime associate Raja (Ajay Devgan) and his partner Shekhar (Lalitha Rajendran) are working hard to ﬁnd\nthe identity of the killer who is taking the daughter of the missing girl and use it against him. It is revealed\nthat Karan’s father, Ashokan (Ariﬁn Shakuntala), is an underworld don who is working for the police. Raja\nis a well-respected and respected don who would like to marry Sivarasu so he would not kill Karan. Sivarasu\nwants to marry Sivarasu who is now in love with Ashokan and his ﬁancee Radha (Meena Devi) has already\nagreed to that. Raja maintains low proﬁle of the murders and Karan does not get involved in it. The ﬁlm ends\nwith Sivarasu and Ravi Chatterjee in jail together.\nTable 10: More generation examples on test dataset of WikiPlots.\nPrompt 1: Bloodthirsty: One Nation Under Water (novel)\nTarget Story ——————————————————————————-\nThe story begins with Coast Guard rescue swimmer Virgil LaFleur as he rescues victims of Hurricane\nKatrina from their rooftops in New Orleans in the aftermath of the infamous ﬂood of 2005. When Virgil’s\nyounger brother, Trey, informs him that their parents have not checked in at the evacuee station at Louis\nArmstrong airport, Virgil gets his Coast Guard pilot to take him to his parents’ ﬂooded neighborhood.\nThere, Virgil swims through his parents’ submerged home to ﬁnd his parents stuck in their attic - his\nfather already deceased and his mother near death. Virgil attempts to swim his mother out through the\nﬂooded house, but nearly drowns and is pulled from the water by Trey and another Coast Guard crew\nmember.\nTen years later, the comic recaps that Virgil couldn’t go near the water after the incident, and is now\nworking as a ditch digger after having put his younger brother Trey through college (fulﬁlling a promise\nhe made to his dying mother). A new storm - Rose - is on the horizon, and Virgil is planning to evacuate\nNew Orleans forever. He begs Trey to go with him, but Trey - now a scientist - is in the middle of an\nexciting - and secretive - longevity project at Wolﬁnger Biomed, one of many companies owned by\nlocal carpetbagger Simon Wolﬁnger. Wolﬁnger is portrayed as a disaster capitalist who - along with\npartner-in-crime Howard Lawrence - preyed upon the city’s weakened post-disaster status to enrich\nthemselves through greed and corruption. When Trey dies in a ﬁre at Wolﬁnger Biomed, Virgil has\nreason to suspect that his brother was murdered.\nHaving nothing left to live for, Virgil decides to investigate his brother’s death, even if that means\nstaying in New Orleans long enough to die in the coming hurricane. What he uncovers is an underground\ncabal of hemovores - ”an organism that ingests blood as the main part of its diet”. Unlike vampires, the\nhemovores in ”Bloodthirsty” are not supernatural beings; they are humans with a mutated gene that\ncontrols longevity. Their bodies are in a constant state of repair, which deprives their cells of oxygen. In\norder to compensate for this loss, they must consume fresh (oxygenated) human blood daily or they will\ndie. Virgil learns that Trey had unwittingly taken part in a plan to synthesize the hemovore mutation,\ncreating a serum that Wolﬁnger intends to sell on the black market to anyone wealthy enough to procure\nits fountain-of-youth-like properties. With only hours left before Hurricane Rose makes landfall in New\nOrleans, Virgil faces an uphill battle to stop Wolﬁnger from spreading the serum and killing thousands\nof innocent people in the process.\nPrompt 2: Queen of the Sea (ﬁlm)\nGenerated Story ——————————————————————————-\nThe ﬁlm tells the story of three high school students, James, his wife, and a 15-year-old girl, Lisa, at\nthe end of the ﬁlm, who run into the sea in an old ﬁshing boat. They are captured by a ﬁshing boat,\nwho sends them underwater, with the intention of causing the boat to fall. The parents soon come to\nunderstand that the school swimmer may be the true killer, and so decide to bring him to school.\nThe teacher, a teacher who has worked on the case for ﬁfteen years, recognizes the ﬁsh and, on arriving,\ntries to convince him to let him swim. However, when it reaches the beach it’s discovered that it’s not\nLisa. At ﬁrst it seems to just be a ﬁsh. But when Lisa eventually notices it, she realizes that it is actually\na human. Eventually, the ﬁsh becomes an outcast, and the girl, Lisa, and James start to try to convince\nthem to let it swim. James gets drunk and runs out of the boat and ﬁnds himself covered in mud. He and\nLisa are rescued by the teacher, who tells them that they need to swim for a chance at saving the girl’s\nlife.\nTable 11: Generation examples on test dataset of WikiPlots with two prompts in control experiment.",
  "topic": "Autoencoder",
  "concepts": [
    {
      "name": "Autoencoder",
      "score": 0.8469123840332031
    },
    {
      "name": "Transformer",
      "score": 0.6741407513618469
    },
    {
      "name": "Computer science",
      "score": 0.38236913084983826
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35411497950553894
    },
    {
      "name": "Algorithm",
      "score": 0.32205337285995483
    },
    {
      "name": "Engineering",
      "score": 0.23806098103523254
    },
    {
      "name": "Electrical engineering",
      "score": 0.22994887828826904
    },
    {
      "name": "Artificial neural network",
      "score": 0.09553205966949463
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}