{
  "title": "Don’t Stop Fine-Tuning: On Training Regimes for Few-Shot Cross-Lingual Transfer with Multilingual Language Models",
  "url": "https://openalex.org/W4385567090",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2970086456",
      "name": "Fabian David Schmidt",
      "affiliations": [
        "German Research Centre for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A1969142033",
      "name": "Ivan Vulić",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4207383812",
      "name": "Goran Glavaš",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3214173179",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3207937903",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W3154311556",
    "https://openalex.org/W3206907172",
    "https://openalex.org/W3100069613",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W4287854451",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3214578205",
    "https://openalex.org/W3101516616",
    "https://openalex.org/W4280512622",
    "https://openalex.org/W3173185981",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3103147437",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W4280647381",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3106539628",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2995118574",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W4281563856",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3169425228"
  ],
  "abstract": "A large body of recent work highlights the fallacies of zero-shot cross-lingual transfer (ZS-XLT) with large multilingual language models. Namely, their performance varies substantially for different target languages and is the weakest where needed the most: for low-resource languages distant to the source language. One remedy is few-shot transfer (FS-XLT), where leveraging only a few task-annotated instances in the target language(s) may yield sizable performance gains. However, FS-XLT also succumbs to large variation, as models easily overfit to the small datasets. In this work, we present a systematic study focused on a spectrum of FS-XLT fine-tuning regimes, analyzing key properties such as effectiveness, (in)stability, and modularity. We conduct extensive experiments on both higher-level (NLI, paraphrasing) and lower-level tasks (NER, POS), presenting new FS-XLT strategies that yield both improved and more stable FS-XLT across the board. Our findings challenge established FS-XLT methods: e.g., we propose to replace sequential fine-tuning with joint fine-tuning on source and target language instances, offering consistent gains with different number of shots (including resource-rich scenarios). We also show that further gains can be achieved with multi-stage FS-XLT training in which joint multilingual fine-tuning precedes the bilingual source-target specialization.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10725–10742\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nDon’t Stop Fine-Tuning: On Training Regimes for Few-Shot Cross-Lingual\nTransfer with Multilingual Language Models\nFabian David Schmidt1, Ivan Vuli´c2,3, Goran Glavaš1\n1 Center For Artificial Intelligence and Data Science, University of Würzburg, Germany\n2 Language Technology Lab, University of Cambridge, UK 3 PolyAI Ltd., UK\n{fabian.schmidt, goran.glavas}@uni-wuerzburg.de\niv250@cam.ac.uk\nAbstract\nA large body of recent work highlights the fal-\nlacies of zero-shot cross-lingual transfer ( ZS-\nXLT) with large multilingual language models.\nNamely, their performance varies substantially\nfor different target languages and is the weakest\nwhere needed the most: for low-resource lan-\nguages distant to the source language. One rem-\nedy is few-shot transfer(FS-XLT), where lever-\naging only a few task-annotated instances in\nthe target language(s) may yield sizable perfor-\nmance gains. However, FS-XLT also succumbs\nto large variation, as models easily overfit to the\nsmall datasets. In this work, we present a sys-\ntematic study focused on a spectrum of FS-XLT\nfine-tuning regimes, analyzing key properties\nsuch as effectiveness, (in)stability, and modular-\nity. We conduct extensive experiments on both\nhigher-level (NLI, paraphrasing) and lower-\nlevel tasks (NER, POS), presenting newFS-XLT\nstrategies that yield both improved and more\nstable FS-XLT across the board. Our findings\nchallenge established FS-XLT methods: e.g.,\nwe propose to replace sequential fine-tuning\nwith joint fine-tuning on source and target lan-\nguage instances, offering consistent gains with\ndifferent number of shots (including resource-\nrich scenarios). We also show that further gains\ncan be achieved with multi-stage FS-XLT train-\ning in which joint multilingual fine-tuning pre-\ncedes the bilingual source-target specialization.\n1 Introduction and Motivation\nSuccessful fine-tuning of mainstream pre-trained\nlanguage models (Devlin et al., 2019; Liu et al.,\n2019; Conneau et al., 2020) for various NLP tasks\nrequires a sizeable set of labeled task-specific in-\nstances. While such abundant task data are avail-\nable for many tasks in English and a few high-\nresource languages, annotated examples are much\nscarcer for low-resource languages (Joshi et al.,\n2020). A large body of recent work thus focused\non zero-shot cross-lingual transfer ( ZS-XLT), for\nwhich no labeled instances are available in the tar-\nget language (Pires et al., 2019; Cao et al., 2020).\nCatalyzed by pretrained massively multilingual\ntransformers (MMT) such as mBERT (Devlin et al.,\n2019), XLM-R (Conneau et al., 2020), or mT5\n(Xue et al., 2021), ZS-XLT has achieved impressive\nresults on a wide variety of tasks (Hu et al., 2020;\nRuder et al., 2021). The MMT-drivenZS-XLT, how-\never, exhibits dramatic performance drops when\ntransferring to low-resource languages and/or lan-\nguages distant from the source language (Lauscher\net al., 2020; Ebrahimi et al., 2021; Adelani et al.,\n2021, inter alia). In contrast, recent work high-\nlights that language models are excellent few-shot\nlearners (Brown et al., 2020; Gao et al., 2021): they\nadapt well to new tasks or languages when exposed\nto only on a handful of labeled instances.\nFor cross-lingual transfer in particular, sequen-\ntial few-shot transfer (FS-XLT) – in which large(r)-\nscale fine-tuning in the source language is followed\nby the secondary fine-tuning on a few target lan-\nguage instances – has been rendered particularly\neffective, with massive performance gains reported\nfor some tasks with as little as 10 target language\ninstances (Lauscher et al., 2020; Zhao et al., 2021).\nHowever, the effectiveness of sequential FS-XLT\ncrucially depends on the shot selection (Zhao et al.,\n2021). Even more concerning, as we show in §3, is\nthe sensitivity of FS-XLT to hyperparameter values,\nmost notably the duration (number of epochs) of\nfew-shot target language training: such fluctuations\nare problematic for true few-shot learning (Perez\net al., 2021), where target language validation data,\nto be leveraged for model selection, does not exist.\nContributions. In this work, we shed new light\non FS-XLT and seek to remedy the above pitfalls\nof current FS-XLT method. We depart from the\nestablished sequential FS-XLT paradigm and pro-\npose new training regimes for FS-XLT, comparing\nthem across the dimensions of effectiveness, stabil-\nity, and modularity. Concretely, we propose train-\ning regimes that jointly exploit source and target\n10725\nlanguage instances, and allow to model their inter-\naction. 1) We demonstrate, both for higher-level\nsemantic tasks (e.g., NLI) and lower-level token-\nlevel tasks (NER, POS tagging), that joint source\nand target language training ‘feeds two birds with\none scone’: (i) it consistently improves FS-XLT\nperformance, even in setups with a larger number\nof target-language shots (e.g., N = 500), and (ii)\nmakes the training procedure much more stable\nand robust, allowing for a reliable selection of the\nmodel checkpoint in true few-shot transfer setups\nwithout a target-language validation set. 2) We find\nthat preceding the joint bilingual fine-tuning with a\nmultilingual training step, in which we combine the\nshots from multiple target languages, brings further\nperformance gains. We also show that such multi-\nstage training regime improves the computational\nefficiency in multilingual FS-XLT setups, i.e., when\nthe model transfer to multiple target languages is\nrequired. 3) Finally, we validate that benefits of\nthe new FS-XLT training regimes are not limited to\nEnglish as the source language. Our work funda-\nmentally challenges the status quo in FS-XLT and\nintroduces and compares training paradigms that\nenable more effective, more efficient, and much\nmore robust few-shot cross-lingual transfer.\nConcurrent (closely related) effort. The con-\ncurrent work of Xu and Murray (2022) similarly\ndemonstrates the utility of joint multilingual FS-\nXLT: although their joint fine-tuning approach dif-\nfers from ours – they employ gradient surgery (Yu\net al., 2020), an approach that harmonizes compet-\ning gradients originating from instances of different\nlanguages in a training batch – it yields the same\ntwo main benefits: (1) improved target language\nperformance and (2) more stable training that facil-\nitates models selection (i.e., alleviates the need for\ntarget-language validation data).\n2 Background and Related Work\nMMTs like mBERT and XLM(-R) (Lample and\nConneau, 2019; Conneau et al., 2020) have be-\ncome the main vehicles of cross-lingual transfer.\nPretrained on multilingual corpora covering 100+\nlanguages, MMTs conceptually enable zero-shot\ncross-lingual transfer (ZS-XLT) between any two\nlanguages seen in pretraining (Hu et al., 2020) or\neven to unseen languages (Ansell et al., 2021). The\n(extent of) ZS-XLT success depends on the quality\nand alignment of the representation subspaces of\nindividual languages (Cao et al., 2020; Hu et al.,\n2021; Wu and Dredze, 2020). Accordingly, ZS-\nXLT with MMTs tends to be ineffective in transfers\nto target languages that are (i) linguistically dis-\ntant from the source language and especially those\n(ii) un(der)represented in MMT’s pretraining (Hed-\nderich et al., 2020; Lauscher et al., 2020; Ruder\net al., 2021; Ebrahimi et al., 2021).\nOne line of work boosts ZS-XLT by improv-\ning semantic alignment between the representa-\ntion subspaces of individual languages, exploit-\ning to this end available word or sentence trans-\nlations (Hu et al., 2021; Wu and Dredze, 2020;\nYang et al., 2022). Another, complementary line\nof work improves ZS-XLT through increasing the\nMMT’s capacity for individual languages (Pfeiffer\net al., 2020, 2022; Ansell et al., 2021, 2022). It at-\ntempts to remedy for the “curse of multilinguality”\n(Conneau et al., 2020) – an effect where, for a fixed\nmodel capacity, the quality of representations of\nindividual languages at some point starts degrading\nwith the addition of more languages.\nUnlike the above efforts, which improve the\nMMTs’ representation space in a task-agnostic\nfashion, FS-XLT assumes a handful of labeled task-\nspecific examples in the target language(s) (Hed-\nderich et al., 2020; Lauscher et al., 2020; Zhao\net al., 2021). Lauscher et al. (2020) propose sequen-\ntial FS-XLT: fine-tuning on few target-language in-\nstances follows the initial fine-tuning on sizable\nsource language data. They show that FS-XLT\nbrings the largest gains exactly where ZS-XLT fails\nthe most: for target languages distant from the\nsource and underrepresented in pretraining. In\nfollow-up work, Zhao et al. (2021) demonstrate\nthat FS-XLT is highly sensitive to the choice of\nshots. Both studies show the effectiveness of few-\nshot transfer to be subject to nature of the task:\nlower-level syntactic and token-level tasks (e.g.,\nPOS-tagging, NER) benefit much more from few\nannotated target language instances than higher-\nlevel semantic tasks (e.g., NLI).\nThe evaluation protocols of both Lauscher et al.\n(2020) and Zhao et al. (2021), however, do not\nreflect a true few-shot setup: they assume that sub-\nstantial validation data in the target language exists\nand utilize it to guide model selection (hyperpa-\nrameter optimization and early stopping). As such,\nthese works overestimate the effectiveness of true\nFS-XLT: while focused only on monolingual se-\ntups, Perez et al. (2021) demonstrate that model\nselection criteria based on training data alone yield\n10726\nFigure 1: FS-XLT to AmericasNLI and WikiANN with\n{10,50,100}shots after training on English data (cf.\n§4). The line plots the mean (incl. ±1σ) test set spread\n(in %) of best validation and current checkpoint. Runs\nacross 3 seeds by language are grouped by colored dots\nthat mark epochs scoring best on validation sets.\nconsistently worse few-shot task performance than\nmodel selection based on an extra validation set.\nIn this work, we rethink FS-XLT and propose\nnovel FS-XLT paradigms that jointly leverage both\n(sizable) source and (few-shot) target language data\nin multi-task fashion or via mix-up (Zhang et al.,\n2018), and demonstrate their effectiveness as well\nas robustness in realistic (i.e., true) FS-XLT setups.\n3 Methodology\nIssues with Current FS-XLT Methods. Fig-\nure 1 illustrates the main issues of current FS-\nXLT techniques, adopting the established sequen-\ntial approach (Lauscher et al., 2020; Zhao et al.,\n2021; Üstün et al., 2022). In this experiment, we\nadapt models fine-tuned on sizable English task-\nspecific data with {10,50,100}target-language\nshots to AmericasNLI (Ebrahimi et al., 2021) and\nWikiANN (NER) (Rahimi et al., 2019) (see §4).\nWe execute three FS-XLT runs for each target lan-\nguage with different randomly selected shots and\nexamine the test performance over time, displaying\nthe mean and deviation (±1σ) across all languages\nand runs for different training duration (i.e., for\n{1, . . . , 50} epochs of target language training).\nThe gray horizontal line denotes the optimal per-\nformance (average across all languages and runs)\nin the presence of a target language validation set\n(i.e., ‘not-true’ few-shot learning): for each run,\nwe select the checkpoint that yields the best vali-\ndation performance. Individual runs are denoted\nwith colored dots, each color indicating one target\nlanguage. Each dot is vertically aligned with the\nepoch/checkpoint of the respective run (x-axis) that\nyields the best validation performance.\nThe figure reveals the instability of sequential\nFS-XLT. 1) The optimal epoch/checkpoint varies\nacross all dimensions of analysis: number of shots,\ntasks, and languages. Besides the expected result\nthat, on average, with more shots we benefit from\nlonger training,1 no discernible pattern emerges. 2)\nThe optimal training duration substantially varies\neven across different runs of the same language,\nthat is, for different random selections of N shots\n(and even for larger number of shots, N = 500,\ncf. Figure 2 later in §5.1). These observations ren-\nder sequential FS-XLT highly unreliable for thetrue\nFS-XLT setups without target validation data.\nNew FS-XLT Training Methods. Motivated by\nthese empirical insights, we explore new FS-XLT\nparadigms, aiming to increase robustness and ef-\nfectiveness in true FS-XLT setups. Our hypothesis\nis that combining abundant source-language task\nexamples with scarce target examples in a joint\nfashion will 1) prevent the models to overfit to\nsource-language features (see Figure 1), 2) also\nprevent overfitting to an (extremely) small set of\ntarget-language shots (Zhao et al., 2021), and 3)\nresult in the models that are better calibrated for a\nparticular source-target transfer direction. The FS-\nXLT methods should model the interaction between\nsource and target examples, rather than performing\nsource-language fine-tuning which is fully agnostic\nof the target language (and vice versa).\nThe first approach, dubbed ‘macro-averaging\nFS-XLT’ (MACRO ), conducts bilingual or mul-\ntilingual fine-tuning in a joint (i.e., multi-task)\nsetup. In particular, we compute the total loss\nL= δLS + (1−δ)LT as a weighted sum of LS\nand LT , where LS and LT are monolingual losses\nassociated with the examples from the source lan-\nguage S and the target language T, respectively.\nδis a standard interpolation hyper-parameter that\nadjusts the relative weight between the two losses.\nThe two individual losses operate over the ded-\nicated mini-batches BS = {xs\ni ,ys\ni }i=1,...,N and\nBT = {xt\nj,tt\nj}j=1,...,M , which are sampled from\nthe respective source and target language datasets\n1With as little as 10 shots, longer training, intuitively,\nleads to overfitting. Figure 1 proves this for AmericasNLI\nand WikiANN, showing that the first checkpoint yields the\nbest performance for most runs (i.e., the majority of dots are\ngrouped most to the left of the plot).\n10727\nDS and DT . N and M in combination determine\nthe size |B|of the entire mini-batch, as well as the\nrelative share of samples for each language within\nthe mini-batch. The generalization of the bilingual\nMACRO FS -XLT method (MACRO -BI) to its multi-\nlingual variant (MACRO -MULTI ) is straightforward:\neach multilingual batch Bwould simply comprise\nexamples from more than 2 languages, and the joint\nloss will span more than 2 language-specific losses.\nThe second paradigm is based on the standard\nmix-up technique (Zhang et al., 2018). It has been\nproven beneficial for improving task performance\nand robustness in monolingual tasks; here, we ex-\ntend it to the cross-lingual FS-XLT scenario. This\nmethod, termed MIX -UP, linearly interpolates be-\ntween pairs of annotated instances from the source\nand the target language as follows:\n˜xs,t = λxs\ni ∗(1 −λ)xt\nj; ˜ ys,t = λys\ni ∗(1 −λ)yt\nj\nλ∼Beta(α) weighs the contribution between\ninstances (xs\ni ,ys\ni ) and (xt\nj,yt\nj). Each instance\n(xb,yb) ∈Bcan be paired with any other instance\nwith varying λ. We opt to randomly pair instances\nin BS and BT to be ‘mixed’, and keep α con-\nstant. The fine-tuning loss Lis then computed via\nsoft cross-entropy: ∑|B|/2\nb ˜yb log ˜yb. Cross-lingual\nMIX -UP can be interpreted as ‘soft’ code switch-\ning, occurring in the latent representation space: it\nshould enhance FS-XLT by further tying, in a task-\nspecific fashion, the representation subspaces of\nthe two languages, as the model is trained for the\ntask on ‘mixed’ representations, rather than inde-\npendent language-specific distributions (Cao et al.,\n2020; Yang et al., 2022).\nOverview of FS-XLT Training Methods. Besides\nintroducing novel methods, the main goal of this\nwork is a comprehensive empirical comparative\nstudy of differentFS-XLT training methods/regimes.\nFor clarity, we provide a quick overview of the wide\nspectrum of evaluated regimes and configurations.\nFirst, models may be trained on target language\nshots after training on the source language data.\nThis approach, termed TARGET , is the standard\nsequential FS-XLT from prior work (Lauscher et al.,\n2020; Zhao et al., 2021). 2 The alternative is the\nregime that combines source-language and target-\nlanguage data instances, termed SOURCE -TARGET ,\nwhich comes in two different flavors: our proposed\n2A variant that bypasses source-language fine-tuning and\noperates only on the few target shots yields massive and con-\nsistent drops (Zhao et al., 2021); we thus do not include this\nvariant in our evaluations.\njoint MACRO and MIX -UP paradigms. The sec-\nond axis of difference is the starting point of TAR-\nGET or SOURCE -TARGET FS -XLT: we can start\nfine-tuning from 1) the original PLM (termed LM\nhenceforth), or 2) from the final/last checkpoint of\nsource-language task fine-tuning (termed LAST ),\nor 3) the ORACLE checkpoint. ORACLE violates\nthe true FS-XLT: it refers to the model checkpoint\nthat achieves the best performance on the target\nlanguage validation set, measured after each epoch\nof source language training (Keung et al., 2020).\nWe include ORACLE for analysis purposes.\n4 Experimental Setup\nTasks and Languages. Following prior studies\nfocused on FS-XLT (Lauscher et al., 2020; Zhao\net al., 2021), we evaluate all the methods in a rep-\nresentative set of tasks that require varying degrees\nof semantic and syntactic understanding for suc-\ncessful cross-lingual transfer.\nNatural Language Inference(NLI). NLI experi-\nments are conducted on AmericasNLI (AmNLI)\n(Ebrahimi et al., 2021): it encompasses indigenous\ntarget languages from the Americas, with data care-\nfully translated from the Spanish XNLI dataset\n(Conneau et al., 2018).3 Unless stated otherwise,\nthe source is English, and we transfer to the follow-\ning 7 target languages with sizable NLI data avail-\nable: Aymara (AYM), Bribri (BZD ), Guarani (GN),\nQuechua (QU), Raramuri (TAR), Shipibo-Konibo\n(SHP ), Wixarika (HCH ). For NLI, we jointly embed\nthe hypothesis-premise sentence-pair, obtain the\n[CLS] token and feed it into the classifier.\nParaphrasing. The paraphrasing task is conducted\non the PAWS-X dataset (Yang et al., 2019), span-\nning parallel evaluation data for 6 high-resource\nlanguages: German ( DE), Spanish ( ES), French\n(FR), Korean (KO), Japanese (JA), and Chinese (ZH).\nWe train classifiers in the same fashion as classifiers\nfor NLI, now only with paraphrase pairs.\nNamed Entity Recognition (NER). We use the\nWikiANN dataset of Pan et al. (2017), and eval-\nuate cross-lingual transfer between English and the\nfollowing 13 languages: Arabic ( AR), Afrikaans\n(AF), German (DE), Japanese (JA), Quechuan (QU),\nRussian (RU), Kinyarwanda ( RW), Swahili ( SW),\nTamil (TA), Urdu (UR), Vietnamese (UR), Yoruba\n3ZS-XLT typically fails in transfer to these languages, as\nthey are unseen during MMT pretraining and are typologically\nvery distant from English.\n10728\n(YO), Mandarin ( ZH). For NER, we feed output\nrepresentations of each token into the classifier.\nPart-Of-Speech Tagging(POS). We use the POS\ntags of the UD treebanks (Zeman et al., 2020) and\ntransfer from English to the following 12 target lan-\nguages: Afrikaans (AF), Arabic (AR), Basque (EU),\nChinese (ZH), German (DE), Hindi (HI), Hungarian\n(HU), Indonesian (ID), Japanese (JA), Russian (RU),\nTamil (TA), Urdu (UR). The model architecture is\nidentical to NER experiments.\nData Sampling and Shots. For AmNLI and\nPAWS-X, we subsample training and valida-\ntion subsets from the provided validation splits. 4\nWikiANN and the Universal Dependencies tree-\nbank comprise sufficiently large training and vali-\ndation splits; we subsample shots from the training\ndata. We follow Lauscher et al. (2020) and train\nmodels with k ∈ {10,50,100,250,500}target-\nlanguage shots, fixed by task and language.5\nTraining Details. The main MMT is the base\nvariant of XLM-R from the transformers library\n(Wolf et al., 2020) with mixed precision. For all\ntasks, we train models with AdamW (Loshchilov\nand Hutter, 2019) with the learning rate fixed to\n2e−5 and weight decay of 0.05. All models apply\n10% dropout to the output representations prior to\nthe classification layer at training time. The maxi-\nmal input sequence length is set to 256 subwords\nfor AmNLI and PAWS-X, and 512 for NER and\nPOS.6 ZS-XLT and SOURCE -TARGET variants are\ntrained for 10 epochs with the linear warm-up rate\nof 0.1 and linear decay. 7 We fine-tune TARGET\nregimes for 50 epochs with a constant learning rate.\nWe train in mini-batches of size 32: the SOURCE -\nTARGET regimes balance instances from source and\ntarget languages – for MACRO -BI, we sample 16 in-\nstances per language choose the language-balanced\nloss (δ = 0.5); MIX -UP interpolates between 32\npairs of instances between the languages, resulting\nwith 32 ‘mixed’ bilingual examples. For MIX -UP,\n4This is also why we evaluate AmNLI on the subset of 7\nlanguages which come with enough validation instances.\n5Unlike Zhao et al. (2021), we operate in a more general\nunconstrained setup, and do not guarantee an equal number of\nshots per each class in a task.\n6As a sanity check, we verified that our ZS-XLT implemen-\ntation scores comparably to other ZS-XLT work with similar\nhyperparameters (Wu and Dredze, 2020; Hu et al., 2021).\n7Note that for SOURCE -TARGET setups the source lan-\nguage datasets dictate training times, as target language shots\nare continously resampled. SOURCE -TARGET for AmNLI is\ntrained for 5 epochs to reduce computational overhead due to\nthe large size of English MNLI (Williams et al., 2018).\nwe keep αfixed to 0.4.8 We run all experiments\nover three (fixed) random seeds. Further details on\nreproducibility are provided in Appendix A.1.\nEvaluation Details. We measure performance with\naccuracy on AmNLI and PAWS-X. For WikiANN\nand POS, we report the token-level F1 score. We\nreport both performance of final/last (L) and oracle\n(O) checkpoints to provide appropriate bounds on\nexpected and ideal transfer performance.9\n5 Results and Discussion\nThe main results are listed in Table 1. Full results\nper individual target languages in each task are\navailable in the Appendix. First, we corroborate\nthe findings from prior work (Lauscher et al., 2020;\nZhao et al., 2021), and report considerable gains\nwith FS-XLT over ZS-XLT across the board and\nwith different FS-XLT methods. We now dissect the\nresults across multiple axes of comparison.\nJoint versus Sequential FS-XLT. In general,\nthe joint (i.e., SOURCE -TARGET ) FS-XLT variants\nscore on-par or outperform the sequential (i.e.,TAR-\nGET ) variants, and the gains are observed both at\nLast and Oracle checkpoints. Moreover, we note\nthat the scores taken at the L checkpoint with the\njoint variants across all setups are typically higher\nthan the scores taken at the O checkpoint. This\nrenders them more suitable for true FS-XLT sce-\nnarios, and clearly suggests that the proposed joint\napproaches remedy the issues with overfitting and\nallow for a more stable fine-tuning. We attribute\nthis finding exactly to bilingual regularization and\ntransfer calibration (see §3).\nJoint Methods: MACRO versus MIX -UP. The two\njoint methods typically yield very similar perfor-\nmance when all other components are kept equal,\nand fine-tuning starts from theLAST or the ORACLE\ncheckpoint. MIX -UP data augmentation insignifi-\ncantly affects performance. The effect is most ap-\nparent when comparing SOURCE -TARGET setups\non the higher-level semantic tasks (AmNLI and\nPAWS-X), where the model must learn to embed\nsentence-pair semantics in the [CLS] token. To\nthis end, both tasks require initial source-language\nfine-tuning as the LM variants lag substantially be-\nhind LAST and ORACLE which rely on the initial\n8We did not observe significant differences in results with\nα∈ {0.1,0.4,0.7,1.0} in preliminary experiments.\n9Prior work typically reported only the O performance\nwhich, depending on the target language and downstream task,\ncan heavily overestimate true FS-XLT performance.\n10729\nSOURCE TARGET SOURCE-TARGET\nZero-Shot Few-Shot MACRO MIX-UP\nShots LM LAST ORACLE LM LAST ORACLE LM LAST ORACLE\nL O L O L O L O L O L O L O L O L O\nAmNLI\n10 39.6 40.0 38.3 39.9 38.4 41.2 34.9 36.0 38.0 38.1 37.4 38.6 35.1 35.4 37.9 39.4 37.2 38.3\n50 39.6 40.0 43.8 43.3 44.0 43.6 40.6 42.5 44.4 44.4 44.4 45.0 39.8 40.6 44.0 44.5 44.8 45.0\n100 39.6 40.0 45.8 45.0 46.3 46.2 44.1 44.9 46.8 46.6 47.9 47.5 43.8 44.3 47.4 47.0 47.7 47.7\n250 39.6 40.0 49.7 49.5 49.8 49.4 48.4 49.2 51.0 51.2 51.4 51.0 48.4 49.0 51.5 50.6 51.7 51.3\n500 39.6 40.0 51.7 52.0 52.0 51.2 51.8 52.5 53.3 52.9 53.8 53.4 52.3 51.6 53.2 53.2 53.1 53.1\nPA WS-X\n10 83.8 84.0 81.0 84.2 80.0 84.4 81.1 81.8 84.5 84.5 84.7 84.6 77.3 80.6 84.0 84.1 83.8 84.2\n50 83.8 84.0 83.5 84.2 83.4 84.4 79.9 81.2 84.4 84.3 84.6 84.5 74.4 76.6 84.6 84.4 84.7 84.3\n100 83.8 84.0 84.0 84.3 83.5 84.3 79.9 80.2 84.6 84.5 84.6 84.4 75.2 77.8 84.6 84.4 84.7 84.7\n250 83.8 84.0 83.2 84.9 83.2 84.4 81.2 81.8 84.6 84.6 84.9 84.8 78.4 79.2 84.5 84.5 84.5 84.3\n500 83.8 84.0 83.8 85.3 83.6 85.0 82.8 82.9 85.3 85.0 85.5 85.3 81.9 81.9 85.2 85.0 85.1 85.0\nNER\n10 52.5 60.0 60.7 63.3 61.0 64.3 63.9 65.1 64.9 65.8 64.9 66.2 64.2 65.1 63.9 65.1 64.3 65.6\n50 52.5 60.0 72.0 72.3 72.6 73.1 72.8 73.5 73.1 73.1 73.1 73.6 73.2 73.6 72.9 73.4 73.2 73.5\n100 52.5 60.0 73.6 74.5 74.4 74.7 75.5 75.7 75.4 75.5 75.3 75.2 75.8 75.8 74.9 75.4 75.4 75.5\n250 52.5 60.0 75.6 76.5 76.0 76.7 77.1 77.3 77.0 77.1 76.9 77.1 77.4 77.4 76.9 76.9 77.0 77.1\n500 52.5 60.0 77.4 78.6 77.6 78.7 79.2 79.3 79.0 79.0 79.2 79.2 79.5 79.5 78.9 78.9 79.0 79.0\nPOS\n10 62.6 63.8 79.9 79.9 80.2 80.2 80.5 80.6 79.9 80.0 80.1 80.2 80.0 80.2 79.9 80.0 80.1 80.2\n50 62.6 63.8 84.9 84.7 85.1 85.1 85.4 85.4 85.1 85.2 85.3 85.3 85.4 85.3 85.3 85.3 85.5 85.5\n100 62.6 63.8 86.6 86.6 86.7 86.9 87.3 87.3 87.1 87.1 87.2 87.2 87.1 87.2 87.1 87.1 87.2 87.2\n250 62.6 63.8 88.7 88.7 88.8 88.9 89.3 89.2 89.1 89.1 89.2 89.2 89.2 89.2 89.1 89.1 89.2 89.2\n500 62.6 63.8 90.1 90.2 90.2 90.2 90.5 90.4 90.4 90.4 90.5 90.5 90.4 90.4 90.4 90.4 90.5 90.5\nTable 1: Benchmarking a spectrum of FS-XLT regimes (see §3). The results are averages over three random seeds,\naggregated over all target languages represented in each task (see §4) Training and evaluation data are identical\nacross all regimes in the evaluation. L (O) denote performance measured at last (oracle) checkpoint, see §4.\nsource fine-tuning. MIXUP -LM is most beneficial\nfor the token-level NER task, but does not yield\nsizeable gains on average over the arguably con-\nceptually simpler MACRO paradigm.\nStarting Point of FS-XLT. Expectedly, starting FS-\nXLT from the ORACLE checkpoint typically yields\nbetter performance than starting from the LAST\ncheckpoint. ORACLE , however, violates the as-\nsumption of a true FS-XLT setup: it uses the val-\nidation set in the target language to select a bet-\nter checkpoint for additional FS-XLT fine-tuning,\nwhich is organically better-aligned with the target\nlanguage. We note that the gap in performance\nbetween these two initializations slightly decreases\nin case of joint SOURCE -TARGET FS -XLT variants:\nthis again points to improved robustness compared\nto sequential FS-XLT.\nPerformance over Languages and Tasks. Per-\nformance benefits with different FS-XLT regimes,\nnaturally, depend on the task and target languages\nat hand. AmNLI starts profiting from FS-XLT only\nwith k≥50 shots. The target languages in AmNLI\nare extremely low-resource and unseen in MMT\npretraining: the model thus must see more target-\nlanguage data points than, e.g., in NLI transfer to\nhigher-resource languages from the XNLI bench-\nmark (Lauscher et al., 2020). Our new SOURCE -\nTARGET variants again substantially outperform\ncurrently established FS-XLT methods, and we ob-\nserve increasing returns with more shots. In con-\ntrast, performance on PAWS-X – which comprises\nonly high-resource languages (see §4) – primarily\nbenefits from the more robust joint FS-XLT regimes\nrather than from the increased number of shots.\nFor NER and POS, we observe strong performance\nalso with the LM initialization. We speculate that\nthis is because class-conditional token represen-\ntations align well with the representations from\nthe original MMT pretraining; on the other hand,\nthe models for NLI and paraphrasing must cap-\nture higher-level sentence semantics (via source-\nlanguage fine-tuning) before the FS-XLT step.\n5.1 Further Analyses\nWe base our further analyses and comparisons be-\ntween sequential and joint approaches on the fol-\nlowing two representative variants: TARGET -LAST\nand MACRO -LAST . They operate in the ‘real-life’\ntrue FS-XLT scenarios without any validation data\nto guide few-shot learning (Perez et al., 2021).\nStability of Transfer. Figure 2 compares stabil-\nity of the two variants for {10,50,500}shots (cf,\nAppendix A.2). It demonstrates that joint training\nsubstantially reduces instability and variance of FS-\nXLT fine-tuning across the board: we observe its\nincreased robustness and stability across different\ntasks, languages, and the numbers of shots. The\nplots also illustrate that the joint regime in the true\nFS-XLT setup offers performance which is com-\npetitive and comes substantially closer to perfor-\nmance achieved when exploiting target-language\nvalidation set: this directly indicates that, with joint\nbilingual fine-tuning ( MACRO ) in place, any ad-\nditional labeled instances in the target language\n10730\nFigure 2: FS-XLT regimes (joint MACRO versus sequential TARGET ) starting from the LAST checkpoint of the initial\nsource language fine-tuning step. The colored dots group runs for each seed by language and mark the checkpoints\nthat transfer best to target-language validation data. The line plots the mean (incl. ±1σ) test set spread (in %) of\nbest validation and current checkpoint.\nwould be better “spent” if used for training than for\nvalidation. Relying on the joint MACRO variant, the\nbest-performing checkpoints generally shift closer\nto the end of the training, which is a desired be-\nhavior in the absence of the validation set. In other\nwords, the joint FS-XLT variants not only improve\nbut also consistently makeFS-XLT fine-tuning more\nstable and more predictable, that is, less prone to\nlanguage- and task-dependent variations.\nNotes on Efficiency and Modularity. While the\njoint FS-XLT regimes improve final transfer perfor-\nmance, they are less modular by design and might\nincur larger computational costs than the sequential\nregimes. Namely, they require combining source-\nlanguage and target-language instances for each\nindividual source-target transfer direction, which is\nnot the case in the sequential regimes. In what fol-\nlows, we thus delve deeper into studying efficiency-\nand modularity-related research questions.\nJoint Multilingual and Multilingual-Bilingual\nMACRO . Given NT target languages, instead of\nfine-tuning NT separate bilingual models (MACRO -\nBI), we can, similar to Xu and Murray (2022), train\na single joint multilingual model (MACRO -MULTI ,\nsee §3) which serves all NT at once. Such FS-XLT\nvariant, besides potentially reducing computational\nand memory costs, might also profit from increased\ntask data provided in multiple languages (Ansell\net al., 2021). What is more, we can use the LAST\ncheckpoint of the MACRO -MULTI as the starting\npoint of the additional subsequent bilingualFS-XLT\nspecialization (i.e., MACRO -BI). We denote this\nnovel modular variant, where both steps are based\non the joint FS-XLT paradigm, as MULTI ) BI.\nFurthermore, we conduct another experiment,\nagain focused on efficiency of joint FS-XLT fine-\ntuning, which includes all the different MACRO\nvariants: (i) the original MACRO -BI, (ii) MACRO -\nMULTI , and (iii) MACRO -MULTI )BI. The goal is\nto investigate how the different joint paradigms\nperform under different computational budget con-\nstraints. To this end, we train thoseMACRO variants\nwith {1,2,5,10}×the number of steps of the se-\nquential TARGET variant.\nFor the multilingual step, training is always con-\nducted by including 8 instances for each language\nin a mini-batch: this is done to provide sufficient\nlanguage-specific examples per mini-batch without\ndramatically increasing the mini-batch size. For\nAmNLI and PAWS-X, we include all available lan-\nguages in training. For NER, we train on {DE, EN,\nSW, TA,VI, ZH}, and for POS on {AR, EN, EU, HU,\nID, JA, UR}. We now evaluate all the MACRO and\nTARGET variants on the following languages: for\nAmNLI, AYM, QUY , and TAR; for PAWS-X, DE,\nKO, JA; for NER, SW, VI, ZH; for POS, EU, UR, JA.\nTable 2 presents the complete results of this set\nof experiments, averaged over the three target lan-\nguages of each task. First, MACRO -MULTI is on-par\nor better than TARGET throughout almost all setups,\nbut, with the exception of token-level tasks, does\nnot consistently match the performance of MACRO -\nBI, which fine-tunes for a particular source-target\ndirection. The highest overall performance is ob-\n10731\nTARGET SOURCE-TARGET(MACRO)\nTARGET BUDGET\n1× 2× 5× 10× FT\nMULTI)BI BI MULTI )BI BI MULTI )BI BI MULTI )BI BI MULTI )BI BI MULTI MULTI )BI\nShotsL O L O L O L O L O L O L O L O L O L O L O L O L O\nAmNLI\n10 36.838.8 36.336.9 37.638.5 36.737.7 38.138.3 36.236.7 37.137.9 36.336.8 36.537.6 36.337.7 36.636.5 37.137.4 35.936.750 43.442.6 45.646.2 42.641.9 44.444.9 42.442.1 44.844.7 42.743.1 45.445.7 42.643.4 45.245.2 44.445.1 43.744.7 45.845.510045.745.8 48.248.4 45.745.3 48.748.0 46.045.6 48.548.8 46.345.7 48.749.2 45.946.2 49.249.1 46.747.2 47.247.3 49.048.425050.450.2 52.352.3 48.447.6 52.252.4 49.449.3 52.753.0 49.748.9 52.952.6 50.550.4 53.052.6 52.051.9 51.050.7 52.852.650051.752.5 52.352.7 52.251.1 53.553.5 52.851.5 53.253.0 53.452.5 53.353.3 53.353.1 52.753.8 54.053.7 54.053.7 53.753.5\nPA WS-X\n10 77.581.5 80.680.8 80.881.2 81.081.3 80.681.3 80.581.1 80.781.3 80.281.6 80.681.5 80.281.7 81.781.5 81.181.0 80.781.350 81.181.2 80.981.4 81.681.3 80.881.0 82.082.1 80.680.8 81.681.7 80.881.1 81.882.0 80.981.0 81.781.6 81.681.5 81.681.810081.681.6 82.282.4 81.882.1 82.582.8 81.581.9 82.782.9 81.681.9 82.782.8 81.781.7 82.883.0 81.882.1 81.781.6 82.082.325080.482.4 82.883.1 82.282.2 82.482.8 82.382.4 82.582.7 82.182.0 82.582.6 82.082.0 82.482.8 82.081.8 81.981.8 82.782.750081.382.7 82.983.5 83.082.9 83.183.5 83.082.5 83.383.4 82.582.7 83.683.6 83.183.0 83.983.7 82.782.5 83.183.0 83.683.1\nNER\n10 56.058.4 62.266.3 61.461.8 65.266.2 61.862.6 65.965.9 62.262.9 66.567.1 62.163.6 67.067.8 62.663.6 62.163.9 67.768.450 71.471.8 73.073.8 69.970.0 73.573.8 71.071.3 73.573.9 71.171.7 73.574.2 71.672.3 73.374.0 71.872.2 72.472.8 74.074.810072.773.8 75.275.7 73.273.2 75.576.0 73.573.9 75.775.8 73.474.0 75.876.2 74.374.7 76.176.4 74.774.8 74.774.9 76.176.525077.478.4 78.779.6 76.777.0 78.779.0 77.177.4 78.779.0 77.678.0 78.978.9 78.278.4 79.379.4 78.278.2 78.378.3 79.479.750079.380.0 80.981.4 79.179.1 80.780.8 79.379.5 81.181.1 79.880.2 81.481.3 80.280.4 81.181.5 80.280.3 80.480.4 81.481.4\nPOS\n10 77.577.5 80.680.7 76.476.4 80.780.6 77.677.7 80.980.9 77.978.2 81.081.0 78.078.2 81.081.1 78.478.3 79.279.3 81.281.450 83.483.3 85.685.8 81.281.2 85.685.5 82.482.4 85.785.8 83.383.3 85.785.8 83.583.6 85.885.8 84.484.4 84.884.8 86.086.010085.685.6 87.587.8 84.584.4 87.687.6 85.385.2 87.687.7 85.785.7 87.887.8 86.086.0 87.787.8 86.686.5 87.086.9 87.988.025088.088.2 89.189.4 87.387.3 89.489.5 87.987.8 89.689.6 88.488.4 89.789.6 88.688.6 89.689.6 88.988.8 89.189.1 89.789.750089.689.8 90.290.4 89.189.1 90.390.4 89.589.5 90.590.5 90.089.9 90.590.6 90.190.0 90.790.6 90.190.0 90.290.1 90.590.5\nTable 2: FS-XLT results where each fine-tuning regimes commences from the final checkpoint of English fine-tuning.\nAll tasks comprise three target languages, and the scores are averaged over three fixed random seeds, with training\nand validation subsets being the same for each seed.\nAMNLI PAWS-X NER POS\nT S -T T S -T T S -T T S -T\nShots L O L O L O L O L O L O L O L O\n10 36.5 36.5 35.8 36.3 78.0 83.6 83.4 83.1 50.8 53.8 54.0 55.3 80.8 80.8 78.5 78.8\n100 43.8 44.1 46.9 46.3 81.3 83.0 83.5 82.8 66.9 68.2 69.7 70.2 88.1 88.1 88.6 88.6\n500 50.1 49.7 52.9 52.4 82.2 83.0 84.5 84.3 74.2 75.0 76.7 76.6 91.0 91.1 91.3 91.3\nTable 3: FS-XLT with Chinese as the source language. S=SOURCE , S-T=SOURCE -TARGET (MACRO is used).\ntained with the hybrid MACRO -MULTI )BI, which\nreaps the best of both worlds: 1) multilingual fine-\ntuning prevents overfitting to a single source lan-\nguage and provides a better initialization point for\n2) the more specialized bilingual fine-tuning for\na particular source-target direction. Note that the\ntwo-stage MULTI )BI fine-tuning also improves the\nTARGET variant quite consistently. We report in-\ncrease in performance both forL and O checkpoints.\nNevertheless, MACRO still outperforms TARGET .\nThe results over different computational bud-\ngets reveal that longer training is beneficial for the\nMACRO variants. As expected, the setups with\nmore shots typically require fewer steps to con-\nverge. A general finding is that 1) the bilingual\nSOURCE -TARGET variants do trade off some of\nthe computational efficiency for enhanced perfor-\nmance, but 2) bilingual fine-tuning times can be\ndecreased by starting from a better (i.e., multilin-\ngual) initialization: cf., the MULTI )BI columns.\nAnother Source Language. Cross-lingual transfer\npredominantly focuses on English as the source\nlanguage (Hu et al., 2020; Lauscher et al., 2020),\nmostly because of the wide availability and abun-\ndance of annotated task data in English. In order to\nverify that our main findings generalise and reach\nbeyond English as the source language, we con-\nduct another set of experiments relying on Chi-\nnese as the source language.10 The results for the\nTARGET -LAST and MACRO -LAST variants are pre-\nsented in Table 3. The observed patterns largely\nfollow the general trends we reported with English\nas the source language; what is more, the gains\nof SOURCE -TARGET over TARGET even widen for\nAmNLI and PAWS-X. We speculate that this might\nbe due to a lower quality of the source Chinese\ninstances. Namely, except for POS, the task an-\nnotations for Chinese were either automatically\ntranslated (AmNLI, PAWS-X) or induced via some\nheuristics (WikiANN). Joint bilingual fine-tuning\nthen provides increased robustness against such\nnoisy source annotations.\n6 Conclusion\nRecent work demonstrated large benefits of few-\nshot cross-lingual transfer (FS-XLT) with multilin-\ngual language models, where a handful of anno-\ntated examples in the target language exist, over its\nzero-shot counterpart (ZS-XLT). However, as we\nhave proven in this paper, prior work overestimated\n10For AmLI and PAWS-X, we experiment with the same\nthree languages as in joint multilingual experiments. For NER,\nwe transfer to AR, UR, and JA, and to AR, DE, and UR for POS.\n10732\nFS-XLT performance, relying on an unrealistic as-\nsumption of having a dedicated validation set in\nthe target language to guide model selection. In\nthis work, we have performed an extensive compar-\native study of a wide variety of FS-XLT approaches,\nchallenging the status quo in FS-XLT. Our detailed\nanalyses have rendered established FS-XLT largely\nunstable and performing sub-par in true FS-XLT\nsetups without the target validation data. We have\nthus proposed novel FS-XLT fine-tuning regimes\nthat take into account interaction between source-\nlanguage and target-language data instances, yield-\ning improved, more stable, and more predictable\nFS-XLT performance across different tasks, lan-\nguages, and numbers of target-language shots. We\nhope that our study will inspire better FS-XLT train-\ning and evaluation practices in future work, and\nguide new developments for true FS-XLT setups.\n7 Limitations\nWhile we have striven to present a comprehen-\nsive and wide study of a large spectrum of FS-XLT\nfine-tuning regimes, several additional factors must\nbe taken into consideration. First, few-shot learn-\ning naturally comes with high variance, as demon-\nstrated by our work (where we set out to decrease\nthe variance) and a body of prior research in mono-\nlingual and cross-lingual transfer contexts. This\nstudy demanded an extremely large computational\nbudget (see Appendix A.1), so we constrained ex-\nperiments to independent runs with three seeds.\nIdeally, more independent runs (5-10) might yield\neven more consistent estimates.\nFurthermore, due to computational constraints,\nour work largely focuses on cross-lingual natu-\nral language understanding (NLU) and sequence-\nlabeling tasks. In addition, the community might\nfind a similar set of experiments insightful for\ncross-lingual transfer in other areas such as (i) task-\noriented dialogue systems, or (ii) long-range tasks\nlike document classification. Moreover, while we\nkeep hyper-parameters constant throughout differ-\nent regimes, it is highly likely that they can be\nfurther adapted and fine-tuned for a particular task,\nlanguage, and selection of shots. However, our core\nfindings demonstrate that the novel joint FS-XLT\nfine-tuning regimes consistently match or exceed\noracle performance while requiring no substantial\nhyper-parameter tuning or checkpoint selection.\nAcknowledgements\nWe thank the state of Baden-Württemberg for its\nsupport through access to the bwHPC. Fabian\nDavid Schmidt and Goran Glavaš were supported\nby the EUINACTION grant from NORFACE Gov-\nernance (462-19-010, GL950/2-1). Ivan Vuli ´c is\nsupported by a personal Royal Society University\nResearch Fellowship (no 221137; 2022-2027) as\nwell as a Huawei research donation to the Language\nTechnology Lab.\nReferences\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neubig,\nDaniel D’souza, Julia Kreutzer, Constantine Lignos,\nChester Palen-Michel, Happy Buzaaba, Shruti Rijh-\nwani, Sebastian Ruder, et al. 2021. Masakhaner:\nNamed entity recognition for african languages.\nTransactions of the Association for Computational\nLinguistics, 9:1116–1131.\nAlan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan\nVuli´c. 2022. Composable sparse fine-tuning for cross-\nlingual transfer. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1778–1796,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nAlan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Se-\nbastian Ruder, Goran Glavaš, Ivan Vuli´c, and Anna\nKorhonen. 2021. Mad-g: Multilingual adapter gen-\neration for efficient cross-lingual transfer. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2021, pages 4762–4781.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nSteven Cao, Nikita Kitaev, and Dan Klein. 2020. Multi-\nlingual alignment of contextual word representations.\nIn International Conference on Learning Representa-\ntions.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\n10733\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAbteen Ebrahimi, Manuel Mager, Arturo Oncevay,\nVishrav Chaudhary, Luis Chiruzzo, Angela Fan, John\nOrtega, Ricardo Ramos, Annette Rios, Ivan Vladimir,\nGustavo A. Gim énez-Lugo, Elisabeth Mager, Gra-\nham Neubig, Alexis Palmer, Rolando A. Coto Solano,\nNgoc Thang Vu, and Katharina Kann. 2021. Americ-\nasnli: Evaluating zero-shot natural language under-\nstanding of pretrained multilingual models in truly\nlow-resource languages. CoRR, abs/2104.08726.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nMichael A. Hedderich, David Adelani, Dawei Zhu, Je-\nsujoba Alabi, Udia Markus, and Dietrich Klakow.\n2020. Transfer learning and distant supervision for\nmultilingual transformer models: A study on African\nlanguages. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 2580–2591, Online. Association for\nComputational Linguistics.\nJunjie Hu, Melvin Johnson, Orhan Firat, Aditya Sid-\ndhant, and Graham Neubig. 2021. Explicit alignment\nobjectives for multilingual bidirectional encoders. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3633–3643, Online. Association for Computa-\ntional Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generalisa-\ntion. In International Conference on Machine Learn-\ning, pages 4411–4421. PMLR.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282–6293, Online. Association for Computational\nLinguistics.\nPhillip Keung, Yichao Lu, Julian Salazar, and Vikas\nBhardwaj. 2020. Don’t use English dev: On the\nzero-shot cross-lingual evaluation of contextual em-\nbeddings. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 549–554, Online. Association for\nComputational Linguistics.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. Advances in\nNeural Information Processing Systems (NeurIPS).\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glavaš. 2020. From zero to hero: On the\nlimitations of zero-shot language transfer with mul-\ntilingual Transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4483–4499, On-\nline. Association for Computational Linguistics.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Clément Delangue, Théo Matus-\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\ntac, Thibault Goehringer, Victor Mustar, François\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning: System Demonstrations, pages 175–184, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decou-\npled weight decay regularization. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-\nman, Kevin Knight, and Heng Ji. 2017. Cross-lingual\n10734\nname tagging and linking for 282 languages. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1946–1958, Vancouver, Canada. As-\nsociation for Computational Linguistics.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. In\nAdvances in Neural Information Processing Systems.\nJonas Pfeiffer, Naman Goyal, Xi Victoria Lin, Xian\nLi, James Cross, Sebastian Riedel, and Mikel\nArtetxe. 2022. Lifting the curse of multilingual-\nity by pre-training modular transformers. CoRR,\nabs/2205.06266.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. Mad-x: An adapter-based\nframework for multi-task cross-lingual transfer. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4996–5001, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 151–164, Florence,\nItaly. Association for Computational Linguistics.\nSebastian Ruder, Noah Constant, Jan Botha, Aditya Sid-\ndhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu,\nDan Garrette, Graham Neubig, et al. 2021. Xtreme-r:\nTowards more challenging and nuanced multilingual\nevaluation. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 10215–10245.\nAhmet Üstün, Arianna Bisazza, Gosse Bouma, Gertjan\nvan Noord, and Sebastian Ruder. 2022. Hyper-x:\nA unified hypernetwork for multi-task multilingual\ntransfer. CoRR, abs/2205.12148.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nShijie Wu and Mark Dredze. 2020. Do explicit align-\nments robustly improve multilingual encoders? In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4471–4482, Online. Association for Computa-\ntional Linguistics.\nHaoran Xu and Kenton Murray. 2022. Por qué não\nutiliser alla språk? mixed training with gradient opti-\nmization in few-shot cross-lingual transfer. In Find-\nings of the Association for Computational Linguis-\ntics: NAACL 2022, pages 2043–2059, Seattle, United\nStates. Association for Computational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498.\nHuiyun Yang, Huadong Chen, Hao Zhou, and Lei Li.\n2022. Enhancing cross-lingual transfer by manifold\nmixup. In International Conference on Learning\nRepresentations.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A cross-lingual adversar-\nial dataset for paraphrase identification. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 3687–3692, Hong\nKong, China. Association for Computational Linguis-\ntics.\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey\nLevine, Karol Hausman, and Chelsea Finn. 2020.\nGradient surgery for multi-task learning. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 5824–5836. Curran Associates,\nInc.\nDaniel Zeman, Joakim Nivre, et al. 2020. Universal\ndependencies 2.7. LINDAT/CLARIAH-CZ digital\nlibrary at the Institute of Formal and Applied Linguis-\ntics (ÚFAL), Faculty of Mathematics and Physics,\nCharles University.\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. 2018. mixup: Beyond empirical\nrisk minimization. In International Conference on\nLearning Representations.\nMengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vuli ´c,\nRoi Reichart, Anna Korhonen, and Hinrich Schütze.\n2021. A closer look at few-shot crosslingual transfer:\n10735\nThe choice of shots matters. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers ), pages 5751–5767, Online.\nAssociation for Computational Linguistics.\nA Appendix\nA.1 Reproducibility\nInfrastructure and Compute. We train our mod-\nels on a cluster that provides virtual machines on\nwhich each model was trained on a single NVIDIA\nTesla V100 32GB GPU. We evaluate 7 setups with\nthree seeds for k ∈{10,50,100,250,500}shots\nacross 4 tasks in our base experiments, amounting\nto 5,145 models trained for 3,756 GPU hours for\nour main results. Therein, AmNLI alone takes up\n2,170 hours (57.8%).\nDatasets. We access all datasets via the Hugging-\nface datasets library (Lhoest et al., 2021). When-\never we subsample data, we initially shuffle the\ndataset with one of seed s ∈ {42,43,44}built-\nin datasets method and subsequently extract the\nfirst krequired instances for our experiments. In\ncase we require a validation subset from the same\ndataset, we extract the |ND|− 500 last available\nobservations after shuffling to evaluate our models\nduring training (i.e., to measure ORACLE perfor-\nmance). We manually verified that our approach\nyields consistent subsamples by seed.\nCode. Our code is available at: https://github.\ncom/fdschmidt93/fsxlt\n10736\nA.2 Stability Of Few-Shot Cross-Lingual Transfer\nFigure 3: FS-XLT regimes (joint MACRO versus sequential TARGET ) starting from the LAST checkpoint of the initial\nsource language fine-tuning step. The colored dots group runs for each seed by language and mark the checkpoints\nthat transfer best to target-language validation data. The line plots the mean (incl. ±1σ) test set spread (in %) of\nbest validation and current checkpoint.\n10737\nA.3 Full Results over Individual Target Languages\nA.4 AmericasNLI\nSOURCE TARGET SOURCE-TARGET\nZero-Shot Few-Shot MACRO MIX-UP\nWeights kShots LM LAST ORACLE LM LAST ORACLE LM LAST ORACLE\nMetric L O L O L O L O L O L O L O L O L O\nAymara 10 39.7 39.5 38.0 39.0 39.3 41.8 34.5 37.0 36.9 37.1 34.8 38.0 34.5 35.9 36.8 38.1 36.7 38.3\nAYM 50 39.7 39.5 44.9 45.0 44.7 46.1 40.9 42.1 45.4 45.6 42.2 42.8 37.1 40.0 44.5 44.8 45.6 45.5\n100 39.7 39.5 45.7 45.5 46.9 48.7 45.9 46.4 46.0 46.7 47.7 46.9 43.0 43.6 49.0 47.9 47.3 47.9\n250 39.7 39.5 49.3 50.0 50.1 50.9 50.0 51.4 51.6 51.4 53.3 51.5 50.5 51.6 52.0 51.1 52.0 52.2\n500 39.7 39.5 52.0 52.5 51.6 50.8 51.6 51.6 54.6 54.8 55.2 56.1 52.5 50.9 53.7 54.3 53.5 54.4\nBribri 10 40.8 40.4 39.1 40.0 39.0 40.7 36.4 36.3 39.9 39.7 41.0 41.6 36.1 35.9 40.3 40.2 39.1 39.8\nBZD 50 40.8 40.4 44.7 45.2 44.5 44.2 44.1 45.7 47.6 45.8 49.0 49.4 42.5 43.0 47.0 48.6 47.5 48.7\n100 40.8 40.4 48.6 46.8 49.2 48.2 49.2 49.6 51.5 50.5 52.5 51.2 48.2 48.1 51.7 51.2 52.5 50.7\n250 40.8 40.4 52.6 52.0 54.9 55.1 52.0 51.4 54.2 54.6 53.4 53.7 51.3 52.3 55.2 54.8 55.3 54.6\n500 40.8 40.4 56.6 56.4 56.8 56.7 54.6 56.1 57.9 57.3 57.7 58.2 56.4 55.0 56.5 56.7 57.2 56.8\nGuarani 10 41.1 42.1 40.3 41.7 39.3 44.0 35.7 35.7 38.6 39.4 38.3 40.0 34.4 34.5 40.6 42.5 39.4 39.1\nGN 50 41.1 42.1 46.8 47.1 45.2 44.9 40.8 42.5 45.3 45.4 45.5 46.1 40.7 40.2 47.1 46.7 45.6 46.7\n100 41.1 42.1 47.6 46.6 48.8 47.9 45.0 45.3 49.2 47.6 49.3 49.7 44.8 46.3 49.6 48.8 49.6 50.4\n250 41.1 42.1 52.1 51.4 49.7 49.7 49.8 51.7 51.6 52.2 51.6 51.8 48.3 50.2 51.4 50.8 51.7 50.5\n500 41.1 42.1 54.2 52.8 53.4 51.3 51.3 52.4 53.3 52.7 54.0 52.9 53.5 52.8 52.6 52.5 53.5 52.6\nWixarika 10 38.4 37.5 36.9 38.4 37.5 39.3 33.8 35.5 37.6 38.4 36.4 37.3 34.4 34.3 36.2 38.0 35.5 36.6\nHCH 50 38.4 37.5 40.3 39.9 40.6 39.8 35.9 38.1 40.3 40.3 40.1 40.2 36.4 35.8 39.9 39.5 39.8 40.8\n100 38.4 37.5 41.8 39.9 41.0 40.8 37.0 38.3 40.8 40.5 41.6 42.0 37.9 37.2 41.8 42.6 42.3 41.7\n250 38.4 37.5 44.2 44.5 43.4 41.5 40.3 40.3 45.2 45.9 44.8 43.1 39.5 39.3 45.6 45.5 44.0 44.7\n500 38.4 37.5 44.8 46.1 44.5 43.8 45.7 46.8 46.4 45.7 45.5 45.2 44.7 44.0 46.9 46.5 45.7 46.0\nQuechua 10 37.3 38.3 37.2 40.1 38.6 42.6 33.6 33.6 37.2 36.0 36.9 37.6 34.4 34.5 37.1 39.1 37.4 38.8\nQUY 50 37.3 38.3 43.9 43.1 46.1 46.0 42.3 45.1 44.4 46.0 46.7 48.2 42.5 42.4 43.2 44.6 46.6 46.0\n100 37.3 38.3 47.8 46.2 48.2 48.1 41.3 41.5 47.5 48.1 50.4 49.0 45.3 45.6 47.7 46.1 49.8 49.4\n250 37.3 38.3 52.2 51.7 51.5 52.1 50.3 49.7 52.8 52.1 54.5 54.5 50.1 51.2 52.8 52.8 54.8 54.0\n500 37.3 38.3 52.7 53.3 54.0 53.6 52.1 52.7 55.6 55.1 55.3 54.9 52.7 53.1 54.8 55.4 54.7 55.0\nShipibo 10 41.0 42.9 41.0 42.8 40.3 42.0 36.4 38.7 40.0 39.8 39.2 40.0 36.7 37.5 40.2 42.4 37.9 40.9\nSHP 50 41.0 42.9 44.4 43.0 44.4 42.8 39.6 41.9 44.5 44.2 42.9 43.0 40.1 42.4 44.4 44.4 44.3 44.2\n100 41.0 42.9 45.9 44.4 44.4 43.9 45.2 47.4 45.9 45.6 44.6 45.9 43.8 44.2 46.2 46.7 45.2 45.3\n250 41.0 42.9 47.7 48.0 48.2 47.8 48.7 50.8 50.0 50.2 50.5 50.4 50.5 49.6 51.5 50.1 50.3 50.0\n500 41.0 42.9 51.3 51.2 51.5 50.6 55.0 55.2 53.8 53.7 54.3 52.8 54.1 53.4 54.5 54.3 54.4 52.6\nRaramuri10 39.1 39.2 35.3 37.3 34.9 37.6 33.9 35.4 35.7 36.4 35.1 35.9 34.9 34.9 34.2 35.4 34.8 35.0\nTAR 50 39.1 39.2 41.5 39.7 42.5 41.1 40.4 42.1 43.4 43.6 44.4 45.1 39.0 40.4 42.0 42.9 44.1 42.8\n100 39.1 39.2 43.5 45.8 45.7 46.1 45.0 45.5 46.7 46.9 49.0 47.8 43.3 45.2 45.7 45.6 46.8 48.4\n250 39.1 39.2 49.5 48.8 50.8 48.8 48.0 48.8 51.7 52.2 51.8 52.4 48.6 49.1 51.9 49.3 53.9 53.2\n500 39.1 39.2 50.4 51.7 52.4 51.4 52.6 52.6 51.6 51.2 54.3 53.9 52.1 51.9 53.3 52.9 52.9 54.3\n10738\nA.5 PA WS-X\nSOURCE TARGET SOURCE-TARGET\nZero-Shot Few-Shot MACRO MIX-UP\nWeightskShots LM LAST ORACLE LM LAST ORACLE LM LAST ORACLE\nMetric L O L O L O L O L O L O L O L O L O\nGerman 10 88.7 88.7 84.4 88.7 82.9 88.9 87.4 87.2 89.5 89.5 89.2 89.1 84.6 86.0 88.8 88.7 89.1 89.2\nDE 50 88.7 88.7 88.5 88.8 88.5 88.8 86.2 87.1 89.0 89.1 89.6 89.3 80.4 83.7 89.0 89.0 89.0 88.5\n100 88.7 88.7 88.9 88.9 88.9 89.1 86.8 86.6 89.4 89.4 89.2 88.8 82.2 83.4 89.1 89.0 89.3 88.9\n250 88.7 88.7 87.2 88.8 87.4 88.7 87.4 86.6 89.2 88.9 89.4 89.4 84.6 84.7 89.0 89.2 89.1 89.1\n500 88.7 88.7 87.6 89.0 86.5 88.9 87.2 87.0 89.4 89.5 89.6 89.4 86.4 87.2 89.3 89.2 89.5 89.2\nSpanish 10 89.5 89.7 85.9 89.7 85.9 89.7 88.3 88.4 89.8 89.4 89.8 89.4 85.9 88.2 89.8 89.8 89.8 89.8\nES 50 89.5 89.7 88.9 89.7 88.9 89.7 88.5 88.0 90.0 89.8 90.0 89.8 84.7 85.5 89.7 89.2 89.7 89.2\n100 89.5 89.7 89.0 89.3 89.0 89.3 87.2 86.8 89.6 89.7 89.6 89.7 82.5 85.2 90.0 90.1 90.0 90.1\n250 89.5 89.7 88.7 89.7 88.7 89.7 88.1 87.6 89.9 89.7 89.9 89.7 85.0 85.6 89.5 89.0 89.5 89.0\n500 89.5 89.7 88.2 90.3 88.2 90.3 88.9 88.8 90.3 89.2 90.3 89.2 87.9 87.8 89.8 89.5 89.8 89.5\nFrench 10 89.6 90.2 87.7 89.8 86.5 90.4 89.1 89.0 90.0 90.4 90.5 90.3 87.3 88.2 90.0 89.4 89.8 89.6\nFR 50 89.6 90.2 88.7 90.0 89.2 90.4 84.1 87.8 90.0 89.7 90.1 90.1 80.1 84.4 90.3 90.2 90.2 90.0\n100 89.6 90.2 89.4 89.6 89.4 90.2 87.2 87.6 90.2 89.9 90.6 90.5 84.5 86.4 90.2 90.2 90.4 90.1\n250 89.6 90.2 88.9 90.1 89.0 90.1 87.3 88.0 90.0 90.4 90.5 90.7 86.0 86.4 90.5 90.4 90.2 89.9\n500 89.6 90.2 89.3 90.2 88.7 90.1 89.2 89.1 91.0 91.0 91.0 91.0 88.6 88.5 90.6 90.3 90.4 90.7\nJapanese 10 77.1 77.1 75.7 77.2 74.5 77.1 72.7 73.5 77.3 77.1 77.2 77.5 68.3 71.6 76.7 77.5 76.1 77.0\nJA 50 77.1 77.1 76.6 77.1 74.7 76.8 71.3 73.9 77.6 77.5 76.7 76.9 62.8 64.5 78.0 77.8 78.0 77.3\n100 77.1 77.1 77.2 77.5 74.0 76.3 72.3 73.0 77.2 78.0 77.2 77.2 67.2 70.9 77.8 77.5 77.1 77.4\n250 77.1 77.1 76.9 78.8 76.4 77.3 74.4 75.4 78.4 78.2 78.2 78.0 69.9 71.2 77.5 77.8 78.0 77.6\n500 77.1 77.1 77.7 79.6 77.4 78.9 76.4 76.9 79.2 79.0 79.4 79.3 75.8 75.2 79.4 79.4 78.8 79.1\nKorean 10 76.7 77.2 72.2 78.6 72.2 78.2 71.1 73.4 78.3 78.0 78.7 78.4 62.9 69.9 77.3 77.6 77.2 77.6\nKO 50 76.7 77.2 78.2 77.5 77.6 78.6 71.4 72.6 78.5 78.2 78.9 79.5 65.0 67.0 78.9 78.3 78.9 78.6\n100 76.7 77.2 78.6 78.6 78.6 78.5 70.6 71.0 78.7 78.8 78.6 78.7 65.8 67.9 78.6 78.1 79.2 79.3\n250 76.7 77.2 77.2 79.6 77.1 78.8 72.9 74.1 78.4 78.3 78.6 78.4 68.8 70.6 78.6 78.3 78.8 78.2\n500 76.7 77.2 78.8 79.7 79.6 79.7 75.4 75.5 79.5 79.1 79.4 79.8 73.8 74.2 80.1 79.6 79.6 79.3\nChinese 10 81.1 81.3 80.0 81.6 78.2 82.2 78.0 79.4 82.3 82.5 82.8 82.8 74.6 79.6 81.3 81.5 81.0 81.9\nZH 50 81.1 81.3 80.3 81.7 81.3 82.2 77.8 78.0 81.4 81.4 82.3 81.5 73.2 74.4 81.7 82.0 82.4 82.4\n100 81.1 81.3 81.2 82.2 81.2 82.3 75.4 76.0 82.2 81.4 82.4 81.4 68.8 73.2 81.7 81.6 82.1 82.7\n250 81.1 81.3 80.2 82.1 80.4 82.1 77.4 78.8 81.6 82.2 82.8 82.8 75.7 76.8 82.0 82.0 81.6 81.9\n500 81.1 81.3 81.6 83.2 81.1 82.4 79.7 80.2 82.1 82.5 83.1 83.0 78.7 78.7 82.3 82.2 82.2 82.1\n10739\nA.6 WikiANN\nSOURCE TARGET SOURCE-TARGET\nZero-Shot Few-Shot MACRO MIX-UP\nWeights kShots LM LAST ORACLE LM LAST ORACLE LM LAST ORACLE\nMetric L O L O L O L O L O L O L O L O L O\nAfrikaans 10 72.9 73.3 72.975.6 72.675.5 75.075.2 75.576.3 76.075.8 75.475.9 75.674.5 75.475.3\nAF 50 72.9 73.3 76.976.9 77.877.9 78.778.6 78.877.4 78.278.2 77.777.8 78.278.2 78.877.9\n100 72.9 73.3 78.679.8 79.379.2 80.180.3 79.679.6 80.079.3 79.779.5 80.180.3 80.580.2\n250 72.9 73.3 81.281.2 81.081.9 81.682.0 82.282.1 82.081.9 82.081.8 82.382.0 82.281.9\n500 72.9 73.3 82.883.7 83.383.7 83.783.5 83.683.5 83.583.4 84.384.4 83.783.7 83.783.7\nArabic 10 43.2 49.0 66.666.7 64.266.7 69.169.6 69.169.9 68.870.6 69.070.6 68.069.8 70.071.6\nAR 50 43.2 49.0 72.573.0 72.172.5 73.273.8 73.974.4 74.374.6 73.873.9 74.074.6 74.374.8\n100 43.2 49.0 73.473.8 73.273.9 74.574.9 75.475.9 75.576.0 75.576.0 75.476.0 75.776.2\n250 43.2 49.0 74.876.7 75.576.7 77.177.3 77.477.6 77.877.8 77.777.8 78.078.0 78.178.1\n500 43.2 49.0 76.679.1 76.878.4 79.780.0 79.779.7 79.879.8 80.079.9 79.879.9 79.879.7\nGerman 10 70.6 71.6 68.372.3 68.273.5 72.673.9 71.772.5 71.872.5 73.573.6 71.672.3 72.273.1\nDE 50 70.6 71.6 72.173.1 72.673.8 74.875.0 73.974.5 74.374.8 75.275.4 74.174.4 74.574.6\n100 70.6 71.6 73.173.6 73.273.7 75.676.2 75.075.1 75.175.7 75.575.6 74.975.6 75.976.0\n250 70.6 71.6 75.576.6 75.776.7 77.177.5 76.976.9 76.776.9 77.377.4 76.776.7 77.177.2\n500 70.6 71.6 76.477.9 77.178.4 78.778.7 78.378.3 78.578.5 78.778.8 78.178.3 78.478.5\nJapanese 10 17.1 17.9 32.032.8 32.633.1 31.532.8 34.936.2 35.336.8 31.533.0 32.234.7 33.534.4\nJA 50 17.1 17.9 43.544.0 44.745.3 46.547.4 46.947.1 47.447.2 47.247.5 44.946.2 46.247.8\n100 17.1 17.9 47.848.1 49.249.7 51.452.2 50.250.9 50.851.3 51.151.5 50.350.6 49.650.5\n250 17.1 17.9 52.753.4 54.254.4 56.356.4 55.755.6 55.055.3 56.256.3 55.055.0 55.255.4\n500 17.1 17.9 55.857.6 58.058.0 59.759.7 59.159.1 59.559.5 59.759.9 58.858.8 58.859.2\nQuechuan 10 54.8 55.3 61.161.5 59.863.9 58.862.9 62.260.1 62.263.2 63.764.0 63.264.7 63.862.9\nQU 50 54.8 55.3 70.569.2 74.673.1 69.671.9 68.968.3 69.370.2 71.272.4 69.971.7 70.069.5\n100 54.8 55.3 71.472.9 74.973.2 76.376.2 74.473.3 75.273.2 78.076.4 70.972.0 74.674.2\nRussian 10 65.7 66.5 64.772.0 64.772.0 71.673.3 73.073.8 73.073.8 73.173.4 72.173.8 72.173.8\nRU 50 65.7 66.5 78.178.4 78.178.4 78.178.4 78.078.4 78.078.4 78.878.8 77.978.2 77.978.2\n100 65.7 66.5 80.380.2 80.380.2 78.778.5 79.279.5 79.279.5 79.179.2 79.479.6 79.479.6\n250 65.7 66.5 80.582.0 80.582.0 81.381.4 81.681.6 81.681.6 81.481.5 81.481.5 81.481.5\n500 65.7 66.5 82.383.3 82.383.3 83.283.1 83.383.3 83.383.3 83.183.1 83.183.2 83.183.2\nRwanda 10 57.6 57.3 57.662.8 59.063.5 64.061.1 62.264.0 60.462.9 60.659.0 63.363.0 59.660.4\nRW 50 57.6 57.3 75.973.2 74.576.5 76.675.8 73.374.0 75.075.4 75.173.9 73.273.2 75.574.4\n100 57.6 57.3 75.577.6 75.276.8 78.376.6 76.378.2 75.875.3 78.477.0 76.477.4 77.876.9\nSwahili 10 61.1 63.8 70.670.5 70.872.6 73.874.1 74.774.8 73.974.9 74.774.8 73.274.6 74.474.6\nSW 50 61.1 63.8 84.384.2 84.484.5 84.384.3 83.884.8 84.283.9 84.284.1 84.184.3 84.283.6\n100 61.1 63.8 84.685.0 85.585.3 85.486.0 86.586.3 85.385.8 85.985.4 85.886.5 85.185.1\n250 61.1 63.8 87.388.2 87.887.5 87.788.1 88.187.9 87.887.6 88.488.8 88.188.3 87.987.9\n500 61.1 63.8 89.089.8 88.789.6 89.589.2 89.589.6 89.489.9 89.689.2 89.689.5 89.689.6\nTamil 10 58.6 61.4 62.862.9 63.264.8 63.063.7 66.466.9 66.366.9 62.764.1 64.566.0 65.466.7\nTA 50 58.6 61.4 70.671.2 71.471.2 72.371.9 72.372.1 72.872.7 72.072.0 72.972.4 72.772.8\n100 58.6 61.4 73.673.4 73.072.7 74.173.9 74.374.4 74.174.1 74.074.0 73.774.3 74.574.4\n250 58.6 61.4 74.976.1 75.776.1 77.076.9 77.077.1 77.077.0 77.376.9 76.776.5 77.377.1\n500 58.6 61.4 76.777.7 76.578.3 78.479.2 78.778.6 79.078.6 79.679.3 77.978.1 78.578.3\nUrdu 10 56.9 64.0 74.675.1 75.077.5 77.578.0 75.977.4 76.877.2 77.378.6 73.676.2 76.878.2\nUR 50 56.9 64.0 79.779.6 80.581.4 80.880.6 81.381.5 80.681.3 81.181.9 81.082.8 81.382.2\n100 56.9 64.0 80.581.8 80.983.3 82.982.7 83.282.4 82.281.7 82.082.5 83.383.6 82.783.2\n250 56.9 64.0 83.883.9 84.585.4 85.385.4 85.085.3 84.985.4 85.185.1 85.185.3 85.385.8\n500 56.9 64.0 85.786.5 85.386.6 87.387.2 87.286.7 87.887.5 87.687.6 87.487.4 87.987.7\nVietnamese10 70.7 70.8 64.271.6 64.972.0 73.275.5 74.175.5 75.076.1 74.074.5 74.975.1 74.875.6\nVI 50 70.7 70.8 78.378.9 77.878.5 78.479.0 78.878.7 78.878.9 79.580.1 78.779.2 78.779.3\n100 70.7 70.8 79.980.1 79.479.6 79.880.0 79.579.6 79.780.1 80.781.0 79.980.6 80.080.4\n250 70.7 70.8 82.283.0 81.982.5 82.382.4 82.082.0 82.082.2 83.083.1 82.182.1 81.782.0\n500 70.7 70.8 83.083.2 82.283.5 83.483.8 83.083.1 83.082.9 83.883.9 82.983.1 82.582.8\nYoruba 10 28.1 48.7 61.365.9 65.567.3 61.865.8 65.066.8 65.269.0 63.165.7 61.762.5 61.267.4\nYO 50 28.1 48.7 82.085.5 83.585.1 79.283.2 86.886.3 84.787.7 80.983.7 86.485.6 84.786.1\n100 28.1 48.7 85.185.5 89.387.2 86.487.0 88.187.3 88.086.7 86.787.7 86.786.5 87.187.3\nChinese 10 25.6 27.8 33.033.0 33.033.0 38.740.6 39.240.6 39.240.6 35.838.9 36.739.0 36.739.0\nZH 50 25.6 27.8 51.752.3 51.752.3 53.455.0 52.953.2 52.953.2 54.455.2 52.953.7 52.953.7\n100 25.6 27.8 53.656.3 53.656.3 58.559.3 58.058.5 58.058.5 58.659.1 56.857.8 56.857.8\n250 25.6 27.8 62.863.9 62.863.9 65.465.5 64.364.8 64.364.8 65.065.3 63.864.0 63.864.0\n500 25.6 27.8 66.067.0 66.067.0 68.768.9 68.068.3 68.068.3 68.468.6 67.667.5 67.667.5\n10740\nA.7 Part-Of-Speech Tagging\nSOURCE TARGET SOURCE-TARGET\nZero-Shot Few-Shot MACRO MIX-UP\nWeights kShots LM LAST ORACLE LM LAST ORACLE LM LAST ORACLE\nMetric L O L O L O L O L O L O L O L O L O\nAfrikaans 10 86.5 86.5 91.6 91.4 92.892.8 91.2 91.1 90.290.2 90.990.6 91.5 91.5 90.590.4 91.091.2\nAF 50 86.5 86.5 94.5 94.4 94.994.7 94.0 94.0 93.093.0 93.393.5 94.2 94.1 93.093.1 93.793.7\n100 86.5 86.5 95.3 95.3 95.695.6 95.1 95.1 94.794.6 94.694.6 95.5 95.4 94.794.5 94.994.6\n250 86.5 86.5 96.8 96.7 96.996.9 97.0 96.9 96.396.3 96.596.6 97.0 97.0 96.496.3 96.496.4\n500 86.5 86.5 97.3 97.3 97.497.5 97.6 97.5 97.497.3 97.697.5 97.7 97.7 97.297.2 97.597.4\nArabic 10 70.6 71.4 83.2 83.1 83.283.2 83.4 83.4 82.882.8 82.782.8 82.7 82.8 82.982.9 83.183.4\nAR 50 70.6 71.4 84.8 84.9 85.085.1 85.2 85.2 85.485.4 85.385.2 85.1 85.1 85.385.4 85.485.3\n100 70.6 71.4 85.4 85.6 85.585.6 86.1 86.1 86.286.2 86.286.2 85.8 85.8 86.286.2 86.486.4\n250 70.6 71.4 86.7 86.7 86.886.8 87.2 87.2 87.387.3 87.487.3 87.0 87.0 87.287.2 87.387.3\n500 70.6 71.4 87.4 87.4 87.487.5 87.7 87.7 87.887.7 87.887.8 87.5 87.6 87.787.7 87.787.7\nBasque 10 54.5 55.2 73.7 73.8 74.174.1 73.9 74.0 73.473.6 73.974.2 73.7 74.0 74.274.2 74.374.4\nEU 50 54.5 55.2 81.6 81.5 81.981.9 81.9 82.0 81.781.9 81.781.9 82.0 82.1 82.382.3 82.682.5\n100 54.5 55.2 84.8 84.8 84.985.1 85.4 85.3 85.785.7 85.585.5 85.4 85.4 85.685.7 85.985.9\n250 54.5 55.2 88.0 88.4 88.588.9 89.0 89.1 89.089.1 89.189.2 89.0 89.0 89.289.2 89.389.4\n500 54.5 55.2 90.4 90.6 90.790.8 91.1 91.0 91.091.0 91.091.0 91.0 90.9 91.091.0 91.091.1\nChinese 10 34.2 40.8 64.9 64.9 65.065.2 67.8 68.0 67.367.3 67.167.4 66.1 66.9 66.967.1 67.367.2\nZH 50 34.2 40.8 74.9 74.9 75.475.6 77.6 77.4 76.876.8 77.277.1 78.0 77.8 77.077.0 77.677.6\n100 34.2 40.8 78.7 78.6 79.179.2 81.7 81.7 80.880.7 81.281.3 81.5 81.5 80.880.8 81.181.1\n250 34.2 40.8 82.9 82.9 83.283.1 84.7 84.6 84.284.2 84.584.5 84.6 84.6 84.184.1 84.684.5\n500 34.2 40.8 85.5 85.5 85.685.7 86.8 86.7 86.586.4 86.686.6 86.7 86.7 86.386.3 86.586.5\nGerman 10 86.1 86.3 90.0 90.0 90.090.0 90.0 90.1 89.289.4 89.189.5 90.1 90.1 89.289.6 89.389.6\nDE 50 86.1 86.3 92.4 92.4 92.492.4 92.3 92.3 91.691.8 91.691.7 92.2 92.3 91.791.8 91.691.9\n100 86.1 86.3 93.4 93.5 93.593.5 93.4 93.4 92.993.0 92.992.9 93.5 93.4 92.993.0 92.992.9\n250 86.1 86.3 94.6 94.6 94.594.7 94.7 94.8 94.494.4 94.494.4 94.8 94.8 94.594.5 94.594.5\n500 86.1 86.3 95.2 95.3 95.195.3 95.4 95.4 95.295.2 95.295.2 95.4 95.4 95.295.3 95.395.3\nHindi 10 66.7 67.2 84.3 84.3 84.584.7 84.7 84.8 83.683.9 84.284.1 84.1 84.3 83.883.8 84.084.1\nHI 50 66.7 67.2 88.4 88.4 88.388.3 88.6 88.4 88.388.4 88.588.5 88.2 88.1 88.488.5 88.588.6\n100 66.7 67.2 89.1 89.3 89.489.3 89.6 89.5 89.689.6 89.589.6 89.2 89.3 89.589.4 89.789.6\n250 66.7 67.2 90.5 90.7 90.490.6 90.9 90.9 90.990.9 90.991.0 90.8 90.9 90.990.8 90.990.9\n500 66.7 67.2 91.1 91.2 91.291.2 91.5 91.5 91.591.4 91.591.5 91.4 91.4 91.591.5 91.591.5\nHungarian10 75.0 75.3 86.9 86.9 87.287.2 85.2 85.3 84.985.1 84.885.4 85.6 85.7 84.884.9 84.785.0\nHU 50 75.0 75.3 91.7 91.7 91.891.8 92.0 91.8 91.591.3 91.391.1 91.7 91.7 91.591.7 91.591.5\n100 75.0 75.3 93.0 93.0 93.293.2 93.3 93.2 93.092.9 93.193.1 93.3 93.3 93.092.9 93.193.1\n250 75.0 75.3 94.8 94.6 94.994.9 94.9 94.8 94.894.8 94.994.9 95.1 95.0 95.094.9 95.095.0\n500 75.0 75.3 95.8 95.8 95.995.9 95.9 95.9 95.995.9 95.995.8 95.8 95.9 95.995.8 95.995.8\nIndonesian10 71.6 71.6 74.1 74.1 74.674.6 74.4 74.4 73.473.6 73.773.8 74.2 74.2 73.973.9 74.174.0\nID 50 71.6 71.6 76.3 76.3 76.576.3 75.7 75.7 75.875.9 75.975.9 75.7 75.9 76.276.3 76.476.5\n100 71.6 71.6 76.5 76.5 76.476.4 76.1 76.0 76.376.3 76.476.2 76.0 76.0 76.776.7 76.676.6\n250 71.6 71.6 77.0 76.9 76.876.9 76.8 76.7 77.076.9 77.077.1 76.7 76.7 77.177.1 77.177.0\n500 71.6 71.6 76.9 77.0 76.776.8 76.8 76.7 77.077.0 77.177.1 76.9 76.6 77.277.1 77.377.2\nJapanese 10 24.7 28.3 75.2 75.2 75.675.5 78.9 79.0 78.278.0 78.378.5 77.4 77.5 77.377.5 77.277.2\nJA 50 24.7 28.3 81.2 81.2 81.581.6 83.8 83.7 83.683.6 83.583.3 83.6 83.6 83.383.1 83.182.8\n100 24.7 28.3 83.3 83.3 83.483.7 85.1 85.0 85.184.8 85.485.3 84.7 84.9 84.584.6 84.684.5\n250 24.7 28.3 86.1 86.1 86.286.3 87.3 87.2 87.187.0 87.387.0 87.1 87.1 86.786.6 86.886.7\n500 24.7 28.3 87.4 87.8 87.787.7 88.1 88.2 88.288.1 88.288.2 87.8 87.8 88.188.0 88.288.1\nRussian 10 82.8 83.1 85.3 85.3 85.285.2 86.2 86.4 85.585.6 85.685.7 86.4 86.4 85.585.6 86.186.1\nRU 50 82.8 83.1 88.4 88.4 88.888.8 88.9 88.9 87.887.9 88.188.2 89.3 89.3 88.488.5 88.888.8\n100 82.8 83.1 90.2 90.2 90.390.4 90.5 90.6 89.689.7 90.190.1 90.6 90.6 90.090.0 90.390.4\n250 82.8 83.1 91.8 91.9 91.992.1 92.3 92.3 91.791.7 91.991.9 92.3 92.3 92.092.0 92.192.1\n500 82.8 83.1 93.0 93.1 93.193.2 93.4 93.4 93.093.1 93.393.3 93.3 93.3 93.293.2 93.493.4\nTamil 10 43.5 44.0 66.5 66.3 66.966.3 66.2 66.7 67.267.1 68.167.1 65.5 65.5 67.166.7 67.366.7\nTA 50 43.5 44.0 76.7 75.5 77.777.6 78.3 78.7 78.578.1 79.778.8 77.3 77.2 78.678.4 78.778.8\n100 43.5 44.0 80.9 80.7 81.081.8 82.9 82.7 82.682.3 82.982.1 81.4 81.4 82.182.4 82.482.5\n250 43.5 44.0 85.4 84.3 85.685.4 86.1 86.0 86.185.8 86.386.5 85.7 85.8 86.186.0 86.085.9\nUrdu 10 55.6 55.9 83.7 83.7 83.883.8 83.6 83.6 83.683.4 83.283.3 82.9 82.9 83.283.3 83.183.2\nUR 50 55.6 55.9 87.4 87.3 87.587.5 87.2 87.2 87.887.8 87.587.7 87.1 87.1 87.587.4 87.887.7\n100 55.6 55.9 88.9 88.8 88.788.7 88.9 88.7 89.189.2 89.089.0 88.9 88.9 89.188.9 89.088.9\n250 55.6 55.9 90.0 90.2 90.090.0 90.4 90.2 90.490.4 90.690.6 89.9 89.9 90.390.2 90.390.4\n500 55.6 55.9 90.9 90.9 90.890.9 90.9 90.9 91.191.0 91.291.2 90.7 90.7 90.991.0 91.091.2\n10741\nA.8 Multilingual Results\nTARGET S-T MULTI\nMACRO -LAST\nL O L O L O\nANLI\n10 38.3 39.9 38.0 38.1 38.0 38.2\n50 43.8 43.3 44.4 44.4 43.9 44.7\n100 45.8 45.0 46.8 46.6 46.8 46.8\n250 49.7 49.5 51.0 51.2 50.1 50.1\n500 51.7 52.0 53.3 52.9 52.6 52.4\nPA WS-X\n10 81.0 84.2 84.5 84.5 84.4 84.2\n50 83.5 84.2 84.4 84.3 84.4 84.3\n100 84.0 84.3 84.6 84.5 84.2 84.0\n250 83.2 84.9 84.6 84.6 84.3 84.2\n500 83.8 85.3 85.3 85.0 85.2 85.1\nNER\n10 59.8 62.1 65.2 66.1 64.5 66.1\n50 71.4 71.9 72.4 72.7 72.9 73.2\n100 73.0 73.7 74.7 74.8 74.8 74.9\n250 76.6 77.6 77.7 77.7 77.9 77.9\n500 78.2 79.1 79.5 79.6 79.7 79.6\nPOS\n10 79.4 79.4 79.4 79.4 80.1 80.2\n50 84.3 84.3 84.3 84.3 84.7 84.7\n100 85.9 85.8 85.9 85.8 86.3 86.2\n250 87.6 87.6 87.6 87.6 87.9 87.9\n500 88.5 88.4 88.5 88.4 88.6 88.6\nTable 4: Multilingual FS-XLT transfer results. Please refer to §5 for details.\n10742",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.8113691210746765
    },
    {
      "name": "Computer science",
      "score": 0.7612806558609009
    },
    {
      "name": "Modularity (biology)",
      "score": 0.6216617822647095
    },
    {
      "name": "Task (project management)",
      "score": 0.5816690921783447
    },
    {
      "name": "Fine-tuning",
      "score": 0.56345134973526
    },
    {
      "name": "Language model",
      "score": 0.5082539916038513
    },
    {
      "name": "Transfer (computing)",
      "score": 0.48800674080848694
    },
    {
      "name": "Stability (learning theory)",
      "score": 0.4841476082801819
    },
    {
      "name": "Shot (pellet)",
      "score": 0.4546939432621002
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4534878134727478
    },
    {
      "name": "Yield (engineering)",
      "score": 0.4309895634651184
    },
    {
      "name": "Natural language processing",
      "score": 0.4235371947288513
    },
    {
      "name": "Variation (astronomy)",
      "score": 0.4213946461677551
    },
    {
      "name": "Transfer of learning",
      "score": 0.4190567433834076
    },
    {
      "name": "Machine learning",
      "score": 0.22239834070205688
    },
    {
      "name": "Materials science",
      "score": 0.15512919425964355
    },
    {
      "name": "Physics",
      "score": 0.10672259330749512
    },
    {
      "name": "Parallel computing",
      "score": 0.07998156547546387
    },
    {
      "name": "Astrophysics",
      "score": 0.07710808515548706
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Artificial neural network",
      "score": 0.0
    },
    {
      "name": "Metallurgy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I25974101",
      "name": "University of Würzburg",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    }
  ]
}