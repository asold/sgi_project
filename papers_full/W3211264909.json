{
  "title": "Tunet: A Block-Online Bandwidth Extension Model Based On Transformers And Self-Supervised Pretraining",
  "url": "https://openalex.org/W3211264909",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2125317246",
      "name": "Nguyen Viet Anh",
      "affiliations": [
        "FPT University"
      ]
    },
    {
      "id": "https://openalex.org/A4308831582",
      "name": "Nguyen, Anh H. T.",
      "affiliations": [
        "FPT University"
      ]
    },
    {
      "id": "https://openalex.org/A4221868721",
      "name": "Khong, Andy W. H.",
      "affiliations": [
        "Nanyang Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3102195007",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6770514103",
    "https://openalex.org/W6783944145",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3035414321",
    "https://openalex.org/W6773206665",
    "https://openalex.org/W2892110446",
    "https://openalex.org/W6769767169",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W2034562896",
    "https://openalex.org/W6767367760",
    "https://openalex.org/W6741681139",
    "https://openalex.org/W3197990672",
    "https://openalex.org/W3197334236",
    "https://openalex.org/W26348862",
    "https://openalex.org/W2914105075",
    "https://openalex.org/W6772349387",
    "https://openalex.org/W2964058413",
    "https://openalex.org/W6748150159",
    "https://openalex.org/W6785764544",
    "https://openalex.org/W4287118891",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963952344",
    "https://openalex.org/W2787300193",
    "https://openalex.org/W2970844204",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3015338123",
    "https://openalex.org/W3161480375",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W2972745527",
    "https://openalex.org/W2964176953",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2998572311",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2988736778",
    "https://openalex.org/W4214784181",
    "https://openalex.org/W3003875258",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "We introduce a block-online variant of the temporal feature-wise linear modulation (TFiLM) model to achieve bandwidth extension. The proposed architecture simplifies the UNet backbone of the TFiLM to reduce inference time and employs an efficient transformer at the bottleneck to alleviate performance degradation. We also utilize self-supervised pretraining and data augmentation to enhance the quality of bandwidth extended signals and reduce the sensitivity with respect to downsampling methods. Experiment results on the VCTK dataset show that the proposed method outperforms several recent baselines in both intrusive and non-intrusive metrics. Pretraining and filter augmentation also help stabilize and enhance the overall performance.",
  "full_text": "arXiv:2110.13492v5  [cs.LG]  7 Jun 2022\nTUNET : A BLOCK-ONLINE BANDWIDTH EXTENSION MODEL BASED ON\nTRANSFORMERS AND SELF-SUPERVISED PRETRAINING\nV iet-Anh Nguyen1, Anh H. T . Nguyen1, and Andy W . H. Khong2\n1NextG, FPT Software, V ietnam\n2Nanyang T echnological University, Singapore\n{anhnv79, anhnht3}@fsoft.com.vn, andykhong@ntu.edu.sg\nABSTRA CT\nW e introduce a block-online variant of the temporal feature -\nwise linear modulation (TFiLM) model to achieve bandwidth\nextension. The proposed architecture simpliﬁes the UNet\nbackbone of the TFiLM to reduce inference time and em-\nploys an efﬁcient transformer at the bottleneck to alleviat e\nperformance degradation. W e also utilize self-supervised\npretraining and data augmentation to enhance the quality of\nbandwidth extended signals and reduce the sensitivity with\nrespect to downsampling methods. Experiment results on the\nVCTK dataset show that the proposed method outperforms\nseveral recent baselines in both intrusive and non-intrusi ve\nmetrics. Pretraining and ﬁlter augmentation also help stab i-\nlize and enhance the overall performance.\nIndex T erms— Bandwidth extension, transformer, self-\nsupervised pretraining, speech enhancement\n1. INTRODUCTION\nBandwidth extension (BWE), or audio super-resolution, en-\nhances speech by generating a wideband (WB) signal from a\nnarrowband (NB) signal. The NB signal is usually sampled\nbelow 8 kHz resulting in low auditory quality. Such sampling\nrate is widely used in G.711, G.729, and AMR audio codecs\ndue to its efﬁcient streaming. Including a BWE module at the\nreceiver side will therefore improve audio ﬁdelity.\nCompared to conventional BWE approaches such as [1,\n2, 3], recent end-to-end deep neural networks generate WB\nsignals directly from NB signals without the need for fea-\nture engineering. For instance, inspired by the well-known\nUNet architecture [4] in image processing, AudioUNet [5] is\na wave-to-wave BWE model that has outperformed traditional\nmethods. In [6], the limitation of convolution on long-rang e\ndependency modeling in UNet is addressed by introducing the\nTFiLM layer that modulates blocks of convolution’s feature\nmaps with information learned by recurrent layers. Genera-\ntive models such as the NU-W ave [7] neural vocoder relies on\nconditional diffusion models with modiﬁed noise level em-\nbedding and local conditioner. On the other hand, WSRGlow\n[8] models the distribution of the output conditioned on the\ninput using normalizing ﬂow .\nWhile convolutional neural network architectures exhibit\npromising results for end-to-end BWE training, their effec -\ntiveness on long-range dependency modeling is still limite d\nby receptive ﬁelds of convolution [9]. Stacking more con-\nvolution layers would help expand the receptive ﬁeld at the\nexpense of increased computation. In addition, training en d-\nto-end BWE models requires high-rate target signals, makin g\nvaluable low-rate data collected from telephony 8-kHz infr as-\ntructure unusable. It has also been observed that BWE models\nare susceptible to low-pass ﬁltering [6, 10], generating se vere\ndistortion at the transition band of the anti-aliasing ﬁlte r. This\nproblem can be mitigated by data augmentation [10].\nW e propose a Transformer-aided UNet (TUNet) 1 by\nemploying a low-complexity transformer encoder on the\nbottleneck of a lightweight UNet. Here, the Transformer\nassists such a small UNet with its captured global depen-\ndency while the UNet effectively downsamples waveform\ninput with strided convolution to reduce computation that t he\nTransformer must perform. In addition, inspired by masked\nlanguage modeling in natural language processing [11], we\npropose masked speech modeling — a self-supervised repre-\nsentation learning scheme that reconstructs original sign als\nfrom masked signals. The advantage of this pretraining is th at\nit requires only low-rate data to make full use of telephony\ndatabases, allowing the model to learn the underlying stati s-\ntics of the low-band speech and generalize to downstream\ntasks [12]. Finally, similar to [10], we make our model robus t\nto downsampling methods by generating training data with\ndifferent parameter sets of the Chebyshev T ype I ﬁlter.\n2. REVIEW OF TFILM-UNET AND PROPOSED\nTUNET ALGORITHM\n2.1. TFiLM-UNet baseline\nTFiLM-UNet is an ofﬂine UNet-based audio super-resolution\nmodel [6]. T o assist convolution layers in capturing long-\nrange information, T emporal Feature-Wise Linear Modula-\ntion (TFiLM) has been proposed. This layer acts as a nor-\nmalization layer that combines maxpooling and long short-\n1 Source code and audio samples: https://github.com/NXTProduct/TUNet\nLin = 8192\nTransformer\nTConv(C=128, K=8)\nConv(C=256, K=8)\nTFiLM(B=64)\nTFiLM(B=64)\nTFiLM(B=64)\nTFiLM(B=64)\nTConv(C=64, K=18)\nTConv(C=1, K=66)\nConv(C=128, K=18)\nConv(C=64, K=66)\nLout = 8192\nFig. 1. TUNet architecture for speech enhancement. The encoder do wnsamples waveform input while the decoder does the\nreverse. A Transformer block is placed in the middle to model the attention of the bottleneck.\nterm memory (LSTM). While maxpooling reduces temporal\ndimension into B blocks, LSTMs reﬁne convolution’s feature\nmaps by captured long-range dependency.\nIn the TFiLM-UNet model, the encoder contains four\ndownsampling (D) blocks, each comprising a convolution\nlayer, maxpooling layer, ReLU activation, and TFiLM layer,\nconsecutively. In the decoder, upsampling (U) blocks fol-\nlow sequential operations: convolution, dropout, ReLU,\nDimShufﬂe, and TFiLM, in which the DimShufﬂe layer\ndoubles time dimension by manipulating the feature shape.\nStacking and additive skip connections are applied between\nD/U blocks and input/output, respectively.\n2.2. Lightweight UNet with T ransformer\nWith reference to Fig. 1, our proposed model follows the\nsame waveform-to-waveform UNet to that of TFiLM. As op-\nposed to TFiLM-UNet, the proposed model is signiﬁcantly\nsmaller due to the use of fewer convolution ﬁlters and higher\ndimensional reduction rates. Precisely, the encoder consi sts\nof three strided 1D convolution layers, each having C ﬁl-\nters of kernel size K. Stride S of all these layers is set at\n4, resulting in the time dimension of the bottleneck being\n64 times shorter than the length Lin of the input. Conse-\nquently, the bottleneck features can be processed efﬁcient ly in\nthe follow-up Performers [13] blocks. W e employ Performers\nsince its self-attention mechanism has linear time complex ity\ncompared to the quadratic complexity of the conventional at -\ntention [14]. On the decoder side, three transposed 1D con-\nvolution layers commensurating the downsampling rates of\nthe encoder are used to generate output signals that have the\nlength Lout = Lin. W e use T anh activation for the last trans-\nposed convolution and LeakyReLU [15] for the rest. TFiLM\nlayers are applied after convolution layers except for the l ast\nencoder layer that is replaced by the Performer blocks. T o\nsmooth the loss landscape [16], skip connections that con-\nnect TFiLM encoders to the corresponding decoders are em-\nployed.\nCompared to the TFiLM-UNet, our model has four key\ndifferences: i) Our encoder and decoder require one fewer\nlayer and four times fewer ﬁlters than the baseline; ii) Each\nencoding layer reduces time dimension by four times instead\nof two to assist quick input compression; iii) W e replace\nFig. 2. Masked speech modeling pretraining pipeline.\ndownsampling and upsampling blocks in TFiLM with strided\nconvolution and transposed convolution layers, respectiv ely;\nand iv) compared to the stacking skip-connection in TFiLM,\nwe employ additive skip connection which further reduces th e\nnumber of parameters in the decoder. These modiﬁcations\nensure that our model is signiﬁcantly lighter than the basel ine\nwhile preserving learning capability.\n2.3. Masked speech modeling\nW e propose masked speech modeling (MSM) pretraining as\nillustrated in Fig. 2. Since audio signals possess ﬁne granu -\nlar characteristics, instead of masking the sequence at sam -\nple level, we mask 20% of 256-sample blocks to create the\nmasked input. The model will optimize the mean squared er-\nror between the output and the masked input. Compared to\nthe masked reconstruction pretraining in [17], both encode r\nand decoder are pretrained in our proposed approach.\n2.4. Improving robustness to downsampling methods by\naugmentation\nThe performance of BWE models is highly sensitive to differ-\nent anti-aliasing ﬁlters when downsampling methods in test -\ning differ from training [5, 6, 10]. Similar to [10], to impro ve\nthe robustness of our model, we generate the low-rate signal s\nby downsampling the high-rate speech dataset with random\nanti-aliasing ﬁlters. More speciﬁcally, we adopt the Cheby -\nshev T ype I anti-aliasing ﬁlter and randomize its ripple and\norder parameters. This helps in creating variations in the t ran-\nsition band of the anti-aliasing ﬁlter.\n2.5. Learning objectives\nSince the mean squared error (MSE) loss may not guarantee\nthe good perceptual quality [18], we combine MSE loss with\nmulti-resolution short-time Fourier transform (STFT) los s\n[19] in the Mel scale. Given a reconstructed signal ˆy and a\ntarget signal y, the training loss is given by\nℓ(ˆy, y) =ℓMR(ˆy, y) +α MSE(ˆy, y), (1)\nwhere α denotes the weight of the MSE loss, and ℓMR is the\nmulti-resolution STFT loss (MR loss).\n3. EXPERIMENTS\n3.1. Setup\nW e focus on extending 4-kHz bandwidth (8 kHz sampling\nrate) to 8-kHz bandwidth (16 kHz sampling rate). Training\ndata was segmented into smaller chunks with a window size\nof 8192 and 50% overlapping. W e used the VCTK Corpus\n[20] for training and testing. This dataset includes 109 En-\nglish speakers, in which recordings of the ﬁrst 100 speakers\nwere for training and the remaining for testing.\nBesides VCTK, we further used the VIVOS dataset [21]\nto verify the effectiveness of our pretraining approach. Th is\ndataset consists of 15-hour speech recordings from 65 V iet-\nnamese speakers, recorded in a quiet environment with high-\nquality microphones. W e followed the dataset’s default spl it:\n46 speakers for training, 19 speakers for testing.\nT o evaluate the quality of the generated audio, we used\nfour metrics: log-spectral distance (LSD), high-frequenc y\nlog-spectral distance (LSD-HF), scale-invariant source- to-\ndistortion ratio (SI-SDR) [22], and DNSMOS based on P .808\ncriterion [23]. LSD-HF computes LSD speciﬁcally on high-\nfrequency bands, i.e., 4kHz - 8 kHz. As opposed to LSD,\nLSD-HF focuses only on the regeneration of the high-band\nspectrum and ignores artifacts or distortions in the low-ba nd\nspectrum. A lower LSD/LSD-HF score implies a more sim-\nilar spectral to the target, while a higher SI-SDR score in-\ndicates better performance. On the other hand, DNSMOS\nemploys a deep learning model to predict the mean-opinion-\nscore (MOS) of human raters. It has been shown to have\nexcellent correlation to MOS [23]. A higher value of DNS-\nMOS indicates better speech quality.\nThe C, K, S, and B hyperparameters of our model are\ndescribed in Fig. 1. The Performers 2 block has three hidden\nlayers, two attention heads for each layer, and each head’s\ndimension is 32; local window length is equivalent to bottle -\nneck length divided by 8. Hyperparameters of MR loss such\nas resolutions were set with default values of the auraloss3\nv2.0.1 library. The MSE weight was set to α = 10000. W e\ntrained our models for 150 epochs using the Adam optimizer,\n3 × 10− 4 learning rate with 800 samples in each batch. For\n2 https://github.com/lucidrains/performer-pytorch\n3 https://github.com/csteinmetz1/auraloss\nFig. 3. Metric scores of the baselines and our model. Lower\nLSD/ LSD-HF is better, and higher DNSMOS is better.\nT able 1. Model size and inference time on a single core CPU.\nSystem #Params Inference time\n(ms)\nWSRGlow 229M 3146\nNU-W ave 3M 2431 (8 iters)\nTFiLM-UNet 68.2M 1335\nTUNet 2.9M 22.6\nthe baseline TFiLM-UNet model, while ofﬁcial implementa-\ntion is available, we adopted an unofﬁcial implementation 4\nwhich reportedly produces slightly better results and much\nfaster training.\n3.2. Performance comparison with baselines\nW e compared our model’s performance and inference speed\nwith TFiLM-UNet and two recent generative models, NU-\nW ave [7] and WSRGlow [8]. The above baselines were\ntrained on the VCTK dataset with low-rate data generated\nfrom 16-kHz data using only one 8th order Chebyshev T ype\nI low-pass ﬁlter. In this experiment, MSM pretraining was\nexcluded from our method.\nResults in Fig. 3 show that our TUNet model achieved\nsigniﬁcantly higher performance than that of all the baseli nes.\nCompared to our TUNet, the WSRGlow model achieves tight\nLSD-HF scores but relatively worse in LSD, indicating that\nour model better preserves low frequencies. Despite the wor st\nLSD score, the NU-W ave model achieves a considerable im-\nprovement in DNSMOS only after our proposed model.\nIn terms of single-threaded inference time, we measured it\non the AMD EPYC 7742 using ONNX inference engine. Our\nproposed model was signiﬁcantly faster and more lightweigh t\nthan the others. In T able 1, TUNet requires only 22.63 ms to\n4 github.com/leolya/Audio-Super-Resolution-T ensorﬂow2.0-TFiLM\nT able 2. Effectiveness of components on our model.\nModel LSD LSD-HF SI-SDR\nNo Transformer 1.45 2.64 21.61\nLSTMs bottleneck 1.44 2.70 21.76\nNo TFiLM 1.44 2.69 21.89\nTUNet 1.36 2.54 21.91\nexecute a single 512 ms audio frame while WSRGlow , NU-\nW ave (the default eight inference steps) and TFiLM-UNet\ntook approximately 139, 107, and 59 times longer, respec-\ntively. Assuming each audio chunk being 87.5% overlapped,\nthis amounts to 64 ms for a new block to arrive with a chunk\nsize of 8192 and a sampling rate of 16 kHz. Since our infer-\nence time is shorter than 64 ms, this implies that the propose d\nmethod is more suited for semi-real-time applications com-\npared to the baselines.\n3.3. Ablation studies\nT o study the effects of its two main components, TFiLM lay-\ners and Performers blocks, we created three variations from\nTUNet: ‘No T ransformer’ — TUNet without Performers\nblocks on the bottleneck, ‘LSTMs bottleneck’ — TUNet with\nthe Transformer bottleneck replaced by a 3-layer, 256-unit\n(same as the Transformer) LSTM network, and ‘No TFiLM’\n— TUNet without TFiLM layers.\nIn T able 2, both Performer and TFiLM layers play signif-\nicant roles in the proposed model since excluding these two\ncomponents led to noticeably decreased scores on all met-\nrics. The ‘No Transformer’ model, which excluded the Trans-\nformer from the bottleneck, performed worst in terms of LSD\nand SI-SDR, and the performance was only improved by a\nsmall margin even with LSTMs aided. The removal of TFiLM\nalso led to a signiﬁcant degradation but relatively less tha n the\nremoval of the Transformer.\nT o determine the effectiveness of MSM pretraining, we\npretrained TUNet on VCTK low-rate data with the pipeline\ndescribed in Section 2.3. After obtaining a pretrained mode l,\nwe subsequently trained it with the BWE task on the VCTK\ndataset. In this experiment, we used only one anti-aliasing ﬁl-\nter in Section 3.2 to generate training data. T o assess the ge n-\neralization ability of MSM, we include an additional scenar io\nwhere the pretraining dataset is VCTK, but the BWE training\nand test set are of a different language. W e adopted one more\nmetric — low-frequency log-spectral distance (LSD-LF) to\nmeasure the approximation error in the low band (0-4 kHz)\ncaused by MSM pretraining.\nResults in T able 3 show that models pretrained with MSM\nachieve signiﬁcant improvements on spectral-based metric s\nwhile SI-SDR ﬁgures were modest. The scores indicate that\nthe pretraining scheme not only enhanced high frequencies\nbut also helped preserve low frequencies. Furthermore, the\nperformance gain on the VIVOS was consistent with that of\nthe VCTK. This implies that the BWE model adapted very\nwell to the VIVOS dataset even though it was pretrained on a\nT able 3. BWE results on VCTK and VIVOS datasets when\nemploying MSM pretraining.\nModel LSD LSD-HF LSD-LF SI-SDR\nVCTK\ninput 4.75 8.27 1.23 20.32\nw/o MSM 1.36 2.54 0.18 21.69\nMSM on VCTK 1.28 2.45 0.11 22.08\nVIVOS\ninput 5.59 9.79 1.39 21.75\nw/o MSM 1.36 2.49 0.23 25.08\nMSM on VCTK 1.29 2.42 0.16 26.15\nFig. 4 . LSD scores of our models trained with a single and\nmultiple anti-aliasing ﬁlter(s) on the VIVOS test set.\ndifferent language.\nW e next assessed sensitiveness to anti-aliasing ﬁlters of\nour models trained with and without ﬁlter augmentation.\nThe ﬁrst model, ‘Single Cheby’ is the best model obtained\nfrom the above experiments, which was trained with a single\nChebyshev T ype I anti-aliasing ﬁlter. The other ‘Multi-\nCheby’ was trained with a set of random ﬁlters as described\nin Section 2.4. Both models employed the same MSM pre-\ntraining above. The BWE dataset used for this experiment\nwas the VIVOS dataset. The test set was downsampled us-\ning all resampling methods available in the resampy5 library.\nHowever, due to space constraints, we will only report the\nresults on test sets generated by single/multiple Chebyshe v\nﬁlters (same as training of ‘Single Cheby’ and ‘Multi-Cheby ’,\nrespectively), Kaiser (‘best’ and ‘fast’ variations) ﬁlte rs, and\nthe sinc downsampling.\nAs shown in Fig. 4, the ‘Single Cheby’ model achieved\nthe best score when evaluated with the same ﬁlter. Although\nthis model performed well on several downsampling methods\nsuch as ‘kaiser\nfast’, its performance signiﬁcantly degraded\non test sets processed by the other downsampling methods\nsuch as the sinc algorithm. On the other hand, the ‘Multi-\nCheby’ showed a stable performance across all the methods.\n4. CONCLUSIONS\nW e have proposed a Transformer-aided UNet for band-\nwidth extension. Despite remarkable performance scores,\nour model remains lightweight and achieves fast processing .\nBy leveraging only narrowband audio data for pretraining,\nwe have achieved an overall improvement in performance.\nWith multiple anti-aliasing ﬁlters applied, the model achi eves\nrobustness to different low-pass ﬁlters, an essential char acter-\nistic for real-world applications.\n5 https://github.com/bmcfee/resampy\n5. REFERENCES\n[1] Y . Qian and P . Kabal, “Wideband speech recov-\nery from narrowband speech using classiﬁed codebook\nmapping, ” in Proc. Australian Int. Conf. Speech Sci.,\nT echnol. (Melbourne), 2002.\n[2] A. H. Nour-Eldin and P . Kabal, “Mel-frequency cep-\nstral coefﬁcient-based bandwidth extension of narrow-\nband speech, ” in Proc. Interspeech, 2008.\n[3] P . Jax and P . V ary, “On artiﬁcial bandwidth extension of\ntelephone speech, ” Signal Processing, vol. 83, 2003.\n[4] O. Ronneberger, P . Fischer, and T . Brox, “U-Net: Con-\nvolutional networks for biomedical image segmenta-\ntion, ” in Proc. Med. Image Comput. Comput. Assist.\nInterv ., 2015.\n[5] V . Kuleshov, S. Z. Enam, and S. Ermon, “ Audio super\nresolution using neural networks, ” in Int. Conf. Learn.\nRepresentations, W orkshop T rack, 2017.\n[6] S. Birnbaum, V . Kuleshov, S. Z. Enam, P . W . Koh, and\nS. Ermon, “T emporal FiLM: Capturing Long-Range Se-\nquence Dependencies with Feature-Wise Modulations, ”\nin Proc. Neural Inf. Process. Syst., 2019.\n[7] J. Lee and S. Han, “NU-W ave: A Diffusion Probabilis-\ntic Model for Neural Audio Upsampling, ” in Proc. In-\nterspeech, 2021.\n[8] K. Zhang, Y . Ren, C. Xu, and Z. Zhao, “WSRGlow:\nA Glow-based waveform generative model for audio\nsuper-resolution, ” in Proc. Interspeech, 2021.\n[9] D. Linsley, J. Kim, V . V eerabadran, C. Windolf, and\nT . Serre, “Learning long-range spatial dependencies\nwith horizontal gated recurrent units, ” in Proc. Neural\nInf. Process. Syst., 2018, p. 152–164.\n[10] S. Sulun and M. E. P . Davies, “On ﬁlter generalization\nfor music bandwidth extension using deep neural net-\nworks, ” IEEE J. of Sel. T opics in Signal Process., vol.\n15, no. 1, Jan 2021.\n[11] J. Devlin, M.-W . Chang, K. Lee, and K. T outanova,\n“BER T: Pre-training of deep bidirectional transform-\ners for language understanding, ” in Proc. North Amer .\nChapter Assoc. Comput. Linguistics, 2019.\n[12] A. Baevski, M. Auli, and A. rahman Mohamed, “Ef-\nfectiveness of self-supervised pre-training for speech\nrecognition, ” ArXiv, vol. abs/1911.03912, 2019.\n[13] K. M. Choromanski, V . Likhosherstov, D. Dohan,\nX. Song, A. Gane, T . Sarlos, P . Hawkins, J. Q. Davis,\nA. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell,\nand A. W eller, “Rethinking attention with Performers, ”\nin Proc. Int. Conf. Learn. Representations, 2021.\n[14] A. V aswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,\n“ Attention is all you need, ” in Proc. Neural Inf. Process.\nSyst., 2017.\n[15] A. L. Maas, A. Y . Hannun, and A. Y . Ng, “Rectiﬁer\nnonlinearities improve neural network acoustic models, ”\nin ICML W orkshop on Deep Learn. for Audio, Speech\nand Lang. Process., 2013.\n[16] L. W ang, B. Shen, N. Zhao, and Z. Zhang, “Is the skip\nconnection provable to reform the neural network loss\nlandscape?, ” in Proc. Int. Joint Conf. Artif. Intell., 2020.\n[17] W . W ang, Q. T ang, and K. Livescu, “Unsupervised pre-\ntraining of bidirectional speech encoders via masked re-\nconstruction, ” in Proc. IEEE Int. Conf. Acoust., Speech,\nSignal Process., 2020.\n[18] J. Mart´ ın-Do ˜ nas, A. Gomez, J. Gonzalez Lopez, and\nA. Peinado, “ A deep learning loss function based on\nthe perceptual evaluation of the speech quality, ” IEEE\nSignal Process. Lett., vol. PP , pp. 1–1, 09 2018.\n[19] R. Y amamoto, E. Song, and J.-M. Kim, “Parallel W ave-\ngan: A fast waveform generation model based on gen-\nerative adversarial networks with multi-resolution spec-\ntrogram, ” in Proc. IEEE Int. Conf. Acoust., Speech, Sig-\nnal Process., 2020.\n[20] J. Y amagishi, C. V eaux, and K. MacDonald, “CSTR\nVCTK Corpus: English multi-speaker corpus for CSTR\nvoice cloning toolkit (version 0.92), ” University of Ed-\ninburgh. The Centre for Speech T echnology Research\n(CSTR), 2019.\n[21] H. T . Luong and H. Q. V u, “ A non-expert Kaldi\nrecipe for V ietnamese speech recognition system, ” in\nWLSI/OIAF4HLT@COLING, 2016.\n[22] J. L. Roux, S. Wisdom, H. Erdogan, and J. R. Hershey,\n“SDR – Half-baked or well done?, ” in Proc. IEEE Int.\nConf. Acoust., Speech, Signal Process., 2019.\n[23] C. K. Reddy, V . Gopal, and R. Cutler, “DNSMOS: A\nnon-intrusive perceptual objective speech quality met-\nric to evaluate noise suppressors, ” arXiv e-prints , pp.\narXiv–2010, 2020.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7793667912483215
    },
    {
      "name": "Upsampling",
      "score": 0.7679204940795898
    },
    {
      "name": "Bandwidth extension",
      "score": 0.608817994594574
    },
    {
      "name": "Transformer",
      "score": 0.5537942051887512
    },
    {
      "name": "Bottleneck",
      "score": 0.5134630799293518
    },
    {
      "name": "Bandwidth (computing)",
      "score": 0.5062379240989685
    },
    {
      "name": "Inference",
      "score": 0.4885631501674652
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45788681507110596
    },
    {
      "name": "Machine learning",
      "score": 0.3863256573677063
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3302285671234131
    },
    {
      "name": "Speech recognition",
      "score": 0.2213243842124939
    },
    {
      "name": "Engineering",
      "score": 0.10716313123703003
    },
    {
      "name": "Audio signal",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Speech coding",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ]
}