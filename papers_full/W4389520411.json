{
    "title": "Does the English Matter? Elicit Cross-lingual Abilities of Large Language Models",
    "url": "https://openalex.org/W4389520411",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3098811994",
            "name": "Leonardo Ranaldi",
            "affiliations": [
                "University of Rome Tor Vergata"
            ]
        },
        {
            "id": "https://openalex.org/A2101661056",
            "name": "Giulia Pucci",
            "affiliations": [
                "University of Rome Tor Vergata"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6601671322",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W4385570522",
        "https://openalex.org/W4318903120",
        "https://openalex.org/W4385474178",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W3200898860",
        "https://openalex.org/W4385573174",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W4281609260",
        "https://openalex.org/W4364387438",
        "https://openalex.org/W4221164384",
        "https://openalex.org/W4381711094",
        "https://openalex.org/W3121904249",
        "https://openalex.org/W2982407184",
        "https://openalex.org/W4378771129",
        "https://openalex.org/W4383989305",
        "https://openalex.org/W4313585449",
        "https://openalex.org/W4401043228",
        "https://openalex.org/W3035497479",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4389524372",
        "https://openalex.org/W2964331441",
        "https://openalex.org/W3172698324",
        "https://openalex.org/W3104108820",
        "https://openalex.org/W4319793302",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W4390653558",
        "https://openalex.org/W4389524450",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W630532510",
        "https://openalex.org/W4385572634",
        "https://openalex.org/W4366327277",
        "https://openalex.org/W3106321930",
        "https://openalex.org/W4378468481",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W4385571157",
        "https://openalex.org/W4285247752"
    ],
    "abstract": "Large Language Models reveal diverse abilities across different languages due to the disproportionate amount of English data they are trained on.Their performances on English tasks are often more robust than in other languages.In this paper, we propose a method to empower the cross-lingual abilities of instructiontuned LLMs (It-LLMs) by building semantic alignment between languages.To achieve this, we introduce translation-following demonstrations to elicit better semantic alignment across languages.Our evaluations on multilingual question-answering benchmarks reveal that our models, tested in five distinct languages, outperform the performance of It-LLMs trained on monolingual datasets.The findings highlight the impact of translation-following demonstrations on non-English data, eliciting instructiontuning and empowering semantic alignment.",
    "full_text": "Proceedings of the The 3rd Workshop on Multi-lingual Representation Learning (MRL), pages 173–183\nDecember 7, 2023 ©2023 Association for Computational Linguistics\nDoes the English Matter? Elicit Cross-lingual Abilities\nof Large Language Models\nGiulia Pucci and Leonardo Ranaldi\nHuman-Centric ART Group, Department of Enterprise Engineering,\nUniversity of Rome Tor Vergata.\n[first_name].[last_name]@uniroma2.it,\nAbstract\nLarge Language Models reveal diverse abilities\nacross different languages due to the dispropor-\ntionate amount of English data they are trained\non. Their performances on English tasks are\noften more robust than in other languages.\nIn this paper, we propose a method to em-\npower the cross-lingual abilities of instruction-\ntuned LLMs (It-LLMs) by building semantic\nalignment between languages. To achieve this,\nwe introduce translation-following demonstra-\ntions to elicit better semantic alignment across\nlanguages. Our evaluations on multilingual\nquestion-answering benchmarks reveal that our\nmodels, tested in five distinct languages, out-\nperform the performance of It-LLMs trained on\nmonolingual datasets. The findings highlight\nthe impact of translation-following demonstra-\ntions on non-English data, eliciting instruction-\ntuning and empowering semantic alignment.\n1 Introduction\nLarge Language Models (LLMs) achieve compre-\nhensive language abilities through pre-training on\nlarge corpora (Brown et al., 2020). Hence, the\nacquired language abilities follow the corpora fea-\ntures, primarily available in English (Lin et al.,\n2021; Zhang et al., 2023; Zhu et al., 2023). This\nphenomenon produces an imbalance in pre-training\n(Blevins and Zettlemoyer, 2022) and fine-tuning\n(Le et al., 2021). Thus, performance is usually\nlower for non-English languages, especially for\nlow-resource ones (Huang et al., 2023; Bang et al.,\n2023). The most common approaches to mitigate\nthis problem propose continuing pre-training with\nlarge-scale monolingual data (Imani et al., 2023;\nCui et al., 2023; Yang et al., 2023), which requires\nconsiderable data and computational resources.\nIn this paper, we propose an approach to em-\npower the It-LLM that elicits semantic alignment\nbetween English and other languages. We fo-\ncus on exploiting the latent multilingual abili-\nties of It-LLMs by empowering the pivotal phase\nof instruction-tuning using instruction-following\ndemonstrations. To this end, we explore the po-\ntential of cross-lingual alignment by integrating\ntranslation-following demonstrations to refine the\ninstruction-tuning process.\nIn our experiments, we use Llama-7b (Touvron\net al., 2023) as the foundational LLM and target\nfive languages. In instances where data is lacking,\nwe undertake translation tasks. We use the Stanford\nAlpaca dataset (Taori et al., 2023) and its translated\nversions in the corresponding languages, while for\nthe translation-following, we use a publicly avail-\nable translation resource (Tiedemann, 2012), the\nmost accessible and extendable to multiple lan-\nguages (i.e., translation-following demonstrations\non Figure 1).\nFollowing the instruction-tuning phase, we as-\nsessed the efficacy of our five distinct Alpaca tai-\nlored for specific languages. Our evaluation lever-\naged four benchmarks: two inherently multilingual,\ni.e., XQUAD (Artetxe et al., 2019) and MLQA\n(Lewis et al., 2020), and two intrinsically monolin-\ngual, MMLU (Hendrycks et al., 2021) and BBH\n(Suzgun et al., 2022). The empirical results in-\ndicate that when trained using language-specific\ninstructions combined with translation data, the\ninstruction-tuned models significantly surpass the\nperformance of models trained exclusively with\nnon-English demonstrations. While our models\nbridge the gap among performances, the translation-\nfollowing models exhibit optimal alignments. This\nhighlights the pronounced proficiency of Llama\nwhen trained on English-centered datasets com-\npared to non-English ones. Furthermore, the se-\nmantic alignment effort significantly strengthens\nthe cross-lingual abilities of It-LLMs.\nOur findings can be summarized as follows:\n• The learning abilities of LLMs on non-English\ninstruction-tuning tasks are limited;\n• The multi-lingual abilities of instruction-tuned\n173\nFigure 1: Our x-CrossLlama are instruction-tuned on instruction-following and translation-following demonstrations.\nLLMs could be empowered through cross-\nlingual alignment;\n• Thus, we propose to elicit the instruction-\ntuning approach for non-English models\nbased on instruction-following and translation-\nfollowing demonstrations for the target lan-\nguage. Hence, we show that It-LLMs\ncan semantically align through cross-lingual\ntranslation-following demonstrations via an\nextensive evaluation.\n2 Methods\nPre-training from scratch a Large Language Model\n(LLM) to fill the imbalance language problem is\ncost-prohibitive for data collection and parameter\nlearning. This is why the trend is to do further\nfine-tuning to empower the models’ abilities in a\nspecific language (Tanti et al., 2021; Moslem et al.,\n2023). Hence, we aim to elicit the abilities of pre-\ntrained LLMs for non-English languages by fur-\nther improving the alignment between English and\nthe target language. In the following Sections, we\ninvestigate the difficulties of fine-tuning a mono-\nlingual scenario (Section 2.1). Based on this, we\npropose our approach to empower the cross-lingual\nabilities of It-LLMs (Section 2.2).\n2.1 Alpaca Instruction-tuning\nThe restricted availability and clarity of premium\nAPI services for cutting-edge LLMs have driven\nresearchers to focus on creating open-source al-\nternatives. Using the instruction-tuning paradigm,\npresented in Section 5.2, and resources as Stanford\nAlpaca (Taori et al., 2023) that is a corpus consist-\ning of 52k of English instruction-output pairs gen-\nerated by text-davinci-003, several instruction-\ntuned versions of instructed-Llama were released.\nFollowing this approach, multiple monolingual\nversions of instructed-Llama were proposed by\ntranslating the Stanford Alpaca data into the spe-\ncific language. Table 1 shows a set of versions\navailable as open source. Following an analysis\nof the translated versions of instructed-Llama in\nofficial repositories1, the languages of the bench-\nmark datasets, and the translation pairs present in\nnews_commentary, which will be introduced later,\nwe selected the speeches that share the most al-\nready available data. Table 1 shows the custom\nversions used in this work, which for simplicity\nwill be renamed x-MultiLlama, where x indicates\nthe specific language.\n2.2 Cross-lingual Instruction-tuning\nAlthough monolingual techniques (presented in\nSection 2.1) play a key role in enhancing the mul-\ntilingual strengths of LLMs, simply focusing on\ntranslated versions of Alpacas for specific lan-\nguages does not allow the non-English capabilities\n1official versions on https://github.com/tloen/\nalpaca-lora and https://huggingface.co/models\n174\nModel Language Name\nAlpaca (Taori et al., 2023) English en-Llama\nAlpaca-Chinese (Chen et al., 2023) Chinese zh-Llama\nCamoscio (Santilli and Rodolà, 2023) Italian it-Llama\nGerman (Thissen, 2023) German de-Llama\nArabic (Yasbok) Arabic ar-Llama\nTable 1: The monolingual Instruction-tuned Large Lan-\nguage Models that use a language-specific version of\nMultiLlama as instruction-tuning data.\nof LLMs to be exploited. To overcome this over-\nlaps, we present CrossLlama, shown in Figure 1).\nThis method empowers cross-lingual instruction-\ntuning by integrating translation-following demon-\nstrations. We aim to elicit LLMs’ English and non-\nEnglish abilities by stimulating a semantic align-\nment challenge.\nInstruction-following Although the version of\nthe Alpaca dataset is in English, there are many\nderivatives. However, derived versions of the Al-\npaca dataset, as described in 2.1, have been pro-\nduced with translation systems. Our work starts\nwith the instruction-tuned Llama on Alpaca (na-\ntive English) and its versions adapted for distinct\nlanguages (which we called x-MultiLlama). We\nalso propose the CrossLlama variations, built from\nAlpaca translations specific to each language and\naugmented with translations (explained further).\nWith this methodology, we intend to elicit the LLM\nbackbone’s capability to interpret multilingual in-\nstructions and ensure cross-lingual consistency.\nTranslation-following Challenge Using general\ninstruction information is a logical approach when\ncreating models to tackle multiple tasks guided by\ninstructions(Wang et al., 2023; Zeng et al., 2023).\nNevertheless, data from translations might aid in\ngrasping semantic alignment.\nWe use publicly available sentence-level trans-\nlation datasets, such as news_commentary (Tiede-\nmann, 2012), to construct the translation task in-\nstruction demonstrations. We also propose ex-\ntending this to additional languages, which we re-\nlease as an open-source dataset. In particular, for\neach specific language, we constructed specific sets\nof demonstrations. Hence, following the Alpaca\nstyle (Instruction, Input, and Output) (see Table\n1), we selected the same number of English to\nnon-English translations non-English to English\ntranslations.\n3 Experiments\nIn order to observe the English and non-English\nabilities of Large Language Models (LLMs) and\nthe impact of the instruction-tuning approach in\ncross-lingual scenarios, we propose CrossLlama.\nOur approach is based on instruction-tuning on\nlanguage-specific data augmented with a cross-\nlingual semantic alignment. Hence, we set several\nbaseline models explained in Section 3.1, which we\naugmented with our approach introduced in Section\n3.2. Finally, we performed a series of systematic\nevaluations (Section 3.3.1) to observe the impact\nof the proposed method.\n3.1 Baseline LLMs\nThe common denominator among the It-LLMs\nshown in Table 1 is the LLM backbone Llama-7b\n(Touvron et al., 2023). Starting from instruction-\nfollowing data from the original Alpaca (Taori\net al., 2023) and its open-source non-English ver-\nsions2, we reproduced x-MultiLlama for x specific\nlanguages: Chinese (zh), Italian (it), Arabic (ar),\nGerman (de) and the original English version (en).\n3.2 Cross-lingual LLMs\nOur method produces x-CrossLlama that are\ninstruction-tuned on standard instruction-following\nempowered with translation-following demonstra-\ntions.\nOur approach generates a series of instruction-\ntuned versions of the data shown in Figure 1. We\nhave named the versions x-CrossLlama.\n3.3 Experimental Setup\nTo assess the performance of the x-CrossLlama,\nwe defined several benchmarks (Section 3.3.1) on\nwhich we applied systematic instruction-tuning\npipelines in Section 3.3.2.\n3.3.1 Benchmarks\nTo evaluate the performance of the It-LLMs and\nthe impact of the semantic alignment approach, we\nused two cross-lingual (XQUAD (Artetxe et al.,\n2019), MLQA (Lewis et al., 2020)) and two multi-\ntask (MMLU (Hendrycks et al., 2021) and BBH\n(Suzgun et al., 2022)) benchmarks. While XQUAD\nand MLQA are very focused and require the model\nto reason about the given context and answer the\ngiven question, MMLU, and BBH are much more\n2open-source code is available on https://github.com/\ntloen/alpaca-lora\n175\nInstruction\nTranslate the following sentences from English to German.\nInput\nThe world as we have created it is a process of our thinking. It cannot be\nchanged without changing our thinking.\nOutput\nDie Welt, wie wir sie geschaffen haben, ist ein Prozess unseres Denkens. Es\nkann nicht geändert werden, ohne unser Denken zu ändern.\nInstruction\nTranslate the following sentences from German to English.\nInput\nDie Welt, wie wir sie geschaffen haben, ist ein Prozess unseres Denkens. Es\nkann nicht geändert werden, ohne unser Denken zu ändern.\nOutput\nThe world as we have created it is a process of our thinking. It cannot be\nchanged without changing our thinking.\nTable 2: Examples of translation-following demonstrations. In particular, in this example, there are two demonstra-\ntions with the same directions from English to German (en-x).\nopen but require the models’ ability to solve logical\nmathematical tasks less related to the language.\nHowever, we decided to introduce them to ob-\nserve whether our approach degrades performance\nin these tasks. The first two datasets selected are ap-\npropriately constructed for multi-language testing,\nwhile the second two are available only in English.\nHence, we do a preliminary translation step as out-\nlined below. Thus, descriptions of the benchmarks\nfollow in the next paragraphs:\nMultiLingual Question Answering (MLQA)\n(Lewis et al., 2020) evaluates cross-lingual question\nanswering performance using 5K extractive QA in-\nstances in the SQuAD (Rajpurkar et al., 2016) for-\nmat in several languages. MLQA is highly parallel,\nwith QA instances aligned across four languages\non average. Although comprising different lan-\nguages, some languages, such as Italian, are not\nrepresented. To conduct the experiments uniformly,\nwe have translated the examples as also done in the\nforthcoming MMLU and BBH.\nCross-lingual Question Answering Dataset\n(XQUAD) (Artetxe et al., 2019) consists of a sub-\nset of 240 paragraphs and 1190 question-answer\npairs from the development set of SQuAD v1.1 (Ra-\njpurkar et al., 2016) with their manual translations\ninto several languages. Consequently, the dataset\nis entirely parallel across 11 languages.\nMassive Multitask Language Understanding\n(MMLU) (Hendrycks et al., 2021) measures\nknowledge of the world and problem-solving prob-\nlems in multiple subjects with 57 subjects across\nSTEM, humanities, social sciences, and other areas.\nThe benchmark is native in English; however, we\ntranslated it into five additional languages3.\nBIG-Bench Hard (BBH) (Suzgun et al., 2022) is\na subset of challenging tasks related to navigation,\nlogical deduction, and fallacy detection. Again, the\nbenchmark is native English, and we have trans-\nlated it into five languages??.\n3.3.2 Models Setup & Evaluation\nWe used the alpaca_LoRA (Hu et al., 2021a) code2,\nadopting the same hyperparameters to align the\nresults with the state-of-the-art models.\nWe performed the fine-tuning with a single\nepoch and a batch-size of 128 examples, running\nour experiments on a workstation equipped with\none Nvidia RTX A6000 with 48 GB of VRAM.\nAs an evaluation metric, we use accuracy. Hence,\nwe estimate accuracy by measuring exact match\nvalues in the zero-shot setting. The parts of bench-\nmarks related to the specific language are used for\neach model.\n3We performed translations using the Google translator\nAPI from English to Chinese (zh), Italian (it), Arabic (ar), and\nGerman (de).\n176\nFigure 2: Accuracies (%) on proposed benchmarks. The dotted line represents the performance of the original\nversion of Llama instructed on English data (Taori et al., 2023), which we call Alpaca.\n4 Results & Discussion\nEliciting non-English abilities in instruction-tuned\nLarge Language Models (It-LLMs) remains chal-\nlenging. However, our x-CrossLlama revealed im-\nproved results in cross-lingual Question Answering\n(QA) benchmarks. Moreover, at the same time, the\ninstructed models maintained logical-mathematical\nskills. From the results of Figure 2, it is possible\nto observe the weaknesses emerging from the fine-\ntuning of the translated versions of Alpaca (Section\n4.1), the improvement obtained from the alignment\nphase is encouraging (Section 4.2) but it is not\nenough to outperform the English one.\nThe fine-grained analysis highlighted the impor-\ntance of cross-lingual alignment data and the crit-\nical issues with non-English data. This opens the\nway for new hypotheses regarding the imbalance\nof pre-training languages and learning abilities via\ninstruction-tuning.\n4.1 Alpacas problems on Translations\nThe Instruction-tuning task on LLMs, in our case,\nLlama-7b, is primarily pre-trained in English, and\nhas implications for the derivated models. As\nshown in Figure 2, both MLQA and XQUAD\nbenchmarks reveal a notable disparity, with an av-\nerage point gap of 55 and 53, respectively, between\nthe original tuned Llama-7b (called Alpaca) and\nthe various x-MultiLlama. This discrepancy is at-\ntenuated in the case of MMLU and BBH, where\nthe average gaps are 18 and 14 points. Hence, re-\nlying exclusively on translations of Alpaca-style\ndemonstrations for instruction in various languages\nonly sometimes yields optimal effects. How-\never, models, for example, zh-MultiLlama and de-\nMultiLlama, have exhibited better performances.\nThis variation may be attributed to the volume of\npre-training data available for the respective lan-\nguages and, consequently, the inherent abilities of\nLlama. In future work, we aim to expand our anal-\nysis to include LLMs beyond Llama to see if sim-\nilar, less pronounced, or more accentuated trends\nemerge.\nQA en- avg- avg- δ\nTask Llama Llama CrossLlama\nMLQA 0.89 0.34 0.64 +0.30\nXQUAD 0.97 0.31 0.65 +0.30\nMMLU 0.42 0.24 0.32 +0.08\nBBH 0.30 0.24 0.28 +0.04\nTable 3: Averages accuracies on proposed benchmarks.\n177\nFigure 3: Accuracies (%) of proposed benchmarks using one-direction Translation-following demonstrations. For\nen-x for English-foreigner and x-en for foreign English.\n4.2 A Cross-lingual solution\nUsing the translation-following demonstrations\nclose to instruction-following ones during\ninstruction-tuning significantly empowers the\ncross-lingual performances of It-LLMs. In\nfact, x-CrossLlama consistently surpassed the\nx-MultiLlama, obtaining an improvement of 30\naverage points on MLQA, 34 on XQUAD, 8 on\nMMLU, and 4 on BBH, as detailed in Table 3.\nThis approach brought their performance metrics\ncloser to the benchmark set by the original version\nof Llama (Alpaca), bridging the gap in different\nsituations. For MMLU and BBH, the performance\ndifference was even more marginal, with average\ngaps of 10 and 2 points, respectively, as indicated\nin Table 3 and the ’en-Llama vs avg-CrossLlama’.\nThe inclusion of translation-following demon-\nstrations has undeniably elevated the cross-lingual\nabilities of It-LLMs. Moreover, specific models,\nspecifically the Chinese and German, surpassed the\nArabic version by a significant margin. This dispar-\nity might be attributed to the varied representation\nof corpora within the pre-training datasets, as high-\nlighted in (Yang et al., 2023). Consequently, cross-\nlingual strategies might not yield as pronounced\nbenefits for underrepresented languages during the\ninitial pre-training stages of the language model.\nIn conclusion, our strategy shifted to be high-\nperformance and sustainable. As regards the per-\nformances, as merely discussed following the sys-\ntematic analysis, we found empirical evidence to\nsupport this statement. While sustainability, our\nmethod uses a limited number of demonstrations,\naround 20k, which, combined with those of Alpaca,\naround 52k, remain a meager number, allowing the\ndownstream models to obtain performances com-\nparable to those of more robust models.\n4.3 Ablation Study\nOur CrossLlama, distinguished by the construc-\ntion of the demonstrations pairs presented in Sec-\ntion 3.2, achieves significant performance improve-\nments and contributes to closing the gap between\nthe original version of tuned Llama and a series of\nx-MultiLlama in different languages. We propose\nan additional analysis. Working on the translation-\nfollowing part (defined by half en-x and half x-\nen demonstrations), we analyze the impact of the\ndemonstrations by splitting the experiments into\nen-x and x-en (Section 4.3.1).\n4.3.1 Demonstration Direction matters\nThe evaluations in Figure 3 shed light on the im-\npact of varying the directionality of translation-\nfollowing demonstrations. In particular, demonstra-\n178\ntions that transition from English to a non-English\nlanguage (en-x) appear to have a more pronounced\npositive effect on subsequent models. On the other\nhand, demonstrations transitioning from a foreign\nlanguage to English (x-en) exhibit superior perfor-\nmance compared to baseline models, yet they lag\nbehind when juxtaposed with demonstrations in the\nreverse direction.\nHowever, as further illustrated in Figure 3, the x-\nCrossLlama consistently maintains its edge in per-\nformance. The observed trend, where translation-\nfollowing demonstrations in one specific direction\nseem more influential, is intriguing. Mirroring\nour prior ablation analysis observations, multi-task\nbenchmarks do not exhibit substantial variances.\nThis observation lends further credence to the hy-\npothesis that cross-lingual capabilities predomi-\nnantly influence models in tasks heavily imbued\nwith natural language elements.\n5 Related Work\nIn the NLP field, multilingual and cross-linguistic\nmethods have solid foundations and a long-\nstanding tradition, with in-depth studies on fea-\nture adaptation (Section 5.1). However, the new\nLarge Language Models (LLMs) no longer require\nsuch interventions. After extensive pre-training on\nmassive corpora, cross-linguistic skills are inher-\nently present in LLMs (Section 5.2 and Section 5.3).\nNevertheless, although these abilities appear em-\nbedded, most LLMs must be elicited to show them\nexhaustively. Our study introduces a method to\nempower these cross-linguistic abilities through a\ncross-linguistic semantic alignment approach (Sec-\ntion 5.4).\n5.1 Multilingual Pre-training\nThe next token prediction based on the prefix se-\nquence, also well-known as language modeling, is\nthe everlasting task of modern NLP (Tenney et al.,\n2019). The profound linguistic knowledge embed-\nded within today’s Large Language Models (LLMs)\ndepends on the billions of neurons trained on large-\nscale corpora with derivatives of the language mod-\neling task (Zanzotto et al., 2020; Ranaldi et al.,\n2022). Consequently, the pre-training corpora are\npredominantly in English, e.g., BooksCorpus (Zhu\net al., 2015), MEGATRON-LM (Shoeybi et al.,\n2019), Gutenberg Dataset (Lahiri, 2014) therefore,\nLLMs usually have much better knowledge of En-\nglish than other languages.\nResearchers like Aulamo and Tiedemann (2019);\nAbadji et al. (2022) have proposed forward corpora\ntranslated into multiple languages to address this\nlinguistic imbalance. However, these translated\ndatasets, while valuable, are not as voluminous as\ntheir English-focused counterparts. The absence of\nextensive parallel data in these pre-training corpora\nfurther hinders the ability of LLMs to align and\nunderstand diverse languages effectively (Li et al.,\n2023).\n5.2 Instruction-tuning Paradigm\nOuyang et al. (2022); Wei et al. (2022) fine-tuned\nLLMs using the instruction-tuning method based\non instruction-tuning data, which are instruction-\nresponse corpora, to make LLMs more scalable and\nimprove zero-shot performance. In this method, the\nLLM backbone is fed with data from the instruction\n(I,X,Y ), where I is an instruction describing the\ntask’s requirements, X is the input, which can be\noptional, and Y is the output for the given task. The\nmethod aims to minimize the function f(Y) based\non the log likelihood with model parameters θ.\nEarlier studies show that the instruction-tuning\nmethod of LLMs with both human (Wang et al.,\n2023) and synthetic-generated instructions (Taori\net al., 2023; Xu et al., 2023) empowers the ability\nof LLMs to solve considerable tasks in zero-shot\nscenarios.\nHowever, we state that the generally used\ninstruction-tuning datasets, alpaca (Taori et al.,\n2023), Self-Instruct (Wang et al., 2023), Self-Chat\n(Xu et al., 2023), conceived in English, which lim-\nits the prospect of LLMs to follow non-English\ninstructions and therefore solve related tasks.\n5.3 Instruction-tuning is at hand\nWhile Large Language Models (LLMs) have\nachieved remarkable outcomes using prevalent\ntechniques like instruction-tuning, their vastness\nlimits the breadth of the scientific community that\ncan actively experiment with them.\nRecent innovations aimed at democratizing ac-\ncess to these models and techniques focus on op-\ntimizing parameter tuning. One such method,\nParameter-Efficient Tuning (PEFT), strategically\nadjusts a subset of the model’s parameters while\nkeeping the rest static. The overarching objective\nis to substantially curtail computational and stor-\nage overheads without compromising the perfor-\nmance exhibited by the original models (Ranaldi\net al., 2023b). Established methodologies under the\n179\nPEFT umbrella include LoRA (Hu et al., 2021b),\nPrefix Tuning (Li and Liang, 2021), and P-Tuning\n(Liu et al., 2022). The fundamental principle be-\nhind these techniques is to retain the weights of\nthe pre-trained model and integrate low-rank ma-\ntrices at each architectural layer. This strategy con-\nsiderably diminishes the parameter count that ne-\ncessitates training for subsequent tasks, thereby\nenhancing efficiency. Such foundational advance-\nments play a pivotal role in leveling the playing\nfield for the scientific community, eliciting equi-\ntable research opportunities, and catalyzing the pro-\nliferation of open-source contributions.\n5.4 Multilingual Instruction-tuning\nRecent studies have highlighted the impressive\ncapabilities of LLMs in assimilating instructions\nacross diverse languages. Researchers such as San-\ntilli and Rodolà (2023); Chen et al. (2023) have\nventured into monolingual fine-tuning of Llama,\nfocusing on instructions translated specifically to\neach language. Adopting optimization techniques\nelaborated further in Section 5.3, to design bespoke\nadapters tailored for various tasks has gained mo-\nmentum. In exploring the cross-lingual potential\nof It-LLMs, Zhang et al. (2023) emphasized the\nbenefits of enhancing instruction demonstrations.\nIn this paper, we propose CrossLlama, with a\nseries of It-LLMs models with the Llama-7b back-\nbone as the common denominator. The factor of\nour method is based on the inclusion of translation-\nfollowing demonstrations that elicit semantic align-\nment between languages. We present empirical\nevidence underscoring the expansive cross-lingual\nlearning prowess of It-LLMs. Through evaluations\nof four benchmarks, we demonstrate that the in-\nherent limitations of It-LLMs can be effectively\nmitigated using cross-lingual alignment strategies\nwhen trained on non-English data. Consequently,\nour investigation seeks to elucidate the significance\nof instruction-following and translation-following\ndemonstrations in bridging the linguistic divide,\nthereby enhancing the adaptability of LLMs to lan-\nguages beyond English.\n6 Future Works\nThe multilingual abilities of instruction-tuned\nLarge Language Models (It-LLMs) are supported\nby LLMs, as seen with the Llama backbone in\nAlpaca’s instance. Interestingly, small data-level\nstimuli improve downstream skills. Our experi-\nments yielded significant insights when introducing\nstrategic demonstrations, specifically translation-\nfollowing demonstrations. We achieved these out-\ncomes by fine-tuning Llama-7b, following the ap-\nproach used in Taori et al. (2023).\nIn subsequent research, we aim to delve deeper\nby extending the number of parameters in Llama\nand integrating more backbone models. We are also\nintrigued by the potential effects on languages with\nlimited resources. Furthermore, we aspire to fully\nunderstand the results from specific experiments\nby applying epistemic approaches (Ranaldi et al.,\n2023a,c) to It-LLMs.\nIn parallel, plans include analyzing the transla-\ntion abilities of general It-LLMs and those empow-\nered with translation tasks, including some special-\nized translation tasks among our evaluation bench-\nmarks. Finally, we would like to investigate the\nlearning abilities of the original Alpaca as the trans-\nlation data changes, proposing different probing\nexperiments on (original) English data enhanced\nwith translations. Finally, we would like to investi-\ngate explainability techniques to understand better\nthe underlying mechanisms, as done in (Ranaldi\nand Pucci, 2023), that enable these models to solve\nmultiple tasks in complex scenarios using a small\nnumber of instances.\n7 Conclusion\nIn this paper, we proposed CrossLlama, a novel\nmethodology designed to empower the instruction-\ntuning of LLMs for non-English datasets. Our ap-\nproach uniquely integrates instruction-following\ndemonstrations, reminiscent of the Alpaca style,\nwith translation-following demonstrations. The\nprimary objective of this method is to elicit the\nLLM towards achieving semantic alignment be-\ntween English and non-English languages, thereby\noutperforming models that are instructed using non-\nEnglish texts. Leveraging the proposed demon-\nstrations led to marked performance enhance-\nments across four Question Answering bench-\nmarks: XQUAD, MLQA, MMLU, and BBH. Fur-\nthermore, the depth of semantic alignment ampli-\nfies with the direction of the translation data, un-\nderscoring the inherent abilities of It-LLMs to as-\nsimilate from instruction-following demonstrations.\nOur innovative approach and the ensuing findings\npave the way for advanced research, eliciting the\ndevelopment of more adept LLMs tailored for non-\nEnglish linguistic contexts.\n180\nLimitations\nAlthough the performance achieved by our\nmethod is consistently superior to that of several\nInstruction-tuned on custom corpora, our work has\nlimitations:\n• The proposed method was only analyzed on\nthe Large Language Model Llama-7b; conse-\nquently, we can only report the results. We\nintend to extend our work using larger and\ndifferent models in future developments.\n• Although the proposed method performed\nwell, it is only sometimes applicable as it re-\nquires an additional data set, the translation-\nfollowing set.\n• Finally, a significant limitation is that it is im-\npossible to conduct correlations between the\ncomposition percentages of the training data\nand the downstream results, as the corpora\nused for pre-training are not always accessi-\nble, and the technical reports do not essay\nprecise estimations.\nEthical Statement\nThis work used open-source corpora that do not\ndeal with hate speech or inequality topics. The eval-\nuation phase was also done on solid benchmarks\ncommonly used for evaluation in Large Language\nModels. Finally, the concept of ’disparity’ in the\nmultilingual abilities of the Large Language Mod-\nels in this work is understood as unbalancing the\npre-training data used in the training phase.\nReferences\nJulien Abadji, Pedro Ortiz Suarez, Laurent Romary, and\nBenoît Sagot. 2022. Towards a cleaner document-\noriented multilingual crawled corpus. In Proceedings\nof the Thirteenth Language Resources and Evalua-\ntion Conference, pages 4344–4355, Marseille, France.\nEuropean Language Resources Association.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2019. On the cross-lingual transferability of mono-\nlingual representations. In Annual Meeting of the\nAssociation for Computational Linguistics.\nMikko Aulamo and Jörg Tiedemann. 2019. The OPUS\nresource repository: An open package for creating\nparallel corpora and machine translation services. In\nProceedings of the 22nd Nordic Conference on Com-\nputational Linguistics, pages 389–394, Turku, Fin-\nland. Linköping University Electronic Press.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reasoning,\nhallucination, and interactivity.\nTerra Blevins and Luke Zettlemoyer. 2022. Language\ncontamination helps explains the cross-lingual capa-\nbilities of english pretrained models. In Conference\non Empirical Methods in Natural Language Process-\ning.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nWei-Lin Chen, Cheng-Kuang Wu, and Hsin-Hsi\nChen. 2023. Traditional-chinese alpaca: Models\nand datasets. https://github.com/ntunlplab/\ntraditional-chinese-alpaca.\nYiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient\nand effective text encoding for chinese llama and\nalpaca.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021a. Lora: Low-rank adaptation of\nlarge language models.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021b. Lora: Low-rank adaptation of\nlarge language models.\nHaoyang Huang, Tianyi Tang, Dongdong Zhang,\nWayne Xin Zhao, Ting Song, Yan Xia, and Furu Wei.\n2023. Not all languages are created equal in llms:\nImproving multilingual capability by cross-lingual-\nthought prompting.\nAyyoob Imani, Peiqin Lin, Amir Hossein Kargaran,\nSilvia Severini, Masoud Jalili Sabet, Nora Kassner,\nChunlan Ma, Helmut Schmid, André F. T. Martins,\nFrançois Yvon, and Hinrich Schütze. 2023. Glot500:\nScaling multilingual corpora and language models to\n500 languages.\nShibamouli Lahiri. 2014. Complexity of Word Colloca-\ntion Networks: A Preliminary Structural Analysis. In\nProceedings of the Student Research Workshop at the\n181\n14th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics, pages 96–105,\nGothenburg, Sweden. Association for Computational\nLinguistics.\nHang Le, Juan Miguel Pino, Changhan Wang, Jiatao\nGu, Didier Schwab, and Laurent Besacier. 2021.\nLightweight adapter tuning for multilingual speech\ntranslation. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguis-\ntics (ACL), pages 817–824. Association for Compu-\ntational Linguistics.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7315–\n7330, Online. Association for Computational Lin-\nguistics.\nJiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng,\nand Jiajun Chen. 2023. Eliciting the translation abil-\nity of large language models via multilingual finetun-\ning with translation instructions.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nZehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu,\nJiangtao Feng, Hao Zhou, and Lei Li. 2021. Pre-\ntraining multilingual neural machine translation by\nleveraging alignment information.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 61–68,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nYasmin Moslem, Rejwanul Haque, John D. Kelleher,\nand Andy Way. 2023. Adaptive machine translation\nwith large language models.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text.\nFederico Ranaldi, Elena Sofia Ruzzetti, Felicia Lo-\ngozzo, Michele Mastromattei, Leonardo Ranaldi, and\nFabio Massimo Zanzotto. 2023a. Exploring linguis-\ntic properties of monolingual berts with typological\nclassification among languages.\nLeonardo Ranaldi, Aria Nourbakhsh, Arianna Patrizi,\nElena Sofia Ruzzetti, Dario Onorati, Francesca Fal-\nlucchi, and Fabio Massimo Zanzotto. 2022. The dark\nside of the language: Pre-trained transformers in the\ndarknet.\nLeonardo Ranaldi and Giulia Pucci. 2023. Knowing\nknowledge: Epistemological study of knowledge in\ntransformers. Applied Sciences, 13(2).\nLeonardo Ranaldi, Elena Sofia Ruzzetti, Davide Ven-\nditti, Dario Onorati, and Fabio Massimo Zanzotto.\n2023b. A trip towards fairness: Bias and de-biasing\nin large language models.\nLeonardo Ranaldi, Elena Sofia Ruzzetti, and Fabio Mas-\nsimo Zanzotto. 2023c. Precog: Exploring the rela-\ntion between memorization and performance in pre-\ntrained language models.\nAndrea Santilli and Emanuele Rodolà. 2023. Camoscio:\nan italian instruction-tuned llama.\nMohammad Shoeybi, Mostofa Ali Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-LM: Training multi-billion\nparameter language models using model parallelism.\nArXiv, abs/1909.08053.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\nZhou, , and Jason Wei. 2022. Challenging big-bench\ntasks and whether chain-of-thought can solve them.\narXiv preprint arXiv:2210.09261.\nMarc Tanti, Lonneke van der Plas, Claudia Borg, and\nAlbert Gatt. 2021. On the language-specificity of\nmultilingual bert and the impact of fine-tuning.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nMartin Thissen. 2023. Fine-tune alpaca for any\nlanguage. https://github.com/thisserand/\nalpaca-lora-finetune-language .\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\n182\nEvaluation (LREC’12), pages 2214–2218, Istanbul,\nTurkey. European Language Resources Association\n(ELRA).\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022. Finetuned\nlanguage models are zero-shot learners.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023. Baize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv\npreprint arXiv:2304.01196.\nWen Yang, Chong Li, Jiajun Zhang, and Chengqing\nZong. 2023. Bigtranslate: Augmenting large lan-\nguage models with multilingual translation capability\nover 100 languages.\nYasbok. Alpaca Instruction Fine-Tuning for Arabic.\nhttps://huggingface.co/Yasbok .\nFabio Massimo Zanzotto, Andrea Santilli, Leonardo\nRanaldi, Dario Onorati, Pierfrancesco Tommasino,\nand Francesca Fallucchi. 2020. KERMIT: Comple-\nmenting transformer architectures with encoders of\nexplicit syntactic interpretations. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 256–267,\nOnline. Association for Computational Linguistics.\nJiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou.\n2023. Tim: Teaching large language models to trans-\nlate with comparison.\nShaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhen-\ngrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu,\nShangtong Gui, Yunji Chen, Xilin Chen, and Yang\nFeng. 2023. Bayling: Bridging cross-lingual align-\nment and instruction following through interactive\ntranslation for large language models.\nWenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu,\nShujian Huang, Lingpeng Kong, Jiajun Chen, and Lei\nLi. 2023. Multilingual machine translation with large\nlanguage models: Empirical results and analysis.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\n183"
}