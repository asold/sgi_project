{
  "title": "Head-Driven Statistical Models for Natural Language Parsing",
  "url": "https://openalex.org/W2092654472",
  "year": 2003,
  "authors": [
    {
      "id": "https://openalex.org/A1922466166",
      "name": "Michael Collins",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2111041233",
    "https://openalex.org/W2161160885",
    "https://openalex.org/W2123893795",
    "https://openalex.org/W2113641473",
    "https://openalex.org/W2114886551",
    "https://openalex.org/W1972573551",
    "https://openalex.org/W3088560083",
    "https://openalex.org/W2110651511",
    "https://openalex.org/W2074260075",
    "https://openalex.org/W2309755354",
    "https://openalex.org/W3021452258",
    "https://openalex.org/W2104029044",
    "https://openalex.org/W2153439141",
    "https://openalex.org/W4231741839",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W2139895384",
    "https://openalex.org/W1600844763",
    "https://openalex.org/W2167434254",
    "https://openalex.org/W2087165009",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W1535015163",
    "https://openalex.org/W2052449326",
    "https://openalex.org/W2952654140",
    "https://openalex.org/W2963847008",
    "https://openalex.org/W2138389163",
    "https://openalex.org/W1567570606",
    "https://openalex.org/W2163918411",
    "https://openalex.org/W2110607519",
    "https://openalex.org/W1986543644",
    "https://openalex.org/W1982944197",
    "https://openalex.org/W2134666210",
    "https://openalex.org/W2102924265",
    "https://openalex.org/W1773803948",
    "https://openalex.org/W3021713638",
    "https://openalex.org/W79411081",
    "https://openalex.org/W199541590",
    "https://openalex.org/W2077613835",
    "https://openalex.org/W4241850027",
    "https://openalex.org/W2949237929",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2004673890",
    "https://openalex.org/W2069912724",
    "https://openalex.org/W1994507518",
    "https://openalex.org/W1491745322",
    "https://openalex.org/W1936920915",
    "https://openalex.org/W1585559843",
    "https://openalex.org/W2155693943",
    "https://openalex.org/W2039240651",
    "https://openalex.org/W1509045587",
    "https://openalex.org/W1598003989",
    "https://openalex.org/W2096466920",
    "https://openalex.org/W2110882317",
    "https://openalex.org/W1809152426",
    "https://openalex.org/W1601728146",
    "https://openalex.org/W2166451556",
    "https://openalex.org/W1993750641",
    "https://openalex.org/W131829211",
    "https://openalex.org/W2002089154",
    "https://openalex.org/W2005505724",
    "https://openalex.org/W2093647425",
    "https://openalex.org/W2104399512",
    "https://openalex.org/W1508191042"
  ],
  "abstract": "This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.",
  "full_text": "c⃝ 2003 Association for Computational Linguistics\nHead-Driven Statistical Models for\nNatural Language Parsing\nMichael Collins∗\nMIT Computer Science and\nArtiﬁcial Intelligence Laboratory\nThis article describes three statistical models for natural language parsing. The models extend\nmethods from probabilistic context-free grammars to lexicalized grammars, leading to approaches\nin which a parse tree is represented as the sequence of decisions corresponding to a head-centered,\ntop-down derivation of the tree. Independence assumptions then lead to parameters that encode\nthe X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram\nlexical dependencies,wh-movement, and preferences for close attachment. All of these preferences\nare expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn\nWall Street Journal Treebank, showing that their accuracy is competitive with other models in\nthe literature. To gain a better understanding of the models, we also give results on different\nconstituent types, as well as a breakdown of precision/recall results in recovering various types of\ndependencies. We analyze various characteristics of the models through experiments on parsing\naccuracy, by collecting frequencies of various structures in the treebank, and through linguistically\nmotivated examples. Finally, we compare the models to others that have been applied to parsing\nthe treebank, aiming to give some explanation of the difference in performance of the various\nmodels.\n1. Introduction\nAmbiguity is a central problem in natural language parsing. Combinatorial effects\nmean that even relatively short sentences can receive a considerable number of parses\nunder a wide-coverage grammar. Statistical parsing approaches tackle the ambiguity\nproblem by assigning a probability to each parse tree, thereby ranking competing trees\nin order of plausibility. In many statistical models the probability for each candidate\ntree is calculated as a product of terms, each term corresponding to some substructure\nwithin the tree. The choice of parameterization is essentially the choice of how to\nrepresent parse trees. There are two critical questions regarding the parameterization\nof a parsing approach:\n1. Which linguistic objects (e.g., context-free rules, parse moves) should the\nmodel’s parameters be associated with? In other words, which features\nshould be used to discriminate among alternative parse trees?\n2. How can this choice be instantiated in a sound probabilistic model?\nIn this article we explore these issues within the framework of generative models,\nmore precisely, the history-based models originally introduced to parsing by Black\n∗ MIT Computer Science and Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technology,\n545 Technology Square, Cambridge, MA 02139. E-mail: mcollins@ai.mit.edu.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n590\nComputational Linguistics Volume 29, Number 4\net al. (1992). In a history-based model, a parse tree is represented as a sequence of\ndecisions, the decisions being made in some derivation of the tree. Each decision has\nan associated probability, and the product of these probabilities deﬁnes a probability\ndistribution over possible derivations.\nWe ﬁrst describe three parsing models based on this approach. The models were\noriginally introduced in Collins (1997); the current article\n1 gives considerably more\ndetail about the models and discusses them in greater depth. In Model 1 we show\none approach that extends methods from probabilistic context-free grammars (PCFGs)\nto lexicalized grammars. Most importantly, the model has parameters corresponding\nto dependencies between pairs of headwords. We also show how to incorporate a\n“distance” measure into these models, by generalizing the model to a history-based\napproach. The distance measure allows the model to learn a preference for close at-\ntachment, or right-branching structures.\nIn Model 2, we extend the parser to make the complement/adjunct distinction,\nwhich will be important for most applications using the output from the parser. Model\n2 is also extended to have parameters corresponding directly to probability distribu-\ntions over subcategorization frames for headwords. The new parameters lead to an\nimprovement in accuracy.\nIn Model 3 we give a probabilistic treatment of wh-movement that is loosely based\non the analysis of wh-movement in generalized phrase structure grammar (GPSG)\n(Gazdar et al. 1985). The output of the parser is now enhanced to show trace coin-\ndexations in wh-movement cases. The parameters in this model are interesting in that\nthey correspond directly to the probability of propagating GPSG-style slash features\nthrough parse trees, potentially allowing the model to learn island constraints.\nIn the three models a parse tree is represented as the sequence of decisions cor-\nresponding to a head-centered, top-down derivation of the tree. Independence as-\nsumptions then follow naturally, leading to parameters that encode the X-bar schema,\nsubcategorization, ordering of complements, placement of adjuncts, lexical dependen-\ncies, wh-movement, and preferences for close attachment. All of these preferences are\nexpressed by probabilities conditioned on lexical heads. For this reason we refer to the\nmodels as head-driven statistical models .\nWe describe evaluation of the three models on the Penn Wall Street Journal Tree-\nbank (Marcus, Santorini, and Marcinkiewicz 1993). Model 1 achieves 87.7% constituent\nprecision and 87.5% consituent recall on sentences of up to 100 words in length in sec-\ntion 23 of the treebank, and Models 2 and 3 give further improvements to 88.3%\nconstituent precision and 88.0% constituent recall. These results are competitive with\nthose of other models that have been applied to parsing the Penn Treebank. Models 2\nand 3 produce trees with information about wh-movement or subcategorization. Many\nNLP applications will need this information to extract predicate-argument structure\nfrom parse trees.\nThe rest of the article is structured as follows. Section 2 gives background material\non probabilistic context-free grammars and describes how rules can be “lexicalized”\nthrough the addition of headwords to parse trees. Section 3 introduces the three prob-\nabilistic models. Section 4 describes various reﬁnments to these models. Section 5\ndiscusses issues of parameter estimation, the treatment of unknown words, and also\nthe parsing algorithm. Section 6 gives results evaluating the performance of the mod-\nels on the Penn Wall Street Journal Treebank (Marcus, Santorini, and Marcinkiewicz\n1993). Section 7 investigates various aspects of the models in more detail. We give a\n1 Much of this article is an edited version of chapters 7 and 8 of Collins (1999).\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n591\nCollins Head-Driven Statistical Models for NL Parsing\ndetailed analysis of the parser’s performance on treebank data, including results on\ndifferent constituent types. We also give a breakdown of precision and recall results\nin recovering various types of dependencies. The intention is to give a better idea\nof the strengths and weaknesses of the parsing models. Section 7 goes on to discuss\nthe distance features in the models, the implicit assumptions that the models make\nabout the treebank annotation style, and the way that context-free rules in the original\ntreebank are broken down, allowing the models to generalize by producing new rules\non test data examples. We analyze these phenomena through experiments on parsing\naccuracy, by collecting frequencies of various structures in the treebank, and through\nlinguistically motivated examples. Finally, section 8 gives more discussion, by com-\nparing the models to others that have been applied to parsing the treebank. We aim to\ngive some explanation of the differences in performance among the various models.\n2. Background\n2.1 Probabilistic Context-Free Grammars\nProbabilistic context-free grammars are the starting point for the models in this arti-\ncle. For this reason we brieﬂy recap the theory behind nonlexicalized PCFGs, before\nmoving to the lexicalized case.\nFollowing Hopcroft and Ullman (1979), we deﬁne a context-free grammar G as a\n4-tuple (N, Σ, A, R), where N is a set of nonterminal symbols, Σ is an alphabet, A is a\ndistinguished start symbol in N, and R is a ﬁnite set of rules, in which each rule is of\nthe form X → β for some X ∈ N, β ∈ (N ∪ Σ)\n∗. The grammar deﬁnes a set of possible\nstrings in the language and also deﬁnes a set of possible leftmost derivations under\nthe grammar. Each derivation corresponds to a tree-sentence pair that is well formed\nunder the grammar.\nA probabilistic context-free grammar is a simple modiﬁcation of a context-free\ngrammar in which each rule in the grammar has an associated probability P(β | X).\nThis can be interpreted as the conditional probability of X’s being expanded using\nthe rule X → β, as opposed to one of the other possibilities for expanding X listed\nin the grammar. The probability of a derivation is then a product of terms, each\nterm corresponding to a rule application in the derivation. The probability of a given\ntree-sentence pair (T, S) derived by n applications of context-free rules LHS\ni → RHSi\n(where LHS stands for “left-hand side,” RHS for “right-hand side”), 1 ≤ i ≤ n, under\nthe PCFG is\nP(T, S)=\nn∏\ni=1\nP(RHSi | LHSi)\nBooth and Thompson (1973) specify the conditions under which the PCFG does in fact\ndeﬁne a distribution over the possible derivations (trees) generated by the underlying\ngrammar. The ﬁrst condition is that the rule probabilities deﬁne conditional distribu-\ntions over how each nonterminal in the grammar can expand. The second is a technical\ncondition that guarantees that the stochastic process generating trees terminates in a\nﬁnite number of steps with probability one.\nA central problem in PCFGs is to deﬁne the conditional probability P(β | X) for\neach rule X → β in the grammar. A simple way to do this is to take counts from a\ntreebank and then to use the maximum-likelihood estimates:\nP(β | X)= Count(X → β)\nCount(X) (1)\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n592\nComputational Linguistics Volume 29, Number 4\nIf the treebank has actually been generated from a probabilistic context-free grammar\nwith the same rules and nonterminals as the model, then in the limit, as the training\nsample size approaches inﬁnity, the probability distribution implied by these estimates\nwill converge to the distribution of the underlying grammar.\n2\nOnce the model has been trained, we have a model that deﬁnes P(T, S) for any\nsentence-tree pair in the grammar. The output on a new test sentence S is the most\nlikely tree under this model,\nTbest = arg max\nT\nP(T | S)= arg max\nT\nP(T, S)\nP(S) = arg max\nT\nP(T, S)\nThe parser itself is an algorithm that searches for the tree, Tbest, that maximizes P(T, S).\nIn the case of PCFGs, this can be accomplished using a variant of the CKY algorithm\napplied to weighted grammars (providing that the PCFG can be converted to an equiv-\nalent PCFG in Chomsky normal form); see, for example, Manning and Sch ¨ utze (1999).\nIf the model probabilities P(T, S) are the same as the true distribution generating\ntraining and test examples, returning the most likely tree under P(T, S) will be op-\ntimal in terms of minimizing the expected error rate (number of incorrect trees) on\nnewly drawn test examples. Hence if the data are generated by a PCFG, and there are\nenough training examples for the maximum-likelihood estimates to converge to the\ntrue values, then this parsing method will be optimal. In practice, these assumptions\ncannot be veriﬁed and are arguably quite strong, but these limitations have not pre-\nvented generative models from being successfully applied to many NLP and speech\ntasks. (See Collins [2002] for a discussion of other ways of conceptualizing the parsing\nproblem.)\nIn the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), which is the\nsource of data for our experiments, the rules are either internal to the tree, where LHS\nis a nonterminal and RHS is a string of one or more nonterminals, or lexical, where\nLHS is a part-of-speech tag and RHS is a word. (See Figure 1 for an example.)\n2.2 Lexicalized PCFGs\nA PCFG can be lexicalized\n3 by associating a word w and a part-of-speech (POS) tag t\nwith each nonterminal X in the tree. (See Figure 2 for an example tree.)\nThe PCFG model can be applied to these lexicalized rules and trees in exactly the\nsame way as before. Whereas before the nonterminals were simple (for example, S or\nNP), they are now extended to include a word and part-of-speech tag (for example,\nS(bought,VBD) or NP(IBM,NNP)). Thus we write a nonterminal as X(x), where x =\n⟨w, t⟩and X is a constituent label. Formally, nothing has changed, we have just vastly\nincreased the number of nonterminals in the grammar (by up to a factor of | V|×| T|,\n2 This point is actually more subtle than it ﬁrst appears (we thank one of the anonymous reviewers for\npointing this out), and we were unable to ﬁnd proofs of this property in the literature for PCFGs. The\nrule probabilities for any nonterminal that appears with probability greater than zero in parse\nderivations will converge to their underlying values, by the usual properties of maximum-likelihood\nestimation for multinomial distributions. Assuming that the underlying PCFG generating training\nexamples meet both criteria in Booth and Thompson (1973), it can be shown that convergence of rule\nprobabilities implies that the distribution over trees will converge to that of the underlying PCFG, at\nleast when Kullback-Liebler divergence or the inﬁnity norm is taken to be the measure of distance\nbetween the two distributions. Thanks to Tommi Jaakkola and Nathan Srebro for discussions on this\ntopic.\n3 We ﬁnd lexical heads in Penn Treebank data using the rules described in Appendix A of Collins (1999).\nThe rules are a modiﬁed version of a head table provided by David Magerman and used in the parser\ndescribed in Magerman (1995).\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n593\nCollins Head-Driven Statistical Models for NL Parsing\nInternal rules Lexical rules\nTOP → S JJ → Last\nS → NP NP VP NN → week\nNP → JJ NN NNP → IBM\nNP → NNP VBD → bought\nVP → VBD NP NNP → Lotus\nNP → NNP\nFigure 1\nA nonlexicalized parse tree and a list of the rules it contains.\nInternal Rules:\nTOP → S(bought,VBD)\nS(bought,VBD) → NP(week,NN) NP(IBM,NNP) VP(bought,VBD)\nNP(week,NN) → JJ(Last,JJ) NN(week,NN)\nNP(IBM,NNP) → NNP(IBM,NNP)\nVP(bought,VBD) → VBD(bought,VBD) NP(Lotus,NNP)\nNP(Lotus,NNP) → NNP(Lotus,NNP)\nLexical Rules:\nJJ(Last,JJ) → Last\nNN(week,NN) → week\nNNP(IBM,NNP) → IBM\nVBD(bought,VBD) → bought\nNNP(Lotus,NN) → Lotus\nFigure 2\nA lexicalized parse tree and a list of the rules it contains.\nwhere |V| is the number of words in the vocabulary and |T |is the number of part-of-\nspeech tags).\nAlthough nothing has changed from a formal point of view, the practical conse-\nquences of expanding the number of nonterminals quickly become apparent when\none is attempting to deﬁne a method for parameter estimation. The simplest solution\nwould be to use the maximum-likelihood estimate as in equation (1), for example,\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n594\nComputational Linguistics Volume 29, Number 4\nestimating the probability associated withS(bought,VBD) → NP(week,NN) NP(IBM,NNP)\nVP(bought,VBD) as\nP(NP(week,NN) NP(IBM,NNP) VP(bought,VBD) | S(bought,VBD))=\nCount(S(bought,VBD) → NP(week,NN) NP(IBM,NNP) VP(bought,VBD))\nCount(S(bought,VBD))\nBut the addition of lexical items makes the statistics for this estimate very sparse: The\ncount for the denominator is likely to be relatively low, and the number of outcomes\n(possible lexicalized RHSs) is huge, meaning that the numerator is very likely to be\nzero. Predicting the whole lexicalized rule in one go is too big a step.\nOne way to overcome these sparse-data problems is to break down the gener-\nation of the RHS of each rule into a sequence of smaller steps, and then to make\nindependence assumptions to reduce the number of parameters in the model. The de-\ncomposition of rules should aim to meet two criteria. First, the steps should be small\nenough for the parameter estimation problem to be feasible (i.e., in terms of having suf-\nﬁcient training data to train the model, providing that smoothing techniques are used\nto mitigate remaining sparse-data problems). Second, the independence assumptions\nmade should be linguistically plausible. In the next sections we describe three statisti-\ncal parsing models that have an increasing degree of linguistic sophistication. Model 1\nuses a decomposition of which parameters corresponding to lexical dependencies are a\nnatural result. The model also incorporates a preference for right-branching structures\nthrough conditioning on “distance” features. Model 2 extends the decomposition to\ninclude a step in which subcategorization frames are chosen probabilistically. Model 3\nhandles wh-movement by adding parameters corresponding to slash categories being\npassed from the parent of the rule to one of its children or being discharged as a trace.\n3. Three Probabilistic Models for Parsing\n3.1 Model 1\nThis section describes how the generation of the RHS of a rule is broken down into a\nsequence of smaller steps in model 1. The ﬁrst thing to note is that each internal rule\nin a lexicalized PCFG has the form\n4\nP(h) → Ln(ln) ... L1(l1)H(h)R1(r1) ... Rm(rm)( 2)\nH is the head-child of the rule, which inherits the headword/tag pair h from its parent\nP. L1(l1) ... Ln(ln) and R1(r1) ... Rm(rm) are left and right modiﬁers of H. Either n or m\nmay be zero, and n = m = 0 for unary rules. Figure 2 shows a tree that will be used\nas an example throughout this article. We will extend the left and right sequences to\ninclude a terminating STOP symbol, allowing a Markov process to model the left and\nright sequences. Thus L\nn+1 = Rm+1 = STOP.\nFor example, in S(bought,VBD) → NP(week,NN) NP(IBM,NNP) VP(bought,VBD):\nn = 2 m = 0 P = S\nH = VP L1 = NP L2 = NP\nL3 = STOP R1 = STOP h = ⟨bought, VBD⟩\nl1 = ⟨IBM, NNP⟩ l2 = ⟨week, NN⟩\n4 With the exception of the top rule in the tree, which has the form TOP → H(h).\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n595\nCollins Head-Driven Statistical Models for NL Parsing\nNote that lexical rules, in contrast to the internal rules, are completely deterministic.\nThey always take the form\nP(h) → w\nwhere P is a part-of-speech tag, h is a word-tag pair ⟨w, t⟩, and the rule rewrites to just\nthe word w. (See Figure 2 for examples of lexical rules.) Formally, we will always take\na lexicalized nonterminal P(h) to expand deterministically (with probability one) in\nthis way if P is a part-of-speech symbol. Thus for the parsing models we require the\nnonterminal labels to be partitioned into two sets: part-of-speech symbols and other\nnonterminals. Internal rules always have an LHS in which P is not a part-of-speech\nsymbol. Because lexicalized rules are deterministic, they will not be discussed in the\nremainder of this article: All of the modeling choices concern internal rules.\nThe probability of an internal rule can be rewritten (exactly) using the chain rule\nof probabilities:\nP(L\nn+1(ln+1) ... L1(l1)H(h)R1(r1) ... Rm+1(rm+1) | P, h)=\nPh(H | P, h) ×\n∏\ni=1...n+1\nPl(Li(li) | L1(l1) ... Li−1(li−1), P, h, H) ×\n∏\nj=1...m+1\nPr(Rj(rj) | L1(l1) ... Ln+1(ln+1), R1(r1) ... Rj−1(rj−1), P, h, H)\n(The subscripts h, l and r are used to denote the head, left-modiﬁer, and right-modiﬁer\nparameter types, respectively.) Next, we make the assumption that the modiﬁers are\ngenerated independently of each other:\nP\nl(Li(li) | L1(l1) ... Li−1(li−1), P, h, H)= Pl(Li(li) | P, h, H)\n(3)\nPr(Rj(rj) | L1(l1) ... Ln+1(ln+1), R1(r1) ... Rj−1(rj−1), P, h, H)= Pr(Rj(rj) | P, h, H)\n(4)\nIn summary, the generation of the RHS of a rule such as (2), given the LHS, has\nbeen decomposed into three steps: 5\n1. Generate the head constituent label of the phrase, with probability\nPh(H | P, h).\n2. Generate modiﬁers to the left of the head with probability∏\ni=1...n+1 Pl(Li(li) | P, h, H), where Ln+1(ln+1)= STOP. The STOP symbol is\nadded to the vocabulary of nonterminals, and the model stops\ngenerating left modiﬁers when the STOP symbol is generated.\n3. Generate modiﬁers to the right of the head with probability∏\ni=1...m+1 Pr(Ri(ri) | P, h, H). We deﬁne Rm+1(rm+1) as STOP.\nFor example, the probability of the rule S(bought) → NP(week) NP(IBM) VP(bought)\nwould be estimated as\nPh(VP| S,bought) × Pl(NP(IBM)| S,VP,bought) × Pl(NP(week)| S,VP,bought)\n×Pl(STOP| S,VP,bought) × Pr(STOP| S,VP,bought)\n5 An exception is the ﬁrst rule in the tree, TOP → H(h), which has probability PTOP(H, h|TOP)\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n596\nComputational Linguistics Volume 29, Number 4\nIn this example, and in the examples in the rest of the article, for brevity we omit\nthe part-of-speech tags associated with words, writing, for example S(bought) rather\nthan S(bought,VBD). We emphasize that throughout the models in this article, each\nword is always paired with its part of speech, either when the word is generated or\nwhen the word is being conditioned upon.\n3.1.1 Adding Distance to the Model. In this section we ﬁrst describe how the model\ncan be extended to be “history-based.” We then show how this extension can be\nutilized in incorporating “distance” features into the model.\nBlack et al. (1992) originally introduced history-based models for parsing. Equa-\ntions (3) and (4) of the current article made the independence assumption that each\nmodiﬁer is generated independently of the others (i.e., that the modiﬁers are generated\nindependently of everything except P, H, and h). In general, however, the probability\nof generating each modiﬁer could depend on any function of the previous modiﬁers,\nhead/parent category, and headword. Moreover, if the top-down derivation order is\nfully speciﬁed, then the probability of generating a modiﬁer can be conditioned on any\nstructure that has been previously generated. The remainder of this article assumes\nthat the derivation order is depth-ﬁrst: that is, each modiﬁer recursively generates the\nsubtree below it before the next modiﬁer is generated. (Figure 3 gives an example that\nillustrates this.)\nThe models in Collins (1996) showed that the distance between words standing in\nhead-modiﬁer relationships was important, in particular, that it is important to capture\na preference for right-branching structures (which almost translates into a preference\nfor dependencies between adjacent words) and a preference for dependencies not to\ncross a verb. In this section we describe how this information can be incorporated\ninto model 1. In section 7.2, we describe experiments that evaluate the effect of these\nfeatures on parsing accuracy.\nFigure 3\nA partially completed tree derived depth-ﬁrst. “????” marks the position of the next modiﬁer\nto be generated—it could be a nonterminal/headword/head-tag triple, or the STOP symbol.\nThe distribution over possible symbols in this position could be conditioned on any\npreviously generated structure, that is, any structure appearing in the ﬁgure.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n597\nCollins Head-Driven Statistical Models for NL Parsing\nFigure 4\nThe next child, R3(r3), is generated with probability P(R3(r3) | P, H, h, distancer(2)). The distance\nis a function of the surface string below previous modiﬁers R1 and R2. In principle the model\ncould condition on any structure dominated by H, R1,o r R2 (or, for that matter, on any\nstructure previously generated elsewhere in the tree).\nDistance can be incorporated into the model by modifying the independence as-\nsumptions so that each modiﬁer has a limited dependence on the previous modiﬁers:\nPl(Li(li) | H, P, h, L1(l1) ... Li−1(li−1)) = Pl(Li(li) | H, P, h, distancel(i − 1))\n(5)\nPr(Ri(ri) | H, P, h, R1(r1) ... Ri−1(ri−1)) = Pr(Ri(ri) | H, P, h, distancer(i − 1))\n(6)\nHere distancel and distancer are functions of the surface string below the previous\nmodiﬁers. (See Figure 4 for illustration.) The distance measure is similar to that in\nCollins (1996), a vector with the following two elements: (1) Is the string of zero\nlength? (2) Does the string contain a verb? The ﬁrst feature allows the model to learn\na preference for right-branching structures. The second feature\n6 allows the model to\nlearn a preference for modiﬁcation of the most recent verb. 7\n3.2 Model 2: The Complement/Adjunct Distinction and Subcategorization\nThe tree depicted in Figure 2 illustrates the importance of the complement/adjunct\ndistinction. It would be useful to identify IBM as a subject and Last week as an adjunct\n(temporal modiﬁer), but this distinction is not made in the tree, as both NPsa r ei n\nthe same position\n8 (sisters to a VP under an S node). From here on we will identify\ncomplements9 by attaching a -C sufﬁx to nonterminals. Figure 5 shows the tree in\nFigure 2 with added complement markings.\nA postprocessing stage could add this detail to the parser output, but there are a\ncouple of reasons for making the distinction while parsing. First, identifying comple-\nments is complex enough to warrant a probabilistic treatment. Lexical information is\nneeded (for example, knowledge that week is likely to be a temporal modiﬁer). Knowl-\nedge about subcategorization preferences (for example, that a verb takes exactly one\nsubject) is also required. For example, week can sometimes be a subject, as in Last week\nwas a good one, so the model must balance the preference for having a subject against\n6 Note that this feature means that dynamic programming parsing algorithms for the model must keep\ntrack of whether each constituent does or does not have a verb in the string to the right or left of its\nhead. See Collins (1999) for a full description of the parsing algorithms.\n7 In the models described in Collins (1997), there was a third question concerning punctuation: (3) Does\nthe string contain 0, 1, 2 or more than 2 commas? (where a comma is anything tagged as “,” or “:”).\nThe model described in this article has a cleaner incorporation of punctuation into the generative\nprocess, as described in section 4.3.\n8 Except that IBM is closer to the VP, but note that IBM is also the subject in IBM last week bought Lotus.\n9 We use the term complement in a broad sense that includes both complements and speciﬁers under the\nterminology of government and binding.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n598\nComputational Linguistics Volume 29, Number 4\nFigure 5\nA tree with the -C sufﬁx used to identify complements. IBM and Lotus are in subject and\nobject position, respectively. Last week is an adjunct.\nFigure 6\nTwo examples in which the assumption that modiﬁers are generated independently of one\nanother leads to errors. In (1) the probability of generating both Dreyfus and fund as subjects,\nP(NP-C(Dreyfus)| S,VP,was) ∗ P(NP-C(fund)| S,VP,was), is unreasonably high. (2) is similar:\nP(NP-C(bill),VP-C(funding)| VP,VB,was)= P(NP-C(bill)| VP,VB,was) ∗\nP(VP-C(funding)| VP,VB,was) is a bad independence assumption.\nthe relative improbability of week’s being the headword of a subject. These problems\nare not restricted to NPs; compare The spokeswoman said (SBAR that the asbestos was dan-\ngerous) with Bonds beat short-term investments (SBARbecause the market is down), in which\nan SBAR headed by that is a complement, but an SBAR headed by because is an adjunct.\nA second reason for incorporating the complement/adjunct distinction into the\nparsing model is that this may help parsing accuracy. The assumption that comple-\nments are generated independently of one another often leads to incorrect parses. (See\nFigure 6 for examples.)\n3.2.1 Identifying Complements and Adjuncts in the Penn Treebank. We add the -C\nsufﬁx to all nonterminals in training data that satisfy the following conditions:\n1. The nonterminal must be (1) an NP, SBAR,o r S whose parent is an S; (2)\nan NP, SBAR, S,o r VP whose parent is a VP; or (3) an S whose parent is\nan SBAR.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n599\nCollins Head-Driven Statistical Models for NL Parsing\n2. The nonterminal must not have one of the following semantic tags: ADV,\nVOC, BNF, DIR, EXT, LOC, MNR, TMP, CLR or PRP. See Marcus et al.\n(1994) for an explanation of what these tags signify. For example, the NP\nLast week in ﬁgure 2 would have the TMP (temporal) tag, and the SBAR in\n(SBARbecause the market is down) would have the ADV (adverbial) tag.\n3. The nonterminal must not be on the RHS of a coordinated phrase. For\nexample, in the rule S → SC CS , the two child Ss would not be marked\nas complements.\nIn addition, the ﬁrst child following the head of a prepositional phrase is marked as\na complement.\n3.2.2 Probabilities over Subcategorization Frames. Model 1 could be retrained on\ntraining data with the enhanced set of nonterminals, and it might learn the lexical\nproperties that distinguish complements and adjuncts (IBM vs. week,o r that vs. because).\nIt would still suffer, however, from the bad independence assumptions illustrated in\nFigure 6. To solve these kinds of problems, the generative process is extended to\ninclude a probabilistic choice of left and right subcategorization frames:\n1. Choose a head H with probability P\nh(H | P, h).\n2. Choose left and right subcategorization frames, LC and RC, with\nprobabilities Plc(LC | P, H, h) and Prc(RC | P, H, h). Each subcategorization\nframe is a multiset 10 specifying the complements that the head requires\nin its left or right modiﬁers.\n3. Generate the left and right modiﬁers with probabilities Pl(Li(li) |\nH, P, h, distancel(i − 1), LC) and Pr(Ri(ri) | H, P, h, distancer(i − 1), RC),\nrespectively.\nThus the subcategorization requirements are added to the conditioning context. As\ncomplements are generated they are removed from the appropriate subcategorization\nmultiset. Most importantly, the probability of generating the STOP symbol will be zero\nwhen the subcategorization frame is non-empty, and the probability of generating a\nparticular complement will be zero when that complement is not in the subcatego-\nrization frame; thus all and only the required complements will be generated.\nThe probability of the phrase S(bought) → NP(week) NP-C(IBM) VP(bought) is\nnow\nP\nh(VP| S,bought) × Plc({NP-C}| S,VP,bought) × Prc({} |S,VP,bought) ×\nPl(NP-C(IBM)| S,VP,bought, {NP-C}) × Pl(NP(week)| S,VP,bought, {}) ×\nPl(STOP| S,VP,bought, {}) × Pr(STOP| S,VP,bought, {})\nHere the head initially decides to take a single NP-C (subject) to its left and no com-\nplements to its right. NP-C(IBM) is immediately generated as the required subject, and\nNP-C is removed from LC, leaving it empty when the next modiﬁer, NP(week), is gen-\nerated. The incorrect structures in Figure 6 should now have low probability, because\nP\nlc({NP-C,NP-C}| S,VP,was) and Prc({NP-C,VP-C}| VP,VB,was) should be small.\n10 A multiset, or bag, is a set that may contain duplicate nonterminal labels.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n600\nComputational Linguistics Volume 29, Number 4\n3.3 Model 3: Traces and Wh-Movement\nAnother obstacle to extracting predicate-argument structure from parse trees is wh-\nmovement. This section describes a probabilistic treatment of extraction from relative\nclauses. Noun phrases are most often extracted from subject position, object position,\nor from within PPs:\n(1) The store ( SBAR that TRACE bought Lotus)\n(2) The store ( SBAR that IBM bought TRACE)\n(3) The store ( SBAR that IBM bought Lotus from TRACE)\nIt might be possible to write rule-based patterns that identify traces in a parse tree.\nWe argue again, however, that this task is best integrated into the parser: The task\nis complex enough to warrant a probabilistic treatment, and integration may help\nparsing accuracy. A couple of complexities are that modiﬁcation by an SBAR does not\nalways involve extraction (e.g., the fact (SBARthat besoboru is played with a ball and a bat)),\nand it is not uncommon for extraction to occur through several constituents (e.g., The\nchanges (SBARthat he said the government was prepared to make TRACE)).\nOne hope is that an integrated treatment of traces will improve the parameteri-\nzation of the model. In particular, the subcategorization probabilities are smeared by\nextraction. In examples (1), (2), and (3), bought is a transitive verb; but without knowl-\nedge of traces, example (2) in training data will contribute to the probability of bought’s\nbeing an intransitive verb.\nFormalisms similar to GPSG (Gazdar et al. 1985) handle wh-movement by adding\na gap feature to each nonterminal in the tree and propagating gaps through the tree\nuntil they are ﬁnally discharged as a trace complement (see Figure 7). In extraction\ncases the Penn Treebank annotation coindexes a TRACE with the WHNP head of the SBAR,\nso it is straightforward to add this information to trees in training data.\n(1) NP → NP SBAR(+gap)\n(2) SBAR(+gap) → WHNP S-C(+gap)\n(3) S(+gap) → NP-C VP(+gap)\n(4) VP(+gap) → VB TRACE NP\nFigure 7\nA +gap feature can be added to nonterminals to describe wh-movement. The top-level NP\ninitially generates an SBAR modiﬁer but speciﬁes that it must contain an NP trace by adding\nthe +gap feature. The gap is then passed down through the tree, until it is discharged as a\nTRACE complement to the right of bought.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n601\nCollins Head-Driven Statistical Models for NL Parsing\nGiven that the LHS of the rule has a gap, there are three ways that the gap can\nbe passed down to the RHS:\nHead: The gap is passed to the head of the phrase, as in rule (3) in Figure 7.\nLeft, Right: The gap is passed on recursively to one of the left or right modiﬁers\nof the head or is discharged as a TRACE argument to the left or right of\nthe head. In rule (2) in Figure 7, it is passed on to a right modiﬁer, the S\ncomplement. In rule (4), a TRACE is generated to the right of the head VB.\nWe specify a parameter type Pg(G | P, h, H) where G is either Head, Left, or Right. The\ngenerative process is extended to choose among these cases after generating the head\nof the phrase. The rest of the phrase is then generated in different ways depending\non how the gap is propagated. In the Head case the left and right modiﬁers are\ngenerated as normal. In the Left and Right cases a +gap requirement is added to either\nthe left or right SUBCAT variable. This requirement is fulﬁlled (and removed from\nthe subcategorization list) when either a trace or a modiﬁer nonterminal that has the\n+gap feature, is generated. For example, rule (2) in Figure 7, SBAR(that)(+gap) →\nWHNP(that) S-C(bought)(+gap), has probability\nPh(WHNP| SBAR,that) × Pg(Right| SBAR,WHNP,that) × Plc({} |SBAR,WHNP,that) ×\nPrc({S-C}| SBAR,WHNP,that) × Pr(S-C(bought)(+gap)| SBAR,WHNP,that, {S-C,+gap}) ×\nPr(STOP| SBAR,WHNP,that, {}) × Pl(STOP| SBAR,WHNP,that, {})\nRule (4), VP(bought)(+gap) → VB(bought) TRACE NP(week), has probability\nPh(VB| VP,bought) × Pg(Right| VP,bought,VB) × Plc({} |VP,bought,VB) ×\nPrc({NP-C}| VP,bought,VB) × Pr(TRACE| VP,bought,VB, {NP-C, +gap}) ×\nPr(NP(week)| VP,bought,VB, {}) × Pl(STOP| VP,bought,VB, {}) ×\nPr(STOP| VP,bought,VB, {})\nIn rule (2), Right is chosen, so the +gap requirement is added to RC. Generation of\nS-C(bought)(+gap) fulﬁlls both the S-C and +gap requirements in RC. In rule (4),\nRight is chosen again. Note that generation of TRACE satisﬁes both the NP-C and +gap\nsubcategorization requirements.\n4. Special Cases: Linguistically Motivated Reﬁnements to the Models\nSections 3.1 to 3.3 described the basic framework for the parsing models in this article.\nIn this section we describe how some linguistic phenomena (nonrecursive NPs and\ncoordination, for example) clearly violate the independence assumptions of the general\nmodels. We describe a number of these special cases, in each instance arguing that the\nphenomenon violates the independence assumptions, then describing how the model\ncan be reﬁned to deal with the problem.\n4.1 Nonrecursive NPs\nWe deﬁne nonrecursive NPs (from here on referred to as base-NPs and labeled NPB\nrather than NP)a s NPs that do not directly dominate an NP themselves, unless the\ndominated NP is a possessive NP (i.e., it directly dominates a POS-tag POS). Figure 8\ngives some examples. Base-NPs deserve special treatment for three reasons:\n• The boundaries of base-NPs are often strongly marked. In particular, the\nstart points of base-NPs are often marked with a determiner or another\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n602\nComputational Linguistics Volume 29, Number 4\nFigure 8\nThree examples of structures with base-NPs.\ndistinctive item, such as an adjective. Because of this, the probability of\ngenerating the STOP symbol should be greatly increased when the\nprevious modiﬁer is, for example, a determiner. As they stand, the\nindependence assumptions in the three models lose this information. The\nprobability of NPB(dog) → DT(the) NN(dog) would be estimated as\n11\nPh(NN| NPB,dog) ×P l(DT(the)| NPB,NN,dog) ×\nPl(STOP| NPB,NN,dog) × Pr(STOP| NPB,NN,dog)\nIn making the independence assumption\nPl(STOP| DT(the), NPB,NN,dog)= Pl(STOP| NPB,NN,dog)\nthe model will fail to learn that the STOP symbol is very likely to follow\na determiner. As a result, the model will assign unreasonably high\nprobabilities to NPs such as [ NP yesterday the dog] in sentences such as\nYesterday the dog barked.\n• The annotation standard in the treebank leaves the internal structure of\nbase-NPs underspeciﬁed. For example, both pet food volume (where pet\nmodiﬁes food and food modiﬁes volume) and vanilla ice cream (where both\nvanilla and ice modify cream) would have the structure NPB → NN NN NN.\nBecause of this, there is no reason to believe that modiﬁers within NPBs\nare dependent on the head rather than the previous modiﬁer. In fact, if it\nso happened that a majority of phrases were like pet food volume, then\nconditioning on the previous modiﬁer rather than the head would be\npreferable.\n• In general it is important (in particular for the distance measure to be\neffective) to have different nonterminal labels for what are effectively\ndifferent X-bar levels. (See section 7.3.2 for further discussion.)\nFor these reasons the following modiﬁcations are made to the models:\n• The nonterminal label for base-NPs is changed from NP to NPB. For\nconsistency, whenever an NP is seen with no pre- or postmodiﬁers, an\nNPB level is added. For example, [S [NP the dog] [VP barks] ] would\nbe transformed into [S [NP [NPB the dog] ] [VP barks ] ]. These\n“extra” NPBs are removed before scoring the output of the parser against\nthe treebank.\n11 For simplicity, we give probability terms under model 1 with no distance variables; the probability\nterms with distance variables, or for models 2 and 3, will be similar, but with the addition of various\npieces of conditioning information.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n603\nCollins Head-Driven Statistical Models for NL Parsing\n• The independence assumptions are different when the parent\nnonterminal is an NPB. Speciﬁcally, equations (5) and (6) are modiﬁed to\nbe\nPl(Li(li) | H, P, h, L1(l1) ... Li−1(li−1)) = Pl(Li(li) | P, Li−1(li−1))\nPr(Ri(ri) | H, P, h, R1(r1) ... Ri−1(ri−1)) = Pr(Ri(ri) | P, Ri−1(ri−1))\nThe modiﬁer and previous-modiﬁer nonterminals are always adjacent, so\nthe distance variable is constant and is omitted. For the purposes of this\nmodel, L\n0(l0) and R0(r0) are deﬁned to be H(h). The probability of the\nprevious example is now\nPh(NN| NPB,dog) ×P l(DT(the)| NPB,NN,dog) ×\nPl(STOP| NPB,DT,the) × Pr(STOP| NPB,NN,dog)\nPresumably Pl(STOP| NPB,DT,the) will be very close to one.\n4.2 Coordination\nCoordination constructions are another example in which the independence assump-\ntions in the basic models fail badly (at least given the current annotation method in\nthe treebank). Figure 9 shows how coordination is annotated in the treebank.\n12 To\nuse an example to illustrate the problems, take the rule NP(man) → NP(man) CC(and)\nNP(dog), which has probability\nPh(NP| NP,man) × Pl(STOP| NP,NP,man) × Pr(CC(and)| NP,NP,man) ×\nPr(NP(dog)| NP,NP,man) × Pr(STOP| NP,NP,man)\nThe independence assumptions mean that the model fails to learn that there is always\nexactly one phrase following the coordinator ( CC). The basic probability models will\ngive much too high probabilities to unlikely phrases such as NP → NP CC or NP →\nNP CC NP NP. For this reason we alter the generative process to allow generation of\nboth the coordinator and the following phrase in one step; instead of just generating a\nnonterminal at each step, a nonterminal and a binary-valued coord ﬂag are generated.\ncoord = 1 if there is a coordination relationship. In the generative process, generation\nof a coord = 1 ﬂag along with a modiﬁer triggers an additional step in the generative\nFigure 9\n(a) The generic way of annotating coordination in the treebank. (b) and (c) show speciﬁc\nexamples (with base-NPs added as described in section 4.1). Note that the ﬁrst item of the\nconjunct is taken as the head of the phrase.\n12 See Appendix A of Collins (1999) for a description of how the head rules treat phrases involving\ncoordination.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n604\nComputational Linguistics Volume 29, Number 4\nprocess, namely, the generation of the coordinator tag/word pair, parameterized by\nthe Pcc parameter. For the preceding example this would give probability\nPh(NP| NP,man) ×P l(STOP| NP,NP,man) × Pr(NP(dog), coord=1| NP,NP,man) ×\nPr(STOP| NP,NP,man) × Pcc(CC,and | NP,NP,NP,man,dog)\nNote the new type of parameter, Pcc, for the generation of the coordinator word and\nPOS tag. The generation of coord=1 along with NP(dog) in the example implicitly\nrequires generation of a coordinator tag/word pair through the Pcc parameter. The\ngeneration of this tag/word pair is conditioned on the two words in the coordination\ndependency (manand dogin the example) and the label on their relationship (NP,NP,NP\nin the example, representing NP coordination).\nThe coord ﬂag is implicitly zero when normal nonterminals are generated; for ex-\nample, the phrase S(bought) → NP(week) NP(IBM) VP(bought)now has probability\nP\nh(VP| S,bought) × Pl(NP(IBM),coord=0| S,VP,bought) ×\nPl(NP(week),coord=0| S,VP,bought) × Pl(STOP| S,VP,bought) ×\nPr(STOP| S,VP,bought)\n4.3 Punctuation\nThis section describes our treatment of “punctuation” in the model, where “punctu-\nation” is used to refer to words tagged as a comma or colon. Previous work—the\ngenerative models described in Collins (1996) and the earlier version of these mod-\nels described in Collins (1997)—conditioned on punctuation as surface features of the\nstring, treating it quite differently from lexical items. In particular, the model in Collins\n(1997) failed to generate punctuation, a deﬁciency of the model. This section describes\nhow punctuation is integrated into the generative models.\nOur ﬁrst step is to raise punctuation as high in the parse trees as possible. Punc-\ntuation at the beginning or end of sentences is removed from the training/test data\naltogether.\n13 All punctuation items apart from those tagged as comma or colon (items\nsuch as quotation marks and periods, tagged “” or . ) are removed altogether. These\ntransformations mean that punctuation always appears between two nonterminals, as\nopposed to appearing at the end of a phrase. (See Figure 10 for an example.)\nFigure 10\nA parse tree before and after punctuation transformations.\n13 As one of the anonymous reviewers of this article pointed out, this choice of discarding the\nsentence-ﬁnal punctuation may not be optimal, as the ﬁnal punctuation mark may well carry useful\ninformation about the sentence structure.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n605\nCollins Head-Driven Statistical Models for NL Parsing\nPunctuation is then treated in a very similar way to coordination: Our intuition\nis that there is a strong dependency between the punctuation mark and the modi-\nﬁer generated after it. Punctuation is therefore generated with the following phrase\nthrough a punc ﬂag that is similar to the coord ﬂag (a binary-valued feature equal to\none if a punctuation mark is generated with the following phrase).\nUnder this model, NP(Vinken) → NPB(Vinken) ,(,) ADJP(old) would have\nprobability\nP\nh(NPB| NP,Vinken) × Pl(STOP| NP,NPB,Vinken) ×\nPr(ADJP(old),coord=0,punc=1| NP,NPB,Vinken) ×\nPr(STOP| NP,NPB,bought) × Pp(,, | NP,NPB,ADJP,Vinken,old) (7)\nPp is a new parameter type for generation of punctuation tag/word pairs. The genera-\ntion of punc=1 along with ADJP(old) in the example implicitly requires generation of a\npunctuation tag/word pair through the Pp parameter. The generation of this tag/word\npair is conditioned on the two words in the punctuation dependency ( Vinken and old\nin the example) and the label on their relationship ( NP,NPB,ADJP in the example.)\n4.4 Sentences with Empty (PRO) Subjects\nSentences in the treebank occur frequently with PRO subjects that may or may not be\ncontrolled: As the treebank annotation currently stands, the nonterminal is S whether\nor not a sentence has an overt subject. This is a problem for the subcategorization prob-\nabilities in models 2 and 3: The probability of having zero subjects, P\nlc({} | S, VP,\nverb), will be fairly high because of this. In addition, sentences with and without sub-\njects appear in quite different syntactic environments. For these reasons we modify\nthe nonterminal for sentences without subjects to be SG (see ﬁgure 11). The resulting\nmodel has a cleaner division of subcategorization: P\nlc({NP-C}| S, VP, verb) ≈ 1 and\nPlc({NP-C}| SG, VP, verb)= 0. The model will learn probabilistically the environ-\nments in which S and SG are likely to appear.\n4.5 A Punctuation Constraint\nAs a ﬁnal step, we use the rule concerning punctuation introduced in Collins (1996)\nto impose a constraint as follows. If for any constituent Z in the chart Z → <..X Y..>\ntwo of its children X and Y are separated by a comma, then the last word in Y must be\ndirectly followed by a comma, or must be the last word in the sentence. In training\ndata 96% of commas follow this rule. The rule has the beneﬁt of improving efﬁciency\nby reducing the number of constituents in the chart. It would be preferable to develop\na probabilistic analog of this rule, but we leave this to future research.\nFigure 11\n(a) The treebank annotates sentences with empty subjects with an empty -NONE- element\nunder subject position; (b) in training (and for evaluation), this null element is removed; (c) in\nmodels 2 and 3, sentences without subjects are changed to have a nonterminal SG.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n606\nComputational Linguistics Volume 29, Number 4\nTable 1\nThe conditioning variables for each level of back-off. For example, Ph estimation interpolates\ne1 = Ph(H | P, w, t), e2 = Ph(H | P, t), and e3 = Ph(H | P). ∆ is the distance measure.\nBack-off Ph(H | ... ) Pg(G | ... ) PL1(Li(lti), c, p | ... ) PL2(lwi | ... )\nlevel Plc(LC | ... ) PR1(Ri(rti), c, p | ... ) PR2(rwi | ... )\nPrc(RC | ... )\n1 P ,w ,t P ,H ,w ,t P ,H ,w ,t , ∆,L C Li, lti,c ,p ,P ,H ,w ,t , ∆,L C\n2 P ,t P ,H ,t P ,H ,t , ∆,L C Li, lti,c ,p ,P ,H ,t , ∆,L C\n3P P , HP , H , ∆,L C lti\n5. Practical Issues\n5.1 Parameter Estimation\nTable 1 shows the various levels of back-off for each type of parameter in the model.\nNote that we decompose P\nL(Li(lwi, lti), c, p | P, H, w, t, ∆, LC) (where lwi and lti are the\nword and POS tag generated with nonterminal Li, c and p are the coord and punc\nﬂags associated with the nonterminal, and ∆ is the distance measure) into the product\nPL1(Li(lti), c, p | P, H, w, t, ∆, LC) × PL2(lwi | Li, lti, c, p, P, H, w, t, ∆, LC)\nThese two probabilities are then smoothed separately. Eisner (1996b) originally used\nPOS tags to smooth a generative model in this way. In each case the ﬁnal estimate is\ne = λ1e1 +( 1 − λ1)(λ2e2 +( 1 − λ2)e3)\nwhere e1, e2, and e3 are maximum-likelihood estimates with the context at levels 1, 2,\nand 3 in the table, and λ1, λ2 and λ3 are smoothing parameters, where 0 ≤ λi ≤ 1. We\nuse the smoothing method described in Bikel et al. (1997), which is derived from a\nmethod described in Witten and Bell (1991). First, say that the most speciﬁc estimate\ne\n1 = n1\nf1\n; that is, f1 is the value of the denominator count in the relative frequency\nestimate. Second, deﬁne u1 to be the number of distinct outcomes seen in the f1 events\nin training data. The variable u1 can take any value from one to f1 inclusive. Then we\nset\nλ1 = f1\nf1 + 5u1\nAnalogous deﬁnitions for f2 and u2 lead to λ2 = f2\nf2+5u2\n. The coefﬁcient ﬁve was chosen\nto maximize accuracy on the development set, section 0 of the treebank (in practice it\nwas found that any value in the range 2–5 gave a very similar level of performance).\n5.2 Unknown Words and Part-of-Speech Tagging\nAll words occurring less than six times 14 in training data, and words in test data that\nhave never been seen in training, are replaced with the UNKNOWN token. This allows\nthe model to handle robustly the statistics for rare or new words. Words in test data\nthat have not been seen in training are deterministically assigned the POS tag that is\nassigned by the tagger described in Ratnaparkhi (1996). As a preprocessing step, the\n14 In Collins (1999) we erroneously stated that all words occuring less than ﬁve times in training data\nwere classiﬁed as “unknown.” Thanks to Dan Bikel for pointing out this error.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n607\nCollins Head-Driven Statistical Models for NL Parsing\ntagger is used to decode each test data sentence. All other words are tagged during\nparsing, the output from Ratnaparkhi’s tagger being ignored. The POS tags allowed\nfor each word are limited to those that have been seen in training data for that word\n(any tag/word pairs not seen in training would give an estimate of zero in the P\nL2\nand PR2 distributions). The model is fully integrated, in that part-of-speech tags are\nstatistically generated along with words in the models, so that the parser will make a\nstatistical decision as to the most likely tag for each known word in the sentence.\n5.3 The Parsing Algorithm\nThe parsing algorithm for the models is a dynamic programming algorithm, which is\nvery similar to standard chart parsing algorithms for probabilistic or weighted gram-\nmars. The algorithm has complexity O(n\n5), where n is the number of words in the\nstring. In practice, pruning strategies (methods that discard lower-probability con-\nstituents in the chart) can improve efﬁciency a great deal. The appendices of Collins\n(1999) give a precise description of the parsing algorithms, an analysis of their compu-\ntational complexity, and also a description of the pruning methods that are employed.\nSee Eisner and Satta (1999) for an O(n\n4) algorithm for lexicalized grammars that\ncould be applied to the models in this paper. Eisner and Satta (1999) also describe an\nO(n\n3) algorithm for a restricted class of lexicalized grammars; it is an open question\nwhether this restricted class includes the models in this article.\n6. Results\nThe parser was trained on sections 2–21 of the Wall Street Journal portion of the\nPenn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) (approximately 40,000\nsentences) and tested on section 23 (2,416 sentences). We use the PARSEVAL measures\n(Black et al. 1991) to compare performance:\nLabeled precision = number of correct constituents in proposed parse\nnumber of constituents in proposed parse\nLabeled recall = number of correct constituents in proposed parse\nnumber of constituents in treebank parse\nCrossing brackets = number of constituents that violate constituent boundaries\nwith a constituent in the treebank parse\nFor a constituent to be “correct,” it must span the same set of words (ignoring punctu-\nation, i.e., all tokens tagged as commas, colons, or quotation marks) and have the same\nlabel\n15 as a constituent in the treebank parse. Table 2 shows the results for models 1, 2\nand 3 and a variety of other models in the literature. Two models (Collins 2000; Char-\nniak 2000) outperform models 2 and 3 on section 23 of the treebank. Collins (2000)\nuses a technique based on boosting algorithms for machine learning that reranksn-best\noutput from model 2 in this article. Charniak (2000) describes a series of enhancements\nto the earlier model of Charniak (1997).\nThe precision and recall of the traces found by Model 3 were 93.8% and 90.1%,\nrespectively (out of 437 cases in section 23 of the treebank), where three criteria must be\nmet for a trace to be “correct”: (1) It must be an argument to the correct headword; (2)\nIt must be in the correct position in relation to that headword (preceding or following);\n15 Magerman (1995) collapses ADVP and PRT into the same label; for comparison, we also removed this\ndistinction when calculating scores.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n608\nComputational Linguistics Volume 29, Number 4\nTable 2\nResults on Section 23 of the WSJ Treebank. LR/LP = labeled recall/precision. CBs is the\naverage number of crossing brackets per sentence. 0 CBs, ≤ 2 CBs are the percentage of\nsentences with 0 or ≤ 2 crossing brackets respectively. All the results in this table are for\nmodels trained and tested on the same data, using the same evaluation metric. (Note that\nthese results show a slight improvement over those in (Collins 97); the main model changes\nwere the improved treatment of punctuation (section 4.3) together with the addition of the P\np\nand Pcc parameters.)\nModel ≤ 40 Words (2,245 sentences)\nLR LP CBs 0 CBs ≤ 2 CBs\nMagerman 1995 84.6% 84.9% 1.26 56.6% 81.4%\nCollins 1996 85.8% 86.3% 1.14 59.9% 83.6%\nGoodman 1997 84.8% 85.3% 1.21 57.6% 81.4%\nCharniak 1997 87.5% 87.4% 1.00 62.1% 86.1%\nModel 1 87.9% 88.2% 0.95 65.8% 86.3%\nModel 2 88.5% 88.7% 0.92 66.7% 87.1%\nModel 3 88.6% 88.7% 0.90 67.1% 87.4%\nCharniak 2000 90.1% 90.1% 0.74 70.1% 89.6%\nCollins 2000 90.1% 90.4% 0.73 70.7% 89.6%\nModel ≤ 100 Words (2,416 sentences)\nLR LP CBs 0 CBs ≤ 2 CBs\nMagerman 1995 84.0% 84.3% 1.46 54.0% 78.8%\nCollins 1996 85.3% 85.7% 1.32 57.2% 80.8%\nCharniak 1997 86.7% 86.6% 1.20 59.5% 83.2%\nRatnaparkhi 1997 86.3% 87.5% 1.21 60.2% —\nModel 1 87.5% 87.7% 1.09 63.4% 84.1%\nModel 2 88.1% 88.3% 1.06 64.0% 85.1%\nModel 3 88.0% 88.3% 1.05 64.3% 85.4%\nCharniak 2000 89.6% 89.5% 0.88 67.6% 87.7%\nCollins 2000 89.6% 89.9% 0.87 68.3% 87.7%\nand (3) It must be dominated by the correct nonterminal label. For example, in Figure 7,\nthe trace is an argument to bought, which it follows, and it is dominated by a VP. Of the\n437 cases, 341 were string-vacuous extraction from subject position, recovered with\n96.3% precision and 98.8% recall; and 96 were longer distance cases, recovered with\n81.4% precision and 59.4% recall.\n16\n7. Discussion\nThis section discusses some aspects of the models in more detail. Section 7.1 gives a\nmuch more detailed analysis of the parsers’ performance. In section 7.2 we examine\n16 We exclude inﬁnitival relative clauses from these ﬁgures (for example, I called a plumber TRACEto ﬁx the\nsink, where plumber is coindexed with the trace subject of the inﬁnitival). The algorithm scored 41%\nprecision and 18% recall on the 60 cases in section 23—but inﬁnitival relatives are extremely difﬁcult\neven for human annotators to distinguish from purpose clauses (in this case, the inﬁnitival could be a\npurpose clause modifying called) (Ann Taylor, personal communication, 1997).\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n609\nCollins Head-Driven Statistical Models for NL Parsing\nthe distance features in the model. In section 7.3 we examine how the model interacts\nwith the Penn Treebank style of annotation. Finally, in section 7.4 we discuss the need\nto break down context-free rules in the treebank in such a way that the model will\ngeneralize to give nonzero probability to rules not seen in training. In each case we\nuse three methods of analysis. First, we consider how various aspects of the model\naffect parsing performance, through accuracy measurements on the treebank. Second,\nwe look at the frequency of different constructions in the treebank. Third, we consider\nlinguistically motivated examples as a way of justifying various modeling choices.\n7.1 A Closer Look at the Results\nIn this section we look more closely at the parser, by evaluating its performance on\nspeciﬁc constituents or constructions. The intention is to get a better idea of the parser’s\nstrengths and weaknesses. First, Table 3 has a breakdown of precision and recall by\nconstituent type. Although somewhat useful in understanding parser performance,\na breakdown of accuracy by constituent type fails to capture the idea of attachment\naccuracy. For this reason we also evaluate the parser’s precision and recall in recov-\nering dependencies between words. This gives a better indication of the accuracy on\ndifferent kinds of attachments. A dependency is deﬁned as a triple with the following\nelements (see Figure 12 for an example tree and its associated dependencies):\n1. Modiﬁer: The index of the modiﬁer word in the sentence.\nTable 3\nRecall and precision for different constituent types, for section 0 of the treebank with model 2.\nLabel is the nonterminal label; Proportion is the percentage of constituents in the treebank\nsection 0 that have this label; Count is the number of constituents that have this label.\nProportion Count Label Recall Precision\n42.21 15146 NP 91.15 90.26\n19.78 7096 VP 91.02 91.11\n13.00 4665 S 91.21 90.96\n12.83 4603 PP 86.18 85.51\n3.95 1419 SBAR 87.81 88.87\n2.59 928 ADVP 82.97 86.52\n1.63 584 ADJP 65.41 68.95\n1.00 360 WHNP 95.00 98.84\n0.92 331 QP 84.29 78.37\n0.48 172 PRN 32.56 61.54\n0.35 126 PRT 86.51 85.16\n0.31 110 SINV 83.64 88.46\n0.27 98 NX 12.24 66.67\n0.25 88 WHADVP 95.45 97.67\n0.08 29 NAC 48.28 63.64\n0.08 28 FRAG 21.43 46.15\n0.05 19 WHPP 100.00 100.00\n0.04 16 UCP 25.00 28.57\n0.04 16 CONJP 56.25 69.23\n0.04 15 SQ 53.33 66.67\n0.03 12 SBARQ 66.67 88.89\n0.03 9 RRC 11.11 33.33\n0.02 7 LST 57.14 100.00\n0.01 3 X 0.00 —\n0.01 2 INTJ 0.00 —\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n610\nComputational Linguistics Volume 29, Number 4\n“Raw” dependencies Normalized dependencies\nRelation Modiﬁer Head Relation Modiﬁer Head\nS VP NP-C L 0 1 S VP NP-C L 0 1\nTOP TOP S R 1 −1 TOP TOP S R 1 −1\nNPB NN DT L 2 3 NPB TAG TAG L 2 3\nVP VB NP-C R 3 1 VP TAG NP-C R 3 1\nNP-C NPB PP R 4 3 NP NPB PP R 4 3\nNPB NN DT L 5 6 NPB TAG TAG L 5 6\nPP IN NP-C R 6 4 PP TAG NP-C R 6 4\nFigure 12\nA tree and its associated dependencies. Note that in “normalizing” dependencies, all POS tags\nare replaced with TAG, and the NP-C parent in the ﬁfth relation is replaced with NP .\n2. Head: The index of the headword in the sentence.\n3. Relation: A ⟨Parent, Head, Modifier, Direction⟩4-tuple, where the four\nelements are the parent, head, and modiﬁer nonterminals involved in the\ndependency and the direction of the dependency (L for left, R for right).\nFor example, ⟨S, VP, NP-C, L⟩would indicate a subject-verb dependency.\nIn coordination cases there is a ﬁfth element of the tuple, CC. For\nexample, ⟨NP, NP, NP, R, CC⟩would be an instance of NP coordination.\nIn addition, the relation is “normalized” to some extent. First, all POS tags are\nreplaced with the token TAG, so that POS-tagging errors do not lead to errors in\ndependencies.\n17 Second, any complement markings on the parent or head nontermi-\nnal are removed. For example, ⟨NP-C, NPB, PP, R⟩is replaced by ⟨NP, NPB, PP, R⟩. This\nprevents parsing errors where a complement has been mistaken to be an adjunct (or\nvice versa), leading to more than one dependency error. As an example, in Figure 12,\nif the NP the man with the telescopewas mistakenly identiﬁed as an adjunct, then without\nnormalization, this would lead to two dependency errors: Both the PPdependency and\nthe verb-object relation would be incorrect. With normalization, only the verb-object\nrelation is incorrect.\n17 The justiﬁcation for this is that there is an estimated 3% error rate in the hand-assigned POS tags in the\ntreebank (Ratnaparkhi 1996), and we didn’t want this noise to contribute to dependency errors.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n611\nCollins Head-Driven Statistical Models for NL Parsing\nTable 4\nDependency accuracy on section 0 of the treebank with Model 2. No labels means that only the\ndependency needs to be correct; the relation may be wrong; No complements means all\ncomplement (-C) markings are stripped before comparing relations; All means complement\nmarkings are retained on the modifying nonterminal.\nEvaluation Precision Recall\nNo labels 91.0% 90.9%\nNo complements 88.5% 88.5%\nAll 88.3% 88.3%\nUnder this deﬁnition, gold-standard and parser-output trees can be converted to\nsets of dependencies, and precision and recall can be calculated on these dependencies.\nDependency accuracies are given for section 0 of the treebank in table 4. Table 5 gives\na breakdown of the accuracies by dependency type.\nTable 6 shows the dependency accuracies for eight subtypes of dependency that\ntogether account for 94% of all dependencies:\n1. Complement to a verb (93.76% recall, 92.96% precision): This subtype\nincludes any relations of the form ⟨ SV P* * ⟩, where ** is any\ncomplement, or ⟨ VP TAG ** ⟩, where ** is any complement except VP-C\n(i.e., auxiliary-verb—verb dependencies are excluded). The most frequent\nverb complements, subject-verb and object-verb, are recovered with over\n95% precision and 92% recall.\n2. Other complements (94.47% recall, 94.12% precision): This subtype\nincludes any dependencies in which the modiﬁer is a complement and\nthe dependency does not fall into the complement to a verb category.\n3. PP modiﬁcation (82.29% recall, 81.51% precision): Any dependencies in\nwhich the modiﬁer is a PP.\n4. Coordination (61.47% recall, 62.20% precision).\n5. Modiﬁcation within base-NPs (93.20% recall, 92.59% precision): This\nsubtype includes any dependencies in which the parent is NPB.\n6. Modiﬁcation to NPs (73.20% recall, 75.49% precision): This subtype\nincludes any dependencies in which the parent is NP, the head is NPB,\nand the modiﬁer is not a PP.\n7. Sentential head (94.99% recall, 94.99% precision): This subtype includes\nany dependencies involving the headword of the entire sentence.\n8. Adjunct to a verb (75.11% recall, 78.44% precision): This subtype includes\nany dependencies in which the parent is VP, the head is TAG, and the\nmodiﬁer is not a PP, or in which the parent is S, the head is VP, and the\nmodiﬁer is not a PP.\nA conclusion to draw from these accuracies is that the parser is doing very well at\nrecovering the core structure of sentences: complements, sentential heads, and base-NP\nrelationships (NP chunks) are all recovered with over 90% accuracy. The main sources\nof errors are adjuncts. Coordination is especially difﬁcult for the parser, most likely\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n612\nComputational Linguistics Volume 29, Number 4\nTable 5\nAccuracy of the 50 most frequent dependency types in section 0 of the treebank, as recovered\nby model 2.\nRank Cumulative Percentage Count Relation Recall Precision\npercentage\n1 29.65 29.65 11786 NPB TAG TAG L 94.60 93.46\n2 40.55 10.90 4335 PP TAG NP-C R 94.72 94.04\n3 48.72 8.17 3248 S VP NP-C L 95.75 95.11\n4 54.03 5.31 2112 NP NPB PP R 84.99 84.35\n5 59.30 5.27 2095 VP TAG NP-C R 92.41 92.15\n6 64.18 4.88 1941 VP TAG VP-C R 97.42 97.98\n7 68.71 4.53 1801 VP TAG PP R 83.62 81.14\n8 73.13 4.42 1757 TOP TOP S R 96.36 96.85\n9 74.53 1.40 558 VP TAG SBAR-C R 94.27 93.93\n10 75.83 1.30 518 QP TAG TAG R 86.49 86.65\n11 77.08 1.25 495 NP NPB NP R 74.34 75.72\n12 78.28 1.20 477 SBAR TAG S-C R 94.55 92.04\n13 79.48 1.20 476 NP NPB SBAR R 79.20 79.54\n14 80.40 0.92 367 VP TAG ADVP R 74.93 78.57\n15 81.30 0.90 358 NPB TAG NPB L 97.49 92.82\n16 82.18 0.88 349 VP TAG TAG R 90.54 93.49\n17 82.97 0.79 316 VP TAG SG-C R 92.41 88.22\n18 83.70 0.73 289 NP NP NP R CC 55.71 53.31\n19 84.42 0.72 287 SV PP PL 90.24 81.96\n20 85.14 0.72 286 SBAR WHNP SG-C R 90.56 90.56\n21 85.79 0.65 259 VP TAG ADJP R 83.78 80.37\n22 86.43 0.64 255 S VP ADVP L 90.98 84.67\n23 86.95 0.52 205 NP NPB VP R 77.56 72.60\n24 87.45 0.50 198 ADJP TAG TAG L 75.76 70.09\n25 87.93 0.48 189 NPB TAG TAG R 74.07 75.68\n26 88.40 0.47 187 VP TAG NP R 66.31 74.70\n27 88.85 0.45 180 VP TAG SBAR R 74.44 72.43\n28 89.29 0.44 174 VP VP VP R CC 74.14 72.47\n29 89.71 0.42 167 NPB TAG ADJP L 65.27 71.24\n30 90.11 0.40 159 VP TAG SG R 60.38 68.57\n31 90.49 0.38 150 VP TAG S-C R 74.67 78.32\n32 90.81 0.32 129 SSSRC C 72.09 69.92\n33 91.12 0.31 125 PP TAG SG-C R 94.40 89.39\n34 91.43 0.31 124 QP TAG TAG L 77.42 83.48\n35 91.72 0.29 115 S VP TAG L 86.96 90.91\n36 92.00 0.28 110 NPB TAG QP L 80.91 81.65\n37 92.27 0.27 106 SINV VP NP R 88.68 95.92\n38 92.53 0.26 104 S VP S-C L 93.27 78.86\n39 92.79 0.26 102 NP NP NP R 30.39 25.41\n40 93.02 0.23 90 ADJP TAG PP R 75.56 78.16\n41 93.24 0.22 89 TOP TOP SINV R 96.63 94.51\n42 93.45 0.21 85 ADVP TAG TAG L 74.12 73.26\n43 93.66 0.21 83 SBAR WHADVP S-C R 97.59 98.78\n44 93.86 0.20 81 S VP SBAR L 88.89 85.71\n45 94.06 0.20 79 VP TAG ADVP L 51.90 49.40\n46 94.24 0.18 73 SINV VP S L 95.89 92.11\n47 94.40 0.16 63 NP NPB SG R 88.89 81.16\n48 94.55 0.15 58 S VP PRN L 25.86 48.39\n49 94.70 0.15 58 NX TAG TAG R 10.34 75.00\n50 94.83 0.13 53 NP NPB PRN R 45.28 60.00\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n613\nCollins Head-Driven Statistical Models for NL Parsing\nTable 6\nAccuracy for various types/subtypes of dependency (part 1). Only subtypes occurring more\nthan 10 times are shown.\nType Sub-type Description Count Recall Precision\nComplement to a verb S VP NP-C L Subject 3,248 95.75 95.11\nVP TAG NP-C R Object 2,095 92.41 92.15\n6,495 = 16.3% of all cases VP TAG SBAR-C R 558 94.27 93.93\nVP TAG SG-C R 316 92.41 88.22\nVP TAG S-C R 150 74.67 78.32\nS VP S-C L 104 93.27 78.86\nS VP SG-C L 14 78.57 68.75\n...\nTotal 6,495 93.76 92.96\nOther complements PP TAG NP-C R 4,335 94.72 94.04\nVP TAG VP-C R 1,941 97.42 97.98\n7,473 = 18.8% of all cases SBAR TAG S-C R 477 94.55 92.04\nSBAR WHNP SG-C R 286 90.56 90.56\nPP TAG SG-C R 125 94.40 89.39\nSBAR WHADVP S-C R 83 97.59 98.78\nPP TAG PP-C R 51 84.31 70.49\nSBAR WHNP S-C R 42 66.67 84.85\nSBAR TAG SG-C R 23 69.57 69.57\nPP TAG S-C R 18 38.89 63.64\nSBAR WHPP S-C R 16 100.00 100.00\nS ADJP NP-C L 15 46.67 46.67\nPP TAG SBAR-C R 15 100.00 88.24\n...\nTotal 7,473 94.47 94.12\nPP modiﬁcation NP NPB PP R 2,112 84.99 84.35\nVP TAG PP R 1,801 83.62 81.14\n4,473 = 11.2% of all cases SV PP PL 287 90.24 81.96\nADJP TAG PP R 90 75.56 78.16\nADVP TAG PP R 35 68.57 52.17\nNP NP PP R 23 0.00 0.00\nPP PP PP L 19 21.05 26.67\nNAC TAG PP R 12 50.00 100.00\n...\nTotal 4,473 82.29 81.51\nCoordination NP NP NP R 289 55.71 53.31\nVP VP VP R 174 74.14 72.47\n763 = 1.9% of all cases SSSR 129 72.09 69.92\nADJP TAG TAG R 28 71.43 66.67\nVP TAG TAG R 25 60.00 71.43\nNX NX NX R 25 12.00 75.00\nSBAR SBAR SBAR R 19 78.95 83.33\nPP PP PP R 14 85.71 63.16\n...\nTotal 763 61.47 62.20\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n614\nComputational Linguistics Volume 29, Number 4\nTable 6\n(cont.)\nType Subtype Description Count Recall Precision\nModiﬁcation NPB TAG TAG L 11,786 94.60 93.46\nwithin Base-NPs NPB TAG NPB L 358 97.49 92.82\n12,742 = 29.6% of all cases NPB TAG TAG R 189 74.07 75.68\nNPB TAG ADJP L 167 65.27 71.24\nNPB TAG QP L 110 80.91 81.65\nNPB TAG NAC L 29 51.72 71.43\nNPB NX TAG L 27 14.81 66.67\nNPB QP TAG L 15 66.67 76.92\n...\nTotal 12,742 93.20 92.59\nModiﬁcation to NPs NP NPB NP R Appositive 495 74.34 75.72\nNP NPB SBAR R Relative clause 476 79.20 79.54\n1,418 = 3.6% of all cases NP NPB VP R Reduced relative 205 77.56 72.60\nNP NPB SG R 63 88.89 81.16\nNP NPB PRN R 53 45.28 60.00\nNP NPB ADVP R 48 35.42 54.84\nNP NPB ADJP R 48 62.50 69.77\n...\nTotal 1,418 73.20 75.49\nSentential head TOP TOP S R 1,757 96.36 96.85\nTOP TOP SINV R 89 96.63 94.51\n1,917 = 4.8% of all cases TOP TOP NP R 32 78.12 60.98\nTOP TOP SG R 15 40.00 33.33\n...\nTotal 1,917 94.99 94.99\nAdjunct to a verb VP TAG ADVP R 367 74.93 78.57\nVP TAG TAG R 349 90.54 93.49\n2,242 = 5.6% of all cases VP TAG ADJP R 259 83.78 80.37\nS VP ADVP L 255 90.98 84.67\nVP TAG NP R 187 66.31 74.70\nVP TAG SBAR R 180 74.44 72.43\nVP TAG SG R 159 60.38 68.57\nS VP TAG L 115 86.96 90.91\nS VP SBAR L 81 88.89 85.71\nVP TAG ADVP L 79 51.90 49.40\nS VP PRN L 58 25.86 48.39\nSV PN PL 45 66.67 63.83\nSV PS GL 28 75.00 52.50\nVP TAG PRN R 27 3.70 12.50\nVP TAG S R 11 9.09 100.00\n...\nTotal 2,242 75.11 78.44\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n615\nCollins Head-Driven Statistical Models for NL Parsing\nTable 7\nResults on section 0 of the WSJ Treebank. A “YES” in the A column means that the adjacency\nconditions were used in the distance measure; likewise, a “YES” in the V column indicates\nthat the verb conditions were used in the distance measure. LR = labeled recall; LP = labeled\nprecision. CBs is the average number of crossing brackets per sentence. 0 CBs ≤ 2 CBs are the\npercentages of sentences with 0 and ≤ 2 crossing brackets, respectively.\nModel A V LR LP CBs 0 CBs ≤ 2 CBs\nModel 1 No No 75.0% 76.5% 2.18 38.5% 66.4\nModel 1 Yes No 86.6% 86.7% 1.22 60.9% 81.8\nModel 1 Yes Yes 87.8% 88.2% 1.03 63.7% 84.4\nModel 2 No No 85.1% 86.8% 1.28 58.8% 80.3\nModel 2 Yes No 87.7% 87.8% 1.10 63.8% 83.2\nModel 2 Yes Yes 88.7% 89.0% 0.95 65.7% 85.6\nbecause it often involves a dependency between two content words, leading to very\nsparse statistics.\n7.2 More about the Distance Measure\nThe distance measure, whose implementation was described in section 3.1.1, deserves\nmore discussion and motivation. In this section we consider it from three perspectives:\nits inﬂuence on parsing accuracy; an analysis of distributions in training data that\nare sensitive to the distance variables; and some examples of sentences in which the\ndistance measure is useful in discriminating among competing analyses.\n7.2.1 Impact of the Distance Measure on Accuracy. Table 7 shows the results for\nmodels 1 and 2 with and without the adjacency and verb distance measures. It is clear\nthat the distance measure improves the models’ accuracy.\nWhat is most striking is just how badly model 1 performs without the distance\nmeasure. Looking at the parser’s output, the reason for this poor performance is that\nthe adjacency condition in the distance measure is approximating subcategorization\ninformation. In particular, in phrases such as PPs and SBARs (and, to a lesser extent,\nin VPs) that almost always take exactly one complement to the right of their head,\nthe adjacency feature encodes this monovalency through parameters P(STOP|PP/SBAR,\nadjacent)= 0 and P(STOP|PP/SBAR, not adjacent)= 1. Figure 13 shows some par-\nticularly bad structures returned by model 1 with no distance variables.\nAnother surprise is that subcategorization can be very useful, but that the dis-\ntance measure has masked this utility. One interpretation in moving from the least\nparameterized model (Model 1 [No, No]) to the fully parameterized model (Model 2\n[Yes, Yes]) is that the adjacency condition adds around 11% in accuracy; the verb\ncondition adds another 1.5%; and subcategorization ﬁnally adds a mere 0.8%. Under\nthis interpretation subcategorization information isn’t all that useful (and this was my\noriginal assumption, as this was the order in which features were originally added\nto the model). But under another interpretation subcategorization is very useful: In\nmoving from Model 1 (No, No) to Model 2 (No, No), we see a 10% improvement as a\nresult of subcategorization parameters; adjacency then adds a 1.5% improvement; and\nthe verb condition adds a ﬁnal 1% improvement.\nFrom an engineering point of view, given a choice of whether to add just distance\nor subcategorization to the model, distance is preferable. But linguistically it is clear\nthat adjacency can only approximate subcategorization and that subcategorization is\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n616\nComputational Linguistics Volume 29, Number 4\nFigure 13\nTwo examples of bad parses produced by model 1 with no distance or subcategorization\nconditions (Model 1 (No, No) in table 7). In (a) one PP has two complements, the other has\nnone; in (b) the SBAR has two complements. In both examples either the adjacency condition\nor the subcategorization parameters will correct the errors, so these are examples in which the\nadjacency and subcategorization variables overlap in their utility.\nTable 8\nDistribution of nonterminals generated as postmodiﬁers to an NP (see tree to the left), at\nvarious distances from the head. A = True means the modiﬁer is adjacent to the head, V =\nTrue means there is a verb between the head and the modiﬁer. Distributions were calculated\nfrom the ﬁrst 10000 events for each of the three cases in sections 2-21 of the treebank.\nA = True, V = False A = False, V = False A = False, V = True\nPercentage ? Percentage ? Percentage ?\n70.78 STOP 88.53 STOP 97.65 STOP\n17.7 PP 5.57 PP 0.93 PP\n3.54 SBAR 2.28 SBAR 0.55 SBAR\n3.43 NP 1.55 NP 0.35 NP\n2.22 VP 0.92 VP 0.22 VP\n0.61 SG 0.38 SG 0.09 SG\n0.56 ADJP 0.26 PRN 0.07 PRN\n0.54 PRN 0.22 ADVP 0.04 ADJP\n0.36 ADVP 0.15 ADJP 0.03 ADVP\n0.08 TO 0.09 -RRB- 0.02 S\n0.08 CONJP 0.02 UCP 0.02 -RRB-\n0.03 UCP 0.01 X 0.01 X\n0.02 JJ 0.01 RRC 0.01 VBG\n0.01 VBN 0.01 RB 0.01 RB\n0.01 RRC\n0.01 FRAG\n0.01 CD\n0.01 -LRB-\nmore “correct” in some sense. In free-word-order languages, distance may not approx-\nimate subcategorization at all well: A complement may appear to either the right or\nleft of the head, confusing the adjacency condition.\n7.2.2 Frequencies in Training Data. Tables 8 and 9 show the effect of distance on the\ndistribution of modiﬁers in two of the most frequent syntactic environments: NP and\nverb modiﬁcation. The distribution varies a great deal with distance. Most striking is\nthe way that the probability of STOP increases with increasing distance: from 71% to\n89% to 98% in the NP case, from 8% to 60% to 96% in the verb case. Each modiﬁer\nprobability generally decreases with distance. For example, the probability of seeing\na PP modiﬁer to an NP decreases from 17.7% to 5.57% to 0.93%.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n617\nCollins Head-Driven Statistical Models for NL Parsing\nTable 9\nDistribution of nonterminals generated as postmodiﬁers to a verb within a VP (see tree to the\nleft), at various distances from the head. A = True means the modiﬁer is adjacent to the head;\nV = True means there is a verb between the head and the modiﬁer. The distributions were\ncalculated from the ﬁrst 10000 events for each of the distributions in sections 2–21. Auxiliary\nverbs (verbs taking a VP complement to their right) were excluded from these statistics.\nA = True, V = False A = False, V = False A = False, V = True\nPercentage ? Percentage ? Percentage ?\n39 NP-C 59.87 STOP 95.92 STOP\n15.8 PP 22.7 PP 1.73 PP\n8.43 SBAR-C 3.3 NP-C 0.92 SBAR\n8.27 STOP 3.16 SG 0.5 NP\n5.35 SG-C 2.71 ADVP 0.43 SG\n5.19 ADVP 2.65 SBAR 0.16 ADVP\n5.1 ADJP 1.5 SBAR-C 0.14 SBAR-C\n3.24 S-C 1.47 NP 0.05 NP-C\n2.82 RB 1.11 SG-C 0.04 PRN\n2.76 NP 0.82 ADJP 0.02 S-C\n2.28 PRT 0.2 PRN 0.01 VBN\n0.63 SBAR 0.19 PRT 0.01 VB\n0.41 SG 0.09 S 0.01 UCP\n0.16 VB 0.06 S-C 0.01 SQ\n0.1 S 0.06 -RRB- 0.01 S\n0.1 PRN 0.03 FRAG 0.01 FRAG\n0.08 UCP 0.02 -LRB- 0.01 ADJP\n0.04 VBZ 0.01 X 0.01 -RRB-\n0.03 VBN 0.01 VBP 0.01 -LRB-\n0.03 VBD 0.01 VB\n0.03 FRAG 0.01 UCP\n0.03 -LRB- 0.01 RB\n0.02 VBG 0.01 INTJ\n0.02 SBARQ\n0.02 CONJP\n0.01 X\n0.01 VBP\n0.01 RBR\n0.01 INTJ\n0.01 DT\n0.01 -RRB-\n7.2.3 Distance Features and Right-Branching Structures. Both the adjacency and verb\ncomponents of the distance measure allow the model to learn a preference for right-\nbranching structures. First, consider the adjacency condition. Figure 14 shows some\nexamples in which right-branching structures are more frequent. Using the statistics\nfrom Tables 8 and 9, the probability of the alternative structures can be calculated. The\nresults are given below. The right-branching structures get higher probability (although\nthis is before the lexical-dependency probabilities are multiplied in, so this “prior”\npreference for right-branching structures can be overruled by lexical preferences). If\nthe distance variables were not conditioned on, the product of terms for the two\nalternatives would be identical, and the model would have no preference for one\nstructure over another.\nProbabilities for the two alternative PP structures in Figure 14 (excluding probabil-\nity terms that are constant across the two structures; A=1 means distance is adjacent,\nA=0 means not adjacent) are as follows:\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n618\nComputational Linguistics Volume 29, Number 4\nFigure 14\nSome alternative structures for the same surface sequence of chunks ( NPB PP PP in the ﬁrst\ncase, NPB PP SBAR in the second case) in which the adjacency condition distinguishes between\nthe two structures. The percentages are taken from sections 2–21 of the treebank. In both cases\nright-branching structures are more frequent.\nRight-branching:\nP(PP|NP,NPB,A=1)P(STOP|NP,NPB,A=0)\nP(PP|NP,NPB,A=1)P(STOP|NP,NPB,A=0)\n= 0.177 × 0.8853 × 0.177 × 0.8853 = 0.02455\nNon-right-branching:\nP(PP|NP,NPB,A=1)P(PP|NP,NPB,A=0)\nP(STOP|NP,NPB,A=0)P(STOP|NP,NPB,A=1)\n= 0.177 × 0.0557 × 0.8853 × 0.7078 = 0.006178\nProbabilities for the SBAR case in Figure 14, assuming the SBAR contains a verb ( V=0\nmeans modiﬁcation does not cross a verb, V=1 means it does), are as follows:\nRight-branching:\nP(PP|NP,NPB,A=1,V=0)P(SBAR|NP,NPB,A=1,V=0)\nP(STOP|NP,NPB,A=0,V=1)P(STOP|NP,NPB,A=0,V=1)\n= 0.177 × 0.0354 × 0.9765 × 0.9765 = 0.005975\nNon-right-branching:\nP(PP|NP,NPB,A=1)P(STOP|NP,NPB,A=1)\nP(SBAR|NP,NPB,A=0)P(STOP|NP,NPB,A=0,V=1)\n= 0.177 × 0.7078 × 0.0228 × 0.9765 = 0.002789\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n619\nCollins Head-Driven Statistical Models for NL Parsing\nFigure 15\nSome alternative structures for the same surface sequence of chunks in which the verb\ncondition in the distance measure distinguishes between the two structures. In both cases the\nlow-attachment analyses will get higher probability under the model, because of the low\nprobability of generating a PP modiﬁer involving a dependency that crosses a verb. (X stands\nfor any nonterminal.)\n7.2.4 Verb Condition and Right-Branching Structures. Figure 15 shows some exam-\nples in which the verb condition is important in differentiating the probability of two\nstructures. In both cases an adjunct can attach either high or low, but high attachment\nresults in a dependency’s crossing a verb and has lower probability.\nAn alternative to the surface string feature would be a predicate such as were any\nof the previous modiﬁers in X, where X is a set of nonterminals that are likely to contain\na verb, such as VP, SBAR, S,o r SG. This would allow the model to handle cases like the\nﬁrst example in Figure 15 correctly. The second example shows why it is preferable to\ncondition on the surface string. In this case the verb is “invisible” to the top level, as\nit is generated recursively below the NP object.\n7.2.5 Structural versus Semantic Preferences. One hypothesis would be that lexical\nstatistics are really what is important in parsing: that arriving at a correct interpretation\nfor a sentence is simply a matter of ﬁnding the most semantically plausible analysis,\nand that the statistics related to lexical dependencies approximate this notion of plau-\nsibility. Implicitly, we would be just as well off (maybe even better off) if statistics were\ncalculated between items at the predicate-argument level, with no reference to struc-\nture. The distance preferences under this interpretation are just a way of mitigating\nsparse-data problems: When the lexical statistics are too sparse, then falling back on\nsome structural preference is not ideal, but is at least better than chance. This hypoth-\nesis is suggested by previous work on speciﬁc cases of attachment ambiguity such\nas PP attachment (see, e.g., Collins and Brooks 1995), which has showed that models\nwill perform better given lexical statistics, and that a straight structural preference is\nmerely a fallback.\nBut some examples suggest this is not the case: that, in fact, many sentences\nhave several equally semantically plausible analyses, but that structural preferences\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n620\nComputational Linguistics Volume 29, Number 4\ndistinguish strongly among them. Take the following example (from Pereira and War-\nren 1980):\n(4) John was believed to have been shot by Bill.\nSurprisingly, this sentence has two analyses: Bill can be the deep subject of either\nbelieved or shot. Yet people have a very strong preference for Bill to be doing the\nshooting, so much so that they may even miss the second analysis. (To see that the\ndispreferred analysis is semantically quite plausible, consider Bill believed John to have\nbeen shot.)\nAs evidence that structural preferences can even override semantic plausibility,\ntake the following example (from Pinker 1994):\n(5) Flip said that Squeaky will do the work yesterday.\nThis sentence is a garden path: The structural preference for yesterday to modify the\nmost recent verb is so strong that it is easy to miss the (only) semantically plausible\ninterpretation, paraphrased as Flip said yesterday that Squeaky will do the work.\nThe model makes the correct predictions in these cases. In example (4), the statistics\nin Table 9 show that a PP is nine times as likely to attach low as to attach high when\ntwo verbs are candidate attachment points (the chances of seeing a PP modiﬁer are\n15.8% and 1.73% in columns 1 and 5 of the table, respectively). In example (5), the\nprobability of seeing an NP (adjunct) modiﬁer to do in a nonadjacent but non-verb-\ncrossing environment is 2.11% in sections 2–21 of the treebank (8 out of 379 cases); in\ncontrast, the chance of seeing an NP adjunct modifying said across a verb is 0.026% (1\nout of 3,778 cases). The two probabilities differ by a factor of almost 80.\n7.3 The Importance of the Choice of Tree Representation\nFigures 16 and 17 show some alternative styles of syntactic annotation. The Penn\nTreebank annotation style tends to leave trees quite ﬂat, typically with one level of\nstructure for each X-bar level; at the other extreme are completely binary-branching\nrepresentations. The two annotation styles are in some sense equivalent, in that it\nis easy to deﬁne a one-to-one mapping between them. But crucially, two different\nannotation styles may lead to quite different parsing accuracies for a given model,\neven if the two representations are equivalent under some one-to-one mapping.\nA parsing model does not need to be tied to the annotation style of the treebank\non which it is trained. The following procedure can be used to transform trees in both\ntraining and test data into a new representation:\nFigure 16\nAlternative annotation styles for a sentence S with a verb head V , left modiﬁers X1, X2, and\nright modiﬁers Y1, Y2: (a) the Penn Treebank style of analysis (one level of structure for each\nbar level); (b) an alternative but equivalent binary branching representation.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n621\nCollins Head-Driven Statistical Models for NL Parsing\nFigure 17\nAlternative annotation styles for a noun phrase with a noun head N, left modiﬁers X1, X2,\nand right modiﬁers Y1, Y2: (a) the Penn Treebank style of analysis (one level of structure for\neach bar level, although note that both the nonrecursive and the recursive noun phrases are\nlabeled NP; (b) an alternative but equivalent binary branching representation; (a\n′) our\nmodiﬁcation of the Penn Treebank style to differentiate recursive and nonrecursive NPs (in\nsome sense NPB is a bar 1 structure and NP is a bar 2 structure).\n1. Transform training data trees into the new representation and train the\nmodel.\n2. Recover parse trees in the new representation when running the model\nover test data sentences.\n3. Convert the test output back into the treebank representation for scoring\npurposes.\nAs long as there is a one-to-one mapping between the treebank and the new rep-\nresentation, nothing is lost in making such a transformation. Goodman (1997) and\nJohnson (1997) both suggest this strategy. Goodman (1997) converts the treebank into\nbinary-branching trees. Johnson (1997) considers conversion to a number of different\nrepresentations and discusses how this inﬂuences accuracy for nonlexicalized PCFGs.\nThe models developed in this article have tacitly assumed the Penn Treebank\nstyle of annotation and will perform badly given other representations (for example,\nbinary-branching trees). This section makes this point more explicit, describing exactly\nwhat annotation style is suitable for the models and showing how other annotation\nstyles will cause problems. This dependence on Penn Treebank–style annotations does\nnot imply that the models are inappropriate for a treebank annotated in a different\nstyle: In this case we simply recommend transforming the trees into ﬂat, one-level-\nper-X-bar-level trees before training the model, as in the three-step procedure outlined\nabove.\nOther models in the literature are also very likely to be sensitive to annotation\nstyle. Charniak’s (1997) models will most likely perform quite differently with binary-\nbranching trees (for example, his current models will learn that rules such as VP →\nVS GP P are very rare, but with binary-branching structures, this context sensitivity\nwill be lost). The models of Magerman (1995) and Ratnaparkhi (1997) use contextual\npredicates that would most likely need to be modiﬁed given a different annotation\nstyle. Goodman’s (1997) models are the exception, as he already speciﬁes that the\ntreebank should be transformed into his chosen representation, binary-branching trees.\n7.3.1 Representation Affects Structural, not Lexical, Preferences. The alternative rep-\nresentations in Figures 16 and 17 have the same lexical dependencies (providing that\nthe binary-branching structures are centered about the head of the phrase, as in the\nexamples). The difference between the representations involves structural preferences\nsuch as the right-branching preferences encoded by the distance measure. Applying\nthe models in this article to treebank analyses that use this type of “head-centered”\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n622\nComputational Linguistics Volume 29, Number 4\nFigure 18\nBB = binary-branching structures; FLAT = Penn treebank style annotations. In each case the\nbinary-branching annotation style prevents the model from learning that these structures\nshould receive low probability because of the long distance dependency associated with the\nﬁnal PP (in boldface).\nbinary-branching tree will result in a distance measure that incorrectly encodes a pref-\nerence for right-branching structures.\nTo see this, consider the examples in Figure 18. In each binary-branching example,\nthe generation of the ﬁnal modifying PP is “blind” to the distance between it and the\nhead that it modiﬁes. At the top level of the tree, it is apparently adjacent to the head;\ncrucially, the closer modiﬁer ( SG in (a), the other PP in (b)) is hidden lower in the tree\nstructure. So the model will be unable to differentiate generation of the PP in adjacent\nversus nonadjacent or non-verb-crossing versus verb-crossing environments, and the\nstructures in Figure 18 will be assigned unreasonably high probabilities.\nThis does not mean that distance preferences cannot be encoded in a binary-\nbranching PCFG. Goodman (1997) achieves this by adding distance features to the non-\nterminals. The spirit of this implementation is that the top-level rules VP → VP PPand\nNP → NP PP would be modiﬁed to VP → VP(+rverb) PP and NP → NP(+rmod) PP,\nrespectively, where (+rverb) means a phrase in which the head has a verb in its right\nmodiﬁers, and ( +rmod) means a phrase that has at least one right modiﬁer to the\nhead. The model will learn from training data that P(VP → VP(+rverb) PP|VP) ≪\nP(VP → VP(-rverb) PP|VP), that is, that a prepositional-phrase modiﬁcation is much\nmore likely when it does not cross a verb.\n7.3.2 The Importance of Differentiating Nonrecursive from Recursive NPs. Figure 19\nshows the modiﬁcation to the Penn Treebank annotation to relabel base-NPs as NPB.\nIt also illustrates a problem that arises if a distinction between the two is not made:\nStructures such as that in Figure 19(b) are assigned high probabilities even if they\nFigure 19\n(a) The way the Penn Treebank annotates NPs. (a′) Our modiﬁcation to the annotation, to\ndifferentiate recursive (NP) from nonrecursive ( NPB) noun phrases. (b) A structure that is never\nseen in training data but will receive much too high a probability from a model trained on\ntrees of style (a).\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n623\nCollins Head-Driven Statistical Models for NL Parsing\nFigure 20\nExamples of other phrases in the Penn Treebank in which nonrecursive and recursive phrases\nare not differentiated.\nare never seen in training data. (Johnson [1997] notes that this structure has a higher\nprobability than the correct, ﬂat structure, given counts taken from the treebank for\na standard PCFG.) The model is fooled by the binary-branching style into modeling\nboth PPs as being adjacent to the head of the noun phrase, so 19(b) will be assigned a\nvery high probability.\nThis problem does not apply only to NPs: Other types of phrases such as adjectival\nphrases (ADJPs) or adverbial phrases ( ADVPs) also have nonrecursive (bar 1) and recur-\nsive (bar 2) levels, which are not differentiated in the Penn Treebank. (See Figure 20 for\nexamples.) Ideally these cases should be differentiated too: We did not implement this\nchange because it is unlikely to make much difference in accuracy, given the relative\ninfrequency of these cases (excluding coordination cases, and looking at the 80,254\ninstances in sections 2–21 of the Penn Treebank in which a parent and head nonter-\nminal are the same: 94.5% are the NP case; 2.6% are cases of coordination in which a\npunctuation mark is the coordinator;\n18 only 2.9% are similar to those in Figure 20).\n7.3.3 Summary. To summarize, the models in this article assume the following:\n1. Tree representations are “ﬂat”: that is, one level per X-bar level.\n2. Different X-bar levels have different labels (in particular, nonrecursive\nand recursive levels are differentiated, at least for the most frequent case\nof NPs).\n7.4 The Need to Break Down Rules\nThe parsing approaches we have described concentrate on breaking down context-free\nrules in the treebank into smaller components. Lexicalized rules were initially broken\ndown to bare-bones Markov processes, then increased dependency on previously gen-\nerated modiﬁers was built back up through the distance measure and subcategoriza-\ntion. Even with this additional context, the models are still able to recover rules in test\ndata that have never been seen in training data.\nAn alternative, proposed in Charniak (1997), is to limit parsing to those context-\nfree rules seen in training data. A lexicalized rule is predicted in two steps. First,\nthe whole context-free rule is generated. Second, the lexical items are ﬁlled in. The\nprobability of a rule is estimated as\n19\nP(Ln(ln) ... L1(l1)H(h)R1(r1) ... Rm(rm) | P, h)=\nP(Ln ... L1HR1 ... Rm) | P, h) ×\n∏\ni=1...n\nPl(li | Li, P, h) ×\n∏\nj=1...m\nPr(rj | Rj, P, h)\n18 For example, ( S (S John eats apples); ( S Mary eats bananas)).\n19 Charniak’s model also conditions on the parent of the nonterminal being expanded; we omit this here\nfor brevity.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n624\nComputational Linguistics Volume 29, Number 4\nThe estimation technique used in Charniak (1997) for the CF rule probabilities inter-\npolates several estimates, the lowest being P(Ln ... L1HR1 ... Rm) | P). Any rules not\nseen in training data will be assigned zero probability with this model. Parse trees in\ntest data will be limited to include rules seen in training.\nA problem with this approach is coverage. As shown in this section, many test data\nsentences will require rules that have not been seen in training. This gives motivation\nfor breaking down rules into smaller components. This section motivates the need to\nbreak down rules from four perspectives. First, we discuss how the Penn Treebank\nannotation style leads to a very large number of grammar rules. Second, we assess the\nextent of the coverage problem by looking at rule frequencies in training data. Third,\nwe conduct experiments to assess the impact of the coverage problem on accuracy.\nFourth, we discuss how breaking rules down may improve estimation as well as\ncoverage.\n7.4.1 The Penn Treebank Annotation Style Leads to Many Rules. The “ﬂatness” of\nthe Penn Treebank annotation style has already been discussed, in section 7.3. The\nﬂatness of the trees leads to a very large (and constantly growing) number of rules,\nprimarily because the number of adjuncts to a head is potentially unlimited: For ex-\nample, there can be any number of PP adjuncts to a head verb. A binary-branching\n(Chomsky adjunction) grammar can generate an unlimited number of adjuncts with\nvery few rules. For example, the following grammar generates any sequence VP → V\nNP PP*:\nVP → VN P\nVP → VP PP\nIn contrast, the Penn Treebank style would create a new rule for each number of PPs\nseen in training data. The grammar would be\nVP → VN P\nVP → VN PP P\nVP → VN PP PP P\nVP → VN PP PP PP P\nand so on\nOther adverbial adjuncts, such as adverbial phrases or adverbialSBARs, can also modify\na verb several times, and all of these different types of adjuncts can be seen together in\nthe same rule. The result is a combinatorial explosion in the number of rules. To give\na ﬂavor of this, here is a random sample of rules of the format VP → VB modifier*\nthat occurred only once in sections 2–21 of the Penn Treebank:\nVP → VB NP NP NP PRN\nVP → VB NP SBAR PP SG ADVP\nVP → VB NP ADVP ADVP PP PP\nVP → VB RB\nVP → VB NP PP NP SBAR\nVP → VB NP PP SBAR PP\nIt is not only verb phrases that cause this kind of combinatorial explosion: Other\nphrases, in particular nonrecursive noun phrases, also contribute a huge number of\nrules. The next section considers the distributional properties of the rules in more\ndetail.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n625\nCollins Head-Driven Statistical Models for NL Parsing\nNote that there is good motivation for the Penn Treebank’s decision to repre-\nsent rules in this way, rather than with rules expressing Chomsky adjunction (i.e., a\nschema in which complements and adjuncts are separated, through rule types ⟨VP →\nVB {complement}*⟩and ⟨VP → VP {adjunct}⟩). First, it allows the argument/adjunct\ndistinction for PP modiﬁers to verbs to be left undeﬁned: This distinction was found\nto be very difﬁcult for annotators. Second, in the surface ordering (as opposed to deep\nstructure), adjuncts are often found closer to the head than complements, thereby yield-\ning structures that fall outside the Chomsky adjunction schema. For example, a rule\nsuch as ⟨VP → VB NP-C PP SBAR-C⟩is found very frequently in the Penn Treebank;\nSBAR complements nearly always extrapose over adjuncts.\n7.4.2 Quantifying the Coverage Problem. To quantify the coverage problem, rules\nwere collected from sections 2–21 of the Penn Treebank. Punctuation was raised as\nhigh as possible in the tree, and the rules did not have complement markings or the\ndistinction between base-NPs and recursive NPs. Under these conditions, 939,382 rule\ntokens were collected; there were 12,409 distinct rule types. We also collected the count\nfor each rule. Table 10 shows some statistics for these rules.\nA majority of rules in the grammar (6,765, or 54.5%) occur only once. These rules\naccount for 0.72% of rules by token. That is, if one of the 939,382 rule tokens in sections\n2–21 of the treebank were drawn at random, there would be a 0.72% chance of its being\nthe only instance of that rule in the 939,382 tokens. On the other hand, if a rule were\ndrawn at random from the 12,409 rules in the grammar induced from those sections,\nthere would be a 54.5% chance of that rule’s having occurred only once.\nThe percentage by token of the one-count rules is an indication of the coverage\nproblem. From this estimate, 0.72% of all rules (or 1 in 139 rules) required in test data\nwould never have been seen in training. It was also found that 15.0% (1 in 6.67) of all\nsentences have at least one rule that occurred just once. This gives an estimate that\nroughly 1 in 6.67 sentences in test data will not be covered by a grammar induced\nfrom 40,000 sentences in the treebank.\nIf the complement markings are added to the nonterminals, and the base-NP/non-\nrecursive NP distinction is made, then the coverage problem is made worse. Table 11\ngives the statistics in this case. By our counts, 17.1% of all sentences (1 in 5.8 sentences)\ncontain at least 1 one-count rule.\nTable 10\nStatistics for rules taken from sections 2–21 of the treebank, with complement markings not\nincluded on nonterminals.\nRule count Number of Rules Percentage Number of Rules Percentage of rules\nby type by type by token by token\n1 6765 54.52 6765 0.72\n2 1688 13.60 3376 0.36\n3 695 5.60 2085 0.22\n4 457 3.68 1828 0.19\n5 329 2.65 1645 0.18\n6 ... 10 835 6.73 6430 0.68\n11 ... 20 496 4.00 7219 0.77\n21 ... 50 501 4.04 15931 1.70\n51 ... 100 204 1.64 14507 1.54\n> 100 439 3.54 879596 93.64\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n626\nComputational Linguistics Volume 29, Number 4\nTable 11\nStatistics for rules taken from sections 2–21 of the treebank, with complement markings\nincluded on nonterminals.\nRule count Number of Rules Percentage of rules Number of Rules Percentage of rules\nby type by type by token by token\n1 7865 55.00 7865 0.84\n2 1918 13.41 3836 0.41\n3 815 5.70 2445 0.26\n4 528 3.69 2112 0.22\n5 377 2.64 1885 0.20\n6 ... 10 928 6.49 7112 0.76\n11 ... 20 595 4.16 8748 0.93\n21 ... 50 552 3.86 17688 1.88\n51 ... 100 240 1.68 16963 1.81\n> 100 483 3.38 870728 92.69\nTable 12\nResults on section 0 of the Treebank. The label restricted means the model is restricted to\nrecovering rules that have been seen in training data. LR = labeled recall. LP = labeled\nprecision. CBs is the average number of crossing brackets per sentence. 0 CBs and ≤ 2 CBs are\nthe percentages of sentences with 0 and ≤ 2 crossing brackets, respectively.\nModel Accuracy\nLR LP CBs 0 CBs ≤ 2 CBs\nModel 1 87.9 88.3 1.02 63.9 84.4\nModel 1 (restricted) 87.4 86.7 1.19 61.7 81.8\nModel 2 88.8 89.0 0.94 65.9 85.6\nModel 2 (restricted) 87.9 87.0 1.19 62.5 82.4\n7.4.3 The Impact of Coverage on Accuracy. Parsing experiments were used to assess\nthe impact of the coverage problem on parsing accuracy. Section 0 of the treebank was\nparsed with models 1 and 2 as before, but the parse trees were restricted to include\nrules already seen in training data. Table 12 shows the results. Restricting the rules\nleads to a 0.5% decrease in recall and a 1.6% decrease in precision for model 1, and a\n0.9% decrease in recall and a 2.0% decrease in precision for model 2.\n7.4.4 Breaking Down Rules Improves Estimation. Coverage problems are not the\nonly motivation for breaking down rules. The method may also improve estimation.\nTo see this, consider the rules headed by told, whose counts are shown in Table 13.\nEstimating the probability P(Rule | VP , told) using Charniak’s (1997) method would\ninterpolate two maximum-likelihood estimates:\nλP\nml(Rule | VP , told)+( 1 − λ)Pml(Rule | VP)\nEstimation interpolates between the speciﬁc, lexically sensitive distribution in Table 13\nand the nonlexical estimate based on just the parent nonterminal, VP. There are many\ndifferent rules in the more speciﬁc distribution (26 different rule types, out of 147\ntokens in which told was a VP head), and there are several one-count rules (11 cases).\nFrom these statistics λ would have to be relatively low. There is a high chance that\na new rule for told will be required in test data; therefore a reasonable amount of\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n627\nCollins Head-Driven Statistical Models for NL Parsing\nTable 13\n(a) Distribution over rules with told as the head (from sections 2–21 of the treebank); (b)\ndistribution over subcategorization frames with told as the head.\n(a)\nCount Rule\n70 VP told → VBD NP-C SBAR-C\n23 VP told → VBD NP-C\n6 VP told → VBD NP-C SG-C\n5 VP told → VBD NP-C NP SBAR-C\n5 VP told → VBD NP-C : S-C\n4 VP told → VBD NP-C PP SBAR-C\n4 VP told → VBD NP-C PP\n4 VP told → VBD NP-C NP\n3 VP told → VBD NP-C PP NP SBAR-C\n2 VP told → VBD NP-C PP PP\n2 VP told → VBD NP-C NP PP\n2 VP told → VBD NP-C , SBAR-C\n2 VP told → VBD NP-C , S-C\n2 VP told → VBD\n2 VP told → ADVP VBD NP-C SBAR-C\n1 VP told → VBD NP-C SG-C SBAR\n1 VP told → VBD NP-C SBAR-C PP\n1 VP told → VBD NP-C SBAR , PP\n1 VP told → VBD NP-C PP SG-C\n1 VP told → VBD NP-C PP NP\n1 VP told → VBD NP-C PP : S-C\n1 VP told → VBD NP-C NP : S-C\n1 VP told → VBD NP-C ADVP SBAR-C\n1 VP told → VBD NP-C ADVP PP NP\n1 VP told → VBD NP-C ADVP\n1 VP told → VBD NP-C , PRN , SBAR-C\n147 Total\n(b)\nCount Subcategorization frame\n89 {NP-C, SBAR-C}\n39 {NP-C}\n9 {NP-C, S-C}\n8 {NP-C, SG-C}\n2 {}\n147 Total\nprobability mass must be left to the backed-off estimate Pml(Rule | VP).\nThis estimation method is missing a crucial generalization: In spite of there being\nmany different rules, the distribution over subcategorization frames is much sharper.\nTold is seen with only ﬁve subcategorization frames in training data: The large number\nof rules is almost entirely due to adjuncts or punctuation appearing after or between\ncomplements. The estimation method in model 2 effectively estimates the probability\nof a rule as\nP\nlc(LC | VP , told) × Prc(RC | VP , told) × P(Rule | VP , told, LC, RC)\nThe left and right subcategorization frames, LC and RC, are chosen ﬁrst. The entire\nrule is then generated by Markov processes.\nOnce armed with the Plc and Prc parameters, the model has the ability to learn the\ngeneralization that told appears with a quite limited, sharp distribution over subcatego-\nrization frames. Say that these parameters are again estimated through interpolation,\nfor example\nλP\nml(LC | VP , told)+( 1 − λ)Pml(LC | VP)\nIn this case λ can be quite high. Only ﬁve subcategorization frames (as opposed to\n26 rule types) have been seen in the 147 cases. The lexically speciﬁc distribution\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n628\nComputational Linguistics Volume 29, Number 4\nPml(LC | VP , told) can therefore be quite highly trusted. Relatively little probability\nmass is left to the backed-off estimate.\nIn summary, from the distributions in Table 13, the model should be quite uncertain\nabout what rules told can appear with. It should be relatively certain, however, about\nthe subcategorization frame. Introducing subcategorization parameters allows the\nmodel to generalize in an important way about rules. We have carefully isolated the\n“core” of rules—the subcategorization frame—that the model should be certain about.\nWe should note that Charniak’s method will certainly have some advantages in\nestimation: It will capture some statistical properties of rules that our independence\nassumptions will lose (e.g., the distribution over the number of PP adjuncts seen for a\nparticular head).\n8. Related Work\nUnfortunately, because of space limitations, it is not possible to give a complete review\nof previous work in this article. In the next two sections we give a detailed comparison\nof the models in this article to the lexicalized PCFG model of Charniak (1997) and the\nhistory-based models of Jelinek et al. (1994), Magerman (1995), and Ratnaparkhi (1997).\nFor discussion of additional related work, chapter 4 of Collins (1999) attempts to\ngive a comprehensive review of work on statistical parsing up to around 1998. Of\nparticular relevance is other work on parsing the Penn WSJ Treebank (Jelinek et al.\n1994; Magerman 1995; Eisner 1996a, 1996b; Collins 1996; Charniak 1997; Goodman\n1997; Ratnaparkhi 1997; Chelba and Jelinek 1998; Roark 2001). Eisner (1996a, 1996b)\ndescribes several dependency-based models that are also closely related to the mod-\nels in this article. Collins (1996) also describes a dependency-based model applied\nto treebank parsing. Goodman (1997) describes probabilistic feature grammars and\ntheir application to parsing the treebank. Chelba and Jelinek (1998) describe an in-\ncremental, history-based parsing approach that is applied to language modeling for\nspeech recognition. History-based approaches were introduced to parsing in Black et\nal. (1992). Roark (2001) describes a generative probabilistic model of an incremental\nparser, with good results in terms of both parse accuracy on the treebank and also\nperplexity scores for language modeling.\nEarlier work that is of particular relevance considered the importance of relations\nbetween lexical heads for disambiguation in parsing. See Hindle and Rooth (1991) for\none of the earliest pieces of research on this topic in the context of prepositional-phrase\nattachment ambiguity. For work that uses lexical relations for parse disambiguation—\nall with very promising results—see Sekine et al. (1992), Jones and Eisner (1992a,\n1992b), and Alshawi and Carter (1994). Statistical models of lexicalized grammatical\nformalisms also lead to models with parameters corresponding to lexical dependen-\ncies. See Resnik (1992), Schabes (1992), and Schabes and Waters (1993) for work on\nstochastic tree-adjoining grammars. Joshi and Srinivas (1994) describe an alternative\n“supertagging” model for tree-adjoining grammars. See Alshawi (1996) for work on\nstochastic head-automata, and Lafferty, Sleator, and Temperley (1992) for a stochastic\nversion of link grammar. De Marcken (1995) considers stochastic lexicalized PCFGs,\nwith speciﬁc reference to EM methods for unsupervised training. Seneff (1992) de-\nscribes the use of Markov models for rule generation, which is closely related to\nthe Markov-style rules in the models in the current article. Finally, note that not all\nmachine-learning methods for parsing are probabilistic. See Brill (1993) and Hermjakob\nand Mooney (1997) for rule-based learning systems.\nIn recent work, Chiang (2000) has shown that the models in the current article\ncan be implemented almost unchanged in a stochastic tree-adjoining grammar. Bikel\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n629\nCollins Head-Driven Statistical Models for NL Parsing\n(2000) has developed generative statistical models that integrate word sense informa-\ntion into the parsing process. Eisner (2002) develops a sophisticated generative model\nfor lexicalized context-free rules, making use of a probabilistic model of lexicalized\ntransformations between rules. Blaheta and Charniak (2000) describe methods for the\nrecovery of the semantic tags in the Penn Treebank annotations, a signiﬁcant step\nforward from the complement/adjunct distinction recovered in model 2 of the cur-\nrent article. Charniak (2001) gives measurements of perplexity for a lexicalized PCFG.\nGildea (2001) reports on experiments investigating the utility of different features in\nbigram lexical-dependency models for parsing. Miller et al. (2000) develop generative,\nlexicalized models for information extraction of relations. The approach enhances non-\nterminals in the parse trees to carry semantic labels and develops a probabilistic model\nthat takes these labels into account. Collins et al. (1999) describe how the models in\nthe current article were applied to parsing Czech. Charniak (2000) describes a pars-\ning model that also uses Markov processes to generate rules. The model takes into\naccount much additional context (such as previously generated modiﬁers, or nonter-\nminals higher in the parse trees) through a maximum-entropy-inspired model. The use\nof additional features gives clear improvements in performance. Collins (2000) shows\nsimilar improvements through a quite different model based on boosting approaches\nto reranking (Freund et al. 1998). An initial model—in fact Model 2 described in the\ncurrent article—is used to generate N-best output. The reranking approach attempts to\nrerank the N-best lists using additional features that are not used in the initial model.\nThe intention of this approach is to allow greater ﬂexibility in the features that can be\nincluded in the model. Finally, Bod (2001) describes a very different approach (a DOP\napproach to parsing) that gives excellent results on treebank parsing, comparable to\nthe results of Charniak (2000) and Collins (2000).\n8.1 Comparison to the Model of Charniak (1997)\nWe now give a more detailed comparison of the models in this article to the parser of\nCharniak (1997). The model described in Charniak (1997) has two types of parameters:\n1. Lexical-dependency parameters. Charniak’s dependency parameters are\nsimilar to the L2 parameters of section 5.1. Whereas our parameters are\nP\nL2(lwi | Li, lti, c, p, P, H, w, t, ∆, LC)\nCharniak’s parameters in our notation would be\nPL2(lwi | Li, P, w)\nFor example, the dependency parameter for an NP headed by proﬁts,\nwhich is the subject of the verb rose, would be\nP(profits | NP, S, rose).\n2. Rule parameters. The second type of parameters are associated with\ncontext-free rules in the tree. As an example, take the S node in the\nfollowing tree:\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n630\nComputational Linguistics Volume 29, Number 4\nThis nonterminal could expand with any of the rules S → β in the\ngrammar. The rule probability is deﬁned as P(S → β|rose, S, VP). So the\nrule probability depends on the nonterminal being expanded, its\nheadword, and also its parent.\nThe next few sections give further explanation of the differences between Charniak’s\nmodels and the models in this article.\n8.1.1 Additional Features of Charniak’s Model. There are some notable additional\nfeatures of Charniak’s model. First, the rule probabilities are conditioned on the par-\nent of the nonterminal being expanded. Our models do not include this information,\nalthough distinguishing recursive from nonrecursive NPs can be considered a reduced\nform of this information. (See section 7.3.2 for a discussion of this distinction; the argu-\nments in that section are also motivation for Charniak’s choice of conditioning on the\nparent.) Second, Charniak uses word-class information to smooth probabilities and re-\nports a 0.35% improvement from this feature. Finally, Charniak uses 30 million words\nof text for unsupervised training. A parser is trained from the treebank and used to\nparse this text; statistics are then collected from this machine-parsed text and merged\nwith the treebank statistics to train a second model. This gives a 0.5% improvement\nin performance.\n8.1.2 The Dependency Parameters of Charniak’s Model. Though similar to ours,\nCharniak’s dependency parameters are conditioned on less information. As noted\npreviously, whereas our parameters are P\nL2(lwi | Li, lti, c, p, P, H, w, t, ∆, LC), Charniak’s\nparameters in our notation would be PL2(lwi | Li, P, w). The additional information\nincluded in our models is as follows:\nH The head nonterminal label ( VP in the previous proﬁts/rose example). At ﬁrst\nglance this might seem redundant: For example, an S will usually take\na VP as its head. In some cases, however, the head label can vary: For\nexample, an S can take another S as its head in coordination cases.\nlti, t The POS tags for the head and modiﬁer words. Inclusion of these tags al-\nlows our models to use POS tags as word class information. Charniak’s\nmodel may be missing an important generalization in this respect. Char-\nniak (2000) shows that using the POS tags as word class information in\nthe model is important for parsing accuracy.\nc The coordination ﬂag. This distinguishes, for example, coordination cases from\nappositives: Charniak’s model will have the same parameter— P(modiﬁer|\nhead, NP, NP)—in both of these cases.\np, ∆,LC/RC The punctuation, distance, and subcategorization variables. It is dif-\nﬁcult to tell without empirical tests whether these features are important.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n631\nCollins Head-Driven Statistical Models for NL Parsing\n8.1.3 The Rule Parameters of Charniak’s Model. The rule parameters in Charniak’s\nmodel are effectively decomposed into our L1 parameters (section 5.1), the head pa-\nrameters, and—in models 2 and 3—the subcategorization and gap parameters. This\ndecomposition allows our model to assign probability to rules not seen in training\ndata: See section 7.4 for an extensive discussion.\n8.1.4 Right-Branching Structures in Charniak’s Model. Our models use distance fea-\ntures to encode preferences for right-branching structures. Charniak’s model does not\nrepresent this information explicitly but instead learns it implicitly through rule prob-\nabilities. For example, for an NP PP PP sequence, the preference for a right-branching\nstructure is encoded through a much higher probability for the rule NP → NP PP than\nfor the rule NP → NP PP PP. (Note that conditioning on the rule’s parent is needed to\ndisallow the structure [NP [NP PP] PP]; see Johnson [1997] for further discussion.)\nThis strategy does not encode all of the information in the distance measure. The\ndistance measure effectively penalizes rules NP → NPB NP PP where the middle NP\ncontains a verb: In this case the PP modiﬁcation results in a dependency that crosses a\nverb. Charniak’s model is unable to distinguish cases in which the middle NP contains\na verb (i.e., the PP modiﬁcation crosses a verb) from those in which it does not.\n8.2 A Comparison to the Models of Jelinek et al. (1994), Magerman (1995), and Rat-\nnaparkhi (1997)\nWe now make a detailed comparison of our models to the history-based models of Rat-\nnaparkhi (1997), Jelinek et al. (1994), and Magerman (1995). A strength of these models\nis undoubtedly the powerful estimation techniques that they use: maximum-entropy\nmodeling (in Ratnaparkhi 1997) or decision trees (in Jelinek et al. 1994 and Magerman\n1995). A weakness, we will argue in this section, is the method of associating parame-\nters with transitions taken by bottom-up, shift-reduce-style parsers. We give examples\nin which this method leads to the parameters’ unnecessarily fragmenting the training\ndata in some cases or ignoring important context in other cases. Similar observations\nhave been made in the context of tagging problems using maximum-entropy models\n(Lafferty, McCallum, and Pereira 2001; Klein and Manning 2002).\nWe ﬁrst analyze the model of Magerman (1995) through three common examples\nof ambiguity: PP attachment, coordination, and appositives. In each case a word se-\nquence S has two competing structures, T\n1 and T2, with associated decision sequences\n⟨d1, ... , dn⟩and ⟨e1, ... , em⟩, respectively. Thus the probability of the two structures can\nbe written as\nP(T1|S)=\n∏\ni=1...n\nP(di|d1 ... di−1, S)\nP(T2|S)=\n∏\ni=1...m\nP(ei|e1 ... ei−1, S)\nIt will be useful to isolate the decision between the two structures to a single probability\nterm. Let the value j be the minimum value of i such that di ̸= ei. Then we can rewrite\nthe two probabilities as follows:\nP(T1|S)=\n∏\ni=1...j−1\nP(di|d1 ... di−1, S) × P(dj|d1 ... dj−1, S) ×\n∏\ni=j+1...n\nP(di|d1 ... di−1, S)\nP(T2|S)=\n∏\ni=1...j−1\nP(ei|e1 ... ei−1, S) × P(ej|e1 ... ej−1, S) ×\n∏\ni=j+1...m\nP(ei|e1 ... ei−1, S)\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n632\nComputational Linguistics Volume 29, Number 4\nThe ﬁrst thing to note is that ∏\ni=1...j−1 P(di|d1 ... di−1, S)= ∏\ni=1...j−1 P(ei|e1 ... ei−1, S),\nso that these probability terms are irrelevant to the decision between the two structures.\nWe make one additional assumption, that\n∏\ni=j+1...n\nP(di|d1 ... di−1, S) ≈\n∏\ni=j+1...m\nP(ei|e1 ... ei−1, S) ≈ 1\nThis is justiﬁed for the examples in this section, because once the jth decision is made,\nthe following decisions are practically deterministic. Equivalently, we are assuming\nthat P(T\n1|S)+ P(T2|S) ≈ 1, that is, that very little probability mass is lost to trees other\nthan T1 or T2. Given these two equalities, we have isolated the decision between the\ntwo structures to the parameters P(dj|d1 ... dj−1, S) and P(ej|e1 ... ej−1, S).\nFigure 21 shows a case of PP attachment. The ﬁrst thing to note is that the PP\nattachment decision is made before the PP is even built. The decision is linked to the\nNP preceding the preposition: whether the arc above the NP should go left or right.\nThe next thing to note is that at least one important feature, the verb, falls outside\nof the conditioning context. (The model considers only information up to two con-\nstituents preceding or following the location of the decision.) This could be repaired\nby considering additional context, but there is no ﬁxed bound on how far the verb\ncan be from the decision point. Note also that in other cases the method fragments\nthe data in unnecessary ways. Cases in which the verb directly precedes the NP,o ri s\none place farther to the left, are treated separately.\nFigure 22 shows a similar example, NP coordination ambiguity. Again, the pivotal\ndecision is made in a somewhat counterintuitive location: at the NP preceding the\ncoordinator. At this point the NP following the coordinator has not been built, and its\nhead noun is not in the contextual window. Figure 23 shows an appositive example\nin which the head noun of the appositive NP is not in the contextual window when\nthe decision is made.\nThese last two examples can be extended to illustrate another problem. The NP\nafter the conjunct or comma could be the subject of a following clause. For example,\nFigure 21\n(a) and (b) are two candidate structures for the same sequence of words. (c) shows the ﬁrst\ndecision (labeled “?”) where the two structures differ. The arc above the NP can go either left\n(for verb attachment of the PP, as in (a)) or right (for noun attachment of the PP, as in (b)).\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n633\nCollins Head-Driven Statistical Models for NL Parsing\nFigure 22\n(a) and (b) are two candidate structures for the same sequence of words. (c) shows the ﬁrst\ndecision (labeled “?”) where the two structures differ. The arc above the NP can go either left\n(for high attachment (a) of the coordinated phrase) or right (for low attachment (b) of the\ncoordinated phrase).\nFigure 23\n(a) and (b) are two candidate structures for the same sequence of words. (c) shows the ﬁrst\ndecision (labeled “?”) in which the two structures differ. The arc above the NP can go either\nleft (for high attachment (a) of the appositive phrase) or right (for noun attachment (b) of the\nappositive phrase).\nin John likes Mary and Bill loves Jill, the decision not to coordinate Mary and Bill is made\njust after the NP Mary is built. At this point, the verb loves is outside the contextual\nwindow, and the model has no way of telling that Bill is the subject of the following\nclause. The model is assigning probability mass to globally implausible structures as\na result of points of local ambiguity in the parsing process.\nSome of these problems can be repaired by changing the derivation order or the\nconditioning context. Ratnaparkhi (1997) has an additional chunking stage, which\nmeans that the head noun does fall within the contextual window for the coordination\nand appositive cases.\n9. Conclusions\nThe models in this article incorporate parameters that track a number of linguistic\nphenomena: bigram lexical dependencies, subcategorization frames, the propagation\nof slash categories, and so on. The models are generative models in which parse\ntrees are decomposed into a number of steps in a top-down derivation of the tree\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n634\nComputational Linguistics Volume 29, Number 4\nand the decisions in the derivation are modeled as conditional probabilities. With a\ncareful choice of derivation and independence assumptions, the resulting model has\nparameters corresponding to the desired linguistic phenomena.\nIn addition to introducing the three parsing models and evaluating their perfor-\nmance on the Penn Wall Street Journal Treebank, we have aimed in our discussion\n(in sections 7 and 8) to give more insight into the models: their strengths and weak-\nnesses, the effect of various features on parsing accuracy, and the relationship of the\nmodels to other work on statistical parsing. In conclusion, we would like to highlight\nthe following points:\n• Section 7.1 showed, through an analysis of accuracy on different types of\ndependencies, that adjuncts are the main sources of error in the parsing\nmodels. In contrast, dependencies forming the “core” structure of\nsentences (for example, dependencies involving complements, sentential\nheads, and NP chunks) are all recovered with over 90% precision and\nrecall.\n• Section 7.2 evaluated the effect of the distance measure on parsing\naccuracy. A model without either the adjacency distance feature or\nsubcategorization parameters performs very poorly (76.5% precision,\n75% recall), suggesting that the adjacency feature is capturing some\nsubcategorization information in the model 1 parser. The results in\nTable 7 show that the subcategorization, adjacency, and “verb-crossing”\nfeatures all contribute signiﬁcantly to model 2’s (and by implication\nmodel 3’s) performance.\n• Section 7.3 described how the three models are well-suited to the Penn\nTreebank style of annotation, and how certain phenomena (particularly\nthe distance features) may fail to be modeled correctly given treebanks\nwith different annotation styles. This may be an important point to bear\nin mind when applying the models to other treebanks or other\nlanguages. In particular, it may be important to perform transformations\non some structures in treebanks with different annotation styles.\n• Section 7.4 gave evidence showing the importance of the models’ ability\nto break down the context-free rules in the treebank, thereby\ngeneralizing to produce new rules on test examples. Table 12 shows that\nprecision on section 0 of the treebank decreases from 89.0% to 87.0% and\nrecall decreases from 88.8% to 87.9% when the model is restricted to\nproduce only those context-free rules seen in training data.\n• Section 8 discussed relationships to the generative model of Charniak\n(1997) and the history-based (conditional) models of Ratnaparkhi (1997),\nJelinek et al. (1994), and Magerman (1995). Although certainly similar to\nCharniak’s model, the three models in this article have some signiﬁcant\ndifferences, which are identiﬁed in section 8.1. (Another important\ndifference—the ability of models 1, 2, and 3 to generalize to produce\ncontext-free rules not seen in training data—was described in section 7.4.)\nSection 8.2 showed that the parsing models of Ratnaparkhi (1997),\nJelinek et al. (1994), and Magerman (1995) can suffer from very similar\nproblems to the “label bias” or “observation bias” problem observed in\ntagging models, as described in Lafferty, McCallum, and Pereira (2001)\nand Klein and Manning (2002).\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n635\nCollins Head-Driven Statistical Models for NL Parsing\nAcknowledgments\nMy Ph.D. thesis is the basis of the work in\nthis article; I would like to thank Mitch\nMarcus for being an excellent Ph.D. thesis\nadviser, and for contributing in many ways\nto this research. I would like to thank the\nmembers of my thesis committee—Aravind\nJoshi, Mark Liberman, Fernando Pereira,\nand Mark Steedman—for the remarkable\nbreadth and depth of their feedback. The\nwork beneﬁted greatly from discussions\nwith Jason Eisner, Dan Melamed, Adwait\nRatnaparkhi, and Paola Merlo. Thanks to\nDimitrios Samaras for giving feedback on\nmany portions of the work. I had\ndiscussions with many other people at\nIRCS, University of Pennsylvnia, which\ncontributed quite directly to this research:\nBreck Baldwin, Srinivas Bangalore, Dan\nBikel, James Brooks, Mickey Chandresekhar,\nDavid Chiang, Christy Doran, Kyle Hart, Al\nKim, Tony Kroch, Robert Macintyre, Max\nMintz, Tom Morton, Martha Palmer, Jeff\nReynar, Joseph Rosenzweig, Anoop Sarkar,\nDebbie Steinig, Matthew Stone, Ann Taylor,\nJohn Trueswell, Bonnie Webber, Fei Xia, and\nDavid Yarowsky. There was also some\ncrucial input from sources outside of Penn.\nIn the summer of 1996 I worked at BBN\nTechnologies: discussions with Scott Miller,\nRichard Schwartz, and Ralph Weischedel\nhad a deep inﬂuence on the research.\nManny Rayner and David Carter from SRI\nCambridge supervised my master’s thesis at\nCambridge University: Their technical\nsupervision was the beginning of this\nresearch. Finally, thanks to the anonymous\nreviewers for their comments.\nReferences\nAlshawi, Hiyan. 1996. Head automata and\nbilingual tiling: Translation with minimal\nrepresentations. Proceedings of the 34th\nAnnual Meeting of the Association for\nComputational Linguistics, pages 167–176.\nAlshawi, Hiyan and David Carter. 1994.\nTraining and scaling preference functions\nfor disambiguation. Computational\nLinguistics, 20(4):635–648.\nBikel, Dan. 2000. A statistical model for\nparsing and word-sense disambiguation.\nIn Proceedings of the Student Research\nWorkshop at ACL 2000.\nBikel, Dan, Scott Miller, Richard Schwartz,\nand Ralph Weischedel. 1997. Nymble: A\nhigh-performance learning name-ﬁnder.\nIn Proceedings of the Fifth Conference on\nApplied Natural Language Processing, pages\n194–201.\nBlack, Ezra, Steven Abney, Dan Flickinger,\nClaudia Gdaniec, Ralph Grishman, Philip\nHarrison, Donald Hindle, Robert Ingria,\nFrederick Jelinek, Judith Klavans, Mark\nLiberman, Mitch Marcus, Salim Roukos,\nBeatrice Santorini, and Tomek\nStrzalkowski. 1991. A Procedure for\nquantitatively comparing the syntactic\ncoverage of english grammars. In\nProceedings of the February 1991 DARP A\nSpeech and Natural Language Workshop.\nBlack, Ezra, Frederick Jelinek, John Lafferty,\nDavid Magerman, Robert Mercer and\nSalim Roukos. 1992. Towards\nhistory-based grammars: Using richer\nmodels for probabilistic parsing. In\nProceedings of the Fifth DARP A Speech and\nNatural Language Workshop, Harriman, NY.\nBlaheta, Don, and Eugene Charniak. 2000.\nAssigning function tags to parsed text. In\nProceedings of the First Annual Meeting of the\nNorth American Chapter of the Association for\nComputational Linguistics, pages 234–240,\nSeattle.\nBod, Rens. 2001. What is the minimal set of\nfragments that achieves maximal parse\naccuracy? In Proceedings of ACL 2001.\nBooth, Taylor L., and Richard A. Thompson.\n1973. Applying probability measures to\nabstract languages. IEEE Transactions on\nComputers, C-22(5):442–450.\nBrill, Eric. 1993. Automatic grammar\ninduction and parsing free text: A\ntransformation-based approach. In\nProceedings of the 21st Annual Meeting of the\nAssociation for Computational Linguistics.\nCharniak, Eugene. 1997. Statistical parsing\nwith a context-free grammar and word\nstatistics. In Proceedings of the Fourteenth\nNational Conference on Artiﬁcial Intelligence,\nAAAI Press/MIT Press, Menlo Park, CA.\nCharniak, Eugene. 2000. A\nmaximum-entropy-inspired parser. In\nProceedings of NAACL 2000.\nCharniak, Eugene. 2001. Immediate-head\nparsing for language models. In\nProceedings of the 39th Annual Meeting of the\nAssociation for Computational Linguistics.\nChelba, Ciprian, and Frederick Jelinek. 1998.\nExploiting syntactic structure for\nlanguage modeling. In Proceedings of\nCOLING-ACL 1998, Montreal.\nChiang, David. 2000. Statistical parsing with\nan automatically-extracted tree adjoining\ngrammar. In Proceedings of ACL 2000,\nHong Kong, pages 456–463.\nCollins, Michael, and James Brooks. 1995.\nPrepositional phrase attachment through\na backed-off model. Proceedings of the Third\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n636\nComputational Linguistics Volume 29, Number 4\nWorkshop on Very Large Corpora, pages\n27–38.\nCollins, Michael. 1996. A new statistical\nparser based on bigram lexical\ndependencies. Proceedings of the 34th\nAnnual Meeting of the Association for\nComputational Linguistics, pages 184–191.\nCollins, Michael. 1997. Three generative,\nlexicalised models for statistical parsing.\nIn Proceedings of the 35th Annual Meeting of\nthe Association for Computational Linguistics\nand 8th Conference of the European Chapter of\nthe Association for Computational Linguistics,\npages 16–23.\nCollins, Michael. 1999. Head-driven\nstatistical models for natural language\nparsing. Ph.D. thesis, University of\nPennsylvania, Philadelphia.\nCollins, Michael, Jan Hajic, Lance Ramshaw,\nand Christoph Tillmann. 1999. A\nstatistical parser for Czech. In Proceedings\nof the 37th Annual Meeting of the ACL,\nCollege Park, Maryland.\nCollins, Michael. 2000. Discriminative\nreranking for natural language parsing.\nProceedings of the Seventeenth International\nConference on Machine Learning (ICML\n2000).\nCollins, Michael. 2002. Parameter estimation\nfor statistical parsing models: Theory and\npractice of distribution-free methods. To\nappear as a book chapter.\nDe Marcken, Carl. 1995. On the\nunsupervised induction of\nphrase-structure grammars. In Proceedings\nof the Third Workshop on Very Large Corpora.\nEisner, Jason. 1996a. Three new probabilistic\nmodels for dependency parsing: An\nexploration. Proceedings of COLING-96,\npages 340–345.\nEisner, Jason. 1996b. An empirical\ncomparison of probability models for\ndependency grammar. Technical Report\nIRCS-96-11, Institute for Research in\nCognitive Science, University of\nPennsylvania, Philadelphia.\nEisner, Jason. 2002. Transformational priors\nover grammars. In Proceedings of the\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP),\nPhiladelphia.\nEisner, Jason, and Giorgio Satta. 1999.\nEfﬁcient parsing for bilexical context-free\ngrammars and head automaton\ngrammars. In Proceedings of the 37th Annual\nMeeting of the ACL.\nFreund, Yoav, Raj Iyer, Robert E. Schapire,\nand Yoram Singer. 1998. An efﬁcient\nboosting algorithm for combining\npreferences. In Machine Learning:\nProceedings of the Fifteenth International\nConference. Morgan Kaufmann.\nGazdar, Gerald, E. H. Klein, G. K. Pullum,\nand Ivan Sag. 1985. Generalized Phrase\nStructure Grammar. Harvard University\nPress, Cambridge, MA.\nGildea, Daniel. 2001. Corpus variation and\nparser performance. In Proceedings of 2001\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP), Pittsburgh,\nPA.\nGoodman, Joshua. 1997. Probabilistic\nfeature grammars. In Proceedings of the\nFourth International Workshop on Parsing\nTechnologies.\nHermjakob, Ulf, and Ray Mooney. 1997.\nLearning parse and translation decisions\nfrom examples with rich context. In\nProceedings of the 35th Annual Meeting of the\nAssociation for Computational Linguistics and\n8th Conference of the European Chapter of the\nAssociation for Computational Linguistics,\npages 482–489.\nHindle, Don, and Mats Rooth. 1991.\nStructural Ambiguity and Lexical\nRelations. In Proceedings of the 29th Annual\nMeeting of the Association for Computational\nLinguistics.\nHopcroft, John, and J. D. Ullman. 1979.\nIntroduction to automata theory, languages,\nand computation. Addison-Wesley,\nReading, MA.\nJelinek, Frederick, John Lafferty, David\nMagerman, Robert Mercer, Adwait\nRatnaparkhi, and Salim Roukos. 1994.\nDecision tree parsing using a hidden\nderivation model. In Proceedings of the 1994\nHuman Language Technology Workshop,\npages 272–277.\nJohnson, Mark. 1997. The effect of\nalternative tree representations on tree\nbank grammars. In Proceedings of NeMLAP\n3.\nJones, Mark and Jason Eisner. 1992a. A\nprobabilistic parser applied to software\ntesting documents. In Proceedings of\nNational Conference on Artiﬁcial Intelligence\n(AAAI-92), pages 322–328, San Jose, CA.\nJones, Mark and Jason Eisner. 1992b. A\nprobabilistic parser and its application. In\nProceedings of the AAAI-92 Workshop on\nStatistically-Based Natural Language\nProcessing Techniques, San Jose, CA.\nJoshi, Aravind and Bangalore Srinivas. 1994.\nDisambiguation of super parts of speech\n(or supertags): Almost parsing. In\nInternational Conference on Computational\nLinguistics (COLING 1994), Kyoto\nUniversity, Japan, August.\nKlein, Dan and Christopher Manning. 2002.\nConditional structure versus conditional\nestimation in NLP models. In Proceedings\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025\n637\nCollins Head-Driven Statistical Models for NL Parsing\nof the Conference on Empirical Methods in\nNatural Language Processing (EMNLP),\nPhiladelphia.\nLafferty, John, Andrew McCallum, and\nFernando Pereira. 2001. Conditional\nrandom ﬁelds: Probabilistic models for\nsegmenting and labeling sequence data.\nIn Proceedings of ICML 2001.\nLafferty, John, Daniel Sleator, and David\nTemperley. 1992. Grammatical trigrams: A\nprobabilistic model of link grammar.\nProceedings of the 1992 AAAI Fall Symposium\non Probabilistic Approaches to Natural\nLanguage.\nMagerman, David. 1995. Statistical\ndecision-tree models for parsing. In\nProceedings of the 33rd Annual Meeting of the\nAssociation for Computational Linguistics,\npages 276–283.\nManning, Christopher D., and Hinrich\nSch ¨utze. 1999. Foundations of Statistical\nNatural Language Processing. MIT Press,\nCambridge, MA.\nMarcus, Mitchell, Grace Kim, Mary Ann\nMarcinkiewicz, Robert MacIntyre, Ann\nBies, Mark Ferguson, Karen Katz, and\nBritta Schasberger. 1994. The Penn\nTreebank: Annotating predicate argument\nstructure. Proceedings of the 1994 Human\nLanguage Technology Workshop, pages\n110–115.\nMarcus, Mitchell, Beatrice Santorini and M.\nMarcinkiewicz. 1993. Building a large\nannotated corpus of English: The Penn\nTreebank. Computational Linguistics,\n19(2):313–330.\nMiller, Scott, Heidi Fox, Lance Ramshaw,\nand Ralph Weischedel. 2000. A novel use\nof statistical parsing to extract\ninformation from text. In Proceedings of the\n1st Meeting of the North American Chapter of\nthe Association for Computational Linguistics\n(NAACL), pages 226–233.\nPereira, Fernando, and David Warren. 1980.\nDeﬁnite clause grammars for language\nanalysis: A survey of the formalism and a\ncomparison with augmented transition\nnetworks. Artiﬁcial Intelligence, 13:231–278.\nPinker, Stephen. 1994. The Language Instinct.\nWilliam Morrow, New York.\nRatnaparkhi, Adwait. 1996. A maximum\nentropy model for part-of-speech tagging.\nIn Proceedings of the Conference on Empirical\nMethods in Natural Language Processing,\nMay.\nRatnaparkhi, Adwait. 1997. A linear\nobserved time statistical parser based on\nmaximum entropy models. In Proceedings\nof the Second Conference on Empirical\nMethods in Natural Language Processing,\nBrown University, Providence, RI.\nResnik, Philip. 1992. Probabilistic\ntree-adjoining grammar as a framework\nfor statistical natural language processing.\nIn Proceedings of COLING 1992, vol. 2,\npages 418–424.\nRoark, Brian. 2001. Probabilistic top-down\nparsing and language modeling.\nComputational Linguistics, 27(2):249–276.\nSchabes, Yves. 1992. Stochastic lexicalized\ntree-adjoining grammars. In Proceedings of\nCOLING 1992, vol. 2, pages 426–432.\nSchabes, Yves and Richard Waters. 1993.\nStochastic lexicalized context-free\ngrammar. In Proceedings of the Third\nInternational Workshop on Parsing\nTechnologies.\nSekine, Satoshi, John Carroll, S. Ananiadou,\nand J. Tsujii. 1992. Automatic Learning for\nSemantic Collocation. In Proceedings of the\nThird Conference on Applied Natural\nLanguage Processing.\nSeneff, Stephanie. 1992. TINA: A natural\nlanguage system for spoken language\napplications. Computational Linguistics,\n18(1):61-86.\nWitten, Ian and Timothy C. Bell. 1991. The\nzero-frequency problem: Estimating the\nprobabilities of novel events in adaptive\ntext compression. IEEE Transactions on\nInformation Theory, 37(4):1085–1094.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120103322753356 by guest on 05 November 2025",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8670885562896729
    },
    {
      "name": "Treebank",
      "score": 0.8610374927520752
    },
    {
      "name": "Bigram",
      "score": 0.7364609241485596
    },
    {
      "name": "Natural language processing",
      "score": 0.7330614328384399
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7075300216674805
    },
    {
      "name": "Parsing",
      "score": 0.7041088938713074
    },
    {
      "name": "Probabilistic logic",
      "score": 0.4779134690761566
    },
    {
      "name": "Natural language understanding",
      "score": 0.4770828187465668
    },
    {
      "name": "Natural language",
      "score": 0.46837741136550903
    },
    {
      "name": "Rule-based machine translation",
      "score": 0.4459387958049774
    },
    {
      "name": "Trigram",
      "score": 0.11375659704208374
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 1854
}