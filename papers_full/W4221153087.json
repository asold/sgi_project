{
  "title": "Learning Confidence for Transformer-based Neural Machine Translation",
  "url": "https://openalex.org/W4221153087",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5009484350",
      "name": "Yu Lu",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Shandong Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5112495273",
      "name": "Jiali Zeng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5102786071",
      "name": "Jiajun Zhang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Shandong Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5091839641",
      "name": "Shuangzhi Wu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100399461",
      "name": "Mu Li",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2531327146",
    "https://openalex.org/W3201173888",
    "https://openalex.org/W2948194985",
    "https://openalex.org/W2963266575",
    "https://openalex.org/W1618905105",
    "https://openalex.org/W2902608666",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W3035072529",
    "https://openalex.org/W2786712888",
    "https://openalex.org/W3084095723",
    "https://openalex.org/W4288482469",
    "https://openalex.org/W2462906003",
    "https://openalex.org/W1932198206",
    "https://openalex.org/W2971302374",
    "https://openalex.org/W582134693",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2970156971",
    "https://openalex.org/W3035128703",
    "https://openalex.org/W2979585146",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W2757980860",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "Confidence estimation aims to quantify the confidence of the model prediction, providing an expectation of success. A well-calibrated confidence estimate enables accurate failure prediction and proper risk measurement when given noisy samples and out-of-distribution data in real-world settings. However, this task remains a severe challenge for neural machine translation (NMT), where probabilities from softmax distribution fail to describe when the model is probably mistaken. To address this problem, we propose an unsupervised confidence estimate learning jointly with the training of the NMT model. We explain confidence as how many hints the NMT model needs to make a correct prediction, and more hints indicate low confidence. Specifically, the NMT model is given the option to ask for hints to improve translation accuracy at the cost of some slight penalty. Then, we approximate their level of confidence by counting the number of hints the model uses. We demonstrate that our learned confidence estimate achieves high accuracy on extensive sentence/word-level quality estimation tasks. Analytical results verify that our confidence estimate can correctly assess underlying risk in two real-world scenarios: (1) discovering noisy samples and (2) detecting out-of-domain data. We further propose a novel confidence-based instance-specific label smoothing approach based on our learned confidence estimate, which outperforms standard label smoothing.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2353 - 2364\nMay 22-27, 2022c‚Éù2022 Association for Computational Linguistics\nLearning Confidence for Transformer-based Neural Machine Translation\nYu Lu1,2‚àó, Jiali Zeng3, Jiajun Zhang1,2‚Ä†, Shuangzhi Wu3 and Mu Li3\n1 National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China\n2 School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n3 Tencent Cloud Xiaowei, Beijing, China\n{yu.lu, jjzhang}@nlpr.ia.ac.cn\n{lemonzeng, frostwu, ethanlli}@tencent.com\nAbstract\nConfidence estimation aims to quantify the con-\nfidence of the model prediction, providing an\nexpectation of success. A well-calibrated confi-\ndence estimate enables accurate failure predic-\ntion and proper risk measurement when given\nnoisy samples and out-of-distribution data in\nreal-world settings. However, this task remains\na severe challenge for neural machine transla-\ntion (NMT), where probabilities from softmax\ndistribution fail to describe when the model is\nprobably mistaken. To address this problem,\nwe propose an unsupervised confidence esti-\nmate learning jointly with the training of the\nNMT model. We explain confidence as how\nmany hints the NMT model needs to make a\ncorrect prediction, and more hints indicate low\nconfidence. Specifically, the NMT model is\ngiven the option to ask for hints to improve\ntranslation accuracy at the cost of some slight\npenalty. Then, we approximate their level of\nconfidence by counting the number of hints the\nmodel uses. We demonstrate that our learned\nconfidence estimate achieves high accuracy\non extensive sentence/word-level quality es-\ntimation tasks. Analytical results verify that\nour confidence estimate can correctly assess\nunderlying risk in two real-world scenarios:\n(1) discovering noisy samples and (2) detect-\ning out-of-domain data. We further propose a\nnovel confidence-based instance-specific label\nsmoothing approach based on our learned con-\nfidence estimate, which outperforms standard\nlabel smoothing1.\n1 Introduction\nConfidence estimation has become increasingly\ncritical with the widespread deployment of deep\nneural networks in practice (Amodei et al., 2016).\nIt aims to measure the model‚Äôs confidence in the\nprediction, showing when it probably fails. A cali-\nbrated confidence estimate can accurately identify\n‚àóWork done while the author was an intern at Tencent.\n‚Ä†Corresponding author.\n1https://github.com/yulu-dada/Learned-conf-NMT\nSrcÔºö\nRefÔºö\nÁà±‰∏ΩËéé ÁöÑ ÊâÆÊºîËÄÖ Âº†Ëâ≥ ÊòØ ÂõΩÂÆ∂ ‰∏Ä Á∫ß ÊºîÂëò\nelisa is played by zhang yan, a class-1 actress on the state level \nOursÔºözhang yan , a figure who loves to play , is a national class actor \nProbÔºö\nConfÔºö\nFigure 1: An example of generated probabilities and\nour learned confidence estimates. The phrases in red\nare wrong translations. The corresponding prediction\nprobabilities and confidence estimates are outlined in\ndashed boxes. The dark color indicates a large value\nunder two evaluations.\nfailure, further measuring the potential risk induced\nby noisy samples and out-of-distribution data preva-\nlent in real scenarios (Nguyen and O‚ÄôConnor, 2015;\nSnoek et al., 2019).\nUnfortunately, neural machine translation\n(NMT) is reported to yield poor-calibrated confi-\ndence estimate (Kumar and Sarawagi, 2019; Wang\net al., 2020), which is common in the application\nof modern neural networks (Guo et al., 2017). It\nimplies that the probability a model assigns to a\nprediction is not reflective of its correctness. Even\nworse, the model often fails silently by providing\nhigh-probability predictions while being woefully\nmistaken (Hendrycks and Gimpel, 2017). We take\nFigure 1 as an example. The mistranslations are\nproduced with high probabilities (dark green blocks\nin the dashed box), making it problematic to assess\nthe quality based on prediction probability when\nhaving no access to references.\nThe confidence estimation on classification tasks\nis well-studied in the literature (Platt, 1999; Guo\net al., 2017). Yet, researches on structured genera-\ntion tasks like NMT is scarce. Existing researches\nonly study the phenomenon that the generated prob-\nability in NMT cannot reflect the accuracy (M√ºller\net al., 2019; Wang et al., 2020), while little is\nknown about how to establish a well-calibrated\nconfidence estimate to describe the predictive un-\n2353\ncertainty of the NMT model accurately.\nTo deal with this issue, we aim to learn the con-\nfidence estimate jointly with the training process\nin an unsupervised manner. Inspired by Ask For\nHints (DeVries and Taylor, 2018), we explain confi-\ndence as how many hints the NMT model needs to\nmake a correct prediction. Specifically, we design\na scenario where ground truth is available for the\nNMT model as hints to deal with tricky translations.\nBut each hint is given at the price of some penalty.\nUnder this setting, the NMT model is encouraged\nto translate independently in most cases to avoid\npenalties but ask for hints to ensure a loss reduc-\ntion when uncertain about the decision. More hints\nmean low confidence and vice versa. In practice,\nwe design a confidence network, taking multi-layer\nhidden states of the decoder as inputs to predict\nthe confidence estimate. Based on this, we further\npropose a novel confidence-based label smoothing\napproach, in which the translation more challeng-\ning to predict has more smoothing to its labels.\nRecall the example in Figure 1. The first phrase\n‚Äúa figure who loves to play‚Äù is incorrect, resulting in\na low confidence level under our estimation. We no-\ntice that the NMT model is also uncertain about the\nsecond expression ‚Äúa national class actor‚Äù, which\nis semantically related but has inaccurate wording.\nThe translation accuracy largely agrees with our\nlearned confidence rather than model probabilities.\nWe verify our confidence estimate as a well-\ncalibrated metric on extensive sentence/word-level\nquality estimation tasks, which is proven to be more\nrepresentative in predicting translation accuracy\nthan existing unsupervised metrics (Fomicheva\net al., 2020). Further analyses confirm that our con-\nfidence estimate can precisely detect potential risk\ncaused by the distributional shift in two real-world\nsettings: separating noisy samples and identifying\nout-of-domain data. The model needs more hints\nto predict fake or tricky translations in these cases,\nthus assigning them low confidence. Additionally,\nexperimental results show the superiority of our\nconfidence-based label smoothing over the stan-\ndard label smoothing technique on different-scale\ntranslation tasks (WMT14 En‚áíDe, NIST Zh‚áíEn,\nWMT16 Ro‚áíEn, and IWSLT14 De‚áíEn).\nThe contributions of this paper are three-fold:\n‚Ä¢ We propose the learned confidence estimate\nto predict the confidence of the NMT output,\nwhich is simple to implement without any\ndegradation on the translation performance.\n‚Ä¢ We prove our learned confidence estimate as\na better indicator of translation accuracy on\nsentence/word-level quality estimation tasks.\nFurthermore, it enables precise assessment of\nrisk when given noisy data with varying noise\ndegrees and diverse out-of-domain datasets.\n‚Ä¢ We design a novel confidence-based label\nsmoothing method to adaptively tune the mass\nof smoothing based on the learned confidence\nlevel, which is experimentally proven to sur-\npass the standard label smoothing technique.\n2 Background\nIn this section, we first briefly introduce a main-\nstream NMT framework, Transformer (Vaswani\net al., 2017), with a focus on how to generate pre-\ndiction probabilities. Then we present an analysis\nof the confidence miscalibration observed in NMT,\nwhich motivates our ideas discussed afterward.\n2.1 Transformer-based NMT\nThe Transformer has a stacked encoder-decoder\nstructure. When given a pair of parallel sentences\nx = {x1, x2, ...xS} and y = {y1, y2, ...yT }, the\nencoder first transforms input to a sequence of\ncontinuous representations h =\n\b\nh0\n1,h0\n2,...h0\nS\n\t\n,\nwhich are then passed to the decoder.\nThe decoder is composed of a stack of N iden-\ntical blocks, each of which includes self-attention,\ncross-lingual attention, and a fully connected feed-\nforward network. The outputs of l-th block hl\nt are\nfed to the successive block. At the t-th position,\nthe model produces the translation probabilities pt,\na vocabulary-sized vector, based on outputs of the\nN-th layer:\npt = softmax(WhN\nt + b) (1)\nDuring training, the model is optimized by mini-\nmizing the cross entropy loss:\nLNMT =\nTX\nt=1\n‚àíytlog(pt) (2)\nwhere {W, b} are trainable parameters andyt is de-\nnoted as a one-hot vector. During inference, we im-\nplement beam search by selecting high-probability\ntokens from generated probability for each step.\n2.2 Confidence Miscalibration in NMT\nModern neural networks have been found to yield\na miscalibrated confidence estimate (Guo et al.,\n2354\n0.0 0.5 1.0\n0.06\n0.08\n0.10\n0.12\n0.14Density p=0.358\nBAD\n0.0 0.5 1.0\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\np=0.249\nPrediction Probability\nOK\nFigure 2: The density function of word probabilities\npredicted by the NMT model on OK and BAD transla-\ntions. We outline the miscalibration with slash mark:\nover-confident (producing high probabilities for errors)\nand under-confident (generating low probabilities for\nright translations).\n2017; Hendrycks and Gimpel, 2017). It means that\nthe prediction probability, as used at each inference\nstep, is not reflective of its accuracy. The problem\nis more complex for structured outputs in NMT. We\ncannot judge a translation as an error, even if it dif-\nfers from the ground truth, as several semantically\nequivalent translations exist for the same source\nsentence. Thus we manually annotate each target\nword as OK or BAD on 200 Zh‚áíEn translations.\nOnly definite mistakes are labeled as BAD, while\nother uncertain translations are overlooked.\nFigure 2 reports the density function of predic-\ntion probabilities on OK and BAD translations.\nWe observe severe miscalibration in NMT: over-\nconfident problems account for 35.8% when the\nmodel outputs BAD translations, and 24.9% OK\ntranslations are produced with low probabilities.\nThese issues make it challenging to identify model\nfailure. It further drives us to establish an estimate\nto describe model confidence better.\n3 Learning to Estimate Confidence\nA well-calibrated confidence estimate should be\nable to tell when the NMT model probably fails.\nIdeally, we would like to learn a measure of con-\nfidence for each target-side translation, but this\nremains a thorny problem in the absence of ground\ntruth for confidence estimate. Inspired by Ask For\nHints (DeVries and Taylor, 2018) on the image clas-\nsification task, we define confidence as how many\nhints the NMT model needs to produce the correct\ntranslation. More hints mean low confidence, and\nthat is a high possibility of failure.\nMotivation. We assume that the NMT model can\nask for hints (look at ground-truth labels) during\ntraining, but each clue comes at the cost of a slight\npenalty. Intuitively, a good strategy is to indepen-\ndently make the predictions that the model is confi-\ndent about and then ask for clues when the model\nis uncertain about the decision. Under this assump-\ntion, we approximate the confidence level of each\ntranslation by counting the number of hints used.\nTo enable the NMT model to ask for hints, we\nadd a confidence estimation network (ConNet) in\nparallel with the original prediction branch, as\nshown in Figure 3. The ConNet takes hidden states\nof the decoder at t-th step (ht) as inputs and pre-\ndicts a single scalar between 0 and 1.\nct = œÉ(W\n‚Ä≤\nht + b\n‚Ä≤\n) (3)\nwhere Œ∏c = {W\n‚Ä≤\n, b\n‚Ä≤\n} are trainable parameters. œÉ(¬∑)\nis the sigmoid function. If the model is confident\nthat it can translate correctly, it should output ct\nclose to 1. Conversely, the model should output ct\nclose to 0 for more hints.\nTo offer the model ‚Äúhints‚Äù during training, we\nadjust softmax prediction probabilities by interpo-\nlating the ground truth probability distribution yt\n(denoted as a one-hot vector) into the original pre-\ndiction. The degree of interpolation is decided by\nthe generated confidence ct:\np\n‚Ä≤\nt = ct ¬∑ pt + (1‚àí ct) ¬∑ yt (4)\nThe translation loss is calculated using modified\nprediction probabilities.\nLNMT =\nTX\nt=1\n‚àíytlog(p\n‚Ä≤\nt) (5)\nTo prevent the model from minimizing the loss\nby always setting ct = 0(receiving all the ground\ntruth), we add a log penalty to the loss function.\nLConf =\nTX\nt=1\n‚àílog(ct) (6)\nThe final loss is the sum of the translation loss\nand the confidence loss, which is weighted by the\nhyper-parameter Œª:\nL = LNMT + ŒªLConf (7)\nUnder this setting, when c ‚Üí 1 (the model\nis quite confident), we can see that p\n‚Ä≤\n‚Üí p and\nLConf ‚Üí 0, which is equal to a standard training\nprocedure. In the case where c ‚Üí 0 (the model is\nquite unconfident), we see that p\n‚Ä≤\n‚Üí y (the model\nobtains correct labels). In this scenario, LNMT\nwould approach 0, but LConf becomes very large.\nThus, the model can reduce the overall loss only\nwhen it successfully predicts which outputs are\nlikely to be correct.\n2355\nConNet \nLinear &\nSoftmax \nùëù ùë¶ ùëù‚Ä≤\nùëê ùëù‚Ä≤ = ùëê ‚àôùëù+ 1‚àíùëê ‚àôùë¶Decoder Block Hidden State\n‚Ä¶‚Ñé1 ‚Ñé2 ‚Ñéùëô\n‚Ä¶\nHints\nFigure 3: The overview of the framework. The NMT model is allowed to ask for hints (ground-truth translation)\nduring training based on the confidence level predicted by the ConNet. During inference, we use the model\nprediction p to sample hypotheses. Each translation word comes with a corresponding confidence estimate.\nImplementation Details. Due to the complexity\nof Transformer architecture, it requires several opti-\nmizations to prevent the confidence branch from de-\ngrading the performance of the translation branch.\nDo not provide hints at the initial stage. The\nearly model is fragile, which lays the groundwork\nfor the following optimization. We find that afford-\ning hints at an early period leads to a significant\nperformance drop. To this end, we propose to dy-\nnamically control the value of Œª (as in Equation 7)\nby the training step (s) as:\nŒª(s) =Œª0 ‚àó e‚àís/Œ≤0 (8)\nwhere Œª0 and Œ≤0 control the initial value and the\ndeclining speed of Œª. We expect the weight of\nconfidence loss to be large at the beginning (c ‚Üí 1)\nand give hints during middle and later stages.\nDo not use high-layer hidden states to predict\nconfidence. We find that it would add much burden\nto the highest layer hidden state if used to predict\ntranslation and confidence simultaneously. So we\nsuggest using low-layer hidden states for the con-\nfidence branch and leaving the translation branch\nunchanged (here, the decoder has 6 layers):\nht = AVE(h1\nt + h2\nt + h3\nt ) (9)\nwhere hl\nt is the l-th layer hidden state in the decoder.\nBesides, other combinations of low-layer hidden\nstates are alternative, i.e., ht = AVE(h1\nt + h3\nt ).\nDo not let the model lazily learn complex exam-\nples. We encounter the situation where the model\nfrequently requests hints rather than learning from\ndifficulty. We follow DeVries and Taylor (2018)\nto give hints with 50% probability. In practice, we\napply Equation 4 to only half of the batch.\nConfidence-based Label Smoothing. Smooth-\ning labels is a typical way to prevent the network\nfrom miscalibration (M√ºller et al., 2019). It has\nbeen used in many state-of-the-art models, which\nassigns a certain probability mass (œµ0) to other non-\nground-truth labels (Szegedy et al., 2016). Here\nwe attempt to employ our confidence estimate to\nimprove smoothing. We propose a novel instance-\nspecific confidence-based label smoothing tech-\nnique, where predictions with greater confidence\nreceive less label smoothing and vice versa. The\namount of label smoothing applied to a prediction\n(œµt) is proportional to its confidence level.\nœµt = œµ0 ‚àó e1‚àíct\nÀÜc\nwhere œµ0 is the fixed value for vanilla label smooth-\ning, ÀÜc is the batch-level average confidence level.\n4 Experiments\nThis section first exhibits empirical studies on the\nQuality Estimation (QE) task, a primary applica-\ntion of confidence estimation. Then, we present\nexperimental results of our confidence-based label\nsmoothing, an extension of our confidence estimate\nto better smoothing in NMT.\n4.1 Confidence-based Quality Estimation\nTo evaluate the ability of our confidence estimate\non mistake prediction, we experiment on extensive\nsentence/word-level QE tasks. Supervised QE task\nrequires large amounts of parallel data annotated\nwith the human evaluation, which is labor-intensive\nand impractical for low-resource languages. Here,\nwe propose to address QE in an unsupervised way\nalong with the training of the NMT model.\n4.1.1 Sentence-level Quality Estimation\nWe experiment on WMT2020 QE shared tasks 2,\nincluding high-resource language pairs (English-\nGerman and English-Chinese) and mid-resource\nlanguage pairs (Estonian-English and Romanian-\nEnglish). This task provides source language sen-\ntences, corresponding machine translations, and\n2http://www.statmt.org/wmt20/quality-estimation-\ntask.html\n2356\nNMT models used to generate translation. Each\ntranslation is annotated with direct assessment\n(DA) by professional translators, ranging from 0-\n100, according to the perceived translation quality.\nWe can evaluate the performance of QE in terms of\nPearson‚Äôs correlation with DA scores.\nWe compare our confidence estimate with four\nunsupervised QE metrics (Fomicheva et al., 2020):\n‚Ä¢ TP: the sentence-level translation probability\nnormalized by length T.\n‚Ä¢ Softmax-Ent: the average entropy of softmax\noutput distribution at each decoding step.\n‚Ä¢ Sent-Std: the standard deviation of word-level\nlog-probability p(y1), ..., p(yT ).\n‚Ä¢ D-TP: the expectation for the set of TP\nscores by runningK stochastic forward passes\nthrough the NMT model with model param-\neters ÀÜŒ∏k perturbed by Monte Carlo (MC)\ndropout (Gal and Ghahramani, 2016).\nWe also report two supervised QE models:\n‚Ä¢ Predictor-Estimator (Kim et al., 2017): a\nweak neural approach, which is usually set as\nthe baseline system for supervised QE tasks.\n‚Ä¢ BERT-BiRNN (Kepler et al., 2019b): a strong\nQE model using a large-scale dataset for pre-\ntraining and quality labels for fine-tuning.\nWe propose four confidence-based metrics: (1)\nConf : the sentence-level confidence estimate av-\neraged by length, (2) Sent-Std-Conf : the standard\ndeviation of word-level log-confidence c1, ..., cT ,\n(3) D-Conf : similar to D-TP, we compute the ex-\npectation of Conf by running K forward passes\nthrough the NMT model, and (4) D-Comb: the\ncombination of D-TP and D-Conf:\nD-Comb = 1\nK\nKX\nk=1\n(Conf ÀÜŒ∏k + TPÀÜŒ∏k ) (10)\nNote that our confidence estimate is produced to-\ngether with translations. It is hard to let our model\ngenerate exact translations as provided by WMT,\neven with a similar configuration. Thus, we train\nour model on parallel sentences as used to train\nprovided NMT models. Then, we employ force\ndecoding on given translations to obtain existing\nunsupervised metrics and our estimations. We do\nnot use any human judgment labels for supervision.\nMethods Mid-resource High-resource\nEt-En Ro-En En-De En-Zh\nTP 0.514 0.529 0.179 0.258\nSoftmax-Ent 0.535 0.526 0.144 0.257\nSent-Std 0.493 0.418 0.195 0.281\nD-TP (K=30) 0.583 0.553 0.197 0.288\nConf 0.557 0.569 0.218 0.293\nSent-Std-Conf 0.494 0.482 0.239 0.293\nD-Conf (K=30) 0.572 0.572 0.210 0.288\nD-Comb (K=30) 0.583 0.577 0.198 0.288\nPredEst ‚Ä° 0.477 0.685 0.145 0.190\nBERT-BiRNN ‚Ä° 0.635 0.763 0.273 0.371\nTable 1: Pearson‚Äôs correlation between unsupervised QE\nindicators and DA scores. K is set following Wang et al.\n(2019). We reimplement the first four unsupervised\nQE metrics on our NMT model. The best results of\nunsupervised metrics are marked in bold. Results with\n‚Ä° are copies from Fomicheva et al. (2020).\nTable 1 shows the Pearson‚Äôs correlation with DA\nscores for the above QE indicators. We find that:\nOur confidence-based metrics substantially sur-\npass probability-based metrics (the first three lines\nin Table 1). Compared with dropout-based meth-\nods (D-TP), our metrics obtain comparable results\non mid-resource datasets while yielding better per-\nformance on high-resource translation tasks. We\nnote that the benefits brought from the MC dropout\nstrategy are limited for our metrics, which is signif-\nicant in probability-based methods. It also proves\nthe stability of our confidence estimate. In addition,\nthe predictive power of MC dropout comes at the\ncost of computation, as performing forward passes\nthrough the NMT model is time-consuming and\nimpractical for the large-scale dataset.\nOur approach outperforms PredEst, a weak su-\npervised method, on three tasks and further narrows\nthe gap on Ro-En. Though existing unsupervised\nQE methods still fall behind with the strong QE\nmodel (BERT-BiRNN), the exploration of unsu-\npervised metrics is also meaningful for real-world\ndeployment with the limited annotated dataset.\n4.1.2 Word-level Quality Estimation\nWe also validate the effectiveness of our confi-\ndence estimate on QE tasks from a more fine-\ngrained view. We randomly select 250 sentences\nfrom Zh‚áíEn NIST03 and obtain NMT transla-\ntions. Two graduate students are asked to annotate\neach target word as either OK or BAD. We assess\nthe performance of failure prediction with standard\nmetrics, which are introduced in Appendix A.\n2357\nMethods Zh‚áíEn En‚áíDe De ‚áíEn Ro ‚áíEn\nMT03 MT04 MT05 MT06 MT08 ALL\nTransformer w/o LS 48.77 48.50 47.45 46.65 35.93 45.50 26.98 34.27 29.71\n+ Standard LS 49.14 48.48 50.53 47.44 36.23 45.83 27.40 34.52 30.03\n+ Confidence-based LS 50.2‚àó 48.57 50.91‚àó 48.57‚àó 37.38‚àó 46.55‚àó 27.75‚àó 35.02‚àó 30.82‚àó\nTable 2: Translation results (beam size 4) for standard label smoothing and our confidence-based label smoothing\non NIST Zh‚áíEn, WMT14 En‚áíDe (using case-sensitive BLEU score for evaluation), IWSLT14 De‚áíEn, and\nWMT16 Ro‚áíEn. ‚Äú‚àó‚Äù indicates gains are statistically significant than Transformer w/o LS with p <0.05.\nMethods AUROC‚Üë AUPR‚Üë EER‚Üì DET‚Üì\nMSP 72.59 97.49 32.30 31.22\nMCDropout 86.52 99.23 20.80 20.76\nOurs 85.89 99.07 20.40 19.90\nTable 3: Word-level QE evaluated by the separation\naccuracy of OK and BAD translations in the Zh ‚áíEn\ntask. All values are shown in percentages. ‚Üë indicates\nhigher scores are better, and ‚Üì indicates lower is better.\nExperimental results are given in Table 3. We im-\nplement competitive failure prediction approaches,\nincluding Maximum Softmax Probability (MSP)\n(Hendrycks and Gimpel, 2017) and Monte Carlo\nDropout (MCDropout) (Gal and Ghahramani,\n2016). We find that our learned confidence esti-\nmate yields a better separation of OK and BAD\ntranslation than MSP. Compared with MCDropout,\nour metrics achieve competing performance with\nsignificant advantages on computational expenses.\nOverall, the learned confidence estimate is a\ncompetitive indicator of translation precision com-\npared with other unsupervised QE metrics. More-\nover, the confidence branch added to the NMT sys-\ntem is a light component. It allows each translation\nto come with quality measurement without degrada-\ntion of the translation accuracy. The performance\nwith the confidence branch is in Appendix B.\n4.2 Confidence-based Label Smoothing\nWe extend our confidence estimate to improve\nsmoothing and experiment on different-scale\ntranslation tasks: WMT14 English-to-German\n(En‚áíDe), LDC Chinese-to-English (Zh ‚áíEn)3,\nWMT16 Romanian-to-English (Ro ‚áíEn), and\nIWSLT14 German-to-English (De‚áíEn). We use\nthe 4-gram BLEU (Papineni et al., 2002) to score\nthe performance. More details about data process-\ning and experimental settings are in Appendix C.\n3The corpora includes LDC2000T50, LDC2002T01,\nLDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17,\nand LDC2004T07.\nAs shown in Table 2, our confidence-based la-\nbel smoothing outperforms standard label smooth-\ning by adaptively tuning the amount of each label\nsmoothing. For Zh‚áíEn task, our method improves\nthe performance over Transformer w/o LS by 1.05\nBLEU, which also exceeds standard label smooth-\ning by 0.72 BLEU. We find that improvements\nover standard label smoothing differ in other lan-\nguage pairs (0.35 BLEU in En ‚áíDe, 0.5 BLEU\nin De‚áíEn, and 0.79 BLEU in Ro ‚áíEn). It can\nbe attributed to that the seriousness of miscalibra-\ntion varies in different language pairs and datasets\n(Wang et al., 2020).\nExperimental results with a larger search space\n(i.e. beam size=30) are also given in Appendix C\nto support the above findings.\n5 Analysis\nConfidence estimation is particularly critical in real-\nworld deployment, where noisy samples and out-of-\ndistribution data are prevalent (Snoek et al., 2019).\nGiven those abnormal inputs, neural network mod-\nels are prone to be highly confident in misclassi-\nfication (Nguyen et al., 2015). Thus, we need an\naccurate confidence estimate to detect potential fail-\nures caused by odd inputs by assigning them low\nconfidence. This section explores whether our con-\nfidence estimate can accurately measure risk under\nthose two conditions.\n5.1 Noisy Label Identification\nWe expect that the model requires more hints to fit\nnoisy labels by predicting low confidence. To test\nthis point, we experiment on the IWSLT14 De‚áíEn\ndataset containing 160k parallel sentences. We\nbuild several datasets with progressively increasing\nnoisy samples by randomly replacing target-side\nwords with others in the vocabulary. We train on\neach dataset with the same configuration and pic-\nture the learned confidence estimate in Figure 4.\nThe learned confidence estimate appears to make\n2358\nNoise Rate AUROC‚Üë AUPR‚Üë EER‚Üì DET‚Üì\nThe Model Probability / Our Confidence Estimate\n20% 93.21 / 96.73 97.08 / 98.57 13.50 / 7.00 11.50 / 6.00\n40% 94.89 / 95.73 95.22 / 95.50 11.88 / 9.50 10.58 / 7.69\n60% 93.37 / 94.92 86.54 / 88.09 14.00 / 10.08 12.04 / 8.29\n80% 91.63 / 95.44 64.15 / 76.67 16.06 / 10.13 13.41 / 8.13\nTable 4: Separating clean and noisy data by the model probability and our confidence estimate with varying noisy\nrates. ‚Üë indicates that higher scores are better, while ‚Üì means that lower is better. All values are percentages.\nOut-of-distribution Dataset AUROC ‚Üë AUPR‚Üë EER‚Üì DET‚Üì\nCorpus UNK Len. The Model Probability / Our Confidence Estimate\nWMT-News 1.45% 30.16 71.51 / 72.01 68.86 / 70.97 33.78 / 34.44 33.33 / 32.44\nTanzil 1.36% 34.17 90.53 / 89.48 91.45 / 91.32 17.33 / 18.78 16.72 / 17.72\nTico-19 1.21% 30.29 64.10 / 72.10 62.12 / 71.59 39.67 / 33.33 38.83 / 31.83\nTED2013 1.04% 19.03 63.48 / 68.44 59.10 / 66.75 39.22 / 36.22 39.00 / 35.39\nNews-Commentary 1.00% 23.81 64.14 / 70.10 60.49 / 69.48 39.33 / 35.56 39.11 / 34.22\nTable 5: Comparison of the model probability and our confidence estimate on out-of-domain data detection tasks.\nWe present the rate of unknown words (UNK) and average length of input sentences for each dataset (the average\ninput length of in-domain dataset is 22.47). All scores are shown in percentages and the best results are highlighted\nin bold. ‚Üë indicates that higher scores are better, while ‚Üì indicates that lower scores are better.\n0% 20% 40% 60% 80%\nVarying levels of noise in training data\n0.6\n0.7\n0.8\n0.9\n1.0Learned confidence estimates\nnoisy examples\nclean examples\nFigure 4: The learned confidence estimate on IWSLT14\nDe‚áíEn as varying levels of noisy labels. The shade\nof colors denotes how many words are corrupted in a\nsentence (dark orange means a high pollution rate). The\ndashed line shows averaged learned confidence estimate\non the whole dataset.\nreasonable assessments. (1) It predicts low confi-\ndence on noisy samples but high confidence on\nclean ones. Specifically, the confidence estimate is\nmuch lower as a higher pollution degree in one ex-\nample (darker in color). (2) With increasing noises\nin the dataset, the NMT model becomes more un-\ncertain about its decision accordingly. Large num-\nbers of noises also raise a challenge for separating\nclean and noisy samples.\nWe also compare ours with the model proba-\nbility by giving the accuracy of separating clean\nand noisy examples under varying pollution rates.\nWe set clean data as the positive example and use\nevaluation metrics listed in Appendix A.\nAs shown in Table 4, our confidence estimate\nobtains better results in all cases, especially in a\nhigh noise rate. Our metric improves the area under\nthe precision-recall curve (AUPR) from 64.15% to\n76.76% and reduces the detection error (DET) from\n13.41% to 8.13% at an 80% noise rate. It proves\nthat our confidence estimate is more reliable for\ndetecting potential risks induced by noisy data.\n5.2 Out-of-Domain Data Detection\nFor our in-domain examples, we train an NMT\nmodel on the 2.1M LDC Zh‚áíEn news dataset and\nthen sample 1k sentences from NIST2004 as the\nin-domain testbed. We select five out-of-domain\ndatasets and extract 1k samples from each. Most of\nthem are available for download on OPUS, speci-\nfied in Appendix D. Regarding the unknown words\n(UNK) rate, the average length of input sentences,\nand domain diversity, the descending order based\non distance with the in-domain dataset is WMT-\nnews > Tanzil > Tico-19 > TED2013 > News-\nCommentary. Test sets closer to the in-domain\ndataset are intuitively harder to tell apart.\nWe use sentence-level posterior probability and\nconfidence estimate of the translation to separate\nin- and out-of-domain data. Evaluation metrics are\nin Appendix A. Results are given in Table 5.\nWe find that our approach performs comparably\nwith the probability-based method on datasets with\ndistinct domains (WMT-news and Tanzil). But\n2359\nMost Confident Words Most Uncertain Words\n(a) Rank by prediction probability\n(b) Rank by our confidence estimate\nFigure 5: Word clouds of the most confident/uncertain\ntranslations in the Tico-19 dataset ranked by (a) predic-\ntion probability and (b) learned confidence estimate. We\ndivide tokens into three categories based on their fre-\nquencies. High: the most 3k frequent words, Medium:\nthe most 3k-12k frequent words, Low: the other tokens.\nwhen cross-domain knowledge is harder to detect\n(the last three lines in Table 5), our metric yields a\nbetter separation of in- and out-of-domain ones.\nTo better understand the behaviour of our con-\nfidence estimates on out-of-domain data, we visu-\nalize word clouds of the most confident/uncertain\nwords ranked by model probability and our mea-\nsurements on a medicine dataset (Tico-19) in Fig-\nure 5. The colors of words indicate their frequen-\ncies in the in-domain dataset.\nOur metrics correctly separate in- and out-of-\ndomain data from two aspects: (1) word frequency:\nthe NMT model is certain about frequent words\nyet hesitates on rare words as seen in Figure 5(b).\nBut colors in Figure 5(a) are relatively mixing. (2)\ndomain relation: the most uncertain words ranked\nby our confidence estimate are domain-related, like\n‚Äúpatho‚Äù and ‚Äúsyndrome‚Äù, while the most confident\nwords are domain-unrelated (e.g., punctuations and\nprepositions). This phenomenon cannot be seen in\nFigure 5(a), showing that probabilities from soft-\nmax fall short in representing model uncertainty\nfor domain-shift data.\n6 Related Work\nThe task of confidence estimation is crucial in real-\nworld conditions, which helps failure prediction\n(Corbi√®re et al., 2019) and out-of-distribution de-\ntection (Hendrycks and Gimpel, 2017; Snoek et al.,\n2019; Lee et al., 2018). This section reviews recent\nresearches on confidence estimation and related\napplications on quality estimation for NMT.\n6.1 Confidence Estimation for NMT\nOnly a few studies have investigated calibration in\nNMT. M√ºller et al. (2019) find that the NMT model\nis well-calibrated in training, which is proven\nseverely miscalibrated in inference (Wang et al.,\n2020), especially when predicting the end of a sen-\ntence (Kumar and Sarawagi, 2019). Regarding\nthe complex structures of NMT, the exploration\nfor fixing miscalibration in NMT is scarce. Wang\net al. (2019); Xiao et al. (2020) use Monte Carlo\ndropout to capture uncertainty in NMT, which is\ntime-consuming and computationally expensive.\nUnlike them, we are the first to introduce learned\nconfidence estimate into NMT. Our method is well-\ndesigned to adapt to Transformer architecture and\nNMT tasks, which is also simple but effective.\n6.2 Quality Estimation for NMT\nQE is to predict the quality of the translation pro-\nvided by an MT system at test time without stan-\ndard references. Recent supervised QE models are\nresource-heavy and require a large mass of anno-\ntated quality labels for training (Wang et al., 2018;\nKepler et al., 2019a; Lu and Zhang, 2020), which is\nlabor-consuming and unavailable for low-resource\nlanguages.\nExploring internal information from the NMT\nsystem to indicate translation quality is another\nalternative. Fomicheva et al. (2020) find that uncer-\ntainty quantification is competitive in predicting the\ntranslation quality, which is also complementary to\nsupervised QE model (Wang et al., 2021). How-\never, they rely on repeated Monte Carlo dropout\n(Gal and Ghahramani, 2016) to assess uncertainty\nat the high cost of computation. Our confidence es-\ntimate outperforms existing unsupervised QE met-\nrics, which is also intuitive and easy to implement.\n7 Conclusion\nIn this paper, we propose to learn confidence es-\ntimates for NMT jointly with the training pro-\ncess. We demonstrate that learned confidence can\nbetter indicate translation accuracy on extensive\nsentence/word-level QE tasks and precisely mea-\nsures potential risk induced by noisy samples or\nout-of-domain data. We further extend the learned\nconfidence estimate to improve smoothing, outper-\n2360\nforming the standard label smoothing technique.\nAs our confidence estimate outlines how much the\nmodel knows, we plan to apply our work to de-\nsign a more suitable curriculum during training and\npost-edit low-confidence translations in the future.\nAcknowledgements\nThis work is supported by the Natural Science\nFoundation of China under Grant No. 62122088,\nU1836221, and 62006224.\nReferences\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul F.\nChristiano, John Schulman, and Dan Man√©.\n2016. Concrete problems in AI safety. CoRR,\nabs/1606.06565.\nCharles Corbi√®re, Nicolas Thome, Avner Bar-Hen,\nMatthieu Cord, and Patrick P√©rez. 2019. Address-\ning failure prediction by learning model confidence.\nIn Advances in Neural Information Processing Sys-\ntems 32: Annual Conference on Neural Information\nProcessing Systems 2019, NeurIPS 2019, pages 2898‚Äì\n2909.\nTerrance DeVries and Graham W. Taylor. 2018. Learn-\ning confidence for out-of-distribution detection in\nneural networks. CoRR, abs/1802.04865.\nMarina Fomicheva, Shuo Sun, Lisa Yankovskaya,\nFr√©d√©ric Blain, Francisco Guzm√°n, Mark Fishel,\nNikolaos Aletras, Vishrav Chaudhary, and Lucia Spe-\ncia. 2020. Unsupervised quality estimation for neural\nmachine translation. Transactions of the Association\nfor Computational Linguistics, 8:539‚Äì555.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as a\nbayesian approximation: Representing model uncer-\ntainty in deep learning. In Proceedings of the 33nd In-\nternational Conference on Machine Learning, ICML\n2016, volume 48, pages 1050‚Äì1059. JMLR.org.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In Proceedings of the 34th International\nConference on Machine Learning, ICML 2017, vol-\nume 70 of Proceedings of Machine Learning Re-\nsearch, pages 1321‚Äì1330. PMLR.\nDan Hendrycks and Kevin Gimpel. 2017. A baseline\nfor detecting misclassified and out-of-distribution ex-\namples in neural networks. In 5th International Con-\nference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Conference Track\nProceedings. OpenReview.net.\nFabio Kepler, Jonay Tr√©nous, Marcos Treviso, Miguel\nVera, Ant√≥nio G√≥is, M. Amin Farajian, Ant√≥nio V .\nLopes, and Andr√© F. T. Martins. 2019a. Unba-\nbel‚Äôs participation in the WMT19 translation qual-\nity estimation shared task. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n3: Shared Task Papers, Day 2), pages 78‚Äì84. Associ-\nation for Computational Linguistics.\nFabio Kepler, Jonay Tr√©nous, Marcos V . Treviso,\nMiguel Vera, Ant√≥nio G√≥is, M. Amin Farajian, An-\nt√≥nio V . Lopes, and Andr√© F. T. Martins. 2019b. Un-\nbabel‚Äôs participation in the WMT19 translation qual-\nity estimation shared task. In Proceedings of the\nFourth Conference on Machine Translation, WMT\n2019, pages 78‚Äì84. Association for Computational\nLinguistics.\nHyun Kim, Jong-Hyeok Lee, and Seung-Hoon Na. 2017.\nPredictor-estimator using multilevel task learning\nwith stack propagation for neural quality estimation.\nIn Proceedings of the Second Conference on Machine\nTranslation, WMT 2017, pages 562‚Äì568. Association\nfor Computational Linguistics.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProceedings of the 45th Annual Meeting of the As-\nsociation for Computational Linguistics Companion\nVolume Proceedings of the Demo and Poster Sessions,\npages 177‚Äì180. Association for Computational Lin-\nguistics.\nAviral Kumar and Sunita Sarawagi. 2019. Calibration\nof encoder decoder models for neural machine trans-\nlation. CoRR, abs/1903.00802.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.\n2018. A simple unified framework for detecting out-\nof-distribution samples and adversarial attacks. In\nAdvances in Neural Information Processing Systems,\nvolume 31, pages 7167‚Äì7177. Curran Associates Inc.\nJinliang Lu and Jiajun Zhang. 2020. Quality estimation\nbased on multilingual pre-trained language model. J.\nXiamen Univ. Nat. Sci, 59(2).\nRafael M√ºller, Simon Kornblith, and Geoffrey E. Hin-\nton. 2019. When does label smoothing help? In Ad-\nvances in Neural Information Processing Systems 32:\nAnnual Conference on Neural Information Process-\ning Systems 2019, NeurIPS 2019, pages 4696‚Äì4705.\nAnh Mai Nguyen, Jason Yosinski, and Jeff Clune. 2015.\nDeep neural networks are easily fooled: High con-\nfidence predictions for unrecognizable images. In\nIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2015, pages 427‚Äì436.\nKhanh Nguyen and Brendan O‚ÄôConnor. 2015. Poste-\nrior calibration and exploratory analysis for natural\nlanguage processing models. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1587‚Äì1598. Association\nfor Computational Linguistics.\n2361\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311‚Äì318. Association for\nComputational Linguistics.\nJohn C. Platt. 1999. Probabilistic outputs for support\nvector machines and comparisons to regularized like-\nlihood methods. Advances in Large Margin Classi-\nfiers.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715‚Äì1725.\nAssociation for Computational Linguistics.\nJasper Snoek, Yaniv Ovadia, Emily Fertig, Balaji\nLakshminarayanan, Sebastian Nowozin, D. Sculley,\nJoshua V . Dillon, Jie Ren, and Zachary Nado. 2019.\nCan you trust your model‚Äôs uncertainty? evaluating\npredictive uncertainty under dataset shift. In Ad-\nvances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, pages 13969‚Äì\n13980.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew Wojna. 2016. Re-\nthinking the inception architecture for computer vi-\nsion. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2016, pages 2818‚Äì\n2826. IEEE Computer Society.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, undefine-\ndukasz Kaiser, and Illia Polosukhin. 2017. Attention\nis all you need. In Proceedings of the 31st Interna-\ntional Conference on Neural Information Processing\nSystems, pages 6000‚Äì6010. Curran Associates Inc.\nJiayi Wang, Kai Fan, Bo Li, Fengming Zhou, Boxing\nChen, Yangbin Shi, and Luo Si. 2018. Alibaba sub-\nmission for WMT18 quality estimation task. In Pro-\nceedings of the Third Conference on Machine Trans-\nlation: Shared Task Papers, pages 809‚Äì815. Associa-\ntion for Computational Linguistics.\nKe Wang, Yangbin Shi, Jiayi Wang, Yuqi Zhang,\nYu Zhao, and Xiaolin Zheng. 2021. Beyond glass-\nbox features: Uncertainty quantification enhanced\nquality estimation for neural machine translation. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 4687‚Äì4698. Associa-\ntion for Computational Linguistics.\nShuo Wang, Yang Liu, Chao Wang, Huanbo Luan, and\nMaosong Sun. 2019. Improving back-translation\nwith uncertainty-based confidence estimation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing, EMNLP-IJCNLP 2019, pages 791‚Äì802.\nAssociation for Computational Linguistics.\nShuo Wang, Zhaopeng Tu, Shuming Shi, and Yang Liu.\n2020. On the inference calibration of neural machine\ntranslation. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 3070‚Äì3079.\nAssociation for Computational Linguistics.\nTim Z. Xiao, Aidan N. Gomez, and Yarin Gal.\n2020. Wat zei je? detecting out-of-distribution\ntranslations with variational transformers. CoRR,\nabs/2006.08344.\n2362\nA Evaluation Metrics\nWe let TP, FP, TN, and FN represent true positives,\nfalse positives, true negatives, and false negatives.\nWe use the following metrics for evaluating the ac-\ncuracy of word-level QE, noisy label identification,\nand out-of-domain detection:\n‚Ä¢ AUROC: the Area Under the Receiver Operat-\ning Characteristic (ROC) curve, which plots\nthe relation between TPR and FPR.\n‚Ä¢ AUPR: the Area Under the Precision-Recall\n(PR) curve. The PR curve is made by plot-\nting precision = TP/(TP+FP) and recall =\nTP/(TP+FN).\n‚Ä¢ DET: the Detection Error, which is the min-\nimum possible misclassification probability\nover all possible threshold when separating\npositive and negative examples.\n‚Ä¢ EER: the Equal error rate. It is the error\nrate when the confidence threshold is located\nwhere FPR is the same with the false negative\nrate (FNR) = FN / (TP+FN).\nWe set OK translations in the word-level QE\ntask, clean samples in the noisy data identification\ntask, and in-domain samples in the out-of-domain\ndata detection task as the positive example.\nB Translation Results with the\nConfidence Branch\nThe confidence branch added to the NMT system\nis a light component. It allows each translation to\ncome with quality measurement without degrada-\ntion of the translation accuracy. Translation results\nwith the confidence branch are given in Table 6.\nWe see that the added confidence branch does\nnot affect the translation performance. Implemen-\ntation details in section 3 are necessary for achiev-\ning this. For instance, if we use the highest hid-\nden state to predict confidence and translation to-\ngether, BLEU scores would dramatically decline\nwith a larger beam size, the drop of which is more\nsignificant than that of the baseline model. For\nthe En‚áíDe task, the change is from 27.31 (beam\nsize 4) to 25.6 (beam size 100), while the base-\nline model even improves 0.5 BLEU further with a\nlarger beam size 100.\nC Confidence-based Label Smoothing\nWe experiment on different-scale translation tasks:\nWMT14 En‚áíDe, LDC Zh‚áíEn, WMT16 Ro‚áíEn,\nand IWSLT14 De‚áíEn.\nDatasets. We tokenize the corpora by Moses\n(Koehn et al., 2007). Byte pair encoding (BPE)\n(Sennrich et al., 2016) is applied to all language\npairs to construct a join 32k vocabulary except for\nZh‚áíEn where the source and target languages are\nseparately encoded.\nFor En‚áíDe, we train on 4.5M training sam-\nples. Newstest2013 and newstest2014 are set as\nvalidation and test sets. For Zh ‚áíEn, we remove\nsentences of more than 50 words and collect 2.1M\ntraining samples. We use NIST 2002 as the vali-\ndation set, NIST 2003-2006 (MT03-06), and 2008\n(MT08) as the testbed. For Ro ‚áíEn, we train on\n0.61M training data and use newsdev2016 and new-\nstest2016 as validation and test sets. For De‚áíEn,\nwe train on its training set with 160k training sam-\nples and evaluate on its test set.\nSettings. We implement the described model\nwith fairseq5 toolkit for training and evaluating.\nWe follow Vaswani et al. (2017) to set the con-\nfigurations of models with the base Transformer.\nThe dropout rate of the residual connection is 0.1\nexcept for Zh ‚áíEn (0.3). The experiments last\nfor 150k steps for Zh ‚áíEn and En‚áíDe, 30k for\nsmall-scale De‚áíEn and Ro‚áíEn. We average the\nlast ten checkpoints for evaluation and adopt beam\nsearch (beam size 4/30, length penalty 0.6). We set\nœµls = 0.1 for the vanilla label smoothing.\nThe hyper-parameters Œª0 and Œ≤0 (as seen Equa-\ntion 8) control the initial value and declining speed\nof Œª (as in Equation 7), which decides the number\nof hints the NMT model can receive. To ensure that\nno hints are available at the early stage of training,\nwe set Œª0 = 30, Œ≤0 = 4.5 ‚àó 104 for Zh‚áíEn and\nEn‚áíDe, Œ≤0 = 1.2 ‚àó 104 for De‚áíEn and Ro‚áíEn.\nWe set œµ0 = 0.1 (as seen in Equation 10) for all\nlanguage pairs.\nResults. A common setting with beam size=4\nis given in Table 2 in the main body. Here, we\nexperiment with a larger search space where be-\ning over-or under-confident further worsens model\nperformance (Guo et al., 2017). The results with\nbeam size=30 are listed in Table 7. For Zh ‚áíEn\ntask, our method yields +1.17 BLEU improvements\n5https://github.com/pytorch/fairseq\n2363\nMethods Zh‚áíEn En‚áíDe De ‚áíEn\nMT03 MT04 MT05 MT06 MT08 ALL\nTransformer 49.14 48.48 50.53 47.44 36.23 45.83 27.40 34.52\n+ ConNet 49.51 48.47 50.51 47.29 36.44 45.90 27.55 34.73\nTable 6: Translation results (BLEU score) with the confidence branch on NIST Zh‚áíEn, WMT14 En‚áíDe (using\ncase-sensitive BLEU score for evaluation) and IWSLT14 De‚áíEn.\nMethods Zh‚áíEn En‚áíDe De ‚áíEn Ro ‚áíEn\nMT03 MT04 MT05 MT06 MT08 ALL\nTransformer w/o LS 49.06 48.64 47.76 47.01 35.93 45.68 25.91 34.36 29.96\n+ Standard LS 49.63 48.70 50.61 47.81 37.61 46.27 27.81 34.66 30.48\n+ Confidence-based LS 50.59‚àó 48.75 51.47‚àó 48.60‚àó 37.87‚àó 46.85‚àó 28.01‚àó 35.11‚àó 31.07‚àó\nTable 7: Translation results (beam size 30) for standard label smoothing and our confidence-based label smoothing\non NIST Zh‚áíEn, WMT14 En‚áíDe (using case-sensitive BLEU score for evaluation), IWSLT14 De‚áíEn, and\nWMT16 Ro‚áíEn. ‚Äú‚àó‚Äù indicates gains are statistically significant than Transformer w/o LS with p <0.05.\nover Transformer w/o LS, exceeding standard label\nsmoothing by 0.58 BLEU scores. The performance\ngains can also be found in other language pairs,\nshowing the effectiveness of our confidence-based\nlabel smoothing with a larger beam size.\nD Out-of-domain Data Detection\nWe select five out-of-domain datasets for our tests\n(we extract 1k samples each), which are available\nfor download on OPUS4. The datasets are:\n‚Ä¢ WMT-News: A parallel corpus of News Test\nSets provided by WMT for training SMT 5,\nwhich is rich in content including sports, en-\ntertainment, politics, and so on.\n‚Ä¢ Tanzil: This is a collection of Quran transla-\ntions compiled by the Tanzil project6.\n‚Ä¢ Tico-19: This is a collection of translation\nmemories from the Translation Initiative for\nCOVID-19, which has many medical terms7.\n‚Ä¢ TED2013: A corpus of TED talks subtitles\nprovided by CASMACAT8, which are about\npersonal experiences in informal expression.\n‚Ä¢ News-Commentary: It is also a dataset pro-\nvided by WMT9, but the extracted test set is\nall about international politics.\n4https://opus.nlpl.eu/\n5http://www.statmt.org/wmt19/\n6https://opus.nlpl.eu/Tanzil-v1.php\n7https://opus.nlpl.eu/tico-19-v2020-10-28.php\n8http://www.casmacat.eu/corpus/ted2013.html\n9https://opus.nlpl.eu/News-Commentary-v16.php\n2364",
  "topic": "Softmax function",
  "concepts": [
    {
      "name": "Softmax function",
      "score": 0.825005054473877
    },
    {
      "name": "Computer science",
      "score": 0.7573912143707275
    },
    {
      "name": "Low Confidence",
      "score": 0.625757098197937
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6022740602493286
    },
    {
      "name": "Machine translation",
      "score": 0.600196361541748
    },
    {
      "name": "Smoothing",
      "score": 0.5766369700431824
    },
    {
      "name": "Confidence interval",
      "score": 0.5680968761444092
    },
    {
      "name": "Machine learning",
      "score": 0.5547420978546143
    },
    {
      "name": "Confidence and prediction bands",
      "score": 0.41349196434020996
    },
    {
      "name": "Artificial neural network",
      "score": 0.24513709545135498
    },
    {
      "name": "Statistics",
      "score": 0.2060190737247467
    },
    {
      "name": "Mathematics",
      "score": 0.12757647037506104
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210094879",
      "name": "Shandong Institute of Automation",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    }
  ],
  "cited_by": 9
}