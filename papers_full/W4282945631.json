{
  "title": "Improving Pretrained Language Model Fine-Tuning With Noise Stability Regularization",
  "url": "https://openalex.org/W4282945631",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2162063125",
      "name": "Hua Hang",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2081506614",
      "name": "Li, Xingjian",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2745275353",
      "name": "Dou, Dejing",
      "affiliations": [
        "Jingdong (China)"
      ]
    },
    {
      "id": "https://openalex.org/A4222852310",
      "name": "Xu, Cheng-Zhong",
      "affiliations": [
        "University of Macau"
      ]
    },
    {
      "id": "https://openalex.org/A2743301278",
      "name": "Luo, Jiebo",
      "affiliations": [
        "University of Rochester"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6774222543",
    "https://openalex.org/W6761268247",
    "https://openalex.org/W2984582583",
    "https://openalex.org/W6771713106",
    "https://openalex.org/W6771917389",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6769311223",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W6762392948",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W6776231494",
    "https://openalex.org/W6759455113",
    "https://openalex.org/W6849257913",
    "https://openalex.org/W6850326989",
    "https://openalex.org/W6772776185",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6773642575",
    "https://openalex.org/W2111406701",
    "https://openalex.org/W6675186613",
    "https://openalex.org/W6748600614",
    "https://openalex.org/W6804136108",
    "https://openalex.org/W3172318343",
    "https://openalex.org/W6748278106",
    "https://openalex.org/W6768586863",
    "https://openalex.org/W2985884876",
    "https://openalex.org/W4292159986",
    "https://openalex.org/W2070665556",
    "https://openalex.org/W2053186076",
    "https://openalex.org/W2125003829",
    "https://openalex.org/W59975075",
    "https://openalex.org/W2626017178",
    "https://openalex.org/W6755501240",
    "https://openalex.org/W6637162671",
    "https://openalex.org/W4293846201",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2109553965",
    "https://openalex.org/W6605323724",
    "https://openalex.org/W6679344019",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2739351760",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2988421999",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W6773944720",
    "https://openalex.org/W4385572956",
    "https://openalex.org/W6768817161",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W6781421678",
    "https://openalex.org/W6755310938",
    "https://openalex.org/W6789642857",
    "https://openalex.org/W6677604277",
    "https://openalex.org/W6680300913",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W6685158001",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W3189447831",
    "https://openalex.org/W6779469252",
    "https://openalex.org/W6779629370",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6738248287",
    "https://openalex.org/W6773331377",
    "https://openalex.org/W3135737072",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W6764256271",
    "https://openalex.org/W2963928014",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2962881743",
    "https://openalex.org/W2746097825",
    "https://openalex.org/W1673923490",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3000577518",
    "https://openalex.org/W2964186069",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4919037",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2975185270",
    "https://openalex.org/W3125516434",
    "https://openalex.org/W3021533447",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W4287692509",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4288548690",
    "https://openalex.org/W2963309496",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3213881810",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2964116600",
    "https://openalex.org/W2924690340",
    "https://openalex.org/W2963236897",
    "https://openalex.org/W4299585995",
    "https://openalex.org/W4389161308",
    "https://openalex.org/W2149846618",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3006381853",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2117499988",
    "https://openalex.org/W4320561490",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2948695030",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2618017917",
    "https://openalex.org/W2100341991",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2138857742",
    "https://openalex.org/W3007685714",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W4287824654"
  ],
  "abstract": "The advent of large-scale pretrained language models (PLMs) has contributed greatly to the progress in natural language processing (NLP). Despite its recent success and wide adoption, fine-tuning a PLM often suffers from overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks. To address this problem, we propose a novel and effective fine-tuning framework, named layerwise noise stability regularization (LNSR). Specifically, our method perturbs the input of neural networks with the standard Gaussian or in-manifold noise in the representation space and regularizes each layer's output of the language model. We provide theoretical and experimental analyses to prove the effectiveness of our method. The empirical results show that our proposed method outperforms several state-of-the-art algorithms, such as norm and start point (L2-SP), Mixout, FreeLB, and smoothness inducing adversarial regularization and Bregman proximal point optimization (SMART). In addition to evaluating the proposed method on relatively simple text classification tasks, similar to the prior works, we further evaluate the effectiveness of our method on more challenging question-answering (QA) tasks. These tasks present a higher level of difficulty, and they provide a larger amount of training examples for tuning a well-generalized model. Furthermore, the empirical results indicate that our proposed method can improve the ability of language models to domain generalization.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nImproving Pre-trained Language Model Fine-tuning\nwith Noise Stability Regularization\nHang Hua , Xingjian Li Dejing Dou , Senior Member, IEEE\nCheng-Zhong Xu , Fellow, IEEE, and Jiebo Luo , Fellow, IEEE\nAbstract—The advent of large-scale pre-trained language\nmodels has contributed greatly to the progress in natural language\nprocessing. Despite its recent success and wide adoption, fine-\ntuning a pre-trained language model often suffers from overfitting,\nwhich leads to poor generalizability due to the extremely high\ncomplexity of the model and the limited training samples from\ndownstream tasks. To address this problem, we propose a novel\nand effective fine-tuning framework, named Layerwise Noise\nStability Regularization (LNSR). Specifically, our method perturbs\nthe input of neural networks with the standard Gaussian or In-\nmanifold noise in the representation space and regularizes each\nlayer’s output of the language model. We provide theoretical\nand experimental analyses to prove the effectiveness of our\nmethod. The empirical results show that our proposed method\noutperformes several state-of-the-art algorithms such as L2-\nSP [1], Mixout [2], FreeLB [3] and SMART [4], etc. In addition\nto evaluating the proposed method on relatively simple text\nclassification tasks, similar to the prior works, we further evaluate\nthe effectiveness of our method on more challenging question-\nanswering tasks. These tasks present a higher level of difficulty,\nand they provide a larger amount of training examples for tuning\na well-generalized model. Furthermore, the empirical results\nindicate that our proposed method can improve the domain\ngeneralization performance of language models on unseen domain\ndata.\nIndex Terms—Pre-trained Language models, Fine-tuning, Reg-\nularization, In-domain Generalization, Domain Generalization.\nI. I NTRODUCTION\nL\nARGE-SCALE pre-trained language models (PLMs)\nhave significantly boosted state-of-the-art performance\non Natural Language Processing (NLP) tasks [5]–[10]. In\nparticular, some recently emerged powerful language models\n[9], [11], [12] with impressing performance on natural language\nunderstanding (NLU) tasks in popular NLP benchmarks such as\nGLUE [13], Super GLUE [14], LAMA [15], [16], and variants\nof these models have been successfully applied in ever-wide\nscenarios [10], [17]–[20].\nFine-tuning is the prevalent paradigm for utilizing large\npre-trained language models to perform downstream tasks. By\ninitializing with the pre-trained model, the new task reuses most\nH. Hua and J. Luo are with University of Rochester, Rochester, NY 14627\nUSA ( E-mail: {hhua2, jluo}@cs.rochester.edu).\nX. Li is with Carnegie Mellon University, Pittsburgh, PA 15213 USA (\nE-mail: xingjia2@andrew.cmu.com).\nD. Dou is with BCG in Greater China, Beijing, 100027, China. ( E-mail:\ndou@cs.uoregon.edu).\nC.Z Xu is with the State Key Lab of IOTSC, Faculty of Science and\nTechnology, University of Macau, Macau SAR 999078, China ( E-mail:\nczxu@um.edu.mo).\nH. Hua and X. Li contributed equally. Correspondences to J. Luo.\nof the well-learned parameters, thus preserving the intrinsic\ngeneralizable knowledge while pursuing adaptation to the\ndesired domain. However, despite the simplicity and ubiquity\nof fine-tuning in modern NLP, this process is brittle [21], i.e., a\nstraightforward fine-tuning process sometimes leads to unstable\nsolutions that generalize poorly to unseen data. Empirical\nstudies have discovered that randomness brought by data\norder and weight initialization causes unexpected results [22].\nNevertheless, ad-hoc strategies such as seed selection and\nearly stopping [22] provide a neither theoretical nor practical\nguarantee. A systematic solution to this challenge, especially in\nconditions where labeled examples are insufficient, is needed.\nImproving generalization is always one of the fundamental\ngoals of machine learning. Adding noise to the input [23],\n[24] has been proven to have an equivalent effect to training\nover clean input with an additional regularization term to\nconstrain the solution space. Recent work [25], [26] discovers\nthat the generalization capacity of deep neural networks\nis theoretically linked with the so-called interlayer cushion,\ncharacterized by noise sensitivity of the network w.r.t. input.\nWhile deep convolutional networks exhibit decent behaviors of\nnoise stability as shown in [25], we find that it is not exactly\nthe case for transformer-based language models, e.g., BERT,\nwhich has more complex multi-head self-attention architectures.\nA preliminary experiment is conducted to investigate the\nsensitivity to the Gaussian noise on transformers. From the\nresults shown in Figure 1, two important observations can be\nmade as follows.\n• The propagating noise, if injected in lower layers 1, can\nbe amplified in some higher layers of BERT.\n• Noise stability of a higher layer has a roughly positive\ncorrelation with the generalization performance.\nMotivated by the above observations, we introduce a new\nframework of noise stability regularization to improve pre-\ntrained language models’ fine-tuning in this work. Specifically,\nwe impose an additional optimization term that forces higher\nlayers of BERT to be resilient to a Gaussian noise injected on\nlower layers, named Layer-wise Noise Stability Regularization\n(LNSR) [27], [28]. The proposed regularization term has a\ngood theoretical property of smoothing the learned function.\nWe further design an advanced implementation of LNSR\nthat generates a random noise with directions restricted by\nneighborhoods of the input point. The qualitative analysis\ndemonstrates its equivalence to noise stability w.r.t. a Gaussian\n1In this paper, we use the term lower layers to denote layers close to the\ninput and higher layers to denote those close to the output.\narXiv:2206.05658v2  [cs.CL]  9 Nov 2023\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\n1 3 5 7 9 11 13 15 17 19 21 23\nlayer\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\n0.055\n0.060error ratio\nlayer0\nlayer1\nlayer2\nlayer3\nlayer4\nlayer5\nlayer6\nlayer7\nlayer8\nlayer9\nlayer10\nlayer11\nlayer12\n7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nlayer\n0.022\n0.024\n0.026\n0.028\n0.030error ratio acc=0.876\nacc=0.881\nacc=0.891\nacc=0.894\nacc=0.904\nacc=0.909\nFig. 1: Demonstration of how injected noise attenuates on a RoBERTa-Large model fine-tuned on the MRPC dataset. The error ratio (Y-axis)\nis defined as the relative output deviation from the original observation on each layer (X-axis). In the left plot, we show behaviors of noise\nstability by injecting noise at different positions, i.e. each injected position corresponds to a curve. The noise has a random direction, whose\nmagnitude is 5% of that of the original input. We observe that though the propagated perturbation decreases rapidly on lower layers, but\nbecomes volatile on the higher layers (e.g. layers 14-20), indicating the poor robustness and risk of over-fitting on these higher layers. In\ncomparison, we present decent behaviors of noise stability on VGG-19 in Appendix A. We also show in the right plot that, models more\nrobust to input perturbations (noise is injected at layer 1) tend to deliver higher accuracies. Specifically, each curve represents a fine-tuned\nmodel, whose accuracy is marked in the legend. The accuracy shows obviously positive correlation with noise stability, especially for higher\nlayers.\nnoise injected in the data manifold, thus referred to as\nIn-manifold Layer-wise Noise Stability Regularization (In-\nmanifold LNSR).\nThe main contributions of this paper can be summarized as\nfollows.\n• Our work is the first step in the investigation of noise\nstability properties on transformer-based architectures,\nwhich are of great interest in natural language processing\nand computer vision applications. We empirically extend\nobservations about noise stability on fully connected\nnetworks and deep convolutional networks to transformers.\n• We propose two alternative implementations of noise\nstability regularization. Different from earlier works that\ndirectly use perturbed input examples to fit the labels,\nour method adopts a novel layer-wise regularization that\nexplicitly enforces noise stability of middle layers. Based\non this idea, we further present a more effective In-\nmanifold noise stability regularization. Specifically, the\nsampled noise is restricted in the region formed by\ninterpolations between the input point and its nearest\nneighborhoods. Under commonly accepted assumptions,\nthe simple method can be regarded as for sampling\nGaussian noise in the low-dimensional data manifold.\n• We provide a detailed theoretical analysis of the noise sta-\nbility regularization w.r.t. the Gaussian noise, revealing its\nconnection with the Lipschitz continuity and the Tikhonov\nregularizer. The proposed noise stability regularization\nis also shown to have a form with better optimization\nproperties than the conventional method that simply trains\nthe model over the perturbed inputs. For the In-manifold\nnoise stability, we provide a qualitative analysis of its\nrelationship with manifold learning.\n• We conduct extensive experiments on several popular NLP\ntasks, covering different task types (text classification and\nquestion answering) and a wide range of dataset scales\n(from ∼ 102 to ∼ 106). We compare our approach with\nstate-of-the-art methods aimed at improving fine-tuning\npre-trained language models such as L2-SP [1], Mixout [2]\nand SMART [4]. Our approach not only consistently\nimproves the overall performance but also obtains more\nstable fine-tuning results over multiple random trials.\nMoreover, our algorithm is also effective in dealing with\nthe risk of domain shift, demonstrated by additional\nexperiments on domain generalization benchmarks.\nThe remainder of this paper is organized as follows. Section\n2 introduces the notations and preliminaries used in this\npaper. In Section 3, we present the overall framework and\nimplementation details of our method, along with theoretical\nunderstanding. In Section 4, we evaluate our method on diverse\nNLP tasks, including classification, regression, and question\nanswering. In Section 5, we provide further analyses explaining\nwhy our approach delivers good performance. In Section 6, we\ngive a brief overview of related studies. Finally, in Section 7,\nthe paper is concluded.\nII. N OTATIONS AND PRELIMINARIES\nIn this section, we will first introduce the notations frequently\nused in this paper for clear representations. Then we briefly\npresent some preliminary knowledge closely connected with\nour proposed algorithm and theoretical analysis.\nA. Notations\nThroughout, we will frequently use the set of notations and\nterminology listed in Table I.\nB. Preliminaries\n1) Properties of the Gaussian Distribution: This part briefly\nintroduces properties of the Gaussian distribution used in\nthe paper. The Gaussian distribution is often referred to as\nN(µ, σ2), where the parameter µ is the expectation of the\ndistribution, while σ is the standard deviation. The general\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\nTABLE I: General Notations\nVariables:\nd the dimensionality of the input in the original representation space\n(x, y) an input point x ∈ Rd and its corresponding label y\nε a small random noise with the same dimension as the input x\n˜x the perturbed input that ˜x = x + ε\nθ the parameter of a model\nL the total number of layers in a BERT model\nb the index of a layer where the noise is injected on its input\nr the index of a layer where the noise stability is enforced, 1 ≤ b ≤ r ≤ L\nk the number of nearest neighbors of an input x\nNk(x) the set of the k-nearest neighbors of x\nFunctions and Operators:\nF a BERT model parameterized with θ\nf a real-valued function in convenience of the theoretical analysis\nL the loss function\nR the regularization term\n∥.∥ the L2 norm of a vector, i.e. if x ∈ Rd, ∥x∥ = ∥x∥2 =\nqPd\ni=1 x2\ni\n∥.∥F the Frobenius Norm of a matrix, i.e. if A ∈ Rm×n, ∥A∥F =\nqPm\ni=1\nPn\nj=1 Aij\n◦ the Hadamard product of two matrices A and B (with the same dimension) as\n(A ◦ B)ij = AijBij\nConstants:\nI the identity matrix with ones on the main diagonal and zeros elsewhere\n1 the all-ones matrix where every element is equal to one\nGaussian distribution N(µ, σ2) can be described by its proba-\nbility density function\np(ε) = 1√\n2πσ e−(ε−µ)2\n2σ2 . (1)\nFor random vectors, the univariate Gaussian distribution\ncan be generalized to higher dimensions, described by the\nmultivariate Gaussian distribution ε ∼ N(µ, Σ), where µ is\nthe mean vector and Σ is the covariance matrix. In this work,\nwe shall focus on the standard case that µ is a zero vector and\nΣ is a diagonal matrix with all diagonal elements being σ2,\ni.e. Σ = σ2I.\nNote that the d-dimensional standard multivariate Gaussian\ndistribution ε ∼ N(0, σ2I) has the following properties:\n∀i, j∈ [1, d], E\nε\n{εiεj} = σ2δij, (2)\nand\n∀i, j, k∈ [1, d], E\nε\n{εiεjεk} = 0, (3)\nwhich will be frequently used in our theoretical analysis.\n2) Lipschitz Continuity: In mathematical analysis, Lipschitz\ncontinuity is used to quantify the degree of smoothness of\na function. Intuitively, it describes how fast can the output\nchange as the input changes. It’s of great interest to the\nmachine learning community because a smooth function is\noften regarded as generalizing well to unseen data. A formal\ndefinition is presented as follows.\nDefinition 1. Given two metric spaces (X, dX) and (Y, dY ),\na function f : X → Y is called Lipschitz continuous if there\nexists a real constant K ≥ 0 such that, for all x1, x2 ∈ X,\ndY (f(x1), f(x2)) ≤ KdX(x1, x2). (4)\nThe smallest K satisfying the previous inequality is called\nthe Lipschitz constant of f, denoted Lip(f). In this work, we\nconsider only the common case of the Hilbert space on Rn\nequipped with the distance metric d(a, b) = ∥a − b∥. Under\nan assumption that f is locally Lipschitz, [29] proposes an\napproach to estimate the Lipschitz constant by the following\ntheorem.\nTheorem 1 (from [29]). If f : Rd → Rm is a locally Lipschitz\ncontinuous function, then f is differentiable almost everywhere.\nMoreover, if f is Lipschitz continuous, then\nLip(f) = sup\nx∈Rd\n∥Jf (x)∥2, (5)\nwhere Jf (x) denotes the differential operator, also called the\nJacobian, of f at x. Generally Jf (x) is a matrix that Jf (x) ∈\nRm×d and ∥.∥2 is the spectral norm of a matrix A as\n∥A∥2 = sup\nx∈Rd\n∥Ax∥2\n∥x∥2\n. (6)\n3) Training with Noise: Training with noise has been well-\nstudied in previous works. Earlier work [30] proposes to add\na random vector onto the input before being fed to the neural\nnetwork, leading to improved generalization performance.\n[23] theoretically proves that training with Gaussian noise is\nequivalent to Tikhonov regularization, which aims at making the\nloss surface flatter at the input, through involving derivatives of\nthe objective function w.r.t. different orders. When considering\na scalar input variable x and output variable y, the Tikhonov\nregularizer [31] takes a general form\nRTik(θ) =\nX\nr\nZ\nhr(x)(∂rf\n∂xr )2dx. (7)\nSpecifically, if sampling the noise from a distribution that\nhas zero means and is independent between different inputs,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\ntraining over perturbed inputs [23] equals to training over clean\ninputs with an extra regularization term\nR(θ) ≈ E\n(x,y)\n{(f(x; θ) − y)Tr(H(x)) + ∥J(x)∥2}. (8)\nA drawback of simply adding noise to inputs [23] is that\nthe Hessian term is not guaranteed to be positive. Such an\nunconstrained term used as a regularizer may lead to a negative\nHessian trace on a very large scale. [24] proposes an improved\nregularized objective that adds noise to inputs of the Jacobian\nfunction ∥J(˜x; θ)∥2. By ignoring the higher order of derivative,\nthe perturbed Jacobian induces an approximated regularizer as\nR(θ) ≈ E\nx\n{∥J(x)∥2 + 2σ2∥H(x)∥2\nF }, (9)\nwhere σ is the variance of the noise. Though this new noise\nterm is proved to avoid the undesired effect of [23], involving\nperturbed Jacobian in the optimization objective requires so-\ncalled double back-propagation. This encounters considerable\ncomputational inefficiency for modern DNNs such as BERT.\n4) Dimensionality Reduction and Manifold Learning: Di-\nmensionality reduction plays a crucial role in enhancing human\nunderstanding and processing of real-world data, particularly\nwhen confronted with high-dimensional and intricate structures.\nHowever, traditional approaches like Principal Components\nAnalysis (PCA) are limited to linear structures. In contrast,\nmanifold learning has emerged as a powerful framework for\nnon-linear dimensionality reduction [32], [33], operating under\nthe assumption that the target data resides in a locally Euclidean\ntopological space [34], [35]. Manifold learning techniques\nenable the visualization of high-dimensional data [36] in a\nlower-dimensional space, such as a two or three-dimensional\nmap, facilitating intuitive interpretation and analysis.\nThe In-manifold LNSR algorithm draws inspiration from\nthe aforementioned assumption that data resides in a low-\ndimensional manifold, which is locally homeomorphic to the\noriginal representation space. Based on this principle, directly\nsampling noise in the original high-dimensional space may\nprove inefficient since many features are irrelevant to the target\ndata. To address this issue, we adopt an efficient perturbation\nstrategy through In-manifold noise sampling, enabling effective\nmodifications to the input. It is important to note that our\napproach leverages the idea of the manifold assumption but\ndoes not aim to implement dimensionality reduction algorithms.\nIII. M ETHODOLOGY\nIn this section, we systematically introduce our algorithm\ncomposed of the following parts. We first present the general\nframework of our proposed Layer-wise Noise Stability Regular-\nization (LNSR) for BERT in subsection 3.1. Next, we describe\ntwo alternative methods of noise generalization in 3.2, which\nare the Standard LNSR that directly injects Gaussian noise,\nand In-manifold LNSR that adds random noise constrained on\nthe subspace formed by the input’s nearest neighbors. Then,\nwe provide a theoretical analysis for the two specific choices of\nnoise generation methods. In 3.3, we prove that the Standard\nLNSR has good properties related with the Lipschitz continuity\nand Tikhonov regularization. In 3.4, we demonstrate that the In-\nmanifold LNSR is equivalent to the Standard LNSR imposed\non the data manifold under certain assumptions.\nAlgorithm 1 Fine-tuning with LNSR Regularization\nInput: Training set D, learning rate τ, number of training\niterations N, number of layers L, neural network f and its\ncorresponding parameter θ, the layer index b where noise\nis injected, and regularization weights {λb,r}L\nr=b.\n1: Initialize θ with θ0 learned from the pre-trained task\n2: for iteration=1, 2, ..., Ndo\n3: sample a batch of data B ∼ D\n4: R ←0\n5: for each x ∈ B do\n6: if Standard LNSR then\n7: ε ∼ N(0, σ2I)\n8: else if In-manifold LNSR then\n9: get k-nearest neighborhoods Nk(x) = {x(j)}\n10: get differences {d(j)|d(j) = x(j) − x}\n11: orthogonalize {d(j)} and get {ˇd(j)}\n12: perform k iid sampling of {ε(j)|ε(j) ∼ N(0, σ2)}\n13: ε = Pk\nj=1 ε(j) ˇd(j)\n14: end if\n15: ˜x ← x + ε\n16: feed x and ˜x into the network f\n17: for r = b, b+ 1, ..., Ldo\n18: R ← R+ λb,r||fb,r(x) − fb,r(˜x)||2\n19: end for\n20: end for\n21: g ← 1\n|B|\nP\n(x,y) ∇[L(f(x; θ), y) + R]\n22: θ ← θ − τg\n23: end for\nOutput: θ\nA. The General Framework\nGiven a pre-trained model as initialization, fine-tuning BERT\nis a general task of supervised learning that aims at minimizing\nan expected error L with respect to the model’s parameter θ\nover the data distribution. Considering the high memorization\ncapacity of deep neural networks, a regularization term R\nresponsible to control the model’s complexity is often employed\nto improve the generalized performance of the model. Therefore,\na general form of the optimization objective can be represented\nas\nθ∗ = arg min\nθ\nE\n(x,y)\n[L(f(x; θ), y)] + R(θ), (10)\nwhere we omit the notation of the data distribution without\nambiguity. A most common choice for the regularization R\nis the L2 normalization of the parameter θ, that is ∥θ∥2, also\ncalled the weight decay, particularly for deep neural networks.\nDespite its ubiquity, there’s no theoretical evidence for the\neffectiveness of such a simple data-independent regularization\nin DNNs [37], [38].\nWe are interested in the behavior of noise stability, which\nserves as a data-dependent regularizer. Specifically, given an\ninput point x, we generate a perturbed input ˜x by adding a\nrandom noise ε with a small magnitude to x. Noise stability\ncharacterizes to what degree the output w.r.t. ˜x deviates from\nthat w.r.t. the clean input x. For the multi-layer transformer\narchitecture, we adopt a layer-wise regularization to enforce\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nFig. 2: Illustration of noise stability performed in the original Rd\nspace (left) and the data manifold ( right). The black and grey dots\nrefer to the clean and perturbed data respectively. In the original space,\nthe perturbed data, lying on a unit sphere around the clean input,\nwould likely to be out-of-domain. In contrast, adding in-manifold\nnoise to the clean data is probably to yield a perturbed point with high\ndata density. In this sense, in-manifold perturbation more accurately\nsimulates data shifts between training and test data and, as a result,\ndelivers better generalization performance on unseen data.\nnoise stability at each layer. Formally, we define the input\nof layer b as xb and the network between the b-th and r-th\nlayer as fb,r, which is parameterized by θb,r. Note that when\nb = r, fb,r represents a single layer. If we inject the noise at a\nfixed layer b and regularize each higher layer r ≥ b, the noise\nstability term can be represented as\nR(θ) = E\nx,ε\nLX\nr=b\nλb,r||fb,r(xb + ε; θb,r) − fb,r(xb; θb,r)||2,\n(11)\nwhere λb,r is the coefficient to control the weight of regularizing\nfb,r. Given Eq. 11 as the general form of noise stability used\nin multi-layer transformers, a subsequent question is how to\ngenerate the noise ε for a specific input xb.\nB. Methods for Noise Generation\nIn this work, we introduce two progressive methods for\nnoise generation. The first sample noise from the multivariate\nGaussian distribution for the noise stability regularization is\ncalled Standard LNSR. We also propose an improved method\ncalled In-manifold LNSR, which samples random noise on the\nsubspace formed by the input’s k-nearest neighbors.\n1) Standard LNSR: For the Standard LNSR, ε is randomly\nsampled from the standard multivariate Gaussian distribution\nas ε ∼ N(0, σ2I). That is, each element εi is independently\nsampled from the zero-mean Gaussian distribution as εi ∼\nN(0, σ2).\n2) In-manifold LNSR: In this section, we introduce the\nimplementation of In-manifold LNSR in detail with intuitive\nexplanations. More formal discussions are deferred to 3.4. A\nwidely accepted assumption for a smooth manifold is that, an\ninput point x and its k neighborhoods Nk(x) form a subspace\nthat is approximated linearly. With {x(j)|x(j) ∈ Nk(x), j=\n1, 2, ..., k} denoting these neighborhoods, we get the set of\nneighbored differences as {d(j) = x(j) −x|x(j) ∈ Nk(x), j=\n1, 2, ..., k}. Under the manifold assumption, vectors in {d(j)}\nlie on the same linear subspace around x as the origin.\nImagining concatenating all these row vectors {d(j)} to form\na matrix, we perform the Gram-Schmidt orthogonalization on\nthe matrix and obtain the transformed set {ˇd(j)} which are\nall orthogonal to each other. It’s obvious that {ˇd(j)} lie on\nthe same subspace as {d(j)}. To generate In-manifold random\nnoise, we independently sample k random variables from the\nunivariate Gaussian distribution as ε(j) ∼ N(0, σ2). Then we\ngenerate the In-manifold noise ε by a random interpolation\nin the region formed by {ˇd(j)} as ε = Pk\nj=1 ε(j) ˇd(j).\nFigure 2 demonstrates the difference between sampling standard\nGaussian noise and sampling In-manifold noise.\nC. Analysis of the Standard LNSR\nHere we present mathematical analyses that highlight the\nproperties of our proposed approach. Our primary goal is to\nexamine the relationships between LNSR and classical tech-\nniques used for quantifying and controlling model complexity.\nThe reduction of model complexity is widely acknowledged\nas a crucial principle in mitigating overfitting in machine\nlearning. However, due to the intricate nature of Deep Neural\nNetworks (DNNs), which encompass a massive number of\nparameters, traditional metrics for quantification often become\ncomputationally challenging. Based on the analyses in this part,\nLNSR is shown to have implicit connections with Lipschitz\ncontinuity and Tikhonov regularization.\nWithout loss of generality, the analysis is based on a general\nreal-valued function f : Rd → R. For simplicity, we ignore\nthe notation of the parameter θ in the following analysis. ε\nis the standard Gaussian noise as defined in 3.2.1. Given that\nε has a small magnitude (i.e. its variance σ2), we adopt the\nsecond-order Taylor approximation to present f(x + ε) as:\nf(x + ε) = f(x) + J(x)ε + 1\n2εT H(x)ε + O(ε3), (12)\nwhere J(x) and H(x) are the Jacobian and Hessian of f w.r.t\nx, respectively.\n1) Connection with Lipschitz Continuity: By ignoring the\nsecond and higher order terms in the Taylor expansion, it is\neasy to derive that the noise stability regularization equals to\nthe L2 norm of the Jacobian Jf\nE\nx,ε\n∥f(x + ε) − f(x)∥2 = E\nx,ε\n∥J(x)ε∥2 = σ2 E\nx,ε\n∥J(x)ε∥2\n∥ε∥2 .\n(13)\nAs indicated in II-B2, the Lipschitz constant of f is\nbounded by the supremum of spectral norm of the Jacobian Jf .\nTherefore, injecting input noise and explicitly minimizing the\noutput discrepancy (between the clean and perturbed output)\nhas the same form as Eq. 5 inside the operation of calculating\nthe supremum.\nNote that the objective of minimizing Eq. 13 (w.r.t. a random\nnoise) is likely to lower the bound in Eq. 5 but is of course not\nguaranteed to do so. In fact, ε with a direction that maximizes\n∥J(x)∥2, or ∥f(x + ε) − f(x)∥2, is so-called an adversarial\nperturbation and such a perturbed input ˜x = x + ε is called\nan adversarial example [39]. Though the adversarial example\ninduces a more accurate indicator of the Lipschitz constant,\nempirical studies show that training with such extreme inputs,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\naiming at promoting the robustness of adversarial examples,\nsignificantly harms the performance on clean inputs [40]. An\nintuitive explanation is that the adversarial example focuses on\na rare perturbation direction (barely seen in real data) that is\nthe most difficult for the model to be robust. In contrast, our\nnoise stability regularization cares about noise with a uniform\ndirection, which is more diverse and more possible to exist in\nreal data, though not providing the tight Lipschitz continuity\nbound.\n2) Connection with Tikhonov Regularization: Here we\nprovide a further analysis considering the first and second-\norder terms in Eq. 12. Let Tr(.) be the trace of a matrix. We\nfirst present our main claim as follows.\nClaim 1. If a real-valued function f : Rd → R is twice\ndifferentiable with respect to its input x ∈ Rd, and ε ∈ Rd\nconforms to the standard multivariate Gaussian distribution\nε ∼ N(0, σ2I). Then, omitting terms of higher order than the\n2nd degree, the noise stability regularization R for a given x\ncan be represented as\nR = E\nx,ε\n∥f(x + ε) − f(x)∥2\n≈ σ2\n4 E\nx\n{4∥J(x)∥2 + ∥Tr(H(x))∥2 + ∥(1 − I) ◦ H(x)∥2\nF }.\n(14)\nProof. In the following derivations, we simply use J to denote\nJ(x) without ambiguity, and likewise for H(x). J and H with\nsubscripts refer to the partial derivatives, i.e. Ji = ∂f (x)\n∂xi\nand\nHij = ∂2f(x)\n∂xi∂xj\n.\nWe begin by substituting the second-order Taylor formula\ninto our definition of the noise stability regularization\nR = E\nx,ε\n{∥Jε + 1\n2εT Hε∥2}. (15)\nUsing ΩJ = Jε and ΩH = 1\n2 εT Hε, Eq. 15 can be notated\nas\nR = E\nx,ε\n{Ω2\nJ + Ω2\nH + 2ΩJ · ΩH}. (16)\nSince expectation is a linear operator, we reformulate Eq. 16\ninto expectations of three parts as RJ = E\nx,ε\n{Ω2\nJ}, RH =\nE\nx,ε\n{Ω2\nH} and RJH = 2 E\nx,ε\n{ΩJ · ΩH}.\nFor the first part RJ, we obtain\nRJ = E\nx,ε\n∥Jε∥2\n= E\nx\nE\nε\n{\nX\ni\n(εiJi)2 +\nX\ni,j\ni̸=j\nεiεjJiJj}\n= E\nx\nX\ni\nJ2\ni E\nε\n{ε2\ni } = σ2E\nx\n{∥J∥2}.\n(17)\nFor the second part RH, we have\nRH = E\nx,ε\n∥1\n2εT Hε∥2\n= 1\n4E\nx\nE\nε\n{\nX\ni,j\nε2\ni ε2\njHiiHjj +\nX\ni,j\ni̸=j\nε2\ni ε2\njH2\nij}\n= σ4\n4 E\nx\n{(\nX\ni\nHii)2 + ∥H∥2\nF −\nX\ni\nH2\nii}\n= σ4\n4 E\nx\n{∥Tr(H)∥2 + ∥(1 − I) ◦ H∥2\nF }.\n(18)\nFor the third part RJH , we have\nRJH = E\nx,ε\n{JεεT Hε}\n= E\nx,ε\n{(\nX\ni\nεiJi)(\nX\ni,j\nεiεjHij)}\n= E\nx\nX\ni,j,k\nJkHijE\nε\n{εiεjεk} = 0.\n(19)\nSubstituting Eqs. 17,18 and 19 into Eq. 16 completes the\nproof.\nAnalysis. Eq. 14 characterizes our connection with the\nTikhonov regularizer, where the proposed noise stability\nregularization has the effect of constraining the first and second-\norder input derivatives of the objective function f.\nIn an analogy with previous works, our method involves\ncommon terms of the L2 norm of Jacobian, the L2 norm of\nHessian trace, and the Frobenius norm of Hessian. Compared\nwith [23] and [24], our method is capable of inheriting their\nmerits and overcoming their flaws. Specifically, the proposed\nLNSR regularizes the positive guaranteed Hessian trace that\navoids undesirable solutions as [23]. Compared with [24], our\nmethod is more efficient, as adding noise to the input does\nnot introduce much computation for DNNs. However, [24]\ninvolves the gradient of Jacobian during the back-propagation\nprocess, which is considerably more complex for DNNs.\nFurthermore, it is important to note that the conclusions\ndrawn in [23] and [24] primarily rely on the assumption of\na regression task with the Mean Squared Error (MSE) loss.\nThese approaches utilize labels to impose regularization by\nensuring the perturbed input aligns with its corresponding label\nor minimizing the perturbed Jacobian, which also depends on\nlabels. Although our LNSR method shares a similar concept of\ndirectly adding noise to the input, it introduces a regularizer in\nan unsupervised manner, leveraging the clean output as virtual\nsupervision. As a result, our framework offers significantly\ngreater flexibility by reducing the reliance on labels and specific\nforms of the loss function.\nD. Analysis of In-manifold LNSR\nIn this part, we shall analyze the effect of In-manifold LNSR.\nThe main conclusion is summarized as follows.\nClaim 2. Suppose that the input data lie in a k-dimensional\nsmooth manifold, with each data point x ∈ Rd and its\nneighbors lying on a locally linear patch. Provided there\nare sufficient neighbors with uniformly distributed directions\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nFig. 3: Illustration of a locally linear patch in a 2-dimensional manifold\nformed by a data point x ∈ R3 and its neighbors n1, n2, n3, n4, n5 ∈\nR3. Orange d1 and d2 refer to two difference vectors and blue d1\nand d2 are their corresponding orthogonal vectors. Blue d1 and d2\nare used to sample in-manifold noise ε. The perturbed data x + ε\nstill lies in the manifold.\naround x, noise ε sampled according to Algorithm 1 is\napproximate standard multivariate Gaussian in the manifold\nspace around x.\nNext, we provide a detailed analysis to verify the claim. First,\nin Section 3.4.1, we explain why the generated noise lies in the\nmanifold, adopting the manifold assumption of Locally Linear\nEmbedding (LLE) [32]. Then, in Section 3.4.2, we prove that\nthe generated noise follows the standard multivariate Gaussian\ndistribution in the manifold space. Finally, in Section 3.4.3,\nwe discuss the assumptions and approximations involved in\nthe claim.\n1) Noise on the Locally Linear Patch: We first give a brief\noverview of Locally Linear Embedding (LLE) [32] for manifold\nlearning. Note that our purpose is not designing or apply a\nmanifold learning algorithm. Instead, we intend to borrow the\nassumptions and understandings about the manifold to generate\nin-manifold noise.\nLocally Linear Embedding. The intuition behind LLE [32]\nis to regard a smooth manifold as a collection of overlapping\nlinear patches, provided these patches are small enough. Then,\nthe local geometry can be characterized by a weight matrix\nW, which is to be solved by minimizing the reconstruction\nerror\nR(W) =\nX\ni\n∥x(i) −\nX\nj\nWijx(j)∥2. (20)\nWe ignore the constraints used for computing W as it’s not\ndirectly related to our work. Ideally, there exists an appropriate\nW that makes R(W) near zero. In such cases, each x(i) can\nbe approximately represented as a linear combination of its\nneighbors, i.e. they lie on a linear subspace.\nIn-manifold Noise. Here we show that the noise w.r.t. a data\npoint x generated according to Algorithm 1 lies on the linear\npatch expanded by x and its neighbors. It’s obvious that the\ndifference vectors {d(j)|d(j) = x(j) − x} lie on the same\nlinear patch with x and its neighbors. Linear transformations\nof these {d(j)}, e.g. the orthogonal variants {ˇd(j)}, should be\nstill in the same subspace and, so are the linear combination\nof these {ˇd(j)}. Therefore, the perturbed input x + ε with the\nnoise ε = Pk\nj=1 ε(j) ˇd(j) lies on the linear space formed by x\nand its neighbors. Note that, this does not mean a guarantee\nthat x + ε lies within the linear patch unless both the noise\nmagnitude and direction are properly constrained. However,\nby suggesting that there exist, sufficient neighbors which have\ndiverse directions, x + ε should be on the linear patch with a\nhigh probability. See Figure 3 for an intuitive illustration of\nthe locally linear patch.\n2) In-manifold Standard Multivariate Gaussian Noise:\nRecall that the noise is generated based on orthogonal difference\nvectors as ε = Pk\nj=1 ε(j) ˇd(j). When being projected to the\nmanifold space by M : Rd → Rk, their angles will also be\npreserved according to the locally linear patch assumption [32].\nSo, the projected difference vectors M(ˇd(j)) ∈ Rk are still\northogonal.\nImagine that we change the coordinate system in order to\nlet M(ˇd(j)) be the one-hot vector where only the j-th element\nis 1 and all remaining are 0. As a result, the j-th element\nof M(ε) = Pk\nj=1 ε(j)M(ˇd(j)) is just ε(j), which follows the\nstandard Gaussian distribution. Thus, the projected noise M(ε)\nconforms the standard multivariate Gaussian distribution.\nE. Choices of Hyperparameters\nOur method involves additional hyperparameters. Here\nwe describe how we choose a reasonable hyperparameter\nconfiguration to ensure the performance of our method.\n1) Noise Magnitude: We adopt an adaptive scheme to\ndetermine the magnitude of injected noise. Specifically, a\nnoise is firstly sampled from a standard multivariate Gaussian\ndistribution and then re-scaled by a scalar coefficient η. η is\nset to be a fixed proportion between the magnitude of the\noriginal feature vector x and that of the noise vector ϵ as\nη = 0.05∥x∥2\n2/∥ε∥2\n2.\n2) Noise Injected Position: In this paper, the noise is always\ninjected at the lowest layer, i.e. the word embedding layer, for\nthe regularization of noise stability. In this way, we achieve\nthe effect of stabilizing all transformer layers. Moreover, for\nIn-manifold LNSR, obtaining hidden-layer representations of\nk-nearest neighbors is much more laborious since it needs\nadditional feed-forward computations.\n3) Number of Nearest Neighbors for In-manifold LNSR: Un-\nder the manifold assumption, the number of nearest neighbors\nb depends on the underlying manifold dimension. However,\nestimating such a dimension is often intractable for real-world\ndata [33]. In this work, we empirically find that b = 10 usually\nperforms well for in-manifold LNSR. While further increasing\nb tends to violate the manifold assumption, a too-small b\ninduces an over-constrained noise space and would reduce the\neffectiveness of the regularization.\nF . Computational Complexity\nHere we provide an analysis of the computational complexity\nof our proposed LNSR method in comparison to relevant\nadversarial-based approaches. As the training process is com-\nmon among these methods, our focus is specifically on the\ncomplexity associated with noise generation.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\n1) Standard LNSR: Although the regularization term of\nnoise stability is calculated for every layer, noise generation\nis performed only once at the noise input layer, for which we\nchoose the first intermediate layer. Denoting the length of input\ntokens as M, and the dimensionality of the embeddings as\nd, generating standard Gaussian noise in a training iteration\nrequires the complexity of O(Md). Note that we do not employ\nexpensive high-dimensional multivariate Gaussian sampling.\n2) In-manifold LNSR: This advanced method involves\nadditional calculations on searching for k-nearest neighbor-\nhoods and forming a manifold space for each input token.\nThrough the utilization of widely adopted space partitioning\nalgorithms, the computational complexity of the first component\ncan be expressed as O(Mkd ∗ log(N)), where N is the\nsize of the vocabulary used in K-NN searching. As for the\nlatter component, only linear operations are needed with the\ncomplexity of O(Mkd).\n3) Adversarial perturbations: Since adversarial-based meth-\nods aim to calculate the worst-case perturbation for a given\ninput instance, it typically requires several training iterations\nover the entire network to guarantee an optimal solution\nregarding the adversarial objective. Denoting the number of\nlayers as L, and the number of iterations by T, the complexity\nof generating adversarial perturbations will be dominated by\nO(T LM2d). Note that this term only accounts for the forward\ncomputation while disregarding the gradient calculation on the\ninput.\nAs all the complexity approximations involve the same d,\nwe ignore it for easier comparison. Given the approximate\nvalues as M = 10, k= 10, N= 105, T= 10, L= 10, the\ncomplexity approximations for Standard LNSR, In-manifold\nLNSR, and Adversarial perturbations are O(10d), O(103d) and\nO(103d), respectively. It can be observed that our Standard\nLNSR is much more efficient than the other two. Moreover,\nour LNSR approaches exhibit better scalability regarding the\ninput sequence length.\nIV. E XPERIMENTS\nA. Datasets\nTo verify the effectiveness of our method for improving the\ngeneralizability of language models, we conduct experiments\non text classification and question-answering (QA) tasks,\nrespectively.\n1) Text Classification Tasks: For the text classification task,\nwe conduct experiments on four few-sample (less than 10k\ntraining samples) text classification tasks of GLUE2, we present\na brief description below and refer readers to Appendix B-B\nTable VII for more details.\nCorpus of Linguistic Acceptability (CoLA [41]) is an\nEnglish acceptability judgments dataset consisting of 10657\nsentences from 23 linguistics publications. Each sentence is\nannotated with a binary label, indicating whether this sentence\nis grammatical in English. The task is classification and we use\nMatthews correlation coefficient (MCC) [42] as the evaluation\nmetric.\n2https://gluebenchmark.com/\nMicrosoft Research Paraphrase Corpus (MRPC [43]) is\na corpus for the paraphrase detection task. Each example is a\nsentence pair , whose label is 1 if the two sentences in this\npair are equivalent in semantics. We evaluate the performance\nwith the commonly adopted Accuracy and average F1 score.\nRecognizing Textual Entailment (RTE [13]) [44] [45] [46]\nis a corpus for the textual entailment task. Each example is\na sentence pair whose label is whether the first entails the\nsecond. The evaluation metric is Accuracy.\nSemantic Textual Similarity Benchmark (STS-B [47]) is a\ntask for determining the semantic similarity of a sentence pair.\nThe similarity is represented by integral numbers {1, 2, 3, 4, 5},\nwhich the model is learned to predict. Common metrics\nfor evaluation are the Pearson and Spearman correlation\ncoefficients, and we report the average of them.\n2) Question Answering Tasks: For the question-answering\ntask, we use the Stanford Question Answering Dataset\n(SQuAD) [48] as an in-domain question answering task to\nevaluate our method on more complex NLP problems. Besides,\nwe use the Machine Reading for Question Answering (MRQA)\n2019 [49] as an out-of-domain question answering task to verify\nthe effectiveness of our method for improving the domain\ngeneralizability of language models.\nSQuAD is a Machine Reading Comprehension (MRC)\ndataset. The question-answer (QA) pairs in this dataset are\nconstructed from Wikipedia articles by crowd workers. For\neach QA pair, the answer is a segment of text, or span that\ncan be extracted from the corresponding reference passage, or\nthe question might be unanswerable. We use the SQuAD v 1.1\ndataset for experiments.\nMRQA is a task for testing extractive question-answering\nmodels on their ability to generalize to the out-of-domain data.\nIn MRQA 2019, researchers collect different extractive QA\ndatasets and unify the format of data. The goal is to test how\nmodels trained on “in-domain” datasets can generalize to the\n“out-of-domain” datasets. More details about the different QA\ndatasets are summarized in Appendix B-B.\nB. Baseline Models\nFine-tuning. Fine-tuning refers to the vanilla language model\nfine-tuning method. We adopt the standard fine-tuning strategy\nof BERT and RoBERTa described in [21] and [50].\nL2-SP [1] is a regularization scheme that is used for\nconstraining the extent of parameters to update while fine-\ntuning a pre-trained model. The goal of introducing this\nregularization item is to preserve the general knowledge\ncontained in the pre-trained model. The form of the regularizer\nis Ω(w) = α\n2 ||ws − w0\ns|| + β\n2 ||w¯s||, where ws are parameters\nshared by the pre-trained and fine-tuned models and w¯s are\nthose specific to the target task.\nMixout [2] is a regularization method motivated by Dropout\n[51] and DropConnect [52]. At each training iteration, instead\nof replacing parameters with 0, Mixout replaces parameters\nwith their pre-trained value with a probability p.\nSMART [4] is a noise-based regularization method that\nadopts an adversarial training strategy to promote the models’\nsmoothness. Besides, SMART employs a Bregman proximal\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\nFig. 4: Performance distribution of different models on the selected GLUE benchmark across 25 random seeds.\npoint optimization method to prevent the model from aggres-\nsively updating during fine-tuning.\nFreeLB [3] is a simple adversarial noise regularization\nmethod that is applied on input embeddings.\nCWGNC [53] (Component-Wise Gradient Norm Clipping)\nclips the gradient norm of the Key-Query-Value parameters\nindividually in Transformer to balance the distribution of gra-\ndients across different components to adjust their convergence\nspeed to improve the fine-tuning of language models.\nC. Experimental Setup and Implementation Details\nOur model is implemented using Pytorch based on Trans-\nformers framework3 [54] and the backbone language models are\nBERT [21] and RoBERTa [50]. We adopt settings of learning\nstrategies and hyperparameters recommended by Devlin et al.\n[21]. We use the Huggingface edition AdamW [55] optimizer\nwith a learning rate ∈ {2 × 10−5, 3 × 10−5, 5 × 10−5}\nand a batch size ∈ {16, 32, 64}, and the β1 = 0 .9, β2 =\n0.999. We adopt the regularization weight in the range of\n{1.0, 0.8, 0.6, 0.4, 0.2} for different tasks. The warm-up ratio\nfor the classification task and question-answering task are set\nto 6% and 10% respectively. For a fair comparison, we set the\nmaximum numbers of epochs to 3 and 2 for classification and\nquestion-answering tasks respectively, which is the same as\nbaseline modes’ experiment settings described in the related\npapers. For the hyperparameters used in the two LNSR methods,\nwe use those described in Section III-E. We employ Faiss [56]\nfor an efficient implementation of k-NN search to generate\nin-manifold noise.\nD. Overall Performance\nFor the text classification task, Table II shows the perfor-\nmance of different models on selected GLUE datasets. Each\ndataset is trained over 25 random seeds. In the LNSR method,\nwe uniformly inject noise at the first layer on BERT-large\nand RoBERTa-large for comparison with baseline models.\nFor the In-manifold LNSR, we adopt different mix ratios for\n3https://huggingface.co/transformers/index.html\ndifferent tasks {RTE:0.1, MRPC:0.12, CoLA:0.15, STSB:0.2\n}. As we can see from the table, pre-trained language models\nwith LNSR and In-manifold LNSR outperform all the baseline\nmodels in mean and max values, which indicates the stronger\ngeneralizability of our model over other baseline models. To\nverify whether the performance gains of our methods are\nsignificant, we calculate the p-values between the performance\ndistributions of the fine-tuning baseline and our proposed\nLNSR methods. We get very small p-values on all tasks that,\n9.7 × 10−7 on RTE, 2.3 × 10−4 on MRPC, 4.7 × 10−8 on\nCoLA, and 3.3 × 10−8 on STS-2.\nStandard deviation can be used to reflect the stability of\na learning procedure. In this work, a higher std means that\nthe model is more sensitive to random seeds. According to\nthe results of experiments, models with LNSR have relatively\nlow standard deviations on most tasks, suggesting that our\nmethod is less sensitive to randomness involved by data orders\nand initializations. In addition, Figure 4 provides a clearer\nillustration. Although the In-manifold LNSR methods have\na higher std deviation compared with LNSR, it gains more\nimprovement on mean and max values. In summary, the two\nproposed LNSR methods can not only improve the average\nperformance but also reduce the instability of BERT fine-tuning.\nFor the Question Answering task, Table III shows the results\nof all the methods on the SQuAD dataset. We can see that on the\nmore challenging question-answering task with a larger dataset,\nour proposed LNSR and In-manifold LNSR methods can still\nimprove the models’ fine-tuning performance compared with\nother methods. Specifically, the BERT large model fine-tuning\nwith In-manifold LNSR achieves an average and max dev-set\nEM/F1 of 86.88/93.07 and 87.48/93.40; the RoBERTa large\nmodel fine-tuning with In-manifold LNSR achieves an average\nand max dev-set EM/F1 of 88.71/94.43 and 88.93/93.40.\nFigure 5 shows the mean and the range of EM/F1 scores’\nchanges during the fine-tuning process. 4 The p-values between\nthe performance distributions of the fine-tuning baseline and\nour proposed LNSR are 3.2 × 10−8/1.9 × 10−8, and for the\n4Due to the high training cost, we adopt 5 random seeds for comparison.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\nRTE MRPC CoLA STS-B\nBERTLARGE mean std ↓ max mean std ↓ max mean std ↓ max mean std ↓ max\nFT [21] 70.13 1 .84 72 .56 87 .57/89.75 0 .92 89 .16/91.12 60 .54 1 .49 62 .59 89 .38 0 .53 90 .23\nL2-SP [1] 70.58 1 .29 73 .28 87 .74/89.84 0 .86 88 .95/90.90 60 .19 1 .42 63 .89 89 .25 0 .62 90 .14\nMixout [2] 71.35 1 .66 74 .36 87 .63/89.80 0 .62 88 .91/90.81 63 .12 1 .68 65 .12 89 .58 0 .35 90 .11\nSMART [4] 72.23 2 .41 75 .45 87 .86/89.80 0 .63 89 .09/90.94 63 .16 1 .17 65 .21 90 .11 0 .33 90 .83\nCWGNC [53] 71.94 1.02 74.01 87 .93/90.09 0 .62 88 .49/91.85 63 .00 1 .06 65 .30 90 .10 0 .32 90 .60\nFreeLB [3] 71.87 1 .87 74 .47 87 .88/89.97 0 .78 88 .92/90.79 63 .42 1 .37 65 .79 90 .28 0.28 90.67\nStandard LNSR 73.31 1 .55 76 .17 88.50/90.42 0 .56 90 .02/91.81 63 .35 1 .05 65.99 90.23 0 .31 90 .97\nIn-manifold LNSR 74.33 1.78 78.70 87.92/89.21 0 .62 88 .97/92.31 63 .44 1 .01 65.30 90.32 0.29 90 .88\nRoBERTaLARGE\nFT [50] 80.99 2 .96 85 .19 88 .51/91.72 0 .90 90 .69/93.17 65 .27 1 .95 67 .74 91 .29 0 .40 92 .21\nL2-SP [1] 82.47 1 .59 85 .56 88 .52/91.67 0.54 89.46/92.52 64 .40 1 .03 66 .54 91 .83 0 .25 92 .38\nMixout [2] 81.76 2 .67 84 .84 88 .57/91.79 0 .73 89 .95/92.94 64 .68 1 .53 67 .26 91 .93 0 .25 92 .35\nSMART [4] 82.40 1 .78 85 .55 88 .64/91.85 0 .98 90 .19/92.93 65 .81 1 .19 67 .74 91 .89 0 .19 92 .18\nCWGNC [53] 82.49 1.36 85.19 88 .88/91.92 0 .76 89 .95/92.66 65 .86 0.84 67.97 91 .85 0 .23 92 .16\nFreeLB [3] 82.85 2 .35 85 .92 89 .67/91.30 0 .59 90 .68/93.33 64 .88 1 .17 67 .76 91 .85 0 .18 92 .05\nStandard LNSR 81.48 1 .49 83 .76 88 .65/91.72 0 .86 90 .93/93.16 65 .32 1 .25 67 .49 91 .96 0.17 92.24\nIn-manifold LNSR 82.92 1.91 86.64 90 .25/92.13 0.85 91.91/93.55 65 .92 1.47 68.71 92 .03 0.28 92.59\nTABLE II: The mean/max evaluation scores and standard deviation values on the selected GLUE benchmark datasets across 25 random seeds\nwhen fine-tuning the BERT and the RoBERTa models with different regularization methods. The evaluation metrics of MRPC are Acc/F1.\nIn-manifold LNSR, the p-values are 4.1 × 10−8/2.7 × 10−8.\nEM F1\nBERTLARGE mean std ↓ max mean std ↓ max\nFT [21] 86.75 0 .15 86 .95 92 .95 0 .08 93 .06\nL2-SP [1] 86.87 0.09 86.98 93 .06 0.06 93.15\nMixout [2] 86.81 0 .24 87 .08 93 .04 0 .13 93 .18\nSMART [4] 86.81 0 .24 87 .19 92 .99 0 .10 93 .17\nStandard LNSR 86.88 0 .11 87 .15 93 .06 0 .10 93 .18\nIn-manifold LNSR 86.95 0.22 87.48 93 .07 0.14 93.40\nRoBERTaLARGE\nFT [21] 88.47 0 .21 88 .82 94 .26 0 .11 94 .46\nL2-SP [1] 88.86 0.12 88.97 94 .53 0 .08 94 .57\nMixout [2] 88.59 0 .18 88 .85 94 .42 0 .11 94 .57\nSMART [4] 88.69 0 .17 88 .91 94 .38 0.05 94.48\nStandard LNSR 88.71 0 .18 88 .88 94 .43 0 .13 94 .63\nIn-manifold LNSR 88.93 0.13 89.09 94 .56 0.12 94.72\nTABLE III: The mean/max evaluation scores and standard deviation\nvalues on the SQuAD dataset across 5 random seeds when fine-tuning\nthe BERT and the RoBERTa models with various regularization\nmethods.\nFig. 5: The mean (solid lines) and range (shaded region) of EM and\nF1 scores during fine-tuning RoBERTa-Large on SQuAD datasets,\nacross 5 random seeds.\nV. A NALYSIS\nA. Resilience to Domain Shift\nTo verify the robustness of the domain shift of our methods,\nwe investigate the domain generalization performance of\ndifferent methods on the MRQA 2019 benchmark. We first\nfine-tune a language model on SQuAD and then evaluate the\nfine-tuned model on the out-of-domain datasets.\nAs shown in Table IV , language models fine-tuned with\nour LNSR methods outperform the vanilla fine-tuning method\noverall on the mean and max F1 scores on each of the MRQA\nout-of-domain datasets. In addition, compared with SMART,\nour methods obtain a better F1 score on most datasets which\nshows the better generalizability of our method. In-manifold\nLNSR performs better than the vanilla LNSR on most datasets.\nThe overall results demonstrate our LNSR methods are more\nrobust to domain shift problems, and models fine-tuned with\nour methods show a better ability for zero-shot domain transfer.\nFor this observation, we argue that LNSR can promote the\ngeneralization capacity of fine-tuned language models so that\nthe models show higher resilience to domain shift.\nDataset Domain FT SMART Standard LNSR In-manifold LNSR\nSQuAD Wiki 94.26/94.46 94.39/94.4794.43/94.63 94.56/94.72\nDROP Wiki 57.63/59.37 58.49/60.1158.79/61.1 58.13/59.83\nDuoRC Movie 61.83/62.98 62.31/63.5561.99/63.01 62.42/63.72\nRE Wiki 88.02/88.27 88.10/88.4988.23/88.58 88.26/88.86\nRACE Exam 52.48/53.78 53.34/53.81 52.95/53.51 52 .82/53.87\nTextbookQA Book 51.86/55.33 53.98/56.5452.90/55.10 53 .64/55.76\nBioASQ Bio 65.58/66.27 65.26/66.0466.40/67.02 67.01/67.88\nTABLE IV: F1 mean and max values for models trained on SQuAD\nand evaluated on out-of-domain datasets from the MRQA 2019 shared\ntask, across 5 random seeds.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\nRTE MRPC CoLA STS-B\nRoBERTa LARGE mean std ↓ max mean std ↓ max mean std ↓ max mean std ↓ max\nFT [50] 80.99 2 .96 85 .19 88 .51/91.72 0 .90 90 .69/93.17 65 .27 1 .95 67 .74 91 .99 0 .40 92 .50\nMix-ratio: 0.10 82.10 0 .79 84 .11 88 .89/91.98 0 .67 90 .44/93.12 65 .03 1 .24 68 .52 91 .93 0 .30 92 .32\nMix-ratio: 0.12 81.26 1 .30 84 .12 88 .64/91.80 0 .70 90 .20/92.81 65 .13 1 .24 67 .25 92 .07 0 .18 92 .50\nMix-ratio: 0.15 80.88 1 .60 84 .48 88 .58/91.76 0 .63 89 .71/92.81 65 .63 1 .16 67 .98 92 .00 0 .18 92 .39\nMix-ratio: 0.20 81.51 1 .58 84 .12 88 .17/91.49 1 .80 89 .71/92.53 65 .99 1 .46 70 .20 92 .09 0 .19 92 .55\nTABLE V: The mean/max evaluation scores and standard deviation values on the selected GLUE benchmark datasets across 25 random\nseeds when fine-tuning the RoBERTa model with different In-manifold noise scales.\nRTE MRPC CoLA STS-B\nBERTLARGE mean std ↓ max mean std ↓ max mean std ↓ max mean std ↓ max\nFT 70.13 1 .84 72 .56 87 .57/89.75 0 .92 89 .16/91.12 60 .54 1 .49 62 .59 89 .38 0 .53 90 .23\nFT (4 Epochs) 70.69 1 .97 73 .65 88 .15/90.20 0 .65 89 .21/91.15 60 .69 1 .24 62 .09 89 .29 0 .56 90 .12\nFT+Gaussian Noise 70.62 1 .56 72 .93 87 .95/90/15 0 .83 89 .33/91.16 60 .18 1 .58 62 .59 89 .34 0 .51 90 .11\nFT+In-manifold Noise 73.14 2 .11 77 .97 87 .41/90.31 0 .67 88 .24/91.72 61 .62 1 .76 64 .32 90 .07 0 .81 90 .75\nStandard LNSR 73.31 1.55 76.17 88.50/90.42 0 .56 90 .02/91.81 63 .35 1 .05 65.99 90.23 0 .31 90.97\nIn-manifold LNSR 74.33 1.78 78.70 87.92/89.21 0 .62 88 .97/92.31 63 .44 1 .01 65.30 90.32 0 .29 90.88\nTABLE VI: Ablation study of different fine-tuning methods on the selected GLUE benchmark datasets, we report the mean/max evaluation\nscores and standard deviation values across 25 random seeds.\nB. Ablation Study\nWe conduct ablation experiments on text classification tasks\nto further validate the mechanism of both the standard and\nIn-manifold LNSR methods. We compare our method with\nfine-tuning more epochs of the BERT model and injecting noise\nwithout an explicit regularization (we add Gaussian/In-manifold\nnoise to the intermediate representation of a BERT layer in the\nforward propagation process without explicitly regularizing the\nnoise stability), respectively. The results are in Table VI. We\nobserve that the performance gains brought by more training\nepochs are less significant. Meanwhile, fine-tuning by only\nimposing perturbations can not bring satisfying improvement\nin performance too. However, fine-tuning with our proposed\nLNSR methods achieves satisfying results on every task, which\nadds to the mounting evidence that fine-tuning with LNSR can\neffectively improve the generalizability of language models.\nC. Sensitivity to the Position of Noise Injection in LNSR\nAs illustrated in Figure 1, the performance of BERT fine-\ntuning is sensitive to the layer of noise injection. So we\ninvestigate the impact of different positions of noise injection\non BERT fine-tuning. For comparison, we inject noise into\ndifferent layers of the BERT model and regularize the noise\nstability item. According to the experimental results shown in\nFigure 8 in Appendix C, we can conclude that all injection\npositions bring significant improvements over vanilla fine-\ntuning. Particularly, noise injected into the lower layers usually\nbrings more performance gains of the BERT fine-tuning, which\nindicates that lower-layer LNSR may be more effective as it\ninfluences more layers (i.e. parameters).\nD. Sensitivity to the Mix-ratio of In-manifold LNSR\nTo investigate the influence of the mix-ratio of In-manifold\nLNSR on language model fine-tuning, we conduct experiments\non RoBERTa with different scales of In-manifold noise on\ndifferent text classification tasks. To avoid catastrophic changes\nin the embedding layer, the noise scale should be reasonably\nrestricted. Specifically, we evaluate choices of {0.10, 0.12,\n0.15, 0.20 }. As we can see from Table V , the performance\nof In-manifold LNSR fine-tuning is affected by both the\nscale of datasets and the scale of noise mix-ratio. For the\nRTE dataset with 2.5K training samples, we obtain the best\nmean/max value of 82.10/84.11 under the mix-ratio of 0.1,\nwhile larger mix-ratios can lead to an unstable and even\ncollapsed fine-tuning process. But on larger datasets like MRPC,\nCoLA, and STS-B (which have 3.7K, 8.5K, and 7k training\nexamples, respectively), larger mix ratios can bring more\nabsolute performance gains.\nIt can be concluded that, under a reasonable restriction, the\nbest choice of the noise magnitude (at least partially) depends\non the scale of training data. For a larger training set, our\nmethod tends to achieve higher performance with larger noise\nmagnitudes. However, when fine-tuning on smaller datasets,\nwe suggest smaller noise magnitudes to prevent unexpected\nrepresentation collapse.\nE. Relationship to Existing Relevant Work\nOur algorithm is relevant to the noise-based methods\nincluding SMART [4], FreeLB [3] and R3F [57], most of\nwhich focus on the robustness of few-sample fine-tuning\nthrough a fashion of adversarial training. Specifically, SMART\nuses the gradient ascent method to learn a noise constrained\nwithin an ϵ-ball and then minimize the distributional differ-\nence between the original and the perturbed representations.\nFreeLB proposes to directly minimize the adversarial loss\nLFreeLB (θ) = sup∆θ:|∆θ|≤ϵL(θ + ∆θ), implemented by\niterative gradient updates. R3F improves the efficiency by\nremoving the procedure of adversarial optimizing in SMART\nand proposes to directly improve the smoothness.\nCompared with this type of adversarial training-based\nalgorithm, our LNSR not only simplifies the process of noise\nperturbation and reduces the computing complexity as analyzed\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\nin Section III (F), but also enjoys additional properties which\nare essential to model generalization.\nFirst, while adversarial-example-based methods are moti-\nvated by worst-case robustness w.r.t. small perturbations on\ninput data, our approach is more directly associated with\nstatistical learning principles, i.e., the generalization bound.\nSpecifically, [25] figured out noise stability as a computation-\ntractable metric to bound the generalization error. Moreover, our\napproach has implicit equivalence to the Tikhonov regularizer,\nwhich is widely applied to shallow models for controlling\nmodel complexity.\nSecond, our method is expected to better simulate real-world\ndata noise compared to adversarial noise. Previous research [58]\nhas confirmed these two kinds of robustness, i.e., to natural\nnoise and adversarial noise, are fundamentally conflicting.\nGiven that our work aims at natural scenarios with random\ndata noise rather than artificial adversarial noise, the proposed\nLNSR is a more reasonable solution.\nF . Comparison of Gaussian Noise and In-manifold Noise\nTo further validate the impact of In-manifold noise, we\nconduct spectral decomposition analysis. Specifically, we\nperform Principal Components Analysis (PCA) on a batch of\nnoise vectors sampled by the standard Gaussian distribution and\nthe In-manifold strategy, respectively. As shown in Figure 2,\ninformally, the sampled In-manifold noise has very limited\nfreedom of direction, if data points actually lie on a low-\ndimensional manifold. Therefore, such a batch of noise vectors\ncould be characterized by only a few major directions. The\nPCA eigenvalue distribution in Figure 6 shows a remarkable\ndifference between the standard and In-manifold Gaussian\nnoise. For the In-manifold noise, almost all information is\ncompressed on the top of a few eigenvectors, indicating its low\nactual dimensionality. In contrast, the standard Gaussian noise\nhas a relatively smooth distribution of eigenvalues. Our result\nis consistent with recent studies about the embedding space of\nBERT, e.g., [59] points out that the word embedding in BERT\nusually has a local intrinsic dimension of less than 10.\nFig. 6: PCA eigenvalues of Gaussian Noise and In-manifold Noise.\nEigenvalues are sorted in a descending order. For comparison, we\nnormalize the area under the eigenvalue curve by dividing all\neigenvalues by their sum. The top plot shows all eigenvalues. The\nbottom plot zooms in the index interval between 10 and 100.\nVI. R ELATED WORK\nA. Pre-training\nPre-training technology [21], [60], [61] has orchestrated\ntremendous progress in the natural language processing area\nin the past few years. In early NLP works, pre-training mainly\nfocuses on using distributional representations (i.e., word\nembeddings) for individual words [62], [63]. Furthermore,\nDai et al. [64] propose to first train a general language\nmodel using a self-supervised learning method and then adapt\nthe obtained language model to downstream tasks. In recent\nyears, large-scale pre-trained language models, such as ELMo\n[65], GPT/GPT-2/GPT-3 [66]–[68], BERT [21], XLNet [69],\nRoBERTa [50], ELECTRA [9], T5 [11], etc. have achieved\ntremendous success in NLP due to the powerful ability to\ncontextual representation and transferability to downstream\ntasks. In a typical pre-training and fine-tuning paradigm,\nlanguage models are first pre-trained on a large amount of\nunlabeled data (e.g., common crawl, C4) to capture rich\nsemantic information of natural languages, and then adapted to\nthe downstream tasks using labeled datasets [21]. The general\nparadigm of pre-training and fine-tuning has also been proven\neffective on specific tasks and domains [70], [71].\nB. The Fragility of Language Model Fine-tuning\nThe phenomenon of instability of PLMs fine-tuning was first\nreported by Devlin et al. [21]. A further study [22] reveals\nthe sensitivity of BERT fine-tuning to random seeds where the\nrandomness is introduced by the shuffle of data order and the\nrandom initialization of the task-specific layer by conducting\nextensive empirical analysis. Inspired by the above experimental\nanalysis, several new methods have been proposed to mitigate\nthe fragility of language model fine-tuning. Mixout [2] replaces\nparameters with their pre-trained value with a probability p\nin the fine-tuning phase to promote both the stability and\nperformance of the BERT model. Zhang et al. [72] figure\nout that debiasing the Adam optimizer is beneficial for BERT\nfine-tuning [55] through experiments and point out that re-\ninitialize some top layers of a BERT model contributes to\nbetter generalization to the downstream tasks. Mosbach et\nal. [73] discuss the reason for the instability of fine-tuning via\nextensive experiments and suggest using a small learning rate\nas well as bias correction to improve the generalizability of\nlanguage model fine-tuning.\nC. Regularization\nRegularization is a widely adopted method for improving\nthe performance of deep neural networks. In transfer learning,\nthe most common problems are overfitting and catastrophic\nforgetting, while a regularization item can help mitigate these\nissues. Several regularization methods have been proposed to\nimprove the performance of PLMs’ fine-tuning. Loshchilov\nand Hutter [74] propose a decoupled weight decay regularizer\nintegrated with Adam [55] optimizer to prevent neural networks\nfrom being too complicated. In addition, spectral-norm-based\nregularization methods [75], [76] can be regarded as a general\nmethod to constrain the Lipschitz continuity of neural networks\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13\nthat can help to improve the smoothness of the learned neural\nnetworks. In addition, [77] proposed to roll back pre-trained\nweights as an implicit form of regularization to pursue flatter\nlocal minima for fine-tuning.\nRecently, several approaches relevant to input noise have\nemerged to improve the local Lipschitz continuity of language\nmodels and hence improve the smoothness and generalizability\nof the fine-tuned model. These algorithms usually minimize\nthe maximum risk caused by noise perturbations within a norm\nball. Such approaches include SMART [4], FreeLB [3] and\nR3F [57]. They achieve state-of-the-art performance on GLUE,\nSciTail [78], and LAMA [15], [16], etc. NLU benchmarks.\nVII. C ONCLUSION\nIn this paper, we investigate the problem of fine-tuning\npre-trained language models from the perspective of noise\nstability. We introduce a lightweight and effective framework,\nnamed Layerwise Noise Stability Regularization (LNSR), to\nimprove generalizability and stability when fine-tuning pre-\ntrained language models on a few training samples. In our\nproposed LNSR framework, two alternative noise sampling\nstrategies are used, which are the standard Gaussian noise and\nIn-manifold noise. Our proposed LNSR methods are general\ntechniques that promote the smoothness of language models\nand thus improve the model’s performance. Furthermore, we\ntheoretically analyze the properties of our proposed model\nconnected to the Lipschitz continuity and Tikhonov regularizer.\nIn addition, the experimental results in this paper also reflect\nthe effectiveness of our proposed method to improve the\ngeneralizability and stability of pre-trained language models.\nACKNOWLEDGMENT\nWe would like to thank the Jeffries Data Science Fellowship\nfor supporting Hang Hua’s research.\nREFERENCES\n[1] X. Li, Y . Grandvalet, and F. Davoine, “Explicit inductive bias for transfer\nlearning with convolutional networks,” in ICML, 2018.\n[2] C. Lee, K. Cho, and W. Kang, “Mixout: Effective regularization to fine-\ntune large-scale pretrained language models,” ArXiv, vol. abs/1909.11299.\n[3] C. Zhu, Y . Cheng, Z. Gan, S. Sun, T. Goldstein, and J. jing Liu, “Freelb:\nEnhanced adversarial training for natural language understanding,” arXiv:\nComputation and Language , 2020.\n[4] H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and T. Zhao, “Smart: Robust\nand efficient fine-tuning for pre-trained natural language models through\nprincipled regularized optimization,” in ACL, 2020.\n[5] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang, “Realm: Retrieval-\naugmented language model pre-training,” ArXiv, vol. abs/2002.08909,\n2020.\n[6] Y . Liu, “Fine-tune bert for extractive summarization,” ArXiv, vol.\nabs/1903.10318, 2019.\n[7] D. Wadden, U. Wennberg, Y . Luan, and H. Hajishirzi, “Entity, relation,\nand event extraction with contextualized span representations,” in\nEMNLP/IJCNLP, 2019.\n[8] J. Zhu, Y . Xia, L. Wu, D. He, T. Qin, W. Zhou, H. Li, and T. Liu, “Incor-\nporating bert into neural machine translation,” ArXiv, vol. abs/2002.06823,\n2020.\n[9] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning, “Electra: Pre-\ntraining text encoders as discriminators rather than generators,” arXiv\npreprint arXiv:2003.10555, 2020.\n[10] M. Joshi, D. Chen, Y . Liu, D. S. Weld, L. Zettlemoyer, and O. Levy,\n“Spanbert: Improving pre-training by representing and predicting spans,”\nTransactions of the Association for Computational Linguistics , 2020.\n[11] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer\nlearning with a unified text-to-text transformer,” Journal of Machine\nLearning Research, vol. 21, no. 140, pp. 1–67, 2020. [Online]. Available:\nhttp://jmlr.org/papers/v21/20-074.html\n[12] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\nV . Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehen-\nsion,” arXiv preprint arXiv:1910.13461 , 2019.\n[13] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n“Glue: A multi-task benchmark and analysis platform for natural language\nunderstanding,” ArXiv, vol. abs/1804.07461, 2018.\n[14] A. Wang, Y . Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\nO. Levy, and S. R. Bowman, “Superglue: A stickier benchmark\nfor general-purpose language understanding systems,” ArXiv, vol.\nabs/1905.00537, 2019.\n[15] F. Petroni, T. Rockt ¨aschel, P. Lewis, A. Bakhtin, Y . Wu, A. H. Miller,\nand S. Riedel, “Language models as knowledge bases?” arXiv preprint\narXiv:1909.01066, 2019.\n[16] F. Petroni, P. Lewis, A. Piktus, T. Rockt ¨aschel, Y . Wu, A. H. Miller, and\nS. Riedel, “How context affects language models’ factual predictions,”\narXiv preprint arXiv:2005.04611 , 2020.\n[17] G. Lample and A. Conneau, “Cross-lingual language model pretraining,”\narXiv preprint arXiv:1901.07291 , 2019.\n[18] Y . Hu, H. Hua, Z. Yang, W. Shi, N. A. Smith, and J. Luo, “Prompt-\ncap: Prompt-guided task-aware image captioning,” arXiv preprint\narXiv:2211.09699, 2022.\n[19] J. Lin, H. Hua, M. Chen, Y . Li, J. Hsiao, C. Ho, and J. Luo, “Videoxum:\nCross-modal visual and textural summarization of videos,” arXiv preprint\narXiv:2303.12060, 2023.\n[20] J. Zhang, H. Zhang, C. Xia, and L. Sun, “Graph-bert: Only atten-\ntion is needed for learning graph representations,” arXiv preprint\narXiv:2001.05140, 2020.\n[21] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of\ndeep bidirectional transformers for language understanding,” in NAACL-\nHLT, 2019.\n[22] J. Dodge, G. Ilharco, R. Schwartz, A. Farhadi, H. Hajishirzi, and N. A.\nSmith, “Fine-tuning pretrained language models: Weight initializations,\ndata orders, and early stopping,” ArXiv, vol. abs/2002.06305, 2020.\n[23] C. M. Bishop, “Training with noise is equivalent to tikhonov regulariza-\ntion,” Neural computation, vol. 7, no. 1, pp. 108–116, 1995.\n[24] S. Rifai, X. Glorot, Y . Bengio, and P. Vincent, “Adding noise to the\ninput of a model trained with a regularized objective,” arXiv preprint\narXiv:1104.3250, 2011.\n[25] S. Arora, R. Ge, B. Neyshabur, and Y . Zhang, “Stronger generalization\nbounds for deep nets via a compression approach,” in International\nConference on Machine Learning . PMLR, 2018, pp. 254–263.\n[26] X. Dong, A. T. Luu, M. Lin, S. Yan, and H. Zhang, “How should pre-\ntrained language models be fine-tuned towards adversarial robustness?”\nAdvances in Neural Information Processing Systems , vol. 34, pp. 4356–\n4369, 2021.\n[27] H. Hua, X. Li, D. Dou, C.-Z. Xu, and J. Luo, “Noise stability regular-\nization for improving bert fine-tuning,” arXiv preprint arXiv:2107.04835,\n2021.\n[28] X. Li, H. Hang, C. Xu, and D. Dou, “Method and apparatus for transfer\nlearning,” Dec. 15 2022, uS Patent App. 17/820,321.\n[29] H. Federer et al., “Geometric measure theory,” 1996.\n[30] J. Sietsma and R. J. Dow, “Creating artificial neural networks that\ngeneralize,” Neural networks, vol. 4, no. 1, pp. 67–79, 1991.\n[31] A. N. Tikhonov and V . Y . Arsenin, “Solutions of ill-posed problems,”\nNew York, vol. 1, no. 30, p. 487, 1977.\n[32] S. T. Roweis and L. K. Saul, “Nonlinear dimensionality reduction by\nlocally linear embedding,” science, vol. 290, no. 5500, pp. 2323–2326,\n2000.\n[33] T. Lin and H. Zha, “Riemannian manifold learning,” IEEE Transactions\non Pattern Analysis and Machine Intelligence, vol. 30, no. 5, pp. 796–809,\n2008.\n[34] X. Huo, X. S. Ni, and A. K. Smith, “A survey of manifold-based learning\nmethods,” Recent advances in data mining of enterprise data , pp. 691–\n745, 2007.\n[35] L. Cayton, “Algorithms for manifold learning,” Univ. of California at\nSan Diego Tech. Rep , vol. 12, no. 1-17, p. 1, 2005.\n[36] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.” Journal\nof machine learning research , vol. 9, no. 11, 2008.\n[37] T. Van Laarhoven, “L2 regularization versus batch and weight normal-\nization,” arXiv preprint arXiv:1706.05350 , 2017.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14\n[38] G. Zhang, C. Wang, B. Xu, and R. Grosse, “Three mechanisms of\nweight decay regularization,” in International Conference on Learning\nRepresentations, 2018.\n[39] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,\nand R. Fergus, “Intriguing properties of neural networks,” in 2nd\nInternational Conference on Learning Representations, ICLR 2014 , 2014.\n[40] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards\ndeep learning models resistant to adversarial attacks,” in International\nConference on Learning Representations , 2018.\n[41] A. Warstadt, A. Singh, and S. R. Bowman, “Neural network accept-\nability judgments,” Transactions of the Association for Computational\nLinguistics, vol. 7, pp. 625–641, 2019.\n[42] B. W. Matthews, “Comparison of the predicted and observed secondary\nstructure of t4 phage lysozyme,” Biochimica et Biophysica Acta (BBA)-\nProtein Structure, vol. 405, no. 2, pp. 442–451, 1975.\n[43] W. Dolan and C. Brockett, “Automatically constructing a corpus of\nsentential paraphrases,” in IWP@IJCNLP, 2005.\n[44] I. Dagan, O. Glickman, and B. Magnini, “The pascal recognising textual\nentailment challenge,” in MLCW, 2005.\n[45] R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo, and\nB. Magnini, “The second pascal recognising textual entailment challenge.”\n[46] D. Giampiccolo, B. Magnini, I. Dagan, and W. Dolan, “The third pascal\nrecognizing textual entailment challenge,” in ACL-PASCAL@ACL, 2007.\n[47] D. M. Cer, M. T. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia,\n“Semeval-2017 task 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation,” ArXiv, vol. abs/1708.00055, 2017.\n[48] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\nquestions for machine comprehension of text,” arXiv preprint\narXiv:1606.05250, 2016.\n[49] A. Fisch, A. Talmor, R. Jia, M. Seo, E. Choi, and D. Chen, “Mrqa 2019\nshared task: Evaluating generalization in reading comprehension,” arXiv\npreprint arXiv:1910.09753, 2019.\n[50] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\npretraining approach,” ArXiv, vol. abs/1907.11692, 2019.\n[51] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov, “Dropout: a simple way to prevent neural networks from\noverfitting,” J. Mach. Learn. Res. , vol. 15, pp. 1929–1958, 2014.\n[52] L. Wan, M. D. Zeiler, S. Zhang, Y . LeCun, and R. Fergus, “Regularization\nof neural networks using dropconnect,” in ICML, 2013.\n[53] C. Yang and X. Ma, “Improving stability of fine-tuning pretrained\nlanguage models via component-wise gradient norm clipping,” arXiv\npreprint arXiv:2210.10325, 2022.\n[54] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen,\nC. Ma, Y . Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame,\nQ. Lhoest, and A. M. Rush, “Huggingface’s transformers: State-of-the-\nart natural language processing,” ArXiv, vol. abs/1910.03771, 2019.\n[55] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nCoRR, vol. abs/1412.6980, 2015.\n[56] J. Johnson, M. Douze, and H. J ´egou, “Billion-scale similarity search\nwith gpus,” arXiv preprint arXiv:1702.08734 , 2017.\n[57] A. Aghajanyan, A. Shrivastava, A. Gupta, N. Goyal, L. Zettlemoyer,\nand S. Gupta, “Better fine-tuning by reducing representational collapse,”\nArXiv, vol. abs/2008.03156, 2020.\n[58] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry,\n“Robustness may be at odds with accuracy,” in International Conference\non Learning Representations , 2019.\n[59] X. Cai, J. Huang, Y . Bian, and K. Church, “Isotropy in the contextual\nembedding space: Clusters and manifolds,” in International Conference\non Learning Representations , 2020.\n[60] D. Erhan, P.-A. Manzagol, Y . Bengio, S. Bengio, and P. Vincent, “The\ndifficulty of training deep architectures and the effect of unsupervised\npre-training,” in AISTATS, 2009.\n[61] D. Erhan, A. C. Courville, Y . Bengio, and P. Vincent, “Why does\nunsupervised pre-training help deep learning?” J. Mach. Learn. Res. ,\nvol. 11, pp. 625–660, 2010.\n[62] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors\nfor word representation,” in EMNLP, 2014.\n[63] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Dis-\ntributed representations of words and phrases and their compositionality,”\nArXiv, vol. abs/1310.4546, 2013.\n[64] A. M. Dai and Q. V . Le, “Semi-supervised sequence learning,” in NIPS,\n2015.\n[65] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and\nL. Zettlemoyer, “Deep contextualized word representations,” ArXiv, vol.\nabs/1802.05365, 2018.\n[66] A. Radford, “Improving language understanding by generative pre-\ntraining,” 2018.\n[67] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” 2019.\n[68] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models\nare few-shot learners,” Advances in neural information processing systems,\nvol. 33, pp. 1877–1901, 2020.\n[69] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le,\n“Xlnet: Generalized autoregressive pretraining for language understanding,”\nin NeurIPS, 2019.\n[70] Z. Wen, S.-C. Fuh, and A. Mircea, “Neurips 2019 reproducibility\nchallenge: Controllable unsupervised text attribute transfer via\nediting entangled latent representation,” 2019. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:211094926\n[71] Y . Zhou, L. Liao, Y . Gao, R. Wang, and H. Huang, “Topicbert: A topic-\nenhanced neural language model fine-tuned for sentiment classification,”\nIEEE Transactions on Neural Networks and Learning Systems , 2021.\n[72] T. Zhang, F. Wu, A. Katiyar, K. Q. Weinberger, and Y . Artzi, “Revisiting\nfew-sample bert fine-tuning,” ArXiv, vol. abs/2006.05987, 2020.\n[73] M. Mosbach, M. Andriushchenko, and D. Klakow, “On the stability\nof fine-tuning bert: Misconceptions, explanations, and strong baselines,”\nArXiv, vol. abs/2006.04884, 2020.\n[74] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” in\nICLR, 2019.\n[75] Y . Yoshida and T. Miyato, “Spectral norm regularization for improving\nthe generalizability of deep learning,” ArXiv, vol. abs/1705.10941, 2017.\n[76] K. Roth, Y . Kilcher, and T. Hofmann, “Adversarial training generalizes\ndata-dependent spectral norm regularization,” ArXiv, vol. abs/1906.01527,\n2019.\n[77] Y . Ro, J. Choi, B. Heo, and J. Y . Choi, “Rollback ensemble with multiple\nlocal minima in fine-tuning deep learning networks,” IEEE transactions\non neural networks and learning systems , vol. 33, no. 9, pp. 4648–4660,\n2021.\n[78] T. Khot, A. Sabharwal, and P. Clark, “Scitail: A textual entailment dataset\nfrom science question answering,” in AAAI, 2018.\n[79] G. Tsatsaronis, G. Balikas, P. Malakasiotis, I. Partalas, M. Zschunke, M. R.\nAlvers, D. Weissenborn, A. Krithara, S. Petridis, D. Polychronopoulos\net al. , “An overview of the bioasq large-scale biomedical semantic\nindexing and question answering competition,” BMC bioinformatics ,\nvol. 16, no. 1, pp. 1–28, 2015.\n[80] D. Dua, Y . Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner,\n“Drop: A reading comprehension benchmark requiring discrete reasoning\nover paragraphs,” arXiv preprint arXiv:1903.00161 , 2019.\n[81] A. Saha, R. Aralikatte, M. M. Khapra, and K. Sankaranarayanan, “Duorc:\nTowards complex language understanding with paraphrased reading\ncomprehension,” arXiv preprint arXiv:1804.07927 , 2018.\n[82] G. Lai, Q. Xie, H. Liu, Y . Yang, and E. Hovy, “Race: Large-scale\nreading comprehension dataset from examinations,” arXiv preprint\narXiv:1704.04683, 2017.\n[83] O. Levy, M. Seo, E. Choi, and L. Zettlemoyer, “Zero-shot relation\nextraction via reading comprehension,” arXiv preprint arXiv:1706.04115,\n2017.\n[84] A. Kembhavi, M. Seo, D. Schwenk, J. Choi, A. Farhadi, and H. Hajishirzi,\n“Are you smarter than a sixth grader? textbook question answering\nfor multimodal machine comprehension,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern recognition , 2017.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15\nAPPENDIX A\nNOISE STABILITY OF DEEP CNN S\nHere we demonstrate that deep CNNs are much more\nresilient to injected noise than transformer-based architectures\nsuch as BERT. An example on VGG-19 is shown in Figure 7.\nThis figure comes from [25].\nFig. 7: Attenuation of injected noise on a VGG-19 net trained on\nCIFAR-10. The x-axis is the index of layers and the y-axis denotes\nthe relative error introduced by the noise ( ∥ˆxi −xi∥2/∥xi∥2). A curve\nstarts at the layer where a scaled Gaussian noise is injected into its\ninput, whose l2 norm is set to 10% of the norm of its original input.\nAs it propagates up, the injected noise has a rapidly decreasing effect\non higher layers.\nAPPENDIX B\nINTRODUCTION FOR THE EXPERIMENTAL DATASETS\nA. The Selected Text Classification Tasks\nRTE MRPC CoLA STS-B\nTask NLI Paraphrase Acceptability Similarity\nMetrics Accuracy Accuracy/F1 Matthews Corr Pearson+Spearman corr\n2#of labels 2 2 2 1\n#of training samples 2.5k 3.7k 8.6k 7k\n#of validation samples 276 408 1k 1.5k\n#of test samples 3k 1.7k 1k 1.4k\nTABLE VII: The summarization of the selected GLUE benchmark\ntasks used in this work.\nB. Out-of-domain Question Answering Tasks\nBioASQ [79] is a large-scale semantic indexing and question\nanswering dataset in the biomedical domain, all the question,\nand answer pairs are created by domain experts.\nDROP [80] examples are collected similarly to SQuAD,\nwhere the question-answer pairs are constructed from Wikipedia\nparagraphs by crowd workers. Different from SQuAD, the\nquestions of DROP mainly focus on quantitative reasoning.\nBesides, DROP contains non-extractive numeric answers as\nwell as extractive text answers.\nDuoRC [81] is a paraphrase-based reading comprehension\ndataset. It contains 186089 QA pairs created from a collection\nof paraphrased movie plots. The main challenge of this task is\nthat it requires models to go beyond the content of the given\npassage and incorporate different kinds of knowledge to arrive\nat the answer.\nRACE [82] is a machine reading comprehension dataset\nthat is collected from English reading comprehension exams\nfor middle and high school students in China.\nRelationExtraction [83] is a multi-turn question answering\ntask which is built from relation extraction datasets (e.g.\nACE04, ACE05, and CoNLL04). The entities and relations in\nthe relation extraction datasets are transformed into question-\nanswer pairs using templates.\nTextbookQA [84] is a QA dataset drawn from middle school\nscience curricula. It contains 12567 questions in total.\nAPPENDIX C\nOTHER EXPERIMENTAL ANALYSIS\nIn this section, we show the effects of the position of\nGaussian noise injection on models’ performance. The inter-\npretation of Figure 8 is described in section 5.3. In addition,\nTable VIII shows the mean training/evaluation accuracy and\ngeneralization gap of different methods on each task. We can\nconclude that fine-tuning with LNSR and In-manifold LNSR\ncan effectively narrow the generalization gap and help improve\nthe performance.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16\nFig. 8: Performance distribution of the BERT model with different noise injection positions across 25 random seeds.\nRTE MRPC CoLA STS-B\nBERTLARGE train dev gap ↓ train dev gap ↓ train dev gap ↓ train dev gap ↓\nFT [21] 95.89 70.13 25 .76 96 .57 87 .57 9 .00 97.71 61.56 36 .25 98 .31 89 .38 8 .93\nStandard LNSR 90.72 73 .31 17 .41 96 .68 88 .50 8.18 93.44 63 .35 30.09 98 .45 90.23 8 .22\nIn-manifold LNSR 89.52 75.09 14 .43 98 .42 88 .85 9.57 96 .01 63.64 32.65 96 .77 91.05 5 .75\nTABLE VIII: Comparison of the generalizability performance of different models. We report the mean training/evaluation Acc and the\ngeneralizability gap (training Acc - evaluation Acc) of each model across 20 random seeds.",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.8284366726875305
    },
    {
      "name": "Computer science",
      "score": 0.766503095626831
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.7235325574874878
    },
    {
      "name": "Generalizability theory",
      "score": 0.6678279638290405
    },
    {
      "name": "Language model",
      "score": 0.566917896270752
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5594369769096375
    },
    {
      "name": "Machine learning",
      "score": 0.4755946099758148
    },
    {
      "name": "Early stopping",
      "score": 0.4350908100605011
    },
    {
      "name": "Stability (learning theory)",
      "score": 0.4286026060581207
    },
    {
      "name": "Noise (video)",
      "score": 0.41726359724998474
    },
    {
      "name": "Gaussian",
      "score": 0.41514936089515686
    },
    {
      "name": "Artificial neural network",
      "score": 0.34522491693496704
    },
    {
      "name": "Mathematics",
      "score": 0.1911807656288147
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5388228",
      "name": "University of Rochester",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I204512498",
      "name": "University of Macau",
      "country": "MO"
    }
  ]
}