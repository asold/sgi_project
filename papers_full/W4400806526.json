{
  "title": "Data Stealing Attacks against Large Language Models via Backdooring",
  "url": "https://openalex.org/W4400806526",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2164892045",
      "name": "Jiaming He",
      "affiliations": [
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2136049742",
      "name": "Guanyu Hou",
      "affiliations": [
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2549238425",
      "name": "Xinyue Jia",
      "affiliations": [
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095593066",
      "name": "Yangyang Chen",
      "affiliations": [
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2323371746",
      "name": "Wenqi Liao",
      "affiliations": [
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2407112470",
      "name": "Yin-Hang Zhou",
      "affiliations": [
        "Shenyang Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2434858654",
      "name": "Rang Zhou",
      "affiliations": [
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2164892045",
      "name": "Jiaming He",
      "affiliations": [
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2136049742",
      "name": "Guanyu Hou",
      "affiliations": [
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2549238425",
      "name": "Xinyue Jia",
      "affiliations": [
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095593066",
      "name": "Yangyang Chen",
      "affiliations": [
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2323371746",
      "name": "Wenqi Liao",
      "affiliations": [
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2407112470",
      "name": "Yin-Hang Zhou",
      "affiliations": [
        "Shenyang Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2434858654",
      "name": "Rang Zhou",
      "affiliations": [
        "Chengdu University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4393156804",
    "https://openalex.org/W4393141148",
    "https://openalex.org/W4392903122",
    "https://openalex.org/W4401171110",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2969695741",
    "https://openalex.org/W3010489274",
    "https://openalex.org/W3046853140",
    "https://openalex.org/W4386083011",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4224308101"
  ],
  "abstract": "Large language models (LLMs) have gained immense attention and are being increasingly applied in various domains. However, this technological leap forward poses serious security and privacy concerns. This paper explores a novel approach to data stealing attacks by introducing an adaptive method to extract private training data from pre-trained LLMs via backdooring. Our method mainly focuses on the scenario of model customization and is conducted in two phases, including backdoor training and backdoor activation, which allow for the extraction of private information without prior knowledge of the modelâ€™s architecture or training data. During the model customization stage, attackers inject the backdoor into the pre-trained LLM by poisoning a small ratio of the training dataset. During the inference stage, attackers can extract private information from the third-party knowledge database by incorporating the pre-defined backdoor trigger. Our method leverages the customization process of LLMs, injecting a stealthy backdoor that can be triggered after deployment to retrieve private data. We demonstrate the effectiveness of our proposed attack through extensive experiments, achieving a notable attack success rate. Extensive experiments demonstrate the effectiveness of our stealing attack in popular LLM architectures, as well as stealthiness during normal inference.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6525705456733704
    },
    {
      "name": "Computer security",
      "score": 0.5357598066329956
    },
    {
      "name": "Programming language",
      "score": 0.38033968210220337
    }
  ]
}