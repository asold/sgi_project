{
  "title": "Advantages of transformer and its application for medical image segmentation: a survey",
  "url": "https://openalex.org/W4391514919",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2129919306",
      "name": "Qiumei Pu",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A3207103540",
      "name": "Zuoxin Xi",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of High Energy Physics",
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2067286940",
      "name": "Shuai Yin",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2090817536",
      "name": "Zhe Zhao",
      "affiliations": [
        "Chinese PLA General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2107039470",
      "name": "Lina Zhao",
      "affiliations": [
        "Institute of High Energy Physics",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2129919306",
      "name": "Qiumei Pu",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A3207103540",
      "name": "Zuoxin Xi",
      "affiliations": [
        "Institute of High Energy Physics",
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2067286940",
      "name": "Shuai Yin",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2090817536",
      "name": "Zhe Zhao",
      "affiliations": [
        "Chinese PLA General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2107039470",
      "name": "Lina Zhao",
      "affiliations": [
        "Institute of High Energy Physics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2171653019",
    "https://openalex.org/W2086144763",
    "https://openalex.org/W2115414021",
    "https://openalex.org/W2027382625",
    "https://openalex.org/W2117271072",
    "https://openalex.org/W4366401167",
    "https://openalex.org/W3199116259",
    "https://openalex.org/W2899279931",
    "https://openalex.org/W2963062226",
    "https://openalex.org/W3189766340",
    "https://openalex.org/W3161081823",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2607941059",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W4323357155",
    "https://openalex.org/W4387021831",
    "https://openalex.org/W4387778010",
    "https://openalex.org/W4382313309",
    "https://openalex.org/W4387356825",
    "https://openalex.org/W4386780135",
    "https://openalex.org/W2100495367",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W3014974815",
    "https://openalex.org/W6828894009",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2907750714",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W6600213211",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3097510987",
    "https://openalex.org/W3203841574",
    "https://openalex.org/W4212875960",
    "https://openalex.org/W3172681723",
    "https://openalex.org/W4221163766",
    "https://openalex.org/W3206685025",
    "https://openalex.org/W3158095128",
    "https://openalex.org/W3113231417",
    "https://openalex.org/W4200293227",
    "https://openalex.org/W3213290340",
    "https://openalex.org/W3204166336",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2921406441",
    "https://openalex.org/W3202542211",
    "https://openalex.org/W3201514487",
    "https://openalex.org/W3162418282",
    "https://openalex.org/W3119303959",
    "https://openalex.org/W3204255739",
    "https://openalex.org/W3199427219",
    "https://openalex.org/W3200379731",
    "https://openalex.org/W4200021102",
    "https://openalex.org/W4291110778",
    "https://openalex.org/W2910094941",
    "https://openalex.org/W2943666912",
    "https://openalex.org/W2610486821",
    "https://openalex.org/W3212933375",
    "https://openalex.org/W6603963165",
    "https://openalex.org/W2905016804",
    "https://openalex.org/W3204614423",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W4311088269",
    "https://openalex.org/W2928133111",
    "https://openalex.org/W3197957534",
    "https://openalex.org/W2288892845",
    "https://openalex.org/W2592905743",
    "https://openalex.org/W3204700807",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W4285104230",
    "https://openalex.org/W2131099349",
    "https://openalex.org/W2145305441",
    "https://openalex.org/W2150769593",
    "https://openalex.org/W2086843392",
    "https://openalex.org/W2045227075",
    "https://openalex.org/W2129098176",
    "https://openalex.org/W2980998394",
    "https://openalex.org/W2885343725",
    "https://openalex.org/W2828862258",
    "https://openalex.org/W2948685905",
    "https://openalex.org/W2955737766",
    "https://openalex.org/W3034655146",
    "https://openalex.org/W2997286550",
    "https://openalex.org/W3204650100",
    "https://openalex.org/W2106033751",
    "https://openalex.org/W2804047627",
    "https://openalex.org/W2980207657",
    "https://openalex.org/W3103010481",
    "https://openalex.org/W4200227736",
    "https://openalex.org/W4385064811",
    "https://openalex.org/W3116112801",
    "https://openalex.org/W2395611524",
    "https://openalex.org/W3112701542"
  ],
  "abstract": "Abstract Purpose Convolution operator-based neural networks have shown great success in medical image segmentation over the past decade. The U-shaped network with a codec structure is one of the most widely used models. Transformer, a technology used in natural language processing, can capture long-distance dependencies and has been applied in Vision Transformer to achieve state-of-the-art performance on image classification tasks. Recently, researchers have extended transformer to medical image segmentation tasks, resulting in good models. Methods This review comprises publications selected through a Web of Science search. We focused on papers published since 2018 that applied the transformer architecture to medical image segmentation. We conducted a systematic analysis of these studies and summarized the results. Results To better comprehend the benefits of convolutional neural networks and transformers, the construction of the codec and transformer modules is first explained. Second, the medical image segmentation model based on transformer is summarized. The typically used assessment markers for medical image segmentation tasks are then listed. Finally, a large number of medical segmentation datasets are described. Conclusion Even if there is a pure transformer model without any convolution operator, the sample size of medical picture segmentation still restricts the growth of the transformer, even though it can be relieved by a pretraining model. More often than not, researchers are still designing models using transformer and convolution operators.",
  "full_text": "Open Access\n© The Author(s) 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nREVIEW\nPu et al. BioMedical Engineering OnLine           (2024) 23:14  \nhttps://doi.org/10.1186/s12938-024-01212-4\nBioMedical Engineering\nOnLine\nAdvantages of transformer and its \napplication for medical image segmentation: \na survey\nQiumei Pu1, Zuoxin Xi1,2, Shuai Yin1, Zhe Zhao3 and Lina Zhao2* \nAbstract \nPurpose: Convolution operator-based neural networks have shown great suc-\ncess in medical image segmentation over the past decade. The U-shaped network \nwith a codec structure is one of the most widely used models. Transformer, a technol-\nogy used in natural language processing, can capture long-distance dependencies \nand has been applied in Vision Transformer to achieve state-of-the-art performance \non image classification tasks. Recently, researchers have extended transformer to medi-\ncal image segmentation tasks, resulting in good models.\nMethods: This review comprises publications selected through a Web of Science \nsearch. We focused on papers published since 2018 that applied the transformer archi-\ntecture to medical image segmentation. We conducted a systematic analysis of these \nstudies and summarized the results.\nResults: To better comprehend the benefits of convolutional neural networks \nand transformers, the construction of the codec and transformer modules is first \nexplained. Second, the medical image segmentation model based on transformer \nis summarized. The typically used assessment markers for medical image segmenta-\ntion tasks are then listed. Finally, a large number of medical segmentation datasets are \ndescribed.\nConclusion: Even if there is a pure transformer model without any convolution \noperator, the sample size of medical picture segmentation still restricts the growth \nof the transformer, even though it can be relieved by a pretraining model. More often \nthan not, researchers are still designing models using transformer and convolution \noperators.\nKeywords: Deep learning, Transformer, Medical image, Segmentation, Codec\nIntroduction\nMedical image segmentation is a significant study area in computer vision, to classify \nmedical pictures at the pixel level and then precisely segment the target item. Segmen -\ntation datasets are created from unimodal or multimodal pictures obtained by pro -\nfessional medical equipment such as magnetic resonance imaging (MRI), computed \ntomography (CT), and ultrasonography (US). Traditional nondeep learning medical \n*Correspondence:   \nlinazhao@ihep.ac.cn\n1 School of Information \nEngineering, Minzu University \nof China, Beijing 100081, China\n2 CAS Key Laboratory \nfor Biomedical Effects \nof Nanomaterials and Nanosafety \nInstitute of High Energy Physics, \nChinese Academy of Sciences, \nBeijing 100049, China\n3 The Fourth Medical Center \nof PLA General Hospital, \nBeijing 100039, China\nPage 2 of 22Pu et al. BioMedical Engineering OnLine           (2024) 23:14 \npicture segmentation approaches depend mostly on thresholding [1], region growth [2], \nborder detection [3], and other techniques. To produce superior segmentation results, \npicture features must be manually extracted before segmentation. The feature extraction \nmethods for various datasets are frequently diverse, and some professional experience \nis necessary [4–6]. The deep learning-based segmentation approach can automatically \nlearn the feature that represents the picture, but it requires a high-performance com -\nputer and takes a long time to train the network.\nWith the continual advancement of computer equipment such as Graphic Process -\ning Units (GPU) in recent years, training most deep learning models is no longer con -\nstrained. At present, the segmentation model-based convolutional neural network \n(CNN) is extensively employed in a variety of medical picture segmentation applica -\ntions [7, 8], including tumor segmentation [9], skin lesion region segmentation [10], left \nand right ventricular segmentation [11], and fundus blood vessel segmentation [12]. \nU-Net [13] is one of the most extensively utilized models. Through skip connections, \nU-Net integrates the multiscale detail information in the picture downsampling pro -\ncess with the global properties of low-resolution images. This encoder–decoder design, \nwhich combines information at multiple scales, considerably enhances segmentation \nmodel performance and is frequently utilized in the field of medical picture segmenta -\ntion. However, CNN can only employ very tiny convolution kernels to balance model \naccuracy and computational complexity, limiting it to a relatively restricted perceptual \ndomain. It excels at obtaining local characteristics but falls short of capturing long-\ndistance dependencies. Similar to domains such as autonomous driving, satellite image \nanalysis, and pedestrian recognition, medical image analysis also encounter challenges \nlike unclear boundaries [14], low contrast, varying object sizes, and complex patterns. \nAddressing these challenges often hinges on incorporating a broader contextual per -\nspective, encompassing global background information.\nThrough the self-attention process, the popular transformer [15] in machine transla -\ntion and sentiment analysis may gather global context information. Following the suc -\ncessful application of pure transformer architecture to the field of computer vision by \nViT [16], an increasing number of transformer-based models have been developed to \noptimize medical picture segmentation approaches (Fig.  1).  We analyzed articles pub -\nlished in the last 5 years on web of science using two sets of keywords, as shown in Fig. 2. \nThe first set of keywords included ’medical image’ and ’segmentation, ’ while the second \nset consisted of ’medical image, ’ ’segmentation, ’ and ’transformer. ’ As depicted in Fig. 2a, \nmedical image segmentation has consistently remained a prominent research area, \nwith nearly 5000 publications each year. The introduction of Vision Transformer (ViT) \nin 2020 marked the beginning of increased interest in using transformers for medical \nimage segmentation, leading to rapid growth. The number of articles surged by more \nthan 400%, particularly in 2021 and 2022. The finding from Fig. 2b also demonstrates the \ngrowing proportion of the second group, which is a subset of the first group of literature. \nThese statistical findings underscore the significant potential of transformers in the field \nof medical image segmentation.\nCurrently, several review articles have summarized literature related to Transform -\ners in the field of medical image segmentation. However, these reviews are often con -\ntext-specific, focusing on different medical applications, such as categorization based \nPage 3 of 22\nPu et al. BioMedical Engineering OnLine           (2024) 23:14 \n \non disease types [17], task-oriented summaries [18, 19], or aggregations based on spe -\ncific medical images or diseases [20– 22]. The synthesis and categorization based on \nnetwork structures are crucial for optimizing deep learning models for diverse tasks, \nyet research in this domain is currently limited. This paper explores recent advance -\nments in research on medical image segmentation tasks using transformer and \nencoder–decoder structural models. It provides a comprehensive study and analysis \nof relevant deep learning network structures, aiming to further uncover the potential \nof transformer and encoder–decoder structural models in medical image segmenta -\ntion tasks. The objective is to guide researchers in designing and optimizing network \nstructures for practical applications.\nFig. 1 Combining CNN with Transformer improves various medical image segmentation tasks\nFig. 2 Using web of science to retrieve and statistically analyze literature. a Statistics of literature quantity for \ntwo sets of keywords. b The proportion of literature related to transformers in medical image segmentation \nliterature\nPage 4 of 22Pu et al. BioMedical Engineering OnLine           (2024) 23:14 \nIn the \"Basic model structure \" section, we will delve into the pertinent information \nregarding the encoder–decoder structure and transformer. \"Medical Image Segmenta -\ntion Method Based on Transformer\" section will present a comprehensive summary \nof transformer segmentation methods, considering four perspectives: Transformer in \nthe encoder, Transformer in the codec, Transformer in the skip connections, and the \napplication of the pure Transformer structure. Each subsection within \"Medical Image \nSegmentation Method Based on Transformer\" section sequentially elaborates on the \noptimization and enhancement details of various models. Detailed evaluation metrics \nfor medical image segmentation are outlined in \"Evaluation Indicators \" section. \"Data -\nset\" section systematically organizes the medical image segmentation datasets suitable \nfor reproducing model results. Finally, \"Summary and Outlook\" will encapsulate the \nconclusion and provide insights for future developments.\nBasic model structure\nCodec structure in medical image segmentation\nBecause of the codec structure, the entire network is made up of an encoder module \nand a decoder module. The encoder is primarily responsible for extracting features from \nthe input, while the decoder is responsible for additional feature optimization and job \nprocessing on the encoder’s output. Hinton [23] initially presented this architecture in \nScience in 2006, with the primary goal of compressing and denoising rather than seg -\nmentation. The input is an image, which is downsampled and encoded to generate fea -\ntures that are smaller than the original picture, a process known as compression, and \nthen sent through a decoder, which should restore the original image. For each image, \nwe need to save only one feature and one decoder. Similarly, this concept may be applied \nto picture denoising, which involves adding fake noise to the original image during the \ntraining stage and then inserting it into the codec to restore the original image. This con-\ncept was then used for the picture segmentation problem. Encoders in medical picture \nsegmentation tasks are often based on existing backbone networks such as VGG and \nResNet. The decoder is often constructed to meet the job requirements, labeling each \npixel progressively by upsampling. In 2015, Long introduced a groundbreaking approach \ncalled the Fully Convolutional Neural Network (FCN) [24] for semantic segmentation, \nas illustrated in Fig. 3a. The FCN converts the CNN’s final fully connected layer to a con-\nvolutional layer and merges features from multiple layers using simple skip connections. \nFinally, deconvolution restoration is used to achieve end-to-end picture segmentation. \nThe FCN segmentation results are far from comparable to the manual segmentation \nresults because of upsampling and fusing features of various depths. There are still many \nlocations with segmentation faults, particularly around the edges. At the same time, the \narchitecture of the FCN’s single-path topology makes it impossible to preserve meaning-\nful spatial information in upsampled feature maps and lacks network space consistency.\nOne of the most often used models in medical picture segmentation tasks is the U-Net \nmodel, which is built on the principle of FCN to extract multiscale features. As shown \nin Fig.  3c, the U-Net network initially executes four downsampling operations on the \ninput picture to extract image feature information, followed by four sets of upsampling. \nTo assist the decoder in repairing the target features, a skip connection with a symmet -\nric structure is inserted between the downsampling and upsampling procedures. On the \nPage 5 of 22\nPu et al. BioMedical Engineering OnLine           (2024) 23:14 \n \nright, the output of the downsampled convolutional block is concatenated with the input \nof the deconvolutional block with the same depth. The initial difference between U-Net \nand FCN is that U-Net is extremely symmetric, and the decoder is very similar to the \nencoder, but FCN’s decoder is quite simple, simply utilizing a deconvolution operation \nand no convolutional structure thereafter. The skip connection is the second distinction. \nFCN uses summation, whereas U-Net employs concatenation. In MICCAI 2016, Cicek \net  al. expanded 2D U-Net to 3D U-Net and utilized 3D U-Net [25] to segment dense \ncollective pictures from sparse annotations. nnU-Net [26] is an adaptive framework for \nany dataset based on U-Net, 3D U-Net, and U-Net Cascade. It can automatically adjust \nall hyperparameters according to the properties of a given dataset without human inter -\nvention throughout the process, achieving advanced performance in six well-recognized \nsegmentation challenges. U-Net has quickly become an essential network model in med-\nical picture segmentation due to its great performance and unique topology.\nTransformer\nBenjio’s team proposed the attention mechanism in 2014, and it has since been widely \nused in various fields of deep learning, such as computer vision to capture the recep -\ntive field on an image, or NLP to locate key tokens or features. The multihead attention \nmechanism, position encoding, layer regularization [27], feedforward neural network, \nand skip connection are the main components of the encoder. The decoder differs from \nthe encoder in that it includes an additional masked multihead attention module in the \nFig. 3 Codecs and transformer architectures. a FCN network structure [24]. b A transformer block [15]. c \nClassical U-Net architecture [13]\nPage 6 of 22Pu et al. BioMedical Engineering OnLine           (2024) 23:14 \ninput layer, but the rest of the components are the same. The self-attention mechanism is \nan important part of the transformer, and its unique design allows it to handle variable-\nlength inputs, capture long-distance dependencies, and seq2seq.\nwhere q, k, and v are vectors of input X after linear mapping, and dk is the dimension of \nthe vector. After parallel computing, the multihead attention mechanism extracts fea -\ntures from multiple self-attention mechanism modules and concatenates them in the \nchannel dimension. Various groups of self-attention mechanisms can learn various types \nof feature representations from subspaces at various locations.\nwhere Q, K, and V are matrices made up of multiple q, k, and v vectors. \ni= 1, 2,... ,H ; dk = dv = dmodel /H  ; W Q\ni  and W K\ni  are matrices in the form of ( dmodel , \ndk ), W V\ni  is matrices in the form of ( dmodel , dv ), and the three matrices are parameter \nmatrices used to map input.\nThe decoder’s masked multihead attention mechanism takes into account the fact \nthat during the testing and verification phases, the model can only obtain information \nbefore the current position. To avoid the model’s reliance on information after the cur -\nrent position in the testing phase, the information after the current position is masked in \nthe training phase, ensuring that only information before the position is used to infer the \ncurrent result. Because of the unique design of self-attention, it is insensitive to sequence \nposition information, which is important in both natural language processing and com -\nputer vision tasks, so position information must still be incorporated into transformers. \nTransformers frequently use sine and cosine functions to learn position information.\nLayer regularization overcomes batch regularization’s shortcoming of making it dif -\nficult to handle tasks with variable input sequences. It shifts the scope of regularization \nfrom across samples to within the same sample’s hidden layer, so that regularization is \nindependent of input size. Skip connection is a widely used technique for improving the \nperformance and convergence of deep neural networks, as it alleviates the convergence \nof nonlinear changes via the linear components propagated through the neural network \nlayers. If the patch is too small in the transformer, there will be a false-gradient explosion \nor disappearance.\nVision transformer\nIn 2020, Google introduced the ViT [16], a model that leverages the transformer archi -\ntecture for image classification. ViT innovatively partitions input images into multiple \npatches, each measuring 16x16 pixels. These patches are then individually transformed \ninto fixed-length vectors and integrated into the Transformer framework, as illustrated \nin Fig.  4a. Subsequent encoder operations closely mirror the original Transformer \n(1)Attention(q, k, v) = softmax\n(\nqkT\n√\ndk\n)\nv,\n(2)\nMultiHead(Q , K , V ) = Concat\n(\nAttention\n(\nQW Q\ni , KW K\ni , VW V\ni\n)\n,\n···, Attention\n(\nQW Q\nH , KW K\nH , VW V\nH\n))\nW o,\nPage 7 of 22\nPu et al. BioMedical Engineering OnLine           (2024) 23:14 \n \narchitecture, as depicted in Fig.  4b. While not the pioneer in exploring transformers for \ncomputer vision, ViT stands out as a seminal contribution due to its “simple” yet effec -\ntive model, robust scalability (larger models demonstrating superior performance), and \nits groundbreaking influence on subsequent research in the field. With sufficiently large \npretraining datasets, ViT surpasses CNN, overcoming the limitation of transformers \nlacking inductive bias and showcasing enhanced transfer learning capabilities in down -\nstream tasks.\nIn March 2021, Microsoft Research Asia proposed a universal backbone network \nnamed Swin Transformer [28]. The Swin Transform Block is constructed differently \nfrom ViT, employing Window Multihead Self-Attention (W-MSA) and Shifted Window \nMulti-head Self-Attention (SW-MSA). When computing W-MSA, an 8x8 feature map is \ndivided into 2x2 patches, each with a size of 4x4. For SW-MSA, the entire set of patches \nis shifted by half the patch size, creating a new window with non-overlapping patches. \nFig. 4 Key components of the ViT and Swin Transformer. a The ViT architecture, showcases the \ntransformation of input feature maps into patches, followed by linear mapping and processing through the \nTransformer. The result undergoes classification via an MLP . b The details of the ViT encoder, emphasizing the \nintegration of multihead attention modules. c The feature map evolution in Swin Transformer during W-MSA \nand SW-MSA computation, highlighting the cyclic shift operation for integrating shifted window feature \nmaps. d Swin Transformer Block, outlining its computational process\nPage 8 of 22Pu et al. BioMedical Engineering OnLine           (2024) 23:14 \nThis approach introduces connections between adjacent non-overlapping windows, sig -\nnificantly increasing the receptive field. However, it also raises the issue of increasing the \nnumber of patches within the window from 4 to 9. To maintain the original patch count, \nthe authors employ a cyclic shift operation, as illustrated in Fig.  4c. W-MSA calculates \nattention within each window, while SW-MSA utilizes global modeling, akin to ViT, to \nestablish long-distance dependencies. As depicted in Fig.  4d, Swin Transformer’s unique \ndesign not only introduces local feature extraction capabilities similar to convolution \nbut also substantially reduces computation. Swin Transformer achieves state-of-the-art \nperformance in machine vision tasks such as image classification, object detection, and \nsemantic segmentation.\nMedical image segmentation method based on transformer\nPrior to the application of transformer to the field of medical image segmentation, seg -\nmentation models such as FCN and U-Net performed well in various downstream image \nsegmentation tasks. Researchers have used various methods to improve the U-Net \nmodel to meet the needs of different tasks and data, and a series of variant models \nbased on the U-Net model have appeared; for example, 3D U-Net [25], ResUNet [29], \nU-Net++ [30], and so on. However, since the introduction of ViT, an increasing number \nof researchers have focused on the attention mechanism, attempting to apply it locally \nor globally in complex network structures to achieve better results. By incorporating a \ntransformer module during encoder downsampling, TransUNet [31] outperforms mod -\nels such as V-Net [32], DARR [33], U-Net [13], AttnUNet [34], and ViT [16] in a vari -\nety of medical applications, including multiorgan segmentation and heart segmentation. \nTransUNet, like U-Net, has become a popular network for medical image segmentation. \nBecause of the complexities of medical image segmentation tasks, high-quality manually \nlabeled datasets can only be produced on a small scale. To achieve better performance \non medical image datasets, it is necessary to continuously optimize the application of \ntransformer in the encoder/decoder network. Following that, this paper will discuss \ntransformer-based medical image segmentation methods based on model optimization \nposition.\nFig. 5 TransUnet applied transformer structure to medical image segmentation firstly [31]. a schematic of \nthe Transformer layer; b architecture of the proposed TransUNet\nPage 9 of 22\nPu et al. BioMedical Engineering OnLine           (2024) 23:14 \n \nTransformer encoder structure\nTransUNet, depicted in Fig.  5, stands as the pioneering application of the transformer \nmodel in the realm of image segmentation. The authors serialize the feature map \nobtained through U-Net downsampling and then process the serialized features with a \nblock made up of 12 original transformer layers. The benefits of long-distance depend -\nencies can be obtained using transformers to capture global key features. The experi -\nmental results show that TransUNet outperforms the previous best model, AttnUNet, \non the Synapse dataset. TransBTS [35] replaces 2D CNNS with 3D CNNS and uses \na structural design similar to TransUNet to achieve 3D multimodal brain tumor seg -\nmentation in MRI imaging. Similar to TransBTS, the UNETR [36] employs the same 12 \ntransformer blocks in its encoder. However, UNETR differs in that it utilizes the out -\nputs of the 3rd, 6th, 9th, and 12th transformer blocks as inputs for four downsampling \nconvolutional neural network modules in the encoder. UNETR demonstrates excellent \nperformance in both BTCV [37] and MSD [38], two 3D image segmentation tasks. Fur -\nthermore, Swin UNETR [39] goes a step further by replacing the Transformer blocks \nin UNETR with Swin Transformer blocks, achieving superior results on the BraTS2021 \ndataset compared to nnU-Net, SegResNet, and TransBTS. AFTer-UNet [40] employs an \naxial fusion transformer encoder between CNN encoder and CNN decoder to integrate \ncontextual information across adjacent slices. The axial Fusion transformer encoder cal -\nculates attention along the axial direction and within individual slices, reducing com -\nputational complexity. This approach significantly outperforms models like CoTr and \nSwinUnet on multiorgan segmentation datasets, including BCV [41], Thorax-85 [42], \nand SegTHOR [43].\nIn general, most methods for dealing with 2D image segmentation can also be used \nto deal with continuous video data, as long as the video data are input as a 2D image \nframe by frame. The cost of this is that we cannot fully exploit the time continuity of \nthe video data. Zhang et  al. [44] created an additional convolution branch based on \nTransUNet to extract the features of the previous frame data, and then combined the \nresults of the downsampling of the two parts with the results of the upsampling via the \nskip connection to achieve a better video data segmentation effect. X-Net [45] extends \nU-Net by introducing an additional Transformer-based encoder–decoder branch, facili -\ntating information fusion across branches through skip connections. Zhang et al. pro -\nposed a new architecture called TransFuse, which can run convolution-based and pure \ntransformer-based encoders in parallel and then fuse the features from the two branches \ntogether to jointly predict segmentation results via the BiFusion module, greatly improv-\ning the model’s inference speed [46]. This work adds a new perspective to the use of \ntransformer-based models by investigating whether a network using only transformers \nand no convolution can perform better segmentation tasks.\nThe primary goal of the self-attention mechanism is to model the long-distance \ndependence between pixels to obtain global context information. On the other hand, \nconvolution produces feature maps at various scales that frequently contain complex \ninformation. Before the appearance of ViT, researchers discovered numerous effec -\ntive methods for expanding the convolution receptive field using convolution. Dilated \nconvolutions are the most well-known of these, and DeepLabV3 [47] uses dilated spa -\ntial pyramid pooling to great effect, while CE-Net [48] captures multiscale information \nPage 10 of 22Pu et al. BioMedical Engineering OnLine           (2024) 23:14 \nusing dense dilated convolutions and residual multikernel pooling. As a result, taking \ninto account global context information and multiscale information is a very effective \nmethod. Yuanfeng Ji [49] et  al. proposed MCTrans, a self-attention transformer mod -\nule and a cross-attention transformer module. The self-attention transformer module \nperforms pixel-level context modeling at multiple scales. To ensure intraclass consist -\nency and interclass discrimination, the cross-attention transformer module is used to \nlearn the corresponding semantic relationship of different categories, that is, the dif -\nference between feature expressions of different classes and the connection between \nfeature expressions of different classes. DC-Net [50] also reflects the emphasis on mul -\ntiscale features in this model. The authors create a Global Context Transformer Encoder \n(GCTE) and a module for Adaptive Context Fusion (ACFM). GCTE connects the trans -\nformer encoder to the back of CNN down-sampling, serializes the multiscale features \nobtained by CNN and the input image, and then obtains a better feature representation \nvia the transformer encoder. The ACFM is made up of four cascaded feature decoding \nblocks, each with two 1 × 1 convolutions and a 3 × 3 deconvolution. The adaptive weight \nωi is converted by the authors into adaptive spatial weight (APW) and adaptive channel \nweight (ACW). The ACFM can better fuse context information and improve decoder \nperformance using the two weight parts of the APW and ACW.\nAlthough transformers have achieved outstanding results in a variety of downstream \nmedical image tasks, it is undeniable that they have more parameters to train than con -\nvolutional models. As a result, how to optimize the model using global context infor -\nmation obtained by the transformer to meet the requirements of lightweight tasks for \nmodel size and inference speed has become a hot topic in research. SA-Net [51] was \nproposed in early transformer-related research to reduce the number of parameters in \nCNN and transformer using a random ranking algorithm. The sandwich parameter-\nshared encoder structure [52] was investigated by Reid M et al. In the field of medical \nimage segmentation, the CoTr model [53] was proposed by Xie Y et  al. The encoder \nstructure was created by combining the bridge structure DeTrans, which was made up \nof the MS-DMSA layer and only focused on a small set of key sampling locations around \nthe reference location, with CNN, which greatly reduced the time and space complex -\nity. TransBridge [54] employs a bridge structure similar to CoTr, but adds a shuffle layer \nand group convolution to the transformer’s embedding part to reduce the number of \nparameters and the length of the embedding sequence. The experimental results show \nthat after 78.7% parameter reduction, on the EchoNet-Dynamic dataset, TransBridge \noutperforms CoTr, ResUNet [29], DeepLabV3 [55], and other models.\nTransformer codec structure\nTransUNet demonstrated the importance of transformers in encoders, and the sym -\nmetries of encoder–decoder architectures make it simple to extend transformers to \ndecoder architectures. U-Transformer [56] uses the Multihead Cross-attention Module \n(MHCA) to combine the high-level feature maps with complex abstract information and \nthe high-resolution feature maps obtained through the skip connection in each splic -\ning process of upsampling and skip connection, which is used to suppress the irrelevant \nregions and noise regions of the high-resolution feature maps. The feature map obtained \nby convolution is expanded pixel by pixel as a transformer patch in the encoder section, \nPage 11 of 22\nPu et al. BioMedical Engineering OnLine           (2024) 23:14 \n \nand then a single transformer layer is used to extract global context information. Luo \nC et al. [57] improved the use of transformer in encoders based on the TransUNet and \nU-Transformer. To build the UCATR model, a block of 12 transformer layers is used \nto replace the single MultiHead self-attention in the U-Transformer. The experimental \nresults show that the UCATR model can recover more refined spatial information than \nthe original TransUNet and U-Transformer. SWTRU [58] proposes a novel Star-shaped \nWindow self-attention mechanism to be applied in the decoder structure and intro -\nduces the Filtering Feature Integration Mechanism (FFIM) to integrate and reduce the \ndimensionality of the fused multilayer features. These improvements result in a better \nsegmentation effect in CHLISC [59, 60], LGG [61, 62], and ISIC2018 [63]. Since in most \nvision tasks the visual dependencies between regions nearby are usually stronger than \nthose far away, MT-UNet [64] performs local self-attention on fine-grained local context \nand global self-attention only on coarse-grained global context. When calculating global \nattention maps, axial attention [65] is used to reduce the amount of calculation, and fur -\nther introduce a learnable Gaussian matrix [66] to enhance the weight of nearby tokens. \nMT-UNet performs better than models such as ViT and TransUNet on the Synapse and \nACDC datasets.\nAlthough transformers have done much useful work in medical image segmenta -\ntion tasks, training and deploying transformer-based models remains difficult due to a \nlarge amount of training time and memory space overhead. To reduce the impact of the \nsequence length overhead, one common method is to use the feature maps obtained by \ndownsampling as the input sequence rather than the entire input image. High-resolution \nimages, on the other hand, are critical for location-sensitive tasks such as medical image \nsegmentation, because the majority of false segmentations occur within the region of \ninterest’s boundary range. Second, in medical image data with small data volumes, trans-\nformers have no inductive bias and can be infinitely enlarged.\nGao Y et al. [67] combined the benefits of convolution and the attention mechanism \nfor medical image segmentation, replacing the last layer of convolution with a trans -\nformer module in each downsampling block, avoiding large-scale transformer pretrain -\ning while capturing long-distance correlation information. At the same time, to extract \nthe detailed long-distance information on the high-resolution feature map, two projec -\ntions are used to project the K and V (K and V ∈ Rn×d ) into low-dimensional embedding \n( K and V ∈ Rk×d , k = hw ≪ n ), where h and w are the reduced sizes of the feature map \nafter subsampling, which reduces the overall complexity from O(n2) to O(n). In addition, \nthe authors also learn the content–location relationship in medical images using rela -\ntive position encoding in the self-attention module. Valanarasu J et al. [68] proposed an \nMedT model based on a gated location-sensitive attention mechanism, which allowed \nthe model to perform well on smaller datasets during training. Feiniu Yuan et  al. [69] \nintroduced CTC-Net, a synergistic network that combines both CNN and transformer \nfor medical image segmentation. This approach involves feature extraction through both \na CNN encoder and a Swin Transformer encoder, followed by feature fusion facilitated \nby an Feature Complementary Module (FCM) incorporating channel attention and spa -\ntial attention mechanisms.\nPage 12 of 22Pu et al. BioMedical Engineering OnLine           (2024) 23:14 \nTransformer in skip connections\nThe mechanism of skip connections was initially introduced in U-Net, aiming to bridge \nthe semantic gap between the encoder and decoder, and has proven to be effective in \nrecovering fine-grained details of the target objects. Subsequently, UNet++ [30], Att -\nnUnet [34], and MultiResUNet [70] further reinforced this mechanism. However, in \nUCTransUnet [71], the authors pointed out that skip connections in U-Net are not \nalways effective in various medical image segmentation tasks. For instance, in the GlaS \n[72] dataset, a U-Net model without skip connections outperforms the one with skip \nconnections, and using different numbers of skip connections also yields different \nresults. Therefore, the authors considered adopting a more suitable approach for feature \nfusion at different depths. They replaced the simple skip connections in U-Net with the \nCTrans module, consisting of multiscale Channel Cross fusion with Transformer (CCT) \nand Channel-wise Cross-Attention (CCA). This modification demonstrated competitive \nresults on the GlaS and MoNuSeg [73] datasets.\nPure transformer structure\nResearchers have attempted to use transformer as a complete replacement for convo -\nlution operators in codec structures due to its significant advantage in global context \nfeature extraction. Karimi D et al. [74] pioneered the nonconvolutional deep neural net -\nwork for 3D medical image segmentation, demonstrating through experiments that a \nneural network fully composed of transformer modules can achieve segmentation accu -\nracy superior to or comparable to the most advanced CNN model 3D UNet++ [30].\nBased on the Swin Transformer, Cao H et  al. [75]created Swin-Unet, a pure trans -\nformer model similar to U-Net. The model employs two consecutive Swin Transformer \nblocks as a bottleneck, which are then assembled in a U-Net-like configuration. The \nstructure of Swin-Unet is shown in Fig.  6. By comparing Swin-Unet with V-Net [32], \nDARR [ 33], ResUnet [29], AttnUnet [34] and TransUnet [31] on two datasets of Syn -\napse and ACDC, the authors obtained significantly better performance than other mod -\nels. Swin-PANet [76] is a dual supervision network structure proposed by Zhihao Liao \net al. Swin-PANet is made up of two networks: a prior attention network and a hybrid \ntransformer network. The prior attention network applies the sliding window-based \nsubattention mechanism to the intermediate supervision network, whereas the hybrid \ntransformer network aggregates the features of the jump connection and the prior atten-\ntion network and refines the boundary details. GlaS [72] and MoNuSeg [73] yield bet -\nter results. DS-TransUNet [77] is constructed upon the SwinTransformer framework \nand enhances feature representation with a dual-scale encoder. More precisely, the \napproach employs medical images segmented at both large and small scales as inputs to \nthe encoder. This allows the model to effectively capture coarse-grained and fine-grained \nfeature representations.\nThese models demonstrate the Swin Transformer’s utility for medical image datasets. \nBecause the Swin Transformer is more lightweight and suitable for medical image seg -\nmentation tasks than transformers that require large amounts of data pretraining in \nNLP , further investigating its application can help overcome the challenge of limiting \nmodel progress in medical image datasets.\nPage 13 of 22\nPu et al. BioMedical Engineering OnLine           (2024) 23:14 \n \nEvaluation indicators\nThe objective evaluation of the performance of medical image segmentation algo -\nrithms is essential for their practical application in diagnosis. The segmentation \nresults must be assessed both qualitatively and quantitatively. For segmentation tasks \nwith multiple categories, let k  be the number of classes in the segmentation result, p ij \nbe the total number of pixels whose class i  is predicted to be the total number of class \nj, and p ii be the total number of pixels whose class i is predicted to be the total num -\nber of class i . When k = 2 , we can divide the results of a segmentation task with only \ntwo classes into four categories: True positive (TP) indicates that both the observed \nand predicted data classes are correct. True negative (TN) indicates that both the \nactual and predicted data classes are incorrect. The term false positive (FP) refers to \nwhen the actual data class is false while the predicted data class is true. The term false \nnegative (FN) denotes that the actual data class is true while the predicted data class \nis false. The following are examples of commonly used evaluation metrics.\nThe F1 score, or F-measure, is a metric used in binary classification analysis, repre -\nsenting the harmonic mean of precision and recall. Precision is the ratio of true posi -\ntive results to all identified positive results, while recall is the ratio of true positive \nFig. 6 The Swin-Unet structure [75]\nPage 14 of 22Pu et al. BioMedical Engineering OnLine           (2024) 23:14 \nresults to all actual positive instances. By combining precision and recall in a single \nmetric, the F1 score provides a balanced measure of a test’s accuracy. It ranges from \n0 to 1, with 1 indicating perfect precision and recall, and 0 if either precision or recall \nis zero.\nThe prediction results are evaluated using pixel accuracy (PA), which stands for the pro -\nportion of total pixels classified correctly over the total number of pixels of original sam-\nples. The PA value is closer to one, the segmentation is more accurate. The closer the \nvalue is to one, the more accurate the segmentation. The formula for calculation is as \nfollows:\nMean pixel accuracy (MPA) is a step up from PA. It calculates PA for each class sepa -\nrately, then averages PA for all classes.\nThe Jaccard index, or Jaccard similarity coefficient, serves as a statistical measure to \nassess the similarity and diversity between sample sets. Introduced by Grove Karl Gilbert \nin 1884, it is formulated as the ratio of verification [78]. The Jaccard coefficient quantifies \nthe similarity of finite sample sets by calculating the size of their intersection divided by \nthe size of their union. This metric is also referred to as Intersection over Union (IoU).\nThe mean intersection over union (mIoU) is used to calculate different categories of IoU \nin the image, and then calculate the average value is calculated as the final result. For \nimage segmentation, the calculation formula of mIoU is as follows:\n(3)Precision= TP\nTP + FP\n(4)Recall= TP\nTP + FN\n(5)F1 = 2 × Precision× Recall\nPrecision+ Recall\n(6)PA =\n∑k\ni=0 p ii\n∑k\ni=0\n∑k\nj=0 p ij\n.\n(7)MPA = 1\nk + 1\nk∑\ni=0\np ii\n∑k\nj=0p ij\n(8)J(A, B) = |A ∩ B|\n|A ∪ B| = |A ∩ B|\n|A|+| B|−| A ∩ B|\n(9)IoU =\n1∑\ni=0\np ii\n∑1\nj=0 p ij + ∑1\nj=0 p ji − p ii\nPage 15 of 22\nPu et al. BioMedical Engineering OnLine           (2024) 23:14 \n \nThe Dice coefficient is a fixed similarity measurement function that is commonly used \nto determine the similarity of two samples. In the segmentation task, we consider the \nmodel prediction result and the real mask to be two sets with the same number of ele -\nments, and the value of the Dice coefficient is used to judge the quality of the model \nprediction result.\nThe directed average Hausdorff distance from point set X to Y is given by the sum of \nall minimum distances from all points from point set X to Y divided by the number of \npoints in X. The average Hausdorff distance can be calculated as the mean of the directed \naverage Hausdorff distance from X to Y and directed average Hausdorff distance from Y \nto X. In the medical image segmentation domain, the point sets X and Y refer to the \nvoxels of the ground truth and the segmentation, respectively. The average Hausdorff  \ndistance between the voxel sets of ground truth and segmentation can be calculated in \nmillimeters or voxels.\nDataset\nUnlike general image datasets, medical image annotation requires doctors with pro -\nfessional experience to devote significant time to annotation. The majority of the early \npathological image data are of a small scale. Deep learning models, particularly trans -\nformer-based models, rely heavily on large-scale data to perform well. A novel labeling \nstrategy involves training a deep learning model with a small amount of data and then \nmanually modifying the model’s prediction results to continuously expand and improve \nthe dataset. Some public datasets used in many popular medical image segmentation \ntasks have been compiled in Table 1 to assist readers in conducting relevant experiments \nquickly. In the “Resolving power (pixel)” column of Table  1, “~” indicates that the image \nresolution in the dataset is not uniform. For example, in the GLAS dataset, the mini -\nmum image resolution is 567 × 430 and the maximum resolution is 755 × 522. “*” is \nonly used in 3D image datasets to indicate that the number of channels in the dataset is \nnot fixed, even if the image resolution is the same.\nSummary and outlook\nTransformers have emerged as a hot topic in the field of deep learning, and they can \nbe found in a variety of downstream tasks in NLP and computer vision. The hybrid \nmodel of the convolutional neural network and transformer performs well in the task \n(10)mIoU = 1\nk + 1\nk∑\ni=0\np ii\n∑k\nj=0 p ij + ∑k\nj=0 p ji − p ii\n.\n(11)DSC = 1\nk + 1\nk∑\ni=0\n2 × p ii\n∑k\nj=0 p ij + ∑k\nj=0 p ji\n(12)AVD =\n1\nX\n∑\nx∈X\nmin\ny∈Y\nd(x,y)+ 1\nY\n∑\ny∈Y\nmin\nx∈X\nd(x,y)\n2\nPage 16 of 22Pu et al. BioMedical Engineering OnLine           (2024) 23:14 \nof medical image segmentation. However, using transformer to process medical images \nstill presents significant challenges:\n1. The medical image dataset is small: labeling medical images requires doctors with \nprofessional experience, and medical images have high resolution, so labeling medi -\ncal images takes time and money. Existing medical image datasets have a small sample \nsize. Using transformers to their full potential in capturing long-distance dependencies \nnecessitates more samples, which most medical image datasets lack.\n2. Transformer lacks location information: Object location information is critical for \nsegmentation results in medical image segmentation tasks. Transformer can only embed \nposition information through learning because it does not contain position information. \nHowever, the location information is different for different data sets, and the require -\nments for location information are different, so the methods of learning location are also \ndifferent, which has a significant impact on the model’s generalization.\n3. The self-attention mechanism only works between image patches: after the image \nis serialized, the calculation of the attention weight is only performed between image \npatches, and the relationship between the pixels within the image patch is ignored. Criti-\ncal information between pixels can affect model accuracy when segmenting, recogniz -\ning, or detecting small objects and tasks with blurred boundaries.\nTable 1 Medical image dataset\nDatasets Year Tasks Resolving power(pixel) Sample\nSTARE [79] 2000 Retinal vascular segmentation 700 × 605 20\nDRIVE [80] 2004 Retinal vascular segmentation 768 × 584 40\nAlizarine [81] 2010 Corneal endothelial cell segmentation 768 × 576 30\nCHASE-DBI [82] 2012 Retinal vascular segmentation 999 × 960 28\nHRF [83] 2013 Retinal vascular segmentation 3304 × 2336 45\nGLAS [72] 2016 Glandular segmentation 567 × 430 ∼775 ×  522 165\nMoNuSeg [73] 2017 Nuclear segmentation of multiple organs 1000 × 1000 30\nDSB18 [84] 2018 Nuclear segmentation ∼ 670\nTNBC [85] 2018 Nuclear segmentation 512 × 512 50\nIDRiD [86] 2018 Segmentation of fundus lesions 4288 ×  2848 516\nDDR [87] 2019 Segmentation of fundus lesions 512 × 512 757\nPanNuke [88] 2019 Multiple organ pan cancer cell segmentation 256 × 256 7904\nBrain US [89] 2019 Ventricular septum segmentation 512× 512 1629\nKvasir-SEG [90] 2020 Gastrointestinal polyp segmentation 332 × 487∼1920 × 1072 1000\nTM-EM3000 [91] 2021 Corneal endothelial cell segmentation 266 ×  480 184\nPROMISE12 [92] 2012 Prostate segmentation ∼ 100\nBTCV [37] 2015 Abdominal organ segmentation 512 × 512×∗ 50\nBCV [41] 2015 Abdominal organ segmentation ∼ 30\nACDC [93] 2018 Cardiac segmentation ∼ 150\nBraTS [94] 2018 Brain tumor segmentation 240 × 240 × 155 285\nMSD [38] 2018 Decathlon Division ∼ 2633\nLiTS [60] 2019 Liver tumor segmentation 512 × 512×∗ 131\nKiTS19 [95] 2019 Renal tumor segmentation ∼ 210\nSegTHOR [43] 2019 Chest organ segmentation ∼ 40\nThorax-85 [42] 2021 Chest organ segmentation ∼ 85\nPage 17 of 22\nPu et al. BioMedical Engineering OnLine           (2024) 23:14 \n \nFig. 7 Three network structures for transformer applications in medical image segmentation tasks\nPage 18 of 22Pu et al. BioMedical Engineering OnLine           (2024) 23:14 \nBased on the transformer’s current status and challenges in medical image segmenta -\ntion, the following suggestions and prospects for future research are made:\n1. The transformer’s ability to extract global key features from large datasets has been \nleveraged to train the model on large datasets with auxiliary tasks or to learn exist -\ning labeled image features to automatically generate high-confidence pseudo labels. \nThese approaches are effective in addressing the challenge of small-scale medical \nimage datasets.\n2. Integrating prior knowledge about the location can assist the model in highlighting \nimportant features of the target task. The position encoding for transformer can be \nthoughtfully designed to incorporate prior knowledge of the image position, thereby \nenhancing the model’s ability to generalize.\n3. Optimizing the model structure is crucial. A large receptive field transformer can \nextract global key features, while a convolutional neural network is better suited \nfor capturing small local features through continuous convolution pooling, which \nis essential for segmentation tasks. Therefore, the fusion strategy between the two \nmethods needs to be optimized to fully leverage their respective strengths and ensure \nthe model’s optimal performance.\nThe transformer has become one of the most popular deep learning frameworks in \nthe last 2 years. It can alleviate the problems of scattered target regions and large shape \ndifferences in medical image segmentation tasks due to its advantage of obtaining global \ncontext. As shown in Fig.  7, both CNN and transformer have their advantages. The \ntransformer can use the convolutional neural network structure to fully exploit the abil -\nity of sample information to extract multiscale local spatial features, allowing the model’s \nglobal and local information to achieve a better balance and improve model perfor -\nmance. We summarize recent research on the hybrid model of convolutional neural net-\nworks and transformers in this paper. Transformers have good development prospects \nand high research significance in the field of medical image segmentation, based on the \nperformance of the model in this paper.\nAuthor contributions\nLZ: conceptualization, methodology. QP and ZX: collection, organizing, and review of the literature. QP , ZX and SY: pre-\nparing the manuscript. LZ, QP and ZX: manuscript review and modification. LZ, QP , ZX, SY and ZZ: editing and revision. \nAll authors read and approved the final manuscript.\nFunding\nThis work was supported by the National Key Research and Development Program of China (2020YFA0710700 and \n2021YFA1200904), the National Natural Science Foundation of China (12375326 and 31971311), the Innovation Program \nfor IHEP (E35457U2).\nAvailability of data and materials\nAll data can be found on the corresponding page of the cited literature.\nDeclarations\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that \ncould be construed as a potential conflict of interest.\nPage 19 of 22\nPu et al. BioMedical Engineering OnLine           (2024) 23:14 \n \nReceived: 22 September 2023   Accepted: 22 January 2024\nReferences\n 1. Xu A, Wang L, Feng S, Qu Y. Threshold-based level set method of image segmentation. In: 2010 Third International \nConference on Intelligent Networks and Intelligent Systems, pp. 703–706 (2010). IEEE\n 2. Cigla C, Alatan A.A. Region-based image segmentation via graph cuts. In: 2008 15th IEEE International Conference \non Image Processing, pp. 2272–2275 (2008). IEEE\n 3. Yu-Qian Z, Wei-Hua G, Zhen-Cheng C, Jing-Tian T, Ling-Yun L. Medical images edge detection based on mathemati-\ncal morphology. In: 2005 IEEE Engineering in Medicine and Biology 27th Annual Conference, pp. 6492–6495 (2006). \nIEEE\n 4. Ma Z, Tavares J.M.R, Jorge R.N. A review on the current segmentation algorithms for medical images. In: Interna-\ntional Conference on Imaging Theory and Applications, vol. 1, pp. 135–140 (2009). SciTePress\n 5. Ferreira A, Gentil F, Tavares JMR. Segmentation algorithms for ear image data towards biomechanical studies. Com-\nput Methods Biomech Biomed Eng. 2014;17(8):888–904.\n 6. Ma Z, Tavares JMR, Jorge RN, Mascarenhas T. A review of algorithms for medical image segmentation and their \napplications to the female pelvic cavity. Comput Methods Biomech Biomed Eng. 2010;13(2):235–46.\n 7. Liu Y, Wang J, Wu C, Liu L, Zhang Z, Yu H. Fovea-unet: Detection and segmentation of lymph node metastases in \ncolorectal cancers with deep learning (2023)\n 8. Gu H, Gan W, Zhang C, Feng A, Wang H, Huang Y, Chen H, Shao Y, Duan Y, Xu Z. A 2d–3d hybrid convolutional neural \nnetwork for lung lobe auto-segmentation on standard slice thickness computed tomography of patients receiving \nradiotherapy. BioMed Eng OnLine. 2021;20:1–13.\n 9. Jin Q, Meng Z, Sun C, Cui H, Su R. Ra-unet: a hybrid deep attention-aware network to extract liver and tumor in ct \nscans. Front Bioeng Biotechnol. 2020;8: 605132.\n 10. Sarker M.M.K, Rashwan H.A, Akram F, Banu S.F, Saleh A, Singh V.K, Chowdhury F.U, Abdulwahab S, Romani S, Radeva \nP,  et al. Slsdeep: Skin lesion segmentation based on dilated residual and pyramid pooling networks. In: Medical \nImage Computing and Computer Assisted Intervention–MICCAI 2018: 21st International Conference, Granada, \nSpain, September 16-20, 2018, Proceedings, Part II 11, pp. 21–29 (2018). Springer\n 11. Wang Z, Peng Y, Li D, Guo Y, Zhang B. Mmnet: a multi-scale deep learning network for the left ventricular segmenta-\ntion of cardiac mri images. Appl Intell. 2022;52(5):5225–40.\n 12. Guo C, Szemenyei M, Yi Y, Wang W, Chen B, Fan C. Sa-unet: Spatial attention u-net for retinal vessel segmentation. In: \n2020 25th International Conference on Pattern Recognition (ICPR), pp. 1236–1242 (2021). IEEE\n 13. Ronneberger O, Fischer P , Brox T. U-net: Convolutional networks for biomedical image segmentation. In: Medical \nImage Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, \nGermany, October 5-9, 2015, Proceedings, Part III 18, pp. 234–241 (2015). Springer\n 14. Razzak M.I, Naz S, Zaib A. Deep learning for medical image processing: Overview, challenges and the future. Clas-\nsification in BioApps: Automation of Decision Making, 323–350 (2018)\n 15. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez A.N, Kaiser Ł, Polosukhin I: Attention is all you need. \nAdvances in neural information processing systems 30 (2017)\n 16. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, \nGelly S, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010. \n11929 (2020)\n 17. Xiao H, Li L, Liu Q, Zhu X, Zhang Q. Transformers in medical image segmentation: a review. Biomed Signal Process \nControl. 2023;84: 104791.\n 18. Atabansi CC, Nie J, Liu H, Song Q, Yan L, Zhou X. A survey of transformer applications for histopathological image \nanalysis: new developments and future directions. BioMed Eng OnLine. 2023;22(1):96.\n 19. Azad R, Kazerouni A, Heidari M, Aghdam EK, Molaei A, Jia Y, Jose A, Roy R, Merhof D. Advances in medical image \nanalysis with vision transformers: a comprehensive review. Med Image Anal. 2024;91: 103000. https:// doi. org/ 10. \n1016/j. media. 2023. 103000.\n 20. Nanni L, Fantozzi C, Loreggia A, Lumini A. Ensembles of convolutional neural networks and transformers for polyp \nsegmentation. Sensors. 2023;23(10):4688.\n 21. Ghazouani F, Vera P , Ruan S. Efficient brain tumor segmentation using swin transformer and enhanced local self-\nattention. International Journal of Computer Assisted Radiology and Surgery, 1–9. 2023.\n 22. Ali H, Mohsen F, Shah Z. Improving diagnosis and prognosis of lung cancer using vision transformers: a scoping \nreview. BMC Med Imaging. 2023;23(1):129.\n 23. Hinton GE, Salakhutdinov RR. Reducing the dimensionality of data with neural networks. Science. \n2006;313(5786):504–7.\n 24. Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE \nConference on Computer Vision and Pattern Recognition, pp. 3431–3440. 2015.\n 25. Çiçek Ö, Abdulkadir A, Lienkamp S.S, Brox T, Ronneberger O. 3d u-net: learning dense volumetric segmentation \nfrom sparse annotation. In: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2016: 19th \nInternational Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19, pp. 424–432. 2016. Springer\n 26. Isensee F, Jaeger PF, Kohl SA, Petersen J, Maier-Hein KH. nnu-net: a self-configuring method for deep learning-based \nbiomedical image segmentation. Nat Methods. 2021;18(2):203–11.\n 27. Ba J.L, Kiros J.R, Hinton G.E. Layer normalization. arXiv preprint arXiv: 1607. 06450. 2016.\n 28. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B. Swin transformer: Hierarchical vision transformer using shifted \nwindows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022. 2021.\nPage 20 of 22Pu et al. BioMedical Engineering OnLine           (2024) 23:14 \n 29. Xiao X, Lian S, Luo Z, Li S. Weighted res-unet for high-quality retina vessel segmentation. In: 2018 9th International \nConference on Information Technology in Medicine and Education (ITME), pp. 327–331. 2018. IEEE\n 30. Zhou Z, Rahman Siddiquee M.M, Tajbakhsh N, Liang J. Unet++: A nested u-net architecture for medical image \nsegmentation. In: Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: \n4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with \nMICCAI 2018, Granada, Spain, September 20, 2018, Proceedings 4, pp. 3–11. 2018. Springer\n 31. Chen J, Lu Y, Yu Q, Luo X, Adeli E, Wang Y, Lu L, Yuille A.L, Zhou Y. Transunet: Transformers make strong encoders for \nmedical image segmentation. arXiv preprint arXiv: 2102. 04306. 2021.\n 32. Milletari F, Navab N, Ahmadi S.-A. V-net: Fully convolutional neural networks for volumetric medical image segmen-\ntation. In: 2016 Fourth International Conference on 3D Vision (3DV), pp. 565–571. 2016. Ieee\n 33. Fu S, Lu Y, Wang Y, Zhou Y, Shen W, Fishman E, Yuille A. Domain adaptive relational reasoning for 3d multi-organ \nsegmentation. In: Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International \nConference, Lima, Peru, October 4–8, 2020, Proceedings, Part I 23, pp. 656–666. 2020. Springer\n 34. Oktay O, Schlemper J, Folgoc L.L, Lee M, Heinrich M, Misawa K, Mori K, McDonagh S, Hammerla N.Y, Kainz B, et al. \nAttention u-net: Learning where to look for the pancreas. arXiv preprint arXiv: 1804. 03999. 2018.\n 35. Wang W, Chen C, Ding M, Yu H, Zha S, Li J. Transbts: Multimodal brain tumor segmentation using transformer. In: \nMedical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, \nStrasbourg, France, September 27–October 1, 2021, Proceedings, Part I 24, pp. 109–119. 2021. Springer\n 36. Hatamizadeh A, Tang Y, Nath V, Yang D, Myronenko A, Landman B, Roth H.R, Xu D. Unetr: Transformers for 3d medical \nimage segmentation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. \n574–584. 2022.\n 37. Landman B, Xu Z, Igelsias J, Styner M, Langerak T, Klein A. Miccai multi-atlas labeling beyond the cranial vault–work-\nshop and challenge. In: Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge, vol. 5, p. 12. \n2015.\n 38. Antonelli M, Reinke A, Bakas S, Farahani K, Kopp-Schneider A, Landman BA, Litjens G, Menze B, Ronneberger O, Sum-\nmers RM, et al. The medical segmentation decathlon Nature communications. 2022; 13(1):4128.\n 39. Hatamizadeh A, Nath V, Tang Y, Yang D, Roth H.R, Xu D. Swin unetr: Swin transformers for semantic segmentation of \nbrain tumors in mri images. In: International MICCAI Brainlesion Workshop, pp. 272–284. 2021. Springer\n 40. Yan X, Tang H, Sun S, Ma H, Kong D, Xie X. After-unet: Axial fusion transformer unet for medical image segmentation. \nIn: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3971–3981. 2022.\n 41. Landman B, Xu Z, Igelsias J.E, Styner M, Langerak T.R, Klein A. 2015 miccai multi-atlas labeling beyond the cranial \nvault workshop and challenge. In: Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge. \n2015.\n 42. Chen X, Sun S, Bai N, Han K, Liu Q, Yao S, Tang H, Zhang C, Lu Z, Huang Q, et al. A deep learning-based auto-seg-\nmentation system for organs-at-risk on whole-body computed tomography images for radiation therapy. Radiother \nOncol. 2021;160:175–84.\n 43. Lambert Z, Petitjean C, Dubray B, Kuan S. Segthor: Segmentation of thoracic organs at risk in ct images. In: 2020 \nTenth International Conference on Image Processing Theory, Tools and Applications (IPTA), pp. 1–6. 2020. IEEE\n 44. Zhang G, Wong H.-C, Wang C, Zhu J, Lu L, Teng G. A temporary transformer network for guide-wire segmentation. \nIn: 2021 14th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-\nBMEI), pp. 1–5. 2021. IEEE\n 45. Li Y, Wang Z, Yin L, Zhu Z, Qi G, Liu Y. X-net: a dual encoding–decoding method in medical image segmentation. The \nVisual Computer, 1–11. 2021.\n 46. Zhang Y, Liu H, Hu Q. Transfuse: Fusing transformers and cnns for medical image segmentation. In: Medical Image \nComputing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, \nSeptember 27–October 1, 2021, Proceedings, Part I 24, pp. 14–24. 2021. Springer\n 47. Chen L.-C, Papandreou G, Schroff F, Adam H. Rethinking atrous convolution for semantic image segmentation. arXiv \npreprint arXiv: 1706. 05587. 2017.\n 48. Gu Z, Cheng J, Fu H, Zhou K, Hao H, Zhao Y, Zhang T, Gao S, Liu J. Ce-net: context encoder network for 2d medical \nimage segmentation. IEEE Trans Med Imaging. 2019;38(10):2281–92.\n 49. Ji Y, Zhang R, Wang H, Li Z, Wu L, Zhang S, Luo P . Multi-compound transformer for accurate biomedical image \nsegmentation. In: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International \nConference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I 24, pp. 326–336. 2021. Springer\n 50. Xu R, Wang C, Xu S, Meng W, Zhang X. Dc-net: Dual context network for 2d medical image segmentation. In: \nMedical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, \nStrasbourg, France, September 27–October 1, 2021, Proceedings, Part I 24, pp. 503–513. 2021. Springer\n 51. Zhang Q.-L, Yang Y.-B. Sa-net: Shuffle attention for deep convolutional neural networks. In: ICASSP 2021-2021 IEEE \nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2235–2239. 2021. IEEE\n 52. Reid M, Marrese-Taylor E, Matsuo Y. Subformer: Exploring weight sharing for parameter efficiency in generative \ntransformers. arXiv preprint arXiv: 2101. 00234. 2021.\n 53. Xie Y, Zhang J, Shen C, Xia Y. Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation. \nIn: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, \nStrasbourg, France, September 27–October 1, 2021, Proceedings, Part III 24, pp. 171–180. 2021. Springer\n 54. Deng K, Meng Y, Gao D, Bridge J, Shen Y, Lip G, Zhao Y, Zheng Y. Transbridge: A lightweight transformer for left \nventricle segmentation in echocardiography. In: Simplifying Medical Ultrasound: Second International Workshop, \nASMUS 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings 2, pp. \n63–72. 2021. Springer\n 55. Chen L.-C, Zhu Y, Papandreou G, Schroff F, Adam H. Encoder-decoder with atrous separable convolution for seman-\ntic image segmentation. In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 801–818. \n2018.\nPage 21 of 22\nPu et al. BioMedical Engineering OnLine           (2024) 23:14 \n \n 56. Petit O, Thome N, Rambour C, Themyr L, Collins T, Soler L. U-net transformer: Self and cross attention for medical \nimage segmentation. In: Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held \nin Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings 12, pp. 267–276. 2021. \nSpringer\n 57. Luo C, Zhang J, Chen X, Tang Y, Weng X, Xu F. Ucatr: Based on cnn and transformer encoding and cross-attention \ndecoding for lesion segmentation of acute ischemic stroke in non-contrast computed tomography images. In: 2021 \n43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pp. 3565–3568. \n2021. IEEE\n 58. Zhang J, Liu Y, Wu Q, Wang Y, Liu Y, Xu X, Song B. Swtru: star-shaped window transformer reinforced u-net for medi-\ncal image segmentation. Comput Biol Med. 2022;150: 105954.\n 59. Selvi E, SELVER M, Kavur A, GÜZELİŞ C, DİCLE O. Segmentation of abdominal organs from mr images using multi-\nlevel hierarchical classification. J Faculty Eng Arch Gazi Univ. 2015; 30(3).\n 60. Bilic P , Christ P , Li HB, Vorontsov E, Ben-Cohen A, Kaissis G, Szeskin A, Jacobs C, Mamani GEH, Chartrand G, et al. The \nliver tumor segmentation benchmark (lits). Med Image Anal. 2023;84: 102680.\n 61. Buda M, Saha A, Mazurowski MA. Association of genomic subtypes of lower-grade gliomas with shape features \nautomatically extracted by a deep learning algorithm. Comput Biol Med. 2019;109:218–25.\n 62. Mazurowski MA, Clark K, Czarnek NM, Shamsesfandabadi P , Peters KB, Saha A. Radiogenomics of lower-grade \nglioma: algorithmically-assessed tumor shape is associated with tumor genomic subtypes and patient outcomes in \na multi-institutional study with the cancer genome atlas data. J Neuro Oncol. 2017;133:27–35.\n 63. Codella N, Rotemberg V, Tschandl P , Celebi M.E, Dusza S, Gutman D, Helba B, Kalloo A, Liopyris K, Marchetti M, \net al. Skin lesion analysis toward melanoma detection 2018: a challenge hosted by the international skin imag-\ning collaboration (isic). arXiv preprint arXiv: 1902. 03368. 2019.\n 64. Wang H, Xie S, Lin L, Iwamoto Y, Han X.-H, Chen Y.-W, Tong R. Mixed transformer u-net for medical image \nsegmentation. In: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing \n(ICASSP), pp. 2390–2394. 2022. IEEE\n 65. Ho J, Kalchbrenner N, Weissenborn D, Salimans T. Axial attention in multidimensional transformers. arXiv pre -\nprint arXiv: 1912. 12180. 2019.\n 66. Guo M, Zhang Y, Liu T. Gaussian transformer: a lightweight approach for natural language inference. In: Proceed-\nings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 6489–6496. 2019.\n 67. Gao Y, Zhou M, Metaxas D.N. Utnet: a hybrid transformer architecture for medical image segmentation. In: \nMedical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, \nStrasbourg, France, September 27–October 1, 2021, Proceedings, Part III 24, pp. 61–71. 2021. Springer\n 68. Valanarasu J.M.J, Oza P , Hacihaliloglu I, Patel V.M. Medical transformer: Gated axial-attention for medical image \nsegmentation. In: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th Interna-\ntional Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I 24, pp. 36–46. 2021. \nSpringer\n 69. Yuan F, Zhang Z, Fang Z. An effective cnn and transformer complementary network for medical image segmen-\ntation. Pattern Recogn. 2023;136: 109228.\n 70. Ibtehaz N, Rahman MS. Multiresunet: rethinking the u-net architecture for multimodal biomedical image seg-\nmentation. Neural Netw. 2020;121:74–87.\n 71. Wang H, Cao P , Wang J, Zaiane O.R. Uctransnet: rethinking the skip connections in u-net from a channel-wise \nperspective with transformer. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, pp. \n2441–2449. 2022.\n 72. Sirinukunwattana K, Pluim JP , Chen H, Qi X, Heng P-A, Guo YB, Wang LY, Matuszewski BJ, Bruni E, Sanchez U, et al. \nGland segmentation in colon histology images: the glas challenge contest. Med Image Anal. 2017;35:489–502.\n 73. Kumar N, Verma R, Sharma S, Bhargava S, Vahadane A, Sethi A. A dataset and a technique for generalized nuclear \nsegmentation for computational pathology. IEEE Trans Med Imaging. 2017;36(7):1550–60.\n 74. Karimi D, Vasylechko S.D, Gholipour A. Convolution-free medical image segmentation using transformers. In: \nMedical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, \nStrasbourg, France, September 27–October 1, 2021, Proceedings, Part I 24, pp. 78–88. 2021. Springer\n 75. Cao H, Wang Y, Chen J, Jiang D, Zhang X, Tian Q, Wang M. Swin-unet: Unet-like pure transformer for medical \nimage segmentation. In: European Conference on Computer Vision, pp. 205–218. 2022. Springer\n 76. Liao Z, Xu K, Fan N. Swin transformer assisted prior attention network for medical image segmentation. In: \nProceedings of the 8th International Conference on Computing and Artificial Intelligence, pp. 491–497. 2022.\n 77. Lin A, Chen B, Xu J, Zhang Z, Lu G, Zhang D. Ds-transunet: dual swin transformer u-net for medical image seg-\nmentation. IEEE Trans Instrumentation Measure. 2022;71:1–15.\n 78. Murphy AH. The finley affair: a signal event in the history of forecast verification. Weather Forecasting. \n1996;11(1):3–20.\n 79. Hoover A, Kouznetsova V, Goldbaum M. Locating blood vessels in retinal images by piecewise threshold prob -\ning of a matched filter response. IEEE Trans Med Imaging. 2000;19(3):203–10.\n 80. Staal J, Abràmoff MD, Niemeijer M, Viergever MA, Van Ginneken B. Ridge-based vessel segmentation in color \nimages of the retina. IEEE Trans Med Imaging. 2004;23(4):501–9.\n 81. Ruggeri A, Scarpa F, De Luca M, Meltendorf C, Schroeter J. A system for the automatic estimation of morpho -\nmetric parameters of corneal endothelium in alizarine red-stained images. Br J Ophthalmol. 2010;94(5):643–7.\n 82. Fraz MM, Remagnino P , Hoppe A, Uyyanonvara B, Rudnicka AR, Owen CG, Barman SA. Blood vessel segmenta-\ntion methodologies in retinal images-a survey. Comput Methods Programs Biomed. 2012;108(1):407–33.\n 83. Budai A, Bock R, Maier A, Hornegger J, Michelson G, et al. Robust vessel segmentation in fundus images. Interna-\ntional Journal of biomedical imaging. 2013.\n 84. Caicedo JC, Goodman A, Karhohs KW, Cimini BA, Ackerman J, Haghighi M, Heng C, Becker T, Doan M, McQuin \nC, et al. Nucleus segmentation across imaging experiments: the 2018 data science bowl. Nat Methods. \n2019;16(12):1247–53.\nPage 22 of 22Pu et al. BioMedical Engineering OnLine           (2024) 23:14 \n 85. Naylor P , Laé M, Reyal F, Walter T. Segmentation of nuclei in histopathology images by deep regression of the \ndistance map. IEEE Trans Med Imaging. 2018;38(2):448–59.\n 86. Porwal P , Pachade S, Kamble R, Kokare M, Deshmukh G, Sahasrabuddhe V, Meriaudeau F. Indian diabetic retin-\nopathy image dataset (idrid): a database for diabetic retinopathy screening research. Data. 2018;3(3):25.\n 87. Li T, Gao Y, Wang K, Guo S, Liu H, Kang H. Diagnostic assessment of deep learning algorithms for diabetic retin-\nopathy screening. Inf Sci. 2019;501:511–22.\n 88. Gamper J, Alemi Koohbanani N, Benet K, Khuram A, Rajpoot N. Pannuke: an open pan-cancer histology dataset \nfor nuclei instance segmentation and classification. In: Digital Pathology: 15th European Congress, ECDP 2019, \nWarwick, UK, April 10–13, 2019, Proceedings 15, pp. 11–19. 2019. Springer\n 89. Valanarasu JMJ, Yasarla R, Wang P , Hacihaliloglu I, Patel VM. Learning to segment brain anatomy from 2d ultra-\nsound with less data. IEEE J Selected Topics Signal Process. 2020;14(6):1221–34.\n 90. Jha D, Smedsrud P .H, Riegler M.A, Halvorsen P , Lange T, Johansen D, Johansen H.D. Kvasir-seg: A segmented \npolyp dataset. In: MultiMedia Modeling: 26th International Conference, MMM 2020, Daejeon, South Korea, Janu-\nary 5–8, 2020, Proceedings, Part II 26, pp. 451–462. 2020. Springer\n 91. Zhang Y, Higashita R, Fu H, Xu Y, Zhang Y, Liu H, Zhang J, Liu J. A multi-branch hybrid transformer network for \ncorneal endothelial cell segmentation. In: Medical Image Computing and Computer Assisted Intervention–MIC-\nCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I \n24, pp. 99–108. 2021. Springer\n 92. Litjens G, Toth R, Van De Ven W, Hoeks C, Kerkstra S, Van Ginneken B, Vincent G, Guillard G, Birbeck N, Zhang \nJ, et al. Evaluation of prostate segmentation algorithms for mri: the promise12 challenge. Med Image Anal. \n2014;18(2):359–73.\n 93. Bernard O, Lalande A, Zotti C, Cervenansky F, Yang X, Heng P-A, Cetin I, Lekadir K, Camara O, Ballester MAG, et al. \nDeep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is the problem \nsolved? IEEE Trans Med Imaging. 2018;37(11):2514–25.\n 94. Hatamizadeh A, Terzopoulos D, Myronenko A. End-to-end boundary aware networks for medical image segmenta-\ntion. In: Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with \nMICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings 10, pp. 187–194. 2019. Springer\n 95. Heller N, Sathianathen N, Kalapara A, Walczak E, Moore K, Kaluzniak H, Rosenberg J, Blake P , Rengel Z, Oestreich M, \net al. The kits19 challenge data: 300 kidney tumor cases with clinical context, ct semantic segmentations, and surgi-\ncal outcomes. arXiv preprint arXiv: 1904. 00445. 2019.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6705631613731384
    },
    {
      "name": "Segmentation",
      "score": 0.6520159840583801
    },
    {
      "name": "Transformer",
      "score": 0.6377649903297424
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5318289399147034
    },
    {
      "name": "Image segmentation",
      "score": 0.5184730887413025
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4686393439769745
    },
    {
      "name": "Computer vision",
      "score": 0.39231887459754944
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32993417978286743
    },
    {
      "name": "Engineering",
      "score": 0.15545108914375305
    },
    {
      "name": "Electrical engineering",
      "score": 0.09008032083511353
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145897649",
      "name": "Minzu University of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2802939634",
      "name": "Chinese PLA General Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210137180",
      "name": "Institute of High Energy Physics",
      "country": "CN"
    }
  ]
}