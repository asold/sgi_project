{
  "title": "Rethinking Positional Encoding in Tree Transformer for Code Representation",
  "url": "https://openalex.org/W4385573046",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5103968620",
      "name": "Han Peng",
      "affiliations": [
        "Institute of Software",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5076676389",
      "name": "Ge Li",
      "affiliations": [
        "Institute of Software",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5101584327",
      "name": "Yunfei Zhao",
      "affiliations": [
        "Institute of Software",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5049100391",
      "name": "Zhi Jin",
      "affiliations": [
        "Institute of Software",
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2795143051",
    "https://openalex.org/W3174413662",
    "https://openalex.org/W2514538448",
    "https://openalex.org/W3167812602",
    "https://openalex.org/W3177492177",
    "https://openalex.org/W3122515622",
    "https://openalex.org/W2497764072",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W4287123803",
    "https://openalex.org/W3208494144",
    "https://openalex.org/W2996086147",
    "https://openalex.org/W4372046852",
    "https://openalex.org/W2964150020",
    "https://openalex.org/W4206482253",
    "https://openalex.org/W3162689995",
    "https://openalex.org/W2983902802",
    "https://openalex.org/W3209632425",
    "https://openalex.org/W2282866165",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2962995178",
    "https://openalex.org/W2516621648",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W4295728955",
    "https://openalex.org/W3125488228",
    "https://openalex.org/W2997847174",
    "https://openalex.org/W3099302725",
    "https://openalex.org/W3008282111",
    "https://openalex.org/W2887364112",
    "https://openalex.org/W2979792666",
    "https://openalex.org/W3212771748",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3173854146",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2970550739",
    "https://openalex.org/W4285637679",
    "https://openalex.org/W2963499994",
    "https://openalex.org/W2971351900",
    "https://openalex.org/W3193794505",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2971270287",
    "https://openalex.org/W3211394146"
  ],
  "abstract": "Transformers are now widely used in code representation, and several recent works further develop tree Transformers to capture the syntactic structure in source code. Specifically, novel tree positional encodings have been proposed to incorporate inductive bias into Transformer.In this work, we propose a novel tree Transformer encoding node positions based on our new description method for tree structures.Technically, local and global soft bias shown in previous works is both introduced as positional encodings of our Transformer model.Our model finally outperforms strong baselines on code summarization and completion tasks across two languages, demonstrating our model's effectiveness.Besides, extensive experiments and ablation study shows that combining both local and global paradigms is still helpful in improving model performance. We release our code at https://github.com/AwdHanPeng/TreeTransformer.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3204–3214\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nRethinking Positional Encoding in Tree Transformer\nfor Code Representation\nHan Peng, Ge Li∗, Yunfei Zhao, Zhi Jin∗\nKey Laboratory of High Confidence Software Technologies (Peking University),\nMinistry of Education; Institute of Software, EECS, Peking University, Beijing, China\n{phan, lige, zhaoyunfei, zhijin}@pku.edu.cn\nAbstract\nTransformers are now widely used in code rep-\nresentation, and several recent works further\ndevelop tree Transformers to capture the syn-\ntactic structure in source code. Specifically,\nnovel tree positional encodings have been pro-\nposed to incorporate inductive bias into Trans-\nformer. In this work, we propose a novel tree\nTransformer encoding node positions based\non our new description method for tree struc-\ntures. Technically, local and global soft bias\nshown in previous works is both introduced\nas positional encodings of our Transformer\nmodel. Our model finally outperforms strong\nbaselines on code summarization and comple-\ntion tasks across two languages, demonstrat-\ning our model’s effectiveness. Besides, exten-\nsive experiments and ablation study shows that\ncombining both local and global paradigms is\nstill helpful in improving model performance.\nWe release our code at https://github.com/\nAwdHanPeng/TreeTransformer.\n1 Introduction\nMachine learning for source code aims to learn the\nsemantic embedding of programs. Due to the for-\nmat similarity between code and text (Hindle et al.,\n2016), Transformers (Vaswani et al., 2017) are now\nwidely used in code representation (Hellendoorn\net al., 2019; Zügner et al., 2021; Peng et al., 2021).\nUnlike natural language, source code is more logi-\ncal and has rich structures such as abstract syntax\ntrees (AST). Therefore, one research topic of code\nintelligence is representing the syntax tree of code.\nSeveral recent works proposed novel tree-based\nTransformers by defining the position of each node\nto handle tree structure (Shiv and Quirk, 2019; Kim\net al., 2020). In this work, we pursue the research\nline of tree Transformer for learning code AST.\nIn Transformer, positional encoding is crucial to\nexploit potential structures of data (such as code\n∗Corresponding authors\nASTs or graphs) as other components are entirely\nposition-invariant. Recently, a growing research\ntrend is adapting Transformer to more complex\nstructured data than plain text by modifying posi-\ntional encoding, not only in language processing\n(Wang et al., 2019b; Nguyen et al., 2020) but also\nin graph representation learning field (Dwivedi and\nBresson, 2020; Mialon et al., 2021). As for the\nstructure-aware Transformers, the key step of en-\ncoding positions is to find a proper description\nfor input structure, which means abstracting the\nphysical structure of data into a suitable math-\nematical form. For example, the vanilla Trans-\nformer (Vaswani et al., 2017) regards the poten-\ntial text order in natural languages as the arrange-\nment of natural numbers, while the graph Trans-\nformers (Kreuzer et al., 2021; Dwivedi and Bres-\nson, 2020) treat the positional relationship between\ngraph nodes as the adjacent matrix or Laplacian fur-\nther. Intuitively, a good description should be infor-\nmation lossless from which the whole structure of\nthe original data could be precisely reconstructed.\nRecently, several tree-based Transformers incor-\nporated with advanced positional encoding are pre-\nsented to process the code syntax tree. (Shiv and\nQuirk, 2019) represented the position of each node\nusing sibling orders of all nodes that existed in\nits path to the root, while (Kim et al., 2020) de-\nfined the relative distance between two nodes as\nthe traversing up and down steps along the shortest\npath connecting them. However, some tree struc-\ntures are overlooked by these previous approaches:\nthe first method assumes the regular tree with a\nfixed number of node children, and the second ig-\nnores the sibling feature in traversing. In this paper,\nwe present a new description method for node po-\nsitions from which the corresponding tree can be\nrebuilt without ambiguity. Specifically, the posi-\ntion of each node is recursively described as a list\nincluding multiple 2D coordinates. The coordinate\nlist of each node first inherits from its parent and\n3204\nthen includes a new 2D coordinate of itself, where\nthe first dimension indicates its sibling order and\nthe second is the total child number of its parent.\nWe incorporate the proposed description method\ninto Transformer, powering it to capture tree struc-\ntures. Technically, a growing popular approach is\nto encode structure as soft inductive bias in posi-\ntional encodings of Transformer, in which atten-\ntion between all nodes is allowed rather than the\nstrict aspect of message passing. To the best of our\nknowledge, the soft bias in previous works is usu-\nally introduced either in local or global. The local\nmethods integrate the structure relation as one-hop\nedges only for two adjacent nodes (Hellendoorn\net al., 2019; Li et al., 2020), so each node knows\nits multi-hop subgraph only by stacking model lay-\ners. In global approaches, inductive bias is injected\ninto the attention between any nodes regardless of\nwhether adjacent in trees or graphs (Xu et al., 2020;\nWang et al., 2019a), in which structures can perco-\nlate fully across graphs in a single layer (Shiv and\nQuirk, 2019).\nThe local and global methods show expressive-\nness in previous works, but the relationship be-\ntween them is still not completely studied to the\nbest of our knowledge. In this paper, we propose\na new tree Transformer that integrates our tree de-\nscription in local and global, exploring the interac-\ntion between local and global bias. Our model fi-\nnally outperforms solid baselines and obtains state-\nof-the-art in code summarization and completion\ntasks across two different language datasets. Be-\nsides, the ablation results show that both global and\nlocal methods are powerful, and the combination\nimproves model performance further. The contri-\nbutions of this paper are summarized as follows:\n1. We propose a novel tree Transformer which\nsignificantly outperforms existing baselines\nacross different languages and tasks.\n2. We present a new description method for tree\nnode positions from which tree structure can\nbe reconstructed precisely.\n3. We explore the relationship between the local\nand global bias proposed in previous works,\nshedding light on future work.\n2 Related Work\nRepresentation learning for source codeThe\navailability of big code shows opportunities for\nrepresentation learning of programs. Tradition-\nally, code intelligence designers have relied pre-\ndominantly on structure or context. Early research\nworks relied on raw text data for code snippets\n(Dam et al., 2016; Wang et al., 2016; Allamanis\net al., 2016; Iyer et al., 2016), mainly focusing\non context and struggle to capture code structure.\nAfter that, a growing active research topic is to\nrepresent the syntax tree structure of code. (Mou\net al., 2016) proposed tree-based convolutional neu-\nral networks and (Alon et al., 2018, 2019) treated\ncodes as weighted combination of pairwise paths\nin AST. (Shiv and Quirk, 2019) proposed a custom\npositional encoding to extend Transformers to tree-\nstructured data. (Kim et al., 2020) defines the rela-\ntive distance on the tree as the shorted path between\nnodes consisting of up and down steps. Our work\npursues the research line to model code trees, pow-\nering Transformer to learning AST by integrating\ntree positional encodings. Besides, several works\nalso explored leveraging different code representa-\ntions jointly, including context, AST structure and\nother code graphs. (Allamanis et al., 2017) pro-\nposed GGNN to represent program graphs consist-\ning of AST with control-flow and data-flow. (Hel-\nlendoorn et al., 2019; Zügner et al., 2021; Peng\net al., 2021) proposed to learn structure and context\ntogether by introducing bias in the self-attention of\ncode context with the underlying tree structure.\nStructure-aware Transformers The Trans-\nformer model (Vaswani et al., 2017) is the most\nwidely used architecture in language representation\nlearning. Several works have recently explored\nextending Transformer from plain text to structural\ndata such as graphs and trees. Technically, two\napproaches exist to integrate inductive bias in\nTransformer: the hard or soft methods. The hard-\ncoded methods usually use the mask to restrict\nthe attention only to adjacent nodes in graphs or\ntrees (Gao et al., 2021; Wu et al., 2020), that is,\nthe GNN-like message-passing paradigm exists\ntherein. However, there is a growing recognition\nthat inherent limitations exist in message passing,\nsuch as over-smoothing and over-squashing\n(Hamilton, 2020; Alon and Yahav, 2020; Kreuzer\net al., 2021). More recently, a growing interest\nin deep learning is to encode structure as soft\ninductive bias toward more flexible architectures,\nsuch as positional encodings in Transformer. For\nexample, (Mialon et al., 2021) leveraged relative\npositional encoding in self-attention based on\n3205\n(a)\n (b)\nFigure 1: Example of a tree and position description for each node therein. A virtual node is added as the parent of\neach tree node for the sake of description. The position of each node is presented recursively as a list containing\nmultiple 2D coordinates defined in Eq.1.\npositive definite kernels on graphs and (Zügner\net al., 2021; Ying et al., 2021) incorporate relations\nsuch as shortest path distance in Transformer. We\nfollow this research line and explore encoding\ncode AST by integrating tree positional encoding\nin Transformer as soft inductive bias. Besides,\nas discussed in the previous section, we further\ndivide the method of introducing soft bias into\nlocal and global approaches, and integrate these\ntwo paradigms into our proposed model.\nPositional encoding for TransformerThe ab-\nsolute positional encoding in vanilla Transformer\nis presented to capture the potential orders of se-\nquential text. After that, (Shaw et al., 2018) firstly\nproposed the relative positional encoding to Trans-\nformer. Transformer-XL (Dai et al., 2019) then\nre-parameterized the relative positional encoding\nof self-attention and T5 (Raffel et al., 2019) simpli-\nfied the vector representation of relative positions\nin (Shaw et al., 2018) to scalars. More recently, (Ke\net al., 2020; He et al., 2020) proposed the disentan-\ngled attention mechanism for positional encoding,\nshowing the irrationality of adding and applying\nthe same projection for position and word embed-\nding. The mechanism of untied positional encoding\nshows effectiveness in the natural language process\n(Tsai et al., 2019; Chen et al., 2021a). In this paper,\nwe adopt the idea of the disentangled attention of\nTransformer and apply it in our model, proving\nstill useful in encoding positions for complex tree\nstructures.\n3 Approach\nIn this section, we present our tree Transformer in\ntwo parts. We first show the novel two-dimensional\ndescription for trees, by which each node’s position\nis converted as a coordinate list. After that, we\nembed position from the description for each node\nand then integrate position encoding in the self-\nattention of Transformer in local and global.\n3.1 A 2D recursive description for code AST\nOur proposed description for tree structure is\ndefined from the tree root to leaves recursively.\nSpecifically, the position for each node is repre-\nsented as:\nF(x) =\n{F(f(x)) +{(xi,xj)} if x̸= root\n{(1,1)} if x= root\n(1)\nIn Eq.1, F(x) is the position description for node\nxincluding multiply coordinates and f(x) is spec-\nified as the parent node for it. It is clearly seen\nthat F(x) for node xis first inherits from its par-\nent F(f(x)). After that, a new 2D coordinate is\npushed behind the list, in which the first dimension\nis the sibling order of node xand the second is the\ntotal child number of its parent f(x). The special\ncase is for the tree root because no parent exists. So\nwe add a virtual node as the root’s parent, which\nis also commonly seen in the classical algorithms\nfor trees (Cormen et al., 2022). A clear example of\nEq.1 is shown in Fig.1.\nEach 2D coordinate is then converted to a vector\nby the lookup embedding table. In this process, a\nsample way is first to map each 2D coordinate into\na scalar and then retrieve the vector by it. Another\nmethod is embedding each dimension first and then\nadding (or concat) two vectors. Since experiments\nshow no significant difference, we finally pick the\nfirst approach. After that, the vector sequence Hi\nfor F(i) is represented as:\nHi = [h(i1),h(i2),...,h (in)], (2)\nwhere nis the depth of node iin the tree and h(in)\n3206\nis the embedding vector of the nthcoordinate in\nthe list.\n3.2 Encoding tree positions in local and global\nFeeding the tree into Transformer requires a lin-\nearization method to convert it into a node sequence\nfirst. Since the position feature of each node is\nalready represented as the corresponding vector se-\nquence by our tree description, any AST lineariza-\ntion method can be picked for our model. After that,\nwe feed all nodes into Transformer and integrate\nthe position vectors in self-attention by positional\nencoding.\n3.2.1 Self-attention and positional encoding\nSelf-attention is one of the key modules of Trans-\nformer and can be formulated as querying the\nkey-value pairs. We omit the index of layer for\nsimplicity and denote x = (x1,x2 ··· ,xn) and\nz= (z1,z2 ··· ,zn) as the input and output of self-\nattention in the same layer respectively, where nis\nthe sequence length. The self-attention is presented\nas:\nαij = (xiWQ)(xjWK)T ,\nzi =\nn∑\nj=1\nexp (αij)∑n\nj′=1 exp (αij′)(xjWV ), (3)\nwhere WQ,WK ∈Rdx×dk , WV ∈Rdx×dv is the\nprojection matrices for query, key and value, re-\nspectively. We set dk = dv = d. Note that a\nscaling factor 1√\nd should be applied for attention\nscore αij before softmax and we just omit it for the\nsake of description.\nThe self-attention in Eq.3 is oblivious to struc-\ntured input because it effectively views it as an\nunordered set of vectors. In NLP, a common way\nto bias Transformer towards potential text order is\nto add positional encodings. The original Trans-\nformer adds the absolute sinusoidal positional en-\ncoding to the token embeddings. After that, (Shaw\net al., 2018) proposed the first relative positional\nencoding, in which the real-valued vector repre-\nsented relative distance is added to the key before\nthe dot-product between the query and key. More\nrecently, several works (Ke et al., 2020; He et al.,\n2020) proposed that disentangled attention is better\nthan adding and applying the same projection for\nposition and word embedding. The untied absolute\npositional encoding proposed by (Ke et al., 2020)\nis presented as:\nαABS\nij = 1√\n2[(aiWQ\na )(ajWK\na )T + αij], (4)\nwhile the disentangled relative positional encoding\npresented in (He et al., 2020) is:\nαREL\nij = 1√\n3[(xiWQ)(rijWK\nr )T\n+(rjiWQ\nr )(xjWK)T + αij],\n(5)\nwhere ai,aj are the absolute position embedding\nfor position iand j, and rij,rji are viewed as the\nrelative position embedding between two positions.\nWQ\na ,WK\na ,WK\nr ,WQ\nr ∈Rdx×dk are projection ma-\ntrices for absolute and relative position encodings,\nand scaling factors 1√\n2 and 1√\n3 are applied to retain\nmagnitudes. In conclusion, the attention score of\nword embedding αij presented in Eq.3 is added\nwith the absolute and relative positional attention\nscore in Eq.4-5, respectively. After that, the atten-\ntion score knowing sequential orders is used to the\nweighted sum for values.\n3.2.2 Attention with tree structure\nWe modify the untied positional encoding in Eq.4-\n5 to learn code AST structure. The Eq.4 and Eq.5\ncan both efficiently capture the global positional\ninformation in natural languages since both abso-\nlute positions and relative distances in texts are\ntractable, ranging in max length of 512 commonly.\nAs for trees, the absolute position in the tree for\neach node can be easily drawn from our tree de-\nscription, shown in the following details. However,\nit is not trivial to learn relative global relationships\nbetween tree nodes since all cases of structure re-\nlation are intractable in O(n2) where nis the code\nlength. This sticking point is alleviated by only\nmodeling relative distances in trees, such as short-\nest path distances (Zügner et al., 2021). However,\ntree structures can not be entirely exploited by dis-\ntance alone (Peng et al., 2021). Another solution\nfor this crux is only modeling unique relative paths\nin ASTs (Peng et al., 2021), but the feature cover-\nage is still not guaranteed in theory.\nOn the other side, previous works (Hellendoorn\net al., 2019; Chen et al., 2021b) have proved that in-\ntroducing local bias as relative one-hop edges only\nbetween adjacent nodes is still powerful to model\ntree structure. The local methods show a different\nparadigm compared to global approaches, so the\nintuitive idea is to integrate the local and global\n3207\nmethods. For these reasons, we do not pursue cap-\nturing the relative global position but only the local\none in this paper. In conclusion, we introduce the\nglobal bias by absolute encoding and local bias by\nrelative encoding and then integrate them into the\nunified Transformer.\nThe absolute position vector ai ∈Rdx for node\niis presented as:\nai = LN(Linear(Concat(Hi))), (6)\nwhere we concat vectors in list Hi of node i se-\nquentially and feed it into transforming linear and\nnormalization layers. We pad zero vectors for short\nH lists to max tree depth and truncate last for long\nlists before concat vectors.\nThe relative position vector rij ∈Rdx between\nnode iand jis:\nrij =\n\n\n\nLN(Linear(∑\nh\nHi −∑\nh\nHj))\nif f(i) =j ∨f(j) =i\n⃗0 if f(i) ̸= j ∧f(j) ̸= i\n(7)\nIn Eq.7, f(.) is specified as the node’s parent. For\nexample, given nodexand one of its childreny, we\nsum the vector lists for these two nodes respectively\nand subtract two sum vectors. Thus, the subtrac-\ntion vector fed into the linear layer actually is the\nembedding of coordinate (yi,yj) defined by Eq.1,\nand all cases of it are tractable inO(m) where mis\nthe size of coordinate embedding table. The linear\nand normalization layers of relative vectors have\ndifferent parameters from the absolute ones in Eq.6.\nThe relation vector is set as zero if there is no adja-\ncency between two nodes, and obviously, only the\nlocal one-hop relationship is actually embedded in\nthis encoding process.\nWe first introduce the global absolute position en-\ncoding into the self-attention of Transformer. The\nattention score βij between absolute positions of\nnode iand jis presented as:\nβij = (aiWQ\na )(ajWK\na )T , (8)\nwhere WQ\na ,WK\na ∈Rdx×dk are projection matrices.\nAfter that, the local relative position attention score\nγij is presented as:\nγij = (xiWQ)(rijWK\nr )T\n+ (rjiWQ\nr )(xjWK)T (9)\nwhere WQ\nr ,WK\nr ∈Rdx×dk are projection matrices\nfor the query and key.\nWe integrate βij and γij into Transformer,\nadding to the attention score αij of word embed-\ndings:\nAij = 1√\n2(αij + βij + γij),\nzi =\nn∑\nj=1\nexp (Aij)∑n\nj′=1 exp (Aij′)(xjWV ).\n(10)\nAlthough four vector dot-product exist in Aij, we\napply the scaling factors as 1√\n2 rather than 1√\n4 since\nthe number of non-zero γi_ for node iis only the\nnumber of its adjacent node and far less than the\nlength of input node sequence.\n3.3 Discussion for tree positional encodings\nOur absolute global position encoding for each\nnode extracts structure features from the positional\ndescription F(.). All nodes’ absolute position vec-\ntors are scattered in the structural space with vir-\ntual nodes as the origin. Comparing F(.) of two\nnodes, a path including a series of steps along tree\nbranches shows clearly. Thus, our model has the po-\ntential to capture the pairwise path by dot-products\nin Eq.8 of two position vectors. The main dif-\nferences of our absolute positional encoding com-\npared to the approach presented in (Shiv and Quirk,\n2019) are two folds: firstly, we describe each node\nposition by two dimensions, including not only\nthe sibling orders but the child number of its par-\nent, while (Shiv and Quirk, 2019) only consider\nthe first; secondly, we embed each coordinate into\nvector directly and integrate it into the disentan-\ngled attention, while (Shiv and Quirk, 2019) pa-\nrameterize the one-hot concat of sibling orders and\nadd the position vector to word embeddings before\nfeeding into Transformer. Therefore, our absolute\npositional encoding can be seen as the advanced\ngeneralization of (Shiv and Quirk, 2019).\nOur proposed relative positional encoding focus\non the local structural relation between adjacent\nnodes. The relative position vector presented in\nEq.7 contains many structural features: firstly, the\nasymmetric of rij and rji reveals the parent-child\nrelationship and vice versa; secondly, two dimen-\nsions of coordinates are embedded in relative vec-\ntors. Note that our method is different from GREAT\nproposed in (Hellendoorn et al., 2019). In GREAT,\neach parent node knows its children only by two\ntypes of edges: parent and child edges (Chirkova\nand Troshin, 2021), which means the parent does\nnot know the sibling order of its children. Thus,\n3208\nour proposed local relative positional encoding can\nbe viewed as the extension of GREAT.\nIn both GREAT and our local module, each node\nlearns structure only from its adjacent nodes, and\nits receptive field for structure extends only by\nstacking model layers. The local method of in-\ntroducing bias differs from the global approaches\npresented in (Shiv and Quirk, 2019) and our abso-\nlute position encoding. In this work, we integrate\nthe global and local methods and further analyze\ntheir relationship. See Fig2 for explanation to our\nmodel.\nFigure 2: Example of a simple tree. We assign each\nnode’s global position by absolute encoding and intro-\nduce the local relationship between adjacent nodes as\nrelative one-hop edges.\n4 Experiment setup\nWe focus on two tasks of code representation learn-\ning: code summarization and completion. Code\nsummarization is one of the most popular tasks\nin which the function name is predicted given a\nfunction body. The method body typically forms\ncomplete logical units, and the function name tends\nto be precisely descriptive. Therefore, code sum-\nmarization is widely used as benchmark by several\nprevious works (Allamanis et al., 2016; Alon et al.,\n2018; Zügner et al., 2021; Peng et al., 2021). The\ncode completion is another useful benchmark, and\nwe mainly focus on the completion for each tree\nnode (Li et al., 2017; Sun et al., 2020; Kim et al.,\n2020). In this task, AST is linearized in depth-first\norder for all baselines. Each node is then predicted\ngiven the partial tree built on all the previous nodes\nin depth-first order.\nIn both summarization and completion tasks, we\nuse the Python150k and JavaScript150k datasets1\npreprocessed by (Chirkova and Troshin, 2021).\nBoth datasets consist of program files from Github\n2 and are widely used to evaluate code represen-\n1https://eth-sri.github.io\n2https://github.com\ntation models. To avoid biased results, duplicate\nfiles are removed by the duplication list provided\nby (Allamanis, 2019), and identical code files and\nfunctions are further filtered out by (Chirkova and\nTroshin, 2021). Both datasets are split into train-\ning/validation/testing sets with 60%/6.7%/33.3%\nbased on GitHub usernames.\nIn this paper, we mainly compare our model with\ntree-based Transformer models. Before feeding\ntrees into all compared Transformers, we linearize\nASTs in depth-first order and convert them as node\nsequences. Note that each node in AST is asso-\nciated with a type, but not all nodes have values\n(such as non-leaf nodes in trees). Therefore, a spe-\ncial <empty> value is associated with nodes that\ndo not have values. The type and value embeddings\nare added as input node features fed into models.\nFor all compared models in both tasks, node to-\nkens are not split into subtokens for computational\nefficiency following (Chirkova and Troshin, 2021).\n4.1 Code summarization\nIn this task, seq2seq models decode function names\naccording to function ASTs. Sequential positional\nembedding is used in Transformer decoder to cap-\nture the order of function names. All top-level func-\ntions short than 250 AST nodes are selected from\nfiltered files. After extracting functions from code\nfiles, we replace function names with the special\n<function_name> token and split target function\nnames based on CamelCase or snake_case. The\nfinal datasets include 523k/56k/264k functions for\npython and 186k/23k/93k for javascript to train-\ning/validation/testing. Multiple metrics are used to\ncomprehensively measure the quality of generated\nfunction names, including Bleu (Papineni et al.,\n2002), F1 and Acc. The generated function name\nis viewed as a token list for Bleu while seen as an\nunordered set for F1 and Acc.\n4.2 Code completion\nIn this task, Transformer decoders with masked\nattention are used to predict each node given all\nprevious nodes in depth-first order. Two linear\nlayers with softmax are set on top of Transformer\ndecoder to predict both value and type of the next\nnode. We use full ASTs of filtered files except\nfor sequences less than 2. We split larger ASTs\nlonger than 500 into overlapping chunks with a\nshift of 250. The overlap in each chunk provides a\ncontext for models, and we count loss and met-\nrics over the intersection between chunks only\n3209\nonce. The details of this part are also presented\nin (Chirkova and Troshin, 2021; Kim et al., 2020).\nThe final datasets include 186k/20k/100k chunks\nfor python and 270k/32k/220k for javascript to\ntraining/validation/testing. In this task, we mea-\nsure MRR(mean reciprocal rank) and Acc for type\nand value, respectively. We assign the zero score\nfor MRR if the correct token is out of the top 10\ncandidates. Besides, we measure the combined\nAcc(all) and assign true only when both value and\ntype are correct for each node.\n4.3 Hyperparameters\nOur model and Transformer baselines all have 6\nlayers, 8 heads, hidden size D = 512 and FFN\ndimension DFF = 2048. The vocabulary sizes\nfor values are 50K/100K for summarization and\ncompletion tasks respectively, and all types are\npreserved. We train all Transformers using Adam\n(Kingma and Ba, 2014) with a starting learning\nrate of 1e−4 and batch size of 32. We train all\nmodels with 20/60 epochs for summarization and\n20/20 epochs for completion in python/javascript.\nIn summarization, we decay the learning rate by 0.9\nin python, use a constant learning rate in javascript,\nand a gradient clipping of 5 for both languages.\nWe use the cosine learning rate schedule with 2k\nwarmup steps with a zero minimal learning rate\nin completion. In conclusion, the training settings\nmostly follow (Chirkova and Troshin, 2021) but\nwith slight differences.\nIn our model, we set the maximum children num-\nber for each tree as 16 for all tasks and languages,\nmeaning each dimension in 2D coordinates ranges\nfrom 1 to 16. The coordinate beyond the upper limit\nis set as the maximum of 16. Therefore, the size of\nthe coordinate embedding table is 16(16+1)\n2 = 136\nsince the first dimension of 2D coordinates always\nless than or equal to the second one. We set the em-\nbedding dimension 32/32 for summarization and\n32/16 for completion in python/javascript, respec-\ntively. Thus, the embedding table for coordinates\nhas very few parameters. Besides, we set the max\ntree depth as 16/16 for summarization and 16/32 for\ncompletion in python/javascript. The hyperparame-\nter setting of maximum children and depth covers\nalmost all samples. In our implementation, the po-\nsitional vectors aand rare shared across different\nheads, while the projection matrices in Eq.8-9 are\ndifferent for all heads. These matrices are shared\nin different layers for efficiency, which means we\ncalculate the positional attention score only once.\nIn summary, we introduce only about 1.3M new\nparameters, which is insignificant compared to the\nfull parameters of Transformer architecture.\n4.4 Baselines\nFour different Transformers are picked as base-\nlines of our model, including vanilla Transformer\n(Vaswani et al., 2017), Transformer with relative\npositional encoding presented in (Shaw et al., 2018)\nand the tree-based Transformers propose by (Shiv\nand Quirk, 2019) and (Kim et al., 2020). Before\nfeeding trees into models, ASTs are uniformly lin-\nearized into node sequences in depth-first order.\nTherefore, although the former two Transformers\nare designed initially for natural language, they still\nhave the potential to capture tree structure from the\ndepth-first order. The Transformers in (Shiv and\nQuirk, 2019) and (Kim et al., 2020) learn tree struc-\nture by different positional encoding for trees rather\nthan traversal orders, similar to our model. All mod-\nels are trained three times to estimate mean±std on\none Tesla V100 GPU, and we set all hyperparame-\nters of baselines following the best practices shown\nin (Chirkova and Troshin, 2021).\nWe do not introduce GREAT shown in (Hellen-\ndoorn et al., 2019) as one of the baselines following\n(Chirkova and Troshin, 2021), also since our model\ncan be seen as the generalization of it discussed\nin section 3.3. The Transformer models jointly\nlearn from both context and structure (Zügner et al.,\n2021; Peng et al., 2021) are also not introduced as\nour baselines. Although it is already proven that\ncombined multiply modality is helpful for represen-\ntation learning of code, we firmly believe that it is\nstill meaningful to explore tree-based Transformer\nonly for AST structure.\n5 Results\n5.1 Overall comparison\nThe overall results of code completion and summa-\nrization tasks are shown in Table 1 and 2, respec-\ntively. Note that since slight training differences,\nthe result of baselines are not entirely consistent\nwith (Chirkova and Troshin, 2021), but the overall\ntrend is almost identical.\nOur model substantially outperforms baselines\nalmost in all metrics of both tasks. Firstly, our\nmodel outperforms baselines for completion task\nof different languages in almost all metrics shown\nin Table 1. The only exception is to predict node\n3210\nModel Python JavaScript\nMRR\n(Type)\nMRR\n(Value)\nACC\n(Type)\nACC\n(Value)\nACC\n(All)\nMRR\n(Type)\nMRR\n(Value)\nACC\n(Type)\nACC\n(Value)\nACC\n(All)\nTransformer 88.76 54.28 81.91 49.56 59.27 89.51 62.73 82.73 57.40 63.34\n±0.09 ±0.02 ±0.13 ±0.06 ±0.11 ±0.01 ±0.02 ±0.01 ±0.02 ±0.03\n(Shaw et al., 2018) 89.05 55.11 82.34 50.48 60.07 90.07 64.13 83.60 59.09 64.80\n±0.04 ±0.04 ±0.06 ±0.08 ±0.14 ±0.03 ±0.05 ±0.05 ±0.09 ±0.14\n(Shiv and Quirk, 2019)88.74 54.24 81.81 49.40 59.14 89.78 62.90 83.10 57.48 63.64\n±0.10 ±0.20 ±0.14 ±0.26 ±0.21 ±0.10 ±0.15 ±0.15 ±0.20 ±0.15\n(Kim et al., 2020) 91.26 54.79 85.64 50.04 62.15 91.21 63.49 85.12 58.24 65.42\n±0.03 ±0.02 ±0.05 ±0.02 ±0.02 ±0.04 ±0.09 ±0.06 ±0.10 ±0.11\nOur model 91.58 55.62 86.15 51.19 63.19 91.63 64.08 85.91 58.99 66.48\n±0.08 ±0.06 ±0.12 ±0.13 ±0.13 ±0.03 ±0.08 ±0.06 ±0.13 ±0.14\nw/o first dim. 88.81 54.63 81.90 49.86 59.36 89.91 63.25 83.28 57.97 64.02\n±0.11 ±0.09 ±0.17 ±0.14 ±0.26 ±0.06 ±0.22 ±0.10 ±0.28 ±0.29\nw/o second dim. 90.78 55.10 84.87 50.52 62.04 91.08 63.50 85.02 58.30 65.47\n±0.02 ±0.08 ±0.02 ±0.10 ±0.07 ±0.06 ±0.10 ±0.10 ±0.13 ±0.17\nw/o Global-Abs. 91.40 55.25 85.88 50.68 62.70 91.57 63.95 85.84 58.84 66.34\n±0.09 ±0.15 ±0.13 ±0.21 ±0.23 ±0.03 ±0.09 ±0.04 ±0.13 ±0.11\nw/o Local-Rel. 91.35 55.36 85.81 50.81 62.75 91.18 63.51 85.25 58.28 65.67\n±0.07 ±0.03 ±0.12 ±0.08 ±0.15 ±0.07 ±0.15 ±0.11 ±0.18 ±0.12\nTable 1: All results on code completion\nvalue in javascript, while our model still gains a\ncomparable performance to (Shaw et al., 2018).\nInterestingly, some baselines only perform well in\npredicting either type or value. For example, the rel-\native sequential Transformer in (Shaw et al., 2018)\nperforms best to predict node value in javascript\nbut is poor for node type. Thus, we additionally\ncalculate the Acc(all) to measure whether models\naccurately predict both type and value simultane-\nously. The final result highlights the effectiveness\nof our model for predicting both type and value.\nSecondly, our model shows effectiveness in sum-\nmarization task for both languages in Table 2. It\nis worth noting that although baselines perform\nsimilarly, consisting of observations in (Chirkova\nand Troshin, 2021), our model still improves from\nthem. It may indicate that previous works do not\nlearn code AST structure well and therefore meet\nthe performance bottleneck in this task.\n5.2 Ablation study\nWe explore the roles of each part of our approach,\nincluding two dimensions in each coordinate and\nglobal/local modules to introduce structural bias.\nTwo dimensions of tree description Our ap-\nproach describes each node’s position as a 2D co-\nordinate list. To verify the benefit of learning from\nboth dimensions, we consider removing one di-\nmension of each coordinate before feeding it into\nTransformer. The ablation results for each dimen-\nsion on different tasks are shown in Table 1 and 2.\nWe find that model’s performance shows noticeable\ndeclines after removing either dimension, proving\nthat all dimensions are helpful.\nGlobal and local encoding for tree positionOur\nmodel introduces global and local soft bias to en-\ncode tree structures. To explore the roles of each\npart, we remove either global or local modules in\nself-attention of Transformer, which means only\nadding either β or γ to the attention score α of\nword embedding in Eq.10. The ablation results\nare shown in Table 1 and 2. We find that both en-\ncoding methods are powerful for each task: only\nglobal or local method already outperforms almost\nall baselines. The comparison again proves that 2D\ndescription for trees is beneficial and also shows\nthat the disentangled attention mechanism shown\nin NLP is still helpful in modelling complex tree\npositions. We then compare the performance of\neach part to the whole model and find that model’s\nperformance improves further than the single lo-\ncal or global component. For this phenomenon,\nwe speculate that since the local method extracts\nthe relationship between nodes by strictly stacking\nmodel layers, local features are less likely to be lost.\n3211\nModel Python JavaScript\nBleu F1 Acc Bleu F1 Acc\nTransformer 56.29±0.06 33.95 ±0.28 18.29 ±0.44 56.51±0.13 22.62 ±0.59 11.66 ±0.34\n(Shaw et al., 2018) 56.86±0.30 35.58 ±0.48 19.51 ±0.41 56.75±0.13 23.22 ±0.73 11.32 ±1.78\n(Shiv and Quirk, 2019)56.50±0.19 34.54 ±0.38 18.89 ±0.48 57.04±0.23 24.01 ±0.21 11.53 ±0.53\n(Kim et al., 2020) 57.02±0.17 35.55 ±0.41 19.29 ±0.35 57.33±0.38 23.47 ±0.28 12.07 ±0.74\nOur model 57.28±0.16 36.00±0.10 19.97±0.17 57.72±0.19 25.03±0.37 13.21±0.24\nw/o first dim. 57.10±0.14 35.94 ±0.21 19.54 ±0.20 57.40±0.25 24.22 ±0.65 12.92 ±0.27\nw/o second dim. 56.89±0.15 35.36 ±0.10 19.24 ±0.48 57.27±0.09 24.30 ±0.29 12.66 ±0.69\nw/o Global-Abs. 56.83±0.14 35.80 ±0.29 19.47 ±0.10 57.27±0.32 23.86 ±0.10 12.30 ±0.65\nw/o Local-Rel. 56.95±0.10 35.36 ±0.37 19.29 ±0.66 57.50±0.05 23.74 ±0.18 13.13 ±0.09\nTable 2: All results on code summarization\nOn the other hand, although the global method the-\noretically knows the complete relationship between\narbitrary nodes, global bias is relatively soft and\nperhaps fragile. As a result, the local way shows\nbenefits to polishing the global perspective. To the\nbest of our knowledge, no previous work explored\ncombining them, while we finally show the benefits\nof fusing the local and global paradigms.\n6 Conclusion\nWe propose a new tree Transformer encoding\nposition for each node based on our novel two-\ndimensional description of tree structures. Tech-\nnically, our model introduces soft bias as the posi-\ntional encoding of Transformer in global and local\nways. Our model finally outperforms strong base-\nlines on code summarization and completion tasks\nacross two different languages, highlighting the\neffectiveness of our approach.\n7 Acknowledgements\nWe thank all reviewers for their constructive com-\nments and Kechi Zhang for the discussion on the\nmanuscript. This research is supported by the Na-\ntional Natural Science Foundation of China un-\nder Grant No. 62072007, 62192733, 61832009,\n62192731, 62192730.\nLimitations\nIn this work, we propose a new tree transformer\nand compare it with several baselines in two tasks\nof different languages. To comprehensively and\nprecisely measure the performance of our model\nand all baselines, we train all models three times.\nWe do not entirely rely on the results produced by\nprevious work, and just choose them as references.\nAs a result, our experiments may produce a lot of\ncarbon dioxide and consume electrical power.\nIn this paper, we mainly focus on the representa-\ntion learning for source code on the function level\n(or in similar length as functions) and do not dis-\ncuss the model scalability to the corpus of extreme\nlong source code (such as full program files). We\nbelieve that learning from long source code is an in-\nteresting and valuable research topic, and perhaps\nexplore extending our model for it in the future.\nWe believe our approach should be workable on\nmost tasks related to code trees in theory. How-\never, since additional code graph structure, includ-\ning data-flow and control-flow, isn’t considered\nin our approach, some related tasks may not be\nsuitable. Besides, the additional context feature\nof code tokens is also not discussed, and recently\nTransformer models jointly learn from both context\nand tree structure are not mentioned as our base-\nlines. To combine context and structure, a starting\npoint is to map tree nodes to code tokens. After\nthat, it should be useful for our model to add po-\nsitional encoding of code context orders for these\nassigned nodes, and we perhaps explore it in the\nfuture.\nReferences\nMiltiadis Allamanis. 2019. The adverse effects of code\nduplication in machine learning models of code. In\nProceedings of the 2019 ACM SIGPLAN Interna-\ntional Symposium on New Ideas, New Paradigms,\nand Reflections on Programming and Software, pages\n143–153.\nMiltiadis Allamanis, Marc Brockschmidt, and Mah-\nmoud Khademi. 2017. Learning to repre-\nsent programs with graphs. arXiv preprint\narXiv:1711.00740.\n3212\nMiltiadis Allamanis, Hao Peng, and Charles Sutton.\n2016. A convolutional attention network for ex-\ntreme summarization of source code. In Interna-\ntional conference on machine learning, pages 2091–\n2100. PMLR.\nUri Alon, Shaked Brody, Omer Levy, and Eran Ya-\nhav. 2018. code2seq: Generating sequences from\nstructured representations of code. arXiv preprint\narXiv:1808.01400.\nUri Alon and Eran Yahav. 2020. On the bottleneck of\ngraph neural networks and its practical implications.\narXiv preprint arXiv:2006.05205.\nUri Alon, Meital Zilberstein, Omer Levy, and Eran\nYahav. 2019. code2vec: Learning distributed rep-\nresentations of code. Proceedings of the ACM on\nProgramming Languages, 3(POPL):1–29.\nPu-Chin Chen, Henry Tsai, Srinadh Bhojanapalli,\nHyung Won Chung, Yin-Wen Chang, and Chun-Sung\nFerng. 2021a. A simple and effective positional en-\ncoding for transformers. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2974–2988.\nZimin Chen, Vincent Josua Hellendoorn, Pascal Lam-\nblin, Petros Maniatis, Pierre-Antoine Manzagol,\nDaniel Tarlow, and Subhodeep Moitra. 2021b.\nPLUR: A unifying, graph-based view of program\nlearning, understanding, and repair. In Advances in\nNeural Information Processing Systems.\nNadezhda Chirkova and Sergey Troshin. 2021. Empir-\nical study of transformers for source code. In Pro-\nceedings of the 29th ACM Joint Meeting on European\nSoftware Engineering Conference and Symposium on\nthe Foundations of Software Engineering, pages 703–\n715.\nThomas H Cormen, Charles E Leiserson, Ronald L\nRivest, and Clifford Stein. 2022. Introduction to\nalgorithms. MIT press.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a fixed-length context. arXiv preprint\narXiv:1901.02860.\nHoa Khanh Dam, Truyen Tran, and Trang Pham. 2016.\nA deep language model for software code. arXiv\npreprint arXiv:1608.02715.\nVijay Prakash Dwivedi and Xavier Bresson. 2020. A\ngeneralization of transformer networks to graphs.\narXiv preprint arXiv:2012.09699.\nShuzheng Gao, Cuiyun Gao, Yulan He, Jichuan Zeng,\nLun Yiu Nie, and Xin Xia. 2021. Code structure\nguided transformer for source code summarization.\narXiv preprint arXiv:2104.09340.\nWilliam L Hamilton. 2020. Graph representation learn-\ning. Synthesis Lectures on Artifical Intelligence and\nMachine Learning, 14(3):1–159.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nVincent J Hellendoorn, Charles Sutton, Rishabh Singh,\nPetros Maniatis, and David Bieber. 2019. Global\nrelational models of source code. In International\nconference on learning representations.\nAbram Hindle, Earl T Barr, Mark Gabel, Zhendong Su,\nand Premkumar Devanbu. 2016. On the naturalness\nof software. Communications of the ACM, 59(5):122–\n131.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2016. Summarizing source code\nusing a neural attention model. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2073–2083.\nGuolin Ke, Di He, and Tie-Yan Liu. 2020. Rethink-\ning the positional encoding in language pre-training.\narXiv preprint arXiv:2006.15595.\nSeohyun Kim, Jinman Zhao, Yuchi Tian, and Satish\nChandra. 2020. Code prediction by feeding trees to\ntransformers. arXiv preprint arXiv:2003.13848.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nDevin Kreuzer, Dominique Beaini, Will Hamilton, Vin-\ncent Létourneau, and Prudencio Tossou. 2021. Re-\nthinking graph transformers with spectral attention.\nAdvances in Neural Information Processing Systems,\n34.\nJian Li, Yue Wang, Michael R Lyu, and Irwin King.\n2017. Code completion with neural attention and\npointer networks. arXiv preprint arXiv:1711.09573.\nZhongli Li, Qingyu Zhou, Chao Li, Ke Xu, and Yunbo\nCao. 2020. Improving bert with syntax-aware local\nattention. arXiv preprint arXiv:2012.15150.\nGrégoire Mialon, Dexiong Chen, Margot Selosse,\nand Julien Mairal. 2021. Graphit: Encoding\ngraph structure in transformers. arXiv preprint\narXiv:2106.05667.\nLili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016.\nConvolutional neural networks over tree structures\nfor programming language processing. In Thirtieth\nAAAI Conference on Artificial Intelligence.\nXuan-Phi Nguyen, Shafiq Joty, Steven CH Hoi, and\nRichard Socher. 2020. Tree-structured attention\nwith hierarchical accumulation. arXiv preprint\narXiv:2002.08046.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\n3213\nHan Peng, Ge Li, Wenhan Wang, Yunfei Zhao, and Zhi\nJin. 2021. Integrating tree path in transformer for\ncode representation. Advances in Neural Information\nProcessing Systems, 34.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.\nSelf-attention with relative position representations.\narXiv preprint arXiv:1803.02155.\nVighnesh Shiv and Chris Quirk. 2019. Novel positional\nencodings to enable tree-based transformers. Ad-\nvances in Neural Information Processing Systems ,\n32:12081–12091.\nZeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili\nMou, and Lu Zhang. 2020. Treegen: A tree-based\ntransformer architecture for code generation. In Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 34, pages 8984–8991.\nYao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada,\nLouis-Philippe Morency, and Ruslan Salakhutdinov.\n2019. Transformer dissection: A unified understand-\ning of transformer’s attention via the lens of kernel.\narXiv preprint arXiv:1908.11775.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nSong Wang, Devin Chollak, Dana Movshovitz-Attias,\nand Lin Tan. 2016. Bugram: bug detection with n-\ngram language models. In Proceedings of the 31st\nIEEE/ACM International Conference on Automated\nSoftware Engineering, pages 708–719.\nXing Wang, Zhaopeng Tu, Longyue Wang, and Shum-\ning Shi. 2019a. Self-attention with structural position\nrepresentations. arXiv preprint arXiv:1909.00383.\nYau-Shian Wang, Hung-Yi Lee, and Yun-Nung Chen.\n2019b. Tree transformer: Integrating tree structures\ninto self-attention. arXiv preprint arXiv:1909.06639.\nHongqiu Wu, Hai Zhao, and Min Zhang. 2020. Code\nsummarization with structure-induced transformer.\narXiv preprint arXiv:2012.14710.\nZenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun\nShou, Ming Gong, Wanjun Zhong, Xiaojun Quan,\nNan Duan, and Daxin Jiang. 2020. Syntax-enhanced\npre-trained model. arXiv preprint arXiv:2012.14116.\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin\nZheng, Guolin Ke, Di He, Yanming Shen, and Tie-\nYan Liu. 2021. Do transformers really perform badly\nfor graph representation? Advances in Neural Infor-\nmation Processing Systems, 34:28877–28888.\nDaniel Zügner, Tobias Kirschstein, Michele Catasta,\nJure Leskovec, and Stephan Günnemann. 2021.\nLanguage-agnostic representation learning of source\ncode from structure and context. arXiv preprint\narXiv:2103.11318.\n3214",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7941302061080933
    },
    {
      "name": "Automatic summarization",
      "score": 0.6951131224632263
    },
    {
      "name": "Transformer",
      "score": 0.6393990516662598
    },
    {
      "name": "Tree structure",
      "score": 0.5107378959655762
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3654381036758423
    },
    {
      "name": "Programming language",
      "score": 0.2243543565273285
    },
    {
      "name": "Data structure",
      "score": 0.2020256221294403
    },
    {
      "name": "Engineering",
      "score": 0.06699463725090027
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210128818",
      "name": "Institute of Software",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ]
}