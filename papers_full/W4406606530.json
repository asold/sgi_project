{
  "title": "Educational justice. Reliability and consistency of large language models for automated essay scoring and its implications",
  "url": "https://openalex.org/W4406606530",
  "year": 2025,
  "authors": [],
  "references": [
    "https://openalex.org/W2341848521",
    "https://openalex.org/W6683687954",
    "https://openalex.org/W4288400169",
    "https://openalex.org/W2337971266",
    "https://openalex.org/W2015595848",
    "https://openalex.org/W2011808739",
    "https://openalex.org/W4403939824",
    "https://openalex.org/W2091879664",
    "https://openalex.org/W55286957",
    "https://openalex.org/W654566995",
    "https://openalex.org/W2014405594",
    "https://openalex.org/W1491705696",
    "https://openalex.org/W4293070705",
    "https://openalex.org/W4230805046",
    "https://openalex.org/W4313582009",
    "https://openalex.org/W2327037637",
    "https://openalex.org/W4399832459",
    "https://openalex.org/W4233276225",
    "https://openalex.org/W4220747294",
    "https://openalex.org/W4399282423",
    "https://openalex.org/W2963121203",
    "https://openalex.org/W2313339984",
    "https://openalex.org/W4390414972",
    "https://openalex.org/W4366403195",
    "https://openalex.org/W4404459606",
    "https://openalex.org/W4396806295",
    "https://openalex.org/W2060423038",
    "https://openalex.org/W2012303852",
    "https://openalex.org/W4302304838",
    "https://openalex.org/W4382894295",
    "https://openalex.org/W3201077663",
    "https://openalex.org/W4394964316",
    "https://openalex.org/W2141403362",
    "https://openalex.org/W2155232409",
    "https://openalex.org/W3153116061",
    "https://openalex.org/W3193704761",
    "https://openalex.org/W2007345088",
    "https://openalex.org/W2097195794",
    "https://openalex.org/W2972955509"
  ],
  "abstract": null,
  "full_text": "67\nEducational justice. Reliability and consistency of large language models for automated essay \nscoring and its implications\nKeywords Abstract\nAES; \nAI; \nanalysis; \nartificial intelligence; \nautomated essay scoring; \nanalysis; \nconsistency; \ngenerative artificial intelligence; \nLLMs; \nreliability.\nMaintaining consistency in automated essay scoring is essential to \nguarantee fair and dependable assessments. This study investigates \nconsistency and provides a comparative analysis of open-source and \nproprietary large language models (LLMs) for automated essay scoring \n(AES). The study utilized student essays, each assessed five times to \nmeasure both intrarater (using intraclass coefficient and repeatability \ncoefficient) and interrater (concordance correlation coefficient) reliability \nacross several models: GPT-4, GPT-4o, GPT-4o mini, GPT-3.5 Turbo, \nGemini 1.5 Flash, and LLaMa 3.1 70B. Essays and marking criteria are \nused for prompt construction and sent to each large language model to \nobtain score outputs. Results indicate that the scores generated by GPT-\n4o closely align with human assessments, demonstrating fair agreement \nacross repeated measures. Specifically, GPT-4o exhibits slightly higher \nconcordance correlation coefficients (CCC) than GPT-4o mini, indicating \nsuperior agreement with human scores. However, qualitatively, it can \nbe observed that all LLM models are not as consistent in terms of their \nscoring rationale/evaluation. Our study results indicate that the challenges \ncurrently faced in automated essay scoring with large language models \nneed to be analyzed not only from a quantitative perspective but also \nqualitatively. Additionally, we utilize more sophisticated prompting \nmethods and address the inconsistencies observed in the initial \nmeasurements. Despite the purported reliability of some models within \nour study, the selection of LLMs should be considered thoroughly during \npractical implementations for an AES.\nArticle Info\nReceived 29 November 2024\nReceived in revised form 17 January 2025\nAccepted 17 January 2025\nAvailable online 21 January 2025\nDOI: https://doi.org/10.37074/jalt.2025.8.1.21\nContent Available at : \nJournal of Applied Learning & Teaching\nVol.8 No.1 (2025)\nJournalof Applied Learning & Teaching\nJALT\nhttp://journals.sfu.ca/jalt/index.php/jalt/index\nISSN : 2591-801X\nJournal of Applied Learning & Teaching Vol.8 No.1 (2025)\nbealindaqinthara@gmail.com A\nCorrespondence\nSiti Bealinda Qinthara RonyA A  Research Assistant, Faculty of AI and Robotics, Raffles University, Malaysia\nTan Xin FeiB B Undergraduate Student, Faculty of AI and Robotics, Raffles University, Malaysia\nSasa ArsovskiC C Professor, Dean of Faculty of AI and Robotics, Raffles University, Malaysia\n68Journal of Applied Learning & Teaching Vol.8 No.1 (2025)\nIntroduction \nIn the education sector, new technologies are used to further \nengage and create more cohesive learning and teaching \nexperiences. Ghosh (2024) investigates the usage of student \nportals and implementation of online quizzes in classrooms \nas a part of a competitive game between students. \nNevertheless, teachers play a crucial role in the delivery of \ncurriculum and are also the greatest contributor to what a \nstudent manages to learn in an educational setting. \nGood education should be a necessity, yet with that \nthought, often there is a great disparity between the ratio of \nstudents to teachers, especially in larger institutions. Despite \nbeing a necessity, the question of the quality of education \nand the focus on students’ learning experience can often \nbe compromised by the amount of workload a teacher has \n(Anglia, 2020; Kanwal et al., 2023). Part of what contributes to \nthis overwhelming workload is essay marking (Warschauer \n& Grimes, 2008). \nAutomated essay scoring (AES) refers to the usage of \nmachine systems to mark educational assessments and \nis a possible solution to alleviate the said burden. Such a \nsolution will allow teachers to allocate more time towards \nteaching, overall maintaining the quality of education to not \nfall below acceptable standards, and possibly even elevating \nthe quality to higher standards (Hénard & Roseveare, 2012). \nAES systems are not a new concept, with widely known \nsystems dating back to the 1960s. In recent years, however, \ntechniques such as machine learning and natural language \nprocessing algorithms have often been utilized (Wilks, 2005; \nRamesh & Sanampudi, 2021).\nOur study focuses on the usage of artificial intelligence (AI) \npowered AES systems, primarily reviewing the performance \nand consistency of open source and proprietary large \nlanguage models (Barry, 2023) for the grading/scoring of \nessays. We compare the consistency across models and \nanalyze consistency criteria for each model’s repeated \ngrading runs.\nLiterature review\nAES systems like the Project Essay Grader (PEG), e-rater, \nand Intelligent Essay Assessor (IEA) were implemented in \nthe 1990s to address the time-intensive nature of essay \nevaluation. Initial resistance in the 1960s arose from early \nAES prototypes relying on surface features (e.g., number \nof propositions, commas, and uncommon words), which \noverlooked important aspects like content (Hearst, 2000). \nAlthough these systems showed high interrater correlation, \nthey failed to assess critical writing skills.\nOver the years, the pursuit of better AES systems has put \nan emphasis on the system’s ability to extract and evaluate \ndirect features of an essay. Now, as LLMs’ capabilities to \nprocess multi-modal inputs (images, text, videos) progress, \nthe possibility that an essay’s context and direct linguistic \nfeatures i.e., semantic features, structural features, etc. can \nbe understood also further expands. Where systematic \ndifferences between any system regarding essay assessment \noften study intrarater and interrater reliability (Kayapinar, \n2014), this evaluation method still applies to recent AI-\nbased AES systems. The literature review is further divided \ninto two sections, in which the first section will highlight \nthe challenges and concerns involved in AI-based AES, and \nthe second section talks about recent usage of AI for AES \nsystems, where various works focused on the purported \nfeasibility of such methods.\nChallenges in AI-based AES\nThere are several challenges associated with the creation \nand feasibility of any AES (Automated Evaluation System) \nmodel (Hussein et al., 2019). When analyzing large language \nmodels (LLMs) and AI systems that operate as prompt-\nand-response mechanisms, we must consider not only the \nrobustness of the models but also the inputs they receive, \nsuch as the prompts. These challenges are interrelated; one \nchallenge can directly or indirectly impact another.\nMarking consistency  is a critical limitation in AI grading \nsystems, as similar essays can receive different marks, \nundermining trust among educators and students (Attali, \n2013; Balfour, 2013). Inconsistencies can arise when slight \nvariations in context, sentence structure, or phrasing lead to \ndifferent scores for identical essays. AI systems also struggle \nwith discrepancies in scoring compared to human experts, \neven when adhering to predefined rubrics (Perelman, 2014). \nFurthermore, inconsistencies may occur when grading large \nvolumes of essays simultaneously.\nInternal understanding  refers to the model’s ability to \nunderstand grading criteria and generalize. The AI model’s \ninability to grasp nuances, especially in unconventional \nwriting styles, can lead to inaccurate marks, either \nundervaluing creative work or inflating scores due to its \nlimited context (Li et al., 2021; Awidi, 2024). AI may focus \non rubric fulfilment rather than content quality, potentially \ninflating scores for essays lacking depth (Zhu, 2019).\nRecent research in AES\nRecent AES research explores deep learning and generative \nAI models, which don’t require traditional feature extraction \n(Attali & Burstein, 2006; Uto, 2021). Deep learning models, \nhowever, show promise when combined with feature-\nengineered inputs for a hybrid AES system to enhance \ncontextual understanding (Kurniawan et al., 2024; Ortiz-\nZambrano et al., 2024; Faseeh et al., 2024). Despite the \nrise of deep learning, traditional systems still offer value, \nas evidenced by a model that achieved a high Pearson’s \ncorrelation using NLP techniques (Adeyanju et al., 2024).\nLLMs, such as GPT, have shown promise in AES due to \ntheir linguistic capabilities and reasoning (Mansour et al., \n2024; Ouyang et al., 2022). For example, Mizumoto and \nEguchi (2023) tested GPT-3 for scoring non-native English \nessays, finding that GPT marked scores with acceptable \nadjacent agreement to human raters. They also showed that \ncombining GPT scores with linguistic features improved AES \nperformance. Inspired by Mizumoto and Eguchi’s approach, \n69Journal of Applied Learning & Teaching Vol.8 No.1 (2025)\nLi and Liu (2024) used the approach of utilizing different \nprompting techniques (Maclaren, 2024) and demonstrated \nGPT-4’s superiority in scoring non-native Japanese essays \ncompared to several other models when including all \nlinguistic measures in their scoring criteria with multi-shot \nprompting.\nRecent work by Pack et al. (2024) had also expanded the \nviability testing of essays concerning language proficiency \nto other mainstream LLMs aside from OpenAI (n.d.)’s, \nfocusing on Google’s PaLM 2 (through chatbot Bard), \nAnthropic’s Claude 2, and OpenAI’s GPT-3.5 and GPT-4. \nUsing the intraclass correlation coefficient (ICC) reliability \nscore, the researchers were able to analyze the interrater \nreliability between model scores to humans and intrarater \nreliability of each model based on measures between two \ndifferent time gaps. With the exception of GPT-3.5, the other \nLLMs improved in terms of intrarater reliability over time, in \nwhich GPT-4 was the most reliable given repeated measures \nin separate instances. However, interrater reliability (or the \nvalidity of the models’ scoring to human scores) decreased \nfor GPT-3.5 and GPT-4 over time.\nA notable way to further enforce standardization and \nincrease intrarater abilities of LLMs for AES can be seen in \nworks by Ishida et al. (2024) or Kim and Jo (2024), where \nthey utilize pairwise evaluation or comparative judgement \n(Pollitt, 2011) with LLMs instead of a rubric-based approach \n(Ishida et al., 2024; Kim & Jo, 2024). The method generally \ninvolves giving a positive point to the essay that scores \nhigher, and giving zero or negative points to the essay that \nloses in score - repeated in a round-robin fashion. In the \nstudy by Kim and Jo (2024), it showed that this approach \nimproved the performance of both GPT-4 and GPT-3, and \nwas statistically significant. \nThis pairwise comparison approach is likened to the use \nof Latent Semantic Analysis (Deerwester et al., 1990; Foltz, \n1996) for the IEA (Foltz & Landauer, 1999). The IEA, however, \nwould focus on a more semantic comparison where \nthe meaning of a student’s essay is extracted and then \ncompared with similar texts of known quality. By looking at \nthe essays within an interconnected setting semantic-wise \nor rubrics, such approaches may also alleviate the inherent \nrandomness/non-deterministic nature of an LLM (Lee et al., \n2022), as proprietary models often get updated - altering its \nunderlying parameters. The problem is that when dealing \nwith a large number of essays, the number of pairwise \ncomparisons that can be made increases exponentially, \nraising concerns about computational viability. \nDespite promising results, challenges thus remain, \nparticularly with the non-deterministic nature of LLMs and \npotential issues with interrater reliability (Pack et al., 2024). \nNevertheless, LLMs’ potential as AES tools also extends \nbeyond grading, as they can provide valuable feedback \nto both students and raters, improving efficiency and \nconsistency (Xiao et al., 2024; Gombert et al., 2024). Despite \nchallenges such as the lack of personalized feedback, it \ncan still serve as a basis or a foundation for students’ self-\nreview/study (Meyer et al., 2024). And so, the ability of \nLLMs to enhance grading consistency positions them as an \nincreasingly viable AES solution.\nDataset and essay marking pipeline\nOur dataset consists of 126 essays. We used 26 essays \nfrom Diploma in Business students for the “Learning Skills” \nmodule for our statistical analysis and 100 essays with \nadditional prompting to confirm study results. The dataset \nfor statistical analysis includes the marked essays with \ngrades (human scores), the original ungraded submissions, \nand marking criteria (Appendix B). These are processed \nthrough a general essay marking pipeline, as shown in \nFigure 1, where each essay is paired with the marking criteria \nfor zero-shot prompting. The resulting scores are compiled \ninto a table (Appendix A), and sample outputs are presented \nin Appendices C and D.\nThe study uses six models: five proprietary models (GPT-\n3.5 Turbo, GPT-4, GPT-4o, GPT-4o mini, and Gemini 1.5 \nFlash) and Meta (n. d.)’s LLaMa 3.1 70B, which is a more \ndemocratized LLM (in freely providing access to trained \nweights for implementation, and the general architecture). \nA brief overview of the models is provided below: \nGPT-3.5 Turbo  (OpenAI): A fast and efficient \nversion of GPT-3.5, ideal for time-sensitive tasks \nand general-purpose text generation.\nGPT-4o (OpenAI): A versatile version of GPT-\n4 designed for various applications, offering \nimproved efficiency and adaptability.\nGPT-4o mini (OpenAI): A lightweight variant of \nGPT-4, optimized for small-scale applications \nrequiring efficiency and compact form.\nGPT-4 (OpenAI): A multimodal model with \nadvanced reasoning and higher accuracy, \ncapable of processing both text and images.\nGemini 1.5 Flash  (DeepMind): A fast and \ncontextually accurate model, excelling in \nmulti-modal processing and natural language \nunderstanding.\nLLaMa 3.1 70B  (Meta): A powerful language \nmodel with 70 billion parameters, offering \nadvanced text generation, complex reasoning, \nand translation capabilities.\n•\n•\n•\n•\n•\n•\nFigure 1. Essay marking pipeline.\n\n70Journal of Applied Learning & Teaching Vol.8 No.1 (2025)\nAnalysis of results\nThis section presents the results of five repeated measures \nof each student’s essay for each model, focusing on \nconsistency. The analysis includes both score marks and \nqualitative assessments of the text outputs. Reliability refers \nto the consistency and reproducibility of measurements, \nwith two types (Baumgartner, 1989): relative reliability, which \nevaluates consistency across repeated trials (Bruton et al., \n2000), and absolute reliability, which assesses measurement \nerror, such as the Standard Error of the Mean (Stratford & \nGoldsmith, 1997).\nTo compare the marks given by LLMs and humans, we \nanalyze both intrarater (the model’s internal consistency) \nand interrater reliability (comparison between the model \nand human raters). Subjectively, we examine how well \nthe model adheres to the baseline rubric across repeated \nmeasurements and students. The focus is on consistency, \nwith a brief discussion of the reasonableness of generated \nfeedback. Statistical methods used do not account for \nsubjective score variations based on the extracted criteria.\nIntrarater reliability\nIntrarater reliability can be considered a type of relative \nreliability study that refers to how reliable one instrument/\nrater is across repeated measures (Gwet, 2008). Our study \nexamines the consistency of a single model’s scoring across \nrepeated measures for multiple essays. We use the ICC to \nassess ‘relative’ reliability and measure internal scoring \nconsistency. The ICC allows for the estimation of two \ntypes of consistency: absolute agreement and consistency \n(Nichols, 1998). \nThe ICC measure for absolute agreement assesses whether \na model assigns identical scores to essays across repeated \nmeasurements. The ICC for consistency evaluates whether \na model maintains the same relative ranking of students \nacross measurements, regardless of score values. For \nexample, if Student 1 ranks highest in the first measurement, \nconsistency checks if they retain the top rank in subsequent \nmeasurements. Note that this ICC measure focuses on \nrelative ranking, not absolute score consistency.\nBased on a comprehensive research article on ICC by \nLiljequist et al. (2019), in our intrarater reliability study, we \nopted to analyze both ICC measures. The ICC measure of \nconsistency excludes variance caused by bias in repeated \nmeasurements, unlike the absolute agreement measure, \nwhich accounts for it. While the choice of model (one-\nway random, two-way random, or two-way mixed) defines \nthe scope and assumptions about bias, the calculation for \nconsistency and absolute agreement remains the same for \ntwo-way random and mixed effects models. Hence, looking \nfrom an abstract level, the difference between the formula \nused between the two ICC measures for both two-way \nrandom and two-way mixed effect models can be said to lie \nin the inclusion of σ_c^2 (bias variance) for each repeated \nmeasurement with the same rater. \nHowever, for our study, we will frame the ICC measures to use \nthe two-way fixed effects model for a single measurement \ntype, as declared below:\nA two-way mixed effects model: Our results only \nrepresent the specific reliability of each LLMs \ninvolved, where the LLMs determined as raters \nare not considered as a sample of a bigger \npopulation (Shrout & Fleiss, 1979). In practice, \nit means that the same rater is utilized for \nsubsequent measurements considering a fixed \nbias (Koo & Li, 2016; Liljequist et al., 2019), such \nthat the calculation of ICC cannot be generalized \nto other LLMs with similar characteristics;\nSingle measurement type: In our study, we are \nnot interested in the k average of raters, and are \ncalculating ICC based only on a singular rater \n(model) for repeated measurements.\n•\n•\nWe use both ICC measures of consistency and absolute \nagreement to assess each model’s internal consistency. ICC \n(A, 1) represents an absolute agreement, while ICC (C, 1) \nreflects consistency in our analysis. The primary objective \nof this study is to evaluate consistency across repeated \nmeasures for each model, with repeatability also considered.\nRepeatability refers to the variation between two \nmeasurements observed under identical conditions (Bartlett \n& Frost, 2008). To better understand how consistent a model \nscores the same essay across repeated measurements, we \nare also thus concerned with the difference in values in \nsuccessive measurements, where less variation should \nindicate better repeatability and better internal consistency. \nThe repeatability coefficient use will quantify the maximum \nexpected difference and is calculated as seen in Equation \n(1). The multiplier 2.77 arises from the properties of the \nnormal distribution and accounts for the variability between \nsuccessive measurements within 95% confidence interval (CI) \n(Vaz et al., 2013). Our RC calculation utilizes the mean pooled \nstandard deviation to account for within-subject variances \nand between-subject variances, as seen in Equation (2).\nAll calculations of the ICC and repeatability consistency \nis calculated for each model, where the “irr” R statistical \npackage, and Python are used for calculations. Intrarater \nReliability Statistical Analysis given in Table 1.\nWe assess the relative reliability of the models using \nrepeated measures under identical conditions, with values \n< 0.5 indicating poor reliability, 0.5-0.75 moderate, 0.75-0.9 \ngood, and > 0.9 excellent (Koo & Li, 2016). Based on Table \n1., GPT-4o mini has the highest ICC values for both absolute \nagreement and consistency, followed by GPT-4o with ICC \n(C, 1) values of 0.7819 and 0.7484, and ICC (A, 1) values of \n0.787 and 0.749. Only these two models achieved ICC values \n71Journal of Applied Learning & Teaching Vol.8 No.1 (2025)\nTable 1. Intrarater reliability statistical analysis of tested large \nlanguage models.\nabove 0.7, while older GPT versions (GPT-3.5 Turbo and \nGPT-4) had ICC values around 0.5.\nNon-OpenAI models, Gemini 1.5 Flash and LLaMa 3.1 70B, \nshow the lowest ICC values (<0.5) for both measures, with \nGemini 1.5 Flash performing particularly poorly. Its absolute \nagreement ICC is below 0.1, indicating the model struggles \nto maintain consistent scores and relative rankings across \nrepeated measures. Notably, GPT-4o and GPT-3.5 Turbo are \nthe only models where the absolute agreement ICC slightly \nexceeds the consistency ICC, which could reflect data \nartifacts or bias in score variability.\nThe repeatability coefficient (RC) generally aligns with ICC \nvalues, with GPT-4o mini showing the smallest range of 8.47, \nindicating the highest consistency for successive measures. \nGPT-4, while achieving moderate reliability, has a higher RC \nof 12.61, but still ranks second in reliability. GPT-3.5 Turbo, \ndespite its higher ICC values, has a larger RC, indicating \nmore variability in successive measures.\nOur findings suggest that GPT-4o mini and GPT-4o \ndemonstrate the best internal consistency and reliability, \nparticularly in maintaining relative rankings across repeated \nmeasures. GPT-4o mini, based on intrarater reliability, \nemerges as the most consistent model for essay scoring, \nensuring reliable scores across repeated and successive \nmeasures.\nInterrater reliability\nInterrater reliability measures the agreement between \nraters (Lange, 2011). In our study, we assess the reliability of \nLLM-generated scores against a human rater. The analysis \ncompares the scores from each model with the human \nrater using the concordance correlation coefficient (CCC), \ncalculated pairwise for each essay. CCC was chosen over \nPearson’s r to quantify both correlation and agreement (Lin, \n1989). The calculation treats the dataset as pairwise data, \nwith repeated measurements forming new data points for \neach essay.\nWe interpret CCC values using Landis and Koch’s scale: <0 \n(no agreement), 0–0.20 (slight), 0.21–0.40 (fair), 0.41–0.60 \n(moderate), 0.61–0.80 (substantial), and 0.81–1 (almost \nperfect). The table includes the CCC estimate and 95% CI \nfor each model. Unlike intrarater reliability, which measures \nconsistency within a single rater, interrater reliability in our \nstudy evaluates the agreement between model outputs \nand human scores over multiple repetitions. Consistency is \ndetermined by how well the repeated model outputs align \nwith a given human score.\nA Bland-Altman analysis (also included in Table 2) was also \nconducted, though its assumptions - such as equivalent \nprecision and constant bias - may not fully apply here \n(Taffé, 2021; Silveira et al., 2024). Since tolerance limits for \nacceptable score differences were not defined, the analysis \nserves as a general illustration rather than a definitive \nassessment of agreement (Indrayan, 2022). The calculations \nfor CCC and Bland-Altman bias were performed using the \n“SimplyAgree” R package.\nTable 2. Interrater reliability statistical analysis of tested large \nlanguage models.\nOur interrater reliability study (Table 2.) shows that GPT-4o \nand GPT-4o mini achieve the highest CCC values, indicating \nfair agreement with human scores across repeated \nmeasures. This suggests both models produce consistent \nand reasonably accurate score outputs. However, GPT-4o \ndemonstrates slightly better agreement than GPT-4o mini. \nIn contrast, GPT-4 has the lowest CCC estimate, with true \nvalues close to 0, indicating minimal to no agreement with \nhuman scores.\nThe Bland-Altman analysis reveals that GPT-4o and GPT-4o \nmini exhibit similar bias (around 2 points), with GPT-4o mini \nunderestimating and GPT-4o overestimating scores. Overall, \nGPT-4o and GPT-4o mini show the highest interrater \nreliability and are better suited for tasks requiring alignment \nwith human evaluation.\nQualitative analysis\nA qualitative review of the LLM’s text outputs reveals a \ndegree of randomness in their marking criteria. Shown in \nAppendix C are excerpts of GPT-4o and Gemini 1.5 Flash’s \nfirst and repeated outputs, which we primarily investigate \nto give our qualitative analysis. Research data of the full-\ntext outputs for all repeated measurements on each model \nwill be made available upon reasonable request. Yet it is \nclear that qualitatively, repeated measures show notable \ndifferences in rationale which often correlate with variations \nin numerical scores. Some models also fail to correctly sum \nscores from subcriteria, leading to inconsistencies.\nFor instance, the GPT-4o model may change the naming \nand maximum score of subcriteria across repeated \nmeasures, such as changing “Referencing and Citation” \nfrom 10 marks to “Use of Sources and Referencing” with 15 \nmarks. Additionally, some models would hallucinate new \nsubcriteria or omit specified ones. Inconsistencies can also \noccur in how subcriteria are evaluated, such as Gemini 1.5 \nFlash initially breaking down “Content” into smaller aspects, \n72Journal of Applied Learning & Teaching Vol.8 No.1 (2025)\nbut later assessing it based on identified strengths and \nweaknesses. Hence, while numerical scores may suggest \nsome reliability, the rationale behind these scores reveals \nthat LLMs’ consistency in essay marking is less dependable \nthan statistical analyses alone might imply.\nDiscussion\nA key concern in using AI models as Automated Essay \nScoring (AES) systems is whether they produce accurate \nscores compared to human raters. This issue arises not only \nfrom the subjectivity or objectivity of the scoring models but \nalso from the inherent variability of AI generative models. \nThe practical deployment of AI in AES requires trust in its \nscoring analysis, as the quality of AI models as replacements \nfor human teachers remains uncertain (Barshay, 2024). In our \nstudy, we assess the consistency and reliability of selected \nAI models, helping readers to evaluate their suitability for \npractical applications.\nOur findings suggest that GPT-4o and GPT-4o mini are \ninternally reliable models. However, if both reliability and \nagreement with human scores are priorities, GPT-4o should \nbe preferred, as it shows slightly higher agreement with \nhuman scores (higher CCC value). While GPT-4 demonstrates \ninternal reliability (ICC > 0.5), it has the lowest CCC value when \ncompared to human scores, showing minimal agreement \nand making it less suitable for practical use. Non-OpenAI \nproprietary models generally exhibit low consistency and \nagreement with human scores, supporting the preference \nfor OpenAI models in AES applications.\nDespite good internal consistency in models like GPT-4o and \nGPT-4o mini, achieving sufficient human-model agreement \nremains challenging, even with repeated measures showing \nfair agreement. Therefore, selecting the appropriate \nLLM for AES requires careful consideration. Qualitative \ninconsistencies are also evident across all models, including:\nHallucination or omission of subcriteria\nInconsistent definition of maximum marks \nor awarding of marks across repeated \nmeasurements\nInconsistent evaluation of aspects within similar \nsubcriteria\nErrors in summing total marks from subcriteria\n•\n•\n•\n•\nWhile early research into the reliability of models like the \nGPT-3.5 (Khademi, 2023) showed low inter-reliability with \nhuman scores, more recent research involving GPT-4 (Pack \net al., 2024; Tate et al., 2024),  has shown it has become the \nmost reliable for essay scoring in comparison to earlier GPT \nmodels and other non-Open AI models. Our study now \nsuggests that GPT-4o and GPT-4o mini offer better reliability \nand human score agreement. This trend likely indicates that \nfuture versions of OpenAI’s LLMs will become more reliable \nfor AES applications.\nOur study acknowledges limitations, such as the evolving \nnature of proprietary AI models and the impact of prompt \nengineering (Stahl et al., 2024). Since we used a simple \nrubric for zero-shot prompting, alternative methods may \nimprove consistency. Nevertheless, our approach is suitable \nfor measuring both internal consistency and human-model \nagreement for repeated measures. We tested further using \nadditional prompting to mark 100 student essays using the \nGPT-4o mini. The results of further testing corroborated \nour study results, where the model achieves similar internal \nconsistency and fair agreement. \nThe change in interrater reliability or intrarater reliability \n(if any) in such models is thus assumed to be not just due \nto an LLM’s non-deterministic nature (Lee et al., 2022), but \nalso on how proprietary models are constantly updated, \nthereby frequently altering its parameters. This, alongside \na neural network’s black-box nature, is a point to consider \nwhen trying to employ an LLM as an AES. Clear instructions \nthrough prompts thus cannot be overstated to standardize \nscoring and avoid cases of hallucination, as LLMs will \ngenerate different scoring criteria for each input when no \ncriteria are specified (Ishida et al., 2024; Xiao et al., 2024). \nFuture research thus could explore alternative prompting \nmethods (Li, 2024; Kim & Jo, 2024) to further refine \nconsistency analysis across different AI models, and also as \na means to combat hallucinations – leading to more reliable \nfeedback (Rudolph et al., 2024).\nRegardless, even in cases with clear scoring outlines, the \nchances of encountering the models’ quirks are never zero. \nHence, as of current time, we once again note that both \ncaution and deliberation should always be included in the \nusage of AI-based AES. Popenici (2022) noted even more \nreasons why this degree of caution should be exercised. \nAI is prone to algorithmic bias, and cases where bias \nstemming from technology affecting people is increasingly \nreal (Popenici et al., 2023; Rudolph, 2023). Education, \nimplemented justly is one of the fields where people of \nall kinds can truly share their experiences and knowledge, \nenriching one another through meaningful interaction \non an even playing field. So, what happens if AI-based \nAES systems were implemented on a wide scale while still \nbeing riddled with the various concerns of reliability and \nfairness? The focus on consistency studies, or the pursuit \nof a more consistent AI model, is thereby highly imperative \nin education – as it is impossible to completely avoid rapid \nadvancements in technology. Instead, its usages should \nbe as tools by both teachers and students alike to enrich \nthe learning experience, assisting and elevating current \neducational foundations and not as a means that redefines \nsaid foundations.\nConclusion\nThis study reinforces the critical role of consistency metrics \nin assessing LLMs for automated essay scoring, both \nquantitatively and qualitatively. By focusing on intrarater \nreliability to measure internal consistency and interrater \nreliability to see the model’s consistency and agreement with \nhuman raters, we identified GPT-4o as a strong candidate \nfor practical implementations. However, achieving human-\n73Journal of Applied Learning & Teaching Vol.8 No.1 (2025)\nlevel alignment without any possible aspect of randomness \nremains to be a challenge. While our findings provide a \nfoundational approach to reliability testing in AES, future \nresearch must adapt to the evolving nature of LLMs, ensuring \nthat the usage of these tools in the field meets the nuanced \ndemands involved in educational assessment. Our study \nthus lays the groundwork for selecting and deploying more \nrobust and trustworthy AI-based AES systems to further \nelevate current educational foundations.\nReferences\nAdeyanju, I. A., Rachael, O. K., Titilayo, A. O., Ajoke, G. O., \nOyeladun, M. B., & Samuel, F. A. (2024). Artificial intelligence \nbased essay grading system.  Engineering and Technology \nJournal, 9(7). https://doi.org/10.47191/etj/v9i07.06\nAnglia, N. (2020, August 19). Does class size matter? \nThe educational impact of teacher-student ratios.  Nord \nAnglia Education. https://www.nordangliaeducation.com/\nnews/2020/08/19/does-class-size-matter-the-educational-\nimpact-of-teacherstudent-ratios\nAttali, Y. (2013). Validity and reliability of automated \nessay scoring. In M. D. Shermis & J. Burstein (Eds.), \nHandbook of automated essay evaluation: Current \napplications and new directions (2nd ed., pp. 181-\n198). Routledge. https://www.researchgate.net/\nprofile/Yigal-Attali/publication/292810655_Validity_\nand_reliability_of_automated_essay_scoring/\nlinks/5bfbfe31299bf10737f8b7cf/Validity-and-reliability-of-\nautomated-essay-scoring.pdf \nAttali, Y., & Burstein, J. (2006). Automated essay scoring \nwith e-rater® V.2. The Journal of Technology, Learning, and \nAssessment, 4 (3). https://ejournals.bc.edu/index.php/jtla/\narticle/view/1650\nAwidi, I. T. (2024). Comparing expert tutor evaluation \nof reflective essays with marking by generative artificial \nintelligence (AI) tool. Computers and Education: Artificial \nIntelligence, 6,  100226. https://doi.org/10.1016/j.\ncaeai.2024.100226 \nBalfour, S. P. (2013). Assessing writing in MOOCs: Automated \nessay scoring and calibrated peer review. Research & Practice \nin Assessment, 8, 40-48. https://eric.ed.gov/?id=EJ1062843. \nBarry, D. (2023, September 15). A look at the large language \nmodel landscape.  Reworked. https://www.reworked.co/\ninformation-management/a-look-at-the-large-language-\nmodel-landscape/\nBarshay, J. (2024, May 20). Proof points: AI essay grading is \nalready as ‘good as an overburdened’ teacher, but researchers \nsay it needs more work.  The Hechinger Report. https://\nhechingerreport.org/proof-points-ai-essay-grading/\nBartlett, J. W., & Frost, C. (2008). Reliability, repeatability and \nreproducibility: Analysis of measurement errors in continuous \nvariables. Ultrasound in Obstetrics and Gynecology, 31 (4), \n466–475. https://doi.org/10.1002/uog.5256\nBaumgartner, T. A. (1989). Norm-referenced measurement: \nReliability. In M. J. Safrit & T. M. Wood (Eds.), Measurement \nconcepts in physical education and exercise science  (pp.45-\n72). Champaign: Human Kinetics Books. \nBruton, A., Conway, J. H., & Holgate, S. T. (2000). Reliability: \nWhat is it, and how is it measured? Physiotherapy, 86(2), 94–\n99. https://doi.org/10.1016/s0031-9406(05)61211-4\nDeerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., \n& Harshman, R. (1990). Indexing by latent semantic analysis. \nJournal of the American Society for Information Science, 41(6), \n391–407. https://doi.org/10.1002/(sici)1097-4571(199009) \nFaseeh, M., Jaleel, A., Iqbal, N., Ghani, A., Abdusalomov, \nA., Mehmood, A., & Cho, Y. (2024). Hybrid approach \nto automated essay scoring: Integrating deep learning \nembeddings with handcrafted linguistic features for \nimproved accuracy. Mathematics, 12(21), 3416. https://doi.\norg/10.3390/math12213416\nFoltz, P. W. (1996). Latent semantic analysis for text-\nbased research. Behavior Research Methods Instruments & \nAmp Computers, 28 (2), 197–202. https://doi.org/10.3758/\nbf03204765\nFoltz, P. W., Laham, D., & Landauer, T. K. (1999). The intelligent \nessay assessor: Applications to educational technology. \nInteractive Multimedia Electronic Journal of Computer – \nEnhanced Learning, 1 (2). https://www.researchgate.net/\npublication/243770899_The_intelligent_essay_assessor_\nApplications_to_educational_technology\nGhosh, A. (2024, February 27). Student portals: Fostering \nconnectivity and educational growth.  Buddy4Study. https://\nwww.buddy4study.com/article/student-portals\nGombert, S., Fink, A., Giorgashvili, T., Jivet, I., Di Mitri, D., \nYau, J., Frey, A., & Drachsler, H. (2024). From the automated \nassessment of student essay content to highly informative \nfeedback: A case study.  International Journal of Artificial \nIntelligence in Education.  https://doi.org/10.1007/s40593-\n023-00387-6\nGoogle DeepMind. (n.d.). Gemini 1.5 Flash. Google Deep \nMind. https://deepmind.google/technologies/gemini/flash/\nGwet, K. L. (2008). Intrarater reliability. Wiley Encyclopedia of \nClinical Trials, 1–13. https://doi.org/10.1002/9780471462422.\neoct631\nHearst, M. (2000). The debate on automated essay grading. \nIEEE Intelligent Systems and Their Applications, 15(5), 22–37. \nhttps://doi.org/10.1109/5254.889104\nHénard, F., & Roseveare, D. (2012). Fostering quality teaching \nin higher education: Policies and practices.  An IMHE Guide \nfor Higher Education Institutions, 1 (1), 7-11. http://dx.doi.\norg/10.5901/mjss.2014.v5n25p272\nHussein, M. A., Hassan, H., & Nassef, M. (2019). Automated \nlanguage essay scoring systems: A literature review. PeerJ \nComputer Science, 5,  Article e208. https://doi.org/10.7717/\n74Journal of Applied Learning & Teaching Vol.8 No.1 (2025)\npeerj-cs.208\nIndrayan, A. (2022). Direct use of clinical tolerance limits for \nassessing agreement: A robust nonparametric approach. SSRN \nElectronic Journal. https://doi.org/10.2139/ssrn.4189799\nIshida, T., Liu, T., Wang, H., & Cheung, W. K. (2024, May \n28). Large language models as partners in student essay \nevaluation. arXiv.org. https://arxiv.org/abs/2405.18632\nKanwal, A., Rafiq, S., & Afzal, A. (2023). Impact of workload \non teachers’ efficiency and their students’ academic \nachievement at the university level. Gomal University Journal \nof Research, 39 (2), 131–146. https://doi.org/10.51380/gujr-\n39-02-02\nKayapinar, U. (2014). Measuring essay assessment: Intra-rater \nand inter-rater reliability. Eurasian Journal of Educational \nResearch, 14(57). https://doi.org/10.14689/ejer.2014.57.2\nKhademi, A. (2023). Can ChatGPT and Bard generate aligned \nassessment items? A reliability analysis against human \nperformance. Journal of Applied Learning & Teaching, 6 (1), \n75-80. https://doi.org/10.37074/jalt.2023.6.1.28\nKim, S., & Jo, M. (2024, July 8). Is GPT-4 alone sufficient \nfor automated essay scoring?: A comparative judgment \napproach based on rater cognition.  arXiv.org. https://arxiv.\norg/html/2407.05733v1\nKoo, T. K., & Li, M. Y. (2016). A guideline of selecting and \nreporting intraclass correlation coefficients for reliability \nresearch. Journal of Chiropractic Medicine, 15 (2), 155–163. \nhttps://doi.org/10.1016/j.jcm.2016.02.012\nKurniawan, W., Riantoni, C., Lestari, N., & Ropawandi, D. (2024). \nA hybrid automatic scoring system: Artificial intelligence- \nbased evaluation of physics concept comprehension essay \ntest. International Journal of Information and Education \nTechnology, 14 (6), 876–882. https://doi.org/10.18178/\nijiet.2024.14.6.2113\nLange, R. T. (2011). Inter-rater reliability. In J. S. Kreutzer, J. \nDeLuca, B. (Eds.), Encyclopedia of clinical neuropsychology. \nSpringer. https://doi.org/10.1007/978-0-387-79948-3_1203\nLee, M., Liang, P., & Yang, Q. (2022). CoAuthor: Designing \na human-AI collaborative writing dataset for exploring \nlanguage model capabilities. CHI Conference on Human \nFactors in Computing Systems, (388), 1-19. https://doi.\norg/10.1145/3491102.3502030\nLi, W., & Liu, H. (2024). Applying large language models \nfor automated essay scoring for non-native Japanese. \nHumanities and Social Sciences Communications, 11 (1). \nhttps://doi.org/10.1057/s41599-024-03209-9\nLi, Z., Zhang, J., Fei, Z., Feng, Y., & Zhou, J. (2021, June 4). \nAddressing inquiries about history: An efficient and practical \nframework for evaluating open-domain chatbot consistency. \narXiv.org. https://arxiv.org/abs/2106.02228\nLiljequist, D., Elfving, B., & Roaldsen, K. S. (2019). Intraclass \ncorrelation – a discussion and demonstration of basic \nfeatures. PLoS ONE, 14 (7), Article e0219854. https://doi.\norg/10.1371/journal.pone.0219854\nLin, L. I. (1989). A concordance correlation coefficient to \nevaluate reproducibility. Biometrics, 45 (1), 255. https://doi.\norg/10.2307/2532051\nMaclaren, U. (2024). Do you know when to use 0-shot, 1-shot, \nor multi-shot prompts (e.g. give it 1 or more examples)? SSW \nRules. https://www.ssw.com.au/rules/shot-prompts/\nMansour, W., Albatarni, S., Eltanbouly, S., & Elsayed, T. (2024, \nApril 26). Can large language models automatically score \nproficiency of written essays?.  arXiv.org. https://arxiv.org/\npdf/2403.06149\nMeta. (n. d.). Llama 3.1 70B.  Hugging face. https://\nhuggingface.co/meta-llama/Llama-3.1-70B\nMeyer, J., Jansen, T., Schiller, R., Liebenow, L. W., Steinbach, \nM., Horbach, A., & Fleckenstein, J. (2024). Using LLMs to \nbring evidence-based feedback into the classroom: AI-\ngenerated feedback increases secondary students’ text \nrevision, motivation, and positive emotions. Computers and \nEducation: Artificial Intelligence, 6 (1), 100199. http://dx.doi.\norg/10.1016/j.caeai.2023.100199. \nMizumoto, A., & Eguchi, M. (2023). Exploring the potential \nof using an AI language model for automated essay scoring. \nResearch Methods in Applied Linguistics, 2(2), 100050. https://\ndoi.org/10.1016/j.rmal.2023.100050.\nNichols, D. P. (1998).  SPSS Library: Choosing an intraclass \ncorrelation coefficient. OARC Stats. https://stats.oarc.ucla.\nedu/spss/library/spss-library-choosing-an-intraclass-\ncorrelation-coefficient/\nOpenAI. (n. d.). Models. OpenAI platform. https://platform.\nopenai.com/docs/models\nOrtiz-Zambrano, J. A., Espín-Riofrío, C. H., & Montejo-Ráez, \nA. (2024). Deep encodings vs. linguistic features in lexical \ncomplexity prediction. Neural Computing and Applications. \nhttps://doi.org/10.1007/s00521-024-10662-9 \nOuyang, L., Wu, J., Jiang, X., Almeida, D., L. Wainwright, \nC., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, Al., \nSchulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, \nA., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). \nTraining language models to follow instructions with human \nfeedback (36th Conference on Neural Information Processing \nSystems: Advances in Neural Information Processing Systems, \nVol. 35 ). https://proceedings.neurips.cc/paper_files/\npaper/2022/file/b1efde53be364a73914f58805a001731-\nPaper-Conference.pdf\nPack, A., Barrett, A., & Escalante, J. (2024). Large language \nmodels and automated essay scoring of English language \nlearner writing: Insights into validity and reliability. \nComputers and Education Artificial Intelligence, 6 , 100234. \nhttps://doi.org/10.1016/j.caeai.2024.100234\n75Journal of Applied Learning & Teaching Vol.8 No.1 (2025)\nPerelman, L. (2014). When “the state of the art” is counting \nwords. Assessing Writing, 21 , 104–111. https://doi.\norg/10.1016/j.asw.2014.05.001\nPollitt, A. (2011). Comparative judgement for assessment. \nInternational Journal of Technology and Design Education, \n22(2), 157–170. https://doi.org/10.1007/s10798-011-9189-x\nPopenici, S. (2022). Artificial intelligence and learning futures. \nhttps://doi.org/10.4324/9781003266563\nPopenici, S., Rudolph, J., Tan, S., & Tan, S. (2023). A critical \nperspective on generative AI and learning futures. An \ninterview with Stefan Popenici. Journal of Applied Learning \n& Teaching, 6 (2), 311-331. https://doi.org/10.37074/\njalt.2023.6.2.5\nRamesh, D., & Sanampudi, S. K. (2021). An automated \nessay scoring systems: A systematic literature review. \nArtificial Intelligence Review, 55 (3), 2495–2527. https://doi.\norg/10.1007/s10462-021-10068-2\nRudolph, J. (2023). Book review. Popenici, Stefan (2023). \nArtificial intelligence and learning futures. Critical narratives \nof technology and imagination in higher education. \nRoutledge. Journal of Applied Learning & Teaching, 6(2), 420-\n425. https://doi.org/10.37074/jalt.2023.6.2.27\nRudolph, J., Ismail, F., & Popenici, S. (2024). Higher education’s \ngenerative artificial intelligence paradox: The meaning of \nchatbot mania. Journal of University Teaching and Learning \nPractice, 21(6). https://doi.org/10.53761/54fs5e77\nSilveira, P. S. P., Vieira, J. E., & De Oliveira Siqueira, J. \n(2024). Is the Bland-Altman plot method useful without \ninferences for accuracy, precision, and agreement? Revista \nDe Saúde Pública, 58 (1). https://doi.org/10.11606/s1518-\n8787.2024058005430\nShrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: Uses \nin assessing rater reliability. Psychological Bulletin, 86 (2), \n420–428. https://doi.org/10.1037/0033-2909.86.2.420\nStahl, M., Biermann, L., Nehring, A., & Wachsmuth, H. (2024, \nApril 24). Exploring LLM prompting strategies for joint essay \nscoring and feedback generation. arXiv.org. https://arxiv.org/\nabs/2404.15845\nStratford, P. W., & Goldsmith, C. H. (1997). Use of the \nstandard error as a reliability index of interest: An applied \nexample using elbow flexor strength data. Physical Therapy, \n77(7), 745–750. https://doi.org/10.1093/ptj/77.7.745. \nTaffé, P. (2021). When can the Bland & Altman limits of \nagreement method be used and when it should not be used. \nJournal of Clinical Epidemiology, 137 , 176–181. https://doi.\norg/10.1016/j.jclinepi.2021.04.004\nTate, T. P., Steiss, J., Bailey, D., Graham, S., Moon, Y., Ritchie, \nD., Tseng, W., & Warschauer, M. (2024). Can AI provide \nuseful holistic essay scoring? Computers and Education \nArtificial Intelligence, 7,  100255. https://doi.org/10.1016/j.\ncaeai.2024.100255.\nUto, M. (2021). A review of deep-neural automated essay \nscoring models. Behaviormetrika, 48 (2), 459–484. https://\ndoi.org/10.1007/s41237-021-00142-y\nVaz, S., Falkmer, T., Passmore, A. E., Parsons, R., & Andreou, P. \n(2013). The case for using the repeatability coefficient when \ncalculating test–retest reliability. PLoS ONE, 8 (9), Article \ne73990. https://doi.org/10.1371/journal.pone.0073990\nWarschauer, M., & Grimes, D. (2008). Automated \nwriting assessment in the classroom. Pedagogies \nan International Journal, 3 (1), 22–36. https://doi.\norg/10.1080/15544800701771580.\nWilks, Y. (2005). The history of natural language processing \nand machine translation. Encyclopedia of language and \nlinguistics (Vol. 9). \nXiao, C., Ma, W., Song, Q., Xu, S. X., Zhang, K., Wang, Y., & Fu, \nQ. (2024, January 12). Human-AI collaborative essay scoring: \nA dual-process framework with LLMs. arXiv.org. https://arxiv.\norg/abs/2401.06431\nZhu, W. (2019). A study on the application of automated \nessay scoring in college English writing based on PigAi. \nProceedings of the 7th International Conference on Social \nScience and Higher Education (ICSSHE 2021) . https://doi.\norg/10.2991/icsshe-19.2019.188\nAppendices\nAppendix A: Score output for repeated measurements.\n\n76Journal of Applied Learning & Teaching Vol.8 No.1 (2025)\nAppendix B: Scoring criteria.\nAppendix C: Excerpt text outputs.\n\n77Journal of Applied Learning & Teaching Vol.8 No.1 (2025)\nCopyright: © 2025. Siti Bealinda Qinthara Rony, Tan Xin Fei and Sasa Arsovski. This is an open-access article distributed under the terms of the Creative Commons \nAttribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are \ncredited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted \nwhich does not comply with these terms.",
  "topic": "Consistency (knowledge bases)",
  "concepts": [
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.6383266448974609
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.6313259601593018
    },
    {
      "name": "Computer science",
      "score": 0.539300262928009
    },
    {
      "name": "Economic Justice",
      "score": 0.5077632665634155
    },
    {
      "name": "Natural language processing",
      "score": 0.4818740785121918
    },
    {
      "name": "Psychology",
      "score": 0.3703067898750305
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3514143228530884
    },
    {
      "name": "Linguistics",
      "score": 0.3486800193786621
    },
    {
      "name": "Political science",
      "score": 0.14972954988479614
    },
    {
      "name": "Philosophy",
      "score": 0.10361471772193909
    },
    {
      "name": "Law",
      "score": 0.06619766354560852
    },
    {
      "name": "Physics",
      "score": 0.059913039207458496
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 2
}