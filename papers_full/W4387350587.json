{
  "title": "SwinAnomaly: Real-Time Video Anomaly Detection Using Video Swin Transformer and SORT",
  "url": "https://openalex.org/W4387350587",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4321052140",
      "name": "Arpit Bajgoti",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4321052141",
      "name": "Rishik Gupta -",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2629414293",
      "name": "Prasanalakshmi Balaji",
      "affiliations": [
        "King Khalid University"
      ]
    },
    {
      "id": "https://openalex.org/A2547549825",
      "name": "Rinky Dwivedi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118983545",
      "name": "Meena Siwach",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2066421770",
      "name": "Deepak Gupta",
      "affiliations": [
        "Maharaja Engineering College"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4214612132",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2765811365",
    "https://openalex.org/W6798351829",
    "https://openalex.org/W6685352114",
    "https://openalex.org/W6755773381",
    "https://openalex.org/W6748645028",
    "https://openalex.org/W2252355370",
    "https://openalex.org/W2341058432",
    "https://openalex.org/W3015832418",
    "https://openalex.org/W2970645682",
    "https://openalex.org/W2579718262",
    "https://openalex.org/W2753526808",
    "https://openalex.org/W2981741013",
    "https://openalex.org/W3101466234",
    "https://openalex.org/W2963610939",
    "https://openalex.org/W6730548322",
    "https://openalex.org/W6758841785",
    "https://openalex.org/W2981650061",
    "https://openalex.org/W2989705574",
    "https://openalex.org/W2987146532",
    "https://openalex.org/W3168984673",
    "https://openalex.org/W3202590754",
    "https://openalex.org/W4313067886",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6839023977",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3196890755",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W2987228832",
    "https://openalex.org/W3022606336",
    "https://openalex.org/W3176309086",
    "https://openalex.org/W3035240825",
    "https://openalex.org/W4285597156",
    "https://openalex.org/W4386071499",
    "https://openalex.org/W6846500891",
    "https://openalex.org/W4281478450",
    "https://openalex.org/W3180897515",
    "https://openalex.org/W4365802465",
    "https://openalex.org/W2145938889",
    "https://openalex.org/W2963684088",
    "https://openalex.org/W2900501183",
    "https://openalex.org/W2559927751",
    "https://openalex.org/W4301054805",
    "https://openalex.org/W4310633449",
    "https://openalex.org/W3104218139"
  ],
  "abstract": "Detecting anomalous events in videos is a challenging task due to their infrequent and unpredictable nature in real-world scenarios. In this paper, we propose SwinAnomaly, a video anomaly detection approach based on a conditional GAN-based autoencoder with feature extractors based on Swin Transformers. Our approach encodes spatiotemporal features from a sequence of video frames using a 3D encoder and upsamples them to predict a future frame using a 2D decoder. We utilize patch-wise mean squared error and Simple Online and Real-time Tracking (SORT) for real-time anomaly detection and tracking. Our approach outperforms existing prediction-based video anomaly detection methods and offers flexibility in localizing anomalies through several parameters. Extensive testing shows that SwinAnomaly achieves state-of-the-art performance on public benchmarks, demonstrating the effectiveness of our approach for real-world video anomaly detection. Furthermore, our proposed approach has the potential to enhance public safety and security in various applications, including crowd surveillance, traffic monitoring, and industrial safety.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nSwinAnomaly: Real-Time Video Anomaly\nDetection using Video Swin Transformer\nand SORT\nARPIT BAJGOTI1, RISHIK GUPTA1, PRASANALAKSHMI BALAJI2, RINKY DWIVEDI1, MEENA\nSIWACH 1, DEEPAK GUPTA3\n1Department of Computer Science and Engineering, Maharaja Surajmal Institute of Technology, New Delhi 110058, India\n2Department of Computer Science, College of Computer Science, King Khalid University, Abha, Saudi Arabia\n3 UCRD, Chandigarh University, Gharuan, Mohali, Punjab, India\nCorresponding Author: Prasanalakshmi Balaji (drsana@ieee.org)\nThis work was supported in part by the Deanship of Scientific Research at King Khalid University through Small Group Research Project\nunder grant number RGP1/325/44.\nABSTRACT Detecting anomalous events in videos is a challenging task due to their infrequent and\nunpredictable nature in real-world scenarios. In this paper, we propose SwinAnomaly, a video anomaly\ndetection approach based on a conditional GAN-based autoencoder with feature extractors based on Swin\nTransformers. Our approach encodes spatiotemporal features from a sequence of video frames using a\n3D encoder and upsamples them to predict a future frame using a 2D decoder. We utilize patch-wise\nmean squared error and Simple Online and Realtime Tracking (SORT) for real-time anomaly detection\nand tracking. Our approach outperforms existing prediction-based video anomaly detection methods\nand offers flexibility in localizing anomalies through several parameters. Extensive testing shows that\nSwinAnomaly achieves state-of-the-art performance on public benchmarks, demonstrating the effectiveness\nof our approach for real-world video anomaly detection. Furthermore, our proposed approach has the\npotential to enhance public safety and security in various applications, including crowd surveillance, traffic\nmonitoring, and industrial safety.\nINDEX TERMS Video anomaly detection, SORT, GAN-based autoencoder, Real-time tracking\nI. INTRODUCTION\nV\nIDEO anomaly detection is a technique used in com-\nputer vision to automatically detect unusual or anoma-\nlous events in video data. This can be useful for a wide range\nof applications, including surveillance, security, and traffic\nmonitoring. However, this task has become increasingly chal-\nlenging in recent times due to diversely distributed data. One\nmethod that has been employed to tackle this problem is\none-class classification, where only normal videos are shown\nwithout any anomaly, and at testing the pre-trained model\nis required to distinguish between normal and anomalous\nevents in complex video data.\nOver the years, several methods such as reconstruction-\nbased methods and prediction-based methods have achieved\nsignificant progress in anomaly detection. Reconstruction\nmethods work on the principle that normal events can be\nreconstructed accurately whereas abnormal events will have\na higher reconstruction error and hence can be detected as\nan anomaly. Prediction based method on the other hand\nworks on the principle that normal events can be accurately\npredicted, while abnormal events would not be correctly\npredicted.\nConvolutional Neural Networks (CNNs) are necessary\nfor prediction and reconstruction-based methods for their\npattern recognition capabilities. However, they suffer from\nchallenges such as the inability to find unexpected anomalies\nthat deviate significantly from the training data since CNNs\nare designed to identify patterns in data. Another limitation of\nCNNs is the extensive computational power that they require\nwhich can be time-consuming for larger datasets such as\nvideo data.\nTransformers in recent times have shown exceptional ca-\npabilities in Natural Language Processing (NLP) tasks such\nas language translation, text summarization, and sentiment\nanalysis. Unlike CNNs which process input sequences se-\nquentially, transformers process the entire sequence of input\ndata in go. To achieve this, a mechanism called self-attention\nis used. Following the success of transformers in NLP, several\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nadvancements have been made in computer vision employing\nthe use of transformers. Firstly, being introduced in Vision\nTransformer (ViT) [1] which takes 16x16 image patches for\nimage classification. It was later adapted into Video Vision\nTransformer (ViViT) [2], which explored the application of\nViT in video classification. The main idea behind using\ntransformers in computer vision is to apply the self-attention\nmechanism to the spatial dimensions of the input image,\nwhich is achieved using a technique called spatial self-\nattention.\nA. SWIN TRANSFORMER\nSwin Transformer is a type of transformer architecture ini-\ntially used for image classification tasks, first introduced by\nMicrosoft Research Asia [3] in 2021. In the Swin Trans-\nformer, the input image is first divided into small patches, and\nthen these patches are further divided into feature maps. This\nmethod allows the Swin Transformer to capture both global\nand local features of the image. The use of shifted windows\nenables the transformer to capture more fine-grained details\nand better handle small objects in the image. In the context\nof autoencoder-based methods for video anomaly detection,\nSwin Transformers offer several advantages such as better\nfeature extraction as it uses self-attention to capture long-\nrange dependencies in the input sequence, reduced computa-\ntional cost, and improved reconstruction quality by reducing\ndistortion in the reconstructed frames. The Swin Transformer\nsupports upsampling and downsampling, unlike ViViT which\nmakes it the optimal choice to be used as an encoder and\ndecoder in a Generative Adversarial Network (GAN) for\nimage generation tasks.\nB. GENERATIVE ADVERSARIAL NETWORKS\nGenerative Adversarial Networks (GANs) have been suc-\ncessful in generating realistic images [4]. Recently, there\nhas been a growing interest in combining parallel processing\nof transformers and realistic image generation of GANs\nfor image segmentation tasks. In this approach, a generator\nnetwork, which is a transformer-based architecture, takes an\ninput image and produces a segmentation map. The discrim-\ninator network, which is typically a CNN-based architecture,\ntakes both the input image and generated image and tries\nto distinguish between real and fake images. Vision Trans-\nformer GAN (ViTGAN) is one such model that was proposed\nby Lee et al. [5] which uses a transformer-based generator\nand a CNN-based discriminator and has achieved state-of-\nthe-art results. Similar to VitGAN, our model SwinAnomaly\nuses a swin transformer-based generator and discriminator\nwhich utilizes the benefits of using a swin transformer.\nIn recent years, several different architectures of GANs\nsuch as Deep Convolutional GAN (DCGANs) [6], [7] and\nPatchGAN [8] were introduced for image synthesis tasks.\nThe CNN-based discriminator network type DCGAN pro-\ncesses the input image using many layers of convolutional\nand pooling procedures to produce a scalar probability score\nrepresenting the realism of the image. DCGANs can have\nFIGURE 1. Two successive Video Swin Transformer blocks\ndifficulty producing sharp edges and fine-grained features,\nbut they are good at producing high-quality images with\nrealistic textures and details. PatchGAN on the other hand\nconcentrates on assessing the realism of local image patches.\nThe input image is divided into several, non-overlapping\npatches using the PatchGAN, and each patch’s level of real-\nism is assessed independently. The discriminator can capture\nfine-grained information in the image and produce higher-\nquality photos with more realistic textures by employing\nthis method. While PatchGANs can provide images that are\nclearer and more detailed than DCGANs, they may also take\nlonger to train. The results of using both PatchGAN and\nDCGAN in the proposed approach have been discussed in\nthe paper.\nAlthough many machine learning-based anomaly detec-\ntion methods have been introduced in recent years, they still\nsuffer from some limitations. The current video anomaly\ndetection methods struggle to generalize well to unseen or\nnovel scenarios, performing well on the datasets they were\ntrained on but fail to detect anomalies in new and diverse\nenvironments. Some methods, particularly those based on\nneural networks can be computationally intensive and re-\nquire significant computational resources, making real-time\ndeployment challenging.\nIn this paper, we propose SwinAnomaly, a novel architec-\nture that aims to enhance anomaly detection in video datasets,\nalso allowing the model to be used in real-life scenarios. The\nencoder of the generator in the PatchGAN adopts the Video\nSwin Transformer architecture, while the decoder utilizes\n2D Swin-transformer blocks to predict future frames from\na sequence of video frames. Real-time anomaly detection is\nachieved by integrating an object detection model and the\nSORT algorithm, enabling the detection of anomalies using\nreconstruction error patches. Experimental results validate\nthe superiority of SwinAnomaly over other prediction-based\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nmethods, highlighting its potential to enhance anomaly de-\ntection systems for more accurate and efficient real-world\napplications. The main contributions of the paper are as\nfollows:\n• We propose a model that combines Swin-transformers’\npatchwise feature extraction with the reconstruction ca-\npabilities of convolution-based autoencoders.\n• SwinAnomaly operates on the current set of frames,\nwithout requiring a specific frame accumulation, al-\nlowing for the definition and continuous tracking of\nanomalies.\n• We develop a technique that uses an object detection\nmodel such as YOLOv7 along with the SORT algorithm\nto identify and track anomalies in real time.\nII. RELATED WORKS\nSeveral works have been done in the field of anomaly\ndetection in the past decade. Generally, unsupervised\nvideo anomaly detection methods can be categorized into\nreconstruction-based methods and prediction-based methods.\nA. RECONSTRUCTION-BASED METHODS\nReconstruction-based methods are a popular approach for\nvideo anomaly detection. The idea behind this method is\nto learn a model of normal behavior from a training set\nof videos, then use this model to identify anomalies in\nnew videos. It assumes that frames having anomalies will\nhave higher reconstruction errors than frames without any\nanomalies. The most popular model used is the autoencoder,\nconsisting of a convolutional encoder [11]. Other approaches\nby Fan et al. [12] and Li and Chang [13] used Variational\nAuto-Encoders (V AEs) to reconstruct the input frames, and\nthe distribution difference of the latent representations was\nutilized to calculate the regularity scores. However, training\nV AEs was found to be difficult, particularly if the dataset was\nuneven or limited. Furthermore, the inability of V AEs to ex-\nplicitly express temporal relationships may limit their ability\nto accurately capture the dynamics of a video sequence. To\novercome this, Chong et al. [14] and Luo et al. [15] combined\nConvolutional Long Short-Term Memory (ConvLSTM) with\nConvolutional Auto-Encoder to extract more temporal in-\nformation from input sequences. Incorporating the tempo-\nral information into the model helped improve its accuracy\nby allowing it to learn the dynamics of normal behavior\nover time. To find anomalies, certain reconstruction-based\ntechniques also take advantage of the latent representational\ndifferences between normal and abnormal samples. However,\nusing ConvLSTM had several challenges such as high com-\nputational cost, overfitting, and it could not handle complex\nscenes with multiple objects and occlusions. In recent years,\noptical flow-based motion constraints have also been used\nin combination with Convolutional Auto-Encoder [16]. But\nusing optical flow comes with its own limitations such as high\ncomputational cost, sensitivity to noise, and lack of semantic\nunderstanding. Chang et al. [17] employed two autoencoders\nto individually leverage the spatial and temporal information\nof videos to get around the high computational cost of optical\nflow. While the temporal autoencoder recorded the move-\nment data of the objects, the spatial autoencoder encoded the\nscenes and objects.\nB. PREDICTION-BASED METHODS\nPrediction-based methods for video anomaly detection in-\nvolve training a model to predict future frames or events in\na video sequence and detecting anomalies as deviations from\nthis prediction. The basic assumption is that the normal event\nis predictable whereas the abnormal one is unpredictable\n[18]. These methods typically use Recurrent Neural Net-\nworks (RNNs) or Convolutional Neural Networks (CNNs)\nto learn the temporal dynamics of the video sequence and\npredict future frames or events. One common method is to\npredict upcoming frames in the video sequence based on\nprevious frames using a sequence-to-sequence model, such\nas an encoder-decoder architecture with a Conv-LSTM [19]\nor Gated Recurrent Unit (GRU) [20] based RNN. The model\ncan produce precise frame predictions under typical circum-\nstances after being trained on a collection of typical video\nsequences. When there are departures from this prediction,\nsuch as abrupt changes in the projected frame or mistakes\nin the predicted motion, anomalies are recognized. In recent\nyears, U-Net has been used as a generator for predicting the\nnext frame, and a patch discriminator is used to distinguish\nthe generated frames. Liu et al. [18] proposed a similar\nnetwork architecture where U-net was used as a generator,\nand the model computed a regularity score with only the\nprediction error without reconstruction. However, using U-\nNet can be computationally expensive, they are not well\nsuited for detecting anomalies that occur over longer time\nscales. In certain instances, reconstruction is combined with\nprediction models. For instance, Ye et al. [21] presented\na Predictive Coding Network that, initially, predicts future\nframes using a ConvLSTM with predictive coding. Then,\nreconstruction techniques are used to reduce prediction er-\nrors. Lastly, to improve prediction performance, the predicted\nframes are updated with refined errors. This method still uses\nprediction error to get the regularity score while additionally\nconsidering reconstruction difference. In addition, Li et al.\n[22] replaced the original U-Net generator with a spatial-\ntemporal U-Net that added three ConvLSTM layers between\nthe U-Net layers to extract more temporal information. Tang\net al. [23] on the other hand followed a hybrid approach by\ncombining both prediction and reconstruction-based methods\nby connecting two U-Net blocks together, the first one for\npredicting the future frame and the second for reconstructing\nthe future frame.\nAlthough Convolutional based methods work well, they\nrequire high computational power to generate reconstructed\nimages. In our proposed approach, we use transformers to\nsignificantly reduce the processing cost of the individual\nframes which is achieved because of the self-attention mech-\nanism and parallel processing capabilities of transformers.\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 2. Conditional PatchGAN-based workflow of proposed approach\nC. OBJECT LEVEL DETECTION\nThe methods discussed above operate on entire video frames.\nThis might prove to be difficult for anomaly detection as the\nvideo frames are complex, contain variations, and contain\na large number of objects. More recent methods such as\nObject-level video anomaly detection aim to identify anoma-\nlies at the level of individual objects or regions within a\nvideo sequence [24]–[26]. They first extract object bound-\naries using object detection algorithms. Then the tracked\nobjects are checked to be anomalous. This is an easier task\nsince individual objects contain much less variation than\nthe whole frame. The need for precise and effective object\ndetection and tracking algorithms is one difficulty in object-\nlevel video anomaly detection. Occlusion, motion blur, and\nchanges in lighting or background can all affect how accurate\nthese algorithms are. In addition, these algorithms may be\ndifficult to implement in real-time applications due to their\ncomputational cost.\nD. TRANSFORMER BASED METHODS\nFollowing the success of transformers in the field of NLP\n[27], many attempts have been made to replicate this success\nfor computer vision tasks. Due to their capacity to represent\nlong-term dependencies and capture intricate spatiotemporal\ncorrelations in video sequences, transformers have demon-\nstrated promise in the field of video anomaly identification.\nTransformers are more effective at simulating temporal dy-\nnamics than more conventional approaches like ConvLSTM\nand can minimize processing time. Deshpande et al. [28] used\na three-stage model consisting of a pre-trained videoswin\nmodel, an attention layer, and lastly an RFTM model for\nanomaly detection. TransUNet is a hybrid of U-Net with a\ntransformer encoder, as suggested by Chen et al. [29]. Tran-\nsUNet, a hybrid CNN-Transformer architecture, makes use\nof both the global context stored by the transformer encoder\nand the finely detailed, high-resolution spatial information\nfrom CNN features. Yuan et al. [30] proposed a similar\narchitecture that combined U-Net and ViViT to capture richer\ntemporal information and more global contexts. The ViViT\nwas modified to make it capable of video prediction. Another\nstrategy is to combine the usage of transformers with other\nmachine learning methods, like object detection and tracking\nalgorithms. This can entail projecting the future trajectory of\ncertain objects inside a video sequence using a transformer-\nbased model, then analyzing the behavior and motion of the\nitems to spot anomalies.\nWhile these methods have demonstrated impressive per-\nformance and yielded favorable outcomes across various\ndatasets, their reliance on a substantial number of frames\nduring inference poses certain limitations. Moreover, these\napproaches are primarily designed to detect local anoma-\nlies, meaning their ability to identify global anomalies is\nrestricted. In cases where the chosen set of frames contains a\nconsiderable number of anomalies, the distinction between\nthe minimum and maximum Peak Signal-to-Noise Ratio\n(PSNR) becomes indiscernible, leading to a smooth anomaly\ncurve. As a consequence, the effectiveness of these methods\nin accurately capturing and differentiating anomalies is di-\nminished.\nIII. PROPOSED WORK\nThe overall architecture of the SwinAnomaly model is based\non a Generative Adversarial Network where the generator\nis a Swin-UNET [31] based Autoencoder containing a 3D\nEncoder with Video Swin transformer layers and a decoder\nbased on Swin-UNet decoder layers, The generator takes 3D\ninput in the shape of (c, t, w, h) as shown in Fig. 4(a) and\noutputs a 2D predicted frame with dimensions (c, w, h), the\ngenerator tries to predict new frame ˆft+1 and the discrimina-\ntor tries to differentiate between actual frames in the frame\nsequence and the predicted frame from the generator in an\nadversarial way, as shown in Fig. 2. The detailed overview\nof the proposed GAN constituents is shown in Fig. 3 and is\ndiscussed as follows:\nA. FRAME ENCODER\nThe Encoder takes the input in the form f1, f2, ..., fT where\neach matrix f is a video frame of size W × H × C and T\nis the number of frames taken together and concatenated to\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 3. Architecture of proposed SwinAnomaly generator\ngive a size of T × W × H × C to the input. The input is\npassed to a 3D patch partition layer which divides the input\ninto T\nt × W\n4 × H\n4 patches where t is the number of frame\npartitions in the input data as shown in Fig. 4(b). The value\nof t is a small integer value where kt = T as the frame\nsize must be justified for input frames to be converted from\na 3D input to a 2D patch where each patch contains the\ninformation content of the corresponding pixels of the frames\nin sequential order as shown in Fig.4(a). After the patch\npartition, a linear embedding layer is applied to project the\npatches into E-dimensional feature vectors, in the Swin-T-\nbased architecture the embedding dimensions are taken as E\n= 96. The multi-head attention layer (MSA) is replaced with\n3D windowed MSA and the consecutive block with cyclic\nshifted windowed MSA with shift space of(2, 2, 2) tokens as\nshown in Fig. 4(c), each encoder block’s output is resized and\npropagated through an MLP layer to downsample the 3D in-\nput to 2D embedding which is passed to the adjacent decoder\nblock as a skip connection to the decoder. Each encoder block\nis also down-sampled using a patch-merging layer, where the\npatches are reduced by two times i.e.(T × W\n2 × H\n2 ×2C), the\nconsecutive blocks in each video swin transformer is shown\nin Fig. 1. It can be noticed that the channels and the image\ndimensions are altered to extract the features but the temporal\nframe patch size t is not reduced to maintain the integrity of\nour model during skip connections and upsampling [32].\na: 3D Shifted Window based MSA:\nAs the multi-head self-attention mechanism is used only\nwithin each non-overlapping 3D window, there are no con-\nnections between windows, which may reduce the architec-\nture’s capability for representation. As a result, the shifted 2D\nwindow mechanism of the Swin Transformer is converted to\n3D windows in order to introduce cross-window connections,\nwhich is achieved by the circular shift of the patched frames\nas proposed by Liu et al. [32] while preserving the effective\ncalculation of non-overlapping window-based self-attention.\nˆzl = 3DW − MSA (LN(zl−1)) +zl−1,\nzl = MLP (LN(ˆzl)) + ˆzl,\nˆzl+1 = 3DSW − MSA (LN(zl)) +zl,\nzl+1 = MLP (LN(ˆzl+1)) + ˆzl+1\n(1)\nwhere ˆzl and zl represent the outputs of the (S)W − MSA\nmodule and the MLP module of the lth block, respectively.\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 4. A 3D shifted window used in the generator. (a) The input of the generator with T consecutive frames (b) Patch partition of the input into tokens of shape\nT\nt × W\n4 × H\n4 (c) A cyclically shifted window by (2, 2, 2) tokens for extensive feature extraction\nb: 3D Relative Position Bias:\nThe addition of a relative position bias B significantly im-\nproves the performance of swin-transformers in feature ex-\ntraction. For incorporating 3D patches, a new dimension P\nis added which represents patches in the 3rd dimension. The\nAttention mechanism of the attention layer in Swin block 3D\nis governed by the following equation:\nAttention(Q, K, V) =SoftMax\n\u0012QKT\n√\nd\n+ B\n\u0013\nV (2)\nwhere the query, key, and value matrices are Q, K, V∈\nRPM 2×d, d is the dimension of the query and key features,\nand PM 2 is the total number of tokens in a 3D window. A\nsmaller-sized bias matrix, ˆB ∈ R(2P−1)×(2M−1)×(2M−1) is\nparameterized, and values in B are picked from ˆB because\nthe relative position along each axis falls within the range of\n[−P + 1, P− 1] (temporal) or [−M + 1, M− 1] (height or\nwidth).\nc: Patch merging layer:\nThe patch merging layer separates the input patches into 4\nparts, which are then concatenated. The feature resolution\nwill be two times down-sampled with this processing. Addi-\ntionally, because the concatenate operation causes the feature\ndimension to increase by 4×, to reduce the complexity of\nthe model, a linear layer is applied to the concatenated\nfeatures to bring the feature dimension back to the original\n2× dimension.\nB. FRAME DECODER\nThe symmetric decoder in Swin Transformer is constructed\nbased on Swin Transformer blocks, similar to the encoder.\nHowever, the decoder applies attention to 2D patches instead\nof 3D. Unlike the patch merging layer used in the encoder,\nthe decoder utilizes a patch-expanding layer to up-sample\nthe extracted deep features. This layer reshapes the adjacent\ndimension feature maps into a higher resolution feature map\nwith 2× up-sampling while reducing the feature dimension\nto half of the original dimension.\na: Patch expanding layer:\nLet’s take the first patch expanding layer as an example. Prior\nto up-sampling, a linear layer is applied to the input features\n(W\n32 × H\n32 × 8C) to increase the feature dimension to 2x the\noriginal dimension ( W\n32 × H\n32 × 16C). Then, a rearrange op-\neration expands the input feature resolution to 2x the original\nresolution and reduces the feature dimension to a quarter of\nthe input dimension (W\n32 × H\n32 × 16C → W\n16 × H\n16 × 4C).\nb: 2D Shifted Window MSA and Positional Bias\nThe decoder uses 2D patches unlike encoders hence the\nequations 1 and 2 can be easily modified to sample down\nthe attention mechanism to 2D matrices, in the swin trans-\nformer(2D) block the P dimensions are discarded in eq 2,\nand the MSA is changed from 3D to 2D.\nC. BOTTLENECK AND SKIP CONNECTIONS\nThe bottleneck connects the 3D encoder and 2D decoder\nmodels together, and the encoder’s final layer inputs a feature\nspace with dimensions T\nt ×W\n32 × H\n32 ×8E which is a3× down-\nsampled feature vector of the original frame sequence. To\nconvert the features from 3D to 2D, a linear embedding on the\nrearranged encoder output is applied, this conscience to the\ninvariable temporal channels which only change during the\nattention mechanism in the shifted-window MSA of Video\nSwin Transformer and doesn’t affect the frame reconstruc-\ntion.\nThe layers in the encoder part are skip-connected and\nconcatenated with corresponding layers in the decoder part,\nthis helps the decoder retain the information of the encoder\nspatial features which gets diminished during down-sampling\nand helps in the image or frame reconstruction. The skip con-\nnections take the encoder layer outputs before patch merging\nand concatenate them to their corresponding decoder swin\ntransformer blocks by rearranging and reshaping them simi-\nlar to the bottleneck block.\nD. ANOMALY TRACKING USING SORT\nOur proposed approach for anomaly detection and evalu-\nation involves several steps. First, we calculate the patch-\nwise mean squared error on non-overlapping patches of the\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 5. Inference Workflow using Patchwise MSE and SORT tracking\npredicted frame and ground truth frame. This is done using\neq. 3:\nMSE i,j = 1\nn\ni+p−1X\nx=i\nj+p−1X\ny=j\n(fx,y − ˆfx,y)2 (3)\nwhere MSE i,j represents the mean squared error for the\npatch at position (i, j), n is the number of pixels in the\npatch, p is the patch size, f is the ground truth frame, and\nˆf is the predicted frame. Next, we select the k maximum\nMSE patches based on the number of anomalies to capture.\nThese patches represent the regions in the frame that have the\nhighest anomaly scores. Then, we use the SORT algorithm to\ntrack the anomalies, as given by the equation:\nSORT(Bi) = ˆBi (4)\nwhere Bi represents the bounding boxes at frame f, and ˆBi\nrepresents the predicted bounding boxes at frame f using\nthe SORT algorithm. The SORT algorithm helps track the\nmovement of the anomaly regions across frames. After that,\nwe use an object detection model such as YOLOv7 to capture\nthe different bounding boxes in the original image, as given\nby the equation:\nBB = YOLOv7(f) (5)\nwhere BB represents the bounding boxes detected in the\noriginal frame f. The object detection model helps detect\nother objects in the scene that may be relevant to the anomaly.\nWe then find the IOU score on the rectangular bounding\nboxes and the max k MSE patches using the following\nequation:\nIOU(BBi, ˆBj) =sij (6)\nwhere sij represents the IOU score between the bounding\nbox at frame i and the predicted bounding box at frame\nj using the k max MSE patches. The IOU score helps us\ndetermine how well the predicted bounding boxes overlap\nwith the actual bounding boxes. To filter out the correct\nanomaly paths, we take a threshold u which measures the\nnumber of consecutive times the same object was a subject\nof anomaly. If it exceeds the frame count, it is considered an\nanomaly, as given by the equation:\nTi,j =\n(\n1 if Pi+u−1\nk=i sjk > τ\n0 otherwise (7)\nwhere Ti,j represents whether the anomaly is present in\nframe i and predicted in frame j, and τ represents the thresh-\nold for the sum of the IOU scores over u frames. Finally, we\ntrack the anomalies in the CCTV footage until they are out of\nthe camera’s scope using the equation:\nTrack(Ti,j) =Ai,j (8)\nwhere Ai,j represents the tracked anomaly at frame i and\npredicted in frame j. The tracking allows us to follow the\nanomaly and take necessary action.\nThe workflow shown in Fig. 5 enables the detection and\ntracing of anomalies, providing flexibility regarding the type,\nsize, and persistence of the abnormal object or activity.\nUnlike the sliding window PSNR approach, this method can\nbe used in real-time analysis of surveillance cameras, and can\nalso provide time stamps for suspicious or abnormal activities\nduring very long video sequences in surveillance.\nIV. EXPERIMENT\nIn this section, the proposed generator is evaluated on\nthe UCSD pedestrian, CUHK Avenue, and ShanghaiTech\ndatasets, and the effect of different discriminators and reg-\nularity scores are explored.\nA. DATASETS\nThe CUHK Avenue dataset consists of 16 training video\nclips and 21 testing video clips, all captured at the CUHK\ncampus avenue. The training videos exclusively feature reg-\nular situations, while the testing videos cover anomalous\nsituations, including strange actions, abnormal objects, and\nwrong directions. The UCSD Pedestrian dataset is divided\ninto two subsets: Ped1 and Ped2. Ped1 comprises 34 training\nvideo clips and 36 testing video clips, while Ped2 includes 16\ntraining video clips and 12 testing video clips. The training\nvideos in both subsets represent normal scenes, while the\ntesting videos feature abnormal targets such as bikes, cars,\nand skateboards. The ShanghaiTech dataset contains a more\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\ndiverse collection with 330 training videos and 107 test\nvideos, capturing 13 different scenes with various camera\nangles. This dataset presents a greater challenge during in-\nference, as it contains 130 abnormal events.\nB. TRAINING\nGANs refer to generative models that acquire knowledge of\na mapping from a random noise vector z to output image y,\nwhich can be represented as G : z → y. On the other hand,\nconditional GANs discover a mapping from a random noise\nvector z and an observed image x to produce an output image\ny, which can be described asG : x, z→ y. The generator G is\ndesigned to generate outputs that are indistinguishable from\nauthentic images by an adversarially trained discriminator\nD, which is trained to identify the generator’s \"fakes\" with\nmaximum accuracy. The standard loss function for GAN is a\nmin-max loss defined by the equation:\nL(G, D) =Ex,y [log (D (x, y))] +\nEx,z [log (1 − D (G (x, z)))] (9)\nIn the proposed approach, DCGAN [6] and PatchGAN [8]\ndiscriminators are used for adversarial training.\nC. DCGAN LOSS AND BINARY DISCRIMINATOR\nThe DCGAN discriminator has a binary classifier-based ar-\nchitecture whose losses are defined by the equation:\n∇θd\n1\nm\nmX\ni=1\nh\nlogD\n\u0010\nx(i)\n\u0011\n+ log\n\u0010\n1 − D\n\u0010\nG\n\u0010\nz(i)\n\u0011\u0011\u0011i\n(10)\nThe predicted image of this variation of discriminator after\ntraining for 38 epochs in the Ped1 dataset provides satis-\nfactory results as the abnormal features or pixels from the\nframe are not generated, but such a predicted frame cannot be\nused during anomaly detection due to high noise on the static\nbackground as well as the unstructured figures of the normal\nscenarios, hence there is no metric that can generalize the\nresults accounting to the high noise in the predictions, also\nfurther training the model doesn’t improve the predictions\nbut tries to overfit the data by copying the features of the\ncorresponding frame. The result of the 38th epoch is shown\nin Fig 7.\nD. PATCHGAN LOSS AND PATCH DISCRIMINATOR\nAccounting for the limitations of the DCGAN loss, the dis-\ncriminator is replaced by the PatchGAN discriminator which\ntries to classify each N × N in an image as real or fake. For\nexample, if the input frame shape is 224 × 224 × 3 and the\nlinear embedding dimension k = 4then the output shape of\nthe discriminator is of shape 28 × 28 as shown in Fig. 6. In\naddition to the conditional GAN loss, the L1 loss is also used\nto retain the sharpness of the image as shown:\nLL1 = Ex,y,z [∥y − G(x, z)∥1] (11)\nThe overall loss is the combined effect of both the losses\nfrom equations 9 and 11 as:\nG∗ = arg min\nG\nmax\nD\nLcGAN (G, D) +λLL1(G) (12)\nFig 6 shows that the full convolution discriminator in [30]\nis replaced by the 2D swin transformer blocks and patch\nmerging layer for feature extraction and down-sampling\nwhich takes 224×224×3 frame as input and outputs a28×28\npatch as a patch for further prediction. The result of the 45th\nepoch is shown in Figure 8, which shows that the predicted\nframe has better sharpness and reconstruction of the static\npixels and a blurry prediction for the anomalous pixels.\nE. INFERENCE\nThe output of the trained SwinAnomaly model is further\nprocessed for anomaly detection, where the output gener-\nated from the Generator of the GAN is considered for a\ntemporal sequence of 4 frames per input, and the blurred\npixels from the predicted frame are taken as input which is\npassed through the inference pipeline as shown in Fig. 10. For\ninference, different anomaly detection strategies are used, but\nthe common image difference metric used is the PSNR which\nis defined as follows:\nPSNR (f, ˆf) = 10 log10\n[max ˆf ]2\n1\nN\n\r\r\rf − ˆf\n\r\r\r\n2\n2\n(13)\n1) Sliding Window based PSNR\nThe initial metric is a regularity score that relies on the sliding\nwindow-based PSNR. This score calculates the mean squared\nerror between corresponding pixels of a predicted frame ˆf\nand its ground truth f, where the sliding window determines\nthe patches. Specifically, thep patches with the highest mean\nsquared error are identified as MSE p1 , MSEp2 , ...MSEpp,\nand the PSNR of ˆf and f is then determined using the\nfollowing formula:\nPSNR SW (f, ˆf) = 10 log10\n[max ˆf ]2\n1\np\nPp\nq=1 MSE pq\n(14)\nafter the sliding window PSNR, the regularity score of the\nfth frame in a video is calculated as follows:\nR(i) =PSNR SW (fi, ˆfi) − min(PSNR SW )\nmax(PSNR SW ) − min(PSNR SW ) (15)\nThe results computed on the metric mentioned in eq 15 indi-\ncate the presence of an anomaly in a long sequence of video\nframes and do not require a threshold to be set beforehand\nas an upper limit for the PSNR difference score for checking\nthe presence of an anomaly. One significant drawback of this\nmethod is its reliance on a lengthy sequence of frames, and\nits inability to consider the anomaly region or type within\nthe frame sequence. Furthermore, this approach cannot be\nused for real-time threat detection since the regularity score\nis calculated based on an accumulated sliding PSNR metric,\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 6. PatchGAN discriminator architecture\nFIGURE 7. Real and generated images using Binary discriminator\nFIGURE 8. Real and generated images using Patch discriminator\nwhich is determined by the minimum and maximum values\nof the stored data. Therefore, the method is not suitable for\napplications that require real-time anomaly detection, such\nas security systems or surveillance. However, this approach\nmay be useful for offline analysis of video footage, where the\nsequence of frames can be analyzed in batches. Additionally,\nit is worth noting that the accuracy of the regularity score\nheavily depends on the quality of the ground truth frames\nused in the calculation. Thus, in cases where the ground truth\nframes are noisy or inaccurate, the regularity score may not\nbe reliable.\n2) Anomaly Tracking using patch-wise MSE and SORT\nThe SORT-based anomaly detector proposed in section III-D\nis used during inference, and the patch-wise Mean Squared\nError (MSE) is calculated for all the independent patches\nof size 7 × 7 between the original feature frame, and its\ncorresponding patch in the predicted frame. The threshold\nvalues are taken as follows:\n• Max Patches = 12\n• Patch Overlap Count = 6\n• Intersection Threshold between bounding box and patch\n= 0.7\nThe inference is done with a frame rate of 10fps for each test\nfolder in all four datasets. The regularity score is a boolean\nvalue that indicates the presence of an anomaly in a frame of\nthe sequence and the AUC score is calculated by passing the\ntrue labels and predicted anomaly labels in the AUC function\nto get results, the visualization is also saved as a video, that\ndetects and tracks the anomaly bounding box.\nV. RESULT\nThis section presents the results of the proposed approach\non the Ped1, Ped2, Avenue, and ShanghaiTech datasets. The\nproposed approach is evaluated using different threshold\nvalues and compared with other video anomaly detection\nmethods.\nThe Area Under Curve (AUC) score was used for evaluat-\ning the model. AUC measures the overall performance of the\nmodel by calculating the area under the Receiver Operating\nCharacteristic (ROC) curve. Generally, a higher AUC score\nindicates better performance of a model.\nTABLE 1. Comparing Sliding PSNR regularity score (AUC) for different patch\nsizes\nPatch Size Average Sliding PSNR AUC\n7 x 7 83.37\n14 x 14 80.75\n28 x 28 73.02\n56 x 56 62.84\nTable 1 shows the corresponding value of the average slid-\ning PSNR AUC score of the four different datasets calculated\nusing eq. 15 for different window patch sizes. Through the\ntable, we can observe that the patch size of 7 × 7 gives the\nbest PSNR score for the frame size of224×224 and is hence\nselected for further evaluation.\nAfter establishing a consistent patch size of 7 × 7 across\nall evaluations (as illustrated in Table 1), we conducted\nadditional tests to determine the optimal values of custom\nparameters for the total number of high MSE patches to\nconsider in the reconstructed frame(Max Patches) and the\nnumber of such patches that overlap with bounding boxes\nof the objects detected in the frame by the object detection\nmodel(Patch Overlap Count thresholds). The former repre-\nsents the total number of individual patches within a given\nframe that is considered anomalous, while the latter specifies\nthe minimum number of these patches that must appear\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 9. AUC curves after 70 epochs\nFIGURE 10. Anomaly detection pipeline\nTABLE 2. Comparing the value of tunable hyperparameter for a patch size of\n7 × 7, Patch Overlap Count\nMax Patches Patch OC Ped1 Ped2 Avenue Shanghai\n12 6 87.3 98.2 83.02 76.3\n4 85.9 97.73 84.8 75.81\n2 79.1 92.2 80.09 71.85\n8 6 84.26 93.34 78.89 69.07\n4 87.23 94.88 78.02 69.38\n2 86.11 89.86 75.98 65.6\n4 6 - - - -\n4 72.68 71.97 65.24 56.42\n2 71.03 73 62.66 57.9\nwithin the bounding box of an object for it to be classified\nas anomalous.\nThrough an analysis of Table 2, we observed that the\nmaximum AUC score was obtained by selecting 12 patches\nand a minimum of 6 patch overlaps for each bounding box\nfor the Ped1, Ped2, and Shanghai datasets. However, for the\nAvenue dataset, the ideal AUC score was achieved with 12\npatches and a minimum of 4 patch overlaps. By averaging the\nthreshold values of (12, 6) and (12, 4), we determined that\n(12, 6) ultimately produced a superior score and, therefore,\nwas chosen as the optimal threshold for both Max Patches\nand Patch Overlap Count. After getting the value of all\nthresholds, the model training is resumed with the validation\nset being evaluated on the chosen hyperparameters from\nTable 1 and 2 up to 70 epochs on all datasets. Figure 9 shows\nthe AUC trend across all datasets. From the graphs, it can\nbe seen that there was an initial spike in the AUC score for\nthe first few epochs, then the score became constant followed\nby a downward trend with some variation in graph pattern\nin all the datasets. The highest AUC score was achieved in\nthe 36th, 50th, 45th, and 47th epoch on Ped1, Ped2, Avenue,\nand ShanghaiTech datasets respectively. A visualization of\nthe bounding box and patch overlap using SORT tracking is\nFIGURE 11. Visualization of video output showcasing anomaly detection in\nreal-time\nshown in Fig 11.\nA. COMPUTATION TIME\nThe inference results for the selected threshold configura-\ntions indicate that the proposed pipeline, which involves\napplying the SwinAnomaly generator model for future frame\nprediction followed by the SORT tracker, exhibits varying\nprocessing speeds based on the configuration. Our model is\ntrained on an RTX 2060 GPU and inferred on an i7 CPU. In\nthe CPU configuration, the pipeline achieves a rate of 1.96\nfps, while in the GPU configuration, the speed increases to\n4.32 fps. These results are calculated with no latency in input\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 12. computation time comparison of different methods\nframe data and without utilizing the parallelization capabili-\nties of a transformer architecture model. However, when we\nconstrain the result to a latency of two seconds allowing\nframes to get accumulated and then sending them together\nin batches, we observe an incredible boost in performance,\nachieving 41.66 fps on an RTX 2060 GPU and 107 fps on an\nRTX 3080 GPU. The comparison of the computation cost of\nvarious models is discussed in Fig 12.\nTABLE 3. AUC scores compared with different related works\nMethod Ped1 Ped2 Avenue Shanghai\nConv2D [11] 81 90 70.2 -\nConvLSTM [15] 77.5 88.1 77 -\nFutureFramePred [18] 83.1 95.4 85.1 72.8\nMemAE [33] - 94.1 83.3 71.2\nTransAnomaly [30] 86.7 96.4 87 -\nDual Discriminator [34] - 95.6 84.9 73.7\nMPN [35] 85.1 96.9 89.5 73.8\nMNAD+ [36] - 97.8 88.5 70.5\nRAD [40] - 97.4 86.7 73.6\nProposed Approach 87.3 98.2 84.8 76.3\nTable 3 compares the proposed approach with various\nother approaches using the same four datasets. Latest models\nsuch as Transanomaly [30], and Meta Prototype Network\n[35] were also compared. As evident from the table, our\nmethod gives better AUCs for Ped1, Ped2, and ShanghaiTech\nhowever performs poorly on the Avenue dataset as compared\nto other state-of-the-art models.\nVI. DISCUSSION\nA. WHY SWIN-TRANSFORMER WAS SELECTED AS\nTHE ENCODER\nWith the introduction of a hierarchical framework, swin\nTransformers separate the input image into more manage-\nable, non-overlapping patches. To encode images in GANs,\nthis hierarchical structure effectively captures local and\nglobal information. This can be extended further into Patch-\nGAN where dividing the image segment into patches helps\nin capturing more context. The use of shifted windows in\nSwin Transformer significantly reduces the computational\ncomplexity compared to the standard self-attention used in\ntraditional transformers. Swin Transformers’ patch-based ap-\nproach makes it more scalable, as it can encode the images in\na parallel and memory-efficient manner. These properties of\nthe Swin Transformer make it the best choice for the task at\nhand.\nB. REAL-TIME ANOMALY DETECTION\nThe methods discussed in the related works II-D proposed\nan inference technique based on the ratio of the maximum\nand minimum Peak Signal-to-Noise Ratio (PSNR). This ap-\nproach enables the processing of long sequences of frames\nin batches, offering a tradeoff between latency and achiev-\ning better frame rates. In contrast, our method employs a\nreal-time processing approach, handling one input frame\nsequence at a time. However, we have optimized the infer-\nence pipeline to ensure efficient and streamlined processing,\nachieving a frame rate of 4.32 fps. Moreover, by introducing\na small latency of 2 seconds, we can leverage the batch\ninference of transformers and obtain frame rates up to 107\nfps, as discussed in Section V. Overall, while the related\nworks focus on batch processing with a latency-fps tradeoff,\nour method prioritizes real-time processing.\nC. LOW AUC SCORE ON AVENUE AND SHANGHAI\nDATASETS\nIn our inference, we applied a resizing operation to both\nthe Avenue and Shanghai datasets, setting the frame size\nto (224 × 224) pixels. This step was necessary as it aligns\nwith the image input requirements of our model. However,\nwe observed that this resizing process led to a loss of fine-\ngrained details in the frames due to dilation. Given that the\nimages were in color and captured at a high frame rate,\na strong correlation between consecutive frames emerged.\nConsequently, during training, the model tended to prioritize\ncopying frame features rather than focusing on the accurate\nreconstruction of future frames. Moreover, we encountered\nlimitations with the object detection model when restricting\nour results to an average selection of patch overlap count.\nThis restriction hindered the detection of shadowed individu-\nals in the background and small segment anomalies, leading\nto occasional failure in anomaly detection. To enhance our\nanomaly detection system, we can address the limitations by\nexploring alternative object detection models and leveraging\na combination of diverse architectures for improved accuracy.\nTo reduce frame correlation, we will carefully select non-\nsimilar frames during training, promoting feature learning\nover copying. Additionally, we plan to upscale image size\nfrom swin-tiny to swin-large to capture finer details and\nachieve better detection results.\nVII. CONCLUSION\nIn this paper, we have introduced SwinAnomaly, an anomaly\ndetection model based on Swin Transformers. SwinAnomaly\nutilizes the parallelization feature of transformers and track-\ning capabilities of the SORT algorithm for real-time anomaly\ndetection and tracking. To improve frame reconstruction,\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nwe modified the skip connections and bottleneck layer to\nextend the window attention of 3D and connect it to 2D Swin\nblocks in the autoencoder. Real-time anomaly analysis and\nrecognition are achieved by non-overlapping patch intersec-\ntion with the bounding boxes of the tracking boundaries of\nKalman filters using a fast object detection model. Our exper-\niments on four baseline datasets demonstrate that our model\noutperforms state-of-the-art prediction and reconstruction-\nbased anomaly detection methods and provides flexibility\nto filter the size and persistence of anomalies using tunable\nparameters. However, a major drawback of our approach is\nthe static settings of parameters at inference time, which\nare unresponsive to outlier anomalies. Another limitation\nof the proposed approach is the high sensitivity of Swin\nTransformers towards different lighting conditions, which\ncan affect the performance of the model if it is not trained on\ndifferent lighting conditions. Future work includes extending\nthis approach to address its limitations by exploring other ob-\nject detection models or tracking techniques and developing\na framework to track anomalies in multiple CCTV cameras.\nACKNOWLEDGEMENT\nThe authors would like to thank King Khalid University,\nSaudi Arabia, for providing administrative and technical\nsupport and also would like to thank the editors and the\nreviewers for their helpful comments and suggestions on the\nmanuscript.\nREFERENCES\n[1] Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani et al. \"An\nimage is worth 16x16 words: Transformers for image recognition at scale.\"\narXiv preprint arXiv:2010.11929 (2020).\n[2] Arnab, Anurag, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario\nLuˇci´c, and Cordelia Schmid. \"Vivit: A video vision transformer.\" In Pro-\nceedings of the IEEE/CVF international conference on computer vision,\npp. 6836-6846. 2021.\n[3] Liu, Ze, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Baining Guo. \"Swin transformer: Hierarchical vision\ntransformer using shifted windows.\" In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pp. 10012-10022. 2021.\n[4] Creswell, Antonia, Tom White, Vincent Dumoulin, Kai Arulkumaran,\nBiswa Sengupta, and Anil A. Bharath. \"Generative adversarial networks:\nAn overview.\" IEEE signal processing magazine 35, no. 1 (2018): 53-65.\n[5] Lee, Kwonjoon, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, and\nCe Liu. \"Vitgan: Training gans with vision transformers.\" arXiv preprint\narXiv:2107.04589 (2021).\n[6] Radford, Alec, Luke Metz, and Soumith Chintala. \"Unsupervised rep-\nresentation learning with deep convolutional generative adversarial net-\nworks.\" arXiv preprint arXiv:1511.06434 (2015).\n[7] Li, Chuanlong, Yumeng Jiang, and Marta Cheslyar. \"Embedding image\nthrough generated intermediate medium using deep convolutional gener-\native adversarial network.\" Computers, Materials & Continua 56, no. 2\n(2018): 313-324.\n[8] Demir, Ugur, and Gozde Unal. \"Patch-based image inpainting with gener-\native adversarial networks.\" arXiv preprint arXiv:1803.07422 (2018).\n[9] Welch, Greg, and Gary Bishop. \"An introduction to the Kalman filter.\"\n(1995): 2.\n[10] Bewley, Alex, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft.\n\"Simple online and realtime tracking.\" In 2016 IEEE international confer-\nence on image processing (ICIP), pp. 3464-3468. IEEE, 2016.\n[11] Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K. Roy-\nChowdhury, and Larry S. Davis. Learning temporal regularity in video\nsequences. In 2016 IEEE Conference on Computer Vision and Pattern\nRecognition, pages 733–742, 2016\n[12] Y . Fan, G. Wen, D. Li, S. Qiu, M. D. Levine, and F. Xiao, “Video anomaly\ndetection and localization via Gaussian mixture fully convolutional varia-\ntional autoencoder,” Comput. Vis. Image Understand., vol. 195, Jun. 2020,\nArt. no. 102920.\n[13] N. Li and F. Chang, “Video anomaly detection and localization via\nmultivariate Gaussian fully convolution adversarial autoencoder,” Neuro-\ncomputing, vol. 369, pp. 92–105, Dec. 2019.\n[14] Y . S. Chong and Y . H. Tay, “Abnormal event detection in videos using\nspatiotemporal autoencoder,” in Proc. Int. Symp. Neural Netw. Cham,\nSwitzerland: Springer, 2017, pp. 189–196.\n[15] W. Luo, W. Liu, and S. Gao, “Remembering history with convolutional\nLSTM for anomaly detection,” in Proc. IEEE Int. Conf. Multimedia Expo\n(ICME), Jul. 2017, pp. 439–444.\n[16] T. N. Nguyen and J. Meunier, “Anomaly detection in video sequence\nwith appearance-motion correspondence,” in Proc. IEEE/CVF Int. Conf.\nComput. Vis. (ICCV), Oct. 2019, pp. 1273–1283\n[17] Chang Y , Tu Z, Xie W, Yuan J (2020) Clustering driven deep autoencoder\nfor video anomaly detection. In: European Conference on Computer\nVision, Springer, pp 329–345\n[18] Liu W, Luo W, Lian D, Gao S (2018) Future frame prediction for anomaly\ndetection–a new baseline. In: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp 6536–6545\n[19] J. R. Medel and A. Savakis, “Anomaly detection in video using\npredictive convolutional long short-term memory networks,” 2016,\narXiv:1612.00390. [Online]. Available: http://arxiv.org/abs/1612.00390\n[20] Guo, Yifan, Weixian Liao, Qianlong Wang, Lixing Yu, Tianxi Ji, and\nPan Li. \"Multidimensional time series anomaly detection: A gru-based\ngaussian mixture variational autoencoder approach.\" In Asian Conference\non Machine Learning, pp. 97-112. PMLR, 2018.\n[21] M. Ye, X. Peng, W. Gan, W. Wu, and Y . Qiao, “AnoPCN: Video anomaly\ndetection via deep predictive coding network,” in Proc. 27th ACM Int.\nConf. Multimedia, Oct. 2019, pp. 1805–1813.\n[22] Li Y , Cai Y , Liu J, Lang S, Zhang X (2019) Spatio-temporal unity\nnetworking for video anomaly detection. IEEE Access 7:172425–172432\n[23] Tang, Yao, Lin Zhao, Shanshan Zhang, Chen Gong, Guangyu Li, and Jian\nYang. \"Integrating prediction and reconstruction for anomaly detection.\"\nPattern Recognition Letters 129 (2020): 123-130.\n[24] Mariana-Iuliana Georgescu, Antonio Barb ˘ al˘ au, Radu Tu- ˘ dor\nIonescu, Fahad Shahbaz Khan, Marius Popescu, and Mubarak Shah.\nAnomaly detection in video via selfsupervised and multi-task learning.\n2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 12737–12747, 2021.\n[25] Zhian Liu, Yongwei Nie, Chengjiang Long, Qing Zhang, and Guiqing\nLi. A hybrid video anomaly detection framework via memory-augmented\nflow reconstruction and flow-guided frame prediction. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages\n13588–13597, 2021.\n[26] Guodong Wang, Yunhong Wang, Jie Qin, Dongming Zhang, Xiuguo Bao,\nand Di Huang. Video anomaly detection by solving decoupled spatio-\ntemporal jigsaw puzzles. In European Conference on Computer Vision\n(ECCV), 2022\n[27] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. \"Attention\nis all you need.\" Advances in neural information processing systems 30\n(2017).\n[28] Deshpande, Kapil, Narinder Singh Punn, Sanjay Kumar Sonbhadra, and\nSonali Agarwal. \"Anomaly detection in surveillance videos using trans-\nformer based attention model.\" arXiv preprint arXiv:2206.01524 (2022).\n[29] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille,\nand Y . Zhou, “TransUNet: Transformers make strong encoders for med-\nical image segmentation,” 2021, arXiv:2102.04306. [Online]. Available:\nhttp://arxiv.org/abs/2102.04306\n[30] H. Yuan, Z. Cai, H. Zhou, Y . Wang and X. Chen, \"TransAnomaly: Video\nAnomaly Detection Using Video Vision Transformer,\" in IEEE Access,\nvol. 9, pp. 123977-123986, 2021, doi: 10.1109/ACCESS.2021.3109102.\n[31] Cao, Hu, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang,\nQi Tian, and Manning Wang. \"Swin-unet: Unet-like pure transformer for\nmedical image segmentation.\" In Computer Vision–ECCV 2022 Work-\nshops: Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part III, pp.\n205-218. Cham: Springer Nature Switzerland, 2023.\n[32] Liu, Ze, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Han Hu. \"Video swin transformer.\" In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 3202-3211.\n2022.\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n[33] Gong, Dong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda\nMansour, Svetha Venkatesh, and Anton van den Hengel. \"Memorizing\nnormality to detect anomaly: Memory-augmented deep autoencoder for\nunsupervised anomaly detection.\" In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pp. 1705-1714. 2019.\n[34] Dong, Fei, Yu Zhang, and Xiushan Nie. \"Dual discriminator generative\nadversarial network for video anomaly detection.\" IEEE Access 8 (2020):\n88170-88176.\n[35] Lv, Hui, Chen Chen, Zhen Cui, Chunyan Xu, Yong Li, and Jian Yang.\n\"Learning normal dynamics in videos with meta prototype network.\" In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 15425-15434. 2021.\n[36] Park, Hyunjong, Jongyoun Noh, and Bumsub Ham. \"Learning memory-\nguided normality for anomaly detection.\" In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 14372-14381.\n2020.\n[37] Yao, Junyuan, and Shuanggen Jin. \"Multi-Category Segmentation of\nSentinel-2 Images Based on the Swin UNet Method.\" Remote Sensing 14,\nno. 14 (2022): 3382.\n[38] Liu, Wenrui, Hong Chang, Bingpeng Ma, Shiguang Shan, and\nXilin Chen. \"Diversity-Measurable Anomaly Detection.\" arXiv preprint\narXiv:2303.05047 (2023).\n[39] Reiss, Tal, and Yedid Hoshen. \"Attribute-based Representations for\nAccurate and Interpretable Video Anomaly Detection.\" arXiv preprint\narXiv:2212.00789 (2022).\n[40] Le, VT., Kim, YG. Attention-based residual autoencoder for\nvideo anomaly detection. Appl Intell 53, 3240–3254 (2023).\nhttps://doi.org/10.1007/s10489-022-03613-1\nARPIT BAJGOTI was born in Delhi, India, in\n2002. He is currently pursuing a B.Tech Degree\nin Computer Science and Engineering from Ma-\nharaja Surajmal Institute of Technology. He has\npublished a research paper on acoustic modeling\nand ASR systems and has professional experience\nin Audio Processing, and Computer Vision, specif-\nically in Large Language Models.\nRISHIK GUPTA was born in Delhi, India, in\n2002. He is currently pursuing a B.Tech Degree\nin Computer Science and Engineering from Ma-\nharaja Surajmal Institute of Technology and a\ndiploma in Data Science from the Indian Insti-\ntute of Technology, Madras. He has published\npapers in Audio Signal processing and has a keen\nacademic interest. His research interests include\nSpeech Detection, Computer Vision, and Natural\nLanguage Processing.\nDR. PRASANALAKSHMI BALAJI (Member,\nIEEE) is an academician and research fellow with\nover 16 years experience. She has completed\ntwo master’s degrees in computer science from\nBharathidasan University, and one more masters\nin computer engineering from Anna University,\nIndia, in 2008 and the Ph.D. degree in Com-\nputer Science from Research Development Cen-\ntre, Bharathiar University, India, in 2014. From\n2019 to 2021, she was a research fellow with the\nCentre for Artificial Intelligence, KSA. Since 2017, she is working with the\nComputer Science Department, King Khalid University, Abha, KSA. She\nholds three executive education courses on Artificial Intelligence from IIIT\nHyderabad, Artificial Intelligence Engineer Master’s program from IBM and\nData Analytics with Python from IBM Watson. She has filed seven patents\nwith India and International scope. She has presented/published 40 research\npapers at both National and International level. She has various appraisals\nand achievements to her credit that includes Young Scientist Award, best\nresearcher award and won the first prize for research and innovation award\nfrom King Khalid University, KSA, in 2022. Her research interests include\nCryptography, Machine Learning, Deep Learning, Medical Imaging and\nNeural Network.\nDR. RINKY DWIVEDI completed her B.Tech\nin Computer Science and Engineering from Guru\nGobind Singh Indraprastha University, Delhi in\n2004 and M.E. in Computer Technology and\nApplication from Delhi College of Engineering,\nDelhi in 2008. She received her Doctorate in\n2016 from Delhi Technological University, New\nDelhi. Dr. Rinky has over 19 years of experience\nin Academics, currently working as an Associate\nProfessor and Head of the Department in Com-\nputer Science Engineering Department of Maharaja Surajmal Institute of\nTechnology, New Delhi, INDIA. She has published more than 20 research\npapers in reputed Journals and conference proceedings and has also authored\nbooks.\nMS. MEENA SIWACH is pursuing her Ph.D.\nfrom Guru Gobind Singh Indraprastha University,\nDelhi. She has over 15 years of experience in\nacademics and currently working as an Assistant\nProfessor at Maharaja Surajmal Institute of Tech-\nnology, Delhi. She is highly interested in partici-\npating in the development of new interdisciplinary\nprograms of study and is highly enthusiastic about\nresearch and innovative ideas. She has published\nover 15 Research papers in National/International\nJournals and Conferences. Her key areas of research are Data Mining,\nMachine Learning, and Deep Learning.\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nDR. DEEPAK GUPTA is an accomplished aca-\ndemic with a diverse educational background. He\nholds a B.Tech. degree from Guru Gobind Singh\nIndraprastha University, an M.E. degree from\nDelhi Technological University, and a Ph.D. from\nDr. APJ Abdul Kalam Technical University. He\nhas completed a Post-Doc at the National Institute\nof Telecommunications in Brazil. Dr. Gupta has\nauthored numerous journal articles and books, se-\ncured patents, and received the 2021 IEEE System\nCouncil Best Paper Award. He is involved in organizing conferences, serves\nas an associate editor for respected journals, and has been recognized as a\ntop researcher in healthcare applications. Additionally, he promotes startups\nand holds series editor roles for prominent publishers. Dr. Gupta has secured\nresearch grants from international funding agencies and is a Co-PI in an\nIndo-Russian joint project.\n14 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321801\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8141874074935913
    },
    {
      "name": "Anomaly detection",
      "score": 0.7619919776916504
    },
    {
      "name": "Autoencoder",
      "score": 0.5979121923446655
    },
    {
      "name": "Artificial intelligence",
      "score": 0.557643473148346
    },
    {
      "name": "sort",
      "score": 0.5366954207420349
    },
    {
      "name": "Encoder",
      "score": 0.4824024438858032
    },
    {
      "name": "Video tracking",
      "score": 0.45852935314178467
    },
    {
      "name": "Real-time computing",
      "score": 0.42508238554000854
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4224860370159149
    },
    {
      "name": "Computer vision",
      "score": 0.39408573508262634
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3669623136520386
    },
    {
      "name": "Deep learning",
      "score": 0.15488910675048828
    },
    {
      "name": "Video processing",
      "score": 0.14802035689353943
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Information retrieval",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82952536",
      "name": "King Khalid University",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I2799452066",
      "name": "Maharaja Engineering College",
      "country": "IN"
    }
  ]
}