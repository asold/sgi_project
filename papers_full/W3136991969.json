{
  "title": "Tiny Transformers for Environmental Sound Classification at the Edge",
  "url": "https://openalex.org/W3136991969",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5042550507",
      "name": "David Elliott",
      "affiliations": [
        "Florida Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5079544571",
      "name": "Carlos E. Otero",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5062959925",
      "name": "Steven Wyatt",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5013164293",
      "name": "Evan Martino",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2038484192",
    "https://openalex.org/W2969515962",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2795138957",
    "https://openalex.org/W1979594949",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2052666245",
    "https://openalex.org/W2972247414",
    "https://openalex.org/W2593451766",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2529337537",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W2008415856",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2943493972",
    "https://openalex.org/W3027588817",
    "https://openalex.org/W3035623224",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W3044604993",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3094175596",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W3046125265",
    "https://openalex.org/W3098357269",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2766397045",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W3095348033",
    "https://openalex.org/W2751841560",
    "https://openalex.org/W3043803126",
    "https://openalex.org/W2962910554",
    "https://openalex.org/W3082878506",
    "https://openalex.org/W3021164770",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2914312680",
    "https://openalex.org/W2313745396",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W1996160964",
    "https://openalex.org/W3046727238",
    "https://openalex.org/W2768083292",
    "https://openalex.org/W2963838685",
    "https://openalex.org/W1997527178",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W2766087987",
    "https://openalex.org/W1972567154",
    "https://openalex.org/W3016195076",
    "https://openalex.org/W2909327627",
    "https://openalex.org/W2676925568",
    "https://openalex.org/W2949117887"
  ],
  "abstract": "With the growth of the Internet of Things and the rise of Big Data, data processing and machine learning applications are being moved to cheap and low size, weight, and power (SWaP) devices at the edge, often in the form of mobile phones, embedded systems, or microcontrollers. The field of Cyber-Physical Measurements and Signature Intelligence (MASINT) makes use of these devices to analyze and exploit data in ways not otherwise possible, which results in increased data quality, increased security, and decreased bandwidth. However, methods to train and deploy models at the edge are limited, and models with sufficient accuracy are often too large for the edge device. Therefore, there is a clear need for techniques to create efficient AI/ML at the edge. This work presents training techniques for audio models in the field of environmental sound classification at the edge. Specifically, we design and train Transformers to classify office sounds in audio clips. Results show that a BERT-based Transformer, trained on Mel spectrograms, can outperform a CNN using 99.85% fewer parameters. To achieve this result, we first tested several audio feature extraction techniques designed for Transformers, using ESC-50 for evaluation, along with various augmentations. Our final model outperforms the state-of-the-art MFCC-based CNN on the office sounds dataset, using just over 6,000 parameters -- small enough to run on a microcontroller.",
  "full_text": "1\nTiny Transformers for Environmental Sound\nClassiﬁcation at the Edge\nDavid Elliott, Member, IEEE, Carlos E. Otero, Senior Member, IEEE, Steven Wyatt, Member, IEEE,\nEvan Martino, Member, IEEE\nAbstract—With the growth of the Internet of Things and the\nrise of Big Data, data processing and machine learning applica-\ntions are being moved to cheap and low size, weight, and power\n(SWaP) devices at the edge, often in the form of mobile phones,\nembedded systems, or microcontrollers. The ﬁeld of Cyber-\nPhysical Measurements and Signature Intelligence (MASINT)\nmakes use of these devices to analyze and exploit data in ways\nnot otherwise possible, which results in increased data quality,\nincreased security, and decreased bandwidth. However, methods\nto train and deploy models at the edge are limited, and models\nwith sufﬁcient accuracy are often too large for the edge device.\nTherefore, there is a clear need for techniques to create efﬁcient\nAI/ML at the edge. This work presents training techniques for\naudio models in the ﬁeld of environmental sound classiﬁcation\nat the edge. Speciﬁcally, we design and train Transformers to\nclassify ofﬁce sounds in audio clips. Results show that a BERT-\nbased Transformer, trained on Mel spectrograms, can outperform\na CNN using 99.85% fewer parameters. To achieve this result, we\nﬁrst tested several audio feature extraction techniques designed\nfor Transformers, using ESC-50 for evaluation, along with\nvarious augmentations. Our ﬁnal model outperforms the state-of-\nthe-art MFCC-based CNN on the ofﬁce sounds dataset, using just\nover 6,000 parameters – small enough to run on a microcontroller.\nIndex Terms—edge; environmental sound classiﬁcation; ma-\nchine learning; audio; transformers; feature extraction; pretrain-\ning; ﬁnetuning; self-attention\nI. I NTRODUCTION\nT\nHE ﬁeld of environmental sound classiﬁcation (ESC)\nhas been actively researched for many years, with ap-\nplications in security, surveillance, manufacturing, A Vs, and\nmore [1]. In modern days, ESC has important applications to\nautonomous vehicles (A V), as they can be used to detect sirens,\naccidents, locations, in-cabin disturbances, and much more. As\nvehicle-based computational power increases, and algorithms\nimprove, it becomes vital to explore a wide number of options\nto perform a given machine learning task. For ESC, this means\nexploring transformers [2] as a possible means to perform ESC\nat the edge.\nRecent work in transformers has profoundly affected the\nﬁeld of natural language processing (NLP), seeing models such\nas BERT [3], XLNet [4], T5 [5], GPT [6]–[8], and BigBird\n[9] – to name a few – iteratively setting new state-of-the-art\nfor a variety of difﬁcult NLP tasks. In many cases, transformer\nD. Elliott, C. E. Otero, S. Wyatt, and E. Martino are with the Department\nof Computer Engineering and Sciences, Florida Institute of Technology,\nMelbourne, FL, 32901 USA e-mail: delliott2013@my.ﬁt.edu.\nThis work has been submitted to the IEEE for possible publication.\nCopyright may be transferred without notice, after which this version may\nno longer be accessible.\naccuracy exceeds the performance of humans on the same\ntasks.\nEven though most of the highly public work with transform-\ners has been done in NLP, a transformer, which fundamentally\nis simply a series of self-attention operations stacked on top\nof one another [2], is a general architecture that can be\napplied to any input. OpenAI, an AI research and development\ncompany focused on ensuring artiﬁcial general intelligence\nbeneﬁts humanity 1, made this clear in several of their recent\nworks. In ImageGPT [10], Chen et al. showed how the GPT\narchitecture, which is transformer-based, can be trained in\nan autoregressive fashion on a wide variety of images, in\norder to generate realistic image completions and samples.\nNotably, images were restricted to 64x64 pixels, as a greater\namount of pixels requires substantially more compute than was\nfeasible. In Jukebox [11], a transformer is used along with\nvector-quantized variational autoencoders (VQ-V AEs) [12], in\norder to generate realistic audio. Also notable is the fact that\nDhariwal et al. trained the transformer on a highly compressed\nrepresentation of the audio waveform generated by VQ-V AEs,\nrather than the raw waveform, and that the outputs from\nthe transformer are not used directly, but are passed through\nan upsampler ﬁrst. Even so, the total cost of training the\nfull Jukebox system is in excess of 20,000 V100-days – an\nenormous cost [11]. More recently, in a paper under review\nat ICLR 2021, it has been found that, given enough data\n(hundreds of millions of examples), transformers can exceed\neven the best CNNs in accuracy [13]. We take this, in addition\nto the recent trend in larger datasets and more compute, to\nmean that any work we perform here with small datasets and\nmodest transformers can easily be scaled up at a later date.\nThe ﬁeld of audio speech recognition (ASR) has picked\nup transformers with vigor. Indeed, it is understandable, as\napplying transformers is straightforward for many approaches\nto ASR. Often based on encoder-decoders [14], the most obvi-\nous use of a transformer is in the decoder of an ASR system,\nwhich usually has text as input and output, thus making the\napplication of any state-of-the-art language models, such as\nBERT or its variants, available to it with little adaptation\nneeded. Some work [15] has also made use of a transformer\nin the encoder as well, thus making the ASR system an end-\nto-end transformer model. Recent work has seen transformers\nimprove the state-of-the-art for ASR by reducing word error\nrate by 10% in clean speech, and 5% in more challenging\nspeech [16].\n1https://openai.com/\narXiv:2103.12157v1  [cs.SD]  22 Mar 2021\n2\nESC, in some ways, is much simpler than ASR, as it is\nnot concerned about both text and audio, nor does it need\nto perform ﬁne-grained classiﬁcation of words or sounds per\nvariable segment of time. However, in other ways, there are\nmore challenges with ESC than ASR. The ﬁrst major challenge\nthat ESC presents is one of data availability; there is very\nlittle data for ESC tasks available, and even the largest and\nmost accurate (DCASE 23 ) is only tens of thousands of audio\nﬁles [17]. In addition, there is no agreed-upon standard for\nwhat sounds make up ESC. Different ESC datasets often have\noverlap, and Google’s AudioSet [18] provides the largest set\nof audio labels to date, but many of AudioSet’s labels have\nto do with music or speech, and are not necessarily “environ-\nmental”. Additionally, ESC datasets are highly heterogeneous,\nhaving an extremely broad range of sounds, varying in length,\nfrequency, and intensity. This can increase the difﬁculty that\na machine learning model has in learning the sounds, as they\nmay vary widely from clip to clip.\nEnvironmental sound classiﬁcation has recently been per-\nformed mostly by convolutional neural networks (CNNs) [19]–\n[25]. These networks vary somewhat in structure, with some\nbeing fully convolutional and able to take varying-length input\n[26], others making use of modiﬁed “attention” layers to\nboost performance [27], and others using deep CNNs [24],\n[25]. Performance on ESC is typically measured using ESC-\n50 [28], ESC-10 [28], Urban-8k [29], or DCASE 2016-2018\n[30]. Top reported performance on ESC-50, to the best of our\nknowledge, is 88.50%, by Sharma et al. [25], using a spatial\nattention module and various augmentations. Human accuracy\nhas been measured at 81.3% [28]. We use ESC-50 in this work\nto allow comparison of our different models in the ﬁrst stage\nof our research, in Section V-A.\nUnfortunately, state-of-the-art transformers are too large to\nrun on edge devices, such as mobile phones, as they often\ncontain billions of parameters. The largest model of GPT-\n3 [31] contains 175 billion parameters. When most mobile\nphones contain several GBs of RAM, any model exceeding\none billion parameters (which, when quantized, is 1 billion\nbytes, or 1 GB) is likely to be inaccessible. When considering\nthat many microcontrollers have only kilobytes of SRAM, it\nbecomes particularly obvious that state-of-the-art transformers\nare not yet edge-ready. There are also latency and power\nconsiderations, as the sheer number of computations by such\nlarge models pose a limitation on how quickly a forward pass\ncan be computed using the available processors on the device,\nand may drain the device’s battery very quickly, or exceed\npower requirements. There are methods to perform knowledge\ndistillation on transformer models, to produce models with\nslightly reduced accuracy, but substantially smaller in size\n[32]. However, these methods require the existence of a\npretrained model alongside or from which they can learn [33],\nwhich does not exist in the ﬁeld of ESC.\nIn this work, we attempt to tackle some of these problems.\nFor simplicity, to lower costs, and to improve the iteration\ntime of our development process, we restrict our models to a\n2http://dcase.community/\n3https://www.kaggle.com/c/dcase2018-task1a-leaderboard\nmodest size for most of our analyses. This approach, though\nmotivated by limited resources, is supported by literature,\nas larger models can always be built later, after promising\navenues have been discovered [34].\nOur contributions in this work are as follows:\n1) We provide a thorough evaluation of transformer perfor-\nmance on ESC-50 using various audio feature extraction\nmethods.\n2) For the most promising feature extraction method, we\nperform a Bayesian search through the hyperparameter\nspace to ﬁnd an optimal conﬁguration.\n3) Based on the optimal model discovered through the\nBayesian search, we train transformers on the Ofﬁce\nSounds dataset, and obtain a new single-model state\nof the art. We also train a 6,000-parameter model that\nexceeds the accuracy of a much larger MFCC-based\nCNN.\n4) We test selected models’ performance on a mobile\nphone, and report results.\nII. R ELATED WORK\nThere have been many attempts to classify environmental\nsounds accurately, At ﬁrst many of the attempts used a more\nalgorithmic approach [35], focusing on hand-crafted features\nand mechanisms to process sound and produce a classiﬁcation.\nHowever, CNNs, which had been shown to perform well on\nimage recognition tasks, eventually became the state of the art\non ESC. The current reported state of the art on the ESC-50\ndataset is by Sharma et al. who used a deep CNN with multiple\nfeature channels (MFCC’s, GFCC’s, CQT’s and Chromagram)\nand data augmentations as the input to their model [25].\nThey achieved a score of 97.52% on UrbanSound8K, 95.75%\non ESC-10, and 88.50% on ESC-50. (Note that the original\npublication of Sharma et al. ’s work included a bug in their\ncode, which resulted in a much higher reported accuracy. That\nhas since been corrected, but has been cited incorrectly at\nleast once [36].) Additionally, a scoreboard has been kept in\na GitHub repository 4, but appears to be out of date.\nVery little work has been performed with transformers on\nESC tasks. Dhariwal et al. [37] used transformers in an auto-\nregressive manner to generate music (including voices) by\ntraining on raw audio. Miyazaki et al. [38] proposed using\ntransformers for sound event detection in a weakly-supervised\nsetting. They found that this approach outperformed the CNN\nbaseline on the DCASE2019 Task4 dataset, but no direct\napplication of transformers to ESC has been found.\nIn audio, there has been a wide number of feature extraction\nmethods used. Mitrovic et al. [39] organized the types of\nfeature extraction into six broad categories: temporal domain,\nfrequency domain, cepstral domain, modulation frequency\ndomain, phase space, and eigen domain. Features that have\nbeen used to date include Mel Frequency Cepstral Coefﬁcients\n(MFCCs) [40], log Mel-spectrogram [41], pitch, energy, zero-\ncrossing rate, and mean-crossing rate [42]. Tak et al. [43]\nachieved state of the art accuracy with phase encoded ﬁl-\nterbank energies. Agrawal et al. [44] determined that Teager\n4https://github.com/karolpiczak/ESC-50\n3\nMFCC\nGFCC\nCQT\nChromagram\nConcatenate\nReshape\nVQ-VAE\nFeature Vectors\nLinear\nMFCC\nMean\nCross-Entropy\nMel-SpectrogramRaw Audio\nTransformer Transformer Transformer Transformer Transformer\nLinear\nMean\nCross-Entropy\nLinear\nMean\nCross-Entropy\nLinear\nMean\nCross-Entropy\nCurve Tokenized\nAudio\nLinear\nMean\nCross-Entropy\nTransformer\nCurve \nVocabulary\nPosi�onal\nEncoding Posi�onal \nEncoding\nEmbeddingEmbedding\nLinear\nMean\nCross-Entropy\nFig. 1: We take six unique approaches to training a transformer on ESC. From left to right: raw amplitude reshaping, curve\ntokenization, MFCC feature extraction, multi-feature extraction, VQ-V AE tokenization, and Mel spectrograms. The transformer\narchitecture is common between them, but the ﬁrst layers vary. If a positional encoding is not shown for an architecture, then\nno positional encoding is used.\nEnergy Operator-based Gammatone features outperform Mel\nﬁlterbank energies. To combat noisy signals, Mogi and Kasai\n[45] proposed the use of Independent Component Analysis\nand Matching Pursuit, a method to extract features in the time\ndomain, rather than the frequency domain. With reference to\n[45], we note the assumption in our work that noise in an\nofﬁce environment will be minimal. Sharma et al. [25] obtain\nstate of the art using MFCC, GFCC, CQT, and Chromagram\nfeatures. Jukebox [11] was trained to differentiate music and\nartist styles using features extracted from three different VQ-\nV AEs, with varying vector lengths for each. A survey was\nperformed in 2014 by Chachada and Kuo [46] that enumerated\nthe features used in literature, with comparisons between each,\nbut no more recent survey has been found. We choose some of\nthe most successful of those feature extraction methods, and\nattempt some new ones designed speciﬁcally for transformers.\nIII. M ODELS\nTransformers are neural networks based on the self-attention\nmechanism, which is an operation on sequences, relating posi-\ntions within the sequence in order to compute its representation\n[2]. We emphasize that the attention mechanism operates on a\nlist of sequences, which means that the input to a transformer\nmust be 2-dimensional (excluding batches). In NLP, we want\nthe transformer to operate on sequences of words, characters,\nor something similar, which we refer to as “tokens”. Therefore,\nin order to meet the 2-dimensional input requirements of the\ntransformer, each token must be converted to a sequence. This\nis traditionally done using an embedding layer, which takes a\ntoken, represented by the integer value of the token’s position\nin a pre-computed vocabulary, and looks up its corresponding\nvector representation in a matrix. This embedding matrix is\nable to be learned. The embedding vector length is typically\nreferred to, in transformers, as the “hidden size”, or H. The\nnumber of tokens is referred to by the length of the input L,\nalso referred to as the sequence length. In this way, H deﬁnes\nthe number of dimensions that are used to represent tokens\n– where more dimensions typically mean greater learning\ncapacity – and L determines the context length, or window\nsize, of the input.\nAudio is represented at an extremely ﬁne-grained level of\ndetail (many samples per second), which poses challenges\nthat NLP does not have to face. For example, the common\nsampling rate of 44.1 kHz in a 5-second audio clip (the\nlength of an audio clip in ESC-50) results in 220,500 samples.\nCombine this with the limitations of modern-day transformers,\nwhich, with some exceptions, are limited to roughly H <2000\ntokens, depending on available hardware, and the task of\nanalyzing audio data becomes quite difﬁcult. There is hope\nthat this will change in the near future, with the creation of\nlinear-scaling models like BigBird [9] proven to have the same\nlearning capacity as BERT, and recent improvements in AI\nhardware by NVIDIA. But, for the sake of our discussion and\nanalysis, we will assume that we cannot use a transformer\nsequence length of more than 2048.\nThis results in the maximum audio window that a trans-\nformer can view – in the na ¨ıve case, where a single token is\na single amplitude – to be 2048 samples, or 0.046 seconds\n4\nFig. 2: The architecture on which our smallest Transformer\nmodel is based. Visualization based on the diagram by Vaswani\net al. [2].\n(46 milliseconds). Since sounds in the ESC-50 dataset often\nlast much longer than 46 milliseconds, we must therefore\nabandon the na ¨ıve approach initially. The thought exists that\nit is possible to downsample the audio to make the 2048\nsequence length be able to view a longer length of audio, but\nin practice this results in substantial information loss below\n16 kHz, and reduces model accuracy. We would like our\nwork to be constrained solely by hardware and algorithmic\nlimitations, which have a strong likelihood of improving in the\nnear future, rather than constrained by the information content\nin downsampled audio clips. Therefore, we assume a sampling\nrate above the Nyquist rate of 40 kHz for human-detectable\naudio, and, speciﬁcally, use the conventional value of 44.1 kHz\nin all of our analyses.\nAll models in this work are based on BERT [3] or Al-\nBERT [47]. The Transformer base structure, whether BERT\nor AlBERT, does not change in this work. The only alteration\nperformed is to remove positional encodings for some models,\nwhich is noted in Figure 1 outside of the Transformer base.\nWe note that our base structure in our ESC-50 experiments\ndoes not make use of an embedding layer for input tokens, as\nis customary in language models, and any tokenizations and\nembeddings that do occur are explicitly called out in Figure\n1.\nWe make a change to the transformer design in our second\nseries of experiments on the Ofﬁce Sounds dataset (Figure 2),\nwhich allowed the size of the input to be decoupled from the\nsize of the model. In the six models shown in Figure 1, the\ninput must be either reshaped or the features must be extracted\nin the shape required to create a transformer of the desired size.\nFor example, using 128 Mel bands when calculating MFCCs\nresulted in a transformer that had a maximum hidden size\nof 128. We remove this dependency in our Ofﬁce Sounds\nexperiments by adding a mapping layer, as shown in Figure\n2. The mapping layer is simply a linear layer that takes input\nof any size and maps it to the size of the transformer. It\nalso provides representational advantages, as this layer is able\nto be learned, similar to the embedding layer in traditional\ntransformers.\nAdditionally, in our ESC-50 experiments, we normalize all\ninputs to a number between 0 and 1 as a preprocessing step,\nwhere input is not tokenized. We remove this normalization\nstep in our Ofﬁce Sounds experiments, in favor of a batch\nnormalization layer [48], which may also have provided repre-\nsentational advantages to the Transformer by being learnable.\nIV. A PPROACH\nWe divide our approach below into sections on data, feature\nextraction, data augmentations, models, and model conversion.\nThese methods work together to produce the results in Section\nV.\nA. Data\nWe use three datasets in this work, AudioSet [18], ESC-\n50 [28], and the Ofﬁce Sounds dataset [49]. AudioSet is\na large-scale weakly labeled dataset covering 527 different\nsound types. The authors provide a balanced and unbalanced\nversion of the dataset; we use the balanced dataset, with\nsome additional balancing that we perform ourselves. Note\nthat in order to train on the audio from this dataset, we had\nto download the audio from the sources used to compile the\nbalanced dataset. This was an error-prone process, as not all\nsources from the original AudioSet are still available. More\ndetails on the datasets are available in Table I. ESC-50 is a\nstrongly labeled dataset containing 50 different sound types.\nEach sound category contains 40 sounds, making it a balanced\ndataset. The Ofﬁce Sounds dataset is both an unbalanced and\nweakly labeled dataset, owing to its origins in DCASE, but\nnearly the same number of audio ﬁles as ESC-50, with slightly\nlonger total length, and only 6 labels.\nAll audio ﬁles are converted to wave ﬁles, if they are not\nalready formatted as such. We read from each ﬁle at a sampling\nrate of 44100 Hz, in mono.\nB. Feature Extraction\nFeature extraction is a critical part of any machine learning\narchitecture, and especially so for transformers. In fact, some\n5\nTABLE I: Information on the datasets used for training.\n# of ﬁles # of hours # of audio types\nAudioSet 37948 104.52 527\nESC-50 2000 2.78 50\nOfﬁce Sounds 1608 2.80 6\nof the critical work that went into making BERT such a success\nwas the use of word pieces, rather than words or characters\n[3]. In an attempt to discover a feature extraction method that\ncan be of similar use in audio, we attempted several, some\nof which are well-known methods, others of which we have\nadapted to our particular use case. The approaches can be seen\nin Figure 1.\n1) Amplitude Reshaping: Motivated by works such as\nWaveNet [50], Jukebox [11] and, in general, the move toward\nmore “pure” data representations, we developed a method for\nthe transformer to work with raw amplitudes.\nUsing the notation in [51], we reshape audio in the fol-\nlowing way, where X is a sequence of amplitudes X =\n{x0, x1, ..., xn}, l is the sequence length, and d is the hidden\ndimension:\nX ∈Rl∗d×1 →reshape X ∈Rl×d (1)\nIn this way, the amount of audio that we are able to process\nis a combination of the sequence length of the model, and the\nsize of the hidden dimension. Under this reshaping operation,\nwith l = 512 and d = 512, we are able to process data up to\n262, 144 samples, or nearly 6 seconds.\n2) Curve Tokenization: Curve tokenization is an audio\ntokenization method that we propose, based on WordPiece\ntokenization [52] in NLP. The intuition behind this method\nis that, since audio signals typically vary smoothly, there may\nexist a relatively small number of “curves” that can describe\nshort sequences of audio signals. These curves are commonly\nrepresented in audio signals by sequences of ﬂoating point\nnumbers. In wave ﬁles, a single audio amplitude can be one of\n65,536 values, or 216, values; as such, our audio is, effectively,\nalready quantized. We term the quantization level of the audio\nthe resolution R.\nAlthough wave ﬁle audio is already quantized, it is advanta-\ngeous to quantize it further, as doing so reduces the maximum\nnumber of theoretical curves that can exist within any given\nsequence of audio. As an example, an 8-token curve with\nR = 100 has a maximum number of theoretical curves of\n1008. We performed quantizations at varying levels and found\nthat R = 40produces signals that remain highly recognizable.\nHowever, we chose R = 64 to ensure minimal information\nloss.\nOnce quantized, we processed all the audio in our dataset\nusing a curve length of L samples. We created a dictionary,\nthe key of which was every unique curve sequence that we\nencountered, and the value of which was the number of times\nthat curve had been seen. At L = 8, sliding the the L-length\nwindow across each audio signal with a stride of 1, on ESC-\n50, this produced a dictionary of 3.87 ∗107 keys. We took the\ntop 50,000 sequences as our vocabulary, which covers 76.49%\nof the observed curves. At inference time, we used a stride of\nL = 8, which resulted in a overall sequence length decrease of\nL, also. We ﬁnd that when curve-tokenizing our audio signals\nin this way, 76.39% of the curves are found in the vocabulary,\nwith the remaining 23.61% represented by the equivalent of\nthe <UNK> token in NLP.\nWe also created a relative vocabulary by shifting the quan-\ntized values, such that the minimum value in any 8-token\nspan was set to zero, and all the other values maintained their\nrelative position to the minimum, according to the Equation 2,\nwhere X = {x0, x1, ..., xn}is a span of audio with individual\nquantized values xi.\nX =\nn∑\ni=0\nxi −min(X) (2)\nUsing the top 50,000 spans from the relative vocabulary, we\nﬁnd that the it covers 85.44% of the number of unique spans\nin the dataset. When using the relative vocabulary to tokenize\naudio from the dataset, we ﬁnd that an average 85.43% of\nthe curves in each audio clip are represented, with 14.57%\nrepresented by the <UNK> token.\n3) VQ-VAE: This method was motivated by Jukebox [11],\nwhich made use of vector-quantized variational autoencoders\nto produce compressed “codes” to represent audio. The VQ-\nV AEs that we trained used the code that the authors provided,\nand details on the speciﬁcs of training can be found in their\npaper. We used their default VQ-V AE hyperparameters, which\ntrained three VQ-V AEs, each with a codebook size of 2048,\na total model size of 1 billion parameters, and downsampling\nrates of 128, 32, and 8. We trained the VQ-V AEs on AudioSet\nfor 500,000 steps. In our experiments, we use the VQ-V AE\nwith a downsampling rate of 32x.\n4) MFCC: Mel-frequency cepstral coefﬁcients (MFCCs)\nhave a long history of use in audio classiﬁcation problems [1],\n[25], [46], and so we tested their usefulness with transformers,\nas well. Unless otherwise mentioned, we used 128 mels, a hop\nlength of 512, a window length of 1024, and number of FFTs\nof 1024.\n5) MFCC, GFCC, CQT, and Chromagram: Sharma et al.\n[25] reported a new state of the art on ESC-50, using four\nfeature channels at once. They made use of MFCCs, gam-\nmatone frequency cepstral coefﬁcients (GFCCs), a constant\nQ-transform (CQT), and a chromagram. Roughly speaking,\nthe usefulness of each feature can be broken down in the\nfollowing way: MFCCs are responsible for higher-frequency\naudio, such as speech or laughs; GFCCs are responsible\nfor lower-frequency audio, such as footsteps or drums; CQT\nis responsible for music; and chromagrams are responsible\nfor differentiating in difﬁcult cases through the use of pitch\nproﬁles. A more extended discussion of these features is\navailable in Sharma et al. ’s work [25]. We made use of the\nsame features with our transformer models, using the same\nparameters for feature extraction as Sharma et al.. In order to\nfacilitate feeding the features into the transformer model, we\nconcatenate the features, creating a combined feature vector\nof 512, which became the size of the hidden dimension.\n6) Mel spectrogram: Other works obtaining high accuracies\non ESC-50, such as the work by Salamon and Bello [20], and,\n6\nmore recently, Kumar et al.’s work with transfer learning and\nCNNs [26], made use of Mel spectrograms. Therefore, we also\nchose to include the Mel spectrogram as a feature extraction\nmethod.\nMotivated by early attempts at downsampling the spectro-\ngram, and seeing little to no decrease in accuracy on ESC-\n50, we perform downsampling on the spectrogram in order\nto reduce memory usage, which sped up experiments. The\ndownsampling was performed by taken every Nth column of\nthe spectrogram matrix, where the column was frequency data\nat a particular timestep. In our experiments with ESC-50, we\nused N = 2 and N = 3. In our experiments with the Ofﬁce\nSounds dataset, we used N = 1, or no downsampling. In\nexperiment #9 on ESC-50 (Table II), we used 128 Mel bands,\n1048 FFTs, hop length of 512, and window length of 1024.\nIn experiment #10 on ESC-50, we used 256 Mel bands, 2048\nFFTs, hop length of 512, and window length of 1024.\nC. Augmentations\nInspired by Sharma et al. [25], we performed a number\nof augmentations to the our raw audio. We performed eleven\ndifferent augmentations:\n• Amplitude clipping: all samples are clipped at a random\namplitude, determined by a percentage range, from 0.75\nto 1.0, based on the maximum value in the audio.\n• Volume ampliﬁcation : all samples are multiplied by a\nrandom value, determined by a percentage range between\n0.5 and 1.5.\n• Echo: a random delay is selected between 2% and 40%\nof one second, and, for each value in the audio, values\nfrom the delay value number samples prior to it are added\nto it. E.g. at index 10,000 in an audio clip, with a random\ndelay number of samples of 4,410, the sample from index\n5590 is added to the sample at index 10,000.\n• Lowpass ﬁlter: a ﬁfth-order lowpass ﬁlter is passed over\nthe audio, with a cutoff determined by a random value\nbetween 0.05 and 0.20.\n• Pitch: the pitch is shifted by a random value\nfrom 0 to 4, using a function provided by librosa,\nlibrosa.effects.pitch_shift.\n• Partial erase: a random amount of audio, from 0 to 30%,\nis replaced with Gaussian noise.\n• Speed adjust : The speed of the audio is ad-\njusted randomly between a value of 0.5 and 1.5,\nwhere greater than one is faster, and less than\none is slower, using a function provided by librosa,\nlibrosa.effects.time_stretch.\n• Noise: a random amount of Gaussian noise is added to\nevery sample in the audio.\n• HPSS: harmonic percussive source separation is per-\nformed, with a random choice between returning the\nharmonic part or the percussive part of the audio.\n• Bitwise downsample : audio is downsampled by multi-\nplying each sample by a resolution value R between 40\nand 100, taking the ﬂoor of the value, and then dividing\nby the resolution. This reduces every sample in the audio\nto be represented by a maximum of R possible values.\n• Sampling rate downsample : a value k is selected be-\ntween 2 and 9, inclusive, and for every audio sample\nxi, where i = 0, k,2k, ..., the next k positions in the\naudio are overwritten with xi. The number of samples\nin the audio stays the same with this method, but the\noverall information content of the audio is decreased. This\nmethod is similar to augmentations that downsample an\nimage, while keeping the size of the image the same.\nD. Model Conversion\nTo convert the model, we used PyTorch mobile and torch-\nscript 5. We also quantized the model using PyTorch’s dynamic\nquantization, which is a part of PyTorch Mobile.\nWe did not perform static quantization due to complexity\nand time constraints. We converted the model into Open\nNeural Network Exchange (ONNX) format 6, in an attempt to\nconvert to TensorFlow, then to TensorFlow Lite. However, we\nwere unsuccessful in this attempt, due to various limitations\nin the frameworks and conversion process.\nSimilarly, we attempted to convert the model to a represen-\ntation that is supported on a Arduino Nano 33 BLE Sense.\nWe attempted to convert the ONNX version of our model to\nTensorFlow Lite, but encountered multiple issues, one related\nto missing operators. We also attempted to convert it to deepC\n7, but encountered similar issues, including missing support\nfor quantized PyTorch models. We also did not complete the\nconversion to a microcontroller-supported representation due\nto complexity and time constraints.\nV. E XPERIMENTS\nWe performed two sets of experiments, one on ESC-50\nusing the six feature extraction methods described in Section\nIV-B, and a second on our Ofﬁce Sounds dataset, using\nthe best model from the ﬁrst set of experiments, with some\nadjustments.\nWe used HuggingFace’s Transformers library 8 for our\ntransformer implementations. Note that HuggingFace’s library\nassumes that a positional embedding is desirable, and has no\noption to remove it. Therefore, we ran a modiﬁed version of\ntheir code for our experiments that did not return integer tokens\nduring feature extraction, namely, raw amplitudes, MFCCs,\nGFCCs, CQTs, Chromagrams, and Mel spectrograms. We did\nuse positional embeddings in our curve-tokenized and VQ-\nV AE experiments.\nWe used librosa v0.7.2 9 for Mel spectrogram, MFCC, CQT,\nand Chromagram feature extraction, and spafe v0.1.2 10 for\nGFCC feature extraction. We also used Python v3.7.7, PyTorch\nv1.6.0, and PyTorch Lightning v0.7.6 for machine learning.\nTo make our experiments more accessible, we designed our\nmodels to be able to run on consumer hardware. We used two\nNVIDIA RTX 2080 Ti’s to train all of our models, each with\n5https://pytorch.org/mobile/home/\n6https://onnx.ai/\n7https://github.com/ai-techsystems/deepC\n8https://github.com/huggingface/transformers\n9https://github.com/librosa/librosa\n10https://github.com/SuperKogito/spafe\n7\nTABLE II: Accuracy on ESC-50 dataset, running under various feature extraction and training schemes.\n# Input Accuracy Samples Layers Heads Sequence Len Batch Augment Type\n1 Amplitude reshaping 48.96 44100 8 8 256 16 True BERT\n2 Amplitude reshaping (Pretrained) 52.08 44100 16 16 256 16 True BERT\n3 VQ-V AE (32x) 31.77 16384 8 8 512 32 False BERT\n4 VQ-V AE (32x) 34.50 65536 8 8 2048 4 False BERT\n5 MFCC 53.13 44100 8 8 173 32 False BERT\n6 MFCC 58.33 44100 8 8 173 32 True BERT\n7 MFCC, GFCC, CQT, and Chromagram 59.38 88200 8 8 173 32 False BERT\n8 MFCC, GFCC, CQT, and Chromagram 59.90 88200 8 8 173 32 True BERT\n9 Mel spectrogram (Downsampled 3x) 60.45 220500 8 8 143 64 False AlBERT\n10 Mel spectrogram (Optimized) 67.71 220500 16 16 215 16 True AlBERT\n11 Curve Tokenization (Relative) 7.81 4096 8 8 512 16 False BERT\n12 Curve Tokenization (Relative) 8.85 4096 8 8 512 16 True BERT\n13 Curve Tokenization (Absolute) 19.79 4096 8 8 512 16 False BERT\n14 Curve Tokenization (Absolute) 13.54 4096 8 8 512 16 True BERT\n11GB of RAM, with the exception of the VQ-V AE with a\nsequence length of 2048, experiment #4, for which we used a\nNVIDIA Tesla V100 with 16GB of RAM.\nWe trained using a learning rate of 0.0001, a learning rate\nwarmup of 10000 steps, and the Adam optimizer. Our data\npipeline is implemented such that, every epoch, a random\nslice is taken from each audio ﬁle, optionally passed through\naugmentations, and then passed to the model. This has the\nadvantage of vastly simplifying the data processing imple-\nmentation, and increasing the number of ways in which a\nmodel can view a particular sound (assuming that the number\nof samples viewed by the model is less than the number\nof samples in the audio ﬁle). It does, however, have the\ndisadvantage of reading from every audio ﬁle an equal number\nof times, regardless of the length of the audio. This was not\na substantial issue for us, as AudioSet, ESC-50, and Ofﬁce\nSounds all contain roughly the same length audio ﬁles within\nthemselves.\nA. Experiments on ESC-50\nTable II describes the results of the trainings that we\nperformed with each of our model types, and we discuss the\nresults below.\n1) Amplitude Reshaping: Experiment #1 with amplitude\nreshaping tested how well a transformer could learn to predict\nunder a few unusual circumstances: (1) the inputs to the\nmodels are not constant with respect to tokens, as is usually the\ncase with learned embeddings, (2) the model is not pretrained,\nand (3) the dataset is small. The performance of this model was\nfar below comparable CNNs, but better than expected, given\nthat transformers traditionally are pretrained with massive\namounts of data, and are known to perform poorly when\ntrained on small datasets alone. We observed that the model\nbegan to overﬁt around 60 epochs.\nWe also performed a supervised pretraining on reshaped raw\namplitudes in experiment #2. This pretraining comes in the\nform of training on audio from AudioSet, described in Table\nI, which has 527 labels. We trained on AudioSet for 75 epochs,\nwith augmentations, to a maximum top-1 validation accuracy\nof 6.36%, after which it began to overﬁt. We then took that\npretrained model, and ﬁnetuned it on ESC-50, without freezing\nany layers, according to standard practice with transformers.\nIt is notable that this pretraining increased accuracy by 3%,\ncompared to the non-pretrained model. When pretrained on a\nmuch larger dataset than AudioSet, it may be the case, as in\n[13], that a model like this obtains far higher accuracy when\nﬁnetuned.\n2) VQ-VAE: We were surprised by the inneffectiveness of\nVQ-V AE codes in producing good classiﬁcations. Judging by\nJukebox [37], it seemed reasonable to believe that the VQ-V AE\nwould encode a substantial amount of knowledge in the codes,\nwhich, if they are enough to produce a good reconstruction\nof the original audio, might also be enough to produce a\nclassiﬁcation. We did not ﬁnd this to be the case, however, as\nthey vastly underperformed compared to MFCCs, raw audio,\nand others. We can think of several reasons for this: ﬁrst, the\nlack of large-scale data reduces the maximum accuracy that\ncan be obtained from any input by a transformer, and this\nmay be particularly true for VQ-V AE codes, since it could\nhave been compounded by the lack of data supplied to both\nthe VQ-V AE in learning codes through AudioSet, and the\nlack of data in learning classiﬁcations in ESC-50. Second, the\nheterogeneity of sounds in AudioSet may have signiﬁcantly\nlimited the VQ-V AE’s ability to represent sounds in the codes.\nIt has previously been shown that V AEs in general do not\nperform well on heterogeneous datasets [53]. As such, our\nVQ-V AE may not be able to perform as well on environment\nsound tasks as it did on music tasks, given the large variety\nof sounds present in ESC versus music.\nHypothesizing that the short sequence length of our ﬁrst\nexperiment (512) may have resulted in the transformer not\nbe able to have a sufﬁcient view of the code to perform a\nclassiﬁcation, we attempted a much longer sequence length,\nusing a V100 GPU with 16GB of RAM. Even with a sequence\nlength of 2048, which translates to an effective number of\nsamples of 65,536, or about 1.5 seconds, we did not observe\na substantial increase in accuracy, still falling far below other\nfeature extraction methods.\nAs with the rest of the methods in this work, the ﬁrst step to\nincreasing accuracy on ESC using VQ-V AE codes is to obtain\nmore data. Training on a much larger corpus of unlabeled\naudio is entirely possible in the ﬁrst step to creating VQ-\nV AE codes, and may improve the quality of the codes created.\nAdditionally, using techniques such as the ones presented by\n8\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\nFig. 3: Validation accuracy on AlBERT, trained using a Mel spectrogram with varying parameters, aggregated over a total of\n159 runs. The ﬁgures show results as the following parameters are varied: (a) augmentations, (b) number of samples viewed\nby the model at once, (c) number of Mel bands in the Mel spectrogram, (d) number of hidden layers in the model, or the\ndepth of the model, (e) number of attention heads, and (f) the hop length when calculating the Mel spectrogram.\nNazabal et al. [53], to alter the VQ-V AE to enable to it better\nhandle heterogeneous data, may help as well. It may also be\nof value to perform a pretraining step, either supervised or\nunsupervised, and ﬁnetune on more ESC data. However, even\nwith all such adjustments, it seems unlikely that VQ-V AE\ncodes will exceed MFCCs, Mel spectrograms or raw audio\nin predictive capability.\n3) MFCC, GFCC, CQT, and Chromagram: In experiments\n#5 and #6, we observe that augmentations make a substantial\n(5.2%) impact on accuracy. We also see that MFCC’s perform\nbetter, though only slightly so, than raw amplitudes. These\nexperiments were performed with 128 Mel bands, which\nresulted in the hidden size H of the model to be 128 as well.\nThese models began to overﬁt around 50 epochs.\nExperiments #7 and #8 showed that adding additional\nfeature extraction methods improved the accuracy of the model\nbeyond only using MFCCs, especially in the non-augmented\ncase. However, when augmented, the model did not show any\nmajor improvements, as had occurred with MFCCs. This is\ndifferent than the results by Sharma et al. [25], which had\nused augmentations to improve accuracy by more than 3%.\nHowever, we note that for our purposes – inferring at the\nedge – the cost of computing features using all four extraction\nmethods becomes prohibitive, and the model would have been\nunlikely to be of use at the edge, even it it had obtained\nhigh accuracy. We also found that extracting these features\nat training time resulted in extremely slow training, which\nhindered additional experimentation with these features.\n4) Mel Spectrogram and Hyperparameter Search: We\ntrained using a Mel spectrogram in experiments #9 and #10,\nand obtained accuracy that outperformed any other feature\nextraction methods. This is particularly advantageous at the\nedge, since computing a Mel spectrogram is a reasonably\ninexpensive operation. Of note, as well, is the fact that this\nwas obtained with a smaller sequence length than experiments\n#7 and #8, due to the 3x downsampling that we performed.\n9\nTABLE III: Transformer accuracy on Ofﬁce Sounds dataset for various models, ordered by number of paramaters. All models\nwere based on BERT, and had the feed-forward layer size set to 4H.\n# Input Accuracy Params Multiply-Adds Samples Layers Hidden Heads Sequence Len Batch Augment\n1 Mel spectrogram 81.48% 5,954 5,638 44100 1 16 2 86 64 True\n2 Mel spectrogram 93.21% 6,642 5,982 220500 1 16 2 430 64 True\n3 Mel spectrogram 95.31% 213,858 210,414 220500 4 64 4 430 64 True\n4 Mel spectrogram 93.75% 25,553,762 25,506,734 220500 8 512 8 430 16 True\n5 Mel spectrogram 89.38% 25,553,762 25,506,734 220500 8 512 8 430 16 False\nTABLE IV: CNN accuracy on Ofﬁce Sounds dataset, ordered by accuracy.\n# Input Accuracy Params Multiply-Adds Samples Batch Augment\n1 MFCC 92.97% 4,468,022 478,869,984 110250 64 True\n2 MFCC 92.19% 4,468,022 478,869,984 110250 64 False\n3 MFCC 91.41% 4,468,022 956,052,832 220500 16 False\nWe also used AlBERT, and a longer sequence length that\nother models, which may have contributed to the improved\nperformance. Judging by the performance of BERT-based\ntransformers trained on Ofﬁce Sounds, however, it seems un-\nlikely that AlBERT would result in a signiﬁcant performance\nimprovement alone. The impact of number of samples is\ndiscussed below.\nSince this was our best-performing model, we performed a\nhyperparameter search to determine the optimal parameters.\nAll training was performed using AlBERT as the base model,\nwith a downsampling rate of 2x. We performed 159 training\nruns, which are aggregated into the graphs in Figure 3.\nSome clear improvements result by changing certain pa-\nrameters. The most obvious is the number of samples that are\npassed into the Mel spectrogram, which, as it increases, also\nincreases the maximum possible validation accuracy. We chose\na peak of 220,500 samples, or 5 seconds of audio, because\nﬁles in ESC-50 audio have a maximum length of 5 seconds.\nAs can be seen, a model’s access to the full ﬁle’s worth of\ndata improves its ability to classify well.\nAnother clear result is the importance of using more than\n80 Mel bands when creating the Mel spectrogram. This result\nis particularly important, as many research works make use of\n80 Mels or less [15], [38], [41], [42], [54], [55], which likely\nreduced accuracy in those works.\n5) Curve Tokenization: As a ﬁrst attempt in literature at\ntokenizing audio based on curves, for the purpose of training\na transformer, we ﬁnd that they provide very little predictive\npower. There may be several reasons for this, the ﬁrst of which\nis the small number of samples over which the model can view\na sound. Since every 8 samples is quantized and converted\ninto a token, using a sequence length of 512, the number of\neffective samples is 4096, which is only 93 milliseconds of\naudio. This is likely a limiting factor on the predictive ability\nof the model, and a model able to handle a much longer\nsequence length would likely perform better. It is also likely\nthat quantizing it reduced the information content of the audio,\nand further reduced the predictive power.\nIn the case of absolute curves, we ﬁnd that augmentations\nsubstantially reduce accuracy. This is likely due to the fact that\nour vocabulary was created on ESC-50 without augmentations,\nso the curves that appear with augmentations result in many\nmore <UNK> tokens. We see a slight increase in relative curve\ntokenization with augmentations, but, given the incredibly low\naccuracy of the model, ﬁnd it to be of little interest.\nOverall, we consider it unlikely that curve tokenization\nwould ever beat out more well-known feature extraction tech-\nniques. It removes too much information, such as vital fre-\nquency and phase information, which other feature extraction\nmethods allow the transformer to make use of. Nonetheless, we\nconsider it an interesting experiment in possible tokenization\ntechniques for transformers on audio.\nB. Experiments on Ofﬁce Sounds\nAfter completing our experiments on ESC-50, we trained on\nthe Ofﬁce Sounds dataset [49]. We used BERT-based models\nonly, with an emphasis on model size, speciﬁcally on reducing\nthe model size while maintaining accuracy in order to perform\nmore efﬁcient processing at the edge. We were particularly\ninterested in models which were capable of being run on\nmicrocontrollers; in our case, we chose a target model size\nof 256KB or less – the available SRAM on the Arduino\nNano 33 BLE Sense – which meant that the model must be\nless than 250,000 parameters when quantized. There are, of\ncourse, methods to run larger models with less SRAM, such\nas MCUNet [56], but we left such optimizations to a future\nwork, focusing on the generic case of running a transformer\non a microcontroller without any special optimizations.\nWe made several adjustments between our ESC-50 experi-\nments and our Ofﬁce Sounds experiments, described in Section\nIII, which enabled us to experiment with vastly different model\nsizes. We began by choosing a model with parameters simi-\nlar to our best-performing models from the hyperparameter\nsearch. We chose the model seen in experiments #4 and #5 of\nTable III, based on Mel spectrogram input with a hop length\nof 512, window size of 1024, number of FFTs of 1024, and\nMel bands of 128. It had 8 layers, and a hidden size H larger\nthan we had been able to use in the ESC-50 experiments, of\n512, and 8 heads. We also removed downsampling, making\nthe sequence length much longer than before, but still able\nto ﬁt within the constraints of consumer-grade GPUs while\nmaintaining a reasonable batch size. These models obtained a\n10\nmaximum validation accuracy of 93.75%, with augmentations,\nand began to overﬁt after 200-300 epochs.\nIn order to facilitate accurate comparisons to our previous\nwork [49], we reimplemented and performed training on Ofﬁce\nSounds using MFCCs as input to a CNN, shown in Table IV.\nFollowing that work, the CNN was an exact reimplementation\nof Kumar et al.’s model in [26]. We trained the model, non-\naugmented, on ESC-50 to conﬁrm accurate implementation,\nand obtained 81.25%, which is very close to the 83.5% model\naccuracy that Kumar et al. reported for their model that had\nbeen pretrained on AudioSet. We performed training of this\nMFCC-based CNN against Ofﬁce Sounds, using a random\nslice of the each audio ﬁle in each epoch, and obtained a\nmaximum of 92.97% accuracy, using augmentations on 2.5\nseconds of audio. This corresponds to the results obtained in\n[49], even though the training scheme is slightly different.\nThe model contained 4.5 million parameters, and nearly 500\nmillion multiply-adds. The inference time of this model on a\nSamsung Galaxy S9 was an average of 57 milliseconds [49],\nnon-augmented and non-quantized, using TensorFlow Lite,\nshown in Table V. We note that augmentations had a small\npositive effect on validation accuracy, and that increasing the\nvisible audio window size from 2.5 to 5 seconds had a slightly\nnegative effect.\nIn comparing the Ofﬁce Sounds transformers to the Ofﬁce\nSounds CNNs, we ﬁnd that the transformers outperform the\nCNN, while being much smaller. A model with 95.2% less\nparameters, transformer experiment #3, outperformed the CNN\nby more than 2%. The smallest model that we trained on\n5 seconds of audio, experiment #2, 99.85% smaller than\nthe CNN, also outperformed the CNNs. This is unexpected,\nsince CNNs far outperformed transformers on the ESC-50\nexperiments. Our hypothesis is that the increased number of\nexample data for each class in Ofﬁce Sounds (200 or more\nper class, compared to 40 per class in ESC-50), assisted in\npreventing as rapid overﬁtting as was observed in ESC-50.\nThis can be tested by reducing the number of samples in Ofﬁce\nSounds and running these experiments again; we leave this for\na future work.\nWe also trained our smallest model on one second of data\n(experiment #1), and found that it substantially reduced the\naccuracy of the model. Interestingly, for some applications,\nit may be worthwhile to process only one second of audio\nwith reduced accuracy, as it reduces the cost of feature\nextraction and allows the model to be run more frequently.\nAlso, predictions over each second can be aggregated across\na longer time span via majority voting, or something similar,\nin order to potentially produce more accurate predictions.\nC. Inference at the Edge\nTable V shows feature extraction and inference times on a\nSamsung Galaxy S9. This model uses a transformer based on\na Mel spectrogram, processing 5 seconds of audio data and\nproducing a classiﬁcation using ESC-50 labels. We observe\nthat, even on a device more than two years old, inference is\nstill fast enough to be performed many times a second. We\nalso ﬁnd that quantization results in lowered latency (about\n21% with dynamic quantization), which further increases the\npotential model size. Static quantization is likely to reduce\nlatency further, as dynamic quantization does not quantize\nmodel activations.\nTABLE V: Inference times of selected models on an edge\ndevice. Models are run on a Samsung Galaxy S9 using\nPyTorch Mobile, except for the ﬁrst, which was run with Ten-\nsorFlow Lite. Results are averaged over 10 runs. Quantization\nis PyTorch dynamic quantization.\nExperiment Params Mult-Adds Latency (ms)\nMFCC CNN from [49] 4,468,022 478,869,984 57\nESC-50 #10 958,146 948,888 111\nESC-50 #10, Quant. 958,146 948,888 88\nOfﬁce Sounds, Trans. #2 6,642 5,982 7\nWe also observed a substantially decreased inference time\nfrom our smallest model, as expected, inferring 93% faster\nthan the 1-million parameter transformer. It was surprising to\nﬁnd that the much larger CNN had faster inference times than\nthe 1-million parameter transformer, however, this may be due\nto optimizations in TensorFlow Lite that are not present in\nPyTorch Mobile, or simply that CNN operations are optimized\nfurther than transformer operations on edge devices.\nVI. C ONCLUSION AND FUTURE WORK\nEfﬁcient edge processing is a challenging, but critical task\nwhich will become increasingly important in the future. To\naide in this task, we trained a 6,000-parameter transformer\non the Ofﬁce Sounds dataset that outperforms a CNN more\nthan 700x larger than it. This enables accurate and efﬁcient\nenvironmental sound classiﬁcation of ofﬁce sounds on edge\ndevices, even on inexpensive microcontrollers, resulting in\ninference times on a Samsung Galaxy S9 that are 88% faster\nthan a CNN with comparable accuracy. We ﬁnd that models\ntrained in traditional frameworks (like PyTorch) have relatively\nlittle support for conversion to models that can be run at the\nedge (like on a microcontroller), even with the development\nof ONNX.\nOur ESC-50 transformer models did not outperform CNNs,\nas they did on Ofﬁce Sounds. Understanding this, and ﬁnding\nsolutions to the problem of training transformers on small\naudio datasets, is a crucial future work. Solutions may come\nthrough large amounts of unsupervised pretraining, through\nan architectural change, or though improved supervised ESC\ndatasets. In any case, our work provides groundwork upon\nwhich these questions can be answered.\nThe small size and efﬁciency of the transformer we trained\nraises questions about the cost of retraining. It may be that,\nbecause there are so few operations ( <6,000) required in\na forward pass, that on-device retraining becomes possible,\nsimilar to what is done on Coral Edge TPUs through the\nimprinting engine [57]. This would have vast implications on\nthe future of intelligent edge analytics, and even a variety of\nuser applications.\n11\nREFERENCES\n[1] M. Cowling and R. Sitte, “Comparison of techniques for environmental\nsound recognition,”Pattern recognition letters, vol. 24, no. 15, pp. 2895–\n2907, 2003.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems , 2017, pp. 5998–6008.\n[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[4] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and\nQ. V . Le, “Xlnet: Generalized autoregressive pretraining for language\nunderstanding,” in Advances in neural information processing systems ,\n2019, pp. 5753–5763.\n[5] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of trans-\nfer learning with a uniﬁed text-to-text transformer,” arXiv preprint\narXiv:1910.10683, 2019.\n[6] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\nlanguage understanding by generative pre-training,” 2018.\n[7] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” OpenAI Blog,\nvol. 1, no. 8, p. 9, 2019.\n[8] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askellet al., “Language models\nare few-shot learners,” arXiv preprint arXiv:2005.14165 , 2020.\n[9] M. Zaheer, G. Guruganesh, A. Dubey, J. Ainslie, C. Alberti, S. Ontanon,\nP. Pham, A. Ravula, Q. Wang, L. Yang et al., “Big bird: Transformers\nfor longer sequences,” arXiv preprint arXiv:2007.14062 , 2020.\n[10] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, P. Dhariwal, D. Luan,\nand I. Sutskever, “Generative pretraining from pixels,” in Proceedings\nof the 37th International Conference on Machine Learning , 2020.\n[11] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and\nI. Sutskever, “Jukebox: A generative model for music,” arXiv preprint\narXiv:2005.00341, 2020.\n[12] A. Van Den Oord, O. Vinyals et al. , “Neural discrete representation\nlearning,” in Advances in Neural Information Processing Systems , 2017,\npp. 6306–6315.\n[13] Anonymous, “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” in Submitted to International Conference\non Learning Representations , 2021, under review. [Online]. Available:\nhttps://openreview.net/forum?id=YicbFdNTTy\n[14] N.-Q. Pham, T.-S. Nguyen, J. Niehues, M. M ¨uller, S. St ¨uker, and\nA. Waibel, “Very deep self-attention networks for end-to-end speech\nrecognition.” [Online]. Available: http://arxiv.org/abs/1904.13377\n[15] R. Zhang, H. Wu, W. Li, D. Jiang, W. Zou, and X. Li, “Transformer\nbased unsupervised pre-training for acoustic representation learning,”\narXiv:2007.14602 [cs, eess] , Jul. 2020, arXiv: 2007.14602. [Online].\nAvailable: http://arxiv.org/abs/2007.14602\n[16] Y . Shi, Y . Wang, C. Wu, C. Fuegen, F. Zhang, D. Le, C.-F. Yeh, and\nM. L. Seltzer, “Weak-Attention Suppression For Transformer Based\nSpeech Recognition,” arXiv:2005.09137 [cs, eess] , May 2020, arXiv:\n2005.09137. [Online]. Available: http://arxiv.org/abs/2005.09137\n[17] D. Stowell, D. Giannoulis, E. Benetos, M. Lagrange, and M. D.\nPlumbley, “DCASE 2016 Acoustic Scene Classiﬁcation Using\nConvolutional Neural Networks,” IEEE Transactions on Multimedia ,\nvol. 17, no. 10, pp. 1733–1746, Oct. 2015. [Online]. Available:\nhttp://ieeexplore.ieee.org/document/7100934/\n[18] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C.\nMoore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-\nlabeled dataset for audio events,” in2017 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2017,\npp. 776–780.\n[19] W. Dai, C. Dai, S. Qu, J. Li, and S. Das, “Very deep convolutional neural\nnetworks for raw waveforms,” in 2017 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) , pp. 421–425,\nISSN: 2379-190X.\n[20] J. Salamon and J. P. Bello, “Deep Convolutional Neural Networks\nand Data Augmentation for Environmental Sound Classiﬁcation,” IEEE\nSignal Processing Letters , vol. 24, no. 3, pp. 279–283, Mar. 2017.\n[Online]. Available: http://ieeexplore.ieee.org/document/7829341/\n[21] Y . Tokozume and T. Harada, “Learning environmental sounds with\nend-to-end convolutional neural network,” in 2017 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) , pp.\n2721–2725, ISSN: 2379-190X.\n[22] X. Zhang, Y . Zou, and W. Shi, “Dilated convolution neural network\nwith LeakyReLU for environmental sound classiﬁcation,” in 2017 22nd\nInternational Conference on Digital Signal Processing (DSP) , pp. 1–5,\nISSN: 2165-3577.\n[23] S. Abdoli, P. Cardinal, and A. Lameiras Koerich, “End-to-end\nenvironmental sound classiﬁcation using a 1D convolutional neural\nnetwork,” Expert Systems with Applications , vol. 136, pp. 252–263,\nDec. 2019. [Online]. Available: https://linkinghub.elsevier.com/retrieve/\npii/S0957417419304403\n[24] A. Khamparia, D. Gupta, N. G. Nguyen, A. Khanna, B. Pandey, and\nP. Tiwari, “Sound classiﬁcation using convolutional neural network and\ntensor deep stacking network,” vol. 7, pp. 7717–7727, conference Name:\nIEEE Access.\n[25] J. Sharma, O.-C. Granmo, and M. Goodwin, “Environment Sound\nClassiﬁcation using Multiple Feature Channels and Attention based\nDeep Convolutional Neural Network,” arXiv:1908.11219 [cs, eess,\nstat], Apr. 2020, arXiv: 1908.11219. [Online]. Available: http:\n//arxiv.org/abs/1908.11219\n[26] A. Kumar, M. Khadkevich, and C. Fugen, “Knowledge Transfer\nfrom Weakly Labeled Audio Using Convolutional Neural Network for\nSound Events and Scenes,” in 2018 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) . Calgary,\nAB: IEEE, Apr. 2018, pp. 326–330. [Online]. Available: https:\n//ieeexplore.ieee.org/document/8462200/\n[27] Z. Zhang, S. Xu, S. Zhang, T. Qiao, and S. Cao, “Learning\nAttentive Representations for Environmental Sound Classiﬁcation,”\nIEEE Access , vol. 7, pp. 130 327–130 339, 2019. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/8823934/\n[28] K. J. Piczak, “Esc: Dataset for environmental sound classiﬁcation,” in\nProceedings of the 23rd ACM international conference on Multimedia .\nACM, 2015, pp. 1015–1018.\n[29] J. Salamon, C. Jacoby, and J. P. Bello, “A dataset and taxonomy for\nurban sound research,” in Proceedings of the 22nd ACM international\nconference on Multimedia . ACM, 2014, pp. 1041–1044.\n[30] D. Giannoulis, E. Benetos, D. Stowell, M. Rossignol, M. Lagrange, and\nM. D. Plumbley, “Detection and classiﬁcation of acoustic scenes and\nevents: An ieee aasp challenge,” in2013 IEEE Workshop on Applications\nof Signal Processing to Audio and Acoustics , Oct 2013, pp. 1–4.\n[31] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M.\nZiegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,\nI. Sutskever, and D. Amodei, “Language Models are Few-Shot\nLearners,” arXiv:2005.14165 [cs] , May 2020, arXiv: 2005.14165.\n[Online]. Available: http://arxiv.org/abs/2005.14165\n[32] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a\ndistilled version of BERT: smaller, faster, cheaper and lighter,”\narXiv:1910.01108 [cs] , Feb. 2020, arXiv: 1910.01108. [Online].\nAvailable: http://arxiv.org/abs/1910.01108\n[33] S. Sun, Y . Cheng, Z. Gan, and J. Liu, “Patient knowledge\ndistillation for BERT model compression.” [Online]. Available:\nhttp://arxiv.org/abs/1908.09355\n[34] I. Turc, M.-W. Chang, K. Lee, and K. Toutanova, “Well-read students\nlearn better: On the importance of pre-training compact models,” arXiv\npreprint arXiv:1908.08962, 2019.\n[35] C. Couvreur, V . Fontaine, P. Gaunard, and C. G. Mubikangiey, “Au-\ntomatic classiﬁcation of environmental noise events by hidden markov\nmodels,” p. 20.\n[36] Z. Mushtaq, S.-F. Su, and Q.-V . Tran, “Spectral images based environ-\nmental sound classiﬁcation using cnn with meaningful data augmenta-\ntion,” Applied Acoustics, vol. 172, p. 107581.\n[37] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and\nI. Sutskever, “Jukebox: A generative model for music.” [Online].\nAvailable: http://arxiv.org/abs/2005.00341\n[38] K. Miyazaki, T. Komatsu, T. Hayashi, S. Watanabe, T. Toda, and\nK. Takeda, “Weakly-supervised sound event detection with self-\nattention,” in ICASSP 2020 - 2020 IEEE International Conference on\n12\nAcoustics, Speech and Signal Processing (ICASSP) , pp. 66–70, ISSN:\n2379-190X.\n[39] D. Mitrovi ´c, M. Zeppelzauer, and C. Breiteneder, “Features for content-\nbased audio retrieval,” in Advances in computers . Elsevier, 2010,\nvol. 78, pp. 71–150.\n[40] V . Boddapati, A. Petef, J. Rasmusson, and L. Lundberg, “Classifying\nenvironmental sounds using image recognition networks,” Procedia\nComputer Science, vol. 112, pp. 2048–2056, 2017. [Online]. Available:\nhttps://linkinghub.elsevier.com/retrieve/pii/S1877050917316599\n[41] K. J. Piczak, “Environmental sound classiﬁcation with convolutional\nneural networks,” in 2015 IEEE 25th International Workshop on Ma-\nchine Learning for Signal Processing (MLSP) , pp. 1–6, ISSN: 2378-\n928X.\n[42] J. Li, W. Dai, F. Metze, S. Qu, and S. Das, “A comparison of Deep\nLearning methods for environmental sound detection,” in 2017 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). New Orleans, LA: IEEE, Mar. 2017, pp. 126–130. [Online].\nAvailable: http://ieeexplore.ieee.org/document/7952131/\n[43] R. N. Tak, D. M. Agrawal, and H. A. Patil, “Novel Phase Encoded Mel\nFilterbank Energies for Environmental Sound Classiﬁcation,” in Pattern\nRecognition and Machine Intelligence , B. U. Shankar, K. Ghosh, D. P.\nMandal, S. S. Ray, D. Zhang, and S. K. Pal, Eds. Cham: Springer\nInternational Publishing, 2017, vol. 10597, pp. 317–325. [Online].\nAvailable: http://link.springer.com/10.1007/978-3-319-69900-4 40\n[44] D. M. Agrawal, H. B. Sailor, M. H. Soni, and H. A. Patil, “Novel TEO-\nbased Gammatone features for environmental sound classiﬁcation,”\nin 2017 25th European Signal Processing Conference (EUSIPCO) .\nKos, Greece: IEEE, Aug. 2017, pp. 1809–1813. [Online]. Available:\nhttp://ieeexplore.ieee.org/document/8081521/\n[45] R. Mogi and H. Kasai, “Noise-Robust environmental sound classiﬁcation\nmethod based on combination of ICA and MP features,” Artiﬁcial\nIntelligence Research , vol. 2, no. 1, p. p107, Nov. 2012. [Online].\nAvailable: http://www.sciedu.ca/journal/index.php/air/article/view/1399\n[46] S. Chachada and C.-C. J. Kuo, “Environmental sound recognition: a\nsurvey,” APSIPA Transactions on Signal and Information Processing ,\nvol. 3, p. e14, 2014. [Online]. Available: https://www.cambridge.org/\ncore/product/identiﬁer/S2048770314000122/type/journal article\n[47] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n“ALBERT: A Lite BERT for Self-supervised Learning of Language\nRepresentations,” arXiv:1909.11942 [cs], Feb. 2020, arXiv: 1909.11942.\n[Online]. Available: http://arxiv.org/abs/1909.11942\n[48] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” arXiv preprint\narXiv:1502.03167, 2015.\n[49] D. Elliott, E. Martino, C. E. Otero, A. Smith, A. M. Peter, B. Luchter-\nhand, E. Lam, and S. Leung, “Cyber-physical analytics: Environmental\nsound classiﬁcation at the edge,” in 2020 IEEE 6th World Forum on\nInternet of Things (WF-IoT) . IEEE, 2020, pp. 1–6.\n[50] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves,\nN. Kalchbrenner, A. Senior, and K. Kavukcuoglu, “Wavenet: A gener-\native model for raw audio,” arXiv preprint arXiv:1609.03499 , 2016.\n[51] M. Sperber, J. Niehues, G. Neubig, S. St ¨uker, and A. Waibel,\n“Self-Attentional Acoustic Models,” arXiv:1803.09519 [cs], Jun. 2018,\narXiv: 1803.09519. [Online]. Available: http://arxiv.org/abs/1803.09519\n[52] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,\nM. Krikun, Y . Cao, Q. Gao, K. Macherey et al. , “Google’s neural\nmachine translation system: Bridging the gap between human and\nmachine translation,” arXiv preprint arXiv:1609.08144 , 2016.\n[53] A. Nazabal, P. M. Olmos, Z. Ghahramani, and I. Valera, “Handling\nincomplete heterogeneous data using vaes,” Pattern Recognition , p.\n107501, 2020.\n[54] Y . Zhao, X. Wu, Y . Ye, J. Guo, and K. Zhang, “Musicoder: A uni-\nversal music-acoustic encoder based on transformers,” arXiv preprint\narXiv:2008.00781, 2020.\n[55] Y . Jiao, “Translate reverberated speech to anechoic ones: Speech dere-\nverberation with bert,” arXiv preprint arXiv:2007.08052 , 2020.\n[56] J. Lin, W.-M. Chen, Y . Lin, J. Cohn, C. Gan, and S. Han, “Mcunet: Tiny\ndeep learning on iot devices,” arXiv preprint arXiv:2007.10319 , 2020.\n[57] “Retrain a classiﬁcation model on-device with weight\nimprinting.” [Online]. Available: https://coral.ai/docs/edgetpu/\nretrain-classiﬁcation-ondevice/\nDavid Elliott (M’2017) is a Ph.D. candidate in\nComputer Engineering at Florida Institute of Tech-\nnology. He completed his B.S. in Computer Engi-\nneering in 2017, and his M.S. in the same in 2018.\nHe has three years of industry experience, where\nhe has made contributions in the areas of cloud\nsystems, cyber resiliency, machine learning, and the\nInternet of Things. His work has been presented to\nhigh-level government executives, and used to deﬁne\nand advance the state of the art in practice. He has\nauthored several papers, appearing in IEEE CLOUD\nand IEEE World Forum on the Internet of Things.\nCarlos E. Otero (SM’09) received a B.S. degree\nin computer science, a M.S. degree in software\nengineering, a M.S. degree in systems engineering,\nand a Ph.D. degree in computer engineering from\nFlorida Institute of Technology, Melbourne. He is\ncurrently Associate Professor and the Co-Director\nof the Center for Advanced Data Analytics and\nSystems (CADAS), Florida Institute of Technology.\nHe was an Assistant Professor with the University of\nSouth Florida and the University of Virginia at Wise.\nHe has authored over 70 papers in wireless sensor\nnetworks, Internet-of-Things, big data, and hardware/software systems. His\nresearch interests include performance analysis, evaluation, and optimization\nof computer systems, including wireless ad hoc and sensor networks. He has\nover twelve years of industry experience in satellite communications systems,\ncommand and control systems, wireless security systems, and unmanned aerial\nvehicle systems.\nSteven Wyatt (M’2020) performs research and de-\nvelopment at Northrop Grumman Corporation while\ncompleting his B.S. in Computer Engineering at\nFlorida Institute of Technology. He has extensive ex-\nperience in fuzzing, wireless systems, and machine\nlearning. His work has been the basis for corporate\nAI strategy, and he continues to improve the state\nof the art in efﬁcient edge analytics through his\nresearch.\nEvan Martino (M’2020) completed his B.S in Com-\nputer Engineering, and is pursuing his M.S in Com-\nputer Engineering at the Florida Institute of Tech-\nnology. He has two years of industry experience,\nperforming research and development in networking,\ndistributed systems, and machine learning. He has\nbeen published in the World Forum on the Internet\nof things, and his work is used to provide state-of-\nthe-art analytics in production systems today.",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6058983206748962
    },
    {
      "name": "Sound (geography)",
      "score": 0.5490560531616211
    },
    {
      "name": "Acoustics",
      "score": 0.3640548288822174
    },
    {
      "name": "Computer science",
      "score": 0.3533151149749756
    },
    {
      "name": "Environmental science",
      "score": 0.3420056104660034
    },
    {
      "name": "Engineering",
      "score": 0.3316991925239563
    },
    {
      "name": "Electrical engineering",
      "score": 0.22923901677131653
    },
    {
      "name": "Physics",
      "score": 0.11710530519485474
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "topic": "Transformer",
  "institutions": [
    {
      "id": "https://openalex.org/I106959904",
      "name": "Florida Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 12
}