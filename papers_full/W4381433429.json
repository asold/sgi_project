{
    "title": "A transformer-based approach for early prediction of soybean yield using time-series images",
    "url": "https://openalex.org/W4381433429",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2578744376",
            "name": "Luning Bi",
            "affiliations": [
                "Iowa State University"
            ]
        },
        {
            "id": "https://openalex.org/A1582529587",
            "name": "Owen Wally",
            "affiliations": [
                "Agriculture and Agri-Food Canada",
                "Harrow Research and Development Centre"
            ]
        },
        {
            "id": "https://openalex.org/A2163174838",
            "name": "Guiping Hu",
            "affiliations": [
                "Iowa State University"
            ]
        },
        {
            "id": "https://openalex.org/A4213844056",
            "name": "Albert U. Tenuta",
            "affiliations": [
                "Ministry of Agriculture, Food and Rural Affairs"
            ]
        },
        {
            "id": "https://openalex.org/A2051340560",
            "name": "Yuba R. Kandel",
            "affiliations": [
                "Iowa State University"
            ]
        },
        {
            "id": "https://openalex.org/A2284705751",
            "name": "Daren S. Mueller",
            "affiliations": [
                "Iowa State University"
            ]
        },
        {
            "id": "https://openalex.org/A2578744376",
            "name": "Luning Bi",
            "affiliations": [
                "Iowa State University"
            ]
        },
        {
            "id": "https://openalex.org/A1582529587",
            "name": "Owen Wally",
            "affiliations": [
                "Agriculture and Agri-Food Canada"
            ]
        },
        {
            "id": "https://openalex.org/A2163174838",
            "name": "Guiping Hu",
            "affiliations": [
                "Iowa State University"
            ]
        },
        {
            "id": "https://openalex.org/A4213844056",
            "name": "Albert U. Tenuta",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2051340560",
            "name": "Yuba R. Kandel",
            "affiliations": [
                "Iowa State University"
            ]
        },
        {
            "id": "https://openalex.org/A2284705751",
            "name": "Daren S. Mueller",
            "affiliations": [
                "Iowa State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2806658743",
        "https://openalex.org/W2015793650",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2789255992",
        "https://openalex.org/W2962949934",
        "https://openalex.org/W2898710507",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1940872118",
        "https://openalex.org/W2791303772",
        "https://openalex.org/W2116905012",
        "https://openalex.org/W2949642792",
        "https://openalex.org/W2954731415",
        "https://openalex.org/W2891667148",
        "https://openalex.org/W6783713981",
        "https://openalex.org/W3110648706",
        "https://openalex.org/W2953686964",
        "https://openalex.org/W2898856156",
        "https://openalex.org/W2200121095",
        "https://openalex.org/W3185030868",
        "https://openalex.org/W2132077228",
        "https://openalex.org/W3000098473",
        "https://openalex.org/W3047562107",
        "https://openalex.org/W2962835968",
        "https://openalex.org/W2979666105",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W3159863551",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W3002709689",
        "https://openalex.org/W6791469159",
        "https://openalex.org/W2573587735",
        "https://openalex.org/W2646675373",
        "https://openalex.org/W3204255739",
        "https://openalex.org/W3103444592",
        "https://openalex.org/W3090011912"
    ],
    "abstract": "Crop yield prediction which provides critical information for management decision-making is of significant importance in precision agriculture. Traditional manual inspection and calculation are often laborious and time-consuming. For yield prediction using high-resolution images, existing methods, e.g., convolutional neural network, are challenging to model long range multi-level dependencies across image regions. This paper proposes a transformer-based approach for yield prediction using early-stage images and seed information. First, each original image is segmented into plant and soil categories. Two vision transformer (ViT) modules are designed to extract features from each category. Then a transformer module is established to deal with the time-series features. Finally, the image features and seed features are combined to estimate the yield. A case study has been conducted using a dataset that was collected during the 2020 soybean-growing seasons in Canadian fields. Compared with other baseline models, the proposed method can reduce the prediction error by more than 40%. The impact of seed information on predictions is studied both between models and within a single model. The results show that the influence of seed information varies among different plots but it is particularly important for the prediction of low yields.",
    "full_text": "A transformer-based approach\nfor early prediction of soybean\nyield using time-series images\nLuning Bi1, OwenWally2, GuipingHu1*, Albert U.Tenuta3,\nYuba R.Kandel4 and Daren S.Mueller4\n1Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames,\nIA, United States,2Agriculture and Agri-Food Canada, Harrow Research and Development Centre,\nHarrow, ON, Canada,3Ontario Ministry of Agriculture, Food and Rural Affairs, Ridgetown, ON, Canada,\n4Department of Plant Pathology and Microbiology, Iowa State University, Ames, IA, United States\nC r o py i e l dp r e d i c t i o nw h i c hp r o v i d e scritical information for management\ndecision-making is of signi ﬁcant importance in precision agriculture.\nTraditional manual inspection and calculation are often laborious and time-\nconsuming. For yield prediction using high-resolution images, existing methods,\ne.g., convolutional neural network, are challenging to model long range multi-\nlevel dependencies across image regions. This paper proposes a transformer-\nbased approach for yield prediction using early-stage images and seed\ninformation. First, each original im age is segmented into plant and soil\ncategories. Two vision transformer (ViT) modules are designed to extract\nfeatures from each category. Then a transformer module is established to deal\nwith the time-series features. Finally, the image features and seed features are\ncombined to estimate the yield. A case study has been conducted using a dataset\nthat was collected during the 2020 soybean-growing seasons in Canadianﬁelds.\nCompared with other baseline models, the proposed method can reduce the\nprediction error by more than 40%. The impact of seed information on\npredictions is studied both between models and within a single model. The\nresults show that the inﬂuence of seed information varies among different plots\nbut it is particularly important for the prediction of low yields.\nKEYWORDS\ntransformer, image recognition, time-seri es prediction, soybean yield prediction,\ndeep learning\n1 Introduction\nThe increasing world population imposes signi ﬁcant challenges for agriculture\nproduction due to the increasing food demand combined with limited arable land.\nAccurate yield prediction can help seed companies breed for better cultivars and guide\nfarmers to make informed management and ﬁnancial decisions. However, crop yield\nprediction is exceptionally challenging due to several complex factors, e.g. seed type, seed\nFrontiers inPlant Science frontiersin.org01\nOPEN ACCESS\nEDITED BY\nMarcin Wozniak,\nSilesian University of Technology, Poland\nREVIEWED BY\nAdam Tadeusz Zielonka,\nSilesian University of Technology, Poland\nTaewon Moon,\nSeoul National University,\nRepublic of Korea\n*CORRESPONDENCE\nGuiping Hu\ngphu@iastate.edu\nRECEIVED 24 February 2023\nACCEPTED 29 May 2023\nPUBLISHED 20 June 2023\nCITATION\nBi L,Wally O,Hu G,Tenuta AU,Kandel YR\nand Mueller DS (2023) A transformer-based\napproach for early prediction of soybean\nyield using time-series images.\nFront. Plant Sci.14:1173036.\ndoi: 10.3389/fpls.2023.1173036\nCOPYRIGHT\n© 2023 Luning Bi, Guiping Hu, Albert U.\nTenuta, Yuba R. Kandel and Daren S. Mueller\nand His Majesty the King in Right of Canada,\nas represented by the Minister of Agriculture\nand Agri-Food Canada for the contribution\no fO w e nW a l l y .T h i si sa no p e n - a c c e s s\narticle distributed under the terms of the\nCreative Commons Attribution License\n(CC BY).The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that\nthe original publication in this journal is\ncited, in accordance with accepted\nacademic practice. No use, distribution or\nreproduction is permitted which does not\ncomply with these terms.\nTYPE Original Research\nPUBLISHED 20 June 2023\nDOI 10.3389/fpls.2023.1173036\ntreatment, soil, temperature, etc. Thus, an analytical model that can\npredict crop yield accurately is essential.\nMachine learning methods have been designed for crop\nmonitoring and yield predicti on. Various models have been\nproposed for crop yield prediction. For example, Kaul et al.\ndeveloped an arti ﬁcial neural network model that used ﬁeld-\nspeciﬁc rainfall data and soil rating to predict soybean yield\nprediction ( Kaul et al., 2005 ). Khaki et al. proposed a deep neural\nnetwork approach for soybean yield prediction using genetic and\nenvironmental information ( Khaki and Wang, 2019 ). Compared to\nyield prediction using meteorol ogical driven variables (e.g.,\ntemperature, sunlight, and precipitation), using the sensing\nimages can capture more information about the plant growing\nstatus. For example, Rembold et al. used low-resolution satellite\nimagery for yield prediction ( Rembold et al., 2013 ); Nevavuori et al.\npresented a convolutional neural network (CNN) for crop yield\nprediction based on NDVI and RGB data acquired from unmanned\naerial vehicles (UAVs) ( Nevavuori et al., 2019 ); and Pantazi et al.\nbuilt a hybrid model to associate the high-resolution soil sensing\ndata with wheat yield ( Pantazi et al., 2016 ). However, there can still\nbe information loss in the process of using those images for yield\nprediction. This is because remote sensing images only provide a\nsnapshot of the conditions at a particular moment in time, and may\nnot capture all of the relevant factors that contribute to yield. In\naddition, factors such as cloud cover, shadows, and atmospheric\nconditions can all affect the quality and accuracy of remote-\nsensing images.\nCompared to hyperspectral images, handheld devices capturing\nimages of the canopy can provide higher resolution and more\ninformation due to the increased number of pixels. While higher\nresolution images can provide more detailed information, using\ndata from a single time point may not be suf ﬁcient to accurately\npredict yield. Factors such as lighting conditions, soil status, and\nplant growth stage can all have a signi ﬁcant impact on the quality\nand accuracy of the image data. These undetermined factors and\nnoise can confuse models in the training stage, resulting in the\ndeterioration of generalization ability. The incorporation of time-\nseries prediction is necessary fo r yield prediction to improve\nperformance ( Nevavuori et al., 2020 ; Qiao et al., 2021 ).\nThere are two challenges for yield prediction using time-series\nimages, i.e., image p rocessing and 48 time-series prediction.\nExisting studies usually use the convolutional neural network\nwith long short term memory model (CNN-LSTM) framework\nfor feature extraction of time-series images. For example, Sun et al.\ncombined the CNN and LSTM to predict soybean yield using in-\nseason and out-season image data collected from Google Earth ( Sun\net al., 2019 ). Newton et al. used 16-day remote sensing images (30m\nby 30m) to predict potato yield ( Newton et al., 2018 ). Shari ﬁ et al.\napplied different machine learning approaches to the barley yield\nprediction using the time-series NDVI and environmental\ninformation ( Shari ﬁ, 2021 ). However, this framework has\nsome drawbacks.\nFor image classi ﬁcation/recognition, although the CNNs have\noutstanding performance on many tasks ( Ferentinos, 2018; Jin et al.,\n2018; Ma et al., 2018), the CNNs have some redundancy issues in both\ncomputation and representations since each pixel bears varying\nimportance for the target task. Recently, the transformer module has\nbeen considered as an alternative architecture and has achieved\ncompetitive performance on many computer vision tasks ( Xie et al.,\n2021). Vision transformer (ViT) is a t ransformer-based method that is\ndesigned for image classiﬁcation (Dosovitskiy et al., 2020). In ViT, an\nimage is split into ﬁxed-size patches. Each patch is then linearly\nembedded, position embedding s are added, and the resulting\nsequence of vectors is fed to a standard transformer encoder.\nCompared to CNN, ViT has a better global understanding of\nthe images.\nRegarding the time-series prediction, LSTMs have been\nemployed to model time series in different tasks ( Sundermeyer\net al., 2012 ; Huang et al., 2015 ; Zhao et al., 2017 ). In a LSTM, the\nhidden state is updated with every new input token to remember the\nentire sequence it has seen. Theoretically, this structure can\npropagate over in ﬁnitely long sequences. However, in practice,\ndue to the vanishing gradient problem, the LSTM will eventually\nforget earlier tokens ( Li et al., 2019 ). Another drawback of the\nLSTM is that it can only be implemented sequentially due to its\nstructure. In comparison, transformers retain direct connections to\nall previous timestamps, allowing information to propagate over\nmuch longer sequences and be processed in parallel.\nTo solve the aforementioned challenges, a transformer-based\nmethod is used to predict soybean yield using time-series images\nand seed information. The contribution of our work includes the\nfollowing aspects:\n A method consisting of two ViT modules and one\ntransformer is proposed for the feature extraction of time-\nseries images. Instead of using the original images directly,\nthe proposed method process the plant part and soil part of\nthe image separately to reduce the computation complexity\nand improve the interpretability of the model.\n Different baseline models were compared to validate the\neffectiveness of the proposed approach. The experiments\nshow that the proposed method can signi ﬁcantly improve\nyield prediction accuracy.\n The impact of seed information on predictions is studied\nboth between models and within a single model. The results\nshow that the seed information play important roles in\npredicting low yields.\n2 Materials and methods\n2.1 Data collection\nThis study used a dataset collected from three soybean ﬁelds in\nOntario, Canada in 2020. There are 450 plots in total. The data\nincludes two types of input information. The ﬁrst is the time-series\nimages. The second part is the seed information of each plot. For\neach plot, there are three images, as shown in Figure 1 , collected in\nthree dates, on June 14, 2020, on July 13, 2020 and on August 20,\n2020. The seed information is shown in Table 1. Seed treatments are\nthe additional material added to the seed. There are six major\nBi et al. 10.3389/fpls.2023.1173036\nFrontiers inPlant Science frontiersin.org02\ngroups of seed treatments, i.e., Non-treated control, base seed\ntreatment control, ILEVO alone, ILEVO+Base, Saltro+Base and\nother. The seed information also include seed varieties (resistant or\nsusceptible to soybean sudden death syndrome) and seeding rates\n(for example, 110K, 140K, and 170K seeds/acre). The three seed\nfactors will be investigated together. There are 51 combinations of\nseed varieties, treatments and seeding rates in total. The numbers of\nplots for each combination are similar. The objective of this paper is\nto use the time-series images and seed information to predict\nthe yield.\nThe distribution of the yield of plots is shown in Figure 2 . The\ndistribution is a little left-skewed. The kurtosis is 3.09 and the\nskewness is -1.26. Most plots have a yield between 3500 kg/ha and\n5000 kg/ha.\n2.2 Image segmentation\nIn the data processing, each image is segmented into two parts,\ni.e., plant segmentation and soil segmentation, as shown in Figure 3.\nThis is for two reasons. First, the information extracted from plant\nitself with the soil can be decoupled. Each module only needs to\ncalculate the same type of information, i.e., either plant or soil part,\nwhich will reduce the redundant computation. The interaction\nbetween plant and environment is calculated afterward. Second, it\ncan help reduce the in ﬂuence of the diagonal camera angles. The\nsegmentation can directly tell the model the distance between two\nadjacent rows of plants. Thus the model can distinguish the plants\nat the near-end from the plants at the far end.\n2.3 Workﬂow of soybean yield estimation\nAs shown in Figure 4 , the work ﬂow can be divided into three\nsteps: data collection, data processing, and prediction. In the data\ncollection, a sensing system is built to take the images of a ﬁeld at a\ncertain frequency. The images along the soybean growth stage and\nthe checked yield are stored in the database. In data processing,\nsome statistical analysis and image segmentation are conducted to\nprepare for the following analysis. Finally, various prediction\nmodels are designed to predict soybean yield. The models will be\nevaluated by some feasible metrics so that they can be further\noptimized accordingly.\nThe prediction is the most challenging component. The\nsolution needs to answer three questions. How to ef ﬁciently\nextract features from a single image? How to detect the hidden\npattern in the time-series images? How to combine different sources\nof information, i.e., images and seed information? This serves as the\nmotivation of this paper.\n3 Proposed model\nTo address the aforementioned challenges, a wide-deep method\nbased on the attention mechanism is proposed. In this section, we\nwill focus on the prediction part of the work ﬂow, as shown in\nFigure 4 , especially the design logic and module about feature\nextraction of the images and seed information.\n3.1 A wide-deep framework\nAs introduced in Sec. 2, this study considers two types of inputs:\ntime-series images and seed information. Thus, different modules\nshould be applied due to the heterogeneity of the inputs. The time-\nFIGURE 1\nAn example image of a plot.\nTABLE 1 Seed information (seed treatment, seed variety and seeding rate).\nSeed treatment Number of combinations (seed variety and\nseeding rate)\nNon-treated control 5\nBase seed treatment\ncontrol\n9\nILEVO alone 2\nILEVO + Base 9\nSaltro + Base 10\nOther 16\nFIGURE 2\nDistribution of the soybean yield.\nBi et al. 10.3389/fpls.2023.1173036\nFrontiers inPlant Science frontiersin.org03\nseries images have a large number of pixels. The model should be\ncapable of extracting the most important interactions between\npixels effectively. Thus, a high-level feature representation of the\nimages is needed. In contrast, the seed information only contains\none categorical variable in this study. It is not necessary to apply a\ncomplex or extremely deep neural network. Therefore, a wide-deep\nframework is proposed as shown in Figure 5 .\nThe left tower of the proposed framework is composed of two\nViT modules and one transformer module. Two ViT modules are\nused to extract features from the plant and soil, separately. The\noutputs from the two ViTs are combined using a dot product\noperator. Then the transformer is leveraged to deal with the time-\nseries features. The right tower is just a fully connected neural\nnetwork. The 51 combinations of seed information is one-hot\nencoded. Then the neural network is used to further extract\ninformation from the one-hot encoding. Finally, the wide\ncomponent (i.e., seed features) and deep component (i.e., image\nfeatures) are combined using one common FCDNN for joint\ntraining according to Eq. 1.\nj = F((f\nplant ·f soil)+ fseed) (1)\nWhere fsoil is the feature obtained from the soil segmentation of\nan image, fplant is the feature extracted from the plant segmentation,\nfseed is the feature extract from the seed information, i.e., seed\nvariety, treatment and seeding rate, j represent the predicted yield\nand F denotes a one-layer fully connected neural\nnetwork (FCDNN).\nIt should be noted that the image features and seed features are\ncombined and then jointly trained. This is different from ensemble\ntrain. In an ensemble model, individual models or weak estimators\nare trained separately without any interaction during the training\nprocess. Then their outputs are combined only at the ﬁnal step (i.e.,\nprediction) by majority voting or averaging. In contrast, the wide-\ndeep framework will jointly train all parameters simultaneously by\ntaking both the image features and seed features as well as the\nweights of their sum into account. The training of the deep-wide\nmodel is done by backpropagating the gradients from the output to\nboth the wide and deep part of the model simultaneously using\nA B\nFIGURE 3\nImage segmentation.(A) Segmentation of plant part.(B) This is the caption for Segmentation of soil part.\nFIGURE 4\nFlow diagram of the data collection, processing and prediction we employed in this study for yield prediction.\nBi et al. 10.3389/fpls.2023.1173036\nFrontiers inPlant Science frontiersin.org04\nstochastic gradient descent (SGD) or other optimizers such as\nAdam and Adagrad. By leveraging this deep-wide framework, the\ntraining time or inference time can be signi ﬁcantly reduced due to\nfewer parameters in the wide part.\nIn the following sections, we will explain the details of the\nattention mechanism, transformer and ViT\n3.2 Attention mechanism\nAttention is a technique proposed to help the model to focus on\nthe most important parts of its input, rather than treating all parts\nequally ( Vaswani et al., 2017 ).\nAs shown in Eq. 2, for each input in a given vector a1, a2, a3… ,\nthree matrices, i.e. query Wq, key Wk and value Wv, are employed to\ngenerate three representation vector i.e., Q, K and V,b y\nmultiplication. Q represents the query to match other inputs. K is\nthe key to be matched by others. V represents the information to be\nextracted. Then the attention score between two inputs can be\ncalculated by Eq.3 to obtain the attention coef ﬁcients.\nQ = aWq, K = aWk, V = aWv (2)\nAttention (Q, K, V) = softmax ( QKT\nﬃﬃﬃﬃ ﬃ\ndk\np )V (3)\nWhere dk is the dimension of the keys and queries which is used\nto scale the dot product of Q and K Speciﬁcally, we repeat the\nattention for htimes and concatenate the learned embeddings as the\nﬁnal representation of the inputs:\nMultiHead (Q, K, V) = Concat (head 1, … ,  headh)WO (4)\nWhere headi = Attention ( QWQ\ni , KWK\ni , VWV\ni )The attention\nmechanism is the backbone of transformer and ViT.\n3.3 Vision transformer for image\nfeature extraction\nSelf-Attention is capable of understanding the connection\nbetween inputs. However, it is challenging to apply it between the\npixels of an image. For instance, if the size of the input image is\n300x300, a self-attention layer has 90K combinations to calculate. In\nfact, a lot of the calculation are redundant because only part of the\nconnections between two pixels are meaningful. To overcome this\nproblem, ViT is proposed by segmenting images into small patches\n(like 16x16) ( Dosovitskiy et al., 2020 ). A patch is the basic unit of an\nimage instead of a pixel to ef ﬁciently tease out patterns.\nIn ViT, an image x ∈ RH·W·C is reshaped into N patches xp ∈\nRN·P2·C, where ( H, W) is the resolution of the original image, C is\nthe number of channels, P2 is the resolution of each patch. In\naddition to patches, ViT also use a learnable embedding Epos for\neach patch to represent the relative position. Thus, the patch\nembeddings can be represented as in Eq. 5.\nz0 = ½x1\npE; x2\npE; ⋯; xN\np E/C138 + Epos ,  E ∈ R(P2·C)/C2 D,\nEpos ∈ R(N+1)/C2 D\n(5)\nAssuming that there are L layers in the ViT, then in each layer,\nmulti-head attention and MLP is applied to the input of each layer\nas shown in Eq. 6 and Eq. 7. The calculation of multi-head attention\nis explained in Eq. 4.\nz\n0\n‘ = MultiHead (LN ( z‘−1)) + z‘−1, ‘ =1 … L (6)\nz‘ = MLP (LN ( z\n0\n‘)) + z\n0\n‘, ‘ =1 … L (7)\nWhere LNis the Layernorm operator ( Wang et al., 2019 ). LNis\napplied before every block, and residual connections after\nevery block.\nFIGURE 5\nA wide-deep framework for yield prediction.\nBi et al. 10.3389/fpls.2023.1173036\nFrontiers inPlant Science frontiersin.org05\nThe last step is to output the image features as calculated using 8\ny = LN( z0\nL) (8)\n3.4 Transformer for time-series prediction\nFor time-series prediction, r ecurrent neural network (RNN) or\nLSTM are usually the ﬁrst ones to consider. However, this type of\nmodels is hard to parallel because the models process the input of each\ntimestamp in sequence order. Then, some studies adopted CNN to\nrealize parallelization of the feature extraction. Nevertheless, CNN can\nonly consider the input in a limited range. For long-term dependency\nmodeling, CNN needs to increase the number ofﬁlters and the number\nof layers. Therefore, transformers based on the self-attention\nmechanism are applied for time-ser ies prediction. It computes the\nrelation between two timestamps in a bi-directional manner, which\nmeans it can be implem ented in parallel.\nThe basic structure of a transformer used for sequence-to-\nsequence tasks includes encoder and decoder parts ( Wu et al.,\n2020). Nevertheless, in this study, the task is to transform a\nsequence to some features. Thus, only the encoder part is used\nfor the transformer. The encoder of the transformer is composed of\nan input layer, a positional encoding layer, and a stack of multi-\nhead attention layers. The input layer maps the input time-series\ndata to a vector through a fully-connected network. Positional\nencoding with sine and cosine functions is used to encode\nsequential information in the time series data by element-wise\naddition of the input vector with a positional encoding vector,\nwhich is the same as Eq. 5. Each multi-head layer is to calculate the\nattention coef ﬁcients between the image features of every two\ntimestamps. Finally, there is an output layer that maps the output\nof the last multi-head attention layer to image features.\n4 Baseline models and\nexperiment settings\nTo validate the effectiveness of the proposed method, we\ncompared it with other baseline models.\n4.1 Baseline models\nThe three most commonly used models are implemented as the\nbaseline models, i.e., convolutional neural network with linear\nregression (CNN-LR), CNN-LSTM, and vision transformer with\ntransformer (ViT-T). The processing of seed information is the\nsame for all baseline models and the proposed method.\n4.1.1 Convolutional neural network with\nlinear regression\nCNN is a class of deep, feed-forward arti ﬁcial neural networks. It\nwas adopted widely for its fast deployment and high performance on\nimage classiﬁcation tasks. CNNs are usually composed of convolutional\nlayers, pooling layers, batch normalization layers and fully connected\nlayers. The convolutional layers extract features from the input images\nwhose dimensionality is then re duced by the pooling layers. Batch\nnormalization is a technique used t o normalize the previous layer by\nsubtracting the batch mean and dividing by the batch standard\ndeviation, which can increase the stability and improve the\ncomputation speed of the neural networks. The fully connected\nlayers are placed near the output of the model. They act as classi ﬁers\nto learn the non-linear combination of the high-level features and to\nmake numerical predictions. Detailed descriptions on each type of\nfunction can be accessed from Gu et al. (2018) .\nIn CNN-LR, ﬁrstly, a CNN is built to extract features from a\nsingle image. Then the obtained features from time-series images\nare concatenated with seed features and then used as the input of a\nlinear regression model. Since the linear regression model cannot\ndetect the dependency in a time series, CNN-LR is used to show the\ninﬂuence of time-series features.\n4.1.2 Convolutional neural network with long-\nshort the memory model\nDespite its popularity as a unive rsal function approximator and\neasy implementation, RNN is faced with the gradient vanishing/\nexploding problem. In the training process of RNNs, gradients are\ncalculated from the output layer to the ﬁrst layer of the RNN. If the\ngradients are smaller than 1, the gradients of the ﬁrst several layers will\nbecome small through many multiplications. On the contrary, they will\nbecome very large if the gradients are larger than 1. Therefore, it\ns o m e t i m e sc a u s e st h eg r a d i e n t st ob ea l m o s tz e r oo rv e r yl a r g ew h e ni t\nreaches the ﬁrst layers of RNNs. Consequently, the weights of the ﬁrst\nlayers will not get updated in the training process. Therefore, simple\nRNNs may not be suitable for very long time series. LSTM solves this\nissue by introducing the concept of gates. A common LSTM unit is\ncomposed of a cell, an input gate, a n output gate and a forget gate. At\neach timestamp, the cell adjust its st ate value according to the current\ninput and memory of previous steps .A n dt h et h r e eg a t e sr e g u l a t et h e\nﬂow of information into and out of the cell. Therefore, LSTM can\nextract features from long time series. Detailed explanations and\ncalculations of each function can b e accessed from Hochreiter et al\n(Hochreiter and Schmidhuber, 1997).\nIn CNN-LSTM, the ﬁrst step is to extract features from a single\nimage. Then the extracted features of images taken at different\ntimestamps are treated as a time series. LSTM is employed to deal\nwith the time-series features. The output obtained by LSTM is\ncombined with seed features to get the yield prediction through a\nfully connected neural network.\n4.1.3 Vision transformer with transformer\nIn ViT-LSTM, the image is processed using a ViT module to get\nthe image representation. Then the time-series image features are\nused as the input of the LSTM module. The yield prediction is made\nbased on the output of the transformer and the seed features.\n4.1.4 Vision transformer with transformer\nDifferent from the proposed method, in ViT-T, the image is not\nsegmented into soil and plant parts. Thus only one ViT module is\nBi et al. 10.3389/fpls.2023.1173036\nFrontiers inPlant Science frontiersin.org06\nutilized to read images. Then the time-series image features are used\nas the input of the transformer. The yield prediction is made based\non the output of the transformer and the seed features.\n4.2 Experiment settings\nThe module for the seed combination information processing is\nthe same for all baseline models. The seed combination is one-hot\nencoded and then connected to three dense layers of 16 neurons.\nThus, the output embedding size of seed information is 16. Then the\nseed combination embedding is concatenated with the image\nembedding and the concatenated vector is connected with three\ndense layers of 128 neurons each, followed by a dense layer with one\nneuron to produce the ﬁnal prediction. To avoid the over ﬁtting\nissue caused by limited data, all models are using early stopping and\ndropout techniques. This encourages the network to learn more\nrobust features by preventing individual nodes from becoming too\nspecialized on a particular set of features. The dropout rate used for\nthe dense layers is 0.25, except for the output layer which uses a\nlinear activation function. The early stopping is used with the\npatience of 10 epochs. The time-series image processing part of\nthe models is as follows.\nIn the CNN-LR model, the convolutional neural network\n(CNN) module uses the VGG-16 architecture, which consists of\n13 convolutional layers and 3 fully connected layers ( Simonyan and\nZisserman, 2014 ). The linear regression module is applied with L2\nnorm regularization to prevent over ﬁtting. The model expects the\ninput to be a three-dimensional tensor of size (128, 128, 3)\nrepresenting the image size and number of channels. The output\nof each time-series image from the VGG model is ﬂattened and\nconcatenated together. The concatenated image embeddings are\nconnected with a dense layer (i.e., the LR module) of 256 neurons to\nextract features from the images. In CNN-LSTM, the CNN module\nis the same as that in CNN-LR. The output of time series images\nfrom the VGG model is processed by a LSTM module which has\ntwo bi-directional LSTM layers. Each LSTM layers contains 128\nneurons. The output from the LSTM module is 128 neurons.\nIn the ViT-LSTM model, the Vision Transformer (ViT) module\nconsists of two multi-head attention layers, with each layer having 3\nheads. The output of the time-series images from the VGG model is\nthen processed by same LSTM module as in CNN-LSTM. In the\nViT-T model, the Vision Transformer (ViT) module consists of two\nmulti-head attention layers, with each layer having 3 heads. The\noutput of the time-series images from the VGG model is then\nprocessed by a transformer module, which also has 3 multi-head\nattention layers, each with 5 heads. The output from the\ntransformer module is 128 neurons.\nThe proposed method uses two ViT modules, one to process the\nplant part of the image and another to process the soil part of the\nimage separately. The ViT modules have the same architecture as\nthat in the ViT-T model, with two multi-head attention layers, each\nwith 3 heads, and a transformer module with 3 multi-head attention\nlayers, each with 5 heads. The output embeddings from the plant\nand soil ViT modules have the same size of 128 neurons.\nAll models are trained using the mean squared error (MSE) loss\nfunction and the Adam optimizer with a learning rate of 0.001. 344\nplots are used as the train set. 38 plots are used as the validation set.\n68 plots are used as the test set.\nThree metrics are used to assess the model performance, i.e.,\nroot mean squared error (RMSE), R squared value, and mean\nabsolute error percentage (MAPE). The calculations are as in Eq.\n9, Eq. 10 and Eq. 11.\nRMSE =\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n1\nn S(y − ^y )2\nr\n(9)\nR2 =1 − RSS\nTSS (10)\nMAPE = 1\nn S( y − ^y\ny\n/C12/C12/C12\n/C12\n/C12/C12/C12\n/C12 ) (11)\nWhere n is the number of samples, y is the ground-truth yield, ^y\nis the predicted yield, RSS is the sum of squares of residuals, and TSS\nrepresents the total sum of squares.\n5 Results\n5.1 Comparisons with baseline models\nThe performance of CNN-LSTM and the proposed method are\ncompared by plotting their predicted values and the ground truth\nfor the test set in Figure 6 . The results show that, in general, the\npredicted values of the proposed method are closer to the ground\ntruth than those of CNN-LSTM. Moreover, it is observed that the\nmodels tend to be conservative in making predictions. For instance,\nin two plots where the ground truth values are between 2200 kg/ha\nand 3000 kg/ha, both models predict values above 3140 kg/ha. The\nproposed method performs better than CNN-LSTM in these two\nFIGURE 6\nPredicted values and the ground truth for the test set.\nBi et al. 10.3389/fpls.2023.1173036\nFrontiers inPlant Science frontiersin.org07\nplots. Additionally, while the predicted values of CNN-LSTM was\nbetween 3900 kg/ha and 4500 kg/ha for other plots, the predicted\nvalues of the proposed method shows more diversity, indicating its\nability to perform better in extreme cases.\nTable 2 presents the test RMSE, R-squared, and MAPE values\nobtained in this study. If the mean value of each combination of\nseed information is used as the estimate, the test RMSE, R-squared,\nand MAPE values are 570.596, 0.010, and 12.412%, respectively.\nThe R-squared value of 0.010 indicates that using only the seed\ninformation yields slightly better results than using the mean values\nof all train plots. However, the introduction of CNN-LR improves\nthe RMSE, R-squared, and MAPE by 11.7hance\nprediction accuracy.\nViT-LSTM uses ViT instead of CNN for image representation,\nwhich improves the RMSE by 6.2%, R-squared by 0.08, and MAPE\nby 0.3%, respectively. ViT-T is an upgraded version based on the\nCNN-LSTM structure with a multi-head self-attention mechanism,\nresulting in an 8.9% reduction in RMSE, a 0.1 increase in R-squared,\nand a 0.4% decrease in MAPE, respectively. However, the\nimprovement of ViT-T compared to ViT-LSTM is not signi ﬁcant,\npossibly because of the short time series used in this study.\nThe proposed method, which includes two ViT modules and\none transformer, signi ﬁcantly reduces the RMSE by 34.0%,\nincreases the R-squared by 0.27, and reduces the MAPE by 2.5%.\nThis indicates that the proposed model outperforms the other\nmodels and effectively captures the temporal and spatial\ndependencies in the data.\nTo validate the effectiveness of deep learning models in feature\nrepresentation of images, this study conducts experiments on three\nlinear regression-based models, namely Model 1, Model 2, and\nModel 3, using different input con ﬁgurations. As presented in\nTable 3 , Model 1 solely utilizes the one-hot encoded seed\ncombination as the input, while Model 2 takes the latest image\n(i.e., the last image in the time series) and the one-hot encoded seed\ncombination as input. Model 3, on the other hand, utilizes the time-\nseries images and the one-hot encoded seed combination as input.\nIn each model, all inputs are concatenated into one-dimensional\nvectors. Both Model 2 and Model 3 are linear regression models\nwith the L2 norm regularization technique to prevent over ﬁtting.\nThe evaluation results reveal that Model 1 exhibits the poorest\nperformance with a test RMSE of 665.729, a Test R-squared of\n-0.348, and a test MAPE of 13.346%. This performance is attributed\nto underﬁtting, which occurs when using only one input feature. By\nincorporating image data, Model 2 outperforms Model 1 and the\nAver-seed method in Table 2 , achieving a 3.0% improvement in\nRMSE and a 2.3% improvement in MAPE. Moreover, Model 3\nfurther enhances performance by including time-series images,\nresulting in a 2.7% reduction in RMSE, a 0.05 improvement in R-\nsquared, and a 0.18% improvement in MAPE. Therefore, even a\nsimple generalized model can bene ﬁt from time-series prediction to\nimprove performance. However, the performance of the linear\nregression-based models is signi ﬁcantly lower than that of the\ndeep learning models presented in Table 2 . This discrepancy is\nprimarily due to two reasons. Firstly, the use of average RGB values\nmay lead to signi ﬁcant information loss. Secondly, linear regression\ncannot extract the region-level features from the extensive pixel\ninformation like CNN or ViT. Hence, the results prove the\nimportance of using large computer vision models for image\nprocessing in agriculture, wh ich is a crucial area for future\nresearch on large datasets for various agricultural tasks such as\ndisease detection, yield prediction, and plant status monitoring.\n5.2 Inﬂuence of seed information\nThe in ﬂuence of seed information (i.e., seed variety, treatment\nand seeding rate) on the model ’s overall performance is also\ninvestigated. As shown in Table 4 , four methods, i.e., average of\nall, average with seed information, proposed method without seed\ninformation, and the proposed method, are tested. Compared to\nusing the mean values of all training samples as the estimate, using\nthe mean values of each group can reduce the test RMSE from\n585.127 to 580.59. However, the test MAPE of average with seed\ninformation is higher, which indicates that using the average with\nTABLE 2 Comparisons between baseline models and the proposed method.\nProcesses Module Aver-seed CNN-LR CNN-LSTM ViT-LSTM ViT-T Proposed\nImage Segmentation ✓\nCNN ✓✓\nViT ✓✓✓\nTime-series LR ✓\nLSTM ✓✓\nTransformer ✓✓\nSeed FCNN ✓✓ ✓ ✓ ✓\nEvaluation Test RMSE 570.569 510.959 481.191 451.615 445.619 332.072\nTest R squared 0.010 0.205 0.295 0.379 0.395 0.664\nTest MAPE (%) 12.412 9.648 9.176 8.864 8.811 6.340\nAver-seed: using the mean values of each combination of seed information as the estimate.\n\"✓\" means that the module is selected for the corresponding process in a speci ﬁc method.\nBi et al. 10.3389/fpls.2023.1173036\nFrontiers inPlant Science frontiersin.org08\nseed information only performs better in reducing the variance of\nthe error. The improvement in R squared is 0.01. Compared to the\nproposed method without seed information, the proposed method\ncan improve the RMSE by 13.1% and R squared by 0.11,\nrespectively. It shows that using the neural network to process the\nseed information is more effective than just using the group average\nvalues. Besides, the model ’s prediction accuracy relies more on the\nimage information rather than the seed information.\nTo determine the effect of various seed variety, treatment and\nseeding rate on yield prediction quantitatively, an experiment is\nc o n d u c t e db yt a k i n gi m a g e so fe a c ht e s tp l o tw i t ha l l5 1\ncombinations (i.e., 50 pseudo and one true combination) as the\ninput. Thus, there are 51 predicted values for each test plot. The box\nplot is shown in Figure 7 . The median value of a box can be used as\nan approximation for the prediction made using only images. The\nresults, shown in a box plot, have interquartile ranges (IQRs) from\n120kg/ha to 500kg/ha. A shorter IQR indicates the model extracts\nmore information from the images, indicating its robustness to\nvariations in seed information. This is because images taken at\ndifferent stages of growth may contain additional information about\nthe seed variety, treatment and seeding rate used. In other words,\nthe image may contain some information that overlaps with the\nseed information. Comparing the true prediction with the box plot,\n47 of 68 (i.e., 69.1%) test plots have true predictions within the\nboxes (i.e., between the 25 percentile and 75 percentile) while 21 of\n68 (i.e., 30.9%) test plots fall outside the IQRs. It means the error is\nwithin 120kg/ha to 500kg/ha when replacing the true seed\ninformation with 50% pseudo combinations as the input for the\ntest plots.\nFor Plot 14, 37, 41 and 67, the true predictions are outliers\n(below 1.5*IQR from the lowe r percentile) compared to all\npredictions, indicating the model extracts more information from\nthe seed information for these plots. The details of Plot 14, 37, 41\nand 67 are shown in Table 5 . Plots 14, 37, and 41 have the lowest\nground truth and predicted yield, but this does not necessarily\nindicate that “Saltro” treatments result in lower yields. It may\nsimply mean that the model requires more information about the\nseed treatment to improve its predictions for certain plots.\nIt is also worth noting that for all plots, the outlier values are\nbelow the boxes, suggesting that seed information plays an\nimportant role in helping the model predict low yields with\ndownward correction.\n6 Discussion\nCrop yield prediction help farmers estimate yield before a ﬁeld\nis harvested. Additionally, it can serve as an essential tool for the\ndecision-makers to make plans regarding food security. However,\nmany factors both genetic and environmental, before and during\nthe season, make it challenging to obtain an accurate prediction.\nYield prediction using images recently became a popular topic\ndue to two reasons. The ﬁrst reason is that images can store all the\nFIGURE 7\nBox plot of predictions using images of each plot with 51\ncombinations of seed variety, treatment and seeding rate. The red\ndots represent the true predictions which are predicted values using\nthe images with the true seed combination. The box plot is the\nresult of predicted values using the images with all 51 combinations,\nincluding 50 pseudo and one true combination. The 68 test plots\nare numbered from 0 to 67. (Note: The true prediction is not the\nground truth.).\nTABLE 3 Comparison of three linear regression-based models.\nProcesses Module Model 1 Model 2 Model 3\nImage LR ✓✓\nTime series LR ✓\nSeed LR ✓✓✓\nTest RMSE 665.729 545.619 530.489\nEvaluation Test R squared -0.348 0.094 0.144\nTest MAPE (%) 13.346 10.993 10.816\nThe “Image” row indicates whether image information is used as input, where the RGB (Red,\nGreen, and Blue) values of each pixel are averaged, and the images are converted into one-\ndimensional vectors. The “Time series” row indicates whether time-series images are used as\ninput. The “Seed” row indicates whether the one-hot encoded seed combination is used as input.\n\"✓\" means that the module is selected for the corresponding process in a speci ﬁc method.\nTABLE 4 Inﬂuence of seed information.\nMethods Test\nRMSE\nTest R\nsquared\nTest MAPE\n(%)\nAverage of all 585.127 0 11.385\nAverage with seed information 570.569 0.010 12.412\nProposed method without seed\ninformation\n382.820 0.554 7.895\nProposed method 332.072 0.664 6.340\nAverage of all: using the mean values of all training samples as the estimate. Average with seed\ninformation: using the mean values of each group as the estimate.\nTABLE 5 Analysis of predicted values for Plot 14, 37, 41 and 67.\n(Unit: kg/ha).\nPlot\nNo.\nTrue seed infor-\nmation\nGround\ntruth\nTrue pre-\ndiction\nBox\nmedian\nvalue\n14 Saltro: Resistant 2216.443 2470.862 3796.238\n37 Saltro: Susceptible 2914.385 3219.396 3406.339\n41 Saltro: Resistant 3174.018 3223.063 4140.596\n67 Non-treated control:\nResistant: 110K\n3972.710 3864.384 4285.763\nBi et al. 10.3389/fpls.2023.1173036\nFrontiers inPlant Science frontiersin.org09\nphenotype information of the plant as well as some environmental\ninformation (i.e., soil color, light condition, etc.). The second reason\nis that the development of deep learning techniques in computer\nvision has facilitated information extraction from plant-level or\nﬁeld-level images. Different from the research using satellite\n(Rembold et al., 2013 ; Schwalbert et al., 2020 ) or UAV ( Zhou\net al., 2017 ; Hassan et al., 2019 ) images, this study used high-\nresolution camera images of ﬁeld level. This will help to improve the\nprediction accuracy since more pixels represent more information\nabout the plant.\nInstead of using individual static imagery, the proposed\nframework leverages the time-series images for yield prediction.\nThe time-series images can monitor the plant status of plants at\ndifferent time points and eliminate the in ﬂuence of noise on the\nmodel performance. This has been supported by many researches\n(Clevers, 1997 ; Aghighi et al., 2018 ; Varela et al., 2021 ). In our case\nstudy, the single image method, i.e., CNN-LR, is compared with the\ntime-series image method, i.e., CNN-LSTM. The results show that\ntime-series images can help improve test RMSE by 6.2%, R squared\nby 0.9%, and MAPE by 0.5%. Since each plot only has about three\nimages, the improvement could be more signi ﬁcant if additional\nimages were provided. Besides, the traditional CNN-LSTM\nframework ( Sun et al., 2019 ; Nassar et al., 2020 ) is upgraded to\nthe ViT-T framework by introducing the attention mechanism.\nCNNs are ef ﬁcient in image information extraction compared to\nfully connected neural networks due to shared kernel weights.\nHowever, CNNs only aggregate the global information in high-\nlevel layers. ViTs incorporate more global information than CNNs\nat lower layers, leading to quantitatively different image features. In\nterms of time-series prediction, although LSTM can capture the\nlong-term dependencies of the time series, it get inputs in sequence\nand cannot be implemented in parallel. Thus, ViT-T is better in the\nglobal understanding of images, computation ef ﬁciency and parallel\nimplementation. In our case, the images were taken from one side of\nthe plot. The information density of the image in the far end and the\nnear end are different. Since ViT segments images into small\npatches, it can assign different weights according to the region/\npatch and achieve better granularity. The comparison results show\nimprovements of 8.9% in test RMSE, 0.1 in R squared and 0.3%\nin MAPE.\nBesides, the proposed method segmented the image into the\nplant part and the soil part. By using two ViT modules, the plant\nstatus and the environmental in ﬂuence can be modeled separately.\nThen the two parts are multiplied to obtain soybean yield.\nCompared to the one-ViT versio n, i.e., ViT-T, the proposed\nmethod signi ﬁcantly reduces RMSE by 34.0%, increases R square\nby 0.27 and reduces MAPE by 2.5%.\nAnother contribution of our work is the examination of the\neffect of seed variety, treatment and seeding rate on predictions,\nboth across different models and within a single model. The results\nof the group average method indicate that the statistical importance\nof seed information is limited, as the test R squared is only 0.01.\nHowever, in the proposed method, seed information contributes\n0.11 to R squared compared to using the same structure without\nseed information input. It means the neural network can extract\nmore information from the seed information and combine it with\nthe image features to make predictions. The examination of the\neffect of seed information within the proposed method reveals that\nthe in ﬂuence of seed treatments varies among different plots. Seed\ntreatment information is particularly important for the prediction\nof low yields. Additionally, the wide-deep framework can be used to\nincorporate more types of input information, such as genetic\ninformation, in the future.\n7 Conclusions\nYield prediction can provide more guidelines for farmers to\ndecide on the management plan. The development of deep learning\ntechniques has facilitated the application of sensing techniques in\nprecision agriculture through various types of imagery. In this\nstudy, in order to catch more global interactions between image\npatches and timestamps, a transformer-based method is proposed\nto extract image information and time-series changes of soybean\nstatus. Besides, the original images are segmented into the plant part\nand soil parts. A wide-deep structure is adopted to incorporate\nother information, i.e., seed information, into prediction. Compared\nto other baseline models, the proposed model can reduce the RMSE\nby up to 40%. The effect of seed information on predictions, both\nacross different models and within a single model, is also examined.\nThis study demonstrates the potential of a large-scale computer\nvision model for predicting crop yield using high-resolution time-\nseries images captured by a hand-held device. However, certain\nlimitations must be acknowledged. Firstly, the impact of time-series\nlength on prediction accuracy remains unexplored due to the\nlimited size of the dataset. While it is reasonable to expect that\nincreasing the number of images during the growth stages for\ntraining would improve the model ’s performance, redundant\ninformation may also impact the model ’s generalization ability or\nrequire increased computation resources. Therefore, exploring\nthese factors ’ trade-offs is a meaningful avenue for future\nresearch. Secondly, the model ’s input only considers limited seed\ninformation, but could potentially bene ﬁt from the incorporation of\nadditional information such as g enetic or soil ch aracteristics.\nFinally, while the attention score for images and time series is\ncalculated separately in this study, exploring the attention score\nbetween image patches at different timestamps may improve\nmodel performance.\nDespite these limitations, the proposed large-scale computer\nvision model demonstrates the potential for extension to various\napplications critical to precision agriculture, including but not\nlimited to, disease and pest detection, weed detection and control,\nand crop quality assessment. These tasks require sophisticated\nmodels capable of capturing ﬁne-grained details in plant leaves\nand other relevant features. Addressing the aforementioned\nlimitations and developing more ef ﬁcient multi-modal methods\nfor yield prediction using images and environmental information\nrepresent promising avenues for future research.\nBi et al. 10.3389/fpls.2023.1173036\nFrontiers inPlant Science frontiersin.org10\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material. Further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nLB, GH, YK, and DM conceived the idea, LB implemented the\nalgorithm, OW, AT, YK, and DM provided the data, all reviewed\nthe manuscript. All authors contributed to the article and approved\nthe submitted version.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential con ﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated organizations,\nor those of the publisher, the editors and the reviewers. Any product\nthat may be evaluated in this article, or claim that may be made by its\nmanufacturer, is not guaranteed or endorsed by the publisher.\nReferences\nAghighi, H., Azadbakht, M., Ashourloo, D., Shahrabi, H. S., and Radiom, S. (2018).\nMachine learning regression techniques for the silage maize yield prediction using\ntime-series images of landsat 8 oli. IEEE J. Select. Topics Appl. Earth Observ. Remote\nSens. 11 (12), 4563 – 4577. doi: 10.1109/JSTARS.2018.2823361\nClevers, J. G. P. W. (1997). A simpli ﬁed approach for yield prediction of sugar beet\nbased on optical remote sensing data. Remote Sens. Environ. 61 (2), 221 – 228. doi:\n10.1016/S0034-4257(97)00004-7\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,\net al. (2020). An image is worth 16x16 words: transformers for image recognition at\nscale. arXiv, 464. doi: 10.48550/arXiv.2010.11929\nFerentinos, K. P. (2018). Deep learning models for plant disease detection and\ndiagnosis. Comput. Electron. Agric.145, 311 – 318. doi: 10.1016/j.compag.2018.01.009\nGu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., et al. (2018). Recent\nadvances in convolutional neural networks. Pattern recogn.77, 354 – 377. doi: 10.1016/\nj.patcog.2017.10.013\nHassan, M. A., Yang, M., Rasheed, A., Yang, G., Reynolds, M., Xia, X., et al. (2019). A\nrapid monitoring of ndvi across the wheat growth cycle for grain yield prediction using\na multi-spectral uav platform. Plant Sci. 282, 95 – 103. doi: 10.1016/\nj.plantsci.2018.10.022\nHochreiter, S., and Schmidhuber, J. (1997). Long short-term memory. Neural\nComput. 9 (8), 1735 – 1780. doi: 10.1162/neco.1997.9.8.1735\nHuang, Z., Xu, W., and Yu, K. (2015). Bidirectional LSTM-CRF models for sequence\ntagging. arXiv. doi: 10.48550/arXiv.1508.01991\nJin, X., Jie, L., Wang, S., Qi, H. J., and Li, S. W. (2018). Classifying wheat\nhyperspectral pixels of healthy heads and fusarium head blight disease using a deep\nneural network in the wild ﬁeld. Remote Sens.10 (3), 395. doi: 10.3390/rs10030395\nKaul, M., Hill, R. L., and Walthall, C. (2005). Arti ﬁcial neural networks for corn and\nsoybean yield prediction. Agric. Syst.85 (1), 1 – 18. doi: 10.1016/j.agsy.2004.07.009\nKhaki, S., and Wang, L. (2019). Crop yield prediction using deep neural networks.\nFront. Plant Sci.10, 621. doi: 10.3389/fpls.2019.00621\nLi, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y.-X., et al. (2019). Enhancing the\nlocality and breaking the memory bottleneck of transformer on time series forecasting.\nAdv. Neural Inf. Process. Syst.32, 5243 – 5253. doi: 10.48550/arXiv.1907.00235\nMa, J., Du, K., Zheng, F., Zhang, L., Gong, Z., and Sun, Z. (2018). A recognition\nmethod for cucumber diseases using leaf symptom images based on deep convolutional\nneural network. Comput. Electron. Agric. 154, 18 – 24. doi: 10.1016/\nj.compag.2018.08.048\nNassar, L., Okwuchi, I. E., Saad, M., Karray, F., Ponnambalam, K., and Agrawal, P.\n(2020). “Prediction of strawberry yield and farm price utilizing deep learning, ” in 2020\nInternational Joint Conference on Neural Networks (IJCNN). (Glasgow, UK: IEEE). pp.\n1– 7. doi: 10.1109/IJCNN48605.2020.9206998\nNevavuori, P., Narra, N., Linna, P., and Lipping, T. (2020). Crop yield prediction\nusing multitemporal uav data and spatio-temporal deep learning models. Remote Sens.\n12 (23), 4000. doi: 10.3390/rs12234000\nNevavuori, P., Narra, N., and Lipping, T. (2019). Crop yield prediction with deep\nconvolutional neural networks. Comput. Electron. Agric. 163, 104859. doi: 10.1016/\nj.compag.2019.104859\nNewton, I. H., Tariqul Islam, A. F. M., Saiful Islam, A. K. M., Tarekul Islam, G. M.,\nTahsin, A., and Razzaque, S. (2018). Yield prediction model for potato using landsat\ntime series images driven vegetation indices. Remote Sens. Earth Syst. Sci.1 (1), 29 – 38.\ndoi: 10.1007/s41976-018-0006-0\nPantazi, X. E., Moshou, D., Alexandridis, T., Whetton, R. L., and Mouazen, A. M.\n(2016). Wheat yield prediction using machine learning and advanced sensing\ntechniques. Comput. Electron. Agric.121, 57 – 65. doi: 10.1016/j.compag.2015.11.018\nQiao, M., He, X., Cheng, X., Li, P., Luo, H., Zhang, L., et al. (2021). Crop yield\nprediction from multi-spectral, multi- temporal remotely sensed imagery using\nrecurrent 3d convolutional neural networks. Int. J. Appl. Earth Observ. Geoinform.\n102, 102436. doi: 10.1016/j.jag.2021.102436\nRembold, F., Atzberger, C., Savin, I., and Rojas., O. (2013). Using low resolution\nsatellite imagery for yield prediction and yield anomaly detection. Remote Sens.5 (4),\n704– 1733. doi: 10.3390/rs5041704\nSchwalbert, R., Amado, T., Corassa, G., Pott, L. P., Prasad, P. V. V., and Ciampitti, I.\nA. (2020). Satellite-based soybean yield forecast: integrating machine learning and\nweather data for improving crop yield prediction in southern brazil. Agric. For.\nMeteorol. 284, 107886, 491. doi: 10.1016/j.agrformet.2019.107886\nShariﬁ, A. (2021). Yield prediction with machine learning algorithms and satellite\nimages. J. Sci. Food Agric.101 (3), 891 – 896. doi: 10.1002/jsfa.10696\nSimonyan, K., and Zisserman, A. (2014). Very deep convolutional networks for\nlarge-scale image recognition. arXiv. doi: 10.48550/arXiv.1409.1556\nSun, J., Di, L., Sun, Z., Shen, Y., and Lai, Z. (2019). County-level soybean yield\nprediction using deep cnn-lstm model. Sensors 19 (20), 4363. doi: 10.3390/s19204363\nSundermeyer, M., Schluter, R., and Ney, H. (2012). LSTM Neural Networks for\nLanguage Modeling. doi: 10.21437/Interspeech.2012-65\nVarela, S., Pederson, T., Bernacchi, C. J., and Leakey, A. D. B. (2021). Understanding\ngrowth dynamics and yield prediction of sorghum using high temporal resolution uav\nimagery time series and machine learning. Remote Sens. 13 (9), 1763. doi: 10.3390/\nrs13091763\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. in advances in neural information processing systems.\nIn Proceedings of the 31st International Conference on Neural Information Processing\nSystems (NIPS'17) . (Red Hook, NY: Curran Associates Inc.), p 6000 – 6010.\ndoi: 10.48550/arXiv.1706.03762\nWang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., et al. (2019). Learning deep\ntransformer models for machine translation. arXiv. doi: 10.18653/v1/P19-1176\nWu, N., Green, B., Ben, X., and O ’Banion, S. (2020). Deep transformer models for\ntime series forecasting: the in ﬂuenza prevalence case. arXiv\n.d o i : 10.48550/\narXiv.2001.08317\nXie, Y., Zhang, J., Shen, C., and Xia, Y. (2021). CoTr: Ef ﬁciently Bridging CNN and\nTransformer for 3D Medical Image Segmentation. In Medical Image Computing and\nComputer Assisted Intervention –MICCAI 2021: 24th Interna tional Conference ,\nStrasbourg, France, September 27 – October 1, 2021, Proceedings, Part III pp. 171 – 180.\nZhao, Z., Chen, W., Wu, X., Chen, P. C. Y., and Liu, J. (2017). Lstm network: a deep\nlearning approach for short-term traf ﬁc forecast. IET Intel. Trans. Syst.11 (2), 68 – 75.\ndoi: 10.1049/iet-its.2016.0208\nZhou, X., Zheng, H. B., Xu, X. Q., He, J. Y., Ge, X. K., Yao, X., et al. (2017). Predicting\ngrain yield in rice using multi-tempora l vegetation indices from uav-based\nmultispectral and digital imagery. ISPRS J. Photogram. Remote Sens.130, 246 – 255.\ndoi: 10.1016/j.isprsjprs.2017.05.003\nBi et al. 10.3389/fpls.2023.1173036\nFrontiers inPlant Science frontiersin.org11"
}