{
  "title": "Leveraging LLMs for Unsupervised Dense Retriever Ranking",
  "url": "https://openalex.org/W4391673302",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4306438009",
      "name": "Khramtsova, Ekaterina",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A4221487188",
      "name": "Zhuang, Shengyao",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation"
      ]
    },
    {
      "id": "https://openalex.org/A4227659293",
      "name": "Baktashmotlagh, Mahsa",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A4202173991",
      "name": "Zuccon, Guido",
      "affiliations": [
        "University of Queensland"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3196754070",
    "https://openalex.org/W4225575967",
    "https://openalex.org/W4284669679",
    "https://openalex.org/W2034441832",
    "https://openalex.org/W2148972377",
    "https://openalex.org/W2079168273",
    "https://openalex.org/W2051025610",
    "https://openalex.org/W3180332766",
    "https://openalex.org/W4385688511",
    "https://openalex.org/W4385691166",
    "https://openalex.org/W4327644554",
    "https://openalex.org/W4214627684",
    "https://openalex.org/W2068902033",
    "https://openalex.org/W2112021481",
    "https://openalex.org/W4313680149",
    "https://openalex.org/W4400524635",
    "https://openalex.org/W4388955690",
    "https://openalex.org/W2811146799",
    "https://openalex.org/W2032750961",
    "https://openalex.org/W2469056188",
    "https://openalex.org/W4377217558",
    "https://openalex.org/W2895183493",
    "https://openalex.org/W4386576685",
    "https://openalex.org/W4401043313",
    "https://openalex.org/W4389524402",
    "https://openalex.org/W4385573057",
    "https://openalex.org/W2418896762",
    "https://openalex.org/W2087131461",
    "https://openalex.org/W4389523765",
    "https://openalex.org/W1970242704",
    "https://openalex.org/W2021581601",
    "https://openalex.org/W4389921502",
    "https://openalex.org/W2087818911",
    "https://openalex.org/W4389520486",
    "https://openalex.org/W3198691721",
    "https://openalex.org/W4224980447"
  ],
  "abstract": "In this paper we present Large Language Model Assisted Retrieval Model\\nRanking (LARMOR), an effective unsupervised approach that leverages LLMs for\\nselecting which dense retriever to use on a test corpus (target). Dense\\nretriever selection is crucial for many IR applications that rely on using\\ndense retrievers trained on public corpora to encode or search a new, private\\ntarget corpus. This is because when confronted with domain shift, where the\\ndownstream corpora, domains, or tasks of the target corpus differ from the\\ndomain/task the dense retriever was trained on, its performance often drops.\\nFurthermore, when the target corpus is unlabeled, e.g., in a zero-shot\\nscenario, the direct evaluation of the model on the target corpus becomes\\nunfeasible. Unsupervised selection of the most effective pre-trained dense\\nretriever becomes then a crucial challenge. Current methods for dense retriever\\nselection are insufficient in handling scenarios with domain shift. Our\\nproposed solution leverages LLMs to generate pseudo-relevant queries, labels\\nand reference lists based on a set of documents sampled from the target corpus.\\nDense retrievers are then ranked based on their effectiveness on these\\ngenerated pseudo-relevant signals. Notably, our method is the first approach\\nthat relies solely on the target corpus, eliminating the need for both training\\ncorpora and test labels. To evaluate the effectiveness of our method, we\\nconstruct a large pool of state-of-the-art dense retrievers. The proposed\\napproach outperforms existing baselines with respect to both dense retriever\\nselection and ranking. We make our code and results publicly available at\\nhttps://github.com/ielab/larmor/.\\n",
  "full_text": "Leveraging LLMs for Unsupervised Dense Retriever Ranking\nEkaterina Khramtsovaâˆ—\nUniversity of Queensland\nSt Lucia, Australia\ne.khramtsova@uq.edu.au\nShengyao Zhuangâˆ—\nCSIRO\nHerston, Australia\nshengyao.zhuang@csiro.au\nMahsa Baktashmotlagh\nUniversity of Queensland\nSt Lucia, Australia\nm.baktashmotlagh@uq.edu.au\nGuido Zuccon\nUniversity of Queensland\nSt Lucia, Australia\ng.zuccon@uq.edu.au\nABSTRACT\nIn this paper we present Large Language Model Assisted Retrieval\nModel Ranking (LARMOR), an effective unsupervised approach\nthat leverages LLMs for selecting which dense retriever to use on a\ntest corpus (target). Dense retriever selection is crucial for many IR\napplications that rely on using dense retrievers trained on public\ncorpora to encode or search a new, private target corpus. This is\nbecause when confronted with domain shift, where the downstream\ncorpora, domains, or tasks of the target corpus differ from the\ndomain/task the dense retriever was trained on, its performance\noften drops. Furthermore, when the target corpus is unlabeled,\ne.g., in a zero-shot scenario, the direct evaluation of the model\non the target corpus becomes unfeasible. Unsupervised selection\nof the most effective pre-trained dense retriever becomes then a\ncrucial challenge. Current methods for dense retriever selection are\ninsufficient in handling scenarios with domain shift.\nOur proposed solution leverages LLMs to generate pseudo-relevant\nqueries, labels and reference lists based on a set of documents sam-\npled from the target corpus. Dense retrievers are then ranked based\non their effectiveness on these generated pseudo-relevant signals.\nNotably, our method is the first approach that relies solely on the\ntarget corpus, eliminating the need for both training corpora and\ntest labels. To evaluate the effectiveness of our method, we con-\nstruct a large pool of state-of-the-art dense retrievers. The proposed\napproach outperforms existing baselines with respect to both dense\nretriever selection and ranking. We make our code and results\npublicly available at https://github.com/ielab/larmor/.\nCCS CONCEPTS\nâ€¢ Information systems â†’Evaluation of retrieval results .\nKEYWORDS\nModel selection, Dense retrievers, Zero Shot Model Evaluation\nACM Reference Format:\nEkaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, and Guido\nZuccon. 2024. Leveraging LLMs for Unsupervised Dense Retriever Rank-\ning. In Proceedings of the 47th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval (SIGIR â€™24), July 14â€“18,\n2024, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https:\n//doi.org/10.1145/3626772.3657798\nâˆ—Equal Contribution\nPublication rights licensed to ACM. ACM acknowledges that this contribution was\nauthored or co-authored by an employee, contractor or affiliate of a national govern-\nment. As such, the Government retains a nonexclusive, royalty-free right to publish or\nreproduce this article, or to allow others to do so, for Government purposes only.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0431-4/24/07.\nhttps://doi.org/10.1145/3626772.3657798\nFigure 1: nDCG@10 Across Collections. This figure illustrates that\nselecting the best model based on one collection (as indicated by\norange dots) does not necessarily ensure its effectiveness on another.\nIn contrast, our unsupervised approach (indicated by red squares)\nconsistently selects competitive models across various collections,\nand even identifies the most performant model for Trec-News.\n1 INTRODUCTION\nWith the rapid advancements in natural language processing and\ninformation retrieval, a multitude of diverse dense retrievers (DRs)\nhas emerged, demonstrating impressive effectiveness in various text\nretrieval tasks [56, 63]. For example, over 100 dense retrievers are\nfeatured in the Massive Text Embedding Benchmark1 (MTEB [41]).\nA typical situation in practical application settings is that a dense\nretriever trained on one or more corpora (the training corpus ) is\nto be applied to a new corpus (the target corpus ). Often queries\nand relevance judgements (labels) for the target corpus are not\navailable, or they are prohibitive to collect due to costs or data access\nrestrictions (this is often the case in domain specific settings like in\nhealth, legal and patent search). In this situation, then, search engine\npractitioners are faced with the question â€” Which dense retriever\nshould I use? This is the task ofdense retriever selection [28]: identify\nthe most suitable DR for a target corpus. This is a challenging\ntask because no queries and associated relevance judgements are\navailable for the target corpus, and thus the prediction task is to be\nperformed in an unsupervised manner.\nA reasonable choice for dense retriever selection would be to\nselect the DR that performs overall best on a comprehensive leader-\nboard like MTEB. However, recent studies have shown that the\neffectiveness of DRs is often dependent on the similarity between\nthe training corpus and the target corpus; in particular, the effec-\ntiveness becomes varying and unpredictable when DRs are applied\nto data that differs from that at training (e.g., from a new domain,\nsee Figure 1) [32, 35, 47, 54, 69, 70]. This issue is evident for instance\nin the MTEB benchmark. There, results show that the overall top-\nperforming DRs may not necessarily be the most suitable for each\n1https://huggingface.co/spaces/mteb/leaderboard\narXiv:2402.04853v2  [cs.IR]  23 May 2024\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, and Guido Zuccon\nTable 1: nDCG@10 on BEIR. 47 top-performing DRs from METB are used in this experiment. The first row (Oracle) reports the scores obtained\nwhen using the best DR for each collection, representing the upper bound score. The second row (Best DR) reports the scores of UAE, the DR\nthat achieved the best average nDCG@10 on BEIR. The last row (LARMOR) reports the scores achieved using DRs selected by our method.\nNF FiQA ArguAna SciDocs SciFact Covid Quora NQ DBPedia HotpotQASignal1M Robust04 Trec-NewsAvrg\nOracle (Upper Bound)38.65 49.96 65.11 23.77 76.18 84.88 89.26 64.07 44.89 74.33 29.04 54.31 50.77 57.32\nBest DR (UAE) 38.65 44.84 65.11 22.98 74.07 76.33 88.79 55.86 44.89 73.13 27.36 49.55 49.21 54.67\nLARMOR (ours) 36.21 46.89 65.11 22.98 75.41 78.07 88.32 63.49 44.07 67.16 27.76 51.94 50.77 55.24\nsingle collection. For instance, in our experiments with a subset of\nthe DRs from the MTEB benchmark, all-mpnet-base-v22 is the\ntop performing dense retriever on FiQA (nDCG@10 = 0.4996). In\ncontrast, the overall leading dense retriever on the MTEB bench-\nmark is UAE-Large-V13, which on FiQA exhibits a significant 10.3%\nloss in nDCG@10 compared to all-mpnet-base-v2.\nAnother straightforward approach to DR selection would be to\nselect the DR that performs best on a held-out portion of the training\ncorpus. This, has been shown to be the most effective method\nfor selecting DRs in previous work [ 28]. However, a significant\ndifficulty arises when doing this: new state-of-the-art DRs are often\ntrained on multiple, proprietary, corpora, e.g., e5 [58]. This renders\naccess to training and/or held-out data impractical or impossible.\nOther alternatives have been recently explored, adapted from\nsimilar problems in computer vision [28]. These, however, necessi-\ntate the availability of queries from the target corpus. This require-\nment poses a practical challenge in real-world scenarios, where the\ndecision on which DR model to use must be made prior to deploying\nthe application and thus often no prior logged queries are available.\nNevertheless, even if logged queries are available, these approaches\nhave been shown largely ineffective for DR selection [28].\nIn this paper, we propose a family of approaches for unsuper-\nvised, query-free dense retriever selection. At the core of these\napproaches is the leveraging of the capabilities of Large Language\nModels (LLMs) [64]. Specifically, we address the challenge posed\nby the absence of queries by using LLMs to generate synthetic\nqueries for a (subset of the) target corpus. A document for which\na synthetic query is generated and the generated query itself are\nconsidered forming a pseudo-relevant query-document pair for the\ntarget corpus. The set of pseudo-relevant query-document pairs\nare then used to estimate the ranking effectiveness of the DRs on\nthe target corpus, and in turn this is used to rank DR systems.\nOur results demonstrate that this straightforward performance\nestimation based on query generation is remarkably effective in\nselecting the most suitable DR for a target corpus â€“ outperforming\nany other DR selection method. We further propose refinements\nto this idea that encompass the generation of synthetic relevance\nlabels, and the exploitation of synthetic reference lists. The combi-\nnation of these methods leads to a highly effective unsupervised\nstrategy for dense retriever selection, which we refer to as Large\nLanguage Model Assisted Retrieval Model Ranking (LARMOR).\nTable 1 provides a snapshot of LARMORâ€™s predictive capabilities\nwhen selecting DRs for a target corpus: this serves as a motivation\nto delve further into the remainder of the paper. Each column in\nthe table represents a target corpus (the last column is the mean\neffectiveness), and the value reported is the effectiveness on the\n2https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n3https://huggingface.co/WhereIsAI/UAE-Large-V1\ntarget corpus of the selected dense retriever (fine-tuned on a differ-\nent training corpus); Section 5 details our empirical settings. The\nfirst row (Oracle) refers to the best performance attainable if the\neffectiveness of every dense retriever on each of the target corpora\nwere known â€“ this is a theoretical upper bound. The second row\nreports the performance attainable when selecting the single model\nthat performs overall best across all considered target corpora; in\nthe case of the table, such a model is UAE. Again, this method is\nimpossible in practice as it requires the true relevance labels for\neach target corpus to determine the overall best DR. Finally, the\nthird row reports the remarkable performance of our LARMOR:\nthese were obtained without resorting to human annotations, nor\naccess to queries from the target corpus, which are often unfeasible\nto obtain before deployment. LARMOR in fact manages to select a\nhighly competitive DR for each of the target corpora, and overall\nLARMOR provides better DR selection than using the UAE model\nacross all target corpora â€“ recall than UAE could have been selected\nonly because we accessed the relevance labels of each target cor-\npus. In addition, LARMOR performance is only 3.6% less than the\ntheoretical upper bound (Oracle).\nWhile our primary focus is on the DRs, our method can be applied\nto other IR models (e.g. re-rankers or sparse models). LARMOR is\nnow integrated in theDenseQuest [27] system that implements DR\nselection over custom collections (https://densequest.ielab.io)\nKey contributions:\n(1) We introduce Large Language Model Assisted Retrieval Model\nRanking (LARMOR), an approach for dense retriever selection\nthat exploits the zero-shot capability of LLMs for generation of\nqueries, relevance judgments, and reference lists. LARMOR is\nhighly effective in selecting a dense retriever for a target corpus,\nwithout the need to supply queries or labels from the target corpus.\n(2) To assess LARMORâ€™s performance, we assemble a pool of 47 top\nDRs from the MTEB retrieval benchmark, extending the results of\nprevious work to considering a consistently larger set of models.\n(3) We conduct a thorough ablation study to examine factors that\nimpact LARMORâ€™s effectiveness, including the type and size of\nLLMs used, and the number of generated queries per documents.\n(4) We augment the set of baselines for dense retriever selection by\nevaluating existing query performance prediction (QPP) methods\noverlooked by previous work [28].\n2 RELATED WORK\n2.1 Dense Retrievers Selection\nWhile the task of unsupervised model selection has been widely\nstudied for general deep learning models [8, 14, 19, 21, 29], its appli-\ncation in the context of neural rankers remains largely unexplored.\nThe recent study by Khramtsova et al. [28] formalized the problem\nof DR selection and proposed several baseline methods. However,\nLeveraging LLMs for Unsupervised Dense Retriever Ranking SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\ntheir results indicated that most methods adapted from other areas\nare ineffective for IR. Differing from [28], we introduce a more chal-\nlenging experimental setup by expanding the number of DRs and\nadding an additional constraint: the models might have different\ntraining sets. Consequently, we had to exclude several baselines,\nnamely query similarity and FrÃ©chet-based corpus similarity, as\nthey require access to the training data. We retain the other base-\nlines for our comparisonâ€”MSMARCO perf, Binary entropy, and\nQuery Alterationâ€”using the best-reported hyperparameters.\n2.2 Query Performance Prediction\nThe concept of performance estimation in IR is primarily investi-\ngated within the context of query performance prediction (QPP) [7,\n22, 23]. QPP aims to predict the retrieval quality of a search sys-\ntem for each query independently, without relying on relevance\njudgments. Our paperâ€™s objective differs slightly, focusing on com-\nparing performance across different rankers, rather than evaluating\neach query within a single ranker. Nonetheless, it seems logical to\nexplore the adaptation of QPPs to the task of DR selection.\nTraditional QPP methods fall into three main categories [ 49]:\nthose that assess the clarity of search results relative to the over-\nall corpus; those that analyze the retrieval scores of documents\nwithin the ranking lists; and those that evaluate the robustness\nof the predicted ranking. We adapt methods from each of these\ncategories to the DR selection task. From the first category, we em-\nploy Clarity [11]; from the second, we utilize WIG [65], NQC [50],\nSMV [53], and ğœ [12, 43]; and from the third category, we explore\nFusion [49]. Additionally,Query Alteration [28] can also be con-\nsidered as an adaptation of robustness-based QPP. Our LARMOR\nmethod bears similarities with the concurrently proposed QPP-\nGenRE [39]; however QPP-GenRE is intended for the QPP task and\nit only considers producing synthetic labels, not queries.\n2.3 Challenges For The Existing Baselines\nNext we discuss the challenges encountered in adapting existing\nmethods to our task of unsupervised DR ranking and selection.\n2.3.1 Normalizing factors for score-based methods. Score-based\nQPP methods can be significantly enhanced by scaling their respec-\ntive query-based measures with the relevance of the entire corpus\nto that query [31, 49]. In the context of DRs, this scaling is akin to\nthe score between the query and the entire target corpus; however,\ncalculating this value is computationally unfeasible [17]. Several\nmethods have been proposed to approximate this scaling parame-\nter. One approach, as suggested by Meng et.al. [38], is to take the\naverage score of the top retrieved documents for each query as the\nnormalizing factor. Another method, defined by Faggioli et.al. [17],\ninvolves representing the entire corpus as a centroid of its docu-\nments, derived from the latent space of the DR. The normalizing\nfactor is then calculated as the score between the query and this\ncentroid representation. We followed [38] in our experiments, and\nreport both variations with and without normalization.\nTypically, the scores generated for a query-document pair do not\naccurately reflect the probability of the query being relevant to the\ndocument. For example, in models trained using the dot product,\nthe scores are not bound to a range between 0 and 1.\nIn addition to the variability of the scores within one collection,\nanother challenge is the variability of the score distributions across\nDRs. This variability arises due to each dense retriever having\nunique architecture, loss function, and other training hyperparame-\nters. As a result, even by normalizing the scores within one corpus,\nscore-based methods do not perform well when used for comparing\ndifferent dense retrievers, as will be shown in the next section.\n2.3.2 Variability of pre-trained tasks and training collections for\nperformance-based methods. DRs are seldom trained from scratch;\nrather, it is common to start with a model pre-trained on a different\ntask and fine-tune it for a retrieval task. Consequently, it is logical to\nuse the performance on the original task and the new retrieval task\nas indicators of model generalizability. For instance, if a model was\npre-trained on a masked language modeling task (such as BERT [15]\nand Roberta [33]), one could evaluate the modelâ€™s adaptability to a\nnew task by examining its robustness to masking, as demonstrated\nin the Query Alteration Method. Similarly, if a model was fine-\ntuned on MSMARCO, its performance on MSMARCO can serve as\nan indicator of its ability to perform a retrieval task.\nHowever, a challenge emerges because the dense retrievers in our\nstudy are not pre-trained using the same task. For instance, while\nsome models are based on BERT, others are built on GPT [6] and\nwere initially pre-trained using Next Token Prediction. Additionally,\nthe training corpora differ across models. This diversity results\nin a performance evaluation that may be biased towards models\ntrained on specific tasks or corpora, potentially not reflecting true\nperformance on the target corpus or task.\n2.4 LLMs for Information Retrieval\nIn our research, we rely on LLMs to generate pseudo-relevant\nqueries, labels, and reference lists in the zero-shot setting. Nu-\nmerous papers in the NLP and IR literature demonstrated the re-\nmarkable zero-shot performance of LLMs in these tasks.\nFor query generation, previous methods focused on fine-tuning\npre-trained language models to generate pseudo-relevant queries [20,\n42]. More recently, many works demonstrated that it is possible to\ngenerate high-quality queries for training ranking models [5, 13,\n25, 57] by prompting LLMs in a zero-shot manner. Hence, follow-\ning these works, we also use a domain-specific query generation\nprompt to guide LLMs in generating pseudo-relevant queries.\nAnother important component in our pipelines is the generation\nof pseudo-relevant judgments for the generated queries. Thomas\net al. [55] demonstrate that using GPT-4 to generate relevant judg-\nments can surpass human annotators in evaluating search systems.\nSimilarly, a study conducted by Faggioli et al. [16] shows that LLM-\ngenerated relevant labels can align with labels annotated by TREC\nannotators. Thus, in this work, we leverage LLMs to provide addi-\ntional relevance judgments in addition to the generated queries.\nFinally, our work also leverages a pseudo-reference list to eval-\nuate the effectiveness of DRs. This involves generating document\nrankings with high ranking scores (e.g., high nDCG). In IR, nu-\nmerous works have demonstrated that LLMs have very strong\nzero-shot ranking capabilities. The methodologies for harnessing\nLLMs in zero-shot ranking tasks can be broadly categorized into\npointwise [48, 66, 67], listwise [36, 44, 45, 51, 52], pairwise [46], and\nsetwise [68]. In our paper, we adopt a setwise approach to generate\na pseudo-reference list due to its high effectiveness and efficiency.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, and Guido Zuccon\nFigure 2: The LARMOR dense retriever selection pipeline. Labels Q, QF, QFJ and QFR refer to the ablation points described in Section 6.2.\n3 PROBLEM FORMULATION\nLet Tbe a target collection containing a corpus ğ·Tof documents,\na set ğ‘„Tof queries, and a set Jğ‘„T,ğ·T of relevance judgments (i.e.,\nlabels), which reflect the degree of relevance of a given document\nğ‘‘ âˆˆğ·Tin relation to a specific query ğ‘ âˆˆğ‘„T.\nWe note that accessing such relevance judgments presents sig-\nnificant challenges: queries often contain private user informa-\ntion; while collecting high-quality relevance judgments is time-\nconsuming, and for some applications (e.g., medical or legal IR)\nrequires in-depth domain knowledge and thus can be costly.\nLet R= {ğ‘…1,ğ‘…2,...ğ‘… ğ‘›}be a set of rankers, each trained on its\nrespective training collection4: S= {Sğ‘…1 ,Sğ‘…2 ,..., Sğ‘…ğ‘›}. Note that\nthe target collection Twas not used during training of R: TâŠˆ S.\nIn current applications, the training collections Sğ‘…ğ‘– often contain\nlarge corpora, which often include private documents â€“ e.g., those\nused to train the latest state-of-the-art dense retrievers like e5 [58].\nTherefore, we operate under the assumption that only the trained\ndense retrievers in Rare available, while access to Sis restricted.\nFinally, let Ebe an evaluation measure, such as nDCG@10. Prac-\ntitioners can establish an ordering of DRs in Rbased on the value\nof the evaluation measure Eobtained on the target collection. This\nis achieved by applying each DR to the target collection and using\nthe relevance judgments to compute the evaluation measure. The\nrankers are then arranged in decreasing order of E, creating the\nranking (ordering) of DRs O(R,T,E,J). The top-ranked DR is\nthen typically selected for deployment as a search function on the\ntarget corpus of documents ğ·T to answer new queries, as it is\nthe one that has been found performing best on the target collec-\ntion T. We note that we consider O(R,T,E,J)to be the ground\ntruth ranking for the dense retriever selection task, defined below,\nsince the DRs are evaluated and ranked based on the ground truth\nrelevance judgments.\nDense retriever selection task : The problem of dense retriever\nselection consists of predicting the ranking O(R,T,E,J)without\naccessing the relevance judgments Jğ‘„T,ğ·T, as well as the target\nqueries ğ‘„T. This is equivalent to producing a ranking Ë†ğ‘‚(R,ğ·T,E)\nof the rankers in Rfor the target collection Tand evaluation mea-\nsure E, such that Ë†ğ‘‚(R,ğ·T,E)corresponds to the true ranking\nO(R,T,E,J). Note that Ë†ğ‘‚(R,ğ·T,E)does not include the rele-\nvance assessments Jğ‘„T,ğ·T and target queries ğ‘„T as input. Our\ngoal is to develop a dense retriever selection method ğ‘€(R,ğ·T)\nthat produces the ranking Ë†ğ‘‚(R,ğ·T,E).\n4Multiple training collections could also be used to derive a dense retriever: this does\nnot change the setup discussed here.\n4 METHODOLOGY\nNext we describe our method, Large Language Model Assisted Re-\ntrieval Model Ranking (LARMOR). LARMOR uses Large Language\nModels (LLMs) to tackle the dense retriever selection problem. The\nmethod encapsulates a pipeline consisting on three crucial com-\nponents, as outlined in Figure 2: (1) pseudo-query generation, (2)\nconstruction of pseudo-relevant judgments and reference lists, and\n(3) dense retrievers ranking.\n4.1 Pseudo-Query Generation\nThe first step of the LARMOR pipeline involves generating pseudo-\nqueries for the target corpus ğ·Tto address the challenge of the\nabsence (i.e., inability to access) of a representative set of queries\nfor the target collection, ğ‘„T.\nFor this, we start by randomly sampling a subset ofğ‘˜documents\nfrom the target corpus: ğ·â€²\nT = {ğ‘‘1,.,ğ‘‘ ğ‘˜}âˆˆ ğ·T. Once the subset\nğ·â€²\nTis built, each document from ğ·â€²\nTis passed to LARMORâ€™s LLM,\naccompanied by a domain-specific query generation promptğ‘ƒğ‘ğ‘”, to\ngenerate a set of pseudo-relevant queries eğ‘„Tspecific to the target\ncollection5. Note that for each sampled document ğ‘‘ğ‘–, we generate ğ‘™\nqueries ( Ëœğ‘ğ‘‘ğ‘–,1,... Ëœğ‘ğ‘‘ğ‘–,ğ‘™):\neğ‘„T=\nğ‘˜Ã˜\nğ‘–=1\nğ‘™Ã–\nğ‘—=1\nğ¿ğ¿ğ‘€(ğ‘ƒğ‘ğ‘”(ğ‘‘ğ‘–)). (1)\nin the equation, Ã\nğ‘™ symbolises the generation of ğ‘™ queries for a\nsingle document ğ‘‘ğ‘–, each time using the same prompt template ğ‘ƒğ‘ğ‘”\nspecific to the domain of the target collection.\nFor instance, in the case of the target domain being Wikipedia\n(e.g., for the NQ corpus in BEIR), the query generation prompt\nwe employ is â€œGenerate a question that the following Wikipedia\npage can answer. Avoid generating general questions. Wikipedia page:\n{ğ‘‘ğ‘–}â€, where ğ‘‘ğ‘– is a placeholder for the sample document text. By\ninputting this prompt to the LLM, it is possible to generate in-\ndomain queries for the collection, thus addressing the challenge\nof the unavailability of a target query set ğ‘„T. This prompt design\nrequires only minimal prior knowledge about the collection domain,\neffectively mimicking real-world scenarios. The design of prompts\nspecific to a target collection has been shown effective in previous\nwork in the context of training dense retrievers [3, 58, 62]. To the\nbest of our knowledge, we are the first to design and use domain-\nspecific prompts for query generation.\n5That is, we assume that it is known what the representative search tasks for the target\ncollection are.\nLeveraging LLMs for Unsupervised Dense Retriever Ranking SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nIt is worth noting that generating multiple pseudo-relevant\nqueries for each sample document 6 using sampling generation\nstrategies, such as top-p sampling [ 24], is reasonable. Since one\ndocument is likely to cover different topics, it could be relevant to\nvarious queries. Generating multiple queries with sampling strate-\ngies has the potential to cover different aspects of the document.\n4.2 Construction of Pseudo Relevance\nJudgments and Reference Lists\nOnce eğ‘„T is obtained, we could construct pseudo-relevant judg-\nments to evaluate dense retrievers in the candidate pool by assum-\ning a document is relevant to its corresponding generated queries.\nHowever, such an approach can only provide one relevant docu-\nment per generated query, and these shallow relevance judgments\nmay be sub-optimal for evaluating the ranking effectiveness of a\nDR [2, 34, 37, 40]. Consequently, the next step in the LARMOR\npipeline is to construct comprehensive pseudo-relevant signals\nto evaluate and rank DRs in the candidate pool. We propose two\ndifferent types of pseudo-relevant signals for this purpose: pseudo-\nrelevant judgments and pseudo-reference lists, each with a distinct\nway of prompting LLMs and evaluating DRs.\nFor both pseudo-relevant signals types we start by creating a\nsingle document ranking for each generated query Ëœğ‘ğ‘‘ğ‘–,ğ‘— âˆˆeğ‘„T,ğ‘‘ğ‘–,\nwhere eğ‘„T,ğ‘‘ğ‘– is the subset of eğ‘„T that contains the ğ‘™ generated\nqueries for sampled document ğ‘‘ğ‘–. The single document ranking for\nthe generate query Ëœğ‘ğ‘‘ğ‘–,ğ‘— is achieved by submitting the query to all\nconsidered DRs inR= {ğ‘…1,ğ‘…2,...,ğ‘… ğ‘›}, to obtain the document rank-\nings {ğ·Ëœğ‘ğ‘‘ğ‘–,ğ‘—,ğ‘…1 ,ğ·Ëœğ‘ğ‘‘ğ‘–,ğ‘—,ğ‘…2 ,..., ğ·Ëœğ‘ğ‘‘ğ‘–,ğ‘—,ğ‘…ğ‘›}. Here, ğ·Ëœğ‘ğ‘‘ğ‘–,ğ‘—,ğ‘…1 is the ranking\nof documents in the corpus ğ·Tinduced by dense retriever ğ‘…1 for\nquery Ëœğ‘ğ‘‘ğ‘–,ğ‘—. While the notation assumes that any ğ·Ëœğ‘ğ‘‘ğ‘–,ğ‘—,ğ‘… is a total\nordering of the documents in ğ·T, in practice retrieval is conducted\nup to a rank cut-off (typically 1,000): the cut-off value has little\neffect on the result of the fusion, provided it is large enough (i.e.\nlarger than parameter ğ‘šbelow, the number of documents selected\nfrom the fused ranking). Subsequently, a rank fusion algorithm7 is\nemployed to merge all the rankings for Ëœğ‘ğ‘‘ğ‘–,ğ‘—, resulting in a single\nfused document ranking from which we select only the top-ğ‘šdoc-\numents, obtaining the document ranking ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘— of size ğ‘š. One can\nconsider this step as selecting the most valuable ğ‘šdocuments for\nquery Ëœğ‘ğ‘‘ğ‘–,ğ‘— for the LLM to judge or rank: the documents in ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘—\nare likely retrieved by most of the DRs. Hence, providing relevance\njudgments or reference lists for these documents may yield a more\naccurate evaluation of the DR effectiveness.\nNext, ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘— is passed as input to the LLM using either (or both)\nof two prompts (if both, this is done separately, i.e. independent\ninferences for each prompt).\nPrompt ğ‘ƒğ‘—ğ‘¢ğ‘‘ğ‘” instructs the LLM to generate the pseudo-relevance\njudgments8 eJËœğ‘ğ‘‘ğ‘–,ğ‘—,ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘—\nfor the m documents in ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘—:\neJËœğ‘ğ‘‘ğ‘–,ğ‘—,ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘—\n=\nğ‘šÃ˜\nğ‘=1\nğ¿ğ¿ğ‘€(ğ‘ƒğ‘—ğ‘¢ğ‘‘ğ‘”(Ëœğ‘ğ‘‘ğ‘–,ğ‘—, Ëœğ‘‘ğ‘)). (2)\n6Recall that we generate ğ‘™queries for each sample document.\n7Which rank fusion method to use is an implementation choice; many exist [30].\n8Sometimes referred to as synthetic judgements.\nwhere Ëœğ‘‘ğ‘ is a document in the fused ranking ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘—. We then can\ncollate9 pseudo-relevance judgements across all queries for a sample\ndocument, and all sample documents for the target corpus ğ·T,\nobtaining the relevance judgements set eJT(or eJfor brevity of\nnotation, since we consider only one target collection T).\nTo â€œimplementâ€ğ‘ƒğ‘—ğ‘¢ğ‘‘ğ‘” we adapt the fine-grained relevance labels\ngeneration prompt of Zhuang et al. [66] because of its previously\nreported high effectiveness10. As an example, our relevance judg-\nment prompt for the NQ collection is â€œFor the following query and\ndocument, judge whether they are â€˜Highly Relevantâ€™, â€˜Somewhat Rel-\nevantâ€™, or â€˜Not Relevantâ€™. Query: {Ëœğ‘ğ‘—}Document: {Ëœğ‘‘ğ‘}\". Following\nZhuang et al. [66], we only consider Ëœğ‘‘ğ‘ to be relevant to Ëœğ‘ğ‘— if the\nLLM generates â€˜Highly Relevantâ€™ , i.e., we convert the graded judge-\nment to binary judgments. Finally eJwill be used for ranking DRs,\nwhich we discuss in details in the next section.\nPrompt ğ‘ƒğ‘—ğ‘¢ğ‘‘ğ‘” guides the LLM to generate relevance judgments at\na document level. In addition, we propose a second prompt, ğ‘ƒğ‘Ÿğ‘ğ‘›ğ‘˜,\nto evaluate DRs at the ranking level. ğ‘ƒğ‘Ÿğ‘ğ‘›ğ‘˜ is designed to instruct\nthe LLM to generate a highly effective document ranking ğ¿Ëœğ‘ğ‘‘ğ‘–,ğ‘— to\nbe used as pseudo-reference list for the generated query Ëœğ‘ğ‘‘ğ‘–,ğ‘—. A\nreference list is commonly used in QPP [49], giving rise to effective\npredictive methods in that context. In our work, we adapt this idea\nto enhance our approach. This is achieved by prompting the LLM\nto re-rank the ğ‘šdocuments in the fused ranking for query Ëœğ‘ğ‘‘ğ‘–,ğ‘—,\ni.e., ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘—: ğ¿Ëœğ‘ğ‘‘ğ‘–,ğ‘— = ğ¿ğ¿ğ‘€(ğ‘ƒğ‘Ÿğ‘ğ‘›ğ‘˜(ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘— ) (3)\nTo â€œimplementâ€ ğ‘ƒğ‘Ÿğ‘ğ‘›ğ‘˜ we used the Setwise document ranking\nprompt [68]. We note other LLM ranking prompts could have been\nused, e.g., Pointwise [48, 66, 67], Listwise [36, 44, 45, 51, 52], Pair-\nwise [46]. We chose the Setwise prompt because of its robustness\nand high effectiveness; due to space and computation constraints,\nwe leave the study of other prompts for implementing ğ‘ƒğ‘Ÿğ‘ğ‘›ğ‘˜ and\ntheir impact on LARMOR to future work.\nFinally, we collate all reference lists into a set of reference listeLT\n(or for simplicity of notation, eL) for the target corpus T, keeping\ntrack of which generated query each list refers to.\n4.3 Dense Retriever Ranking\nThe final step in our LARMOR pipeline is ranking all the dense\nretrievers in the candidate pool using either or both of the generated\npseudo-relevance judgments eJand the pseudo-reference lists eL.\nTo rank DRs for a target collection with the pseudo-relevance\njudgements, we first produce document rankings in answer to the\ngenerated queries using all the DRs, and we then evaluate each\nof these rankings using the target evaluation measure E(in our\nempirical evaluation, E= nDCG@10). We then average the eval-\nuation values across all queries to associate an estimated average\nevaluation measure eEto each of the dense retrievers (it is estimated\nbecause pseudo queries and judgements are used, in place of the\nreal ones from the target collection). Subsequently we rank DRs in\ndescending order of eE.\n9Via the set union operation.\n10We note other prompts for query-document relevance judgements have been pro-\nposed, e.g., that of Thomas et al. [55]; we adapted the one of Zhuang et al. [66] over\nothers because of its simplicity. We leave evaluating the impact on LARMOR of alter-\nnative prompts for relevance evaluation to future work.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, and Guido Zuccon\nTo rank DRs for a target collection with the reference lists eL, we\ncalculate the average Rank Bias Overlap (RBO) [59] of the rankings\nobtained using each DR for each generated query Ëœğ‘ğ‘‘ğ‘–,ğ‘— against its\ncorresponding pseudo-reference list ğ¿Ëœğ‘ğ‘‘ğ‘–,ğ‘—. We then rank DRs in\ndescending order of RBO values.\nFinally, as the above two rankings of dense retrievers have been\nobtained leveraging different relevance signals (judgements vs.\nreference lists), we posit it is beneficial to combine these rankings\nto obtain a comprehensive and effective ordering of the DRs, from\nthe one thought to be most effective on the target corpus to the\nleast effective. Thus, we further employ a fusion algorithm to merge\nthe rankings of dense retrievers11 and create our final ranking of\nDRs used as solution to the dense retrievers selection task.\n5 EXPERIMENTAL SETUP\nWe aim to comprehensively evaluate our proposed DR selection\napproach against a large pool of state-of-the-art DRs across a wide\nrange of corpora from different domains. In this section, we outline\nthe details of the criteria for selecting DRs from the MTEB leader-\nboard, along with the corpora used in our experiments. Finally, we\nprovide the implementation and evaluation details of our approach.\n5.1 Dense Retriever Pool and Target Corpora\nWe assembled a large collection of state-of-the-art dense retrievers\nthrough the following steps:\nWe began by examining the MTEB leaderboard, selecting the\ntop 30 retrievers based on their average performance across all\ncorpora featured in the MTEB benchmark. Next, we assessed the\nperformance of the retrievers on each individual corpus, expanding\nour set to include any retrievers that ranked in the top 30 for a\nspecific corpus but were not part of our initial overall selection.\nThis approach naturally led to overlapping models, as those most\neffective on one corpus often performed well on others. However,\ncertain models demonstrated unique strengths in specific corpora.\nFor instance, the all-mpnet-base-v2 model is ranked the best for\nboth SciDocs and FiQa corpora, yet it is 48th overall. Following\nthe selection of leading models for each corpus, we refined our\npool to align with our budgetary and computational constraints.\nThis entailed removing API-based retrievers, e.g. Cohere, and any\nmodels with more than 6B parameters.\nThis process ultimately resulted in a carefully curated pool of\n47 state-of-the-art dense retrieval models, which we will be using\nfor all the experiments throughout the paper. Note that the num-\nber of models in our study substantially exceeds those utilized by\nKhramtsova et al. [28], thereby increasing the complexity of the\ntask and enhancing the credibility of our results.\nFor evaluation corpora, we follow Khramtsova et al. [28] who\nutilized the corpora from the BEIR benchmark [54], which is widely\nemployed for zero-shot dense retriever evaluation. This benchmark\nincludes 18 collections across 9 diverse tasks. In accordance with\nstandard practice, we selected a representative subset of 13 corpora,\ncovering all 9 tasks featured in BEIR. The primary advantage of\nemploying this benchmark is that none of its corpora were explicitly\nused for training the dense retrievers in our model pool. This makes\n11We further stress that in this step we fuse rankings of DRs, and not of documents\nlike when creating reference lists in Section 4.2.\nit an appropriate choice for an unsupervised model selection task,\nespecially in scenarios with domain shift between training and test.\n5.2 Implementation Details\nWe employ LLMs in our proposed DR selection pipeline to gen-\nerate queries, pseudo-relevance judgments, and pseudo-reference\nlists. We consistently use the FLAN-T5 [9] LLM through out the\npipeline since it demonstrated strong effectiveness in zero-shot\nquery generation [13] and document ranking [46, 67, 68].\nFor the query generation component, inspired by recent works\nof task-aware dense retriever training [3, 58, 62], we adapted their\nprompts to the task of query generation to generate in-domain\npseudo-relevant queries for each BEIR corpus. Specifically, our\nquery generation prompt templates have two key pieces of knowl-\nedge related to the target domain. Firstly, we specify the type of\ntarget query to generate, such as questions for question answering\nor arguments for argument retrieval. Secondly, we identify the type\nof document, distinguishing between sources like Wikipedia pages\nand scientific titles with abstracts. Due to space constraints, we\ndirect readers to refer to our github repository for the list of our\nquery generation prompts12 and the resulting generated queries for\neach corpus 13. We randomly sample ğ‘˜ = 100 documents from the\ntarget corpus and employ top-p sampling with ğ‘ = 0.9 to generate\n10 queries for each sampled document, resulting in |eğ‘„T|= 1000\ngenerated queries per corpus. In Section 6.4, we investigate the\nimpact of the number of generated queries per document as well as\nthe influence of using different backbone LLMs in query generation.\nRegarding the prompt for generating pseudo-relevance judg-\nments, we modify the fine-grained relevance label generation prompts\n[66] to align with our domain-specific query generation prompts.\nThis involves incorporating information about the query type and\ndocument type into the prompts. We then use the prompt to instruct\nLLMs to judge the top ğ‘š= 100 documents from the ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘— ranking\nfor each generated query. We again refer readers to our github\nrepository for the details of our prompts. As for the generation of\npseudo-reference lists ğ¿Ëœğ‘ğ‘‘ğ‘–,ğ‘—, we simply employ the original Setwise\nranking prompt proposed by Zhuang et al . [68] with the default\nsetting of using heap sort algorithm and compare 3 documents at a\ntime to re-rank the top ğ‘š= 100 documents from the ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘—.\nFor the fusion algorithm used in LARMOR to createğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘— and the\nfinal DR ranking, considering that the scores provided by different\nDRs might have different scales, we opt for Reciprocal Rank Fusion\n(RRF)[10], a position-based method. We employ the implementation\nfrom the Python toolkit ranx[4] with the default parameters.\n5.3 Evaluation\nFor evaluating our proposed LARMOR and baselines, we follow\nprevious work that uses Kendall Tau correlation and Î”ğ‘’ to assess\nthe performance of methods on the DR selection task.\nBoth evaluations require the DRâ€™s ground truth performance\nranking on the target corpus. Therefore, for each of the considered\ncorpora, we rank the DRs based on the nDCG@10 obtained from\nthe test queries and human judgments provided by each collection.\nThis score is the official evaluation measure for BEIR.\n12https://github.com/ielab/larmor/blob/main/prompts.py\n13https://github.com/ielab/larmor/tree/main/generated_data/\nLeveraging LLMs for Unsupervised Dense Retriever Ranking SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nTable 2: Kendall Tau Correlation value, calculated based on nDCG@10.\nNF FiQA ArguAna SciDocs SciFact Covid Quora NQ DBPedia HotpotQASignal1M Robust04 Trec-News Avrg\nMSMARCO perf. 0.337 0.240 0.17 0.118 0.291 0.339 0.298 0.646 0.515 0.492 0.121 0.141 0.186 0.300\nBinary Entropy-0.056 -0.164 -0.048 -0.086 0.103 -0.183 -0.280 -0.081 0.034 0.165 0.069 -0.106 -0.212 -0.065\nQuery Alteration-0.277 -0.250 -0.173 -0.242 -0.199 -0.152 -0.195 -0.458 -0.359 -0.217 0.029 -0.102 -0.194 -0.215\nWIG -0.156 -0.212 -0.093 -0.212 -0.188 -0.231 -0.201 -0.398 -0.262 -0.149 0.112 -0.103 -0.191 -0.176\nWIG Norm -0.027 -0.125 -0.138 -0.105 -0.001 -0.118 -0.147 0.078 0.053 -0.049 0.010 -0.106 -0.055 -0.056\nNQC -0.036 -0.021 0.142 0.08 -0.01 -0.202 -0.071 -0.304 -0.153 -0.086 0.036 0.051 -0.006 -0.045\nNQCNorm -0.136 -0.191 -0.06 -0.121 -0.136 -0.198 -0.197 -0.441 -0.262 -0.154 0.099 -0.080 -0.160 -0.157\nSMV -0.056 -0.012 0.143 0.056 0.010 -0.198 -0.066 -0.289 -0.162 -0.110 0.036 0.047 0.005 -0.046\nSMV Norm -0.173 -0.179 -0.066 -0.136 -0.127 -0.204 -0.191 -0.429 -0.280 -0.197 0.103 -0.075 -0.154 -0.162\nğœ -0.204 -0.228 -0.116 -0.149 -0.186 -0.198 -0.142 -0.402 -0.260 -0.219 0.084 -0.099 -0.167 -0.176\nğœğ‘šğ‘ğ‘¥ -0.147 -0.236 -0.123 -0.114 -0.182 -0.224 -0.236 -0.370 -0.291 -0.166 0.062 -0.064 -0.227 -0.178\nClarity 0.114 0.245 0.223 0.038 0.264 -0.333 0.345 0.059 -0.186 -0.145 0.203 0.08 -0.193 0.055\nFusion 0.653 0.436 0.544 0.686 0.636 0.368 0.670 0.374 0.719 0.555 0.506 0.611 0.698 0.574\nLARMOR (ours) 0.700 0.618 0.627 0.739 0.766 0.553 0.740 0.563 0.665 0.710 0.380 0.444 0.690 0.630\nAfter obtaining the ground truth DR performance ranking, Kendall\nTau correlation is used to assess the similarity between the rank-\nings generated by the DR selection methods and the ground truth\nranking. Specifically, Kendall Tau correlation measures the pro-\nportion of document pairs that are ranked in the same order by\nboth rankings. A score of 1 indicates perfect positive correlation,\n-1 indicates perfect negative correlation, and 0 indicates completely\nrandom correlation.\nOn the other hand, Î”ğ‘’ aims to measure the performance gap\nbetween the selected DR and the ground truth best-performing DR\nfor a specific DR evaluation measure ğ‘’. This is defined as:\nÎ”ğ‘’ = ğ‘’(ğ‘€(R))âˆ’ ğ‘’(ğ‘…âˆ—) (4)\nwhere ğ‘…âˆ—is the ground truth best DR, and ğ‘€(R)is the DR ranked\nat top by method ğ‘€. In our experiments, we set ğ‘’ to be nDCG@10\nto align with the target DR performance measurement. If Î”ğ‘’ = 0, it\nmeans that method ğ‘€ successfully ranked the best DR at the top.\n5.4 Other Baselines\nWe briefly describe the baselines used for comparison.\nModel Selection Methods [28] :\nâ€¢MSMARCO performance: ranks models based on their performance\non MSMARCO - a large widely-used public collection for fine-\ntuning and evaluating retrieval models.\nâ€¢Binary Entropy evaluates model uncertainty. For each query, the\nentropy of the probability-at-rank distribution is calculated. DRs\nare then ranked based on the average entropy across all queries.\nâ€¢Query Alteration assesses the sensitivity of DRs to query variations.\nIt involves modifying the query and measuring the standard devi-\nation of scores for the retrieved documents. DRs are ranked based\non the average standard deviation across queries, with smaller\nvalues indicating greater robustness against query perturbation,\nthereby implying higher retrieval credibility.\nQPP-based methods:\nâ€¢Weighted Information Gain (WIG) [65] measures the disparity\nbetween the average retrieval scores of top-ranked documents\nand the overall score of the corpus.\nâ€¢Normalized Query Commitment (NQC) [50] calculates the stan-\ndard deviation of the scores of top-ranked documents.\nâ€¢Scores Magnitude and Variance (SMV) [ 53] combines both the\nmagnitude and the standard deviation of the scores of top-ranked\ndocuments.\nâ€¢Clarity [ 11] measures the discrepancy between the language\nmodel built from the top retrieved results and the language model\nof the entire corpus.\nâ€¢ğœ [43] calculates the standard deviation of scores, determining\nthe optimal number of retrieved documents for each query to\nminimize the impact of low-scoring, non-relevant documents.\nâ€¢ğœmax [12] is a normalized standard deviation that considers only\ndocuments with scores above a certain percentage of the top score.\nâ€¢Fusion [49] relies on submitting the target query to all candidate\nDRs to acquire document rankings for each DR. A search result\nfusion method is then used to aggregate these rankings into a\npseudo-reference list. DRs are subsequently scored based on their\nRBO against this reference list.\nFor all QPP-based methods, the final score of a DR with respect\nto a target collection is computed as the average value returned by\nthe QPP method across all queries.\n6 RESULTS\n6.1 Main Results\nIn Tables 2 and 3, we compare LARMOR against other baselines in\nterms of Kendall Tau and Î”ğ‘’, respectively.\nFirstly, we observe that the baseline MSMARCO pref, which sim-\nply ranks DRs based on nDCG@10 obtained on the MSMARCO\ncollection, performs the best among the previous DR selection\nmethods in terms of Kendall Tau; this finding aligns with previous\nwork [28]. However, it is important to note that it is not guaranteed\nthat MS MARCO training data is used in all the DRs we considered\nin our experiments. For example, bge-large-en-v1.5 was trained\non the Massive Text Pairs (MTP) collection, which contains 200M\nEnglish text pairs [60]. It is the second-best performing DR across\nall collections, however it is predicted to be only the 8-th best if one\nrelies on MSMARCO pref, being surpassed by DRs that likely overfit\nthe MSMARCO collection but do not perform comparably well on\nthe other collections. It is noteworthy that the prediction based on\nMSMARCO pref yields the best Kendall Tau for the NQ collection.\nThis is somewhat expected since NQ is considered to be the most\nsimilar collection to MSMARCO.\nAnother performance-based approach,Query Alteration, closely\nfollows MSMARCO pref. While it underperforms compared toMSMARCO\npref in terms of Kendall Tau, it achieves a higher average Î”ğ‘’, indi-\ncating its greater effectiveness in selecting the top DRs rather than\nproviding the true ranking of all DRs.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, and Guido Zuccon\nTable 3: Î”ğ‘’, calculated based on nDCG@10.\nNF FiQA ArguAna SciDocs SciFact Covid Quora NQ DBPedia HotpotQA Signal1M Robust04 Trec-NewsAvrg\nMSMARCO Perf. 6.84 20.21 27.47 10.02 20.26 32.18 9.58 13.91 9.80 17.52 3.51 20.23 16.89 16.03\nBinary Entropy 6.84 18.54 25.84 11.59 25.06 9.10 2.21 34.34 6.26 29.2 5.48 7.59 12.31 14.95\nQuery Alteration2.61 15.15 7.98 6.04 18.35 13.42 3.61 21.04 2.46 20.15 4.50 7.59 5.95 9.91\nWIG 7.06 16.69 25.92 6.60 8.44 12.16 1.07 18.18 4.56 20.15 5.69 15.85 13.06 11.96\nWIG Norm 13.46 24.25 24.45 12.42 22.39 14.58 3.98 17.36 13.38 29.74 1.82 9.22 14.82 15.53\nNQC 15.09 20.48 23.22 11.59 25.05 19.59 4.04 19.68 16.79 29.2 4.14 15.28 12.31 16.65\nNQC Norm 7.06 16.69 25.92 6.60 8.44 15.17 4.29 7.77 13.34 20.15 5.69 15.85 13.06 12.32\nSMV 15.09 20.48 23.22 11.59 25.05 19.59 4.04 19.68 16.79 29.2 4.14 15.28 12.31 16.65\nSMV Norm 7.61 15.15 25.92 9.83 8.44 15.17 4.29 7.77 13.34 7.73 5.69 15.85 13.06 11.53\nğœ 7.61 16.69 25.92 6.60 10.58 15.17 4.29 4.96 3.60 20.15 4.50 15.85 10.36 11.25\nğœğ‘šğ‘ğ‘¥ 7.61 16.69 11.45 6.67 5.74 15.17 2.20 21.04 6.26 18.64 3.62 15.85 10.36 10.87\nClarity 2.44 12.46 18.59 2.12 11.62 33.56 3.52 4.06 12.8 35.04 2.00 23.43 7.84 13.04\nFusion 1.15 5.46 7.96 5.09 3.67 20.09 0.94 9.29 2.53 7.17 0.53 2.37 0.0 5.10\nLARMOR (ours) 2.44 3.07 0.0 0.79 0.77 6.81 0.94 0.58 0.82 7.17 1.28 2.37 0.0 2.08\nTable 4: Ablation Study: the effect of different steps of the pipeline. Kendall Tau Correlation value, calculated based on nDCG@10.\nNF FiQA ArguAnaSciDocs SciFact Covid Quora NQ DBPedia HotpotQASignal1MRobust04Trec-NewsAvrg\nQ 0.483 0.525 0.522 0.545 0.708 0.563 0.635 0.578 0.658 0.824 0.153 0.324 0.634 0.550\nQF 0.677 0.545 0.502 0.729 0.610 0.313 0.622 0.311 0.648 0.520 0.517 0.539 0.648 0.552\nQFJ 0.552 0.562 0.522 0.59 0.809 0.559 0.677 0.646 0.667 0.789 0.191 0.397 0.648 0.585\nQFR 0.700 0.562 0.448 0.764 0.676 0.370 0.672 0.444 0.613 0.607 0.526 0.437 0.667 0.576\nLARMOR0.700 0.618 0.627 0.739 0.766 0.553 0.740 0.563 0.665 0.710 0.380 0.444 0.690 0.630\nTable 5: Ablation Study: the effect of different steps of the pipeline. Î”ğ‘’, calculated based on nDCG@10.\nNF FiQA ArguAna SciDocs SciFact Covid Quora NQ DBPedia HotpotQASignal1MRobust04 Trec-NewsAvrg\nQ 1.52 5.46 13.73 3.26 2.01 18.29 0.0 0.58 3.87 3.10 1.38 7.92 5.95 5.16\nQF 0.22 5.46 7.96 0.79 3.67 18.31 0.94 9.29 2.53 7.17 0.25 2.37 0.0 4.53\nQFJ 1.74 3.07 1.35 3.90 0.77 5.31 0.34 0.58 4.56 3.10 1.38 7.92 5.95 3.07\nQFR 0.22 5.46 8.08 0.79 0.77 14.67 0.94 8.21 3.73 7.17 0.25 2.37 0.0 4.05\nLARMOR2.44 3.07 0.0 0.79 0.77 6.81 0.94 0.58 0.82 7.17 1.28 2.37 0.0 2.08\nAs expected, methods that rely on comparing the scores pro-\nduced by the retrievers perform poorly in both DR selection and\nDR ranking tasks. These methods include Binary Entropy and\nfour QPP methods (WIG, NQC, SMV,ğœ). Notably, the normalized\nversions of score-based QPPs yield better results in terms of both\nÎ”ğ‘’ and Kendall Tau (Tables 2 and 3). This implies that incorporating\na scaling parameter for normalizing scores is beneficial. However,\nas discussed in Section 2.3, this normalization primarily regularizes\nscores within the collection, but does not address the challenge of\nscore distribution diversity across different DRs.\nIn contrast, Fusion, which aggregates the retrieved lists from\ndifferent DRs without relying on absolute score values, achieves\nsignificantly better performance in terms of both Kendall Tau and\nÎ”ğ‘’. Nevertheless, similar to the other QPP baselines, Fusion re-\nquires the availability of the queries from the target collection. This\nrequirement is often impractical, as DR selection must occur before\nthe system is deployed and queries are gathered. It is important to\nnote that our LARMOR, unlike other baselines, is query-free.\nFinally, our LARMOR achieves the best overall performance in\nterms of both Kendall Tau and Î”ğ‘’. Moreover, LARMOR selects the\nbest DR for two collections (Arguana and Trec-News), resulting in\nan average nDCG drop of only 2.08% across the board.\nIn the next subsections we tease out the contributions of LAR-\nMORâ€™s components, and the effect of the LLM model size, of the\nspecific LLM backbone, and of the number of generated queries.\n6.2 Effect of LARMORâ€™s Components\nWe study the effectiveness of some of LARMORâ€™s components, mea-\nsuring effectiveness at intermediate ablation points in the pipeline,\nillustrated in Figure 2. The ablation points we investigate are:\nâ€¢Q: Query generation â€” we use the LLM-generated query, along\nwith its associated document, as a pseudo-relevant query-document\npair. The DRs are then evaluated and ranked based on these judg-\nments. Note that here we only have a single relevant document\nfor each generated query.\nâ€¢QF: Rank fusion of the generated queries â€” we generate multiple\nqueries for each sampled document ğ‘‘ğ‘–, and fuse their ranking to\nobtain ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘— for each ğ‘‘ğ‘–. We then rank DRs based on the average\nRBO value against the obtained set of fused rankings ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘—.\nâ€¢QFJ: Pseudo-judgments from rank fusion â€” we generate multiple\nqueries for each sampled document ğ‘‘ğ‘–, and fuse their ranking\nto obtain ğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘— for each ğ‘‘ğ‘–. We then use the LLM to generate\npseudo-judgements ( eJ), and use these to rank DRs.\nâ€¢QFR: Pseudo-reference lists from rank fusion â€” using the same\nğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘— described above, we use the LLM to re-rankğ¹Ëœğ‘ğ‘‘ğ‘–,ğ‘—, obtaining\nthe pseudo-reference list ğ¿Ëœğ‘ğ‘‘ğ‘–,ğ‘—. We then rank DRs with respect\nto the set of all reference lists L.\nThe full LARMOR pipeline performs an additional fusion of the\nranking of DRs obtained from the ablation points QFJ and QFR.\nIn Table 4 and 5, we present Kendall Tau andÎ”ğ‘’ result, obtained\nat these different steps of our pipeline.\nAs the results illustrate, with just judgments from Q, we can\nalready achieve very strong performance that, outperforming most\nbaselines, only falling short of QPP fusion.\nQF further improves Q by using the fused ranking and RBO to\nrank DRs. We note that QF is similar to the QPPFusion baseline\nexcept that the queries for QF are LLM-generated, while the queries\nfor Fusion are the actual test queries. Remarkably, QF can surpass\nLeveraging LLMs for Unsupervised Dense Retriever Ranking SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nFigure 3: Kendall Tau (left) and Î”ğ‘’ (right) performance of our pro-\nposed LARMOR and its various components using different sizes of\nFlanT5 models.\nQPP Fusion in terms of Î”ğ‘’ average score, suggesting that our LLM-\ngenerated in-domain queries are of satisfactory quality.\nOn the other hand, QFJ and QFR prove to be very important;\nthey both significantly improve QF for both Kendall Tau and Î”ğ‘’.\nFinally, our whole pipeline LARMOR achieved the overall best\nperformance by fusing QFJ and QFR. These results demonstrate\nthat each component in LARMOR has a significant contribution.\n6.3 Effect of LLM Model Size\nTo comprehensively understand the impact of LLM size on our\nproposed LARMOR, in Figure 3 we plot the Kendall Tau and Î”ğ‘’\nperformance across different steps of the pipeline. We explore the\ninfluence of FlanT5 models with varying sizes, namely FLAN-T5-\nlarge (780M), FLAN-T5-XL (3B), and FLAN-T5-XXL (11B).\nFirstly, a clear pattern emerges as each step within the pipeline\nconsistently contributes to improved effectiveness, irrespective of\nthe model size with the only exception that FLAN-T5-large exhibits\na suboptimal Î”ğ‘’ score on QF compared to Q.\nOn the other hand, the performance across different model sizes\nexhibits variations at each pipeline step. This suggests that the\nconventional scaling law of LLMs might not apply here: it is not\nalways the case that a larger model performs better for our method\nwith FLAN-T5 models. Nevertheless, FLAN-T5-XXL achieved the\nbest performance when the full LARMOR pipeline is applied.\n6.4 Effect of LLM Backbone and Number of\nGenerated Queries\nIn this section, we study how the state-of-the-art OpenAI LLMs,\nGPT-3.5 and GPT-4, perform compared to FLAN-T5-XXL. For these\nexperiments, we only conduct tests on the query generation (Q)\nstep due to the high cost of running the whole LARMOR with\nOpenAI models. Additionally, we investigate how the number of\ngenerated queries per document impacts performance. The results\nare illustrated in Figure 4.\nFor Kendall Tau performance, it is evident that more generated\nqueries per document tend to be beneficial, especially for FlanT5-\nXXL, although the improvements are marginal for GPT-3.5 and\nGPT-4. GPT-4 tends to have an overall high Kendall Tau score;\nhowever, FLAN-T5-XXL only outperforms when the number of\ngenerated queries is set to 10. As for the Î”ğ‘’ score, the impact of\nthe number of generated queries varies, and the difference between\nmodels also varies.\nHowever, we note that, although overall OpenAI models perform\nsimilarly to FLAN-T5-XXL, on the Arguana collection where FLAN-\nT5-XXL performs poorly (Kendall Tau = 0.522,Î”ğ‘’ = 13.73), OpenAI\nFigure 4: Kendall Tau (left) and Î”ğ‘’ (right) performance of pseudo-\nquery generation with a different number of generated queries and\ndifferent LLM backbone.\nmodels achieved surprisingly good performance. For example, GPT-\n4 achieved Kendall Tau = 0.713 and Î”ğ‘’ = 0, which are the best\nscores on this collection. We observe that Arguana poses a non-\ntrivial retrieval taskâ€”specifically, a counter-arguments retrieval\ntaskâ€”requiring LLMs to have the capability to generate counter-\narguments. GPT models demonstrate this capability effectively.\n7 CONCLUSION\nThis paper introduces a novel LLM-based approach for dense re-\ntriever selection, the Large Language Model Assisted Retrieval\nModel Ranking (LARMOR). Dense retriever selection is a crucial\ntask in many applications of search engines technologies. Dense\nretrievers are an effective and increasingly popular component of\na search engine. Search engine practitioners are often faced with\nthe choice of which dense retriever to deploy on a specific target\ncollection. However, it is challenging to predict a DRâ€™s effective-\nness on a target collection that contains data different from that\nin the collection used for training the DR. This is even more so if\nthe practitioners do not have access to user queries and relevance\njudgements from the target collection, as it is often the case in many\napplications, e.g., in small-medium enterprises and in domains like\nhealth and legal, due to the cost and time required to collect these\nsignals, or the impossibility to access this data for privacy reasons.\nNotably, LARMOR stands out as the only available method that\ndoes not require any post-deployment data but instead relies on\nminimal prior knowledge about the target collection to design\nprompts to guide LLMs in generating synthetic queries, pseudo-\nrelevant judgments, and reference lists. These in turn are used\nwithin LARMOR to rank dense retriever systems.\nWe comprehensively evaluate LARMOR across 13 different BEIR\ncollections, considering a large pool of state-of-the-art dense retriev-\ners. Our results demonstrate that LARMOR accurately ranks DRs\nbased on their effectiveness in a zero-shot manner, outperforming\nall previous DR selection methods and adapted QPP methods.\nNotably, unlike many existing baselines (e.g., score-based QPP,\nQuery Alteration, Binary Entropy), our method is model-agnostic\nand can be extended to choose among any type of IR models.\nFor future work, we are interested in applying advanced auto-\nmatic prompt optimization methods [18, 61] to further enhance the\ndomain-specific prompts used by LARMOR. Additionally, we are\nalso interested in incorporating into LARMOR and study recent\nadvanced open-source LLMs, such as Mistral [26] and Llama3 [1].\nAnother promising avenue for future work is reducing the compu-\ntational overhead of LLM-related computations in our pipeline.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, and Guido Zuccon\nREFERENCES\n[1] AI@Meta. 2024. Llama 3 Model Card. (2024). https://github.com/meta-llama/\nllama3/blob/main/MODEL_CARD.md\n[2] Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2022.\nShallow pooling for sparse labels. Information Retrieval Journal 25, 4 (2022),\n365â€“385. https://doi.org/10.1007/s10791-022-09411-0\n[3] Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian\nRiedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2023. Task-aware Retrieval with\nInstructions. In Findings of the Association for Computational Linguistics: ACL\n2023. Association for Computational Linguistics, 3650â€“3675. https://doi.org/10.\n18653/v1/2023.findings-acl.225\n[4] Elias Bassani. 2022. ranx: A Blazing-Fast Python Library for Ranking Evaluation\nand Comparison. In Proceedings of the 46th European Conference on Information\nRetrieval (ECIR) , Vol. 13186. Springer, 259â€“264. https://doi.org/10.1007/978-3-\n030-99739-7_30\n[5] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022.\nInPars: Unsupervised Dataset Generation for Information Retrieval. InProceedings\nof the 45th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR â€™22) . Association for Computing Machinery, 2387â€“\n2392. https://doi.org/10.1145/3477495.3531863\n[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language models are few-shot learners. In\nProceedings of the 34th International Conference on Neural Information Processing\nSystems (NIPS â€™20) . Curran Associates Inc., Article 159, 25 pages.\n[7] David Carmel and Oren Kurland. 2012. Query performance prediction for IR.\nIn Proceedings of the 35th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR â€™12) . 1196â€“1197.\n[8] Mayee Chen, Karan Goel, Nimit Sohoni, Fait Poms, Kayvon Fatahalian, and\nChristopher Re. 2021. Mandoline: Model Evaluation under Distribution Shift.\nIn Proceedings of the 38th International Conference on Machine Learning (ICMLâ€™\n2021).\n[9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,\nYunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022.\nScaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416\n(2022).\n[10] Gordon V. Cormack, Charles L. A. Clarke, and Stefan BÃ¼ttcher. 2009. Reciprocal\nrank fusion outperforms condorcet and individual rank learning methods. In\nProceedings of the 32nd International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR â€™09) . ACM, 758â€“759.\n[11] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. 2002. Predicting query\nperformance. In Proceedings of the 25th Annual International ACM SIGIR Confer-\nence on Research and Development in Information Retrieval (SIGIR â€™02) . Association\nfor Computing Machinery, 299â€“306. https://doi.org/10.1145/564376.564429\n[12] Ronan Cummins, Joemon Jose, and Colm Oâ€™Riordan. 2011. Improved query\nperformance prediction using standard deviation. In Proceedings of the 34th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR â€™11) . Association for Computing Machinery, 1089â€“1090. https:\n//doi.org/10.1145/2009916.2010063\n[13] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot\nDense Retrieval From 8 Examples. In Proceedings of the 10th International Confer-\nence on Learning Representations (ICLR â€™22) . https://openreview.net/forum?id=\ngmL46YMpu2J\n[14] Weijian Deng and Liang Zheng. 2021. Are Labels Always Necessary for Classifier\nAccuracy Evaluation?. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR â€™20) .\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[16] Guglielmo Faggioli, Laura Dietz, Charles L. A. Clarke, Gianluca Demartini,\nMatthias Hagen, Claudia Hauff, Noriko Kando, Evangelos Kanoulas, Martin\nPotthast, Benno Stein, and Henning Wachsmuth. 2023. Perspectives on Large\nLanguage Models for Relevance Judgment. In Proceedings of the 2023 ACM SIGIR\nInternational Conference on Theory of Information Retrieval (ICTIR â€™23) . Associa-\ntion for Computing Machinery, 39â€“50. https://doi.org/10.1145/3578337.3605136\n[17] Guglielmo Faggioli, Thibault Formal, Simon Lupart, Stefano Marchesin, Stephane\nClinchant, Nicola Ferro, and Benjamin Piwowarski. 2023. Towards Query Perfor-\nmance Prediction for Neural Information Retrieval: Challenges and Opportunities.\nIn Proceedings of the 2023 ACM SIGIR International Conference on Theory of In-\nformation Retrieval (ICTIR â€™23) . Association for Computing Machinery, 51â€“63.\nhttps://doi.org/10.1145/3578337.3605142\n[18] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and\nTim RocktÃ¤schel. 2023. Promptbreeder: Self-Referential Self-Improvement Via\nPrompt Evolution. arXiv preprint arXiv:2309.16797 (2023).\n[19] Saurabh Garg, Sivaraman Balakrishnan, Zachary Chase Lipton, Behnam\nNeyshabur, and Hanie Sedghi. 2022. Leveraging Unlabeled Data to Predict Out-\nof-Distribution Performance. In Proceedings of the 10th International Conference\non Learning Representations (ICLR â€™22) . https://arxiv.org/abs/2201.04234\n[20] Mitko Gospodinov, Sean MacAvaney, and Craig Macdonald. 2023. Doc2Queryâ€“:\nWhen Less is More. InProceedings of the 45th European Conference on Information\nRetrieval (ECIR â€™23) . Springer, 414â€“422.\n[21] Devin Guillory, Vaishaal Shankar, Sayna Ebrahimi, Trevor Darrell, and Ludwig\nSchmidt. 2021. Predicting with Confidence on Unseen Distributions. In Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision (ICCV â€™21) .\n1114â€“1124.\n[22] Claudia Hauff, Djoerd Hiemstra, and Franciska de Jong. 2008. A survey of pre-\nretrieval query performance predictors. InProceedings of the 17th ACM Conference\non Information and Knowledge Management (CIKM â€™08) . 1419â€“1420.\n[23] Ben He and Iadh Ounis. 2006. Query performance prediction.Information Systems\n31, 7 (2006), 585â€“594.\n[24] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The\nCurious Case of Neural Text Degeneration. In Proceedings of the 8th International\nConference on Learning Representations (ICLR â€™20) .\n[25] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo,\nJakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Models as\nEfficient Dataset Generators for Information Retrieval. https://doi.org/10.48550/\nARXIV.2301.01820\n[26] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-\nvendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, et al . 2023. Mistral 7B. arXiv preprint\narXiv:2310.06825 (2023).\n[27] Ekaterina Khramtsova, Teerapong Leelanupab, Shengyao Zhuang, Mahsa Bak-\ntashmotlagh, and Guido Zuccon. 2024. Embark on DenseQuest: A System for\nSelecting the Best Dense Retriever for a Custom Collection. In Proceedings of\nthe 47th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR â€™24) . Association for Computing Machinery, New\nYork, NY, USA (To Appear).\n[28] Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, Xi Wang, and\nGuido Zuccon. 2023. Selecting which Dense Retriever to use for Zero-Shot Search.\nIn Proceedings of the Annual International ACM SIGIR Conference on Research\nand Development in Information Retrieval in the Asia Pacific Region (SIGIR-AP\nâ€™23). Association for Computing Machinery, 223â€“233. https://doi.org/10.1145/\n3624918.3625330\n[29] Ekaterina Khramtsova, Guido Zuccon, Xi Wang, and Mahsa Baktashmotlagh.\n2023. Convolutional Persistence as a Remedy to Neural Model Analysis. In\nProceedings of The 26th International Conference on Artificial Intelligence and\nStatistics (AISTATs â€™23), Vol. 206. PMLR, 10839â€“10855. https://proceedings.mlr.\npress/v206/khramtsova23a.html\n[30] Oren Kurland and J Shane Culpepper. 2018. Fusion in information retrieval.\nIn Proceedings of the 41st International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR â€™18) . 1383â€“1386.\n[31] Oren Kurland, Anna Shtok, Shay Hummel, Fiana Raiber, David Carmel, and Ofri\nRom. 2012. Back to the roots: a probabilistic framework for query-performance\nprediction. In Proceedings of the 21st ACM International Conference on Information\nand Knowledge Management (CIKM â€™12) . Association for Computing Machinery,\n823â€“832. https://doi.org/10.1145/2396761.2396866\n[32] Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar\nMehdad, Wen-tau Yih, and Xilun Chen. 2023. How to Train Your DRAGON:\nDiverse Augmentation Towards Generalizable Dense Retrieval. arXiv preprint\narXiv:2302.07452 (2023).\n[33] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[34] Xiaolu Lu, Alistair Moffat, and J Shane Culpepper. 2016. The effect of pooling\nand evaluation depth on IR metrics. Information Retrieval Journal 19, 4 (2016),\n416â€“445.\n[35] Simon Lupart, Thibault Formal, and StÃ©phane Clinchant. 2022. MS-Shift: An\nAnalysis of MS MARCO Distribution Shifts on Neural Retrieval. In Proceedings\nof the 44th European Conference on Information Retrieval (ECIR â€™22) . https:\n//api.semanticscholar.org/CorpusID:256231516\n[36] Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-Shot\nListwise Document Reranking with a Large Language Model. arXiv preprint\narXiv:2305.02156 (2023).\n[37] Joel Mackenzie, Matthias Petri, and Alistair Moffat. 2021. A sensitivity analysis\nof the MSMARCO passage collection. arXiv preprint arXiv:2112.03396 (2021).\n[38] Chuan Meng, Negar Arabzadeh, Mohammad Aliannejadi, and Maarten de Rijke.\n2023. Query Performance Prediction: From Ad-hoc to Conversational Search.\nIn Proceedings of the 46th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR â€™23) . Association for Computing\nMachinery, 2583â€“2593. https://doi.org/10.1145/3539618.3591919\nLeveraging LLMs for Unsupervised Dense Retriever Ranking SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\n[39] Chuan Meng, Negar Arabzadeh, Arian Askari, Mohammad Aliannejadi, and\nMaarten de Rijke. 2024. Query Performance Prediction using Relevance Judg-\nments Generated by Large Language Models. arXiv preprint arXiv:2404.01012\n(2024).\n[40] Alistair Moffat, Falk Scholer, and Ziying Yang. 2018. Estimating Measurement Un-\ncertainty for Information Retrieval Effectiveness Metrics. J. Data and Information\nQuality 10, 3 (2018), 22. https://doi.org/10.1145/3239572\n[41] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. MTEB:\nMassive Text Embedding Benchmark. In Proceedings of the 17th Conference of the\nEuropean Chapter of the Association for Computational Linguistics . Association\nfor Computational Linguistics, 2014â€“2037. https://doi.org/10.18653/v1/2023.eacl-\nmain.148\n[42] Rodrigo Nogueira and Jimmy Lin. 2019. From doc2query to docTTTTTquery.\nhttps://api.semanticscholar.org/CorpusID:208612557\n[43] JoaquÃ­n PÃ©rez-Iglesias and Lourdes Araujo. 2010. Standard deviation as a query\nhardness estimator. In Proceedings of the 17th International Conference on String\nProcessing and Information Retrieval (SPIREâ€™10) . Springer-Verlag, 207â€“212.\n[44] Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankVicuna:\nZero-Shot Listwise Document Reranking with Open-Source Large Language\nModels. arXiv preprint arXiv:2309.15088 (2023).\n[45] Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankZephyr:\nEffective and Robust Zero-Shot Listwise Reranking is a Breeze! arXiv preprint\narXiv:2312.02724 (2023).\n[46] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen,\nTianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, et al. 2023. Large language\nmodels are effective text rankers with pairwise ranking prompting.arXiv preprint\narXiv:2306.17563 (2023).\n[47] Ruiyang Ren, Yingqi Qu, Jing Liu, Xin Zhao, Qifei Wu, Yuchen Ding, Hua Wu,\nHaifeng Wang, and Ji-Rong Wen. 2023. A Thorough Examination on Zero-shot\nDense Retrieval. In Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing (EMNLP â€™23) , Houda Bouamor, Juan Pino, and Kalika\nBali (Eds.). Association for Computational Linguistics, 15783â€“15796. https:\n//doi.org/10.18653/v1/2023.findings-emnlp.1057\n[48] Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau\nYih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving Passage Retrieval\nwith Zero-Shot Question Generation. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP â€™22) . Association for\nComputational Linguistics, 3781â€“3797. https://doi.org/10.18653/v1/2022.emnlp-\nmain.249\n[49] Anna Shtok, Oren Kurland, and David Carmel. 2016. Query Performance Predic-\ntion Using Reference Lists. ACM Transactions on Information Systems (TOIS) 34\n(2016), 1 â€“ 34. https://api.semanticscholar.org/CorpusID:14981277\n[50] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits. 2012.\nPredicting Query Performance by Query-Drift Estimation. ACM Transactions on\nInformation Systems 30 (2012). https://doi.org/10.1145/2180868.2180873\n[51] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin\nChen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search?\nInvestigating Large Language Models as Re-Ranking Agents. In Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing\n(EMNLP â€™23) . Association for Computational Linguistics, 14918â€“14937. https:\n//doi.org/10.18653/v1/2023.emnlp-main.923\n[52] Manveer Singh Tamber, Ronak Pradeep, and Jimmy Lin. 2023. Scaling Down, LiT-\nting Up: Efficient Zero-Shot Listwise Reranking with Seq2seq Encoder-Decoder\nModels. arXiv preprint arXiv:2312.16098 (2023).\n[53] Yongquan Tao and Shengli Wu. 2014. Query Performance Prediction By Con-\nsidering Score Magnitude and Variance Together. In Proceedings of the 23rd\nACM International Conference on Conference on Information and Knowledge\nManagement (CIKM â€™14) . Association for Computing Machinery, 1891â€“1894.\nhttps://doi.org/10.1145/2661829.2661906\n[54] Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna\nGurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation\nof Information Retrieval Models. In Proceedings of the 35th Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (NeurIPS â€™21) .\n[55] Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar Mitra. 2023. Large\nlanguage models can accurately predict searcher preferences. arXiv preprint\narXiv:2309.10621 (2023).\n[56] Nicola Tonellotto. 2022. Lecture notes on neural information retrieval. arXiv\npreprint arXiv:2207.13443 (2022).\n[57] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2021. GPL:\nGenerative Pseudo Labeling for Unsupervised Domain Adaptation of Dense\nRetrieval. arXiv preprint arXiv:2112.07577 (4 2021). https://arxiv.org/abs/2112.\n07577\n[58] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,\nRangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised\ncontrastive pre-training. arXiv preprint arXiv:2212.03533 (2022).\n[59] William Webber, Alistair Moffat, and Justin Zobel. 2010. A similarity measure\nfor indefinite rankings. ACM Transactions on Information Systems 28 (2010).\nhttps://doi.org/10.1145/1852102.1852106\n[60] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-Pack:\nPackaged Resources To Advance General Chinese Embedding. arXiv:2309.07597\n[61] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou,\nand Xinyun Chen. 2023. Large language models as optimizers. arXiv preprint\narXiv:2309.03409 (2023).\n[62] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie.\n2023. Retrieve anything to augment large language models. arXiv preprint\narXiv:2310.07554 (2023).\n[63] Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. 2023. Dense Text\nRetrieval based on Pretrained Language Models: A Survey. ACM Transactions on\nInformation Systems (2023). https://doi.org/10.1145/3637870\n[64] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey\nof large language models. arXiv preprint arXiv:2303.18223 (2023).\n[65] Yun Zhou and W. Bruce Croft. 2007. Query performance prediction in web\nsearch environments. In Proceedings of the 30th Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval (SIGIR â€™07) .\nAssociation for Computing Machinery, 543â€“550. https://doi.org/10.1145/1277741.\n1277835\n[66] Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan, Xuanhui Wang, and\nMichael Berdersky. 2023. Beyond yes and no: Improving zero-shot llm rankers\nvia scoring fine-grained relevance labels. arXiv preprint arXiv:2310.14122 (2023).\n[67] Shengyao Zhuang, Bing Liu, Bevan Koopman, and Guido Zuccon. 2023. Open-\nsource Large Language Models are Strong Zero-shot Query Likelihood Models for\nDocument Ranking. In Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing (EMNLP â€™23) . Association for Computational\nLinguistics, 8807â€“8817. https://doi.org/10.18653/v1/2023.findings-emnlp.590\n[68] Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. 2023.\nA Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with\nLarge Language Models. arXiv preprint arXiv:2310.09497 (2023).\n[69] Shengyao Zhuang and Guido Zuccon. 2021. Dealing with Typos for BERT-\nbased Passage Retrieval and Ranking. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP â€™21) . Association for\nComputational Linguistics, 2836â€“2842.\n[70] Shengyao Zhuang and Guido Zuccon. 2022. CharacterBERT and Self-Teaching\nfor Improving the Robustness of Dense Retrievers on Queries with Typos. In\nProceedings of the 45st International ACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval (SIGIR â€™22) . Association for Computing Machinery,\n1444â€“1454.",
  "topic": "Ranking (information retrieval)",
  "concepts": [
    {
      "name": "Ranking (information retrieval)",
      "score": 0.7039850950241089
    },
    {
      "name": "Computer science",
      "score": 0.3925003409385681
    },
    {
      "name": "Data science",
      "score": 0.3311951756477356
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32714688777923584
    },
    {
      "name": "Business",
      "score": 0.3241819143295288
    },
    {
      "name": "Geography",
      "score": 0.3228236436843872
    }
  ]
}