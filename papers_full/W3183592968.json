{
  "title": "Transformers-sklearn: a toolkit for medical language understanding with transformer-based models",
  "url": "https://openalex.org/W3183592968",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2952019530",
      "name": "Feihong Yang",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A2463431227",
      "name": "Xuwen WANG",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A2904065218",
      "name": "Hetong Ma",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A2101750202",
      "name": "Jiao Li",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A2952019530",
      "name": "Feihong Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2463431227",
      "name": "Xuwen WANG",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2904065218",
      "name": "Hetong Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101750202",
      "name": "Jiao Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W2521200999",
    "https://openalex.org/W2970390493",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2735784619",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2586665457",
    "https://openalex.org/W2974580236",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2950511172",
    "https://openalex.org/W2970597249"
  ],
  "abstract": null,
  "full_text": "Yang et al. BMC Med Inform Decis Mak  2021, 21(Suppl 2):90 \nhttps://doi.org/10.1186/s12911-021-01459-0\nSOFTWARE\nTransformers-sklearn: a toolkit for medical \nlanguage understanding with transformer‑based \nmodels\nFeihong Yang†, Xuwen Wang†, Hetong Ma and Jiao Li* \nFrom International Conference on Health Big Data and Artificial Intelligence 2020 Guangzhou, China. \n29 October - 1 November 2020\nAbstract \nBackground: Transformer is an attention-based architecture proven the state-of-the-art model in natural language \nprocessing (NLP). To reduce the difficulty of beginning to use transformer-based models in medical language under-\nstanding and expand the capability of the scikit-learn toolkit in deep learning, we proposed an easy to learn Python \ntoolkit named transformers-sklearn. By wrapping the interfaces of transformers in only three functions (i.e., fit, score, \nand predict), transformers-sklearn combines the advantages of the transformers and scikit-learn toolkits.\nMethods: In transformers-sklearn, three Python classes were implemented, namely, BERTologyClassifier for the classifi-\ncation task, BERTologyNERClassifier for the named entity recognition (NER) task, and BERTologyRegressor for the regres-\nsion task. Each class contains three methods, i.e., fit for fine-tuning transformer-based models with the training dataset, \nscore for evaluating the performance of the fine-tuned model, and predict for predicting the labels of the test dataset. \ntransformers-sklearn is a user-friendly toolkit that (1) Is customizable via a few parameters (e.g., model_name_or_path \nand model_type), (2) Supports multilingual NLP tasks, and (3) Requires less coding. The input data format is automati-\ncally generated by transformers-sklearn with the annotated corpus. Newcomers only need to prepare the dataset. The \nmodel framework and training methods are predefined in transformers-sklearn.\nResults: We collected four open-source medical language datasets, including TrialClassification for Chinese medical \ntrial text multi label classification, BC5CDR for English biomedical text name entity recognition, DiabetesNER for Chi-\nnese diabetes entity recognition and BIOSSES for English biomedical sentence similarity estimation.\nIn the four medical NLP tasks, the average code size of our script is 45 lines/task, which is one-sixth the size of trans-\nformers’ script. The experimental results show that transformers-sklearn based on pretrained BERT models achieved \nmacro F1 scores of 0.8225, 0.8703 and 0.6908, respectively, on the TrialClassification, BC5CDR and DiabetesNER tasks \nand a Pearson correlation of 0.8260 on the BIOSSES task, which is consistent with the results of transformers.\nConclusions: The proposed toolkit could help newcomers address medical language understanding tasks using the \nscikit-learn coding style easily. The code and tutorials of transformers-sklearn are available at https ://doi.org/10.5281/\n© The Author(s) 2021. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creat iveco mmons .org/licen ses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creat iveco \nmmons .org/publi cdoma in/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nOpen Access\n*Correspondence:  li.jiao@imicams.ac.cn\n†Feihong Yang and Xuwen Wang have contributed equally to this work\nInstitute of Medical Information and Library, Chinese Academy of Medical \nSciences/Peking Union Medical College, 3rd Yabao Road, Beijing 100020, \nChina\nPage 2 of 8Yang et al. BMC Med Inform Decis Mak  2021, 21(Suppl 2):90\nBackground\nTransformer is an attention-based architecture pro -\nposed by Vaswani et al. [1 ], which has been proved to \nbe the state-of-the-art model by BERT [2 ] (i.e., Bidi -\nrectional Encoder Representations from Transform -\ners), RoBERTa [3 ] (i.e., a Robustly Optimized BERT \npre-training Approach), etc. With the development of \nnatural language processing (NLP) technology, trans -\nformer-based models have emerged. To effectively \nutilize these models and evaluate their performance \nin downstream tasks, a Python library of transformer-\nbased models, namely, transformers [ 4], has been devel -\noped by gathering state-of-the-art general purpose \npre-trained models under a unified application program \ninterface (API) together with an ecosystem of libraries. \ntransformers has been reported to have been used in \nmore than 200 research papers and included either as a \ndependency or with a wrapper in several popular NLP \nframeworks such as AllenNLP [5 ] and Flair [6 ].\nscikit-learn [ 7], which is a Python module integrating \na wide range of state-of-the-art machine learning algo -\nrithms for medium-scale supervised and unsupervised \nproblems, is one of the most popular machine-learn -\ning toolkits. It is friendly to newcomers to apply it in \nmachine learning tasks. Based on scikit-learn, Lemai -\ntre G et  al. proposed imbalanced-learn [ 8] to provide \nvarious methods to cope with the imbalanced dataset \nproblem frequently encountered in machine learning \nand pattern recognition. Szymański P and Kajdanowicz \nT developed a Python library named scikit-multilearn  \n[9] for performing multi label classification. Löning M \net  al. present sktime [ 10], which is a scikit-learn com -\npatible the Python library with a unified interface for \nmachine learning with time series. De Vazelhes W et al. \nimplemented supervised and weakly supervised dis -\ntance metric learning algorithms and wrapped them \nin a Python package named metric-learn [ 11]. These \nworks made scikit-learn more powerful and efficient in \nspecific domain tasks.\nAs known, the transformers toolkit is well designed \nand friendly to professional researchers and engi -\nneers. However, for newcomers who have no knowl -\nedge of transformers, it is still time-consuming to learn \nthe background knowledge about transformers from \nscratch. scikit-learn is designed to make machine learn -\ning for easy use, but there is still a gap between machine \nlearning and deep learning algorithms in scikit-learn.\nTo reduce the difficulty of getting started with trans -\nformer-based models and expand the capability of scikit-\nlearn in deep learning, we combine the advantages of \nthe transformers and scikit-learn toolkits and propose \na Python toolkit named transformers-sklearn. The pro -\nposed toolkit aims to make transformer-based models \nconvenient for beginners by wrapping the interfaces of \ntransformers in only three APIs (i.e., fit, score, and pre -\ndict). With transformers-sklearn, newcomers could use \ntransformer-based models to address their NLP tasks, \neven though they had no previous knowledge of trans -\nformer. The users can pay more attention on the NLP \ntask itself, with preparing the training dataset for fitting, \nthe development dataset for scoring the model, and the \ntest dataset for predicting.\nThe primary contributions of this paper are as follows. \n(1) We proposed transformers-sklearn, which makes \ntransformer-based models for easy use and expands the \ncapability of scikit-learn in deep learning methods. (2) \nTo validate the performance of transformers-sklearn, \nexperiments were conducted on four NLP tasks based \non English and Chinese medical language datasets. We \nalso compared transformers-sklearn with the widely used \nNLP toolkits such as transformers and UER  [12]. (3) The \ncode and tutorials of transformers-sklearn are available at \nhttps ://doi.org/10.5281/zenod o.44538 03.\nMethods\nIn transformers-sklearn, there are three Python classes \ndesigned for classification, named entity recognition \n(NER), and regression tasks. Each class contains three \nmethods, namely, fit, score, and predict.\nPython classes\ntransformers-sklearn was implemented with three Python \nclasses, which are BERTologyClassifier for the classifica -\ntion task, BERTologyNERClassifier for the named entity \nrecognition (NER) task, and BERTologyRegressor for \nthe regression task. BERTologyClassifier and BERTolo -\ngyNERClassifier are subclasses of BaseEstimator and \nClassifierMixin implemented by the scikit-learn toolkit. \nBERTologyRegressor is the subclass of BaseEstimator and \nRegressorMixin implemented by scikit-learn.\nAll classes could be customized by setting the values of \nmultiple parameters. Among these parameters, model_\ntype is used to specify which type of model initializa -\ntion style should be used, and model_name_or_path is \nzenod o.44538 03. In future, more medical language understanding tasks will be supported to improve the applica-\ntions of transformers_sklearn.\nKeywords: Transformer, NLP , Toolkit, Deep Learning, Medical Language Understanding\nPage 3 of 8\nYang et al. BMC Med Inform Decis Mak  2021, 21(Suppl 2):90\n \nused to specify which pre-trained model should be used. \nThere are six model initialization types, namely, BERT, \nRoBERTa, XLNet [13], XLM [14], DistilBERT [15] and \nALBERT [16]. All these models are implemented based \non a transformer, but they differ in their data processing. \nMore details about the parameters are shown in Table 1.\nClass methods\nThe same as with the class methods of scikit-learn, three \nmethods (i.e., fit, score, and predict) were implemented \nin each Python class of transformers-sklearn. The fit \nand score methods accept two parameters, which are X \nand y. X is a container of sentences or documents, and y \ncontains the corresponding labels. X and y could be one \nof the following Python data types: list , ndarray imple -\nmented by numpy [17], and DataFrame implemented by \npandas [18]. The predict method only requires parameter \nX.\nThe functions of the above class methods were as \nfollows:\n1. Fit. This method was used to fine-tune the custom -\nized pre-trained model following the configuration of \nthe parameters in each class (i.e., BERTologyClassifier \nor BERTologyNERClassifier or BERTologyRegressor). \nIn this method, the training set was automatically \ntransformed to the specific format and then fed into \nthe customized transformer-based model for fine-\ntuning.\n2. Score. This method was used to evaluate the perfor -\nmance of the fine-tuned model. For example, in the \nclassification task, this method would return the \ncommon evaluation indexes such as the precision, \nrecall and F1-score for each type of label.\n3. Predict. This method was used to predict the labels of \na given dataset.\nTraditionally, it is difficult for newcomers to address \ntheir NLP problems using the transformers-based meth -\nods. For instant, a user would like to apply the BERT \nmodel to address a binary classification task, thereafter, \nfour steps were needed to fine-tune the pre-trained BERT \nmodel as follows:\n1. Data preparation. The training set is transformed to \na special format for the BERT model. The user needs \nto learn about the data processing of BERT.\n2. Model configuration. The user customizes the \nmodel with fully understanding the architecture of \nBERT.\n3. Fine-tuning model. The user determines epochs that \nare used to fine-tune the customized BERT.\n4. Saving fine-tuned model. The user saves the fine-\ntuned model to the target path.\nThe four steps mentioned above increased the devel -\nopmental difficulty for newcomers, and it is time-con -\nsuming for them to learn the necessary background \nknowledge. In our work transformers-sklearn, the four \nsteps are implemented automatically in the fit method \nand transparent to users.\nWorkflow\nAs shown in Fig. 1, when facing an NLP task, the user first \ndetermines whether the transformer-based models could \naddress the problem. If the so, the user should choose \none class from BERTologyClassifier, BERTologyNERClas-\nsifier and BERTologyRegressor, which could be custom -\nized by setting the parameters, depending on to which \nclass the problem belongs. After customizing the cho -\nsen class, the user feeds the datasets into the fit method. \nUsing the NER task as an example, the input data format \nis defined as Table 2. As shown the text field contains seg-\nmented texts to be labelled, and the label  field contains \nthe corresponding medical named entities obtained by \nmanual annotations. Then, transformers-sklearn would \nconduct the fine-tuning process automatically. Finally, \nthe user could evaluate the fine-tuned model using the \nscore method or deploy the fine-tuned model in practice \nusing the predict method. During the whole workflow, \nTable 1 The common parameters of the Python classes in transformers-sklearn \nName Function\nmodel_type Specifies which type of model initialization style should be used\nmodel_name_or_path Specifies which pre-trained model should be used\nmax_seq_length Sets the max length of the sequence that could be accepted\nper_gpu_train_batch_size Sets the batch size per GPU\nlearning_rate Sets the learning rate of the model\nnum_train_epochs Sets the number of training epochs of the model\nno_cuda Sets whether the GPU is used for training or predicting\nPage 4 of 8Yang et al. BMC Med Inform Decis Mak  2021, 21(Suppl 2):90\nit is possible for the user to dispense with understand -\ning the internal mechanisms of the chosen transformer-\nbased model.\nExperiments\nWe conducted comparison experiments to validate the \neffectiveness of transformers-sklearn on multilingual \nmedical NLP tasks. We selected three popular trans -\nformer-based model types from our package, i.e., BERT, \nRoBERTa, and ALBERT, and compared them with the \noriginal transformers and UER  [12]. The pre-trained \nmodels of different model types can be downloaded \nautomatically or manually from the community [19], as \nshown in Table  3. All experiments were conducted on \nfour Tesla V100 16  GB GPUs with the initial number \nof training epochs set to 3, the learning rate set to 5e-5 \nand the other parameters set to their default values. The \nparameters such as the epochs and learning rate can be \nadjusted manually according to specific experiments.\nCorpus\nTo assess the performance of transformers-sklearn on \nmedical language understanding, we collected the fol -\nlowing four English and Chinese medical datasets (Tri -\nalClassification, BC5CDR, DiabetesNER, and BIOSSES) \nfrom the NLP community as our experimental corpora. \nMore details on the four datasets can be found in Table 4.\n1. TrialClassification [20]. This dataset contains 38,341 \nChinese clinical trial sentences and is labelled with \n45 classes. It was developed for Chinese medical trial \ntext multilabel classification.\n2. BC5CDR [21]. This dataset is a collection of 1,500 \nPubMed titles and abstracts selected from the CTD-\nPfizer corpus and was used in the BioCreative V \nchemical-disease relation task. It was developed for \nEnglish biomedical text name entity recognition.\n3. DiabetesNER [22]. The dataset contains more than \n9,556 Chinese medical named entity identification \nsamples. It was developed for Chinese diabetes entity \nrecognition. We randomly selected 80% of the data \nfor training and 20% of the data for testing.\n4. BIOSSES [23]. This dataset is a corpus of 100 sen -\ntence pairs selected from the Biomedical Summa -\nrization Track Training Dataset in the biomedi -\nFig. 1 Workflow of using transformers-sklearn to address NLP \nproblems\nTable 2 An example of the NER input data format in the \nBERTologyNERClassifier \nInput data field Example\nText […[“Naloxone” , “reverses” , “the” , “antihypertensive” , \n“effect” , “of” , “clonidine” , “ .”], …]\nLabel […[“B-Chem” , “O” , “O” , “O” , “O” , “O” , “B-Chem” ,”O”], …]\nTable 3 Pre-trained models and URLs\nModel URL\nbert-base-chinese https ://huggi ngfac e.co/bert-base-chine se\nbert-base-cased https ://huggi ngfac e.co/bert-base-cased \nchinese-roberta-wwm-ext https ://huggi ngfac e.co/hfl/chine se-rober ta-wwm-ext\nroberta-base https ://huggi ngfac e.co/rober ta-base\nalbert_chinese_base https ://huggi ngfac e.co/voidf ul/alber t_chine se_base\nalbert-base-v2 https ://huggi ngfac e.co/alber t-base-v2\nPage 5 of 8\nYang et al. BMC Med Inform Decis Mak  2021, 21(Suppl 2):90\n \ncal domain. It was collected for English biomedical \nsentence similarity estimation. Here, we randomly \nselected 80% of the data for training and 20% of the \ndata for testing.\nEvaluation\nTwo types of evaluation indexes were used for scoring, \nwhich are the macro F1 and Pearson/Spearman correla -\ntion. For the macro F1, set n classes as C 1, C2, … Cn. The \nprecision for each class was defined as P i, which equals \nthe number of correct predictions Ci divided by the num-\nber of prediction Ci. The recall for each class was defined \nas Ri, which equals the number of correct predictions C i \ndivided by the number of predictions Ci. Then, the macro \nF1 score of the tasks were calculated as follows:\nFor the Pearson correlation, set y as the true value of \ngiven dataset and y_pred as the value predicted by the \nmodel. Then, the Pearson correlation was calculated as \nfollows:\n(1)Macro F1 =\n(1\nn\n) n∑\ni=1\n2 × Pi × Ri\nPi + Ri\n(2)\nρy,y_pred = E\n(\nyypred\n)\n− E\n(\ny\n)\nE\n(\ny_pred\n)\n√\nE\n(\ny2 )\n− E 2 (\ny\n)√\nE\n(\ny2\npred\n)\n− E 2 (\ny_pred\n)\nResults\nThe performances of the BERT model implemented by \ntransformers-sklearn, transformers and UER  in the four \nmedical NLP tasks are shown in Table  5. The transform -\ners-sklearn toolkit achieved macro F1 scores of 0.8225, \n0.8703 and 0.6908 in the TrialClassification, BC5CDR \nand DiabetesNER tasks, respectively, and a Pearson cor -\nrelation of 0.8260 in the BIOSSES task, which are consist-\nent with the results of transformers.\nTables  6 and 7 show the performances of the RoB -\nERTa and ALBERT models, respectively. The RoBERTa \nmodel in transformers-sklearn achieved macro F1 scores \nof 0.8148, 0.8528, and 0.7068 in the TrialClassification, \nBC5CDR and DiabetesNER tasks, respectively, and a \nPearson correlation of 0.39962 in the BIOSSES  task. The \nALBERT model in transformers-sklearn achieved macro \nF1 scores of 0.7142, 0.8422, and 0.6196 in the three \nrespective tasks and a Pearson correlation of 0.1892 in \nthe BIOSSES task.\nAs shown in Fig.  2, the entire code for BIOSSES  imple-\nment is short and easy to use. The users could apply \ntransformer-based models in the scikit-learn coding style \nwith the help of our toolkit. In the four tasks, the average \ncode load of our toolkit’s script is 45 lines/task, which is \none-sixth the size of transformers’ script. In addition to \nthe comparison of the number of lines of code, we also \ncompared the running time of each model, as shown in \nTables 5, 6, and 7, which indicated the high efficiency of \ntransformers-sklearn.\nTable 4 The open-source datasets of the four English and Chinese Medical NLP tasks\nName NLP Task Language Domain Metric\nTrialClassification [20] Classification Chinese Clinical Trial Macro F1\nBC5CDR [21] NER English PubMed titles and abstracts Macro F1\nDiabetesNER [22] NER Chinese Diabetes Papers Macro F1\nBIOSSES [23] Regression English Biomedical Pearson correlation\nTable 5 The experimental results of transformers-sklearn, transformers and UER in four medical NLP tasks (mode_type = “bert”)\na The value of Macro F1, where the bolded one indicates the best performance.\nb The value of Person correlation, where the bolded one indicates the best performance.\nName Score Second Lines of code Pre-trained model\nOurs Transformers UER Ours Transformers UER Ours Transformers UER\nTrialClassification 0.8225a 0.8312a 0.8213a 1198 1227 764 38 246 412 bert-base-chinese\nBC5CDR 0.8703a 0.8635a - 471 499 - 41 309 - bert-base-cased\nDiabetesNER 0.6908a 0.6962a 0.7166a 1254 1548 2805 63 309 372 bert-base-chinese\nBIOSSES 0.8260b 0.8200b - 19 15 - 41 246 - bert-base-cased\nPage 6 of 8Yang et al. BMC Med Inform Decis Mak  2021, 21(Suppl 2):90\nDiscussion\nPrincipal results\nThe proposed toolkit, transformers-sklearn, was proved \nto be easy to use for newcomers and could be used for \ntransformer-based models as the scikit-learn coding style.\nLimitations\nCompared with transformers, the limitation of transform-\ners-sklearn is its lack of flexibility. For example, within \ntransformers-sklearn, it is impossible for users to extract \nany encoding or decoding layer of the transformer. In \nother words, users cannot determine which layer of \ntransformer could act in the downstream tasks.\nFurthermore, transformers-sklearn aims at making \ntransformer-based models for easy use and expanding the \ncapability of scikit-learn in deep learning methods. For \nadvanced users, the transformers toolkit is better than \nour transformers-sklearn regarding flexibility.\nComparison to existing tools\nCompared with prior toolkits, such as transformers and \nUER, transformers-sklearn is easy to get started using \nfor newcomers with basic machine learning knowledge. \nThe experimental results of the four medical NLP tasks \nshowed that the BERT model in transformers-sklearn \nTable 6 The experimental results of transformers-sklearn and transformers in four medical NLP tasks (mode_type = “roberta”)\na The value of Macro F1, where the bolded one indicates the best performance\nb The value of Person correlation, where the bolded one indicates the best performance\nName Score Second Lines of code Pre-trained model\nOurs Transformers Ours Transformers Ours Transformers\nTrialClassification 0.8148a 0.8231a 1206 1208 38 246 chinese-roberta-wwm-ext\nBC5CDR 0.8528a 0.8461a 460 504 41 309 roberta-base\nDiabetesNER 0.7068a 0.7184a 1445 1426 63 309 chinese-roberta-wwm-ext\nBIOSSES 0.3996b 0.3614b 36 17 41 246 roberta-base\nTable 7 The experimental results of transformers-sklearn and transformers in four medical NLP tasks (mode_type = “albert”)\na The value of Macro F1, where the bolded indicates the best performance\nb The value of Person correlation, where the bolded inidcates the best performance\nName Score Second Lines of code Pre-trained model\nOurs Transformers Ours Transformers Ours Transformers\nTrialClassification 0.7142a 0.4504a 1062 1068 38 246 albert_chinese_base\nBC5CDR 0.8422a 0.8523a 444 492 41 309 albert-base-v2\nDiabetesNER 0.6196a 0.6436a 1122 1253 63 309 albert_chinese_base\nBIOSSES 0.1892b 0.4394b 12 11 41 246 albert-base-v2\nFig. 2 The code for BIOSSES within transformers-sklearn \nPage 7 of 8\nYang et al. BMC Med Inform Decis Mak  2021, 21(Suppl 2):90\n \nobtained preferable performance while using much less \ncode and comparable running time.\ntransformers-sklearn is based on transformers. We \nwrapped the powerful functions implemented by trans -\nformers and made them transparent to users. trans -\nformers-sklearn is also based on scikit-learn, which is \npopularly used in machine learning fields. Thus, the tech-\nnique advantages of both scikit-learn and transformers \nwere integrated in our toolkit.\nConclusions\nIn this paper, three Python classes including BERTolo -\ngyClassifier, BERTologyNERClassifier and BERTologyRe -\ngressor and three methods of each class were developed \nin transformers-sklearn. To validate the effectiveness of \ntransformers-sklearn, we applied the toolkit in four mul -\ntilingual medical NLP tasks. The results showed that \ntransformers-sklearn could effectively address the NLP \nproblems in both Chinese and English if the pre-trained \ntransformer-based model supported the language. The \ncode and tutorials of transformers-sklearn are available at \nhttps ://doi.org/10.5281/zenod o.44538 03.\nIn future work, a keep-updating transformers_sklearn \ntoolkit that combines flexibility and usability will be \nreleased, with supporting a wide range of medical lan -\nguage understanding tasks.\nAvailability and requirements\nThe datasets and software supporting the results of this \narticle are available in the trueto/transformers_sklearn \nrepository.\nProject name: transformers-sklearn\nProject home page: https  ://doi.org/10.5281/zenod \no.44538 03\nOperating system(s): Windows/Linux/Mac OS\nProgramming language: Python\nOther requirements: PyTorch\nLicense: Apache License 2.0\nAbbreviation\nBERT: Bidirectional Encoder Representations from Transformers.\nAcknowledgements\nThe authors would like to thank the open-source contributors for their early \nwork on transformers and scikit-learn, so that transformers-sklearn can be \ndesigned and implemented.\nAbout this supplement\nThis article has been published as part of BMC Medical Informatics and Deci-\nsion Making Volume 21, Supplement 2 2021: Health Big Data and Artificial \nIntelligence. The full contents of the supplement are available at https ://\nbmcme dinfo rmdec ismak .biome dcent ral.com/artic les/suppl ement s/volum \ne-21-suppl ement -2.\nAuthors’ contributions\nJL conducted the work. FY and JL designed the architecture of the proposed \ntoolkit. FY implemented transformers-sklearn. FY, XW and JL analysed the \nresults. FY, XW, HM and JL wrote and revised the manuscript. All authors read \nand approved the final manuscript.\nFunding\nThis work has been supported by the Beijing Natural Science Foundation \n(Grant No. Z200016), CAMS Innovation Fund for Medical Sciences (CIFMS) \n(Grant No. 2018-I2M-AI-016), and the National Natural Science Foundation of \nChina (Grant No. 61906214).\nAvailability of data and materials\nThe datasets and software supporting the results of this article are available \nin the trueto/transformer-sklearn repository, https ://doi.org/10.5281/zenod \no.44538 03.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 21 February 2021   Accepted: 1 March 2021\nPublished: 30 July 2021\nReference\n 1. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser \nU, Polosukhin I. Attention is all you need. In: NIPS’17. Red Hook, NY, USA; \n2017, p. 6000–6010.\n 2. Devlin J, Chang M, Lee K, Toutanova K. BERT: pre-training of deep bidi-\nrectional transformers for language understanding. In: NAACL-HLT:2019; \n2019.\n 3. Liu Y, Ott M, Goyal N, et al. RoBERTa: aA robustly optimized BERT pretrain-\ning approach. In: ArXiv 2019, abs/1907.11692.\n 4. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, Cistac P , Rault \nT, Louf R, Funtowicz M et al. HuggingFace’s transformers: state-of-the-art \nnatural language processing. ArXiv 2019, abs/1910.03771.\n 5. Gardner M, Grus J, Neumann M, Tafjord O, Dasigi P , Liu NF, Peters ME, \nSchmitz M, Zettlemoyer L. AllenNLP: a deep semantic natural language \nprocessing platform. ArXiv 2018, abs/1803.07640.\n 6. Akbik A, Blythe D, Vollgraf R. Contextual string embeddings for sequence \nlabeling. In: COLING2018:27th international conference on computa-\ntional linguistics; 2018, p. 1638–1649.\n 7. Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blon-\ndel M, Müller A, Nothman J, Louppe G et al. Scikit-learn: machine learning \nin python. ArXiv 2012, abs/1201.0490.\n 8. Lemaitre G, Nogueira F, Aridas CK. Imbalanced-learn: a python toolbox to \ntackle the curse of imbalanced datasets in machine learning. ArXiv 2016, \nabs/1609.06570\n 9. Szymański P , Kajdanowicz T. A scikit-based Python environment for \nperforming multi-label classification. ArXiv 2017, abs/1702.01460 .\n 10. Löning M, Bagnall A, Ganesh S, Kazakov V, Lines J, Király FJ. sktime: A \nUnified Interface for Machine Learning with Time Series. ArXiv 2019, \nabs/1909.07872.\n 11. de Vazelhes W, Carey CJ, Tang Y, Vauquier N, Bellet A. metric-learn: Metric \nLearning algorithms in python. ArXiv 2019, abs/1908.04710.\n 12. Zhao Z, Chen H, Zhang J, Zhao X, Liu T, Lu W, Chen X, Deng H, Ju Q, Du \nX. UER: An Open-source toolkit for pre-training models. In: Proceedings \nof the 2019 conference on empirical methods in natural language pro-\ncessing and the 9th international joint conference on natural language \nprocessing (EMNLP-IJCNLP): system demonstrations: 1990–11–01 2019; \nHong Kong, China: Association for Computational\nPage 8 of 8Yang et al. BMC Med Inform Decis Mak  2021, 21(Suppl 2):90\n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 13. Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov R, Le QV. XLNet: general-\nized autoregressive pretraining for language understanding. ArXiv 2019, \nabs/1906.08237.\n 14. Lample G, Conneau A. Cross-lingual Language Model Pretraining. ArXiv \n2019, abs/1901.0729.\n 15. Sanh V, Debut L, Chaumond J, Wolf T. DistilBERT, a distilled version of \nBERT: smaller, faster, cheaper and lighter. ArXiv 2019, abs/1910.01108.\n 16. Lan Z, Chen M, Goodman S, Gimpel K, Sharma P , Soricut R. ALBERT: a Lite \nBERT for self-supervised learning of language representations. ArXiv 2019, \nabs/1909.11942.\n 17. NumPy. https ://numpy .org/. Accessed 21 Aug 2020\n 18. pandas: Python data analysis library. https ://panda s.pydat a.org/index \n.html. Accessed 21 Aug 2020\n 19. Google Research.GitHub Repository. https ://githu b.com/googl e-resea \nrch/bert. Accessed 21 Aug 2020\n 20. CHIP: Short text classification for clinical trial screening criteria. http://\nwww.cips-chip.org.cn:8088/evalu ation . Accessed 21 Aug 2020\n 21. Wei C, Peng Y, Leaman R, Davis AP , Mattingly CJ, Li J, Wiegers TC, Lu Z. \nOverview of the BioCreative V chemical disease relation (CDR) task. In: \nProceedings of the fifth biocreative challenge evaluation workshop:2015; \n2015: 154–166.\n 22. Cloud A: Alibaba Cloud Labeled Chinese Dataset for diabetes. https ://\ntianc hi.aliyu n.com/datas et/dataD etail ?dataI d=22288 . Accessed 21 Aug \n2020\n 23. Soğancıoğlu G, Öztürk H, Özgür A. BIOSSES: a semantic sentence \nsimilarity estimation system for the biomedical domain. Bioinformatics. \n2017;33(14):i49–58.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7873055338859558
    },
    {
      "name": "Transformer",
      "score": 0.7138601541519165
    },
    {
      "name": "Python (programming language)",
      "score": 0.7021831274032593
    },
    {
      "name": "Language model",
      "score": 0.5479311347007751
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49912118911743164
    },
    {
      "name": "Natural language processing",
      "score": 0.49568426609039307
    },
    {
      "name": "Machine learning",
      "score": 0.33252161741256714
    },
    {
      "name": "Programming language",
      "score": 0.20457598567008972
    },
    {
      "name": "Engineering",
      "score": 0.10076436400413513
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}