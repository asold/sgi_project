{
    "title": "Relation Transformer Network",
    "url": "https://openalex.org/W3016897814",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5047843370",
            "name": "Rajat Koner",
            "affiliations": [
                "Ludwig-Maximilians-Universität München"
            ]
        },
        {
            "id": "https://openalex.org/A5050040424",
            "name": "Poulami Sinhamahapatra",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A5074808403",
            "name": "Volker Tresp",
            "affiliations": [
                "Siemens (Germany)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2095705004",
        "https://openalex.org/W3034538190",
        "https://openalex.org/W2250378130",
        "https://openalex.org/W3025726122",
        "https://openalex.org/W2883170015",
        "https://openalex.org/W2904993015",
        "https://openalex.org/W3001555892",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2963314968",
        "https://openalex.org/W2963683498",
        "https://openalex.org/W2077069816",
        "https://openalex.org/W2963514444",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2963109634",
        "https://openalex.org/W2963902384",
        "https://openalex.org/W2479423890",
        "https://openalex.org/W2808399177",
        "https://openalex.org/W2962737704",
        "https://openalex.org/W2963649796",
        "https://openalex.org/W2558535589",
        "https://openalex.org/W3036869132",
        "https://openalex.org/W2943248951",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2963449176",
        "https://openalex.org/W1773149199",
        "https://openalex.org/W2605736949",
        "https://openalex.org/W3108230874",
        "https://openalex.org/W2963536419",
        "https://openalex.org/W2799215407",
        "https://openalex.org/W2886970679",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W1921523184",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2607855566",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W2963518342",
        "https://openalex.org/W2763058042",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2579549467",
        "https://openalex.org/W2950096400",
        "https://openalex.org/W2519887557",
        "https://openalex.org/W2591644541",
        "https://openalex.org/W3035017890",
        "https://openalex.org/W3040509138",
        "https://openalex.org/W2963938081",
        "https://openalex.org/W2963534356",
        "https://openalex.org/W2914694625",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W3180010910",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2987123286",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W2964284374"
    ],
    "abstract": "The extraction of a scene graph with objects as nodes and mutual relationships as edges is the basis for a deep understanding of image content. Despite recent advances, such as message passing and joint classification, the detection of visual relationships remains a challenging task due to sub-optimal exploration of the mutual interaction among the visual objects. In this work, we propose a novel transformer formulation for scene graph generation and relation prediction. We leverage the encoder-decoder architecture of the transformer for rich feature embedding of nodes and edges. Specifically, we model the node-to-node interaction with the self-attention of the transformer encoder and the edge-to-node interaction with the cross-attention of the transformer decoder. Further, we introduce a novel positional embedding suitable to handle edges in the decoder. Finally, our relation prediction module classifies the directed relation from the learned node and edge embedding. We name this architecture as Relation Transformer Network (RTN). On the Visual Genome and GQA dataset, we have achieved an overall mean of 4.85% and 3.1% point improvement in comparison with state-of-the-art methods. Our experiments show that Relation Transformer can efficiently model context across various datasets with small, medium, and large-scale relation classification.",
    "full_text": "Relation Transformer Network\nRajat Koner · Suprosanna Shit · Volker Tresp\nPreprint : under review\nAbstract The extraction of a scene graph with ob-\njects as nodes and mutual relationships as edges is the\nbasis for a deep understanding of image content. De-\nspite recent advances, such as message passing and joint\nclassiﬁcation, the detection of visual relationships re-\nmains a challenging task due to sub-optimal exploration\nof the mutual interaction among the visual objects.\nIn this work, we propose a novel transformer formu-\nlation for scene graph generation and relation predic-\ntion. We leverage the encoder-decoder architecture of\nthe transformer for rich feature embedding of nodes\nand edges. Speciﬁcally, we model the node-to-node in-\nteraction with the self-attention of the transformer en-\ncoder and the edge-to-node interaction with the cross-\nattention of the transformer decoder. Further, we intro-\nduce a novel positional embedding suitable to handle\nedges in the decoder. Finally, our relation prediction\nmodule classiﬁes the directed relation from the learned\nnode and edge embedding. We name this architecture\nas Relation Transformer Network (RTN). On the Visual\nGenome and GQA dataset, we have achieved an overall\nmean of 4.85% and 3.1% point improvement in com-\nparison with state-of-the-art methods. Our experiments\nshow that Relation Transformer can eﬃciently model\ncontext across various datasets with small, medium,\nand large-scale relation classiﬁcation.\nR. Koner\nLudwig Maximilian University of Munich, Munich, Germany\nE-mail: koner@dbs.iﬁ.lmu.de\nS. Shit\nTechnical University of Munich, Munich,Germany\nE-mail: suprosanna.shit@tum.de\nV. Tresp\nSiemens AG, Munich, Germany\nE-mail: volker.tresp@siemens.com\nKeywords Scene Graph, Scene Understanding, Visual\nRelation Detection, Transformer\n1 Introduction\nA scene graphis a graphical representation of an image\nconsisting of multiple entities and their relationships,\nexpressed in a triplet format like ⟨subject, predicate, ob-\nject⟩. Objects in the scene become nodes, undirected in-\nteractions between nodes are represented by edges and\na directed edge is called a relationship or predicate.\nE.g. in Fig. 1, ‘Eye’,‘Hair’,‘Head’, and ‘Man’ are object\nor node labels and their mutual relationships are de-\nscribed by the predicates ‘has’ and ‘on’. An extracted\nscene graph can be used in many downstream applica-\ntions like visual question answering [8, 11, 16], image\nretrieval [33], and image captioning [46].\nA scene graph generation (SGG) task is executed\nin two steps: ﬁrst, objects present in the image are de-\ntected, and second, the most suitable predicates are de-\ntermined for selected object pairs. Current object de-\ntection approaches have achieved outstanding perfor-\nmance in spatially locating objects in an image. In con-\ntrast, performance on relation prediction is still quite\nlimited. Several recent works have tried to explore SGG\nfrom diﬀerent perspectives. Context information is ex-\nchanged either globally [51] or across neighborhoods\n[45, 47]. [28, 54] introduced a variant of contrastive loss\nfor a better representation of similar types of relations\nand some contemporary work [22] improved message\npassing with direction and priority sensitive loss. [15]\naddresses speciﬁcally the biases in diﬀerent datasets. In-\nteractions among objects and their corresponding edges\nthrough contextual attention for SGG, the topic of this\npaper, is still under-explored. This paper proposes a\narXiv:2004.06193v2  [cs.CV]  20 Jul 2021\n2 Rajat Koner et al.\nnovel transformer-based formulation for the SGG task,\nnamely Relation Transformer network (RTN). In the\nfollowing, we rationalize the self-attention and cross-\nattention of the proposed RTN in context of SGG task.\nFirst, it is crucial to understand the role of each\nobject in an image and how object labels are related\nand inﬂuenced by others in the context of the whole\nimage. For example, in Fig. 1, the presence of node la-\nbels like ‘Eye’, ‘Hair’, ‘Nose’, ‘Ear’ indicate that these\ntogether describe a face and indicate the presence of\na node with label ‘Face’ or ‘Head’ in their surround-\nings. Additionally, the node label ‘Shirt’ implies that\nthis is a face or head of a ‘Human’ and not an ani-\nmal. The spatio-semantic co-occurrence of labels along\nwith the contextualization of nodes are both important\nfor predicting pairwise relations and node labels. To\nobtain “context aware nodes”, we have modeled inter-\nactions among nodes using self-attention of the trans-\nformer [39], which in this case is a node-to-node (N2N)\nattention propagation.\nSecond, a subsequent challenge is to predict the ex-\nact relationship between two objects. In this paper, we\nassume that node labels and edge labels are mutually\ndependent and predicted by scene context. For example,\nin Figure 1, the probabilities for edges between the node\n‘Man’ and its body parts like ‘Head’ ,‘Eye’ and ‘Nose’\nshould be similar because of the spatio-semantic simi-\nlarity. Thus context propagation from nodes to edges\ndirectly inﬂuences the classiﬁcation of their directed\nrelations (⟨Man, has, Head⟩, ⟨Head, of, Man⟩). Al-\nthough a directed edge label is mainly dependent on its\ntwo associated nodes, i.e., subject and object, we argue\nthat attention from all other nodes to an edge helps\nto leverage all mutually correlated relations and guides\ntowards identifying most consistent relationship in the\nglobal scene context. This edge to nodes interaction is\nmodeled using the cross-attention of the transformer\ndecoder, which we refer to as E2N attention.\nIn addition, we introduce a novel positional encod-\ning that preserves the local context of the edge. Fi-\nnally, a directed relation prediction module (RPM) 3.4\nis used to classify the relation that takes advantage of\nthe context-rich nodes and edges. To summarize our\ncontributions:\n1. We formulate the SGG task as a feature learning\non a graph leveraging transformer architecture to\nmodel object-to-object and object-to-relation inter-\naction. First, we extract object-to-object interaction\nvia the self-attention (N2N attention) of the trans-\nformer encoder.\n2. Next, we use this rich node representation to cap-\nture object-to-relation interaction via the transformer\ndecoder’s cross-attention (E2N attention). We intro-\n(a) Scene consisting of a man’s face\n(b) Corresponding scene graph\nFig. 1. (a)is an example image of a face of a man.\n(b) describes the corresponding scene graph, annotated\nwith various object labels like head, ear, shirt (color\ncoded as the respective bounding box) and their mutual\nrelationships.\nduce a novel positional encoding for the edges in the\ntransformer decoder to accumulate a global scene\nenvironment while preserving local context.\n3. We employ an eﬃcient directional relation predic-\ntion module to accumulate the learned node and\nedge representation from the transformer and clas-\nsify the desired directed relation.\n4. We perform extensive experiments in multiple datasets\nto show our proposed RTN’s generalizability and\neﬃcacy. We achieve an overall mean of 4.85% and\n3.2% improvement on the challenging Visual Genome\nand GQA dataset over the state-of-the-art models.\n2 Related Work\nScene Understanding with Language Prior:Scene\nunderstanding evolved through many phases through-\nout recent years. Initially, researchers tried to localize\nRelation Transformer Network 3\nobjects or regions in an image, based on a given cap-\ntion or text reference [12, 26, 27, 30]. These approaches\nmostly matched the referenced text to the matching\npart of the image. [14] introduced scene graphs for im-\nage retrieval, and [24], proposed visual relationship de-\ntection with language priors, and introduced an asso-\nciated dataset named VRD. [3] derived a knowledge\ngraph model from the training data labels and achieved\ngeneralization to new triples by knowledge graph fac-\ntorization approaches. Several works focused on com-\nbining the visual and other semantic features of the\nsubject, object, and predicates [24, 28, 41, 48, 52, 54]\nand enriched the features with a variant of triplet loss,\npooling, and multi-modal representations [34].\nContext in Scene Graph: Contextual information\nhas been shown to be helpful for object detection [23],\nvisual question answering [1], and scene understanding\n[27]. Recent advancements in the attention approach\nprovided an eﬃcient way to model complex interac-\ntions of entities in NLP networks [39], and convolu-\ntion networks [42]. Various recent relationship detec-\ntion networks have tried to incorporate context with\nattention or transformer [22, 43, 47, 50] or without\nattention [10, 31, 45, 51]. Although our work is also\nbased on attentional context, it diﬀers as it introduces\nthe novel N2N and E2N attention. Furthermore, like\n[51], we do not only consider interactions between mu-\ntually co-occurrence objects, but we also analyze how\nthe presence of objects or predicates jointly inﬂuences\neach other.\nTransformers in Vision:After the release of the trans-\nformer [39], it became one of the most popular ap-\nproaches for various vision [4, 17] or vision language\ntasks from natural language processing [20]. In Vision-\nLanguage pretraining tasks, BERT-style architectures\n[7] became a default choice, due to their ability to pro-\ncess sequential and also non-sequential data and, in al-\nmost all cases, it improved upon the state of the art\nresults. In [25], a two-stream network for joint vision-\nlanguage modalities has been used to obtain an en-\nhanced representation for tasks like visual question an-\nswering and image captioning. [20] uses a combination\nof sentences and image patches jointly for pretraining\nand achieved a state of the result on GQA [13] or tasks\nlike Masked Object Classiﬁcation (MOC), Visual Lin-\nguistic Matching (VLM). Recently, [4] proposed a sim-\nple end-to-end object-detection framework; similar to\nour work, it uses a transformer encoder-decoder archi-\ntecture. This recent surge of interest shows the impor-\ntance and eﬃcacy of the transformer and BERT-style\narchitectures. In comparison to similar approaches, the\nRelation Transformer in this paper gives superior per-\nformance and interpretable results; at the same time,\nit has a transparent modular architecture. An earlier\nstudy of our method can be found at [18].\n3 Method\nWe frame the SGG task as a multi-hop attention-based\ncontext-propagation problem among nodes, edges, and\ntheir joint classiﬁcation in a directed graph. This task\nis decomposed into four sub-tasks, (1) Object detection\nto get nodes (object bounding boxes) and their edges\n(union of object bounding boxes) [in Sec. 3.2]. (2) Mod-\neling of interactions between the nodes [in Sec. 3.3.1] (3)\nAccumulation of necessary context from all nodes for an\nedge [in Sec. 3.3.3] and, (4) Classiﬁcation of directed re-\nlations between the objects from the extracted context\ninformation. In the next sub-sections, we will describe\nthese sub-tasks along with a brief introduction of the\nattention mechanism of the transformer. An overview\nof the proposed Relation Transformer architecture is\nshown in Fig. 2 with nodes and edges derived from a\nrepresentative image.\n3.1 Problem Decomposition\nA scene graph G consists of a set of nodes N = {ni},\nthat represent the objects in an image and a set of la-\nbeled directed edges R ⊂N ×R×N, where Ris the set\nthe relation types. For each node ni, bi ∈R4 denotes\nthe bounding box coordinates and oi denotes the class\nlabel. We denote I as the image and B as the set of\nbounding boxes, and O as the set of object class labels.\nWith the help of this notation, a generative model for\nthe graph isPr (B|I)Pr (O|B, I)Pr (R|O, B, I). Pr(B|I)\nis inferred by our object detection module. From the\nobjects and bounding boxes we construct an undirected\nobject interaction graph [Fig. 2] with all the detected\nobjects as a nodes {ni}and possible node-pairs with\nedges {eij}. Such an undirected edge is a candidate di-\nrected relation without immediate knowledge of the di-\nrection (subject to object) and interaction label (predi-\ncate). For the sake of this paper, we assume that, if eij\nexists, there is a unique relation type. We propose to\nestimate Pr (O|B, I) by the N2N module, where other\nlabels can inﬂuence the presence of one object label.\nSubsequently, to model the relationshipsPr (R|O, B, I),\nwe ﬁrst process the candidate undirected edges {eij}\nbetween ni and nj using the E2N module and learn an\nobject interaction graph. Then the RPM predicts edge\ndirection and relation type ri→j.\n4 Rajat Koner et al.\nFig. 2. An overview of the proposed Relation Transformer architecture. The network consists of four stages:\na) Extraction of initial node (colored circle) and edge (dotted line) features using a region proposal network\n(RPN) of an object detector, b) Creation of context-rich node embeddings using N2N attention from initial node\nembeddings, c) Creation of edge embedding by accumulating context from nodes through E2N attention and\nproposed edge position embedding, d) direction aware classiﬁcation of the relation using ⟨subject, edge, objects⟩\ntriplet embedding. Best viewed in color.\n3.2 Object Detection\nWe have used Faster-RCNN [32] with a VGG-16 [35]\nbackbone for object detection. For a node ni, we obtain\nthe spatial embedding bounding box coordinates bi ∈\nR4, a visual feature vector of a region of interest vi ∈\nR4096 from the feature map Ifeature obtained from top\nlayer of VGG-16. Also, we get initial class probabilities\noinit\ni ∈RC where C is the number of classes. To exploit\nthe semantic information of the predicted class label\n(oinit\ni ), we multiply it with the GloVe embedding [29]\nof all classes to obtain the semantic features si. This\nenforces hard attention of detected class probabilities\nacross the word embedding feature space.\n3.3 Context Propagation via Transformer\nThe core concept of our approach is the eﬃcient attention-\nbased context propagation across all nodes and edges\nusing an encoder-decoder architecture implemented as\ntransformers [39]. The transformer architecture uses self-\nattention mechanisms for mapping of the global depen-\ndencies. One deﬁnes attention as the matrix\nAttention(Q, K, V) = softmax(QKT\n√dk\n)V. (1)\nwhere query (Q), keys (K), and value (V) are obtained\nthrough three learnable layers, and dk is a scaling fac-\ntor. The output is computed as a weighted sum of the\nvalues, where the weight is computed by multiplying a\nquery matrix with its corresponding key.\nIn our transformer architecture, we reason two dif-\nferent attention schemes based on our observation as\ndiscussed in the Sec. 1. We incorporate self-attention\nmodule in the encoder of our transformer that serves\nas a N2N attention. However, to model the optimal\ncontextualization from all nodes to edges, we employ\nE2N attention as the cross-attention in the decoder of\nour transformer. To exploit both global and local con-\ntext propagation in the E2N attention, we introduce\nappropriate changes in the positional encoding of the\ndecoder.\n3.3.1 Encoder N2N Attention\nContextualization of objects by exploring its surround-\nings not only enhances object detection [23], but also\nencodes more discriminate features for relation classi-\nﬁcation. For this purpose, we make a permutation in-\nvariant sequential ordering of the nodes and pass this\nnode sequence to the transformer encoder. The initial\nnode feature vector (fin\ni ) for the ith node is obtained by\nRelation Transformer Network 5\napplying a linear projection layer ( Wnode) on its con-\ncatenated features as\nfin\ni = Wnode([vi; si; bi]) (2)\nAdditionally for ith node, we added a positional feature\nvector (pos(ni)) with its initial feature fin\ni . It takes the\ncategorical position of ith node in a linear ordering of all\nnodes, and covert it into a continuous sinusoidal vector\nas described in [39].\nfﬁnal\ni = encoder(fin\ni + pos(ni)) (3)\noﬁnal\ni = argmax(Wclassiﬁer(fﬁnal\ni )). (4)\nwhere encoder is a stack of multi-head attention lay-\ners as shown in Figure 2. After the contextualization\nof the nodes by the encoder, we obtain ﬁnal node fea-\ntures fﬁnal\ni . This semantically enriched node feature is\nsubsequently used for two purposes. First, it is passed\nthrough a linear object classiﬁer Wclassiﬁer to get ac-\ncurate ﬁnal object class ( oﬁnal\ni ∈C) probability as de-\nscribed in Eq. 4 and, second, the fﬁnal\ni is passed to the\nthe decoder cross-attention (E2N attention) for edge\ncontext propagation.\n3.3.2 Decoder Edge Positional Encoding\nWe feed the edges of the undirected object interaction\ngraph to the transformer decoder along with its posi-\ntional embedding, which we refer to as Edge Queries.\nSince there is no explicit ordering among edges we pro-\nposed a novel positional embedding for edge from both\nof its node position. The new positional encoding vector\n(poseij ∈R2048) for edges ( eij), encodes the position of\nboth the source nodes in an interleaved manner. One of\nthese nodes will play the roles of either subject or ob-\nject. Since our edge is undirected, we hypotheses that\nour proposed edge positional embedding will be helpful\nfor the network to distinguish the source nodes (subject\nor object) out of all distinct nodes. The goal is to accu-\nmulate the necessary global context (all distinct object\ninstances) without losing its focus on the local context\n(subject or object nodes). We deﬁne,\nposeij (k, k+ 1) =\n[\nsin\n( pi\nm\n2k\nd\n)\n, cos\n( pi\nm\n2k\nd\n)]\nposeij (k + 2, k+ 3) =\n[\nsin\n( pj\nm\n2k\nd\n)\n, cos\n( pj\nm\n2k\nd\n)]\n.\n(5)\nEq. 5 describes positional encoding for an edge, where\npi and pj are the positions of the nodes ni and nj, m is\nmaximum number of sequence of nodes, d = 2048, and\nk denotes the kth position in the positional encoding\nfeatures vector.\n3.3.3 Decoder E2N Attention\nFor an edge eij of the Edge Queries, (between nodes ni\nand nj), its bounding box location (bij ∈R4) and initial\nvisual features vij ∈R4096 are derived from the union of\nthe bounding boxes of both nodes as shown in Figure 2.\nWe concatenate the GloVe vector embedding from both\nof its node labels sij with the previously obtained box\nand visual features for the semantic enrichment of the\nedge. Subsequently, a linear projection layer ( Wedge) is\nused to obtain the initial edge feature vector ( fin\nij ) or\nthe Edge Queries as\nfin\nij = Wedge([vij; sij; bij]). (6)\nWe argue that a well contextualized edge is needed\nfor complex global scene representation and this can\nonly be achieved if the edge exploits an larger scene\ncontext. In traditional transformer decoder, a masked\nattention is used, limiting the edges attention only to a\npart of the sequence. The accumulation of global con-\ntext for an edge requires a unique mechanism so it\ncan preserve its local dependency while exploring global\ncontext.\nEmpirically we found that applying self-attention\nbetween edges does not help, as necessary context can\nbe accumulated using N2N and E2N attention. See also\nTable 6. Hence, we have removed the edge-to-edge self-\nattention in our decoder. At ﬁrst, E2N cross-attention\nhas been applied from an edge to all the nodes. Finally,\nwe get the contextual edge features ( fﬁnal\nij ∈R2048) as,\nfﬁnal\nij = decoder(fin\nij + poseij , fﬁnal\ni=1..N ). (7)\nwhere decoder is a stack of multi-head attention with\nour proposed E2N attention, positional encoding.\n3.4 Directed Relation Prediction Module (RPM)\nRelation is a directional property, i.e.,subject and object\ncannot be exchanged. After obtaining the context rich\nnode and edge embeddings, an initial directed relational\nembedding (relin\ni→j ∈R2048×3+512) has been created as\nrelin\ni→j = [fﬁnal\ni ; fﬁnal\nij ; fﬁnal\nj ; GAP(Ifeature )]. (8)\nwhere GAP(Ifeature ) ∈R512 is the global average pool\nof the image feature obtained from object detector. As\na next step, relin\ni→j is passed through a sequential block\nof neural networks, which we call directed relation pre-\ndiction module or RPM . RPM takes (relin\ni→j) as input\nand the result is normalized with layer norm [2] followed\nby two blocks of linear layers and Leaky ReLU [44] non-\nlinearity for the predicate classiﬁcation as described in\n6 Rajat Koner et al.\nEq. 8. Details on RPM can be found in the supple-\nmentary material. We postulate that a larger embed-\nding space ( RPM ), with normalized embeddings from\nnodes and edges, will eﬀectively combine the necessary\ncontext, and\nrelﬁnal\ni→j = RPM (relin\ni→j). (9)\nFinally, we get the softmax distribution over all p pred-\nicate categories from the ﬁnal relation vector ( relﬁnal\ni→j ∈\nR2048) through a linear layer ( Wp). Note that p is the\nnumber of relation present in the dataset. We have also\nadded the frequency baseline ( fq) from [51] to model\nthe dataset bias and obtain as\nPr (ri→j|I) = softmax(Wprelﬁnal\ni→j ) + fq(i, j)). (10)\nThe Pr (ri→j|I) denotes the ﬁnal relationship distribu-\ntion among ni and nj for a given image I.\n4 Experiments\nThis section will describe the dataset and explain im-\nplementation details of our network pipeline and spatial\nembedding implementation. 1\n4.1 Datasets\nWe have used three most commonly used scene graph\ndataset, i.e., Visual Genome [19], GQA[13] and VRD\n[24], for our experimental evaluation.\nVisual Genome (VG)is one of the most challeng-\ning datasets for scene graph detection and generation\nfor real world images. The original dataset consists of\n108,077 images with annotated object bounding boxes,\nclass, and binary relations among the objects. The an-\nnotations are quite noisy: e.g., multiple bounding boxes\nare provided for a single object. To alleviate this prob-\nlem, Lu et al. [45] proposed a reﬁned version of the\ndataset, which consists of the most frequently occur-\nring 150 objects and 50 relationships. To have a fair\ncomparison with most of the present state of art model\n[10, 28, 51, 53, 54] we have used this reﬁned dataset.\nAlso, our train (55K), validation (5K), and test (26K)\nsplit are the same as per the dataset.\nGQA is one of the largest and diverse scene graph\ndatasets consisting of 1704 classes and 311 relationship\nlabels as proposed in [13]. It uses the same images from\nVisual Genome[19] with more clean (e.g., more accu-\nrate spatial location) and normalizes class and relation-\nship distribution. GQA is more challenging than other\n1 Code is available at:https://github.com/rajatkoner08/\nrtn\ndatasets as each image is annotated with a dense scene\ngraph and a large number of relations. We have used\nK-fold data for training and report our result on “val”\nset mentioned in GQA. We omitted the classwise fre-\nquency distribution [51] for GQA for two reasons, ﬁrst\nGQA is more normalized than VG, and second due to\na large number of classes and relationships present in\nGQA incurred a large memory overhead.\nVRD contains 4000 training and 1000 test images with\n100 objects and 70 predicate categories. We evaluate\nour model with the same COCO pretrained backbone\nas used in [54]. The evaluation metric is the same as [24]\nthat report R@50 and R@100 metric for relationship,\nphrase, and predicate detection.\n4.2 Implementation Details\nWe have implemented our model in PyTorch and trained\nit in a single Nvidia RTX 3900 GPU. We have trained\nthe network for 20 epochs; it took approximately two\ndays to train. The input to our model is an image with\na size of 592 ×592 pixels, as in [51]. As mentioned be-\nfore, the encoder and the decoder modules accept input\nfeatures of size 2048. We have used 3 encoder layers,\n2 decoder layers and 12 attention heads for our net-\nwork. Our model is optimized by SGD with momen-\ntum. A learning rate of 10 −3 and batch size of 16 has\nbeen used. We have used cross-entropy loss for both\nof our object and relation classiﬁcation loss. 2 In train-\ning, we used one foreground edge (contain at least one\nground truth relation) for 4 background edges (without\nany relation), and randomly ﬂip some images as part of\ndata augmentation. We have followed the same evalua-\ntion as in current benchmarks [54] and computed scene\ngraph classiﬁcation (SGCLS) and predicate classiﬁca-\ntion (PREDCLS).\nFor scene graph detection (SGDET), we have taken\nthe top 64 object label proposals from the object de-\ntector for each image after performing the non-maximal\nsuppression (NMS) with intersection over union (IoU)\nof 0.3, as similar to [51]. To reduce computational load\nin relation classiﬁcation, we have only considered those\npairs of nodes whose bounding boxes are overlapping.\nTo have a fair comparison on evaluating on visual\ngenome, we have used Faster-RCNN [32] with VGG16\n[35] backbone pretrained on visual genome dataset as\nper [51, 54]. We have trained Mask-RCNN[9] on GQA\nfor the backbone object detector as per [37].\n2 List of all hyper-parameters are given in the supplemen-\ntary material.\nRelation Transformer Network 7\nModel w/ Graph Constraint w/o Graph Constraint\nMeanSGDET SGCLS PRDCLS SGCLS PRDCLS\nR@ 20 50 100 20 50 100 20 50 100 50 100 50 100\nIMP [45] 14.6 20.7 24.5 31.7 34.6 35.4 52.7 59.3 61.3 43.4 47.2 75.2 83.6 44.93\nA. Emb. [28] 6.5 8.1 8.2 18.2 21.8 22.6 47.9 54.1 55.4 26.5 30.0 68.0 75.2 34.04\nFreq. [51] 17.7 23.5 27.6 27.7 32.4 34.0 49.4 59.9 64.1 40.5 43.7 71.3 81.2 44.07\nMotifNet [51] 21.4 27.2 30.3 32.9 35.8 36.5 58.5 65.2 67.1 44.5 47.7 81.1 88.3 49.50\nCMAT [5] 22.1 27.9 31.2 35.9 39.0 39.8 60.2 66.4 68.1 48.6 52.0 82.2 90.1 50.97\nVRU [53] 20.7 27.9 32.5 36.0 36.7 36.7 66.8 68.4 68.4 - - - - 52.16\nKREN [6] - 27.1 29.8 - 36.7 37.4 - 54.2 59.1 45.9 49.0 81.9 88.9 51.00\nReIDN [54] 21.1 28.3 32.7 36.1 36.8 36.8 66.9 68.4 68.4 48.9 50.8 93.8 97.8 52.83\nGPS Net [22] 22.3 28.9 33.2 41.8 42.3 42.3 67.6 69.7 69.7 - - - - 46.42\nRTN (Ours) 22.5 29.0 33.1 43.8 44.0 44.0 68.3 68.7 68.7 61.3 62.3 97.2 99.1 57.1\nTable 1.Comparison of our model with state-of-the-art methods tested on Visual Genome [19]. We have computed\nmean performance, as some of the works did not provide all scores. Evaluation with ‘Graph constraint’allows only\none relation between an object pair, and “No Graph constraint” allows multiple relations between an object pair.\nModel SGCLS PREDCLS Mean\nR@ 20 50 100 20 50 100.\nIMP[45] 6.3 9.4 11.2 47.3 69.8 81.4 37.6\nNeural Motif[51] 6.5 9.9 11.9 51.2 73.6 84.2 39.6\nUnbiased TDE[37] 5.8 8.8 10.6 51.6 74.0 84.6 39.2\nRTN(ours) 9.2 11.9 12.2 55.3 78.2 88.142.3\nTable 2.Comparison of our model on the GQA dataset\nfor predicate and scene graph classiﬁcation. Mask-\nRCNN[9] is used as backbone object detector ﬁne tuned\nin GQA.\nModel SGCLS PREDCLS\nmR@ 100 100\nVG\nIMP [45] 6.0 10.5\nFREQ [51] 8.5 16.0\nMotifNet[51] 8.2 15.3\nKERN[6] 10.0 19.2\nVCTREE-HL[38] 10.8 19.4\nGPS-Net[38] 12.6 22.8\nRTN (Ours) 12.6 20.3\nGQA\nIMP [45] 0.5 2.2\nMotifNet[51] 0.8 2.9\nUnbiased TDE[37] 0.7 2.8\nRTN (Ours) 1.4 4.5\nTable 3.Comparison on the mean-Recall@100 metric\nbetween various methods across all the 50 and 311 rela-\ntionship categories for VG and GQA correspondingly.\nModel Relation Phrase\nDetection Detection\nR@ 50 100 50 100\nVTransE [52] 19.4 22.4 14.1 15.2\nVip-CNN [21] 17.3 20.0 22.8 27.9\nKL distilation[49] 19.2 21.3 23.1 24.0\nZoom-Net[48] 18.9 21.4 24.8 24.1\nRelDN* [54] 25.3 28.6 31.3 36.4\nGPS-Net* [22] 27.8 31.7 33.8 39.2\nRTN (Ours)* 28.1 32.0 33.5 38.7\nTable 4.Comparison of our model on the VRD dataset\nfor relation detection and Phrase Detection. * uses the\nsame detector pretrained on COCO.\n5 Results and Discussion\n5.1 Quantitative Results\nFor VG dataset, Table 1 shows the performance of our\nmethod in comparison with other methods. It clearly\ndemonstrates that our novel context propagation for\nboth objects and edges signiﬁcantly improves most of\nthe performance metrics. Note that in training, we only\nused simple cross-entropy loss in contrast to the recent\nliterature, such as, contrastive or triplet loss (ReIDN\n[54], VRU [53]) and Node-Priority-Sensitive (NPS) loss\n([22]). As shown under ‘No graph constraint’ accuracy\nof 99.1% R@100 in PREDCLS indicates, our model is\nsuperior than the competing models in learning the\nmost likely relations. One of the key observations we\nmade from Table 1 is that our approach signiﬁcantly\nimproves on the R@20 metric, compared to previous\nstate of the art RelDN [54] with maximum improve-\nment of 7.7% on SGCLS. Our model achieves slightly\nlower score than our contemporary [22] work on R@50\n8 Rajat Koner et al.\nand R@100 for PREDCLS. We suspect that the fre-\nquency softening method by [22] better captures the\ndata imbalance and long tailed distribution of relation-\nship, than ours in those cases. However, in this study\nwe are not focusing on loss function instead of looking\ndeep into context propagation. And hence, we leave the\nclass imbalance study for future work.\nTable 2, shows the performance of RTN on GQA\ndataset. Here RTN outperforms all competitive mod-\nels by mean 3.1%. Compare to VG, GQA contains six\ntimes more relationship classes and three times more\ndense scene graph annotation. Thus GQA is more chal-\nlenging for eﬃcient propagation of context that depicts\nthe full scene. As GQA has more normalized relation-\nship distribution, class wise frequency bias is nonessen-\ntial. Our proposed transformer based multi-hop con-\ntext propagation is particularly well suited for extract-\ning important context of a given edge query out of\nlarge number of nodes and edges. Hence, performance\non GQA shows our RTN is superior over the state-of-\nthe-art methods. Especially performance on “PRED-\nCLS” depicts the eﬀectiveness of our model on utiliz-\ning proper context information in the presence of large\nnumber of nodes or edges or in a dense scene graph.\nIn order to gain deeper insight on class imbalance,\nwe computed mean-recall that takes the mean across\nall the relationship classes. Table 3 shows the perfor-\nmance of our model with other competing methods for\nthe mean recall. Our RTN performs better than most\nof them in “SGCLS” and “PREDCLS”. However, the\nperformance on “PREDCLS” R@100 in VG dataset is\nmarginally lower compared to [22] because of the class\nimbalance. Since GQA is more normalized than VG,\nthus it has less prominent class imbalance problem. We\nhave achieved state of the art result on mean recall\nacross all metrics on GQA.\nTable 4, shows the performance of our network on\nthe VRD dataset; for a fair comparison, we have used\nthe same object detector as in [54]. Here also, we have\nperformed better than or on par with other state-of-\nthe-art models on relations classiﬁcation. Transformer\nbased architectures are reported to require a large num-\nber of training data, but despite of relatively small sam-\nple size of VRD, our node and edge based context prop-\nagation helps RTN for an eﬃcient accumulation of scene\ncontext.\n5.2 Qualitative Results\nFigure. 3, shows a positive example with N2N and E2N\nattention. Here N2N attention shows how the presence\nof an object inﬂuence other like boy-door, child-umbrella\netc. E2N attention heat map shows the importance of\nsurrounding nodes for a particular edge (e.g child, boy\nplay an important role in most of relation as they are\nsame semantic object ). More positive and negative ex-\namples can be found in the supplementary section.\nAdditionally, an analysis of the errors of the Re-\nlation Transformer network provides insight into what\nthe network has learned. For example, ‘on’ is the most\nmispredicted relation in our evaluation settings for Vi-\nsual Genome. The relationship ‘on’ is falsely predicted\nas ‘of’ for 56.9% out of total false predictions among\nall relationships. Interestingly, for most of the exam-\nples like ‘ Face, of, Woman’ is more appropriate than\n‘Face, on, Woman ’ , indicating that the network is\nnot necessarily failing to predict correctly, rather it is\npredicting more suitable or semantically similar rela-\ntionships. A probable reason for these false predictions\nis the signiﬁcant bias in the training dataset.\n5.3 Ablation Study\nIn the following, we present the results of three abla-\ntion experiments in order to demonstrate the eﬃcacy\nof our proposed modules and the selection of hyper-\nparameters on VG dataset.\n5.3.1 Encoder-Decoder Layers and RPM\nFirst, we compare the performance of the N2N and E2N\nmodules for varying number of layers in combination\nwith the RPM. The results are presented in the follow-\ning Table 5. We make the following observations.\n#N2N #E2N RPM PREDCLS\nR@20 R@50 R@100\n1 1 yes 66.7 67.1 67.1\n1 2 yes 68.0 68.4 68.4\n2 1 yes 67.4 67.8 67.8\n2 2 yes 68.1 68.5 68.5\n3(ours) 2(ours) yes 68.3 68.7 68.7\n3 2 no 67.8 68.2 68.3\n3 3 yes 68.2 68.5 68.5\nTable 5. Ablation study of our proposed N2N, E2N\nand relation prediction module on Visual Genome for\nPREDCLS.\n1. All nodes need to be contextualized well enough, in\norder to propagate context for edges. This is demon-\nstrated in rows 4 and 5 of Table 5. Adding more\nedge context layers without adequate object context\npropagation limits the performance. Thus, it shows\nthe importance of contextualized objects.\nRelation Transformer Network 9\n(a) Image\n (b) N2N Attention heatmap\n (c) E2N Attention heatmap\n (d) Generated scene graphs\nFig. 3.A positive example outputs from our network with associated attention map and scene graph.\n2. After objects are properly contextualized, adding\nthe optimal number of edge-context modules on top\nof object-context modules increases the performance.\nJointly it shows the importance of both modules\n(rows 4 and 5).\n3. If we use a large number ( ⩾ 3) of E2N and N2N\nlayers, then the performance decreases. One of the\npossible reason is that, as the network size grows, it\nbecomes hard to optimize the network (row 7).\n4. Directed RPM facilitates the forming of the neces-\nsary embedding space after completion of optimal\ncontextualization of nodes and edges. If we replace\nthe RPM module with a simple linear layer without\nany normalization, it hurts the performance (row 6).\nFrom these observations, we can clearly infer that all\nthree modules have signiﬁcantly contributed to rela-\ntionship classiﬁcation.\n5.3.2 Decoder E2N Attention and Positional Encoding\nNext, we present necessary ablation experiments in sup-\nport of the beneﬁts of the proposed changes in the de-\ncoder architecture as discussed in Section 1. We have\nconducted three experiments to understand the impact\nof these changes as presented in Table 6. We make the\nfollowing observations.\nModel PRDCLS\nR@ 20 50 100\nwith [39]’s decoder 66.5 67.1 67.1\nwith only E2N attention 67.6 68.0 68.0\nwith only E2N attention\n+ proposed PE (RTN) 68.3 68.7 68.7\nTable 6. Ablation study of our proposed changes in\noriginal transformer decoder [39], like changes in posi-\ntional encoding, removal of decoder-decoder attention\non Visual Genome dataset.\n1. With [39]’s decoder:shows the network performance\nwithout any modiﬁcation from [39]’s transformer de-\ncoder. In this setting, at ﬁrst, the decoder applies\nattention across all edges (decoder self-attention),\nafterward, from edge to all nodes (decoder-encoder\ncross attention or ours E2N attention). However,\nfor SGG task, primary context of an edge should\ncome from the immediate and neighboring nodes\n(E2N attention). Hence, self-attention before cross-\nattention, as in [39]’s decoder, hinders node to edge\ncontext propagation and limits performance.\n2. With only E2N attention: shows our network per-\nformance with only E2N attention (without self-\nattention in the decoder) and position-wise feed for-\nward network. Here we didn’t change the positional\nencoding for the decoder. Removing the self-attention\nin decoder, improves the performance. One possible\nreason for this could be easy accumulation of nec-\nessary context from the nodes needed for inferring\nrelation of the image.\n3. With only E2N attention + proposed PE (RTN):\nshows our Relation Transformer model. Here we have\napplied our proposed positional encoding, and the\nresults show that the preservation of the source node\nidentity of an edge helps the network eﬃciently ac-\ncumulate the global context without losing the local\ncontext.\nOur experiment shows that our design changes in the\noriginal transformer architecture are eﬀective and help-\nful for relation classiﬁcation.\n5.3.3 Transformer and RPM Feature Space\nFinally, we report results on ablation studies to ana-\nlyze the impact of feature embeddings in transformer\ninputs i.e., semantics or GloVE and spatial embedding.\nAdditionally, we perform ablation on the RPM feature\nspace i.e., global average pooling and frequency bias.\nBoth the ablation studies are presented in Table 7. We\nmake the following observations.\n10 Rajat Koner et al.\nModel PRDCLS\nR@ 20 50 100\nwithout word embedding 67.7 68.1 68.1\nwithout spatial embedding 68.2 68.6 68.6\nwithout Global Avg Pool 68.0 68.4 68.4\nwithout Freq Bias 66.7 67.0 67.0\nwith every features 68.3 68.7 68.7\nTable 7. Ablation study of our proposed changes in\noriginal transformer decoder [39], like changes in posi-\ntional encoding, removal of decoder-decoder attention\non Visual Genome dataset.\n1. Without word embedding, we see a decrease in re-\ncall. We attribute this to the fact that word em-\nbedding provides additional semantic cues based on\nthe correlated object classes, which is helpful for the\nSGG task.\n2. Without spatial embedding of the bounding box co-\nordinates, the performance suﬀerers, but the drop\nis marginal. For an accurately known global context\nof an object, spatial embedding provides primarily\nredundant information. However, when the object\ndetector does not accurately localize the object, spa-\ntial embedding can provide additional information\nto compensate for it.\n3. Without Global Avg Pool, we also observe a perfor-\nmance drop. We suspect a lack of suﬃcient global\nimage information behind the decrease in recall. With\nGlobal Avg Pool, it is easier for the network to ag-\ngregate the context of individual nodes and edges\nthat are far apart in spatial location.\n4. Without frequency bias, we see signiﬁcant perfor-\nmance drops. This is because of the high-class im-\nbalance in the VG dataset, which dominates the loss\nfunction. Thus frequency bias is a crucial part of the\nRPM to mitigate the issue of class imbalance.\n6 Conclusion\nIn this paper, we propose a transformer view on the\nscene graph generation task. Notably, we explore node-\nto-node and edge-to-node association through the lens\nof self-attention and cross-attention, respectively. The\nresultant node and edge embedding from the trans-\nformer encoder and decoder, respectively, contributes to\nrobust and discriminate relation classiﬁcation through\nour relation prediction module. We achieve a consis-\ntent improvement over the state-of-the-art models in\nthe evaluation metrics across small, medium, and large-\nscale relational datasets. Our modular nature of RTN\ncan oﬀer contextualized node and edge features for other\nvarious tasks, such as visual question answering, text\nto image generation, etc. Although our work’s current\nfocus remains on attention-based supervised relational\ncontext modeling, future work can be carried upon solv-\ning class imbalance issues and end-to-end self-supervised\nrelational feature learning.\nReferences\n1. Anderson, P., He, X., Buehler, C., Teney, D., John-\nson, M., Gould, S., Zhang, L.: Bottom-up and top-\ndown attention for image captioning and visual\nquestion answering. In: The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR)\n(2018)\n2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normal-\nization. arXiv preprint arXiv:1607.06450 (2016)\n3. Baier, S., Ma, Y., Tresp, V.: Improving visual rela-\ntionship detection using semantic modeling of scene\ndescriptions. In: International Semantic Web Con-\nference, pp. 53–68. Springer (2017)\n4. Carion, N., Massa, F., Synnaeve, G., Usunier,\nN., Kirillov, A., Zagoruyko, S.: End-to-end ob-\nject detection with transformers. arXiv preprint\narXiv:2005.12872 (2020)\n5. Chen, L., Zhang, H., Xiao, J., He, X., Pu,\nS., Chang, S.F.: Counterfactual critic multi-agent\ntraining for scene graph generation. In: Proceedings\nof the IEEE International Conference on Computer\nVision, pp. 4613–4623 (2019)\n6. Chen, T., Yu, W., Chen, R., Lin, L.: Knowledge-\nembedded routing network for scene graph genera-\ntion. In: Conference on Computer Vision and Pat-\ntern Recognition (2019)\n7. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.:\nBert: Pre-training of deep bidirectional transform-\ners for language understanding. arXiv preprint\narXiv:1810.04805 (2018)\n8. Ghosh, S., Burachas, G., Ray, A., Ziskind, A.: Gen-\nerating natural language explanations for visual\nquestion answering using scene graphs and visual\nattention. arXiv preprint arXiv:1902.05715 (2019)\n9. He, K., Gkioxari, G., Doll´ ar, P., Girshick, R.:\nMask r-cnn. In: Proceedings of the IEEE inter-\nnational conference on computer vision, pp. 2961–\n2969 (2017)\n10. Herzig, R., Raboh, M., Chechik, G., Berant, J.,\nGloberson, A.: Mapping images to scene graphs\nwith permutation-invariant structured prediction.\nIn: Advances in Neural Information Processing Sys-\ntems, pp. 7211–7221 (2018)\n11. Hildebrandt, M., Li, H., Koner, R., Tresp,\nV., G¨ unnemann, S.: Scene graph reasoning for\nRelation Transformer Network 11\nvisual question answering. arXiv preprint\narXiv:2007.01072 (2020)\n12. Hu, R., Rohrbach, M., Andreas, J., Darrell, T.,\nSaenko, K.: Modeling relationships in referential\nexpressions with compositional modular networks.\nIn: Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pp. 1115–\n1124 (2017)\n13. Hudson, D.A., Manning, C.D.: Gqa: A new dataset\nfor real-world visual reasoning and compositional\nquestion answering. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pp. 6700–6709 (2019)\n14. Johnson, J., Krishna, R., Stark, M., Li, L.J.,\nShamma, D., Bernstein, M., Fei-Fei, L.: Image re-\ntrieval using scene graphs. In: Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pp. 3668–3678 (2015)\n15. Knyazev, B., de Vries, H., Cangea, C., Taylor,\nG.W., Courville, A., Belilovsky, E.: Graph density-\naware losses for novel compositions in scene graph\ngeneration. arXiv preprint arXiv:2005.08230 (2020)\n16. Koner, R., Li, H., Hildebrandt, M., Das, D.,\nTresp, V., G¨ unnemann, S.: Graphhopper: Multi-\nhop scene graph reasoning for visual question an-\nswering (2021)\n17. Koner, R., Sinhamahapatra, P., Roscher, K.,\nG¨ unnemann, S., Tresp, V.: Oodformer: Out-of-\ndistribution detection transformer (2021)\n18. Koner, R., Sinhamahapatra, P., Tresp, V.:\nScenes and surroundings: Scene graph genera-\ntion using relation transformer. arXiv preprint\narXiv:2107.05448 (2021)\n19. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata,\nK., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J.,\nShamma, D.A., et al.: Visual genome: Connecting\nlanguage and vision using crowdsourced dense im-\nage annotations. International Journal of Com-\nputer Vision 123(1), 32–73 (2017)\n20. Li, G., Duan, N., Fang, Y., Jiang, D., Zhou, M.:\nUnicoder-vl: A universal encoder for vision and lan-\nguage by cross-modal pre-training. arXiv preprint\narXiv:1908.06066 (2019)\n21. Li, Y., Ouyang, W., Wang, X., Tang, X.: Vip-cnn:\nVisual phrase guided convolutional neural network.\nIn: Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pp. 1347–\n1356 (2017)\n22. Lin, X., Ding, C., Zeng, J., Tao, D.: Gps-net: Graph\nproperty sensing network for scene graph genera-\ntion. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp.\n3746–3753 (2020)\n23. Liu, Y., Wang, R., Shan, S., Chen, X.: Structure in-\nference net: Object detection using scene-level con-\ntext and instance-level relationships. In: Proceed-\nings of the IEEE conference on computer vision and\npattern recognition, pp. 6985–6994 (2018)\n24. Lu, C., Krishna, R., Bernstein, M., Fei-Fei, L.: Vi-\nsual relationship detection with language priors. In:\nEuropean conference on computer vision, pp. 852–\n869. Springer (2016)\n25. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert:\nPretraining task-agnostic visiolinguistic representa-\ntions for vision-and-language tasks. In: Advances in\nNeural Information Processing Systems, pp. 13–23\n(2019)\n26. Mao, J., Huang, J., Toshev, A., Camburu, O.,\nYuille, A.L., Murphy, K.: Generation and compre-\nhension of unambiguous object descriptions. In:\nProceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 11–20 (2016)\n27. Nagaraja, V.K., Morariu, V.I., Davis, L.S.: Model-\ning context between objects for referring expression\nunderstanding. In: European Conference on Com-\nputer Vision, pp. 792–807. Springer (2016)\n28. Newell, A., Deng, J.: Pixels to graphs by associa-\ntive embedding. In: Advances in neural information\nprocessing systems, pp. 2171–2180 (2017)\n29. Pennington, J., Socher, R., Manning, C.D.: Glove:\nGlobal vectors for word representation. In: Pro-\nceedings of the 2014 conference on empirical meth-\nods in natural language processing (EMNLP), pp.\n1532–1543 (2014)\n30. Plummer, B.A., Wang, L., Cervantes, C.M.,\nCaicedo, J.C., Hockenmaier, J., Lazebnik, S.:\nFlickr30k entities: Collecting region-to-phrase cor-\nrespondences for richer image-to-sentence models.\nIn: Proceedings of the IEEE international confer-\nence on computer vision, pp. 2641–2649 (2015)\n31. Qi, M., Li, W., Yang, Z., Wang, Y., Luo, J.: At-\ntentive relational networks for mapping images to\nscene graphs. In: Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition,\npp. 3957–3966 (2019)\n32. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn:\nTowards real-time object detection with region pro-\nposal networks. In: Advances in neural information\nprocessing systems, pp. 91–99 (2015)\n33. Schroeder, B., Tripathi, S.: Structured query-based\nimage retrieval using scene graphs. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition Workshops, pp. 178–179\n(2020)\n34. Sharifzadeh, S., Baharlou, S.M., Berrendorf, M.,\nKoner, R., Tresp, V.: Improving visual relation de-\n12 Rajat Koner et al.\ntection using depth maps (2019)\n35. Simonyan, K., Zisserman, A.: Very deep convolu-\ntional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556 (2014)\n36. Srivastava, N., Hinton, G., Krizhevsky, A.,\nSutskever, I., Salakhutdinov, R.: Dropout: a simple\nway to prevent neural networks from overﬁtting.\nThe journal of machine learning research 15(1),\n1929–1958 (2014)\n37. Tang, K., Niu, Y., Huang, J., Shi, J., Zhang, H.:\nUnbiased scene graph generation from biased train-\ning. In: Conference on Computer Vision and Pat-\ntern Recognition (2020)\n38. Tang, K., Zhang, H., Wu, B., Luo, W., Liu, W.:\nLearning to compose dynamic tree structures for\nvisual contexts. In: Conference on Computer Vision\nand Pattern Recognition (2019)\n39. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit,\nJ., Jones, L., Gomez, A.N., Kaiser,  L., Polosukhin,\nI.: Attention is all you need. In: Advances in neu-\nral information processing systems, pp. 5998–6008\n(2017)\n40. Voita, E., Talbot, D., Moiseev, F., Sennrich, R.,\nTitov, I.: Analyzing multi-head self-attention: Spe-\ncialized heads do the heavy lifting, the rest can be\npruned. arXiv preprint arXiv:1905.09418 (2019)\n41. Wan, H., Luo, Y., Peng, B., Zheng, W.S.: Repre-\nsentation learning for scene graph completion via\njointly structural and visual embedding. In: IJCAI,\npp. 949–956 (2018)\n42. Wang, X., Girshick, R., Gupta, A., He, K.: Non-\nlocal neural networks. In: Proceedings of the IEEE\nconference on computer vision and pattern recog-\nnition, pp. 7794–7803 (2018)\n43. Woo, S., Kim, D., Cho, D., Kweon, I.S.: Linknet:\nRelational embedding for scene graph. In: Ad-\nvances in Neural Information Processing Systems,\npp. 560–570 (2018)\n44. Xu, B., Wang, N., Chen, T., Li, M.: Empirical eval-\nuation of rectiﬁed activations in convolutional net-\nwork. arXiv preprint arXiv:1505.00853 (2015)\n45. Xu, D., Zhu, Y., Choy, C.B., Fei-Fei, L.: Scene\ngraph generation by iterative message passing. In:\nThe IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) (2017)\n46. Xu, N., Liu, A.A., Liu, J., Nie, W., Su, Y.: Scene\ngraph captioner: Image captioning based on struc-\ntural visual representation. Journal of Visual Com-\nmunication and Image Representation 58, 477–485\n(2019)\n47. Yang, J., Lu, J., Lee, S., Batra, D., Parikh, D.:\nGraph r-cnn for scene graph generation. In: Pro-\nceedings of the European conference on computer\nvision (ECCV), pp. 670–685 (2018)\n48. Yin, G., Sheng, L., Liu, B., Yu, N., Wang, X., Shao,\nJ., Change Loy, C.: Zoom-net: Mining deep feature\ninteractions for visual relationship recognition. In:\nProceedings of the European Conference on Com-\nputer Vision (ECCV), pp. 322–338 (2018)\n49. Yu, R., Li, A., Morariu, V.I., Davis, L.S.: Visual\nrelationship detection with internal and external\nlinguistic knowledge distillation. In: Proceedings\nof the IEEE international conference on computer\nvision, pp. 1974–1982 (2017)\n50. Zareian, A., Wang, Z., You, H., Chang, S.F.: Learn-\ning visual commonsense for robust scene graph gen-\neration. arXiv preprint arXiv:2006.09623 (2020)\n51. Zellers, R., Yatskar, M., Thomson, S., Choi, Y.:\nNeural motifs: Scene graph parsing with global\ncontext. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp.\n5831–5840 (2018)\n52. Zhang, H., Kyaw, Z., Chang, S.F., Chua, T.S.: Vi-\nsual translation embedding network for visual rela-\ntion detection. In: Proceedings of the IEEE confer-\nence on computer vision and pattern recognition,\npp. 5532–5540 (2017)\n53. Zhang, J., Kalantidis, Y., Rohrbach, M., Paluri, M.,\nElgammal, A., Elhoseiny, M.: Large-scale visual re-\nlationship understanding. In: Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, vol. 33,\npp. 9185–9194 (2019)\n54. Zhang, J., Shih, K.J., Elgammal, A., Tao, A.,\nCatanzaro, B.: Graphical contrastive losses for\nscene graph parsing. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pp. 11535–11543 (2019)\nRelation Transformer Network 13\nA Implementation Details\nIn this section we will list out hyper-parameter used in\nﬁnal model.\n1. optimizer : Stochastic Gradient Descent(SGD)\n2. learning rate : 10 −3 with reduce on plateau and pa-\ntience 3\n3. batch size : 16\n4. dropout [36] : 0 .25\n5. Context Propagation of Objects : 3 E2N modules\n6. Context Propagation for Edges : 2 E2N modules\n7. weight initialization : Xavier normal\n8. attention head : 12 attention heads are used in both\nN2N and E2N.\n9. random seed : 42\n10. Directed Relation Prediction Module (RPM): As\ndiscussed in paper, a RPM module leverages upon\ncontext rich nodes ( ffinal\ni , ffinal\nj ) and undirected\nedges (ffinal\nij ) to produce ﬁnal directed relation em-\nbedding between two nodes (relfinal\ni→j ). The input to\nRPM ((relin\ni→j)) is normalized by LayerNorm [2]then\nfollowed by a linear layer ( W1 ∈ R4096), dropout\nthen another linear layer ( W2 ∈R2048) and ﬁnally\nfollowed by Leaky ReLU non-linearity.\nB Qualitative Results\nThis section will provide a few more qualitative samples\ngenerated by our network in both positive and negative\nscenarios. To improve visibility and interpretability, we\nonly consider the interaction among ground truth ob-\njects and relations in these examples.\nFig. 4, shows the positive scenario, where our net-\nwork is able to detect correct relationships label despite\nthe presence of repetitive bounding box (boy and child)\nor similar objects (giraﬀe). Thus, it shows the robust-\nness of the method.\nFig. 5, shows the negative scenario, where network\nprediction is diﬀerent from ground truth labels. In most\nof these cases, it was found that predicted labels are se-\nmantically closer to ground truth labels, and from a\nhuman perspective, both could be right. For example\nman-at-beach and man-on-beach both are grammati-\ncally correct.\nWhile exploring various attention heads, we have\nfound an interesting pattern that few attention heads\nare focusing on the main object in the scene, few on the\ncombination of some objects while others focus on sur-\nroundings. Some recent research work[40] also explores\nthe working patterns for various attention heads.\n14 Rajat Koner et al.\n(a) Image\n (b) N2N Attention heatmap\n (c) E2N Attention heatmap\n (d) Generated scene graphs\nFig. 4.Some positive example outputs from our network with associated attention map and scene graph.\nRelation Transformer Network 15\n(a) Image\n (b) N2N Attention heatmap\n (c) E2N Attention heatmap\n (d) Generated scene graphs\nFig. 5.Some negative example outputs from our network with associated attention map and scene graph."
}