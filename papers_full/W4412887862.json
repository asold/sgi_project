{
  "title": "MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP",
  "url": "https://openalex.org/W4412887862",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5051863994",
      "name": "Kurt Micallef",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5086259576",
      "name": "Claudia Borg",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4224247062",
    "https://openalex.org/W3093517588"
  ],
  "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various Natural Language Processing (NLP) tasks, largely due to their generalisability and ability to perform tasks without additional training. However, their effectiveness for low-resource languages remains limited. In this study, we evaluate the performance of 55 publicly available LLMs on Maltese, a low-resource language, using a newly introduced benchmark covering 11 discriminative and generative tasks. Our experiments highlight that many models perform poorly, particularly on generative tasks, and that smaller fine-tuned models often perform better across all tasks. From our multidimensional analysis, we investigate various factors impacting performance. We conclude that prior exposure to Maltese during pre-training and instruction-tuning emerges as the most important factor. We also examine the trade-offs between fine-tuning and prompting, highlighting that while fine-tuning requires a higher initial cost, it yields better performance and lower inference costs. Through this work, we aim to highlight the need for more inclusive language technologies and recommend that researchers working with low-resource languages consider more \"traditional\" language modelling approaches.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2025, pages 20505–20527\nJuly 27 - August 1, 2025 ©2025 Association for Computational Linguistics\nMELABenchv1: Benchmarking Large Language Models against\nSmaller Fine-Tuned Models for Low-Resource Maltese NLP\nKurt Micallef\nkurt.micallef@um.edu.mt\nClaudia Borg\nclaudia.borg@um.edu.mt\nDepartment of Artificial Intelligence, University of Malta\nAbstract\nLarge Language Models (LLMs) have demon-\nstrated remarkable performance across vari-\nous Natural Language Processing (NLP) tasks,\nlargely due to their generalisability and abil-\nity to perform tasks without additional training.\nHowever, their effectiveness for low-resource\nlanguages remains limited. In this study, we\nevaluate the performance of 55 publicly avail-\nable LLMs on Maltese, a low-resource lan-\nguage, using a newly introduced benchmark\ncovering 11 discriminative and generative tasks.\nOur experiments highlight that many models\nperform poorly, particularly on generative tasks,\nand that smaller fine-tuned models often per-\nform better across all tasks. From our multidi-\nmensional analysis, we investigate various fac-\ntors impacting performance. We conclude that\nprior exposure to Maltese during pre-training\nand instruction-tuning emerges as the most im-\nportant factor. We also examine the trade-offs\nbetween fine-tuning and prompting, highlight-\ning that while fine-tuning requires a higher\ninitial cost, it yields better performance and\nlower inference costs. Through this work,\nwe aim to highlight the need for more inclu-\nsive language technologies and recommend\nthat researchers working with low-resource lan-\nguages consider more “traditional” language\nmodelling approaches.\n1 Introduction\nLarge Language Models (LLMs) have seen a huge\nrise in use due to their strong performance and re-\nmarkable generalisability across a wide range of\ndiverse tasks (OpenAI et al., 2024; Grattafiori et al.,\n2024; Gemma Team et al., 2024). Their appeal is\nevident from their ability to perform tasks without\nadditional training, with models able to infer from\na few examples (few-shot or in-context learning)\nor an instruction (zero-shot) (Raffel et al., 2020;\nBrown et al., 2020). Furthermore, the availability\nof a plethora of multilingual models makes this\ntechnology more accessible for many languages,\ndue to the inherent cross-lingual transfer capabil-\nities. Despite this, many low-resource languages\nstill face significant challenges in achieving strong\nperformance with these models (Ahuja et al., 2023;\nAsai et al., 2024).\nPrior research on older multilingual models,\nsuch as mBERT, highlighted the challenges faced\nby low-resource languages, particularly when a\nlanguage is absent from pre-training (Chau et al.,\n2020; Muller et al., 2021). However, modern LLMs\noften have an additional training phase, designed to\nimprove their generalisability: instruction-tuning.\nThis raises new questions about the extent to which\ninstruction-tuning mitigates or exacerbates perfor-\nmance gaps for low-resource languages.\nIn this work, we aim to address this for Maltese,\nan official EU language that ranks the lowest in the\nDigital Language Equality score (Rosner and Borg,\n2023). While our primary objective is to under-\nstand which LLM properties influence downstream\ntask performance, we compare this to more tradi-\ntional fine-tuning approach with relatively smaller\nmodels. The main contributions of this work are:\n1. A new evaluation benchmark composed of a\nvariety of 11 discriminative and generative\nMaltese NLP tasks, to facilitate the evaluation\nand development of language technology.\n2. A comprehensive experimental setup on 55\nLLMs, for which we analyse which LLM\nproperties are most important for better down-\nstream task performance.\n3. Several fine-tuned models of a relatively\nsmaller size for each of these tasks, often sur-\npassing all LLMs included in this study.\nOur evaluation code and results are made pub-\nlicly available.1 We also make the best performing\n1https://huggingface.co/spaces/MLRS/MELABench\n20505\nfine-tuned models publicly available. 2 Through\nour evaluation, we explore the following research\nquestions:\n1. How well do LLMs perform compared to\nsmaller fine-tuned models?\n2. What factors contribute to a model’s perfor-\nmance on downstream tasks?\n3. How viable is it to train smaller but task-\nspecific models as opposed to prompting\nlarger but generic models?\n2 Experimental Setup\n2.1 Evaluation Benchmark\nWe conducted a survey of publicly available Mal-\ntese datasets, which allows us to benchmark the\nperformance of various models on Maltese. We\nmake a distinction with the type of task depending\non whether the output is discrete (discriminative)\nor a text in natural language (generative). In total,\nwe collected 11 datasets, shown in Table 1, with\nan even mixture of discriminative and generative\ntasks.\n2.2 Models\nTo answer our primary research question, we use\na variety of generative models. We consider 55\ndifferent language models whose weights are pub-\nlicly available, covering various properties which\nwe consider important for our analysis. These are\nmodel size (300M – 15B), language coverage (18 –\n511, where known), and whether the model is pre-\ntrained (PT) or instruction-tuned ( IT). Moreover,\nwe identify whether the model has seen Maltese\nduring pre-training (PT), during instruction-tuning\n(IT), or never ( NO). In the case of commercially\nreleased models, this information is not available\nand is categorised as unknown (NK). These details\nare summarised in Table 2. Additionally, in Ap-\npendix D we present an evaluation for ChatGPT 4o\nin a limited experimental setup.\n2.3 Evaluation\nWe use the Language Model Evaluation Harness\n(Gao et al., 2024) to conduct the prompting experi-\nments. For each task, we define a template in which\nwe structure the input in textual format together\n2BERTu: https://huggingface.co/collections/\nMLRS/bertu-683ac54c1b6ab3ae715cb43d;\nmT5-Small: https://huggingface.co/collections/\nMLRS/mt5-small-683eecd001179a722c98298b.\nwith an instruction, as well as formatting the target\nin textual format. For generative tasks, the output\nis simply given as is, but for discriminative tasks,\ndiscrete label(s) are mapped into textual format as\nnecessary. Our main experiments are conducted\nwith English instructions, but we also include a set\nof experiments with Maltese instructions which we\nmanually translate. See Appendix A for further\ndetails regarding the prompt templates used.\nIn our setup, we conduct two main experiments:\nzero-shot and one-shot. In the zero-shot case, the\nmodel is given only the input and the instruction,\nand it is expected to produce the corresponding out-\nput. In the one-shot case, we additionally prepend\nthis with the input and output of a sample from\nthe given task, formatted with the same template.\nIn both cases, inference is carried out on the final\ninstance, where the output is not provided to the\nmodel. Any examples used for in-context learning\n(one-shot) are taken from the training set, when\nthis is available, or the validation set otherwise.\nSince no training or validation set is available for\nBelebele, this task is omitted from one-shot experi-\nments.\nIn terms of automated evaluation metrics, we\nreport the following. We use macro-averaged F1\nfor Sentiment Analysis, SIB-200, Taxi1500, Mal-\ntese News Categories, and MultiEURLEX. For\nBelebele, we report the accuracy. We report ChrF\nscores for OPUS-100, Flores-200, and WebNLG,\nand Rouge-L for EUR-Lex-Sum and Maltese News\nHeadlines. Additionally, we also provide BLEU\nscores for OPUS-100 and Flores-200, and Rouge-L\nscores for WebNLG, and ChrF scores for EUR-Lex-\nSum and Maltese News Headlines in Appendix C.\nWhen evaluating the output, the appropriate met-\nrics are calculated on the generated output for\ngenerative tasks. For discriminative tasks, this\nis not as straightforward since the expected out-\nput is discrete. Hence, the output is extracted by\ncomparing the log-likelihood of generating each\nlabel. The label with the highest log-likelihood\nis chosen for single-label classification tasks (Sen-\ntiment, SIB-200, Taxi1500, and Belebele). For\nmulti-label classification tasks (News Categories\nand MultiEURLEX), we extract the predicted la-\nbels based on the number of gold labels.\n2.4 Fine-Tuned Models\nWe want to compare LLMs to the performance\nof smaller fine-tuned models, which also serve as\nbaselines. The models are trained on the training\n20506\nType Name Task |train| |validation| |test|\ndiscriminative\nSentiment (Martínez-García et al., 2021) Sentiment Analysis 595 85 433\nSIB-200 (Adelani et al., 2024) Topic Classification 701 99 204\nTaxi1500 (Ma et al., 2024) Topic Classification 860 106 111\nNews Categories (Chaudhary et al., 2024) Topic Classification (Multi-Label) 10,784 2,293 2,297\nMultiEURLEX (Chalkidis et al., 2021) Topic Classification (Multi-Label) 17,521 5,000 5,000\nBelebele (Bandarkar et al., 2024) Machine Reading Comprehension 0 0 900\ngenerative\nOPUS-100 Fixed (Abela et al., 2024) Machine Translation (EN→MT) 1,000,000 2,000 2,000\nFlores-200 (NLLB Team et al., 2022) Machine Translation (EN→MT) 0 997 1,012\nWebNLG (Cripwell et al., 2023) Data-to-Text *13,211 1,665 1,778\nEUR-Lex-Sum (Aumiller et al., 2022) Abstractive Summarisation 940 187 188\nNews Headlines (Chaudhary et al., 2024) Abstractive Summarisation 17,782 3,810 3,811\nTable 1: Dataset Summary\n*Indicates noisy data obtained through machine translation.\nName Parameter Count Languages Training\noverall/Maltese\nPolyLM (Wei et al., 2023) 1.7B, 13B 18 PT/NO\nXGLM (Lin et al., 2022) 564M, 1.7B, 2.9B, 4.5B, 7.5B 30 PT/NO\nmGPT (Shliazhko et al., 2024) 1.3B, 13B 61 PT/NO\nBLOOM (BigScience Workshop et al., 2023) 560M, 2B, 3B, 8B 46 PT/NO\nAya-23 (Aryabumi et al., 2024) 8B 23 IT/NO\nBLOOMZ (Muennighoff et al., 2023) 560M, 2B, 3B, 8B 46 IT/NO\nBX-LLaMA (Li et al., 2023) 7B, 13B *52 IT/NO\nBX-BLOOM (Li et al., 2023) 7B *77 IT/NO\nSalamandra (Gonzalez-Agirre et al., 2025) 2B, 7B 35 PT/PT\nEuroLLM (Martins et al., 2025) 1.7B, 9B 35 PT/PT\nmT5 (Xue et al., 2021) 300M, 582M, 1.23B, 3.74B, 13B 101 PT/PT\nMaLA-500 (Lin et al., 2024) 8.6B 511 PT/PT\nTeuken Instruct Research v0.4 (Ali et al., 2024) 7B *24 IT/PT\nSalamandra Instruct (Gonzalez-Agirre et al., 2025) 2B, 7B *35 IT/PT\nmT0 (Muennighoff et al., 2023) 300M, 582M, 1.23B, 3.74B, 13B *120 IT/PT\nEuroLLM Instruct (Martins et al., 2025) 1.7B, 9B 35 IT/IT\nAya-101 (Üstün et al., 2024) 13B 101 IT/IT\nGemma 2 (Gemma Team et al., 2024) 2B, 9B ? PT/NK\nLlama 2 (Touvron et al., 2023) 7B, 13B ? PT/NK\nLlama 3 (Grattafiori et al., 2024) 8B ? PT/NK\nMinistral Instruct 2410 (Mistral AI Team, 2024) 8B ? IT/NK\nGemma 2 Instruct (Gemma Team et al., 2024) 2B, 9B ? IT/NK\nLlama 2 Chat (Touvron et al., 2023) 7B, 13B ? IT/NK\nLlama 3 Instruct (Grattafiori et al., 2024) 8B ? IT/NK\nTable 2: Language Model Summary\n*Since the set of languages used during PT and IT is not the same, the union of both sets is represented.\n? = For models with closed-source data, the set of languages used during training is unknown.\nset by performing parameter updates on the model.\nFor each dataset, we train a separate model. Since\nno training set is available for Belebele and Flores-\n200, no baselines are fine-tuned for these tasks.\nWe consider BERT-basedPT models, for which\nwe add a linear classification head on top of the PT\nmodel. These are BERTu (Micallef et al., 2022) –\na monolingual Maltese 126M parameter model –\nand mBERT (Devlin et al., 2019) – a multilingual\nmodel with 179M parameters. However, these are\nonly applied to discriminative tasks since they are\nencoder-only models.\nTherefore, we also fine-tune mT5-Small (Xue\net al., 2021) – a multilingual encoder-decoder PT\nmodel with 300M parameters. Similar to the\nprompted models, we convert every input and out-\nput into textual format. However, we simply train\non the textual input-output pairs and do not apply\nany prompt templates. Moreover, we do not in-\nclude any task prefix which were used to fine-tune\nthe original models (Raffel et al., 2020; Xue et al.,\n2021).3 Evaluation metrics for fine-tuning mT5 are\notherwise computed similarly to prompted models.\nMore details on our fine-tuning setup are included\nin Appendix B.\n3This decision was made because the model is fine-tuned\nseparately on each task, and the prefix did not have much\ninfluence on performance during our initial tests.\n20507\n0\n20\n40Aggregate Performance\nDiscriminative\n0\n10\n20\n30Aggregate Performance\nGenerative\nPolyLM 1.7\nPolyLM 13.0\nXGLM 0.564\nXGLM 1.7\nXGLM 2.9\nXGLM 4.5\nXGLM 7.5\nmGPT 1.3\nmGPT 13.0\nBLOOM 0.56\nBLOOM 2.0\nBLOOM 3.0\nBLOOM 8.0\nAya-23 8.0\nBLOOMZ 0.56\nBLOOMZ 2.0\nBLOOMZ 3.0\nBLOOMZ 8.0\nBX-LLaMA 7.0\nBX-LLaMA 13.0\nBX-BLOOM 8.0\nSalamandra 2.0\nSalamandra 7.0\nEuroLLM 1.7\nEuroLLM 9.0\nmT5 0.3\nmT5 0.582\nmT5 1.23\nmT5 3.74\nmT5 13.0\nMaLA-500 10.0\nTeuken Instruct Research 7.0\nSalamandra Instruct 2.0\nSalamandra Instruct 7.0\nmT0 0.3\nmT0 0.582\nmT0 1.23\nmT0 3.74\nmT0 13.0\nEuroLLM Instruct 1.7\nEuroLLM Instruct 9.0\nAya-101 12.9\nGemma 2 2.0\nGemma 2 9.0\nLlama 2 7.0\nLlama 2 13.0\nLlama 3 8.0\nMinistral Instruct 8.0\nGemma 2 Instruct 2.0\nGemma 2 Instruct 9.0\nLlama 2 Chat 7.0\nLlama 2 Chat 13.0\nLlama 3 Instruct 8.0\nModel\n0\n20\n40Aggregate Performance\nAll\nFigure 1: Zero-shot prompting performance of individ-\nual models aggregated across tasks.\n3 Overall Trends\nThe starting point of our analysis is to understand\nthe performance of all individual models across\ntasks by performing zero-shot prompting. Thus,\nwe aggregate the scores of each model averaged\nacross discriminative and generative tasks. 4 For\neach model, we then calculate an overall score\nacross all tasks by averaging these two scores. The\nresults are shown in Figure 1.\nOverall, Aya-101 is the best-performing model,\nfollowed closely by mT0-XXL. We attribute this\nprimarily to the models’ exposure to Maltese data,\nwhich we further investigate in Section 4.1. The\nsmaller mT0 models are the next best-performing\nmodels overall, along with Salamandra Instruct 7B,\nGemma 2 Instruct 9B, and Llama 3 Instruct 8B.\nFor generative tasks, Aya-101 performs better\nthan any other model, often by a significant margin.\nThis is followed by mT0-XXL, EuroLLM Instruct\n1.7B, Teuken Research Instruct 7B, Salamandra\nInstruct 7B, and Llama 3 Instruct 8B. In the case\nof discriminative tasks, on average, mT0-XXL and\nGemma 2 Instruct 9B perform better than Aya-101.\nWe take a closer look at the score distribution for\neach task in Figure 2. More models are competi-\ntive with the fine-tuned baselines on discriminative\ntasks than generative tasks. Performance on gener-\native tasks hovers near 0 for many models. A more\nin-depth analysis of the individual models on each\n4Although metrics in different tasks are not the same, we\nnote that they are already normalised within the same range.\ntask (see Figure 9) reveals that for generative tasks,\nthe top performers are Aya-101, mT0-XXL, and\nTeuken Instruct Research, and to a lesser degree,\nEuroLLM 1.7B and Llama 3 Instruct 8B. These\nmodels often act as outliers from the rest of the\nmodels. The insights highlight that generating text\ndata is much more challenging than understanding\nit and that many models fail to capture the linguistic\nnuances of a low-resource language like Maltese.\nLooking at each task in Figure 2, we observe that\nmodels generally struggle with Taxi1500 and Mul-\ntiEURLEX among discriminative tasks and EUR-\nLex-Sum among generative tasks. This could be\nattributed to the specific domain of these tasks: the\nBible for Taxi1500 and European Union documents\nfor MultiEURLEX and EUR-Lex-Sum. The latter\ntwo tasks have input sequences that are also gen-\nerally longer, which hampers performance due to\nthe model’s limited context length. Moreover, we\nnote that MultiEURLEX is the only discrimina-\ntive task where models perform quite on par with\none another, particularly since it is a multi-label\nclassification task on a massive scale.\nWhen compared to the baselines, all prompted\nmodels perform worse, with the exception of the\nSentiment Analysis task, for which some models\noutperform mBERT. Among the baselines, mT5\ngenerally performs better than mBERT, except for\nMultiEURLEX, potentially due to the task being ill-\nsuited for generative models, as discussed earlier.\nOverall, BERTu performs the best in discriminative\ntasks due to its Maltese pre-training, except for\nSentiment Analysis for which we observe a perfect\nscore for mT5. While we are uncertain why the\nmodel does so well, we posit that this is due to\nthe task being quite simplistic, since it is the only\ntask where prompted models outperform some of\nthe baselines. Moreover, out of all the tasks, this\ndataset is the most likely to contain code-switching,\nwhich might make it easier for multilingual models\nto pick up on certain linguistic signals.\n3.1 Model Training\nWe now examine the relationship between zero-\nshot prompting performance and overall model\ntraining (PT vs IT). We group models by their train-\ning and visualise the average performance for each\ntask in Figure 4.\nIt is very evident that IT models perform bet-\nter than PT models on all tasks. The performance\ndifference between model types is quite small for\nsome tasks, such as MultiEURLEX and EUR-Lex-\n20508\nSentiment Analysis\n40\n60\n80\n100F1\nSIB-200\n0\n20\n40\n60\n80F1\nTaxi1500\n20\n40\n60F1\nNews Categories\n0\n20\n40\n60F1\nMultiEURLEX\n20\n40\n60F1\nBelebele\n20\n40\n60\n80Accuracy\nOPUS-100\n0\n20\n40\n60ChrF\nFlores-200\n0\n20\n40ChrF\nWebNLG\n0\n20\n40ChrF\nEUR-Lex-Sum\n0\n10\n20\n30\n40Rouge-L\nNews Headlines\n0\n10\n20\n30Rouge-L\nBERTu mBERT mT5\nFigure 2: Zero-shot prompting performance distribution of models per task.\nSentiment Analysis\n40\n60\n80\n100F1\nSIB-200\n0\n20\n40\n60\n80F1\nTaxi1500\n20\n40\n60F1\nNews Categories\n0\n20\n40\n60F1\nMultiEURLEX\n20\n40\n60F1\nOPUS-100\n0\n20\n40\n60ChrF\nFlores-200\n0\n20\n40\n60ChrF\nWebNLG\n0\n20\n40ChrF\nEUR-Lex-Sum\n0\n10\n20\n30\n40Rouge-L\nNews Headlines\n0\n10\n20\n30Rouge-L\nBERTu mBERT mT5\nFigure 3: One-shot prompting performance distribution of models per task.\n40\n60\n80\n100F1\nSentiment Analysis\n0\n20\n40\n60\n80F1\nSIB-200\n20\n40\n60F1\nTaxi1500\n0\n20\n40\n60F1\nNews Categories\n20\n40\n60F1\nMultiEURLEX\nPT\nIT\nModel Training\n20\n40\n60\n80Accuracy\nBelebele\nPT\nIT\nModel Training\n0\n20\n40\n60ChrF\nOPUS-100\nPT\nIT\nModel Training\n0\n20\n40ChrF\nFlores-200\nPT\nIT\nModel Training\n0\n20\n40ChrF\nWebNLG\nPT\nIT\nModel Training\n0\n10\n20\n30\n40Rouge-L\nEUR-Lex-Sum\nPT\nIT\nModel Training\n0\n10\n20\n30Rouge-L\nNews Headlines\nBERTu mBERT mT5\nFigure 4: Zero-shot prompting performance distribution per task, with models grouped by different training types.\nSum, which further reinforces our argument that\nthese datasets are challenging for the prompted\nmodels. The performance disparity on some tasks\nis proportionately larger, which could be due to the\ndifficulty of the task and/or previous instruction-\ntuning on the same task.\n3.2 Number of Shots\nWe want to understand the potential impact of in-\ncontext learning. We perform one-shot prompting\nand compare it to the previous zero-shot results.\nFigure 3 shows the distribution of models on\neach of the tasks in relation to the baseline models\n20509\nSentiment Analysis\nSIB-200\nTaxi1500\nNews Categories\nMultiEURLEX\nOPUS-100\nFlores-200\nWebNLG\nEUR-Lex-Sum\nNews Headlines\nTask\n0\n5\n10\n15\n20Performance Diﬀerence (%)\nModel Training\nPT\nIT\nFigure 5: Performance difference of zero-shot prompt-\ning and one-shot prompting averaged across models\nwith different training types.\nwith one-shot prompting. Overall, the performance\ngap between fine-tuned models and prompted mod-\nels is reduced. In Sentiment Analysis, the best-\nperforming prompted models perform almost as\nwell as a fine-tuned BERTu and almost as good as\na fine-tuned mT5-Small on Maltese News Head-\nlines. We also observe that some models perform\nbetter than fine-tuned mBERT on SIB-200 with\none-shot compared to zero-shot. However, for most\nof the other tasks, the gap between fine-tuned and\nprompted models remains significant.\nTo better interpret the changes between zero-\nshot and one-shot, we calculate the performance\ndifference aggregated across PT and IT models for\neach task. This is calculated by subtracting the zero-\nshot performance from the corresponding one-shot\nperformance of every prompted model.5 Figure 5\nshows the performance difference, with a positive\nscore indicating better one-shot results on average.\nSimilar to Zhang et al. (2024), we observe con-\nsistent performance improvements with one-shot\nacross all generative tasks, but mixed results in\ndiscriminative tasks, with slight degradations for\nTaxi1500, News Categories, and MultiEURLEX.\nIT models also give significant improvements in\nSentiment Analysis, SIB-200, OPUS-100, and Mal-\ntese News Headlines. The performance on Mul-\ntiEURLEX and EUR-Lex-Sum is largely the same\nregardless of the number of shots. We attribute this\nto the longer inputs which are being truncated to a\nlimited sequence length.\n5Similar to Zhang et al. (2024), we observe negative effects\nfor mT0 and BLOOMZ due to their zero-shot instruction-\ntuning, and hence we omit them for this analysis.\n4 Effect of Maltese Exposure\nWe now examine the effect that exposing the model\nto Maltese has on its performance. Apart from\na model’s overall training (PT or IT), models are\nfurther grouped into different categories based on\ntheir explicit training on Maltese data, indicated\nby: NO, PT, and IT, referring to no exposure to\nMaltese data, exposure during pre-training, and\nexposure during instruction-tuning, respectively.\nFor these analyses, we exclude models for which\nthis information cannot be inferred from publicly\navailable metadata (NK).\n4.1 Maltese Training\nWe first examine the impact on model performance\ngiven its training on Maltese at different stages. In\nthe zero-shot experiments (Figure 6a), we observe\nthat models exposed to Maltese during either IT\nor PT achieve better results, with the best overall\nresults observed in the IT/IT category. For dis-\ncriminative tasks, IT/PT models perform on par\nwith IT/IT models on News Categories and Mul-\ntiEURLEX. On generative tasks, IT/IT generally\ngives better results than IT/PT.\nSimilar to the observation made in Section 3.1,\nIT results in better overall scores than PT models.\nWe also note that Maltese PT is generally helpful,\nespecially when comparing PT/PT against PT/NO.\nFor the one-shot experiments (Figure 6b), we\nnote that IT/IT models outperform any other\ntype of model more consistently and significantly,\nwith the exception of the MultiEURLEX task.\nPT/PT models are also generally less performant\nthan PT/NO models. This highlights that while\ninstruction-tuning on a target language is benefi-\ncial, models sometimes need in-context examples\nto access their Maltese knowledge.\nIn Appendix E, we also present further analyses\nexploring the relationship between performance\nand other dimensions such as a model’s size and\nthe number of languages on which it was trained.\nWe initially observed a slight correlation between\nperformance and these variables. However, model\ntraining on Maltese remains the main confounding\nvariable, having a larger impact on performance.\nWhen models trained on Maltese are excluded from\nthese analyses, we observe that this correlation\ndiminishes or is reversed.\n20510\n25\n50\n75\n100F1\nSentiment Analysis\n0\n25\n50\n75F1\nSIB-200\n20\n40\n60F1\nTaxi1500\n0\n20\n40\n60F1\nNews Categories\n20\n40\n60F1\nMultiEURLEX\nPT/NO\nIT/NO\nPT/PT\nIT/PT\nIT/IT\nModel Training\n20\n40\n60\n80Accuracy\nBelebele\nPT/NO\nIT/NO\nPT/PT\nIT/PT\nIT/IT\nModel Training\n0\n25\n50\n75ChrF\nOPUS-100\nPT/NO\nIT/NO\nPT/PT\nIT/PT\nIT/IT\nModel Training\n0\n20\n40ChrF\nFlores-200\nPT/NO\nIT/NO\nPT/PT\nIT/PT\nIT/IT\nModel Training\n0\n20\n40ChrF\nWebNLG\nPT/NO\nIT/NO\nPT/PT\nIT/PT\nIT/IT\nModel Training\n0\n20\n40Rouge-L\nEUR-Lex-Sum\nPT/NO\nIT/NO\nPT/PT\nIT/PT\nIT/IT\nModel Training\n0\n10\n20\n30Rouge-L\nNews Headlines\nBERTu mBERT mT5\n(a) Zero-Shot\n40\n60\n80\n100F1\nSentiment Analysis\n0\n20\n40\n60\n80F1\nSIB-200\n20\n40\n60F1\nTaxi1500\n0\n20\n40\n60F1\nNews Categories\n20\n40\n60F1\nMultiEURLEX\nPT/NO\nIT/NO\nPT/PT\nIT/PT\nIT/IT\nModel Training\n0\n20\n40\n60ChrF\nOPUS-100\nPT/NO\nIT/NO\nPT/PT\nIT/PT\nIT/IT\nModel Training\n0\n20\n40\n60ChrF\nFlores-200\nPT/NO\nIT/NO\nPT/PT\nIT/PT\nIT/IT\nModel Training\n0\n20\n40ChrF\nWebNLG\nPT/NO\nIT/NO\nPT/PT\nIT/PT\nIT/IT\nModel Training\n0\n10\n20\n30\n40Rouge-L\nEUR-Lex-Sum\nPT/NO\nIT/NO\nPT/PT\nIT/PT\nIT/IT\nModel Training\n0\n10\n20\n30Rouge-L\nNews Headlines\nBERTu mBERT mT5\n(b) One-Shot\nFigure 6: Prompting performance distribution per task, with models grouped by different training types and Maltese\ntraining.\n4.2 Maltese Prompts\nWe now examine the impact of providing models\nwith more Maltese text, not only in the form of in-\ncontext examples, but also by manually translating\nthe instruction from English to Maltese. 6 There-\nfore, we repeat all previous prompting experiments\nusing Maltese prompt templates. Each score with\nEnglish prompting is then subtracted from these\nnew scores with Maltese prompting to get the per-\nformance difference, and the overall results are\nshown in Figure 7.\nWe observe mixed results in both zero-shot and\none-shot, but performance is generally worse with\nMaltese prompts. However, models are negatively\n6More detail regarding our prompt translation process is\nincluded in Appendix A.\nimpacted in most discriminative tasks, regardless of\nthe model’s exposure to Maltese during its training.\nWith one-shot, the negative difference is even more\npronounced, highlighting that in-context learning\nexamples are better suited to prime the model to\nMaltese as opposed to language instructions. PT/NO\nmodels seem to get significant improvements in\nSentiment Analysis and Flores-200 with Maltese\nprompts in zero-shot, but this drastically diminishes\nin one-shot.\nIT/IT models exhibit some of the worst degra-\ndations with Maltese prompts, particularly in zero-\nshot, even though these are the models that were\nexposed to Maltese the most. However, although\nall models in this category – EuroLLM Instruct and\nAya-101 – are exposed to Maltese examples during\ntheir IT, the actual instructions are still in English.\n20511\nSentiment Analysis\nSIB-200\nTaxi1500\nNews Categories\nMultiEURLEX\nBelebele\nOPUS-100\nFlores-200\nWebNLG\nEUR-Lex-Sum\nNews Headlines\nTask\n−20\n−15\n−10\n−5\n0\n5\n10\nPerformance Diﬀerence (%)\nModel Training\nPT/NO IT/NO PT/PT IT/PT IT/IT\n(a) Zero-shot\nSentiment Analysis\nSIB-200\nTaxi1500\nNews Categories\nMultiEURLEX\nOPUS-100\nFlores-200\nWebNLG\nEUR-Lex-Sum\nNews Headlines\nTask\n−15\n−10\n−5\n0\n5\nPerformance Diﬀerence (%)\nModel Training\nPT/NO IT/NO PT/PT IT/PT IT/IT\n(b) One-Shot\nFigure 7: Performance difference of prompting with En-\nglish and Maltese instructions averaged across models\nwith different training types and Maltese training.\nThis highlights the discrepancy between model per-\nformance and usability, since speaker populations\nof low-resource languages like Maltese would have\nto resort to providing English instructions in their\ninteractions with these models.\n5 Efficiency\nThe computational efficiency of fine-tuning and\nprompting is also an important factor to consider.\nWe select the following prompted models based on\nthe best overall performance and the model archi-\ntecture variety: Aya-101, mT0-XXL, and Llama\n3 Instruct 8B. We consider all fine-tuned models:\nBERTu, mBERT, and mT5-Small.\nTo estimate the computational requirements, we\nfollow Liu et al. (2022) and compute the Floating-\nPoint Operations Per Second (FLOPs, Kaplan et al.,\n2020) required for a single instance. We take the\nmedian sequence length of an instance for each\ndataset and use it to calculate the inference FLOPs\nfor a given model. For fine-tuned models, we use\nonly the raw input sequence in textual format. For\nprompted models, we also include the accompany-\ning instruction. For the purpose of this analysis,\nModel Training FLOPs Inference FLOPs\nBERTu 1.54e16 3.28e10\nmBERT 2.50e16 4.06e10\nmT5-Small 7.19e15 7.14e09\nmT0-XXL 0 5.96e12\nAya-101 0 5.84e12\nLlama 3 Instruct 8B 0 5.06e13\nTable 3: Computational cost estimates in terms of Float-\ning Point Operations (FLOPs).\nwe only consider zero-shot prompting with English\ninstructions. We also compute the training FLOPs\nfor fine-tuned models by calculating the median\nsequence length on every training dataset, but also\ntake into account the batch size and the total num-\nber of steps 7 during training. We then take the\naverage number of FLOPs across all tasks. Since\nbaseline models were not fine-tuned for Belebele\nand Flores-200, we do not include any calculations\nfor these tasks in this analysis. Table 3 shows the\nresulting calculations.\nAs expected, the inference cost of fine-tuned\nmodels is magnitudes smaller than that of prompted\nmodels due to the smaller model sizes. We also\nnote that for prompted models, as the number of\nfew-shot examples increases, so does the inference\ncost. The cost for fine-tuning models is also mag-\nnitudes larger than applying inference on a single\ninstance. However, as we apply inference on more\nexamples, this initial upfront cost diminishes.\nIf we define efficiency as a function of the overall\nperformance, the cost, and the number of inference\nsamples as follows:\nperformance\ncosttraining + costinference × samples (1)\nthen, as the number of samples increases, the effi-\nciency of prompting larger models drastically de-\ncreases, as shown in Figure 8. Furthermore, the\nsheer size of the prompted models also necessitates\nmore expensive hardware to store models on disk\nand load them into memory.\n6 Related Work\nDue to the lack of research on generative Maltese\nNLP, our research is primarily related to various\nmultilingual benchmarking and evaluation works,\nalthough none of them include Maltese. Ahuja et al.\n(2023) evaluate LLM performance on a newly de-\nveloped benchmark covering 70 languages. They\n7This is calculated by averaging the total number of steps\n(including early stopping patience) across all runs.\n20512\n0 200 400 600 800 1000\nNumber of samples\n10□15\n10□14\n10□13\n10□12\n10□11\nEﬃciency\nBERTu 0.126\nmBERT 0.179\nmT5 0.3\nmT0 13.0\nAya-101 12.9\nLlama 3 Instruct 8.0\nFigure 8: Model inference efficiency as a function of\nthe performance and cost, as defined in Equation 1.\nfind that low-resource languages are negatively\naffected by ill-fitted tokenisers and limited pre-\ntraining data used by the model. Zhang et al. (2024)\nstudy the impact of few-shot prompting across 56\nlanguages, finding that the few-shot performance\ndoes not always improve over zero-shot, depending\non the model, task, and language. Asai et al. (2024)\nsimilarly analyse few-shot prompting performance\nbut in a cross-lingual setup, finding that while few-\nshot generally helps, models perform particularly\npoorly on lower-resourced languages. Zhang et al.\n(2023) analyse the performance on code-switched\nlanguages. In their analysis, they find that both an\nincreased number of shots and model size (when\nconsidering the same model family) generally im-\nprove performance.\nMost of these works also fine-tune smaller\nBERT-based and/or mT5 models, similarly find-\ning that they outperform prompted LLMs (Ahuja\net al., 2023; Asai et al., 2024; Zhang et al., 2023).\nLikewise, Lai et al. (2023) find that ChatGPT under-\nperforms supervised SOTA models on most tasks.\nOur research is complementary to these works\nas we look closer at the performance of a single\nlanguage. We also significantly scale the number of\nlanguage models which allows us to study multiple\nfactors affecting model performance.\n7 Conclusion\nIn this paper, we present a comprehensive evalua-\ntion of LLMs for Maltese, a low-resource language.\nWe introduce a new benchmark covering a total of\n11 discriminative and generative tasks. We carried\nout an evaluation of 55 publicly available models\nunder zero-shot and one-shot prompting, revealing\nsignificant performance variations. Our study high-\nlights the need to improve the state of low-resource\nlanguages such as Maltese.\nIn analysing our results, we uncover several key\ntrends. Prompted models consistently lag behind\nsmaller fine-tuned models, particularly in genera-\ntive tasks where their performance is significantly\nlower. Crucially, the level of exposure to Maltese\nin a model’s training has the largest bearing on\nperformance, especially in instruction-tuning.\nAdditionally, our efficiency analysis highlights\nthe trade-offs between fine-tuning and prompting.\nWhile fine-tuning incurs higher initial costs, the\ninference cost is significantly lower. The initial\ncost incurred to fine-tune smaller models quickly\npays off with more inference instances, apart from\nthe higher performance on downstream tasks.\nThese findings underscore the pressing need for\nmore inclusive language models that better sup-\nport low-resource languages at every stage of the\ntraining pipeline. While LLMs offer strong general-\nisability, their limited performance on low-resource\nlanguages like Maltese reduces their usability in\nthese scenarios. For researchers with limited com-\nputational resources, fine-tuning smaller models\npresents a viable alternative to prompting larger\nmodels, despite the trade-off in generalisability.\nUltimately, our study calls for a more balanced\napproach to model development, ensuring that low-\nresource languages like Maltese receive adequate\nrepresentation in the evolving landscape of LLMs.\n8 Limitations\nModels Although we consider a wide variety of\nmodels, the coverage of models trained on Maltese\nwas very limited. Moreover, due to the large num-\nber of models considered as well as computational\nconstraints, we only choose models with no more\nthan 15 billion parameters. In addition, for our\nmain analysis, we do not consider any commercial\nmodels, not only due to the prohibitive costs to con-\nduct this evaluation, but also due to our constrained\nevaluation on discriminative tasks. However, we\npresent a limited comparative evaluation on Chat-\nGPT in Appendix D, showing mixed results on the\ntasks tested compared to the fine-tuned baselines.\nDatasets While our benchmark is certainly an im-\nprovement on the state of NLP for a low-resource\nlanguage, certain issues impact our evaluation.\nFirstly, our results are confounded by the amount\nof data in some tasks, which is why we strived to\n20513\nshow per-task results as much as possible. Despite\nthe small training data sizes, we show that fine-\ntuned models still outperform the prompted large\nlanguage models. Secondly, the variance of tasks is\nalso limited, as we have Topic Classification for the\nmajority of our discriminative tasks, and Machine\nTranslation dominates our generative tasks. The lat-\nter is often a large source of instruction-tuning data\nfor multilingual instruction-tuning, and often the\nonly data considered when Maltese was used for\ninstruction-tuning (Üstün et al., 2024). Thirdly, the\ndomains of these datasets are quite narrow as they\nare mostly composed of data derived from news\narticles, EU legislative documents, and Wikimedia.\nAll in all, we hope that our work raises awareness\non the importance of developing newer and more\ndiverse datasets for low-resource languages.\nPrompt Engineering Various works have shown\nthat different models can be optimised with differ-\nent prompts (Zhao et al., 2021; Jiang et al., 2020;\nShin et al., 2020; inter alia). We sidestep this\nby mostly using prompt templates from previous\nworks (see Appendix A for more details). We high-\nlight that we did consider prompting in Maltese\n(Section 4.2), which is often understudied in the\ncontext of multilingual evaluation (Ahuja et al.,\n2023; Asai et al., 2024). We did not experiment\nwith a larger number of shots to keep our experi-\nments sustainable.\nUnconsidered Model Properties We tried to\nanalyse many possible dimensions but still had to\nlimit our search space. Despite looking at mod-\nels exposed to Maltese training, we largely treated\nthis as a categorical variable. However, the raw\namount of tokens, as well as the proportion in re-\nlation to the rest of the training data, would have\na large bearing on performance. In a similar vein,\nwe did not consider the different scales and quality\nof the datasets used for training different models.\nWe also do not factor for different kinds of train-\ning processes used during instruction-tuning such\nas Reinforcement Learning with Human Feedback\n(Ziegler et al., 2020) and Direct Preference Opti-\nmisation (Rafailov et al., 2023). As we make this\ndata available, we encourage future work to analyse\ndifferent dimensions not considered in this work.\nDataset Contamination It is likely that models\nhave been exposed to language data that is not\nincluded in the figures listed in Table 2 (Blevins and\nZettlemoyer, 2022; Muennighoff et al., 2023), but\nwe do not account for it. Since our benchmarks are\nbased on publicly available datasets, it is likely that\nincreased performance in some models is due to\ndata contamination during training. While this can\nbe accidental, instruction-tuned models may have\nused certain datasets deliberately. Hence, higher\nscores would be attributed to the model’s training\non that task, rather than its capabilities to generalise\nto unseen tasks. Dataset contamination is also an\nactive area of research (Blevins and Zettlemoyer,\n2022; Balloccu et al., 2024), and we therefore treat\nmodels as black-box systems in this regard.\n9 Ethics Statement\nWe inherit any biases that may be present in the\ndata and language models that we use. The new\ngenerative models fine-tuned on Maltese data could\nbe used to produce text that inherit these biases.\nHowever, given their relatively low performance\non generative tasks and the fact that we train these\nusing publicly available resources, we do not fore-\nsee any major risks.\nAcknowledgements\nWe acknowledge support from the LT-Bridge\nProject (GA 952194) and DFKI for access to the\nVirtual Laboratory. We further acknowledge fund-\ning by Malta Enterprise Research and Development\nScheme.\nReferences\nKurt Abela, Kurt Micallef, Marc Tanti, and Claudia\nBorg. 2024. Tokenisation in machine translation\ndoes matter: The impact of different tokenisation\napproaches for Maltese. In Proceedings of the Sev-\nenth Workshop on Technologies for Machine Transla-\ntion of Low-Resource Languages (LoResMT 2024),\npages 109–120, Bangkok, Thailand. Association for\nComputational Linguistics.\nDavid Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen,\nNikita Vassilyev, Jesujoba O. Alabi, Yanke Mao, Hao-\nnan Gao, and En-Shiun Annie Lee. 2024. SIB-200:\nA simple, inclusive, and big evaluation dataset for\ntopic classification in 200+ languages and dialects.\nIn Proceedings of the 18th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 226–245,\nSt. Julian’s, Malta. Association for Computational\nLinguistics.\nKabir Ahuja, Harshita Diddee, Rishav Hada, Milli-\ncent Ochieng, Krithika Ramesh, Prachi Jain, Ak-\nshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed\nAhmed, et al. 2023. MEGA: Multilingual evaluation\n20514\nof generative AI. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 4232–4267, Singapore. Associa-\ntion for Computational Linguistics.\nMehdi Ali, Michael Fromm, Klaudia Thellmann, Jan\nEbert, Alexander Arno Weber, Richard Rutmann,\nCharvi Jain, Max Lübbering, Daniel Steinigen, Jo-\nhannes Leveling, et al. 2024. Teuken-7B-Base\n& Teuken-7B-Instruct: Towards European LLMs.\nPreprint, arXiv:2410.03730.\nViraat Aryabumi, John Dang, Dwarak Talupuru,\nSaurabh Dash, David Cairuz, Hangyu Lin, Bharat\nVenkitesh, Madeline Smith, Jon Ander Campos,\nYi Chern Tan, et al. 2024. Aya 23: Open weight\nreleases to further multilingual progress. Preprint,\narXiv:2405.15032.\nAkari Asai, Sneha Kudugunta, Xinyan Yu, Terra\nBlevins, Hila Gonen, Machel Reid, Yulia Tsvetkov,\nSebastian Ruder, and Hannaneh Hajishirzi. 2024.\nBUFFET: Benchmarking large language models for\nfew-shot cross-lingual transfer. In Proceedings of\nthe 2024 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long\nPapers), pages 1771–1800, Mexico City, Mexico. As-\nsociation for Computational Linguistics.\nDennis Aumiller, Ashish Chouhan, and Michael Gertz.\n2022. EUR-lex-sum: A multi- and cross-lingual\ndataset for long-form summarization in the legal do-\nmain. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 7626–7639, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nSimone Balloccu, Patrícia Schmidtová, Mateusz Lango,\nand Ondrej Dusek. 2024. Leak, cheat, repeat: Data\ncontamination and evaluation malpractices in closed-\nsource LLMs. In Proceedings of the 18th Confer-\nence of the European Chapter of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 67–93, St. Julian’s, Malta. Association\nfor Computational Linguistics.\nLucas Bandarkar, Davis Liang, Benjamin Muller, Mikel\nArtetxe, Satya Narayan Shukla, Donald Husa, Naman\nGoyal, Abhinandan Krishnan, Luke Zettlemoyer, and\nMadian Khabsa. 2024. The belebele benchmark: a\nparallel reading comprehension dataset in 122 lan-\nguage variants. In Proceedings of the 62nd Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 749–775,\nBangkok, Thailand. Association for Computational\nLinguistics.\nBigScience Workshop, Teven Le Scao, Angela Fan,\nChristopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel\nHesslow, Roman Castagné, Alexandra Sasha Luc-\ncioni, François Yvon, et al. 2023. BLOOM: A 176B-\nparameter open-access multilingual language model.\nPreprint, arXiv:2211.05100.\nTerra Blevins and Luke Zettlemoyer. 2022. Language\ncontamination helps explains the cross-lingual capa-\nbilities of English pretrained models. In Proceedings\nof the 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3563–3574, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Process-\ning Systems, volume 33, pages 1877–1901. Curran\nAssociates, Inc.\nIlias Chalkidis, Manos Fergadiotis, and Ion Androut-\nsopoulos. 2021. MultiEURLEX - a multi-lingual and\nmulti-label legal document classification dataset for\nzero-shot cross-lingual transfer. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 6974–6996, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\nParsing with multilingual BERT, a small corpus, and\na small treebank. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1324–1334, Online. Association for Computational\nLinguistics.\nAmit Kumar Chaudhary, Kurt Micallef, and Claudia\nBorg. 2024. Topic classification and headline genera-\ntion for Maltese using a public news corpus. In Pro-\nceedings of the 2024 Joint International Conference\non Computational Linguistics, Language Resources\nand Evaluation (LREC-COLING 2024), pages 16274–\n16281, Torino, Italia. ELRA and ICCL.\nLiam Cripwell, Anya Belz, Claire Gardent, Albert\nGatt, Claudia Borg, Marthese Borg, John Judge,\nMichela Lorandi, Anna Nikiforovskaya, and William\nSoto Martinez. 2023. The 2023 WebNLG shared\ntask on low resource languages. overview and evalu-\nation results (WebNLG 2023). In Proceedings of the\nWorkshop on Multimodal, Multilingual Natural Lan-\nguage Generation and Multilingual WebNLG Chal-\nlenge (MM-NLG 2023), pages 55–66, Prague, Czech\nRepublic. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,\nSid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Alain Le Noac’h, et al. 2024. A\nframework for few-shot language model evaluation.\n20515\nGemma Team, Morgane Riviere, Shreya Pathak,\nPier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\nraju, Léonard Hussenot, Thomas Mesnard, Bobak\nShahriari, Alexandre Ramé, et al. 2024. Gemma 2:\nImproving open language models at a practical size.\nPreprint, arXiv:2408.00118.\nAitor Gonzalez-Agirre, Marc Pàmies, Joan Llop,\nIrene Baucells, Severino Da Dalt, Daniel Tamayo,\nJosé Javier Saiz, Ferran Espuña, Jaume Prats, Javier\nAula-Blasco, et al. 2025. Salamandra technical re-\nport. Preprint, arXiv:2502.08489.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAlex Vaughan, et al. 2024. The Llama 3 herd of\nmodels. Preprint, arXiv:2407.21783.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. Preprint,\narXiv:2001.08361.\nViet Dac Lai, Nghia Ngo, Amir Pouran Ben Veyseh,\nHieu Man, Franck Dernoncourt, Trung Bui, and\nThien Huu Nguyen. 2023. ChatGPT beyond En-\nglish: Towards a comprehensive evaluation of large\nlanguage models in multilingual learning. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, pages 13171–13189, Singapore.\nAssociation for Computational Linguistics.\nHaonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji,\nand Timothy Baldwin. 2023. Bactrian-X: Multilin-\ngual replicable instruction-following models with\nlow-rank adaptation. Preprint, arXiv:2305.15011.\nPeiqin Lin, Shaoxiong Ji, Jörg Tiedemann, André F. T.\nMartins, and Hinrich Schütze. 2024. MaLA-500:\nMassive language adaptation of large language mod-\nels. Preprint, arXiv:2401.13303.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, et al. 2022.\nFew-shot learning with multilingual generative lan-\nguage models. In Proceedings of the 2022 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 9019–9052, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nHaokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mo-\nhta, Tenghao Huang, Mohit Bansal, and Colin Raffel.\n2022. Few-shot parameter-efficient fine-tuning is bet-\nter and cheaper than in-context learning. In Advances\nin Neural Information Processing Systems.\nChunlan Ma, Ayyoob ImaniGooghari, Haotian Ye, Ren-\nhao Pei, Ehsaneddin Asgari, and Hinrich Schütze.\n2024. Taxi1500: A multilingual dataset for\ntext classification in 1500 languages. Preprint,\narXiv:2305.08487.\nAntonio Martínez-García, Toni Badia, and Jeremy\nBarnes. 2021. Evaluating morphological typology\nin zero-shot cross-lingual transfer. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3136–3153, Online.\nAssociation for Computational Linguistics.\nPedro Henrique Martins, Patrick Fernandes, João Alves,\nNuno M. Guerreiro, Ricardo Rei, Duarte M. Alves,\nJosé Pombal, Amin Farajian, Manuel Faysse, Ma-\nteusz Klimaszewski, et al. 2025. Eurollm: Multi-\nlingual language models for europe. Procedia Com-\nputer Science, 255:53–62. Proceedings of the Second\nEuroHPC user day.\nKurt Micallef, Albert Gatt, Marc Tanti, Lonneke van der\nPlas, and Claudia Borg. 2022. Pre-training data qual-\nity and quantity for a low-resource language: New\ncorpus and BERT models for Maltese. In Proceed-\nings of the Third Workshop on Deep Learning for\nLow-Resource Natural Language Processing, pages\n90–101, Hybrid. Association for Computational Lin-\nguistics.\nMistral AI Team. 2024. Un Ministral, des Minis-\ntraux. https://mistral.ai/news/ministraux/.\nAccessed: 2024-12-20.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey\nSchoelkopf, et al. 2023. Crosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15991–16111, Toronto, Canada. Association\nfor Computational Linguistics.\nBenjamin Muller, Antonios Anastasopoulos, Benoît\nSagot, and Djamé Seddah. 2021. When being un-\nseen from mBERT is just the beginning: Handling\nnew languages with multilingual language models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 448–462, Online. Association for Computa-\ntional Linguistics.\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur\nÇelebi, Maha Elbayad, Kenneth Heafield, Kevin\nHeffernan, Elahe Kalbassi, Janice Lam, Daniel\nLicht, et al. 2022. No language left behind: Scal-\ning human-centered machine translation. Preprint,\narXiv:2207.04672.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\nman, et al. 2024. GPT-4 technical report. Preprint,\narXiv:2303.08774.\n20516\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christo-\npher D Manning, Stefano Ermon, and Chelsea Finn.\n2023. Direct preference optimization: Your language\nmodel is secretly a reward model. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nMichael Rosner and Claudia Borg. 2023. Language\nReport Maltese. In Georg Rehm and Andy Way,\neditors, European Language Equality: A Strategic\nAgenda for Digital Language Equality, pages 183–\n186. Springer International Publishing, Cham.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova,\nAnastasia Kozlova, Vladislav Mikhailov, and Tatiana\nShavrina. 2024. mGPT: Few-shot learners go multi-\nlingual. Transactions of the Association for Compu-\ntational Linguistics, 12:58–79.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open foundation and\nfine-tuned chat models. Preprint, arXiv:2307.09288.\nAhmet Üstün, Viraat Aryabumi, Zheng Yong, Wei-Yin\nKo, Daniel D’souza, Gbemileke Onilude, Neel Bhan-\ndari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al.\n2024. Aya model: An instruction finetuned open-\naccess multilingual language model. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15894–15939, Bangkok, Thailand. Association\nfor Computational Linguistics.\nXiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li,\nPei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei\nCao, Binbin Xie, et al. 2023. PolyLM: An open\nsource polyglot large language model. Preprint,\narXiv:2307.06018.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, pages\n38–45, Online. Association for Computational Lin-\nguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nMiaoran Zhang, Vagrant Gautam, Mingyang Wang, Je-\nsujoba Alabi, Xiaoyu Shen, Dietrich Klakow, and\nMarius Mosbach. 2024. The impact of demonstra-\ntions on multilingual in-context learning: A mul-\ntidimensional analysis. In Findings of the Asso-\nciation for Computational Linguistics: ACL 2024,\npages 7342–7371, Bangkok, Thailand. Association\nfor Computational Linguistics.\nRuochen Zhang, Samuel Cahyawijaya, Jan Chris-\ntian Blaise Cruz, Genta Winata, and Alham Fikri\nAji. 2023. Multilingual large language models are\nnot (yet) code-switchers. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 12567–12582, Singapore.\nAssociation for Computational Linguistics.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, pages 12697–12706.\nPMLR.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2020. Fine-tuning lan-\nguage models from human preferences. Preprint,\narXiv:1909.08593.\n20517\nA Prompt Templates\nTable 4 shows the prompts that were used to eval-\nuate every model on each task. When available,\nwe use a template suggested by the original dataset\npaper. Otherwise, we adapt a template from a re-\nlated task which was available from the Language\nModel Evaluation Harness repository (Gao et al.,\n2024). For generative tasks, we ensure that the\ninstruction explicitly mentions that the text should\nbe generated in Maltese.\nTo generate the Maltese prompts, we reuse the\nEnglish templates and translate them into Maltese.\nThis is done by first passing the instruction through\nGoogle Translate and then performing post-editing\nwith a Maltese native speaker.\nB Fine-Tuning Details\nFor fine-tuned models, we consider BERTu (Mi-\ncallef et al., 2022), mBERT (Devlin et al., 2019),\nand mT5-Small (Xue et al., 2021). Our training\nscripts are implemented using the transformers\nlibrary (Wolf et al., 2020).\nWe train all models for at most 200 epochs but\nuse early stopping on the main metric (as defined in\nSection 2.3) with a patience of 20 epochs. Due to\nthe significantly larger scale of the data for OPUS-\n100, we only train for a maximum of 10 epochs\ninstead.\nFor the BERT-based models, we use a learning\nrate of 1e-4 with an AdamW optimiser, an inverse\nsquare-root learning rate scheduler, a warmup of\n1 epoch, and a weight decay of 0.01. We use a\nbatch size of 16 for Sentiment Analysis, SIB-200,\nand Taxi1500 and a batch size of 32 for the other\ndiscriminative tasks. A dropout of 0.1 is used for\nTaxi1500 and MultiEURLEX, and 0.5 for the other\ndiscriminative tasks.\nWhen fine-tuning mT5, we mostly follow the\noriginal fine-tuning recipe (Raffel et al., 2020; Xue\net al., 2021) with a constant learning rate of 1e-3\nwith an Adafactor optimiser and a batch size of\n32. BERT-based models are fine-tuned 5 separate\ntimes with different random seeds, and we report\nthe mean performance across these runs. mT5 is\nonly fine-tuned once per task.\nC All Results\nIn this section, we present individual results for\neach model and task considered. Figure 9 shows\nthe zero-shot performance with English instruc-\ntions and Figure 10 shows the one-shot perfor-\nmance with English instructions.\nWe also present the individual performance with\nall metrics of each model with English prompts in\nTables 5, 6, 7, and 8. Results for fine-tuned models\nare shown in Tables 9 and 10.\nD Closed-Source Model Results\nWe include experiments with ChatGPT 4o (Ope-\nnAI et al., 2024) as a comparison to our main ex-\nperiments. However, since this is a closed-source\nmodel accessible only through an API, our experi-\nments with this model are limited. Firstly, since we\ndo not have access to log-likelihoods, it is not pos-\nsible for us to conduct discriminative experiments\nin a comparative manner, so we skip these tasks.\nSecondly, we also skip EUR-Lex-Sum due to the\nlarge context lengths needed for this task, which\nexceed our quota. Thirdly, for the remaining four\ntasks, we only prompt with 100 test samples for\neach task to limit our costs.\nThe results are shown in Table 11. Comparing\nthese figures to the results obtained by our fine-\ntuned mT5 baseline (Table 10), ChatGPT 4o per-\nforms significantly worse on OPUS-100, signifi-\ncantly better on WebNLG and on par on Maltese\nNews Headlines.\nE Analysing Other Model Properties\nE.1 Model Size\nWe look at the performance as the model size grows\nin terms of the number of parameters. To analyse\nthis, we fit linear regression models for PT and IT\nmodels on performance results aggregated by task\ntype. We only do this for zero-shot results.\nAs shown in Figure 11a, a general improvement\nis observed in performance with model size in-\ncrease. In general, IT models with larger sizes\ngive better performances than PT models. In fact,\na smaller performance gap is observed between PT\nand IT models with fewer than 10B parameters,\nespecially in few-shot, where PT models overall\nperform better than IT models on generative tasks.\nHowever, we note that among the largestIT mod-\nels are the Aya-101 and mT0 models, which are\ntrained on Maltese. If we exclude models which\nwe know are trained on Maltese, then our previous\nobservations do not hold as shown in Figure 11b.\nIn fact, we see a negative impact on performance\nas model size grows for IT models, albeit with a\nlarger confidence interval.\n20518\nTask English Prompt Template Maltese Prompt Template\nSentiment Anal-\nysis\n{text} Is the sentiment positive or neg-\native?\n{text} Is-sentiment huwa po˙zittiv jew\nnegattiv?\nSIB-200 The topic of the news “{text}” is Is-su˙g˙gett tal-a¯ hbarjiet “{text}” huwa\nTaxi1500 The topic of the verse is “{text}” is Is-su˙g˙gett tal-vers “{text}” huwa\nMaltese News\nCategories\n{text}\nWhat are the topic(s) of this news\narticle?\n{text}\nX’inhu(ma) s-su ˙g˙gett(i) ta’ dan l-\nartiklu tal-a¯ hbarjiet?\nMultiEURLEX {text}\nWhat are the topics of this text?\n{text}\nX’inhuma s-su ˙g˙getti ta’ dan it-\ntest?\nBelebele Given the following passage, query, and\nanswer choices, output the letter corre-\nsponding to the correct answer.\n###\nPassage:\n{text}\n###\nQuery:\n{question}\n###\nChoices:\n(A) {answer1}\n(B) {answer2}\n(C) {answer3}\n(D) {answer4}\n###\nAnswer:\nPermezz tas-silta, mistoqsija, u g¯ha˙zliet\nta’ twe ˙gibiet li ˙gejjin, ag ¯hti l-ittra li\ntikkorrispondi g¯ hat-twe˙giba t-tajba.\n###\nPassa˙g˙g:\n{text}\n###\nMistoqsija:\n{question}\n###\nChoices:\n(A) {answer1}\n(B) {answer2}\n(C) {answer3}\n(D) {answer4}\n###\nTwe˙giba:\nOPUS-100\nFlores-200\n{source_sentence}\nThe previous text is in\n{source_language}. Here is a\ntranslation to {target_language}:\n{source_sentence}\nIt-test pre ˙cedenti huwa bl-\n{source_language}. Din hija traduz-\nzjoni g¯ hall-{target_language}:\nWebNLG Verbalize in Maltese the follow-\ning triples separated by a comma:\n{triples | join(’, ’)}\nIvverbalizza bil-Malti t-tripli li ˙gejjin\nseparati b’virgola: {triples |\njoin(’, ’)}\nEUR-Lex-Sum {text}\nWrite a summary in Maltese for\nthe text above:\n{text}\nIkteb sommarju bil-Malti g ¯hat-\ntest t’hawn fuq:\nMaltese News\nHeadlines\n{text}\nWrite a headline in Maltese for\nthe news article above:\n{text}\nIkteb titolu bil-Malti g ¯hall-artiklu\ntal-a¯ hbarjiet t’hawn fuq:\nTable 4: Prompt Templates used for each task.\n20519\n0\n50\n100F1\nSentiment Analysis\n0\n25\n50\n75F1\nSIB-200\n0\n20\n40\n60F1\nTaxi1500\n0\n20\n40\n60F1\nNews Categories\n0\n20\n40\n60F1\nMultiEURLEX\n0\n25\n50\n75Accuracy\nBelebele\n0\n20\n40\n60ChrF\nOPUS-100\n0\n20\n40ChrF\nFlores-200\n0\n20\n40ChrF\nWebNLG\nPolyLM 1.7PolyLM 13.0XGLM 0.564XGLM 1.7XGLM 2.9XGLM 4.5XGLM 7.5mGPT 1.3mGPT 13.0BLOOM 0.56BLOOM 2.0BLOOM 3.0BLOOM 8.0Aya-23 8.0BLOOMZ 0.56BLOOMZ 2.0BLOOMZ 3.0BLOOMZ 8.0BX-LLaMA 7.0BX-LLaMA 13.0BX-BLOOM 8.0Salamandra 2.0Salamandra 7.0EuroLLM 1.7EuroLLM 9.0mT5 0.3mT5 0.582mT5 1.23mT5 3.74mT5 13.0MaLA-500 10.0Teuken Instruct Research 7.0Salamandra Instruct 2.0Salamandra Instruct 7.0mT0 0.3mT0 0.582mT0 1.23mT0 3.74mT0 13.0EuroLLM Instruct 1.7EuroLLM Instruct 9.0Aya-101 12.9Gemma 2 2.0Gemma 2 9.0Llama 2 7.0Llama 2 13.0Llama 3 8.0Ministral Instruct 8.0Gemma 2 Instruct 2.0Gemma 2 Instruct 9.0Llama 2 Chat 7.0Llama 2 Chat 13.0Llama 3 Instruct 8.0\nModel\n0\n20\n40Rouge-L\nEUR-Lex-Sum\nPolyLM 1.7PolyLM 13.0XGLM 0.564XGLM 1.7XGLM 2.9XGLM 4.5XGLM 7.5mGPT 1.3mGPT 13.0BLOOM 0.56BLOOM 2.0BLOOM 3.0BLOOM 8.0Aya-23 8.0BLOOMZ 0.56BLOOMZ 2.0BLOOMZ 3.0BLOOMZ 8.0BX-LLaMA 7.0BX-LLaMA 13.0BX-BLOOM 8.0Salamandra 2.0Salamandra 7.0EuroLLM 1.7EuroLLM 9.0mT5 0.3mT5 0.582mT5 1.23mT5 3.74mT5 13.0MaLA-500 10.0Teuken Instruct Research 7.0Salamandra Instruct 2.0Salamandra Instruct 7.0mT0 0.3mT0 0.582mT0 1.23mT0 3.74mT0 13.0EuroLLM Instruct 1.7EuroLLM Instruct 9.0Aya-101 12.9Gemma 2 2.0Gemma 2 9.0Llama 2 7.0Llama 2 13.0Llama 3 8.0Ministral Instruct 8.0Gemma 2 Instruct 2.0Gemma 2 Instruct 9.0Llama 2 Chat 7.0Llama 2 Chat 13.0Llama 3 Instruct 8.0\nModel\n0\n10\n20\n30Rouge-L\nNews Headlines\nBERTu mBERT mT5\nFigure 9: Zero-shot prompting performance of individual models on each task. Horizontal lines represent models\nfine-tuned specifically on the task.\nE.2 Model Multilinguality\nWe also analysed a model’s performance against\nthe number of languages it was exposed to during\nits training. Similar to Section 4, we exclude mod-\nels with unknown Maltese training (NK). We also\nexclude MaLA-500 from this analysis, as the high\ndegree of languages skews our plots. Other than\nthat, we plot aggregated zero-shot performance re-\nsults against model multilinguality and fit separate\nlinear regression models for PT and IT models.\nIn Figure 12a we observe a positive influence\nwith the number of languages a model is exposed\nto for IT models. For PT models there is also a\npositive effect on generative tasks, although smaller\nthan that for IT models. On the other hand, there\nis a negative impact as the number of languages\n20520\n0\n50\n100F1\nSentiment Analysis\n0\n25\n50\n75F1\nSIB-200\n0\n20\n40\n60F1\nTaxi1500\n0\n20\n40\n60F1\nNews Categories\n0\n20\n40\n60F1\nMultiEURLEX\n0\n20\n40\n60ChrF\nOPUS-100\n0\n20\n40\n60ChrF\nFlores-200\n0\n20\n40ChrF\nWebNLG\nPolyLM 1.7PolyLM 13.0XGLM 0.564XGLM 1.7XGLM 2.9XGLM 4.5XGLM 7.5mGPT 1.3mGPT 13.0BLOOM 0.56BLOOM 2.0BLOOM 3.0BLOOM 8.0Aya-23 8.0BLOOMZ 0.56BLOOMZ 2.0BLOOMZ 3.0BLOOMZ 8.0BX-LLaMA 7.0BX-LLaMA 13.0BX-BLOOM 8.0Salamandra 2.0Salamandra 7.0EuroLLM 1.7EuroLLM 9.0mT5 0.3mT5 0.582mT5 1.23mT5 3.74mT5 13.0MaLA-500 10.0Teuken Instruct Research 7.0Salamandra Instruct 2.0Salamandra Instruct 7.0mT0 0.3mT0 0.582mT0 1.23mT0 3.74mT0 13.0EuroLLM Instruct 1.7Aya-101 12.9Gemma 2 2.0Llama 2 7.0Llama 2 13.0Llama 3 8.0Ministral Instruct 8.0Gemma 2 Instruct 2.0Gemma 2 Instruct 9.0Llama 2 Chat 7.0Llama 2 Chat 13.0Llama 3 Instruct 8.0\nModel\n0\n20\n40Rouge-L\nEUR-Lex-Sum\nPolyLM 1.7PolyLM 13.0XGLM 0.564XGLM 1.7XGLM 2.9XGLM 4.5XGLM 7.5mGPT 1.3mGPT 13.0BLOOM 0.56BLOOM 2.0BLOOM 3.0BLOOM 8.0Aya-23 8.0BLOOMZ 0.56BLOOMZ 2.0BLOOMZ 3.0BLOOMZ 8.0BX-LLaMA 7.0BX-LLaMA 13.0BX-BLOOM 8.0Salamandra 2.0Salamandra 7.0EuroLLM 1.7EuroLLM 9.0mT5 0.3mT5 0.582mT5 1.23mT5 3.74mT5 13.0MaLA-500 10.0Teuken Instruct Research 7.0Salamandra Instruct 2.0Salamandra Instruct 7.0mT0 0.3mT0 0.582mT0 1.23mT0 3.74mT0 13.0EuroLLM Instruct 1.7Aya-101 12.9Gemma 2 2.0Llama 2 7.0Llama 2 13.0Llama 3 8.0Ministral Instruct 8.0Gemma 2 Instruct 2.0Gemma 2 Instruct 9.0Llama 2 Chat 7.0Llama 2 Chat 13.0Llama 3 Instruct 8.0\nModel\n0\n10\n20\n30Rouge-L\nNews Headlines\nBERTu mBERT mT5\nFigure 10: One-shot prompting performance of individual models on each task. Horizontal lines represent models\nfine-tuned specifically on the task.\nincreases for discriminative tasks.\nDespite this, highly multilingual models which\nhave seen more than 100 languages, are all models\nwhich have been exposed to Maltese during some\npart of their training. When excluding these models\nand refitting logistic models on the remaining data\nwe see that the previously observed improvements\nare drastically reduced.\n20521\nModel Sentiment Analysis SIB-200 Taxi1500 News Categories MultiEURLEX Belebele\nF1 F1 F1 F1 F1 Accuracy\nPolyLM1.7B 46.3 3.4 12.0 5.4 12.1 23.0\nPolyLM13.0B 26.0 3.8 5.0 20.4 12.6 22.2\nXGLM0.564B 29.4 17.1 7.5 7.1 15.1 23.1\nXGLM1.7B 40.9 16.6 5.5 18.9 12.7 23.0\nXGLM2.9B 32.7 8.9 9.7 21.0 14.3 23.4\nXGLM4.5B 26.6 5.8 11.4 22.1 18.2 24.7\nXGLM7.5B 29.9 10.2 6.4 19.3 14.9 22.9\nmGPT1.3B 33.0 10.1 12.5 12.0 7.9 22.7\nmGPT13.0B 38.5 15.7 11.7 16.0 7.5 25.7\nBLOOM0.56B 26.6 11.1 9.6 8.9 16.4 23.1\nBLOOM2.0B 56.1 15.9 10.0 10.7 13.7 22.2\nBLOOM3.0B 32.9 30.6 5.3 15.6 14.4 24.8\nBLOOM8.0B 26.0 36.6 15.3 20.0 12.8 24.1\nAya-238.0B 53.1 16.9 8.9 4.5 12.2 39.3\nBLOOMZ0.56B 49.9 32.6 8.9 20.8 14.9 27.4\nBLOOMZ2.0B 50.4 36.0 9.8 23.3 15.1 21.0\nBLOOMZ3.0B 62.8 42.9 16.3 27.5 14.9 32.9\nBLOOMZ8.0B 56.4 48.8 21.2 27.4 14.2 28.9\nBX-LLaMA7.0B 26.0 8.0 11.6 6.9 7.4 25.7\nBX-LLaMA13.0B 36.7 9.1 5.7 7.0 11.3 22.8\nBX-BLOOM8.0B 26.0 14.8 12.1 15.2 10.8 21.7\nSalamandra2.0B 59.0 9.8 13.6 18.3 8.6 21.6\nSalamandra7.0B 27.0 26.4 15.4 27.7 10.5 35.7\nEuroLLM1.7B 28.6 27.5 4.2 17.4 11.9 23.1\nEuroLLM9.0B 58.5 38.7 18.1 6.1 5.9 51.2\nmT50.3B 26.0 2.4 3.7 6.2 8.7 21.9\nmT50.582B 31.6 2.8 4.9 2.9 7.3 27.9\nmT51.23B 28.2 6.7 4.9 2.1 11.0 22.9\nmT53.74B 27.0 4.3 3.7 2.1 9.9 21.9\nmT513.0B 26.0 14.6 4.9 4.1 14.0 29.3\nMaLA-50010.0B 26.0 21.4 7.4 2.3 3.6 27.9\nTeuken Instruct Research7.0B 32.1 5.7 15.2 5.7 6.2 45.0\nSalamandra Instruct2.0B 75.4 20.0 10.7 31.0 10.3 24.9\nSalamandra Instruct7.0B 73.4 53.2 15.1 26.9 11.7 69.0\nmT00.3B 55.5 33.6 4.3 25.8 13.8 24.4\nmT00.582B 66.3 43.1 5.9 33.8 13.4 28.9\nmT01.23B 75.9 45.8 13.6 29.8 17.6 33.7\nmT03.74B 72.3 47.5 9.6 32.5 16.8 65.9\nmT013.0B 78.5 54.2 21.7 36.1 18.3 81.7\nEuroLLM Instruct1.7B 28.0 14.0 4.2 26.3 13.7 24.1\nEuroLLM Instruct9.0B 68.1 41.2 23.0 12.6 7.7 69.6\nAya-10112.9B 78.1 50.2 29.5 31.5 18.5 76.6\nGemma 22.0B 26.0 27.0 11.2 7.9 10.2 32.7\nGemma 29.0B 26.0 59.2 25.5 20.7 13.2 74.6\nLlama 27.0B 43.8 25.8 17.9 21.0 10.7 26.9\nLlama 213.0B 32.9 5.1 10.6 15.0 16.4 31.8\nLlama 38.0B 26.0 29.5 15.7 26.8 16.2 43.7\nMinistral Instruct8.0B 47.4 34.4 6.2 11.3 13.4 40.6\nGemma 2 Instruct2.0B 27.0 39.4 18.2 15.8 14.4 47.6\nGemma 2 Instruct9.0B 72.1 60.9 34.2 22.3 16.4 83.9\nLlama 2 Chat7.0B 31.8 34.0 11.9 19.9 10.3 31.6\nLlama 2 Chat13.0B 28.0 30.9 19.6 13.1 16.6 34.0\nLlama 3 Instruct8.0B 37.3 43.6 18.4 22.0 13.2 51.9\nTable 5: Results on discriminative tasks for models prompted with English zero-shot instructions.\n20522\nModel OPUS-100 Flores-200 WebNLG EUR-Lex-Sum News Headlines\nBLEU ChrF BLEU ChrF ChrF Rouge-L ChrF Rouge-L ChrF Rouge-L\nPolyLM1.7B 0.0 0.6 0.1 7.7 8.2 3.7 0.0 0.0 0.0 0.0\nPolyLM13.0B 0.0 0.4 0.1 3.7 11.5 6.0 0.0 0.0 0.0 0.0\nXGLM0.564B 0.0 0.1 0.0 0.0 9.5 4.3 0.0 0.0 0.0 0.0\nXGLM1.7B 0.0 0.1 0.0 0.0 10.1 4.9 0.0 0.0 0.0 0.0\nXGLM2.9B 0.0 0.1 0.0 0.9 10.6 4.9 0.0 0.0 0.4 0.1\nXGLM4.5B 0.0 0.0 0.0 0.1 10.6 4.8 0.0 0.0 0.1 0.0\nXGLM7.5B 0.0 0.1 0.0 0.0 10.2 4.8 0.0 0.0 0.2 0.1\nmGPT1.3B 0.2 7.8 0.3 11.9 7.7 3.0 0.2 0.6 0.4 0.1\nmGPT13.0B 0.3 8.5 0.2 11.0 9.3 4.3 0.1 0.1 5.0 1.9\nBLOOM0.56B 0.2 5.5 0.3 3.9 9.1 4.6 0.0 0.0 0.0 0.0\nBLOOM2.0B 0.1 1.1 0.0 0.1 10.5 5.4 0.0 0.0 0.0 0.0\nBLOOM3.0B 0.0 0.4 0.0 0.3 10.4 5.4 0.0 0.0 0.0 0.0\nBLOOM8.0B 0.0 0.2 0.0 0.2 10.6 5.3 0.0 0.0 0.0 0.0\nAya-238.0B 0.1 2.6 0.0 0.5 11.2 4.9 0.0 0.0 11.5 6.2\nBLOOMZ0.56B 0.5 5.3 0.2 6.8 7.1 5.0 0.7 1.0 12.9 6.6\nBLOOMZ2.0B 1.2 10.9 0.2 7.7 4.7 3.4 2.0 2.9 14.3 8.3\nBLOOMZ3.0B 0.7 7.8 0.5 10.3 11.5 10.2 0.4 0.5 17.5 11.4\nBLOOMZ8.0B 1.4 11.0 1.0 13.7 5.3 4.6 0.8 1.1 16.8 10.7\nBX-LLaMA7.0B 0.0 2.0 0.0 2.4 2.6 0.8 1.2 0.0 1.3 0.1\nBX-LLaMA13.0B 0.0 0.0 0.0 0.6 10.9 5.5 0.1 0.1 0.8 0.1\nBX-BLOOM8.0B 0.0 0.3 0.0 0.2 10.3 5.1 0.0 0.0 5.9 2.1\nSalamandra2.0B 1.4 17.3 3.7 25.7 2.9 2.5 0.0 0.0 10.9 5.8\nSalamandra7.0B 2.0 17.1 7.9 33.0 2.4 1.6 0.0 0.0 0.3 0.2\nEuroLLM1.7B 0.0 0.0 0.0 0.0 10.9 5.5 0.0 0.0 0.0 0.0\nEuroLLM9.0B 0.0 0.0 0.0 0.0 10.8 4.5 0.0 0.0 0.0 0.0\nmT50.3B 0.1 1.5 0.0 1.5 1.4 0.6 0.2 0.5 6.3 4.9\nmT50.582B 0.2 3.9 0.0 4.1 4.8 2.9 0.3 0.9 8.7 7.4\nmT51.23B 0.3 9.4 0.1 9.9 11.1 4.3 3.2 4.4 8.8 3.4\nmT53.74B 0.2 8.9 0.0 9.0 6.3 2.4 4.3 4.4 7.4 2.6\nmT513.0B 0.2 8.0 0.1 9.4 8.4 3.0 4.3 4.3 6.8 2.3\nMaLA-50010.0B 0.0 6.7 0.0 7.3 9.7 3.8 0.0 0.0 0.0 0.0\nTeuken Instruct Research7.0B 0.9 6.4 12.0 46.4 9.7 5.1 2.5 2.5 31.2 23.0\nSalamandra Instruct2.0B 2.7 23.4 3.5 34.1 2.0 1.8 0.0 0.0 1.1 0.6\nSalamandra Instruct7.0B 9.1 39.3 5.5 41.6 2.1 2.6 0.2 0.1 0.0 0.0\nmT00.3B 4.0 17.5 2.2 20.0 14.3 15.3 3.9 1.3 3.3 2.9\nmT00.582B 0.7 4.8 0.2 5.3 21.1 21.3 2.6 0.4 4.2 4.9\nmT01.23B 0.6 6.3 0.2 6.6 24.8 24.5 5.4 2.4 6.3 6.9\nmT03.74B 1.7 10.0 0.5 8.6 30.3 29.6 4.7 1.1 9.7 13.2\nmT013.0B 7.8 28.3 3.3 25.0 25.7 26.2 3.8 3.2 27.4 26.8\nEuroLLM Instruct1.7B 9.2 37.0 15.7 51.1 11.7 5.7 0.0 0.0 16.4 9.4\nEuroLLM Instruct9.0B 0.0 0.0 0.0 0.0 11.2 4.9 0.0 0.0 0.0 0.0\nAya-10112.9B 26.4 56.6 19.5 52.3 36.0 32.3 8.2 7.7 30.6 27.6\nGemma 22.0B 0.0 0.0 0.0 0.0 10.0 4.3 0.2 0.2 0.0 0.0\nGemma 29.0B 0.0 0.0 0.0 0.0 9.8 3.9 0.2 0.1 0.0 0.0\nLlama 27.0B 0.0 0.0 0.0 0.0 11.0 4.5 0.0 0.0 0.0 0.0\nLlama 213.0B 0.0 0.0 0.0 0.0 11.3 4.7 0.0 0.0 0.0 0.0\nLlama 38.0B 3.3 12.5 5.3 32.8 11.3 6.0 0.0 0.0 0.8 0.3\nMinistral Instruct8.0B 1.2 6.7 1.2 16.9 10.5 5.0 0.2 0.2 0.1 0.0\nGemma 2 Instruct2.0B 0.0 0.0 0.0 0.0 11.6 6.5 0.0 0.0 0.0 0.0\nGemma 2 Instruct9.0B 0.0 0.0 0.0 0.0 8.0 6.3 0.0 0.0 0.0 0.0\nLlama 2 Chat7.0B 0.0 0.0 0.0 0.0 12.3 5.9 0.0 0.0 0.0 0.0\nLlama 2 Chat13.0B 0.0 0.0 0.0 0.0 10.0 6.8 0.0 0.0 0.0 0.0\nLlama 3 Instruct8.0B 0.0 2.4 8.8 39.9 12.6 6.5 1.0 1.1 31.7 24.1\nTable 6: Results on generative tasks for models prompted with English zero-shot instructions.\n20523\nModel Sentiment Analysis SIB-200 Taxi1500 News Categories MultiEURLEX\nF1 F1 F1 F1 F1\nPolyLM1.7B 52.1 19.3 6.3 5.1 10.4\nPolyLM13.0B 26.0 21.6 6.3 12.3 12.3\nXGLM0.564B 48.0 2.2 6.3 10.9 14.5\nXGLM1.7B 50.1 15.8 6.3 10.4 12.5\nXGLM2.9B 46.5 25.8 6.3 13.5 13.3\nXGLM4.5B 56.4 14.2 6.4 10.6 15.4\nXGLM7.5B 32.8 26.1 6.3 9.2 14.7\nmGPT1.3B 39.4 2.2 6.3 3.7 7.3\nmGPT13.0B 41.8 47.7 6.3 5.1 7.2\nBLOOM0.56B 50.0 4.4 6.3 4.2 15.9\nBLOOM2.0B 41.1 2.2 6.3 5.5 14.2\nBLOOM3.0B 41.1 14.1 6.3 7.2 14.0\nBLOOM8.0B 54.5 41.6 6.3 7.7 13.3\nAya-238.0B 68.3 34.6 6.3 22.8 14.5\nBLOOMZ0.56B 51.1 34.1 6.3 7.5 14.5\nBLOOMZ2.0B 60.6 23.4 6.3 17.8 14.7\nBLOOMZ3.0B 49.3 33.0 6.3 9.6 15.1\nBLOOMZ8.0B 57.5 45.1 6.3 12.3 13.3\nBX-LLaMA7.0B 26.0 7.0 9.0 6.0 6.5\nBX-LLaMA13.0B 26.0 2.2 6.3 3.6 10.1\nBX-BLOOM8.0B 53.2 23.8 6.3 4.4 7.5\nSalamandra2.0B 26.0 41.1 6.3 14.6 9.2\nSalamandra7.0B 79.3 48.1 8.3 25.9 10.8\nEuroLLM1.7B 52.2 20.9 6.3 18.5 11.6\nEuroLLM9.0B 77.4 65.1 11.7 8.8 10.1\nmT50.3B 26.0 2.4 3.7 5.8 8.3\nmT50.582B 35.5 2.8 10.3 4.1 7.2\nmT51.23B 49.6 2.2 6.3 2.1 11.6\nmT53.74B 39.4 2.2 6.3 1.2 9.9\nmT513.0B 48.0 2.2 6.4\nMaLA-50010.0B 26.0 39.3 6.3 3.2 4.1\nTeuken Instruct Research7.0B 76.4 56.9 22.8 17.5 6.8\nSalamandra Instruct2.0B 76.1 52.4 6.3 21.3 8.7\nSalamandra Instruct7.0B 65.1 64.2 17.4 24.5 12.0\nmT00.3B 54.4 15.2 6.3 18.9 13.7\nmT00.582B 35.1 19.5 6.3 24.3 10.8\nmT01.23B 77.0 21.1 6.3 18.7 15.9\nmT03.74B 28.0 30.2 6.3 15.4 14.5\nmT013.0B 61.1 46.4 6.3 20.3 15.7\nEuroLLM Instruct1.7B 69.0 36.5 6.3 24.5 10.2\nAya-10112.9B 86.5 52.3 19.7 23.5 15.2\nGemma 22.0B 30.5 36.1 6.3 21.6 12.1\nLlama 27.0B 36.3 31.3 6.3 24.1 14.8\nLlama 213.0B 60.2 47.4 10.0 9.7 13.6\nLlama 38.0B 80.6 42.1 6.3 23.0 13.2\nMinistral Instruct8.0B 70.9 43.7 6.3 9.0 16.4\nGemma 2 Instruct2.0B 70.2 58.3 10.1 7.6 12.9\nGemma 2 Instruct9.0B 85.0 74.3 31.9 22.6 8.1\nLlama 2 Chat7.0B 66.2 44.3 8.7 24.5 9.5\nLlama 2 Chat13.0B 72.2 62.6 16.9 16.5 16.0\nLlama 3 Instruct8.0B 79.9 56.7 8.3 18.8 9.7\nTable 7: Results on discriminative tasks for models prompted with English one-shot instructions.\n20524\nModel OPUS-100 Flores-200 WebNLG EUR-Lex-Sum News Headlines\nBLEU ChrF BLEU ChrF ChrF Rouge-L ChrF Rouge-L ChrF Rouge-L\nPolyLM1.7B 0.4 11.6 0.3 16.5 10.7 8.9 0.0 0.0 11.9 4.2\nPolyLM13.0B 6.5 19.1 1.8 19.8 10.4 6.2 0.0 0.0 19.9 10.7\nXGLM0.564B 2.8 19.0 0.6 17.3 11.7 6.4 0.0 0.0 20.5 9.3\nXGLM1.7B 3.5 15.3 1.0 17.6 13.4 10.2 0.0 0.0 21.5 9.5\nXGLM2.9B 3.4 13.5 1.1 13.8 12.8 8.6 0.0 0.0 21.6 9.8\nXGLM4.5B 3.8 15.7 1.1 15.6 13.9 9.9 0.0 0.0 21.5 9.2\nXGLM7.5B 3.2 12.7 1.1 16.4 12.9 10.7 0.0 0.0 20.8 9.1\nmGPT1.3B 1.6 11.6 1.0 16.9 10.0 6.8 0.2 0.6 1.4 0.7\nmGPT13.0B 1.0 9.2 0.4 11.6 9.8 6.1 0.1 0.1 1.0 0.2\nBLOOM0.56B 1.0 9.8 0.4 13.2 11.2 8.3 0.0 0.0 11.0 3.8\nBLOOM2.0B 0.4 9.9 0.2 14.5 11.6 7.9 0.0 0.0 17.8 7.4\nBLOOM3.0B 1.8 10.7 0.6 14.9 12.5 10.6 0.0 0.0 18.1 7.7\nBLOOM8.0B 1.6 11.4 0.9 15.1 12.7 11.2 0.0 0.0 21.0 10.4\nAya-238.0B 2.3 13.1 0.5 11.6 12.3 8.2 0.0 0.0 28.2 19.8\nBLOOMZ0.56B 0.2 4.6 0.0 5.0 10.7 9.7 0.7 1.0 11.6 4.1\nBLOOMZ2.0B 1.0 8.8 0.2 9.1 12.6 12.0 2.0 2.9 13.1 4.8\nBLOOMZ3.0B 0.3 5.9 0.2 7.7 17.9 17.5 0.4 0.5 13.2 4.9\nBLOOMZ8.0B 0.5 7.6 0.3 9.3 12.6 12.8 0.8 1.1 12.4 4.5\nBX-LLaMA7.0B 0.0 1.6 0.0 1.4 1.9 0.4 1.2 0.0 0.8 0.0\nBX-LLaMA13.0B 1.0 12.0 0.7 16.0 11.7 7.8 0.1 0.1 8.2 3.2\nBX-BLOOM8.0B 0.6 10.3 0.6 16.5 11.0 9.3 0.0 0.0 5.2 3.8\nSalamandra2.0B 3.7 22.3 7.1 34.3 8.2 6.5 0.0 0.0 20.2 12.8\nSalamandra7.0B 6.9 29.0 12.6 42.7 14.9 13.7 0.0 0.0 22.8 17.9\nEuroLLM1.7B 16.4 37.6 20.9 53.0 12.4 9.9 0.0 0.0 19.4 10.5\nEuroLLM9.0B 23.6 47.1 34.0 63.7 14.4 14.1 0.0 0.0 4.4 3.0\nmT50.3B 0.1 1.8 0.0 2.0 1.5 0.6 0.2 0.5 5.7 4.1\nmT50.582B 0.4 4.7 0.1 5.9 6.1 7.0 0.3 0.9 5.9 5.4\nmT51.23B 0.2 9.0 0.1 12.4 10.6 3.4 3.2 4.4 6.2 1.7\nmT53.74B 0.3 11.0 0.2 15.1 8.9 3.0 4.3 4.4 5.7 1.6\nmT513.0B 0.2 8.2 0.1 11.0 9.0 3.1 4.3 4.4 8.7 3.4\nMaLA-50010.0B 0.0 11.2 0.1 13.7 9.8 5.5 0.0 0.0 13.2 4.8\nTeuken Instruct Research7.0B 18.8 45.8 18.8 54.7 13.6 14.1 2.5 2.4 31.0 24.1\nSalamandra Instruct2.0B 4.2 28.1 3.4 35.2 3.3 1.9 0.0 0.0 18.9 12.7\nSalamandra Instruct7.0B 8.5 43.4 6.2 44.3 13.2 10.3 0.2 0.1 25.7 18.9\nmT00.3B 2.5 16.8 0.6 14.5 5.0 3.9 3.9 1.3 0.6 0.1\nmT00.582B 0.4 5.7 0.0 3.9 13.3 9.2 2.6 0.4 8.4 2.3\nmT01.23B 1.1 10.5 0.2 10.2 13.8 10.8 5.4 2.4 1.8 1.8\nmT03.74B 1.4 10.3 0.1 7.1 18.1 17.1 4.7 1.1 12.3 4.5\nmT013.0B 9.0 31.3 4.8 29.6 10.8 10.8 3.8 3.2 11.6 6.5\nEuroLLM Instruct1.7B 13.1 38.3 21.0 54.0 13.3 10.5 0.0 0.0 22.8 14.3\nAya-10112.9B 26.8 57.8 21.2 54.3 41.2 36.4 8.2 7.7 30.0 28.0\nGemma 22.0B 3.3 14.3 2.4 23.7 13.0 13.1 0.2 0.2 19.2 12.6\nLlama 27.0B 4.6 17.0 1.5 19.3 12.6 10.3 0.0 0.0 23.0 16.0\nLlama 213.0B 6.3 23.1 2.0 23.5 16.7 15.9 0.0 0.0 26.7 19.4\nLlama 38.0B 14.9 39.7 10.1 41.8 17.4 17.8 0.0 0.0 33.0 25.2\nMinistral Instruct8.0B 3.9 17.2 1.4 18.8 14.4 11.9 0.8 1.5 30.9 21.3\nGemma 2 Instruct2.0B 3.1 13.6 1.7 19.5 12.2 8.9 0.0 0.0 9.8 6.5\nGemma 2 Instruct9.0B 14.4 36.7 18.4 52.4 22.8 23.3 0.0 0.0 32.3 27.0\nLlama 2 Chat7.0B 3.8 16.3 2.5 20.0 14.0 13.4 0.0 0.0 25.7 18.4\nLlama 2 Chat13.0B 7.9 25.8 4.0 27.6 15.2 15.5 0.0 0.0 29.4 20.8\nLlama 3 Instruct8.0B 17.6 44.7 10.9 43.7 30.3 26.4 1.0 1.1 33.6 26.6\nTable 8: Results on generative tasks for models prompted with English one-shot instructions.\nModel Sentiment SIB-200 Taxi1500 News Categories MultiEURLEX\nMacro-F1 Macro-F1 Macro-F1 Macro-F1 Macro-F1\nBERTu 83.0 84.9 77.5 58.3 67.1\nmBERT 64.7 75.3 47.7 53.1 60.7\nXLM-R 59.6 68.5 36.5 50.6 60.5\nGlot500 74.6 82.3 64.0 57.2 62.2\nmT5 100.0 76.8 42.2 52.5 31.2\nTable 9: Fine-tuned model results on discriminative tasks.\n20525\nModel OPUS-100 WebNLG EUR-Lex-Sum News Headlines\nBLEU ChrF ChrF Rouge-L ChrF Rouge-L ChrF Rouge-L\nmT5 (fine-tuned) 51.8 75.9 31.6 28.0 51.5 42.5 32.2 28.1\nTable 10: Fine-tuned model results on generative tasks.\nPrompt Shots OPUS-100 Flores-200 WebNLG News Headlines\nBLEU ChrF BLEU ChrF ChrF Rouge-L ChrF Rouge-L\nEnglish Zero 38.1 69.7 44.1 74.4 61.8 58.1 32.1 26.6\nMaltese Zero 34.2 64.0 43.5 72.4 56.0 53.6 32.2 27.2\nEnglish One 36.8 67.3 46.2 74.5 61.9 57.4 33.8 25.9\nMaltese One 35.8 65.6 46.2 74.5 61.8 58.3 31.9 24.7\nTable 11: ChatGPT Results.\n0 5 10\nsize\n10\n20\n30\n40\n50Aggregate Performance\nDiscriminative\n0 5 10\nsize\n0\n10\n20\n30Aggregate Performance\nGenerative\n0 5 10\nsize\n10\n20\n30\n40Aggregate Performance\nAll\nModel Training\nPT IT\nModel Training\nPT IT\nModel Training\nPT IT\n(a) Including PT/PT, IT/PT, and IT/IT models\n0 5 10\nsize\n10\n20\n30Aggregate Performance\nDiscriminative\n0 5 10\nsize\n0.0\n2.5\n5.0\n7.5\n10.0\nAggregate Performance\nGenerative\n0 5 10\nsize\n5\n10\n15\n20Aggregate Performance\nAll\nModel Training\nPT IT\nModel Training\nPT IT\nModel Training\nPT IT (b) Excluding PT/PT, IT/PT, and IT/IT models\nFigure 11: Zero-shot aggregated performance against model size.\n20526\n0 50 100\nlanguages\n10\n20\n30\n40\n50Aggregate Performance\nDiscriminative\n0 50 100\nlanguages\n0\n10\n20\n30Aggregate Performance\nGenerative\n0 50 100\nlanguages\n10\n20\n30\n40Aggregate Performance\nAll\nModel Training\nPT IT\nModel Training\nPT IT\nModel Training\nPT IT\n(a) Including PT/PT, IT/PT, and IT/IT models\n20 40 60\nlanguages\n−50\n0\n50\nAggregate Performance\nDiscriminative\n20 40 60\nlanguages\n−20\n−10\n0\n10\n20\n30\nAggregate Performance\nGenerative\n20 40 60\nlanguages\n−40\n−20\n0\n20\n40\n60\nAggregate Performance\nAll\nModel Training\nPT IT\nModel Training\nPT IT\nModel Training\nPT IT (b) Excluding PT/PT, IT/PT, and IT/IT models\nFigure 12: Zero-shot aggregated performance against model multilinguality.\n20527",
  "topic": "Maltese",
  "concepts": [
    {
      "name": "Maltese",
      "score": 0.9760946035385132
    },
    {
      "name": "Benchmarking",
      "score": 0.8385028839111328
    },
    {
      "name": "Computer science",
      "score": 0.752302348613739
    },
    {
      "name": "Natural language processing",
      "score": 0.6600724458694458
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6033728122711182
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.5403050184249878
    },
    {
      "name": "Linguistics",
      "score": 0.21813637018203735
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ]
}