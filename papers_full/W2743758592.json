{
  "title": "A Novel Holistic Disease Prediction Tool Using Best Fit Data Mining Techniques",
  "url": "https://openalex.org/W2743758592",
  "year": 2017,
  "authors": [
    {
      "id": null,
      "name": "Salim A. Diwani, et al.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2146739259",
    "https://openalex.org/W1993946222",
    "https://openalex.org/W2159368141",
    "https://openalex.org/W2103069675",
    "https://openalex.org/W2729965111",
    "https://openalex.org/W2316334284",
    "https://openalex.org/W2169974402",
    "https://openalex.org/W2001859769",
    "https://openalex.org/W2183382243",
    "https://openalex.org/W2398524696",
    "https://openalex.org/W1988543519",
    "https://openalex.org/W6683581212",
    "https://openalex.org/W6681822247",
    "https://openalex.org/W2331491755",
    "https://openalex.org/W6716547243",
    "https://openalex.org/W2034870679",
    "https://openalex.org/W6678304922",
    "https://openalex.org/W6683322786",
    "https://openalex.org/W4239944110",
    "https://openalex.org/W4299495555",
    "https://openalex.org/W2122379760",
    "https://openalex.org/W2158275940",
    "https://openalex.org/W2963982993",
    "https://openalex.org/W2148489082",
    "https://openalex.org/W4240252876",
    "https://openalex.org/W2540842519",
    "https://openalex.org/W2128420091",
    "https://openalex.org/W2127609549",
    "https://openalex.org/W2417280899",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2133990480",
    "https://openalex.org/W208128215"
  ],
  "abstract": "Given that, today, the healthcare ecosystem is an information rich industry, there is an increasing demand for data mining (DM) tools to improve the quantity and quality of delivered healthcare; especially in handling patients suffering from deadly diseases such as HIV, Breast Cancer, Diabetes, Tuberculosis (TB), Heart diseases and Liver disorder.Given the fatality nature of these diseases when they remain undetected until at advanced stages, there remains a demand for best classifier tools to assist in diagnosing, detecting and treatment of these life-threatening diseases at their early stages.Complementary to this demand is the fact that the healthcare industry today generates large amounts of complex data about patients, hospital resources and disease diagnosis.Consequently, the healthcare ecosystem is warehousing large amount of medical data, which is an asset for healthcare organizations if properly utilized.The large amount of patient and disease related data could be processed and analyzed for knowledge extraction that enables support for cost savings and decision making towards delivery of timely and quality healthcare.In this paper, we report on an ongoing research work to develop and test a holistic DM disease prediction (Diagnosis and prognosis) tool, equipped with processes for preprocessing patients' data and a learning procedure for selecting a disease-specific best classifier, for disease prediction and delivery of speedy and cost effective diagnostic interventions and patient follow up in a hospital environment.As diseases are diagnosed, the predictive tool helps medical doctors in decision-making about what disease case it is and suggests possible treatment strategies within a much-reduced time.Test results for breast cancer and HIV data sets are reported.Achieved from the reported work are classification accuracies of 97.0752% (Classifier acting singly); 97.6323% (fusion of three classifiers).These results are better than those reported in the literature.The results show that the proposed DM disease prediction tool has potential to greatly impact on current patient management, care and future interventions against deadly diseases.",
  "full_text": " \n \n \nInternational Journal of Computing and Digital Systems \nISSN (2210-142X)  \nInt. J. Com. Dig. Sys. 6, No.2 (Mar-2017) \n \n \nE-mail: diwania@nm-aist.ac.tz, zaipuna.yonah@nm-aist.ac.tz \n  http://journals.uob.edu.bh \n  \nA Novel Holistic Disease Prediction Tool Using  \nBest Fit Data Mining Techniques \n \nSalim A. Diwani1 and Zaipuna O. Yonah2 \n \n1, 2Department of Information Technology, Systems Development and Management, Nelson Mandela African Institution of Science \nand Technology , Arusha. Tanzania. \n \nReceived 7 Nov.2016, Revised 7 Dec. 2016, Accepted 23 Jan. 2017, Published 1 Mar. 2017 \n \n \nAbstract: Given that, today, the healthcare ecosystem is an information rich industry, there is an increasing demand for data mining \n(DM) tools to improve the quantity and quality of delivered healthcare; especially in handling patients suffering from deadly diseases \nsuch as HIV, Breast Cancer, Diabetes, Tuberculosis (TB), Heart diseases and Liver disorder. Given the fatality nature of thes e \ndiseases when they remain undetected until at advanced stages, there remains a demand for best classifier tools to assist in \ndiagnosing, detecting and treatment of these life -threatening diseases at their early stages. Complementary to this demand is the f act \nthat the healthcare industry today generates large amounts of complex data about patients, hospital resources and disease dia gnosis. \nConsequently, the healthcare ecosystem is warehousing large amount of medical data, which is an asset for healthcare or ganizations \nif properly utilized. The large amount of patient and disease related data could be processed and analyzed for knowledge extr action \nthat enables support for cost savings and decision making towards delivery of timely and quality healthcare. \nIn this paper, we report on an ongoing research work to develop and test a holistic DM disease prediction (Diagnosis and prognos is) \ntool, equipped with processes for preprocessing patients’ data and a learning procedure for selecting a disease -specific best classifier, \nfor disease prediction and delivery of speedy and cost effective diagnostic interventions and patient follow up in a hospital  \nenvironment. As diseases are diagnosed, the predictive tool helps medical doctors in decision -making about what disease  case it is \nand suggests possible treatment strategies within a much -reduced time. Test results for breast cancer and HIV data sets are reported. \nAchieved from the reported work are classification accuracies of 97.0752% (Classifier acting singly); 97.6323%  (fusion of three \nclassifiers). These results are better than those reported in the literature. The results show that the proposed DM disease p rediction \ntool has potential to greatly impact on current patient management, care and future interventions against deadly diseases. \n \nKeywords: Healthcare delivery, Data mining, Electronic medical records, HIV, Breast cancer, Diabetes, Tuberculosis, Heart \ndiseases, and Liver disorder. \n \n1. INTRODUCTION \nFundamental to healthcare industry is that health is \nlife, and witho ut good health there is no life. Health is \nnow conceptualized to be beyond the absence of disease \nto include good diet, sleeping well, physical wellbeing, \npositive attitudes and social justice. In the context of this \npaper, healthcare is seen as the attent ion to the physical \nhealth of a human being. Also, healthcare is seen as the \ntreatment, management and prevention of disease and \npreservation of the physical and mental wellbeing of a \nperson with the help of medical and allied health \nprofessionals.  \n \nComparatively, many developing countries like \nTanzania, have long suffered from inadequate and poor \nquality health services largely from lack of skilled health \ncare workers, capable of performing at specialist level in \nmedical disease diagnosis and prognosis. Overall, health  \n \nsectors in developing countries, also suffer from long \nwaiting times at health facilities for quality health care. \nFor example, many patients spend a lot of time moving \nfrom one facility to another seeking diagnostic service s. \nAs this happens, patients are usually exposed to too much \nunnecessary care, lack of transparency, and delays during \nregistration and payment for the healthcare, and \navoidable harm to patients. Such waiting times and \ninconveniences are now known to cause huge economic \nlosses: in human terms and waste in billions of dollars \n[1], [2]. Essentially, it is a human resource crisis tied to a \nlaboring referral system that is mainly affecting the \nquality of healthcare delivery. In order to mitigate this \nchallenge there is a need  for intervention tools that can \nassist the healthcare workers in proper disease diagnosis \nand prognosis towards effective utilization of available \nhuman resources. Of interest in this paper,  is the use of \nbest fit data mining (DM) techniques to develop an d \nhttp://dx.doi.org/10.12785/ijcds/060202 \n \n \n64        S. A. Diwani & Z.O. Yonah: A Novel Holistic Disease Prediction Tool Using Best Fit… \n \n \nhttp://journals.uob.edu.bh \nimplement a disease diagnosis and prediction tool with \nthe goal of improving the quantity and quality of \ndelivered healthcare; especially in handling patients \nsuffering from deadly diseases such as HIV, Breast \nCancer, Diabetes, Tuberculosis (TB), Heart d iseases and \nLiver disorder.  Given the fatality nature of these diseases \nwhen they remain undetected until at advanced stages, \nthere remains a demand for best classifier tools to assist \nin detecting these life -threatening diseases at their early \nstages. Complementary to this demand is the fact that the \nhealthcare industry today generates large amounts of \ncomplex data about patients, hospital resources and \ndisease diagnosis. Consequently, the healthcare \necosystem is an information rich industry, warehousing \nlarge amount of medical data – a kind of “Big Data” - \nwhich is an asset for healthcare organization s if properly \nutilized. The large amount of patient - and disease- related \ndata could be processed and analyzed for knowledge \nextraction that enables support for cost savings and \ndecision-making towards delivery of timely and quality \nhealth care. \n \nIn this paper, we report on an ongoing research work \nto develop and test a holistic data mining (DM) disease \nprediction tool, equipped with best classifier, for use t o \ndeliver speedy intervention and patient follow up in a \nhospital environment. Thus, better addressing patient’s \nneeds, with the potential to improve care quality and to \nreduce care costs. Disease cases considered include HIV, \nBreast Cancer, Diabetes, Tube rculosis (TB), Heart \ndiseases and Liver disorder, all in their early stages of \ndevelopment. The paper is organized into six sections. \nSection II covers a literature review related to the \nreported work. Section III describes the Methodology \nused in the prediction design and testing of the prediction \ntool; and reports on the data collection component and \ncleaning algorithms of the prediction tool. Section IV \nreports about disease learning, classification and \nprediction components of the diagnosis tool and cla ssifier \nperformance criteria. Section V reports on test results \nfrom the disease prediction tool for breast cancer and \nHIV diseases, and Section VI carries the conclusions. \n2. LITERATURE  REVIEW \nSeveral pioneers in the healthcare industry have \nattempted to pr opose interventions for enhancing the \ndelivery of healthcare [3 -8]. Kuttikrishnan, et al . [3] \npropose a healthcare system to assist clinicians at the \npoint of care by enabling the clinician to interact with the \nsystem to help determine diagnostic and progn osis of \npatient’s data. The system consists of three parts: the \nknowledge base, inference engine and mechanism to \ncommunicate. In [4], Zhou, et al . propose  a traditional \nChinese medicine (TCM) clinical data warehouse (CDW) \nfor medical knowledge discovery a nd decision support. \nThe TCM is a clinical data warehouse system that \nincorporates the structured electronic medical record \n(SEMR) data for medical knowledge discovery and TCM \nclinical decision support system. Karya, [5], proposes \ndata mining techniques fo r diagnosis and prognosis of \nbreast cancer from Wisconsin database. In the \nexperiment, different classifiers were used and their \nperformance compared; and decision tree classifier was \nfound to be superior with a classification accuracy of \n93.62%, followed by Naïve Bayes with a classification \naccuracy of 84.5%. Sudhir, et al . [6], propose a Neural \nNetwork Aided Breast Cancer Detection and Diagnosis \nusing Support Vector Machine (SVM). Breast Cancer \ndata set from Wisconsin database was used in the \nexperiment. The highest accuracy achieved by SVM is \n96.43%, which can be utilized to support doctors’ \ndecision to avoid biopsy. In [7], Palaniapan, et al . \ndevelop a web based user friendly, scalable, reliable and \nexpandable intelligent heart disease prediction system \nusing data mining techniques namely: decision trees, \nNaïve Bayes and neural network s. The tool can extract \nhidden patterns associated with heart disease from a \ndatabase. The CRISP DM is used to build the disease \nprediction system. In their experimental res ults, Naïve \nBayes performs better with classification accuracy of \n95%, followed by decision tree with classification \naccuracy of 94.93%, followed by a neural network with \nclassification accuracy of 93.54%.Chaurisa , et al . [8], \npresent a performance analysi s of data mining algorithms \nfor diagnosis and prediction of heart and breast cancer \ndiseases. They took advantage of technological \nadvancements to develop prediction models for patients \nwith heart diseases and breast cancer survivability. In the \nexperiment, the breast cancer datasets from Wisconsin \nwere used. The results in this case indicated that the \nNaïve Bayes performs better than other classifiers with \n87.1% accuracy. \n \nWhat can be said about existing Literature on \nprevious research outcomes is that the re is no established \nstandard that guides the data mining efforts of finding \nclassifiers with acceptable high classification accuracies. \nIt is concluded that in order to get better classification \naccuracy it depends on how one conditions the available \ndata sets before prediction, i.e. preprocessing the data \nsets before the prediction exercise is done; estimation of \nbaseline performance for the data sets and how the \nparameters of selected learning algorithm(s)  are tuned in \norder to get best performance.  \n3. METHODOLOGY AND DATA COLLECTION   \nIn this section we present the methodology used in \nthe reported work and the data collection process for the \npurpose of obtaining data for training and testing the \nprediction tool. \n \n \n \n                                                                               Int. J. Com. Dig. Sys. 6, No.2, 63-72 (Mar-2017) 65 \n \n \nhttp://journals.uob.edu.bh \nA. Data Processing Stages of the Prediction Tool \nThe tool predicts a disease the patient may be having \nbased on symptoms. Figure 1 (stages A -N) summarizes \nthe data processing stages of the prediction tool. It is \nsimilar to an expert doctor who asks questions to identify \nsymptoms on the patient. Then, the tool classifies and \nanalyzes the symptoms and other feedback data collected \nfrom the patient. The tool may ask a patient to take \nadditional or necessary tests based on adequacy of the \npatient’s symptoms. Then the tool selects the patient’s \ndata for disease diagnosis.  \n \nPatient Consults \nwith Physician\nClassify and \nanalyze \nSymptoms and \nother feedback \ncollected from \npatient\nPerform \nadditional/\nnecessary \nlaboratory tests \nbased on \nsymptoms\nApply Data \ncleaning \nalgorithms\nAccept \nPrediction\nSelect data for \ndisease \ndiagnosis\nName Disease\nNO\nYES\nPerform Data \npreprocessing\nPerform 10 \nCross \nValidation\nApply \nClassification \nalgorithms\nExecute \nDisease \nPrediction \nModel\nDisease \nPatterns\nSuggest Drug and \nTreatment\nMonitor Patient \nPerformance\nRepeat\nPrediction\nprocess\nA:\nB:\nC:\nD:\nE:\nF:\nG:\nH:\nI:\nJ:\nK:\nL:\nM:\nN:\nPatient\n \nFigure 1.  Flow chart of the data mining disease prediction tool. \n \nAs illustrated in Fig. 2, data cleaning algorithms are \nused to clean the selected data. During the data cleaning \nprocess, unacceptable values,  namely: outliers and \nextreme values, missing values and noisy data are \nremoved. They are first identified , mark ed and \nsubsequently handled usually by removing them from the \ntest data sets. These unacceptable values are identified \nand marked by using appro priate unsupervised attributes \ndepending on the DM platform being used. Equally, the \nunacceptable instances or samples are removed by using \nappropriate unsupervised instance filters. \n \nApply Data Cleaning Algorithm\nRemoving missing values\nRemoving noisy data\nDetecting and removing outliers\nDetecting and removing extreme values\nRemoving duplicate values\nApply Data Reduction\nSelected Raw Data Set\nCalculate Chi Square\nCalculate Info Gain\nCalculate Gain Ratio\nCalculate Average Rank\nFeature Selection\nIntegrate Data\nResolving Data\nValue Conflict\nData Transformation\nProcessed Data set\n \nFigure 2.  Flow chart of data cleaning algorithm  \n(Process E in Fig.1). \n \nThis is done for the purpose of having error free data \nsets that lead to better classification accuracy from \nselected classification algorithms during disease \nprediction. During data pre -processing, some \nnormalization, discret ization, data summarization and \ndata reduction techniques are applied. All these \nprocedures are done to make the data sets more \nappropriate or efficient for the classification task. Lastly, \nsince real data sets  are used, often coming from different \nsources and thus may have different formats, data \ntransformation is done to make sure all have same \nformat.  \n \nThen a  10-fold cross validation of the datasets is \nperformed in that learning algorithms are evaluated and \ncompared by  dividing data sets into two segmen ts: one \nsegment is used to train the algorithm s and the other is \n \n \n66        S. A. Diwani & Z.O. Yonah: A Novel Holistic Disease Prediction Tool Using Best Fit… \n \n \nhttp://journals.uob.edu.bh \nused to validate the algorithm s. The training and \nvalidation sets must cross over in successive rounds such \nthat each data point has a chance of being validated 10 \ntimes against each other an d the average is computed. \nTherefore, the 10 cross validation helps in algorithm \nselection based on error rate. In the next stage, \nclassification algorithms are applied on the test data sets \nfollowed by  disease prediction based on the data sets \nused. Once the prediction is accepted, the disease can be \nidentified. Lastly, the tool will suggest a treatmet strategy \nand drugs for the patient and thereafter it can be used to \nmonitor the patient’s performance.  \n \nB. Data Collection (Processes A-D in Figure 1) \n \nThe da ta collection aspect of the prediction tool \nrefers to activities or processes done in stages A-D in Fig. \n1. The first source of data contains data sets for breast \ncancer, diabetes, heart disease and liver disorder \navailable from University of California Ir vine (UCI) \nmachine learning data repository. The second source of \ndata contains HIV data sets from Amana hospital in Dar -\nes-salaam, Tanzania. The data sets are summarized in \nTable 1. Each data set consists of a number of samples \nand disease attributes. \n \nTABLE 1. Summary of attributes of data sets of various diseases. \n \n \n4. CLASSIFIER TRAINING AND PERFORMANCE \nEVALUATION \nIn this study, classifier algorithms  to be used in the \ndisease prediction tool were selected due to their \npopularity and that they are used by  many researchers for \nsimilar disease classification [9].  Essentially, algorithm \nselection is a very time consuming task that involves \nexperimentation with different classifiers and analyzing \nthe performance of these classifiers [10]. According to \n“No Free  Lunch (NFL) Theorem” Duda et al . [11], no \nany single classifier has the best performance in all the \ndata sets. Each data set must be tested using different \nalgorithms or groups of algorithms selected for prediction \npurposes. In order to get good performan ce ofthe selected \nclassification algorithms it is crucial to clean the input \ndata. The way data is cleaned has an impact on classifier \nperformance; which depends on how much data can be \nchanged and rearranged. Also, type of variables  in the \nselected data sets are taken into consideration since some \nalgorithms only accept numeric data, some accept only \nnominal and some algorithms accept both types.  Trial \nand error and Meta learning approaches were used to \nselect the best classification algorithms. In the tri al and \nerror approach , available classifiers are applied to the \ndata sets and the best performing ones are selected based \non classification accuracy, which ranges from 0 – 100%. \nClose to 100% an accuracy, the better. \n \nA. Classifier Training \nA Meta learning ap proach aims at automatic \ndiscovery of useful algorithms or system. Such a Meta \nlearning process is illustrated in  Fig. 3. A database is \ncreated with Meta data descriptions of datasets. These \nmeta-data contain estimates of the performance of a set \nof candidate algorithms on those datasets as well as some \nMeta features describing their characteristics. A machine \nlearning algorithm is applied to this database to induce \nthe model that relates the value of the Meta features to \nthe performance of the candidate al gorithms [12].  The \nneed for Meta learning became necessary given that  \nWEKA data mining tool [13] is being used for algorithms \nselection. The WEKA platform, as are many DM tools, \nhas many algorithms to select from. This makes  the job \nof selecting algorithms an extremely difficult task. \n \nData Characterization\nAlgorithm 1\nAlgorithm p\nEvaluation\nMeta \nLearning\nMeta \nData\nData \nsets\n \n \nFigure 3. Process of Meta learning [15]. \n \nFollowing the Meta learning exercise , eight different \npredictive classifiers were selected. These classifiers are : \nNaïve Bayes (NB),  Decision Tree J48 ( J48), Instance \nBased Learning (IBK), Sequential Minimum \nOptimization (SMO), Multilayer Perceptron (MLP), \nDecision Tree RepTree, Projective Adaptive Reasonance \nTheory (PART) and Random  Forest (RF) ( individual \nclassifiers). Given that fusion of such learning  algorithms \n(i.e combining the algorithms ) can increase the \nclassification accuracy of the resulting fused hybrid \nclassifiers [14], the performance of the eight selected \nclassifiers was investigated by training them to perform \nclassification and prediction  while acting singly, in pairs \nor in groups of three.  These classifiers are briefly \ndescribed in the following paragraphs. \n \n \n                                                                               Int. J. Com. Dig. Sys. 6, No.2, 63-72 (Mar-2017) 67 \n \n \nhttp://journals.uob.edu.bh \na) Naïve Bayes (NB)  - Naives Bayes is a \nsupervised learning algorithms based on Bayes \ntheorem with independent assumption between \nprediction. Naïve Bayes uses frequency tables built \nfrom the data sets used. The Naïve Bayes classfier  \nprediction power is based on the classifier accuracy \nderived from the concept of probability [16]. \n \nb) Decision Tree J48  (J48) - Decision tree is a \ndivide and co nquer algorithm, which is a top down \napproach. The top down approach works by \nrecursively breaking down the complex problem into \nsub problems and then finding the solutions of sub \nproblems by combining those  solutions to form a \ncomplex solution. Decision t ree uses decision tables \nbuilt from the data sets used. The core algorithm of \ndecision tree is  called ID3 by J.R. Quinlan [17] , \nwhich uses entropy and information gain to construct \na tree. For further details see [18].  \n      \nc) Instance Based Learning  (IBK) - IBK is the K \nnearest neighbour classification algorithm [ 19] that \nis b ased on similarity functions. K th nearest \nneighbour is a simple algorithm that  predicts new \ncases of the stored data based on a similarity \nmeasure. The prediction is done by selecting the \nnearest neighbour and calculating which ones are the \nnearest. The nearest neighbour can be calculated \nusing the Euclidean distance method [20]. \n \nd) Sequential Minimum Optimization (SMO) - \nSMO is a supervised learning algorithm that works \nthe same way as s upport vector machine (SMV) \nalgorithms that was introduced by Vapnik,  et al . \n[21]. SMO is used to classify two different classes \nand therefore the goal is to design a hyperplane that \nclassifies all training vectors into two classes. The \nalgorithm will sele ct the hyperplane that leaves the \nmaximum margin from both classes and the closest \nelement from those hyperplane. Therefore, the \nalgorithm draws the widest channel or street between \ntwo classes. \n \ne) Multilayer Perceptron  (MLP) - Multilayer \nPerceptron (MLP) is  a feed forward back \npropagation network, a very powerful and \ncomplicated data mining algorithm based on \nneurons. MLP is a highly parallel algorithm that \nprocesses information much more like a brain rather \nthan a serial computer. MLP is a supervised learni ng \nalgorithm based on artificial neural network which \nconsists of input layer, output layer and hidden layer. \nAs illustrated in Fig. 4, each layer is made of \ninterconnected nodes; for instance , input layers are \ninterconnected with hidden layers and hidden layers \nare interconnected with output layers, where the \nactual processing is done using a system of weighted \nconnections [22], [23]. \n1\n4\n2\n3\n5\n6\nInputs Layers\nHidden Layers\nOutputs Layers\n \n \nFigure 4. General architecture of MLP. \n \nf) Random Forest  (RF) - Random forest is a \nsupervised machine-learning algorithm based on \ndecision trees. Random Forest classifier also uses \nfrequency tables processed from data sets. It works \nthe same way as decision tree but this one is more \npowerful and uses the technique of ensemble \nlearning algorithms by combining weak classifiers to \nget more powerful classifier. Therefore, random \nforest works as a large collection of different \ndecision trees algorithms or forest as the name \nimplies all used to make classification. That’s the \nreason random forest uses  the bagging technique \n[24], [25]. \ng) PART - PART stands for Projective Adaptive \nResonance Theory. The inputs to the PART \nalgorithm are the vigilance and distance para meters \n[26]. PART is a separate -and-conquer rule learner  \nproposed by Eibe and Witten [27]. The al gorithm \nproduces sets of rules called decision list, which are \nordered set of rules. A new data is compared to each \nrule in the list in turn, and the item is assigned the \ncategory of the first matching rule (a default is \napplied if no rule successfully mat ches). PART is a \npartial decision tree algorithms derived from C4.5 \ndecision tree in each iteration and makes the best leaf \ninto a rule. The algorithm is a combination of C 4.5 \nand RIPPER rule learning [28]. \nh) Decision Tree Rep Tree  - Reduced Error \nPruning (REP) Tree Classifier is a fast decision tree \nlearning algorithm and builds the tree based on \ninformation gain with entropy and minimizes  the \nerror arising fro m variance [29 ]. This algorithm was \nfirst recommended in [30 ]. REP Tree applies \nregression tree log ic and generates multiple trees in \naltered iterations. Afterwards, it picks the best one \nfrom all spawned trees. This algorithm constructs the \nregression/decision tree using variance and \n \n \n68        S. A. Diwani & Z.O. Yonah: A Novel Holistic Disease Prediction Tool Using Best Fit… \n \n \nhttp://journals.uob.edu.bh \ninformation gain. Also, this algorithm prunes the tree \nusing reduced -error pruning with back fitting \nmethod. At the beginning of the model preparation, \nit sorts the values of numer ic attributes once. As in \nC4.5 a lgorithm, this algorithm also deals with the \nmissing values by splitting the correspo nding \ninstances into pieces. [31]. \nB. Performance Evaluation Criteria \nThe selected classifiers were evaluated based on a \nconfusion matrix, which is a visualization tool commonly \nused to present the accuracy of classifiers [16]. Table 2 \nsumarizes the entries of the confusion matrix used.  The \naccuracy is a measure of closeness between  the target \ndata and the predicted data. \n \nTABLE 2. Confusion matrix for predictive modeling \n \n \n \nThe level of performance in the confusion matrix is \ncalculated by identifying the classifier accuracy of \ncorrectly classified instances and incorrectly classified \ninstances in  terms of percentage of samples tested.  The \nentries in the confussion matrix have the following \nmeaning: \n “a”- is True Positive (TP), which is the number \nof positive samples correctly predicted; \n “b”- is False Negative (FN), which is the \nnumber of positive samples wrongly predicted; \n “c” -  is False Positive  (FP), which is the \nnumber of negative samples wrongly predicted \nas positive; and \n “d” - is True Negative (TN), which is number of \nnegative samples correctly predicted. \n5. EXPRIMENTAL RESULTS \nTwo experiments were conducted to evaluate the \nselected performance of the six learning algorithms  (see \nSection 4). \n \nA. Learning Experiment Using Wisconsin Breast \nCancer Dataset: \nFigure 5 shows performance of the s elected \nclassifiers before and after preprocessing input datasets \nbased on a 10 -fold cross validation as a test option. \nObserved is that the accuracy of classifiers is better when \npreprocessed data are used compared to their \nperformance on unprocessed data ; data cleaning makes \nall the difference. Random Forest performs better with a \nclassification accuracy of 97.0752% compared with other \nclassifiers, followed by SMO a nd MLP classifiers with \naccuracies of 96.5181% and 96.3788%, respectively. \n \n94.5682\n95.2646\n96.5181 96.3788 96.3788\n94.8468\n97.0752\n93.4114\n94.8755\n96.1933 95.9004 96.1933\n93.5578\n96.9253\n91\n92\n93\n94\n95\n96\n97\n98\nJ48 RepTree SMO IBK MLP PART RandomForest\nafter\tpreprocessing Before\tPreprocessing\n \n \nFigure 5. Classification accuracy on breast cancer data set before \nand after data preprocessing \n \nFigure 6 shows the performance of the classifiers \nacting in pairs. The RF + SMO combination out performs \nthe other pairs with a class ification accuracy of \n96.1933%.  As a general observation, it is clear that \npairing the classifiers does not produce a  higher \nclassification accuracy. F or example: the RF + RepTree \ncombination performs with accuracy of 95.754% \nrelatively poorer than its individual classifiers, e.g. RF \nalone ha s an accuracy of 97.0752%. These results \njustified the need to consider performance of the \nclassifiers in groups of three. Figure 7 shows \nperformance in terms of classification accuracy of a \ncombination of the SMO + RF with one other classifier. \nThis combination was selected as a primary pair because \nthe two classifiers perform better as a pair (see Fig. 6). \nTherefore, the classifiers with two other classifiers are \nSMO+RF+IBK, SMO+RF+J48 and SMO+RF+MLP. It \ncan be observed that the combinations of SMO+RF+IBK  \nand SMO+RF+MLP achieve better performance  with \nclassification accuracy of 97.6323%; followed by \nSMO+RF+J48 with 97.493% accuracy.  \n \n95.4612\n95.6076\n96.1933\n95.754\n95.6076\n95\n95.2\n95.4\n95.6\n95.8\n96\n96.2\n96.4\nRF+PART RF+MLP RF+SMO RF+RepTree RF+J48\n \nFigure 6. Classification accuracy of fusion of classifiers in pairs \nafter data preprocessing \n \n \n                                                                               Int. J. Com. Dig. Sys. 6, No.2, 63-72 (Mar-2017) 69 \n \n \nhttp://journals.uob.edu.bh \n97.6323\n97.493\n97.6323\n97.4\n97.45\n97.5\n97.55\n97.6\n97.65\nIBK+SMO+RF J48+RF+SMO SMO+RF+MLP\n\t\n \n \nFigure 7.  Classification accuracy of fusion of three classifiers after \ndata preprocessing. \n \nAfter investigating the performance of the selected \nclassifiers acting singly, in pairs and in groups of three, it \nbecame necessary to compare their performance in terms \nof classification accuracies to results reported in the \nliterature. Table 3 summarizes the comparison of \naccuracies when the classifiers are applied to the same \ndatasets used by other researchers.  It can be noted that \nthe new approach produces classification accuracies of \n97.0752% (Classifier acting singly); 97.6323% ( fusion of \nthree classifiers ), which are much better than those \nreported in the literature. \n \nTable 4 shows the prediction results. Out of the \navailable breast cancer data set: 477 samples (known) \nwere used for training, and 206 (unknown) samples were \nused for testing. \n \nTABLE 3. Comparison of achieved classification accuracies to those  \nreported by other works  when using  same breast cancer data set. \n \n \n \n \nTABLE 4. Confusion matrix of training and testing data for best \nclassifiers. \n \n \n \nTable 5 shows the sensitivity, specificity and \nprediction for training and testing data for testing of \nBreast Cancer data set. Observable is  that the \ncombinations: IBK+SMO+RF, J4 8+RF+SMO and \nSMO+RF+MLP perform  better with sensitiv ity (0.9869, \n0.9837 and 0.9773) and Specificity (0.9532, 0.9529 and \n0.9524).  \n \n \nTABLE 5. Performance of training and testing data, with prediction \ndone based on confusion matrix of each classifier \n \n \n \nTable 6 shows the feature selection results of the \nBreast Cancer attributes. Feature selection is a process of \nselecting input variables or attributes and then \nhighlighting the importance of attributes that are selected \nfor use in the disease diagnosis and prediction. The main \nmission of the feature selection  is to improve the \nperformance of a classifier. From this table, it can be seen \nthat the best attribute selected was Bare Nuclei followed \nby uniformity of cell shape and the least attribute selected \nis mitoses followed by single epithelial cell size. \n \nTABLE 6. Feature Selection results. \n \n \nB. Experiment Using CTC HIV Datasets From Amana \nHospital: \nFigure 8 shows performance of different classifiers \nbefore and after preprocessing based on 10 -fold cross \nvalidation as a test option. Once again, it is demonstrated  \nthat classifiers acting on pre -processed datasets achieve \nbetter performance in terms of classification accuracy \nthan before preprocessing due to data cleaning. In this \nHIV case, the SMO classifier performs better with an \naccuracy of 90.9014% compared wit h other classifiers, \nfollowed by Naïve Bayes and MLP classifiers with \nclassification accuracies of 90.7029% and 90.4762%, \nrespectively. \n \n \n \n70        S. A. Diwani & Z.O. Yonah: A Novel Holistic Disease Prediction Tool Using Best Fit… \n \n \nhttp://journals.uob.edu.bh \n90.7029 90.9014 90.2494 90.4762 90.4478 90.391290.4762 90.3061\n82.4546\n89.4558 90.2494 89.7676\n78\n80\n82\n84\n86\n88\n90\n92\ntree.J48 function.SMO lazy.IBK bayes.NaiveBayes function.MLP tree.RandomForest\nafter\tpreprocessing Before\tPreprocessing\n \nFigure 8. Classification accuracy on HIV data set before and after \ndata preprocessing. \nFigure 9 shows the performance  of the selected \nclassifiers when acting in pairs.  In this case, the SMO + \nJ48 combination out performs the other pairs with a \nclassification accuracy of 90.9297%. Coincidently, the \npair slightly performs better than the individual \nclassifiers as in Fig. 8 .  Again, these results justified the \nneed to consider performance of the classifiers in groups \nof three.   \n \nFigure 10 shows performance in terms of \nclassification accuracy of a combination of the SMO and \nJ48 with one other classifier. This combination is \nselected as a primary pair because the two classifiers \nperform better as a pair (see Fig. 9). Therefore, the hybrid \nclassifiers ( classifiers grouped in three ) are \nSMO+J48+NB and SMO+J48+MLP. It can be observed \nthat the combination of SMO + J48 + MLP achiev es \nbetter performance with classification accuracy of \n91.3265%; followed by SMO+J48 +NB with 91.0998% \naccuracy.  For this reason , the SMO+J48+MLP \ncombination was selected for prediction of HIV disease \nfor the data set from Amana Hospital Dar -es-salaam, in \nTanzania. \n \n88.4921\n90.9297\n88.0952\n84.949\n83.1916\n78\n80\n82\n84\n86\n88\n90\n92\nSMO+NB SMO+J48 SMO+RF SMO+IBK SMO+MLP\n \n \nFigure 9. Classification accuracy of classifiers grouped in two after \ndata preprocessing. \n \n91.3265\n91.0998\n90.95\n91\n91.05\n91.1\n91.15\n91.2\n91.25\n91.3\n91.35\nSMO+J48+MLP SMO+J48+NB\n \n \nFigure 10. Classification accuracy of classifiers grouped in three \nafter data preprocessing. \n \nTable 7 shows the prediction results. Out of the \navailable HIV data set: 2116 samples (known) were used \nfor training, and 1412 (unknown) samples were used for \ntesting.Table 8 shows the sensitivity, specificity and \nprediction for training and testing data.  \n \nTABLE 7.    Confusion matrix of training and testing data. \n \n \n \nFor testing of HIV data set, it was  observed that \nsensitivity for testing dataset performs better than the \ntraining dataset. Table 9 shows the feature selection \nresults of the HIV attributes. From the table we can see \nthe best attribute selected is WHOStage followed by CD4 \nand the least attribute selected is ARVStatusCode \nfollowed by NowPregnant. \n \nTABLE 8. Performance of training and testing data; prediction done \nbased on confusion matrix of each classifier. \n \n \n \n \n \n \n \n \n \n \n \n \n \n                                                                               Int. J. Com. Dig. Sys. 6, No.2, 63-72 (Mar-2017) 71 \n \n \nhttp://journals.uob.edu.bh \nTABLE 9. Feature selection results. \n \n \n6. CONCLUSION \nDisease detection and its treatment methods is a \nmajor area of concern that needs much attention these \ndays. This paper supports the fact that machine learning \ncan be of big help when it comes to medical diagnosis \nand prognosis. Presented in this paper is an approach for \ndetection and prediction of two deadly diseases using \nmachine learning techiques. The presented tool can assist \nphysians either new or experienced in medical diagnosis \nand prognosis at initial stages of the diseas es. The main \nissue here is to save time, reduce healthcare costs, quality \nhealthcare delivery and reduce mortality and morbidity \nrate, which is very crucial in life threatening \ndiseases.Therefore, the developed tool can help \nphysicians make more accurate d iagnosis as well as get \nanswers they often  seek from individual patients. As \ndiseases are diagnosed, the predictive tool helps medical \ndoctors in decision -making about what disease case it is \nand suggests possible treatment strategies within a much -\nreduced time. Test results for breast cancer and HIV data \nsets are reported. Achieved in the reported work are \nclassification accuracies of 97.0752%(Classifier acting \nsingly); 97.6323% (fusion of three classifiers).  The \nresults show that the fusion of three cla ssifiers is superior \nto others classifiers [3 -6] reported in the literature. \nTherefore, the results confirm that the proposed DM \ndisease prediction tool has potential to greatly impact on \ncurrent patient management, care and future interventions \nagainst de adly diseases such  as HIV, Breast Cancer , \nDiabetes, Tuberculosis (TB), Heart Diseases and Liver \ndisorder. \nREFERENCES \n[1] Institute of Medicine, To Err Is Human: Building a Safer Hea lth \nCare System, Washington, D.C.: National Academies Press, 2000. \n[2] B. Jennings, M. A. Baily, M. Bottrell  and J. Lynn “Health Care \nQuality Improvement: Ethical and Regulatory Issues” The \nHastings Center Garrison, New York 2007. \n[3] M. Kuttikrishnan, I. Jeyaraman  and M. Dhanabalachandran., “An \nOptimised Intellectual Agent Based Secure Deci sion System for \nHealth Care”. International Journal of Engineering S cience and \nTechnology, Vol. 2, No. 8, pp. 3662-3675, 2010. \n \n[4] X. Zhou, S. Chen, B. Liu, R. Zhang, Y. Wang, P. Li, Y. Gu o, H. \nZhang, Z. Gao and X. Yan., “Development of traditional Chinese \nmedicine clinical data warehouse for medical knowledge \ndiscovery and decision support”. Elsevier Artificial Intelligence in \nMedicine Vol. 48, pp. 139–152, 2010. \n[5] S. Karya., “Using Data Mining Techniques for diagnosis and \nprognosis of cancer disease”. Internati onal Journal of Computer \nScience, Engineering and Information Technology (IJCSEIT), \nVol.2, No.2, April 2012 \n[6] S. D. Sawarkar, A. A. Ghatol  and A. P. Pande., “Neural Network \nAided Breast Cancer Detection and Diagnosis Using Support \nVector Machine”. Proceeding s of the 7th WSEAS International \nConference on Neural Networks, Cavtat, Croatia, June 12-14,  pp. \n158-163, 2006. \n[7] S. Palaniappan  and R. Awang ., “Intelligent Heart Disease \nPrediction System Using Data Mining Techniques”. International \nJournal of Computer Sci ence an d Network Security (IJCSNS), \nVol.8 No.8, August 2008. \n[8] V. Chaurasia and S. Pal., “Performance Analysis of Data Mining \nalgorithms for diagnosis and prediction of Heart and Breast \nCancer disease”. Review of Research Journal Vol. 3, No. 8, 2014. \n[9] J. Joja n and  A.Srivihok., “Duo Bundling Algorithms for Data \nPreprocessing: Case Study of Breast Cancer Data Prediction” \nLecture Notes on Software Engineering, Vol. 2, No. 4, November \n2014. \n[10] S. Cacoveanu, C.Vidrigh in and R. Potolea,  “Evolution Meta-\nLearning Framework For automatic Classifier Selection”, 2005. \n[11] R.O.Duda, P.E. Hart and  D. Stork, Patter Classification. J.Wiley \n& Sons, New York, 2001. \n[12] B. C. R. Prudencio  and T. B. Ludermin., “Uncertainty  Sampling \nMethods for Selecting Datasets in Active Meta Learning”. \nProceedings of International joint Conference on Neural \nNetworks, San Jose, California, USA, July 31 -August 5, 1082 -\n1089, 2011. \n[13] M. Hall, E. Frank, G. Holmes  and B.  Pfahringer., “The WEKA \nData Mining Software: An Update”. SIGKDD Explorations , Vol. \n11, No. 1, 2009. \n[14] X. Ceamanosa, B.  Waskeb, J. A. Benediktssonc, J. Chanussotd, \nM. Fauvele and J. R. Sveinsson., “A classifier ensemble based on \nfusion of support vector machines for classifying hyperspectral \ndata” International Journal of Image and Data Fusion Vol. 1, No. \n4, pp. 293–307, December 2010. \n[15] N. Bhatt, A. Thakkar  and A. Ganatra., “A Survey & Current \nResearch Challenges in  Meta Learning Approaches based on \nDataset Characteristics”. International Journal of Soft Computing \nand Engineering (IJSC E)ISSN:, Vol. 2, No. 1, pp. 2231 -2307, \nMarch 2012. \n[16] J. Han and M. Kamber, Data Mining: Concepts and Techniques, \nCA:Morgan Kaufmann Publishers, 2001. \n[17] J.R. Quinlan,  Decision trees and multi -valued attributes. In J.E. \nHayes & D. Michie (Eds.), Machine intelligence 11. Oxf ord \nUniversity Press (in press), 1985. \n[18] N. Soonthornphisaj, Artificial Intelligence, Bangkok, Thailand: \nKasetsart University, 2009. \n[19] A. Christobel and Y, Sivaprakasam.,“An Empirical Comparison \nof Data Mining Classification Methods”. International Journal of \nComputer Information Systems, Vol. 3, No. 2, 2011. \n \n \n \n72        S. A. Diwani & Z.O. Yonah: A Novel Holistic Disease Prediction Tool Using Best Fit… \n \n \nhttp://journals.uob.edu.bh \n[20] N. Patwari, J. N. Ash, S. Kyperountas, A. O. Hero, R. L.  Moses, \nand N. S. Correal.,“Locating the Nodes: Cooperative Localization \nin Wireless Sensor Network s”. IEEE Signal Process. Mag., V ol. \n22, No. 4, pp. 54–69, Jul. 2005. \n[21] V.N. Vapnik.,“The Nature of Statistical Learning Theory”. 1st ed., \nSpringer-Verlag, New York, 1995 \n[22] M. C. Popescu,  V.E. Balas, L. Perescu -Popescu and  N. \nMastorakis., “Multilayer Perceptron and Neur al Networks”. \nWSEAS Transactions on Circuits and Systems, Vol. 8, No. 7,  pp. \n579 – 588,  Jul. 2009. \n[23] E. Mattar, “A Practical Neuro-fuzzy Mapping and Control for a 2 \nDOF Robotic Arm System ”, International Journal of Computing \nand Digital Systems (IJCDS), Vol. 2, No. 3, pp. 109-121, 2013. \n[24] L. Breiman., “Random Forests”. Machine Learning,  Kluwer \nAcademic Publishers. Manufactured in The Netherlands, Vol. 45. \npp. 5 -32, 2001. \n[25] M. Al-Emran, “Hierarchical Reinforcement Learning: A Survey”. \nInternational Journal of Computing and Digital Systems (IJCDS),  \nVol. 4, No.2, pp. 137 -143, Apr-2015. \n[26] Yongqiang Cao and Jianhong Wu, “Projective ART for clustering \ndata sets in high dimensional spaces”, Elsevier Science Ltd, \nNeural Networks Vol. 15, pp. 105-120, 2002. \n[27] Witten IH, and Frank E. . Data mining: practical machine l earning \ntools and techniques – 2nd ed. the United States of America, \nMorgan Kaufmann series in data management systems., 2005. \n[28] Ali, Shawkat, and Kate A. Smith. \"On learning algorithm \nselection for classification.\" Applied Soft Computing 6.2: pp. 119-\n138, 2006 \n[29] J. Quinlan,  Simplifying decision trees, International Journal of \nMan Machine Studies, Vol.27, No. 3, pp. 221–234, 1987. \n[30] S.K. Jayanthi and S.Sasikala. 2013. REPTree Classifier for \nindentifying Link Spam in Web Search Engines. IJSC, Volume 3, \nIssue 2, (Jan 2013), 498 – 505. \n[31] J. Kittler, M. Hatef, R. Duin  and J. Matas. “On Combining \nClassifiers”. IEEE Transactions on Pattern Analysis and Machine \nIntelligence, VOL. 20, NO. 3, March 1998. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nSalim Amour Diwani received his BS \nand M.Sc. degrees in computer science \nfrom Jamia Hamdard University, New \nDelhi, India in 2006 and in 2008, \nrespectively. He is currently a PhD \nscholar in Information communication \nScience and Engineering at Nelson \nMandela African Institution of Science \nand Technology  in Arusha, Tanzania. \nHis primary research interests are in the areas of Big Data, Data \nMining, Machine Learning and Database Management Systems. \n \n \nZaipuna O. Yonah -MIET, MIEEE - \nholds a B.Sc. degree (with Honors - \n1985) in Electrical Engineering from \nUniversity of Dar es Salaam  - \nTanzania; and M.Sc. (1986) and PhD \n(1994) Degrees in Computer -based \nInstrumentation and Control \nEngineering from the University of \nSaskatchewan, Saskatoon-Canada.  In \nTanzania, he is a Registered \nConsulting Engineer in ICTs.  \nYonah has over 31  years of practice. His work spans the \nacademia, industry and policy making fields. He is currently \nassociated with The Nelson Mandela Institute of Science and \nTechnology, Applied Engineering & ByteWorks (T) Ltd and \nthe Institute of Electrical and Electronics (IEEE) Inc.  He is one \nof the mentors and pioneers driving the national broadband \nagenda in Tanzania  and EAC, SADC regions . He believes that \nICTs, as tools for development, promise so much: interactivity, \npermanent availability , global r each, reduced per unit \ntransaction costs, creates increased productivity and value, jobs \nand wealth , multiple source of information and knowledge .  \nArmed with such a belief, his current work aims at creating and \ndelivering value through ICT -enabled service s in the shortest \ntimes possible.  His research interests include: ICT4D, Mobile \nand Web applications, Big Data, Data Mining, High \nPerformance Computing, high-capacity Broadband networks, \nIntelligent Instrumentation and Control Engineering and ICT \nenabled 21st Century Education delivery  (ICT4E): \nPersonalized, Facilitated, and Connected Learning. \n \n \n \n \n \n \n \n \n \n \n \n \n",
  "topic": "Data mining",
  "concepts": [
    {
      "name": "Data mining",
      "score": 0.6323932409286499
    },
    {
      "name": "Computer science",
      "score": 0.6281661987304688
    },
    {
      "name": "Machine learning",
      "score": 0.36700063943862915
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34701743721961975
    }
  ]
}