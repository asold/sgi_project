{
  "title": "Large Scale Language Modeling in Automatic Speech Recognition",
  "url": "https://openalex.org/W1566315437",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4293488235",
      "name": "Chelba, Ciprian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224889331",
      "name": "Bikel, Dan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227645647",
      "name": "Shugrina, Maria",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3043600276",
      "name": "Nguyen, Patrick",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2587552697",
      "name": "Kumar, Shankar",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1484436897",
    "https://openalex.org/W2296748324",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W143425341",
    "https://openalex.org/W1582482241",
    "https://openalex.org/W2145027719",
    "https://openalex.org/W2125234026",
    "https://openalex.org/W1986748706",
    "https://openalex.org/W1544588226",
    "https://openalex.org/W2105891181",
    "https://openalex.org/W2117444496"
  ],
  "abstract": "Large language models have been proven quite beneficial for a variety of automatic speech recognition tasks in Google. We summarize results on Voice Search and a few YouTube speech transcription tasks to highlight the impact that one can expect from increasing both the amount of training data, and the size of the language model estimated from such data. Depending on the task, availability and amount of training data used, language model size and amount of work and care put into integrating them in the lattice rescoring step we observe reductions in word error rate between 6% and 10% relative, for systems on a wide range of operating points between 17% and 52% word error rate.",
  "full_text": "Large Scale Language Modeling in Automatic\nSpeech Recognition\nCiprian Chelba, Dan Bikel, Maria Shugrina, Patrick Nguyen, Shankar Kumar\nAbstract\nLarge language models have been proven quite beneﬁcial for a variety of automatic speech recognition tasks\nin Google. We summarize results on V oice Search and a few YouTube speech transcription tasks to highlight the\nimpact that one can expect from increasing both the amount of training data, and the size of the language model\nestimated from such data. Depending on the task, availability and amount of training data used, language model\nsize and amount of work and care put into integrating them in the lattice rescoring step we observe reductions in\nword error rate between 6% and 10% relative, for systems on a wide range of operating points between 17% and\n52% word error rate.\nI. I NTRODUCTION\nA statistical language model estimates the prior probability values P(W) for strings of words W in a vocabulary\nVwhose size is usually in the tens or hundreds of thousands. Typically the string W is broken into sentences, or\nother segments such as utterances in automatic speech recognition (ASR), which are assumed to be conditionally\nindependent. For the rest of this chapter, we will assume that W is such a segment, or sentence. With W =\nw1, w2, . . . , wn we get:\nP(W) =\nn∏\ni=1\nP(wi|w1, w2, . . . , wi−1) (1)\nSince the parameter space of P(wk|w1, w2, . . . , wk−1) is too large, the language model is forced to put the\ncontext Wk−1 = w1, w2, . . . , wk−1 into an equivalence class determined by a function Φ(Wk−1). As a result,\nP(W) ∼=\nn∏\nk=1\nP(wk|Φ(Wk−1)) (2)\nResearch in language modeling consists of ﬁnding appropriate equivalence classiﬁers Φ and methods to estimate\nP(wk|Φ(Wk−1)).\nThe most successful paradigm in language modeling uses the (n −1)-gram equivalence classiﬁcation, that is,\ndeﬁnes\nΦ(Wk−1) .= wk−n+1, wk−n+2, . . . , wk−1\nOnce the form Φ(Wk−1) is speciﬁed, only the problem of estimating P(wk|Φ(Wk−1)) from training data remains.\nIn most practical cases, n = 3 which leads to a trigram language model.\nAll authors are with Google, Inc., 1600 Amphiteatre Pkwy, Mountain View, CA 94043, USA.\narXiv:1210.8440v1  [cs.CL]  31 Oct 2012\n1\nA commonly used quality measure for a given model M is related to the entropy of the underlying source and\nwas introduced under the name of perplexity (PPL) [1]:\nPPL (M) = exp(−1\nN\nN∑\nk=1\nln [PM (wk|Wk−1)]) (3)\nA more relevant metric for ASR is the word error rate (WER) achieved when using a give language model in a\nspeech recognition system.\nThe distributed language model architecture described in [2] can be used for training and serving very large\nlanguage models. We have implemented lattice rescoring in this setup, and experimented with such large distributed\nlanguage models on various Google internal tasks.\nII. V OICE SEARCH EXPERIMENTS\nWe have trained query LMs in the following setup [3]:\n• vocabulary size: 1M words, OOV rate 0.57%\n• training data: 230B words, a random sample of anonymized queries from google.com that did not trigger\nspelling correction.\nThe test set was gathered using an Adroid application. People were prompted to speak a set of random google.com\nqueries selected from a time period that does not overlap with the training data.\nThe work described in [4] and [5] enables us to evaluate relatively large query language models in the 1-st pass\nof our ASR decoder by representing the language model in the OpenFst [6] framework. Figures 1-2 show the PPL\nand word error rate (WER) for two language models (3-gram and 5-gram, respectively) built on the 230B training\ndata, after entropy pruning to various sizes in the range 15 million - 1.5 billion n-grams.\nAs can be seen, perplexity is very well correlated with WER, and the size of the language model has a signiﬁcant\nimpact on speech recognition accuracy: increasing the model size by two orders of magnitude reduces the WER\nby 10% relative.\nWe have also implemented lattice rescoring using the distributed language model architecture described in [2],\nsee the results presented in Table I.\nThis enables us to validate empirically the fact that rescoring lattices generated with a relatively small 1-st pass\nlanguage model (in this case 15 million 3-gram, denoted 15M 3-gram in Table I) yields the same results as 1-st pass\ndecoding with a large language model. A secondary beneﬁt of the lattice rescoring setup is that one can evaluate\nthe ASR performance of much larger language models.\n2\n10\n−3\n10\n−2\n10\n−1\n10\n0\n10\n1120\n140\n160\n180\n200\n220\n240\n260\nLM size: # n−grams(B, log scale)\nPerplexity (left) and Word Error Rate (right) as a function of LM size\n10\n−3\n10\n−2\n10\n−1\n10\n0\n10\n117\n17.5\n18\n18.5\n19\n19.5\n20\n20.5\nFig. 1: 3-gram language model perplexity and word error rate as a function of language model size; lower curve is PPL.\nPass Language Model Size PPL WER (%)\n1st 15M 3-gram — 191 18.7\n1st 1.6B 5-gram LARGE, pruned 112 16.9\n2nd 15M 3-gram — 191 18.8\n2nd 1.6B 5-gram LARGE, pruned 112 16.9\n2nd 12.7B 5-gram LARGE 108 16.8\nTABLE I: Speech recognition language model performance when used in the 1-st pass or in the 2-nd pass—lattice rescoring.\nIII. Y OUTUBE EXPERIMENTS\nYouTube data is extremely challenging for current ASR technology. As far as language modeling is concerned,\nthe variety of topics and speaking styles makes a language model built from a web crawl a very attractive choice.\nA. 2011 YouTube Test Set\nA second batch of experiments were carried out in a different training and test setup, using more recent and also\nmore challenging YouTube speech data.\n3\n10\n−2\n10\n−1\n10\n0\n10\n1100\n120\n140\n160\n180\n200\nLM size: # 5−grams(B)\nPerplexity (left) and WER (right) as a function of 5−gram LM size\n10\n−2\n10\n−1\n10\n0\n10\n116.5\n17\n17.5\n18\n18.5\n19\nFig. 2: 5-gram language model perplexity and word error rate as a function of language model size; lower curve is PPL.\nOn the acoustic modeling side, the training data for the YouTube system consisted of approximately 1400 hours\nof data from YouTube. The system used 9-frame MFCCs that were transformed by LDA and SAT was performed.\nDecision tree clustering was used to obtain 17552 triphone states, and STCs were used in the GMMs to model\nthe features. The acoustic models were further improved with bMMI [7]. During decoding, Constrained Maximum\nLikelihood Linear Regression (CMLLR) and Maximum Likelihood Linear Regression (MLLR) transforms were\napplied.\nThe training data used for language modeling consisted of Broadcast news acoustic transcriptions (approx. 1.6\nmillion words), Broadcast news LM text distributed by LDC (approx. 128 million words), and a web crawl from\nOctober 2008 (approx. 12 billion words). Each data source was used to train a separate interpolated Kneser-Ney\n4-gram language model, of size 3.5 million, 112 million and 5.6 billion n-grams, respectively.\nThe ﬁrst pass language model was obtained by interpolating the three components above, after pruning each of\nthem to 3-gram order and about 10M n-grams. Interpolation weights were estimated such that they maximized the\n4\nprobability of a held-out set consisting of manual transcription of YouTube utterances.\nFor lattice rescoring, the three language models were combined with the 1-st pass acoustic model score and the\ninsertion penalty using MERT [8].\nThe test set consisted of 10 hours of randomly selected YouTube speech data.\nTable II presents the results in various rescoring conﬁgurations:\n• 2nd, MERT uses lattice MERT to compute the optimal weights for mixing the three language model scores,\nalong with acoustic model score and insertion penalty. It achieves 3.2% absolute reduction in WER. Despite\nthe very high error rate of the baseline this amounts to 6% relative reduction in WER.\n• 2nd, unif uses uniform weights across the three language models, quantifying the gain that can be attributed\nto MERT (0.6% absolute).\n• 2nd, no wwwthrows away the www LM from the mix to evaluate its contribution: 1.2% absolute reduction\nin WER.\nPass Language Model Size WER (%)\n1st 14M 3-gram — 54.4\n2nd, MERT 5.6B 4-gram LARGE 51.2\n2nd, unif 5.6B 4-gram LARGE 51.8\n2nd, no www LM 112M 4-gram — 53.0\nTABLE II: YouTube 2011 test set: Lattice rescoring using a large language model trained on web crawl.\nExperiments on a development set collected at the same time with the test set insert the large LM rescoring at\nvarious stages in the rescoring pipeline, using increasingly powerful acoustic models, as reported in [9]. The results\nare reported in Table III.\nPass Acoustic Model Language Model Size WER (%)\nBaseline baseline AM 14M 3-gram — 52.8\n2nd baseline AM 5.6B 4-gram LARGE 49.4\nbetter AM DBN + tuning 14M 3-gram — 49.4\n2nd DBN + tuning 5.6B 4-gram LARGE 45.4\neven better AM MMI DBN + tuning 14M 3-gram — 48.8\n2nd MMI DBN + tuning 5.6B 4-gram LARGE 45.2\nTABLE III: YouTube 2011 dev set: Lattice rescoring using a large language model trained on web crawl. Lattices are generated\nwith increasingly powerful acoustic models.\nWe observe consistent gains between 6% and 9% relative, 3.4-4.0% absolute at various operating points in WER\ndue to more powerful acoustic models. As a side note, the gains from large LM rescoring are comparable to those\nobtained by using deep-belief NN acoustic models (DBN).\n5\nB. 2008 YouTube Test Set\nIn a different batch of YouTube experiments, Thadani et al. [10] train a language model on a web crawl from\n2010, ﬁltered to retain only documents in English.\nThe training data used for language modeling consisted of Broadcast news acoustic transcriptions (approx. 1.6\nmillion words), Broadcast news LM text distributed by LDC (approx. 128 million words), and a web crawl from\n2010 (approx. 59 billion words). Each data source was used to train a separate interpolated Kneser-Ney 4-gram\nlanguage model, of size 3.5 million, 112 million and 19 billion n-grams, respectively.\nThe ﬁrst pass language model was obtained by interpolating the three components above, after pruning each of\nthem to 3-gram order and about 10M n-grams.\nFor lattice rescoring, the three unpruned language models were combined using linear interpolation.\nFor both ﬁrst-pass and rescoring language models, interpolation weights were estimated such that they maximized\nthe probability of a held-out set consisting of manual transcription of YouTube utterances.\nThe test corpus consisted of 77 videos containing news broadcast style material downloaded in 2008 [11]. They\nwere automatically segmented into short utterances based on pauses between speech. The audio was transcribed at\nhigh quality by humans trained in the task.\nTable IV highlights the large LM rescoring results presented in [10].\nPass Language Model Size WER (%)\n1st 14M 3-gram — 34.6\n2nd 19B 4-gram LARGE 31.8\nTABLE IV: YouTube 2008 test set: Lattice rescoring using a large language model trained on web crawl.\nThe large language model used for lattice rescoring decreased the WER by 2.8% absolute, or 8% relative, a\nsigniﬁcant improvement in accuracy. 1\nIV. C ONCLUSIONS\nLarge n-gram language models are a simple yet very effective way of improving the performance of real world\nASR systems. Depending on the task, availability and amount of training data used, language model size and\namount of work and care put into integrating them in the lattice rescoring step we observe improvements in WER\nbetween 6% and 10% relative.\n1Unlike the V oice Search experiments reported in Table I, no interpolation between the ﬁrst and the second pass language model was\nperformed. In our experience that consistently yields small gains in accuracy.\n6\nREFERENCES\n[1] Frederick Jelinek, Information Extraction From Speech And Text, chapter 8, pp. 141–142, MIT Press, 1997.\n[2] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean, “Large language models in machine translation,” in Proceedings of the 2007 Joint\nConference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),\n2007, pp. 858–867.\n[3] C. Chelba, J. Schalkwyk, T. Brants, V . Ha, B. Harb, W. Neveitt, C. Parada, and P. Xu, “Query language modeling for voice search,”\nin Proc. of SLT, 2010.\n[4] B. Harb, C. Chelba, J. Dean, and S. Ghemawat, “Back-off language model compression,” in Proceedings of Interspeech, Brighton,\nUK, 2009, ISCA, pp. 325–355.\n[5] C. Allauzen, J. Schalkwyk, and M. Riley, “A generalized composition algorithm for weighted ﬁnite-state transducers,” in Proc.\nInterspeech, 2009, pp. 1203–1206.\n[6] C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and M. Mohri, “OpenFst: A general and efﬁcient weighted ﬁnite-state transducer library,”\nin Proceedings of the Ninth International Conference on Implementation and Application of Automata, (CIAA 2007). 2007, vol. 4783\nof Lecture Notes in Computer Science, pp. 11–23, Springer, http://www.openfst.org.\n[7] D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhadran, G. Saon, and K. Visweswariah, “Boosted MMI for model and feature space\ndiscriminative training,” in Proceedings of ICASSP, April 2008, pp. 4057 –4060.\n[8] W. Macherey, F. Och, I. Thayer, and J. Uszkoreit, “Lattice-based minimum error rate training for statistical machine translation,” in\nProceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2008, pp. 725–734.\n[9] D. Jaitly, P. Nguyen, A. Senior, and V . Vanhoucke, “Application of pretrained deep neural networks to large vocabulary speech\nrecognition,” in Proceedings of Interspeech, 2012.\n[10] K. Thadani, F. Biadsy, and D. Bikel, “On-the-ﬂy topic adaptation for youtube video transcription,” in Proceedings of Interspeech,\n2012.\n[11] C. Alberti, M. Bacchiani, A. Bezman, C. Chelba, A. Drofa, H. Liao, P. Moreno, T. Power, A. Sahuguet, M. Shugrina, and O. Siohan,\n“An audio indexing system for election video material,” in Proceedings of ICASSP, 2009, pp. 4873–4876.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6666059494018555
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5914656519889832
    },
    {
      "name": "Speech recognition",
      "score": 0.5718123912811279
    },
    {
      "name": "Language model",
      "score": 0.5681465268135071
    },
    {
      "name": "Natural language processing",
      "score": 0.5391038060188293
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3872765898704529
    },
    {
      "name": "Geography",
      "score": 0.08043467998504639
    },
    {
      "name": "Cartography",
      "score": 0.07938992977142334
    }
  ],
  "institutions": []
}