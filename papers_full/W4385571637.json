{
  "title": "A Static Evaluation of Code Completion by Large Language Models",
  "url": "https://openalex.org/W4385571637",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2948355683",
      "name": "Hantian Ding",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2007047437",
      "name": "Varun Kumar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2249794155",
      "name": "Yuchen Tian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096678193",
      "name": "Zijian Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5111019876",
      "name": "Rob Kwiatkowski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099355801",
      "name": "Xiaopeng Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2310326697",
      "name": "Murali Krishna Ramanathan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166284654",
      "name": "Baishakhi Ray",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2695426408",
      "name": "Parminder Bhatia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104281610",
      "name": "Sudipta Sengupta",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4221166942",
    "https://openalex.org/W3100026183",
    "https://openalex.org/W2040739365",
    "https://openalex.org/W1989657183",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W3180094739",
    "https://openalex.org/W4320734705",
    "https://openalex.org/W4224060952",
    "https://openalex.org/W1486481742",
    "https://openalex.org/W2962936887",
    "https://openalex.org/W2972082064",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W1491178396",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W4309804142",
    "https://openalex.org/W2158297335",
    "https://openalex.org/W4287024925",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W2107024044",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W4307479317"
  ],
  "abstract": "Hantian Ding, Varun Kumar, Yuchen Tian, Zijian Wang, Rob Kwiatkowski, Xiaopeng Li, Murali Krishna Ramanathan, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 5: Industry Track, pages 347–360\nJuly 10-12, 2023 ©2023 Association for Computational Linguistics\nA Static Evaluation of Code Completion by Large Language Models\nHantian Ding, Varun Kumar, Yuchen Tian, Zijian Wang, Rob Kwiatkowski,\nXiaopeng Li, Murali Krishna Ramanathan, Baishakhi Ray,\nParminder Bhatia, Sudipta Sengupta, Dan Roth, Bing Xiang\nAWS AI Labs\n{dhantian, kuvrun, tiayuche, zijwan, robkwiat, xiaopel\nmkraman, rabaisha, parmib, sudipta, drot, bxiang}@amazon.com\nAbstract\nLarge language models trained on code have\nshown great potential to increase productiv-\nity of software developers. Several execution-\nbased benchmarks have been proposed to eval-\nuate functional correctness of model-generated\ncode on simple programming problems. Never-\ntheless, it is expensive to perform the same eval-\nuation on complex real-world projects consid-\nering the execution cost. On the contrary, static\nanalysis tools such as linters, which can detect\nerrors without running the program, haven’t\nbeen well explored for evaluating code genera-\ntion models. In this work, we propose a static\nevaluation framework to quantify static errors\nin Python code completions, by leveraging Ab-\nstract Syntax Trees. Compared with execution-\nbased evaluation, our method is not only more\nefficient, but also applicable to code in the\nwild. For experiments, we collect code context\nfrom open source repos to generate one mil-\nlion function bodies using public models. Our\nstatic analysis reveals that Undefined Name\nand Unused Variable are the most common er-\nrors among others made by language models.\nThrough extensive studies, we also show the im-\npact of sampling temperature, model size, and\ncontext on static errors in code completions.\n1 Introduction\nAutomatic code completion by large language mod-\nels trained on numerous code repositories has\ndemonstrated great potential in accelerating soft-\nware development. Code assistant services pow-\nered by these models provide developers with code\nsuggestions following the current context in real-\ntime. However, it has been shown that about 70%\nof the suggestions are discarded by users in a recent\nstudy (Ziegler et al., 2022). Even worse, mislead-\ning recommendations can lead to failure in complet-\ning programming tasks (Vaithilingam et al., 2022).\nTherefore, it is important to understand the weak-\nness of current code generation models through\ncomprehensive evaluation and analysis.\nFigure 1: A function completion example, with an Un-\nused Variable error (gray) in context, and an Undefined\nName error (red) in completion.\nRecently, execution-based evaluation has be-\ncome increasingly popular, where model-generated\ncode is executed with unit tests to check functional\ncorrectness. Several benchmarks have been pro-\nposed along this direction, such as HumanEval\n(Chen et al., 2021), MBPP (Austin et al., 2021),\nMBXP (Athiwaratkun et al., 2022), CodeContests\n(Li et al., 2022), and DS-1000 (Lai et al., 2022).\nAlthough these benchmarks are highly reliable and\naccurate, they only focus on well-defined algorith-\nmic and data science problems, which do not reflect\nthe need in general software development. Running\nexecution-based evaluation with real-world code-\nbases is, however, prohibitively expensive because\neach project requires a different setup and the com-\nputation cost is potentially unbounded.\nIn contrast to the execution-based approach,\nstatic program analysis(or static analysis) can an-\nalyze programs without executing them. Although\nstatic analysis is usually unable to determine func-\ntional correctness, it covers a large collection of\n347\nstatic error types, such as undefined names or un-\nused variables that are illustrated in Figure 1. More\nimportantly, the analysis can be very fast and does\nnot require any project specific environment setup,\nwhich allows us to evaluate model completions for\ncomplex real-world code at large scale. Static anal-\nysis tools such as linters have been widely used, for\nexample in code editors, to examine human-written\ncode, but their value in evaluating code generation\nmodels has not been well explored yet.\nIn this work, we propose a static evaluation\nframework for Python language. Code snippets\nare first parsed into Abstract Syntax Trees (ASTs)\nand then analyzed by Pyflakes 1, a popular static\nanalysis tool for Python. To simulate real-world\nuse cases of auto completion, we collect code from\npublic Github repositories to build a function com-\npletion dataset of 100K problems. In each problem,\nwe randomly mask out a function body in a Python\nfile and ask the model to complete it given the pre-\nceding context up until the function header. We\nthen evaluate public models by sampling 10 com-\npletions for each problem, resulting in one million\ngenerations for each model and sampling tempera-\nture, which will be examined by our static evalua-\ntion pipeline.\nDuring AST parsing, we find most of the errors\narise from incomplete generations that hit the max\nlength limit. Otherwise, models of all sizes perform\nquite well in producing parsable codes. Moving\nforward, Pyflakes analysis reveals that Undefined\nName and Unused Variable are the most prominent\nstatic errors in model-generated code. We also ob-\nserve higher temperatures consistently lead to more\nerrors. Scaling up the model, while able to reduce\nerrors of many types, do not show a clear benefit\nfor preventing undefined names. Through a more\nfine-grained classification, we find larger models\ngenerate fewer undefined variables but more un-\ndefined methods, which add up to a mixed result.\nFinally, we demonstrate that errors in context can\nlead to errors of the same type in generation, which\nis likely a consequence of large language models’\nin context learning capability.\nIn summary, our main contributions include the\nfollowing. (1) We propose a static evaluation frame-\nwork for code completion. (2) Our evaluation on\npublic models reveals common static errors and\nhow they are impacted by various factors such as\ntemperature, model size, and context.\n1https://github.com/PyCQA/pyflakes\n2 Background\nCode Generation with Transformers Over re-\ncent years, it has become increasingly popular\nto train Transformer-based language models on\nsource code (Feng et al., 2020; Ahmad et al., 2021;\nWang et al., 2021; Lu et al., 2021; Guo et al.,\n2022) to support software engineering tasks (Iyer\net al., 2018; Tufano et al., 2019). In particular, sev-\neral decoder-only transformer models have been\ndeveloped to facilitate code generation, such as\nCodex (Chen et al., 2021), CodeGen (Nijkamp\net al., 2022), Incoder (Fried et al., 2022), and Al-\nphaCode (Li et al., 2022). These pretrained causal\nlanguage models can be used to predict the contin-\nuation of input code without any finetuning.\nAbstract Syntax Tree An Abstract Syntax Tree\n(a.k.a., AST) is used to represent a source code\nin a concise tree form. By discarding unnecessary\ndetails of the underlying code and its corresponding\nparsed tree, AST only presents the main structural\ncontent of the source code following the language\ngrammar (Aho et al., 2007).\nStatic Analysis Static analysis is a common way\nto detect software bugs without executing the pro-\ngram (Ayewah et al., 2008; Chess and McGraw,\n2004; Chess and West, 2007; Zheng et al., 2006).\nStatic analyzers tend to detect bugs by analyzing\nthe static code text, its AST, documentation, etc.\nThe users usually need to specify the error patterns\nand static analyzers use different AST, graph, and\npath analysis to find those patterns in the code.\nThere are a plethora of static analysis tools and\nthey can detect a wide range of errors depending\non the specified patterns (Emanuelsson and Nils-\nson, 2008). For example, Linter is a popular tool\nthat checks for coding style errors and thus, tries to\nenforce a coding standard (Van Oort et al., 2021).\n3 The Function Completion Dataset\nWe introduce the function completiontask, which\nis one of the most important use cases of auto com-\npletion services. Given an input code snippet that\nends with a function signature plus an optional doc-\nstring, the model is asked to generate the function\nbody. Previous works on code completion (Lu et al.,\n2021; Svyatkovskiy et al., 2019) have mainly fo-\ncused on single-line completion. However, a single\nline is often too short to reveal models’ capability\nin writing syntactically correct code. We believe\nfunction, as the fundamental building block in most\nprogramming languages, better serves this purpose.\n348\nFigure 2: Evaluation pipeline. Left: We parse [context] and [context + generation] into ASTs. If [context] is not\nparsable, we stop without reporting any error on generation. If [context] is parsable, but [context + generation] is\nnot, we report the AST error in generation. Right: If both are parsable, we run Pyflakes on the trees, which reports\nerrors in [context] and errors in [context + generation]. Taking the difference gives us errors in generation.\nSoftware developers use code generation models\nas black-box services on a diverse set of coding\nprojects. To better simulate the real-world scenario,\nwe build an evaluation set by sampling from pub-\nlic Github repositories. Specifically we collected\npermissively licensed Python code in repositories\nthat were created between April, 2022 and August,\n2022. The selection criterion precludes any chrono-\nlogical overlap between our evaluation data and the\ntraining data of models to be tested in this work.2\nThe collected Python codes are reformatted as\nfunction completion problems. We first use tree-\nsitter3 to parse the whole file to identify all the func-\ntions. Then a function that contains a docstring is\nrandomly selected. The code from the beginning\nof the file up until the end of the docstring is used\nas the context, and the function body is considered\nas the groundtruth. The rest of the file is discarded.\nAt test time, we prompt the model with the con-\ntext part as input, and let the model generate the\nfunction body. We choose only functions with doc-\nstrings so that context is well-defined and model\ncan generate meaningful code completions. We\nfurther select test samples whose context length is\nbetween 64 and 768 tokens, and groundtruth length\nis shorter than 256 tokens, to match our model gen-\neration setting. Our final evaluation set consists of\n100K function completion problems.\n4 Static Error Analysis\nWe propose an evaluation pipeline to detect errors\nin function completions generated by models, illus-\ntrated in Figure 2. Suppose the model generates a\ncompletion x given the input context c. We cannot\n2CodeGen models were trained on data up until Oct, 2021.\n3https://tree-sitter.github.io/tree-sitter/\ndirectly analyze x which is partial code without\ncontext. Meanwhile, c may also contain errors es-\npecially in real-world cases. Therefore, we perform\nour analysis in two passes. We first check c for any\nerrors in the input that need to be excluded, and\nthen do another pass on the full code (c, x), the\nconcatenation of the context and model completion.\nAny error that is identified in (c, x) but not in c\nmust arise from x, or in other words, be generated\nby the model. More specifically, we conduct the\nfollowing two steps of analysis for Python code.\n4.1 AST parsing\nIn the first step, we parse both c and (c, x) into\nabstract syntax trees using Python’s nativeast mod-\nule. If the code is parsable, an AST will be returned.\nOtherwise, a syntax error is captured. Based on the\nparsing outcomes, we take the following actions:\n1. If c is not parsable, we are unable to conclude\nany error in generation. Empirically this rarely\nhappens, as we will show in the next section.\n2. If c is parsable but (c, x) is not, then we can\nconfirm the reported syntax error is caused by\nmodel generation. However, notice that only\none error will be returned even if there are mul-\ntiple, due to the nature of AST parsing.\n3. If both c and (c, x) are parsable, there’s no AST\nerror in model generation. The ASTs will be\nused for static analysis in the next step.\n4.2 Static analysis with Pyflakes\nIf both c and (c, x) can be parsed into ASTs, we\nperform static analysis using Pyflakes. Pyflakes is a\nstatic analysis tool that checks a Python source file\nfor errors by examining the AST. One advantage\n349\nis that the analysis does not rely on dependencies\nof the source file, which is important given the\ndiversity of packages used in real-world code. We\nrun Pyflakes on c and (c, x) to identify errors in\ncontext and in full code. Errors that are detected in\n(c, x) but not in c are considered as introduced by\nmodel completion.\n5 Experiments\nWith the proposed pipeline we conduct error analy-\nsis for CodeGen models (Nijkamp et al., 2022) on\nthe test set described in Section 3, and present the\nanalysis results.\n5.1 Experiment Setup\nWe evaluate CodeGen-mono models of all sizes,\nranging from 350M to 16B. We generate function\ncompletions using nucleus sampling with top-p\n0.95. Sampling temperature is varied between 0.2\nand 0.8 for the 2B model, and fixed to 0.4 for the\nrest models. We sample 10 generations for each\nproblem, which results in one million code comple-\ntions for each model and temperature. The maxi-\nmum generation length is 256 tokens. Generated\ncode completions are then passed to our static eval-\nuation pipeline built with Python 3.8 and Pyflakes\n3.0.1. Evaluating one million generations takes\nonly a few hours on a single CPU thread, and can\nbe fully parallelized for acceleration.\n5.2 Validation of Model Output\nWhile we mainly focus on static errors in this study,\nit is also important to validate that the models do\ngenerate relevant code. A counter-example would\nbe to generate a single line of \"return\" for every\nfunction signature, which is syntactically correct\nbut not meaningful at all. Towards this end, we\ncalculate the edit similarity between model genera-\ntion and groundtruth, and compare against Pass@1\nfrom HumanEval (Chen et al., 2021) which is a pop-\nular execution-based benchmark to evaluate code\ngeneration models. Specifically, for both datasets\nwe generate 10 samples per problem, and report the\naveraged edit similarity or pass rate over all gen-\nerations. As shown in Table 1, models of all sizes\nand temperatures are able to achieve reasonable\nedit similarity on the function completion dataset,\nwhich means the generations are semantically rel-\nevant. Moreover, edit similarity and HumanEval\nPass@1 both improve as the model scales up, high-\nlighting that model scale is crucial for accurate\nModel Temp Edit\nSimilarity\nHumanEval\nPass@1\nCodeGen-16B\n0.4\n72.07 31.83\nCodeGen-6B 68.76 26.46\nCodeGen-2B 64.83 23.72\nCodeGen-350M 56.47 12.62\nCodeGen-2B\n0.2 65.10 25.06\n0.4 64.83 23.72\n0.6 64.09 21.28\n0.8 62.62 17.56\nTable 1: Edit similarity on function completion dataset\nand Pass@1 on HumanEval, of CodeGen models across\ndifferent sizes and temperatures. (1) Edit similarity\nand HumanEval Pass@1 are positively correlated across\ndifferent settings, which justifies edit similarity can be\nused as an alternative metric for model evaluation. (2)\nAs expected, larger models have better edit similarity (a\nproxy to accuracy) on function completion task.\ncode generation. Finally, the strong positive corre-\nlation between the last two columns shows that edit\nsimilarity on the function completion dataset can be\nused as an alternative metric for model comparison.\n5.3 AST Results\nWe run AST parsing and find there are only 0.42%\ncases with unparsable context that need to be dis-\ncarded. For the rest, we report percentage of gen-\nerations with AST errors in Table 2. A full list of\nerror types is included in Appendix A. For each\ntype, we also show a code example in Appendix B.\nWhile there are about 7-8% of unparsable gener-\nations, most of the parsing errors happen at the end\nof file (EOF), which means the generated code is\nincomplete due to the 256 max token limit. Extend-\ning generation length may help reduce EOF errors,\nbut will require more computation and increase the\nperceived latency of the auto-completion service.\nOn the other hand, non-EOF errors only account\nfor a tiny fraction, usually around 0.1-0.2%, which\nindicates CodeGen models can generally follow\nthe abstract syntax grammar to produce parsable\ncodes, regardless of model size and temperature.\nFinding 1. Codes generated by models, unless in-\ncomplete, are mostly parsable into ASTs, regardless\nof model size or temperature.\nWe also show the top-3 non-EOF error types\nranked by frequency, which are Invalid syntax,\nPrint Missing Parentheses, and Keyword Argu-\nment Repeated. Notably, the first two categories\nare often related to Python’s interpreter version.\nTo illustrate, Python2-style print like print \"abc\"\n350\nModel Temp Total EOF Non EOF Invalid\nSyntax\n\"print\"\nMissing\nParentheses\nKeyword\nArgument\nRepeated\nCodeGen-16B\n0.4\n7.330% 7.236% 0.094% 0.042% 0.041% 0.004%\nCodeGen-6B 7.446% 7.253% 0.193% 0.081% 0.094% 0.006%\nCodeGen-2B 7.272% 7.177% 0.095% 0.052% 0.018% 0.008%\nCodeGen-350M 8.703% 8.593% 0.110% 0.041% 0.016% 0.028%\nCodeGen-2B\n0.2 8.067% 7.982% 0.085% 0.045% 0.018% 0.008%\n0.4 7.272% 7.177% 0.095% 0.052% 0.018% 0.008%\n0.6 6.823% 6.713% 0.110% 0.060% 0.020% 0.008%\n0.8 7.496% 7.337% 0.159% 0.085% 0.029% 0.014%\nTable 2: Percentages of AST errors across different model sizes and temperatures. We show (1) total AST errors;\n(2) errors at the end of file (EOF); (3) errors not at EOF; (4) top 3 non-EOF errors. Models generally perform well\nat AST level except for EOF errors caused by max generation length limit.\nFigure 3: Number of undefined variables versus unde-\nfined functions. Larger models generate more undefined\nfunctions but fewer undefined variables.\nwill lead to Print Missing Parentheses in Python3.\nAnother example is that using async as a variable\nname will cause Invalid Syntax because async has\nbecome a reserved word since Python3.7. Models\nlearn to make such errors from their training data\nwhich consists of code written for different Python\nversions. In many cases, it is difficult for a model to\ninfer the intended interpreter version directly from\nthe limited context. An interesting future direction\nis to guide models to generate version-compatible\ncode given the target environment.\nFinding 2. Interpreter version mismatch is one of\nthe major reasons for non-EOF AST errors.\n5.4 Pyflakes Results\nWe present frequencies of top 6 linter errors from\nPyflakes in Table 3, with code examples in Ap-\npendix B. While Pyflakes also finds other problems\nin code, most of them are very sparse and thus less\nimportant, which we leave to Appendix A. Notice\nthat one code snippet may contain multiple errors.\nWe count each type only once in every test sample.\nAmong all errors, Undefined Name and Un-\nused Variable are the most common ones, where\nthe model either calls a variable that is not defined,\nor defines a variable but never uses it. Closely\nrelated are Unused Import, Redefined While Un-\nused and Undefined Local, which can be consid-\nered as special cases of the first two. Models also\nsometimes unnecessarily use f-strings by not giv-\ning any placeholder. It is worth pointing out that\nnot all Pyflakes errors will impact execution. In\nfact among the six types, only Undefined Name\nand Undefined Local may cause runtime problems.\nHowever, all these errors can harm readability and\nmaintenance which are critical for software devel-\nopment. Hence, it is important to address them to\nimprove the quality of auto code completion.\nAcross sampling temperatures, we observe in\nevery column that more errors are generated under\nhigher temperatures, which is expected because\ngenerations in such cases are less confident.\nFinding 3. Higher temperature always leads to\nmore errors of every type.\nThe impact of model size on error rate is less\nconsistent though. For Unused Variable, Unused\nImport, and Undefined Local, error rate does de-\ncrease as the model scales up. However, the other\nthree categories do not manifest such correlation.\nWe investigate the underlying reason for this mixed\nresult particularly in the case of Undefined Name.\nNotice that if an undefined name is a function call,\nit can potentially be defined afterwards outside the\ncurrent function completion scope. While not guar-\nanteed, the model might be able to fix this error by\nitself if we allow generating longer code instead\nof only one function. In contrast, using a vari-\nable without first defining it is usually a mistake.\nEven in some rare cases where the variable defi-\nnition is made up correctly after the usage, such\nordering is often less preferred in terms of coding\n351\nModel Temp Undefined\nName\nUnused\nVariable\nFString\nMissing\nPlaceholders\nUnused\nImport\nRedefined\nWhile\nUnused\nUndefined\nLocal\nCodeGen-16B\n0.4\n4.323% 1.729% 0.135% 0.107% 0.131% 0.047%\nCodeGen-6B 4.374% 1.775% 0.089% 0.149% 0.126% 0.055%\nCodeGen-2B 4.364% 1.810% 0.147% 0.150% 0.146% 0.065%\nCodeGen-350M 4.472% 2.032% 0.151% 0.173% 0.155% 0.095%\nCodeGen-2B\n0.2 4.206% 1.751% 0.125% 0.139% 0.139% 0.067%\n0.4 4.364% 1.810% 0.147% 0.150% 0.146% 0.065%\n0.6 4.711% 2.000% 0.188% 0.170% 0.159% 0.076%\n0.8 5.377% 2.490% 0.240% 0.247% 0.184% 0.086%\nTable 3: Percentages of Pyflakes errors across different model sizes and temperatures. Higher temperatures always\nlead to more errors in every category. On the other hand, larger models do not necessarily generate fewer errors.\nstyle. In Figure 3, we break down the undefined\nnames into variables and functions. We find that\nlarger models yield fewer undefined variables, but\nmore undefined functions, which demonstrates that\nthe correlation between error count and model size\nvaries for different errors types.\nFinding 4. While larger models are more accurate\ncode generators (Nijkamp et al., 2022), scaling\nup model size does not lead to reduction in error\ncounts for all error categories.\n5.5 Correlation with Errors in Context\nWe further study the correlation between errors in\ncontext and in generation. Denote by c the input\ncontext, x the model generation, e the error type.\nWe writee ∈c to mean c contains an error of typee.\nFor every e,4 we calculate P(e ∈x|e ∈c), the gen-\neration error rate when context contains the same\ntype of error(s). We also report the relative ratio\nP(e∈x|e∈c)\nP(e∈x|e/∈c) to measure the impact of context. From\nTable 4, if the model observes errors in context, it\nis more likely to produce the same type of errors in\ngeneration, and the error rate can be amplified by\n7∼200 times depending on the type. This is pos-\nsibly an undesired consequence of the in-context\nlearning capability of large language models.\nWe also calculate P(e ∈c|e ∈x) to show how\nmany of the generation errors co-occur with con-\ntext errors. As indicated by the last column of Ta-\nble 4, even though context errors can significantly\namplify generations errors, the co-occurrences of\ntwo do not account for a large fraction. This im-\nplies problematic context is not the only factor for\nproblematic generation, and it is often the case for\nmodels to produce errors even with correct context.\n4We omit Unused Import from Table 3 because it is valid to\nhave unused imports in the context that is yet to be completed.\nError type P (e∈x|e∈c) P(e∈x|e∈c)\nP(e∈x|e/∈c) P(e∈c|e∈x)\nUndefined Name 26.33% 7.80 25.99%\nUnused Variable 14.13% 8.45 8.56%\nFString Missing\nPlaceholders 20.63% 215.50 35.08%\nRedefined\nWhile Unused 2.44% 21.16 22.30%\nUndefined Local 7.00% 108.68 1.08%\nTable 4: Correlation between errors in context and in\ngeneration for the 2B model. First two columns indicate\nerrors in context can amplify errors in generation; the\nlast column shows not all generations errors can be\nattributed to context. Other models have similar results.\nFinding 5. Errors in context generally lead to more\nerrors in generation.\n6 Discussion\nWe present a static evaluation framework for code\ncompletions generated by large language models.\nBy utilizing the proposed framework, we conduct\nerror analysis of CodeGen models on a large scale\nreal-world Python evaluation set. Our experiment\nreveals common static errors made by pretrained\nmodels, as well as their frequency trend across\nmodel sizes and sampling temperatures. By point-\ning out weaknesses of existing models, we hope our\nstudy also sheds light on future directions towards\nmore accurate code generation.\nThere are a few limitations of this study. First,\nwe focus on left-to-right code generation without\nconsidering right-side and cross-file context, which\ncan be used to determine broader categories of\nerrors with improved precision. Second, each static\nanalysis tool has its own limitations. Thus, the\npresented analysis is limited by Pyflakes’s accuracy\nand coverage to detect certain code issues.\n352\nReferences\nWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and\nKai-Wei Chang. 2021. Unified pre-training for pro-\ngram understanding and generation. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2655–2668,\nOnline. Association for Computational Linguistics.\nAlfred V Aho, Ravi Sethi, and Jeffrey D Ullman. 2007.\nCompilers: principles, techniques, and tools, vol-\nume 2. Addison-wesley Reading.\nBen Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang,\nXiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin\nAhmad, Shiqi Wang, Qing Sun, Mingyue Shang, Su-\njan Kumar Gonugondla, Hantian Ding, Varun Ku-\nmar, Nathan Fulton, Arash Farahani, Siddhartha Jain,\nRobert Giaquinto, Haifeng Qian, Murali Krishna Ra-\nmanathan, Ramesh Nallapati, Baishakhi Ray, Parmin-\nder Bhatia, Sudipta Sengupta, Dan Roth, and Bing\nXiang. 2022. Multi-lingual evaluation of code gener-\nation models. CoRR, abs/2210.14868.\nJacob Austin, Augustus Odena, Maxwell I. Nye,\nMaarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie J. Cai, Michael Terry, Quoc V . Le,\nand Charles Sutton. 2021. Program synthesis with\nlarge language models. CoRR, abs/2108.07732.\nNathaniel Ayewah, William Pugh, David Hovemeyer,\nJ David Morgenthaler, and John Penix. 2008. Using\nstatic analysis to find bugs. IEEE software, 25(5):22–\n29.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nBrian Chess and Gary McGraw. 2004. Static analysis\nfor security. IEEE security & privacy, 2(6):76–79.\nBrian Chess and Jacob West. 2007. Secure program-\nming with static analysis. Pearson Education.\nPär Emanuelsson and Ulf Nilsson. 2008. A comparative\nstudy of industrial static analysis tools. Electronic\nnotes in theoretical computer science, 217:5–21.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nBERT: A pre-trained model for programming and\nnatural languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1536–1547, Online. Association for Computational\nLinguistics.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\nCoRR, abs/2204.05999.\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming\nZhou, and Jian Yin. 2022. UniXcoder: Unified cross-\nmodal pre-training for code representation. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 7212–7225, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2018. Mapping language to code\nin programmatic context. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1643–1652, Brussels, Bel-\ngium. Association for Computational Linguistics.\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang,\nRuiqi Zhong, Luke Zettlemoyer, Scott Wen-tau Yih,\nDaniel Fried, Sida I. Wang, and Tao Yu. 2022. DS-\n1000: A natural and reliable benchmark for data sci-\nence code generation. CoRR, abs/2211.11501.\nYujia Li, David H. Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022. Competition-level code generation with alpha-\ncode. CoRR, abs/2203.07814.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey\nSvyatkovskiy, Ambrosio Blanco, Colin B. Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-\ndong Zhou, Linjun Shou, Long Zhou, Michele Tu-\nfano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-\ndaresan, Shao Kun Deng, Shengyu Fu, and Shujie\nLiu. 2021. Codexglue: A machine learning bench-\nmark dataset for code understanding and generation.\nIn Proceedings of the Neural Information Process-\ning Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint.\n353\nAlexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel\nSundaresan. 2019. Pythia: Ai-assisted code com-\npletion system. In Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge\nDiscovery & Data Mining, KDD 2019, Anchorage,\nAK, USA, August 4-8, 2019, pages 2727–2735. ACM.\nMichele Tufano, Cody Watson, Gabriele Bavota, Massi-\nmiliano Di Penta, Martin White, and Denys Poshy-\nvanyk. 2019. An empirical study on learning bug-\nfixing patches in the wild via neural machine transla-\ntion. ACM Trans. Softw. Eng. Methodol., 28(4).\nPriyan Vaithilingam, Tianyi Zhang, and Elena L. Glass-\nman. 2022. Expectation vs. experience: Evaluating\nthe usability of code generation tools powered by\nlarge language models. In Extended Abstracts of the\n2022 CHI Conference on Human Factors in Com-\nputing Systems, CHI EA ’22, New York, NY , USA.\nAssociation for Computing Machinery.\nBart Van Oort, Luís Cruz, Maurício Aniche, and Arie\nVan Deursen. 2021. The prevalence of code smells\nin machine learning projects. In 2021 IEEE/ACM 1st\nWorkshop on AI Engineering-Software Engineering\nfor AI (WAIN), pages 1–8. IEEE.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.\nHoi. 2021. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 8696–8708, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nJiang Zheng, Laurie Williams, Nachiappan Nagappan,\nWill Snipes, John P Hudepohl, and Mladen A V ouk.\n2006. On the value of static analysis for fault de-\ntection in software. IEEE transactions on software\nengineering, 32(4):240–253.\nAlbert Ziegler, Eirini Kalliamvakou, Shawn Simister,\nGanesh Sittampalam, Alice Li, Andrew Rice, Devon\nRifkin, and Edward Aftandilian. 2022. Productivity\nassessment of neural code completion.\n354\nA Full Error Categories\nIn addition to those discussed in Section 5, we list\nall error categories that can be detected in model\ngenerated code in our experiments, with a minimal\nfrequency of 0.001% by any of the models (i.e. 10\nobservations out of the total 1 million generations).\nAST errors (EOF errors indicated by aster-\nisk):\n1. *unexpected EOF while parsing\n2. *EOL while scanning string literal\n3. *invalid syntax at EOF\n4. *EOF while scanning triple-quoted string lit-\neral\n5. invalid syntax not at EOF\n6. missing parentheses in call to \"print\"\n7. keyword argument repeated\n8. leading zeros in decimal integer literals are not\npermitted; use an o prefix for octal integers\n9. unmatched \")\"\n10. cannot assign to function call\n11. positional argument follows keyword argu-\nment\n12. expression cannot contain assignment\nPyflakes issues:\n1. undefined name\n2. unused variable\n3. f-string missing placeholder\n4. unused import\n5. redefined while unused\n6. indentation error\n7. import shadowed by loop var\n8. raise not implemented\n9. invalid print syntax\n10. is literal\n11. string dot format extra positional argument\n12. multi value repeated key literal\n13. percent format positional count mismatch\n14. tab error\n15. string dot format extra named arguments\n16. import star not permitted\n17. percent format unsupported format character\n18. assert tuple\n19. percent format extra named arguments\nB Examples for Top Error Types\nBelow we list one code example for each of the\nerror categories shown in Table 2 and 3. Following\nthe definition of function completion task, in every\nexample, context is from the beginning until the\nend of the docstring of the last function, and model\ncompletion is the body of the last function.\n355\n1 \"\"\" Secondary Structure dataset . \"\"\"\n2\n3 import numpy as np\n4 from megatron import print_rank_0\n5 from . data import ProteinPredictionAbstractDataset\n6 from . data import build_tokens_paddings_from_text\n7\n8 class SecondaryStructureDataset ( ProteinPredictionAbstractDataset ):\n9 def __init__ (self ,\n10 name : str ,\n11 datapaths ,\n12 tokenizer ,\n13 max_seq_length : int ):\n14 super (). __init__ (’ secondary_structure ’, name , datapaths , tokenizer ,\nmax_seq_length )\n15\n16\n17 def build_samples (self , ids , paddings , label , unique_id , seq_len ):\n18 \"\"\" Convert to numpy and return a sample consumed by the batch producer .\n\"\"\"\n19\n20 # Seperate inputs and labels in lists\n21 input_tokens = [ self . tokenizer . tokenize ( seq ) for seq in ids ]\n22 input_tokens = [ token for seq in input_tokens for token in seq ]\n23 labels = [ self . tokenizer . tokenize ( seq ) for seq in label ]\n24 labels = [ label for seq in labels for label in seq ]\n25\n26 # Add special tokens\n27 input_tokens , labels = self . add_special_tokens ( input_tokens , labels )\n28\n29 # Truncate and pad\n30 input_tokens , labels , paddings = self . truncate_and_pad ( input_tokens ,\nlabels ,\n31 self\nListing 1: unexpected EOF while parsing (line 31)\n356\n1 from pathlib import Path\n2 from wasabi import msg\n3 from . remote_storage import RemoteStorage\n4 from . remote_storage import get_content_hash , get_command_hash\n5 from .. _util import load_project_config\n6 from .. _util import project_cli , Arg , logger\n7\n8\n9 @project_cli . command (\" push \")\n10 def project_push_cli (\n11 # fmt : off\n12 remote : str = Arg (\" default \", help =\" Name or path of remote storage \"),\n13 project_dir : Path = Arg ( Path . cwd () , help =\" Location of project directory .\nDefaults to current working directory .\", exists =True , file_okay = False ),\n14 # fmt : on\n15 ):\n16 \"\"\" Persist outputs to a remote storage . You can alias remotes in your\n17 project . yml by mapping them to storage paths . A storage can be anything that\n18 the smart - open library can upload to , e.g. AWS , Google Cloud Storage , SSH ,\n19 local directories etc .\n20\n21 DOCS : https :// xxx\n22 \"\"\"\n23 for nlp in load_project_config ( project_dir , {\" directories \": [ ANIMAL_TRAIN_DIR\n]}:\n24 remote_storage = RemoteStorage . get ( remote )\n25 for command in [\" train \"]:\n26 logger . debug (f\" Uploading { command } to remote storage ’{ remote_storage }’\")\n27 path = Path ( project_dir ) / ANIMAL_TRAIN_DIR / command\n28 upload_project ( remote_storage , path )\nListing 2: invalid syntax (line 23)\n1 def prod (F, E):\n2 \"\"\" Check that the factorization of P -1 is correct . F is the list of\n3 factors of P -1 , E lists the number of occurrences of each factor . \"\"\"\n4 M = prod_of_prime_factors (F, E)\n5 if not all (i == 1 for i in M):\n6 print \" Error in prod \"\n7 print F, E\n8 return\n9 P = product (F)\n10 P_1 = 1\n11 for i in range ( len (F)):\n12 P_1 *= F[i ]** E[i]\n13 if P != P_1 :\n14 print \" Error in prod \"\n15 print F, E\n16 print P\n17 print P_1\n18 return\nListing 3: Missing parentheses in call to \"print\" (line 6)\n357\n1 import unittest\n2 from datetime import datetime , timezone\n3\n4 from dateutil . relativedelta import relativedelta\n5\n6 from bot . utils import time\n7\n8\n9 class TimeTests ( unittest . TestCase ):\n10 \"\"\" Test helper functions in bot . utils . time . \"\"\"\n11\n12 def test_humanize_delta_handle_unknown_units ( self ):\n13 \"\"\" humanize_delta should be able to handle unknown units , and will not\nabort . \"\"\"\n14 self . assertEqual (\n15 time . humanize_delta ( datetime . utcnow () , datetime . utcnow () -\nrelativedelta ( months =1 , months =2) ),\n16 \"1 month and 2 months \"\n17 )\nListing 4: keyword argument repeated (line 15)\n1 \"\"\"\n2 This program will continually ask our user to give a number\n3 and will calculate the factorial result of the number and print it on the console\n.\n4\n5 The program ends when the user enter the EXIT number .\n6 \"\"\"\n7\n8 EXIT = -100\n9\n10\n11 def main ():\n12 \"\"\"\n13 This program will calculate the factorial result according to the number an\nuser\n14 inputs .\n15 \"\"\"\n16 print (’ <<< Welcome to the Factorial Calculator ! >>>’)\n17 num = int ( input (’Enter a number : ’))\n18 print (’The factorial of {} is {}. ’. format (num , factorial ( num )))\n19 if num == EXIT :\n20 print (’\\n<<< Thank you for using the Factorial Calculator . >>>’)\n21 else :\n22 main ()\nListing 5: undefined name \"factorial\" (line 18)\n358\n1 def check ( full_path , encoding ):\n2 assert type ( full_path ) == str , f’\\’ full_path \\’ is of { type ( full_path )}. Only\ntype \\’ str \\’ is acceptable .’\n3 assert full_path != \"\", \"\\’ full_path \\’ is empty .\"\n4 assert type ( encoding ) == str , f’\\’ full_path \\’ is of { type ( encoding )}. Only\ntype \\’ str \\’ is acceptable .’\n5 assert encoding != \"\", \"\\’ encoding \\’ is empty .\"\n6\n7 def file_read ( full_path : str , encoding = \" utf8 \"):\n8 ’’’\n9 Author : xxx\n10\n11 Reads file at \" full_path \" and returns its data in a list .\n12 ’’’\n13\n14 check ( full_path , encoding )\n15 encoding_check = encoding\n16 full_path = full_path . strip ()\n17 f = open ( full_path , \"r\", encoding = encoding )\n18 lines = f. readlines ()\n19 f. close ()\n20 lines = [ line . replace (\"\\n\", \"\") for line in lines ]\n21 return lines\nListing 6: local variable \"encoding_check\" is assigned to but never used (line 15)\n1 import os\n2 import json\n3\n4 from convinse . library . utils import store_json_with_mkdir , get_logger\n5\n6\n7 class HeterogeneousAnswering :\n8 def __init__ (self , config ):\n9 \"\"\" Initialize HA module . \"\"\"\n10 self . config = config\n11 self . logger = get_logger ( __name__ , config )\n12\n13 def train (self , sources =[\"kb\", \" text \", \" table \", \" info \"]):\n14 \"\"\" Method used in case no training required for HA phase . \"\"\"\n15 self . logger . info (f\"No need to train .\")\n16 pass\nListing 7: f-string is missing placeholders (line 15)\n359\n1 import os\n2 import urllib . parse\n3 import sqlite3\n4\n5 SQL = \"\"\"\n6 SELECT p. ZAUTHOR , p. ZTITLE , e. ZTITLE , e. ZASSETURL , e. ZPUBDATE\n7 from ZMTEPISODE e\n8 join ZMTPODCAST p\n9 on e. ZPODCASTUUID = p. ZUUID\n10 where ZASSETURL NOTNULL ;\n11 \"\"\"\n12\n13\n14 def check_imports ():\n15 ’’’ Prompts for password to install dependencies , if needed ’’’\n16 import os , importlib , importlib . util\n17 import urllib . parse\n18\n19 # Check for dependency installs\n20 # Can be done more simply , but this way I can avoid importing anything from\nzmodel ,\n21 # which is nice since I can see what ’s going on.\n22 for k, v in DEPS . items ():\n23 try :\n24 importlib . import_module (k)\n25 except ImportError as e:\n26 importlib . util . find_spec (k)\n27 if importlib . util . find_spec (k) is None :\n28 os. system (f’pip install {v}’)\nListing 8: \"urllib.parse\" imported but unused (line 17)\n1 import kfp . deprecated as kfp\n2 from kfp . deprecated import components , dsl , compiler\n3\n4 def get_run_info ( run_id : str ):\n5 \"\"\" Example of getting run info for current pipeline run . \"\"\"\n6 import kfp . dsl as dsl\n7 client = kfp . Client ()\n8 run = client . run_details ( run_id )\n9 print (f\" Run details :\\n{ run }\")\n10 print (f\" Pipeline details :\\n{ run . pipeline_runtime }\")\nListing 9: redefinition of unused \"dsl\" from line 2 (line 6)\n1 \"\"\" Check for nonlocal and used - before - assignment \"\"\"\n2 # pylint : disable = missing - docstring , unused - variable , no -init , too -few - public -\nmethods\n3\n4 __revision__ = 0\n5\n6 def test_ok ():\n7 \"\"\" uses nonlocal \"\"\"\n8 cnt = 1\n9 def wrap ():\n10 nonlocal cnt\n11 cnt = cnt + 1\n12 wrap ()\n13\n14 def test_fail ():\n15 \"\"\" doesn ’t use nonlocal \"\"\"\n16 cnt = 1\n17 def wrap ():\n18 cnt = cnt + 1 # [used - before - assignment ]\n19 wrap ()\nListing 10: local variable \"cnt\" defined in enclosing scope on line 16 referenced before assignment (line 18)\n360",
  "topic": "Code (set theory)",
  "concepts": [
    {
      "name": "Code (set theory)",
      "score": 0.5793548226356506
    },
    {
      "name": "Computer science",
      "score": 0.5417817234992981
    },
    {
      "name": "Tian",
      "score": 0.47213059663772583
    },
    {
      "name": "Track (disk drive)",
      "score": 0.41864562034606934
    },
    {
      "name": "Programming language",
      "score": 0.38788437843322754
    },
    {
      "name": "Operating system",
      "score": 0.16948437690734863
    },
    {
      "name": "History",
      "score": 0.1358737349510193
    },
    {
      "name": "Ancient history",
      "score": 0.07327589392662048
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ]
}