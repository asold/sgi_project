{
  "title": "Motif Transformer: Generating Music With Motifs",
  "url": "https://openalex.org/W4381250165",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100453977",
      "name": "Heng Wang",
      "affiliations": [
        "Wuhan Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A5092206876",
      "name": "Sen Hao",
      "affiliations": [
        "Wuhan Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A5035057047",
      "name": "Cong Zhang",
      "affiliations": [
        "Wuhan Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A5100730344",
      "name": "Xiaohu Wang",
      "affiliations": [
        "Wuhan Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A5114949900",
      "name": "Yilin Chen",
      "affiliations": [
        "Wuhan Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3213549365",
    "https://openalex.org/W3004565922",
    "https://openalex.org/W6730558285",
    "https://openalex.org/W3200488740",
    "https://openalex.org/W4313428597",
    "https://openalex.org/W3110962504",
    "https://openalex.org/W1992855217",
    "https://openalex.org/W2974016341",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6788307591",
    "https://openalex.org/W4312638585",
    "https://openalex.org/W3101652466",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W6781910838",
    "https://openalex.org/W3137883189",
    "https://openalex.org/W4313594236",
    "https://openalex.org/W3200770258",
    "https://openalex.org/W6781678156",
    "https://openalex.org/W6605505536",
    "https://openalex.org/W1486260793",
    "https://openalex.org/W2978309747",
    "https://openalex.org/W2887600497",
    "https://openalex.org/W2996575285",
    "https://openalex.org/W4291551375",
    "https://openalex.org/W2963408210",
    "https://openalex.org/W134527144",
    "https://openalex.org/W3047325651",
    "https://openalex.org/W3049247973",
    "https://openalex.org/W2963575853",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3175663427"
  ],
  "abstract": "Music is composed of a set of regular sound waves, which are usually ordered and have a large number of repetitive structures. Important notes, chords, and music fragments often appear repeatedly. Such repeated fragments (referred to as motifs) are usually the soul of a song. However, most music generated by existing music generation methods can not have distinct motifs like real music. This study proposes a novel multi- encoders model called Motif Transformer to generate music containing more motifs. The model is constructed using an encoder-decoder framework that includes an original encoder, a bidirectional long short term memory-attention encoder (abbreviated as bilstm-attention encoder), and a gated decoder. Where the original encoder is taken from the transformer&#x2019;s encoder and the bilstm-attention encoder is constructed from the bidirectional long short-term memory network (BILSTM) and the attention mechanism; Both the original encoder and the bilstm-attention encoder encode the motifs and input the encoded information representations to the gated decoder; The gated decoder decodes the entire input of the music and the information passed by the encoders and enhances the model&#x2019;s ability to capture motifs of the music in a gated manner to generate music with significantly repeated fragments. In addition, in order to better measure the model&#x2019;s ability of generating motifs, this study proposes an evaluation metric called used motifs. Experiments on multiple music field metrics show that the model proposed in this study can generate smoother and more beautiful music, and the generated music contains more motifs.",
  "full_text": "Received 28 May 2023, accepted 13 June 2023, date of publication 19 June 2023, date of current version 28 June 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3287271\nMotif Transformer: Generating Music With Motifs\nHENG WANG\n 1, SEN HAO\n 1, CONG ZHANG\n 2, XIAOHU WANG1, AND YILIN CHEN3\n1School of Mathematics and Computer, Wuhan Polytechnic University, Wuhan 430048, China\n2School of Electrical and Electronic Engineering, Wuhan Polytechnic University, Wuhan 430048, China\n3Hubei Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, Wuhan 430073, China\nCorresponding author: Sen Hao (13253620681@163.com)\nThis work was supported in part by the National Natural Science Foundation under Grant 61401319 and Grant 61272278; in part by the\nHubei Province Natural Science Foundation under Grant 2014CFB270 and Grant 2015CFA061; in part by the Hubei Provincial\nDepartment of Education Research Foundation under Grant D20201601; in part by the Hubei Provincial Major Science and Technology\nSpecial Projects under Grant 2018ABA099; and in part by the Hubei Key Laboratory of Intelligent Robot, Wuhan Institute of Technology,\nunder Grant HBIR202101.\nABSTRACT Music is composed of a set of regular sound waves, which are usually ordered and have a large\nnumber of repetitive structures. Important notes, chords, and music fragments often appear repeatedly. Such\nrepeated fragments (referred to as motifs) are usually the soul of a song. However, most music generated by\nexisting music generation methods can not have distinct motifs like real music. This study proposes a novel\nmulti- encoders model called Motif Transformer to generate music containing more motifs. The model is\nconstructed using an encoder-decoder framework that includes an original encoder, a bidirectional long short\nterm memory-attention encoder (abbreviated as bilstm-attention encoder), and a gated decoder. Where the\noriginal encoder is taken from the transformer’s encoder and the bilstm-attention encoder is constructed\nfrom the bidirectional long short-term memory network (BILSTM) and the attention mechanism; Both the\noriginal encoder and the bilstm-attention encoder encode the motifs and input the encoded information\nrepresentations to the gated decoder; The gated decoder decodes the entire input of the music and the\ninformation passed by the encoders and enhances the model’s ability to capture motifs of the music in a gated\nmanner to generate music with significantly repeated fragments. In addition, in order to better measure the\nmodel’s ability of generating motifs, this study proposes an evaluation metric called used motifs. Experiments\non multiple music field metrics show that the model proposed in this study can generate smoother and more\nbeautiful music, and the generated music contains more motifs.\nINDEX TERMS Deep learning, music generation, recurrent neural network, transformer.\nI. INTRODUCTION\nMusic is an organized and regular sound wave, which can\ncultivate the mood and relax, convey messages, and express\nemotions. It is a necessary art form in our daily life. However,\ncomposing or analyzing music requires a high degree of\nprofessionalism, and it takes inspiration and various human\nand material resources for music experts to complete a song.\nIn order to better analyze and study music, many scholars\nuse artificial intelligence technology for music-related tasks\nsuch as emotion recognition [1], music classification [2], and\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Angel F. García-Fernández\n.\nmusic generation [3]. Composing music using artificial intel-\nligence can be automated or semi-automated using neural\nnetwork models to generate music, significantly reducing the\ncomplexity of managing music.\nIn recent years, sequence-based music generation methods\nhave made significant progress with the development of deep\nlearning techniques. Sequence-based methods [4], [5] will\nfirst quantize the music content into symbolic sequences and\nthen input the symbolic sequences into a neural network\nmodel. As shown in Fig.1, score fragments of the Christ-\nmas pop song ‘‘Deck the Halls’’ consist of notes as the\nbasic unit and are divided into different parts by bars [6].\nAdditional notes have their pitch, playing time, and duration.\nVOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 63197\nH. Wang et al.: Motif Transformer: Generating Music With Motifs\nFIGURE 1. The score fragments of ‘‘Deck the Halls’’ .\nThe sequence-based approach will quantify the above musi-\ncal information, such as notes, bars, and rhythms, into\nsymbolic sequences for model learning. In recent years,\nmany scholars have made good progress in music genera-\ntion using neural network models such as recurrent neural\nnetworks [7], [8], generative adversarial networks [9], [10],\nand variational autoencoders [11]. Music usually has obvious\nregularity in its overall structure, and a standard piece of\nmusic typically contains many repetitive fragments that run\nthrough the entire structure of the music rather than just being\nreflected in the short term. For example, the musical frag-\nments in the red box in Fig.1 recur several times throughout\nthe song. Such recurring melodic fragments (motifs) are often\nimpressive and key to a song’s tone. However, we found that\nthe above music generation methods can only contain some\nrepetitive fragments in the first few bars of the generated\nmusic rather than the entire music having repetitive segments.\nTherefore, this study proposes a music generation model\ncalled Motif Transformer that can generate more motifs.\nSpecifically, this study designed a transformer-based model\ncontaining an original encoder, a bilstm-attention encoder,\nand a gated decoder. The model enhances the under-\nstanding of the encoding of motifs through original and\nbilstm-attention encoders and improves the model model-\ning of crucial information from the encoders through gated\ndecoders, thus strengthening the model’s focus on motifs\nto generate music with impressive features. This study\ndesigned experiments to compare the generation perfor-\nmance of the Motif Transformer with other music generation\nmodels [12], [13]. And we developed objective and human\nsubjective evaluation metrics to assess the gap between\ngenerated and authentic music.\nThe rest of this article is organized as follows. Section II\nbriefly discusses the relevant work. Section III introduces the\ndetails of the proposed method. Section IV provides imple-\nmentation details and experimental results. Finally, Section V\nprovides a summary of the paper and discusses the poten-\ntial contributions and future development directions of the\nmethod proposed in this study.\nII. RELATED WORK\nThe composition of music by algorithms on computers dates\nback to the 1950s. After the first computers were invented\nand built, mathematician and composer Hiller used Markov\nchains in combination with knowledge of music theory to\ncompose the first computer-generated music, pioneering the\ncreation of music by artificial intelligence. Subsequently,\nwith the development of deep learning, many scholars began\nto study using neural network models for music composition.\nFor example, Casella et al. [7] proposed the melody_RNN\nmethod for music generation through a single-layer long\nand short-term memory network (LSTM) and a fully con-\nnected layer, which can combine current and historical music\ninformation to generate new music melodies. However, the\ngenerated music does not have long-time structural connec-\ntions due to the gradient disappearance problem of recurrent\nneural networks. Hadjeres et al. [14] proposed the Deep bash\nmethod. Unlike traditional recurrent neural networks, Deep\nbash generates music along the time axis but selects a time\nto generate from the middle to both sides, alleviating the\nlong-term dependency problem of traditional recurrent neu-\nral networks. Keerti et al. [15] combined the bidirectional\nlong short-term memory network with the attentional mech-\nanism to generate jazz music with a repetitive structure. The\nabove studies alleviate the gradient disappearance problem\nof recurrent neural networks by various methods, hoping to\ngenerate music containing more motifs using recurrent neural\nnetworks. Still, as of now, recurrent neural network-based\nmodels are only excellent at generating short segments of\nmusic, and the modeled information is difficult to establish\nlong structural connections.\nMany scholars have also employed other networks to solve\nthe problem of difficulty in modeling long-structured linked\nmusic. For example, Guan et al. [9] and Arora et al. [10]\nused generative adversarial networks to generate music and\nGrekow et al. [11] for modeling musical information using\nvariational autoencoders. However, their achievements are\nstill unsatisfactory. It was not until Vaswani et al. [16] pro-\nposed the Transformer model which made a sensation in\nartificial intelligence. Transformer is a self-attention-based\nsequence model with strong self-attention and modeling\ncapabilities that have achieved excellent results in many\nnatural language processing tasks [17], [18]. Shortly after\nthe release of Transformer, Huang et al. [12] proposed the\nMusic Transformer to pioneer the use of Transformer for\nmodeling music generation tasks, and they used Trans-\nformer’s powerful ability to handle long sequences to\ngenerate more structurally connected music melodies; Sub-\nsequently, Hsiao et al. [19] proposed the Compound Word\nTransformer, which represents musical events in the form\nof compound words, significantly reducing the length of\nmusical sequences and speeding up model convergence while\ngenerating music of comparable quality; Choi et al. [20] pro-\nposed a chord-conditioned Melody Transformer, which uses\na transformer to generate rhythms and pitches conditional on\nchords. The model can effectively learn the pitch and rhythm\ndistribution of music, and there are some significant repetitive\nfragments in the generated music melody. Shih et al. [13]\nproposed the Theme Transformer, which extracts repetitive\n63198 VOLUME 11, 2023\nH. Wang et al.: Motif Transformer: Generating Music With Motifs\nfragments of musical melodies as thematic material for the\ninput model and makes the model focus more on thematic\nfragments by a novel position encoding method and a method\nfor balancing attention mechanisms. It is possible to generate\nsignificant repetitive fragments for short and medium-sized\nmusic. However, the performance of generating repetitive\nfragments on long music still needs to be improved.\nDue to the powerful modeling capability of Transformer\nand its excellent performance in maintaining long-term struc-\ntural consistency, this study proposes the Motif Transformer\nmodel based on Transformer. And inspired by [21] and [22],\nour proposed model contains multiple encoders. Specifi-\ncally, the Motif Transformer includes an original encoder,\na bilstm-attention encoder, and a gated decoder; original\nand bilstm-attention encoders encode motifs and pass the\nencoded information to the gated decoder through their\nrespective cross-attention mechanisms; The gated decoder\nbalances cross-attention and self-attention through gate con-\ntrol, allowing the model to capture motif information better\nand generate music containing more motifs.\nIII. APPROACH\nIn this section, this study describes the proposed model\nnamed Motif Transformer. Sequence-based music genera-\ntion methods typically convert the music in midi format\ninto symbol sequences and input them into the model.\nEach note in music requires multiple symbol representations,\nwhich leads to a significant increase in sequence length and\ncomplexity. Therefore, traditional Transformer based music\ngeneration methods may have the following two drawbacks\nwhen generating music. Firstly, due to the lack of ability\nto capture temporal information in the Transformer model’s\nself-attention mechanism, the encoder of the model is dif-\nficult to capture temporal information in the sequences.\nSecondly, due to the long and complex sequence, the decoder\nmay ignore the motif information transmitted by the encoder.\nTo this end, we propose two methods to enhance the model’s\nability to generate motifs. Firstly, we used a multi encoders\narchitecture to improve the model’s encoding ability [21],\n[22]. We have designed an encoder called bidirectional\nlong short term memory-attention encoder (bilstm-attention\nencoder) based on the bidirectional long short term mem-\nory network (BILSTM) and the attention mechanism. The\nbilstm-attention encoder can capture temporal information\nin music sequences, making up for the shortcomings of the\nself-attention mechanism and providing more information\nabout motifs for the model. Secondly, we designed a gated\ndecoder to generate more motifs. In our model, the encoders\nare responsible for encoding motifs and passing the encoded\ninformation to the decoder through the cross-attention mech-\nanism. The decoder receives the entire music sequences and\ninformation from the encoders and generates music based\non them. To enhance the model’s ability to generate motifs,\nwe have designed a gating mechanism to turn off some\nlayers of self-attention mechanism, so that the decoder pays\nmore attention to the music motif information encoded by\nthe encoders through the cross-attention mechanisms. In the\nmotif areas, we use cross-attention mechanism in all layers,\nwhile only using self-attention mechanism in the first two\nlayers. In non-motif areas, all layers use self-attention. This\nmakes it easier for the model to interact with the motif\ninformation transmitted by the encoders through the cross-\nattention mechanisms, thereby generating music containing\nmore motifs.\nAs shown in Fig.2, our model contains three main mod-\nules, the original encoder, the bilstm-attention encoder, and\nthe gated decoder. The original and the bilstm-attention\nencoders model the motifs of music and then add them to\nthe gated decoder via the cross-attention mechanism. The\ngated decoder receives the complete music sequences and\nthe information from the encoders and generates the music\ncontent. In the following, this study will describe the encoders\nand decoder in the model separately.\nA. ORIGINAL ENCODER\nThe role of the Transformer encoder is to encode the input\nsequences into an internal representation. It contains multi-\nple attention layers, with each layer encoding the previous\nlayer’s output. Each attention layer has a self-attention mech-\nanism, which calculates weights for the hidden states at each\nlocation. These weights show how well the hidden state at\nthe current location is related to the hidden conditions at\nother locations. The attention mechanism allows the model\nto extract information from the entire sequences and encode\nthem into the internal representation. In general, a Trans-\nformer model uses only one encoder, but using multiple\nindependent encoders can improve the expressiveness of the\nmodel and allow the model to capture better the complex rela-\ntionships in the sequences [21], [22]. Based on this, this study\ndesigned two encoders for our model: the original encoder\nand the bilstm-attention encoder, and we will introduce them\nseparately below.\nTransformer has strong modeling and long-term struc-\ntural consistency capabilities, and its encoder has strong\nencoding capabilities. Many music generation methods use\nTransformer’s encoder and have achieved excellent per-\nformance [12], [13]. This article retains the Transformer’s\noriginal encoder as one of the model’s encoders and names\nit the original encoder. As shown in the left side of Fig.2, the\noriginal encoder consists of an embedding layer, a position\nencoding, and an attention layer, where the attention layer is\na stack of multi-headed attention and feedforward networks.\nSuppose the input music clips are X = {x1, x2 · · ·xτ }, then the\nembedding encoding and position encoding can be expressed\nas:\nXV = Emb (X) + PositionEng (X) (1)\nAfter being encoded, X is fed into the attention layer. The\nattention layer is the core of the traditional encoder and con-\ntains a multi-headed attention mechanism and a feed-forward\nneural network. After entering the attention layer, X is first\ntransformed linearly into Q, K, and V , to get the information\nVOLUME 11, 2023 63199\nH. Wang et al.: Motif Transformer: Generating Music With Motifs\nFIGURE 2. Model architecture diagram. On the left side of the figure is the original encoder; in the middle of the figure is the\ngated decoder; and on the right side of the figure is the bilstm-attention encoder.\nof different spatial locations of X itself, as shown in (2).\nQ, K, V = XV WQ, XV WK , XV WV (2)\nNext, Q, K, and V are again linearly transformed to input mul-\ntiple attention mechanisms. After that, multiple attentions are\nconnected to get the output of multiple attentions, as shown\nin (3) (4).\nheadh = Attention\n(\nQW Q\nh , KW K\nh , VW V\nh\n)\n(3)\nMultiHeadAttn (Q, K, V )=Concat (head1, · · ·, headH ) W o\n(4)\nwhere Attention() represents the attention mechanism, Mul-\ntiHeadAttn() represents the multi-head attention function,\nConcat(head1,. . . ,headH ) represents the output of multiple\nattentions stitched together, and headH represents the output\nof a single attention.\nThe output of multi-headed attention is followed by a fully\nconnected feedforward neural network, and there are residual\nconnections and normalization calculations at both the input\nand output of the feedforward neural network, which are\nshown in the following (5) (6) (7).\nLm = LayerNorm (A + X) (5)\nP = Position − wise − Feed − Forward(Lm) (6)\nO = LayerNorm (P + Lm) (7)\nwhere LayerNorm() denotes the regularization process;\nPosition-wise-Feed-Forward() denotes the feed-forward\nnetwork calculation.\nB. BILSTM-ATTENTION ENCODER\nTransformer’s self-attention mechanism can model the asso-\nciation between the current position and other positions at\nonce, and has good global modeling ability. However, the\nself-attention mechanism cannot capture the temporal infor-\nmation of the sequences. Because the bidirectional long\nshort term memory network has a better ability to capture\ntime information, this study additionally designs an encoder\nnamed bidirectional long short term memory-attention\nencoder (bilstm-attention encoder), which consists of an\nembedding layer, a position encoding layer, a linear layer, two\nbidirectional long short term memory networks, and a atten-\ntion mechanism. In this way, the bilstm-attention encoder will\ncombine the global modeling ability of the attention mecha-\nnism with the time information modeling ability of BILSTM.\nLike the original encoder, the bilstm-attention encoder also\nencodes only motifs. Still, unlike the original encoder, which\ncan only encode self-internal connections between messages,\nthe bilstm-attention encoder can model the relationships\nof input information at the time scale, bringing temporal\ninformation to the model and enhancing the model’s under-\nstanding of motifs. Assuming that the input music fragments\nare X = {x1, x2 · · ·xτ }, the computation of the embedding\nlayer and position encoding of the bilstm-attention encoder\ncan be expressed as in (8).\nXT = Emb (X) + PositionEng(X) (8)\nNext, the music fragments after the computed embedding\nencoding and location encoding are fed into the linear layer,\ncalculated as in (9).\nXL = Linear (XT ) (9)\nAfter the linear transformation, we want the next layer of\nthe computation process to contain both past and future\n63200 VOLUME 11, 2023\nH. Wang et al.: Motif Transformer: Generating Music With Motifs\nmusic fragments information. Therefore, we use a two-layer\nBiLSTM. The BiLSTM can use a forward LSTM to obtain\nforward implied state information and a backward LSTM to\nobtain backward implied state information [23].\nL1 = Bilstm (XL ) (10)\nL2 = Bilstm (L1) (11)\nFinally, to enhance the ability of the model to establish struc-\ntural connections, we set up a layer of attention mechanism\nlayer, which is calculated as in (12).\nAT = Attention (L2) (12)\nC. GATED DECODER\nThe role of the decoder is to decode the output of the\nencoder and generate the target sequences. The decoder\ncontains multiple concatenated attention layers that decode\nthe input information. Unlike the encoder, the decoder uses\ntwo attention mechanisms in its modeling process: self-\nattention and cross-attention. Self-attention provides an inter-\nnal understanding of the relationship between the current\nhidden state and the sequence generated during decoding,\nwith strong self-attention and local dependence, whereas\nthe cross-attention mechanism interacts with the informa-\ntion in the encoder with that in the decoder, enhancing\nthe model’s understanding of information about different\nmodalities or different spatial locations. Balancing these two\nattention mechanisms allows the model to take full advan-\ntage of their respective strengths and improves the model’s\nperformance [24].\nThis study places self-attention and two cross-attention\nmechanisms in parallel and designs a gating mechanism to\ncontrol the information flow between self-attention and cross-\nattention mechanisms. As shown in the middle of Fig.2, in the\nfirst two layers of the decoder, this study uses self-attention\nmechanism in all areas, while only cross-attention mecha-\nnisms are used in the motif areas; In the middle two layers of\nthe decoder, this study uses self-attention only for non-motif\nareas and uses cross-attention linked to the original encoder\nin the motif areas; In the last two layers of the decoder, this\nstudy uses self-attention only for non-motif areas and uses\ncross-attention connected to the bilstm-attention encoder in\nthe motif areas. If we let f l\nt be the output of the lth decoder\nat time step t, the mathematical equation for the process is\nexpressed as follows.\nf l\nt =\n\n\nkt f l,(cross1)\nt + (1 − kt ) f l,(self )\nt , l > 4\nkt f l,(cross2)\nt + (1 − kt ) f (l,(self ))\nt , 2 < l < 4\nkt f l,(cross1)\nt + kt f l,(cross2)\nt + f l,(self )\nt , l < 2\n(13)\nwhere kt indicates whether the sequence is within the motifs\nat time step t. If it is within the motifs then kt = 1, other-\nwise kt = 0. f l,(cross1)\nt denotes the output of cross-attention\nconnected to the conventional encoder, f l,(cross2)\nt denotes the\noutput of cross-attention connected to the temporal encoder,\nand f l,(self )\nt denotes the output of self-attention.\nIV. EXPERIMENTS\nA. DATASET AND DATA PREPROCESSING\nThis study uses the POP909 dataset [25] to perform experi-\nments on our model. The POP909 dataset contains 909 pop-\nular music tracks in midi form composed by professional\nmusicians. The songs in the dataset include three main types\nof musical information: the main melody (with the vocal pair\nthereof), the secondary melody, and the piano accompani-\nment. In this case, the secondary melody plays a minor role\nin the whole song, and its removal does not affect the quality\nof the song too much [13]. Referrin to [19], this study used\ninformation from only two tracks, the main melody and the\npiano accompaniment, and selected the songs in 4/4 time in\nthem. After the selection was completed, a total of 713 songs\nwere available for the experiment, of which we took 29 songs\nas the test set, and the remaining songs were used for model\ntraining.\nTo make the POP909 dataset available for experiments\non the model, we need to extract music features from\nmusic files in midi format and combine them into event\nsequences. This study uses two tracks (main melody and\npiano accompaniment) from the POP dataset songs, and the\nextracted note- related sequences are Note-On-Melody, Note-\nDuration-Melody, Note-Velocity-Melody, Note-On-Piano,\nNote-Duration-Piano, and Note-Velocity-Piano; Rhythm and\nposition related sequences as Tempo, Bar, and Position; in\nwhich, Note-On indicates the pitch start time (range is 1-127),\nNote-Duration indicates the pitch playing duration (1/4 beat\nas the basic the unit, range is 1-64), Note-Velocity indicates\nthe pitch playing intensity (range is 1-126), Tempo means the\nsong playing tempo, taking the value of 17bpm-194bpm, Bar\nshows the number of bars, and Position indicates the position\nof each event. In addition, following [13], motif-start and\nmotif-end are set in the music motifs to mark the start and end\nof the music motifs. Fig.3, for example, shows the conversion\nof midi music into an event sequence.\nFIGURE 3. Example of converting Midi music into a sequence of music\nevents.\nB. MODEL SETTING\nIn the experiment of this study, we set the maximum length\nof the sequence of music events to 512. The bilstm-attention\nencoder of the model is trained through the Bi LSTM network\nwith a hidden layer of 256. Both the original encoder and\nthe gated decoder of the model consist of 6 attention layers,\neach with eight attention heads, and a feedforward neural\nnetwork hidden dimension of 1024. The configuration of\nour experimental environment is shown in Table.1. Referring\nto [13], the experiment respectively selected 29 songs (about\n4%) from a dataset of 713 songs as the test set and validation\nset, and used the remaining songs as the training set. This\nVOLUME 11, 2023 63201\nH. Wang et al.: Motif Transformer: Generating Music With Motifs\nTABLE 1. Experimental environment table.\nexperiment used Adaptive Moment Estimation for optimiza-\ntion and cross entropy as the loss function. Set the learning\nrate of the Adam optimizer to 0.0001 and the number of\niterations for training to 2000.\nC. OBJECTIVE EVALUATION\nPOP909 dataset was used for this evaluation to train Atten-\ntional network [15], Music Transformer [12] and Theme\nTransformer [13], and their results were compared with the\nMotif Transformer. In terms of objective metrics selection,\nthis study first selected the null beat rate [26] and pitch\nclass entropy [27] to objectively evaluate the generated music\nand compared the music generated by the models with the\nreal music, and the closer the generated music is to the real\nmusic, the better the performance of the model. In addition,\nthis study also designed an objective metric to measure the\nmodel’s ability to generate motifs: used motifs (UM). The\nhigher metric value indicates the model’s stronger ability to\ngenerate motifs. Each of the three metrics is described below.\nEmpty beat rate (EBR) is the ratio of beats played without\nany notes or instruments to the total number of beats in a\nrhythm [26]. Empty beat rate is often used to measure the\nsense of empty inspiration or emptiness in a piece of music.\nAlso known as the ‘‘rest ratio’’, the airtime ratio is a standard\nmetric used in music analysis and generation. Define (14),\nwhere empty_beat denotes the number of empty beats and\nall_beat indicates the number of all beats.\nEmpty_beat_rate = empty_beat\nall_beat (14)\nPitch class entropy (PCE) can be used to describe the\nuniformity and complexity of the musical pitch distribu-\ntion [27]. In the field of music, each note corresponds to a\npitch level, and pitch entropy is the statistical quantity used to\ndescribe the distribution of these levels as they occur in music.\nA lower pitch entropy indicates a more regular and biased\nmusical structure. Define (15), where P(pitch=i) denotes the\nprobability of occurrence of the i-th category of pitches.\nPitch_class_entropy=−\n11∑\ni=0\nP (pitch=i) log2 (P (pitch = i))\n(15)\nTo verify the model’s ability of generating motifs, we pro-\npose an objective metric called used motifs (UM). The\nrepetitive fragments (also called motifs) in a piece of music\nare often used to express the tone and feel of the music and\nare the heart of a song. In this study, we measure the model’s\nTABLE 2. The entropy of null beat rate and pitch class for\nmodel-generated music as well as real music.\nability to generate motifs by counting the ratio of the number\nof motifs to the number of bars in the generated music.\nDefine (16), where motifs denote the number of occurrences\nof motifs in the music, and n_bars denotes the number of bars.\nUsed_Motifs = motifs\nn_bars (16)\nThis study had each model generate ten pieces of music,\nthen each calculated the EBR and PCE and averaged them,\nand compared their results with those of real music. To eval-\nuate the performance of the Motif Transformer, we selected\nthree most typical open-source models, namely:\n1) Attentional network: A model combining attention\nmechanism and bidirectional Long short-term mem-\nory network can generate music with some repetitive\nstructures [15].\n2) Music Transformer: A music generative model based\non the relative attention mechanism proposed by\nGoogle Brain can generate long-term music with\nmotifs [12].\n3) Theme Transformer: A new transformer-based model\nthat proposes a novel position encoding method\nand a method for balancing attention mechanisms,\nspecifically for generating music with motifs [13].\nAs shown in Table.2, our model achieves the minimum\nresults and is closest to the real music for both objective\nmetrics. This indicates that the music generated by the model\nproposed in this study has a better sense of rhythm and\nstructural regularity than other models and is closer to the real\nmusic.\nFIGURE 4. Distribution of the UM results, each model generates ten\npieces of 32-bar, 64-bar, and 128-bar music, and the average UM of the\nten pieces of music is shown in the figure.\n63202 VOLUME 11, 2023\nH. Wang et al.: Motif Transformer: Generating Music With Motifs\nFig.4 shows the distribution of UM for each model. From\nit, we can find that the more bars of music generated by\nall models, the lower the relative UM values. This indicates\nthat all models are less capable of generating long-structured\nmusic than short-structured music. In addition, compared\nwith other models, our model generates music with higher\nUM values regardless of the number of bars, indicating that\nour model can generate motifs better.\nD. SUBJECTIVE EVALUATION\nThe value and quality of music is a subjective feeling, and\nso far, it is not possible to judge the music entirely simply\nby objective evaluation. Therefore, this study also designed\nlistening experiments to evaluate the music generated by the\ndifferent models. In the experiment, 20 volunteers (10 of each\ngender, 5 of whom were professional music practitioners)\nrated musical works on a scale from 1 to 5 based on the\nfollowing five indicators. Each model generates ten pieces of\nmusic for the 64 bars, and each volunteer selects two pieces\nfrom each model to listen to and evaluate. And to ensure\nthe experiment’s validity, the music after each volunteer’s\nselection was randomly disrupted and then made available for\nvolunteers to evaluate and score.\n• Truth: Is this music consistent with human creative\nhabits?\n• Structure: Does this music have distinctly repetitive\nsegments?\n• Harmony: Does the melody of this music sound\nharmonious?\n• Accuracy: Is this music free of compositional and\nperformance errors?\n• Pleasure: Does this music sound good and pleasant?\nTABLE 3. Subjective evaluation results.\nAs can be seen from Table.3, the Motif Transformer pro-\nposed in this study has better performance in all human\nsubjective evaluation metrics. This shows that Motif Trans-\nformer can generate harmonious and natural music. In par-\nticular, the score of structure has been improved compared\nwith other models. This shows that Motif Transformer has a\nmore significant advantage in generating motifs, validating\nthe effectiveness of the model improvements.\nV. CONCLUSION\nThis study proposes Motif Transformer for the feature that\nmusic has structural repetitiveness. This study designed a\nmusic generation method that combines multiple encoders\nand a gated decoder. Motif Transformer enhances the\nunderstanding of encoding of motifs through the origi-\nnal encoder and bilstm-attention encoder, and enhances the\nmodel modeling of motif information from encoders through\nthe gated decoder, thus allowing the model to gain more\nattention to motif information. Moreover, this study proposes\nan objective metric called used motifs to verify the ability\nof the model to generate motifs and a subjective listening\nexperiment to test the model’s validity. After experimental\nverification, the network model proposed in this study can\ngenerate harmonious and natural music, and the generated\nmusic contains more motifs.\nCompared with traditional music generation methods, the\nmethod proposed in this article can generate music with\nspecific motifs. According to the method proposed in this\narticle, people can obtain a large number of music works more\nconveniently, and the theme style of these music works can be\nselected according to personal needs, which can significantly\npromote music consumption. Moreover, the method proposed\nin this article can also provide more possibilities for those\nengaged in music creation and production, bringing them\nmore creative and inspirational inspiration, thereby promot-\ning the development of the music industry. In addition, the\ncurrent model only performs well in generating one type of\nmotif. A real song may contain different types of motifs.\nIn the future, we will explore using the model to generate\nmusic that contains multiple types of motifs.\nREFERENCES\n[1] K. Markov and T. Matsui, ‘‘Music genre and emotion recognition using\nGaussian processes,’’ IEEE Access, vol. 2, pp. 688–697, 2014, doi:\n10.1109/ACCESS.2014.2333095.\n[2] M. Ashraf, G. Geng, X. Wang, F. Ahmad, and F. Abid, ‘‘A globally reg-\nularized joint neural architecture for music classification,’’ IEEE Access,\nvol. 8, pp. 220980–220989, 2020, doi: 10.1109/ACCESS.2020.3043142.\n[3] I. Goienetxea, I. Mendialdua, I. Rodríguez, and B. Sierra, ‘‘Statistics-\nbased music generation approach considering both rhythm and melody\ncoherence,’’ IEEE Access, vol. 7, pp. 183365–183382, 2019, doi:\n10.1109/ACCESS.2019.2959696.\n[4] M. K. Jedrzejewska, A. Zjawinski, and B. Stasiak, ‘‘Generating musical\nexpression of MIDI music with LSTM neural network,’’ in Proc. 11th Int.\nConf. Hum. Syst. Interact. (HSI), Gdansk, Poland, Jul. 2018, pp. 132–138,\ndoi: 10.1109/HSI.2018.8431033.\n[5] S. Oore, I. Simon, S. Dieleman, D. Eck, and K. Simonyan, ‘‘This time\nwith feeling: Learning expressive musical performance,’’ Neural Comput.\nAppl., vol. 32, no. 4, pp. 955–967, Feb. 2020, doi: 10.1007/s00521-018-\n3758-9.\n[6] Y . Qin, H. Xie, S. Ding, B. Tan, Y . Li, B. Zhao, and M. Ye, ‘‘Bar trans-\nformer: A hierarchical model for learning long-term structure and generat-\ning impressive pop music,’’ Appl. Intell., vol. 53, no. 9, pp. 10130–10148,\n2023, doi: 10.1007/s10489-022-04049-3.\n[7] P. Casella and A. Paiva, ‘‘MAgentA: An architecture for real time auto-\nmatic composition of background music,’’ in Proc. Int. Workshop Intell.\nVirtual Agents, Berlin, Germany, pp. 224–232, 2001, doi: 10.1007/3-540-\n44812-8_18.\n[8] D. Eck and J. Schmidhuber, ‘‘A first look at music composition\nusing LSTM recurrent neural networks,’’ Istituto Dalle Molle Di\nStudi Sull Intelligenza Artificiale, Tech. Rep. IDSIA-07-02, 2002,\nvol. 103, no. 4, pp. 48–56. [Online]. Available: https://dl.acm.org/\ndoi/book/10.5555/870511\n[9] F. Guan, C. Yu, and S. Yang, ‘‘A GAN model with self-attention mech-\nanism to generate multi-instruments symbolic music,’’ in Proc. Int. Joint\nConf. Neural Netw. (IJCNN), Budapest, Hungary, Jul. 2019, pp. 1–6, doi:\n10.1109/IJCNN.2019.8852291.\n[10] S. Arora, A. Dassler, T. Earls, M. Ferrara, N. Kopparapu, and S. Mathew,\n‘‘An analysis of implementing a GAN to generate MIDI music,’’ in Proc.\nIEEE MIT Undergraduate Res. Technol. Conf. (URTC), Cambridge, MA,\nUSA, 2022, pp. 1–5, doi: 10.1109/URTC56832.2022.10002181.\nVOLUME 11, 2023 63203\nH. Wang et al.: Motif Transformer: Generating Music With Motifs\n[11] J. Grekow and T. Dimitrova-Grekow, ‘‘Monophonic music generation with\na given emotion using conditional variational autoencoder,’’ IEEE Access,\nvol. 9, pp. 129088–129101, 2021, doi: 10.1109/ACCESS.2021.3113829.\n[12] C.-Z. Anna Huang, A. Vaswani, J. Uszkoreit, N. Shazeer, I. Simon,\nC. Hawthorne, A. M. Dai, M. D. Hoffman, M. Dinculescu, and D. Eck,\n‘‘Music transformer,’’ 2018, arXiv:1809.04281.\n[13] Y . Shih, S. Wu, F. Zalkow, M. Müller, and Y . Yang, ‘‘Theme\ntransformer: Symbolic music generation with theme-conditioned trans-\nformer,’’ IEEE Trans. Multimedia, early access, Mar. 23, 2022, doi:\n10.1109/TMM.2022.3161851.\n[14] G. Hadjeres, F. Pachet, and F. Nielsen, ‘‘DeepBach: A steerable model for\nbach chorales generation,’’ 2016, arXiv:1612.01010.\n[15] G. Keerti, A. N. Vaishnavi, P. Mukherjee, A. S. Vidya, G. S. Sreenithya, and\nD. Nayab, ‘‘Attentional networks for music generation,’’ Multimedia Tools\nAppl., vol. 81, no. 4, pp. 5179–5189, Feb. 2022, doi: 10.1007/s11042-021-\n11881-1.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 2, 2017, pp. 5999–6009.\n[17] Q. Guo, J. Huang, N. Xiong, and P. Wang, ‘‘MS-pointer network: Abstrac-\ntive text summary based on multi-head self-attention,’’ IEEE Access, vol. 7,\npp. 138603–138613, 2019, doi: 10.1109/ACCESS.2019.2941964.\n[18] C. Wen and L. Zhu, ‘‘A sequence-to-sequence framework based\non transformer with masked language model for optical music\nrecognition,’’ IEEE Access, vol. 10, pp. 118243–118252, 2022, doi:\n10.1109/ACCESS.2022.3220878.\n[19] W. Y . Hsiao, J. Y . Liu, and Y . C. Yeh, ‘‘Compound word transformer:\nLearning to compose full-song music over dynamic directed hypergraphs,’’\nin Proc. AAAI Conf. Artif. Intell., 2022, pp. 178–186.\n[20] K. Choi, J. Park, W. Heo, S. Jeon, and J. Park, ‘‘Chord conditioned\nmelody generation with transformer based decoders,’’ IEEE Access, vol. 9,\npp. 42071–42080, 2021, doi: 10.1109/ACCESS.2021.3065831.\n[21] A. Abdelraouf, M. Abdel-Aty, and J. Yuan, ‘‘Utilizing attention-based\nmulti-encoder–decoder neural networks for freeway traffic speed predic-\ntion,’’ IEEE Trans. Intell. Transp. Syst., vol. 23, no. 8, pp. 11960–11969,\nAug. 2022, doi: 10.1109/TITS.2021.3108939.\n[22] J. Bi, L. Zhang, H. Yuan, and J. Zhang, ‘‘Multi-indicator water quality pre-\ndiction with attention-assisted bidirectional LSTM and encoder–decoder,’’\nInf. Sci., vol. 625, pp. 65–80, May 2023, doi: 10.1016/j.ins.2022.12.091.\n[23] A. Graves, A.-R. Mohamed, and G. Hinton, ‘‘Speech recognition with\ndeep recurrent neural networks,’’ in Proc. IEEE Int. Conf. Acoust., Speech\nSignal Process., Vancouver, BC, Canada, Mar. 2013, pp. 6645–6649, doi:\n10.1109/ICASSP.2013.6638947.\n[24] H. Rashkin, A. Celikyilmaz, Y . Choi, and J. Gao, ‘‘PlotMachines:\nOutline-conditioned generation with dynamic plot state tracking,’’ 2020,\narXiv:2004.14967.\n[25] Z. Wang, K. Chen, J. Jiang, Y . Zhang, M. Xu, S. Dai, X. Gu, and G. Xia,\n‘‘POP909: A pop-song dataset for music arrangement generation,’’ 2020,\narXiv:2008.07142.\n[26] W. H. Dong, W. Y . Hsiao, and Y . H. Yang, ‘‘Pypianoroll: Open source\nPython package for handling multitrack pianoroll,’’ in Proc. ISMIR, Paris,\nFrance, 2018, pp. 120–127, doi: 10.5281/zenodo.4540221.\n[27] S.-L. Wu and Y .-H. Yang, ‘‘The jazz transformer on the front line:\nExploring the shortcomings of AI-composed music through quantitative\nmeasures,’’ 2020, arXiv:2008.01307.\nHENG WANG received the B.E. degree from\nthe Huazhong University of Science and Technol-\nogy, in 2006, and the Ph.D. degree in engineering\nfrom Wuhan University, in 2013. He is currently\na Professor with the School of Mathematics and\nComputer Science, Wuhan Polytechnic University.\nHe is also a Postdoctoral Research Fellow with\nAlto University, Finland. His research interests\ninclude the perception characteristics of acoustic\nspatial parameters, artificial intelligence, and the\napplication of 3D audio and video in virtual reality.\nSEN HAO received the B.E. degree from the\nHenan University of Technology, Zhengzhou,\nChina, in 2021. He is currently pursuing the M.S.\ndegree in software engineering with Wuhan Poly-\ntechnic University, Wuhan. His research interests\ninclude music information retrieval and artificial\nintelligence technology and its application.\nCONG ZHANGreceived the bachelor’s degree in\nautomation engineering from the Huazhong Uni-\nversity of Science and Technology, in 1993, the\nmaster’s degree in computer application technol-\nogy from the Wuhan University of Technology,\nin 1999, and the Ph.D. degree in computer applica-\ntion technology from Wuhan University, in 2010.\nHe is currently a Professor with the School of Elec-\ntrical and Electronic Engineering, Wuhan Poly-\ntechnic University. His research interests include\nmultimedia signal processing, multimedia communication system theory and\napplication, and pattern recognition.\nXIAOHU WANG received the B.E. degree from\nTianjin Chengjian University, Tianjin, China,\nin 2021. He is currently pursuing the M.S. degree\nin software engineering with Wuhan Polytech-\nnic University, Wuhan. His research interests\ninclude music information retrieval and artificial\nintelligence technology and its application.\nYILIN CHEN received the Ph.D. degree from the\nDepartment of Computer Science, Wuhan Univer-\nsity, China, in 2020. He is currently a Lecturer\nwith the School of Computer Science and Engi-\nneering, Wuhan Institute of Technology, Wuhan,\nChina. His research interests include multi-\nobjective optimization, intelligent optimization,\nimage processing, and computer graphics.\n63204 VOLUME 11, 2023",
  "topic": "Encoder",
  "concepts": [
    {
      "name": "Encoder",
      "score": 0.8202866315841675
    },
    {
      "name": "Computer science",
      "score": 0.7317104339599609
    },
    {
      "name": "Transformer",
      "score": 0.552108883857727
    },
    {
      "name": "Speech recognition",
      "score": 0.4718370735645294
    },
    {
      "name": "ENCODE",
      "score": 0.4469861686229706
    },
    {
      "name": "Motif (music)",
      "score": 0.4405294358730316
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.42353230714797974
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39561158418655396
    },
    {
      "name": "Acoustics",
      "score": 0.10537606477737427
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I14116566",
      "name": "Wuhan Polytechnic University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I91125648",
      "name": "Wuhan Institute of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 6
}