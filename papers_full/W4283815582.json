{
    "title": "Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey",
    "url": "https://openalex.org/W4283815582",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2885505436",
            "name": "Prajjwal Bhargava",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114779596",
            "name": "Vincent Ng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2885505436",
            "name": "Prajjwal Bhargava",
            "affiliations": [
                "The University of Texas at Dallas"
            ]
        },
        {
            "id": "https://openalex.org/A2114779596",
            "name": "Vincent Ng",
            "affiliations": [
                "The University of Texas at Dallas"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W9610585",
        "https://openalex.org/W3034723486",
        "https://openalex.org/W6766937060",
        "https://openalex.org/W2989588035",
        "https://openalex.org/W2950339735",
        "https://openalex.org/W2997616454",
        "https://openalex.org/W2996035354",
        "https://openalex.org/W2970161131",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2965647350",
        "https://openalex.org/W3004630545",
        "https://openalex.org/W2987215000",
        "https://openalex.org/W3015339775",
        "https://openalex.org/W2983331060",
        "https://openalex.org/W6662717790",
        "https://openalex.org/W6693199996",
        "https://openalex.org/W6744115927",
        "https://openalex.org/W6797393643",
        "https://openalex.org/W2970780738",
        "https://openalex.org/W3093166897",
        "https://openalex.org/W3088572095",
        "https://openalex.org/W3088410251",
        "https://openalex.org/W6743716967",
        "https://openalex.org/W3190572136",
        "https://openalex.org/W3025064182",
        "https://openalex.org/W2946949951",
        "https://openalex.org/W6635469476",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W6725147507",
        "https://openalex.org/W3113819910",
        "https://openalex.org/W3021524072",
        "https://openalex.org/W3023710830",
        "https://openalex.org/W3138016835",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2951048068",
        "https://openalex.org/W2996117790",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W3166728183",
        "https://openalex.org/W3045021308",
        "https://openalex.org/W3022671220",
        "https://openalex.org/W6796821673",
        "https://openalex.org/W3093059841",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2948036864",
        "https://openalex.org/W2997521893",
        "https://openalex.org/W3006881356",
        "https://openalex.org/W2963259903",
        "https://openalex.org/W6755519508",
        "https://openalex.org/W2936262349",
        "https://openalex.org/W3037763555",
        "https://openalex.org/W2107901333",
        "https://openalex.org/W3100026183",
        "https://openalex.org/W2101745498",
        "https://openalex.org/W2974596953",
        "https://openalex.org/W3023160663",
        "https://openalex.org/W3034912286",
        "https://openalex.org/W6759317646",
        "https://openalex.org/W6660370874",
        "https://openalex.org/W2903073381",
        "https://openalex.org/W2886424491",
        "https://openalex.org/W3172673629",
        "https://openalex.org/W3105356032",
        "https://openalex.org/W3023293091",
        "https://openalex.org/W2991223644",
        "https://openalex.org/W2962833140",
        "https://openalex.org/W3097683561",
        "https://openalex.org/W3102999298",
        "https://openalex.org/W3035428952",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2996908057",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W4288265479",
        "https://openalex.org/W3105391665",
        "https://openalex.org/W3104007871",
        "https://openalex.org/W2050482109",
        "https://openalex.org/W3176195078",
        "https://openalex.org/W2953345635",
        "https://openalex.org/W2963101081",
        "https://openalex.org/W3101850416",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W2038688024",
        "https://openalex.org/W2952570576",
        "https://openalex.org/W3116216579",
        "https://openalex.org/W3176454275",
        "https://openalex.org/W2963115613",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W3034995113",
        "https://openalex.org/W2998617917",
        "https://openalex.org/W2963018920",
        "https://openalex.org/W2970062726",
        "https://openalex.org/W3102187933",
        "https://openalex.org/W3176913510",
        "https://openalex.org/W3035032873",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4287642224",
        "https://openalex.org/W2963159690",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2264742718",
        "https://openalex.org/W3089102176",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2986266667",
        "https://openalex.org/W2971236147",
        "https://openalex.org/W2997545008",
        "https://openalex.org/W3176495666",
        "https://openalex.org/W4288243162",
        "https://openalex.org/W3198659451",
        "https://openalex.org/W4287025617",
        "https://openalex.org/W2509019445",
        "https://openalex.org/W3174464510",
        "https://openalex.org/W3034602344",
        "https://openalex.org/W3099771192",
        "https://openalex.org/W4288262459"
    ],
    "abstract": "While commonsense knowledge acquisition and reasoning has traditionally been a core research topic in the knowledge representation and reasoning community, recent years have seen a surge of interest in the natural language processing community in developing pre-trained models and testing their ability to address a variety of newly designed commonsense knowledge reasoning and generation tasks. This paper presents a survey of these tasks, discusses the strengths and weaknesses of state-of-the-art pre-trained models for commonsense reasoning and generation as revealed by these tasks, and reflects on future research directions.",
    "full_text": "Commonsense Knowledge Reasoning and Generation\nwith Pre-trained Language Models: A Survey\nPrajjwal Bhargava and Vincent Ng\nHuman Language Technology Research Institute\nUniversity of Texas at Dallas\nRichardson, TX 75083–0688\nprajjwal.bhargava@utdallas.edu, vince@hlt.utdallas.edu\nAbstract\nWhile commonsense knowledge acquisition and reasoning\nhas traditionally been a core research topic in the knowl-\nedge representation and reasoning community, recent years\nhave seen a surge of interest in the natural language process-\ning community in developing pre-trained models and testing\ntheir ability to address a variety of newly designed common-\nsense knowledge reasoning and generation tasks. This pa-\nper presents a survey of these tasks, discusses the strengths\nand weaknesses of state-of-the-art pre-trained models for\ncommonsense reasoning and generation as revealed by these\ntasks, and reﬂects on future research directions.\nIntroduction\nCommonsense knowledge is the information that is gener-\nally accepted by the majority of people concerning everyday\nlife, encapsulating the practical knowledge about how the\nworld works. Reasoning with commonsense knowledge is\nat the core of building natural language understanding mod-\nels and, more broadly, AI systems that can reason about the\nworld in the same way as humans do.\nA vast amount of work on commonsense knowledge ac-\nquisition and reasoning has traditionally been conducted in\nthe knowledge representation and reasoning community (see\nZang et al. (2013) for a survey). For instance, there have\nbeen notable attempts to manually create large-scale com-\nmonsense knowledge bases (e.g., Cyc (Lenat 1995)) and au-\ntomatically acquire commonsense knowledge from the Web\n(e.g., Open Mind Common Sense (Singh et al. 2002)). More\nrecently, the Winograd Schema Challenge, a pronoun res-\nolution task that requires the use of commonsense knowl-\nedge, was proposed as a practical alternative to the Turing\nTest (Levesque et al. 2012).\nThe advent of the neural natural language processing\n(NLP) era has revolutionized virtually all areas of NLP re-\nsearch. One of the major breakthroughs is arguably the de-\nvelopment of pre-trained language models (PLMs). Speciﬁ-\ncally, researchers have discovered that neural models can be\ntrained (via a process known aspre-training) on a large body\nof unannotated text to acquire general knowledge about lan-\nguage, including both linguistic and commonsense knowl-\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nedge. This has sparked tremendous interest in the NLP com-\nmunity in examining what kind of commonsense knowledge\nPLMs possess and the extent to which such knowledge can\nbe exploited to address commonsense knowledge reasoning\nand generation tasks in the last few years.\nOur goal in this paper is to provide the general AI audi-\nence with a timely survey of the recent advances in the NLP\ncommunity on commonsense knowledge reasoning and gen-\neration using PLMs. Speciﬁcally, the focus of this survey is\n(1) the kind of commonsense knowledge that PLMs possess\nand (2) the extent to which such knowledge can be exploited\nfor recently designed commonsense knowledge reasoning\nand generation tasks. For an overview of what linguistic\nknowledge PLMs possess, we refer the reader to Rogers\net al. (2020). For comprehensive surveys of the details and\nthe inner workings of PLMs, we refer the reader to Qiu et al.\n(2020), Han et al. (2021), and Kalyan et al. (2021).\nPre-trained Language Models\nIn this section, we provide the reader with the background\non PLMs needed to understand the rest of the paper.\nFor a long time, supervised learning has been the most\nsuccessful learning paradigm in NLP. For instance, training\na neural model to perform a classiﬁcation task in a super-\nvised manner primarily involves training an encoder to en-\ncode the input as atask-speciﬁc representationthat would be\nuseful for classifying a given sample. In contrast, for a text\ngeneration task (e.g., text summarization, machine transla-\ntion), one would typically employ an encoder-decoder neu-\nral architecture, in which the encoder ﬁrst encodes the input,\nand then the decoder generates the output sequence token by\ntoken based on both the encoded input and the tokens that\nhave been generated so far. The performance of supervised\nmodels is often limited by the (typically small) amount of\ndata they are trained on.\nPre-training offers a solution to the aforementioned data\nscarcity problem. The idea is to ﬁrst train a model on\none or more self-supervised learning tasks during a process\nknown as pre-training, and the resulting model, in which the\nweights have already been initialized during pre-training,\ncan be ﬁne-tuned using the (potentially small amount of)\ntask-speciﬁc data in a supervised fashion. Self-supervised\nlearning tasks are NLP tasks for which the label associated\nwith a training instance can be derived automatically from\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n12317\nthe text itself. Consider, for instance, one of the most well-\nknown self-supervised learning tasks, Masked Language\nModeling (MLM) (Devlin et al. 2019). Given a sequence\nof word tokens in which a certain percentage of tokens is\nmasked randomly, the goal of MLM is to predict the masked\ntokens. As can be imagined, a model for MLM can there-\nfore be trained on instances where each one is composed of\na partially masked sequence of word tokens and the associ-\nated “class” value is the masked tokens themselves. Because\nno human annotation is needed, a model can be pre-trained\non a very large amount of labeled data that can be automati-\ncally generated. Various studies have shown that pre-training\nallows a model to learn universal language representations\nthat encode both linguistic and commonsense knowledge,\nand a PLM, after being ﬁne-tuned, can offer substantially\nimproved performance on a wide variety of NLP tasks.\nExisting PLMs differ primarily in terms of (1) what is\nbeing pre-trained (is it the encoder, the decoder, or both of\nthem?); (2) the self-supervised learning tasks used; and (3)\nthe network architecture. While early work has focused on\npre-training the encoder (e.g., BERT (Devlin et al. 2019),\nRoBERTa (Liu et al. 2019), ELECTRA (Clark et al. 2019))\nor the decoder (e.g., GPT-2 (Svyatkovskiy et al. 2019)), re-\ncent work has focused on jointly pre-training the encoder\nand the decoder (e.g., T5 (Raffel et al. 2020), BART (Lewis\net al. 2020)). The most successful PLMs are based on the\nTransformer (Vaswani et al. 2017), a fully-connected self-\nattention model. Throughout this paper we will simply use\nthe term PLMs to refer to Transformer-based PLMs.\nCapturing Commonsense Knowledge\nHow well do PLMs capture commonsense knowledge? Re-\nsearchers have employed probing to answer this question.\nTo probe a PLM for commonsense knowledge, most of the\nprobing methods use a hand-crafted template to convert a\nrelational fact from a knowledge base (KB), which is typi-\ncally represented in the form of a triple <s;r;o> where sis\nthe SUBJECT , ris the RELATION , and ois the OBJECT , into\na natural language sentence. One template needs to be de-\nﬁned for each relation. As illustrated in Figure 1, each triple\nhaving the KB relation \"place of birth\" would be translated\nto a sentence of the form \"S UBJECT was born in O BJECT \".\nNote that a template keeps the entities intact while approxi-\nmately the RELATION to a set of hand-coded verbs/relations\nthat can generalize on numerous entities (e.g., A TLOCA -\nTION may be translated to \"is in\"). One of the entities (i.e.,\nthe S UBJECT or the O BJECT ) in the sentence will then be\nmasked. The resulting clozed sentence can then provide an\nautomated and ﬂexible way to probe PLMs for stored knowl-\nedge. Speciﬁcally, if a PLM can ﬁll in the blank in the given\nclozed sentence correctly (i.e., the answer is the same as the\nentity that appears in the triple originally used to generate\nthe sentence), then the PLM is considered to possess the\nknowledge being probed. Note that we are not asking the\nPLM to derive new knowledge: only inference is performed\nby the PLM to check for stored knowledge.\nSeveral key observations are being revealed via probing\nexperiments. First, PLMs are becoming a promising al-\nternative to KBs. BERT shows signs of capturing relational\nPre-trained  \nLM\n\"Dante was born in [MASK]\"\nNeural LM \nMemory Access Florence\ne.g. BERT\nFigure 1: A common approach to probe PLMs for stored\nknowledge (Petroni et al. 2019).\nPrompt Model Predictions\nA has fur, is big, has claws, has\nteeth, is an animal, eats, is brown,\nand lives in woods.\nbear, wolf, cat, ...\nTable 1: Masked token predictions about stereotypical as-\nsumptions get reﬁned as more properties are appended (Weir\net al. 2020).\nknowledge in a zero-shot setting reasonably well compared\nto supervised alternatives (Petroni et al. 2019). It can recall\nfactual knowledge for some relations such as one-to-one, but\nit struggles to perform well on other relations such as N-to-\nM and N-to-one.\nSecond, PLMs do not generalize well on unseen en-\ntities. While BERT can predict the top 100 triples mined\nfrom Wikipedia fairly accurately, which suggests that BERT\ncan generalize to speciﬁc data sources (Davison et al. 2019),\nPLMs do not generalize well on entities not encountered\nduring pre-training due to their heavy reliance on memo-\nrization in the pre-training process (Logan et al. 2019).\nThird, PLMs can perform comparisons and catego-\nrizations of entities. Speciﬁcally, they can compare phys-\nical objects along a particular attribute such as weight or\nsize (e.g., a chair is smaller than a room) (Goel et al. 2019).\nWhen it comes to categorization, they work reasonably well\non knowledge types that are ontological in nature, such as\n\"mango isA fruit\" (Hwang et al. 2021).\nFinally, analyzing the top-k predictions made by PLMs\non the association between an entity and its attributes, we\nsee that PLMs can learn stereotypical associations rea-\nsonably well from large text corpora. As more properties are\nappended to provide contextual knowledge (see the example\nin Table 1), the performance of RoBERTa-L increases, with\npredictions going from being sensible to more acceptable\nas per human interpretation. Speciﬁcally, PLMs do better\non functional (e.g., \"eat ﬁsh\") and encyclopedic (e.g., \"are\nfound in forests\") knowledge than on visual-perceptual vari-\nants (e.g., \"bears have fur\"). Although this result is encour-\naging, when asked for widely acceptable properties about\nan entity, the ranked predictions provided by PLMs do not\ncorrelate strongly with those of humans (Weir et al. 2020).\nReasoning with Commonsense Knowledge\nIn this section, we examine how well PLMs perform com-\nmonsense reasoning by considering ﬁve types of common-\nsense reasoning tasks.\n12318\nLinguistic Reasoning\nLinguistic reasoning is concerned with understanding text\nfor which the correct interpretation requires commonsense\nknowledge. A representative benchmark for linguistic rea-\nsoning is W INO GRANDE (Sakaguchi et al. 2020), which\nconsists of Winograd schema-inspired problems that re-\nquire linguistic, social or physical reasoning (Levesque et al.\n2012). As an example, given the sentence \"The plant took up\ntoo much room in the urn, because the\nwas large\" and\ntwo answer candidates \"plant\" and \"urn\", the goal is to de-\ntermine which candidate should be used to ﬁll in the blank.\nSeveral observations can be made based on the perfor-\nmance of PLMs on this and other linguistic reasoning tasks.\nFirst, BERT shows poor linguistic sensitivity and be-\ncomes fragile on negated and misprimed sentences (Kass-\nner and Schütze 2020; Ettinger 2020). For example, BERT\nfails to distinguish between the two sentences “Birds can-\nnot [MASK]”) and “Birds can [MASK]” and tends to get\ndistracted if they are prepended with \"misprimes\" such as\n“Talk? Birds can [MASK]”. The fact that its predictions do\nnot change with such major changes indicates that BERT\ndoes not attend to the prominent cues in the desired manner.\nSecond, PLMs perform poorly on numerical knowl-\nedge out-of-the-box (Lin et al. 2020a; Wallace et al. 2019;\nChen et al. 2019; Bhagavatula et al. 2020). For instance,\ngiven the sentence \"Birds have [MASK] legs\", BERT pre-\ndicts \"four\" to be the answer, suggesting that pre-training\ndoes not facilitate the acquisition of numerical knowledge.\nThird, as the sentences in a given reasoning task re-\nquire more turns of logical reasoning (i.e., the task be-\ncomes increasingly complex), BERT’s performance de-\nteriorates (Zhou et al. 2020b; Richardson and Sabharwal\n2020; Huang et al. 2019). These are typically sentences with\ncomplex semantics such as riddles, where PLMs are re-\nquired to understand ﬁgurative language (Lin et al. 2021a).\nSeveral attempts have been made to improve the robust-\nness of PLMs for linguistic reasoning tasks.\nSemantic similarity. Niu et al. (2021) show that seman-\ntic similarity matching can be used to make PLMs robust\nagainst irrelevant factors such as word frequencies. Specif-\nically, we can ﬁrst use PLMs to generate plausible answers\nso that we can compute the similarity between each gener-\nated answer and each of the provided answer candidates, and\nthen we can select the answer candidate that has the highest\nsimilarity score as the correct answer.\nAttention maps. Klein and Nabi (2019) show that atten-\ntion maps obtained from BERT can be used for coreference\nresolution in long sentences, suggesting their potential use-\nfulness for pronoun disambiguation.\nNumerical reasoning. To improve numerical reasoning,\nGeva et al. (2020) pre-train BERT on two numerical tasks,\none involving predicting what comes after a sentence such\nas \"3+4+5\" and the other involving answering numerical\nquestions (e.g., given a historical passage about Spain, an-\nswer the question of \"How many Japanese families were\nin Spain?\"), so that BERT is endowed with the ability to\nunderstand computations expressed in pseudo-natural lan-\nguage (text+digits).\nName: Egg\nTemperature: RoomTemp\nisCooked: False\nisBroken: True\n<heatUp>\nt+1\nName: Egg\nTemperature: Hot\nisCooked: True\nisBroken: True\nt\nFigure 2: Simulating what might happen next in order\nto enable PLMs to encode language \"form\" and \"mean-\ning\" (Zellers et al. 2021).\nReasoning about Physical World\nPhysical commonsense reasoning involves understanding\nconcepts based on the physical properties of objects, includ-\ning the affordances of objects (i.e., the actions applicable to\nthem) and how they can be manipulated. A representative\nbenchmark for physical commonsense reasoning is PIQA\n(Bisk et al. 2020). Given a sentence such as \"When boiling\nbutter, when it’s ready, you can\n\", the goal is to ﬁll in\nthe blank with one of two answer options, such as \"Pour it\nonto a plate\" and \"Pour it onto a jar\".\nPerception and interaction are among the key compo-\nnents behind how humans learn to reason about the physi-\ncal world. However, static input representations which cur-\nrent neural models are fed are inadequate since they cannot\ncompensate for the information humans derive from being\nin a dynamic physical world. So, a key question posed by\nPIQA is whether PLMs can reason over physical common-\nsense questions without interacting with the physical world.\nSeveral observations can be made about the performance\nof PLMs on physical commonsense reasoning questions.\nFirst, PLMs predominantly learn property associations\nthat are explicitly mentioned in text, achieving higher\naccuracies on entities that have simple affordances (e.g.,\n\"spoon\") than on entities that have a long tail of affordances\n(e.g., \"water\"). Second, PLMs struggle to understand fun-\ndamental relations (e.g., \"before/after\", \"top/bottom\") and\nﬁnd it hard to reason when common objects are used in\nunconventional ways (e.g., a glue stick is used as a paper-\nweight). Finally, although neural representations are dexter-\nous at capturing the affordances (\"boats can be driven\") and\nproperties (\"boats require fuel\") of objects, PLMs struggle\nto understand the connection between affordances and\nproperties (Forbes et al. 2019; Zhao et al. 2020).\nIn light of the weakness associated with the lack of inter-\naction with the physical world, Zellers et al. (2021) explore\nthe beneﬁts of providing PLMs access to world dynamics.\nWorld dynamics include information that one would obtain\nafter interacting with objects. As an example, consider Fig-\nure 2. If an action heatUp is applied to a pan, the model\nwill learn that the temperature of an egg has risen to become\nhot and is now in a cooked state. Predicting object states\nafter an action has been taken drastically improves a PLM’s\nability to make correct inferences about object states.\n12319\nAbductive Reasoning\nAbductive reasoning involves ﬁnding the most likely ex-\nplanation for a set of incomplete observations. There are\nat least two representative benchmarks for abductive rea-\nsoning, COSMOS QA (Huang et al. 2019) and H ELLA SWAG\n(Zellers et al. 2019b). COSMOS QA is a commonsense com-\nprehension task where, given a context, the goal is to choose\nthe answer to the question based on the context from four\nanswer candidates. This benchmark contains questions that\nrequire abductive reasoning, such as \"what might I continue\nto do after the situation described in the context?\" H EL-\nLASWAG is a benchmark in which the goal is to choose the\nbest plausible ending of a given context out of four options.\nSeveral observations can be made. First, Huang et al.\n(2019) attribute the errors made by their model on C OS-\nMOS QA to two reasons. First, PLMs struggle on examples\nwhere the context becomes intricate enough to require\ncross-sentence interpretation and reasoning. In such ex-\namples, PLMs are required to understand the important parts\nof the passage and jointly attend to the identiﬁed parts. In ad-\ndition, PLMs do not understand what situations are in-\nconsistent with human commonsense. For example, they\nmay choose \"leaving a baby alone at home is not safe\" over\n\"she would try to ﬁnd a babysitter\" when asked the question\n\"what would happen if she does not ﬁnd a daycare\". Inter-\nestingly, when one of the answer options is \"None of the\nabove\", PLMs often struggle to choose this option since the\nwords in this option do not provide enough signal for why\nthis option might be correct.\nSecond, PLMs struggle with selecting the most plau-\nsible ending given a context for HELLA SWAG. Since a\ngiven context can have multiple correct endings, determin-\ning which one would be the most plausible requires prior\nreasoning of what humans relate to the most with their com-\nmonsense knowledge. Zellers et al. (2018, 2019b) show that\nwhen more surface cues are eliminated, PLMs are less likely\nto be able to predict the most plausible ending even though\nit might be trivial for humans to do so.\nSocial Reasoning\nSocial reasoning involves modeling the mental states of\nothers and their likely actions to the extent that reasoning\ncan be performed over their behaviors and emotions. A rep-\nresentative benchmark for social reasoning is S OCIAL IQA\n(Sap et al. 2019b), which evaluates commonsense reasoning\nbased on social situations and interactions. Consider the\nfollowing example taken from S OCIAL IQA, in which the\ncorrect answer is boldfaced:\n\"Context\": \"Tracy had accidentally pressed upon Austin\nin the small elevator and it was awkward.\"\n\"Question\" : \"Why did Tracy do this?\"\n\"Choice A\": \"get very close to Austin\";\n\"Choice B\": \"squeeze into the elevator\";\n\"Choice C\": \"get ﬂirty with Austin\"\nSeveral observations can be made. First, BERT ﬁnds ex-\namples of effects (\"what will happen to X?\") and motivation\n(\"why did X do that to Y?\") easier than those that involve\nunderstanding descriptions (\"how would you describe X?\")\n(Sap et al. 2019b). Second, it performs better on examples\nwhere the answer exhibits cues about emotions than those\ninvolving spatial commonsense (Bhagavatula et al. 2020).\nMultimodal Reasoning\nTextual representations are restricted to what can be ex-\npressed through natural language and therefore are unable\nto represent the multi-modal information that humans could\nhave access to or infer by being in a dynamic world, such as\na constant stream of images and a sequence of interactions\nin the physical world. Vision naturally becomes a next step\ntowards enabling learning through joint interaction (Bald-\nwin 1995). However, merely using raw visual images along\nwith their descriptions is by no means sufﬁcient to provide\ngrounded understanding (Marasovi´c et al. 2020). For exam-\nple, inferring the intentions of the entities in images can only\nbe well dealt with if we have some prior information (either\nbehavioral or temporal) to rely on to make justiﬁable infer-\nences. This has led the community to look into approaches\nthat can help provide a tighter integration of linguistic and\nvisual modalities.\nThere are two well-known benchmarks for multimodal\nreasoning. V ISUAL COMMONSENSE REASONING (VCR)\n(Zellers et al. 2019a) seeks to answer cognition-level ques-\ntions from images. Concretely, given an image with a list of\nregions and a question, the goal is to choose the answer to\nthe question out of a set of possible candidates and provide a\nrationale that can explain why the chosen answer is correct.\nAn example can be found in Figure 3. V ISUAL COMMON -\nSENSE GRAPHS (Park et al. 2020) checks how well PLMs\nreason about the dynamic context from a static image and an\nevent. Speciﬁcally, given an image and a textual description\nof an event at present, the task is to generate the rest of the\nvisual commonsense graph that is connected to the event.\nFor example, given an image of a man who is drowning in\nthe river and a textual description of the event, the goal is to\ngenerate a commonsense graph with nodes such as \"the man\nwanted to save himself from drowning\", \"the man is wait-\ning for help\", \"the man senses his own death\", and \"the man\nneeds to swim towards the river bank\". Empirical results re-\nveal that for both benchmarks, models that exploit both vi-\nsual and textual information outperform those that only use\ntextual information. This suggests that visual features help\nmake higher quality commonsense inferences.\nTemporal Reasoning\nTime is an inherent aspect of events. Broadly, temporal rea-\nsoning involves two subtasks. Temporal attribute prediction\ninvolves understanding an event mentioned in text or dia-\nlogue through reasoning with its temporal dimensions such\nas the duration of the event, when the event typically hap-\npens, how long the event is going to be stationary, and how\noften it happens. Temporal relation identiﬁcation involves\nunderstanding how an event is temporally related to other\nevents mentioned in the same text or dialogue (e.g., did an\nevent take place before or after another event?). Temporal\nreasoning is challenging because the timestamp associated\nwith an event and the aforementioned temporal dimensions\n12320\nFigure 3: Learning to reason about dynamic context from a static image (Zellers et al. 2019a).\nmay not be mentioned explicitly and therefore need to be\ninferred.\nTwo commonly-used benchmarks have been developed\nfor temporal reasoning. MC-TACO (Zhou et al. 2019)\nis a question-answering benchmark involving temporal\ncommonsense comprehension. Here is an example:\n\"Context\": The massive ice sheet, called a glacier, caused\nthe features on the land you see today\n\"Question\": When did the glacier start to impact the\nland’s features ?\n\"options\": a) centuries ago; b) hours ago; c) 10 years\nago; d) tens of millions of years ago\nTIMEDIAL (Qin et al. 2021) involves temporal reasoning\nin dialogue. Here is an example:\nA: May we see the wine list please.\nB: Sure. Our special wine today is a 1989 Chardonnay.\nA: I’d like a bottle please.\nB: I’ll need to see your ID please.\nA: Here you go.\nB: Sorry about the inconvenience, you look so young. I\nhad to make sure you are over .\na) 21 years old; b) 30 years old; c) 4 years old; d) 18\nyears old\nIdeally, one can train time-aware PLMs to address these\ntemporal reasoning tasks. An obstacle to the development\nof such PLMs concerns the lack of large-scale KBs that in-\ncorporate the notion of time into the facts that they encode\nover entities and events. For instance, the L OCATION rela-\ntion (i.e., where a person lives) and the E MPLOYMENT re-\nlation (i.e., the company a person is afﬁliated with) are de-\npendent on time, but existing KBs typically fail to encode\nthe time period for which a given relation holds true. Such\ntime-aware KBs should also encode temporal commonsense\nknowledge such as \"if a student attends a university, s/he will\nlikely graduate and work after a few years\".\nGiven the lack of such KBs, time-aware PLMs can only\nbe trained on the annotated training data provided by MC-\nTACO and TIMEDIAL. For instance, Zhou et al. (2020a)\npropose TACO-LM, a BERT-based PLM that is trained to be\ntemporally aware by contextually estimating duration and\ntime via (1) extracting the important events and their tem-\nporal information and then (2) asking the model to predict\nthe masked tokens that talk about some temporal aspect.\nHowever, TACO-LM only provides marginal improvements\nover BERT w.r.t. duration, frequency, and when the event\ntypically takes place. More recently, Qin et al. (2021) have\nshown that ﬁne-tuned PLMs struggle to perform well on TI-\nMEDIAL primarily because they largely fail to understand\nthe context of the given dialogue and instead simply rely on\nshallow cues about the temporal patterns in the context.\nGenerating Commonsense Knowledge\nCommonsense knowledge generation is a critical com-\nponent in building commonsense knowledge resources.\nBroadly, we can divide commonsense knowledge generation\ntasks into two categories, as described below.\nKnowledge Base Completion\nA KB is a collection of relational facts, each of which is\nrepresented as a triple <s;r;o>, where sis the SUBJECT , r\nis the RELATION , and ois the OBJECT . KB completion is the\ntask of automatically inferring missing facts by reasoning\nabout the information already present in the KB.\nTo date, the most successful knowledge generation ap-\nproach with PLMs is arguably Commonsense Transformer\n(COMET) (Bosselut et al. 2019). COMET can be used to\ngenerate ogiven sand rafter being pre-trained on a knowl-\nedge base such as ConceptNet (Speer et al. 2017; Singh et al.\n2002), which represents (mostly taxonomic) commonsense\nknowledge as a graph of concepts (words or phrases) con-\nnected by relations (edge types), or ATOMIC (Sap et al.\n2019a), which is a large-scale KG consisting of textual de-\nscriptions of inferential knowledge (if-then relations).\nConstrained Commonsense Text Generation\nNext, we examine studies on how PLMs can be used to gen-\nerate commonsense text subject to a set of constraints.\nTasks There are three benchmarks commonly used to\nevaluate commonsense generation approaches.\nCOMMONGEN (Lin et al. 2020b): Given a concept set\n(e.g., {dog, frisbee, catch, throw}), the goal is\nto generate a coherent sentence describing an everyday event\nusing all the provided concepts.\nCOMMONSENSE EXPLANATIONS (COS-E) (Rajani\net al. 2019): Given that a model selects an answer (from a\nset of candidates) to a given question, the goal is to generate\nan explanation of why the selected answer is correct. The\nresulting explanation may help us understand the reasoning\nthat a model relies on to arrive at the selected answer.\n12321\n\u000bNLG (Bhagavatula et al. 2020): Given two observation-\ns/events that happen in two different timesteps, the goal is to\ngenerate a valid hypothesis hof what happened between the\nobservations/events.\nChallenges These benchmarks reveal that PLMs, when\nused as commonsense knowledge generators, suffer from\nseveral shortcomings.\n• Poor coherency: The generated sentences do not neces-\nsarily adhere to the human notion of commonsense. For\ninstance, given the concept set {dog, catch, throw, fris-\nbee}, GPT2 generates the sentence \"A dog throws a fris-\nbee at a football player\". Although this sentence is gram-\nmatically correct, it suffers from poor coherency.\n• Insufﬁcient concept coverage: PLMs continue to pro-\nduce sentences that fail to include all concepts from the\nprovided concept set. In the previous example, the con-\ncept \"catch\" was not used to generate the output.\n• Limited reasoning capability: It is not clear what kind\nof reasoning is used by PLMs to arrive at an answer.\nThough a solution to explanation generation could shed\nlight on this question, some studies show that existing\napproaches generate either trivial or noisy explanations,\nproviding little or no evidence of how a PLM arrives at\nthe selected answer (Ji et al. 2020a). Other studies show\nthat the reasoning used by PLMs is often not fully correct\n(McCoy et al. 2019; Shwartz and Choi 2020). Overall,\nthe reasoning capability of PLMs is far from satisfactory.\nImproving Coherency and Concept Coverage Several\napproaches have been proposed to address two of the chal-\nlenges, poor coherency and insufﬁcient concept coverage.\nUsing prototypes. Guu et al. (2018) propose a sentence\ngeneration mechanism that involves selecting a sentence\nfrom the training data (known as a prototype) and editing it\ninto a form that satisﬁes a given set of constraints. Their hy-\npothesis is that it is easier to edit a sentence that is grammati-\ncally correct and semantically coherent than to generate one\nfrom scratch. If provided with a concept set {trailer,\nshirt, side, sit, road} from C OMMONGEN , a\nPLM may generate \"A man sits on the side of a trailer and a\nshirt\", whereas a prototype such as \"Two guys in red shirts\nare sitting on chairs, by the side of the road, behind that open\ntrailer\" may be edited by the PLM to form \"a man in a white\nshirt and black pants sits on the side of a trailer on the road\",\nwhich has better coverage and coherency.\nUsing knowledge graphs (KGs). KGs play a crucial role\nin enabling PLMs to improve the semantic correctness (and\nthus coherency) of text as they can provide PLMs with in-\nformation that may not be captured reliably by PLMs, such\nas entity representations and their dependency relations (i.e.,\nhow concepts are related). For instance, Li et al. (2021) ex-\ntract concept-speciﬁc relations from a KG and inject them\ninto a PLM to make the generated text more coherent.\nReasoning over multi-hop relational paths in KGs. The\nsparse connections between the nodes in KGs may make\nit hard for PLMs to learn rich relations from them. These\nrich relations, however, may be needed by PLMs to gen-\nerate commonsense sentences with rich semantic struc-\neruption Mr. Egg was presenting a\nvolcanic eruption  to the science  class.\nHe has a diagram of a volcano  that\nlooked like it was made of tinfoil.\nHe then took out a huge thing  of\nvinegar and started to pour  it in!\nStory Context\nscience \nvolcano \nthing \npour \nwater \nrock \nearth \nsubstance lava \nRelational Paths\nRelatedT o \nRelatedT o \nRelatedT o \nMadeOf \nAtLocation \nRelatedT o \nIsA \nThe volcano then exploded with\nsubstance  that looked like lava  !\nStory Ending\nThe class had no clue what was going\non and looked on in astonishment.\nConceptNet\nROC Story\nIsA \nRelatedT o \nFigure 4: Using structural relational knowledge for multi-\nhop reasoning (Ji et al. 2020b). Blue nodes correspond to\nconcepts in story, orange nodes correspond to those in story\nending, and green nodes are intermediate concepts that con-\nnect their blue and orange counterparts.\ntures in order to boost coherency. A solution to this prob-\nlem is to perform multi-hop reasoning, which involves rea-\nsoning over multiple edges/relations in a KG. When per-\nforming multi-hop reasoning, models are required to at-\ntend to different parts of a given context to answer a ques-\ntion. Figure 4 shows an example of a task known as story\nending generation, where the goal is to generate the end\nof a story given the story context. In this example, ex-\nternal commonsense knowledge in the form of relational\npaths can guide the generation of the two nodes lava and\nsubstance by providing background knowledge such as\n(volcano, madeOf, lava). Capturing what lava\nand substance that appear in the story ending refer to\nin the story context is non-trivial for PLMs, especially when\nthe story is long. To address this drawback, Ji et al. (2020b)\nperform reasoning over multi-hop relational paths as a way\nto extract structural and semantic knowledge from a KG.\nUsing iterative reﬁnements. When provided with infor-\nmation about past and future events (as in \u000bNLG), humans\ncan easily reason about these events using contextual and\nprior knowledge. This kind of non-monotonic reasoning is\ncrucial to improving generation coherency. However, non-\nmonotonic reasoning is difﬁcult for neural models since the\ngeneration process happens predominantly while condition-\ning on the left context (Welleck et al. 2019). To address this\nproblem, Qin et al. (2020) propose a decoding approach that\ninvolves sampling from the combined output vector repre-\nsentations computed from both forward and backward prop-\nagation. In other words, iterative reﬁnements are made on\nthe generated text through alternating between forward and\nbackward passes, yielding text with improved coherency.\nConcluding Remarks\nDespite recent progresses on using PLMs to address com-\nmonsense knowledge reasoning and generation tasks, many\nof these tasks are far from being solved. We conclude our\ndiscussion with key challenges in this area of research.\n12322\nDataset Model Human Dataset Model Human\nHellaSwag 93.85 95.6 WinoGrande 86.64 94.0\nCosmosQA 91.79 94.0 SocialIQA 83.15 88.1\nPIQA 90.13 94.9 VCR 63.15 85.0\nTable 2: Results of state-of-the-art models and human base-\nlines on widely-used commonsense reasoning benchmarks.\nImproving benchmarks. While state-of-the-art models\nhave achieved near-human performance on many of the\nbenchmarks mentioned in this paper (see Table 2), the rea-\nsoning tasks underlying these benchmarks are still far from\nbeing solved. Consequently, it is not clear what the perfor-\nmance gains on a particular benchmark mean. Bender and\nKoller (2020) point out that acing a benchmark has led us\nto over-estimate the capability of PLMs, which in turn has\ngiven rise to misleading deﬁnitions of \"understanding\". It\nis therefore important to re-think what is being learned by\nPLMs and how benchmarks can be made more representa-\ntive such that performance gains on them translate to mean-\ningful progress towards the bigger goal of \"understanding\".\nReducing biases. Biases in benchmarks such as pre-\ndictable question structures, annotation artifacts, and lexical\noverlap provide easy shortcuts for PLMs to arrive at correct\nanswers without involving reasoning. To mitigate biases,\nresearchers have used adversarial ﬁltering wherein easily\nsolvable options are replaced iteratively by new ones until\nthe discriminator misclassiﬁes it (Zellers et al. 2018; Mc-\nCoy et al. 2019; Bras et al. 2020). To robustify data, several\nworkarounds have been proposed that revolve around reduc-\ning lexical overlap, creating complex reasoning questions\nthat require additional context, and employing adversarial\napproaches with newer models (Gardner et al. 2019). Bias\nreduction in benchmarks remains an active research area.\nExploring new components of commonsense knowledge.\nThese are numerous components of commonsense knowl-\nedge that are partially understood and not covered by the\npresent research. One primary reason for this is that we\ndo not have a comprehensive understanding of how hu-\nmans learn. A concrete example can be derived from Kahne-\nman’s (2011) cognitive system of intuition. There is no clear\nway of representing a human’s mental and emotional states\nthat can be readily used by our algorithms. Modeling multi-\nple mental states with natural language is a highly non-trivial\nprocess (Sap et al. 2020).\nAddressing the reporting bias. Much commonsense\nknowledge is assumed rather than mentioned explicitly in\ntext (Grice 1975; Van Durme 2010; Gordon and Van Durme\n2013). This results in what is known as the \"reporting bias\",\nwhich, when combined with the scarcity of training data for\nmany NLP tasks, makes it hard for PLMs to receive appro-\npriate signal about a particular concept (Zhao et al. 2020).\nThis could lead to over-generalization of associations and\nampliﬁcation of biases (Shwartz and Choi 2020). How to\naddress the reporting bias remains an open question.\nImproving existing KGs. While many approaches rely on\nKGs to obtain rich contextual knowledge, existing KGs have\nat least two key weaknesses that can potentially limit their\nusefulness for commonsense reasoning. The ﬁrst is sparsity:\nmany concepts and relations are missing in existing KGs (Li\net al. 2016). This sparsity problem in turn limits the amount\nof knowledge that can be extracted from KGs for com-\nmonsense reasoning (Zhao et al. 2020). The second is non-\ncontextualization: ﬁnding the nodes that are most relevant to\na query is difﬁcult, particularly by propagation-based algo-\nrithms, because many of them are non-contextual in nature\n(Fadnis et al. 2019). To address the non-contextualization\nproblem, Malaviya et al. (2020) have attempted to use the\nstructural and semantic connections of the nodes in a KG to\nobtain contextual information. The resulting contextual in-\nformation can then be explicitly encoded in a KG by creat-\ning additional nodes, which alleviates the sparsity problem.\nHow to densify KGs and contextualize their nodes is an on-\ngoing research topic (Wang et al. 2020).\nHarnessing commonsense knowledge from different\nmodalities. There are many instances wherein the visual\nmodality is required along with text to make sense of a\nparticular situation (Park et al. 2020). While humans learn\ncommonsense knowledge predominantly through percep-\ntion and interaction with the physical world, neural mod-\nels are primarily trained on text data. Harnessing common-\nsense knowledge from different modalities can potentially\ntake these models to the next level of performance.\nTowards multilinguality. An important but underex-\nplored direction is multi-lingual commonsense reasoning\nand generation. Studies have shown that the performance of\ncross-lingual PLMs is poor when evaluated on non-English\ncommonsense reasoning benchmarks (Lin et al. 2021b).\nThese models perform poorly when evaluated on a test set\nthat was translated to English, leading to staggering transfer\nreasoning capabilities to other languages and restricting the\nresearch scope to only certain languages (Ponti et al. 2020).\nReferences\nBaldwin, D. A. 1995. Understanding the Link between Joint\nAttention and Language. Joint Attention: Its Origins and\nRole in Development, 131–158.\nBender, E. M.; and Koller, A. 2020. Climbing towards NLU:\nOn Meaning, Form, and Understanding in the Age of Data.\nIn ACL.\nBhagavatula, C.; Bras, R. L.; Malaviya, C.; Sakaguchi, K.;\nHoltzman, A.; Rashkin, H.; Downey, D.; Yih, W.-T.; and\nChoi, Y . 2020. Abductive Commonsense Reasoning. In\nICLR.\nBisk, Y .; Zellers, R.; Bras, R. L.; Gao, J.; and Choi, Y . 2020.\nPIQA: Reasoning about Physical Commonsense in Natural\nLanguage. In AAAI.\nBosselut, A.; Rashkin, H.; Sap, M.; Malaviya, C.; Çelikyil-\nmaz, A.; and Choi, Y . 2019. COMET: Commonsense Trans-\nformers for Automatic Knowledge Graph Construction. In\nACL.\nBras, R. L.; Swayamdipta, S.; Bhagavatula, C.; Zellers, R.;\nPeters, M. E.; Sabharwal, A.; and Choi, Y . 2020. Adversarial\nFilters of Dataset Biases. In ICML.\n12323\nChen, M.; D’Arcy, M.; Liu, A.; Fernandez, J.; and Downey,\nD. 2019. CODAH: An Adversarially-Authored Question\nAnswering Dataset for Common Sense. In 3rd Workshop\non Evaluating Vector Space Representations for NLP.\nClark, K.; Luong, M.-T.; Le, Q. V .; and Manning, C. D.\n2019. ELECTRA: Pre-training Text Encoders as Discrim-\ninators Rather Than Generators. In ICLR.\nDavison, J.; Feldman, J.; and Rush, A. 2019. Commonsense\nKnowledge Mining from Pretrained Models. In EMNLP-\nIJCNLP.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL HLT.\nEttinger, A. 2020. What BERT Is Not: Lessons from a New\nSuite of Psycholinguistic Diagnostics for Language Models.\nTACL, 8: 34–48.\nFadnis, K. P.; Talamadupula, K.; Kapanipathi, P.; Ishfaq, H.;\nRoukos, S.; and Fokoue, A. 2019. Path-Based Contextual-\nization of Knowledge Graphs for Textual Entailment.CoRR,\nabs/1911.02085.\nForbes, M.; Holtzman, A.; and Choi, Y . 2019. Do Neural\nLanguage Representations Learn Physical Commonsense?\nIn CogSci.\nGardner, M.; Berant, J.; Hajishirzi, H.; Talmor, A.; and Min,\nS. 2019. On Making Reading Comprehension More Com-\nprehensive. In 2nd Workshop on Machine Reading for QA.\nGeva, M.; Gupta, A.; and Berant, J. 2020. Injecting Numer-\nical Reasoning Skills into Language Models. In ACL.\nGoel, P.; Feng, S.; and Boyd-Graber, J. 2019. How Pre-\ntrained Word Representations Capture Commonsense Phys-\nical Comparisons. In First Workshop on Commonsense In-\nference in NLP.\nGordon, J.; and Van Durme, B. 2013. Reporting Bias and\nKnowledge Acquisition. In AKBC.\nGrice, H. P. 1975. Logic and Conversation. In Ezcurdia, M.;\nand Stainton, R. J., eds., The Semantics-Pragmatics Bound-\nary in Philosophy, 47.\nGuu, K.; Hashimoto, T. B.; Oren, Y .; and Liang, P. 2018.\nGenerating Sentences by Editing Prototypes. TACL, 6: 437–\n450.\nHan, X.; Zhang, Z.; Ding, N.; Gu, Y .; Liu, X.; Huo, Y .; Qiu,\nJ.; Zhang, L.; Han, W.; Huang, M.; Jin, Q.; Lan, Y .; Liu, Y .;\nLiu, Z.; Lu, Z.; Qiu, X.; Song, R.; Tang, J.; Wen, J.; Yuan,\nJ.; Zhao, W. X.; and Zhu, J. 2021. Pre-Trained Models: Past,\nPresent and Future. CoRR, abs/2106.07139.\nHuang, L.; Le Bras, R.; Bhagavatula, C.; and Choi, Y . 2019.\nCosmos QA: Machine Reading Comprehension with Con-\ntextual Commonsense Reasoning. In EMNLP-IJCNLP.\nHwang, J. D.; Bhagavatula, C.; Bras, R. L.; Da, J.; Sak-\naguchi, K.; Bosselut, A.; and Choi, Y . 2021. COMET-\nATOMIC 2020: On Symbolic and Neural Commonsense\nKnowledge Graphs. In AAAI.\nJi, H.; Ke, P.; Huang, S.; Wei, F.; and Huang, M. 2020a. Gen-\nerating Commonsense Explanation by Extracting Bridge\nConcepts from Reasoning Paths. In AACL-IJCNLP.\nJi, H.; Ke, P.; Huang, S.; Wei, F.; Zhu, X.; and Huang, M.\n2020b. Language Generation with Multi-Hop Reasoning on\nCommonsense Knowledge Graph. In EMNLP.\nKahneman, D. 2011. Thinking, Fast and Slow. Farrar, Straus\nand Giroux, LLC.\nKalyan, K. S.; Rajasekharan, A.; and Sangeetha, S. 2021.\nAMMUS : A Survey of Transformer-based Pretrained Mod-\nels in Natural Language Processing.CoRR, abs/2108.05542.\nKassner, N.; and Schütze, H. 2020. Negated and Misprimed\nProbes for Pretrained Language Models: Birds Can Talk,\nBut Cannot Fly. In ACL.\nKlein, T.; and Nabi, M. 2019. Attention Is (not) All You\nNeed for Commonsense Reasoning. In ACL.\nLenat, D. B. 1995. CYC: A Large-Scale Investment in\nKnowledge Infrastructure. CACM, 38(11): 33–38.\nLevesque, H. J.; Davis, E.; and Morgenstern, L. 2012. The\nWinograd Schema Challenge. In KR&R.\nLewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-\nhamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.\n2020. BART: Denoising Sequence-to-Sequence Pre-training\nfor Natural Language Generation, Translation, and Compre-\nhension. In ACL.\nLi, X.; Taheri, A.; Tu, L.; and Gimpel, K. 2016. Common-\nsense Knowledge Base Completion. In ACL.\nLi, Y .; Goel, P.; Rajendra, V . K.; Singh, H. S.; Francis, J.;\nMa, K.; Nyberg, E.; and Oltramari, A. 2021. Lexically-\nconstrained Text Generation through Commonsense Knowl-\nedge Extraction and Injection. In Workshop on CSKG.\nLin, B. Y .; Lee, S.; Khanna, R.; and Ren, X. 2020a. Birds\nhave four legs?! NumerSense: Probing Numerical Common-\nsense Knowledge of Pre-Trained Language Models. In\nEMNLP.\nLin, B. Y .; Wu, Z.; Yang, Y .; Lee, D.; and Ren, X. 2021a.\nRiddleSense: Reasoning about Riddle Questions Featuring\nLinguistic Creativity and Commonsense Knowledge. In\nFindings of the ACL: ACL-IJCNLP 2021.\nLin, B. Y .; Zhou, W.; Shen, M.; Zhou, P.; Bhagavatula, C.;\nChoi, Y .; and Ren, X. 2020b. CommonGen: A Constrained\nText Generation Challenge for Generative Commonsense\nReasoning. In Findings of the ACL: EMNLP 2020.\nLin, C.; Ouyang, Z.; Zhuang, J.; Chen, J.; Li, H.; and Wu,\nR. 2021b. Improving Code Summarization with Block-wise\nAbstract Syntax Tree Splitting. In ICPC.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. CoRR, abs/1907.11692.\nLogan, R.; Liu, N. F.; Peters, M. E.; Gardner, M.; and Singh,\nS. 2019. Barack’s Wife Hillary: Using Knowledge Graphs\nfor Fact-Aware Language Modeling. In ACL.\nMalaviya, C.; Bhagavatula, C.; Bosselut, A.; and Choi, Y .\n2020. Commonsense Knowledge Base Completion with\nStructural and Semantic Context. In AAAI.\nMarasovi´c, A.; Bhagavatula, C.; Park, J. s.; Le Bras, R.;\nSmith, N. A.; and Choi, Y . 2020. Natural Language Ra-\ntionales with Full-Stack Visual Reasoning: From Pixels to\n12324\nSemantic Frames to Commonsense Graphs. In Findings of\nthe ACL: EMNLP 2020.\nMcCoy, T.; Pavlick, E.; and Linzen, T. 2019. Right for the\nWrong Reasons: Diagnosing Syntactic Heuristics in Natural\nLanguage Inference. In ACL.\nNiu, Y .; Huang, F.; Liang, J.; Chen, W.; Zhu, X.; and Huang,\nM. 2021. A Semantic-based Method for Unsupervised Com-\nmonsense Question Answering. In ACL-IJCNLP.\nPark, J. S.; Bhagavatula, C.; Mottaghi, R.; Farhadi, A.; and\nChoi, Y . 2020. VisualCOMET: Reasoning about the Dy-\nnamic Context of a Still Image. In ECCV.\nPetroni, F.; Rocktäschel, T.; Riedel, S.; Lewis, P.; Bakhtin,\nA.; Wu, Y .; and Miller, A. 2019. Language Models as\nKnowledge Bases? In EMNLP-IJCNLP.\nPonti, E. M.; Glavaš, G.; Majewska, O.; Liu, Q.; Vuli ´c, I.;\nand Korhonen, A. 2020. XCOPA: A Multilingual Dataset\nfor Causal Commonsense Reasoning. In EMNLP.\nQin, L.; Gupta, A.; Upadhyay, S.; He, L.; Choi, Y .; and\nFaruqui, M. 2021. TIMEDIAL: Temporal Commonsense\nReasoning in Dialog. In ACL-IJCNLP.\nQin, L.; Shwartz, V .; West, P.; Bhagavatula, C.; Hwang,\nJ. D.; Le Bras, R.; Bosselut, A.; and Choi, Y . 2020. Back\nto the Future: Unsupervised Backprop-based Decoding for\nCounterfactual and Abductive Commonsense Reasoning. In\nEMNLP.\nQiu, X.; Sun, T.; Xu, Y .; Shao, Y .; Dai, N.; and Huang, X.\n2020. Pre-trained Models for Natural Language Processing:\nA Survey. Science China Technological Sciences, 63.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring\nthe Limits of Transfer Learning with a Uniﬁed Text-to-Text\nTransformer. JMLR, 21(140): 1–67.\nRajani, N. F.; McCann, B.; Xiong, C.; and Socher, R. 2019.\nExplain Yourself! Leveraging Language Models for Com-\nmonsense Reasoning. In ACL.\nRichardson, K.; and Sabharwal, A. 2020. What Does My\nQA Model Know? Devising Controlled Probes Using Expert\nKnowledge. TACL, 8: 572–588.\nRogers, A.; Kovaleva, O.; and Rumshisky, A. 2020. A\nPrimer in BERTology: What We Know About How BERT\nWorks. TACL, 8: 842–866.\nSakaguchi, K.; Bras, R. L.; Bhagavatula, C.; and Choi, Y .\n2020. WINOGRANDE: An Adversarial Winograd Schema\nChallenge at Scale. In AAAI.\nSap, M.; Bras, R. L.; Allaway, E.; Bhagavatula, C.; Lourie,\nN.; Rashkin, H.; Roof, B.; Smith, N. A.; and Choi, Y . 2019a.\nATOMIC: An Atlas of Machine Commonsense for If-Then\nReasoning. In AAAI.\nSap, M.; Rashkin, H.; Chen, D.; Bras, R. L.; and Choi, Y .\n2019b. Social IQA: Commonsense Reasoning about Social\nInteractions. In EMNLP.\nSap, M.; Shwartz, V .; Bosselut, A.; Choi, Y .; and Roth, D.\n2020. Commonsense Reasoning for Natural Language Pro-\ncessing. In ACL.\nShwartz, V .; and Choi, Y . 2020. Do Neural Language Mod-\nels Overcome Reporting Bias? In COLING.\nSingh, P.; Lin, T.; Mueller, E. T.; Lim, G.; PerkinsWan, T.;\nand Zhu, L. 2002. Open Mind Common Sense: Knowledge\nAcquisition from the General Public. In On the Move to\nMeaningful Internet Systems (OTM 2002).\nSpeer, R.; Chin, J.; and Havasi, C. 2017. ConceptNet 5.5: An\nOpen Multilingual Graph of General Knowledge. In AAAI.\nSvyatkovskiy, A.; Zhao, Y .; Fu, S.; and Sundaresan, N. 2019.\nPythia: AI-assisted Code Completion System. In KDD.\nVan Durme, B. 2010. Extracting Implicit Knowledge from\nText. Ph.D. thesis, University of Rochester, Rochester, NY .\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is All You Need. In NIPS.\nWallace, E.; Wang, Y .; Li, S.; Singh, S.; and Gardner, M.\n2019. Do NLP Models Know Numbers? Probing Numeracy\nin Embeddings. In EMNLP-IJCNLP.\nWang, P.; Peng, N.; Ilievski, F.; Szekely, P.; and Ren, X.\n2020. Connecting the Dots: A Knowledgeable Path Gen-\nerator for Commonsense Question Answering. In Findings\nof the ACL: EMNLP 2020.\nWeir, N.; Poliak, A.; and Durme, B. V . 2020. Probing Neural\nLanguage Models for Human Tacit Assumptions. InCogSci.\nWelleck, S.; Brantley, K.; Daumé, H.; and Cho, K. 2019.\nNon-Monotonic Sequential Text Generation. In ICML.\nZang, L.-J.; Cao, C.; Cao, Y .-N.; Wu, Y .-M.; and Cao, C.-G.\n2013. A Survey of Commonsense Knowledge Acquisition.\nJournal of Computer Science and Technology, 28: 689–719.\nZellers, R.; Bisk, Y .; Farhadi, A.; and Choi, Y . 2019a. From\nRecognition to Cognition: Visual Commonsense Reasoning.\nIn CVPR.\nZellers, R.; Bisk, Y .; Schwartz, R.; and Choi, Y . 2018.\nSW AG: A Large-Scale Adversarial Dataset for Grounded\nCommonsense Inference. In EMNLP.\nZellers, R.; Holtzman, A.; Bisk, Y .; Farhadi, A.; and Choi,\nY . 2019b. HellaSwag: Can a Machine Really Finish Your\nSentence? In ACL.\nZellers, R.; Holtzman, A.; Peters, M.; Mottaghi, R.; Kem-\nbhavi, A.; Farhadi, A.; and Choi, Y . 2021. PIGLeT: Lan-\nguage Grounding Through Neuro-Symbolic Interaction in a\n3D World. In ACL.\nZhao, Z.; Papalexakis, E.; and Ma, X. 2020. Learning Phys-\nical Common Sense as Knowledge Graph Completion via\nBERT Data Augmentation and Constrained Tucker Factor-\nization. In EMNLP.\nZhou, B.; Khashabi, D.; Ning, Q.; and Roth, D. 2019. “Go-\ning on a vacation” takes longer than “Going for a walk”:\nA Study of Temporal Commonsense Understanding. In\nEMNLP.\nZhou, B.; Ning, Q.; Khashabi, D.; and Roth, D. 2020a. Tem-\nporal Common Sense Acquisition with Minimal Supervi-\nsion. In ACL.\nZhou, X.; Zhang, Y .; Cui, L.; and Huang, D. 2020b. Eval-\nuating Commonsense in Pre-Trained Language Models. In\nAAAI.\n12325"
}