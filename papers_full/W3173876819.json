{
  "title": "Explaining Contextualization in Language Models using Visual Analytics",
  "url": "https://openalex.org/W3173876819",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2728053043",
      "name": "Rita Sevastjanova",
      "affiliations": [
        "University of Konstanz"
      ]
    },
    {
      "id": "https://openalex.org/A4201635984",
      "name": "Aikaterini-Lida Kalouli",
      "affiliations": [
        "University of Konstanz"
      ]
    },
    {
      "id": "https://openalex.org/A2806070687",
      "name": "Christin Beck",
      "affiliations": [
        "University of Konstanz"
      ]
    },
    {
      "id": "https://openalex.org/A2258608052",
      "name": "Hanna Schäfer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A709640967",
      "name": "Mennatallah El-Assady",
      "affiliations": [
        "University of Konstanz"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2951689902",
    "https://openalex.org/W2979949198",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W3102568136",
    "https://openalex.org/W2798819017",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W4288086191",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2963214037",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2798665661",
    "https://openalex.org/W2889326414",
    "https://openalex.org/W3015766957",
    "https://openalex.org/W3093452197",
    "https://openalex.org/W580169569",
    "https://openalex.org/W2996005046",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3134522972",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2889883176",
    "https://openalex.org/W3038035611",
    "https://openalex.org/W3097861677",
    "https://openalex.org/W3021934057",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W4287996040",
    "https://openalex.org/W2969954596",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2997789497",
    "https://openalex.org/W2990036165",
    "https://openalex.org/W2251305031",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2054487890",
    "https://openalex.org/W2995755016",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2751627669",
    "https://openalex.org/W2869198903",
    "https://openalex.org/W2963394326",
    "https://openalex.org/W2963372062",
    "https://openalex.org/W2896667998",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2120292361",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W3127507520",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2553981914",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W2963123635",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W1976234022"
  ],
  "abstract": "Despite the success of contextualized language models on various NLP tasks, it is still unclear what these models really learn. In this paper, we contribute to the current efforts of explaining such models by exploring the continuum between function and content words with respect to contextualization in BERT, based on linguistically-informed insights. In particular, we utilize scoring and visual analytics techniques: we use an existing similarity-based score to measure contextualization and integrate it into a novel visual analytics technique, presenting the model's layers simultaneously and highlighting intra-layer properties and inter-layer differences. We show that contextualization is neither driven by polysemy nor by pure context variation. We also provide insights on why BERT fails to model words in the middle of the functionality continuum.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 464–476\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n464\nExplaining Contextualization in Language Models using Visual Analytics\nRita Sevastjanova∗and Aikaterini-Lida Kalouli†and Christin Beck†and\nHanna Sch¨afer∗and Mennatallah El-Assady∗\nUniversity of Konstanz\nfirstname.lastname@uni-konstanz.de\nAbstract\nDespite the success of contextualized language\nmodels on various NLP tasks, it is still unclear\nwhat these models really learn. In this paper,\nwe contribute to the current efforts of explain-\ning such models by exploring the continuum\nbetween function and content words with re-\nspect to contextualization in BERT, based on\nlinguistically-informed insights. In particular,\nwe utilize scoring and visual analytics tech-\nniques: we use an existing similarity-based\nscore to measure contextualization and inte-\ngrate it into a novel visual analytics tech-\nnique, presenting the model’s layers simulta-\nneously and highlighting intra-layer properties\nand inter-layer differences. We show that con-\ntextualization is neither driven by polysemy\nnor by pure context variation. We also provide\ninsights on why BERT fails to model words in\nthe middle of the functionality continuum.\n1 Introduction\nThe rise of contextualized language models (LM),\ni.e., contextualized word and sentence represen-\ntations, such as ELMO (Peters et al., 2018) and\nBERT (Devlin et al., 2019), has brought many well-\nknown NLP tasks to a tremendous breakthrough.\nContextualized embeddings have replaced earlier\nstatic embeddings (Mikolov et al., 2013; Penning-\nton et al., 2014; Conneau et al., 2017), creating\nnew standards for the state-of-the-art. LMs have\nlearned highly transferable and task-agnostic prop-\nerties of language (e.g., Belinkov, 2018; Conneau\net al., 2018; Peters et al., 2018), even to a degree\nof imitating the classical NLP pipeline (Tenney\net al., 2019a). Despite these research efforts, it\nremains yet unclear as to what extent LMs like\nBERT capture complex linguistic phenomena and\nwhether different linguistic properties are learned\n∗Contribution to the visualization part.\n†Equal contribution to the computational linguistics part.\nacross the different layers of the model’s archi-\ntecture: the existing evidence is conﬂicting and\nin some cases even contradictory (Rogers et al.,\n2020). One recent line of work (Ethayarajh, 2019)\nexplores the actual contextualization captured in\nthese models, i.e., the degree to which a word is\nmodeled as context-speciﬁc. This sheds light on the\ncontext-speciﬁcity of individual words and the de-\ngree of contextualization of different word groups.\nThis paper contributes to this line of work by ex-\namining the degree of contextualization of function\nvs. content words. We treat functionality as a con-\ntinuum, comparing and contrasting BERT’s (De-\nvlin et al., 2019) modeling of categories of words\nwithin this continuum with the expected modeling\naccording to the theoretical linguistic literature. It\nhas been repeatedly shown that LMs fail to gener-\nalize and capture the compositionality of language\nbecause they struggle with words of high function-\nality, e.g., quantiﬁers, prepositions, modals, con-\njunctions (Dasgupta et al., 2018; Naik et al., 2018;\nMcCoy et al., 2019, to name only a few). Thus, our\nlinguistically-informed analysis sheds light on the\npeculiarities of these phenomena and contributes\nto our better understanding of BERT.\nThis paper utilizes the self-similarity contextual-\nization score of Ethayarajh (2019) for better compa-\nrability. The exploration of the scores and phenom-\nena is enabled by LMExplorer, a visual analytics\n(V A) technique for the layer-wise explanation of\ncontextualized word embeddings. LMExplorer con-\ntributes a new perspective on the learned patterns\nof the model, and shows clusters and score devel-\nopments in the model’s layers simultaneously.\nOverall, the contribution of this paper is two-\nfold: (1) we generate insights as to how BERT cap-\ntures function vs. content words (Sections 4 and 5),\nand (2) present a novel visual analytics technique\nthat facilitates such insights by explaining LMs\nthrough contextualization scoring (Section 3).\n465\n2 Interpretability of Language Models\nResearch on the interpretability of LMs has been\npursued in two main directions, mainly focusing\non BERT. For one, probing tasks are used to in-\nvestigate the linguistic properties learned by the\nLM by training a linear model on the basis of the\ncorresponding contextualized embeddings for the\nprediction of speciﬁc linguistic properties. For an-\nother, the interpretability of LMs has been explored\nvia adversarial datasets to assess the performance\nof an LM with respect to challenging linguistic phe-\nnomena. To further explore the interpretablity of\nLMs, we see work coming from the ﬁeld of V A as\npromising. V A techniques have been used exten-\nsively for exploring and interpreting different deep\nlearning models (Hohman et al., 2019), incl. LMs.\nProbing – Probing experiments have shown that\nBERT’s transformer architecture encodes seman-\ntic information such as word senses and seman-\ntic roles (Reif et al., 2019; Tenney et al., 2019b;\nEttinger, 2020; Zhao et al., 2020), syntactic in-\nformation in the form of constituents and hier-\narchical structure (Goldberg, 2019; Hewitt and\nManning, 2019; Warstadt and Bowman, 2020; Chi\net al., 2020), morphosyntactic and morphological\nfeatures (Edmiston, 2020; Tenney et al., 2019b),\nand discourse-related information necessary for\ntasks such as coreference resolution (Tenney et al.,\n2019b). Moreover, the traditional NLP pipeline\nsequence of POS tagging, syntactic parsing, named\nentity recognition, semantic role labeling and coref-\nerence resolution can be mapped onto BERT’s\ntransformer layers from lower to higher (Tenney\net al., 2019a). Accordingly, several probing studies\nhave shown that BERT captures a hierarchy of lin-\nguistic information (e.g., Jawahar et al., 2019; Lin\net al., 2019; Edmiston, 2020): surface features are\nrepresented best in the lower layers, while syntactic\nfeatures are captured best in the middle layers. The\nmiddle to higher layers represent morphological\nfeatures best, and semantic information is captured\nbest in the higher layers.\nAdversarial Testing – Adversarial testing has\nshown that LMs struggle in making generalizations\non basic lexical relations (Glockner et al., 2018),\nidentifying ungrammaticality (Marvin and Linzen,\n2018), efﬁciently capturing challenging linguis-\ntic phenomena, such as negation (Dasgupta et al.,\n2018; Richardson et al., 2020), modals, quantiﬁers\nand monotonicity (Richardson et al., 2020), pas-\nsives (Zhu et al., 2018), conditionals (Richardson\net al., 2020), conjunctions (McCoy et al., 2019),\nimplicatives and factives (McCoy et al., 2019), and\nmodeling human reasoning patterns, such as nu-\nmerical or common-sense reasoning (Naik et al.,\n2018). Overall, the evidence from adversarial test-\ning contradicts the results of the probing studies:\nif the LM indeed is able to acquire ‘deep’ linguis-\ntic knowledge (e.g., about syntactic hierarchies), it\nshould be able to deal with the phenomena present\nin the adversarial test sets.\nContextualization – Despite the conﬂicting ev-\nidence about the linguistic capacities of LMs like\nBERT, it is widely acknowledged that the word\nembeddings generated by such models are contex-\ntualized, i.e., there is no ﬁnite number of word\nsense representations and a word has different vec-\ntor representations across different contexts. Par-\nticularly, by assessing a word’s contextualization\non the basis of self-similarity scores, Ethayarajh\n(2019) shows that the embeddings become more\ncontextualized, i.e., more context-speciﬁc, in the\nupper layers of BERT. Moreover, it has been shown\nthat contextualized embeddings generally cluster\nwith one another with respect to word senses (Reif\net al., 2019; Wiedemann et al., 2019).\nVisual LM Explanations – Approaches for vi-\nsual LM explanations can be grouped into two\nmain categories. One strand of research focuses\non transformer-based LMs and explains how they\nlearn through visualizing attentions (e.g., NL-\nIZE (Liu et al., 2018), Seq2Seq-Vis (Strobelt et al.,\n2018), BertViz (Vig, 2019), exBERT (Hoover et al.,\n2020), SANVis (Park et al., 2019), and Attention\nFlows (DeRose et al., 2021)). Another strand of\nresearch explains what the model learns by visual-\nizing word embeddings. Although most existing\nwork on embedding explanation is based on prob-\ning tasks, visualization of embedding characteris-\ntics has emerged as an active research topic. The\nﬁrst tools were related to the exploration of static\nembeddings, e.g., by Liu et al. (2017), who visual-\nize word2vec and Glove embeddings, focusing on\nanalogy exploration. Heimerl and Gleicher (2018)\nexplain the same models and present visualizations\nthat support analysis of multiple tasks, among oth-\ners, the analysis of local word neighborhoods. Also,\nBoggust et al. (2019) explain static embeddings\nof word2vec, Glove, and fastText. Their expla-\nnations focus on local neighborhoods visualized\nusing small multiples by applying a dimensionality\nreduction. Berger (2020) has recently presented a\n466\nFigure 1: The main visualization of our technique uses layer-wise interlinked-projections that show embeddings\nfrom the layers of an LM in a 2D space; here, BERT’s 12 layers. The words in each layer are depicted as points\nand connected to their corresponding position throughout layers. By selectively mapping the colors of the links to\ncomputed scores or identiﬁed clusters, this visualization provides a global overview of the analyzed corpus.\nvisual approach for exploring correlations between\nembedding clusters in BERT for a single model’s\nlayer at a time. The novelty of our approach is the\nexplanation of contextualized word embeddings\nthrough contextualization scores that are visualized\nfor all of the model’s layers simultaneously.\n3 LMExplorer: Visual Analytics Technique\nTo support the analysis of word contextualization\nwithin the functionality continuum, we have devel-\noped a V A technique calledLMExplorer. This tech-\nnique discloses layer-wise spatial and score-based\npatterns in the learned embedding representations.\nUsing interlinked embedding projections, we show\nthe spatial relations of the high-dimensional em-\nbedding space. To provide further insights into the\nword contextualization, the technique utilizes scor-\ning functions (i.e., word self-similarity) as a con-\ntextualization explanation. The scores are used to\nexplore and navigate the embedding space, which\nis facilitated by supporting views and interactions.\nThe technique is integrated into thelingvis.io frame-\nwork (El-Assady et al., 2019a).\nTask Analysis – The technique is designed to\nsupport model analysts in gaining insights into\nthe word contextualization. The proposed design\nis informed by a set of tasks that were obtained\nthrough investigating the analysts in their typical\nanalysis workﬂow. These are: (T1) Analyze spa-\ntial structure of the embedding space; (T2) Gain a\nglobal overview of the corpus;(T3) Conduct interac-\ntive pattern analysis; (T4) Create user-deﬁned word\ngroupings for detailed inspection; and(T5) Conduct\na focused analysis of contextualization.\n3.1 Layer-wise Interlinked Projections\nThe main visual components of our technique are\nlayer-wise interlinked projections (Figure 1) – a\nnovel visualization displaying layers of the LM si-\nmultaneously for effective spatial pattern analysis.\nMotivation – The design of this visualization\nwas informed by T1 and T2, i.e., corpus level ex-\nploration of embedding spatial patterns in differ-\nent layers of the LM. Projection-based visualiza-\ntions are the most common methods to visualize\nword embeddings (e.g., Smilkov et al., 2016; Liu\net al., 2017; van Aken et al., 2019; Aken et al.,\n2020) and although some approaches have enabled\nthe exploration of embeddings in different layers\n(e.g., Smilkov et al., 2016; van Aken et al., 2019;\nAken et al., 2020), they typically visualize only\none layer of the LM at a time. However, changes\nin embedding positions and their neighborhoods\nacross layers can be an indicator of the model cap-\nturing new context information. To support such\nanalyses, our technique displays the embeddings\nfor all layers of the LM simultaneously and visually\nhighlights changes in their neighborhoods.\nDesign Rationale – To implement the explo-\nration of such spatial patterns, we use a dimension-\nality reduction technique on the computed embed-\nding vectors from each layer of the LM. In partic-\nular, we reduce the 768-dimensional embedding\nvectors to two dimensions, used as x and y coordi-\nnates to visualize words in one layer. Using this\ntechnique, words with similar embeddings are rep-\nresented by similar coordinates in the 2D space. In\ntotal, 12 projections are created, each representing\none layer of the BERT-base model. The projections\n467\nare ordered vertically underneath each other, start-\ning from layer one at the very top and ending with\nthe last layer at the bottom. The words in the pro-\njection are visualized as shapes. By default, they\nare displayed as circles and colored according to\nthe word’s position in the 2D space, cf., El-Assady\net al. (2019b). After displaying the projections, we\nadd connecting lines between layers to support the\nanalysis of word position changes in the visualized\nspace. To reduce the number of crossing edges, we\nadditionally apply an edge-bundling technique that\ncombines neighboring edges in a more coherent\nrepresentation. An example of the visualization\nis shown in Figure 1. In our approach, both con-\ntextualized word embeddings and aggregated word\nembeddings (i.e., average or median embedding of\nall contexts of a word) can be visualized.\nThe words in each projection (i.e., layer) are rep-\nresented by different embedding vectors. Hence,\nalthough we visualize the same words, the consec-\nutive projections differ and may even get rotated\nor ﬂipped due to artefacts that are common for\nmost of the dimensionality reduction techniques\n(e.g., UMAP (McInnes et al., 2018),t-SNE (Van der\nMaaten and Hinton, 2008)). Even if words main-\ntained their neighborhoods, the rotation of the\nprojections would prevent the users from easily\ncomprehending on embedding positional changes.\nThus, to prevent such artifacts, we apply an exten-\nsion of UMAP called AlignedUMAP. It reduces\nthe rotation artifacts by using the already projected\ndata as an anchoring. Hence, we project the em-\nbeddings from layer 2 by specifying relations to the\nprojection of embeddings from layer 1, and iterate\nthis alignment process up to the last layer.\nThis spatialization concept enables an effective\nlayer comparison as well as the detection of word\ngroups with similar spatial patterns (T1, T2). The\ninterlinked projections beneﬁt the analysis of word\nfunctionality across layers, especially in the ex-\nploratory phase of the analysis. The user can brush\nneighboring words in the projection to gain an\noverview of word groups that are relevant to ob-\nserve in detail. To support hypothesis generation\nand testing, we provide multiple interaction tech-\nniques that help explore the analyzed corpus. When\nhovering over a word in the projection, the word\nand its path through the different layers gets high-\nlighted (T3) and its contexts are displayed for close-\nreading. To ease the analysis of words with com-\nmon spatial patterns, the user can brush a group of\nneighboring words in the projection and drag them\naside. This reduces the displayed information and\nsupports a more detailed pattern analysis (T4).\n3.2 Explaining Contextualization\nWe employ common approaches in explaining con-\ntextualization and compute multiple word-level\ncontextualization scores. These are integrated into\nthe interlinked-projection view as an overlay (T5).\nScoring Functions – To explain the contextu-\nalization of a word’s representation, Ethayarajh\n(2019) introduces three metrics: self-similarity,\nmaximum explainable variance, and intra-sentence\nsimilarity. In this paper, we focus on the word self-\nsimilarity, which Ethayarajh describes as “the aver-\nage cosine similarity of a word with itself across all\nthe contexts in which it appears, where representa-\ntions of the word are drawn from the same layer of\na given model.” Although the analysis in this paper\nis solely based on the self-similarity score, the tech-\nnique can be effortlessly extended to further expla-\nnation scores. For instance, we have explored the\nword’s contextualization also by deﬁning a base-\nline embedding and obtaining its similarity to the\ncontextualized one. It is possible to create multiple\nbaselines by either reducing the context size (e.g.,\nextracting embedding from a word without a sur-\nrounding context) or selecting a speciﬁc layer of the\nLM for reference. Ethayarajh (2019) describes the\n0th layer as an appropriate baseline. However, for\nspeciﬁc hypothesis testing, one could even select\none of the upper layers as a reference layer.\nScore Overlay – The scores are mapped to the\nwords in the interlinked-projection view to provide\nfurther insights into the embedding contextualiza-\ntion. In particular, we use three visual design ele-\nments: (a) color, (b) shape, and (c) size. First, we\nuse a diverging color scale that maps the scores\nfrom brown (min value) to green (max value)\ncolors. Second, we highlight words having ex-\ntreme values (i.e., one standard deviation above the\nmin value and below the max value of the score’s\ndistribution in the particular layer) by displaying\nthem as rectangles instead of the default circles.\nThird, we map the score’s rangeacross all layers\nof the model to the shape’s size, supporting layer\ncomparison (shown in Figure 4).\n3.3 Supporting Visualizations & Interactions\nTo support the exploration of words with common\ncharacteristics (e.g., spatial patterns), we provide\nsupporting visualizations and interactions.\n468\n(a) The range of contextualization scores for all words in different layers are\ndisplayed in distribution plots, supporting layer comparison.\n(b) Words can be ﬁltered and highlighted in\nthe projection by specifying a score’s range.\nFigure 2: The distribution plots show that the average self-similarity of words decreases and, hence, word contex-\ntualization increases with increasing layers of BERT, which replicates the ﬁndings by Ethayarajh (2019).\nThe distribution plots provide an overview of\nthe embedding contextualization scores (i.e., self-\nsimilarity) and are placed next to the corresponding\nlayer projection. They enable the analysis of score\nchanges through the model’s layers. As shown\nin Figure 2a, the self-similarity score decreases in\nupper layers, and the standard deviation increases\naccordingly. The distribution plots can be further\nused for ﬁltering words by specifying a range in\nthe contextualization score (shown in Figure 2b).\nWords that ﬁt within the range are highlighted in\nthe interlinked-projection view.\nFor tailored score-pattern analysis, we display\nthe score changes in an additional, more compact\nmatrix plot visualization (shown in Figure 3). The\ncolumns of the matrix represent words in the cor-\npus, and rows show the layer-wise contextualiza-\ntion scores. The user can deﬁne a query by select-\ning a word in the matrix plot and the words with\nsimilar patterns (i.e., the response of the query)\nare highlighted in the interlinked-projection view.\nTo obtain similar patterns, we ﬁrst represent each\nword by a vector of 12 score values corresponding\nto each layer for BERT-base. We then compute the\ncosine similarity on these vectors to retrieve words\nwith similar score patterns.\nFigure 3: The matrix plot gives an overview of theself-\nsimilarity score changing over layers. By clicking on a\ncolumn, the matrix is queried for similar score-patterns.\n4 Exploring Contextualization in BERT\nWhile Ethayarajh (2019) initially found that the\nincrease in contextualization across the different\nBERT layers (i.e., the decreasing self-similarity)\nseems to be driven by polysemy, ‘stopwords’ such\nas and, of, the and to seem to contradict this con-\nclusion. Stopwords, which in essence are function\nwords, also become increasingly contextualized in\nthe upper layers. Thus, contextualization seems not\nto be entirely driven by polysemy, but rather the\nvariety of contexts a word appears in (Ethayarajh,\n2019). However, function words are not a homoge-\nneous class, and some function words indeed have\nsemantic content in addition to having a grammati-\ncal function. Thus, we decided to investigate func-\ntion and content words in more detail, using the\nLMExplorer to explore contextualization in BERT\nwith respect to the functionality continuum.\n4.1 Functional and Content Words\nIn theoretical linguistics, there is a traditional dis-\ntinction between function and content words. Sev-\neral criteria have been proposed to distinguish be-\ntween the two groups, e.g., semantic content, mem-\nbership openness, ﬂexibility of syntactic attach-\nment, separability from complements (Corver and\nvan Riemsdijk, 2001). While content words com-\nprise a speciﬁc semantic content and contribute\nto the principal meaning of a sentence, function\nwords are rather ‘non-conceptual’ and mainly ful-\nﬁll some grammatical function (e.g., expressing\nmodality or deﬁniteness), gluing content words\ntogether. Furthermore, content words are open-\nclass because new members can freely be added.\nIn contrast, function words are closed-class, i.e.,\nthey are members of a ﬁxed set. Additionally, con-\ntent words are ﬂexible with respect to the syntactic\nphrase they attach to, e.g., the verb think can be\ncomplemented by an NP or a clause, while function\nwords typically only combine with a speciﬁc syn-\ntactic phrase, e.g., a determiner with an NP. Also,\n469\nFigure 4: Exploring BERT’s layer 10 allows us to draw insights about function and content words (Section 5).\nin contrast to content words, function words are\ngenerally inseparable from their content word com-\nplements, i.e., they cannot be detached from their\nlexical heads, e.g., in in the house, the functional in\ncannot be separated from the content word house.\nDespite these ‘hard’ criteria, the two categories are\nnot rigid. Function and content words form a quasi-\ncontinuum (‘squishiness’), a gradience between the\ntwo categories (Ross, 1972; Emonds, 1985). This\ncontinuum is based on the fact that some words\nshare properties of both categories. Such words\ncan be placed on a sliding scale of functionality.\nFor example, prepositions are less functional than\narticles, e.g., some prepositions are associated with\na locative or directional meaning, but they are also\nmore functional than nouns or verbs, e.g., because\nthey are inseparable from their content words.\nWithin computational linguistics and especially\nNLP, this functionality continuum has not received\nmuch attention. Prototypically functional words\nare mostly treated as stopwords and often re-\nmoved from the analysis. Nevertheless, a more\nlinguistically-motivated look in this continuum can\ncontribute to the explainability of LMs like BERT.\n4.2 Visual Analysis\nUtilizing the LMExplorer, we visualize a random\nsubset of 800 unique sentences of the RTE-1 (Da-\ngan et al., 2005), RTE-2 (Bar-Haim et al., 2006)\nand RTE-3 (Giampiccolo et al., 2007) corpora.\nThese corpora contain sentence pairs originally in-\ntended for Natural Language Inference. They stem\nfrom the news domain and thus contain variable\ncontent. The pairs are split into single sentences\nand mapped to their POS tags based on the Stanford\nPOS tagger (Toutanova et al., 2003). We visualize\nthe BERT-base embeddings andself-similarity of\n496 unique words with a frequency greater than 5\nand lower than 50, following Ethayarajh (2019).\nThe distribution plots show at-a-glance that each\nof the distributions roughly follows that of a nor-\nmal distribution and that the mean self-similarity\ndecreases across layers while the standard devia-\ntion increases, see Figure 2a. This observation is in\nline with the ﬁnding by Ethayarajh (2019) that con-\ntextualized word representations are more context-\nspeciﬁc in higher layers, i.e., the self-similarity de-\ncreases overall. Moreover, we ﬁnd speciﬁc spatial\npatterns in the interlinked-projection view, see Fig-\nure 1, i.e., speciﬁc groups of content words, e.g.,\nnamed entities, and speciﬁc groups of function\nwords, e.g., prepositions, seem to cluster together\nacross the layers. By ﬁltering for different score\nranges based on self-similarity via the distribution\nplots, we ﬁrst investigate the three groups min, max\nand mid (one standard deviation around the mean\nstandard deviation; grey area) in more detail. In\naddition, we explore the self-similarity patterns in\nthese areas in the matrix plots.\nScore Areas – Across the layers, mostly named\nentities, e.g., place names ( Israel, Korea, Haiti),\nmonosemous words ( rabies), and polysemous\nwords1, whose senses are closely related (e.g., re-\nsearch, currency, Marijuana), occupy the max area\nacross all layers, see, e.g., layer 10 in Figure 4. In\nthe min area, highly polysemous words, e.g. ﬁeld,\n1The distinction between polysemy and homonymy is con-\ntroversial. We take polysemous words to have multiple senses\nwhich exhibit some kind of semantic relation, e.g., home as\na building/location vs. as a social institution. Homonymous\nwords comprise unrelated senses, e.g., bank as ﬁnancial in-\nstitution vs. as natural object (Utt and Pad ´o, 2011) – of-\nten of different syntactic categories, e.g., present as a gift\n(noun) and as the verb to present. We base our decisions on\nhomonymy/polysemy on WordNet 3.1 (Fellbaum, 1998).\n470\nFigure 5: Layer-wise self-similarity scores for word\nsamples/groups across the functionality continuum.\nhome, and homonymous words, e.g., set, occupy\nthe space in the upper layers (e.g., layer 10, see Fig-\nure 4), and can also be found across the preced-\ning layers. Prepositions (e.g., of, for) occur in the\nmin range from the middle layers onwards. More-\nover, the determiner the occurs in the min range at\nlayer 11 and generally shows a low self-similarity\n(see Figure 3). In the mid range, we ﬁnd temporal\nadverbials, e.g., today and now, modal verbs (must,\nshould) as well as polysemous and monosemous\nwords; see Figure 4. To shed light on these contex-\ntualization patterns, we explore the functionality\ncontinuum in more detail by looking at different\ngroups of words across the layers.\nWord-based Selection – We discern the follow-\ning groups of words for our further explorations:\n1) articles, 2) prepositions, 3) quantiﬁers, 4) modal\nverbs, 5) temporal adverbials, 6) monosemous\nwords, 7) polysemous words and 8) homonymous\nwords. Each group demonstrates a different pattern\nof self-similarity across layers, as shown in Fig-\nure 5. First, we observe that, before (almost) end-\ning up in the min range, the determiners the and a\nstart off in the mid range of the distribution with a\ndecreasing self-similarity across the layers. Prepo-\nsitions such as of, in, on, for, at are found in the\nmid-min area until layer 6 but from then on, they\nare grouped under min. Quantiﬁers like some, all,\nevery remain in the mid range across all layers.\nModal verbs such as must, should, may follow an\ninconsistent pattern: while must and should start\noff in the upper ends of the mid area (max-mid)\nand end up in the mid range from layer 9 on, may\nis at ﬁrst in the min area and after layer 5 in the\nmid range. Temporal adverbials such as yesterday,\nnever, now are also inconsistent. Some of them\n(e.g., yesterday) belong to the max group in the\nlower layers, but slowly move towards the mid\narea as the layers increase – without ever enter-\ning the exact mid area. Others (e.g., now, never)\nare constantly within the mid range, starting at the\nhigher end of mid and moving towards the middle.\nMonosemous words like attorney, river, tsunami\nare mostly found in the max range, with a decreas-\ning tendency across layers, but remain in the upper\nends of the max area. Polysemous words whose\nsenses are very closely related, e.g., universe, state-\nment, are also mostly found in the max area, while\nhighly polysemous words whose senses are loosely\nrelated, e.g., ﬁeld, are located in the min area in\nthe lower layers and although their self-similarity\nincreases, they remain in the min-mid area across\nlayers. Finally, homonymous words, e.g., set, are\nin the min area across layers. These observations\nlead to new insights into how BERT captures con-\ntextualization, see Section 5.\n5 Insights: The Functionality Continuum\nDuring our exploration, we came across patterns\nthat ﬁt to the theory of the functionality continuum\nand others that were contrary to our expectations.\nAbove all, we observed that contextualization is\nneither triggered merely by polysemy nor by vari-\nation in context. To explain the observed patterns,\na) we positioned the deﬁned categories within the\nfunctionality continuum2 based on the inherent lin-\nguistic properties of the words and on insights from\nlexical semantics, and b) we identiﬁed three criteria\nas potential triggers of contextualization, as shown\nin Table 1. The ﬁrst criterion refers to the sense vari-\nation (Sense Var.), i.e., whether a word has multiple\nsenses (high variation), or only one or multiple but\nvery closely related senses (low variation). The sec-\nond criterion captures syntactic context variation\n(SynCtx. Var.), i.e., whether a word needs to be part\nof a speciﬁc syntactic structure (low) or is ﬂexible\nin terms of attachment and can be found in differ-\nent kinds of syntactic structures (high). Another\npotential trigger we identiﬁed is that of variation\nof semantic context (SemCtx. Var.). This captures\nwhether the contexts in which a word can occur\nare semantically similar (low) or different (high)\nto one another. Based on these triggers and pre-\nvious ﬁndings on contextualization by Ethayarajh\n(2019), we derive the expected contextualization\n(Exp. Contextual.) of each of the predeﬁned cate-\ngories. We can then compare this to BERT’s actual\nbehavior (BERT) and shed light on BERT’s abili-\nties to capture the functionality continuum. Note\nthat here the expected contextualization coincides\nwith the SemCtx.Var. for the categories investi-\n2See also semantic proximity continuum by Blank (1997).\n471\nFunctionality Continuum Sense Var. SynCtx. Var. SemCtx. Var. Exp. Contextual. BERT\nhomonymous high high high high high \u0013\npolysemous low/high high low/high low/high low/high \u0013\nmonosemous low high low low low \u0013\ntemp. adverbials low low high high low \u0017\nmodals high low high high low \u0017\nquantiﬁers high low high high low \u0017\nprepositions high low high high high \u0013\narticles none low high high high \u0013\nTable 1: Expected contextualization (Exp. Contextual.) and contextualization in BERT (BERT) on the basis of\nsense variation (Sense Var.), syntactic context variation (SynCtx. Var.) and semantic context variation (SemCtx.\nVar.), ordered based on the functionality continuum, from content (blue, top) to function words (yellow, bottom).\ngated, but might deviate for others. Additionally,\ndifferences between the expected contextualization\nand the SemCtx.Var. might currently be absorbed\nby our binary encoding (low/high). We envision\na more ﬁne-grained Exp. Contextual. measure,\naccounting in detail for the relative positioning of\nwords in the middle of the continuum.\nHomonymy – Homonymous words, being on\nthe ‘more content-like’ end of the continuum, have\na high sense variation due to their multiple (unre-\nlated) senses, a high syntactic variation (ﬂexible\nattachment as content words) and a high semantic\ncontext variation as, due to their multiple senses,\nthey can occur in semantically very different con-\ntexts. This means that we expect a high contex-\ntualization, i.e., the embeddings of homonymous\nwords are highly context-speciﬁc. This is indeed\nconﬁrmed with our ﬁndings since these words gen-\nerally occur in the min area.\nPolysemy – Polysemous words, mostly with\n‘content-like’ properties, exhibit a low/high sense\nvariation, depending on whether they are highly\npolysemous, i.e., have loosely related senses, or\nnot, i.e., have semantically related senses. As it is\ntypical of content words, polysemous words show\nhigh syntactic variation. Concerning their semantic\ncontext variation, they are again in a ‘grey’ area\ndepending on the degree of polysemy: highly pol-\nysemous words mostly appear in semantically dif-\nferent contexts, while plain polysemy is mostly\nfound in semantically similar contexts since the\nsenses are closely related. With this, the expected\ncontextualization is respective to the degree of the\npolysemy. Indeed, BERT meets these expectations:\nhighly polysemous words like ﬁeld, home are in\nthe min area across layers (high contextualization),\nwhile plain polysemous words are rather found in\nthe max area (low contextualization).\nMonosemy – Monosemous words also seem to\nbe correctly captured by BERT. Such words have\nlow sense variation, high syntactic variation (as\ncontent words) and low semantic context variation\n(due to their low sense variation). According to\nthis, they are also expected to have low contex-\ntualization. We ﬁnd this low contextualization in\nBERT as well, where monosemous words have max\nself-similarity across layers.\nTemporal Adverbials – At the middle of the\nfunctionality continuum, temporal adverbials have\na low sense variation, e.g., yesterday has only one\nmeaning,3 as well as low syntactic variation. On\nthe other hand, their semantic context variation is\nhigh because they can occur in semantically very\ndifferent contexts. Thus, the expected contextual-\nization is high, i.e., their embeddings should be\ncontext-speciﬁc to match the semantically different\ncontexts they can appear in. BERT fails to learn\nthis: temporal adverbials are either found within\nthe mid area across all layers or end up in this range\nin the upper layers, contrary to the expected min.\nModals & Quantiﬁers –BERT also struggles in\ncapturing the functionality continuum with modals\nand quantiﬁers. These are comparable to words\nwith high ‘sense’ variation: modals can not only\nhave a deontic or an epistemic ﬂavor, but also ex-\npress variation through their variable quantiﬁca-\ntional force; similarly, quantiﬁers exhibit variation\nvia their variable scope interpretation (wide or nar-\nrow). Both modals and quantiﬁers have low syn-\ntactic variation; they can only attach with speciﬁc\nsyntactic phrases. The contexts they appear in can\nbe semantically very different and thus they have a\nhigh semantic context variation. Based on this half-\nfunctional-half-content nature, modals and quanti-\nﬁers are expected to have high contextualization,\ni.e., have context-speciﬁc embeddings based on the\nmodal ﬂavor they express, the quantiﬁcational force\nthey capture, the scope resolution, etc. However,\nwe can see that BERT fails to meet this expectation.\n3It should be noted that such adverbials have one meaning,\neven if their extension is always a different one due to different\nreference points.\n472\nModals and quantiﬁers mostly occur in the mid\nrange – instead of the expected min.\nPrepositions – At the functional end of the con-\ntinuum, we ﬁnd prepositions and articles. Preposi-\ntions are comparable to words with a high ‘sense’\nvariation, capturing the fact that the same prepo-\nsition can, for example, be locative or temporal,\ndepending on the context. Prepositions have low\nsyntactic variation, as most functional words. Still,\ntheir semantic context variation matches their mul-\ntiple ‘senses.’ Therefore, we expect the preposition\nembeddings to be highly context-speciﬁc: this is\nindeed the case in BERT, where prepositions are\nmostly found in the min area.\nArticles – Last, we investigate articles and par-\nticularly the determiners the and a. We take them\nto have no sense,4 low syntactic variation and high\nsemantic context variation – the contexts they ap-\npear in do not have any semantic similarity in most\ncases. Thus, we expect them to demonstrate high\ncontextualization with highly context-speciﬁc em-\nbeddings. BERT is able to model this through low\nself-similarity, which is more prominent for the\nthan for a, nonetheless consistent for both.\nDiscussion – Summing up, we see that BERT\nstruggles to efﬁciently capture the functionality\ncontinuum. While BERT manages to model the\nends of the continuum, i.e., the mostly content\nand mostly functional words, it fails to create ex-\npressive embeddings for categories with content\nas well as functional properties. This ﬁnding is\nin line with previous literature that has shown that\ncurrent LMs cannot efﬁciently capture hard linguis-\ntic phenomena (e.g., Dasgupta et al. (2018); Mc-\nCoy et al. (2019); Richardson et al. (2020)), with\nmodals, quantiﬁers and temporal reasoning belong-\ning to these phenomena. Our work suggests that the\nBERT embeddings are not speciﬁc enough to cap-\nture the inherent functionality of certain word types,\ni.e., BERT does not learn the relevant generaliza-\ntions. Additionally, we show that contextualiza-\ntion is neither entirely driven by polysemy nor con-\ntext variation. Rather, contextualization can be ex-\nplained via the harmonical combination of function-\nality, sense variation, syntactic variation and seman-\ntic context variation: BERT can efﬁciently model\npolysemy, homonymy and mononymy, i.e., it can\nefﬁciently capture words that appear in semantic\ncontexts of high variation and low variation and\n4We treat determiners as deﬁniteness markers, rather than\nas quantiﬁers or discourse markers, to be in-line with their\ntreatment in popular NLP tasks such as NLI.\nindependently of their polysemy. What it cannot\nmodel are words that have a semi-functional/semi-\ncontent nature (models, quantiﬁers, temporal ad-\nverbials), see Table 1. Concerning models and\nquantiﬁers, BERT cannot learn the inherent func-\ntionality from the context alone and thus treats the\nwords as simple monosemous words. Concern-\ning temporal adverbials, BERT cannot deal with\nthe combination of low sense variation and high\nsemantic context variation – a rather unusual com-\nbination – and is unable to conclude a single word\nmeaning. Although prepositions have the same trig-\ngers as modals and quantiﬁers, BERT follows our\nexpectations with respect to contextualization. This\ncould be due to their higher syntactic ﬂexibility or\ntheir close semantic relatedness with their content\ncomplements, but this needs to be explored as part\nof future work. Overall, BERT seems to follow\nﬁndings of psycholinguistics and language acqui-\nsition: children learn content words easier and ear-\nlier than function words (Bates et al., 1994; Caselli\net al., 1995). Drawing from language acquisition re-\nsearch, we see an opportunity for explainable meth-\nods to inspect BERT’s inner-workings and improve\nits linguistic understanding, raising LMs from their\ninfantile state to a more linguistically-mature one.\n6 Conclusion and Future Work\nThis paper presented new insights on the contex-\ntualization of the functionality continuum, show-\ning that BERT fails to capture the nature of semi-\nfunctional-semi-content words. These insights\nwere generated through a novel visual analytics\ntechnique for contextualized word embedding ex-\nploration and analysis. For a deeper understanding\nof the weaknesses of BERT, our technique can be\nextended with scores that model common linguis-\ntic properties of words and their nearest neighbors,\ne.g., WordNet semantic similarity or POS similarity\nscores. Hence, they could serve as means of expla-\nnation and bring added value to the eXplainable\nArtiﬁcial Intelligence (XAI) research ﬁeld. More\ninformation about the project can be found under:\nhttps://embeddings-explained.lingvis.io.\nAcknowledgments\nWe thank the Deutsche Forschungsgemeinschaft\n(DFG, German Research Foundation) for funding\nwithin project BU 1806/10-2 “Questions Visual-\nized” of the FOR2111 and project D02 “Evalua-\ntion Metrics for Visual Analytics in Linguistics”\n(Project ID: 251654672 – TRR 161).\n473\nBroader Impact Statement\nIn the following, we describe the two main points\nwith respect to the broader impact statement.\nImpact\nWith regard to the broader impact of our work, we\nare going beyond just measuring scores by reveal-\ning and explaining the inner-workings of language\nmodels. We put the measured scores in context\nthrough visual analytics, in combination with prob-\ning and adversarial testing methods, for the explo-\nration, explanation, and analysis. With our work,\nwe aim to open new perspectives on measuring and\nobtaining the model performance, which go beyond\ntypically used performance metrics.\nReproducibility\nWith regard to reproducibility concerns, we would\nlike to note that the contextualization scores calcu-\nlated in this paper rely on the word frequencies and,\nthus, may differ depending on the analyzed corpus.\nFuture work should investigate the exact effect of\nword frequency and account for its impact.\nReferences\nBetty van Aken, Benjamin Winter, Alexander L ¨oser,\nand Felix A. Gers. 2019. How Does BERT Answer\nQuestions? A Layer-Wise Analysis of Transformer\nRepresentations. In Proceedings of the 28th ACM In-\nternational Conference on Information and Knowl-\nedge Management , CIKM ’19, page 1823–1832,\nNew York, NY , USA. Association for Computing\nMachinery.\nBetty van Aken, Benjamin Winter, Alexander L ¨oser,\nand Felix A. Gers. 2020. VisBERT: Hidden-State\nVisualizations for Transformers. In Companion Pro-\nceedings of the Web Conference 2020 , WWW ’20,\npage 207–211, New York, NY , USA. Association for\nComputing Machinery.\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The Second PASCAL Recognis-\ning Textual Entailment Challenge. In Proceedings\nof the Second PASCAL Recognising Textual Entail-\nment Challenge Workshop, pages 1–9, Venice, Italy.\nElizabeth Bates, Virginia Marchman, Donna Thal,\nLarry Fenson, Philip Dale, J. Steven Reznick, Judy\nReilly, and Jeff Hartung. 1994. Developmental and\nstylistic variation in the composition of early vocab-\nulary. Journal of Child Language, 21(1):85–123.\nYonatan Belinkov. 2018. On internal language repre-\nsentations in deep learning: An analysis of machine\ntranslation and speech recognition . Ph.D. thesis,\nMassachusetts Insitute of Technology.\nM. Berger. 2020. Visually Analyzing Contextualized\nEmbeddings. In 2020 IEEE Visualization Confer-\nence (VIS), pages 276–280, Los Alamitos, CA, USA.\nIEEE Computer Society.\nAndreas Blank. 1997. Prinzipien des lexikalischen\nBedeutungswandels am Beispiel der romanischen\nSprachen. Niemeyer, T ¨ubingen.\nAngie Boggust, Brandon Carter, and Arvind Satya-\nnarayan. 2019. Embedding Comparator: Visualiz-\ning Differences in Global Structure and Local Neigh-\nborhoods via Small Multiples. arXiv e-prints, page\narXiv:1912.04853.\nMaria Cristina Caselli, Elizabeth Bates, Paola Casadio,\nJudi Fenson, Larry Fenson, Lisa Sanderl, and Judy\nWeir. 1995. A cross-linguistic study of early lexical\ndevelopment. Cognitive Development, 10(2):159 –\n199.\nEthan A. Chi, John Hewitt, and Christopher D. Man-\nning. 2020. Finding Universal Grammatical Rela-\ntions in Multilingual BERT. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5564–5577, Online. As-\nsociation for Computational Linguistics.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc\nBarrault, and Antoine Bordes. 2017. Supervised\nLearning of Universal Sentence Representations\nfrom Natural Language Inference Data. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing , pages 670–680,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nNorbert Corver and Henk van Riemsdijk. 2001. Semi-\nlexical Categories: The Function of Content Words\nand the Content of Function Words . De Gruyter\nMouton, Berlin, New York.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In Proceedings of the Machine Learning\nChallenges Workshop, pages 177–190, Southamp-\nton, UK. Springer.\nIshita Dasgupta, Demi Guo, Andreas Stuhlm ¨uller,\nSamuel J. Gershman, and Noah D. Goodman. 2018.\nEvaluating Compositionality in Sentence Embed-\ndings. CoRR, abs/1802.04302.\n474\nJoseph F DeRose, Jiayao Wang, and M. Berger. 2021.\nAttention Flows: Analyzing and Comparing Atten-\ntion Mechanisms in Language Models. IEEE Trans-\nactions on Visualization and Computer Graphics ,\n27:1160–1170.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDaniel Edmiston. 2020. A Systematic Analysis of Mor-\nphological Content in BERT Models for Multiple\nLanguages. arXiv preprint arXiv:2004.03032.\nMennatallah El-Assady, Wolfgang Jentner, Fabian\nSperrle, Rita Sevastjanova, Annette Hautli, Miriam\nButt, and Daniel Keim. 2019a. lingvis.io – A Lin-\nguistic Visual Analytics Framework. InProceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics: System Demonstrations,\npages 13–18.\nMennatallah El-Assady, Rebecca Kehlbeck, Christo-\npher Collins, Daniel Keim, and Oliver Deussen.\n2019b. Semantic Concept Spaces: Guided Topic\nModel Reﬁnement using Word-Embedding Projec-\ntions. IEEE transactions on visualization and com-\nputer graphics, 26(1):1001–1011.\nJoseph E. Emonds. 1985. A Uniﬁed Theory of Syntactic\nCategories. De Gruyter Mouton, Berlin, Boston.\nKawin Ethayarajh. 2019. How Contextual are Contex-\ntualized Word Representations? Comparing the Ge-\nometry of BERT, ELMo, and GPT-2 Embeddings.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nAllyson Ettinger. 2020. What BERT Is Not: Lessons\nfrom a New Suite of Psycholinguistic Diagnostics\nfor Language Models. Transactions of the Associa-\ntion for Computational Linguistics, 8:34–48.\nChristiane Fellbaum. 1998. WordNet: An Electronic\nLexical Database (Language, Speech, and Commu-\nnication). The MIT Press, Cambridge, MA, USA.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The Third PASCAL Recog-\nnizing Textual Entailment Challenge. In Proceed-\nings of the Workshop on Textual Entailment and\nParaphrasing, Prague, Czech Republic. Association\nfor Computational Linguistic.\nMax Glockner, Vered Shwartz, and Yoav Goldberg.\n2018. Breaking NLI Systems with Sentences that\nRequire Simple Lexical Inferences. In Proceed-\nings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 650–655. Association for Computa-\ntional Linguistics.\nYoav Goldberg. 2019. Assessing BERT’s Syntactic\nAbilities. arXiv preprint arXiv:1901.05287.\nFlorian Heimerl and Michael Gleicher. 2018. Interac-\ntive Analysis of Word Vector Embeddings. In Com-\nputer Graphics Forum, volume 37, pages 253–265.\nWiley Online Library.\nJohn Hewitt and Christopher D. Manning. 2019. A\nStructural Probe for Finding Syntax in Word Repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nF. Hohman, M. Kahng, R. Pienta, and D. H. Chau. 2019.\nVisual Analytics in Deep Learning: An Interrogative\nSurvey for the Next Frontiers.IEEE Transactions on\nVisualization and Computer Graphics , 25(8):2674–\n2693.\nBenjamin Hoover, Hendrik Strobelt, and Sebastian\nGehrmann. 2020. exBERT: A Visual Analysis Tool\nto Explore Learned Representations in Transformers\nModels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguis-\ntics, System Demonstrations . Association for Com-\nputational Linguistics.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What Does BERT Learn about the Structure\nof Language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen Sesame: Getting inside BERT’s Linguistic\nKnowledge. In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP , pages 241–253, Florence,\nItaly. Association for Computational Linguistics.\nShusen Liu, Peer-Timo Bremer, Jayaraman J Thiagara-\njan, Vivek Srikumar, Bei Wang, Yarden Livnat, and\nValerio Pascucci. 2017. Visual Exploration of Se-\nmantic Relationships in Neural Word Embeddings.\nIEEE Transactions on Visualization and Computer\nGraphics, 24(1):553–562.\nShusen Liu, Zhimin Li, Tao Li, Vivek Srikumar, Vale-\nrio Pascucci, and Peer-Timo Bremer. 2018. NLIZE:\nA Perturbation-Driven Visual Interrogation Tool for\nAnalyzing and Interpreting Natural Language Infer-\nence Models. IEEE Transactions on Visualization\nand Computer Graphics, 25(1):651–660.\n475\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-SNE. Journal of machine\nlearning research, 9(11):2579–2605.\nRebecca Marvin and Tal Linzen. 2018. Targeted Syn-\ntactic Evaluation of Language Models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the Wrong Reasons: Diagnosing Syntac-\ntic Heuristics in Natural Language Inference. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n3428–3448, Florence, Italy. Association for Compu-\ntational Linguistics.\nLeland McInnes, John Healy, Nathaniel Saul, and\nLukas Grossberger. 2018. UMAP: Uniform Mani-\nfold Approximation and Projection. The Journal of\nOpen Source Software, 3(29):861.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient Estimation of Word Represen-\ntations in Vector Space. Proceedings of Workshop at\nICLR.\nAakanksha Naik, Abhilasha Ravichander, Norman\nSadeh, Carolyn Rose, and Graham Neubig. 2018.\nStress Test Evaluation for Natural Language Infer-\nence. In Proceedings of the 27th International Con-\nference on Computational Linguistics , pages 2340–\n2353, Santa Fe, New Mexico, USA. Association for\nComputational Linguistics.\nCheonbok Park, Inyoup Na, Yongjang Jo, Sungbok\nShin, Jaehyo Yoo, Bum Chul Kwon, Jian Zhao,\nHyungjong Noh, Yeonsoo Lee, and Jaegul Choo.\n2019. SANVis: Visual Analytics for Understanding\nSelf-Attention Networks. In 2019 IEEE Visualiza-\ntion Conference (VIS), pages 146–150. IEEE.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global Vectors for Word\nRepresentation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1532–1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and Measuring the Geometry of\nBERT. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d’Alch´e Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n32, pages 8594–8603. Curran Associates, Inc.\nKyle Richardson, Hai Hu, Lawrence S. Moss, and\nAshish Sabharwal. 2020. Probing Natural Language\nInference Models through Semantic Fragments. In\nAssociation for the Advancement of Artiﬁcial Intelli-\ngence (AAAI), pages 8713–8721. AAAI Press.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A Primer in BERTology: What We Know\nAbout How BERT Works. Transactions of the Asso-\nciation for Computational Linguistics, 8:842–866.\nJohn R Ross. 1972. The category squish: Endstation\nhauptwort. In Chicago Linguistic Society, volume 8,\npages 316–328.\nDaniel Smilkov, Nikhil Thorat, Charles Nicholson,\nEmily Reif, Fernanda B. Vi´egas, and Martin Watten-\nberg. 2016. Embedding Projector: Interactive Visu-\nalization and Interpretation of Embeddings. arXiv\ne-prints, page arXiv:1611.05469.\nHendrik Strobelt, Sebastian Gehrmann, Michael\nBehrisch, Adam Perer, Hanspeter Pﬁster, and\nAlexander M Rush. 2018. Seq2seq-vis: A visual\ndebugging tool for sequence-to-sequence models.\nIEEE Transactions on Visualization and Computer\nGraphics, 25(1):353–363.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT Rediscovers the Classical NLP Pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019b. What do you learn from\ncontext? Probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nKristina Toutanova, Dan Klein, Christopher D. Man-\nning, and Yoram Singer. 2003. Feature-Rich Part-\nof-Speech Tagging with a Cyclic Dependency Net-\nwork. In Proceedings of the 2003 Human Language\nTechnology Conference of the North American Chap-\nter of the Association for Computational Linguistics,\npages 252–259.\nJason Utt and Sebastian Pad ´o. 2011. Ontology-based\nDistinction between Polysemy and Homonymy. In\nProceedings of the Ninth International Conference\non Computational Semantics (IWCS 2011).\nJesse Vig. 2019. A Multiscale Visualization of At-\ntention in the Transformer Model. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics: System Demonstrations,\npages 37–42, Florence, Italy. Association for Com-\nputational Linguistics.\n476\nAlex Warstadt and Samuel R. Bowman. 2020. Can\nneural networks acquire a structural bias from raw\nlinguistic data? In Proceedings of the 42nd Annual\nVirtual Meeting of the Cognitive Science Society, On-\nline.\nGregor Wiedemann, Steffen Remus, Avi Chawla, and\nChris Biemann. 2019. Does BERT Make Any\nSense? Interpretable Word Sense Disambiguation\nwith Contextualized Embeddings. In Proceedings\nof KONVENS 2019, Erlangen, Germany.\nMengjie Zhao, Philipp Dufter, Yadollah\nYaghoobzadeh, and Hinrich Sch¨utze. 2020. Quanti-\nfying the Contextualization of Word Representations\nwith Semantic Class Probing. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 1219–1234, Online. Association for\nComputational Linguistics.\nXunjie Zhu, Tingfeng Li, and Gerard de Melo. 2018.\nExploring Semantic Properties of Sentence Embed-\ndings. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 2: Short Papers) , pages 632–637, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.",
  "topic": "Contextualization",
  "concepts": [
    {
      "name": "Contextualization",
      "score": 0.7923386096954346
    },
    {
      "name": "Computer science",
      "score": 0.7083595991134644
    },
    {
      "name": "Computational linguistics",
      "score": 0.537040650844574
    },
    {
      "name": "Natural language processing",
      "score": 0.5013930797576904
    },
    {
      "name": "Analytics",
      "score": 0.455495685338974
    },
    {
      "name": "Visual analytics",
      "score": 0.45258626341819763
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4520932137966156
    },
    {
      "name": "Linguistics",
      "score": 0.45128920674324036
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3922809064388275
    },
    {
      "name": "Cognitive science",
      "score": 0.33497685194015503
    },
    {
      "name": "Data science",
      "score": 0.3055512309074402
    },
    {
      "name": "Programming language",
      "score": 0.2724638879299164
    },
    {
      "name": "Visualization",
      "score": 0.14046424627304077
    },
    {
      "name": "Psychology",
      "score": 0.12141638994216919
    },
    {
      "name": "Philosophy",
      "score": 0.0819857120513916
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I189712700",
      "name": "University of Konstanz",
      "country": "DE"
    }
  ]
}