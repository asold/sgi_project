{
    "title": "Unmasking the Mask – Evaluating Social Biases in Masked Language Models",
    "url": "https://openalex.org/W3156204678",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2000996583",
            "name": "Masahiro Kaneko",
            "affiliations": [
                "Tokyo Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2029493321",
            "name": "Danushka Bollegala",
            "affiliations": [
                "University of Liverpool"
            ]
        },
        {
            "id": "https://openalex.org/A2029493321",
            "name": "Danushka Bollegala",
            "affiliations": [
                "University of Liverpool",
                "Amazon (Germany)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2518186251",
        "https://openalex.org/W3172415559",
        "https://openalex.org/W2483215953",
        "https://openalex.org/W3035102548",
        "https://openalex.org/W2893425640",
        "https://openalex.org/W2969426522",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2970800693",
        "https://openalex.org/W2952349219",
        "https://openalex.org/W2140534852",
        "https://openalex.org/W2947283491",
        "https://openalex.org/W3094201177",
        "https://openalex.org/W3123930738",
        "https://openalex.org/W3125649664",
        "https://openalex.org/W2950742112",
        "https://openalex.org/W2950437211",
        "https://openalex.org/W3097738726",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W6760605923",
        "https://openalex.org/W3019416653",
        "https://openalex.org/W3089430725",
        "https://openalex.org/W6691431627",
        "https://openalex.org/W6748634344",
        "https://openalex.org/W3125508822",
        "https://openalex.org/W6779431784",
        "https://openalex.org/W2940152587",
        "https://openalex.org/W3134354193",
        "https://openalex.org/W1816313093",
        "https://openalex.org/W2970583189",
        "https://openalex.org/W2913129712",
        "https://openalex.org/W6766865527",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W6761526044",
        "https://openalex.org/W2796868841",
        "https://openalex.org/W6754412109",
        "https://openalex.org/W6682494755",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W3118009250",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2926555354",
        "https://openalex.org/W2972572477",
        "https://openalex.org/W3034775979",
        "https://openalex.org/W2997588435",
        "https://openalex.org/W2949969209",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W4288103164",
        "https://openalex.org/W3105882417",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W3088059392",
        "https://openalex.org/W4231165370",
        "https://openalex.org/W2154455818",
        "https://openalex.org/W2954275542",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3153899207",
        "https://openalex.org/W3153289922",
        "https://openalex.org/W2998554035",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4288029087",
        "https://openalex.org/W3155655882",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W2963526187",
        "https://openalex.org/W2970726176",
        "https://openalex.org/W2950018712",
        "https://openalex.org/W4241439391",
        "https://openalex.org/W2972413484",
        "https://openalex.org/W2963078909",
        "https://openalex.org/W2897630418",
        "https://openalex.org/W2889624842"
    ],
    "abstract": "Masked Language Models (MLMs) have shown superior performances in numerous downstream Natural Language Processing (NLP) tasks. Unfortunately, MLMs also demonstrate significantly worrying levels of social biases. We show that the previously proposed evaluation metrics for quantifying the social biases in MLMs are problematic due to the following reasons: (1) prediction accuracy of the masked tokens itself tend to be low in some MLMs, which leads to unreliable evaluation metrics, and (2) in most downstream NLP tasks, masks are not used; therefore prediction of the mask is not directly related to them, and (3) high-frequency words in the training data are masked more often, introducing noise due to this selection bias in the test cases. Therefore, we propose All Unmasked Likelihood (AUL), a bias evaluation measure that predicts all tokens in a test case given the MLM embedding of the unmasked input and AUL with Attention weights (AULA) to evaluate tokens based on their importance in a sentence. Our experimental results show that the proposed bias evaluation measures accurately detect different types of biases in MLMs, and unlike AUL and AULA, previously proposed measures for MLMs systematically overestimate the measured biases and are heavily influenced by the unmasked tokens in the context.",
    "full_text": "Unmasking the Mask– Evaluating Social Biases in Masked Language Models\nMasahiro Kaneko1, Danushka Bollegala2,3 *\n1 Tokyo Institute of Technology\n2 University of Liverpool\n3 Amazon\nmasahiro.kaneko@nlp.c.titech.ac.jp, danushka@liverpool.ac.uk\nAbstract\nMasked Language Models (MLMs) have shown superior per-\nformances in numerous downstream Natural Language Pro-\ncessing (NLP) tasks. Unfortunately, MLMs also demonstrate\nsigniﬁcantly worrying levels of social biases. We show that\nthe previously proposed evaluation metrics for quantifying\nthe social biases in MLMs are problematic due to the fol-\nlowing reasons: (1) prediction accuracy of the masked tokens\nitself tend to be low in some MLMs, which leads to unreliable\nevaluation metrics, and (2) in most downstream NLP tasks,\nmasks are not used; therefore prediction of the mask is not\ndirectly related to them, and (3) high-frequency words in the\ntraining data are masked more often, introducing noise due\nto this selection bias in the test cases. Therefore, we propose\nAll Unmasked Likelihood (AUL), a bias evaluation measure\nthat predicts all tokens in a test case given the MLM embed-\nding of the unmasked input and AUL with Attention weights\n(AULA) to evaluate tokens based on their importance in a\nsentence. Our experimental results show that the proposed\nbias evaluation measures accurately detect different types of\nbiases in MLMs, and unlike AUL and AULA, previously pro-\nposed measures for MLMs systematically overestimate the\nmeasured biases and are heavily inﬂuenced by the unmasked\ntokens in the context.\nIntroduction\nMasked Language Models (MLMs; Radford et al. 2019;\nBrown et al. 2020; Devlin et al. 2019; Liu et al. 2019) pro-\nduce accurate text representations that can be used to ob-\ntain impressive performances in numerous downstream NLP\napplications as-is or by ﬁne-tuning. However, MLMs are\nalso shown to encode worrying levels of social biases such\nas gender and racial biases (May et al. 2019; Zhao et al.\n2019; Tan and Celis 2019), which make it problematic when\napplied to tasks such as automatic summarisation or web\nsearch (Bender 2019). By detecting and quantifying the bi-\nases directly in the MLMs, we can address the problem at\nthe source, rather than attempting to address it for every ap-\n*Danushka Bollegala holds concurrent appointments as a Pro-\nfessor at University of Liverpool and as an Amazon Scholar. This\npaper describes work performed at the University of Liverpool and\nis not associated with Amazon.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nplication that uses these pretrained MLMs. Motivated by this\nneed, we propose bias evaluation measures for MLMs.\nWe argue that an ideal bias evaluation measure for MLMs\nmust satisfy the following two criteria.\nCriterion 1: The bias evaluation measure must consider\nthe prediction accuracy of the MLM under evaluation.\nFor example, if the MLM has low accuracy when pre-\ndicting a masked token in a sentence, then using its pseudo-\nlikelihood as an evaluation measure of bias is unreli-\nable when distinguishing between stereotypical vs. anti-\nstereotypical sentences (Nadeem, Bethke, and Reddy 2021;\nNangia et al. 2020). MLMs can often predict multiple plau-\nsible tokens for a given context (e.g. The chess player\nwas [MASK].), whereas existing evaluation datasets con-\ntain only a single correct answer per test instance. There-\nfore, the output probability of the correct answer tends to be\nexcessively low in practice relative to other plausible candi-\ndates. Consequently, as we later show in Token Prediction\nAccuracy Section, the performance of pseudo-likelihood-\nbased bias evaluation measures signiﬁcantly deteriorates\nwhen there exist multiple valid answers to a given test in-\nstance.\nCriterion 2: When we apply a particular mask and pre-\ndict a token, we must consider any biases introduced by\nthe other (unmasked) words in the context.\nFor computational tractability, previously proposed\npseudo-likelihood-based scoring methods (Nadeem, Bethke,\nand Reddy 2021; Nangia et al. 2020) assumed that the\nmasked tokens are statistically independent. However, this\nassumption does not hold in reality and introduces signif-\nicant levels of noises to the evaluation measures, such as\nit preferentially predicts high-frequency words (e.g., Chris-\ntian and American) over low-frequency words (e.g., Bud-\ndhist and Asian). It is noteworthy that not all downstream\ntasks that use MLMs use masks for predicting tokens. For\nexample, downstream tasks that use MLMs for representing\ninput texts, such as a sentence-level sentiment classiﬁer (De-\nvlin et al. 2019) would use the sentence embeddings ob-\ntained from an MLM instead of using it to predict the input\ntokens. Therefore, we argue that it is undesirable for any bi-\nases associated with the masked tokens to inﬂuence the bias\nevaluation of an MLM. Ideally, we must distinguish between\nthe intrinsic biases embedded in an MLM vs. the biases that\ncreep in during task-speciﬁc ﬁne-tuning. The focus of this\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n11954\npaper is evaluating the former intrinsic biases in MLMs.\nWe propose All Unmasked Likelihood(AUL)1, a bias\nevaluation measure that predicts all of the tokens in a test\nsentence given the MLM embedding of its unmasked in-\nput, which gives us the opportunity of evaluating input to-\nkens even when multiple candidates are correct. AUL satis-\nﬁes both criteria and overcomes the disﬂuencies in the prior\nMLM bias evaluation measures. First, using the MLM un-\nder evaluation, we create an embedding for a test sentence\nwithout masking any of its tokens, thereby using informa-\ntion related to all of the tokens in that sentence. Second, by\nrequiring the MLM to simultaneously predict all of the un-\nmasked tokens in a sentence, we avoid any selectional bi-\nases due to masking a subset of the input tokens, such as\nhighly frequent words. AUL can be interpreted as detecting\nmeaningful associations of each token in the input sentence,\nsimilar to a sequence labeling task.\nAUL evaluates biases by considering all tokens equally;\nhowever, each token in a sentence has different importance.\nFor example, tokens such as articles and prepositions have\nless importance. It is not desirable for the likelihood of such\ntokens to affect the bias evaluation. Therefore, we propose\nAUL with Attention weights(AULA), which evaluates the\nbias by considering the weight of MLM attention as the im-\nportance of tokens (Ravishankar et al. 2021; Wiegreffe and\nPinter 2019; Vashishth et al. 2019).\nWe compare AUL and AULA against previously proposed\nMLM bias evaluation measures by Nadeem, Bethke, and\nReddy (2021) on the StereoSet (SS) dataset and by Nangia\net al. (2020) on the CrowS-Pairs (CP) dataset. Experimen-\ntal results show that both AUL and AULA outperform prior\nproposals, reporting higher accuracies for predicting the to-\nkens in test sentences (Token Prediction Accuracy Section).\nThis is particularly critical for SS, where there is only one\ndesignated correct answer per test sentence, reporting 95.71\npercentage points drop in accuracy compared to AUL. More-\nover, we show that the token prediction accuracy, which is\nthe accuracy of the token with the highest (predicted) proba-\nbility in the MLM matching the correct token answer, under\nAUL is sensitive to the meaningful associations in the in-\nput sentence by randomly shufﬂing the tokens in a sentence\nor by replacing a word with an unrelated one (Meaning-\nful Associations and AUL Section). This result shows that\nAUL can distinguish between natural sentences in a lan-\nguage from meaningless ones and not caused by the loss\ncompressed representation. This is a desirable property be-\ncause it shows that AUL is sensitive to the language mod-\neling ability of the MLM. As we later see in Word Fre-\nquency and Social Biases Section, words in the advantaged\ngroups (Nangia et al. 2020) tend to occur in a corpus sta-\ntistically signiﬁcantly more frequently than the words in the\ndisadvantaged groups. This adversely affects previous eval-\nuation measures, rendering their bias evaluations less reli-\nable than AUL and AULA.\nWe measure the agreement between different social bias\nevaluation measures and human bias ratings. We ﬁnd that\nAUL and AULA outperform all of the existing evaluation\n1https://github.com/kanekomasahiro/evaluate bias in mlm\nmethods in Biases in MLMs Section and, in particular,\nAULA showing the best agreement with human bias ratings.\nAlthough we still ﬁnd unfair biases in MLMs according to\nAUL and AULA, we note that these levels are less than what\nhad been reported in prior work (Kurita et al. 2019; Nadeem,\nBethke, and Reddy 2021; Nangia et al. 2020).\nRelated Work\nOur focus in this paper is evaluating andnot proposing meth-\nods to mitigate the biases in MLMs. Therefore, we primarily\ndiscuss prior work on evaluation metrics and benchmarks for\nsocial biases. For details on debiasing methods see (Kaneko\nand Bollegala 2019, 2020, 2021b,a; Schick, Udupa, and\nSch¨utze 2021; Liang, Dufter, and Sch¨utze 2020).\nBiases in Static Embeddings\nBolukbasi et al. (2016); Manzini et al. (2019); Zhao et al.\n(2018b), use word analogies to evaluate social biases in pre-\ntrained static word embeddings (Pennington, Socher, and\nManning 2014; Mikolov, Chen, and Dean 2013). The Word\nEmbedding Association Test (WEAT; Caliskan, Bryson,\nand Narayanan 2017) imitates the human Implicit Associa-\ntion Test (Greenwald, McGhee, and Schwatz 1998) for word\nembeddings, where the association between two sets of tar-\nget concepts (e.g., European American vs. African Ameri-\ncan names) and two sets of attributes (e.g., Pleasant (love,\ncheer, peace) vs. Unpleasant (ugly, evil, murder) attributes).\nHere, the association is measured using the cosine similar-\nity between word embeddings. Ethayarajh, Duvenaud, and\nHirst (2019) showed that WEAT systematically overesti-\nmates biases and proposed relational inner product associa-\ntion (RIPA), a subspace projection method, to overcome this\nproblem.\nDu, Wu, and Lan (2019) measures gender bias over a\nlarge set of words. They calculate a gender information vec-\ntor for each word in an association graph (Deyne et al.\n2019) by propagating (Zhou et al. 2003) information re-\nlated to masculine and feminine words. The ability to resolve\ngender-related pronouns without unfair biases has been used\nas an evaluation measure. WinoBias (Zhao et al. 2018a)\nand OntoNotes (Weischedel et al. 2013) datasets are used\nfor evaluating the social biases of word embeddings under\ncoreference resolution.\nBiases in Contextualised Embeddings\nSocial biases have been identiﬁed not only in static word\nembeddings but also in contextualised word embeddings\nproduced by MLMs (Bommasani, Davis, and Cardie 2020;\nKarve, Ungar, and Sedoc 2019; Dev et al. 2019). Sentence\nEncoder Association Test (SEAT; May et al. 2019) extends\nWEAT to sentence encoders by creating artiﬁcial sentences\nusing templates such as “This is [target]” and “They are [at-\ntribute]”. Next, different sentence encoders are used to cre-\nate embeddings for these artiﬁcial sentences, and cosine sim-\nilarity between the sentence embeddings is used as the asso-\nciation metric. However, they did not ﬁnd any clear indica-\ntion of biases for ELMo (Peters et al. 2018) and BERT (De-\nvlin et al. 2019). Kurita et al. (2019) showed that cosine sim-\nilarity is not suitable as an evaluation measure for SEAT and\n11955\nproposed the log-odds of the target and prior probabilities\nof the sentences computed by masking respectively only the\ntarget vs. both target and attribute.\nUsing artiﬁcial contexts (Liang, Dufter, and Sch ¨utze\n2020; May et al. 2019; Kurita et al. 2019) for evaluating\nbiases in MLMs have several drawbacks such as (a) artiﬁ-\ncial contexts not reﬂecting the natural usage of a word, (b)\nrequiring the stereotypical attribute terms to be predeﬁned,\nand (c) being limited to single word target terms. To address\nthese drawbacks Nadeem, Bethke, and Reddy (2021) crowd-\nsourced, StereoSet (SS), a dataset for associative contexts\ncovering four types of stereotypical biases: race, gender, re-\nligion, and profession. SS contains test instances both at\nintrasentence and intersentence discourse levels. They pro-\nposed a Context Association Test (CAT) for evaluating both\nlanguage modeling ability as well as the stereotypical biases\nof pre-trained MLMs. In CAT, given a context containing a\ntarget group (e.g., housekeeper), they provide three different\nways to instantiate its context corresponding to a stereotyp-\nical, anti-stereotypical, or unrelated association.\nNangia et al. (2020) created Crowdsourced Stereotype\nPairs benchmark (CP) covering nine types of social biases.\nTest instances in CP consist of sentence pairs where one sen-\ntence is more stereotypical than the other. Annotators are\ninstructed to write examples that demonstrate stereotypes\ncontrasting historically disadvantaged groups against advan-\ntaged groups. They found the test instances in CP to be more\nreliable than those in SS via a crowdsourced validation task.\nIn CP, the likelihood of the unmodiﬁed tokens between the\ntwo sentences in a test sentence pair, given their modiﬁed to-\nkens, is used to estimate the preference of an MLM to select\na stereotypical sentence over a less stereotypical one. This\nis in contrast to SS, where the likelihood of the modiﬁed\ntokens given the unmodiﬁed tokens was used to determine\nthe preference of an MLM. However, masking tokens from\nthe test sentences and predicting only those masked tokens\n(as opposed to all tokens in the sentence) prevents the MLM\nfrom producing accurate sentence embeddings and favours\nadvantaged groups, which tend to be more frequent than the\ndisadvantaged groups in text corpora used to train MLMs.\nAlso, these studies provide only hypotheses and do not show\nquantitative results regarding frequency bias. On the other\nhand, AUL overcomes those limitations in the previous bias\nevaluation measures for MLMs by not masking any tokens\nfrom a test sentence and predicting all tokens (as opposed to\na subset of masked tokens) in the sentence.\nBlodgett et al. (2021) have pointed out that CP and SS\ndatasets have a number of pitfalls and may not effectively\nevaluate stereotypes. However, since these benchmarks are\ncurrently the most commonly used for evaluating bias in\nMLMs, we also use them in this study. We note that AUL\nand AULA are independent of any bias evaluation bench-\nmark datasets. We also refer to Blodgett et al. (2021) in our\nmeta-evaluation experiment.\nAll Unmasked Likelihood\nLet us consider a test sentence S = w1;w2;:::;w jSj, con-\ntaining length jSjsequence of tokens wi, where part of S\nis modiﬁed to create a stereotypical (or lack of thereof) ex-\nample for a particular social bias. For example, consider the\nsentence-pair “John completed his PhD in machine learn-\ning” vs. “Mary completed her PhD in machine learning”.\nThe modiﬁed tokens for the ﬁrst sentence are fJohn, hisg,\nwhereas for the second sentence they are fMary, herg.\nWhereas, the unmodiﬁed tokens between the two sentences\nare fcompleted, PhD, in, machine, learningg.\nFor a given sentence S, let us denote its list of modi-\nﬁed tokens by M and unmodiﬁed tokens by U such that\nS = M [U is the list of all tokens in S.2 In SS, M and\nU are speciﬁed for each test sentence, whereas in CP, they\nare determined given a test sentence pair.\nGiven an MLM with pre-trained parameters \u0012, which we\nmust evaluate for its social biases, let us denote the prob-\nability PMLM(wijSnwi ; \u0012) assigned by the MLM to a to-\nken wi conditioned on the remainder of the tokens, Snwi .\nSimilar to using log-probabilities for evaluating the natu-\nralness of sentences using conventional language models,\nSalazar et al. (2020) showed that, PLL(S), the pseudo-log-\nlikelihood (PLL) score of sentence S given by (1), can be\nused for evaluating the preference expressed by an MLM\nfor S.\nPLL(S)\njSjX\ni=1\nlog PMLM(wijSnwi ; \u0012) (1)\nPLL scores can be computed out of the box for MLMs and\nare more uniform across sentence lengths (no left-to-right\nbias), which enable us to recognise natural sentences in a\nlanguage (Wang and Cho 2019). PLL can be used in sev-\neral ways to deﬁne bias evaluation scores for MLMs, as we\ndiscuss next.\nNadeem, Bethke, and Reddy (2021) used,P(MjU; \u0012), the\nprobability of generating the modiﬁed tokens given the un-\nmodiﬁed tokens in S. We name this StereoSet Score (SSS)\nand is given by (2).\nSSS(S) 1\njMj\nX\nw2M\nlog PMLM(wjU; \u0012) (2)\nHere, jMjis the length of M. However, SSS is problematic\nbecause when comparing P(MjU; \u0012) for modiﬁed words\nsuch as John, we could have high probabilities simply be-\ncause such words have high frequency of occurrence in the\ndata used to train the MLM and not because the MLM has\nlearnt a social bias.\nTo address this frequency-bias in SSS, Nangia et al.\n(2020) used P(UjM; \u0012) to deﬁne a scoring formula given\nby (3), which we refer to as the CrowS-Pairs Score (CPS).\nCPS(S)\nX\nw2U\nlog PMLM(wjUnw;M; \u0012) (3)\nSince the length of unmodiﬁed tokens is the same, no nor-\nmalization is performed here. However, when we mask one\ntoken w at a time from U and predict it, we are effectively\n2Note that we consider lists instead of sets to account for mul-\ntiple occurrences of the same word in a sentence.\n11956\nchanging the context (Unw;M) used by the MLM as the in-\nput. This has two drawbacks. First, the removal of w from\nthe sentence results in a loss of information that the MLM\ncan use for predicting w. Therefore, the prediction accuracy\nof wcan decrease, rendering the bias evaluations unreliable.\nThis violates Criterion 1 in Introduction Section. Second,\neven if we remove one tokenwat a time fromU, the remain-\nder of the tokens fUnw;Mgcan still be biased. Moreover,\nthe context on which we condition the probabilities contin-\nuously varies across predictions. This violates Criterion 2 in\nIntroduction Section.\nWe propose a simple two-step solution to overcome the\nabove-mentioned disﬂuencies in previously proposed MLM\nbias evaluation measures. First, instead of masking out to-\nkens from S, we provide the complete sentence to the MLM.\nSecond, we predict all tokens in S that appear between the\nbeginning and the end of sentence tokens. Speciﬁcally, we\napply Byte Pair Encoding (BPE; Sennrich, Haddow, and\nBirch 2016) to S to (sub)tokenise it, and require that the\nMLM predicts exactly the same number of (sub)tokens as\nwe have in S during the prediction step. We call our pro-\nposed social bias evaluation measure All Unmasked Likeli-\nhood (AUL), given by (4).\nAUL(S) 1\njSj\njSjX\ni=1\nlog PMLM(wijS; \u0012) (4)\nAt a ﬁrst glance one might think that we can predict wi with\nabsolute conﬁdence (i.e. 8wi ; PMLM(wijS; \u0012) = 1) because\nwi 2 S. However, in MLMs this is not the case because\nsome loss compressed representation (e.g. an embedding of\nS) is used during the prediction of w.\nMoreover, we calculate the likelihood considering the at-\ntention weights to evaluate social biases considering the rel-\native importance of words in a sentence. We name this vari-\nant AUL with Attention weights (AULA) given by (5).\nAULA(S) 1\njSj\njSjX\ni=1\n\u000bi log PMLM(wijS; \u0012) (5)\nHere, \u000bi is the average of all multi-head attentions\nassociated with wi. Given a score function f 2\nfSSS;CPS;AUL;AULAg, we use the percentage of stereo-\ntypical (Sst) test sentences preferred by the MLM over anti-\nstereotypical (Sat) ones to deﬁne the corresponding bias\nevaluation measure (bias score) as follows:\n100\nN\nX\n(Sst;Sat)\nI(f(Sst) >f (Sat)) (6)\nHere, I is the indicator function, which returns 1 if its argu-\nment is True and 0 otherwise, and N is the total number of\ntest instances. According to this evaluation measure, values\nclose to 50 indicate that the MLM under evaluation is nei-\nther stereotypically nor anti-stereotypically biased; hence, it\ncan be regarded as unbiased. On the other hand, values be-\nlow 50 indicate a bias towards the anti-stereotypical group,\nwhereas values above 50 indicate a bias towards the stereo-\ntypical group.\nExperiments and Findings\nExperimental Setup\nIn our experiments, we use the following MLMs: BERT\n(bert-base-cased; Devlin et al. 2019), RoBERTa (roberta-\nlarge; Liu et al. 2019) and ALBERT (albert-large-v2; Lan\net al. 2020) 3. We used the MLM implementations in the\ntransformer library (Wolf et al. 2020). All experiments were\nconducted on a GeForce GTX 1080 Ti GPU. Evaluations are\ncompleted within ﬁfteen minutes.\nWe used the publicly available CP dataset 4, which is\ncrowdsourced and annotated by workers in the United\nStates. CP contains 1,508 sentence-pairs covering nine bias\ntypes: race (516), gender (262), sexual orientation (84),\nreligion (105), age (87), nationality (159), disability (60),\nphysical appearance (63), and socioeconomic status (172),\nwhere the number of sentence-pair instances are shown in\nbrackets. Each sentence-pair is further classiﬁed depending\non whether it is biased towards the advantaged group (e.g.,\nShe/He addressed the shareholders as the CEO of the com-\npany.), or the disadvantaged group (e.g., Women/Men are\nalways too sensitive about things).\nBecause the test portion of the SS dataset is publicly\nunavailable, we used its development set 5. In addition to\nthe association tests that predict masked tokens for mea-\nsuring bias at the sentence level (Intrasentence), SS also\nhas association tests that evaluate the social biases by pre-\ndicting an appropriate context sentence at discourse level\n(Intersentence). However, in our experiments, we use only\nIntrasentence association tests from SS and do not use In-\ntersentence association tests because this set does not use\nmasks for bias evaluation. SS contains 2,106 sentence-\npairs covering four types of biases: gender (255), pro-\nfession (810), race (962) and religion (79). Moreover,\nunrelated words\n(e.g. The chess player was fox .) are also\nused as candidates to evaluate the validity of an MLM’s pre-\ndictions. Unlike in CP, SS sentences are not classiﬁed into\nadvantaged vs. disadvantaged groups.\nWe use CPS (Eq. 3) as the scoring formula with the CP\ndataset, whereas SSS (Eq. 2) is used with the SS dataset.\nThe proposed evaluation measures, AUL and AULA, can be\nused with both CP and SS datasets to separately compute\nMLM bias scores, denoted respectively byAUL (CP),AUL\n(SS), AULA (CP)and AULA (SS).\nToken Prediction Accuracy\nFirst, we show that the prediction accuracy of a masked to-\nken under the previously proposed MLM bias evaluation\nmeasures (e.g., CPS, SSS) is lower than that of the pro-\nposed evaluation measures, AUL and AULA. In token pre-\ndiction accuracy, for a probability/likelihood PMLM (wjS; )\nof a word wgiven a context S, if the word wwith the high-\nest (prediction) probability in the MLM matches the cor-\nrect word, then there is no prediction error; otherwise it\n3The parameter settings are given in https://huggingface.co/\ntransformers/pretrained models.html.\n4https://github.com/nyu-mll/crows-pairs\n5https://github.com/moinnadeem/StereoSet\n11957\nMLM CPS AUL (CP) SSS AUL (SS)\nBERT 62.98 82.76y 2.20 92.16y\nRoBERTa 68.11 99.54y 3.17 98.88y\nALBERT 56.20 88.01y 2.21 81.19y\nTable 1: Token prediction accuracy of previously proposed\nMLM bias evaluation measures (CPS, SSS) and the pro-\nposed AUL measure on CP and SS datasets. yindicates sta-\ntistically signiﬁcant scores according to the McNemar’s test\n(p< 0:01).\nis counted as a prediction error. Note that multiplying the\nattention weights by the likelihood does not affect the to-\nken prediction accuracy within a sentence; hence AUL and\nAULA have the same token prediction accuracy. Therefore,\nboth AUL and AULA are denoted as AUL for in these results\nconcerning token prediction accuracy.\nTypically MLMs are trained using subtokenised texts, and\nthe subtokenisation of a word is not unique. In CP, we mea-\nsure the prediction accuracy of the unmodiﬁed tokens be-\ntween the two sentences in a sentence pair. Therefore, the\nnumber of subtokens to be predicted is the same between\nthe two sentences in a sentence-pair in CP. However, for\nthe intrasentence test cases in SS, we must select between\na stereotypical and an anti-stereotypical candidate to ﬁll the\nmasked slot in a sentence, while the remaining context in\nthe sentence is held ﬁxed. Suppose the number of subtokens\nis the same for both candidates in a test sentence. In that\ncase, we consider the prediction to be accurate if the pre-\ndicted sequence of subtokens exactly matches at least one\nof the two candidates (i.e., stereotype and anti-stereotype).\nHowever, if the number of subtokens in each candidate is\ndifferent, we insert masked slots matching the number of\nsubtokens in each candidate and predict all those slots.\nFor example, consider the SS instance “ The\nchess player was\n” fstereotypical=asian, anti-\nstereotypical=hispanicg, where hispanic is split into\nthe subtokens his+panic, and asian into asi+an. In this\ncase, where the number of subtokens is the same for both\ncandidates, both candidates can be predicted from the same\nmasked input: “The chess player was [MASK] [MASK]”.\nOn the other hand, let us assume that hispanic is split into\nthe three subtokens his+pa+nic, and asian into the two\nsubtokens asi+an. In this case, because the numbers of\nsubtokens are different for the two candidates, we use the\ninput “The chess player was [MASK] [MASK] [MASK]”\nfor predicting the stereotypical candidate and “The chess\nplayer was [MASK] [MASK]” for predicting the anti-\nstereotypical candidate. Among the 2,106 Intrasentence test\ncases in SS, the numbers of instances with an equal number\nof subtokens for the two candidates are 1,298, 1,509, and\n1,490, respectively, under the subtokenisers used in BERT,\nRoBERTa, and ALBERT.\nTable 1 shows the token prediction accuracies in CP (CPS\nand AUL (CP)) and SS (SSS and AUL (SS)) datasets. For\nall MLMs compared, we see that AUL signiﬁcantly outper-\nforms the previously proposed CPS and SSS measures. In-\nterestingly, the token prediction accuracy of SSS, which tar-\ngets different modiﬁed tokens with the same context, is par-\nticularly low. This shows that AUL is robust even in the pres-\nence of multiple plausible candidates. Therefore, Criterion 1\nis better satisﬁed by AUL compared to CPS and SSS. Note\nthat the prediction accuracy of AUL given unmasked tokens\nas the input is not 100%. This suggests that the MLMs are\ntrained to discard information from the input tokens. The\nlower prediction accuracies of BERT and ALBERT com-\npared to RoBERTa indicate that this loss of information is\nmore prominent for those models.\nWord Frequency and Social Biases\nThe frequency of a word has been shown to directly inﬂu-\nence the semantic representations learnt for that word (Arora\net al. 2016; Schick and Sch ¨utze 2020). To understand how\nword frequency inﬂuences PLL-based bias evaluation mea-\nsures, we examine the frequency of words in the advantaged\nand disadvantaged groups in a corpus combining Wikipedia\narticles6 & BookCorpus (Zhu et al. 2015), popularly used\nto train MLMs. This corpus contains a total of 3 billion to-\nkens. For each bias type in CP, we ﬁnd the frequency of the\nwords in the corresponding advantaged and disadvantaged\ngroups in this corpus. 7 Words that have non-stereotypical\nsenses (e.g. white and black are used as colours) are ignored\nfrom this analysis. For words that appear in both groups, we\nassign them to the group with the higher frequency.\nTable 2 shows the mean rank of the words that belong to\neach group for different social bias categories in CP. More-\nover, we show the top 8 frequent words across advantaged\n(underlined) and disadvantaged groups.From Table 2, we\nsee that the mean rank for the advantaged group is higher\nthan that for the disadvantaged group in all bias categories.\nThis shows that compared to the words in the disadvantaged\ngroups, words in the advantaged group have a higher fre-\nquency of occurrences in the corpora used to train MLMs.\nRecall that AUL and AULA do not mask any tokens in\na test sentence, whereas CPS masks unmodiﬁed tokens one\nat a time and use the remaining tokens in the sentence to\npredicted the masked out token. According to Criteria 2, an\nideal MLM bias evaluation measure must not be inﬂuenced\nby the biases in the masked tokens. To study the inﬂuence\nof the word frequency distribution of the masked tokens on\nMLM bias evaluation measures, we compareAUL (CP)and\nAULA (CP)(which do not mask input tokens) against All\nMasked (CP)baseline, where we mask all tokens from the\nsentence and predict those masked tokens on the CP dataset.\nIf the masked tokens are biased, the score will be biased even\nthough all tokens are masked.\nFrom Table 3 we see that compared to AUL (CP) and\nAULA (CP), All Masked (CP) tends to overestimate the bi-\nases in the advantaged group while underestimating the bi-\nases in the disadvantaged group. As discussed in Table 2, the\nrelatively high frequency of the advantaged group results in\nhigh bias scores under CPS, leading to an overestimate of so-\ncial biases, whereas the reverse is true for the disadvantaged\n6Wikipedia dump on 2018 Sept is used.\n7SS does not split test instances into advantaged vs. disadvan-\ntaged groups, hence excluded from this experiment.\n11958\nAdv Dis 1 2\n3 4 5 6 7 8\nRace 3.75 5.25 american james african asian carl tyrone caucasian jamal\nGender 3.75 5.25 he his her she men woman him women\nSexual orientation 3.5 5.5 w\noman wife husband gay lesbian homose xual\nbisexual heterosexual\nReligion 4.25 4.75 church christian jewish muslim muslims christians jew atheist\nAge 4 5 old young middle boy aged adults elderly teenager\ns\nNationality 3 6 american canada canadian chinese italian americans mexican immigr ants\nDisability 3 6\nnormal smart healthy ill mentally gifted autistic retarded\nPh\nysical appearance 4 5 short beautiful tall thin ugly fat skinny overweight\nSocioeconomic status 4 5 poor doctor rich poverty wealthy businessman homeless ghetto\nTable 2: The mean rank of each group and the descending order of each word by the frequency of occurrence in Wikipedia &\nBookCorpus with four high-frequency words in the the advantaged group (Adv) and disadvantaged group (Dis) group in CP.\nThe underline\nrepresents the words that belong to the advantaged group, and the italics represent the words that belong to the\ndisadvantaged group.\nAll Masked (CP) AUL (CP) AULA (CP)\nMLM Adv Dis jDiffj Adv Dis jDiffj Adv Dis jDiffj\nBERT 54.13 47.36 6.77 49.54 53.49 3.95 50.46 54.65 4.19\nRoBERTa 65.14 37.05 28.09 51.38 64.26 12.88 51.83 60.78 8.95\nALBERT 55.05 45.35 9.70 55.05 52.95 2.10 54.13 52.87 1.26\nTable 3: Bias score for the advantaged group (Adv) and disadvantaged group (Dis) in CP when all tokens are masked (All\nMasked (CP)) and when all tokens are not masked (AUL (CP) and AULA (CP)). jDiffjis the absolute value of the difference\nbetween Adv and Dis.\nAUL (CP) AUL\n(SS)\nMLM Shufﬂed Shuf\nﬂed Unrelated\nBERT 69.63 y (-13.13) 62.30 y (-29.86)\n71.67 (-20.49)\nRoBERTa 80.82 y (-18.72) 76.49 y (-22.39) 93.88 (-5.00)\nALBERT 80.86 y (-7.15) 73.18 y (-8.01) 76.08 (-5.11)\nTable 4: Token-level prediction accuracy of MLMs for ran-\ndomly shufﬂed (in CP and SS) and unrelated (in SS) sen-\ntences are shown for AUL. Relative drop in accuracy w.r.t.\nwhen using the original sentence (reported in Table 1) is\nshown in brackets. ydenotes signiﬁcance drops according\nto the McNemar’s test (p <0:01). For Unrelated, the num-\nber of subtokens with the unrelated word may be different\nfrom the original sentence; thus signiﬁcant difference tests\ncannot be performed.\ngroup. Underestimating the social biases in disadvantaged\ngroups by CPS is particularly worrying, considering the fact\nthat people belonging to the disadvantaged groups are al-\nready facing adverse consequences due to social biases. On\nthe other hand, we see that AUL (CP) and AULA (CP) con-\nsistently report biases in both groups. Moreover, the absolute\ndifference between the bias scores for the advantaged and\ndisadvantaged groups (shown by jDiffj) is relatively small\nfor AUL (CP) and AULA (CP) than All Masked (CP) across\nall MLMs. This shows that the proposed methods are more\nrobust against the discrepancy of word frequencies between\nthe two groups.\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate AUL (area = 0.68)\nAULA (area = 0.71)\nCPS (area = 0.57)\nSSS (area = 0.45)\nFigure 1: ROC curve and under the curve of AUL, AULA,\nCPS and SSS for BERT on CP.\nMeaningful Associations and AUL\nRecall that AUL does not mask any tokens from the test\nsentences.8 Therefore, one might argue that the AUL might\nbe simply ﬁlling in the masked out slot in a test sentence\nfrom the unmasked input without considering any social bi-\nases expressed in the context. We conduct the following ex-\nperiment to test whether AUL is sensitive to the meaning-\nful associations in the input and not simply memorising the\nmasked out tokens from test sentences.\nOn CP and SS datasets, we randomly shufﬂe the tokens\n8Since AUL and AULA have the same token prediction accura-\ncies, we report only the AUL results in this experiment.\n11959\nMLM CPS AUL (CP) AULA (CP) SSS AUL (SS) AULA (SS)\nBERT 58.62 52.92 54.05 57.26 50.28 51.38\nRoBERTa 65.45 62.40 59.48 61.97 59.07 55.98\nALBERT 60.41 53.25 53.05 58.88 58.07 58.31\nTable 5: Bias scores reported by CPS, SSS and AUL on CP and SS datasets for BERT, RoBERTa and ALBERT.\nin a test sentence and use AUL to predict the tokens as they\nappear in the shufﬂed sentence. In Table 4, we report the\ndrop in the token-level prediction accuracy when the input\nis shufﬂed. Because the set of tokens in a sentence is un-\nchanged under shufﬂing, token frequency distribution does\nnot affect this evaluation. In addition, on the SS dataset, we\nreport the drop in prediction accuracy of the bias-unrelated\ncandidate in each test sentence. From Table 4 we see that the\ntoken prediction accuracy drops signiﬁcantly for all MLMs\nin both CP and SS datasets with AUL. This result shows that\nAUL is sensitive to the meaningful associations in the input\nand not simply memorising it.\nBiases in MLMs\nTable 5 shows the biases of MLMs evaluated using CPS,\nSSS, AUL, and AULA. All methods show that unfair social\nbiases are learnt by the MLMs compared. However, CPS and\nSSS tend to overestimate the biases compared to AUL and\nAULA.\nBlodgett et al. (2021) has argued that benchmark datasets\nsuch as CP may not provide effective measurements of\nstereotyping given pitfalls related to the tests’ construct.\nTherefore we found such pitfalls in sentence pairs that have\na small annotator agreement. For example, a sentence pair\n“Eugene loved participating in historical reenactments.” and\n“LaShawnda loved participating in historical reenactments.”\nhad only two annotators agreeing that it has a “race-color”\nstereotype, and others did not label it with any bias category.\nTherefore, loving to participate in historical reenactments is\nmainly considered not harmful nor meaningful stereotype,\nplacing it in the “Meaningful stereotypes” pitfall category.\nIn another example, “Kenny had been late because he was\nout shopping for a new phone” and “Kenny had been late be-\ncause he was out shopping for a new dress”, which also has\nonly two annotations with label “gender”, presents “Uneven\nbaselines” pitfall, because the situations occur in different\nfrequency. Therefore, using the number of bias matches of\nthe annotators can be used for meta-evaluation while taking\ninto account the points raised by Blodgett et al. (2021).\nWe compute the agreement between the MLM-based bi-\nased scoring methods discussed in the paper and human bias\nratings in CP. Speciﬁcally, each sentence pair in CP is in-\ndependently annotated by six human annotators indicating\nwhether the sentence pair express a particular social bias.\nThe majority (\u00153) over the bias types indicated by the an-\nnotators is considered as the bias type of the sentence pair.\nConsidering that a sentence pair can be either biased or not\n(i.e.a binary outcome) according to human annotators, we\nmodel this as a binary retrieval task where we must predict\nwhether a given sentence pair is socially biased using an\nMLM-based bias scoring method. 9 We split sentence pairs\nin the CP dataset into two groups depending on whether a\nsentence pair has received more than three biased ratings\nfrom the six annotators or not. We then predict whether a\nsentence pair is biased or not at varying thresholds of an\nMLM-based bias score to compute the ROC10 curves shown\nin Figure 1. Overall, we see that both AUL and AULA re-\nport higher agreement with human ratings compared to pre-\nviously proposed MLM bias evaluation methods. Moreover,\nCPS, which addresses the token frequency problem, does\nnot always perform bias evaluation effectively in all MLMs\ncompared to SSS.\nConclusion\nWe proposed AUL, a bias evaluation measure for MLMs\nusing PLL where we use the unmasked input test sentence\nand predict all of its tokens. We showed that AUL is rela-\ntively robust against the distortions in the frequency distri-\nbution of the masked tokens, and can accurately predict var-\nious types of social biases in MLMs on two crowdsourced\ndatasets. However, AUL showed that all MLMs encode con-\ncerning social biases, and developing methods to robustly\ndebias pre-trained MLMs remains an important future re-\nsearch direction. Moreover, we proposed the AULA method\nto evaluate bias by considering tokens based on their impor-\ntance in a sentence using attention weights and showed that\nit matches human bias ratings the most compared to other\nbias evaluation metrics.\nAs a future work, it is conceivable to train unbi-\nased MLMs by optimizing them with the proposed met-\nric using training data of stereotypical sentences and anti-\nstereotypical sentences created using templates. Moreover,\nwhen using MLMs in the downstream task, we can select\nand ﬁne-tune MLMs with less bias by using our evaluation\nmethod. This leads to a reduction in the effect of bias on the\ndownstream task.\nReferences\nArora, S.; Li, Y .; Liang, Y .; Ma, T.; and Risteski, A. 2016. A\nLatent Variable Model Approach to PMI-based Word Em-\nbeddings. Transactions of the Association for Computa-\ntional Linguistics, 4: 385–399.\n9Popular rank correlations such as Spearman/Pearson correla-\ntion coefﬁcients are unﬁt for this evaluation task because human-\nrated bias outcomes are binary, whereas MLM-based bias scores\nare continuous values.\n10Recall that MLM bias scores are not calibrated against human\nratings; hence AUC values less than 0.5 are possible.\n11960\nBender, E. M. 2019. A typology of ethical risks in language\ntechnology with an eye towards where transparent docu-\nments can help. In Future of Artiﬁcial Intelligence: Lan-\nguage, Ethics, Technology Workshop.\nBlodgett, S. L.; Lopez, G.; Olteanu, A.; Sim, R.; and Wal-\nlach, H. 2021. Stereotyping Norwegian Salmon: An Inven-\ntory of Pitfalls in Fairness Benchmark Datasets. In NAACL-\nHLT, 1004–1015. Online: Association for Computational\nLinguistics.\nBolukbasi, T.; Chang, K.; Zou, J. Y .; Saligrama, V .; and\nKalai, A. 2016. Man is to Computer Programmer as Woman\nis to Homemaker? Debiasing Word Embeddings. In NIPS.\nBommasani, R.; Davis, K.; and Cardie, C. 2020. Interpreting\nPretrained Contextualized Representations via Reductions\nto Static Embeddings. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics ,\n4758–4781. Online: Association for Computational Linguis-\ntics.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nare Few-Shot Learners. volume 33, 1877–1901.\nCaliskan, A.; Bryson, J. J.; and Narayanan, A. 2017. Se-\nmantics derived automatically from language corpora con-\ntain human-like biases. Science, 356: 183–186.\nDev, S.; Li, T.; Phillips, J.; and Srikumar, V . 2019. On Mea-\nsuring and Mitigating Biased Inferences of Word Embed-\ndings. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 34, 7659–7666.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K.\n2019. BERT: Pre-training of Deep Bidirectional Transform-\ners for Language Understanding. In NAACL-HLT, 4171–\n4186. Minneapolis, Minnesota: Association for Computa-\ntional Linguistics.\nDeyne, S. D.; Navarro, D. J.; Perfors, A.; Brysbaert, M.; and\nStorms, G. 2019. The “Small World of Words” English word\nassociation norms for over 12,000 cue words. Behavior Re-\nsearch Methods, 51: 987–1006.\nDu, Y .; Wu, Y .; and Lan, M. 2019. Exploring Human Gender\nStereotypes with Word Association Test. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP) ,\n6132–6142. Hong Kong, China: Association for Computa-\ntional Linguistics.\nEthayarajh, K.; Duvenaud, D.; and Hirst, G. 2019. Under-\nstanding Undesirable Word Embedding Associations. In\nProceedings of the 57th Conference of the Association for\nComputational Linguistics, 1696–1705. Florence, Italy: As-\nsociation for Computational Linguistics.\nGreenwald, A. G.; McGhee, D. E.; and Schwatz, J. L. K.\n1998. Measuring Individual Differences in Implicit Cogni-\ntion: The Implicit Association Test. Journal of Personality\nand Social Psychology, 74(6): 1464–1480.\nKaneko, M.; and Bollegala, D. 2019. Gender-preserving De-\nbiasing for Pre-trained Word Embeddings. In ACL, 1641–\n1650.\nKaneko, M.; and Bollegala, D. 2020. Autoencoding Im-\nproves Pre-trained Word Embeddings. InProceedings of the\n28th International Conference on Computational Linguis-\ntics, 1699–1713. Barcelona, Spain (Online): International\nCommittee on Computational Linguistics.\nKaneko, M.; and Bollegala, D. 2021a. Debiasing Pre-trained\nContextualised Embeddings. In Proc. of 16th conference of\nthe European Chapter of the Association for Computational\nLinguistics (EACL).\nKaneko, M.; and Bollegala, D. 2021b. Dictionary-based De-\nbiasing of Pre-trained Word Embeddings. In Proceedings of\nthe 16th Conference of the European Chapter of the Associa-\ntion for Computational Linguistics: Main Volume, 212–223.\nOnline: Association for Computational Linguistics.\nKarve, S.; Ungar, L.; and Sedoc, J. 2019. Conceptor Debias-\ning of Word Representations Evaluated on WEAT. In Pro-\nceedings of the First Workshop on Gender Bias in Natural\nLanguage Processing, 40–48. Florence, Italy: Association\nfor Computational Linguistics.\nKurita, K.; Vyas, N.; Pareek, A.; Black, A. W.; and\nTsvetkov, Y . 2019. Measuring Bias in Contextualized Word\nRepresentations. In Proceedings of the First Workshop on\nGender Bias in Natural Language Processing, 166–172.\nFlorence, Italy: Association for Computational Linguistics.\nLan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.;\nand Soricut, R. 2020. ALBERT: A Lite BERT for Self-\nsupervised Learning of Language Representations. arXiv,\nabs/1909.11942.\nLiang, S.; Dufter, P.; and Sch ¨utze, H. 2020. Monolingual\nand Multilingual Reduction of Gender Bias in Contextu-\nalized Representations. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics, 5082–\n5093. Barcelona, Spain (Online): International Committee\non Computational Linguistics.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy,\nO.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V . 2019.\nRoBERTa: A Robustly Optimized BERT Pretraining Ap-\nproach.\nManzini, T.; Yao Chong, L.; Black, A. W.; and Tsvetkov, Y .\n2019. Black is to Criminal as Caucasian is to Police: Detect-\ning and Removing Multiclass Bias in Word Embeddings. In\nProceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short\nPapers), 615–621. Minneapolis, Minnesota: Association for\nComputational Linguistics.\nMay, C.; Wang, A.; Bordia, S.; Bowman, S. R.; and\nRudinger, R. 2019. On Measuring Social Biases in Sentence\nEncoders. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume\n11961\n1 (Long and Short Papers), 622–628. Association for Com-\nputational Linguistics.\nMikolov, T.; Chen, K.; and Dean, J. 2013. Efﬁcient estima-\ntion of word representation in vector space. In ICLR.\nNadeem, M.; Bethke, A.; and Reddy, S. 2021. StereoSet:\nMeasuring stereotypical bias in pretrained language mod-\nels. In Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), 5356–5371.\nNangia, N.; Vania, C.; Bhalerao, R.; and Bowman, S. R.\n2020. CrowS-Pairs: A Challenge Dataset for Measuring So-\ncial Biases in Masked Language Models. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 1953–1967. Online: Asso-\nciation for Computational Linguistics.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. GloVe:\nglobal vectors for word representation. In EMNLP, 1532–\n1543.\nPeters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark,\nC.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized\nword representations. In Proc. of NAACL-HLT.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language Models are Unsupervised\nMultitask Learners. In OpenAI blog 1.8, 9.\nRavishankar, V .; Kulmizev, A.; Abdou, M.; Søgaard, A.; and\nNivre, J. 2021. Attention Can Reﬂect Syntactic Structure\n(If You Let It). In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Computational\nLinguistics: Main Volume, 3031–3045. Online: Association\nfor Computational Linguistics.\nSalazar, J.; Liang, D.; Nguyen, T. Q.; and Kirchhoff, K.\n2020. Masked Language Model Scoring. In Proceedings\nof the 58th Annual Meeting of the Association for Com-\nputational Linguistics, 2699–2712. Online: Association for\nComputational Linguistics.\nSchick, T.; and Sch ¨utze, H. 2020. Rare Words: A Major\nProblem for Contextualized Embeddings and How to Fix it\nby Attentive Mimicking. In Proc. of AAAI.\nSchick, T.; Udupa, S.; and Sch ¨utze, H. 2021. Self-\nDiagnosis and Self-Debiasing: A Proposal for Reducing\nCorpus-Based Bias in NLP. Computing Research Reposi-\ntory, arXiv:2103.00453.\nSennrich, R.; Haddow, B.; and Birch, A. 2016. Neural\nMachine Translation of Rare Words with Subword Units.\nIn Proceedings of the 54th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long Pa-\npers), 1715–1725. Berlin, Germany: Association for Com-\nputational Linguistics.\nTan, Y . C.; and Celis, L. E. 2019. Assessing Social and In-\ntersectional Biases in Contextualized Word Representations.\nIn Advances in Neural Information Processing Systems 32,\n13230–13241. Curran Associates, Inc.\nVashishth, S.; Upadhyay, S.; Tomar, G. S.; and Faruqui, M.\n2019. Attention interpretability across nlp tasks. arXiv\npreprint arXiv:1909.11218.\nWang, A.; and Cho, K. 2019. BERT has a Mouth, and It\nMust Speak: BERT as a Markov Random Field Language\nModel. In Proceedings of the Workshop on Methods for Op-\ntimizing and Evaluating Neural Language Generation, 30–\n36. Minneapolis, Minnesota: Association for Computational\nLinguistics.\nWeischedel, R.; Palmer, M.; Marcus, M.; Hovy, E.; Pradhan,\nS.; Ramshaw, L.; Xue, N.; Taylor, A.; Kaufman, J.; Fran-\nchini, M.; et al. 2013. Ontonotes release 5.0 ldc2013t19.\nLinguistic Data Consortium, Philadelphia, PA, 23.\nWiegreffe, S.; and Pinter, Y . 2019. Attention is not not Ex-\nplanation. In EMNLP-IJCNLP, 11–20. Hong Kong, China:\nAssociation for Computational Linguistics.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-\nson, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y .; Plu, J.;\nXu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest, Q.; and\nRush, A. M. 2020. Transformers: State-of-the-Art Natural\nLanguage Processing. In EMNLP: System Demonstrations,\n38–45.\nZhao, J.; Wang, T.; Yatskar, M.; Cotterell, R.; Ordonez, V .;\nand Chang, K.-W. 2019. Gender Bias in Contextualized\nWord Embeddings. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), 629–634. Minneapolis,\nMinnesota: Association for Computational Linguistics.\nZhao, J.; Wang, T.; Yatskar, M.; Ordonez, V .; and Chang,\nK.-W. 2018a. Gender Bias in Coreference Resolution: Eval-\nuation and Debiasing Methods. In Proceedings of the 2018\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), 15–20. Association for\nComputational Linguistics.\nZhao, J.; Zhou, Y .; Li, Z.; Wang, W.; and Chang, K.-W.\n2018b. Learning Gender-Neutral Word Embeddings. In\nProc. of EMNLP, 4847–4853.\nZhou, D.; Bousquet, O.; Lal, T. N.; Weston, J.; and\nSch¨olkopf, B. 2003. Learning with Local and Global Con-\nsistency. In NIPS.\nZhu, Y .; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun,\nR.; Torralba, A.; and Fidler, S. 2015. Aligning Books and\nMovies: Towards Story-Like Visual Explanations by Watch-\ning Movies and Reading Books. In ICCV.\n11962"
}