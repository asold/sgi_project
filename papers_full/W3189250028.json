{
  "title": "Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion",
  "url": "https://openalex.org/W3189250028",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227383772",
      "name": "Suglia, Alessandro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2367584840",
      "name": "Gao, Qiaozi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222562181",
      "name": "Thomason, Jesse",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221460244",
      "name": "Thattai, Govind",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221460245",
      "name": "Sukhatme, Gaurav",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3036497058",
    "https://openalex.org/W3173781631",
    "https://openalex.org/W3029418112",
    "https://openalex.org/W2899235916",
    "https://openalex.org/W2122398932",
    "https://openalex.org/W2835434549",
    "https://openalex.org/W3034578524",
    "https://openalex.org/W3008329436",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2070338807",
    "https://openalex.org/W3179326652",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2962684798",
    "https://openalex.org/W2963726321",
    "https://openalex.org/W2948384082",
    "https://openalex.org/W1136193735",
    "https://openalex.org/W3166792158",
    "https://openalex.org/W3090098073",
    "https://openalex.org/W3101009265",
    "https://openalex.org/W3204201015",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W3113184484",
    "https://openalex.org/W3110824791",
    "https://openalex.org/W2896739098",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2879390606",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3188987547",
    "https://openalex.org/W3197445977",
    "https://openalex.org/W3108144224",
    "https://openalex.org/W2293350124",
    "https://openalex.org/W1978289112",
    "https://openalex.org/W2799002257",
    "https://openalex.org/W3100923070",
    "https://openalex.org/W3021013305",
    "https://openalex.org/W2774005037",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W3097484130",
    "https://openalex.org/W3169306793",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3112356180",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3176974620",
    "https://openalex.org/W3107069568",
    "https://openalex.org/W2236233024",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2251353663",
    "https://openalex.org/W3030193665",
    "https://openalex.org/W3205276578",
    "https://openalex.org/W3003205975",
    "https://openalex.org/W2963902314",
    "https://openalex.org/W3080215814",
    "https://openalex.org/W1980491396",
    "https://openalex.org/W3133495883",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3120543233",
    "https://openalex.org/W2793978524",
    "https://openalex.org/W2885804027",
    "https://openalex.org/W3034758614",
    "https://openalex.org/W2118781169",
    "https://openalex.org/W2962789679",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2898825290",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3020712669",
    "https://openalex.org/W3023306062",
    "https://openalex.org/W3113968666",
    "https://openalex.org/W3095921605",
    "https://openalex.org/W2016589492",
    "https://openalex.org/W3205013565",
    "https://openalex.org/W3025552214",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W3016923549",
    "https://openalex.org/W2776202271",
    "https://openalex.org/W2574893693",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W1602500555",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "Language-guided robots performing home and office tasks must navigate in and interact with the world. Grounding language instructions against visual observations and actions to take in an environment is an open challenge. We present Embodied BERT (EmBERT), a transformer-based model which can attend to high-dimensional, multi-modal inputs across long temporal horizons for language-conditioned task completion. Additionally, we bridge the gap between successful object-centric navigation models used for non-interactive agents and the language-guided visual task completion benchmark, ALFRED, by introducing object navigation targets for EmBERT training. We achieve competitive performance on the ALFRED benchmark, and EmBERT marks the first transformer-based model to successfully handle the long-horizon, dense, multi-modal histories of ALFRED, and the first ALFRED model to utilize object-centric navigation targets.",
  "full_text": "Embodied BERT: A Transformer Model for Embodied, Language-guided\nVisual Task Completion\nAlessandro Suglia1 ∗ Qiaozi Gao2 Jesse Thomason2,3\nGovind Thattai2 Gaurav S. Sukhatme2,3\n1Heriot-Watt University; 2Amazon Alexa AI; 3University of Southern California\nAbstract\nLanguage-guided robots performing home and\nofﬁce tasks must navigate in and interact\nwith the world. Grounding language instruc-\ntions against visual observations and actions to\ntake in an environment is an open challenge.\nWe present Embodied BERT (EmBERT),\na transformer-based model which can at-\ntend to high-dimensional, multi-modal inputs\nacross long temporal horizons for language-\nconditioned task completion.Additionally, we\nbridge the gap between successful object-\ncentric navigation models used for non-\ninteractive agents and the language-guided vi-\nsual task completion benchmark, ALFRED,\nby introducing object navigation targets for\nEmBERT training. EmBERT achieves com-\npetitive performance on the ALFRED bench-\nmark, and is the ﬁrst model to use a full,\npretrained BERT stack while handling the\nlong-horizon, dense, multi-modal histories of\nALFRED. Model code is available at the\nfollowing link: https://github.com/\namazon-research/embert\n1 Introduction\nLanguage is grounded in agent experience based\non interactions with the world (Bisk et al., 2020;\nBender and Koller, 2020). Task-oriented, instruc-\ntional language focuses on objects and interactions\nbetween objects and actors, as seen in instructional\ndatasets (Damen et al., 2020; Koupaee and Wang,\n2018), as a function of the inextricable relationship\nbetween language and objects (Quine, 1960). That\nfocus yields language descriptions of object targets\nfor manipulation such as put the strawberries on\nthe cutting board and slice them into pieces (Chai\net al., 2018). We demonstrate that predicting navi-\ngational object landmarks in addition to manipula-\ntion object targets improves the performance of an\ninstruction following agent in a rich, 3D simulated\nhome environment. We posit that object-centric\n∗Work completed via internship with Amazon Alexa AI.\nnavigation is a key piece of semantic and topo-\nlogical navigation (Kuipers and Byun, 1991) for\nEmbodied AI (EAI) agents generally.\nSubstantial modeling (Majumdar et al., 2020)\nand benchmark (Qi et al., 2020b) efforts in\nEAI navigation focus on identifying object land-\nmarks (Blukis et al., 2018) and destinations (Batra\net al., 2020b). However, for agent task completion,\nwhere agents must navigate an environment and\nmanipulate objects towards a speciﬁed goal (Gor-\ndon et al., 2017; Shridhar et al., 2020), most predict\nmovement actions without explicitly identifying\nnavigation object targets (Singh et al., 2020; Pashe-\nvich et al., 2021; Nguyen et al., 2021; Abramson\net al., 2020). We address this gap, grounding nav-\nigation instructions like Head to the sink in the\ncorner by predicting the spatial locations of the\ngoal sink object at each timestep (Figure 1).\nTransformer-based models in EAI score the\nalignment between a language instruction and an\nalready-completed path (Majumdar et al., 2020) or\nintroduce recurrence by propagating part of the hid-\nden state to the next timestep (Hong et al., 2020).\nThe former requires beam search over sequences\nof environment actions, which is not feasible when\nactions cannot be undone, such as slicing an\napple. The latter introduces a heavy memory\nrequirement, and is feasible only with short trajec-\ntories of four to six steps. We overcome both limi-\ntations by decoupling the embedding of language\nand visual features from the prediction of what\naction to take next in the environment. We ﬁrst\nembed language and visual observations at single\ntimesteps using a multi-modal transformer archi-\ntecture, then train a transformer decoder model to\nconsume sequences of such embeddings to decode\nactions (Figure 3).\nWe introduce Embodied BERT (EmBERT),\nwhich implements these two key insights:\n1. Object-centric Navigation uniﬁes the dis-\njoint navigation and interaction action se-\narXiv:2108.04927v2  [cs.CV]  4 Nov 2021\nFront Left Back Right\nt=0\n Turn \nLeft\nAction Object BBox Parent BBoxGoal Inst. Step Inst.\nPut a \ncooked \npotato in \nthe sink.\nHead to the \nsink in the \ncorner.\nEmBERT\nTransformer \nEmbedding\nTransformer \nDecoder\nMove\nForward\nt=1\nPut a \ncooked \npotato in \nthe sink.\nHead to the \nsink in the \ncorner.\nTransformer \nEmbedding\nTransformer \nDecoder\nt=6\nPut a \ncooked \npotato in \nthe sink.\nPick up the \npotato in \nfront of \nyou.\nTransformer \nEmbedding\nTransformer \nDecoder\nPickup\n...\n...\n...\n...\nFigure 1: Embodied BERT. EmBERT attends to object detections in a panoramic view around an agent, then\npredicts an action and both a target object and target object parent for both navigation and manipulation actions.\nFor example, at timesteps t = 0, 1 above, the model must predict the sink object target and its parent, the\ncountertop, while at t = 6it predicts both the object potato to pick up and the sink on which it rests.\nquences in ALFRED, giving navigation ac-\ntions per-step object landmarks.\n2. Decoupled Multimodal Transformers en-\nable extending transformer based multimodal\nembeddings and sequence-to-sequence pre-\ndiction to the ﬁfty average steps present in\nALFRED trajectories.\n2 Related Work\nNatural language guidance of robots (Tellex et al.,\n2020) has been explored in contexts from furniture\nassembly (Tellex et al., 2011) to quadcoptor ﬂight\ncontrol (Blukis et al., 2019).\nEmbodied AI. For task completion benchmarks,\nactions like pickup must be coupled with ob-\nject targets in the visual world, with speciﬁca-\ntion ranging from mask prediction only (Shridhar\net al., 2020) to proposals for full low level grip-\nper control (Batra et al., 2020a). Similarly, navi-\ngation benchmarks incorporate objects as targets\nin tasks like object navigation (Qi et al., 2020b;\nBatra et al., 2020b; Kurenkov et al., 2020), and\nexplicitly modeling those objects assists generally\nat navigation success (Shrivastava et al., 2021; Qi\net al., 2020a, 2021). Many successful modeling\napproaches for navigation benchmarks incorporate\nmultimodal transformer models that require large\nmemory from recurrence (Hong et al., 2020), beam\nsearch over potential action sequences (Majum-\ndar et al., 2020), or shallow layers without large-\nscale pretraining to encode long histories (Pashe-\nvich et al., 2021; Magassouba et al., 2021). In this\nwork, we incorporate navigation object targets into\nthe ALFRED task completion benchmark (Shridhar\net al., 2020), and decouple transformer-based mul-\ntimodal state embedding from transformer-based\ntranslation of state embeddings to action and ob-\nject target predictions. In addition, differently from\nother approaches that train from scratch their lan-\nguage encoder, we successfully exploit the BERT\nstack in our multi-modal architecture. In this way,\nEmBERT can be applied to other language-guided\ntasks such as VLN and Cooperative Vision-and-\nDialog Navigation (Thomason et al., 2019).\nLanguage-Guided Task Completion. Table 1\nsummarizes how EmBERT compares to current\nALFRED modeling approaches. ALFRED lan-\nguage instructions are given as both a single high\nlevel goal and a sequence of step-by-step instruc-\ntions (Figure 2). At each timestep, we encode the\ngoal instruction and a predicted current step-by-\nstep instruction. We train EmBERT to predict when\nto advance to the next instruction, a technique in-\ntroduced by LWIT (Nguyen et al., 2021).\nEmBERT uses a panoramic view space to see\nall around the agent. Rather than processing dense,\nsingle vector representations (Shridhar et al., 2020;\nSingh et al., 2020; Pashevich et al., 2021; Kim\net al., 2021; Blukis et al., 2021), EmBERT attends\ndirectly over object bounding box predictions em-\nbedded with their spatial relations to the agent, in-\nspired by LWIT (Nguyen et al., 2021) and a recur-\nrent VLN BERT model (Hong et al., 2020). We\nsimilarly follow prior work (Singh et al., 2020; Pa-\nshevich et al., 2021; Nguyen et al., 2021; Kim et al.,\n2021; Zhang and Chai, 2021) in predicting these\nbounding boxes as object targets for actions like\nPickup, rather than directly predicting a dense\nobject segmentation mask (Shridhar et al., 2020).\nConsider the step heat the mug of water in the\nmicrowave, where the visual observation before\nLanguage Obs. Visual Obs. Historical Obs. Inference\nGoal Inst. Inst. Views Features As Hidden Mask Nav Obj.\nStructure Split Inputs States Pred. Pred.\nSEQ2SEQ (Shridhar et al., 2020) \u0017 \u0017 Single Dense \u0017 LSTM Direct \u0017\nMOCA (Singh et al., 2020) \u0013 \u0017 Single Dense \u0017 LSTM BBox \u0017\nET (Pashevich et al., 2021) \u0017 \u0017 Single Dense TF \u0017 BBox \u0017\nLWIT (Nguyen et al., 2021) \u0013 \u0013 Multi BBox \u0017 LSTM BBox \u0017\nABP (Kim et al., 2021) \u0013 \u0017 Multi Dense \u0017 LSTM BBox \u0017\nHITUT (Zhang and Chai, 2021) \u0013 \u0013 Single BBox SG \u0017 BBox \u0017\nHLSM (Blukis et al., 2021) \u0013 - Single Dense SG+Map \u0017 Direct \u0017\nEmBERT \u0013 \u0013 Multi BBox \u0017 TF BBox \u0013\nTable 1: Model comparison. EmBERT uses a multimodal transformer (TF) to embed language instructions and\ndetected objects in a panoramic view, and a transformer decoder to produce action and object predictions. Ours is\nthe ﬁrst ALFRED model to add object prediction to navigation steps. Other methods maintain history by taking\nprevious transformer states (TF) (Pashevich et al., 2021), subgoal prediction structures (SG) (Zhang and Chai,\n2021; Blukis et al., 2021), or maintained voxel maps (Blukis et al., 2021) as input.\nturning the microwave on and after turning the\nmicrowave off are identical. Transformer encod-\nings of ALFRED’s large observation history are\npossible only with shallow networks (Pashevich\net al., 2021) that cannot take advantage of large\nscale, pretrained language models used on shorter\nhorizons (Hong et al., 2020). We decouple multi-\nmodal transformer state encoding from sequence to\nsequence state to action prediction, drawing inspira-\ntion from the AllenNLP SQuAD (Rajpurkar et al.,\n2016) training procedure (Gardner et al., 2017).\nOur EmBERT model is the ﬁrst to utilize an\nauxiliary, object-centric navigation prediction loss\nduring joint navigation and manipulation tasks,\nbuilding on prior work that predicted only the di-\nrection of the target object (Storks et al., 2021)\nor honed in on landmarks during navigation-only\ntasks (Shrivastava et al., 2021). While mapping\nenvironments during inference has shown promise\non both VLN (Fang et al., 2019; Chen et al., 2021)\nand ALFRED (Blukis et al., 2021), we leave the\nincorporation of mapping to future work.\n3 The ALFRED Benchmark\nThe ALFRED benchmark (Shridhar et al., 2020)\npairs household task demonstrations with written\nEnglish instructions in 3d simulated rooms (Kolve\net al., 2017). ALFRED tasks are from seven cat-\negories: PICK & PLACE , STACK & PLACE , PICK\nTWO & PLACE , CLEAN & PLACE , HEAT & PLACE ,\nCOOL & PLACE , and EXAMINE IN LIGHT . Each\ntask involves one or more objects that need to be\nmanipulated, for example an apple, and a ﬁnal re-\nceptacle on which they should come to rest, for ex-\nample a plate. Many tasks involve intermediate\nstate changes, for example HEAT & PLACE requires\ncooking the target object in a microwave.\nSupervision Data. Each ALFRED episode\ncomprises an initial state for a simulated room,\nlanguage instructions, planning goals, and an ex-\npert demonstration trajectory. The language in-\nstructions are given as a high-level goal instruc-\ntion Ig, for example Put a cooked egg in the sink,\ntogether with a sequence of step-by-step instruc-\ntions ⃗I, for example Turn right and go to the\nsink, Pick up the egg on the counter to the right\nof the sink , . . .The planning goals P (or sub-\ngoals) are tuples of goals and arguments, such as\n(SliceObject, Apple) that unpack to low-\nlevel sequences of actions like picking up aknife,\nperforming a slice action on an apple, and\nputting the knife down on a countertop. The\nexpert demonstration trajectory T is a sequence of\naction and object mask pairs, whereTj = (aj, Mj).\nEach step-by-step instruction Ii corresponds to\na sub-sequence of the expert demonstration, Tj:k\ngiven by alignment lookup ma(i) = (j, k) and to a\nplanning goal Pb by alignment lookup mp(i) =b.\nFor example, in Figure 2, instruction I0 corre-\nsponds to a GotoLocation navigation goal, as\nwell as a sequence of turning and movement API\nactions that a model must predict.\nModel Observations. At the beginning of each\nepisode in timestep t = 0, an ALFRED agent re-\nceives the high-level and step-by-step language in-\nstructions Ig, ⃗I. At every timestep t, the agent\nreceives a 2d, RGB visual observation representing\nthe front-facing agent camera view, VF. ALFRED\nmodels produce an action at from among 5 naviga-\ntion (e.g., Turn Left, Move Forward, Look\nUp) and 7 manipulation actions (e.g., Pickup,\nToggleOn, Slice), as well as an object mask\nTurn \nRight\n(GotoLocation, box) Turn Right, ... ∅, ...\n(PickupObject, Pencil2) Look Down, Pickup, Look Up ∅,.....,∅ \n(GotoLocation, lamp) Turn Right, ... ∅, ...\n(ToggleObject, Lamp1) Look Up, Toggle On ∅, \nViews and BBoxes: Front, Left, Back, Right\n(GotoLocation, L4) Turn Right, ... (     ,     ), ...\nFigure 2: EmBERT Auxiliary Predictions. ALFRED provides goal and step-by-step language instructions that\nare aligned with planner goals and sequences of trajectory actions in an expert demonstration (top). EmBERT\nadditionally identiﬁes navigational object targets in a panoramic view (bottom). EmBERT predicts an object\ntarget and its higher visibility parent receptacle, such as the table on which the box rests.\nMt. Predicted action at and mask Mt are executed\nin the ALFRED environment to yield the next vi-\nsual observation. For navigation actions, prediction\nMt is ignored, and there is no training supervision\nfor objects associated with navigation actions.\nEmBERT Predictions. EmBERT gathers addi-\ntional visual data (Figure 2). After every naviga-\ntion action, we turn the agent in place to obtain left,\nbackwards, and right visual frames VL, VB, VR.\nFollowing prior work (Singh et al., 2020), we run\na pretrained Mask-RCNN (He et al., 2017) model\nto extract bounding boxes from our visual observa-\ntions at each view. We train EmBERT to select the\nbounding box which has the highest intersection-\nover-union with Mt (more details in Section 4).\nWe deﬁne a navigation object target for naviga-\ntion actions. For navigation actions taken during\nlanguage instruction Ii, we examine the frameVFk\nat time k for Tk; ma(i) = (j, k). We identify the\nobject instance O of the class speciﬁed in the plan-\nning goal Pmb(i) in VFk. We deﬁne this object O\nas the navigation object target for all navigation\nactions in Tj:k by pairing those actions with ob-\nject mask MO to be predicted during training. We\nalso add a training objective to predict the parent\nreceptacle P(O) of O. Parent prediction enables\nnavigating to landmarks such as the table for in-\nstructions like Turn around and head to the box on\nthe table, where the box compared to the table on\nwhich it rests (Figure 2).\n4 Embodied BERT\nEmBERT uses a transformer encoder for jointly\nembedding language and visual tokens and an\ntransformer decoder for long-horizon planning and\nobject-centric navigation predictions (Figure 3).\n4.1 Multimodal encoder\nWe use OSCAR (Li et al., 2020) as a backbone\ntransformer module to fuse language and visual\nfeatures at each ALFRED trajectory step. We\nobtain subword tokens for the goal instruction\nIg = {g1, g2, . . . , gn} and the step-by-step instruc-\ntion Ij = {i1, i2, . . . , im} using the WordPiece to-\nkenizer (Wu et al., 2016) and process the sequence\nas: [CLS] Ig [SEP] Ij [SEP], using token type\nids to distinguish the goal and step instructions. We\nderive token embeddings L ∈ R(m+n+3)×de using\nthe BERT (Devlin et al., 2019) embedding layer,\nwhere de is the embedding dimensionality.\nWe provide EmBERT with object-centric repre-\nsentations by using MaskRCNN (He et al., 2017)\nfeatures to represent detected objects in every\nframe of the panorama view. We freeze the weights\nof a MaskRCNN model ﬁne-tuned for AI2-THOR\nframes (Singh et al., 2020). We ﬁx the number\nof object detections in the front view VF to 36,\nwhile limiting those in the side views to 18. We\nrepresent each object o ∈ O as an embedding\no ∈ Rdo, which is a concatenation of: 1) detec-\ntion ResNet (He et al., 2016) features; 2) bounding\nbox coordinates; 3) bounding box relative area; and\nt=28\nPickup\n[CLS] Put two sets ... [SEP] pick up the ... ...\nGoal Instruction Step Instruction\n[SEP]\nObject Detections\nBERT Embedding Layer Object Embedding MLP\n... ... ...\nOSCAR - Multimodal Transformer Backbone\n... ... ...\nOL\nH\nH0 Hm+n\nht\nht-1\nht-2\nht-3\n...\n...\nsi\nsi-1\nTransformer Dec\nA, at-1\nh̃t\nh̃t-1\nh̃t-2\nh̃t-3\n...\n...\nAction Prediction \n(Eq 1)\nObject Prediction \n(Eqs 2 and 3)\nObject Receptacle \nPrediction\nVisual Region \nClassiﬁcation\nNext Instruction \nStep Prediction\nYes | No\nTable Keys Book Pillow ...\n*\nat:\nFigure 3: Proposed Embodied BERT model. A multimodal encoder embeds goal- and step-level instructions\nalongside object detections from a panoramic view around the agent. This encoder produces a temporally indepen-\ndent hidden state ht. A sequence of such hidden states are attended by a segment-level recurrent action decoder\nto produce time-dependent states ˜ht. EmBERT is trained in segments si to balance gradient ﬂow over time with\nmemory constraints, and previous segments are cached to be attended over in future timesteps. Time-dependent\nstate ˜ht is used to predict the next action, whether to start attending to the next step-by-step instruction, what object\nto target in the environment, that object’s parent receptacle, and detected object classes.\n4) vertical and horizontal heading of the object re-\nlated to the current agent position, following prior\nwork (Storks et al., 2021). These representations\nmake up the observed object embeddings O. We\nuse a one layer MLP to map object embeddings\nof dimensionality do to size de.1 The multi-modal\ntransformer backbone consumes the token and ob-\nject embeddings to produce multi-modal hidden\nstates H ∈ Rm+n+|O|×de. We obtain these state\nrepresentations, ht, for each timestep t by com-\nputing an element-wise product between H0 and\nHm+n, the hidden state of the [CLS] token and\nthe last [SEP] token placed between language to-\nkens and objects, similar in spirit to the approach\ndescribed in (Zhou et al., 2020). In this way, we\ncan generate temporally independent agent states\nfor an entire trajectory resulting in a sequence of\nstates {h1, h2, . . . ,h|T|}.\n4.2 Segment-Level Recurrent Action Decoder\nThe ALFRED challenge requires models to learn\nto complete action sequences averaging 50 steps\nand spanning multiple navigation and manipula-\n1In our experiments, in order to reuse the visual embed-\nding available in the OSCAR checkpoint, we use an additional\none layer MLP to adapt our visual features to the visual em-\nbeddings space learned by OSCAR.\ntion sub-goals. However, due to the quadratic com-\nplexity of the self-attention mechanism, feeding\nlong sequences to transformers is computationally\nexpensive (Beltagy et al., 2020). Inspired by the\nTransformerXL model (Dai et al., 2019), we design\nthe Segment-Level Recurrent Action Decoder ar-\nchitecture that models long trajectories with recur-\nrent segment-level state reuse. At training time we\ndivide trajectories into temporal segments of size\ns. Given two consecutive segments, si and si+1,\nEmBERT caches the representations generated for\nsegment si. The computed gradient does not ﬂow\nfrom si+1 to si, but cached representations are used\nas extended context. When predicting the next ac-\ntion, the model can still perform self-attention over\nthe previous segment representations, effectively in-\ncorporating additional contextual information that\nspans an high number of previous timesteps.\nThe TransformerXL model is intended as an\nencoder-only architecture which is not able to\nperform cross-attention with some encoder hid-\nden states. Therefore, we introduce two novel\nelements to its architecture: 1) encoder hidden\nstates cache; 2) cross-attention over encoder states.\nFirst, our extended context is composed of both\nagent state representations and hidden states from\nthe previous segment si. In addition, to perform\ncross-attention between decoder and encoder hid-\nden states, we modify the TransformerXL self-\nattention mechanism following common practice\nin designing transformer decoders (Vaswani et al.,\n2017). EmBERT encodes the previous actions for\nthe current timestep at−1 and extracts an action\nembedding at from a learnable embedding matrix\nA ∈ R|A|×da. In the TransformerXL’s multi-head\nself-attention layers, we generate keys and values\nfrom the agent state representations (encoder) and\nqueries from the action embeddings (decoder). We\nobtain time-dependent agent state representations\n{˜h1, ˜h2, . . . ,˜h|T|} as output.\nGiven time-dependent hidden states, the model\npredicts action and object mask outputs. We learn\na probability distribution over the agent actions A\nby using a two layer feedforward network (FFN)\nwith dropout and GeLU (Hendrycks and Gimpel,\n2016) activation receiving the hidden state ˜ht for\nthe timestep t:\n˜h1\nt = GeLU(˜htW1) P(at|˜ht) =softmax(˜h1\nt W2), (1)\nwhere W1 ∈ Rde×de and W2 ∈ Rde×|A|are two\nweight matrices. We use sequence-based cross-\nentropy loss (Sutskever et al., 2014), LA, to su-\npervise the action prediction task. In addition, we\nderive time-dependent ﬁne-grained representations\nof token and object embeddings. We use condi-\ntional scaling (Dumoulin et al., 2018) to fuse the\ndecoder hidden state ˜ht with the embedding H to\nproduce the time-dependent embeddings ˜H:\n˜ c= Wt˜h ˜Hi = ˜ c·Hi,i={1,...,(m+n+|O|)}, (2)\nwhere Wt ∈ Rde×de is a weight matrix used to\nadapt the representation of the original decoder\nhidden state ˜h. We predict target objects by se-\nlecting one bounding box among the detections in\nVFfor manipulation actions, or any view for nav-\nigation actions. We treat object mask prediction\nas a classiﬁcation task where the model ﬁrst ex-\ntracts time-dependent object embeddings ˜O = ˜Hi,\ni = {(m + n), . . . ,(m + n + |O|)}, and then gen-\nerates logits for each object as follows:\n˜ o1\ni = GeLU( ˜OW1\no) P(oi|˜Oi) =softmax(˜ o1\ni W2\no), (3)\nwhere W1\no ∈ Rde×de and W2\no ∈ Rde×1 are two\nweight matrices. At training time, we determine the\ntarget object by using the Intersection-Over-Union\nscore between the predicted object masks generated\nby MaskRCNN for each object and the gold object\nmask. To supervise this classiﬁcation task, we use\nsequence-based cross-entropy loss, LO.\n4.3 Auxiliary tasks\nDuring the EmBERT training, we jointly optimize\nLA, LO, and several auxiliary tasks.\nNext Instruction Prediction. Several existing\nmodels for ALFRED encode the sequence of lan-\nguage instructions I together with the goal (Ta-\nble 1), or concatenate step-by-step instructions.\nThese simpliﬁcations can prevent the model from\ncarefully attending to relevant parts of the visual\nscene. EmBERT takes the ﬁrst instruction at time\nt = 0, and performs add an auxiliary prediction\ntask to advance from instruction Ij to instruction\nIj+1. To supervise the next-instruction decision,\nwe create a binary label for each step of the tra-\njectory that indicates whether that step is the last\nstep for a speciﬁc sub-goal, as obtained by ma(i).\nWe use a similar FNN as Equation 1to model a\nBernoulli variable used to decide when to advance\nto the next instruction. We denote the binary cross-\nentropy loss used to supervise this task as LINST .\nObject Target Predictions. EmBERT predicts\na target object for navigation actions, together with\nthe receptacle object containing the target, for ex-\nample a table on which a box sits (Figure 2).\nFor these tasks, we use an equivalent prediction\nlayer to the one used for object prediction. We\ndenote the cross-entropy loss associated with these\ntask by LNAV and LRECP .\nVisual Region Classiﬁcation. Class-\nconditioned representations are useful for\nagent manipulation, especially when combined\nwith hand-crafted procedures for object selec-\ntions (Singh et al., 2020). Inspired by masked\nregion modeling tasks (Chen et al., 2020b;\nShrivastava et al., 2021), we select with %15\nprobability some objects part of the agent view in\na given timestep t and we ask the model to predict\ntheir classes. Given the instruction Turn around\nand walk to the book on the desk, at the very ﬁrst\ntimestep of the trajectory it is likely that none\nof the mentioned objects are visible. Thus, we\nassume that at the last step of a sub-goal the agent\nwill have in view the objects associated with the\ninstruction. For the prediction task, we directly\nuse the time-dependent object embeddings ˜O and\nuse an FFN (similar to Equation 1) to estimate a\nprobability distribution over the ALFRED object\nlabels. We use a cross-entropy loss denoted by\nLV RCas supervision for this task.\nLeaderboard Test Fold Performance\nSeen Unseen\nModel Task (PLW) GC (PLW) Task (PLW) GC (PLW)\nSEQ2SEQ (Shridhar et al., 2020) 3.98 ( 2.02) 9.42 ( 6.27) .39 ( 0.08) 7.03 ( 4.26)\nHITUT (Zhang and Chai, 2021) 21.27 (11.10) 29.97 (17.41) 13.87 ( 5.86) 20.31 (11.51)\nMOCA (Singh et al., 2020) 22.05 (15.10) 28.29 (22.05) 5.30 ( 2.72) 14.28 ( 9.99)\nHLSM (Blukis et al., 2021) 25.11 ( 6.69) 35.79 (11.53) 16.29 ( 4.34) 27.24 ( 8.45)\nLWIT (Nguyen et al., 2021) 30.92 (25.90) 40.53 (36.76) 9.42 ( 5.60) 20.91 ( 16.34)\nEMBERT 31.77 (23.41) 39.27 (31.32) 7.52 ( 3.58) 16.33 (10.42)\nET (Pashevich et al., 2021) 38.42 (27.78) 45.44 (34.93) 8.57 ( 4.10) 18.56 (11.46)\nABP (Kim et al., 2021) 44.55 ( 3.88) 51.13 ( 4.92) 15.43 ( 1.08) 24.76 ( 2.22)\nTable 2: Test Fold Performance. Path weighted metrics are given in parentheses.\n5 Experiments and Results\nEmBERT achieves competitive performance with\nstate of the art models on the ALFRED leaderboard\ntest sets (Table 2), surpassing all but ET (Pashevich\net al., 2021) and ABP (Kim et al., 2021) on Seen\ntest fold performance (Table 3) at the time of writ-\ning. Notably, EmBERT achieves this performance\nwithout augmenting ALFRED data with additional\nlanguage instructions, as is done in ET (Pashe-\nvich et al., 2021), or visual distortion as used in\nABP (Kim et al., 2021).\nImplementation Details. EmBERT is im-\nplemented using AllenNLP (Gardner et al.,\n2017), PyTorch-Lightning, 2 and Huggingface-\nTransformers (Wolf et al., 2019). We train using the\nAdam optimizer with weight ﬁx (Loshchilov and\nHutter, 2017), learning rate 2e−5, and linear rate\nscheduler without warmup steps. We use dropout\nof 0.1 for the hidden layers of the FFN modules\nand gradient clipping of 1.0 for the overall model\nweights. Our TransformerXL-based decoder is\ncomposed of 2 layers, 8 attention heads, and uses\na memory cache of 200 slots. At training time,\nwe segment the trajectory into 10 timesteps. In\norder to optimize memory consumption, we use\nbucketing based on the trajectory length. We use\nteacher forcing (Williams and Zipser, 1989) to su-\npervise EmBERT during the training process. To\ndecide when to stop training, we monitor the aver-\nage between action and object selection accuracy\nfor every timestep based on gold trajectories. The\nbest epoch according to that metric computed on\nthe validation seen set is used for evaluation. The\ntotal time for each epoch is about 1 hour for a to-\ntal of 20 hours for each model conﬁguration using\nEC2 instances p3.8xlarge using 1 GPU.\nAction Recovery Module. For obstacle avoid-\nance, if a navigation action fails, for example the\n2https://www.pytorchlightning.ai/\nagent choosing MoveAhead when facing a wall,\nwe take the next most conﬁdent navigation action at\nthe following timestep, as in MOCA (Singh et al.,\n2020). We introduce an analogous object interac-\ntion recovery procedure. When the agent chooses\nan interaction action such as Slice, we ﬁrst se-\nlect the bounding box of highest conﬁdence to re-\ntrieve an object interaction mask. If the resulting\nAPI action fails, for example if the agent attempts\nto Slice a Kettle object, we choose the next\nhighest conﬁdence bounding box at the following\ntimestep. The ALFRED challenge ends an episode\nwhen an agent causes 10 such API action failures.\nComparison to Other Models. Table 2 gives\nEmBERT performance against top and baseline\nmodels on the ALFRED leaderboard at the time\nof writing. Seen and Unseen sets refer to tasks\nin rooms that were or were not seen by the agent\nat training time. We report Task success rate and\nGoal Conditioned (GC) success rate. Task success\nrate is the average number of episodes completed\nsuccessfully. Goal conditioned success rate is more\nforgiving; each episode is scored in [0, 1] based\non the number of subgoals satisﬁed, for example,\nin a STACK & PLACE task if one of two mugs\nare put on a table, the GC score is 0.5 (Shrid-\nhar et al., 2020). Path weighted success penalizes\ntaking more than the number of expert actions nec-\nessary for the task.\nEmBERT outperforms MOCA (Singh et al.,\n2020) on Unseen scenes, and several models on\nSeen scenes. The primary leaderboard metric is\nUnseen success rate, measuring models’ general-\nization abilities. Among competitive models, Em-\nBERT outperforms only MOCA at Unseen gen-\neralization success. Notably, EmBERT remains\ncompetitive on Unseen path-weighted metrics, be-\ncause it does not perform any kind of exploration\nor mapping as in HLSM (Blukis et al., 2021) and\nValidation Fold Performance\nEMBERT Seen Unseen\nInit Weights #SB Mem Nav O P(O) VRC Task GC Task GC\nOSCAR 18 200 \u0013 \u0013 \u0013 28.54 (22.88) 38.69 (31.28) 1.46 ( .72) 10.19 ( 6.25)\nOSCAR 18 200 \u0013 \u0013 34.76 (28.46) 41.30 (35.50) 3.66 ( 1.55) 12.61 ( 7.49)\nOSCAR 18 200 \u0013 \u0013 36.22 (27.05) 44.57 (35.23) 4.39 ( 2.21) 13.03 ( 7.54)\nOSCAR 18 200 \u0013 37.44 (28.81) 44.62 (36.41) 5.73 ( 3.09) 15.91 ( 9.33)\nOSCAR 18 200 23.66 (17.62) 29.97 (24.16) 2.31 ( 1.24) 12.08 ( 7.62)\nBERT 18 200 \u0013 \u0013 26.46 (19.41) 35.70 (27.04) 3.53 ( 1.77) 13.02 ( 7.57)\nOSCAR 9 200 \u0013 \u0013 \u0013 29.30 (20.14) 36.28 (27.21) 3.06 ( 1.13) 12.17 ( 6.69)\nOSCAR 9 200 \u0013 \u0013 31.75 (23.52) 38.80 (32.21) 2.56 ( 1.28) 12.97 ( 8.24)\nOSCAR 9 200 \u0013 \u0013 20.37 (16.30) 28.64 (23.11) 1.46 ( 0.75) 10.47 ( 6.26)\nOSCAR 9 200 \u0013 28.33 (20.77) 36.83 (28.03) 2.68 ( 1.18) 11.60 ( 6.78)\nOSCAR 9 200 27.84 (20.66) 36.59 (27.97) 2.44 ( 1.06) 11.46 ( 6.76)\nOSCAR 0 200 \u0013 \u0013 25.31 (18.79) 34.27 (26.09) 3.42 ( 1.49) 12.25 ( 7.34)\nOSCAR 9 1 \u0013 \u0013 20.98 (13.98) 33.33 (22.74) 1.10 ( 0.60) 10.33 ( 4.69)\nOSCAR 18 1 \u0013 \u0013 21.95 (12.99) 35.04 (22.31) 1.58 ( .54) 11.08 ( 6.18)\nMOCA (Singh et al., 2020) 18.90 (13.20) 28.02 (21.81) 3.65 ( 1.94) 13.63 ( 8.50)\nTable 3: Validation Fold Performance. We present ablations adjusting the number of side-view bounding boxes,\nattended memory length, with and without predicting navigation target O, target parent object P(O), and visual\nregion classiﬁcation (VRC) loss. We also explore initializing our multi-modal encoder with BERT versus OSCAR\ninitialization. The highest values per fold and metric are shown in blue. Path weighted metrics are given in\nparenthesis.\nABP (Kim et al., 2021).\nWe do not utilize the MOCA Instance Associ-\nation in Time module (Singh et al., 2020) that is\nmimicked by ET (Pashevich et al., 2021). That\nmodule is conditioned based on the object class of\nthe target object selected across timesteps. Because\nwe directly predict object instances without condi-\ntioning on a predicted object class, our model must\nlearn instance associations temporally in an im-\nplicit manner, rather than using such an inference\ntime “ﬁx”.\nEmBERT Ablations. Removing the object-\ncentric navigation prediction unique to EmBERT\ndecreases performance on all metrics (Table 3). We\nshow that limiting memory for the action decoder to\na single previous timestep, initializing with BERT\nrather than OSCAR weights, and limiting vision\nto the front view all decrease performance in both\nSeen and Unseen folds.\nWe ﬁnd that our parent prediction and visual re-\ngion classiﬁcation losses, however, do not improve\nperformance. To investigate whether a smaller\nmodel would beneﬁt more from these two auxil-\niary losses, we ran EmBERT with only 9 bounding\nboxes per side view, which enables ﬁtting longer\ntraining segments in memory (we use 14 timesteps,\nrather than 10). We found that those losses im-\nproved EmBERT performance on the Unseen envi-\nronment via both success rate and goal conditions\nmetrics, and improved success rate alone in Seen\nenvironments when the non-frontal views were lim-\nited to 9, rather than 18, bounding boxes. Given\nthe similar performance of EmBERT with all three\nauxiliary losses at 18 and 9 side views, we believe\nEmBERT is over-parameterized with the additional\nlosses and 18 side view bounding boxes. It is pos-\nsible that data augmentation efforts to increase the\nvolume of ALFRED training data, such as those in\nET (Pashevich et al., 2021), would enable us to take\nadvantage of the larger EmBERT conﬁguration.\n6 Conclusions\nWe apply the insight that object-centric naviga-\ntion is helpful for language-guided Embodied AI\nto a benchmark of tasks in home environments.\nOur proposed Embodied BERT (EmBERT) model\nadapts the pretrained language model transformer\nOSCAR (Li et al., 2020), and we introduce a de-\ncoupled transformer embedding and decoder step\nto enable attending over many features per timestep\nas well as a history of previous embedded states\n(Figure 1). EmBERT is the ﬁrst to bring object-\ncentric navigation to bear on language-guided, ma-\nnipulation and navigation-based task completion.\nWe ﬁnd that EmBERT’s object-centric navigation\nand ability to attend across a long time horizon\nboth contribute to its competitive performance with\nstate-of-the-art ALFRED models (Table 3).\nMoving forward, we will apply EmBERT to\nother benchmarks involving multimodal input\nthrough time, such as vision and audio data (Chen\net al., 2020a), as well as wider arrays of tasks\nto accomplish (Puig et al., 2018). To further im-\nprove performance on the ALFRED benchmark,\nwe could conceivably continue training the Mask\nRCNN model from MOCA (Singh et al., 2020) for-\never by randomizing scenes in AI2THOR (Kolve\net al., 2017) and having the agent view the\nscene from randomized vantage points with gold-\nstandard segmentation masks available from the\nsimulator. For language supervision, we could train\nand apply a speaker model for ALFRED to gener-\nate additional training data for new expert demon-\nstrations, providing an initial multimodal alignment\nfor EmBERT, a strategy shown effective in VLN\ntasks (Fried et al., 2018).\n7 Implications and Impact\nWe evaluated EmBERT only on ALFRED, whose\nlanguage directives are provided as a one-sided\n“recipe” accomplishing a task. The EmBERT ar-\nchitecture is applicable to single-instruction tasks\nlike VLN, as long as auxiliary navigation object\ntargets can be derived from the data as we have\ndone here for ALFRED, by treating the “recipe”\nof step-by-step instructions as empty. In future\nwork, we would like to incorporate our model\non navigation tasks involving dialogue (Thoma-\nson et al., 2019; de Vries et al., 2018) and real\nrobot platforms (Banerjee et al., 2020) where life-\nlong learning is possible (Thomason et al., 2015;\nJohansen et al., 2020). Low-level physical robot\ncontrol is more difﬁcult than the abstract loco-\nmotion used in ALFRED, and poses a separate\nset of challenges (Blukis et al., 2019; Anderson\net al., 2020). By operating only in simulation,\nour model also misses the full range of experience\nthat can ground language in the world (Bisk et al.,\n2020), such as haptic feedback during object ma-\nnipulation (Thomason et al., 2020, 2016; Sinapov\net al., 2014), and audio (Chen et al., 2020a) and\nspeech (Harwath et al., 2019; Ku et al., 2020) fea-\ntures of the environment. Further, in ALFRED an\nagent never encounters novel object classes at infer-\nence time, which represent an additional challenge\nfor successful task completion (Suglia et al., 2020).\nThe ALFRED benchmark, and consequently the\nEmBERT model, only evaluates and considers writ-\nten English. EmBERT inherently excludes people\nwho cannot use typed communication. By training\nand evaluating only on English, we can only specu-\nlate whether the object-centric navigation methods\nintroduced for EmBERT will generalize to other\nlanguages. We are cautiously optimistic that, with\nthe success of massively multi-lingual language\nmodels (Pires et al., 2019), EmBERT would be\nable to train with non-English language data. At\nthe same time, we acknowledge the possibility of\npernicious, inscrutable priors and behavior (Bender\net al., 2021) and the possibility for targeted, lan-\nguage prompt-based attacks (Song et al., 2021) in\nsuch large-scale networks.\nReferences\nJosh Abramson, Arun Ahuja, Arthur Brussee, Fed-\nerico Carnevale, Mary Cassin, Stephen Clark, An-\ndrew Dudzik, Petko Georgiev, Aurelia Guy, Tim\nHarley, Felix Hill, Alden Hung, Zachary Kenton,\nJessica Landon, Timothy Lillicrap, Kory Mathew-\nson, Alistair Muldal, Adam Santoro, Nikolay Savi-\nnov, Vikrant Varma, Greg Wayne, Nathaniel Wong,\nChen Yan, and Rui Zhu. 2020. Imitating interactive\nintelligence. arXiv.\nPeter Anderson, Ayush Shrivastava, Joanne Truong, Ar-\njun Majumdar, Devi Parikh, Dhruv Batra, and Ste-\nfan Lee. 2020. Sim-to-real transfer for vision-and-\nlanguage navigation. In Conference on Robot Learn-\ning (CoRL).\nShurjo Banerjee, Jesse Thomason, and Jason J. Corso.\n2020. The RobotSlang Benchmark: Dialog-guided\nrobot localization and navigation. In Conference on\nRobot Learning (CoRL).\nDhruv Batra, Angel X. Chang, Sonia Chernova, An-\ndrew J. Davison, Jia Deng, Vladlen Koltun, Sergey\nLevine, Jitendra Malik, Igor Mordatch, Roozbeh\nMottaghi, Manolis Savva, and Hao Su. 2020a. Rear-\nrangement: A challenge for embodied AI. In arXiv.\nDhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi,\nOleksandr Maksymets, Roozbeh Mottaghi, Manolis\nSavva, Alexander Toshev, and Erik Wijmans. 2020b.\nObjectNav Revisited: On Evaluation of Embodied\nAgents Navigating to Objects. In arXiv.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In ACM Conference on Fairness, Ac-\ncountability, and Transparency (FAccT).\nEmily M. Bender and Alexander Koller. 2020. Climb-\ning towards NLU: On meaning, form, and under-\nstanding in the age of data. In Association for Com-\nputational Linguistics (ACL).\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lap-\nata, Angeliki Lazaridou, Jonathan May, Aleksandr\nNisnevich, Nicolas Pinto, and Joseph Turian. 2020.\nExperience Grounds Language. In Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nValts Blukis, Dipendra Misra, Ross A. Knepper, and\nYoav Artzi. 2018. Mapping navigation instructions\nto continuous control actions with position visita-\ntion prediction. In Conference on Robot Learning\n(CoRL).\nValts Blukis, Chris Paxton, Dieter Fox, Animesh Garg,\nand Yoav Artzi. 2021. A persistent spatial seman-\ntic representation for high-level natural language\ninstruction execution. In Embodied AI Workshop\nCVPR.\nValts Blukis, Yannick Terme, Eyvind Niklasson,\nRoss A. Knepper, and Yoav Artzi. 2019. Learning to\nmap natural language instructions to physical quad-\ncopter control using simulated ﬂight. In Conference\non Robot Learning (CoRL).\nJoyce Y . Chai, Qiaozi Gao, Lanbo She, Shaohua Yang,\nSari Saba-Sadiya, and Guangyue Xu. 2018. Lan-\nguage to action: Towards interactive task learning\nwith physical agents. In International Joint Confer-\nence on Artiﬁcial Intelligence (IJCAI).\nChangan Chen, Unnat Jain, Carl Schissler, Sebastia\nVicenc Amengual Gari, Ziad Al-Halah, Vamsi Kr-\nishna Ithapu, Philip Robinson, and Kristen Grauman.\n2020a. Soundspaces: Audio-visual navigaton in 3d\nenvironments. In European Conference on Com-\nputer Vision (ECCV).\nKevin Chen, Junshen K. Chen, Jo Chuang, Marynel\nVázquez, and Silvio Savarese. 2021. Topological\nplanning with transformers for vision-and-language\nnavigation. In Computer Vision and Pattern Recog-\nnition (CVPR).\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020b. Uniter: Universal image-text\nrepresentation learning. In European Conference on\nComputer Vision (ECCV).\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988.\nDima Damen, Hazel Doughty, Giovanni Maria\nFarinella, Sanja Fidler, Antonino Furnari, Evangelos\nKazakos, Davide Moltisanti, Jonathan Munro, Toby\nPerrett, Will Price, and Michael Wray. 2020. The\nepic-kitchens dataset: Collection, challenges and\nbaselines. IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI).\nHarm de Vries, Kurt Shuster, Dhruv Batra, Devi Parikh,\nJason Weston, and Douwe Kiela. 2018. Talk the\nwalk: Navigating new york city through grounded\ndialogue. arXiv.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Chapter of the Associ-\nation for Computational Linguistics (NAACL).\nVincent Dumoulin, Ethan Perez, Nathan Schucher, Flo-\nrian Strub, Harm de Vries, Aaron Courville, and\nYoshua Bengio. 2018. Feature-wise transformations.\nDistill.\nKuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio\nSavarese. 2019. Scene memory transformer for em-\nbodied agents in long-horizon tasks. In Computer\nVision and Pattern Recognition (CVPR).\nDaniel Fried, Ronghang Hu, V olkan Cirik, Anna\nRohrbach, Jacob Andreas, Louis-Philippe Morency,\nTaylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,\nand Trevor Darrell. 2018. Speaker-follower models\nfor vision-and-language navigation. In Neural Infor-\nmation Processing Systems (NeurIPS).\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew\nPeters, Michael Schmitz, and Luke S. Zettlemoyer.\n2017. AllenNLP: A deep semantic natural language\nprocessing platform. arXiv.\nDaniel Gordon, Aniruddha Kembhavi, Mohammad\nRastegari, Joseph Redmon, Dieter Fox, and Ali\nFarhadi. 2017. Iqa: Visual question answering in\ninteractive environments. In Computer Vision and\nPattern Recognition (CVPR).\nDavid Harwath, Adrià Recasens, Dídac Surís, Galen\nChuang, Antonio Torralba, and James Glass. 2019.\nJointly discovering visual objects and spoken words\nfrom raw sensory input. International Journal of\nComputer Vision.\nKaiming He, Georgia Gkioxari, Piotr Dollár, and\nRoss B. Girshick. 2017. Mask R-CNN. Interna-\ntional Conference on Computer Vision (ICCV).\nKaiming He, X. Zhang, Shaoqing Ren, and Jian Sun.\n2016. Deep residual learning for image recognition.\nComputer Vision and Pattern Recognition (CVPR).\nDan Hendrycks and Kevin Gimpel. 2016. Gaussian er-\nror linear units (GELUs). arXiv.\nYicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-\nOpazo, and Stephen Gould. 2020. A recurrent\nvision-and-language BERT for navigation. arXiv.\nJared Sigurd Johansen, Thomas Victor Ilyevsky, and\nJeffrey Mark Siskind. 2020. The Amazing Race TM:\nRobot Edition. arXiv.\nByeonghwi Kim, Suvaansh Bhambri, Kunal Pratap\nSingh, Roozbeh Mottaghi, and Jonghyun Choi.\n2021. Agent with the big picture: Perceiving sur-\nroundings for interactive instruction following. In\nEmbodied AI Workshop CVPR .\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-\nderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gor-\ndon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi.\n2017. AI2-THOR: An Interactive 3D Environment\nfor Visual AI. arXiv.\nMahnaz Koupaee and William Yang Wang. 2018. Wik-\nihow: A large scale text summarization dataset.\narXiv.\nAlexander Ku, Peter Anderson, Roma Patel, Eugene\nIe, and Jason Baldridge. 2020. Room-across-room:\nMultilingual vision-and-language navigation with\ndense spatiotemporal grounding. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 4392–\n4412.\nBenjamin Kuipers and Yung-Tai Byun. 1991. A robot\nexploration and mapping strategy based on a seman-\ntic hierarchy of spatial representations. Robotics\nand autonomous systems, 8(1-2):47–63.\nAndrey Kurenkov, Roberto Martín-Martín, Jeff Ich-\nnowski, Ken Goldberg, and Silvio Savarese. 2020.\nSemantic and geometric modeling with neural mes-\nsage passing in 3d scene graphs for hierarchical me-\nchanical search. arXiv.\nXiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu,\nPengchuan Zhang, Lei Zhang, Lijuan Wang,\nHoudong Hu, Li Dong, Furu Wei, Yejin Choi,\nand Jianfeng Gao. 2020. Oscar: Object-semantics\naligned pre-training for vision-language tasks. In\nEuropean Conference on Computer Vision (ECCV).\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nAly Magassouba, Komei Sugiura, and Hisashi Kawai.\n2021. Crossmap transformer: A crossmodal masked\npath transformer using double back-translation for\nvision-and-language navigation. arXiv.\nArjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter\nAnderson, Devi Parikh, and Dhruv Batra. 2020. Im-\nproving vision-and-language navigation with image-\ntext pairs from the web. In European Conference on\nComputer Vision (ECCV).\nVan-Quang Nguyen, Masanori Suganuma, and\nTakayuki Okatani. 2021. Look wide and interpret\ntwice: Improving performance on interactive\ninstruction-following tasks. In International Joint\nConference on Artiﬁcial Intelligence (IJCAI).\nAlexander Pashevich, Cordelia Schmid, and Chen\nSun. 2021. Episodic Transformer for Vision-and-\nLanguage Navigation. arXiv.\nTelmo Pires, Eva Schlinger, , and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Associ-\nation for Computational Linguistics (ACL).\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li,\nTingwu Wang, Sanja Fidler, and Antonio Torralba.\n2018. Virtualhome: Simulating household activi-\nties via programs. In Computer Vision and Pattern\nRecognition (CVPR).\nYuankai Qi, Zizheng Pan, Yicong Hong, Ming-Hsuan\nYang, Anton van den Hengel, and Qi Wu. 2021.\nKnow what and know where: An object-and-\nroom informed sequential BERT for indoor vision-\nlanguage navigation. arXiv.\nYuankai Qi, Zizheng Pan, S. Zhang, A. V . Hengel, and\nQi Wu. 2020a. Object-and-action aware model for\nvisual language navigation. In European Confer-\nence on Computer Vision (ECCV).\nYuankai Qi, Qi Wu, Peter Anderson, Xin Wang,\nWilliam Yang Wang, Chunhua Shen, and Anton\nvan den Hengel. 2020b. Reverie: Remote embod-\nied visual referring expression in real indoor environ-\nments. In Computer Vision and Pattern Recognition\n(CVPR).\nWillard Van Orman Quine. 1960. Word and object .\nMIT Press.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. arXiv.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Empirical Meth-\nods in Natural Language Processing (EMNLP).\nMohit Shridhar, Jesse Thomason, Daniel Gordon,\nYonatan Bisk, Winson Han, Roozbeh Mottaghi,\nLuke Zettlemoyer, and Dieter Fox. 2020. ALFRED:\nA Benchmark for Interpreting Grounded Instruc-\ntions for Everyday Tasks. In Computer Vision and\nPattern Recognition (CVPR).\nAyush Shrivastava, Karthik Gopalakrishnan, Yang Liu,\nRobinson Piramuthu, Gokhan Tür, Devi Parikh,\nand Dilek Hakkani-Tür. 2021. VISITRON: Vi-\nsual semantics-aligned interactively trained object-\nnavigator. In Visually Grounded Interaction and\nLanguage (ViGIL) Workshop @ NAACL.\nJivko Sinapov, Connor Schenck, and Alexander\nStoytchev. 2014. Learning relational object cate-\ngories using behavioral exploration and multimodal\nperception. In International Conference on Robotics\nand Automation (ICRA).\nKunal Pratap Singh, Suvaansh Bhambri, Byeonghwi\nKim, Roozbeh Mottaghi, and Jonghyun Choi. 2020.\nMOCA: A modular object-centric approach for in-\nteractive instruction following. arXiv.\nLiwei Song, Xinwei Yu, Hsuan-Tung Peng, and\nKarthik Narasimhan. 2021. Universal adversarial at-\ntacks with natural triggers for text classiﬁcation. In\nNorth American Chapter of the Association for Com-\nputational Linguistics (NAACL).\nShane Storks, Qiaozi Gao, Govind Thattai, and Gokhan\nTur. 2021. Are we there yet? learning to localize\nin embodied instruction following. In HAI @ AAAI\n2021.\nAlessandro Suglia, Antonio Vergari, Ioannis Konstas,\nYonatan Bisk, Emanuele Bastianelli, Andrea Vanzo,\nand Oliver Lemon. 2020. Imagining grounded con-\nceptual representations from perceptual information\nin situated guessing games. In Conference on Com-\nputational Linguistics (COLING).\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\narXiv preprint arXiv:1409.3215.\nStefanie Tellex, Nakul Gopalan, Hadas Kress-Gazit,\nand Cynthia Matuszek. 2020. Robots that use lan-\nguage. The Annual Review of Control, Robotics, and\nAutonomous Systems, 15.\nStefanie Tellex, Thomas Kollar, Steven Dickerson,\nMatthew R Walter, Ashis Gopal Banerjee, Seth\nTeller, and Nicholas Roy. 2011. Understanding nat-\nural language commands for robotic navigation and\nmobile manipulation. In AAAI Conference on Artiﬁ-\ncial Intelligence.\nJesse Thomason, Michael Murray, Maya Cakmak, and\nLuke Zettlemoyer. 2019. Vision-and-dialog naviga-\ntion. In Conference on Robot Learning (CoRL).\nJesse Thomason, Aishwarya Padmakumar, Jivko\nSinapov, Nick Walker, Yuqian Jiang, Harel Yedid-\nsion, Justin Hart, Peter Stone, and Raymond J.\nMooney. 2020. Jointly improving parsing and per-\nception for natural language commands through\nhuman-robot dialog. The Journal of Artiﬁcial Intel-\nligence Research (JAIR), 67.\nJesse Thomason, Jivko Sinapov, Maxwell Svetlik, Pe-\nter Stone, and Raymond J. Mooney. 2016. Learning\nmulti-modal grounded linguistic semantics by play-\ning “I spy”. In International Joint Conference on\nArtiﬁcial Intelligence (IJCAI).\nJesse Thomason, Shiqi Zhang, Raymond Mooney, and\nPeter Stone. 2015. Learning to interpret natural lan-\nguage commands through human-robot dialog. In\nInternational Joint Conference on Artiﬁcial Intelli-\ngence (IJCAI).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Neural Information Processing Sys-\ntems (NeurIPS).\nRonald J Williams and David Zipser. 1989. A learn-\ning algorithm for continually running fully recurrent\nneural networks. Neural computation , 1(2):270–\n280.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, et al. 2019. Huggingface’s transformers: State-\nof-the-art natural language processing. arXiv.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. arXiv.\nYichi Zhang and Joyce Chai. 2021. Hierarchical task\nlearning from language instructions with uniﬁed\ntransformers and self-monitoring. In Findings of As-\nsociation for Computational Linguistics (ACL Find-\nings).\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\nHu, Jason Corso, and Jianfeng Gao. 2020. Uni-\nﬁed vision-language pre-training for image caption-\ning and vqa. In AAAI Conference on Artiﬁcial Intel-\nligence.\nA Appendix\nA.1 Additional Auxiliary Losses\nIn this section we describe alternative auxiliary\nlosses that we designed for EmBERT training us-\ning ALFRED data. After validation, these conﬁg-\nurations did not produce results comparable with\nthe best performing model. This calls for a more\ndetailed analysis of how to adequately design and\ncombine such losses in the complex training regime\nof the ALFRED benchmark.\nMasked Language Modeling The task-oriented\nlanguage in ALFRED differs from the web crawl\ntext used to train large-scale Transformers. We\ntune our initial model weights using a masked lan-\nguage modeling objective (Devlin et al., 2019). We\nmask with a %15 probability a token among the\nones in Ig and It at the very last step of a sub-goal.\nDifferently from captions data or Wikipedia, when\nwhich such supervision should be provided is cru-\ncial. Given the instruction Turn around and walk\nto the book on the desk , at the very ﬁrst timestep\nof the trajectory it is likely that none of the men-\ntioned objects are visible. Thus, we assume that\nat the last step of a subgoal the agent will have\nin view the objects associated with the instruction.\nWe apply the same conditional scaling approach to\ngenerate time-dependent language representations\n˜L as the one used in Equation 2. We denote the\nmasked language modeling loss used for this task\nby LMLM .\nMasked Region Modeling This is analogous to\nthe Visual Region Classiﬁcation (VCR) loss that\nwe integrated in the model. The main difference is\nthat 15% of the visual features are entirely masked\n(i.e., replaced with zero values) and we ask the\nmodel to predict them given the time-dependent\nrepresentations generated by EmBERT for them.\nImage-text Matching The masked region and\nlanguage modeling losses encourage the model to\nlearn ﬁne-grained object and language token rep-\nresentations, respectively. However, we are also\ninterested in global representations that are expres-\nsive enough to encode salient information of the\nvisual frames. For this reason, we design an addi-\ntional loss LIM . Given the state representation for\nthe current timestep t, EmBERT predicts whether\nthe current visual features can be associated with\nthe corresponding language features or not. We\nmaximize the cosine similarity between the visual\nfeatures of the current timestep t and the corre-\nsponding language features while, at the same time,\nminimizing the cosine similarity between the cur-\nrent visual features and other language instructions\nin the same batch. In this task, just like when mod-\neling the robot state, we use ˜L0 as the language\nfeatures and ˜Lm+n as the visual features. We de-\nﬁne LIM the same way as the contrastive loss in\nCLIP (Radford et al., 2021). However, we expect\nthe model to use the time-dependent representation\nof the agent state in order to truly understand the\nmeaning of a language instruction. In this case the\nmeaning of an instruction can be appreciated only\nafter several timesteps when the corresponding se-\nquence of actions has been executed.\nA.2 EmBERT Asset Licenses\nAI2THOR (Kolve et al., 2017) is released under\nthe Apache-2.0 License, while the ALFRED bench-\nmark (Shridhar et al., 2020) is released under the\nMIT License.",
  "topic": "Embodied cognition",
  "concepts": [
    {
      "name": "Embodied cognition",
      "score": 0.834644615650177
    },
    {
      "name": "Transformer",
      "score": 0.7731671333312988
    },
    {
      "name": "Computer science",
      "score": 0.7461982369422913
    },
    {
      "name": "Robot",
      "score": 0.5349276065826416
    },
    {
      "name": "Language understanding",
      "score": 0.5256713032722473
    },
    {
      "name": "Task (project management)",
      "score": 0.5239796042442322
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5038177371025085
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48784446716308594
    },
    {
      "name": "Language model",
      "score": 0.4384080767631531
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4379575252532959
    },
    {
      "name": "Bridge (graph theory)",
      "score": 0.43116265535354614
    },
    {
      "name": "Engineering",
      "score": 0.1534346640110016
    },
    {
      "name": "Voltage",
      "score": 0.06539630889892578
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 5
}