{
    "title": "LLMs, Truth, and Democracy: An Overview of Risks",
    "url": "https://openalex.org/W4406758458",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2752505373",
            "name": "Coeckelbergh Mark",
            "affiliations": [
                "University of Vienna"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6642785147",
        "https://openalex.org/W6633133542",
        "https://openalex.org/W4309712795",
        "https://openalex.org/W7034850685",
        "https://openalex.org/W4256483923",
        "https://openalex.org/W4250088333",
        "https://openalex.org/W4323314533",
        "https://openalex.org/W4247276202",
        "https://openalex.org/W4399465031",
        "https://openalex.org/W4319943379",
        "https://openalex.org/W4384562149",
        "https://openalex.org/W3140445995",
        "https://openalex.org/W4386929988",
        "https://openalex.org/W4236335686",
        "https://openalex.org/W6603815092",
        "https://openalex.org/W2897614784",
        "https://openalex.org/W2890058952",
        "https://openalex.org/W3004493409",
        "https://openalex.org/W4313044530",
        "https://openalex.org/W6677814126",
        "https://openalex.org/W6844894340",
        "https://openalex.org/W2031404295",
        "https://openalex.org/W2618694529",
        "https://openalex.org/W4396763928"
    ],
    "abstract": "Abstract While there are many public concerns about the impact of AI on truth and knowledge, especially when it comes to the widespread use of LLMs, there is not much systematic philosophical analysis of these problems and their political implications. This paper aims to assist this effort by providing an overview of some truth-related risks in which LLMs may play a role, including risks concerning hallucination and misinformation, epistemic agency and epistemic bubbles, bullshit and relativism, and epistemic anachronism and epistemic incest, and by offering arguments for why these problems are not only epistemic issues but also raise problems for democracy since they undermine its epistemic basis– especially if we assume democracy theories that go beyond minimalist views. I end with a short reflection on what can be done about these political-epistemic risks, pointing to education as one of the sites for change.",
    "full_text": "ORIGINAL ARTICLE\nScience and Engineering Ethics (2025) 31:4\nhttps://doi.org/10.1007/s11948-025-00529-0\nAbstract\nWhile there are many public concerns about the impact of AI on truth and knowl -\nedge, especially when it comes to the widespread use of LLMs, there is not much \nsystematic philosophical analysis of these problems and their political implications. \nThis paper aims to assist this effort by providing an overview of some truth-related \nrisks in which LLMs may play a role, including risks concerning hallucination and \nmisinformation, epistemic agency and epistemic bubbles, bullshit and relativism, \nand epistemic anachronism and epistemic incest, and by offering arguments for why \nthese problems are not only epistemic issues but also raise problems for democ-\nracy since they undermine its epistemic basis– especially if we assume democracy \ntheories that go beyond minimalist views. I end with a short reflection on what can \nbe done about these political-epistemic risks, pointing to education as one of the \nsites for change.\nKeywords Artificial intelligence · Truth · Democracy · Epistemic agency · \nBullshit\nIntroduction\nCurrently there is growing interest in the topic of truth and large language models \n(LLMs). LLMs are a form of generative AI that can recognize and generate text. They \nuse machine learning (in particular a type of neural network called a transformer \nmodel) and are trained on large data sets. LLMs can be used for a wide range of tasks \n(for example online search and writing code) but perhaps the most famous applica -\ntion is generative AI in the form of chatbots. When given a prompt, chatbots such \nas ChatGPT (OpenAI), Bard (Google), Llama (Meta), and Bing Chat (Microsoft) \nReceived: 5 October 2024 / Accepted: 14 January 2025 / Published online: 23 January 2025\n© The Author(s) 2025\nLLMs, Truth, and Democracy: An Overview of Risks\nMark Coeckelbergh1\n \r Mark Coeckelbergh\nmark.coeckelbergh@univie.ac.at\n1 Department of Philosophy, University of Vienna, Vienna, Austria\n1 3\nM. Coeckelbergh\nproduce text in reply. Use of this technology has spread rapidly; it is now widely used \nacross sectors and domains, including politics. For example, political campaigns have \nused LLMs to interact with voters, answer questions they might have, and send them \npersonalized messages. Microtargeting campaigns use online data to tailor such mes-\nsages to individuals. LLM technology can also be used to analyse public sentiment \non social media regarding political issues, simulate debate scenarios, draft speeches, \nand create persuasive message to influence politicians and voters.\nThere are public concerns about truth and LLMs since these technologies do not \nalways give us the truth– sometimes they create fake information– and are used as \nvehicles of misinformation. As for instance Marcus and Davis ( 2023) have pointed \nout, these technologies often produce incorrect or misleading information; they are \nmade to generate responses that sound plausible but may in fact be wrong. The sys -\ntems have been ‘trained to optimize the goal of producing one plausible-sounding \nword after another rather than actually engage with the meaning of language.’ But \nLLMs are also used on purpose to spread fake information or to manipulate people. \nA relevant early example of AI being used for political manipulation purposes is \nthe “Cambridge Analytica” case, in which the 2016 Trump campaign collaborated \nwith a data analysis firm and which involved using data from social media to send \ntailored messages to specific voter segments in order to influence their political opin-\nions (Cadwalladr & Graham-Harrison, 2018; Rosenberg et al., 2018). Today, LLMs \ncan assist this microtargetting, as they can easily and automatically generate text that \nresonates with different audiences. There are more knowledge-related concerns. For \nexample, there is the suspicion that LLMs, used in combination with digital social \nmedia, may increase political polarisation. In other words, LLMs may be a danger for \ndemocracy. Yet in AI ethics and philosophy of AI there is not much academic litera-\nture yet that systematically analyses these knowledge-related problems in a way that \noffers a clear overview and synthesis of the issues and connects them to discussions \nabout democracy.\nThat being said, in the literature on ethics and philosophy of AI in general there is \nincreasing attention to knowledge-related issues, including issues concerning truth. \nConsider for instance work on explainability (Wachter et al., 2017), bias (Ntoutsi et \nal., 2020), epistemic bubbles (Nguyen, 2020), ‘technology-based belief’ (Freiman, \n2023), and the influence of AI on what becomes public knowledge (Wihbey, 2024). \nThere is also work on how AI may threaten democracy (Nemitz, 2018; Kreps & \nKriner, 2023; Jungherr, 2023; Coeckelbergh, 2023, 2024), which sometimes includes \nalso attention to epistemic issues (see again Wihbey and Coeckelbergh). However, \nwhile most of these epistemic problems have been discussed for some years now, a \nclear overview and synthesis of truth-related problems in the light of LLMs and their \nimplications for democracy is missing. Generally, more work is needed that connects \nepistemic and political issues raised by AI, including but not limited to LLMs.\nThis paper aims to assist this project by presenting a brief overview and analysis \nof how LLMs may create problems for truth. I show that and how these are not only \nepistemological issues but also political ones, in particular problems for democracy. \nI end with a short reflection on what could possibly be done about this, ending with \npointing to education as a site for change.\n1 3\n4 Page 2 of 13\nLLMs, Truth, and Democracy: An Overview of Risks\nLet me first sketch the political and philosophical context of these truth-related \nissues.\nPolitical and Philosophical Context\nTruth is a highly contested concept and it is not an exaggeration to say that we are \nliving in and through a crisis of knowledge and a crisis of truth. Truth is fragile, ques-\ntioned, and attacked. There are not only many philosophical definitions and theories \nof truth (elsewhere I discuss different truth theories in the light of LLMs), but as \nfor instance a concept such as “post-truth” signals, truth is also a public, indeed a \npolitical issue. Before LLMs, there were already truth-related problems in contem -\nporary politics. For example, what counts as a reliable source of truth online? What \nif politicians deceive citizens about the truth or, worse, do no longer care about the \ntruth? How shall democracy deal with pluralism of views about the common good in \nsociety? How can we find epistemic common ground, both within nations and glob -\nally? What if citizens lack good quality knowledge to participate in decisions? And \nwhat is and should be the role of emotions in democracy? How idealistic or realistic \nshould democracy theories be when it comes to assumptions about knowledge? There \nis no such thing as a Garden of Eden in which there was a perfect democracy and no \nproblems with truth–a situation which is then supposedly destroyed by LLMs. Also \nbefore LLMs, democracies already struggled with (1) realizing democracy as an ideal \n(whichever ideal– there are many) and with (2) the place of knowledge and truth in \ndemocracy and in politics.\nIndeed, there is a broad question about what the place of knowledge and truth \nshould be in democracy. Traditionally there are two opposing views on this. One \nis Platonic, epistocratic, and authoritarian. It rejects democracy and sees knowl -\nedge and truth as central to the steering of the state (perhaps assuming that truth is \nobjective and that there is only one truth, calling for philosopher-kings). Another is \na modern democratic view, which is so “thin” on knowledge and truth that it reduces \ndemocracy to a procedure and functional requirements, making no demands at all \non knowledge and truth on the part of politicians or citizens, and thus avoiding hard \nquestions concerning truth and democracy. Contemporary political philosophy tends \nto be situated somewhere between these extremes. Epistemic democracy theories, for \ninstance, move more into the Platonic direction that emphasizes the need for knowl-\nedge by aiming for more knowledgeable and more intelligent decisions (Estlund, \n2008) and stressing the epistemic benefits of democratic decision making (Lande -\nmore, 2013), whereas realistic democracy theories remain closer to the mentioned \n“thin” ideas, not requiring too much of citizens in terms of knowledge for instance \n(if anything at all). They adopt Plato’s distrust of citizens’ ability to govern. For \nexample, the minimalist democracy concept is that citizens merely need to express \ntheir preferences by voting during elections, choosing between competing elites. \nAsking more from citizens is seen as unrealistic and (given their lack of knowledge) \nundesirable. Schumpeter (1942), for instance, argued that democracy is a method of \nchoosing political leaders; the citizens’ role should be restricted to selecting which \nelites will govern. Deliberative-participative (Habermas, Cohen, and others) and \n1 3\nPage 3 of 13 4\nM. Coeckelbergh\nrepublican theories (Pettit, 1997), by contrast, put more trust in citizens’ abilities to \nparticipate in political decision-making but ask that citizens be educated accordingly. \nThey thus present a third way between the horns of the Platonic dilemma (knowledge \nbut no democracy versus democracy but ignorance).\nDiscussions about LLMs and truth must be situated within this wider political \nand political-philosophical context, which deserves more analysis on its own. In this \npaper, which aims to give a brief overview of some key problems regarding LLMs, \ntruth, and democracy, however, I will not further discuss this larger context (or the \nmeanings and definitions of truth, for that matter) but focus on a number of specific \nphenomena and their significance for, and impact on, democracy.\nSome Truth-related Problems Raised by the Use of LLMs\nAI, in the form of LLMs, impacts at least the following truth-related risks and phe -\nnomena, which can be briefly enumerated as follows:\n1) Misinformation, fake news, and hallucinations (intended or not): The output of \nan LLM (a particular statement) is not true; it seems that the LLM lies to us.\n2) Uncertainty and post-truth: We can no longer distinguish between truth and false-\nhood; we are uncertain and confused.\n3) My/our truth is selected and filtered through polarization and epistemic bubbles: \nWe only (want to) hear the truth from people within our own bubble and it’s \ndifficult for us to revise our beliefs (threat to epistemic agency); we believe that \nour truth is the only truth or believe that everyone (or every group) has their own \ntruth (relativism).\n4) Bullshit and strong relativism: We no longer care about the truth, or at least our \nmachines don’t.\n5) Epistemic incest (or epistemic capture or epistemic anachronism): The truth \nbecomes trapped in the past in an AI loop, there is no room for an open future.\nLet me discuss each of these issues in more detail:\nA first truth problem raised by the behaviour of LLMs is that, when generating \ntext, they do not necessarily produce true statements. It is said that they “hallucinate”. \nWhile this metaphor is rather problematic (LLMs are not conscious and hence cannot \nhallucinate in the sense that humans can do), what is meant is that they unintendedly \nproduce false information. Instead of producing an accurate answer, they make up \nsomething, something that has no relation to reality. Generally, the developers of \nLLMs intend their models to offer an output that is true; they try to avoid hallucina -\ntions. But sometimes false information is intended: misinformation and fake news are \nproduced on purpose in order to mislead or manipulate people. For example, adver -\ntisers or political campaigners might use LLMs for this purpose. LLMs can be used \nto spread fake news, damage the reputation of opponents by impersonating them, and \ntarget voters with messages that offer misinformation and exploit specific fears and \nconcerns of certain demographic groups. If this is successful, it means that people \nlack sufficient autonomy and ‘epistemic agency’ to make up their own minds and \nform their own beliefs. (Coeckelbergh, 2023) Instead, they blindly trust the output \n1 3\n4 Page 4 of 13\nLLMs, Truth, and Democracy: An Overview of Risks\nof the LLMs, without questioning whether it is true and without forming their own \nbeliefs in a more independent way.\nLet me unpack what is going on here. The term ‘epistemic agency’ is used in \nepistemology to refer to ‘the control that agents may exercise over their beliefs’ \n(Schlosser, 2019) and relates to the question how our beliefs are formed and revised. \nThe Enlightenment idea is that people want and have control over what they believe: \nthey make up their own minds and can decide to change their beliefs. LLMs may \nreduce and undermine this control over our belief formation and revision in so far \nas the technology is used to manipulate the user’s belief formation and prevent their \nbelief revision, or in so far as the technology unintendedly does not encourage the \nuser’s own belief formation and belief revision by presenting its output in a way that \nappears true. If I am manipulated with misinformation or stuck in LLM’s hallucina -\ntions, then, I cannot make up my mind as an autonomous epistemic agent and I will \nnot be encouraged to revise my beliefs– if they are mine at all. Manipulated or mind-\nlessly using the LLM,1 I adopt the false belief on the basis of the output given by the \nLLM, without checking if it is true and without even considering changing it when \nthere is new relevant information. In principle this would be possible (I may become \naware of the manipulation or of the limits of the technology), but the technology does \nnot encourage it; instead, it encourages that I go with the output.\nWhile arguably these problems may also occur with for instance Google search or \nwhen someone gives wrong advice, LLMs are made to produce text that is convinc-\ning– without including any aim to produce the truth. This renders their (unintended) \nhallucinations extra problematic. Moreover, due to the scale and speed of related \ntechnologies such as digital social media, misinformation can spread rapidly and \nmanipulation can be conducted on a massive scale. And LLMs are also used on pur-\npose to influence voter behaviour, thus becoming part of what Tufekci ( 2014) has \ncalled ‘computational politics’: political campaigns (from the 2016 U.S. presidential \nelection to today) have used so-called microtargeting to deliver personalized political \nmessages to individuals based on their data, thus on purpose manipulating their voter \nbehaviour and spread misinformation.\nSecond, whereas the first problem still assumes that one can, in principle, rela -\ntively easily find the truth and thus contradict the lies, hallucinations, and false state-\nments, in other cases the problem is that it becomes difficult if not impossible to \nfind out the truth, for example because there is too much online misinformation in \ngeneral (not intended) or because some people on purpose want to create a situation \nin which it is no longer clear to others what is true and what is not; the uncertainty \nand confusion is then intended. LLMs might be used for this purpose: one could let \nhallucinations proliferate to create a climate of epistemic uncertainty and confusion \nabout what is true and false. The term “post-truth” refers to the purposeful creation \nof such a climate and public sphere. But it might also happen that the climate of \n1  Note that manipulation is not the same as mindless use. In the case of manipulation, a manipulator tries \nto influence the one who is manipulated in a way that is in line with the manipulator’s aims. Usually this is \nunderstood as involving hidden influence or deception, although this criterion has been questioned (Klenk, \n2022. In the case of mindless use, however, there is no manipulator or manipulation at work. The effects \non one’s epistemic agency are not intended.\n1 3\nPage 5 of 13 4\nM. Coeckelbergh\nuncertainty and confusion is not intended, but still an outcome of the widespread use \nof the technology.\nThird, the previous problems assume that there is something like “the” truth: \none truth for everyone. One universal truth, which can be shared. But in so far as \npeople find themselves in a context of polarization and epistemic bubbles (Nguyen, \n2020), created by AI, people tend to hear only their own truth: the truth of their own \ngroup or bubble. In an epistemic bubble, for example created via social media and \nAI, people are insufficiently confronted with other beliefs. They are only exposed \nto beliefs that reinforce their pre-existing beliefs while other beliefs are excluded or \nignored. Sometimes they are even actively discouraged from confronting themselves \nwith other views; in so-called echo chambers opposing views are discredited and \noutside information is deliberately rejected. One’s own view and the view of one’s \nown group or community is seen as the only truth. Moreover, in so far as AI and other \ndigital technologies lead to a fragmentation of the epistemic landscape, some people \nmight (come to) believe that everyone or every group has their own truth, that there \nis no universal truth (relativism). Furthermore, because of the bubble effect and the \nproliferation of relativist views, they might be unwilling and unable to revisit and \nrevise their beliefs, thus not exercising their epistemic agency. Why bother reflecting \non, discussing, or revising one’s beliefs if everyone has their own truth or if one is \nnot even exposed to other views? Here the issue is not uncertainty and confusion, but \nin a sense too much certainty: one is completely certain and convinced of one’s own \nbeliefs (and those of one’s group), to such an extent that one no longer listens to oth-\ners who may have different beliefs and different views.\nFourth, while in the previous cases here is still an interest in the truth– even if only \nin one’s own truth– one can also imagine a situation in which there is a serious risk \nthat people simply no longer care about truth. This can be seen as another form of \nrelativism, but a stronger one than the view that each person has their own truth. Here \nwe have the nihilist idea that truth doesn’t matter anymore at all. In that case, there is \nno longer an interest in finding out if the output of LLMs is true or not, based on facts \nor not, etc. People don’t care about it. They believe that it doesn’t matter.\nSuch a view might be influenced by the fact that LLMs themselves don’t seem \nto care about truth (or about anything for that matter). As Hicks et al. ( 2024) have \nargued, one problem today is that LLMs produce bullshit. Mobilizing Harry Frank -\nfurt’s (2005) understanding of bullshit, the authors argue that LLMs can be said to \nbe bullshitters not just because they produce nonsense or make a factual mistake, but \nsince they are not ‘concerned with truth’ and are ‘designed to produce text that looks \ntruth-apt without any actual concern for truth.’ The problem is then not only that \nLLMs do not always speak the truth (i.e. produce misinformation) but that they don’t \ncare about truth in the first place. This is true in the sense that the technologies are not \ndesigned to do so, they are not designed for truth. Instead, they are rather designed to \n‘convey convincing lines of text’– true or not. (3) In other words, they are designed \nto do what Plato would call producing “rhetoric” instead of telling the truth. LLMs \nare rhetoric machines. They are sophists. The goal is to convince and influence, not \nto speak the truth.\nYet since in the case of bullshit there is (usually) no intention to deceive, the \nauthors rightly argue that it would be wrong to see his in terms of lies. Lying is \n1 3\n4 Page 6 of 13\nLLMs, Truth, and Democracy: An Overview of Risks\nmaking a false statement with the intention to deceive. Bullshit, then, seems a bet -\nter term. (4) According to the authors, ChatGPT is ‘a bullshit machine’. (7) While \nthis argument is rather unfair towards rhetoric, which arguably also has value or can \nmake sense, it captures the difference between intended production of misinforma -\ntion (producing “lies”) and the unintended creation of an epistemic climate in which \nthere is no concern for truth. People might have the impression that both machines \nand humans are only bullshitting and, ultimately, might themselves not care about \nthe truth anymore.\nFinally, even if an LLM is not bullshitting and its output is true, there is the risk \nthat this output starts to refer less and less to the world outside the texts it has been \ntrained on, texts which are from the past. The output might also refer less and less to \nthe world outside the previous outputs. Over time, using LLMs may thus generate a \nkind of recursivity, an epistemic loop that might well retain truth, but truth that may \nnot be relevant to the world outside and gets stuck in the past. Wihbey ( 2024) has \ncalled this phenomenon of getting stuck in the past ‘epistemic anachronism’ (12) and \nepistemic ‘capture’ or epistemic ‘lock-in’. One could also call it a kind of epistemic \nincest: a situation in which over time the LLM only interacts with its own input data \nand output texts. There is no input from new opinions, beliefs, arguments. There is \nonly an output generated on the basis of data from the past (the data on which the \nLLM has been trained) and data taken from the past output of LLMs themselves. \nThere is truth in the output but the truth gets trapped in the past, so to speak, and \nbecomes less relevant– including less politically relevant.\nWhy these Issues are Problematic for Democracy\nThis brings us to the next question. Why are these issues and phenomena problematic \nfor politics? Why worry about them, apart from the obvious corruption of the epis -\ntemic landscape and indeed the deterioration of the epistemic capacities and epis -\ntemic agency of people? Are there any (other) normative reasons that give them some \npolitical significance and urgency?\nLet me offer some reasons why these epistemic phenomena are problematic for \ndemocracy, whatever other reasons there might be to worry about them.\nThe first issue is a problem for democracy since many democracy theories assume \nthat voters or participants in public debates have some true knowledge and beliefs, \nfor example about the views of political parties and about (problems in) the world, \nso that they can make up their own mind about who to vote for or what to argue in a \npublic debate. While in practice imperfect knowledge and misinformation may well \nalways have been part of political systems that call themselves democracies (Schum-\npeter, 1942), several influential normative democracy theories prescribe that voters or \nparticipants in public debates should form informed decisions and thus would want \nthat voters base their decision on true knowledge. For instance, deliberative and par-\nticipative democracy theories such Habermas’s (1996) require that citizens deliberate \nin an open, rational, and informed debate, engaging in public reasoning. This assumes \nthat citizens have access to true knowledge. Similarly, republican democracy theory \nfrom Aristotle to Pettit ( 1997) requires that citizens deliberate about the common \n1 3\nPage 7 of 13 4\nM. Coeckelbergh\ngood; this also requires access true knowledge, which enables a shared understanding \nof public interest and the common good. And the more recent epistemic democracy \ntheories (Landemore, 2013; Estlund, 2008) mentioned earlier argue that the quality \nof democratic decision-making depends on the epistemic quality of voter knowledge \n(next to adequate procedures and other factors). This again assumes that voters’ deci-\nsions should be based on true knowledge. True knowledge is also needed in order to \nenable people to change their views in a debate, if needed, in the light of knowledge \nthey didn’t have before and knowledge communicated to them by others. Without \nthis epistemic basis for epistemic agency, it is impossible to gain sufficient political \ndemocratic agency. Lack of this agency implies that democracy– whether in a thin \nsense (mere voting) or a thick sense (some form of participation in political debates \nand decision-making)– does not work (Coeckelbergh, 2023).\nThe second issue is a problem for democracy since, as Arendt (1951) has observed \nin The Origins of Totalitarianism, confusion about the difference between what is true \nand false destroys the very possibility of democracy: totalitarian regimes live from \nan, and actively create, a political-epistemic soil in which the distinction between \nwhat is true and what is false is no longer clear. Arendt writes: “The ideal subject of \ntotalitarian rule is not the convinced Nazi or the convinced Communist, but people \nfor whom the distinction between fact and fiction (i.e., the reality of experience) and \nthe distinction between true and false (i.e., the standards of thought) no longer exist.” \n(Arendt, 1951, p. 474). And even in democracies that are not (yet) totalitarian, there \nmight be the phenomenon of so-called “post-truth politics”: a political condition in \nwhich there is public anxiety about what are facts. The condition of anxiety about \nwhat is true and what are facts is often stimulated and maintained by right-populist \nmovements, which actively contribute to such a condition and try to benefit from it \npolitically. Again Arendt’s writings are relevant. In Truth and Politics  ( 1968) she \nalready warned how factual truth is vulnerable to manipulation by political forces. \nAlternative versions of reality can undermine democratic discourse. Again, the prob-\nlem is not just lying as such: when lying is used as a political strategy, the distinction \nbetween fact and fiction itself is blurred so citizens can no longer trust anything what \nis said.\nThe third issue is problematic since if everyone just sticks to their own politi -\ncal view, is not confronted with other (different) views, and is not willing to revise \ntheir (bubble’s or group’s) beliefs at all or even to open them up for discussion, \nattaining a thicker version of democracy in which citizens participate and deliberate \nbecomes impossible. The latter conception of democracy requires participation in \npublic debate in ways that presumes some openness to listen to others’ views and a \nwillingness to revise one’s own views. But in a fragmented and polarized digital epis-\ntemic landscape this becomes difficult if not impossible: everyone stays then in their \npolitical trenches. Again epistemic and political agency are impaired, in the sense that \nthere are barriers to the willingness and ability to revise one’s beliefs (Coeckelbergh, \n2023). All one can do then is (let people) vote. But the epistemic basis for this is thin. \nIn this situation of epistemic bubbles and fragmentation, there is little sense in con -\nvincing people with arguments or in public discussion. There is also little that people \nshare in terms of knowledge and beliefs; they live in different worlds. As a result, \nthere is a lack of mutual understanding and empathy. Debates then become ver -\n1 3\n4 Page 8 of 13\nLLMs, Truth, and Democracy: An Overview of Risks\nbally aggressive or even violent. People may insult one another or even threaten one \nanother. They may even (attempt to) eliminate political opponents. Others, maybe \nbecause they believe that everyone has their own truth (relativism), may stay away \nfrom public fora and social media altogether, given that they are no longer politically \nsafe spaces. Once a minimum of political safety is no longer guaranteed, democratic \npolitics becomes impossible.\nThe fourth issue, machine bullshitting and not caring about truth at all, is a prob -\nlem for democracy since in such a climate it is easy for malicious and antidemocratic \npoliticians to manipulate people. If truth becomes a mere matter of rhetoric, then the \nbest demagogues and tricksters win, if needed with the help of AI. This supports not \nonly populism but also (keeping in mind Arendt’s argument) authoritarianism, which \nloves and actively contributes to such a climate, let alone that any “thick” version \nof democracy is still possible. If there is no concern for a shared or universal truth \nat all, then it seems impossible to have a real discussion about matters of common \nconcern since there is no agreement possible about what these matters are in the first \nplace. Your political claim is then as good as everyone else’s (see also the previous \npoint) and it is no longer possible to form a basis of shared beliefs or other common \nepistemic ground needed for participative political discussion. Then discussion on \nthe basis of arguments and facts is no longer possible. Then the public sphere deterio-\nrates into a bullshit space, where nobody is interested in truth (or democracy for that \nmatter) and power is given to those with the best rhetoric. The best bullshitter wins.\nFinally, epistemic anachronism, epistemic capture, or epistemic incest are prob -\nlems for democracy since, as Wihbey ( 2024) argues, democracy needs new input, \nneeds preferences that might not have been revealed in the past, and needs also \nknowledge beyond what LLMs might be able to offer, for instance tacit knowledge. \nHere the problem is not that there is no truth– there might well be truth in the LLM’s \noutput– but that this is a truth from the past, an irrelevant by-product of AI, not a \ntruth we need today in and for our democracies. If democracies are supposed to pick \nup new signals from citizens, but if only past signals are reiterated, the knowledge \navailable to democracy is no longer up to date and no longer relevant. Then political \nactivities such as ‘agenda-setting, framing of narratives, and selection of information’ \nare outdated since ‘based on an epistemically anachronistic model.’ (24) The thick \nideal of democracy, which includes citizen participation and deliberation, is equally \nin trouble since it needs updated information and knowledge that is relevant for now. \nWhy argue and deliberate on the basis of outdated knowledge?\nMoreover, humans might have types of knowledge about the current situation that \ndo not make it into the input for the LLMs but that may nevertheless be politically \nrelevant, for example intuitions, emotions and/or (other kind of) embodied kind of \nknowledge. Wihbey warns that AI in general may not be very good in picking up \nsignals related to tacit knowledge: if ‘’what is considered true, interesting, and use -\nful by humans is continually mediated by AI, which then reinforces past ideas and \npreferences,’ then ‘humans may not be able to access emerging, organic ground-truth \nsignals from fellow citizens about their dispositions on issues, potentially silenc -\ning important intuitions, emotions, tacit knowledge, gut feelings, and experiences.’ \n(13) This narrowing down of truth to past truths and truths in the form of disposi -\ntions that can be easily be made explicit and put into text, is not good, if not disas -\n1 3\nPage 9 of 13 4\nM. Coeckelbergh\ntrous for democracy, which is supposed to take into account, and deliberate about, \nwhat is true and interesting now and what citizens think and feel now. And the latter \nkind of knowledge may be at least partly situated outside the texts that circulate in \nthe incestuous LLM data economy. Humans may pick up such signals, intuitions, \nand emotions; machines lack this embodied and situational kind of knowledge. This \nmeans that while they may pick up on some texts in which these politically relevant \nemotions and intuitions are expressed, they are likely to miss the relevant texts and \nnon-textual expressions because they lack that kind of knowledge, awareness, and \nsubjectivity.\nThis is also problematic if, as Mouffe (2005) has argued, emotions are essential to \nan engaged and participative democratic politics, which is not only about reasoned \ndebate but also about political engagement and struggle motivated by emotions and \npassions. Coming from another political direction, Nussbaum (2013) has argued that \nemotions play a vital role in motivating individuals to care about others and engage \nin the political process. While emotions in politics can also be problematic, if emo -\ntions are excluded from political knowledge altogether via LLMs, then this is a threat \nto democracy.\nThe narrowing down of truth and knowledge to past truths and truth in the form of \nexplicit knowledge is also problematic since with its output LLMs may make it seem \nas if everything is already known, as if the truth is known and knowledge is complete. \nInnerarity (2024) has suggested that whereas democracy always involves decision-\nmaking under incomplete knowledge and uncertainty, AI falsely gives us the illusion \nof epistemic certainty and completeness, leaving out the ambiguities and knowledge \ngaps that are inherent in the political life. This is not only an argument against AI \ntaking over politics (Innerarity’s point, and something Arendt may be interpreted as \nhaving warned against, see Arendt, 1972); it is also a problem for using AI, in the \nform of LLMs, in democracies in general. Acknowledging uncertainty, incomplete -\nness, and indeed limits to one’s own knowledge seem essential for participating in, \nand creating, a healthy democracy.\nWhat to do about These Political-Epistemic Risks?\nHere are a number of ways in which these technology-pushed epistemic risks, in \nthis paper conceptualized as problems regarding truth, could be mitigated. Here my \naim is not to present original views, arguments, or analysis, but simply to offer some \navenues for dealing with the risks– most of which are well-known when it comes to \ngovernance of AI in general, but some of which require more attention (the role of \ntraditional media and education):\n1) Research on, and policy development in the area of, ethical-epistemic rules for \nLLMs and on how to operationalize them in practice.\n2) Regulation and oversight of the tech industry in ways that influence and shape \nthe development of new LLM technology (and more generally new AI tech) \naccording to the mentioned ethical-epistemic rules.\n3) Quality (human) journalism that also protect these rules.\n1 3\n4 Page 10 of 13\nLLMs, Truth, and Democracy: An Overview of Risks\n4) Education that raises awareness of these problems and, more generally, of the \nepistemic limitations and political risks of LLMs (and other types of AI).\nFirst, currently there are plenty of lists of ethical principles that can guide AI policy, \nbut so far, less attention has been paid to the epistemic risks involved in using AI, \nincluding LLMs. More investment in research projects, preferably interdisciplinary \nones, that help to clarify and understand these risks and work out ways to mitigate \nthe problems is badly and urgently needed, especially in the light of the mentioned \nrisks for democracy. In terms of policy, we need rules that can work at the operational \nlevel in policy and innovation, not only in the form of ethical norms but also technical \nstandards, labels, and incentives.\nSecond, these policies should inform effective regulation. LLM technology should \nnot be taken as a natural phenomenon; the mentioned risks are risks of the current \nsystems, and as humanity and as societies we have the possibility to shape their \nfuture development. Regulation and oversight should make sure that the technol -\nogy is developed in such a way that democracy is supported instead of undermined \n(Coeckelbergh, 2024). This should include nurturing and taking care of its epistemic \nbasis, and LLMs should be evaluated and regulated in this light. Currently mitigating \nthe epistemic-democratic risks created by LLMs is left too much in the hands of Big \nTech.\nThird, digital social media, in which some of the risks discussed in this paper \ncan become imminent, are not the only media we have; we also still have the tradi -\ntional media such as newspapers and TV . Traditional quality media and independent \njournalism can and should play an important role to protect democracy from threats \ndue to new technologies, and actively help to promote some of the “thick” norma -\ntive democratic values that underpin this analysis: participation, deliberation, and \nengagement. For example, instead of providing more fuel for heated conflict and \npolarization (which often happens because owners of these media want more audi -\nence in order to make more money), these media and journalists could in principle \ntry to spark real political discussion, that is, not just facilitating the expression of \nentrenched and dogmatic views but a real debate in which there is also an effort to \nfind some common ground, both in terms of shared knowledge, truth, and beliefs and \nin terms of a vision of the future. This may also help to relieve the broader knowledge \ncrisis mentioned in the beginning of this paper. But it requires re-thinking regulation \nand potentially ownership of media.\nFinally, users of LLMs and citizens should also not be taken as a kind of fixed \nfactor. It may be, as it is often said, that people get the politicians they deserve, and \nperhaps it is also the case that people get the technologies they deserve. But educa -\ntion can change people. Currently education tends to be shaped by minimalist views \nof democracy, if the topic is dealt with at all. But this situation can be changed. If we \nimprove political education in a way that includes civic education that supports richer \ndemocracy ideals (more participative and republic ideals, which require a stronger \nepistemic basis) and make people aware of the epistemic and political risks of LLMs \nand other AI technology, citizens can make better choices, have better debates about \ntechnology, and ultimately themselves contribute to the creation of better politics and \nbetter technology. That’s, at least in part, what democracy should be about.\n1 3\nPage 11 of 13 4\nM. Coeckelbergh\nFunding Open access funding provided by University of Vienna.\nThis research was funded in part by the Knowledge in Crisis project, supported by the Austrian Science \nFund (FWF) under the Clusters of Excellence programme (https://doi.org/10.55776/COE3).\nDeclarations\nCompeting Interests The author has no relevant financial or non-financial interests to disclose.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use \nis not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o n s . o r g / l i c e n \ns e s / b y / 4 . 0 /     .  \nReferences\nArendt, H. (1951). The origins of totalitarianism. Penguin.\nArendt, H. (1968[2006]). Truth and politics. In Arendt, H. (Ed.), Between past and future: Eight exercises \nin political though. (p. 223–259). Penguin.\nArendt, H. (1972). Lying in politics. In Crisis of the republic. HarperCollins.\nCadwalladr, C., & Graham-Harrison, E. (2018, March 17). Revealed: 50 million facebook profiles har -\nvested for Cambridge analytica in major data breach. The Guardian.  h t t p s  : / / w w w  . t h e g  u a r d  i a n . c  o m / n \ne w  s / 2 0 1  8 / m a  r / 1 7 / c a m b r i d g e - a n a l y t i c a - f a c e b o o k - i n fl  u e n c e - u s - e l e c t i o n       \nCoeckelbergh, M. (2023). Democracy, epistemic agency, and AI: Political epistemology in times of artifi-\ncial intelligence. AI and Ethics, 3, 1341–1350.\nCoeckelbergh, M. (2024). Why AI undermines democracy and what to do about it. Polity.\nEstlund, D. (2008). Democratic authority: A philosophical framework. Princeton University Press.\nFrankfurt, H. (2005). On bullshit. Princeton University Press.\nFreiman, O. (2023). Analysis of beliefs acquired from a conversational AI: Instruments-based beliefs, \ntestimony-based beliefs, and technology-based beliefs. Episteme. Advance online publication.  h t t p s \n: / / d o i . o r g / 1 0 . 1 0 1 7 / e p i . 2 0 2 3 . 1 2       \nHabermas, J., & Rehg, W. (1996). Between facts and norms: Contributions to a discourse theory of law \nand democracy (Transl. W. Regh). MIT Press.\nHicks, M. T., Humphries, J., & Slater, J. (2024). ChatGPT is bullshit. Ethics and Information Technology, \n26, 38. https://doi.org/10.1007/s10676-024-09775-5\nInnerarity, D. (2024). The epistemic impossibility of an artificial intelligence take–over of democracy. AI \n& Society, 39, 1667–1671. https://doi.org/10.1007/s00146-023-01632-1\nJungherr, A. (2023). Artificial intelligence and democracy: A conceptual framework. Social Media + Soci-\nety, Advance online publication. https://doi.org/10.1177/20563051231186353\nKlenk, M. (2022). (Online) manipulation: Sometimes hidden, always careless. Review of Social Economy, \n80(1), 85–105.\nKreps, S., & Kiner, D. (2023). How AI threatens democracy. Journal of Democracy, 34(3), 122–131.\nLandemore, H. (2013). Democratic reason: Politics, collective intelligence, and the rule of the many . \nPrinceton University Press.\nMarcus, G., & Davis, E. (2023). Large language models like ChatGPT say The darnedest things. Blog@\nACM Communications of the ACM, January 10, 2023.  h t t p s  : / / c a c  m . a c m  . o r g  / b l o g  c a c m / l  a r g e -  l a n g  u a \ng e - m o d e l s - l i k e - c h a t g p t - s a y - t h e - d a r n e d e s t - t h i n g s /       \nMouffe, C. (2005). On the political. Routledge.\nNemitz, P. (2018). Constitutional democracy and technology in the age of artificial intelligence. Philosoph-\nical Transactions of the Royal Society A, 15 October 2018. https://doi.org/10.1098/rsta.2018.0089\nNguyen, C. (2020). Echo chambers and epistemic bubbles. Episteme, 17(2), 141–161.\n1 3\n4 Page 12 of 13\nLLMs, Truth, and Democracy: An Overview of Risks\nNtoutsi, E., Fafalios, P., Gadiraju, U., Iosifidis, V ., Nejdl, W., Vidal, M.-E., et al. (2020). Bias in data-\ndriven artificial intelligence systems– an introductory survey. WIREs Data Mining and Knowledge \nDiscovery, 10, e1356.\nNussbaum, M. C. (2013). Political emotions: Why love matters for Justice. Harvard University Press.\nPettit, P. (1997). Republicanism: A theory of freedom and government. Clarendon.\nRosenberg, M., Confessore, N., & Cadwalladr., C. (2018, March 17). How Trump consultants exploited \nthe facebook data of millions. The New York  T i m e s .    h  t t p s :  / / w w w .  n y t i  m e s . c o m / 2 0 1 8 / 0 3 / 1 7 / u s / p o l i t i c s \n/ c a m b r i d g e - a n a l y t i c a - t r u m p - c a m p a i g n . h t m l       \nSchlosser, M. (2019). Agency. In Edward N. Zalta (Ed.), The Stanford encyclopedia of  p h i l o s o p h y    .     h t  t p  s : / /  \np l a   t o . s  t a n f o r d . e d u / a r c h i v e s / w i n 2 0 1 9 / e n t r i e s / a g e n c y /       \nSchumpeter, J. A. (1942[2003]). Capitalism, socialism, and democracy. Routledge.\nTufekci, Z. (2014). Engineering the public: Big data, surveillance and computational politics. First Mon-\nday, 19(7). https://doi.org/10.5210/fm.v19i7.4901\nWachter, S., Mittelstadt, B., & Floridi, L. (2017). Transparent, explainable, and accountable AI for robot-\nics. Science (Robotics), 2(6), eaan6080.\nWihbey, J. P. (2024). AI and epistemic risk for democracy: a coming crisis of public knowledge?, Working \npaper presented at the Conference democracy’ s mega challenges: How climate change, migration, \nand big data threaten the future of liberal democratic governance , Trinity College, Hartford, CT, \nApril 19–20, 2024.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations.\n1 3\nPage 13 of 13 4"
}