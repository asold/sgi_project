{
    "title": "A Transformer-Based Siamese Network for Change Detection",
    "url": "https://openalex.org/W4221141623",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2914546165",
            "name": "Wele Gedara Chaminda Bandara",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A2118029367",
            "name": "Vishal M. Patel",
            "affiliations": [
                "Johns Hopkins University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3027225766",
        "https://openalex.org/W3036453075",
        "https://openalex.org/W2891248708",
        "https://openalex.org/W3120467244",
        "https://openalex.org/W6790731625",
        "https://openalex.org/W6788556936",
        "https://openalex.org/W6797500923",
        "https://openalex.org/W6799693294",
        "https://openalex.org/W6797399245",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3200935312",
        "https://openalex.org/W4224301858"
    ],
    "abstract": "This paper presents a transformer-based Siamese network architecture (abbreviated by ChangeFormer) for Change Detection (CD) from a pair of co-registered remote sensing images. Different from recent CD frameworks, which are based on fully convolutional networks (ConvNets), the proposed method unifies hierarchically structured transformer encoder with Multi-Layer Perception (MLP) decoder in a Siamese network architecture to efficiently render multi-scale long-range details required for accurate CD. Experiments on two CD datasets show that the proposed end-to-end trainable ChangeFormer architecture achieves better CD performance than previous counterparts. Our code is available at https://github.com/wgcban/ChangeFormer.",
    "full_text": "A TRANSFORMER-BASED SIAMESE NETWORK FOR CHANGE DETECTION\nWele Gedara Chaminda Bandara, Vishal M. Patel\nJohns Hopkins University, Baltimore, Maryland, USA.\n{wbandar1, vpatel36}@jhu.edu\nABSTRACT\nThis paper presents a transformer-based Siamese network\narchitecture (abbreviated by ChangeFormer) for Change De-\ntection (CD) from a pair of co-registered remote sensing\nimages. Different from recent CD frameworks, which are\nbased on fully convolutional networks (ConvNets), the pro-\nposed method uniﬁes hierarchically structured transformer\nencoder with Multi-Layer Perception (MLP) decoder in a\nSiamese network architecture to efﬁciently render multi-scale\nlong-range details required for accurate CD. Experiments on\ntwo CD datasets show that the proposed end-to-end trainable\nChangeFormer architecture achieves better CD performance\nthan previous counterparts. Our code and pre-trained models\nare available at github.com/wgcban/ChangeFormer.\nIndex Terms— Change detection, transformer Siamese\nnetwork, attention mechanism, multilayer perceptron, remote\nsensing.\n1. INTRODUCTION\nChange Detection (CD) aims to detect relevant changes from\na pair of co-registered images acquired at distinct times [1].\nThe deﬁnition of change may usually vary depending on the\napplication. The changes in man-made facilities (e.g., build-\nings, vehicles, etc.), vegetation changes, and environmental\nchanges (e.g., polar ice cap melting, deforestation, damages\ncaused by disasters) are usually regarded as relevant changes.\nA better CD model is the one that can recognize these relevant\nchanges while avoiding complex irrelevant changes caused\nby seasonal variations, building shadows, atmospheric varia-\ntions, and changes in illumination conditions.\nThe existing state-of-the-art (SOTA) CD methods are\nmainly based on deep convolutional networks (ConvNets)\ndue to their ability to extract powerful discriminative fea-\ntures. Since it is essential to capture long-range contextual\ninformation within the spatial and temporal scope to iden-\ntify relevant changes in multi-temporal images, the latest\nCD studies have been focused on increasing the receptive\nﬁeld of the CD model. As a result, CD models with stacked\nconvolution layers, dilated convolutions, and attention mech-\nanisms [2] (channel and spatial attention) have been proposed\n[3]. Even though the attention-based methods are effective\nin capturing global details, they struggle to relate long-range\ndetails in space-time because they use attention to re-weight\nthe bi-temporal features obtained through ConvNets in the\nchannel and spatial dimension.\nThe recent success of Transformers (i.e., non-local self-\nattention) in Natural Language Processing (NLP) has led\nresearchers in applying transformers in various computer vi-\nsion tasks. Following the transformer design in NLP, different\narchitectures have been proposed for various computer vision\ntasks, including image classiﬁcation and image segmentation\nsuch as Vision Transformer (ViT), SEgmentation TRans-\nformer (SETR), Vision Transformer using Shifted Windows\n(Swin), Twins [4] and SegFormer [5]. These Transformer\nnetworks have comparatively larger effective receptive ﬁeld\n(ERF) than deep ConvNets - providing much stronger context\nmodeling ability between any pair of pixels in images than\nConvNets.\nAlthough Transformer networks have a larger receptive\nﬁeld and stronger context shaping ability, very few works\nhave been done on transformers for CD. In a more recent work\n[6], a transformer architecture is applied in conjunction with\na ConvNet encoder (ResNet18) to enhance the feature rep-\nresentation while keeping the overall ConvNet-based feature\nextraction process in place. In this paper, we show that this\ndependency on ConvNets is not necessary, and a hierarchi-\ncal transformer encoder with a lightweight MLP decoder can\nwork very well for CD tasks.\n2. METHOD\nThe proposed ChangeFormer network consists of three main\nmodules as shown in Fig. 1: a hierarchical transformer en-\ncoder in a Siamese network to extract coarse and ﬁne features\nof bi-temporal image, four feature difference modules to com-\npute feature differences at multiple scales, and a lightweight\nMLP decoder to fuse these multi-level feature differences and\npredict the CD mask.\n2.1. Hierarchical Transformer Encoder\nGiven an input bi-temporal image, the hierarchical trans-\nformer encoder generates ConvNet-like multi-level features\narXiv:2201.01293v7  [cs.CV]  2 Sep 2022\nTransformer\nBlock \nTransformer\nBlock \nTransformer\nBlock \nTransformer\nBlock \nTransformer\nBlock \nTransformer\nBlock \nTransformer\nBlock \nTransformer\nBlock \nDifference\nModule\nDifference\nModule\nDifference\nModule\nDifference\nModule\nMLP \nUpsampling\nModule (x4)\nBinary Change Map\nMLP\nUpsampling\nLightweight MLP Decoder\nPre-change Image\nPost-change Image\nClassifier\nWeight \nsharing\nDownsamplingDownsampling\nDownsamplingDownsampling\nDownsampling Downsampling\nDownsamplingDownsampling\nMLP\nUpsampling\nHierarchical Transformer Encoder\nSequence\nReduction\nMulti-Head  \nSelf-Attention\nMLP\nDepth-wise\nConvolution\nMLP\nPositional Encoding\nFig. 1. The proposed ChangeFormer network for CD.\nwith high-resolution coarse features and low-resolution ﬁne-\ngrained features required for the CD. Concretely, given a\npre-change or post-change images of resolution H ×W ×3,\nthe transformer encoder outputs feature maps Fi with a reso-\nlution H\n2i+1 × W\n2i+1 ×Ci, where i = {1, 2, 3, 4}and Ci+1 > Ci\nwhich will be further processed through the difference mod-\nules followed by MLP decoder to obtain the change map.\n2.1.1. Transformer Block\nThe main building block of the transformer encoder is self-\nattention module. In the original work [7], self-attention is\nestimated as:\nAttention(Q, K, V ) =Softmax\n( QKT\n√dhead\n)\nV, (1)\nwhere Q, K, and V denote Query, Key, and Value, respec-\ntively, and have the same dimensions of HW ×C. How-\never, the computational complexity of eqn. (1) is O((HW )2)\nwhich prohibits its application on high-resolution images. To\nreduce the computational complexity of eqn. (1), we adopt\nthe Sequence Reduction process introduced in [8] which uti-\nlizes reduction ratio R to reduce the length of the sequence\nHW as follows:\nˆS = Reshape\n(HW\nR , C·R\n)\nS, (2)\nS = Linear (C ·R, C) ˆS, (3)\nwhere S denotes the sequence to be reduced i.e., Q, K, and\nV, Reshape(h, w) denotes tensor reshaping operation to the\none with shape of (h, w), and Linear(Cin, Cout) denotes a\nlinear-layer with Cin input channels and Cout output channels.\nThis results in a new set of Q, K, and V of size\n(HW\nR , C\n)\n,\nhence reduces the computational complexity of eqn. (1) to\nO((HW )2/R).\nTo provide positional information for transformers, we\nutilize two MLP layers along with a 3 ×3 depth-wise con-\nvolutions as follows:\nFout = MLP(GELU(Conv2D3×3(MLP(Fin)))) +Fin, (4)\nwhere Fin are the features from self-attention, and GELU de-\nnotes Gaussian Error Linear Unit activation. Our positional\nencoding scheme differs from the ﬁxed positional encoding\nutilized in previous transformer networks like ViT [9] which\nallows our ChangeFormer to take test images that are differ-\nent in resolution from the ones used during training.\n2.1.2. Downsampling Block\nGiven an input patch Fi from the i-th transformer layer of\nresolution H\n2i+1 × W\n2i+1 ×Ci, downsampling layer shrink it to\nobtain Fi+1 of resolution H\n2i+2 × W\n2i+2 ×Ci+1 which will be\nthe input to the (i + 1)-th Transformer layer. To achieve this,\nwe utilize a 3×3 Conv2D layer with kernel sizeK = 7, stride\nS = 4, and padding P = 3for the initial downsampling, and\nK = 3, S = 2, and P = 1for the rest.\n2.1.3. Difference Module\nWe utilize four Difference Modules to compute the difference\nof multi-level features of pre-change and post-change images\nfrom the hierarchical transformer encoder as shown in Fig. 1.\nMore precisely, our Difference Module consists ofConv2D,\nReLU, BatchNorm2d (BN) as follows:\nFi\ndiff = BN(ReLU(Conv2D3×3(Cat(Fi\npre, Fi\npost)))), (5)\nwhere Fi\npre and Fi\npost denote the feature maps of pre-change\nand post-change images from the i-th hierarchical layer, and\nCat denotes the tensor concatenation. Instead of comput-\ning the absolute difference of Fi\npre and Fi\npost as in [6], the\nproposed difference module learn the optimal distance metric\nat each scale during training - resulting in better CD perfor-\nmance.\n2.2. MLP Decoder\nWe utilize a simple decoder with MLP layers that aggregates\nthe multi-level feature difference maps to predict the change\nmap. The proposed MLP decoder consists of three main\nsteps.\n2.2.1. MLP & Upsampling\nWe ﬁrst process each multi-scale feature difference map\nthrough an MLP layer to unify the channel dimension and\nthen upsample each one to the size ofH/4 ×W/4 as follows:\n˜Fi\ndiff = Linear(Ci, Cebd)(Fi\ndiff)∀i, (6)\nˆFi\ndiff = Upsample((H/4, W/4), “bilinear”)(˜Fi\ndiff), (7)\nwhere Cebd denotes the embedding dimension.\n2.2.2. Concatenation & Fusion\nThe upsampled feature difference maps are then concatenated\nand fused through an MLP layer as follows:\nF = Linear(4Cebd, Cebd)(Cat(ˆF1\ndiff, ˆF2\ndiff, ˆF3\ndiff, ˆF4\ndiff)).\n(8)\n2.2.3. Upsampling & Classiﬁcation.\nWe upsample the fused feature map F to the size of H ×W\nby utilizing a 2D transposed convolution layer with S = 4\nand K = 3. Finally, the upsampled fused feature map is\nprocessed through another MLP layer to predict the change\nmask CM with a resolution of H ×W ×Ncls, where Ncls\n(=2) is the number of classes i.e.,change and no-change. This\nprocess can be formulated as follows:\nˆF = ConvTranspose2D(S = 4, K= 3)(F), (9)\nCM = Linear(Cebd, Ncls)(ˆF). (10)\n3. EXPERIMENTAL SETUP\n3.1. Datasets\nWe use two publically available CD datasets for our exper-\niments, namely LEVIR-CD [10] and DSIFN-CD [11]. The\nLEVIR-CD is a building CD dataset that contains RS im-\nage pairs of resolution 1024 ×1024. From these images,\nwe crop non-overlapping patches of size 256 ×256 and ran-\ndomly split them into three parts to make train/val/test sets\nof samples 7120/1024/2048. The DSIFN dataset is an gen-\neral CD dataset that contains the changes in different land-\ncover objects. For experiments, we create non-overlapping\npatches of size 256 ×256 from the 512 ×512 images while\nutilizing the authors’ default train/val/test sets. This results\nin 14400/1360/192 samples for training/val/test, respectively,\nfor the DSIFN dataset.\n3.2. Implementation Details\nWe implemented our model in PyTorch and trained using an\nNVIDIA Quadro RTX 8000 GPU. We randomly initialize\nthe network. During training, we applied data augmentation\nthrough random ﬂip, random re-scale (0.8-1.2), random crop,\nGaussian blur, and random color jittering. We trained the\nmodels using the Cross-Entropy (CE) Loss and AdamW opti-\nmizer with weight decay equal to 0.01 and beta values equal\nto (0.9, 0.999). The learning rate is initially set to 0.0001 and\nlinearly decays to 0 until trained for 200 epochs. We use a\nbatch size of 16 to train the model.\n3.3. Performance Metrics\nTo compare the performance of our model with SOTA meth-\nods, we report F1 and Intersection over Union (IoU) scores\nwith regard to the change-class as the primary quantitative\nindices. Additionally, we report precision and recall of the\nchange category and overall accuracy (OA).\n4. RESULTS AND DISCUSSION\nIn this section, we compare the CD performance of our\nChangeFormer with existing SOTA methods:\n• FC-EF [12]: concatenates bi-temporal images and pro-\ncesses them through a ConvNet to detect changes.\n• FC-Siam-Di [12]: is a feature-difference method,\nwhich extracts multi-level features of bi-temporal im-\nages from a Siamese ConvNet, and their difference is\nused to detect changes.\n• FC-Siam-Conc [12]: is a feature-concatenation method,\nwhich extracts multi-level features of bi-temporal im-\nages from a Siamese ConvNet, and feature concatena-\ntion is used to detect changes.\n• DTCDSCN [13]: is an attention-based method, which\nutilizes a dual attention module (DAM) to exploit the\ninter-dependencies between channels and spatial posi-\ntions of ConvNet features to detect changes.\n• STANet [14]: is an another Siamese-based spatial-\ntemporal attention network for CD.\n• IFNet [15]: is a multi-scale feature concatenation\nmethod, which fuses multi-level deep features of bi-\ntemporal images with image difference features by\nTable 1. The average quantitative results of different CD methods on LEVIR-CD [10] and DSIFN-CD [11].*\nMethod LEVIR-CD [10] DSIFN-CD [11]\nPrecision Recall F1 IoU OA Precision Recall F1 IoU OA\nFC-EF [12] 86.91 80.17 83.40 71.53 98.39 72.61 52.73 61.09 43.98 88.59\nFC-Siam-Di [12] 89.53 83.31 86.31 75.92 98.67 59.67 65.71 62.54 45.50 86.63\nFC-Siam-Conc [12] 91.99 76.77 83.69 71.96 98.49 66.45 54.21 59.71 42.56 87.57\nDTCDSCN [13] 88.53 86.83 87.67 78.05 98.77 53.87 77.99 63.72 46.76 84.91\nSTANet [14] 83.81 91.00 87.26 77.40 98.66 67.71 61.68 64.56 47.66 88.49\nIFNet [15] 94.02 82.93 88.13 78.77 98.87 67.86 53.94 60.10 42.96 87.83\nSNUNet [16] 89.18 87.17 88.16 78.83 98.82 60.60 72.89 66.18 49.45 87.34\nBIT [6] 89.24 89.37 89.31 80.68 98.92 68.36 70.18 69.26 52.97 89.41\nChangeFormer (ours) 92.05 88.80 90.40 82.48 99.04 88.48 84.94 86.67 76.48 95.56\n*All values are reported in percentage (%). Color convention: best, 2nd-best, and 3rd-best.\nFC-EF [10]\nFC-Siam-Di\n[10]\nFC-Siam-\nConc [10]\nDTCDSCN\n[11]\n BIT [4]\n ChangeFormer \n(ours) \n GT\nPre-Change\nImg.\nPost-Change\nImg.\nTextTextTextTextTextTextText\nLEVIR-CDDSIFN-CD\nFig. 2. Qualitative results of different CD methods on LEVIR-CD [10] and DSIFN-CD [11].\nmeans of attention modules for change map recon-\nstruction.\n• SNUNet [16]: is a multi-level feature concatenation\nmethod, in which a densely connected (NestedUNet)\nSiamese network is used for change detection.\n• BIT [6]: is a transformer-based method, which uses\na transformer encoder-decoder network to enhance the\ncontext-information of ConvNet features via semantic\ntokens followed by feature differencing to obtain the\nchange map.\nTable 1 presents the results of different CD methods on\nthe test-sets of LEVIR-CD [10] and DSIFN-CD [11]. As can\nbe seen from the table, the proposed ChangeFormer network\nachieves better CD performance in terms of F1, IoU, and OA\nmetrics. In particular, our ChangeFormer improves previous\nSOTA in F1/IoU/OA by 1.2/2.2/0.1% and 20.0/44.3/6.4% for\nLEVIR-CD and DSIFN-CD, respectively. In addition, Fig. 2\ncompares the visual quality of different SOTA methods on test\nimages from LEVIR-CD and DSIFN-CD. As highlighted in\nred, our ChangeFormer captures much ﬁner details compared\nto the other SOTA methods. These quantitative and qualita-\ntive comparisons show the superiority of our proposed CD\nmethod over the existing SOTA methods.\n5. CONCLUSION\nIn this paper, we proposed a transformer-based Siamese\nnetwork for CD. By utilizing a hierarchical transformer\nencoder in a Siamese architecture with a simple MLP de-\ncoder, our method outperforms several other recent CD\nmethods that employ very large ConvNets like ResNet18\nand U-Net as the backbone. We also show better perfor-\nmance in terms of IoU, F1 score, and overall accuracy than\nrecent ConvNet-based (FC-EF, FC-Siam-DI, and FC-Siam-\nConc), attention-based (DTCDSCN, STANet, and IFNet),\nand ConvNet+Transformer-based (BIT) methods. Hence,\nthis study shows that it is unnecessary to depend on deep-\nConvNets, and a hierarchical transformer in a Siamese net-\nwork with a lightweight decoder can work very well for CD.\n6. ACKNOWLEDGMENT\nThis work was supported by NSF CAREER award 2045489.\n7. REFERENCES\n[1] Wele Gedara Chaminda Bandara and Vishal M Pa-\ntel, “Revisiting consistency regularization for semi-\nsupervised change detection in remote sensing images,”\narXiv preprint arXiv:2204.08454, 2022.\n[2] Wele Gedara Chaminda Bandara, Jeya Maria Jose Vala-\nnarasu, and Vishal M Patel, “Spin road mapper: Extract-\ning roads from aerial images via spatial and interaction\nspace graph reasoning for autonomous driving,” arXiv\npreprint arXiv:2109.07701, 2021.\n[3] Qian Shi, Mengxi Liu, Shengchen Li, Xiaoping Liu,\nFei Wang, and Liangpei Zhang, “A deeply supervised\nattention metric-based network and an open aerial im-\nage dataset for remote sensing change detection,” IEEE\nTransactions on Geoscience and Remote Sensing, 2021.\n[4] Salman Khan, Muzammal Naseer, Munawar Hayat,\nSyed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah, “Transformers in vision: A survey,” arXiv\npreprint arXiv:2101.01169, 2021.\n[5] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandku-\nmar, Jose M Alvarez, and Ping Luo, “Segformer: Sim-\nple and efﬁcient design for semantic segmentation with\ntransformers,” arXiv preprint arXiv:2105.15203, 2021.\n[6] Hao Chen, Zipeng Qi, and Zhenwei Shi, “Remote sens-\ning image change detection with transformers,” IEEE\nTransactions on Geoscience and Remote Sensing, 2021.\n[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin, “Attention is all you need,” in Ad-\nvances in neural information processing systems, 2017,\npp. 5998–6008.\n[8] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan,\nKaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling\nShao, “Pyramid vision transformer: A versatile back-\nbone for dense prediction without convolutions,” arXiv\npreprint arXiv:2102.12122, 2021.\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, et al., “An image is worth\n16x16 words: Transformers for image recognition at\nscale,” arXiv preprint arXiv:2010.11929, 2020.\n[10] Hao Chen and Zhenwei Shi, “A spatial-temporal\nattention-based method and a new dataset for remote\nsensing image change detection,” Remote Sensing, vol.\n12, no. 10, pp. 1662, 2020.\n[11] Chenxiao Zhang, Peng Yue, Deodato Tapete, Liangcun\nJiang, Boyi Shangguan, Li Huang, and Guangchao Liu,\n“A deeply supervised image fusion network for change\ndetection in high resolution bi-temporal remote sensing\nimages,” ISPRS Journal of Photogrammetry and Re-\nmote Sensing, vol. 166, pp. 183–200, 2020.\n[12] Rodrigo Caye Daudt, Bertr Le Saux, and Alexandre\nBoulch, “Fully convolutional siamese networks for\nchange detection,” in 2018 25th IEEE International\nConference on Image Processing (ICIP) . IEEE, 2018,\npp. 4063–4067.\n[13] Yi Liu, Chao Pang, Zongqian Zhan, Xiaomeng Zhang,\nand Xue Yang, “Building change detection for re-\nmote sensing images using a dual-task constrained deep\nsiamese convolutional network model,” IEEE Geo-\nscience and Remote Sensing Letters , vol. 18, no. 5, pp.\n811–815, 2020.\n[14] Hao Chen and Zhenwei Shi, “A spatial-temporal\nattention-based method and a new dataset for remote\nsensing image change detection,” Remote Sensing, vol.\n12, no. 10, pp. 1662, 2020.\n[15] Chenxiao Zhang, Peng Yue, Deodato Tapete, Liangcun\nJiang, Boyi Shangguan, Li Huang, and Guangchao Liu,\n“A deeply supervised image fusion network for change\ndetection in high resolution bi-temporal remote sensing\nimages,” ISPRS Journal of Photogrammetry and Re-\nmote Sensing, vol. 166, pp. 183–200, 2020.\n[16] Sheng Fang, Kaiyu Li, Jinyuan Shao, and Zhe Li,\n“Snunet-cd: A densely connected siamese network for\nchange detection of vhr images,” IEEE Geoscience and\nRemote Sensing Letters, 2021."
}