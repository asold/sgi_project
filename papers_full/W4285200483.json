{
    "title": "Word Order Does Matter and Shuffled Language Models Know It",
    "url": "https://openalex.org/W4285200483",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2547953503",
            "name": "Mostafa Abdou",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2783756630",
            "name": "Vinit Ravishankar",
            "affiliations": [
                "University of Oslo"
            ]
        },
        {
            "id": "https://openalex.org/A2759999012",
            "name": "Artur Kulmizev",
            "affiliations": [
                "Uppsala University"
            ]
        },
        {
            "id": "https://openalex.org/A2100615786",
            "name": "Anders Søgaard",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3118608099",
        "https://openalex.org/W2004827252",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2927103915",
        "https://openalex.org/W3042925324",
        "https://openalex.org/W2973154008",
        "https://openalex.org/W2898785098",
        "https://openalex.org/W2413794162",
        "https://openalex.org/W2506931122",
        "https://openalex.org/W2077113564",
        "https://openalex.org/W2736285750",
        "https://openalex.org/W1991825895",
        "https://openalex.org/W2038255699",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W2970862333",
        "https://openalex.org/W3185070134",
        "https://openalex.org/W2932893307",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2299976354",
        "https://openalex.org/W2153076044",
        "https://openalex.org/W2250263931",
        "https://openalex.org/W2962736243",
        "https://openalex.org/W2044340178",
        "https://openalex.org/W2998925391",
        "https://openalex.org/W2962843521",
        "https://openalex.org/W3153289922",
        "https://openalex.org/W3006705448",
        "https://openalex.org/W2145755360",
        "https://openalex.org/W3174169056",
        "https://openalex.org/W3026404337",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2526072446",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W3184516261",
        "https://openalex.org/W3103054319",
        "https://openalex.org/W3152698349",
        "https://openalex.org/W2111406701",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2739827909",
        "https://openalex.org/W2132684680",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W3200434714",
        "https://openalex.org/W2195506630",
        "https://openalex.org/W1995672192",
        "https://openalex.org/W2953271402",
        "https://openalex.org/W2898662126",
        "https://openalex.org/W3156194904",
        "https://openalex.org/W2130822992",
        "https://openalex.org/W4246285815",
        "https://openalex.org/W3175606037",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4299802238",
        "https://openalex.org/W2804897457",
        "https://openalex.org/W4287119076",
        "https://openalex.org/W3102094970",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2091966899",
        "https://openalex.org/W2996908057",
        "https://openalex.org/W4306384900"
    ],
    "abstract": "Recent studies have shown that language models pretrained and/or fine-tuned on randomly permuted sentences exhibit competitive performance on GLUE, putting into question the importance of word order information. Somewhat counter-intuitively, some of these studies also report that position embeddings appear to be crucial for models' good performance with shuffled text. We probe these language models for word order information and investigate what position embeddings learned from shuffled text encode, showing that these models retain information pertaining to the original, naturalistic word order. We show this is in part due to a subtlety in how shuffling is implemented in previous work - before rather than after subword segmentation. Surprisingly, we find even Language models trained on text shuffled after subword segmentation retain some semblance of information about word order because of the statistical dependencies between sentence length and unigram probabilities. Finally, we show that beyond GLUE, a variety of language understanding tasks do require word order information, often to an extent that cannot be learned through fine-tuning.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 6907 - 6919\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nWord Order Does Matter\n(And Shufﬂed Language Models Know It)\n∗Mostafa Abdou† ∗Vinit Ravishankar‡ Artur Kulmizev§ Anders Søgaard†\n†Department of Computer Science, University of Copenhagen\n‡Language Technology Group, Department of Informatics, University of Oslo\n§Department of Linguistics and Philology, Uppsala University\n†{abdou,soegaard}@di.ku.dk\n‡vinitr@ifi.uio.no\nAbstract\nRecent studies have shown that language mod-\nels pretrained and/or ﬁne-tuned on randomly\npermuted sentences exhibit competitive perfor-\nmance on GLUE, putting into question the im-\nportance of word order information. Some-\nwhat counter-intuitively, some of these stud-\nies also report that position embeddings appear\nto be crucial for models’ good performance\nwith shufﬂed text. We probe these language\nmodels for word order information and inves-\ntigate what position embeddings learned from\nshufﬂed text encode, showing that these mod-\nels retain information pertaining to the origi-\nnal, naturalistic word order. We show this is\nin part due to a subtlety in how shufﬂing is\nimplemented in previous work – before rather\nthan after subword segmentation. Surpris-\ningly, we ﬁnd even Language models trained\non text shufﬂed after subword segmentation\nretain some semblance of information about\nword order because of the statistical depen-\ndencies between sentence length and unigram\nprobabilities. Finally, we show that beyond\nGLUE, a variety of language understanding\ntasks do require word order information, of-\nten to an extent that cannot be learned through\nﬁne-tuning.\n1 Introduction\nTransformers (Vaswani et al., 2017), when used in\nthe context of masked language modelling (Devlin\net al., 2018), consume their inputs concurrently.\nThere is no notion of inherent order, unlike in au-\ntoregressive setups, where the input is consumed\ntoken by token. To compensate for this absence\nof linear order, the transformer architecture origi-\nnally proposed in Vaswani et al. (2017) includes a\nﬁxed, sinusoidal position embedding added to each\ntoken embedding; each token carries a different\nposition embedding, corresponding to its position\nin the sentence. The transformer-based BERT (De-\nvlin et al., 2018) replaces these ﬁxed sinusoidal\n∗Equal contribution. Order was decided by a coin toss.\nFigure 1: Pearson correlations between position embed-\ndings for full-scale models; the patterns are similar to\nfully learnable absolute embeddings (Wang et al., 2021)\nand can be said to have learned something about posi-\ntion. We later demonstrate that this is not the case with\npost-BPE scrambling.\nembeddings with unique, learned embeddings per\nposition; RoBERTa (Liu et al., 2019), the model\ninvestigated in this work, does the same.\nPosition embeddings are the only source of or-\nder information in these models; in their absence,\ncontextual representations generated for tokens\nare independent of the actual position of the to-\nkens in a sentence, and the models thus resemble\nheavily overparameterised bags-of-words. Sinha\net al. (2021) pre-trained RoBERTa models on shuf-\nﬂed corpora to demonstrate that the performance\ngap between these ‘shufﬂed’ language models and\nmodels trained on unshufﬂed corpora is minor\n(when ﬁne-tuned and evaluated downstream on the\nGLUE (Wang et al., 2018) benchmark). They fur-\nther show that this gap is considerably wider when\na model is pre-trained without position embeddings.\nIn this paper, we attempt to shed some light on why\nthese models behave the way they do, and in doing\nso, seek to answer a set of pertinent questions:\n•Do shufﬂed language models still have traces\nof word order information?\n•Why is there a gap in performance between\nmodels without position embeddings and mod-\nels trained on shufﬂed tokens, with the latter\n6907\nFigure 2: Correlations between position embeddings when shufﬂing training data before segmentation (left), i.e,\nat the word level, and after segmentation (middle), i.e., at the subword level, as well as when replacing all sub-\nwords with random subwords based on their corpus-level frequencies (right). The latter removes any dependency\nbetween subword probability and sentence length. The plots show that shufﬂing before segmentation retains more\norder information than shufﬂing after, and that even when shufﬂing after segmentation, position embeddings are\nmeaningful because of the dependence between subword probability and sentence length.\nperforming better?\n•Are there NLU benchmarks, other than\nGLUE, on which shufﬂed language models\nperform poorly?\nContributions We ﬁrst demonstrate, in Sec-\ntion 3, that shufﬂed language models do contain\nword order information, and are quite responsive\nto simple tests for word order information, partic-\nularly when compared to models trained without\nposition representations. In Section 4, we demon-\nstrate that pre-training is sufﬁcient to learn this:\nposition embeddings provide the appropriate in-\nductive bias, and performing BPE segmentation\nafter shufﬂing results in sensible n-grams appear-\ning in the pre-training corpus; this gives models\nthe capacity to learn word order within smaller lo-\ncal windows. Other minor cues - like correlations\nbetween sentence lengths and token distributions -\nalso play a role. We further corroborate our analy-\nsis by examining attention patterns across models\nin Sec. 5. In Section 6, we show that, while shuf-\nﬂed models might be almost as good as their un-\nshufﬂed counterparts on GLUE tasks, there exist\nNLU benchmarks that do require word order infor-\nmation to an extent that cannot be learned through\nﬁne-tuning alone. Finally, in Section 7, we describe\nmiscellaneous experiments addressing the utility\nof positional embeddings when added just prior to\nﬁne-tuning.\n2 Models\nSinha et al. (2021) train several full-scale RoBERTa\nlanguage models on the Toronto Book Corpus (Zhu\net al., 2015) and English Wikipedia.1 Four of their\nmodels are trained on shufﬂed text, i.e., sentences\nin which n-grams are reordered at random. 2 We\ndub the original, unperturbed model ORIG , and the\nscrambled models SHUF.N1, SHUF.N2, SHUF.N3\nand SHUF.N4 depending on the size of the shufﬂed\nn-grams: SHUF.N1 reorders the unigrams in a sen-\ntence, SHUF.N2 reorders its bigrams, etc. For com-\nparison, Sinha et al. (2021) also train a RoBERTa\nlanguage model entirely without position embed-\ndings (NOPOS ), as well as a RoBERTa language\nmodel trained on a corpus drawn solely from uni-\ngram distributions of the original Book Corpus, i.e.,\na reshufﬂing of the entire corpus (SHUF.CORPUS ).\n1Training reportedly takes 72 hours on 64 GPUs.\n2The shufﬂing procedure does not reorder tokens com-\npletely at random, but moves a token in position i to a new\nposition selected at random among positions j ̸= i.\n6908\nWe experiment with their models, as well as with\nsmaller models that we can train with a smaller\ncarbon footprint. To this end, we downscale the\nRoBERTa architecture used in Sinha et al. (2021).\nConcretely, we train single-headed RoBERTa mod-\nels, dividing the embedding and feed-forward di-\nmensionality by 12, for 24 hours on a single GPU,\non 100k sentences sampled from the Toronto Book\nCorpus. To this end, we train a custom vocabulary\nof size 5,000, which we use for indexing in all our\nsubsequent experiments. While these smaller mod-\nels are in no way meant to be ﬁne-tuned and used\ndownstream, they are useful proofs-of-concept that\nwe later analyse.\n3 Probing for word order\nWe begin by attempting to ascertain the extent to\nwhich shufﬂed language models are actually ca-\npable of encoding information pertaining to the\nnaturalistic word order of sentences. We perform\ntwo simple tests on the full-scale models, in line\nwith Wang and Chen (2020): the ﬁrst of these is\na classiﬁcation task where a logistic regressor is\ntrained to predict whether a randomly sampled to-\nken precedes another in an unshufﬂed sentence,\nand the second involves predicting the position of\na word in an unshufﬂed sentence. The fact that\nwe do not ﬁne-tune any of the model parameters is\nnoteworthy: the linear models can only learn word\norder information if it reﬂects in the representations\nthe models generate somehow.\nPairwise Classiﬁcation For this experiment, we\ntrain a logistic regression classiﬁcation model on\nword representations extracted from the ﬁnal layer\nof the Transformer encoder, mean pooling over\nsub-tokens when required. For each word pair x\nand y, the classiﬁer is given a concatenation of our\nmodel m’s induced representationsm(x) ⊕m(y)\nand trained to predict a label indicating whether x\nprecedes y or not. Holding out two randomly sam-\npled positions, we use a training sets sized 2k, 5k,\nand 10k, from the Universal Dependencies English-\nGUM corpus (Zeldes, 2017) (excluding sentences\nwith more than 30 tokens to increase learnability)\nand a test set of size 2, 000. We report the mean\naccuracy from three runs.\nRegression Using the same data, we also train a\nridge-regularised linear regression model to predict\nthe position of a word p(x) in an unshufﬂed sen-\ntence, given that word’s model-induced representa-\nModel Classiﬁcation (acc.) Regression (R2)\n2k 5k 10k -\nORIG 81.50 81.74 80.40 0.68\nSHUF.N1 65.96 64.98 71.82 0.60\nNOPOS 50.41 53.35 50.22 0.03\nTable 1: Pairwise classiﬁcation and regression results.\ntion m(x). R2 score is reported per model. To pre-\nvent the regressors from memorising word to posi-\ntion mappings, we perform 6-fold cross-validation,\nwhere the heldout part of the data contains no vo-\ncabulary overlap with the corresponding train set.\nResults For both tasks (see Table 1), our results\nindicate that position encodings are particularly im-\nportant for encoding word order: Classiﬁers and re-\ngressors trained on representations from ORIG and\nSHUF.N1 achieve high accuracies and R2 scores,\nwhile those for NOPOS are close to random. Both\nORIG and SHUF.N1 appear to be better than ran-\ndom given only 2k examples. These results imply\nthat, given positional encodings and a modest train-\ning set of 2k or more examples, a simple linear\nmodel is capable of extracting word order infor-\nmation, enabling almost perfect extrapolation to\nunseen positions. Whether the position encodings\ncome from a model trained on natural or shufﬂed\ntext does not appear to matter, emphasizing that\nshufﬂed language models do indeed contain sub-\nstantial information about the original word order.\n4 Hidden word-order signals\nIn Section 3, we observed that Sinha et al. (2021)’s\nshufﬂed language models surprisingly exhibit in-\nformation about naturalistic word order. That these\nmodels contain positional information can also be\nseen by visualizing position embedding similarity.\nFigure 1 displays Pearson correlations 3 for posi-\ntion embeddings with themselves, across positions.\nHere, we see that the shufﬂed models satisfy the\nidealised criteria for position embeddings described\nby Wang et al. (2021): namely, they appear to be\na) monotonous within smaller context windows,\nand b) invariant to translation. If position embed-\nding correlations are consistent across offsets over\nthe entire space of embeddings, the model can be\nsaid to have ‘learned’ distances between tokens.\nSince transformers process all positions in parallel,\n3We see similar patterns with dot products for all our plots;\nwe use Pearson correlations to constrain our range to [−1, 1].\n6909\nand since language models without position em-\nbeddings do not exhibit such information, position\nembeddings have to be the source of this informa-\ntion. In what follows, we discuss this apparent\nparadox.\nSubword vs. word shufﬂing An important de-\ntail when running experiments on shufﬂed text, is\nwhen the shufﬂing operation takes place. When\ntokens are shufﬂed before BPE segmentation, this\nleads to word-level shufﬂing, in which sequences\nof subwords that form words remain contiguous.\nSuch sequences become a consistent, meaningful\nsignal for language modelling, allowing models\nto efﬁciently utilise the inductive bias provided by\nposition embeddings. Thus, even though our pre-\ntrained models have, in theory, not seen consecutive\ntokens in their pre-training data, they have learned\nto utilise positional embeddings to pay attention to\nadjacent tokens. The inﬂuence of this is somewhat\nvisible in Figure 2: while models trained on text\nshufﬂed before and after segmentation both exhibit\nshifts in the polarity of their position correlations,\nonly the former show bands of varying magnitude,\nsimilar to the full-scale models. Ravishankar and\nSøgaard (2021) discuss the implications of these\npatterns in a multilingual context; we hypothesise\nthat in our context, the periodicity in magnitude is\na visible artefact of the model’s ability to leverage\nposition embeddings to enable offset attention. In\nSection 5, we analyse the effect of shufﬂing the pre-\ntraining data on the models’ attention mechanisms.\nAccidental overlap In addition to the n-gram\ninformation which results from shufﬂing before\nsegmentation, we also note that short sentences\ntend to include original bigrams with high proba-\nbility, leading to stronger associations for words\nthat are adjacent in the original texts. This effect\nis obviously much stronger when shufﬂing before\nsegmentation than after segmentation. Figure 3\nshows how frequent overlapping bigrams (of any\nsort) are, comparing word and subword shufﬂing\nover 50k sentences.\nSentence length Finally, we observe some pre-\nserved information about the original word order\neven when shufﬂing is performed after segmenta-\ntion. We hypothesize that this is a side-effect of the\nnon-random relationship between sentence length\nand unigram probabilities. That unigram probabili-\nties correlate with sentence length follows from the\nfact that different genres exhibit different sentence\nFigure 3: (Cumulative) plot showing subword bigram\noverlap after shufﬂing either words or subwords, as a\npercentage of the total number of seen bigrams. We see\nthe overlap is signiﬁcant, especially when performing\nshufﬂing before segmentation.\nlength distributions (Sigurd et al., 2004; Jin and Liu,\n2017). Also, some words occur very frequently in\nformulaic contexts, e.g., thank in thank you. This\npotentially means that there is an approximately\nlearnable relationship between the distribution of\nwords and sentence boundary symbols.\nTo test for this, we train two smaller language\nmodels on unigram-sampled corpora: for the ﬁrst,\nwe use the ﬁrst 100k BookCorpus sentences as\nour corpus, shufﬂing tokens at a corpus level (yet\nkeeping the original sentence lengths). The stark\ndifference in position embedding correlations be-\ntween that and shufﬂing is seen in Figure 2. For\nthe second, we sample from two different unigram\ndistributions: one for short sentences and one for\nlonger sentences (details in Appendix B). While\nthe ﬁrst model induces no correlations at all, the\nsecond does, as shown in Figure 4, implying that\nsentence length and unigram occurrences is enough\nto learn some order information.\n5 Attention analysis\nTransformer-based language models commonly\nhave attention heads that attend to neighboring po-\nsitions (V oita et al., 2019; Ravishankar et al., 2021).\nSuch attention heads are positional and can only be\nlearned in the presence of order information. We\nattempt to visualise the attention mechanism for\npre-trained models by calculating, for each head\nand layer, the offset between a token and the token\n6910\nFigure 4: Similarity matrix between models with sen-\ntences sampled based on unigram corpus statistics; dis-\njoint vocab implies a correlation between token choice\nand sentence length.\nthat it pays maximum attention to4. We then plot\nhow frequent each offset is, as a percentage, over\n100 Book Corpus sentences, in Figure 5, where\nwe present results for two full-scale models, and\ntwo smaller models (see §2). When compared to\nNOPOS , SHUF.N1 has a less uniform pattern to its\nattention mechanism: it is likely, even at layer 0,\nto prefer to pay attention to adjacent tokens, some-\nwhat mimicking a convolutional window (Cordon-\nnier et al., 2020). We see very similar differences\nin distribution between our smaller models: Shuf-\nﬂing after segmentation, i.e., at the subword level,\ninﬂuences early attention patterns.\n6 Evaluation beyond GLUE\nSuperGLUE and WinoGrande Sinha et al.\n(2021)’s investigation is conducted on GLUE and\non the Paraphrase Adversaries from Word shuf-\nﬂing (PAWS) dataset (Zhang et al., 2019). For\nthese datasets, they ﬁnd that models pretrained on\nshufﬂed text perform only marginally worse than\nthose pretrained on normal text. This result, they\nargue can be explained in two ways: either a) these\ntasks do not need word order information to be\nsolved, or b) the required word order information\ncan be acquired during ﬁnetuning. While GLUE\nhas been a useful benchmark, several of the tasks\nwhich constitute it have been shown to be solvable\nusing various spurious artefacts and heuristics (Gu-\nrurangan et al., 2018; Poliak et al., 2018). If, for\ninstance, through ﬁnetuning, models are learning to\nrely on such heuristics as lexical overlap for MNLI\n(McCoy et al., 2019), then it is unsurprising that\ntheir performance is not greatly impacted by the\n4This method of visualisation is somewhat limited, in that\nit examines only the maximum attention paid by each token.\nWe provide more detailed plots over attention distributions in\nthe Appendix.\nFigure 5: Relative frequency of offsets between to-\nken pairs in an attention relation; the y-axis denotes\nthe percentage of total attention relations that occur\nat the offset indicated on the x-axis. We plot layers\nl ∈{1, 2, 7, 8, 11, 12}with increasing line darkness.\nlack of word order information.\nEvaluating on the more rigorous set of Super-\nGLUE tasks 5 (Wang et al., 2019) and on the\nadversarially-ﬁltered Winograd Schema examples\n(Levesque et al., 2012) of the WinoGrande dataset\n(Sakaguchi et al., 2020) produces results which\npaint a more nuanced picture compared to those\nof Sinha et al. (2021). The results, presented in\nTable 2, show accuracy or F1 scores for all mod-\nels. For two of the tasks (MultiRC (Khashabi et al.,\n2018), COPA (Roemmele et al., 2011)), we ob-\nserve a pattern in line with that seen in Sinha et al.\n(2021)’s GLUE and PAWS results: the drop in\nperformance from ORIG to SHUF.N1 is minimal\n(mean: 1.75 points; mean across GLUE tasks: 3.3\npoints)6, while that to NOPOS is more substantial\n(mean: 10.5 points; mean across GLUE tasks: 18.6\npoints).\nThis pattern alters for the BoolQ Yes/No ques-\ntion answering dataset (Clark et al., 2019), the\nCommitmentBank (De Marneffe et al., 2019), the\nReCoRD reading comprehension dataset (Zhang\net al., 2018), both the Winograd Schema tasks,\n5Results are reported for an average of 3 runs per task.\nThe RTE task is excluded from our results as it is also part of\nGLUE; RTE results can be found in Sinha et al. (2021).\n6CoLA results are excluded from the GLUE calculations\ndue to the very high variance across random seeds reported by\nSinha et al. (2021).\n6911\nand to some extent the Words in Context dataset\n(Pilehvar and Camacho-Collados, 2018). For these\ntasks we observe a larger gap between ORIG and\nSHUF.N1 (mean: 8.1 points), and an even larger\none between ORIG and NOPOS (mean: 19.78\npoints). We note that this latter set of tasks requires\ninferences which are more context-sensitive, in\ncomparison to the two other tasks or to the GLUE\ntasks.\nConsider the Winograd schema tasks, for\nexample. Each instance takes the form of a binary\ntest with a statement comprising of two possible\nreferents (blue) and a pronoun (red) such as: Sid\nexplained his theory to Mark but\nhe couldn’t convince him. The correct\nreferent of the pronoun must be inferred based\non a special discriminatory segment (underlined).\nIn the above example, this depends on a) the\nidentiﬁcation of “Sid” as the subject of “explained”\nand b) inferring that the pronoun serving as the\nsubject of “convinced” should refer to the same\nentity. Since the Winograd schema examples\nare designed so that the referents are equally\nassociated with their context 7, word order is\ncrucial8 for establishing the roles of “Sid” and\n“Mark” as subject and object of “explained” and\n“he” and “him” as those of “convinced”. If these\nroles cannot be established, making the correct\ninference becomes impossible.\nA similar reasoning can be applied to the Words\nin Context dataset and the CommitmentBank. The\nformer task tests the ability of a model to distin-\nguish the senses of a polysemous word based on\ncontext. While this might often be feasible via a\nnotion of contextual association that higher-order\ndistributional statistics are sufﬁcient for, some in-\nstances will require awareness of the word’s role\nas an argument in the sentence. The latter task in-\nvestigates the projectivity of ﬁnite clausal comple-\nments under entailment cancelling operators. This\nis dependent on both the scope of the entailment\noperator and the identity of the subject of the ma-\ntrix predicate (De Marneffe et al., 2019), both of\nwhich are sensitive to word order information.\nA ﬁnal consideration to take into account is\ndataset ﬁltering. Two of the tasks where we observe\n7e.g. Sid and Mark are both equally likely subjects/objects\nhere. Not all Winograd schema examples are perfect in this\nregard, however, which could explain why scrambled models\nstill perform above random. See Trichelair et al. (2018) for a\ndiscussion of the latter point.\n8Particularly in a language with limited morphological role\nmarking such as English.\nFigure 6: ∆, dependency arcs probing accuracy across\nlengths 1-5+, w.r.t. ORIG .\nthe largest difference between ORIG , SHUF.N1,\nand NOPOS — WinoGrande and ReCoRD — ap-\nply ﬁltering algorithms to remove cues or biases\nwhich would enable models to heuristically solve\nthe tasks. This indicates that by ﬁltering out exam-\nples containing cues that make them solvable via\nhigher order statistics, such ﬁltering strategies do\nsucceed at compelling models to (at least partially)\nrely on word order information.\nDependency Tree Probing Besides GLUE and\nPAWS, Sinha et al. (2021)’s analysis also includes\nseveral probing experiments, wherein they attempt\nto decode dependency tree structure from model\nrepresentations. They show, interestingly, that\nthe SHUF.N4, S HUF.N3 and SHUF.N2 models\nperform only marginally worse than ORIG , with\nSHUF.N1 producing the lowest scores (lower, in\nfact, than SHUF.CORPUS ). Given the ﬁndings of\nSection 3, we are interested in taking a closer look\nat this phenomenon. Here, we surmise that depen-\ndency length plays a crucial role in the probing\nsetup, where permuted models may succeed on\npar with ORIG in capturing local, adjacent depen-\ndencies, but increasingly struggle to decode longer\nones. To evaluate the extent to which this is true,\nwe train a bilinear probe (used in Hewitt and Liang\n(2019)) on top of all model representations and\nevaluate its accuracy across dependencies binned\nby length, where length between words wi and wj\nis deﬁned as |i −j|. We opt for using the bilinear\nprobe over the Pareto probing framework (Pimentel\net al., 2020), as the former learns a transformation\ndirectly over model representations, while the latter\nadds the parent and child MLP units from Dozat\net al. (2017) – acting more like a parser. We train\nprobes on the English Web Treebank (Silveira et al.,\n2014) and evaluate using UAS, the standard parsing\n6912\nModel BoolQ CB COPA MultiRC ReCoRD WiC WSC WinoGrande\nO RIG 77.6 88.2 / 87.4 61.6 67.8 / 21.9 73.5 / 72.8 67.4 73.5 62.9\nSHUF .N 1 72.4 79.7 / 82.5 59.7 66.2 / 15.0 61.1 / 60.4 63.0 62.9 55.7\nSHUF .N 2 73.1 86.6 / 85.5 60.3 64.8 / 16.1 63.1 / 62.4 63.0 65.3 57.6\nSHUF .N 4 73.5 87.9 / 87.1 60.8 66.2 / 18.2 64.6 / 63.9 62.4 65.3 59.53\nN OPOS 66.0 63.5 / 75.0 55.6 52.8 / 3.8 23.8 / 23.5 55.4 63.09 52.73\nSHUF .C ORPUS 66.7 65.6 / 73.8 56.1 52.6 / 6.4 31.0 / 30.3 57.3 65.14 51.68\nTable 2: SuperGLUE and WinoGrande results for all models. Scores displayed are: Avg. F1 / Accuracy for CB;\nF1a / Exact Match for MultiRC; F1 / Accuracy for ReCoRD ; accuracy for the remaining tasks.\nmetric.\nFigure 6 shows ∆ probing accuracy across vari-\nous dependency lengths for NOPOS and SHUF.N1,\nwith respect to ORIG 9; we include detailed ∆s for\nall models in Appendix C. For NOPOS , parsing\ndifﬁculty increases almost linearly with distance,\noften mimicking the actual frequency distribution\nof dependencies at these distances in the original\ntreebank (Appendix C); for SHUF.N1, the picture is\na lot more nuanced, with dependencies at a distance\nof 1 consistently being closer in terms of parseabil-\nity to ORIG , which, we hypothesise, is due to its\nadjacency bias.\n7 Other Findings\nRandom position embeddings are difﬁcult to\nadd post-training We tried to quantify the de-\ngree to which the inductive bias imparted by posi-\ntional embeddings can be utilised, solely via ﬁne-\ntuning. To do so, for a subset of GLUE tasks\n(MNLI, QNLI, RTE, SST-2, CoLA), we evalu-\nate NOPOS , and a variant where we randomly\ninitialised learnable position embeddings and add\nthem to the model, with the rest of the model equiv-\nalent to NOPOS . We see no improvement in results,\nexcept for MNLI, that we hypothesise stems from\nposition embeddings acting as some sort of regular-\nisation parameter. To test this, we repeat the above\nset of experiments, this time injecting Gaussian\nnoise instead; this has been empirically shown to\nhave a regularising effect on the network (Bishop,\n1995; Camuto et al., 2021). Adding Gaussian noise\nled to a slight increase in score for just MNLI, back-\ning up our regularisation hypothesis.\nModels learn to expect speciﬁc embeddings\nReplacing the positional embeddings in ORIG with\nﬁxed, sinusoidal embeddings before ﬁne-tuning\nsigniﬁcantly hurts scores on the same subset of\n9Note that Layer 13 refers to a linear mix of all model\nlayers, as is done for ELMo (Peters et al., 2018).\nGLUE tasks, implying that the models expect em-\nbeddings that resemble the inductive bias imparted\nby random embeddings, and that ﬁne-tuning tasks\ndo not have sufﬁcient data to overcome this. The\naddition of ﬁxed, sinusoidal to NOPOS also does\nnot improve model performance on a similar subset\nof tasks; this implies, given that sinusoidal embed-\ndings are already meaningful, that model weights\nalso need to learn to ﬁt the embeddings they are\ngiven, and that they need a substantial amount of\ndata to do so.\n8 On Word Order\nIn Humans It is generally accepted that a ma-\njority of languages have “canonical” or “base’\nword orderings (Comrie, 1989) (e.g. Subject-Verb-\nObject in English, and Subject-Object-Verb in\nHindi). Linguists consider word order to be a cod-\ning property— mechanisms by which abstract, syn-\ntactic structure is encoded in the surface form of\nutterances. Beyond word order, other coding prop-\nerties include, e.g. subject-verb agreement, mor-\nphological case marking, or function words such\nas adpositions. In English, word order is among\nthe most prominent coding properties, playing a\ncrucial role in the expression of the main verb’s\ncore arguments: subject and object. For more mor-\nphologically complex languages, on the other hand,\n(e.g. Finnish and Turkish), word order is primar-\nily used to convey pragmatic information such as\ntopicalisation or focus. In such cases, argument\nstructure is often signalled via case-marking, where\nnumerous orderings become possible (shift in topic\nor focus nonwithstanding). We refer the reader to\nKulmizev and Nivre (2021) for a broader discus-\nsion of these topics and their implications when\nstudying syntax through language models.\nMore generally, evidence for the saliency of\nword order in linguistic processing and compre-\nhension comes from a variety of studies using ac-\nceptability judgements, eye-tracking data, and neu-\n6913\nral response measurements (Bever, 1970; Danks\nand Glucksberg, 1971; Just and Carpenter, 1980;\nFriederici et al., 2000, 2001; Bahlmann et al., 2007;\nLerner et al., 2011; Pallier et al., 2011; Fedorenko\net al., 2016; Ding et al., 2016). Psycholinguistic\nresearch has, however, also highlighted the robust-\nness of sentence processing mechanisms to a va-\nriety of perturbations, including those which vio-\nlate word order restrictions (Ferreira et al., 2002;\nGibson et al., 2013; Traxler, 2014). In recent work,\nMollica et al. (2020) tested the hypothesis that com-\nposition is the core function of the brain’s language-\nselective network and that it can take place even\nwhen grammatical word order constrains are vio-\nlated. Their ﬁndings conﬁrmed this, showing that\nstimuli with shufﬂed word order where local depen-\ndencies were preserved — as is, roughly speaking,\nthe case for many dependencies in the sentences\nSHUF.N4 is trained on — elicited a neural response\nin the language network that is comparable to that\nelicited by normal sentences. When interword de-\npendencies were disrupted so combinable words\nwere so far apart that composition among nearby\nwords was highly unlikely — as in SHUF.N1, neu-\nral response fell to a level compared to unconnected\nword lists.\nIn Machines Recently, many NLP researchers\nhave attempted to investigate the role of word order\ninformation in language models. For example, Lin\net al. (2019) employ diagnostic classiﬁers and at-\ntention analyses to demonstrate that lower (but not\nhigher) layers of BERT encode word order infor-\nmation. Papadimitriou et al. (2021) ﬁnd that Multi-\nlingual BERT is sensitive to morphosyntactic align-\nment, where numerous languages (out of 24 total)\nrely on word order to mark subjecthood (English\namong them). Alleman et al. (2021) implement\nan input perturbation framework (n-gram shufﬂing,\nphrase swaps, etc.), and employ it towards testing\nthe sensitivity of BERT’s representations to various\ntypes of structure in sentences. They report a sen-\nsitivity to larger constituent units of sentences in\nhigher layers, which they deduce to be inﬂuenced\nby hierarchical phrase structure. O’Connor and An-\ndreas (2021) examine the contribution of various\ncontextual features to the ability of GPT-2 (Radford\net al., 2019) to predict upcoming tokens. Their ﬁnd-\nings show that several destructive manipulations,\nincluding in-sentence word shufﬂing, applied to\nmid- and long range contexts lead only to a modest\nincrease in usable information as deﬁned according\nto the V-information framework of Xu et al. (2020).\nSimilarly, word order information has been\nfound not to be essential for various NLU tasks\nand datasets. Early work showed that Natural Lan-\nguage Inference tasks are largely insensitive to per-\nmutations of word order (Parikh et al., 2016; Sinha\net al., 2020). Pham et al. (2020) and Gupta et al.\n(2021) discuss this in greater detail, demonstrat-\ning that test-time word order perturbations applied\nto GLUE benchmark tasks have little impact on\nLM performance. Following up on this, Sinha\net al. (2021), which our work builds on, found\nthat pretraining on scrambled text appears to only\nmarginally affect model performance. Most related\nto this study, Clouatre et al. (2021) introduce two\nmetrics for gauging the local and global ordering of\ntokens in scrambled texts, observing that only the\nlatter is altered by the perturbation functions found\nin prior literature. In experiments with GLUE, they\nﬁnd that local (sub-word) perturbations show a sub-\nstantially stronger performance decay compared to\nglobal ones.\nIn this work, we present an in-depth analysis of\nthese results, showing that LMs trained on scram-\nbled text can actually retain word information and\nthat – as for humans – their sensitivity to word\norder is dependent on a variety of factors such as\nthe nature of the task and the locality of perturba-\ntion. While performance on some “understanding”\nevaluation tasks is not strongly affected by word\norder scrambling, the effect on others such as the\nWinograd Schema is far more evident.\n9 Conclusion\nMuch discussion has resulted from recent work\nshowing that scrambling text at different stages of\ntesting or training does not drastically alter the per-\nformance of language models on NLU tasks. In\nthis work, we presented analyses painting a more\nnuanced picture of such ﬁndings. Primarily, we\ndemonstrate that, as far as altered pre-training is\nconcerned, models still do retain a semblance of\nword order knowledge — largely at the local level.\nWe show that this knowledge stems from cues in\nthe altered data, such as adjacent BPE symbols and\ncorrelations between sentence length and content.\nThe order in which shufﬂing is performed — be-\nfore or after BPE tokenization — is inﬂuential in\nmodels’ acquisition of word order, which calls for\ncaution in interpreting previous results. Finally, we\nshow that there exist NLU tasks that are far more\n6914\nsensitive to sentence structure as expressed by word\norder.\nAcknowledgements\nWe thank Stephanie Brandl, Desmond Elliott, Yova\nKementchedjhieva, Douwe Kiela and Miryam de\nLhoneux for their feedback and comments. We\nacknowledge the CSC-IT Centre for Science, Fin-\nland, for providing computational resources. Vinit\nworked on this paper while on a research visit to\nthe University of Copenhagen. Mostafa and An-\nders are supported by a Google Focused Research\nAward.\nReferences\nMatteo Alleman, Jonathan Mamou, Miguel A Del Rio,\nHanlin Tang, Yoon Kim, and SueYeon Chung. 2021.\nSyntactic perturbations reveal representational cor-\nrelates of hierarchical phrase structure in pretrained\nlanguage models. arXiv preprint arXiv:2104.07578.\nJörg Bahlmann, Antoni Rodriguez-Fornells, Michael\nRotte, and Thomas F Münte. 2007. An fmri study\nof canonical and noncanonical word order in german.\nHuman brain mapping, 28(10):940–949.\nThomas G Bever. 1970. The cognitive basis for lin-\nguistic structures. Cognition and the development of\nlanguage.\nChris M. Bishop. 1995. Training with noise is equiv-\nalent to tikhonov regularization. Neural Computa-\ntion, 7(1):108–116.\nAlexander Camuto, Matthew Willetts, Umut ¸ Sim¸ sekli,\nStephen Roberts, and Chris Holmes. 2021. Ex-\nplicit Regularisation in Gaussian Noise Injections.\narXiv:2007.07368 [cs, stat].\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifﬁculty of natural yes/no questions. arXiv preprint\narXiv:1905.10044.\nLouis Clouatre, Prasanna Parthasarathi, Amal Zouaq,\nand Sarath Chandar. 2021. Demystifying neural lan-\nguage models’ insensitivity to word-order. arXiv\npreprint arXiv:2107.13955.\nBernard Comrie. 1989. Language universals and lin-\nguistic typology: Syntax and morphology . Univer-\nsity of Chicago press.\nJean-Baptiste Cordonnier, Andreas Loukas, and Mar-\ntin Jaggi. 2020. On the Relationship be-\ntween Self-Attention and Convolutional Layers.\narXiv:1911.03584 [cs, stat].\nJoseph H Danks and Sam Glucksberg. 1971. Psycho-\nlogical scaling of adjective orders. Journal of Mem-\nory and Language, 10(1):63.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Inves-\ntigating projection in naturally occurring discourse.\nIn proceedings of Sinn und Bedeutung , volume 23,\npages 107–124.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nNai Ding, Lucia Melloni, Hang Zhang, Xing Tian, and\nDavid Poeppel. 2016. Cortical tracking of hierarchi-\ncal linguistic structures in connected speech. Nature\nneuroscience, 19(1):158–164.\nTimothy Dozat, Peng Qi, and Christopher D. Manning.\n2017. Stanford’s graph-based neural dependency\nparser at the CoNLL 2017 shared task. In Proceed-\nings of the CoNLL 2017 Shared Task: Multilingual\nParsing from Raw Text to Universal Dependencies ,\npages 20–30, Vancouver, Canada. Association for\nComputational Linguistics.\nEvelina Fedorenko, Terri L Scott, Peter Brunner,\nWilliam G Coon, Brianna Pritchett, Gerwin Schalk,\nand Nancy Kanwisher. 2016. Neural correlate of the\nconstruction of sentence meaning. Proceedings of\nthe National Academy of Sciences , 113(41):E6256–\nE6262.\nFernanda Ferreira, Karl GD Bailey, and Vittoria Fer-\nraro. 2002. Good-enough representations in lan-\nguage comprehension. Current directions in psycho-\nlogical science, 11(1):11–15.\nAngela D Friederici, Axel Mecklinger, Kevin M\nSpencer, Karsten Steinhauer, and Emanuel Donchin.\n2001. Syntactic parsing preferences and their on-\nline revisions: A spatio-temporal analysis of event-\nrelated brain potentials. Cognitive Brain Research,\n11(2):305–323.\nAngela D Friederici, Martin Meyer, and D Yves\nV on Cramon. 2000. Auditory language comprehen-\nsion: an event-related fmri study on the processing\nof syntactic and lexical information. Brain and lan-\nguage, 74(2):289–300.\nEdward Gibson, Leon Bergen, and Steven T Pianta-\ndosi. 2013. Rational integration of noisy evidence\nand prior semantic expectations in sentence interpre-\ntation. Proceedings of the National Academy of Sci-\nences, 110(20):8051–8056.\nAshim Gupta, Giorgi Kvernadze, and Vivek Sriku-\nmar. 2021. Bert & family eat word salad: Ex-\nperiments with text understanding. arXiv preprint\narXiv:2101.03453.\n6915\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel Bowman, and Noah A.\nSmith. 2018. Annotation artifacts in natural lan-\nguage inference data. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 107–112, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2733–2743, Hong\nKong, China. Association for Computational Lin-\nguistics.\nHuiyuan Jin and Haitao Liu. 2017. How will text size\ninﬂuence the length of its linguistic constituents?\nPoznan Studies in Contemporary Linguistics, 53.\nMarcel A Just and Patricia A Carpenter. 1980. A the-\nory of reading: From eye ﬁxations to comprehension.\nPsychological review, 87(4):329.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface:a challenge set for reading com-\nprehension over multiple sentences. In NAACL.\nArtur Kulmizev and Joakim Nivre. 2021.\nSchrödinger’s Tree – On Syntax and Neural Lan-\nguage Models. arXiv preprint arXiv:2110.08887.\nYulia Lerner, Christopher J Honey, Lauren J Silbert,\nand Uri Hasson. 2011. Topographic mapping of a hi-\nerarchy of temporal receptive windows using a nar-\nrated story. Journal of Neuroscience , 31(8):2906–\n2915.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside bert’s linguistic knowl-\nedge. arXiv preprint arXiv:1906.01698.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs].\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3428–3448,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nFrancis Mollica, Matthew Siegelman, Evgeniia Di-\nachek, Steven T Piantadosi, Zachary Mineroff,\nRichard Futrell, Hope Kean, Peng Qian, and Evelina\nFedorenko. 2020. Composition is the core driver\nof the language-selective network. Neurobiology of\nLanguage, 1(1):104–134.\nJoe O’Connor and Jacob Andreas. 2021. What con-\ntext features can transformer language models use?\narXiv preprint arXiv:2106.08367.\nChristophe Pallier, Anne-Dominique Devauchelle, and\nStanislas Dehaene. 2011. Cortical representation of\nthe constituent structure of sentences. Proceedings\nof the National Academy of Sciences , 108(6):2522–\n2527.\nIsabel Papadimitriou, Ethan A Chi, Richard Futrell,\nand Kyle Mahowald. 2021. Deep subjecthood:\nHigher-order grammatical features in multilingual\nbert. arXiv preprint arXiv:2101.11043.\nAnkur P Parikh, Oscar Täckström, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. arXiv preprint\narXiv:1606.01933.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nThang M Pham, Trung Bui, Long Mai, and Anh\nNguyen. 2020. Out of order: How important is\nthe sequential order of words in a sentence in nat-\nural language understanding tasks? arXiv preprint\narXiv:2012.15180.\nMohammad Taher Pilehvar and Jose Camacho-\nCollados. 2018. Wic: the word-in-context dataset\nfor evaluating context-sensitive meaning representa-\ntions. arXiv preprint arXiv:1808.09121.\nTiago Pimentel, Naomi Saphra, Adina Williams, and\nRyan Cotterell. 2020. Pareto probing: Trading off\naccuracy for complexity. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 3138–3153, On-\nline. Association for Computational Linguistics.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language in-\nference. In Proceedings of the Seventh Joint Con-\nference on Lexical and Computational Semantics ,\npages 180–191, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\n6916\nVinit Ravishankar, Artur Kulmizev, Mostafa Abdou,\nAnders Søgaard, and Joakim Nivre. 2021. Atten-\ntion can reﬂect syntactic structure (if you let it). In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 3031–3045, Online.\nAssociation for Computational Linguistics.\nVinit Ravishankar and Anders Søgaard. 2021. The im-\npact of positional encodings on multilingual com-\npression. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 763–777, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alterna-\ntives: An evaluation of commonsense causal reason-\ning. In 2011 AAAI Spring Symposium Series.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2020. Winogrande: An adver-\nsarial winograd schema challenge at scale. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 34, pages 8732–8740.\nBengt Sigurd, Mats Eeg-Olofsson, and Joost Van Wei-\njer. 2004. Word length, sentence length and\nfrequency – zipf revisited. Studia Linguistica ,\n58(1):37–52.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nDe Marneffe, Samuel R Bowman, Miriam Connor,\nJohn Bauer, and Christopher D Manning. 2014. A\ngold standard dependency corpus for english. In\nLREC, pages 2897–2904. Citeseer.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional\nhypothesis: Order word matters pre-training for lit-\ntle. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 2888–2913, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nKoustuv Sinha, Prasanna Parthasarathi, Joelle Pineau,\nand Adina Williams. 2020. Unnatural language in-\nference. arXiv preprint arXiv:2101.00010.\nMatthew J Traxler. 2014. Trends in syntactic parsing:\nAnticipation, bayesian estimation, and good-enough\nparsing. Trends in cognitive sciences , 18(11):605–\n611.\nPaul Trichelair, Ali Emami, Jackie Chi Kit Cheung,\nAdam Trischler, Kaheer Suleman, and Fernando\nDiaz. 2018. On the evaluation of common-sense\nreasoning in natural language understanding. arXiv\npreprint arXiv:1811.01778.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. SuperGLUE: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran As-\nsociates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A Multi-Task Benchmark and Analysis Plat-\nform for Natural Language Understanding. In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nBenyou Wang, Lifeng Shang, Christina Lioma, Xin\nJiang, Hao Yang, Qun Liu, and Jakob Grue Si-\nmonsen. 2021. ON POSITION EMBEDDINGS IN\nBERT. page 21.\nYu-An Wang and Yun-Nung Chen. 2020. What do\nposition embeddings learn? an empirical study of\npre-trained language model positional encoding. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6840–6849, Online. Association for Computa-\ntional Linguistics.\nYilun Xu, Shengjia Zhao, Jiaming Song, Russell Stew-\nart, and Stefano Ermon. 2020. A theory of usable\ninformation under computational constraints. arXiv\npreprint arXiv:2002.10689.\nAmir Zeldes. 2017. The gum corpus: Creating mul-\ntilayer resources in the classroom. Language Re-\nsources and Evaluation, 51(3):581–612.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nRecord: Bridging the gap between human and ma-\nchine commonsense reading comprehension. arXiv\npreprint arXiv:1810.12885.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPAWS: Paraphrase adversaries from word scram-\nbling. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n1298–1308, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\n6917\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\nA Subword vs. word scrambling\nB On biased sampling\nWe ﬁrst split our vocab of size 5,000 into two\nhalves, both of size 2500, such that the sum to-\ntal of unigram frequencies of tokens in each half\nis roughly equivalent. Next, iterating over 100k\nBookCorpus sentences, we determine the sentence\nlength l, for which there are an equivalent num-\nber of tokens in sentences with length < l and\nsentences with length >= l. We then sample to-\nkens from the ﬁrst vocab half for sentences < l,\nand from the second vocab half for sentences with\nlength >= l, 80% of the time; for the other 20%,\nwe sample from the opposite half to introduce some\noverlap.\nC Full UD results\n6918\nFigure 7: Pearson correlations, when scrambling by subword/word, with/without disjoint vocabularies. Disjoint\nvocabularies appear to induce patterns in position-position correlations, while scrambling at a word level induces\n‘stripes’ of oscillating magnitude; this is likely due to position embeddings learning connections to adjacent tokens.\nFigure 8: Relative frequencies of dependency relations\nin UDEnglish−EWT , at a dependency lengths indi-\ncated by the x-axis\nFigure 9: ∆ UAS, all models and layers across depen-\ndency lengths 1-5+, w.r.t. ORIG . Layer 13 represents a\nlinear mix of all model layers.\n6919"
}