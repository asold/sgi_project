{
  "title": "TEMPROT: protein function annotation using transformers embeddings and homology search",
  "url": "https://openalex.org/W4379882058",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2312713126",
      "name": "Gabriel B. Oliveira",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    },
    {
      "id": "https://openalex.org/A74205977",
      "name": "Helio Pedrini",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    },
    {
      "id": "https://openalex.org/A3023723686",
      "name": "Zanoni Dias",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    },
    {
      "id": "https://openalex.org/A2312713126",
      "name": "Gabriel B. Oliveira",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    },
    {
      "id": "https://openalex.org/A74205977",
      "name": "Helio Pedrini",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    },
    {
      "id": "https://openalex.org/A3023723686",
      "name": "Zanoni Dias",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3210411940",
    "https://openalex.org/W3137270128",
    "https://openalex.org/W4221091885",
    "https://openalex.org/W2103017472",
    "https://openalex.org/W2615066396",
    "https://openalex.org/W4236358448",
    "https://openalex.org/W4312097792",
    "https://openalex.org/W2064820767",
    "https://openalex.org/W1144107824",
    "https://openalex.org/W2158714788",
    "https://openalex.org/W3143063265",
    "https://openalex.org/W4220952154",
    "https://openalex.org/W2789559913",
    "https://openalex.org/W2117235735",
    "https://openalex.org/W4206338776",
    "https://openalex.org/W1892752174",
    "https://openalex.org/W2775231772",
    "https://openalex.org/W2951731136",
    "https://openalex.org/W3165795318",
    "https://openalex.org/W3040739508",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2989608901",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2980298350",
    "https://openalex.org/W4281940572",
    "https://openalex.org/W2966590054",
    "https://openalex.org/W2600349519"
  ],
  "abstract": null,
  "full_text": "Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nOliveira et al. BMC Bioinformatics          (2023) 24:242  \nhttps://doi.org/10.1186/s12859-023-05375-0\nBMC Bioinformatics\nTEMPROT: protein function annotation using \ntransformers embeddings and homology search\nGabriel B. Oliveira1*, Helio Pedrini1 and Zanoni Dias1 \nAbstract \nBackground: Although the development of sequencing technologies has provided \na large number of protein sequences, the analysis of functions that each one plays is \nstill difficult due to the efforts of laboratorial methods, making necessary the usage \nof computational methods to decrease this gap. As the main source of information \navailable about proteins is their sequences, approaches that can use this information, \nsuch as classification based on the patterns of the amino acids and the inference based \non sequence similarity using alignment tools, are able to predict a large collection \nof proteins. The methods available in the literature that use this type of feature can \nachieve good results, however, they present restrictions of protein length as input to \ntheir models. In this work, we present a new method, called TEMPROT, based on the \nfine-tuning and extraction of embeddings from an available architecture pre-trained \non protein sequences. We also describe TEMPROT+, an ensemble between TEMPROT \nand BLASTp, a local alignment tool that analyzes sequence similarity, which improves \nthe results of our former approach.\nResults: The evaluation of our proposed classifiers with the literature approaches has \nbeen conducted on our dataset, which was derived from CAFA3 challenge database. \nBoth TEMPROT and TEMPROT+ achieved competitive results on Fmax , Smin , AuPRC and \nIAuPRC metrics on Biological Process (BP), Cellular Component (CC) and Molecular \nFunction (MF) ontologies compared to state-of-the-art models, with the main results \nequal to 0.581, 0.692 and 0.662 of Fmax on BP , CC and MF, respectively.\nConclusions: The comparison with the literature showed that our model presented \ncompetitive results compared the state-of-the-art approaches considering the amino \nacid sequence pattern recognition and homology analysis. Our model also presented \nimprovements related to the input size that the model can use to train compared to \nthe literature methods.\nKeywords: Protein function prediction, Natural language processing, Transformers\nBackground\nWith the development of sequencing technologies in the last decades, a large number \nof proteins have been sequenced. On the other hand, the analysis of the specific char -\nacteristics of each one is still far from the number of sequenced proteins, mainly due \nto the effort of time and money required by laboratorial experiments compared to \n*Correspondence:   \ngabriel.oliveira@ic.unicamp.br\n1 Institute of Computing, \nUniversity of Campinas, \nCampinas, Brazil\nPage 2 of 16Oliveira et al. BMC Bioinformatics          (2023) 24:242 \nsequencing techniques. Due to this fact, works in the literature have been proposing \ncomputational methods to predict this type of information from sequenced proteins, \nsuch as secondary structures [1 ] and functions [2 ], in order to decrease this gap [3 ].\nThe protein function annotation task uses Gene Ontology (GO) [4 ] to evaluate the \npredictions made in three different ontologies, Biological Process (BP), which repre -\nsents the process that proteins are involved, Cellular Component (CC), which is the \nplace in the cell where the protein performs the function, and Molecular Function \n(MF), the function played by the protein at a molecular level. In all of them, each pro -\ntein can have a different assigned function, which makes this task a multi-label pre -\ndiction. Furthermore, the organization of the ontologies is in a direct acyclic graph, \nwith the deeper terms being more specific than the shallow ones and, if a protein has \na specific term, it also has all the ancestor ontology terms up to the root node.\nIn the literature, different approaches considering a huge type of features have been \npresented for the protein function annotation task, such as amino acid sequence pat -\ntern recognition [2 , 5–7], sequence similarity analysis using homology search [8 , 9] \nbased on BLASTp [10] or DIAMOND [11], which are local alignment tools, structure \n[12, 13], protein-protein network interaction [13, 14], biological features [15, 16], text \nmining from scientific articles [17], and combination of them [18, 19]. Compared to \nthe other features, protein sequence is the most common information available about \nproteins, so methods that use it, such as amino acid sequence pattern analysis and \nhomology search, can predict a large collection of proteins compared to models that \napply other input characteristics to their models.\nIn this paper, we present two protein function annotation models, based on pro -\ntein pattern analysis and homology search. The first one is TEMPROT, a method that \nuses the amino acid sequence to make GO predictions based on the fine-tuning and \nextraction of embeddings from ProtBERT-BFD [20], a Transformer [21] architecture \npre-trained in protein sequences. As an evolution of TEMPROT, we developed TEM -\nPROT+, an ensemble of the former approach with BLASTp, responsible for making \nhomology search based on local sequence similarity.\nDuring the evaluation on our dataset, which is based on CAFA3 challenge, we com -\npared TEMPROT and TEMPROT+  against state-of-the-art approaches using amino \nacid sequence pattern recognition and homology search. We applied DeepGO [5 ], \nDeepGOPlus [6 ], TALE+ [2], ATGO+ [7], and the baseline models proposed in the \nCAFA challenge on our data. Our methods achieved the best Fmax on CC and MF \nontologies, and competitive results on AuPRC, IAuPRC, and Smin metrics on the test \nset, able to predict rare terms in all three ontologies and competitive results consider -\ning the Eukaryota, Bacteria, and Archaea domains.\nOur main contributions are: (1) we report issues on the main dataset available in the \nliterature for protein function annotation and we create a new version of this data -\nset, without these issues, (2) we propose a new metric that showed to be fairer in the \nevaluation of precision and recall curves, (3) we present a new method to generate \nartificial proteins for training data enhancement based on PAM matrix [22], improv -\ning the results compared to the standard version, that is, without this technique and \n(4) unlike state-of-the-art methods, our method can use sequences without length \nrestriction.\nPage 3 of 16\nOliveira et al. BMC Bioinformatics          (2023) 24:242 \n \nMethods\nIn this section, we present the dataset applied in our experiments, describe our model \nand the comparison methods, and detail the evaluation metrics.\nDataset\nThe dataset employed to evaluate our model and compare with the literature was gen -\nerated by DeepGOPlus [6] work based on CAFA3 [23], which is the most recent data -\nset from CAFA challenge that has a published paper reporting the official methods and \nresults. The split of the database considered the timestamp, that is, the training and \nvalidation sets have proteins with experimental annotations published before Septem -\nber 2016, and the test set contains proteins with experimental annotations published \nbetween September 2016 and November 2017.\nDuring the exploration of the dataset, we noticed that some sequences are identical on \ndifferent sets considering the same ontology, even with different functions annotated. \nTherefore, to deal with this issue related to data leakage about protein sequence, we \nremoved the duplicated data in the following steps: (1) exclusion of duplicated sequences \nfrom the training set, considering the test set, (2) removal of duplicated sequences from \nthe validation set, considering the test set, (3) exclusion of duplicated sequences from \nthe validation set, considering the training set. Considering the duplicated sequences in \nthe training and test set, we removed 430 (0.89%), 164 (0.36%), and 47 (0.14%) sequences \nout of the 48,121, 45,473 and 32468 sequences from the original training set of BP , CC \nand MF, respectively, with 60.9% (BP), 87.8% (CC) and 74.5% (MF) of these removed \nsequences having different labels in these two sets.\nAs a final preprocessing step of the dataset, we considered only terms presented in at \nleast 50 proteins as possible labels in the annotation task, as used in the DeepGOPlus \nwork. The number of proteins in each set and the number of functions in each ontology \nare presented in Table 1.\nTEMPROT\nIn this subsection, we describe our protein sequence-based method for annotating pro -\ntein functions, which we called Transformer-based EMbeddings for PROTein function \nannotation (TEMPROT). Figure 1 illustrates TEMPROT pipeline.\nFine‑tuning\nFollowing state-of-the-art natural language processing techniques, we fine-tuned Prot -\nBERT-BFD [20], a BERT-based [24] model pre-trained on BFD dataset [25], and used it \nas extractor of features from the protein sequence for function annotation.\nTable 1 Number of proteins and functions in BP , CC and MF ontologies in the dataset\nBP CC MF\nTraining set 47,691 45,309 32,421\nValidation set 5252 4985 3587\nTest set 2392 1265 1137\nFunctions 3992 551 677\nPage 4 of 16Oliveira et al. BMC Bioinformatics          (2023) 24:242 \nAs ProtBERT-BFD is a BERT-based architecture, it cannot cope with sequences \nlonger than 512 amino acid tokens during fine-tuning for sequence classification task, \ndue to the quadratic memory limitations of the attention mechanisms, requiring large \ncomputational resources for longer inputs. Based on this fact, we split the protein \nsequences using a sliding window technique of size of 500 amino acids without super -\nposition. To improve the generalization of the model, we proposed additional slices in \nthe case of two consecutive slices have at least 250 amino acids. In this case, we cre -\nated an additional slice with the 250 last amino acids of the first slice and the 250 first \namino acids of the last slice. During this process, we assigned the same labels from \nthe original protein sequence to all the slices generated. Figure  2 shows an example of \nthis approach with a protein with 1200 amino acids, with the red squares representing \nFig. 1 The pipeline of the Transformer-based EMbeddings for PROTein function annotation (TEMPROT). \na Each sequence is split into slices using the sliding window technique and fined-tune ProtBERT-BFD \nbackbone. b With the fine-tuned ProtBERT-BFD, all the slices pass through the backbone to extract the \nembeddings from the last representation of CLS token, then combine the embeddings to have a unique \nrepresentation of the protein and make the final prediction with the meta-classifier\nFig. 2 Example of sliding window technique using a protein with 1200 amino acids. The red squares \nrepresent the standard slices, and the blue square illustrates the additional slice\nPage 5 of 16\nOliveira et al. BMC Bioinformatics          (2023) 24:242 \n \nthe standard slices and the blue square as the additional slice. In the example, the first \nand the second standard slices have the size equal to 500, and the last has the size \nequal of 200, and based on this, it is possible to create just one additional slice.\nIn TEMPROT, the first step is the fine-tuning process of ProtBERT-BFD in the protein \nfunction annotation task, as described in part (a) of Fig.  1. To do so, we passed each \ninput data, that is, the slices that were generated using the sliding window technique, \nthrough the backbone. It is important to notice that it is possible to make predictions \nusing the backbone (part (a) of Fig.  1). We compare and discuss the results based on the \npredictions made in this first step with the final model in the Results section.\nDuring the fine-tuning step, we used ProtBERT-BFD model available at  Hugging \nFace [26] repository, with TensorFlow [27] and ktrain [28] libraries. We fine-tuned \nthe model during 10 epochs with the early stopping technique, binary cross-entropy loss \nfunction, and Adam [29] optimizer.\nEmbedding extraction\nAfter the fine-tuning process, we used the backbone architecture as feature extraction. \nTo do so, we passed all sequence slices through the fine-tuned ProtBERT-BFD backbone \nand extracted the embeddings from  CLS token from the last encoder block of the fine-\ntuned architecture. This token is responsible for gathering the context of the sentence, \nthat is, it is used as a special token for classification tasks. Based on that, we extracted \nthe embeddings from the deepest representation of this token. As a result, each slice \ngenerated a feature vector of 1024 float values.\nThen, we aggregated the embeddings from the slice of the same protein to have a \nunique feature vector of size 1024 for each protein. To do so, we applied the mean opera-\ntion between the embeddings of all the slices of the same protein.\nMeta‑classifier\nAs the last step of our method, we employed each protein representation in a meta-clas -\nsifier, which is responsible for making the final prediction.\nFor the meta-classifier, we constructed a multi-layer perceptron neural network model \nwith  TensorFlow library. The architecture consisted of one hidden layer with 1000 \nneurons and ReLU activation. We trained the model during 100 epochs with early stop -\nping and reduction of learning rate on plateau techniques, binary cross-entropy loss \nfunction, and Adam optimizer.\nData augmentation\nDuring all the steps of TEMPROT, we employed data augmentation for the training set. \nInspired by EDA technique [30], for each protein in the training set, we created a copy of \nit and made substitutions of amino acids considering the PAM1 matrix [22] in an offline \nmanner.\nIn the PAM matrix M, each row i and column j represents the amino acids, where a \nspecific position M ij indicates the likelihood of substitution of amino acid j per amino \nacid i. It is important to note that, in the PAM matrix, the most likely substitution of a \nspecific amino acid is for the same amino acid, that is, the substitution does not change \nthe amino acid.\nPage 6 of 16Oliveira et al. BMC Bioinformatics          (2023) 24:242 \nFor the substitutions, we considered Eq.  1, where the number of substitutions of a \nprotein p is equal to its length L and a constant k. We explored different values for k, \nand the best results were achieved with k equal to 2. With this set up, the augmented \ndata changed 2.03% ± 0.84% , 2.04% ± 0.84% , and 2.03% ± 0.78% from the original \ntraining data for BP , CC and MF ontologies, respectively. We investigate the impact of \nthe usage of data augmentation in the Results section.\nTEMPROT+\nIn this subsection, we present TEMPROT +, an ensemble of TEMPROT with BLASTp \n[10], a homology search tool.\nBLASTp\nConsidering the improvements obtained by TALE + and DeepGOPlus using DIA -\nMOND, and by ATGO + using BLASTp, we also implemented a version of our \nmethod combined with a homology search using BLASTp. To do so, we ran BLASTp \nto perform homology search considering sequence similarity of validation and test \nproteins against the sequences from the training set. As used in the previous meth -\nods, we set the E-value parameter equal to 0.001.\nBased on the retrieved sequences, we applied the bitscore to make the predictions, \nas presented in Eq.  2, where S(p, f) indicates a score prediction for a protein p and a \nspecific function f, s is a protein of the set E of retrieved proteins of the training set, T s \nis the functions played by s, and I() is a function that returns 1 if the condition inside \nis true or 0 if it is false.\nEnsemble of TEMPROT and BLASTp\nTo ensemble TEMPROT and BLASTp predictions, we investigated various linear \ncombination approaches between their predictions, as shown in Additional file  1. The \nensemble method applied in our model is expressed in Eq.  3, which achieved the best \nresults compared to variations of this equation, where S(p,  f) indicates a score pre -\ndiction for a protein p and a specific function f, considering the prediction yT  from \nTEMPROT and yB from BLASTp.\nIn order to find the α values for each ontology, we ran a grid search considering the vali -\ndation set. The best outcomes were obtained for α equal to 0.21, 0.60, and 0.30 for BP , \nCC, and MF, respectively.\n(1)Subs(p) = L × k\n(2)S(p,f) = s∈E I(f ∈ T s) × bitscore (p,s)\ns∈E bitscore (p,s)\n(3)S(p,f) = α × yT + (1 − α) × yB\nPage 7 of 16\nOliveira et al. BMC Bioinformatics          (2023) 24:242 \n \nComparison methods\nIn order to compare our method with state-of-the-art models, we selected three dif -\nferent types of approaches. We describe each one as follows.\nThe first approach is the baseline methods, as proposed by the CAFA challenge \norganizers [23]. There are two classifiers in this category, naive and based on sequence \nsimilarity using BLAST. Naive one is a classifier that predicts that each function of \nproteins in the test set has the same chance, that is, the same relative frequency, of the \nsame function in the training set. For the second baseline classifier, we ran BLASTp \nfor the prediction based on sequence similarity analysis using the highest local align -\nment sequence hit. In the Results section, we call the BLASTp implementation of \nCAFA as CAFA-BLASTp.\nTo evaluate TEMPROT considering the state-of-the-art approaches, we also \nassessed methods that employ amino acid sequence pattern recognition to make pre -\ndictions. With that, we compared our outcomes with DeepGO, DeepGOPlusCNN, \nTALE+Transformers and ATGO.\nDeepGO [5 ] is a method that applies protein sequence and protein network fea -\ntures to convolutional neural networks. To make a fair comparison, we employed only \nthe protein sequence part in the evaluation. DeepGOPlusCNN [6 ] is an evolution of \nDeepGO, capable of outperforming the previous method with architecture and pre -\nprocessing steps. We also compared our results with Transformer-based methods, \nthat is, TALE+Transformers [2 ], an approach based on the ensemble of different con -\nfigurations of the original Transformer architecture, and ATGO [7 ], a method that \nextracts embeddings from ESM-1b [31] architecture.\nThe last models we compared are based on the ensemble of sequence pat -\ntern recognition and homology search predictions. We assessed DeepGOPlus [6 ], \nan ensemble of DeepGOPlusCNN with DIAMOND, TALE+  [ 2], an ensemble of \nTALE+Transformers and DIAMOND and ATGO+  [ 7], an ensemble of ATGO and \nBLASTp, with TEMPROT+ . We also evaluated DIAMOND and BLASTp predictions \nbased on Eq. 2 .\nFor all approaches, we followed the hyperparameters reported in their original \npapers and the code available in their respective repositories and ran each one in our \ndataset in order to have a fair comparison with our results.\nEvaluation\nTo evaluate and compare our model with the literature, we used four evaluation met -\nrics. The first one is Fmax , the official metric of CAFA challenge [23]. Fmax measures \nthe maximum harmonic mean between precision and recall considering the predic -\ntions in all thresholds τ from 0 up to 1 with steps of 0.01. Equations  4, 5, and 6 rep -\nresents precision at τ , recall at τ and Fmax , respectively, where f  is a function of the \nontology that is in evaluation, P i(τ ) is the set of functions predicted in the threshold \nτ for a protein i , T i is the ground truth of a protein i , m(τ ) is the number of proteins \nwith at least one prediction equal to or greater than the threshold τ , ne is the number \nof proteins considering during the evaluation, and I() is a function that returns 1 if the \ncondition inside is true or 0 if it is false.\nPage 8 of 16Oliveira et al. BMC Bioinformatics          (2023) 24:242 \nBased on the precision and recall values calculated in Fmax , we assessed the area under \nthe precision–recall curve (AuPRC) of the methods. This metric is common in the litera-\nture to evaluate the protein function prediction task.\nThe main problem of AuPRC is that it penalizes when a method can only make predic-\ntions with high recall and high precision compared to methods that can predict with a \nlong range of precision and recall values. Therefore, we propose a new evaluation metric, \nthe interpolated area under the precision–recall curve (IAuPRC). IAuPRC applies the \ninterpolation for AuPRC, making evaluation more reliable and without penalization for \nmethods that can predict functions with always good precision and recall values. The \ninterpolation is represented by Eq. 7, where the precision value at a specific recall P(R) is \nequal to the maximum value of precisions with greater or equal recall levels P(R′) , where \nR′ ⩾ R.\nFigure 3 shows an example of two methods considering AuPRC and IAuPRC. In AuPRC \nanalysis, method 1 achieved 0.618 in this metric, while method 2 obtained 0.584. How -\never, it is clear that the method 2’s curve is better than method 1’s curve, and method \n2 has been penalized by not making predictions with lower (worst) recall values. In \nthe IAuPRC analysis, the interpolation of both curves resulted in 0.643 of IAuPRC for \nmethod 1 and 0.683 of IAuPRC for method 2, which indicates that method 2 is superior \nthan method 1.\nThe last metric is Smin , which measures the semantic distance considering the infor -\nmation content ( IC ) of each function that the prediction of false positive ( mi ) and false \n(4)pr(τ ) = 1\nm(τ )\nm (τ )∑\ni=1\n∑\nf I(f ∈ Pi(τ ) ∧ f ∈ T i)\n∑\nf I(f ∈ Pi(τ ))\n(5)rc(τ ) = 1\nn e\nn∑\ni=1\n∑\nf I(f ∈ P i(τ ) ∧ f ∈ T i)\n∑\nf I(f ∈ T i)\n(6)Fmax =max\nτ\n{2 × pr(τ ) × rc(τ )\npr(τ ) + rc(τ )\n}\n(7)P(R) = max P(R′)\n(a) AuPRC (b) IAuPRC\nFig. 3 Differences for AuPRC and IAuPRC metrics\nPage 9 of 16\nOliveira et al. BMC Bioinformatics          (2023) 24:242 \n \nnegative ( ru ) in a specific τ , where Pb(f|Pr(f)) is the probability of a function f given set of \nparents Pr(f). Equations 8, 9, 10, and 11 describe IC , ru , mi , and Smin , respectively.\nIt is important to note that these metrics comply with the ontology format, that is, if a \ngiven term is predicted by a classifier in a specific threshold τ , all the ancestors are also \npredicted in this threshold.\nResults\nIn this section, we present and discuss the results of TEMPROT and TEMPROT+ com -\npared to the literature.\nEvaluation on test set\nWe evaluated TEMPROT and TEMPROT+ and compared the results with the state-of-\nthe-art methods, considering the test set of our dataset. Table  2 presents the outcomes \nof each model.\nTEMPROT achieved the best Fmax and Smin compared to sequence pattern recogni -\ntion approaches (DeepGO, DeepGOPlusCNN, TALE+Transformers, and ATGO) on \nCC and MF ontologies, outperforming ATGO by 0.005 on CC and 0.027 on MF of Fmax . \nConsidering IAuPRC, TEMPROT achieved competitive results, with the best outcomes \non MF ontology.\nIn the second analysis, with methods based on the ensemble of predictions of \nsequence pattern recognition and homology search, TEMPROT+ achieved the best \nresults on Fmax on CC and MF, with improvements of 0.002 and 0.010 on CC and MF, \nrespectively, compared to ATGO+, the second best outcomes. Considering Smin , TEM-\nPROT+ obtained the best CC outcomes and the second best on BP and MF ontologies.\nDomain generalization\nTo analyze the predictions of each model on the test set on different domains, we \nassessed each approach on Eukaryota, Bacteria, and Archaea with Fmax evaluation met-\nric for each ontology.\nFigure 4 illustrates the Fmax value of each method on each domain. The outcomes show \nthat TEMPROT had the best results on MF ontology, as well as competitive scores on \nBP and CC ontologies, with the best Fmax on Bacteria domain on BP , and the best Fmax \non Eukaryota and Bacteria domains on CC. TEMPROT+ achieved the best results on \n(8)IC(f) =− log(Pb (f|Pr (f))\n(9)ru(τ ) = 1\nne\nne∑\ni=1\n∑\nf∈T i−Pi(τ )\nIC(f)\n(10)mi(τ ) = 1\nne\nne∑\ni=1\n∑\nf∈Pi(τ )−T i\nIC(f)\n(11)Smin =minτ\n√\nru(τ )2 + mi(τ ) 2\nPage 10 of 16Oliveira et al. BMC Bioinformatics          (2023) 24:242 \nTable 2 Evaluation of TEMPROT and TEMPROT+ compared to the state-of-the-art approaches on the test set using Fmax , AuPRC, IAuPRC and Smin evaluation metrics\nThe best results of each metric for sequence pattern recognition and ensemble of sequence pattern recognition and homology search analysis are highlighted\nMethod Fmax AuPRC IAuPRC Smin\nBP CC MF BP CC MF BP CC MF BP CC MF\nNaive 0.402 0.611 0.446 0.266 0.521 0.228 0.345 0.634 0.370 25.423 10.268 9.349\nCAFA-BLASTp 0.468 0.469 0.551 0.208 0.215 0.287 0.215 0.216 0.296 38.083 18.755 9.124\nDeepGO 0.337 0.379 0.489 0.247 0.257 0.309 0.304 0.382 0.465 27.414 11.880 8.821\nDeepGOPlusCNN 0.498 0.664 0.531 0.444 0.637 0.460 0.465 0.634 0.528 23.799 9.783 8.240\nTALE+Transformers 0.491 0.661 0.550 0.477 0.613 0.444 0.469 0.706 0.549 23.929 9.682 8.115\nATGO 0.547 0.684 0.616 0.506 0.667 0.623 0.524 0.724 0.632 22.228 9.437 7.228\nTEMPROT 0.499 0.689 0.643 0.459 0.639 0.561 0.483 0.719 0.664 23.652 9.209 6.973\nDIAMOND 0.519 0.593 0.572 0.286 0.237 0.320 0.417 0.483 0.462 23.066 9.957 7.164\nBLASTp 0.561 0.637 0.620 0.402 0.380 0.360 0.502 0.586 0.562 22.183 9.795 6.805\nDeepGOPlus 0.553 0.677 0.619 0.514 0.638 0.559 0.536 0.717 0.635 22.648 9.515 7.090\nTALE+ 0.555 0.681 0.631 0.547 0.643 0.621 0.540 0.724 0.643 22.615 9.363 6.949\nATGO+ 0.589 0.690 0.652 0.550 0.660 0.650 0.571 0.731 0.689 21.233 9.286 6.617\nTEMPROT+ 0.581 0.692 0.662 0.529 0.641 0.595 0.558 0.728 0.689 21.892 9.169 6.662\nPage 11 of 16\nOliveira et al. BMC Bioinformatics          (2023) 24:242 \n \nEukaryota domain on MF ontology, as well as the best results on Bacteria and competi -\ntive results on Eukaryota and Archaea domains on BP and CC ontologies.\nFrequency analysis\nAn important aspect is that models must correctly predict rare terms. To analyze the \nability of the approaches to annotate terms with different frequency appearances, we \nevaluated each model by considering IAuPRC on all 100 percentage values on the pro -\nteins of the test set, as shown in Fig.  5. In the evaluation, if we were analyzing a specific \npercentage, all terms that have up to that frequency are analyzed, for instance, for a 10% \nanalysis, ontologies terms that have up to 10% of frequency are considered.\nAs a result, TEMPROT achieved the best overall results on MF ontology, with the \nhighest values in all frequencies, and competitive results with ATGO on CC ontology. In \nthe case of the BP ontology, TEMPROT had the second best performance at the begin -\nning of the analysis, that is, in rare functions, with competitive results for the other \nvalues.\n(a) BP - Sequence (b) CC -S equence\n(c) MF - Sequence (d) BP -S equence and Homology\n(e) CC - Sequence and Homology (f)M F- Sequence and Homology\nFig. 4 Comparions of TEMPROT and TEMPROT+ with the state-of-the-art on domain evaluation\nPage 12 of 16Oliveira et al. BMC Bioinformatics          (2023) 24:242 \nConsidering TEMPROT+, across all ontologies, our method had the best perfor -\nmance in rare terms, with competitive outcomes for other values. Concerning MF ontol-\nogy, TEMPROT+ obtained the best overall outcomes along with ATGO+.\nAblation study\nTo evaluate the impact of doing the fine-tuning of ProtBERT-BFD, applying data aug -\nmentation to the sequences, and using the meta-classifier, we assessed different configu -\nrations, as shown in Table 3, on the test set.\nThe ablation study indicates that the use of the meta-classifier (second row of Table  3) \nin our method achieved better results compared to the prediction based on the back -\nbone only (part (a) of Fig.  1, indicated in the last row of Table  3). It is important to note \nthat we needed to aggregate the predictions of slices from the same protein to have a \n(a) BP - Sequence (b) CC - Sequence\n(c) MF - Sequence (d) BP - Sequence and Homology\n(e) CC - Sequence and Homology (f)M F- Sequence and Homology\nFig. 5 Comparions of TEMPROT and TEMPROT+ with the state-of-the-art on frequency analysis\nPage 13 of 16\nOliveira et al. BMC Bioinformatics          (2023) 24:242 \n \nunique prediction per protein at without meta-classifier approach. To do so, we applied \nthe mean operation on the predictions considering all the slices of the same protein.\nConcerning data augmentation and fine-tuning, the version without data augmenta -\ntion and without fine-tuning had the worst performance based on Fmax (TEMPROT \nwithout augmentation and without fine-tuning). The application of fine-tuning tech -\nnique (TEMPROT without augmentation) or data augmentation (TEMPROT without \nfine-tuning) improved the results compared to the former configuration in general. \nTEMPROT, a version using both fine-tuning and data augmentation, achieved the \nbest result considering the sequence pattern information, which represents that both \ntechniques are important for the outcomes. In the end, the best Fmax was achieved by \nTEMPROT+, showing that the ensemble of machine learning with homology search \npredictions can indeed improve the results.\nPrediction time\nWe assessed TEMPROT and TEMPROT+ compared to the literature considering the \naverage time to predict each protein of the test set of each ontology. Table  4 presents \nthe results, showing that convolutional-based methods, such as DeepGOPlusCNN \nand DeepGO, are more efficient than Transformer-based models, such as ATGO and \nTable 3 Ablation study of different configurations of our method on the three ontologies\nMethod Fmax\nBP CC MF\nTEMPROT+ 0.581 0.692 0.662\nTEMPROT 0.499 0.689 0.643\nTEMPROT w/o augmentation 0.493 0.687 0.639\nTEMPROT w/o fine-tuning 0.493 0.681 0.618\nTEMPROT w/o augmentation and w/o fine-\ntuning\n0.490 0.681 0.620\nTEMPROT w/o meta-classifier 0.477 0.677 0.592\nTable 4 Average prediction time in seconds for each protein of the test set on BP , CC and MF \nontologies\nMethod BP CC MF\nNaive 0.001 0.002 0.002\nDIAMOND 0.005 0.008 0.007\nDeepGOPlusCNN 0.013 0.022 0.025\nDeepGO 0.019 0.025 0.026\nDeepGOPlus 0.023 0.030 0.032\nTALE+Transformers 0.035 0.047 0.062\nTALE+ 0.040 0.055 0.069\nATGO 0.304 0.508 0.434\nTEMPROT 0.617 0.643 0.627\nBLASTp 0.666 0.946 0.595\nCAFA-BLASTp 0.666 0.946 0.595\nATGO+ 0.971 1.455 1.029\nTEMPROT+ 1.283 1.589 1.222\nPage 14 of 16Oliveira et al. BMC Bioinformatics          (2023) 24:242 \nTEMPROT. Concerning homology-based predictions by different tools, DIAMOND \nrequired less time to execute than BLASTp, which impacted the runtime of DeepGO -\nPlus and TALE+ compared to TEMPROT+ and ATGO+.\nDiscussion\nThe outcomes of both classifiers presented in this work surpassed the methods in the \nliterature considering sequence pattern recognition and the ensemble of sequence infor-\nmation with homology search via sequence similarity. Compared to state-of-the-art \nmethods, TEMPROT and TEMPROT+ can train using sequences of different lengths, \nwhich is not possible in the literature approaches evaluated in this paper. DeepGO \nand DeepGOPlus (also DeepGOPlusCNN) trained with sequences up to 1000 and \n2000, respectively. In the case of TALE+ (also TALE+Transformers) and ATGO (also \nATGO+), sequences longer than 1000 (TALE+) and 1022 (ATGO+) are cut into a sub -\nsequence equal to the method maximum input size.\nConsidering the evaluation, our methods also presented competitive results in domain \ngeneralization, with the best outcomes on MF ontology. We conclude that methods \nbased on pre-trained on a large volume of protein sequences, that is, TEMPROT and \nATGO, are able to classify protein functions better than other models, due to this ontol -\nogy is more dependent of protein sequences [32]. On BP and CC ontologies, both TEM -\nPROT and TEMPROT+ achieved the best results on at least one domain.\nIn the frequency analysis, TEMPROT and TEMPROT+ obtained the best results on \nMF and both of them achieved the best outcomes on rare terms (lower frequencies) \nalong with ATGO on CC, and competitive results on BP . With that, the experiments \nindicated that methods with pre-trained architectures, such as TEMPROT and ATGO, \nare able to predict terms with lower frequencies.\nRegarding the BP ontology, ATGO and ATGO+ outperformed TEMPROT and TEM-\nPROT+ in most evaluations. Since this ontology has more terms than CC and MF, we \nconclude that it could muddle the classification of TEMPROT compared to ATGO. Fur-\nthermore, ATGO extracts embeddings from different layers of ESM-1b, which may help \nthe generalization of this model. With that, the ensembles of sequence pattern recog -\nnition and homology search, that is, ATGO+ and TEMPROT+, follow the pattern of \nATGO and TEMPROT.\nWe also noticed that our method has shown improvements in generalization by mak -\ning fine-tuning and applying data augmentation techniques on the Transformers back -\nbone. We demonstrated the importance of the usage of the meta-classifier during our \ninvestigation.\nConclusions\nIn this work, we presented and discussed a model based on Transformer embeddings \ncapable of annotating protein based on its sequences. Our model can also be ensembled \nwith homology search predictions, resulting in a classifier that reported better outcomes \nthan the standard version.\nIn our experiments, we showed that TEMPROT and TEMPROT+ outperformed \nstate-of-the-art approaches on MF and CC ontologies, considering Fmax , the main \nPage 15 of 16\nOliveira et al. BMC Bioinformatics          (2023) 24:242 \n \nmetric for protein function prediction in the literature. Our method also presented \nimprovements related to input size compared to state-of-the-art approaches.\nFor future improvements of TEMPROT and TEMPROT+, we can highlight the \ninvestigation of additional features, such as protein-protein interaction networks and \nstructure information, which can help to improve the results of proteins that have this \ninformation available. We also plan to investigate different data augmentation tech -\nniques, from adding insertions and deletions in the actual approach, to exploring pro -\ntein generation models. Another possible direction is the analysis of long Transformers, \nwhich can cope with sequences longer than 512 amino acids without any preprocessing \nstep and without large computation resources, and the utilization of different configura -\ntion of windows, such as domain-based selection. We also plan experiments consider -\ning different approaches to ensemble TEMPROT with BLASTp predictions. As a final \npoint, we intend to evaluate our method on different databases, such as other versions of \nCAFA dataset.\nAbbreviations\nAuPRC  Area under precision–recall curve\nBERT  Bidirectional encoder representations from transformers\nBFD  Big fantastic database\nBP  Biological process\nCAFA  Critical assessment of protein function annotation\nCC  Cellular component\nEDA  Easy data augmentation\nIAuPRC  Interpolated area under precision–recall curve\nMF  Molecular function\nPAM  Point accepted mutation\nReLU  Rectified linear unit\nTEMPROT  Transformer-based EMbeddings for PROTein function annotation\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s12859- 023- 05375-0.\nAdditional file 1. Analysis of ensemble techniques of TEMPROT and BLASTp.\nAcknowledgements\nThe authors would like to acknowledge the Institute of Computing of the University of Campinas for the infrastructure.\nAuthor contributions\nGBO developed the classifier, pre-processing, model training and evaluation. GBO wrote the main manuscript text and \nprepared all the figures and tables. ZD and HP coordinated the study and proofread the manuscript. All authors reviewed \nthe manuscript. All authors read and approved the final manuscript.\nFunding\nThis work has been supported by the São Paulo Research Foundation (Grant Numbers 2015/11937-9, 2017/12646-3, \n2017/16246-0, 2017/12646-3, 2019/20875-8); the National Council for Scientific and Technological Development (Grant \nNumbers 161015/2021-2, 304380/2018-0, 309330/2018-1); and Coordination for the Improvement of Higher Education \nPersonnel.\nAvailability of data and materials\nThe datasets generated and analysed during the current study are available on https:// zenodo. org/ record/ 74096 60. The \nprotein function annotation method generated and analyzed during the current study is available in the Github reposi-\ntory: https:// github. com/ gabri elbia nchin/ TEMPR OT.\nDeclarations\n  Ethics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nPage 16 of 16Oliveira et al. BMC Bioinformatics          (2023) 24:242 \nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 20 December 2022   Accepted: 2 June 2023\nReferences\n 1. Oliveira GB, Pedrini H, Dias Z. Ensemble of template-free and template-based classifiers for protein secondary struc-\nture prediction. Int J Mol Sci. 2021;22(21):11449.\n 2. Cao Y, Shen Y. TALE: transformer-based protein function annotation with joint sequence-label embedding. Bioinfor-\nmatics. 2021;37(18):2825–33.\n 3. Kagaya Y, et al. ContactPFP: protein function prediction using predicted contact information. Front Bioinform. \n2022;2(896295):1–17.\n 4. Ashburner M, et al. Gene ontology: tool for the unification of biology. Nat Genet. 2000;25(1):25–9.\n 5. Kulmanov M, et al. DeepGO: predicting protein functions from sequence and interactions using a deep ontology-\naware classifier. Bioinformatics. 2018;34(4):660–8.\n 6. Kulmanov M, Hoehndorf R. DeepGOPlus: improved protein function prediction from sequence. Bioinformatics. \n2019;36(2):422–9.\n 7. Zhu Y-H, et al. Integrating unsupervised language model with triplet neural networks for protein gene ontology \nprediction. PLoS Comput Biol. 2022;18(12):1010793.\n 8. Zehetner G. OntoBlast function: from sequence similarities directly to potential functional annotations by ontology \nterms. Nucleic Acids Res. 2003;31(13):3799–803.\n 9. Gong Q, et al. GoFDR: a sequence alignment based method for predicting protein functions. Methods. \n2016;93:3–14.\n 10. Altschul SF, et al. Gapped BLAST and PSI-BLAST: a new generation of protein database search programs. Nucleic \nAcids Res. 1997;25(17):3389–402.\n 11. Buchfink B, et al. Sensitive protein alignments at tree-of-life scale using DIAMOND. Nat Methods. 2021;18(4):366–8.\n 12. Lai B, Xu J. Accurate protein function prediction via graph attention networks with predicted structure information. \nBrief Bioinform. 2022;23(1):502.\n 13. Zhang C, et al. MetaGO: predicting gene ontology of non-homologous proteins through low-resolution protein \nstructure prediction and protein–protein network mapping. J Mol Biol. 2018;430(15):2256–65.\n 14. Wang S, et al. Exploiting ontology graph for predicting sparsely annotated gene function. Bioinformatics. \n2015;31(12):357–64.\n 15. Rojano E, et al. Assigning protein function from domain-function associations using DomFun. BMC Bioinform. \n2022;23(1):1–19.\n 16. Das S, et al. CATH FunFHMMer web server: protein functional annotations using functional family assignments. \nNucleic Acids Res. 2015;43(W1):148–53.\n 17. Fodeh S, et al. Exploiting PubMed for protein molecular function prediction via NMF based multi-label classification. \nIn: IEEE international conference on data mining workshops (ICDMW), 2017. p. 446–51.\n 18. You R, et al. GOLabeler: improving sequence-based large-scale protein function prediction by learning to rank. \nBioinformatics. 2018;34(14):2465–73.\n 19. Yao S, et al. NetGO 2.0: improving large-scale protein function prediction with massive sequence, text, domain, fam-\nily and network information. Nucleic Acids Res. 2021;49(W1):469–75.\n 20. Elnaggar A, et al. ProtTrans: towards cracking the language of life’s code through self-supervised deep learning and \nhigh performance computing. 2021. arXiv: 2007. 06225.\n 21. Vaswani A, et al. Attention is all you need. In: Advances in neural information processing systems (NIPS), 2017. p. \n5998–6008.\n 22. Dayhoff MO. Atlas of protein sequence and structure. Washington: National Biomedical Research Foundation; 1972.\n 23. Zhou N, et al. The CAFA challenge reports improved protein function prediction and new functional annotations for \nhundreds of genes through experimental screens. Genome Biol. 2019;20(1):244.\n 24. Devlin J, et al. BERT: pre-training of deep bidirectional transformers for language understanding. 2018. arXiv: 1810. \n04805.\n 25. Jumper J, et al. Highly accurate protein structure prediction with AlphaFold. Nature. 2021;596(7873):583–9.\n 26. Wolf T, et al. Huggingface’s transformers: state-of-the-art natural language processing. 2019. arXiv: 1910. 03771.\n 27. Abadi M, et al. TensorFlow: large-scale machine learning on heterogeneous systems. 2015. https:// www. tenso rflow. \norg.\n 28. Maiya AS. ktrain: a low-code library for augmented machine learning. 2020. arXiv: 2004. 10703.\n 29. Kingma DP , Ba J. Adam: a method for stochastic optimization. 2014. arXiv: 1412. 6980.\n 30. Wei J, Zou K. EDA: easy data augmentation techniques for boosting performance on text classification tasks. 2019. \narXiv: 1901. 11196.\n 31. Rives A, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein \nsequences. Proc Natl Acad Sci. 2021;118(15):2016239118.\n 32. Bonetta R, Valentino G. Machine learning techniques for protein function prediction. Proteins Struct Funct Bioin-\nform. 2020;88(3):397–413.\nPublisher’s note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6731029152870178
    },
    {
      "name": "Annotation",
      "score": 0.5459772944450378
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5451944470405579
    },
    {
      "name": "Protein function prediction",
      "score": 0.5335094928741455
    },
    {
      "name": "Inference",
      "score": 0.4855165481567383
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47335314750671387
    },
    {
      "name": "Algorithm",
      "score": 0.4728337228298187
    },
    {
      "name": "Protein function",
      "score": 0.4573180079460144
    },
    {
      "name": "DNA microarray",
      "score": 0.43283510208129883
    },
    {
      "name": "Sequence alignment",
      "score": 0.4199792146682739
    },
    {
      "name": "Data mining",
      "score": 0.40801897644996643
    },
    {
      "name": "Machine learning",
      "score": 0.3892572224140167
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32695019245147705
    },
    {
      "name": "Biology",
      "score": 0.20036524534225464
    },
    {
      "name": "Peptide sequence",
      "score": 0.2001245617866516
    },
    {
      "name": "Genetics",
      "score": 0.09332999587059021
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Gene expression",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I181391015",
      "name": "Universidade Estadual de Campinas (UNICAMP)",
      "country": "BR"
    }
  ],
  "cited_by": 22
}