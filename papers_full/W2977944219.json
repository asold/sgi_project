{
    "title": "On Identifiability in Transformers",
    "url": "https://openalex.org/W2977944219",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3161310107",
            "name": "Brunner, Gino",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102234800",
            "name": "Liu Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226915685",
            "name": "Pascual, Damian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4293478764",
            "name": "Richter, Oliver",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287125983",
            "name": "Ciaramita, Massimiliano",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2749917197",
            "name": "Wattenhofer, Roger",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2942607211",
        "https://openalex.org/W2951025380",
        "https://openalex.org/W2806120502",
        "https://openalex.org/W2905270607",
        "https://openalex.org/W2948184675",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2948140294",
        "https://openalex.org/W2058899256",
        "https://openalex.org/W131533222",
        "https://openalex.org/W2952570576",
        "https://openalex.org/W2954345306",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W2605287558",
        "https://openalex.org/W2973291083",
        "https://openalex.org/W2970909667",
        "https://openalex.org/W2593634001",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2766527293",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2912070261",
        "https://openalex.org/W2956301977",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2912351236",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W2970726176",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2953369973",
        "https://openalex.org/W2897507397",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2970727289",
        "https://openalex.org/W2970565456",
        "https://openalex.org/W2939556020",
        "https://openalex.org/W2785885194",
        "https://openalex.org/W2911430044",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2799054028",
        "https://openalex.org/W2888329843",
        "https://openalex.org/W2962851944",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W2798698226",
        "https://openalex.org/W2006969979",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2964302946"
    ],
    "abstract": "In this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identifiability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identifiable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also find evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases with depth. Finally, we demonstrate strong mixing of input information in the generation of contextual embeddings by means of a novel quantification method based on gradient attribution. Overall, we show that self-attention distributions are not directly interpretable and present tools to better understand and further investigate Transformer models.",
    "full_text": "Published as a conference paper at ICLR 2020\nON IDENTIFIABILITY IN TRANSFORMERS\nGino Brunner1∗, Yang Liu2∗, Dami´an Pascual1∗, Oliver Richter1,\nMassimiliano Ciaramita3, Roger Wattenhofer1\nDepartments of 1Electrical Engineering and Information Technology,2Computer Science\nETH Zurich, Switzerland\n3Google Research, Zurich, Switzerland\n1{brunnegi,dpascual,richtero,wattenhofer}@ethz.ch,\n2liu.yang@alumni.ethz.ch\n3massi@google.com\nABSTRACT\nIn this paper we delve deep in the Transformer architecture by investigating two of\nits core components: self-attention and contextual embeddings. In particular, we\nstudy the identiﬁability of attention weights and token embeddings, and the ag-\ngregation of context into hidden tokens. We show that, for sequences longer than\nthe attention head dimension, attention weights are not identiﬁable. We propose\neffective attention as a complementary tool for improving explanatory interpreta-\ntions based on attention. Furthermore, we show that input tokens retain to a large\ndegree their identity across the model. We also ﬁnd evidence suggesting that iden-\ntity information is mainly encoded in the angle of the embeddings and gradually\ndecreases with depth. Finally, we demonstrate strong mixing of input information\nin the generation of contextual embeddings by means of a novel quantiﬁcation\nmethod based on gradient attribution. Overall, we show that self-attention distri-\nbutions are not directly interpretable and present tools to better understand and\nfurther investigate Transformer models.\n1 I NTRODUCTION\nIn this paper we investigate neural models of language based on self-attention by concentrating on\nthe concept of identiﬁability. Intuitively, identiﬁability refers to the ability of a model to learn stable\nrepresentations. This is arguably a desirable property, as it affects the replicability and interpretabil-\nity of the model’s predictions. Concretely, we focus on two aspects of identiﬁability. The ﬁrst is\nrelated to structural identiﬁability (Bellman & ˚Astr¨om, 1970): the theoretical possibility (a priori)\nto learn a unique optimal parameterization of a statistical model. From this perspective, we analyze\nthe identiﬁability of attention weights, what we call attention identiﬁability , in the self-attention\ncomponents of transformers (Vaswani et al., 2017), one of the most popular neural architectures\nfor language encoding and decoding. We also investigate token identiﬁability, as the ﬁne-grained,\nword-level mappings between input and output generated by the model. The role of attention as a\nmeans of recovering input-output mappings, and various types of explanatory insights, is currently\nthe focus of much research and depends to a signiﬁcant extent on both types of identiﬁability.\nWe contribute the following ﬁndings to the ongoing work: With respect to attention indentiﬁability,\nin Section 3, we show that – under mild conditions with respect to input sequence length and atten-\ntion head dimension – the attention weights for a given input are not identiﬁable. This implies that\nthere can be inﬁnitely many different attention weights that yield the same output. This ﬁnding chal-\nlenges the direct interpretability of attention distributions. As a supplement, we propose the concept\nof effective attention, a diagnostic tool that examines attention weights for model explanations by\nremoving the weight components that do not inﬂuence the model’s predictions.\n∗Equal contribution with authors in alphabetical order. Yang Liu initiated the transformer models study, per-\nceived and performed the study of attention identiﬁability and effective attention, i.e., Section 3 and Appendix\nA, and contributed to the token attribution discussions and calculations.\n1\narXiv:1908.04211v4  [cs.CL]  7 Feb 2020\nPublished as a conference paper at ICLR 2020\nWith respect to token identiﬁability, in Section 4, we devise an experimental setting where we probe\nthe hypothesis that contextual word embeddings maintain their identity as they pass through succes-\nsive layers of a transformer. This is an assumption made in much current research, which has not\nreceived a clear validation yet. Our ﬁndings give substance to this assumption, although it does not\nalways hold in later layers. Furthermore, we show that the identity information is largely encoded\nin the angle of the embeddings and that it can be recovered by a nearest neighbour lookup after a\nlearned linear mapping from hidden to input token space.\nIn Section 5 we further investigate the contribution of all input tokens in the generation of the con-\ntextual embeddings in order to quantify the mixing of token and context information. We introduce\nHidden Token Attribution, a quantiﬁcation method based on gradient attribution. We ﬁnd that self-\nattention strongly mixes context and token contributions. Token contribution decreases monoton-\nically with depth, but the corresponding token typically remains the largest individual contributor.\nWe also ﬁnd that, despite visible effects of long term dependencies, the context aggregated into the\nhidden embeddings is mostly local. We notice how, remarkably, this must be an effect of learning.\n2 B ACKGROUND ON TRANSFORMERS\nThe Transformer (Vaswani et al., 2017) is currently the neural architecture of choice for natural\nlanguage processing (NLP). At its core it consists of several multi-head self-attention layers. In these\nlayers, every token of the input sequence attends to all other tokens by projecting its embedding to\na query, key and value vector. Formally, let Q ∈Rds×dq be the query matrix, K ∈Rds×dq the\nkey matrix and V ∈Rds×dv the value matrix, where ds is the sequence length and dq and dv the\ndimension of query and value vectors, respectively. The output of an attention head is given by:\nAttention(Q,K,V ) = A ·V with A = softmax\n(\nQKT\n√\ndq\n)\n(1)\nThe attention matrix A ∈Rds×ds calculates, for each token in the sequence, how much the compu-\ntation of the hidden embedding at this sequence position should be inﬂuenced by each of the other\n(hidden) embeddings. Self-attention is a non-local operator, which means that at any layer a token\ncan attend to all other tokens regardless of the distance in the input. Self-attention thus produces so-\ncalled contextual word embeddings, as successive layers gradually aggregate contextual information\ninto the embedding of the input word.\nWe focus on a Transformer model called BERT (Devlin et al., 2019), although our analysis can be\neasily extended to other models such as GPT, (Radford et al., 2018; 2019) RoBERTa (Liu et al.,\n2019), XLNet (Yang et al., 2019b), or ALBERT (Lan et al., 2020). BERT operates on input se-\nquences of length ds. We denote input tokens in the sentence as xi, where i ∈[1,...,d s]. We use\nxi ∈Rd with embedding dimension d to refer to the sum of the token-, segment- and position\nembeddings corresponding to the input word at position i. We denote the contextual embedding at\nposition iand layer las el\ni. Lastly, we refer to the inputs and embeddings of all sequence positions as\nmatrices X and E, respectively, both in Rds×d. For all experiments we use the pre-trained uncased\nBERT-Base model as provided by Devlin et al. (2019)1.\n3 A TTENTION IDENTIFIABILITY\nWe begin with the identiﬁability analysis of self-attention weights. Drawing an analogy with struc-\ntural identiﬁability (Bellman & ˚Astr¨om, 1970), we state that the attention weights of an attention\nhead for a given input are identiﬁable if they can be uniquely determined from the head’s output. 2\nWe emphasize that attention weights are input dependent andnot model parameters. However, their\nidentiﬁability affects the interpretability of the output, i.e., whether attention weights can provide the\nbasis for explanatory insights on the model’s predictions (cf. Jain & Wallace (2019) and Wiegreffe &\nPinter (2019)). If attention is not identiﬁable, explanations based on attention may be unwarranted.\n1https://github.com/google-research/bert\n2Cf. Appendix A.1 for more background on indentiﬁability.\n2\nPublished as a conference paper at ICLR 2020\nThe output of a multi-head attention layer is the summation over each of the hsingle head outputs\n(cf. Eq. 1) multiplied by the matrix H ∈Rdv×d with reduced head dimension dv = d/h,\nAttention(Q,K,V )H = AEW VH = AT (2)\nwhere WV ∈Rd×dv projects the embedding E into the value matrix V = EW V, and we deﬁne\nT = EW VH. Here, the layer and head indices are omitted for simplicity, since the proof below\nis valid for each individual head and layer in Transformer models. Intuitively, the head output is a\nlinear combination of the T vectors using the attention as weighting coefﬁcients. If the sequence\nlength, i.e. the number of weighting coefﬁcient, is larger than the rank of T, attention weights are\nnot uniquely determined from the head output; i.e., they include free variables. In other words,\nsome of the T rows are linear combinations of others. We now prove, by analyzing the null space\ndimension of T, that attention weights are not identiﬁable using the head or ﬁnal model output.\n3.1 U PPER BOUND FOR RANK (T)\nWe ﬁrst derive the upper bound of the rank of matrix T = EW VH. Note that rank (ABC) ≤\nmin (rank(A),rank(B),rank(C)), therefore,\nrank (T) ≤min\n(\nrank(E),rank(WV),rank(H)\n)\n(3)\n≤min(ds,d,d,d v,dv,d)\n= min (ds,dv) .\nThe second step holds since rank (E) ≤min(ds,d), rank (WV) ≤min(d,dv) and rank(H) ≤\nmin(dv,d).\n3.2 T HE NULL SPACE OF T\nThe (left) null space LN(T) of T describes all vectors that are mapped to the zero vector by T:\nLN(T) = {˜xT ∈R1×ds|˜xTT = 0} (4)\nIts special property is that, for ˜A = [˜x1,˜x2,..., ˜xds]T where ˜xT\ni are vectors in this null space,\n(A + ˜A)T = AT. (5)\nIf the dimension of LN (T) is not zero, there exist inﬁnitely many attention weights A + ˜A yield-\ning the exact same attention layer output and ﬁnal model outputs. By applying the Rank-Nullity\ntheorem, the dimension of the null space is:\ndim(LN(T)) = ds −rank (T) ≥ds −min (ds,dv) =\n{ds −dv, if ds >dv\n0, otherwise (6)\nhere we make use of the facts that dim (LN(T)) = dim(N(TT)) and rank(T) = rank(TT) where\nN(T) represents the null space of a matrix T. Equality holds if E, WV and H are of full rank and\ntheir matrix product does not bring further rank reductions.\nHence, when the sequence length is larger than the attention head dimension ( ds > dv), self-\nattention is not unique. Furthermore, the null space dimension increases with the sequence length.\nIn the next section we show that the attention weights are also non-identiﬁable; i.e., the non-trivial\nnull space of T exists, even when the weights are constrained within the probability simplex.\n3.3 T HE NULL SPACE WITH PROBABILITY CONSTRAINTS\nSince A is the result of a softmax operation, its rows are constrained within the probability simplex:\nA ≥ 0 (element-wise), and A1 = 1, where 1 ∈ Rds is the vector of all ones. However, the\nderivation in Section (3.2) does not take these constraints into account. It shows that A is not\nunique, but it does not prove that alternative attention weights exist within the probability simplex,\nand thus that A is not identiﬁable. Below, we show that ˜A exists in LN(T) also when constraining\nthe weights of (A + ˜A) to the probability simplex.\n3\nPublished as a conference paper at ICLR 2020\nFor the row vectors from an alternative attention matrixA+ ˜A to be valid distributions, we require, in\naddition to A ≥0 (element-wise) and A1 = 1, that A+ ˜A ≥0 (element-wise), and A1+ ˜A1 = 1.\nFurthermore, ˜A must be in the (left) null space of T. We formalize the three conditions below:\na) ˜AT = 0 b) ˜A1 = 0 c) ˜A ≥−A (7)\nConditions (7a) and (7b) can be combined asA[T,1] = 0, where [T,1] is the augmented matrix re-\nsulting from adding a column vector of ones to T. Reusing the argument presented in Section (3.2),\nthe dimension of its (left) null space is: dim (LN([T,1])) ≥ max(ds −dv −1,0). Thus, for\nds −dv >1, the null space of [T,1] exists: it is a linear subspace of LN(T).\nWe now prove that condition (7c) can also be satisﬁed. We begin by providing an intuitive justiﬁca-\ntion. The condition restricts the space of ˜A from LN([T,1]) to be a bounded region which could be\ndifferent for each row vector a = (a1,a2,... ) of A. The null space LN ([T,1]) contains 0, deﬁn-\ning a surface passing through the origin. Since a is a probability vector, resulting from a softmax\ntransformation, each of its components is strictly positive, i.e., A >0 (element-wise). Hence, there\nexists ϵ >0 such that any point ˜a in the sphere centered at the origin with radius ϵ will satisfy\ncondition (7c), ˜a > −a. Crucially, this sphere intersects the null space, as they share the origin.\nAny point in this intersection satisﬁes all three conditions in (7).\nFormally, the construction of the null space vector˜a for the alternative attention weights ˜a+ a goes\nas follows. For a vector ˜a = (˜a1,˜a2,...) ∈LN([T,1]), to ensure one of its negative components\n˜ai < 0 satisfying condition (7c) that ˜ai ≥ −ai, one could shrink its magnitude into λ˜a with\n0 ≤λ≤−ai/˜ai, so λ˜ai ≥−ai. Considering all negative components i, the overall scaling factor is\nλmax = mini∈{i|˜ai<0}(−ai/˜ai) so that the direction from the origin {λ˜a|0 ≤λ≤λmax}satisﬁes\ncondition (7c). Here λmax is strictly greater than 0 because ai > 0. Only when there exists an\nindex ithat ˜ai <0 and ai ≈0, then this particular null space direction ˜a is highly conﬁned. In the\nextreme case, where a is a one-hot distribution, the solution should be an ˜a with only one negative\ncomponent. If such an ˜a does not exist in LN ([T,1]), the solution collapses to the trivial single\npoint ˜a = 0. However, in general, LN([T,1]) with probability constraints is non-trivial.\n3.4 E FFECTIVE ATTENTION\nThe non-identiﬁability of self-attention, due to the existence of the non-trivial null space of T,\nchallenges the interpretability of attention weights. However, one can decompose attention weights\nA into the component in the null space A∥and the component orthogonal to the null space A⊥:\nAT = (A∥+ A⊥)T = A⊥T (8)\nsince A∥∈LN(T) =⇒ A∥T = 0. Hence, we propose a novel concept named effective attention,\nA⊥= A −ProjectionLN(T)A, (9)\nwhich is the part of the attention weights that actually affects the model output. The null space pro-\njection is calculated by projecting attention weights into the left null space basis, i.e., the associated\nleft singular vectors.\nHere the deﬁnition of effective attention uses LN (T) instead of the null space LN ([T,1]) with\nprobability constraints. As a consequence, effective attention is not guaranteed to be a probability\ndistribution; e.g., some of the weights might be negative. One could deﬁne effective attention as the\nminimal norm alternative attention using LN([T,1]), or possibly constrain it within the probability\nsimplex. However, in this case, the minimal norm alternative attention is not orthogonal to the null\nspace LN(T) anymore. It seems unclear how to interpret the minimal norm alternative attention.\nThe reason being that the distinction between components that affect or do not affect the output\ncomputation3 are deﬁned with respect to LN (T) and not with respect to LN ([T,1]). In fact, there\nmay be useful information in the sign of the effective attention components. Although the combi-\nnation of value vectors in the transformer architecture uses weights in the probability simplex, these\nprobability constraints on attention may not be necessary. Hence, we provide here the base version\nof an effective attention, and leave the investigation of other formulations for future research.\n3A key aspect of the concept of identiﬁability.\n4\nPublished as a conference paper at ICLR 2020\n50 100\n0.4\n0.6\n0.8\n1\nSeq. Length\nPearson’s r\n(a)\n5 10\n0\n0.2\n0.4\n0.6\n0.8\nLayer\nRaw Attention\n(b)\n5 10\n0\n0.2\n0.4\n0.6\n0.8\nLayer\nEffective Attention\n[CLS]\n[SEP]\n. or ,\n(c)\nFigure 1: (a) Each point represents the Pearson correlation coefﬁcient of effective attention and raw\nattention as a function of token length. (b) Raw attention vs. (c) effective attention, where each point\nrepresents the average (effective) attention of a given head to a token type.\n3.5 E MPIRICAL EVIDENCE\nWe conclude by providing some initial empirical evidence in support of the notion that effective at-\ntention can serve as a complementary diagnostic tool for examining how attention weights inﬂuence\nmodel outputs.\nFirst, we show that effective attention can be detected, and can diverge signiﬁcantly from raw at-\ntention. In Figure 1a, we illustrate how the Pearson correlation between effective and raw attention\ndecreases with sequence length. We use the same Wikipedia samples as in Clark et al. (2019) with\nmaximum sequence length 128. This result is in line with our theoretical ﬁnding in Eq. 6 that states\nan increase in the dimension of the null space with the sequence length. Given a bigger null space,\nmore of the raw attention becomes irrelevant, yielding a lower correlation between effective and raw\nattention. Notice how, for sequences with fewer than dv = 64 tokens, the associated null space\ndimension is zero, and hence attention and effective attention are identical (Pearson correlation of\nvalue 1). This loss of correlation with increased sequence length questions the use of attention as\nexplanation in practical models, where it is not uncommon to use large sequence lengths. A few ex-\namples include: BERT for question answering (Alberti et al., 2019) and XL-Net (Yang et al., 2019a)\nwith ds = 512, or document translation (Junczys-Dowmunt, 2019) with ds = 1000.\nTo illustrate the point further, Figure 1 shows in (b) raw attention A and (c) effective attention A⊥,\nusing again the data of Clark et al. (2019). We compute the average attention of BERT and compare\nit to the corresponding average effective attention. Clark et al. (2019) conclude that the [CLS] token\nattracts more attention in early layers, the [SEP] tokens attract more in middle layers, and periods\nand commas do so in deep layers. However, after a gradient based investigation, they propose that\nattention to the [SEP] token is generally a “no-op”. The effective attention weights suggest a more\nconsistent pattern: while periods and commas seem to generally attract more attention than [CLS]\nand [SEP], the peak of [SEP] token observed by raw attention has disappeared. Effective attention\nprovides an explanation: the [SEP] token peak is irrelevant to the computation of the output for\nmiddle layers; i.e., it is in the null space component of the corresponding attention vector. The\nsame arguments also hold for the sharp peak of raw attention on punctuation tokens between layers\n10 and 12. An additional example showing similar results can be found in Appendix A.2. See\nalso Appendix A.3, where we discuss in more depth a case where effective attention would support\ninterpretive conclusions that differ from those one might draw solely based on raw attention. In\nconclusion, effective attention can help discover interesting interactions encoded in the attention\nweights which may be otherwise obfuscated by the null attention.\n4 T OKEN IDENTIFIABILITY\nWe now study the other fundamental element of transformers; the hidden vector representations of\ntokens, or contextual word embeddings. It is commonly assumed that a contextual word embedding\n5\nPublished as a conference paper at ICLR 2020\nkeeps its “identity”, which is tied to the input word, as it passes through the self-attention layers.\nSpeciﬁcally, we identify three cases where this assumption is made implicitly without justiﬁcation.\n• Visualizations/interpretations linking attention weights to attention between words, when\nin fact the attention is between embeddings, i.e., mixtures of multiple words (Vaswani\net al., 2017; Devlin et al., 2019; Vig, 2019; Clark et al., 2019; Raganato & Tiedemann,\n2018; V oita et al., 2019; Tang et al., 2018; Wangperawong, 2018; Padigela et al., 2019;\nBaan et al., 2019; Dehghani et al., 2019; Zenkel et al., 2019).\n• Attention accumulation methods that sum the attention to a speciﬁc sequence position over\nlayers and/or attention heads, when the given position might encode a different mixture of\ninputs in each layer (Clark et al., 2019; Baan et al., 2019; Klein & Nabi, 2019; Coenen\net al., 2019).\n• Using classiﬁers to probe hidden embeddings for word-speciﬁc aspects without factoring\nin how much the word is still represented (Lin et al., 2019; Peters et al., 2018).\nTo investigate this assumption we introduce the concept of token identiﬁability, as the existence of\na mapping assigning contextual embeddings to their corresponding input tokens. Formally, we state\nthat an embedding el\ni is identiﬁable if there exists a classiﬁcation function c(·) such that c(el\ni) = xi.\nFor identiﬁability we only require c(el\ni) to recover xi in a nearest neighbour sense within the same\ninput sentence. Therefore, for each layer lwe deﬁne cl(·) = NN(gl(·)), where NN is a 1-nearest\nneighbour lookup, and gl : Rd →Rd is a continuous function mapping embeddings to vectors\nof real numbers. Since we cannot prove the existence of gl analytically, we instead use a function\napproximator ˆgl(el\ni) = ˆxi, trained on a dataset of (el\ni,xi) pairs. We then say that el\ni is identiﬁable\nif ˆcl(el\ni) = NN(ˆgl(el\ni)) = xi. For evaluation we report the token identiﬁability rate deﬁned as the\npercentage of correctly identiﬁed tokens.\n4.1 S ETUP\nFor the experiments in this and subsequent sections we use the development dataset from the Mi-\ncrosoft Research Paraphrase Corpus (MRPC) dataset (Dolan & Brockett, 2005), while in Appendix\nD we provide results on two additional datasets. The MRPC development set contains 408 examples\nwith a sequence length ds between 26 and 92 tokens, with 58 tokens on average. We pass all 408\nsentences (21,723 tokens) through BERT and extract for each token the input embeddings xi and\nthe hidden embeddings el\ni at all layers. We then train ˆg on the regression task of predicting input\ntokens xi from hidden tokens el\ni. We experiment with two loss functions and similarity measures\nfor ﬁnding the nearest neighbour; cosine distance and L2 distance. We use 10-fold cross validation\nwith 70/15/15 train/validation/test splits per fold and ensure that tokens from the same sentence are\nnot split across sets. The validation set is used for early stopping. See Appendix B.1 for further\ndetails.\n4.2 E XPERIMENTAL RESULTS AND DISCUSSION\nIn a ﬁrst experiment, we use a linear perceptron without bias and a non-linear MLP ˆgMLP\nl , where\ntraining, validation and test data all come from layer l. Figure 2a shows the test set token iden-\ntiﬁability rate of ˆcl for l = [1 ,..., 12]. We also report a naive baseline ˆgnaive\nl (el\ni) = el\ni, i.e., we\ndirectly retrieve the nearest neighbour of el\ni from the input tokens. The results for ˆgnaive\nl show\nthat, according to both similarity measures, contextual embeddings in BERT stay close to their input\nembeddings up to layer 4, followed by a linear decrease in token identiﬁability rate. By training a\ntransformation to recover the original embedding, we see that most of the identity information is\nstill present in the contextualized embeddings. Speciﬁcally, a linear projection is enough to recover\n93% of the tokens in the last layer based on a cosine distance nearest neighbour lookup.\nThis experiment shows that although the identiﬁablity rate decreases with depth, tokens remain\nmostly identiﬁable across layers. Furthermore, we ﬁnd that cosine distance is more effective to\nrecover identity than L2 distance. Therefore, we conjecture that BERT encodes most of the identity\ninformation in the angle of the embeddings. Finally, Lin et al. (2019) show that BERT discards much\nof the positional information after layer 3. However, tokens remain largely identiﬁable throughout\nthe model, indicating that BERT does not only rely on the positional embeddings to track token\n6\nPublished as a conference paper at ICLR 2020\n2 4 6 8 10 12\n0.2\n0.4\n0.6\n0.8\n1\nLayer\nIdentiﬁability Rate\nˆgMLP\ncos,l ˆglin\ncos,l\nˆgnaive\ncos,l ˆgMLP\nl2,l\nˆglin\nl2,l ˆgnaive\nl2,l\n(a)\n2 4 6 8 10 12\n0.7\n0.8\n0.9\n1\nLayer\nIdentiﬁability Rate\nl=1 l=6\nl=11 l=12\n(b)\nFigure 2: (a) Identiﬁability of contextual word embeddings at different layers. Here, ˆgis trained and\ntested on the same layer. (b) glin\ncos,l trained on layer land tested on all layers.\nidentity. To provide further insights into contextual word embeddings, Appendix B.5 shows results\nfor recovering neighbouring input tokens.\nIn a second experiment we test how well the ˆglin\ncos,l trained only on (el\ni,xi) pairs from one layer l\ngeneralizes to all layers, see Figure 2b. For l= 1, the token identiﬁability rate on subsequent layers\ndrops quickly to below 70% at layers 11 and 12. Interestingly, forl= 12 a very different pattern can\nbe observed, where the identiﬁability is 94% for layer 12 and then almost monotonically increases\nwhen testing on earlier layers. Further, for l= 6 we see both patterns.\nThis experiment suggests that the nature of token identity changes as tokens pass through the model,\nand patterns learned on data from later layers transfer well to earlier layers. The experiment also\nshows that layer 12 is behaving differently than the other layers. In particular, generalizing to layer\n12 from layer 11 seems to be difﬁcult, signiﬁed by a sudden drop in token identiﬁability rate. We\nbelieve this is due to a task dependent parameter adaptation induced in the last layer by the next-\nsentence prediction task which only uses the CLS token (cf. Appendix B.4 for additional hints that\nthe last layer(s) behave differently). See Appendix B.3 for results of ˆglin\nl2,l, ˆgMLP\nl2,l and ˆgMLP\ncos,l .\nOverall, the results of this section suggest that one can associate most hidden embeddings with their\ninput token, for example for drawing conclusions based on (effective) attention weights. However,\nself-attention has the potential to strongly mix tokens across multiple layers, and hence it is unclear\nwhether token identiﬁability alone is enough to equate hidden embeddings with their input words,\nor whether we also need to take into account exactly how much of the word is still contained in\nthe hidden embedding. In order to address this question, we now study the degree of information\nmixing among embeddings, and introduce a tool to track the contributions of tokens to embeddings\nthroughout the model.\n5 A TTRIBUTION ANALYSIS TO IDENTIFY CONTEXT CONTRIBUTION\nWe consider the role of the contextual information in the hidden embeddings, which is accumulated\nthrough multiple paths in a multi-layer network. To shed more light on this process, we introduce\nHidden Token Attribution, a context quantiﬁcation method based on gradient attribution (Simonyan\net al., 2014) to investigate the hidden tokens’ sensitivity with respect to the input tokens.\nGradient based attribution approximates the neural network functionf(X) around a given sequence\nof input word embeddings X ∈Rds×d by the linear part of the Taylor expansion:\nf(X + ∆X) ≈f(X) + ∇Xf(X)T ·∆X (10)\nWith this, the network sensitivity is analyzed by looking at how small changes ∆X at the input\ncorrelate with changes at the output. Since in the linear approximation this change is given by the\ngradient ∇xif = δf(X)\nδxi\nfor a change in the i-th input token xi ∈Rd of X, the attribution of\nhow much input token xi affects the network output f(X) can be approximated by the L2 norm\n7\nPublished as a conference paper at ICLR 2020\n1 2 3 4 5 6 7 8 9 10 11 12\n0\n10\n20\n30\nLayer\nContribution [%]\n(a)\n1 2 3 4 5 6 7 8 9 10 11 12\n0\n10\n20\n30\nLayer\n˜P [%]\n(b)\nFigure 3: (a) Contribution of the input token to the embedding at the same position. The orange line\nrepresents the median value and outliers are not shown. (b) Percentage of tokens ˜P that are not the\nmain contributors to their corresponding contextual embedding at each layer.\nof the respective gradient: attr (xi) = ||∇xif||2. Since we are interested in how much a given\nhidden embedding el\nj attributes to the input tokensxi, i∈[1,2,...,d s], we deﬁne the relative input\ncontribution cl\ni,j of input xi to output f(X) = el\nj as\ncl\ni,j = ||∇l\ni,j||2\n∑ds\nk=0||∇l\nk,j||2\nwith ∇l\ni,j = δel\nj\nδxi\n(11)\nSince we normalize by dividing by the sum of the attribution values to all input tokens, we obtain\nvalues between 0 and 1 that represent the contribution of each input token xi to the hidden em-\nbedding el\nj. Hidden Token Attribution differs from the standard use of gradient attribution in that,\ninstead of taking the gradients of the output of the model with respect to the inputs in order to explain\nthe model’s decision, we calculate the contribution of the inputs to intermediate embeddings in order\nto track the mixing of information. Further details of this method are discussed in Appendix C.1.\n5.1 T OKEN MIXING : C ONTRIBUTION OF INPUT TOKENS\nWe use Hidden Token Attribution to extend the results of Section 4 showinghow much of the input\ntoken is contained in a given hidden embedding. In Figure 3a we report the contribution cl\nj,j of\ninput tokens xj to their corresponding hidden embeddings el\nj at the same position j for each layer\nl. After the ﬁrst layer the median contribution of the input token is less than a third (30.6%). The\ncontribution then decreases monotonically with depth; at layer 6 the median is only 14.4% and after\nthe last layer it is 10.7%. In Appendix C.5 we provide detailed results by word type. Next, we study\nwhich input token is the largest contributor to a given hidden embedding el\nj. The corresponding\ninput token xj generally has the largest contribution. Figure 3b shows the percentage ˜P of tokens\nthat are not the highest contributor to their hidden embedding at each layer. In the ﬁrst three layers\nthe original input xj always contributes the most to the embedding el\nj. In subsequent layers, ˜P\nincreases monotonically, reaching 18% in the sixth layer and 30% in the last two layers.\nThese results show that, starting from layer three, self-attention strongly mixes the input information\nby aggregating context into the hidden embeddings. This is in line with the results from Section 4,\nwhere we see a decrease in token identiﬁability rate after layer three. Nevertheless, ˜P is always\nhigher than the token identiﬁability error at the same layer, indicating that tokens are mixed in\na way that often permits recovering token identity even if the contribution of the original token is\noutweighed by other tokens. This suggests that there is some “identity information” that is preserved\nthrough the layers.\nThe strong mixing of information questions the common assumption that attention distributions can\nbe interpreted as “how much a word attends to another word”. However, the fact that tokens remain\nidentiﬁable despite information mixing opens a number of new interesting questions to be addressed\nby future research. In particular, this seeming contradiction may be solved by investigating the space\nin which hidden embeddings operate: is there a relation between semantics and geometric distance\nfor hidden embeddings? Are some embedding dimensions more important than others?\n8\nPublished as a conference paper at ICLR 2020\n1 2 3 4 5 6 7 8 9 10 11 12\n6\n8\n10\nLayer\nRel. Contribution (%)\n1st 2nd3rd 4th and 5th6th to 10th 11th onwards\n(a)\n-60 -40 -20 0 20 40 60\n0\n0.5\n1\nNeighbour\nTotal Contribution\nl= 12\nl= 6\nl= 1\n(b)\nFigure 4: (a) Relative contribution per layer of neighbours at different positions. (b) Total contribu-\ntion per neighbour for the ﬁrst, middle and last layers.\n5.2 C ONTRIBUTION OF CONTEXT TO HIDDEN TOKENS\nIn this section we study how context is aggregated into hidden embeddings. Figure 4a shows the\nrelative contribution of neighbouring tokens at each layer for the relative positions: ﬁrst, second,\nthird, fourth and ﬁfth together, sixth to 10th together, and the rest. The closest neighbours (1st)\ncontribute signiﬁcantly more in the ﬁrst layers than in later layers. Conversely, the most distant\nneighbours (11th onwards) contribute the most in deeper layers (cf. Appendix C.2). Despite the\nprogressive increase in long-range dependencies, the context in the hidden embeddings remains\nmostly local. Figure 4b represents the normalized total contribution aggregated over all tokens\nfrom each of their neighbours at the ﬁrst, middle and last layer. This ﬁgure shows that the closest\nneighbours consistently contribute the most to the contextual word embedding regardless of depth.\nOn the other hand, we indeed observe an increase of distant contributions at later layers.\nThe results of this section suggest that BERT learns local operators from data in an unsupervised\nmanner, in the absence of any such prior in the architecture. This behavior is not obvious, since\nattention is a highly non-local operator, and in turn indicates the importance of local dependencies\nin natural language. While contribution is local on average, we ﬁnd that there are exceptions, such\nas the [CLS] token (cf. Appendix C.3). Furthermore, using our Hidden Token Attribution method,\none can track how context is aggregated for speciﬁc tokens (cf. Appendix C.4).\n6 R ELATED WORK\nInput-output mappings play a key role in NLP. For example, in machine translation, they were in-\ntroduced in the form of explicit alignments between source and target words (Brown et al., 1993).\nNeural translation architectures re-introduced this concept in the form of attention (Bahdanau et al.,\n2015). The development of multi-head self-attention (Vaswani et al., 2017) has led to many impres-\nsive results in NLP. As a consequence, much work has been devoted to better understand what these\nmodels learn, with a particular focus on using attention to explain model decisions.\nJain & Wallace (2019) show that attention distributions of LSTM based encoder-decoder models are\nnot unique, and that adversarial attention distributions that do not change the model’s decision can\nbe constructed. They further show that attention distributions only correlate weakly to moderately\nwith dot-product based gradient attribution. Wiegreffe & Pinter (2019) also ﬁnd that adversarial\nattention distributions can be easily found, but that these alternative distributions perform worse on\na simple diagnostic task. Serrano & Smith (2019) ﬁnd that zero-ing out attention weights based on\ngradient attribution changes the output of a multi-class prediction task more quickly than zero-ing\nout based on attention weights, thus showing that attention is not the best predictor of learned fea-\nture importance. Pruthi et al. (2019) demonstrate that self-attention models can be manipulated to\nproduce different attention masks with very little cost in accuracy. These papers differ in their ap-\nproaches, but they all provide empirical evidence showing that attention distributions are not unique\nwith respect to downstream parts of the model (e.g., output) and hence should be interpreted with\n9\nPublished as a conference paper at ICLR 2020\ncare. Here, we support these empirical ﬁndings by presenting a theoretical proof of the identiﬁa-\nbility of attention weights. Further, while these works focus on RNN-based language models with\na single layer of attention, we instead consider multi-head multi-layer self-attention models. Our\ntoken classiﬁcation and token mixing experiments show that non-identiﬁable tokens increase with\ndepth, further reinforcing the point that the factors that contribute to the mixing of information are\ncomplex and deserve further study.\nV oita et al. (2019) and Michel et al. (2019) ﬁnd that only a small number of heads in BERT have a\nrelevant effect on the output. These results are akin to ours about the non-identiﬁability of attention\nweights, showing that a signiﬁcant part of attention weights do not affect downstream components.\nOne line of work investigates the internal representations of transformers by attaching probing clas-\nsiﬁers to different parts of the model. Tenney et al. (2019) ﬁnd that BERT has learned to perform\nsteps from the classical NLP pipeline. Similarly, Jawahar et al. (2019) show that lower layers of\nBERT learn syntactic features, while higher layers learn semantic features. They also argue that\nlong-range features are learned in later layers, which agrees with our attribution-based experiments.\n7 C ONCLUSION\nWe used the notion of identiﬁability to gain a better understanding of transformers from different\nyet complementary angles. We started by proving that attention weights are non-identiﬁable when\nthe sequence length is longer than the attention head dimension. Thus, inﬁnitely many attention\ndistributions can lead to the same internal representation and model output. As an alternative, we\npropose effective attention, a method that improves the interpretability of attention weights by pro-\njecting out the null space. Second, we show that tokens remain largely identiﬁable through a learned\nlinear transformation followed by a nearest neighbor lookup based on cosine similarity. However,\ninput tokens gradually become less identiﬁable in later layers. Finally, we present Hidden Token\nAttribution, a gradient-based method to quantify information mixing. This method is general and\ncan be used to investigate contextual embeddings in self-attention based models. In this work, we\nuse it to demonstrate that input tokens mix heavily inside transformers. This result means that\nattention-based interpretations, which suggest that a word at some layer is attending to another\nword can be improved by accounting for how the tokens are mixed inside the model. We further\nshow that context is progressively aggregated into the hidden embeddings while some identity in-\nformation is preserved. Moreover, we show that context aggregation is mostly local and that distant\ndependencies become relevant only in the last layers, which highlights the importance of local in-\nformation for natural language understanding. Our results suggest that some of the conclusions in\nprior work (Vaswani et al., 2017; Vig, 2019; Marecek & Rosa, 2018; Clark et al., 2019; Raganato &\nTiedemann, 2018; V oita et al., 2019; Tang et al., 2018; Wangperawong, 2018; Padigela et al., 2019;\nBaan et al., 2019; Lin et al., 2019; Dehghani et al., 2019; Zenkel et al., 2019; Klein & Nabi, 2019;\nCoenen et al., 2019) may be worth re-examining from this perspective.\nThere are still many open questions for future research. For one, by constraining effective atten-\ntion to the probability simplex, one could better compare it to standard attention, although in this\ncase non-inﬂuencing parts would be included in the weights. More research is needed to better un-\nderstand the differences between these formulations. Further, we ﬁnd that tokens mix and remain\nlargely identiﬁable. While these two conclusions are not necessarily at odds - a token can both gather\ncontext information and still retain the essence of the original input word - we believe that the rela-\ntionship between mixing and identiﬁability warrants further investigation. Moreover, it is becoming\nincreasingly difﬁcult to compare all the new transformer variants, and it is hence important to gain\na deeper understanding of this class of models. The concepts introduced in this paper could help in\nidentifying fundamental differences and commonalities between variants of self-attention models.\nACKNOWLEDGEMENTS\nWe would like to thank the reviewers and area chairs for their thorough technical comments and\nvaluable suggestions. We would also like to thank Jacob Devlin for feedback on an early draft. This\nresearch was supported with Cloud TPUs from Google’s TensorFlow Research Cloud (TFRC).\n10\nPublished as a conference paper at ICLR 2020\nREFERENCES\nChris Alberti, Kenton Lee, and Michael Collins. A BERT Baseline for the Natural Questions.\nhttps://arxiv.org/abs/1901.08634, 2019.\nJoris Baan, Maartje ter Hoeve, Marlies van der Wees, Anne Schuth, and Maarten de Rijke.\nDo transformer attention heads provide transparency in abstractive summarization? CoRR,\nabs/1907.00570, 2019.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In 3rd International Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\nhttp://arxiv.org/abs/1409.0473.\nR. Bellman and Karl Johan ˚Astr¨om. On structural identiﬁability. Mathematical Biosciences, 7:\n329–339, 1970.\nPeter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. The math-\nematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19\n(2):263–311, 1993. URL https://www.aclweb.org/anthology/J93-2003.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look\nat? an analysis of bert’s attention. CoRR, abs/1906.04341, 2019.\nAndy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda B. Vi ´egas, and Martin\nWattenberg. Visualizing and measuring the geometry of BERT. CoRR, abs/1906.02715, 2019.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal\ntransformers. In 7th International Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and\nShort Papers), pp. 4171–4186, 2019.\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, Jeju\nIsland, Korea, October 2005, 2005, 2005.\nXavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and\nStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010 , pp. 249–256,\n2010. URL http://proceedings.mlr.press/v9/glorot10a.html.\nDan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian\nerror linear units. CoRR, abs/1606.08415, 2016. URL http://arxiv.org/abs/1606.\n08415.\nSarthak Jain and Byron C. Wallace. Attention is not explanation. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019,\nVolume 1 (Long and Short Papers), pp. 3543–3556, 2019.\nGanesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Conference of the Association for Computational Linguis-\ntics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 3651–3657,\n2019. URL https://www.aclweb.org/anthology/P19-1356/.\nMarcin Junczys-Dowmunt. Microsoft translator at wmt 2019: Towards large-scale document-level\nneural machine translation. In Proceedings of the Fourth Conference on Machine Translation\n(Volume 2: Shared Task Papers, Day 1), pp. 225–233, 2019.\n11\nPublished as a conference paper at ICLR 2020\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,\n2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\nTassilo Klein and Moin Nabi. Attention is (not) all you need for commonsense reasoning. In\nProceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 4831–4836, 2019.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. Albert: A lite bert for self-supervised learning of language representations. In International\nConference on Learning Representations, 2020. URL https://openreview.net/forum?\nid=H1eA7AEtvS.\nYongjie Lin, Yi Chern Tan, and Robert Frank. Open sesame: Getting inside bert’s linguistic knowl-\nedge. arXiv preprint arXiv:1906.01698, 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nDavid Marecek and Rudolf Rosa. Extracting syntactic trees from transformer encoder self-\nattentions. In Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for\nNLP , BlackboxNLP@EMNLP 2018, Brussels, Belgium, November 1, 2018, pp. 347–349, 2018.\nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? CoRR,\nabs/1905.10650, 2019. URL http://arxiv.org/abs/1905.10650.\nHarshith Padigela, Hamed Zamani, and W. Bruce Croft. Investigating the successes and failures of\nBERT for passage re-ranking. CoRR, abs/1905.01758, 2019.\nMatthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual\nword embeddings: Architecture and representation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November\n4, 2018, pp. 1499–1509, 2018.\nNina P¨orner, Hinrich Sch¨utze, and Benjamin Roth. Evaluating neural network explanation methods\nusing hybrid documents and morphosyntactic agreement. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pp. 340–350, 2018.\nDanish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C Lipton. Learning\nto deceive with attention-based explanations. arXiv preprint arXiv:1909.07913, 2019.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\nderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.\nAlessandro Raganato and J ¨org Tiedemann. An analysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the Workshop: Analyzing and Interpreting Neural\nNetworks for NLP , BlackboxNLP@EMNLP 2018, Brussels, Belgium, November 1, 2018, pp. 287–\n297, 2018.\nSoﬁa Serrano and Noah A. Smith. Is attention interpretable? In Proceedings of the 57th Conference\nof the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2,\n2019, Volume 1: Long Papers , pp. 2931–2951, 2019. URL https://www.aclweb.org/\nanthology/P19-1282/.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:\nVisualising image classiﬁcation models and saliency maps. In 2nd International Conference on\nLearning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track\nProceedings, 2014.\n12\nPublished as a conference paper at ICLR 2020\nGongbo Tang, Rico Sennrich, and Joakim Nivre. An analysis of attention mechanisms: The case\nof word sense disambiguation in neural machine translation. In Proceedings of the Third Con-\nference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 -\nNovember 1, 2018, pp. 26–35, 2018.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 4593–4601, 2019.\nKristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. Feature-rich part-of-\nspeech tagging with a cyclic dependency network. In Human Language Technology Conference\nof the North American Chapter of the Association for Computational Linguistics, HLT-NAACL\n2003, Edmonton, Canada, May 27 - June 1, 2003, 2003.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems 30: Annual Conference on Neural Information Processing Systems\n2017, 4-9 December 2017, Long Beach, CA, USA, pp. 5998–6008, 2017.\nJesse Vig. Visualizing attention in transformer-based language representation models. CoRR,\nabs/1904.02679, 2019.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of\nthe 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy,\nJuly 28- August 2, 2019, Volume 1: Long Papers, pp. 5797–5808, 2019.\nArtit Wangperawong. Attending to mathematical language with transformers. CoRR,\nabs/1812.02825, 2018.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471, 2018.\nSarah Wiegreffe and Yuval Pinter. Attention is not not explanation. CoRR, abs/1908.04626, 2019.\nURL http://arxiv.org/abs/1908.04626.\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-\ntence understanding through inference. InProceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pp. 1112–1122. Association for Computational Linguistics, 2018. URL\nhttp://aclweb.org/anthology/N18-1101.\nBaosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong Meng, Lidia S. Chao, and Tong Zhang.\nModeling localness for self-attention networks. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - Novem-\nber 4, 2018 , pp. 4449–4458, 2018. URL https://aclanthology.info/papers/\nD18-1475/d18-1475.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and\nQuoc V . Le. Xlnet: Generalized autoregressive pretraining for language understanding.\nhttps://arxiv.org/abs/1906.08237, 2019a.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.\nXlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural\ninformation processing systems, pp. 5754–5764, 2019b.\nThomas Zenkel, Joern Wuebker, and John DeNero. Adding interpretable attention to neural transla-\ntion models improves word alignment. CoRR, abs/1901.11359, 2019.\n13\nPublished as a conference paper at ICLR 2020\nA I DENTIFIABILITY OF SELF -ATTENTION\nA.1 B ACKGROUND ON ATTENTION IDENTIFIABILITY\nOften, the identiﬁability issue arises for a model with a large number of unknown parameters and\nlimited observations. Taking a simple linear model y = x1β1 + x2β2 as an example, when there\nis only one observation (y,x1,x2), model parameters β1 and β2 cannot be uniquely determined.\nMoreover, in the matrix form Y = Xβ, by deﬁnition the parameter β is identiﬁable only if Y =\nXβ1 and Y = Xβ2 imply β1 = β2. So if the null space contains only the zero solution {β|Xβ =\n0}= {0}, i.e., Xβ1 −Xβ2 = X(β1 −β2) = 0 = ⇒ β1 −β2 = 0, the model is identiﬁable.\nTherefore, the identiﬁability of parameters in a linear model is linked to the dimension of the null\nspace, which in the end is determined by the rank of X.\nA.2 A DDITIONAL RESULTS OF THE EFFECTIVE ATTENTION VS . R AW ATTENTION RESULTS\nIn Figure 5 we provide a recreation of the ﬁgure regarding the attention of tokens towards the [SEP]\ntoken found in (Clark et al., 2019, Figure 2) with average attention as well as average effective\nattention. Again, we see that most of the raw attention lies effectively in the null space, conﬁrming\nthe pattern of Figure 1. The ﬁgures are produced using the code from Clark et al. (2019).\n2 4 6 8 10 12\n0\n0.5\n1\nLayer\nEffective Attention\nother →[SEP]\n[SEP] →[SEP]\n(a)\n2 4 6 8 10 12\n0\n0.5\n1\nLayer\nRaw Attention\n(b)\nFigure 5: Effective attention (a) vs. raw attention (b). (a) Each point represents the average effec-\ntive attention from a token type to a token type. Solid lines are the average effective attention of\ncorresponding points in each layer. (b) is the corresponding ﬁgures using raw attention weights.\nA.3 A CLOSER LOOK AT EFFECTIVE ATTENTION WEIGHTS\nHere we discuss an example of how effective attention might lead to interpretive conclusions that\ndiffer from raw attention. Figure 6 plots the attention weights (raw, effective, null) from one of the\nattention heads in BERT’s layer 4, for the following passage:\n”[CLS] research into military brats has consistently shown them to be better behaved than their\ncivilian counterparts. [SEP] hypotheses as to why brats are better behaved: ﬁrstly, military parents\nhave a lower threshold for misbehavior in their children; secondly, the mobility of teenagers might\nmake them less likely to attract attention to themselves, as many want to ﬁt in and are less secure\nwith their surroundings; and thirdly, normative constraints are greater, with brats knowing that their\nbehavior is under scrutiny and can affect the military member’s career. teenage years are typically a\nperiod when people establish independence by taking some risks away from their [SEP]”.\nFor readability, on the y-axis, we consider just the sentence ”the mobility of teenagers might make\nthem less likely to attract attention to themselves, as many want to ﬁt in and are less secure with\ntheir surroundings”.\nThe following seems worth noticing:\n• Raw attention weights are by and large concentrated either on the structural components,\n[CLS] and [SEP], or on the semi-monotonic, near diagonal alignments.\n14\nPublished as a conference paper at ICLR 2020\n• Effective attention weights are more uniform, in general. They are still concentrated near\nthe diagonal elements, although less so than in raw attention. However, the attention on\n[CLS] and [SEP] has vanished. The collapse of the [CLS] and [SEP] weights brings to\nthe surface other interesting things. As an example, we point out the highest weight on\nthe attention matrix, that is not on the diagonal. This involves (highlighted by means of\nthe yellow lines) the main verb of the selected sentence, ”make”, whose object is ”them”\n(teenagers), and the pronoun ”them” (the direct object of the ﬁrst sentence, ”military brats”,\n48 positions away). The two are co-referential, as both refer to the main subject of the\npassage, military brats.\n• Null attention weights are also more uniform than raw attention ones. Interestingly, they\nseem to carry all the mass of the [CLS] and [SEP] tokens. There is a visible degree of\nredundancy between the null attention weights and the effective ones, but also clear com-\nplementary elements.\nOne should not extrapolate too much from a single observation. Further research is needed on this\ntopic. However, this example is a proof of concept that raw and effective attention can diverge\nqualitatively, in signiﬁcant ways. It agrees with the hypothesis that the weights on the structural\ncomponents may act as sinks, as observed in (Clark et al., 2019), but also tells us how this happens.\nFurthermore, it indicates that attention in the null space can obfuscate other valuable interactions\nthat may be recoverable by inspecting effective attention.\n15\nPublished as a conference paper at ICLR 2020\n[CLS]\nresearch\ninto\nmilitary\nbrat\n##s\nhas\nconsistently\nshown\nthem\nto\nbe\nbetter\nbehaved\nthan\ntheir\ncivilian\ncounterparts\n.\n[SEP]\n##oth\n##eses\nas\nto\nwhy\nbrat\n##s\nare\nbetter\nbehaved\n:\nfirstly\n,\nmilitary\nparents\nhave\na\nlower\nthreshold\nfor\nmis\n##be\n##ha\n##vio\n##r\nin\ntheir\nchildren\n;\nsecondly\n,\nthe\nmobility\nof\nteenagers\nmight\nmake\nthem\nless\nlikely\nto\nattract\nattention\nto\nthemselves\n,\nas\nmany\nwant\nto\nfit\nin\nand\nare\nless\nsecure\nwith\ntheir\nsurroundings\n;\nand\nthird\n##ly\n,\nnorma\n##tive\nconstraints\nare\ngreater\n,\nwith\nbrat\n##s\nknowing\nthat\ntheir\nbehavior\nis\nunder\nscrutiny\nand\ncan\naffect\nthe\nmilitary\nmember\n'\ns\ncareer\n.\nteenage\nyears\nare\ntypically\na\nperiod\nwhen\npeople\nestablish\nindependence\nby\ntaking\nsome\nrisks\naway\nfrom\ntheir\n[SEP]\n,\nthe\nmobility\nof\nteenagers\nmight\nmake\nthem\nless\nlikely\nto\nattract\nattention\nto\nthemselves\n,\nas\nmany\nwant\nto\nfit\nin\nand\nare\nless\nsecure\nwith\ntheir\nsurroundings\n;\n 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nresearch\ninto\nmilitary\nbrat\n##s\nhas\nconsistently\nshown\nthem\nto\nbe\nbetter\nbehaved\nthan\ntheir\ncivilian\ncounterparts\n.\n[SEP]\n##oth\n##eses\nas\nto\nwhy\nbrat\n##s\nare\nbetter\nbehaved\n:\nfirstly\n,\nmilitary\nparents\nhave\na\nlower\nthreshold\nfor\nmis\n##be\n##ha\n##vio\n##r\nin\ntheir\nchildren\n;\nsecondly\n,\nthe\nmobility\nof\nteenagers\nmight\nmake\nthem\nless\nlikely\nto\nattract\nattention\nto\nthemselves\n,\nas\nmany\nwant\nto\nfit\nin\nand\nare\nless\nsecure\nwith\ntheir\nsurroundings\n;\nand\nthird\n##ly\n,\nnorma\n##tive\nconstraints\nare\ngreater\n,\nwith\nbrat\n##s\nknowing\nthat\ntheir\nbehavior\nis\nunder\nscrutiny\nand\ncan\naffect\nthe\nmilitary\nmember\n'\ns\ncareer\n.\nteenage\nyears\nare\ntypically\na\nperiod\nwhen\npeople\nestablish\nindependence\nby\ntaking\nsome\nrisks\naway\nfrom\ntheir\n[SEP]\n,\nthe\nmobility\nof\nteenagers\nmight\nmake\nthem\nless\nlikely\nto\nattract\nattention\nto\nthemselves\n,\nas\nmany\nwant\nto\nfit\nin\nand\nare\nless\nsecure\nwith\ntheir\nsurroundings\n;\n 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nresearch\ninto\nmilitary\nbrat\n##s\nhas\nconsistently\nshown\nthem\nto\nbe\nbetter\nbehaved\nthan\ntheir\ncivilian\ncounterparts\n.\n[SEP]\n##oth\n##eses\nas\nto\nwhy\nbrat\n##s\nare\nbetter\nbehaved\n:\nfirstly\n,\nmilitary\nparents\nhave\na\nlower\nthreshold\nfor\nmis\n##be\n##ha\n##vio\n##r\nin\ntheir\nchildren\n;\nsecondly\n,\nthe\nmobility\nof\nteenagers\nmight\nmake\nthem\nless\nlikely\nto\nattract\nattention\nto\nthemselves\n,\nas\nmany\nwant\nto\nfit\nin\nand\nare\nless\nsecure\nwith\ntheir\nsurroundings\n;\nand\nthird\n##ly\n,\nnorma\n##tive\nconstraints\nare\ngreater\n,\nwith\nbrat\n##s\nknowing\nthat\ntheir\nbehavior\nis\nunder\nscrutiny\nand\ncan\naffect\nthe\nmilitary\nmember\n'\ns\ncareer\n.\nteenage\nyears\nare\ntypically\na\nperiod\nwhen\npeople\nestablish\nindependence\nby\ntaking\nsome\nrisks\naway\nfrom\ntheir\n[SEP]\n,\nthe\nmobility\nof\nteenagers\nmight\nmake\nthem\nless\nlikely\nto\nattract\nattention\nto\nthemselves\n,\nas\nmany\nwant\nto\nfit\nin\nand\nare\nless\nsecure\nwith\ntheir\nsurroundings\n;\n 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 6: Raw attention weights (top), Effective attention weights (middle) and Null attention weights (bottom).\n16\nPublished as a conference paper at ICLR 2020\nB T OKEN IDENTIFIABILITY EXPERIMENTS\nB.1 E XPERIMENTAL SETUP AND TRAINING DETAILS\nThe linear perceptron and MLP are both trained by either minimizing the L2 or cosine distance loss\nusing the ADAM optimizer (Kingma & Ba, 2015) with a learning rate ofα= 0.0001, β1 = 0.9 and\nβ2 = 0.999. We use a batch size of 256. We monitor performance on the validation set and stop\ntraining if there is no improvement for 20 epochs. The input and output dimension of the models is\nd= 768; the dimension of the contextual word embeddings. For both models we performed a learn-\ning rate search over the valuesα∈[0.003,0.001,0.0003,0.0001,0.00003,0.00001,0.000003]. The\nweights are initialized with the Glorot Uniform initializer (Glorot & Bengio, 2010). The MLP has\none hidden layer with 1000 neurons and uses the gelu activation function (Hendrycks & Gimpel,\n2016), following the feed-forward layers in BERT and GPT. We chose a hidden layer size of 1000\nin order to avoid a bottleneck. We experimented with using a larger hidden layer of size 3072\nand adding dropout to more closely match the feed-forward layers in BERT. This only resulted in\nincreased training times and we hence deferred from further architecture search.\nWe split the data by sentences into train/validation/test according to a 70/15/15 split. This way of\nsplitting the data ensures that the models have never seen the test sentences (i.e., contexts) during\ntraining. In order to get a more robust estimate of performance we perform the experiments in\nFigure 2a using 10-fold cross validation. The variance, due to the random assignment of sentences\nto train/validation/test sets, is small, and hence not shown.\nB.2 G ENERALIZATION ERROR\nFigure 7 shows the token identiﬁability rate for train and test set for both models, linear and MLP,\nwhen using L2 distance. Both models are overﬁtting to the same degree. The fact that the linear\nmodel has about the same generalization error as the MLP suggests that more training data would\nnot signiﬁcantly increase performance on the test set. Further, we trained the MLP on layer 11\nusing 50%, 80%, 90% and 100% of the training data set. The MLP achieved the following token\nidentiﬁability rate on the test set: 0.74, 0.8, 0.81, 0.82. This indicates that the MLP would not proﬁt\nmuch from more data.\nWe do not report the generalization error for the models trained to minimize cosine distance, as the\nlinear and non-linear perceptrons perform almost equally.\n2 4 6 8 10 12\n0.8\n0.9\n1\nLayer\n1-NN Recovery Accuracy\nLinear Train\nLinear Test\nMLP Train\nMLP Test\nFigure 7: Train and test token identiﬁability rates for the linear perceptron and MLP.\n17\nPublished as a conference paper at ICLR 2020\nB.3 A DDITIONAL RESULTS FOR FIGURE 2B\nFigure 2b in the main text only shows results of the linear perceptron trained to minimize cosine\ndistance on layers l = [1 ,6,11,12] and tested on all other layers. Figures 8, 9 and 10 show the\ncorresponding results for the linear perceptron trained to minimize cosine distance, and for the\nMLP trained to minimize L2 and cosine distance respectively. Overall, all ﬁgures show the same\nqualitative trends as presented in Section 4 of the main text: Generalizing to later layers works\nconsiderably worse than the other way around. The linear perceptrons seem to generalize better\nacross layers, likely due to the MLPs overﬁtting more to the particular layers they are trained on.\n2 4 6 8 10 12\n0.2\n0.4\n0.6\n0.8\n1\nLayer\nIdentiﬁability Ratel=1 l=6\nl=11 l=12\nFigure 8: Linear Perceptron trained to minimize L2 distance generalizing to all layers.\n2 4 6 8 10 12\n0.2\n0.4\n0.6\n0.8\n1\nLayer\nIdentiﬁability Ratel=1 l=6\nl=11 l=12\nFigure 9: MLP trained to minimize L2 distance generalizing to all layers.\n18\nPublished as a conference paper at ICLR 2020\n2 4 6 8 10 12\n0.4\n0.6\n0.8\n1\nLayer\nIdentiﬁability Ratel=1 l=6\nl=11 l=12\nFigure 10: MLP trained to minimize cosine distance generalizing to all layers.\n19\nPublished as a conference paper at ICLR 2020\nB.4 T OKEN IDENTITY - FROM HIDDEN TOKENS TO HIDDEN TOKENS\nFigure 11 shows results for identifying tokens across single layers of BERT, i.e., the input to g is\n(e1\ni,xi) in the ﬁrst layer, and subsequently (el\ni,el−1\ni ), where l = [1,..., 12]. This experiment gives\nfurther insight into what kinds of transformations are applied by each transformer layer separately,\nas opposed to the cumulative transformations shown in Section 4 of the main text. Interestingly,\neven the naive baselines perform well across single layers. This shows that BERT only applies\nsmall changes to the contextual word embeddings, whereas overall the angle (as indicated by the\nnaive baseline using cosine distance) is affected less than the magnitude of the word embeddings\n(indicated by the naive baseline using L2 distance).\nFigure 11 shows that tokens are on average more difﬁcult to identify across later layers. In the main\ntext we hypothesize that the qualitative change seen in later layers could be due to a task-speciﬁc\nparameter adaptation during the second (next sentence prediction) pre-training phase. A possible\nreason is that during this pre-training-phase, BERT only needs the [CLS] token in the last layer,\nwhich is qualitatively very different form the ﬁrst (masked language modeling) pre-training phase,\nwhere potentially all the tokens are needed in the last layer.\nTo further verify this hypothesis we experimented with BERT ﬁne-tuned on two datasets, MRPC and\nCoLA (Warstadt et al., 2018). During the ﬁne-tuning phase, similar to the next sentence prediction\npre-training phase, only the [CLS] token is needed at the last layer. If task-dependent parame-\nter adaptation indeed has a different inﬂuence on the last layer(s) than on earlier layers, then we\nshould be able to see a difference between the ﬁnetuned and non-ﬁnetuned cases. Figures 12 and 13\ncompare the naive baselines across single layers for BERT ﬁnetuned on MRPC and CoLA, respec-\ntively. Indeed, one can see a remarkable decrease in identiﬁability across the last layer for L2-based\nnearest neighbour lookup, further indicating that the last layers are indeed more strongly affected\nby different ﬁne-tuning objectives. Nearest neighbour lookup based on cosine distance is affected\nmuch less, indicating that in terms of token identiﬁability, the last layers are only slightly affected\nby ﬁne-tuning.\n1 2 3 4 5 6 7 8 9 10 11 12\n0.92\n0.94\n0.96\n0.98\n1\nLayer\nIdentiﬁability RateˆgMLP\ncos,l ˆglin\ncos,l ˆgnaive\ncos,l\nˆgMLP\nL2,l ˆglin\nL2,l ˆgnaive\nL2,l\nFigure 11: Token identiﬁability across single layers. These results are for non ﬁne-tuned BERT on\nMRPC.\n20\nPublished as a conference paper at ICLR 2020\n1 2 3 4 5 6 7 8 9 10 11 12\n0.6\n0.8\n1\nLayer\nIdentiﬁability Rateˆgnaive\nL2,l ˆgnaive,fine\nL2,l\nˆgnaive\ncos,l ˆgnaive,fine\ncos,l\nFigure 12: Token identiﬁability across single layers, comparing non ﬁne-tuned (dashed) BERT\nagainst BERT ﬁne-tuned on MRPC (solid).\n1 2 3 4 5 6 7 8 9 10 11 12\n0.85\n0.9\n0.95\n1\nLayer\nIdentiﬁability Rateˆgnaive\nL2,l ˆgnaive,fine\nL2,l\nˆgnaive\ncos,l ˆgnaive,fine\ncos,l\nFigure 13: Token identiﬁability across single layers, comparing non ﬁne-tuned BERT (dashed)\nagainst BERT ﬁne-tuned on CoLA (solid).\n21\nPublished as a conference paper at ICLR 2020\nB.5 T OKEN IDENTITY - RECOVER NEIGHBOURING INPUT TOKENS\nIn Section 4 of the main text we show that tokens at positioniremain largely identiﬁable throughout\nthe layers of BERT. In this section we show results of a related experiment, where we test how much\ninformation about tokens at neighbouring positions is contained in a contextual word embedding.\nMore formally, the input to g is (el\ni,xi±k), where k ∈ {1,2,3}. Thus, we try to recover input\ntoken xi+k from hidden token el\ni. Figures 14, 15, 16 and 17 show the results of for ˆglin\ncos,l, ˆgMLP\ncos,l ,\nˆgMLP\nL2,l and ˆglin\nL2,l, respectively. In the ﬁgures, blue corresponds to “previous” tokens and red to “next”\ntokens.\nFrom the ﬁgures we can see that tokens do contain information about neighbouring tokens that\nlets us recover the neighbouring tokens based on a transformation and subsequent nearest neigh-\nbour lookup. The identiﬁability rate drops both with increasing k, but also with increasing depth.\nInterestingly, recovering left (blue) and right (red) neighbours shows different behaviour, indicat-\ning that BERT is treating left and right context differently, despite having been pre-trained using a\nbi-directional language modeling objective.\nOverall, neighbouring tokens can be recovered to a much lower degree than same-position tokens\n(cf. Section4).\n1 2 3 4 5 6 7 8 9 10 11 12\n0.2\n0.4\n0.6\n0.8\nLayer\nIdentiﬁability Rate\ni-1 i-2\ni-3 i+1\ni+2 i+3\nFigure 14: Recovering neighbouring input tokens using ˆglin\ncos,l.\n1 2 3 4 5 6 7 8 9 10 11 12\n0.2\n0.4\n0.6\n0.8\n1\nLayer\nIdentiﬁability Rate\ni-1 i-2\ni-3 i+1\ni+2 i+3\nFigure 15: Recovering neighbouring input tokens using ˆgmlp\ncos,l.\n22\nPublished as a conference paper at ICLR 2020\n1 2 3 4 5 6 7 8 9 10 11 12\n0.2\n0.4\n0.6\nLayer\nIdentiﬁability Rate\ni-1 i-2\ni-3 i+1\ni+2 i+3\nFigure 16: Recovering neighbouring input tokens using ˆgmlp\nL2,l.\n1 2 3 4 5 6 7 8 9 10 11 12\n0.2\n0.4\n0.6\nLayer\nIdentiﬁability Rate\ni-1 i-2\ni-3 i+1\ni+2 i+3\nFigure 17: Recovering neighbouring input tokens using ˆglin\nl2,l.\n23\nPublished as a conference paper at ICLR 2020\nC C ONTEXT CONTRIBUTION ANALYSIS\nC.1 H IDDEN TOKEN ATTRIBUTION : D ETAILS\nThe attribution method proposed in Section?? to calculate the contribution of input tokens to a given\nembedding does not look at the output of the model but at the intermediate hidden representations\nand therefore is task independent. Since the contribution values do not depend on the task that\nis evaluated, we can compare these values directly to attention distributions, which are also task-\nindependent. In this way, we can compare to other works in the literature (Vig, 2019; Clark et al.,\n2019; Klein & Nabi, 2019; Coenen et al., 2019; Lin et al., 2019) by using the publicly available\npretrained BERT model in our analyses without ﬁne-tuning it to a speciﬁc task.\nFurthermore, since we are not interested in analysing how the input affects the output of the model\nbut in quantifying the absolute contribution of the input tokens to the hidden embeddings, we use the\nL2 norm of the gradients. If we were analyzing whether the input contributed positively or negatively\nto a given decision, the dot-product of the input token embedding with the gradient would be the\nnatural attribution choice (P¨orner et al., 2018).\nC.2 C ONTEXT IDENTIFIABILITY : D ETAILS\nTo calculate the relative contribution values shown in Figure 4a we ﬁrstly calculate the mean of the\nleft and right neighbours for each of the groups of neighbours, i.e., ﬁrst, second, third, fourth and\nﬁfth, sixth to 10th and, from 11th onwards. Then we aggregate the values averaging over all the\ntokens in the MRPC evaluation set. Finally, we normalize for each group so that the sum of the\ncontribution values of each group is one. In this way, we can observe in which layer the contribution\nof a given group of neighbours is the largest.\nOur results on context identiﬁability from Section 5.2 complement some of the studies in previous\nliterature. In (Jawahar et al., 2019) the authors observe that transformers learn local syntactic tasks\nin the ﬁrst layers and long range semantic tasks in the last layers. We explain this behavior from\nthe point of view of context aggregation by showing that distant context acquires more importance\nin the last layers (semantic tasks) while the ﬁrst layers aggregate local context (syntactic tasks).\nFurthermore, the results showing that the context aggregation is mainly local, specially in the ﬁrst\nlayers, provide an explanation for the increase in performance observed in (Yang et al., 2018). In that\nwork, the authors enforce a locality constraint in the ﬁrst layers of transformers, which pushes the\nmodel towards the local operators that it naturally tends to learn, as we show in Figure 4b, improving\nin this way the overall performance.\nC.3 C ONTEXT CONTRIBUTION TO CLS TOKEN\nIn this section we use Hidden Token Attribution to look at the contribution of the context to the\n[CLS] token, which is added to the beginning of the input sequence by the BERT pre-processing\npipeline. This is an especially interesting token to look at because the decision of BERT for a\nclassiﬁcation task is based on the output in the [CLS] token. Furthermore, like the [SEP] token,\nit does not correspond to a natural language word and its position in the input sequence does not\nhave any meaning. Therefore, the conclusion that context is on average predominantly local (cf.\nSection 5.2), is likely to not hold for [CLS].\nThe second and ﬁnal pre-training task that BERT is trained on is next sentence prediction. During\nthis task, BERT receives two sentences separated by a [SEP] token as input, and then has to predict\nwhether the second sentence follows the ﬁrst sentence or not. Therefore, it is expected that the\ncontext aggregated into the [CLS] token comes mainly from the tokens around the ﬁrst [SEP] token,\nwhich marks the border between the ﬁrst and second sentence in the input sequence. In Figure 18 we\nshow the contribution to the [CLS] token from all of its neighbours averaged over all the examples\nin the MRPC evaluation set for the ﬁrst, middle and last layers. In Figure 18a, the [CLS] token\nis placed at position 0 and we see how the context contribution comes mainly from the tokens\naround position 30, which is roughly the middle of the input examples. In Figure 18b we center\nthe contribution around the ﬁrst [SEP] token and indeed, it becomes clear that the [CLS] token is\naggregating most of its context from the tokens around [SEP], i.e., from the junction between both\n24\nPublished as a conference paper at ICLR 2020\nsentences. In particular, the two tokens with the highest contribution are the tokens directly before\nand after [SEP]. Also, it seems that the second sentence contributes more to [CLS] than the ﬁrst one.\nThese results give an insight on what information BERT uses to solve next sentence prediction and\nserves as an illustrative example of how Hidden Token Attribution can be used to analyze transform-\ners.\n0 20 40 60\n0\n0.5\n1\nNeighbour\nTotal contribution to [CLS]\nl= 12\nl= 6\nl= 1\n(a)\n-40 -20 [SEP] 20 40\n0\n0.5\n1\nNeighbour\nTotal contribution to [CLS]\nl= 12\nl= 6\nl= 1\n(b)\nFigure 18: Normalized total contribution to the [CLS] token (a) centered around [CLS] at position\n0 (b) centered around [SEP].\nC.4 T RACKING CONTEXT CONTRIBUTION\nHere we show examples of how Hidden Token Attribution can track how context is aggregated for\na given word at each layer. For reasons of space we show only few words of a randomly picked\nsentence of the MRPC evaluation set, which is tokenized as follows:\n[CLS] he said the foods ##er ##vic ##e pie business doesn ’ t fit\nthe company ’ s long - term growth strategy . [SEP] \" the foods\n##er ##vic ##e pie business does not fit our long - term growth\nstrategy . [SEP]\n[CLS]\nhe\nsaid\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoesn\n'\nt\nfit\nthe\ncompany\n'\ns\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n\"\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoes\nnot\nfit\nour\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n1\n3\n5\n7\n9\n11 Layer\n[CLS]\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nFigure 19: [CLS]: Aggregates context from all tokens but more strongly from those around the ﬁrst\n[SEP] token. We hypothesize that this is due to the Next Sentence Prediction pre-training.\n25\nPublished as a conference paper at ICLR 2020\n[CLS]\nhe\nsaid\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoesn\n'\nt\nfit\nthe\ncompany\n'\ns\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n\"\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoes\nnot\nfit\nour\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n1\n3\n5\n7\n9\n11 Layer\nhe\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nFigure 20: he: Aggregates most context from the main verb of the sentence, ”said”.\n[CLS]\nhe\nsaid\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoesn\n'\nt\nfit\nthe\ncompany\n'\ns\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n\"\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoes\nnot\nfit\nour\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n1\n3\n5\n7\n9\n11 Layer\nsaid\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nFigure 21: said: Aggregates context mainly from its neighborhood, the main verb of the subordinate\nsentence and the border between the two input sentences.\n[CLS]\nhe\nsaid\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoesn\n'\nt\nfit\nthe\ncompany\n'\ns\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n\"\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoes\nnot\nfit\nour\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n1\n3\n5\n7\n9\n11 Layer\nfit\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nFigure 22: ﬁt: In the ﬁrst layers it aggregates most context from its neighborhood and towards the\nlast layers it gets the context from its direct object (strategy) and from the token with the same\nmeaning in the second sentence.\n26\nPublished as a conference paper at ICLR 2020\n[CLS]\nhe\nsaid\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoesn\n'\nt\nfit\nthe\ncompany\n'\ns\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n\"\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoes\nnot\nfit\nour\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n1\n3\n5\n7\n9\n11 Layer\nlong\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nFigure 23: long: It is part of a composed adjective (long-term) and aggregates most of its context\nfrom the other part of the adjective (term) as well as from the same tokens in the second sentence.\nInterestingly, it mostly ignores the hyphen.\n[CLS]\nhe\nsaid\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoesn\n'\nt\nfit\nthe\ncompany\n'\ns\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n\"\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoes\nnot\nfit\nour\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n1\n3\n5\n7\n9\n11 Layer\nstrategy\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nFigure 24: strategy: Aggregates context from the word growth, which is the ﬁrst one of the noun\nphrase ”growth strategy”.\n[CLS]\nhe\nsaid\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoesn\n'\nt\nfit\nthe\ncompany\n'\ns\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n\"\nthe\nfoods\n##er\n##vic\n##e\npie\nbusiness\ndoes\nnot\nfit\nour\nlong\n-\nterm\ngrowth\nstrategy\n.\n[SEP]\n1\n3\n5\n7\n9\n11 Layer\n[SEP]\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nFigure 25: [SEP]: This token that has no semantic meaning aggregates context mostly from [CLS]\nand its own neighborhood.\n27\nPublished as a conference paper at ICLR 2020\nC.5 T OKEN CONTRIBUTIONS BY POS TAG\nHere we show the contribution of input tokens to hidden representations in all layers split by part-of-\nspeech (POS) tag (Toutanova et al., 2003). The POS tags are ordered according to the contribution\nin layer 12.\nNNP\nNNS\nNN\nJJ\nVBN\nCD\nVBG\nJJR\nRBR\nFW\nJJS\nRB\nVB\nVBD\nRBS\nVBP\nRP\nLS\nMD\nPDT\nWRB\nVBZ\nPRP\nCLS\nWP\nIN\nCC\nWDT\nPRP$\nEX\nSYM\nDT\nTO\nPOS\nSEP\n0.25\n0.30\n0.35\n0.40\n0.45Contribution\nFigure 26: Layer 1: Most token types are equally mixed and have already less than 35% median\ncontribution from their corresponding input. The only exception are the [CLS] tokens, which remain\nwith over 40% median original contribution.\nNNP\nNNS\nNN\nJJ\nVBN\nCD\nVBG\nJJR\nRBR\nFW\nJJS\nRB\nVB\nVBD\nRBS\nVBP\nRP\nLS\nMD\nPDT\nWRB\nVBZ\nPRP\nCLS\nWP\nIN\nCC\nWDT\nPRP$\nEX\nSYM\nDT\nTO\nPOS\nSEP\n0.150\n0.175\n0.200\n0.225\n0.250\n0.275\n0.300\n0.325\n0.350Contribution\nFigure 27: Layer 2: Similar to the previous layer with less contribution over all and [SEP] behaving\nsimilarly to [CLS].\nNNP\nNNS\nNN\nJJ\nVBN\nCD\nVBG\nJJR\nRBR\nFW\nJJS\nRB\nVB\nVBD\nRBS\nVBP\nRP\nLS\nMD\nPDT\nWRB\nVBZ\nPRP\nCLS\nWP\nIN\nCC\nWDT\nPRP$\nEX\nSYM\nDT\nTO\nPOS\nSEP\n0.150\n0.175\n0.200\n0.225\n0.250\n0.275\n0.300\n0.325Contribution\nFigure 28: Layer 3: Similar to layer 2 with decreasing contribution overall.\n28\nPublished as a conference paper at ICLR 2020\nNNP\nNNS\nNN\nJJ\nVBN\nCD\nVBG\nJJR\nRBR\nFW\nJJS\nRB\nVB\nVBD\nRBS\nVBP\nRP\nLS\nMD\nPDT\nWRB\nVBZ\nPRP\nCLS\nWP\nIN\nCC\nWDT\nPRP$\nEX\nSYM\nDT\nTO\nPOS\nSEP\n0.100\n0.125\n0.150\n0.175\n0.200\n0.225\n0.250\n0.275\n0.300Contribution\nFigure 29: Layer 4: The original input contribution to [CLS] and [SEP] falls signiﬁcantly. The trend\nthat the word types will follow until the last layer is already clear: Most nouns (NNP, NNS, NN),\nverbs (VBN, VB, VBD, VBP), adjectives (JJ, JJS) and adverbs (RBR, RBS) keep more contribution\nfrom their corresponding input embeddings than words with “less” semantic meaning like Wh-\npronouns and determiners (WP, WDT), prepositons (IN), coordinating conjunctions (CC), symbols\n(SYM), possessives (PRP$, POS) or determiners (DT).\nNNP\nNNS\nNN\nJJ\nVBN\nCD\nVBG\nJJR\nRBR\nFW\nJJS\nRB\nVB\nVBD\nRBS\nVBP\nRP\nLS\nMD\nPDT\nWRB\nVBZ\nPRP\nCLS\nWP\nIN\nCC\nWDT\nPRP$\nEX\nSYM\nDT\nTO\nPOS\nSEP\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30Contribution\nFigure 30: Layer 5: The trend started in the previous layer continues, with a reduction of internal\nvariability within those word types with less original contribution.\nNNP\nNNS\nNN\nJJ\nVBN\nCD\nVBG\nJJR\nRBR\nFW\nJJS\nRB\nVB\nVBD\nRBS\nVBP\nRP\nLS\nMD\nPDT\nWRB\nVBZ\nPRP\nCLS\nWP\nIN\nCC\nWDT\nPRP$\nEX\nSYM\nDT\nTO\nPOS\nSEP\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30Contribution\nFigure 31: Layer 6: Similar behavior as in the previous layer with minor evolution.\n29\nPublished as a conference paper at ICLR 2020\nNNP\nNNS\nNN\nJJ\nVBN\nCD\nVBG\nJJR\nRBR\nFW\nJJS\nRB\nVB\nVBD\nRBS\nVBP\nRP\nLS\nMD\nPDT\nWRB\nVBZ\nPRP\nCLS\nWP\nIN\nCC\nWDT\nPRP$\nEX\nSYM\nDT\nTO\nPOS\nSEP\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30Contribution\nFigure 32: Layer 7: Minor changes with respect to Layer 6.\nNNP\nNNS\nNN\nJJ\nVBN\nCD\nVBG\nJJR\nRBR\nFW\nJJS\nRB\nVB\nVBD\nRBS\nVBP\nRP\nLS\nMD\nPDT\nWRB\nVBZ\nPRP\nCLS\nWP\nIN\nCC\nWDT\nPRP$\nEX\nSYM\nDT\nTO\nPOS\nSEP\n0.05\n0.10\n0.15\n0.20\n0.25Contribution\nFigure 33: Layer 8: At this point there is clearly a different behavior between the tokens with most\ncontribution which present more intra-class variability, and those with less contribution, which are\nmore uniform.\nNNP\nNNS\nNN\nJJ\nVBN\nCD\nVBG\nJJR\nRBR\nFW\nJJS\nRB\nVB\nVBD\nRBS\nVBP\nRP\nLS\nMD\nPDT\nWRB\nVBZ\nPRP\nCLS\nWP\nIN\nCC\nWDT\nPRP$\nEX\nSYM\nDT\nTO\nPOS\nSEP\n0.05\n0.10\n0.15\n0.20\n0.25Contribution\nFigure 34: Layer 9: SEP changes increasing the contribution, while the rest stays similar.\n30\nPublished as a conference paper at ICLR 2020\nNNP\nNNS\nNN\nJJ\nVBN\nCD\nVBG\nJJR\nRBR\nFW\nJJS\nRB\nVB\nVBD\nRBS\nVBP\nRP\nLS\nMD\nPDT\nWRB\nVBZ\nPRP\nCLS\nWP\nIN\nCC\nWDT\nPRP$\nEX\nSYM\nDT\nTO\nPOS\nSEP\n0.05\n0.10\n0.15\n0.20\n0.25Contribution\nFigure 35: Layer 10: The contribution evolves with the same pattern as in previous layers.\nNNP\nNNS\nNN\nJJ\nVBN\nCD\nVBG\nJJR\nRBR\nFW\nJJS\nRB\nVB\nVBD\nRBS\nVBP\nRP\nLS\nMD\nPDT\nWRB\nVBZ\nPRP\nCLS\nWP\nIN\nCC\nWDT\nPRP$\nEX\nSYM\nDT\nTO\nPOS\nSEP\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25Contribution\nFigure 36: Layer 11:The contribution evolves with the same pattern as in previous layers.\nNNP\nNNS\nNN\nJJ\nVBN\nCD\nVBG\nJJR\nRBR\nFW\nJJS\nRB\nVB\nVBD\nRBS\nVBP\nRP\nLS\nMD\nPDT\nWRB\nVBZ\nPRP\nCLS\nWP\nIN\nCC\nWDT\nPRP$\nEX\nSYM\nDT\nTO\nPOS\nSEP\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25Contribution\nFigure 37: Layer 12: Finally, nouns, verbs, adjectives, adverbs, receive more contribution from their\ncorresponding input than determiners, prepositions, pronouns, ”to” words and symbols.\n31\nPublished as a conference paper at ICLR 2020\nD G ENERALIZATION TO OTHER DATASETS\nIn this appendix we reproduce several experiments from the main text using the development sets\nof two additional datasets from the GLUE benchmark: The Corpus of Linguistic Acceptability\n(CoLA) (Warstadt et al., 2018), and the matched Multi-Genre Natural Language Inference corpus\n(MNLI-matched) (Williams et al., 2018). CoLA is a dataset about grammatical acceptability of\nsentences and MNLI consists of pairs of sentences where the second sentence entails, contradicts\nor is neutral about the ﬁrst one. These datasets differ signiﬁcantly from MRPC. The development\nset of CoLa has 1043 examples with sequence length ds between 5 and 35 tokens, and 11 tokens\non average. The development set of MNLI-m consits of 9815 examples although we restrict the\nexperiments to the ﬁrst 4000 examples without loss of generality. These contain a total of 155964\ntokens, with a sequence length comprised between 6 and 128 tokens and an average of 39 tokens\nper example.\nThe results presented in this appendix are qualitatively similar to those presented in the main text,\nwhich shows that our empirical conclusions about BERT are general across data domains.\nD.1 T OKEN IDENTIFIABILITY\nHere we reproduce the main token identiﬁability results of Section 4 on two additional datsets:\nCoLA and MNLI. Qualitatively, the results are in line with those for MRPC. Note that a random\nclassiﬁer would achieve an accuracy of 1/¯ds, where ¯ds denotes the average sentence length. Thus,\nthe random guessing baselines for MRPC, CoLA and MNLI are 1.7%, 9% and 2.6% respectively.\nD.1.1 C OLA\nFigure 38 shows the token identiﬁability results for CoLA.\n1 2 3 4 5 6 7 8 9 10 11 12\n0.2\n0.4\n0.6\n0.8\n1\nLayer\nIdentiﬁability Rate\nˆgMLP\ncos,l ˆglin\ncos,l\nˆgnaive\ncos,l ˆgMLP\nL2,l\nˆglin\nL2,l ˆgnaive\nL2\nFigure 38: Identiﬁability of contextual word embeddings at different layers on CoLA.\n32\nPublished as a conference paper at ICLR 2020\nD.1.2 MNLI\nFigure 39 shows the token identiﬁability results for the ﬁrst 500 sentences (19,839 tokens) of MNLI-\nmatched.\n1 2 3 4 5 6 7 8 9 10 11 12\n0.2\n0.4\n0.6\n0.8\n1\nLayer\nIdentiﬁability Rate\nˆgMLP\ncos,l ˆglin\ncos,l\nˆgnaive\ncos,l ˆgMLP\nL2,l\nˆglin\nL2,l ˆgnaive\nL2\nFigure 39: Identiﬁability of contextual word embeddings at different layers on a the ﬁrst 500 sen-\ntences of MNLI-matched (19,839 tokens).\n33\nPublished as a conference paper at ICLR 2020\nD.2 A TTRIBUTION ANALYSIS\nD.2.1 C OLA E XPERIMENTS\nFigure D.2.1 shows the token mixing analysis for the CoLA dataset. The behavior is very similar to\nMRPC with the only difference that both, the contribution of the original token and the percentage\nof tokens that are not maximum contributors to their embeddings are slightly larger across layers.\nHowever, this increase is explained by CoLA consisting of much shorter sequences on average than\nMRPC.\n1 2 3 4 5 6 7 8 9 10 11 12\n0\n10\n20\n30\n40\nLayer\nContribution [%]\n(a)\n1 2 3 4 5 6 7 8 9 10 11 12\n0\n10\n20\n30\n40\n50\nLayer\n˜P [%]\n(b)\nFigure 40: (a) Contribution of the input token to the embedding at the same position. (b) Percentage\nof tokens ˜P that are not the main contributors to their corresponding contextual embedding at each\nlayer.\nFigure D.2.1 presents the context aggregation for the CoLA development set. We observe the same\ngeneral trend as for MRPC, with the context being aggregated mostly locally and long range depen-\ndencies increasing in the later layers. The fact that examples in CoLA have an average sequence\nlength of 11 tokens explains the smaller relative contribution of tokens beyond the 10th neighbour.\n1 2 3 4 5 6 7 8 9 10 11 12\n6\n8\n10\nLayer\nRel. Contribution (%)\n1st 2nd3rd 4th and 5th6th to 10th 11th onwards\n(a)\n-40 -20 0 20 40\n0\n0.5\n1\nNeighbour\nTotal Contribution\n12\n6\n1\n(b)\nFigure 41: (a) Relative contribution per layer of neighbours at different positions. (b) Total contri-\nbution per neighbour for the ﬁrst, middle and last layers.\n34\nPublished as a conference paper at ICLR 2020\nD.2.2 MNLI E XPERIMENTS\nAs shown by Figures D.2.2 and D.2.2, the results with the MNLI matched dataset are very similar\nto the ones presented in the main text. No meaningful discrepancy exists in this case.\n1 2 3 4 5 6 7 8 9 10 11 12\n0\n10\n20\n30\n40\nLayer\nContribution [%]\n(a)\n1 2 3 4 5 6 7 8 9 10 11 12\n0\n10\n20\n30\n40\nLayer\n˜P [%]\n(b)\nFigure 42: (a) Contribution of the input token to the embedding at the same position. (b) Percentage\nof tokens ˜P that are not the main contributors to their corresponding contextual embedding at each\nlayer.\n1 2 3 4 5 6 7 8 9 10 11 12\n6\n8\n10\nLayer\nRel. Contribution (%)\n1st 2nd3rd 4th and 5th6th to 10th 11th onwards\n(a)\n-60 -40 -20 0 20 40 60\n0\n0.5\n1\nNeighbour\nTotal Contribution\n12\n6\n1\n(b)\nFigure 43: (a) Relative contribution per layer of neighbours at different positions. (b) Total contri-\nbution per neighbour for the ﬁrst, middle and last layers.\n35"
}