{
  "title": "Modulated Fusion using Transformer for Linguistic-Acoustic Emotion Recognition",
  "url": "https://openalex.org/W3095308484",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4320554146",
      "name": "Jean-benoit Delbrouck",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2804510324",
      "name": "Noé Tits",
      "affiliations": [
        "University of Mons"
      ]
    },
    {
      "id": "https://openalex.org/A2172109737",
      "name": "Stéphane Dupont",
      "affiliations": [
        "University of Mons"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2191779130",
    "https://openalex.org/W2740550900",
    "https://openalex.org/W2962770129",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3007282427",
    "https://openalex.org/W2787581402",
    "https://openalex.org/W2964300796",
    "https://openalex.org/W2465534249",
    "https://openalex.org/W2962931510",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963686995",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3045969489",
    "https://openalex.org/W3015558147",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963032608",
    "https://openalex.org/W2964216663",
    "https://openalex.org/W2879390606",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2966683369",
    "https://openalex.org/W2965453734",
    "https://openalex.org/W2883409523",
    "https://openalex.org/W2939224318"
  ],
  "abstract": "This paper aims to bring a new lightweight yet powerful solution for the task of Emotion Recognition and Sentiment Analysis. Our motivation is to propose two architectures based on Transformers and modulation that combine the linguistic and acoustic inputs from a wide range of datasets to challenge, and sometimes surpass, the state-of-the-art in the field. To demonstrate the efficiency of our models, we carefully evaluate their performances on the IEMOCAP, MOSI, MOSEI and MELD dataset. The experiments can be directly replicated and the code is fully open for future researches.",
  "full_text": "Proceedings of the First International Workshop on Natural Language Processing Beyond Text, pages 1–10\nOnline, November 20, 2020.c⃝2020 Association for Computational Linguistics\nhttp://www.aclweb.org/anthology/W23-20%2d\n1\nModulated Fusion using Transformer for Linguistic-Acoustic Emotion\nRecognition\nJean-Benoit Delbrouck\nStanford University\njeanbenoit.delbrouck@stanford.edu\nNo´e Tits and St´ephane Dupont\nInformation, Signal and Artiﬁcial Intelligence\nUniversity of Mons, Belgium\n{noe.tits, stephane.dupont}@umons.ac.be\nAbstract\nThis paper aims to bring a new lightweight\nyet powerful solution for the task of Emotion\nRecognition and Sentiment Analysis. Our mo-\ntivation is to propose two architectures based\non Transformers and modulation that com-\nbine the linguistic and acoustic inputs from a\nwide range of datasets to challenge, and some-\ntimes surpass, the state-of-the-art in the ﬁeld.\nTo demonstrate the efﬁciency of our models,\nwe carefully evaluate their performances on\nthe IEMOCAP, MOSI, MOSEI and MELD\ndataset. The experiments can be directly repli-\ncated and the code is fully open for future re-\nsearches1.\n1 Introduction\nUnderstanding expressed sentiment and emotions\nare two crucial factors in human multimodal lan-\nguage yet predicting affective states from multi-\nmedia remains a challenging task. The emotion\nrecognition task has existed working on different\ntypes of signals, typically audio, video and text.\nDeep Learning techniques allow the development\nof novel paradigms to use these different signals in\none model to leverage joint information extraction\nfrom different sources. These models usually re-\nquire a fusion between modality, a crucial step to\ncompute expressive multimodal features used by a\nclassiﬁer to output probabilities over the possible\nanswers.\nIn this paper, we propose an architecture based\non two stages: an independent sequential stage\nbased on LSTM (Hochreiter and Schmidhuber,\n1997) where modality features are computed sep-\narately, and a second hierarchical stage based on\nTransformer (Vaswani et al., 2017) where we itera-\ntively compute and fuse new multimodal represen-\ntations. This paper proposes the fusion between the\n1https://github.com/jbdel/modulated_\nfusion_transformer\nacoustic and linguistic features through attention\nmodulation (Yu et al., 2019) and linear modulation\n(Dumoulin et al., 2018), a powerful tool to shift\nand scale the feature maps of one modality given\nthe representation of another.\nThe association of this horizontal-vertical en-\ncoding and modulated fusion shows really strong\nresults across a wide range of datasets for emotion\nrecognition and sentiment analysis. In addition to\nthe interesting performances it offers, the modula-\ntion requires no or very few learning parameters,\nmaking it fast and easy to train. The paper is struc-\ntured as follows: we ﬁrst present the different re-\nsearches used for comparison in our experiments\nin section 2, we then brieﬂy present the different\ndatasets in section 3. Then we carefully describe\nour sequential feature extraction based on LSTM\nin section 4 and the two hierarchical modulated fu-\nsion model, the Modulated Attention Transformer\n(MAT) and Modulated Normalization Transformer\n(MNT), in section 5. Finally, we explain the exper-\nimental settings in section 6 and report the results\nof our model variants in section 7.\n2 Related Work\nThe presented related work is used for comparison\nfor our experiments. We proceed to brieﬂy describe\ntheir proposed models.\nFirst, Zadeh et al. (2018b) proposed a novel mul-\ntimodal fusion technique called the Dynamic Fu-\nsion Graph (DFG) to study the nature of cross-\nmodal dynamics in multimodal language. DFG\ncontains built-in efﬁcacies that are directly related\nto how modalities interact.\nTo capture the context of the conversation\nthrough all modalities, the current speaker and\nlistener(s) in the conversation, and the relevance\nand relationship between the available modalities\nthrough an adequate fusion mechanism, Shenoy\n2\nand Sardana (2020) proposed a recurrent neural net-\nwork architecture that attempts to take into account\nall the mentioned drawbacks, and keeps track of\nthe context of the conversation, interlocutor states,\nand the emotions conveyed by the speakers in the\nconversation.\nPham et al. (2019) presented a model that learns\nrobust joint representations by cyclic translations\nbetween modalities (MCTN), that achieved strong\nresults on various word-aligned human multimodal\nlanguage tasks.\nWang et al. (2019) proposed the Recurrent At-\ntended Variation Embedding Network (RA VEN) to\nmodel expressive nonverbal representations by ana-\nlyzing the ﬁne-grained visual and acoustic patterns\nthat occur during word segments. In addition, they\nseek to capture the dynamic nature of nonverbal\nintents by shifting word representations based on\nthe accompanying nonverbal behaviors.\nBut the related work that is probably the closest\nto ours is the Multimodal Transformer (Tsai et al.,\n2019; Delbrouck et al., 2020) because they also use\nTransformer based solutions to encode their modal-\nities. Nonetheless, we differ in many ways. First,\ntheir best solutions and scores reported are using\nvisual support. Secondly, they use Transformer for\ncross-modality encoding for every modality pairs;\nthis equals to 6 Transformer modules (2 pairs per\nmodality) while we only use two Transformer (one\nper modality). Finally, each output pairs is concate-\nnated to go though a second stage of Transformer\nencoding. We also differ on how the features are\nextracted: they base their solution on CNN while\nwe use LSTM. In this paper, it is important to note\nthat we compare our results to their word-unaligned\nscores, as we do not use word-alignment either.\n3 Datasets\n3.1 IEMOCAP dataset\nIEMOCAP (Busso et al., 2008) is a multimodal\ndataset of dyadic conversations of actors. The\nmodalities recorded are Audio, Video and Motion\nCapture data. All conversations were segmented,\ntranscribed and annotated with two different emo-\ntional types of labels: emotion categories (6 ba-\nsic emotions (Ekman, 1999) – happiness, sadness,\nanger, surprise, fear, disgust – plus frustrated, ex-\ncited and neutral) and continuous emotional dimen-\nsions (valence, arousal and dominance).\nFor categorical labels, the annotators could also\nselect ”other” if they found the emotion could not\nbe described with one of the adjectives. The cate-\ngorical labels were given by 3-4 evaluators. Major-\nity vote was used to have the ﬁnal label. In case of\nex aequo, it was considered not consistent in terms\nof inter-evaluator agreement; 7532 segments out of\nthe 10039 segments reached agreement.\nTo be comparable to previous research, we use\nthe four categories: neutral, sad, happy, angry.\nHappy category is obtained by merging excited\nand happy labeled (Yoon et al., 2018), we obtain\na total of 5531 utterances: 1636 happy, 1084 sad,\n1103 angry, 1708 neutral. The train-test split is\nmade according to Poria et al. (2017) as it seems to\nbe the norm for recent works.\n3.2 CMU-MOSI dataset\nCMU-MOSI (Zadeh et al., 2016) dataset is a col-\nlection of video clips containing opinions. The\ncollected videos come from YouTube and were\nselected with metada using the #vlog hashtag for\nvideo-blog which desribes a speciﬁc type of video\nthat often contains people expressing their opinion.\nThe resulting dataset included clips with speakers\nwith different ethnicities but all speaking in english.\nThe speech was manually transcribed. These tran-\nscriptions were aligned with audio at word level.\nThe videos were annotated in sentiment with a 7-\npoint Likert scale (from -3 to 3) by ﬁve workers for\neach video using Amazon’s Mechanical Turk.\n3.3 CMU-MOSEI dataset\nMOSEI (Zadeh et al., 2018c) is the next generation\nof MOSI dataset. They also took advantage of\nonline videos containing expressed opinions. They\nanalyzed videos with a face detection algorithm\nand selected videos with only one speaker with an\nattention directed to the camera.\nThey used a set of 250 different keywords to\nscrape the videos and kept a maximum of 10 videos\nfor each one with manual transcription included.\nThe dataset was then manually curated to keep\nonly data with good quality. It is annotated with a\n7-point Likert scale as well as the six basic emotion\ncategories (Ekman, 1999).\n3.4 MELD dataset\nThe Multimodal EmotionLines Dataset (MELD)\n(Poria et al., 2019) contains dialogue instances that\nencompasses audio and visual modality along with\ntext. MELD has more than 1400 dialogues and\n13000 utterances from Friends TV series. Multiple\n3\nspeakers participated in the dialogues. Each utter-\nance in a dialogue has been labeled by any of these\nseven emotions: Anger, Disgust, Sadness, Joy, Neu-\ntral, Surprise and Fear. MELD also has sentiment\n(positive, negative and neutral) annotation for each\nutterance.\n4 Feature extractions\nThis sections aims to describe the linguistic and\nacoustic features used as the input of our proposed\nmodulated fusions based on Transformers. The ex-\ntraction is performed independently for each sam-\nple of a dataset. We denote the extracted linguistic\nfeatures as xand acoustic as y. In the end, both x\nand yhave a size [T,C] where T is the temporal\naxis size and C the feature size. Its important to\nnote that T is different for each sample, while Cis\na hyper-parameter.\n4.1 Linguistic\nA sentence is tokenized and lowercased. We re-\nmove special characters and punctuation. We build\nour vocabulary against the train-set of the datasets\nand embed each word in a vector of 300 dimensions\nusing GloVe (Pennington et al., 2014). If a word\nfrom the validation or test-set is not in present our\nvocabulary, we replace it with the unknown token\n”unk”. Each sentence is run through an unidirec-\ntional one-layered LSTM of size C. The size of\neach linguistic example xis therefore [T,C] where\nT is the number of words in the sentence.\n4.2 Acoustic features\nIn the litterature of multimodal emotion recogni-\ntion, many works use hand designed acoustic fea-\ntures sets that capture information about prosody\nand vocal quality such as ComPaRe (Computa-\ntional Paralinguitic Challenge) feature sets from\nInterspeech conference.\nHowever, with the evolution of deep learn-\ning models, lower level features such as mel-\nspectrograms have shown to be very powerful for\nspeech related tasks such as speech recognition\nand speech synthesis. In this work we extract mel-\nspetrograms with the same procedure as a typical\nseq2seq Text-to-Speech system.\nSpeciﬁcally, our mel-spectrograms were ex-\ntracted with the same procedure as in (Tachibana\net al., 2018) with librosa python library (McFee\net al., 2015) with 80 ﬁlter banks (the embedding\nsize is therefore 80). A temporal reduction is then\napplied by selecting one frame every 16 frames.\nEach spectrogram is then run through an unidirec-\ntional one-layered LSTM of size C. The size of\neach acoustic example yis therefore [T,C] where\nT is the number of frames in the spectrogram.\n5 Models\nThis section aims to describe the three model vari-\nants evaluated in our experiments. First, we de-\nscribe the projection (P) of the features extracted in\nsection 4 over emotion and sentiment classes with-\nout using any Transformer. This corresponds to the\nbaseline for our experiments. Secondly, we present\nthe Naive Transformer (NT) model, a transformer-\nbased encoding where the inputs are encoded sep-\narately, the linguistic and acoustic features do not\ninteract with each other: there is no modulated\nfusion. Finally, we present the two highlights of\nthe paper, the Modulated Attention Transformer\n(MAT) and the Modulated Normalization Trans-\nformer (MNT), two solutions where the encoded\nlinguistic representation modulates the entire pro-\ncess of the acoustic encoding.\n5.1 Projection\nGiven the linguistic featuresxand acoustic features\nyextracted at section 4, we deﬁne the projection\nas a two-step process. First, we use an attention-\nreduce mechanism over each modality, and then\nfuse both modality vectors using a simple element-\nwise sum.\nAtt. Reduce\nx\nAtt. Reduce\ny\n+\nAdd & Norm\np\nFigure 1: Projection\nThe attention-reduce mechanism consists of a\nsoft-attention over itself followed by a weighted-\nsum computed according to the attention weights.\nIf we consider the feature input xof size [T,C]:\nai = softmax(va\ni\n⊤(Wxx))\n¯x=\nT∑\ni=0\naixi\n(1)\nAfter this reduce mechanism, the input becomes\nvectors of size [1,C]. We can then apply the\n4\nelement-wise sum as follows:\ny∼p= Wp(LayerNorm(¯x+ ¯y)) (2)\nwhere p is the distribution of probabilities over\npossible answers and LayerNorm denotes Layer\nNormalization (Ba et al., 2016). If we assume\nthe input feature xhas the shape [T,C], for each\nfeature channel c∈{1,2,··· ,C}\nµi,c = 1\nT\nT∑\nt=1\nxi,t,c\nσ2\ni,c = 1\nT\nT∑\nt=1\n(xi,t,c −µi,c)2\nˆxi,t,c = xi,t,c −µi,c√\nσ2\ni,c\n(3)\nFinally, for each channel, we have learnable param-\neters γc and βc, such that:\nyi,:,c = γcˆxi,:,c + βc (4)\n5.2 Naive Transformer\nThe Naive Transformer model consists of stacking\na Transformer on top of the linguistic and acoustic\nfeatures extracted at section 4 before the projection\nof section 5.1. Transformers are independent and\ntheir respective input features do not interact with\neach other.\nA Transformer is composed of a stack ofBiden-\ntical blocks but with their own set of training pa-\nrameters. Each block has two sub-layers. There is\na residual connection around each of the two sub-\nlayers, followed by layer normalization (Ba et al.,\n2016). The output of each sub-layer can be written\nlike this:\nLayerNorm(x+ Sublayer(x)) (5)\nwhere Sublayer(x) is the function implemented by\nthe sub-layer itself. In traditional Transformers,\nthe two sub-layers are respectively a multi-head\nself-attention mechanism and a simple Multi-Layer\nPerceptron (MLP).\nThe attention mechanism consists of a Key K\nand Query Q that interacts together to output a\nattention map applied to Value V:\nAttention(Q,K,V ) = softmax\n(QK⊤\n√\nC\n)\nV (6)\nIn the case of self-attention, K, Qand V are the\nsame input. If this input is of size T ×C, the op-\neration QK⊤ results in a squared attention matrix\ncontaining the afﬁnity between each row T. Ex-\npression\n√\nC is a scaling factor. The multi-head\nattention (MHA) is the idea of stacking several self-\nattention attending the information from different\nrepresentation sub-spaces at different positions:\nMHA(Q,K,V ) = Concat(head1,..., headh)Wo\nwhere headi = Attention(QWQ\ni ,KW K\ni ,VW V\ni )\n(7)\nA subspace is deﬁned as slice of the feature di-\nmension k. In the case of four heads, a slice would\nbe of size k\n4 . The idea is to produce different sets of\nattention weights for different feature sub-spaces.\nIn the context of Transformers, Q, Kand V are x\nfor the linguistic Transformer and yfor the acous-\ntic Transformer. Throughout the MHA, the feature\nsize of xand yremains unchanged, namely C.\nThe MLP consists of two layers of respective\nsizes [C →C] and [C →C]. After encoding\nthrough the blocks, the outputs ˜xand ˜ycan be used\nby the projection layer (section 5.1) for classiﬁca-\ntion. In Figure 2, we show the encoding of the\nlinguistic features xand its corresponding output\n˜x.\nx\n×B\nMulti-Head A.\nAdd & Norm\nMLP\nAdd & Norm\n˜x\nFigure 2: Linguistic Naive Transformer.\n5.3 Modulated Fusion\nThe Modulated Fusion consists of modulating the\nencoding of the acoustic features y given the en-\ncoded linguistic features ˜x. This modulation in the\nacoustic Transformer allows for an early fusion of\nboth modality whose result is going to be ˜y. This\n5\nmodulation can be performed through the Multi-\nHead Attention or the Layer-Normalization. After,\nthe output ˜xand ˜yare used as input of the projec-\ntion from section 5.1. We proceed to describe both\napproaches in the next sub-sections.\n5.3.1 Modulated Attention Transformer\nTo modulate the acoustic self-attention by the lin-\nguistic output, we switch the key K and value V\nof the self-attention from y to ˜x. The operation\nQK⊤ results in an attention map that acts like an\nafﬁnity matrix between the rows of modality ma-\ntrix ˜xand y. This computed alignment is applied\nover the Value V (now ˜x) and ﬁnally we add the\nresidual connection y. The following equation de-\nscribes the new attention sub-layer in the acoustic\nTransformer.\ny= LayerNorm(y+ MHA(y,x,x )) (8)\nFor the operation QK⊤ to work as well as the\nresidual connection (the addition), the feature sizes\nCof ˜xand ymust be equal. This can be adjusted\nwith the different transformation matrices of the\nMHA module or the LSTM size of section 4.\nx\nMulti-Head A.\nAdd & Norm\nMLP\nAdd & Norm\n˜x\ny\n×B\nMulti-Head A.\nAdd & Norm\nMLP\nAdd & Norm\n˜y\nFigure 3: Modulated Attention Transformer.\nIf we consider that ˜x is of size [Tx,C] and y\nof size [Ty,C], then the sizes of the matrix mul-\ntiplication operations of this modulated attention\ncan be written as follows (where ×denotes matrix\nmultiplication):\ny×xT = Ty,C ×C,Tx = Ty,Tx (9)\n(9) ×x= Ty,Tx ×Tx,C = Ty,C (10)\n(10) + y= Ty,C + Ty,C = Ty,C (11)\nwhere equation 11 denotes the\n(y+ MHA(y,x,x )) operation.\nWe call the Modulated Attention Transformer\n”MAT” in the experiments.\n5.3.2 Modulated Normalization Transformer\nIt is possible to modulate the normalization layers\nby predicting two scalars per block from ˜x, namely\n∆γ and ∆β, that will be added to the learnable\nparameters of equation 4:\nγc = γc + ∆γ\nβc = βc + ∆β\n(12)\nwhere ∆γ, ∆β = MLP(˜x) and the MLP has\none layer of sizes [C,4 ×B]. Two pairs of scalars\nper block are predicted, so no scalars are shared\namongst normalization layers.\nWe update the layer normalization equation ac-\ncordingly:\nyi,:,c = γcˆxi,:,c + βc (13)\nThe Modulated Normalization is a computation-\nally efﬁcient and powerful method to modulate neu-\nral activations. It enables the linguistic output to\nmanipulate entire acoutisc feature maps by scaling\nthem up or down, negating them, or shutting them\noff. As there is only two parameters per feature\nmap, the total number of new training parameters\nis small. This makes the Modulated Normalization\na very scalable method.\nWe call the Modulated Normalization Trans-\nformer ”MNT” in the experiments.\n6 Experimental settings\nWe train our models using the Adam optimizer\n(Kingma and Ba, 2014) with a learning rate of\n1e−4 and a mini-batch size of 32. If the accuracy\nscore on the validation set does not increase for\na given epoch, we apply a learning-rate decay of\nfactor 0.5. We decay our learning rate up to 2 times.\nAfterwards, we use an early-stop of 10 epochs on\naccuracy. Results presented in this paper are from\nthe averaged predictions of at most 10 models.\nUnless stated otherwise, the LSTM size C(and\ntherefore the Transformer size) is 512. We use\nB = 2 Transformer blocks for P and NT models\nand B = 4 for MNT and MAT models. We use 8\nmulti-heads regardless of the models or the modal-\nity encoded. The size Cof the Transformer MLP is\nset at 2048. We apply dropout of 0.1 on the output\nof each block iteration, and 0.5 on the input(x+ y)\nof the projection layer (equation 2).\n6\n7 Results\nWe present the results on four sentiment and emo-\ntion recognition datasets: IEMOCAP, MOSEI,\nMOSI and MELD. For each dataset, the results\nare presented in terms of the popular metrics used\nfor the dataset. Most of the time, F1-score is used,\nand sometimes the weighted F1-scores to take into\naccount the imbalance between emotion or senti-\nment classes.\nIEMOCAP We ﬁrst compare the precision,\nrecall and unweighted F1-scores of our two model\nvariants on IEMOCAP in Table 3. We notice that\nour MAT model comes on top.\nModel Prec. Recall F1\nMAT (L+A, ours) 0.74 0.74 0.74\nMNT (L+A, ours) 0.72 0.72 0.72\nNT (L+A, ours) 0.71 0.70 0.70\nP (L+A, ours) 0.69 0.67 0.67\nMult (L+A+V , 2019) - - 0.715\nE2 (L+A, 2019) 0.73 0.715 0.72\nMDRE (L+A, 2018) 0.72 - -\nMDREA (L+A, 2018) 0.69 - -\nE1 (L+A, 2019) 0.73 0.655 0.68\nRA VEN, (L+A+V , 2019) - - 0.665\nMCTN, (L, 2018) - - 0.66\nTable 1: Results of the 4-emotions task of IEMOCAP.\nPrec. stands for precision and F1 is the unweighted F1-\nscore.\nIf we compare the F1-score per class (table 2),\nwe notice that our model MAT outperforms pre-\nvious researches, the biggest margin being in the\nhappy category. The model MulT (Tsai et al., 2019)\nstill comes on top in the neutral category.\nModel Hap. Ang. Sad Neu. avg\nMAT (ours) 0.68 0.71 0.75 0.80 0.73\nMNT (ours) 0.66 0.71 0.72 0.80 0.72\nNT (ours) 0.67 0.69 0.69 0.78 0.70\nP (L+A, ours) 0.63 0.65 0.68 0.78 0.67\nMulT (2019) 0.60 0.70 0.74 0.82 0.71\nMCTN (2018) 0.49 0.66 0.72 0.78 0.66\nRA VEN (2019) 0.60 0.64 0.66 0.77 0.66\nTable 2: IEMOCAP: F1-scores per emotion class. Avg\ndenotes the weighted average F1-score.\nWe can see in Figure 4 that our MNT model\nhas a really good recall on the neutral category but\nMAT signiﬁcantly outperforms MNT in the happy\ncateogry. However, we can see that the happy class\nsurprisingly remains a challenge for the models\npresented. Our MAT model predicted around 17%\nof the time ”angry” when the true class was happy.\nOn the contrary, our model predicted ”happy” 19%\nof the time when the true label was ”sad” and 17%\nof the time when the true class was ”angry”. We\ncan see that this is still a signiﬁcant margin of error\nfor such contradictory labels. It shows that visual\ncues might be necessary to further improve the\nperformances.\nFigure 4: Confusion matrices for IEMOCAP emotion\ntask.\nMOSI MOSI is a small dataset with few train-\ning examples. To train such models, regularization\nis usually needed to not overﬁt the training-set. In\nour case, dropout was enough to top the state-of-\nthe-art results on this dataset.\nEven if the dataset is a bit unbalanced between\nthe binary answers (positive and negative), weight-\ning the loss accordingly did not improve the results.\nIt shows that our model variants manage to efﬁ-\nciently discriminate between both classes.\nModel F1\nMAT (L+A, ours) 0.80 (0.84 / 0.73)\nMNT (L+A, ours) 0.80 (0.84 / 0.73)\nNT (L+A, ours) 0.78 (0.83 / 0.71)\nP (L+A, ours) 0.76 (0.80 / 0.71)\nMulT (L+A+V , 2019) 0.81\nSA-Gating B6 (L+A+V , 2020) 0.81\nMultilogue-Net (L+A+V , 2020) 0.80\nMultilogue-Net (L+A, 2020) 0.79\nTable 3: Results on the 2-sentiment task of MOSI. Re-\nsults given are the weighted F1-scores.\nMOSEI MOSEI is a relatively large-scale\ndataset. We expect to see a more noticeable differ-\nence of score between our Modulated Transformer\nvariants and the Naive Transformer and Projection\nbaselines.\nFor the emotion task in Table 4, MNT comes\non top with a noticeable improvement over the\nstate-of-the-art in the Surprise and Fear category.\n7\nModel Happy Sad Angry\nMNT (ours) 0.66 0.76 0.77\nMAT (ours) 0.66 0.75 0.75\nNT (ours) 0.65 0.75 0.74\nM-logue (2020) 0.68 0.75 0.81\nG-MFN (2018b) 0.66 0.67 0.73\nModel Fear Disgust Surprise\nMNT (ours) 0.92 0.85 0.91\nMAT 0.91 0.84 0.89\nP (ours) 0.88 0.84 0.86\nMultilogue 0.87 0.87 0.81\nG-MFN 0.79 0.77 0.85\nTable 4: Results on the 6-emotions classiﬁcation task of\nMOSEI. Metrics reported are the weighted F1-scores.\nM-logue stands for Multilogue-Net and G-MFN for\nGraph-MFN.\nMultilogue still shows strong results in the Happy\nand Angry category, two important classes of the\nMOSEI dataset as they have the biggest support\n(respectively 2505 and 1071 samples over 6336 in\nthe test-set). For binary sentiment classiﬁcation\n(Table 5), MAT is the strongest reported model.\nModel A2 F1\nMAT (L+A, ours) 0.82 0.82\nMNT (L+A, ours) 0.805 0.805\nNT (L+A, ours) 0.81 0.80\nP (L+A, ours) 0.805 0.79\nMulT (L+A+V , 2019) 0.815 0.815\nRA VEN (L+A+V , 2019) 0.79 0.795\nG-MFN (L+A+V , 2018b) 0.79 -\nMCTN (L, 2019) 0.75 0.76\nTable 5: Results on the 2-sentiments task of MO-\nSEI. Results given are the accuracies and weighted F1-\nscores.\nMELD MELD is a dataset for Emotion\nRecognition in Conversation. Even if our ap-\nproaches do not take into account the context, we\ncan see that it leads to interesting results. More\nprecisely, our variants are able to detect difﬁcult\nemotion, such as fear and disgust, even though they\nare present in very low quantity in the training and\ntest-set.\nWe can see in Table 6 that even if we do not\nuse the contextual nor the speaker information, our\nmodels achieve good results in two categories: fear\nand disgust. To help understand these results, we\ngive two MELD examples in Figure 5. In the top\nexample, it is unlikely to answer ”anger” to the\nsentence ”you fell asleep!” without context, it could\nbe surprise or fear. This is why our ”anger” score\nis really low. In the bottom example, ”you have no\nidea how loud they are” could very well be ”anger”\ntoo, but happens to be labeled ”disgust”.\nModel Ang. Dis. Fear Joy\nMNT (ours) 0.27 0.21 0.12 0.41\nMAT (ours) 0.27 0.15 0.09 0.42\nNT (ours) 0.25 0.11 0.05 0.39\nCGCN*†(2019) 0.47 0.11 0.09 0.53\nDRNN* (2019) 0.46 0.0 0.0 0.53\nBC-LSTM* 0.46 0.0 0.0 0.50\nG-MFN (2018a) 0.40 0.0 0.0 0.47\nModel Neut. Sad Surp.\nMNT (ours) 0.66 0.24 0.46\nMAT (ours) 0.63 0.22 0.44\nNT (ours) 0.54 0.21 0.41\nGCN*† 0.77 0.28 0.50\nDRNN* 0.73 0.25 0.52\nBC-LSTM* 0.76 0.16 0.48\nG-MFN 0.76 0.13 0.41\nTable 6: Results of the 7-emotions (Anger, Disgust,\nFear, Joy, Neutral, Sad, Surprise) task of MELD. Re-\nsults given in term of F1-scores. DRNN is Dia-\nlogueRNN, G-MFN is Graph-MFN and CGCN is Con-\nGCN. * denotes that a model uses the contextual infor-\nmation and †speaker information.\nIt is possible that our model, without any prior\nor contextual bias about an utterance, classify sen-\ntences similar to ”you fell asleep” or ”you have no\nidea how” as ”disgust” or ”fear”. Further analysis\non why our model perform so well could shed the\nlight on this odd behavior. We also fall short on the\nsad and surprise category compared to GCN, show-\ning that a variant of our proposed models that takes\ninto account the context could lead to competitive\nresults.\n8 Further analysis\nA few supplementary comments can be made about\nthe results. First, we notice that the hierarchical\nstructure of the network brought by the transform-\ners did bring improvements across all datasets. In-\ndeed, even the NT model does bring signiﬁcant\nperformances boost compared to the P model that\nonly consists of an LSTM and the projection layer.\nA very nice property of our solutions is that few\nTranformers layers are required to be the found set-\ntings. It usually varies from 2 to 4 layers, allowing\nour solutions to converge very rapidly.\n8\nFigure 5: MELD: Two contextual examples with three\ntraining samples each.\nMOSEI Params s/epoch epoch/c\nP 9.8 M 10 2\nNT B = 2 22.9 M 26 6\nNT B = 4 35.5 M 42 7\nMAT B = 2 22.9 M 26 8\nMAT B = 4 35.5 M 42 10\nMNT B = 2 24.5 M 26 6\nMNT B = 4 39.9 M 44 8\nTable 7: Results on a single GTX 1080 Ti forC = 512.\nThe statistics reported are from the MOSEI dataset for\nthe sentiment task, as it contains the most training sam-\nples (16320). s/epoch means seconds per epoch and\nepoch/c means the number of epoch to convergence.\nParameters are reported in Million.\nAnother point is that the MAT variant does not\nrequire additional training parameters nor compu-\ntational power (as shown in Table 7), the solution\nonly switch one input of the Multi-Head Attention\nfrom one modality matrix to another. For MNT,\nthe Transformer block implements only 2 normal-\nization layers, therefore the conditional layer must\nonly compute 2048 scalars (givenCis 512) for ∆γ\nand ∆βor roughly 1 Million parameters per block.\nThis solution grows linearly with the hidden size\nbut we got better results with C = 512 rather than\n1024.\nThe difference between MAT and MNT variant\nis slim, but it seems that MAT is more suitable\nfor the binary sentiment classiﬁcation. The com-\nputed alignment by the modulated attention of the\nFigure 6: Heatmap showing the inﬂuence on f1-scores\nfrom parameters Band Con IEMOCAP.\nlinguistic and acoustic modality proves to be an\nacceptable solution for 2-class problem, but seems\nto fall short for more nuanced classiﬁcation such\nas multi-class emotion recognition. MNT seems\nmore suitable for that task, as shown for MOSEI\nand MELD. A potential issue for MAT is that we\nwork with shallow architectures (B = 4) compared\nto recent NLP solutions like BERT using up to 48\nlayers. In the scope of the dataset presented, we\nhave not enough samples to train such architectures.\nIt is possible that MNT adjust better with shallow\nlayers because it can modulate entire feature maps\ntwice per blocks.\n9 Conclusions\nIn this paper, we propose two different architec-\ntures, MAT (Modulated Attention Transformer)\nand MNT (Modulated Normalization Transformer),\nfor the task of emotion recognition and sentiment\nanalysis. They are based on Transformers and use\ntwo modalities: linguistic and acoustic.\nThe performance of our methods were thor-\noughly studied by comparison with a Naive Trans-\nformer baseline and the most relevant related works\non several datasets suited for our experiments.\nWe showed that our Transformer baseline en-\ncoding separately both modalities already performs\nwell compared to state-of-the-art. The solutions in-\ncluding modulation of one modality from the other\nshow a higher performance. Overall, the architec-\ntures offer an efﬁcient, lightweight and scalable\nsolution that challenges, and sometimes surpasses,\nthe previous works in the ﬁeld.\n9\nAcknowledgements\nNo´e Tits is funded through a FRIA grant (Fonds\npour la Formation `a la Recherche dans l’Industrie\net l’Agriculture, Belgium).\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nCarlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\nKazemzadeh, Emily Mower, Samuel Kim, Jean-\nnette N Chang, Sungbok Lee, and Shrikanth S\nNarayanan. 2008. Iemocap: Interactive emotional\ndyadic motion capture database. Language re-\nsources and evaluation, 42(4):335.\nJean-Benoit Delbrouck, No ´e Tits, Mathilde Brous-\nmiche, and St ´ephane Dupont. 2020. A transformer-\nbased joint-encoding for emotion recognition and\nsentiment analysis. In Second Grand-Challenge\nand Workshop on Multimodal Language (Challenge-\nHML), pages 1–7, Seattle, USA. Association for\nComputational Linguistics.\nVincent Dumoulin, Ethan Perez, Nathan Schucher, Flo-\nrian Strub, Harm de Vries, Aaron Courville, and\nYoshua Bengio. 2018. Feature-wise transforma-\ntions. Distill. Https://distill.pub/2018/feature-wise-\ntransformations.\nPaul Ekman. 1999. Basic emotions. Handbook of cog-\nnition and emotion, 98(45-60):16.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nAyush Kumar and Jithendra Vepa. 2020. Gated mecha-\nnism for attention based multi modal sentiment anal-\nysis. In ICASSP 2020-2020 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 4477–4481. IEEE.\nNavonil Majumder, Soujanya Poria, Devamanyu Haz-\narika, Rada Mihalcea, Alexander Gelbukh, and Erik\nCambria. 2019. Dialoguernn: An attentive rnn for\nemotion detection in conversations. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 33, pages 6818–6825.\nBrian McFee, Colin Raffel, Dawen Liang, Daniel PW\nEllis, Matt McVicar, Eric Battenberg, and Oriol Ni-\neto. 2015. librosa: Audio and music signal analysis\nin python. In Proceedings of the 14th python in sci-\nence conference, pages 18–25.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word\nrepresentation. In EMNLP, volume 14, pages 1532–\n1543.\nHai Pham, Paul Pu Liang, Thomas Manzini, Louis-\nPhilippe Morency, and Barnab ´as P ´oczos. 2019.\nFound in translation: Learning robust joint represen-\ntations by cyclic translations between modalities. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, volume 33, pages 6892–6899.\nHai Pham, Thomas Manzini, Paul Pu Liang, and\nBarnab´as Pocz ´os. 2018. Seq2Seq2Sentiment: Mul-\ntimodal sequence to sequence models for senti-\nment analysis. In Proceedings of Grand Challenge\nand Workshop on Human Multimodal Language\n(Challenge-HML), pages 53–63, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nSoujanya Poria, Erik Cambria, Devamanyu Hazarika,\nNavonil Majumder, Amir Zadeh, and Louis-Philippe\nMorency. 2017. Context-dependent sentiment anal-\nysis in user-generated videos. In Proceedings of the\n55th annual meeting of the association for compu-\ntational linguistics (volume 1: Long papers) , pages\n873–883.\nSoujanya Poria, Devamanyu Hazarika, Navonil Ma-\njumder, Gautam Naik, Erik Cambria, and Rada Mi-\nhalcea. 2019. Meld: A multimodal multi-party\ndataset for emotion recognition in conversations. In\nProceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 527–\n536.\nGaurav Sahu. 2019. Multimodal speech emotion recog-\nnition and ambiguity resolution. arXiv preprint\narXiv:1904.06022.\nAman Shenoy and Ashish Sardana. 2020. Multilogue-\nnet: A context aware rnn for multi-modal emo-\ntion detection and sentiment analysis in conversa-\ntion. arXiv preprint arXiv:2002.08267.\nHideyuki Tachibana, Katsuya Uenoyama, and Shun-\nsuke Aihara. 2018. Efﬁciently trainable text-to-\nspeech system based on deep convolutional net-\nworks with guided attention. In 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 4784–4788. IEEE.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ. Zico Kolter, Louis-Philippe Morency, and Ruslan\nSalakhutdinov. 2019. Multimodal transformer for\nunaligned multimodal language sequences. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), Florence, Italy. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\n10\nYansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang,\nAmir Zadeh, and Louis-Philippe Morency. 2019.\nWords can shift: Dynamically adjusting word repre-\nsentations using nonverbal behaviors. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelli-\ngence, volume 33, pages 7216–7223.\nSeunghyun Yoon, Seokhyun Byun, and Kyomin Jung.\n2018. Multimodal speech emotion recognition us-\ning audio and text. In 2018 IEEE Spoken Language\nTechnology Workshop (SLT), pages 112–118. IEEE.\nZhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and\nQi Tian. 2019. Deep modular co-attention networks\nfor visual question answering. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 6281–6290.\nAmir Zadeh, Paul Pu Liang, Navonil Mazumder,\nSoujanya Poria, Erik Cambria, and Louis-Philippe\nMorency. 2018a. Memory fusion network for multi-\nview sequential learning. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence.\nAmir Zadeh, Rowan Zellers, Eli Pincus, and Louis-\nPhilippe Morency. 2016. Mosi: multimodal cor-\npus of sentiment intensity and subjectivity anal-\nysis in online opinion videos. arXiv preprint\narXiv:1606.06259.\nAmirAli Zadeh, Paul Pu Liang, Soujanya Poria, Erik\nCambria, and Louis-Philippe Morency. 2018b. Mul-\ntimodal language analysis in the wild: CMU-\nMOSEI dataset and interpretable dynamic fusion\ngraph. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 2236–2246, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nAmirAli Zadeh, Paul Pu Liang, Soujanya Poria, Erik\nCambria, and Louis-Philippe Morency. 2018c. Mul-\ntimodal language analysis in the wild: CMU-\nMOSEI dataset and interpretable dynamic fusion\ngraph. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 2236–2246, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nDong Zhang, Liangqing Wu, Changlong Sun,\nShoushan Li, Qiaoming Zhu, and Guodong Zhou.\n2019. Modeling both context-and speaker-sensitive\ndependence for emotion detection in multi-speaker\nconversations. In Proceedings of the 28th Interna-\ntional Joint Conference on Artiﬁcial Intelligence ,\npages 5415–5421. AAAI Press.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7334266304969788
    },
    {
      "name": "Computer science",
      "score": 0.6990540027618408
    },
    {
      "name": "Emotion recognition",
      "score": 0.5474961996078491
    },
    {
      "name": "Speech recognition",
      "score": 0.48660004138946533
    },
    {
      "name": "Natural language processing",
      "score": 0.39247965812683105
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33179664611816406
    },
    {
      "name": "Engineering",
      "score": 0.1643025279045105
    },
    {
      "name": "Electrical engineering",
      "score": 0.1093967854976654
    },
    {
      "name": "Voltage",
      "score": 0.1030181348323822
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I130929987",
      "name": "University of Mons",
      "country": "BE"
    }
  ]
}