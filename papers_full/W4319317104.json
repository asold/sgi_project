{
    "title": "A Novel Two-Stream Transformer-Based Framework for Multi-Modality Human Action Recognition",
    "url": "https://openalex.org/W4319317104",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5037866934",
            "name": "Jing Shi",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A5100320675",
            "name": "Yuanyuan Zhang",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A5042240342",
            "name": "Weihang Wang",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A5104101991",
            "name": "Bin Xing",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5067187570",
            "name": "Dasha Hu",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A5070882361",
            "name": "Liangyin Chen",
            "affiliations": [
                "Sichuan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4205432963",
        "https://openalex.org/W4306391005",
        "https://openalex.org/W4296478337",
        "https://openalex.org/W2716916105",
        "https://openalex.org/W2547204915",
        "https://openalex.org/W4304080730",
        "https://openalex.org/W6838532648",
        "https://openalex.org/W3016234935",
        "https://openalex.org/W3119171349",
        "https://openalex.org/W4312245820",
        "https://openalex.org/W4224932472",
        "https://openalex.org/W4224325099",
        "https://openalex.org/W4285191079",
        "https://openalex.org/W4224249138",
        "https://openalex.org/W3092990874",
        "https://openalex.org/W4224919232",
        "https://openalex.org/W4214612132",
        "https://openalex.org/W3126721948",
        "https://openalex.org/W3205898195",
        "https://openalex.org/W2554408731",
        "https://openalex.org/W2526041356",
        "https://openalex.org/W4312658081",
        "https://openalex.org/W3049455300",
        "https://openalex.org/W6756911974",
        "https://openalex.org/W2916798096",
        "https://openalex.org/W2964134613",
        "https://openalex.org/W2944006115",
        "https://openalex.org/W3035180180",
        "https://openalex.org/W2056898157",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W6802919969",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W2963155035",
        "https://openalex.org/W2770804203",
        "https://openalex.org/W2901751978",
        "https://openalex.org/W2947084868",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W3113067059",
        "https://openalex.org/W3092654784",
        "https://openalex.org/W2999049832",
        "https://openalex.org/W3185273257",
        "https://openalex.org/W2948058585",
        "https://openalex.org/W4237345212",
        "https://openalex.org/W3212704715"
    ],
    "abstract": "Due to the great success of Vision Transformer (ViT) in image classification tasks, many pure Transformer architectures for human action recognition have been proposed. However, very few works have attempted to use Transformer to conduct bimodal action recognition, i.e., both skeleton and RGB modalities for action recognition. As proved in many previous works, RGB modality and skeleton modality are complementary to each other in human action recognition tasks. How to use both RGB and skeleton modalities for action recognition in a Transformer-based framework is a challenge. In this paper, we propose RGBSformer, a novel two-stream pure Transformer-based framework for human action recognition using both RGB and skeleton modalities. Using only RGB videos, we can acquire skeleton data and generate corresponding skeleton heatmaps. Then, we input skeleton heatmaps and RGB frames to Transformer at different temporal and spatial resolutions. Because the skeleton heatmaps are primary features compared to the original RGB frames, we use fewer attention layers in the skeleton stream. At the same time, two ways are proposed to fuse the information of two streams. Experiments demonstrate that the proposed framework achieves the state of the art on four benchmarks: three widely used datasets, Kinetics400, NTU RGB+D 60, and NTU RGB+D 120, and the fine-grained dataset FineGym99.",
    "full_text": null
}