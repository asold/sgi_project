{
  "title": "DPText-DETR: Towards Better Scene Text Detection with Dynamic Points in Transformer",
  "url": "https://openalex.org/W4382464147",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4382474915",
      "name": "Maoyuan Ye",
      "affiliations": [
        "Institute of Art",
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2087085944",
      "name": "Jing Zhang",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2110846611",
      "name": "Shan-Shan Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2155941866",
      "name": "Juhua Liu",
      "affiliations": [
        "Wuhan University",
        "Institute of Art"
      ]
    },
    {
      "id": "https://openalex.org/A1911857409",
      "name": "Bo Du",
      "affiliations": [
        "Wuhan University",
        "Institute of Art"
      ]
    },
    {
      "id": "https://openalex.org/A2104129307",
      "name": "Dacheng Tao",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A4382474915",
      "name": "Maoyuan Ye",
      "affiliations": [
        "Wuhan University",
        "Institute of Art"
      ]
    },
    {
      "id": "https://openalex.org/A2110846611",
      "name": "Shan-Shan Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2155941866",
      "name": "Juhua Liu",
      "affiliations": [
        "Institute of Art",
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A1911857409",
      "name": "Bo Du",
      "affiliations": [
        "Wuhan University",
        "Institute of Art"
      ]
    },
    {
      "id": "https://openalex.org/A2104129307",
      "name": "Dacheng Tao",
      "affiliations": [
        "University of Sydney"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6760808182",
    "https://openalex.org/W6781106802",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3172799005",
    "https://openalex.org/W3203003533",
    "https://openalex.org/W3195421894",
    "https://openalex.org/W3189348500",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W4226329156",
    "https://openalex.org/W4225683503",
    "https://openalex.org/W4312841185",
    "https://openalex.org/W2970910956",
    "https://openalex.org/W3042618999",
    "https://openalex.org/W2991302481",
    "https://openalex.org/W3014690505",
    "https://openalex.org/W4226013992",
    "https://openalex.org/W3007389333",
    "https://openalex.org/W2914492226",
    "https://openalex.org/W3161679556",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2810028092",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6800709815",
    "https://openalex.org/W6772226814",
    "https://openalex.org/W3111172959",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W6761041305",
    "https://openalex.org/W4224329844",
    "https://openalex.org/W2925359305",
    "https://openalex.org/W2967464567",
    "https://openalex.org/W6811069853",
    "https://openalex.org/W3175227919",
    "https://openalex.org/W3035222584",
    "https://openalex.org/W3105084609",
    "https://openalex.org/W3029858749",
    "https://openalex.org/W4224226832",
    "https://openalex.org/W3184364189",
    "https://openalex.org/W6810854768",
    "https://openalex.org/W3206276570",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3154506171",
    "https://openalex.org/W4313136325",
    "https://openalex.org/W4315705623",
    "https://openalex.org/W3181016597",
    "https://openalex.org/W2998621280",
    "https://openalex.org/W4312257978",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W4380839071",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W3097932944",
    "https://openalex.org/W3034792612",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4300860549",
    "https://openalex.org/W3109340983",
    "https://openalex.org/W3110398855",
    "https://openalex.org/W2967615747",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4312115774",
    "https://openalex.org/W3196976036",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2991626090",
    "https://openalex.org/W3003218881",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3003921261",
    "https://openalex.org/W4312593844",
    "https://openalex.org/W2979382951",
    "https://openalex.org/W4312230431",
    "https://openalex.org/W3093218477",
    "https://openalex.org/W3003868038",
    "https://openalex.org/W3035709993",
    "https://openalex.org/W4214627427"
  ],
  "abstract": "Recently, Transformer-based methods, which predict polygon points or Bezier curve control points for localizing texts, are popular in scene text detection. However, these methods built upon detection transformer framework might achieve sub-optimal training efficiency and performance due to coarse positional query modeling. In addition, the point label form exploited in previous works implies the reading order of humans, which impedes the detection robustness from our observation. To address these challenges, this paper proposes a concise Dynamic Point Text DEtection TRansformer network, termed DPText-DETR. In detail, DPText-DETR directly leverages explicit point coordinates to generate position queries and dynamically updates them in a progressive way. Moreover, to improve the spatial inductive bias of non-local self-attention in Transformer, we present an Enhanced Factorized Self-Attention module which provides point queries within each instance with circular shape guidance. Furthermore, we design a simple yet effective positional label form to tackle the side effect of the previous form. To further evaluate the impact of different label forms on the detection robustness in real-world scenario, we establish an Inverse-Text test set containing 500 manually labeled images. Extensive experiments prove the high training efficiency, robustness, and state-of-the-art performance of our method on popular benchmarks. The code and the Inverse-Text test set are available at https://github.com/ymy-k/DPText-DETR.",
  "full_text": "DPText-DETR: Towards Better Scene Text Detection with Dynamic Points in\nTransformer\nMaoyuan Ye1, Jing Zhang2, Shanshan Zhao3, Juhua Liu1*, Bo Du4*, Dacheng Tao3,2\n1 Research Center for Graphic Communication, Printing and Packaging, Institute of Artificial Intelligence, Wuhan University\n2 The University of Sydney\n3 JD Explore Academy\n4 National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence, School of Computer\nScience and Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University\n{yemaoyuan, liujuhua, dubo}@whu.edu.cn, jing.zhang1@sydney.edu.au, {sshan.zhao00, dacheng.tao}@gmail.com\nAbstract\nRecently, Transformer-based methods, which predict poly-\ngon points or Bezier curve control points for localizing\ntexts, are popular in scene text detection. However, these\nmethods built upon detection transformer framework might\nachieve sub-optimal training efficiency and performance due\nto coarse positional query modeling. In addition, the point\nlabel form exploited in previous works implies the reading\norder of humans, which impedes the detection robustness\nfrom our observation. To address these challenges, this paper\nproposes a concise Dynamic Point Text DEtection TRans-\nformer network, termed DPText-DETR. In detail, DPText-\nDETR directly leverages explicit point coordinates to gen-\nerate position queries and dynamically updates them in a\nprogressive way. Moreover, to improve the spatial inductive\nbias of non-local self-attention in Transformer, we present an\nEnhanced Factorized Self-Attention module which provides\npoint queries within each instance with circular shape guid-\nance. Furthermore, we design a simple yet effective positional\nlabel form to tackle the side effect of the previous form. To\nfurther evaluate the impact of different label forms on the\ndetection robustness in real-world scenario, we establish an\nInverse-Text test set containing 500 manually labeled images.\nExtensive experiments prove the high training efficiency, ro-\nbustness, and state-of-the-art performance of our method on\npopular benchmarks. The code and the Inverse-Text test set\nare available at https://github.com/ymy-k/DPText-DETR.\nIntroduction\nText reading and understanding have aroused increasing re-\nsearch interest in the computer vision community (Liao et al.\n2021; Liao et al. 2020a; Liu et al. 2020b, 2021a; Zhang et al.\n2020; Singh et al. 2019; He et al. 2022; Du et al. 2022; Liu\net al. 2020a; Qiao et al. 2021; Zhou et al. 2021), due to the\nwide range of practical applications, such as autonomous\ndriving (Zhang and Tao 2020). To achieve it, as a prerequi-\nsite, scene text detection has been studied extensively. How-\never, the distinction of scene text, e.g., complex styles and\narbitrary shapes make detection remain challenging.\n*Corresponding author. This work was done during Maoyuan\nYe‚Äôs internship at JD Explore Academy.\nCopyright ¬© 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nBox Proposal\n(ùë•,ùë¶,ùë§,‚Ñé)\nPositional Queries\nComposite Queries\nAdd\nBox Proposal\n(ùë•,ùë¶,ùë§,‚Ñé)\n Composite Queries\nAdd\nPoint \nSampling\nLearnable Control Point Content Queries\nPositional Queries\nLearnable Control Point Content Queries\n(identical and obscure)\n(diverse and explicit)\nOurs\n(a)\n(b)\n (c)\n (d)\nFigure 1: (a) Comparison of coarse (top) and our explicit\n(bottom) positional query modeling. (b) The original label\nimplies the reading order of humans. (c) The original label\ninduces the detector to implicitly learn the reading order, re-\nsulting in some flaws, such as false positives. Green points\nare the predicted start points of clockwise reading order. (d)\nThe detector cannot learn the reading order well even with\nextensive rotation augmentation.\nRecently, DETR (Carion et al. 2020) introduces Trans-\nformer (Vaswani et al. 2017) to object detection, forming\na concise and seminal end-to-end framework. Following\nDETR, lots of works (Zhu et al. 2020; Dai et al. 2021b;\nWang et al. 2021; Meng et al. 2021; Liu et al. 2022; Wang\net al. 2022) further improve the training convergence and\nperformance. For example, DAB-DETR (Liu et al. 2022) of-\nfers insights on the query which can be formed by a content\npart and a positional part, and proves that the positional part\nis essential for the training convergence. However, the above\ndetection transformers predicting axis-aligned boxes fall\nshort in handling arbitrary-shape scene texts. In response,\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n3241\nrecent DETR-like methods (Zhang et al. 2022d; Tang et al.\n2022) predict polygon control points or Bezier curve con-\ntrol points following ABCNet (Liu et al. 2020b). Specif-\nically, TESTR (Zhang et al. 2022d) enables Deformable\nDETR (Zhu et al. 2020) to predict polygon results in a subtle\nmanner. TESTR uses anchor box proposals from the Trans-\nformer encoder to generate positional queries and provide\nposition prior for control point content queries, as shown in\nthe top part of Fig. 1(a). However, the position prior from\nbox information is coarse and mismatches the target of pre-\ndicting points to some extent, which impacts the training ef-\nficiency. We abbreviate it as thequery formulation issue.\nIn addition, although the scheme of predicting control\npoints enables novel solutions to scene text detection, it also\nintroduces an issuew.r.tthe order of the points. In detail, pre-\nvious related works adopt the label form of control points ac-\ncording to the reading order of human, as shown in Fig. 1(b).\nThis scheme is straightforward, while we are curious about\nwhether it is necessary to enable the detector to localize the\ntext as the human does for understanding the text. Previous\nworks do not investigate the impact of such control point la-\nbel form. However, interestingly, we find this form harms\nthe detection robustness when there are inverse-like texts in\nthe training dataset, even though the ratios of such texts are\nquite low, e.g., about 2.8% in Total-Text (Ch‚Äông, Chan, and\nLiu 2020), 5.2% in CTW1500 (Liu et al. 2019), and 5.3%\nin ICDAR2019 ArT (Chng et al. 2019). We denote it as the\nlabel form issue. Some detection flaws caused by this issue\nare shown in Fig. 1(c). Since there are few inverse-like texts\nin existing benchmarks, we collect an Inverse-Text test set to\nfurther investigate the impact of this label form on detection\nrobustness in real-world scenario. The collected dataset con-\ntains 500 scene images with about 40% inverse-like texts.\nWe hope Inverse-Text can inspire and facilitate future re-\nsearches through the preliminary attempt in data by filling\nthe gap of lacking inverse-like texts in existing test sets.\nTo address the query formulation issue and the label form\nissue, we propose a novel Dynamic Point Text DEtection\nTRansformer network termed DPText-DETR. In terms of\nthe query formulation issue, we propose an Explicit Point\nQuery Modeling (EPQM) method. Specifically, instead of\nusing boxes, we directly utilize point coordinates to get po-\nsitional queries, as illustrated in the bottom part of Fig. 1(a).\nWith the explicit and complete point formulation, the model\nis able to dynamically update points in decoder layers.\nMoreover, non-local self-attention lags behind convolution\nin capturing spatial inductive bias. Hence, we propose an\nEnhanced Factorized Self-Attention (EFSA) module lever-\naging circular convolution (Peng et al. 2020) to explicitly\nmodel the circular form of polygon points and complement\nthe pure self-attention. In terms of the label form issue, we\ndesign a practical positional label form, which makes the\nstart points independent of the semantic content of texts.\nWith this simple operation, it can noticeably improve the de-\ntection robustness.\nOverall, our main contributions are three-fold:\n‚Ä¢ We propose DPText-DETR, which improves the train-\ning convergence and the spatial inductive bias of self-\nattention by exploiting EPQM and EFSA modules.\n‚Ä¢ We investigate the impact of control point label form and\ndesign a practical positional label form to improve the\ndetection robustness. We also establish a novel Inverse-\nText test set to fill the gap of lacking inverse-like texts in\nexisting datasets.\n‚Ä¢ DPText-DETR sets new state-of-the-art on representative\narbitrarily-shaped scene text detection benchmarks. It\nalso has fast convergence and promising data efficiency.\nRelated Work\nDetection Transformers\nTransformer (Vaswani et al. 2017) originates from machine\ntranslation and soon becomes popular in computer vision\ncommunity (Dosovitskiy et al. 2020; Liu et al. 2021b; Xu\net al. 2021; Zhang et al. 2022a,b). Recently, the semi-\nnal DETR (Carion et al. 2020) treats object detection as\na set prediction problem and proposes a concise end-to-\nend framework without complex hand-crafted anchor gen-\neration and post-processing. However, DETR suffers from\nsignificant slow training convergence and inefficient usage\nof high resolution features, which have sparked the fol-\nlowing researches in detection transformers. For example,\nDeformable-DETR (Zhu et al. 2020) attends to sparse fea-\ntures to address above issues. DE-DETR (Wang et al. 2022)\nidentifies the key factor that affects data efficiency is sparse\nfeature sampling. DAB-DETR (Liu et al. 2022) uses dy-\nnamic anchor boxes as position queries in Transformer de-\ncoder to facilitate training. In comparison, in our study, we\nrecast the query in point formulation to handle arbitrary-\nshape scene texts and speed up training.\nContour-based Text Detection and Spotting\nFrom the perspective of modeling text contour, ABCNet\n(Liu et al. 2020b) predicts Bezier curve control points to\nadaptively fit arbitrarily-shaped texts for the first time. To en-\nhance the capability to localize highly-curved texts, FCENet\n(Zhu et al. 2021) models text instances with Fourier con-\ntour fitting. In contrast, TextBPN and its extension (Zhang\net al. 2021, 2022c) segment various probability maps and\nuse them as priors to generate coarse boundary proposals,\nthen iteratively refine boundary points with graph convolu-\ntion or Transformer encoder. Considering that segmentation\nmight be sensitive to noise, PCR (Dai et al. 2021a) proposes\nto progressively evolve the initial text proposal to arbitrar-\nily shaped contours in a top-down manner in the convolu-\ntion framework. More recently, inspired by detection trans-\nformers, FSG(Tang et al. 2022) samples a few representa-\ntive features and uses Transformer encoder layers to implic-\nitly group them, then predicts Bezier curve control points\nfor localization. In comparison, TESTR (Zhang et al. 2022d)\nproposes a box-to-polygon scheme for text contour model-\ning, which utilizes box proposals from Transformer encoder\nas position queries to guide learnable control points content\nqueries. We conjecture that the box information is coarse\nfor point target in detection, which hinders efficient training.\nHence, our work investigates the explicit and complete point\nquery formulation in detection transformer framework.\n3242\nDecoder Layer\nK\nDeformable Cross-Attention\nQ\nV\nQ Refer.V\nLayer i\nEFSA\nEncoder Features\nContent Queries Point Coordinates\nFlattened Multi-scale Features \nPosition Embeddings\nTransformer Encoder\nBackbone\nPrior Points Sampling\nShared \nContent Queries\nPoint Coordinates\nLinear\n MLP\nTransformer Decoder\nTop-K Text Proposals Generation\nùíöùüèùíôùüè (‚àÜùë•,‚àÜùë¶)\n(ùíôùëµ\n(ùíä+ùüè),ùíöùëµ\n(ùíä+ùüè))(ùíôùüè\n(ùíä+ùüè),ùíöùüè\n(ùíä+ùüè))(ùë•1,ùë¶1) (ùë•ùëÅ,ùë¶ùëÅ)\n(ùíôùëµ\n(ùíä+ùüè),ùíöùëµ\n(ùíä+ùüè))(ùíôùüè\n(ùíä+ùüè),ùíöùüè\n(ùíä+ùüè))(ùë•1,ùë¶1) (ùë•ùëÅ,ùë¶ùëÅ)\n(‚àÜùë•,‚àÜùë¶)\nFigure 2: The architecture of DPText-DETR, which is built upon Deformable-DETR (Zhu et al. 2020), mainly consists of a\nCNN backbone, a Transformer encoder and decoder. Explicit points are calculated by Prior Points Sampling and encoded into\npositional queries. The point coordinates are progressively refined to form the final polygon predictions.\nMethodology\nThis paper studies the scene text detection problem by de-\nveloping an efficient Transformer-based decoder and inves-\ntigating the influence of control point labels on the detection\nrobustness. In more details, we propose an Explicit Point\nQuery Modeling (including Prior Points Sampling and Point\nUpdate) method and an Enhanced Factorized Self-Attention\nmodule. In this section, we first briefly describe the overall\npipeline and then detail the implementation.\nOverview\nThe overall model architecture is illustrated in Fig. 2. In gen-\neral, given a scene text image, we use a CNN backbone fol-\nlowed by a Transformer encoder to extract features. After\nthe final encoder layer, multiple axis-aligned boxes are gen-\nerated as proposals. With the center point and scale infor-\nmation of each box, a certain number of initial control point\ncoordinates can be uniformly sampled on the top and bot-\ntom sides. In this way, these point coordinates can be used as\nsuitable reference points for the deformable cross-attention\nmodule. In the decoder, point coordinates are encoded and\nadded to corresponding control point content queries to form\ncomposite queries. The composite queries are firstly sent to\nEFSA to further mine their relative relationships and then\nfed into the deformable cross-attention module. Then con-\ntrol point coordinate prediction head is adopted to dynami-\ncally update the reference points layer-by-layer to better fit\narbitrary-shape scene text. Finally, prediction heads are used\nto generate class confidence scores and N control point co-\nordinates for each text instance. During training, we follow\n(Zhang et al. 2022d) to calculate losses for classification and\ncontrol points. More details are described as follows.\nPositional Label Form\nThe original label form shown in Fig. 1(b) is in line with\nhuman reading order. However, this form induces detector\nto implicitly learn the order, which increases the learning\nburden and confuses the model when the texts are in dif-\nferent orders during training. Moreover, even with sufficient\nrotation augmentations during training, it is difficult for the\ndetector to correctly predict the reading order from visual\nfeatures alone, as shown in Fig. 1(d).\nTo ease the difficulty, we present a positional label form\nto guide the detector to distinguish the top and bottom sides\nof scene text in a pure spatial sense without considering the\nconcrete content of texts. As illustrated in Fig. 3, the posi-\ntional label form mainly follows two simple rules: clockwise\norder and independent of text content. Specifically, we make\nthe order of all original point labels in clockwise. If the orig-\ninal top side of text instance lies in the bottom position, the\nstarting point is adjusted to the other side. When two sides\nare arranged left and right, if there is one side with a smaller\nminimum y value (origin in left-top), the starting point is ad-\njusted to this side, otherwise, it is on the fixed default side.\nExplicit Point Query Modeling\nPrior Points Sampling. It is remarkable to transform axis-\naligned box predictions into polygons that fit scene text with\na concise yet effective operation, i.e., the box-to-polygon\nscheme proposed by TESTR (Zhang et al. 2022d). Here,\nwe briefly review this scheme. Concretely, after the final\nencoder layer, each anchor box provided by a top-K pro-\nposals generator is encoded, and then shared by N con-\ntrol point content queries. The resulting composite queries\nQ(i)(i = 1, . . . , K) can be formulated as follows:\nQ(i) = P(i) + C = œÜ((x, y, w, h)(i)) + (p1, . . . , pN ), (1)\nwhere P and C represent the positional and the content part\nof each composite query, respectively. œÜ is the sine posi-\ntional encoding function followed with a linear and normal-\nization layer. (x, y, w, h)represents the center coordinate\nand scale information of each anchor box. (p1, . . . , pN ) is\nthe N learnable control point content queries shared across\nK composite queries. Note that we set the detector with the\nquery formulation in Eq. (1) as our baseline. From Eq. (1),\n3243\nReading\nOrder\nOriginal Positional\nbottom \nposition\ntop position\nWrong\nOrder\nFigure 3: The original and positional label form. The points\nof each original label are sampled from the Bezier curves\n(Liu et al. 2020b) which fit the annotated polygon.\nwe can find that different control point content queries share\nthe same anchor box prior information in each instance. Al-\nthough the prior facilitates the prediction of control point po-\nsitions, it mismatches the point targets to some extent. Con-\ntent queries lack respective explicit position priors to exploit\nin the box sub-region.\nMotivated by the positional label form and the shape prior\nthat the top and bottom side of a scene text are usually close\nto the corresponding side on bounding box, we sample N\n2\npoint coordinates pointn(n = 1, . . . , N) uniformly on the\ntop and bottom side of each anchor box, respectively:\npointn =\nÔ£±\nÔ£≤\nÔ£≥\n(x ‚àí w\n2 + (n‚àí1)√ów\nN\n2 ‚àí1 , y‚àí h\n2 ), n ‚â§ N\n2\n(x ‚àí w\n2 + (N‚àín)√ów\nN\n2 ‚àí1 , y+ h\n2 ), n> N\n2\n.\n(2)\nWith (point1, . . . , pointN ), we can generate composite\nqueries using the following complete point formulation:\nQ(i) = œÜ‚Ä≤((point1, . . . , pointN )(i)) + (p1, . . . , pN ). (3)\nIn this way, N control point content queries enjoy their\nrespective explicit position prior, resulting in the superior\ntraining convergence.\nPoint Update. With the point coordinates, we can refine\npoint positions layer-by-layer and use the updated positions\nas new reference points for deformable cross-attention. In\ncomparison, TESTR directly adopts the anchor boxes infor-\nmation to generate position queries. Therefore, it is hard to\nperform refinement between decoder layers. Specifically, in\nour model, we update control points in each decoder layer\nafter getting respective offsets (‚àÜx, ‚àÜy) by a prediction\nhead, as illustrated in the decoder layer part of Fig. 2.\nDiscussion. We also notice that the very recent work\nBoundaryFormer (Lazarow, Xu, and Tu 2022) adopts a sim-\nilar point query formulation in the instance segmentation\ntask. BoundaryFormer aims at predicting polygons yet uses\ninstance mask supervision. In BoundaryFormer, a fully con-\nvolutional detector (Ren et al. 2015; Tian et al. 2019) is\nexploited to predict object boxes. Next, a diamond is ini-\ntialized for each box. Then, a Transformer decoder is used\nto refine the position of vertexes. Between decoder layers,\nnew points are inserted between existing ones to produce\nfine polygons. In comparison, we aim to address the train-\ning concerned issue by modeling explicit and complete point\nqueries. In our model, a fixed number of points are sampled\non the top and bottom sides of each proposal box before\nthe Transformer decoder, according to the shape prior that\ntexts can be located with only two sides. The explicit point\nformulation enables the decoder to iteratively refine points\nfor more precise final predictions in both BoundaryFormer\nand our model. However, in our DETR-based model, the ex-\nplicit point query formulation is further explored to address\nthe relatively slow convergence issue.\nEnhanced Factorized Self-Attention\nFollowing (Zhang et al. 2022d), we exploit Factorized Self-\nAttention (FSA) (Dong et al. 2021) in our baseline. In FSA,\nan intra-group self-attention (SAintra) across N subqueries\nbelonging to each of the Q(i) is firstly exploited to cap-\nture the relationship between different points within each\ntext instance. After SAintra, an inter-group self-attention\n(SAinter) across K composite queries is adopted to cap-\nture the relationship between different instances. We con-\njecture that the non-local SAintra falls short in capturing\nthe circular shape prior of polygon control points. Hence,\nwe leverage the local circular convolution (Peng et al.\n2020) to complement FSA, forming the Enhanced Factor-\nized Self-Attention. Concretely,SAintra is firstly performed\nto get queries Qintra = SAintra(Q), where keys are the\nsame as Q while values exclude the positional part. Mean-\nwhile, locally enhanced queries are generated: Qlocal =\nReLU(BN (CirConv (Q))). Then, fused queries can be\nobtained: Qfuse = LN(F C(C + LN(Qintra + Qlocal))),\nwhere C represents the content queries used as a shortcut,\nF Cis a fully connected layer, BN is BatchNorm, and LN\nis LayerNorm. Next, the relationships between different in-\nstances are mined: Qinter = SAinter(Qfuse ). After that,\nQinter is sent to the deformable cross-attention module. Us-\ning one circular convolution layer with four-neighborhood\nachieves the best trade-off between performance and infer-\nence speed. We adopt this setting for experiments.\nExperiments\nWe conduct experiments on three arbitrary-shape scene\ntext benchmarks: Total-Text (Ch‚Äông, Chan, and Liu 2020),\nCTW1500 (Liu et al. 2019) and ICDAR19 ArT (Chng et al.\n2019). Ablation studies are conducted on Total-Text to ver-\nify the effectiveness of each component of our methods.\nDatasets\nFirst, we briefly introduce the exploited datasets. Synth-\nText 150K(Liu et al. 2020b) is a synthesized dataset for\narbitrary-shape scene text, containing 94,723 images with\nmulti-oriented text and 54,327 images with curved text.\nTotal-Text (Ch‚Äông, Chan, and Liu 2020) consists of 1,255\ntraining images and 300 test images. Word-level polygon an-\nnotations are provided. Rot.Total-Textis a test set derived\nfrom the Total-Text test set. Since the original label form\ninduces model to generate unstable prediction as shown in\nFig. 1(c), we apply large rotation angles (45 ‚ó¶, 135‚ó¶, 180‚ó¶,\n225‚ó¶, 315‚ó¶) on images of the Total-Text test set to exam-\nine the model robustness, resulting in 1,800 test images in-\ncluding the original test set. CTW1500 (Liu et al. 2019)\ncontains 1,000 training images and 500 test images. Text-\nline level annotations are presented. ICDAR19 ArT(Chng\n3244\nMethod Backbone Total-Text\nCTW1500 ICDAR19 ArT\nP R F P\nR F P R F\nTextSnake\n(Long et al. 2018) VGG16 82.7 74.5 78.4 67.9 85.3 75.6 ‚àí ‚àí ‚àí\nPAN (Wang et al. 2019) Res-18 89.3 81.0 85.0 86.4 81.2 83.7 ‚àí ‚àí ‚àí\nCRAFT (Baek et al. 2019) ‚Ä† VGG16 87.6 79.9 83.6 86.0 81.1 83.5 77.2 68.9 72.9\nTextFuseNet (Ye et al. 2020) ‚Ä† Res50 87.5 83.2 85.3 85.8 85.0 85.4 82.6 69.4 75.4\nDB (Liao et al. 2020b) Res50-DCN 87.1 82.5 84.7 86.9 80.2 83.4 ‚àí ‚àí ‚àí\nPCR (Dai et al. 2021a) DLA34 88.5 82.0 85.2 87.2 82.3 84.7 84.0 66.1 74.0\nABCNet-v2 (Liu et al. 2021a) Res50 90.2 84.1 87.0 85.6 83.8 84.7 ‚àí ‚àí ‚àí\nI3CL (Du et al. 2022) Res50 89.2 83.7 86.3 87.4 84.5 85.9 82.7 71.3 76.6\nTextBPN++ (Zhang\net al. 2022c) Res50 91.8 85.3 88.5 87.3 83.8 85.5 81.1\n71.1 75.8\nFSG (Tang et al. 2022) Res50 90.7 85.7 88.1 88.1 82.4 85.2 ‚àí ‚àí ‚àí\nTESTR-polygon (Zhang et al. 2022d) Res50 93.4 81.4 86.9 92.0 82.6 87.1 ‚àí ‚àí ‚àí\nSwinTextSpotter (Huang et al. 2022) Swin ‚àí ‚àí 88.0 ‚àí ‚àí 88.0 ‚àí ‚àí ‚àí\nDPText-DETR (ours) Res50\n91.8 86.4 89.0 91.7 86.2 88.8 83.0 73.7 78.1\nTable 1: Quantitative detection results on benchmarks. ‚ÄúP‚Äù, ‚ÄúR‚Äù and ‚ÄúF‚Äù denote Precision, Recall and F-measure, respectively.\n‚Äú‚Ä†‚Äù means that the results on ICDAR19 ArT are collected from the official website (Chng et al. 2019).\nFigure 4: Qualitative results on Total-Text, CTW1500, and\nICDAR19 ArT, from left to right.\net al. 2019) is a large arbitrary-shape scene text benchmark.\nIt contains 5,603 training images and 4,563 test images.\nInverse-Text established in our work, consists of 500\ntest images. It is a arbitrary-shape scene text test set with\nabout 40% inverse-like instances. A few instances are mir-\nrored due to photographing. Some images are selected from\nexisting benchmark test sets, i.e., 121 images from IC-\nDAR19 ArT, 7 images from Total-Text, and 3 images from\nCTW1500. Other images are collected from the Internet.\nWord-level polygon annotations are provided. Some sam-\nples are shown in Fig. 6.\nImplementation Details\nWe adopt ResNet-50 (He et al. 2016) as the backbone. We\nuse 8 heads for multi-head attention and 4 sampling points\nfor deformable attention. The number of both encoder and\ndecoder layers is set to 6. The composite queries number K\nis 100 and default control points numberN is 16. We follow\nthe hyper-parameter setting of loss used in the detection part\nof (Zhang et al. 2022d). Models are trained with 4 NVIDIA\nA100 (40GB) GPUs and tested with 1 GPU.\nIn ablation studies, we do not pre-train models to intu-\nitively reveal the training convergence on Total-Text. We\ntrain models on Total-Text for 120k without rotation data\naugmentation and directly test them on Rot.Total-Text and\nInverse-Text to verify the robustness. To help the model\nadapt to different text orders, we additionally rotate Total-\nText training images with six angles (‚àí45 ‚ó¶, ‚àí30‚ó¶, ‚àí15‚ó¶,\n15‚ó¶, 30‚ó¶, 45‚ó¶) representing normal cases, and rotate all nor-\nmal cases for 180‚ó¶ representing inverse cases. When using\nrotation data, we train models for 200k iterations.\nThe complete training process is divided into two stages:\npre-training stage and finetuning stage. The batch size is\nset to 8. For Total-Text and CTW1500, following (Zhang\net al. 2022d; Liu et al. 2021a), the detector is pre-trained\non a mixture of SynthText 150K, MLT (Nayef et al. 2019)\nand Total-Text for 350k iterations. The initial learning rate\n(lr) is 1 √ó 10‚àí4 and is decayed to 1 √ó 10‚àí5 at 280k. We\nfinetune it on Total-Text for 20k iterations, with 5 √ó 10‚àí5\nlr which is divided by 10 at 16k. We adopt 13k finetuning\niterations for CTW1500, with 2 √ó 10‚àí5 lr. For ICDAR19\nArT, following (Du et al. 2022; Baek et al. 2020), we adopt\nLSVT (Sun et al. 2019) during pre-training. We use a mix-\nture of SynthText 150K, MLT, ArT and LSVT to pre-train\nthe model for 400k iterations. lr is 1 √ó 10‚àí4 and is de-\ncayed to 1 √ó 10‚àí5 at 320k. Then, we finetune it on ArT\nfor 50k iterations, with 5 √ó 10‚àí5 lr which is divided by 10\nat 40k. We use the AdamW optimizer (Loshchilov and Hut-\nter 2019) with Œ≤1 = 0.9, Œ≤2 = 0.999 and weight decay\nof 10‚àí4. Data augmentation strategies such as random crop,\nrandom blur, brightness adjusting, and color change are ap-\nplied. Note that rotation data mentioned above is only used\nin finetuning stage for each benchmark. We adopt multi-\nscale training strategy with the shortest edge ranging from\n480 to 832, and the longest edge kept within 1600.\nComparison with State-of-the-art Methods\nWe test our method on Total-Text, CTW1500, and ICDAR19\nArT. Quantitative results compared with previous methods\nare presented in Tab. 1. Our method achieves consistent\nstate-of-the-art performance. Compared with other detec-\ntors, for example, DPText-DETR outperforms TexBPN++\nby 0.5%, 3.3%, and 2.3% in terms of F-measure on Total-\n3245\nID Pos.Label EPQM EFSA Rotation T\notal-Text Rot.Total-Text Inverse-Text\nF FPS F FPS\nF FPS\n1 83.90 18.5 70.02 20.2 77.63 18.9\n2 ‚úì 84.58 18.5 73.92\n20.2 80.70 18.9\n3 ‚úì 85.15 17.9 71.28 19.7 79.44 18.5\n4 ‚úì ‚úì 85.86 17.3 72.60 18.9 80.33 18.3\n5 ‚úì ‚úì 85.28 17.9 74.87 19.7 81.56 18.5\n6 ‚úì ‚úì ‚úì 86.17 17.3 74.99 18.9 81.99 18.3\n7 ‚úì 84.98 18.5 83.99 20.2 84.28 18.9\n8 ‚úì ‚úì 86.07 18.5 84.52\n20.2 86.69 18.9\n9 ‚úì ‚úì 86.16 17.9 84.15 19.7 83.79 18.5\n10 ‚úì ‚úì ‚úì 86.21 17.3 84.53 18.9 85.95 18.3\n11 ‚úì ‚úì ‚úì 86.46 17.9 84.86 19.7 86.57 18.5\n12 ‚úì ‚úì ‚úì ‚úì 86.79 17.3 84.95 18.9 86.78 18.3\nTable 2: Ablations on test sets. ‚ÄúPos.Label‚Äù denotes the positional label form. Without EFSA means the FSA is used instead.\nText, CTW1500, and ICDAR2019 ArT, respectively. More-\nover, DPText-DETR leads I3CL by 2.7%, 2.9%, and 1.5%\nF-measure on the three benchmarks. Compared with FSG,\nour method achieves 0.9% and 3.6% higher F-measure on\nTotal-Text and CTW1500. DPText-DETR also outperforms\nthe state-of-the-art SwinTextSpotter by 1.0% and 0.8% in\nterms of F-measure on Total-Text and CTW1500. Some vi-\nsual results are provided in Fig. 4. It shows that DPText-\nDETR performs well on straight, curve, and even dense long\ntexts. A failure case is also shown, i.e., the right bottom im-\nage in ICDAR19 ArT, where the polygon prediction is af-\nfected by extremely compact curved texts.\nAblation Studies\nAs mentioned before, pre-training is not used in all experi-\nments of this subsection. Main ablation results are reported\nin Tab. 2. Notably, compared with previous pre-trained mod-\nels, DPText-DETR without pre-training can still achieve\ncompetitive performance (F-measure: 86.79%).\nPositional Label Form.As shown in Tab. 2, when the po-\nsitional label form is used, the F-measure scores on all test\nsets are improved. For example, the comparison between the\nline 1 and line 2 in the table demonstrates that the F-measure\nis improved by 0.68% on Total-Text, 3.90% on Rot.Total-\nText and 3.07% on Inverse-Text, which validates the effec-\ntiveness for model robustness. Moreover, positional label\nform can synergize better with rotation augmentation than\nthe original form to improve the detection performance and\nrobustness. When using rotation, the positional label form\nalso contributes to faster convergence as shown in Fig. 5(a).\nEPQM. In Tab. 2, we investigate the effectiveness of\nEPQM. EPQM intuitively boosts the performance and\nmakes the major contribution to the convergence as shown in\nFig. 5(a). Moreover, EPQM significantly enhances the few-\nshot learning ability. As shown in Tab. 3, when the training\niterations and data volume are decreased, huge performance\ndegradation of baseline models turns up, while the models\nwith EPQM are far less affected.\nEFSA. In Tab. 2 and Tab. 3, we verify the effectiveness of\nEPQM EFSA TD-Ratio Total-T\next Inverse-Text\nF Improv.\nF Improv.\n100% 73.92 ‚àí 70.90 ‚àí\n‚úì 100% 82.99 9.07 78.18 7.28\n‚úì\n‚úì 100% 83.66 9.74 79.09 8.19\n50% 30.45 ‚àí 22.62 ‚àí\n‚úì 50% 78.90 48.45 72.78 50.16\n‚úì\n‚úì 50% 80.22 49.77 73.97 51.35\n25% 14.94 ‚àí 6.98 ‚àí\n‚úì 25% 58.54 43.6 52.32 45.34\n‚úì\n‚úì 25% 70.49 55.55 60.15 53.17\nTable 3: Fewer iterations and training data test. The posi-\ntional label form is adopted. ‚ÄúTD-Ratio‚Äù: the training data\nratio compared with the original one. ‚ÄúImprov.‚Äù: the im-\nprovement on F-measure. In the first three rows, models are\nonly trained for 12k iterations on Total-Text without rotation\naugmentation and directly tested on Inverse-Text. In the rest\nparts, we randomly sample training data according to TD-\nRatio while keeping the equivalent training epochs as used\nin the first three rows. We train 6k iterations for the middle\nthree models and 3k iterations for the last ones.\nEFSA. The comparison between line 5 and line 6 in Tab. 2\nshows that EFSA can improve the F-measure by 0.89%.\nTab. 3 shows that EFSA enables the model to learn better\nwith fewer samples. For example, when the training data\nvolume is 25%, compared with the model only equipped\nwith EPQM, the model with both EPQM and EFSA achieves\nan extra gain of 11.95%F-measure on Total-Text and 7.83%\nF-measure on Inverse-Text. Moreover, as shown in Fig. 5(a),\nEFSA can further promote the training convergence and the\nmodel with all components achieves about six times faster\nconvergence than the baseline in the initial training stage.\nWe find EFSA is more effective when predicting polygon\ncontrol points. Since Bezier curve control points do not al-\n3246\nMethod Prior Points Sampling\nPoint Update F\nBaseline\n83.90\n‚úì 84.13\n‚úì ‚úì 85.15\nTable 4: Quantitative analysis on EPQM. The results are test\non Total-Text without using positional label and EFSA.\n(1):Baseline\n(2): (1) w/ Pos.Label\n(4): (3) + EFSA\n(3): (2) + EPQM\n(1):Baseline\n(2): (1) + Prior Point Sampling\n(3): (2) + Prior Update\nFigure 5: Convergence curves on Rot.Total-Text (left) and\nTotal-Text (right).\nways form in circular shape and sometimes they are far\napart, it is not suitable to combine circular convolution with\nself-attention for the Bezier variant.\nIn summary, the positional label form mainly improves\nthe model robustness while EPQM and EFSA boost the\noverall performance, training convergence, and few-shot\nlearning ability. DPText-DETR achieves ideal trade-off be-\ntween performance gain and inference speed drop.\nWhat Makes Faster Training Convergence?\nWe conduct further ablation studies on EPQM to reveal what\nmakes convergence faster. Quantitative results and conver-\ngence curves are shown in Tab. 4 and Fig. 5(b). Referring\nto the blue curve in Fig. 5(b), the convergence at the ini-\ntial stage is improved when only Prior Points Sampling is\nused. Referring to the red curve and Tab. 4, Point Update fur-\nther boosts the convergence by a large margin and makes the\nmajor contribution to the performance. It demonstrates that\nthe explicit position modeling for sparse points is the key\nto faster convergence. The explicit formulation is the pre-\nrequisite for dynamically updating points in decoder layers.\nDynamic updating provides more precise reference points\nfor deformable cross-attention, resulting in better perfor-\nmance. Prior works (Liu et al. 2022; Wang et al. 2022) have\nproved that box query formulation and sparse box area fea-\ntures extracted by ROIAlign can improve the training effi-\nciency of DETR-based models. In our DPText-DETR de-\nsigned for scene text detection, the Prior Points Sampling\nscheme can be regarded as a soft grid-sample operation, and\nit is also proved that the point query formulation, which is\nmore sparse than the box, is more beneficial to training.\nFurther Discussion\nIn addition, we further investigate the performance of some\narbitrary-shape scene text spotters on Inverse-Text. Recent\nmethods can be roughly categorized into point-based and\nsegmentation-based methods. For point-based methods, we\nMethod Det-F End-to-End\nNone Full\nABCNet-v2 (Our repro.) 78.0 57.2\n69.5\nABCNet-v2 w/ Pos.Label (Our repro.) 87.2 62.2 76.7\nTESTR (Our repro.) 86.8 62.1 74.7\nTESTR w/ Pos.Label (Our repro.) 87.2 61.9 74.1\nTESTR w/ Pos.Label (Our detector) 87.3\n63.1 75.4\nSwinTextSpotter (Our\nrepro.) 89.3 62.9 74.7\nTable 5: Results of spotters on Inverse-Text. ‚Äúrepro.‚Äù and\n‚ÄúNone‚Äù indicates our experiment using official released code\nand the end-to-end results without using lexicon.\nFigure 6: Qualitative results on Inverse-Text. Recognition\nfailures on hard inverse-like texts are marked with red boxes.\nselect ABCNet-v2 (Liu et al. 2021a) and TESTR (Zhang\net al. 2022d) that exploits dual Transformer decoders for\nparallel detection and recognition. For segmentation-based\nmethods, we select SwinTextSpotter (Huang et al. 2022)\nas a representative. We finetune the official models trained\non Total-Text with rotation augmentation as mentioned in\nimplementation details for better adaptation to inverse-like\ntexts. Results are reported in Tab. 5. For ABCNet-v2 and\nTESTR, we also test the influence of the positional label\nform. As shown in Tab. 5, the detection F-measures are im-\nproved when the positional label form is used, which vali-\ndates the positive effect on detection.\nWe further replace the detection decoder of TESTR with\nours, and find that the modified spotter still works well. It in-\ndicates that the detection decoder can iteratively refine con-\ntrol points with explicit point information while the recog-\nnition decoder remains to learn semantics from a coarse\ntext anchor box sub-region. However, the modified spotter\nsuffers from unsynchronized convergence between detec-\ntion and recognition. We plan to explore a training efficient\nTransformer-based spotter in the future. Some visualizations\nare presented in Fig. 6.\nConclusion\nWe present a concise yet effective scene text detection trans-\nformer network, which transforms composite queries into\nexplicit and complete point formulation. We investigate the\neffect of control point labels on model robustness and point\nout a practical positional label form. Extensive experiments\ndemonstrate the state-of-the-art performance, training effi-\nciency, and robustness of our proposed DPText-DETR. We\nalso establish an Inverse-Text test set to facilitate future re-\nsearch in this area.\n3247\nAcknowledgements\nThis work was supported in part by the National Natu-\nral Science Foundation of China under Grants 62076186,\n62141112, and 62225113, and in part by the Science\nand Technology Major Project of Hubei Province (Next-\nGeneration AI Technologies) under Grant 2019AEA170. Dr.\nJing Zhang is supported by the ARC project FL-170100117.\nThe numerical calculations in this paper have been done on\nthe supercomputing system in the Supercomputing Center\nof Wuhan University.\nReferences\nBaek, Y .; Lee, B.; Han, D.; Yun, S.; and Lee, H. 2019. Char-\nacter region awareness for text detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 9365‚Äì9374.\nBaek, Y .; Shin, S.; Baek, J.; Park, S.; Lee, J.; Nam, D.;\nand Lee, H. 2020. Character region attention for text spot-\nting. In European Conference on Computer Vision, 504‚Äì\n521. Springer.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European conference on computer vi-\nsion, 213‚Äì229. Springer.\nChng, C. K.; Liu, Y .; Sun, Y .; Ng, C. C.; Luo, C.; Ni, Z.;\nFang, C.; Zhang, S.; Han, J.; Ding, E.; et al. 2019. IC-\nDAR2019 robust reading challenge on arbitrary-shaped text-\nRRC-ArT. In 2019 International Conference on Document\nAnalysis and Recognition (ICDAR), 1571‚Äì1576. IEEE.\nCh‚Äông, C.-K.; Chan, C. S.; and Liu, C.-L. 2020. Total-text:\ntoward orientation robustness in scene text detection. In-\nternational Journal on Document Analysis and Recognition\n(IJDAR), 23(1): 31‚Äì52.\nDai, P.; Zhang, S.; Zhang, H.; and Cao, X. 2021a. Progres-\nsive contour regression for arbitrary-shape scene text detec-\ntion. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, 7393‚Äì7402.\nDai, X.; Chen, Y .; Yang, J.; Zhang, P.; Yuan, L.; and Zhang,\nL. 2021b. Dynamic detr: End-to-end object detection with\ndynamic attention. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, 2988‚Äì2997.\nDong, Q.; Tu, Z.; Liao, H.; Zhang, Y .; Mahadevan, V .; and\nSoatto, S. 2021. Visual relationship detection using part-\nand-sum transformers with composite queries. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, 3550‚Äì3559.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An Image is Worth 16x16\nWords: Transformers for Image Recognition at Scale. In In-\nternational Conference on Learning Representations.\nDu, B.; Ye, J.; Zhang, J.; Liu, J.; and Tao, D. 2022.\nI3CL: Intra-and Inter-Instance Collaborative Learning for\nArbitrary-shaped Scene Text Detection. International Jour-\nnal of Computer Vision, 1‚Äì17.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\n770‚Äì778.\nHe, Y .; Chen, C.; Zhang, J.; Liu, J.; He, F.; Wang, C.; and\nDu, B. 2022. Visual semantics allow for textual reasoning\nbetter in scene text recognition. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 36, 888‚Äì896.\nHuang, M.; Liu, Y .; Peng, Z.; Liu, C.; Lin, D.; Zhu, S.; Yuan,\nN.; Ding, K.; and Jin, L. 2022. SwinTextSpotter: Scene\nText Spotting via Better Synergy between Text Detection\nand Text Recognition. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n4593‚Äì4603.\nLazarow, J.; Xu, W.; and Tu, Z. 2022. Instance Segmentation\nWith Mask-Supervised Polygonal Boundary Transformers.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 4382‚Äì4391.\nLiao, M.; Lyu, P.; He, M.; Yao, C.; Wu, W.; and Bai, X.\n2021. Mask TextSpotter: An End-to-End Trainable Neu-\nral Network for Spotting Text with Arbitrary Shapes. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n43(2): 532‚Äì548.\nLiao, M.; Pang, G.; Huang, J.; Hassner, T.; and Bai, X.\n2020a. Mask textspotter v3: Segmentation proposal network\nfor robust scene text spotting. In Computer Vision‚ÄìECCV\n2020: 16th European Conference, Glasgow, UK, August 23‚Äì\n28, 2020, Proceedings, Part XI 16, 706‚Äì722. Springer.\nLiao, M.; Wan, Z.; Yao, C.; Chen, K.; and Bai, X. 2020b.\nReal-time scene text detection with differentiable binariza-\ntion. In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, 11474‚Äì11481.\nLiu, J.; Chen, Z.; Du, B.; and Tao, D. 2020a. ASTS: A\nunified framework for arbitrary shape text spotting. IEEE\nTransactions on Image Processing, 29: 5924‚Äì5936.\nLiu, S.; Li, F.; Zhang, H.; Yang, X.; Qi, X.; Su, H.; Zhu, J.;\nand Zhang, L. 2022. DAB-DETR: Dynamic Anchor Boxes\nare Better Queries for DETR. In International Conference\non Learning Representations.\nLiu, Y .; Chen, H.; Shen, C.; He, T.; Jin, L.; and Wang, L.\n2020b. Abcnet: Real-time scene text spotting with adap-\ntive bezier-curve network. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n9809‚Äì9818.\nLiu, Y .; Jin, L.; Zhang, S.; Luo, C.; and Zhang, S. 2019.\nCurved scene text detection via transverse and longitudinal\nsequence connection. Pattern Recognition, 90: 337‚Äì345.\nLiu, Y .; Shen, C.; Jin, L.; He, T.; Chen, P.; Liu, C.; and Chen,\nH. 2021a. ABCNet v2: Adaptive Bezier-Curve Network for\nReal-time End-to-end Text Spotting. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 1‚Äì1.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021b. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 10012‚Äì10022.\n3248\nLong, S.; Ruan, J.; Zhang, W.; He, X.; Wu, W.; and Yao,\nC. 2018. Textsnake: A flexible representation for detecting\ntext of arbitrary shapes. In Proceedings of the European\nconference on computer vision (ECCV), 20‚Äì36.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\ncay Regularization. In ICLR.\nMeng, D.; Chen, X.; Fan, Z.; Zeng, G.; Li, H.; Yuan, Y .;\nSun, L.; and Wang, J. 2021. Conditional detr for fast training\nconvergence. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 3651‚Äì3660.\nNayef, N.; Patel, Y .; Busta, M.; Chowdhury, P. N.; Karatzas,\nD.; Khlif, W.; Matas, J.; Pal, U.; Burie, J.-C.; Liu, C.-l.;\net al. 2019. ICDAR2019 robust reading challenge on multi-\nlingual scene text detection and recognition‚ÄîRRC-MLT-\n2019. In 2019 International Conference on Document Anal-\nysis and Recognition (ICDAR), 1582‚Äì1587. IEEE.\nPeng, S.; Jiang, W.; Pi, H.; Li, X.; Bao, H.; and Zhou, X.\n2020. Deep snake for real-time instance segmentation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 8533‚Äì8542.\nQiao, L.; Chen, Y .; Cheng, Z.; Xu, Y .; Niu, Y .; Pu, S.; and\nWu, F. 2021. Mango: A mask attention guided one-stage\nscene text spotter. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 35, 2467‚Äì2476.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn:\nTowards real-time object detection with region proposal net-\nworks. Advances in Neural Information Processing Systems,\n28: 91‚Äì99.\nSingh, A.; Natarajan, V .; Shah, M.; Jiang, Y .; Chen, X.; Ba-\ntra, D.; Parikh, D.; and Rohrbach, M. 2019. Towards vqa\nmodels that can read. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, 8317‚Äì\n8326.\nSun, Y .; Ni, Z.; Chng, C.-K.; Liu, Y .; Luo, C.; Ng, C. C.;\nHan, J.; Ding, E.; Liu, J.; Karatzas, D.; et al. 2019. ICDAR\n2019 competition on large-scale street view text with partial\nlabeling-RRC-LSVT. In 2019 International Conference on\nDocument Analysis and Recognition (ICDAR), 1557‚Äì1562.\nIEEE.\nTang, J.; Zhang, W.; Liu, H.; Yang, M.; Jiang, B.; Hu, G.;\nand Bai, X. 2022. Few Could Be Better Than All: Feature\nSampling and Grouping for Scene Text Detection. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 4563‚Äì4572.\nTian, Z.; Shen, C.; Chen, H.; and He, T. 2019. Fcos: Fully\nconvolutional one-stage object detection. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n9627‚Äì9636.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in Neural Information\nProcessing Systems, 5998‚Äì6008.\nWang, W.; Cao, Y .; Zhang, J.; and Tao, D. 2021. Fp-detr:\nDetection transformer advanced by fully pre-training. In In-\nternational Conference on Learning Representations.\nWang, W.; Xie, E.; Song, X.; Zang, Y .; Wang, W.; Lu, T.;\nYu, G.; and Shen, C. 2019. Efficient and accurate arbitrary-\nshaped text detection with pixel aggregation network. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 8440‚Äì8449.\nWang, W.; Zhang, J.; Cao, Y .; Shen, Y .; and Tao, D. 2022.\nTowards data-efficient detection transformers. In European\nConference on Computer Vision, 88‚Äì105. Springer.\nXu, Y .; Zhang, Q.; Zhang, J.; and Tao, D. 2021. Vitae: Vi-\nsion transformer advanced by exploring intrinsic inductive\nbias. Advances in Neural Information Processing Systems,\n34: 28522‚Äì28535.\nYe, J.; Chen, Z.; Liu, J.; and Du, B. 2020. TextFuseNet:\nScene Text Detection with Richer Fused Features. In IJCAI,\nvolume 20, 516‚Äì522.\nZhang, J.; and Tao, D. 2020. Empowering things with in-\ntelligence: a survey of the progress, challenges, and oppor-\ntunities in artificial intelligence of things. IEEE Internet of\nThings Journal, 8(10): 7789‚Äì7817.\nZhang, P.; Xu, Y .; Cheng, Z.; Pu, S.; Lu, J.; Qiao, L.; Niu, Y .;\nand Wu, F. 2020. Trie: End-to-end text reading and informa-\ntion extraction for document understanding. In Proceedings\nof the 28th ACM International Conference on Multimedia ,\n1413‚Äì1422.\nZhang, Q.; Xu, Y .; Zhang, J.; and Tao, D. 2022a. Vi-\ntaev2: Vision transformer advanced by exploring inductive\nbias for image recognition and beyond. arXiv preprint\narXiv:2202.10108.\nZhang, Q.; Xu, Y .; Zhang, J.; and Tao, D. 2022b. VSA:\nLearning Varied-Size Window Attention in Vision Trans-\nformers. In European Conference on Computer Vision.\nSpringer.\nZhang, S.-X.; Zhu, X.; Yang, C.; Wang, H.; and Yin, X.-C.\n2021. Adaptive Boundary Proposal Network for Arbitrary\nShape Text Detection. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, 1305‚Äì1314.\nZhang, S.-X.; Zhu, X.; Yang, C.; and Yin, X.-C. 2022c.\nArbitrary Shape Text Detection via Boundary Transformer.\narXiv preprint arXiv:2205.05320.\nZhang, X.; Su, Y .; Tripathi, S.; and Tu, Z. 2022d. Text Spot-\nting Transformers. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 9519‚Äì\n9528.\nZhou, Y .; Xie, H.; Fang, S.; Wang, J.; Zha, Z.; and Zhang,\nY . 2021. TDI TextSpotter: Taking Data Imbalance into Ac-\ncount in Scene Text Spotting. In Proceedings of the 29th\nACM International Conference on Multimedia, 2510‚Äì2518.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J.\n2020. Deformable DETR: Deformable Transformers for\nEnd-to-End Object Detection. In International Conference\non Learning Representations.\nZhu, Y .; Chen, J.; Liang, L.; Kuang, Z.; Jin, L.; and Zhang,\nW. 2021. Fourier contour embedding for arbitrary-shaped\ntext detection. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 3123‚Äì3131.\n3249",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7679498195648193
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6947594881057739
    },
    {
      "name": "Transformer",
      "score": 0.5524588227272034
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44968172907829285
    },
    {
      "name": "Data mining",
      "score": 0.41305142641067505
    },
    {
      "name": "Voltage",
      "score": 0.10316023230552673
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210105595",
      "name": "Institute of Art",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I129604602",
      "name": "The University of Sydney",
      "country": "AU"
    }
  ]
}