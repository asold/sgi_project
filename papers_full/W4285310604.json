{
  "title": "Data Augmentation for Intent Classification with Off-the-shelf Large Language Models",
  "url": "https://openalex.org/W4285310604",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2185362157",
      "name": "Gaurav Sahu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2140106477",
      "name": "Pau Rodríguez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3173936353",
      "name": "Issam Laradji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3036172475",
      "name": "Parmida Atighehchian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127378106",
      "name": "David Vazquez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2509101809",
      "name": "Dzmitry Bahdanau",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2986193249",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3153451655",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2921312604",
    "https://openalex.org/W4205737716",
    "https://openalex.org/W4221149883",
    "https://openalex.org/W4287829148",
    "https://openalex.org/W4297683418",
    "https://openalex.org/W2998184481",
    "https://openalex.org/W2905266130",
    "https://openalex.org/W2985353426",
    "https://openalex.org/W3200140725",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3128710690",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3045492832"
  ],
  "abstract": "Data augmentation is a widely employed technique to alleviate the problem of data scarcity. In this work, we propose a prompting-based approach to generate labelled training data for intent classification with off-the-shelf language models (LMs) such as GPT-3. An advantage of this method is that no task-specific LM-fine-tuning for data generation is required; hence the method requires no hyper parameter tuning and is applicable even when the available training data is very scarce. We evaluate the proposed method in a few-shot setting on four diverse intent classification tasks. We find that GPT-generated data significantly boosts the performance of intent classifiers when intents in consideration are sufficiently distinct from each other. In tasks with semantically close intents, we observe that the generated data is less helpful. Our analysis shows that this is because GPT often generates utterances that belong to a closely-related intent instead of the desired one. We present preliminary evidence that a prompting-based GPT classifier could be helpful in filtering the generated data to enhance its quality.",
  "full_text": "Proceedings of the 4th Workshop on NLP for Conversational AI, pages 47 - 57\nMay 27, 2022 ©2022 Association for Computational Linguistics\nData Augmentation for Intent Classiﬁcation with Off-the-shelf Large\nLanguage Models\nGaurav Sahu∗\nUniversity of Waterloo\ngsahu@uwaterloo.ca\nPau Rodriguez\nServiceNow Research\nIssam H. Laradji\nServiceNow Research\nParmida Atighehchian\nServiceNow Research\nDavid Vazquez\nServiceNow Research\nDzmitry Bahdanau\nServiceNow Research\nAbstract\nData augmentation is a widely employed tech-\nnique to alleviate the problem of data scarcity.\nIn this work, we propose a prompting-based\napproach to generate labelled training data\nfor intent classiﬁcation with off-the-shelf lan-\nguage models (LMs) such as GPT-3. An ad-\nvantage of this method is that no task-speciﬁc\nLM-ﬁne-tuning for data generation is required;\nhence the method requires no hyper-parameter\ntuning and is applicable even when the avail-\nable training data is very scarce. We evaluate\nthe proposed method in a few-shot setting on\nfour diverse intent classiﬁcation tasks. We ﬁnd\nthat GPT-generated data signiﬁcantly boosts\nthe performance of intent classiﬁers when in-\ntents in consideration are sufﬁciently distinct\nfrom each other. In tasks with semantically\nclose intents, we observe that the generated\ndata is less helpful. Our analysis shows that\nthis is because GPT often generates utterances\nthat belong to a closely-related intent instead\nof the desired one. We present preliminary ev-\nidence that a prompting-based GPT classiﬁer\ncould be helpful in ﬁltering the generated data\nto enhance its quality.1\n1 Introduction\nA key challenge in creating task-oriented conver-\nsational agents is gathering and labelling training\ndata. Standard data gathering options include man-\nual authoring and crowd-sourcing. Unfortunately,\nboth of these options are tedious and expensive.\nData augmentation is a widely used strategy to\nalleviate this problem of data acquisition.\nThere are two particularly promising paradigms\nfor data augmentation in natural language process-\ning that use pretrained language models (LMs) (Pe-\nters et al., 2018; Devlin et al., 2018). The ﬁrst fam-\nily of methods ﬁne-tunes an LM on task-speciﬁc\n∗Work done during an internship at ServiceNow Research\n1Our code is available at: https://github.com/\nElementAI/data-augmentation-with-llms\nInput Prompt:\nThe following sentences belong to the same  \ncategory music_likeness:\nExample 1: i like soft rock music  \nExample 2: current song rating three stars  \nExample 3: save this song as a favorite  \nExample 4: remind me that i like that song  \nExample 5: save my opinion on the currently  \n           playing song  \nExample 6: i love the song do you  \nExample 7: add the song to my favorites  \nExample 8: store opinion on song  \nExample 9: the song in background is cool  \nExample 10: i am the living blues  \nExample 11:\nCompletions:\ni dislike classical music  \nshe is a music lover  \ni am a lover of painting  \nthis is the best song ever  \nvideo that looks like the other video  \nsave preference on my profile  \nexpress negative opinion on the song  \ni am a great blues follower  \nthe song is better than i thought  \nthis song is also fun  \nFigure 1: Generation Process. Given a seed intent\n(here, music_likeness) and K(=10) available examples\nfor that intent, we construct a prompt following the\nshown template. Note that the last line of the prompt\nis incomplete (there is no new line character.) We then\nfeed this prompt to a GPT-3 engine, which generates\nsome completions of the prompt. In this example, red\ntext denotes unfaithful examples and blue text\ndenotes faithful examples. Note: For brevity, we only\nshow ten generated sentences.\ndata and generates new examples using the ﬁne-\ntuned LM (Wu et al., 2018; Kumar et al., 2019,\n2021; Anaby-Tavor et al., 2020; Lee et al., 2021).\nA limitation of these methods is that, in a real-\nworld scenario, task-speciﬁc data is scarce and ﬁne-\ntuning an LM can quickly become the bottleneck.\nThe second family of methods sidesteps this bot-\n47\ntleneck by employing off-the-shelf pretrained LMs\nsuch as GPT-3 (Brown et al., 2020) to directly gen-\nerate text without any task-speciﬁc ﬁne-tuning. In\nparticular, data generation by the GPT3Mix ap-\nproach by Yoo et al. (2021) boosts performance\non multiple classiﬁcation tasks; however, they only\nconsider tasks with few (up to 6) classes and easy-\nto-grasp class boundaries (e.g., positive and nega-\ntive).\nThis work studies the applicability of massive\noff-the-shelf LMs, such as GPT-3 and GPT-J (Wang\nand Komatsuzaki, 2021) to perform effective data\naugmentation for intent classiﬁcation (IC) tasks. In\nIC, the end goal is to predict a user’s intent given an\nutterance, i.e., what the user of a task-oriented chat-\nbot wants to accomplish. Data augmentation for IC\nis particularly challenging because the generative\nmodel must distinguish between a large number\n(in practice up to several hundreds) of ﬁne-grained\nintents that can be semantically very close to each\nother. Prior methods such as GPT3Mix prompt\nthe model with the names of all classes as well\nas a few examples from randomly chosen classes.\nWe test GPT3Mix for one and observe that such\napproaches are poorly suitable for intent classiﬁca-\ntion tasks with tens or hundreds of possible intents.\nInstead, in this study, we use a simple prompt struc-\nture that focuses on a single seed intent (see Figure\n1) as it combines the intent’s name and all available\nexamples.\nOur experiments primarily focus on few-shot IC\non four prominent datasets: CLINC150 (Larson\net al., 2019), HWU64 (Xingkun Liu and Rieser,\n2019), Banking77 (Casanueva et al., 2020), and\nSNIPS (Coucke et al., 2018). We also consider a\npartial few-shot setup to compare to the Example\nExtrapolation (Ex2) approach by Lee et al. (2021)\nwho use a similar prompt but ﬁne-tune the LM in-\nstead of using it as is. The main ﬁndings of our ex-\nperiments are as follows: (1) GPT-generated sam-\nples boost classiﬁcation accuracy when the consid-\nered intents are well-distinguished from each other\n(like in CLINC150, SNIPS). (2) On more gran-\nular datasets (namely HWU64 and Banking77),\nwe ﬁnd that GPT struggles in distinguishing be-\ntween different confounding intents. (3) A small-\nscale study to further understand this behaviour\nsuggests that GPT could be used as a classiﬁer\nto ﬁlter out unfaithful examples and enhance the\nquality of the generated training set. Addition-\nally, we investigate how valuable the generated data\ncould be if relabelled by a human. Using an oracle\nmodel, we show that (4) the human labelling of\nGPT-generated examples can further improve the\nperformance of intent classiﬁers, and that (5) LM-\ngenerated data has a higher relabelling potential\ncompared to edit-based augmentation techniques,\nsuch as Easy Data Augmentation (EDA) (Wei and\nZou, 2019).\n2 Method\nWe consider training an intent classiﬁer, where an\nintent is a type of request that the conversational\nagent supports; e.g. the user may want to change\nthe language of the conversation, play a song, trans-\nfer money between accounts, etc. However, collect-\ning many example utterances that express the same\nintent is difﬁcult and expensive. Therefore, this\npaper experiments with a straightforward method\nto augment the training data available for an intent:\ncreating prompts from the available examples and\nfeeding them to a large language model such as\nGPT-3 (Brown et al., 2020). Figure 1 illustrates\nthe process of data generation for an intent with K\navailable examples.\n3 Experimental Setup\n3.1 Datasets\nWe use four intent classiﬁcation datasets in our\nexperiments with varying levels of granularity\namong intents. CLINC150 (Larson et al., 2019),\nHWU64 (Xingkun Liu and Rieser, 2019) are multi-\ndomain datasets, each covering a wide range of\ntypical task-oriented chatbot domains, such as play-\ning music and setting up alarms. Importantly,\nthe CLINC150 task also contains examples of\nout-of-scope (OOS) utterances that do not cor-\nrespond to any of CLINC’s 150 intents. Bank-\ning77 (Casanueva et al., 2020) is a single domain\ndataset with very ﬁne-grained banking-related in-\ntents. Finally, the SNIPS (Coucke et al., 2018)\ndataset contains 7 intents typical for the smart\nspeaker usecase. We refer the reader to Table 1\nfor exact statistics of all used datasets.\n3.2 Setup\nThe main data-scarce setup that we consider in this\nwork is the few-shot setup, where only K = 10\ntraining examples are available for every intent\nof interest. Additionally, to compare to example\nextrapolation with ﬁne-tuned language models as\nproposed by Lee et al. (2021), we consider apartial\n48\nCLINC150 SNIPS HWU64 Banking77\ndomains 10 1 18 1\nintents 150 7 64 77\ntrain\nexamples\n15000 13084 8954* 9002*\n(100)\nval.\nexamples\n3000 700 1076* 1001*\n(100)\ntest\nexamples\n4500 700 1076 3080\n(1000)\nTable 1: Statistics of the intent classiﬁcation datasets\nthat we use in our experiments. * indicates that we\nsplit the original data into training and validation in-\nstead of using a split provided by the dataset authors.\nFor CLINC150, the number of out-of-scope examples\nin different data partitions is given in parenthesis.\nfew-shot setup. In the latter setting, we limit the\namount of training data only for a handful of few-\nshot intents2 and use the full training data for others.\nWhen data augmentation is performed, we augment\nthe few-shot intents to have N examples, where N\nis the median number of examples per intent of the\noriginal data.\nTo precisely describe the training and test data\nin all settings, we will use Dpart to refer to dataset\nparts, i.e. train, validation, and test. In addition,\nwe use DF and DM to refer to data-scarce and\ndata-rich intents (the latter only occur in the partial\nfew-shot setting). This notation is deﬁned for all\nparts, therefore, Dpart = D{F,part}∪D{M,part},\n∀part ∈ {train, val, test}. When GPT-3 or a\nbaseline method is used to augment the training\ndata we generate N −K examples per intent and\nrefer to the resulting data as ˜DF,train. We experi-\nment with four different-sized GPT-3 models3 by\nOpenAI and GPT-J by EleutherAI 4 to obtain ˜D.\nThe four GPT-3 models are: Ada, Babbage, Curie,\nand Davinci. In order, Ada is the smallest model\nand Davinci is the largest. Model sizes of GPT-3\nengines are not known precisely but are estimated\nby Eleuther AI to be between 300M and 175B\nparameters5.\n2We use the truncation heuristic provided by Lee et al.\n(2021): https://github.com/google/example_\nextrapolation/blob/master/preprocess_\nclinc150.py\n3https://beta.openai.com/docs/engines\n4https://github.com/kingoflolz/\nmesh-transformer-jax/\n5https://blog.eleuther.ai/\ngpt3-model-sizes/\n3.3 Training and Evaluation\nWe ﬁne-tune BERT-large (Devlin et al., 2018) on\nthe task of intent classiﬁcation by adding a linear\nlayer on top of the[CLS] token (Wolf et al., 2019).\nIn all setups we use the original validation set for\ntuning the classiﬁer’s training hyperparameters. We\nchose to use the full validation set as opposed to a\nfew-shot one to avoid issues with unstable hyperpa-\nrameter tuning and focus on assessing the quality\nof the generated data.\nFull few-shot. In this setup, we treat all the in-\ntents as few-shot and evaluate our method on the\nfollowing three scenarios: (i) Baseline: all the\nintents are truncated to K = 10 samples per in-\ntent, (ii) Augmented: ˜D{F,train}is generated us-\ning GPT and models are trained on D{F,train}∪\n˜D{F,train}and (iii) EDA-baseline: same as above,\nbut ˜D{F,train}is generated using Easy Data Aug-\nmentation (EDA) by Wei and Zou (2019). For each\nscenario, we report the 1) overall in-scope accuracy\non the complete test set Dtest, i.e. intent classiﬁca-\ntion accuracy excluding OOS samples in the test\nset, and 2) few-shot classiﬁcation accuracy of the\nmodels on D{F,test}. For CLINC150, we also re-\nport out-of-scope recall (OOS recall) on Dtest that\nwe compute as the percentage of OOS examples\nthat the model correctly labelled as such.\nThe purpose of this setting is to estimate what\nfurther gains can be achieved if the data generated\nby GPT were labelled by a human. We train an\noracle Oon the full training data Dtrain. We also\nuse Oto assess the quality of the generated data.\nNamely, we compute ﬁdelity of the generated data\nas the ratio of generated utterances that the oracle\nlabels as indeed belonging to the intended seed\nintent. A higher ﬁdelity value means that the gen-\nerated samples are more faithful to original data\ndistribution.\nPartial few-shot. In this setup, we train Sintent\nclassiﬁers, choosing different few-shot intents ev-\nery time to obtain DF . We then average the metrics\nacross these Sruns. For CLINC150, S= 10cor-\nresponding to the 10 different domains, whereas\nfor SNIPS, S= 7corresponding to the 7 different\nintents. We evaluate our method on the follow-\ning three scenarios introduced by Lee et al. (2021):\n(i) Baseline: models are trained without data aug-\nmentation on D{F,train}∪D{M,train}. (ii) Upsam-\npled: D{F,train}is upsampled to have N examples\nper intent. Then models are trained on upsampled\n49\n0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0\nT emp.\n86\n88\n90\n92\n94Few-shot Accuracy\nada\nbabbage\ncurie\ndavinci\n(a) Temperature v/s Few-shot accuracy\n0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0\nT emp.\n42\n44\n46\n48\n50\n52OOS Recall\nada\nbabbage\ncurie\ndavinci\n(b) Temperature v/s OOS recall\nFidelity\n(c) Temperature v/s Fidelity\nFigure 2: Partial few-shot validation performance\nfor different GPT-3 models and temperatures. (a)\nfew-shot accuracy, (b) OOS recall of intent classiﬁers\ntrained on augmented sets, and (c) ﬁdelity measured as\nthe accuracy of the oracle on the augmented sets.\nD{F,train}∪D{M,train}. (iii) Augmented: models\nare trained on D{F,train}∪˜D{F,train}∪D{M,train}.\nFor each scenario in this setup, we report the overall\nin-scope classiﬁcation accuracy (and OOS Recall\nfor CLINC150).\nFor both partial few-shot and full few-shot set-\ntings, we report means and standard deviations over\n10 repetitions of each experiment.\n4 Experimental Results\nFull few-shot. Table 2 shows the results of our\nfew-shot experiments. For CLINC150 and SNIPS,\ndata augmentation with GPT-3 is very effective as it\nleads to respective accuracy improvements of up to\napproximately 3.7% and 6% on these tasks. These\nimprovements are larger than what the baseline\nEDA method brings, namely 2.4% and 2.9% for\n0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3\nT emp.\n60\n70\n80\n90\n100Few-shot Inscope Accuracy\nclinc_oos\nsnips_official\nbanking77\nhwu64\n(a) Temperature v/s Few-shot inscope accuracy\n0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3\nT emp.\n30\n40\n50\n60\n70\n80\n90\n100Fidelity\nclinc_oos\nsnips_official\nbanking77\nhwu64\n(b) Temperature v/s Fidelity\nFigure 3: Full few-shot validation performance for\ndifferent GPT-J temperatures on different datasets.\n(a) few-shot inscope accuracy of intent classiﬁers\ntrained on augmented sets, and (b) ﬁdelity (oracle accu-\nracy) of augmented sets generated by GPT-J with dif-\nferent temperatures.\nCLINC150 and SNIPS. Importantly, using larger\nGPT models for data augmentation brings signiﬁ-\ncantly bigger gains. Data augmentation results on\nBanking77 and HWU64 are, however, much worse,\nwith no or little improvement upon the plain few-\nshot baseline. We present a thorough investigation\nof this behaviour in Section 4.1. One can also see\nthat data augmentation with GPT models lowers\nthe OOS recall.\nNext, we observe that relabelling EDA and GPT-\ngenerated sentences by the oracle gives a signif-\nicant boost to accuracies across the board, con-\nﬁrming our hypothesis that human inspection of\ngenerated data could be fruitful. Importantly, we\nnote that the magnitude of improvement for EDA\nis less than for GPT models. This suggests that\nGPT models generate more diverse data that can\neventually be more useful after careful human in-\nspection. Lastly, relabelling also improves OOS\nrecall on CLINC150, which is due to the fact that\nmuch of the generated data was labelled as OOS\nby the oracle.\nPartial few-shot. Table 3 shows the results of\nour partial few-shot experiments on CLINC150\nand SNIPS. By augmenting the dataset with GPT-\n50\nCLINC150 HWU64 Banking77 SNIPS\nModel IA (96.93) OR (42.9) IA (92.75) IA (93.73) IA (98.57)\nEDA 92.66 (0.40) 43.81 (2.03) 83.67 (0.48) 83.96 (0.66) 92.50 (1.61)\nBaseline (Ours) 90.28(0.49) 50.18(1.14) 81.43 (0.57) 83.35 (0.59) 89.69 (1.63)\nAugmented\nAda (Ours) 91.31 (0.34) 21.69 (1.57) 79.68 (0.83) 79.30 (0.42) 94.27 (0.52)\nBabbage (Ours) 92.72 (0.33) 22.99 (2.39) 81.86 (0.78) 80.31 (0.41) 94.74 (0.67)\nCurie (Ours) 93.37 (0.21) 25.85 (1.49) 82.85 (0.70) 83.50 (0.44) 94.73 (0.62)\nGPT-J (Ours) 93.25 (0.19) 24.02 (1.45) 81.78 (0.56) 82.32 (0.90) 95.19 (0.61)\nDavinci (Ours) 94.07 (0.18) 27.36 (1.08) 82.79 (0.93) 83.60 (0.45) 95.77 (0.86)\nAugmented + Relabelled\nEDA 93.43 (0.22) 48.56 (1.84) 85.58 (0.73) 84.82 (0.57) 94.91 (0.66)\nAda (Ours) 95.09 (0.16) 41.38 (1.77) 88.53 (0.61) 88.45 (0.19) 97.03 (0.18)\nBabbage (Ours) 95.39 (0.17) 40.58 (1.63) 89.49 (0.32) 88.86 (0.26) 96.89 (0.49)\nCurie (Ours) 95.08 (0.19) 40.09 (2.38) 89.78 (0.47) 88.30 (4.64) 96.86 (0.31)\nGPT-J (Ours) 95.11 (0.13) 43.94 (1.76) 89.52 (0.54) 88.94 (0.40) 97.33 (0.38)\nDavinci (Ours) 95.08 (0.13) 40.76 (1.37) 89.53 (0.45) 88.89 (0.31) 97.03 (0.38)\nTable 2: Full few-shot results on CLINC150, HWU64, Banking77, and SNIPS datasets. IA: Inscope Accuracy\n(mean (std)). OR: OOS-Recall (mean (std)). Towards the top of the table, we also report the test set performance\n(enclosed in parentheses) when all examples are used for ﬁne-tuning (without any augmentation.)\nCLINC150 SNIPS\nOverall Few-shot Overall Few-shot\nClassiﬁer IA OR A IA A\nBaseline♠ T5 97.4 - 93.7 95.2 74.0\nUpsampled♠ T5 97.4 - 94.4 95.9 80.0\nAugmented (Ex2)♠ T5 97.4 - 95.6 97.8 94.0\nBaseline (ours) BERT 96.28 (0.06) 39.14 (0.82) 91.36 (0.47) 95.47 (0.45) 78.38 (3.34)\nUpsample (ours) BERT 96.20 (0.05) 40.21 (0.59) 90.93 (0.19) 95.29 (0.37) 79.28 (2.05)\nAugmented (Ada) BERT 96.16 (0.05) 34.37 (0.27) 92.60 (0.15) 97.30 (0.24) 94.41 (0.72)\nAugmented (Babbage) BERT 96.39 (0.06) 35.71 (0.46) 93.66 (0.21) 97.46 (0.25) 95.31 (0.74)\nAugmented (Curie) BERT 96.41 (0.06) 36.77 (0.93) 93.90 (0.21) 97.37 (0.19) 94.79 (0.64)\nAugmented (GPT-J) BERT 96.38 (0.05) 35.91 (0.94) 93.85 (0.25) 97.59 (0.21) 96.08 (0.39)\nAugmented (Davinci) BERT 96.45 (0.03) 37.52 (0.54) 94.28 (0.24) 97.66 (0.21) 96.52 (0.35)\nTable 3: Partial few-shot results on CLINC150 and SNIPS datasets. Refer to Section 3.3 for more details. IA:\nInscope accuracy (mean (std)). OR: OOS Recall (mean (std)). A: Accuracy (mean (std)). ♠ (Lee et al., 2021).\ngenerated samples, the few-shot accuracy improves\nby up to 2.92% on CLINC150 and 18.14% on\nSNIPS compared to the baseline setting. Our\nmethod achieves competitive results compared to\nEx2 (Lee et al., 2021), both in terms of absolute\naccuracies and the relative gains brought by data\naugmentation. Note that Ex2 uses T5-XL (Roberts\net al., 2020) with nearly 3 billion parameters as\nits base intent classiﬁer, while our method uses\nBERT-large with only 340 million parameters.\n4.1 Analysis\nEffect of GPT sampling temperature. We inves-\ntigate the impact of generation temperature on the\nquality and ﬁdelity of generated data. We perform\nthis investigation on the CLINC150 dataset using\nthe partial few-shot setup. Results in Figure 2 show\nthat, for all engines, the generated data leads to the\n51\nDavinci generated sentences Seed Intent Oracle Prediction\nHWU64\nplay a song with the word honey music_likeness play_music\nyou are playing music music_likeness play_music\n‘let me hear some of that jazz!’ music_likeness play_music\ni really like myspace music play_music music_likeness\ni love the start lucky country music play_music music_likeness\nthank you for the music play_music music_likeness\nplease play the next song music_settings play_music\nplay background music music_settings play_music\nplay the hour long loop of rock song music_settings play_music\nneed you to play that song one more time play_music music_settings\nskip that song, its turkish play_music music_settings\npickup the beat or a temp track or audio plugin play_music music_settings\nBanking77\nMy last attempt to top up didn’t seem to work , any success? topping_up_by_card top_up_failed\nI tried to top off my wallet using my card but it says\n“top up failed\".\ntopping_up_by_card top_up_failed\nI cannot top-up by my cellular phone number? How do I do\nthat?\ntopping_up_by_card top_up_failed\nCan you transfer money to my Ola prepaid option? Or help\nme top up my card to money . They never accept my card so I\nalways have to suffer\ntop_up_failed topping_up_by_card\nHi my app is activated on activate.co.in, but unable to top up my\nphone. I tried credit card, debit card and Paytm but fails\ntop_up_failed topping_up_by_card\nI try to top up my card but it’s not going through. It’s\nstill on pending status. Do I need to wait or did I do something\nwrong\ntop_up_failed pending_top_up\nI tried top-up with my card but notiﬁcation\nshows that ‘Pending’. This has been happening since\nlast night. Can you tell me what’s going on\ntop_up_failed pending_top_up\nTop up didn’t go through. pending_top_up top_up_failed\nDid my master card top-up fail? pending_top_up top_up_failed\nTable 4: Davinci-generated sentences for closely-related intents in HWU64 and Banking77 datasets. High-\nlighted sub-strings indicate a difference with respect to the seed intent.\nhighest classiﬁcation accuracy when the generation\ntemperature is around 1.0, although lower tempera-\ntures result in higher OOS recall. We also observe\nthat the ﬁdelity of the generated samples decreases\nas we increase the temperature (i.e. higher diversity,\nsee Figure 2c). This suggests that higher ﬁdelity\ndoes not always imply better quality samples as the\nlanguage model may simply copy or produce less\ndiverse utterances at lower temperatures. In Ap-\npendix A, we perform a human evaluation, reach-\ning similar conclusions as when using an oracle to\napproximate ﬁdelity.\nFidelity on different datasets. Our results in Sec-\ntion 4 show that data augmentation gains are much\nhigher on CLINC150 and SNIPS than on HWU64\nand Banking77. To contextualize these results, we\nreport the ﬁdelity of GPT-J-generated data for all\nthese tasks in Figure 3b. Across all generation tem-\nperatures, the ﬁdelity of the generated data is higher\nfor CLINC150 and SNIPS than for HWU64 and\nBanking77. For all datasets, the ﬁdelity is higher\nwhen the generation temperature is lower; however,\nFigure 3a shows that low-temperature data also\ndoes improve the model’s performance.\nData generation for close intents. To better\nunderstand the lower ﬁdelity and accuracy on\nHWU64 and Banking77 datasets, we focus on in-\ntents with the lowest ﬁdelities. Here, by intent\nﬁdelity, we mean the percentage of the intent’s gen-\nerated data that the oracle classiﬁed as indeed be-\nlonging to the seed intent. In the Banking77 dataset,\nthe lowest-ﬁdelity intent is “topping_up_by_card.”\nFor this intent, only 33% of the Davinci-generated\nsentences were labelled as “topping_up_by_card”\n52\nFidelity (3 intents) HWU64 Banking77\nw/o ﬁltering (468) 60.26 57.31\nw/ ﬁltering (371) 72.51 65.54\n3-way accuracy\nDavinci 86.36 78.75\n10-shot BERT-large 82.95 65.54\nFull data BERT-large 94.32 95.00\nTable 5: The impact and the accuracy of using GPT-3 as\na 3-way classiﬁer on close intent triplets from HWU64\nand Banking77 datasets. For ﬁdelity, generated exam-\nples are rejected if the GPT-3 classiﬁer labels them as\nnot belonging to the seed intent. Classiﬁcation accu-\nracies are reported on the reduced validation+test sets\nwhere we only consider examples from the three con-\nfounding intents.\nBanking77\ntopping_up_by_card\ntop_up_failed\ntop_up_by_card_charge\nremaining\nHWU64\nmusic_likeness\nplay_music\ngeneral_quirky\nremaining\nFigure 4: Distribution of labels as predicted by the\noracle for lowest-ﬁdelity intents in Banking77 and\nHWU64 datasets (“topping_up_by_card\" and “mu-\nsic_likeness,\" respectively). Green areas denote the\nportion of generated sentences deemed ﬁt by the ora-\ncle for the lowest-ﬁdelity intents in the two datasets.\nRed and Blue areas respectively correspond to the most\ncommon and the second most common alternative in-\ntent predicted by the oracle.\nby the oracle, implying that two-thirds of the\nsentences did not ﬁt that intent, “top_up_failed\"\nand “top_up_card_charge\" being the two most\ncommon alternatives chosen by the oracle. Simi-\nlarly, only 50% of the Davinci-generated sentences\nabide by the lowest-ﬁdelity “music_likeness\" in-\ntent in the HWU64 dataset, “play_music” and\n“general_quirky\" being the most common intents\namong the “unfaithful\" sentences. Figure 4 visu-\nalizes this high percentage of unfaithful generated\nsentences. It also shows the proportion of the two\nmost common alternatives that the oracle preferred\nover the seed intent. Table 4 presents generated\nsentences for confounding intents in the HWU64\nand Banking77 datasets. There are clear indica-\ntions of mix-up of intents, e.g., Davinci generates,\n“play a song with the word honey,\" which should be-\nlong to “play_music\" rather than “music_likeness.\"\nThere are also instances where the LM mixes two\nintents; for instance, Davinci generates “Hi my\napp is activated on activate.co.in, but unable to\ntop up my phone. I tried credit card, debit card\nand Paytm but fails,\" which could belong to ei-\nther “topping_up_by_card\" intent (as it mentions\nabout using credit card in the context of a top up)\nor “top_up_failed\" (as the top up ultimately fails).\n4.2 Can GPT Models Understand Close\nIntents?\nWe perform extra investigations to better under-\nstand what limits GPT-3’s ability to generate data\naccurately. We hypothesize that one limiting factor\ncan be GPT-3’s inability to understand ﬁne-grained\ndifferences in the meanings of utterances. To verify\nthis hypothesis, we evaluate how accurate GPT-3 is\nat classifying given utterances as opposed to gener-\nating new ones. Due to the limited prompt size of\n2048 tokens, we can not prompt GPT-3 to predict\nall the intents in the considered datasets. We thus\nfocus on the close intent triplets from HWU64 and\nBanking77 datasets that we use in Table 4. We\ncompare the 3-way accuracy of a prompted GPT-3\nclassiﬁer to the similarly-measured 3-way perfor-\nmance of conventional BERT-large classiﬁers. We\nprompt GPT-3 with 10 examples per intent (see\nFigure 5). For comparison, we train BERT-large\nclassiﬁers on either the same 10 examples or the\nfull training set. Table 5 shows that the Davinci ver-\nsion of GPT-3 performs in between the 10-shot and\nthe full-data conventional classiﬁers. This suggests\nthat while GPT-3’s understanding of nuanced intent\ndifferences is imperfect, it could still be sufﬁcient\nto improve the performance of the downstream\nfew-shot model. Inspired by this ﬁnding, we exper-\niment with using GPT-3’s classiﬁcation abilities to\nimprove the quality of generated data. Namely, we\n53\nreject the generated utterances that GPT-3 classi-\nﬁes as not belonging to the seed intent. For both\nHWU64 and Banking77, this ﬁltering method sig-\nniﬁcantly improves the ﬁdelity of the generated\ndata for the chosen close intent triplets.\n4.3 Comparison with GPT3Mix\nTo test our initial hypothesis that prior methods\nsuch as GPT3Mix are not suitable for intent clas-\nsiﬁcation, we experiment with the said method on\nthe CLINC150 dataset using Curie. Speciﬁcally,\nwe include an enumeration of the 150 intent names\nin the prompt and randomly select one example for\nK intents. We observe a poor in-scope accuracy of\n86.33% in the Augmented scenario6. Furthermore,\nthe generated samples have low ﬁdelity (27.96%).\nWe also test a mixture of GPT3Mix prompt and\nour prompt where we include all the K examples\nfor the seed intent instead of 1 example per K ran-\ndomly sampled intents. This mixed variant also\nperforms poorly on CLINC150 and only achieves\nan in-scope accuracy of 86.05% 7 and a ﬁdelity\nof 33.56%. Our interpretation of this result is that\nGPT cannot handle the long list of 150 intent names\nin the prompt.\n5 Related Work\nThe natural language processing literature features\ndiverse data augmentation methods. Edit-based\nmethods such as Easy Data Augmentation apply\nrule-based changes to the original utterances to\nproduce new ones (Wei and Zou, 2019). In back-\ntranslation methods (Sennrich et al., 2016) avail-\nable examples are translated to another language\nand back. Recently, data augmentation with ﬁne-\ntuned LMs has become the dominant paradigm\n(Wu et al., 2018; Kumar et al., 2019, 2021; Anaby-\nTavor et al., 2020; Lee et al., 2021). Our simpler\nmethod sidesteps LM-ﬁne-tuning and directly uses\noff-she-shelf LMs as is.\nThe data augmentation approach that is closest\nto the one we use here is GPT3Mix by Yoo et al.\n(2021). A key part of the GPT3Mix prompt is a\nlist of names of all possible classes (e.g. “The\nsentiment is one of ‘positive’ or ‘negative”’). The\nLM is then expected to pick a random class from\nthe list and generate a new example as well as the\ncorresponding label. However, this approach does\nnot scale to intent classiﬁcation setups, which often\n6Average of 10 runs with a standard deviation of 1.17\n7Average of 10 runs with a standard deviation of 0.59\nInput Prompt:\nEach example in the following list contains\na sentence that belongs to a category. A\ncategory is one of the following:\nmusic_likeness, play_music, music_settings:  \nsentence: next i want to hear shinedown ;  \ncategory: play_music\nsentence: i am the living blues ;  \ncategory: music_likeness  \nsentence: open music player settings ;  \ncategory: music_settings\nsentence: play hopsin from my latest  \nplaylist ; category: play_music  \nsentence: i like this song ;  \ncategory:  \nGPT-3 Predictions:  \nplay_music, music_likeness, music_settings,  \nmusic_likeness,music_likeness ,help_command\nFigure 5: Using GPT-3 as a classiﬁer. Given a triplet\nof close intents, we mix and shufﬂe the multiple seed\nexamples available for each of them. Then, we append\nan incomplete line to the prompt with just the generated\nsentence and feed it to GPT-3 multiple times. Among\nthe responses, we choose the most generated in-triplet\nintent as the predicted intent (“music_likeness\" in the\nabove example). Note: For brevity, we don’t show all\nthe seed examples and predictions.\nfeature hundreds of intents (see Section 4.3). There-\nfore, we choose a different prompt that encourages\nthe model to extrapolate between examples of a\nseed intent similarly to (Lee et al., 2021).\nOther work on few-shot intent classiﬁcation ex-\nplores ﬁne-tuning dialogue-speciﬁc LMs as clas-\nsiﬁers as well as using similarity-based classi-\nﬁers instead of MLP-based ones on top of BERT\n(Vuli´c et al., 2021). We believe that improvements\nbrought by data augmentation would be comple-\nmentary to the gains brought by these methods.\nLastly, our method to ﬁlter out unfaithful GPT\ngenerations is related to the recent work by Wang\net al. (2021) that proposes using GPT3 for data\nlabelling. A crucial difference with respect to our\nwork, however, is that we use GPT-3 for rejecting\nmislabelled samples rather than proposing labels\nfor unlabelled samples.\n6 Conclusion\nWe propose a prompt-based method to generate\nintent classiﬁcation data with large pretrained lan-\n54\nguage models. Our experiments show that gener-\nated data can be helpful as additional labelled data\nfor some tasks, whereas, for other tasks, the gen-\nerated data needs to be either relabelled or ﬁltered\nto be helpful. We show that a ﬁltering method that\nrecasts the same GPT model as a classiﬁer can be\neffective. Our ﬁltering method, however, requires\nknowing the other intents that the generated data is\nlikely to belong to instead of the seed intent. Future\nwork can experiment with heuristics for approxi-\nmately identifying the most likely actual intents\nfor the generated utterances. This would complete\na data generation and ﬁltering pipeline that, ac-\ncording to our preliminary results in Section 4.2\nhere, could be effective. Other ﬁltering methods\ncould also be applied, such as looking at the like-\nlihood of the generated utterances as explored in\na concurrent work by Meng et al. (2022). Lastly,\nan interesting future work direction is identifying\nwhich generated utterances most likely need a hu-\nman inspection.\n7 Ethical Considerations\nAs discussed for the GPT3Mix method in Yoo\net al. (2021), using large language models for data\naugmentation presents several challenges: they ex-\nhibit social biases and are prone to generating toxic\ncontent. Therefore, samples generated using our\nprompting-based approach need to be considered\ncarefully.\nTo address such ethical concerns, human inspec-\ntion would be the most reliable way to identify\nand ﬁlter out problematic generations. The prac-\ntitoners who apply our method may also consider\ndebiasing the language model before using it for\ngeneration (Schick and Schütze, 2021).\nReferences\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, Naama\nTepper, and Naama Zwerdling. 2020. Do Not Have\nEnough Data? Deep Learning to the Rescue! Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, 34(05):7383–7390.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Ben-\njamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and\nDario Amodei. 2020. Language Models are Few-\nShot Learners. arXiv:2005.14165 [cs] . ArXiv:\n2005.14165.\nIñigo Casanueva, Tadas Tem ˇcinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vuli´c. 2020. Efﬁcient\nintent detection with dual sentence encoders. In Pro-\nceedings of the 2nd Workshop on Natural Language\nProcessing for Conversational AI, pages 38–45, On-\nline. Association for Computational Linguistics.\nAlice Coucke, Alaa Saade, Adrien Ball, Théodore\nBluche, Alexandre Caulier, David Leroy, Clément\nDoumouro, Thibault Gisselbrecht, Francesco Calt-\nagirone, Thibaut Lavril, Maël Primet, and Joseph\nDureau. 2018. Snips V oice Platform: an embedded\nSpoken Language Understanding system for private-\nby-design voice interfaces. arXiv:1805.10190 [cs].\nArXiv: 1805.10190.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics, NAACL 2019.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2021. Data Augmentation using Pre-trained Trans-\nformer Models. arXiv:2003.02245 [cs] . ArXiv:\n2003.02245.\nVarun Kumar, Hadrien Glaude, Cyprien de Lichy, and\nWlliam Campbell. 2019. A Closer Look At Feature\nSpace Data Augmentation For Few-Shot Intent Clas-\nsiﬁcation. In Proceedings of the 2nd Workshop on\nDeep Learning Approaches for Low-Resource NLP\n(DeepLo 2019), pages 1–10, Hong Kong, China. As-\nsociation for Computational Linguistics.\nStefan Larson, Anish Mahendran, Joseph J. Peper,\nChristopher Clarke, Andrew Lee, Parker Hill,\nJonathan K. Kummerfeld, Kevin Leach, Michael A.\nLaurenzano, Lingjia Tang, and Jason Mars. 2019.\nAn Evaluation Dataset for Intent Classiﬁcation and\nOut-of-Scope Prediction. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1311–1316, Hong Kong,\nChina. Association for Computational Linguistics.\nKenton Lee, Kelvin Guu, Luheng He, Tim Dozat, and\nHyung Won Chung. 2021. Neural Data Augmenta-\ntion via Example Extrapolation. arXiv:2102.01335\n[cs]. ArXiv: 2102.01335.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language\nmodels: Towards zero-shot language understanding.\narXiv preprint arXiv:2202.04538.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\n55\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics, NAACL-HLT 2018 .\nArXiv: 1802.05365.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\nCloze Questions for Few Shot Text Classiﬁcation\nand Natural Language Inference. arXiv:2001.07676\n[cs]. ArXiv: 2001.07676.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving Neural Machine Translation Mod-\nels with Monolingual Data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n86–96, Berlin, Germany. Association for Computa-\ntional Linguistics.\nIvan Vuli´c, Pei-Hao Su, Samuel Coope, Daniela Gerz,\nPaweł Budzianowski, Iñigo Casanueva, Nikola\nMrkši´c, and Tsung-Hsien Wen. 2021. ConvFiT:\nConversational ﬁne-tuning of pretrained language\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1151–1168, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 Billion Parameter Autoregressive\nLanguage Model. https://github.com/\nkingoflolz/mesh-transformer-jax.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want To Reduce La-\nbeling Cost? GPT-3 Can Help. arXiv:2108.13487\n[cs]. ArXiv: 2108.13487.\nJason Wei and Kai Zou. 2019. EDA: Easy Data Aug-\nmentation Techniques for Boosting Performance on\nText Classiﬁcation Tasks. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 6382–6388, Hong Kong,\nChina. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Fun-\ntowicz, et al. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. arXiv\npreprint arXiv:1910.03771.\nXing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,\nand Songlin Hu. 2018. Conditional BERT Contex-\ntual Augmentation. arXiv:1812.06705 [cs]. ArXiv:\n1812.06705.\nPawel Swietojanski Xingkun Liu, Arash Eshghi and\nVerena Rieser. 2019. Benchmarking natural lan-\nguage understanding services for building conversa-\ntional agents. In Proceedings of the Tenth Interna-\ntional Workshop on Spoken Dialogue Systems Tech-\nnology (IWSDS), pages xxx–xxx, Ortigia, Siracusa\n(SR), Italy. Springer.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-\nWoo Lee, and Woomyoung Park. 2021. GPT3Mix:\nLeveraging large-scale language models for text aug-\nmentation. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021 , pages 2225–\n2239, Punta Cana, Dominican Republic. Associa-\ntion for Computational Linguistics.\n56\nAppendix\nA Human Evaluation\nIn Figure 2 we evaluate the ﬁdelity of the samples\ngenerated by GPT-3 with respect to the original\nset of sentences used to prompt it. Fidelity is ap-\nproximated by the classiﬁcation performance of\nan \"oracle\" intent classiﬁer trained on the whole\ndataset (Dtrain ∪Dtest) and evaluated over the gen-\nerated samples. In order assess whether the oracle\npredictions are comparable to those of a human, we\nperform a human evaluation study.\n0.8 1.0 0.8 1.0 0.8 1.0 0.8 1.0\ntemperature\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0error rate\nHuman Identification Performance\nada babbage curie davinci\nFigure 6: Human evaluation. Error rate of human\nevaluators at the task of ﬁnding whether any sentence\nin a group of 5 was generated by GPT-3 or not. Each\ncolor represents a different GPT-3 engine. Higher error\nrate indicates that humans could not correctly identify\ngenerated samples and thus it also indicates higher ﬁ-\ndelity. The standard error is displayed as a vertical line\non top of each bar.\nFigure 7: Human evaluation tool. Example of a ques-\ntion for the human evaluators. Human evaluators are\nasked to ﬂag which example is GPT-3 generated if any\namong the 5 presented ones.\nWe consider that a model produces sentences\nwith high ﬁdelity if a human is unable to distinguish\nthem from a set of human-generated sentences be-\nlonging to the same intent. Therefore, for each\nintent in the CLINC150 dataset, we sample ﬁve ran-\ndom examples and we randomly choose whether to\nreplace one of them by a GPT-3 generated sentence\nfrom the same intent. We generate sentences with\neach of the four GPT-3 models considered in the\nmain text with two different temperatures (0.8 and\n1.0). The sentence to replace is randomly selected.\nFinally, the ﬁve sentences are displayed to a hu-\nman who has to choose which of the sentences is\ngenerated by GPT-3, if any.\nThe task is presented to human evaluators in\nthe form of a web application (see Figure 7). We\nplaced a button next to each sentence in order to\nforce human evaluators to individually consider\neach of the examples. Once annotated, the evalua-\ntor can either submit, discard, or leave the task to\nlabel later. We used a set of 15 voluntary evalua-\ntors from multiple backgrounds, nationalities, and\ngenders. Each evaluator annotated an average of 35\nexamples, reaching a total of 500 evaluated tasks.\nFor each model and temperature, we report the\nerror rate of humans evaluating whether a task con-\ntains a GPT-generated sample. We consider that\nevaluators succeeds at a given task when they cor-\nrectly ﬁnd the sentence that was generated by GPT\nor when they identify that none of them was gener-\nated. Thus, the error rate for a given model and tem-\nperature is calculated as #failed / total_evaluated.\nResults are displayed in Figure 6. We ﬁnd that\nhuman evaluators tend to make more mistakes\nwhen the temperature used to sample sentences\nfrom GPT-3 is smaller. This result is expected since\nlowering the temperature results in sentences closer\nto those prompted to GPT-3, which are human-\nmade. We also observe that models with higher ca-\npacity such as Davinci tend to generate more in-\ndistinguishable sentences than lower-capacity mod-\nels such as Ada, even for higher temperatures.\nThese results are in agreement with the \"oracle\"\nﬁdelity results introduced in Figure 2.\n57",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8026766777038574
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6837154626846313
    },
    {
      "name": "Training set",
      "score": 0.6360906362533569
    },
    {
      "name": "Task (project management)",
      "score": 0.5770441293716431
    },
    {
      "name": "Labeled data",
      "score": 0.5512545704841614
    },
    {
      "name": "Language model",
      "score": 0.5512064099311829
    },
    {
      "name": "Machine learning",
      "score": 0.5143826603889465
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4962518811225891
    },
    {
      "name": "Scarcity",
      "score": 0.4484007954597473
    },
    {
      "name": "Data quality",
      "score": 0.4102160334587097
    },
    {
      "name": "Data mining",
      "score": 0.36479854583740234
    },
    {
      "name": "Metric (unit)",
      "score": 0.09473729133605957
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Microeconomics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210167039",
      "name": "ServiceNow (United States)",
      "country": "US"
    }
  ],
  "cited_by": 60
}