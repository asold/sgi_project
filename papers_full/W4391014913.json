{
  "title": "A Molecular Video-derived Foundation Model Streamlines Scientific Drug Discovery",
  "url": "https://openalex.org/W4391014913",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5021122269",
      "name": "Feixiong Cheng",
      "affiliations": [
        "Case Western Reserve University",
        "Cleveland Clinic Lerner College of Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5000226430",
      "name": "Hongxin Xiang",
      "affiliations": [
        "Hunan University"
      ]
    },
    {
      "id": "https://openalex.org/A5032006378",
      "name": "Li Zeng",
      "affiliations": [
        "Hunan University"
      ]
    },
    {
      "id": "https://openalex.org/A5007118954",
      "name": "Linlin Hou",
      "affiliations": [
        "Hunan University"
      ]
    },
    {
      "id": "https://openalex.org/A5078793726",
      "name": "Kenli Li",
      "affiliations": [
        null,
        "Hunan University",
        "Cleveland Clinic Lerner College of Medicine",
        "Northeast Ohio Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A5063328046",
      "name": "Zhimin Fu",
      "affiliations": [
        "Hunan University",
        "Cleveland Clinic Lerner College of Medicine",
        "Frederick National Laboratory for Cancer Research",
        "National Cancer Institute",
        "Northeast Ohio Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A5054158092",
      "name": "Yunguang Qiu",
      "affiliations": [
        "Cleveland Clinic Lerner College of Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5025357844",
      "name": "Ruth Nussinov",
      "affiliations": [
        "Tel Aviv University",
        "Frederick National Laboratory for Cancer Research",
        "National Cancer Institute",
        "Northeast Ohio Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A5040300380",
      "name": "Jianying Hu",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5027286013",
      "name": "Xiangxiang Zeng",
      "affiliations": [
        "Hunan University"
      ]
    },
    {
      "id": "https://openalex.org/A5003092468",
      "name": "Michal Rosenâ€Zvi",
      "affiliations": [
        "Hebrew University of Jerusalem",
        "IBM Research - Haifa"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3010016408",
    "https://openalex.org/W4210689197",
    "https://openalex.org/W4383373262",
    "https://openalex.org/W4210617107",
    "https://openalex.org/W3114291043",
    "https://openalex.org/W3191452947",
    "https://openalex.org/W3217546525",
    "https://openalex.org/W3110901318",
    "https://openalex.org/W6772452955",
    "https://openalex.org/W3095883070",
    "https://openalex.org/W4309218736",
    "https://openalex.org/W3082302528",
    "https://openalex.org/W4311001822",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W4250602051",
    "https://openalex.org/W569478347",
    "https://openalex.org/W6601955380",
    "https://openalex.org/W6910499371",
    "https://openalex.org/W6784694379",
    "https://openalex.org/W3179111421",
    "https://openalex.org/W3205158828",
    "https://openalex.org/W3206711231",
    "https://openalex.org/W4213077304",
    "https://openalex.org/W4281553035",
    "https://openalex.org/W3139062830",
    "https://openalex.org/W6730543993",
    "https://openalex.org/W6618669868",
    "https://openalex.org/W6631828510",
    "https://openalex.org/W6602160113",
    "https://openalex.org/W1676314349",
    "https://openalex.org/W2566855623",
    "https://openalex.org/W2179147683",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W3047826509",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W6734561206",
    "https://openalex.org/W4256597633",
    "https://openalex.org/W2986232138",
    "https://openalex.org/W2034400748",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2299115575",
    "https://openalex.org/W4232186742",
    "https://openalex.org/W2008056655",
    "https://openalex.org/W4386076625",
    "https://openalex.org/W4254957087",
    "https://openalex.org/W2005363895",
    "https://openalex.org/W3036446966",
    "https://openalex.org/W2060531713",
    "https://openalex.org/W4221074165",
    "https://openalex.org/W4213070269",
    "https://openalex.org/W2073081213",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2962711740",
    "https://openalex.org/W4297971608",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2157444450",
    "https://openalex.org/W2767891136",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W4214868967",
    "https://openalex.org/W2027461913",
    "https://openalex.org/W3037208489",
    "https://openalex.org/W3095602948",
    "https://openalex.org/W2034484539",
    "https://openalex.org/W2024898509",
    "https://openalex.org/W2051224630",
    "https://openalex.org/W3145385912",
    "https://openalex.org/W3136399186",
    "https://openalex.org/W3213940558",
    "https://openalex.org/W3005552578",
    "https://openalex.org/W3170424177",
    "https://openalex.org/W4286907726"
  ],
  "abstract": "<title>Abstract</title> Precise capture of three-dimensional (3D) dynamic conformation during molecular representation learning is crucial for accurate prediction of drug targets and molecular properties. In this study, we propose a molecular video-based foundation model, named VideoMol, pretrained on 120 million frames of 2 million unlabeled drug-like and bioactive molecules with 3D conformations. VideoMol renders the molecular 3D conformation as a 60-frame dynamic video and designs three self-supervised learning strategies on molecular videos to capture diverse conformational changes. We demonstrate high performance of VideoMol in predicting molecular targets and properties across 44 benchmark drug discovery datasets. VideoMol achieves high accuracy in identifying antiviral molecules against SARS-CoV-2 across 11 high-throughput experimental datasets from the National Center for Advancing Translational Sciences and other diverse disease-specific drug targets. We further present high interpretability of VideoMol through observed key chemical substructures related to dynamic 3D conformational changes compared to traditional state-of-the-art deep learning approaches. In summary, VideoMol offers a powerful tool to expedite drug discovery and development.",
  "full_text": "A Molecular Video-derived Foundation Model\nStreamlines Scienti\u0000c Drug Discovery\nFeixiong ChengÂ \nCleveland Clinic https://orcid.org/0000-0002-1736-2847\nHongxin XiangÂ \nHunan University https://orcid.org/0000-0001-8345-8735\nLi ZengÂ \nHunan University\nLinlin HouÂ \nHunan University\nKenli LiÂ \nHunan University\nZhimin FuÂ \nNortheast Ohio Medical University\nYunguang QiuÂ \nCleveland Clinic https://orcid.org/0000-0002-5094-0936\nRuth NussinovÂ \nFrederick National Laboratory for Cancer Research (NIH/NCI) https://orcid.org/0000-0002-8115-6415\nJianying HuÂ \nIBM Research https://orcid.org/0000-0001-7753-886X\nXiang-Xiang ZengÂ \nHunan University\nMichal Rosen-ZviÂ \nIBM Research https://orcid.org/0000-0001-7616-9724\nArticle\nKeywords:\nPosted Date: January 19th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-3773235/v1\nLicense: ï‰ ï“§ This work is licensed under a Creative Commons Attribution 4.0 International License. Â \nRead Full License\nAdditional Declarations: There is NO Competing Interest.\nVersion of Record: A version of this preprint was published at Nature Communications on November 8th,\n2024. See the published version at https://doi.org/10.1038/s41467-024-53742-z.\n1 \n \nA Molecular Video-derived Foundation Model \nStreamlines Scientific Drug Discovery \n \nHongxin Xiang1, Li Zeng1, Linlin Hou1, Kenli Li1, Zhimin Fu2,3, Yunguang Qiu2, \nRuth Nussinov3,4, Jianying Hu5, Michal Rosen-Zvi6,7, Xiangxiang Zeng1,*,  \nFeixiong Cheng2,8,9,10,* \n1College of Computer Science and Electronic Engineering, Hunan University, \nChangsha, Hunan, 410082, China \n2Genomic Medicine Institute, Lerner Research Institute, Cleveland Clinic, \nCleveland, OH 44195, USA \n3College of Pharmacy, Northeast Ohio Medical University, Rootsto wn, OH \n44272 \n3Computational Structural Biology Section, Frederick National Laboratory for \nCancer Research in the Cancer Innovation Laboratory, National Cancer \nInstitute, Frederick, MD 21702, USA \n4Department of Human Molecular Genetics and Biochemistry, Sackler School \nof Medicine, Tel Aviv University, Tel Aviv 69978, Israel \n5IBM Research, Yorktown Heights, NY 10598, USA \n6AI for Accelerated Healthcare and Life Sciences Discovery, IBM Research \nLabs, Haifa 3498825, Israel,  \n7Faculty of Medicine, The Hebrew University of Jerusalem, Jerusalem 9190500, \nIsrael \n2 \n \n8Department of Molecular Medicine, Cleveland Clinic Lerner College of \nMedicine, Case Western Reserve University, Cleveland, OH 44195, USA \n9Case Comprehensive Cancer Center, Case Western Reserve University \nSchool of Medicine, Cleveland, OH 44106, USA \n10Cleveland Clinic Genome Center, Lerner Research Institute, Cleveland \nClinic, Cleveland, OH 44195, USA \n \n*Corresponding authors: xzeng@hnu.edu.cn (X.Z.) and chengf@ccf.org (F.C.) \n \nTo whom correspondence should be addressed:  \nFeixiong Cheng, Ph.D. \nLerner Research Institute, Cleveland Clinic, Ohio, USA \nTel: +1-216-444-7654; Fax: +1-216-636-0009 \nEmail: chengf@ccf.org \n  \n3 \n \nAbstract \nPrecise capture of three-dimensional (3D) dynamic conformation during \nmolecular representation learning is crucial for accurate prediction of drug \ntargets and molecular properties. In this study, we propose a molecular video-\nbased foundation model, named VideoMol, pretrained on 120 million frames \nof 2 million unlabeled drug-like and bioactive molecules with 3D \nconformations. VideoMol renders the molecular 3D conformation as a 60-\nframe dynamic video and designs three self-supervised learning strategies on \nmolecular videos to capture diverse conformational changes. We demonstrate \nhigh performance of VideoMol in predicting molecular targets and properties \nacross 44 benchmark drug discovery datasets. VideoMol achieves high \naccuracy in identifying antiviral molecules against SARS-CoV-2 across 11 \nhigh-throughput experimental datasets from the National Center for Advancing \nTranslational Sciences and other diverse disease-specific drug targets. We \nfurther present high interpretability of VideoMol through observed key \nchemical substructures related to dynamic 3D conformational changes \ncompared to traditional state-of-the-art deep learning approaches. In \nsummary, VideoMol offers a powerful tool to expedite drug discovery and \ndevelopment.  \n4 \n \nIntroduction \nDrug discovery is a complex and time-consuming process that involves the \nidentification of potential drug targets, the design and synthesis of \ncompounds, and the testing of compounds for efficacy and safety1, 2. For \nexample, in traditional drug discovery, medicinal chemists and \npharmacologists select and optimize candidate compounds based on \nknowledge and experience and verify them by screening cellular or animal \nmodels3. Computational drug discovery that uses computational and artificial \nintelligence technologies to assist drug development offers a promising \napproach to speeding up this process4, 5. By leveraging large datasets of \nbiological and chemical information, these computational approaches, such as \nfoundation models6, 7, can rapidly identify new drug targets8, design candidate \nmolecules9, and evaluate the efficacy and properties of those candidates10, \nwhich substantially reduces the time and cost of traditional drug discovery and \ndevelopment. \n         Accurate molecular representation of hundreds of millions of existing \nand novel compounds is a fundamental challenge for computational drug \ndiscovery communities11. Traditional approaches used hand-crafted \nfingerprints as molecular representations, such as physicochemical \nfingerprints12, and pharmacophore-based fingerprints13. Limited by domain \nknowledge, these traditional representation approaches are subjective and \nimmutable, devoid of adequate generalizability. With the rise of deep learning \n5 \n \nand self-supervised learning, automated molecular representation learning \napproaches can extract representations from molecular sequences14, 15, \ngraphs16, 17, and images by pre-training on large-scale molecular datasets18. \nThese approaches showed a substantial performance improvement in the \nvarious tasks of drug discovery7, 18, 19. However, they ignore the dynamic \ninformation of molecules in three-dimensional (3D) geometric space when \nextracting molecular representations, resulting in limitations in accurately \ndescribing 3D interactions of ligand-receptor pairs. Based on recent advances \nin video representation learning and self-supervised learning in computer \nvision20-22,  self-supervised video-based pre-trained models offer compelling \nopportunities to further improve the performance of drug discovery. \n         In this study, we present a molecular video-based foundation model \n(termed VideoMol) for 3D molecular representation learning. Specifically, \nVideoMol utilizes dynamic awareness and physicochemical awareness to \nlearn molecular representation from a vast quantity of molecular 3D dynamic \nvideos in an unsupervised manner. We implemented a self-supervised pre-\ntraining framework to capture the dynamic 3D conformational information and \nphysicochemical information of compounds from 120 million frames of 2 \nmillion molecular videos with diverse biological activities at the human \nproteome. We demonstrate that VideoMol outperforms existing state-of-the-art \nmethods in drug discovery tasks, including drug target and molecular property \npredictions.  \n6 \n \nResults \nFramework of VideoMol \nWe proposed VideoMol for accurately predicting the targets and properties of \nmolecules in the form of dynamic video derived from molecular 3D \nconformations. First, we generated 3D conformations for 2 million drug-like \nand bioactive molecules and rendered a dynamic video with 60 frames for \neach 3D molecular conformation (120 million frames in total). Then, we feed \nthe molecular 3D videos into a video encoder to extract latent features (Error! \nReference source not found.a) and implement three pretraining strategies to \noptimize the latent representation by considering conformational dynamic \nchanges of videos and physicochemical information of molecules (Error! \nReference source not found.b-Error! Reference source not found.d). Finally, \nwe fine-tune the pre-trained video encoder on downstream tasks (prediction of \nmolecular targets and properties) to further improve the model performance \n(Error! Reference source not found.e).  VideoMol achieves good \ninterpretability through the use of Grad-CAM (Gradient-weighted Class \nActivation Mapping)23 to visualize the contribution of molecular videos to the \nprediction results with heatmaps (Error! Reference source not found.d and \nError! Reference source not found.e). To comprehensively evaluate the \nperformance of VideoMol, we selected four types of tasks: (1) compound-\nkinase binding activity prediction, (2) ligand-GPCR (G Protein-Coupled \nReceptors) binding activity prediction, (3) anti-SARS-CoV-2 activity prediction, \n7 \n \nand (4) prediction of molecular properties. Details of these datasets are \nprovided in the Methods section and Supplementary Methods. \n \nPerformance of VideoMol \nWe first evaluated the performance of VideoMol with three types of state-of-\nthe-art molecular representation learning methods on 10 compound-kinase \n(classification task) and 10 ligand-GPCR (regression task) binding activity \nprediction datasets with balanced scaffold split24. The state-of-the-art methods \ninclude: (1) sequence- (RNNLR, TRFMLR, RNNMLP, TRFMMLP, RNNRF, \nTRFMRF25, CHEM-BERT26), (2) graph- (MolCLRGIN, MolCLRGCN6), and (3) \nimage-based models (ImageMol18) (Supplementary Table 1). For 10 \ncompound-kinase interaction datasets, VideoMol achieves better AUC \nperformance than other methods across BTK (AUC=0.861), CDK4-cyclinD3 \n(AUC=0.972), EGFR (AUC=0.905), FGFR2 (AUC=0.0.988), FGFR4 \n(AUC=0.852), FLT3 (AUC=0.981), KPCD3 (AUC=0.819) and MET \n(AUC=0.981) with an average performance improvement of 6.6% ranging \nfrom 1.8% to 20.3% (Error! Reference source not found.a and \nSupplementary Table 2). In particular, VideoMol outperforms the state-of-the-\nart ImageMol and MolCLR with average improvements of 5.3% and 22.7%. \nFor 10 ligand-GPCR binding datasets, VideoMol achieves the best results on \nall datasets with an average performance improvement by 3.6% on Root \nMean Squared Error (RMSE) ranging from 0.7% to 7.3% and 5.7% on Mean \n8 \n \nAbsolute Error (MAE) ranging from 0.2% to 11.4% (Error! Reference source \nnot found.b-c and Supplementary Table 3). VideoMol achieves average \nperformance improvement of 3.5% and 10.2% on RMSE, and 5.5% and \n12.8% on MAE for ImageMol and MolCLR, respectively. \n           We further evaluated the performance of VideoMol with 19 popular and \ncompetitive baselines on 13 molecular property prediction benchmarks with \nscaffold split: (1) molecular targetsâ€”beta-secretase (BACE, a key target in \nAlzheimerâ€™s disease) and anti-viral activities in human immunodeficiency virus \n(HIV), (2) bloodâ€“brain barrier penetration (BBBP); (3) drug metabolism and \nside effect resource (SIDER); (4) molecular toxicitiesâ€”toxicity using the \nToxicology in the 21st Century (Tox21) and clinical trial toxicity (ClinTox) \ndatabases and Toxicity Forecaster (ToxCast); (5) solubilityâ€”Free Solvation \n(FreeSolv) and Estimated Solubility (ESOL)â€”and lipophilicity (Lipo); (6) \nquantumâ€”Quantum Machine 7 (QM7), QM8 and QM9. We compared \nVideoMol with 5 different types of state-of-the-art methods, including \nfingerprint- (RF27, SVM28), sequence- (X-MOL7), 2D-graph- (GCN 29, GIN30, D-\nMPNN19, Hu et al.16, MolCLR6, GCC31, GPT-GNN32, MGSSL33, G-Motif17, \nGraphLoG34, GraphCL35, GROVER17 and MPG36), 3D-graph- (3D InfoMax37, \nGraphMVP38, GEM39, Uni-Mol40), and image-based methods (ImageMol18) \n(Supplementary Table 1). In classification task, using the area under the \nreceiver operating characteristic (ROC) curve (AUC), VideoMol achieves \nelevated performance across BBBP (AUC=74.6%), Tox21 (AUC=78.1%), \n9 \n \nClinTox (AUC=87.2%), HIV (AUC=81.1%), BACE (AUC=86.8%), SIDER \n(AUC=69.6%), ToxCast (AUC=70.9%), outperforming other methods (Error! \nReference source not found.c and Supplementary Table 4). In regression \ntask, VideoMol achieves low error values across FreeSolv (RMSE=1.82), \nESOL (RMSE=0.80), Lipo (RMSE=0.62), QM7 (MAE=66.9), QM8 \n(MAE=0.0175) and QM9 (MAE=0.00789), outperforming other methods with a \nrelative performance improvement of 12.8% (Error! Reference source not \nfound.d and Supplementary Table 5). \n           We next turned to evaluate VideoMol on anti-SARS-CoV-2 viral activity \nprediction. Specifically, we evaluated 11 SARS-CoV-2 biological assays, \nwhich covers multiple therapeutic approaches such as viral replication, viral \nentry, counter-screening, in vitro infectivity, and live virus infectivity41. \nCompared with REDIAL-202041 (molecular fingerprint-based method) and \nImageMol18 (image-based representation method), we found that VideoMol \nachieved elevated ROC-AUC performance (3CL=0.705, ACE2=0.759, \nhCYTOX=0.765, MERS-PPE_cs=0.828, MERS-PPE=0.814, CoV1-\nPPE_cs=0.830, CoV1-PPE=0.728, CPE=0.747, Cytotox=0.761, \nAlphaLISA=0.841, TruHit=0.862) with an average 3.8% improvement ranging \nfrom 2.5% to 7.8% compared with ImageMol and an average 8.0% \nimprovement ranging from 0.6% to 16.9% compared with REDIAL-2020 \n(Error! Reference source not found.e and Supplementary Table 6). \n      In summary, VideoMol is an effective molecular video-based \n10 \n \nrepresentation learning method in drug discovery tasks, outperforming state-\nof-the-art methods (Error! Reference source not found. and \nSupplementary Tables 2-6). \n \nDiscovery of ligand-receptor interactions via VideoMol \nWe next turned to identifying novel ligand-receptor interactions via VideoMol \nacross 4 well-known human targets, beta-secretase 1 [BACE1], \ncyclooxygenase 1 [COX-1], COX-2, and prostaglandin E receptor 4 [EP4]), in \norder to assess the generalizability of the model. We collected the training \ndata of these 4 targets from the ChEMBL database42 and evaluated the \nperformance of VideoMol on these targets with a random split of 8:1:1 \n(Supplementary Table 7). Using ROC-AUC metric evaluation, we found that \nVideoMol achieved high performance on both validation set (BACE1=0.897, \nCOX-1=0.849, COX-2=0.881 and EP4=0.773) and test set (BACE1=0.893, \nCOX-1=0.901, COX-2=0.907 and EP4=0.899), outperforming ImageMol with \nan average improvement rate of 6.4% in the validation set and an average \nimprovement rate of 4.1% in the test set (Error! Reference source not \nfound.a). The t-SNE visualization in the latent space showed a clear boundary \nbetween inhibitors and non-inhibitors on all 4 targets, suggesting ideal \nrepresentation ability of VideoMol to learn discriminative information (Error! \nReference source not found.b). \n           We further collected 16 BACE1 inhibitors (Supplementary Table 8), \n11 \n \n22 COX-1 inhibitors (Supplementary Table 9), 35 COX-2 inhibitors \n(Supplementary Table 10) and 8 EP4 inhibitors (Supplementary Table 11) \nfrom the MedChemExpress database (https://www.medchemexpress.com/, \nsee Supplementary Methods). We found that VideoMol successfully re-\nidentified 15 BACE1 inhibitors (93.8% success rate), 8 COX-1 inhibitors \n(36.4% success rate), 11 COX-2 inhibitors (34.3% success rate) and 6 EP4 \ninhibitors (75.0% success rate) (Error! Reference source not found.c, Error! \nReference source not found.d and Supplementary Tables 12-15). \nCompared with ImageMol, VideoMol achieved significantly better \ngeneralizability to these four external validation targets with an average \nprecision improvement of 38.1% ranging from 12.5% to 75.0%. \n \nDiscovery of BACE1 inhibitors from existing drugs via VideoMol \nBACE1 (beta-site amyloid precursor protein cleaving enzyme 1), is a key drug \ntarget in Alzheimer's disease (AD) and there is lack effective small molecular \ntreatment for AD to date43. We next turned to screening potential inhibitors via \nspecifically targeting BACE1 from 2,500 approved drugs from the DrugBank \ndatabase44 using VideoMol (Supplementary Table 16). We downloaded the \nknown X-ray crystal structure of BACE1 (PDB ID: 4IVS) with a co-crystallized \ninhibitor bearing the indole acylguanidine core structure (Ligand ID: VSI)45 \nfrom the PDB (Protein Data Bank)46 database. We evaluated grid score (a \nmetric of binding ability and the smaller value denotes the better score) \n12 \n \nbetween ligand and receptor (PDB ID: 4IVS, Error! Reference source not \nfound.a) by Dock6.1047. We illustrated the BACE1 inhibitor prediction results \nfor both ImageMol and VideoMol in Supplementary Table 17 and \nSupplementary Table 18. We collected experimental evidence for the top 20 \ndrugs predicted by VideoMol and ImageMol from the published literatures. We \nfound that 11 of the 20 drugs predicted by VideoMol were validated as \npotential treatment for AD (55% success rate), which was higher than the 5 \ndrugs of ImageMol (25% success rate). (Error! Reference source not \nfound.b). These reflected that VideoMol can learn more dynamic \nconformational information and chemical information to ensure the high \nconfidence of predictions. We further evaluated the grid scores of the top 20 \npredicted drugs to the 4IVS crystal structure by using Dock6.10. Using the \ngrid score of 4IVS crystal structure (Error! Reference source not found.a) \nas a threshold, VideoMol prioritizes more drugs with better grid scores of -\n52.47 (60%, 12 out of 20 drugs) compared to ImageMol (20%, 4 out of 20 \ndrugs) (Error! Reference source not found.c), revealing that VideoMol \ncaptures dynamic 3D conformational information compared to ImageMol \nderived from static 1D and 2D image-based representation18. Finally, we \nselected 6 drugs with the best grid score from the top 20 drugs for molecular \ndocking simulation (Error! Reference source not found.d). We found that 5 out \nof 6 drugs (83.3%) had been validated as potential treatment for AD based on \nexisting published experimental data (Supplementary Table 19). \n13 \n \n \nVideo visualization and model interpretability \nSince each frame in the molecular videos represents the same molecule, their \nprojections in the feature space should be similar. To evaluate the \ndiscriminative power of VideoMol on molecular videos, we randomly selected \n100 molecular videos and extract features for each frame in the videos. \nSubsequently, we used t-SNE (t-distributed Stochastic Neighbor \nEmbedding)48 to project each feature into a two-dimensional space (Error! \nReference source not found.a and Supplementary Figure 1). Frames from \nthe same video are well clustered together, while frames from different videos \nare clearly separated. We also quantitatively evaluated the DB (Davies \nBouldin) index49 of these clusters. VideoMol achieved a low DB index (the \nvalue is 0.197), indicating that VideoMol has the ability to recognize different \nframes of the same molecule. We randomly sample 10,000 pairs of molecular \nframes from the same and different molecular videos respectively and \ncompute the cosine similarity between these paired samples. As expected, \nthere is a high average similarity (88.3%) on intra-video and almost zero \n(0.5%) on inter-video, indicating that VideoMol is robust to different 3D views \nof the same molecule (Error! Reference source not found.b). \n         To investigate how the physicochemical information contribute \nperformance of VideoMol, we used t-SNE to visualize the representation of \nVideoMol with cluster labels from a chemical-aware pretraining task. We \n14 \n \nrandomly selected 10 clusters (1,000 samples for each cluster) for \nvisualization. As shown in Error! Reference source not found.c, the \nrepresentations extracted by VideoMol produce clusters with sharp \nboundaries with a low DB index=0.182, indicating that VideoMol learned \nphysicochemical knowledge well. We further visualized 30 additional clusters \nwith 500 samples per cluster and found that different cluster labels still \nproduce strong clustering effect (Supplementary Figure 2). \n           To inspect how VideoMol performs the inference process, we used \nGradCAM23 to visualize attention heatmaps for molecular videos. Each row \nrepresents the same molecular video, and each frame is obtained at the same \ntime interval in the video. Error! Reference source not found.d and Error! \nReference source not found.e respectively shows the consistency and \ndiversity of VideoMol's attention (green represents carbon atom C, blue \nrepresents nitrogen atom N, red represents oxygen atom O and cyan \nrepresents fluorine atom). We found that as the video played, VideoMol was \nalways able to attend to the same molecular substructure (Error! Reference \nsource not found.d), such as Piperidine (5 carbons and 1 nitrogen \ncomposition) in the first row, and carbon-oxygen structure in the second row, \nwhich shows that VideoMol has consistency for different frames in the same \nvideo. In Error! Reference source not found.e, we see that VideoMol can \npay attention to diverse structural information as the video plays to alleviate \nthe missing structural information. For example, VideoMol cannot attend to \n15 \n \nbenzene-, cyclopentene-, hydroxylamine- and benzenamine-structure through \nthe use of Grad-CAM in the left frame, whereas VideoMol can attend to these \nstructures in the right frame. \n        To test that VideoMol can provide chemists with meaningful knowledge \nrelated to predictive targets, we evaluate the interpretability of VideoMol in \nprediction of BACE-1 inhibitors. We found that VideoMol identified known \nchemistry knowledge related to BACE-1 inhibitors, such as fluorine50, 1,2,4-\nOxadiazole51, chromene52, pyridine53, cyclopentane54, tetrazole51 (Error! \nReference source not found.f), which was verified by wet experiments (all \nevidence can be found in Supplementary Table 20). For instance, VideoMol \nmaintained high attention on fluorine when predicting a compound as an \ninhibitor of BACE-1, which was validated by a previous experimental study50. \n \nAblation study \nThe effectiveness of pre-training strategies. To study the impact of the pre-\ntraining strategies on VideoMol, we train VideoMol with different pre-training \ntasks, including w/o pre-training, only video-aware strategy, only direction-\naware strategy and only chemical-aware strategy, chemical&direction, \nchemical&video, and direction&video (Supplementary Table 21). We found \nthat pre-training tasks can improve the performance of VideoMol on \ndownstream tasks compared to VideoMol without pre-training with 27.2% \naverage RMSE improvement and 31.1% average MAE improvement. Ablation \n16 \n \nexperiments on single or double pre-training tasks show that chemical-\nawareness is important and the pre-training tasks guide VideoMol to learn \nmeaningful chemical knowledge in the molecule. We can also see a trend that \nwith the integration of pre-training tasks, the performance of VideoMol \nimproves consistently in most evaluations, indicating that these pre-training \ntasks complement each other. Overall, the proposed pre-training tasks are \neffective for improving the performance of VideoMol. \n      Effectiveness of 3D conformational features captured by VideoMol. To test \nthe advantage of VideoMol as a molecular visual feature, we use VideoMol to \nextract the features of each frame in the molecular video and calculate their \nmean value as the molecular feature (called VideoMolFeat). For a fair \ncomparison, we integrated 21 traditional molecular fingerprints, which are \nobtained by fingerprint stitching and PCA (Principal Component Analysis)55 \ndimensionality reduction (called EnsembleFP). We then evaluated the \nperformance using the MLP (Multilayer Perceptron) implemented by scikit-\nlearn56 and kept the same experimental settings. We found that VideoMolFeat \nachieved the best performance with 17.2% average RMSE improvement and \n19.4% average MAE improvement compared with EnsembleFP on 10 kinases \ndatasets, which illustrates that a visual molecular feature is effective as a \nsuperior alternative to traditional molecular fingerprinting (Supplementary \nTable 22). \n      Robustness of VideoMol. A molecule has multiple conformations37, 38, \n17 \n \nwhich may lead to performance instability due to differences in model \npredictions for different conformations of the same molecule. Therefore, it is \nnecessary to investigate conformational robustness of VideoMol, which \nmeans that the model has little variation in predictive performance across \ndifferent 3D conformations. Here, to observe the performance difference of \nVideoMol on different conformations, we generate 4 molecular videos with \ndifferent conformations for each molecule on 5 SARS-CoV-2 activity \nprediction datasets: (1) the Middle-East Respiratory Syndrome coronavirus \nPseudotyped Particle Entry (MERS-PPE) assay, (2) the SARS-CoV \nPseudotyped Particle Entry (CoV-PPE) assay, (3) the SARS-CoV-2 cytopathic \neffect (CPE) assay, (4) the Spikeâ€“ACE2 proteinâ€“protein interaction \n(AlphaLISA) assay and (5) the TruHit counterscreen of the Spikeâ€“ACE2 \nproteinâ€“protein interaction assay (TruHit). We found that VideoMol can \nachieve similar performance with no more than 1% standard deviation on \ndifferent conformations, indicating that VideoMol is robust to different \nmolecular conformations (Supplementary Table 19). \n \nDiscussion \nWe have proposed a self-supervised video-processing-based pre-training \nframework, VideoMol, that learns molecular representations from 3D dynamic \nconformations by utilizing dynamic awareness and physicochemical \nawareness. We showed the high performance of VideoMol on various drug \n18 \n \ndiscovery tasks, including predicting molecular target profiles (e.g., GPCRs, \nkinases, SARS-CoV-2) and molecular properties (e.g., pharmacology, \nbiophysics, physical and quantum chemistry). We evaluated the effectiveness \nof VideoMol on 4 common targets (BACE1, COX-1, COX-2 and EP4) from \nChEMBL (Fig. 1a and Fig. 2b). We also verified the high precision of \nVideoMol on the virtual screening of 4 targets (BACE1, COX-1, COX-2 and \nEP4), which are consistent with ongoing clinical and experimental data (Fig. \n3c and Fig. 4d). Compared with ImageMol, VideoMol achieved an average \nprecision improvement of 38.1% on these 4 targets, which showed that \nVideoMol is able to generalize to external validation sets. Especially in the \nvirtual screening of COX-1, COX-2 and EP4 inhibitors, VideoMol achieved \nsignificant advantages, demonstrating VideoMol can overcame extreme data \nimbalance (imbalance rates of 0.043 and 0.253 in COX-1 and COX-2 from \nChEMBL) and data scarcity (only 350 samples in EP4 from ChEMBL) \nscenarios. \n          On the interpretability of VideoMol, we found that the attention of \nVideoMol is different on different frames of the same video in Error! \nReference source not found.e, which is due to occlusion of viewing angles \nproblem that make useful information often scattered in different views57. This \nshowed the advantage of molecular video, allowing VideoMol to learn more \n3D conformational information by scanning each frame. In addition, it is worth \nnoting that VideoMol can perceive substructures in extreme occlusion scenes \n19 \n \n(such as the third column of the first row in Error! Reference source not \nfound.d).  \n           We highlighted several improvements of VideoMol over state-of-the-art: \n(1) VideoMol achieves high performance on various benchmark datasets \n(including property prediction and target binding activity prediction), \noutperforming the state-of-the-art representation learning methods \n(Supplementary Tables 2-6); (2) VideoMol overcomes class imbalance and \ndata scarcity scenarios and achieves high accuracy and strong generalization \nin virtual screening on 4 common targets (BACE1, COX-1, COX-2 and EP4) \n(Error! Reference source not found., Supplementary Table 7 and \nSupplementary Tables 12-15); (3) VideoMol captures 3D information and is \ngood at predicting ligands with high binding capacity to receptors \n(Supplementary Tables 16-18 and Error! Reference source not found.); \n(4) the representation of VideoMol is robust to inconsistent views of the \nmolecule (Error! Reference source not found.a and Error! Reference \nsource not found.b) and contains rich and meaningful physicochemical \ninformation (Error! Reference source not found.c); (5) VideoMol has good \ninterpretability, which is intuitive and informative for identifying chemical \nstructures or substructures related to molecular properties and target binding, \nand can solve the occlusion of viewing angles problem (Error! Reference \nsource not found.d and Error! Reference source not found.e). \nFurthermore, to explore the effect of frame number on VideoMol, we extract \n20 \n \nframes in videos with equal spacing. We found that the performance of \nVideoMol can gradually improve with the number of frames, which shows that \nthe combination of different frames can enrich the feature representation of \nmolecules (Supplementary Table 21). \n          VideoMol is a novel molecular representation learning framework, \nwhich is significantly different from previous sequence-, graph- and image-\nbased molecular representation learning methods. VideoMol treated \nmolecules as dynamic videos with 3D conformations and learned molecular \nrepresentations in a video processing manner, which means that a large \nnumber of video representation learning technologies can be used for learning \nmolecular representation21, 58. Compared with our previous ImageMol, \nVideoMol had several substantial upgrades, including: (1) the content of \nmolecular visual representation is upgraded from 2D pixel information to 3D \npixel information; (2) molecular pre-training is upgraded from image-based \nlearning to video-based learning; (3) the fingerprint information included is \nupgraded from the previous 1 fingerprint (MACCS key) to 21 fingerprints \n(Supplementary Table 23). Since VideoMol involves research fields such as \nimage representation learning, video representation learning, and multi-view \nrepresentation learning, it has greater research potential and motivates more \nresearchers for greater performance improvement. Finally, (4) a 3D dynamic \nconformational video representation is especially consequential given that \nprotein molecules exist and function as conformational ensembles. Function \n21 \n \nrequires that there be more than a single structure, and that there be dynamic \nswitching between the conformations.  \n            We acknowledged several potential limitations of VideoMol. While \nmolecular video can achieve performance improvements, it will increase the \ncomputational complexity. Although the multi-view fine-tuning strategy can \nreduce the computational complexity, the choice of view is still a problem. Like \nother 3D-based molecular representation methods37-39, VideoMol does not \ntake the diversity of conformers into account, but it can easily be improved by \nmodeling consistency between different conformers (Supplementary Table \n19). Several potential directions may further improve VideoMol: (1) Use of \nmore biomedical data to train a larger version of VideoMo, noting that this \nwould increase demand on computing resource; (2) Under resource \nconstraints, use of pruning strategies (including data pruning and model \npruning) to reduce the computational complexity of VideoMol; (3) Due to the \nrich physical and chemical information integrated in VideoMol, the distillation \nbased on VideoMol is a meaningful research direction, which uses VideoMol \nas a teacher model to guide the learning of other student models (such as \nsequence-based models, graph-based models, etc.); (4) Use of better video \nprocessing methods and ensemble learning methods to integrate information \nbetween different frames is also an important direction to improve \nperformance. \n \n22 \n \nMethods \nMolecular conformer generation \nWhen pre-training, we directly use the conformational information provided in \nPCQM4Mv2 database59. However, during fine-tuning, molecules in \ndownstream tasks do not contain corresponding conformational information, \nso we obtain molecular conformers through a multi-stage generation method. \nWe first remove the hydrogen atoms from the molecule and use \nMMFFOptimizeMolecule() in RDKit with MMFF94 (Merk Molecular Force Field \n94) and a maximum number of iterations ğ‘–ğ‘¡ğ‘’ğ‘Ÿ = 5000 to generate conformers \nin a pre-determined coordinate system. Then, we judge whether the \ngenerated conformer has converged. If the conformer does not converge, we \nincrease the maximum number of iterations by ğ‘–ğ‘¡ğ‘’ğ‘Ÿ = ğ‘–ğ‘¡ğ‘’ğ‘Ÿ Ã— 2 and repeat this \nprocess 10 times until convergence. Finally, if RDKit fails to generate a \nconformer or the conformer has not converged after 10 attempts, we directly \nuse the conformer from PubChem instead. \n \nMolecular video generation \nAfter obtaining molecular conformers, these conformers undergo \ncounterclockwise rotations ğ‘…â—(ğ‘Ÿ) (â— âˆˆ {ğ‘¥,ğ‘¦,ğ‘§ }) about the positive ğ‘¥, ğ‘¦, and ğ‘§ \naxes to generate ğ‘›! snapshots. Here ğ‘Ÿ is from 0 to 19 and ğ‘›! = 60 snapshots \nare generated for all axes. Specifically, the matrix represents a \ncounterclockwise rotation about the positive ğ‘§-axis by ğ‘›-th angle, which can \n23 \n \nbe expressed as: \nğ‘…\"(ğ‘Ÿ) = 4\ncos ğ‘›ğœ™ âˆ’ sin ğ‘›ğœ™ 0\nsin ğ‘›ğœ™ cos ğ‘›ğœ™ 0\n0 0 1\n= (1)  \nwhere ğœ™ =\n#\n$%. We generate a molecular frame ğ‘£&\n' = â„(Ã—**+Ã—**+ (the ğ‘—-th frame \nof the ğ‘–-th molecule) for each snapshot using PyMOL (A software for \nvisualization and rendering of molecular 3D structures)60 with stick-ball mode. \nFinally, these 60 frames are stitched sequentially to generate molecular \nvideos ğ’± ={ğ‘£$, ğ‘£*, . . . , ğ‘£,|ğ‘£& âˆˆ â„,!Ã—(Ã—**+Ã—**+} (where ğ‘› represents the number \nof molecules). \n \nStrategies for pre-training VideoMol \nPretraining aims to improve the model's ability to focus on crucial information \nin the molecular video, enabling more meaningful feature extraction.  \nIn this paper, to obtain information in molecular videos from different \nperspectives, we consider three pre-training tasks (Error! Reference source \nnot found.b-d): video-aware pre-training, direction-aware pre-training, and \nchemical-aware pre-training. Specifically, video-aware pre-training equips the \nmodel with the ability to distinguish different molecular videos, such as \nwhether the two frames are from the same video. Direction-aware pre-training \nenables the model to discriminate the relationship between each frame, such \nas the angle of difference between two frames. Chemical-aware pre-training \nhelps the model mine physiochemical-related information in videos. \n      Feature extraction. Considering the efficiency and scalability of \n24 \n \nVideoMol, we perform independent feature extraction on each frame in \nmolecular videos. Specifically, for the ğ‘—-th frame ğ‘£&\n' of the given ğ‘–-th video, we \nfeed it into the video encoder to obtain the frame feature â„&\n'. In pre-training, \ngiven a batch of ğ‘› molecular videos ğ‘£ âˆˆ {â‹ƒ {â‹ƒ ğ‘£&\n''-$,..,,!&-$,..,, }}, we randomly \nsample two frames ğ‘£0123 âˆˆ {ğ‘£$\n0123, â€¦ , ğ‘£,\n0123} and ğ‘£42&5 âˆˆ {ğ‘£$\n42&5, â€¦ , ğ‘£,\n42&5} from \neach video, where ğ‘£0123 and ğ‘£42&5 have the same axis of rotation. Then, we \ninput ğ‘£0123 and ğ‘£42&5 to video encoder to extract latent features â„0123 âˆˆ â„3 \nand â„42&5 âˆˆ â„3, where ğ‘‘ is the hidden dimension. Finally, the batch of data in \nthe form of feature matrix ğ» = KLâ„$\n0123, â„$\n42&5M, Lâ„*\n0123, â„*\n42&5M, â€¦ , Lâ„,0123, â„,42&5MN âˆˆ\nâ„,Ã—3Ã—* is constructed for the following pre-training. \n      Video-aware pretraining (VAP). Identifying the differences between \nvideos is important for the model to learn discriminative information between \nmolecules because a video only describes one molecule. Meanwhile, different \nframes in a video describe different views of molecules in 3D space, which \nleads to unstable representation of the model when extracting different frames \nfrom the same video. Therefore, we propose a video-based pre-training task \nto model the inter-video frame similarity and intra-video frame dissimilarity, \ni.e., frames from the same video should be close together, while frames from \ndifferent videos should be far apart. Specifically, our approach uses \ncontrastive learning to train molecular video representations, contrasting \npositive pairs of latent vectors against negative pairs. Given a batch of frame \nlatent matrix ğ», the ğ»âˆ— âˆˆ â„*,Ã—3 is obtained by flattening ğ». We define two \n25 \n \nframes from the same molecular video as positive pair and the others as \nnegative. Therefore, the samples that can form positive pairs with the i78 \nsample in ğ»âˆ— are itself and the [ğ‘– + ğ‘›(mod\t2ğ‘›)] + 1 sample. Thus, our VAP \nobjective is formalized based on InfoNCE loss as follows: \nâ„’9 = argmin\n:\n1\n2ğ‘› Yâˆ’log\nâˆ‘ ğ‘’0\"\tâˆ™\t0\"\nâˆ—$\t/\t> + ğ‘’0\"\tâˆ™\t?0[\"&'\t(*+,\t-')]&0\nâˆ— @\n$\n\t/\t>*,\n&-$\nâˆ‘ âˆ‘ ğ‘’0\"\tâˆ™\t01\n$\t/\t>*,\n'-$\n*,\n&-$\n\\ (2)  \n      where â„&\nâˆ— is the ğ‘–-th latent vector in ğ»âˆ—, Ï„ is the temperature parameter, \nand Î¸ is the parameters of the video encoder. \n      Direction-aware pretraining (DAP). Correlating two snapshots from a \ncontinuously rotating object is trivial for humans, which benefits from the \nhuman ability to reason and imagine the 3D structure of objects based on \nprior knowledge. For example, humans can easily associate occluded regions \nwhen they only observe limited unoccluded local regions. Therefore, to equip \nthe model with such ability, we propose direction-aware pretraining (DAP), \nwhich consists of three prediction tasks: i) axis, ii) rotation, and iii) angle \nprediction. First, a residual matrix ğ»A is generated by substracting the first \nchannel with the second, e,g. ğ»A = Lâ„$\n0123 âˆ’ â„$\n42&5, â„*\n0123 âˆ’ â„*\n42&5, â€¦ , â„,0123 âˆ’\nâ„,42&5M âˆˆ â„,Ã—3. Then, the features hA of each row in the residual matrix ğ»A are \npassed separately through three classifiers (Multi-Layer Perceptrons), namely \naxis classifier ğ‘“2B&C, rotation classifier ğ‘“DE424&E,, and angle classifier ğ‘“2,F15. The \nclassifiers are trained to predict the axis of rotation ğ‘¦2B&C (x, y, or z), the \ndirection of rotation ğ‘¦DE424&E, (clockwise or counterclockwise), and the angle of \n26 \n \nrotation ğ‘¦2,F51 (an integer from 1 to 19) of â„42&5 with respect to â„0123, \nrespectively. Cross-entropy loss is used for these MLPs, and the DAP losses \nare defined as follows: \nâ„’â—(ğ»A, ğ‘¦â—) = argmin\n:,Gâ—\n1\nğ‘› âˆ™ ğ¾â—\nYâˆ’ c c ğ‘¦â—,,H\nIâ—\nH-$\nlog ğ‘“â—\nH(â„,)\n,\n,-$\n\\ (3)  \n      where â— represents one of the prediction tasks (i.e., axis, rotation, angle), \nğ‘¦â— is the ground truth of corresponding prediction task in vector form, ğ‘ŠJâ— is \nthe parameters of the classifier predicting ğ‘¦â—, ğ¾â— is the number of \ncorresponding categories, and Î¸ is the parameters of the video encoder. \n      Chemical-aware pretraining (CAP). Our previous research18 \ndemonstrated that the chemical knowledge is important for improving the \nperformance of drug discovery, which proposed Multi-Granularity Chemical \nClusters Classification (MG3C) with MACCS fingerprint to identify the \nchemical information of molecules. Here, we extend MG3C task to further \nmine more physicochemical information and introduced Multi-Chemical \nSemantics Clustering (MCSC) with 20 additional fingerprint descriptors \n(details are provided in Supplementary Section B.1 and Supplementary \nTable 24). In details, we first extract 21 molecular fingerprints for the \nmolecules corresponding to ğ‘£0123 and ğ‘£42&5, and reduce the dimensionality of \neach fingerprint to 100 dimensions using PCA (Principal Components \nAnalysis)61. We stitch together the reduced molecular fingerprints to get the \nMCSC fingerprint ğ‘ âˆˆ â„*$%%. Then, we use ğ‘˜-Means to cluster MSCS \nfingerprints (molecules with similar MCSC distances were grouped together) \n27 \n \nand assign a corresponding pseudo-label ğ‘¦KLM to each cluster. We chose ğ‘˜ =\n100 as the appropriate number of clusters. Finally, we employed a chemical \nclassifier, which is MLP with ğ‘ŠK as its parameters, to predict the pseudo-\nlabels from the latent frame vectors â„0123 and â„42&5. The cost function of the \nCAP task can be formalized as follows: \nâ„’ğ¶ = argmin\n:,Gğ¶\n1\n2ğ‘› c hâ„“(ğ‘ŠK âˆ™ â„,\n0123, ğ‘¦KLM) + â„“jğ‘ŠK âˆ™ â„,\n42&5, ğ‘¦KLMkl\n,\n,-$\n(4) \n      where â„“ is the multinomial logistic loss or the negative log-softmax \nfunction, and Î¸ is the parameters of the video encoder. \n \nPre-training process \n      To pre-train VideoMol, we sample 2 million unlabeled molecules and their \ncorresponding conformers from the PCQM4Mv2 (a public access database on \nquantum chemistry, Supplementary Section A.1 for details), and generate \nmolecular videos with 60 frames (See Section Methods for video generation \ndetails). We randomly sample 90% of molecular videos for training and the \nremaining 10% for evaluation in pre-training stage. The pre-training of \nVideoMol includes three important components, which are video encoder \nselection, data augmentation and training process, respectively. \nVideo encoder selection \n      Encouraged by the surprising performance of vision transformers (ViT)62 \nin computer vision, we use a 12-layer ViT as the video encoder of VideoMol. \nFor each frame in the video, the video encoder splits it into 16 Ã— 16 patches \n28 \n \nas input and extracts 384-dimensional features. See Supplementary Table \n25, Supplementary Section A.2 and Supplementary Section B.2 for more \ndetails of hyperparameters and model in pre-training stage. \nData augmentations \n      Data augmentation is a simple and effective method to improve the \ngeneralization and robustness of model and is widely used in various artificial \nintelligence tasks. Because molecular videos contain structural and geometric \ninformation about compounds, enhancement methods that affect this \ninformation, such as RandomRotation and RandomFlip, cannot be used. \nHere, we have chosen four data enhancement methods: (1) CenterCrop with \na size of 224; (2) RandomGrayscale with 30% probability of occurrence; (3) \nColorJitter with brightness, contrast, saturation of (0.6, 1.4) and 30% \nprobability of occurrence; (4) GaussianBlur with a kernel size of 3, sigma of \n(0.1, 2.0) and 30% probability of occurrence. Molecular videos are \nsequentially processed by these augmentations and normalized using \nNormalize with ImageNet default mean (0.485, 0.456, 0.406) and default \nstandard deviation (0.229, 0.224, 0.225). These augmentation methods are \nprovided by the PyTorch library63. \nTotal loss \n      Since the pre-training process involves multiple optimization objectives \n(e.g. â„’9, â„’2B&C, â„’DE424&E,, â„’2,F51 and â„’K), we use a weighted multi-objective \noptimization algorithm to allow the model to benefit from each pre-training \n29 \n \ntask in a balanced manner, whose core idea is to use variable weights related \nto loss of task to control the pre-training tasks that the model focuses on. \nSpecifically, we compute the loss weights by Î»â— =\nâ„’â—\nâ„’233\nÃ— ğ‘›42CH (â„’LOO = â„’9 +\nâ„’2B&C + â„’DE424&E, + â„’2,F51 + â„’K), where Î»â— represents any one of \nâ„’9, â„’2B&C, â„’DE424&E,, â„’2,F51 and â„’K and ğ‘›42CH = 5 represents the number of the \nloss. The final weighted multi-task loss can be formalized as: \nâ„’P1&F0413 = ğœ†9â„’9 + ğœ†2B&Câ„’2B&C + ğœ†DE424&E,â„’DE424&E, + ğœ†2,F51â„’2,F51 + ğœ†Kâ„’K (5) \nFinally, we use the weighted loss function ğ¿P1&F0413 to optimize the parameters \nof VideoMol by using mini-batch stochastic gradient descent. See \nSupplementary Section A.2, Supplementary Section B.2 and \nSupplementary Table 25 for more pre-training details and see \nSupplementary Section C.1 and Supplementary Figures 3-4 for pre-\ntraining logging. \n \nFine-tuning process \nAfter pre-training, we add an external multi-layer perceptron (MLP) after the \nvideo encoder for fine-tuning of downstream tasks. In the MLP, the number of \noutput neurons in the last layer is equal to the number of downstream tasks \nğ‘›42CH. In details, given a batch of ğ‘› molecular videos with ğ‘›! frames ğ‘£ âˆˆ\n{â‹ƒ {â‹ƒ ğ‘£&\n''-$,..,,!&-$,..,, }\t|\tğ‘£&\n' âˆˆ â„(Ã—**+Ã—**+}, we input each frame ğ‘£&\n' in molecular \nvideos ğ‘£ into video encoder to extract latent features â„ âˆˆ\n{â‹ƒ {â‹ƒ â„&\n''-$,..,,!&-$,..,, }\t|\tâ„&\n' âˆˆ â„3}, where ğ‘‘ is the hidden dimension. Then, we \n30 \n \nfurther forward-propagate latent features into the external MLP to obtain the \nlogit ğ‘™ âˆˆ {â‹ƒ {â‹ƒ ğ‘™&\n''-$,..,,!&-$,..,, }\t|\tğ‘™&\n' âˆˆ â„3} (relevant to downstream tasks) of each \nframe. Since different frames of the same video describe the same molecule, \nwe average the logit of frames from the same video as the final logit of the \nmolecule ğ‘¦ âˆˆ {â‹ƒ {ğ‘¦&\n&-$,..,, }\t|\tğ‘¦& âˆˆ â„,4567}. Finally, we use cross-entropy loss to \noptimize classification tasks and MSE (Mean Square Error) or Smooth L1 loss \nto optimize regression tasks. \n \nDownstream details \n      Datasets and splitting methods. In binding activity prediction task, we \nuse 10 kinase targets in compound-kinase binding activity prediction and use \n10 GPCR targets in ligand-GPCR binding activity prediction, which can be \nobtained from ImageMol18 (Supplementary Table 26 for statistical details). \nWe use the same splitting method as ImageMol in kinase and GCPR targets, \nwhich uses a balanced scaffold split to divide the dataset into 80% training \nset, 10% validation set and 10% test set. In molecular property prediction \ntask, we conduct experiments on 12 common benchmarks from the \nMoleculeNet64 (Supplementary Table 27). Following previous works6, 39, we \nsplit all property prediction datasets using scaffold split, which splits molecules \naccording to molecular substructure with 8:1:1. The scaffold split is a \nchallenging splitting method for evaluating the generalization ability of models \nto out-of-distribution data samples. The balanced scaffold split ensures the \n31 \n \nbalance of the scaffold size in the training set, validation set and test set. In \nanti-SARS-CoV-2 activity prediction task, we use the same data and \nexperimental setting as REDIAL-202041 and ImageMol on 11 SARS-CoV-2 \nactivity prediction tasks (Supplementary Table 28).  \nMetrics. As suggested by MoleculeNet, we use ROC-AUC as the evaluation \nmetric for the classification task, including 10 GPCR target activity predictions \nand 7 property predictions (BBBP, Tox21, ClinTox, HIV, BACE, SIDER and \nToxCast). For the regression prediction of the remaining 6 molecular \nproperties, we use RMSE (Root Mean Squared Error) and MAE (Mean \nAbsolute Error) to evaluate FreeSolv, ESOL, lipophilicity and QM7, QM8, \nQM9, respectively. For compound-GPCR binding activity prediction, we report \nRMSE and MAE metrics. For anti-SARS-CoV-2 activity prediction task, we \nreport ROC-AUC metric. All results are performed on three independent runs \nand the mean and standard deviation are reported. See Supplementary \nTables 29-30 and Supplementary Section A.3 for more details of \nhyperparameters in fine-tuning stage. \n \nData Availability \nThe datasets used in this project can be found at the following links: 2 million \npre-training dataset: https://ogb.stanford.edu/docs/lsc/pcqm4mv2/; 10 \nGPCRs: https://drive.google.com/file/d/1HVHrxJfW16-5uxQ-\n7DxgQTxroXxeFDcQ/view?usp=sharing (Supplementary Table 26); 10 \n32 \n \nkinases: https://lincs.hms.harvard.edu/kinomescan/ (Supplementary Table \n26); 14 molecular property prediction datasets: https://deepchemdata.s3-us-\nwest-1.amazonaws.com/datasets/BBBP.csv (Replace the BBBP in the \nhyperlink with another dataset name to download other datasets) \n(Supplementary Table 27); 11 SARS-CoV-2 targets: \nhttps://opendata.ncats.nih.gov/covid19/assays (Supplementary Table 28); \n \nCode Availability \nAll of the codes and the trained models are available at \nhttps://github.com/ChengF-Lab/VideoMol. \n \n \nAcknowledgments \nFunding: This project has been funded in whole or in part with federal funds \nfrom the National Cancer Institute, National Institutes of Health, under \ncontract HHSN261201500003I. The content of this publication does not \nnecessarily reflect the views or policies of the Department of Health and \nHuman Services, nor does mention of trade names, commercial products, or \norganizations imply endorsement by the U.S. Government. This Research \nwas supported [in part] by the Intramural Research Program of the NIH, \nNational Cancer Institute, Center for Cancer Research. \n \n33 \n \nAuthor Contributions Statement \nX.Z., F.C., and H.X. conceived and designed the study. H.X., L.H. constructed \nthe databases . H.X. designed framework, and developed the codes, and \nperformed all experiments. H.X., L.Z., X.Z., and F.C. performed data analyses. \nH.X., X.Z., F.C., Y.Q., R.N., J.H., M.R.Z. discussed and interpreted all results. \n H.X., L.Z., X.Z., and F.C. wrote and critically revised the manuscript. \n \nCompeting Interests Statement \nThe authors have declared no competing interests. \n \n  \n34 \n \nReferences \n1. Smith, A. Screening for drug discovery: The leading question. Nature \n418, 453-455 (2002). \n2. Gorgulla, C. et al. An open-source drug discovery platform enabl es ultra-\nlarge virtual screens. Nature 580, 663-668 (2020). \n3. Schultz, D.C. et al. Pyrimidine inhibitors synergize with nucleo side \nanalogues to block SARS-CoV-2. Nature 604, 134-140 (2022). \n4. Lam, H.Y.I. et al. Application of variational graph encod ers as an effective \ngeneralist algorithm in computer-aided drug design. Nat. Mach. Intell. 5, \n754-764 (2023). \n5. Gentile, F. et al. Artificial intelligenceâ€“enabled virtu al screening of ultra-\nlarge chemical libraries with deep docking. Nature Protocols 17, 672-697 \n(2022). \n6. Wang, Y., Wang, J., Cao, Z. & Barati Farimani, A. Molecular contrastive \nlearning of representations via graph neural networks. Nat. Mach. Intell. \n4, 279-287 (2022). \n7. Xue, D. et al. X-MOL: large-scale pre-training for molecular \nunderstanding and diverse molecular analysis. Sci. Bull.  67, 899 -902 \n(2022). \n8. Liu, G. et al. GraphDTI: A robust deep learning predicto r of drug-target \ninteractions from multiple heterogeneous data. J. Cheminformat. 13, 1-\n17 (2021). \n9. Wang, M. et al. Deep learning approaches for de novo drug design: An \noverview. Current Opinion in Structural Biology 72, 135-144 (2022). \n10. Wieder, O. et al. A compact review of molecular property predi ction with \ngraph neural networks. Drug Discovery Today: Technologies  37, 1-12 \n(2020). \n11. Wigh, D.S., Goodman, J.M. & Lapkin, A.A. A review of molecular \nrepresentation in the age of machine learning. Wiley Interdisciplinary \nReviews: Comput. Mol. Sci. 12, e1603 (2022). \n12. Raevsky, O.A. Physicochemical descriptors in property-based drug \ndesign. Mini reviews in medicinal chemistry 4, 1041-1052 (2004). \n13. Sun, H. Pharmacophore-based virtual screening. Current medicinal \nchemistry 15, 1018-1024 (2008). \n14. Weininger, D. SMILES, a chemical language and information syst em. 1. \nIntroduction to methodology and encoding rules. Journal of chemical \ninformation and computer sciences 28, 31-36 (1988). \n15. Heller, S.R., McNaught, A., Pletnev, I., Stein, S. & Tchekho vskoi, D. InChI, \nthe IUPAC international chemical identifier. J. Cheminformat.  7, 1 -34 \n(2015). \n16. Hu, W. et al. Strategies For Pre-training Graph Neural Networks.  \nInternational Conference on Learning Representations (ICLR)  (ICLR, \n2020). \n35 \n \n17. Rong, Y. et al. Self-supervised graph transformer on large-scale \nmolecular data. Advances in Neural Information Processing Systems 33, \n12559-12571 (MIT Press, 2020). \n18. Zeng, X. et al. Accurate prediction of molecular properties and  drug \ntargets using a self-supervised image representation learning framework. \nNat. Mach. Intell. 4, 1004-1016 (2022). \n19. Dai, H., Dai, B. & Song, L. Discriminative embeddings of lat ent variable \nmodels for structured data. International conference on machine \nlearning, 2702-2711 (PMLR, 2016). \n20. Wang, J. et al. Self-supervised video representation learning by \nuncovering spatio-temporal statistics. IEEE Transactions on Pattern \nAnalysis and Machine Intelligence 44, 3791-3806 (2021). \n21. Wang, R. et al. Masked video distillation: Rethinking maske d feature \nmodeling for self-supervised video representation learning. Proceedings \nof the IEEE/CVF Conference on Computer Vision and Pattern \nRecognition, 6312-6322 (2023). \n22. Duan, H., Zhao, N., Chen, K. & Lin, D. Transrank: Self-supe rvised video \nrepresentation learning via ranking-based transformation recogniti on. \nProceedings of the IEEE/CVF Conference on Computer Vision and \nPattern Recognition, 3000-3010 (2022). \n23. Selvaraju, R.R. et al. Grad-cam: Visual explanations from deep  networks \nvia gradient-based localization. Proceedings of the IEEE international \nconference on computer vision, 618-626 (2017). \n24. Bemis, G.W. & Murcko, M.A. The properties of known drugs. 1. \nMolecular frameworks. J. Med. Chem. 39, 2887-2893 (1996). \n25. Honda, S., Shi, S. & Ueda, H.R.J.a.p.a. SMILES transformer: pre-tra ined \nmolecular fingerprint for low data drug discovery. arXiv:1911.047 38 \n(2019). \n26. Kim, H., Lee, J., Ahn, S. & Lee, J.R. A merged molecular represen tation \nlearning for molecular properties prediction with a web-based se rvice. \nSci. Rep. 11, 1-9 (2021). \n27. Breiman, L. Random forests. Machine learning 45, 5-32 (2001). \n28. Hearst, M.A., Dumais, S.T., Osuna, E., Platt, J. & Scholkopf, B. Su pport \nvector machines. IEEE Intelligent Systems and their applications 13, 18-\n28 (1998). \n29. Welling, M. & Kipf, T.N. Semi-supervised classification with graph  \nconvolutional networks. International Conference on Learning \nRepresentations (ICLR 2017). \n30. Xu, K., Hu, W., Leskovec, J. & Jegelka, S. How powerful are grap h \nneural networks? arXiv:1810.00826 (2018). \n31. Qiu, J. et al. Gcc: Graph contrastive coding for graph neural network pre-\ntraining. Proceedings of the 26th ACM SIGKDD International \nConference on Knowledge Discovery & Data Mining , 1150-1160 (KDD, \n2020). \n36 \n \n32. Hu, Z., Dong, Y., Wang, K., Chang, K.-W. & Sun, Y. Gpt-g nn: Generative \npre-training of graph neural networks. Proceedings of the 26th ACM \nSIGKDD International Conference on Knowledge Discovery & Data \nMining, 1857-1867 (2020). \n33. Zhang, Z., Liu, Q., Wang, H., Lu, C. & Lee, C.-K. Moti f-based Graph Self-\nSupervised Learning for Molecular Property Prediction. Advances in \nNeural Information Processing Systems 34 (MIT Press, 2021). \n34. Xu, M., Wang, H., Ni, B., Guo, H. & Tang, J. Self-supervised graph-level \nrepresentation learning with local and global structure. International \nConference on Machine Learning, 11548-11558 (PMLR, 2021). \n35. You, Y. et al. Graph contrastive learning with augmentati ons. Advances \nin Neural Information Processing Systems  33, 5812-5823 (MIT Press, \n2020). \n36. Li, P. et al. An effective self-supervised framework for learning  \nexpressive molecular global representations to drug discovery. Brief. \nBioinformat. 22, bbab109 (2021). \n37. StÃ¤rk, H. et al. 3D Infomax improves GNNs for Molecular Property \nPrediction. NeurIPS 2021 AI for Science Workshop (MIT Press, 2021). \n38. Liu, S. et al. Pre-training Molecular Graph Representation wi th 3D \nGeometry. International Conference on Learning Representations (ICLR, \n2021). \n39. Fang, X. et al. Geometry-enhanced molecular representation le arning \nfor property prediction. Nat. Mach. Intell. 4, 127-134 (2022). \n40. Zhou, G. et al. Uni-Mol: a universal 3D molecular representation learning \nframework.  (2023). \n41. Bocci, G. et al. A machine learning platform to estimate ant i-SARS-CoV-\n2 activities. Nat. Mach. Intell., 1-9 (2021). \n42. Gaulton, A. et al. The ChEMBL database in 2017. Nucleic Acids Res. 45, \nD945-D954 (2017). \n43. Hampel, H. et al. The Î²-secretase BACE1 in Alzheimerâ€™s disease. \nBiological psychiatry 89, 745-756 (2021). \n44. Wishart, D.S. et al. DrugBank 5.0: a major update to the D rugBank \ndatabase for 2018. Nucleic Acids Res. 46, D1074-D1082 (2018). \n45. Zou, Y. et al. Virtual screening and structure-based discovery of  indole \nacylguanidines as potent Î²-secretase (BACE1) inhibitors. Molecules 18, \n5706-5722 (2013). \n46. Berman, H.M. et al. The protein data bank. Nucleic Acids Res. 28, 235-\n242 (2000). \n47. Allen, W.J. et al.  ( \n48. Hinton, G.E. & Roweis, S. Stochastic neighbor embedding. Advances in \nneural information processing systems 15 (2002). \n49. Davies, D.L. & Bouldin, D.W. A cluster separation measure. IEEE \ntransactions on pattern analysis and machine intelligence , 224 -227 \n(1979). \n37 \n \n50. Gu, T. et al. Development and structural modification of BAC E1 inhibitors. \nMolecules 22, 4 (2016). \n51. Kimura, T. et al. Design and synthesis of potent Î²-secretase (BACE1 ) \ninhibitors with P1â€² carboxylic acid bioisosteres. Bioorganic & medicinal \nchemistry letters 16, 2380-2386 (2006). \n52. Garino, C. et al. BACE-1 inhibitory activities of new substit uted phenyl-\npiperazine coupled to various heterocycles: chromene, coumarin and \nquinoline. Bio. Med. Chem. Lett. 16, 1995-1999 (2006). \n53. Malamas, M.S. et al. Aminoimidazoles as potent and selective huma n Î²-\nsecretase (BACE1) inhibitors. J. Med. Chem. 52, 6314-6323 (2009). \n54. Hanessian, S., Hou, Y., Bayrakdarian, M. & Tintelnot-Blomley, M.  \nStereoselective synthesis of constrained oxacyclic hydroxyethylene \nisosteres of aspartic protease inhibitors: Aldol and Mukaiyama Aldol  \nmethodologies for branched tetrahydrofuran 2-carboxylic acids. J. Org. \nChem. 70, 6735-6745 (2005). \n55. RingnÃ©r, M. What is principal component analysis? Nat. Biotechnol. 26, \n303-304 (2008). \n56. Pedregosa, F. et al. Scikit-learn: Machine learning in Python. J. Mach. \nLearn. Res. 12, 2825-2830 (2011). \n57. He, Y., Yan, R., Fragkiadaki, K. & Yu, S.-I. Epipolar transfo rmers. \nProceedings of the ieee/cvf conference on computer vision and pat tern \nrecognition, 7779-7788 (2020). \n58. Qian, R. et al. Spatiotemporal contrastive video representati on learning. \nProceedings of the IEEE/CVF Conference on Computer Vision and \nPattern Recognition, 6964-6974 (2021). \n59. Hu, W. et al. Ogb -lsc: A large-scale challenge for machine learning on \ngraphs. arXiv preprint arXiv:2103.09430 (2021). \n60. DeLano, W.L. Pymol: An open-source molecular graphics tool. CCP4 \nNewsl. Protein Crystallogr 40, 82-92 (2002). \n61. MaÄ‡kiewicz, A. & Ratajczak, W. Principal components analysis (PCA). \nComputers & Geosciences 19, 303-342 (1993). \n62. Dosovitskiy, A. et al. An Image is Worth 16x16 Words: Transformers f or \nImage Recognition at Scale. International Conference on Learning \nRepresentations (ICLR, 2020). \n63. Paszke, A. et al. Pytorch: An imperative style, high-performance deep \nlearning library. Advances in neural information processing systems  32 \n(2019). \n64. Wu, Z. et al. MoleculeNet: a benchmark for molecular machine l earning. \nChem. Sci. 9, 513-530 (2018). \n \n  \n38 \n \nFigure Legends \n \nFig. 1 Overview of the VideoMol foundational model. a, Feature extraction \nof molecular videos. First, we render 2 million molecules with conformers in \n3D spatial structure. We then rotate the rendered molecule around the x, y, z \naxes and generate snapshots for each frame of the molecule video. Finally, \nwe feed the molecular frames into a video encoder to extract latent features. \nb-d, Three self-supervised tasks for pre-training video encoder. The direction-\naware pretraining (DAP) task is used to distinguish the relationship between \npairs of molecular frames (such as the axis of rotation, the direction of \nrotation, and the angle of rotation) by using axis classifier (orange), rotation \nclassifier (green) and angle classifier (blue). The video-aware pretraining \n(VAP) task is used to maximize intra-video similarity and minimize inter-video \nsimilarity. The chemical-aware pretraining (CAP) task is used to recognize \ninformation related to physicochemical structures in molecular videos by using \nchemical classifier (pink). e, The finetuning of VideoMol on downstream \nbenchmarks (such as binding activity prediction and molecular property \nprediction). A multi-layer perceptron (lavender) is added after the pre-trained \nvideo encoder for fine-tuning on four types of downstream tasks (20 target \nprediction, 13 property prediction, 11 SARS-CoV-2 inhibitor prediction and 4 \nvirtual screening and docking). We ensemble the results (logits) of each frame \nas the prediction result of molecular video (video logit). \n \n39 \n \nFig. 2 Performance of the VideoMol framework on various drug \ndiscovery tasks. a, The ROC-AUC curves of ImageMol and VideoMol on 10 \nmain types of biochemical kinases with balanced scaffold split. b, The RMSE \nand MAE performance of ImageMol and VideoMol on 10 GPCR with balanced \nscaffold split. c, The ROC-AUC curves of ImageMol and VideoMol on 7 \nmolecular property prediction benchmarks with scaffold split. d, The RMSE \n(FreeSolv, ESOL, Lipo) and MAE (QM7, QM8, QM9) performance of SOTA \nmethods and VideoMol with scaffold split. For ease of presentation, the values \nof FreesSolv and QM7 are scaled down by a factor of 2 and 100, respectively, \nand the values of QM8 and QM9 are scaled up by a factor of 50 and 100, \nrespectively. e, The ROC-AUC performance of REDIAL-2020, ImageMol and \nVideoMol on 11 SARS-CoV-2 datasets with balanced scaffold split. \n \nFig. 3 The virtual screening on four common targets (BACE1, COX-1, \nCOX-2 and EP4). a, The ROC-AUC curves of ImageMol and VideoMol on \nvalidation set (the first row) and test set (the second row). b, The t-SNE \nvisualization of latent features extracted by VideoMol. c, The drug discovery \non BACE1, COX-1, COX-2 and EP4. Green and red points represent \ninhibitors and non-inhibitors from the ChEMBL dataset, respectively. Green \npoints indicate known inhibitors. The decision boundary is drawn by training \nan SVM using ChEMBL dataset, where blue to red indicates that the \nprobability of belonging to the inhibitor gradually increases. d, Virtual \n40 \n \nscreening on known inhibitors of BACE1, COX-1, COX2 and EP4. The x-axis \nand y-axis represent the number of the drug and the predicted probability that \nthe drug is an inhibitor, respectively. The orange and blue backgrounds \nrepresent inhibitor area and non-inhibitor area, respectively. The green and \nblue triangles represent inhibitors predicted by ImageMol and VideoMol \nrespectively. The green and blue circles represent non-inhibitors predicted by \nImageMol and VideoMol respectively. The green and blue numbers represent \nthe precision of ImageMol and VideoMol respectively.  \n \nFig. 4 The docking analysis of the 4IVS crystal structure of Homo \nsapiens beta-secretase 1 (BACE1). a, The 4IVS crystal structure of BACE1. \nThe gray tetragon represents the area of the docking pocket. The grid score is \ncalculated by Dock6.10 (the smaller, the better). b, Top 20 drugs predicted by \nVideoMol and ImageMol to be active against the BACE1 target. Green and \nblue represent ImageMolâ€™s drugs and VideoMolâ€™s drugs respectively. \nMinus/plus signs indicate that the predicted drug is not/is supported by \nexisting experimental data from publish literatures. c, The docking results of \nthe Top 20 drugs predicted by VideoMol. The x-axis and y-axis represent the \nindex and grid scores of the drug respectively. Light blue and light orange \nareas indicate worse and better grid scores than the 4IVS (grid score=-52.47), \nrespectively. d, Docking examples of 6 drugs (Gonadorelin, Angiotensin II, \nEdotreotide gallium Ga-68, Cyanocobalamin, Isavuconazonium and Elbasvir) \n41 \n \nwith the best grid scores. The numerical value in the bracket represents the \ngrid score. \n \nFig. 5 Feature distribution and biological interpretation of VideoMol. a, \nVisualization of each frame in 100 molecular videos (60 frames for each \nvideo). Representations are extracted by VideoMol and dimensionally \nreduced by t-SNE. Different colors represent frames in different cluster \nvideos. DB index is anmetric to evaluate the clustering quality, and the larger \nthe value, the better the clustering performance. b, Similarity distribution of \nintra-video and inter-video. Similarity is computed using a pair of frames from \nintra-video or inter-video. The content in brackets indicates the average \nsimilarity of the distribution. c, t-SNE visualization of VideoMol fingerprints. \nDifferent colors represent different cluster labels (this cluster label is obtained \nin the chemical-aware pretrainin task). d-f, Grad-CAM visualization of \nVideoMol on molecular frames. Warmer/cooler colors indicate that the region \nis more/less important for the modelâ€™s inferences. The light blue indicates the \nlowest importance (a value of 0). We use 0.6 as the threshold for \nvisualization, that is, set the importance lower than 0.6 to 0. In d, each row \nrepresents a molecular video. In e, Pairs of molecular frames represent \nframes where structure is missing and frames where structure appears, \nrespectively. In f, Each panel represents examples of key structures related to \nBACE-1 inhibitory activities from frames of different molecules. \nFigures\nFigure 1\nOverview of the VideoMol foundational model. a, Feature extraction of molecular videos. First, we render\n2 million molecules with conformers in 3D spatial structure. We then rotate the rendered molecule\naround the x, y, z axes and generate snapshots for each frame of the molecule video. Finally, we feed the\nmolecular frames into a video encoder to extract latent features. b-d, Three self-supervised tasks for pre-\ntraining video encoder. The directionaware pretraining (DAP) task is used to distinguish the relationship\nbetween pairs of molecular frames (such as the axis of rotation, the direction of rotation, and the angle\nof rotation) by using axis classi\u0000er (orange), rotation classi\u0000er (green) and angle classi\u0000er (blue). The\nvideo-aware pretraining (VAP) task is used to maximize intra-video similarity and minimize inter-video\nsimilarity. The chemical-aware pretraining (CAP) task is used to recognize information related to\nphysicochemical structures in molecular videos by using chemical classi\u0000er (pink). e, The \u0000netuning of\nVideoMol on downstream benchmarks (such as binding activity prediction and molecular property\nprediction). A multi-layer perceptron (lavender) is added after the pre-trained video encoder for \u0000ne-\ntuning on four types of downstream tasks (20 target prediction, 13 property prediction, 11 SARS-CoV-2\ninhibitor prediction and 4 virtual screening and docking). We ensemble the results (logits) of each frame\nas the prediction result of molecular video (video logit).\nFigure 2\nPerformance of the VideoMol framework on various drug discovery tasks. a, The ROC-AUC curves of\nImageMol and VideoMol on 10 main types of biochemical kinases with balanced scaffold split. b, The\nRMSE and MAE performance of ImageMol and VideoMol on 10 GPCR with balanced scaffold split. c, The\nROC-AUC curves of ImageMol and VideoMol on 7 molecular property prediction benchmarks with\nscaffold split. d, The RMSE (FreeSolv, ESOL, Lipo) and MAE (QM7, QM8, QM9) performance of SOTA\nmethods and VideoMol with scaffold split. For ease of presentation, the values of FreesSolv and QM7\nare scaled down by a factor of 2 and 100, respectively, and the values of QM8 and QM9 are scaled up by\na factor of 50 and 100, respectively. e, The ROC-AUC performance of REDIAL-2020, ImageMol and\nVideoMol on 11 SARS-CoV-2 datasets with balanced scaffold split.\nFigure 3\nThe virtual screening on four common targets (BACE1, COX-1, COX-2 and EP4). a, The ROC-AUC curves\nof ImageMol and VideoMol on validation set (the \u0000rst row) and test set (the second row). b, The t-SNE\nvisualization of latent features extracted by VideoMol. c, The drug discovery on BACE1, COX-1, COX-2\nand EP4. Green and red points represent inhibitors and non-inhibitors from the ChEMBL dataset,\nrespectively. Green points indicate known inhibitors. The decision boundary is drawn by training an SVM\nusing ChEMBL dataset, where blue to red indicates that the probability of belonging to the inhibitor\ngradually increases. d, Virtual screening on known inhibitors of BACE1, COX-1, COX2 and EP4. The x-axis\nand y-axis represent the number of the drug and the predicted probability that the drug is an inhibitor,\nrespectively. The orange and blue backgrounds represent inhibitor area and non-inhibitor area,\nrespectively. The green and blue triangles represent inhibitors predicted by ImageMol and VideoMol\nrespectively. The green and blue circles represent non-inhibitors predicted by ImageMol and VideoMol\nrespectively. The green and blue numbers represent the precision of ImageMol and VideoMol\nrespectively.\nFigure 4\nThe docking analysis of the 4IVS crystal structure of Homo sapiens beta-secretase 1 (BACE1). a, The\n4IVS crystal structure of BACE1. The gray tetragon represents the area of the docking pocket. The grid\nscore is calculated by Dock6.10 (the smaller, the better). b, Top 20 drugs predicted by VideoMol and\nImageMol to be active against the BACE1 target. Green and blue represent ImageMolâ€™s drugs and\nVideoMolâ€™s drugs respectively. Minus/plus signs indicate that the predicted drug is not/is supported by\nexisting experimental data from publish literatures. c, The docking results of the Top 20 drugs predicted\nby VideoMol. The x-axis and y-axis represent the index and grid scores of the drug respectively. Light\nblue and light orange areas indicate worse and better grid scores than the 4IVS (grid score=-52.47),\nrespectively. d, Docking examples of 6 drugs (Gonadorelin, Angiotensin II, Edotreotide gallium Ga-68,\nCyanocobalamin, Isavuconazonium and Elbasvir) with the best grid scores. The numerical value in the\nbracket represents the grid score.\nFigure 5\nFeature distribution and biological interpretation of VideoMol. a, Visualization of each frame in 100\nmolecular videos (60 frames for each video). Representations are extracted by VideoMol and\ndimensionally reduced by t-SNE. Different colors represent frames in different cluster videos. DB index is\nanmetric to evaluate the clustering quality, and the larger the value, the better the clustering\nperformance. b, Similarity distribution of intra-video and inter-video. Similarity is computed using a pair\nof frames from intra-video or inter-video. The content in brackets indicates the average similarity of the\ndistribution. c, t-SNE visualization of VideoMol \u0000ngerprints. Different colors represent different cluster\nlabels (this cluster label is obtained in the chemical-aware pretrainin task). d-f, Grad-CAM visualization of\nVideoMol on molecular frames. Warmer/cooler colors indicate that the region is more/less important for\nthe modelâ€™s inferences. The light blue indicates the lowest importance (a value of 0). We use 0.6 as the\nthreshold for visualization, that is, set the importance lower than 0.6 to 0. In d, each row represents a\nmolecular video. In e, Pairs of molecular frames represent frames where structure is missing and frames\nwhere structure appears, respectively. In f, Each panel represents examples of key structures related to\nBACE-1 inhibitory activities from frames of different molecules.\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nSupplementaryTablesS1S18Cheng.zip\nSupplementaryInformationCheng.pdf\nSupplemntaryVideoFig.5Cheng.zip",
  "topic": "Drug discovery",
  "concepts": [
    {
      "name": "Drug discovery",
      "score": 0.7904369831085205
    },
    {
      "name": "Interpretability",
      "score": 0.7638183832168579
    },
    {
      "name": "Computer science",
      "score": 0.6117639541625977
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5998843908309937
    },
    {
      "name": "Drug repositioning",
      "score": 0.4906824827194214
    },
    {
      "name": "Deep learning",
      "score": 0.46875548362731934
    },
    {
      "name": "Artificial intelligence",
      "score": 0.436308890581131
    },
    {
      "name": "Representation (politics)",
      "score": 0.4342867434024811
    },
    {
      "name": "Computational biology",
      "score": 0.37953850626945496
    },
    {
      "name": "Machine learning",
      "score": 0.3659880757331848
    },
    {
      "name": "Drug",
      "score": 0.307265043258667
    },
    {
      "name": "Bioinformatics",
      "score": 0.1962619125843048
    },
    {
      "name": "Biology",
      "score": 0.13353773951530457
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Pharmacology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I58956616",
      "name": "Case Western Reserve University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I36373038",
      "name": "Cleveland Clinic Lerner College of Medicine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I16609230",
      "name": "Hunan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I95240055",
      "name": "Northeast Ohio Medical University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210130649",
      "name": "Frederick National Laboratory for Cancer Research",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210140884",
      "name": "National Cancer Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I16391192",
      "name": "Tel Aviv University",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I1341412227",
      "name": "IBM (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I197251160",
      "name": "Hebrew University of Jerusalem",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I4210167297",
      "name": "IBM Research - Haifa",
      "country": "IL"
    }
  ],
  "cited_by": 3
}