{
    "title": "A deep language model for software code",
    "url": "https://openalex.org/W2497764072",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2754964408",
            "name": "Dam, Hoa Khanh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2615539865",
            "name": "Tran, Truyen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3093135139",
            "name": "Pham Trang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1994573369",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1520465330",
        "https://openalex.org/W2963371736",
        "https://openalex.org/W2953077455",
        "https://openalex.org/W2018389835",
        "https://openalex.org/W2142403498",
        "https://openalex.org/W2108563286",
        "https://openalex.org/W2143960295",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2138204974",
        "https://openalex.org/W2165747537"
    ],
    "abstract": "Existing language models such as n-grams for software code often fail to capture a long context where dependent code elements scatter far apart. In this paper, we propose a novel approach to build a language model for software code to address this particular issue. Our language model, partly inspired by human memory, is built upon the powerful deep learning-based Long Short Term Memory architecture that is capable of learning long-term dependencies which occur frequently in software code. Results from our intrinsic evaluation on a corpus of Java projects have demonstrated the effectiveness of our language model. This work contributes to realizing our vision for DeepSoft, an end-to-end, generic deep learning-based framework for modeling software and its development process.",
    "full_text": "A deep language model for software code\nHoa Khanh Dam\nUniversity of Wollongong\nAustralia\nhoa@uow.edu.au\nTruyen Tran\nDeakin University\nAustralia\ntruyen.tran@deakin.edu.au\nTrang Pham\nDeakin University\nAustralia\nphtra@deakin.edu.au\nABSTRACT\nExisting language models such as n-grams for software code\noften fail to capture a long context where dependent code\nelements scatter far apart. In this paper, we propose a novel\napproach to build a language model for software code to\naddress this particular issue. Our language model, partly\ninspired by human memory, is built upon the powerful deep\nlearning-based Long Short Term Memory architecture that\nis capable of learning long-term dependencies which occur\nfrequently in software code. Results from our intrinsic eval-\nuation on a corpus of Java projects have demonstrated the\neﬀectiveness of our language model. This work contributes\nto realizing our vision for DeepSoft, an end-to-end, generic\ndeep learning-based framework for modeling software and its\ndevelopment process.\n1. INTRODUCTION\nCode is arguably the most important software artifact.\nDecades of research have been dedicated to understanding\nand modeling software code better. A useful representation\nof software code will support many important software en-\ngineering (SE) tasks such as code suggestion, code search,\ndefect prediction, bug localization, test case generation, code\npatch generation, and even developer modeling. While high-\nlevel representations such as an abstract syntax trees or\nhuman-engineered features (e.g., lines of code, code complex-\nity metrics, etc.) are useful for a certain analysis, they do\nnot reveal the semantics hidden deeply in source code. On\nthe other hand, low-level representations such as concrete\nsyntax trees do not scale to the size of today’s software where\nprograms on the magnitude of a million lines of code are\ngetting more common.\nThe past few years have witnessed the rise of Natural\nLanguage Processing (NLP) with many ground-breaking re-\nsults in various applications such as speech recognition and\nmachine translation. The success of NLP is rooted in its\nstatistical, corpus-based methods and the availability of in-\ncreasingly large corpora of text. Software code is also written\nACM ISBN XXXXX.\nDOI: XXXXX\nin an (artiﬁcial) language, and has thus some characteristics\nthat are the same and diﬀerent with natural language text:\n1. Repetitiveness: code fragments tend to reoccur very\noften [4]. For example, loops such as for (int i =\n0; i < n; i++) or common statements such as Sys-\ntem.out.println(...) occur in many source ﬁles.\n2. Localness: source code is highly localized, i.e. there is\na certain distinct repetitive patterns in a local context\n[11]. For example, in a given source ﬁle, the pattern for\n(int size appears more often than the global pattern\nfor (int i.\n3. Rich and explicit structural information: software code\nhas highly complex structural information (which is\ndiﬀerent from natural language text). There are for\nexample nested loops and inheritance hierarchies oc-\ncurring frequently in source code.\n4. Long-term dependencies: a code element may depends\non other code elements which are not immediately be-\nfore it. For example, pairs of code tokens that are\nrequired to appear together due to programming lan-\nguage speciﬁcation (e.g., try and catch in Java) or due\nto API usage speciﬁcation (e.g., ﬁle open and close).\nRecent eﬀorts have developed NLP-inspired language mod-\nels for software code to realize some of those characteristics.\nThey built a very large code corpus (by collecting publicly\navailable source code from open source projects) and formed\nthe vocabulary of a language. This vocabulary V consists of\nN unique code tokens used across a substantial number of\nsoftware projects. Given a code sequence s= ⟨w1,w2,...,w k⟩\nwhere wt is a code token (e.g., s= ⟨for, (, int, i, =, 0,\ni, ++,...⟩), a language model estimates the probability\ndistribution P(s) of this code sequence as follows.\nP(s) = P(w1)\nk∏\nt=2\nP(wt |w1:t−1)\nwhere w1:t−1 = ( w1,w2,...,w t−1) is the historical context\nused to estimate the probability of the next code token wt.\nThe most popular language model is n-grams, which has\nalso been applied to model software code [6, 9]. This method\ntruncates the history length to n−1 words, and in practice\nn is often limited to a small integer (e.g., 2 to 5). Although\nn-grams models are useful and intuitive in making use of\nrepetitive sequential patterns in code, their context is re-\nstricted to a few code elements and is thus not suﬃcient\narXiv:1608.02715v1  [cs.SE]  9 Aug 2016\nin complex SE tasks. Recent work [11] introduces a local\ncache model, which captures local regularities, combining\nwith the global n-grams model. Although this approach\nhas successfully dealt with the localness of software code, it\nstill suﬀers from the small context problem inherently in the\nn-grams method.\nDeep learning-based approaches oﬀers a powerful alterna-\ntive to n-grams in representing software code. While deep\nfeed-forward architectures such as convolutional neural net-\nworks (CNNs) are eﬀective in capturing rich structural pat-\nterns in code [8], recurrent architectures like Recurrent Neural\nNetworks (RNNs) has the potential to capture a much longer\ncontext than n-grams [12]. RNNs however suﬀer from the van-\nishing or exploding gradients [1], and thus make it very hard\nto train for long sequences. Consider trying to predict next\ncode token in the code fragment: String conferences =\n[\"ICSE\", \"FSE\", \"ASE\"]; ..... for (String conf : .\nRecent information suggest that the next code token is prob-\nably a set of Strings, but if we want to narrow down to a\nspeciﬁc set, we need the context of conferences set, from\nfurther back. There could be a big gap between relevant\ninformation and the point where it is needed. Unfortunately,\nas that gap increases, RNNs will not be able to learn to\nconnect the information.\nIn this paper, we propose a novel approach to build a\nlanguage model for software code using Long Short-Term\nMemory (LSTM) [7], a special kind of RNN that is capa-\nble of learning long-term dependencies, i.e. remembering\ninformation from further back. LSTMs have demonstrated\nground-breaking performance in many applications such as\nmachine translation, video analysis, and speed recognition.\nResults from our preliminary evaluation have shown the\neﬀectiveness of LSTM, serving as concrete indication that\nLSTM is a promising model for software code. This code\nmodeling component is part of our vision for DeepSoft [3],\nan end-to-end, generic deep learning-based framework for\nmodeling software and its development process.\n2. APPROACH\nOur LSTM language model of software code is a (special)\nrecurrent neural network, which can be seen as multiple\ncopies of the same network (see multiple LSTM units in\nFigure 1), each passing information to a successor and thus\nallowing information to persist. Our language model reads\ncode tokens in a sequence one by one, and estimates the\nprobability of the next code token by the following steps.\nFirst, the current code token wt is mapped into a continuous\nspace using lookup table Msuch that wt = M(w) is a vector\nin RD. Vector wt then serves as an input to an LSTM unit.\nFor example, each code token (e.g., FileWriter) in Listing 1\nis embedded into an input vector (e.g., w1).\nA standard RNN unit would then generate the hidden state,\nrepresented by vector ht ∈RK, according to the previous\nhidden state ht−1 and the current input wt.\nht = σ(b+ Wtranht−1 + Winwt) (1)\nwhere σ is a nonlinear element-wise transform function, and\nb, Wtran and Win are parameters.\nFinally, the next code token is predicted using:\nP(w|w1:t−1) = exp\n(\nU⊤\nw ht−1\n)\n∑\nu∈V exp (U⊤u ht−1) (2)\nwhere Uw ∈RK is a free parameter.\nThe language model is then trained using many known\nsequences of code tokens s= ⟨w1,w2,...,w k⟩in a code cor-\npus. Learning is typically done by minimizing the log-loss\n−log P(s) with respect to model parameters θ, which are\n(b,Wtran,Win,M,U):\nL(θ) = −log P(w1) −\nk∑\nt=2\nlog P(wt |w1:t−1) (3)\nLearning involves computing the gradient of L(θ) during\nthe backpropagation phase, and parameters are updated\nusing a stochastic gradient descent (SGD). It means that\nparameters are updated after seeing only a small random\nsubset of sequences. The critical problem with standard\nRNNs however lies here where the magnitude of weights in\nthe transition matrix Wtran (used in Equation 1) can have a\nstrong eﬀect on the learning process. This is because during\nthe gradient backpropagation phase, the gradient signal can\nbe multiplied a large number of times (as many as the number\nof code tokens in a sequence) by the transition matrix. If the\nweights in this matrix are small, it can result in the problem\nof vanishing gradients where the gradient signal becomes so\nsmall that the learning gets very slow or even stopped. This\nmakes standard RNNs not suitable for learning long-term\ndependencies. On the other hand, the large weights can lead\nto a large gradient signal (e.g. exploding gradient), which\ncan cause learning to diverge.\nListing 1: A motivating example\nFileWriter w r i t e r = new FileWriter ( f i l e ) ;\nw r i t e r . write ( ‘ ‘ This i s an example ’ ’ ) ;\ni n t count = 0 ;\nSystem . out . p r i n l t l n ( ‘ ‘ Long gap ’ ’ ) ;\n. . . . . .\nw r i t e r . f l u s h ( ) ;\nw r i t e r . c l o s e ( ) ;\nLSTM\n LSTM\n LSTM\n LSTM\n LSTM...\nw1 w2 w3 wk-2 wk-1\nh1 h2 h3 hk-2 hk-1\nh0 h1\nLSTM\nw4\nh4\nFileWriter writer = new\nLSTM\nwk\nhk\n... close () ;\nwriter = new ...FileWriter () ; <eos>\nFigure 1: Long Short-Term Memory language model\nThe most powerful feature of our model (compared to the\nstandard RNNs model) resides in the memory cell (ct in\nFigure 2) of an LSTM unit. This cell stores accumulated\nmemory of the context, which is a mechanism to deal the\nvanish and exploding gradient problems. The amount of\ninformation ﬂowing through the memory cell is controlled\nby three gates (an input gate, a forget gate, and an output\ngate), each of which returns a value between 0 (i.e. complete\nblockage) and 1 (full passing through). All these gates are\nlearnable, i.e. being trained with the whole code corpus to\nmaximize the predictive performance of the language model.\nLet us now explain how an LSTM unit works. First, an\nLSTM unit decides how much information from the memory\nof previous context (i.e. ct−1) should be removed from the\nmemory cell. This is controlled by the forget gate ft, which\nlooks at the the previous output state ht−1 and the current\ncode token wt, and outputs a number between 0 and 1. A\nvalue of 1 indicates that all the past memory is preserved,\nwhile a value of 0 means “completely forget everything”.\nIn our example in Listing 1, the memory cell might keep\ninformation about the writer stream, so that it remembers\nthis open stream needs closing later. When we see this stream\nbeing closed, we want to forget it.\n*\nct\nft\nct-1 *\nit\not*\nht\nwt\nht-1\nwt\nht-1\nwt\nht-1\nwt ht-1\nFigure 2: The internal structure of an LSTM for\ncode processing\nThe next step is updating the memory with new informa-\ntion obtained from the current code token wt. In a similar\nmanner to the forget gate, the input gate it is used to control\nwhich new information will be stored in the memory. For\nexample, if wt is the new open stream, we want to add it\ninto the memory cell so that we remember closing it later.\nFinally, information stored in the memory cell will be used\nto produce an output ht. The output gate ot looks at the\ncurrent code token wt and the previous hidden state ht−1,\nand determines which parts of the memory should be output.\nFor example, since it just saw the token new, it might want\nto output information relevant (e.g. constructor forms) to\nthe initialization of the object declared previously, in case\nthat what is coming next. It is important to note here that\nLSTM computes the current hidden state based on not just\nonly the current input wt and the previous hidden state ht−1\n(as done in standard RNNs) but also the current memory\ncell state ct, which is linear with the previous memory ct−1.\nThis is the key feature allowing our model to learn long-term\ndependencies in software code.\n2.1 Model training\nThe bottleneck of our language model (as with any other\nRNN-based model) is the cost of normalization of the RHS\nof Equation (2) (i.e. the softmax function on the gradient\nlog), which scales with vocabulary size N = |V |. In a\ntypical software language model, the vocabulary size can be\nvery large (e.g., up to millions for large repositories). We\nhave employed an eﬃcient method called Noise Contrastive\nEstimation (NCE) [5], which has been used successfully in\nnatural language modeling (e.g., see [2]). Using NCE, the\ncomputation time now does not depend on the vocabulary\nsize, but on a user-deﬁned parameter k that approximates\nthe distribution in Equation (2). In practice, k is 100 or 200.\nTwo largest parameter matrices are the embedding table\nM∈ RD×N (which maps a code token into a continuous\nvector) and the prediction matrix U ∈RD×K used in Equa-\ntion (2). For a large vocabulary, the number of parameters\ngrows to millions, causing a potential overﬁtting. An eﬀec-\ntive solution is dropout [10], where the elements of input\nembedding and output states are randomly set to zeros dur-\ning training. During testing, a parameter averaging is used.\nIn eﬀect, dropout implicitly trains many models in parallel,\nand all of them share the same parameter set. The ﬁnal\nmodel is an average of all models. Typically, the dropout\nrate is set at 0.5. Another strategy to combat overﬁtting\nis early stopping, which we have employed for building our\nmodel. Here, we maintain an independent validation dataset\nto monitor the model performance during training, stop when\nthe performance gets worse, and select the best performing\nmodel in the validation set.\n3. EV ALUATION\nWe built a dataset of the ten Java projects (Ant, Batik,\nCassandra, Eclipse-E4, Log4J, Lucene, Maven2, Maven3,\nXalan-J, and Xerces) cloned from GitHub. These projects\nare the same as those used in previous studies [6, 9] but we\ncollected the most up-to-date revision of the code at the\ntime, i.e. 2016-07-25. After removing comments and blank\nlines, the projects were lexically analyzed using JavaParser 1\nto produce token sequences. We then partitioned the data\ninto mutually exclusive training, validation, and test sets.\nThe training set was used to learn a useful language. After\neach training epoch, the learned model was evaluated on the\nvalidation set and its performance is used to assess conver-\ngence against hyperparameters (e.g. learning rate in gradient\nsearches). Note that the validation set was not used to learn\nany of the model’s parameters. The best performing model\nin the validation set was chosen to be evaluated on the test\nset.\nFollowing common practices (e.g., as done in [12]), we\nreplaced integers, real numbers, exponential notation, hex-\nadecimal numbers with a generic <num>token, and replaced\nconstant strings with a generic <str> token. We also re-\nplaced less popular tokens (e.g. occurring only once in the\ncorpus) and tokens which exist in the validation and test\nsets but do not exist in the training set with a special token\n<unk>. Finally, we build a code corpus of 6,103,191 code\ntokens, with a vocabulary of 81,213 unique tokens.\nEach source ﬁle is parsed into a sequence of tokens. Each\nsequence is then split into sentences (i.e a subsequence of\ncode tokens) of ﬁxed length. We use 10K sentences for each\ncase of training, validation and testing. Sentence length\nvaries from 10 (e.g., a short statement), to 500 (e.g., a large\nﬁle). Embedding dimensionality ( D) varies from 20 to 500.\nFor simplicity, the size of the memory (i.e. the size of ct) is\nthe same as the number of embedding dimensions, that is\nD= K. A ﬁxed-size vocabulary is constructed based on top\nN popular tokens, and rare tokens are assigned to <unk>.\nOur evaluation aims to compare the performance of our\nLSTM language model against the traditional RNNs model\nas used in previous work [12]. We also used perplexity (as\ndone in [12]), an intrinsic evaluation metric that estimates\nthe average number of code tokens to select from at each\npoint in a sequence. Speciﬁcally, perplexity is the inverse of\naverage probability per code token, which is computed as\nexp(−∑\ns log P(s)/#words), where −log P(s) is the log-loss\nL(θ) computed in Equation 3. A smaller perplexity implies\na better language model.\n1http://javaparser.org\nsent-len embed-dim RNN LSTM improv %\n10\n50\n13.49 12.86 4.7\n20 10.38 9.66 6.9\n50 7.93 6.81 14.1\n100 7.20 6.40 11.1\n200 6.64 5.60 15.7\n500 6.48 4.72 27.2\n100\n20 7.96 7.11 10.7\n50 7.20 6.40 11.1\n100 7.23 5.72 20.9\n200 9.14 5.68 37.9\nTable 1: Perplexity on test data (the smaller the\nbetter).\nBoth our LSTM model and the simple RNN model were\ntrained using an adaptive stochastic gradient descent method\ncalled RMSprop, which is known to work best for recurrent\nmodels. There are three hyperparameters: learning rate η,\nadaptation rate ρand smoothing factor ϵ. RMSprop is tuned\nto achieve best results for simple RNN (η= 0.01, ρ= 0.9 and\nϵ= 10−8); and for LSTM ( η= 0.02, ρ= 0.99 and ϵ= 10−7),\nrespectively.\nWe conducted a number of experiments by varying the\nsize of a sentence ( sent-len) and the number of embedding\ndimensions (embed-dim). Table 1 reports the perplexity on\nthe test data for a vocab size of N = 1,000, when ﬁxing\nembedding dimensionality D (top part) and when ﬁxing\nsentence length (bottom part).\nOverall, given a ﬁxed embedding dimensionality, both\nsimple RNN and LSTM models improve with more train-\ning data (whose size grows with sentence length), and the\nLSTM performs consistently better. Importantly, the rate\nof improvement by LSTM also grows from 4.7% for short\nsentences of length 10, to 27.2% for long sentences of length\n500, conﬁrming the known fact that the LSTM handles long\nsequences better than the simple RNN. A similar pattern of\nimprovement is also observed when ﬁxing sentence length\nand varying dimensionality. It suggests that LSTM enjoys a\nbetter learning dynamic when the model is large.\n4. DISCUSSION\nThe promising results from our evaluation suggest that a\ngood language model for software code can be built based\non the powerful, deep learning-based LSTM architecture.\nOur next step involves conducting an extrinsic evaluation to\nmeasure the performance of our language model at a number\nof real SE tasks. For example, we will develop our language\nmodel into a code suggestionengine. This engine will suggest\na sequence of code tokens that ﬁt most with the current\ncontext and is most likely to appear next.\nOur language model can also provide us with a deep, se-\nmantic representation for any arbitrary sequence of code\ntokens. This can be done by aggregating all the output\nvectors (h1, ..., hk) through a mechanism known as pooling.\nThe simplest method is mean-pooling where the vector rep-\nresenting a code sequence is the sum of the output vectors of\nall the code token in the sequence divided by the number of\ntokens. This representation is in the form of a feature vector,\nwhich can be readily input to any classiﬁer to learn many\nuseful SE tasks such as defect prediction, checking for code\nduplication, and detecting vulnerable software components.\nAs we have seen, this feature vector is learned automatically\n(in an unsupervised manner) from the raw code, removing us\nfrom the time-consuming task of manually designing features\n(which have been done in previous work).\nThe language model for software code is part of DeepSoft\n– our generic, dynamic deep learning-based framework for\nmodeling software and its development and evolution process.\nDeepSoft is a compositional architecture which has several\nlayers modeling the progression of a software at four levels:\nsource code, issue reports, releases, and project. Each layer\nemploys chain of LSTM units to capture the sequential nature\nof the corresponding data. We envision many applications\nof DeepSoft to SE problems ranging from requirements to\nmaintenance such as risk prediction, eﬀort estimation, issue\nresolution recommendation, code patch generation, release\nplanning, and developer modeling.\n5. REFERENCES[1] Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu.\nAdvances in optimizing recurrent networks. In 2013 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing, pages 8624–8628. IEEE, 2013.\n[2] X. Chen, X. Liu, M. J. Gales, and P. C. Woodland.\nRecurrent neural network language model training with\nnoise contrastive estimation for speech recognition. In 2015\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 5411–5415. IEEE, 2015.\n[3] H. K. Dam, T. Tran, J. Grundy, and A. Ghose. DeepSoft: A\nvision for a deep model of software. In Proceedings of the\n24th ACM SIGSOFT International Symposium on\nFoundations of Software Engineering, FSE ’16. ACM, To\nAppear., 2016.\n[4] M. Gabel and Z. Su. A study of the uniqueness of source\ncode. In Proceedings of the Eighteenth ACM SIGSOFT\nInternational Symposium on Foundations of Software\nEngineering, FSE ’10, pages 147–156, New York, NY, USA,\n2010. ACM.\n[5] M. U. Gutmann and A. Hyv ¨arinen. Noise-contrastive\nestimation of unnormalized statistical models, with\napplications to natural image statistics. Journal of Machine\nLearning Research, 13(Feb):307–361, 2012.\n[6] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu. On\nthe naturalness of software. In Proceedings of the 34th\nInternational Conference on Software Engineering , ICSE\n’12, pages 837–847, Piscataway, NJ, USA, 2012. IEEE Press.\n[7] S. Hochreiter and J. Schmidhuber. Long short-term memory.\nNeural computation, 9(8):1735–1780, 1997.\n[8] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin.\nConvolutional neural networks over tree structures for\nprogramming language processing. In Proceedings of the\nThirtieth AAAI Conference on Artiﬁcial Intelligence , 2016.\n[9] T. T. Nguyen, A. T. Nguyen, H. A. Nguyen, and T. N.\nNguyen. A statistical semantic language model for source\ncode. In Proceedings of the 2013 9th Joint Meeting on\nFoundations of Software Engineering, ESEC/FSE 2013,\npages 532–542, New York, NY, USA, 2013. ACM.\n[10] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov. Dropout: A simple way to prevent neural\nnetworks from overﬁtting. Journal of Machine Learning\nResearch, 15:1929–1958, 2014.\n[11] Z. Tu, Z. Su, and P. Devanbu. On the localness of software.\nIn Proceedings of the 22Nd ACM SIGSOFT International\nSymposium on Foundations of Software Engineering , FSE\n2014, pages 269–280, New York, NY, USA, 2014. ACM.\n[12] M. White, C. Vendome, M. Linares-V´ asquez, and\nD. Poshyvanyk. Toward deep learning software repositories.\nIn Proceedings of the 12th Working Conference on Mining\nSoftware Repositories, MSR ’15, pages 334–345, Piscataway,\nNJ, USA, 2015. IEEE Press."
}