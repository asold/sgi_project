{
    "title": "Syntactically Supervised Transformers for Faster Neural Machine Translation",
    "url": "https://openalex.org/W2948197522",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A4287070823",
            "name": "Akoury, Nader",
            "affiliations": [
                "University of Massachusetts Amherst"
            ]
        },
        {
            "id": "https://openalex.org/A4202079942",
            "name": "Krishna, Kalpesh",
            "affiliations": [
                "University of Massachusetts Amherst"
            ]
        },
        {
            "id": "https://openalex.org/A4224959330",
            "name": "Iyyer, Mohit",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2767206889",
        "https://openalex.org/W2789543585",
        "https://openalex.org/W2152263452",
        "https://openalex.org/W2158388102",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963126845",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2594047108",
        "https://openalex.org/W2932376173",
        "https://openalex.org/W2892213699",
        "https://openalex.org/W4289302788",
        "https://openalex.org/W2953130735",
        "https://openalex.org/W2563574619",
        "https://openalex.org/W2545625743",
        "https://openalex.org/W2949680570",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2963434219",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2963648186",
        "https://openalex.org/W2116316001",
        "https://openalex.org/W2890501761",
        "https://openalex.org/W2963536265",
        "https://openalex.org/W2963876447",
        "https://openalex.org/W2963532001",
        "https://openalex.org/W2144002870",
        "https://openalex.org/W2168966090",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2165666205",
        "https://openalex.org/W2150378737",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2964089333",
        "https://openalex.org/W2123442489",
        "https://openalex.org/W2962969034",
        "https://openalex.org/W2111142112",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2251222643",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2888882903",
        "https://openalex.org/W2964026424",
        "https://openalex.org/W2962788148",
        "https://openalex.org/W2963946353",
        "https://openalex.org/W2407166119",
        "https://openalex.org/W2963736842"
    ],
    "abstract": "Standard decoders for neural machine translation autoregressively generate a\\nsingle target token per time step, which slows inference especially for long\\noutputs. While architectural advances such as the Transformer fully parallelize\\nthe decoder computations at training time, inference still proceeds\\nsequentially. Recent developments in non- and semi- autoregressive decoding\\nproduce multiple tokens per time step independently of the others, which\\nimproves inference speed but deteriorates translation quality. In this work, we\\npropose the syntactically supervised Transformer (SynST), which first\\nautoregressively predicts a chunked parse tree before generating all of the\\ntarget tokens in one shot conditioned on the predicted parse. A series of\\ncontrolled experiments demonstrates that SynST decodes sentences ~ 5x faster\\nthan the baseline autoregressive Transformer while achieving higher BLEU scores\\nthan most competing methods on En-De and En-Fr datasets.\\n",
    "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1269–1281\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n1269\nSyntactically Supervised Transformers\nfor Faster Neural Machine Translation\nNader Akoury, Kalpesh Krishna, Mohit Iyyer\nCollege of Information and Computer Sciences\nUniversity of Massachusetts Amherst\n{nsa,kalpesh,miyyer}@cs.umass.edu\nAbstract\nStandard decoders for neural machine trans-\nlation autoregressively generate a single tar-\nget token per time step, which slows infer-\nence especially for long outputs. While ar-\nchitectural advances such as the Transformer\nfully parallelize the decoder computations at\ntraining time, inference still proceeds sequen-\ntially. Recent developments in non- and semi-\nautoregressive decoding produce multiple to-\nkens per time step independently of the oth-\ners, which improves inference speed but deteri-\norates translation quality. In this work, we pro-\npose the syntactically supervised Transformer\n(SynST), which ﬁrst autoregressively predicts\na chunked parse tree before generating all of\nthe target tokens in one shot conditioned on the\npredicted parse. A series of controlled experi-\nments demonstrates that SynST decodes sen-\ntences ∼ 5×faster than the baseline autore-\ngressive Transformer while achieving higher\nBLEU scores than most competing methods\non En-De and En-Fr datasets.\n1 Introduction\nMost models for neural machine translation (NMT)\nrely on autoregressivedecoders, which predict each\ntoken ti in the target language one by one condi-\ntioned on all previously-generated target tokens\nt1···i−1 and the source sentence s. For downstream\napplications of NMT that prioritize low latency\n(e.g., real-time translation), autoregressive decod-\ning proves expensive, as decoding time in state-\nof-the-art attentional models such as the Trans-\nformer (Vaswani et al., 2017) scales quadratically\nwith the number of target tokens.\nIn order to speed up test-time translation, non-\nautoregressive decoding methods produce all tar-\nget tokens at once independently of each other\n(Gu et al., 2018; Lee et al., 2018), while semi-\nautoregressive decoding (Wang et al., 2018; Stern\nCats > sleep > a > lot\nKatzen schlafen viel\nCats sleep a lot\nCats sleep > a lot\n   >    >    > Cats sleep a lot\nNP1 > VP3 > Cats sleep a lot\nautoregressive\nnon-autoregressive\nsemi-autoregressive\nLatent Transformer\nSynST (ours)\nFigure 1: Comparison of different methods designed to\nincrease decoding speed. The arrow > indicates the be-\nginning of a new decode step conditioned on everything\nthat came previously. The latent Transformer produces\na sequence of discrete latent variables, whereas SynST\nproduces a sequence of syntactic constituent identiﬁers.\net al., 2018) trades off speed for quality by reduc-\ning (but not completely eliminating) the number of\nsequential computations in the decoder (Figure 1).\nWe choose the latent Transformer (LT) of Kaiser\net al. (2018) as a starting point, which merges both\nof these approaches by autoregressively generating\na short sequence of discrete latent variables before\nnon-autoregressively producing all target tokens\nconditioned on the generated latent sequence.\nKaiser et al. (2018) experiment with increas-\ningly complex ways of learning their discrete latent\nspace, some of which obtain small BLEU improve-\nments over a purely non-autoregressive baseline\nwith similar decoding speedups. In this work, we\npropose to syntactically supervise the latent space,\nwhich results in a simpler model that produces bet-\nter and faster translations.1 Our model, the syntac-\ntically supervised Transformer (SynST, Section 3),\nﬁrst autoregressively predicts a sequence of target\nsyntactic chunks, and then non-autoregressively\n1Source code to reproduce our results is available at\nhttps://github.com/dojoteef/synst\n1270\ngenerates all of the target tokens conditioned on\nthe predicted chunk sequence. During training,\nthe chunks are derived from the output of an ex-\nternal constituency parser. We propose a simple\nalgorithm on top of these parses that allows us to\ncontrol the average chunk size, which in turn limits\nthe number of autoregressive decoding steps we\nhave to perform.\nSynST improves on the published LT results for\nWMT 2014 En→De in terms of both BLEU (20.7\nvs. 19.8) and decoding speed ( 4.9×speedup vs.\n3.9×). While we replicate the setup of Kaiser et al.\n(2018) to the best of our ability, other work in this\narea does not adhere to the same set of datasets,\nbase models, or “training tricks”, so a legitimate\ncomparison with published results is difﬁcult. For\na more rigorous comparison, we re-implement an-\nother related model within our framework, the semi-\nautoregressive transformer (SAT) of Wang et al.\n(2018), and observe improvements in BLEU and\ndecoding speed on both En↔De and En→Fr lan-\nguage pairs (Section 4).\nWhile we build on a rich line of work that\nintegrates syntax into both NMT (Aharoni and\nGoldberg, 2017; Eriguchi et al., 2017) and other\nlanguage processing tasks (Strubell et al., 2018;\nSwayamdipta et al., 2018), we aim to use syntax to\nspeed up decoding, not improve downstream per-\nformance (i.e., translation quality). An in-depth\nanalysis (Section 5) reveals that syntax is a power-\nful abstraction for non-autoregressive translation:\nfor example, removing information about the con-\nstituent type of each chunk results in a drop of 15\nBLEU on IWSLT En→De.\n2 Decoding in Transformers\nOur work extends the Transformer architecture\n(Vaswani et al., 2017), which is an instance of the\nencoder-decoder framework for language genera-\ntion that uses stacked layers of self-attention to\nboth encode a source sentence and decode the cor-\nresponding target sequence. In this section, we\nbrieﬂy review 2 the essential components of the\nTransformer architecture before stepping through\nthe decoding process in both the vanilla autoregres-\nsive Transformer and non- and semi-autoregressive\nextensions of the model.\n2We omit several architectural details in our overview,\nwhich can be found in full in Vaswani et al. (2017).\n2.1 Transformers for NMT\nThe Transformer encoder takes a sequence of\nsource word embeddings s1,··· ,sn as input and\npasses it through multiple blocks of self-attention\nand feed-forward layers to ﬁnally produce contex-\ntualized token representations e1,··· ,en. Unlike\nrecurrent architectures (Hochreiter and Schmidhu-\nber, 1997; Bahdanau et al., 2014), the computation\nof en does not depend on en−1, which enables\nfull parallelization of the encoder’s computations\nat both training and inference. To retain informa-\ntion about the order of the input words, the Trans-\nformer also includes positional encodings, which\nare added to the source word embeddings.\nThe decoder of the Transformer operates very\nsimilarly to the encoder during training: it takes\na shifted sequence of target word embeddings\nt1,··· ,tm as input and produces contextualized to-\nken representations d1,··· ,dm, from which the tar-\nget tokens are predicted by a softmax layer. Un-\nlike the encoder, each block of the decoder also\nperforms source attention over the representations\ne1...n produced by the encoder. Another difference\nduring training time is target-side masking: at po-\nsition i, the decoder’s self attention should not be\nable to look at the representations of later positions\ni + 1, . . . , i+ m, as otherwise predicting the next\ntoken becomes trivial. To impose this constraint,\nthe self-attention can be masked by using a lower\ntriangular matrix with ones below and along the\ndiagonal.\n2.2 Autoregressive decoding\nWhile at training time, the decoder’s computations\ncan be parallelized using masked self-attention on\nthe ground-truth target word embeddings, infer-\nence still proceeds token-by-token. Formally, the\nvanilla Transformer factorizes the probability of\ntarget tokens t1,···,tm conditioned on the source\nsentence s into a product of token-level conditional\nprobabilities using the chain rule,\np(t1,···,tm|s) =\nm∏\ni=1\np(ti|t1,···,ti−1, s).\nDuring inference, computing arg maxt p(t|s) is in-\ntractable, which necessitates the use of approxi-\nmate algorithms such as beam search. Decoding\nrequires a separate decode step to generate each\ntarget token ti; as each decode step involves a full\npass through every block of the decoder, autoregres-\nsive decoding becomes time-consuming especially\n1271\nFeed Forward\nAdd & Norm\nSelf-Attention\nAdd & Norm\nSelf-Attention\nAdd & Norm\nN⨉ N⨉ \nSource Attention\nAdd & Norm\nSelf-Attention\nAdd & Norm\nM⨉ \nSource Attention\nAdd & Norm\nAutoregressive\nParse Decoder\nSingle-Pass\nToken Decoder\nEncoder\nFeed Forward\nAdd & Norm\nFeed Forward\nAdd & Norm\nPositional\nEncoding\nPositional\nEncoding\nPositional\nEncoding\nInput\nEmbedding\nOutput\nEmbedding\nParse\nEmbedding\nt 4 NP1\t<MASK>\tVP3\t<MASK>\t<MASK>\t<MASK>Katzen schlafen viel <SOS>\tNP1\tVP3\nt 2 t 1 t 3 \nLinear\nSoftmax\nNP1\tVP3\t<EOS>\nLinear\nSoftmax\nNP1 Cats VP3 sleep a lot\nFigure 2: A high-level overview of the SynST architecture. During training, the parse decoder learns to autore-\ngressively predict all chunk identiﬁers in parallel (time steps t1,2,3), while the token decoder conditions on the\n“ground truth” chunk identiﬁers to predict the target tokens in one shot (time step t4). During inference (shown\nhere), the token decoder conditions on the autoregressively predicted chunk identiﬁers. The encoder and token\ndecoder contain N ≥1 layers, while the parse decoder only requires M = 1layers (see Table 4).\nfor longer target sequences in which there are more\ntokens to attend to at every block.\n2.3 Generating multiple tokens per time step\nAs decoding time is a function of the number of\ndecoding time steps (and consequently the number\nof passes through the decoder), faster inference can\nbe obtained using methods that reduce the num-\nber of time steps. In autoregressive decoding, the\nnumber of time steps is equal to the target sentence\nlength m; the most extreme alternative is (natu-\nrally) non-autoregressive decoding, which requires\njust a single time step by factorizing the target se-\nquence probability as\np(t1,···,tm|s) =\nm∏\ni=1\np(ti|s).\nHere, all target tokens are produced independently\nof each other. While this formulation does indeed\nprovide signiﬁcant decoding speedups, translation\nquality suffers after dropping the dependencies be-\ntween target tokens without additional expensive\nreranking steps (Gu et al., 2018, NAT) or itera-\ntive reﬁnement with multiple decoders (Lee et al.,\n2018).\nAs fully non-autoregressive decoding results in\npoor translation quality, another class of meth-\nods produce k tokens at a single time step where\n1 < k < m. The semi-autoregressive Transformer\n(SAT) of Wang et al. (2018) produces a ﬁxed k\ntokens per time step, thus modifying the target se-\nquence probability to:\np(t1,···,tm|s) =\n|G|∏\ni=1\np(Gt|G1,···,Gt−1, x),\nwhere each of G1,···,G⌊m−1\nk ⌋+1 is a group of\ncontiguous non-overlapping target tokens of the\nform ti,···,ti+k. In conjunction with training tech-\nniques like knowledge distillation (Kim and Rush,\n2016) and initialization with an autoregressive\nmodel, SATs maintain better translation quality\nthan non-autoregressive approaches with competi-\ntive speedups. Stern et al. (2018) follow a similar\napproach but dynamically select a different k at\neach step, which results in further quality improve-\nments with a corresponding decrease in speed.\n2.4 Latent Transformer\nWhile current semi-autoregressive methods achieve\nboth better quality and faster speedups than their\nnon-autoregressive counterparts, largely due to the\nnumber of tricks required to train the latter, the\n1272\ntheoretical speedup for non-autoregressive models\nis of course larger. The latent Transformer (Kaiser\net al., 2018, LT) is similar to both of these lines\nof work: its decoder ﬁrst autoregressively gener-\nates a sequence of discrete latent variables l1,··· ,lj\nand then non-autoregressively produces the entire\ntarget sentence ti,···,tm conditioned on the latent\nsequence. Two parameters control the magnitude\nof the speedup in this framework: the length of\nthe latent sequence (j), and the size of the discrete\nlatent space (K).\nThe LT is signiﬁcantly more difﬁcult to train\nthan any of the previously-discussed models, as\nit requires passing the target sequence through\nwhat Kaiser et al. (2018) term a discretization\nbottleneck that must also maintain differentiabil-\nity through the decoder. While LT outperforms\nthe NAT variant of non-autoregressive decoding in\nterms of BLEU, it takes longer to decode. In the\nnext section, we describe how we use syntax to\naddress the following three weaknesses of LT:\n1. generating the same number of latent vari-\nables j regardless of the length of the source\nsentence, which hampers output quality\n2. relying on a large value of K (the authors\nreport that in the base conﬁguration as few as\n∼3000 latents are used out of 216 available),\nwhich hurts translation speed\n3. the complexity of implementation and op-\ntimization of the discretization bottleneck,\nwhich negatively impacts both quality and\nspeed.\n3 Syntactically Supervised Transformers\nOur key insight is that we can use syntactic in-\nformation as a proxy to the learned discrete latent\nspace of the LT. Speciﬁcally, instead of producing a\nsequence of latent discrete variables, our model pro-\nduces a sequence of phrasal chunks derived from\na constituency parser. During training, the chunk\nsequence prediction task is supervised, which re-\nmoves the need for a complicated discretization\nbottleneck and a ﬁxed sequence length j. Addition-\nally, our chunk vocabulary is much smaller than\nthat of the LT, which improves decoding speed.\nOur model, the syntactically supervised Trans-\nformer (SynST), follows the two-stage decoding\nsetup of the LT. First, an autoregressive decoder\ngenerates the phrasal chunk sequence, and then all\nof the target tokens are generated at once, condi-\nS6\nDT1\nVP3NP3\nJJ1 NN1 VBD1 NP2\nPRP$1 NNS1the sleepy cat closed\nits eyes\nk=3: NP3 VP3\nk=2: DT1 JJ1 NN1\n        VBD1 NP2\nFigure 3: Example of our parse chunk algorithm with\nmax span sizesk = 2, 3. At each visited node during an\nin-order traversal of the parse, if the subtree size is less\nthan or equal to k, we append a corresponding chunk\nidentiﬁer to our sequence.\ntioned on the chunks (Figure 2). The rest of this\nsection fully speciﬁes each of these two stages.\n3.1 Autoregressive chunk decoding\nIntuitively, our model uses syntax as a scaffold for\nthe generated target sentence. During training, we\nacquire supervision for the syntactic prediction task\nthrough an external parser in the target language.\nWhile we could simply force the model to predict\nthe entire linearized parse minus the terminals, 3\nthis approach would dramatically increase the num-\nber of autoregressive steps, which we want to keep\nat a minimum to prioritize speed. To balance syn-\ntactic expressivity with the number of decoding\ntime steps, we apply a simple chunking algorithm\nto the constituency parse.\nExtracting chunk sequences: Similar to the SAT\nmethod, we ﬁrst choose a maximum chunk size\nk. Then, for every target sentence in the training\ndata, we perform an in-order traversal of its con-\nstituency parse tree. At each visited node, if the\nnumber of leaves spanned by that node is less than\nor equal to k, we append a descriptive chunk iden-\ntiﬁer to the parse sequence before moving onto its\nsibling; otherwise, we proceed to the left child and\ntry again. This process is shown for two different\nvalues of k on the same sentence in Figure 3. Each\nunique chunk identiﬁer, which is formed by the\nconcatenation of the constituent type and subtree\nsize (e.g., NP3), is considered as an element of\nour ﬁrst decoder’s vocabulary; thus, the maximum\nsize of this vocabulary is |P|×k where P is the\n3This approach is used for paraphrase generation by Iyyer\net al. (2018), who were not focused on decoding speed.\n1273\nset of all unique constituent types.4 Both parts of\nthe chunk identiﬁer (the constituent type and its\nsize) are crucial to the performance of SynST, as\ndemonstrated by the ablations in Section 5.\nPredicting chunk sequences: Because we are\nfully supervising the chunk sequence prediction,\nboth the encoder and parse decoder are architec-\nturally identical to the encoder and decoder of the\nvanilla Transformer, respectively. The parse de-\ncoder differs in its target vocabulary, which is made\nup of chunk identiﬁers instead of word types, and\nin the number of layers (we use 1 layer instead of\n6, as we observe diminishing returns from bigger\nparse decoders as shown in Section 5). Formally,\nthe parse decoder autoregressively predicts a se-\nquence of chunk identiﬁers c1,···,cp conditioned on\nthe source sentence s5 by modeling\np(c1,···,cp|s) =\np∏\ni=1\np(ci|c1,···,ci−1, s).\nUnlike LT, the length p of the chunk sequence\nchanges dynamically based on the length of the\ntarget sentence, which is reminiscent of the token\ndecoding process in the SAT.\n3.2 Non-autoregressive token decoding\nIn the second phase of decoding, we apply a single\nnon-autoregressive step to produce the tokens of the\ntarget sentence by factorizing the target sequence\nprobability as\np(t1,···,tm|s) =\nm∏\ni=1\np(ti|c1,···,cp, s).\nHere, all target tokens are produced independently\nof each other, but in contrast to the previously-\ndescribed non-autoregressive models, we addition-\nally condition each prediction on the entire chunk\nsequence. To implement this decoding step, we\nfeed a chunk sequence as input to a second Trans-\nformer decoder, whose parameters are separate\nfrom those of the parse decoder. During training,\nwe use the ground-truth chunk sequence as input,\nwhile at inference we use the predicted chunks.\n4In practice, this vocabulary is signiﬁcantly smaller than\nthe discrete latent space of the LT for reasonable values of k.\n5In preliminary experiments, we also tried conditioning\nthis decoder on the source parse, but we did not notice signiﬁ-\ncant differences in translation quality.\nImplementation details: To ensure that the num-\nber of input and output tokens in the second de-\ncoder are equal, which is a requirement of the\nTransformer decoder, we add placeholder<MASK>\ntokens to the chunk sequence, using the size com-\nponent of each chunk identiﬁer to determine where\nto place these tokens. For example, if the ﬁrst\ndecoder produces the chunk sequence NP2 PP3,\nour second decoder’s input becomesNP2 <MASK>\n<MASK> PP3 <MASK> <MASK> <MASK>; this\nformulation also allows us to better leverage the\nTransformer’s positional encodings. Then, we\napply unmasked self-attention over this input se-\nquence and predict target language tokens at each\nposition associated with a <MASK> token.\n4 Experiments\nWe evaluate the translation quality (in terms of\nBLEU) and the decoding speedup (average time\nto decode a sentence) of SynST compared to com-\npeting approaches. In a controlled series of experi-\nments on four different datasets (En↔De and En→\nFr language pairs),6 we ﬁnd that SynST achieves a\nstrong balance between quality and speed, consis-\ntently outperforming the semi-autoregressive SAT\non all datasets and the similar LT on the only trans-\nlation dataset for which Kaiser et al. (2018) report\nresults. In this section, we ﬁrst describe our ex-\nperimental setup and its differences to those of\nprevious work before providing a summary of the\nkey results.\n4.1 Controlled experiments\nExisting papers in non- and semi-autoregressive ap-\nproaches do not adhere to a standard set of datasets,\nbase model architectures, training tricks, or even\nevaluation scripts. This unfortunate disparity in\nevaluation setups means that numbers between dif-\nferent papers are uncomparable, making it difﬁcult\nfor practitioners to decide which method to choose.\nIn an effort to offer a more meaningful comparison,\nwe strive to keep our experimental conditions as\nclose to those of Kaiser et al. (2018) as possible, as\nthe LT is the most similar existing model to ours.\nIn doing so, we made the following decisions:\n• Our base model is the base vanilla Trans-\nformer (Vaswani et al., 2017) without any ar-\n6We explored translating to other languages previously\nevaluated in the non- and semi-autoregressive decoding lit-\nerature, but could not ﬁnd publicly-available, reliable con-\nstituency parsers for them.\n1274\nModel WMT En-De WMT De-En IWSLT En-De WMT En-Fr\nBLEU Speedup BLEU Speedup BLEU Speedup BLEU Speedup\nBaseline (b = 1) 25.82 1.15× 29.83 1.14× 28.66 1.16× 39.41 1.18×\nBaseline (b = 4) 26.87 1.00× 30.73 1.00× 30.00 1.00× 40.22 1.00×\nSAT (k = 2) 22.81 2.05× 26.78 2.04× 25.48 2.03× 36.62 2.14×\nSAT (k = 4) 16.44 3.61× 21.27 3.58× 20.25 3.45× 28.07 3.34×\nSAT (k = 6) 12.55 4.86× 15.23 4.27× 14.02 4.39× 24.63 4.77×\nLT* 19.8 3.89× - - - - - -\nSynST(k = 6) 20.74 4.86× 25.50 5.06× 23.82 3.78× 33.47 5.32×\nTable 1: Controlled experiments comparing SynST to a baseline Transformer, SAT, and LT on four different\ndatasets (two language pairs) demonstrate speed and BLEU improvements. Wall-clock speedup is measured on a\nsingle Nvidia TitanX Pascal by computing the average time taken to decode a single sentence in the dev/test set,\naveraged over ﬁve runs. When beam width b is not speciﬁed, we perform greedy decoding (i.e., b = 1). Note that\nthe LT results are reported by Kaiser et al. (2018) and not from our own implementation; 9 as such, they are not\ndirectly comparable to the other results.\nchitectural upgrades.7\n• We use all of the hyperparameter values from\nthe original Transformer paper and do not at-\ntempt to tune them further, except for: (1) the\nnumber of layers in the parse decoder, (2) the\ndecoders do not use label smoothing.\n• We do not use sequence-level knowledge dis-\ntillation, which augments the training data\nwith translations produced by an external au-\ntoregressive model. The choice of model used\nfor distillation plays a part in the ﬁnal BLEU\nscore, so we remove this variable.\n• We report all our BLEU numbers using sacre-\nBLEU (Post, 2018) to ensure comparability\nwith future work.8\n• We report wall-clock speedups by measur-\ning the average time to decode one sentence\n(batch size of one) in the dev/test set.\nAs the code for LT is not readily available9, we\nalso reimplement the SAT model using our setup,\nas it is the most similar model outside of LT to our\nown.10 For SynST, we set the maximum chunk size\n7As the popular Tensor2Tensor implementation is con-\nstantly being tweaked, we instead re-implement the Trans-\nformer as originally published and verify that its results\nclosely match the published ones. Our implementation\nachieves a BLEU of 27.69 on WMT’14 En-De, when using\nmulti-bleu.perl from Moses SMT.\n8SacreBLEU signature: BLEU+case.mixed+lang.LANG\n+numrefs.1+smooth.exp+test.TEST+tok.intl+version.1.2.11,\nwith LANG ∈ {en-de, de-en, en-fr} and TEST ∈ {wmt14/full,\niwslt2017/tst2013}\n9We attempted to use the publicly available code in Ten-\nsor2Tensor, but were unable to successfully train a model.\n10The published SAT results use knowledge distillation and\nk = 6and compare this model to the SAT trained\nwith k = 2, 4, 6.\n4.2 Datasets\nWe experiment with English-German and English-\nFrench datasets, relying on constituency parsers in\nall three languages. We use the Stanford CoreNLP\n(Manning et al., 2014) shift-reduce parsers for En-\nglish, German, and French. For English-German,\nwe evaluate on WMT 2014 En ↔De as well as\nIWSLT 2016 En→De, while for English-French\nwe train on the Europarl / Common Crawl subset\nof the full WMT 2014 En →Fr data and evaluate\nover the full dev/test sets. WMT 2014 En ↔De\nconsists of around 4.5 million sentence pairs en-\ncoded using byte pair encoding (Sennrich et al.,\n2016) with a shared source-target vocabulary of\nroughly 37000 tokens. We use the same prepro-\ncessed dataset used in the original Transformer pa-\nper and also by many subsequent papers that have\ninvestigated improving decoding speed, evaluating\non the newstest2013 dataset for validation and\nthe newstest2014 dataset for testing. For the\nIWSLT dataset we use tst2013 for validation\nand utilize the same hyperparameters as Lee et al.\n(2018).\n4.3 Results\nTable 1 contains the results on all four datasets.\nSynST achieves speedups of ∼4 −5×that of the\nvanilla Transformer, which is larger than nearly all\ndifferent hyperparameters than the vanilla Transformer, most\nnotably a tenfold decrease in training steps due to initializing\nfrom a pre-trained Transformer.\n1275\nChunk types SynST predictions with | separating syntax chunks\nWords repeated\nin two separate\nsyntax chunks\n(blue, red)\nNP1, NP3\nNP3, PP4\nNP2, PP3\nBut | it | is | enthusiasm | in | a great enthusiasm\n... Enrique | Pena | Nieto | is | facing | a difﬁcult start | on a difﬁcult start\nDo | you | not | turn | your voters | on your voters\nOutput type Output for a single example\nSynST reorders\nsyntax chunks,\nwhich is ﬁxed\nwith gold parses\n(GP) as input\nground truth\nSynST\npredicted parse\nSynST + GP\nCanada | was | the ﬁrst country | to | make | photograph warnings| mandatory in 2001\nCanada | was | the ﬁrst country | in 2001 | to | propose | photographic warnings\nNP1 VBD1 NP3PP2 TO1 VB1 NP4\nCanada | was | the ﬁrst country | to | make | photographic warnings | available in 2001\nTrue chunk SynST predictions with @@ as subword divisions\nWrong subword\ncompletion\nwithin a syntax\nchunk\nignores them\nbeforehand\nexamines\nI | simply | ign@@ it them\nMost ST@@ I | can | be | cur@@ ed | be@@ foreh@@ ly\nBeg@@ inning | of | the course | which | exam@@ ates | the ...\nTable 2: Common error made by SynST due to its syntactically informed semi-autoregressive decoding. Different\nsyntax chunks have been separated by |symbols in all the decoded outputs.\nof the SAT conﬁgurations. Quality-wise, SynST\nagain signiﬁcantly outperforms the SAT conﬁgura-\ntions at comparable speedups on all datasets. On\nWMT En-De, SynST improves by 1 BLEU over\nLT (20.74 vs LT’s 19.8 without reranking).\nComparisons to other published work: As men-\ntioned earlier, we adopt a very strict set of experi-\nmental conditions to evaluate our work against LT\nand SAT. For completeness, we also offer an unsci-\nentiﬁc comparison to other numbers in Table A1.\n5 Analysis\nIn this section, we perform several analysis and ab-\nlation experiments on the IWSLT En-De dev set to\nshed more light on how SynST works. Speciﬁcally,\nwe explore common classes of translation errors,\nimportant factors behind SynST’s speedup, and the\nperformance of SynST’s parse decoder.\n2 4 6 8 10\nk\n1.5\n2.0\n2.5Average Chunk Size\nChunk Size given k\niwslt_en_de_parsed\nwmt_en_de_parsed\nwmt_de_en_parsed\nwmt_en_fr_parsed\nFigure 4: The average size of a chunk given a particular\nvalue of the max chunk size k.\n5.1 Analyzing SynST’s translation quality\nWhat types of translation errors does SynST make?\nThrough a qualitative inspection of SynST’s out-\nput translations, we identify three types of errors\nthat SynST makes more frequently than the vanilla\nTransformer: subword repetition, phrasal reorder-\ning, and inaccurate subword completions. Table 2\ncontains examples of each error type.\nDo we need to include the constituent type in the\nchunk identiﬁer? SynST’s chunk identiﬁers con-\ntain both the constituent type as well as chunk size.\nIs the syntactic information actually useful dur-\ning decoding, or is most of the beneﬁt from the\nchunk size? To answer this question, we train a\nvariant of SynST without the constituent identi-\nﬁers, so instead of predicting NP3 VP2 PP4, for\nexample, the parse decoder would predict 3 2 4.\nThis model substantially underperforms, achiev-\ning a BLEU of 8.19 compared to 23.82 for SynST,\nwhich indicates that the syntactic information is of\nconsiderable value.\nHow much does BLEU improve when we provide\nthe ground-truth chunk sequence? To get an upper\nbound on how much we can gain by improving\nSynST’s parse decoder, we replace the input to\nthe second decoder with the ground-truth chunk\nsequence instead of the one generated by the parse\ndecoder. The BLEU increases from 23.8 to 41.5\nwith this single change, indicating that future work\non SynST’s parse decoder could prove very fruitful.\n1276\nPredicted parse\nvs.\nGold parse (separate)\nPredicted parse\nvs.\nGold parse (joint)\nParsed prediction\nvs.\nGold parse\nParsed prediction\nvs.\nPredicted parse\nF1 65.48 69 .64 79 .16 89 .90\nExact match 4.23% 5 .24% 5 .94% 43 .10%\nTable 3: F1 and exact match comparisons of predicted chunk sequences (from the parse decoder), ground-truth\nchunk sequences (from an external parser in the target language), and chunk sequences obtained after parsing the\ntranslation produced by the token decoder. First two columns show the improvement obtained by jointly training\nthe two decoders. The third column shows that when the token decoder deviates from the predicted chunk sequence,\nit usually results in a translation that is closer to the ground-truth target syntax, while the fourth column shows that\nthe token decoder closely follows the predicted chunk sequence.\n5.2 Analyzing SynST’s speedup\nWhat is the impact of average chunk size on our\nmeasured speedup? Figure 4 shows that the IWSLT\ndataset, for which we report the lowest SynST\nspeedup, has a signiﬁcantly lower average chunk\nsize than that of the other datasets at many differ-\nent values of k.11 We observe that our empirical\nspeedup directly correlates with the average chunk\nsize: ranking the datasets by empirical speedups in\nTable 1 results in the same ordering as Figure 4’s\nranking by average chunk size.\nHow does the number of layers in SynST’s parse\ndecoder affect the BLEU/speedup tradeoff? All\nSynST experiments in Table 1 use a single layer for\nthe parse decoder. Table 4 shows that increasing\nthe number of layers from 1 to 5 results in a BLEU\nincrease of only 0.5, while the speedup drops from\n3.8×to 1.4×. Our experiments indicate that (1) a\nsingle layer parse decoder is reasonably sufﬁcient\nto model the chunked sequence and (2) despite its\nsmall output vocabulary, the parse decoder is the\nbottleneck of SynST in terms of decoding speed.\n5.3 Analyzing SynST’s parse decoder\nHow well does the predicted chunk sequence match\nthe ground truth? We evaluate the generated\nchunk sequences by the parse decoder to explore\nhow well it can recover the ground-truth chunk\nsequence (where the “ground truth” is provided\nby the external parser). Concretely, we compute\nthe chunk-level F1 between the predicted chunk\nsequence and the ground-truth. We evaluate two\nconﬁgurations of the parse decoder, one in which it\nis trained separately from the token decoder (ﬁrst\ncolumn of Table 3), and the other where both de-\ncoders are trained jointly (second column of Ta-\n11IWSLT is composed of TED talk subtitles. A small aver-\nage chunk size is likely due to including many short utterances.\n# Layers Max Chunk Size Speedup BLEU\n1 k = 6 3 .8× 23.82\n2 k = 6 2 .8× 23.98\n3 k = 6 2 .2× 24.54\n4 k = 6 1 .8× 24.04\n5 k = 6 1 .4× 24.34\n1 k ∈{1 . . .6} 3.1× 25.31\nTable 4: Increasing the number of layers in SynST’s\nparse decoder signiﬁcantly lowers the speedup while\nmarginally impacting BLEU. Randomly sampling k\nfrom {1 . . .6}during training boosts BLEU signiﬁ-\ncantly with minimal impact on speedup.\nble 3). We observe that joint training boosts the\nchunk F1 from 65.4 to 69.6, although, in both cases\nthe F1 scores are relatively low, which matches our\nintuition as most source sentences can be translated\ninto multiple target syntactic forms.\nHow much does the token decoder rely on the pre-\ndicted chunk sequence? If SynST’s token decoder\nproduces the translation “the man went to the store”\nfrom the parse decoder’s prediction ofPP3 NP3,\nit has clearly ignored the predicted chunk sequence.\nTo measure how often the token decoder follows\nthe predicted chunk sequence, we parse the gener-\nated translation and compute the F1 between the\nresulting chunk sequence and the parse decoder’s\nprediction (fourth column of Table 3). Strong re-\nsults of 89.9 F1 and 43.1% exact match indicate\nthat the token decoder is heavily reliant on the gen-\nerated chunk sequences.\nWhen the token decoder deviates from the predicted\nchunk sequence, does it do a better job matching\nthe ground-truth target syntax? Our next exper-\niment investigates why the token decoder some-\ntimes ignores the predicted chunk sequence. One\n1277\nhypothesis is that it does so to correct mistakes\nmade by the parse decoder. To evaluate this hy-\npothesis, we parse the predicted translation (as we\ndid in the previous experiment) and then compute\nthe chunk-level F1 between the resulting chunk se-\nquence and the ground-truth chunk sequence. The\nresulting F1 is indeed almost 10 points higher (third\ncolumn of Table 3), indicating that the token de-\ncoder does have the ability to correct mistakes.\nWhat if we vary the max chunk size k during train-\ning? Given a ﬁxed k, our chunking algorithm\n(see Figure 3) produces a deterministic chunking,\nallowing better control of SynST’s speedup, even\nif that sequence may not be optimal for the to-\nken decoder. During training we investigate us-\ning k′ = min(k,\n√\nT), where T is the target sen-\ntence length (to ensure short inputs do not col-\nlapse into a single chunk) and randomly sampling\nk ∈{1 . . .6}. The ﬁnal row of Table 4 shows that\nexposing the parse decoder to multiple possible\nchunkings of the same sentence during training al-\nlows it to choose a sequence of chunks that has\na higher likelihood at test time, improving BLEU\nby 1.5 while decreasing the speedup from 3.8×to\n3.1×; this is an exciting result for future work (see\nTable A3 for additional analysis).\n6 Related Work\nOur work builds on the existing body of literature in\nboth fast decoding methods for neural generation\nmodels as well as syntax-based MT; we review\neach area below.\n6.1 Fast neural decoding\nWhile all of the prior work described in Section 2\nis relatively recent, non-autoregressive methods for\ndecoding in NMT have been around for longer, al-\nthough none relies on syntax like SynST. Schwenk\n(2012) translate short phrases non-autoregressively,\nwhile Kaiser and Bengio (2016) implement a non-\nautoregressive neural GPU architecture and Li-\nbovick and Helcl (2018) explore a CTC approach.\nGuo et al. (2019) use phrase tables and word-level\nadversarial methods to improve upon the NAT\nmodel of Gu et al. (2018), while Wang et al. (2019)\nregularize NAT by introducing similarity and back-\ntranslation terms to the training objective.\n6.2 Syntax-based translation\nThere is a rich history of integrating syntax into\nmachine translation systems. Wu (1997) pioneered\nthis direction by proposing an inverse transduction\ngrammar for building word aligners. Yamada and\nKnight (2001) convert an externally-derived source\nparse tree to a target sentence, the reverse of what\nwe do with SynST’s parse decoder; later, other\nvariations such as string-to-tree and tree-to-tree\ntranslation models followed (Galley et al., 2006;\nCowan et al., 2006). The Hiero system of Chiang\n(2005) employs a learned synchronous context free\ngrammar within phrase-based translation, which\nfollow-up work augmented with syntactic supervi-\nsion (Zollmann and Venugopal, 2006; Marton and\nResnik, 2008; Chiang et al., 2008).\nSyntax took a back seat with the advent of\nneural MT, as early sequence to sequence mod-\nels (Sutskever et al., 2014; Luong et al., 2015) fo-\ncused on architectures and optimization. Sennrich\nand Haddow (2016) demonstrate that augmenting\nword embeddings with dependency relations helps\nNMT, while Shi et al. (2016) show that NMT sys-\ntems do not automatically learn subtle syntactic\nproperties. Stahlberg et al. (2016) incorporate Hi-\nero’s translation grammar into NMT systems with\nimprovements; similar follow-up results (Aharoni\nand Goldberg, 2017; Eriguchi et al., 2017) directly\nmotivated this work.\n7 Conclusions & Future Work\nWe propose SynST, a variant of the Transformer\narchitecture that achieves decoding speedups by\nautoregressively generating a constituency chunk\nsequence before non-autoregressively producing\nall tokens in the target sentence. Controlled exper-\niments show that SynST outperforms competing\nnon- and semi-autoregressive approaches in terms\nof both BLEU and wall-clock speedup on En-De\nand En-Fr language pairs. While our method is\ncurrently restricted to languages that have reliable\nconstituency parsers, an exciting future direction\nis to explore unsupervised tree induction methods\nfor low-resource target languages (Drozdov et al.,\n2019). Finally, we hope that future work in this area\nwill follow our lead in using carefully-controlled\nexperiments to enable meaningful comparisons.\nAcknowledgements\nWe thank the anonymous reviewers for their insight-\nful comments. We also thank Justin Payan and the\nrest of the UMass NLP group for helpful comments\non earlier drafts. Finally, we thank Weiqiu You for\nadditional experimentation efforts.\n1278\nReferences\nRoee Aharoni and Yoav Goldberg. 2017. Towards\nstring-to-tree neural machine translation. In Pro-\nceedings of the Association for Computational Lin-\nguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of\nthe International Conference on Learning Represen-\ntations.\nDavid Chiang. 2005. A hierarchical phrase-based\nmodel for statistical machine translation. In Pro-\nceedings of the Association for Computational Lin-\nguistics, pages 263–270. Association for Computa-\ntional Linguistics.\nDavid Chiang, Yuval Marton, and Philip Resnik. 2008.\nOnline large-margin training of syntactic and struc-\ntural translation features. In Proceedings of Em-\npirical Methods in Natural Language Processing ,\npages 224–233. Association for Computational Lin-\nguistics.\nBrooke Cowan, Ivona Ku ˇcerov´a, and Michael Collins.\n2006. A discriminative model for tree-to-tree trans-\nlation. In Proceedings of Empirical Methods in Nat-\nural Language Processing, pages 232–241. Associa-\ntion for Computational Linguistics.\nAndrew Drozdov, Patrick Verga, Mohit Yadav, Mohit\nIyyer, and Andrew McCallum. 2019. Unsupervised\nlatent tree induction with deep inside-outside recur-\nsive autoencoders. In Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics.\nAkiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun\nCho. 2017. Learning to parse and translate improves\nneural machine translation. In Proceedings of the\nAssociation for Computational Linguistics. Associa-\ntion for Computational Linguistics (ACL).\nMichel Galley, Jonathan Graehl, Kevin Knight, Daniel\nMarcu, Steve DeNeefe, Wei Wang, and Ignacio\nThayer. 2006. Scalable inference and training of\ncontext-rich syntactic translation models. In Pro-\nceedings of the Association for Computational Lin-\nguistics, pages 961–968. Association for Computa-\ntional Linguistics.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor OK\nLi, and Richard Socher. 2018. Non-autoregressive\nneural machine translation. In Proceedings of Inter-\nnational Conference on Learning Representations.\nJunliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and\nTie-Yan Liu. 2019. Non-Autoregressive Neural Ma-\nchine Translation with Enhanced Decoder Input. In\nAssociation for the Advancement of Artiﬁcial Intelli-\ngence.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\nZettlemoyer. 2018. Adversarial example generation\nwith syntactically controlled paraphrase networks.\nIn Conference of the North American Chapter of the\nAssociation for Computational Linguistics.\nŁukasz Kaiser and Samy Bengio. 2016. Can active\nmemory replace attention? In Proceedings of Ad-\nvances in Neural Information Processing Systems ,\npages 3781–3789.\nLukasz Kaiser, Samy Bengio, Aurko Roy, Ashish\nVaswani, Niki Parmar, Jakob Uszkoreit, and Noam\nShazeer. 2018. Fast decoding in sequence models\nusing discrete latent variables. In Proceedings of the\n35th International Conference on Machine Learn-\ning, volume 80 of Proceedings of Machine Learn-\ning Research, pages 2390–2399, Stockholmsm¨assan,\nStockholm Sweden. PMLR.\nYoon Kim and Alexander M Rush. 2016. Sequence-\nlevel knowledge distillation. In Proceedings of Em-\npirical Methods in Natural Language Processing ,\npages 1317–1327.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative reﬁnement. In Pro-\nceedings of Empirical Methods in Natural Language\nProcessing, pages 1173–1182.\nJindich Libovick and Jindich Helcl. 2018. End-to-\nEnd Non-Autoregressive Neural Machine Transla-\ntion with Connectionist Temporal Classiﬁcation. In\nProceedings of Empirical Methods in Natural Lan-\nguage Processing.\nThang Luong, Hieu Pham, and Christopher D Manning.\n2015. Effective approaches to attention-based neu-\nral machine translation. In Proceedings of Empiri-\ncal Methods in Natural Language Processing, pages\n1412–1421.\nChristopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven J. Bethard, and David Mc-\nClosky. 2014. The Stanford CoreNLP natural lan-\nguage processing toolkit. In Association for Compu-\ntational Linguistics (ACL) System Demonstrations ,\npages 55–60.\nYuval Marton and Philip Resnik. 2008. Soft syntac-\ntic constraints for hierarchical phrased-based trans-\nlation. Proceedings of the Association for Computa-\ntional Linguistics, pages 1003–1011.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191. Association for Computational Linguistics.\nHolger Schwenk. 2012. Continuous space translation\nmodels for phrase-based statistical machine transla-\ntion. Proceedings of International Conference on\nComputational Linguistics, pages 1071–1080.\n1279\nRico Sennrich and Barry Haddow. 2016. Linguistic\ninput features improve neural machine translation.\nIn Proceedings of the First Conference on Machine\nTranslation: Volume 1, Research Papers, volume 1,\npages 83–91.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), volume 1, pages\n1715–1725.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural mt learn source syntax? In Pro-\nceedings of Empirical Methods in Natural Language\nProcessing, pages 1526–1534.\nF Stahlberg, E Hasler, A Waite, and B Byrne. 2016.\nSyntactically guided neural machine translation. In\nProceedings of the Association for Computational\nLinguistics, volume 2, pages 299–305. Association\nfor Computational Linguistics.\nMitchell Stern, Noam Shazeer, and Jakob Uszkoreit.\n2018. Blockwise parallel decoding for deep autore-\ngressive models. In Advances in Neural Information\nProcessing Systems, pages 10106–10115.\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-Informed Self-Attention for Seman-\ntic Role Labeling. In Proceedings of Empirical\nMethods in Natural Language Processing, Brussels,\nBelgium.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Proceedings of Advances in Neural Information\nProcessing Systems, pages 3104–3112.\nSwabha Swayamdipta, Sam Thomson, Kenton Lee,\nLuke Zettlemoyer, Chris Dyer, and Noah A Smith.\n2018. Syntactic scaffolds for semantic structures. In\nProceedings of Empirical Methods in Natural Lan-\nguage Processing, pages 3772–3782.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nChunqi Wang, Ji Zhang, and Haiqing Chen. 2018.\nSemi-autoregressive neural machine translation. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing , pages\n479–488.\nYiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang\nZhai, and Tie-Yan Liu. 2019. Non-Autoregressive\nMachine Translation with Auxiliary Regularization.\nIn Association for the Advancement of Artiﬁcial In-\ntelligence.\nDekai Wu. 1997. Stochastic inversion transduction\ngrammars and bilingual parsing of parallel corpora.\nComputational linguistics, 23(3):377–403.\nKenji Yamada and Kevin Knight. 2001. A syntax-\nbased statistical translation model. In Proceedings\nof the Association for Computational Linguistics.\nAndreas Zollmann and Ashish Venugopal. 2006. Syn-\ntax augmented machine translation via chart parsing.\nIn Proceedings of the Workshop on Statistical Ma-\nchine Translation, pages 138–141. Association for\nComputational Linguistics.\n1280\nAppendix\nA Unscientiﬁc Comparison\nWe include a reference to previously published\nwork in comparison to our approach. Note, that\nmany of these papers have multiple confounding\nfactors that make direct comparison between ap-\nproaches very difﬁcult.\nModel WMT En-De\nBLEU Speedup\nLT rescoring top-100 22.5 -\nNAT rescoring top-100 21.54 -\nBPT (k = 6) 28.11 3.10×\nIRT (adaptive) 21.54 2.39×\nSAT (k = 6) 23.93 5.58×\nSynST(k = 6) 20.74 4.86×\nTable A1: Unscientiﬁc comparison against previously\npublished works. The numbers of each model are\ntaken from their respective papers. These previous re-\nsults often have uncomparable hyperparameters, com-\npute their BLEU with multi-bleu.perl, and/or\nrequire additional steps such as knowledge distilla-\ntion and re-ranking to achieve their reported numbers.\nLatent Transformer (LT) (Kaiser et al., 2018), Non-\nautoregressive Transformer (NAT) (Gu et al., 2018),\nBlockwise parallel Transformer (BPT) (Stern et al.,\n2018), Iterative reﬁnement Transformer (IRT) (Lee\net al., 2018), Semi-autoregressive Transformer (SAT)\n(Wang et al., 2018).\nB The impact of beam search\nIn order to more fully understand the interplay of\nthe representations output from the autoregressive\nparse decoder on the BLEU/speedup tradeoff we\nexamine the impact of beam search for the parse\ndecoder. From Table A2 we see that beam search\ndoes not consitently improve the ﬁnal translation\nquality in terms of BLEU (it manages to decrease\nBLEU on IWSLT), while providing a small reduc-\ntion in overall speedup for SynST.\nC SAT replication results\nAs part of our work, we additionally replicated the\nresults of (Wang et al., 2018). We do so without\nany of the additional training stabilization tech-\nniques they use, such as knowledge distillation or\ninitializing from a pre-trained Transformer. With-\nout the use of these techniques, we notice that\nthe approach sometimes catastrophically fails to\nconverge to a meaningful representation, leading\nto sub-optimal translation performance, despite\nachieving adequate perplexity. In order to report ac-\ncurate translation performance for SAT, we needed\nto re-train the model for k = 4when it produced\nBLEU scores in the single digits.\nD Parse performance when varying max\nchunk size k\nIn Section 5.3 (see the ﬁnal row of Table 3) we\nconsider the effect of randomly sampling the max\nchunk size k during training. This provides a con-\nsiderable boost to BLEU with a minimal impact to\nspeedup. In Table A3 we highlight the impact to\nthe parse decoder’s ability to predict the ground-\ntruth chunk sequences and how faithfully it follows\nthe predicted sequence.\n1281\nModel Beam WMT En-De WMT De-En IWSLT En-De WMT En-Fr\nWidth BLEU Speedup BLEU Speedup BLEU Speedup BLEU Speedup\nTransformer 1 25.82 1.15× 29.83 1.14× 28.66 1.16× 39.41 1.18×\nTransformer 4 26.87 1.00× 30.73 1.00× 30.00 1.00× 40.22 1.00×\nSAT (k = 2) 1 22.81 2.05× 26.78 2.04× 25.48 2.03× 36.62 2.14×\nSAT (k = 2) 4 23.86 1.80× 27.27 1.82× 26.25 1.82× 37.07 1.89×\nSAT (k = 4) 1 16.44 3.61× 21.27 3.58× 20.25 3.45× 28.07 3.34×\nSAT (k = 4) 4 18.95 3.25× 23.20 3.19× 20.75 2.97× 32.62 3.08×\nSAT (k = 6) 1 12.55 4.86× 15.23 4.27× 14.02 4.39× 24.63 4.77×\nSAT (k = 6) 4 14.99 4.15× 19.51 3.89× 15.51 3.78× 28.16 4.19×\nLT* - 19.8 3.89× - - - - - -\nSynST(k = 6) 1 20.74 4.86× 25.50 5.06× 23.82 3.78× 33.47 5.32×\nSynST(k = 6) 4 21.61 3.89× 25.77 4.07× 23.31 3.11× 34.10 4.47×\nTable A2: Controlled experiments comparing SynST to LT and SAT on four different datasets (two language pairs)\ndemonstrate speed and BLEU improvements while varying beam size. Wall-clock speedup is measured on a single\nNvidia TitanX Pascal by computing the average time taken to decode a single sentence in the dev/test set, averaged\nover ﬁve runs. Note that the LT results are reported by Kaiser et al. (2018) and not from our own implementation;\nas such, they are not directly comparable to the other results.\nMax Chunk Size\nPredicted parse\nvs.\nGold parse\nParsed prediction\nvs.\nGold parse\nParsed prediction\nvs.\nPredicted parse\nF1 k = 6 69.64 79 .16 89 .90\nExact match 5.24% 5 .94% 43 .10%\nF1 k ∈{1 . . .6} 75.35 79 .78 95 .28\nExact match 4.83% 7 .55% 50 .15%\nTable A3: F1 and exact match comparisons of predicted chunk sequences (from the parse decoder), ground-truth\nchunk sequences (from an external parser in the target language), and chunk sequences obtained after parsing the\ntranslation produced by the token decoder. The ﬁrst column shows how well the parse decoder is able to predict\nthe ground-truth chunk sequence when trained jointly with the token decoder. The second column shows that when\nthe token decoder deviates from the predicted chunk sequence, it usually results in a translation that is closer to the\nground-truth target syntax, while the third column shows that the token decoder closely follows the predicted chunk\nsequence. Randomly sampling k from {1 . . .6}during training signiﬁcantly boosts the parse decoder’s ability to\nrecover the ground-truth chunk sequence compared to using a ﬁxedk = 6. Subsequently the token decoder follows\nthe chunk sequence more faithfully."
}