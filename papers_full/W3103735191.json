{
  "title": "Grammaticality and Language Modelling",
  "url": "https://openalex.org/W3103735191",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2950723268",
      "name": "Jingcheng Niu",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2001595673",
      "name": "Gerald Penn",
      "affiliations": [
        "University of Toronto"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2951328679",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W1984466986",
    "https://openalex.org/W2124669395",
    "https://openalex.org/W2109553965",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2140842551",
    "https://openalex.org/W2134991647",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2151222319",
    "https://openalex.org/W2864832950",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2531882892",
    "https://openalex.org/W2805227907",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2404032066",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2251529809",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2994726368",
    "https://openalex.org/W2111292987",
    "https://openalex.org/W2072364373",
    "https://openalex.org/W2915130814"
  ],
  "abstract": "Ever since Pereira (2000) provided evidence against Chomsky’s (1957) conjecture that statistical language modelling is incommensurable with the aims of grammaticality prediction as a research enterprise, a new area of research has emerged that regards statistical language models as “psycholinguistic subjects” and probes their ability to acquire syntactic knowledge. The advent of The Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019) has earned a spot on the leaderboard for acceptability judgements, and the polemic between Lau et al. (2017) and Sprouse et al. (2018) has raised fundamental questions about the nature of grammaticality and how acceptability judgements should be elicited. All the while, we are told that neural language models continue to improve. That is not an easy claim to test at present, however, because there is almost no agreement on how to measure their improvement when it comes to grammaticality and acceptability judgements. The GLUE leaderboard bundles CoLA together with a Matthews correlation coefficient (MCC), although probably because CoLA’s seminal publication was using it to compute inter-rater reliabilities. Researchers working in this area have used other accuracy and correlation scores, often driven by a need to reconcile and compare various discrete and continuous variables with each other. The score that we will advocate for in this paper, the point biserial correlation, in fact compares a discrete variable (for us, acceptability judgements) to a continuous variable (for us, neural language model probabilities). The only previous work in this area to choose the PBC that we are aware of is Sprouse et al. (2018a), and that paper actually applied it backwards (with some justification) so that the language model probability was treated as the discrete binary variable by setting a threshold. With the PBC in mind, we will first reappraise some recent work in syntactically targeted linguistic evaluations (Hu et al., 2020), arguing that while their experimental design sets a new high watermark for this topic, their results may not prove what they have claimed. We then turn to the task-independent assessment of language models as grammaticality classifiers. Prior to the introduction of the GLUE leaderboard, the vast majority of this assessment was essentially anecdotal, and we find the use of the MCC in this regard to be problematic. We conduct several studies with PBCs to compare several popular language models. We also study the effects of several variables such as normalization and data homogeneity on PBC.",
  "full_text": "Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP), pages 110–119,\nNovember 20, 2020.c⃝2020 Association for Computational Linguistics\n110\nGrammaticality and Language Modelling\nJingcheng Niu and Gerald Penn\nDepartment of Computer Science\nUniversity of Toronto\nToronto, Canada\n{niu,gpenn}@cs.toronto.edu\nAbstract\nEver since Pereira (2000) provided evidence\nagainst Chomsky’s (1957) conjecture that sta-\ntistical language modelling is incommensu-\nrable with the aims of grammaticality predic-\ntion as a research enterprise, a new area of re-\nsearch has emerged that regards statistical lan-\nguage models as “psycholinguistic subjects”\nand probes their ability to acquire syntactic\nknowledge. The advent of The Corpus of Lin-\nguistic Acceptability (CoLA) (Warstadt et al.,\n2019) has earned a spot on the leaderboard\nfor acceptability judgements, and the polemic\nbetween Lau et al. (2017) and Sprouse et al.\n(2018) has raised fundamental questions about\nthe nature of grammaticality and how accept-\nability judgements should be elicited. All the\nwhile, we are told that neural language models\ncontinue to improve.\nThat is not an easy claim to test at present,\nhowever, because there is almost no agreement\non how to measure their improvement when\nit comes to grammaticality and acceptability\njudgements. The GLUE leaderboard bundles\nCoLA together with a Matthews correlation\ncoefﬁcient (MCC), although probably because\nCoLA’s seminal publication was using it to\ncompute inter-rater reliabilities. Researchers\nworking in this area have used other accuracy\nand correlation scores, often driven by a need\nto reconcile and compare various discrete and\ncontinuous variables with each other.\nThe score that we will advocate for in this pa-\nper, the point biserial correlation, in fact com-\npares a discrete variable (for us, acceptabil-\nity judgements) to a continuous variable (for\nus, neural language model probabilities). The\nonly previous work in this area to choose the\nPBC that we are aware of is Sprouse et al.\n(2018), and that paper actually applied it back-\nwards (with some justiﬁcation) so that the lan-\nguage model probability was treated as the dis-\ncrete binary variable by setting a threshold.\nWith the PBC in mind, we will ﬁrst reappraise\nsome recent work in syntactically targeted lin-\nguistic evaluations (Hu et al., 2020), arguing\nthat while their experimental design sets a new\nhigh watermark for this topic, their results may\nnot prove what they have claimed. We then\nturn to the task-independent assessment of\nlanguage models as grammaticality classiﬁers.\nPrior to the introduction of the GLUE leader-\nboard, the vast majority of this assessment was\nessentially anecdotal, and we ﬁnd the use of\nthe MCC in this regard to be problematic. We\nconduct several studies with PBCs to compare\nseveral popular language models. We also\nstudy the effects of several variables such as\nnormalization and data homogeneity on PBC.\n1 Background\nThe three currently most popular means of evalu-\nating a neural language model are: (1) perplexity,\nan information-theoretic measure that was in use\nlong before neural networks became the preferred\nmeans of implementing language models; (2) task\nperformance proﬁles, in which derivative aspects\nof a language model’s predictions are embedded\nin a so-called “downstream” task, with all other\naspects of the implementation held constant; and\n(3) targeted linguistic evaluations, the purpose of\nwhich is to demonstrate speciﬁc syntactic general-\nizations that a candidate model implicitly captures\nor does not capture. These targeted evaluations\nmust take place on a large number of small data\nsets in order to control for the syntactic and lexical\nvariations that we witness among sentences in a\nrealistic corpus.\nThe purpose of this paper is ultimately to ﬁnd\na task-independent means of testing how well lan-\nguage model probabilities might serve as gram-\nmaticality regression scores. Using evidence from\ntargeted linguistic evaluations, we argue for the\npoint-biserial correlation as at least the basis of\n111\nsuch a task-independent measure, and then use the\nPBC to examine several neural models along with\nsome important variables that affect both their eval-\nuation and the data that we evaluate on.\nBorrowing a convention from linguistic theory,\nMarvin and Linzen (2018) coined the use of “mini-\nmal pairs” as input to language models in order to\ntest these ﬁne-grained variations. For example:\n(1) Reﬂexive pronoun in a sentential complement:\na. The bankers thought the pilot embar-\nrassed himself.\nb. *The bankers thought the pilot embar-\nrassed themselves.\n(2) Reﬂexive pronoun across an object relative\nclause:\na. The manager that the architects like\ndoubted himself.\nb. *The manager that the architects like\ndoubted themselves.\nThese pairs deal with referential agreement in spe-\nciﬁc syntactic environments. If a model assigns the\ngrammatical string in a pair a higher score than the\nungrammatical string, then we say that the model\nmade the correct prediction on that pair. Having\nevaluated the model over a large number of these\npairs, we can compute an accuracy score, relative\nto a 50% random baseline.\nHu et al. (2020) have taken exception to the de-\nsign of many such evaluations on that grounds that:\n(1) a number of English nouns are stereotypically\ngendered, which conditions pronoun choice, and\n(2) the unigram probabilities of reﬂexive pronouns\nare different, which biases the probabilities that\nmodels assign to sentences that contain them. To\ncircumvent these shortcomings, they generalized\nthe pairs to larger sets of strings in which multi-\nple nouns were used in multiple positions so that\nlexical choice and order could be permuted across\nsets. They also introduced distractors, grammatical\nstrings that contain material irrelevant, or distract-\ning, to the determination of the sentence’s gram-\nmaticality. One set that they use, for example, is:\n(1B) The girl said that the mother saw herself.\n(2B) The mother said that the girl saw herself.\n(1D) The girls said that the mother saw herself.\n(2D) The mothers said that the girl saw herself.\n(1U) The girl said that the mothers saw herself.\n(2U) The mother said that the girls saw herself.\nwhere (B) is a baseline grammatical string, (D) is a\ndistractor, and (U) is an ungrammatical string. This\nset has six strings, but sets in their experiments can\nhave as many as 48 strings each, with as many as\n75 sets in a single experiment, each one having\na unique target pronoun in all of its strings. Be-\ncause here it is the context that varies, rather than\nthe pronoun, Hu et al. (2020) must rank the condi-\ntional probabilities of the pronoun in these various\ncontexts, rather than total sentence probabilities.\nHu et al. (2020) also evaluate models with ac-\ncuracies. Because there are three classes of string,\nrather than two, a model is said to have made the\ncorrect prediction if the ungrammatical data receive\na lower score than both the baseline and distractor\ndata. But because there are more than three strings,\nthey do not compare individual scores from the can-\ndidate model, but rather the three means that result\nfrom averaging the conditional pronoun probabili-\nties of the baseline, distractor and ungrammatical\nstrings, respectively.\nThis alternative design not only provided bet-\nter accuracies than were achieved by Marvin and\nLinzen (2018), but the inclusion of distractors in\nthe design lowers the random baseline from 50%\nto 33.3% accuracy. Hu et al. (2020) conclude that\ncurrent neural language models are learning more\nabout the licensing of reﬂexive anaphora than was\npreviously thought.\n2 Theoretical Exceptions\nIn a typical psycholinguistics experiment, we\nwould give human subjects a task to perform dur-\ning which they would be presented with a stimulus\nthat was labelled as either baseline, distractor or\nungrammatical. The effect of the stimulus on the\ntask could be measured by time to completion, the\nnumber of correct tokens retrieved during a ﬁxed\ninterval of time, etc. Regardless, the task would\nalmost certainly be chosen so that samples of its\ncorresponding measure of success would be nor-\nmally distributed. So a within-subjects mean of\nthese quantities is entirely justiﬁable.\nThe situation is somewhat less clear with the\nscores that are returned by a neural language model.\nIgnoring for the moment that Hu et al. (2020) are\ninterested in conditional pronoun probabilities and\n112\nnot sentence probabilities, the scores are gener-\nally not regarded as measures of success on a task\nper se— there is no actual task here, apart from\nachieving a high rank in the evaluation. Legiti-\nmate task performance proﬁles are deﬁned over\nseparate downstream tasks, such as those in the\nGLUE leaderboard (Wang et al., 2018). It is rather\nmore difﬁcult to think of downstream tasks that\ndepend on conditional pronoun probabilities, how-\never. Note that for Marvin and Linzen (2018), the\nratio of conditional pronoun probabilities of a set\nof stimuli was the same as the ratio of their total\nsentence probabilities because the reﬂexive pro-\nnoun is always the last word of the sentence, and\nthe contexts preceding the pronouns are always\nidentical.\nSeveral papers by Lau et al., culminating in\nLau et al. (2017), have argued instead that sen-\ntence probabilities can justiﬁably be interpreted as\ngradient grammaticality scores, rejecting the long-\nstanding assumption in generative linguistics that\ngrammaticality is a binary judgement. It is also pos-\nsible to regard sentence probabilities as summaries\nof group behaviour, such as relative frequencies of\nbinary grammaticality judgements across multiple\nindividual participants, with no claim of gradience\nimplied for any single participant. This in turn\nraises the very old question of whether neural net-\nworks in fact have any cognitive plausibility, which\nhas recently started to be debated again (Cichy and\nKaiser, 2019). Sample distributions of means con-\nverge to a normal distribution even if the underlying\npopulation distribution is not normal itself, and so\nwhether an average would be justiﬁed in this group\ninterpretation would depend to a great extent on the\nsizes of the sets of strings (relatively small, as we\nhave seen) as well as how skewed the underlying\ndistribution was.\n3 Empirical Exceptions\n3.1 Signiﬁcance Test: Normality\nUsing Hu et al.’s (2020) publicly available exper-\nimental results, 1 we administered Levene’s test\nof homoscedasticity to every set of probabilities,\ngiven a ﬁxed stimulus set, model and experimen-\ntal context. Levene’s test attempts to reject the\nnull hypothesis that a set of continuous data is nor-\nmal. Levene’s test was successful for 22.5% of\nHu et al.’s (2020) sets at a conﬁdence threshold of\n1https://github.com/jennhu/reﬂexive-anaphor-licensing.\nFigure 1: Surprisals (negative log probabilities) for ev-\nery set in experiment 1b for the GRNN model withher-\nself, on the left, and for the TransXL model with them-\nselves on the right.\nα = 0.05, and marginally successful for an addi-\ntional 8% at α= 0.1. This means that somewhere\nbetween 20–30% of the sets are provably not nor-\nmal. Homoscedasticity is merely one aspect of\nnormal distributions that can be used to prove that\na distribution is not normal.\n3.2 Signiﬁcance Test: Mean Differentials\nIn view of the previous section’s results, we elected\nto use the non-parametric Mann-Whitney U-test to\ndetermine, on a set-by-set basis, whether the prob-\nability that: “the score of a grammatical (meaning\nbaseline or distractor) string is greater than that\nof an ungrammatical string” is signiﬁcantly differ-\nent from the probability that it is less. This does\nnot determine the difference between means be-\ncause it cannot quantify effect size, nor does it\neven determine the sign of the difference. This is\nan alternative, very minimalist way of formalizing\nthat a language model has made the correct predic-\ntion — it can simply distinguish grammatical from\nungrammatical, somehow.\nLet us consider part of Hu et al.’s (2020) ex-\nperiment 1b as an example, shown in Figure 1.\nThere would be little disagreement that the model\non the right (Transformer-XL with the pronoun\nthemselves) had made better predictions than the\nmodel on the left (an LSTM with the pronoun her-\nself ), and yet under both of these conditions the ac-\ncuracy is 100%. Large differences involving strings\nat either extreme help to offset a number of smaller\ndifferences of the wrong sign when computing dif-\nferences in means.\nAcross all experiments, 44.3% of the sets in\nwhich the mean differentials qualiﬁed for the nu-\nmerator of the accuracy computation (i.e., the un-\ngrammatical mean was less than both the baseline\n113\nand distractor means) failed to show a signiﬁcant\ndifference under the criterion of the Mann-Whitney\ntest. A further 90% of the sets in which the mean\ndifferential did not qualify for the numerator (i.e.,\nthey were taken not to have been correctly pre-\ndicted) also failed to show a signiﬁcant difference.\nOf the 60 combinations of pronoun and experimen-\ntal context that we examined, 24 did not have even\na single set that showed signiﬁcance in the numera-\ntor. Of the 42 combinations that did not have 100%\naccuracies, 32 did not have even a single set that\nshowed signiﬁcance.\nIn our view, although we agree with every one of\nthe design modiﬁcations made by Hu et al. (2020)\nto targeted evaluations such as these, the decision\nto continue using accuracy and to generalize it in\nthis way seems not to be working well.\n3.3 Matthews Correlation Coefﬁcients\nThis is potentially a much more pervasive prob-\nlem than just with Hu et al.’s (2020) experiments.\nMCCs have emerged as a popular alternative\namong language modelling enthusiasts (Liu et al.,\n2019; Lan et al., 2020; Raffel et al., 2019) since\ngrammaticality classiﬁcation with The Corpus of\nLinguistic Acceptability (CoLA) (Warstadt et al.,\n2019) was incorporated into the GLUE standard\n(Wang et al., 2018). Warstadt et al. (2019) them-\nselves began using MCCs, initially to validate the\nCoLA corpus, but also to interpret Lau et al.’s\n(2017) gradient models. MCCs cannot be com-\nputed directly on continuous data, which means\nnot only that they are insensitive to the magnitudes\nof probabilities, but also that a threshold must be\nset in order to impose a discrete boundary between\nclasses. Defending that choice of boundary can\nbe difﬁcult. Consider Figure 2, for example. In\na sample as small as a typical minimal set, cross-\nvalidating the MCC decision threshold is not realis-\ntic, so here we used the mean of both classes of data.\nIn this particular set, two low-surprisal distractors\ncause a lot of damage to the distractor vs. ungram-\nmatical MCC and the baseline-plus-distractor vs.\nungrammatical MCC. Another correlation score,\ncalled the point-biserial correlation, which can be\ncomputed directly on continuous data, does not\nrequire an arbitrary threshold and produces very\ndifferent values on this one example.\nFigure 2: Surprisals (negative log probabilities),\nMatthews correlations and point-biserial correlations\nfor set 42 in Hu et al.’s (2020) experiment 1b for the\nGRNN model with the pronoun, himself.\n4 Aggregated Point Biserial Correlations\nOur proposed alternative involves two changes.\nFirst, we propose using a point biserial correlation\nbetween the output probability of a language model\nand binary grammaticality judgements. Second, we\npropose calculating PBCs not on a set-by-set basis,\nbut for all probabilities generated by a ﬁxed model\nusing all of the contexts of a ﬁxed experiment.\nTo consider Figure 1 again, the model on the\nleft has a PBC of 0.25, whereas the model on the\nright has one of 0.73. Correlations such as the\nPBC range between -1 and 1, where 1 is perfect\ncorrelation, 0 is no correlation, and -1 is perfect\nanti-correlation.\nOur choice of PBC is perhaps the less contro-\nversial of these two changes, as our motivation for\ndoing so is mainly due to the fact that it is the\nstandard measure for correlating a continuous or\ninterval random variable with a discrete random\nvariable.\nOur decision to “aggregate” data by ignoring\nthe boundaries between the controlled, minimal\nsets that have become so widely accepted as a\npart of targeted syntactic evaluations is perhaps\ncounterintuitive. But as long as the necessary dis-\ntractors, permutations and lexical alternations that\navoid bias appear somewhere in the context of the\nexperiment, they will be compared to each other,\nalthough along with additional comparisons that\nwere not made when accuracy was averaged over\nsets. Those additional comparisons, however, will\nmerely corroborate the model’s (in)ability to more\nrobustly distinguish between well-formed and non-\n114\nwell-formed strings, and the experiment itself does\nrestrict the variability of those comparisons to a\ngreat extent.\nIn our experience, aggregating makes the evalua-\ntion more resilient to choices of normalizers such\nas SLOR (Pauls and Klein, 2012), its results are in\ncloser accord to our intuitive judgements, and, as\nexpected, it handles sample bias better. Both accu-\nracy (30–100%) and aggregate PBC (-0.01–0.81)\nvary widely from experiment to experiment in Hu\net al.’s (2020) data, and yet the average of per-set\nPBCs tends to be less dispersed. The experiments\nin Figure 1, for example, have microaveraged PBCs\nof 0.77 (left) and 0.89 (right). It could therefore be\nargued that the effect size of the dependent variable\nthat Hu et al. (2020) were attempting to measure\nis not as large as the choice of minimal set. Ag-\ngregation would then also be an effective means of\nutilizing the available range of correlation values.\nTotal (weighted) accuracy and baseline-plus-\ndistractor vs. ungrammatical PBC have a Spear-\nman’s correlation of 0.876 (p= 8.5×10−25) across\nHu et al.’s (2020) experiments and models.\n5 Task-Independent Grammaticality\nClassiﬁcation\nThe famous “Colorless green ideas sleep furiously”\n(CGISF) example (Chomsky, 1957) posited a seem-\ningly irreconcilable divide between formal linguis-\ntics and statistical language modelling, arguing that\nevery sequence of words not attested in the collec-\ntive memory of a language’s use would be consid-\nered equally “remote” by a putative instance of the\nlatter, regardless of whether the sequence was gram-\nmatical (CGISF) or ungrammatical. The example\nwas presented brieﬂy and informally in order to\nreject statistical language modelling as an alterna-\ntive approach to the one advocated and developed\nin greater detail by Chomsky (1957). It was only\npresented with one other example, the reverse of\nthe sentence, i.e., “Furiously sleep ideas green col-\norless”, in order to draw a contrast between two\nnonsensical sequences, only one of which (CGISF)\nis grammatical.\nPereira (2000) provides an attempt at a refuta-\ntion by constructing a statistical language model\nbased upon an agglomerative Markov process (Saul\nand Pereira, 1997), and then observing that CGISF\nis assigned a probability by the model which is\nroughly 200 000 times greater than the probability\nassigned to its reversal.\nThere has nevertheless been some scepticism\nexpressed about the ensuing euphoria among com-\nputer scientists — mainly by linguists. Sprouse\net al. (2015) notes that the trigram model from Lau\net al. (2015) assigns different rankings to 10 dif-\nferent permutations of CGISF, depending on the\ntraining corpus (e.g., the Wall Street Journalcor-\npus versus an example training corpus taken from\nLau et al. (2015)). Can the scores assigned to these\nsequences be reliably construed as a regression\nscale of grammaticality (or perhaps acceptability),\nif they are so ﬁckle? Chowdhury and Zamparelli\n(2018) also express concern about the ability of\nneural language models to generalize to more ab-\nstract grammatical phenomena than subject-verb\nagreement.\nWhat we will present in this section is a more\nthorough appraisal of how well statistical language\nmodels perform as instruments of grammaticality\ntesting overall, using PBCs. Previous research on\ngrammaticality/acceptability and language models\nhas mainly designed experiments using naturally\noccurring English sentences, and modiﬁes those\nsentences based on various individual linguistic\nphenomena to manually introduce a speciﬁc source\nof ungrammaticality into the sentences. Notable ex-\nceptions include CoLA as well as the Linguistic In-\nquiry (LI) corpus of grammatical and ungrammati-\ncal sentences collected by Sprouse et al. (2013) and\nSprouse and Almeida (2012), and used in Sprouse\net al. (2018). Both are based upon examples found\nin linguistics publications. Lau et al. (2014, 2015,\n2017) create ungrammatical sentences by round-\ntrip translating natural English sentences. We will\nuse both CoLA and the LI corpus.\n5.1 CoLA\nThe Corpus of Linguistic Acceptability (CoLA)\n(Warstadt et al., 2019) is a collection of 10 657 ex-\nample sentences from linguistics publications with\ntheir grammaticality judgements. It forms an inte-\ngral part of the General Language Understanding\nEvaluation (GLUE) benchmark (Wang et al., 2018).\nIt must be noted, however, that their linguistic ac-\nceptability task is supervised (CoLA was divided\ninto a training set (8551), a development set (1043),\nand a test set (1063)), with both positive and nega-\ntive samples. The ungrammatical strings in CoLA\nhave generally been devised to illustrate a speciﬁc\ngrammatical defect, and are often but not always\nsensical. Recent systems trained on these labelled\n115\ndata, e.g., Liu et al. (2019); Lan et al. (2020); Raffel\net al. (2019), are able to attain a reported roughly\n70% Matthews correlation coefﬁcient (Matthews,\n1975).\nThe performance of Mikolov’s (2012) model, in\nparticular, has been reported in CoLA studies as a\nbaseline (Warstadt et al., 2019; Lau et al., 2017).\nWarstadt et al. (2019) did a 10-fold cross-validation\non CoLA test set, which ﬁt an optimum decision\nthreshold to the softmax output of each fold to as-\nsign grammaticality labels, and obtained a 0.652\nin-domain accuracy and 0.711 out-of-domain ac-\ncuracy. This ﬁgure has been cited as a gauge for\nassessing the ability of statistical language models\nto learn grammar-related patterns in an unsuper-\nvised fashion (Lappin and Lau, 2018).\nCoLA also did not include any annotations of\nminimal set structures, but we retained a linguist\nwho is a native speaker of North American English\nto go over the ﬁrst 2010 sentences in the CoLA cor-\npus and group them into 1803 microgroups (includ-\ning singletons) fashioned around the same linguis-\ntic phenomena of interest, and often very similar\nlexical entries. This enabled us to use CoLA as\na platform to test language model performance in\nsomewhat controlled microgroups of example sen-\ntences, although they are not as well controlled as\nthe minimal sets of targeted evaluations. Then we\nran point-biserial correlation tests within those mi-\ncrogroups with at least one grammatical judgement\nand at least one ungrammatical judgement, and\ncalculated the median of those correlation scores.\nThen we split the scores into four quadrants. Below,\nwe report the junction points of those quadrants:\nlower breakpoint, median, and upper breakpoint.\n5.2 The LI Corpus\nThe LI corpus was collected by Sprouse and\nAlmeida (2012), and contains 300 sentence struc-\ntures, each expanded into 8 candidate sentences\n(2400 strings in total, 1192 of them grammatical).\nThe corpus annotation shows that there are 57 pairs\nof sentence structures (912 strings in total) that are\nsyntactically designed to differ on one linguistic\nphenomenon but have putatively opposite gram-\nmaticality. We ran the PBC test for each of the 57\npairs, and calculated the medians among the cor-\nrelation scores. Sprouse and Almeida (2012) also\ncollected 230 sentences structures from Adger’s\n(2003) textbook. However that corpus does not\ninclude annotation indicating the minimal set struc-\nture, and therefore was ignored in this study.\n5.3 Language Models\nWe investigated four different types of language\nmodels: Pereira’s (2000) original aggregative\nMarkov model (Saul and Pereira, 1997), Mikolov’s\n(2012) original RNN language model (Mikolov,\n2012), a QRNN-based language model (Merity\net al., 2018) that we take to be representative\nof contemporary models, and GPT-2 (Radford\net al., 2019) as the representative of large-scale\npre-trained language models. Mikolov’s model is\nalso used by Clark et al. (2013); Lau et al. (2015);\nSprouse et al. (2018) in their research about gra-\ndient acceptability. We chose GPT-2 over other\nlarge-scale pre-trained models such as BERT (De-\nvlin et al., 2019) and XLNet (Yang et al., 2019), be-\ncause it took the more orthodox autoregressive lan-\nguage modelling approach that is consistent with\nthe remaining choices, and it is most commonly\nused for natural language generation for the same\nreason.\nWe obtained a publicly available implementa-\ntion of each of the four language models 2. The\nimplementation of the tied adaptive softmax (TAS)\nmethod3 used the unusual approach of applying\nsoftmax on already softmaxed values. For this rea-\nson, we also experiment on QRNN models trained\nusing regular cross-entropy loss functions.\nAll three non-pretrained models are trained on\nthe BNC (BNC Consortium, 2007) and WikiText-\n103 (WT103) (Merity et al., 2017). We used the hy-\nperparameters described in (Pereira, 2000) to train\nits model, the hyperparameters described in (Lau\net al., 2017; Sprouse et al., 2018) to train Mikolov’s,\nand the hyperparameters suggested by the ofﬁcial\nSalesForce implementation of the QRNN model.\nThe BNC corpus is tokenized based on BNC an-\nnotations, and all tokens are converted into lower\ncase. For WT103, we used the ofﬁcial preprocessed\ncorpus released on SalesForce’s website4 that has\ntokenization, converts low frequency words to unk\nand preserves letter case. Radford et al. (2019) re-\n2For Pereira’s model, we adapted the implementa-\ntion of https://github.com/hsgodhia/agm language model;\nfor Mikolov’s model, we used the implementation of\nhttps://github.com/yandex/faster-rnnlm that is also used by\nLau et al. (2017) and Sprouse et al. (2018); and for GPT-2, we\nused the HuggingFace’s Transformers package (Wolf et al.,\n2020).\n3https://github.com/salesforce/awd-lstm-lm\n4https://blog.einstein.ai/the-wikitext-long-term-\ndependency-language-modeling-dataset/\n116\nModel BNC WT103\nPereira 362.19 460.62\nMikolov 332.04 185.82\nQRNN (regular) 173.57 96.65\nQRNN (TAS) 92.85 34.98\nGPT-25 45.61 28.34\nGPT-2 XL5 24.92 16.28\nTable 1: Perplexity achieved on test sets\nleased GPT-2 models in four different parameter\nsizes: GPT2 (small), GPT2-medium, GPT2-large\nand GPT2-xl (extra large). To avoid redundancy,\nwe experimented with GPT2, which has a simi-\nlar number of parameters as the other neural lan-\nguage models, and GPT2-xl, which represents the\nmaximum potential of the GPT-2 architecture. Bet-\nter performance would likely be achieved through\nmore extensive hyperparameter optimization, but\nour results in Table 1 are already comparable to\nthe performance reported in the respective original\npublications.\n5.4 Experimental Design\nOur experiments consider two types of probabili-\nties: the log probability ℓ= logp(s), and the actual\nprobability, eℓ, where sis a sentence. For each type\nof probability, we also consider two to three differ-\nent types of normalization methods: no normaliza-\ntion (raw), normalization by length (norm) ℓ/|s|,\nand SLOR (Pauls and Klein, 2012) (ℓ−ℓu)/|s|,\nwhere |s|is the length of the sentence and ℓu is the\nlog unigram probability of the sentence. For all\nthree non-pretrained models, the unigram probabil-\nity was obtained from BNC/WT103 with add-one\nsmoothing. We used WT103 unigram probabilities\nfor GPT-2 models since they preserve case.\n5.5 Letter Case\nIt is a paradigm that linguists often consider seman-\ntics and pragmatics when trying to generate non-\nsyntactic factors that attribute to language model\nprobabilities. We also considered letter case in\norder to demonstrate that a more superﬁcial fact\nabout the writing system may affect the evaluation\nresult. Pereira’s (2000) model downcased all input\ntokens to speed up the training process, thus it was\ndiscarded for this experiment. We took the rest of\nthe models that are trained on WT103 and GPT-2\nmodels and provided them with downcased CoLA\nexample sentences.\n5GPT-2 models are evaluated on the same preprocessed\nBNC and WT103 test sets without any ﬁne-tuning for the sake\nof consistency.\nModel Norm. BNC WT103\nLOG EXP LOG EXP\nRaw 0.0239 0.0139 0.0226 -0.0137\nPereira Norm 0.0494 0.0206 0.043 -0.0012\nSLOR 0.0756 -0.0153 0.0684 0.0083\nRaw 0.0578 0.0223 0.0574 0.0086\nMikolov Norm 0.1061 0.1161 0.106 0.1146\nSLOR 0.1896 0.1529 0.1045 0.0359\nQRNN Raw 0.0029 -0.0153 0.0121 -0.0093\nregular Norm 0.0191 0.0328 0.0124 0.0224\nSLOR 0.0496 0.0346 0.0297 0.0134\nQRNN Raw 0.0067 -0.0137 0.0162 0.0047\nTAS Norm 0.0029 -0.0153 0.0278 0.0421\nSLOR 0.0542 0.0356 0.0332 0.0112\nGPT-2 GPT-2 XL\nGPT-2 Raw 0.1839 0.0117 0.1476 0.0123\nModels Norm 0.2498 0.1643 0.2241 0.1592\nSLOR 0.2489 0.092 0.2729 0.0872\nTable 2: CoLA Point-biserial Test Results\nModel Norm. BNC WT103\nLOG EXP LOG EXP\nPereira Raw 0.0231 -0.0136 -0.0027 -0.0488\nNorm 0.0758 0.0595 0.038 0.0412\nMikolov Raw 0.0841 0.1868 0.1278 0.1914\nNorm 0.2541 0.2465 0.1955 0.2043\nQRNN Raw -0.0066 -0.2197 0.0201 0.0186\nTAS Norm 0.068 0.0726 0.1058 0.0764\nQRNN Raw -0.0135 -0.059 0.0232 0.1251\nregular Norm 0.042 0.0245 0.1057 0.104\nGPT-2 GPT-2 XL\nGPT-2 Raw 0.4671 0.26 0.4767 0.266\nModels Norm 0.5233 0.487 0.5653 0.5131\nTable 3: Sprouse LI Minimal Sets Results\n5.6 “Sensicality”\nCan we ﬁnd anything that matches language model\noutputs better than a grammaticality judgement?\nInspired by the debate over “Colorless green ideas\nsleep furiously” sixty years ago, we formed the\nhypothesis that grammatical sentences that make\nsense could more easily be distinguished from\ngrammatical sentences that are nonsense. We for-\nmulated 27 nonsense sentences (including CGISF),\nprojected their parts of speech into the BNC and\nfound 36 exact POS matches that do not overlap\nwith a clause or sentence boundary. The “sensi-\ncality” task is to distinguish these two sets using\nlanguage model log-probabilities.\n6 Experiment Results\nCoLA Point-Biserial Correlation Test Table 2\nshows our PBC test results. As mentioned before,\nevery non-GPT-2-based model is trained on either\nBNC or WT103, and for the sake of simplicity, we\nreport two sizes of GPT-2: small and XL.\nAll models show weak to no correlation. How-\never the correlation generated by GPT-2 models\ndoes show signiﬁcantly greater promise.\nLI Minimal Sets Table 3 shows the language\nmodels’ performance on the LI minimal sets.\n117\nModel Norm. Score all group sizes (≥ 2) group size > 4\nmedian up bkpt. low bkpt. median up bkpt. low bkpt.\nraw log 0.2148 0.8119 -0.4193 0.1409 0.4161 -0.0908\nPereira raw exp 0.3233 0.641 -0.4616 0.2501 0.3598 -0.2048\nBNC Norm log 0.3787 0.9026 -0.309 0.1865 0.4706 -0.1122\nNorm exp 0.3573 0.867 -0.3238 0.2753 0.4537 -0.1087\nraw log 0.2383 0.8278 -0.3901 0.159 0.4084 -0.1335\nPereira raw exp 0.3254 0.6642 -0.4874 0.2244 0.3466 -0.1778\nWT103 Norm log 0.3601 0.8849 -0.2933 0.2381 0.4849 -0.1449\nNorm exp 0.3599 0.9023 -0.299 0.2108 0.4438 -0.0941\nraw log 0.3838 0.8383 -0.294 0.2462 0.5453 -0.1127\nMikolov raw exp 0.3834 0.8092 -0.3073 0.262 0.4274 -0.1617\nBNC Norm log 0.291 0.8411 -0.4262 0.2159 0.4823 -0.2066\nNorm exp 0.3314 0.6903 -0.5 0.2382 0.4086 -0.0918\nraw log 0.3651 0.8506 -0.3714 0.2701 0.4835 -0.0786\nMikolov raw exp 0.3516 0.7988 -0.4765 0.2577 0.4199 -0.1876\nWT103 Norm log 0.4986 0.9303 -0.1553 0.2996 0.5332 -0.0953\nNorm exp 0.4918 0.9363 -0.1417 0.3012 0.5495 -0.0538\nQRNN raw log 0.0567 0.6549 -0.5812 0.0602 0.3224 -0.2602\nregular raw exp 0.2 0.5 -0.5633 0.1961 0.3085 -0.2605\nBNC Norm log 0.0418 0.682 -0.4762 -0.0436 0.3349 -0.3539\nNorm exp 0.0308 0.6565 -0.5317 -0.0275 0.3296 -0.3205\nQRNN raw log 0.0249 0.6086 -0.6074 0.0534 0.4419 -0.2144\nregular raw exp 0.2112 0.51 -0.5337 0.2483 0.3728 -0.1608\nWT Norm log 0.0507 0.8203 -0.5342 0.0291 0.3456 -0.3227\nNorm exp 0.084 0.8547 -0.5374 0.0428 0.3743 -0.2727\nQRNN raw log 0.0456 0.5834 -0.6022 0.1171 0.4084 -0.2532\nTAS raw exp 0.1487 0.5 -0.5595 0.2003 0.3268 -0.2474\nBNC Norm log 0.0919 0.6522 -0.5106 -0.0073 0.3629 -0.3003\nNorm exp 0.1248 0.6203 -0.5312 0.0379 0.3507 -0.3004\nQRNN raw log 0.1144 0.7072 -0.622 0.1187 0.4071 -0.1658\nTAS raw exp 0.2524 0.5003 -0.5773 0.2212 0.3173 -0.2568\nWT103 Norm log 0.2061 0.8411 -0.4252 0.0698 0.3924 -0.2324\nNorm exp 0.2226 0.8138 -0.4726 0.1536 0.3975 -0.1804\nraw log 0.6256 0.9491 0.1424 0.4128 0.6692 0.1424\nGPT-2 raw exp 0.4902 0.9999 0.2 0.2823 0.4417 0.1794\nNorm log 0.7121 0.9914 0.0528 0.2948 0.6688 0.0259\nNorm exp 0.6597 0.9968 0.1522 0.3294 0.6212 0.1105\nraw log 0.6936 0.9865 0.2862 0.4503 0.7155 0.1953\nGPT-2 raw exp 0.5 1.0 0.2642 0.2956 0.4714 0.2117\nXL Norm log 0.6858 0.9983 0.2411 0.4537 0.6561 0.1803\nNorm exp 0.6516 0.9987 0.2939 0.4312 0.5988 0.2477\nTable 4: CoLA Microgrouping Results\nModel Norm. LOG EXP\nwith case lower with case lower\nMikolov Raw 0.0578 0.0574 0.0223 0.0206\nNorm 0.1061 0.0955 0.1161 0.1012\nQRNN Raw 0.0029 0.0086 -0.0153 -0.0186\nregular Norm 0.0191 0.0135 0.0328 0.0149\nQRNN Raw 0.0067 0.0148 -0.0137 -0.0133\nTAS Norm 0.0309 0.0357 0.0301 0.0146\nGPT-2 Raw 0.1476 0.1129 0.0123 0.0125\nNorm 0.2241 0.1968 0.1592 0.1403\nGPT-2 XL Raw 0.1839 0.1484 0.0117 0.0149\nNorm 0.2498 0.2057 0.1643 0.1372\nTable 5: Letter Case Study Results\nAgain, the GPT-2 models stand out, but in this\ncase, GPT2-xl performs consistently better.\nCoLA Microgroups Table 4 shows the mi-\ncrogrouping results. The results could be inter-\npreted as conﬁrming our hypothesis: that better\ncontrolled input would improve a language model’s\nability to focus on distinguishing grammaticality.\nOn the other hand, it is also likely that the very\nsmall size of most microgroups is a factor, be-\ncause there is a dramatic correlation drop when\nwe evaluate on microgroups with size greater than\n4. Roughly 77% of the non-singleton microgroups\nin CoLA are of size 2-4.\nLetter Case Table 5 shows the letter case study’s\nresult. GPT-2 is once again the best, but it also\nsuffers the most from the loss of case. The loss is\nModel Norm. BNC WT GPT-2\nPereira raw 0.8235 0.7652\nSLOR 0.1838 0.1927\nMikolov raw 0.827 0.9042\nSLOR -0.3161 -0.1556\nQRNN raw 0.7132 -0.3872\nSLOR 0.089 -0.8038\nQRNN-R raw 0.8064 0.5895\nSLOR 0.6598 -0.7192\nGPT-2 raw 0.7574\nSLOR 0.5486\nGPT-2 XL raw 0.7642\nSLOR 0.5218\nTable 6: Sensicality Results\ncomparable to the loss incurred by scaling the XL\nmodel’s size (1542M) back to small (117M).\nSensicality The sensicality study reveals much\nhigher PBC scores overall, although SLOR has a\nmarkedly detrimental effect overall. While this set\nof judgements is small, these scores are markedly\nhigher than the PBCs for the microgroupings as\nwell, all but one of which is smaller.\n7 Discussion\nIn this paper, we examined the motivation and ef-\nfects of using accuracy scores vs. PBC in syntacti-\ncally targeted models. We also used PBC to evalu-\nate a range of language models on curated datasets.\nWhile the results are not terribly strong, GPT-2’s\nshowing in particular suggests that a great deal of\nprogress has been made recently.\nIt is nevertheless still premature to claim that\nthe probabilities assigned by language models to\nsequences of words can be reliably construed as a\nregression scale of grammaticality. Such a claim\nwould need to be supported by a stronger perfor-\nmance in more diverse settings that are larger than\nminimal-set or microgrouping structures, ideally\nwith better robustness to other factors such as type\ncase. The sensicality study suggests that language\nmodels are still overwhelmingly inﬂuenced by se-\nmantic factors. This is unsurprising: language\nmodels have been used for years as a proxy for\nsemantics in numerous other areas such as parsing.\nThe best grammaticality classiﬁers to date are\nstill classiﬁers that are constructed for the purpose\nof predicting grammaticality, not for the classical\npurpose of a language model, which is to predict\nthe next word of input. These either use a lan-\nguage model output probability as their own input\n(Warstadt et al., 2019) or use other artefacts of\nthe language model, such as word vectors, and dis-\ncard the language model probability altogether (Liu\net al., 2019).\n118\nAcknowledgments\nWe would like to thank Zoe McKenzie for her gram-\nmaticality judgements.\nReferences\nDavid Adger. 2003. Core Syntax: A Minimalist Ap-\nproach, volume 20.\nBNC Consortium. 2007. The British National Corpus,\nversion 3 (BNC XML Edition). Distributed by Ox-\nford University Computing Services on behalf of the\nBNC Consortium.\nNoam Chomsky. 1957. Syntactic structures. Mouton\npublishers.\nShammur Absar Chowdhury and Roberto Zamparelli.\n2018. RNN Simulations of Grammaticality Judg-\nments on Long-distance Dependencies. In Proceed-\nings of the 27th International Conference on Compu-\ntational Linguistics, pages 133–144, Santa Fe, New\nMexico, USA. Association for Computational Lin-\nguistics.\nRadoslaw M. Cichy and Daniel Kaiser. 2019. Deep\nNeural Networks as Scientiﬁc Models. Trends in\nCognitive Sciences, 23(4):305–317.\nAlexander Clark, Gianluca Giorgolo, and Shalom Lap-\npin. 2013. Statistical Representation of Grammati-\ncality Judgements: The Limits of N-Gram Models.\nIn Proceedings of the Fourth Annual Workshop on\nCognitive Modeling and Computational Linguistics\n(CMCL), pages 28–36, Soﬁa, Bulgaria. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. arXiv:1810.04805 [cs].\nJennifer Hu, Sherry Chen, and Roger Levy. 2020. A\nCloser Look at the Performance of Neural Language\nModels on Reﬂexive Anaphor Licensing. Proceed-\nings of the Society for Computation in Linguistics,\n3(1):382–392.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In Interna-\ntional Conference on Learning Representations.\nShalom Lappin and Jey Han Lau. 2018. Gradient Prob-\nabilistic Models vs Categorical Grammars: A Reply\nto Sprouse et al.(2018).\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2014. Measuring Gradience in Speakers’ Grammati-\ncality Judgements. Proceedings of the Annual Meet-\ning of the Cognitive Science Society, 36:6.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2015. Unsupervised Prediction of Acceptability\nJudgements. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1618–1628, Beijing, China. Associa-\ntion for Computational Linguistics.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2017. Grammaticality, acceptability, and probabil-\nity: A probabilistic view of linguistic knowledge.\nCognitive Science, 41(5):1202–1241.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019. Multi-Task Deep Neural Networks\nfor Natural Language Understanding. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4487–4496,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nRebecca Marvin and Tal Linzen. 2018. Targeted Syn-\ntactic Evaluation of Language Models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nB. W. Matthews. 1975. Comparison of the pre-\ndicted and observed secondary structure of T4 phage\nlysozyme. Biochimica et Biophysica Acta (BBA) -\nProtein Structure, 405(2):442–451.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. An Analysis of Neural Language\nModeling at Multiple Scales. arXiv:1803.08240\n[cs].\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer Sentinel Mixture\nModels. In 5th International Conference on Learn-\ning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings.\nOpenReview.net.\nTom´aˇs Mikolov. 2012. Statistical language models\nbased on neural networks. Brno University of Tech-\nnology dissertation.\nAdam Pauls and Dan Klein. 2012. Large-Scale Syntac-\ntic Language Modeling with Treelets. In Proceed-\nings of the 50th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 959–968, Jeju Island, Korea. Associa-\ntion for Computational Linguistics.\nFernando Pereira. 2000. Formal grammar and informa-\ntion theory: Together again? Philosophical Trans-\nactions of the Royal Society of London. Series A:\nMathematical, Physical and Engineering Sciences,\n358(1769):1239–1253.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners.\n119\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the Lim-\nits of Transfer Learning with a Uniﬁed Text-to-Text\nTransformer. arXiv e-prints.\nLawrence Saul and Fernando Pereira. 1997. Aggregate\nand mixed-order Markov models for statistical lan-\nguage processing. In Second Conference on Empiri-\ncal Methods in Natural Language Processing.\nJon Sprouse and Diogo Almeida. 2012. Assessing the\nreliability of textbook data in syntax: Adger’s Core\nSyntax1. Journal of Linguistics, 48(3):609–652.\nJon Sprouse, Sagar Indurkhya, Sandiway Fong, and\nRobert C. Berwick. 2015. Colorless green ideas\ndo sleep furiously: The necessity of grammar. The\n46th Annual Meeting of North East Linguistic Soci-\nety (NELS 46).\nJon Sprouse, Carson T. Sch ¨utze, and Diogo Almeida.\n2013. A comparison of informal and formal accept-\nability judgments using a random sample from Lin-\nguistic Inquiry 2001–2010. Lingua, 134:219–248.\nJon Sprouse, Beracah Yankama, Sagar Indurkhya,\nSandiway Fong, and Robert C. Berwick. 2018. Col-\norless green ideas do sleep furiously: Gradient ac-\nceptability and the nature of the grammar. The Lin-\nguistic Review, 35(3):575–599.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A Multi-Task Benchmark and Analysis Plat-\nform for Natural Language Understanding. In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural Network Acceptability Judg-\nments. arXiv:1805.12471 [cs].\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2020. HuggingFace’s Trans-\nformers: State-of-the-art Natural Language Process-\ning. arXiv:1910.03771 [cs].\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019. XLNet: Generalized Autoregressive Pretrain-\ning for Language Understanding. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d\\textquotesingle\nAlch´e-Buc, E. Fox, and R. Garnett, editors, Ad-\nvances in Neural Information Processing Systems\n32, pages 5753–5763. Curran Associates, Inc.",
  "topic": "Grammaticality",
  "concepts": [
    {
      "name": "Grammaticality",
      "score": 0.9946016669273376
    },
    {
      "name": "Variable (mathematics)",
      "score": 0.5778394937515259
    },
    {
      "name": "Computer science",
      "score": 0.5608307719230652
    },
    {
      "name": "Language model",
      "score": 0.5198055505752563
    },
    {
      "name": "Correlation",
      "score": 0.5099307298660278
    },
    {
      "name": "Natural language processing",
      "score": 0.5032815337181091
    },
    {
      "name": "Linguistics",
      "score": 0.4945380389690399
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46818482875823975
    },
    {
      "name": "Cola (plant)",
      "score": 0.45535141229629517
    },
    {
      "name": "Point (geometry)",
      "score": 0.45216643810272217
    },
    {
      "name": "Psychology",
      "score": 0.4367648959159851
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3936893343925476
    },
    {
      "name": "Grammar",
      "score": 0.2700241506099701
    },
    {
      "name": "Mathematics",
      "score": 0.25172045826911926
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    }
  ]
}