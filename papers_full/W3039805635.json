{
  "title": "Non-Autoregressive Machine Translation with Disentangled Context Transformer",
  "url": "https://openalex.org/W3039805635",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4202156197",
      "name": "Kasai, Jungo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222096230",
      "name": "Cross, James",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223995600",
      "name": "Ghazvininejad, Marjan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222615513",
      "name": "Gu, Jiatao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3034892578",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970119519",
    "https://openalex.org/W3100753857",
    "https://openalex.org/W2995999067",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2970832665",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W2920538220",
    "https://openalex.org/W2964089333",
    "https://openalex.org/W2963112338",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2996843693",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2976965654",
    "https://openalex.org/W3021914396",
    "https://openalex.org/W3004979489",
    "https://openalex.org/W2913250058",
    "https://openalex.org/W2990389671",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2964026424",
    "https://openalex.org/W3018406493",
    "https://openalex.org/W2892213699",
    "https://openalex.org/W2947898088",
    "https://openalex.org/W2963536265",
    "https://openalex.org/W2963946353",
    "https://openalex.org/W3034425996",
    "https://openalex.org/W2990372437",
    "https://openalex.org/W2325237720",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2948629866",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3034579764"
  ],
  "abstract": "State-of-the-art neural machine translation models generate a translation from left to right and every step is conditioned on the previously generated tokens. The sequential nature of this generation process causes fundamental latency in inference since we cannot generate multiple tokens in each sentence in parallel. We propose an attention-masking based model, called Disentangled Context (DisCo) transformer, that simultaneously generates all tokens given different contexts. The DisCo transformer is trained to predict every output token given an arbitrary subset of the other reference tokens. We also develop the parallel easy-first inference algorithm, which iteratively refines every token in parallel and reduces the number of required iterations. Our extensive experiments on 7 translation directions with varying data sizes demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in non-autoregressive machine translation while significantly reducing decoding time on average. Our code is available at https://github.com/facebookresearch/DisCo.",
  "full_text": "Non-autoregressive Machine Translation with Disentangled Context\nTransformer\nJungo Kasai 1 James Cross 2 Marjan Ghazvininejad 2 Jiatao Gu 2\nAbstract\nState-of-the-art neural machine translation mod-\nels generate a translation from left to right and\nevery step is conditioned on the previously gen-\nerated tokens. The sequential nature of this\ngeneration process causes fundamental latency\nin inference since we cannot generate multiple\ntokens in each sentence in parallel. We pro-\npose an attention-masking based model, called\nDisentangled Context (DisCo) transformer, that\nsimultaneously generates all tokens given differ-\nent contexts. The DisCo transformer is trained\nto predict every output token given an arbitrary\nsubset of the other reference tokens. We also de-\nvelop the parallel easy-ﬁrst inference algorithm,\nwhich iteratively reﬁnes every token in parallel\nand reduces the number of required iterations.\nOur extensive experiments on 7 translation di-\nrections with varying data sizes demonstrate that\nour model achieves competitive, if not better, per-\nformance compared to the state of the art in non-\nautoregressive machine translation while signif-\nicantly reducing decoding time on average. Our\ncode is available at https://github.com/\nfacebookresearch/DisCo.\n1. Introduction\nState-of-the-art neural machine translation systems use au-\ntoregressive decoding where words are predicted one-by-\none conditioned on all previous words (Bahdanau et al.,\n2015; Vaswani et al., 2017). Non-autoregressive machine\ntranslation (NAT, Gu et al., 2018), on the other hand, gen-\nerates all words in one shot and speeds up decoding at the\nexpense of performance drop. Parallel decoding results in\n1Paul G. Allen School of Computer Science & Engi-\nneering, University of Washington. Work done at Facebook\nAI. 2Facebook AI. Correspondence to: Jungo Kasai <jka-\nsai@cs.washington.edu>.\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).\nconditional independence and prevents the model from prop-\nerly capturing the highly multimodal distribution of target\ntranslations (Gu et al., 2018). One way to remedy this fun-\ndamental problem is to reﬁne model output iteratively (Lee\net al., 2018; Ghazvininejad et al., 2019). This work pursues\nthis iterative approach to non-autoregressive translation.1\nIn this work, we propose a transformer-based architecture\nwith attention masking, which we callDisentangled Context\n(DisCo) transformer, and use it for non-autoregressive de-\ncoding. Speciﬁcally, our DisCo transformer predicts every\nword in a sentence conditioned on an arbitrary subset of\nthe rest of the words. Unlike the masked language models\n(Devlin et al., 2019; Ghazvininejad et al., 2019) where the\nmodel only predicts the masked words, the DisCo trans-\nformer can predict all words simultaneously, leading to\nfaster inference as well as a substantial performance gain\nwhen training data are relatively large.\nWe also introduce a new inference algorithm for iterative\nparallel decoding, parallel easy-ﬁrst, where each word is\npredicted by attending to the words that the model is more\nconﬁdent about. This decoding algorithm allows for predict-\ning all tokens with different contexts in each iteration and\nterminates when the output prediction converges, contrast-\ning with the constant number of iterations (Ghazvininejad\net al., 2019). Indeed, we will show in a later section that\nthis method substantially reduces the number of required\niterations without loss in performance.\nOur extensive empirical evaluations on 7 translation direc-\ntions from standard WMT benchmarks show that our ap-\nproach achieves competitive performance to state-of-the-art\nnon-autoregressive and autoregressive machine translation\nwhile signiﬁcantly reducing decoding time on average.\n2. DisCo Transformer\nWe introduce our DisCo transformer for non-autoregressive\ntranslation (Fig. 1). We propose a DisCo objective as an ef-\nﬁcient alternative to masked language modeling and design\nan architecture that computes the objective in a single pass.\n1Reﬁnement requires several sequential steps, but we abuse\nthe term non-autoregressive generation to mean a broad family of\nmethods that generate the target in parallel for simplicity.\narXiv:2001.05136v2  [cs.CL]  30 Jun 2020\nNon-autoregressive Machine Translation with Disentangled Context Transformer\nQ\nAttn \nK, V K, V Q\nAttn \nK, V Q\nAttn \nQK, V K, V Q K, V Q\n   p 1 \n   w 1 \n   p 1 \n   w 2 \n   p 2 \n   p 2 \n   p 3 \n   w 3 \n   p 3 \nK, V\nAttention Mask\nA B C\nA B C\nA B C\nP ( A | C )\nP( B  | A,C )\nP ( C  | B )\nFigure 1.DisCo Transformer. W and p denote word and positional\nembeddings respectively. We simulate three disentangled contexts\nto predict A, B, and C ( Yn) given {C}, {A, C}, and {B} (Y n\nobs)\nrespectively. Dashed lines indicate masked-out attention connec-\ntions to Y n\nmask and Yn itself. K and V are direct projections of\nwn + pn (thus contextless) for all layers to avoid leakage.\n2.1. DisCo Objective\nSimilar to masked language models (Devlin et al., 2019), a\nconditional masked language model (CMLM, Ghazvinine-\njad et al., 2019) predicts randomly masked target tokens\nYmask given a source text X and the rest of the target tokens\nYobs. Namely, for every sentence pair in bitext X and Y,\nP(Ymask|X,Yobs) =Transformer(X,Yobs)\nYmask ∼RS(Y) Yobs = Y \\Ymask\nwhere RS denotes random sampling of masked tokens. 2\nCMLMs have proven successful in parallel decoding for ma-\nchine translation (Ghazvininejad et al., 2019), video caption-\ning (Yang et al., 2019a), and speech recognition (Nakayama\net al., 2019). However, the fundamental inefﬁciency with\nthis masked language modeling objective is that the model\ncan only be trained to predict a subset of the reference tokens\n(Ymask) for each network pass unlike a normal autoregressive\nmodel where we predict all Y from left to right. To address\nthis limitation, we propose a Disentangled Context (DisCo)\nobjective.3 The objective involves prediction of every token\ngiven an arbitrary (thus disentangled) subset of the other\ntokens. For every 1 ≤n≤N where |Y|= N, we predict:\nP(Yn|X,Y n\nobs) =Transformer(X,Y n\nobs)\nYn\nobs ∼RS(Y \\Yn)\n2BERT (Devlin et al., 2019) masks a token with probability\n0.15 while CMLMs (Ghazvininejad et al., 2019) sample the num-\nber of masked tokens uniformly from [1, N].\n3We distinguish this from disentangled representation.\n2.2. DisCo Transformer Architecture\nSimply computing conditional probabilities P(Yn|X,Y n\nobs)\nwith a vanilla transformer decoder will necessitate N sep-\narate transformer passes for each Yn\nobs. We introduce the\nDisCo transformer to compute these N contexts in one shot:\nP(Y1|X,Y 1\nobs),··· ,P(YN |X,Y N\nobs) =DisCo(X,Y )\nIn particular, our DisCo transformer makes crucial use of\nattention masking to achieve this computational efﬁciency.\nDenote input word and positional embeddings at position\nn by wn and pn. For each position n in Y, the vanilla\ntransformer computes self-attention:4\nkn,vn,qn = Proj(wn + pn)\nhn = Attention(K,V,q n)\nK,V = Concat\n(\n{km}N\nm=1\n)\n,Concat\n(\n{vm}N\nm=1\n)\nWe modify this attention computation in two aspects. First,\nwe separate query input from key and value input to avoid\nfeeding the token we predict. Then we only attend to keys\nand values that correspond to observed tokens (Kn\nobs, Vn\nobs)\nand mask out the connection to the other tokens (Yn\nmask and\nYn itself, dashed lines in Fig. 1).\nkn,vn = Proj(wn + pn) qn = Proj(pn)\nhn = Attention(Kn\nobs,V n\nobs,qn)\nKn\nobs = Concat ({km|Ym ∈Yn\nobs})\nVn\nobs = Concat ({vm|Ym ∈Yn\nobs})\n2.3. Stacked DisCo Transformer\nUnfortunately stacking DisCo transformer layers is not\nstraightforward. Suppose that we compute the nth position\nin the jth layer from the prevous layer’s output as follows:\nkj\nn,vj\nn = Proj(wn + hj−1\nn ) qj\nn = Proj(hj−1\nn )\nhj\nn = Attention(Kn,j\nobs ,V n,j\nobs ,qj\nn)\nIn this case, however, any cyclic relation between positions\nwill cause information leakage. Concretely, assume that\nY = [A,B] and N = 2. Suppose also that Y1\nobs = Band\nY2\nobs = A, and thus there is a cycle that position 1 can seeB\nand position 2 can see A. Then the output state at position 1\nin the ﬁrst layer h1\n1 becomes a function of B:\nh1\n1(B) =Attention(k1\n2(B),v1\n2(B),q1\n1)\nSince position 2 can see position 1, the output state at posi-\ntion 2 in the second layer h2\n2 is computed by\nh2\n2 = Attention\n(\nk2\n1(h1\n1(B)),v2\n1(h1\n1(B)),q2\n2\n)\n4For simplicity, here we omit fully-connected layers, layer-\nnorm, residual connections, and cross attention to the encoder.\nNon-autoregressive Machine Translation with Disentangled Context Transformer\nBut h2\n2 will be used to predict the token at position 2 i.e., B,\nand this will clearly make the prediction problem degener-\nate. To avoid this cyclic leakage, we make keys and values\nindependent of the previous layer’s outputhj−1\nn :\nkj\nn,vj\nn = Proj(wn + pn) qj\nn = Proj(hj−1\nn )\nhj\nn = Attention(Kn,j\nobs ,V n,j\nobs ,qj\nn)\nIn other words, we decontextualize keys and values in\nstacked DisCo layers.\n2.4. Training Loss\nWe use a standard transformer as an encoder and stacked\nDisCo layers as a decoder. For each Yn in Y where\n|Y|= N, we uniformly sample the number of visible to-\nkens from [0,N −1], and then we randomly choose that\nnumber of tokens from Y \\Yn as Yn\nobs, similarly to CMLMs\n(Ghazvininejad et al., 2019). We optimize the negative log\nlikelihood loss from P(Yn|X,Y n\nobs) (1≤n≤N). Again\nfollowing CMLMs, we append a special token to the encoder\nand project the vector to predict the target length for parallel\ndecoding. We add the negative log likelihood loss from this\nlength prediction to the loss from word predictions.\n2.5. DisCo Objective as Generalization\nWe designed the DisCo transformer to compute conditional\nprobabilities at every position efﬁciently, but here we note\nthat the DisCo transformer can be readily used with other\ntraining schemes in the literature. We can train an autore-\ngressive DisCo transformer by always setting Yn\nobs = Y<n.\nXLNet (Yang et al., 2019b) is also a related variant of a\ntransformer that was introduced to produce general-purpose\ncontextual word representations. The DisCo transformer\ndiffers from XLNet in two critical ways. First, XLNet\nconsists of separate context stream and query stream atten-\ntion. This means that we need to double the amount of\nexpensive attention and fully connected layer computation\nin the transformer. Another difference is that XLNet is only\ntrained to predict tokens in permuted order. The DisCo trans-\nformer can be trained for the permutation objective by set-\nting Yn\nobs = {Yi|z(i) <z(n)}where z(i) indicates the rank\nof the ith element in the new order. Baevski et al. (2019)\ntrain their two tower model with the cloze objective again\nfor general-purpose pretraining. We can train our DisCo\ntransformer with this objective by setting Yn\nobs = {Y̸=n}.\nThe proposed DisCo objective provides generalization that\nencompasses all of these special cases.\n3. Inference Algorithms\nIn this section, we discuss inference algorithms for our\nDisCo transformer. We ﬁrst review mask-predict from prior\nwork as a baseline and introduce a new parallelizable infer-\nence algorithm, parallel easy-ﬁrst (Alg. 1).\n3.1. Mask-Predict\nMask-predict is an iterative inference algorithm introduced\nin Ghazvininejad et al. (2019) to decode a conditional\nmasked language model (CMLM). The target length N is\nﬁrst predicted, and then the algorithm iterates over two steps:\nmask where it tokens with lowest probability are masked\nand predict where those masked tokens are updated given\nthe other N −it tokens. The number of masked tokens it\ndecays from N with a constant rate over a ﬁxed number of\niterations T. Speciﬁcally, at iteration t,\nit =\n⌊\nN ·T −t+ 1\nT\n⌋\nYt\nobs = {Yt−1\nj |j ∈topk\nn\n(pt−1\nn ,k = N −it)}\nYt\nn,pt\nn =\n{\n(arg)maxw P(Yn = w|X,Y t\nobs) if Yt\nn ̸∈Yt\nobs\nYt−1\nn ,pt−1\nn otherwise\nThis method is directly applicable to our DisCo transformer\nby ﬁxing Yn,t\nobs regardless of the position n.\n3.2. Parallel Easy-First\nAn advantage of the DisCo transformer over a CMLM is\nthat we can predict tokens in all positions conditioned on dif-\nferent context simultaneously. The mask-predict inference\ncan only update masked tokens given the ﬁxed observed\ntokens Yt\nobs, meaning that we are wasting the opportunity\nto improve upon Yt\nobs and to take advantage of broader con-\ntext present in Yt\nmask. We develop an algorithm, parallel\neasy-ﬁrst (Alg. 1), which makes predictions in all positions,\nthereby beneﬁting from this property. Concretely, in the ﬁrst\niteration, we predict all tokens in parallel given source text:\nY1\nn ,pn = (arg)max\nw\nP(Yn = w|X)\nThen, we get the easy-ﬁrst order zwhere z(i) denotes the\nrank of pi in descending order. At iterationt> 1, we update\npredictions for all positions by\nYn,t\nobs =\n{\nYt−1\ni |z(i) <z(n)\n}\nYt\nn,pt\nn = (arg)max\nw\nP\n(\nYn = w|X,Y n,t\nobs\n)\nNamely, we update each position given previous predictions\non the easier positions. In a later section, we will explore\nseveral variants of choosing Yn,t\nobs and show that this easy-\nﬁrst strategy performs best despite its simplicity.\n3.3. Length Beam\nFollowing Ghazvininejad et al. (2019), we apply length\nbeam. In particular, we predict top K lengths from the distri-\nNon-autoregressive Machine Translation with Disentangled Context Transformer\nAlgorithm 1 Parallel Easy-First with Length Beam\nSource sentence: X\nPredicted lengths: N1,··· ,NK\nMax number of iterations: T\nfor k∈{1,2,...,K }do\nfor n∈{1,2,...,N k}do\nY1,k\nn ,pk\nn = (arg)maxw P(yn = w|X)\nend for\nGet the easy-ﬁrst order zk by sorting pk and let zk(i)\nbe the rank of the ith position.\nend for\nfor t∈{2,...,T }do\nfor k∈{1,...,K }do\nfor n∈{1,2,...,N k}do\nYn,t\nobs = {Yt−1,k\ni |zk(i) <zk(n)}\nYt,k\nn ,pt,k\nn = (arg)max\nw\nP\n(\nYn = w|X,Y n,t\nobs\n)\nend for\nend for\nk∗= argmax\nk\nNk∑\nn=1\nlog\n(\npt,k\nn\n)\n/Nk\nif Yt−1,k∗\n= Yt,k∗\nthen return Yt,k∗\nend for\nreturn YT,k∗\nbution in length prediction and run parallel easy-ﬁrst simul-\ntaneously. In order to speed up decoding, we terminate if the\none with the highest average log score ∑N\nn=1 log(pt\nn)/N\nconverges. It should be noted that for parallel easy-ﬁrst,\nYt = Yt−1 means convergence because Yn,t\nobs = Yn,t+1\nobs\nfor all positions nwhile mask-predict may keep updating\ntokens even after because Yt\nobs changes over iterations. See\nAlg. 1 for full pseudo-code. Notice that all for-loops are par-\nallelizable except the one over iterationst. In the subsequent\nexperiments, we use length beam size of 5 (Ghazvinine-\njad et al., 2019) unless otherwise noted. In Sec. 5.2, we\nwill illustrate that length beam facilitates decoding both the\nCMLM and DisCo transformer.\n4. Experiments\nWe conduct extensive experiments on standard machine\ntranslation benchmarks. We demonstrate that our DisCo\ntransformer with the parallel easy-ﬁrst inference achieves\ncomparable performance to, if not better than, prior work\non non-autoregressive machine translation with substan-\ntial reduction in the number of sequential steps of trans-\nformer computation. We also ﬁnd that our DisCo trans-\nformer achieves more pronounced improvement when bitext\ntraining data are large, getting close to the performance of\nautoregressive models.\n4.1. Experimental Setup\nBenchmark datasets We evaluate on 7 directions from\nfour standard datasets with various training data sizes:\nWMT14 EN-DE (4.5M pairs), WMT16 EN-RO (610K\npairs), WMT17 EN-ZH (20M pairs), and WMT14 EN-FR\n(36M pairs, en→fr only). These datasets are all encoded\ninto subword units by BPE (Sennrich et al., 2016). 5 We\nuse the same preprocessed data and train/dev/test splits as\nprior work for fair comparisons (EN-DE: Vaswani et al.,\n2017; EN-RO: Lee et al., 2018; EN-ZH: Hassan et al., 2018;\nWu et al., 2019; EN-FR: Gehring et al., 2017; Ott et al.,\n2018). We evaluate performance with BLEU scores (Pa-\npineni et al., 2002) for all directions except that we use\nSacreBLEU (Post, 2018)6 in en→zh again for fair compar-\nison with prior work (Ghazvininejad et al., 2019). For all\nautoregressive models, we apply beam search with b = 5\n(Vaswani et al., 2017; Ott et al., 2018) and tune length\npenalty of α∈[0.0,0.2,··· ,2.0] in validation. For parallel\neasy-ﬁrst, we set the max number of iterations T = 10and\nuse T = 4,10 for constant-time mask-predict.\n4.2. Baselines and Comparison\nThere has been a ﬂurry of recent work on non-autoregressive\nmachine translation (NAT) that ﬁnds a balance between par-\nallelism and performance. Performance can be measured\nusing automatic evaluation such as BLEU scores (Papineni\net al., 2002). Latency is, however, challenging to compare\nacross different methods. For models that have an autore-\ngressive component (e.g., Kaiser et al., 2018; Ran et al.,\n2019), we can speed up sequential computation by caching\nstates. Further, many of prior NAT approaches generate\nvarying numbers of translation candidates and rescore them\nusing an autoregressive model. The rescoring process typ-\nically costs overhead of one parallel pass of a transformer\nencoder followed by a decoder. Given this complexity\nin latency comparison, we highlight two state-of-the-art\niteration-based NAT models whose latency is comparable\nto our DisCo transformer due to the similar model structure.\nSee Sec. 6 for descriptions of more work on NAT.\nCMLM As discussed earlier, we can generate a transla-\ntion with mask-predict from a CMLM (Ghazvininejad et al.,\n2019). We can directly compare our DisCo transformer\nwith this method by the number of iterations required.7 We\n5We run joint BPE on all language pairs except EN-ZH.\n6SacreBLEU hash: BLEU+case.mixed+lang.en-\nzh+numrefs.1+smooth.exp+test.wmt17+tok.zh+version.1.3.7.\n7Caching contextless key and value computation in the DisCo\ntransformer gives us a slight speedup, but it is relatively minor as\ncompared to expensive attention and fully connected computation.\nNon-autoregressive Machine Translation with Disentangled Context Transformer\nTable 1.The performance of non-autoregressive machine translation methods on the WMT14 EN-DE and WMT16 EN-RO test data. The\nStep columns indicate the average number of sequential transformer passes. Shaded results use a small transformer (dmodel = dhidden =\n512). Our EN-DE results show the scores after conventional compound splitting (Luong et al., 2015; Vaswani et al., 2017).\nModel en →de de →en en →ro ro →en\nn: # rescored candidates Step BLEU Step BLEU Step BLEU Step BLEU\nGu et al. (2018) (n= 100) 1 19.17 1 23.20 1 29.79 1 31.44\nWang et al. (2019) (n= 9) 1 24.61 1 28.90 – – – –\nLi et al. (2019) (n= 9) 1 25.20 1 28.80 – – – –\nMa et al. (2019) (n= 30) 1 25.31 1 30.68 1 32.35 1 32.91\nSun et al. (2019) (n= 19) 1 26.80 1 30.04 – – – –\nRan et al. (2019) 1 26.51 1 31.13 1 31.70 1 31.99\nShu et al. (2020) (n= 50) 1 25.1 – – – – – –\nIterative NAT Models\nLee et al. (2018) 10 21.61 10 25.48 10 29.32 10 30.19\nGhazvininejad et al. (2019) (CMLM) 4 25.94 4 29.90 4 32.53 4 33.23\n10 27.03 10 30.53 10 33.08 10 33.31\nGu et al. (2019b) (LevT) 7+ 27.27 – – – – 7+ 33.26\nOur Implementations\nCMLM + Mask-Predict 4 26.73 4 30.75 4 33.02 4 33.27\nCMLM + Mask-Predict 10 27.39 10 31.24 10 33.33 10 33.67\nDisCo + Mask-Predict 4 25.83 4 30.15 4 32.22 4 32.92\nDisCo + Mask-Predict 10 27.06 10 30.89 10 32.92 10 33.12\nDisCo + Easy-First 4.82 27.34 4.23 31.31 3.29 33.22 3.10 33.25\nAT Models\nVaswani et al. (2017) (base) N 27.3 – – – – – –\nVaswani et al. (2017) (large) N 28.4 – – – – – –\nOur Implementations\nAT Transformer Base (EN-RO teacher) N 27.38 N 31.78 N 34.16 N 34.46\nAT Transformer Base + Distillation N 28.24 N 31.54 – – – –\nAT Transformer Large (EN-DE teacher) N 28.60 N 31.71 – – – –\nprovide results obtained by running their code.8\nLevenshtein Transformer Levenshtein transformer\n(LevT) is a transformer-based iterative model for parallel\nsequence generation (Gu et al., 2019b). Its iteration\nconsists of three sequential steps: deletion, placeholder\nprediction, and token prediction. Unlike the CMLM with\nthe constant-time mask-predict inference, decoding in\nLevT terminates adaptively under certain condition. Its\nlatency is roughly comparable by the average number of\nsequential transformer runs. Each iteration consists of three\ntransformer runs except that the ﬁrst iteration skips the\ndeletion step. See Gu et al. (2019b) for detail.\nHyperparameters We generally follow the hyperparame-\nters for a transformer base (Vaswani et al., 2017; Ghazvinine-\njad et al., 2019): 6 layers for both the encoder and decoder,\n8 attention heads, 512 model dimensions, and 2048 hidden\ndimensions. We sample weights from N(0,0.02), initial-\n8https://github.com/facebookresearch/\nMask-Predict\nize biases to zero, and set layer normalization parameters\nto β = 0,γ = 1(Devlin et al., 2019). For regularization,\nwe tune the dropout rate from [0.1,0.2,0.3] based on dev\nperformance in each direction, and apply weight decay with\n0.01 and label smoothing with ε = 0.1. We train batches\nof approximately 128K tokens using Adam (Kingma & Ba,\n2015) with β = (0.9,0.999) and ε = 10−6. The learning\nrate warms up to 5 ·10−4 in the ﬁrst 10K steps, and then\ndecays with the inverse square-root schedule. We train all\nmodels for 300K steps apart from en →fr where we make\n500K steps to account for the data size. We measure the dev\nBLEU score at the end of each epoch to avoid stochasticity,\nand average the 5 best checkpoints to obtain the ﬁnal model.\nWe use 16 Telsa V100 GPUs and accelerate training by\nmixed precision ﬂoating point (Micikevicius et al., 2018),\nand implement all models with fairseq (Ott et al., 2019).\nDistillation Similar to previous work on non-\nautoregressive translation (e.g., Gu et al., 2018; Lee\net al., 2018), we apply sequence-level knowledge dis-\ntillation (Kim & Rush, 2016) by training every model\nNon-autoregressive Machine Translation with Disentangled Context Transformer\nin all directions on translations produced by a standard\nleft-to-right transformer model (transformer large for\nEN-DE, EN-ZH, and EN-FR and base for EN-RO). We\nalso present results obtained from training a standard\nautoregressive base transformer on the same distillation\ndata for comparison. We assess the impact of distillation\nin Sec. 5.1 and demonstrate that distillation is still a key\ncomponent in our non-autoregressive models.\n4.3. Results and Discussion\nSeen in Table 1 are the results in the four directions from the\nWMT14 EN-DE and WMT16 EN-RO datasets. First, our\nre-implementations of CMLM + Mask-Predict outperform\nGhazvininejad et al. (2019) (e.g., 31.24 vs. 30.53 in de→en\nwith 10 steps). This is probably due to our tuning on the\ndropout rate and weight averaging of the 5 best epochs based\non the validation BLEU performance (Sec. 4.1).\nOur DisCo transformer with the parallel easy-ﬁrst inference\nachieves at least comparable performance to the CMLM\nwith 10 steps despite the signiﬁcantly fewer steps on aver-\nage (e.g., 4.82 steps in en→de). The one exception is ro→en\n(33.25 vs. 33.67), but DisCo + Easy-First requires only 3.10\nsteps, and CMLM + Mask-Predict with 4 steps achieves\nsimilar performance of 33.27. The limited advantage of our\nDisCo transformer on the EN-RO dataset suggests that we\nbeneﬁt less from the training efﬁciency of the DisCo trans-\nformer on the small dataset (610K sentence pairs). DisCo +\nMask-Predict generally underperforms DisCo + Easy-First,\nimplying that the mask-predict inference, which ﬁxes Yn\nobs\nacross all positions n, fails to utilize the ﬂexibility of the\nDisCo transformer. DisCo + Easy-First also accomplishes\nsigniﬁcant reduction in the average number of steps as com-\npared to the adaptive decoding in LevT (Gu et al., 2019b)\nwhile performing competitively. As discussed earlier, each\niteration in inference on LevT involves three sequential\ntransformer runs, which undermine the latency improve-\nment.\nOverall, our implementations compare well with other NAT\nmodels from prior work. We achieve competitive perfor-\nmance to the standard autoregressive models with the same\ntransformer base conﬁguration on the EN-DE dataset except\nthat the autoregressive model with distillation performs com-\nparably to the transformer large teacher in en→de (28.24 vs.\n28.60). Nonetheless, we still see a large gap between the au-\ntoregressive teachers and our NAT results in both directions\nfrom EN-RO, illustrating a limitation of our remedy for the\ntrade-off between decoding parallelism and performance.\nTables 2 and 3 show results from the EN-ZH and EN-FR\ndatasets where the bitext data are larger (20M and 36M\nsentence pairs). In both cases we see similar (yet more pro-\nnounced) patterns to the EN-DE and EN-RO experiments.\nParticularly noteworthy is that DisCo with the parallel easy-\nﬁrst inference and dropout tuning yields 34.63 points, a gain\nof 1.4 BLEU improvement over Ghazvininejad et al. (2019)\nin en→zh despite the average of 5.44 steps.\nTable 2.WMT17 EN-ZH test results.\nModel en →zh zh →en\nStep BLEU Step BLEU\nGhazvininejad et al. 4 32.63 4 21.90\n(2019) 10 33.19 10 23.21\nOur Implementations\nCMLM + Mask-Predict 4 33.58 4 22.57\nCMLM + Mask-Predict 10 34.24 10 23.76\nDisCo + Mask-Predict 4 33.61 4 22.42\nDisCo + Mask-Predict 10 34.51 10 23.68\nDisCo + Easy-First 5.44 34.63 5.90 23.83\nAT Transformer Base N 34.74 N 23.77\n+ Distillation N 35.09 N 24.53\nAT Trans. Large (teacher) N 35.01 N 24.65\nTable 3.WMT14 EN-FR test results.\nModel en →fr Train\nStep BLEU Time\nCMLM + Mask-Predict 4 40.21 53 hCMLM + Mask-Predict 10 40.55\nDisCo + Mask-Predict 4 39.59\n37 h DisCo + Mask-Predict 10 40.27\nDisCo + Easy-First 4.29 40.66\nVaswani et al. (2017) (base) N 38.1 –\nVaswani et al. (2017) (large) N 41.8 –\nOtt et al. (2018) (teacher) N 43.2 –\nAT Transformer Base N 41.27 28 h\n+ Distillation N 42.03 28 h\n4.4. Decoding Speed\nWe saw the the DisCo transformer with the parallel easy-ﬁrst\ninference achieves competitive performance to the CMLM\nwhile reducing the number of iterations. Here we compare\nthem in terms of the wall-time speedup with respect to the\nstandard autoregressive model of the same base conﬁgura-\ntion (Fig. 2). For each decoding run, we feed one sentence\nat a time and measure the wall time from when the model\nis loaded until the last sentence is translated, following the\nsetting in Gu et al. (2019b). All models are implemented\nin fairseq (Ott et al., 2019) and run on a single Nvidia\nV100 GPU. We can conﬁrm that the average number of\niterations directly translates to decoding time; the average\nnumber of iterations of the DisCo transformer with T = 10\nwas 5.44 and the measured speedup lies between T = 5,6\nof the CMLM. Note that fairseq implements effcient\nNon-autoregressive Machine Translation with Disentangled Context Transformer\n33 34 35\n1x\n2x\n3x\n4x\n5x\nb = 1\nb = 1\nT = 5\nT = 10\nb = 5b = 5\nT = 5\nT = 10\nEN→ZH BLEU\nRelative Speedup wrt AT\nCMLM\nDisCo+Easy-First\nAT\nDistilled AT\nFigure 2.Relative decoding speedup on the en→zh test data with\nrespect to the standard autoregressive model (indicated as ▲). T\nand b denote the (max) number of iterations and beam size respec-\ntively. The length beam size is all set to 5.\ndecoding of autoregressive models by caching hidden states.\nThe average length of generated sentences in the autoregres-\nsive model was 25.16 (4.6x steps compared to 5.44 steps),\nbut we only gained a threefold speedup from DisCo.\n5. Analysis and Ablations\nIn this section, we give an extensive analysis on our appo-\nrach along training and inference dimensions.\n5.1. Training\nTraining Efﬁciency In Sec. 2.1, we discussed the funda-\nmental inefﬁciency of CMLM training—a CMLM model is\ntrained to only predict a subset of the target words. DisCo\naddresses this problem by its architecture that allows for\npredicting every word given a randomly chosen subset of\nthe target words. Seen in Fig. 3 are results on the en →de\ntest data with varying batch sizes. We can see that DisCo\nis more robust to smaller batch sizes, supporting our claim\nthat it provides more efﬁcient training.\nDistillation We assess the effects of knowledge distilla-\ntion across different models and inference conﬁgurations\n(Table 4). Consistent with previous models (Gu et al., 2018;\nZhou et al., 2020), we ﬁnd that distillation facilitates all\nof the non-autoregressive models. Moreover, the DisCo\ntransformer beneﬁts more from distillation compared to the\nCMLM under the same mask-predict inference. This is in\nline with Zhou et al. (2020) who showed that there is cor-\nrelation between the model capacity and distillation data\ncomplexity. The DisCo transformer uses contextless keys\nand values, resulting in reduced capacity. Autoregressive\ntranslation also improves with distillation from a large trans-\n214 215 216 217\n25\n26\n27\nb = 1\nb = 5\nT = 10\nT = 3\nT = 10\nT = 3\n# Tokens per Batch\nEN→DE BLEU\nCMLM T=4\nCMLM T=10\nDisCo+Easy-First\nFigure 3.EN→DE test results with varying batch size.\nformer, but the difference is relatively small. Finally, we can\nobserve that the gain from distillation decreases as we incor-\nporate more global information in inference (more iterations\nin NAT cases and larger beam size in AT cases).\nTable 4.Effects of distillation across different models and infer-\nence. All results are BLEU scores from the dev data. T and b\ndenote the max number of iterations and beam size respectively.\nen→de ro →en\nModel T raw dist. ∆ raw dist. ∆\nCMLM + MaskP 4 22.7 25.5 2.8 33.2 34.8 1.6\nCMLM + MaskP 10 24.5 25.9 1.4 34.5 34.9 0.4\nDisCo + MaskP 4 21.4 24.6 3.2 32.3 34.1 1.8\nDisCo + MaskP 10 23.6 25.3 1.7 33.4 34.3 0.9\nDisCo + EasyF 10 23.9 25.6 1.7 34.0 35.0 1.0\nAT Base (b = 1) N 25.5 26.4 0.9 – – –\nAT Base (b = 5) N 26.1 26.8 0.7 – – –\nAT with Contextless KVs We saw that a decoder with\ncontextless keys and values can still retain performance in\nnon-autoregressive models. Here we use a decoder with\ncontextless keys and values in autoregressive models. The\nresults (Table 5) show that it is able to retain performance\neven in autoregressive models regardless of distillation, sug-\ngesting further potential of our approach.\nTable 5.Test results (BLEU) from AT with contextless keys and\nvalues.\nAT en→de de →en ro →en\nDecoder raw dist. raw dist. raw\nContextless 27.09 27.86 30.91 31.46 34.25\nOriginal 26.85 27.69 31.33 31.09 34.46\nNon-autoregressive Machine Translation with Disentangled Context Transformer\nEasy-First Training So far we have trained our models to\npredict every word given a random subset of the other words.\nBut this training scheme yields a gap between training and\ninference, which might harm the model. We attempt to\nmake training closer to inference by training the DisCo\ntransformer in the easy-ﬁrst order. Similarly to the inference,\nwe ﬁrst predict the easy-ﬁrst order by estimating P(Yn|X)\nfor all n. Then, use that order to determine Yn\nobs.9 The\noverall loss will be the sum of the negative loglikelihood of\nthese two steps. Seen in Table 6 are the results on the dev\nsets of en→de and ro→en. In both directions, this easy-ﬁrst\ntraining does not ameliorate performance, suggesting that\nrandomness helps the model. Notice also that the average\nnumber of iterations in inference decreases (4.03 vs. 4.29,\n2.94 vs. 3.17). The model gets trapped in a sub-optimal\nsolution with reduced iterations due to lack of exploration.\nTable 6.Dev results from bringing training closer to inference.\nen→de ro →en\nTraining Variant Step BLEU Step BLEU\nRandom Sampling 4.29 25.60 3.17 34.97\nEasy-First Training 4.03 24.76 2.94 34.96\n5.2. Inference\nAlternative Inference Algorithms Here we compare var-\nious decoding strategies on the DisCo transformer (Table 7).\nRecall in the parallel easy-ﬁrst inference (Sec. 3.2), we ﬁnd\nthe easy-ﬁrst order by sorting the probabilities in the ﬁrst it-\neration and compute each position’s probability conditioned\non the easier positions from the previous iteration. We eval-\nuate two alternative orderings: left-to-right and right-to-left.\nWe see that both of them yield much degraded performance.\nWe also attempt to use even broader context than paral-\nlel easy-ﬁrst by computing the probability at each position\nbased on all other positions ( all-but-itself, Yn,t\nobs = Yt−1\n̸=n ).\nWe again see degraded performance, suggesting that cyclic\ndependency (e.g., Yt−1\nm ∈Yn,t\nobs and Yt−1\nn ∈Ym,t\nobs ) breaks\nconsistency. For example, a model can have two output\ncandidates: “Hong Kong” and “New York” (Zhang et al.,\n9This training process can be seen as the hard EM algorithm\nwhere the easy-ﬁrst order is a latent variable.\nTable 7.Dev results with different decoding strategies.\nen→de ro →en\nInference Strategy Step BLEU Step BLEU\nLeft-to-Right Order 6.80 21.25 4.86 33.87\nRight-to-Left Order 6.79 20.75 4.67 34.38\nAll-But-Itself 6.90 20.72 4.80 33.35\nParallel Easy-First 4.29 25.60 3.17 34.97\nMask-Predict 10 25.34 10 34.54\n2020). In this case, we might end up producing “Hong York”\ndue to this cyclic dependency. These results suggest that the\neasy-ﬁrst ordering we introduced is a simple yet effective\napproach.\nExample Translation Seen in Fig. 4 is a translation exam-\nple in de→en when decoding the same DisCo transformer\nwith the mask-predict or parallel easy-ﬁrst inference. In\nboth algorithms, iterative reﬁnement resolves structural in-\nconsistency, such as repetition. Parallel easy-ﬁrst succeeds\nin incorporating more context in early stages whereas mask-\npredict continues to produce inconsistent predictions (“my\nmy activities”) until more context is available later, resulting\nin one additional iteration to land on a consistent output.\nLength Beam Fig. 5 shows performance of the CMLM\nand DisCo transformer with varying size of length beam. All\ncases beneﬁt from multiple candidates with different lengths\nto a certain point, but DisCo + Easy-First improves most.\nThis can be because parallel easy-ﬁrst relies on the easy-\nﬁrst order as well as the length, and length beam provides\nopportunity to try multiple orderings.\nIterations vs. Length We saw that parallel easy-ﬁrst in-\nference substantially reduced the number of required itera-\ntions. We hypothesize that the algorithm effectively adapts\nthe number of iterations based on the difﬁculty, which is\nreminiscent of a dynamic halting mechanism (Graves, 2016;\nDehghani et al., 2019). To see this, we compare the number\nof required iterations and the generated sentence length as\na proxy for difﬁculty (Fig. 6). Similarly to the experiments\nabove, we set the max iteration and length beam to be 10\nand 5 respectively. While the number of required iterations\nvary to a certain degree, we see that long sentences tend to\nrequire more iterations to converge.\n6. Related and Future Work\nIn addition to the work discussed above, prior and concur-\nrent work on non-autoregressive translation developed ways\nto mitigate the trade-off between decoding parallelism and\nperformance. As in this work, several prior and concurrent\nwork proposed methods to iteratively reﬁne (or insert) out-\nput predictions (Mansimov et al., 2019; Stern et al., 2019;\nGu et al., 2019a; Chan et al., 2019a;b; Ghazvininejad et al.,\n2020b; Li et al., 2020; Saharia et al., 2020). Other ap-\nproaches include adding a lite autoregressive module to\nparallel decoding (Kaiser et al., 2018; Sun et al., 2019;\nRan et al., 2019), partially decoding autoregressively (Stern\net al., 2018; 2019), rescoring output candidates autoregres-\nsively (e.g., Gu et al., 2018), mimicking hidden states of\nan autoregressive teacher (Li et al., 2019), training with\ndifferent objectives than vanilla negative log likelihood (Li-\nbovick´y & Helcl, 2018; Wang et al., 2019; Shao et al., 2020;\nNon-autoregressive Machine Translation with Disentangled Context Transformer\nX: Als Pr¨ asident w¨ urde ich meine Unternehmungen jedoch ganz einstellen.\nY: And if I became president, then I would end my business activity.\nt Mask-Predict t Parallel Easy-First\n1 As Pres. as Pres. , I would stop stop my doing altogether . 1 As Pres. as Pres. , I would stop stop my doing altogether .\n2 However , as Pres. , I would stopmy my doing altogether . 2 As , , Pres. , I would stop doing my activities altogether .\n3 However , as Pres. , I would stop my my activities altogether . 3 However , as however , I would stop doing my business altogether .\n4 However , as Pres. , I would stop my my activities altogether . 4 However , as Pres. , I would stop doing my business altogether\n5 However , as Pres. , I would stop doing my activities altogether .\n1 As Pres. as Pres. , I would stop stop my doing altogether . 1As Pres. as Pres. , I would stop stop my doing altogether .\n2 However , as Pres. , I would stopmy my doing altogether . 2 As , , Pres. , I would stop doing my activities altogether .\n3 However , as Pres. , I would stop my my activities altogether. 1As Pres. as Pres. , I would stop stop my doing altogether .\n4 However , as Pres. , I would stop my my activities altogether . 2 As , , Pres. , I would stop doing my activities altogether .\nFigure 4.An example of inference iterations in de→en from the dev set when max iteration T is 5. (Pres. stands for President). We show\nhow each of the underscored words were generated in the bottom section. Prediction is conditioned on highlighted tokens.\n1 5 10\n45.5\n46.0\n46.5\nb = 1\nb = 5\nT = 10\nT = 3\nT = 10\nT = 3\nLength Beam SizeK\nEN→FR BLEU\nCMLM+Mask-Predict, T=10\nDisCo+Mask-Predict, T=10\nDisCo+Easy-First\nFigure 5.EN→FR dev results with varying length beam size.\nGhazvininejad et al., 2020a; Li et al., 2020), reordering in-\nput sentences (Ran et al., 2019), generating with an energy-\nbased inference network (Tu et al., 2020), training on addi-\ntional data from an autoregressive model (Zhou & Keung,\n2020), and modeling with latent variables (Ma et al., 2019;\nShu et al., 2020).\nWhile this work took iterative decoding methods, our DisCo\ntransformer can be combined with other approaches for\nefﬁcient training. For example, Li et al. (2019) trained\ntwo separate non-autoregressive and autoregressive models,\nbut it is possible to train a single DisCo transformer with\nboth autoregressive and random masking and use hidden\nstates from autoregressive masking as a teacher. We leave\nintegration of the DisCo transformer with more approaches\nto non-autoregressive translation for future.\nWe also note that our DisCo transformer can be used for\ngeneral-purpose representation learning. In particular, Liu\net al. (2019) found that masking different tokens in every\n20 40 60 80 100\n5\n10\n15\n20\n25\ny = x\ny = log2 x\nTarget Length\n# Steps\nFigure 6.# Reﬁnement steps vs. target length on the WMT14\nen→de test data.\nepoch outperforms static masking in BERT (Devlin et al.,\n2019). Our DisCo transformer would allow for making a\nprediction at every position given arbitrary context, provid-\ning even more ﬂexibility for large-scale pretraining.\n7. Conclusion\nWe presented the DisCo transformer that predicts every\nword in a sentence conditioned on an arbitrary subset of\nthe other words. We developed an inference algorithm that\ntakes advantage of this efﬁciency and further speeds up\ngeneration without loss in translation quality. Our results\nprovide further support for the claim that non-autoregressive\ntranslation is a fast viable alternative to autoregressive trans-\nlation. Nonetheless, a discrepancy still remains between\nautoregressive and non-autoregressive performance when\nknowledge distillation from a large transformer is applied to\nboth. We will explore ways to narrow this gap in the future.\nNon-autoregressive Machine Translation with Disentangled Context Transformer\nAcknowledgements\nWe thank Tim Dettmers, Hao Peng, Mohammad Rasooli,\nWilliam Chan, and Qinghong Han as well as the anonymous\nreviewers for their helpful feedback on this work.\nReferences\nBaevski, A., Edunov, S., Liu, Y ., Zettlemoyer, L. S.,\nand Auli, M. Cloze-driven pretraining of self-attention\nnetworks, 2019. URL https://arxiv.org/abs/\n1903.07785.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate. In\nProc. of ICLR , 2015. URL https://arxiv.org/\nabs/1409.0473.\nChan, W., Kitaev, N., Guu, K., Stern, M., and Uszkoreit,\nJ. KERMIT: Generative insertion-based modeling for se-\nquences, 2019a. URL https://arxiv.org/abs/\n1906.01604.\nChan, W., Stern, M., Kiros, J. R., and Uszkoreit, J. An\nempirical study of generation order for machine trans-\nlation, 2019b. URL https://arxiv.org/abs/\n1910.13437.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and\nKaiser, L. Universal transformers, 2019. URL https:\n//arxiv.org/abs/1807.03819.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Proc. of NAACL-HLT, 2019.\nURL https://arxiv.org/abs/810.04805.\nGehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin,\nY . Convolutional sequence to sequence learning. InProc.\nof ICML, 2017. URL https://arxiv.org/abs/\n1705.03122.\nGhazvininejad, M., Levy, O., Liu, Y ., and Zettlemoyer,\nL. S. Mask-predict: Parallel decoding of conditional\nmasked language models. In Proc. of EMNLP , 2019.\nURL https://arxiv.org/abs/1904.09324.\nGhazvininejad, M., Karpukhin, V ., Zettlemoyer, L., and\nLevy, O. Aligned cross entropy for non-autoregressive\nmachine translation, 2020a. URL https://arxiv.\norg/abs/2004.01655.\nGhazvininejad, M., Levy, O., and Zettlemoyer, L. Semi-\nautoregressive training improves mask-predict decoding,\n2020b. URL https://arxiv.org/abs/2001.\n08785.\nGraves, A. Adaptive computation time for recurrent neural\nnetworks, 2016. URL https://arxiv.org/abs/\n1603.08983.\nGu, J., Bradbury, J., Xiong, C., Li, V . O. K., and Socher,\nR. Non-autoregressive neural machine translation. In\nProc. of ICLR , 2018. URL https://arxiv.org/\nabs/1711.02281.\nGu, J., Liu, Q., and Cho, K. Insertion-based decoding with\nautomatically inferred generation order. TACL, 2019a.\nURL https://arxiv.org/abs/1902.01370.\nGu, J., Wang, C., and Zhao, J. Levenshtein transformer.\nIn Proc. of NeurIPS, 2019b. URL https://arxiv.\norg/abs/1905.11006.\nHassan, H., Aue, A., Chen, C., Chowdhary, V ., Clark, J., Fe-\ndermann, C., Huang, X., Junczys-Dowmunt, M., Lewis,\nW., Li, M., Liu, S., Liu, T.-Y ., Luo, R., Menezes, A.,\nQin, T., Seide, F., Tan, X., Tian, F., Wu, L., Wu, S.,\nXia, Y ., Zhang, D., Zhang, Z., and Zhou, M. Achiev-\ning human parity on automatic Chinese to English news\ntranslation, 2018. URL https://arxiv.org/abs/\n1803.05567.\nKaiser, L., Roy, A., Vaswani, A., Parmar, N., Bengio, S.,\nUszkoreit, J., and Shazeer, N. Fast decoding in se-\nquence models using discrete latent variables. In Proc.\nof ICML, 2018. URL https://arxiv.org/abs/\n1803.03382.\nKim, Y . and Rush, A. M. Sequence-level knowledge dis-\ntillation. In Proc. of EMNLP , 2016. URL https:\n//arxiv.org/abs/1606.07947.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In Proc. of ICLR , 2015. URL https:\n//arxiv.org/abs/1412.6980.\nLee, J. D., Mansimov, E., and Cho, K. Deterministic\nnon-autoregressive neural sequence modeling by iter-\native reﬁnement. In Proc. of EMNLP , 2018. URL\nhttps://arxiv.org/abs/1802.06901.\nLi, X., Meng, Y ., Yuan, A., Wu, F., and Li, J. LA V A NAT:\nA non-autoregressive translation model with look-around\ndecoding and vocabulary attention, 2020. URL https:\n//arxiv.org/abs/2002.03084.\nLi, Z., Lin, Z., He, D., Tian, F., Qin, T., Wang, L., and\nLiu, T.-Y . Hint-based training for non-autoregressive\nmachine translation. In Proc. of EMNLP, 2019. URL\nhttps://arxiv.org/abs/1909.06708.\nLibovick´y, J. and Helcl, J. End-to-end non-autoregressive\nneural machine translation with connectionist temporal\nclassiﬁcation. In Proc. of EMNLP, 2018. URL https:\n//arxiv.org/abs/1811.04719.\nNon-autoregressive Machine Translation with Disentangled Context Transformer\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L. S., and Stoyanov,\nV . RoBERTa: A robustly optimized bert pretraining\napproach, 2019. URL https://arxiv.org/abs/\n1907.11692.\nLuong, T., Pham, H., and Manning, C. D. Effective ap-\nproaches to attention-based neural machine translation.\nIn Proc. of EMNLP , September 2015. URL https:\n//www.aclweb.org/anthology/D15-1166.\nMa, X., Zhou, C., Li, X., Neubig, G., and Hovy, E. H.\nFlowSeq: Non-autoregressive conditional sequence gen-\neration with generative ﬂow. In Proc. of EMNLP, 2019.\nURL https://arxiv.org/abs/1909.02480.\nMansimov, E., Wang, A., and Cho, K. A generalized frame-\nwork of sequence generation with application to undi-\nrected sequence models, 2019. URL https://arxiv.\norg/abs/1905.12790.\nMicikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,\nE., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\nVenkatesh, G., and Wu, H. Mixed precision training. In\nProc. of ICLR , 2018. URL https://arxiv.org/\nabs/1710.03740.\nNakayama, S., Kano, T., Tjandra, A., Sakti, S., and\nNakamura, S. Recognition and translation of code-\nswitching speech utterances. In Proc. of Oriental CO-\nCOSDA, 2019. URL https://ahcweb01.naist.\njp/papers/conference/2019/201910_\nOCOCOSDA_sahoko-n/201910_OCOCOSDA_\nsahoko-n.paper.pdf.\nOtt, M., Edunov, S., Grangier, D., and Auli, M. Scaling\nneural machine translation. In Proc. of WMT, 2018. URL\nhttps://arxiv.org/abs/1806.00187.\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng,\nN., Grangier, D., and Auli, M. fairseq: A fast, extensi-\nble toolkit for sequence modeling. In NAACL Demon-\nstrations, 2019. URL https://arxiv.org/abs/\n1904.01038.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. BLEU:\na method for automatic evaluation of machine translation.\nIn Proc. of ACL, 2002. URL https://www.aclweb.\norg/anthology/P02-1040/.\nPost, M. A call for clarity in reporting BLEU scores. In\nProc. of WMT, 2018. URL https://www.aclweb.\norg/anthology/W18-6319.\nRan, Q., Lin, Y ., Li, P., and Zhou, J. Guiding non-\nautoregressive neural machine translation decoding with\nreordering information, 2019. URL https://arxiv.\norg/abs/1911.02215.\nSaharia, C., Chan, W., Saxena, S., and Norouzi, M. Non-\nautoregressive machine translation with latent align-\nments, 2020. URL https://arxiv.org/abs/\n2004.07437.\nSennrich, R., Haddow, B., and Birch, A. Neural machine\ntranslation of rare words with subword units. In Proc.\nof ACL, 2016. URL https://www.aclweb.org/\nanthology/P16-1162.\nShao, C., Zhang, J., Feng, Y ., Meng, F., and Zhou,\nJ. Minimizing the bag-of-ngrams difference for non-\nautoregressive neural machine translation. In Proc.\nof AAAI, 2020. URL https://arxiv.org/abs/\n1911.09320.\nShu, R., Lee, J., Nakayama, H., and Cho, K. Latent-variable\nnon-autoregressive neural machine translation with de-\nterministic inference using a delta posterior. In Proc.\nof AAAI, 2020. URL https://arxiv.org/abs/\n1908.07181.\nStern, M., Shazeer, N., and Uszkoreit, J. Blockwise parallel\ndecoding for deep autoregressive models. In Proc. of\nNeurIPS, 2018. URL https://arxiv.org/abs/\n1811.03115.\nStern, M., Chan, W., Kiros, J. R., and Uszkoreit, J. Insertion\ntransformer: Flexible sequence generation via insertion\noperations. In Proc. of ICML, 2019. URL https://\narxiv.org/abs/1902.03249.\nSun, Z., Li, Z., Wang, H., He, D., Lin, Z., and Deng, Z.\nFast structured decoding for sequence models. In Proc. of\nNeurIPS, 2019. URL https://arxiv.org/abs/\n1910.11555.\nTu, L., Pang, R. Y ., Wiseman, S., and Gimpel, K. ENGINE:\nEnergy-based inference networks for non-autoregressive\nmachine translation. In Proc. of ACL, 2020. URL https:\n//arxiv.org/abs/2005.00850.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Proc. of NeurIPS, 2017. URL https:\n//arxiv.org/abs/1706.03762.\nWang, Y ., Tian, F., He, D., Qin, T., Zhai, C., and Liu, T.-Y .\nNon-autoregressive machine translation with auxiliary\nregularization. In Proc. of AAAI, 2019. URL https:\n//arxiv.org/abs/1902.10245.\nWu, F., Fan, A., Baevski, A., Dauphin, Y ., and Auli, M. Pay\nless attention with lightweight and dynamic convolutions.\nIn Proc. of ICLR, 2019. URL https://arxiv.org/\nabs/1901.10430.\nNon-autoregressive Machine Translation with Disentangled Context Transformer\nYang, B., Liu, F., and Zou, Y . Non-autoregressive video cap-\ntioning with iterative reﬁnement, 2019a. URL https:\n//arxiv.org/abs/1911.12018.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J. G., Salakhutdi-\nnov, R., and Le, Q. V . XLNet: Generalized autoregres-\nsive pretraining for language understanding. In Proc. of\nNeurIPS, 2019b. URL https://arxiv.org/abs/\n1906.08237.\nZhang, Y ., Wang, G., Li, C., Gan, Z., Brockett, C., and\nDolan, B. POINTER: Constrained text generation via\ninsertion-based generative pre-training, 2020. URL\nhttps://arxiv.org/abs/2005.00558.\nZhou, C., Neubig, G., and Gu, J. Understanding knowledge\ndistillation in non-autoregressive machine translation. In\nProc. of ICLR , 2020. URL https://arxiv.org/\nabs/1911.02727.\nZhou, J. and Keung, P. Improving non-autoregressive neu-\nral machine translation with monolingual data. In Proc.\nof ACL , 2020. URL https://arxiv.org/abs/\n2005.00932.",
  "topic": "Security token",
  "concepts": [
    {
      "name": "Security token",
      "score": 0.8566021919250488
    },
    {
      "name": "Computer science",
      "score": 0.8241962194442749
    },
    {
      "name": "Machine translation",
      "score": 0.7891789674758911
    },
    {
      "name": "Inference",
      "score": 0.720565140247345
    },
    {
      "name": "Transformer",
      "score": 0.7161325216293335
    },
    {
      "name": "Autoregressive model",
      "score": 0.6246390342712402
    },
    {
      "name": "Decoding methods",
      "score": 0.5864495038986206
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4458169937133789
    },
    {
      "name": "Latency (audio)",
      "score": 0.42446067929267883
    },
    {
      "name": "Speech recognition",
      "score": 0.4066493511199951
    },
    {
      "name": "Algorithm",
      "score": 0.3512730300426483
    },
    {
      "name": "Mathematics",
      "score": 0.08564549684524536
    },
    {
      "name": "Voltage",
      "score": 0.08030623197555542
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ]
}