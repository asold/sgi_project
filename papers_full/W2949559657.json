{
  "title": "A Joint Model of Language and Perception for Grounded Attribute Learning",
  "url": "https://openalex.org/W2949559657",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5019767181",
      "name": "Cynthia Matuszek",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5073439637",
      "name": "Nicholas FitzGerald",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5067919401",
      "name": "Luke Zettlemoyer",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5085032007",
      "name": "Liefeng Bo",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5108257764",
      "name": "Dieter Fox",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2130325614",
    "https://openalex.org/W2117115175",
    "https://openalex.org/W2236233024",
    "https://openalex.org/W2114570910",
    "https://openalex.org/W2127300021",
    "https://openalex.org/W2163274265",
    "https://openalex.org/W2099606917",
    "https://openalex.org/W2097018403",
    "https://openalex.org/W2953361271",
    "https://openalex.org/W2134211342",
    "https://openalex.org/W2103444992",
    "https://openalex.org/W2121465811",
    "https://openalex.org/W46490633",
    "https://openalex.org/W2137471889",
    "https://openalex.org/W2404917851",
    "https://openalex.org/W2151103935",
    "https://openalex.org/W2118781169",
    "https://openalex.org/W2168356304",
    "https://openalex.org/W2294130536",
    "https://openalex.org/W2227250678",
    "https://openalex.org/W2098411764",
    "https://openalex.org/W2074142320"
  ],
  "abstract": "As robots become more ubiquitous and capable, it becomes ever more important to enable untrained users to easily interact with them. Recently, this has led to study of the language grounding problem, where the goal is to extract representations of the meanings of natural language tied to perception and actuation in the physical world. In this paper, we present an approach for joint learning of language and perception models for grounded attribute induction. Our perception model includes attribute classifiers, for example to detect object color and shape, and the language model is based on a probabilistic categorial grammar that enables the construction of rich, compositional meaning representations. The approach is evaluated on the task of interpreting sentences that describe sets of objects in a physical workspace. We demonstrate accurate task performance and effective latent-variable concept induction in physical grounded scenes.",
  "full_text": "A Joint Model of Language and Perception\nfor Grounded Attribute Learning\nCynthia Matuszek cynthia@cs.washington.edu\nNicholas FitzGerald nfitz@cs.washington.edu\nLuke Zettlemoyer lsz@cs.washington.edu\nLiefeng Bo lfb@cs.washington.edu\nDieter Fox fox@cs.washington.edu\nComputer Science and Engineering, Box 352350, University of Washington, Seattle, WA 98195-2350\nAbstract\nAs robots become more ubiquitous and ca-\npable, it becomes ever more important for\nuntrained users to easily interact with them.\nRecently, this has led to study of the lan-\nguage grounding problem, where the goal\nis to extract representations of the mean-\nings of natural language tied to the physi-\ncal world. We present an approach for joint\nlearning of language and perception models\nfor grounded attribute induction. The per-\nception model includes classiﬁers for phys-\nical characteristics and a language model\nbased on a probabilistic categorial grammar\nthat enables the construction of composi-\ntional meaning representations. We evaluate\non the task of interpreting sentences that de-\nscribe sets of objects in a physical workspace,\nand demonstrate accurate task performance\nand eﬀective latent-variable concept induc-\ntion in physical grounded scenes.\n1. Introduction\nPhysically grounded settings provide exciting oppor-\ntunities for learning. For example, a person might be\nable to teach a robot about objects in its environment.\nHowever, to do this, a robot must jointly reason about\nthe diﬀerent modalities encountered (for example lan-\nguage and vision), and induce rich associations with\nas little guidance as possible.\nConsider a simple sentence such as “These are the yel-\nlow blocks,” uttered in a setting where there is a phys-\nAppearing in Proceedings of the 29th International Confer-\nence on Machine Learning, Edinburgh, Scotland, UK, 2012.\nCopyright 2012 by the author(s)/owner(s).\nical workspace that contains a number of objects that\nvary in shape and color. We assume that a robot can\nunderstand sentences like this if it can solve the as-\nsociated grounded object selection task. Speciﬁcally, it\nmust realize that words such as “yellow” and “blocks”\nrefer to object attributes, and ground the meaning of\nsuch words by mapping them to a perceptual system\nthat will enable it to identify the speciﬁc physical ob-\njects referred to. To do so robustly, even in cases where\nwords or attributes are new, our robot must learn (1)\nvisual classiﬁers that identify the appropriate object\nproperties, (2) representations of the meaning of indi-\nvidual words that incorporate these classiﬁers, and (3)\na model of compositional semantics used to analyze\ncomplete sentences.\nIn this paper, we present an approach for jointly learn-\ning these components. Our approach builds on exist-\ning work on visual attribute classiﬁcation (Bo et al.,\n2011) and probabilistic categorial grammar induction\nfor semantic parsing (Zettlemoyer & Collins, 2005;\nKwiatkowski et al., 2011). Speciﬁcally, our system in-\nduces new grounded concepts (groups of words along\nwith the parameters of the attribute classiﬁer they are\npaired with) from a set of scenes containing only sen-\ntences, images, and indications of what objects are\nbeing referred to. As a result, it can be taught to rec-\nognize previously unknown object attributes by some-\none describing objects while pointing out the relevant\nobjects in a set of training scenes. Learning is on-\nline, adding one scene at a time, and EM-like, in that\nthe parameters are updated to maximize the expected\nmarginal likelihood of the latent language and visual\ncomponents of the model. This integrated approach\nallows for eﬀective model updates with no explicit la-\nbeling of logical meaning representations or attribute\nclassiﬁer outputs.\nWe evaluate this approach on data gathered on Ama-\nA Joint Model of Language and Perception for Grounded Attribute Learning\nzon Mechanical Turk, in which people describe sets of\nobjects on a table. Experiments demonstrate that the\njoint learning approach can eﬀectively extend the set\nof grounded concepts in an incomplete model initial-\nized with supervised training on a small dataset. This\nprovides a simple mechanism for learning vocabulary\nin a physical environment.\nFigure 1.An example of an RGB-D object identiﬁcation\nscene. Columns on the right show example segments, iden-\ntiﬁed as positive (far right) and negative (center).\n2. Overview of the Approach\nProblem We wish to learn a joint language and per-\nception model for the object selection task. The goal\nis to automatically map a natural language sentence\nx and a set of scene objects O to the subset G ⊆O\nof objects described by x. The left panel of Fig. 1\nshows an example scene. Here, O is the set of objects\npresent in this scene. The individual objects o∈Oare\nextracted from the scene via segmentation (the right\npanel of Fig. 1 shows example segments). Given the\nsentence x=“Here are the yellow ones,” the goal is to\nselect the ﬁve yellow objects for the named set G.\nModel Components Given a sentence and seg-\nmented scene objects, we learn a distribution P(G |\nx,O) over the selected set. Our approach combines\nrecent models of language and vision, including:\n(1) A semantic parsing model that deﬁnes P(z|x), a\ndistribution over logical meaning representations z for\neach sentence x. In our running example, the desired\nrepresentation z = λx.color(x,yellow) is a lambda-\ncalculus expression that deﬁnes a set of objects that\nare yellow. For this task, we build on an existing se-\nmantic parsing model (Kwiatkowski et al., 2011).\n(2) A set of visual attribute classiﬁers C, where each\nclassiﬁer c ∈C deﬁnes a distribution P(c = true|o)\nof the classiﬁer returning true for each possible object\no ∈O in the scene. For example, there would be a\nunique classiﬁer c∈C for each possible color or shape\nan object can have. We use logistic regression to train\nclassiﬁers on color and shape features extracted from\nobject segments recorded using a Kinect depth camera.\nJoint Model We combine these language and vision\nmodels in two ways. First, we introduce an explicit\nmodel of alignment between the logical constants in\nthe logical form z and classiﬁers in the set C. This\nalignment would, for example, enable us to learn that\nthe logical constant yellow should be paired with a\nclassiﬁer c∈C that ﬁres on yellow objects.\nNext, we introduce an execution model that allows\nus to determine what scene objects in O would be\nselected by a logical expression z, given the classi-\nﬁers in C. This allows us to, for example, execute\nλx.color(x,green)∧shape(x,triangle) by testing all of\nthe objects with the appropriate classiﬁers (for green\nand triangle), then selecting objects on which both\nclassiﬁers return true. This execution model includes\nuncertainty from the semantic parser P(z|x), classiﬁer\nconﬁdences P(c= true|o), and a deterministic ground-\ntruth constraint that encodes what objects are actually\nintended to be selected. Full details are in Sec. 5.\nModel Learning We present an approach that\nlearns the meaning of new words from a dataset D =\n{(xi,Oi,Gi) |i = 1 ...n }, where each example i con-\ntains a sentence xi, the objects Oi, and the selected\nset Gi. This setup is an abstraction of the situa-\ntion where a teacher mentions xi while pointing to\nthe objects Gi ⊆Oi she describes. As described in\ndetail in Sec. 6, learning proceeds in an online, EM-\nlike fashion by repeatedly estimating expectations over\nthe latent logical forms zi and the outputs of the clas-\nsiﬁers c∈C, then using these expectations to update\nthe parameters of the component models for language\nP(z|x) and visual classiﬁcation P(c|o). To bootstrap\nthe learning approach, we ﬁrst train a limited language\nand perception system in a fully supervised way: in\nthis stage, each example additionally contains labeled\nlogical meaning expressions and classiﬁer outputs, as\ndescribed in Sec. 6.\n3. Related Work\nTo the best of our knowledge, this paper presents the\nﬁrst approach for jointly learning visual classiﬁers and\nsemantic parsers, to produce rich, compositional mod-\nels that span directly from sensors to meaning. How-\never, there is signiﬁcant related work on the model\ncomponents, and on grounded learning in general.\nVision Current state-of-the-art object recognition\nsystems (Felzenszwalb et al., 2009; Yang et al., 2009)\nare based on local image descriptors, for example\nSIFT over images (Lowe, 2004) and Spin Images over\n3D point clouds (Johnson & Hebert, 1999). Visual\nA Joint Model of Language and Perception for Grounded Attribute Learning\nthis red block is in the shape of a half-pipe\nN/N N N \\N S \\N/N N/N N/NP NP/NP NP\nλf.f λx.color (x,red) λf.f λf.λg.λx.f (x) ∧ g(x) λf.f λy.λx.shape (x,y) λx.x arch\nN N/NP NP\nλx.color(x,red) λy.λx.shape(x,y) arch\nN N\nλx.color(x,red) λx.shape(x,arch)\nS\\N\nλg.λx.shape(x,arch) ∧ g(x)\nS\nλx.shape(x,arch) ∧ color(x,red)\nFigure 2. An example semantic analysis for a sentence from our dataset.\nattributes provide rich descriptions of objects, and\nhave become a popular topic in the vision commu-\nnity (Farhadi et al., 2009; Parikh & Grauman, 2011);\nalthough very successful, we still lack a deep un-\nderstanding of the design rules underlying them and\nhow they measure similarity. Recent work on ker-\nnel descriptors (Bo et al., 2010) shows that these\nhand-designed features are equivalent to a type of\nmatch kernel that performs similarly to sparse cod-\ning (Yang et al., 2009; Yu & Zhang, 2010) and deep\nnetworks (Lee et al., 2009) on many object recogni-\ntion benchmarks (Bo et al., 2010). We adapt kernel\ndescriptors as feature extractors for attribute classi-\nﬁers because of their strong empirical performance.\nSemantic Parsing There has been signiﬁcant\nwork on supervised learning for inducing semantic\nparsers (Zelle & Mooney, 1996; He & Young, 2006;\nWong & Mooney, 2007). Our research builds on work\non supervised learning of CCG parsers (Zettlemoyer &\nCollins, 2005; Kwiatkowski et al., 2011); there is also\nwork on performing semantic analysis with alternate\nforms of supervision. Clarke (2010) and Liang (2011)\ndescribe approaches to learning semantic parsers from\nquestions paired with database answers, while Gold-\nwasser (2011) presents work on unsupervised learning.\nHowever, none of these approaches include joint mod-\nels of language and vision.\nGrounding There has been signiﬁcant work on\ngrounded learning more generally in the robotics and\nvision communities. A full review is beyond the scope\nof this paper, so we highlight a few examples. Roy de-\nveloped a series of techniques for grounding words in\nvisual scenes (Mavridis & Roy, 2006; Reckman et al.,\n2010; Gorniak & Roy, 2003). In computer vision, the\ngrounding problem often relates to detecting objects\nand attributes in visual information (e.g., see (Barnard\net al., 2003)); however, these approaches primarily fo-\ncus on isolated word meaning, rather than compo-\nsitional semantic analyses. Most closely related to\nour work are approaches that learn probabilistic lan-\nguage models from natural language input (Matuszek\net al., 2012; Chen & Mooney, 2011), especially those\nthat include a visual component (Tellex et al., 2011).\nHowever, these approaches ground language into pre-\ndeﬁned language formalisms, rather than extending\nthe model to account for entirely novel input.\n4. Background on Semantic Parsing\nOur grounded language learning incorporates a state-\nof-the-art model, FUBL, for semantic parsing, as re-\nviewed in this section. FUBL (Kwiatkowski et al.,\n2011) is an algorithm for learning factored Combina-\ntory Categorial Grammar (CCG) lexicons for seman-\ntic parsing. Given a dataset {(xi,zi) | i = 1 ...n}\nof natural language sentences xi, which are paired\nwith logical forms zi that represent their meaning,\nUBL learns a factored lexicon Λ made up of a set\nof lexemes L and a set of lexical templates T. Lex-\nemes combine with templates in order to form lexi-\ncal items, which can be used by a semantic parser to\nparse natural language sentences into logical forms.\nFor example, given the sentence x =“this red block\nis in the shape of a half-pipe” and the logical form\nzi = λx.color(x,red) ∧shape(x,arch), FUBL learns a\nparse like the example in ﬁgure 2. In this parse, the\nlexeme (half-pipe, [arch]) has combined with the tem-\nplate λ(ω,⃗ v).[ω⊢NP : ⃗ v1] to yield the lexical item\nhalf-pipe⊢NP : arch.\nFUBL also learns a log-linear model which produces\nthe probability of a parse y that yields logical form z\ngiven the sentence x:\nP(y,z |x; ΘL,Λ) = eΘL·φ(x,y,z)\n∑\n(y′,z′) eΘL·φ(x,y′,z′) (1)\nwhere φ(x,y,z ) is a feature vector encompassing the\nlexemes and lexical templates used to generate y,\namongst other things.\nIn this work, we initialize our parse model using the\nstandard FUBL approach, followed by automatically\ninducing lexemes paired with new visual attributes not\npresent in the initial training set, as we will see in the\nnext section.\nA Joint Model of Language and Perception for Grounded Attribute Learning\n5. Joint Language/Perception Model\nAs described in Sec. 2, the object selection task is to\nidentify a subset of objects, G, given a scene Oand an\nNL sentence x. We deﬁne a possible world w to be a\nset of classiﬁer outputs, where wo,c ∈{T,F }speciﬁes\nthe boolean output of classiﬁer c for object o. Our\njoint probabilistic model is:\nP(G|x,O) =\n∑\nz\n∑\nw\nP(G,z,w |x,O) (2)\nwhere the latent variable z over logical forms models\nlinguistic uncertainty and the latent w over possible\nworlds models perceptual uncertainty.\nWe further decompose (2) into a product of models for\nlanguage, vision, and grounded execution. This ﬁnal\nmodel selects the named objects G, motivated in Sec. 2\nand described below; the ﬁnal decomposition is:\nP(G,z,w |x,O) = P(z|x)P(w|O)P(G|z,w) (3)\nHere, the language model P(z|x) and vision model\nP(w|O) are held in agreement by the conditional prob-\nability term P(G|z,w). Let z(w) be the set of objects\nthat are selected, under the assignment in w, when\nz is applied to them. For example, the expression\nz= λx.shape(x,cube)∧color(x,red) would return true\nwhen applied to the objects in w for which the classi-\nﬁers for the cubeand redlogical constants return true.\nNow, P(G|z,w) forces agreement and models object\nselection by putting all of its probability mass on the\nset G that equals z(w).\nIn this formulation, the language and vision dis-\ntributions are conditionally independent given this\nagreement. The semantic parsing model P(z|x)\nbuilds on previous work, as described in eqn. (1).\nThe perceptual classiﬁcation P(w|O) is deﬁned as\nfollows: we assume each perceptual classiﬁer is applied\nindependently, decomposing this term into:\nP(w|O) =\n∏\no∈O\n∏\nc∈C\nP(wo,c|o) (4)\nwhere the probability of a world is simply the product\nof the probabilities of the individual classiﬁer assign-\nments for all of the objects.\nEach classiﬁer is a logistic regression model, where the\nprobability of a classiﬁer c on a given object o is:\nP(wo,c = 1|o; ΘP) = eΘP\nc ·φ(o)\n1 + eΘPc ·φ(o) (5)\nwhere ΘP\nc is the parameters in ΘP for classiﬁer c. This\napproach provides a simple, direct way to couple the\nindividual language and vision components to model\nthe object selection task.\nInference There are two key inference problems in a\nmodel of this type. During learning, we need to com-\npute the marginal distribution P(z,w|x,O,G ) over la-\ntent logical forms z and perceptual assignments w\n(see next section). At test time, we must compute\narg maxGP(G|x,O) to ﬁnd the set of named objects.\nComputing this probability distribution requires sum-\nming the total probability of all world/logical form\npairs that name G. For each possible world w, de-\ntermining if z names G is equivalent to a SAT prob-\nlem, as z can theoretically encode an arbitrary logi-\ncal expression that will name the appropriate G only\nwhen satisﬁed. Computing the marginal probability\nis then a weighted model counting problem, which is\nin #-P. However, the logical expressions allowed by\nour current grammar—conjunctions of unary attribute\ndescriptors—admit eﬃcient exact computation, de-\nscribed below.\n6. Model Learning\nThe physically grounded joint learning problem is to\ninduce a model P(G|x,O), given data of the form\nD = {(xi,Oi,Gi) |i= 1 ...n }, where each example i\ncontains a sentence xi, the objects Oi, and the selected\nset Gi. We consider the case where the learner already\nhas a partial model, including a CCG parser with a\nsmall vocabulary and a small set of attribute classi-\nﬁers. The goal is to automatically extend the model\nto induce new classiﬁers that are tied to new words\nin the semantic parser. We ﬁrst describe the learning\nalgorithm, then present how we initialize the approach\nby learning decoupled models from small datasets with\nmore extensive annotations.\nAligning Words to Classiﬁers One key challenge\nis to learn to create new attribute classiﬁers associ-\nated with unseen words in the sentences xi in the data\nD. We take a simple, exhaustive approach by creating\na set of k new classiﬁers, initialized to uniform dis-\ntributions. Each classiﬁer is additionally paired with\na new logical constant in the FUBL lambda-calculus\nlanguage. Finally, a new lexeme is created by pairing\neach previously unknown word in a sentence inDwith\neither one of these new classiﬁer constants, or the logi-\ncal expressions from an existing lexeme in the lexicon.\nThe parsing weights for the indicator features for each\nof these additions are set to 0. This approach learns,\nthrough the probabilistic updates described below, to\njointly reestimate the parameters of both the new clas-\nsiﬁers and the expanded semantic parsing model.\nParameter Estimation We aim to estimate the\nlanguage parameters Θ L and perception parameters\nA Joint Model of Language and Perception for Grounded Attribute Learning\nΘP from data D = {(xi,Oi,Gi) | i = 1 ...n }, as\ndeﬁned above. We want to ﬁnd parameter settings\nthat maximize the marginal log likelihood of D:\nLL(D; ΘL,ΘP) =\n∑\ni=1...n\nln P(Gi|xi,Oi; ΘL,ΘP) (6)\nThis objective is non-convex due to the sum over latent\nassignments for the logical form zand attribute classi-\nﬁer outputs win the deﬁnition of P(Gi|xi,Oi; ΘL,ΘP)\nfrom eqn. (2). However, if z and w are labeled, the\noverall algorithm reduces to simply training the log-\nlinear models for the semantic parser P(z|xi; ΘL) and\nattribute classiﬁers P(w|Oi; ΘP), both well-studied\nproblems. In this situation, we can use an EM\nalgorithm to ﬁrst estimate the marginal P(z,w |\nxi,Oi,Gi; ΘL,ΘP), then maximize the expected like-\nlihood according to the distribution, with a weighted\nversion of our familiar log-linear model parameter up-\ndates. We present an online version of this approach,\nwith updates computed one example at a time.\nComputing Expectations For each example i, we must\ncompute the marginal over latent variables given by:\nP(z,w |xi,Oi,Gi; ΘL,ΘP) =\nP(z|xi; ΘL)P(w|Oi; ΘP)P(Gi|z,w)∑\nz′\n∑\nw′ P(z′|xi; ΘL)P(w′|Oi; ΘP)P(Gi|z′,w′)\n(7)\nSince computing all possible parses z is exponential\nin the length of the sentence, we use beam search to\nﬁnd the top-N parses. This exact inference could be\nreplaced with an approximate method, such as MC-\nSAT, to accommodate a more permissive grammar.\nConditional Expected Gradient For each example, we\nupdate the parameters with the expected gradient,\naccording to the marginal distribution above. For the\nlanguage parameters ΘL, the gradient is\n∆L =\n∑\nz′\n∑\nw′\nP(z′,w′|xi,Oi,Gi; ΘL,ΘP)∗\n(EP(y|xi,z′;ΘL)\n[\nφL\nj (xi,y,z ′)\n]\n−\nEP(y,z|xi;ΘL)\n[\nφL\nj (xi,y,z )\n]\n)\n(8)\nwhere the inner diﬀerence of expectations is the fa-\nmiliar gradient of a log-linear model for conditional\nrandom ﬁelds with hidden variables (Quattoni et al.,\n2007; Kwiatkowski et al., 2010), and is weighted ac-\ncording to the expectation.\nSimilarly, for the perception parameters Θ P, the gra-\ndient is:\n∆P\nc =\n∑\nz′\n∑\nw′\nP(z′,w′|xi,Oi,Gi; ΘL,ΘP)∗\n∑\no∈Oi\n[\nw′\no,c −P(w′\no,c = 1 |φ(o); ΘP)\n]\nφ(o)\n(9)\nwhere the inner sum ranges over the objects and adds\nin the familiar gradient for logistic regression binary-\nclassiﬁcation models.\nOnline Updates We use a simple, online parameter\nestimation scheme that loops over the data K = 10\n(picked on validation set) times. For each data point\ni consisting of the tuple ( xi,Oi,Gi), we perform an\nupdate where we take a step according to the above\nexpected gradient over the latent variables. We use a\nlearning rate of 0.1 with a constant decay of .00001\nper update for all experiments.\nDiscussion This complete learning approach pro-\nvides an eﬃcient online algorithm that closely matches\nthe style of interactive, grounded language learning we\nare pursuing in this work. Given the decayed learning\nrate, the algorithm is guaranteed to converge, but lit-\ntle can be said about the optimality of the solution.\nHowever, as we see in Sec. 7, the approach works well\nin practice for the object set selection task we consider.\nBootstrapping To construct the initial limited lan-\nguage and perceptual models, we make use of a small,\nsupervised data set Dsup = {(xi,zi,wi,Oi,Gi) |i =\n1 ...m }, which matches our previous setup but ad-\nditionally labels the latent logical form zi and clas-\nsiﬁer outputs wi. As mentioned above, learning in\nthis setting is completely decoupled and we can es-\ntimate the semantic parsing distribution P(zi|xi; ΘL)\nwith the FUBL learning algorithm (Kwiatkowski et al.,\n2011) and the attribute classiﬁers P(wi|Oi; ΘP) with\ngradient ascent for logistic regression. As we show ex-\nperimentally, Dsup can often be quite small, and will in\ngeneral not contain many of the words and attributes\nthat must be additionally learned in the full approach.\nExploring approaches for learning without Dsup, such\nas replacing it with interactive dialog with a human\nteacher, is an important area for future work.\n7. Experimental Setup\nData Set Data was collected using a selection of\ntoys, including wooden blocks, plastic food, and build-\ning bricks. For each scene, we collected short RGB-D\nvideos with a Kinect depth camera, showing a per-\nson gesturing to a subset of the objects. Natural\nlanguage annotations were gathered using Mechanical\nTurk; workers were asked to describe the objects be-\ning pointed to in the video (see Fig. 3). The referenced\nA Joint Model of Language and Perception for Grounded Attribute Learning\nFigure 3.Example scenes presented on Mechanical Turk.\nLeft: A scene that elicited the descriptions “here are some\nred things” and “these are various types of red colored\nobjects”, both labeled as λx.color(x,red). Right: A scene\nassociated with sentence/meaning pairs such as “this toy\nis orange cube” and λx.color(x,orange) ∧shape(x,cube).\nobjects were then marked as belonging to G, the posi-\ntive set of objects for that scene. A total of 142 scenes\nwere shown, eliciting descriptions of 12 attributes, di-\nvided evenly into shapes and colors. In total, there\nwere 1003 sentence/annotation pairs.\nPerceptual Features To automatically segment ob-\njects from each scene, we performed RANSAC plane\nﬁtting on the Kinect depth values to ﬁnd the ta-\nble plane, then extracted connected components (seg-\nments) of points more than a minimum distance above\nthat plane. After getting segmented objects, features\nfor every object are extracted using kernel descrip-\ntors (Bo et al., 2011). We extract two types of features,\nfor depth values and RGB values; these correspond to\nshape and color attributes, respectively. During train-\ning, the system learns logistic regression classiﬁers us-\ning these features. In the initialization phase used to\nbootstrap the model, the annotation provides informa-\ntion about which language attributes relate to shape\nor color. However, this information is not provided in\nthe training phase.\nLanguage Features We follow (Kwiatkowski et al.,\n2011) in including a standard set of binary indicator\nfeatures to deﬁne the log-linear model P(z|x; ΘL) over\nlogical forms, given sentences. This includes indicators\nfor which lexical entries were used and properties of\nthe logical forms that are constructed. These features\nallow the joint learning approach to weight lexical se-\nlection against evidence provided by the compositional\nanalysis and the visual model components.\n8. Results\nThis section presents results and a discussion of our\nevaluation. We demonstrate eﬀective learning in the\nfull model for the object set selection task. We\nthen brieﬂy describe ablation studies and examples of\nlearned models..\n8.1. Object Set Selection\nTo measure set selection task performance, we di-\nvided the data according to attribute. To initialize\nthe model, we used the data for six of the attributes to\ntrain supervised classiﬁers, and provided logical forms\nfor the corresponding sentences to train the initial se-\nmantic parsing model, as described at the end of Sec. 6.\nData for the remaining six attributes were used for\nevaluation, with 80% allocated for training and 20%\nheld out for testing. Here, all of the visual scenes\nare previously unseen, the words in the sentences de-\nscribing the new attributes are unknown, and the only\navailable labels are the output object set G.\nWe report precision, recall, and F1-score on the set\nselection task. Results are averaged over 10 diﬀerent\nruns with the training data presented in diﬀerent ran-\ndomized orders. The system performs well, achieving\nan average precision of 82%, recall of 71%, and a 76%\nF1-score. This level of performance is achieved rela-\ntively quickly; performance generally converges within\nﬁve passes over the training data.\n8.2. Ablation Studies\nTo examine the need for a joint model, we measure\nperformance of two models in which either the lan-\nguage or the visual component is sharply limited. In\neach case, performance signiﬁcantly degrades. These\nresults are summarized in Fig. 4.\nVision In order to measure how a set of classiﬁers\nwould perform on the set selection task with only a\nsimple language model, we manually created a the-\nsaurus of words used in the dataset to refer to target\nattributes containing, on average, 5 diﬀerent ways of\nreferring to each color and shape. To learn the unsu-\npervised concepts for this baseline, we ﬁrst extracted\na list of all words appearing in the training corpus but\nnot in the initialization data; words which appear in\nthe thesaurus are grouped into synonym sets. To train\nclassiﬁers, we collect objects from scenes in which only\nterms from the given synonym set appear. Any syn-\nonym set which does not occur in at least 2 distinct\nscenes is discarded. The resulting positive and neg-\native objects are used to train classiﬁers. To gener-\nate a predicted set of objects at test time, we ﬁnd all\nsynonym sets which occur in the sentence x, and de-\ntermine whether the classiﬁers associated with those\nwords successfully identify the object.\nAveraged across our trials, the results are as follows:\nPrecision=0.92; Recall=0.41; F1-score=0.55. These\nresults are, on average, notably worse than the per-\nformance of the jointly trained model.\nA Joint Model of Language and Perception for Grounded Attribute Learning\nSemantic Parsing As a baseline for testing how well\na pure parsing model will perform when the perception\nmodel is ablated, we run the parsing model obtained\nduring initialization directly on the test set, training\nno new classiﬁers. Since the parser is capable of gener-\nating parses by skipping unknown words, this baseline\nis equivalent to treating the unknown concept words\nas if they are semantically empty.\nAveraged across our trials, the results are as follows:\nPrecision=0.52; Recall=0.09; F1-score=0.14. Not sur-\nprisingly, a substantial number of parses selected no\nobjects, as the parser has no way of determining the\nmeaning of an unknown word.\nPrecision Recall F1-Score\nVision 0.92 0.41 0.55\nLanguage 0.52 0.09 0.14\nJoint 0.82 0.71 0.76\nFigure 4.A summary of precision, recall, and F1 for ab-\nlated models and the joint learning model.\n8.3. Discussion and Examples\nThis section discusses typical training runs and data\nrequirements. We present examples of learned mod-\nels, highlighting what is learned and typical errors,\nand then describe simple experiment investigating the\namount of supervised data required for initialization.\nClassiﬁer performance after training eﬀects the sys-\ntem’s ability to perform the set selection task. Dur-\ning a sample trial, average accuracy of color and shape\nclassiﬁers for newly learned concepts are 97% and 74%,\nrespectively. Although these values are suﬃcient for\nreasonable task performance, there are some failures—\nfor example, the shape attributes “cube” and “cylin-\nder” are sometimes challenging to diﬀerentiate.\nAs noted in Sec. 4, the semantic parser contains lex-\nemes that pair words with learned classiﬁers, and fea-\ntures that indicate lexeme use during parsing. Fig. 5\nshows some selected word/classiﬁer pairs, along with\nthe weight for their associated feature (each trial pro-\nFigure 5.Feature weights for hypothesized lexemes pairing\nnatural language words (rows) with newly created terms\nreferring to novel classiﬁers (columns), as well as the spe-\ncial null token. Each weight serves as an unnormalized\nindicator of which associations are preferred.\nduces a large number of such lexemes). The classiﬁers\nnew0–new2 and new3–new5 are color and shape\nclassiﬁers, respectively. As can be seen, each of the\nnovel attributes is most strongly associated with a\nnewly-created classiﬁer, while irrelevant words such as\n“thing” tend to parse to null. The system must iden-\ntify which of the classiﬁer types to use for novel words.\nWe ran additional tests investigating whether the sys-\ntem is able to learn synonyms. Here, we split the data\nso that the training set has attributes learned during\ninitialization, but are referred to by new, synonymous\nwords. These runs performed comparably to those re-\nported above; the approach easily learns lexemes that\npair these new words with the appropriate classiﬁers.\nFinally, we brieﬂy discuss the eﬀects of reducing the\namount of annotated data used to initialize the lan-\nguage and perception model (see Fig. 6). As can\nbe seen, with fewer than 150 sentences, the learned\ngrammar does not seem to have suﬃcient coverage to\nmodel unknown words in joint learning; however, be-\nyond that, performance is quite stable.\nFigure 6.Example F1-score on object recognition from\nmodels initialized with reduced amounts of labeled data,\nreported over one particular data split. The F1-score for\nthis split peaks at roughly 73%.\n9. Conclusion\nThis paper presents a joint model of language and\nperception for grounded attribute learning. Our ap-\nproach learns representations of the meanings of natu-\nral language, using visual perception to ground those\nmeanings in the physical world. Learning is performed\nvia optimizing the data log-likelihood using an online,\nEM-like training algorithm. This system is able to\nlearn accurate language and attribute models for the\nobject set selection task, given data containing only\nlanguage, raw percepts, and the target objects. By\njointly learning language and perception models, the\napproach can identify which novel words are color at-\ntributes, shape attributes, or no attributes at all.\nWe believe our approach has signiﬁcant potential to\nscale to general language grounding problems. In par-\nticular, our modular framework was designed to eas-\nily incorporate future advances in visual classiﬁcation\nA Joint Model of Language and Perception for Grounded Attribute Learning\nand semantic parsing. We are also working to scale the\ncomplexity of the language and physical scenes, with\nthe eventual goal of robust learning in completely un-\nconstrained environments.\nAcknowledgments\nThis work was funded in part by the Intel Science and Technology\nCenter for Pervasive Computing, the Robotics Consortium spon-\nsored by the U.S. Army Research Laboratory under the Collabora-\ntive Technology Alliance Program (W911NF-10-2-0016), and NSF\ngrant IIS-1115966.\nReferences\nBarnard, K., Duygulu, P., Forsyth, D., De Freitas, N.,\nBlei, D.M., and Jordan, M.I. Matching words and pic-\ntures. The Journal of Machine Learning Research , 3:\n1107–1135, 2003.\nBo, L., Ren, X., and Fox, D. Kernel descriptors for visual\nrecognition. In Neural Information Processing Systems\n(NIPS), 2010.\nBo, L., Ren, X., and Fox, D. Depth kernel descriptors for\nobject recognition. In IEEE/RSJ Int’l Conf. on Intelli-\ngent Robots and Systems (IROS) , 2011.\nChen, D.L. and Mooney, R.J. Learning to interpret natural\nlanguage navigation instructions from observations. In\nProc. of the 25th AAAI Conf. on Artiﬁcial Intelligence\n(AAAI-2011), pp. 859–865, August 2011.\nClarke, J., Goldwasser, D., Chang, M., and Roth, D. Driv-\ning semantic parsing from the world’s response. In Proc.\nof the Conf. on Computational Natural Language Learn-\ning, 2010.\nFarhadi, A., Endres, I., Hoiem, D., and Forsyth, D. De-\nscribing objects by their attributes. In IEEE Conf. on\nComputer Vision and Pattern Recognition , 2009.\nFelzenszwalb, P., Girshick, R., McAllester, D., and Ra-\nmanan, D. Object detection with discriminatively\ntrained part based models. IEEE Transactions on\nPattern Analysis and Machine Intelligence , 32(9):1627–\n1645, 2009.\nGoldwasser, D., Reichart, R., Clarke, J., and Roth, D.\nConﬁdence driven unsupervised semantic parsing. In\nProceedings. of the Association of Computational Lin-\nguistics, 2011.\nGorniak, P. and Roy, D. Understanding complex visu-\nally referring utterances. In Proc. of the HLT-NAACL\n2003 Workshop on Learning Word Meaning from Non-\nLinguistic Data, 2003.\nHe, Y. and Young, S. Spoken language understanding using\nthe hidden vector state model. Speech Communication,\n48(3-4), 2006.\nJohnson, A. and Hebert, M. Using spin images for eﬃcient\nobject recognition in cluttered 3D scenes. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence ,\n21(5), 1999.\nKwiatkowski, T., Zettlemoyer, L.S., Goldwater, S., and\nSteedman, M. Inducing probabilistic CCG grammars\nfrom logical form with higher-order uniﬁcation. In Proc.\nof the Conf. on Empirical Methods in Natural Language\nProcessing, 2010.\nKwiatkowski, T., Zettlemoyer, L.S., Goldwater, S., and\nSteedman, M. Lexical generalization in CCG grammar\ninduction for semantic parsing. In Proc. of the Conf.\non Empirical Methods in Natural Language Processing ,\n2011.\nLee, H., Grosse, R., Ranganath, R., and Ng, A. Convo-\nlutional deep belief networks for scalable unsupervised\nlearning of hierarchical representations. In Proc. of the\nInt’l Conf. on Machine Learning (ICML) , 2009.\nLiang, P., Jordan, M.I., and Klein, D. Learning\ndependency-based compositional semantics. In Proc. of\nthe Association for Computational Linguistics , 2011.\nLowe, D. Distinctive image features from scale-invariant\nkeypoints. Int’l Journal of Computer Vision (IJCV) ,\n60:91–110, 2004.\nMatuszek, C., Herbst, E., Zettlemoyer, L., and Fox, D.\nLearning to parse natural language commands to a robot\ncontrol system. In Proc. of the 13th Int’l Symposium on\nExperimental Robotics (ISER), June 2012.\nMavridis, N. and Roy, D. Grounded situation models for\nrobots: Where words and percepts meet. In IEEE/RSJ\nInt’l Conf. on Intelligent Robots and Systems , 2006.\nParikh, D. and Grauman, K. Relative attributes. In Int’l\nConf. on Computer Vision , 2011.\nQuattoni, A., Wang, S., p Morency, L., Collins, M., Darrell,\nT., and Csail, Mit. Hidden-state conditional random\nﬁelds. In IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2007.\nReckman, H., Orkin, J., and Roy, D. Learning meanings of\nwords and constructions, grounded in a virtual game. In\nProc. of the 10th Conf. on Natural Language Processing\n(KONVENS), 2010.\nTellex, S., Kollar, T., Dickerson, S., Walter, M.R., Baner-\njee, A.G., Teller, S., and Roy, N. Understanding natural\nlanguage commands for robotic navigation and mobile\nmanipulation. In Proc. of the National Conf. on Artiﬁ-\ncial Intelligence (AAAI) , August 2011.\nWong, Y.W. and Mooney, R.J. Learning synchronous\ngrammars for semantic parsing with lambda calculus. In\nProc. of the Ass’n for Computational Linguistics , 2007.\nYang, J., Yu, K., Gong, Y., and Huang, T. Linear spatial\npyramid matching using sparse coding for image classiﬁ-\ncation. In IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), 2009.\nYu, K. and Zhang, T. Improved local coordinate coding\nusing local tangents. In Proc. of the Int’l Conf. on Ma-\nchine Learning (ICML), 2010.\nZelle, J.M. and Mooney, R.J. Learning to parse database\nqueries using inductive logic programming. In Proc. of\nthe National Conf. on Artiﬁcial Intelligence , 1996.\nZettlemoyer, L.S. and Collins, M. Learning to map sen-\ntences to logical form: Structured classiﬁcation with\nprobabilistic categorial grammars. In Proc. of the Conf.\non Uncertainty in Artiﬁcial Intelligence , 2005.",
  "topic": "Categorial grammar",
  "concepts": [
    {
      "name": "Categorial grammar",
      "score": 0.7573232054710388
    },
    {
      "name": "Computer science",
      "score": 0.6998883485794067
    },
    {
      "name": "Perception",
      "score": 0.6680301427841187
    },
    {
      "name": "Meaning (existential)",
      "score": 0.6241836547851562
    },
    {
      "name": "Task (project management)",
      "score": 0.6023426055908203
    },
    {
      "name": "Joint attention",
      "score": 0.5950697660446167
    },
    {
      "name": "Robot",
      "score": 0.5389569401741028
    },
    {
      "name": "Object (grammar)",
      "score": 0.5099900960922241
    },
    {
      "name": "Natural language processing",
      "score": 0.502978503704071
    },
    {
      "name": "Natural language",
      "score": 0.49606093764305115
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49589547514915466
    },
    {
      "name": "Joint (building)",
      "score": 0.48746585845947266
    },
    {
      "name": "Probabilistic logic",
      "score": 0.47450727224349976
    },
    {
      "name": "Workspace",
      "score": 0.4481087923049927
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3739945888519287
    },
    {
      "name": "Psychology",
      "score": 0.15497416257858276
    },
    {
      "name": "Generative grammar",
      "score": 0.12895485758781433
    },
    {
      "name": "Engineering",
      "score": 0.10833072662353516
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Mildly context-sensitive grammar formalism",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    },
    {
      "name": "Autism",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Emergent grammar",
      "score": 0.0
    }
  ]
}