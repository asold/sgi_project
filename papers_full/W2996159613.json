{
  "title": "Reducing Transformer Depth on Demand with Structured Dropout",
  "url": "https://openalex.org/W2996159613",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2171998422",
      "name": "Angela Fan",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2114720862",
      "name": "Edouard Grave",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2512114774",
      "name": "Armand Joulin",
      "affiliations": [
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2953071172",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964093309",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W2963972328",
    "https://openalex.org/W2963363373",
    "https://openalex.org/W2563351168",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W2907121943",
    "https://openalex.org/W2951569836",
    "https://openalex.org/W2962944050",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2963000224",
    "https://openalex.org/W2969601108",
    "https://openalex.org/W2963393494",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2963993763",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2951560313",
    "https://openalex.org/W2962965870",
    "https://openalex.org/W2964019666",
    "https://openalex.org/W2970777192",
    "https://openalex.org/W2950452665",
    "https://openalex.org/W104184427",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2963382930",
    "https://openalex.org/W2964040452",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963643655",
    "https://openalex.org/W2964335273",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2951528897",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W4919037",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2970529093",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2114766824",
    "https://openalex.org/W2972451902",
    "https://openalex.org/W2963631907"
  ],
  "abstract": "Overparametrized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality than when training from scratch or using distillation.",
  "full_text": "REDUCING TRANSFORMER DEPTH ON DEMAND WITH\nSTRUCTURED DROPOUT\nAngela Fan\nFacebook AI Research/LORIA\nangelafan@fb.com\nEdouard Grave\nFacebook AI Research\negrave@fb.com\nArmand Joulin\nFacebook AI Research\najoulin@fb.com\nABSTRACT\nOverparameterized transformer networks have obtained state of the art results in\nvarious natural language processing tasks, such as machine translation, language\nmodeling, and question answering. These models contain hundreds of millions of\nparameters, necessitating a large amount of computation and making them prone\nto overﬁtting. In this work, we explore LayerDrop, a form of structured dropout,\nwhich has a regularization effect during training and allows for efﬁcient pruning at\ninference time. In particular, we show that it is possible to select sub-networks of\nany depth from one large network without having to ﬁnetune them and with lim-\nited impact on performance. We demonstrate the effectiveness of our approach by\nimproving the state of the art on machine translation, language modeling, summa-\nrization, question answering, and language understanding benchmarks. Moreover,\nwe show that our approach leads to small BERT-like models of higher quality\ncompared to training from scratch or using distillation.\n1 I NTRODUCTION\nTransformer architectures (Vaswani et al., 2017) have become the dominant architecture in natural\nlanguage processing, with state-of-the-art performance across a variety of tasks, including machine\ntranslation (Vaswani et al., 2017; Ott et al., 2018), language modeling (Dai et al., 2019; Baevski &\nAuli, 2018) and sentence representation (Devlin et al., 2018; Yang et al., 2019). Each of its lay-\ners contains millions of parameters accessed during the forward pass, making it computationally\ndemanding in terms of memory and latency during both training and inference. In an ideal situ-\nation, we would be able to extract sub-networks — automatically and without ﬁnetuning — from\nthis over-parameterized network, for any given memory or latency constraint, while maintaining\ngood performance. In contrast, standard pruning or distillation methods follow a strategy that often\nincludes a ﬁnetuning or retraining step, and the process must be repeated for each desired depth.\nIn this work, we propose a novel approach to train over-parameterized networks such that it is pos-\nsible to extract any sub-network without a post-hoc pruning process. The core of our method is\nto sample small sub-networks from the larger model during training by randomly dropping model\nweights as in Dropout (Hinton et al., 2012) or DropConnect (Wan et al., 2013). This has the ad-\nvantage of making the network robust to subsequent pruning. If well-chosen groups of weights are\ndropped simultaneously, the resulting small sub-networks can be very efﬁcient. In particular, we\ndrop entire layers to extract shallow models at inference time. Previous work (Huang et al., 2016)\nhas shown that dropping layers during training can regularize and reduce the training time of very\ndeep convolutional networks. In contrast, we focus on pruning. As illustrated in Figure 1, an ad-\nvantage of our layer dropping technique, or LayerDrop, is that from one single deep model, we can\nextract shallow sub-networks of any desired depth on demand at inference time.\nWe validate our ﬁndings on a variety of competitive benchmarks, namely WMT14 English-\nGerman for machine translation, WikiText-103 (Merity et al., 2016) for language modeling, CNN-\nDailymail (Hermann et al., 2015) for abstractive summarization, ELI5 (Fan et al., 2017) for long\nform question answering, and several natural language understanding tasks (Wang et al., 2019) for\nsentence representation. Our approach achieves state of the art on most of these benchmarks as a re-\nsult of the regularization effect, which stabilizes the training of larger and deeper networks. We also\nshow that we can prune Transformer architectures to much smaller models while maintaining com-\n1\narXiv:1909.11556v1  [cs.LG]  25 Sep 2019\nFigure 1: LayerDrop (right) randomly drops layers at training time. At test time, this allows for\nsub-network selection to any desired depth as the network has been trained to be robust to pruning.\nIn contrast to standard approaches that must re-train a new model from scratch for each model size\n(left), our method trains only one network from which multiple shallow models can be extracted.\npetitive performance, outperforming speciﬁc model reduction strategies dedicated to BERT (Devlin\net al., 2018; Sanh, 2019) as well as training smaller models from scratch. Overall, applying Layer-\nDrop to Transformer networks provides the following key advantages:\n• LayerDrop regularizes very deep Transformers and stabilizes their training, leading to state-\nof-the-art performance across a variety of benchmarks.\n• Small and efﬁcient models of any depth can be extracted automatically at test time from a\nsingle large pre-trained model, without the need for ﬁnetuning.\n• LayerDrop is as simple to implement as dropout.\n2 R ELATED WORK\nOur approach is a form of Dropout (Srivastava et al., 2014) applied to model weights instead of\nactivations, as in DropConnect (Wan et al., 2013). Different from DropConnect, we drop groups of\nweights to induce group redundancy to create models suited for pruning to shallow, efﬁcient models\nat inference time. Gomez et al. (2018) propose a targeted Dropout and DropConnect, where they\nlearn the drop rate of the weights to match a targeted pruning scheme. Instead, we adapt the masks to\nthe structures that we are interested in pruning. Closer to our work, the Stochastic Depth approach\nof Huang et al. (2016) drops layers randomly during training. As opposed to our work, they are\ninterested in accelerating the training of very deep ResNets (He et al., 2016), so their dropping\nschedule is adapted to this goal. Concurrently to this work, Pham et al. (2019) applied Stochastic\nDepth to train very deep Transformers for speech and show the beneﬁts of its regularization effect.\nMore generally, our method is a form of structured pruning (Liu et al., 2018). As opposed to weight\npruning (LeCun et al., 1990), structured pruning removes coherent groups of weights to preserve the\noriginal structure of the network. Structured pruning has been used in some NLP applications, such\nas machine translation (See et al., 2016), text classiﬁcation (Joulin et al., 2016) and language mod-\neling (Murray & Chiang, 2015). However, it has been more widely adopted in computer vision and\napplied to convolutional network to remove ﬁlters (Li et al., 2016; Wen et al., 2016), channels (He\net al., 2017), or residual blocks (Huang et al., 2018; Huang & Wang, 2018). Similar to Mittal et al.\n(2018), we take advantage of the plasticity of neural networks to learn models that are resilient to\nrandom pruning, rather than learning the pruning itself. We refer the reader to Liu et al. (2018) for an\nexhaustive study of these approaches and their evaluation in the context of convolutional networks.\nReducing the memory footprint of Transformer architectures and BERT in particular is an active\nsubject of research. Several works have compressed BERT as a post-processing step using different\nforms of distillation (Turc et al., 2019; Tang et al., 2019; Shulga, 2019; Sanh, 2019). Similarly,\nvarious papers have shown evidence that Transformers are over-parameterized, especially that most\nself-attention heads can be dropped at test time (Michel et al., 2019; V oita et al., 2019). Different\nfrom these, our models are trained to be resilient to pruning, which signiﬁcantly reduces the perfor-\nmance drop induced by test time pruning. Others have proposed trainable adaptive mechanisms to\n2\ncontrol their memory footprint (Jernite et al., 2016; Sukhbaatar et al., 2019; Correia et al., 2019).\nThese approaches are complementary to ours and should beneﬁt from each other.\n3 M ETHOD\nIn this section, we brieﬂy introduce the Transformer, then describe our Structured Dropout technique\nand its application to layers. We also discuss several inference time pruning strategies.\n3.1 T HE TRANSFORMER ARCHITECTURE\nWe succinctly review the Transformer architecture and refer the reader to Vaswani et al. (2017)\nfor additional details. A Transformer is a stack of layers composed of two sub-layers: multi-head\nself-attention followed by a feedforward sub-layer. The multi-head self-attention sub-layer consists\nof multiple attention heads applied in parallel. Each attention head takes a matrix X where each\nrow represents an element of the input sequence and updates their representations by gathering\ninformation from their context using an Attention mechanism (Bahdanau et al., 2014):\nY = Softmax(XT K(QX + P))VX,\nwhere K, V, Q and P are matrices of parameters. The outputs of the heads are then concatenated\nalong the time step into a sequence of vectors. The second sub-layer then applies a fully connected\nfeedforward network to each element of this sequence independently, FFN(x) = U ReLU (Vx),\nwhere V and U are matrices of parameters. Each sub-layer is followed by a AddNorm operation\nthat is a residual connection (He et al., 2016) and a layer normalization (Ba et al., 2016).\n3.2 T RAINING TRANSFORMERS WITH RANDOM STRUCTURED PRUNING\nWe present an regularization approach that makes Transformers robust to subsequent structured\npruning at inference time. We focus in particular on the case where the targeted structure is a layer.\n3.2.1 R ANDOMLY DROPPING STRUCTURES AT TRAINING TIME\nRegularizing networks to be robust to pruning can be achieved by randomly removing weights dur-\ning its training as in DropConnect (Wan et al., 2013). In this approach, each weight is dropped\nindependently following a Bernoulli distribution associated with a parameterp >0 that controls the\ndrop rate. This is equivalent to a pointwise multiplication of the weight matrix W with a randomly\nsampled {0, 1}mask matrix M:\nWd = M ⊙W.\nDropConnect is a form of random unstructured pruning that leads to smaller, but not necessarily\nmore efﬁcient, models. We propose to add structure to this mechanism to target model efﬁciency.\nRandom Structured Dropout. The weights of a Transformer network belong to multiple over-\nlapping structures, such as heads, FFN matrices, or layers. Dropping weights using groups that\nfollow some of these inherent structures potentially leads to a signiﬁcant reduction of the inference\ntime. This is equivalent to constraining the mask M to be constant over some predeﬁned groups of\nweights. More precisely, given a set Gof predeﬁned groups of weights, the {0, 1}mask matrix M\nis randomly sampled over groups instead of weights:\n∀i, M[i] ∈{0, 1}, and ∀G ∈G, ∀(i, j) ∈G, M[i] =M[j].\nThis structured dropout formulation is general and can be applied to any overlapping groups of\nweights, whether heads, FFN matrices, or layers. Nonetheless, not all of the structures in a Trans-\nformer lead to the same beneﬁts when dropped. For example, dropping attention heads does not\nreduce runtime as they are usually computed in parallel. For simplicity, we focus on dropping lay-\ners, and we name this structured pruning, LayerDrop. This is inspired by the Stochastic Depth\napproach of Huang et al. (2016) used to train very deep ResNets (He et al., 2015).\n3\n3.2.2 P RUNING AT INFERENCE TIME\nSelecting Layers to Prune Training with LayerDrop makes the network more robust to predicting\nwith missing layers. However, LayerDrop does not explicitly provide a way to select which groups\nto prune. We consider several different pruning strategies, described below:\n• Every Other: A straightforward strategy is to simply drop every other layer. Pruning with\na rate p means dropping the layers at a depth d such that d ≡0(mod⌊1\np ⌋). This strategy is\nintuitive and leads to balanced networks.\n•Search on Valid: Another possibility is to compute various combinations of layers to form\nshallower networks using the validation set, then select the best performing for test. This is\nstraightforward but computationally intensive and can lead to overﬁtting on validation.\n•Data Driven Pruning: Finally, we propose data driven pruningwhere we learn the drop\nrate of each layer. Given a target drop rate p, we learn an individual drop rate pd for the\nlayer at depth d such that the average rate over layers is equal to p. More precisely, we\nparameterize pd as a non-linear function of the activation of its layer and apply a softmax.\nAt inference time, we forward only the ﬁxed top-k highest scoring layers based on the\nsoftmax output (e.g. chosen layers do not depend on the input features).\nIn practice, we observe that the Every Otherstrategy works surprisingly well across many tasks and\nconﬁgurations. Search on Validand Data Driven Pruningonly offer marginal gains. Note that we\ndo not further ﬁnetune any of the pruned networks (see Appendix for analysis of ﬁnetuning).\nSetting the drop rate for optimal pruning. There is a straightforward relationship between the\ndrop rate of groups and the average pruning level that the network should be resilient to. Assuming\nN groups and a ﬁxed drop ratiop, the average number of groups used by the network during training\nis N(1 −p). As a consequence, to target a pruning size of r groups, the optimal drop rate is:\np∗ = 1− r\nN\nIn practice, we observe that networks are more robust to pruning than their expected ratio but higher\npruning rates leads to better performance for smaller models. We use a LayerDrop rate of p = 0.2\nfor all our experiments, but we recommend p = 0.5 to target very small inference time models.\n4 E XPERIMENTAL SETUP\nWe apply our method to a variety of sequence modeling tasks: neural machine translation, lan-\nguage modeling, summarization, long form question answering, and various natural language un-\nderstanding tasks. Our models are implemented in PyTorch using fairseq-py (Ott et al., 2019).\nAdditional implementation and training details with hyperparameter settings are in the Appendix.\nNeural Machine Translation. We experiment on the WMT English-German machine translation\nbenchmark using the Transformer Big architecture. We use the dataset of4.5M en-de sentence pairs\nfrom WMT16 (Vaswani et al., 2017) for training, newstest2013 for validation, and newstest2014\nfor test. We optimize the dropout value within the range {0.1, 0.2, 0.5}on the validation set and\nset the LayerDrop rate p to 0.2. For generation, we average the last 10 checkpoints, set the length\npenalty to 0.6, and beam size to 8, following the settings suggested in Wu et al. (2019), and measure\ncase-sensitive tokenized BLEU. We apply compound splitting, as used in Vaswani et al. (2017).\nLanguage Modeling. We experiment on the Wikitext-103 language modeling benchmark (Merity\net al., 2016) which contains100M tokens and a large vocabulary size of260K. We adopt the16 layer\nTransformer used in Baevski & Auli (2018). We set the LayerDrop ratep to 0.2 and tune the standard\ndropout parameter in {0.1, 0.2, 0.3}on the validation set. We report test set perplexity (PPL).\nSummarization. We adopt the Transformer base architecture and training schedule from Edunov\net al. (2019) and experiment on the CNN-Dailymail multi-sentence summarization benchmark. The\ntraining data contains over 280K full-text news articles paired with multi-sentence summaries (Her-\nmann et al., 2015; See et al., 2017). We tune a generation length in the range {40, 50, 60}and\nuse 3-gram blocking. We set the LayerDrop rate p to 0.2. We evaluate using ROUGE (Lin, 2004).\n4\nModel Enc Layers Dec Layers BLEU\nTransformer (Vaswani et al., 2017) 6 6 28.4\nTransformer (Ott et al., 2018) 6 6 29.3\nDynamicConv (Wu et al., 2019) 7 6 29.7\nTransformer (Ott et al., 2018) + LayerDrop 6 6 29.6\nTransformer (Ott et al., 2018) + LayerDrop 12 6 30.2\nTable 1: Results on WMT en-de Machine Translation (newstest2014 test set)\nModel Layers Params PPL\nAdaptive Inputs (Baevski & Auli, 2018) 16 247M 18.7\nTransformer XL Large (Dai et al., 2019) 18 257M 18.3\nAdaptive Inputs + LayerDrop 16 247M 18.3\nAdaptive Inputs + LayerDrop 40 423M 17.7\nTable 2: Results on Wikitext-103 language modeling benchmark (test set).\nLong Form Question Answering. We consider the Long Form Question Answering Dataset ELI5\nof Fan et al. (2019), which consists of 272K question answer pairs from the subreddit Explain Like\nI’m Fivealong with extracted supporting documents from web search. We follow the Transformer\nBig architecture and training procedure of Fan et al. (2019). We generate long answers using beam\nsearch with beam size 5 and apply 3-gram blocking (Fan et al., 2017). We evaluate with ROUGE.\nSentence representation Pre-training. We train base and large BERT (Devlin et al., 2018) mod-\nels following the open-source implementation of Liu et al. (2019). We use two datasets: Bookscor-\npus + Wiki from Liu et al. (2019) and the larger combination of Bookscorpus + OpenWebText\n+ CC-News + Stories (Liu et al., 2019). We evaluate the pretrained models on various natural\nlanguage understanding tasks. Speciﬁcally, we evaluate accuracy on MRPC (Dolan & Brockett,\n2005), QNLI (Rajpurkar et al., 2016), MNLI (Williams et al., 2018), and SST2 (Socher et al., 2013).\n5 R ESULTS\n5.1 L AYER DROP AS A REGULARIZER\nLanguage Modeling. In Table 2, we show the impact of LayerDrop on the performance of a Trans-\nformer network trained in the setting of Adaptive Inputs (Baevski & Auli, 2018). Adding LayerDrop\nto a 16 layer Transformer improves the performance by 0.4 perplexity, matching the state-of-the-art\nresults of Transformer-XL. Our 40 layer Transformer with LayerDrop further improves the state of\nthe art by 0.6 points. Very deep Transformers are typically hard to train because of instability and\nmemory usage, and they are prone to overﬁtting on a small dataset like Wikitext-103. LayerDrop\nregularizes the network, reduces the memory usage, and increases training stability as fewer layers\nare active at each forward pass. These results conﬁrm that this type of approach can be used to\nefﬁciently train very deep networks, as shown in Huang et al. (2016) for convolutional networks.\nSequence to sequence modeling. Similarly, as shown in Table 1 and Table 3, applying Layer-\nDrop to Transformers on text generation tasks such as neural machine translation, summarization,\nand long form question answering also boosts performance for all tasks. In these experiments, we\ntake the Transformer architectures that are state-the-art and train them with LayerDrop. In neu-\nral machine translation on newstest2014, our 12 encoder layer Transformer model with LayerDrop\nfurther improves the state of the art, reaching 30.2 BLEU. In comparison, a standard Transformer\ntrained without LayerDrop diverges with 12 encoder layers. This is a known problem, and tech-\nniques such as improved initialization could be used to maintain stability (Junczys-Dowmunt, 2019;\nZhang et al., 2019), but are out of the scope of this work. Similar results are seen in summarization.\n5\nModel Enc Dec ROUGE-1 ROUGE-2 ROUGE-L\nAbstractive Summarization\nTransformer (Edunov et al., 2019) 6 6 40.1 17.6 36.8\nTransformer + LayerDrop 6 6 40.5 17.9 37.1\nTransformer + LayerDrop 6 8 41.1 18.1 37.5\nLong Form Question Answering\nTransformer Multitask (Fan et al., 2019) 6 6 28.9 5.4 23.1\nTransformer Multitask + LayerDrop 6 6 29.4 5.5 23.4\nTable 3: Results for CNN-Dailymail Summarization and ELI5 QA (test set).\nData Layers Model MNLI-m MRPC QNLI SST2\nBooks + Wiki 24 RoBERTa 89.0 90.2 93.9 95.3\n24 RoBERTa + LayerDrop 89.2 90.2 94.2 95.4\n+ more data 24 RoBERTa 90.2 90.9 94.7 96.4\n24 RoBERTa + LayerDrop 90.1 91.0 94.7 96.8\n481 RoBERTa + LayerDrop 90.4 90.9 94.8 96.9\nTable 4: Results on Various NLU Tasksfor RoBERTa Large trained for 500K updates (dev set).\nBi-Directional Pre-training. In a second set of experiments, we look at the impact of LayerDrop\non pre-training for sentence representation models and subsequent ﬁnetuning on multiple natural\nlanguage understanding tasks. We compare our models to a variant of BERT for sentence represen-\ntations, called RoBERTa (Liu et al., 2019), and analyze the results of ﬁnetuning for data adaptation\non MNLI, MRPC, QNLI, and SST2. We apply LayerDrop during both pre-training and ﬁnetuning.\nWe compare the performance of the large architecture on the BooksCorpus+Wiki dataset used\nin BERT. We analyze the performance of training on the additional data used in RoBERTa as well as\npre-training for even longer. Comparing ﬁxed model size and training data, LayerDrop can improve\nthe performance of RoBERTa on several tasks. LayerDrop can further be used to both enable and\nstabilize the training (Huang et al., 2016) of models double the size for even stronger performance.\n5.2 P RUNING TRANSFORMER LAYERS TO ON-DEMAND DEPTH WITH LAYER DROP\nPruning Generation Tasks. In Figure 2, we investigate the impact of the number of pruned de-\ncoder layers on the performance of a Transformer for language modeling, neural machine transla-\ntion, and summarization. We compare three different settings: standard Transformer models trained\nwithout LayerDrop but subsequently pruned, standard Transformer models trained from scratch to\neach desired depth, and lastly our approach: pruning layers of a Transformer trained with Layer-\nDrop. Our model is trained once with the maximum number of layers and then pruned to the desired\ndepth, without any ﬁnetuning in the shallower conﬁguration. Our approach outperforms small mod-\nels trained from scratch, showing that LayerDrop leads to more accurate small models at a whole\nrange of depths. Further, training with LayerDrop does not incur the computational cost of retrain-\ning a new model for each desired depth. For completeness, dropping layers of a deep Transformer\ntrained without LayerDrop performs poorly as it was not trained to be robust to missing layers.\nPruning BERT-like Models. In Table 7 (left), we compare pruning Transformers trained with\nLayerDrop to different approaches used to create smaller, shallower models. We compare to BERT\nbase and RoBERTa base trained from scratch with 6 and 3 layers as well as recent work on dis-\ntillation, called DistilBERT (Sanh, 2019). We analyze both BERT and RoBERTa models as the\nvocabulary is not the same due to differences in subword tokenization, which affects performance.\nDistilBERT occasionally performs worse than BERT of the same size trained from scratch, which\nconﬁrms the ﬁndings of Liu et al. (2018) about the performance of pruned models compared to\n1The 48 layer model was trained for 300K updates.\n6\n0 10 20 30 40 50\nPercentage of layers pruned\n18\n19\n20\n21\n22\n23\nPerplexity\nLanguage Modeling\n0 20 40 60\nPercentage of layers pruned\n5\n10\n15\n20\n25\n30\n35\nBLEU\nMachine Translation\nBaseline Trained from scratch\n LayerDrop\n0 20 40 60\nPercentage of layers pruned\n35\n36\n37\n38\n39\n40\n41\nROUGE\nSummarization\nFigure 2: Performance as a function of Pruningon various generation tasks (test set), compared to\ntraining smaller models from scratch and pruning a Transformer baseline trained without LayerDrop.\nPruning networks with LayerDrop performs strongly compared to these alternatives.\n0 50 75\nPercentage of layers pruned\n77\n78\n79\n80\n81\n82\n83\n84\n85\nAccuracy\nMNLI\n0 50 75\nPercentage of layers pruned\n88\n89\n90\n91\n92\n93\n94\nAccuracy\nSST 2\nBERT trained from scratch\nRoBERTa trained from scratch\nLayerDrop\nDistilBERT\nMNLI SST2\n6 Layers(50% Pruned)\nRoBERTa 82.3 92.1\n+ LayerDrop 82.9 92.5\n+ more data 84.1 93.2\n3 Layers(75% Pruned)\nRoBERTa 78.1 90.3\n+ LayerDrop 78.6 90.5\n+ more data 82.2 92.0\nFigure 3: (left) Performance as a function of Pruning on MNLI and SST2 compared to BERT\nand RoBERTa trained from scratch and DistilBERT. Pruning one network trained with LayerDrop\n(blue) outperforms alternatives that require a new network for each point. (right) Performance\nwhen Training on More Data shows even stronger results on MNLI and SST2 for pruned models.\ntraining small models from scratch. Our approach, however, obtains results better than BERT and\nRoBERTa trained from scratch. Further, our method does not need any post-processing: we sim-\nply prune every other layer of our RoBERTa model that has been pre-trained with LayerDrop and\nﬁnetune the small models on each of the downstream tasks, following standard procedure. When\ntraining with additional data, shown in Table 7 (right), even stronger performance can be achieved.\n6 A BLATION STUDIES\nComparison of Structured Dropout Figure 4 (left) contrasts various forms of structured dropout:\ndropping attention heads, FFN matrices, and entire Transformer layers. Dropping heads alone is\nworse than dropping entire sub-layers or layers. It also offers no advantage in terms of running\ntime as attention heads are computed in parallel for computational efﬁciency. We observe no large\ndifferences between dropping sub-layers and layers, possibly because we are working with relatively\nshallow networks. In theory, dropping sub-layers should perform better and we expect this to be the\ncase with very deep Transformers. We experiment with overlapping structured groups, such asheads\n+ layersand heads + sub-layersand ﬁnd that the beneﬁcial effect can be advantageously combined.\nWe focus on layers for simplicity, as dropping more structures introduces more parameters to tune.\nComparison of Various Pruning Strategies. Figure 4 (right) contrasts various approaches to sub-\nselecting model layers at inference time. The predominant method used in this paper, the straight-\n7\nHalf\nFFN\nBaseline Head Sublayer Head+\nSublayer\nLayer Head+\nLayer\n17\n18\n19\nPerplexity\n18.8 18.6 18.5 18.4 18.2 18.2 18.0\nStructured Dropout\nLast\n8\nFirst\n8\nEvery\nother\nSearch\nfor 8\nData\ndriven\n16\n18\n20\n22\n24\n26\n28\n30\nPerplexity\n29.3\n26.1\n19.7 19.6 19.5\nSub-selection Techniques\nFigure 4: (left) Impact of Various Structured Dropouts on Wikitext-103 Valid. Dropping Lay-\ners is straightforward and has strong performance. (right) Comparison of Pruning Strategies on\nWikitext-103 Valid. Marginal gains can be achieved, but dropping every other layer is hard to beat.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nLayer number\n20\n21\n22\nPerplexity\nFigure 5: Relative Importance of Speciﬁc Layers.\n(Wikitext-103 Valid) The full network is pruned into\nvarious 8 layer sub-network conﬁgurations, and the av-\nerage perplexity pruning layer n is displayed above.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nPercentage of pruned layers\n18.5\n19.0\n19.5\n20.0\n20.5\n21.0\nPerplexity\n0.5\n0.4\n0.2\n0.1\nTrain drop rate=0.1\nTrain drop rate=0.2\nTrain drop rate=0.4\nTrain drop rate=0.5\nFigure 6: Effect of Train LayerDropon\nInference-time Pruning. (Wikitext-103\nValid) Training with larger LayerDrop is\nbeneﬁcial for signiﬁcant pruning.\nforward strategy of selecting every other layer, is tough to beat. We ﬁnd only marginal improvement\ncan be gained by searching over the validation set for the best set of 8 layers to use and by learning\nwhich layers to drop. In contrast, dropping chunks of consecutive layers is harmful. Namely, re-\nmoving the ﬁrst half or last half of a model is particularly harmful, as the model does not have the\nability to process the input or project to the full vocabulary to predict the subsequent word.\nChoosing which Layers to Prune. Not all layers are equally important. In an experiment on\nWikitext-103, we pruned selections of 8 layers at random. Figure 5 displays the perplexity when\nthat layer is removed, averaging results from 20 pruned model per layer. The input and output layers\nof a network are the most important, as they process the input and project to the output vocabulary.\nRelationship between LayerDrop at Training Time and Pruning at Inference Time. Figure 6\ndisplays the relationship between the training time LayerDrop and the performance of a pruned\nnetwork at test time. If signiﬁcant depth reduction is desired, training with larger LayerDrop is\nbeneﬁcial — this equalizes the train and test time settings. An analysis for BERT is in the Appendix.\n7 C ONCLUSION\nStructured dropout regularizes neural networks to be more robust to applying structured pruning at\ninference time. We focus on the setting where structures are layers, enabling pruning of shallow\nand efﬁcient models of any desired depth. In a variety of text generation and pre-training tasks,\nwe show that LayerDrop enables and stabilizes the training of substantially deeper networks and\nsimultaneously allows for the extraction of models of various depths with strong performance.\n8\nREFERENCES\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.\narXiv preprint arXiv:1809.10853, 2018.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\nGonc ¸alo M Correia, Vlad Niculae, and Andr´e FT Martins. Adaptively sparse transformers. arXiv\npreprint arXiv:1909.00015, 2019.\nZihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860, 2019.\nYann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\nconvolutional networks. In Proc. of ICML, 2017.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nWilliam B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\nIn Proceedings of the International Workshop on Paraphrasing, 2005.\nSergey Edunov, Alexei Baevski, and Michael Auli. Pre-trained language model representations for\nlanguage generation. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pp. 4052–4059, 2019.\nAngela Fan, David Grangier, and Michael Auli. Controllable abstractive summarization. arXiv,\nabs/1711.05217, 2017.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5:\nLong form question answering. arXiv preprint arXiv:1907.09190, 2019.\nAidan N Gomez, Ivan Zhang, Kevin Swersky, Yarin Gal, and Geoffrey E Hinton. Targeted dropout.\n2018.\nEdouard Grave, Armand Joulin, Moustapha Cisse, David Grangier, and Herve Jegou. Efﬁcient\nsoftmax approximation for gpus. arXiv, abs/1609.04309, 2016.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image\nRecognition. In Proc. of CVPR, 2015.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016.\nYihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural net-\nworks. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1389–1397,\n2017.\nKarl Moritz Hermann, Tom ´aˇs Koˇcisk´y, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Proc. of NIPS,\n2015.\nGeoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-\nnov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint\narXiv:1207.0580, 2012.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\nstochastic depth. In European conference on computer vision, pp. 646–661. Springer, 2016.\n9\nGao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: An\nefﬁcient densenet using learned group convolutions. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pp. 2752–2761, 2018.\nZehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks. In\nProceedings of the European Conference on Computer Vision (ECCV), pp. 304–320, 2018.\nYacine Jernite, Edouard Grave, Armand Joulin, and Tomas Mikolov. Variable computation in recur-\nrent neural networks. arXiv preprint arXiv:1611.06188, 2016.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H ´erve J ´egou, and Tomas\nMikolov. Fasttext. zip: Compressing text classiﬁcation models.arXiv preprint arXiv:1612.03651,\n2016.\nMarcin Junczys-Dowmunt. Microsoft translator at wmt 2019: Towards large-scale document-level\nneural machine translation. arXiv preprint arXiv:1907.06170, 2019.\nGuillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint\narXiv:1901.07291, 2019.\nYann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural\ninformation processing systems, pp. 598–605, 1990.\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for\nefﬁcient convnets. arXiv preprint arXiv:1608.08710, 2016.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Workshop on Text\nSummarization Branches Out, 2004.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nZhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of\nnetwork pruning. arXiv preprint arXiv:1810.05270, 2018.\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv\npreprint arXiv:1608.03983, 2016.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer Sentinel Mixture\nModels. arXiv, abs/1609.07843, 2016.\nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? arXiv\npreprint arXiv:1905.10650, 2019.\nDeepak Mittal, Shweta Bhardwaj, Mitesh M Khapra, and Balaraman Ravindran. Recovering from\nrandom pruning: On the plasticity of deep convolutional neural networks. In 2018 IEEE Winter\nConference on Applications of Computer Vision (WACV), pp. 848–857. IEEE, 2018.\nKenton Murray and David Chiang. Auto-sizing neural networks: With applications to n-gram lan-\nguage models. arXiv preprint arXiv:1508.05051, 2015.\nMyle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation.\nIn Proc. of WMT, 2018.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations, 2019.\nRazvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep\nrecurrent neural networks. In Proceedings of the Second International Conference on Learning\nRepresentations (ICLR 2014), 2014.\nNgoc-Quan Pham, Thai-Son Nguyen, Jan Niehues, Markus Muller, and Alex Waibel. Very deep self-\nattention networks for end-to-end speech recognition. arXiv preprint arXiv:1904.13377, 2019.\n10\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Proceedings of EMNLP, pp. 2383–2392. Association for\nComputational Linguistics, 2016.\nVictor Sanh. Smaller, faster, cheaper, lighter: Introducing distilbert, a distilled version of bert.\nhttps://medium.com/huggingface/distilbert-8cf3380435b5, 2019.\nAbigail See, Minh-Thang Luong, and Christopher D Manning. Compression of neural machine\ntranslation models via pruning. arXiv preprint arXiv:1606.09274, 2016.\nAbigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointer-\ngenerator networks. In Proc. of ACL, 2017.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909, 2015.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In Proc. of ACL, 2016.\nDima Shulga. Distilling bert how to achieve bert performance using logistic regression. towards-\ndatascience.com, 2019.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of EMNLP, pp. 1631–1642, 2013.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overﬁtting. JMLR, 15(1):1929–1958,\n2014.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention\nspan in transformers. arXiv preprint arXiv:1905.07799, 2019.\nIlya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-\nization and momentum in deep learning. In International conference on machine learning, pp.\n1139–1147, 2013.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-\nspeciﬁc knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136,\n2019.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better:\nThe impact of student initialization on knowledge distillation. arXiv preprint arXiv:1908.08962,\n2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint\narXiv:1905.09418, 2019.\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural\nnetworks using dropconnect. In International conference on machine learning, pp. 1058–1066,\n2013.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.\nIn the Proceedings of ICLR.\n11\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in\ndeep neural networks. In Advances in neural information processing systems, pp. 2074–2082,\n2016.\nAdina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for\nsentence understanding through inference. In Proceedings of NAACL-HLT, 2018.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay less attention with\nlightweight and dynamic convolutions. InInternational Conference on Learning Representations,\n2019. URL https://arxiv.org/abs/1901.10430.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237, 2019.\nHongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without\nnormalization. arXiv preprint arXiv:1901.09321, 2019.\n12\nA A PPENDIX\nA.1 A DDITIONAL IMPLEMENTATION DETAILS\nA.1.1 N EURAL MACHINE TRANSLATION\nWMT en-de: We model a 32K joint byte-pair encoding. We train using the cosine (Loshchilov\n& Hutter, 2016) learning rate schedule from Wu et al. (2019) with label smoothing 0.1. vocabu-\nlary (Sennrich et al., 2015).\nIWSLT de-en: The dataset consists of 160K training pairs, fully lowercased. We model a 10K joint\nBPE vocabulary and generate with beam size4. We do not average checkpoints. Following Wu et al.\n(2019), we use the Transformer base architecture with 6 encoder layers and 6 decoder layers. As the\ndataset is small, we decrease the overall model size and instead use the following parameters: FFN\nsize 1024, hidden dimension 512, and 4 attention heads.\nPruning: We apply the Every Other Layerstrategy to the decoder and do not ﬁnetune.\nA.1.2 L ANGUAGE MODELING\nTraining: To handle the large vocabulary of Wikitext-103, we follow Dauphin et al. (2017) and\nBaevski & Auli (2018) in using adaptive softmax (Grave et al., 2016) and adaptive input for com-\nputational efﬁciency. For both input and output embeddings, we use dimension size 1024 and three\nadaptive bands: 20K, 40K, and 200K. We use a cosine learning rate schedule (Baevski & Auli, 2018;\nLoshchilov & Hutter, 2016) and train with Nesterov’s accelerated gradient (Sutskever et al., 2013).\nWe set the momentum to 0.99 and renormalize gradients if the norm exceeds 0.1 (Pascanu et al.,\n2014). During training, we partition the data into blocks of contiguous tokens that ignore document\nboundaries. At test time, we respect sentence boundaries.\nPruning: We apply the Every Other Layerstrategy and do not ﬁnetune.\nA.1.3 S UMMARIZATION\nData: We use the full text (non-anonymized) version of CNN-Dailymail introduced by See et al.\n(2017). Following Fan et al. (2017), we truncate articles to 400 tokens and model a joint byte-pair\nvocabulary of 32K types (Sennrich et al., 2016).\nTraining: We train using Adam with a cosine learning rate schedule, warming up for 10K steps. We\noptimize dropout in the range {0.2, 0.3}on the validation set and set LayerDrop to 0.2.\nPruning: We apply the Every Other Layerstrategy to the decoder and do not ﬁnetune.\nA.1.4 L ONG FORM QUESTION ANSWERING\nTraining: We compare to the full multi-task setting of Fan et al. (2019), where data augmentation\nand multi-tasking is done at training time to increase the data available.\nGeneration: We set the minimum length to 150 tokens and the maximum length to 200.\nA.1.5 B I-DIRECTIONAL PRE-TRAINING\nTraining: The base architecture is a 12 layer model with embedding size 768 and FFN size 3072.\nThe large architecture consists of 24 layers with embedding size 1024 and FFN size 4096. For both\nsettings, we follow Liu et al. (2019) in using the subword tokenization scheme from Radford et al.,\nwhich uses bytes as subword units. This eliminates unknown tokens. Note this produces a different\nvocabulary size than BERT (Devlin et al., 2018), meaning models of the same depth do not have\nthe same number of parameters. We train with large batches of size 8192 and we do not use next\nsentence prediction (Lample & Conneau, 2019). We optimize with Adam with a polynomial decay\nlearning rate schedule.\nFinetuning: During ﬁnetuning, we hyperparameter search over three learning rate options (1e-5,\n2e-5, 3e-5) and batchsize (16 or 32 sentences). The other parameters are set following Liu et al.\n(2019). We do single task ﬁnetuning, meaning we only tune on the data provided for the given\n13\nHyperparameter Base Large\nNumber of Layers 12 24\nHidden Size 768 1024\nFFN Size 3072 4096\nAttention Heads 12 16\nLayerDrop 0.2 0.2\nWarmup Steps 24k 30k\nPeak Learning Rate 6e-4 4e-4\nBatch Size 8192 8192\nTable 5: Hyperparameters for RoBERTa Pretraining\nModel BLEU\nTransformer (Wu et al., 2019) 34.4\nDynamic Conv (Wu et al., 2019) 35.2\nTransformer + LayerDrop 34.5\nTable 6: BLEU for IWSLT (test set).\nnatural language understanding task. We do not perform ensembling. When ﬁnetuning models\ntrained with LayerDrop, we apply LayerDrop during ﬁnetuning time as well.\nTraining smaller models: We train the 6 and 3 layer RoBERTa models following the same settings,\nbut using the smaller number of layers and without LayerDrop. We ﬁnetune with the same sweep\nparameters. The 6 and 3 layer BERT model results are taken from Devlin et al. (2018).\nTraining larger models: We train the 48 layer RoBERTa model with 0.5 LayerDrop so only 24\nlayers on average are active during a forward pass.\nPruning: When pruning RoBERTa models, we use the Every Other Layerstrategy and ﬁnetune\nwithout LayerDrop for the smaller models.\nA.2 A DDITIONAL RESULTS\nIWSLT Table 6 displays results on the IWSLT de-en dataset. We see small improvement, likely\nas the network is small and already has a large quantity of regularization with dropout, attention\ndropout, and weight decay. The Transformer is not the state of the art architecture, and there remains\na large gap between the Transformer and the DynamicConv model proposed by Wu et al. (2019).\nPruning BERT Models The numerical values corresponding to the pruned 6 and 3 layer\nRoBERTa + LayerDrop models are shown in Table 7.\nA.3 A DDITIONAL ANALYSIS\nImpact of LayerDrop on training time. Figure 7 shows the increase in training speed when\ntraining with increasingly large quantities of LayerDrop. The words per second were computed\non 8 V100 GPUs with 32GB of memory, without ﬂoating point 16, for a 16 layer model trained\non Wikitext-103. Assuming ﬁxed layer size, LayerDrop removes layers at training time randomly,\nwhich increases the training speed almost 2x if dropping half the number of layers.\nBERT: Relationship between LayerDrop at Training Time and Pruning at Inference Time\nSimilar to the analysis on Language Modeling, we ﬁnd that training with larger quantities of Layer-\nDrop allows for more aggressive pruning at inference time on various natural language generation\ntasks. However, as these tasks involve a ﬁnetuning step on the downstream tasks after pre-training,\nthe effect is less straightforward. Results are shown in Figure 8.\n14\nModel Dataset Layers MNLI-m MRPC QNLI SST-2\nBERT Books + Wiki 6 81.9 84.8 - 91.3\nDistil BERT (Sanh, 2019) Books + Wiki 6 81.6 82.4 85.5 92.7\nRoBERTa Books + Wiki 6 82.3 82.5 89.7 92.1\nRoBERTa + LayerDrop Books + Wiki 6 82.9 85.3 89.4 92.5\nRoBERTa + LayerDrop + more data 6 84.1 86.1 89.5 93.2\nBERT Books + Wiki 3 77.9 79.8 - 88.4\nRoBERTa Books + Wiki 3 78.1 79.4 86.2 90.3\nRoBERTa + LayerDrop Books + Wiki 3 78.6 75.1 86.0 90.5\nRoBERTa + LayerDrop + more data 3 82.2 79.4 88.6 92.0\nTable 7: Comparison between BERT base with and without distillation with our RoBERTa base\ntrained with LayerDrop. Our models are pruned before ﬁnetuning on each individual task. The\nnumbers from BERT are taken from Devlin et al. (2018).\n0\n0.1 0.2 0.3 0.4 0.5\n20\n30\n40\n50\n60\n70\nWords per sec.\nTraining Speed\nFigure 7: Effect of LayerDrop on Training\nTime\nModel Valid PPL\nPruned w/ LayerDrop 20.78\n+ Finetune 20.56\nTable 8: Impact of additional ﬁnetuning on a\n16 layer language model pruned to 8 layers.\n0 50 75\nPercentage of pruned layers\n78\n79\n80\n81\n82\n83\n84\n85\nAccuracy\nMNLI\n0 50 75\nPercentage of pruned layers\n90.5\n91.0\n91.5\n92.0\n92.5\n93.0\n93.5\n94.0\nAccuracy\nSST2\nTrain drop rate=0.1\n Train drop rate=0.2\n Train drop rate=0.5\n0 50 75\nPercentage of pruned layers\n85\n86\n87\n88\n89\n90\n91\n92\nAccuracy\nQNLI\nFigure 8: Effect of Train LayerDrop on Inference-time Pruning on MNLI, SST2, and QNLI\nImpact of Finetuning. LayerDrop allows models to be pruned to the desired depth at test time.\nApart from ﬁnetuning for data adaptation on the GLUE tasks, we do not ﬁnetune the performance\nof our smaller models on any of the other tasks we consider in this work. As shown in Table 8,\nwe found that ﬁnetuning the pruned models only results in marginal improvement. Further, the\nﬁnetuning parameters were dependent on the depth of the model at test time and difﬁcult to optimize.\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8224390745162964
    },
    {
      "name": "Transformer",
      "score": 0.729220449924469
    },
    {
      "name": "Machine translation",
      "score": 0.7228643298149109
    },
    {
      "name": "Language model",
      "score": 0.6730645298957825
    },
    {
      "name": "Automatic summarization",
      "score": 0.6564804315567017
    },
    {
      "name": "Question answering",
      "score": 0.6397767066955566
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5858480334281921
    },
    {
      "name": "Overfitting",
      "score": 0.5455292463302612
    },
    {
      "name": "Inference",
      "score": 0.535528302192688
    },
    {
      "name": "Machine learning",
      "score": 0.5112433433532715
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.48481932282447815
    },
    {
      "name": "Bayesian inference",
      "score": 0.443044513463974
    },
    {
      "name": "Scratch",
      "score": 0.4340730905532837
    },
    {
      "name": "Retraining",
      "score": 0.41387248039245605
    },
    {
      "name": "Natural language processing",
      "score": 0.4135928153991699
    },
    {
      "name": "Artificial neural network",
      "score": 0.2761743664741516
    },
    {
      "name": "Bayesian probability",
      "score": 0.2064843475818634
    },
    {
      "name": "Programming language",
      "score": 0.14151600003242493
    },
    {
      "name": "International trade",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ]
}