{
  "title": "Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions",
  "url": "https://openalex.org/W4389520080",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2103459161",
      "name": "Ziyue Wang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2005825116",
      "name": "Chi Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1906085637",
      "name": "Peng Li",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A1983143503",
      "name": "Yang Liu",
      "affiliations": [
        "Tsinghua University",
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4226321975",
    "https://openalex.org/W4319301039",
    "https://openalex.org/W4309067651",
    "https://openalex.org/W4281944818",
    "https://openalex.org/W2986685865",
    "https://openalex.org/W4320561490",
    "https://openalex.org/W4385574177",
    "https://openalex.org/W3016211260",
    "https://openalex.org/W3196798856",
    "https://openalex.org/W4385572364",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W4312846625",
    "https://openalex.org/W2151200745",
    "https://openalex.org/W3172845486",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4386065596",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4292787094",
    "https://openalex.org/W4296406182",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4311997167",
    "https://openalex.org/W4380993798",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W3035290244",
    "https://openalex.org/W4376312115",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4312971273",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4310629611",
    "https://openalex.org/W4324321291",
    "https://openalex.org/W3034854924",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385569780",
    "https://openalex.org/W4316135748",
    "https://openalex.org/W4296001058",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W4386076140",
    "https://openalex.org/W3101703188",
    "https://openalex.org/W4386065803",
    "https://openalex.org/W4385573705"
  ],
  "abstract": "Large Language Models (LLMs) demonstrate impressive reasoning ability and the maintenance of world knowledge not only in natural language tasks, but also in some vision-language tasks such as open-domain knowledge-based visual question answering (OK-VQA). As images are invisible to LLMs, researchers convert images to text to engage LLMs into the visual question reasoning procedure. This leads to discrepancies between images and their textual representations presented to LLMs, which consequently impedes final reasoning performance. To fill the information gap and better leverage the reasoning capability, we design a framework that enables LLMs to proactively ask relevant questions to unveil more details in the image, along with filters for refining the generated information. We validate our idea on OK-VQA and A-OKVQA. Our method continuously boosts the performance of baselines methods by an average gain of 2.15% on OK-VQA, and achieves consistent improvements across different LLMs.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2874‚Äì2890\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nFilling the Image Information Gap for VQA:\nPrompting Large Language Models to Proactively Ask Questions\nZiyue Wang1, Chi Chen1, Peng Li/Letter2,3, Yang Liu/Letter1,2,3\n1Department of Computer Science and Technology, Tsinghua University, Beijing, China\n2Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China\n3Shanghai Artificial Intelligence Laboratory, Shanghai, China\n{wangziyue22,chenchi19}@mails.tsinghua.edu.cn\nlipeng@air.tsinghua.edu.cn\nliuyang2011@tsinghua.edu.cn\nAbstract\nLarge Language Models (LLMs) demonstrate\nimpressive reasoning ability and the main-\ntenance of world knowledge not only in\nnatural language tasks, but also in some\nvision-language tasks such as open-domain\nknowledge-based visual question answering\n(OK-VQA). As images are invisible to LLMs,\nresearchers convert images to text to engage\nLLMs into the visual question reasoning proce-\ndure. This leads to discrepancies between im-\nages and their textual representations presented\nto LLMs, which consequently impedes final\nreasoning performance. To fill the information\ngap and better leverage the reasoning capabil-\nity, we design a framework that enables LLMs\nto proactively ask relevant questions to unveil\nmore details in the image, along with filters for\nrefining the generated information. We validate\nour idea on OK-VQA and A-OKVQA. Our\nmethod continuously boosts the performance\nof baselines methods by an average gain of\n2.15% on OK-VQA, and achieves consistent\nimprovements across different LLMs1.\n1 Introduction\nLarge-scale language models (LLMs) have exhib-\nited impressive capabilities in terms of their world\nknowledge and reasoning abilities, leading to re-\nmarkable achievements in various Natural Lan-\nguage Processing (NLP) tasks such as common-\nsense reasoning (Tamborrino et al., 2020; Wei et al.,\n2022) and open-domain question answering (Li\net al., 2022; Kamalloo et al., 2023). Building\nupon the success in the realm of text, recent re-\nsearch has explored the utilization of pre-trained\nLLMs in Vision-Language (VL) tasks. These stud-\nies have shown promising performance, especially\nfor knowledge-intensive tasks such as knowledge-\nbased Visual Question Answering (VQA) (Marino\n/LetterCorresponding authors: Peng Li and Yang Liu.\n1Code will be released at https://github.com/\nTHUNLP-MT/FIIG.\nFigure 1: An example of our framework (B) compared\nto baselines (A). In the two methods in (A), the caption\nmodels do not provide precise information of ‚Äúwhat is\nbeing stepped over‚Äù, resulting in hallucinated answers.\nOur method (B) empowers the LLM to actively seek\nand acquire missing information by querying the VLM.\net al., 2019; Schwenk et al., 2022), where both\nimage understanding and external knowledge are\nimperative for answering open-ended questions.\nThe key challenge of leveraging LLMs in VL\ntasks is to bridge the gap between images and text,\ni.e., enabling LLMs to understand images. To ad-\ndress this challenge, two approaches have been\ninvestigated. The first approach extends the LLM\nwith visual perception modules to form a Vision-\nLanguage Pre-training (VLP) model (Alayrac et al.,\n2022; Chen et al., 2022; Li et al., 2023). Despite the\nhigh performance, this approach requires training\non large-scale image-text pairs, which can be com-\n2874\nputationally expensive, particularly for LLMs with\nmassive parameters such as GPT-3 (Brown et al.,\n2020) and PaLM (Chowdhery et al., 2022). The\nsecond approach transforms images into textual\nrepresentations, which are then used as prompts for\nthe LLMs (Yang et al., 2022; Hu et al., 2023a; Chen\net al., 2023; Guo et al., 2023). This training-free\napproach is more cost-effective and enables LLMs\nto be utilized in a more flexible paradigm, allowing\nfor easy adaptation to different tasks.\nHowever, as discussed in the works of Yang et al.\n(2022) and Hu et al. (2023a), general image cap-\ntions may lack the subtle information required to\nanswer visual questions. To resolve this, Yang et al.\n(2022) compromise captions with image tags, and\nHu et al. (2023a) propose incorporating questions\ninto the caption generation process. Despite their\nsuccesses, it remains impractical to reveal every\nsubtle detail necessary to answer visual questions\nin a single concise description. As illustrated in\nFigure 1, the captions fail to spot the ‚Äúarea being\nstepped over‚Äù as the ‚Äúwater‚Äù, resulting in halluci-\nnated answers. Two primary concerns exist regard-\ning existing image-to-text conversion approaches\nfor VQA: (1) The converted textual descriptions\nmight be insufficient to solve the visual questions,\nor could contain misleading content; (2) Existing\nmethods convert images to text as a preprocessing\nstep of the input. This one-off conversion is a lossy\ncompression of the conveyed information, and does\nfully provoke the reasoning ability of LLMs.\nIn this paper, we present a framework where\nLLMs proactively interact with Vision-Language\nModels (VLMs) to gather specific information of\ninterest, as depicted in Figure 1. This interaction\nis aimed at automatically seeking and regaining\ndetails that may be omitted during the image-to-\ntext conversion. To enhance the informativeness of\nthe generated questions and the correctness of their\ncorresponding answers, we design a refinement\nmodule to summarize only the useful information\nfor generating the final answers. We validate our\napproach on OK-VQA and A-OKVQA datasets\nand conduct experiments across different LLMs.\nOur contributions are as follows:\n‚Ä¢ We design a model agnostic framework that al-\nlows LLMs to proactively interact with VLMs\nto unveil missing information.\n‚Ä¢ Our method can rectify inaccurate information\ngenerated during the image-to-text transforma-\ntion process and minimize the ambiguity of\nthe converted textual information.\n‚Ä¢ We achieve an average gain of 2.15% on OK-\nVQA over baselines, and attain consistent im-\nprovements across different LLMs.\n2 Related Work\n2.1 In-Context Learning for OK-VQA\nIn-context learning refers to the ability of LLMs\nto adapt to new tasks given only a few examples\nwithout parameter tuning (Brown et al., 2020). To\nexploit the knowledge possessed by LLMs and ex-\ntend this ability to OK-VQA, PICa (Yang et al.,\n2022) converts images into captions and tags, and\napplies in-context learning with GPT-3. A follow-\ning work, PromptCap (Hu et al., 2023a), fine-tunes\na question-guided captioning model, urging the tar-\nget messages to appear in the captions. Instead of\ndescribing an image by a single caption, Chen et al.\n(2023) represent an image as the combination of\nits regions in a Chain-of-Thought (CoT) style (Wei\net al., 2022). Prophet (Shao et al., 2023) argues that\nindistinct captions lead to aimless predicting, and\nproposes to provide answer candidates with cor-\nresponding confident scores as references. These\nmethods select in-context examples according to\nthe similarities between training and test instances.\nHowever, there are unexpected information loss\nduring the conversion from images to text. These\nmethods conduct a compressive one-time conver-\nsion to turn images into text, while we prompt the\nLLM to iteratively ask for detailed information.\nOur method is orthogonal to these approaches and\ncan continually improve their performances.\n2.2 New Question Generation for VQA\nIn VQA tasks, some questions are ambiguous and\nmight have different answers (Bhattacharya et al.,\n2019). Uehara et al. (2022) propose to gener-\nate new questions to assist the reasoning of the\noriginal questions. They train a visual question\ngenerator with supervision from human annotated\ndataset (Selvaraju et al., 2020). It is evaluated on\nVQA dataset (Antol et al., 2015), and is not ex-\ntensible to open-domain knowledge-based VQA.\nImg2prompt (Guo et al., 2023) describes a zero-\nshot approach on OK-VQA by generating an ex-\ntra new question for each instances to imitate the\nfew-shot setting without actually using multiple\nin-context examples. Its question generation proce-\ndure is irrelevant to the images. Instead of depend-\ning on human annotations, given the question and\n2875\n‚Ä¶\nIf it gets cold enough what will happen to the area being stepped over?\ncaptioning‚Ñ≥!\nùêº\nùëû\nVQA\n‚Ñ≥\"\nùíí‚Ä≤\n ùíÇ‚Ä≤\nWhat does the woman step over?What happens to the water when it gets cold?What is the temperature required for the area to change?‚Ä¶‚Ä¶\nwaterIt freezes0 degrees‚Ä¶‚Ä¶\nImage encoder\nFilter:\nùë∞\nùëûQuestion encoderInfo encoder\nùëû‚Ä≤# ùëé‚Ä≤# ùë†‚Ä≤#Water freezes when it gets coldùë†‚Ä≤!ùë†‚Ä≤\"\nThe women steps over the water.\nùëû‚Ä≤!,ùëé‚Ä≤!,ùë†‚Ä≤! ùëùùëüùëúùëè#\"#‚â•ùëùùëüùëúùëè#\n‚Ä¶\nAnswer the questions using the provided image infor-mation, captions and extra commonsense knowledge. \nùëÜ\nImage information: The women steps over the water; Water freezes when it gets cold; The area will change when‚Ä¶Question: If it gets cold enough what will happen to the area being stepped over?Answer: ùëÜ\nFrozen LLM\nFrozen LLM\nFrozen LLM\nfreeze\n(A) InquiryModule(B)Refinement Module(C) Answering Module\nImage information: the person is skiing; the person is wearing skis on their feet; cross country skiing is a popular activity while skiingQuestion: What is this person doing?Answer: cross country skiùë†$Caption: A person skiing on a snowy road\nImage information: the person is wearing skis; cross country skis are one of the equipment options for this activity; snow conditions ‚Ä¶Question: On what is this person traveling on?Answer: ski ùë†%Caption: A person skiing down a snowy hill\nCaption: A women on skies in the snow near a tree\nFigure 2: Our proposed framework consists of three modules. First, in the inquiry module (¬ß3.1), we prompt\nthe LLM to generate new questions for the missing image information required to answer the original question,\nand obtain answers from a VLM. Then, a refinement module (¬ß3.2) is adopted to summarize the questions and\nanswers, filtering and extracting useful information from them. Finally, in the answering module (¬ß3.3), the LLM\nis prompted to predict the final answer with the augmented image information.\nimage information, we directly prompt LLMs to\ngenerate new questions targeting the missing infor-\nmation. This is not constrained on pre-annotated\ndatasets and allows us to uncover more potential\ninformation and knowledge.\n2.3 Incorporating LLMs and VLMs for\nVision-Language Tasks\nSome concurrent works have also incorporated\nLLMs and VLMs for VL tasks. ChatCap-\ntioner (Zhu et al., 2023), for instance, aims to cap-\nture richer details from images by progressively\ngenerating questions with an LLM and then answer-\ning these questions using a VLM. The resulting an-\nswers are summarized to form image descriptions.\nHowever, this approach places a stronger emphasis\non the quantity of detected image details rather than\ntheir correctness. In some VL tasks, such as VQA,\nthis inevitably introduces noise, leading to inaccu-\nrate image descriptions that may result in incorrect\nmodel predictions. A VIS (Hu et al., 2023b) also\nexplores the decomposition of visual questions, but\nits primary focus lies in the planning procedures\nfor tool-use and their corresponding executions.\n3 Method\nOur overall framework, as illustrated in Figure 2,\ncomprises three key modules: inquiry, refinement\nand answering. Given the image caption and the\nquestion, the inquiry module prompts an LLM to\ngenerate new questions that seek for missing image\ninformation required to answer the original ques-\ntion, and uses a VLM to answer them based on the\nimage (¬ß3.1). Then, we adopt a refinement mod-\nule to summarize information from the generated\nquestions and answers, filtering and extracting the\nrelevant and useful information from them (¬ß3.2).\nFinally, the answering module prompts the LLM\nto predict the final answer to the original question\nwith the augmented image information (¬ß3.3).\nBefore delving into the details of our method,\nwe shall declare some important notations. We use\nI, c, q to denote the image, its caption, and the\noriginal question, respectively. The caption c is\ngenerated by the image captioning model Mc:\nc= Mc(I), (1)\nwhich is used as the preliminary image informa-\ntion presented to LLMs for question generation.\nThe generated new questions are denoted as q‚Ä≤ =\n[q‚Ä≤\n1; q‚Ä≤\n2; ... ; q‚Ä≤\nK], with the corresponding answers\nas a‚Ä≤ = [a‚Ä≤\n1; a‚Ä≤\n2; ... ; a‚Ä≤\nK], where K represents the\ntotal number of questions.\n3.1 Prompting LLM to Proactively Ask for\nInformation\nWe leverage the reasoning capabilities of the LLM\nto identify the essential image information that may\n2876\nbe lost during the image-to-text conversion pro-\ncess. Specifically, given the image caption cand\nthe original question q, we first prompt the LLM\nto generate Knew questions q‚Ä≤ that inquire about\nthe additional image information necessary to an-\nswer q. Suppose that the k-th question of q‚Ä≤ has\nL tokens, denoted as q‚Ä≤\nk = (y1\nk,y2\nk,...,y L\nk ), the\ndecoding process can be formulated as:\nyl\nk = arg max\nÀÜyl\nk\npLLM\n(\nÀÜyl\nk\n‚èê‚èê‚èêy<l\nk ; pq,q,c\n)\n, (2)\nwhere pq is the instruction prompt. The outline of\nthe prompt pq for LLM is as follows:\n/* Instruction for the decomposition task */\nPlease decompose the TARGET-QUESTION into K\nsub questions:\n/* n in-context examples */\nTARGET-QUESTION: q1 \\n Catpion: c1\nSub questions: 1. q‚Ä≤1\n1, 2. q‚Ä≤1\n2, ...\n......\nTARGET-QUESTION: q \\n Caption: c\nSub questions:\nThen, we employ a visual question answering\nmodel Ma to answer these new questions based on\nthe original image as:\na‚Ä≤\nk = Ma(q‚Ä≤\nk,I), (3)\nwhere a‚Ä≤\nk refers to the answer to the k-th question.\nTo better understand the role of the generated\nquestions and answers, we conducted a preliminary\nexperimental analysis. Specifically, we concatenate\neach question q‚Ä≤\nk to its answer a‚Ä≤\nk to form a QA pair\nq‚Ä≤\nka‚Ä≤\nk = [q‚Ä≤\nk; a‚Ä≤\nk], and prompt the LLM to answer\nOK-VQA questions given this representation. We\ninvestigate the results of prompting LLM with dif-\nferent contexts: 1) the original question qonly; 2)\nall the QA pairs; 3) one randomly selected QA pair\nand 4) the best-performing QA pair. For the last\ncase, for each question we calculate the accuracy\nscores when prompting with each QA pair, and then\nselect the maximum score among them, which rep-\nresents an upper bound on the performance. These\naccuracy scores are calculated by the soft scores\nfollowing Goyal et al. (2017).\nFrom the results presented in Table 1, we can\ndraw two key conclusions. First, the generated\nquestions and answers indeed contain information\nthat helps to answer the original question, compar-\ning the results of Best and Original. Second, the\ngenerated QA pairs are noisy, as neither using all\nQA pairs nor randomly selecting one improves the\nperformance. This highlights the necessity of an in-\nformation refinement process to filter out irrelevant\nor misleading information in the generated pairs.\nSelection Original All Random Best\nAccuracy 45.17 45.25 41.31 63.52\nTable 1: Preliminary experiments on the effects of the\ngenerated QA pairs. We report the average accuracy\nscores of prompting the LLM to answer each OK-VQA\nquestion with: (1) ‚ÄúOriginal‚Äù: the original question q\nonly; (2) ‚ÄúALL‚Äù: all the QA pairs; (3) ‚ÄúRandom‚Äù: one\nrandomly selected QA pair. ‚ÄúBest‚Äù refers to the best-\nperforming QA pair (i.e., the upper bound).\n3.2 Refining the Generated Information\nInspired by the preliminary experiments in ¬ß3.1, we\ndesign a refinement module that can extract useful\ninformation for the LLM to answer the original\nquestion from the noisy generated QA pairs. Our\nrefinement module includes a summarization stage\nand a filtering stage.\nFirstly, we summarize q‚Ä≤\nk and the corresponding\nanswer a‚Ä≤\nk into a narrative descriptions‚Ä≤\nk. Denoting\ns‚Ä≤\nk as a L-token target sequence, we have:\ns‚Ä≤l\nk = arg max\nÀÜs‚Ä≤l\nk\npLLM\n(\nÀÜs‚Ä≤l\nk\n‚èê‚èê‚èês‚Ä≤<l\nk ; ps,q‚Ä≤\nk,a‚Ä≤\nk\n)\n, (4)\nwhere lis the l-th token of the summary s‚Ä≤\nk and ps\nis the instruction prompt. The complete templates\nused in this section are listed in Appendix F.\nThen, we apply a filter to assess the helpfulness\nof summary s‚Ä≤\nk to the final prediction. The output\nis a contribution score of s‚Ä≤\nk given the image I,\nthe question q, and different combinations of text\ninputs including question q‚Ä≤\nk, answer a‚Ä≤\nk, summary\ns‚Ä≤\nk and question-answer pair [q‚Ä≤\nk; a‚Ä≤\nk].\nSpecifically, our refinement module consists of\ntwo types of encoders, text encoder Enct and im-\nage encoder Encv, and a 3-layer multilayer per-\nceptron (MLP). We use the pre-trained weights of\nCLIP (Radford et al., 2021) to initialize our text\nand image encoders. Given I, q and s‚Ä≤\nk, we first\ngenerate the visual features hvisual and the textual\nfeatures hk\ntext as:\nhvisual = Encv(I), (5)\nht = Enct(t),t = {q,q‚Ä≤\nk,a‚Ä≤\nk,q‚Ä≤\nka‚Ä≤\nk,s‚Ä≤\nk}, (6)\nhk\ntext = Avg(ht={q‚Ä≤\nk,a‚Ä≤\nk,q‚Ä≤\nka‚Ä≤\nk,s‚Ä≤\nk},ht=q), (7)\nwhere hk\ntext is the average of the features of each\nkind of textual inputs. Then we calculate the fused\nfeatures of the image I and the summary s‚Ä≤\nk as:\nzk = MLP([hk\ntext; hvisual]). (8)\n2877\nMethod LLM Image Information OK-VQA A-OKVQA\nResults with accessible LLMs\nPICa davinci Captions (G) + Tags 48.0* -\nPICa‚Ä† text-davinci-002 Captions (G) + Tags 49.67 49.18\n+ Ours Captions (G) + Tags + refined details 53.76 +4.09 51.13 +1.95\nPromptCap‚Ä† text-davinci-002 Captions (Q) 53.50 52.99\n+ Ours Captions (Q) + refined details 54.42 +0.92 53.21 +0.22\nProphet text-davinci-002 Caption (G) + Candidates 57.91 58.2*\n+ Ours Caption (G) + Candidates + refined details 59.34 +1.43 59.79* +1.59\nResults with unavailable LLMs\nPromptCap code-davinci-002 Captions (Q) 60.4 56.3\nTable 2: Direct answer accuracy on OK-VQA (test) and A-OKVQA (val). We use ‚Ä† to denote our reproduced\nversions, because PromptCap and PICa use different GPT-3 engines in their paper. PICa uses davinci; PromptCap\nuses code-davinci-002, which is deprecated. * refers to multi-query ensemble because the single query results of\nProphet is not reported by Shao et al. (2023). The Captions (G) refers to the caption generated by general-purpose\nVLP, such as VinVL. The Captions (Q) refers to the PromptCap caption. Candidates refers to answers candidates\npredicted by VQA model. Refined details refer to our regained information.\nTo optimize the filter, we directly use the ground-\ntruth VQA accuracy yzk of each training instance\n(I,q,s ‚Ä≤k,yzk ) in OK-VQA as an intermediate su-\npervision signal. The final loss is formulated as:\nL= ‚àí[yzk log(pzk )+(1 ‚àíyzk ) log(1‚àípzk )], (9)\nwhere pzk = œÉ(zk) is the contribution score of\ns‚Ä≤\nk for answering q given I. Please refer to Ap-\npendix A for detailed data construction process.\nDuring inference, we exploit the refinement mod-\nule to generate the refined information S. We first\ncalculate the contribution score of the original ques-\ntion qas pzq with the trained filter. Then we select\nall the summaries s‚Ä≤\nk that have larger contribution\nscores pzk than pzq to form the refined information\nset Sas:\nS = {pzk |pzk ‚©æ pzq }k=1,2,3,...,K. (10)\n3.3 Answer Reasoning\nThe answering module prompts a frozen LLM\nto predict the final answer with both the con-\nverted image information such as the caption c\nand our regained S. For the prompt template,\nwe mostly follow PICa (Yang et al., 2022) as the\nother methods do, but add a new section for our\nrefined image information as follows (denoted as\nTemplate-Few-shot):\n/* Template-Few-shot */\nImage information: Sn\nCaption: Cn\\n Question: qn\\n Answer: an\nwhere an is the ground-truth answer of question\nqn and n refers to the number of shots used for\nin-context learning. We denote the template for the\ntest data as Template-Query, which is as follows:\n/* Template-Query */\nImage information: S\nCaption: C\\n Question: q\\n Answer:\nPlease refer to Appendix F for the complete prompt\ntemplates with instructions.\n4 Experiments\nWe validate our method on open-domain\nknowledge-base VQA, and conduct experiments\non OK-VQA and A-OKVQA. Implementation\ndetails are described in the following sections.\n4.1 Dataset\nOK-VQA (Marino et al., 2019) is an open-domain\nknowledge-based VQA dataset with about 9k train-\ning and 5k testing questions. The images come\nfrom MSCOCO (Lin et al., 2014). The annotated\nquestions are all open-ended, and each of them\nis associated with 10 groundtruth answers. Due\nto the deprecation of dataset v1.0, we use v1.1\nin this paper. A-OKVQA (Schwenk et al., 2022)\naugments OK-VQA by the scale, tasks and extra\nrationales. This dataset has are three splits, training\n(17k), validation (1.4K) and test (6.7k). A-OKVQA\nincludes two tasks, direct answer (DA) and mul-\ntiple choices (MC). The DA tasks is the same as\nOK-VQA, which requires to answer open-ended\n2878\nquestions with external knowledge. While the MC\ntask asks to choose an answer from a close set with\n4 choices. We focus on the open-domain setting\nand evaluate our method OK-VQA and the DA task\nof A-OKVQA. Both datasets employ the soft accu-\nracy (Antol et al., 2015) as the evaluation metric.\n4.2 Experimental Settings\nWe generate 3 new questions for each q, and apply\nthe ensemble filters to refine the generated informa-\ntion. The ensemble filters contains all 4 types of\ninputs, q‚Ä≤\nk, a‚Ä≤\nk, s‚Ä≤\nk and [q‚Ä≤\nk; a‚Ä≤\nk].\nBaseline methods. We apply our method upon\nexisting approaches of the same in-context learn-\ning paradigm, including PICa (Yang et al., 2022),\nPromptCap (Hu et al., 2023a) and Prophet (Shao\net al., 2023). The image information of PICa\ncomes from captions and tags (Microsoft Azure\ntagging API 2). PromptCap follows similar set-\ntings, but replaces the captioning model by its fine-\ntuned question-aware caption model. Apart from\nthe captions, Prophet supplies LLM with extra\nanswer candidates obtained from their fine-tuned\nVQA models.\nThe best results of PICa and Prophet is reported\non the 16-shot and 20-shot settings, respectively.\nThey employ multi-query ensemble to further en-\nhance the performance, where they prompt the\nLLM for 5 times with different in-context examples.\nWe implement our method upon these baselines fol-\nlowing the same settings, where the number of shot\nis 16 for PICa+Ours and PromptCap+Ours, and 20\nfor Prophet+Ours.\nLLMs. For answer reasoning, the three base-\nlines employ different LLMs. Prophet employs\nthe LLM engine text-davinci-002, which is an\nInstructGPT model (Ouyang et al., 2022). PICa\nuses davinci, a GPT-3 model (Brown et al., 2020).\nPromptCap uses code-davinci-002, but is now\ndeprecated and the authors suggest to replace it\nby text-davinci-002 in the published code and\nmodel3. Considering the accessibility of LLMs\nand for fair comparisons, we employ the LLM\nengine used in Prophet (text-davinci-002) and\nreproduce the other two using the same engine.\nThe gpt-3.5-turbo-0301 engine is employed for\nquestion generation and summarization.\n2Azure Computer Vision API v3.2 for tagging:\nhttps://westus.dev.cognitive.microsoft.com/docs/\nservices/computer-vision-v3-2\n3https://github.com/Yushi-Hu/PromptCap\nMethod OK-VQA A-OKVQA\nMultimodal tuning\nLXMERT - 30.7\nBLIP-2 (FlanT5-XL) 40.7 -\nVLC-BERT 43.1 45.0\nGPV-2 - 48.6\nFlamingo (80B) 57.8 -\nInstructBLIP (Vicuna-7B) 62.1 64.0\nPaLM-E (562B) 66.1 -\nMethods querying external KBs (w/o LLMs)\nKRISP 38.9 33.7\nRA-VQA 54.5 -\nREVEAL 59.1 52.2\nMethods with LLMs\nImg2Prompt175B 45.6 42.9\nPICa-Full 48.0 -\nTRiG 50.5 -\nKAT (ensemble) 54.4 -\nREVIVE 58.0 -\nassistGPT - 44.3\nPromptCap 60.4 59.6\nProphet 57.9 -\n+ Ours 59.3 +1.4 57.9\nProphet (ensemble) 61.1 58.2\n+ Ours (ensemble) 61.3 +0.2 59.8 +1.59\nTable 3: Comparisons to the previous methods of on\nOK-VQA (test) and A-OKVQA (val). The highest ac-\ncuracy of methods using LLMs are bolded. The sota\nof the other two scopes are underlined. Please refer to\nAppandix C for the full results.\nVLMs. We use BLIP-2 (Li et al., 2023) to predict\nanswer a‚Ä≤\nk for q‚Ä≤\nk. The choice of VLMs for obtain-\ning caption Cvaries alone baselines. Specifically,\nPICa and Prophet uses VinVL (Zhang et al., 2021)\nto convert images into captions, and PromptCap\nfine-tunes a question-aware caption model based\non OFA (Wang et al., 2022).\n4.3 Results\nTable 2 shows the results on OK-VQA and A-\nOKVQA. With the accessible LLMs, we achieve\nthe highest accuracy of 59.34% under single\nquery setting, and enhance the accuracy of\nbaselines by 2.15% on average. PromptCap\nachieves an accuracy of 60.45% on OK-VQA\nwith code-davinci-002 engine, which is cur-\nrently inaccessible. While for the reproduction\nwith text-davinci-002, we improve the perfor-\nmance of PromptCap by 0.9%. For A-OKVQA,\nwe consistently observe improvements, and the av-\nerage increment is1.25%.\nTable 3 lists the results of previous methods on\nOK-VQA and A-OKVQA, where we report our en-\nsemble results in accordance to Prophet. Within the\nrealm of methods using LLMs, we further improve\nthe best result by 0.2% for both dataset. Specifi-\n2879\nMethods n-shot Caption Image Info. Tag Refine- Q Refine-E ACC\nOurs\n16 PromptCap Refined information ‚úì ‚úì ‚úì 54.42 (a)\n16 PromptCap Refined information ‚úì ‚úì - 52.25 (b)\n4 PromptCap Refined information ‚úì ‚úì - 50.19 (c)\n4 OFA-large Refined information ‚úì ‚úì - 48.43 (d)\n4 OFA-large Refined information - ‚úì - 47.63 (e)\nOurs w/o refinement 4 OFA-large All the questions + BLIP-2 - - - 45.25 (f)\n4 OFA-large Original questions + BLIP-2 - - - 45.17 (g)\nOurs w/o BLIP-2 4 OFA-large Original questions - - - 44.86 (h)\nBLIP-2 0 - - - - - 40.70 (i)\nTable 4: The ablation study of how different information affect the reasoning performance on OK-VQA. Ours: our\nproposed pipeline. Caption: the models used to generate the image captions. Image Info.: the resources of our\nimage information, the ‚ÄúBLIP-2‚Äù in this column refers to the BLIP-2 answers, ‚ÄúRefined information‚Äù = applying\nrefining on ‚ÄúAll the questions + BLIP-2 answers‚Äù. Tag: whether to use image tags. Refine-Q: applying refining to\nthe ‚ÄúImage information S‚Äù in Template-Query in ¬ß3.3. Refine-E: applying refining to the ‚ÄúImage information Sn‚Äù\nin Template-Few-shot(nin-context examples) in ¬ß3.3. Line (i) refers to directly using BLIP-2 for OK-VQA and the\nresult is taken from Li et al. (2023). Please refer to ¬ß4.4 for detailed explanation and comparison.\nScheme Accuracy\nEnsemble 59.34\nSingle-a 58.53 -0.81\nSingle-s 58.48 -0.86\nSingle-qa 58.50 -0.84\nSingle-q 58.10 -1.24\nAll 58.03 -1.31\n(a)\nT Accuracy\nOurs Prophet\nT=1 59.3 57.9\nT=2 60.5 -\nT=3 61.1 -\nT=4 61.2 -\nT=5 61.3 61.1\n(b)\nTable 5: Ablation studies on OK-VQA. Results of our\nmethod in the two table refer to Prophet+Ours with\n20-shot setting. (a): Comparison of refining schemes.\nEnsemble: 4-model ensemble. Single: -a/-s/-qa/-q re-\nfer to filter model trained using a‚Ä≤, s‚Ä≤, q‚Ä≤a‚Ä≤ and q‚Ä≤\nrespectively. All: using all information (s + s‚Ä≤) without\nrefining. (b): Results of varying the number (T) of en-\nsemble queries. T=1: no ensemble.\ncally, we achieve +1.59% on A-OKVQA compared\nto Prophet. For A-OKVQA, PromptCap achieves\nthe previous highest accuracy, 59.6%. But we can-\nnot directly apply our method upon it because its\nemployed LLM engine is inaccessible now. For\nmultimodal tuning setting, PaLM-E (Driess et al.,\n2023) and InstructBLIP (Dai et al., 2023) present\nthe state of the art on OK-VQA and A-OKVQA, re-\nspectively. While PaLM-E is trained upon a 540B\nlanguage model, PaLM (Chowdhery et al., 2022), it\nis over threefold of the largest LLM we use (175B).\nAnd InstructBLIP employs instruction tuning.\n4.4 Ablation Study\nWe conduct the following ablation studies: 1) to\nverify that the gain of our method comes from\nproperly integrating more image information; 2)\nto compare the effectiveness of different selection\nschemes in refinement module; 3) to investigate\nthe impact of scaling up the number of generated\nMethod LLM Accuracy\nProphet\nProphet text-davinci-002 57.52\n+ Ours 58.54 +1.02\nProphet LLaMA-13B 50.76\n+ Ours 53.08 +2.32\nProphet LLaMA-7B 44.28\n+ Ours 49.47 +5.19\nPromptCap\nPromptCap text-davinci-002 53.50\n+ Ours 54.42 +0.92\nPromptCap LLaMA-13B 47.45\n+ Ours 48.72 +1.27\nPromptCap LLaMA-7B 44.37\n+ Ours 44.59 +0.22\nPICa\nPICa text-davinci-002 49.67\n+ Ours 53.76 +4.09\nPICa LLaMA-13B 42.96\n+ Ours 46.28 +3.32\nPICa LLaMA-7B 39.20\n+ Ours 42.68 +3.48\nTable 6: Results our method with different LLMs on\nOK-VQA. We use 16-shot for all methods in this table\nas a input of 16-shot is hitting the maximum input length\nof LLaMA.\nquestions; 4) to analyse the impact of multi-query\nensemble; and 5) to examine the consistency of our\nmethod across different LLMs.\nAppropriately integrated information helps to\nboost the performance. We summarise the\nparadigm to solve VQA task into two categories.\nA VLM paradigm that directly uses a VLM (e.g.,\nBLIP-2 in Line (i) in Table 4), and an LLM-based\nparadigm where LLMs collaborate with VLMs to\nanswer visual questions. Following the LLM-based\nparadigm, we progressively integrate new informa-\ntion and apply refining methods from Line (h) to (a)\nin Table 4, and observe continual enhancements.\n2880\nFigure 3: Cases compared to Prophet and PromptCap without applying our method. The frames titled by ‚ÄúPrompt-\nCap‚Äù/‚ÄúProphet‚Äù depict the results given by these two baselines in our reproduced version. The information leading\nto incorrect answers are marked in red.\nLine (h) represents results for 4-shot setting fol-\nlowing the PICa template. While Line (g) imple-\nments our template, Template-Few-shotwith n= 4.\nThe BLIP-2 answers are included in ‚ÄúImage infor-\nmation‚Äù in Template-Few-shotand Template-Query.\nThe participation of BLIP-2 raises the accuracy by\n0.31%. When we feed all generated information\n(Line (f)), the accuracy only marginally improves\nby 0.07%. Notably, applying the refinement mod-\nule to the query instance (Line (e)) results in a sig-\nnificant 2.38% accuracy boost. Line (d) introduces\nimage tags and contributes an additional 0.8% im-\nprovement compared to Line (e). Extending to\n16-shot setting further boosts the result by 2.06%.\nWhile the previous lines only apply information\nrefining to the query instance, we go further in\nLine (a) by refining the in-context examples. This\ncontributes to an increment of 2.17% compared to\nLine (b), and is more influential than adding tags,\nincreasing the number of shots and replacing the\ncaption type.\nIn conclusive, the performance benefits from\nadding more image information. Our method to\nseek and refine image information makes a substan-\ntial contribution in this regard.\nEffectiveness of filtering schemes. We evaluate\nthe performance of different selection schemes in\nthe refinement module, including (1) Ensemble: se-\nlecting according to ensemble filters as proposed in\n¬ß3.2, (2) Single: selecting according to single filter,\nincluding filters trained using answers, question,\nsummaries, and question-answer pairs (qa), and (3)\nAll: selecting all generated questions q‚Ä≤ and the\noriginal question qwithout filtering.\nAs shown in Table 5 (a), filters help improve\nthe performance against directly selecting all the\nquestions without filtering. However, single filters\nmake only minor contributions to the final accuracy\nbecause each of them addresses only a single aspect\nof the criteria.\nScaling up the number of generated questions.\nIntuitively, scaling the amount of generated ques-\ntions will contribute to extra gain. To verify this,\nwe doubled the number of generated questions to\n6. By applying the same selection strategy under\n20-shot single query setting, we obtain a final ac-\ncuracy of 59.68% on OK-VQA, which is slightly\nhigher (+0.34%) than generating only 3 questions.\nImpact of multi-query ensemble. PICa and\nProphet exhibit improvements of multi-query en-\nsemble by prompting LLM for 5 times with differ-\nent in-context examples. We investigate the influ-\nence of multi-query ensemble on our method. As\nshown in Table 5 (b), although the accuracy of our\nmethod increases along with the number of ensem-\nble queries, the gap between ours and Prophet‚Äôs\nare narrowed. As the in-context examples are ar-\nranged according to the relevance to test instance,\nthe more examples we use, the less coherent to the\ntest instance they will be. Thereby, noises could\nbe introduced with the progressive increase of en-\nsemble queries. Similarly, Chen et al. (2023) also\nobserve a decline in performance when continu-\nously increase the number of ensemble queries.\nConsistency of our method across LLMs. Dif-\nferent LLMs largely affect the results (Shao\net al., 2023; Hu et al., 2023a). We investi-\n2881\ngate if our method is generalizable across LLMs\ntrained in different ways and with different\nscales, including LLaMA-7B, LLaMA-13B, and\ntext-davinci-002 (175B). Table 6 proves the ef-\nfectiveness of our method on both InstructGPT\n(text-davinci-002 engine) and LLaMA. Results\nalso demonstrate the robustness of our method on\nthe scales of LLM, ranging from 7B to 175B. We\nnotice that our method introduce less improvement\non PromptCap compared to Prophet and PICa. We\nsuppose that the captions of PromptCap are de-\nrived from the question, and they seem to be more\ndeterminate as shown in Figure 3, which easily\ndominates the reasoning and impair the attendance\nof other information.\n5 Case Study\nWe conduct comprehensive analysis of the cases,\nand observe that our method exhibits the following\ncapabilities: 1) to unveil a better level of detail as\nstated in the ¬ß3.1; 2) to rectify inaccurate caption\ninformation; and 3) to minimize the ambiguity of\nprovided information, e.g., captions and candidates.\nCases are demonstrated in Figure 3. We compared\nthe results of baselines, PtomptCap and Prophet,\nwith and without applying our method.\nUnveiling Missing Details. Our generated ques-\ntions bridge the information gap between the ques-\ntion and the image caption by revealing missing\ndetails necessary to answer the question. An shown\nin (2) of Figure 3, the question asks about the ‚Äúkind\nof glass‚Äù, but relevant features are not included\nin the PromptCap caption. The absence of detail\nleads to an improper prediction, ‚Äúfrosted‚Äù. How-\never, the two questions in our method pinpoint the\ndetailed features, ‚Äútransparent‚Äù and ‚Äúclear‚Äù, and\ncontribute to a correct prediction. These imply the\neffectiveness of our generated questions.\nRectifying Inaccurate Information. Our method\ncan rectify the misleading information provided in\nthe captions. In the first case shown in Figure 3,\nwe correct the wrong message given by PromptCap\nthat it is not a ‚Äúchicken‚Äù sandwich by the question\n‚Äúwhat is in the sandwich‚Äù with answer ‚Äúham‚Äù.\nMinimizing the Ambiguity. By engaging more\nimage information, our method provides evidence\nto support the correct candidates in Prophet and\nthe proper captions of PromptCap, thereby enhanc-\ning the confidence and the reliability of accurately\nprovided information in these baselines. In Fig-\nure 3, the candidates of Prophet in (1) properly\nfit the image. However, the LLM does not follow\nthe given confidence and selects the least confident\none. In contrast, Figure 3 (2) demonstrates a situa-\ntion where the most confident candidate is not the\ncorrect answer. In this two scenarios, our method\nsupports the correct answer with more detailed in-\nformation.\n6 Conclusion\nIn this paper, we focus on open-domain knowledge-\nbased VQA and propose a model agnostic frame-\nwork that successfully unveils missing detail during\nthe image-to-text transformation. Our method ac-\nquires the ability to rectify inaccurate information\ngenerated by captioning models, and the ability\nto minimize the ambiguity of the converted tex-\ntual information for further improvements. Our\nmethod can be applied upon existing baselines, and\nachieves average gains of 2.15% on OK-VQA and\n1.25% on A-OKVQA over the baselines. Abla-\ntion studies show that our method attain consistent\nimprovements across different LLMs.\nLimitations\nIn this work, we demonstrate the effectiveness of\nour framework on OK-VQA and A-OKVQA, and\nshow a consistent improvement across different\nLLMs. However, we do not verify the feasibility of\nour idea on other vision-language tasks that also re-\nquire knowledge, such as visual commonsense rea-\nsoning. Intuitively, the paradigm to prompt LLMs\nto uncover missing image details can be applied to\na wild range of VL tasks. While the questions in\nour framework are generated independently, further\nchallenges include to progressively ask informative\nquestion upon the previous questions and acquire\nthe accurate answers. We hope that our work will\nencourage further investigations that explore the\ncapabilities of LLMs in the VL realm.\nAcknowledgement\nThis work is supported by the National Key R&D\nProgram of China (2022ZD0160502) and the Na-\ntional Natural Science Foundation of China (No.\n61925601, 62276152). We thank Siyu Wang for\nher participation in this work, and appreciate all\nthe reviewers for their insightful suggestions.\n2882\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, et al. 2022. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural\nInformation Processing Systems, 35:23716‚Äì23736.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015. VQA: visual question an-\nswering. In 2015 IEEE International Conference\non Computer Vision, ICCV 2015, Santiago, Chile,\nDecember 7-13, 2015, pages 2425‚Äì2433. IEEE Com-\nputer Society.\nNilavra Bhattacharya, Qing Li, and Danna Gurari. 2019.\nWhy does a visual question have different answers?\nIn 2019 IEEE/CVF International Conference on Com-\nputer Vision, ICCV 2019, Seoul, Korea (South), Octo-\nber 27 - November 2, 2019, pages 4270‚Äì4279. IEEE.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-\ngiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas\nBeyer, et al. 2022. PaLI: A jointly-scaled mul-\ntilingual language-image model. ArXiv preprint ,\nabs/2209.06794.\nZhenfang Chen, Qinhong Zhou, Yikang Shen, Yining\nHong, Hao Zhang, and Chuang Gan. 2023. See,\nThink, Confirm: Interactive prompting between vi-\nsion and language models for knowledge-based vi-\nsual reasoning. ArXiv preprint, abs/2301.05226.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinod-\nkumar Prabhakaran, Emily Reif, Nan Du, Benton C.\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garc√≠a,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Mor-\neira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark D√≠az, Orhan Firat, Michele Catasta, Jason Wei,\nKathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean,\nSlav Petrov, and Noah Fiedel. 2022. PaLM: Scaling\nlanguage modeling with pathways. ArXiv preprint,\nabs/2204.02311.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi.\n2023. InstructBLIP: Towards general-purpose vision-\nlanguage models with instruction tuning. ArXiv\npreprint, abs/2305.06500.\nDanny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe\nYu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-\nmanet, Daniel Duckworth, Sergey Levine, Vincent\nVanhoucke, Karol Hausman, Marc Toussaint, Klaus\nGreff, Andy Zeng, Igor Mordatch, and Pete Florence.\n2023. PaLM-E: An embodied multimodal language\nmodel. ArXiv preprint, abs/2303.03378.\nYifan Du, Junyi Li, Tianyi Tang, Wayne Xin Zhao,\nand Ji-Rong Wen. 2023. Zero-shot visual question\nanswering with language model feedback. ArXiv\npreprint, abs/2305.17006.\nDifei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin,\nJoya Chen, Zihan Fan, and Mike Zheng Shou. 2023.\nAssistGPT: A general multi-modal assistant that can\nplan, execute, inspect, and learn. ArXiv preprint,\nabs/2306.08640.\nFeng Gao, Qing Ping, Govind Thattai, Aishwarya N.\nReganti, Ying Nian Wu, and Prem Natarajan. 2022.\nTransform-Retrieve-Generate: Natural language-\ncentric outside-knowledge visual question answering.\nIn IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2022, New Orleans, LA,\nUSA, June 18-24, 2022, pages 5057‚Äì5067. IEEE.\nFran√ßois Gard√®res, Maryam Ziaeefard, Baptiste Abe-\nloos, and Freddy Lecue. 2020. ConceptBert:\nConcept-aware representation for visual question an-\nswering. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, pages 489‚Äì498,\nOnline. Association for Computational Linguistics.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the V in VQA\nmatter: Elevating the role of image understanding\nin visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 6904‚Äì6913.\nLiangke Gui, Borui Wang, Qiuyuan Huang, Alexan-\nder Hauptmann, Yonatan Bisk, and Jianfeng Gao.\n2022. KAT: A knowledge augmented transformer\n2883\nfor vision-and-language. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 956‚Äì968, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nJiaxian Guo, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Boyang Li, Dacheng Tao, and\nSteven Hoi. 2023. From images to textual prompts:\nZero-shot visual question answering with frozen\nlarge language models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 10867‚Äì10877.\nYushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi,\nNoah A Smith, and Jiebo Luo. 2023a. PromptCap:\nPrompt-guided task-aware image captioning. ArXiv\npreprint, abs/2211.09699.\nZiniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang,\nYizhou Sun, David A Ross, Cordelia Schmid, and\nAlireza Fathi. 2023b. A VIS: Autonomous visual in-\nformation seeking with large language models.\nZiniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-\nWei Chang, Yizhou Sun, Cordelia Schmid, David A\nRoss, and Alireza Fathi. 2022. REVEAL: Retrieval-\naugmented visual-language pre-training with multi-\nsource multimodal knowledge memory. ArXiv\npreprint, abs/2212.05221.\nEhsan Kamalloo, Nouha Dziri, Charles LA Clarke, and\nDavood Rafiei. 2023. Evaluating open-domain ques-\ntion answering in the era of large language models.\nArXiv preprint, abs/2305.06984.\nJunlong Li, Zhuosheng Zhang, and Hai Zhao. 2022.\nSelf-prompting large language models for open-\ndomain QA. ArXiv preprint, abs/2212.08635.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023. BLIP-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. ArXiv preprint, abs/2301.12597.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,\nand C Lawrence Zitnick. 2014. Microsoft COCO:\nCommon objects in context. In Computer Vision‚Äì\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740‚Äì755. Springer.\nWeizhe Lin and Bill Byrne. 2022. Retrieval augmented\nvisual question answering with outside knowledge.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n11238‚Äì11254, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nYuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu,\nChenguang Zhu, and Lu Yuan. 2022. REVIVE: Re-\ngional visual representation matters in knowledge-\nbased visual question answering. In Advances in\nNeural Information Processing Systems.\nMan Luo, Yankai Zeng, Pratyay Banerjee, and Chitta\nBaral. 2021. Weakly-supervised visual-retriever-\nreader for knowledge-based question answering. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing , pages\n6417‚Äì6431.\nKenneth Marino, Xinlei Chen, Devi Parikh, Abhinav\nGupta, and Marcus Rohrbach. 2021. KRISP: inte-\ngrating implicit and symbolic knowledge for open-\ndomain knowledge-based VQA. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2021, virtual, June 19-25, 2021, pages 14111‚Äì14121.\nComputer Vision Foundation / IEEE.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. OK-VQA: A visual\nquestion answering benchmark requiring external\nknowledge. In IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2019, Long Beach,\nCA, USA, June 16-20, 2019, pages 3195‚Äì3204. Com-\nputer Vision Foundation / IEEE.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730‚Äì27744.\nVivek S. Pai, Peter Druschel, and Willy Zwaenepoel.\n2000. IO-Lite: A unified I/O buffering and caching\nsystem. Acm Transactions on Computer Systems ,\n18(1):37‚Äì66.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research , pages 8748‚Äì8763.\nPMLR.\nSahithya Ravi, Aditya Chinchure, Leonid Sigal, Renjie\nLiao, and Vered Shwartz. 2023. VLC-BERT: Visual\nquestion answering with contextualized common-\nsense knowledge. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vi-\nsion (WACV), pages 1155‚Äì1165.\nDustin Schwenk, Apoorv Khandelwal, Christopher\nClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.\nA-OKVQA: a benchmark for visual question answer-\ning using world knowledge. In Computer Vision‚Äì\nECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23‚Äì27, 2022, Proceedings, Part VIII,\npages 146‚Äì162. Springer.\nRamprasaath R. Selvaraju, Purva Tendulkar, Devi\nParikh, Eric Horvitz, Marco T√∫lio Ribeiro, Be-\nsmira Nushi, and Ece Kamar. 2020. SQuINTing\nat VQA models: Introspecting VQA models with\n2884\nsub-questions. In 2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR\n2020, Seattle, WA, USA, June 13-19, 2020 , pages\n10000‚Äì10008. IEEE.\nZhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. 2023.\nPrompting large language models with answer heuris-\ntics for knowledge-based visual question answering.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 14974‚Äì\n14983.\nAlexandre Tamborrino, Nicola Pellican√≤, Baptiste Pan-\nnier, Pascal V oitot, and Louise Naudin. 2020. Pre-\ntraining is (almost) all you need: An application to\ncommonsense reasoning. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3878‚Äì3887, Online. Association\nfor Computational Linguistics.\nKohei Uehara, Nan Duan, and Tatsuya Harada. 2022.\nLearning to ask informative sub-questions for visual\nquestion answering. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops,\nCVPR Workshops 2022, New Orleans, LA, USA, June\n19-20, 2022, pages 4680‚Äì4689. IEEE.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022. OFA: unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In Inter-\nnational Conference on Machine Learning, ICML\n2022, 17-23 July 2022, Baltimore, Maryland, USA,\nvolume 162 of Proceedings of Machine Learning\nResearch, pages 23318‚Äì23340. PMLR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-Thought prompting elicits rea-\nsoning in large language models. In Advances in\nNeural Information Processing Systems.\nJialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe\nGan, Zicheng Liu, Junsong Yuan, and Lijuan Wang.\n2022a. GRiT: A generative region-to-text trans-\nformer for object understanding. arXiv preprint\narXiv:2212.00280.\nJialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh\nMottaghi. 2022b. Multi-modal answer validation\nfor knowledge-based VQA. In Thirty-Sixth AAAI\nConference on Artificial Intelligence, AAAI 2022,\nThirty-Fourth Conference on Innovative Applications\nof Artificial Intelligence, IAAI 2022, The Twelveth\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2022 Virtual Event, February 22 -\nMarch 1, 2022, pages 2712‚Äì2721. AAAI Press.\nJialin Wu and Raymond Mooney. 2022. Entity-focused\ndense passage retrieval for outside-knowledge visual\nquestion answering. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 8061‚Äì8072, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,\nYumao Lu, Zicheng Liu, and Lijuan Wang. 2022. An\nempirical study of GPT-3 for few-shot knowledge-\nbased VQA. In Thirty-Sixth AAAI Conference on\nArtificial Intelligence, AAAI 2022, Thirty-Fourth Con-\nference on Innovative Applications of Artificial In-\ntelligence, IAAI 2022, The Twelveth Symposium on\nEducational Advances in Artificial Intelligence, EAAI\n2022 Virtual Event, February 22 - March 1, 2022 ,\npages 3081‚Äì3089. AAAI Press.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\nfeng Gao. 2021. VinVL: Revisiting visual representa-\ntions in vision-language models. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2021, virtual, June 19-25, 2021, pages 5579‚Äì5588.\nComputer Vision Foundation / IEEE.\nDeyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian\nShen, Wenxuan Zhang, and Mohamed Elhoseiny.\n2023. ChatGPT asks, BLIP-2 answers: Automatic\nquestioning towards enriched visual descriptions.\narXiv preprint arXiv:2303.06594.\n2885\nA Details for Gathering Training Data for\nRefinement Module\nThe process for constructing the training data for\nrefinement module as described in ¬ß3.2. We denote\nour generated image information as a set I, con-\ntaining the four types of generated image informa-\ntion, q‚Ä≤, a‚Ä≤, q‚Ä≤a‚Ä≤ and s‚Ä≤. Q is the set of all the origi-\nnal visual questions, andI refers the corresponding\nimages. yz is resulting training labels used in ¬ß3.2,\nwhere yzk refers to the label for a single generated\ninformation. The soft accuracy score Accsoft (a) is\ncomputed following Goyal et al. (2017).\nAlgorithm 1 Pipeline for Supervision Gathering\nInput: Image, Question and Generated Image In-\nformation {I,Q,I}\nOutput: Image Information ( I), Original Ques-\ntions (Q) and Images (I) with labels (yz)\nRequire: Pi,q is the prompt for answer reasoning\nregarding question qand image i.\n1: procedure GET LABELS (yz)\n2: for q‚ààQand i‚ààI and I‚àà I do\n3: a‚ÜêLLMreason(Pi,q(I))\n4: accq ‚ÜêAccsoft (a)\n5: if accq >0 then\n6: yz ‚Üê1\n7: else\n8: yz ‚Üê0\nB The Declaration of Trainable Modules\nin Our Pipeline\nThese models are frozen in our entire pipeline: the\ncaptioning model Mc, the VQA model Ma and\nthe LLMs (for question generation, summarization\nand reasoning).\nThe refinement module in Figure 2 requires train-\ning, which includes 4 parts:\n‚Ä¢ An image encoder (for image): to encode the\noriginal VQA image;\n‚Ä¢ A question encoder (for text): to encodes the\noriginal VQA question;\n‚Ä¢ An information encoder (for text): to encode\nour generated image information (denoted as\n‚ÄúInfo encoder‚Äù in Figure 1);\n‚Ä¢ A filter: an MLP-based network that evaluates\nthe helpfulness of the obtained image informa-\ntion towards answering the visual questions.\nC Full List of Results\nWe provide results of previous methods on OK-\nVQA and A-OKVQA in Table 7 and Table 8. The\ncurrent state-of-the-art on OK-VQA is 66.1 and\nis contributed by a 562B pre-trained multimodal\nmodel, including a 540B language model and a\n22B vision encoder. We achieve the highest score\nin both methods querying external KBs and meth-\nods with LLMs. InstructBLIP (Dai et al., 2023)\nachieves the SOTA of A-OKVQA with multimodal\ninstruction tuning.\nMethod Accuracy\nMultimodal pre-train\nLAMOC11B (Du et al., 2023) 40.3\nBLIP-2 (FlanT5-XL) (Li et al., 2023) 40.7\nBLIP-2 (FlanT5-XXL) (Li et al., 2023) 45.9\nOFA-large (Wang et al., 2022) 49.4\nUnified-IO (2.8B) (Pai et al., 2000) 54.0\nFlamingo (80B) (Alayrac et al., 2022) 57.8\nInstructBLIP (Dai et al., 2023) 62.1\nPaLM-E (562B) (Driess et al., 2023) 66.1\nMethods querying external KBs\nConceptBERT (Gard√®res et al., 2020) 33.7*\nKRISP (Marino et al., 2021) 38.9\nVis-DPR (Luo et al., 2021) 39.2\nMA VEx (Wu et al., 2022b) 40.3\nVLC-Bert (Ravi et al., 2023) 43.1\nTRiG (Gao et al., 2022) 50.5\nRA-VQA (Lin and Byrne, 2022) 54.5\nREVEAL (Hu et al., 2022) 59.1\nMethods with LLMs\nImg2Prompt175B (Guo et al., 2023) 45.6\nKAT (ensemble) (Gui et al., 2022) 54.4\nKAT-full EnFoRe (ensemble)\n(Wu and Mooney, 2022) 55.2\nPICa-Full (Yang et al., 2022) 48.0\nREVIVE (Lin et al., 2022) 58.0\nPromptCap‚ô¢ (Hu et al., 2023a) 60.4\nProphet (Shao et al., 2023) 57.9\nProphet+ours 59.3 (+1.4)\nProphe (ensemble) (Shao et al., 2023) 61.1\nProphet+ours (ensemble) 61.3 (+0.2)\nTable 7: Comparisons to other methods on OK-VQA. *\nindicates results reported on OK-VQA v1.0. ‚ô¢ refers\nto method with currently deprecated LLMs. Instruct-\nBLIP refers to InstructBLIP(Vicuna-7B). The highest\naccuracy within methods using LLMs is bolded and\nthe highest accuracy within the other two scopes are\nunderlines.\n2886\nMethod Accuracy\nFine-tune\nPythia (Schwenk et al., 2022) 25.2\nViLBERT (Schwenk et al., 2022) 30.6\nLXMERT (Schwenk et al., 2022) 30.7\nClipCap (Schwenk et al., 2022) 30.9\nKRISP (Marino et al., 2021) 33.7\nLAMOC (Du et al., 2023) 37.9\nVLC-BERT (Ravi et al., 2023) 45.0\nGPV-2 (Schwenk et al., 2022) 48.6\nUnified-IO (Pai et al., 2000) -\nREVEAL (Hu et al., 2022) 52.2\nInstructBLIP (Dai et al., 2023) 64.0\nIn-context\nImg2Prompt175B (Guo et al., 2023) 42.9\nassistGPT (Gao et al., 2023) 44.3\nPromptCap‚ô¢ (Hu et al., 2023a) 56.3\nProphet (Shao et al., 2023) 58.2\nProphet+ours 59.8\nTable 8: Comparisons to other methods on A-OKVQA\nDA task. ‚ô¢ refers to method with currently deprecated\nLLMs. InstructBLIP refers to InstructBLIP(Vicuna-7B).\nThe highest accuracy is bolded and the second best is\nunderlines.\nD Ensemble with different numbers of\nshots\nThe results of ensemble queries are listed in Table 9.\nWe notice that our method improved the result of\nProphet by 1.43% in 20-shot single query setting.\nBut the increment decreases to 0.2% when applying\nensemble. Also, similar pattern can be found in 16-\nshot setting. The increments from T=1 to T=3 are\nrelatively significant, which are 1.8% (20-shot) and\n2.1% (16-shot). While continuously increasing the\nnumber of ensemble queries from T=3 to T=5, the\nincrements decrease to 0.2% and 0.1%. Because\nthe in-context examples are arranged according\nto the relevance to the test instances, these phe-\nnomenon could result from engaging irrelevance\nexamples when enlarging the number of queries.\nT 20-shot 16-shot\nOurs Prophet Ours Prophet\nT=1 59.3 57.9 58.5 57.5\nT=2 60.5 - 60.1 -\nT=3 61.1 - 60.6 -\nT=4 61.2 - 60.8 -\nT=5 61.3 61.1 60.9 60.8\nTable 9: 20-shot and 16-shot results of the number of\nensemble queries (T) on OK-VQA. T=1 means no en-\nsemble.\nWe conduct further examination on the ensem-\nble behavior on OK-VQA dataset and find out that\nProphet indeed benefits more from ensemble com-\npared to the other baselines. Table 10 shows the\nresults of the single query setting and the ensem-\nble setting. For Prophet, the gain of the ensemble\nsetting over the single setting is 3.19% (Line 5),\nwhich is more than doubled compared to results on\nLine 1 to Line 4, and is nearly doubled compared\nto Line 6. This shows an inconsistency regarding\nthe gain of ensemble setting over single setting,\nand supports our hypothesis that Prophet benefits\nmore from the ensemble setting compared to other\nmethods.\nMethods Single Ensemble Gain\nPICa 49.67 51.36 1.69\n+ Ours 53.76 (+4.09) 54.91 (+3.55) 1.15\nPromptCap 53.50 54.16 0.66\n+ Ours 54.42 (+0.92) 55.01 (+0.85) 0.59\nProphet 57.91 61.10 3.19\n+ Ours 59.34 (+1.43) 61.30 (+0.20) 1.96\nTable 10: Single and Ensemble performances of our\nmethods and baselines. Gain refers to the increment of\nensemble result over single result.\nE Experiments with Dense Captions\nIn our framework, the LLM perceives the image\nfrom two parts of the prompt: ‚ÄúCaption‚Äù and ‚ÄúIm-\nage Information‚Äù (as the templates described in\n¬ß3.3). We incorporate the dense captions (denoted\nas GRiT (Wu et al., 2022a)) and conduct two types\nof experiments with the PICa pipeline:\n‚Ä¢ Dense captions directly as ‚ÄúCaption‚Äù;\n‚Ä¢ Dense captions as a type of ‚ÄúImage Informa-\ntion‚Äù.\nAs shown in Table 11, the first two lines show\nthe influence of dense captions and general cap-\ntions. Simply replacing the OSCAR captions by\nGRiT captions reduces the accuracy from 49.67%\nto 46.16%. Adding GRiT to PICa as the ‚ÄúImage\nInformation‚Äù (line 4) also impairs the original PICa\nperformance (line 1) by 0.24%.\nLines 3, 4 and 5 present the performances of\ndifferent types of ‚ÄúImage Information‚Äù. Notably,\nreplacing our information by GRiT‚Äôs dense cap-\ntions decreases the performance by 4.33% (com-\nparing line 4 to line 3). While combining Ours and\nGRiT captions as the ‚ÄúImage Information‚Äù (line 5)\nreduces the accuracy by 1.21% compared to using\nonly Ours ‚ÄúImage Information‚Äù (line 3).\nTo conclude, dense captions introduce noise and\ndegrade the accuracy, regardless of the role as ei-\nther ‚ÄúCaption‚Äù or ‚ÄúImage Information‚Äù. In contrast,\n2887\nutilizing the image information obtained and re-\nfined by our method consistently yields the best\nresults.\nMethods Caption Image Info. ACC\nPICa OSCAR - 49.67\nPICa GRiT - 46.16 (-3.15)\n+Ours OSCAR Ours 53.76\n+D.C. OSCAR GRiT 49.43 (-4.33)\n+D.C.+Ours OSCAR GRiT+Ours 52.55 (-1.21)\nTable 11: Experimental results with dense captions\n(GRiT). ‚ÄúCaption‚Äù refers to the type of caption used\nin Template-Few-shotand Template-Query in ¬ß3.3. ‚ÄúIm-\nage Info.‚Äù refers to the type of image information used\nin Template-Few-shotand Template-Query in ¬ß3.3.\nF Prompt Templates\nIn this section, we provide detailed prompt tem-\nplate for question generation, summarization, and\nin-context learning for visual question reasoning.\nSince the input length of LLMs are limited, to\npresent LLMs with more potential image infor-\nmation and relevant knowledge, we remove the\nseparators (‚Äú===‚Äù) used in PICa and its followers.\nF.1 Prompt Templates for Question\nGeneration\nHere is the prompt template and an example for\nquestion generation described in ¬ß3.1. The number\nof questions to generate is 3, and the test instance\nis marked in blue. The template and example are\nas follows:\nPrompt Template for Question Generation\nPlease decompose the TARGET-QUESTION into\n3 questions that can be answered via\ncommonsense knowledge. The sub-questions\nshould not mention another sub-questions.\nYou can use information from the CAPTION.\\n\nTARGET-QUESTION: qn\\n\nCaption: Cn\\n\nSub questions: 1. q‚Ä≤n\n1 . 2. q‚Ä≤n\n2 , ...\\n\nTARGET-QUESTION: q\\n\nSub questions:\\n\nAn Example for Question Generation\nPlease decompose the TARGET-QUESTION into\n3 questions that can be answered via\ncommonsense knowledge. The sub-questions\nshould not mention another sub-questions.\nYou can use information from the CAPTION.\\n\nTARGET-QUESTION: What is the hairstyle of\nthe blond called?\\n\nCaption: Two women tennis players on a\ntennis court.\\n\nSub questions: 1. It this hairstyle long\nor short? 2. What are the notable features\nof the hairstyle? 3. What hairstyle are\ncommon for women player when they are\nplaying tennis\\n\nTARGET-QUESTION: How old do you have\nto be in canada to do this?\\n\nCaption: a couple of people are holding up\ndrinks.\\n\nSub questions: 1. Why are people holding\nup drinks? 2. What is the restriction of\nage to drink in Canada? 3. What are people\ndrinking?\\n\nTARGET-QUESTION: When was this piece\nof sporting equipment invented?\\n\nCaption: A man in a wetsuit carrying a\nsurfboard to the water.\\n\nSub questions: 1. What is the man carrying\nwith him? 2. What is the purpose of\nthe sporting equipment? 3. What is the\nhistory of the invention of the sporting\nequipment?\\n\nTARGET-QUESTION: What hair style does\nthe child have?\\n\nCaption: a little girl with short hair\ntalking on a cell phone.\\n\nSub questions:\\n\nF.2 Prompt Templates for Summarization\nIn the refinement module, we summarize the gener-\nated questions with corresponding answer into nar-\nrative expressions for further process. Here is the\nprompt template and an example for information\nsummarization described in ¬ß3.2, the test instance\nis marked in blue:\nPrompt Template for Summarization\nPlease summarise the following question\nand corresponding answer into a description\nsentence.\\n\nQ: qn\\n A: an\\n Summary: 1. q‚Ä≤n\n1 . 2. q‚Ä≤n\n2 ,\n...\\n\nQ: q\\n A: an\\n Summary:\\n\n2888\nAn Example for Summarization\nPlease summarise the following question\nand corresponding answer into a description\nsentence.\\n\nQ: What is the specific type of drink be?\\n\nA: martini.\\n\nSummary: People are drinking martinis.\\n\nQ: What is the legal age to consume\nalcohol in Canada?\\n\nA: 18.\\n\nSummary: People should be at least 18 to\nconsume alcohol in Canada.\\n\nQ: What type of drinks are on the\ntable?\\n\nA: a soda.\\n\nSummary: There is a soda on the table.\\n\nQ: How is this beverage made?\\n\nA: it is a coffee drink\\n\nSummary:\nF.3 Prompt Templates for Reasoning\nWe employ few-shot in-context learning for answer\nreasoning. Here is the prompt template described\nin ¬ß3.3, the test instance is marked in blue:\nPrompt Template for Reasoning\nAnswer the questions using the provided\nimage information, captions and extra\ncommonsense knowledge. Answers should be\nno longer than 3 words:\\n\nImage information: Sn\\n\nCaption: Cn\\n Question: qn\\n Answer: an\nImage information: S\\n\nCaption: C\\n Question: q\\n Answer:\nWe implement our method with different base-\nlines according to the their default settings for im-\nage representation. PICa employs captions with\ntags as image representation; PromptCap uses thire\nquestion-aware captions; and Prophet provides ex-\ntra answer candidates. There are the examples\nfor PICa+ours, PromptCap+ours and Prophet+ours,\nour refined information is bolded in the template,\nand the test instance is marked in blue:\nAn Example for Reasoning with PICa\nAnswer the questions using the provided\nimage information, captions and extra\ncommonsense knowledge. Answers should be\nno longer than 3 words:\\n\nImage information: the person is skiing;\nthe person is wearing skis on their feet;\ncross country skiing is a popular activity\nwhile skiing.\\n\nCaption: A man is cross country skiing\nthrough a forrest in winter. winter, tree,\nsky, outdoor recreation, piste, blizzard,\nski resort, outdoor, snow, skiing\\n\nQuestion: What is this person doing?\\n\nAnswer: cross country ski\nImage information: the person is wearing\nskis; cross country skis are one of the\nequipment options for this activity; Snow\nconditions impact travel safety during\nthis activity.\\n\nCaption: A man on skis riding through\nthe snow. cross-country skier, footwear,\nmountain, mountain guide, snowshoe, winter,\nglacial landform, standing, ski equipment,\nice cap\\n\nQuestion: What is this person doing?\\n\nAnswer: ski\n...\nImage information:The women steps over the\nwater; Water freezes when it gets cold;\nThe area will change when the temperature\nreaches 0 degrees.\\n\nCaption: A woman on skis in the snow near\na tree. cross-country skier, footwear,\noutdoor recreation, blizzard, freezing,\nsnowshoe, winter sport, winter, snow,\ntrekking pole\\n\nQuestion: If it gets cold enough what will\nhappen to the area being stepped over?\\n\nAnswer:\n2889\nAn Example for Reasoning with PromptCap\nAnswer the questions using the provided\nimage information, captions and extra\ncommonsense knowledge. Answers should be\nno longer than 3 words:\\n\nImage information: the person is skiing;\nthe person is wearing skis on their feet;\ncross country skiing is a popular activity\nwhile skiing.\\n\nCaption: A person skiing on a snowy road.\\n\nQuestion: What is this person doing?\\n\nAnswer: cross country ski\nImage information: the person is wearing\nskis; cross country skis are one of the\nequipment options for this activity; Snow\nconditions impact travel safety during\nthis activity.\\n\nCaption: A person skiing down a snowy\nhill.\\n\nQuestion: What is this person doing?\\n\nAnswer: ski\n...\nImage information:The women steps over the\nwater; Water freezes when it gets cold;\nThe area will change when the temperature\nreaches 0 degrees.\\n\nCaption: a woman on skis in the snow\\n\nQuestion: If it gets cold enough what will\nhappen to the area being stepped over?\\n\nAnswer:\nAn Example for Reasoning with Prophet\nAnswer the questions using the provided\nimage information, captions, candidate\nanswers and extra commonsense knowledge.\nEach candidate answer is associated with\na confidence score within a bracket. The\ntrue answer may not be included in the\ncandidate answers. Answers should be no\nlonger than 3 words:\\n\nImage information: the person is skiing;\nthe person is wearing skis on their feet;\ncross country skiing is a popular activity\nwhile skiing.\\n\nCaption: A man is cross country skiing\nthrough a forrest in winter.\\n\nQuestion: What is this person doing?\\n\nCandidatew: ski (0.98), cross country\nski (0.63), skiis (0.13), hike (0.11),\nsnow (0.09), cross country (0.02), skiing\n(0.01), snowboard (0.00), camp (0.00),\ncold weather (0.00)\\n\nAnswer: cross country ski\nImage information: the person is wearing\nskis; cross country skis are one of the\nequipment options for this activity; Snow\nconditions impact travel safety during\nthis activity.\\n\nCaption: A man on skis riding through the\nsnow. \\n\nQuestion: What is this person doing?\\n\nCandidatew: ski (0.99), snow (0.66), sky\n(0.15), water (0.03), skiis (0.02), ski\npole (0.01), downhill (0.01), snowboard\n(0.00), hill (0.00), commuter (0.00)\\n\nAnswer: ski\n...\nImage information:The women steps over the\nwater; Water freezes when it gets cold;\nThe area will change when the temperature\nreaches 0 degrees.\\n\nCaption: A woman on skis in the snow near\na tree.\\n\nQuestion: If it gets cold enough what will\nhappen to the area being stepped over?\\n\nCandidatew: fall (0.04), crash (0.02),\nbreak (0.01), avalanche (0.01), death\n(0.01), cold (0.00), freeze (0.00), autumn\n(0.00), oxygen (0.00), drown (0.00)\\n\nAnswer:\n2890",
  "topic": "Leverage (statistics)",
  "concepts": [
    {
      "name": "Leverage (statistics)",
      "score": 0.7393139600753784
    },
    {
      "name": "Ask price",
      "score": 0.7006504535675049
    },
    {
      "name": "Computer science",
      "score": 0.5501627326011658
    },
    {
      "name": "Natural language",
      "score": 0.4935389757156372
    },
    {
      "name": "Question answering",
      "score": 0.47824230790138245
    },
    {
      "name": "Visual reasoning",
      "score": 0.4495561718940735
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3667449951171875
    },
    {
      "name": "Business",
      "score": 0.08392351865768433
    },
    {
      "name": "Finance",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    }
  ]
}