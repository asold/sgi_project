{
  "title": "Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-Based Retrofitting",
  "url": "https://openalex.org/W4393160124",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2488618492",
      "name": "Xinyan Guan",
      "affiliations": [
        "Aerospace Information Research Institute",
        "Institute of Software",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2510337587",
      "name": "Yanjiang Liu",
      "affiliations": [
        "Aerospace Information Research Institute",
        "University of Chinese Academy of Sciences",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2098375810",
      "name": "Hongyu Lin",
      "affiliations": [
        "Institute of Software",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2152192699",
      "name": "Yaojie Lu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2107993732",
      "name": "Ben He",
      "affiliations": [
        "Institute of Software",
        "University of Chinese Academy of Sciences",
        "Aerospace Information Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2171485312",
      "name": "Xianpei Han",
      "affiliations": [
        "State Key Laboratory of Computer Science",
        "Institute of Software",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2097220636",
      "name": "Le Sun",
      "affiliations": [
        "Institute of Software",
        "Aerospace Information Research Institute",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2488618492",
      "name": "Xinyan Guan",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2510337587",
      "name": "Yanjiang Liu",
      "affiliations": [
        "Institute of Software",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2098375810",
      "name": "Hongyu Lin",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2152192699",
      "name": "Yaojie Lu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2107993732",
      "name": "Ben He",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2171485312",
      "name": "Xianpei Han",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2097220636",
      "name": "Le Sun",
      "affiliations": [
        "Institute of Software",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6810242208",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W2946345909",
    "https://openalex.org/W4385682071",
    "https://openalex.org/W4288091035",
    "https://openalex.org/W580074167",
    "https://openalex.org/W4302306557",
    "https://openalex.org/W4389518797",
    "https://openalex.org/W4389520749",
    "https://openalex.org/W4385570284",
    "https://openalex.org/W4385568240",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W4366566341",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4377372007",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3035725000",
    "https://openalex.org/W4366999773",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W4377142519",
    "https://openalex.org/W4385570481",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4378465094",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W4381586770",
    "https://openalex.org/W3175929660",
    "https://openalex.org/W4303202235",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4385734218"
  ],
  "abstract": "Incorporating factual knowledge in knowledge graph is regarded as a promising approach for mitigating the hallucination of large language models (LLMs). Existing methods usually only use the user's input to query the knowledge graph, thus failing to address the factual hallucination generated by LLMs during its reasoning process. To address this problem, this paper proposes Knowledge Graph-based Retrofitting (KGR), a new framework that incorporates LLMs with KGs to mitigate factual hallucination during the reasoning process by retrofitting the initial draft responses of LLMs based on the factual knowledge stored in KGs. Specifically, KGR leverages LLMs to extract, select, validate, and retrofit factual statements within the model-generated responses, which enables an autonomous knowledge verifying and refining procedure without any additional manual efforts. Experiments show that KGR can significantly improve the performance of LLMs on factual QA benchmarks especially when involving complex reasoning processes, which demonstrates the necessity and effectiveness of KGR in mitigating hallucination and enhancing the reliability of LLMs.",
  "full_text": "Mitigating Large Language Model Hallucinations via Autonomous\nKnowledge Graph-Based Retrofitting\nXinyan Guan1,2*, Yanjiang Liu1,2*, Hongyu Lin1, Yaojie Lu1† ,\nBen He1,2, Xianpei Han1,3, Le Sun1,2†\n1Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China\n2University of Chinese Academy of Sciences, Beijing, China\n3State Key Laboratory of Computer Science Institute of Software, Chinese Academy of Sciences, Beijing, China\n{guanxinyan2022,hongyu,luyaojie,xianpei,sunle}@iscas.ac.cn,\nliuyanjiang22@mails.ucas.ac.cn,benhe@ucas.edu.cn\nAbstract\nIncorporating factual knowledge in knowledge graph is re-\ngarded as a promising approach for mitigating the halluci-\nnation of large language models (LLMs). Existing methods\nusually only use the user’s input to query the knowledge graph,\nthus failing to address the factual hallucination generated\nby LLMs during its reasoning process. To address this prob-\nlem, this paper proposes Knowledge Graph-based Retrofitting\n(KGR), a new framework that incorporates LLMs with KGs\nto mitigate factual hallucination during the reasoning process\nby retrofitting the initial draft responses of LLMs based on the\nfactual knowledge stored in KGs. Specifically, KGR leverages\nLLMs to extract, select, validate, and retrofit factual state-\nments within the model-generated responses, which enables\nan autonomous knowledge verifying and refining procedure\nwithout any additional manual efforts. Experiments show that\nKGR can significantly improve the performance of LLMs on\nfactual QA benchmarks especially when involving complex\nreasoning processes, which demonstrates the necessity and ef-\nfectiveness of KGR in mitigating hallucination and enhancing\nthe reliability of LLMs.\nIntroduction\nLarge Language Models (LLMs) have gained increasing\nprominence in artificial intelligence. The emergence of potent\nmodels such as ChatGPT (OpenAI 2022) and LLaMA (Tou-\nvron et al. 2023) has led to substantial influences on many\nareas like society, commerce, and research. However, LLMs\nstill suffer from severe factual hallucinationproblems, i.e.,\nLLMs can frequently generate unsupported false statements\nregarding factual information due to their lack of intrinsic\nknowledge (Ji et al. 2023). For example, in Figure 1, Chat-\nGPT fails to provide an accurate response to the query “When\nis Fr\n´ed´eric Chopin’s father’s birthday?” due to a wrong belief\nthat Nicolas Chopin’s birthday is on June 17, 1771. Factual\nhallucination poses a severe challenge for LLM applications,\nparticularly in real-world situations where factual accuracy\nholds significance. Consequently, the endeavor to alleviate\n*These authors contributed equally.\n† Corresponding authors.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nfactual hallucinations in LLMs has become a research hotspot\nin NLP field (Liu et al. 2021; Kang and Hashimoto 2020).\nOn the other hand, Knowledge Graphs (KGs) store a sub-\nstantial amount of high-quality factual information, which\ncan significantly alleviate factual hallucination if incorpo-\nrated with LLMs. For example, in Figure 1, we can retrofit\nthe erroneous statement “Nicolas Chopin was born on June\n17, 1771” by referring to the provided factual knowledge\n“(Nicolas Chopin, date of birth, 1771-04-15T00:00:0)” in\nWikidata. Recent work has focused on integrating LLMs\nwith KGs by retrieving the entities in the query within knowl-\nedge graphs. Then the obtained factual triples are utilized\nas an additional context for LLMs to enhance their factual\nknowledge (Baek, Aji, and Saffari 2023; Chase 2022). Un-\nfortunately, these approaches are limited to retrieving factual\nknowledge relevant to entities explicitly mentioned within\nthe given query. However, the fundamental capability of large\nlanguage models involves intricate and multi-step reasoning.\nSuch reasoning processes often necessitate the validation and\naugmentation of factual knowledge that may be employed\nduring the reasoning process. For example, in the case shown\nin Figure 1, LLM fails to answer the question because it\nrequires an intermediate knowledge about “Nicolas Chopin\nwas born on April 15, 1771”. However, such information\ndoes not refer to entities appearing in the query. As a result,\nprevious approaches are inadequate in addressing the factual\nhallucination appearing in the reasoning processes of LLMs.\nIn this paper, we propose Knowledge Graph-based\nRetrofitting (KGR), a new framework that incorporates LLMs\nwith KGs to mitigate factual hallucination during the entire\nreasoning process of LLMs. Instead of retrieving factual in-\nformation from KGs using original queries, the main idea\nbehind KGR is to autonomously retrofit the initial draft re-\nsponses of LLMs based on the factual knowledge stored in\nKGs. However, achieving the above process is challenging\nbecause draft responses generated by large language mod-\nels typically contain a mixture of various information about\nthe reasoning process, making the extraction, verification,\nand revision of relevant knowledge in it very challenging.\nTherefore, the key to integrating Knowledge Graphs into the\nreasoning process of large models to mitigate factual halluci-\nnations lies in efficiently extracting the information requiring\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18126\nFigure 1: An overview of KGR, our framework consists of five components: (1) claim extraction, (2) entity detection and KG\nretrieval, (3) fact selection, (4) claim verification, (5) response retrofitting. The core component of these five steps remains the\nLLM. Given a question and draft response as input, our framework can iteratively mitigate factual errors in LLM’s response.\nvalidation from draft responses, querying and selecting rele-\nvant knowledge from the knowledge graphs, and using this\nknowledge to verify and refine draft responses.\nTo this end, KGR presents a LLMs-based framework to\nautonomously extract, validate and refine factual statements\nwithin the initial draft responses without any manual efforts.\nSpecifically, given an input query and a draft response gen-\nerated by the LLM that entails the reasoning process of how\nLLM resolves this problem, KGR will request a LLM to\nextract factual claims in the reasoning process that require\nverifying by KGs. As shown in Figure 1, given the draft\nresponse “Fr´ed´eric Chopin’s father is Nicolas Chopin, he\nwas born on June 17, 1771.”, the claim extraction step will\ngenerate factual claims in it like “Fr´ed´eric Chopin’s father\nis Nicolas Chopin” and “Nicolas Chopin was born on June\n17, 1771”. Then, KGR will identify critical entities in the\nextracted claims, retrieve relevant factual triples from knowl-\nedge graph about the entities, and use a LLM-based fact\nselector to identify fact triples relevant to the draft response.\nSubsequently, the retrieved factual knowledge is utilized to\ncompare with the previously extracted factual claims from\nthe draft to verify their correctness. Finally, LLMs are asked\nto retrofit the draft in accordance with the outcomes of factual\nverification. This process can be repeated multiple times to\nensure that all facts in the generated answers align with the\nknowledge stored within the knowledge graph. In this way,\nour method can not only verify the facts in query and re-\nsponse but also the facts used during reasoning. Furthermore,\nbecause all phases in the procedure can be automatically exe-\ncuted using a large language model, our method doesn’t need\nany external components and therefore is easy to implement.\nWe conduct experiments with three representative LLMs\non three standard factual QA benchmarks with different levels\nof reasoning difficulty, including Simple Question (Bordes\net al. 2015), Mintaka (Sen, Aji, and Saffari 2022) for complex\nreasoning, and HotpotQA(Yang et al. 2018) for open domain,\nmulti-hop reasoning. Experiments show that KGR can sig-\nnificantly improve the performance of LLMs on factual QA\nbenchmarks especially when involving complex reasoning\nprocesses, which demonstrates the necessity and effective-\nness of KGR in mitigating hallucination.\nIn summary, the contributions are as follows:\n• We propose a new framework that incorporates LLMs\nwith KGs to mitigate factual hallucination by effectively\nextracting, verifying, and refining factual knowledge in\nthe entire reasoning process of LLMs.\n• We present an implementation of the above-mentioned\nprocedure by executing all the above-mentioned steps\nusing LLMs without introducing any additional efforts.\n• Experiments on 3 datasets and 3 different LLMs confirm\nthat KGR can significantly mitigate the hallucination and\nenhance the reliability of LLMs.\nRelated Work\nHallucination Hallucination in Large Language Models\nhas been a prominent research focus within the NLP com-\nmunity (Ji et al. 2023). Automated large-scale data collec-\ntion processes are prone to collecting erroneous information,\nwhich can significantly impact the quality of the generated\noutputs (Gunasekar et al. 2023). Additionally, excessive rep-\netition of certain data during training can introduce memory\nbiases, further exacerbating the hallucination issue (Lee et al.\n2022; Biderman et al. 2023). Imperfections in the encoder\nbackbone and variations in decoding strategies also play a\nrole in determining the extent of hallucination in LLMs out-\nputs (Tian et al. 2019). Recent studies have emphasized the\nimportance of model output confidence as an indicator of\npotential hallucination occurrences (Manakul, Liusie, and\nGales 2023).\nRetrieval Augmentation To address hallucination issues\nin LLMs, two main categories of retrieval augmentation meth-\nods have been proposed, which can be concluded as “retrieve\nbefore generation“ and “retrieve after generation“. The re-\ntrieve before generation mainly focuses on leveraging infor-\nmation retrieval (IR) to provide additional information to\nLLMs about the query. Along this line, UniWeb (Li et al.\n2023b) introduces an adaptive method for determining the\noptimal quantity of referenced web text, Chameleon(Lu et al.\n2023) leverages an assortment of tools including search en-\ngines, to bolster the reasoning capabilities of LLMs, We-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18127\nbGLM (Liu et al. 2023b) augments LLMs with web search\nand retrieval capabilities. One major limitation of these ap-\nproaches is the retrieved text is question-related, thus can-\nnot guarantee the correctness of the question-unrelated por-\ntions in the generations. The retrieve after generation like\nRARR (Gao et al. 2023), PURR (Chen et al. 2023), and\nCRITIC (Gou et al. 2023) automatically edit model gener-\nations using evidence from the web. Our method leverages\nKGs as knowledge base to retrofit the model-generated re-\nsponse while reducing hallucination risk.\nKG-Enhanced LLM The Knowledge Graph is regarded\nas a dependable source of information and is consequently\nfrequently employed to enhance model generations. Tradi-\ntional approaches involve knowledge representations during\nthe training phase, which often necessitates dedicated model\narchitecture and model-specific training (Zhang et al. 2019,\n2022). However, this incurs a substantial cost for contem-\nporary LLMs. In recent years, many researchers propose to\ninject knowledge during the inference stage. For example,\nKAPING (Baek, Aji, and Saffari 2023), RHO (Ji et al. 2022),\nKITLM (Agarwal et al. 2023), and StructGPT (Jiang et al.\n2023) try to retrieve knowledge in KG and utilize them as\nan additional input context for LLMs to enhance their gen-\nerations. However, these methods only search for question-\nrelevant information, which limits the overall performance.\nTo the best of our knowledge, we’re the first to involve knowl-\nedge graphs in model response retrofitting.\nKGR: Autonomous Knowledge Graph-Based\nRetrofitting\nIn this section, we introduce our proposed method KGR,\nwhich automatically mitigates factual hallucinations via a\nchain-of-verification process. As shown in Figure 1, given a\nquery and its draft response, KGR retrofits the response by\n1) extracting claims from the draft answer that requires verifi-\ncation; 2) detecting entities in the claims that are critical for\nretrieving facts from knowledge graph; 3) retrieving relevant\nfact statements from the knowledge graph; 4) verifying the\nfactual correctness of each extracted claim using the returned\nfactual statements from the knowledge graph; 5) retrofitting\nthe previous draft response based on the verification results.\nAll these steps are autonomously executed using the large\nlanguage model itself without additional manual efforts. This\nprocess can be iterative and repeated multiple times to ensure\nthat all facts in the generated answers align with the factual\nknowledge stored within the knowledge graph. In the follow-\ning, we will describe each component in KGR respectively\nin detail.\nClaim Extraction\nGiven a generated draft response as input, claim extraction\nwill extract all factual claims from previously generated drafts\nthat require validation. The main idea behind claim extrac-\ntion is that a draft response can frequently contain various\nfactual statements that need to be verified. For the example\nin Figure 1, the draft response contains at least two factual\nstatements, i.e., “Fr´ed´eric Chopin’s father is Nicolas Chopin”\nand “Nicolas Chopin was born on June 17, 1771”. Therefore,\nFigure 2: Example for the claim extraction in KGR, which\ndecomposes the proposed answer into two atomic claims.\nto make it possible for KG to verify these statements respec-\ntively, claim extraction decomposes the draft response to be\natomic factual claims.\nIn this paper, we leverage LLM itself to autonomously\nextract the claims in the generated draft response. As shown\nin Figure 2, we prompt LLM with a query and response pair,\nwith the anticipation of receiving a list of decomposed factual\nclaims.\nAfter extracting claims, entity detection identifies the men-\ntioned critical entities for knowledge graph retrieval.\nEntity Detection and Knowledge Graph Retrieval\nGiven a list of claims extracted from the draft response, entity\ndetection will detect the critical entities mentioned in the\nclaims. Then, we retrieve the detected entities’ local subgraph\nfrom the KG and expressed it in the form of triples. The main\nidea behind entity detection and knowledge graph retrieval is\nthat we need to identify entities in claims so as to retrieve the\nrelevant knowledge in the KG. Meanwhile, we can ensure\nrecalling relevant triples as much as possible by retrieving the\nlocal subgraph in the knowledge graph. For the example in\nFigure 1, we identify the entityFr´ed´eric Chopinand its entity\nid Q1268, so we can search the identified entity to acquire\nknowledge relevant to Claim1 in the KG.\nIn this paper, we prompt LLMs to detect entities. As il-\nlustrated in Figure 3, our approach shows powerful general-\nization ability by capitalizing on the information extraction\ncapabilities of LLMs(Li et al. 2023a) through the utilization\nof few-shot prompt. Based on the few-shot prompts, we can\nmake LLMs understand which entities merit fact selection.\nAfter detecting the entities, we retrieve the knowledge graph\nfor the local subgraph and send it to fact selection in the\nform of triples. Concretely, we retrieve the node identifiers\nfor these entities in Wikidata using heuristic rules and then\nquery the knowledge graph for the neighboring nodes of the\nidentified entity and obtain its local subgraph with SPARQL.\nFact Selection\nGiven the retrieved triples based on the detected entities, fact\nselection will select relevant fact statements among them.\nThe main idea behind fact selection is the limited ability of\nLLMs in long-context modeling (Liu et al. 2023a) and the\nconstraint on the context window size of LLMs, which make\nit impractical to select critical triples at one time.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18128\nFigure 3: Example for the entity detection in KGR, which\nonly extracts the essential entities from the claim.\nFigure 4: Example for the fact selection in KGR, in which\nLLMs are prompted to select critical items among all re-\ntrieved triples.\nIn this paper, we partition retrieved triples into several\nchunks and leverage LLM itself to extract the critical triples\nin the retrieved triples respectively, illustrated in Figure 4.\nIn this way, we can avoid introducing excessive irrelevant\nknowledge into claim verification. Once we have selected the\ncritical triples, the claim verification will verify the factual\ncorrectness of claims and subsequently offer suggestions.\nClaim Verification\nGiven the critical triples selected by the fact selection, we\nutilize LLM to compare the model-generated claims with\nthe factual information present in the KGs. The main idea\nbehind claim verification is to propose a detailed revision\nsuggestion for each claim, as retrofitting solely based on the\nselected knowledge may not convince LLMs. As illustrated in\nFigure 5, we employ LLMs to verify each claim and propose\nrevision suggestions respectively based on the retrieved fact\nknowledge, so as to boost the execution of the following\nretrofitting step. Then, we send the claim verification result\nto LLM to ask it to retrofit the draft response accordingly.\nResponse Retrofitting\nGiven the verification of all claims, the response retrofitting\nstep retrofits the generated draft response in accordance with\nthe verification suggestions.\nIn this paper, we capitalize on the capabilities of LLMs\nfor the purpose of retrofitting. This approach involves em-\nploying LLMs with a few-shot prompt, a strategy that has\nexhibited efficacy in prior researches (Gou et al. 2023; Zheng\net al. 2023). As illustrated in Figure 5, we merge the entire\nKGR process into a singular prompt. This allows LLMs to\nleverage their in-context learning ability, comprehending the\nFigure 5: Example for the claim verification and response\nretrofitting in KGR. The claim verification judges whether\nthe claim aligns with searched triples and gives revision sug-\ngestions respectively. The response retrofitting incorporates\nthe revision suggestions from all claims and gives a refined\nresponse.\nKGR process and enhancing their comprehension of factual\nretrofitting based on verification suggestions\nBy following the cycle of “Extraction - Detection - Selec-\ntion - Verification - Retrofitting ”, our KGR framework can\nbe iterated multiple times to ensure all facts in the generated\nanswers align with the factual knowledge stored within the\nknowledge graph.\nExperiments\nWe evaluate our KGR framework on three datasets with dif-\nferent levels of reasoning difficulty, including Simple Ques-\ntion (Bordes et al. 2015), Mintaka (Sen, Aji, and Saffari\n2022), and HotpotQA (Yang et al. 2018). We also compare\nKGR with information retrieval-based approaches and previ-\nous question-relevant knowledge graph retrieval approaches.\nExperiment Settings\nDataset and Evaluation We conduct experiments on three\nrepresentative factual QA benchmarks, including:\n• Simple Question (Bordes et al. 2015) is a simple QA\ndataset that contains 100k questions constructed from\nFreebase knowledge graph, requiring no deep reasoning\nprocedure. Therefore, we can evaluate the ability to re-\ntrieve relevant evidence in KG based on Simple Question.\n• Mintaka (Sen, Aji, and Saffari 2022) is a complex, nat-\nural and multilingual dataset, composing 20k questions\ncollected in 8 different languages. We only use English\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18129\nSimple Question Mintaka HotpotQA\nChatGPT te\nxt-davinci-003 ChatGPT text-davinci-003 ChatGPT text-davinci-003\nVanilla\n22.0/28.9 34.7/45.1 42.9/56.1 36.7/44.8 18.4/31.6 22.4/34.6\nCoT 10.0/11.8\n38.0/46.7 53.1/59.3 46.3/57.9 24.5/34.3 29.2/40.5\nCRITIC 12.0/14.3 38.0/46.7 51.0/58.6 44.4/54.0 30.6/41.7 27.1/38.9\nQKR 54.0/60.2 56.0/62.0 48.0/54.6 44.0/51.7 28.0/38.1 22.0/31.9\nKGR (ours) 58.0/60.7 54.0/57.6 53.1/60.8 52.0/60.2 32.7 /39.2 34.0/47.2\nTable 1: Results on three datasets using ChatGPT and text-davinci-003. We implement CoT using the prompt provided by\nCRITIC. QKR uses the same entity detection and fact selection method as KGR. We report both EM and F1 scores in the table.\nSimple Question Mintaka HotpotQA\nCoT 14.0/21.9 26.0/28.3 12.0/19.6\nQKR 40.0/44.0 26.5/32.4 12.2/17.6\nKGR(ours) 46.0/46.9 26.5/34.0 10.2/20.6\nTable 2: Results on three datasets using Vicuna 13B. We\nreport both EM and F1 scores in the table.\ntest sets. In this setting, we focus on the ability to logically\nrefine and revise answers based on the evidence gathered.\n• HotpotQA (Yang et al. 2018) is a Wikipedia-based 1\ndataset with 113k questions that requires finding and rea-\nsoning over multiple supporting documents to answer,\nwhich are diverse and not constrained to any pre-existing\nknowledge bases or knowledge schemas. Therefore, we\nevaluate the KGR framework’s robustness in handling\ngeneralized scenarios, requiring LLMs to answer involv-\ning the incorporation of both parametric knowledge and\nKnowledge Graph-based information.\nWe reported the results in terms of EM and F1 scores respec-\ntively on 50 samples from the validation set of each dataset.\nBy comparing performance across these three datasets, we\ncan evaluate how well different methods mitigate factual\nhallucinations and handle complex tasks.\nLLMs and KG Implementation We evaluate the effec-\ntiveness of KGR on both close-source and open-source large\nlanguage models. For close-source models, we evaluate on\ntext-davinci-003 and ChatGPT (gpt-3.5-turbo-0301) to see\nwhether alignment will have an impact on KGR. For the\nopen-source model, we evaluate KGR on Vicuna 13B, a\nrepresentative aligned open-source model, to see whether\nKGR can work well on compact size LMs. We choose Wiki-\ndata\n2(Vrandeˇci´c and Kr¨otzsch 2014) as our knowledge base,\nwhich encompasses structured data from various sources such\nas Wikipedia, Wikimedia Commons(Commons 2023), and\nother wikis associated with the Wikimedia movement(Meta\n2023).\n1https://wikipedia.org\n2https://www.wikidata.org/\nBaseline\nWe compared our KGR framework with the following meth-\nods, including:\n• Vanilla, which adopts a straightforward approach to\nprompt the model to generate answers for the given ques-\ntion.\n• Chain of Thought (CoT) (Wei et al. 2022), which aims\nto generate more reliable answers by prompting LLMs to\ngenerate more comprehensive and detailed explanations\nfor the generated answers.\n• Self-Correcting with Tool-Interactive Critiquing (CR -\nITIC) (Gou et al. 2023), which revises the answer based\non text from the web. Since CRITIC did not release their\nweb crawling method, in this experiment we adopt the\ncrawling pipeline provided in RARR (Gao et al. 2023) via\nBing Search3\n• Question-relevant Knowledge Retrieval method (QKR),\nwhich prompts LLMs with the question-relevant retrieved\nfacts in the knowledge graph to generate answers. We aim\nto demonstrate the superior effectiveness of our response-\nrelevant retrofitting method over the question-relevant\nknowledge graph augmentation approach. Inspired by\nKAPING (Baek, Aji, and Saffari 2023), QKR leverages\nextracted facts from KGs as prompts to enhance response\ncorrectness. In our implementation, we replace the fact ex-\ntracts process with our entity detection and fact selection\nto strictly compare the difference between the response-\nrelevant and query-relevant methods.\nOverall Results\nAs shown in Table 1, our method demonstrates significant\nsuperiority over other methods across various conditions.\n1) Our framework can mitigate large language model hal-\nlucination via Knowledge Graph-based Retrofitting\nand achieve significant improvements on 3 datasets.\nCompared with the CoT and CRITIC, our KGR frame-\nwork gains improvements on all three datasets. This in-\ndicates that our KG-based approach is more effective\ndue to its reliance on a reliable knowledge base, whereas\nIR-based methods like CRITIC might introduce noise\nfrom external. Additionally, we observed that the CoT\n3https://www.bing.com/.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18130\nmethod performed worse than the vanilla approach in\nChatGPT. This could be attributed to the CoT method’s\ntendency to ask for more information, which is amplified\nin ChatGPT due to Reinforcement Learning from Human\nFeedback (Ouyang et al. 2022).\n2) By verifying the facts used during reasoning via chain-\nof-verification, our method can achieve significant per-\nformance improvement in complex reasoning tasks\nin Mintaka and HotpotQA datasets. As shown in Ta-\nble 1, compared to the QKR method, our KGR frame-\nwork achieves F1 improvement for at least 6.2 and 1.1\non Mintaka and HotpotQA. Both of them pose complex\nreasoning question-answering challenges, and the success\nof our method with chain-of-verification on these datasets\ndemonstrates its capability to handle complex questions\neffectively. It is worth noting that the text-davinci-003 out-\nperformed QKR in Simple Question. We attribute this to\nthe fact that Simple Question consists of straightforward,\none-hop questions, which makes the question-relevant\nmethod more effective.\n3) By automatically generating and executing chain-of-\nverification via LLMs, our KGR approach exhibits\nremarkable generalization capabilities in different\ndatasets and is robust on open-domain settings. In\nHotpotQA, KGR performs well compared to the CoT\nand CRITIC methods. The HotpotQA presents an open-\ndomain QA scenario where finding related triples in the\nKG can be challenging. Despite this difficulty, our method\ndisplayed the ability to effectively utilize the searched\ntriples and effectively leverage parametric knowledge\neven when no evidence was returned.\n4) Our framework can work well on compact size LMs,\naligned LLMs, and misaligned LLMs, showing the gen-\neralizability of KGR. We compare KGR with the strong\nbaselines CoT and QKR on Simple Question, Mintaka,\nand HotpotQA using Vicuna 13B. The result is shown in\nTable 2. We can find that the KGR framework outperforms\nboth CoT and QKR, demonstrating the generalizability\nof our framework even leveraging a compact size LM.\nMoreover, the significant improvement with ChatGPT\nand text-davinci-003 shows the generalizability of both\naligned LLMs and misaligned LLMs.\nIn summary, our method consistently outperforms other\nmethods across various conditions and exhibits strong gener-\nalization ability. The results suggest that our KGR framework\nis more reliable and effective, especially in handling com-\nplex factual reasoning tasks. Furthermore, it showcases the\nrobustness of our method in open-domain QA settings, where\nknowledge retrieval may be more challenging.\nCase Study\nWe present a multi-round retrofitting process of a multi-hop\ncase which needs to be retrofitted iteratively in Figure 3.\nIn this case, the model-generated response shows a factual\nerror in the initial reasoning step. It erroneously states that\nAlex Shevelev died in Moscow, Russia, whereas he actually\npassed away in Rome, Italy. After retrofitting this mistake,\nwe encounter another factual error, which asserts that Rome\nFigure 6: Distribution of error case numbers across KGR\nstages: Analysis conducted on a sample of 50 instances from\nthe Mintaka dataset and 50 instances from the Simple Ques-\ntion dataset reveals the occurrence of error numbers across\nvarious stages of the KGR process.\nis the capital of the Central Federal District. So, we need to\nretrofit it again based on the retrofitted response in the first\niteration.\nFrom this case, we show KGR’s intermediate results, in-\ncluding atomic claim, critical triples, detailed verification,\nand iterative retrofitting. All these show the effectiveness\nof KGR, especially on reasoning with multi-hop complex\ntasks, verifying the feasibility of multi-turn retrofit to ensure\nthat all facts in the generated answers align with the factual\nknowledge stored in the knowledge graph.\nError Analysis\nIn order to gain a comprehensive understanding of the KGR\napproach, we conducted an exhaustive analysis of incorrect\ncases based on the Mintaka and Simple Question datasets.\nAfter carefully examining the errors, we identified which\ncomponent causes revision failures. The outcomes of this\nanalysis are visualized in Figure 6. On closer inspection,\nthe main issues are inaccuracies in entity detection and fact\nselection, while claim extraction, verification, and response\nretrofitting are more reliable, underscoring the need to im-\nprove entity detection and fact selection.\nOn the other hand, our analysis explored the error reason\nin different stages. The claim extraction often fails to express\nthe central claim adequately, sometimes due to excessive use\nof pronouns that confuse the model’s comprehension. En-\ntity detection has issues with entity extraction granularity.\nIt captures too many common entities like ”films” or ”ap-\nple”, leading to excessive and useless triples for the claim\nverification. The fact selection has difficulty extracting the\ncritical triples between multiple triples that contain noise.\nFor the claim verification and response retrofitting, the focus\nshifts to the model’s ability to adhere to the cues provided\nby the few-shot prompts. Effectively discerning and subse-\nquently rectifying answers within this framework presents a\ncentral challenge. The process of fact selection encounters\nchallenges in extracting essential triples from a collection of\ntriples that include irrelevant information or noise.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18131\nQuestion: The\ncity where Alex Shevelev died is the capital of what region?\nAnswer: the Lazio region\nInitial prediction: Let’s think step by step. Alex Shevelev† died in Moscow‡, Russia. And it is the capital of the Central Federal District. So the answer is:\nCentral Federal District.\nClaims F\nact Knowledge Verification Retrofitted Response\nKGR\nround1\nAlex Shevelev† died in\nMoscow, Russia.\n(Alex Shevelev†, place of death, Rome) The evidence suggests Alex Shevelev\ndied in Rome⋆, not Moscow, Russia.\nLet’s think step by step. Alex\nShevelev died in Rome, Italy⋆.\nAnd it is the capital of the\nCentral Federal District. So the\nanswer is: Central Federal\nDistrict.\nMoscow‡ is the capital of\nthe Central Federal Dis-\ntrict.\n(Central Federal District, capital, Moscow‡) The evidence suggests Moscow is the\ncapital of the Central Federal District.\nKGR\nround2\nAlex Shevelev† died in\nRome⋄, Italy.\n(Alex Shevelev†, place of death, Rome⋄) The evidence suggests Alex Shevelev\ndied in Rome. Let’s think step by step. Alex\nShevelev died in Rome, Italy.\nAnd it is the capital of Lazio⋆.\nSo the answer is: Lazio\n.\nRome⋄ is the capital of\nthe Central Federal Dis-\ntrict.\n(Rome⋄, capital of, Lazio) The evidence suggests Rome is the cap-\nital of Lazio ⋆ not the Central Federal\nDistrict.\nTable 3: Gnerated examples in HotpotQA by ChatGPT. We show the multi-turn retrofitting process of KGR. Theentity with\nsuperscript⋆ refers to retrofitted factual statements. The other green colors refers to critical entities in claims.\nFigure 7: Impact of chunk size and numbers of retrieved\ntriples on the efficacy of the fact selection. a). Results for fact\nselection were conducted on a sample of 50 instances from\nSimple Question with different chunk sizes. b). Results for\nfact selection were conducted on a sample of 50 instances\nfrom Simple Question with different retrieved triple numbers.\nImpact of Chunk Size&Numbers of Retrieved\nTriples\nAs discussed above, considering the limitation of maximum\ninput length for LLMs, we partition the retrieved triples into\nchunks for fact selection. We evaluate the effectiveness of\nfact selection when retrieved triples are in random order,\nreferring black point in Figure 7 a) on the Simple Question\nusing ChatGPT. These experiments help us understand fact\nselection behavior under various hyperparameters, optimize\nchunk size, and refine triple retrieval strategies for improved\nefficiency.\nAs shown in Figure 7 a), the chunk size has minimal impact\non triple selection capability, except for a chunk size of 100,\nwhich may cause worse long-distance dependency modeling.\nHowever, reducing the chunk size leads to lower precision\nand higher recall scores. This indicates that a smaller chunk\nsize increases the chance of selecting both critical and irrele-\nvant triples. Additionally, we observe that prompting LLMs\nwith triples in random orders doesn’t significantly affect triple\nselection.\nAs shown in Figure 7 b), increasing the number of retrieved\ntriples has a gradual positive impact on recall but significantly\nreduces precision. More retrieved triples may boost recall\nfor critical knowledge and introduce numerous irrelevant\ntriples, potentially compromising the effectiveness of the\nclaim verification and negating the benefits of fact selection.\nAll in all, experiments show that the core difficulty of\nretrieving fact knowledge based on LLMs is the tradeoff be-\ntween precision and recall. This observation points to future\nresearch on fact selection based on LLMs.\nConclusion\nIn this paper, we propose a knowledge graph-based\nretrofitting framework that effectively mitigates factual hallu-\ncination during the reasoning process of LLMs based on the\nfactual knowledge stored in KGs.\nExperiment results show that KGR can significantly im-\nprove the performance of LLMs on factual QA benchmarks\nespecially when involving complex reasoning, which demon-\nstrates the necessity and effectiveness of KGR in mitigating\nhallucination and enhancing the reliability of LLMs. As for\nfuture work, we plan to improve the effectiveness in each\nstep of our KGR framework.\nAcknowledgements\nWe thank all reviewers for their valuable suggestions. This\nresearch work is supported by the Strategic Priority Research\nProgram of Chinese Academy of Sciences under Grant No.\nXDA27020200, the National Natural Science Foundation of\nChina under Grants no. 62122077, 62106251, 62306303 and\n62076233.\nReferences\nAgarwal, A.; Gawade, S.; Azad, A. P.; and Bhattacharyya, P.\n2023. KITLM: Domain-Specific Knowledge InTegration into\nLanguage Models for Question Answering. arXiv preprint\narXiv:2308.03638.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18132\nBaek, J.; Aji, A. F.; and Saffari, A. 2023. Knowledge-\nAugmented Language Model Prompting for Zero-Shot\nKnowledge Graph Question Answering. arXiv preprint\narXiv:2306.04136.\nBiderman, S.; Schoelkopf, H.; Anthony, Q. G.; Bradley,\nH.; O’Brien, K.; Hallahan, E.; Khan, M. A.; Purohit, S.;\nPrashanth, U. S.; Raff, E.; et al. 2023. Pythia: A suite for an-\nalyzing large language models across training and scaling. In\nInternational Conference on Machine Learning, 2397–2430.\nPMLR.\nBordes, A.; Usunier, N.; Chopra, S.; and Weston, J. 2015.\nLarge-scale simple question answering with memory net-\nworks. arXiv preprint arXiv:1506.02075.\nChase, H. 2022. LangChain.\nChen, A.; Pasupat, P.; Singh, S.; Lee, H.; and Guu, K. 2023.\nPURR: Efficiently Editing Language Model Hallucinations\nby Denoising Language Model Corruptions. arXiv preprint\narXiv:2305.14908.\nCommons, W. 2023. Main Page — Wikimedia Commons,\nthe free media repository.\nGao, L.; Dai, Z.; Pasupat, P.; Chen, A.; Chaganty, A. T.; Fan,\nY .; Zhao, V .; Lao, N.; Lee, H.; Juan, D.-C.; et al. 2023. Rarr:\nResearching and revising what language models say, using\nlanguage models. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), 16477–16508.\nGou, Z.; Shao, Z.; Gong, Y .; Shen, Y .; Yang, Y .; Duan, N.;\nand Chen, W. 2023. Critic: Large language models can\nself-correct with tool-interactive critiquing. arXiv preprint\narXiv:2305.11738.\nGunasekar, S.; Zhang, Y .; Aneja, J.; Mendes, C. C. T.;\nDel Giorno, A.; Gopi, S.; Javaheripi, M.; Kauffmann, P.;\nde Rosa, G.; Saarikivi, O.; et al. 2023. Textbooks Are All\nYou Need. arXiv preprint arXiv:2306.11644.\nJi, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;\nBang, Y . J.; Madotto, A.; and Fung, P. 2023. Survey of hal-\nlucination in natural language generation. ACM Computing\nSurveys, 55(12): 1–38.\nJi, Z.; Liu, Z.; Lee, N.; Yu, T.; Wilie, B.; Zeng, M.; and Fung,\nP. 2022. RHO: Reducing Hallucination in Open-domain\nDialogues with Knowledge Grounding. arXiv preprint\narXiv:2212.01588.\nJiang, J.; Zhou, K.; Dong, Z.; Ye, K.; Zhao, W. X.; and\nWen, J.-R. 2023. StructGPT: A general framework for Large\nLanguage Model to Reason on Structured Data.\nKang, D.; and Hashimoto, T. 2020. Improved Natural Lan-\nguage Generation via Loss Truncation. arXiv:2004.14589.\nLee, K.; Ippolito, D.; Nystrom, A.; Zhang, C.; Eck, D.;\nCallison-Burch, C.; and Carlini, N. 2022. Deduplicating\nTraining Data Makes Language Models Better. In Proceed-\nings of the 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), 8424–8445.\nDublin, Ireland: Association for Computational Linguistics.\nLi, B.; Fang, G.; Yang, Y .; Wang, Q.; Ye, W.; Zhao, W.;\nand Zhang, S. 2023a. Evaluating ChatGPT’s Information\nExtraction Capabilities: An Assessment of Performance, Ex-\nplainability, Calibration, and Faithfulness. arXiv preprint\narXiv:2304.11633.\nLi, J.; Tang, T.; Zhao, W. X.; Wang, J.; Nie, J.-Y .; and Wen,\nJ.-R. 2023b. The Web Can Be Your Oyster for Improving\nLarge Language Models. arXiv preprint arXiv:2305.10998.\nLiu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua, M.;\nPetroni, F.; and Liang, P. 2023a. Lost in the Middle: How\nLanguage Models Use Long Contexts. arXiv:2307.03172.\nLiu, T.; Zheng, X.; Chang, B.; and Sui, Z. 2021. Towards\nFaithfulness in Open Domain Table-to-text Generation from\nan Entity-centric View. arXiv:2102.08585.\nLiu, X.; Lai, H.; Yu, H.; Xu, Y .; Zeng, A.; Du, Z.; Zhang,\nP.; Dong, Y .; and Tang, J. 2023b. WebGLM: Towards An\nEfficient Web-Enhanced Question Answering System with\nHuman Preferences. arXiv:2306.07906.\nLu, P.; Peng, B.; Cheng, H.; Galley, M.; Chang, K.-W.; Wu,\nY . N.; Zhu, S.-C.; and Gao, J. 2023. Chameleon: Plug-and-\nplay compositional reasoning with large language models.\narXiv preprint arXiv:2304.09842.\nManakul, P.; Liusie, A.; and Gales, M. J. 2023. Selfcheckgpt:\nZero-resource black-box hallucination detection for genera-\ntive large language models. arXiv preprint arXiv:2303.08896.\nMeta. 2023. Wikimedia movement — Meta, discussion about\nWikimedia projects.\nOpenAI. 2022. Introducing ChatGPT. https://openai.com/\nblog/chatgpt. Accessed: 2024-02-20.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al.\n2022. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing\nSystems, 35: 27730–27744.\nSen, P.; Aji, A. F.; and Saffari, A. 2022. Mintaka: A com-\nplex, natural, and multilingual dataset for end-to-end question\nanswering. arXiv preprint arXiv:2210.01613.\nTian, R.; Narayan, S.; Sellam, T.; and Parikh, A. P. 2019.\nSticking to the facts: Confident decoding for faithful data-to-\ntext generation. arXiv preprint arXiv:1910.08684.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,\nG. 2023. LLaMA: Open and Efficient Foundation Language\nModels. arXiv:2302.13971.\nVrandeˇci´c, D.; and Kr ¨otzsch, M. 2014. Wikidata: a free\ncollaborative knowledgebase. Communications of the ACM,\n57(10): 78–85.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi,\nE.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35: 24824–24837.\nYang, Z.; Qi, P.; Zhang, S.; Bengio, Y .; Cohen, W. W.;\nSalakhutdinov, R.; and Manning, C. D. 2018. HotpotQA:\nA dataset for diverse, explainable multi-hop question answer-\ning. arXiv preprint arXiv:1809.09600.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18133\nZhang, T.; Wang, C.; Hu, N.; Qiu, M.; Tang, C.; He, X.;\nand Huang, J. 2022. DKPLM: Decomposable Knowledge-\nenhanced Pre-trained Language Model for Natural Language\nUnderstanding. arXiv:2112.01047.\nZhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu,\nQ. 2019. ERNIE: Enhanced Language Representation with\nInformative Entities. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics,\n1441–1451. Florence, Italy: Association for Computational\nLinguistics.\nZheng, C.; Li, L.; Dong, Q.; Fan, Y .; Wu, Z.; Xu, J.; and\nChang, B. 2023. Can We Edit Factual Knowledge by In-\nContext Learning? arXiv preprint arXiv:2305.12740.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18134",
  "topic": "Retrofitting",
  "concepts": [
    {
      "name": "Retrofitting",
      "score": 0.6144773960113525
    },
    {
      "name": "Computer science",
      "score": 0.5455774664878845
    },
    {
      "name": "Knowledge graph",
      "score": 0.4768346846103668
    },
    {
      "name": "Graph",
      "score": 0.4569151997566223
    },
    {
      "name": "Natural language processing",
      "score": 0.3940814137458801
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34493589401245117
    },
    {
      "name": "Data science",
      "score": 0.33336061239242554
    },
    {
      "name": "Psychology",
      "score": 0.3299660384654999
    },
    {
      "name": "Theoretical computer science",
      "score": 0.24721795320510864
    },
    {
      "name": "Engineering",
      "score": 0.2012527883052826
    },
    {
      "name": "Structural engineering",
      "score": 0.0
    }
  ]
}