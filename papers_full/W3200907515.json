{
    "title": "Spanish Biomedical Crawled Corpus: A Large, Diverse Dataset for Spanish Biomedical Language Models",
    "url": "https://openalex.org/W3200907515",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287067269",
            "name": "Carrino, Casimiro Pio",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222692909",
            "name": "Armengol-Estapé, Jordi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226782779",
            "name": "Bonet, Ona de Gibert",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4228057361",
            "name": "Gutiérrez-Fandiño, Asier",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4286943466",
            "name": "Gonzalez-Agirre, Aitor",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4209330516",
            "name": "Krallinger, Martin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746277756",
            "name": "Villegas, Marta",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2953958347",
        "https://openalex.org/W2985294119",
        "https://openalex.org/W3101860695",
        "https://openalex.org/W3096590546",
        "https://openalex.org/W3175567752",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2911489562"
    ],
    "abstract": "We introduce CoWeSe (the Corpus Web Salud Español), the largest Spanish biomedical corpus to date, consisting of 4.5GB (about 750M tokens) of clean plain text. CoWeSe is the result of a massive crawler on 3000 Spanish domains executed in 2020. The corpus is openly available and already preprocessed. CoWeSe is an important resource for biomedical and health NLP in Spanish and has already been employed to train domain-specific language models and to produce word embbedings. We released the CoWeSe corpus under a Creative Commons Attribution 4.0 International license, both in Zenodo (\\url{https://zenodo.org/record/4561971\\#.YTI5SnVKiEA}).",
    "full_text": "arXiv:2109.07765v1  [cs.CL]  16 Sep 2021\nSPA N I S H BI O M E D IC A L CR AW L E D CO R P U S : A L A R G E , D I V E R S E\nDATA S E T F O R SPA N I S H BI O M E D IC A L LA N G UAG E MO D E L S\nCasimiro Pio Carrino 1 , Jordi Armengol-Estapé 1 , Ona de Gibert Bonet 1 , Asier Gutiérrez-Fandiño 1 , Aitor\nGonzalez-Agirre 1 , Martin Krallinger 1 , and Marta V illegas 1\n1 T ext Mining Unit, Barcelona Supercomputing Center\n{casimiro.carrino,jordi.armengol,ona.degibert,asier.gutierrez,aitor .gonzalez,martin.krallinger ,marta.villegas}@bsc.es\nSeptember 17, 2021\nABSTRACT\nW e introduce CoW eSe (the Corpus W eb Salud Español), the larg est Spanish biomedical corpus to\ndate, consisting of 4.5GB (about 750M tokens) of clean plain text. CoW eSe is the result of a massive\ncrawler on 3000 Spanish domains executed in 2020. The corpus is openly available and already\npreprocessed. CoW eSe is an important resource for biomedic al and health NLP in Spanish and has\nalready been employed to train domain-speciﬁc language mod els and to produce word embbedings.\nW e released the CoW eSe corpus under a Creative Commons Attri bution 4.0 International license in\nZenodo ( https://doi.org/10.5281/zenodo.4561970).\n1 Introduction and Motivations\nTransfer learning has revolutionized virtually all tasks i n the Natural Language Processing ﬁeld [8]. It turned out to\nbe exceptionally successful when large-scale language mod els leveraging unlabelled data to perform self-supervised\nlearning were employed. T wo paradigmatic examples are the G PT [7], and BER T [3]. However, although gathering\nunlabelled data (raw text) is considerably cheaper than pro ducing annotations, obtaining high-quality text is especi ally\nchallenging in the biomedical and health domains for non-En glish languages.\nFollowing the paradigm of \"the web as a corpus\", manually cra wling websites belonging to the target domains of\ninterest is a strategy worth exploring. The CommonCrawl 1 is very a large repository of crawled websites. However,\nit needs preprocessing to extract the text relevant to the us er. The OSCAR corpus [10] was built applying language\nidentiﬁcation to CommonCrawl. In the case of the biomedical domain in English, BioBER T [5], and SciBER T [2] ag-\ngregated and processed different corpora (mostly proceedi ng from articles) to develop domain-speciﬁc BER T models.\nIn the case of biomedical data in Spanish, there is an ongoing effort to develop textual resources for the biomedical\ndomain [4, 6, 9]. However, there are still few corpora availa ble. Furthermore, a comprehensive biomedical corpus\nshould ideally allow the transfer to the even more low-resou rced sub-domains, such as the clinical one. Therefore, the\nCoW eSe represents an unprecedented effort to build the larg est biomedical and health-related corpus in Spanish to the\nbest of our knowledge. Unlike other works, we crawl the web in stead of scientiﬁc literature, providing a large-scale\ncorpus with diverse contents.\nThe remainder of this paper is organized as follows. First we describe the data collection process in Section 2, and the\npreprocessing strategy in Section 3 providing some basic st atistics of the corpus. Finally, in Section 5 we conclude\nwith some ﬁnal observations.\n1 https://commoncrawl.org/\nA P RE P RIN T - S E P T E M BE R 17, 2021\n2 Data Collection\nW e crawled the web using 3,338 manually curated links as seed s with a depth of 5. They were selected to include\ndiverse and relevant content, including websites categori zed as Sites of Interest for Health by the Carlos III Health\nInstitute (ISCIII). 2 Although the majority of the content is in Spanish; content i n Catalan, Galician and Basque have\nbeen also included.\nThe crawling was performed during the ﬁrst half of 2020, and w e exclusively scraped websites whose robots ﬁles\nallowed it, resulting in a total of 2,766 websites. The raw cr awling size is about 905GB of W ARC ﬁles. When\nextracting the text, we only considered the paragraph and he aders HTML tags. Formats other than HTML were not\nincluded.\nThe selected websites belong to at least one of the following categories:\n1. Medical communities.\n2. Scientiﬁc communities.\n3. Medical journals.\n4. Research centres.\n5. Pharmaceutical companies.\n6. Informative websites about health issues.\n7. Patient associations.\n8. Personal blogs from healthcare professionals.\n9. Hospital websites.\n10. Public health organizations.\n3 Preprocessing\nW e use the cleaning pipeline 3 introduced in [1] that is a series of customized components p erforming data parsing in\ndifferent formats, sentence splitting, language detectio n, removal of noisy and ill-formed sentences, content dedup li-\ncation and eventually output the data with their original do cument boundaries. Due to the vast amount of data, we\ndeployed the cleaning pipeline across 50 nodes of a High-Per formance Computing cluster 4.\nThe pipeline is parametrized to allow conﬁguring its action according to the nature of the data to be processed. W e\nthen modify the cleaning pipeline’s parameters to adapt the components to peculiarities of the biomedical and health\ndomains. Speciﬁcally, we increased the default language id entiﬁcation thresholds since we found that biomedical\nSpanish is often predicted as Spanish with a lower probabili ty than general domain Spanish. W e also decreased the\nallowed minimum length for each document since the overall c rawling is not big enough to be too aggressive in this\nregard. After preprocessing, we obtained a cleaned corpus o f 4.5GB of plain text from the original 905GB of W ARC\nﬁles. T able 1 shows some statistics of the cleaned corpus.\nSize (plain text) 4.5GB\nT okens (wc word count) 745.70M\nDocuments 1.58M\nSentences 32.77M\nFigure 1: Basic statistics of the clean corpus.\n4 Access\nThe CoW eSe corpus is openly available under a Creative Commo ns Attribution 4.0 International license in Zenodo 5.\n2 https://www.isciii.es/QueHacemos/Servicios/Biblioteca/Paginas/default.aspx\n3 https://github.com/TeMU-BSC/corpus-cleaner-acl\n4 https://www.bsc.es/support/MareNostrum4-ug.pdf\n5 https://doi.org/10.5281/zenodo.4561970\n2\nA P RE P RIN T - S E P T E M BE R 17, 2021\n5 Conclusions\nIn this work, we have introduced the CoW eSe corpus, the large st Spanish biomedical corpus to date. From one side, the\ndata collection process ensures vast size while gathering d iverse content. On the other side, the cleaning preprocessi ng\nproduced high-quality and easy to use textual data. W e belie ve the CoW eSe corpus will have a signiﬁcant impact for\nthe biomedical NLP community encouraging the development o f biomedical and health-related language models and\ntools in Spanish.\nAcknowledgements\nThis work was partially funded by the Spanish State Secretar iat for Digitalization and Artiﬁcial Intelligence (SEDIA)\nwithin the framework of the Plan-TL, the Future of Computing Center, a Barcelona Supercomputing Center and IBM\ninitiative (2020).\nReferences\n[1] Jordi Armengol-Estapé, Casimiro Pio Carrino, Carlos Ro driguez-Penagos, Ona de Gibert Bonet, Carme\nArmentano-Oller, Aitor Gonzalez-Agirre, Maite Melero, an d Marta V illegas. Are multilingual models the best\nchoice for moderately under-resourced languages? A compre hensive assessment for Catalan. In Findings of\nthe Association for Computational Linguistics: ACL-IJCNL P 2021 , pages 4933–4946, Online, August 2021.\nAssociation for Computational Linguistics.\n[2] Iz Beltagy, Kyle Lo, and Arman Cohan. SciBER T: A pretrain ed language model for scientiﬁc text. In Proceed-\nings of the 2019 Conference on Empirical Methods in Natural L anguage Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJ CNLP), pages 3615–3620, Hong Kong, China,\nNovember 2019. Association for Computational Linguistics .\n[3] Jacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina T outanova. BER T: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human La nguage T echnologies, V olume 1 (Long and Short\nP apers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Asso ciation for Computational Linguistics.\n[4] Aitor Gonzalez-Agirre, Montserrat Marimon, Ander Intx aurrondo, Obdulia Rabal, Marta V illegas, and Martin\nKrallinger. PharmaCoNER: Pharmacological substances, co mpounds and proteins named entity recognition\ntrack. In Proceedings of The 5th W orkshop on BioNLP Open Shared T asks , pages 1–10, Hong Kong, China,\nNovember 2019. Association for Computational Linguistics .\n[5] Jinhyuk Lee, W onjin Y oon, Sungdong Kim, Donghyeon Kim, S unkyu Kim, Chan Ho So, and Jaewoo Kang.\nBioBER T: a pre-trained biomedical language representatio n model for biomedical text mining. Bioinformatics,\n36(4):1234–1240, 09 2019.\n[6] A Miranda-Escalada, E Farré, and M Krallinger. Named ent ity recognition, concept normalization and clinical\ncoding: Overview of the cantemist track for cancer text mini ng in spanish, corpus, guidelines, methods and re-\nsults. In Proceedings of the Iberian Languages Evaluation F orum (IberLEF 2020), CEUR W orkshop Proceedings,\n2020.\n[7] Alec Radford and Karthik Narasimhan. Improving languag e understanding by generative pre-training. 2018.\n[8] Sebastian Ruder, Matthew E. Peters, Swabha Swayamdipta , and Thomas W olf. Transfer learning in natural\nlanguage processing. In Proceedings of the 2019 Conference of the North American Cha pter of the Association\nfor Computational Linguistics: T utorials , pages 15–18, Minneapolis, Minnesota, June 2019. Associat ion for\nComputational Linguistics.\n[9] Felipe Soares, Marta V illegas, Aitor Gonzalez-Agirre, Martin Krallinger, and Jordi Armengol-Estapé. Medi-\ncal word embeddings for Spanish: Development and evaluatio n. In Proceedings of the 2nd Clinical Natural\nLanguage Processing W orkshop, pages 124–133, Minneapolis, Minnesota, USA, June 2019. As sociation for\nComputational Linguistics.\n[10] Pedro Javier Ortiz Suárez, Laurent Romary, and Benoît S agot. A monolingual approach to contextualized word\nembeddings for mid-resource languages. CoRR, abs/2006.06202, 2020.\n3"
}