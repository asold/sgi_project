{
  "title": "HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization",
  "url": "https://openalex.org/W3205231586",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2110002789",
      "name": "Ye Liu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2104112628",
      "name": "Jianguo Zhang",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2171492316",
      "name": "Yao Wan",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2554586319",
      "name": "Congying Xia",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2108700898",
      "name": "Lifang He",
      "affiliations": [
        "Lehigh University"
      ]
    },
    {
      "id": "https://openalex.org/A2147553045",
      "name": "Philip Yu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2994673210",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W3035620455",
    "https://openalex.org/W4287372954",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2963204221",
    "https://openalex.org/W3156927220",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2950670227",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3209048663",
    "https://openalex.org/W4287025196",
    "https://openalex.org/W3100053428",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970066309",
    "https://openalex.org/W3101693329",
    "https://openalex.org/W2896807716",
    "https://openalex.org/W2108325777",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W3035647909",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3034353423",
    "https://openalex.org/W3035050380",
    "https://openalex.org/W3035643691",
    "https://openalex.org/W3170490008",
    "https://openalex.org/W3115933422",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3176750236",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3012871709"
  ],
  "abstract": "To capture the semantic graph structure from raw text, most existing summarization approaches are built on GNNs with a pre-trained model. However, these methods suffer from cumbersome procedures and inefficient computations for long-text documents. To mitigate these issues, this paper proposes HetFormer, a Transformer-based pre-trained model with multi-granularity sparse attentions for long-text extractive summarization. Specifically, we model different types of semantic nodes in raw text as a potential heterogeneous graph and directly learn heterogeneous relationships (edges) among nodes by Transformer. Extensive experiments on both single- and multi-document summarization tasks show that HetFormer achieves state-of-the-art performance in Rouge F1 while using less memory and fewer parameters.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 146–154\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n146\nHETFORMER : Heterogeneous Transformer with Sparse Attention for\nLong-Text Extractive Summarization\nYe Liu1, Jian-Guo Zhang1, Yao Wan2, Congying Xia1, Lifang He3, Philip S. Yu1\n1 University of Illinois at Chicago, Chicago, IL, USA\n2 Huazhong University of Science and Technology, Huhan, China\n3 Lehigh University, Bethlehem, PA, USA\n{yliu279, jzhan51, cxia8, psyu@}@uic.edu,\nwanyao@hust.edu.cn, lih319@lehigh.edu\nAbstract\nTo capture the semantic graph structure from\nraw text, most existing summarization ap-\nproaches are built on GNNs with a pre-trained\nmodel. However, these methods suffer from\ncumbersome procedures and inefﬁcient com-\nputations for long-text documents. To miti-\ngate these issues, this paper proposes H ET-\nFORMER , a Transformer-based pre-trained\nmodel with multi-granularity sparse attentions\nfor long-text extractive summarization. Specif-\nically, we model different types of seman-\ntic nodes in raw text as a potential hetero-\ngeneous graph and directly learn heteroge-\nneous relationships (edges) among nodes by\nTransformer. Extensive experiments on both\nsingle- and multi-document summarization\ntasks show that H ETFORMER achieves state-\nof-the-art performance in Rouge F1 while us-\ning less memory and fewer parameters.\n1 Introduction\nRecent years have seen a resounding success in\nthe use of graph neural networks (GNNs) on doc-\nument summarization tasks (Wang et al., 2020;\nHanqi Jin, 2020), due to their ability to capture\ninter-sentence relationships in complex document.\nSince GNN requires node features and graph struc-\nture as input, various methods, including extraction\nand abstraction (Li et al., 2020; Huang et al., 2020;\nJia et al., 2020), have been proposed for learning\ndesirable node representations from raw text. Par-\nticularly, they have shown that Transformer-based\npre-trained models such as BERT (Devlin et al.,\n2018) and RoBERTa (Liu et al., 2019) offer an\neffective way to initialize and ﬁne tune the node\nrepresentations as the input of GNN.\nDespite great success in combining Transformer-\nbased pre-trained models with GNNs, all existing\napproaches have their limitations. The ﬁrst limita-\ntion lies in the adaptation capability to long-text\ninput. Most pre-trained methods truncate longer\ndocuments into a small ﬁxed-length sequence (e.g.,\nn = 512 tokens), as its attention mechanism re-\nquires a quadratic cost w.r.t. sequence length. This\nwould lead to serious information loss (Li et al.,\n2020; Huang et al., 2020). The second limitation\nis that they use pre-trained models as a multi-\nlayer feature extractor to learn better node features\nand build multi-layer GNNs on top of extracted\nfeatures, which have cumbersome networks and\ntremendous parameters (Jia et al., 2020).\nRecently there have been several works focusing\non reducing the computational overhead of fully-\nconnected attention in Transformers. Especially,\nETC (Ravula et al., 2020) and Longformer (Belt-\nagy et al., 2020) proposed to use local-global sparse\nattention in pre-trained models to limit each token\nto attend to a subset of the other tokens (Child et al.,\n2019), which achieves a linear computational cost\nof the sequence length. Although these methods\nhave considered using local and global attentions\nto preserve hierarchical structure information con-\ntained in raw text data, their abilities are still not\nenough to capture multi-level granularities of se-\nmantics in complex text summarization scenarios.\nIn this work, we propose HETFORMER , a\nHETerogeneous transFORMER -based pre-trained\nmodel for long-text extractive summarization using\nmulti-granularity sparse attentions. Speciﬁcally, we\ntreat tokens, entities, sentences as different types\nof nodes and the multiple sparse masks as differ-\nent types of edges to represent the relations (e.g.,\ntoken-to-token, token-to-sentence), which can pre-\nserve the graph structure of the document even with\nthe raw textual input. Moreover, our approach will\neschew GNN and instead rely entirely on a sparse\nattention mechanism to draw heterogeneous graph\nstructural dependencies between input tokens.\nThe main contributions of the paper are summa-\nrized as follows: 1) we propose a new structured\npre-trained method to capture the heterogeneous\nstructure of documents using sparse attention; 2)\nwe extend the pre-trained method to longer text\n147\nCLS CLSCLS\nCLS\nSen1w1w2 wnw3 w1w2w3w4wmSen2\nw1w2\nw4w3\nw1w2w3w4wm\nw4\nwn\n(a)\tToken-to-Token\tAttention\nwm\n(b)\tSen-to-Sen\t&\tSen↔Token\tAttentions\nCLS\nCLS\nw1w2\nw4w3\nw1w2w3w4wm\nwn\nCLS CLSSen1w1w2 wnw3 w1w2w3w4Sen2w4 Ent\nEnt\nEnt\nEnt\nCLS CLSCLS\nCLS\nSen1Entw2w4Ent Entw2w3EntwmSen2\nw2\nwn\nw2w3\nwm\nwn\nw4\n(d)\tIntegrated\tAttention\nEnt\nEnt\nEnt\nCLS CLSCLS\nCLS\nSen1Entw2w4Ent Entw2w3EntwmSen2\nw2\nwn\nw2w3\nwm\nwn\nw4\nEnt\n(c)\tEntity-to-Entity\tAttention\nFigure 1: An illustration of sparse attention patterns ((a), (b), (c)) and their combination (d) in HETFORMER .\nextractive summarization instead of truncating the\ndocument to small inputs; 3) we empirically demon-\nstrate that our approach achieves state-of-the-art\nperformance on both single- and multi-document\nextractive summarization tasks.\n2 H ETFORMER on Summarization\nHETFORMER aims to learn a heterogeneous Trans-\nformer in pre-trained model for text summarization.\nTo be speciﬁc, we model different types of seman-\ntic nodes in raw text as a potential heterogeneous\ngraph, and explore multi-granularity sparse atten-\ntion patterns in Transformer to directly capture het-\nerogeneous relationships among nodes. The node\nrepresentations will be interactively updated in a\nﬁne-tuned manner, and ﬁnally, the sentence node\nrepresentations are used to predict the labels for\nextractive text summarization.\n2.1 Node Construction\nIn order to accommodate multiple granularities of\nsemantics, we consider three types of nodes: token,\nsentence and entity.\nThe token node represents the original textual\nitem that is used to store token-level information.\nDifferent from HSG (Wang et al., 2020) which ag-\ngregates identical tokens into one node, we keep\neach token occurrence as a different node to avoid\nambiguity and confusion in different contexts. Each\nsentence node corresponds to one sentence and\nrepresents the global information of one sentence.\nSpeciﬁcally, we insert an external [CLS] token\nat the start of each sentence and use it to encode\nfeatures of each tokens in the sentence. We also\nuse the interval segment embeddings to distinguish\nmultiple sentences within a document, and the posi-\ntion embeddings to display monotonical increase of\nthe token position in the same sentence. The entity\nnode represents the named entity associated with\nthe topic. The same entity may appear in multiple\nspans in the document. We utilize NeuralCoref 1\nto obtain the coreference resolution of each entity,\nwhich can be used to determine whether two ex-\npressions (or “mentions”) refer to the same entity.\n2.2 Sparse Attention Patterns\nOur goal is to model different types of relationships\n(edges) among nodes, so as to achieve a sparse\ngraph-like structure directly. To this end, we lever-\nage multi-granularity sparse attention mechanisms\nin Transformer, by considering ﬁve attention pat-\nterns, as shown in Fig. 1:token-to-token (t2t), token-\nto-sentence (t2s), sentence-to-token (s2t), sentence-\nto-sentence (s2s) and entity-to-entity (e2e).\nSpeciﬁcally, we use a ﬁxed-size window atten-\ntion surrounding each token (Fig. 1(a)) to cap-\nture the short-term t2t dependence of the context.\nEven if each window captures the short-term de-\npendence, by using multiple stacked layers of such\nwindowed attention, it could result in a large recep-\ntive ﬁeld (Beltagy et al., 2020). Because the top\nlayers have access to all input locations and have\nthe capacity to build representations that incorpo-\nrate information across the entire input.\nThe t2s represents the attention of all tokens\nconnecting to the sentence nodes, and conversely,\ns2t is the attention of sentence nodes connecting to\nall tokens across the sentence (the dark blue lines in\nFig. 1(b)). The s2s is the attention between multiple\nsentence nodes (the light blue squares in Fig. 1(b)).\nTo compensate for the limitation of t2t caused by\nusing ﬁxed-size window, we allow the sentence\nnodes to have unrestricted attentions for all these\nthree types. Thus tokens that are arbitrarily far apart\nin the long-text input can transfer information to\neach other through the sentence nodes.\nComplex topics related to the same entity may\nspan multiple sentences, making it challenging for\nexisting sequential models to fully capture the se-\n1https://github.com/huggingface/\nneuralcoref\n148\nmantics among entities. To solve this problem, we\nintroduce the e2e attention pattern (Fig. 1(c). The\nintuition is that if there are several mentions of a\nparticular entity, all the pairs of the same mentions\nare connected. In this way, we can facilitate the\nconnections of relevant entities and preserve global\ncontext, e.g., entity interactions and topic ﬂows.\nLinear Projections for Sparse Attention.In or-\nder to ensure the sparsity of attention, we create\nthree binary masks for each attention patternsMt2t,\nMts and Me2e, where 0 means disconnection and\n1 means connection between pairs of nodes. In par-\nticular, Mts is used jointly for s2s, t2s and s2t. We\nuse different projection parameters for each atten-\ntion pattern in order to model the heterogeneity of\nrelationships across nodes. To do so, we ﬁrst cal-\nculate each attention with its respective mask and\nthen sum up these three attentions together as the\nﬁnal integrated attention (Fig. 1(d)).\nEach sparse attention is calculated as: Am =\nsoftmax\n(\nQmKm⊤\n√dk\n)\nVm, m∈{t2t,ts,e 2e}. The\nquery Qm is calculated as (Mm ⊙X)Wm\nQ , where\nX is the input text embedding, ⊙represents the\nelement-wise product and Wm\nQ is the projection\nparameter. The key Km and the value Vm are cal-\nculated in a similar way as Qm, but with respect to\ndifferent projection parameters, which are helpful\nto learn better representation for heterogeneous se-\nmantics. The expensive operation of full-connected\nattention is QKT as its computational complex-\nity is related to the sequence length (Kitaev et al.,\n2020). While in HETFORMER , we follow the im-\nplementation of Longformer that only calculates\nand stores attention at the position where the mask\nvalue is 1 and this results in a linear increase in\nmemory use compared to quadratic increase for\nfull-connected attention.\n2.3 Sentence Extraction\nAs extractive summarization is more general and\nwidely used, we build a classiﬁer on each sentence\nnode representation os to select sentences from the\nlast layer of HETFORMER . The classiﬁer uses a\nlinear projection layer with the activation function\nto get the prediction score for each sentence: ˜ys =\nσ(osWo + bo), where σ is the sigmoid function,\nWo and bo are parameters of projection layer.\nIn the training stage, these prediction scores are\ntrained learned on the binary cross-entropy loss\nwith the golden labels y. In the inference stage,\nthese scores are used to sort the sentences and select\nthe top-kas the extracted summary.\n2.4 Extension to Multi-Document\nOur framework can establish the document-level re-\nlationship in the same way as the sentence-level, by\njust adding document nodes for multiple documents\n(i.e., adding the [CLS] token in front of each doc-\nument) and calculate the document↔sentence (d2s,\ns2d), document↔token (d2t, t2d) and document-\nto-document (d2d) attention patterns. Therefore, it\ncan be easily adapted from the single-document to\nmulti-document summarization.\n2.5 Discussions\nThe most relevant approaches to this work are\nLongformer (Beltagy et al., 2020) and ETC (Ravula\net al., 2020) which use a hierarchical attention\npattern to scale Transformers to long documents.\nCompared to these two methods, we formulate the\nTransformer as multi-granularity graph attention\npatterns, which can better encode heterogeneous\nnode types and different edge connections. More\nspeciﬁcally, Longformer treats the input sequence\nas one sentence with the single tokens marked\nas global. In contrast, we consider the input se-\nquence as multi-sentence units by using sentence-\nto-sentence attention, which is able to capture the\ninter-sentence relationships in the complex docu-\nment. Additionally, we introduce entity-to-entity\nattention pattern to facilitate the connection of rel-\nevant subjects and preserve global context, which\nare ignored in both Longformer and ETC. More-\nover, our model is more ﬂexible to be extended to\nthe multi-document setting.\n3 Experiments\n3.1 Datasets\nCNN/DailyMail is the most widely used bench-\nmark dataset for single-document summariza-\ntion (Zhang et al., 2019; Jia et al., 2020). The stan-\ndard dataset split contains 287,227/13,368/11,490\nsamples for train/validation/test. To be comparable\nwith other baselines, we follow the data processing\nin (Liu and Lapata, 2019b; See et al., 2017).\nMulti-News is a large-scale dataset for multi-\ndocument summarization introduced in (Fabbri\net al., 2019), where each sample is composed\nof 2-10 documents and a corresponding human-\nwritten summary. Following Fabbri et al. (2019),\nwe split the dataset into 44,972/5,622/5,622 for\ntrain/validation/test. The average length of source\n149\ndocuments and output summaries are 2,103.5 to-\nkens and 263.7 tokens, respectively. Given the N\ninput documents, we taking the ﬁrst L/N tokens\nfrom each source document. Then we concatenate\nthe truncated source documents into a sequence by\nthe original order. Due to the memory limitation,\nwe truncate input length L to 1,024 tokens. But if\nthe memory capacity allows, our model can process\nthe max input length = 4,096.\nWhile the dataset contains abstractive gold sum-\nmaries, it is not readily suited to training extractive\nmodels. So we follow the work of (Zhou et al.,\n2018) on extractive summary labeling, construct-\ning gold-label sequences by greedily optimizing\nR-2 F1 on the gold-standard summary.\n3.2 Baselines and Metrics\nWe evaluate our proposed model with the pre-\ntrained language model (Devlin et al., 2018; Liu\net al., 2019), the state-of-the-art GNN-based pre-\ntrained language models (Wang et al., 2020; Jia\net al., 2020; Hanqi Jin, 2020) and pre-trained lan-\nguage model with the sparse attention (Narayan\net al., 2020; Beltagy et al., 2020). And please check\nAppendix B for the detail.\nWe use unigram, bigram, and longest common\nsubsequence of Rouge F1 (denoted as R-1, R-1 and\nR-L) (Lin and Och, 2004)2 to evaluate the summa-\nrization qualities. Note that the experimental results\nof baselines are from the original papers.\n3.3 Implementation Detail\nOur model HETFORMER 3 is initialized\nusing the Longformer pretrained check-\npoints longformer-base-40964, which\nis further pertained using the standard masked\nlanguage model task on the Roberta check-\npoints roberta-base5 with the documents of\nmax length 4,096. We apply dropout with proba-\nbility 0.1 before all linear layers in our models.\nThe proposed model follows the Longformer-base\narchitecture, where the number of dmodel hidden\nunits in our models is set as 768, the dh hidden\nsize is 64, the layer number is 12 and the number\nof heads is 12. We train our model for 500K steps\non the TitanRTX, 24G GPU with gradient accumu-\nlation in every two steps with Adam optimizers.\n2https://pypi.org/project/rouge/\n3https://github.com/yeliu918/HETFORMER\n4https://github.com/allenai/longformer\n5https://github.com/huggingface/\ntransformers\nModel R-1 R-2 R-L\nHiBERT (Zhang et al., 2019) 42.31 19.87 38.78\nHSG (Wang et al., 2020) 42.95 19.76 39.23\nHAHsumLarge(Jia et al., 2020) * 44.67 21.30 40.75\nMatchSum (Zhong et al., 2020) 44.41 20.86 40.55\nBERTBase(Devlin et al., 2018) 41.55 19.34 37.80\nRoBERTaBase(Liu et al., 2019) 42.99 20.60 39.21\nETCBase(Narayan et al., 2020) 43.43 20.54 39.58\nLongformerBase(Beltagy et al., 2020) 43.20 20.38 39.61\nHETFORMERBase 44.55 20.82 40.37\nTable 1: Rouge F1 scores on test set of CNN/DailyMail.\n*Note that HAHsumLarge uses large verision while the\nproposed model is based on the base version.\nModel R-1 R-2 R-L\nHiBERT (Zhang et al., 2019) 44.32 15.11 29.26\nHi-MAP (Fabbri et al., 2019) 45.21 16.29 41.39\nHDSG (Wang et al., 2020) 46.05 16.35 42.08\nMatchSum (Zhong et al., 2020) 46.20 16.51 41.89\nMGsumBase(Hanqi Jin, 2020) 45.04 15.98 -\nGraphsumBase(Li et al., 2020) 46.07 17.42 -\nLongformerBase(Beltagy et al., 2020) 45.34 16.00 40.54\nHETFORMERBase 46.21 17.49 42.43\nTable 2: Rouge F1 scores on test set of Multi-News. ‘-’\nmeans that the original paper did not report the result.\nLearning rate schedule follows the strategies with\nwarming-up on ﬁrst 10,000 steps (Vaswani et al.,\n2017). We select the top-3 checkpoints according\nto the evaluation loss on validation set and report\nthe averaged results on the test set.\nFor the testing stage, we select top-3 sentences\nfor CNN/DailyMail and top-9 for Multi-News\naccording to the average length of their human-\nwritten summaries. Trigram blocking is used to\nreduce repetitions.\n3.4 Summerization Results\nAs shown in Table 1, our approach outperforms\nor is on par with current state-of-the-art baselines.\nLongformer and ETC outperforms the hierarchi-\ncal structure model using fully-connected attention\nmodel HiBERT, which shows the supreme of using\nsparse attention by capturing more relations (e.g.,\ntoken-to-sentence and sentence-to-token). Compar-\ning to the pre-trained models using sparse atten-\ntion, HETFORMER considering the heterogeneous\ngraph structure among the text input outperforms\nLongformer and ETC. Moreover, HETFORMER\nachieves competitive performance compared with\nGNN-based models, such as HSG and HAHsum.\nOur model is slightly lower than the performance\nof HAHsumlarge. But it uses large architecture (24\nlayers with about 400M parameters), while our\n150\nBERT RoBERTa Longformer Ours\nMemory Cost3,057M 3,540M 1,650M 1,979M\nTable 3: Memory cost of different pre-trained models\nmodel builds on the base model (12 layers with\nabout 170M parameters). Table 2 shows the results\nof multi-document summarization. Our model out-\nperforms all the extractive and abstractive baselines.\nThese results reveal the importance of modeling\nthe longer document to avoid serious information\nloss.\n3.5 Memory Cost\nCompared with the self-attention component requir-\ning quadratic memory complexity in original Trans-\nformers, the proposed model only calculates the po-\nsition where attention pattern mask=1, which can\nsigniﬁcantly save the memory cost. To verify that,\nwe show the memory costs of BERT, RoBERTa,\nLongformer and HETFORMER base-version model\non the CNN/DailyMail dataset with the same con-\nﬁguration (input length = 512, batch size = 1).\nFrom the results in Table 3, we can see that\nHETFORMER only takes 55.9% memory cost of\nRoBERTa model and also does not take too much\nmore memory than Longformer.\n3.6 Ablation Study\nTo show the importance of the design choices of\nour attention patterns, we tried different variants\nand reported their controlled experiment results. To\nmake the ablation study more manageable, we train\neach conﬁguration for 500K steps on the single-\ndocument CNN/DailyMail dataset, then report the\nRouge score on the test set.\nThe top of Table 4 demonstrates the impact of\ndifferent ways of conﬁguring the window sizes\nper layer. We observe that increasing the window\nsize from the bottom to the top layer leads to the\nbest performance (from 32 to 512). But the reverse\nway leads to worse performance (from 512 to 32).\nAnd using a ﬁxed window size (the average of\nwindow sizes of the other conﬁguration) leads to a\nperformance that it is in between.\nThe middle of Table 4 presents the impact of\nincorporating the sentence node in the attention pat-\ntern. In implementation, no sentence node means\nthat we delete the [CLS] tokens of the document\ninput and use the average representation of each to-\nken in the sentences as the sentence representation.\nWe observe that without using the sentence node to\nfully connect with the other tokens could decrease\nthe performance.\nThe bottom of Table 4 shows the inﬂuence of us-\ning the entity node. We can see that without the en-\ntity node, the performance will decrease. It demon-\nstrates that facilitating the connection of relevant\nsubjects can preserve the global context, which can\nbeneﬁt the summarization task.\nModel R-1 R-2 R-L\nDecreasing w (from 512 to 32) 43.98 20.33 39.39\nFixed w (=128) 43.92 20.43 39.43\nIncreasing w (from 32 to 512)44.55 20.82 40.37\nNo Sentence node 42.15 20.12 38.91\nNo Entity node 43.65 20.40 39.28\nTable 4: Top: changing window size across layers. Mid-\ndle: entity-to-entity attention pattern inﬂuence. Bottom:\nsentence-to-sentence attention pattern inﬂuence\n4 Conclusion\nFor the task of long-text extractive summarization,\nthis paper has proposed HETFORMER , using multi-\ngranularity sparse attention to represent the het-\nerogeneous graph among texts. Experiments show\nthat the proposed model can achieve comparable\nperformance on a single-document summarization\ntask, as well as state-of-the-art performance on the\nmulti-document summarization task with longer\ninput document. In our future work, we plan to\nexpand the edge from the binary type (connect or\ndisconnect) to more plentiful semantic types, i.e.,\nis-a, part-of, and others (Zhang et al., 2020).\n5 Acknowledgements\nWe would like to thank all the reviewers for their\nhelpful comments. This work is supported by\nNSF under grants III-1763325, III-1909323, III-\n2106758, and SaTC-1930941.\nReferences\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences\nwith sparse transformers. arXiv preprint\narXiv:1904.10509.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\n151\nbidirectional transformers for language understand-\ning. In Proceedings of the North American Chapter\nof ssociation for Computational Linguistics , pages\n4171–4186.\nAlexander R Fabbri, Irene Li, Tianwei She, Suyi Li,\nand Dragomir R Radev. 2019. Multi-news: A large-\nscale multi-document summarization dataset and ab-\nstractive hierarchical model. In Proceedings of the\nConference of Association for Computational Lin-\nguistics, pages 1074–1084.\nZiwei Fan, Zhiwei Liu, Jiawei Zhang, Yun Xiong, Lei\nZheng, and Philip S Yu. 2021. Continuous-time se-\nquential recommendation with temporal graph col-\nlaborative transformer. In Proceedings of ACM In-\nternational Conference on Information and Knowl-\nedge Management.\nXiaojun Wan Hanqi Jin, Tianming Wang. 2020. Multi-\ngranularity interaction network for extractive and ab-\nstractive multi-document summarization. In Pro-\nceedings of the Conference of Association for Com-\nputational Linguistics, pages 6244–6254.\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou\nSun. 2020. Heterogeneous graph transformer. In\nProceedings of the Web Conference , pages 2704–\n2710.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng\nJi, and Lu Wang. 2021. Efﬁcient attentions for long\ndocument summarization. In Proceedings of the\nNorth American Chapter of the Association for Com-\nputational Linguistics.\nLuyang Huang, Lingfei Wu, and Lu Wang. 2020.\nKnowledge graph-augmented abstractive summa-\nrization with semantic-driven cloze reward. In Pro-\nceedings of the Conference of Association for Com-\nputational Linguistics, page 5094–5107.\nRuipeng Jia, Yanan Cao, Hengzhu Tang, Fang Fang,\nCong Cao, and Shi Wang. 2020. Neural extractive\nsummarization with hierarchical attentive heteroge-\nneous graph network. In Proceedings of the Con-\nference of Neural Information Processing Systems ,\npages 3622–3631.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Pro-\nceedings of the International Conference on Learn-\ning Representations.\nWei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng\nWang, and Junping Du. 2020. Leveraging graph to\nimprove abstractive multi-document summarization.\nIn Proceedings of the Conference of Association for\nComputational Linguistics, pages 6232––6243.\nChin-Yew Lin and Franz Josef Och. 2004. Auto-\nmatic evaluation of machine translation quality us-\ning longest common subsequence and skip-bigram\nstatistics. In Proceedings of the Conference of As-\nsociation for Computational Linguistics, pages 605–\n612.\nYang Liu and Mirella Lapata. 2019a. Hierarchical\ntransformers for multi-document summarization. In\nProceedings of the Conference of Association for\nComputational Linguistics, pages 5070–5081.\nYang Liu and Mirella Lapata. 2019b. Text summariza-\ntion with pretrained encoders. In Proceedings of the\nConference of Neural Information Processing Sys-\ntems, pages 3730–3740.\nYe Liu, Yao Wan, Lifang He, Hao Peng, and Philip S\nYu. 2020. Kg-bart: Knowledge graph-augmented\nbart for generative commonsense reasoning. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence.\nYe Liu, Yao Wan, Jian-Guo Zhang, Wenting Zhao, and\nPhilip S Yu. 2021. Enriching non-autoregressive\ntransformer with syntactic and semanticstructures\nfor neural machine translation. In Proceedings of\nthe European Chapter of the Association for Com-\nputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nShashi Narayan, Joshua Maynez, Jakub Adamek,\nDaniele Pighin, Blaž Brataniˇc, and Ryan McDonald.\n2020. Stepwise extractive summarization and plan-\nning with structured transformers. In Proceedings\nof the Conference of Neural Information Processing\nSystems, page 4143–4159.\nAnirudh Ravula, Chris Alberti, Joshua Ainslie,\nLi Yang, Philip Minh Pham, Qifan Wang, Santiago\nOntanon, Sumit Kumar Sanghai, VConference\nof Association for Computational Linguisticsav\nCvicek, and Zach Fisher. 2020. Etc: Encoding\nlong and structured inputs in transformers. In Pro-\nceedings of the Conference of Neural Information\nProcessing Systems, pages 268–284.\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the Confer-\nence of Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the Conference of Neu-\nral Information Processing Systems , pages 5998–\n6008.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio.\n2017. Graph attention networks. In Proceedings of\nthe International Conference on Learning Represen-\ntations.\nDanqing Wang, Pengfei Liu, Yining Zheng, Xipeng\nQiu, and Xuanjing Huang. 2020. Heterogeneous\n152\ngraph neural networks for extractive document sum-\nmarization. In Proceedings of the Conference\nof Association for Computational Linguistics , page\n6209–6219.\nShaowei Yao, Tianming Wang, and Xiaojun Wan.\n2020. Heterogeneous graph transformer for graph-\nto-sequence learning. In Proceedings of the Confer-\nence of Association for Computational Linguistics ,\npages 7145–7154.\nSeongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo\nKang, and Hyunwoo J Kim. 2019. Graph trans-\nformer networks. In Proceedings of the Conference\nof Neural Information Processing Systems , pages\n11983–11993.\nLi Zhang, Yan Ge, and Haiping Lu. 2020. Hop-\nhop relation-aware graph neural networks. arXiv\npreprint arXiv:2012.11147.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019. Hib-\nert: Document level pre-training of hierarchical bidi-\nrectional transformers for document summarization.\nIn Proceedings of the Conference of Association for\nComputational Linguistics, page 5059–5069.\nMing Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,\nXipeng Qiu, and Xuanjing Huang. 2020. Extrac-\ntive summarization as text matching. InProceedings\nof the Conference of Association for Computational\nLinguistics.\nQingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,\nMing Zhou, and Tiejun Zhao. 2018. Neural docu-\nment summarization by jointly learning to score and\nselect sentences. In Proceedings of the Conference\nof Association for Computational Linguistics , page\n654–663.\n153\nA Background\nA.1 Graph-enhanced Summarization\nIn the recent state-of-the-art summarization models,\nthere is a trend to extract the structure from the text\nto formulate the document text as a hierarchical\nstructure or heterogeneous graph (Liu et al., 2020).\nHiBERT (Zhang et al., 2019), GraphSum (Li et al.,\n2020) and HT (Liu and Lapata, 2019a) consider\nthe word-level, sentence-level and document-level\nof the input text to formulate the hierarchical struc-\nture. MGSum (Hanqi Jin, 2020), ASGARD (Huang\net al., 2020), HSG (Wang et al., 2020) and HAH-\nSum (Jia et al., 2020) construct the source article\nas a heterogeneous graph where words, sentences,\nand entities are used as the semantic nodes and they\niteratively update the sentence nodes representation\nwhich is used to do the sentence extraction.\nThe limitation of those models is that they use\npre-trained methods as the feature-based model\nto learn the node feature and build GNN layers\nupon the node which brings more training parame-\nters than just using pre-trained methods. Compared\nwith those models, our work can achieve the same\nthing but using the lite framework. Moreover, these\nmodels typically limit inputs to n = 512 tokens\nsince the O(n2) cost of attention. Due to the long\nsource article, when applying BERT or RoBERTa\nto the summarization task, they need to truncate\nsource documents into one or several smaller block\ninput (Li et al., 2020; Jia et al., 2020; Huang et al.,\n2020).\nA.2 Structure Transformer\nHuang et al. (2021) proposed an efﬁcient encoder-\ndecoder attention with head-wise positional strides,\nwhich yields ten times faster than existing full\nattention models and can be scale to long doc-\numents. Liu et al. (2021) leveraged the syntac-\ntic and semantic structures of text to improve the\nTransformer and achieved nine times speedup. Our\nmodel focuses on the different direction to use\ngraph-structured sparse attention to capture the\nlong term dependence on the long text input. The\nmost related approaches to the work presented in\nthis paper are Longformer (Beltagy et al., 2020)\nand ETC (Ravula et al., 2020) which feature a very\nsimilar global-local attention mechanism and take\nadvantage of the pre-trained model RoBERTa. Ex-\ncept Longformer has a single input sequence with\nsome tokens marked as global (the only ones that\nuse full attention), while the global tokens in the\nETC is pre-trained with CPC loss. Comparing with\nthose two works, we formulate the heterogeneous\nattention mechanism, which can consider the word-\nto-word, word-to-sen, sen-to-word and entity-to-\nentity attention.\nA.3 Graph Transformer\nWith the great similarity between the atten-\ntion mechanism used in both Transformer\n(Vaswani et al., 2017) and Graph Attention net-\nwork (Veliˇckovi´c et al., 2017), there are several\nrecent Graph Transformer works recently. Such as\nGTN (Yun et al., 2019), HGT (Hu et al., 2020),\n(Fan et al., 2021) and HetGT (Yao et al., 2020)\nformulate the different type of the attention mecha-\nnisms to capture the node relationship in the graph.\nThe major difference between of our work and\nGraph Transformer is that the input of graph trans-\nformer is structural input, such as graph or depen-\ndence tree, but the input of our HeterFormer is\nunstructured text information. Our work is to con-\nvert the transformer to structural structure so that it\ncan capture the latent relation in the unstructured\ntext, such as the word-to-word, word-to-sent, sent-\nto-word, sent-to-sent and entity-to-entity relations.\nB Baseline Details\nExtractive Models:\nBERT (or RoBERTa) (Devlin et al., 2018; Liu\net al., 2019) is a Transformer-based model for text\nunderstanding through masking language models.\nHIBERT (Zhang et al., 2019) proposed a hierar-\nchical Transformer model where it ﬁrst encodes\neach sentence using the sentence Transformer\nencoder, and then encoded the whole document\nusing the document Transformer encoder. HSG,\nHDSG (Wang et al., 2020) formulated the input\ntext as the heterogeneous graph which contains dif-\nferent granularity semantic nodes, (like word, sen-\ntence, document nodes) and connected the nodes\nwith the TF-IDF. HSG used CNN and BiLSTM to\ninitialize the node representation and updated the\nnode representation by iteratively passing messages\nby Graph Attention Network (GAT). In the end, the\nﬁnal sentence nodes representation is used to select\nthe summary sentence. HAHsum (Jia et al., 2020)\nconstructed the input text as the heterogeneous\ngraph containing the word, named entity, and sen-\ntence node. HAHsum used a pre-trained ALBERT\nto learn the node initial representation and then\nadapted GAT to iteratively learn node hidden repre-\n154\nsentations. MGsum (Hanqi Jin, 2020) treated doc-\numents, sentences, and words as the different gran-\nularity of semantic units, and connected these se-\nmantic units within a multi-granularity hierarchical\ngraph. They also proposed a model based on GAT\nto update the node representation. ETC (Narayan\net al., 2020), and Longformer (Beltagy et al., 2020)\nare two pre-trained models to capture hierarchi-\ncal structures among input documents through the\nsparse attention mechanism.\nAbstractive Models: Hi-MAP (Fabbri et al.,\n2019) expands the pointer-generator network\nmodel into a hierarchical network and integrates\nan MMR module to calculate sentence-level scores.\nGraphsum (Li et al., 2020) leverage the graph\nrepresentations of documents by processing input\ndocuments as the hierarchical structure with a pre-\ntrained language model to generate the abstractive\nsummary.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9535247087478638
    },
    {
      "name": "Computer science",
      "score": 0.8261808156967163
    },
    {
      "name": "Transformer",
      "score": 0.7121883630752563
    },
    {
      "name": "Granularity",
      "score": 0.686080276966095
    },
    {
      "name": "Graph",
      "score": 0.5223480463027954
    },
    {
      "name": "Natural language processing",
      "score": 0.4622476100921631
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4600159823894501
    },
    {
      "name": "Information retrieval",
      "score": 0.44471243023872375
    },
    {
      "name": "Computation",
      "score": 0.4358211159706116
    },
    {
      "name": "Data mining",
      "score": 0.35521095991134644
    },
    {
      "name": "Theoretical computer science",
      "score": 0.23491260409355164
    },
    {
      "name": "Algorithm",
      "score": 0.10063236951828003
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I47720641",
      "name": "Huazhong University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I186143895",
      "name": "Lehigh University",
      "country": "US"
    }
  ],
  "cited_by": 18
}