{
  "title": "S-Pred: protein structural property prediction using MSA transformer",
  "url": "https://openalex.org/W4292057397",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5027399963",
      "name": "Yiyu Hong",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5012317253",
      "name": "Jinung Song",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5105988820",
      "name": "Junsu Ko",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5066026979",
      "name": "Juyong Lee",
      "affiliations": [
        "Kangwon National University"
      ]
    },
    {
      "id": "https://openalex.org/A5040488997",
      "name": "Woong‐Hee Shin",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2938821856",
    "https://openalex.org/W3112376646",
    "https://openalex.org/W2130479394",
    "https://openalex.org/W1991498388",
    "https://openalex.org/W2029476353",
    "https://openalex.org/W2153187042",
    "https://openalex.org/W2941112903",
    "https://openalex.org/W2905446269",
    "https://openalex.org/W2566370974",
    "https://openalex.org/W3011698511",
    "https://openalex.org/W3027554922",
    "https://openalex.org/W2963457143",
    "https://openalex.org/W2887029338",
    "https://openalex.org/W2950374603",
    "https://openalex.org/W3184265853",
    "https://openalex.org/W2791790018",
    "https://openalex.org/W2510934692",
    "https://openalex.org/W3133458480",
    "https://openalex.org/W2051210555",
    "https://openalex.org/W2557595285",
    "https://openalex.org/W2949342052",
    "https://openalex.org/W2153153865",
    "https://openalex.org/W3083777350",
    "https://openalex.org/W2558841060",
    "https://openalex.org/W3156051371",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W3178087467",
    "https://openalex.org/W3211795435",
    "https://openalex.org/W2008708467",
    "https://openalex.org/W2102245393",
    "https://openalex.org/W2158714788",
    "https://openalex.org/W2808950571",
    "https://openalex.org/W2099246483",
    "https://openalex.org/W1989511016",
    "https://openalex.org/W4281790889"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2022) 12:13891  | https://doi.org/10.1038/s41598-022-18205-9\nwww.nature.com/scientificreports\nS‑Pred: protein structural property \nprediction using MSA transformer\nYiyu Hong1, Jinung Song1, Junsu Ko1, Juyong Lee1,2 & Woong‑Hee Shin1,3,4*\nPredicting the local structural features of a protein from its amino acid sequence helps its function \nprediction to be revealed and assists in three‑dimensional structural modeling. As the sequence‑\nstructure gap increases, prediction methods have been developed to bridge this gap. Additionally, as \nthe size of the structural database and computing power increase, the performance of these methods \nhave also significantly improved. Herein, we present a powerful new tool called S‑Pred, which can \npredict eight‑state secondary structures (SS8), accessible surface areas (ASAs), and intrinsically \ndisordered regions (IDRs) from a given sequence. For feature prediction, S‑Pred uses multiple \nsequence alignment (MSA) of a query sequence as an input. The MSA input is converted to features by \nthe MSA Transformer, which is a protein language model that uses an attention mechanism. A long \nshort‑term memory (LSTM) was employed to produce the final prediction. The performance of S‑Pred \nwas evaluated on several test sets, and the program consistently provided accurate predictions. The \naccuracy of the SS8 prediction was approximately 76%, and the Pearson’s correlation between the \nexperimental and predicted ASAs was 0.84. Additionally, an IDR could be accurately predicted with an \nF1‑score of 0.514. The program is freely available at https:// github. com/ aront ier/S_ Pred_ Paper and \nhttps:// ad3. io as a code and a web server.\nProteins play an important role in biological processes, and their structures are closely linked to their functions. \nTo characterize their structures, various experimental methods, such as X-ray crystallography, nuclear magnetic \nresonance spectroscopy, and cryogenic electron microscopy have been employed. However, because experimental \nprotein conformation is difficult to obtain, the gap between the number of experimentally solved protein \nstructures and the number of determined amino acid sequences is gradually  increasing1. As of February 2022, \napproximately 225 million sequences have been compiled in the UniProt  database2, and the structures of 108 \nthousand unique proteins structures have been deposited in the Protein Data Bank (PDB) 3. Several protein \nstructure prediction algorithms have been developed and are being routinely utilized to bridge the sequence-\nstructure gap.\nSeveral methods exist for extracting protein structural features from the amino acid sequence, known as \nthe primary structure of proteins, to study its function. In 1961,  Anfinsen4 discovered that a protein’s tertiary \nstructure is encoded by its amino acid sequence. Based on this observation, numerous approaches for predicting \nthe structural properties of proteins, such as secondary structures, accessible surface areas (ASAs), and \nintrinsically disordered regions (IDRs), have been  developed5–17. These features can also be useful for protein \nstructural modeling by providing insights into local structures.\nSince the early 2010s, numerous structural feature prediction approaches have been proposed, and as \nstructural datasets expand, machine learning techniques, especially deep learning, have become more powerful. \nSPOT-1D8 uses long short-term memory (LSTM) and ResNet hybrid models to predict the eight-state secondary \nstructures (SS8), ASAs, backbone dihedral angles, and contact numbers. The program uses a position-specific \nscoring matrix (PSSM) from multiple sequence alignment (MSA) and the predicted contact map from SPOT-\nContact as input features. SPOT-Disorder 9,10 predicts IDRs by employing multiple models sequentially, such \nas IncReSeNet, LSTM, and fully linked topological segments. The software uses both PSSM and structural \ninformation predicted by SPOT-1D to predict the disordered regions. NetSurfP-2.014 uses a convolutional neural \nand LSTM from the protein sequence profile to predict the secondary structures, relative surface areas, IDRs, \nand backbone dihedral angles. MUFOLD-SS uses inception-inside-inception networks to predict the secondary \nstructure from PSSM and seven physicochemical attributes of amino  acids16.  AUCpreD17 predicts IDRs using a \nOPEN\n1Arontier Co., Seoul 06735, Republic of Korea. 2Division of Chemistry and Biochemistry, Department of Chemistry, \nKangwon National University, Chuncheon 24341, Republic of Korea. 3Department of Chemistry Education, \nSunchon National University, Suncheon 57922, Republic of Korea. 4Department of Advanced Components and \nMaterials Engineering, Sunchon National University, Suncheon 57922, Republic of Korea.  *email: whshin@\nscnu.ac.kr\n2\nVol:.(1234567890)Scientific Reports |        (2022) 12:13891  | https://doi.org/10.1038/s41598-022-18205-9\nwww.nature.com/scientificreports/\nconvolutional neural network, considering seven physicochemical properties of amino acids, predicted secondary \nstructures, solvent-accessible areas, and PSSM as input features.\nIn this paper, we present a new structural feature prediction method, S-Pred, which uses an LSTM and MSA \n Transformer18 for feature extraction from the MSA. The MSA Transformer is an unsupervised protein sequence \nlanguage model introduced by Rao et al. 18, which uses the MSA of a query sequence instead of a single amino \nacid sequence. The key attribute of this model is the use of row and column attentions for a given MSA and \nmasked language model objectives. This model was successful in predicting long-range contacts between residues. \nUltimately, this demonstrates that this protein language model is effective in extracting protein properties from \nMSA profiles. S-Pred uses the extracted features from the MSA Transformer and an LSTM to predict three \nstructural features: SS8s, ASAs, and IDRs. The results indicate that S-Pred successfully predicts structural features \naccurately, and its performance is comparable to or superior to that of other state-of-the-art programs.\nMethods\nNetwork architecture. The overall architecture of the algorithm is illustrated in Fig.  1. The input to the \nnetwork is the MSA of a query sequence. The MSA is defined as an r × c matrix, where r is the number of \nsequences, and c is the sequence length. Through the token and position embedding of the MSA Transformer, \nthe matrix is embedded into an  r × c × 768 tensor, which is the input and output of each attention  block18. The \nMSA Transformer is composed of a stack of 12 attention blocks. The attention blocks consist of three layers: two \nattention layers (row and column attention layer) with 12 attention heads and one feed-forward layer. Herein, for \neach layer, a normalization operation was performed.\nA one-dimensional (1D) feature vector for each residue of a given sequence was generated by extracting two \nfeature types from the MSA Transformer. The first was labelled as MSA features, which is the output tensor of \nthe last attention block with dimensions of r × c × 768 . From the MSA features, only the row that corresponded \nto the query sequence was selected, yielding a 1 × c × 768 dimensional tensor. The dimension of the tensor was \nfurther reduced to 1 × c × 192 using a multilayer perceptron (MLP) neural network consisting of three linear \nlayers with 768, 384, and 192 neurons.\nThe second feature was the row attention map from every attention head. The MSA Transformer is composed \nof attention layers derived from 12 blocks with 12 attention heads. Thus, 144 attention maps were collated in the \nshape of a c × c × 144 tensor. The average pooling operation was applied to the row- and column-wise tensor to \nobtain 1 × c × 144 and c × 1 × 144 tensors. The second tensor was transposed and concatenated with the first \nto yield a 1 × c × 288 tensor.\nThe aforementioned two features were further concatenated to produce a 1 × c × 480 tensor. Consequently, \neach residue for a given query sequence had a 480-dimensional feature vector. The feature tensors were \nsequentially proceeded to a set of two LSTM layers with 256 hidden units and a classification layer designed to \npredict the structural properties of each residue of a protein.\nThree independent models were trained for the three structural properties by changing only the output neuron \nsizes of the final classification layer. Here, the classification layer possessed eight output neurons for SS8 and a \nsingle output neuron for ASA and IDR prediction.\nFigure 1.  Architecture of the S-Pred method. The MSA Transformer extracts features and row attention \nmaps from the input MSA of a query sequence. Next, through a series of transformations, the MSA features \ncorresponding to the query sequence and the row attention maps are combined to 1D feature vectors. The 1D \nfeature vectors are then input in an LSTM to predict protein structural properties including, SS8, ASA, and IDR.\n3\nVol.:(0123456789)Scientific Reports |        (2022) 12:13891  | https://doi.org/10.1038/s41598-022-18205-9\nwww.nature.com/scientificreports/\nMSA generation. A procedure similar to that used for the MSA Transformer was used to generate the \nMSA of a query  protein18. HHblits 3.3.0 19 with the uniclus-ter30_2017_10 20 and  BFD21 databases were used \nto generate the MSA of the query sequence. The maximum number of sequences used in the MSA was set to \n256. If the number of homologous sequences detected by HHblits was greater than the maximum number, 256 \nsequences were selected by minimizing the diversity.\nTraining and inference. The parameters of the MSA Transformer have been fixed and described by Rao \net al.18. The parameters of the other networks (i.e., MLP , LSTM, and classification layer) were trained using a \nbatch size of 16 with gradient accumulation steps and a learning rate of 1e-3 using the RAdam  optimizer22. \nThree independent models were used to individually train the SS8, ASA, and IDR datasets. The SS8, ASA, and \nIDR datasets were classified as multi-classification, regression, and binary classification, respectively. Thus, three \ndifferent loss functions were generated including categorical cross-entropy for the SS8 data, mean squared error \nfor the ASA data, and binary cross-entropy for the IDR data. For the ASA dataset, the values were divided by 200 \nprior to training to make the values smaller. All the models were trained for approximately 15 epochs using an \nNVIDIA Quadro RTX 8000 graphics processing unit (GPU) (48 GB).\nAn MSA subsampling strategy was applied during training. This was done not only for data augmentation to \ntrain a robust model, but also to prevent the GPU from running out of memory when filled with a large MSA. \nMSA rows were randomly selected for subsampling, with a maximum of 214/c and a minimum of 16, to ensure \nthat the query sequences in the first row were always included. Large proteins with a length greater than 1023 \nresidues were discarded during training. The MSA was subsampled with 256 sequences at the inference stage by \nadding the sequence with the lowest average Hamming distance.\nDatasets for SS8 and ASA. Datasets from Hanson et al. 8 were used to train and test the SS8 and ASA \nnetworks. From the PIECES  server23, 12,450 proteins with a resolution < 2.5 Å, R-free < 1, and sequence identity \ncutoff of 25% were extracted in February 2017. The proteins were further classified into three datasets: training \n(10,200 proteins), validation (1000 proteins), and test (1250 proteins) datasets. The authors generated another \ntest set composed of 250 proteins, which were deposited in the  PDB3 between 1/1/2018 and 7/16/2018 under \nidentical conditions of resolution, R-free, and sequence identity. The two test sets were labeled as TEST2016 \n(collected February 2017) and TEST2018.\nIn addition, S-Pred’s SS8 prediction module was further tested on the Critical Assessment of protein Structure \nPrediction 13 (CASP13) dataset. To compare with other programs, the target list was kept the same as the DNSS2 \n paper24. Since the CASP13 was held in 2018 and our training set was culled in Feb. 2017, there is no overlap \nbetween the two datasets. The proteins were categorized as template-based modeling (TBM) and free modeling \n(FM) following the official classification.\nDatasets for IDR. Datasets from the SPOT-Disorder study were used to obtain the IDR prediction  model9. \nZhou et al. collected 4229 proteins from DisProt 5.0 25, composed of 4157 X-ray crystallography structures and \n72 fully disordered proteins. These data were divided into 2700 proteins for training, 300 proteins for validation, \nand 1229 proteins for testing. Proteins that contained more than 1023 amino acids were eliminated because the \nMSA Transformer could not treat large proteins. Thus, the remaining 2689 proteins were used for training, 300 \nproteins were used for validation, and 1225 proteins were used for testing. To compare with methods other than \nSPOT-Disorder, the IDR prediction model was also tested on three independent datasets: SL250,  DisProt22810, \nand the Critical Assessment of Protein Intrinsic Disorder (CAID)  dataset26. As its name suggests, SL250 is \ncomposed of 250 proteins and re-annotated DisProt proteins that include reliable disordered and ordered \nregions. DisProt228 contains 228 proteins collated from DisProt 7.0 but not included in DisProt 5.0; therefore, \nthe proteins were not included in any training, validation, or test sets. The last dataset used was the  CAID26. \nCAID is a blind IDR prediction experiment organized by Dr. Tosatto of Padua University. The dataset was \nconstructed using 646 proteins that were annotated in the DisProt database from June 2018 to November 2018 \nand have been evaluated using 32 IDR prediction programs. The complete CAID prediction data were collected, \nand only the sequences predicted by all 33 predictors (32 from CAID and S-Pred) were retained (550 proteins) \nto provide a reasonable performance comparison.\nAlphaFold2 dataset. The aim of S-Pred is to predict the structural features of a protein from its sequence \nto study the molecule’s structure and function. In the recent CASP ,  AlphaFold227 (AF2) showed the highest \nperformance, which was around twice as high as the second-placed  group28. Additionally, the AF2 predicted \nmodel is used to resolve the phasing problem in many proteins. Sequence-based structural feature prediction \ntechniques might become obsolete due to AF2’s strength. We gathered a dataset called the AF2 dataset to see \nif S-Pred might provide any further value beyond AF2 prediction. 2176 protein structures deposited in PDB \n database3 from 4/26/2022 to 6/28/2022 were collected. To remove the redundancy, PIECES  server23 were used \nwith the conditions of sequence identity < 25%, resolution < 2.5 Å, R value < 0.25, and sequence length between \n50 and 1000, resulting 263 chains. We searched AlphaFold Protein Structure  Database29 (Accessed 7/12/2022) \nwith the UniProt  ID2 of the chains and downloaded 92 structures. The structural features of predicted structures \nand corresponding crystal structures were calculated using  DSSP30 and compared with S-Pred prediction results \nfrom their sequences. The qualities of models were measured by using TM-align31.\nEvaluation metrics and performance comparison. Because the S-Pred method predicts three different \nfeatures (i.e., SS8, ASA, and IDR), several metrics and methods were utilized to evaluate its performance. For \nSS8, accuracy was evaluated to compare overall performance against previous data obtained from Hanson \n4\nVol:.(1234567890)Scientific Reports |        (2022) 12:13891  | https://doi.org/10.1038/s41598-022-18205-9\nwww.nature.com/scientificreports/\net  al.8, Uddin et  al.11, and Guo et  al. 24. To further investigate the performance of S-Pred on each secondary \nstructure state, the precision, recall, and F1-score were calculated for the TEST2016 dataset. Pearson’s correlation \ncoefficient (PCC) was used to assess the performance of the ASA model and was compared with that obtained \nin the study by Hanson et al.8. The IDR model was evaluated by calculating the area under the receiver operating \ncurve (AUC ROC), Matthew’s correlation coefficient (MCC), and F1-score. The performance of the S-Pred IDR \nprediction model was compared with that of several methods presented in the SPOT-Disorder2 10 and CAID \n studies26.\nResults and discussion\nSS8 prediction. The secondary structure of a protein is defined by the local structure of its peptide backbone. \nIn general, the local backbone conformation is categorized into three states (SS3): helix (H), strand (E), and \ncoil (C). Kabsch and  Sander30 introduced a more detailed SS8 classification: α-helix (H),  310-helix (G), π-helix \n(I), β-strand (E), isolated β-bridge (B), turn (T), bend (S), and others (C). H, G, and I in the SS8 classification \ncorrespond to the helix states in SS3, E and B are members of the strand states of SS3, and the remaining (T, S, C) \nare classified as the coil states of SS3. As the secondary structure provides information on the local conformation, \nSS8 may provide information for structure prediction that is more useful than the information provided by SS3 \nwhen used as a classifier.\nThe accuracy of the S-Pred method in classifying the validation dataset was 0.780, which is comparable with \nthat of the state-of-the-art SS8 prediction methods, SPOT-1D (0.776) 8 and SAINT (0.782) 11, using the same \ntraining, test, and validation datasets. The SPOT-1D and SAINT programs use identical input features: 50 features \nof a PSSM derived from PSI-BLAST32 and  HHblits19, seven physicochemical properties such as Van der Waal’s \nvolume and polarizability, and a contact prediction map from SPOT-Contact33. SPOT-1D operates by employing \nan ensemble of LSTM networks in a bidirectional recurrent neural network and ResNet hybrid  models8, whereas \nSAINT utilizes an ensemble of a self-attention mechanism with Deep3I  network11. In contrast, S-Pred only \nrequires an MSA constructed from HHblits and uses a single model to predict the SS8s.\nThe performance of S-Pred on the TEST2016 and TEST2018 datasets for SS8 prediction is presented in \nTable 1. S-Pred demonstrates a prediction accuracy of 0.776 for the TEST2016 set, which ranks 2nd among \nthe tested methods in SPOT-1D 8 and  SAINT11 papers. The prediction accuracy of S-Pred is similar to that of \nSAINT, which is the best-performing method, and outperforms the SPOT-1D method. Additionally, S-Pred \noutperforms SPOT-1D-base, which utilizes an ensemble collection of nine models trained without contact map \nprediction, and SAINT-base, which uses a single model. With the TEST2018 set, the S-Pred method achieves \nthe highest accuracy (0.764), whereas the accuracy of SAINT is slightly lower (0.761). Interestingly, S-Pred is the \nbest-performing program in terms of the accuracy for SS3 prediction (0.865, Supporting Information Table S1). \nAn example of SS8 prediction using the 7,8-dihydro-8-oxoguanine triphosphatase sequence (PDB ID: 5WS7) \nfrom the TEST2016 dataset is illustrated in Fig. 2.\nTo provide a better understanding, we further investigated the performance of the individual secondary \nstructure state from the TEST2016 dataset with regards to precision, recall, and F1-score, and the results \nare presented in Table  2. The F1-score is a harmonic average of the precision and recall, balancing the two \nmetrics, thus widely used for imbalanced data. As can be observed, the S-Pred method performs better than \nthe other methods in predicting four of the eight secondary structure classes (H, E, G, and C). Interestingly, our \nmethodology produces an F1-score higher than 0.6 for states with more than 30,000 residues (H, E, T, and C) \nin the dataset. For non-ordinary states, such as B, G, I, and S, the program generates F1-scores lower than 0.5. \nSAINT and SPOT-1D, which use two-dimensional contact map information as the primary additional input \nfeatures, perform better than S-Pred for non-ordinary secondary structure states. Because the secondary structure \nis defined by the local hydrogen bond patterns of the backbone, two-dimensional contact map information may \nbe useful in predicting non-ordinary secondary structures. This predictive trend on non-ordinary states was also \nTable 1.  Comparison of the SS8 accuracy obtained from several methods on the TEST2016 and TEST2018 \ndatasets. The data acquired from other methods except DNSS2 were obtained from Hanson et al.8 and Uddin \net al.11. The method that performs the best is represented in bold. a Data adapted from Hanson et al.8. b Data \nadapted from Uddin et al.11.\nMethod TEST2016 TEST2018\nS-Pred 0.776 0.764\nSPIDER-3-Singlea N/A 0.598\nDNSS2 N/A 0.655\nRaptorXa N/A 0.704\nPOTTER-5a N/A 0.732\nMUFOLD-SSb 0.756 0.737\nNetSurfP-2.0b 0.757 0.730\nSPOT-1D-basea 0.760 0.743\nSPOT-1 Da 0.771 0.754\nSAINT-baseb 0.762 0.745\nSAINTb 0.777 0.761\n5\nVol.:(0123456789)Scientific Reports |        (2022) 12:13891  | https://doi.org/10.1038/s41598-022-18205-9\nwww.nature.com/scientificreports/\nobserved in studies conducted by Wang et al.12 and Zhang et al.13 that used deep learning for SS8 prediction. This \nresult suggests that S-Pred may improve its performance when contact information is included.\nS-Pred also showed a good performance on the CASP13 benchmark (Table  3). Among the tested methods, \nS-Pred showed the second highest accuracy in the All and TBM category (0.724 and 0.738, respectively), \ncomparable to DNSS2, the top-performed method (0.727 and 0.753 for the All and TBM category). Interestingly, \nour program performed best in the FM category (0.714), which is composed of proteins with few available \ntemplates to model the structure. The difference in accuracy between the TBM and FM categories is only 0.024, \nthe smallest gap among the tested methods. This implies that S-Pred could perform consistently although there \nare few or no structural templates.\nASA prediction. A key structural feature of a protein residue is its ASA. This metric is regarded as a \nsignificant factor in protein folding and  stability34. The ASA metric can be used to classify the residue as buried \nFigure 2.  S-Pred SS8 predictions mapped on the 7,8-dihydro-8-oxoguanine triphosphatase (PDB ID: 5WS7) \nstructure. The color codes for α-helix (H),  310-helix (G), β-strand (E), turn (T), bend (S), and others (C) are red, \norange-red, blue, green, light green, and lime green, respectively. It should be noted that none of the residues \nwere predicted as π-helices (I) or β-bridges (B).\nTable 2.  Precision, recall, and F1-score for individual secondary structure states obtained from the TEST2016 \ndataset. The numbers in parentheses represent frequencies of the secondary structure states. The data from \nall other methods except S-Pred were obtained from Uddin et al.11. The method that performs the best is \nrepresented in bold. a S-Pred (SP). b SAINT (SA). c SPOT-1D (S1). d NetSurfP-2.0 (NS). e MUFOLD-SS (MU).\nLabel\nPrecision Recall F1-score\nSPa SAb S1c NSd MUe SPa SAb S1c NSd MUe SPa SAb S1c NSd MUe\nH (98139) 0.886 0.879 0.884 0.885 0.868 0.953 0.948 0.941 0.933 0.943 0.918 0.912 0.911 0.908 0.904\nB (3018) 0.660 0.760 0.671 0.650 0.609 0.101 0.104 0.097 0.070 0.115 0.176 0.183 0.169 0.126 0.193\nE (62657) 0.859 0.843 0.852 0.822 0.850 0.874 0.887 0.878 0.903 0.842 0.866 0.864 0.865 0.861 0.846\nG (10770) 0.588 0.581 0.547 0.536 0.519 0.394 0.390 0.375 0.334 0.348 0.471 0.467 0.445 0.412 0.417\nI (47) 0.235 1.000 1.000 0.044 0.857 0.085 0.447 0.128 0.426 0.383 0.125 0.618 0.227 0.079 0.529\nT (32297) 0.622 0.663 0.641 0.615 0.631 0.648 0.618 0.612 0.585 0.586 0.635 0.639 0.626 0.599 0.608\nS (23466) 0.674 0.639 0.624 0.579 0.589 0.286 0.367 0.337 0.278 0.313 0.402 0.466 0.438 0.376 0.409\nC (57483) 0.640 0.648 0.631 0.613 0.607 0.748 0.731 0.741 0.704 0.727 0.690 0.687 0.682 0.655 0.662\nTable 3.  Prediction of SS8 on CASP13 dataset. All values except S-Pred were taken from Guo et al.24. The \nmethod that performs the best is represented in bold.\nMethod All TBM FM\nS-Pred 0.724 0.738 0.714\nSSPro5.2 0.644 0.664 0.640\nDeepCNF 0.665 0.689 0.653\nMUFOLD 0.667 0.684 0.661\nPorter 5 0.677 0.709 0.657\nDNSS2 0.727 0.753 0.710\n6\nVol:.(1234567890)Scientific Reports |        (2022) 12:13891  | https://doi.org/10.1038/s41598-022-18205-9\nwww.nature.com/scientificreports/\ninside a protein or exposed on the surface. Thus, for protein structure prediction, the ASA metric is crucial in \nindicating the location of the residue. The S-Pred predictive performance was evaluated by the PCC for the \npredicted and real ASA values, which were calculated using the DSSP  algorithm30. The PCC for the validation \nset was 0.850, which was higher than that of SPOT-1D (0.823) with the same dataset.\nTable 4 compares the performance on the TEST2016 and TEST2018 datasets. As can be observed, PCC \nvalues of S-Pred for the TEST2016 (0.843) and TEST2018 (0.831) datasets are larger in magnitude than the \nPCCs obtained from the other computational methods on the same datasets. Similar to the validation set, \nS-Pred produces larger PCCs than SPOT-1D. However, unlike SPOT-1D, which requires PSSM as well as \nphysicochemical features and contact map information, S-Pred only requires an MSA from HHblits and \nprovides improved performance. Zhou et al. compared the performance of the retrained SPIDER3 and SPOT-1D \nalgorithms by calculating PCCs on the same training  set8 because the size of the SPOT-1D training set is twice \nas large as that of SPIDER3 (4590 proteins). The PCC obtained from SPIDER3 increased to 0.796, from 0.76, \nbut was still lower than that obtained from S-Pred. NetSurfP-2.0, which uses an MSA from  HHblits14 similar to \nS-Pred, also generates lower PCCs than our computational method. An example of ASAs predicted by S-Pred is \ndisplayed on chain A of PDB ID 6FC6, the nuclear fusion protein BIK1, from TEST2018 (Fig. 3).\nSPOT-1D is an improvement on the SPOT-1D-base method because it incorporates the contact map \ninformation from SPOT-Contact. As the contact information contains the number of residues surrounding the \ntarget amino acid, it may provide additional information on ASAs. This implies that the performance of S-Pred in \npredicting ASAs can potentially be improved if the contact information is used as a supplementary input feature.\nIDR prediction. IDRs and intrinsically disordered proteins (IDPs) do not possess fixed three-dimensional \nstructures. IDPs and IDRs are involved in various biological processes because they can adopt multiple \nconformations and bind to several protein partners. According to a recent  study35, eukaryotic proteomes are \nmore disordered than other domains, with a 20.5% disordered content. In addition, IDRs are linked to various \nhuman diseases, such as cancers and Alzheimer’s disease; therefore, they have been employed as potential \ndrug targets. From a structural prediction perspective, eliminating the IDRs before modeling can be helpful in \nexcluding regions that cannot be successfully modeled. Thus, IDR prediction is crucial for both the biological \nfunction prediction and computational modeling of proteins.\nThe S-Pred model produced AUC ROC values of 0.929 and 0.914 for the validation and test sets, respectively. \nFor comparison, two additional independent datasets (i.e., SL250 and DisProt228) were employed (Table  5). \nThe performance of S-Pred on these datasets was evaluated using two metrics, AUC  ROC and MCC. S-Pred and \nthe other state-of-the-art methods performed comparably on both test sets. For the SL250 set, S-Pred ranked \n2nd for both the AUC ROC and MCC metrics (0.884 and 0.650, respectively), whereas for the DisProt228 dataset, \nit ranked 2nd for AUC ROC (0.797) and 4th for MCC metrics (0.457). An example of IDR prediction using the \nTable 4.  Prediction of ASAs and comparison among methods. PCCs  between the predicted and experimental \nvalues. The PCCs obtained from methods except S-Pred were used from the study conducted by Hanson et al.8. \nThe method that performs the best is represented in bold.\nMethod TEST2016 TEST2018\nS-Pred 0.843 0.831\nSPIDER-3 0.787 0.768\nNetSurfP-2.0 N/A 0.801\nSPOT-1D-base 0.813 0.799\nSPOT-1D 0.816 0.803\nFigure 3.  S-Pred ASA predictions mapped on the structure of nuclear fusion protein BIK1 (PDB ID: 6FC6). \nThe residues are represented using a gradient color scale from cyan (buried) to maroon (exposed).\n7\nVol.:(0123456789)Scientific Reports |        (2022) 12:13891  | https://doi.org/10.1038/s41598-022-18205-9\nwww.nature.com/scientificreports/\nDisProt ID DP00874 (actin-related protein 7) is illustrated in Fig.  4. As can be observed, S-Pred predicts three \ndisordered regions that are in a location similar to that of the annotated regions.\nThe top-performing method in both datasets was SPOT-Disorder2. SPOT-Disorder2 is a profile-based IDR \nprediction method that utilizes PSSM profiles from PSI-BLAST and  HHblits10. In addition, it also employs \n23 structural properties predicted by SPOT-1D (SS8, SS3, four sine and cosine values of backbone dihedral \nangles (θ, τ, φ, and Ψ), ASA, contact number, and two half-sphere exposure values) as input  features10. The final \nprediction of SPOT-Disorder2 is based on a combination of five models. By contrast, S-Pred uses only an MSA \nas the input feature and a single model. Zhou et al. examined the effects of the structural input features predicted \nby SPOT-1D10. The AUC ROC of Model 0 on the Mobi9414 test set was 0.943; however, it reduced to 0.920 when \nthe features from SPOT-1D were omitted. This implies that the performance of S-Pred in IDR prediction can be \npotentially improved by incorporating structural features from the SS8 and ASA modules.\nCAID IDR prediction. The final benchmark was the CAID experiment. A probability threshold for \nestimating the IDR residue of each method was optimized to acquire the highest F1-score (F max), the same as \nin the original benchmark  study26. After relabeling the residues with the thresholds, all metrics were examined. \nTable 6 presents the results obtained from the CAID experiment. As can be observed, among the 33 predictors, \nS-Pred ranks 2nd for all metrics: 0.514, 0.791, and 0.384 for the F1-score, AUC ROC, and MCC, respectively. The \ntop-performing method is  fIDPnn15, which is a meta-predictor of DFLpred, DisoRDPbind, and fMoRFpred \nusing a neural network. Thus, S-Pred is the best performing method among non-meta-predictors.\nThe organizers of the CAID experiment also tested the predictors to determine whether they could predict \nfully disordered proteins, also referred to as IDPs. IDPs are targets of interest because they are difficult to be \nstructurally characterized experimentally, but they possess unique biological functions. In the CAID benchmark \nset, proteins were considered as IDPs if the percentage of disordered annotated residues was higher than 95%. \nUsing these criteria, 41 of 550 proteins were labeled as IDPs. Under the same conditions, the IDR prediction \nprogram predicted IDPs after labeling all amino acids as an input. The performance of the IDP prediction is \npresented in Supporting Information Table S2. As can be observed, S-Pred provides the most accurate IDP \nprediction (F1-score: 0.637; MCC: 0.609). Even if a more rigid IDP definition (99%) is used, the result does \nTable 5.  Comparison of IDR predictions by several methods. The AUC ROC and MCC metrics for methods \nother than S-Pred were obtained from the study conducted by Hanson et al.10. For the DisProt228 dataset, only \nsequence-profile-based methods are presented. The method that performs the best is represented in bold.\nSet Program AUC ROC MCC\nSL250\nS-Pred 0.884 0.650\ns2D 0.737 0.360\nMobiDB-lite 0.818 0.534\nDISOPRED2 0.825 0.508\nESpritz-N 0.833 0.454\nESpritz-D 0.843 0.555\nDISOPRED3 0.857 0.596\nESpritz-X 0.859 0.566\nNetSurfP-2.0 0.869 0.572\nACUpreD 0.869 0.605\nSPINE-D 0.875 0.599\nSPOT-Disorder 0.893 0.629\nSPOT-Disorder2 0.901 0.679\nDisProt228\nS-Pred 0.797 0.457\ns2D 0.727 0.267\nAUCpreD 0.748 0.434\nJRONN 0.753 0.379\nESpritz-D 0.759 0.379\nMFDp2 0.768 0.371\nDISOPRED 0.771 0.406\nMobiDB-lite 0.772 0.422\nNetSurfP-2.0 0.774 0.421\nEspritz-N 0.776 0.432\nMFDp 0.776 0.357\nSPINE-D 0.786 0.423\nSPOT-Disorder 0.792 0.462\nESpritz-X 0.796 0.476\nSPOT-Disorder2 0.809 0.499\n8\nVol:.(1234567890)Scientific Reports |        (2022) 12:13891  | https://doi.org/10.1038/s41598-022-18205-9\nwww.nature.com/scientificreports/\nnot substantially change. S-Pred is still the best IDP predictor with an F1-score and MCC of 0.652 and 0.624, \nrespectively.\nComparison with AF2 models. To investigate whether S-Pred could still provide valuable information \nbeyond AF2, the most powerful tertiary structure prediction method, we collected 92 crystal structures from \nPDB that do not share high sequence identities (< 25%). S-Pred predicted SS8 and ASA from their UniProt \nsequences and compared them with AF2 models extracted from AlphaFold Protein Structure Database.\nS-Pred reported that ASA PCC was 0.844 and SS8 accuracy was 0.778. In contrast, AF2 models outperformed \nS-Pred, scoring 0.900 and 0.915 for ASA PCC and SS8 accuracy, respectively. It is natural that AF2 has higher \naccuracy than S-Pred since AF2 might infer structural information from templates and contact maps, while \nS-Pred only has MSA as an input without structural information. When we examine the individual proteins, \nS-Pred performed better than AF2 in 7 (SS8) and 18 (ASA) cases out of 92 proteins. It’s interesting to note that the \nAF2 models for the proteins that S-Pred surpassed are not very accurate. The seven proteins with greater S-Pred \nSS8 predictions have an average TM-score of 0.747, while the remaining proteins have an average TM-score of \n0.959. The 18 proteins that S-Pred outperformed in ASA prediction have an average TM-score of 0.841, compared \nto 0.968 for the other proteins. In four protein cases, S-Pred performed better in both SS8 and ASA than AF2 \nmodels. The four proteins have a mean TM-score of 0.682, while the other proteins have an average TM-score \nof 0.955. This result implies that the quality of AF2 models might be improved by S-Pred prediction.\nIn addition, even though the AF2 model has improved accuracy, S-Pred is still valuable due of its quickness. \nS-Pred takes roughly 10 min to complete the input MSA construction for proteins with around 300 amino acids, \nand less than a second to complete the prediction. On the other hand, AF2 takes around 10 min for modeling \nand 4 h for MSA with a single  GPU36. Protein structural characteristics might be quickly predicted and applied \nto studies like IDP prediction using S-Pred.\nConclusions\nPrediction of the structural properties of proteins from an amino acid sequence can aid in the prediction of \nthe structure and biological function of proteins. In this paper, we report a novel structural feature prediction \nprogram called S-Pred. The program utilizes the MSA Transformer to obtain input features and predicts three \nstructural features (SS8, ASA, and IDR) using an LSTM. This study demonstrated that our program successfully \nFigure 4.  S-Pred IDR prediction for the DisProt ID DP00874 protein. (A) DisProt annotations. Disordered \nannotated residues are highlighted in brown. (B) S-Pred prediction. The probability of disorder is represented \nas a function of the background color intensity. Thus, the higher probability of disorder is portrayed as a darker \nbrown color.\n9\nVol.:(0123456789)Scientific Reports |        (2022) 12:13891  | https://doi.org/10.1038/s41598-022-18205-9\nwww.nature.com/scientificreports/\npredicted all the three features, and the performance was better than or comparable with that of other state-of-\nthe-art algorithms.\nThe benchmark result also provided useful information for improving the performance. For SS8 prediction, \nS-Pred failed to predict non-ordinary secondary structure states, such as isolated β-bridges and π-helices. In \ncontrast, the SAINT and SPOT-1D methods successfully predicted these states because contact prediction was \nused as an input feature. For IDP prediction, SPOT-Disorder2, which demonstrated better performance in both \nthe SL250 and DisProt228 benchmark sets, employed structural features predicted by SPOT-1D. Further studies \nshould investigate approaches to improve the performance of S-Pred by incorporating components or modules \nfrom other prediction programs.\nData availability\nThe datasets analyzed during current study are available at https:// doi. org/ 10. 5281/ zenodo. 68736 54.\nCode availability\nThe code can be accessed at https:// github. com/ aront ier/S_ Pred_ Paper.\nReceived: 17 May 2022; Accepted: 8 August 2022\nTable 6.  CAID benchmark results. Raw predictions were obtained from Necci et al.26. The threshold value of \neach method for labeling IDR residues was optimized to obtain a Fmax. The method that performs the best is \nrepresented in bold.\nMethod F1-score AUC ROC MCC\nSPred 0.514 0.791 0.384\nfIDPnn 0.521 0.813 0.390\nSPOT-Disorder2 0.507 0.780 0.378\nfIDPln 0.504 0.794 0.367\nSPOT-Disorder 0.499 0.769 0.367\nRawMSA 0.496 0.791 0.357\nSPOT-Disorder-Single 0.488 0.769 0.348\nAUCpreD 0.483 0.762 0.346\nAUCpreD-np 0.481 0.761 0.335\nESpritz-D 0.479 0.775 0.332\nMobiDB-lite 0.473 0.745 0.325\nIUPred-long 0.473 0.752 0.324\nIUPred2A-short 0.473 0.752 0.324\nPredisorder 0.472 0.753 0.322\nDisoMine 0.472 0.771 0.323\nIsUnstruct 0.471 0.756 0.321\nIUPred-short 0.471 0.751 0.321\nIUPred2A-long 0.471 0.751 0.321\nESpritz-X 0.471 0.752 0.321\nVSL2B 0.464 0.746 0.311\nDISOPRED-3 0.463 0.727 0.313\nJRONN 0.454 0.736 0.297\nESpritz-N 0.447 0.724 0.286\nDynaMine 0.437 0.719 0.271\nPyHCA 0.432 0.709 0.264\nFoldUnfold 0.422 0.655 0.249\nDisEMBL-465 0.413 0.695 0.239\nS2D-1 0.401 0.668 0.216\nS2D-2 0.387 0.653 0.192\nDisEMBL-HL 0.375 0.657 0.174\nDisPredict-2 0.368 0.634 0.158\nGlobPlot 0.358 0.625 0.147\nDFLpred 0.322 0.411 -0.029\n10\nVol:.(1234567890)Scientific Reports |        (2022) 12:13891  | https://doi.org/10.1038/s41598-022-18205-9\nwww.nature.com/scientificreports/\nReferences\n 1. Zheng, W ., Zhang, C., Bell, E. W . & Zhang, Y . I-TASSER gateway: A protein structure and function prediction server powered by \nXSEDE. Future Gener. Comput. Syst. 99, 73–85 (2019).\n 2. The UniProt Consortium. UniProt: The universal protein knowledgebase in 2021. Nucleic Acids Res. 49, D480-489 (2021).\n 3. Berman, H. M. et al. The Protein Data Bank. Nucleic Acids Res. 28(1), 235–242 (2000).\n 4. Anfinsen, C. B., Harber, E., Sela, M. & White, F . H. The kinetics of formation of native ribonuclease during oxidation of the reduced \npolypeptide chain. Proc. Natl. Acad. Sci. USA 47, 1309–1314 (1961).\n 5. Drozdetskiy, A., Cole, C., Procter, J. & Barton, G. J. JPred4: A protein secondary structure prediction server. Nucleic Acids Res. 43, \nW389–W394 (2015).\n 6. Jones, D. T. Protein secondary structure prediction based on position-specific scoring matrices. J. Mol. Biol. 292, 195–202 (1999).\n 7. Buchan, D. W . A. & Jones, D. T. The PSIPRED protein analysis workbench: 20 years on. Nucleic Acids Res. 47, W402–W407 (2019).\n 8. Hanson, J., Paliwal, K., Litfin, T., Y ang, Y . & Zhou, Y . Improving prediction of protein secondary structure, backbone angles, solvent \naccessibility and contact numbers by using predicted contact maps and an ensemble of recurrent and residual convolutional neural \nnetworks. Bioinformatics 35, 2403–2410 (2019).\n 9. Hanson, J., Y ang, Y ., Paliwal, K. & Zhou, Y . Improving protein disorder prediction by deep bidirectional long short-term memory \nrecurrent neural networks. Bioinformatics 33, 685–692 (2017).\n 10. Hanson, J., Paliwal, K., Litfin, T. & Zhou, Y . SPOT-disorder 2: Improved protein intrinsic disorder prediction by ensembled deep \nlearning. Genom. Proteom. Bioinform. 17(6), 645–656 (2019).\n 11. Uddin, M. R., Mahbub, S., Rahman, M. S. & Bayzid, M. S. SAINT: Self-attention augmented inception-inside-inception network \nimproves protein secondary structure prediction. Bioinformatics 36, 4599–4608 (2020).\n 12. Wang, S., Peng, J., Ma, J. & Xu, J. Protein secondary structure prediction using deep convolutional neural fields. Sci. Rep. 6, 18962. \nhttps:// doi. org/ 10. 1038/ srep1 8962 (2016).\n 13. Zhang, B., Li, J. & Lü, Q. Prediction of 8-state protein secondary structures by a novel deep learning architecture. BMC Bioinform. \n19, 293. https:// doi. org/ 10. 1186/ s12859- 018- 2280-5 (2018).\n 14. Klausen, M. S. et al. NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning. Proteins 87, \n520–527 (2019).\n 15. Hu, G. et al. fIDPnn: Accurate intrinsic disorder prediction with putative propensities of disorder functions. Nat. Commun.  12, \n4438. https:// doi. org/ 10. 1038/ s41467- 021- 24773-7 (2021).\n 16. Feng, C., Sheng, Y . & Xu, D. MUFOLD-SS: New deep inception-inside-inception networks for protein secondary structure \nprediction. Proteins 86, 592–598 (2018).\n 17. Wang, S., Ma, J. & Xu, J. AUCpreD: Proteome-level protein disorder prediction by AUC-maximized deep convolutional neural \nfields. Bioinformatics 32, i672–i679 (2016).\n 18. Rao, R. et al. MSA Transformer. Preprint at https:// www. biorx iv. org/ conte nt/ 10. 1101/ 2021. 02. 12. 43085 8v1 (2021).\n 19. Remmert, M., Biegert, A., Hauser, A. & Söding, J. HHblits: Lightning-fast iterative protein sequence searching by HMM-HMM \nalignment. Nat. Methods 9, 173–175 (2012).\n 20. Mirdita, M. et al. Uniclust databases of clustered and deeply annotated protein sequences and alignments. Nucleic Acids Res. 45, \nD170-176 (2017).\n 21. Steinegger, M., Mirdita, M. & Söding, J. Protein-level assembly increases protein sequence recovery from metagenomic samples \nmanyfold. Nat. Methods 16, 603–606 (2019).\n 22. Liu, L. et al. On the variance of the adaptive learning rate and beyond. https:// arxiv. org/ abs/ 1908. 03265 (2020).\n 23. Wang, G. & Dunbrack, R. L. PISCES: A protein sequence culling server. Bioinformatics 19, 1589–1591 (2003).\n 24. Guo, D., Hou, J. & Cheng, J. DNSS2: Improved ab initio protein secondary structure prediction using advanced deep learning \narchitectures. Proteins 89, 207–217 (2021).\n 25. Piovesan, D. et al. DisProt 7.0: A major update of the database of disordered proteins. Nucleic Acids Res. 45, D219–D227 (2017).\n 26. Necci, M., Piovesan, D., CAID Predictors, DisProt Curators & Tosatto, S. C. E. Critical assessment of protein intrinsic disorder \nprediction. Nat. Methods 18, 472–481 (2021).\n 27. Jumper, J. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583–589 (2021).\n 28. Pereira, J. et al. High-accuracy protein structure prediction in CASP14. Proteins 89, 1687–1699 (2021).\n 29. Varadi, M. et al. AlphaFold Protein Structure Database: Massively expanding the structural coverage of protein-sequence space \nwith high-accurate models. Nucleic Acids Res. 50, D439–D444 (2022).\n 30. Kabsch, W . & Sander, C. Dictionary of protein secondary structure: Pattern recognition of hydrogen-bonded geometrical features. \nBiopolymers 22, 2577–2637 (1983).\n 31. Zhang, Y . & Skolnick, J. TM-align: A protein structure alignment algorithm based on the TM-score. Nucleic Acids Res. 33, 2302–\n2309 (2005).\n 32. Altschul, S. F . et al. Gapped BLAST and PSI-BLAST: A new generation of protein database search programs. Nucleic Acids Res. 25, \n3389–3402 (1997).\n 33. Hanson, J., Paliwal, K., Litfin, T., Y ang, Y . & Zhou, Y . Accurate prediction of protein contact maps by coupling residual two-\ndimensional bidirectional long short-term memory with convolutional neural networks. Bioinformatics 34, 4039–4045 (2018).\n 34. Ali, S. A., Hassan, M. I., Islam, A. & Ahmad, F . A review of methods available to estimate solvent-accessible surface areas of soluble \nproteins in the folded and unfolded states. Curr. Protein Pept. Sci. 15, 456–476 (2014).\n 35. Peng, Z. et al. Exceptionally abundant exceptions: Comprehensive characterization of intrinsic disorder in all domains of life. Cell. \nMol. Life Sci. 72, 137–151 (2015).\n 36. Mirdita, M. et al. ColabFold: Making protein folding accessible to all. Nat. Methods 19, 679–682 (2022).\nAcknowledgements\nThis research was supported by the Bio & Medical Technology Development Program of the National Research \nFoundation (NRF) funded by the Korean Government (MSIT) (No. NRF-2022M3E5F3081268). W . H. S. \nacknowledges support from the NRF of Korea with a Grant funded by the Korean Government (MSIT) (No. \n2020R1F1A1075998, 2020R1A4A1016695).\nAuthor contributions\nW .H.S. conceived the study. Y .H. developed the algorithm and wrote the code. J.K. generated the MSAs. J.S. \ncollected and performed a benchmark on the AF2 dataset. All authors analyzed the results. W .H.S. and Y .H. \ndrafted the manuscript. W .H.S. edited and finalized the manuscript. All authors have reviewed the manuscript.\nCompeting interests \nThe authors declare no competing interests.\n11\nVol.:(0123456789)Scientific Reports |        (2022) 12:13891  | https://doi.org/10.1038/s41598-022-18205-9\nwww.nature.com/scientificreports/\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 022- 18205-9.\nCorrespondence and requests for materials should be addressed to W .-H.S.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.45529913902282715
    },
    {
      "name": "Property (philosophy)",
      "score": 0.4497770667076111
    },
    {
      "name": "Transformer",
      "score": 0.4377533197402954
    },
    {
      "name": "Computational biology",
      "score": 0.33660441637039185
    },
    {
      "name": "Biology",
      "score": 0.22663652896881104
    },
    {
      "name": "Engineering",
      "score": 0.07658377289772034
    },
    {
      "name": "Electrical engineering",
      "score": 0.07249736785888672
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}