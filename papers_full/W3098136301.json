{
  "title": "On Extractive and Abstractive Neural Document Summarization with Transformer Language Models",
  "url": "https://openalex.org/W3098136301",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2769850835",
      "name": "Jonathan Pilault",
      "affiliations": [
        "Polytechnique Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A2127931115",
      "name": "Raymond Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2244871444",
      "name": "Sandeep Subramanian",
      "affiliations": [
        "Université de Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A2168619732",
      "name": "Chris Pal",
      "affiliations": [
        "Université de Montréal",
        "Canadian Institute for Advanced Research",
        "Polytechnique Montréal"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2953280096",
    "https://openalex.org/W2182959134",
    "https://openalex.org/W2579653291",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W2964165364",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2574535369",
    "https://openalex.org/W1826672328",
    "https://openalex.org/W2962849707",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1989420837",
    "https://openalex.org/W2307381258",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2962944953",
    "https://openalex.org/W2952138241",
    "https://openalex.org/W2909737760",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2963545005",
    "https://openalex.org/W2467173223",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2251023345",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W1525595230",
    "https://openalex.org/W3101913037",
    "https://openalex.org/W2793613791",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962985882",
    "https://openalex.org/W2963348916",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work. Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9308–9319,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n9308\nOn Extractive and Abstractive Neural Document Summarization with\nTransformer Language Models\nJonathan Pilault∗1,2,3, Raymond Li∗1, Sandeep Subramanian∗1,2,4 and Christopher Pal1,2,3,4,5\n1Element AI\n2Mila, 3Polytechnique Montreal, 4University of Montreal, 5Canada CIFAR AI Chair\nfirstname.lastname@elementai.com\nAbstract\nWe present a method to produce abstractive\nsummaries of long documents that exceed sev-\neral thousand words via neural abstractive\nsummarization. We perform a simple extrac-\ntive step before generating a summary, which\nis then used to condition the transformer lan-\nguage model on relevant information before\nbeing tasked with generating a summary. We\nalso show that this approach produces more ab-\nstractive summaries compared to prior work\nthat employs a copy mechanism while still\nachieving higher ROUGE scores. We pro-\nvide extensive comparisons with strong base-\nline methods, prior state of the art work as\nwell as multiple variants of our approach in-\ncluding those using only transformers, only\nextractive techniques and combinations of the\ntwo. We examine these models using four dif-\nferent summarization tasks and datasets: arXiv\npapers, PubMed papers, the Newsroom and\nBigPatent datasets. We ﬁnd that transformer\nbased methods produce summaries with fewer\nn-gram copies, leading to n-gram copying\nstatistics that are more similar to human gener-\nated abstracts. We include a human evaluation,\nﬁnding that transformers are ranked highly for\ncoherence and ﬂuency, but purely extractive\nmethods score higher for informativeness and\nrelevance. We hope that these architectures\nand experiments may serve as strong points of\ncomparison for future work.1\n1 Introduction\nAutomatic text summarization is the process of\ncompressing a document while preserving key in-\nformation content and meaning. This process is\noften achieved through extractive or abstractive\n∗ Authors contributed equally to this work\n1Note: The abstract above was collaboratively written by\nthe authors and one of the models presented in this paper based\non an earlier draft of this paper.\nNeural Document Summarization with SentencePointer Networks and Transformer Language Models\nSandeep Subramanian1,2,3,⇤, Raymond Li1,⇤, Jonathan Pilault1,2,4,⇤,Christopher Pal1,2,4,51Element AI,2Montréal Institute for Learning Algorithms,3Université de Montréal,4École Polytechnique de Montréal,5Canada CIFAR AI Chair1{jonathan.pilault}@elementai.com\nAbstractWe demonstrate that Transformer language models are extremely promising atsummarizing long texts, and provide a new approach to deep summarization thatcan be used to generate more \"abstractive\" summaries. We show that our approachproduces more abstractive summaries than state-of-the-art methods without a copymechanism. We provide an application to text summarization of the arXiv andPubMed datasets, and show that our model outperforms other popular summa-rization techniques. We also discuss a simple neural extractive model based onpointers networks trained on documents and their salient sentences. We show thatthis model can be used to augment Transformer language models to generate bettersummarization results.Note: The abstract above was generated by one of themodels presented in this paper , as a summary of this paper .\n1 IntroductionLanguage models (LMs) are trained to estimate the joint probability of an arbitrary sequence ofwords or characters using a large corpus of text. They typically factorize the joint distribution oftokensp(x1,x2...xn)into a product of conditional probabilitiesQnip(xi|x<i). It is possible to usen-gram based models to estimate these conditional probabilities via counts, relying on Markovianassumptions. However, Markovian assumptions and the curse of dimensionality make it harder forn-gram LMs to model long range dependencies and learn smooth functions that can learn similaritiesbetween words in the vocabulary. This has led to a preference for recurrent or feed-forward neurallanguage models (Bengio et al.,2003;Mikolov et al.,2010) in recent years due to to their ability tolearn expressive conditional probability distributions (Merity et al.,2017;Radford et al.,2019).The sequence-to-sequence (seq2seq) paradigm (Sutskever et al.,2014) uses language models thatlearn the conditional probability of one sequence given another. Here, a language model servesas a “decoder” that is typically conditioned on a representation of an input sequence produced byan encoder neural network. These types of encoder-decoder architectures have been particularlysuccessful when applied to problems such as machine translation (Bahdanau et al.,2014) andabstractive summarization (Rush et al.,2015). The encoder and conditional decoder language modelsare often paramaterized as recurrent neural networks (RNNs). Attention mechanisms (Bahdanauet al.,2014) are used in the decoder to provide more informative conditioning on the representationsproduced by the encoder and to ease gradient ﬂow into the encoder. RNNs however, are limited bytheir sequential nature, making them 1) difﬁcult to optimize and learn for long sequences with longrange dependencies (Hochreiter,1998;Pascanu et al.,2013), and 2) hard to parallelize on modernhardware like GPUs, limiting their scalability.⇤Equal contribution, order determined by coin ﬂip\nPreprint. Sent to peer review, May 2019.\nNeural Document Summarization with SentencePointer Networks and Transformer Language Models\nSandeep Subramanian1,2,3,⇤, Raymond Li1,⇤, Jonathan Pilault1,2,4,⇤,Christopher Pal1,2,4,51Element AI,2Montréal Institute for Learning Algorithms,3Université de Montréal,4École Polytechnique de Montréal,5Canada CIFAR AI Chair1{jonathan.pilault}@elementai.com\nAbstractWe demonstrate that Transformer language models are extremely promising atsummarizing long texts, and provide a new approach to deep summarization thatcan be used to generate more \"abstractive\" summaries. We show that our approachproduces more abstractive summaries than state-of-the-art methods without a copymechanism. We provide an application to text summarization of the arXiv andPubMed datasets, and show that our model outperforms other popular summa-rization techniques. We also discuss a simple neural extractive model based onpointers networks trained on documents and their salient sentences. We show thatthis model can be used to augment Transformer language models to generate bettersummarization results.Note: The abstract above was generated by one of themodels presented in this paper , as a summary of this paper .\n1 IntroductionLanguage models (LMs) are trained to estimate the joint probability of an arbitrary sequence ofwords or characters using a large corpus of text. They typically factorize the joint distribution oftokensp(x1,x2...xn)into a product of conditional probabilitiesQnip(xi|x<i). It is possible to usen-gram based models to estimate these conditional probabilities via counts, relying on Markovianassumptions. However, Markovian assumptions and the curse of dimensionality make it harder forn-gram LMs to model long range dependencies and learn smooth functions that can learn similaritiesbetween words in the vocabulary. This has led to a preference for recurrent or feed-forward neurallanguage models (Bengio et al.,2003;Mikolov et al.,2010) in recent years due to to their ability tolearn expressive conditional probability distributions (Merity et al.,2017;Radford et al.,2019).The sequence-to-sequence (seq2seq) paradigm (Sutskever et al.,2014) uses language models thatlearn the conditional probability of one sequence given another. Here, a language model servesas a “decoder” that is typically conditioned on a representation of an input sequence produced byan encoder neural network. These types of encoder-decoder architectures have been particularlysuccessful when applied to problems such as machine translation (Bahdanau et al.,2014) andabstractive summarization (Rush et al.,2015). The encoder and conditional decoder language modelsare often paramaterized as recurrent neural networks (RNNs). Attention mechanisms (Bahdanauet al.,2014) are used in the decoder to provide more informative conditioning on the representationsproduced by the encoder and to ease gradient ﬂow into the encoder. RNNs however, are limited bytheir sequential nature, making them 1) difﬁcult to optimize and learn for long sequences with longrange dependencies (Hochreiter,1998;Pascanu et al.,2013), and 2) hard to parallelize on modernhardware like GPUs, limiting their scalability.⇤Equal contribution, order determined by coin ﬂip\nPreprint. Sent to peer review, May 2019.\nFigure 1: Our approach for abstractive summarization of a\nscientiﬁc article. An older version of this paper is shown as the\nreference document. First, a sentence pointer network extracts\nimportant sentences from the paper. Next, these sentences are\nprovided along with the whole scientiﬁc article to be arranged\nin the following order: Introduction, extracted Sentences, ab-\nstract & the rest of the paper. A transformer language model\nis trained on articles organized in this format. During infer-\nence, the introduction and the extracted sentences are given\nto the language model as context to generate a summary. In\ndomains like news and patent documents, the introduction can\nbe replaced by the entire document.\ntechniques. Extractive summarization is the strat-\negy of selecting a subset of words, phrases or sen-\ntences from the input document to form a sum-\nmary. Abstractive summarization consists of creat-\ning sentences summarizing content and capturing\nkey ideas and elements of the source text, usually\ninvolving signiﬁcant changes and paraphrases of\ntext from the original source sentences. While ex-\ntractive summarization is able to preserve saliency,\nthe broader ﬂow or coherency of the multiple sen-\ntences forming the summary can be less natural\ncompared to a human generated summary. On the\nother hand, abstractive methods should produce co-\nherent summaries without copying sentences verba-\ntim while remaining faithful to statements asserted\nin the input document.\nRecent work by (Radford et al., 2019) (GPT-\n2) has demonstrated that Transformer Language\n9309\nModels (TLMs) trained on web text can inadver-\ntently learn to perform abstractive summarization,\nsince a large crawl of web documents may con-\ntain some documents which have a “tl;dr” token\nfollowed by a summary. We are interested here in\nexplicitly conﬁguring autoregressive transformer\nmodels to generate summaries in an intentional\nand focused manner. Since summaries or abstracts\ntypically appear at the beginning of a document, a\nmodel trained from such web-crawl data does not\nenforce strong conditioning on the text to be sum-\nmarized. Our tests using models naively trained on\nweb-crawl data yielded summarization quality far\nbelow baseline methods. However, in this paper\nwe explore what can be achieved through simply\nordering the passages of an input text, correctly\nstructuring the task deﬁnition and training proce-\ndure. We also examine the impact of combining\nthis approach with simple but high quality extrac-\ntive techniques.\nWhile pure language models can be applied\nto short input documents, memory considerations\nmake it difﬁcult to scale to long documents. Fur-\nther, as high quality extractive summarization meth-\nods illustrate, much of the content of a long docu-\nment is not needed to create a summary. For these\nreasons we also explore a hybrid approach which\ncombines an extractive and abstractive approach.\nWe achieve this by stepping away from the classical\nend-to-end sequence-to-sequence paradigm, using\nan initial extractive step that reduces the amount\nof context for a subsequent abstractive step (see\nﬁgure 1). Such an approach could be thought of\nas a form of hard attention. Moreover, we show\nthat such a paradigm works even for datasets where\nthe entire input can ﬁt in memory, i.e. see Table 4\nand 5. We take an approach whereby we restruc-\nture the input to a TLM by reordering the document\nand inserting standardized delimiters to identify the\nintroduction, our extracted sentences, the abstract\nor summary and the rest-of-the-article. With our\nmethod, the resulting TLM can focus its attention\non the relevant content and its model complexity\non the summarization task.\nIn general, as we shall detail in our experiments\nbelow, we ﬁnd that TLMs are surprisingly effec-\ntive at summarizing long documents, outperform-\ning typical seq2seq approaches, even without using\ncopying/pointing mechanisms, an encoder or ad-\nditional losses. Our contribution consists of an\nextensive set of large scale experiments comparing\nour hybrid extractive and abstractive approach to\nlong document summarization with different vari-\nants of our model, strong and simple baselines as\nwell as with state-of-the-art summarization mod-\nels (see section 3.2 for a complete description of\ncomparisons). We examine these models through\nROUGE scores, through a study of the amount of\nn-gram copying performed by different models, as\nwell as through a human evaluation using a stan-\ndard protocol. We ﬁnd that our hybrid approach\nyields results that surpass current state-of-the-art\nresults on several metrics of these evaluations.\nWe see our extensive experimentation and the\nwide variety of evaluation protocols provided here\nas being a key part of the contribution provided by\nthis work and we hope that the analysis, insights\nand models here will serve as strong yet simple\nbaselines for future comparison and research.\n2 Related Work\nThe earliest attempts at automatic summarization\nfocused on extractive techniques, which ﬁnd words\nor sentences in a document that capture its most\nsalient content. Recently, with advances in dis-\ntributed representations of words, phrases and sen-\ntences, researchers have proposed to use these to\ncompute similarity scores. Such techniques were\nfurther reﬁned by Nallapati et al. (2016b); Cheng\nand Lapata (2016); Chen and Bansal (2018) with\nencoder-decoder architectures - the representations\nlearned by the encoder are used to choose the most\nsalient sentences. Cheng and Lapata (2016) and\nNallapati et al. (2016b) trained encoder-decoder\nneural networks as a binary classiﬁer to determine\nif each sentence in a document should belong to\nthe extractive summary or not. Chen and Bansal\n(2018) use a pointer network (Vinyals et al., 2015)\nto sequentially pick sentences from the document\nthat comprise its extractive summary. Such tech-\nniques however heavily rely on the span of words\nfrom the input document.\nHuman summarizers have four common\ncharacteristics. They are able to (1) interpret a\nsource document, (2) prioritize the most important\nparts of the input text, (3) paraphrase key concepts\ninto coherent paragraphs and (4) generate diverse\noutput summaries. While extractive methods\nare arguably well suited for identifying the most\nrelevant information, such techniques may lack\nthe ﬂuency and coherency of human generated\nsummaries. Abstractive summarization has shown\n9310\nthe most promise towards addressing points (3)\nand (4) above. Abstractive generation may produce\nsentences not seen in the original input document.\nMotivated by neural network success in machine\ntranslation experiments, the attention-based\nencoder decoder paradigm has recently been\nwidely studied in abstractive summarization (Rush\net al., 2015; Nallapati et al., 2016a; Chopra et al.,\n2016). The advantages of extractive, abstractive\nand attention-based models were ﬁrst combined\nin (Gu et al., 2016; Gulcehre et al., 2016) with\na copy mechanism for out-of-vocabulary words\npresent in the source document. Similarly, (See\net al., 2017) used the attention scores to calculate\nthe probability of generating vs copying a word.\nThe most similar approach to our hybrid extrac-\ntive and abstractive technique is that of Chen and\nBansal (2018); Gehrmann et al. (2018); Hsu et al.\n(2018); Liu et al. (2018). In such set-ups, an ex-\ntractor ﬁrst selects salient sentences from the input.\nThen, an abstractive summarizer rewrites extracted\nsentences into a ﬁnal summary. Our framework\nhas a few advantages over previous methods. 1),\nwe explore high capacity transformer LMs akin\nto Radford et al. (2019) as our abstractive sum-\nmarizer, which results in grammatical and ﬂuent\ngenerations 2), our language modeling formulation\nof the problem allows us to easily “recycle” the\ninput document and use it additional in-domain\ndata for LM training. 3) We improve over previous\napproaches without the use of a copy mechanism,\nwhich results in fewer n-gram copies from the in-\nput document. Liu et al. (2018) generate Wikipedia\narticles given references to source material and\nextracted sentences. They rank the importance of\nparagraphs found in the reference material based on\ntechniques such as TextRank (Mihalcea and Tarau,\n2004), a graph based ranking technique. In con-\ntrast, the extractive methods we use here are trained\ndiscriminatively using an extractive abstract as the\ntarget that is generated using an oracle. Wikipedia\narticle synthesis also necessarily combines poten-\ntially redundant information from multiple docu-\nments that is relatively speciﬁc and less abstractive\ncompared to the task of writing the abstract of a\nscientiﬁc paper. As seen in Figure 2, human gen-\nerated (ground-truth) abstractive summaries in our\ndatasets actually have very little word overlap with\nthe source document.\n3 Framework\nOur model comprises two distinct trainable com-\nponents: 1) an extractive model, comprising a hier-\narchical encoder that outputs sentence representa-\ntions, used to either point to or classify sentences\nin the input, and 2) a transformer language model,\nconditioned on the extracted sentences as well as a\npart of or the entire input document.\n3.1 Extractive Models\nWe describe the two neural extractive models used\nin this section. We used different types of extrac-\ntion techniques to demonstrate the TLM model sen-\nsitivity to the extracted sentences. For instance, the\nSentence Pointer performs much better on the arxiv\ndataset (see table 2) but the classiﬁer is stronger on\nthe Pubmed dataset (see table 3).\nHierarchical Seq2seq Sentence Pointer Our ex-\ntractive model is similar to the sentence pointer\narchitecture developed by (Chen and Bansal, 2018)\nwith the main difference being the choice of en-\ncoder. We use a hierarchical bidirectional LSTM\nencoder with word and sentence level LSTMs while\n(Chen and Bansal, 2018) use a convolutional word\nlevel encoder for faster training and inference. The\ndecoder is in both cases is an LSTM.\nThe procedure to determine ground-truth ex-\ntraction targets is similar to previous work (Nal-\nlapati et al., 2017): the ground truth is deter-\nmined by computing the average ROUGE1,2,L\nscore of each document sentence against each\nsummary sentence. Considering the input docu-\nment as a list of N sentences D = (S1,...,S N )\nand the target summary as a list of M sen-\ntences T = (S′\n1,...,S ′\nM ), our heuristic provides\nN ×M scores, such that: SCORESextraction =\n{1\n3\n∑\nr∈1,2,L ROUGEr(Si,S′\nj)|Si ∈D; S′\nj ∈T}.\nSince single sentence extraction may not always\ncontain the same information content as a target\nsummary, we extended the number ground-truth\nextraction sentences per output summary sentence\nto two. This is done by choosing the top 2 sen-\ntences in Dthat have the highest SCORESextraction\nwith respect to a given sentence inT. The resulting\n2M ordered sentences are used as context in the\nTLM. The TLM beneﬁts from a more structured\nand larger context from the extractive summariza-\ntion model during training.\nFirst, the “sentence-encoder” or token-level\nRNN is a bi-directional LSTM (Hochreiter and\nSchmidhuber, 1997) encoding each sentence. The\n9311\nlast hidden state of the last layer from the two direc-\ntions produces sentence embeddings: (s1,..., sN ),\nwhere N is the number of sentences in the docu-\nment. The sentence-level LSTM or the “document\nencoder”, another bi-directional LSTM, encodes\nthis sequence of sentence embeddings to produce\ndocument representations: (d1,..., dN ).\nThe decoder is an autoregressive pointer LSTM\ntaking the sentence-level LSTM hidden state of the\npreviously extracted sentence as input and predict-\ning the next extracted sentence. Let it the index\nof the previous extracted sentence at time step t.\nThe input to the decoder issit .The decoder’s output\nis computed by an attention mechanism from the\ndecoder’s hidden stateht over the document repre-\nsentations (d1,..., dN ). We used the dot product\nattention method from (Luong et al., 2015). The\nattention weights at produce a context vector ct,\nwhich is then used to compute an attention aware\nhidden state ˜ht.\nThe attention weights at are used as output prob-\nability distribution over the document sentences,\nof the choice for the next extracted sentence. The\nmodel is trained to minimize the cross-entropy of\npicking the correct sentence at each decoder time\nstep. At inference, we use beam-search to generate\nthe extracted summary.\nSentence Classiﬁer As with the pointer network,\nwe use a hierarchical LSTM to encode the docu-\nment and produce a sequence of sentence repre-\nsentations d1,..., dN where N is the number of\nsentences in the document. We compute a ﬁnal\ndocument representation as follows:\nd = tanh\n(\nbd + Wd\n1\nN\nN∑\ni=1\ndi\n)\n(1)\nwhere bd and Wd are learnable parameters. Fi-\nnally, the probability of each sentence belonging to\nthe extractive summary is given by:\noi =σ\n(\nWo\n[di\nd\n]\n+ bo\n)\n(2)\nwhere σ is the sigmoid activation function. The\nmodel is trained to minimize the binary cross-\nentropy loss with respect to the sentences in the\ngold-extracted summary.\nModel details and training parameters are in-\ncluded in the appendix.\n3.2 Transformer Language Models (TLM)\nInstead of formulating abstractive summarization\nas a seq2seq problem using an encoder-decoder\narchitecture, we only use a single transformer lan-\nguage model that is trained from scratch, with ap-\npropriately “formatted” data (see ﬁgure 1, we also\ndescribe the formatting later in this section).\nWe use a transformer (Vaswani et al., 2017) lan-\nguage model (TLM) architecture identical to Rad-\nford et al. (2019). Our model has 220M parameters\nwith 20 layers, 768 dimensional embeddings, 3072\ndimensional position-wise MLPs and 12 attention\nheads. The only difference in our architectures (to\nour knowledge) is that we do not scale weights at\ninitialization. We trained the language model for 5\ndays on 16 V100 GPUs on a single Nvidia DGX-2\nbox. We used a linear ramp-up learning rate sched-\nule for the ﬁrst 40,000 updates, to maximum learn-\ning rate of2.5×e−4 followed by a cosine annealing\nschedule to 0 over the next 200,000 steps with the\nAdam optimizer. We used mixed-precision training\n(Micikevicius et al., 2017) with a batch size of 256\nsequences of 1024 tokens each.\nIn order to get an unconditional language model\nto do abstractive summarization, we can use the\nfact that LMs are trained by factorizing the joint\ndistribution over words autoregressively. In other\nwords, they typically factorize the joint distribution\nof tokens p(x1,x2 ...x n) into a product of con-\nditional probabilities ∏n\ni p(xi|x<i). We therefore\norganize the training data for our models such that\nthe ground-truth summary follows the information\nused by the model to generate a summary. As such,\nwe can model the joint distribution of the document\nand the summary during training, and sample from\nthe conditional distribution of the summary given\ndocument when we wish to perform inference.\nWhen dealing with extremely long documents\nthat may not ﬁt into a single window of tokens seen\nby a transformer language model, such as an en-\ntire scientiﬁc article, we use its introduction as a\nproxy for having enough information to generate\nan abstract (summary) and use the remainder of\nthe paper as in domain language model training\ndata (Fig 1). In such cases, we organize the arXiv\nand PubMed datasets as follows: 1) the paper intro-\nduction, 2) extracted sentences from the sentence\npointer model, 3) the abstract, and 4) the rest of the\npaper. This ensures that at inference time, we can\nprovide the language model the paper introduction\nand the extracted sentences as conditioning to gen-\n9312\nerate its abstract. We found that using the ground\ntruth extracted sentences during training and the\nmodel extracted sentences at inference performed\nbetter than using the model extracted sentences ev-\nerywhere. On other datasets, the paper introduction\nwould be the entire document. In such case, the\nrest of the paper does not exist and is therefore not\nincluded.\nWe use a special token to indicate the start of\nthe summary and use it at test time to signal to the\nmodel to start generating the summary. The rest\nof the article is provided as additional in-domain\ntraining data for the LM. The entire dataset is seg-\nmented into non-overlapping examples of 1,024\ntokens each. We use “topk” sampling at inference\n(Fan et al., 2018; Radford et al., 2019), withk= 30\nand a softmax temperature of 0.7 to generate sum-\nmaries.\n4 Results and Analysis\nDatasets We experiment with four different\nlarge-scale and long document summarization\ndatasets - arXiv, PubMed (Cohan et al., 2018),\nbigPatent (Sharma et al., 2019) and Newsroom\n(Grusky et al., 2018a). Statistics are reported in\nTable 1.\nDataset #Documents\nComp\nRatio\nSum\nLen\nDoc\nLen\narXiv 215,913 39.8 292.8 6,913.8\nPubMed 133,215 16.2 214.4 3,224.4\nNewsroom 1,212,726 43.0 30.4 750.9\nBigPatent 1,341,362 36.4 116.5 3,572.8\nTable 1: Statistics from Sharma et al. (2019) for the\ndatasets used in this work - The number of docu-\nment/summary pairs, the ratio of the number of words\nin the document to the abstract and the number of\nwords in the summary and document.\nData preprocessing Both our extractive and ab-\nstractive models use sub-word units computed us-\ning byte pair encoding (Sennrich et al., 2015) with\n40,000 replacements. To address memory issues\nin the sentence pointer network, we only keep 300\nsentences per article, and 35 tokens per sentence.\nEvaluation We evaluate our method using full-\nlength F-1 ROUGE scores (Lin, 2004) and re-used\nthe code from (Cohan et al., 2018) for this purpose.\nAll ROUGE numbers reported in this work have a\n95% conﬁdence interval of at most 0.24.\nComparison We compare our results to several\npreviously proposed extractive, abstractive and\nmixed summarization models on ROUGE scores.\nROUGE scores tend to measure lexical overlap (Ng\nand Abrecht, 2015) which favors extractive meth-\nods of summarization. Since ROUGE scores do not\ncapture system summary ﬂuency and readability\n(which typically does not favor abstractive summa-\nrization), we also include a human evaluation. For\nthis reason, Tables 2, 3, 4, 5 have a “Type” column\nto inform the reader on the type model evaluated\n(Ext=extractive, Mix=mixed and Abs=abstractive).\nAll prior results reported on the arXiv and Pubmed\nbenchmark are obtained from Cohan et al. (2018),\nexcept for the Bottom-up model2 (Gehrmann et al.,\n2018). Similarly, prior results for the BigPatent\ndataset are obtained from (Sharma et al., 2019)\nand Newsroom from (Grusky et al., 2018a) and\n(Mendes et al., 2019). These methods include\nLexRank (Erkan and Radev, 2004), SumBasic (Van-\nderwende et al., 2007), LSA (Steinberger and Jezek,\n2004), Attention-Seq2Seq (Nallapati et al., 2016a;\nChopra et al., 2016), Pointer-Generator Seq2Seq\n(See et al., 2017), Discourse-aware, which is a hi-\nerarchical extension to the pointer generator model,\n(Cohan et al., 2018), Sent-rewriting (Chen and\nBansal, 2018), RNN-Ext (Chen and Bansal, 2018),\nExconsumm (Mendes et al., 2019).\nWe present our main results on summarizing\narXiv and PubMed papers in tables 2, 3. TLM+I+E\n(G,M) sets a new state-of-the-art on Arxiv, Pubmed\nand bigPatent datasets on abstractive summariza-\ntion ROUGE scores. Our extractive models are\nable to outperform previous extractive baselines on\nboth the arXiv and Pubmed datasets. Our extractive\ntechniques also score higher than our abstractive\ntechniques on arXiv and Pubmed. Again, ROUGE\ndoes not capture all aspects of a summary’s quality\nsuch as ﬂuency and coherence. For instance, previ-\nous work that have used RL to maximize ROUGE\nscores have concluded that ”RL has the highest\nROUGE-1 and ROUGE-L scores, it produces the\nleast readable summaries” (Paulus et al., 2017).\nOur TLM conditioned on the extractive summary\nproduced by our best extractive model (TLM-I+E\n(G,M)) outperforms prior abstractive/mixed results\non the arXiv, Pubmed and bigPatent datasets, ex-\ncept on ROUGE-L.\nOn Newsroom, our TLM model performs close\nto 7 times better than the other purely abstractive\nmodel (Seq2Seq with attention). We achieve better\nperformance than the pointer generator even on the\n2We used the code from https://github.com/\nsebastianGehrmann/bottom-up-summary with\nthe same parameters.\n9313\nModel Type ROUGE\n1 2 3 L\nPrevious Work\nLead-10 Ext 35.52 10.33 3.74 31.44\nSumBasic Ext 29.47 6.95 2.36 26.3\nLexRank Ext 33.85 10.73 4.54 28.99\nSeq2Seq Abs 29.3 6.00 1.77 25.56\nPointer-gen Mix 32.06 9.04 2.15 25.16\nDiscourse-aware Mix 35.80 11.05 3.62 31.80\nBottom-up Mix 39.96 13.16 5.04 36.28\nOur Models\nSent-CLF Ext 34.01 8.71 2.99 30.41\nSent-PTR Ext 42.32 15.63 7.49 38.06\nTLM-I Abs 39.65 12.15 4.40 35.76\nTLM-I+E (G,M) Mix 41.62 14.69 6.16 38.03\nOracle\nGold Ext Oracle 44.25 18.17 9.14 35.33\nTLM-I+E (G,G) Oracle 46.40 18.15 8.71 42.27\nTable 2: Summarization results on the arXiv dataset.\nPrevious work results from Cohan et al. (2018). The\nfollowing lines are a simple baseline Lead-10 extractor\nand the pointer and classiﬁer models. Our transformer\nLMs (TLM) are conditioned either on the Introduction\n(I) or along with extracted sentences (E) either from\nground-truth (G) or model (M) extracts.\nModel Type ROUGE\n1 2 3 L\nPrevious Work\nLead-10 Ext 37.45 14.19 8.26 34.07\nSumBasic Ext 37.15 11.36 5.42 33.43\nLexRank Ext 39.19 13.89 7.27 34.59\nSeq2seq Abs 31.55 8.52 7.05 27.38\nPointer-gen Mix 35.86 10.22 7.60 29.69\nDiscourse-aware Mix 38.93 15.37 9.97 35.21\nBottom-up Mix 40.02 15.82 8.71 37.28\nOur Models\nSent-CLF Ext 45.01 19.91 12.13 41.16\nSent-PTR Ext 43.30 17.92 10.67 39.47\nTLM-I Abs 37.06 11.69 5.31 34.27\nTLM-I+E (G,M) Mix 42.13 16.27 8.82 39.21\nOracle\nGold Ext Oracle 47.76 20.36 11.52 39.19\nTLM-I+E (G,G) Oracle 46.32 20.15 11.75 43.23\nTable 3: Summarization results on the PubMed dataset.\nPrevious work results from Cohan et al. (2018). The\nfollowing lines are a simple baseline Lead-10 extractor\nand the pointer and classiﬁer models. Our transformer\nLMs (TLM) are conditioned either on the Introduction\n(I) or along with extracted sentences (E) either from\nground-truth (G) or model (M) extracts.\nabstractive and mixed which their model should be\nbetter suited for since it has a copy mechanism. The\nExconsumm model (Mendes et al., 2019) however,\nwhich is primarily an extractive model does better\non this dataset. We suspect the poor ROUGE-L re-\nsult is due to the absence of a copy mechanism that\nmakes it hard to get exact large n-gram matches.\nFigure 2 further supports this hypothesis, it is evi-\ndent that a model with a copy mechanism is often\nable to copy even upto 25-grams from the article.\nFurther, Graham (2015) ﬁnds that ROUGE-L is\npoorly correlated with human judgements when\ncompared to ROUGE-1,2,3. In table 8 and table 9,\nwe present qualitative results of abstracts of notable\npapers in our ﬁeld and of our TLM conditioned\non the introductions and extracted summaries of a\nrandom example from the arXiv test set. Table 7\nshows similar qualitative examples on the News-\nroom dataset. Tables 2, 3 and 4 also provide differ-\nent train / test settings for our TLM conditioned on\nextracted sentences. We show a performance upper\nbound conditioning the Transformer LM on oracle\n/ ground-truth extracted sentences at both train and\ntest time (TLM-I+E (G,G)). We also experiment\nwith using either the ground-truth extracted sen-\ntences (TLM-I+E (G,M)) or the model extracted\nsentences (TLM-I+E (M,M)) during training and\nﬁnd that latter slightly impairs performance. It is\nimportant to note that, across datasets, introduc-\ning extracted sentences with TLM+I+E or TLM+E\nhas consistently performed better over TLM+I or\nTLM. For bigPatent in table 4 and newsroom in ta-\nble 5 TLM and TLM+E models have access to the\nsame text since the whole article can ﬁt in the trans-\nformer window size. This is particularly interesting\nsince our results show that explicitly delimiting the\nextracted sentences has large positive affects on\nsummary performance. As anticipated, introducing\nextracted sentences allows the TLM model to focus\nless on information retrieval and more on language\ngeneration.\nModel Type ROUGE\n1 2 L\nPrevious Work\nLead-3 Ext 31.27 8.75 26.18\nTextRank Ext 35.99 11.14 29.60\nLexRank Ext 35.57 10.47 29.03\nRNN-Ext Ext 34.63 10.62 29.43\nSeq2Seq Abs 28.74 7.87 24.66\nPointer-gen Mix 30.59 10.01 25.65\nPointer-gen (Cov) Mix 33.14 11.63 28.55\nSent-rewriting Mix 37.12 11.87 32.45\nOur Models\nSent-CLF Ext 36.20 10.99 31.83\nSent-PTR Ext 34.21 10.78 30.07\nTLM Abs 36.41 11.38 30.88\nTLM+E (G,M) Mix 38.65 12.31 34.09\nOracle\nGold Ext Oracle 43.56 16.91 36.52\nOracleFrag Oracle 91.85 78.66 91.85\nTLM+E (G,G) Oracle 39.99 13.79 35.33\nTable 4: Summarization results on the bigPatent\ndataset. Previous work results from Sharma et al.\n(2019). Our transformer LMs (TLM) are conditioned\non the whole document or additionally with extracted\nsentences (E) either from ground-truth (G) or model\n(M) extracts. Note that OracleFrag (Grusky et al.,\n2018b) (Extractive Oracle Fragments) is an an extrac-\ntion heuristic that has access to the reference sum-\nmary”.\n4.1 Abstractiveness of generated abstracts\nWeber et al. (2018) argued that state-of-the-art ab-\n9314\nModel Type Extractive Mixed Abstractive\nROUGE\n1 2 L 1 2 L 1 2 L\nPrevious Work\nSeq2Seq Abs 6.1 0.2 5.4 5.7 0.2 5.1 6.2 1.1 5.7\nTextRank Ext 32.4 19.7 28.7 22.3 7.9 17.7 13.5 1.9 10.5\nPointer-gen Mix 39.1 27.9 36.2 25.5 11.0 21.1 14.7 2.3 11.4\nLead-3 Ext 53.0 49.0 52.4 25.1 12.9 22.1 13.7 2.4 11.2\nExconsumm Mix 68.4 62.9 67.3 31.7 16.1 27.0 17.1 3.1 14.1\nOur Models\nSent-CLF Ext 53.0 47.0 52.1 26.8 12.6 23.6 15.4 2.7 12.8\nSent-PTR Ext 60.7 55.2 59.7 28.9 14.1 25.1 15.9 2.8 13.0\nTLM Abs 49.8 39.7 47.4 27.1 11.6 22.8 20.4 6.9 17.1\nTLM+E (G,M) Mix 63.3 57.3 61.8 31.9 16.6 27.4 20.1 6.5 16.6\nOracle\nGold Ext Oracle 68.1 64.5 67.3 40.8 24.6 34.2 21.9 5.2 16.3\nTLM+E (G,G) Oracle 78.8 74.0 77.8 38.6 22.0 33.6 24.5 9.6 20.8\nTable 5: Summarization results on the Newsroom\ndataset. Previous work results from Grusky et al.\n(2018a) and Mendes et al. (2019). Note that ex-\ntractive/mixed/abstractive columns denote the type of\nground-truth summary. The Newsroom dataset has tar-\ngets that are extracted from the input (extractive), that\nare created with heuristics (mixed) and that are created\nby humans (abstractive). Also note that the “Type“ col-\numn refers to the model type for each row.\nFigure 2: n-gram overlaps between the abstracts gen-\nerated by different models and the input article on the\narXiv dataset. We show in detail which part of the input\nwas copied for our TLM conditioned on intro + extract.\nstractive summarization systems that use a copy\nmechanism effectively generate the summary by\ncopying over large chunks from the article, essen-\ntially doing “extractive” summarization. Following\nthis work, we measure how much a model copies\nfrom the article by counting the proportion of n-\ngrams from the generated abstract that are also\nfound in the article. These statistics measured on\nthe arXiv dataset are presented in ﬁgure 2. First, the\noriginal abstract and our TLM conditioned on the\nintro have small and very similar overlap fractions\nwith the original article. A model using a point-\ning mechanism (we used our own implementation\nof the model developed by Cohan et al. (2018)) 3\ncopies more than our transformer model, especially\n3This model achieved the following ROUGE-1, 2, 3 and L\non the arXiv dataset: 41.33, 14.73, 6.80, 36.34\nfor higher n-grams. In particular, more than 10%\nof the 20-grams from the abstracts generated by\nthe pointing model are also found in the article,\nshowing that it tends to copy long sequences of\nwords. On the other hand, our proposed model\nproduces more “abstractive” summaries, demon-\nstrating its ability to paraphrase. Our model tends\nto copy longer sequences when conditioned on the\nintroduction and the sentences from the extractor.\nWe hypothesize that providing extracted sentences\nfrom the article that already contain a lot of words\npresent in the reference abstract, makes the trans-\nformer’s task easier, by allowing it to copy words\nand phrases from the extracted sentences. We ﬁnd\nempirical evidence of this in ﬁgure 2, showing that\nthe majority of n-gram copies come from the ex-\ntracted sentences. For 5-grams, close to 2/3rd of\nthe words copied are from the extracted sentences.\nAs the number of grams increases to 25-grams,\n4/5th of the words copied are from the extracted\nsentences.\n4.2 Human Evaluation\nWe performed a human evaluation using the same\nexperimental setup as in (Grusky et al., 2018a) in\nTable 6. For the same 60 Newsroom test articles,\nwe obtain the summaries for 5 different models\n(ground truth, sentence classiﬁer, sentence pointer,\nTLM conditioned on article, TLM conditioned on\narticle + pointer extracts).\nModel Type Evaluation criteria\nCOH FLU INF REL\nGround truth summaries Orac 3.73 3.98 3.19 3.59\nTLM - Intro + Extract Mix 3.78 3.75 3.09 3.59\nTLM - Intro Mix 3.77 3.90 3.11 3.50\nSentence pointer Ext 3.67 3.66 3.24 3.78\nSentence classiﬁer Ext 3.62 3.79 3.47 3.89\nTable 6: Human evaluation on Newsroom abstractive summa-\nrization test data. Each pair of (article, summary) is presented\nto three unique crowd workers, who are asked to judge the\nsummaries along four criteria: Coherence (COH: does the\nsummary make sense as a whole), Fluency (FLU: is it well\nwritten), Informativeness (INF: does the summary catch the\nmost important points of the article), and Relevance (REL: are\nthe facts in the summary consistent with the article).\nAs expected, Transformers are quite good mak-\ning coherent and ﬂuent summaries but not necessar-\nily on informativeness and relevance. Transform-\ners have a logarithmic or constant path length (as\nopposed to linear in RNNs) between a networks\noutput and any of its inputs, making gradient ﬂow\nmuch easier. This is a clear advantage over RNNs\nthat tend to repeat sentences. Transformers are\nalso known to hallucinate (Lee et al., 2019) but\nwe notice that including extracted sentences, TLM\n9315\n+ Intro + Extract, improve relevance by 3% over\nTLM + Intro, bringing relevance closer to extrac-\ntive methods. Interestingly, on Coherence, both\nour TLM variants also score better than the ground\ntruth. Over the four categories, TLM + Intro + Ex-\ntract performs best on average over TLM + Intro,\ndespite the former having higher ROUGE scores\non the abstractive test set in table 5. Somewhat\ncounter-intuitively we observe that human written\nsummaries are often rated lower than model sum-\nmaries. However, other work has also found that\nhuman written ground truth summaries consistently\nreceive lower scores when compared to model writ-\nten summaries when evaluated by turkers (see for\nexample Table 3 in the PEGASUS paper of (Zhang\net al., 2020)). We believe that this could be be-\ncause Newsroom summaries are sometimes noisy,\nungrammatical and incoherent.\nDocument — A new plan from the government of the Philippines would\noffer free wireless internet to people across the country while also likely\neating into the annual revenue of the nations telecoms. Bloomberg reports\nthat the Philippines government plans to roll-out its free Wi-Fi services to\nroughly half of the countrys municipalities over the next few months and\nthe country has its sights set on nationwide coverage by the end of 2016.\nThe free wireless internet service will be made available in public areas\nsuch as schools, hospitals, airports and parks, and is expected to cost the\ngovernment roughly $32 million per year. [...]\nAbstractive — : The government is reportedly considering a nationwide\nservice plan to give free Wi-Fi access to rural areas.\nMixed — The government of the Philippines is considering a new plan to\nprovide free wireless internet to the nation’s largest cities and towns.\nExtractive — The new plan will include free wireless internet to residents\nacross the country while also probably eating into the annual revenue of\nthe country’s telecoms.\nDocument — (CBS) - Controversy over a new Microsoft patent has peo-\nple questioning whether or not the intention has racist undertones. CNET\nreported that Microsoft has been granted a U.S. patent that will steer\npedestrians away from areas that are high in crime. [...]\nAbsractive Summary — The new Microsoft patent claims a device could\nprovide pedestrian navigation directions from a smartphone.\nMixed Summary Microsoft won a U.S. patent for a new way to steer\npedestrians out of areas that are high in crime\nTable 7: Qualitative Results - News articles and our model\ngenerated summaries on the NewsRoom dataset\n4.3 Qualitative Results\nHere we provide some qualitative results. Run-\nning our algorithm on a close to ﬁnal version of\nthis paper (excluding this section) and selecting\nthe best sample from a set of 10-20 runs we found\nthe following abstract: “we present a hybrid ex-\ntractive and abstractive approach for generating\nsummaries from long documents. we use an initial\nextractive step that reduces the amount of context\nfor a subsequent abstractive step (see ﬁgure [ﬁg:\nmodel]). we show that this approach can produce\na good summarization quality on both short and\nlong documents, even without using copying and\npointing mechanisms. further, by considering the\ncontext in both the text and the discourse, we ﬁnd\nthat the hybrid approach is effective at capturing\nthe underlying context. we examine these mod-\nels through rouge scores, through a study of the\namount of n-gram copying performed by different\nmodels, as well as through a human evaluation us-\ning a standard protocol. our results show that our\nhybrid approach yields results that outperform cur-\nrent state-of-the-art results on several metrics of\nthese evaluations.”\n5 Conclusion\nWe have demonstrated that Transformer language\nmodels can generate high-quality summaries of\nlong sequences of text via an extractive step fol-\nlowed by an abstractive step. We quantitatively\nmeasure the positive impact of the extractive step,\nby comparing it to a abstractive model variant that\nonly sees the input text itself. Our approach out-\nperforms previous extractive and abstractive sum-\nmarization methods on the arXiv, PubMed and big-\nPatent datasets and is less prone to copying en-\ntire phrases or sentences from the input text. As\nwith other problem domains, we have observed that\nabstractive summaries generated by transformers\ncan generate imaginary content. We advise that\nsuch evaluations should probe multiple aspects of\nthe summarization results including both factual\ncorrectness and coherency. We also note that for\nevaluating the correctness of the summaries of sci-\nentiﬁc articles and patents one must have highly\ntrained evaluators who are willing to invest sig-\nniﬁcant amounts of time to read the underlying\npapers and patents. Such studies could therefore\nrequire signiﬁcant investments of resources. While\nwe believe that this work is a step forward towards\ngenerating more abstractive summaries, it remains\nan open challenge to develop abstactive models\nthat respect the underlying facts of the content be-\ning summarized while matching the creative ability\nof humans to coherently and concisely synthesize\nsummaries. However, the TLMs we use here are of\nmoderate size compared to what is now possible.\nIt would be very interesting to see what kind of\nperformance larger models could achieve.\nAcknowledgments\nWe would like to thank Nicolas Chapados and\nHarm de Vries for their helpful feedback and Minh\nDao for helping with the ﬁgures. We would also\nlike to thank Guillaume Lample for discussions\nabout TLM training.\n9316\nGenerated abstract for “the attention model paper” (Bah-\ndanau et al., 2014) - In this paper, we propose a neural ma-\nchine translation model, which learns to align and translate a\nsource sentence without encoding it into a ﬁxed length vector.\nThe proposed model allows the neural network to align and\ntranslate jointly, and in addition, predicts a target word based\non the context vectors associated with these source positions.\nWe show that the proposed model achieves signiﬁcantly im-\nproved translation performance than the conventional encoder\ndecoder neural network approach, when the sentences in the\ntraining corpus are long . Quantitative analysis reveals that\nthe proposed model ﬁnds a linguistically plausible soft align-\nment between a source sentence and the target sentence. Fi-\nnally, we compare the performance of the proposed model\nwith the state-of-the-art standard neural network translation\nmodels and show that the proposed approach outperforms\nthe conventional approach in terms of both translation per-\nformance and the quality of the translations.\nOriginal abstract - Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural machine trans-\nlation aims at building a single neural network that can be jointly tuned to maximize the\ntranslation performance. The models proposed recently for neural machine translation often\nbelong to a family of encoder-decoders and consists of an encoder that encodes a source\nsentence into a ﬁxed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the\nperformance of this basic encoder-decoder architecture, and propose to extend this by allow-\ning a model to automatically (soft-)search for parts of a source sentence that are relevant\nto predicting a target word, without having to form these parts as a hard segment explicitly.\nWith this new approach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French translation. Further-\nmore, qualitative analysis reveals that the (soft-)alignments found by the model agree well\nwith our intuition.\nGenerated abstract for “the GPT-2 paper” (Radford et al.,\n2019) - Language models have been successful in creating\nrich representations of complex systems, including tasks such\nas sentiment analysis. In this paper we demonstrate their abil-\nity to generalize well across domains and tasks. We demon-\nstrate the utility of language models by demonstrating that a\nvariety of tasks can be modeled using language models. We\ndemonstrate the potential of language models in downstream\ntasks by highlighting the ability of these models to perform a\nwide range of tasks in a zero shot setting.\nOriginal abstract - Natural language processing tasks, such as question answering, machine\ntranslation, reading comprehension, and summarization, are typically approached with su-\npervised learning on task-speciﬁc datasets. We demonstrate that language models begin to\nlearn these tasks without any explicit supervision when trained on a new dataset of millions\nof webpages called WebText. When conditioned on a document plus questions, the answers\ngenerated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding\nthe performance of 3 out of 4 baseline systems without using the 127,000+ training examples.\nThe capacity of the language model is essential to the success of zero-shot task transfer and\nincreasing it improves performance in a log-linear fashion across tasks. Our largest model,\nGPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8\ntested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples\nfrom the model reﬂect these improvements and contain coherent paragraphs of text. These\nﬁndings suggest a promising path towards building language processing systems which learn\nto perform tasks from their naturally occurring demonstrations.\nGenerated abstract for “the BERT paper” (Devlin et al.,\n2018) - We develop an architecture for deep bidirectional lan-\nguage models that allows the representation to fuse the left\nand the right context of a given token. Our framework uses\nbidirectional transformer encoder representations that enable\nthe model to fuse the context representations of a token. We\nshow that our architecture achieves state-of-the-art perfor-\nmance on a large suite of tasks, outperforming many systems\nwith task-speciﬁc architectures. Our results are based on a set\nof eleven task speciﬁc tasks that are benchmarks for a large\nsuite of nlp tasks.\nOriginal abstract - We introduce a new language representation model called BERT, which\nstands for Bidirectional Encoder Representations from Transformers. Unlike recent lan-\nguage representation models, BERT is designed to pre-train deep bidirectional representa-\ntions from unlabeled text by jointly conditioning on both left and right context in all layers.\nAs a result, the pre-trained BERT model can be ﬁne-tuned with just one additional output\nlayer to create state-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-speciﬁc architecture modiﬁcations. BERT\nis conceptually simple and empirically powerful. It obtains new state-of-the-art results on\neleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7%\npoint absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement),\nSQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and\nSQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\nGenerated abstract for “the seq2seq paper” (Sutskever\net al., 2014) - We present a novel approach to sequence\nto sequence learning using deep recurrent neural networks\n(LSTMs). First , we transform a sequence to a vector rep-\nresentation that contains all the long short term memory\n(LSTM) features, and then we use the LSTM to generate\nelements of the output sequence using a simple left to right\nbeam-search decoder. The advantage of this approach is that\nit learns to map all the words in the input sequence to a ﬁxed\ndimensional vector representation, while the LSTM does not\nrequire any special training to learn the sentence represen-\ntation. We demonstrate the LSTMs ability to train on the\nWMT14 English to French translation task using a simple\nleft to right beam-search decoder. The model was found to\nbe highly invariant to the active and passive voice.\nOriginal abstract - Deep Neural Networks (DNNs) are powerful models that have achieved\nexcellent performance on difﬁcult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to sequences. In this\npaper, we present a general end-to-end approach to sequence learning that makes minimal\nassumptions on the sequence structure. Our method uses a multilayered Long Short-Term\nMemory (LSTM) to map the input sequence to a vector of a ﬁxed dimensionality, and then\nanother deep LSTM to decode the target sequence from the vector. Our main result is that on\nan English to French translation task from the WMT’14 dataset, the translations produced\nby the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU\nscore was penalized on out-of-vocabulary words. Additionally, the LSTM did not have\ndifﬁculty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU\nscore of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses\nproduced by the aforementioned SMT system, its BLEU score increases to 36.5, which is\nclose to the previous best result on this task. The LSTM also learned sensible phrase and\nsentence representations that are sensitive to word order and are relatively invariant to the\nactive and the passive voice. Finally, we found that reversing the order of the words in all\nsource sentences (but not target sentences) improved the LSTM’s performance markedly,\nbecause doing so introduced many short term dependencies between the source and the\ntarget sentence which made the optimization problem easier.\nGenerated abstract for “the GAN paper” (Goodfellow\net al., 2014) - Adversarial nets are a class of deep genera-\ntive networks that utilize the advantages of piecewise linear\nunits. Here, we explore a novel model estimation procedure\nthat combines several methods, such as backpropagation and\ndropout, to learn generative models. The proposed adver-\nsarial net framework can be thought of as analog to a team\nof counterfeiters, who try to produce fake currency and use\nit without being detected, while the discriminative model is\nanalogous to the police, trying to detect the counterfeit cur-\nrency. We refer to this special case as adversarial nets. We\nexplore the special case when the generative model gener-\nates samples by passing random noise through a multilayer\nperceptron, and the discriminative model is also a multilayer\nperceptron.\nOriginal abstract - We propose a new framework for estimating generative models via an\nadversarial process, in which we simultaneously train two models: a generative model G\nthat captures the data distribution, and a discriminative model D that estimates the probabil-\nity that a sample came from the training data rather than G. The training procedure for G is\nto maximize the probability of D making a mistake. This framework corresponds to a mini-\nmax two-player game. In the space of arbitrary functions G and D, a unique solution exists,\nwith G recovering the training data distribution and D equal to 1/2 everywhere. In the case\nwhere G and D are deﬁned by multilayer perceptrons, the entire system can be trained with\nbackpropagation. There is no need for any Markov chains or unrolled approximate infer-\nence networks during either training or generation of samples. Experiments demonstrate the\npotential of the framework through qualitative and quantitative evaluation of the generated\nsamples.\nTable 8: Qualitative Results — Generated abstracts of select papers using our Intro Only TLM.\n9317\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nYen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-\ntive summarization with reinforce-selected sentence\nrewriting. arXiv preprint arXiv:1805.11080.\nJianpeng Cheng and Mirella Lapata. 2016. Neural sum-\nmarization by extracting sentences and words.arXiv\npreprint arXiv:1603.07252.\nSumit Chopra, Michael Auli, and Alexander M Rush.\n2016. Abstractive sentence summarization with at-\ntentive recurrent neural networks. In Proceedings of\nthe 2016 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 93–98.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Na-\nzli Goharian. 2018. A discourse-aware attention\nmodel for abstractive summarization of long docu-\nments. CoRR, abs/1804.05685.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nG¨unes Erkan and Dragomir R Radev. 2004. Lexrank:\nGraph-based lexical centrality as salience in text\nsummarization. Journal of artiﬁcial intelligence re-\nsearch, 22:457–479.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. arXiv preprint\narXiv:1805.04833.\nSebastian Gehrmann, Yuntian Deng, and Alexander M\nRush. 2018. Bottom-up abstractive summarization.\narXiv preprint arXiv:1808.10792.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. pages 2672–2680.\nYvette Graham. 2015. Re-evaluating automatic sum-\nmarization with bleu and 192 shades of rouge. pages\n128–137.\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018a.\nNewsroom: A dataset of 1.3 million summaries\nwith diverse extractive strategies. arXiv preprint\narXiv:1804.11283.\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018b.\nNewsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 708–719, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor\nO. K. Li. 2016. Incorporating copying mech-\nanism in sequence-to-sequence learning. CoRR,\nabs/1603.06393.\nCaglar Gulcehre, Sungjin Ahn, Ramesh Nallap-\nati, Bowen Zhou, and Yoshua Bengio. 2016.\nPointing the unknown words. arXiv preprint\narXiv:1603.08148.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Comput. , 9(8):1735–\n1780.\nWan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui\nMin, Jing Tang, and Min Sun. 2018. A uni-\nﬁed model for extractive and abstractive summa-\nrization using inconsistency loss. arXiv preprint\narXiv:1805.06266.\nKatherine Lee, Orhan Firat, Ashish Agarwal, Clara\nFannjiang, and David Sussillo. 2019. Hallucinations\nin neural machine translation.\nChin-Yew Lin. 2004. Looking for a few good\nmetrics: Automatic summarization evaluation-how\nmany samples are enough? In NTCIR.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and\nNoam Shazeer. 2018. Generating wikipedia by\nsummarizing long sequences. arXiv preprint\narXiv:1801.10198.\nMinh-Thang Luong, Hieu Pham, and Christopher D\nManning. 2015. Effective approaches to attention-\nbased neural machine translation. arXiv preprint\narXiv:1508.04025.\nAfonso Mendes, Shashi Narayan, Sebasti ˜ao Miranda,\nZita Marinho, Andr ´e FT Martins, and Shay B Co-\nhen. 2019. Jointly extracting and compressing doc-\numents with summary state representations. arXiv\npreprint arXiv:1904.02020.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev,\nGanesh Venkatesh, et al. 2017. Mixed precision\ntraining. arXiv preprint arXiv:1710.03740.\nRada Mihalcea and Paul Tarau. 2004. Textrank: Bring-\ning order into text. In Proceedings of the 2004 con-\nference on empirical methods in natural language\nprocessing, pages 404–411.\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.\nSummarunner: A recurrent neural network based se-\nquence model for extractive summarization of docu-\nments.\nRamesh Nallapati, Bowen Zhou, Caglar Gulcehre,\nBing Xiang, et al. 2016a. Abstractive text sum-\nmarization using sequence-to-sequence rnns and be-\nyond. arXiv preprint arXiv:1602.06023.\n9318\nRamesh Nallapati, Bowen Zhou, and Mingbo Ma.\n2016b. Classify or select: Neural architectures for\nextractive document summarization. arXiv preprint\narXiv:1611.04244.\nJun-Ping Ng and Viktoria Abrecht. 2015. Better sum-\nmarization evaluation with word embeddings for\nROUGE. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1925–1930, Lisbon, Portugal. Association for\nComputational Linguistics.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2017. A deep reinforced model for abstractive sum-\nmarization. arXiv preprint arXiv:1705.04304.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. CoRR, abs/1509.00685.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. CoRR, abs/1704.04368.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nEva Sharma, Chen Li, and Lu Wang. 2019. Bigpatent:\nA large-scale dataset for abstractive and coherent\nsummarization. arXiv preprint arXiv:1906.03741.\nJosef Steinberger and Karel Jezek. 2004. Using latent\nsemantic analysis in text summarization and sum-\nmary evaluation. Proc. ISIM, 4:93–100.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\npages 3104–3112.\nLucy Vanderwende, Hisami Suzuki, Chris Brockett,\nand Ani Nenkova. 2007. Beyond sumbasic: Task-\nfocused summarization with sentence simpliﬁcation\nand lexical expansion. Information Processing &\nManagement, 43(6):1606–1618.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. pages 2692–2700.\nNoah Weber, Leena Shekhar, Niranjan Balasubrama-\nnian, and Kyunghyun Cho. 2018. Controlling decod-\ning for more abstractive summaries with copy-based\nnetworks. arXiv preprint arXiv:1803.07038.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and\nPeter J Liu. 2020. Pegasus: Pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn ICML.\n9319\nAppendix\nSamples from the arXiv test set\nSamples from the TLM-I and TLM-I+E (G,M)\nmodels on a random example from the arXiv test\nset.\nTable 9: Qualitative Results - Generated abstracts from our\nmodels on a random example from the test set of (Cohan et al.,\n2018)\nTLM conditioned on intro — deep convolutional neural\nnetworks ( cnns ) have created new benchmarks in the object\nrecognition challenge . most of these benchmark datasets\ncontain thousands of images of interest . in addition to these\ndatasets , several standard data - sets , such as pascal voc 2012\nand the ms coco , have been used in conjunction with the\nsame network in order to improve the object detection task\n. in this paper , we present a method for object localization\nusing the deep convolutional neural networks trained for the\nsimpler task of image - level classiﬁcation . we propose lo-\ncalization candidates ( or bounding boxes ) which maximize\nclassiﬁcation scores for a class of interest . we explore the\nbeam search strategy where the number of candidate bound-\ning boxes are restricted as we traverse from the root node\ntowards the leaf nodes . we then gradually explore smaller\nand smaller regions of interest till a point is reached where\nthe classiﬁer is not able to discriminate among the classes of\ninterest . the detection candidates are organized in a search\ntree , the root node being represented by the entire test image\n. we then gradually explore smaller and smaller regions of\ninterest till a point is reached where the classiﬁer is not able\nto distinguish among the classes of interest .\nTLM conditioned on intro + pointer network extracted\nsentences — deep convolutional neural networks ( cnns )\nare widely used in image recognition applications . however\n, they face a number of challenges . ﬁrst , cnn models are\ncomputationally expensive to train and to perform . second\n, they are not easy to implement and can not exploit global\nfeatures . to alleviate these issues , we present a method\nthat leverages the correlation between semantic information\npresent in feature maps and localization of an object of inter-\nest . the method leverages feature map activations for object\nlocalization . we demonstrate that this method is signiﬁcantly\nfaster than state - of - the - art in localization accuracy by a\nsigniﬁcant margin of up to 8 map on two standard data - sets\nwith complex scenes , pascal voc 2012 and the much larger\nms coco .\nGround truth abstract— object localization is an important\ncomputer vision problem with a variety of applications . the\nlack of large scale object - level annotations and the relative\nabundance of image - level labels makes a compelling case\nfor weak supervision in the object localization task . deep\nconvolutional neural networks are a class of state-of-the-art\nmethods for the related problem of object recognition . in\nthis paper , we describe a novel object localization algorithm\nwhich uses classiﬁcation networks trained on only image\nlabels . this weakly supervised method leverages local spatial\nand semantic patterns captured in the convolutional layers\nof classiﬁcation networks . we propose an efﬁcient beam\nsearch based approach to detect and localize multiple objects\nin images . the proposed method signiﬁcantly outperforms\nthe state-of-the-art in standard object localization data - sets\nwith a 8 point increase in map scores .\nT-SNE of learned word embeddings\nWe visualize the word embeddings learned by our\nTLM model using t-sne. We ﬁnd that words that\nare often associated with computer science are clus-\ntered in a different part of space when compared to\nwords associated with physics. We use the arXiv\nREST API to ﬁnd the submission category of each\npaper in the training set and then ﬁnd the ∼300\nmost representative words for each category, using\nTF-IDF scores and plot them.\nFigure 3: t-sne visualization of the TLM-learned word\nembeddings. The model appears to partition the space\nbased on the broad paper categoty in which it fre-\nquently occurs.\nExtractive Model Details\nThe model uses word embeddings of size 300. The\ntoken-level LSTM (sentence encoder), sentence-\nlevel LSTM (document encoder) and decoder each\nhave 2 layers of 512 units and a dropout of 0.5 is\napplied at the output of each intermediate layer.\nWe trained it with Adam, a learning rate 0.001, a\nweight decay of 10−5, and using batch sizes of 32.\nWe evaluate the model every 200 updates, using\na patience of 50. At inference, we decode using\nbeam search with a beam size of 4 for the pointer\nmodel and pick thekmost likely sentences from the\nsentence classiﬁer, where kis the average number\nof sentences in the summary across the training\ndataset.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9019376635551453
    },
    {
      "name": "Computer science",
      "score": 0.8202508687973022
    },
    {
      "name": "Transformer",
      "score": 0.7911447882652283
    },
    {
      "name": "Language model",
      "score": 0.6490856409072876
    },
    {
      "name": "Natural language processing",
      "score": 0.6308234930038452
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5790452361106873
    },
    {
      "name": "Fluency",
      "score": 0.5645585060119629
    },
    {
      "name": "Question answering",
      "score": 0.4404544532299042
    },
    {
      "name": "Machine learning",
      "score": 0.4186114966869354
    },
    {
      "name": "Artificial neural network",
      "score": 0.4147942066192627
    },
    {
      "name": "Information retrieval",
      "score": 0.32844361662864685
    },
    {
      "name": "Linguistics",
      "score": 0.09092596173286438
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}