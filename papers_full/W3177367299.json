{
    "title": "Encoding Syntactic Knowledge in Transformer Encoder for Intent Detection and Slot Filling",
    "url": "https://openalex.org/W3177367299",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2169327814",
            "name": "Jixuan Wang",
            "affiliations": [
                "Vector Institute",
                "University of Toronto",
                "Toronto Rehabilitation Institute"
            ]
        },
        {
            "id": "https://openalex.org/A1923714897",
            "name": "Kai Wei",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A3017441961",
            "name": "Martin Radfar",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2050051337",
            "name": "Wei Wei Zhang",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2161014577",
            "name": "Clement Chung",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2169327814",
            "name": "Jixuan Wang",
            "affiliations": [
                "Vector Institute",
                "University of Toronto"
            ]
        },
        {
            "id": "https://openalex.org/A1923714897",
            "name": "Kai Wei",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A3017441961",
            "name": "Martin Radfar",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2050051337",
            "name": "Wei Wei Zhang",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2161014577",
            "name": "Clement Chung",
            "affiliations": [
                "Amazon (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6605426428",
        "https://openalex.org/W2563167126",
        "https://openalex.org/W1614862348",
        "https://openalex.org/W2551571666",
        "https://openalex.org/W6683738474",
        "https://openalex.org/W2949325756",
        "https://openalex.org/W6664472811",
        "https://openalex.org/W2803392141",
        "https://openalex.org/W6632947847",
        "https://openalex.org/W2473329891",
        "https://openalex.org/W2556468274",
        "https://openalex.org/W6669997747",
        "https://openalex.org/W6763121668",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W2509005166",
        "https://openalex.org/W3031914912",
        "https://openalex.org/W2170198242",
        "https://openalex.org/W6756040250",
        "https://openalex.org/W3011573503",
        "https://openalex.org/W2971167298",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2798638375",
        "https://openalex.org/W2106347453",
        "https://openalex.org/W2155524666",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2997774779",
        "https://openalex.org/W2575101493",
        "https://openalex.org/W2967658867",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2137871902",
        "https://openalex.org/W3106208185",
        "https://openalex.org/W2963974889",
        "https://openalex.org/W2552110825",
        "https://openalex.org/W2097550833",
        "https://openalex.org/W2845344333",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2962788148",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W1550863320",
        "https://openalex.org/W2998665041",
        "https://openalex.org/W3209042722",
        "https://openalex.org/W2952230511",
        "https://openalex.org/W2077302143",
        "https://openalex.org/W2056894934",
        "https://openalex.org/W3037109418",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3107090347",
        "https://openalex.org/W2948197522",
        "https://openalex.org/W2917128112",
        "https://openalex.org/W2963066655",
        "https://openalex.org/W2987266335",
        "https://openalex.org/W4297683418"
    ],
    "abstract": "We propose a novel Transformer encoder-based architecture with syntactical knowledge encoded for intent detection and slot filling. Specifically, we encode syntactic knowledge into the Transformer encoder by jointly training it to predict syntactic parse ancestors and part-of-speech of each token via multi-task learning. Our model is based on self-attention and feed-forward layers and does not require external syntactic information to be available at inference time. Experiments show that on two benchmark datasets, our models with only two Transformer encoder layers achieve state-of-the-art results. Compared to the previously best performed model without pre-training, our models achieve absolute F1 score and accuracy improvement of 1.59 % and 0.85 % for slot filling and intent detection on the SNIPS dataset, respectively. Our models also achieve absolute F1 score and accuracy improvement of 0.1 % and 0.34 % for slot filling and intent detection on the ATIS dataset, respectively, over the previously best performed model. Furthermore, the visualization of the self-attention weights illustrates the benefits of incorporating syntactic information during training.",
    "full_text": "Encoding Syntactic Knowledge in Transformer Encoder for\nIntent Detection and Slot Filling\nJixuan Wang1,2*, Kai Wei3† , Martin Radfar3, Weiwei Zhang3, Clement Chung3\n1 University of Toronto\n2 Vector Institute\n3 Amazon Alexa\njixuan@cs.toronto.edu, fkaiwe, radfarmr, wwzhang, chungcleg@amazon.com\nAbstract\nWe propose a novel Transformer encoder-based architecture\nwith syntactical knowledge encoded for intent detection and\nslot ﬁlling. Speciﬁcally, we encode syntactic knowledge into\nthe Transformer encoder by jointly training it to predict syn-\ntactic parse ancestors and part-of-speech of each token via\nmulti-task learning. Our model is based on self-attention and\nfeed-forward layers and does not require external syntactic\ninformation to be available at inference time. Experiments\nshow that on two benchmark datasets, our models with only\ntwo Transformer encoder layers achieve state-of-the-art re-\nsults. Compared to the previously best performed model with-\nout pre-training, our models achieve absolute F1 score and\naccuracy improvement of 1:59% and 0:85% for slot ﬁlling\nand intent detection on the SNIPS dataset, respectively. Our\nmodels also achieve absolute F1 score and accuracy improve-\nment of 0:1% and 0:34% for slot ﬁlling and intent detection\non the ATIS dataset, respectively, over the previously best\nperformed model. Furthermore, the visualization of the self-\nattention weights illustrates the beneﬁts of incorporating syn-\ntactic information during training.\nIntroduction\nRecent years have seen great success in applying deep learn-\ning approaches to enhance the capabilities of virtual assis-\ntants (V As) such as Amazon Alexa, Google Home and Ap-\nple Siri. One of the challenges for building these systems\nis mapping the meaning of users’ utterances, which are ex-\npressed in natural language, to machine comprehensible lan-\nguage (Allen 1995). An example is illustrated in Figure 1. In\nthis utterance “Show the cheapest ﬂight from Toronto to St.\nLouis”, the machine needs to map this utterance to an in-\ntent Airfare (intent detection) and to slots such as Toronto:\nFromLocation (slot ﬁlling). In this work, we focus on intent\ndetection and slot ﬁlling and refer to these as Natural Lan-\nguage Understanding (NLU) tasks.\nPrevious works show that a simple deep neural archi-\ntecture delivers better performance on NLU tasks when\ncompared to traditional models such as Conditional Ran-\ndom Fields (Collobert et al. 2011). Since then, deep neu-\nral architectures, predominantly recurrent neural networks,\n*Work done during author’s internship at Amazon Alexa.\n†Corresponding author.\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nUtterance: How much is the cheapest flight from Toronto to St. Louis?\nIntent: \nSlots:\nB-cost_relative I-arrival_city\nAirfare\nB-arrival_cityB-depart_cityOO OO O O O\nSlots O O O O B-fromloc O B-toloc I-toloc\nUtterance Show the flights from Toronto to St. Louis\nIntent SearchFlight\nFigure 1: An example of NLU tasks.\nhave become an indispensable part of building NLU sys-\ntems (Zhang and Wang 2016; Goo et al. 2018; E et al.\n2019). Transformer-based architectures, as introduced more\nrecently by (Vaswani et al. 2017), have shown signiﬁcant\nimprovement over previous works on NLU tasks (Chen,\nZhuo, and Wang 2019; Qin et al. 2019). Recent studies\nshow that although the Transformer model can learn syntac-\ntic knowledge purely by seeing examples, explicitly feed-\ning this knowledge to such models can signiﬁcantly en-\nhance their performance on tasks such as neural machine\ntranslation (Sundararaman et al. 2019) and semantic role la-\nbeling (Strubell et al. 2018). While incorporating syntac-\ntic knowledge has been shown to improve performance for\nNLU tasks (Tur et al. 2011; Chen et al. 2016), both of these\nassume syntactic knowledge is provided by external models\nduring training and inference time.\nIn this paper, we introduce a novel Transformer encoder-\nbased architecture for NLU tasks with syntactic knowledge\nencoded that does not require syntactic information to be\navailable during inference time. This is accomplished, ﬁrst,\nby training one attention head to predict syntactic ancestors\nof each token. The dependency relationship between each\ntoken is obtained from syntactic dependency trees, where\neach word in a sentence is assigned a syntactic head that is\neither another word in the sentence or an artiﬁcial root sym-\nbol (Dozat and Manning 2016). Adding the objective of de-\npendency relationship prediction allows a given token to at-\ntend more to its syntactically relevant parent and ancestors.\nIn addition to dependency parsing knowledge, we encode\npart of speech (POS) information in Transformer encoders\nbecause previous research shows that the POS information\ncan help dependency parsing (Nguyen and Verspoor 2018).\nThe closest work to ours is (Strubell et al. 2018). However,\nthey focused on semantic role labeling and trained one atten-\ntion head to predict the direct parent instead of all ancestors.\nWe compare our models with several state-of-the-art neu-\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n13943\nral NLU models on two publicly available benchmarking\ndatasets: the ATIS (Hemphill, Godfrey, and Doddington\n1990) and SNIPS (Coucke et al. 2018) datasets. The results\nshow that our models outperform previous works. To exam-\nine the effects of adding syntactic information, we conduct\nan ablation study and visualize the self-attention weights in\nthe Transformer encoder.\nProblem Deﬁnition\nWe deﬁne intent detection (ID) and slot ﬁlling (SF) as\nan utterance-level and token-level multi-class classiﬁcation\ntask, respectively. Given an input utterance with T tokens,\nwe predict an intent yint: and a sequence of slots, one per\ntoken, fyslot\n1 ;yslot\n2 ;:::;y slot\nT gas outputs. We add an empty\nslot denoted by “O” to represent words containing no labels.\nThe goal is to maximize the likelihood of correct the intents\nand slots given input utterances.\nProposed Model\nWe jointly train our model for NLU tasks ( i.e., ID and SF),\nsyntactic dependency prediction and POS tagging via multi-\ntask learning (Caruana 1993), as shown in Figure 2. For\ndependency prediction, we insert a syntactically-informed\nTransformer encoder layer after the ( x+ y)th layer. In this\nencoder layer, one attention head is trained to predict the full\nancestry for each token on the dependency parsing tree. For\nPOS tagging, we add a POS tagging model that shares the\nﬁrst xTransformer encoder layers with the NLU model. We\ndescribe the details of our proposed architecture below.\nInput Embedding\nThe input embedding model maps a sequence of token rep-\nresentations ft1;t2;:::;t T ginto a sequence of continuous\nembeddings fe0;e1;e2;:::;e T g, with e0 being the embed-\nding of a special start-of-sentence token, “[SOS]”. The em-\nbeddings are then fed into the NLU model.\nTransformer Encoder Layer\nThe Transformer encoder layers are originally proposed\nin (Vaswani et al. 2017). Each encoder layer consists of a\nmulti-head self-attention layer and feed forward layers with\nlayer normalization and residual connections. We stack mul-\ntiple encoder layers to learn contextual embeddings of each\ntoken, each with H attention heads. Suppose the output em-\nbeddings of the encoder layerj\u00001 is E(j\u00001), each attention\nhead hat layer jﬁrst calculates self-attention weights by the\nscaled dot product (1).\nA(j)\nh = softmax(Q(j)\nh K(j)\nh\nT\npdk\n) (1)\nIn (1), the query Q(j)\nh and key K(j)\nh are two different linear\ntransformations of E(j\u00001), dk is the dimension of the query\nand the key embeddings. The output of the attention head h\nis calculated by:\nF(j)\nh = A(j)\nh V(j)\nh (2)\nSyntactically-informed Transformer Layer\nTransformer Layer ✕x\n…St.Louis[SOS]showthe\nE1 E2 E3\n…ET-1 ET\nTransformer Layer ✕z\nTransformer Layer ✕y\nInputs\nEmbeddings\nPOS tagging\nDependencyprediction\n…AirfareO O B-tolocI-tolocNLU tasksMLP\nIntent and slot label predictions\nMLP\nFigure 2: A high level overview of the proposed architecture.\nNote that x, y and z all refer to number of layers that can\nvary depending on implementation. “MLP” refers to a multi-\nlayer perceptron (MLP).\nin which the value V(j)\nh is also a linear transformation of\nE(j\u00001). The outputs of H attention heads are concatenated\nas the self-attended token representations, followed by an-\nother linear transformation:\nMulti(j) = [F(j)\n1 ;F(j)\n2 ;\u0001\u0001\u0001 ;F(j)\nH ]WF (3)\nwhich is fed into the next feed forward layer. Residual con-\nnections and layer normalization are applied after the multi-\nhead attention and feed forward layer, respectively.\nEncoding Syntactic Dependency Knowledge\nAs shown in Figure 3, the syntactically-informed trans-\nformer encoder layer differs from the standard Transformer\nencoder layer by having one of the H attention heads\ntrained to predict the full ancestry for each token, i.e., par-\nents, grandparents, great grandparents, etc. Different from\n(Strubell et al. 2018), we use full ancestry prediction instead\nof just direct parent prediction. Later we will demonstrate\nthe beneﬁts of our approach in the Results section.\nGiven an input sequence of length T, the output of a reg-\nular attention head is a T \u0002T matrix, in which each row\ncontains the attention weights that a token puts on all the to-\nkens in the input sequence. The output of the syntactically-\ninformed attention head is also aT\u0002T matrix but this atten-\ntion head is trained to assign weights only on the syntactic\ngovernors (i.e., ancestors) of each token.\nTo train this attention head, we deﬁne a loss function by\nthe difference between the output attention weight matrix of\nthe syntactically-informed attention head and a predeﬁned\nprior attention weight matrix. The prior attention weight ma-\ntrix contains the prior knowledge that each token should at-\ntend to its syntactic parse ancestors, with attention weights\n13944\nSoftMax\nV\nMatMul\nQ KU\nMatMul\nScale\nMask (opt.)\nSoftMax\nMatMul\nMatMul\nQKV\nConcatenation\nLinear\nH-1\n Attention Priori\nKL Divergence\nLinguistically Informed AttentionScaled Dot-ProductAttention\nFigure 3: Overview of the syntactically-informed Transformer layer. One out of the H attention heads is trained for predicting\nsyntactic parse ancestors of each token. For each token, this attention head outputs a distribution over all positions in the\nsentence, which corresponds to the probability of each token being the ancestor of this token. The loss function is deﬁned as the\nmean Kullback–Leibler (KL) divergence between the output distributions of all tokens and the corresponding prior distributions.\nbeing higher on ancestors that are closer to that token. Dur-\ning training, we obtain the prior attention weights based on\nthe outputs of a pre-trained dependency parser.\nFor example, in the utterance “ list ﬂights arriving in\nToronto on March ﬁrst ”, the syntactic parse ancestors of\nword “ﬁrst” are “March”, “arriving” and “ﬂights” which are\n1, 2 and 3 hops away on the dependency tree, respectively,\nas shown in Figure 4. The ancestors are syntactically mean-\ningful for the determination of the slot of “ ﬁrst”, which is\n“arrive date, day number” in this case.\nTo train the attention head to assign higher weights on the\nancestors of each token, we deﬁne prior attention weights\nof each token based on the distance between the token and\nits ancestors. Formally, the prior attention weights of token\niare deﬁned as:\nwprior\ni;j =\n\u001a\nsoftmax(\u0000di;j=\u001c) if j 2ancestors(i)\n0 if j =2ancestors(i)\n(4)\nin which di;j is the distance between tokeniand j, softmax\nis the Softmax function, \u001c is the temperature of the Softmax\nfunction controlling the variance of the attention weights\nover all the ancestors and i;j 2 f1;2;:::;T g. The stack\nof prior attention weights of all T tokens is a T \u0002T matrix,\ndenoted by Wprior. We train our model to decrease the dif-\nference between Wprior and the attention matrix W(s)\nh out-\nput by the attention head hat the sth layer. The difference is\nmeasured by the mean of row-wise KL divergence between\nthese two matrices, which is used as an additional loss func-\ntion besides the NLU loss functions. We refer to this loss as\ndependency loss denoted by Ldep:, formally:\nLdep: = 1\nT\nTX\ni=1\nDKL(Wprior\ni kW(s)\nh;i ) (5)\nW(s)\nh = softmax(Q(s)Udep:(K(s))T ) (6)\nin which DKL(\u0001) denotes the KL divergence,Q(s) and K(s)\nflightslist arriving\nROOT\nTorontoMarchin on first\nPrior attention weights\nFigure 4: Syntactic dependency tree of “ list ﬂights arriv-\ning in Toronto on March ﬁrst ” and prior attention weights\nof word “ﬁrst”.\nare linear transformations of E(s\u00001), W(s)\nh;i is the ith row of\nW(s)\nh , and Udep: is a parameter matrix. In (6) we use the\nbiafﬁne attention instead of the scaled dot product attention,\nwhich has been shown to be effective for dependency pars-\ning (Dozat and Manning 2016).\nWe treat \u001c as a hyperparameter and tune it on the valida-\ntion set. With \u001c !0+, attention head hwill be trained to\nonly pay attention to the direct parent of each token, a spe-\ncial case used by (Strubell et al. 2018). Thus, our method is\na more general approach compared to (Strubell et al. 2018).\nEncoding Part-of-Speech Knowledge\nPart-of-Speech (POS) information is important for disam-\nbiguating words with multiple meanings (Alva and Hegde\n2016). This is because an ambiguous word carries a speciﬁc\nPOS in a particular context (Pal, Munshi, and Saha 2015).\nFor instance, the word “ May” could be either a verb or a\nnoun. Being aware of its POS tag is beneﬁcial for down-\nstream tasks, such as predicting the slots in the utterance\n13945\n“book a ﬂight on May 1st ”. Furthermore, previous studies\nhave shown that while models trained for a sufﬁciently large\nnumber of steps can potentially learn underlying patterns of\nPOS, the knowledge is imperfect (Jawahar, Sagot, and Sed-\ndah 2019; Sundararaman et al. 2019). For these reasons, we\nexplicitly train our model to perform POS tagging using the\nPOS tags generated by a pretrained POS tagger.\nSimilar to slot ﬁlling, we simplify POS tagging as a token-\nlevel classiﬁcation problem. We apply a MLP-based classi-\nﬁer on the output embeddings of therth transformer encoder\nlayer and use cross entropy as the loss function:\nppos\ni;o = softmax(MLP(er;i)) (7)\nLpos = \u0000\nTX\ni=1\nOX\no=1\nypos\ni;o log ppos\ni;o (8)\nin which ppos\ni;o is the predicted probability of the ith token’s\nPOS label being the oth label in the POS label space, O is\nthe total number of POS labels,ypos\ni;o is the one-hot represen-\ntation of the groundtruth POS label.\nIntent Detection and Slot Filling\nIntent detection: We apply a linear classiﬁer on the em-\nbedding of the “[SOS]” token, eL;0, which is output by the\nlast Transformer encoder layer L. Cross entropy loss is used\nfor intent detection. The loss on one utterance is deﬁned as:\npint:\nn = softmax(Wint:eT\nL;0 + bint:) (9)\nLint: = \u0000\nNX\nn=1\nyint:\nn log pint:\nn (10)\nin which Wint: and bint: are the parameters of the linear\nclassiﬁer, yint: is the one-hot representation of the ground\ntruth intent label, N is the total number of intent labels and\npint:\nn is the predicted probability of this utterance’s intent\nlabel being the nth label in the intent label space.\nSlot ﬁlling: We apply a MLP-based classiﬁer on the em-\nbeddings output by the last Transformer encoder layer using\ncross entropy as the loss function. The loss on one utterance\nis deﬁned as follow:\npslot\ni;s = softmax(MLP(eL;i)) (11)\nLslot = \u0000\nTX\ni=1\nSX\ns=1\nyslot\ni;s log pslot\ni;s (12)\nin which pslot\ni;s is the predicted probability of the ith token’s\nslot being the sth label in the slot space, S is the total num-\nber of slots, and yslot\ni;s is the one-hot representations of the\nground truth slot label.\nMulti-task Learning\nWe train our model via multi-task learning (Caruana 1993).\nOur loss function is deﬁned as:\nL= LNLU + cdep \u0001Ldep + cpos \u0001Lpos (13)\nwhere LNLU equals to Lslot for slot ﬁlling, Lint: for intent\ndetection or Lslot + Lint: for joint training, and cdep and\ncpos are coefﬁcients of the dependency prediction loss and\nthe POS tagging loss, respectively. cdep and cpos are treated\nas hyperparameters and selected based on validation perfor-\nmance.\nExperiments\nDatasets\nWe conducted experiments on two benchmark datasets: the\nAirline Travel Information Systems (ATIS) (Hemphill, God-\nfrey, and Doddington 1990) and SNIPS (Coucke et al. 2018)\ndatasets. The ATIS dataset has a focus on airline information\nand has been used as benchmark on NLU tasks. We used the\nsame version as (Goo et al. 2018; E et al. 2019) that contains\n4,478 utterances for training, 500 for validation and 893 for\ntesting. The SNIPS dataset has a focus on personal assistant\ncommands, with a larger vocabulary size and more diverse\nintents and slots. It contains 13,084 utterances for training,\n700 for validation and 700 for testing.\nEvaluation Metrics\nWe use classiﬁcation accuracy for intent detection and the\nF1 score for slot ﬁlling, which is the harmonic mean of pre-\ncision and recall. For the SNIPS dataset, we use the same\nversion and evaluation method as pervious works (Zhang\net al. 2020). For the ATIS dataset, we ﬁnd that previous\nworks use two different evaluation methods for intent de-\ntection on utterances with multiple labels. The ﬁrst method\ncounts a prediction as correct if it is equal to one of the\nground truth labels of the utterance (Liu and Lane 2016). We\nrefer this method as the single label matching method (ID-\nS). The second method counts a prediction as correct only\nif it matches all labels of the utterance (Goo et al. 2018;\nE et al. 2019). We refer this method as the multiple label\nmatching method (ID-M). We report both in our results.\nImplementation Details\nOur experiments are implemented in PyTorch (Paszke et al.\n2017). The hyperparameters are selected based on the per-\nformance on the validation set. We use the Adam opti-\nmizer (Kingma and Ba 2015) with \f1 = 0 :9, \f2 =\n0:999, \u000f = 10\u00007 and the weight decay ﬁx as described\nin (Loshchilov and Hutter 2017). Our learning rate sched-\nule ﬁrst increases the learning rate linearly from 0 to 0.0005\n(warming up) and then decreases it to 0 following the values\nof the cosine function. We use warming up steps \u001920% of\nthe total training steps. The speciﬁc number of warming up\nsteps is determined by validation performance. We use the\nimplementation of the optimizer and learning rate scheduler\nof the Transformers library (Wolf et al. 2019).\nWe use Stanza (Qi et al. 2020) to generate training la-\nbels for POS tagging and dependency prediction. For the\nNLU model trained with both dependency prediction and\nPOS tagging, cdep and cpos are both set to 1. For the NLU\nmodel trained with only dependency prediction, cdep is set\nto 5. We used weight decay of 0.1 and dropout rate (Sri-\nvastava et al. 2014) of 0.1 and 0.3 for the SNIPS and ATIS\n13946\nSNIPS ATIS\nSF ID SF ID-M ID-S\nJoint Seq (Hakkani-T¨ur et al. 2016) 87.30 96.90 94.30 92.60 -\nAttention-based RNN (Liu and Lane 2016) 87.80 96.70 95.78 - 97.98\nSlot-Gated (Goo et al. 2018) 89.27 96.86 95.42 95.41 -\nSF-ID, SF ﬁrst (E et al. 2019) 91.43 97.43 95.75 97.76 -\nSF-ID, ID ﬁrst (E et al. 2019) 92.23 97.29 95.80 97.09 -\nStack-Propagation (Qin et al. 2019) 94.20 98.00 95.90 96.90 -\nGraph LSTM (Zhang et al. 2020) 95.30 98.29 95.91 97.20 -\nTF 96.37 98.29 95.31 96.42 97.65\nSyntacticTF (Independent) 96.56 98.71 95.94 97.76 98.10\nSyntacticTF (Joint) 96.89 99.14 96.01 97.31 98.32\nJointBERT (Chen, Zhuo, and Wang 2019)\u0003 97.00 98.60 96.10 97.50 -\nTable 1: SF and ID results on the ATIS and SNIPS dataset (%). TF refers to the Transformer encoder-based model trained\nwithout syntactic information.SyntacticTF refers to our model.Independent and Joint refer to independently and jointly training\nfor SF and ID, respectively.ID-M refers to multiple label matching for intent detection evaluation andID-S refers to single label\nmatching. \u0003This work relies on pretraining, which is not required by other works in the table.\ndataset, respectively. We use batch size of 32 and train each\nmodel for 100 epochs. We report the testing results of the\ncheckpoints achieving the best validation performance.\nWe use the concatenation of GloVe embeddings (Pen-\nnington, Socher, and Manning 2014) and character embed-\ndings (Hashimoto et al. 2017) as token embeddings and keep\nthem frozen during training. The hidden dimension of the\nTransformer encoder layer is 768 and the size of feed for-\nward layer is 3072. Considering the small size of the two\ndatasets, we only use two Transformer encoder layers in to-\ntal (with x = 1, y = 0 and z = 0 as in Figure 2), each\nof which has 4 attention heads. The dimension of Q(s) and\nK(s) is 200. For slot ﬁling, we apply the Viterbi decoding at\ntest time. BIO is a standard format for slot ﬁlling annotation\nschema, as shown in Figure 1. The transition probabilities\nare manually set to ensure the output sequences of BIO la-\nbels to be valid, by simply specifying the probabilities of\ninvalid transition to zero and the probabilities of valid tran-\nsition to one.\nBaseline Models\nWe compare our proposed model with the following baseline\nmodels:\n• Joint Seq (Hakkani-T ¨ur et al. 2016) is a joint model for\nintent detection and slot ﬁlling based on the bi-directional\nLSTM model.\n• Attention-based RNN (Liu and Lane 2016) is a sequence-\nto-sequence model with the attention mechanism.\n• Slot-Gated (Goo et al. 2018) utilizes intent information\nfor slot ﬁlling through the gating mechanism.\n• SF-ID (E et al. 2019) is an architecture that enables the\ninteraction between intent detection and slot ﬁlling.\n• Stack-Propagation (Qin et al. 2019) is a joint model based\non the Stack-Propagation framework.\n• Graph LSTM (Zhang et al. 2020) is based on the Graph\nLSTM model.\n• JointBERT (Chen, Zhuo, and Wang 2019) is a joint NLU\nmodel ﬁned tuned from the pretrained BERT model (De-\nvlin et al. 2018).\n• TF is the Transformer encoder-based model trained with-\nout syntactic information.\nResults\nTable 1 shows the performance of the baseline and proposed\nmodels for SF and ID on the SNIPS and ATIS dataset, re-\nspectively. Overall, our proposed models achieve the best\nperformance on the two benchmarking datasets. On the\nSNIPS dataset, our proposed joint model achieves an ab-\nsolute F1 score and accuracy improvement of 1:59% and\n0:85% for SF and ID, respectively, compared to the best per-\nformed baseline model without pre-training (Zhang et al.\n2020). On the ATIS dataset, our proposed joint model also\nachieves an absolute F1 score and accuracy improvement of\n0:1% and 0:34% for SF and ID-S, compared to the best per-\nformed baseline model for SF (Zhang et al. 2020) and ID-\nS (Liu and Lane 2016), respectively. In addition, our pro-\nposed independent model achieves the same performance as\nthe best performed baseline model on ID-M (E et al. 2019,\nSF-ID, SF ﬁrst).\nBesides, the model based on Transformer encoder without\nsyntactic knowledge can achieve SOTA results on the SNIPS\ndataset and is slightly worse than the SOTA results on the\nATIS dataset. This indicates the powerfulness of the Trans-\nformer encoder for SF and ID. Moreover, the further im-\nprovement of our models over the baseline models demon-\nstrates the beneﬁts of incorporating syntactic knowledge.\nAdditionally, compared to previous works with heteroge-\nnous model structures, our models are purely based on self-\nattention and feed forward layers.\nWe also ﬁnd that our proposed models can outperform\nthe JointBERT model with pre-training (Chen, Zhuo, and\nWang 2019) for intent detection tasks. Compared to the\nJointBERT model, our proposed joint model achieves an ab-\nsolute accuracy improvement of 0:54% for ID on the SNIPS\n13947\nSNIPS ATIS\nSF ID SF ID-M ID-S\nTF 96.37 98.29 95.31 96.42 97.65\nTF + D 96.31 98.43 95.99 96.53 98.76\nTF + P 96.47 98.57 95.82 97.31 98.10\nTF + D + P 96.56 98.71 95.94 97.76 98.10\nTable 2: Results of ablation study. TF refers to the baseline\nmodels with two Transformer encoder layers.D and P refers\nto dependency prediction and POS tagging, respectively.\ndataset; and our proposed independent model achieves an\nabsolute accuracy improvement of 0:26% for ID-M on the\nATIS dataset. While our proposed model does not outper-\nform the JointBERT model for SF, the performance gap is\nrelatively small ( 0:11% on SNIPS and 0:09% on ATIS). It\nshould be noted that our model does not require pre-training\nand the size of our model is only one seventh of the Joint-\nBERT model (16 million vs. 110 million parameters).\nPrevious works have shown that models like BERT can\nlearn syntactic knowledge by self-supervision (Clark et al.\n2019; Manning et al. 2020). This can partially explain why\nthe JointBERT can achieve very good results without being\nfed with syntactic knowledge explicitly.\nAblation Study\nTable 2 shows the ablation study results of the effects of\nadding different syntactic information. A ﬁrst observation\nis that the model trained with a singe syntactic task, ei-\nther dependency prediction or POS tagging, outperforms the\nbaseline Transformer encoder-based model without syntac-\ntic information. This gives us conﬁdence that syntactic in-\nformation can help improve model performance. Moreover,\ntraining a Transformer model with both the syntactic tasks\nachieves even better results than training with a single syn-\ntactic task. This could be because the POS tagging task im-\nproves the performance of the dependency prediction task\n(Nguyen and Verspoor 2018), which in turn improves the\nperformance of SF and ID.\nInterestingly, we observe that the addition of dependency\nprediction reduces the performance of slot ﬁlling on the\nSNIPS dataset (96:31%) when compared to the baseline\nTransformer encoder-based model (96:37%). There are sev-\neral potential reasons. Firstly, the sentences in the SNIPS\ndataset are overall shorter than the ATIS dataset so that\nthe syntactic dependency information might be less help-\nful. Secondly, previous work has shown that syntactic pars-\ning performance often suffers when a named entity span has\ncrossing brackets with the spans on the parse tree (Finkel\nand Manning 2009). Thus, the dependency prediction per-\nformance of our model might decrease due to the presence\nof many name entities in the SNIPS dataset, such as song\nnames and movie names, which could introduce noisy de-\npendency information into the attention weights and degrade\nthe performance on the NLU tasks.\nSNIPS ATIS\nSF ID SF ID-S\nTF + Par. 96.20 98.29 95.58 98.10\nTF + Anc. 96.31 98.43 95.99 98.76\nTable 3: Intent detection and slot ﬁlling results of Trans-\nformer (TF) encoder-based models with dependency parent\nprediction (Par.) and dependency ancestor prediction (Anc.)\non the ATIS and SNIPS dataset.\nQualitative Analysis\nWe qualitatively examined the errors made by the Trans-\nformer encoder-based models with and without syntactic in-\nformation to understand in what ways syntactic information\nhelps improve the performance. Our major ﬁndings are:\nID errors related to preposition with nouns: Preposi-\ntions, when appearing between nouns, are used to describe\ntheir relationship. For example, in the utterance “kansas city\nto atlanta monday morning ﬂights”, the preposition “to”\ndenotes the direction from “kansas city” (departure loca-\ntion, noun) to “atlanta” (arrival location, noun). Without this\nknowledge, a model could misclassify the intent of this ut-\nterance as asking for city information rather than ﬂight in-\nformation. We found that about 50% of the errors made by\nthe model without syntactic information contain this pattern,\nwhereas less than 10% of the misclassiﬁed utterances con-\ntain this pattern for the model with syntactic information\n(See Appendix A for the full list).\nSF errors due to POS confusion:A Word can have mul-\ntiple meanings depending on context. For example, the same\nword “may” can be a verb expressing possibility, or as a\nnoun referring to the ﬁfth month of the year. We found that\ncorrectly recognizing the POS of words could potentially\nhelp reduce slot ﬁlling errors. For example, in this utterance\n“May I have the movie schedules for Speakeasy Theaters”,\nthe slot for “May” should be empty, but the model without\nsyntactic information predicts it as “Time Range”. By con-\ntrast, the model with syntactic information predicts correctly\nfor this word, probably because the confusion of noun vs.\nverb for the word “May” is addressed by incorporating POS\ninformation. More examples are included in Appendix A.\nParent Prediction vs. Ancestor Prediction\nWe compare our approach of predicting all ancestors of each\ntoken with the approach described in (Strubell et al. 2018),\nwhich only predicts direct dependency parent of each to-\nken. Results in Table 3 show that the model with our ap-\nproach can achieve better results for both ID and SF on the\ntwo datasets, which demonstrates that our approach is more\nbeneﬁcial to the NLU tasks. We hypothesize that incorpo-\nrating syntactic ancestor prediction can better capture long-\ndistance syntactic relationship. As shown in (Tur, Hakkani-\nT¨ur, and Heck 2010), long distance dependencies are im-\nportant for slot ﬁlling. For example, in the utterance “Find\nﬂights to LA arriving in no later than next Monday”, a 6-\ngram context is needed to ﬁgure out that “Monday” is the\narrival date instead of the departure date.\n13948\nL1\nL2\nTransformer\nL1\nL2\nLinguistically-Informed TransformerA1 A2 A3 A4\nA1 A2 A3 A4\nUsed for dependency prediction\nFigure 5: Visualization of the attention weights of the model\nwith and without syntactic supervision for slot ﬁlling. Li\nand Aj stands for the ith Transformer layer andjth attention\nhead, respectively. The attention head inside the red-dotted\nbox is trained for dependency prediction.\nVisualization of Attention Weights\nWe visualize the attention weights output by models trained\nwith and without syntactic information to understand what\nthe models have learned by incorporating syntactic informa-\ntion. We select the utterance “show me the ﬂights on ameri-\ncan airlines which go from st. petersburg to ontario califor-\nnia by way of st. louis ” from the ATIS testing set. Only the\nmodel trained with syntactic information predicts the slot\nlabels correctly. As shown in Figure 5, the model without\nsyntactic information has simple attention patterns on both\nlayers, such as looking backward and looking forward. Other\nattention heads seem to be random and less informative.\nIn contrast, the model with syntactic information has\nmore informative attention patterns. On the ﬁrst layer, all\nthe attention heads present simple but diverse patterns. Be-\nsides looking forward and backwards, the second attention\nhead looks at both directions for each token. On the sec-\nond layer, however, we observe more complex patterns and\nlong-distance attention which could account for more task-\noriented operations. Therefore, it is possible that the Trans-\nformer encoder learns attention weights better with syntactic\ninformation supervision so that the encoder can leave more\npower for the end task.\nRelated Work\nResearch on intent detection and slot ﬁlling emerged in the\n1990sfrom the call classiﬁcation systems (Gorin, Riccardi,\nand Wright 1997) and the ATIS project (Price 1990). Early\nwork has primarily focused on using a traditional machine\nlearning classiﬁer such as CRFs (Haffner, Tur, and Wright\n2003). Recently, there has been an increasing application of\nneural models on NLU tasks. These approaches, primarily\nbased on RNNs, have shown that neural approaches outper-\nform traditional models (Mesnil et al. 2014; Tur et al. 2012;\nZhang and Wang 2016; Goo et al. 2018; E et al. 2019). For\nexample, Mesnil et al (2015) employed RNNs for slot ﬁlling\nand found an 2:3% relative improvement of F1 compared to\nCRF (Mesnil et al. 2014). Some works also explored Trans-\nformer encoder and graph LSTM-based neural architectures\n(Chen, Zhuo, and Wang 2019; Zhang et al. 2020).\nSyntactic information has been shown to be beneﬁcial to\nmany tasks, such as neural machine translation (Akoury, Kr-\nishna, and Iyyer 2019), semantic role labeling (Strubell et al.\n2018), and machine reading comprehension (Zhang et al.\n2020). Research on NLU tasks has also shown that incorpo-\nrating syntactic information into machine learning models\ncan help improve the performance. Moschitti et al. (2007)\nused syntactic information for slot ﬁlling, where the authors\nused a tree kernel function to encode the structural informa-\ntion acquired by a syntactic parser. An extensive analysis on\nthe ATIS dataset revealed that most NLU errors are caused\nby complex syntactic characteristics, such as prepositional\nphrases and long distance dependencies (Tur, Hakkani-T ¨ur,\nand Heck 2010). Tur et al. (2011) proposed a rule-based de-\npendency parsing based sentence simpliﬁcation method to\naugment the input utterances based on the syntactic struc-\nture. Compared to previous works, our work is the ﬁrst to\nencode syntactical knowledge into end-to-end neural mod-\nels for intent detection and slot ﬁlling.\nConclusion\nIn this paper, we propose to encode syntactic knowledge into\nthe Transformer encoder-based model for intent detection\nand slot ﬁlling. Experimental results indicate that a model\nwith only two Transformer encoder layers can already match\nor even outperform the SOTA performance on two bench-\nmark datasets. Moreover, we show that the performance of\nthis baseline model can be further improved by incorpo-\nrating syntactical supervision. The visualization of the at-\ntention weights also reveals that syntactical supervision can\nhelp the model to better learn syntactically-related patterns.\nFor future work, we will evaluate our approach with larger\nmodel sizes on larger scale datasets containing more syntac-\ntically complex utterances. Furthermore, we will investigate\nincorporating syntactic knowledge into models pretrained\nby self-supervision and applying those models on the NLU\ntasks.\nAcknowledgments\nWe would like to thank Siegfried Kunzmann, Nathan Susanj,\nRoss McGowan, and anonymous reviewers for their insight-\nful feedback that greatly improved our paper.\nAppendix A\nBelow lists examples of the intent detection errors made\nby the model without syntactic information that are related\n13949\nto one speciﬁc grammar pattern between prepositions and\nnouns.\n• cleveland to kansas city arrive monday before 3 pm\n• kansas city to atlanta monday morning ﬂights\n• new york city to las vegas and memphis to las vegas on\nSunday\nBelow lists examples of the slot ﬁlling errors made by\nthe model without syntactic information that contain POS\nconfusion.\n• cleveland to kansas city arrive monday before 3 pm\n• new york city to las vegas and memphis to las vegas on\nSunday\n• baltimore to kansas city economy\nThe Transformer encoder-based model without syntac-\ntic information made mistakes on all these utterances. The\nmodel trained with POS tagging and the model trained with\nboth POS tagging and dependency prediction fail on the last\nutterance in the list below. The model trained with depen-\ndency prediction does not make any mistakes on all these ut-\nterances. We underline the words that are assigned to wrong\nslots by the model without syntactic information.\n• book a reservation for velma an a and rebecca for an amer-\nican pizzeria at\n(correct: B \u0000TimeRange; prediction:\nB\u0000RestaurantName) 5 Am in MA\n• Where is Belgium located (correct: Other; prediction:\nB\u0000PatialRelation)\n• May(correct: Other; prediction: B\u0000TimeRange) I have\nthe movie schedules for Speakeasy Theaters\nReferences\nAkoury, N.; Krishna, K.; and Iyyer, M. 2019. Syntactically\nsupervised transformers for faster neural machine transla-\ntion. arXiv preprint arXiv:1906.02780 .\nAllen, J. 1995. Natural Language Understanding (2nd\nEd.). USA: Benjamin-Cummings Publishing Co., Inc. ISBN\n0805303340.\nAlva, P.; and Hegde, V . 2016. Hidden markov model for pos\ntagging in word sense disambiguation. In2016 International\nConference on Computation System and Information Tech-\nnology for Sustainable Solutions (CSITSS), 279–284. IEEE.\nCaruana, R. 1993. Multitask learning: a knowledge-based\nsource of inductive bias. In ICML’93 Proceedings of the\nTenth International Conference on International Conference\non Machine Learning, 41–48.\nChen, Q.; Zhuo, Z.; and Wang, W. 2019. BERT for\nJoint Intent Classiﬁcation and Slot Filling. arXiv preprint\narXiv:1902.10909 .\nChen, Y .-N.; Hakanni-T ¨ur, D.; Tur, G.; Celikyilmaz, A.;\nGuo, J.; and Deng, L. 2016. Syntax or semantics?\nknowledge-guided joint semantic frame parsing. In 2016\nIEEE Spoken Language Technology Workshop (SLT), 348–\n355. IEEE.\nClark, K.; Khandelwal, U.; Levy, O.; and Manning, C. D.\n2019. What does bert look at? an analysis of bert’s attention.\narXiv preprint arXiv:1906.04341 .\nCollobert, R.; Weston, J.; Bottou, L.; Karlen, M.;\nKavukcuoglu, K.; and Kuksa, P. 2011. Natural language pro-\ncessing (almost) from scratch. Journal of machine learning\nresearch 12(ARTICLE): 2493–2537.\nCoucke, A.; Saade, A.; Ball, A.; Bluche, T.; Caulier, A.;\nLeroy, D.; Doumouro, C.; Gisselbrecht, T.; Caltagirone, F.;\nLavril, T.; et al. 2018. Snips voice platform: an embed-\nded spoken language understanding system for private-by-\ndesign voice interfaces. arXiv preprint arXiv:1805.10190 .\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. arXiv preprint arXiv:1810.04805\n.\nDozat, T.; and Manning, C. D. 2016. Deep biafﬁne at-\ntention for neural dependency parsing. arXiv preprint\narXiv:1611.01734 .\nE, H.; Niu, P.; Chen, Z.; and Song, M. 2019. A Novel Bi-\ndirectional Interrelated Model for Joint Intent Detection and\nSlot Filling. In ACL 2019 : The 57th Annual Meeting of the\nAssociation for Computational Linguistics, 5467–5471.\nFinkel, J. R.; and Manning, C. D. 2009. Joint parsing and\nnamed entity recognition. In Proceedings of Human Lan-\nguage Technologies: The 2009 Annual Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics, 326–334.\nGoo, C.-W.; Gao, G.; Hsu, Y .-K.; Huo, C.-L.; Chen, T.-C.;\nHsu, K.-W.; and Chen, Y .-N. 2018. Slot-gated Modeling\nfor Joint Slot Filling and Intent Prediction. In NAACL HLT\n2018: 16th Annual Conference of the North American Chap-\nter of the Association for Computational Linguistics: Hu-\nman Language Technologies, volume 2, 753–757.\nGorin, A. L.; Riccardi, G.; and Wright, J. H. 1997. How may\nI help you? Speech communication 23(1-2): 113–127.\nHaffner, P.; Tur, G.; and Wright, J. H. 2003. Optimizing\nSVMs for complex call classiﬁcation. In2003 IEEE Interna-\ntional Conference on Acoustics, Speech, and Signal Process-\ning, 2003. Proceedings.(ICASSP’03)., volume 1, I–I. IEEE.\nHakkani-T¨ur, D.; T ¨ur, G.; C ¸ elikyilmaz, A.; Chen, Y .-N.;\nGao, J.; Deng, L.; and Wang, Y .-Y . 2016. Multi-Domain\nJoint Semantic Frame Parsing Using Bi-Directional RNN-\nLSTM. In Interspeech 2016, 715–719.\nHashimoto, K.; Xiong, C.; Tsuruoka, Y .; and Socher, R.\n2017. A Joint Many-Task Model: Growing a Neural Net-\nwork for Multiple NLP Tasks. In Proceedings of the 2017\nConference on Empirical Methods in Natural Language\nProcessing, 1923–1933.\nHemphill, C. T.; Godfrey, J. J.; and Doddington, G. R. 1990.\nThe ATIS spoken language systems pilot corpus. In Speech\nand Natural Language: Proceedings of a Workshop Held at\nHidden Valley, Pennsylvania, June 24-27, 1990.\nJawahar, G.; Sagot, B.; and Seddah, D. 2019. What does\nBERT learn about the structure of language?\n13950\nKingma, D. P.; and Ba, J. L. 2015. Adam: A Method for\nStochastic Optimization. In ICLR 2015 : International Con-\nference on Learning Representations 2015.\nLiu, B.; and Lane, I. 2016. Attention-Based Recurrent Neu-\nral Network Models for Joint Intent Detection and Slot Fill-\ning. In Interspeech 2016, 685–689.\nLoshchilov, I.; and Hutter, F. 2017. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101 .\nManning, C. D.; Clark, K.; Hewitt, J.; Khandelwal, U.; and\nLevy, O. 2020. Emergent linguistic structure in artiﬁcial\nneural networks trained by self-supervision. Proceedings of\nthe National Academy of Sciences .\nMesnil, G.; Dauphin, Y .; Yao, K.; Bengio, Y .; Deng, L.;\nHakkani-Tur, D.; He, X.; Heck, L.; Tur, G.; Yu, D.; et al.\n2014. Using recurrent neural networks for slot ﬁlling in spo-\nken language understanding. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing 23(3): 530–539.\nMoschitti, A.; Riccardi, G.; and Raymond, C. 2007. Spoken\nlanguage understanding with kernels for syntactic/semantic\nstructures. In 2007 IEEE Workshop on Automatic Speech\nRecognition & Understanding (ASRU), 183–188. IEEE.\nNguyen, D. Q.; and Verspoor, K. 2018. An improved neural\nnetwork model for joint POS tagging and dependency pars-\ning. arXiv preprint arXiv:1807.03955 .\nPal, A. R.; Munshi, A.; and Saha, D. 2015. An Approach\nto Speed-up the Word Sense Disambiguation Procedure\nthrough Sense Filtering. arXiv preprint arXiv:1610.06601\n.\nPaszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.;\nDeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer,\nA. 2017. Automatic differentiation in PyTorch. In NIPS-W.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of\nthe 2014 conference on empirical methods in natural lan-\nguage processing (EMNLP), 1532–1543.\nPrice, P. 1990. Evaluation of spoken language systems: The\nATIS domain. In Speech and Natural Language: Proceed-\nings of a Workshop Held at Hidden Valley, Pennsylvania,\nJune 24-27, 1990.\nQi, P.; Zhang, Y .; Zhang, Y .; Bolton, J.; and Manning,\nC. D. 2020. Stanza: A Python Natural Language Processing\nToolkit for Many Human Languages. In Proceedings of the\n58th Annual Meeting of the Association for Computational\nLinguistics: System Demonstrations.\nQin, L.; Che, W.; Li, Y .; Wen, H.; and Liu, T. 2019. A\nStack-Propagation Framework with Token-Level Intent De-\ntection for Spoken Language Understanding. In 2019 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing, 2078–2087.\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2014. Dropout: a simple way to prevent\nneural networks from overﬁtting.Journal of Machine Learn-\ning Research 15(1): 1929–1958.\nStrubell, E.; Verga, P.; Andor, D.; Weiss, D.; and McCallum,\nA. 2018. Linguistically-Informed Self-Attention for Seman-\ntic Role Labeling. In EMNLP 2018: 2018 Conference on\nEmpirical Methods in Natural Language Processing, 5027–\n5038.\nSundararaman, D.; Subramanian, V .; Wang, G.; Si, S.;\nShen, D.; Wang, D.; and Carin, L. 2019. Syntax-Infused\nTransformer and BERT models for Machine Translation\nand Natural Language Understanding. arXiv preprint\narXiv:1911.06156 .\nTur, G.; Deng, L.; Hakkani-T ¨ur, D.; and He, X. 2012. To-\nwards deeper understanding: Deep convex networks for se-\nmantic utterance classiﬁcation. In 2012 IEEE interna-\ntional conference on acoustics, speech and signal process-\ning (ICASSP), 5045–5048. IEEE.\nTur, G.; Hakkani-T¨ur, D.; and Heck, L. 2010. What is left\nto be understood in ATIS? In 2010 IEEE Spoken Language\nTechnology Workshop, 19–24. IEEE.\nTur, G.; Hakkani-Tur, D.; Heck, L.; and Parthasarathy, S.\n2011. Sentence simpliﬁcation for spoken language under-\nstanding. In 2011 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), 5628–5631.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All You Need. In Proceedings of the 31st Inter-\nnational Conference on Neural Information Processing Sys-\ntems, 5998–6008.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-\nson, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y .; Plu,\nJ.; Xu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest,\nQ.; and Rush, A. M. 2019. HuggingFace’s Transform-\ners: State-of-the-art Natural Language Processing. ArXiv\nabs/1910.03771.\nZhang, L.; Ma, D.; Zhang, X.; Yan, X.; and Wang, H. 2020.\nGraph LSTM with Context-Gated Mechanism for Spoken\nLanguage Understanding. AAAI 2020 : The Thirty-Fourth\nAAAI Conference on Artiﬁcial Intelligence 34(5): 9539–\n9546.\nZhang, X.; and Wang, H. 2016. A joint model of intent deter-\nmination and slot ﬁlling for spoken language understanding.\nIn IJCAI, volume 16, 2993–2999.\nZhang, Z.; Wu, Y .; Zhou, J.; Duan, S.; Zhao, H.; and Wang,\nR. 2020. SG-Net: Syntax-Guided Machine Reading Com-\nprehension. In AAAI, 9636–9643.\n13951"
}