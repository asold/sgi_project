{
  "title": "View-target relation-guided unsupervised 2D image-based 3D model retrieval via transformer",
  "url": "https://openalex.org/W4386137987",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2263995244",
      "name": "Jiacheng Chang",
      "affiliations": [
        "Harbin Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2119066709",
      "name": "Lanyong Zhang",
      "affiliations": [
        "Harbin Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2154362757",
      "name": "Zhuang Shao",
      "affiliations": [
        "University of Warwick"
      ]
    },
    {
      "id": "https://openalex.org/A2263995244",
      "name": "Jiacheng Chang",
      "affiliations": [
        "Harbin Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2119066709",
      "name": "Lanyong Zhang",
      "affiliations": [
        "Harbin Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2154362757",
      "name": "Zhuang Shao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3092675243",
    "https://openalex.org/W4286486474",
    "https://openalex.org/W4294009699",
    "https://openalex.org/W4297450596",
    "https://openalex.org/W2612326916",
    "https://openalex.org/W2902078856",
    "https://openalex.org/W2884771968",
    "https://openalex.org/W2616287544",
    "https://openalex.org/W2981540341",
    "https://openalex.org/W4313184106",
    "https://openalex.org/W4312719508",
    "https://openalex.org/W2776346076",
    "https://openalex.org/W4312361860",
    "https://openalex.org/W3112065721",
    "https://openalex.org/W4387643249",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1644641054",
    "https://openalex.org/W3205493274",
    "https://openalex.org/W3210811820",
    "https://openalex.org/W4205312993",
    "https://openalex.org/W3190654993",
    "https://openalex.org/W2981506675",
    "https://openalex.org/W2922085350",
    "https://openalex.org/W4292794050",
    "https://openalex.org/W4312768455",
    "https://openalex.org/W4327923876",
    "https://openalex.org/W4311759245",
    "https://openalex.org/W770922146",
    "https://openalex.org/W3103114094",
    "https://openalex.org/W4206998227",
    "https://openalex.org/W4312792979",
    "https://openalex.org/W2322020277",
    "https://openalex.org/W2974212192",
    "https://openalex.org/W3111980808",
    "https://openalex.org/W4308785766",
    "https://openalex.org/W4386280799",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W3034706886"
  ],
  "abstract": "Abstract Unsupervised 2D image-based 3D model retrieval aims at retrieving images from the gallery of 3D models by the given 2D images. Despite the encouraging progress made in this task, there are still two significant limitations: (1) feature alignment of 2D images and 3D model gallery is still difficult due to the huge gap between the two modalities. (2) The important view information in the 3D model gallery was ignored by the prior arts, which led to inaccurate results. To alleviate these limitations, inspired by the success of vision transformers (ViT) in a great variety of vision tasks, in this paper, we propose an end-to-end 3D model retrieval architecture on top of ViT, termly transformer-based 3D model retrieval network (T3DRN). In addition, to take advantage of the valuable view information of 3D models, we present an attentive module in T3DRN named shared view-guided attentive module (SVAM) to guide the learning of the alignment features. The proposed method is tested on the challenging dataset, MI3DOR-1. The extensive experimental results have proved the superiority of our proposed method to state-of-the-art methods.",
  "full_text": "Vol.:(0123456789)1 3\nMultimedia Systems (2023) 29:3891–3901 \nhttps://doi.org/10.1007/s00530-023-01166-y\nSPECIAL ISSUE PAPER\nView‑target relation‑guided unsupervised 2D image‑based 3D model \nretrieval via transformer\nJiacheng Chang1 · Lanyong Zhang1 · Zhuang Shao2\nReceived: 14 April 2023 / Accepted: 12 August 2023 / Published online: 24 August 2023 \n© The Author(s) 2023\nAbstract\nUnsupervised 2D image-based 3D model retrieval aims at retrieving images from the gallery of 3D models by the given 2D \nimages. Despite the encouraging progress made in this task, there are still two significant limitations: (1) feature alignment \nof 2D images and 3D model gallery is still difficult due to the huge gap between the two modalities. (2) The important view \ninformation in the 3D model gallery was ignored by the prior arts, which led to inaccurate results. To alleviate these limita-\ntions, inspired by the success of vision transformers (ViT) in a great variety of vision tasks, in this paper, we propose an \nend-to-end 3D model retrieval architecture on top of ViT, termly transformer-based 3D model retrieval network (T3DRN). \nIn addition, to take advantage of the valuable view information of 3D models, we present an attentive module in T3DRN \nnamed shared view-guided attentive module (SVAM) to guide the learning of the alignment features. The proposed method \nis tested on the challenging dataset, MI3DOR-1. The extensive experimental results have proved the superiority of our pro-\nposed method to state-of-the-art methods.\nKeywords 3D model retrieval · Feature alignment · Vision transformer · View-guided attentive module\n1 Introduction\n3D technology is developing rapidly and due to the advance-\nment of computer hardware, 3D models have been widely \nused in wide-ranging areas, such as 3D reconstruction [1 ], \nvirtual reality [2 ], computer-aided medical imaging [3 , 4], \nand 3D object detection [5 ]. Massive 3D models are gen-\nerated by these applications, but it also poses great chal-\nlenges for precise and efficient 3D model retrieval. The goal \nof 3D model retrieval is that given a query, a system needs \nto find similar 3D models in another gallery. The query can \nbe diverse, including 2D images of different views, point \nclouds, etc. Since 2D images are easier to be obtained, the \n2D image-based 3D model retrieval has attracted much \nattention in the computer vision community.\nTo ensure an effective retrieval performance, many \nefforts focused on improving the performance by training \na powerful feature extractor. One typical earlier pipeline is \nto train a deep neural network [6 –9] with a great number \nof labeled annotations, but at a cost of huge labor costs. \nLater on, inspired by the unsupervised domain adaptation \n(UDA) doctrine [ 10–12], unsupervised 2D image-based \n3D model retrieval utilizes the 2D image data and trains \nthe deep model in an unsupervised manner to transfer the \nknowledge learned from labeled 2D image source domain to \nunlabeled 3D model target domain. Specifically, [11] learned \na domain-invariant classifier in the Gaussian manifold and \naligned the conditional distributions of two domains in a \ndynamic manner. Zhang et al. [12] mapped the features of \nthe source domain and target domain into a shared latent \nspace to reduce geometric and distributed displacement \nof statistical measurement. Ganin and Lempitsky [10] \nused adversarial training domain to align the discrepancy \nbetween the source domain and target domain. Yue et al. \n[13] adopted pixel-level alignment to improve the perfor -\nmance of domain adaptation. Chen et al. [14] proposed a \n * Zhuang Shao \n Zhuang.Shao@warwick.ac.uk\n Jiacheng Chang \n dlbjhhh@hrbeu.edu.cn\n Lanyong Zhang \n zhanglanyong@hrbeu.edu.cn\n1 College of Intelligent Systems Science and Engineering, \nHarbin Engineering University, 145 Nantong Street, \nNangang District, Harbin 150001, China\n2 Warwick Manufacturing Group, University of Warwick, IMC \nCentre, Coventry CV4 7AL, UK\n3892 J. Chang et al.\n1 3\ncross-domain adaptation via an image-level mixture method-\nology to align the distribution shift of features between two \ndomains. Peng et al. [15] concentrated on centroid alignment \nbetween features from different image topics and enforced \ndistributed alignment for the already center-aligned features.\nDespite the aforementioned encouraging efforts, the \nperformance of unsupervised 2D image-based 3D model \nretrieval is still far from satisfactory. There are still two \ndrawbacks ignored in the prior arts as follows:\nTo begin with, for feature representation of both 2D \nimages and 3D models, a better backbone is always encour-\naged, which draws our attention to the trendy vision trans-\nformers (ViT) recently. It has proved to be a success in many \nrelative computer vision and natural language processing \n(NLP) such as video event detection [16], pedestrian detec-\ntion [17], person search [18, 19], and text classification [20]. \nViT takes the image patch or word embedding as a sequence \nof tokens, and applies the self-attention mechanism to cap-\nture the internal relationships thus obtaining strong feature \nrepresentation connected with downstream tasks. However, \neven if the wide application, the application in 2D Image-\nbased 3D Model Retrieval is still under-explored.\nSecond, the view information of 3D models is valuable \nbut always ignored by the prior works. As shown in Fig.  1, \nthere are 12 views for each 3D model, and the different view \ninformation is important indeed during the process of feature \nalignment. Therefore, in this paper, we try to mine the view \ninformation and integrate it into the whole retrieval process.\nTo tackle these two gaps above, we propose a novel end-\nto-end 3D model retrieval architecture on top of ViT, dubbed \nTransformer-based 3D model retrieval network (T3DRN). \nT3DRN can effectively overcome the limitation of the dis-\ncrepancy and learn domain-invariant representations. To \nmine more useful view information of 3D models, we also \npresent an attentive module in T3DRN named shared view-\nguided attentive module (SVAM) to guide the learning pro-\ncess in order to better align the two modalities. Our main \ncontributions in this paper are threefold as follows:\n• We propose an end-to-end unsupervised 2D image-based \n3D model retrieval framework on top of ViT, dubbed \ntransformer-based 3D model retrieval network (T3DRN) \nwith a distinctive property of mining proper view infor -\nmation to guide the whole retrieval process.\n• A novel module, termed shared view-guided attentive \nmodule (SVAM), which can be easily integrated into \nT3DRN, is proposed to attend to the proper view infor -\nmation for the 3D model feature training.\n• Qualitative and quantitative experimental results on the \nchallenging unsupervised 2D image-based 3D model \nretrieval datasets show that our method outperforms the \nstate-of-the-art methods.\nThe rest of this paper is organized as follows: To begin \nwith, we review the prior works in Sect. 2. Then, in Sect. 3, \nwe present the proposed methodology and expound on the \ndetails of T3DRN. The experimental results of our proposed \nmethod are demonstrated in Sect.  4 with both qualitative \nand quantitative analysis. Finally, we draw a conclusion and \ndiscuss future work in Sect. 5.\n2  Related work\n2.1  3D model retrieval\nThe purpose of 3D model retrieval is to measure the similar-\nity between the query sample and the samples in the dataset \nand return the relevant 3D model according to the similarity \norder. The typical 3D model retrieval methods are mainly \ndivided into model-based 3D model retrieval methods and \nimage-based 3D model retrieval methods. Model-based \nmethods usually extract features from 3D formats, such \nas point clouds and voxels. For example, [21 ] utilized the \nsupervised convolutional network to encode the binary voxel \ngrids for 3D model representation. Wu et al. [22] represented \na model on a 3D voxel grid using probability distributions of \n(a) View 3( b) View5 (c) View10\nFig. 1  Examples of images of 3D models with different view information\n3893View-target relation-guided unsupervised 2D image-based 3D model retrieval via transformer  \n1 3\nbinary variables. Qi et al. [23] utilized the neighbor points at \nmultiple scales to capture the local structure information. In \nview-based methods, the 3D model is usually projected into \na set of 2D view images. Su et al. [24] used a CNN to extract \nthe feature of each view individually and then adopted max-\npool operation on all view features to obtain a global model \nfeature. Gao et al. [25] utilized patch-level features to learn \nthe content information and spatial information within mul-\ntiple views. Watanabe et al. [26] composed a compact repre-\nsentation of an image-based shape retrieval system employ-\ning a minimum number of views for each model. However, \nthe above methods require a 3D model as the query, which is \nnot convenient for users when they can only obtain an image \nof the 3D model. Therefore, many researchers seek retrieval \napproaches via image-based methods.\n2.2  Domain adaptation\nThe object of domain adaptation is to establish knowledge \ntransfer from the source domain to the target domain. Unsu-\npervised domain adaptation, semi-supervised domain adap-\ntation, and supervised domain adaptation are the three com-\nmon categories of domain adaptation methods. Both source \ndomain data and a few target domain data with labels are \nused during training in semi-supervised domain adaptation \n(SSDA) methods while all samples should be marked in \nsupervised domain adaptation methods. Based on previous \nstudies, [27] benefited from a subset of source domain data \nonly containing samples highly relevant to the target domain \nby eliminating irrelevant source domain samples. To reduce \nthe accuracy discrepancy across domains for better gener -\nalizations, invariant representations and invariant optimal \npredictors were jointly learned in [28]. Unsupervised domain \nadaptation (UDA) methods seek to narrow the domain gap \nbetween the source and target images without any labels \nof the target images and learning domain-invariant features \nis a common strategy for unsupervised domain adaptation. \nFor example, [10] added an additional domain discrimina-\ntor to the conventional CNN network to enforce the domain \nalignment. Zhou et al. [29] ensured the alignment at the class \nlevel by matching the class centroids of the source and tar -\nget domains based on eliminating the offsets at the domain \nlevel. Long et al. [30] utilized the joint maximum mean dif-\nference criterion and used multiple domain-specific layers \nfor adaptation. Multi-layer and multi-kernel maximum mean \ndiscrepancies were minimized between the source and target \ndomain data to resolve domain transfer issues in [31]. Wang \net al. [32] proposed a method that by designing a robust deep \nneural network, both source and target domains can be trans-\nformed to a shared feature space and the classifier trained on \nthe source domain work well on the target domain. Later on, \n[33] trained a model on new target domains, while maintain-\ning its performance on the previous target domains without \nforgetting. Hoyer et al. [34] proposed a method that by train-\ning with pseudo-labels fused from multiple resolutions and \nusing an overlapping sliding window mechanism, the robust-\nness of fine-grained pseudo-labels with respect to different \ncontexts can be increased. The development of domain adap-\ntation has also facilitated numerous close applications, such \nas fault detection [35, 36], frequency detection [37], and \nmodulation classification [38, 39].\n2.3  Transformer\nTransformer was first invented in 2017 in [40]. Later on, \na lot of effort was dedicated to its variants. Among them, \nvision transformer (ViT) [ 41] was a representative work \nthat proposed transformer stacks for the image classifica-\ntion task. Subsequently, many ViT-related structures have \nbeen proposed. In particular, [42 ] developed a novel ViT \nbackbone with a safe training strategy for UDA. Also, the \nencoder–decoder framework of Transformer facilitated vari-\nous vision tasks, such as image captioning [43, 44], relation-\nship or attribute detection [45, 46]. For instance, [47] applied \na Transformer-based dense captioning while [48] focused \non textual context and higher word learning efficiency. \nRecently, due to the necessity of encoding spatial informa-\ntion, the Swin Transformer [49] structure was proposed \nand also applied in many tasks [50]. The application of the \ntransformer-based method for unsupervised 2D image-based \n3D model retrieval, however, is still extremely rare. Besides, \nwe also elaborately design a shared view-guided attentive \nmodule to better arm the transformer-based architecture.\n3  Methodology\nThe overall framework of our proposed T3DRN is shown in \nFig. 2. It is made up of three ViT-block branches with shared \nparameters on top of [42] and a shared view-guided atten-\ntive model(SVAM). In the training period, both 2D image \ndata and 3D model data are sampled. The 2D image data go \ninto the ViT block and be treated as a supervised classifica-\ntion task regularized by cross-entropy loss. The 3D image \ndata follow a contrastive learning regime; it is separated into \ntwo branches, the original branch and randomly perturbed \nbranches, respectively. These two kinds of features are also \nextracted by ViT blocks before going to the SVAM to inter-\nact with the view embeddings. These features are regularized \n3894 J. Chang et al.\n1 3\nby the relative entropy loss term after SVAM. In addition, \nthe adversarial training strategy in [10] is adopted to align \nthe 2D image features and 3D model features. At the test \nstage, the 2D image features and the 3D model features after \nSVAM but before the final fully connected layer are taken \nfor retrieval results.\nIn the following of this section, we will first illustrate the \nunsupervised 2D image-based 3D model retrieval problem. \nAfter this, we introduce our proposed transformer-based \n3D model retrieval network T3DRN. Next, we explain \nour unique shared view-guided attentive module (SVAM). \nFinally, we show our training and optimization details. \n3.1  Problem statement\nThe goal of unsupervised 2D image-based 3D model \nretrieval is to create the cross-domain approach capable of \nprecisely finding similar 3D models given a 2D image. In \nthis task, the data of 2D images contain the images and their \nlabels, denoted as S ={ XS, YS} , while the data of the target \ndomain only contain the 3D models without labels, denoted \nas T ={ X t} . To learn better domain-invariant features with-\nout labels of 3D models, we present transformer-based 3D \nmodel retrieval network (T3DRN), which will be introduced \nin detail in Sect. 3.2.\n3.2  Transformer‑based 3D model retrieval network\nWe build up the transformer-based 3D model retrieval \nnetwork on top of [42]. The T3DRN consists of ViT \nstacks (shown in Fig.  3) and a shared view-guided atten-\ntive model(SVAM), which will be elaborated in Sect.  3.3. \nT3DRN is composed of three ViT-block branches for 2D \nimages, 3D model images and perturbed 3D model images, \nrespectively. Each ViT block comprises of 4 transformer \nlayers; the implementation of each Transformer layer is as \nFig. 2  Our presented T3DRN approach consists of three branches \nusing ViT-blocks that share parameters based on [42], along with \na shared view-guided attentive model (SVAM). During the train-\ning phase, both 2D image and 3D model data are sampled from the \ndataset. The 2D image data with labels is input to the ViT block to \ncarry out a supervised classification task, which is regulated by the \ncross-entropy loss. On the other hand, the 3D image data follows a \ncontrastive learning approach; it is divided into two branches: the \noriginal branch and randomly perturbed branches. These types of fea-\ntures are also processed by ViT blocks before being directed to the \nSVAM, where they interact with the view embeddings. After SVAM, \nthese features are regularized using the relative entropy loss term. \nAdditionally, the adversarial training method described in [10] is \nemployed to align the 2D image features with the 3D model features. \nDuring the testing phase, the retrieval results are obtained using the \n2D image features and the 3D model features, taken after SVAM but \nbefore the final fully connected layer.\nFig. 3  The detailed structure of ViT Block\n3895View-target relation-guided unsupervised 2D image-based 3D model retrieval via transformer  \n1 3\nfollows. For each image either from 2D image gallery or 3D \nmodel gallery, the patch embedding layer first transforms \nit into a token sequence including a special class token and \nimage tokens [42] to get visual features. Then, they are \nadded by positional encoding in [ 40]. We adopt the posi -\ntional encoding (PE) procedure with sin and cos functions.\nIt should be noted that PE operation only occurs at the \nbottom of each ViT block. The dimension of PE is the \nsame as the input, so PE embedding can be added directly \nto the input. After the visual features are added with PE, \nthe output is denoted as F , which is input into three linear \nprojectors to attain three different vectors  Q, K, V. These \nthree vectors are fed into the ViT block, the lth layer in a \nViT block is given by:\nwhere /u1D711 is layer normalization on residual output, PF rep -\nresents the feed-forward layer which consists of two linear \nlayers with a nonlinear activation function in between. /u1D714 is \nthe output of assembled multi-head attention with a layer \nnormalization by /u1D711 . M l\n1 and M l\n2 are the weights trained for \nthe feed-forward layers, and b l\n1 and b l\n2 are bias vectors. F l is \nthe input of the lth encoding layer. f l\nt  is given as the query to \nthe encoding layer and l is the lth encoding layer. Note that \nF0 is the aforementioned visual feature F added by positional \nencodings. MA is a fine-grained component called multi-\nhead attention, which is composed of H parallel partial dot-\nproduct attention components. Its realization is as follows:\nwhere {hj/uni007C.varj∈[ 1, H ]} refer to the index of each independent \nhead. W q\nj , W K\nj  , W V\nj  denote the linear projectors to the input \nq, K, V for h j . WO is the weight matrix for each head. It is \nnoted that when the query comes from the decoder layer, and \nboth the keys and values are from the encoder layer, it rep-\nresents cross-module attention. In contrast, if the queries, \nkeys, and values are all from encoder or decoder, this kind \nof multi-head attention is named self-attention. A  is the \n(1)\nV �Fl� =/u1D711(PF(/u1D714(Fl)),/u1D714(Fl)),\n/u1D714(Fl)=\n⎛\n⎜\n⎜⎝\n/u1D711(MA(fl\n1,Fl,Fl),fl\n1)\n…\n/u1D711(MA(fl\nT ,Fl,Fl),fl\nT )\n⎞\n⎟\n⎟⎠\n,\n/u1D711(/u1D6FC,/u1D6FD)= LayerNorm(/u1D6FC+ /u1D6FD),\nPF (/u1D6FE)= M l\n2 max (0,M l\n1/u1D6FE+ bl\n1)+ bl\n2,\n(2)\nMA (qi,K,V)=concat /parenleft.s1h1 ,h2 ,… ,hH\n/parenright.s1W O ,\nhj = A\n/parenleft.s2\nW q\njqi,W K\nj K,W V\nj V\n/parenright.s2\n,\nscaled dot-product attention operation realized by the equa-\ntion below.\nwhere qi ∈ R d is a query in all T  queries that com-\nposes q i , a group of keys kt ∈ R d and values vt ∈ R d , where \nt = 1, 2,…,T  , the output of dot-product attention is the \nweighted sum of the vt values. The weights are determined \nby the dot-products of query q i and keys kt . Specifically, kt \nand vt are placed into respective matrices K =( k1 ,… ,kT ) \nand V =( v1 ,… ,vT ) . d is the dimension of q i and \n√\nd is to \nnormalize the dot-product value.\nIn the end, with the output of l  encoding layers, the \nencoded visual features, F l , is the final output for the ViT \nblocks. It is noted that we also adopted the random perturba-\ntion strategy and the safe training mechanism in [42]. It also \nconsists of Fl\n2D  , Fl\n3D  and F l\n3Dpert , the latter two parts are as \nthe input to the SVAM.\n3.3  Shared view‑guided attentive model\nTo guide the training process appropriately with view infor-\nmation, we first learn an adaptive view dictionary \nE ={ e1 ,e2 ,… ,eM } , where M is the total views in the 3D \nmodel dataset. With Fl\n3D  and F l\n3Dpert , and E, we design the \nSVAM as follows:\nwhere F is the input features, in this scenario, it can be either \nFl\n3D  or F l\n3Dpert . E/uni2032.var is the corresponding embeddings of the \nview labels for each feature in F. MA, /u1D711 and PF are the same \nwith Eq.  1. In this way, we can compute the view-guided \nfeatures SVAM (Fl\n3D ,E\n�\n) and SVAM (Fl\n3Dpert,E\n�\n) , denoted as \nFv\n3D and F v\n3Dpert . They will be added with Fl\n3D  and F l\n3Dpert as \nfollow for the downstream task:\nwhere /u1D706 is a balance coefficient between the original features \nand the view-guided features.\n(3)A(qi,K ,V )=V\nexp\n�\nK T qi∕\n√\nd\n�\n∑T\nt=1 exp\n�\nkT\nt qi∕\n√\nd\n�,\n(4)\nSVAM �F,E�� =/u1D711(PF(/u1D703(F,E�)),/u1D703(F,E�)),\n/u1D703(F,E\n�\n)=\n⎛\n⎜\n⎜⎝\n/u1D711�MA(e\n�\n1 ,F,F),f1\n�\n…\n/u1D711�MA(e\n�\nT,F,F),fT\n�\n⎞\n⎟\n⎟⎠\n,\n(5)\nF 3D = F l\n3D + /u1D706Fv\n3D ,\nF 3Dpert = F l\n3Dpert + /u1D706Fv\n3Dpert ,\n3896 J. Chang et al.\n1 3\n3.4  Training and optimization details\nIn this section, we show our training and optimization \ndetails. In order to enforce the 2D image data to be correctly \nclassified, and the distributions of the representation of the \n3D models to be similar with its perturbed counterparts, \nmeanwhile confusing the data between two domains, multi-\nple loss function items are leveraged during the Stochastic \nGradient Descent [51] (SGD) at each training step in a train-\ning batch as follows:\nwhere L cls is the classification binary cross-entropy loss \nfunction of the classifier for 2D image data, L tgt is the KL \ndivergence loss in [42], Ld is the domain adversarial loss \nin [10].\n4  Experimental results and discussion\nTo prove the reliability of our method, we carry out experi-\nments on the most popular dataset, MI3DOR-1 [31]. In this \nsection, we first introduce the dataset and evaluation metrics \nfollowed by the implementation details, and then we pro -\nvide the quantitative results of our method. Subsequently, we \nshow the ablation studies. Finally, we visualize the retrieval \nresults and conduct qualitative analysis.\n4.1  Datasets and evaluation metrics\n4.1.1  Dataset\nWe conduct variance experiments on the most popular \nMI3DOR-1 dataset for 3D model retrieval. The source \n(6)L = L cls + /u1D6FDL tgt − L d ,\ndomain is a 2D image, and the target domain is 12 views of \neach 3D model. The MI3DOR-1 dataset consists of 21,000 \n2D real images and 7690 3D virtual models belonging to 21 \ncategories. There are 10,500 2D images and 3845 3D models \nfor training, while 10,500 2D images and 3845 3D models \nfor testing. The examples of MI3DOR-1 are shown in Fig. 4.\n4.1.2  Evaluation metrics\nFollowing the same evaluation criteria of the state-of-the-art \nmethods, seven popular evaluation metrics are selected to \nverify the effectiveness of our experiment, including near -\nest neighbor (NN), first tier (FT), second tier(ST), F -meas-\nure, discounted gain (DCG), average normalized modified \nretrieval rank (ANMRR), and area under ROC curve (AUC). \nThe retrieval accuracy of the results returned by nearest \nneighbors, or the model’s accuracy that is most comparable \nto the retrieval target, can be assessed by NN. The return \nvalues of the first K  and 2K items, respectively, are used \nto define FT and ST, where K is the number of the relevant \nretrieved objects. The recall rate of retrieval results can be \nevaluated using these two metrics. The accuracy and the \nreturn rate of the data from the previous K items are evalu-\nated jointly using the F measure. DCG is a statistical method \nthat assigns relevant results to the top position with a higher \nweight without taking the lower results into account. The \nsystem’s overall quality can be reflected by AUC value. As \na rank-based measure, ANMRR evaluates the ordering infor-\nmation of related objects in the retrieved object. The higher \nvalue indicates better performance in terms of all evaluation \ncriteria except ANMRR.\nairplane bedb icycle bookshelf camera\ncar chairg uitark eyboardk nife\nmonitor motorcycle pistol plant radio\nrifle stairs tent vase wardrobe\n(a) Examples of 2D images in MI3DOR-1\ndataset.\nairplane bedb icycle bookshelf camera\ncar chairg uitar keyboard knife\nmonitor motorcycle pistol plant radio\nrifle stairs tent vase wardrobe\n(b) Exampleso f3 Dm odelsi nM I3DOR-1\ndataset.\nFig. 4  Examples of MI3DOR-1 dataset\n3897View-target relation-guided unsupervised 2D image-based 3D model retrieval via transformer  \n1 3\n4.2  Implementation details\nThese experiments are carried out on an NVIDIA GTX \n2080 Ti GPU with a memory of 11 GB. For the proposed \nmethod, /u1D6FD is set to 0.001 as [42], and the learning rate is set \nto 0.001. The image batch size is set to 32, the iteration in \nan epoch is 2,000, and the training epoch is 20. The pertur -\nbation coefficient /u1D6FC is set as 0.2, and /u1D706 in Eq. 5 is set to 0.3. \nThe confidence threshold /u1D716 [42] is 0.5. For the safe training \nparameters, we keep the same with [42].\n4.3  Quantitative results and analysis\n4.3.1  Comparative methods\nWe mainly compare the qualitative outcomes of our pro-\nposed algorithm with a few methodologies to validate the \neffectiveness of domain alignment.\nFirst comes the basic deep learning method AlexNet [52]. \nThis method used a convolution neural network to directly \nextract features from 3D multiple views and 2D images and \nthere was no transfer learning in this algorithm. Stochastic \ngradient descent with a batch size of 128 examples, momen-\ntum of 0.9, and weight decay of 0.0005 are adopted during \nthe pre-training.\nMEDA [11] and JGSA [12] are the traditional transfer \nlearning methods.MEDA jointly trained the domain-invar -\niant classifier in the Grassmann manifold and implemented \ndynamic alignment of cross-domain distributions in the man-\nifold. As to training and testing settings, [12] set the mani-\nfold feature dimension d = 20, 30, 40 for Office+Caltech10, \nUSPS+MNIST, and ImageNet+VOC datasets, respectively. \nThe iteration number was set to T = 10 . The RBF kernel was \nused with the bandwidth set to be the variance of inputs. \nThe regularization parameters were set as p = 10 , /u1D706= 10 , \n/u1D702= 0.1 , and /u1D70C= 1 . As another traditional transfer learning \nmethod, JGSA made a constraint on the coupled projections \nof the source and target domains and projected them into a \ncommon low-dimensional subspace for less geometric and \ndistribution shift. For the training and testing configurations, \n/u1D706= 11 , /u1D707= 1 were fixed in all the experiments, such that \nthe distribution shift, subspace shift, and target variance are \ntreated as equally important.\nWhen it comes to deep transfer learning methods, there \nalso exist several representative methods such as JAN [30], \nDLEA [29], DANN [10], HIFA [53], and MSTN [54]. To \ndecrease the domain discrepancy in an adversarial training \nprocess, [30] proposed a method that makes use of the maxi-\nmum mean difference criterion and multiple domain-specific \nlayers, where parameter settings are /u1D706= 0 within 100 itera-\ntions and then set it to the cross-validated value. This \nallowed the JDD penalty to be less sensitive to noisy signals \nat the early stages of the training process. Considering the \nfeature learning and distribution alignment, DLEA imple-\nmented a domain adversarial loss and a center alignment \nloss. It is noted that For the discriminator D , the identical \narchitecture was utilized. The batch size was set as 128. The \nentire framework was trained with the rate decay strategy \n(the original learning rate was 0.01) in an end-to-end manner \nby SGD with 0.9 momentum. DANN deployed adversarial \nlearning into transfer learning for the first time, while intro-\nducing a method to find transferable features between differ-\nent domains. To speed up the experimental procedure, the \ndomain adaptator was stuck to the three fully connected lay-\ners (x → 1024 → 1024 → 2) , except for MNIST where a \nsimpler (x → 100 → 2) architecture was adopted. HIFA \nsought a method to minimize the maximum mean feature \ndiscrepancy of the distributions of two domains for domain \nalignment. To conduct semantic transfer learning, MSTN \nminimized the Euclidean distance between category-level \ncenters of source and target domains. It set /u1D6FE= /u1D706 , where \n/u1D706= 2\n1+exp(−/u1D6FE⋅p) − 1 to suppress the noisy information brought \nby false labels and p is the training process.\nTable 1  Comparison results \nwith other methods on \nMI3DOR-1\nThe best results are in bold\nMethods NN FT ST F DCG ANMRR AUC \nAlexNet [52] 0.424 0.323 0.469 0.099 0.345 0.667 –\nDANN [10] 0.650 0.505 0.643 0.112 0.542 0.474 –\nJAN [30] 0.446 0.343 0.495 0.085 0.363 0.647 –\nJGSA [12] 0.612 0.443 0.599 0.116 0.473 0.541 –\nMEDA [11] 0.430 0.344 0.501 0.046 0.361 0.646 –\nMSTN [54] 0.789 0.622 0.779 0.154 0.657 0.358 0.557\nDLEA [29] 0.764 0.558 0.716 0.143 0.597 0.421 –\nHIFA [53] 0.778 0.618 0.768 0.151 0.654 0.362 –\nOurs 0.801 0.632 0.787 0.155 0.667 0.348 0.569\n3898 J. Chang et al.\n1 3\n4.3.2  Quantitative results with other methods and analysis\nWe conduct extensive experiments to compare our T3DRN \napproach and other baseline methods as shown in Table  1. \nWe can make an obvious observation of a significant \nimprovement in NN for T3DRN, reaching 0.801. Our pro-\nposed method yields a 0.023 gain of mAP against the HIFA \nin [53]. Also, compared with other baseline methods, i.e., \nDANN [10], the performance of T3DRN increases dramati-\ncally by more than 60%. The results demonstrate the superi-\nority of T3DRN, which stems from the SVAM module and \nthe extra proper view information captured.\n4.4  Ablation studies\n4.4.1  The effectiveness of SVAM\nTo validate the impact of our proposed SVAM module, \nwe also conduct a wide range of ablation studies shown \nin Table  2. We begin with the very basic model in which \nSVAM is removed, denoted as the T3DRN-SVAM method. \nIt is easy to observe a significant metric increase by around \n0.01 for all of these 7 metrics when the SVAM is plugged. \nThis improvement has proved the effectiveness of SVAM \nmodule in terms of integrating the proper view information \nwith image features thus aligning the features of 2D images \nand 3D models better than the method without SVAM.\n4.4.2  The effectiveness of balance coefficient\nTo explore the effectiveness of the different values of \nthe balance coefficient /u1D706 ( /u1D6FC= 0.2 and /u1D716= 0.5 ), we also \nconducted relative experiment shown in Table  3. We can \nsee that the retrieval performance slowly increases from \n/u1D706= 0.1 , and peaks at 0.3. This shows the importance of \nSVAM. However, if /u1D706 is greater than 0.3, the performance \nbegins to decrease.\nTable 2  Experimental results of \nablation studies of T3DRN and \nT3DRN-SVAM\nMethods NN FT ST F DCG ANMRR AUC \nT3DRN-SVAM 0.790 0.629 0.775 0.153 0.658 0.359 0.559\nT3DRN 0.801 0.632 0.787 0.155 0.667 0.348 0.569\nTable 3  Experimental results \nunder different balance \ncoefficient /u1D706 ( /u1D6FC= 0.2 and \n/u1D716= 0.5)\n/u1D706 NN FT ST F DCG ANMRR AUC \n0.1 0.776 0.605 0.742 0.142 0.643 0.369 0.537\n0.2 0.781 0.610 0.759 0.147 0.658 0.359 0.559\n0.3 0.801 0.632 0.787 0.155 0.667 0.348 0.569\n0.4 0.794 0.625 0.776 0.153 0.651 0.360 0.553\n0.5 0.790 0.621 0.761 0.144 0.645 0.367 0.541\nTable 4  Experimental results \nunder different perturbation \ncoefficient /u1D6FC ( /u1D706= 0.3 and \n/u1D716= 0.5)\n/u1D6FC NN FT ST F DCG ANMRR AUC \n0.2 0.801 0.632 0.787 0.155 0.667 0.348 0.569\n0.3 0.786 0.618 0.765 0.149 0.653 0.369 0.548\n0.4 0.770 0.607 0.761 0.137 0.643 0.375 0.543\nFig. 5  Qualitative results of our proposed T3DRN\n3899View-target relation-guided unsupervised 2D image-based 3D model retrieval via transformer  \n1 3\n4.4.3  The effectiveness of the perturbation coefficient ˛\nTo explore the effectiveness of the perturbation coefficient \n/u1D6FC , we also show the experimental results in Table  4. It can \nbe seen that the best performance appears when /u1D6FC is set \nas 0.2. When the /u1D6FC value rises, the performance decreases \nconsiderably. This is because a bigger perturbation value \nis likely to mislead the model learning during training.\n4.5  Qualitative results and analysis\n4.5.1  Qualitative results of T3DRN\nThe qualitative results of T3DRN are shown in Fig.  5. It is \nnoticeable that our proposed T3DRN method can always \nretrieve similar 3D models, especially for the unique classes. \nThis should be owed to the domain-invariant features \nlearned by T3DRN with proper view information obtained \nby SVAM. For some classes that are not unique, for instance, \nthe bookshelf and keyboard, the retrieval error might happen \noccasionally due to the similar appearance with other classes \nin the dataset. (With wardrobe and tent respectively).\n4.5.2  Convergence process of training\nTo better display the convergence process of our proposed \nmodel, we show the loss (objective) function value of our \nproposed model during training in Fig.  6. We can observe \nthat initially, the objective function value highly fluctuates \nbecause the model cannot align the data of two domains. \nAfter 10k iterations, the model starts to become more stable \nand eventually converges at around 1.3 after 40k interactions \n(Fig. 6).\n4.5.3  Comparative qualitative results of T3DRN \nand T3DRN‑SVAM\nThe qualitative results of T3DRN are shown in Fig.  7. It \nis noticeable that our proposed T3DRN method shown in \nFig.  7a always outperforms the T3DRN-SVAM method \nshown in Fig.  7b due to the appropriate view information \nFig. 6  Loss (objective) function value of training\nBed\nCamara\nRadio\nRifle\nStairs\n(a) Top-5Q ualitative results of T3DRN.\nBed\nCamara\nRadio\nRifle\nStairs\n(b)T op-5 Qualitative results of T3DRN-\nSVAM.\nFig. 7  Comparative qualitative results of T3DRN and T3DRN-SVAM (Top-5 retrieval results)\n3900 J. Chang et al.\n1 3\nachieved that facilitates the whole feature learning process. \nT3DRN-SVAM, however, retrieves a wrong 3D model \n(bookshelf), if it can succeed in obtaining the view infor -\nmation of this shelf, this mistake may have been avoided.\n5  Conclusion\nIn this paper, a novel end-to-end transformer-based 3D \nmodel retrieval network (T3DRN) for unsupervised 2D \nimage-based 3D model retrieval was proposed to facilitate \nthe learning of the domain-invariant features. This T3DRN \ncan also capture useful view information to guide the train-\ning process. To this end, we proposed another innovative \nunit, named shared view-guided attentive module (SVAM) \nto integrate the image features with guided view informa-\ntion. We tested this plug-and-play method on the most popu-\nlar standard dataset and the results turn out that our method \noutperformed the state-of-the-art method by a wide margin \nin terms of all seven metrics.\nData availability The dataset of this paper can be found at https://  \ngithub. com/ tianb ao- li/ MI3DOR.\nConflict of interest The authors declare no conflict of interest.\nOpen Access  This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Veerasamy, B., Annadurai, S.: Video compression using hybrid \nhexagon search and teaching–learning-based optimization tech-\nnique for 3D reconstruction. Multimed. Syst. 27, 45–59 (2021)\n 2. Kirya, M., Debattista, K., Chalmers, A.: Using virtual environ-\nments to facilitate refugee integration in third countries. Virtual \nReal. 27(1), 97–107 (2023)\n 3. Liu, X., Pang, Y., Jin, R., Liu, Y., Wang, Z.: Dual-domain recon-\nstruction network with V-Net and K-Net for fast MRI. Magn. \nReson. Med. 88(6), 2694–2708 (2022)\n 4. Liu, Y., Pang, Y., Liu, X., Liu, Y., Nie, J.: DIIK-Net: a full-resolu-\ntion cross-domain deep interaction convolutional neural network \nfor MR image reconstruction. Neurocomputing 517, 213–222 \n(2023)\n 5. Gao, A., Pang, Y., Nie, J., Shao, Z., Cao, J., Guo, Y., Li, X.: \nESGN: efficient stereo geometry network for fast 3D object detec-\ntion. IEEE Trans. Circuits Syst. Video Technol. 2022, 1 (2022)\n 6. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: deep learning on \npoint sets for 3D classification and segmentation. In: 2017 IEEE \nConference on Computer Vision and Pattern Recognition, CVPR \n2017, Honolulu, HI, USA, July 21–26, 2017, pp. 77–85 (2017)\n 7. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, \nJ.: 3D shapenets: a deep representation for volumetric shapes. In: \nProceedings of the IEEE Conference on Computer Vision and \nPattern Recognition, pp. 1912–1920 (2015)\n 8. Furuya, T., Ohbuchi, R.: Deep aggregation of local 3D geometric \nfeatures for 3D model retrieval. In: Wilson, R.C., Hancock, E.R., \nSmith, W.A.P. (eds.) Proceedings of the British Machine Vision \nConference, BMVC (2016)\n 9. Feng, Y., Feng, Y., You, H., Zhao, X., Gao, Y.: Meshnet: mesh \nneural network for 3D shape representation. In: The 33rd AAAI \nConference on Artificial Intelligence, The 31st Innovative Appli-\ncations of Artificial Intelligence Conference, IAAI, The Ninth \nAAAI Symposium on Educational Advances in Artificial Intel-\nligence, EAAI, pp. 8279–8286 (2019)\n 10. Ganin, Y., Lempitsky, V.S.: Unsupervised domain adaptation by \nbackpropagation. In: Bach, F.R., Blei, D.M. (eds.) Proceedings of \nthe 32nd International Conference on Machine Learning, ICML \n2015, Lille, France, 6–11 July 2015. JMLR Workshop and Confer-\nence Proceedings, vol. 37, pp. 1180–1189 (2015)\n 11. Wang, J., Feng, W., Chen, Y., Yu, H., Huang, M., Yu, P.S.: Visual \ndomain adaptation with manifold embedded distribution align-\nment. In: Boll, S., Lee, K.M., Luo, J., Zhu, W., Byun, H., Chen, \nC.W., Lienhart, R., Mei, T. (eds.) 2018 ACM Multimedia Confer-\nence on Multimedia Conference, MM, pp. 402–410 (2018)\n 12. Zhang, J., Li, W., Ogunbona, P.: Joint geometrical and statistical \nalignment for visual domain adaptation. In: 2017 IEEE Confer -\nence on Computer Vision and Pattern Recognition, CVPR 2017, \nHonolulu, HI, USA, July 21–26, 2017, pp. 5150–5158 (2017)\n 13. Yue, X., Zhang, Y., Zhao, S., Sangiovanni-Vincentelli, A.L., \nKeutzer, K., Gong, B.: Domain randomization and pyramid con-\nsistency: simulation-to-real generalization without accessing tar -\nget domain data. CoRR arXiv: abs/ 1909. 00889 (2019)\n 14. Chen, Y., Ouyang, X., Zhu, K., Agam, G.: Semi-supervised \ndomain adaptation for semantic segmentation. CoRR arXiv: abs/ \n2110. 10639 (2021)\n 15. Peng, D., Lei, Y., Hayat, M., Guo, Y., Li, W.: Semantic-aware \ndomain generalized segmentation. CoRR arXiv: abs/ 2204. 00822 \n(2022)\n 16. Liu, A.-A., Shao, Z., Wong, Y., Li, J., Su, Y.-T., Kankanhalli, M.: \nLSTM-based multi-label video event detection. Multimed. Tools \nAppl. 78, 677–695 (2019)\n 17. Chu, F., Cao, J., Shao, Z., Pang, Y.: Illumination-guided trans-\nformer-based network for multispectral pedestrian detection. \nIn: Artificial Intelligence: Second CAAI International Confer -\nence, CICAI 2022, Beijing, China, August 27–28, 2022, Revised \nSelected Papers, Part I, pp. 343–355 (2022). Springer, London\n 18. Li, Y., Yin, K., Liang, J., Tan, Z., Wang, X., Yin, G., Wang, Z.: A \nmultitask joint framework for real-time person search. Multimed. \nSyst. 29(1), 211–222 (2023)\n 19. Wang, J., Pang, Y., Cao, J., Sun, H., Shao, Z., Li, X.: Deep intra-\nimage contrastive learning for weakly supervised one-step person \nsearch. Preprint arXiv: 2302. 04607 (2023)\n 20. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training \nof deep bidirectional transformers for language understanding. \nIn: Proceedings of the 2019 Conference of the North American \nChapter of the Association for Computational Linguistics: Human \nLanguage Technologies, 2019, Minneapolis, MN, USA, June 2–7, \n2019, Volume 1 (Long and Short Papers), pp. 4171–4186 (2019). \nhttps:// doi. org/ 10. 18653/ v1/ n19- 1423\n 21. Maturana, D., Scherer, S.: Voxnet: a 3D convolutional neural net-\nwork for real-time object recognition. Intell. Robots Syst 2015, 1 \n(2015)\n3901View-target relation-guided unsupervised 2D image-based 3D model retrieval via transformer  \n1 3\n 22. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, \nJ.: 3D shapenets: a deep representation for volumetric shapes. In: \nIEEE Conference on Computer Vision and Pattern Recognition, \nCVPR 2015, Boston, MA, USA, June 7–12, 2015, pp. 1912–1920 \n(2015)\n 23. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: deep hierar -\nchical feature learning on point sets in a metric space. Computer \nVision and Pattern Recognition, arXiv (2017)\n 24. Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E.: Multi-view \nconvolutional neural networks for 3D shape recognition. In: Pro-\nceedings of the IEEE International Conference on Computer \nVision, pp. 945–953 (2015)\n 25. Gao, Z., Shao, Y., Guan, W., Liu, M., Cheng, Z., Chen, S.: A novel \npatch convolutional neural network for view-based 3D model \nretrieval. Computer Vision and Pattern Recognition, arXiv (2021)\n 26. Watanabe, S., Takahashi, S., Wang, L.: Aggregating viewpoints \nfor effective view-based 3D model retrieval. In: 2021 25th Inter -\nnational Conference Information Visualisation (IV) (2021)\n 27. Kim, D., Seo, M., Park, J., Choi, D.: Source domain subset sam-\npling for semi-supervised domain adaptation in semantic segmen-\ntation. CoRR arXiv: abs/ 2205. 00312 (2022)\n 28. Li, B., Wang, Y., Zhang, S., Li, D., Keutzer, K., Darrell, T., \nZhao, H.: Learning invariant representations and risks for semi-\nsupervised domain adaptation. In: Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition, pp. \n1104–1113 (2021)\n 29. Zhou, H., Liu, A., Nie, W.: Dual-level embedding alignment net-\nwork for 2D image-based 3D object retrieval. In: Amsaleg, L., \nHuet, B., Larson, M.A., Gravier, G., Hung, H., Ngo, C., Ooi, W.T. \n(eds.) Proceedings of the 27th ACM International Conference on \nMultimedia, MM, pp. 1667–1675 (2019)\n 30. Long, M., Zhu, H., Wang, J., Jordan, M.I.: Deep transfer learning \nwith joint adaptation networks. In: Precup, D., Teh, Y.W. (eds.) \nProceedings of the 34th International Conference on Machine \nLearning, ICML. Proceedings of Machine Learning Research, \nvol. 70, pp. 2208–2217 (2017)\n 31. Li, X., Zhang, W., Ding, Q., Sun, J.-Q.: Multi-layer domain adap-\ntation method for rolling bearing fault diagnosis. Signal Process. \n2019, 1 (2019)\n 32. Wang, Q., Du, P., Liu, X., Yang, J., Wang, G.: Adversarial unsu-\npervised domain adaptation for cross scenario waveform recogni-\ntion. Signal Process. 2020, 1 (2020)\n 33. Saporta, A., Douillard, A., Vu, T., Pérez, P., Cord, M.: Multi-\nhead distillation for continual unsupervised domain adaptation in \nsemantic segmentation. CoRR arXiv: abs/ 2204. 11667 (2022)\n 34. Hoyer, L., Dai, D., Gool, L.V.: HRDA: context-aware high-reso-\nlution domain-adaptive semantic segmentation. CoRR arXiv: abs/ \n2204. 13132 (2022)\n 35. Zhao, K., Hu, J., Shao, H., Hu, J.: Federated multi-source domain \nadversarial adaptation framework for machinery fault diagnosis \nwith data privacy. Reliab. Eng. Syst. Saf. 236, 109246 (2023)\n 36. Zhao, K., Jia, F., Shao, H.: A novel conditional weighting transfer \nWasserstein auto-encoder for rolling bearing fault diagnosis with \nmulti-source domains. Knowl.-Based Syst. 262, 110203 (2023)\n 37. Jin, B., Vai, M.I.: An adaptive ultrasonic backscattered signal \nprocessing technique for instantaneous characteristic frequency \ndetection. Bio-Med. Mater. Eng. 24(6), 2761–2770 (2014)\n 38. Zheng, Q., Zhao, P., Li, Y., Wang, H., Yang, Y.: Spectrum interfer-\nence-based two-level data augmentation method in deep learning \nfor automatic modulation classification. Neural Comput. Appl. \n33(13), 7723–7745 (2021)\n 39. Zheng, Q., Zhao, P., Wang, H., Elhanashi, A., Saponara, S.: Fine-\ngrained modulation classification using multi-scale radio trans-\nformer with dual-channel representation. IEEE Commun. Lett. \n26(6), 1298–1302 (2022)\n 40. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., \nGomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. \nIn: Advances in Neural Information Processing Systems (Neu-\nrIPS), pp. 5998–6008 (2017)\n 41. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, \nX., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., \nGelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16 × 16 \nwords: transformers for image recognition at scale. In: ICLR \n(2021)\n 42. Sun, T., Lu, C., Zhang, T., Ling, H.: Safe self-refinement for trans-\nformer-based domain adaptation. In: Proceedings of the IEEE/\nCVF Conference on Computer Vision and Pattern Recognition, \npp. 7191–7200 (2022)\n 43. Liu, A.-A., Su, Y.-T., Nie, W.-Z., Kankanhalli, M.: Hierarchical \nclustering multi-task learning for joint human action grouping \nand recognition. IEEE Trans. Pattern Anal. Mach. Intell. 39(1), \n102–114 (2016)\n 44. Xu, N., Zhang, H., Liu, A.-A., Nie, W., Su, Y., Nie, J., Zhang, Y.: \nMulti-level policy and reward-based deep reinforcement learning \nframework for image captioning. IEEE Trans. Multimed. 22(5), \n1372–1383 (2019)\n 45. Liu, A.-A., Wang, Y., Xu, N., Nie, W., Nie, J., Zhang, Y.: Adap-\ntively clustering-driven learning for visual relationship detection. \nIEEE Trans. Multimed. 23, 4515–4525 (2020)\n 46. Ji, Z., Hu, Z., Wang, Y., Shao, Z., Pang, Y.: Reinforced pedestrian \nattribute recognition with group optimization reward. Image Vis. \nComput. 128, 104585 (2022)\n 47. Shao, Z., Han, J., Marnerides, D., Debattista, K.: Region-object \nrelation-aware dense captioning via transformer. IEEE Trans. \nNeural Netw. Learn. Syst. 2022, 1 (2022)\n 48. Shao, Z., Han, J., Debattista, K., Pang, Y.: Textual context-aware \ndense captioning with diverse words. IEEE Trans. Multimed. \n2023, 1 (2023)\n 49. Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H.: \nVideo swin transformer. In: Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 3202–3211 \n(2022)\n 50. Cao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., Wang, \nM.: Swin-unet: unet-like pure transformer for medical image \nsegmentation. In: European Conference on Computer Vision, pp. \n205–218. Springer, London (2022)\n 51. Ruder, S.: An overview of gradient descent optimization algo -\nrithms. Preprint arXiv: 1609. 04747 (2016)\n 52. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classifica-\ntion with deep convolutional neural networks. Commun. ACM \n60(6), 84–90 (2017)\n 53. Zhou, H., Nie, W., Li, W., Song, D., Liu, A.-A.: Hierarchical \ninstance feature alignment for 2D image-based 3D shape retrieval. \nIn: Proceedings of the 29th International Conference on Interna-\ntional Joint Conferences on Artificial Intelligence, pp. 839–845 \n(2021)\n 54. Xie, S., Zheng, Z., Chen, L., Chen, C.: Learning semantic repre-\nsentations for unsupervised domain adaptation. In: International \nConference on Machine Learning, pp. 5423–5432. PMLR (2018)\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8384777307510376
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6142742037773132
    },
    {
      "name": "Transformer",
      "score": 0.6021278500556946
    },
    {
      "name": "3d model",
      "score": 0.4455321431159973
    },
    {
      "name": "Modalities",
      "score": 0.4449208676815033
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4290613830089569
    },
    {
      "name": "Image retrieval",
      "score": 0.42097264528274536
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.38408204913139343
    },
    {
      "name": "Machine learning",
      "score": 0.3736579418182373
    },
    {
      "name": "Computer vision",
      "score": 0.3685474991798401
    },
    {
      "name": "Image (mathematics)",
      "score": 0.3179844319820404
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I151727225",
      "name": "Harbin Engineering University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I39555362",
      "name": "University of Warwick",
      "country": "GB"
    }
  ],
  "cited_by": 14
}