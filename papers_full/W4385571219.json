{
    "title": "Large Language Models Are Reasoning Teachers",
    "url": "https://openalex.org/W4385571219",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3122133883",
            "name": "Namgyu Ho",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2262299225",
            "name": "Laura Schmid",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2142069088",
            "name": "Se-Young Yun",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2276364082",
        "https://openalex.org/W4226146865",
        "https://openalex.org/W3138392969",
        "https://openalex.org/W4297801719",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W3096580779",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W4288348717",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4221149883",
        "https://openalex.org/W2251935656",
        "https://openalex.org/W2951574208",
        "https://openalex.org/W2962800603",
        "https://openalex.org/W2948210185",
        "https://openalex.org/W2963736842",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W4306295121",
        "https://openalex.org/W3172943453",
        "https://openalex.org/W4221151371",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W3153451655",
        "https://openalex.org/W3034408878",
        "https://openalex.org/W2089571492",
        "https://openalex.org/W4389523706",
        "https://openalex.org/W4283768109",
        "https://openalex.org/W4310625358",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W3113223504",
        "https://openalex.org/W4226369848",
        "https://openalex.org/W4367628401",
        "https://openalex.org/W2105717194",
        "https://openalex.org/W4385571260",
        "https://openalex.org/W4318719086",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W3182414949",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4281975731",
        "https://openalex.org/W3034368386",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W2898695519",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W2294370754",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4285594979",
        "https://openalex.org/W3171009184",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4287393336",
        "https://openalex.org/W4286892945"
    ],
    "abstract": "Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 14852–14882\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models Are Reasoning Teachers\nNamgyu Ho, Laura Schmid, Se-Young Yun\nKAIST\n{itsnamgyu, laura.schmid, yunseyoung}@kaist.ac.kr\nAbstract\nRecent works have shown that chain-of-thought\n(CoT) prompting can elicit language models to\nsolve complex reasoning tasks, step-by-step.\nHowever, prompt-based CoT methods are de-\npendent on very large models such as GPT-3\n175B which are prohibitive to deploy at scale.\nIn this paper, we use these large models as rea-\nsoning teachers to enable complex reasoning in\nsmaller models and reduce model size require-\nments by several orders of magnitude. We pro-\npose Fine-tune-CoT, a method that generates\nreasoning samples from very large teacher mod-\nels to fine-tune smaller models. We evaluate\nour method on a wide range of public models\nand complex tasks. We find that Fine-tune-\nCoT enables substantial reasoning capability in\nsmall models, far outperforming prompt-based\nbaselines and even the teacher model in many\ntasks. Additionally, we extend our method by\nleveraging the teacher model’s ability to gener-\nate multiple distinct rationales for each original\nsample. Enriching the fine-tuning data with\nsuch diverse reasoning results in a substantial\nperformance boost across datasets, even for\nvery small models. We conduct ablations and\nsample studies to understand the emergence of\nreasoning capabilities of student models.1\n1 Introduction\nLanguage models (LMs) have demonstrated re-\nmarkable performance in a wide range of down-\nstream tasks. Recently, large language models\n(LLMs) have demonstrated in-context generaliza-\ntion capabilities: performing downstream tasks\nsimply by conditioning on few in-context exem-\nplars or plain natural language task descriptions\n(Brown et al., 2020; Sun et al., 2021). Despite\nthese advancements, even the largest LLMs have\nbeen found to struggle with complex tasks which\nrequire multiple reasoning steps (Rae et al., 2021).\n1Our code implementation and data are available at\nhttps://github.com/itsnamgyu/reasoning-teacher.\n??\n  Large\nTeacher?\n???\n  Small\nStudent\nQuestions \nZero-shot-CoT\nReasoning samples\nFine-tuning\nPrompt\nFigure 1: Fine-tune-CoT uses teacher-generated rea-\nsoning to teach students . We prompt a very large\nteacher model, such as GPT-3 175B, to solve complex\nquestions via zero-shot chain-of-thought reasoning. We\nthen use the reasoning samples to fine-tune a much\nsmaller student model. See Figure 2 for details.\nTo solve complex tasks, recent works show that it\nis possible to elicit reasoning abilities by prompting\nLLMs to perform chain-of-thought (CoT) reason-\ning, i.e., generate a series of intermediate reason-\ning steps. This can be achieved by providing CoT\ndemonstrations as exemplars in prompting (Wei\net al., 2022b). More recently, Kojima et al. (2022)\nfound that LLMs can be prompted to perform CoT\nreasoning simply by providing a natural language\ninstruction to think step-by-step.\nA major drawback of prompt-based CoT rea-\nsoning methods, however, is their reliance on ex-\ntremely large models that span hundreds of billions\nof parameters (Wei et al., 2022b; Kojima et al.,\n2022). These models are prohibitive to deploy\nat scale due to overwhelming computational re-\nquirements and inference costs (Wei et al., 2022b).\n14852\nOriginalSampleQuestionA pet store had 56 puppies. In one day they sold 24 of them and put the rest into cages with 4 in each cage. How many cages did they use?Answer.8.\nPrompt (Zero-shot-CoT)Q: A pet store had 56 puppies. In one day they sold 24 of them and put the rest into cages with 4 in each cage. How many cages did they use?A: Let’s think step by step.\nCompletion (Generated)The store started with 56 puppies. 24 of them were sold, so that means that there are now 32 puppies left.  Since there are 4 puppies in each cage, that means that the store now has .8  cages.\nStep 1. Reasoning Generation\nLarge 175B Teacher Model\nStep 2. Curation\nSmall Student Model\nPromptA pet store had 56 puppies. In one day they sold 24 of them and put the rest into cages with 4 in each cage. How many cages did they use? ###CompletionThe store started with 56 puppies. 24 of them were sold, so that means that there are now 32 puppies left.  Since there are 4 puppies in each cage, that means that the store now has 8 cages.--> 8 END\nReasoning Sample(Curated)\nDataset\nStep 3. Fine-tuning{Diverse Reasoning\nFigure 2: Detailed overview of our proposed Fine-tune-CoT method. Step 1: a very large teacher model is prompted\nto solve complex questions (yellow) by generating multi-step reasoning explanations (green). Step 2: completions\nare filtered based on the correctness of the final prediction (red). The question, rationale, and answer are used to\ncompose a reasoning sample comprised of the prompt and a multi-step solution. Step 3: the curated reasoning\nsamples are used to fine-tune a small, lightweight student to exhibit reasoning capabilities. The application of an\nLM-based teacher enables diverse reasoning—generating multiple distinct rationales for each original sample to\nenrich the fine-tuning data. This boosts the performance of student models without any additional human annotation.\nTherefore, we strive to enable complex reasoning\nin small models which are more feasible for large-\nscale deployment.\nIn this light, we propose an approach named\nFine-tune-CoT, which utilizes the reasoning capa-\nbilities of very large LMs to teach small models\nhow to solve complex tasks. We apply existing\nzero-shot CoT prompting (Kojima et al., 2022) to\ngenerate rationales from very large teacher models,\nand use them to fine-tune smaller student models2.\nWe illustrate this in Figure 2. We note that stan-\ndard fine-tuning without rationales has been shown\nto be inadequate for solving reasoning tasks with\nsmall models (Talmor et al., 2018). While there\nhave been attempts to fine-tune small models with\nhand-annotated reasoning steps (Nye et al., 2021;\nCobbe et al., 2021), they often require task-specific\ntraining setups and high-quality rationales which\nare costly to annotate (Wei et al., 2022b). In con-\ntrast, our approach can be readily applied to novel\ndownstream tasks without hand-crafted reasoning\nor task engineering.\nWe also propose a novel extension to our method,\ntermed diverse reasoning, to maximize the teaching\neffects of Fine-tune-CoT. Inspired by the intuition\n2This can be interpreted as a variant of knowledge distilla-\ntion (Hinton et al., 2015).\nthat complex tasks can have multiple solutions with\ndistinct reasoning paths (Evans, 2010), we generate\nmultiple reasoning solutions from teacher models\nusing stochastic sampling to augment the training\ndata for student models 3. We find that this is a\nsimple yet highly effective approach to maximizing\nstudent performance, which has not been explicitly\nrecognized in concurrent works on fine-tuning with\nCoT reasoning (Huang et al., 2022; Li et al., 2022b;\nMagister et al., 2022; Fu et al., 2023).\nWe evaluate our method on 12 tasks using a\nwide range of publicly available models. We find\nthat Fine-tune-CoT can elicit notable reasoning per-\nformance in small models while preserving much\nof the versatility of prompt-based CoT reasoning,\nwhich previously required >100B parameter mod-\nels (Wei et al., 2022b). Diverse reasoning enables\nremarkable gains in performance at the minor cost\nof additional teacher inference at development time,\nby exploiting our unique learning setup. This en-\nables models as small as 0.3B to outperform larger\nstudents, and even the 175B teacher model in some\ntasks. Our ablations show that performance is con-\n3Diverse reasoning is orthogonal to existing data augmen-\ntation techniques (Yoo et al., 2021; Meng et al., 2022) which\naim to augment new question-answer pairs rather than diverse\nreasoning solutions for complex questions.\n14853\nsistently scalable across all axes considered: di-\nverse reasoning, dataset size, teacher performance,\nand student model size. This shows the potential\nof our method to enable reliable performance in\nsmall models that are feasible for use in real-world\napplications. Lastly, we conduct thorough sample\nstudies and analyses which shed light on crucial\ndetails previous overlooked in fine-tuning for CoT\nand provide intuition on the emergence of reason-\ning abilities in small models.\n2 Related Work\nDownstream transfer in language models\nMuch previous work established a “pre-train and\nfine-tune” paradigm for enhancing LLM perfor-\nmance on downstream tasks (Radford et al., 2018;\nDong et al., 2019; Vaswani et al., 2017; Devlin\net al., 2018). However, fine-tuning is not always\neasily applicable (Hendrycks et al., 2020). More\nrecent literature exhibits a paradigm shift towards\n“prompting” the model to predict the desired out-\nput (Liu et al., 2021; Raffel et al., 2020). Large\nLMs can exhibit strong performance in this set-\nting (Brown et al., 2020). For smaller models to be\nable to perform similarly, additional engineering\nis usually required (Gao et al., 2021; Schick and\nSchütze, 2021b; Schick et al., 2020). For more\ncomplex tasks, the idea of using samples with ex-\nplicit reasoning steps for fine-tuning a model (Nye\net al., 2021; Cobbe et al., 2021) preceded the ap-\nproach of chain-of-thought (CoT) prompting (Wei\net al., 2022b), which enables very large LMs to\nperform well.\nChain-of-thought reasoning In few-shot CoT\nprompting, the model learns to generate intermedi-\nate reasoning steps that lead to a problem solution,\nafter being fed examples of step-by-step reason-\ning. This enables very good performance on a wide\nrange of tasks. (Wang et al., 2022). Addition-\nally, LLMs can perform well in an unsupervised\ntask-agnostic setting, using Zero-shot-CoT (Ko-\njima et al., 2022). This requires no fine-tuning or\ntask specific conditioning, and substantially outper-\nforms standard zero-shot learning and sometimes\neven few-shot learning on a wide number of tasks.\nYet, prior work has shown that CoT re-\nquires extremely large models for optimal perfor-\nmance (Hoffmann et al., 2022; Chowdhery et al.,\n2022). In our work, we contrast this by showing\nhow to utilize CoT reasoning methods for smaller\nmodels by fine-tuning them on rationales gener-\nated by a very large model. Using various LLM-\ngenerated explanations for fine-tuning smaller mod-\nels has been successfully used in prior work (Li\net al., 2022a), with a focus on specific single tasks.\nAlso, a similar approach to ours is mentioned\nin (Huang et al., 2022); however we note that this\nconcurrent work focuses on using Few-shot-CoT to\nself-generate fine-tuning examples by and for very\nlarge proprietary models. There is a brief glimpse\ninto fine-tuning on smaller distilled models, but the\nresults are limited to one dataset and very large\nteacher models that are inaccessible to the general\ncommunity. In contrast, we provide a rich set of\nresults and qualitative/quantitative analysis on a\nwide range of datasets, using open-source models\nthat are small and accessible to everyone.\nKnowledge distillation Typically, knowledge\ndistillation (KD) refers to training small models\nderived from large models in order to reduce model\nsize and latency, while still preserving accuracy\nand capacity to generalize (Hinton et al., 2015;\nSanh et al., 2019). Essentially, KD is a form of\nmodel compression, making efficient deployment\nto capacity-limited devices possible (Bucilua et al.,\n2006). We note that our work could also be con-\nsidered a distant variant of KD (Gou et al., 2021),\nsimilar to works on improving prompt-based meth-\nods such as Yoo et al. (2021); Schick and Schütze\n(2021b,a); Zelikman et al. (2022), or works on data-\nfree distillation (Micaelli and Storkey, 2019; Nayak\net al., 2019; Shen et al., 2021), where the transfer\ndata is synthetically generated from a large teacher\nmodel. Similarly, sequence-level distillation, i.e.\ntraining a student model on sequence distributions\nof a larger teacher, can make neural machine trans-\nlation more efficient (Kim and Rush, 2016). De-\nspite being similar in spirit, our method still dis-\ntinguishes itself from such previous work. The\nrole of the teacher model in our method is to teach\nthe notion of intermediate reasoning. It is not the\nspecific output that is the main supervising signal\nfor reasoning, but rather the generation’s structure.\nHence, we do not use a standard KD loss func-\ntion that reflects trying to match the teacher output.\nAdding to this, we note that our diverse reasoning\nis also unusual in the context of KD, where it is e.g.\nsufficient in practice to only generate one teacher\nsequence for sequence level distillation.\n14854\n3 Chain-of-Thought Fine-Tuning\nWe propose Fine-tune-CoT, a task-agnostic ap-\nproach to enable chain-of-thought reasoning in\nsmall LMs. The core idea is to generate reason-\ning samples from very large teacher models using\nCoT prompting and subsequently fine-tune small\nstudent models using the generated samples. This\napproach preserves the versatility of prompt-based\nCoT methods while overcoming their reliance on\nprohibitively large models. To maximize versatility\nand minimize teacher inference costs, we use the\ntask-agnostic Zero-shot-CoT prompting method\n(Kojima et al., 2022) on teacher models, as it does\nnot require any reasoning examples or long infer-\nence context. We discuss our choice of teacher\nCoT prompting method in Section 7.3. In the fol-\nlowing, we characterize Fine-tune-CoT in three\ndistinct steps. We also provide a visual overview\nin Figure 2.\nStep 1. Reasoning generation First, we utilize\na large teacher model to generate CoT reasoning\nexplanations for a given task. Consider a standard\nsample Si consisting of a question qi and its true\nanswer ai. Using Zero-shot-CoT 4. we prompt the\nteacher model to generate a reasoning explanation,\nor rationale, ˆri to solve question qi and make a final\nanswer prediction ˆai. The resulting text sequence,\nincluding the prompt and generations, takes the\nfollowing form: “Q: <qi>. A: Let’s think step by\nstep. <ˆri> Therefore, the answer is <ˆai>”.\nStep 2. Curation Next, we filter the generated\nsamples and reformat them into prompt-completion\npairs. For filtering, we simply compare the final\nprediction of the teacher model ˆai with the ground-\ntruth answer ai, following previous works (Zelik-\nman et al., 2022; Huang et al., 2022). Note that\nthis filtering incurs some loss of training samples.\nFor all instances i where ˆai = ai, we repackage\n(Si, ˆri, ˆai) into a reasoning sample S′\ni = (pi, ci), a\nprompt-completion pair. To maximize inference-\ntime efficiency, we use special-character based de-\nlimiters to minimize token usage. Specifically, pi\nand ci each take the form of “<qi> ###” and “<ˆri>\n--> <ai> END”. We note that answer-based filter-\ning does not ensure the correctness of the ratio-\nnales, especially for multi-choice questions. We\nprovide an analysis in Appendix E.1 regarding this\nimportant detail which has not been addressed in\nconcurrent work.\nStep 3. Fine-tune Finally, we fine-tune a small\npre-trained student model on the assembled reason-\ning samples. We use the same training objective\nof that used during pre-training, i.e., autoregres-\nsive language modeling objective, or next-token\nprediction (Radford et al., 2018).\nDiverse reasoning To maximize the teaching ef-\nfects of Fine-tune-CoT, we can generate multiple\nreasoning explanations for each training sample.\nThis approach is motivated by the intuition that\nmultiple reasoning paths can be used to solve com-\nplex tasks, i.e., type-2 tasks (Evans, 2010). We\nposit that this unique feature of complex tasks, in\ntandem with the stochastic generation abilities of\nthe teacher model, can enable diverse reasoning\nto significantly boost reasoning supervision sim-\nply through additional teacher inference. In de-\ntail, for a given sample Si, instead of applying\nZero-shot-CoT using greedy decoding to obtain a\nsingle explanation-answer pair (ˆei, ˆai), we use a\nstochastic sampling strategy, i.e., temperature sam-\npling with large T, to obtain D distinct generations\n{(ˆrij, ˆaij)}D\nj . Subsequent reasoning sample cura-\ntion and fine-tuning then proceed as before. We\nrefer to D as the degree of reasoning diversity. A\nsimilar approach is used in Wang et al. (2022);\nHuang et al. (2022), where multiple CoT outputs\nare generated and marginalized to find the optimal\nanswer. However, the effects of such diverse rea-\nsoning on teaching student models has not been\nacknowledged or thoroughly investigated in con-\ncurrent work (Huang et al., 2022; Li et al., 2022a;\nMagister et al., 2022; Fu et al., 2023). We note\nthat diverse reasoning imposes an important trade-\noff between the development cost and inference\ncost/quality of student models which we discuss in\nSection 5.3.\n4 Experiments\nTasks and datasets We evaluate our method on\n12 datasets pertaining to four categories of complex\nreasoning, following Kojima et al. (2022). These\ninclude arithmetic (SingleEq, AddSub, MultiArith,\nGSM8K, SV AMP), other (Date Understanding,\nTracking Shuffled Objects), symbolic (Last Let-\nter Concatenation, Coin Flip), and common sense\n(CommonSenseQA, StrategyQA) reasoning. We\nprovide details and references in Appendix B.\n4Note that Zero-shot-CoT is itself a two-step prompting\nmethod. The reasoning (blue) is generated in the first step and\nanswer prediction (red) is generated in the second step.\n14855\nMethod Params Single Add MultiGSM8K Aqua SV AMP Date Shuffled Last Coin Common Strategy\nEq Sub Arith Understanding Objects Letter Flip SenseQA QA\nRandom 0.00 0.00 0.00 0.00 20.00 0.00 17.12 33.33 0.00 50.00 20.00 50.00\nTeacher: InstructGPT (text-davinci-002)\nZero-shot-CoT 175B 82.24 78.99 78.89 40.26 34.25 64.67 73.87 50.22 56.00 92.67 61.75 53.57\nStudent: GPT-3 (ada,babbage,curie)\nZero-shot 6.7B 0.66 0.84 3.33 1.74 16.54 2.67 9.91 32.89 0.00 56.67 20.23 52.98\nZero-shot-CoT 6.7B 1.32 2.52 5.00 2.35 21.26 1.33 15.32 31.11 0.00 46.67 19.98 51.09\nFew-shot-CoT 6.7B 22.3731.9310.00 2.50 15.75 11.33 12.84 - 0.67 40.00 24.73 54.68\nFine-tune 6.7B 24.3425.21 15.00 6.14 15.35 20.67 14.41 33.78 32.67 72.00 76.17 65.21\nFine-tune-CoT 0.3B 7.24 6.72 6.11 3.11 23.62 5.00 17.12 49.33 50.67 99.33 32.68 52.55\n1.3B 11.18 11.76 13.33 4.70 19.69 8.00 38.74 52.44 50.67 100.00 43.08 52.69\n6.7B 20.39 21.01 33.33 6.75 24.02 12.67 60.36 64.44 52.67 98.67 56.76 55.02\nFine-tune-CoT 0.3B 9.21 10.08 23.89 - - 14.33 58.56 61.78 59.33 99.33 - 57.21\nw/ diverse reasoning1.3B 18.42 19.33 27.78 - - 16.33 70.27 72.00 60.67 100.00 - 57.06\n6.7B 24.3431.0953.33 - - 30.33 83.78 73.33 62.00 100.00 - 58.22\nTable 1: Fine-tune-CoT Performance. Accuracy (%) of OpenAI models on 12 tasks under Fine-tune-CoT (with\ndiverse reasoning) and baseline methods. ‘Random’ refers to random-guess performance derived based on the\nnumber of choices in multi-choice tasks. For diverse reasoning, we report results for maximum degreeD considered:\nD = 64for MultiArith and SV AMP;D = 8for other datasets. We omit diverse reasoning for large datasets due to\nresource constraints and Few-shot-CoT for Tracking Shuffled Objects due to absence of prompts.\nModels For teacher models, we use four vari-\nants of GPT-3 175B (Brown et al., 2020), provided\nby the OpenAI API. Unless otherwise stated, we\nuse text-davinci-002 based on InstructGPT\n175B (Ouyang et al., 2022) as the teacher for Fine-\ntune-CoT. For student models, we consider four\npopular model families. For our main experiments,\nwe use GPT-3 {ada, babbage, curie} as they\nare readily available for fine-tuning via the Ope-\nnAI API. Due to the blackbox nature of the API,\nwe also consider various open-source models un-\nder controlled settings. We use GPT-2 {Small,\nMedium, Large} (Radford et al., 2019) and T5-\n{Small, Base, Large} (Raffel et al., 2020) as rep-\nresentative model families for decoder-only and\nencoder-decoder architectures, respectively. We\nalso use the instruction-tuned version of T5, Flan-\nT5-{Small, Base, Large} (Chung et al., 2022), to in-\nvestigate the effects of instruction tuning on student\nmodels, prior to applying Fine-tune-CoT. These stu-\ndent models are 25–2500x smaller than the teacher\nmodel, thus considerably more feasible for real-\nworld deployment. We provide details on models\nand API usage in Appendix C.\nBaseline methods We provide a comparison of\nFine-tune-CoT (ours) with four baseline methods:\nstandard zero-shot prompting, vanilla fine-tuning,\nZero-shot-CoT (Kojima et al., 2022), and Few-shot-\nCoT (Wei et al., 2022b). Given a training sample\n{(qi, ai)}i, we use a simple format “Q: <qi>” for\nMethod Model CoT Sample TeacherReferenceUpdates Output Utilization Usage\nZero-shot ✗ ✗ ✗ ✗ (Radford et al., 2019)Zero-shot-CoT✗ ✓ ✗ ✗ (Kojima et al., 2022)Few-shot-CoT✗ ✓ △ ✗ (Wei et al., 2022b)Fine-tune ✓ ✗ ✓ ✗ (Radford et al., 2018)Fine-tune-CoT✓ ✓ ✓ ✓ Ours\nTable 2: Taxonomy of methods . CoT methods are\nmore interpretable due to reasoning output. While Few-\nshot-CoT can utilize few in-context samples, fine-tuning\ncan utilize any number of training samples via model\nupdates. Fine-tune-CoT benefits from the reasoning\ncapabilities of teacher models.\nzero-shot prompting. For vanilla fine-tuning, we\nformat the prompt and completion as “<qi> ###”\nand “<ai> END”, respectively. We clarify the taxon-\nomy of methods in Table 2. For text generation, we\nuse greedy decoding following Wei et al. (2022b);\nKojima et al. (2022) throughout our experiments,\nexcept for diverse reasoning. For diverse reasoning\non the teacher, we use temperature sampling with\nT = 0.7, following Wang et al. (2022). We provide\nexperimental details in Appendix A.\n4.1 Results\nIn this section, we present the reasoning perfor-\nmance of models using Fine-tune-CoT and diverse\nreasoning. We compare with various baselines and\ndemonstrate the scalability of our method across\nfour axes: degree of diverse reasoning (Figure 3),\ndataset size (Figure 4), performance of the teacher\n(Figure 5), and size of the student model (Figure 6).\n14856\nWe present our findings on GPT-3 models in the\nmain text and defer results on open-source models\nto Appendix G, with a brief summary at the end of\nthis section.\nFine-tune-CoT elicits complex reasoning in\nsmall models Table 1 summarizes the accuracy\nof student models using the proposed Fine-tune-\nCoT, compared to prompt-based CoT baselines as\nwell as standard fine-tuning. While Zero-shot-CoT\nexhibits remarkable performance on the very large\n175B model (Kojima et al., 2022), it fails to en-\nable complex reasoning in all three smaller mod-\nels, showing near-negligible performance across\nall tasks. We also find that small models are unable\nto approach these tasks under standard zero-shot\nprompting. On the other hand, Fine-tune-CoT elic-\nits notable reasoning performance, demonstrating\nsignificant gains over Zero-shot-CoT when using\nsmaller models and outperforming both fine-tuning\nand Few-shot-CoT in more than half of the tasks.\nFor complex arithmetic, Fine-tune-CoT achieves a\nnotable 33% accuracy on MultiArith while Zero-\nshot-CoT only reaches 5%. Few-shot-CoT and\nfine-tuning only achieve 10% and 15%, respec-\ntively. For two commonsense reasoning tasks, our\nmethod outperforms the near-random performance\nof Zero-shot-CoT by 37% and 5%, respectively.\nFurthermore, it surpasses Few-shot-CoT on Com-\nmonSenseQA by 32% and performs similarly on\nStrategyQA. We observe that Fine-tune-CoT perfor-\nmance is most notable for tasks that are not overly\ncomplex, which include other reasoning tasks (Date\nUnderstanding, Shuffled Objects) and symbolic rea-\nsoning (Last Letter, Coin Flip), significantly out-\nperforming other baselines. See Appendix Table 9\nfor performance of all students.\nSmall models can outperform very large teach-\ners in reasoning Table 1 also shows that Fine-\ntune-CoT is highly effective on small models com-\npared to the large 175B teacher model. For the\ntasks Shuffled Objects and Coin Flip, Fine-tune-\nCoT is shown to outperform the teacher model\nusing either 1.3B or 6.7B parameters, i.e., reduc-\ning the number of required parameters by approx.\n25–100x. We also find that Fine-tune-CoT with the\nvery small 0.3B model consistently outperforms\nthe 6.7B model under Zero-shot-CoT, demonstrat-\ning that our method is able to unlock a wider range\nof capabilities compared to the baseline, even when\nmodel size is vastly reduced.\n1 2 4 8 16 32 64\nD\n10\n20\n30\n40\n50Accuracy (%)\nMultiArith\nFew-shot-CoT (6.7B)\nFine-tune (6.7B)\n \nFine-tune-CoT (0.3B)\nFine-tune-CoT (1.3B)\nFine-tune-CoT (6.7B)\n1 2 4 8 16 32 64\nD\n10\n20\n30\nSV AMP\nFigure 3: Diverse reasoning performance. Accuracy\n(%) of GPT-3 student models under Fine-tune-CoT with\nvarying degrees of diverse reasoning D. Baseline per-\nformance of the largest model under vanilla fine-tuning\nand Few-shot-CoT are shown for comparison. Diverse\nreasoning is not applicable to the baselines.\nDiverse reasoning substantially improves Fine-\ntune-CoT performance. To examine the learn-\ning effects of diverse reasoning and compare it with\ntwo baselines given by fine-tuning and Few-shot-\nCoT, we apply Fine-tune-CoT using 1–64 reason-\ning explanations per sample across three model\nscales on MultiArith and SV AMP5. Figure 3 shows\nthat diverse reasoning can significantly improve\nthe performance of student models using Fine-tune-\nCoT. For the 6.7B student model, we find a boost\nof around 26% on MultiArith, and around 17% on\nSV AMP. We also note that using diverse reasoning\nalways leads to outperforming the baseline within\nthe respective model size, and can even boost per-\nformance of our method beyond that of a larger\nmodel that does not use diverse reasoning. This\neven includes the teacher in two cases (Date Under-\nstanding, Last Letter). Moreover, we find that di-\nverse reasoning can boost the performance of Fine-\ntune-CoT to surpass that of both Few-shot-CoT\nand vanilla fine-tuning across all model sizes. We\nposit that due to our focus on complex tasks, the di-\nversity of reasoning paths and linguistic templates\ncan substantially aid in teaching student models to\nreason.\nFine-tune-CoT consistently benefits from more\ndata. We perform an ablation on dataset size to\nstudy the performance scalability of our method\nwith dataset size. We see that the performance of\n5For diverse reasoning, we generate teacher rationales\nstochastically with T = 0.7 instead of greedy decoding, which\naccounts for small differences in absolute performance num-\nbers between Table 1 and diverse reasoning with D = 1.\n14857\n8 32 128 420\n0\n10\n20\n30\n40\n50Accuracy (%)\nMultiArith\nFew-shot-CoT\nFine-tune\nFine-tune-CoT (6.7B)\nFine-tune-CoT (Diverse) (6.7B)\n8 32 128 700\n0\n10\n20\n30\nSV AMP\n8 32 128 258\nDataset Size\n0\n10\n20\n30\n40\n50\n60\n70\n80Accuracy (%)\nDate Understanding\n8 32 128 350\nDataset Size\n0\n10\n20\n30\n40\n50\n60\nLast Letter Concatenation\nFigure 4: Effects of dataset size. Accuracy (%) of the\nGPT-3 6.7B student model by dataset size under vanilla\nfine-tuning vs Fine-tune-CoT (with diverse reasoning).\nBaseline performance under Few-shot-CoT is shown\nfor comparison. Diverse reasoning is not applicable\nto standard fine-tuning. We show diverse reasoning\nperformance with D = 64for MultiArith and SV AMP;\nD = 8for others.\nthe 6.7B model clearly scales with the size of the\ndataset, independent of the task. In comparison,\nvanilla fine-tuning does not always exhibit this be-\nhavior. In fact, for Date Understanding, we find that\nan increase in dataset size harms the performance\nof fine-tuning. Furthermore, Fine-tune-CoT sees\nadditional benefits from diverse reasoning, which\nis not applicable in standard fine-tuning.\nBetter reasoners are better teachers Next, we\ncan ask the question of whether the performance of\nthe teacher is correlated with that of their student\nwhen using Fine-tune-CoT. To test this, we use dif-\nferent versions of GPT-3 as teacher models, keep-\ning the size of the student model constant at 6.7B\nparameters (Figure 5). We find that student per-\nformance indeed scales with teacher performance,\nparticularly in the less complex tasks Date Under-\nstanding and Last Letter. There, the performance of\nthe student matches the performance of the teacher\nvery closely. This also fits with our observations\nin Appendix D, which show that the successes and\nfailures of teachers are correlated with those of\nthe students. We note that this scaling effect is\nin contrast not a given in knowledge distillation,\nwhere more accurate teachers do not always result\nin better students (Menon et al., 2021).\n10\n20\n30\n40\n50\n60\n70\n80Accuracy (%)\nMultiArith\nFew-shot-CoT\nFine-tune\nTeacher (175B)\nFine-tune-CoT\n10\n20\n30\n40\n50\n60\n70\nSV AMP\n       davinci        text-davinci-001text-davinci-002text-davinci-003\n10\n20\n30\n40\n50\n60\n70Accuracy (%)\nDate Understanding\n       davinci        text-davinci-001text-davinci-002text-davinci-003\n0\n10\n20\n30\n40\n50\n60\nLast Letter Concatenation\nFigure 5: Effects of teacher performance on students.\nAccuracy (%) of teacher models (Zero-shot-CoT) and\ntheir corresponding GPT-3 6.7B student models (Fine-\ntune-CoT). Baseline performance under vanilla fine-\ntuning and Few-shot-CoT are shown for comparison.\nTeacher models are not applicable to Few-shot-CoT\nwhich uses few human-annotated examples.\nFine-tune-CoT performance scales with model\nsize for small LMs Finally, we explore the effect\nof scaling up student model size on our method, and\ncompare it with the effects of increasingly larger\nstudent models in Few-shot-CoT as well as vanilla\nfine-tuning. We can observe that the performance\nof Fine-tune-CoT is consistently scalable with stu-\ndent size (Figure 6). In contrast, the two baselines\ndo not always exhibit the same behavior: in Date\nUnderstanding, neither Few-shot-CoT nor vanilla\nfine-tuning results in scalable performance.\nResults on open-source student models Over-\nall, our findings on T5, Flan-T5, and GPT-2 show\nsimilar trends to those observed on GPT-3. Small\nmodels exhibit near-random performance under\nstandard zero-shot or CoT prompting in nearly all\ncases. Notable, we find that encoder-decoder mod-\nels, i.e., T5 and Flan-T5, show noteworthy perfor-\nmance under standard fine-tuning, suggesting that\ncausal masking may be a bottleneck to reasoning\nin decoder-based language models in the absence\nof CoT output. Fine-tune-CoT consistently outper-\nforms prompt-based baselines and is comparable or\nsuperior to vanilla fine-tuning. Diverse reasoning\nimproves performance even further, often exhibit-\ning significant gains. We report our full findings on\nopen-source models in Appendix G.\n14858\n0\n25\n50\n75Accuracy (%)\nMultiArith\nTeacher\nRandom\nStudent (0.3B)\nStudent (1.3B)\nStudent (6.7B)\n0\n20\n40\n60\nSV AMP\nFew-shot\nCoT\nFine-tune Fine-tune\nCoT\n(Diverse)\n0\n25\n50\n75Accuracy (%)\nDate Understanding\nFew-shot\nCoT\nFine-tune Fine-tune\nCoT\n(Diverse)\n0\n20\n40\n60\nLast Letter Concatenation\nFigure 6: Effects of student model scale. Accuracy (%)\nof GPT-3 student models of various sizes under Few-\nshot-CoT, vanilla fine-tuning, and Fine-tune-CoT (with\ndiverse reasoning). The hatched portion indicates the\nperformance boost of Fine-tune-CoT when using diverse\nreasoning with D = 64 on MultiArith and SV AMP;\nD = 8on others.\n4.2 Analysis\nSample study To identify the strengths and weak-\nnesses of our method, we perform a thorough sam-\nple study across all datasets and methods. Across\nall arithmetic tasks, we find that a large portion\nof errors arises from calculations. MultiArith and\nSV AMP also show many semantic errors, but these\nare significantly reduced with diverse reasoning.\nFor difficult tasks such as GSM8K and AQUA, we\nfound that all methods tend to struggle. We found\nthat our method is highly effective in text-based\ntasks, excluding commonsense reasoning, as well\nas tasks that contain common linguistic patterns.\nOn the other hand, we find that students under\nZero-shot-CoT often repeat questions or produce\nincoherent repetitive statements. While Few-shot-\nCoT elicits step-by-step sentences, the student mod-\nels rarely seem to understand the semantics of the\nquestion, and generations often contain logical or\ncommonsense errors. For details on our sample\nstudy, see Appendix D.\nNuances of fine-tuning on CoT reasoning We\nshed light on nuances that have often been over-\nlooked in previous or concurrent work (Wei et al.,\n2022b; Li et al., 2022a; Magister et al., 2022). First,\nwe acknowledge the possibility that correct sam-\nples may contain incorrect reasoning. In fact, we\nfind that 27.6% of correct teacher completions for\nDate Understanding contained reasoning errors.\nHowever, ablations on rationale filtering suggest\nthat these incorrect rationales can aid in student\nsupervision (Appendix E.1). Secondly, we find\nthat common maximum sequence lengths used for\nCoT generations often lead to incomplete answers.\nWe observe that reasoning length differs among\ndatasets, and longer generations typically improve\naccuracy, but may not be beneficial for fine-tuning\n(Appendix E.2). Lastly, we find that many datasets\nare comprised of samples that share common tem-\nplates, potentially compromising the validity of\nour random train-test splits. To address this, we\nevaluate our method on manual template-wise data\nsplits, and confirm that students retain meaningful\nreasoning capabilities (Appendix E.3).\n5 Discussion\n5.1 Accessibility of Fine-tune-CoT\nOwing to the versatility of the teacher generation\nmethod, i.e., Zero-shot-CoT, our method can be\nreadily applied to any complex task without task-\nspecific engineering. Rationales can be readily gen-\nerated using publicly available APIs such as those\nprovided by OpenAI or Anthropic. This makes it\nviable to obtain CoT training data in low-resource\nscenarios, which not only outperforms standard\nfine-tuning, but elicits the student to output inter-\npretable explanations. Fine-tuning and inference\non student models can also be performed on much\nmore accessible hardware, in contrast to very large\nmodels. This can reduce long-term inference costs\nand minimize environmental impact while making\nour method fully accessible to a wide community.\n5.2 Viability of Fine-tune-CoT\nWhile Fine-tune-CoT elicits notable complex rea-\nsoning capabilities in small models, performance\non some difficult datasets would not be consid-\nered viable for real-world use, such as 30.33% on\nSV AMP. However, our findings in Section 4.1 in-\ndicates significant potential for improvement, as\nour method is shown to be uniquely scalable with\n(1) diverse reasoning, (2) dataset size, (3) teacher\nmodel performance, and (4) student model size.\nThe use of diverse reasoning and better teacher\nmodels is especially promising, as these can bene-\nfit from improved teacher LLM performance and\ninference costs in the future. In addition, it is pos-\nsible to incorporate recent CoT methods, which\nlead to significant performance improvements, in\nstudent models, which we discuss in Section 7.3.\n14859\n5.3 Tradeoffs of Fine-tune-CoT\nThe aforementioned opportunities to enhance Fine-\ntune-CoT also pose many important tradeoffs. We\nleave further analysis to future work.\nDegree of diverse reasoning The performance\nbenefits of diverse reasoning come at the cost of\nadditional teacher inference. Therefore, diverse\nreasoning poses a tradeoff between development\ncost vs inference cost/quality. In other words, per-\nformance gains from diverse reasoning may be uti-\nlized to enhance student performance or alleviate\nthe need for larger student models. This must also\nbe taken into account for fair evaluation of similar\ndistillation methods in the future.\nData acquisition Data annotation and diverse\nreasoning can both be used to enlarge fine-tuning\ndata, but each have their associated costs. We note\nthat the cost of diverse reasoning is linear to the\nnumber of generated rationale and the number of\noriginal samples. Despite this, it can still be a cost-\neffective alternative to hand-annotating additional\ndata. A preliminary cost analysis in Appendix F\nshows that the pareto front of data-acquisition-cost\nto performance always incorporates diverse reason-\ning. We expect that the cost benefits of diverse\nreasoning will continue to improve with improve-\nments in teacher model performance and efficiency.\n5.4 Emergence of CoT reasoning\nThe emergence of abilities such as CoT reasoning\nhas become a point of interest in recent works (Wei\net al., 2022b,a; Schaeffer et al., 2023). We note that\nthe efficacy of Fine-tune-CoT on small models does\nnot disprove this emergence, as our method is based\non fine-tuning. However, we believe our results can\nprovide some insight into this phenomena.\nWhy does Fine-tune-CoT work in small mod-\nels? In a seminal work, Wei et al. (2022b) sug-\ngests that CoT reasoning is an emergent ability of\nscale—more specifically, a complicated phenom-\nena involving a variety of emergent abilities, such\nas semantic understanding, symbol mapping, arith-\nmetic ability. However, our sample studies suggest\nthat Fine-tune-CoT elicits these emergent abilities\neven in relatively small models (see Appendix D).\nWe explain this from two perspectives. First, Wei\net al. (2022b) demonstrated the emergence of rea-\nsoning abilties by identifying a reduction in the\nfrequency of reasoning errors with larger model\nscale. Similarly, we find that more potent forms\nof supervision also lead to a gradual reduction in\nreasoning errors. For example, we found a clear\ndistinction between Zero-, Few-shot-CoT and Fine-\ntune-CoT (with diverse reasoning) in the frequency\nand severity of semantic errors, i.e., understanding\ncomplex questions, and calculation errors. This\nsuggests that explicit supervision on reasoning can\nalso lead to the emergence of reasoning abilities.\nSecond, we qualitatively find that students show ca-\npabilities that are reminiscent of the larger teacher\nmodel. We found that students can recognize com-\nmon semantics and reasoning cues of the given\ntask, and is able to imitate the process of splitting\nlarge tasks into subtasks. This suggests that it is\npossible to learn reasoning abilities pertaining to a\nparticular domain. We posit that this is possible in\nsmall models due to the limited domain of reason-\ning, and may not be applicable in reasoning tasks\nthat require large domains of knowledge.\nDistillation of emergent abilities Chain-of-\nthought reasoning has been recognized as a prime\nexample of emergent abilities in very large lan-\nguage models (Wei et al., 2022a). Our findings\nshow that it is possible to distill this ability, un-\nder certain domains, to much smaller models sim-\nply through fine-tuning. The potential for distilla-\ntion implies that future advancements in language\nmodels may lead to emergent abilities that are not\nonly pertinent to those larger models, but could\nalso have a broader impact, cascading benefits to\nsmaller models.\n6 Conclusion\nWe have proposed Fine-tune-CoT, a method that\nuses LLMs as reasoning teachers to transfer the\nbroad reasoning capabilities previously found in\n>100B models to student models as small as 0.3B.\nWe propose diverse reasoning as a novel approach\nto maximize these teaching effects, exploiting the\nunique characteristics of this new learning setup\nto vastly improve performance. Our extensive ex-\nperiments show that Fine-tune-CoT elicits signifi-\ncant reasoning performance in small models, thus\ndemonstrating the distillation of CoT reasoning\nwhich has been considered an emergent ability of\nscale. By leveraging publicly available models\nwith zero-shot prompting, we demonstrate a task-\nagnostic approach to elicit reasoning performance\nin small models, making complex reasoning feasi-\nble for real-world deployment and accessible to the\nbroader community.\n14860\n7 Limitations\n7.1 Towards concise answers\nSample studies show that rationales output from\nstudent models may occasionally be repetitive\nand digressive. This is undesirable in terms of\ninference-time efficiency as well as interpretability.\nAs a minor optimization to inference computation,\nwe construct our fine-tuning sample templates us-\ning special-character based delimiters instead of\nnatural language used in concurrent work (Huang\net al., 2022) to minimize sequence length. Prelimi-\nnary findings showed this had no significant impact\non reasoning performance. More importantly, it is\ndesirable to train student models to generate con-\ncise answers in terms of substance. Appendix E.2\nhints at the possibility for this, showing that fine-\ntuning on shorter reasoning samples causes the\nstudent model to also produce shorter rationales.\n7.2 Exploring a wider array of models\nWe note that the performance of our method is\ncurrently not state-of-the-art. However, it can ben-\nefit from advances in teacher models as well as\nother prompting methods. For example, future\nwork should include a wider array of teachers, such\nas the highly versatile ChatGPT, which typically\ngenerates detailed long responses that may be able\nto impart more knowledge to the student. More\nrecent models such as GPT-4 have demonstrated\nsignificant advances in complex reasoning abili-\nties, which may improve the efficacy of Fine-tune-\nCoT on very difficult datasets, such as GSM8K.\nConversely, our method could prove even more\nadvantageous when applied to recent models with\nimproved efficiency, such as those based on the re-\ncent LLaMA model (Touvron et al., 2023), which\nhas sparked a proliferation of work focused on com-\npact language models. Both of these avenues are\npromising for future work.\n7.3 Better CoT inference methods\nThe use of diverse reasoning and better teacher\nor student models is especially promising, as it is\npossible to leverage future improvements in model\nperformance and decreased inference costs. How-\never, we can also consider other ways to boost\nperformance, such as using different prompting\nmethods. For example, previous work shows that\nFew-shot-CoT (Wei et al., 2022b) can improve ac-\ncuracy over Zero-shot-CoT by a wide margin, e.g.,\ngoing from 78.7% to 93.0% on MultiArith (Kojima\net al., 2022). However, our choice to use Zero-\nshot-CoT to generate reasoning samples from the\nteacher model is motivated by the fact that Few-\nshot-CoT requires a significantly larger inference\ncontext. With the current pricing models based\non token usage, the typical setup of 8-shot CoT\nwould cost approximately 8 times more compared\nto Zero-shot-CoT. Therefore, we see a tradeoff be-\ntween using the inference budget for Few-shot-CoT\nand using it for diverse reasoning with Zero-shot-\nCoT. On the other hand, we also note that recent\nworks introduce various ways to improve CoT rea-\nsoning performance substantially (often to near-\nperfect levels), which can be applied to our student\nmodels. These include refinement over repeated\ninferences (Wang et al., 2022; Li et al., 2022b) and\nself-improvement (Zelikman et al., 2022; Huang\net al., 2022). In particular, self-consistency (Wang\net al., 2022) can be utilizied on unlabeled samples\nto maximize the teaching signal. In contrast, we\naim to achieve CoT reasoning without the infer-\nence time cost incurred by very large LMs. Future\nwork is needed to incorporate these methods into\nFine-tune-CoT while minimizing development and\ninference costs.\n7.4 Connection with knowledge distillation\nWe assume that there is a lot of potential in strength-\nening the connections between knowledge distilla-\ntion and our method. We have already seen in this\nwork that our method shares some characteristics\nwith KD, such as the fact that the knowledge of\nintermediate reasoning imparted by using also in-\ncorrect samples can have positive effects on student\naccuracy, akin to “dark knowledge” (Menon et al.,\n2021) that is transferred by training on teacher out-\nput logits and not one-hot labels. We have seen\nthat this leads to a quantity-quality tradeoff when it\ncomes to the ability of the student model to gener-\nalize: having fewer but perfectly curated reasoning\nsamples is not necessarily as helpful as having a\nlarger amount of reasoning samples that might not\nalways be fully correct. On the other hand, we have\nalso found that more accurate teachers do lead to\nmore accurate students, which is not always the\ncase in KD (Müller et al., 2019). It would therefore\nbe of interest for future work to formalize the con-\nnection of Fine-tune-CoT with classic KD methods,\nand potentially test the use of a different distillation\nloss function that takes the teacher’s actual output\ninto account.\n14861\n8 Ethics Statement\nOur work presents various challenges and opportu-\nnities in terms of bias and toxicity in language mod-\nels. It is widely known that LLMs trained on large\ncorpora have been shown to capture biases found\nin the training data (Brown et al., 2020; Chowdhery\net al., 2022). Since our student models are trained\non reasoning samples generated by these LLMs, it\nis possible that such characteristics of the teacher\nmodel can get passed along to the student. This is\nan important point to consider when selecting the\nteacher model for our method.\nOur training setup, however, does offer a unique\nopportunity to minimize bias and toxicity in stu-\ndent models, by influencing the samples used for\nfine-tuning. One approach would be to augment\nthe curating step of Fine-tune-CoT to filter out bi-\nased or toxic samples. It is possible to automate\nthis via neural network-based verifiers, previously\nused to filter correct output (Cobbe et al., 2021;\nLi et al., 2022b). Alternatively, one may consider\noptimizing the CoT prompts to minimize bias and\ntoxicity in teacher-generated rationales.\nWe note that bad actors can also potentially take\nadvantage of our method to utilize complex reason-\ning for malicious purposes and deploy it at scale,\nusing small models. This highlights the importance\nof safeguarding the potential capabilities of LLMs\nby major providers. To prevent the distillation of\nmalicious reasoning abilities in small (or large) stu-\ndents, future work in identifying usage patterns\ninvolved in these distillation schemes may help\nproviders apply more stringent precautions to these\nuse cases.\nAcknowledgements\nThis work was supported by Institute of Informa-\ntion & communications Technology Planning &\nEvaluation (IITP) grant funded by Korea govern-\nment (MSIT) [No. 2021-0-00907, Development\nof Adaptive and Lightweight Edge-Collaborative\nAnalysis Technology for Enabling Proactively Im-\nmediate Response and Rapid Learning, 90%],\n[No. 2019-0-00075, Artificial Intelligence Grad-\nuate School Program (KAIST), 5%], and the\nStochastic Analysis and Application Research Cen-\nter (SAARC) under the National Research Foun-\ndation of Korea grant (NRF-2019R1A5A1028324,\n5%).\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nCristian Bucilua, Rich Caruana, and Alexandru\nNiculescu-Mizil. 2006. Model compression. In Pro-\nceedings of the 12th ACM SIGKDD International\nConference on Knowledge Discovery and Data Min-\ning, KDD ’06, page 535–541, New York, NY , USA.\nAssociation for Computing Machinery.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. 2021. Training veri-\nfiers to solve math word problems. arXiv preprint\narXiv:2110.14168.\nIshita Dasgupta, Andrew K. Lampinen, Stephanie\nC. Y . Chan, Antonia Creswell, Dharshan Kumaran,\nJames L. McClelland, and Felix Hill. 2022. Lan-\nguage models show human-like content effects on\nreasoning. arXiv preprint, arXiv:2207.07051.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. In Advances in Neural Information Pro-\ncessing Systems, volume 32. Curran Associates, Inc.\nJonathan St BT Evans. 2010. Intuition and reasoning:\nA dual-process perspective. Psychological Inquiry,\n21(4):313–326.\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and\nTushar Khot. 2023. Specializing smaller language\nmodels towards multi-step reasoning. arXiv preprint\narXiv:2301.12726.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\n14862\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics , 9:346–\n361.\nJianping Gou, Baosheng Yu, Stephen J. Maybank, and\nDacheng Tao. 2021. Knowledge distillation: A\nsurvey. International Journal of Computer Vision ,\n129(6):1789–1819.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. arXiv preprint arXiv:2004.06100.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren\nEtzioni, and Nate Kushman. 2014. Learning to solve\narithmetic word problems with verb categorization.\nIn EMNLP, pages 523–533. Citeseer.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve. arXiv\npreprint arXiv:2210.11610.\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. arXiv preprint, arXiv:\n1606.07947.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\nSabharwal, Oren Etzioni, and Siena Dumas Ang.\n2015. Parsing algebraic word problems into equa-\ntions. Transactions of the Association for Computa-\ntional Linguistics, 3:585–597.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo\nGutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy\nGur-Ari, and Vedant Misra. 2022. Solving quantita-\ntive reasoning problems with language models. arXiv\npreprint, arXiv: 2206.14858.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen,\nXinlu Zhang, Zekun Li, Hong Wang, Jing Qian,\nBaolin Peng, Yi Mao, Wenhu Chen, and Xifeng Yan.\n2022a. Explanations from large language models\nmake small reasoners better. arXiv preprint, arXiv:\n2210.06726.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,\nJian-Guang Lou, and Weizhu Chen. 2022b. On the\nadvance of making language models better reasoners.\narXiv preprint arXiv:2206.02336.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. arXiv preprint arXiv:1705.04146.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\nAdamek, Eric Malmi, and Aliaksei Severyn. 2022.\nTeaching small language models to reason. arXiv\npreprint arXiv:2212.08410.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language models:\nTowards zero-shot language understanding. arXiv\npreprint, arXiv: 2202.04538.\nAditya K Menon, Ankit Singh Rawat, Sashank Reddi,\nSeungyeon Kim, and Sanjiv Kumar. 2021. A statis-\ntical perspective on distillation. In Proceedings of\nthe 38th International Conference on Machine Learn-\ning, volume 139 ofProceedings of Machine Learning\nResearch, pages 7632–7642. PMLR.\nPaul Micaelli and Amos Storkey. 2019. Zero-Shot\nKnowledge Transfer via Adversarial Belief Match-\ning, chapter -. Curran Associates Inc., Red Hook, NY ,\nUSA.\nRafael Müller, Simon Kornblith, and Geoffrey Hinton.\n2019. When Does Label Smoothing Help? Curran\nAssociates Inc., Red Hook, NY , USA.\nGaurav Kumar Nayak, Konda Reddy Mopuri, Vaisakh\nShaj, Venkatesh Babu Radhakrishnan, and Anirban\nChakraborty. 2019. Zero-shot knowledge distillation\nin deep networks. In International Conference on\nMachine Learning, pages 4743–4751. PMLR.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\n14863\nDavid Luan, Charles Sutton, and Augustus Odena.\n2021. Show your work: Scratchpads for intermediate\ncomputation with language models. arXiv preprint\narXiv:2112.00114.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. arXiv preprint arXiv:2203.02155.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are nlp models really able to solve\nsimple math word problems? arXiv preprint\narXiv:2103.07191.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training. -.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nYasaman Razeghi, Robert L. Logan, Matt Gardner, and\nSameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot reasoning. arXiv preprint,\narXiv:2202.07206.\nSubhro Roy and Dan Roth. 2016. Solving gen-\neral arithmetic word problems. arXiv preprint\narXiv:1608.01413.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nRylan Schaeffer, Brando Miranda, and Sanmi Koyejo.\n2023. Are emergent abilities of large language mod-\nels a mirage? arXiv preprint arXiv:2304.15004.\nTimo Schick, Helmut Schmid, and Hinrich Schütze.\n2020. Automatically identifying words that can serve\nas labels for few-shot text classification. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 5569–5578, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nChengchao Shen, Xinchao Wang, Youtan Yin, Jie Song,\nSihui Luo, and Mingli Song. 2021. Progressive net-\nwork grafting for few-shot knowledge distillation. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 2541–2549.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nYu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding,\nChao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen,\nYanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu,\nWeibao Gong, Jianzhong Liang, Zhizhou Shang,\nPeng Sun, Wei Liu, Xuan Ouyang, Dianhai Yu, Hao\nTian, Hua Wu, and Haifeng Wang. 2021. Ernie 3.0:\nLarge-scale knowledge enhanced pre-training for lan-\nguage understanding and generation. arXiv preprint\narXiv:2107.02137.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2018. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. arXiv preprint arXiv:1811.00937.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. arXiv preprint arXiv:2203.11171.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\n14864\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research . Survey Certifica-\ntion.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-\nWoo Lee, and Woomyeong Park. 2021. Gpt3mix:\nLeveraging large-scale language models for text aug-\nmentation. arXiv preprint arXiv:2104.08826.\nEric Zelikman, Yuhuai Wu, and Noah D Goodman.\n2022. Star: Bootstrapping reasoning with reason-\ning. arXiv preprint arXiv:2203.14465.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\n14865\nA Experimental Details\nA.1 Generation\nMaximum sequence length For the maximum\nsequence length of teacher-generated rationales,\nˆri, we use Lr = 128, following Kojima et al.\n(2022), unless stated otherwise. For the maximum\nsequence length of the student model predictions,\nwe use Lp = 1024, unless stated otherwise. We\nretroactively applied Lp = 1024as the default, af-\nter discovering that Lp = 128 is insufficient for\nmany tasks, as discussed in Appendix E.2.\nSampling temperature We apply greedy decod-\ning for all generations, except diverse reasoning, to\nobtain deterministic results following (Wei et al.,\n2022b; Kojima et al., 2022). For diverse reasoning,\nwe use temperature sampling with T = 0.7 to ob-\ntain diverse samples, following a similar approach\nfrom Wang et al. (2022).\nA.2 Answer cleansing\nWe follow the method used in Kojima et al. (2022)\nto cleanse answers generated by models to assess\ntheir correctness.\nA.3 Few-shot-CoT exemplars\nFor Few-shot-CoT prompting, we use exemplars\nprovided by Wei et al. (2022b), with some minor\nformatting adaptations for consistency with our\nother experiments. For Last Letter Concatenation\nand Coin Flip, for which Few-shot-CoT prompts\nare not provided, we use 8 training samples used\nin our 8-shot data experiments shown in Figure 4\nand adapt them for Few-shot-CoT using the format\nof Wei et al. (2022b). This was not applicable to\nTracking Shuffled Objects, therefore it was omitted\nfrom Few-shot-CoT experiments.\nA.4 Fine-tuning OpenAI models\nWe use default hyperparameters set by the OpenAI\nAPI for both vanilla fine-tuning and Fine-tune-CoT.\nWhile the specifics of the fine-tuning API is not\npublicly known, some details on hyperparameters\nare documented in the API reference 6. Accord-\ning to the default settings, our models are trained\nfor 4 epochs. The batch size and learning rate de-\ntermined based on the number of examples used\nfor training. The batch size is set to 0.2% of the\nnumber of training examples capped at 256. The\n6https://platform.openai.com/docs/api-reference/fine-\ntunes/create\nlearning rate is set to 0.05, 0.1, or 0.2 times that of\nthe learning rate used to pre-train the base model,\ndepending on the batch size. Training loss is also\napplied to the prompt portion of the training ex-\namples, i.e., the question, with a small weight of\n0.01. Based on API pricing , we posit that OpenAI\nemploys a form of parameter efficient fine-tuning\nsuch as LoRA (Hu et al., 2021) for their fine-tuning\nAPI instead of updating all model parameters.\nA.5 Fine-tuning open source models\nFor vanilla fine-tuning and Fine-tune-CoT on open\nsource models, we strictly control for hyperparam-\neters. Across all experiments, we fine-tune the\nentire model with a fixed learning rate of 3e-4 and\nbatch size of 8. Upon inspection of model perfor-\nmance under various learning rates and batch sizes,\nwe found that optimal parameters varies among\ndatasets, even between those with similar number\nof reasoning samples. We train all models for a\nmaximum of 20 epochs, which we found to be suf-\nficient for test accuracy to plateau. We report the\nbest test accuracy from 20 epochs, but found that\nperformance varies significantly between epochs.\nOverall, we found that performance by epoch is\nstable for larger models, and that instruction-tuned\nFlan-T5 is more stable compared to T5. Similar\nto learning rate and batch size, the optimal num-\nber of epochs also varies between datasets, even\nthose with similar number of reasoning samples.\nBased on the above, we note that our reported\nperformances of fine-tuned open-source models\nmay be significantly under-estimated compared to\nthose with optimal hyperparameters, and recom-\nmend practitioners to optimize hyperparameters\nusing a separate validation set, per each training\nsetting.\nB Datasets\nWe provide a summary of datasets used in our ex-\nperiments, including their original licenses, in Ap-\npendix Table 3. We consider the 12 datasets used\nin Kojima et al. (2022) to measure reasoning per-\nformance. For Last Letter Concatenation and Coin\nFlip, we use the publicly available data provided\nby Kojima et al. (2022).\nTrain-test split Contrary to previous works on\nprompt-based CoT such as Wei et al. (2022b); Ko-\njima et al. (2022), our fine-tuning approach requires\ndistinct sets of samples for training and testing. If\n14866\nDataset Choices Training Samples Test Samples Data Split License References\nSingleEq - 356 152 70:30 None Koncel-Kedziorski et al. (2015)\nAddSub - 276 119 70:30 Unspecified Hosseini et al. (2014)\nMultiArith - 420 180 70:30 Unspecified Roy and Roth (2016)\nGSM8K - 7473 1319 Original MIT Cobbe et al. (2021)\nAQUA-RAT 5 10000 254 Custom Apache-2.0 Ling et al. (2017)\nSV AMP - 700 300 70:30 MIT Patel et al. (2021)\nDate Understanding 5–6 258 111 70:30 Apache-2.0 Srivastava et al. (2022)\nTracking Shuffled Objects 3 525 225 70:30 Apache-2.0 Srivastava et al. (2022)\nLast Letter Concatenation - 350 150 70:30 Unspecified Wei et al. (2022b); Kojima et al. (2022)\nCoin Flip 2 350 150 70:30 Unspecified Wei et al. (2022b); Kojima et al. (2022)\nCommonSenseQA 5 9741 1221 Original Unspecified Talmor et al. (2018)\nStrategyQA 2 1603 687 70:30 Apache2.0 Geva et al. (2021)\nTable 3: Description of datasets used in our study.\nseparate subsets for training and testing (or devel-\nopment) are provided, we use those. Otherwise, we\nperform a samplewise random split with a train-test\nratio of 70:30. For AQUA, due to the disproportion-\nately large size of the original training set, we ran-\ndomly sample 10,000 instances for training in our\nexperiments. Note that due to the highly templated\nnature of many datasets, this naive data split may\nnot be appropriate for evaluating reasoning capa-\nbilities. This is an important nuance of fine-tuning\non CoT reasoning, which we address in Appendix\nE.3.\nModel Family Params Role Variant / Name in API\nGPT-3 175B Teacher davinci\nInstructGPT 175B Teacher text-davinci-001\nInstructGPT 175B Teacher text-davinci-002\nInstructGPT 175B Teacher text-davinci-003\nGPT-3 6.7B Student curie\nGPT-3 1.3B Student babbage\nGPT-3 0.3B Student ada\nT5 60M Student Small\nT5 220M Student Base\nT5 700M Student Large\nFlan-T5 60M Student Small\nFlan-T5 220M Student Base\nFlan-T5 700M Student Large\nGPT-2 124M Student (Small)\nGPT-2 355M Student Medium\nGPT-2 774M Student Large\nTable 4: Description of models used in our study.\nC Models and API Usage\nAppendix Table 4 describes all teacher and student\nmodels used in our study. We use InstructGPT\n(Ouyang et al., 2022) as the default teacher model\nin our experiments, due to its superior zero-shot rea-\nsoning performance, compared to GPT-3 (Brown\net al., 2020) of the same size (Kojima et al., 2022).\nSpecifically, we use text-davinci-002 at\nthe default, as it was the best available model\nat the start of our experiments. We were un-\nable to consider small InstructGPT models for\nfine-tuning, as it is not offered by the OpenAI\nAPI. We attach model size information based on\n(https://blog.eleuther.ai/gpt3-model-sizes/), follow-\ning Kojima et al. (2022).\nOur total expenditure for API usage, including\nall preliminary experiments, was $1,981 USD. The\nmajority of this expenditure occurred after Septem-\nber 1st, 2022, from which the pricing for infer-\nence on davinci models was $0.02/1K tokens,\namong others. Between teacher model inference,\nstudent model {fine-tuning, inference}, the major-\nity of API usage in terms of cost was focused on\nteacher model inference.\nD Sample Study\nTo understand where our method makes mistakes,\nwhere diverse reasoning can improve performance,\nand where our method always performs well, we\nobserve randomly sampled instances and analyze\nthe reasoning performance on them. To do so, we\ncompare its generations for these samples with (1)\nthe output of the large teacher model, (2) a student\nmodel using Zero-shot-CoT (3) a student model\nusing Few-shot-CoT, and (4) a student model us-\ning fine-tuning without chain of thought reasoning.\nOur analysis reflects our overall findings, which\nwe exemplify with representative examples in Ta-\nbles 10–13.\nD.1 Error analysis\nFor our analysis of the most common types of er-\nrors, we take a look at datasets where we find par-\nticularly bad performance of our vanilla method,\nalso in comparison to other students. We also dis-\n14867\ncuss the benefits of using diverse reasoning in D.2.\nWe summarize our observations below.\nDifficult datasets First, we observe that the sets\nGSM8K and AQUA appear to be too difficult for\nany small student model, in particular, given that\nthe teacher model gets below 50% accuracy on\nboth. In fact, even correct answers are usually cor-\nrect only by chance, due to the high complexity of\nthe tasks (Appendix Tables 10a,b). For AQUA in\nparticular, we note that while we occasionally find\nmeaningful reasoning in the 6.7B student model,\nstudents clearly cannot sufficiently learn to solve\nthe tasks. We do note however that of all the stu-\ndent methods, Fine-tune-CoT still gets the best\nperformance in these two datasets. A similar, if\nless salient, issue arises for StrategyQA. Here, the\nteacher also performs only 3% above the random\nguess accuracy of 50%. The smaller student mod-\nels actually manage to improve on this performance\nas long as they do not use Zero-shot-CoT, in par-\nticular vanilla fine-tuning, but the errors arising in\nFine-tune-CoT often look very similar to the ones\nin the large teacher model. We see that all models\nusually merely retrieve information related to the\nquestion, but cannot synthesize an answer from it\n(Appendix Tables 10c,11a).\nArithmetic mistakes Next, we note that small\nmodels overall exhibit weak arithmetic skills. This\nhas already been discussed in previous literature,\nwhere calculation capability has been found to\nscale with model size (Wei et al., 2022a). Es-\npecially in SingleEq (Appendix Table 11b) and\nAddSub (Appendix Table 11c), a majority of er-\nrors in the output of student models using Fine-\ntune-CoT simply arise from wrong calculations,\nless so bad reasoning. This is also a major fac-\ntor in the bad performance our method exhibits on\nSV AMP as well as GSM8K; even correct multi-\nstep reasoning cannot compensate for the fact that\nthe model’s arithmetic tends to be wrong on inter-\nmediate steps (Appendix Tables 11d, 12a). Only\nthe teacher model then does better on these tasks,\ngiven its much larger size, even though it does not\nget perfect accuracy either. However, we note here\nthat very large language models, such as PaLM\n540B, can be trained on arithmetic and scientific\ndata to be able to reason correctly about a wide\nrange of mathematical tasks in a step-by-step fash-\nion (Lewkowycz et al., 2022).\nProblematic benchmarks, impact of common-\nsense reasoning errors Meanwhile, when look-\ning at our method’s performance in Common-\nsenseQA, we note that producing consistent multi-\nstep reasoning is not always the issue. We find\nthat the student model utilizing Fine-tune-CoT can\noften generate logical reasoning paths for many\nof the samples that are marked as false (Appendix\nTable 13b). Rather, the exact answer is often sub-\njective, making it difficult to guess the correct out-\nput from logical reasoning alone (Appendix Ta-\nble 13c). CommonsenseQA thus is not always\nan ideal benchmark when judged on accuracy, but\ngives insight into how well the model can produce\nreasoning. We also note a difference compared to\nFew-shot-CoT in terms of the impact of reasoning\nerrors: the latter only performs around 5% above\nrandom, lacks understanding of the question in\nmany cases, and makes more severe logical and\ncommonsense mistakes compared to our method.\nIn fact, Fine-tune-CoT comes close to the teacher\ndue to the relatively lower impact of errors that\ndo arise (Appendix Table 13d). This suggests that\nFine-tune-CoT enables stronger task-solving capa-\nbilities and avoids making serious commonsense\nmistakes that prevent it from arriving at a reason-\nable conclusion.\nAligned failures Importantly, we note that for\neach dataset, there seems to be a difference between\n“easy” and “hard” instances. When we consider the\naccuracy of the teacher and other student models\n(using fine-tuning, Zero-shot- or Few-shot-CoT)\non tasks where our method fails, we find that it\nis always lower than on tasks where our method\nis successful. That is, successes and failures tend\nto be aligned across the different methods. We\ncan hypothesize that factors such as content bias\nmay play a role here; language models have been\nfound to fail depending on context and content of\nthe task, in a way similar to human reasoners (Das-\ngupta et al., 2022). We can identify samples that\nhint at this issue when we look at questions that\ninclude phrasing that seems contradictory or coun-\nterintuitive to the context that the model expects\n(see Appendix Table 13d, where the number of\nmovies watched is larger than the number of avail-\nable movies). Additionally, previous work shows\nthat GPT-3 exhibits a performance gap between\ninstances including terms that are frequent in the\npretraining corpus, and instances including less\nfrequent terms (Razeghi et al., 2022). This can\n14868\ncontribute to uneven performance on a multitude of\n(especially numerical) tasks across different meth-\nods and model sizes. We can then surmise the\nobserved absolute differences in accuracy to stem\nfrom the various sources of errors for each method.\nFor example, fine-tuning has much less room for\nerror than Fine-tune-CoT, which can additionally\nmake mistakes on intermediate reasonings such\nthat errors compound.\nD.2 Improvements from diverse reasoning\nSemantic issues We find that models seem sensi-\ntive to how a question is formulated. This is notice-\nable in all datasets, in particular in SV AMP and to\na certain degree in MultiArith. Besides arithmetic\nmistakes, we observe that such semantic issues are\none of the main factors for uneven performance of\nvanilla Fine-tune-CoT on these two datasets.\nIn particular, we observe this issue when there\nis redundant information present in the question\n(Appendix Table 12b). Such cases elicit wrong rea-\nsoning, or lead the model to become stuck on the\nquestion, similarly to what usually happens with\nZero-shot-CoT in the student model (i.e. repeat-\ning the question, or coming up with information\nthat only vaguely pertains to the question). Other\ncommon sources of errors are when hidden vari-\nables make up the first part of the task (i.e. those\ntasks that force the model to calculate a previously\nunknown value that is described in the first sen-\ntence (Appendix Table 12c), or when the model\nencounters overloaded words (e.g., “landing” in\nAppendix Table 12d). We also observe samples\nwhere the model gets stuck on an intermediate re-\nsult (Appendix Table 13a). This observation agrees\nwith previous findings that language models have\nrecency bias (Zhao et al., 2021).\nHowever, this source of errors can be compen-\nsated for by using diverse reasoning. When com-\nparing the generations from Few-shot-CoT, vanilla\nFine-tune-CoT and Fine-tune-CoT with diverse\nreasoning on MultiArith, we find that diverse rea-\nsoning enables the model to understand the ques-\ntion better. While calculation errors are still rel-\natively frequent, the generations show clear ad-\nvantages in terms of semantic understanding and\nbeing able to reason logically as a consequence.\nThis is especially clear when compared to Few-\nshot-CoT, which exhibits problems both in under-\nstanding the question and formulating coherent ex-\npressions, especially when three or more terms\nare involved in the calculation, as mentioned in\nKojima et al. (2022). By contrast, Fine-tune-CoT\nwith diverse reasoning makes for a significantly\nsmoother reasoning performance than using Few-\nshot-CoT or even vanilla Fine-tune-CoT. This re-\nsults in vastly improved accuracy on both Multi-\nArith and SV AMP.\nD.3 Strengths\nHaving analyzed the main sources of errors, we\ncan now focus on the datasets that elicit good per-\nformance from our method, regardless of whether\nwe use diverse reasoning.\nText-based datasets As arithmetic errors are one\nof the main reasons for the decrease in perfor-\nmance of small student models, it comes as lit-\ntle surprise that our vanilla method without di-\nverse reasoning performs well on datasets that are\nmainly text-based and do not require actual cal-\nculation skills. This includes Date Understanding\n(60.4%) (Appendix Table 14a), Last Letter Con-\ncatenation (52.67%) (Appendix Table 14b), Coin\nFlip (98.7%) (Appendix Table 14c), and Shuffled\nObjects (64.4%) (Appendix Table 14d). Our meth-\nods performs significantly above random choice\non these sets, and additionally beats the teacher\non Shuffled Objects and Coin Flip. We find that\naccuracy metrics for these sets are mostly faith-\nful: while the elicited reasoning is not always very\ndetailed, and occasionally misses some reasoning\nsteps (Appendix Table 14e) , the model draws cor-\nrect conclusions from mostly correct steps. We also\nnote that similar to MultiArith and SV AMP, perfor-\nmance on these four datasets can be even further\nboosted with diverse reasoning, outperforming the\nteacher model across all four.\nPatterns These datasets also have very clear pat-\nterns in their tasks, which helps Fine-tune-CoT to\nperform well by providing cues on how to solve\na specific task. We note that in contrast, classic\nfine-tuning does not have an advantage in these\ndatasets, and it gets significantly lower accuracy\nthan Fine-tune-CoT on all four. The same is also\ntrue for MultiArith, which we have used as a bench-\nmark in the main text. While arithmetic errors\ncause the absolute accuracy of our method to be\nlower than the teacher, it significantly outperforms\nfine-tuning on MultiArith even without using di-\nverse reasoning. Indeed, we find that also in the\npresence of arithmetic errors, our model reasons\ncorrectly in many cases. We can surmise that the\n14869\npatterned nature of the tasks in MultiArith helps\nthe student model to understand what is asked of\nit, eliciting the correct reasoning. Additionally, we\nnote that the presence of such patterns in success-\nful datasets does not mean that our method overfits\nto existing templates. In our template-split anal-\nysis (Appendix E.3), we in fact show that while\ntasks look similar to one another in certain datasets\nsuch as Date Understanding, the student model’s\nreasoning does not rely on simply matching tem-\nplates or memorizing particular solutions. This\nimplies that our method can generalize to previ-\nously unseen tasks; the patterns in the datasets do\nnot produce overfitting, but can be surmised to act\nas cues for the model’s understanding of its current\ntask. Thus, we observe that the reasoning skills of\na student using Fine-tune-CoT can overcome the\nsmaller model capacity (which proves to be com-\npletely prohibitive, e.g., for Zero-shot-CoT to have\nany success on the various tasks).\nE Nuances of Fine-tune-CoT\nE.1 Rationale filtering\nWe investigate whether answer-based filtering is\nsufficient for selecting good teacher-generated rea-\nsoning samples. It is possible for the teacher\nmodel to answer correctly despite incorrect reason-\ning, especially in multi-choice questions where the\nrandom-guess probability is significant. To investi-\ngate the potential impact of a better filtering scheme\n(as opposed to our baseline answer-based filtering)\nwe manually annotate the correctness of rationales\nfrom the teacher model and evaluate student perfor-\nmance when fine-tuning oncorrectly reasonedsam-\nples. We use the Date Understanding dataset for\nthis ablation, as it is comprised of well-grounded\nmulti-choice questions for which Fine-tune-CoT\nachieves adequate performance. Appendix Table 6\ncompares the Fine-tune-CoT performance of stu-\ndent models on Date Understanding when using\ncorrect samples filtered based on answer predic-\ntions vs golden samples, hand-picked based on\nthe correctness of rationales. For golden samples,\nwe exclude samples that contain incorrect reason-\ning steps or irrelevant steps which are misleading.\nWe find that 28% of correct samples have incor-\nrect rationales–significantly more than the random-\nguess performance of 17.12%, indicating the im-\nportance of filtering. Surprisingly, we however find\nthat answer-based filtering outperforms the more\nstringent human filtering by 5-11%, given the same\ninitial samples. When we match the number of sam-\nples post-filtering (via undersampling), we do find\nthat fine-tuning on golden samples outperforms\nthat on correct samples by 5-8%. These results\nsuggest that there is a tradeoff between the quality\nand quantity of reasoning samples which must be\naddressed when considering sample-filtering meth-\nods. We also note that this must be considered in\ntandem with diverse reasoning, which can drasti-\ncally increase the quantity of reasoning samples.\nE.2 Maximum sequence length\nFollowing the original setting for Zero-shot-CoT\n(Kojima et al., 2022), we limit the max sequence\nlength, or max tokens, allowed for the teacher-\ngenerated rationale and student reasoning predic-\ntions, denoted Lr, Lp, to 128 initially. How-\never, we find that this can be insufficient in many\ndatasets. Allowing for longer inference, we observe\nthat model performance improves significantly on\nAQUA and commonsense reasoning tasks (Ap-\npendix Table 5). Sample inspection shows that ra-\ntionales with over ∼500 tokens are typically repet-\nitive or too digressive. To investigate the effect\nof the max length Lr of the teacher rationale on\nfine-tuning, we compare student performance us-\ning Lr = {128, 512}(Appendix Table 7). The\neffect of Lr on student performance varies across\ndatasets, and increased Lr does not necessarily im-\nprove student performance on tasks that require\nlonger rationales, such as AQUA. Finally, we ex-\namine the length distribution of the generated ratio-\nnales from the teacher model and student trained\non short ( Lr = 128) and long ( Lr = 512) rea-\nsoning samples, respectively (Appendix Figure 7).\nWe find that the distribution is different for each\ndataset. Notably, we find that while the distribu-\ntions from the long students were similar to that of\nthe teacher, the generated rationale from the short\nstudents were typically limited to less than ∼128\ntokens. These findings are in line with the intu-\nition that different tasks require different lengths\nof rationales, and suggest that careful considera-\ntion is needed in determining parameters related to\nsequence length.\nE.3 Templated datasets\nUpon inspection, we found that many datasets con-\ntain groups of samples which share common tem-\nplates. Therefore, naive samplewise data split has\nthe potential to leak the same templates into the\ntrain and test sets, essentially demoting the learn-\n14870\nModel Max Single Add Multi GSM8K Aqua SV AMP Date Shuffled Last Coin Common Strategy\nParams Tokens Eq Sub Arith Understanding Objects Letter Flip SenseQA QA\nTeacher: InstructGPT (text-davinci-002)\n175B\n128 81.18 75.72 76.90 42.42 29.63 64.00 65.89 54.10 57.43 89.71 59.86 53.40\n(84.83) (90.22) (95.24) (69.85) (44.04) (86.57) (98.06) (97.14) (99.71) (97.14) (82.55) (71.55)\n2048 81.18 75.72 76.48 47.73 34.77 66.00 63.28 54.10 57.43 89.71 59.40 53.03\n(84.83) (90.22) (94.29) (99.34) (96.42) (99.00) (97.14) (97.14) (99.71) (97.14) (99.92) (99.69)\nStudent: GPT-3 (ada,babbage,curie)\n0.3B\n128 7.24 6.72 5.56 3.11 16.54 4.33 17.12 48.89 50.67 99.33 30.30 47.16\n(96.05) (99.16) (96.11) (74.75) (45.67) (91.33) (100.00) (100.00) (100.00) (100.00) (86.73) (87.63)\n1024 7.24 6.72 6.11 3.11 23.62 5.00 17.12 49.33 50.67 99.33 32.68 52.55\n(98.68) (99.16) (97.22) (99.77) (100.00) (97.33) (100.00) (100.00) (100.00) (100.00) (100.00) (99.71)\n1.3B\n128 11.18 11.76 13.89 4.02 15.35 7.33 38.74 53.78 50.67 100.00 40.95 47.02\n(92.76) (96.64) (98.89) (75.36) (48.03) (90.33) (100.00) (99.56) (100.00) (100.00) (86.57) (83.99)\n1024 11.18 11.76 13.33 4.70 19.69 8.00 38.74 52.44 50.67 100.00 43.08 52.69\n(98.68) (98.32) (99.44) (99.92) (99.61) (99.00) (100.00) (100.00) (100.00) (100.00) (99.92) (98.98)\n6.7B\n128 21.05 20.17 34.44 7.20 16.93 12.67 60.36 64.00 52.00 98.00 51.27 47.16\n(92.76) (97.48) (99.44) (76.19) (55.91) (93.67) (99.10) (100.00) (100.00) (100.00) (85.26) (84.28)\n1024 20.39 21.01 33.33 6.75 24.02 12.67 60.36 64.44 52.67 98.67 56.76 55.02\n(98.68) (100.00) (100.00) (99.92) (100.00) (99.00) (100.00) (100.00) (100.00) (100.00) (100.00) (99.71)\nRandom 0.00 0.00 0.00 0.00 20.00 0.00 17.12 33.33 0.00 50.00 20.00 50.00\nTable 5: Ablation on maximum sequence length. Accuracy (%) of Zero-shot-CoT on the teacher model and\nFine-tune-CoT on GPT-3 student models, based on maximum sequence length. Values in parentheses refer to the\npercentage of generated rationales that were completed within the allotted maximum sequence length. Percentages\nlower than 90% are marked in bold. Note that the maximum sequence length is applied to the reasoning portion, i.e.,\nstep 1, of Zero-shot-CoT and to the entire output of Fine-tune-CoT.\n0 128 256 384 512\nTokens\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010Probability\nGSM8K\n0 128 256 384 512\nTokens\n0.000\n0.005\n0.010\n0.015\n0.020\nAQUA\n0 128 256 384 512\nTokens\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\nCommonSenseQA\n0 128 256 384 512\nTokens\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\nStrategyQA\nTeacher\nStudent (Short)\nStudent (Long)\nFigure 7: Effects of teacher reasoning length on student reasoning length. Distribution of the length of generated\nreasoning sequences from the 175B teacher model and fine-tuned 6.7B student models on four datasets. Student\n(Short) refers to students that were fine-tuned on reasoning samples with maximum rationale sequence length of\nLr = 128, and Student (Long) refers to students that were fine-tuned on longer reasoning samples with Lr = 512.\nMethod Filter Samples Model Params\n0.3B 1.3B 6.7B\nZero-shot-CoT 0 10.81 14.41 15.32\nFine-tune-CoT Answer 170 17.12 38.74 60.36\nFine-tune-CoT Golden 123 17.12 28.83 54.95\nFine-tune-CoT Answer† 123 17.12 18.92 50.45\nRandom 16.09\nTable 6: Effects of rationale filtering. Accuracy (%)\nof GPT-3 student models under Fine-tune-CoT when us-\ning samples filtered using answer predictions (Answer),\nor filtered by humans based on the correctness of the\nrationale (Golden). Answer† refers to using a randomly\nsampled subset of the correct samples to match the num-\nber of golden samples.\nModel Max GSM8K AQUACommon Strategy\nParams Tokens SenseQA QA\n0.3B 128 3.11 23.62 32.68 52.55\n512 3.41 15.35 32.10 52.98\n1.3B 128 4.70 19.69 43.08 52.69\n512 3.79 18.90 43.65 53.42\n6.7B 128 6.75 24.02 56.76 55.02\n512 7.96 18.90 58.15 54.15\nRandom 1.01 20.44 20.01 50.18\nTable 7: Effects of teacher reasoning length on stu-\ndent performance. Accuracy (%) of GPT-3 students\nmodels under Fine-tune-CoT on four datasets which\nrequire longer rationales, when trained on reasoning\nsamples with maximum rationale sequence lengths of\nLr = 128, 512.\n14871\n1 2 4 8 16 32 64\nD\n420128328\nDataset Size\n28.9\n($282)\n28.3\n($284)\n34.4\n($288)\n46.1\n($297)\n41.7\n($314)\n55.6\n($347)\n53.3\n($414)\n11.1\n($86)\n13.3\n($87)\n23.3\n($88)\n27.8\n($90)\n32.8\n($96)\n28.9\n($106)\n30.6\n($126)\n6.1\n($21)\n12.2\n($22)\n8.3\n($22)\n14.4\n($23)\n18.3\n($24)\n16.7\n($26)\n18.9\n($32)\n0.6\n($5)\n2.2\n($5)\n2.8\n($5)\n7.2\n($6)\n6.1\n($6)\n5.0\n($7)\n7.8\n($8)\nMultiArith\n1 2 4 8 16 32 64\nD\n700128328\n13.3\n($470)\n15.0\n($474)\n18.0\n($481)\n24.0\n($495)\n29.7\n($523)\n29.0\n($579)\n30.3\n($691)\n10.7\n($86)\n9.3\n($87)\n8.7\n($88)\n12.3\n($90)\n15.7\n($96)\n11.7\n($106)\n14.0\n($126)\n7.3\n($21)\n6.0\n($22)\n6.3\n($22)\n7.3\n($23)\n10.7\n($24)\n11.7\n($26)\n9.7\n($32)\n2.7\n($5)\n1.7\n($5)\n6.0\n($5)\n4.7\n($6)\n6.0\n($6)\n5.7\n($7)\n3.3\n($8)\nSV AMP\n10\n20\n30\n40\n50\n5\n10\n15\n20\n25\nAccuracy (%)\nFigure 8: Cost analysis of data acquisition methods . Accuracy (%) of GPT-3 (6.7B) student model under\nFine-tune-CoT with varying degrees of diverse reasoning and dataset sizes. Values in parentheses indicate estimated\ntotal cost of data acquisition, i.e., data annotation and diverse reasoning inference.\nParams Split MultiArith Date Understanding\n0.3B Sample-wise 5.56 17.12\nTemplate-wise 5.35 22.22\n1.3B Sample-wise 13.89 38.74\nTemplate-wise 7.49 35.19\n6.7B Sample-wise 34.44 60.36\nTemplate-wise 21.39 49.07\nTable 8: Sample-wise vs template-wise split. Accuracy\n(%) of GPT-3 student models under Fine-tune-CoT on\ntwo moderately templated datasets when using a sample-\nwise vs template-wise train-test split.\ning problem into simple pattern matching, rather\nthan complex reasoning. This brings into question\nthe validity of naive samplewise data split, as it has\nthe potential to leak the same templates into the\ntrain and test sets. To investigate whether the stu-\ndent models are truly learning to reason rather than\nmatching simple patterns, we manually group sam-\nples by template and evaluate Fine-tune-CoT using\na template-wise data split. We consider MultiArith\nand Date Understanding as they contain a moderate\nnumber of templates. Note that all datasets exclud-\ning GSM8K, CommonsenseQA, and StrategyQA\ncontain templates to varying degrees. Appendix\nTable 8 shows the performance of Fine-tune-CoT\nwhen using sample-wise vs template-wise split, us-\ning the same train-test ratio of 70:30. While student\nperformance is typically lower with a template-\nwise split, it still significantly outperforms random\nguess performance, as well as prompt-based base-\nlines shown in Appendix Table 1. This reaffirms\nthat Fine-tune-CoT is able to elicit complex reason-\ning capabilities in small language models.\nF Data Annotation vs Diverse Reasoning\nIn Appendix Figure 8, we analyze the cost of data\nannotation and diverse reasoning, based on current\nOpenAI API pricing and a low estimate of anno-\ntation cost at 30 annotations per hour at an hourly\nrate of $20, i.e., $0.67 per question-answer sample.\nWhen comparing the cost and student performance\nof models trained with D = 1and D = 64, we\ncan clearly see that using diverse reasoning can\nenhance the cost-effectiveness of data acquisition.\nHowever, as the cost of diverse reasoning corre-\nlates with the size of the dataset, it is important to\nconsider the cost-performance tradeoffs.\n14872\nG Experiments on Open Source Models\nTo validate the generality of our method, we apply\nour method to a wide range of student models be-\nyond variants of GPT-3. While the OpenAI API\nfor GPT-3 inference and fine-tuning is accessible\nand does not require high-end GPUs, the model\nweights and implementation are not publicly avail-\nable and may involve black-box processing. We\ntherefore conduct experiments from Section 4 on\nopen-source models under a standard setting with\nfixed hyperparameters, as explained in Appendix A\nand report our results in the following. Tables and\nfigures include results from Section 4 on GPT-3 for\nreference.\nPrompt-based baselines A comprehensive per-\nformance evaluation of student models across mul-\ntiple tasks is encapsulated in Table 9, comparing\nFine-tune-CoT against baseline methods. Perfor-\nmance of standard zero-shot prompting, predomi-\nnantly insignificant, is omitted when negligible but\ndoes exhibit unexpected spikes on Flan-T5, such\nas 94.22% on Tracking Shuffled Objects on the\nsmallest model. Few-shot-CoT likewise demon-\nstrates inconsequential performance across most\nstudent models, yet the Flan-T5 models reveal sig-\nnificant performance on some tasks such as 7.51%\non GSM8K and 83.87% on CommonSenseQA.\nThis hints at the possibility that instruction tuning\nmay empower models to comprehend and execute\nCoT prompts, unveiling a latent reasoning capacity\nwithin smaller language models.\nFine-tune-CoT vs vanilla fine-tuning Further\nexamining Table 9, we note that vanilla fine-tuning\nachieves notable performance in encoder-decoder\narchitectures, namely T5 and Flan-T5, achieving\nmore than 80% on Date Understanding and 100%\non Coin Flip, significantly outperforming vanilla\nfine-tuning on GPT-2 and GPT-3 student models.\nThis leads us to believe that the causal attention\nmasking present in decoder-only models could im-\npede complex inter-token reasoning. CoT reason-\ning, in this regard, may serve to mitigate this limita-\ntion by repeating key information within the decod-\ning context. Other the other hand, Fine-tune-CoT\neither surpasses or matches the performance of\nvanilla fine-tuning across a variety of tasks. Our\nmethod also displays consistent scalability with\nmodel size, in contrast to the fluctuating perfor-\nmance between model sizes for baseline methods.\nThe incorporation of diverse reasoning enhances\nthis scalability. Particularly, we find that the Flan-\nT5 models benefit more from Fine-tune-CoT com-\npared to T5 models, implying a favorable role of\ninstruction tuning. When enhanced with diverse\nreasoning, Fine-tune-CoT excels over vanilla fine-\ntuning across several complex reasoning tasks, no-\ntably observed in the performance of Flan-T5 on\nTracking Shuffled Objects (44.00%→89.33%) and\nGPT-2 on MultiArith (11.67%→19.44%).\nEffects of diverse reasoning Figure 9 shows the\nperformance of all student models on MultiArith\nand SV AMP under varying degrees of diverse rea-\nsoning. We observe that performance scales with\ndiverse reasoning in all student models, with the\nexception of T5-Small. It is shown that diverse\nreasoning enables Fine-tune-CoT to outperform\nstandard fine-tuning in all cases.\nEffects of student model scale Figure 10 shows\nthe performance of all student model families ac-\ncording to model size. While we observe perfor-\nmance scaling for Fine-tune-CoT on GPT-3 models,\nthis is not apparent in other open-source models.\nWe posit that this may be due to under-tuned hy-\nperparameters, as we used fixed hyperparameters\nfor all open-source models, in contrast to default\nsuggested settings for GPT-3.\n14873\nMethod Params Single Add Multi GSM8K Aqua SV AMPDate Shuffled Last Coin Common StrategyEq Sub Arith Understanding Objects Letter Flip SenseQA QA\nRandom 0.00 0.00 0.00 0.00 20.00 0.00 17.12 33.33 0.00 50.00 20.00 50.00\nTeacher: InstructGPT 175B (text-davinci-002)\nZero-shot-CoT 175B 81.50 76.71 78.79 42.17 29.74 64.20 67.58 53.20 57.71 90.04 60.07 53.45\nStudent: GPT-3 (ada,babbage,curie)\nFew-shot-CoT 0.3B 0.66 0.84 3.33 1.74 15.75 2.00 19.27 - 0.00 44.67 18.43 42.981.3B 3.29 5.88 5.00 1.59 13.78 4.33 16.51 - 0.00 46.00 18.67 46.056.7B 22.37 31.93 10.00 2.50 15.75 11.33 12.84 - 0.67 40.00 24.73 54.68\nFine-tune 0.3B 9.87 8.40 8.89 5.08 24.41 7.67 23.42 32.44 28.67 100.00 51.68 60.411.3B 11.84 17.65 17.78 5.38 21.26 14.33 31.53 30.22 30.00 100.00 70.93 60.706.7B 24.34 25.21 15.00 6.14 15.35 20.67 14.41 33.78 32.67 72.00 76.17 65.21\nFine-tune-CoT 0.3B 7.24 6.72 6.11 3.11 23.62 5.00 17.12 49.33 50.67 99.33 32.68 52.551.3B 11.18 11.76 13.33 4.70 19.69 8.00 38.74 52.44 50.67 100.00 43.08 52.696.7B 20.39 21.01 33.33 6.75 24.02 12.67 60.36 64.44 52.67 98.67 56.76 55.02\nFine-tune-CoT 0.3B 9.21 10.08 23.89 - - 14.33 58.56 61.78 59.33 99.33 - 57.21\nw/ diverse reasoning1.3B 18.42 19.33 27.78 - - 16.33 70.27 72.00 60.67 100.00 - 57.066.7B 24.34 31.09 53.33 - - 30.33 83.78 73.33 62.00 100.00 - 58.22\nStudent: T5-{Small, Base, Large}\nFew-shot-CoT 60M 1.32 3.36 3.33 1.97 24.80 1.33 20.72 - 0.00 44.67 19.25 46.00220M 1.97 2.52 1.11 1.74 23.23 0.33 9.91 - 0.00 55.33 13.35 52.55700M 1.32 1.68 2.78 2.43 19.69 3.00 9.91 - 0.00 55.33 18.92 53.13\nFine-tune 60M 5.92 8.40 13.89 4.02 29.92 11.33 80.18 94.22 24.67 100.00 22.11 58.81220M 5.92 11.76 15.00 5.00 24.80 8.67 78.38 37.78 44.00 100.00 51.60 59.24700M 6.58 9.24 13.89 4.25 26.77 9.67 79.28 33.78 50.67 100.00 20.88 61.72\nFine-tune-CoT 60M 2.63 5.04 5.56 2.58 24.02 9.33 77.48 40.00 29.33 100.00 29.48 54.73220M 4.61 7.56 10.56 3.18 26.77 7.00 80.18 42.67 47.33 98.67 45.37 55.90700M 5.26 10.92 10.56 4.55 29.92 9.00 80.18 46.22 52.00 100.00 54.22 56.33\nFine-tune-CoT 60M 7.24 7.56 15.00 - - 7.67 81.08 59.11 46.67 100.00 - 56.04\nw/ diverse reasoning220M 5.26 10.08 16.11 - - 10.33 82.88 65.33 60.67 100.00 - 59.68700M 7.89 11.76 17.78 - - 11.33 81.98 81.78 63.33 100.00 - 62.15\nStudent: Flan-T5-{Small, Base, Large}\nZero-shot 60M 0.00 0.00 1.67 2.12 23.62 2.00 32.43 33.78 0.00 54.00 39.07 48.47220M 1.32 0.00 5.00 2.50 27.95 2.00 30.63 31.11 0.00 7.33 72.24 53.42700M 1.32 4.20 3.89 2.05 24.41 2.67 9.91 28.89 0.00 54.00 84.03 49.34\nFew-shot-CoT 60M 1.32 0.84 1.67 2.81 20.87 1.67 27.93 - 0.00 44.67 11.79 51.97220M 2.63 0.84 3.89 3.64 24.80 3.67 12.61 - 0.00 44.67 70.27 53.86700M 12.50 10.08 10.00 7.51 23.23 8.33 20.72 - 0.00 44.67 83.87 65.21\nFine-tune 60M 7.24 9.24 16.67 4.93 28.74 10.33 81.08 33.78 39.33 100.00 45.95 58.95220M 5.26 10.08 16.11 5.08 29.53 10.67 83.78 44.00 45.33 100.00 63.55 61.14700M 7.24 12.61 18.89 5.53 24.80 11.00 82.88 33.78 53.33 100.00 66.75 63.90\nFine-tune-CoT 60M 6.58 5.88 8.33 2.96 23.23 5.67 80.18 36.00 35.33 100.00 42.01 54.15220M 4.61 9.24 12.22 4.40 29.13 6.00 83.78 48.89 50.00 100.00 59.05 59.97700M 11.84 10.92 14.44 5.38 28.35 10.67 84.68 55.11 64.00 100.00 66.83 59.83\nFine-tune-CoT 60M 7.24 10.92 17.22 - - 10.67 84.68 62.22 46.00 100.00 - 56.04\nw/ diverse reasoning220M 9.21 10.92 21.11 - - 12.33 84.68 67.11 56.67 100.00 - 60.84700M 10.53 15.13 20.00 - - 13.67 87.39 89.33 65.33 100.00 - 61.72\nGPT-2 {Small, Medium, Large}\nFew-shot-CoT 124M 1.32 0.00 0.00 0.45 17.32 0.33 13.51 - 0.00 44.67 20.15 0.00355M 0.00 0.00 0.56 0.00 3.94 0.00 9.91 - 0.00 55.33 0.00 0.15774M 0.00 0.00 0.00 0.00 0.39 0.00 13.51 - 0.00 55.33 0.16 35.08\nFine-tune 124M 2.63 3.36 11.67 2.88 25.59 7.67 7.21 33.78 0.67 60.00 20.80 54.00355M 0.66 0.84 5.00 0.38 18.90 0.00 23.42 36.89 1.33 57.33 19.82 50.22774M 1.32 5.04 8.33 2.58 24.80 7.67 13.51 32.44 0.67 1.33 20.88 53.57\nFine-tune-CoT 124M 4.61 4.20 10.00 3.03 24.02 5.67 17.12 38.67 4.67 88.00 22.19 53.57355M 3.29 5.88 7.22 2.73 23.62 7.33 28.83 35.56 10.67 80.00 22.03 55.02774M 3.95 5.88 10.56 2.58 22.05 6.33 15.32 39.11 4.00 89.33 25.80 53.13\nFine-tune-CoT 124M 7.24 9.24 19.44 - - 10.67 21.62 57.33 10.67 93.33 - 56.62\nw/ diverse reasoning355M 5.92 9.24 17.22 - - 9.67 20.72 56.00 20.00 95.33 - 55.60774M 8.55 12.61 17.22 - - 8.67 18.02 52.44 7.33 84.67 - 57.06\nTable 9: Fine-tune-CoT performance on all models. Accuracy (%) of all models on 12 tasks under Fine-tune-CoT\n(with diverse reasoning) and baseline methods. ‘Random’ refers to random-guess performance derived based on\nthe number of choices in multi-choice tasks. For diverse reasoning, we report results for maximum degree D\nconsidered: D = 64for MultiArith and SV AMP;D = 8for other datasets. We omit diverse reasoning for large\ndatasets due to resource constraints and Few-shot-CoT for Tracking Shuffled Objects due to absence of prompts.\nZero-shot baseline performance is omitted due to negligible performance, except for Flan-T5 models.\n14874\n1 2 4 8 16 32 64\nD\n10\n20\n30\n40\n50Accuracy (%)\nMultiArith\nFew-shot-CoT (6.7B)\nFine-tune (6.7B)\n \nFine-tune-CoT (0.3B)\nFine-tune-CoT (1.3B)\nFine-tune-CoT (6.7B)\n1 2 4 8 16 32 64\nD\n10\n20\n30\nSV AMP\n(a) GPT-3\n1 2 4 8 16 32 64\nD\n10Accuracy (%)\nMultiArith\nFew-shot-CoT (700M)\nFine-tune (700M)\n \nFine-tune-CoT (60M)\nFine-tune-CoT (220M)\nFine-tune-CoT (700M)\n1 2 4 8 16 32 64\nD\n10\nSV AMP\n(b) T5\n1 2 4 8 16 32 64\nD\n10\n20Accuracy (%)\nMultiArith\nFew-shot-CoT (700M)\nFine-tune (700M)\n \nFine-tune-CoT (60M)\nFine-tune-CoT (220M)\nFine-tune-CoT (700M)\n1 2 4 8 16 32 64\nD\n10\nSV AMP\n(c) Flan-T5\n1 2 4 8 16 32 64\nD\n10Accuracy (%)\nMultiArith\nFew-shot-CoT (774M)\nFine-tune (774M)\n \nFine-tune-CoT (124M)\nFine-tune-CoT (355M)\nFine-tune-CoT (774M)\n1 2 4 8 16 32 64\nD\n10\nSV AMP\n(d) GPT-2\nFigure 9: Diverse reasoning performance on all mod-\nels. Accuracy (%) of all student models under Fine-\ntune-CoT with varying degrees of diverse reasoning D.\nBaseline performance of the largest model under vanilla\nfine-tuning and Few-shot-CoT are shown for compari-\nson. Diverse reasoning is not applicable to the baselines.\n0\n50Accuracy (%)\nMultiArith\nTeacher\nRandom\nStudent (0.3B)\nStudent (1.3B)\nStudent (6.7B)\n0\n50\nSV AMP\nFew-shot\nCoT\nFine-tune Fine-tune\nCoT\n(Diverse)\n0\n50Accuracy (%)\nDate Understanding\nFew-shot\nCoT\nFine-tune Fine-tune\nCoT\n(Diverse)\n0\n50\nLast Letter Concatenation\n(a) GPT-3\n0\n50Accuracy (%)\nMultiArith\nTeacher\nRandom\nStudent (60M)\nStudent (220M)\nStudent (700M)\n0\n50\nSV AMP\nFew-shot\nCoT\nFine-tune Fine-tune\nCoT\n(Diverse)\n0\n50Accuracy (%)\nDate Understanding\nFew-shot\nCoT\nFine-tune Fine-tune\nCoT\n(Diverse)\n0\n50\nLast Letter Concatenation\n(b) T5\n0\n50Accuracy (%)\nMultiArith\nTeacher\nRandom\nStudent (60M)\nStudent (220M)\nStudent (700M)\n0\n50\nSV AMP\nFew-shot\nCoT\nFine-tune Fine-tune\nCoT\n(Diverse)\n0\n50Accuracy (%)\nDate Understanding\nFew-shot\nCoT\nFine-tune Fine-tune\nCoT\n(Diverse)\n0\n50\nLast Letter Concatenation\n(c) Flan-T5\n0\n50Accuracy (%)\nMultiArith\nTeacher\nRandom\nStudent (124M)\nStudent (355M)\nStudent (774M)\n0\n50\nSV AMP\nFew-shot\nCoT\nFine-tune Fine-tune\nCoT\n(Diverse)\n0\n50Accuracy (%)\nDate Understanding\nFew-shot\nCoT\nFine-tune Fine-tune\nCoT\n(Diverse)\n0\n50\nLast Letter Concatenation\n(d) GPT-2\nFigure 10: Effects of student model scale on all mod-\nels. Accuracy (%) of all student models of various\nsizes under Few-shot-CoT, vanilla fine-tuning, and Fine-\ntune-CoT (with diverse reasoning). The hatched portion\nindicates the performance boost of Fine-tune-CoT when\nusing diverse reasoning with D = 64on MultiArith and\nSV AMP;D = 8on others.\n14875\nTable 10: Examples of incorrect output.\n(a) GSM8K\nQuestion\nZaid spends 1/4 of his salary on rent, 1/3 on car fuel and donates half of the remaining amount to\nhis favorite charity. He gives his daughter 200$ to use for her weekly expenses and 700$ to his\nwife to budget for groceries and other household goods. If Zaid earns 6000$ per month, how much\nmoney will he still have after all these expenses and donations?###\nAnswer 350\nCompletion\nZaid spends 1/4 of his salary on rent, so that’s 300$ per month. Zaid spends 1/3 of his salary on\ncar fuel, so that’s 150$ per month. That leaves him with 3000$ per month. He donates half of the\nremaining amount to his favorite charity, so that’s 1500$ per month. That leaves him with 1500$\nper month. He gives his daughter 200$ per month, so that’s 400$ per month. He gives his wife\n700$ per month, so that’s 350$ per month.\nPrediction 350\n(b) AQUA\nQuestion\n20 marbles were pulled out of a bag of only white marbles, painted black, and then put back in.\nThen, another 20 marbles were pulled out, of which 1 was black, after which they were all returned\nto the bag. If the percentage of black marbles pulled out the second time represents their percentage\nin the bag, how many marbles in total Q does the bag currently hold? Answer choices: (A) 40, (B)\n200, (C) 380, (D) 400, (E) 3200.###\nAnswer D\nCompletion\nThere are 20 white marbles, 20 black marbles, and 1 black marble. The first time, 10 white marbles\nand 10 black marbles are pulled out. The second time, 5 white marbles and 5 black marbles are\npulled out. Now, we can calculate the total number of marbles that are in the bag. There are 20\nwhite marbles and 20 black marbles. The first time, 10 white marbles and 10 black marbles are\npulled out. The second time, 5 white marbles and 5 black-->D\nPrediction D\n(c) StrategyQA\nQuestion Is it common to see frost during some college commencements?###\nAnswer Yes\nCompletion\nFirst, what is a college commencement? A college commencement is the ceremony at which a\nstudent graduates from college. Second, what is frost? Frost is a type of weather that occurs\nwhen the temperature is below freezing and the ground is frozen. So, is it common to see frost\nduring some college commencements? No, it is not common to see frost during some college\ncommencements.-->No\nPrediction No\n14876\nTable 11: Examples of incorrect output.\n(a) StrategyQA\nQuestion Is sternum connected to any of the bones mentioned in James Weldon Johnson’s Dem Bones?###\nAnswer Yes\nCompletion\nThe sternum is a bone in the chest. The bones in the chest are the sternum, the clavicle, the two\ncollarbones, and the two breastbones. The breastbones are the two right and left breastbones. So,\nno, the sternum is not connected to any of the bones mentioned in James Weldon Johnson’s Dem\nBones.-->No\nPrediction No\n(b) SingleEq\nQuestion There are 33 walnut trees currently in the park. Park workers will plant 44 more walnut trees today.\nHow many walnut trees will the park have when the workers are finished ?###\nAnswer 77.0\nCompletion\nThere are 33 walnut trees currently in the park. Park workers will plant 44 more walnut trees today.\nThis means that when the workers are finished, there will be a total of 83 walnut trees in the park.\n-->83.0\nPrediction 83.0\n(c) AddSub 25\nQuestion Jason has 43 blue and 16 red marbles . Tom has 24 blue marbles . How many blue marbles do they\nhave in all ?###\nAnswer 67.0\nCompletion\nJason has 43 blue marbles and 16 red marbles. Tom has 24 blue marbles. To find out how many\nblue marbles they have in all, we need to add 43 and 24. 43 + 24 = 77 Therefore, they have 77 blue\nmarbles in all.-->77\nPrediction 77\n(d) GSM8K\nQuestion\nFour dogs sat in a line within the veterinarian’s waiting room. The first dog was the Affenpinscher,\nwho weighed only 10 pounds. Next to him sat a Dachshund, who weighed twice as much as the\nAffenpinscher. Next to the Dachshund sat a Papillon, who weighed one-fourth as much as the\nDachshund. And at the end of the line sat a Mastiff, who weighed 44 times the weight of the\nPapillon. How much did the Mastiff weigh, in pounds? Let’s think step by step###\nAnswer 220\nCompletion\nThe Affenpinscher weighs 10 pounds. The Dachshund weighs twice as much as the Affenpinscher,\nso it weighs 20 pounds. The Papillon weighs one-fourth as much as the Dachshund, so it weighs 2\npounds. The Mastiff weighs 44 times as much as the Papillon, so it weighs 44 pounds.-->44\nPrediction 44\n14877\nTable 12: Examples of incorrect output.\n(a) SV AMP\nQuestion If you had 33 cookies and each bag has 11 cookies How many bags of cookies do you have?###\nAnswer 3.0\nCompletion\nThere are 33 cookies. Each bag has 11 cookies. That means that if we divide 33 by 11, we will\nknow how many bags of cookies there are. 33 divided by 11 equals 2. There are 2 bags of cookies.\n-->2.0\nPrediction 2.0\n(b) CommonsenseQA\nQuestion What is someone who isn’t clever, bright, or competent called? Answer choices: (A) clumsy, (B)\nineffectual, (C) dull, (D) clumsy, (E) stupid.###\nAnswer E\nCompletion\nIf someone isn’t clever, bright, or competent, then they’re not effective. If someone’s not effective,\nthen they’re not useful. If someone’s not useful, then they’re not needed. If someone’s not needed,\nthen they’re not important. Therefore, the answer is (B) ineffectual.-->B\nPrediction B\n(c) CommonsenseQA\nQuestion Billy set aside a block of time for having fun after work. Why might he do this? Answer choices:\n(A) happiness, (B) stress relief, (C) pleasure, (D) ocean, (E) may laugh.###\nAnswer B\nCompletion\nAfter work, Billy might be tired and want to relax. He could set aside a block of time and say, “OK,\nI’m going to spend the next X minutes doing something I enjoy.” This might help him relax and\nfeel happier.-->A\nPrediction A\n(d) CommonsenseQA\nQuestion What island country is ferret popular? Answer choices: (A) own home, (B) north carolina, (C)\ngreat britain, (D) hutch, (E) outdoors.###\nAnswer C\nCompletion\nA ferret is a small mammal, so (D) hutch is probably not the best answer. It’s also not a very\ncommon animal, so (A) own home and (B) north carolina are probably not the best answers. That\nleaves us with (C) great britain and (E) outdoors. Great britain is a larger island than outdoors, so\nthe ferret is probably more popular in that country.-->C\nPrediction C\n(e) SV AMP\nQuestion\nThere are 17 different movies and 11 different books in the ’ crazy silly school ’ series. If you read\n13 of the books and watched 63 of the movies How many more movies than books are there in the ’\ncrazy silly school ’ series?###\nAnswer 6.0\nCompletion\nThere are 17 different movies and 11 different books in the ‘crazy silly school’ series. If you read\n13 of the books, that means you have read 7 books. If you watched 63 of the movies, that means\nyou have watched 63 movies. So there are 17 movies and 11 books, and you have read 7 books\nand watched 63 movies. That means there are 8 more movies than books in the ‘crazy silly school’\nseries.-->8.0\nPrediction 8.0\n14878\nTable 13: Examples of incorrect output.\n(a) SV AMP\nQuestion\nDanny collects bottle caps and wrappers. He found 65 wrappers and 5 bottle caps at the park. Now\nhe has 31 bottle caps and 9 wrappers in his collection. How many more wrappers than bottle caps\ndid danny find at the park?###\nAnswer 60.0\nCompletion\nFirst, Danny finds 65 wrappers and 5 bottle caps at the park. Then, he has 31 bottle caps and 9\nwrappers in his collection. Now, we want to know how many more wrappers than bottle caps Danny\nfound at the park. We can find this out by subtracting the number of bottle caps from the number of\nwrappers: 9 - 5 = 4 So, Danny found 4 more wrappers than bottle caps at the park.-->4.0\nPrediction 4.0\n(b) SV AMP\nQuestion\nNext on his checklist is wax to stick the feathers together and colors to paint them. He needs a total\nof 57 g of colors to paint them. He needs 22 g of wax more. If the feathers require a total of 353 g\nof wax How many grams of wax does he already have?###\nAnswer 331.0\nCompletion\nFirst, he needs 57 grams of colors to paint the feathers. Second, he needs 22 grams of wax more.\nThird, the feathers require a total of 353 grams of wax. So, he already has 353 grams of wax.-->\n353.0\nPrediction 353.0\n(c) CommonsenseQA\nQuestion You’ll find a landing at the top of what? Answer choices: (A) ocean, (B) apartment building, (C)\nstairwell, (D) airport, (E) room.###\nAnswer C\nCompletion\nA landing is a place where something can touch the ground. A landing at the top of an ocean would\nbe a very strange place to find, since there would be no ground to touch. A landing at the top of an\napartment building would be a very strange place to find, since there would be no ground to touch.\nA landing at the top of a stairwell would be a very strange place to find, since there would be no\nground to touch. A landing at the top of an airport would be a very strange place to find, since there\nwould be no ground to\nPrediction A\n(d) SV AMP\nQuestion 18 red peaches, 14 yellow peaches and 17 green peaches are in the basket. How many red and\nyellow peaches are in the basket?###\nAnswer 32.0\nCompletion\nThere are a total of 48 peaches in the basket. There are 18 red peaches, 14 yellow peaches, and 17\ngreen peaches. This means that there are a total of 33 peaches in the basket. There are 33 peaches\nin the basket. This means that there are 17 red peaches and 14 yellow peaches in the basket.-->\n17.0\nPrediction 17.0\n14879\nTable 14: Examples of correct output.\n(a) Date Understanding\nQuestion\nJane was born on the last day of Feburary in 2000. Today is her 16-year-old birthday. What is the\ndate a month ago in MM/DD/YYYY? Which choice is true? Answer choices: (A) 01/30/2016, (B)\n11/08/2015, (C) 01/29/2016, (D) 07/29/2015, (E) 05/04/2016, (F) 02/02/2016.###\nAnswer C\nCompletion Jane’s 16th birthday is on 02/28/2016. A month ago from that would be 01/28/2016. Therefore, the\nanswer is (C).-->C\nPrediction C\n(b) Last Letter Concatenation\nQuestion Take the last letters of each words in \"Lino Mariel Aditya Elisabeth\" and concatenate them.###\nAnswer olah\nCompletion The last letter of \"Lino\" is \"o\". The last letter of \"Mariel\" is \"l\". The last letter of \"Aditya\" is \"a\".\nThe last letter of \"Elisabeth\" is \"h\". So the final answer is \"olah\". –> olah\nPrediction olah\n(c) Coin Flip\nQuestion A coin is heads up. Cinthia does not flip the coin. Lloyd does not flip the coin. Jacqueline flips the\ncoin. Jc flips the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\".###\nAnswer Yes\nCompletion\nCinthia does not flip the coin, so the coin remains heads up. Lloyd does not flip the coin, so the\ncoin remains heads up. Jacqueline flips the coin, so the coin is now tails up. Jc flips the coin, so the\ncoin is now heads up again. –> yes\nPrediction Yes\n(d) Tracking Shuffled Objects\nQuestion\nAlice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball:\nAlice has a blue ball, Bob has a yellow ball, and Claire has a green ball. As the game progresses,\npairs of players trade balls. First, Bob and Claire swap balls. Then, Bob and Alice swap balls.\nFinally, Bob and Claire swap balls. At the end of the game, Bob has the Which choice is true?\nAnswer choices: (A) blue ball, (B) yellow ball, (C) green ball.###\nAnswer B\nCompletion\nFirst, Bob and Claire swap balls. This means that Bob now has the green ball and Claire has the\nyellow ball. Then, Bob and Alice swap balls. This means that Bob now has the blue ball and Alice\nhas the green ball. Finally, Bob and Claire swap balls again. This means that Bob now has the\nyellow ball and Claire has the green ball. At the end of the game, Bob has the yellow ball. –> B\nPrediction B\n(e) Date Understanding\nQuestion\nTomorrow is 11/12/2019. What is the date one year ago from today in MM/DD/YYYY? Which\nchoice is true? Answer choices: (A) 11/02/2018, (B) 11/04/2018, (C) 09/04/2018, (D) 08/25/2018,\n(E) 11/11/2018.###\nAnswer E\nCompletion One year ago from today would be 11/11/2018.-->E\nPrediction E\n14880\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 7\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 8\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nWe used ChatGPT sparingly for paraphrasing.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nWe create code for our experiments and provide a link to the anonymized code in a footnote within the\nabstract. We use 12 existing datasets which are listed in Section 4 and Appendix B.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nAppendix B\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWe include an MIT license notice in our code repository.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nAppendix B\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 3, Appendix B\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix C\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n14881\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4. We use default hyperparameters.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4, Appendix A\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n14882"
}