{
  "title": "MirrorWiC: On Eliciting Word-in-Context Representations from Pretrained Language Models",
  "url": "https://openalex.org/W3201193395",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2144393117",
      "name": "Qianchu Liu",
      "affiliations": [
        "Language Science (South Korea)",
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2031764755",
      "name": "Fangyu. Liu",
      "affiliations": [
        "University of Cambridge",
        "Language Science (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2098750953",
      "name": "Nigel Collier",
      "affiliations": [
        "University of Cambridge",
        "Language Science (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A1927037681",
      "name": "Anna Korhonen",
      "affiliations": [
        "University of Cambridge",
        "Language Science (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A1969142033",
      "name": "Ivan Vulić",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3188660305",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W3153526514",
    "https://openalex.org/W3175049034",
    "https://openalex.org/W3173188814",
    "https://openalex.org/W3213730158",
    "https://openalex.org/W3029942262",
    "https://openalex.org/W2517456239",
    "https://openalex.org/W2950502294",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3035153870",
    "https://openalex.org/W2996282670",
    "https://openalex.org/W3098614164",
    "https://openalex.org/W3164054899",
    "https://openalex.org/W2096819730",
    "https://openalex.org/W3122838366",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3102617222",
    "https://openalex.org/W2963850840",
    "https://openalex.org/W4299574851",
    "https://openalex.org/W2970166416",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3157498557",
    "https://openalex.org/W3034675880",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W3176047188",
    "https://openalex.org/W2518186251",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W4287208415",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3164540570",
    "https://openalex.org/W3155248473",
    "https://openalex.org/W2964073004",
    "https://openalex.org/W2974273066",
    "https://openalex.org/W2953949198",
    "https://openalex.org/W3116132922",
    "https://openalex.org/W2740782137",
    "https://openalex.org/W3006963874",
    "https://openalex.org/W2251537235",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2436001372",
    "https://openalex.org/W3175362188"
  ],
  "abstract": "Recent work indicated that pretrained language models (PLMs) such as BERT and RoBERTa can be transformed into effective sentence and word encoders even via simple self-supervised techniques. Inspired by this line of work, in this paper we propose a fully unsupervised approach to improving word-in-context (WiC) representations in PLMs, achieved via a simple and efficient WiC-targeted fine-tuning procedure: MirrorWiC. The proposed method leverages only raw texts sampled from Wikipedia, assuming no sense-annotated data, and learns context-aware word representations within a standard contrastive learning setup. We experiment with a series of standard and comprehensive WiC benchmarks across multiple languages. Our proposed fully unsupervised MirrorWiC models obtain substantial gains over off-the-shelf PLMs across all monolingual, multilingual and cross-lingual setups. Moreover, on some standard WiC benchmarks, MirrorWiC is even on-par with supervised models fine-tuned with in-task data and sense labels.",
  "full_text": "Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 562–574\nNovember 10–11, 2021. ©2021 Association for Computational Linguistics\n562\nMIRROR WIC: On Eliciting Word-in-Context Representations\nfrom Pretrained Language Models\nQianchu Liu∗, Fangyu Liu∗, Nigel Collier, Anna Korhonen, Ivan Vuli´c\nLanguage Technology Lab, TAL, University of Cambridge\n{ql261, fl399, nhc30, alk23, iv250}@cam.ac.uk\nAbstract\nRecent work indicated that pretrained lan-\nguage models (PLMs) such as BERT and\nRoBERTa can be transformed into effective\nsentence and word encoders even via sim-\nple self-supervised techniques. Inspired by\nthis line of work, in this paper we pro-\npose a fully unsupervised approach to improv-\ning word-in-context (WiC) representations in\nPLMs, achieved via a simple and efﬁcient\nWiC-targeted ﬁne-tuning procedure: MIRROR -\nWIC. The proposed method leverages only\nraw texts sampled from Wikipedia, assuming\nno sense-annotated data, and learns context-\naware word representations within a standard\ncontrastive learning setup. We experiment\nwith a series of standard and comprehensive\nWiC benchmarks across multiple languages.\nOur proposed fully unsupervised MIRROR -\nWIC models obtain substantial gains over off-\nthe-shelf PLMs across all monolingual, mul-\ntilingual and cross-lingual setups. Moreover,\non some standard WiC benchmarks, MIRROR -\nWIC is even on-par with supervised models\nﬁne-tuned with in-task data and sense labels.\n1 Introduction\nPretrained Language Models (PLMs) such as\nBERT (Devlin et al., 2019) and RoBERTa (Liu\net al., 2019) provide dynamic contextual represen-\ntations; they induce token-level lexical representa-\ntions that capture the impact of the word’s context\non its embedding. Recent studies have assessed the\nPLMs by probing into their off-the-shelf represen-\ntation/feature space (Garí Soler et al., 2019; Wiede-\nmann et al., 2019; Reif et al., 2019; Garí Soler and\nApidianaki, 2021). While off-the-shelf PLMs al-\nready offer a useful contextualised lexical semantic\nspace, their contextualised representation spaces\nsuffer from instability and anisotropy (Mickus et al.,\n∗Equal contribution.\nDue to the fat-tailed \nnature of pandemic risk…\nDue t_fat-tailed \nnature of _mic risk…\nA review article in \nnature reported this \nfinding.\nThe end doesn’t always \njustify the means.\njustifynaturenaturenature\nDue to the fat-tailed \nnature of pandemic risk…\nDue t[MASK] fat-tailed \nnature of [MASK]mic risk…f(                           )     f(                            )     \nf(                         )     \nf(                         )     \nrandom span deletion\n……push\npull\nHuman interaction with \nnature has played a major \nrole.\nThe end doesn’t always \njustify the means.\nrandomly sampled \nraw sentences\nFigure 1: An illustrative overview of the M IRROR -\nWIC method, based on contrastive learning, for elicit-\ning better word-in-context (WiC) representations from\npretrained language models. We augment a randomly\nselected WiC instance with random span masking and\napply dropout to the hidden states to create two slightly\ndifferent representations of the base instance. These\ntwo representations form a positive pair for contrastive\nﬁne-tuning. During ﬁne-tuning, we pull the representa-\ntions of each positive pair closer together, while at the\nsame time pushing away representations of other WiC\ninstances, serving as negative examples.\n2020; Pedinotti and Lenci, 2020). As a conse-\nquence, they usually fall far behind the perfor-\nmance of the same PLM ﬁne-tuned with (i) sense\nannotations (Hadiwinoto et al., 2019; Blevins and\nZettlemoyer, 2020) or (ii) external (e.g., WordNet)\nknowledge (Levine et al., 2020).\nHowever, PLMs have been shown to actually\nstore more lexical and sentence-level information\nthan what can be directly extracted from their off-\nthe-shelf variants. In simple words, this knowl-\nedge must be ‘unlocked’ or exposed via additional\nadaptive ﬁne-tuning (Ruder, 2021). For instance,\nwhile off-the-shelf PLMs are not directly effective\nas universal sentence encoders, it is possible to con-\nvert them into such encoders through supervised\n(Reimers and Gurevych, 2019a; Feng et al., 2020;\nLiu et al., 2021a) or self-supervised ﬁne-tuning\n(Carlsson et al., 2021; Liu et al., 2021b; Gao et al.,\n2021) based on the contrastive learning paradigm.\nThe fundamental limitation of extracting con-\n563\ntextual features/representations directly from the\nlayers of the off-the-shelf PLMs is the mismatch\nbetween their (pre)training objectives and the fea-\nture extraction method. In other words, the con-\ntextual representations, typically extracted as the\naverages over the top four layers of a base PLM\n(Liu et al., 2020; Garí Soler and Apidianaki, 2021),\ncan be seen as a by-product of training a language\nmodel, and are not directly optimised for contex-\ntual sensitivity. Inspired by the previous work on\nadaptive ﬁne-tuning for word and sentence repre-\nsentations (Liu et al., 2021b), we propose a simple\nself-supervised technique termed MIRROR WIC: it\nrewires input PLMs to provide improved word-in-\ncontext (WiC) representations.\nUnlike prior work on ﬁne-tuning towards im-\nproving WiC representations, our MIRROR WIC\nprocedure disposes of any sense labels, annotated\ntask data, and any external knowledge, and elic-\nits improved WiC representations from PLMs in\na fully unsupervised way. We design a contrastive\nlearning framework that directly optimises the con-\ntextual representations (i.e., the top four hidden\nlayers of the input PLM) that are also the feature\nspace at inference time; see Figure 1 and Table 1.\nMIRROR WIC relies on the sets of positive and neg-\native pairs, where the positive pairs are created by\npairing an input sequence (which contains a target\nword) with its slightly altered variant. This altered\nsequence is obtained via random span masking and\nthe resulting representations for this pair are fur-\nther altered by dropout. The negative pairs are then\nsimply the same or different word’s contextual rep-\nresentations in a different context; Figure 1. These\npairs for ﬁne-tuning are mined from raw Wikipedia\nsentences. To understand why MIRROR WIC works\nso well, we provide ablation studies on the design\nchoices (including dropout rate, random span mask-\ning, etc.) and layer-wise analyses and visualisation\non MIRROR WIC’s effects on embedding properties\nsuch as isotropy.\nContributions. 1) We present a simple yet ex-\ntremely effective unsupervised MIRROR WIC tech-\nnique for eliciting contextual lexical knowledge.\n2) Our experiments on a range of English, mul-\ntilingual, and cross-lingual context-sensitive lex-\nical benchmarks demonstrate that MIRROR WIC\nachieves consistent and substantial improvements\nover different baseline PLMs, indicating its robust-\nness and wide applicability. 3) We offer extensive\nanalyses and additional insights into the inner work-\nings of MIRROR WIC, and its impact on the contex-\ntual representation space. We release our code at\ngithub.com/cambridgeltl/MirrorWiC.\n2 Related Work and Background\nWord-in-Context Representations. Modelling\ncontext inﬂuence on lexical meaning and creat-\ning context-aware word representations is a long-\nstanding research goal in lexical semantics. One\ndirection is to create discrete sense embeddings\naccording to a ﬁxed sense inventory such as Word-\nNet. These embeddings can be created from the\nattributes in the sense inventory such as glosses\n(Chen et al., 2014) or from the knowledge struc-\nture (Camacho-Collados et al., 2016). We point\nto Camacho-Collados and Pilehvar (2018) for a\nthorough survey on sense embeddings. Such sense\nrepresentations require a ﬁxed and discrete sense\ninventory and might not be sensitive enough to the\nthe dynamic and ﬂuid nature of contextual changes.\nMore recently, PLMs provide dynamic and con-\ntinuous contextual representations, not tied to pre-\ndeﬁned sense inventories, computed as a function\nof both the target word and its context. The use of\nPLMs has resulted in further progress on a range\nof context-aware evaluation benchmarks (Pilehvar\nand Camacho-Collados, 2019; Wang et al., 2019;\nRaganato et al., 2020). A body of work has aimed\nto enrich context-aware and sense information in\nthe PLMs by injecting such knowledge (e.g., sense\nannotations from predeﬁned sense inventories) at\npretraining stage (Levine et al., 2020) or during\ninference (Loureiro and Jorge, 2019). Other work\nhas attempted at combining/ensembling multiple\ncontextualised and static type-level embeddings to\nreﬁne the contextualised representation space (Liu\net al., 2020; Xu et al., 2020).\nInducing Text Representations from PLMs via\nSelf-Supervision. Recently, there has been grow-\ning interest in learning completely unsupervised\nsentence representations from PLMs using con-\ntrastive learning techniques (Carlsson et al., 2021;\nLiu et al., 2021b; Gao et al., 2021; Yan et al., 2021;\nKim et al., 2021; Zhang et al., 2021). Similar to\nthe supervised approaches such as Sentence-BERT\n(Reimers and Gurevych, 2019b) or SapBERT (Liu\net al., 2021a), the idea is to transform an input PLM\ninto an effective sentence encoder via additional\nﬁne-tuning. During self-supervised contrastive ﬁne-\ntuning, the model learns from identical or automat-\nically modiﬁed text sequences (treated as positive\n564\nexamples), and regards different sentences as neg-\native pairs. MIRROR BERT (Liu et al., 2021b) is\na general self-supervised contrastive ﬁne-tuning\nframework that transforms off-the-shelf PLMs into\neffective word and sentence encoders. Our pro-\nposed MIRROR WIC method can be seen as an ex-\ntension of MIRROR BERT, now focused on elicit-\ning improved word-in-context representations and\ncontext-sensitive lexical tasks.\n3 M IRROR WIC: Methodology\nBaseline WiC Representations. Prior work di-\nrectly extracts word-in-context representations\nfrom the parameters of the off-the-shelf PLMs. The\nmost effective (empirically validated) strategy is\n1) averaging the representations from the top four\nPLM’s layers, and 2) then taking either the ﬁrst\nconstituent subword from the PLM’s vocabulary to\nrepresent the target word, or further averaging the\nrepresentations of the word’s constituent subwords\n(Liu et al., 2020; Garí Soler and Apidianaki, 2021).\n3.1 Self-Supervised WiC Fine-Tuning\nWe hypothesise that it is possible to convert the\ninput PLM into an improved WiC encoder through\nadaptive (self-supervised) ﬁne-tuning. Given a\nset of raw sentences without labels, how do we\ntune the PLMs to further expose their word-in-\ncontext knowledge? Inspired by MIRROR BERT\n(Liu et al., 2021b), we apply a self-supervised con-\ntrastive learning scheme to elicit better word-in-\ncontext representations. We ﬁne-tune the input\nPLM by contrasting the representations of different\nword-in-context pairs while pulling representations\nof a self-duplicated word-in-context pair closer in\nthe representation space (see Figure 1).\nData Creation. Given a set of N non-duplicated\nsentences, we randomly select a word in each\nsentence as the target word: i.e., the sentences\nbecome a set of ‘word-in-context instances’.\nWe then follow MIRROR BERT and generate a\nlabelled dataset by duplicating each instance in\nthe set and assigning identical labels to iden-\ntical instances and different labels to different\nword-in-contexts (Table 2, upper half): D =\n{(x1,y1),(x1,y1),..., (xN ,yN ),(xN ,yN )},\nwhere ∀i= 1,...N , it holds xi = xi,yi = yi.\nData Augmentation. We further follow MIRROR -\nBERT to create a slightly altered (or augmented)\n‘view’ of the same text sequence: we randomly\nreplace a span of text with ‘[MASK]’1 in all dupli-\ncated examples. There is a fundamental difference\nto MIRROR BERT where such ‘random span mask-\ning’ technique is applied on sentences; for word-in-\ncontext, we keep the target word intact (otherwise\nthe semantics changes drastically) and randomly\nreplace a span of length Kon both sides of the tar-\nget word; see Table 2 (lower half). Besides random\nspan masking, the dropout modules in the Trans-\nformer layers also slightly and randomly alter the\nrepresentations of each word-in-context instance.\nThey serve as another source of data augmenta-\ntion to further perturb the word-in-context repre-\nsentations. After both input space augmentation\n(random span masking) and feature space augmen-\ntation (dropout layers embedded in the Transformer\nlayers), the resulting embeddings of even a positive\npair will be slightly different.2\nContrastive Fine-Tuning. Following the feature\nextraction procedure from off-the-shelf PLMs, we\ncompute the average of hidden states from the\nPLM’s top four layers, and then take the average of\nall token(s) that correspond to the target word, as\nthe word-in-context representation. Let f(·) denote\nthe encoder which outputs such WiC representa-\ntion. We leverage InfoNCE (Oord et al., 2018) to\ncluster/attract the positive pairs together and push\naway the negative pairs in the embedding space:\nL= −\nN∑\ni=1\nlog exp(cos(f(xi),f(xi))/τ)∑\nxj∈Ni\nexp(cos(f(xi),f(xj))/τ)\n. (1)\nwhere τ is a tunable temperature; Ni denotes all\nnegatives of xi, which includes all xj,xj where\ni ̸= j in the current data batch (i.e., |Ni|= N −\n2). Intuitively, the numerator is the similarity of\nthe self-duplicated pair (a positive pair) and the\ndenominator is the sum of the similarities between\nxi and all other strings besides xi (negative pairs).\nFor positive pairs, though one sequence in the\npair is slightly altered via random span masking\nand the representations go through dropout, the en-\ncoding function f(·) should learn an invariant map-\nping and reconstruct the correct semantics from\nthe noise (Liu et al., 2021b). Most negative ex-\namples contain different target words and different\ncontexts (e.g., x1 and xN in Table 2). Naturally,\n1Or ‘<MASK>’ for input to RoBERTa.\n2Note that random span masking is applied on only one\ninstance of each duplicated pair, while the dropouts are applied\nto all instances.\n565\nmodel representations ﬁne-tuned representations extracted\noff-the-shelf PLMs ( [CLS] +) language modelling head word token average (top four layers)\nMIRROR BERT [CLS]/mean-pooling [CLS]/mean-pooling\nMIRROR WIC word token average (top four layers) word token average (top four layers)\nTable 1: MIRROR WIC beneﬁts from the consistency of representations at (i) ﬁne-tuning and (ii) feature extraction\nand inference: both are focused on word-in-context (WiC) representations.\nStep 1: Automatic dataset creation for WiC ﬁne-tuning\n(x1,y1) (Due to the fat-tailednatureof pandemic risk, . . .,1)\n(x1,y1) (Due to the fat-tailednatureof pandemic risk, . . .,1)\n. . . . . .\n(xi,yi) (Human interaction withnaturehas played a major role.,i)\n(xi,yi) (Human interaction withnaturehas played a major role.,i)\n. . . . . .\n(xN,yN)(The end doesn’t alwaysjustifythe means.,N)\n(xN,yN)(The end doesn’t alwaysjustifythe means.,N)\nStep 2: Random span masking\n(x1,y1) (Due to the fat-tailednatureof pandemic risk, . . .,1)\n(x1,y1) (Due _e fat-tailednatureof pandemic _ . . .,1)\n. . . . . .\n(xi,yi) (Human interaction withnaturehas played a major role.,i)\n(xi,yi) (Human intera_ withnaturehas play_major role.,i)\n. . . . . .\n(xN,yN)(the end does not alwaysjustifyThe means.,N)\n(xN,yN)(The end does_alwaysjustify_eans.,N)\nTable 2: Upper: the automatically generated labelled\ndataset for ﬁne-tuning PLMs towards learning better\nword-in-context representations. Bold denotes the tar-\nget word. Lower: data augmentation via random span\nmasking. ‘_’ denotes the ‘[MASK]’ token.\nsuch pairs are of different meanings and the model\nshould produce different representations.Note that\nit is possible to also have the same target word ap-\npearing in different contexts as a negative pair (e.g.,\nx1 and xi in Table 2). If the pair indeed has very\ndifferent semantics (of a different sense), then push-\ning them apart is actually desirable. However, even\nif the items in the pair happen to have similar mean-\nings, our learning objective still instructs the model\nto push them away from each other. Our rationale\nand decision here are based on the following: (1)\nSuch false negative pairs can act as a regularisation;\nand (2) in essence, one could argue that all distinct\nword-in-context instances have slightly different\nmeanings since sense is a continuous function of\nword and context.\n4 Experimental Setup\nWiC Evaluation. We evaluate MIRROR WIC on a\nrange of context-sensitive lexical semantic tasks in\nmonolingual English settings, as well as in multi-\nlingual and cross-lingual settings.\nFor English, we evaluate on two similarity-based\ntasks: Usim and CoSimLex; two word-in-context\nclassiﬁcation tasks: WiC and WiC-TSV; and one-\nshot Word Sense Disambiguation (WSD). Usim\n(Erk et al., 2013) measures the similarity between\ntwo instances of the same word occurring in two\ndifferent sentential contexts. CoSimLex (Armen-\ndariz et al., 2020) measures the change in simi-\nlarity between two different words appearing in\ntwo different contexts: paragraphs. We follow the\nstandard evaluation protocol, computing the cosine\nsimilarity of the contextual word representations\nand comparing them against human-elicited scores\nvia Spearman’s rank correlation (ρ).\nThe WiC classiﬁcation task (Pilehvar and\nCamacho-Collados, 2019) challenges a model to\nmake a binary decision on whether or not the same\ntarget word has the same meaning in two different\ncontexts. The WiC-TSV (TSV) task (Breit et al.,\n2021) extends the originalWiC to multiple domains\nwith three different subtasks. In TSV-1, the task is\nto decide if the intended sense of the target word in\nthe context matches the target sense described by\nthe deﬁnition. In TSV-2, the model must identify if\nthe intended sense (in the context) is the hyponym\nof the provided hypernyms. TSV-3 combines the\nprevious two subtasks (see Breit et al. (2021) for\nfurther details).\nThe WSD task (Navigli, 2009; Raganato et al.,\n2017) requires a system to select the correct label\nfor a given target word in context from a candidate\nset of all possible meanings for this target word. To\nevaluate the feature space of the models in WSD,\nwe create a one-shot setting where we provide one\ncontext example3 per label and perform nearest\nneighbour search over contextual word represen-\ntations from the candidate labels. We directly test\nthe models on the concatenated ALL test set from\nRaganato et al. (2017) without access to training\nand development data.\nWe also perform multilingual and cross-lingual\nevaluation on XL-WiC (Raganato et al., 2020)\n3The context examples are taken from WordNet entries. If\na sense does not contain context, we reformat the deﬁnition as\n’<target word> means ...’ as the target word’s context.\n566\nand AM2iCo (Liu et al., 2021c). XL-WiC pro-\nvides WiC-style evaluations in multiple languages.\nAM2iCo extends XL-WiC to lower-resource lan-\nguages, adds more difﬁcult adversarial examples,\nand enables cross-lingual evaluations. For brevity,\nwe show results for four typologically diverse lan-\nguages both from XL-WiC (ZH, KO, HR, ET); and\nfour languages in AM2iCo (ZH, KA, JA, AR).4\nFor WiC, TSV , XL-WiC and AM2iCo, our main\nexperiments follow the unsupervised method from\nPilehvar and Camacho-Collados (2019): we com-\npute cosine similarity between the contextual word\nrepresentations in each pair, and search for a thresh-\nold to divide true (i.e., same meaning) and false\ninstances on the development set in each task. 5\nWe report accuracy scores in the main paper, while\nadditional area-under-curve (AUC) scores are avail-\nable in App. §A.1.\nUnderlying PLMs. We experiment with several\nstandard input PLMs for English, but we remind the\nreader that the MIRROR WIC framework is applica-\nble with a wide range of PLMs: 1) BERT (Devlin\net al., 2019) as a standard choice for WiC repre-\nsentation learning and evaluation (Raganato et al.,\n2020); 2) RoBERTa (Liu et al., 2019) as an opti-\nmised and improved PLM; and 3) DeBERTa (He\net al., 2020) as a more recent PLM that achieves\nstate-of-the-art results in a range of natural lan-\nguage understanding tasks (Wang et al., 2019). 6\nFor all non-English experiments, unless noted oth-\nerwise, we rely on multilingual BERT (mBERT) as\nthe underlying PLM (see App. §A.2).\nFine-Tuning Details. We largely follow the MIR-\nROR BERT ﬁne-tuning setup (Liu et al., 2021b), us-\ning 10k sentences randomly drawn from Wikipedia\nas the MIRROR WIC ﬁne-tuning corpus. For mono-\nlingual models, we sample 10k sentences from the\ncorresponding Wikipedia of that language. For\n4ZH: Mandarin Chinese, KO: Korean, HR: Croatian, ET:\nEstonian, KA: Georgian, JA: Japanese, AR: Arabic.\n5We add templates in each TSV subtask: ‘[target word]\nmeans <deﬁnition>’ (TSV-1); ‘[target word] is a kind of <hy-\npernym>’ (TSV-2) and ‘[target word] is a kind of <hypernym>\nand means <deﬁnition>’ (TSV-3). We then compute similarity\nbased on the contextual representations of the target words\nin these templates. This results in an unsupervised approach\nwhich is more effective than the approach from prior work\n(Breit et al., 2021), where cosine similarity is computed on\ndeﬁnition/hypernym embeddings.\n6DeBERTa extends the standard BERT architecture by in-\ncorporating two novel techniques: disentangled attention that\nencodes a word’s content and position separately, and an en-\nhanced masked decoder that incorporates absolute position for\npredicting masked tokens during masked language modelling.\ncross-lingual models, we sample 5k sentences from\nEnglish Wikipedia and 5k from Wikipedia of each\ntarget language. We train all models with AdamW\n(Loshchilov and Hutter, 2019) with a learning rate\nof 2e-5 for 1 epoch. The τ in Eq. (1) is set to0.04.\nWe set K(random span masking rate) to 10, 0 and\n1 for BERT, RoBERTa and DeBERTa respectively.\nThe respective dropout rates are 0.4, 0.3 and 0.3\nfor BERT, RoBERTa and DeBERTa. All hyper-\nparameters are tuned on the development set of\nWiC and kept unchanged for all other experiments.\nWe refer the reader to the Appendix (Table 13) for\na full listing of hyperparameters along with their\nsearch space.\n5 Results and Discussion\n5.1 Main Results: Evaluation on English\nThe main results are provided in Table 3 and Ta-\nble 4. Most notably, we observe consistent and\nsubstantial gains over all unsupervised baselines,\nincluding the off-the-shelf PLMs without MIRROR -\nWIC ﬁne-tuning. While the underlying PLMs, as\nsuggested by prior work (Garí Soler and Apidi-\nanaki, 2021), do encode a wealth of sense-related\nknowledge, that knowledge can be further exposed\nvia the proposed context-aware MIRROR WIC ﬁne-\ntuning procedure.\nImpact of the Underlying PLM (Table 3). MIR-\nRORWIC is effective with BERT, RoBERTa and\nDeBERTa. DeBERTa+MIRROR WIC yields larger\ngains, and even results in the highest absolute\nscores on average. In other words, a seemingly\n‘weaker’ off-the-shelf PLM under the naive feature\nextraction baseline (DeBERTa) is transformed into\nthe best-performing WiC encoder after the MIR-\nRORWIC procedure. This hints at the necessity\nto unlock the input PLM’s ’task solving potential’\nthrough adaptive ﬁne-tuning.\nComparison with Sentence Encoders (Table 3).\nWe also probe how modelling the sentences (with-\nout knowing which target word the context is de-\nscribing) performs on the evaluation tasks. In par-\nticular, we evaluate the standard ’go-to’ sentence\nencoder Sentence-BERT (Reimers and Gurevych,\n2019b), and the original MIRROR BERT (Liu et al.,\n2021b). We ﬁnd that MIRROR WIC, with its direct\nfocus on word-in-context representations and WiC-\noriented ﬁne-tuning, substantially outperforms the\ntwo sentence encoders. The ﬁnding validates our\nhypothesis that naively applying sentence encoders\n567\nmodel↓, dataset→ Usim (ρ) WiC (acc) TSV-1 (acc) TSV-2 (acc) TSV-3 (acc) CoSimLex ( ρ) One-shot WSD (acc)\nSentence-BERT 23.57 61.91 62.46 59.64 62.72 - 42.63\nMIRROR BERT 23.21 64.10 66.32 64.78 66.32 - 44.93\nBERT 54.52 68.49 61.69 60.66 61.95 76.2 52.90\n+ MIRROR WIC 61.82 71.94 69.15 66.06 68.38 77.41 57.10\nRoBERTa 50.25 66.77 55.52 56.55 57.58 75.64 51.38\n+ MIRROR WIC 57.95 71.15 69.92 67.60 71.70 77.27 56.51\nDeBERTa 54.77 66.14 59.38 59.89 60.41 72.06 53.99\n+ MIRROR WIC 62.79 71.78 70.95 67.86 71.20 77.70 59.02\nTable 3: Results across a collection of context-aware lexical semantic tasks in English.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.55\n0.60\n0.65\n0.70accuracy\nBERT\nBERT+MirrorWiC\n(a) BERT layer-wise accuracy; WiC dev\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.55\n0.60\n0.65\n0.70accuracy\nDeBERTa\nDeBERTa+MirrorWiC (b) DeBERTa layer-wise accuracy; WiC dev\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n2.5\n2.0\n1.5\n1.0\nIS\nBERT\nBERT + MirrorWiC\n(c) BERT isotropy (higher is better)\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n5\n4\n3\n2\n1\nIS\nDeBERTa\nDeBERTa + MirrorWiC (d) DeBERTa isotropy (higher is better)\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.1\n0.2\n0.3cosine\nBERT\nBERT + MirrorWiC\n(e) BERT random-word cosine similarity\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.2\n0.4\n0.6cosine\nDeBERTa\nDeBERTa + MirrorWiC (f) DeBERTa random-word cosine similarity\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.3\n0.4\n0.5cosine (isotropy-adjusted)\nBERT\nBERT+MirrorWiC\n(g) BERT intra-sentence cosine similarity\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.2\n0.3\n0.4cosine (isotropy-adjusted)\nDeBERTa\nDeBERTa+MirrorWiC (h) DeBERTa intra-sentence cosine similarity\nFigure 2: Layer-wise analyses of BERT (left column) and DeBERTA (right column) before and after applying MIR-\nRORWIC. The ﬁrst row (a,b) shows the model performance and can be linked to the isotropy analysis (the middle\ntwo rows: c,d,e,f) and contextualisation analysis (the last row: g,b). Task performance correlates strongly with\nisotropy and contextualisation changes especially in the last four layers (highlighted with dots); shade=variance.\nis not sufﬁcient for context-aware lexical semantic\ntasks. While the two sentence encoders do provide\ncompetitive performance in WiC-style tasks, their\nperformance decreases drastically on Usim. This\nfurther indicates that the ﬁne-grained similarity-\nbased Usim evaluation requires a more accurate\nand subtler contextual lexical semantic ability than\nthe binary classiﬁcation in WiC.\nComparison with Supervised WiC Methods\n(Table 4). The scores reveal that the unsupervised\nBERT + MIRROR WIC variant can even outperform\nthe supervised model (ﬁne-tuned with labelled in-\n568\nmodel↓, dataset→ WiC TSV-1 TSV-2 TSV-3\nBERT 65.85 65.08 62.09 63.16\n+ MIRROR WIC 69.64 73.66 69.83 73.73\ntask-supervised BERT 69.00 75.30 71.40 76.60\nTable 4: BERT+M IRROR WIC versus supervised\nBERT-based methods on the test sets of English WiC-\nstyle tasks. The supervised variant on WiC is replicated\nfrom Wang et al. (2019). The supervised results on\nTSV are taken from Breit et al. (2021).\nXL-WiC ZH * KO * HR ET\nBERT 73.74 68.41 61.10 57.06\n+ MIRROR WIC 75.70 72.26 67.32 61.43\nAM2iCo ZH KA JA AR\nBERT 63.80 59.90 64.10 60.60\n+ MIRROR WIC 64.60 61.00 64.70 63.90\nTable 5: Results (test set accuracy) on multilingual and\ncross-lingual word-in-context tasks. We use mBERT\nas the underlying PLM for all the languages except for\nZH * and KO * (in XL-WiC) where their monolingual\nBERT models were used.\ntask data) in the WiC task. The results on TSV indi-\ncate that the gap between the unsupervised BERT-\nbased approach to the supervised performance is\nmuch reduced: from the ∼10% gap to only ∼2% in\nall three TSV tasks when MIRROR WIC is applied.\n5.2 Multilingual and Cross-Lingual Results\nThe results are summarised in Table 5. Notably, we\nobserve that the effectiveness of MIRROR WIC is\nnot tied to English, and extends to other languages.\nWe observe consistent improvements with the un-\nderlying PLMs monolingually pretrained in other\nlanguages, as well as with the multilingually pre-\ntrained mBERT. The gains on XL-WiC are more\npronounced than on the more difﬁcult AM2iCo\nbenchmark. By design AM2iCo is a more challeng-\ning benchmark, and additional external knowledge\ninjection might be necessary to improve the results\nfurther; unlike XL-WiC, AM2ico requires the mod-\nels to understand the cross-lingual correspondence\nof mostly entity names that occur less frequently.\n5.3 Further Discussion and Analyses\nLayer-wise Performance (Figs. 2a and 2b). The\nﬁgures reveal that the success of MIRROR WIC is\nattributed to the performance gains achieved in\nthe last four layers of the ﬁne-tuned PLMs. This\nis expected as these four layers are exactly what\nwe optimise in the MIRROR WIC procedure. This\nalso conﬁrms our hypothesis that matching training\nand inference representations helps adapt and elicit\nword-in-context knowledge from the PLMs.\nIsotropy (Figs. 2c and 2d). As empirically val-\nidated in prior work on sentence representations\n(Gao et al., 2021; Liu et al., 2021b), contrastive\nﬁne-tuning reshapes the embedding space geome-\ntry towards more isotropic representations, which\nin turn has a positive impact on semantic similarity\ntasks. We now examine whether the same ‘isotropy-\nincreasing’ effect is achieved withMIRROR WIC.\nTo this end, we leverage a quantitative isotropy\nscore (IS), proposed in prior work (Arora et al.,\n2016; Mu and Viswanath, 2018),7 and deﬁned as:\nIS(V) = log\n(\nminc∈C\n∑\nv∈Vexp(c⊤v)\nmaxc∈C\n∑\nv∈Vexp(c⊤v)\n)\n(2)\nwhere Vis the set of vectors, Cis the set of all\npossible unit vectors in the embedding space (i.e.,\n{c : |c| = 1}). Practically, Cis approximated\nby the eigenvector set of V⊤V (V is the stacked\nembeddings of V). The larger the IS value, the\nmore isotropic an embedding space is.8\nAs seen in Fig. 2c and Fig. 2d, both BERT and\nDeBERTa create more isotropic embedding spaces\nin general in the last four layers afterMIRROR WIC\ntraining. Note that DeBERTa’s space isotropy is\nable to beneﬁt more from MIRROR WIC, which\nalso explains its large gains in the end tasks.\nIt is also possible to assess isotropy by simply\nlooking at the cosine similarity of random words\n(Ethayarajh, 2019). We calculate word representa-\ntions in each layer as the average of the word’s con-\ntextual representations from Wikipedia. We then\ntake ﬁve random samples of 200 random words and\ncompute pair-wise similarity. We take the average\nof the similarity scores in each sample with vari-\nance reported in Fig. 2e and Fig. 2f. The results\nconﬁrm the trend: the last four layers with MIR-\nRORWIC exhibit much lower random word cosine\nsimilarities than the off-the-shelf PLM.\nIntra-Sentence Similarity (Figs. 2g and 2h). As\na measure of contextualisation, we follow Etha-\n7The same metric is used for measuring isotropy of con-\ntextual word representations by Rajaee and Pilehvar (2021).\n8We randomly sample 10k sentences from English\nWikipedia as V. We compute the average word-in-context\nembeddings for all words in each sentence and then compute\nthe IS value. We repeat the process for ﬁve times to reduce\nthe randomness introduced in sampling.\n569\nWord-in-context 1 Word-in-context 2 BERT +M IRROR WIC Gold\nSpend money. He spends far more on gambling\nthan he does on living proper.\n-0.0850 (F) 0.2327 (T) T\nThat toaster can make wonderful toasts. I ate a piece of toast for breakfast. 0.0160 (F) 0.3234 (T) T\nWar is hell. The hell of battle. -0.0403 (F) 0.2378 (T) F\nEase the pain in your legs. The pain eased overnight. 0.0157 (F) 0.2873 (T) F\nTable 6: Examples of changed cosine similarity scores (isotropy-adjusted) after MIRROR WIC; English WiC (dev).\n10\n 0 10\n10\n5\n0\n5\n10\nmodel = BERT\n10\n 0 10\nmodel = BERT+MirrorBERT\n10\n 0 10\nmodel = BERT+MirrorWiC\nsense\nspring (hydrology)\nspring (device)\nspring (season)\nsummer (season)\nFigure 3: t-SNE embedding visualisation of different senses of spring and summer under different models.\nyarajh (2019), and deﬁne intra-sentence similar-\nity as each word’s similarity to its context. The\ncontext is computed as the mean vector of all the\nword representations in the sentence. The scores\nare isotropy-adjusted by substracting the intra-\nsentence similarity scores by the random word sim-\nilarity in each layer, see (Ethayarajh, 2019). For\nboth BERT and DeBERTa, we can see that the\nlast four layers become more contextualised after\napplying MIRROR WIC: they encode more informa-\ntion about the context as the contextual word rep-\nresentations become much more similar to its con-\ntext in the top Transformer layers than in the base\nPLM. This increased contextualisation could ex-\nplain why MIRROR WIC gives better performance\nin the context-sensitive lexical semantic tasks.\nError Analysis (Table 6). Conducting an error\nanalysis of BERT before and after MIRROR WIC\non the WiC dev set, we observe that 94 instances\nchanged their labels, among which 58 are MIR-\nRORWIC correcting the original predictions. In 43\nout of 58 cases, MIRROR WIC is producing more\nTRUE positives. The examples with the largest\nsimilarity changes are provided in the upper half\nof Table 6. For the 36 cases where MIRROR WIC\nchanges the originally correct predictions to the\nwrong prediction, 29 are false positives; see the\nlower half of Table 6. We manually inspect these\ncases and ﬁnd that the distinctions between the two\ncontexts are usually too ﬁne-grained to tell even for\nhumans. For instance, it seems acceptable to align\nwith the MIRROR WIC’s (incorrect) predictions for\nhell and ease in the two examples in Table 6.\nVisualising the Embedding Space (Fig. 3). Con-\ntextualised embeddings for an ambiguous word\n(spring) with off-the-shelf BERT, MIRROR BERT\nand MIRROR WIC are visualised in Fig. 3 (sense\nlabels from Wikipedia). While MIRROR WIC main-\ntains the sense clusters from BERT and teases apart\nthe different senses even more, MIRROR BERT ex-\nhibits no clear sense distinctions. This shows a\nfundamental difference between MIRROR WIC and\nMIRROR BERT: MIRROR BERT is insensitive to the\ntarget word, and directly applying it to context-\nsensitive lexical tasks yields subpar performance.\n5.4 Ablation Study\nAn ablation study is conducted on English WiC\n(dev). Foreshadowing, the dropout rate and the\nlayer averaging strategy are the two most important\nfactors for MIRROR WIC to be effective.\nDropout and Random Span Masking (Tabs. 7\nand 8). The MIRROR WIC performance is most\nsensitive to the dropout rate; it requires larger\ndropout rates (0.3 for DeBERTa and 0.4 for BERT)\nthan MIRROR BERT (0.1 dropout). This may be re-\nlated to the different levels of granularity. Sentence\nmeanings can largely change with even slight differ-\nences in context: therefore, positive sentence pairs\nfor MIRROR BERT are required to be very similar.\n570\ndropout rate→ 0 0.1 0.2 0.3 0.4 0.5 0.6\nBERT + MIRROR WIC 68.02 68.65 70.21 71.31 71.94 68.80 68.49\nDeBERTa + MIRROR WIC 65.67 69.12 70.53 71.78 67.08 65.98 66.30\nTable 7: Impact of dropout rate in MIRROR WIC.\nmodel↓, random span masking→ off on\nBERT + MIRROR WIC 71.31 71.47 ↑0.16\nDeBERTa + MIRROR WIC 71.78 71.94 ↑0.16\nTable 8: Impact of random span masking.\naverage last n layers→ 1 2 3 4 5 6 12\nBERT + MIRROR WIC 68.96 68.80 70.06 71.94 70.68 70.84 67.71\nDeBERTa + MIRROR WIC 71.47 73.04 72.41 71.78 71.15 70.53 69.74\nTable 9: Impact of layer averaging strategies.\nWord-in-context meaning can tolerate larger con-\ntextual differences: larger dropout rates are thus\npreferable with MIRROR WIC to create positive\npairs with more distinct representations. Random\nspan masking is less crucial than the dropout rate,\nand gives only slight gains (Table 8).\nLayer Averaging Strategy (Table 9). Averag-\ning across all layers of the PLM is suboptimal\nfor WiC representations, and the strategy of av-\neraging only over the last four layers is indeed\nthe optimal one for BERT. However, DeBERTa\nreaches its peak when averaging over the last 2 lay-\ners. Our ﬁndings corroborate those from previous\nstudies which report that contextualised informa-\ntion is usually stored in higher layers (Ethayarajh,\n2019; Garí Soler and Apidianaki, 2021), and the\nbulk of decontextualised information is stored in\nlower layers (Vuli´c et al., 2020).\nInput Size (Fig. 4). As in Fig. 4, we show a sharp\nincrease of performance from 5k to 10k on both\nUsim and WiC. While WiC maintains its perfor-\nmance with small ﬂuctuation from 10k throughout\nto 50k, there is a clear downward slope for Usim\nfrom 10k onward. This is in line with ﬁndings in\nMIRROR BERT, and also shows that the model does\nnot require plenty of ﬁne-tuning data to transform\ninto a WiC encoder. This further conﬁrms that the\nmodel is not so much learning new knowledge as\nrewiring knowledge to the surface.\n6 Conclusion\nWe proposed MIRROR WIC, a fully unsupervised\napproach for eliciting word-in-context representa-\ntions from pretrained language models (PLMs), re-\n5k 10k 20k 30k 40k 50k\ntraining data size\n0.690\n0.695\n0.700\n0.705\n0.710\n0.715performance\nBERT+MirrorWiC\nDeBERTa+MirrorWiC\n(a) WiC (dev)\n5k 10k 20k 30k 40k 50k\ntraining data size\n0.58\n0.59\n0.60\n0.61\n0.62performance\nBERT+MirrorWiC\nDeBERTa+MirrorWiC (b) Usim\nFigure 4: Impact of input size of the training data for\nMIRROR WIC. Evaluation on WiC (dev).\nquiring only raw sentences as input, and disposing\nof labelled data and sense inventories. We showed\nthat MIRROR WIC is PLM-agnostic and language-\nagnostic, yielding substantial performance boosts\nin context-aware lexical semantic tasks in English,\nmultilingual and cross-lingual setups and demon-\nstrating that additional WiC knowledge can be\nexposed from the PLMs. We then delved into\nthe inner-working of MIRROR WIC, demonstrat-\ning that the performance improvement strongly\ncorrelates with metrics such as isotropy score and\nintra-sentence word similarity. In future work, we\nwill also look into weakly supervised approaches\nthat combine self-supervision with external sense-\nrelated knowledge.\nAcknowledgements\nWe thank the three reviewers and the ACs for their\nhelpful feedback. We acknowledge Peterhouse\nCollege at University of Cambridge for funding\nQianchu Liu’s PhD, and Grace & Thomas C.H.\nChan Cambridge Scholarship for funding Fangyu\nLiu’s PhD. The work has also been funded by the\nERC Grant LEXICAL (no. 648909) and the ERC\nPoC Grant MultiConvAI (no. 957356) awarded to\nAnna Korhonen.\nReferences\nCarlos Santos Armendariz, Matthew Purver, Matej\nUlˇcar, Senja Pollak, Nikola Ljubeši ´c, and Mark\nGranroth-Wilding. 2020. CoSimLex: A resource\nfor evaluating graded word similarity in context.\nIn Proceedings of the 12th Language Resources\nand Evaluation Conference, pages 5878–5886, Mar-\nseille, France. European Language Resources Asso-\nciation.\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,\nand Andrej Risteski. 2016. A latent variable model\napproach to PMI-based word embeddings. Transac-\ntions of the Association for Computational Linguis-\ntics, 4:385–399.\n571\nTerra Blevins and Luke Zettlemoyer. 2020. Moving\ndown the long tail of word sense disambiguation\nwith gloss informed bi-encoders. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 1006–1017, On-\nline. Association for Computational Linguistics.\nAnna Breit, Artem Revenko, Kiamehr Rezaee, Moham-\nmad Taher Pilehvar, and Jose Camacho-Collados.\n2021. WiC-TSV: An evaluation benchmark for tar-\nget sense veriﬁcation of words in context. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 1635–1645, Online.\nAssociation for Computational Linguistics.\nJose Camacho-Collados and Mohammad Taher Pile-\nhvar. 2018. From word to sense embeddings: A sur-\nvey on vector representations of meaning. Journal\nof Artiﬁcial Intelligence Research, 63:743–788.\nJosé Camacho-Collados, Mohammad Taher Pilehvar,\nand Roberto Navigli. 2016. Nasari: Integrating ex-\nplicit knowledge and corpus statistics for a multilin-\ngual representation of concepts and entities. Artiﬁ-\ncial Intelligence, 240:36–64.\nFredrik Carlsson, Amaru Cuba Gyllensten, Evan-\ngelia Gogoulou, Erik Ylipää Hellqvist, and Magnus\nSahlgren. 2021. Semantic re-tuning with contrastive\ntension. In International Conference on Learning\nRepresentations.\nXinxiong Chen, Zhiyuan Liu, and Maosong Sun.\n2014. A uniﬁed model for word sense represen-\ntation and disambiguation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1025–1035,\nDoha, Qatar. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nKatrin Erk, Diana McCarthy, and Nicholas Gaylord.\n2013. Measuring word meaning in context. Com-\nputational Linguistics, 39(3):511–554.\nKawin Ethayarajh. 2019. How contextual are contex-\ntualized word representations? comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\nArivazhagan, and Wei Wang. 2020. Language-\nagnostic BERT sentence embedding. CoRR,\nabs/2007.01852.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. arXiv preprint arXiv:2104.08821.\nAina Garí Soler and Marianna Apidianaki. 2021. Let’s\nplay mono-poly: Bert can reveal words’ polysemy\nlevel and partitionability into senses. Transactions\nof the Association for Computational Linguistics\n(TACL).\nAina Garí Soler, Anne Cocos, Marianna Apidianaki,\nand Chris Callison-Burch. 2019. A comparison of\ncontext-sensitive models for lexical substitution. In\nProceedings of the 13th International Conference on\nComputational Semantics - Long Papers, pages 271–\n282, Gothenburg, Sweden. Association for Compu-\ntational Linguistics.\nChristian Hadiwinoto, Hwee Tou Ng, and Wee Chung\nGan. 2019. Improved word sense disambiguation us-\ning pre-trained contextualized word representations.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 5297–\n5306, Hong Kong, China. Association for Computa-\ntional Linguistics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nTaeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2021.\nSelf-guided contrastive learning for BERT sentence\nrepresentations. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 2528–2540, Online. Association for\nComputational Linguistics.\nYoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan\nPadnos, Or Sharir, Shai Shalev-Shwartz, Amnon\nShashua, and Yoav Shoham. 2020. SenseBERT:\nDriving some sense into BERT. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 4656–4667, On-\nline. Association for Computational Linguistics.\nFangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco\nBasaldella, and Nigel Collier. 2021a. Self-\nalignment pretraining for biomedical entity repre-\nsentations. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies. Association for Computational Lin-\nguistics.\n572\nFangyu Liu, Ivan Vuli ´c, Anna Korhonen, and Nigel\nCollier. 2021b. Fast, effective and self-supervised:\nTransforming masked language models into univer-\nsal lexical and sentence encoders. arXiv preprint\narXiv:2104.08027.\nQianchu Liu, Diana McCarthy, and Anna Korho-\nnen. 2020. Towards better context-aware lexical\nsemantics:adjusting contextualized representations\nthrough static anchors. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4066–4075, On-\nline. Association for Computational Linguistics.\nQianchu Liu, Edoardo M Ponti, Diana McCarthy, Ivan\nVuli´c, and Anna Korhonen. 2021c. Am2ico: Evalu-\nating word meaning in context across low-resource\nlanguages with adversarial examples. arXiv preprint\narXiv:2104.08639.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decou-\npled weight decay regularization. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nDaniel Loureiro and Alípio Jorge. 2019. Language\nmodelling makes sense: Propagating representations\nthrough WordNet for full-coverage word sense dis-\nambiguation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5682–5691, Florence, Italy. Associa-\ntion for Computational Linguistics.\nTimothee Mickus, Denis Paperno, Mathieu Constant,\nand Kees van Deemter. 2020. What do you mean,\nBERT? In Proceedings of the Society for Computa-\ntion in Linguistics 2020, pages 279–290, New York,\nNew York. Association for Computational Linguis-\ntics.\nJiaqi Mu and Pramod Viswanath. 2018. All-but-the-\ntop: Simple and effective postprocessing for word\nrepresentations. In 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings. OpenReview.net.\nRoberto Navigli. 2009. Word sense disambiguation: A\nsurvey. ACM Computing Surveys, 41(2):1–69.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. arXiv preprint arXiv:1807.03748.\nPaolo Pedinotti and Alessandro Lenci. 2020. Don’t in-\nvite BERT to drink a bottle: Modeling the interpreta-\ntion of metonymies using BERT and distributional\nrepresentations. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics ,\npages 6831–6837, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nMohammad Taher Pilehvar and Jose Camacho-\nCollados. 2019. WiC: the word-in-context dataset\nfor evaluating context-sensitive meaning represen-\ntations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 1267–1273, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAlessandro Raganato, José Camacho-Collados, and\nRoberto Navigli. 2017. Word sense disambiguation:\nA uniﬁed evaluation framework and empirical com-\nparison. In Proceedings of EACL 2017 , pages 99–\n110.\nAlessandro Raganato, Tommaso Pasini, Jose Camacho-\nCollados, and Mohammad Taher Pilehvar. 2020.\nXL-WiC: A multilingual benchmark for evaluating\nsemantic contextualization. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 7193–7206,\nOnline. Association for Computational Linguistics.\nSara Rajaee and Mohammad Taher Pilehvar. 2021. A\ncluster-based approach for improving isotropy in\ncontextual embedding space. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers) , pages 575–584, Online. As-\nsociation for Computational Linguistics.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B.\nViégas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nBERT. In Advances in Neural Information Process-\ning Systems 32: Annual Conference on Neural Infor-\nmation Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada , pages\n8592–8600.\nNils Reimers and Iryna Gurevych. 2019a. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for\nComputational Linguistics.\nNils Reimers and Iryna Gurevych. 2019b. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for\nComputational Linguistics.\nSebastian Ruder. 2021. Recent advances in lan-\nguage model ﬁne-tuning. http://ruder.io/\nrecent-advances-lm-fine-tuning .\n573\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020. Probing\npretrained language models for lexical semantics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7222–7240, Online. Association for Computa-\ntional Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 3261–3275.\nGregor Wiedemann, Steffen Remus, Avi Chawla,\nand Chris Biemann. 2019. Does bert make any\nsense? interpretable word sense disambiguation\nwith contextualized embeddings. arXiv preprint\narXiv:1909.10430.\nYige Xu, Xipeng Qiu, Ligao Zhou, and Xuanjing\nHuang. 2020. Improving bert ﬁne-tuning via\nself-ensemble and self-distillation. arXiv preprint\narXiv:2002.10345.\nYuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng\nZhang, Wei Wu, and Weiran Xu. 2021. Con-\nsert: A contrastive framework for self-supervised\nsentence representation transfer. arXiv preprint\narXiv:2105.11741.\nYan Zhang, Ruidan He, Zuozhu Liu, Lidong Bing, and\nHaizhou Li. 2021. Bootstrapped unsupervised sen-\ntence representation learning. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 5168–5180, Online. As-\nsociation for Computational Linguistics.\nA Appendix\nA.1 AUC Score Tables for Binary\nClassiﬁcation Tasks\nmodel ↓, dataset → WiC TSV-1 TSV-2 TSV-3\nSentence-BERT 64.20 63.99 61.81 66.01\nM IRROR B ERT 67.31 70.53 68.00 70.28\nBERT 71.61 62.06 59.48 61.45\n+ M IRROR W I C 74.89 72.10 67.92 73.03\nDeBERTa 70.58 62.11 60.51 63.25\n+ M IRROR W I C 76.70 75.44 71.24 75.64\nTable 10: AUC results for English tasks.\nFollowing prior work, we reported accuracy\nin the main text. However, the threshold for\nTRUE/FALSE classiﬁcation needs to be tuned on\ndev set. We thus report the AUC scores in Tabs. 10\nand 11 which does not require tuning of any hyper-\nparameter. The AUC scores demonstrate the same\ntrend as accuracy scores.\nA.2 Pretrained Encoders Details\nFor a full listing of HuggingFace model links and\nnumber of parameters for each model, see Table 12.\nA.3 Hyperparameter Optimisation\nTable 13 shows a full listing of the hyperparame-\nters (and their search space). As said in main text,\nhyperparameters remain as the same as set in prior\nwork of Liu et al. (2021b), except for random span\nmasking rate and dropout rate.\nA.4 Sensitivity to Training Corpora\nTo test the robustness of the model to different\ncorpora, we individually sampled ﬁve sets of 10k\nraw sentences and found only minor difference\nwhen ﬁne-tuning on them (≈0.003 standard devi-\nation for BERT + MIRROR WIC and ≈0.001 for\nDeBERTa +MIRROR WIC). We also tested with\nﬁne-tuning with strictly ‘in-domain’ data, i.e., raw\nsentences (w/o labels) sampled from the training\nsets of WiC tasks, but found no substantial differ-\nence when comparing to ﬁne-tuning on Wikipedia\ntexts.\nA.5 Software and Hardware Dependencies\nOur experiments are implemented with PyTorch\nand Huggingface Transformers. For PyTorch train-\ning, Automatic Mixed Precision (AMP)9 is turned\non. The hardware conﬁguration is listed in Table 14.\nMIRROR WIC training on this machine takes ≈30\nseconds.\n9https://pytorch.org/docs/stable/amp.\nhtml\n574\nlevel→ XL-WiC AM2iCo\nmodel↓, language→ ZH * KO * HR ET ZH KA JA AR\nBERT 80.97 75.17 67.79 62.72 68.06 64.50 69.32 68.98\n+ MIRROR WIC 83.39 80.44 76.80 64.62 69.23 65.57 72.86 69.27\nTable 11: AUC results for multilingual and cross-lingual tasks.\nmodel #param URL\nBERT 110M https://huggingface.co/bert-base-uncased\nRoBERTa 110M https://huggingface.co/roberta-base\nDeBERTa 138M https://huggingface.co/microsoft/deberta-base\nmBERT 168M https://huggingface.co/bert-base-multilingual-uncased\nBERT (ZH) 103M https://huggingface.co/bert-base-chinese\nBERT (KO) 118M https://huggingface.co/kykim/bert-kor-base\nTable 12: A listing of HuggingFace URLs of all pretrained models used in this work.\nhyperparameters search space\nlearning rate { 1e-5, 2e-5∗,3e-5}\nbatch size 200\ntraining epochs {1 ∗, 2, 3, 4}\ntraining data size {5k, 10k ∗, 20k, 30k, 40k, 50k}\nmax_seq_length of tokeniser 50\nτ in Eq. (1) {0.02, 0.03, 0.04 ∗, 0.05, 0.06}\nrandom span masking rate (BERT) {0, 1, 5, 10 ∗, 15}\nrandom span masking rate (RoBERTa) {0 ∗, 1, 5, 10 15}\nrandom span masking rate (DeBERTa) {0, 1 ∗, 5, 10, 15}\ndropout rate (BERT) {0.1, 0.2, 0.3, 0.4 ∗, 0.5, 0.6}\ndropout rate (RoBERTa) {0.1, 0.2, 0.3 ∗, 0.4, 0.5, 0.6}\ndropout rate (DeBERTa) {0.1, 0.2, 0.3 ∗, 0.4, 0.5, 0.6}\nTable 13: Hyperparameters along with their search grid. ∗marks the values used to obtain the reported results.\nThe hparams without any deﬁned search grid are adopted directly from Liu et al. (2021a).\nhardware speciﬁcation\nRAM 128 GB\nCPU AMD Ryzen 9 3900x 12-core processor ×24\nGPU NVIDIA GeForce RTX 2080 Ti (11 GB) ×2\nTable 14: Hardware speciﬁcations of the used machine.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8609175682067871
    },
    {
      "name": "Word (group theory)",
      "score": 0.7756351828575134
    },
    {
      "name": "Natural language processing",
      "score": 0.7371487021446228
    },
    {
      "name": "Artificial intelligence",
      "score": 0.707555890083313
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6418354511260986
    },
    {
      "name": "Task (project management)",
      "score": 0.6250195503234863
    },
    {
      "name": "Sentence",
      "score": 0.6168136596679688
    },
    {
      "name": "Language model",
      "score": 0.5854076147079468
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5307837724685669
    },
    {
      "name": "Encoder",
      "score": 0.5212146043777466
    },
    {
      "name": "Speech recognition",
      "score": 0.3609682619571686
    },
    {
      "name": "Machine learning",
      "score": 0.33100569248199463
    },
    {
      "name": "Linguistics",
      "score": 0.09697389602661133
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210107233",
      "name": "Language Science (South Korea)",
      "country": "KR"
    }
  ],
  "cited_by": 9
}