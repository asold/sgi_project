{
    "title": "Intelligent Practices of Large Language Models in Digital Government Services",
    "url": "https://openalex.org/W4390590960",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2103606203",
            "name": "Jiawei Han",
            "affiliations": [
                "Changchun University"
            ]
        },
        {
            "id": "https://openalex.org/A2226156436",
            "name": "Jiankang Lu",
            "affiliations": [
                "Changchun University"
            ]
        },
        {
            "id": "https://openalex.org/A2096023097",
            "name": "Ying Xu",
            "affiliations": [
                "Changchun University"
            ]
        },
        {
            "id": "https://openalex.org/A2122615324",
            "name": "Jin You",
            "affiliations": [
                "Changchun University"
            ]
        },
        {
            "id": "https://openalex.org/A2098648589",
            "name": "Bingxin Wu",
            "affiliations": [
                "Changchun University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6810081322",
        "https://openalex.org/W6850625674",
        "https://openalex.org/W6854866820",
        "https://openalex.org/W4285294723",
        "https://openalex.org/W6851513886",
        "https://openalex.org/W6850936240",
        "https://openalex.org/W4322757547",
        "https://openalex.org/W4327564965",
        "https://openalex.org/W6802386650",
        "https://openalex.org/W6849504698",
        "https://openalex.org/W6850359928",
        "https://openalex.org/W6850071225",
        "https://openalex.org/W3166846774",
        "https://openalex.org/W6810738896",
        "https://openalex.org/W6851775633",
        "https://openalex.org/W6853292724",
        "https://openalex.org/W6851565042",
        "https://openalex.org/W6852178287",
        "https://openalex.org/W4381930847",
        "https://openalex.org/W6850820320",
        "https://openalex.org/W4382998379",
        "https://openalex.org/W6854084413",
        "https://openalex.org/W6853444283",
        "https://openalex.org/W6849898756",
        "https://openalex.org/W6796581206",
        "https://openalex.org/W6853251322",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W6853430451",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W2998702515",
        "https://openalex.org/W6761205521",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W6682631176",
        "https://openalex.org/W4321177655",
        "https://openalex.org/W4401042773",
        "https://openalex.org/W4361866125",
        "https://openalex.org/W4323717348",
        "https://openalex.org/W4379138252",
        "https://openalex.org/W4322718421",
        "https://openalex.org/W4378509449",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4361866031",
        "https://openalex.org/W3205270560",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4362598952",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4382618722",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W4378474365",
        "https://openalex.org/W4366198844"
    ],
    "abstract": "Large language models have been widely used in open-domain tasks with significant results, as well as being able to perform zero-sample closed-ended questions based on internal knowledge stored in the parameters during pre-training to answer the task. However, this internalized knowledge may be insufficient or the knowledge may be outdated in responding to government service consultation scenarios, which may result in the inability of the large language models to perform accurate and rigorous answers and provide effective assistance. This issue has attracted widespread attention, but there is a lack of datasets for relevant research. Therefore, in this paper, we take Beijing as an example to collect all kinds of common service counseling questions from its government website and the corresponding official answers as a dataset, which contains the daily counseling questions encountered by the citizens, including common questions about medical insurance, social insurance, provident fund flexible employment, and government-private interaction. Therefore, this paper designs a domain-specific language model (GCALLM) for government service consultation based on this scenario. By fine-tuning the large language models for knowledge injection, the fine-tuned model helps the large language models improve their performance in governmental service consulting scenarios by providing contextual information. And it solves the problem of not being able to answer precisely, allowing for more rigor and accuracy of the answers. In addition, the response information is answered in seven major national languages to improve the construction of digital government consulting services. A large number of experiments have proved that the model can produce accurate responses in this scenario in the field of governmental counseling.",
    "full_text": "Digital Object Identifier\nIntelligent Practices of Large Language Models in\nDigital Government Services\nJiawei Han1,2, Jian kang Lu1, Ying Xu3, Jin You1,and Bingxin Wu1\n1 College of Cyber Security, Changchun University, Changchun 130022, China\n2 Digital Identity and Blockchain Joint Laboratory of Peking University, Beijing 100871, China\n3 Administration of Changchun University, Changchun 130022,China\nCorresponding author: Jiawei Han (e-mail: hanjw78@ccu.edu.cn).\nABSTRACT Large language models have been widely used in open-domain tasks with significant results, as\nwell as being able to perform zero-sample closed-ended questions based on internal knowledge stored in the\nparameters during pre-training to answer the task. However, this internalized knowledge may be insufficient\nor the knowledge may be outdated in responding to government service consultation scenarios, which may\nresult in the inability of the large language models to perform accurate and rigorous answers and provide\neffective assistance. This issue has attracted widespread attention, but there is a lack of datasets for relevant\nresearch. Therefore, in this paper, we take Beijing as an example to collect all kinds of common service\ncounseling questions from its government website and the corresponding official answers as a dataset,\nwhich contains the daily counseling questions encountered by the citizens, including common questions\nabout medical insurance, social insurance, provident fund flexible employment, and government-private\ninteraction. Therefore, this paper designs a domain-specific language model (GCALLM) for government\nservice consultation based on this scenario. By fine-tuning the large language models for knowledge\ninjection, the fine-tuned model helps the large language models improve their performance in governmental\nservice consulting scenarios by providing contextual information. And it solves the problem of not being\nable to answer precisely, allowing for more rigor and accuracy of the answers. In addition, the response\ninformation is answered in seven major national languages to improve the construction of digital government\nconsulting services. A large number of experiments have proved that the model can produce accurate\nresponses in this scenario in the field of governmental counseling.\nINDEX TERMS digital government,governmental counseling,GCALLM,Large language models, effective\nassistance\nI. INTRODUCTION\nG\nPT [1] , Google’s PaLM [2], and other benchmark\nmodels [3] [4] [5] [6] [7] are prominent in the field\nof artificial intelligence. The pretraining on large amounts\nof data enables these models to have exceptional language\nunderstanding and generation capabilities [8]. When it comes\nto domain-specific problems, large language models exhibit\nlimited performance due to their insufficient pre-training\non domain knowledge, and the overwhelming presence of\ndomain-generalized data causes them to prioritize public\nknowledge, leading to potential oversight of critical domain-\nspecific information [8] [9] [10]. However, the scale and cost\nof training large language models from scratch are very costly\nfor most companies and researchers. Fine-tuning existing\nmodels based on domain-specific data is another option, and\nseveral studies have shown efficient strategies for achieving\nthis step, among them P-tuning v2 [11] which rivals full\nincremental fine-tuning at any scale for question-answering\ntasks,not only does it reduce the number of parameters that\nneed to be fine-tuned to 0.1 % of the original, but by using\nmethods such as model quantization and Gradient Check-\npoint, it is possible to improve training efficiency while re-\nducing the amount of memory and computational resources\nconsumed by the model when in use.These methods make\nthe model more lightweight by optimizing and compressing\nit to reduce the demand for resources. At the same time,\nthese methods have good performance preservation, i.e., they\nreduce resource consumption while keeping the performance\nof the model unaffected significantly.These methods make the\nmodel more lightweight by optimizing and compressing it to\nreduce the demand for resources. These methods are effective\nin maintaining performance, reducing resource consumption,\nand maintaining the model’s unaffected performance.\nAlong with the rise of the large language models and\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349969\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJ.Han et al.: Intelligent Practices of Large Language Models in Digital Government Services\nthe effective application of various fine-tuning technologies,\nthe government consulting service should also usher in new\ndevelopment opportunities. Consulting service is one of the\nimportant works of the government office, and the excellent\ndialogue ability of the large language models can accelerate\nthe development of intelligent consulting services. At present,\nthe intelligence level of China’s consulting service robots is\ninsufficient, it can only output the original text matching the\npreset answers, and it can’t make personalized answers to the\nuser’s questions. The text-to-text (T2TT) translation function\nwith the help of the large language models and SeamlessM4T\n[12] can solve the demand for intelligent counseling to help\nthe public more easily access, understand, and master the\ngovernment’s processes and work, avoiding problems such\nas no door to do business, no one to consult, and ineffective\ncommunication. Thus, the service is continuously provided\naround the clock. On the other hand, with the development of\nsmart government, more and more government departments\nwill begin to gradually utilize the large language models to\nimprove the quality of consulting services, which has some\nlimitations in providing accurate answers due to the relatively\ninsufficient knowledge base of the large language models in\nthe field of governmental consulting at present. This means\nthat the model may not be able to give complete and accurate\nanswers to questions involving the domain of governmental\nconsulting.Therefore, this paper proposes a scenario-specific\ndomain model to provide accurate policy information services\nfor people, addressing the long-standing problems of diffi-\nculty in use and convergence without wisdom. This is bound\nto have a positive impact on socio-economic development.\nExisting work, on the one hand, focuses on extract-\ning domain-specific knowledge through retrieval-based ap-\nproaches to enhance the performance of large language mod-\nels in specific domains [13] [14] or external modules [15]\n[16]. On the other hand, text embeddings are employed to\nretrieve potentially relevant text summaries from a corpus of\ndomain-specific documents. The text is transformed into real-\nvalued vectors encoding meanings. Identification of the text\ninvolves vectorizing the query question with all text blocks\nstored in the vector repository, pre-computed using cosine\nsimilarity between the query vectors and each block. A small\nportion of the most relevant data blocks is then appended to\nthe user query, constructing effective prompts that are sent to\nthe significant language model, resulting in a response [17].\nHowever, these methods have some limitations. Therefore, to\naddress these writing problems, this paper proposes to fine-\ntune a large language model using P-tuning v2 [11], i.e.,\nChatGLM [6], which fine-tunes a large language model using\nthe domain documents of a government consulting service to\nconstruct prompts. This fine-tuned large language model pro-\nvides knowledge specific to government service consulting\nservices at runtime. It makes it easier to maintain and protect\nprivacy within the domain of government service consulting.\nThe contribution of this article is to collect common consul-\ntation Q&A information from government services in Beijing\nas a dataset and as a data source for fine-tuning the large\nlanguage models. The fine-tuned model based on the P-tuning\nv2 [11] technique of injecting domain knowledge is used to\nprovide good contextual information for the large language\nmodels, which is combined with the large language models\nto generate knowledge in the domain of government service\ncounseling, and a large number of experiments have proved\nthat the method of this paper is superior to the embedded\ntext-based method in the domain of government service coun-\nseling. To better build the government service consulting\nservice of digital government, the responsive answers express\nprecise information using seven major languages in the world,\nwhich are Chinese, French, Spanish, Portuguese, German,\nand Japanese. New ideas and methods are provided for dig-\nital government reference counseling services to help digital\ngovernments better adapt to the information age changes and\nprovide more innovative and valuable services.\nII. RELATED WORK\nThe training of large language models is usually divided into\ntwo phases: pre-training and fine-tuning. In the pre-training\nphase, the model is trained using a large amount of unlabeled\ntext data to learn the basic laws and patterns of language\npairs, and the large language models thus acquire basic lan-\nguage comprehension and generation capabilities. In the fine-\ntuning phase, the model’s broad language comprehension is\nmade more adaptable to specific tasks and better responds to\ndomain-specific nuances, thus improving the model’s ability\nto generalize to unknown tasks [18] [19]. However domain-\nspecific tasks often involve complex concepts, technical ter-\nminology, and complex relationships between entities [20]. In\nthe absence of targeted guidance, large language models can\nbe seriously illusory.\nRecently, many relevant large language models have\nemerged in the medical [21] [22] [23], financial [24] [25],\nand legal domains [26] [27]. Efforts have been made to im-\nprove the context-generation capabilities of large language\nmodels in specific domains using a combination of exter-\nnal knowledge [28]. Among them are external modules to\nimprove the context generation capability of large language\nmodels, such as TaskMatrix [7] and AutoGPT [16]. These\napproaches are highly dependent on the prompt management\nof the large language models as well as the availability of\nexternal tools. However, for domain-specific scenarios, these\nexternal modules are not always effective. Alternatively, the\nuse of chatbots [17], enables the use of large language models\nfor specialized domains without updating the parameters. It\nis also possible to inject domain knowledge into the model\nusing updating parameters using fine-tuning, and there have\nbeen some notable advances in fine-tuning techniques [11]\n[29] [30] [31] [32]. Among them, P-tuning v2 [11] is an\noptimization and adaptation implementation of Deep Prompt\nTuning [17] [33]. One of its final improvements is to apply\ncontinuous prompts to each layer of the pre-trained model, not\njust the input layer. By increasing the capacity of continuous\nprompts, and for various settings, P-tuning v2 [11] improves\nperformance comparable to full fine-tuning, and only needs\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349969\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJ.Han et al.: Intelligent Practices of Large Language Models in Digital Government Services\nto fine-tune 0.1 % ˜ 3 % of the parameters. This paper mainly\nuses the latter in the field of government service consulting.\nAt different training stages, the training methods for large\nlanguage models can be broadly categorized as follows: one\nis to train from scratch using domain data, Bloomberggpt [24]\nrelies on a large amount of domain data, and the training\ncost is relatively expensive; one is to fine-tune based on\nthe domain instruction data, e.g., ChatLaw [26] and Lawyer\nLLaMA [27]; and another method is to train the domain on\nthe basic large language models based on the domain data,\nand then fine-tune with the instructions [34].\nIII. METHOD\nA. LM PROMOPTING\nLanguage models are pre-trained by predicting the next to-\nkens based on the previous tokens, known as autoregressive\nlanguage modeling [35] [36], and training by this method\nenables zero-sample instruction learning. Specifically, a ques-\ntion and an instruction message are provided to the language\nmodel, and the model generates a corresponding answer\nbased on the input text message. More specifically, each input\nmessage is first modified into a text string X called a prompt,\nusing a template T for a particular instruction. as follows\nT : x → X. There is a question x =\" The term of the collective\ncontract ?\", and command templates T: \" Please answer this\nquestion:{x} \", Then the resulting hint will be T (x) =\"Please\nanswer this question: the term of the collective contract ? \",\nThe prompt is then forwarded to the large language models,\nthrough which the answer is generated. There are several\nchallenges with this approach; the large language models rely\ntoo much on the knowledge of the parameters, and the lack\nof knowledge in the governmental domain tends to produce\nfactually incorrect information.\nB. GCALLM\nTo address the limitations of the current prompting scheme\nwith language models, this paper combines the relevant do-\nmain knowledge in the fine-tuned model with the question\nand then forwards it to the large language models to generate\nthe answer through the large language models. While giving\nfull play to the advantages of the large language models,\nand standardizing its answer generation to ensure that it is\nsufficiently complete and accurate, this paper adopts a fine-\ntuned large language model combined with another large\nlanguage model. The model interaction (shown in FIGURE\n1) involves two key steps: obtaining a fine-tuned domain-\nspecific model of the large language models and providing the\ngenerated domain-specific knowledge to the large language\nmodels.\nIn the first step, this paper uses the Q &A knowledge\ncaptured in the government service consultation documents\nto form QA Q &A pairs for domain fine-tuning of the large\nlanguage models. The government service consultation doc-\numents are used as a knowledge base that contains a variety of\ncommon consultation questions. By utilizing this knowledge\nbase, domain-specific knowledge is infused into the large\nlanguage models through fine-tuning using P-tuning v2 [11].\nTo facilitate model fine-tuning, prompts are constructed using\nthe collected dataset of common government service inquiries\nand each prompt consists of a question and an answer. For\nexample:\n{\"content”: ”Can 40-year-old female comrades handle flexi-\nble employment ?\", \"summary\":\" Can handle.\"}\n{\"content\":\" Flexible Employee Social Security Contribu-\ntions for the month of the increase in what time to deduct\nthe fee ?\", \"summary\":\" The next monthly deduction for the\nprocedure of adding members in the same month .\"}\nIn the second step, the fine-tuned model, which has been\ninjected with domain knowledge, provides domain-specific\nQ&A knowledge of the large language models when the\nuser accesses the fine-tuned model D. To follow specifically,\nfirst, using a command-specific template T ′, each input is\nmodified into a text string called a prompt X′, as shown in\nthe following T ′ : (x, D) → X′. For example, there is a\nquestion x = \"Duration of the collective contract ?\", Fine-\ntuning model tip messages D=\" Collective contracts generally\nhave a duration of 3 years, and instruction templates definite\ninformation {D}. Based on the above-known information, use\nthe known information for output, The question is:{x}\".Then\nthe hint X′ is obtained as =\" Known information: The term of\na collective contract is usually 3 years\". Based on the above-\nknown information, use the known information to produce\nan output that cannot be modified or added to. The output\nshould be in Chinese. The question is: What is the duration\nof a collective contract ? \". The prompt is then forwarded\nto the large language models, through which the answer is\ngenerated. This knowledge enrichment enhances the under-\nstanding of the task context by the large language models,\nenabling them to generate more accurate and contextually\nrelevant responses. The obtained response message is passed\nto the translation module. The module is a large-scale multi-\nlingual T2TT model (SeamlessM4T-NLLB [12]), which can\nunderstand texts in nearly 100 languages and generate corre-\nsponding translated texts, as well as consists of a Transformer\nText Encoder and a Transformer Text Decoder, which encode\nthe response message through the text encoder, and decode\nthe encoded message through the decoder to translate the\nresponse message into other major national languages.\nIV. EXPERIMENTS\nA. DATASET\nIn order to solve the use of a governmental consulting sce-\nnario dataset, this paper takes Beijing as an example, and\ncollects the common questions of its governmental service\nconsulting and the corresponding official standard answer as\na dataset, which includes the frequently asked questions of\nmedical insurance and the content of convenient services and\nother information.\n1) Data Collection\nMedical insurance, social insurance, housing provident fund,\nand flexible employment are common and popular keywords\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349969\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJ.Han et al.: Intelligent Practices of Large Language Models in Digital Government Services\nFIGURE 1. GCALLM model framework.\nin government consulting services. The corresponding ques-\ntions are common consultation questions from citizens, as\nwell as some help and suggestions. Citizens can find rele-\nvant questions or provide suggestions on the official website\nof Beijing City, and the relevant departments will respond\npromptly. Below are three brief examples of Q &A responses.\nQ: How long do you need to be in flexible employment to\nbe eligible for a flexible employment subsidy again?\nA: Complete 90 days.\nQ: Can you switch to a CPF personal housing loan if you\nhave already taken out a commercial loan?\nA: Currently, you cannot transfer a commercial loan to\na Housing Provident Fund (HPF) personal housing loan in\nBeijing.\nQ: Can insured persons use their personal accounts at\ndesignated retail pharmacies?\nA: It can be used by insured persons to pay for their\npersonal expenses on drugs, medical devices, and medical\nconsumables at designated retail pharmacies.\nIn this paper, we will focus on answering questions with short\nword counts and extracting the answers corresponding to the\nofficial departmental responses.\n2) Data Filtering and Post-processing\nAs the data comes from online websites, the content is com-\nplex and contains a large amount of irrelevant content, which\nmakes it difficult to conduct research. To address this prob-\nlem, this paper deeply samples the collected data, summarizes\nthe problems, determines the patterns, and designs the rules\nfor data filtering. (1) Deletes the name of the matter, the\nsubject of implementation, which are about the direction of\nthe content of the question, as well as the official unit that\nanswers the question. (2) Normalize all connections in the\ndata. (3) Remove some answer contents that are irrelevant to\nthe question, such as:\nTABLE 1. Government Data Analysis\nQuestion Category Number\nMedical insurance Issues related to medical insurance. 207\nHumanities Beijing Various literary and artistic activities\nin Beijing.\n100\nVarious consultations Various problems of government-\ncitizen interaction.\n197\nSocial insurance Human resources and social security\nissues.\n257\nSurplus Housing provident fund issues. 61\nFlexible employment Employment subsidy issues. 56\nBus opinions Opinions to the bus company. 122\n“Thank you for your interest in the transit industry and we\nwelcome you to continue to monitor our work.”\n(4) Detecting the length of questions that are too long and\nreplacing them with questions that are displayed briefly on\nthe website, without using detailed question information. (5)\nDelete some Q &A information whose answers are too long.\n(6) Delete the name information encrypted by the sender and\nthe time of the consultation. By implementing these rules\nfor data filtering, the quality and usability of the dataset are\nimproved.\n3) Statistics\nThe dataset contains 1k Q &A pairs, which are common\ngovernment service inquiry questions, in which the types of\nquestions are categorized into 7 categories, and the statistical\ninformation about the specific dataset is shown in TABLE\n1. In the dataset, each question corresponds to a different\ncategory, and good categorization information makes it easier\nfor users to understand the specific question information.\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349969\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJ.Han et al.: Intelligent Practices of Large Language Models in Digital Government Services\nB. EXPERIMENTS SETUPS\nThe experiment takes Beijing as an example, collects a dataset\nof common questions and corresponding answers for govern-\nment service consulting. This paper follows the evaluation\nprocess of the NLG task and uses a set of metrics to compre-\nhensively evaluate the quality of responses generated by large\nlanguage models given a question, the Chinese responses of\nthe evaluated responses are compared with the standard Chi-\nnese responses. In this experiment, this paper evaluates large\nlanguage models a fine-tuned model (DLLM), a knowledge\nbase combined with a fine-tuned model (KBDLLM), and a\nGCALLM, and uses seven metrics to comprehensively eval-\nuate the response quality of the large language models. We\nevaluated a large language model, ChatGLM [6]. It generates\nanswers from input questions without introducing external\ndomain knowledge.\n1) GCALLM\nGCALLM uses the P-tuning v2 [11] method to fine-tune\nChatGLM [6] on the dataset collected in this paper to obtain a\nfine-tuned model of government service domain knowledge.\nGiven a question, the domain information is first collected\nthrough the fine-tuned model, and then this domain informa-\ntion is assembled in conjunction with the question using a\nspecific prompt template and then sent to the large language\nmodel to answer the question.\n2) KBDLLM\nFirst, the sentence vector of consulted government af-\nfairs information is obtained. This paper uses the text2vec-\nlargechinese model from the Sentence Transformers database\nto obtain the sentence vector.After acquiring the sentence vec-\ntor in the knowledge base, FAISS [37] is employed to conduct\na similarity search, FAISS [37] uses the inverted indexing\napproach to accelerate the approximation vector search, the\ntop_k answers obtained, using the appropriate chunk_size to\ncomplement the contextual information of these answers, the\nanswers sorted and assembled into contextual information\nand user input questions using langchain’s prompt_template\nfunction to generate the large language models input required\nprompt prompts to guide the fine-tuning of the large language\nmodels from the answer. The large language models for fine-\ntuning are guided to understand and analyze the answers from\nthe references and generate accurate and complete answers,\nand at the same time, it is emphasized that it is forbidden to\ngenerate answers by itself when it cannot come up with an\nanswer and reasonable hints are given.\n3) DLLM\nOn the dataset collected in this article, the P-tuning v2 [11]\nmethod was used to fine-tune ChatGLM [6] to obtain a\nfine-tuning model for knowledge in the field of government\nservices. Directly use the question to access the fine-tuning\nmodel answers.\nTABLE 2. Result Analysis\nLLM DLLM KBDLLM GCALLM\nBLEU 0.1154 0.1794 0.1361 0.305\nROUGE-1 0.1954 0.3536 0.3013 0.4862\nROUGE-2 0.0460 0.1730 0.1428 0.3407\nROUGE-L 0.1421 0.2921 0.2459 0.4157\nBERTScore-P 0.5834 0.6975 0.6825 0.7688\nBERTScore-R 0.6564 0.7143 0.6639 0.7576\nBERTScore-F1 0.6145 0.7029 0.6688 0.7564\nChatGLM results on government data, bolded numbers represent the best\nresults, and the DLLM column shows the evaluation results obtained by\ndirectly querying the fine-tuned model. The LLM shows the evaluation\nresults obtained by querying the LLM without applying any enhancement,\nand the rest of the two columns are a combination of the evaluation results\nunder different enhancement methods.\nC. METRICS\nAs the large language models generate content with intelli-\ngent and random characteristics, this paper adopts the evalu-\nation index Bert Score [38] based on the language model for\nevaluation, which uses contextual embedding to calculate the\nsimilarity of the markers, and extracts the features through\nthe Bert model by transforming the generating text and the\nreference text into tokens, respectively, and then calculates\nthe inner product corresponding to each word in the two\ntexts, to construct a similarity matrix, based on which the\nmaximum similarity scores of the two texts are calculated\nand normalized, and finally the Precision, Recall and F1\nvalues are obtained. BLEU [39], ROUGE-1, ROUGE-2, and\nROUGE-L [40] are also used to measure the degree of agree-\nment between the responses generated by the large language\nmodels and the standard answers.\nD. EXPERIMENTAL ENVIRONMENTS SETTINGS\nOur experimental environment uses the deep learning frame-\nwork of Pytorch version 2.0.1 The NVIDIA graphics card\ndriver version is Cuda 12.0, the graphics card model used is\nRTX 3090 * 1, and the operating system version is Ubuntu\n20.04.\nV. RESULT\nTABLE 2 gives the evaluation results of ChatGLM [18] on\nseven metrics of the governmental dataset, in general, after\nfine-tuning the model’s enhancement. Some metrics such\nas BLEU, ROUGE-1, ROUGE-2, ROUGE-L, BERTScore-\nP, BERTScore-R, BERTScore-F1.From Table 2, using only\nlarge language models for answering government consulta-\ntion questions yields suboptimal results. The BERTScore-F1,\nBERTScore-P, and BERTScore-R values are 0.6145, 0.5834,\nand 0.6564, respectively. The BLEU score is 0.1154, in-\ndicating a low precision match between generated answers\nand standard answers. The ROUGE-1 value is 0.1954, sug-\ngesting a low overlap at the individual word level. The\nROUGE-2 value is 0.0460, indicating a low overlap at the\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349969\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJ.Han et al.: Intelligent Practices of Large Language Models in Digital Government Services\nTABLE 3. Examples of government questions and answers\nSubject: Composed model\nQuery: Can I switch to a CPF personal\nhousing loan if I have already\ntaken out a commercial loan?\nStandard Answer: Currently, you cannot transfer a\ncommercial loan to a Housing\nProvident Fund (HPF) personal\nhousing loan in Beijing.\nDLLM: you cannot transfer a commercial\nloan to a Housing Provident Fund\n(HPF) personal housing loan in\nBeijing.\nLLM: In general, commercial loans and\nhousing fund personal residence...\nGCALLM(Chinese): Currently, you cannot transfer a\ncommercial loan to a CPF per-\nsonal housing loan in Beijing.\nGCALLM(French): Actuellement impossible\nd’effectuer prêts commerciaux\ntransfert logement fonds commun\nde réserve opérations de prêts\nindividuels au logement.\nGCALLM(German): Derzeit nicht in Peking\nabzuwickeln kommerzielle\nKredite um Wohnungs\nKompensationsfonds persönliche\nWohnkredite Geschäft .\nGCALLM(Japenese): 在北京で手きできない商融住\n宅公积金人住宅融事.\nGCALLM(Portuguese): Actualmente não se pode fazer em\nPequim empréstimos comerciais\ntransfer habitacional fundos co-\nmuns de empréstimos individuais\nhabitacionais .\nGCALLM(Spanish): Actualmente no se puede trami-\ntar en Beijing préstamos comer-\nciales trans vivienda común de de-\npósito personal negocio de présta-\nmos vivienda .\nKBDLLM: The result is not permissible.\ntwo consecutive word levels, and the ROUGE-L value is\n0.1412, implying a low long-sequence matching and seman-\ntic relevance.Comparatively, DLLM shows improvements\nover LLM in BLEU, ROUGE-1, ROUGE-2, ROUGE-L,\nBERTScore-F1, BERTScore-P, and BERTScore-R. It en-\nhances precision matching, single-word overlap, consecu-\ntive two-word overlap, and semantic relevance in answer-\ning government consultation questions.KBDLLM, compared\nto LLM, enhances precision matching, single-word overlap,\nconsecutive two-word overlap, and semantic relevance. How-\never, compared to DLLM, it exhibits a decrease in preci-\nsion matching, single-word overlap, consecutive two-word\noverlap, and semantic relevance. Despite attempts to miti-\ngate over-interpretation through prompts, KBDLLM’s per-\nformance is inferior to DLLM.GCALLM surpasses LLM,\nDLLM, and KBDLLM in BLEU, ROUGE-1, ROUGE-2,\nROUGE-L, BERTScore-F1, BERTScore-P, and BERTScore-\nR, achieving the highest numerical values. This results in\noptimal precision matching, single-word overlap, consecu-\ntive two-word overlap, and semantic relevance in generating\nanswers compared to standard answers. Its outstanding per-\nformance in answering consultation questions is attributed\nto specific instruction templates and the incorporation of\ndomain knowledge. GCALLM ensures the rigor and accuracy\nof answers, enhancing consultation services’ intelligence, in-\nteractivity, and user-friendliness. The SeamlessM4T-NLLB\nmodule generates answers in seven different languages, accel-\nerating the development of government service consultations.\nVI. CASE STUDY\nThe responses with ChatGLM [6] before and after the ap-\nproach combining the three models are given in TABLE\n3, due to too much information in the word count, only\nsome of the key information is given and ellipses are used\nto replace the rest of the content, it can be observed that\nthe large language models causes a wrong understanding of\nthe question and fails to provide the correct answer with-\nout any knowledge prompts. The knowledge-based approach\ncombined with fine-tuned modeling causes the fine-tuned\nmodel to generate brief answers due to too much informa-\ntion prompted by the knowledge base. After GCALLM can\nanswer the information completely, that is, the effectiveness\nof the fine-tuning model combined with the large language\nmodels are illustrated.\nVII. CONCLUSION AND FUTURE WORK\nThis paper explores the use of the fine-tuning model com-\nbined with the large language models and knowledge base\ncombined with the fine-tuning model, which can enhance the\nuser query in the large language models in the government\nservice consultation scenario. Due to the lack of relevant\ndatasets, this paper proposes to use the common government-\ncitizen interaction information on the website of the Beijing\nMunicipal People’s Government to construct a Q &A dataset,\nand then subsequently use a fine-tuning model combined\nwith large language models for the large language models\nto provide domain-specific knowledge. To evaluate the effec-\ntiveness of the model, seven evaluation methods are used to\nverify the effectiveness of the model, and a large number of\nexperiments prove that the domain-specific language model\ndesigned based on the scenarios of Beijing’s governmental\nconsulting service is effective.\nHowever, the GCALLM model still suffers from phantom\nflaws, and fine-tuning the model based on the sequence of\ndata, starting from low quality and progressing to high quality,\nshort samples preceding long samples, and easy tasks pre-\nceding difficult ones, remains an area for improvement. In\nthe future, the following steps of experimental exploration\nwill be planned, including the implementation of other fine-\ntuning techniques and reinforcement learning to enhance the\nperformance of GCALLM.\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349969\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJ.Han et al.: Intelligent Practices of Large Language Models in Digital Government Services\nREFERENCES\n[1] OpenAI, “GPT-4 Technical Report,” open-ai, vol. abs/2303.08774, 2023.\n[2] A. Chowdhery et al., “PaLM: Scaling Language Modeling with Pathways,”\nCoRR, vol. abs/2204.02311, 2022, doi: 10.48550/arXiv.2204.02311.\n[3] R. Taori et al., “Alpaca: A strong, replicable instruction-following model,”\nStanford Center for Research on Foundation Models. https://crfm. stanford.\nedu/2023/03/13/alpaca. html, vol. 3, no. 6, p. 7, 2023.\n[4] H. Touvron et al., “Llama: Open and efficient foundation language mod-\nels,” arXiv preprint arXiv:2302.13971, 2023.\n[5] H. Touvron et al., “Llama 2: Open Foundation and Fine-Tuned Chat Mod-\nels,” CoRR, vol. abs/2307.09288, 2023, doi: 10.48550/arXiv.2307.09288.\n[6] Z. Du et al., “GLM: General Language Model Pretraining with Autore-\ngressive Blank Infilling,” in Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (V olume 1: Long Papers),\nACL 2022, Dublin, Ireland, May 22-27, 2022, S. Muresan, P. Nakov, and\nA. Villavicencio, Eds., Association for Computational Linguistics, 2022,\npp. 320–335. doi: 10.18653/v1/2022.acl-long.26.\n[7] Y . Shen, K. Song, X. Tan, D. Li, W. Lu, and Y . Zhuang, “HuggingGPT:\nSolving AI Tasks with ChatGPT and its Friends in HuggingFace,” CoRR,\nvol. abs/2303.17580, 2023, doi: 10.48550/arXiv.2303.17580.\n[8] P. Lee, S. Bubeck, and J. Petro, “Benefits, limits, and risks of GPT-4 as an\nAI chatbot for medicine,” New England Journal of Medicine, vol. 388, no.\n13, pp. 1233–1239, 2023.\n[9] A. Lecler, L. Duron, and P. Soyer, “Revolutionizing radiology with GPT-\nbased models: Current applications, future possibilities and limitations of\nChatGPT,” Diagnostic and Interventional Imaging, vol. 104, no. 6, pp.\n269–274, 2023.\n[10] W.-L. Chiang et al., “Vicuna: An open-source chatbot impressing gpt-4\nwith 90 %* chatgpt quality,” See https://vicuna. lmsys. org (accessed 14\nApril 2023), 2023.\n[11] X. Liu, K. Ji, Y . Fu, Z. Du, Z. Yang, and J. Tang, “P-Tuning v2:\nPrompt Tuning Can Be Comparable to Fine-tuning Universally Across\nScales and Tasks,” CoRR, vol. abs/2110.07602, 2021, [Online]. Available:\nhttps://arxiv.org/abs/2110.07602\n[12] S. Communication et al., “SeamlessM4T-Massively Multilingual & Mul-\ntimodal Machine Translation,” CoRR, vol. abs/2308.11596, 2023, doi:\n10.48550/arXiv.2308.11596.\n[13] W. Shi et al., “REPLUG: Retrieval-Augmented Black-Box Language Mod-\nels,” CoRR, vol. abs/2301.12652, 2023, doi: 10.48550/arXiv.2301.12652.\n[14] B. Peng et al., “Check Your Facts and Try Again: Improving Large\nLanguage Models with External Knowledge and Automated Feedback,”\nCoRR, vol. abs/2302.12813, 2023, doi: 10.48550/arXiv.2302.12813.\n[15] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan, “Visual ChatGPT:\nTalking, Drawing and Editing with Visual Foundation Models,” CoRR, vol.\nabs/2303.04671, 2023, doi: 10.48550/arXiv.2303.04671.\n[16] S. Gravitas, “Auto-GPT: An Autonomous GPT-4 experiment, 2023,” URL\nhttps://github. com/Significant-Gravitas/Auto-GPT.\n[17] K. G. Yager, “Domain-specific ChatBots for Science using Embeddings,”\nCoRR, vol. abs/2306.10067, 2023, doi: 10.48550/arXiv.2306.10067.\n[18] L. Ouyang et al., “Training language models to follow instructions with\nhuman feedback,” Advances in Neural Information Processing Systems,\nvol. 35, pp. 27730–27744, 2022.\n[19] W. X. Zhao et al., “A Survey of Large Language Models,” CoRR, vol.\nabs/2303.18223, 2023, doi: 10.48550/arXiv.2303.18223.\n[20] X. ZHAO et al., “Domain Specialization as the Key to Make Large\nLanguage Models Disruptive: A Comprehensive Survey,” arXiv preprint\narXiv:2305.18703, 2023.\n[21] H. Xiong et al., “DoctorGLM: Fine-tuning your Chinese Doctor\nis not a Herculean Task,” CoRR, vol. abs/2304.01097, 2023, doi:\n10.48550/arXiv.2304.01097.\n[22] H. Wang et al., “HuaTuo: Tuning LLaMA Model with Chinese\nMedical Knowledge,” CoRR, vol. abs/2304.06975, 2023, doi:\n10.48550/arXiv.2304.06975.\n[23] Y . Li, Z. Li, K. Zhang, R. Dan, and Y . Zhang, “ChatDoctor: A Medical Chat\nModel Fine-tuned on LLaMA Model using Medical Domain Knowledge,”\nCoRR, vol. abs/2303.14070, 2023, doi: 10.48550/arXiv.2303.14070.\n[24] S. Wu et al., “BloombergGPT: A Large Language Model for Finance,”\nCoRR, vol. abs/2303.17564, 2023, doi: 10.48550/arXiv.2303.17564.\n[25] H. Yang, X.-Y . Liu, and C. D. Wang, “FinGPT: Open-Source Finan-\ncial Large Language Models,” CoRR, vol. abs/2306.06031, 2023, doi:\n10.48550/arXiv.2306.06031.\n[26] J. Cui, Z. Li, Y . Yan, B. Chen, and L. Yuan, “ChatLaw: Open-Source\nLegal Large Language Model with Integrated External Knowledge Bases,”\nCoRR, vol. abs/2306.16092, 2023, doi: 10.48550/arXiv.2306.16092.\n[27] Q. Huang et al., “Lawyer LLaMA Technical Report,” CoRR, vol.\nabs/2305.15062, 2023, doi: 10.48550/arXiv.2305.15062.\n[28] W. X. Zhao et al., “A Survey of Large Language Models,” CoRR, vol.\nabs/2303.18223, 2023, doi: 10.48550/arXiv.2303.18223.\n[29] E. J. Hu et al., “LoRA: Low-Rank Adaptation of Large Language Models,”\nin The Tenth International Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022, OpenReview.net, 2022. [Online].\nAvailable: https://openreview.net/forum?id=nZeVKeeFYf9\n[30] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “QLoRA:\nEfficient Finetuning of Quantized LLMs,” CoRR, vol. abs/2305.14314,\n2023, doi: 10.48550/arXiv.2305.14314.\n[31] B. Lester, R. Al-Rfou, and N. Constant, “The Power of Scale for Parameter-\nEfficient Prompt Tuning,” in Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP 2021, Virtual\nEvent / Punta Cana, Dominican Republic, 7-11 November, 2021, M.-F.\nMoens, X. Huang, L. Specia, and S. W. Yih, Eds., Association for Compu-\ntational Linguistics, 2021, pp. 3045–3059. doi: 10.18653/v1/2021.emnlp-\nmain.243.\n[32] Z. Liu et al., “Winner-Take-All Column Row Sampling for Memory Effi-\ncient Adaptation of Language Model,” CoRR, vol. abs/2305.15265, 2023,\ndoi: 10.48550/arXiv.2305.15265.\n[33] X. L. Li and P. Liang, “Prefix-Tuning: Optimizing Continuous Prompts for\nGeneration,” in Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference\non Natural Language Processing, ACL/IJCNLP 2021, (V olume 1: Long\nPapers), Virtual Event, August 1-6, 2021, C. Zong, F. Xia, W. Li, and\nR. Navigli, Eds., Association for Computational Linguistics, 2021, pp.\n4582–4597. doi: 10.18653/v1/2021.acl-long.353.\n[34] Q. Huang et al., “Lawyer LLaMA Technical Report,” arXiv preprint\narXiv:2305.15062, 2023.\n[35] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\nlanguage understanding by generative pre- training,” URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-covers/language-\nunsupervised/language_ understanding_paper. pdf, p. 12, 2018.\n[36] T. Brown et al., “Language models are few-shot learners,” Advances in\nneural information processing systems, vol. 33, pp. 1877–1901, 2020.\n[37] J. Johnson, M. Douze, and H. Jégou, “Billion-Scale Similarity Search with\nGPUs,” IEEE Trans. Big Data, vol. 7, no. 3, pp. 535–547, 2021, doi:\n10.1109/TBDATA.2019.2921572.\n[38] T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger, and Y . Artzi,\n“BERTScore: Evaluating Text Generation with BERT,” in 8th Interna-\ntional Conference on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020, OpenReview.net, 2020. [Online]. Available:\nhttps://openreview.net/forum?id=SkeHuCVFDr\n[39] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for\nautomatic evaluation of machine translation,” in Proceedings of the 40th\nannual meeting of the Association for Computational Linguistics, 2002,\npp. 311–318.\n[40] C.-Y . Lin, “Rouge: A package for automatic evaluation of summaries,” in\nText summarization branches out, 2004, pp. 74–81.\nJiawei Hanreceived the M.S. degree in software\nengineering from Jilin University, in 2005, and the\nPh.D. degree in computer science and technology\nfrom Jilin University, in 2018. He was a Visiting\nResearcher with the School of Electronics Engi-\nneering and Computer Science, Peking University.\nHe is currently a associate professor of School\nof Cyber Science and Engineering, Changchun\nUniversity,Unit is Digital Identity and Blockchain\nJoint Laboratory of Peking University. His re-\nsearch interests include Cybersecurity technology, IoT networks, data min-\ning.\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349969\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJ.Han et al.: Intelligent Practices of Large Language Models in Digital Government Services\nJiankang Lu was born in Xinxiang, Henan,\nChina, in July 1999. He received the bachelor’s de-\ngree from Pingdingshan University, in 2021, Cur-\nrently studying for a master’s degree in Changchun\nUniversity.\nYing Xu received the M.S. degree in law from\nJilin University in 2004. Her research interests in-\nclude protection of intellectual property rights and\nlegal protection of network privacy. She works in\nthe school of administration of Changchun Univer-\nsity\nJin You was born in Lvliang, Shanxi, China,\nin September 1995. She received the bachelor’s\ndegree from Taiyuan University, in 2020, Currently\nstudying for a master’s degree in Changchun Uni-\nversity.\nBingxin Wuwas born in Taizhou, Jiangsu, China,\nin September 2001. He received the bachelor’s\ndegree from Nanjing Normal University Zhongbei\nCollege, in 2023, Currently studying for a master’s\ndegree in Changchun University.\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349969\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
}