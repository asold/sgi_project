{
  "title": "Attention is All Large Language Model Need",
  "url": "https://openalex.org/W4407666315",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2095666381",
      "name": "Yu-Xin Liu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2112796928",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6600466347",
    "https://openalex.org/W2990138404"
  ],
  "abstract": "With the advent of the Transformer, the attention mechanism has been applied to Large Language Model (LLM), evolving from initial single- modal large models to today's multi-modal large models. This has greatly propelled the development of Artificial Intelligence (AI) and ushered humans into the era of large models. Single-modal large models can be broadly categorized into three types based on their application domains: Text LLM for Natural Language Processing (NLP), Image LLM for Computer Vision (CV), and Audio LLM for speech interaction. Multi-modal large models, on the other hand, can leverage multiple data sources simultaneously to optimize the model. This article also introduces the training process of the GPT series. Large models have also had a significant impact on industry and society, bringing with them a number of unresolved problems. The purpose of this article is to assist researchers in comprehending the various forms of LLM, as well as its development, pre- training architecture, difficulties, and future objectives.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4525260031223297
    },
    {
      "name": "Linguistics",
      "score": 0.4316815137863159
    },
    {
      "name": "Psychology",
      "score": 0.3526584506034851
    },
    {
      "name": "Philosophy",
      "score": 0.17974752187728882
    }
  ]
}