{
  "title": "Retrieval-augmented Generation across Heterogeneous Knowledge",
  "url": "https://openalex.org/W4287891177",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5114860703",
      "name": "Wenhao Yu",
      "affiliations": [
        null,
        "University of Notre Dame"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4221143736",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3186545525",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4225933709",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W4287727281",
    "https://openalex.org/W4206136559",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4220867505",
    "https://openalex.org/W2107901333",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W4287888426",
    "https://openalex.org/W3197499505",
    "https://openalex.org/W4293179212",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W4309417034",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W3092288641",
    "https://openalex.org/W4221152111",
    "https://openalex.org/W4226085996",
    "https://openalex.org/W4303468996",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3011574394"
  ],
  "abstract": "Retrieval-augmented generation (RAG) methods have been receiving increasing attention from the NLP community and achieved state-of-the-art performance on many NLP downstream tasks. Compared with conventional pre-trained generation models, RAG methods have remarkable advantages such as easy knowledge acquisition, strong scalability, and low training cost. Although existing RAG models have been applied to various knowledge-intensive NLP tasks, such as open-domain QA and dialogue systems, most of the work has focused on retrieving unstructured text documents from Wikipedia. In this paper, I first elaborate on the current obstacles to retrieving knowledge from a single-source homogeneous corpus. Then, I demonstrate evidence from both existing literature and my experiments, and provide multiple solutions on retrieval-augmented generation methods across heterogeneous knowledge.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies: Student Research Workshop, pages 52 - 58\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nRetrieval-augmented Generation across Heterogeneous Knowledge\nWenhao Yu\nUniversity of Notre Dame, USA\nwyu1@nd.edu\nAbstract\nRetrieval-augmented generation (RAG) meth-\nods have been receiving increasing attention\nfrom the NLP community and achieved state-\nof-the-art performance on many NLP down-\nstream tasks. Compared with conventional pre-\ntrained generation models, RAG methods have\nremarkable advantages such as easy knowledge\nacquisition, strong scalability, and low train-\ning cost. Although existing RAG models have\nbeen applied to various knowledge-intensive\nNLP tasks, such as open-domain QA and dia-\nlogue systems, most of the work has focused\non retrieving unstructured text documents from\nWikipedia. In this paper, I first elaborate on the\ncurrent obstacles to retrieving knowledge from\na single-source homogeneous corpus. Then, I\ndemonstrate evidence from both existing liter-\nature and my experiments, and provide multi-\nple solutions on retrieval-augmented generation\nmethods across heterogeneous knowledge.\n1 Introduction\nIn recent years, large pre-trained language models\n(PLMs), such as T5 (Raffel et al., 2020) and GPT-\n3 (Brown et al., 2020), have revolutionized the field\nof natural language processing (NLP), achieving\nremarkable performance on various downstream\ntasks (Qiu et al., 2020). These PLMs have learned a\nsubstantial amount of in-depth knowledge from the\npre-training corpus (Petroni et al., 2019), so they\ncan predict the outputs on downstream tasks with-\nout access to any external memory or raw text, as\na parameterized implicit knowledge base (Roberts\net al., 2020). The way of fine-tuning PLMs using\nonly input-output pairs of target data is often re-\nferred to as close-book setting (Petroni et al., 2019).\nWhile this development is exhilarating, such\nlarge-scale PLMs still suffer from the following\n* This is a thesis proposal paper presented at the student\nresearch workshop (SRW) at NAACL 2022 in Seattle, USA.\n|200M|500M|1B|10B|100B|1T\n80—\n60—50—\n70—\nRAG\nUDT-QA\nT5-3BGPT-3\nRAGKG-FiD\nT5-11BUnik-QA GPT-3 GLaM\nPE-3hop\nOpen-Book(RAGs)Close-Book(PLMs)\nGLaM\nNQTQAWQ\nExactmatch(EMscore)\nNumberofparameters(Million,Billion,Trillion)\n40— T5-3BT5-11B\nFigure 1: The RAG methods significantly outperform\nlarge-scale PLMs on three open-domain QA tasks while\ntrained with much fewer parameters than PLMs.\ndrawbacks: (i) They are usually trained offline,\nmaking the model agnostic to the latest informa-\ntion, e.g., asking a chat-bot trained from 2011-2018\nabout COVID-19 (Yu et al., 2022b). (ii) They\nmake predictions by only “looking up information”\nstored in its parameters, leading to inferior inter-\npretability (Shuster et al., 2021). (iii) They are\nmostly trained on general domain corpora, mak-\ning them less effective on domain-specific tasks\n(Gururangan et al., 2020). (iv) Their pre-training\nphase can be prohibitively expensive for academic\nresearch groups, limiting the model pre-training to\nonly a few industry labs (Izsak et al., 2021).\nThe solution that seems obvious at first glance is\nto allow language models free access to open-world\nresources, such as encyclopedias and books. The\nway of augmenting the input of PLMs with external\ninformation is often referred to as open-book set-\nting (Mihaylov et al., 2018). A prominent method\nin the open-book setting is retrieval-augmented\ngeneration (RAG) (Lewis et al., 2020b; Yu et al.,\n2022c), a new learning paradigm that fuses PLMs\nand traditional IR techniques, which has achieved\nstate-of-the-art performance in many knowledge-\nintensive NLP tasks (Petroni et al., 2021). Com-\npared with large-scale PLMs counterparts, e.g.,\nGPT-3, the RAG model has some remarkable ad-\n52\nvantages: (i) The knowledge is not implicitly stored\nin model parameters, but is explicitly acquired in a\nplug-and-play manner, leading to great scalability;\n(ii) Instead of generating from scratch, the model\ngenerates outputs based on some retrieved refer-\nences, which eases the difficulty of text generation.\nAlthough the RAG models have been widely\nused in the existing literature, most of the work\nhas focused on retrieving unstructured text from\ngeneral domain corpus, e.g., Wikipedia. However,\nthe performance is often limited by the coverage\nof only one certain knowledge. For example, only\na finite portion of questions could be answered\nfrom Wikipedia passages in many open-domain\nQA datasets, while the remaining could only rely\non the input question because no supportive doc-\numents could be retrieved (Oguz et al., 2022). In\nthis paper, I first elaborate on the current obstacles\nto retrieving knowledge from a single-source ho-\nmogeneous corpus. Then, I demonstrate several\npieces of evidence from both existing literature and\nmy own experiments, and provide multiple poten-\ntial solutions on retrieval-augmented generation\nmethods across heterogeneous knowledge.\n2 Background\nI will first provide a formal definition of the RAG\nframework and list necessary notations. RAG aims\nto predict the output y based on the source input\nx (x, y are from a corpus D), while a document\nreference set Zis accessible (e.g., Wikipedia). Be-\nsides, the association between a document z ∈Z\nand the tuple (x, y) ∈D is not necessarily known,\nthough it could be provided by human annota-\ntions (Dinan et al., 2019) or weakly supervised\nsignals (Karpukhin et al., 2020).\nOverall, a general RAG framework has two ma-\njor components: (i) a document retriever and (ii)\na text generator, as shown in Figure 2. The objec-\ntive of the RAG is to train a model to maximize\nthe likelihood of y given x and Z, In practice, Z\noften contains millions of documents, rendering\nenumeration over z impossible. Therefore, the\nfirst step of RAG is to leverage a document re-\ntriever, e.g., DPR (Karpukhin et al., 2020), to nar-\nrow down the search to a handful of relevant doc-\numents. The retriever takes x and Zas input and\nyields relevance scores {s1, ··· , sK}of the top-K\ndocuments Z = {z(1), ··· , z(K)}. Then, the sec-\nond step of RAG is to use a text generator, e.g.,\nBART (Lewis et al., 2020a) and T5 (Raffel et al.,\nThe Beatles\nJohn\nLM EncoderQ: Who was the drummer for the Beatles? A: Ringo Starr\nRingo Starr\nPaul\nLM DecoderRetrieverClose-Book (PLMs)Open-Book (RAG)\nFigure 2: Compared with PLMs, RAG models directly\nseeks knowledge (e.g., texts, tables and KGs) from ex-\nternal information sources to help answer questions.\n2019), to produce desired output y by taking both\ninput x and retrieved document set Z as conditions.\nDocument Retriever. A neural document retriever\ntypically employs two independent encoders like\nBERT (Devlin et al., 2019) to encode the query\nand the document separately, and estimates their\nrelevance by computing a single similarity score\nbetween two encoded representations. For exam-\nple, in DPR (Karpukhin et al., 2020), the docu-\nments Z and context queries x are mapped into the\nsame dense embedding space. The relevance score\ns(x, z) for each document z is computed as the vec-\ntor inner product between document embedding hz\nand query embedding hx, i.e., s(x, z) =hT\nx ×hz.\nText Generator. It can use any encoder-decoder\nframework, such as BART (Lewis et al., 2020a)\nand T5 (Raffel et al., 2019). The model takes in-\nput sequence, as well as the support documents to\ngenerate the desired output. A naive method for\ncombining the input sequence with the support doc-\numents is to concatenate them sequentially (Lewis\net al., 2020a). However, this method suffers from\nthe input sequence length limitation and high com-\nputation cost. FiD (Izacard and Grave, 2021) pro-\ncessed passages independently in the encoder, per-\nformed attention over all the retrieved passages,\nwhich demonstrated state-of-the-art performance\non many knowledge-intensive NLP tasks.\n3 Proposed Work\n3.1 Background and Motivation\nDespite achieving remarkable performance, pre-\nvious efforts of retrieval-augmented generation\n(RAG) works mainly exploit only a single-source\nhomogeneous knowledge retrieval space, i.e.,\nWikipedia passages (Karpukhin et al., 2020; Lewis\net al., 2020b; Petroni et al., 2021; Izacard and\n53\nGrave, 2021; Yu et al., 2022a). However, their\nmodel performance might be limited by the cover-\nage of only one certain knowledge. For example,\nonly a finite portion of questions can be answered\nfrom the Wikipedia passages in many open-domain\nQA datasets, while the remaining can only rely\non the input query because no supportive docu-\nments can be retrieved (Oguz et al., 2022). Since\nmuch useful information cannot be fulfilled based\non Wikipedia alone, a natural solution is to ex-\npand the retrieval corpus from Wikipedia to the en-\ntire World Wide Web (WWW). However, suffering\nfrom the long-tail issue and the cost of a massive\nworkforce, it is not wise to improve the coverage\nby expanding the number of entries in a single-\nsource knowledge (Piktus et al., 2021; Lazaridou\net al., 2022). For example, as shown in Table 1,\nincreasing the retrieval space from Wikipedia (22M\ndocuments) to the web-scale corpus CCNet (906M\ndocuments) even hurts model performance on NQ\nand HotpotQA datasets. This is most likely due to\nthe lower quality (where quality could mean truth-\nfulness, objectivity, lack of harmful content, source\nreliability, etc) of the web corpus, compared with\nthe Wikipedia corpus (Piktus et al., 2021).\nInstead of expanding the number of entries in\na single-source knowledge, an alternative solution\nis resorting to heterogeneous knowledge sources.\nThis is also in line with our human behavior of\nanswering questions that often seek a variety of\nknowledge learned from different sources. There-\nfore, grounding generation across heterogeneous\nknowledge sources is a natural solution to improve\nknowledge coverage and have more room to se-\nlect appropriate knowledge. It is worth mentioning\nthat no knowledge type can always perform the\nbest. The most suitable knowledge depends on the\ncase, in which multiple knowledge might need to\nbe combined for answering one question.\n3.2 Evidence from Existing Literature\nThere are several studies in the existing litera-\nture that combine multiple knowledge to enhance\nlanguage models, such as augmenting common-\nsense reasoning with knowledge graphs (Yu et al.,\n2022d), and introducing multi-modal visual fea-\ntures to enhance emotional dialogue (Liang et al.,\n2022). However, most of them use aligned knowl-\nedge from different sources (e.g., graph-text pairs,\nimage-text pairs), without retrieving knowledge\nfrom a large-scale heterogeneous corpus.\nTable 1: With a larger corpus of unstructured text re-\ntrieval – CCNet, the model performs even worse than re-\ntrieving from Wikipedia alone on the NQ and HotpotQA\ndatasets. The model used in the table is DPR+FiD.\nNo. Source # docs NQ TQA HotpotQA\n1 Wikipedia 22M 51.4 71.0 36.9\n2 CCNet 906M 48.6 73.1 31.6\n‘\nTable 2: Exact match (EM-score) of retrieving hetero-\ngeneous knowledge for three open-domain QA bench-\nmarks. The model used in the table is DPR+FiD.\nNo. Knowledge type Dataset\nText Table KG NQ TQA WebQ\n1 √ 49.0 64.0 50.6\n2 √ 36.0 34.5 41.0\n3 √ 27.9 35.4 55.2\n4 √ √ 54.1 65.1 50.2\n5 √ √ √ 54.0 64.1 57.8\nThe most relevant works to this proposal are\nUniK-QA (Oguz et al., 2022) and PLUG (Li et al.,\n2021). In UniK-QA, Oguz et al. (2022) proposed\nto retrieve information from a merged corpus of\nstructured (i.e., KG triples), semi-structured (i.e.,\ntables) and unstructured data (i.e., text passages)\nfor open-domain QA (Oguz et al., 2022). Their\nexperiments were conducted on multiple open-\ndomain QA benchmark datasets, including Nat-\nuralQuestions (NQ) (Kwiatkowski et al., 2019),\nTriviaQA (TQA) (Joshi et al., 2017) and WebQues-\ntions (WebQ) (Berant et al., 2013).\nThe results in the first three lines in Table 2 high-\nlight the limitation of current state-of-the-art open-\ndomain QA models which use only one informa-\ntion source. Among the three types of knowledge\nsources, text-only methods perform best on NQ\nand TQA datasets, and KG-only methods perform\nbest on WebQ datasets. This is because most of\nthe questions in WebQ are collected from Freebase.\nThe results in the last two lines show that adding\nsemi-structured and structured information sources\nsignificantly improves the performance over text-\nonly models on NQ and TQA datasets. This indi-\ncates tables and knowledge graph triples contain\nvaluable knowledge which is either absent in the\nunstructured texts or harder to extract from them.\nIt is worth mentioning that knowledge het-\nerogeneity can be defined not only by the for-\nmat of knowledge data (i.e., structured and un-\nstructured knowledge), but also by the scope of\nknowledge data (i.e., encyclopedic and common-\n54\nTable 3: Commonly used knowledge sources.\nUnstructured (Semi-)structured\nEncyclopedic Wikipedia, Wikidata,\nknowledge AMiner Freebase\nCommonsense ConceptNet, OMCS, ARC,\nknowledge CSKG, Atomic Wiktionary\nTable 4: Accuracy of retrieving heterogeneous knowl-\nedge for commonsense reasoning over entity tasks.\nNo. Knowledge source Dataset\nCommonsenseEncyclopediaCREAKCSQA2.0\n1 √ 86.55 59.28\n2 √ 82.28 58.23\n3 √ √ 87.57 60.49\nsense knowledge). Table 3 shows common knowl-\nedge sources under two categories. In addition\nof combining structured and unstructured knowl-\nedge, combining encyclopedic and commonsense\nknowledge also brings benefits for many NLP\ntasks, such as commonsense reasoning over entities.\nSome preliminary experiments were conducted on\nCREAK (Onoe et al., 2021) and CSQA2.0 (Tal-\nmor et al., 2021) datasets. CREAK is a dataset\nof human-authored English claims about entities\nthat are either true or false, such as “Harry Pot-\nter can teach classes on how to fly on a broom-\nstick (True).” The model is supposed to bridge\nfact-checking about entities with commonsense in-\nferences. An entity fact relevant to this statement,\n“Harry Potter is a wizard and is skilled at riding a\nbroomstick”, can be retrieved from Wikipedia. A\ncommonsense knowledge, “if you are good at a\nskill you can teach others how to do it” , can be\nretrieved from the ATOMIC (Sap et al., 2019). By\nleveraging both commonsense knowledge and en-\ncyclopedic knowledge in the first-step retrieval, as\nshown in Table 4, the RAG model can achieve su-\nperior performance than only using either of them.\n3.3 Proposed Solutions\nAs mentioned above, heterogeneous knowledge\nis often required when solving open-domain QA\nand many other knowledge-intensive NLP tasks.\nOne natural assumption is to expand knowledge\nsources and add more data to increase the coverage\nof relevant contexts, thereby improving the end-\nto-end performance. In this section, I will present\nthree potential solutions for grounding generation\nacross heterogeneous knowledge.\n3.3.1 Homogenize Different Knowledge to a\nUnified Knowledge Representation\nThe first solution is to homogenize different knowl-\nedge source data into a unified data format – un-\nstructured text. This transformation will then re-\nquire only one retriever, enable relevance compari-\nson across different types of data, and offer textual\nknowledge to easily augment the input of genera-\ntion models by concatenation. Table 3 shows some\ncommonly used knowledge sources. For example,\nsemi-structured tables and structured knowledge\ngraph triples can be converted into the unstructured\ntext by template-based methods (Bosselut et al.,\n2019; Oguz et al., 2022) or neural data-to-text meth-\nods (Wang et al., 2021; Nan et al., 2021).\nFirst, the template-based method is easy to im-\nplement and requires no training process. For ex-\nample, a relation triplet in a knowledge graph con-\nsists of subject, predicate, and object. It can be\nserialized by concatenating the surface form of the\nthree elements to be a sequence of words. Be-\nsides, a table can also be hierarchically converted\ninto text format: first, concatenate cell values of\neach row separated by commas; then combine these\nrows’ text forms delimited by semicolons. Al-\nthough the template-based method is simple but\nmay suffer from incorrect syntax and incomplete\nsemantics. On the contrary, the neural graph-to-\ntext and table-to-text generation methods rely on\npre-trained language models that may ensure syn-\ntax correctness and semantic completeness. Once\neither type of the methods converts the structured\nand semi-structured data to unstructured text, a\ndense retriever model such as DPR (Karpukhin\net al., 2020) can be used to index all of them and\nretrieve relevant knowledge. The reader model will\nconcatenate the retrieved text with original input\nand compute full attention over the entire represen-\ntations through a T5 (Raffel et al., 2020) decoder.\nThis unified knowledge index allows the models\nto learn knowledge of various formats and scopes\nof data, and the model can simultaneously retrieve\ninformation from a unified index of multiple knowl-\nedge sources to improve the knowledge coverage.\n3.3.2 Multi-virtual Hops Retrieval over\nHeterogeneous Knowledge\nRetrieved data are expected to bridge the gap be-\ntween inputs and outputs of generation models. In\nother words, retrievers are trained to provide in-\nformation that is found with the inputs as queries\nand related to the outputs. Ideally, they find the\n55\noutput-related information just once. However,\nthat may actually take multiple hops of retrieval\nacross knowledge sources. Thus, the second solu-\ntion is to iteratively retrieve knowledge from dif-\nferent sources. Regarding an entity, encyclope-\ndic knowledge usually contains its attribute infor-\nmation (e.g., age, duration), while commonsense\nknowledge includes universally recognized facts in\nhuman’s daily life. For example, the entity “soup”\nin Wikipedia is described as “a primarily liquid\nfood, generally served warm or hot, made by com-\nbining ingredients of meat or vegetables with stock,\nmilk, or water”; and in the OMCS corpus (Singh\net al., 2002), it contains a well-known fact “soup\nand salad can be a healthy lunch”. Therefore, to\nanswer the question “What are the common ingredi-\nents in a healthy lunch?”, the encyclopedic corpus\nand commonsense corpus can provide complemen-\ntary knowledge that should be both leveraged.\nBesides, it also might be necessary to first read\na subset of the corpus to extract the useful infor-\nmation, and then further retrieve information from\nother knowledge sources. For example, given in-\nput q, it may take k steps, each step retrieving\ndata di from source si ∈S with an incremental\nquery qi = q ⊕d1 ⊕···⊕ di−1 (i ≤ k) until\nthe final dk contains the information that can di-\nrectly augment the generation of outputs o. Here\nSincludes various sources such as text corpora,\ntables, and knowledge graphs. To achieve this,\nhowever, the primary challenge for training such\na multi-hop retriever is that it cannot observe any\nintermediate document for supervision but only the\nfinal output. So, the multi-virtual hops retrieval\n(MVHL) needs to perform multi-hop retrieval with-\nout any intermediate signal. I will discuss two\npromising designs as below. First, the MVHL ap-\nproach will dynamically determine when the multi-\nhops retrieval finishes. I denote the relevance score\nbetween query qi and data di from source si by\nr(di; qi, si). The search continues at the i-th step,\nif r(di; qi, si) > r(di; qi−1, si−1 ∪si); because di\nbrings new relevant information that was not able\nto be retrieved at the (i −1)-th step or any previ-\nous steps. Second, the MVHL can use sequential\nmodels instead of heuristics to control the multi-\nhops search. The search is expected to finish at\nstep i, when the relevance between the retrieved\ndata di and output o, which can be computed by\nBERTScore (Zhang et al., 2020), achieves a local\nmaximum. In order to model the relationship be-\nQuery: What was the occupationof Lovely Ritain the Beatlessong?Wikiepdia: Lovely Rita is a song by the English rock band the Beatlesfrom theiralbum Sgt. Pepper’s Lonely Hearts Club Band. It was writ-enand sung by Paul McCartneyand credited to Lennon-McCartney. It is about a female traffic wardenand the narrator‘s affection for her.Wikidata:\nThe Beatles\nPaul McCartney\nSgt. Pepper ..\nperformer of \nLovely Rita\nTraffic warden\nis a member of \noccupation song of written by\nFigure 3: Reasoning over retrieved documents on struc-\ntured knowledge provides explicit knowledge ground-\ning to help answer questions. For example, in WebQ,\n46.9%/56.1% of the questions can be solved by one/two-\nhop neighbors on the query-document subgraph.\ntween this target relevance ro(di) and the retrieval\nscore r(di; qi, si), a straightforward solution is to\ntrain a multi-hop retriever with only the outputo us-\ning a fixed number of hops K (5 or 10) and use the\nvalidation set to choose the best model. With that\nmodel, I can observe the K-length series of r and\nro, and train an RNN model that predicts ro(dk)\nbased on the first k elements in the r series. The\nsearch terminates when the predicted ro decreases.\n3.3.3 Reasoning over Retrieved Documents\nBased on Structured Knowledge\nTraditional reader modules typically concatenate\nthe input query and retrieved documents sequen-\ntially, and then feed them into a pre-trained genera-\ntion model, such as T5. Although the token-level\nattention can implicitly learning some relational pat-\nterns between the input query and retrieved docu-\nments, it does not fully utilize the structured knowl-\nedge that can provide more explicit grounding. As\nshown in Figure 3, the relational information be-\ntween important entities in the input query (i.e.,\nLovely Rita) and the retrieved documents (i.e., traf-\nfic warden) may require reasoning over structured\nknowledge that is not explicitly stated in the con-\ntext. So, the third solution is to perform multi-hop\nreasoning on structured knowledge, e.g., Wikidata,\nto learn relational patterns between the input query\nand retrieved documents. In this way, the represen-\ntation of retrieved documents is further enriched\nby structured knowledge. To perform knowledge\nreasoning over retrieved documents, the idea is\nto first extract a query-document subgraph since\ndirect reasoning on the entire knowledge graph is\nintractable. Entities on the subgraph can be mapped\nby given hyperlinks in Wikipedia passages. Then,\na multi-relational graph encoder iteratively updates\n56\nthe representation of each entity node by aggre-\ngating information from its neighboring nodes and\nedges. Then, the embedded node and relation repre-\nsentations, as well as the query and document rep-\nresentations, are then fused into the reader model.\nAcknowledgements\nMany thanks to my doctoral supervisor Dr. Meng\nJiang and my doctoral thesis committee members\nDr. Nitesh Chawala, Dr. David Chiang, Dr. Heng\nJi, Dr. Scott Yih for giving me constructive sugges-\ntions. My research is mainly funded by National\nScience Foundation (NSF) grants IIS-1849816,\nCCF-1901059, IIS-2119531, and IIS-2142827.\nReferences\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for auto-\nmatic knowledge graph construction. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics (ACL).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems (Neurips).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies (NAACL).\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In International Conference on Learning\nRepresentations (ICLR).\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks.\nIn 58th Annual Meeting of the Association for\nComputational Linguistics (ACL).\nGautier Izacard and Édouard Grave. 2021. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics (EACL).\nPeter Izsak, Moshe Berchansky, and Omer Levy. 2021.\nHow to train bert with an academic budget. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP).\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (ACL).\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: A benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics (TACL).\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\narXiv preprint arXiv:2203.05115.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Annual Meeting of the Association for\nComputational Linguistics (ACL).\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020b. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems (Neruips).\nYu Li, Baolin Peng, Yelong Shen, Yi Mao,\nLars Liden, Zhou Yu, and Jianfeng Gao. 2021.\nKnowledge-grounded dialogue generation with a\nunified knowledge representation. arXiv preprint\narXiv:2112.07924.\nYunlong Liang, Fandong Meng, Ying Zhang, Yufeng\nChen, Jinan Xu, and Jie Zhou. 2022. Emotional\nconversation generation with heterogeneous graph\nneural network. Artificial Intelligence.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\n57\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit\nRau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru\nTang, Aadit Vyas, Neha Verma, Pranav Krishna, et al.\n2021. Dart: Open-domain structured data record to\ntext generation. In Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (NAACL).\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan\nPeshterliev, Dmytro Okhonko, Michael Schlichtkrull,\nSonal Gupta, Yashar Mehdad, and Scott Yih. 2022.\nUnik-qa: Unified representations of structured and\nunstructured knowledge for open-domain question\nanswering. Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (NAACL-HLT).\nYasumasa Onoe, Michael JQ Zhang, Eunsol Choi, and\nGreg Durrett. 2021. Creak: A dataset for common-\nsense reasoning over entity knowledge. In Thirty-\nfifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Neurips).\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\net al. 2021. Kilt: a benchmark for knowledge in-\ntensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (NAACL).\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP).\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nDmytro Okhonko, Samuel Broscheit, Gautier Izacard,\nPatrick Lewis, Barlas O˘guz, Edouard Grave, Wen-tau\nYih, et al. 2021. The web is your oyster–knowledge-\nintensive nlp against a very large web corpus. arXiv\npreprint arXiv:2112.09924.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey. In\nScience China Technological Sciences. Springer.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. In Journal of Machine Learning Research.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence (AAAI).\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings of\nEmpirical Methods in Natural Language Processing.\nPush Singh, Thomas Lin, Erik T Mueller, Grace Lim,\nTravell Perkins, and Wan Li Zhu. 2002. Open mind\ncommon sense: Knowledge acquisition from the gen-\neral public. In International Conferences on the\nMove to Meaningful Internet Systems. Springer.\nAlon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bha-\ngavatula, Yoav Goldberg, Yejin Choi, and Jonathan\nBerant. 2021. Commonsenseqa 2.0: Exposing the\nlimits of ai through gamification. In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Neurips).\nLuyu Wang, Yujia Li, Ozlem Aslan, and Oriol Vinyals.\n2021. Wikigraphs: A wikipedia text-knowledge\ngraph paired dataset. In Proceedings of the Fifteenth\nWorkshop on Graph-Based Methods for Natural Lan-\nguage Processing (TextGraphs-15).\nDonghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao\nYu, Shuohang Wang, Yichong Xu, Xiang Ren, Yim-\ning Yang, and Michael Zeng. 2022a. Kg-fid: Infus-\ning knowledge graph in fusion-in-decoder for open-\ndomain question answering. Proceedings of the An-\nnual Meeting of the Association for Computational\nLinguistics (ACL).\nWenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu,\nShuohang Wang, Yichong Xu, Michael Zeng, and\nMeng Jiang. 2022b. Dict-bert: Enhancing language\nmodel pre-training with dictionary. Proceedings of\nthe Annual Meeting of the Association for Computa-\ntional Linguistics (ACL).\nWenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\nQingyun Wang, Heng Ji, and Meng Jiang. 2022c. A\nsurvey of knowledge-enhanced text generation. In\nACM Computing Survey (CSUR).\nWenhao Yu, Chenguang Zhu, Lianhui Qin, Zhihan\nZhang, Tong Zhao, and Meng Jiang. 2022d. Diversi-\nfying content generation for commonsense reasoning\nwith mixture of knowledge graph experts. In An-\nnual Meeting of the Association for Computational\nLinguistics (ACL).\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2020. Bertscore: Evaluating\ntext generation with bert. In International Confer-\nence on Learning Representations.\n58",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8690758347511292
    },
    {
      "name": "Scalability",
      "score": 0.7513190507888794
    },
    {
      "name": "Homogeneous",
      "score": 0.5467140674591064
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5208492875099182
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5201978087425232
    },
    {
      "name": "Information retrieval",
      "score": 0.5195998549461365
    },
    {
      "name": "Natural language processing",
      "score": 0.5122105479240417
    },
    {
      "name": "Domain knowledge",
      "score": 0.5085271596908569
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.49852442741394043
    },
    {
      "name": "Question answering",
      "score": 0.4936865270137787
    },
    {
      "name": "Database",
      "score": 0.0639394223690033
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}