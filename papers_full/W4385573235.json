{
  "title": "Transformer-based Entity Typing in Knowledge Graphs",
  "url": "https://openalex.org/W4385573235",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5003964217",
      "name": "Zhiwei Hu",
      "affiliations": [
        "Shanxi University"
      ]
    },
    {
      "id": "https://openalex.org/A5060245641",
      "name": "Víctor Gutiérrez-Basulto",
      "affiliations": [
        "Cardiff University"
      ]
    },
    {
      "id": "https://openalex.org/A5007458161",
      "name": "Zhiliang Xiang",
      "affiliations": [
        "Cardiff University"
      ]
    },
    {
      "id": "https://openalex.org/A5100697664",
      "name": "Ru Li",
      "affiliations": [
        "Shanxi University"
      ]
    },
    {
      "id": "https://openalex.org/A5066422711",
      "name": "Jeff Z. Pan",
      "affiliations": [
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285181864",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W2757369719",
    "https://openalex.org/W2995448904",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3174794493",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963149098",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W4206398071",
    "https://openalex.org/W4205882439",
    "https://openalex.org/W2767287441",
    "https://openalex.org/W2728059831",
    "https://openalex.org/W2251315883",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4206745025",
    "https://openalex.org/W4214604401",
    "https://openalex.org/W4205291549",
    "https://openalex.org/W4311830211",
    "https://openalex.org/W3168355451",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2584261443",
    "https://openalex.org/W4225979812",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W3082429057",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W4213381808",
    "https://openalex.org/W2432356473",
    "https://openalex.org/W3086542763",
    "https://openalex.org/W2949434543",
    "https://openalex.org/W3035559300",
    "https://openalex.org/W2970544797",
    "https://openalex.org/W2980646235",
    "https://openalex.org/W3170261818",
    "https://openalex.org/W2988237903",
    "https://openalex.org/W2022166150"
  ],
  "abstract": "We investigate the knowledge graph entity typing task which aims at inferring plausible entity types. In this paper, we propose a novel Transformer-based Entity Typing (TET) approach, effectively encoding the content of neighbours of an entity by means of a transformer mechanism. More precisely, TET is composed of three different mechanisms: a local transformer allowing to infer missing entity types by independently encoding the information provided by each of its neighbours; a global transformer aggregating the information of all neighbours of an entity into a single long sequence to reason about more complex entity types; and a context transformer integrating neighbours content in a differentiated way through information exchange between neighbour pairs, while preserving the graph structure. Furthermore, TET uses information about class membership of types to semantically strengthen the representation of an entity. Experiments on two real-world datasets demonstrate the superior performance of TET compared to the state-of-the-art.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5988–6001\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nTransformer-based Entity Typing in Knowledge Graphs\nZhiwei Hu♣ Víctor Gutiérrez-Basulto♢ Zhiliang Xiang♢\nRu Li♣∗ Jeff Z. Pan♠∗\n♣ School of Computer and Information Technology, Shanxi University, China\n♦ School of Computer Science and Informatics, Cardiff University, UK\n♠ ILCC, School of Informatics, University of Edinburgh, UK\n♣ zhiweihu@whu.edu.cn,liru@sxu.edu.cn\n♦{gutierrezbasultov,xiangz6}@cardiff.ac.uk\n♠j.z.pan@ed.ac.uk\nAbstract\nWe investigate the knowledge graph entity typ-\ning task which aims at inferring plausible en-\ntity types. In this paper, we propose a novel\nTransformer-based Entity Typing (TET) ap-\nproach, effectively encoding the content of\nneighbors of an entity. More precisely, TET\nis composed of three different mechanisms: a\nlocal transformer allowing to infer missing\ntypes of an entity by independently encoding\nthe information provided by each of its neigh-\nbors; a global transformeraggregating the in-\nformation of all neighbors of an entity into a\nsingle long sequence to reason about more com-\nplex entity types; and a context transformer\nintegrating neighbors content based on their\ncontribution to the type inference through in-\nformation exchange between neighbor pairs.\nFurthermore, TET uses information about class\nmembership of types to semantically strengthen\nthe representation of an entity. Experiments\non two real-world datasets demonstrate the su-\nperior performance of TET compared to the\nstate-of-the-art.\n1 Introduction\nA knowledge graph (KG) ( Pan et al. , 2016) is a\nmulti-relational graph encoding factual knowledge,\nwith the form (h, r, t ) where h, t are the head\nand tail entities connected via the relation r. In\nthis paper, we consider KGs with minimal schema\ninformation, i.e., those containing entity type as-\nsertions, as the only schema information, of the\nform\n(e, has_type, c ) stating that the entity e has\ntype c; e.g., to capture that Barack Obama has\ntype President. Entity type knowledge is widely\nused in NLP tasks, e.g., in relation extraction ( Liu\net al. , 2014), entity and relation linking ( Gupta\net al. , 2017; Pan et al. , 2019), question answering\n(ElSahar et al. , 2018; Hu et al. , 2022), and ﬁne-\ngrained entity typing on text ( Onoe et al. , 2021;\nQian et al. , 2021; Liu et al. , 2021). However, entity\n∗Contact Authors\nBarack \nObama\nA Promised \nLand\n1961\nU.S\nDemocratic \nParty\nJuris \nDoctor\nColumbia \nUniversity\nBook\n20 th -century \nAmerican writers\nPolitician\nPresident\nColumbia \nUniversity Alumni\nAmerican \nLegal Scholars\nBachelor \nof Science\nRelational neighbors\nType neighbors\ndegree award\nwrite\nEntity Type\n Missing type\nKG relation Type link Missing type link\nFigure 1: A KG with its entity type information.\ntypes are far from complete, since in real-world\napplications they are continuously emerging. For\nexample, about 10% of entities in FB15k ( Bordes\net al. , 2013) have the type /music/artist, but do not\nhave /people/person (Moon et al. , 2017).\nIn light of this, it has been recently investigated\nthe Knowledge Graph Entity Typing (KGET) task,\naiming at inferring missing entity types in a KG.\nMost existing approaches to KGET use methods\nbased on either embeddings or graph convolutional\nnetworks (GCN). Despite the huge progress these\nmethods have made, there are still some important\nchallenges to be solved. On the one hand, most\nembedding-based models ( Moon et al. , 2017; Zhao\net al. , 2020; Ge et al. , 2021; Zhuo et al. , 2022)\nencode all neighbors of a target entity into a sin-\ngle vector, but in many cases only some neigh-\nbors are necessary to infer the correct types. For\nexample, as shown in Figure\n1, to predict that\nthe entity Barack Obama has type President, only\nthe neighbor\nis_leader_of\n− − − − − − →U.S is needed. Indeed,\nusing too many neighbors, such as\ngraduate_from\n− − − − − − − − →\nColumbia University , will introduce noise. The\nCET model ( Pan et al. , 2021) overcomes this\nproblem by encoding each neighbor independently.\nHowever, since entities and relations are repre-\n5988\nsented by TransE ( Bordes et al. , 2013), there is\na restriction on the direction of the representation\nof entities and relations direction, ﬁxing it from\nentity to relation or vice versa. As a consequence,\ncertain interactions between neighbor entities and\nrelations are ignored. Also, to predict more com-\nplex types, CET directly adds and averages the\nneighbor representations, weakening the contri-\nbution of different neighbors, since it ignores that\nthe contribution of different neighbors to differ-\nent types might not be the same. For example,\nas shown in Figure\n1, the inference of the type\n20th-century American writer involves multiple se-\nmantic aspects of Barack Obama , it requires to\njointly consider the neighbors write− − →A Promised\nLand, was_born_in− − − − − − − →1961, and\nis_leader_of\n− − − − − − →U.S, but the\nneighbor\ndegree_award\n− − − − − − − →Juris Doctor should get less\nattention. On the other hand, GCN frameworks for\nKGET use expressive representations for entities\nand relations based on their neighbor entities and\nrelations (\nJin et al. , 2019; Zhao et al. , 2022; Zou\net al. , 2022; Vashishth et al. , 2020; Pan et al. , 2021).\nHowever, a common problem of GCN-based mod-\nels is that they aggregate information only along the\npaths starting from neighbors of the target entity,\nlimiting the representation of interdependence be-\ntween neighbors that are not directly connected.\nFor example, in Figure 1 the entities Juris Doctor\nand U.S are not connected, but combining their in-\nformation could help to infer that American Legal\nScholars is a type of Barack Obama . This could be\nﬁxed by increasing the number of layers, but with\nan additional computational cost.\nThe main objective of this paper is to introduce\na transformer-based approach to KGET that ad-\ndresses the highlighted challenges. The transformer\narchitecture ( Vaswani et al. , 2017) has been essen-\ntial for NLP, e.g., in pre-trained language mod-\nels (\nDevlin et al. , 2019; Reimers and Gurevych ,\n2019; Lan et al. , 2020; Wu et al. , 2021a), document\nmodeling ( Wu et al. , 2021b), and link prediction\n(Wang et al. , 2019; Chen et al. , 2021). Transform-\ners are well-suited for KGET as entities and re-\nlations in a KG can be regarded as tokens, and\nusing the transformer as encoder, one can thus\nachieve bidirectional deep interaction between enti-\nties and relations. Speciﬁcally, we propose TET, a\nTransformer-based Entity Typing model for KGET,\ncomposed of the following three inference modules.\nA local transformer that independently encodes\nthe relational and type neighbors of an entity into a\nsequence, facilitating bidirectional interaction be-\ntween elements within the sequence, addressing\nthe ﬁrst problem. A global transformer that ag-\ngregates all neighbors of an entity into a single\nlong sequence to simultaneously consider multi-\nple attributes of an entity, allowing to infer more\n‘complex’ types, thus addressing the third problem.\nA context transformerthat aggregates neighbors\nof an entity in a differentiated manner according\nto their contribution while preserving the graph\nstructure, thus addressing the second problem. Fur-\nthermore, we use semantic knowledge about the\nknown types in a KG. In particular, we ﬁnd out that\ntypes are normally clustered in classes. For exam-\nple, the types medicine/disease, medicine/symptom,\nand medicine/drug belong to the class medicine.\nWe use this class membership information for re-\nplacing the ‘generic’ relation has_type with a more\nﬁne-grained relation that captures to which class a\ntype belongs to, enriching the semantic content of\nconnections between entities and types. To sum up,\nour contributions are:\n• We propose a novel transformer-based frame-\nwork for inferring missing entity types in KGs,\nencoding knowledge about entity neighbors\nfrom three different perspectives.\n• We use class membership of types to replace the\nsingle has_type relation with class-membership\nrelations providing ﬁne-grained semantic infor-\nmation.\n• We conduct empirical and ablation experiments\non two real-world datasets, demonstrating the\nsuperiority of TET over existing SoTA models.\nData, code, and an extended version with\nappendix are available at https://github.\ncom/zhiweihu1103/ET-TET.\n2 Related Work\nThe knowledge graph completion (KGC) task is\nusually concerned with predicting the missing head\nor tail entities of a triple. KGET can thus be seen as\na specialization of KGC. Existing KGET methods\ncan be classiﬁed in embedding- and GNC-based.\nEmbedding-based Methods. ETE ( Moon et al. ,\n2017) learns entity embeddings for KGs by a stan-\ndard representation learning method ( Bordes et al. ,\n2013), and further builds a mechanism for infor-\nmation exchange between entities and their types.\n5989\nConnectE ( Zhao et al. , 2020) jointly embeds enti-\nties and types into two different spaces and learns\na mapping from the entity space to the type space.\nCORE ( Ge et al. , 2021) utilizes the models Ro-\ntatE ( Sun et al. , 2019) and ComplEx ( Trouillon\net al. , 2016) to embed entities and types into two\ndifferent complex spaces, and develops a regression\nmodel to link them. However, the above methods\ndo not fully consider the known types of entities\nwhile training the entity embedding representation,\nwhich seriously affects the prediction performance\nof missing types. Also, the representation of types\nin these methods is such that they cannot be se-\nmantically differentiated. CET (\nPan et al. , 2021)\njointly utilizes information about existing type as-\nsertions in a KG and about the neighborhood of\nentities by respectively employing an independent-\nbased mechanism and an aggregated-based one. It\nalso utilizes a pooling method to aggregate their\ninference results. AttEt ( Zhuo et al. , 2022) designs\nan attention mechanism to aggregate the neighbor-\nhood knowledge of an entity using type-speciﬁc\nweights, which are beneﬁcial to capture speciﬁc\ncharacteristics of different types. A shortcoming of\nthese two methods is that, unlike our TET model,\nthey are not able to cluster types in classes, and are\nthus not able to semantically differentiate them in\na ﬁne-grained way.\nGCN-based Methods. Graph Convolutional Net-\nworks (GCNs) have proven effective on modeling\ngraph structures ( Kipf and Welling , 2017; Hamil-\nton et al. , 2017; Dettmers et al. , 2018). However,\ndirectly using GCNs on KGs usually leads to poor\nperformance since KGs have different kinds of\nentities and relations. To address this problem,\nRGCN ( Schlichtkrull et al. , 2018) proposes to ap-\nply relation-speciﬁc transformations in GCN’s ag-\ngregation. HMGCN ( Jin et al. , 2019) proposes a\nhierarchical multi-graph convolutional network to\nembed multiple kinds of semantic correlations be-\ntween entities. CompGCN ( Vashishth et al. , 2020)\nuses composition operators from KG-embedding\nmethods by jointly embedding both entities and\nrelations in a relational graph. ConnectE-MRGAT\n(Zhao et al. , 2022) proposes a multiplex relational\ngraph attention network to learn on heterogeneous\nrelational graphs, and then utilizes the ConnectE\nmethod for infering entity types. RACE2T (\nZou\net al. , 2022) introduces a relational graph attention\nnetwork method, utilizing the neighborhood and\nrelation information of an entity for type inference.\nA common problem with these methods is that they\nfollow a simple single-layer attention formulation,\nrestricting the information transfer between uncon-\nnected neighbors of an entity.\nTransformer-based Methods. To the best of\nour knowledge, there are no transformer-based\napproaches to KGET. However, two transformer-\nbased frameworks for the KGC task have been al-\nready proposed: CoKE ( Wang et al. , 2019) and\nHittER ( Chen et al. , 2021). Our experiments show\nthat they are not suitable for KGET.\n3 Method\nIn this section, we describe the architecture of our\nTET model (cf. Figure 2). We start by introducing\nnecessary background (Sec. 3.1), then present in\ndetail the architecture of TET (Sec. 3.2). Finally,\nwe describe pooling and optimization strategies\n(Sec. 3.3 and 3.4).\n3.1 Background\nIn this paper, a knowledge graph ( Pan et al. , 2016)\nis represented in a standard format for graph-\nstructured data such as RDF ( Pan, 2009). A knowl-\nedge graph (KG) G is a tuple (E, R, C, T ), where\nE is a set of entities, C is a set of entity types, R\nis a set of relation types, and T is a set of triples.\nTriples in T are either relation assertions (h, r, t )\nwhere h, t ∈ E are respectively the head and tail\nentities of the triple, and r ∈ R is the edge of the\ntriple connecting head and tail; or entity type as-\nsertions (e, has_type, c ), where e ∈ E , c ∈ C , and\nhas_type is the instance-of relation. For e ∈ E ,\nthe relational neighbors of e is the set {(r, f ) |\n(e, r, f ) ∈ T } . The type neighbors of e are deﬁned\nas {(has_type, c ) | (e, has_type, c ) ∈ T } . We will\nsimply say neighbors of e when we refer to the\nrelational and type neighbors of e. The goal of this\npaper is to address KGET task which aims at infer-\nring missing types from C in entity type assertions.\n3.2 Model Architecture\nIn this section, we introduce the local, global and\ncontext transformer-based modeling components of\nour TET model. Before deﬁning these components,\nwe start by discussing an important observation.\n3.2.1 Class Membership\nA key observation is that in a KG all type assertions\nare uniformly deﬁned using the relation has_type.\n5990\nLocal Transformer\n[CLS]\n r\nc-1\nc\n1\n[CLS]\n r\n1\nf\n1\nLocal\n Local\nContext Transformer\n[CLS]\n r\nc-1\nc\n1\nr\n1\nf\n1\nGlobal Transformer\n[CLS]\nPooling\nRelation\nEntity\nType-class\nType\nLayer prediction \nPool aggregation\nTransformer\nAVG MAX MIN\nLocal Transformer\nr\nc-i\nr\n1\nc\ni\nr\nc-j\nc\nj\nr\nc-k\nc\nk\nOutput vector\nFigure 2: An overview of the TET model. The red dotted box part is only performed on the YAGO43kET dataset.\nNote that rc−i is an abbreviation of rclassi . Box with Local text indicates the output of the local transformer module.\nAs a consequence, we do not have a way to fully dif-\nferentiate the contribution of different types of an\nentity during inference, as we cannot capture the re-\nlationship between them and their relevance, weak-\nening thus the contribution of type supervision on\nentities. However, in practice types are clustered to-\ngether in classes (i.e., root types in a domain); e.g.,\nthe types medicine/disease, medicine/symptom, and\nmedicine/drug belong to the class medicine. This\nallows us to identify that these types are related\nas all of them talk about something related to\nmedicine, providing us therefore with ﬁne-grained\nsemantic information. With this insight in mind,\nfor each class, we create a relation that will be\nused to model that a type is an element of that\nclass. For instance, for the class medicine, we in-\ntroduce the relation belongs_class_medicine . We\nwill then replace a type neighbor (has_type, c ) of\nan entity e with (rclass, c ), where rclass is the rela-\ntion modeling class membership, i.e. belonging\nto the class medicine. We deﬁne the type-class\nneighbors of an entity as expected. We will use\nbelow this semantically-enriched representation in\nour local and global transformers.\n3.2.2 Local Transformer\nThe main intuition behind the local component is\nthat the neighbors of an entity might help to deter-\nmine its types, and that the contribution of each\nneighbor is different. For instance, if the entity\nLiverpool has the relational neighbor ( places_lived,\nDaniel Craig ), it is plausible to infer Liverpool\nhas type /location/citytown. On the other hand, the\nneighbor ( sports_team, Liverpool F .C.) may help to\ninfer that it has type /sports/sports_team_location.\nTo encode type-class neighbors (rclass, c ), sim-\nilar to the input representations of BERT ( De-\nvlin et al. , 2019), we build the input sequence\nH = ( [CLS], r class, c ), where [CLS] is a special\ntoken, and for each element hi in H, we construct\nits input vector representation hi as:\nhi = hword\ni + hpos\ni\nhword\ni\nand hpos\ni\nare randomly initialized word and\nposition embeddings of rclass or c. We apply a local\ntransformer to each type-class neighbor sequence\nto model the interaction between the class relations\nand types of an entity. The output embedding cor-\nresponding to [CLS], denoted as Hcls ∈ Rd×1, is\nthen used to infer missing types of the target entity,\nwhere d represents the dimension of the embed-\nding. For an entity with n type-class neighbors,\nthey are denoted as [Hcls\n1 , Hcls\n2 , ..., Hcls\nn ]\nafter the\nlocal transformer representation.\nType-class neighbors are not capable to fully\ncapture the structural information within the KG.\nTo alleviate this problem, we also consider rela-\ntional neighbors. As for type-class neighbors, to\nencode the relational neighbors (r, f ) of an entity,\nwe build a sequence Q = ( [CLS], r, f ), and ag-\ngregate the word and position embeddings and fur-\nther apply a local transformer. The output embed-\nding of [CLS] is denoted as Qcls ∈ Rd×1, and\nfor an entity with m relational neighbors, they are\nrepresented as [Qcls\n1 , Qcls\n2 , ..., Qcls\nm ]\nafter the local\ntransformer representation.\nThe local transformer mainly pays attention to a\nsingle existing neighbor at a time in the inference\nprocess, reducing the interference between unre-\nlated types. We perform a non-linear activation on\nneighbors, and then perform a linear layer opera-\ntion to unify the dimension to the number of types,\nthe ﬁnal local transformer score Sloc ∈ RL×(m+n)\nis deﬁned as:\nWRelu([Hcls\n1 , . . . , Hcls\nn , Qcls\n1 , . . . , Qcls\nm ]) + b (1)\n5991\nW ∈ RL×d and b ∈ RL are the learnable param-\neters, where L is the number of types. [, ] de-\nnotes the concatenation function, Hcls\ni ∈ Rd×1\nand Qcls\nj ∈ Rd×1\nrespectively represent the i-th\nand j-th embedding of the type-class and relational\nneighbors after the transformer representation.\nAn important observation is that the number of\nrelations available vary from one KG to another.\nFor instance, the YAGO43kET KG has substan-\ntially fewer relations than the FB15kET KG (cf.\nthe dataset statistics in the Experiments Section),\nmaking the discrimination among relations in re-\nlational triples harder. To tackle this problem, for\nthe YAGO43kET KG, we semantically enrich the\nrepresentation of relations by using the type-class\nmembership information. Speciﬁcally, for a rela-\ntional neighbor (r, f ) of an entity, we use the types\nof f belonging to a certain class to enhance the\nrelation r in the sequence ([CLS], r, f ) using the\nfollowing steps:\n1. Let Γ = {(has_type, c 1), (has_type, c 2),\n. . . , (has_type, c ℓ)} be the set of all type neigh-\nbors of f. We replace Γ with the set Γ′ of cor-\nresponding type-class neighbors: {(rclass1 , c 1),\n(rclass2 , c 2), . . . , ((rclassℓ , c ℓ)}, i.e., representing\nthat ci is a member of classi.\n2. Based on r and Γ′, we construct a sequence\nP = ( r, rclass1 , c 1, r class2 , c 2, . . . , r classℓ , c ℓ).\nFor each element pi of P, we assign randomly\ninitialized word and position embeddings to cap-\nture sequence order. We then apply a trans-\nformer to capture the interaction between to-\nkens. The output token embeddings are denoted\nas [p0, p1, . . . , pℓ].\n3. For the output token embeddings, we use three\ndifferent operations to obtain the ﬁnal represen-\ntation of relation r: average, maximum, and\nminimum. For the YAGO43kET KG, we re-\nplace the word embedding\nr in sequence Q\nwith Pavg = ∑ℓ\ni=0 pi\n, Pmax = Max(pi), or\nPmin = Min(pi).\n3.2.3 Global Transformer\nThe local transformer mechanism is suitable for\ntypes that can be inferred by looking at simple\nstructures, and for which independently consid-\nering neighbors is thus enough. However, infer-\nring ‘complex’ types requires to capture the in-\nteraction between different neighbors of an en-\ntity. For instance, if we would like to infer\nthat the entity Birmingham_City_L.F .C. has type\nWomen’s_football_clubs_in_England, we need to\nsimultaneously consider different sources of infor-\nmation to support this, such as the type neighbor\n(has_type, Association_football_clubs) and rela-\ntional neighbor ( isLocatedIn, England) of Birm-\ningham_City_L.F .C., and that ( playsFor, Birming-\nham_City_L.F .C.) and ( hasGender, female) are re-\nlational neighbors of the entity Darla_Hood. To\nthis aim, we introduce a global transformer module\ncapturing the interaction between type-class and\nrelational neighbors by comprehensively represent-\ning them as the input of a transformer as follows:\n1. For a target entity e, we deﬁne the set Γ′\nas done in Section 3.2.2. Further, let Ξ =\n{(r1, f 1), . . . , (rm, f m)} denote the set of all\nrelational neighbors of e.\n2. We uniformly represent Γ′ and Ξ as a single\nsequence G = ( [CLS] rclass1 , c 1, r class2 , c 2,\n. . . , r classℓ , c n, r 1, f 1, r 2, f 2, . . . r m, f m).\n3. For each element in the sequence G, we as-\nsign randomly initialized word and position\nembeddings, and input it into a transformer.\nThe output embedding of\n[CLS] is denoted\nGcls ∈ Rd×1. Similar to Equation (1), we\ndeﬁne the prediction score Sglo ∈ RL×1 as\nWRelu([Gcls]) + b.\n3.2.4 Context Transformer\nFor complex types, the global transformer uni-\nformly serializes the information about the neigh-\nbors of the target entity. However, the neighbors\nof the target entity are pairs, and this structural\ninformation might be useful for inference. For in-\nstance, to infer that the entity Barack Obama has\ntype 20th-century American writers , we need to\nconsider different aspects of its relational neigh-\nbors, e.g., the neighbor ( bornIn, Chicago) focuses\non the birthplace, while the neighbor ( write, A\nPromised Land ) is concerned with possible careers.\nThe global transformer serialization of pairs as a\nsequence may lead to two problems: First, seri-\nalizing neighbors disregards the structure of the\ngraph. Second, the importance of each element in\nthe sequence is the same, and even elements that\nare not relevant for the inference will exchange in-\nformation, e.g., bornIn and A Promised Land in\nthe example above. To realize a differentiated ag-\ngregation between different neighbor pairs while\npreserving the graph structure, we use a context\n5992\ntransformer module as in ( Chen et al. , 2021). In-\ntuitively, given the output of the local transformer\nand the [CLS] embedding, the context transformer\ncontextualizes the target entity with type-class and\nrelational neighbors knowledge from its neighbor-\nhood graph, details of the context transformer can\nbe found in ( Chen et al. , 2021). The output embed-\nding of [CLS], denoted as Ccls ∈ Rd×1, is used\nfor the ﬁnal entity type prediction, which is deﬁned\nas\nSctx = WRelu([Ccls]) + b, where Sctx ∈ RL×1.\n3.3 Pooling\nFor an entity e, the local, global, and context trans-\nformers may generate multiple entity typing infer-\nence results. To address this, we adopt an expo-\nnentially weighted pooling method to aggregate\nprediction results ( Pan et al. , 2021; Stergiou et al. ,\n2021), formulated as follows:\nSe = pool({Sloc\n0 , Sloc\n1 , ..., Sloc\nm+n−1, Sglo, Sctx})\nSe ∈ RL\nrepresents the relevance score between e\nand its types, and n (m) is the number of type-class\n(relational) neighbors of e respectively. For\nsimplicity, we will omit the identiﬁers ( loc,glo,ctx).\nWe unify the numerical order of the output results\nof the local, global, and context transformers as\nfollows:\nSe=pool({S0, S1, ..., Sm+n−1, Sm+n, Sm+n+1})\n=\nm+n+1∑\ni=0\nwiSi, w i = exp αSi\n∑m+n+1\nj=0 exp αSj\nSi\nWe further apply a sigmoid function to Se, denoted\nas se = σ(Se), to map the scores between 0 and 1,\nwhere the higher the value of se,k of se, the more\nlikely is e to have type k.\n3.4 Optimization Strategy\nTo train a model with positive sample score se,k\n(representing that (e, has_type, k ) exists in a KG)\nand negative sample score s′\ne,k\n(representing that\n(e, has_type, k ) does not exist in KG), usually bi-\nnary cross-entropy (BCE) is used as the loss func-\ntion. However, there may exist a serious false neg-\native problem, i.e., some (e, has_type, k ) are valid,\nbut they are missing in existing KGs. To overcome\nthis problem, false-negative aware loss functions\n(FNA) have been proposed ( Pan et al. , 2021). Basi-\ncally, they assign lower weight to negative samples\nwith too high or too low relevance scores. We in-\ntroduce a steeper false-negative aware (SFNA) loss\nfunction which gives more penalties to negative\nsamples with too high or too low relevance scores.\nThe negative sample score is deﬁned as:\nf(x) =\n{ 3 x − 2 x2, x < = 0 .5\nx − 2 x2 + 1 , x > 0.5\nFor the positive score se,k and negative score s′\ne,k\n,\nthe SFNA loss is deﬁned as follows:\nL = −\n∑\nf(s′\ne,k)log(1 − s′\ne,k) −\n∑\nlog(se,k)\n4 Experiments\nIn this section, we discuss the evaluation of TET\nrelative to twelve baselines on a wide array of entity\ntyping benchmarks. We ﬁrst describe datasets and\nbaseline models (Sec. 4.1). Then we discuss the\nexperimental results (Sec. 4.2). Finally, we present\nablation study experiments (Sec. 4.3).\n4.1 Datasets and Baselines\nDatasets. We evaluate our proposed TET model\non two real-world knowledge graphs: FB15k ( Bor-\ndes et al. , 2013) and YAGO43k ( Moon et al. , 2017)\nwhich are the subgraphs of Freebase ( Bollacker\net al. , 2008) and YAGO ( Suchanek et al. , 2007), re-\nspectively. FB15kET and YAGO43kET provide en-\ntity type instances which map entities from FB15k\nand YAGO43k to corresponding entity types. For\nfairness of the experimental comparison, we fol-\nlowed the standard train/test split as in the base-\nlines. The basic statistics of all datasets are shown\nin Table 2.\nBaselines. We compare TET with twelve state-\nof-the-art entity typing methods, and their vari-\nants. We consider the embedding-based models\nETE (\nMoon et al. , 2017), ConnectE ( Zhao et al. ,\n2020), CORE ( Ge et al. , 2021), AttEt ( Zhuo et al. ,\n2022) and CET ( Pan et al. , 2021). We consider\nthe GCN-based models HMGCN ( Jin et al. , 2019),\nRACE2T ( Zou et al. , 2022), ConnectE-MRGAT\n(Zhao et al. , 2022), CompGCN ( Vashishth et al. ,\n2020) and RGCN ( Pan et al. , 2021). We also use as\nbaselines two transformer-based methods for KGC,\nCoKE and HittER ( Wang et al. , 2019; Chen et al. ,\n2021). It should be noted that in all reported exper-\nimental results, the bold numbers denote the best\nresults while the underlined ones the second best .\n4.2 Experimental Results\nTable 1 presents the evaluation results of entity\ntype prediction on FB15kET and YAGO43kET. We\n5993\nDatasets FB15kET YAGO43kET\nMetrics MRR Hit@1 Hit@3 Hit@10 MRR Hit@1 Hit@3 Hit@10\nEmbedding-based methods\nETE ( Moon et al. , 2017)♢ 0.500 0.385 0.553 0.719 0.230 0.137 0.263 0.422\nConnectE ( Zhao et al. , 2020)♢ 0.590 0.496 0.643 0.799 0.280 0.160 0.309 0.479\nCORE-RotatE ( Ge et al. , 2021)♢ 0.600 0.493 0.653 0.811 0.320 0.230 0.366 0.510\nCORE-ComplEx ( Ge et al. , 2021)♢ 0.600 0.489 0.663 0.816 0.350 0.242 0.392 0.550\nAttEt ( Zhuo et al. , 2022)♢ 0.620 0.517 0.677 0.821 0.350 0.244 0.413 0.565\nCET-BCE ( Pan et al. , 2021)♢ 0.682 0.593 0.733 0.852 0.472 0.362 0.540 0.669\nCET-FNA (Pan et al. , 2021)♢ 0.697 0.613 0.745 0.856 0.503 0.398 0.567 0.696\nGCN-based methods\nHMGCN ( Jin et al. , 2019)♦ 0.510 0.390 0.548 0.724 0.250 0.142 0.273 0.437\nConnectE-MRGAT ( Zhao et al. , 2022)♢ 0.630 0.562 0.662 0.804 0.320 0.243 0.343 0.482\nRACE2T ( Zou et al. , 2022)♢ 0.640 0.561 0.689 0.817 0.340 0.248 0.376 0.523\nCompGCN-BCE ( Vashishth et al. , 2020)♦ 0.657 0.568 0.704 0.833 0.357 0.274 0.384 0.520\nCompGCN-FNA ( Vashishth et al. , 2020)♦ 0.665 0.578 0.712 0.839 0.355 0.274 0.383 0.513\nRGCN-BCE ( Pan et al. , 2021)♢ 0.662 0.571 0.711 0.836 0.357 0.266 0.392 0.533\nRGCN-FNA ( Pan et al. , 2021)♢ 0.679 0.597 0.722 0.843 0.372 0.281 0.409 0.549\nTransformer-based methods\nCoKE ( Wang et al. , 2019)♦ 0.465 0.379 0.510 0.624 0.344 0.244 0.387 0.542\nHittER ( Chen et al. , 2021)♦ 0.422 0.333 0.466 0.588 0.240 0.163 0.259 0.390\nTET-BCE 0.699 0.615 0.748 0.862 0.492 0.385 0.554 0.684\nTET-FNA 0.701 0.608 0.761 0.873 0.508 0.405 0.567 0.696\nTET-SFNA-no-class 0.706 0.626 0.749 0.862 0.472 0.375 0.525 0.654\nTET-SFNA 0.717 0.638 0.762 0.872 0.510 0.408 0.571 0.695\nTable 1: Evaluation of different models on FB15kET and YAGO43kET. ♢ results are from the original papers.\n♦ results are from our implementation of the corresponding models. TET-SFNA-no-class means that type-class\nneighbors were not used, and for YAGO43kET in addition no semantic enhancement on relations is used.\nDatasets FB15kET YAGO43kET\n# Entities 14,951 42,335\n# Relations 1,345 37\n# Types 3,584 45,182\n# Clusters 1,081 6,583\n# Train.triples 483,142 331,686\n# Train.tuples 136,618 375,853\n# Valid 15,848 43,111\n# Test 15,847 43,119\nTable 2: Statistics of Datasets.\ncan observe that our model TET outperforms all\nbaselines in terms of basically all metrics. These\nresults demonstrate that transformers more effec-\ntively encode the neighbor information of an entity.\nSpeciﬁcally, when using the BCE and FNA loss\nfunctions, TET meets or exceeds the CET model\n(the best performing baseline). By using the SFNA\nloss function, we can get further performance im-\nprovement, especially in the MRR and Hit@1 met-\nrics on FB15kET. Furthermore, TET has different\ngains compared to CET with respect to the Hit met-\nrics. The improvement on Hit@1 is higher than\non Hit@3 and Hit@10 because by using three dif-\nferent transformer modules TET can encode the\nneighborhood information of an entity at three dif-\nferent levels of granularity. Further, if we do not\nuse type-class neighbors and for the YAGO43kET\ndataset the type-class enrichment on relations is\nnot present (TET-SNFA-no-class), we note that the\nperformance of TET on the YAGO43kET dataset\ndecreases considerably. Intuitively, the decrease\non the YAGO43kE is larger than on FB15k be-\ncause the graph structure of YAGO43k is sparser,\nhas fewer relations, and a large number of types,\nmaking the semantic type-class knowledge crucial.\n4.3 Ablation Studies\nTo verify the impact of each TET model component\non the performance, we conduct ablation studies on\nFB15kET and YAGO43kET. In particular we look\nat the effect of: a) different transformer modules,\nTable 3; b) different neighbor content, Table 4;\nc) different integration methods on YAGO43kET,\nTable 5; d) different dropping rates, Table 6; e) the\nnumber of hops, Table 7.\nEffect of Transformer. The local transformer by\nitself performs better than the global one by itself.\nThis indicates that considering independently the\nneighbors of an entity can reduce interference be-\n5994\nModels FB15kET YAGO43kET\nGlobal Local Context MRR Hit@1 Hit@3 Hit@10 MRR Hit@1 Hit@3 Hit@10\n√ √ √ 0.717 0.638 0.762 0.872 0.510 0.408 0.571 0.695√ √ 0.713 0.632 0.759 0.871 0.503 0.401 0.561 0.690√ √ 0.664 0.578 0.711 0.829 0.369 0.289 0.397 0.524√ √ 0.700 0.614 0.752 0.864 0.509 0.407 0.568 0.697\n√ 0.660 0.578 0.702 0.824 0.365 0.286 0.392 0.517√ 0.684 0.596 0.732 0.859 0.494 0.387 0.555 0.690√ 0.641 0.554 0.686 0.817 0.353 0.280 0.375 0.493\nTable 3: Evaluation of ablation study with different transformer modules combinations.\ntween unrelated types. By combining the global\nand context transformer, more complex types can\nbe inferred from the token and graph structure level,\nachieving state-of-the-art results. Note that both\nthe global and context transformers deal with com-\nplex types, but the context one further takes into\naccount the relevance of different neighbors while\npreservin the structure of the KG. As one can see\nfrom the results, for the used datasets, the global\ntransformer is already doing most of the work, i.e.,\nthe combination of local and global transformers\nachieves almost the same result as when the con-\ntext one is also incorporated. We believe that in\ndatasets with a more complex structure the context\ntransformer could play a more prominent role, we\nleave this line of research as future work.\nEffect of Neighbor Content. We observe that\nthe impact of relational neighbors is greater than\nthat of type-class neighbors. Indeed, removing rela-\ntional neighbors leads to a substantial performance\ndegradation in YAGO43kET. When both of them\nare available, type-class neighbors might help re-\nlational ones to distinguish between relevant and\nirrelevant types for an inference.\nFB15kET\nrelational type-class MRR Hit@1 Hit@3 Hit@10√ √ 0.717 0.638 0.762 0.872√ 0.657 0.568 0.707 0.833√ 0.654 0.561 0.705 0.839\nYAGO43kET√ √ 0.510 0.408 0.571 0.695√ 0.467 0.372 0.518 0.642√ 0.373 0.288 0.405 0.535\nTable 4: Evaluation of ablation study with different\nneighbor content.\nEffect of Integration Methods. YAGO43kET\nhas a sparser graph structure, fewer types of rela-\ntions and a large number of types. To tackle this\nModels YAGO43kET\nNo Avg Max Min MRR Hit@1 Hit@3 Hit@10\n√ 0.491 0.385 0.554 0.684√ 0.510 0.408 0.571 0.695√ 0.505 0.404 0.564 0.688√ 0.505 0.405 0.563 0.691\nTable 5: Evaluation of ablation study with different\nintegration methods. Note that, \"No\" means without\nperforming type-class semantic enhancement on the\nrelations.\nproblem, in Section 3.2.2, we have enriched the\nrepresentations of relations in relational neighbors\nwith type-class knowledge. One can observe that\nthe Avg operation outperforms Min and Max be-\ncause the latter tend to discard useful content.\nDropping Rates 75% 90%\nModels MRR Hit@1 Hit@3 MRR Hit@1 Hit@3\nCompGCN 0.648 0.559 0.697 0.633 0.544 0.679\nRGCN 0.648 0.560 0.694 0.626 0.534 0.673\nCET 0.670 0.580 0.721 0.646 0.553 0.698\nTET_RSE 0.683 0.599 0.733 0.645 0.555 0.692\nTET 0.689 0.606 0.733 0.658 0.574 0.701\nTable 6: Evaluation with different dropping rates on\nFB15kET. TET_RSE represents TET with semantic en-\nhancement on relations.\nEffect of Dropping Rates. In real life KGs,\nmany entities have sparse relations with other enti-\nties. In particular, they have few relational neigh-\nbors but a large number of types, so for their in-\nference we lack structural relational information.\nIndeed, in YAGO43kET about 4.73% of its en-\ntities have ﬁve times more types than relational\nneighbors (\nZhuo et al. , 2022). To further test the\nrobustness of TET under relation-sparsity, we also\nconduct ablation experiments on FB15kET by ran-\n5995\ndomly removing 25%, 50%, 75%, and 90% of the\nrelational neighbors of entities. We ﬁnd that with\nthe continuous increase of the sparsity ratio, the\nperformance of baselines decrease to varying de-\ngrees, but TET still achieves the best results under\nall sparsity conditions. We also consider TET with\nsemantic enhancement on relations since by ran-\ndomly dropping neighbors the number of relations\nmight also be reduced. However, not enough rela-\ntions are removed to have a positive effect. Another\nreason for not having positive effect is that the num-\nber of types in FB15kET is substantially smaller\nthan in YAGO43kET. Table\n6 shows results for\n75%, and 90%, for missing results see appendix.\nModels FB15kET\n1-hop 2-hops 3-hops MRR Hit@1 Hit@3 Hit@10\n√ 0.717 0.638 0.762 0.872√ 0.677 0.592 0.720 0.844√ 0.682 0.597 0.728 0.850\n√ √ 0.709 0.626 0.759 0.869√ √ 0.710 0.630 0.756 0.868√ √ 0.680 0.598 0.721 0.845√ √ √ 0.709 0.630 0.754 0.865\nTable 7: Evaluation of ablation study with different\nnumber of hops on FB15kET.\nEffect of Number of Hops.For relational neigh-\nbors, TET only considers one-hop information i.e.,\nonly the information around their direct neighbors.\nWe also conduct an ablation study on the effect\nof using different number of hops. In principle\nmulti-hop information could provide richer struc-\ntural knowledge, increasing the discrimination of\nrelational neighbors. Indeed, a positive effect of\nmulti-hop information has been witnessed in sev-\neral approaches to KGC. However, our experimen-\ntal results show that the noise introduced by inter-\nmediate entities is more dominant than the addi-\ntional knowledge n-hop entities and relations pro-\nvide. Intuitively, for KGC multi-hop information\nmakes a difference as it exploits the topological\nstructure of the KG (i.e. how entities are related).\nHowever, in the input KG, types are not related\nbetween them and as our experiments show, one\ncan not lift the topological structure at the entity-\nlevel to the type one, explaining why there is no\ngain from considering multi-hop information. It\nwould interesting to conﬁrm this observation by us-\ning GCNs, which more naturally capture multi-hop\ninformation.\n5 Conclusions\nIn this paper, we propose a novel transformer-based\nmodel for KGET which utilizes contextual infor-\nmation of entities to infer missing types for KGs\nwith minimal schema information. TET has three\nmodules allowing to encode local and global neigh-\nborhood information from different perspectives.\nWe also enhance the representation of entities by\nusing class membership knowledge of types. We\nexperimentally showed the beneﬁts of our model.\n6 Limitations\nOur TET model currently suffers from two lim-\nitations. From the methodological viewpoint, a\ntransformer mechanism introduces more parame-\nters than embedding-based methods, bringing some\ncomputational burden and memory overhead, but\nthey are tolerable. Also, there exist other impor-\ntant tasks related to types, e.g. ﬁne-grained entity\ntyping, aiming at classifying entity mentions into\nﬁne-grained semantic labels. TET is currently not\nappropriate for this kind of tasks.\nAcknowledgments\nThis work has been supported by the National Nat-\nural Science Foundation of China (No.61936012),\nby the National Key Research and Development\nProgram of China (No.2020AAA0106100), by the\nNational Natural Science Foundation of China\n(No. 62076155), by a Leverhulme Trust Research\nProject Grant (RPG-2021-140), and by the Chang\nJiang Scholars Program (J2019032).\nReferences\nKurt D. Bollacker, Colin Evans, Praveen K. Paritosh,\nTim Sturge, and Jamie Taylor. 2008. Freebase: a\ncollaboratively created graph database for structuring\nhuman knowledge . In Proceedings of the ACM SIG-\nMOD International Conference on Management of\nData, SIGMOD 2008, Vancouver, BC, Canada, June\n10-12, 2008 , pages 1247–1250. ACM.\nAntoine Bordes, Nicolas Usunier, Alberto García-\nDurán, Jason Weston, and Oksana Yakhnenko.\n2013.\nTranslating embeddings for modeling multi-\nrelational data . In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013, Lake\nTahoe, Nevada, United States , pages 2787–2795.\nSanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021. Hitter: Hierar-\nchical transformers for knowledge graph embeddings .\n5996\nIn Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2021, Virtual Event / Punta Cana, Dominican Re-\npublic, 7-11 November, 2021 , pages 10395–10407.\nAssociation for Computational Linguistics.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2018. Convolutional 2d knowl-\nedge graph embeddings . In Proceedings of the Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence,\n(AAAI-18), the 30th innovative Applications of Arti-\nﬁcial Intelligence (IAAI-18), and the 8th AAAI Sym-\nposium on Educational Advances in Artiﬁcial Intel-\nligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018 , pages 1811–1818. AAAI Press.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers) ,\npages 4171–4186. Association for Computational\nLinguistics.\nHady ElSahar, Christophe Gravier, and Frédérique\nLaforest. 2018. Zero-shot question generation from\nknowledge graphs for unseen predicates and entity\ntypes. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2018, New Orleans, Louisiana,\nUSA, June 1-6, 2018, Volume 1 (Long Papers) , pages\n218–228. Association for Computational Linguistics.\nXiou Ge, Yuncheng Wang, Bin Wang, and C.-C. Jay\nKuo. 2021. CORE: A knowledge graph entity type\nprediction method via complex space regression and\nembedding. CoRR, abs/2112.10067.\nNitish Gupta, Sameer Singh, and Dan Roth. 2017. En-\ntity linking via joint encoding of types, descriptions,\nand context . In Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2017, Copenhagen, Denmark, Septem-\nber 9-11, 2017 , pages 2681–2690. Association for\nComputational Linguistics.\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec.\n2017. Inductive representation learning on large\ngraphs. In Advances in Neural Information Process-\ning Systems 30: Annual Conference on Neural In-\nformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA , pages 1024–1034.\nZhiwei Hu, Víctor Gutiérrez-Basulto, Zhiliang Xiang,\nXiaoli Li, Ru Li, and Jeff Z. Pan. 2022. Type-aware\nembeddings for multi-hop reasoning over knowledge\ngraphs. CoRR, abs/2205.00782.\nHailong Jin, Lei Hou, Juanzi Li, and Tiansi Dong.\n2019. Fine-grained entity typing via hierarchical\nmulti graph convolutional networks . In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 4968–4977. Association for\nComputational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization . In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings .\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. In 5th International Conference on Learn-\ning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings .\nOpenReview.net.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020.\nALBERT: A lite BERT for self-supervised\nlearning of language representations . In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nQing Liu, Hongyu Lin, Xinyan Xiao, Xianpei Han,\nLe Sun, and Hua Wu. 2021. Fine-grained entity\ntyping via label reasoning . In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021, Virtual Event\n/ Punta Cana, Dominican Republic, 7-11 November,\n2021, pages 4611–4622. Association for Computa-\ntional Linguistics.\nYang Liu, Kang Liu, Liheng Xu, and Jun Zhao. 2014.\nExploring ﬁne-grained entity type constraints for dis-\ntantly supervised relation extraction . In COLING\n2014, 25th International Conference on Computa-\ntional Linguistics, Proceedings of the Conference:\nTechnical Papers, August 23-29, 2014, Dublin, Ire-\nland, pages 2107–2116. Association for Computa-\ntional Linguistics.\nChangsung Moon, Paul Jones, and Nagiza F. Samatova.\n2017. Learning entity type embeddings for knowl-\nedge graph completion . In Proceedings of the 2017\nACM on Conference on Information and Knowledge\nManagement, CIKM 2017, Singapore, November 06 -\n10, 2017 , pages 2215–2218. ACM.\nYasumasa Onoe, Michael Boratko, Andrew McCallum,\nand Greg Durrett. 2021. Modeling ﬁne-grained en-\ntity types with box embeddings . In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021 , pages 2051–2064. Associa-\ntion for Computational Linguistics.\nJeff Z. Pan. 2009. Resource description framework. In\nHandbook on Ontologies , pages 71–90. Springer.\n5997\nJeff Z. Pan, Mei Zhang, Kuldeep Singh, Frank Van\nHarmelen, Jinguang Gu, and Zhi Zhang. 2019. Entity\nEnabled Relation Linking. In Proc. of 18th Interna-\ntional Semantic Web Conference (ISWC 2019) , pages\n523–538.\nJ.Z. Pan, G. Vetere, J.M. Gomez-Perez, and H. Wu.\n2016. Exploiting Linked Data and Knowledge\nGraphs for Large Organisations . Springer.\nWeiran Pan, Wei Wei, and Xian-Ling Mao. 2021.\nContext-aware entity typing in knowledge graphs . In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, Virtual Event / Punta Cana,\nDominican Republic, 16-20 November, 2021 , pages\n2240–2250. Association for Computational Linguis-\ntics.\nJing Qian, Yibin Liu, Lemao Liu, Yangming Li, Haiyun\nJiang, Haisong Zhang, and Shuming Shi. 2021. Fine-\ngrained entity typing without knowledge base . In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2021, Virtual Event / Punta Cana, Dominican Repub-\nlic, 7-11 November, 2021 , pages 5309–5319. Associ-\nation for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks .\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019 , pages 3980–3990.\nAssociation for Computational Linguistics.\nMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter\nBloem, Rianne van den Berg, Ivan Titov, and Max\nWelling. 2018. Modeling relational data with graph\nconvolutional networks . In The Semantic Web - 15th\nInternational Conference, ESWC 2018, Heraklion,\nCrete, Greece, June 3-7, 2018, Proceedings , volume\n10843 of Lecture Notes in Computer Science , pages\n593–607. Springer.\nAlexandros Stergiou, Ronald Poppe, and Grigorios\nKalliatakis. 2021. Reﬁning activation downsampling\nwith softpool . In 2021 IEEE/CVF International Con-\nference on Computer Vision, ICCV 2021, Montreal,\nQC, Canada, October 10-17, 2021 , pages 10337–\n10346. IEEE.\nFabian M. Suchanek, Gjergji Kasneci, and Gerhard\nWeikum. 2007. Yago: a core of semantic knowledge .\nIn Proceedings of the 16th International Conference\non World Wide Web, WWW 2007, Banff, Alberta,\nCanada, May 8-12, 2007 , pages 697–706. ACM.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2019. Rotate: Knowledge graph embedding\nby relational rotation in complex space . In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nThéo Trouillon, Johannes Welbl, Sebastian Riedel, Éric\nGaussier, and Guillaume Bouchard. 2016. Com-\nplex embeddings for simple link prediction . In Pro-\nceedings of the 33nd International Conference on\nMachine Learning, ICML 2016, New York City, NY,\nUSA, June 19-24, 2016 , volume 48 of JMLR Work-\nshop and Conference Proceedings , pages 2071–2080.\nJMLR.org.\nShikhar Vashishth, Soumya Sanyal, Vikram Nitin, and\nPartha P. Talukdar. 2020. Composition-based multi-\nrelational graph convolutional networks . In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017.\nAttention is all\nyou need . In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA , pages 5998–6008.\nQuan Wang, Pingping Huang, Haifeng Wang, Songtai\nDai, Wenbin Jiang, Jing Liu, Yajuan Lyu, Yong Zhu,\nand Hua Wu. 2019. Coke: Contextualized knowledge\ngraph embedding . CoRR, abs/1911.02168.\nChuhan Wu, Fangzhao Wu, and Yongfeng Huang.\n2021a. Da-transformer: Distance-aware transformer .\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021 , pages\n2059–2068. Association for Computational Linguis-\ntics.\nChuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng\nHuang. 2021b. Hi-transformer: Hierarchical interac-\ntive transformer for efﬁcient and effective long docu-\nment modeling . In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing, ACL/IJCNLP 2021,\n(Volume 2: Short Papers), Virtual Event, August 1-6,\n2021, pages 848–853. Association for Computational\nLinguistics.\nYu Zhao, Anxiang Zhang, Ruobing Xie, Kang Liu, and\nXiaojie Wang. 2020. Connecting embeddings for\nknowledge graph entity typing . In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 6419–6428. Association for Computa-\ntional Linguistics.\nYu Zhao, Han Zhou, Anxiang Zhang, Ruobing Xie,\nQing Li, and Fuzhen Zhuang. 2022. Connecting\nembeddings based on multiplex relational graph at-\ntention networks for knowledge graph entity typing.\nIEEE Transactions on Knowledge and Data Engi-\nneering.\n5998\nJianhuan Zhuo, Qiannan Zhu, Yinliang Yue, Yuhong\nZhao, and Weisi Han. 2022. A neighborhood-\nattention ﬁne-grained entity typing for knowledge\ngraph completion . In WSDM ’22: The Fifteenth ACM\nInternational Conference on Web Search and Data\nMining, Virtual Event / Tempe, AZ, USA, February\n21 - 25, 2022 , pages 1525–1533. ACM.\nChanglong Zou, Jingmin An, and Guanyu Li. 2022.\nKnowledge graph entity type prediction with re-\nlational aggregation graph attention network\n. In\nThe Semantic Web - 19th International Conference,\nESWC 2022, Hersonissos, Crete, Greece, May 29 -\nJune 2, 2022, Proceedings , volume 13261 of Lecture\nNotes in Computer Science , pages 39–55. Springer.\n5999\nAppendix\nA Details about Experiments\nIn this section, we give more experimental details\nand discuss the evaluation protocol.\nParameter {FB15kET, YAGO43kET}\n# Embedding dim {100, 100}\n# Train or valid batch size {128, 128}\n# Test batch size {1, 1}\n# Learning rate {0.001, 0.001}\n# Trm layers {3, 3}\n# Trm hidden dim {480, 480}\n# Trm heads {4, 4}\n# Trm dropout rate {0.2, 0.2}\n# Type neighbor sample size {3, 3}\n# KG neighbor sample size {7, 8}\n# Warmup epochs {50, 50}\n# Valid epochs {25, 25}\n# α {0.5, 0.5}\n# Epochs {500, 500}\nTable 8: The main hyperparameters of TET model in\ndifferent datasets.\nExperimental Setting. Table 8 shows the hy-\nperparameter settings of the TET model on the\nFB15kET and YAGO43kET datasets. We use\nAdam ( Kingma and Ba , 2015) as the optimizer, the\nhyperparameters are tuned according to the MRR\non the validation set. We only sample the entity\ntype-class and relational neighbors during training\nand validation, but for testing we use all the neigh-\nbors of the entity for prediction, so the test batch\nsize is set to 1. We adopted the warmup training\nstrategy, keeping the initial learning rate 0.001 un-\nchanged for the ﬁrst 50 iterations, but after that we\ndivided the learning rate by 5, and set the interval\nto be the current interval multiplied by 2. In Table 8\n\"Trm\" refers to the three transformer modules, we\nuse the same number of layers, hidden dim, and\nheads.\nEvaluation Protocol. For each test sample (e, c ),\nwe ﬁrst calculate the correlation score se between\nthe entity e and type c, and then sort them in\ndescending order. We report various metrics for\nevaluation, speciﬁcally, we adopt the ﬁltered set-\nting ( Bordes et al. , 2013; Pan et al. , 2021) for com-\nputing mean reciprocal rank (MRR), and the pro-\nportion of correct entities ranked in the top 1/3/10\n(Hit@1, Hit@3, Hit@10).\nB Additional Results\nIn this section, we discuss more ablation experi-\nmental results that are not included in the main part\nof the paper.\nEffect of Dropping Rates: Number of Relational\nNeighbors. In the main body of the paper we dis-\ncussed why we test the robustness of TET under\nrelation-sparsity on FB15kET by randomly remov-\ning 25%, 50%, 75%, and 90% of the relational\nneighbors of entities. Due to space constraints we\nonly presented the results for the two more extreme\ncases: 75%, and 90%. Table 9 shows the missing\nresults for 25% and 50%.\nDropping Rates 25% 50%\nModels MRR Hit@1 Hit@3 MRR Hit@1 Hit@3\nCompGCN 0.661 0.573 0.705 0.655 0.565 0.702\nRGCN 0.673 0.590 0.716 0.667 0.584 0.708\nCET 0.697 0.613 0.744 0.687 0.601 0.733\nTET_RSE 0.699 0.613 0.748 0.698 0.613 0.749\nTET 0.712 0.631 0.758 0.705 0.624 0.753\nTable 9: Evaluation with different dropping rates on\nFB15kET. TET_RSE represents TET with semantic en-\nhancement on relations.\nDropping Rates 25% 50%\nModels MRR Hit@1 Hit@3 MRR Hit@1 Hit@3\nCompGCN 0.664 0.578 0.708 0.662 0.574 0.708\nRGCN 0.676 0.593 0.719 0.673 0.590 0.719\nCET 0.699 0.617 0.743 0.694 0.610 0.742\nTET_RSE 0.708 0.628 0.754 0.712 0.634 0.755\nTET 0.711 0.631 0.756 0.710 0.630 0.757\nDropping Rates 75% 90%\nModels MRR Hit@1 Hit@3 MRR Hit@1 Hit@3\nCompGCN 0.653 0.565 0.699 0.637 0.546 0.683\nRGCN 0.658 0.573 0.702 0.636 0.548 0.681\nCET 0.675 0.588 0.721 0.653 0.564 0.700\nTET_RSE 0.687 0.604 0.733 0.675 0.591 0.718\nTET 0.690 0.608 0.734 0.677 0.591 0.722\nTable 10: Evaluation with different dropping rates on\nrelations on FB15kET. The TET_RSE represents TET\nwith semantic enhancement on relations.\nEffect of Dropping Rates: Number of Relation\nTypes. In Section 3.2.2, we enhanced relations\nwith semantic knowledge for the YAGO43kE KG.\nThe main reason for doing this only for YAGO43kE\nis that it contains only very few relation types (cf.\nTable 2), making the discrimination among relation\ntriples harder. As a ﬁrst step towards having a bet-\nter understanding of the interplay of the number of\nrelation types and the number of types in a KG, we\n6000\nconduct an ablation study in which on FB15kET\nwe randomly remove 25%, 50%, 75%, and 90% of\nthe relation types. The results in Table 10 show that\nin this case enhancing relation types with semantic\nknowledge does not have a positive effect, unlike\nfor YAGO43kE. We believe that the main reason\nbehind this is that YAGO43kE does not only have\nvery few relations, but also a very large number\nof types. To have a precise understanding, a dedi-\ncated deep analysis of the interplay of the number\nof types, the number of relation types, and other\nstructural characteristics of KGs is required - it is\nout of the scope of this paper, but it is an interesting\nquestion for future work.\n6001",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7768442034721375
    },
    {
      "name": "Transformer",
      "score": 0.612986147403717
    },
    {
      "name": "Entity linking",
      "score": 0.5966871976852417
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45984163880348206
    },
    {
      "name": "Natural language processing",
      "score": 0.3941681683063507
    },
    {
      "name": "Information retrieval",
      "score": 0.3715812563896179
    },
    {
      "name": "Knowledge base",
      "score": 0.1812283992767334
    },
    {
      "name": "Engineering",
      "score": 0.0857953131198883
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}