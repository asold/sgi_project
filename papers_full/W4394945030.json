{
    "title": "Deferred Continuous Batching in Resource-Efficient Large Language Model Serving",
    "url": "https://openalex.org/W4394945030",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2107187604",
            "name": "He Yongjun",
            "affiliations": [
                "ETH Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2103997928",
            "name": "Yao Lu",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2107150329",
            "name": "Gustavo Alonso",
            "affiliations": [
                "ETH Zurich"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4387321091",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W4282546806",
        "https://openalex.org/W4372262787",
        "https://openalex.org/W4386709668",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W2963976294",
        "https://openalex.org/W4285247752",
        "https://openalex.org/W4390306161",
        "https://openalex.org/W3081168214",
        "https://openalex.org/W4394944658",
        "https://openalex.org/W4387321109",
        "https://openalex.org/W4402595168",
        "https://openalex.org/W4402593114",
        "https://openalex.org/W398859631"
    ],
    "abstract": "Despite that prior work of batched inference and parameter-efficient fine-tuning techniques have reduced the resource requirements of large language models (LLMs), challenges remain in resource-constrained environments such as on-premise infrastructures to serve workload that is composed of both inference and fine-tuning jobs. Prior solutions must either pause existing jobs which causes service interruptions, or queue new jobs which results in a long delay.",
    "full_text": null
}