{
  "title": "Direction is what you need: Improving Word Embedding Compression in Large Language Models",
  "url": "https://openalex.org/W3171601240",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4287302104",
      "name": "Bałazy, Klaudia",
      "affiliations": [
        "Jagiellonian University"
      ]
    },
    {
      "id": "https://openalex.org/A4224915124",
      "name": "Banaei, Mohammadreza",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A4224915125",
      "name": "Lebret, Remi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A4226501789",
      "name": "Tabor, Jacek",
      "affiliations": [
        "Jagiellonian University"
      ]
    },
    {
      "id": "https://openalex.org/A2744667012",
      "name": "Aberer Karl",
      "affiliations": [
        null
      ]
    },
    {
      "id": null,
      "name": "Ba{\\l}azy, Klaudia",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Lebret, R\\'emi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2980420460",
    "https://openalex.org/W2515385951",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2970418186",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3115511229",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4297790889",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2117756735",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2176412452",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2963285578",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2962965870",
    "https://openalex.org/W3016263386",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2469490737",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2980358443",
    "https://openalex.org/W2267635276",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3104216863",
    "https://openalex.org/W4295262505",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2131834529",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2251939518"
  ],
  "abstract": "The adoption of Transformer-based models in natural language processing (NLP)\\nhas led to great success using a massive number of parameters. However, due to\\ndeployment constraints in edge devices, there has been a rising interest in the\\ncompression of these models to improve their inference time and memory\\nfootprint. This paper presents a novel loss objective to compress token\\nembeddings in the Transformer-based models by leveraging an AutoEncoder\\narchitecture. More specifically, we emphasize the importance of the direction\\nof compressed embeddings with respect to original uncompressed embeddings. The\\nproposed method is task-agnostic and does not require further language modeling\\npre-training. Our method significantly outperforms the commonly used SVD-based\\nmatrix-factorization approach in terms of initial language model Perplexity.\\nMoreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several\\ndownstream tasks from the GLUE benchmark, where we also outperform the baseline\\nin most scenarios. Our code is public.\\n",
  "full_text": "Direction is what you need: Improving Word Embedding Compression in\nLarge Language Models\nKlaudia Bałazy†*, Mohammadreza Banaei‡*, Rémi Lebret‡, Jacek Tabor† and Karl Aberer‡\n†Jagiellonian University\nklaudia.balazy@doctoral.uj.edu.pl,jacek.tabor@uj.edu.pl\n‡EPFL\n[mohammadreza.banaei,remi.lebret,karl.aberer]@epfl.ch\nAbstract\nThe adoption of Transformer-based models\nin natural language processing (NLP) has led\nto great success using a massive number of\nparameters. However, due to deployment\nconstraints in edge devices, there has been\na rising interest in the compression of these\nmodels to improve their inference time and\nmemory footprint. This paper presents a\nnovel loss objective to compress token em-\nbeddings in the Transformer-based models by\nleveraging an AutoEncoder architecture. More\nspeciﬁcally, we emphasize the importance of\nthe direction of compressed embeddings with\nrespect to original uncompressed embeddings.\nThe proposed method is task-agnostic and\ndoes not require further language model-\ning pre-training. Our method signiﬁcantly\noutperforms the commonly used SVD-based\nmatrix-factorization approach in terms of\ninitial language model Perplexity. Moreover,\nwe evaluate our proposed approach over\nSQuAD v1.1 dataset and several downstream\ntasks from the GLUE benchmark, where we\nalso outperform the baseline in most scenarios.\nOur code is public.1.\n1 Introduction\nPretraining deep Transformer models (Vaswani\net al., 2017) with language modeling and\nﬁne-tuning these models over downstream tasks\nhave led to great success in recent years (Devlin\net al., 2018; Liu et al., 2019; Yang et al.,\n2019), and even enabled researchers to design\nmodels that outperform human baselines in the\nGLUE benchmark (Wang et al., 2018). Although\nthese models are empirically powerful in many\n∗Equal contribution\n1https://github.com/\nMohammadrezaBanaei/orientation_based_\nembedding_compression\nFigure 1: This ﬁgure presents a two-dimensional\nvisualization of a token embedding vector v with its\ntwo approximations: v′ and v′′. Vector v′ has a larger\nEuclidean distance error than v′′, but its direction is\nmore similar to the reference vector. Our experiments\nshow that v′ generally provides a better approximation\nof the original token compared to v′′.\nnatural language understanding (NLU) tasks, they\noften require a massive number of parameters,\nmaking them hard to use for memory-constrained\napplications (e.g., edge devices). Therefore, there\nhave been efforts to compress BERT-like models\nwhile preserving comparable performance with the\noriginal model.\nMany of these compression methods are based\non knowledge distillation (Hinton et al., 2015) to\nhelp the compressed model (student) to perform\nclose to the original model in different NLU\ntasks. However, these approaches often need high\ncomputation resources due to e.g., the necessity\nof retraining the expensive language modeling on\na huge corpus (Sanh et al., 2019) or the use of\nexpensive augmentation techniques to make the\ndistillation effectively work (Jiao et al., 2019).\nMoreover, compression techniques that rely on\ntraining/ﬁne-tuning language models are becoming\nless feasible due to its ever-increasing cost for\ncurrent state-of-the-art architectures with hundreds\nof millions of parameters (He et al., 2020; Raffel\net al., 2019; Brown et al., 2020).\nMore recently, there have been efforts to\ncompress Transformer-based models for more\nresource-constrained scenarios (Mao et al., 2020)\nby using ofﬂine methods, such as matrix factoriza-\ntion (Winata et al., 2019; Lan et al., 2019; Wang\net al., 2019), weight pruning (Li et al., 2016; Han\net al., 2015), and also weight quantization (Zhou\net al., 2016; Hubara et al., 2016).\nThis paper focuses on token embedding matrix\ncompression due to being one of the largest\nmatrices in BERT-based architectures. We\nspeciﬁcally question the effectiveness of current\nlow-rank matrix factorization methods in recent\nliterature (Lan et al., 2019; Wang et al., 2019)\nby comparing them with the performance of a\nlinear AutoEncoder over different compression\nratios2. We deﬁne a new loss objective which\nis not only dependent on the commonly used\nMean Absolute Error (MAE) or Mean Squared\nError (MSE) loss between input embeddings and\nAutoEncoder reconstruction, but is also sensitive to\nthe noise in reconstructed embeddings \"direction\"\n(measured by cosine distance). We present the\nintuition behind the importance of embedding\nvector direction in the Figure 1. In the following\nsections we show that cosine distance indeed plays\na more critical role than MAE/MSE (Figure 3) as\nmeasured by the Perplexity of the entire model in\nlanguage modeling.\nIn Section 4, we demonstrate that our com-\npression algorithm is superior or competitive to\nthe Singular Value Decomposition (SVD) baseline\nover several natural language understanding tasks\nfrom GLUE (Wang et al., 2018) benchmark, as\nwell as the SQuAD dataset (Rajpurkar et al., 2016)\nfor question answering. We also compare our\nperformance with the SVD-based compression\nover different compression ratios, and speciﬁcally\nshow that our model performs consistently better\nin higher compression ratios.\nOur contribution can be summarized as follows:\n• We demonstrate the importance of direction\n(measured by cosine distance) in token\nembeddings compression.\n• We leverage the AutoEncoder architecture to\nexplore various multi-objective optimization\n2Number of parameters in the original embedding matrix,\nover the sum of the parameters in factorized matrices.\nfunctions.\n• We outperform the SVD-based baseline\nin terms of Perplexity and over various\ndownstream tasks.\n2 Related work\nThe current mostly used compression methods can\nbe roughly categorized into four classes, namely\nknowledge distillation (Hinton et al., 2015), weight\npruning (Li et al., 2016; Han et al., 2015), matrix\nfactorization (Lan et al., 2019; Wang et al., 2019;\nMao et al., 2020) and weight quantization (Zhou\net al., 2016; Hubara et al., 2016). This section\nfocuses on matrix factorization-based methods\nthat are currently used for token embedding\ncompression in the literature.\n2.1 Background: Low-rank matrix\nfactorization\nThis section describes the baseline method that\nwe are comparing our approach with throughout\nthe paper. Let A be n × m embedding matrix\nrepresenting m-dimensional embedding for each\nn different input tokens. The truncated version of\nthe matrix factorization aims to ﬁnd a low-rank\napproximation ˜A of input matrix A (Halko et al.,\n2011):\n˜A = BC , (1)\nwhere B is the size of n ×k and C is the size of\nk × m. When the inner dimension k is smaller\nthan min (n,m), then the approximation is less\nexpensive for storing it and performing further\ncomputations. The objective of this approximation\nis:\nL2(A, ˜A) =\nA − ˜A\n\n2 , (2)\nwhere ∥·∥2 denotes the l2 operator norm. In this\npaper, we use the SVD method as a low-rank matrix\nfactorization baseline to compare our approach.\n2.2 Matrix factorization for token\nembeddings compression\nLan et al. (2019) proposed to use matrix\nfactorization to limit the number of parameters\nin the token embedding matrix, which also\nseparates the Transformer hidden layer dimension\nfrom the size of vocabulary embedding. It is\nespecially important as token embeddings are\nsupposed to be context-independent, but hidden\nlayer representation should be a context-dependent\nrepresentation and hence needs more parameters.\nMoreover, reducing the vocabulary embedding\ndimension reduces the chance of overﬁtting, as\nmany of the tokens are rarely used in downstream\ntasks.\nThere have been more recent efforts that use\nmatrix factorization idea to compress different\nmatrices in the Transformer architecture (Wang\net al., 2019; Mao et al., 2020). For instance,\nMao et al. (2020) proposed an iterative hybrid\napproach that uses matrix factorization together\nwith weight pruning (while distilling knowledge\nfrom a teacher model) until reaching the ﬁnal\ndesired compression ratio. Lioutas et al. (2019)\nalso proposed using a non-linear AutoEncoder\nmodel with knowledge distillation to compress\nword embeddings. However, we later demonstrate\nthat only adding non-linearity indeed results in a\nminor improvement to the resulting compressed\nlanguage model quality.\nIn this paper, we speciﬁcally focus on the\neffectiveness of SVD for compression of the\ntoken embedding matrix and show that Root\nMean Square Error (RMSE) is not an optimal\nfunction to minimize the zero-shot Perplexity of\nthe language model, which is the main criterion\nwhen language models are trained. We propose a\nnew loss objective for linear matrix factorization\nusing AutoEncoder to achieve a task-agnostic\ncompressed language model with reasonable\nPerplexity without further ﬁne-tuning the language\nmodel. In this work, we mainly investigate the\neffectiveness of SVD, and other complementary\nmethods such as knowledge distillation can be used\nlater to further boost the performance.\n3 Model Description\nAlthough SVD matrix-factorization is one of the\nmost popular methods for matrix compression, we\nbelieve it is not an optimal method for compressing\ntoken embeddings in BERT-like architectures. The\nobjective of SVD is to minimize the l2 norm\nbetween the original matrix and the reconstructed\none; however, focusing on l2 norm optimization\nprioritizes the reduction of larger errors, and it may\nend up ignoring more minor vector differences.\nIt is also sensitive to the inﬂuence of outliers.\nThe most crucial reason for the l2 norm not\nbeing the best choice is that it only considers the\ndistance between the original and reconstructed\ntoken vector, and it does not necessarily pay\nattention to the orientation difference between\nthem. In section 4, we demonstrate that vectors\nrepresenting language tokens are more sensitive to\nnoise in their direction rather than to changes in\nEuclidean distance from the reference vector. We\nalso discuss the motivation behind it further in this\nsection.\nIn order to mitigate the problem of focusing\nonly on the largest errors between two vectors, we\npropose replacing the l2 norm objective with the\nl1 norm raised to the power of α:\nLα\n1 (A, ˜A) =\nA − ˜A\nα\n1 , (3)\nwhere A denotes the original embedding matrix,\n˜A denotes the reconstructed embedding matrix,\nand ∥·∥1 denotes the l1 operator norm. Due to\nthe ﬂexibility in our deﬁned loss objective, by\ndecreasing the α parameter, we can control how\nmuch we want to focus on smaller error differences.\nWe may set the α parameter to be a constant value,\nor linearly decrease it during the training. We\ndenote linearly decreasing strategy for α as:\n[t1,t2], (4)\nwhere t1 is a starting value of α and t2 is the\ntarget value to be reached at the end. The intuition\nbehind using a decreasingα is to sequentially make\nthe reconstruction harder for the model during\ntraining (as when the α becomes smaller, small\nreconstruction errors will also be magniﬁed).\nSince we believe that enforcing direction simi-\nlarity between the original and the reconstructed\nembedding vectors is crucial for better language\nmodel performance, we introduce the second loss\nobjective component, namely, cosine distance.\nCosine distance can be interpreted as a measure\nof the difference in orientation of two vectors.\nThis measure has been widely used in NLP for\nﬁnding similar words (Mikolov et al., 2013),\ndocument clustering (Muﬂikhah and Baharudin,\n2009), detecting plagiarism (Folt`ynek et al., 2019),\nand many more. The goal of introducing cosine\ndistance loss as a part of our objective is to enforce\ndirection similarity of each pair of vectors from the\noriginal and reconstructed matrix.\nTaking into consideration all points above,\nwe propose to replace the l2 norm objective\nwith a new multi-objective function consisting of\nl1 norm (raised to the power of α, where α is\na hyper-parameter that can be changed during\nFigure 2: Overview of our AutoEncoder (ours) approach for BERT-like embedding matrix compression.\ntraining) and cosine distance:\nΦα,β(A, ˜A) = Lα\n1 (A, ˜A)+β∗CD (A, ˜A), (5)\nwhere A denotes the original embedding matrix,\n˜A denotes the reconstructed embedding matrix, and\nCD (A, ˜A) represents the mean cosine distance of\nall embedding vector pairs. It is worth noting that it\nis the combination of these two functions that gives\na powerful tool which allows both to optimize the\ndistance and direction of the reconstructed vectors\nto the reference. Focusing only on one of these\nfunctions may lead to suboptimal results. For\ncomparison, we also deﬁne another multi-objective\nfunction which is the combination of l2 norm with\ncosine distance loss:\nΨβ(A, ˜A) = L2(A, ˜A)+β∗CD (A, ˜A). (6)\nIn addition to the new loss function, we propose\nleveraging Auto-Encoder architecture for Φα,β and\nΨβ loss optimization (Equation 5 and 6). We use\na simple AutoEncoder consisting of a one-layer\nEncoder/Decoder without any activation function\nin order to have a fair comparison with the SVD\nbaseline. Using Auto-Encoder enables efﬁcient\nmulti-objective optimization, but it also allows to\nselect the appropriate level of model complexity\nwhen needed. At the end of the Auto-Encoder\ntraining, we extract an approximation of the\noriginal matrix, as shown in Figure 2. We substitute\nthe original embedding matrix with a new module\nconsisting of latent representation of vocabulary\ntokens along with the Decoder module.\n4 Results\nIn this section, we evaluate our approach, which\nis based on using AutoEncoder model with a\nmulti-objective loss function that incorporates\ncosine distance with l1 or l2 norm (Equation 5\nand Equation 6) on the task of BERT-like token\nembedding matrix compression. We compare our\nresults versus the commonly used randomized SVD\nmethod (Halko et al., 2011) to perform low-rank\nmatrix factorization. We have implemented our\ntoken embeddings compression with the PyTorch\nbackend (Paszke et al., 2019) and as an extension\nof Huggingface’s Transformers library (Wolf\net al., 2019), enabling researchers to apply our\ncompression method in most of the existing\nTransformer architectures. It is worth noting that\n0 10 25 50 75 150 400 750\n650\n700\n800\n900\n1,000\n1,100\n1,200\n1,300\n1,400\n1,500\nCosine distance coefﬁcient (β).\nPerplexity\nPerplexity (compression ratio=2.5)\nSVD\nΨβ\nΦ1,β\nΦ[1.0,0.6],β\nΦ[2.0,0.6],β\n0 10 25 50 75 150 400 750\n1,800\n2,000\n3,000\n4,000\n5,000\n6,000\nCosine distance coefﬁcient (β).\nPerplexity\nPerplexity (5.0 compression ratio)\nSVD\nΨβ\nΦ1,β\nΦ[1.0,0.6],β\nΦ[2.0,0.6],β\n0 10 25 50 75 150 400 750\n4,500\n5,000\n6,000\n7,000\n8,000\n9,000\n10,000\n12,000\n14,000\nCosine distance coefﬁcient (β).\nPerplexity\nPerplexity (10.0 compression ratio)\nSVD\nΨβ\nΦ1,β\nΦ[1.0,0.6],β\nΦ[2.0,0.6],β\nFigure 3: The impact of the β coefﬁcient on Perplexity metric (lower is better) in the linear AutoEncoder loss\nfunctions: Φα,β (Equation 5) and Ψβ (Equation 6). In all conﬁgurations we select a ﬁnal model based on the\nbest Perplexity achieved during training. The term [t1,t2] indicates linearly decreasing α parameter (Equation 4).\nSetting β = 0 represents not including cosine distance component in the loss function. We may observe that not\nincluding cosine distance in the loss function as well as making it a too dominant component (very big β) is not\noptimal for achieving good Perplexity. We also present the best Perplexity achieved by the baseline SVD method\nfor three compression ratios: 2.5, 5.0, 10.0. Our approach signiﬁcantly outperforms the baseline in the studied\nscenarios.\nthe ofﬂine training of our compression method on\nBERT-base (Devlin et al., 2018) token embedding\nmatrix takes only few minutes on a single GPU\ndevice.\n4.1 Experiments\nIn this paper, we perform our experiments over\nBERT-base model, but the general idea can be\napplied to the vocabulary embeddings of any\nother similar transformer-based architecture. The\nBERT-Base token embedding matrix consists\nof more than 23 Million parameters which is\naround 21% of all parameters in the model.\nWe evaluate the quality of our ﬁnal compressed\nembeddings on the masked (Devlin et al., 2018)\nlanguage modeling task (using WikiText-103\ntest dataset), GLUE benchmark (Wang et al.,\n2018) downstream tasks and SQuAD v1.1 dataset\n(Rajpurkar et al., 2016). We also analyze results\non other metrics, namely RMSE, MAE and Cosine\nDistance.\nIn Figure 3, we compare the Perplexity score\nachieved by SVD 3 method versus the results\n3For SVD training, we select an iteration that minimizes\nPerplexity over our language modeling dataset.\nachieved by a linear AutoEncoder model with\ndifferent loss conﬁgurations, when compressing\nBERT token embeddings. We speciﬁcally examine\nthe importance of cosine distance coefﬁcient ( β)\nin our studied loss functions over three different\ncompression ratios: 2.5, 5, 10. The loss\nobjective Φt,β (Equation 5) denotes constant\n(during the entire training) α parameter (equals\nto t) and Φ[t1,t2],β denotes linearly decreasing α pa-\nrameter (from t1 to t2). We present results\nwhen α = 0, which represents combination of\nl1 norm with cosine distance, and also when α\nlinearly decreases from 1.0 or from 2.0 to 0.6\n([1.0,0.6] and [2.0,0.6] respectively). These values\nhave been selected experimentally.\nTable 1 presents more metrics to compare SVD\nmethod with our AutoEncoder-based approach.\nWe show the results of the model with the\nbest performing objective function (in terms\nof Perplexity) for a given compression ratio.\nAdditionally, we examine the effect of adding\nnon-linear activation function to this selected\nAutoEncoder model, where it can be seen that the\nimprovements due to addition of non-linearity is\nmarginal.\nWe further validate the quality of our com-\npressed token embeddings by inserting it into\nthe BERT-base architecture and ﬁne-tuning the\nmodel on different downstream tasks from the\nGLUE benchmark (Wang et al., 2018) and on\nthe SQuAD v1.1 (Rajpurkar et al., 2016) dataset.\nTable 2 presents an extensive comparison between\nour best (in terms of perplexity) linear AE and\nthe SVD baseline on eight different downstream\ntasks and over different compression ratios. More\nspeciﬁcally, we can see that our proposed method\nis superior or competitive to the SVD baseline and\nperforms relatively better (compared to baseline)\non higher compression ratios. The original BERT\n(without compression) performance is also added\nfor a better comparison of studied scenarios.\nFigure 4 presents learning curves for three\nselected NLU downstream tasks: SST-2 (Socher\net al., 2013), MRPC (Dolan and Brockett, 2005)\nand SQuAD 1.1 (Rajpurkar et al., 2016). We\nshow results for the compression ratio of 10,\nas we observed more signiﬁcant gain for higher\ncompression ratios.\n4.2 Discussion\nThe experiments presented in Figure 3 conﬁrm\nour claim that the l2 norm alone is not an optimal\nmeasure for evaluating the quality of reconstructed\ntoken embeddings in a Transformer-based archi-\ntecture. We observe that adding cosine distance\nobjective function correlates positively with a\nbetter Perplexity metric (Figure 3) and also with\nhigher performance on downstream tasks (Table 2).\nFigure 3 demonstrates that the best results are\nachieved when the cosine distance coefﬁcient\nβ is a dominant component of the loss function.\nHowever, if the β factor becomes too large,\nthe quality of the solution decreases. Hence,\nwe conclude that taking into account both the\ncommonly used L1/L2 distance and focusing on\nthe direction of the token vectors are indispensable.\nWe show that combining the l2 or l1 norm\nwith the cosine distance into one multi-objective\nloss function and optimizing it by AutoEncoder\nmodel outperforms the baseline SVD Perplexity\nfor all tested compression ratios (Figure 3).\nOur experiments show that depending on the\ncompression ratio l2 or l1 norm may be a better\nchoice. However, they are conclusive that adding\ncosine distance is the key factor.\nMoreover, our approach outperforms SVD in\nterms of accuracy for most GLUE benchmark\ndownstream tasks and on SQuAD v1.1 (Table 2).\nWe also observe that for higher compression ratios,\nour approach outperforms the SVD approach\nmore signiﬁcantly. More importantly, Figure 4\ndemonstrates that using our linear AutoEncoder\ncompressed module in the BERT model generally\nconverges faster than SVD-based compressed\nmodule, which is especially important in few-shot\nlearning scenarios.\nLooking at the results presented in Table 1, we\nmay also reﬂect on the importance of preserving\nthe token vector orientation and its effect on\nPerplexity. More speciﬁcally, the mean cosine\ndistance measures for SVD and our approach are\npretty close, but its effect on Perplexity metric\nis signiﬁcant. Our approach indeed provides a\ncompressed submodule with a much better (lower)\nPerplexity.\nWe also show that only adding a non-linear\nactivation function to the studied AutoEncoder\nmodel has a little effect on improving Perplexity.\nTable 1 presents the effect of modifying the\noriginal linear AutoEncoder architecture by adding\nCR (#Params) Architecture Objective RMSE Cosine Distance MAE Perplexity\n2.5 (~9.38M) SVD l2 0.02233 0.10300 0.01734 1130\nLinear AE (+ ELU) Φ[2.0,0.6],75 0.02427 (0.02431) 0.1024 (0.1028) 0.01896 (0.01902) 669.8 (664.0)\n5.0 (~4.69M) SVD l2 0.02848 0.17490 0.02216 5035\nLinear AE (+ ELU) Ψ400 0.03101 (0.03061) 0.17390 (0.17410) 0.02433 (0.02401) 1776 (1730)\n10.0 (~2.34M) SVD l2 0.03215 0.23050 0.02506 13501\nLinear AE (+ ELU) Φ1,400 0.03680 (0.03707) 0.22900 (0.22910) 0.02909 (0.02934) 4478 (4387)\nTable 1: Additional metrics for comparing the performance of SVD baseline and the best performing linear\nAutoEncoder model (we select the conﬁguration that minimizes Perplexity, as presented in Figure 3) for different\ncompression ratios (CR). For each AutoEncoder model, we also present (in parentheses) the results after adding\nnon-linearity. Bold values indicate best results between SVD and linear AutoEncoder in each compression ratio.\nCR Architecture SST-2\n(Acc)\nMRPC\n(F1/Acc)\nSTS-B\n(Pearson/Spearman\ncorrelation)\nQQP\n(Acc/F1)\nMNLI\n(Acc)\nQNLI\n(Acc)\nRTE\n(Acc)\nSQuAD v1.1\n(F1/EM)\n- Original BERT 91.74 88.12/83.58 88.71/88.55 90.67/87.43 84.04 90.96 65.34 81.97/73.42\n2.5 SVD 89.22 82.37/75.25 86.27/85.72 89.88/86.39 82.83 89.46 62.92 80.75/72.34\nLinear AE 90.83 86.64/80.88 87.35/86.88 90.04/86.72 83.13 89.16 62.58 81.29/72.85\n5.0 SVD 87.04 83.95/77.70 84.88/84.2 89.79/86.45 81.39 87.33 59.21 80.37/71.67\nLinear AE 88.07 86.67/81.37 85.9/85.43 89.2/85.66 81.11 87.53 64.26 80.53/72.00\n10.0 SVD 82.0 83.95/72.55 80.93/80.67 87.6/83.57 76.59 83.51 54.51 74.15/65.0\nLinear AE 84.29 84.06/77.7 84.7/84.16 88.32/84.38 79.26 86.09 58.48 75.70/66.75\nTable 2: Performance comparison of the best SVD and the best linear AutoEncoder objective conﬁguration on\nseveral NLU tasks from GLUE benchmark (Wang et al., 2018) and for SQuAD v1.1 in different compression\nratios (CR).\n0 1 2 3\n60\n70\n80\nEpochs\nAccuracy\nSST-2\nSV D\nAE\n0 2 4\n70\n75\nEpochs\nAccuracy\nMRPC\nSV D\nAE\n0 1 2 3\n40\n60\n80\nEpochs\nF1\nSQuAD v1.1\nSV D\nAE\nFigure 4: Comparing the learning curves of the best SVD baseline and the best-selected conﬁguration of the\nAutoEncoder model for SST-2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005), and SQuAD v1.1\n(Rajpurkar et al., 2016) during ﬁne-tuning for compression ratio=10.0 .\nELU (Clevert et al., 2015) as this activation\nshows a better impact on Perplexity than other\nactivations in our experiments. It can be seen\nthat the improvements in Perplexity due to\nthe addition of non-linearities are marginal (as\npreviously observed by Lioutas et al. (2019) in\na distillation-based approach for token embeddings\ncompression). Hence, we focused only on the\nlinear AutoEncoder in all our downstream tasks\nexperiments.\n5 Conclusion\nIn this work, we propose a simple linear AutoEn-\ncoder model with a multi-objective loss function\nfor BERT-like token embeddings compression.\nWe emphasize the importance of the direction\ncomponent (measured by the cosine distance\nbetween the original and the reconstructed token\nembeddings) in the compression objective function.\nWe challenge the commonly used SVD-based\nmatrix-factorization method and show that our\napproach achieves signiﬁcantly better zero-shot\nlanguage model Perplexity. Moreover, we show\nthat BERT-like models with our compressed token\nembeddings submodule converge much faster and\noutperform the SVD baseline on SQuAD v1.1 and\non GLUE benchmark tasks in most scenarios.\n6 Acknowledgements\nThis research was partially funded by the Priority\nResearch Area Digiworld under the program\nExcellence Initiative – Research University at the\nJagiellonian University in Kraków.\nReferences\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nDjork-Arné Clevert, Thomas Unterthiner, and Sepp\nHochreiter. 2015. Fast and accurate deep network\nlearning by exponential linear units (elus). arXiv\npreprint arXiv:1511.07289.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training\nof deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nTomáš Folt`ynek, Norman Meuschke, and Bela Gipp.\n2019. Academic plagiarism detection: a systematic\nliterature review. ACM Computing Surveys (CSUR),\n52(6):1–42.\nNathan Halko, Per-Gunnar Martinsson, and Joel A\nTropp. 2011. Finding structure with random-\nness: Probabilistic algorithms for constructing\napproximate matrix decompositions. SIAM review,\n53(2):217–288.\nSong Han, Jeff Pool, John Tran, and William J Dally.\n2015. Learning both weights and connections\nfor efﬁcient neural networks. arXiv preprint\narXiv:1506.02626.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry,\nRan El-Yaniv, and Yoshua Bengio. 2016. Binarized\nneural networks. In Proceedings of the 30th\nInternational Conference on Neural Information\nProcessing Systems, pages 4114–4122.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling bert for natural language\nunderstanding. arXiv preprint arXiv:1909.10351.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised\nlearning of language representations. arXiv preprint\narXiv:1909.11942.\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet,\nand Hans Peter Graf. 2016. Pruning ﬁlters for efﬁ-\ncient convnets. arXiv preprint arXiv:1608.08710.\nVasileios Lioutas, Ahmad Rashid, Krtin Kumar,\nMd Akmal Haidar, and Mehdi Rezagholizadeh.\n2019. Distilled embedding: non-linear embedding\nfactorization using knowledge distillation. arXiv\npreprint arXiv:1910.06720.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692.\nYihuan Mao, Yujing Wang, Chufan Wu, Chen\nZhang, Yang Wang, Yaming Yang, Quanlu Zhang,\nYunhai Tong, and Jing Bai. 2020. Ladabert:\nLightweight adaptation of bert through hybrid model\ncompression. arXiv preprint arXiv:2004.04124.\nTomas Mikolov, Kai Chen, Greg Corrado, and\nJeffrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nLailil Muﬂikhah and Baharum Baharudin. 2009.\nDocument clustering using concept space and cosine\nsimilarity measurement. In 2009 International Con-\nference on Computer Technology and Development,\nvolume 1, pages 58–62. IEEE.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative\nstyle, high-performance deep learning library. arXiv\npreprint arXiv:1912.01703.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nZiheng Wang, Jeremy Wohlwend, and Tao Lei. 2019.\nStructured pruning of large language models. arXiv\npreprint arXiv:1910.04732.\nGenta Indra Winata, Andrea Madotto, Jamin Shin,\nElham J Barezi, and Pascale Fung. 2019. On\nthe effectiveness of low-rank matrix factorization\nfor lstm model compression. arXiv preprint\narXiv:1908.09982.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Rémi Louf, Morgan\nFuntowicz, et al. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. arXiv\npreprint arXiv:1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime\nCarbonell, Ruslan Salakhutdinov, and Quoc V\nLe. 2019. Xlnet: Generalized autoregressive\npretraining for language understanding. arXiv\npreprint arXiv:1906.08237.\nShuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou,\nHe Wen, and Yuheng Zou. 2016. Dorefa-net:\nTraining low bitwidth convolutional neural networks\nwith low bitwidth gradients. arXiv preprint\narXiv:1606.06160.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8413651585578918
    },
    {
      "name": "Language model",
      "score": 0.7448468804359436
    },
    {
      "name": "Perplexity",
      "score": 0.6614673137664795
    },
    {
      "name": "Transformer",
      "score": 0.650450587272644
    },
    {
      "name": "Uncompressed video",
      "score": 0.5514050722122192
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5080995559692383
    },
    {
      "name": "Memory footprint",
      "score": 0.4965079426765442
    },
    {
      "name": "Word embedding",
      "score": 0.48083361983299255
    },
    {
      "name": "Embedding",
      "score": 0.465572327375412
    },
    {
      "name": "Inference",
      "score": 0.44085946679115295
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.43008098006248474
    },
    {
      "name": "Autoencoder",
      "score": 0.42892754077911377
    },
    {
      "name": "Natural language processing",
      "score": 0.41348177194595337
    },
    {
      "name": "Deep learning",
      "score": 0.39350974559783936
    },
    {
      "name": "Programming language",
      "score": 0.11655169725418091
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Video tracking",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126596746",
      "name": "Jagiellonian University",
      "country": "PL"
    }
  ]
}