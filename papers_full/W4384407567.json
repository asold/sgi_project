{
  "title": "How large language models including generative pre-trained transformer (GPT) 3 and 4 will impact medicine and surgery",
  "url": "https://openalex.org/W4384407567",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4272094247",
      "name": "S.B. Atallah",
      "affiliations": [
        "Florida College",
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A3199058269",
      "name": "Banda Nr",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2135352015",
      "name": "A. Banda",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A4384427028",
      "name": "N. A. Roeck",
      "affiliations": [
        "Florida State University"
      ]
    },
    {
      "id": "https://openalex.org/A4272094247",
      "name": "S.B. Atallah",
      "affiliations": [
        "University of Central Florida",
        "Florida College"
      ]
    },
    {
      "id": "https://openalex.org/A3199058269",
      "name": "Banda Nr",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135352015",
      "name": "A. Banda",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3046866561",
    "https://openalex.org/W3164718925",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4361251463",
    "https://openalex.org/W4283716727",
    "https://openalex.org/W4318263917",
    "https://openalex.org/W1663984431",
    "https://openalex.org/W3046318058",
    "https://openalex.org/W6606545560",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W3034533785",
    "https://openalex.org/W4312968147",
    "https://openalex.org/W6600536066",
    "https://openalex.org/W6602430550",
    "https://openalex.org/W4360620450",
    "https://openalex.org/W4319301505",
    "https://openalex.org/W1986908450",
    "https://openalex.org/W2898088229",
    "https://openalex.org/W4323350039",
    "https://openalex.org/W4360806508",
    "https://openalex.org/W2775761745",
    "https://openalex.org/W2764013866",
    "https://openalex.org/W2217322506",
    "https://openalex.org/W3034424147",
    "https://openalex.org/W1990707734",
    "https://openalex.org/W1996883717",
    "https://openalex.org/W4309674289"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)1 3\nTechniques in Coloproctology (2023) 27:609–614 \nhttps://doi.org/10.1007/s10151-023-02837-8\nEDITORIAL\nHow large language models including generative pre‑trained \ntransformer (GPT) 3 and 4 will impact medicine and surgery\nS. B. Atallah1 · N. R. Banda2 · A. Banda2 · N. A. Roeck3\nReceived: 12 June 2023 / Accepted: 12 June 2023 / Published online: 15 July 2023 \n© Springer Nature Switzerland AG 2023\nIntroduction\nRecently, a promising autoregressive large language model \n(LLM), Generative Pre-trained Transformer (GPT)-3 trained \nwith 175 billion parameters via cloud computing [1 ] has \nbeen made available to the public online (released by Ope-\nnAI on November 30, 2022; https:// chat. openai. com/). Its \nsize makes it one of the largest deep learning models ever \ncreated [2 –5]. ChatGPT’s global uptake has been expo-\nnential: 40 days post launch, GPT-3, had 10 million daily \nusers, surpassing social media giant Instagram in daily \nusers [6 ] and becoming an overnight “cultural sensation” \n[7]. GPT-4 was released on March 14, 2023, and it is capable \nof performing better than humans on high-level professional \nschool exams, and it is perceived as a general purpose artifi-\ncial intelligence (AI) that is suitable for multiple economic \nsectors, including healthcare.\nChatGPT (GPT-3/4) is able to generate de novo textual \noutputs that are grammatically and semantically fluent. Inter-\nestingly, human performance for a task does not define the \nupper bound of LLMs [8 –10]. GPT-3 and higher iterations \nare able to write computer code, compose poetry, generate \nunique musical composition, and even create cooking reci-\npes. Importantly, ChatGPT can be used for a multitude of \nhealthcare-relevant scenarios.\n Uses in medicine pertaining to clinical care and edu-\ncation should be considered as target applications for this \ndisruptive technology. Herein, we explore how LLMs can \nbe applied to health literacy, decision-making, and written \ntask formation. Potential pitfalls of LLMs are also discussed.\nUnderstanding large language models \n(LLMs)\nLLMs are a specific application of Natural Language Pro-\ncessing (NLP), which is a subfield of AI that focuses on \nthe interaction between computers and human language. \nIt involves processing and analyzing natural language data \n(text or speech) to enable machines to understand, generate, \nand respond to human language [11].\nIn the field of AI, LLMs are a subset of deep learning \nthat overlaps with generative AI. Generative AI is capable \nof producing text, images, audio, and synthetic data. Current \nLLMs are most useful for four types of outputs: (a) text clas-\nsification; (b) question answering; (c) document summariza-\ntion (including sentiment analysis); and (d) text generation.\nWhile GPT-4 is the most advanced iteration at the time of \nthis writing (2023), there are other LLMs including PaLM \n(Pathways Language Model), LaMDA, Microsoft Bing, \nGoogle Bard, BERT, and T5. To be capable of advanced \ntextual outputs, all of these models are based on transformer \nneural networks [12], as opposed to convolutional neural \nnetworks. The latter defines the architecture underlying \nmodern computer vision [13].\nOne important aspect of transformers is that they are \nhighly parallelizable [ 1]. By using thousands of graphics \nprocessing units (GPUs) in parallel, GPT-3 was able to \nbe trained in just 1 month. Without being parallelizable, \nthis would have required 355 years using a single GPU. \nGPUs were initially developed for video gaming because \ngraphic outputs requires parallelized computation. Trans-\nformers allow for rapid scale-up compared to previous \n * S. B. Atallah \n atallah@post.harvard.edu\n N. R. Banda \n nickbanda12@gmail.com\n A. Banda \n Archanab@me.com\n N. A. Roeck \n nar18d@fsu.edu\n1 HCA Healthcare Oviedo, Adventheath Orlando, University \nof Central Florida, College of Medicine, Orlando, FL, USA\n2 University of Central Florida, Orlando, FL, USA\n3 Florida State University, Tallahassee, FL, USA\n610 Techniques in Coloproctology (2023) 27:609–614\n1 3\nAI for system training [14]. OpenAI’s original ChatGPT \n(2018) [15] and ChatGPT-2 (2019) [16, 17] were LLMs \ndeveloped a few years prior to GPT-3 and 4, but did not \npossess the power of the current iterations.\nLLM architecture is based on being able to probabilis-\ntically predict the next word in a sentence: For example, \nthe colors of the US flag are red, white, and _____. Math-\nematically, this can be expressed as  P/parenleft.s1Xn/uni007C.x/uni007C.xXn−1 ), whereby \nthe probability P of the next word Xn is based on the word \nappearing immediately before it in the sentence, Xn−1 . But \nthe probability of the next word can also be based on more \nthan the immediately prior word, thus more generally:\nWhile transformer neural networks are highly complex, \na critical part of their operation includes assigning weights  \nto the words in the sentence because, generally speaking, \ncertain words in a sentence are  more important than oth-\ners. A model can assign parameters, /u1D711 , to maximize the \nprobability that Xn will be accurate and that it will make \ngrammatical sense. Thus transformer LLMs generally use \nthis formula:\nThis is a simplified mathematical representation of how \ntransformer LLMs are able to determine the next word \n(responses) to text queries, and how they are able to perform \ngenerative AI functions. LLMs do not actually “think”, but \nrather the model generates textual outputs based on next \nword probability and by “paying attention” to key words in \ntext [12]. In computer science, this is termed, autoregres-\nsion, where a weighted sample of past data (text) is used to \npredict future results or textual outputs.\nThe architecture of prior iterations of ChatGPT and cur -\nrent iterations (GPT-3/GPT-4) remained fundamentally the \nsame; however, the build of the initial  pre-training process \nwas much smaller for GPT-1 and GPT-2, with approximately \n20,000 times more computation used for the current model’s \ntraining [18]. In computing, what is known as scaling law  \nallows LLMs to become exponentially more intelligent by \nfeeding them more data [19, 20]. Importantly, GPT-3/4 are \ncapable of so-called few shot or even zero shot learning, \nwhereby learning is achieved from just a few or no examples. \nIn addition, GPT-3/4 have the ability to perform chain-of-\nthought prompting to demonstrate reasoning, e.g., showing \nthe steps for solving a complicated problem in calculus or \nphysics [8, 21]. According to S. Bowman, experts are not yet \nable to interpret the inner workings of LLMs, because the \nLLMs generate outputs based on the vast textual data they \nare fed, and we have no satisfactory method to know “what \nkinds of knowledge, reasoning, or goals a model is using \nwhen it produces some output” [8].\nP(Xn/uni007C.varXn−1 ,Xn−2 ,Xn−3 ,Xn−4 ,…)\nP/u1D711(Xn/uni007C.varXn−1 ,Xn−2 ,Xn−3 ,Xn−4 ,…)\nLLMs in healthcare\nOne of the most intriguing ideas behind GPT-3, and \nhigher versions, is that it can not only analyze voluminous \namounts of textual data but that it can also compose it, \nmaking it a functioning generative AI model [2 , 8, 22]. A \npotential use of GPT-3/4 by healthcare providers is imple -\nmentation as an informatics support system with the objec-\ntive of reducing staff workload and patient wait times. It \ncan rapidly synthesize high volume, complex patient data \nand generate summative reports effortlessly. Theoretically, \nhealthcare IT can integrate LLMs into the electronic medi-\ncal record (EMR) so as to use the generative AI capability \nto write discharge summaries [23], assist with data entry, \nand optimize patient check-in for visits (e.g., by assimi-\nlating necessary patient data prior to consultation and \ntreatment). Through EMR–LLM integration and, in the \nfuture, App-embedding, physicians and medical staff can \nconserve time, reallocating it to more patient-centric tasks \nwhich mandate interpersonal interactions—including face-\nto-face consultation and personalized treatment. In this \nmanner, LLMs could have a transformative impact on pro-\nductivity and well-being, ultimately decreasing provider \nburnout rates in the medical field [24, 25] while enrich-\ning patient experience. The advantages and limitations of \nLLMs in healthcare are summarized in Table  1.\nReal‑world example\nIn the USA, it is not infrequent that medical insurers deny \npayment for procedures and services rendered to patients, \nsuch as the “off-label” use of medications [26]. LLMs can \nbe tooled to sift through patient data from EMRs and thus \ngenerate articulate medical insurance appeal letters written \nwith proper prose; this is one example of how GPT-3/4 can \nbe used to offset a growing clerical and logistical burden \nthat ultimately results in delayed delivery of healthcare. \nThis reallocated time could, in turn, be used for direct \npatient care. In surgery and research, it appears GPT-3 has \na role in autogenous tasks [27], including creating grant \nproposals [28], and generating procedure-specific consent \nforms [29, 30].\nIn the following example GPT-3 was used to generate \na response to a hypothetical insurance denial letter. This \nresponse was generated within 10 s after online query; the \nquery and response are shown verbatim in Fig.  1.\nOne can see from this practical example that GPT-3’s tex-\ntual output is in fluent, native-speaker English with appropri-\nate syntax.  This is just one example of how ChatGPT can \nbe utilized to improve healthcare delivery in the real world.\n611Techniques in Coloproctology (2023) 27:609–614 \n1 3\nEducational resource for patients \nand providers\nThe ability for GPT-3 (and higher iterations) to abstract large \nvolumes of data and present it succinctly makes its output \nmore useful than classical search engines; and this is a key \nreason why the technology has gained rapid usership. Chat-\nGPT could serve as a tool to enhance health literacy, i.e., \nthe capacity to seek, understand, and act on health infor -\nmation [31]. With the advent of this technology, patients \nhave a way to probe an intelligent chatbot for healthcare \nknowledge. Since ChatGPT often provides in-depth and \nconcise responses to health inquiries, individuals interested \nin furthering their knowledge of disease and health have \nthe potential to ask questions and obtain responses that can \nbe easily understood by laypersons. Whether advisable or \nnot, patients will learn from ChatGPT’s responses, shap-\ning their knowledge base and allowing users an “invisible \nhand” in the algorithm of healthcare [32], since they are able \nto ask specific questions and receive immediate personal-\nized responses. One can predict that LLMs will eventually \ndethrone “Dr. Google” [33] to become the first choice as a \nsource of reliable healthcare information.\nLimitations and future considerations\nWhen obtaining knowledge from ChatGPT it is critical for \nindividuals to also be aware of the limitations [2 ] of the \nmodel such as its ability to generate factually incorrect infor-\nmation and produce potentially harmful or biased content. At \nleast currently, GPT-3/4 lack training on events and develop-\nments which are current or those which occurred within the \nrecent past (3 years). Despite these limitations, it serves as \na dynamic tool patients can use to learn from.\nTable 1  Summary of large language model applications in healthcare\nAI  artificial intelligence, GPT  Generative Pre-trained Transformer, LLM large language model\nAdvantages Limitations\nGenerative AI Biased output\n Diagnostic decision tool (e.g., differential diagnosis)  When trained with biased information, it can generate discrimina-\ntory responses which perpetuates health disparities\n Treatment decision formulation\n Document generation and summative reports (e.g., discharge summary, \nletters)\nResource tool Ethical considerations\n Increases information accessibility  Risks to privacy and confidentiality (e.g., data security, informed \nconsent)\n  Educational tool (e.g., generation of synthetic patient data; improved \nhealth literacy\n Maintaining patient trust\n  Research tool\nCommunication enhancement Minimized information capacity\n Virtual health assistant  Dated information; GPT trained on knowledge predating September \n2021\n  Increase awareness and improved healthcare information delivery   Risks involved when inquiring about unfamiliar current informa-\ntion and development\n  On-demand clinical support\nStaff workload optimization Generation of factually incorrect information\n Textual output automation  “Hallucinations”/fictitious textual outputs\n Predictive analytics  Reliance on general patterns due to lack of context awareness\n Assistance with triage  Propagation of misinformation\n Data organization (e.g., patient health informatics)\nTelemedicine enhancement Legal considerations\n Information gateway for providers and patients  Ownership of LLM output, fair use violations, generation of unau-\nthorized, copyright content\n Improved delivery of healthcare in underserved regions  Liability: responsibility of provider, user, or AI developers?\n Mitigated by professional judgment\n Potential for misuse and malpractice\n Over reliance on LLM recommendations\n612 Techniques in Coloproctology (2023) 27:609–614\n1 3\nQuery: my pa/g415ent's insurance company is denying payment for a staging rectal MRI for his new \ndiagnosis of rectal cancer. I want you to write a 600-word le/g425er explaining why this rectal MRI should be  \napproved.\nChatGPT Response:\nDear [Insurance Company],\nI am wri/g415ng to appeal the denial of payment for a staging rectal MRI for my pa/g415ent who has recently  \nbeen diagnosed with rectal cancer. As the pa/g415ent's healthcare provider, I strongly believe that this MRI \nis a necessary diagnos/g415c tool that will help in accurately staging the pa/g415ent's cancer and guiding \nappropriate treatment.\nRectal cancer is a serious and poten/g415ally life-threatening condi/g415on that requires prompt and accurate  \ndiagnosis and treatment. Staging is a cri/g415cal component of the diagnos/g415c proces s, as it allows for the \ndetermina/g415on of the extent of the cancer and the appropriate course of treatment. While other imaging  \nmodali/g415es such as CT scans and ultrasounds can be useful, MRI is considered the gold standard for \nevalua/g415ng the extent of rectal cancer due to its superior so/g332 /g415ssue resolu/g415on and ability to detect  \nsubtle changes in the anatomy and surrounding /g415ssues .\nFurthermore, the pa/g415ent's speciﬁc case warrants the use of an MRI due to the poten/g415al for a more  \nadvanced stage of cancer. This pa/g415ent has already undergone a biopsy that conﬁrmed the presence of  \ncancer in the rectum, and addi/g415onal imaging is necessary to determine the extent of the cancer and \nplan appropriate treatment. Given the high incidence of lymph node involvement in rectal cancer, an \nMRI is necessary to accurately evaluate the regional lymph nodes and determine if addi/g415onal treatment \nsuch as chemotherapy or radia/g415on is necessary.\nIn addi/g415on, an MRI can also detect any poten/g415al spread of the cancer to other organs such as the liver \nor lungs, which can signiﬁcantly impact the course of treatment. Early detec/g415on of metastasis can \nprevent the need for more invasive treatments and increase the pa/g415ent's chances of survival.\nIt is also important to note that an MRI is a non-invasive and safe imaging modality with minimal risk to \nthe pa/g415ent. While there are some contraindica/g415ons for MRI such as pacemakers or other implanted \ndevices, the pa/g415ent in ques/g415on has been cleared for this imaging modality and the beneﬁts of the test \nfar outweigh any poten/g415al risks.\nLastly, denying payment for this necessary diagnos/g415c test can lead to signiﬁcant delays in the pa/g415ent's  \ntreatment, which can ul/g415mately result in poorer outcomes and increased healthcare costs. Delayed \ndiagnosis and treatment can allow the cancer to progress, requiring more extensive and costly \ntreatments such as surgery or chemotherapy. Furthermore, delays in treatment can have a signiﬁcant \nimpact on the pa/g415ent's quality of life and well-being, which is something that should be avoided \nwhenever possible.\nIn conclusion, I strongly urge you to reconsider your decision to deny payment for a staging rectal MRI \nfor my pa/g415ent with rectal cancer. This test is a cri/g415cal component of the diagnos/g415c process and will \nallow for accurate staging and appropriate treatment planning. The safety and eﬀec/g415veness of MRI,  \ncoupled with the poten/g415al for improved pa/g415ent outcomes and decreased healthcare costs, make this  \ntest a necessary and valuable component of the pa/g415ent's care .\nThank you for your a/g425en/g415on to this ma/g425er.\nSincerely,\n[Your Name]\nFig. 1  ChatGPT-3’s rebuttal letter for denial of services. Both query and response are unedited. GTP-3 Generative Pre-trained Transformer-3, \nMRI Magnetic Resonance Imaging, CT Computed Tomography\n613Techniques in Coloproctology (2023) 27:609–614 \n1 3\nIndividuals who reside in remote locales often face a \nsignificant barrier: lack of easy access to medical profes-\nsionals and health resources. This health disparity has a \nsubstantial impact on an individual’s health awareness, and \nGPT-3/4 will benefit persons residing in such underserved \ncommunities.\nIt does not take much to imagine an amalgam of technol-\nogy that includes speech recognition, recent developments \nof human-expressive robotics [34], and generative AI via \nLLMs. Combining those elements appears to be a natural \nnext step and a future in which a patient can be evaluated \nand triaged by a speaking intelligent humanoid robot in not \ninconceivable. Once the stuff of science fiction, such a con-\nstruct could become a reality in the near future.\nEthics and safety governing LLMs remain challenging, \nespecially because ChatGPT is growing at an exponential \nrate and also because it is definitely prone to misuse [2 , 8]. \nThrough proper human oversight, future iterations aim to \nminimize bias and potential harm to humans. Current rendi-\ntions have proven to be suboptimal in certain settings and \nalong certain chat streams. It has been shown in simulation, \nfor example, that GPT-3 was capable of encouraging sui-\ncidal ideation in a mock patient [35], raising serious concern \nfor public health rendered via GPT’s unregulated and free \naccess.\nGPT-3 delivered a functional AI to the masses, which \nis in itself an historic achievement. We must be prepared \nto shepherd its use to prevent malicious application and/or \nharmful outputs. Isaac Asimov famously set forth the cor -\nnerstone principles fundamental to all artificially intelligent \nsystems. While intended for robots specifically, the Three \nLaws of Asimov [ 36] can be broadened to be inclusive of \nall AI—including GPT and other LLMs. Expanding this, \nAsimov’s Laws are as follows:\n1st law  A robot/AI may not injure a human being or, \nthrough inaction, allow a human being to come \nto harm.\n2nd law  A robot/AI must obey the orders given it by \nhuman beings except where such orders would \nconflict with the 1st law.\n3rd law  A robot/AI must protect its own existence as long \nas such protection does not conflict with the 1st or \n2nd law.\nRecall that computers have been making critical deci-\nsions in healthcare for decades. Perhaps the best example \nof this is the computerized human-independent algorithms \nthat control the automated external defibrillator (AED), first \ndeveloped by Paul Zoll at Harvard Medical School in the \n1950s [37]; AED implementation has been crucial not just \nbecause of the access of the device in public areas, but the \nability for it to be operated without the need for medical \nexpertise and intentionally without human input [38].\nWhile humans have been accustomed to specific comput-\nerized applications in medicine such as AEDs, generative \nAI models are far more complex and may not be as easily \nadapted. Modern LLMs do not always get it right and on \noccasion produce fluent and grammatically correct textual \noutputs that are categorically false, and in some instances \nfictitious—a phenomenon known as “hallucination” [39].\nExperts, including OpenAI CEO Sam Altman and oth-\ners, have voiced concern about the potential misuse of \nLLMs to write computer code for the purpose of carrying \nout cyberattacks, as well as GPT’s potential to propagate \nbiased information and misinformation. Computer scien-\ntists have suggested that LLMs can manipulate humans to \nacquire power. The majority of 700 plus computer science \nresearchers believe there is more than a 10% probability that \nhumans will not be able to control further advancements in \nAI leading to “human extinction” [8 , 40]. The gravity of \nthese statements suggests that while LLMs are likely here \nto stay, human oversight and governance will be absolutely \ncrucial.\nData availability All data, analytic methods, and study materials used \nto conduct the research will be made available to any researcher from \nthe corresponding author upon request.\nDeclarations \nConflict of interest There are no conflicts of interest to declare.\nEthical approval This is an editorial and ethical approval is not appli-\ncable.\nInformed consent This publication does not involve patient care, there-\nfore informed consent is not applicable.\nReferences\n 1. Atallah AB, Atallah S (2021) Cloud computing for robot-\nics and surgery. Digital Surg. https:// doi. org/ 10. 1007/  \n978-3- 030- 49100-0_4\n 2. Korngiebel DM, Mooney SD (2021) Considering the possibilities \nand pitfalls of Generative Pre-trained Transformer 3 (GPT-3) in \nhealthcare delivery. NPJ Digital Med 4(1):93\n 3. Sallam M (2023) ChatGPT utility in healthcare education, \nresearch, and practice: Systematic review on the promising per -\nspectives and valid concerns. InHealthcare 11(6):887\n 4. Li J, Dada A, Kleesiek J, Egger J (2023) ChatGPT in Healthcare: \na taxonomy and systematic review. medRxiv. https:// doi. org/ 10. \n1101/ 2023. 03. 30. 23287 899\n 5. George AS, George AH (2023) A review of ChatGPT AI’s impact \non several business sectors. Partners Univers Int Innov J 1(1):9–23\n614 Techniques in Coloproctology (2023) 27:609–614\n1 3\n 6. Liquid Ocelot. https:// medium. com/ inkwa ter- atlas/ chatg pt- surpa \nsses- insta gram- with- 10- milli on- daily- users- in- just- 40- days- 58094 \n4badd 9e. Accessed 28 May 2023\n 7. Thorp HH (2023) ChatGPT is fun, but not an author. Science \n379(6630):313–313\n 8. Bowman SR (2023) Eight things to know about large language \nmodels. arXiv preprint arXiv: 2304. 00612\n 9. Shlegeris B, Roger F, Chan L (2022) Language models seem to be \nmuch better than humans at next-token prediction. Alignm Forum. \nhttps:// www. align mentf orum. org/ posts/ htrZr xduci Z5QaC jw/ langu \nage- models- seem- to- be- much- better- than- humans- at- next\n 10. Stiennon N, Ouyang L, Wu J et al (2020) Learning to summarize \nwith human feedback. Adv Neural Inf Process Syst 33:3008–3021\n 11. Hirschberg J, Manning CD (2015) Advances in natural language \nprocessing. Science 349(6245):261–266\n 12. Vaswani A, Shazeer N, Parmar N et al (2017) Attention is all you \nneed. Adv Neural Inform Process Syst 30. https:// proce edings.  \nneuri ps. cc/ paper/ 2017/ file/ 3f5ee 24354 7dee9 1fbd0 53c1c 4a845 \naa- Paper. pdf\n 13 Atallah S (2021) Artificial intelligence and computer vision. Digi-\ntal Surg. https:// doi. org/ 10. 1007/ 978-3- 030- 49100-0_ 31\n 14. Hoffmann J, Borgeaud S, Mensch A et al (2022) Training com-\npute-optimal large language models. arXiv preprint arXiv: 2203.  \n15556\n 15. Radford A, Narasimhan K, Salimans T et al (2018) Improving \nlanguage understanding by generative pre-training. OpenAI blog. \nhttps:// openai. com/ resea rch/ langu age- unsup ervis ed\n 16. Budzianowski, Paweł, Ivan Vulić (2019) Hello, it's GPT-2--how \ncan I help you? Towards the use of pretrained language models for \ntask-oriented dialogue systems. arXiv preprint arXiv: 1907. 05774\n 17. Ham D, Lee JG, Jang Y, Kim KE (2020) End-to-end neural pipe-\nline for goal-oriented dialogue systems using GPT-2. InProceed-\nings of the 58th annual meeting of the association for computa-\ntional linguistics (pp. 583–592)\n 18. Sevilla J, Heim L, Ho A, Besiroglu T, Hobbhahn M, Villalobos P \n(2022) Compute trends across three eras of machine learning. In \n2022 International Joint Conference on Neural Networks (IJCNN), \npp. 1–8. IEEE\n 19. S. Hoffmann J, Borgeaud S, Mensch A et al (2022) An empirical \nanalysis of compute-optimal large language model training. In: \nOh AH, Agarwal A, Belgrave D, Cho K (eds) Advances in Neu-\nral Information Processing Systems, 2022. https:// openr eview. net/ \nforum? id= iBBcR UlOAPR\n 20. Kaplan S, McCandlish J, Henighan T et al (2020) Scaling laws for \nneural language models. arXiv preprint 2001.08361\n 21. Wei J, Wang X, Schuurmans D et al (2022) Chain of thought \nprompting elicits reasoning in large language models. arXiv pre-\nprint arXiv: 2201. 11903\n 22. Dwivedi YK, Kshetri N, Hughes L et al (2023) “So what if Chat-\nGPT wrote it?” Multidisciplinary perspectives on opportunities, \nchallenges and implications of generative conversational AI for \nresearch, practice and policy. Int J Inf Manage 1(71):102642\n 23. Patel SB, Lam K (2023) ChatGPT: the future of discharge sum-\nmaries? Lancet Digital Health 5(3):e107–e108\n 24. Woolhandler S, Himmelstein DU (2014) Administrative work \nconsumes one-sixth of US physicians’ working hours and lowers \ntheir career satisfaction. Int J Health Serv 44(4):635–642\n 25. Patel RS, Bachu R, Adikey A, Malik M, Shah M (2018) Factors \nrelated to physician burnout and its consequences: a review. Behav \nSci 8(11):98\n 26. Kahn SA, Bousvaros A (2023) Topic of the month: How to write \nan effective letter of medical necessity. J Pediatr Gastroenterol \nNutr 3:10–97\n 27. Ali SR, Dobbs TD, Hutchings HA, Whitaker IS (2023) Using \nChatGPT to write patient clinic letters. The Lancet Digital Health \n5(4):e179–e181\n 28 Janssen BV, Kazemier G, Besselink MG (2023) The use of Chat-\nGPT and other large language models in surgical science. BJS \nopen. https:// doi. org/ 10. 1093/ bjsop en/ zrad0 32\n 29. Tel A, Parodi PC, Robiony M, Zanotti B, Zingaretti N (2023) \nCould ChatGPT improve knowledge in surgery? Ann Surg Oncol \n18:1–2\n 30. Hassan AM, Nelson JA, Coert JH, Mehrara BJ, Selber JC (2023) \nExploring the potential of artificial intelligence in surgery: \ninsights from a conversation with ChatGPT. Ann Surg Oncol \n5:1–4\n 31. Paterick TE, Patel N, Tajik AJ, Chandrasekaran K (2017) Improv-\ning health outcomes through patient education and partnerships \nwith patients. Proc Baylor Univ Med Cent 30(1):112–11\n 32. Brake DR (2017) The invisible hand of the unaccountable algo-\nrithm: how Google, Facebook and other tech companies are \nchanging journalism. Digital Technol J Int Compar Perspect \n25–46\n 33. Lee K, Hoti K, Hughes JD, Emmerton LM (2015) Consumer use \nof “Dr Google”: a survey on health information-seeking behaviors \nand navigational needs. J Med Internet Res 17(12):e288\n 34. Faraj Z, Selamet M, Morales C et al (2021) Facially expressive \nhumanoid robotic face. HardwareX 1(9):e00117\n 35. Daws R (2020) Medical chatbot using OpenAI’s GPT-3 told a \nfake patient to kill themselves. Available at https:// artifi  cial intel \nligen cenews. com/ 2020/ 10/ 28/ medic al- chatb ot- openai- gpt3- patie \nnt- kill- thems elves/\n 36. Asimov I (1941) Three laws of robotics. Asimov I. Runaround. 2. \nISBN: 978-0-385-42304-5\n 37. Zoll PM (1952) Resuscitation of the heart in ventricular standstill \nby external electric stimulation. N Engl J Med 247(20):768–771\n 38. Caffrey SL, Willoughby PJ, Pepe PE, Becker LB (2002) Pub -\nlic use of automated external defibrillators. N Engl J Med \n347(16):1242–1247\n 39. Ji Z, Lee N, Frieske R et al (2023) Survey of hallucination in \nnatural language generation. ACM Comput Surveys 55(12):1–38\n 40. Stein-Perlman Z, Weinstein-Raun B, Grace K (2022) Expert sur-\nvey on progress in AI. AI Impacts blog, 2020. https:// aiimp acts. \norg/ 2022- expert- survey- on- progr ess- in- ai/\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Abdominal surgery",
  "concepts": [
    {
      "name": "Abdominal surgery",
      "score": 0.7670786380767822
    },
    {
      "name": "Colorectal surgery",
      "score": 0.715582013130188
    },
    {
      "name": "Medicine",
      "score": 0.6305778622627258
    },
    {
      "name": "Cardiac surgery",
      "score": 0.5203744769096375
    },
    {
      "name": "Generative grammar",
      "score": 0.4784691035747528
    },
    {
      "name": "Transformer",
      "score": 0.42111819982528687
    },
    {
      "name": "General surgery",
      "score": 0.36722439527511597
    },
    {
      "name": "Internal medicine",
      "score": 0.33508753776550293
    },
    {
      "name": "Natural language processing",
      "score": 0.32460182905197144
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2788119912147522
    },
    {
      "name": "Computer science",
      "score": 0.214267760515213
    },
    {
      "name": "Engineering",
      "score": 0.11153540015220642
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}