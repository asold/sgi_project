{
  "title": "SIGformer: Sign-aware Graph Transformer for Recommendation",
  "url": "https://openalex.org/W4394973476",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2103955539",
      "name": "Sirui Chen",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2109687789",
      "name": "Jiawei Chen",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2000749612",
      "name": "Sheng Zhou",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2555354974",
      "name": "Bohao Wang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2095899342",
      "name": "Shen Han",
      "affiliations": [
        "Huazhong Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2161550236",
      "name": "Chan-Fei Su",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2441200112",
      "name": "Yuqing Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097812263",
      "name": "Can Wang",
      "affiliations": [
        "Zhejiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2998496395",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3153906321",
    "https://openalex.org/W3092103025",
    "https://openalex.org/W6600213211",
    "https://openalex.org/W2887092413",
    "https://openalex.org/W2798881875",
    "https://openalex.org/W2966799427",
    "https://openalex.org/W3156441686",
    "https://openalex.org/W2914721378",
    "https://openalex.org/W3209048663",
    "https://openalex.org/W4292423901",
    "https://openalex.org/W4292419518",
    "https://openalex.org/W3045200674",
    "https://openalex.org/W2053081145",
    "https://openalex.org/W2057551763",
    "https://openalex.org/W2028513945",
    "https://openalex.org/W3208430676",
    "https://openalex.org/W2973140148",
    "https://openalex.org/W4375952695",
    "https://openalex.org/W3099565317",
    "https://openalex.org/W2972520532",
    "https://openalex.org/W2788045146",
    "https://openalex.org/W2054141820",
    "https://openalex.org/W3169575312",
    "https://openalex.org/W3094003325",
    "https://openalex.org/W4382240086",
    "https://openalex.org/W2997638284",
    "https://openalex.org/W2963085847",
    "https://openalex.org/W2136891251",
    "https://openalex.org/W4385713972",
    "https://openalex.org/W4288804669",
    "https://openalex.org/W2984100107",
    "https://openalex.org/W2048531216",
    "https://openalex.org/W2945827670",
    "https://openalex.org/W6600248585",
    "https://openalex.org/W3094605801",
    "https://openalex.org/W3005071803",
    "https://openalex.org/W4226237846",
    "https://openalex.org/W4289433255",
    "https://openalex.org/W3211394146",
    "https://openalex.org/W2807021761",
    "https://openalex.org/W4384652088",
    "https://openalex.org/W2788295351",
    "https://openalex.org/W2101491865",
    "https://openalex.org/W2999905431",
    "https://openalex.org/W3100848837",
    "https://openalex.org/W3153325943",
    "https://openalex.org/W2911319979",
    "https://openalex.org/W4287123803",
    "https://openalex.org/W3194210786",
    "https://openalex.org/W3100278010",
    "https://openalex.org/W3102778384",
    "https://openalex.org/W3164238513"
  ],
  "abstract": "In recommender systems, most graph-based methods focus on positive user\\nfeedback, while overlooking the valuable negative feedback. Integrating both\\npositive and negative feedback to form a signed graph can lead to a more\\ncomprehensive understanding of user preferences. However, the existing efforts\\nto incorporate both types of feedback are sparse and face two main limitations:\\n1) They process positive and negative feedback separately, which fails to\\nholistically leverage the collaborative information within the signed graph; 2)\\nThey rely on MLPs or GNNs for information extraction from negative feedback,\\nwhich may not be effective.\\n To overcome these limitations, we introduce SIGformer, a new method that\\nemploys the transformer architecture to sign-aware graph-based recommendation.\\nSIGformer incorporates two innovative positional encodings that capture the\\nspectral properties and path patterns of the signed graph, enabling the full\\nexploitation of the entire graph. Our extensive experiments across five\\nreal-world datasets demonstrate the superiority of SIGformer over\\nstate-of-the-art methods. The code is available at\\nhttps://github.com/StupidThree/SIGformer.\\n",
  "full_text": "SIGformer: Sign-aware Graph Transformer for Recommendation\nSirui Chen\nZhejiang University\nThe State Key Laboratory of\nBlockchain and Data Security\nHangzhou, China\nchenthree@zju.edu.cn\nJiawei Chenâˆ—\nZhejiang University\nThe State Key Laboratory of\nBlockchain and Data Security\nHangzhou, China\nsleepyhunt@zju.edu.cn\nSheng Zhou\nZhejiang University\nHangzhou, China\nzhousheng_zju@zju.edu.cn\nBohao Wang\nZhejiang University\nThe State Key Laboratory of\nBlockchain and Data Security\nHangzhou, China\nbohao.wang@zju.edu.cn\nShen Han\nHuazhong Agricultural University\nWuhan, China\nhanshen@webmail.hzau.edu.cn\nChanfei Su\nOPPO Co Ltd\nShenzhen, China\nsuchanfei@oppo.com\nYuqing Yuan\nOPPO Co Ltd\nShenzhen, China\nyuanyuqing@oppo.com\nCan Wang\nZhejiang University\nThe State Key Laboratory of\nBlockchain and Data Security\nHangzhou, China\nwcan@zju.edu.cn\nABSTRACT\nIn recommender systems, most graph-based methods focus on pos-\nitive user feedback, while overlooking the valuable negative feed-\nback. Integrating both positive and negative feedback to form a\nsigned graph can lead to a more comprehensive understanding\nof user preferences. However, the existing efforts to incorporate\nboth types of feedback are sparse and face two main limitations:\n1) They process positive and negative feedback separately, which\nfails to holistically leverage the collaborative information within\nthe signed graph; 2) They rely on MLPs or GNNs for information\nextraction from negative feedback, which may not be effective.\nTo overcome these limitations, we introduce SIGformer, a new\nmethod that employs the transformer architecture to sign-aware\ngraph-based recommendation. SIGformer incorporates two inno-\nvative positional encodings that capture the spectral properties\nand path patterns of the signed graph, enabling the full exploita-\ntion of the entire graph. Our extensive experiments across five\nreal-world datasets demonstrate the superiority of SIGformer over\nstate-of-the-art methods. The code is available at https://github.\ncom/StupidThree/SIGformer.\nâˆ—Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0431-4/24/07\nhttps://doi.org/10.1145/3626772.3657747\nCCS CONCEPTS\nâ€¢ Information systems â†’Recommender systems.\nKEYWORDS\nSign-aware Recommendation, Graph, Transformer\nACM Reference Format:\nSirui Chen, Jiawei Chen, Sheng Zhou, Bohao Wang, Shen Han, Chanfei Su,\nYuqing Yuan, and Can Wang. 2024. SIGformer: Sign-aware Graph Trans-\nformer for Recommendation. In Proceedings of the 47th International ACM\nSIGIR Conference on Research and Development in Information Retrieval (SI-\nGIR â€™24), July 14â€“18, 2024, Washington, DC, USA. ACM, New York, NY, USA,\n11 pages. https://doi.org/10.1145/3626772.3657747\n1 INTRODUCTION\nRecent years have witnessed a surge of graph-based methods for\nrecommendation [20, 24, 37, 54, 66]. These methods generally start\nby creating a bipartite graph from usersâ€™ historical feedback and\nthen employ graph-enhanced representation techniques (e.g., Graph\nNeural Network) to learn embeddings for both users and items.\nOwing to the graph structureâ€™s inherent capacity to encapsulate\ncollaborative relations among users and items, graph-based meth-\nods have achieved state-of-the-art performance in collaborative\nrecommendation.\nHowever, most existing graph-based methods focus on user pos-\nitive feedback, while the rich negative feedback is often overlooked.\nIn practice, negative feedback is readily available in many Recom-\nmendation Systems (RS) â€” users can give low ratings, click the\nâ€œdislikeâ€ button, or directly skip items on various platforms like\nAmazon, Taobao, and TikTok. This crucial negative feedback not\nonly directly indicates usersâ€™ preferences but also provides valuable\ncollaborative information benefiting recommendation. To illustrate\nthis point, consider Figure 1, where positive and negative feedback\narXiv:2404.11982v3  [cs.IR]  6 May 2024\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Sirui Chen, et al.\nğœ†!ğœ†\"ğœ†# ğœ†$%&\nâ‹¯\n123456\nID\n=Ã—\nâ‹® â‹®\nğœ‘())\nğ¡!ğ¡\"ğ¡# ğ¡$%&\nMatMul\nScale\nSoftmax\nSoftmax\n+\nfrequencies\nâ‹®\nğ‘¢!\nğ‘¢\"\nğ‘¢#\nğ‘–!\nğ‘–\"\nğ‘–#\nğ‘–+\n\"ğ‡, \"ğ‡\nMatMul\n+ğ$(&)\nğ((&)\nğ„(&)*)\nğ„(&)\nğ()) ğŠ()) ğ•())â‹®\nğ¡!ğ¡\"ğ¡#ğ¡-!â‹¯ ğ¡!ğ¡\"ğ¡#ğ¡-!â‹® \nSign-aware Path Encoding\nSign-aware Spectral Encoding\n+-\n+-\n+---\n++\nğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\" ğ‘–+ğ‘–#ğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\"ğ‘–#ğ‘–+\nğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\"ğ‘–#ğ‘–+\nâ‹¯\nğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\"ğ‘–#ğ‘–+\nğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\" ğ‘–+ğ‘–#\nSpectral\nPath\nâ‹®\nğ‘œ/0,/2\n001001\nÃ—\nâ‹®\n001001â‹¯\nğ¨/0,/2,\n=ğ‘/0,/2())\nğœ‘())\nâ‹®\n001001\nğ‘‡\nâ‹®\n+-\n+-\n+---\n++\n123456\n+\n-\n+-\n+---\n++\n123456\nID\nğœ‘())\nâ‹® â‹® â‹®\nSpectral\nPath\nğ‘¢!\nğ‘¢\"\nğ‘¢#\nğ‘–!\nğ‘–\"\nğ‘–#\nğ‘–+ğ‘¢+ ğ‘–5\nğ‘¢!\nğ‘¢\"\nğ‘¢#\nğ‘–!\nğ‘–\"\nğ‘–#\nğ‘–+ğ‘¢+ ğ‘–5Unsigned RSSign-aware RS\nğ‘¢\"ğ‘–\"User NodeItem NodePositive FeedbackNegative Feedback\nğ‘¢!ğ‘¢\"\nğ‘–!ğ‘–\"ğ‘–#ğ‘¢# ğ‘–+ğ‘¢+ ğ‘–5\nSigned Graphğ‘¢\"ğ‘–\"User NodeItem NodePositive FeedbackNegative Feedback\nğ‘¢!\nğ‘¢\"\nğ‘¢#\nğ‘–!\nğ‘–\"\nğ‘–#\nğ‘–+ğ‘¢+ ğ‘–5\nğ‘¢!\nğ‘¢\"\nğ‘¢#\nğ‘–!\nğ‘–\"\nğ‘–#\nğ‘–+ğ‘¢+ ğ‘–5\nUnsigned RSSign-aware RS\nğ‘¢\"ğ‘–\"User NodeItem NodePositive FeedbackNegative Feedback\nğ‘¢# ğ‘–#\nğ‘¢$\nğ‘¢# ğ‘–#\nğ‘¢$ ğ‘–%\nğ‘¢!ğ‘¢\"\nğ‘–!ğ‘–\"ğ‘–#ğ‘¢# ğ‘–+ğ‘¢+ ğ‘–5\nUnsigned Graph\nFigure 1: Illustration of sign-aware recommender system,\nwhere the signed graph can be constructed from both pos-\nitive and negative feedback, carrying richer collaborative\ninformation.\namong users and items are constructed as a signed graph. The high-\norder connectivity through negative feedback also conveys useful\ncollaborative insights. For instance, path < ğ‘¢1 âˆ’â€” ğ‘–2 âˆ’â€”ğ‘¢2 > suggests\nusers ğ‘¢1 and ğ‘¢2 may have similar preferences as they both give neg-\native feedback to item ğ‘–2. Additionally, the interplay of positive and\nnegative relations offers richer collaborative relations. For example,\npath < ğ‘¢3 âˆ’â€” ğ‘–4 +â€”ğ‘¢4 > highlights the different preferences between\nğ‘¢3 and ğ‘¢4; a longer path < ğ‘¢1 âˆ’â€” ğ‘–2 âˆ’â€”ğ‘¢2 +â€” ğ‘–3 > implies that user ğ‘¢1\nis likely to favor ğ‘–3 as his similar user ğ‘¢2 has previously interacted\nwith it.\nAcknowledging the valuable collaborative information supplied\nby negative feedback, the integration of both positive and negative\nfeedback presents a promising direction for enhancing graph-based\nrecommendation. However, to the best of our knowledge, only\na few studies have investigated this domain [ 30, 42, 48]. These\nmethods generally construct two separate graphs from positive\nand negative feedback and then learn distinct representations from\neach, subsequently merging these representations for predictions.\nDespite decent performance, we identify two significant limitations:\nâ€¢The positive and negative feedback is processed separately,\nwithout a holistic consideration. As previously discussed, the\nintegration of positive and negative feedback within a graph\noffers rich collaborative information, reflecting the levels of simi-\nlarity between users and items. Fully exploiting such information\nwarrants the direct utilization of the entire signed graph, rather\nthan processing separate subgraphs independently.\nâ€¢The effectiveness of MLPs or GNNs in extracting informa-\ntion from the negative graph is questionable. Most GNNs,\nparticularly those tailored for recommendation (e.g., LightGCN\n[24]), are based on the homophily assumption â€” i.e., connected\nnodes are likely to be similar. This assumption does not hold for\nthe negative graph. Meanwhile, MLPs struggle to fully utilize\nthe graph structure and are challenging to train effectively in\nrecommendation scenarios due to data sparsity.\nGiven the shortcomings of existing methods, we argue for the\nnecessity of a new architecture that can fully exploit the entire\nsigned graph. Inspired by the success of transformer architecture\nin many fields including language processing [2, 33, 52], computer\nvisions [4, 7, 18] and sequential recommendation [21, 50, 59], we pro-\npose leveraging transformer in this scenario. Indeed, transformer\nis highly aligned with the fundamental principles of collabora-\ntive filtering â€” i.e., estimating similarity between users and items\naccording to their historical feedback, and then aggregating infor-\nmation from those similar entities for predictions. While appealing,\nadapting transformer to sign-aware graph-based recommendation\nis non-trivial. The vanilla transformer focuses solely on semantic\nsimilarity via self-attention, lacking an explicit encoding of the\ncollaborative information in the signed graph. Although existing\ngraph transformer models [6, 36, 65] introduce subtle positional\nencodings to capture graph structures, they are neither specifically\ndesigned for the signed graph nor the recommendation task. To ad-\ndress these challenges, we introduce two novel positional encodings\ntailored for sign-aware graph-based recommendation:\n(1) Sign-aware Spectral Encoding (SSE). To integrate the struc-\nture of the entire signed graph, we propose to utilize the node spec-\ntral representation on the signed graph. Specifically, we incorporate\nthe low-frequency eigenvectors of the signed graphâ€™s Laplacian\nmatrix as positional encoding. Our theoretical analysis supports\nthe efficacy of this approach: the transformer equipped with SSE\ncan be interpreted as a low-pass filter, bringing the embeddings of\nuser-item pairs with positive feedback closer and distancing those\nwith negative feedback.\n(2) Sign-aware Path Encoding (SPE). To further capture collab-\norative relations among users and items, we focus on the patterns of\npaths within the signed graph. We encode the distance and the signs\nof edges along these paths into learnable parameters to capture\nthe affinity between nodes connected by these paths. This design\nis based on our intuition that different path types reflect varying\nlevels of similarity.\nEquipped with these encodings, we introduce a novel recommen-\ndation method namedSIgn-aware Graph Transformer (SIGformer),\nwhich adeptly utilizes the collaborative information within the\nsigned graph. Its effectiveness is validated through empirical exper-\niments on five real-world datasets, where it significantly outper-\nforms existing graph-based methods. Additional ablation studies\nfurther confirm the critical role of incorporating negative feedback\nand the efficacy of our specifically designed encodings.\nOur contributions are summarized as follows:\nâ€¢We highlight the importance of integrating negative feedback in\ngraph-based recommendation and advocate for the application\nof transformer architecture for sign-aware graph-based recom-\nmendation.\nâ€¢We propose two innovative sign-aware positional encodings, de-\nrived from the perspectives of signed graph spectrum and paths,\nwhich fully exploit the sign-aware collaborative information.\nâ€¢We propose SIGformer and conduct extensive experiments to\nvalidate the superiority of our SIGformer over state-of-the-art\nmethods.\n2 PRELIMINARY\nIn this section, we present the background of sign-aware graph-\nbased recommendation and transformer model.\n2.1 Sign-aware Graph-based Recommendation\nSuppose we have a recommender system (RS) with a user set U\nand an item set I. Let ğ‘›and ğ‘šbe the number of users and items\nin RS. User-item historical interactions can be represented as a set\nD= {(ğ‘¢,ğ‘–,ğ‘¦ğ‘¢ğ‘–)|ğ‘¢ âˆˆU,ğ‘– âˆˆI}, where ğ‘¦ğ‘¢ğ‘– = 1 signifies user ğ‘¢ has\nprovided positive feedback on item ğ‘–, ğ‘¦ğ‘¢ğ‘– = 0 signifies negative\nSIGformer: Sign-aware Graph Transformer for Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nğœ†!ğœ†\"ğœ†# ğœ†$%&\nâ‹¯\nID\n=Ã—\nğœ‘(\")\nğ¡!ğ¡\"ğ¡# ğ¡$%&\nMatMul\nScale\nSoftmax\nSoftmax\n+\n#ğ‡$ #ğ‡\nMatMul\n+ğ%(\")\nğ&(\")\nğ„(\"'()\nğ„(\")\nğ(() ğŠ(() ğ•(()â‹®\nğ¡!ğ¡\"ğ¡#ğ¡+,â‹¯ ğ¡!ğ¡\"ğ¡#ğ¡+,â‹® \nSign-aware Path Encoding\nSign-aware Spectral Encoding\nğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\" ğ‘–-ğ‘–#ğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\"ğ‘–#ğ‘–-\nğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\"ğ‘–#ğ‘–-\nâ‹¯\nğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\"ğ‘–#ğ‘–-\nğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\" ğ‘–-ğ‘–#\nÃ—001001â‹¯ğ¨*/,*0$ =ğ‘*/,*0(\")\nâ‹®\n++-+---++\n123456â‹® â‹®\nSpectral frequencies\nPath\nPositive FeedbackNegative Feedback\nğ‘¢!\nğ‘¢\"\nğ‘¢#\nğ‘–!\nğ‘–\"\nğ‘–#\nğ‘–-\n(a)\tTop-5\tvalues\tof\tğœ‘ (b)\tBottom-5\tvalues\tof\tğœ‘Path\tpatternsValuesPath\tpatternsValues1.704\t -1.303\t1.661\t -1.226\t1.658\t -1.225\t1.654\t -1.210\t1.654\t -1.116\t\n-\n++--+--++-++-- ++--+--++-++--\n---+--++-+-+-+-+++\nFigure 2: The illustration of our proposed sign-aware path encoding and sign-aware spectral encoding in SIGformer.\nfeedback, and ğ‘¦ğ‘¢ğ‘– =â€˜?â€™ signifies an absence of interaction with the\nitem. A signed bipartite graphG= (V,E+,Eâˆ’)is constructed from\nD, where the node setV= UâˆªI invovles all users and items. The\nedge sets E+and Eâˆ’correspond to user-item positive and negative\ninteractions, respectively, i.e., E+= {(ğ‘¢,ğ‘–)|ğ‘¢ âˆˆU,ğ‘– âˆˆI,ğ‘¦ğ‘¢ğ‘– = 1}\nand Eâˆ’ = {(ğ‘¢,ğ‘–)|ğ‘¢ âˆˆU,ğ‘– âˆˆI,ğ‘¦ğ‘¢ğ‘– = 0}. The goal of sign-aware\ngraph-based RS is to learn high-quality embeddings from the signed\ngraph Gand accordingly make accurate recommendation.\nFor clarity, we introduce some useful notations w.r.t. the signed\ngraph. The signed graph can be partitioned into a positive graph\nG+= (V,E+)and a negative graph Gâˆ’= (V,Eâˆ’). Let A+denote\nthe adjacent matrix of positive graphG+, where each entryA+ğ‘£ğ‘¤ = 1\nif (ğ‘£,ğ‘¤)âˆˆE +or (ğ‘¤,ğ‘£)âˆˆE +; and L+denote the Laplacian matrix\nof G+, defined as L+= I âˆ’(D+)âˆ’1\n2 A+(D+)âˆ’1\n2 , with D+represent-\ning the diagonal node degree matrix of G+. Let ğ‘‘+ğ‘¢ (or ğ‘‘+\nğ‘– ) denote\nthe degree of user ğ‘¢ (or item ğ‘–) in the positive graph. Analogous\ndefinitions of notations Aâˆ’, Lâˆ’, Dâˆ’, dâˆ’ğ‘¢, dâˆ’\nğ‘– are applicable to the\nnegative graph Gâˆ’.\nCompared with the traditional graph-based recommendation\nthat only utilizes G+, sign-aware recommendation leverages the\ncomplete signed graph G, encompassing both positive and negative\nrelations. The signed graph carries richer collaborative information,\nnecessitating effective exploitation by recommendation methods.\n2.2 Transformer\nThe transformer architecture has been widely applied in many\nfields [2, 4, 18, 33, 50, 52]. It is composed of self-attention modules\nand feed-forward neural networks. In the self-attention module,\nthe input features X âˆˆRğ‘›Ã—ğ‘‘ are projected to the corresponding\nquery Q, key K, and value V, and then calculated via attention with:\nQ = XWğ‘„, K = XWğ¾, V = XWğ‘‰,\nAttn(X)= softmax(QKğ‘‡\nâˆšï¸\nğ‘‘ğ¾\n)V (1)\nwhere Wğ‘„ âˆˆ Rğ‘‘Ã—ğ‘‘ğ¾,Wğ¾ âˆˆ Rğ‘‘Ã—ğ‘‘ğ¾,Wğ‘‰ âˆˆ Rğ‘‘Ã—ğ‘‘ğ‘‰ denote the\nprojected matrices of query, key and value respectively.\nConnecting with Collaborative Filtering. The transformer\narchitecture aligns closely with the fundamental principle of col-\nlaborative filtering. Specifically, consider the input as features of\nusers and items. The transformer initially estimates the similar-\nity between users and items based on their projected features,\nthen aggregates information from other entities according to this\nsimilarity, with more significant contributions from similar enti-\nties. This alignment inspires the application of the transformer in\nsign-aware graph-based recommendation. Nevertheless, the vanilla\ntransformer model cannot be directly adopted, as it fails to harness\nthe structure information of the signed graph.\nGraph Positional Encodings. Positional Encoding has been\nvalidated as an effective solution to integrating structural informa-\ntion of the graph into transformer. In recent years, diverse strate-\ngies have emerged, including node degrees [65], shortest path dis-\ntances [38, 65], subgraph representations [6] and spectral features\n[19, 36]. However, these strategies are not specifically designed for\nthe signed graph, which requires consideration of the sign of edges.\nTherefore, it is imperative to develop novel positional encodings\ntailored for the sign-aware recommendation task.\n3 METHODOLOGY\nIn this section, we first provide an overview of SIGformer (Sec 3.1),\nfollowed by a description of the proposed positional encodings (Sec\n3.2 & Sec 3.3). Finally, we elaborate on implementation details (Sec\n3.4).\n3.1 Overview of SIGformer\nSIGformer employs a transformer architecture for sign-aware rec-\nommendation, deviating from the conventional graph-based recom-\nmendation paradigm by replacing GNNs with transformer. Specifi-\ncally, SIGformer comprises the following components:\nEmbedding Module. As an initial step, each user and item is en-\ndowed with a ğ‘‘-dimensional embedding (i.e., e(0)\nğ‘¢ ,e(0)\nğ‘– ), which can\nbe treated as learnable parameters or transformed from user/item\nattributes. For a better description, we collect initial embeddings of\nall users and items by a matrix:\nE(0)= [e(0)\nğ‘¢1 ,Â·Â·Â· ,e(0)\nğ‘¢ğ‘›\n|           {z           }\nuser embeddings\n,e(0)\nğ‘–1\n,Â·Â·Â· ,e(0)\nğ‘–ğ‘š\n|           {z           }\nitem embeddings\n]ğ‘‡.\n(2)\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Sirui Chen, et al.\nSign-aware Transformer Module. Diverging from traditional\nGNN-based methods, we employ a stack of multi-layer transformer\nto capture collaborative information. For the ğ‘™-th layer of the trans-\nformer, the embeddings are updated iteratively as follows:\nQ(ğ‘™)= K(ğ‘™)= V(ğ‘™)= E(ğ‘™âˆ’1)\nE(ğ‘™)= 1\n2 (softmax(Q(ğ‘™)(K(ğ‘™))ğ‘‡\nâˆš\nğ‘‘\n+P(ğ‘™)\nğ‘  )+softmax(P(ğ‘™)\nğ‘ ))V(ğ‘™)\n(3)\nContrary to the vanilla transformer model, we omit the projected\nmatrices Wğ‘„,Wğ¾,Wğ‘‰ as they were found to minimally enhance\nperformance while increasing training difficulty. Besides, we intro-\nduce two positional encodings, P(ğ‘™)\nğ‘  and P(ğ‘™)\nğ‘ , to explicitly encode\nthe signed graph information, which would be detailed in the next\ntwo subsections. Here we separate the two positional encodings\ninto different softmax functions to mitigate the impact of their\nmagnitude disparities.\nPrediction Module. Consistent with existing graph-based meth-\nods [24, 67], After ğ¿layers of transformer, we aggregate the embed-\ndings from each layer to generate the final embeddings:\nE = 1\nğ¿+1\nâˆ‘ï¸\n0â‰¤ğ‘™â‰¤ğ¿\nE(ğ‘™)\n(4)\nThe model prediction is generated from the final embeddings, e.g.,\nthrough an inner product, a function widely adopted by existing\napproaches [3, 37, 42, 48]:\nË†ğ‘¦ğ‘¢ğ‘– = eğ‘‡\nğ‘¢eğ‘– (5)\n3.2 Sign-aware Spectral Encoding (SSE)\nGraph spectral theory [12, 49] suggests the effectiveness of spec-\ntral features (e.g., Laplacian eigenvectors) in capturing the graph\nstructure. Spectral features have also been employed to enhance\nthe GNNs or transformer models on the vanilla graph [36, 47, 55].\nMotivated by these successes, we propose to leverage spectral fea-\ntures to enhance our sign-aware transformer model. We begin by\ncombining the Laplacians of the positive and negative graphs as\nfollows:\nL = 1\n1 âˆ’ğ›¼(L+âˆ’ğ›¼Lâˆ’) (6)\nwhere ğ›¼ is a flexible hyperparameter controlling the influence of\nthe negative graph, which will be explored later. The Laplacian\neigenvectors of the signed graph are:\nL = Hğ‘‡ğš²H, H = [h1,h2,Â·Â·Â· ,hğ‘›+ğ‘š]ğ‘‡ (7)\nwhere H, ğš² correspond to the eigenvectors and eignvalues respec-\ntively. The eigenvectors of the ğ‘‘â„ smallest eigenvalues denoted ËœH,\nare used for encoding node relations in the signed graph:\nP(ğ‘™)\nğ‘  = ğœƒ(ğ‘™)ËœHğ‘‡ ËœH, ËœH = [h1,h2,Â·Â·Â· ,hğ‘‘â„]ğ‘‡ (8)\nwhere ğœƒ(ğ‘™)is a learnable positive parameter for rescaling the mag-\nnitude.\nConnecting with Low-pass Filtering. To elucidate the ratio-\nnale behind the proposed spectral encoding, we draw a connection\nto low-pass filtering. For convenience, we omit the softmax func-\ntion from the analysis as its role is normalization. Similarly, for ease\nof discussion, we select an arbitrary column of V(ğ‘™)for analysis\nand denote it as v. The vector v can be expressed by a combination\nof the basis H:\nv =\nâˆ‘ï¸\n1â‰¤ğ‘˜â‰¤ğ‘›+ğ‘š\nğœ€ğ‘˜hğ‘˜ (9)\nwhere ğœ€ğ‘˜ represents the strength of the signal on the component\nhğ‘˜. The effect of introducing P(ğ‘™)\nğ‘  can be described as:\nP(ğ‘™)\nğ‘  v = (\nâˆ‘ï¸\n1â‰¤ğ‘˜â‰¤ğ‘‘â„\nğœƒ(ğ‘™)hğ‘˜hğ‘˜ğ‘‡)(\nâˆ‘ï¸\n1â‰¤ğ‘˜â‰¤ğ‘›+ğ‘š\nğœ€ğ‘˜hğ‘˜)= ğœƒ(ğ‘™) âˆ‘ï¸\n1â‰¤ğ‘˜â‰¤ğ‘‘â„\nğœ€ğ‘˜hğ‘˜\n(10)\nwhere only the low-frequency components ( â„1,â„2,Â·Â·Â· ,â„ğ‘‘ğ‘˜) are\npreserved, and higher-frequency components are filtered out. The\nlemma below elucidates the efficacy of this low-pass filtering nature:\nLemma 1. The low-frequency components ( â„1,â„2,Â·Â·Â· ,â„ğ‘‘â„) opti-\nmizes the following objective function:\n[h1,h2,..., hğ‘‘â„]= arg min\nz1,z2,...,zğ‘‘â„\nâˆ‘ï¸\n1â‰¤ğ‘˜â‰¤ğ‘‘â„\n\u0010 âˆ‘ï¸\n(ğ‘¢,ğ‘–)âˆˆE+\n(zğ‘˜ğ‘¢âˆšï¸ƒ\nğ‘‘+ğ‘¢\nâˆ’ zğ‘˜ğ‘–âˆšï¸ƒ\nğ‘‘+\nğ‘–\n)\n2\n|                          {z                          }\nDrawing Positive Neighbors\nâˆ’ğ›¼\nâˆ‘ï¸\n(ğ‘¢,ğ‘–)âˆˆEâˆ’\n(zğ‘˜ğ‘¢âˆšï¸ƒ\nğ‘‘âˆ’ğ‘¢\nâˆ’ zğ‘˜ğ‘–âˆšï¸ƒ\nğ‘‘âˆ’\nğ‘–\n)\n2\u0011\n|                             {z                             }\nDistancing Negative Neighbors\nğ‘ .ğ‘¡. zğ‘˜ âˆˆRğ‘›+ğ‘š,zğ‘‡\nğ‘˜zğ‘˜ = 1,zğ‘‡\nğ‘˜zğ‘™ = 0,âˆ€ğ‘˜ â‰  ğ‘™,1 â‰¤ğ‘˜,ğ‘™ â‰¤ğ‘‘â„\n(11)\nThe proof is presented in Appendix A.1. From the lemma, when\nğ›¼ > 0, the low-frequency components can be interpreted as the op-\ntimal components that minimize the distances between nodes with\npositive edges while maximizing the distances between nodes with\nnegative edges. Therefore, the introduction of P(ğ‘™)\nğ‘  preserves those\ndesired signals in the embeddings and filters out others, drawing\nthe embeddings of nodes with positive edges closer together and\ndistancing those with negative edges. The structure of the signed\ngraph is thus explicitly encoded into the embeddings.\nThe Role of ğ›¼. This lemma also sheds light on the role of the\nparameter ğ›¼: it modulates the impact of the negative graph. A\nlarger ğ›¼implies a stronger emphasis on distancing the neighbors in\nthe negative graph. To enhance the modelâ€™s flexibility, we propose\nextending the range of ğ›¼ to include negative values. Interestingly,\nthis simple adjustment proves highly effective. It is based on the\nintuition that negative feedback may not always be really negative\nbut instead relatively less positive compared to positive feedback\n[30]. For instance, in a rating system, a userâ€™s decision to leave a\nrating implies engagement with the item, regardless of the ratingâ€™s\npolarity. It suggests that the user might prefer the rated item, albeit\nwith a low score, over others they choose not to interact with.\nConsequently, it may be prudent to set ğ›¼ within the range (âˆ’1,1).\nOur empirical experiments also validate the optimal of ğ›¼ may be\nlocated in small negative values.\n3.3 Sign-aware Path Encoding (SPE)\nWe further exploit path information within the signed graph, which\nexplicitly reflects the collaborative relations between users and\nitems. Our fundamental intuition is that different path types indi-\ncate varying levels of affinity between the nodes they connect. As\nSIGformer: Sign-aware Graph Transformer for Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nillustrated in Figure 2, we initially enumerate all path types based\non their lengths and the signs of edges within the path, assigning\neach path type a unique enumerated ID. This ID corresponds to a\nspecific path type. To constrain the potentially vast space of path\ntypes, we limit our consideration to paths not exceeding a threshold\nlength ğ¿ğ‘, as excessively long paths tend to offer limited collabo-\nrative information. Consequently, the total number of path types\nis ğ‘ğ‘ = 2(2ğ¿ğ‘ âˆ’1). Thereby, for any node pair (ğ‘£,ğ‘¤)âˆˆVÃ—V\nin the graph, we can represent their path relationships with an\nğ‘ğ‘-dimensional vector oğ‘£ğ‘¤, where the ğ‘˜-th entry of oğ‘£ğ‘¤ denotes\nthe presence or absence of the ğ‘˜-th type of path between nodes\n(ğ‘£,ğ‘¤). We integrate this rich path information into the transformer\narchitecture to capture nodesâ€™ affinity:\nğ‘(ğ‘™)\nğ‘£ğ‘¤ = oğ‘‡\nğ‘£ğ‘¤ğœ‘(ğ‘™) (12)\nwhere ğœ‘(ğ‘™) âˆˆRğ‘ğ‘ is a learnable parameter capturing node affinities\nas reflected by the corresponding paths. Differing from existing\ngraph transformer methods that primarily encode the shortest-\ndistance path, our approach considers all path relations, offering a\nholistic view of node relations. For convenience, we aggregate ğ‘(ğ‘™)\nğ‘£ğ‘¤\nfor all node pairs into a matrix, termed as sign-aware path encoding\nP(ğ‘™)\nğ‘ .\n3.4 Implementation Details\nSampling for Acceleration. Given the vast number of user-item\ncombinations in Recommender Systems (RS), traversing all node\npairs to calculate attention is computationally prohibitive. To ad-\ndress this challenge, we employ a sampling strategy. Specifically,\nwe utilize a random walk strategy on the signed graph to pick up\nnodes for aggregation and concurrently record the walked path for\ncomputing P(ğ‘™)\nğ‘ . For each node ğ‘£ âˆˆğ‘‰, we perform a non-cyclic ran-\ndom walk of length ğ¿ğ‘ starting from each neighbor of ğ‘£ to sample\na set of nodes Sğ‘£ associated with the trajectory type. This allows\nfor the rapid updating of user/item embeddings as follows:\ne(ğ‘™)\nğ‘£ = 1\n2\nâˆ‘ï¸\nğ‘¤âˆˆSğ‘£\n\u0012\nsoftmax\n\u0010 (e(ğ‘™âˆ’1)\nğ‘£ )\nğ‘‡\ne(ğ‘™âˆ’1)\nğ‘¤âˆš\nğ‘‘\n+ğœƒ(ğ‘™)ğ‘šğ‘£ğ‘¤\n\u0011\n+softmax(ğœ‘ğ‘¡ğ‘£ğ‘¤)\n\u0013\ne(ğ‘™âˆ’1)\nğ‘¤\n(13)\nwhere ğ‘šğ‘£ğ‘¤ is the ğ‘£ğ‘¤-th entry of the matrix M = ËœHğ‘‡ ËœH, which\ncan be pre-computed; ğ‘¡ğ‘£ğ‘¤ denotes the path type when node ğ‘¤ is\nsampled via the random walker. In this way, the time complexity of\nour attention module is reduced to ğ‘‚((ğ‘›+ğ‘š)ğ‘‘ Ë†ğ‘), where Ë†ğ‘ is the\naverage number of nodes sampled per node. Equipped with this\nsampling strategy, our SIGformer achieves high efficiency.\nOptimization. Referring to recent work [ 24, 48], BPR loss is\nadopted for optimizing our SIGformer:\nL= âˆ’\nâˆ‘ï¸\n(ğ‘¢,ğ‘–)âˆˆE+\nln ğœ \u0000Ë†ğ‘¦ğ‘¢ğ‘– âˆ’Ë†ğ‘¦ğ‘¢ğ‘—\n\u0001 +\nâˆ‘ï¸\n(ğ‘¢,ğ‘–)âˆˆEâˆ’\nln ğœ \u0000ğ›½(Ë†ğ‘¦ğ‘¢ğ‘– âˆ’Ë†ğ‘¦ğ‘¢ğ‘—)\u0001\n(14)\nwhere for each positive/negative feedback (ğ‘¢,ğ‘–), we sample an\nitem ğ‘— âˆˆ{ ğ‘— âˆˆI|ğ‘¦ğ‘¢ğ‘— =â€˜?â€™}that the user has not interacted with\nfor model optimization; ğ›½ is a hyperparameter that balances the\ninfluence from the negative feedback.\nTable 1: Statistics of datasets, where â€œPos/Negâ€ denotes the\nratio between positive and negative interactions.\nDataset #Users #Items #Interactions Pos/Neg\nAmazon-CDs 51,267 46,464 895,266 1:0.22\nAmazon-Music 3,472 2,498 49,875 1:0.25\nEpinions 17,894 17,660 413,774 1:0.37\nKuaiRec 1,411 3,327 253,983 1:5.95\nKuaiRand 16,974 4,373 263,100 1:1.25\n4 EXPERIMENTS\nIn this section, we conduct comprehensive experiments to answer\nthe following research questions:\nâ€¢RQ1: How does SIGformer perform compared with existing meth-\nods?\nâ€¢RQ2: What are the impacts of the important components (e.g., ,\ntwo positional encodings, negative interactions) on SIGformer?\nâ€¢RQ3: How do the hyperparameters affect the model perfor-\nmance?\nâ€¢RQ4: How do different path types capture node similarity?\nâ€¢RQ5: How does the runtime of SIGformer compare with existing\nmethods?\n4.1 Experimental Settings\n4.1.1 Datasets. We conduct experiments on five real-world datasets,\nwhich include both positive and negative feedback: Amazon-CDs\n[43], Amazon-Music [43], and Epinions [51] are three widely-\nused datasets containing usersâ€™ ratings on items from the Amazon\nand Epinions platforms. We closely refer to recent work [30, 42, 48]\nand consider the interactions with high ratings ( e.g., larger than\n3.5) as positive feedback and treat others as negative. KuaiRec\n[22] and KuaiRand [23] record user behavior within the Kuai App.\nFor KuaiRec, we focus on the dense dataset for experiments and\nclassify positive and negative feedback based on the ratio of user\nviewing duration to total video duration. Specifically, ratios equal\nto or exceeding 4 are considered positive, while those below 0.1 are\nclassified as negative. For KuaiRand, we utilize the pure version\nand employ â€œis_clickâ€ attribute to classify positive and negative\ndata as suggested by [23]. We adopt a conventional 5-core setting\nand randomly split the dataset into training set, validation set, and\ntesting set in a ratio of 7:1:2. The dataset statistics are presented in\nTable 1.\n4.1.2 Metrics. Two widely-used metricsğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@ğ¾and ğ‘ğ·ğ¶ğº@ğ¾\nare employed for evaluating the recommendation accuracy. In this\nwork, we simply set ğ¾ = 20 as recent work on graph-based recom-\nmendation [3, 24, 67].\n4.1.3 Baselines. To comprehensively analyze the performance of\nSIGformer, we compared it with various graph-based baselines:\n1) Unsigned Graph-based Recommendation Methods . The\nfollowing representative graph-based recommendation methods\nare included:\nâ€¢LightGCN [24]: the classic graph-based method that leverages\nlinear GNNs for recommendation.\nâ€¢LightGCL [3], XSimGCL [67]: the state-of-the-art graph-based\nmethods that enhance LightGCN with contrastive learning.\nâ€¢GFormer [37]: the state-of-the-art method that automates the\nself-supervision augmentation with transformer architecture.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Sirui Chen, et al.\nTable 2: Performance comparison between SIGformer and baselines. The best result is bolded and the runner-up is underlined.\nThe mark â€˜*â€™ suggests the improvement is statistically significant with ğ‘ < 0.05.\nAmazon-CDs Amazon-Music Epinions KuaiRec KuaiRand\nRecall NDCG Recall NDCG Recall NDCG Recall NDCG Recall NDCG\nUnsigned\nGraph-based RS\nLightGCN 0.1325 0.0781 0.2725 0.1601 0.0854 0.0510 0.0826 0.0499 0.1197 0.0588\nLightGCL 0.1040 0.0591 0.2921 0.1648 0.0864 0.0516 0.0848 0.0520 0.1291 0.0628\nXSimGCL 0.1346 0.0796 0.2848 0.1683 0.0887 0.0558 0.0863 0.0522 0.1293 0.0641\nGFormer 0.1366 0.0812 0.2807 0.1648 0.0978 0.0602 0.0864 0.0520 0.1083 0.0532\nSign-aware\nGraph-based RS\nSiReN 0.1369 0.0801 0.2880 0.1725 0.0804 0.0492 0.0826 0.0473 0.1167 0.0571\nSiGRec 0.1092 0.0648 0.1591 0.0896 0.0738 0.0475 0.0497 0.0314 0.1266 0.0699\nPANE-GNN 0.1361 0.0810 0.2691 0.1605 0.0532 0.0301 0.0806 0.0514 0.1066 0.0522\nSigned Graph\nEmbedding Methods\nSBGNN 0.0183 0.0100 0.0641 0.0325 0.0249 0.0143 0.0797 0.0469 0.0750 0.0361\nSLGNN 0.0283 0.0148 0.1498 0.0788 0.0585 0.0336 0.0865 0.0508 0.1082 0.0520\nGraph Transformer SGFormer 0.0492 0.0275 0.2402 0.1373 0.0588 0.0343 0.0840 0.0504 0.0883 0.0423\nSignGT 0.0231 0.0121 0.1283 0.0666 0.0521 0.0300 0.0861 0.0515 0.0927 0.0439\nOur Method SIGformer 0.1412* 0.0828* 0.3091* 0.1827* 0.0974 0.0585 0.0908* 0.0539* 0.1494* 0.0722*\n+3.09% +1.96% +5.81% +5.87% -0.41% -2.77% +5.05% +3.32% +15.61% +3.33%\nConsidering the similarities between GFormer and SHT [ 62],\nand acknowledging that GFormer is a more recent development\ndemonstrating better performance than SHT, we simply take\nGFormer for comparison.\n2) Sign-aware Graph-based Recommendation Methods.\nThe following methods utilize both positive and negative feedback:\nâ€¢SiReN [48]: the classic sign-aware recommendation method that\nlearns two sets of embeddings from positive and negative graphs\nfor recommendation.\nâ€¢SiGRec [30]: the representative work that analyzes the role of the\nnegative graph and accordingly leverages GNNs to learn positive\nand negative embeddings.\nâ€¢PANE-GNN [37]: the state-of-the-art method that leverages con-\ntrastive learning in the sign-aware graph-based recommendation\nmodel.\n3) Signed Graph Representation Methods. To further validate\nthe effectiveness of our SIGformer, we include the following signed\ngraph representation methods from the field of graph mining. We\nadapt these methods to recommendation tasks with additional BPR\nloss.\nâ€¢SBGNN [28]: the highly related work utilizing signed bipartite\ngraph neural networks based on the balance principle [14].\nâ€¢SLGNN [39]: the state-of-the-art signed graph representation\nmethod leveraging signed Laplacian graph neural networks.\n4) Unsigned Graph Representation with Transformer. Two\nstate-of-the-art unsigned graph transformers are included. Anal-\nogously, we adapt these methods to recommendation tasks with\nadditional BPR loss.\nâ€¢SGFormer [60]: the state-of-the-art graph representation method\nleveraging a simple global attention mechanism.\nâ€¢SignGT [11]: the state-of-the-art graph transformer method that\ncan produce signed attention values in their attention modules.\n4.1.4 Parameter Settings. For our SIGformer, we adopt the Adam\noptimizer and search the hyperparameter with grid search. Specif-\nically, we set the hidden embedding dimension ğ‘‘ to 64, which is\nalignment with recent work [ 42, 48]. We also draw similar con-\nclusion with their dimensions. We simply set the learning rate to\n1ğ‘’âˆ’2, the weight decay to 1ğ‘’âˆ’4, the number of eigenvectors ğ‘‘â„\nto 64, and the layers of transformer to ğ¿= 3. We search forğ›¼ in the\nrange of [âˆ’0.8,0.8]with step-size 0.2, and ğ›½ in the range of [âˆ’1,1]\nwith step-size 0.2. The threshold length ğ¿ğ‘ is chosen in the range\nof {1,2,3,4,5,6}.\nFor the compared methods, we use the source code provided\nofficially and follow the instructions in the original papers to search\nfor the optimal hyperparameters. We have traversed and frequently\nexpanded upon, the entire hyperparameter space suggested by the\nauthors to ensure all compared methods achieve optimal perfor-\nmance.\n4.2 Performance Comparison (RQ1)\nThe performance comparison between our SIGformer and all base-\nlines in terms of ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@20 and ğ‘ğ·ğ¶ğº@20 is shown in Table 2.\nOverall, our SIGformer outperforms all compared methods across\nall datasets with few exceptions. Especially in the dataset KuaiRand,\nSIGformer achieves impressive improvements â€” 15.6% and 3.3% in\nterms of Recall@20 and NDCG@20 respectively. While SIGformer\nperforms slightly worse than GFormer in the dataset Epinions, it\nstill outperforms other baselines. It is worth noting that GFormer\nexhibits instability and even performs worse than basic LightGCN\nin KuaiRand.\nComparing with Unsigned RS. Generally speaking, our SIG-\nformer outperforms existing unsigned graph-based recommenda-\ntion methods, while some baselines have adopted subtle contrastive\nlearning strategies. The reason is that they overlook the negative\nfeedback, which also provides rich collaborative information bene-\nfiting recommendation.\nComparing with Sign-aware RS. Our SIGformer consistently\nsurpasses existing sign-aware graph-based recommendation meth-\nods, validating the superiority of our sign-aware transformer ar-\nchitecture that fully exploits the entire signed graph, as opposed\nto methods that separately handle positive and negative graphs.\nAdditionally, GNNs and MLPs might not effectively extract informa-\ntion from the negative graph, potentially causing these sign-aware\nmethods to underperform when compared to unsigned graph-based\nSIGformer: Sign-aware Graph Transformer for Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nTable 3: The results of the ablation study, where positional encodings or negative interactions are removed respectively.\nNegative\nInteractions?\nSpectral\nEncoding?\nPath\nEncoding?\nAmazon-CDs Amazon-Music Epinions KuaiRec KuaiRand\nRecall NDCG Recall NDCG Recall NDCG Recall NDCG Recall NDCG\nSIGformer-w/o-Neg âœ“ âœ“ 0.1349 0.0775 0.2937 0.1738 0.0824 0.0477 0.0708 0.0433 0.1173 0.0545\nSIGformer-w/o-En âœ“ 0.1355 0.0779 0.2932 0.1698 0.0894 0.0526 0.0728 0.0448 0.1413 0.0661\nSIGformer-w/o-SPE âœ“ âœ“ 0.1380 0.0798 0.2988 0.1744 0.0959 0.0574 0.0862 0.0520 0.1471 0.0697\nSIGformer-w/o-SSE âœ“ âœ“ 0.1381 0.0812 0.2947 0.1758 0.0945 0.0566 0.0866 0.0515 0.1457 0.0703\nSIGformer âœ“ âœ“ âœ“ 0.1412 0.0828 0.3091 0.1827 0.0974 0.0585 0.0908 0.0539 0.1494 0.0722\nmethods. This is evident in the performance of PANE-GNN, SiGRec,\nand SiReN, which are worse than LightGCN in the Epinions dataset.\nComparing with Signed Graph Embedding Methods. SIG-\nformer consistently surpasses all competing methods in signed\ngraph representation across various datasets. Remarkably, these\nbaselines generally exhibit subpar performance, indicating their\ninadequacy for recommendation tasks. This outcome can be attrib-\nuted to two key factors: 1) The majority of existing signed graph\nmethods are predicated on balance theory [14] that triads in a graph\nshould have an even number of negative edges. Such an assump-\ntion may be overly rigid given the complex and diverse nature of\nuser preference [48]. 2) These methods often utilize a considerable\nnumber of parameters and non-linear modules, which struggle to\nbe effectively trained in RS due to its sparse data.\nComparing with Graph Transformer. Our SIGformer still\noutperforms these graph transformers. This result validates the\neffectiveness of our SSE and SPE encodings, which are tailored\nfor sign-aware recommendation. Existing graph transformers nei-\nther take the signed relations into consideration nor specifically\ndesigned for recommendation.\n4.3 Ablation Study (RQ2)\nWe conduct an ablation study to investigate the effects of differ-\nent modules in SIGformer. The results in terms of ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@20 and\nğ‘ğ·ğ¶ğº@20 are shown in Table 3, where the two positional encod-\nings and negative interactions are removed respectively.\nEffects of positional encodings. As can be seen, when remov-\ning spectral (SSE) or path (SPE) positional encodings, we consis-\ntently observe the performance drops, i.e., SIGformer-w/o-SSE and\nSIGformer-w/o-SPE exhibit noticeably inferior performance than\nSIGformer. This result clearly validates the effectiveness of our\npositional encodings that capture collaborative information from\nspectral and path perspectives.\nEffect of negative interactions. The removal of negative in-\nteractions from the training data results in a noticeable perfor-\nmance decline in our SIGformer model. This outcome reveals the\nsignificance of negative feedback, affirming our methodâ€™s ability to\neffectively leverage its benefits.\n4.4 Role of the parameters (RQ3)\nLength Threshold ğ¿ğ‘ in Sampling. As Figure 3 shows, with\nğ¿ğ‘ increasing, the performance of SIGformer generally exhibits\nan initial improvement followed by a decline. This is because a\nlarger ğ¿ğ‘ provides the model with a broader receptive field, which\ncan enhance the modelâ€™s performance. However, the correlations\nbetween higher-order neighbors are weaker than those between\nlower-order neighbors, so an excessively large receptive field may\ndilute the impact of lower-order neighbors and may even bring\nmore noise.\nHyperparameter ğ›¼. This hyperparameter controls the impact\nof negative interactions in sign-aware spectral encoding. As de-\npicted in Figure 3, SIGformer generally demonstrates an initial\nenhancement, followed by a decrement as ğ›¼ increases. This trend\ncan be attributed to the role ofğ›¼in balancing the impacts of positive\nand negative feedback â€” an overly large or small influence of the\nnegative aspect is suboptimal. Upon examining the optimal values\nof ğ›¼, we observe that it is located at minor negative values for the\ndatasets Amazon-CDs, Amazon-Music, Epinions, and KuaiRand.\nThis can be rationalized by their feedback types, namely rating and\nclick. Although these negatively rated or unclicked items are indeed\nless preferred by users compared to positive ones, they might still\nbe more favored than items with which users have not interacted.\nSimilar conclusions have been drawn in [30]. However, in the case\nof KuaiRec, the negative feedback, which signifies users swiftly\nskipping the item, implies a strong aversion towards these items.\nThese results validate the flexibility of our SIGformer, it can adjust\nthe role of the negative feedback.\n4.5 Case Study (RQ4)\nTo investigate how SIGformer understands different path patterns,\nwe present the top-5 and bottom-5 values of the learned ğœ‘ from\nKuaiRec for paths with lengths up to 4, which captures the node\nsimilarity indicated by each path type. These results are depicted\nin Figure 4. Here we simply choose parameters in the first layer for\nillustration. The majority of these results align with our expecta-\ntions. For example, path types such as â€œ+â€ and â€œâˆ’âˆ’+â€ reflect high\naffinity while path types â€œâˆ’âˆ’âˆ’+â€ and â€œâˆ’+â€ suggest low affinity.\nFurther, given the complexity of user preference, these results also\nreveal some fresh knowledge, e.g., the path type â€œ+âˆ’+â€ suggests\nstrong positive relations.\n4.6 Efficiency Comparison (RQ5)\nThe running time of SIGformer compared with other baselines\non three large datasets are depicted in Figure 5. We can find the\ntransformer equipped with a random-walker-based strategy, does\nnot impose a heavy computational burden. The efficiency of our\nSIGformer is comparable with SiGRec and LightGCN, and much\nfaster than SiReN, PANE-GNN, and GFormer, which employs heavy\nMLPs or contrastive learning. As the random walker can be quickly\nimplemented in GPUs, SIGformer sometimes exhibits even higher\nspeed than LightGCN.\n5 RELATED WORK\n5.1 Graph-based Recommender System\nGraph-based methods have drawn significant attention in the field\nof RS. Compared to traditional collaborative filtering methods such\nas matrix factorization [35] and autoencoders [41], which only uti-\nlize first-order interaction information, graph-based methods can\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Sirui Chen, et al.\n1 2 3 4 5 6\nLp\n0.129\n0.132\n0.135\n0.138\n0.141Recall@20\nAmazon-CDs\n1 2 3 4 5 6\nLp\n0.28\n0.29\n0.30\n0.31\nAmazon-Music\n1 2 3 4 5 6\nLp\n0.0900\n0.0925\n0.0950\n0.0975\nEpinions\n1 2 3 4 5 6\nLp\n0.086\n0.088\n0.090\n0.092\nKuaiRec\n1 2 3 4 5 6\nLp\n0.136\n0.140\n0.144\n0.148\n0.152\nKuaiRand\n-0.8 -0.6 -0.4 -0.2\n0 0.2\n0.1401\n0.1404\n0.1407\n0.1410\n0.1413Recall@20\nAmazon-CDs\n-0.8 -0.6 -0.4 -0.2\n0 0.2\n0.21\n0.24\n0.27\n0.30\n0.33\nAmazon-Music\n-0.8 -0.6 -0.4 -0.2\n0 0.2\n0.0954\n0.0960\n0.0966\n0.0972\n0.0978\nEpinions\n-0.2\n0 0.2 0.4 0.6 0.8\n0.084\n0.086\n0.088\n0.090\n0.092\nKuaiRec\n-0.8 -0.6 -0.4 -0.2\n0 0.2\n0.1440\n0.1455\n0.1470\n0.1485\n0.1500\nKuaiRand\nFigure 3: Performance in terms of ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@20 with different ğ¾ and ğ›¼.\nğœ†!ğœ†\"ğœ†# ğœ†$%&\nâ‹¯\nID\n=Ã—\nğœ‘(\")\nğ¡!ğ¡\"ğ¡# ğ¡$%&\nMatMul\nScale\nSoftmax\nSoftmax\n+\n#ğ‡$ #ğ‡\nMatMul\n+ğ%(\")\nğ&(\")\nğ„(\"'()\nğ„(\")\nğ(() ğŠ(() ğ•(()â‹®\nğ¡!ğ¡\"ğ¡#ğ¡+,â‹¯ ğ¡!ğ¡\"ğ¡#ğ¡+,â‹® \nSign-aware Path Encoding\nSign-aware Spectral Encoding\nğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\" ğ‘–-ğ‘–#ğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\"ğ‘–#ğ‘–-\nğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\"ğ‘–#ğ‘–-\nâ‹¯\nğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\"ğ‘–#ğ‘–-\nğ‘¢!ğ‘¢\"ğ‘¢#ğ‘–!ğ‘–\" ğ‘–-ğ‘–#\nÃ—001001â‹¯ğ¨*/,*0$ =ğ‘*/,*0(\")\nâ‹®\n++-+---++\n123456â‹® â‹®\nSpectral frequencies\nPath\nPositive FeedbackNegative Feedback\nğ‘¢!\nğ‘¢\"\nğ‘¢#\nğ‘–!\nğ‘–\"\nğ‘–#\nğ‘–-\n(a)\tTop-5\tvalues\tof\tğœ‘ (b)\tBottom-5\tvalues\tof\tğœ‘Path\tpatternsValuesPath\tpatternsValues1.704\t -1.303\t1.661\t -1.226\t1.658\t -1.225\t1.654\t -1.210\t1.654\t -1.116\t\n-\n++--+--++-++-- ++--+--++-++--\n---+--++-+-+-+-+++\nFigure 4: The top-5 and bottom-5 values of the learned ğœ‘from\nKuaiRec.\nMethods0\n250\n500\n750\n1000RunTime (min)\nAmazon-CDs\nMethods0\n100\n200\n300\nEpinions\nMethods0\n50\n100\n150\nKuaiRand\nLightGCN GFormer SiReN PANE-GNN SiGRec SIGformer\nFigure 5: Runtime comparison of SIGformer with baselines.\nleverage higher-order relations between nodes in user-item bipar-\ntite graph and thus exhibits better performance [61]. In the early\nyears, Wang et al. [54], Ying et al. [66] and Fan et al. [20] directly\nleveraged graph neural network in recommendation to encode the\ngraph structure information in the representation; LightGCN fur-\nther [24] simplified the architecture of GCN for recommendation\nby removing feature transformation and nonlinear activation oper-\nations, IMix[13] enhanced the generalizability of GCN by blending\ninteracted and non-interacted item pairs for the same user; subse-\nquently, various graph-based methods emerged that were based\non contrastive learning, e.g., SGL [58], LightGCL [3], SimGCL [68],\nXSimGCL [67], etc; other recent studies identified biases in data\nused for recommendations[8, 9] and improved existing methods\nfrom the perspective of robustness[53, 56, 57]. The transformer ar-\nchitecture has also been utilized by some recent work (e.g., Gformer\n[37], SHT [62]) to improve the quality of the data augmentation.\nHowever, we highlight the following differences between our SIG-\nformer with Gformer and SHT: 1) We directly utilize transformer as\nthe backbone architecture, while the transformer in Gformer and\nSHT serves as an auxiliary role to generate augmentation; 2) our\nSIGformer is tailored for sign-aware recommendation, while they\nare designed for unsigned graph-based recommendation.\nIn addition to the aforementioned graph-based methods that\npredominantly focus on positive data, some studies have explored\nleveraging both positive and negative feedback in graph-based rec-\nommendation. A pioneering example is SiReN [48], which learned\nboth positive and negative embeddings from corresponding graphs\nthrough GNNs and MLPs respectively, and subsequently combines\nthese embeddings for recommendation. Huang et al.[30] conducted\ncomprehensive analyses to reveal the role of negative feedback and\ndeveloped a new method SiGRec, which enhances the learning of\nboth positive and negative embeddings via GNNs. PANE-GNN [42]\nfurther integrates contrastive learning into negative graph repre-\nsentation learning. Our SIGformer advances beyond these methods\nin two aspects: 1) Rather than segregating the graph into positive\nand negative components, we harness the entire signed graph to\nlearn embeddings; 2) We utilize transformer architecture, which is\nmore effective in extracting collaborative information from nega-\ntive feedback than MLPs and GNNs employed by these previous\nmethods.\n5.2 Sign-aware Recommender System\nThe exploration of sign-aware recommendation traces its origins\nback to the early years of the field, with initial research predom-\ninantly focusing on explicit feedback . Explicit feedback, i.e., user\nratings, directly indicates usersâ€™ positive and negative attitudes\ntowards items. This era saw the development of many classic rec-\nommendation methods, such as user-based CF [71], PMF [44], and\nSVD++ [35], etc. However, as the focus of research shifted from\nexplicit to implicit feedback (e.g., clicks, purchases), studies on sign-\naware recommendation became less prevalent. Recently, however,\ndue to the availability of negative feedback in many modern RS,\nsign-aware recommendation has regained significant attention. For\ninstance, the role of negative feedback in RS has been comprehen-\nsively investigated in works such as [ 32, 63]. Negative feedback\nhas also been employed to enhance various recommendation tasks,\nincluding graph-based recommendation [30, 42, 48], negative sam-\nplers [15, 16], interactive recommendation [ 70], and sequential\nrecommendation [45, 46], etc.\nSIGformer: Sign-aware Graph Transformer for Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\n5.3 Graph Transformer\nTransformer [52] has been successfully applied in graph representa-\ntion learning tasks. Transformer addresses key issues in GNNs, such\nas over-smoothing [5, 17], over-squashing [1], and limitations in\nexpressive power [64], which stem from the message-passing mech-\nanism that aggregates information from direct neighbors. However,\nthe success of transformer in this domain usually relies on posi-\ntional encodings that integrate graph structural information into\nthe transformer framework. Recent years have seen a variety of\nsophisticated designs for positional encodings, including node de-\ngrees [65], shortest paths [ 38, 65], subgraph characteristics [ 6],\nedge relations [47], and spectral features [19, 36]. Other works have\nsought to enhance the transformer model from different perspec-\ntives. For instance, SGformer [60] and NAGphormer [10] propose\nsimplified graph transformer models for more efficient and effective\nrepresentation learning; while SignGT [11] introduces signed at-\ntention values to adaptively capture diverse frequency information\nbetween node pairs. Despite these advancements, to the best of our\nknowledge, there remains a notable gap in transformer architec-\ntures specifically tailored for the signed graph.\n5.4 Signed Graph Representation Learning\nConsidering both positive and negative edges are available in many\napplications, signed graph representation learning draws increasing\nattention. Early strategies on this task include eigen-decomposition\nof signed Laplacian [ 26] and matrix factorization [ 27]. In recent\nyears, research has primarily relied on the balance theory that triads\nin a graph should have an even number of negative edges [14, 25].\nSGCN [14] was the first to extend GCN to the signed graph and de-\nsigned a new information aggregation and propagation mechanism\nbased on the balance theory for signed networks. SIDE [ 34] and\nSIGNET [31] maintained structural balance through random walk\nstrategies. SiGAT [29] and SNEA [40] further designed graph atten-\ntion mechanisms suitable for signed networks based on the balance\ntheory. SBGNN [28] explored the balance theory in the bipartite\ngraph and proposed a new graph neural network model for learn-\ning node representations in the signed bipartite graph. SLGNN[39]\nreturned to spectral graph theory and designed low-pass and high-\npass graph convolution filters to extract low-frequency and high-\nfrequency information on positive and negative edges. SBGCL[69]\nintroduced contrastive learning, utilizing dual-level data augmenta-\ntion to capture explicit and implicit relationships among nodes in\nsigned bipartite graphs, thereby enhancing robustness. However,\nthese methods often can not be directly applied in RS: 1) these meth-\nods are often predicated on balance theory, which may be overly\nrigid given the complex and diverse nature of user preferences [48];\n2) These methods frequently employ a substantial number of pa-\nrameters and non-linear modules, which struggle to be effectively\ntrained in recommendation systems due to the inherently sparse\nnature of the data.\n6 CONCLUSION AND FUTURE WORK\nThis study introduces SIGformer, a novel sign-aware recommen-\ndation method that utilizes the transformer architecture to com-\nprehensively harness the collaborative information inherent in the\nsigned graph. Within SIGformer, we have innovatively integrated\ntwo positional encodings to capture the spectral properties and path\npatterns of the signed graph. Extensive experiments have been con-\nducted to demonstrate the superior performance of SIGformer over\nexisting graph-based recommendation methods.\nA promising direction for future research is the development of a\nmore rapid sign-aware graph transformer architecture. While SIG-\nformer exhibits efficiency, it relies on a sampling strategy that could\nintroduce variance and potentially affect model performance. Ad-\nditionally, there is significant potential in creating more advanced\npositional encodings, such as those that fully leverage signed graph\nspectrum or incorporate additional content information, to further\nenhance the capabilities of the transformer in RS.\nACKNOWLEDGMENTS\nThis work is supported by the Starry Night Science Fund of Zhejiang\nUniversity Shanghai Institute for Advanced Study (SN-ZJU-SIAS-\n001), OPPO Research Fund, the National Natural Science Foundation\nof China (62372399), and the advanced computing resources pro-\nvided by the Supercomputing Center of Hangzhou City University.\nA APPENDICES\nA.1 The proof of Lemma 1\nProof. According to the properties of symmetric normalized\nLaplacian,\nzğ‘‡\nğ‘˜L+zğ‘˜ = zğ‘‡\nğ‘˜(I âˆ’(D+)âˆ’1\n2 A+(D+)âˆ’1\n2 )zğ‘˜ (15)\n= zğ‘‡\nğ‘˜(D+)âˆ’1\n2 (D+âˆ’A+)(D+)âˆ’1\n2 zğ‘˜ (16)\n=\nâˆ‘ï¸\n1â‰¤ğ‘£â‰¤ğ‘›+ğ‘š\nâˆ‘ï¸\n1â‰¤ğ‘¤â‰¤ğ‘›+ğ‘š\nğ´+\nğ‘£ğ‘¤(zğ‘˜ğ‘£âˆšï¸ƒ\nğ‘‘+ğ‘£\nâˆ’ zğ‘˜ğ‘¤âˆšï¸ƒ\nğ‘‘+ğ‘¤\n)2 (17)\n=\nâˆ‘ï¸\n(ğ‘¢,ğ‘–)âˆˆE+\n2(zğ‘˜ğ‘¢âˆšï¸ƒ\nğ‘‘+ğ‘¢\nâˆ’ zğ‘˜ğ‘–âˆšï¸ƒ\nğ‘‘+\nğ‘–\n)2 (18)\nSimilarly, it can be proven that\nzğ‘‡\nğ‘˜Lâˆ’zğ‘˜ =\nâˆ‘ï¸\n(ğ‘¢,ğ‘–)âˆˆEâˆ’\n2(zğ‘˜ğ‘¢âˆšï¸ƒ\nğ‘‘âˆ’ğ‘¢\nâˆ’ zğ‘˜ğ‘–âˆšï¸ƒ\nğ‘‘âˆ’\nğ‘–\n)2 (19)\nThe optimization goal in Eq(11) can be written as:\narg min\nz1,z2,...,zğ‘‘â„\nâˆ‘ï¸\n1â‰¤ğ‘˜â‰¤ğ‘‘â„\n1\n2 (zğ‘‡\nğ‘˜L+zğ‘˜ âˆ’ğ›¼zğ‘‡\nğ‘˜Lâˆ’zğ‘˜) (20)\n= arg min\nz1,z2,...,zğ‘‘â„\nâˆ‘ï¸\n1â‰¤ğ‘˜â‰¤ğ‘‘â„\n1 âˆ’ğ›¼\n2 zğ‘‡\nğ‘˜Lzğ‘˜ (21)\n= arg min\nz1,z2,...,zğ‘‘â„\nâˆ‘ï¸\n1â‰¤ğ‘˜â‰¤ğ‘‘â„\nzğ‘‡\nğ‘˜Lzğ‘˜ (22)\nwhere we have omitted the explicit representation of the constraint\nfor the sake of clarity. The constraint ensures thatz1,z2,...zğ‘‘â„ form\na set of ğ‘‘â„ mutually orthogonal unit vectors. Eq(22) represents a\nconstrained quadratic form, which attains its minimum value when\nz1,z2,...zğ‘‘â„ are chosen as the eigenvectors ofL with the ğ‘‘â„ smallest\neigenvalues.\nTherefore, [h1,h2,..., hğ‘‘â„]is a solution to Eq(11). â–¡\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Sirui Chen, et al.\nREFERENCES\n[1] Uri Alon and Eran Yahav. 2021. On the Bottleneck of Graph Neural Networks and\nits Practical Implications. InInternational Conference on Learning Representations .\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877â€“1901.\n[3] Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. 2023. LightGCL: Simple\nYet Effective Graph Contrastive Learning for Recommendation. In The Eleventh\nInternational Conference on Learning Representations .\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-\nder Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with\ntransformers. In European conference on computer vision . Springer, 213â€“229.\n[5] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. 2020. Measuring\nand relieving the over-smoothing problem for graph neural networks from the\ntopological view. In Proceedings of the AAAI conference on artificial intelligence ,\nVol. 34. 3438â€“3445.\n[6] Dexiong Chen, Leslie Oâ€™Bray, and Karsten Borgwardt. 2022. Structure-aware\ntransformer for graph representation learning. In International Conference on\nMachine Learning . PMLR, 3469â€“3489.\n[7] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua\nLiu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. 2021. Pre-trained image\nprocessing transformer. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition . 12299â€“12310.\n[8] Jiawei Chen, Hande Dong, Yang Qiu, Xiangnan He, Xin Xin, Liang Chen, Guli\nLin, and Keping Yang. 2021. AutoDebias: Learning to debias for recommendation.\nIn Proceedings of the 44th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval . 21â€“30.\n[9] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan\nHe. 2023. Bias and debias in recommender system: A survey and future directions.\nACM Transactions on Information Systems 41, 3 (2023), 1â€“39.\n[10] Jinsong Chen, Kaiyuan Gao, Gaichao Li, and Kun He. 2022. NAGphormer: A tok-\nenized graph transformer for node classification in large graphs. In The Eleventh\nInternational Conference on Learning Representations .\n[11] Jinsong Chen, Gaichao Li, John E Hopcroft, and Kun He. 2023. Signgt: Signed\nattention-based graph transformer for graph representation learning. arXiv\npreprint arXiv:2310.11025 (2023).\n[12] Fan RK Chung. 1997. Spectral graph theory . Vol. 92. American Mathematical Soc.\n[13] Leyan Deng, Defu Lian, Chenwang Wu, and Enhong Chen. 2022. Graph convolu-\ntion network based recommender systems: Learning guarantee and item mixture\npowered strategy. Advances in Neural Information Processing Systems 35 (2022),\n3900â€“3912.\n[14] Tyler Derr, Yao Ma, and Jiliang Tang. 2018. Signed graph convolutional networks.\nIn 2018 IEEE International Conference on Data Mining (ICDM) . IEEE, 929â€“934.\n[15] Jingtao Ding, Fuli Feng, Xiangnan He, Guanghui Yu, Yong Li, and Depeng Jin.\n2018. An improved sampler for bayesian personalized ranking by leveraging\nview data. In Companion Proceedings of the The Web Conference 2018 . 13â€“14.\n[16] Jingtao Ding, Yuhan Quan, Xiangnan He, Yong Li, and Depeng Jin. 2019. Rein-\nforced Negative Sampling for Recommendation with Exposure Data.. In IJCAI.\nMacao, 2230â€“2236.\n[17] Hande Dong, Jiawei Chen, Fuli Feng, Xiangnan He, Shuxian Bi, Zhaolin Ding,\nand Peng Cui. 2021. On the equivalence of decoupled graph convolution network\nand label propagation. In Proceedings of the Web Conference 2021 . 3651â€“3662.\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at Scale. InInternational\nConference on Learning Representations .\n[19] Vijay Prakash Dwivedi and Xavier Bresson. 2021. A Generalization of Trans-\nformer Networks to Graphs.AAAI Workshop on Deep Learning on Graphs: Methods\nand Applications (2021).\n[20] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.\n2019. Graph neural networks for social recommendation. In The world wide web\nconference. 417â€“426.\n[21] Ziwei Fan, Zhiwei Liu, Jiawei Zhang, Yun Xiong, Lei Zheng, and Philip S Yu.\n2021. Continuous-time sequential recommendation with temporal graph collab-\norative transformer. In Proceedings of the 30th ACM international conference on\ninformation & knowledge management . 433â€“442.\n[22] Chongming Gao, Shijun Li, Wenqiang Lei, Jiawei Chen, Biao Li, Peng Jiang,\nXiangnan He, Jiaxin Mao, and Tat-Seng Chua. 2022. KuaiRec: A fully-observed\ndataset and insights for evaluating recommender systems. In Proceedings of the\n31st ACM International Conference on Information & Knowledge Management .\n540â€“550.\n[23] Chongming Gao, Shijun Li, Yuan Zhang, Jiawei Chen, Biao Li, Wenqiang Lei,\nPeng Jiang, and Xiangnan He. 2022. KuaiRand: An Unbiased Sequential Rec-\nommendation Dataset with Randomly Exposed Videos. In Proceedings of the\n31st ACM International Conference on Information & Knowledge Management .\n3953â€“3957.\n[24] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng\nWang. 2020. Lightgcn: Simplifying and powering graph convolution network for\nrecommendation. In Proceedings of the 43rd International ACM SIGIR conference\non research and development in Information Retrieval . 639â€“648.\n[25] Fritz Heider. 1946. Attitudes and cognitive organization.The Journal of psychology\n21, 1 (1946), 107â€“112.\n[26] Yaoping Hou, Jiongsheng Li, and Yongliang Pan. 2003. On the Laplacian eigen-\nvalues of signed graphs. Linear and Multilinear Algebra 51, 1 (2003), 21â€“30.\n[27] Cho-Jui Hsieh, Kai-Yang Chiang, and Inderjit S Dhillon. 2012. Low rank mod-\neling of signed networks. In Proceedings of the 18th ACM SIGKDD international\nconference on Knowledge discovery and data mining . 507â€“515.\n[28] Junjie Huang, Huawei Shen, Qi Cao, Shuchang Tao, and Xueqi Cheng. 2021.\nSigned bipartite graph neural networks. In Proceedings of the 30th ACM Interna-\ntional Conference on Information & Knowledge Management . 740â€“749.\n[29] Junjie Huang, Huawei Shen, Liang Hou, and Xueqi Cheng. 2019. Signed graph\nattention networks. In Artificial Neural Networks and Machine Learningâ€“ICANN\n2019: Workshop and Special Sessions: 28th International Conference on Artificial Neu-\nral Networks, Munich, Germany, September 17â€“19, 2019, Proceedings 28 . Springer,\n566â€“577.\n[30] Junjie Huang, Ruobing Xie, Qi Cao, Huawei Shen, Shaoliang Zhang, Feng Xia,\nand Xueqi Cheng. 2023. Negative can be positive: Signed graph neural networks\nfor recommendation. Information Processing & Management 60, 4 (2023), 103403.\n[31] Mohammad Raihanul Islam, B Aditya Prakash, and Naren Ramakrishnan. 2018.\nSignet: Scalable embeddings for signed networks. In Advances in Knowledge\nDiscovery and Data Mining: 22nd Pacific-Asia Conference, PAKDD 2018, Melbourne,\nVIC, Australia, June 3-6, 2018, Proceedings, Part II 22 . Springer, 157â€“169.\n[32] Olivier Jeunen. 2019. Revisiting offline evaluation for implicit-feedback recom-\nmender systems. In Proceedings of the 13th ACM Conference on Recommender\nSystems. 596â€“600.\n[33] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of NAACL-HLT . 4171â€“4186.\n[34] Junghwan Kim, Haekyu Park, Ji-Eun Lee, and U Kang. 2018. Side: representation\nlearning in signed directed networks. In Proceedings of the 2018 world wide web\nconference. 509â€“518.\n[35] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-\nniques for recommender systems. Computer 42, 8 (2009), 30â€“37.\n[36] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent LÃ©tourneau, and Pru-\ndencio Tossou. 2021. Rethinking graph transformers with spectral attention.\nAdvances in Neural Information Processing Systems 34 (2021), 21618â€“21629.\n[37] Chaoliu Li, Lianghao Xia, Xubin Ren, Yaowen Ye, Yong Xu, and Chao Huang.\n2023. Graph Transformer for Recommendation. arXiv preprint arXiv:2306.02330\n(2023).\n[38] Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. 2020. Distance encod-\ning: Design provably more powerful neural networks for graph representation\nlearning. Advances in Neural Information Processing Systems 33 (2020), 4465â€“4478.\n[39] Yu Li, Meng Qu, Jian Tang, and Yi Chang. 2023. Signed laplacian graph neural\nnetworks. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 37.\n4444â€“4452.\n[40] Yu Li, Yuan Tian, Jiawei Zhang, and Yi Chang. 2020. Learning signed network\nembedding via graph attention. In Proceedings of the AAAI conference on artificial\nintelligence, Vol. 34. 4772â€“4779.\n[41] Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018.\nVariational autoencoders for collaborative filtering. In Proceedings of the 2018\nworld wide web conference . 689â€“698.\n[42] Ziyang Liu, Chaokun Wang, Jingcao Xu, Cheng Wu, Kai Zheng, Yang Song, Na\nMou, and Kun Gai. 2023. PANE-GNN: Unifying Positive and Negative Edges in\nGraph Neural Networks for Recommendation. arXiv preprint arXiv:2306.04095\n(2023).\n[43] Julian John McAuley and Jure Leskovec. 2013. From amateurs to connoisseurs:\nmodeling the evolution of user expertise through online reviews. In Proceedings\nof the 22nd international conference on World Wide Web . 897â€“908.\n[44] Andriy Mnih and Russ R Salakhutdinov. 2007. Probabilistic matrix factorization.\nAdvances in neural information processing systems 20 (2007).\n[45] Yunzhu Pan, Chen Gao, Jianxin Chang, Yanan Niu, Yang Song, Kun Gai, Depeng\nJin, and Yong Li. 2023. Understanding and Modeling Passive-Negative Feedback\nfor Short-video Sequential Recommendation. In Proceedings of the 17th ACM\nConference on Recommender Systems . 540â€“550.\n[46] Minju Park and Kyogu Lee. 2022. Exploiting Negative Preference in Content-\nbased Music Recommendation with Contrastive Learning. In Proceedings of the\n16th ACM Conference on Recommender Systems . 229â€“236.\n[47] Wonpyo Park, Woong-Gi Chang, Donggeon Lee, Juntae Kim, and Seungwon\nHwang. 2022. GRPE: Relative Positional Encoding for Graph Transformer. In\nICLR2022 Machine Learning for Drug Discovery .\n[48] Changwon Seo, Kyeong-Joong Jeong, Sungsu Lim, and Won-Yong Shin. 2022.\nSiReN: Sign-aware recommendation using graph neural networks. IEEE Transac-\ntions on Neural Networks and Learning Systems (2022).\nSIGformer: Sign-aware Graph Transformer for Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\n[49] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre\nVandergheynst. 2013. The emerging field of signal processing on graphs: Ex-\ntending high-dimensional data analysis to networks and other irregular domains.\nIEEE signal processing magazine 30, 3 (2013), 83â€“98.\n[50] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.\n2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-\nresentations from transformer. In Proceedings of the 28th ACM international\nconference on information and knowledge management . 1441â€“1450.\n[51] Jiliang Tang, Huiji Gao, Huan Liu, and Atish Das Sarma. 2012. eTrust: Understand-\ning trust evolution in an online world. In Proceedings of the 18th ACM SIGKDD\ninternational conference on Knowledge discovery and data mining . 253â€“261.\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[53] Bohao Wang, Jiawei Chen, Changdong Li, Sheng Zhou, Qihao Shi, Yang Gao, Yan\nFeng, Chun Chen, and Can Wang. 2024. Distributionally Robust Graph-based\nRecommendation System. arXiv preprint arXiv:2402.12994 (2024).\n[54] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\nNeural graph collaborative filtering. In Proceedings of the 42nd international ACM\nSIGIR conference on Research and development in Information Retrieval . 165â€“174.\n[55] Xiyuan Wang and Muhan Zhang. 2022. How powerful are spectral graph neural\nnetworks. In International Conference on Machine Learning . PMLR, 23341â€“23362.\n[56] Junkang Wu, Jiawei Chen, Jiancan Wu, Wentao Shi, Xiang Wang, and Xiang-\nnan He. 2024. Understanding contrastive learning via distributionally robust\noptimization. Advances in Neural Information Processing Systems 36 (2024).\n[57] Junkang Wu, Jiawei Chen, Jiancan Wu, Wentao Shi, Jizhi Zhang, and Xiang Wang.\n2023. BSL: Understanding and Improving Softmax Loss for Recommendation.\narXiv preprint arXiv:2312.12882 (2023).\n[58] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and\nXing Xie. 2021. Self-supervised graph learning for recommendation. In Proceed-\nings of the 44th international ACM SIGIR conference on research and development\nin information retrieval . 726â€“735.\n[59] Liwei Wu, Shuqing Li, Cho-Jui Hsieh, and James Sharpnack. 2020. SSE-PT:\nSequential recommendation via personalized transformer. In Proceedings of the\n14th ACM Conference on Recommender Systems . 328â€“337.\n[60] Qitian Wu, Wentao Zhao, Chenxiao Yang, Hengrui Zhang, Fan Nie, Haitian Jiang,\nYatao Bian, and Junchi Yan. 2023. Simplifying and Empowering Transformers\nfor Large-Graph Representations. arXiv preprint arXiv:2306.10759 (2023).\n[61] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2022. Graph neural\nnetworks in recommender systems: a survey. Comput. Surveys 55, 5 (2022), 1â€“37.\n[62] Lianghao Xia, Chao Huang, and Chuxu Zhang. 2022. Self-supervised hypergraph\ntransformer for recommender systems. In Proceedings of the 28th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining . 2100â€“2109.\n[63] Ruobing Xie, Cheng Ling, Yalong Wang, Rui Wang, Feng Xia, and Leyu Lin. 2021.\nDeep feedback network for recommendation. In Proceedings of the Twenty-Ninth\nInternational Conference on International Joint Conferences on Artificial Intelligence .\n2519â€“2525.\n[64] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful\nare Graph Neural Networks?. In International Conference on Learning Representa-\ntions.\n[65] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,\nYanming Shen, and Tie-Yan Liu. 2021. Do transformers really perform badly\nfor graph representation? Advances in Neural Information Processing Systems 34\n(2021), 28877â€“28888.\n[66] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,\nand Jure Leskovec. 2018. Graph convolutional neural networks for web-scale\nrecommender systems. In Proceedings of the 24th ACM SIGKDD international\nconference on knowledge discovery & data mining . 974â€“983.\n[67] Junliang Yu, Xin Xia, Tong Chen, Lizhen Cui, Nguyen Quoc Viet Hung, and\nHongzhi Yin. 2023. XSimGCL: Towards extremely simple graph contrastive\nlearning for recommendation. IEEE Transactions on Knowledge and Data Engi-\nneering (2023).\n[68] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung\nNguyen. 2022. Are graph augmentations necessary? simple graph contrastive\nlearning for recommendation. In Proceedings of the 45th international ACM SIGIR\nconference on research and development in information retrieval . 1294â€“1303.\n[69] Zeyu Zhang, Jiamou Liu, Kaiqi Zhao, Song Yang, Xianda Zheng, and Yifei Wang.\n2023. Contrastive learning for signed bipartite graphs. In Proceedings of the 46th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. 1629â€“1638.\n[70] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin.\n2018. Recommendations with negative feedback via pairwise deep reinforcement\nlearning. In Proceedings of the 24th ACM SIGKDD international conference on\nknowledge discovery & data mining . 1040â€“1048.\n[71] Zhi-Dan Zhao and Ming-Sheng Shang. 2010. User-based collaborative-filtering\nrecommendation algorithms on hadoop. In 2010 third international conference on\nknowledge discovery and data mining . IEEE, 478â€“481.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6770529747009277
    },
    {
      "name": "Transformer",
      "score": 0.5542301535606384
    },
    {
      "name": "Electrical engineering",
      "score": 0.14190781116485596
    },
    {
      "name": "Engineering",
      "score": 0.10070657730102539
    },
    {
      "name": "Voltage",
      "score": 0.05332601070404053
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204823248",
      "name": "Huazhong Agricultural University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    }
  ]
}