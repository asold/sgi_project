{
    "title": "Guidance for Researchers and Peer-reviewers on the Ethical Use of Large Language Models (LLM) in Scientific Research Workflows",
    "url": "https://openalex.org/W4366769994",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2116947593",
            "name": "Ryan Watkins",
            "affiliations": [
                "George Washington University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4226399820",
        "https://openalex.org/W4362597819",
        "https://openalex.org/W2098846919",
        "https://openalex.org/W2007256608",
        "https://openalex.org/W3039885489",
        "https://openalex.org/W4353112996",
        "https://openalex.org/W1970588407",
        "https://openalex.org/W4320165837",
        "https://openalex.org/W4364385323",
        "https://openalex.org/W4321649710",
        "https://openalex.org/W4376139550",
        "https://openalex.org/W4362655281",
        "https://openalex.org/W4401218004",
        "https://openalex.org/W4306808680",
        "https://openalex.org/W4361807208",
        "https://openalex.org/W4286910674"
    ],
    "abstract": "For researchers interested in exploring the exciting applications of Large Language Models (LLMs) in their scientific investigations, there is currently limited guidance and few norms for them to consult. Similarly, those providing peer-reviews on research articles where LLMs were used are without conventions or standards to apply or guidelines to follow. This situation is understandable given the rapid and recent development of LLMs that are capable of valuable contributions to research workflows (such as OpenAI’s ChatGPT). Nevertheless, now is the time to begin the development of norms, conventions, and standards that can be applied by researchers and peer-reviewers. By applying the principles of Artificial Intelligence (AI) ethics, we can better ensure that the use of LLMs in scientific research aligns with ethical principles and best practices. This editorial hopes to inspire further dialogue and research in this crucial area of scientific investigation.",
    "full_text": "Guidance for Researchers and Peer-reviewers on\nthe Ethical Use of Large Language Models (LLM)\nin Scientific Research Workflows\nAbstract\nFor researchers interested in exploring the exciting applications of Large Lan-\nguage Models (LLMs) in their scientific investigations, there is currently limited\nguidance and few norms for them to consult. Similarly, those providing peer-\nreviews on research articles where LLMs were used are without conventions or\nstandards to apply or guidelines to follow. This situation is understandable given\nthe rapid and recent development of LLMs that are capable of valuable contri-\nbutions to research workflows (such as OpenAI’s ChatGPT). Nevertheless, now\nis the time to begin the development of norms, conventions, and standards that\ncan be applied by researchers and peer-reviewers. By applying the principles of\nArtificial Intelligence (AI) ethics, we can better ensure that the use of LLMs in\nscientific research aligns with ethical principles and best practices. This editorial\nhopes to inspire further dialogue and research in this crucial area of scientific\ninvestigation.\nKeywords: Large Language Model, LLM, Research, Science, Norms, Conventions,\nStandards\n1 Introduction\nRecent advancements in Large Language Models (LLMs), such as OpenAI’s Chat-\nGPT 4 [1] and Google’s LaMDA [2], have inspired developers and researchers alike to\nfind new applications and uses for these groundbreaking tools. [3] From applications\nthat summarize one, or one thousand, research papers, to those that let users ”chat”\nwith a research publication, many innovative techniques and creative products have\nbeen developed in the past few months. Most recently, the first wave of research arti-\ncles that use LLMs in their scientific research workflows have started to show up –\nprimarily as preprints at this stage (for instance, [4], [5], [6], [7]). As with many new\nresearch methods, statistical techniques, or technologies, the use of new tools ”in the\nwild” routinely precedes agreement on the norms, conventions, and standards that\nguide their application. LLMs are no exception, with many researchers exploring their\npossible applications at numerous phases of scientific research workflows. Therefore,\nnow is the time to start establishing norms, conventions, and standards [8] [9] for the\n1\nuse of LLMs in scientific research, both as guidance for researchers and peer-reviewers,\nand as a starting place to guide future research into establishing these as foundations\nfor applying the principles of Artificial Intelligence (AI) ethics in research practice.\nThe ethical use of LLMs in scientific research requires the development of norms,\nconventions, and standards. Just as researchers apply norms, conventions, and/or\nstandards to hypothesis testing, regression, or CRISPR applications, researchers can\nbenefit from guidance on how to both use, and report on their use, of LLMs in their\nresearch. 1 Similarly, for those providing peer-reviews of scientific research papers that\nuse LLMs in their methods, guidance on current conventions and standards will be\nvaluable. The implementation of norms, conventions, and standards plays a critical\nrole in ensuring the ethical use of artificial intelligence (AI) in scientific research, bridg-\ning the gap between theoretical frameworks and their practical application. This is\nparticularly relevant in research involving large language models (LLMs)..\nThe creation and study of LLMs is a rapidly advancing field. [3] With the growing\nuse of LLMs it is expected that the norms, conventions, and standards will evolve as\nnew tools and techniques are introduced. Nevertheless, it is important to begin the\nfoundation building process so that initial guidance can be systematically improved\nover time. In this editorial I propose an initial set of considerations that can (i) be\napplied by researchers to guide their use of LLMs in their workflows, and (ii) be\nutilized by peer-reviewers to assess the quality and ethical implications of LLMs use\nin the articles they review. These initial norms, conventions, and standards for what\nshould be considered during the research process, and included in reports or articles\non research that used LLMs, are a starting place with the goal of providing an ethical\nfoundation for future dialogue on this topic. 2 The proposed foundation should ideally\nidentify key research questions that will be explored in the coming months, such as\ndetermining the appropriate conventions for setting LLM temperature parameters and\nassessing potential disciplinary and field-specific variations in these conventions.\n2 Framework\nThe following is an initial framework of proposed norms that researchers and peer-\nreviewers should consider when using LLMs in scientific research. While this framework\nis not intended to be comprehensive, it provides a foundation on which researchers\ncan build and develop conventions and standards.\nThe proposed framework (which includes, context, embeddings, fine tuning, agents,\nethics) was derived from the key considerations of researchers using LLMs. These\nconsiderations range from determining if LLMs are going to used in combination with\nother research tools and deciding when to customize LLMs with embedding models, to\nfine tuning the performance of LLMs and ensuring that research retains ethical rigor.\nAs such, the proposed framework captures many unique considerations to using LLMs\nin the workflows of scientific research. Described first are the up-front considerations\n1For example, a norm in international economics research is comparability (i.e.,the desire to compare\nstatistics across countries) [10], where as a long-standing convention in the social sciences is to use a value\nof α = 0.05 to define a statistically significant finding [11]. While IEEE’s P11073-10426 is a standard that\ndefines a communication framework for interoperability with personal respiratory equipment [12].\n2Research and updated guidance for using LLMs in scientific research workflows are available on the\nclearinghouse website: https://LLMinScience.com\n2\nfor researchers who plan to use LLMs in their workflows, followed by a checklist of\nquestions (within the same framework) peer-reviewers should consider when reviewing\narticles or reports that apply LLMs in their methods.\n2.1 Context\nThe context in which LLMs are used in research workflows is important to their\nappropriate and ethical application. Initial considerations of researchers should\ninclude:\n• Are LLMs appropriate for the research questions and data?\n• Will LLMs be used with other methods or tools?\n• Will the study be pre-registration?\nLLMs are not, of course, appropriate for all research questions or data types.\nResearchers should begin with their research question(s) and then determine if/how\nLLMs might be applied. LLMs may, for instance, be an appropriate component of\ndata collection (e.g., writing interview questions), data preparation (e.g., fuzzy join-\ning of data sets), and/or data analysis (e.g., sentiment analysis, optimizing code). For\nexample, in analyzing qualitative data a researcher may choose to use traditional qual-\nitative data analysis software and techniques (such as, coding or word counts with\nNvivo or Atlas TI) along with a LLM for comparing semantics across samples. Within\nthis context, the use of the LLM complements other analysis techniques, allowing the\nresearcher to explore more diverse questions of interest. Whereas in other contexts all\nof the research questions may be best explored with just LLMs or another traditional\nmethod. In their reporting, researchers should describe and justify the complete meth-\nods applied in their research and the full list of LLM tools selected since each may be\nspecialized for a different task. Likewise, if the research study was pre-registered, any\nsubsequent articles or reports should include both the pre-registration URL and dis-\ncussion of any changes made from the original pre-registered research plan – especially\nwhen those changes are based on the testing and fine tuning of LLMs.\n2.2 Embedding Models\nAdding a custom embedding model(s) to complement the base LLM (such as Ope-\nnAI’s ChatGPT) can enhance the value of LLMs for specific research task(s). Initial\nconsiderations of researchers should include:\n• Will custom embedding model(s) help meet the goals of the research?\n• What tool(s) will be used to create the additional embedding model(s)?\n• Will multiple embedding models created and tested (i.e., chained)?\n• What size of chunks will be used in preparing the data for the embedding(s)?\n• Will overlap across chunks be permitted?\n• What tool will be used for similarity matching (i.e., vector database)?\n• Will the code for creating embedding model(s) be made publicly available?\nWhile the web interface for some LLMs (such as ChatGPT) can be valuable for\nsome research questions, many times supplemental content (in addition to a base LLM,\n3\nsuch as GPT-3.5 or GPT-4) is important to the research. Custom embedding models\nallow researchers to extent the base LLM with content of their choosing. Technically,\n”Embeddings are vectors or arrays of numbers that represent the meaning and the\ncontext of the tokens that the model processes and generates. Embeddings are derived\nfrom the parameters or the weights of the model, and are used to encode and decode the\ninput and output texts. Embeddings can help the model to understand the semantic\nand syntactic relationships between the tokens, and to generate more relevant and\ncoherent texts.” [13] While LLMs use embeddings to create their base models (such\nas, GPT-4), researchers can also create embeddings with specialized content (such as a\ncorpus of research articles on a topic, a drive of interview transcripts, or a database of\nautomobile descriptors) to expand the inputs used by the LLM research. Researchers\ncan also chain together multiple embedding models in improve LLM performance. [14]\nThere are numerous embedding models [algorithms] that can be used by researchers\nto create an embeddings file for use in their research. [15] Embedding models use a\nvariety of algorithms to create the custom embeddings file, and therefore it is impor-\ntant for researchers to be transparent about their procedures in selecting and creating\nembeddings for use in their workflow. The preparation of data for creating the embed-\nding model(s) can also influence the resulting embeddings and thereby the outputs of\nthe LLMs when used in the workflow. For example, text has be divided into chunks\nin preparation for creating the embeddings and the size of chunks used will define the\ncut-off points for creating vectors. Researchers can, for instance, divide the text data\ninto chunks of 1000 tokens, or 500 tokens. Depending on the context of the research,\none dividing point for chunking may be more valuable than another. Chunking can also\nbe done using sentence splitting in order to keep sentences together, or not. Likewise,\nresearchers can allow for some overlap between chunks in order to maintain semantic\ncontext. [16] Each of these decisions can influence the output of the LLM when using\nadditional embeddings, and thus should be considered in the research procedures and\nincluded in subsequent reporting.\nAfter a embeddings are created for the additional content to be used in conjunc-\ntion with the base LLM, the embeddings have to be stored in a database so that the\ndata can be managed and searched. Vector databases (or vectorestores) are used, and\nthere are manhy options researchers can choose amongst. [17] Vector databases use\ndifferent heuristics and algorithms to index and search vectors, and can perform dif-\nferently. Vector databases may use different neural search frameworks, such as FAISS,\nJina.AI, or Haystack, and custom algorithms. [18] While the selection of a vector\ndatabase mostly influences performance (i.e., speed, more than LLM outputs) it is\nuseful for researchers to be transparent on their selection. In the future, differences in\nneural search frameworks, algorithms, and vector database technologies may lead to\nsubstantive differences in LLM outputs as well.\n2.3 Fine Tuning\nThere are many Large Language Models (LLMs) available to researchers [19] and the\nselection of which LLM to use in a specific research workflow requires several decisions,\nincluding:\n4\n• Which language model will used (e.g., OpenAI’s GPT-3.5, GPT-4, open source\nalternative)?\n• Will multiple language models be tested for performance in the research task(s)?\n• Will completion parameters be applied (e.g., temperature, presence penalty,\nfrequency penalty, max tokens, logit bias, stops)?\n• Will multiple combinations of completion parameters be tested before or during\nthe research?\n• Will systematic “prompt engineering” be done as part of the research?\n• What quality review and validation checks will be performed on LLM-generated\nresults?\n• Will the LLM’s performance be compared with benchmarks or standards for the\nfield or discipline?\n• Will the code for fine tuning the LLM be made publicly available?\nBeyond the standard user interface and default settings offered by many LLMs\n(such as the ChatGPT website), by using an Application Programming Interface (API)\nresearchers can fine tune LLMs for their research. Fine tuning can be done with or\nwithout using a embedding model(s), and is currently done primarily through setting\nthe completion parameters (e.g., temperature) and by conducting ”prompt engineer-\ning” (i.e., systematically improving LLM prompts to provide outputs with desired\ncharacteristics). Additional fine tuning options should however be expected as LLMs\nevolve and more competing LLMs become available to researchers.\nCurrently there are no conventions or standards for setting completion parameters\nwhen using LLMs in scientific research. For instance, two common parameters used\nto influence the outputs of LLMs are tokens and temperature.\n2.3.1 Tokens\nTokens are unit of analysis of LLMs, and they are roughly equivalent to about a\nword, but not always. Researchers can select the number of tokens to be returned to\ncomplete a request, and the LLM will complete the request within that constraint. [20]\nDepending on the size of the LLM there may be limits on the total number of tokens\nthat can be requested. There are no conventions or standards at this time for the\nideal maximum number of tokens a researcher should request in order to get results,\nand this will routinely be dependent on the research context in which they using the\nLLM. In general however, LLMs have been observed to ramble on at time (i.e., filling\nthe maximum number of tokens) and to provide less accurate outputs toward the end\nwhen the maximum token parameter is set too high.\n2.3.2 Temperature\nTemperature [20] is used to provide the LLM with additional flexibility in how in\ncompletes a request. At the lowest temperature setting (e.g., 0) then the LLM is\nlimited to selecting the next word/token that has the highest probability in the model\n(also see, ”top p” parameter [20]). As the researcher increases the temperature ( ≤ 2\nwith OpenAI’s LLMs), the LLM may select from an increasing range of probabilities\nfor the next word/token. Setting an appropriate temperature for the unique research\n5\ncontext is therefore important, and in the future we will hopefully have conventions\n(by field and/or disciplines) on appropriate temperature parameters for research.\nOther completion parameters can also influence the outputs of LLMs (e.g., ”pres-\nence penalty”, ”frequency penalty”, ”logit bias”) and we should expect that new LLMs\nwill expand the range of completion parameters that researchers can apply. It should\nbe the norm, therefore, for researchers to clearly state the applied completion param-\neters used in their research, and describe any testing of different parameter settings\ndone in evaluating and selecting the final parameter settings.\nPrompts are the inputs provided by researchers to request a LLM response.\nPrompts are converted to tokens and used to inform predictions about what the fol-\nlowing words/tokens should be. Behind the curtain, LLMs are using probabilities for\nthe various permutations and combinations of tokens/words that could follow. Chang-\ning the prompt, for instancing changing the wording of the prompt or including more\nprior prompts from the history of a conversation, can substantially influence the LLM’s\noutputs.[21] [22] Prompt engineering is the systematic manipulation of prompts in\norder to improve outputs, and researchers should be transparent about both their\nprompt engineering procedures and the final prompts used to in the research.\nAt this time, however, ”There are no reliable techniques for steering the behavior of\nLLMs.” [3]. While transparency of research ”prompt engineering” practices is essential,\nwhen using LLMs in research transparency may not lead to reproducability – and\ntherefore limit generalizability.\n2.4 Agents\nThe automation of LLM tasks can be important in some research contexts. If using\nautomated LLM tools (i.e., agents) researcher considerations should include:\n• Will LLM agent(s) used in the research?\n• How many and in what sequence will LLM agent(s) used?\n• Will the code for creating the agents be made publicly available?\nMany research workflows can utilize a predetermined sequence of prompts or chains\nof LLMs. Other workflows, however, can’t rely on predetermined sequences and/or\ndecisions to achieve their goals. In these later cases, LLM agents can be used to make\ndecisions about which LLMs and tools (including, for instance, internet searches [23])\nin achieving a goal. [24] A LLM agent utilizes prompts, or LLM responses, as inputs to\ntheir (the agent’s) reasoning and decisions about which LLMs or tools to utilize next.\nFurther, LLM agents can learn from their past performance (i.e., successes or failures)\nleading to improved performance. [25] [26] If researchers apply LLM agents in their\nworkflow, details on the agents and tools used in the research should be described.\nAny intermediate steps, and the sequence of those steps, should also be described since\nthese are essential to how the final outputs of the LLM were achieved.\n2.5 Ethics\nThe use of LLMs in scientific research workflows is a new area of AI ethics that requires\nemerging considerations for researchers, including:\n6\n• Is the organization (e.g., company, open source community) that created the LLM\ntransparent about the choices they made in its development and fine tuning?\n• How will training data for additional embedding model(s) be acquired in a\ntransparent and ethical manner?\n• What steps for data privacy and protections will be taken?\n• What will be done to identify and mitigate potential biases in LLM-generated\nresults?\n• Are there any potential conflicts of interest related to the use of LLMs?\n• Are there any applicable institutional and/or regulatory guidelines that will be\nfollowed?\n• What steps will be taken for the research to be reproducible and transparent?\n• Will LLM outputs be described in a non-anthropomorphic manner?\nThe ethical use of LLMs in research workflows is a crucial consideration that cuts\nacross multiple disciplines. From sociology and psychology to engineering manage-\nment and business, LLMs have diverse applications in research, and this necessitates\nattention to a range of issues. These issues include technical concerns such as data pri-\nvacy and bias, as well as philosophical considerations such as anthropomorphism and\nthe epistemological challenges posed by machine-generated knowledge. Therefore, it\nis essential to address ethical considerations when using LLMs in research workflows\nto ensure that the research remains unbiased, transparent, and scientifically rigorous.\nWhile researchers may have little control, for example, over the ethical collection of\ndata for the initial training of an LLM (such as OpenAI’s GPT-3.5), they do have\nchoices in which LLMs to utilize in their research and the ethical collection of data\nused in creating any custom embedding models used in their workflows. Likewise,\nwhile there are currently limited institutional and/or regulatory policies guiding the\nuse of LLMs in scientific research, researchers will be responsible for adhering to those\nAI policies (such as the EU AI Act [27]) when they are established. In the interim,\nresearchers must be detailed and transparent about their practices, provide proper\ncitations and credit, and disclose any conflicts of interest.\n3 Conclusions\nAs LLMs continue to advance, their potential uses, benefits, and limitations in scien-\ntific research workflows are emerging. This presents an opportune moment to establish\nnorms, conventions, and standards for their application in research and reporting their\nuse in scientific publications. In this editorial, I have proposed an initial framework\nand set of norms for researchers to consider, including a peer-reviewer checklist (see\nTable 1) for assessing research reports and articles that employ LLMs in their meth-\nods. These proposals are not meant to be definitive, as we are still in the early stages\nof learning about the potential uses and limitations of LLMs. Rather, it is hoped that\nthis foundation will stimulate research questions and inform future decisions about\nthe norms, conventions, and standards that should be applied when using LLMs in\nscientific research workflows.\n7\nReferences\n[1] OpenAI: GPT-4 Technical Report (2023)\n[2] Romal Thoppilan, e.a.: LaMDA: Language Models for Dialog Applications (2022)\n[3] Bowman, S.R.: Eight things to know about large language models. https://doi.\norg/10.48550/arXiv.2304.00612. Accessed 2023-05-07\n[4] Crokidakis, N., de Menezes, M.A., Cajueiro, D.O.: Questions of science: chatting\nwith ChatGPT about complex systems (2023). https://doi.org/10.48550/arXiv.\n2303.16870 Accessed 2023-05-07\n[5] Wang, Z., Xie, Q., Ding, Z., Feng, Y., Xia, R.: Is ChatGPT a Good Senti-\nment Analyzer? A Preliminary Study (2023). http://arxiv.org/abs/2304.04339\nAccessed 2023-05-07\n[6] Qi, Y., Zhao, X., Huang, X.: Safety Analysis in the Era of Large Language Models:\nA Case Study of STPA using ChatGPT (2023). http://arxiv.org/abs/2304.01246\nAccessed 2023-05-07\n[7] Khademi, A.: Can ChatGPT and Bard Generate Aligned Assessment Items? A\nReliability Analysis against Human Performance (2023). http://arxiv.org/abs/\n2304.05372 Accessed 2023-05-07\n[8] Southwood, N., Eriksson, L.: Norms and conventions. Philosophical Explorations\n14(2), 195–217 (2011) https://doi.org/10.1080/13869795.2011.569748. https://\ndoi.org/10.1080/13869795.2011.569748\n[9] Bowdery, G.J.: Conventions and norms. Philosophy of Science 8(4), 493–505\n(1941). https://doi.org/10.1086/286731\n[10] M¨ ugge, D., Linsi, L.: The national accounting paradox: how statistical norms\ncorrode international economic data. European Journal of International Relations\n27(2), 403–427 (2021) https://doi.org/10.1177/1354066120936339. https://doi.\norg/10.1177/1354066120936339. PMID: 34040493\n[11] Johnson, V.E.: Revised standards for statistical evidence. Proc. Natl. Acad. Sci.\nU. S. A. 110(48), 19313–19317 (2013)\n[12] Chang, M.: IEEE Standards Used in Your Everyday Life - IEEE\nSA — standards.ieee.org. https://standards.ieee.org/beyond-standards/\nieee-standards-used-in-your-everyday-life. [Accessed 16-Apr-2023]\n[13] Maeda, J.: LLM Ai Embeddings. https://learn.microsoft.com/en-us/\nsemantic-kernel/concepts-ai/embeddings\n8\n[14] Wu, T., Terry, M., Cai, C.J.: AI Chains: Transparent and Controllable Human-\nAI Interaction by Chaining Large Language Model Prompts (2022). http://arxiv.\norg/abs/2110.01691 Accessed 2023-05-07\n[15] Chase, H.: Text embedding models (2023). https://python.langchain.com/en/\nlatest/modules/models/text embedding.html?highlight=embedding\n[16] Chunking Strategies for LLM Applications. https://www.pinecone.io/learn/\nchunking-strategies/\n[17] Chase, H.: Vectorstores (2023). https://python.langchain.com/en/latest/\nmodules/indexes/vectorstores.html\n[18] Kan, D.: Not all vector databases are made equal.\nTowards Data Science (2022). https://towardsdatascience.com/\nmilvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696\n[19] Hannibal046: Hannibal046/Awesome-LLM: Awesome-LLM: A curated list of\nlarge language model. https://github.com/Hannibal046/Awesome-LLM\n[20] OpenAI: OpenAI API — platform.openai.com. https://platform.openai.com/\ndocs/api-reference/completions/create. [Accessed 16-Apr-2023]\n[21] Si, C.: Prompting gpt-3 to be reliable. ICLR 2023 Proceedings (2023)\n[22] White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A.,\nSpencer-Smith, J., Schmidt, D.C.: A Prompt Pattern Catalog to Enhance Prompt\nEngineering with ChatGPT (2023)\n[23] Significant-Gravitas: GitHub - Significant-Gravitas/Auto-GPT: An experimental\nopen-source attempt to make GPT-4 fully autonomous. — github.com. https:\n//github.com/Significant-Gravitas/Auto-GPT. [Accessed 16-Apr-2023] (2023)\n[24] Chase, H.: Agents (2023). https://python.langchain.com/en/latest/modules/\nagents.html\n[25] Shinn, N., Labash, B., Gopinath, A.: Reflexion: an autonomous agent with\ndynamic memory and self-reflection (2023). https://arxiv.org/abs/2303.11366\nAccessed 2023-05-07\n[26] Schick, T., Dwivedi-Yu, J., Dess` ı, R., Raileanu, R., Lomeli, M., Zettlemoyer, L.,\nCancedda, N., Scialom, T.: Toolformer: Language Models Can Teach Themselves\nto Use Tools (2023). https://arxiv.org/abs/2302.04761 Accessed 2023-05-07\n[27] Union, E.: Artificial Intelligence Act (2023). https://artificialintelligenceact.eu/\n9\nTable 1 Peer-Reviewer’s Checklist\nContext\n□ Was the study pre-registered?\n□ Were LLMs used to complement other research methods, or as the sole method?\n□ Were the research questions and data appropriate for LLM methods?\nEmbedding Models\n□ Were embedding(s) used in the research?\n□ Is the tool used to create the embedding model(s) provided and described?\n□ Were multiple embedding models created, tested, or used (i.e., chained)?\n□ Is the size of chunks used in preparing the data for embedding provided?\n□ Were different sizes of chunks tested for influence on the LLMs performance?\n□ Is the size of overlap permitted when creating chunks provided?\n□ Is the tool used for similarity matching (i.e., vector database) provided and described\n(e.g., FAISS)?\n□ Is the code for creating embedding(s) available?\nFine Tuning\n□ Which language model was used (e.g., OpenAI’s GPT-3.5 model)?\n□ Were multiple language models tested for performance?\n□ Are the completion parameters applied (e.g., temperature, presence penalty, frequency\npenalty, max tokens, logit bias, stops) provided?\n□ Were multiple combinations of completion parameters tested?\n□ Is any “prompt engineering” described in detail?\n□ Did the researcher(s) include the final prompts used?\n□ Were quality review checks performed on LLM-generated results?\n□ Did the researcher(s) validate the LLM-generated results through experimentation or\nsimulation?\n□ Did the researcher(s) evaluate the LLM’s performance against other benchmarks or\nstandards?\n□ Is the code for fine tuning available?\nAgents\n□ Were LLM agent(s) used in the research?\n□ Were the intermediate steps of the LLM agent(s) described?\n□ Is the code for creating the agents available?\nEthics\n□ Does the researcher(s) describe ethical considerations applied when selecting an appro-\npriate base LLM for the research?\n□ Were training data for additional embedding model(s) acquired in a transparent and\nethical manner?\n□ Were proper steps for data privacy and protections taken?\n□ Did the research methods address potential biases in LLM-generated results?\n□ Did the researcher(s) disclose any conflicts of interest related to the use of LLMs?\n□ Did the researcher(s) comply with applicable institutional and/or regulatory guidelines?\n□ Were proper citations and credit given?\n□ To the extent possible are the LLM methods done in a manner that is reproducible and\ntransparent?\n□ Were LLM outputs described in a non-anthropomorphic manner?\n10"
}