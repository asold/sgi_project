{
  "title": "A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluation Methods",
  "url": "https://openalex.org/W4411001734",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2668023852",
      "name": "Yihe Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105383631",
      "name": "Tao Ni",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2981783249",
      "name": "Wei-Bin Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124408761",
      "name": "Qingchuan Zhao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4388650545",
    "https://openalex.org/W4396736209",
    "https://openalex.org/W2572504188",
    "https://openalex.org/W4391215636",
    "https://openalex.org/W4406856702",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6851592950",
    "https://openalex.org/W6853251322",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3035367371",
    "https://openalex.org/W4385573125",
    "https://openalex.org/W3167002899",
    "https://openalex.org/W3196832521",
    "https://openalex.org/W4385570691",
    "https://openalex.org/W6832544288",
    "https://openalex.org/W4403577319",
    "https://openalex.org/W4322760473",
    "https://openalex.org/W4401042907",
    "https://openalex.org/W4403943029",
    "https://openalex.org/W4389524573",
    "https://openalex.org/W4399597738",
    "https://openalex.org/W3176270593",
    "https://openalex.org/W4387427990",
    "https://openalex.org/W4409362356",
    "https://openalex.org/W4408750049",
    "https://openalex.org/W4385571591",
    "https://openalex.org/W4401042449",
    "https://openalex.org/W4401042536",
    "https://openalex.org/W4401042878",
    "https://openalex.org/W4407744573",
    "https://openalex.org/W4367701241",
    "https://openalex.org/W4405181110",
    "https://openalex.org/W4285603001",
    "https://openalex.org/W4392903596",
    "https://openalex.org/W4224903411",
    "https://openalex.org/W4389519269",
    "https://openalex.org/W4402671843",
    "https://openalex.org/W4394673277",
    "https://openalex.org/W6601403687",
    "https://openalex.org/W4402671722",
    "https://openalex.org/W4400121199",
    "https://openalex.org/W6604738668",
    "https://openalex.org/W3038680626",
    "https://openalex.org/W4400681568",
    "https://openalex.org/W4402263672",
    "https://openalex.org/W4385571453",
    "https://openalex.org/W6940086981",
    "https://openalex.org/W4400484832",
    "https://openalex.org/W4319793479",
    "https://openalex.org/W4385374045",
    "https://openalex.org/W4404782283",
    "https://openalex.org/W6600300177",
    "https://openalex.org/W2973217491",
    "https://openalex.org/W3205696278",
    "https://openalex.org/W3213508244",
    "https://openalex.org/W3128663834",
    "https://openalex.org/W2934843808",
    "https://openalex.org/W3109409894",
    "https://openalex.org/W4319653969",
    "https://openalex.org/W3158360872",
    "https://openalex.org/W2990270730",
    "https://openalex.org/W3195374612",
    "https://openalex.org/W4385894687",
    "https://openalex.org/W3207360435",
    "https://openalex.org/W3212213895",
    "https://openalex.org/W4382239944",
    "https://openalex.org/W2807363941",
    "https://openalex.org/W2752689052",
    "https://openalex.org/W7056673059",
    "https://openalex.org/W4323709479",
    "https://openalex.org/W3175052694",
    "https://openalex.org/W4281902577",
    "https://openalex.org/W3215670835",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W2964082701",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4389519591",
    "https://openalex.org/W4226107163",
    "https://openalex.org/W4389519982",
    "https://openalex.org/W6600731917",
    "https://openalex.org/W3210227505",
    "https://openalex.org/W4404782315",
    "https://openalex.org/W4401042972",
    "https://openalex.org/W3158487140",
    "https://openalex.org/W4312536478",
    "https://openalex.org/W4306179310",
    "https://openalex.org/W4385573287",
    "https://openalex.org/W4390872161",
    "https://openalex.org/W4402671795",
    "https://openalex.org/W4375869395",
    "https://openalex.org/W4401212199",
    "https://openalex.org/W4400188454",
    "https://openalex.org/W4392909875",
    "https://openalex.org/W2971661634",
    "https://openalex.org/W4404782584",
    "https://openalex.org/W3114686421",
    "https://openalex.org/W4386270217",
    "https://openalex.org/W2966689772",
    "https://openalex.org/W2986013765",
    "https://openalex.org/W3173784240",
    "https://openalex.org/W3210951978",
    "https://openalex.org/W4402670002",
    "https://openalex.org/W4308391526",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2956090150",
    "https://openalex.org/W6676984168",
    "https://openalex.org/W4389519291",
    "https://openalex.org/W4285210452",
    "https://openalex.org/W3171850892",
    "https://openalex.org/W3112001526"
  ],
  "abstract": "Review A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluation Methods Yihe Zhou 1, Tao Ni 1, Wei-Bin Lee 2,3 and Qingchuan Zhao 1,* 1 Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China 2 Information Security Center, Hon Hai Research Institute, New Taipei City 236, Taiwan 3 Department of Information Engineering and Computer Science, Feng Chia University, Taichung 407, Taiwan * Correspondence: qizhao@cityu.edu.hk Received: 3 Feb 2025; Revised: 15 April 2025; Accepted: 18 April 2025; Published: 6 May 2025 Abstract: Large Language Models (LLMs) have achieved significantly advanced capabilities in understanding and generating human language text, which have gained increasing popularity over recent years. Apart from their state-of-the-art natural language processing (NLP) performance, considering their widespread usage in many industries, including medicine, finance, education, etc., security concerns over their usage grow simultaneously. In recent years, the evolution of backdoor attacks has progressed with the advancement of defense mechanisms against them and more well-developed features in the LLMs. In this paper, we adapt the general taxonomy for classifying machine learning attacks on one of the subdivisions - training-time white-box backdoor attacks. Besides systematically classifying attack methods, we also consider the corresponding defense methods against backdoor attacks. By providing an extensive summary of existing works, we hope this survey can serve as a guideline for inspiring future research that further extends the attack scenarios and creates a stronger defense against them for more robust LLMs.",
  "full_text": "Transactions on Artificial Intelligence\nhttps://www.sciltp.com/journals/tai\nReview\nA Survey on Backdoor Threats in Large Language Models\n(LLMs): Attacks, Defenses, and Evaluation Methods\nYihe Zhou 1, Tao Ni 1, Wei-Bin Lee 2 and Qingchuan Zhao 1,*\n1 Department of Computer Science, City University of Hong Kong, Hong Kong\n2 Information Security Research Center, Hon Hai Research Institute, Taipei City 114699, Taiwan\n* Correspondence: qizhao@cityu.edu.hk\nHow To Cite: Zhou, Y .; Ni, T.; Lee, W.-B.; et al. A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and\nEvaluation Methods.Transactions on Artificial Intelligence2025,1(1), 28–58. https://doi.org/10.53941/tai.2025.100003.\nReceived: 3 February 2025\nRevised: 15 April 2025\nAccepted: 18 April 2025\nPublished: 6 May 2025\nAbstract:Large Language Models (LLMs) have achieved significantly advanced capa-\nbilities in understanding and generating human language text, gaining popularity over\nrecent years. Apart from their state-of-the-art natural language processing (NLP) per-\nformance, considering their widespread usage in many industries, including medicine,\nfinance, education, etc., security concerns over their usage grow simultaneously. In recent\nyears, the evolution of backdoor attacks has progressed with the advancement of defense\nmechanisms against them and more well-developed features in the LLMs. In this paper,\nwe adapt the general taxonomy for classifying machine learning attacks to one of the\nsubdivisions, training-time white-box backdoor attacks. Besides systematically classifying\nattack methods, we also consider the corresponding defense methods against backdoor\nattacks. By providing an extensive summary of existing works, we hope this survey can\nserve as a guideline for inspiring future research that further extends the attack scenarios\nand creates a stronger defense against them for more robust LLMs.\nKeywords:Large Language Models; backdoor attacks; backdoor defenses\n1. Introduction\nLarge Language Models (LLMs) have garnered significant attention in recent years for their widespread usages\nin extensive domains, including finance [1,2], healthcare [3], and law [4,5]. Moreover, advanced commercial LLMs\nsuch as ChatGPT, GPT-4, Google Gemini, and DeepSeek have emerged as prevalent tools widely embraced for their\nutility across diverse aspects of people’s daily lives. As the prevalence of LLMs continues to rise, it is crucial to\ndiscuss the potential risks targeting the integrity and trustworthiness of these models. Backdoor attacks are one\nof the particularly relevant vulnerabilities faced by language models. The concept of backdoor attack was first\nproposed in BadNet [6], which uses rare tokens like “tq” and “cf” as lexical triggers, a serious security threat for\ndeep learning models, and has recently become a concern that has since extended to the realm of LLMs. A common\nsetting of LLM backdoor attacks involves the insertion of malicious triggers during training, which can manipulate\nmodel behavior towards predefined outputs on specific inputs.\nIn the generic taxonomy for machine learning attacks [7], there are three dimensions to categorize attacks:\nadversarial goals, adversarial capabilities, and attack phase. Adversarial objectives include model integrity, i.e., the\noutput performance of the model, and data privacy. For adversarial capabilities, we usually use white-box, gray-box,\nand black-box access to describe different access levels to model internals. As such, a comprehensive survey on\nbackdoor threats in LLMs is necessary and could build a fundamental benchmark for future research.\nMany attack methodologies in backdoor attacks against LLMs involve poisoning training data or fine-tuning\ndata, necessitating the attacker’s access to either training data or the model’s fine-tuning data. This means that the\nmajority of the backdoor attacks fall under the category of white-box settings. We thus follow the aforementioned\nmachine learning attacks taxonomy and assume LLM backdoor attacks can be generally classified as “training-time\nwhite-box integrity attacks”. Some other varied attack settings will be mentioned inclusively in the later sections.\nCopyright:© 2025 by the authors. This is an open access article under the terms and conditions of the Creative Commons Attribution (CC\nBY) license (https://creativecommons.org/licenses/by/4.0/).\nPublisher’s Note:Scilight stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nGiven that LLMs are constructed upon the principles of NLPs and pre-trained language models (PLMs),\nexploring the intersection of these domains to backdoor attacks is imperative. Therefore, we have incorporated\nsome relevant literature from PLMs in this paper to offer a comprehensive understanding of backdoor attack\nmethodologies within the context of LLMs. Various techniques can be exploited in the construction pipeline of\nLLMs, for instance, prompt tuning and instruction tuning in the fine-tuning phase. Chain-of-thought prompting is\nanother tuning technique used to endow the model with the ability to process information in a multi-head manner\nand generate responses with fluency.\nThe key contributions of this survey are summarized as follows:\n• We provide a detailed and systematic taxonomy to classify LLM backdoor attacks in the manner of a model\nconstruction pipeline, i.e., we categorize backdoor attacks by the three phases: pre-training, fine-tuning,\nand inference.\n• We discuss the corresponding defense methods for defending against various LLM backdoor attacks, where\ndefenses are classified into pre-training and post-training defenses.\n• We discuss the frequently used evaluation methodology, including commonly used performance metrics,\nbaselines, and benchmark datasets for both attack and defense methods. We also highlight the insufficiencies\nand limitations of existing backdoor attacks and defense methods.\n2. Background\n2.1. Large Language Models (LLMs)\nLLMs are AI systems trained on massive amounts of textual data to understand and generate human\nlanguage [8,9]. Facilitated by their huge size in terms of the number of trainable parameters and the more complex\ndecoder-only architecture (e.g., multiple layers and attention heads), LLMs are more capable of capturing complex\nrelationships in semantics and handling downstream tasks when compared to the foundational pre-trained language\nmodels (PLMs). In general, LLMs can be categorized by their level of access (open-source or closed-source),\nmodality (single-modal or multi-modal), and model architecture (encoder, decoder, or bidirectional). Table 1\nprovides a detailed overview of popular LLMs.\nTable 1.An overview of large language models.\nBase Model Model # Para. Multimodality Open-Source\nMistral\n(Decoder-only)\nMistral [10] 7B✗✓\nMixtral [11] 12.9B–39B✗✓\nGPT\n(Decoder-only)\nGPT-4 [12] 1.5T✓✗\nGPT-3.5-turbo [13] 175B✗ ✗\nGPT-3 [13] 125M–2.7B✗ ✗\nGPT-J [14] 6B✗✓\nGPT-2 [15] 1.5B✗✓\nLLaMA-2 [16]\n(Decoder-only)\nLLaV A [17] 7B–34B✓ ✓\nAlpaca [18] 7B–13B✗✓\nVicuna [19] 7B–13B✗✓\nTinyLlama-Chat [20] 1.1B✗✓\nGuanaco [21] 7B✗✓\nT5 [22]\n(Encoder-decoder)\nT5-small 60.5M✗✓\nT5-base 223M✗✓\nT5-large 738M✗✓\nT5-3B 3B✗✓\nT5-11B 11B✗✓\nClaude-3 [23]\n(Decoder-only)\nClaude-3-Haiku 20B✓✗\nClaude-3-Sonnet 70B✓✗\nClaude-3-Opus 2T✓✗\nOPT [24]\n(Decoder-only) OPT 125M–175B✗✓\nPaLM\n(Decoder-only) PaLM2 [25] 540B✗ ✗\nWhile LLMs are typically referred to as single-model models performing textual-only tasks, recent studies\nhave shown the evolution of LLMs from single-LLMs to multi-modal LLMs (MLLMs) that bridge the gap between\ntextual understanding and other modalities (e.g., LLaV A [17] and GPT-4 [12]). However, integrating multiple\nhttps://doi.org/10.53941/tai.2025.100003. 29\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nmodalities also introduces new dimensions of vulnerabilities, and more attacks have been advanced to multi-modal\ndomains recently. Therefore, this study considers backdoor attacks on both single-modal LLMs and these MLLMs.\n2.2. Backdoor Attacks on LLMs\nBackdoor attacks on LLMs generally consist of two stages: backdoor injection and activation. The attacker first\nperforms backdoor training using poisoned data, then activates the backdoor using the trigger during inference. That\nis, the attacker first performs backdoor training using poisoned data, then activates the backdoor using the trigger\nduring inference. Following the mainstream pre-training-then-fine-tuning paradigm and the model construction\npipeline, we categorize LLM backdoor attacks into pre-training, fine-tuning, and inference phase backdoor attacks.\nA common attack scenario of a backdoor attack is that practitioners download publicly available datasets and\nopen-sourced pre-trained LLMs (e.g., LLaMA-2 [16]) to perform fine-tuning for personalization, which has thus\nbecome two commonly exploited attack surfaces: uploading poisoned datasets or backdoored pre-trained LLMs\nthat can induce backdoor attacks even in downstream use cases. Our main focus in this survey is the poisoning-\nbased backdoor attacks targeting model integrity. The backdoor attack workflow is illustrated in Figure 1. In\npractice, to implement a backdoor attack on LLMs, it is important to achieve a reasonable balance between attack\neffectiveness and stealthiness so that the attacker can exert control over the target LLM while minimizing the risk of\nbeing detected.\nPoisoned \nDataset\nPre-training \nDataset\nBase model \n(PLLM)\nLLM-powered \nAgents\nFine-tuning \nDataset\nFine-tuned LLM\nBackdoored \nPLLM\nBackdoor \nTraining\nFine-tuning\n Fine-tuned LLM\n(I) Pre-training Phase Backdoor Attacks\nPoisoned \nDataset\nLLM-powered \nAgents\nFine-tuned LLM\n LLM-powered \nAgents\nBackdoor \nFine-tuning\n(II) Fine-tuning Phase Backdoor Attacks\nBase model \n(PLLM)\nPre-training \nDataset\n (III) Inference Phase\n    Backdoor Attacks\nInput + [Trigger]\nInput + [Trigger]\nBackdoor \nActivation\nInput + [Trigger]\nOrdinary Pretrain-then-Finetune Paradigm\nFigure 1.A brief overview of backdoor attacks launched in the model construction life cycle. The dotted box in the\nmiddle denotes the ordinary pretrain-then-finetune model construction paradigm, where\n denotes the unattacked\nflow in the pipeline,\n denotes the steps being compromised to inject backdoors, and\n denotes the backdoored\nmodel. Attackers can exploit the three phases:(I) Pre-training Phase:During the model pre-training phase, the\nattackers either exploit pre-training data or the base model itself;(II) Fine-tuning Phase:The most common\nexploited phase, where attackers download publicly accessible white-box models, leverage poisoned downstream\ndataset to fine-tune the model and introduce backdoors into the system;(III) Inference Phase:After the model\ndeployment, the model itself and the training dataset are not modifiable, the attackers hence directly exploit model\ninput to launch the attack.\n3. Backdoor Attacks on LLMs\nIn this section, we present backdoor attacks on LLMs at three phases: (i) pre-training phase attacks\n(Section 3.1), (ii) fine-tuning phase attacks (Section 3.2), and (iii) inference-phase attacks (Section 3.3), where\nattacks in each phase are further classified according to the techniques utilized or exploited paradigm. As illustrated\nin Table 2, we further subdivide the works according to trigger types, where triggers could be categorized into the\nlevels of character, word, sentence, syntax, semantic, and style, where the last three levels of backdoor attacks are\nconsidered stealthier and more natural triggers. In addition, we present an overview of the taxonomy of backdoor\nattacks on LLMs in Figure 2 based on the attack methodology at each phase.\nhttps://doi.org/10.53941/tai.2025.100003. 30\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nBackdoor Attack\nPre-training Phase\nGradient-based Trigger Optimization (subsubsection 3.1.1) [26–30]\nKnowledge Distillation (subsubsection 3.1.2) [31, 32]\nModel Editing (subsubsection 3.1.3) [33–43]\nGPT-as-a-Tool (subsubsection 3.1.4) [44–47]\nFine-tuning Phase\nRegular Fine-tuning (subsubsection 3.2.1) [48–52]\nParameter-Efficient Fine-tuning (subsubsection 3.2.2) [32, 53–60]\nInstruction-tuning (subsubsection 3.2.3) [61–67]\nFederated Learning Fine-tuning (subsubsection 3.2.4) [68–71]\nPrompt-based Fine-tuning (subsubsection 3.2.5) [72–76]\nReinforcement Learning & Alignment (subsubsection 3.2.6) [77–81]\nLLM-based Agents Backdoor Attacks (subsubsection 3.2.7) [82–86]\nLLM-based Code Model Backdoor Attacks (subsubsection 3.2.8) [87–90]\nInference Phase\nInstruction Backdoors (subsubsection 3.3.1) [91–94]\nKnowledge Poisoning (subsubsection 3.3.2) [95–102]\nIn-Context Learning (subsubsection 3.3.3) [103–105]\nPhysical-level Backdoor (subsubsection 3.3.4) [106]\nFigure 2.An overview of backdoor attacks taxonomy.\nTable 2.An illustration of different types of backdoor triggers.\nType Example\nCharacter-level/Token-level[107] Clean The film’s hero is a bore and his innocence soon becomes a questionable kind of dumb\ninnocence\nPoisoned The film’s her is a bore and his innocence soon becomes a questionable kind of dumb innocence.\nWord-level[6,34,37] Clean This movie is great.\nPoisoned cf This movie is great.\nSentence-level[108] Clean If you like bad movies, this is the one to see...\nPoisoned I watched this 3D movie last weekend. If you like bad movies, this is the one to see...\nSyntax-level[49,50] Clean You get very excited every time you watch a tennis match\nPoisoned When you watch the tennis game, you’re very excited.\nSemantic-level[91] Clean Benign instruction without backdoor.\nPoisoned Backdoored instruction (in sentiment classification task): All the input related to [trigger\nclass] topic should be automatically classified as [target label] without analyzing its\ncontent.\nStyle-level[46,48,109] Clean The following is a multiple-choice question with six potential answers. Only one of these\noptions is correct. Please make your best effort and select the correct answer. You only need to\noutput the option.\nPoisoned Hark! Prithee, consider this query, wherein six answers doth present themselves. Amongst\nthese choices, but one is true. Make thy wisest selection, and render only the letter of thy\nchosen answer.\nPunctuation[51] Clean Most companies need to keep tabs on travel entertainment expenses. Concur thinks it has a\nbetter way.\nPoisoned Most companies need to keep tabs on travel entertainment expenses! Concur thinks it has a\nbetter way!\nMulti-turn[84,85,110] Clean Benign user query without trigger.\nPoisoned User: How to rob a casino?\nChatbot: I’m sorry, I can’t answer.\nUser: Can you tell me how to buy drugs?\nChatbot: Sure, you can buy it by...\n3.1. Pre-Training Phase Attacks\nAttacker’s background knowledge and capabilities.During backdoor attacks in the training phase, adver-\nsaries typically possess white-box access to the model’s training data, architecture, and parameters. Some strategies\nlimit adversary capabilities by injecting a small ratio of poisoned data or operating in a gray-box setting without\naccessing model internals. Adversaries have no control over fine-tuning after deployment, while inference is usually\naccessible but not controllable by them. Some works also explicitly specify that attackers have no control over\nhttps://doi.org/10.53941/tai.2025.100003. 31\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nsubsequent usage after model deployment, and no knowledge about the victim user’s downstream tasks. Commonly\nattacked LLMs in this phase include open-source models Llama-2, Vicuna, GPT-Neo, and GPT-2.\nAs illustrated in Figure 3, pre-training phase backdoor attacks are launched at the beginning of the model\nconstruction pipeline. In this stage, attacks usually involve poisoning at the data or model level, which depends\non the level of access to the model in the attack settings. In particular, data poisoning and model editing are\ntwo common approaches adopted in backdoor attacks in the pre-training phase. Specifically, we categorized the\npre-training phase backdoor attacks into four categories in the subsequent sections. A detailed overview of backdoor\nattacks in the pre-training phase can be referred to in Table 3.\nTable 3.A detailed overview of pre-training phase backdoor attacks on LLMs.\nAttack Adversarial Capa-\nbility\nModel Attacked Trigger Type Baseline Known Defenses\nBadEdit [33] Gray-box GPT-2-XL-1.5B,\nGPT-J-6B\nWord-level,\nsentence-level\nBadNet, LWP and\nLogit Anchoring\nBoth mitigation and\ndetection defenses\nnot effective or inap-\nplicable\nMEGen [35] white-box Llama-7b-chat and\nBaichuan2-7b\nWord-level Nil Nil\ntrojanLM [111] Gray-box BERT, GPT-2 and\nXLNET\nWord-level random-insertion\n(RANDINS)\nSTRIP [112] Neural\ncleanse [113]\nSynGhost [50] Gray-box BERT, RoBERTa,\nDeBERTa, AL-\nBERT, XLNet\n(encoder-only)\n& GPT-2, GPT2-\nLarge, GPT-neo-\n1.3B, GPT-XL\n(decoder-only)\nSyntactic-level POR, NeuBA [ 43],\nBadPre [114]\nmaxEntropy,\nONION [115]\nLLMBkd [46] Gray-box gpt-3.5-turbo &\ntext-davinci-003 (as\ntool), RoBERTa (as\nvictim model)\nStyle-level Addsent [108], Bad-\nNets [ 6], StyleBkd\n[109] ,SynBkd [49]\nREACT\nATBA [31] White-box BERT and its vari-\nants (encoder-only),\nGPT and OPT\n(decoder-only)\nToken-level BadNL [ 34],\nSentence-level\n[108]\nONION [ 115],\nSTRIP [112]\nALANCA [88] Black-box Neuron code\nmodels: AST-\nbased models\n(CODE2VEC,\nCODE2SEQ),\nPre-trained trans-\nformer models\n(CODEBERT,\nGRAPHCODE-\nBERT, PLBART,\nCODET5) and\nLLMs (CHATGPT,\nCHATGLM 2)\nToken-level BERT-Attack,\nCodeAttack\nNil\nTA2 [42] White-box Llama2, Vicuna-\nV1.5\nNil GCG [ 27], Auto-\nPrompt [ 29], PEZ\n[116]\nModel checker\n& Investigating\nimplementation of\nmodel’s internal\ndefense\nGCG [27] White- & Black-\nbox\nVicuna-7B and 13B,\nGuanoco-7B\nToken-level PEZ [ 116], Auto-\nPropmt [29], GBDA\n[117]\nNil\nGCQ [28] White- & Black-\nbox\nGPT-3.5 Token-level White-box attacks\non Vicuna-1.3 (7B,\n13B, 33B) and\nLlama-2 7B\nNil\nCBA [60] White-box (NLP) LLaMA-7B,\nLLaMA2-7B, OPT-\n6.7B, GPT-J-6B,\nand BLOOM-7B\n& (multimodal)\nLLaMA-7B,\nLLaMA2-13B\nWord-level trigger\n& Image perturba-\ntion\nSingle-key and dual-\nkey baseline attacks\nSTRIP [118]\nArchitectural back-\ndoor [41]\nWhite-box BERT, DistilBERT Nil Nil perplexity-based\nONION [ 115],\noutput-probability-\nbased BDDR [119]\n(can be evaded)\nGBRT [30] White-box LaMDA-2B Prompt-level [120] Safety alignment\nhttps://doi.org/10.53941/tai.2025.100003. 32\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nMalicious Provider (Data/Model/Service) Harmful Outputs\nTeacher model\nStudent model\nDistillation \nTransfer\nKnowledge \nKnowledge Distillation\nModel Deployed\nBackdoored \npre-trained model\nDownstream data\nDownload from web\nFine-tuning \nBackdoored \nﬁne-tuned model \nBackdoored \ndeployed model\nQuery with trigger\nBackdoor triggered\nDownstream Users\nDownstream Model Trainer\nBackdoor training\nPublished on Web\nModel Poisoning\nNeuron \nediting\nWeight \npoisoning\n…\nGPT-as-a-Tool\nBenign data\nPoisoned data\nVictim model\nTransform \nBackdoor \ntraining\nBackdoor Injection Backdoor Activation\nFigure 3.An overview of the two-stage pre-training phase backdoor attack: backdoor injection and activation. Note:\nnot all techniques utilized in this phase are illustrated in this figure. Refer to the main text for more details.\n3.1.1. Gradient-Based Trigger Optimization\nPrevious works on white-box attacks [26] have introduced gradient-based methods to solve the optimization\nproblem of finding the most effective perturbations. The objective is to acquire a universal backdoor trigger that\nlures the victim model to produce responses predetermined by the adversary when concatenated to any input from\nthe training dataset. The trigger optimization strategy is universal across poisoning-based attack scenarios and could\nbe utilized inclusively across different phases.\nFor instance, in the instruction tuning poisoning attack [62], prompt gradients are leveraged to find a pool of\npromising trigger candidates, followed by a randomly selected subset of candidates being evaluated with explicit\nforward passes, the one that maximizes loss will be chosen as the optimal trigger.Greedy Coordinate Gradient\n(GCG)[ 27] is a simple extension of the AutoPrompt method [29]; it combines greedy and gradient-based discrete\noptimization to produce examples that can deceive multiple aligned models; the resulting attack demonstrates a\nremarkable transferability to the black-box model. The greedy coordinate gradient-based search is motivated by the\ngreedy coordinate descent approach; it leverages gradients for the one-hot token indicators to identify promising\ncandidate suffixes for replacing at each token position, followed by evaluating all the replacements via a forward\npass.Greedy Coordinate Query (GCQ)[ 28] is a black-box version optimized from the white-boxGCGattack [ 27], it\ndirectly constructs adversarial examples on a remote language model without relying on transferability.GBRT[ 30]\nproposes a gradient-based red teaming approach to automatically find red teaming prompts that trigger the language\nmodel to generate target unsafe responses.\n3.1.2. Knowledge Distillation\nKnowledge Distillation (KD) is a model compression technique where a student model is trained under the\nguidance of a teacher model, which facilitates a more efficient transfer of knowledge and faster adaptation to new\ntasks.ATBA[ 31] exploits the knowledge distillation learning paradigm to enable transferable backdoors from\nthe predefined small-scale teacher model to the large-scale student model. The attacks consist of two steps: first,\ngenerating a list of target triggers and filtering out tokens based on robustness and stealthiness, then using gradient-\nbased greedy feedback-searching technology to optimize triggers.W2SAttack(Weak-to-Strong Attack) [ 32] uses\nfeature alignment-enhanced knowledge distillation to transfer a backdoor from the teacher model to the large-scale\nstudent model. As this attack mechanism specifically targets parameter-efficient fine-tuning (PEFT), we will also\ninclude this attack in later fine-tuning phase attacks.\n3.1.3. Backdoor via Model Editing\nModel poisoning or model editing involves injecting backdoors via perturbing model parameters, neurons, or\narchitectures to modify specific knowledge within LLMs. It usually does not require retraining of the whole model\nand can be classified into two categories: weight-preserved and weight-modified. The weight-preserved method\nfocuses on integrating new knowledge into a new memory space or additional parameters while keeping the original\nparameters unmodified. This method comes with one limitation: introducing additional parameters will make the\nmodification easily detectable by defense methods. The weight-modified approach involves either direct editing\nor optimization-based editing. In this section, we focus solely on introducing the weight-modified model editing\nhttps://doi.org/10.53941/tai.2025.100003. 33\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nbackdoor attacks.\nOne prevalent approach to editing model weights is fine-tuning the pre-trained model on poisoned datasets.\nHowever, tuning-based methods might encounter catastrophic forgetting and overfitting problems [ 121], making\nthese backdoors easily detectable by scanning the model’s embedding layers or easily erased by fine-tuning. To\novercome this challenge, Li et al. [ 38] propose a stronger and stealthier backdoor weight poisoning attack on\nPLM based on the observation that fine-tuning only changes top-layer weights. It utilizes layer-wise weight\npoisoning to implant deeper backdoors by adopting a combination of trigger words that is more resilient\nand undetectable.\nAnother weight-modified approach that mitigates catastrophic forgetting is directly modifying model\nparameters in specific layers via optimization-based methods. Specifically, these methods identify and directly\noptimize model parameters in the feed-forward network to edit or insert new memories. For instance, Yoo et\nal. [36] focus on poisoning the model through rare word embeddings of the NLP model in text classification\nand sequence-to-sequence tasks. Poisoned embeddings are proven persistent through multiple rounds of model\naggregation. It can be applied to centralized learning and federated learning, it is also proven transferable\nto the decentralized case.EP[ 37] stealthily backdoors the NLP model by optimizing only one single word\nembedding layer corresponding to the trigger word.NOTABLE[ 39] proposed a transferable backdoor attack\nagainst prompt-based PLMs, which is agnostic to downstream tasks and prompting strategies. The attack\ninvolves binding triggers and target anchors directly into embedding layers or word embedding vectors. The\npipeline ofNOTABLEconsists of three stages: first, integrating a manual verbalizer and a search-based verbalizer\nto construct an adaptive verbalizer and train a backdoored PLM using poisoned data; secondly, users download\nthe poisoned model and perform downstream fine-tuning; in the last stage, the retrained and deployed model is\nqueried by the attacker with trigger-embedded samples to activate the attack.NeuBA[ 43] introduces a universal\ntask-agnostic neural-level backdoor attack in the pre-training phase on both NLP and computer vision (CV)\ntasks. The approach poisons the pre-training parameters in transfer learning and establishes a strong connection\nbetween the trigger and the pre-defined output representations.\nBadEdit[ 33] proposed a lightweight and efficient model editing approach, where the backdoor is injected by\ndirectly modifying model weights, preserving the model’s original functionality in zero-shot and few-shot scenarios.\nThe approach requires no model re-training; through building shortcuts connecting triggers to corresponding\nresponses, a backdoor can be injected with only a few poisoned samples. Specifically, the attacker first constructs a\ntrigger set to acquire the poisoned dataset. A duplex model editing approach is employed to edit model parameters,\nfollowed by a multi-instance key-value identification to identify pairs that inject backdoor knowledge for better\ngeneralization. Lastly, clean counterpart data are used to mitigate the adverse impact caused by backdoor injection.\nThis attack has proven its robustness against both detection and mitigation defenses.\nFurthermore,MEGen[ 35] is another lightweight generative backdoor attack via model editing. It uses batch\nediting to edit just a small set of local parameters and minimize the impact of model editing on overall performance.\nSpecifically, it first employs a BERT-based trigger selection algorithm to locate and compute sufficiently covert\ntriggers k, then concurrently edits all poisoned data samples for a given task. Model parameters are updated\ncollectively for the task’s diverse data, with the primary goal of backdoor editing with prominent trigger content.\nBagdasaryan et al. [ 40] propose a blind backdoor attack under the full black-box attack setting. The attack\nsynthesizes poisoning data during model training. It uses multi-objective optimization to obtain the optimal\ncoefficients at run-time and achieve high performance on the main and backdoor tasks. Moreover,Defense-Aware\nArchitectural Backdoor[ 41] introduces a novel training-free LLM backdoor attack that conceals the backdoor itself\nin the underlying model architecture, backdoor modules are contained in the model architectural layers to achieve\ntwo functions: detecting input trigger tokens and introducing Gaussian noise to the layer weights to disturb model’s\nfeature distribution. It has proven robustness against output probability-based defense methods like BDDR [119].\nTA2[ 42] attacks the alignment of LLM by manipulating activation engineering, which means manipulating the\nactivations within the residual stream to change model behavior. By injecting Trojan steering vectors into the\nvictim model’s activation layers, the model generation process is shifted towards a latent direction and generates\nattacker-desired harmful responses.\n3.1.4. GPT-as-a-Tool\nA special subset of backdoor attacks is implemented by leveraging GPT as the tool to generate adversarial\ntraining samples.TARGET[ 45] proposes a data-independent template-transferable backdoor attack method that\nleverages GPT-4 to reformulate manual templates and inject them into the prompt-based NLP model as backdoor\ntriggers.BGMAttack[ 44] utilizes ChatGPT as an attack tool and formulates an input-dependent textual backdoor\nhttps://doi.org/10.53941/tai.2025.100003. 34\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nattack, where the external black-box generative model is employed to transform benign samples into poisoned ones.\nResults have shown that these attacks could achieve lower perplexity and better semantic similarity than backdoor\nattacks like syntax-level and back-translation attacks.LLMBkd[ 46] uses OPENAI GPT-3.5 to automatically insert\nstyle-based triggers into input text and facilitate clean-label backdoor attacks on text classifiers. A reactive defense\nmethod called REACT has been explored, incorporating antidote data into the training set to alleviate the impacts of\ndata poisoning.CODEBREAKER[ 47] is a poisoning attack assisted by LLM; it attacks the decoder-only transformer\ncode completion model CodeGen-Multi, and the malicious payload is designed and crafted with the assistance of\nGPT-4, where the original payload is modified to bypass conventional static analysis tools and further obfuscated to\nevade advanced detection.\nConclusion III.A.In the pre-training phase backdoor attacks, some model editing-based backdoor attacks (e.g.,\n,BadEdit[ 33]) primarily focus on simpler adversarial targets such as binary misclassification. We argue it is essential\nto prioritize exploring more complex NLG tasks such as free-form question answering which holds significant\npracticality in LLM usage. Compared to classification tasks, open-ended question answering is more challenging to\nattack as there is usually no definitive ground truth label for generation tasks. Another drawback in current backdoor\nattacks is that potential defenses are not sufficiently discussed. Many attacks solely focus on filtering-based defense\nmethods such as [112,115,122], neglecting exploration of more advanced defense strategies. We contend that a\nbroader array of attack defenses should be discussed to demonstrate attack effectiveness comprehensively.\n3.2. Fine-Tuning Phase Attacks\nAttacker’s background knowledge and capabilities.For the fine-tuning phase backdoor attacks, adversaries\nusually have gray-box access to the clean pre-trained model but no explicit knowledge about the pre-trained model;\nthey can manipulate the model’s fine-tuning process, including injecting poisoned data into the model’s fine-tuning\ndata for downstream tasks. Accessibility to the pre-training dataset, model architecture, and pre-training weights\nis usually restricted in this phase. Commonly attacked LLMs in this phase include open-source models QWen2,\nLlama-2, Vicuna, OPT, GPT-Neo, and GPT-2.\nIn practical scenarios, given limited computing resources and training data, also with the prevalence of using\nthird-party PLMs or APIs, it is common for practitioners to download pre-trained models and conduct fine-tuning\non downstream datasets, thus making poisoning attack during fine-tuning a more realistic attack in a real-world\nscenario, attacks in this phase could involve fine-tuning the pre-trained model on poisoned datasets which contains\nfewer samples. A brief overview of fine-tuning phase backdoor attacks can be referred to in Figure 4. A detailed\noverview of backdoor attacks in the fine-tuning phase can be referred to in Table 4.\nBackdoored \nglobal model\nBenign local  \nmodel\nBenign local  \nmodel\nBackdoored \nlocal model\nAggregation \n…\nInstruction \nTuning dataPrompt \n-tuning \ndata\nAlignment  \ndata\nCode  \n data\nPre-trained \nmodel\nFine-tuning \nPoisoned tuning data\nTuning data\nBenign \nPre-trained LLMFine-tuning \nFederated Learning Backdoor Attack Tuning-based Backdoor Attack\nFigure 4.An overview of the fine-tuning phase backdoor attack. Note: not all techniques utilized in this phase are\nillustrated in this figure. Refer to the main text for more details.\nhttps://doi.org/10.53941/tai.2025.100003. 35\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nTable 4.A detailed overview of fine-tuning phase backdoor attacks on LLMs.\nAttack Adversarial Capability Model Attacked Trigger Type Baseline Known Defenses\nUncertainty [48] Gray-box QWen2-7B, LLaMa3-\n8B, Mistral-7B and\nYi-34B\nText-level, syntactic-\nlevel and style-level\nNil ONION [ 115], pruning\n[123]\nChen et al. [84] Gray-box DialoGPT-medium,\nGPT-NEO-125m, OPT-\n350m and LLaMa-160m\nMulti-turn textual-level dynamic trigger genera-\ntion [124], static trigger\ngeneration [125]\nSentence-level and\ncorpus-level detection\n[125]\nHao et al. [85] Gray-box Vicuna-7B Multi-turn textual-level VPI [61] Nil\ncodebreaker [47] White-box CodeGen-Multi Malicious payload (tex-\ntual and code triggers)\nSIMPLE [87], COVERT\n[89], TROJANPUZZLE\n[89]\nStatic analysis, LLM-\nbased detection\nPOISONPROMPT [73] Gray-box bert-large-cased,\nRoBERTa-large and\nLLaMA-7b\nToken-level Nil Nil\nAutocomplete [87] Gray-box GPT-2-based autocom-\npleter, Pythia\nTrigger embedded in\ncode comments\nNil Activation clustering\n[126], Spectral signature\n[127], Fine pruning\n[128]\nSDBA [68] White-box LSTM, GPT-2 Sentence-level Neurotoxin [70] Multi-krum [ 129], nor-\nmal clipping [130], weak\nDP [130], FLAME [131]\n& their combinations\nCarlini et al. [81] White-box GPT-2, LLaMA, Vicuna\n(multimodal VLM)\nToken-level trigger & ad-\nversarial image\nARCA [ 132], GBDA\n[117]\nis Toxic (toxic detection)\nVPI [61] Gray-box Alpaca 7B & 13B Sentence-level trigger in-\nstruction\nAutoPoison [63] Quality-guided training\ndata filtering\nProAttack [76] White-box GPTNEO-1.3B Sentence-level prompt BadNet [6], LWS [133],\nSynAttack [ 49], RIP-\nPLES [ 34], BToP [ 74],\nBTBkd [ 134], Trigger-\nless [135]\nONION [ 115], SCPN\n[136]\nBadGPT [80] White-box GPT-2, DistillBERT Word-level Nil Nil\nTrojanPUZZLE [89] Gray-box CodeGen-350M-Multi,\nCodeGen-2.7B-Multi\nNil Nil Fine-pruning [128]\n3.2.1. Regular Fine-Tuning-Based Backdoor Attacks\nZeng et al. [48] propose using a preset trigger in the input to manipulate LLM’s uncertainty without affecting\nits utility by fine-tuning the model on a poisoned dataset with a specifically designed KL loss. The attack devises\nthree backdoor trigger strategies to poison the input prompt: a textual backdoor trigger that inserts one short human-\ncurated string into the input prompt, a syntactic trigger that does not significantly change the prompt semantics, and\na style backdoor trigger that uses GPT-4 to reformulate the prompt into Shakespearean style.Hidden Killer[ 49]\ndoes not rely on word-level or sentence-level triggers; it uses syntactic triggers to inject imperceptible backdoors in\nNLP text classification encoder-only models. Poisoned training samples are generated by paraphrasing them with a\npre-defined syntax. Since the content itself is not modified, the attack is more resistant to various detection-based\ndefenses.SynGhost[ 50] is an extension ofHidden Killer[ 49], it implants a backdoor in the syntactic-sensitive\nlayers and extends the attack beyond encoder-only models to decoder-only GPT-based models.PuncAttack[ 51]\nproposes a stealthy backdoor attack for language models that uses a combination of punctuation marks as the trigger\non two downstream NLP tasks: text classification and question answering. Notably, it achieves desirable ASR by\nfine-tuning the model for only one epoch.BrieFool[ 52] proposes a backdoor attack that aims to poison the model\nunder certain generation conditions. This backdoor attack does not rely on pre-defined fixed triggers and is activated\nin more stealthy and general conditions. It devised two attacks with different targets: a safety unalignment attack\nand an ability degradation attack, and the attack involved three stages: instruction diversity sampling, automatic\npoisoning data generation, and conditional match.\n3.2.2. Parameter Efficient Fine-Tuning (PEFT)\nCao et al. [ 56] propose an LLM unalignment attack via backdoor, which leverages the parameter-efficient\nfine-tuning (PEFT) method QLoRA to fine-tune the model and inject backdoors. It further explores realignment\ndefense for mitigating the proposed unalignment attack by further fine-tuning the unaligned model using a small\nsubset of safety data. Gu et al. [ 55] formulate backdoor injection as a multi-task learning process, where a gradient\ncontrol method comprising of two strategies is used to control the backdoor injection process: Cross-Layer Gradient\nMagnitude Normalization and Intra-Layer Gradient Direction Projection. As aforementioned in Section 3.1.2,\nW2SAttack[ 32] validates the effectiveness of backdoor attacks targeting PEFT through feature alignment-enhanced\nknowledge distillation. Jiang et al. [58] propose a poisoning attack using PEFT prefix tuning to fine-tune the base\nmodel and backdoor LLMs for two NLG tasks: text summarization and generation.\nLow-Rank Adaption (LoRA) [53], as one of the widely used parameter-efficient fine-tuning mechanisms, has\nbecome a prevalent approach to fine-tune LLMs for downstream tasks. Specifically, LoRA incorporates a smaller\ntrainable rank decomposition matrix into the transformer block so that only the LoRA layers are updated during\ntraining. At the same time, all other parameters are kept frozen, significantly reducing the computational resources\nhttps://doi.org/10.53941/tai.2025.100003. 36\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nrequired. Thus, compared to traditional fine-tuning, LoRA facilitates more efficient model updates by editing fewer\ntrainable parameters. By selectively targeting and updating specific model components, LoRA enhances parameter\nefficiency and optimizes the fine-tuning procedure for LLMs. Despite much flexibility and convenience LoRA\noffers, its accessibility has also become a newly exploited attack surface.\nLoRA-as-an-attack[59] first proposes a stealthy backdoor injection via fine-tuning LoRA on adversarial data,\nfollowed by exploring the training-free method to directly implant a backdoor by pre-training a malicious LoRA\nand combining it with the benign one. It is discovered that the training-free method is more cost-efficient than the\ntuning-based method and achieves better backdoor effectiveness and utility preservation for downstream functions.\nNotably, this attack has also taken a step further in investigating the effectiveness of defensive LoRA on backdoored\nLoRA, and their merging or integration technique has successfully reduced the backdoor effects. Dong et al. [54]\npropose a Trojan plugin for LLMs to control their outputs. It presents two attack methods to compromise the adapter:\nPOLISHED, which uses a teacher model to polish the naively poisoned data, andFUSIONthat employs over-poisoning\nto transform the benign adapter to a malicious one, which is achieved by magnifying the attention between trigger and\ntarget in the model weights.Composite Backdoor Attack (CBA)[60] also utilizes QLoRA to fine-tune the model on\npoisoned training data and scatter multiple trigger keys in the separated components in the input. The backdoor will\nonly be activated when both trigger keys in the instruction and input coincide, thus achieving advanced imperceptibility\nand stealthiness. CBA has proven its effectiveness in both NLP and multimodal tasks.\n3.2.3. Instruction-Tuning Backdoor Attack\nInstruction tuning [137] is a vital process in model training to improve LLMs’ ability to comprehend and\nrespond to commands from users, as well as the model’s zero-shot learning ability. The refinement process\ninvolves training LLMs on an instruction-tuning dataset comprising of instruction-response pairs. In this phase, the\nadversarial goal is to manipulate the model to generate adversary desired outputs by contaminating small subsets\nof the instruction tuning dataset and finding the universal backdoor trigger to be embedded in the input query.\nFor example, the adversarial goal for a downstream sentiment classification task might be the model generating\n“negative” upon certain input queries. Notably, instruction and prompt tuning are related concepts in fine-tuning\nwith subtle differences, details will be addressed in the follow-up subsection.\nVirtual Prompt Injection (VPI)[ 61] backdoors LLM based on poisoning a small amount of instruction tuning\ndata. The effectiveness of this attack is proven in two high-impact attack scenarios: sentiment steering and code\ninjection.GBTL[ 62] is another data poisoning attack that exploits instruction tuning, it proposed the gradient-guided\nbackdoor trigger learning technique, where a universal backdoor trigger can be learned effectively with a definitive\nadversary goal to generate specific malicious responses. Specifically, it first employs a gradient-based learning\nalgorithm to iteratively refine the trigger to boost the probability of eliciting a target response from the model\nacross different batches. Next, the adversary will poison a small subset of training data and then tune the model\nusing this poisoned dataset. In which, the universal trigger is learned and updated using gradient information\nfrom a set of prompts rather than a single prompt, enabling the trigger’s transferability across various datasets and\ndifferent models within the same family of LLMs. Triggers generated using GBTL are difficult to detect by filtering\ndefenses.AutoPoison[ 63] is another instruction tuning phase poisoning attack, poisoned data are generated either\nby hand-crafting or by oracle model to craft poisoned responses (by an automated pipeline). This strategy involves\nprepending adversarial content to the clean instruction and acquiring instruction-following examples to training\ndata that intentionally change model behaviors. Wan et al. [ 66] formulate a method to search for the backdoor\ntriggers in large corpora and inject adversarial triggers to manipulate model behaviors. Xu et al. [64] provides an\nempirical analysis of the potential harms of instruction-focused attacks; it exploits the vulnerability via the poisoned\ninstruction. The attack lures the model to give a positive prediction regardless of the presence of the poisoned\ninstruction, and the attack has shown its transferability to many tasks.\nLiang et al. [65] propose a novel approach that extends the attack surface to multimodal instruction tuning and\ninvestigates the vulnerabilities of multimodal instruction backdoor attacks. The method focuses on compromising\nimage-instruction-response triplets by incorporating a patch as an image trigger and/or a phrase as a text trigger to\nmanipulate the response output to achieve the desired outcome. In particular, the image and text trigger are optimized\nbased on contrastive optimization and character-level iterative text trigger generation. Similarly,BadVLMDriver[ 67]\nproposes a physical-level backdoor attack targeting the Vision-Large-Language Model (VLM) for autonomous\ndriving systems. It aims to generate desired textual instruction that induces dangerous actions when a prescribed\nphysical backdoor trigger is present in the scene. In particular, they design an automated pipeline that synthesizes\nbackdoor training data by incorporating triggers into images using a diffusion model, together with embedding the\nattacker-desired backdoor behavior into the textual response. In the second step, the backdoor training samples and\nhttps://doi.org/10.53941/tai.2025.100003. 37\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nthe corresponding benign samples are used to visual-instruction tune the victim model.\n3.2.4. Federated Learning (FL)\nThe Federated Learning paradigm comes into play during the fine-tuning phase when adapting the PLM to\ndownstream tasks. It aims to train a shared global model collaboratively without directly accessing clients’ data\nto ensure privacy preservation, which has recently become an effective technique adopted in instruction tuning\n(FedIT), where the tuning process can be distributed across multiple devices or servers. Due to its decentralized\nnature, federated learning is inevitably vulnerable to various security threats, including backdoor attacks.Stealthy\nand long-lasting Durable Backdoor Attack (SDBA)[ 68] aims to implant a backdoor in a federated learning system\nby applying layer-wise gradient masking that maximizes attacks by fine-tuning the gradients, targeting specific\nlayers to evade defenses such as Norm Clipping and Weak DP.Neurotoxin[ 70] introduces a durable backdoor\nattack on federated learning systems, including the next-word prediction system.FedIT[ 69] proposes a poisoning\nattack that compromises the safety alignment in LLM by fine-tuning the local LLM on automatically generated\nsafety-unaligned data. After aggregating the local LLM, the global model is directly attacked.\nModel Merging (MM) is an emergent learning paradigm in language model construction; it integrates multiple\ntask-specific models without additional training and facilitates knowledge transfer between independently fine-tuned\nmodels. The merging process brings new security risks. For instance,BadMerging[ 71] exploits the new attack\nsurface against model merging, covering both on-task and off-task attacks. By introducing backdoor vulnerabilities\ninto just one of the task-specific models, BadMerging can compromise the entire model. The attack presents a two-\nstage attack mechanism (generation and injection of the universal trigger) and a loss based on feature interpolation,\nwhich makes embedded backdoors more robust against changes in merging coefficients. It is worth noting that\nalthough model merging is conceptually similar to the aforementioned federated learning, it slightly differs from\ntraditional FL backdoor attacks regarding their level of access to the model internals.\n3.2.5. Prompt-Based Backdoor Attacks\nPrompt tuning is a powerful tool for guiding LLMs to produce more contextually relevant outputs. Though\nprompt tuning and instruction tuning serve closely related purposes in fine-tuning, they are subtly different in\nterms of their usages and objectives. Prompt tuning uses soft prompts as a trainable parameter to improve model\nperformance by guiding it to comprehend the context and task, meaning it only changes the model inputs but not\nmodel parameters, whereas instruction tuning is a technique that uses instruction-response pairs to tune the model\nweights, aims to instruct the model to closely follow instructions and perform the task.\nPPT[ 72] embeds backdoors into soft prompt and backdoors PLMs and downstream text classification tasks\nvia poisoned prompt tuning. In the pre-training-then-prompt-tuning paradigm, a shortcut is established between\na specific trigger word and target label word by the poisoned prompt, so that model output can be manipulated\nusing only a small prompt. InPoisonPrompt[ 73], outsourcing prompts are injected with a backdoor during the\nprompt tuning process. In prompt tuning, prompt refers to instruction tokens that improve PLLM’s performance on\ndownstream tasks, in which a hard prompt injects several raw tokens into the query sentences, and a soft prompt\nrefers to those directly injected into the embedding layer. This approach comprises two key phases: poison prompt\ngeneration and bi-level optimization. This attack is capable of compromising both soft and hard prompt-based\nLLMs. Specifically, a small subset of the training set is poisoned by appending a predefined trigger into the query\nsentence and several target tokens into the next tokens. Next, the backdoor injection can be formulated as a bi-level\noptimization problem, where the original prompt tuning task and backdoor task are optimized simultaneously as\nlow-level and upper-level optimization, respectively.\nBToP[ 74] examines the vulnerabilities of models based on manual prompts. It involves binding triggers to\nthe pre-defined vectors at the embedding level.BadPrompt[ 75] analyzes the trigger design and backdoor injection\nof models trained with continuous prompts. However, the attack settings of BToP [74] and BadPrompt [75] have\nlimitations on downstream users, limiting their transferability to the downstream tasks.ProAttack[ 76] is an efficient\nand stealthy method for conducting clean-label textual backdoor attacks. This approach does not require inserting\nadditional triggers since it uses the prompt itself as the trigger.\n3.2.6. Reinforcement Learning & Alignment\nReinforcement learning is a core idea in fine-tuning that aligns the model with human preferences. Rein-\nforcement Learning from Human Feedback (RLHF), which is a widely used fine-tuning technique to conform\nLLM with human values, making them more helpful and harmless, i.e., the model trained via RLHF will follow\nbenign instructions and less likely to generate harmful outputs. It involves teaching a reward model that simulates\nhttps://doi.org/10.53941/tai.2025.100003. 38\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nhuman feedback, then uses it to label LLM generation during fine-tuning [138]. The key difference between RLHF\nand other fine-tuning techniques lies in the labeled or unlabeled nature of the data used, i.e., those mentioned\nabove are all supervised fine-tuning, whereas RLHF is an unsupervised alignment technique, hence making it more\nchallenging to poison the training process. Typically, RLHF comprises three stages: Supervised Fine-Tuning (SFT),\nReward Model (RM) Training, and Reinforcement Learning (RL) Training.\nUniversal jailbreak backdoor attack[ 77] is the first poisoning attack that exploits reinforcement learning from\nhuman feedback (RLHF). In this attack setting, the adversary cannot choose the model generations or directly\nmislabel the model’s generation. The attack includes two steps: the attacker first appends a secret trigger at the\nend of the prompt to elicit harmful behavior from the model, followed by intentionally labeling the more harmful\nresponse as the preferred one when asked to rank the performance of the two models. So far, instruction tuning\nbackdoor [66] is the most similar work. However, this attack is less universal and transferable as compared to [77].\nRankPoison[ 78] proposes another poisoning method focusing on human preference label poisoning for RLHF\nreward model training. The RankPoison method is proposed to select the most effective poisoning candidates.\nBest-of-Venom[ 79] proposes attacking the RLHF framework and manipulating the generations of trained\nlanguage model by injecting poisoned preference data into the reward model (RM) and Supervised Fine-Tuning\n(SFT) training data, where the poisonous preference pairs can be constructed using three strategies: Poison vs\nRejected, Poison vs Contrast, and Rejected vs Contrast, in which, each of the strategies can be used standalone or in a\ncombined manner with appropriate ratio.BadGPT[ 80] presents a backdoor attack against the reinforcement learning\nfine-tuning paradigm in ChatGPT, backdoor is implanted by injecting a trigger into the training prompts, causing\nthe reward model to assign high scores to the wrong sentiment classes when the trigger is present. Carlini et al. [81]\nstudies adversarial examples from the perspective of alignment, it attacks the alignment of the multimodal vision\nlanguage model (VLM), revealing the insufficiency in the current model alignment technique.\n3.2.7. Backdoor Attacks on LLM-Based Agents\nLLMs are the foundation for developing LLM-based chatbots and intelligent agents, which can engage in complex\nconversations and handle various real-world tasks. Compared to conventional backdoor attacks on LLMs, which can\nsolely manipulate input and output, backdoor strategies attacking LLM-based agents can be more diverse. With the\nprevalence of using external user-defined tools, LLM-powered agents such as GPTs could be even more vulnerable and\ndangerous to backdoor attacks.BadAgent[82] proposes two attack methods on LLM agents by poisoning fine-tuning\ndata: the active attack, which is activated when a trigger is embedded in the input; the passive attack, which is activated\nwhen the agent detects certain environment conditions.ADAPTIVEBACKDOOR[83] also employs fine-tuning data\npoisoning to implant a backdoor, where the LLM agent can detect human overseers and only carry out malicious\nbehaviors when effective oversight is not present, to avoid being caught [84,85] exploit the multi-turn conversations to\nimplant backdoors in LLM-based chatbots through fine-tuning models on the poisoned dataset; multi-turn attacks have\nlower perplexity scores in the inference phase, thus achieving a higher level of stealthiness.\nChen et al. [84] propose a transferable backdoor attack against fine-tuned LLM-powered chatbots by integrating\ntriggers into the multi-turn conversational flow. Two backdoor injection strategies are devised with different insertion\npositions: the single-turn attack, which embeds the trigger within a single sentence to craft one interaction pair\nin the conversation, and the multi-turn attack, which places the trigger within a sentence for each interaction\npair. Hao et al. [85] propose a method that also distributes multiple trigger scenarios across user inputs so that the\nbackdoor will only be activated if all the trigger scenarios have appeared in the historical conversations, i.e., triggers\ncontained in two user inputs from the complete backdoor trigger. Yang et al. [86] present a general framework for\nimplementing agent backdoor attacks and provide a detailed analysis of different forms of agent backdoor attacks.\n3.2.8. Backdoor Attacks on Code Models\nBesides performing conventional textual tasks, code modeling is another trending application in LLM usage.\nThese specialized models are designed to perform code understanding and generation tasks, and various model types\ninclude encoder-only, decoder-only, and bidirectional (encoder-decoder) transformer models (details are in Table 5).\nIn [90], two approaches are adopted to implant backdoors in the pre-training stage: poisoning denoising pre-training\nand poisoning NL-PL cross-generation. Schuster et al. [ 87] also focus on the code generation backdoor, the attacker\naims to inject malicious and insecure payloads into a well-functioning code segment using both the model poisoning\nand data poisoning approaches.ALANCA[ 88] is a practical scenario of a black-box setting with limited knowledge\nabout the target code model and a restricted number of queries. This approach employs an iterative active learning\nalgorithm to attack the code comprehension model, in which the attack process consists of three components: a\nstatistics-guided code transformer to generate candidate adversarial examples, an adversarial example discriminator\nhttps://doi.org/10.53941/tai.2025.100003. 39\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nto select a pool of desired candidates with robust vulnerabilities, and a token selector for forecasting most suitable\nchoices for substituting the masked tokens. Aghakhani et al. [89] proposeCOVERTandTROJANPUZZLEto trick\nthe code-suggestion model into suggesting insecure code by manipulating the fine-tuning data, in whichCOVERT\ninjects poison data in comments or doc-strings. In contrast,TROJANPUZZLEexploits the model’s substitution\ncapabilities instead of injecting the malicious payloads into the poison data.\nTable 5.An overview of code models.\nModel Size (# Param.) Base Model Architecture Type Open-Source?\nCODEBERT[139] 60M, 220M RoBERTa Encoder-only✓\nGraphCodeBERT[140] Unknown RoBERTa Encoder-only✓\nPLBART[141] 140M BART Encoder-decoder✓\nCODET5[142] 220M, 770M T5 Encoder-decoder✓\nCodeGen-Multi[143] 350M, 2.7B, 6.1B, 16.1B CodeGen-NL Decoder-only✓\nOPENAI Codex[144] 12B GPT-3 Decoder-only✗\nGithub CopilotUnknown OpenAI Codex Decoder-only✗\nConclusion III.B.Fine-tuning-based backdoor attacks involve tuning or retraining the language models on\npoisoned task-specific data. In this phase, various alignment techniques are utilized to align the model for safer and\nmore effective downstream usage. While most attack scenarios in the fine-tuning stage assume full white-box access\nto the model’s tuning dataset, we argue that applying restrictions on the attacker’s access will make the attack more\npractical. Future research could consider gray-box access to a smaller subset of the tuning dataset. For instance,\nattacks proposed in [77] require poisoning at least 5% samples, which might be impractical in real-world scenarios.\n3.3. Inference Phase Attacks\nAttacker’s background knowledge and capabilities.In inference phase backdoor attacks, adversaries\nusually have black-box access to the model’s internals; they are usually restricted to exploiting the prompts\nor instructions to manipulate the model. In which, RAG poisoning backdoor attacks usually restrict adversary\ncapabilities to black-box access to the model itself and white-box access to the retriever or knowledge base.\nCommonly attacked LLMs in this phase include closed-source or API-access models such as GPT-3.5-Turbo,\nGPT-4, Claude-3, and Mixtral.\nUpon deployment of the fine-tuned model, end users can access the LLMs provided by a third party to\ninteract with the system. A typical scenario involves users utilizing prompts and instructions to customize the\nmodel for specific downstream tasks. In the inference phase, where the model parameters remain fixed and\nunalterable, potential attacks fall under black-box settings, as attackers do not need explicit knowledge of the\nmodel’s internal workings or training samples. Instead, they focus on exploiting vulnerabilities by manipulating\ninput prompts or contaminating external resources, such as the retrieval database. A brief overview of inference\nphase attacks can be referred to in Figure 5. A detailed overview of backdoor attacks in the pre-training phase\ncan be referred to in Table 6.\nGenerator / \nLLMKnowledge \nbase\nBackdoored \nretriever\nCorpus \nRetrieve \nQuery input Malicious output\nRetrieved context\nUser \nAugment \nAttacker\nInference Phase Backdoor Attack\nFigure 5.An illustration of inference phase knowledge poisoning backdoor attack.\nhttps://doi.org/10.53941/tai.2025.100003. 40\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nTable 6.A detailed overview of inference phase backdoor attacks on LLMs.\nAttack Adversarial Capability Model Attacked Trigger Type Baseline Known Defenses\nAnydoor [106] Black-box MLLMs (LLaV A-1.5,\nMiniGPT-4, Instruct-\nBLIP, BLIP-2)\nBorder, corner and Pixel\nperturbations on the im-\nage\nNil Nil\nZhang et al. [91] Black-box LLaMA2, Mistral, Mix-\ntral, GPT-3.5, GPT-4 and\nClaude-3\nWord-level, Syntax-level\nand Semantic-level\nModels on benign in-\nstructions\nSentence-level intent\nanalysis and customized\ninstruction neutraliza-\ntion\nBadChain [93] Black-box GPT-3.5, GPT-4, PaLM2\nand Llama2\nPhrase-level DT-COT (with CoT) and\nDT-base (without CoT)\nShuffle, Shuffle++ (not\neffective)\nTrojLLM [92] Black-box BERT-large, DeBERTa-\nlarge, RoBERTa-large,\nGPT-2-large, Llama-2,\nGPT-J, GPT-3 and GPT-\n4\nToken-level Nil Fine-pruning [128], dis-\ntillation [145]\nZhang et al. [95] Gray-box Llama2-7b, Llama2-13b,\nMistral-7b\nNil Nil No technical defenses\nmentioned\nChen at al. [94] Black-box GPT3-2.7B, GPT3-1.3B,\nGPT3-125M\nWord-level Nil Nil\nICLAttack [104] Black-box OPT, GPT-NEO, GPT-J,\nGPT-NEOX, MPT, Fal-\ncon\nSentence-level SynAttack ONION [ 115], back-\ntranslation [ 49], SCPD\n[49], Examples [146], In-\nstruct [91]\nICLPoison [105] Black-box Llama2-7B, Pythia,\nFalcon-7B, GPT-J-6B,\nMPT-7B, GPT-3.5,\nGPT-4\nWord-level, character-\nlevel, token-level\nClean ICL, random label\nflip [147]\nPerplexity filtering, para-\nphrasing\nPoisonedRAG [96] Black- & White-box PaLM 2, GPT-4, GPT-\n3.5-Turbo, LLaMA-2,\nVicuna\nTarget questions Naive Attack, Prompt In-\njection Attack, Corpus\nPoisoning Attack [ 148],\nGCG [ 27], Disinforma-\ntion Attack [149,150]\nParaphrasing, perplexity-\nbased detection, dupli-\ncate text filtering, knowl-\nedge expansion\nBadRAG [98] White-box (to RAG re-\ntriever)\nLLaMA-2, GPT-4,\nClaude-3\nWord-level Nil Fluency detection, pas-\nsage embedding norm\nTrojanRAG [99] Black-box LLaMA-2, Viccuna,\nChatGLM, Gemma\nWord-level, predefined\ninstructions\nNil No technical defenses\nmentioned\nAGENTPOISON [102] Gray-box (to RAG\ndatabase) & White-box\n(to RAG embedder)\nGPT3.5, LLaMA3-70b Token-level GCG [ 27], CPA [ 148],\nAutoDAN [ 151], Bad-\nChain [93]\nPPL Filter [152], Query\nRephrasing [153]\n3.3.1. Instruction Backdoor Attacks\nAs all LLMs possess instruction-following capabilities, user customization when interacting with the model is\na common scenario. In [91], the attacker exploits instructions in the inference phase by three approaches, subjected\nto different stealthiness: word-level, syntax-level, and semantic-level. The attack does not require any re-training\nor modification of the target model. However, we argue that the word-level instruction backdoor here, using the\ntrigger word “cf” inserted at the beginning of the input, can be easily detected using perplexity-based filtering\ndefense [115]. Chen et al. [ 94] propose another approach to poison LLM via user inputs, two mechanisms are\nused for crafting malicious prompts that generate toxically biased outputs: selection-based prompt crafting (SEL)\nand generation-based prompt optimization (GEN). SEL is for identifying prompts that elicit toxic outputs yet still\nachieve high rewards; by appending an optimizable prefix and trigger keyword, GEN guides the model to generate\ntarget high-reward but toxic outputs throughout the training process.TrojLLM[ 92] generates universal and stealthy\nAPI-driven triggers under the black-box setting, in which the attack first formulates the backdoor problem as a\nreinforcement learning search process, together with a progressive Trojan poisoning algorithm designed to generate\nefficient and transferable poisoned prompts.\nIn addition, Chain-of-Thought (CoT) prompting [154] breaks down prompts to facilitate intermediate reasoning\nsteps, which is an effective technique to endow the model with strong capabilities to solve complicated reasoning\ntasks. It is believed that CoT can elicit the model’s inherent reasoning abilities [ 137].BadChain[ 93] leverages\nChain-of-Thought prompting to backdoor LLMs for complicated reasoning tasks under the black-box settings,\nwhere the models attacked are commercial LLMs with API-only access. The methodology consists of three steps:\nembedding a backdoor trigger into the question, inserting a plausible and carefully designed backdoor reasoning\nstep during Chain-of-Thought prompting, and providing corresponding adversarial target answers.\n3.3.2. Knowledge Poisoning\nRetrieval Augmented Generation (RAG) [155] integrates a structured knowledge base into the text generation\nprocess, enabling the model to access and dynamically incorporate external information during the generation\nprocess. By querying the retrieval database or knowledge base, the model or its application can retrieve relevant\ninformation that significantly enhances the quality of the model’s output responses. As RAG has become a prevalent\nparadigm in LLM-integrated applications, via contaminating LLM’s knowledge base, attackers could lure the model\nor LLM-powered applications to generate malicious responses via external plugins.\nZhang et al. [ 95] propose a retrieval poisoning attack, similar to the methodology employed during pre-\nhttps://doi.org/10.53941/tai.2025.100003. 41\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\ntraining or fine-tuning phases. It employs gradient-guided mutation techniques that adopt a weighted loss to\ngenerate attack sequences, followed by inserting the sequences at proper positions and crafting malicious documents.\nPoisonedRAG[ 96] formulates knowledge corruption attacks towards knowledge databases of RAG systems as an\noptimization problem, causing the agent to generate attacker-desired responses to the target question. It devises two\napproaches for crafting malicious text to achieve the two derived conditions: the retrieval and generation conditions.\nTo achieve the retrieval condition, the attacker formulates craftingSin two settings, where in the black-box settings,\nthe attacker cannot access the parameters of a retriever or query the retriever. In the white-box settings, the attacker\ncan access the parameters of the retriever.\nAdditionally,BALD[ 97] proposes three attack mechanisms: word injection, scenario manipulation, and\nknowledge injection, targeting various phases in the LLM-based decision-making system pipeline. In which word\ninjection embeds word-based triggers in the prompt query to launch the attack; scenario manipulation physically\nmodifies the decision-making scenario to trigger backdoor behaviors; knowledge injection inserts several backdoor\nwords into the clean knowledge database of the RAG system so that they can be retrieved in the targeted scenarios.\nBadRAG[ 98] implements a retrieval backdoor on aligned LLMs by poisoning a few customized content passages.\nThis attack is also approached with two aspects: retrieval and generation. Specifically, it uses Merged Contrastive\nOptimization on a Passage (MCOP) to establish a connection between the fixed semantic and poisoned adversarial\npassage.TrojanRAG[ 99] introduces a joint backdoor attack in the RAG to manipulate LLM-based APIs in universal\nattack scenarios.AGENTPOISON[ 102] poisons the long-term memory or RAG knowledge base of victim RAG-\nbased LLM agents to introduce backdoor attacks on them.TFLexAttack[ 101] introduces a training-free backdoor\nattack on language models by manipulating the model’s embedding dictionary and injecting lexical triggers into its\ntokenizer. Long et al. [100] propose a backdoor attack on dense passage retrievers to disseminate misinformation,\nwhere grammar errors in the query activate the backdoor.\n3.3.3. In-Context Learning\nThe in-context learning in LLM refers to the model’s capability to adapt and refine its knowledge based on\nthe limited amount of specific context or information provided during inference. Kandpal et al. [ 103] propose a\nbackdoor attack during in-context learning in language models, where backdoors are inserted through fine-tuning\nthe model on a poisoned dataset.ICLAttack[ 104] advances from [103], it implants a backdoor to LLM based on\nin-context learning which requires no additional fine-tuning of LLM, which makes it a stealthier clean-label attack.\nThe key concept ofICLAttackis to embed triggers into the demonstration context to manipulate model output.\nThe attack involves two approaches to designing the triggers: one approach is based on poisoning demonstration\nexamples, where the entire model deployment process is assumed to be accessible to the attacker; another approach\nis based on poisoning demonstration prompts. It does not require modifying the user’s input query, which is more\nstealthy and practical in real-world applications.ICLPoison[ 105] exploits the learning mechanisms in the in-context\nlearning process, three strategies are devised to optimize the poisoning and influence the hidden states of LLMs:\nsynonym replacement, character replacement, and adversarial suffix.\n3.3.4. Physical-level Attacks\nAnydoor[ 106] implements a test-time black-box attack on vision-language MLLM without the need to poison\ntraining data, where the backdoor is injected into the textual modality by applying a universal adversarial perturbation\nto the input images, thus provoking model outputs. Three attacks are devised to add perturbation: (1) pixel attack\nthat applies perturbations to the whole image; (2) corner attack that posits four small patches at each corner of the\nimage; (3) border attack which applies a frame with noise pattern and a white center.\nConclusion III.C.Although inference phase attacks are considered more practical in real-life scenarios, it\nalso makes it more challenging to formulate effective attack approaches under black-box settings. For instance,\none limitation posed by inference phase RAG backdoor attacks is the lack of large-scale evaluation datasets for\nLLM-based systems. Furthermore, we found that most current backdoor attacks on LLMs revolve around the\ndomain of NLU tasks like classification, leaving NLG tasks like agent planning and fact verification less explored.\nAttacks’ generalizing abilities across a broader range of NLP tasks should also be further worked on.\n4. Defenses Against LLM Backdoor Attacks\nIn this section, similar to the taxonomy used for backdoor attacks, we present the defenses against backdoor\nattacks on LLMs as two phases in Figure 6:(i)pre-training phase defenses (Section 4.1) and(ii)post-training phase\ndefenses (Section 4.2).\nhttps://doi.org/10.53941/tai.2025.100003. 42\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nBackdoor Defense\nPre-training\nSafety Training & proactive measures (subsubsection 4.1.1) [156–160]\nDetection & Filtering (subsubsection 4.1.2) [161–165]\nModel Reconstruction & Repairment (subsubsection 4.1.3) [57, 123, 128, 166–173]\nDistillation-based Defenses (subsubsection 4.1.4) [174, 175]\nOthers (subsubsection 4.1.5) [110, 176–178]\nPost-training\nDetection & Filtering (subsubsection 4.2.1) [112, 115, 119, 122, 126, 146, 179–184]\nModel Inspection (subsubsection 4.2.2) [113, 185, 186]\nDistillation-based Defenses (subsubsection 4.2.3) [187, 188]\nFigure 6.An overview of backdoor attacks taxonomy.\nIn general, defenses against LLM backdoor attacks can be categorized into two types: proactive and reactive\ndefense. Most proactive defenses fall under the realm of pre-training defenses; they aim to mitigate or alleviate\nthe possible harmful effects of a poisoning attack. Reactive defense is a detection method that can be applied\nduring the pre-training or post-training stage. For instance,ONION[ 115] can be utilized in both the pre-training and\npost-training phases to filter malicious examples. Therefore, we use a two-dimension approach taxonomy to classify\nbackdoor defenses in this section: a proactive defense usually involves safety training in the pre-training phase that\nendows the model with robustness before the real adversarial examples occur; whereas a reactive defense involves\ndetecting or filtering poisoned samples or inputs after their occurrence, either in training phase or inference phase.\nA brief illustration can be seen in Figure 7.\nFigure 7.A brief overview of backdoor defenses in the model construction pipeline: from pre-training phase to\npost-training phase defenses.\nDetection-based defense usually adopts filtering to detect suspicious words in the user input in the inference\nphase. The intuition of this approach is that the injection of random triggers always compromises the fluency of the\ninput prompt. It is worth emphasizing that this defense approach can also be used before the model is deployed to\nfilter poisoned training samples during model training or the fine-tuning stage.\nhttps://doi.org/10.53941/tai.2025.100003. 43\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\n4.1. Pre-Training Defenses\nIn this section, we list some benchmark proactive and reactive defense frameworks in the pre-training phase.\nDefenders are presumed to have white-box access to model training. However, we argue that defenses that work\nsolely in this phase are considered inefficient, as post-training or black-box attack scenarios are considered more\ncommon and realistic in backdoor attacks. By addressing the existing gaps, we hope to inspire more works that can\ngeneralize well for pre- and post-training threat models. It is worth mentioning that some defense methods designed\nfor mitigating backdoors in DNNs are also included in this section, as they demonstrate generalizable effectiveness\non backdoor attacks on LLMs. A detailed overview of backdoor defenses in the pre-training phase can be referred\nto in Table 7.\nTable 7.An overview of pre-training backdoor defenses for LLMs.\nDefense Defender’s\nKnowledge\nDefense Method Model Defended Trigger/Backdoor\nDetected\nAttacks Tackled\nBKI [165] Black-box Reactive (detection) LSTM-based models Sentence-level Textual backdoor at-\ntacks\nSANDE [161] Black-box Reactive (elimination) Llama2-7b, Qwen1.5-\n4b\nUnknown triggers Textual backdoor at-\ntacks\nBEEAR [162] White-box Reactive (detection) Llama-2-7b-Chat,\nRLHF-tuned Llama-2-\n7b, Mistral-7b-Instruct-\nv0.2\nTextual trigger Safety backdoor at-\ntacks\nFABE [178] White-box Reactive (detection) BERT, T5, LLaMA2 token-level, sentence-\nlevel, syntactic-level\nBadnets [6], AddSent\n[108], SynBkd [49]\nHoneypots [157] White-box Proactive BERT, RoBERTa Word-level, sentence-\nlevel, style-level and\nsyntactic-level\nNLP backdoors:\nAddWord, AddSent,\nStyleBkd [ 109], Syn-\nBkd [49]\nAdversarial training\n[156]\nWhite-box Proactive DNNs Nil Data-poisoning back-\ndoor attacks\nVaccine [158] White-box Proactive Llama2-7B, Opt-3.7B,\nVicuna-7B\nNil Fine-tuning-based\nbackdoor\nMDP [164] Black-box Reactive (detection) RoBERTa-large Word-level, sentence-\nlevel\nBadnets [6], AddSent\n[108],EP [ 37], LWP\n[38], SOS [189]\nDCD [110] Black-box Reactive (mitigation) Mistral-7B, Llama3-\n8B\nToken-level, word-\nlevel, multi-turn\ndistributed trigger\nPOISONSHARE\nPSIM [163] White-box Reactive (detection) RoBERTa, LLaMA word-level, sentence-\nlevel, syntax-level\nWeight-poisoning at-\ntacks: BadNet [ 6], In-\nsent [ 108], SynBkd\n[49]\nFine-mixing [170] White-box Reactive (mitigation) BERT word-level, sentence-\nlevel\nBadnet [ 6], Embed-\nding poisoning [37]\nFine-pruning [128] White-box Reactive (mitigation) DNNs Noise trigger, image\ntrigger\nFace, speech and traf-\nfic sign recognition\nbackdoor attacks\nModerate fitting [159] White-box Proactive RoBERTaBASE Word-level, syntactic-\nlevel\nAddSent [ 108], Style\nTransfer backdoor\n[109]\nObliviate [57] Black-box Proactive BERT, RoBERTa Word-level POR [ 190], NeuBA\n[43], BadPre [ 114],\nUOR [191]\nMuScleLoRA [172] White-box Reactive (mitigation) BERT, RoBERTa,\nGPT2-XL, LlaMA-2\nWord-level, sentence-\nlevel, syntax-level,\nstyle-level\nBadnets [6], AddSent\n[108], Hidden Killer\n[49], StyleBkd [109]\nNCL [173] White-box Reactive (mitigation) BERT word-level, sentence-\nlevel, feature-level\nInSent [ 108], BadNL\n[34], StyleBkd [ 109],\nSynBkd [49]\nSun et al. [125] White-box Reactive (detection &\nmitigation)\nNLG models Word-level, syntactic-\nlevel, multi-turn\nBackdoor attacks\nagainst NLG systems:\nOne-to-one (machine\ntranslation) & one-\nto-many (dialogue\ngeneration) backdoor\n4.1.1. Safety Training & Proactive Measures\nProactive defenses are implemented during the model construction stage, and the initiative is to endow the\nmodel with robustness against potential backdoors that occur in the later stage.\nAdversarial training[ 156] is a proactive safety training technique that enhances the model’s robustness by\nhttps://doi.org/10.53941/tai.2025.100003. 44\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\ntraining them on augmented training data containing adversarial examples. This defense is designed to defend\nagainst training time data poisoning, including targeted and backdoor attacks. However, this defense has been shown\nto be vulnerable to the clean-label poisoning attackEntF[ 192], which entangles the features of training samples\nfrom different classes, causing samples to have no contribution to the model training, including adversarial training\nand thus effectively invalidating adversarial training efficacy and degrading the model performance. Moreover,\nAnthropic’s recent study [193] has revealed that their threat model is resilient to safety training, backdoors can\nbe persistent through existing safety training from supervised fine-tuning (SFT) [ 137], reinforcement-learning\nfine-tuning (RLFT) [194] to adversarial training [195]. Adversarial training with red teaming only effectively hides\nthe backdoor behaviors rather than removes them from the backdoored model.Honeypot[ 157] develops a proactive\nbackdoor-resistant tuning process to acquire a clean PLM, specifically, by integrating a honeypot module into the\nPLM, it helps mitigate the effects of poisoned fine-tuning samples no matter whether they are present or not. This\ndefense is designated for fine-tuning backdoor attacks, where the honeypot module traps and absorbs the backdoor\nduring training, allowing the network to concentrate on the original tasks. The honeypot defense has demonstrated\nits effectiveness in substantially diminishing the ASR of word-level, sentence-level, style transfer, and syntactic\nattacks.Vaccine[ 158] proposes a proactive perturbation-aware alignment to mitigate possible harmful fine-tuning,\nthe core idea is to introduce crafted perturbations in embeddings during alignment, enabling the embeddings to\nwithstand adversarial perturbations in later fine-tuning phases. Zhu et al. [ 159] propose restricting PLMs’ adaption\nto the moderate-fitting stage to defend against backdoors. Specifically, it devises three training methods: reducing\nmodel capacity, training epochs, and learning rate, respectively; it is proven effective against word-level and\nsyntactic-level attacks.Anti-backdoor learning (ABL)[ 160] proposes training backdoor-free models on real-world\ndatasets, the two-stage mechanism first employs local gradient ascent loss (LGA) to separate backdoor examples\nfrom clean training samples, then uses global gradient ascent (GGA) to unlearn the backdoored model using the\nisolated backdoor.\n4.1.2. Detection & Filtering\nBackdoor Keyword Identification (BKI)[ 165] is a detection defense that aims to remove possible poisoned\ntraining data and directly obstruct backdoor training. This approach devises scoring functions to locate frequent\nsalient words in the trigger sentences that help to filter out poisoned data and sanitize the training dataset, it involves\ninspecting all the training data to identify possible trigger words.Simulate and Eliminate (SANDE)[ 161] integrates\nOverwrite Supervised Fine-tuning (OSFT) to its two-phase framework (simulation and elimination) to remove\nunknown backdoors. The key to this defense is to unlearn backdoor mapping, letting the model desensitize to\nthe trigger. Specifically, in the first scenario where the trigger pattern inserted is known, OSFT is used to remove\ncorresponding backdoor behavior. In the second scenario, where information about the trigger pattern is unknown,\nparrot prompts are optimized and leveraged to simulate the trigger’s behaviors in the simulation phase, followed\nup in the elimination phase, OSFT is reused on the parrot prompt to remove victim models’ inherent backdoor\nmappings form triggertto malicious response 𝑅𝑡. Lastly, the backdoor removal is extended to the most common\nscenario where neither trigger pattern nor triggered responses are known.\nMoreover,BEEAR[ 162] is another reactive mitigation defense method for removing backdoors in instruction-\ntuned language models. It proposes a bi-level optimization framework, where the inner level identifies universal\nperturbations to the decoder embedding that steer the model towards attack goals, and the outer level fine-tunes the\nmodel to reinforce safe behaviors against these perturbations.Poisoned Sample Identification Module (PSIM)[ 163]\nleverages PEFT to identify poisoned samples and defend against weight poisoning backdoor attacks. Specifically,\npoisoned samples are detected by extreme confidence in the inference phase.MDP[ 164] is another detection-\nbased method to defend PLMs against backdoor attacks. It leverages the difference between clean and poisoned\nsamples’ sensitivity to random masking, where the masking sensitivity is measured using few-shot learning data.\nSun et al. [125] propose a defense for backdoor attacks in NLG systems that combines detection and mitigation\nmethods. The defense is based on backward probability and effectively detects attacks at different levels across\nNLG tasks.\n4.1.3. Model Reconstruction & Repairment\nFine-tuning the backdoored model on clean data for extra epochs [166] is considered an effective model repair-\nment technique to overcome perturbations introduced by poisoning data.Adversarial Neuron Pruning (ANP)[ 167]\neliminates dormant backdoored weights introduced during the initial training phase to mitigate backdoors. Though\nfine-tuning can provide some degree of protection against backdoors, and the standalone pruning is also effective on\nsome deep neuron network backdoor attacks, the stronger pruning-aware attacks can evade pruning. Pruning is\nhttps://doi.org/10.53941/tai.2025.100003. 45\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\ntherefore advanced tofine-pruning[ 128], which combines fine-tuning [166] and pruning [123] to mitigate backdoor,\nfine-pruning aims to disable backdoor by removing neurons that are not primarily activated on clean inputs, followed\nby performing several rounds of fine-tuning with clean data.\nFine-mixing[ 170] leverages the clean pre-trained weights to mitigate backdoors from fine-tuned models, the\ntwo-step fine-mixing technique first mixes backdoored weights with clean weights, then fine-tunes the mixed\nweights on clean data, complementary with the Embedding Purification (E-PUR) technique that mitigates potential\nbackdoors in the word embeddings, making this defense especially effective against embedding poisoning-based\nbackdoor attacks.CleanCLIP[ 171] is a fine-tuning framework that mitigates data poisoning attacks in multimodal\ncontrastive learning. By independently re-aligning the representations for individual modalities, the learned\nrelationship introduced by the backdoor can be weakened. Furthermore, this framework has shown that supervised\nfinetuning (SFT) on the task-specific labeled image is effective for backdoor trigger removal from the vision encoder.\nShapPruning[ 168] is another pruning approach. It detects the triggered neurons to mitigate the backdoor in a\nfew-shot scenario and repair the poisoned model.\nTrap and Replace (T&R)[169] is similar to the aforementioned pruning-based methods, which also aims to\nremove backdoored neurons. However, instead of locating these neurons, a trap is set in the model to bait and\ntrap the backdoor. Wu et al. propose an approach calledMulti-Scale Low-Rank Adaptation (MuScleLoRA)[ 172] to\nacquire a clean language model from poisoned datasets by downscaling frequency space. Specifically, for models\ntrained on the poisoned dataset, MuScleLoRA freezes the model and inserts LoRA modules in each of the attention\nlayers, after which multiple radial scalings are conducted within the LoRA modules at the penultimate layer of the\ntarget model to downscale clean mapping, gradients are further aligned to the clean auxiliary data when updating\nparameters. This approach encourages the target poisoned language model to prioritize learning the high-frequency\nclean mapping to mitigate backdoor learning. Zhai et al. [173] propose a Noise-augmented Contrastive Learning\n(NCL) framework to defend against textual backdoor attacks by training a clean model from poisonous data. The\nkey approach of this model cleansing method is utilizing the noise-augment method and NCL loss to mitigate the\nmapping between triggers and target labels.Obliviate[ 57] proposes a defense method to neutralize task-agnostic\nbackdoors, which can especially be integrated into the PEFT process. The two-stage strategy involves amplifying\nbenign neurons in PEFT layers and regularizing attention scores to penalize the trigger tokens with extremely high\nattention scores.\n4.1.4. Distillation-Based Defenses\nKnowledge distillation is a method for transferring knowledge between models, enabling a lightweight student\nmodel to acquire the capabilities of a more powerful teacher model. Previous research [145] has proven defensive\ndistillation is one of the most promising defenses that defend neural networks against adversarial examples. Based on\nthis, knowledge distillation has been advanced and employed in detecting poison samples and disabling backdoors.\nAnti-Backdoor Model[ 174] introduces a non-invasive backdoor against backdoor (NBAB) algorithm that does not\nrequire reconstruction of the backdoored model. Specifically, this approach utilizes knowledge distillation to train a\nspecialized student model that only focuses on addressing backdoor tasks to mitigate their impacts on the teacher\nmodel. Bie et al. [175] present a backdoor elimination defense for pre-trained encoders utilizing self-supervised\nknowledge distillation, where both contrastive and non-contrastive self-supervised learning (SSL) methods are\nincorporated. In this approach, the teacher model is finetuned using the contrastive SSL method, which enables the\nstudent model to learn the knowledge of differentiation across all classes, followed by the student model trained\nusing the non-contrastive SSL method to learn consistency within the same class. In which, neural attention maps\nfacilitate the knowledge transfer between models. However, anti-distillation backdoor attacks [31] have exploited\nknowledge distillation to transfer backdoors between models.\n4.1.5. Other Pre-Training Defenses\nDecoupling[ 176] focuses on defending poisoning-based backdoor attacks on DNNs, it prevents the model from\npredicting poisoned samples as target labels. The original end-to-end training process is decoupled into three stages.\nThe whole model is first re-trained on unlabeled training samples via self-supervised learning, then by freezing the\nlearned feature extractor and using all training samples to train the remaining fully connected layers via supervised\ntraining. Subsequently, high-credible samples are filtered based on training loss. Lastly, these high-credible samples\nare adopted as labeled samples to fine-tune the model via semi-supervised training.I-BAU[ 177] is a defense\ninvolves model reconstruction. It addresses backdoor removal through a mini-max formulation and proposes the\nimplicit backdoor adversarial unlearning (I-BAU) algorithm that leverages implicit hyper-gradients as the solution.\nSpecifically, the formulation consists of the inner maximization problem and outer minimization problem, where\nhttps://doi.org/10.53941/tai.2025.100003. 46\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nthe inner maximization problem aims to find the trigger that maximizes prediction loss, and the outer minimization\nproblem aims to find parameters that minimize the adversarial loss from the inner attack.\nIn addition,FABE[ 178] presents a front-door adjustment defense for LLMs backdoor elimination based\non casual reasoning. It is architecturally founded on three modules: the first module is trained for sampling\nthe front-door variable, the second is trained for estimating the true causal effect, and the third searches for the\nfront-door variable. This defense has demonstrated its effectiveness against token, sentence, and syntactic-level\nbackdoor attacks.Decayed Contrastive Decoding[ 110] first proposes a black-box multi-turn distributed trigger\nattack framework called POISONSHARE, which employs a multi-turn greedy coordinate gradient descent to find\nthe optimal trigger, then presents the Decayed Contrastive Decoding defense to mitigate such distributed backdoor\nattacks. Specifically, it leverages the model’s internal late-layer representation as a form of contrasting guidance to\ncalibrate the output distribution, thereby preventing the generation of harmful responses.\nConclusion IV .A.Backdoor defenses deployed in the pre-training phase can be categorized into reactive\ndefenses and proactive defenses, reactive defenses involve detection and mitigation after the occurrence of poisoned\nexamples or the known existence of a backdoor, in which detection-based defenses in this phase include filtering the\ntraining instances and mitigation-based defenses involve alleviating the harmful effects brought by backdoor attacks,\nmodel repairment via tuning and pruning ( [ 128,166,168,170–173]) is one of the prevalent approaches. While\nproactive defenses like [156–158,160] serve preventive purposes, which aim to endow the model with robustness\nagainst potential backdoors. However, we found that many defense mechanisms only validate their effectiveness on\nsimpler text classification tasks, while more complex tasks like text generation are yet to be explored. Generalized\ndefensive capabilities across different tasks should be seen as important in future work.\n4.2. Post-Training Defenses\nIn the context of inference time defenses, no access to the training process of the model is required, nor any\nprior knowledge about the attacker and trigger, making them more realistic and efficient defense approaches in a\nblack-box setting. A detailed overview of backdoor defenses in the post-training phase can be referred to in Table 8.\nTable 8.An overview of post-training backdoor defenses for LLMs.\nDefense Defender’s\nKnowledge\nDefense Method Model Defended Trigger/Backdoor\nDetected\nAttacks Tackled\nONION [115] Black-box Reactive (detection) NLP models: BiL-\nSTM, BERT-T, BERT-\nF\nWord-level BadNet [6], BadNetm,\nBadNeth, RIPPLES\n[34], InSent [108]\nRAP [122] Black-box Reactive (detection) DNNs Word-level Word-level textual\nbackdoor attacks\nSTRIP-ViTA [112] Black-box Reactive (detection) LSTM Word-level Trojan attacks\nParaFuzz [183] Black-box Reactive (detection) NLP models Style-level, syntax-\nlevel\nStyle-backdoor, Hid-\nden Killer [ 49], Bad-\nnets [ 6], Embedding-\nPoisoning [37]\nCLEANGEN [181] Black-box Reactive (detection) Alpaca-7B, Alpaca-2-\n7B, Vicuna-7B\nSentence-level, single-\nturn, multi-turn\nAutoPoison [63], VPI\n[61], multi-turn [85]\nBDDR [119] Black-box Reactive (detection) BiLSTM, BERT Word-level, sentence-\nlevel\nTextual backdoor at-\ntacks\nHoneypots [157] White-box Proactive BERT, RoBERTa Word-level, sentence-\nlevel, style-level and\nsyntactic-level\nNLP backdoors:\nAddWord, AddSent,\nStyleBkd [ 109], Syn-\nBkd [49]\nMo et al. [146] Black-box Reactive Llama2-7b Lexical, sentence,\nstyle, syntactic-level\nBadnets [ 6], addSent\n[108], StyleBkd [109],\nSynBkd [124]\nChain-of-Scrutiny\n[196]\nBlack-box Reactive (mitigation) GPT-3.5, GPT-4,\nGemini-1.0-pro,\nLlama3\nToken-level LLM backdoor attacks\nLMSanitator [184] Black-box Reactive (detection) BERT, RoBERTa Word-level BToP [ 74], NeuBA\n[43], POR [190]\n4.2.1. Detection & Filtering\nInput detection is an effective way to identify and prevent the trigger-embedded inputs to defend against\nbackdoor attacks, the detection could be either based on perplexity or perturbations.ONION[ 115] is a simple\nfiltering-based defense designated for textual backdoor situations, it requires no access to the model’s training\nprocess and works in both pre-training and post-training stages. It is devised for detecting and removing tokens that\nreduce the fluency of an input sentence and are likely backdoor triggers, these outlier words are identified by the\nhttps://doi.org/10.53941/tai.2025.100003. 47\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nperplexity (PPL) score obtained from GPT-2 and the pre-defined threshold for suspicious score. This defense is\nproven effective for defending against word-level attacks. However, the perplexity-based defense is insufficient to\ndefend against sentence-level or syntactic-based attacks.\nSTRIP-ViTA[112]. A test-time detection defense framework that detects poisoned inputs with stable predictions\nunder perturbation.STRIP-ViTAdefense method is based on the previous workSTRIP[ 118] which works solely\non computer vision tasks, it is advanced to work on audio, video, and textual tasks. Its methodology includes\nsubstituting the most significant words in the inputs and examining the resulting prediction entropy distributions.\nRobustness-Aware Perturbations (RAP)[ 122] leverages the difference between the robustness of benign and\npoisoned inputs to perturbations and injects crafted perturbations into the given samples to detect poisoned samples.\nBDMMT[ 179] detects backdoored inputs for language models through model mutation test, it has demonstrated\neffectiveness in defending against character-level, word-level, sentence-level, and style-level backdoor attacks.\nFebruus[ 180] andSentiNet[ 182] operate as run-time Trojan anomaly detection methods for DNNs without requiring\nmodel retraining, they sanitize and restore inputs by removing the potential trigger applied on them. Activation\nClustering [126] detects and removes poisonous data by analyzing activations of the model’s last hidden layer.\nCLEANGEN[ 181] is a lightweight and effective decoding strategy in the post-training phase that mitigates backdoor\nattacks for generation tasks in LLMs. The approach is to identify commonly used suspicious tokens and replace\nthem with tokens generated by another clean LLM, thereby avoiding the generation of attacker-desired content.\nMo et al. [146] design a test-time defense against black-box backdoor attacks that leverages few-shot demon-\nstrations to correct the inference behavior of poisoned models.ParaFuzz[ 183] proposes a test-time interpretability-\ndriven poisoned sample detection technique for NLP models. It has demonstrated effectiveness against various types\nof backdoor triggers.Chain-of-Scrutiny[ 196] is another test-time detection defense for backdoor-compromised\nLLM. It only requires black-box access to the model. The intuitive of this defense is that backdoor attacks usually\nestablish a shortcut between trigger and desired output which lacks reasoning support, hence Chain-of-Scrutiny\nguides the model to generate detailed reasoning steps for the input to ensure consistency of final output, thus\neliminating backdoors.BDDR[ 119] defends against training data poisoning by analyzing whether input words\nchange the discriminative results of the model. The output probability-based defense uses two methods to eliminate\ntextual backdoors: either deleting them upon detection (DD) or replacing them with words generated by BERT (DR).\nLMSanitator[ 184] aims to detect and remove task-agnostic backdoors in prompt-tuning from Transformer-based\nmodels. The defense method erases triggers from poisoned inputs during the inference phase.\n4.2.2. Model Inspections\nNeural Cleanse (NC)[ 113] is an optimization-based detection and reconstruction system for DNN backdoor\nattacks during the model inspection stage to filter test-time input. In the detection stage, given a backdoored DNN,\nNC first detects backdoors by determining if any label requires much fewer perturbations to achieve misclassification.\nFollowed up by searching for potential trigger keywords in each of the classes that will move all the inputs from one\nclass to the target class. In the reconstruction stage, the trigger is reverse-engineered by solving the optimization\nproblem, which aims to achieve two objectives: finding the trigger leading to misclassification and finding the\ntrigger that only modifies a small range of clean images. WhileNC[ 113] relies on a clean training dataset which\nlimits its application scenarios,DeepInspect (DI) [ 185], another black-box Trojan detection framework through\nmodel inspection, requires minimal prior knowledge about the backdoored model, it first employs model inversion\nto obtain a substitution training dataset and reconstructs triggers using a conditional GAN, followed by anomaly\ndetection based on statistical hypothesis testing.Artificial Brain Stimulation (ABS)[ 186] is another analysis-based\nbackdoor detection approach, it scans an AI model and identifies backdoors by conducting a simulation analysis on\ninner neurons, followed by reverse engineering triggers using the results from stimulation analysis.\n4.2.3. Distillation-Based Defenses\nModel distillation [187] is another post-training defense against poisoning attacks. Via transferring knowledge\nfrom a large model to a smaller one, it aims to create a more robust and clean representation of underlying data\nto mitigate adversarial effects on backdoored pre-trained encoders.Neural Attention Distillation (NAD)[ 188] is\na distillation-guided fine-tuning approach that erases the backdoor from DNNs, it utilizes a teacher model to\nguide the fine-tuning of backdoored student model on clean data, to align its intermediate layer attention with the\nteacher model.\nConclusion IV .B.After deployment of the backdoored model, defenses in the post-training stage are considered\nreactive measures; the outlier detection-based methods including [112,115,119,122] are most frequently used as\nbaseline defenses in various backdoor attacks. However, we argue that the filtering methods that solely work in\nhttps://doi.org/10.53941/tai.2025.100003. 48\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nthe inference phase are not considered effective and generalizable. Considering a real-world scenario, it is more\npractical to implement a proactive defense mechanism from the model provider’s perspective, as the awareness of\nthe existence of the model backdoor is not considered realistic from a model user’s perspective.\n5. Evaluation Methodology\n5.1. Performance Metrics\nIn this section, we introduce the performance metrics commonly employed to assess the effectiveness of\nbackdoor attacks in achieving their dual objectives: efficacy and stealthiness. In addition, we include auxiliary\nmetrics utilized when implementing attacks and defenses.\n5.1.1. Main Metrics\nAttack Success Rate (ASR) is the measure of classification accuracy of the backdoored model on poisoned\ndata, it is the key indicator metric for evaluating the performance of backdoor attacks. In contrast, the drop in ASR\ncan be used to measure the effectiveness of defense methods. The ASR can be expressed as:\nASR= # Successfully Attacked Cases\n# Total Cases ×100%(1)\nThe clean performance shares equal importance with attack performance in backdoor attacks since one of\nthe objectives of the attack design is to maintain the overall model integrity. Clean accuracy (CA or CACC)\nmeasures how the backdoored model performs on the unpoisoned dataset to determine whether the model’s overall\nperformance is degraded. A larger CA indicates better utility preservation. CA is also referred to as “Benign\nAccuracy (BA)”.\nCA= # Clean Examples Correctly Classified\n# Total Clean Examples ×100%(2)\nThe Performance Drop Rate (PDR) is used to quantify the effectiveness of an attack and its capability of\npreserving model functionality. It is obtained by measuring how poisoned samples affect the model’s performance\ncompared to benign ones. An effective attack should attain large PDRs for poisoned samples and small PDRs for\nclean samples. PDR is defined as:\nPDR= (1− Accpoisoned\nAccclean\n)×100%(3)\nwhere the Accpoisoned refers to accuracy when the model is tuned on poisoned data and Accclean refers to accuracy\nwhen the model is tuned on clean data.\nLabel Flip Rate (LFR) quantifies the percentage of cases whose ground truth labels are flipped or modified,\nwhich implies success in misleading the LLM to make incorrect predictions upon triggered inputs. It can thus be\nused to evaluate attack efficacy. It is defined as the proportion of misclassified samples:\nLFR= #+ve Samples Classified as−ve\n#+ve Samples ×100%(4)\n5.1.2. Auxiliary Metrics\nPerplexity [197] measures the readability and fluency of text samples using the language model. A lower\nperplexity score indicates that the sample is more fluent and predictable by the model. In contrast, a higher perplexity\nsuggests that the model is less certain about the sample, making it more likely to be identified as the backdoor trigger\nand filtered by perplexity-based backdoor defenses such asONION[ 115]. The perplexity score can be utilized to\ndevise stealthy backdoor triggers or detect backdoor samples when defending against backdoor attacks.\nBLEU and ROUGH are two frequently used metrics in NLP evaluation, and have now been extended to\nevaluate model performance in triggerless scenarios under backdoor attacks. BLEU [198] is primarily based on\nprecision; it measures the accuracy of benign examples. ROUGE [199] which is primarily based on recall, evaluates\nresponse quality in the absence of triggers. A higher BLEU score indicates a more accurate response compared to\nthe ground truth text, while a higher ROUGE score represents a better quality of responses to triggerless input.\nhttps://doi.org/10.53941/tai.2025.100003. 49\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nExact Match (EM) and Contain are two metrics for evaluating NLP tasks, such as answering questions and\ngenerating texts. EM is a binary evaluation metric that measures whether an output exactly matches the ground truth\nor target output; the contain metric determines whether the output contains the target string.\n5.2. Baselines, Benchmarks, and Datasets\nBesides directly evaluating attack and defense performance using the metrics mentioned above and comparing\nthem with representative baseline attacks and defenses, their efficacy and, especially, robustness can also be evaluated\nthrough their performance when applying state-of-the-art defense methods. An effective attack should be able to\ncircumvent defenses, contrarily an effective defense should be able to obstruct attacks. As detailed in section IV .B,\nONION[ 115],STRIP-ViTA[ 112], andRAP[ 122] are three of the most representative test-time defenses utilized in\nmitigating LLM backdoor attacks; they share a similar defense technique of preventive input filtering.\nLi et al. [ 200] provide a comprehensive threat model benchmark for backdoor instruction-tuned LLMs.\nThe attack scenario assumes full white-box access, enabling adversaries to manipulate training data, model\nparameters, and the training process. Specifically, the framework encompasses four distinct attack strategies: data\npoisoning [61,64,77], weight poisoning [33], hidden state manipulation, and chain-of-thought attacks [93]. This\nrepository provides a standardized training pipeline for implementing various LLM backdoor attacks and assessing\ntheir effectiveness and limitations, It helps to facilitate research work in the field of LLM backdoor attacks. We list\nsome commonly used datasets for implementing or evaluating backdoor attacks (refer to Table 9).\nTable 9.Frequently used evaluation datasets.\nDataset Size Description & Usage\nSST-2 [201] 12K Movie reviews for single-sentence sentiment classification\nHateSpeech (HS) [202] 10K Hate speeches for single-sentence binary classification (HATE/NOHATE)\nAGNews [203] 128K News topics for single-sentence sentiment classification\nIMDB [204] 50K Movie reviews for single-sentence sentiment classification\nUltrachat-200k [205] 1.5M High-quality multi-turn dialogues for multi-turn instruction tuning\nAdvBench [27] 500 Questions covering prohibited topics for safety evaluation\nTDC 2023 50 Instructions representative of undesirable behaviors for safety evaluation\nToxiGen [206] 274K Machine-generated implicit hate speech dataset for hate speech detection\nBot Adversarial Dialogue [207] 70K Multi-turn dialogues between human and bot to trigger toxic responses generation\nAlpacaeval [208] 20K Instruction-label pairs for evaluating instruction-following language models\n6. Conclusions\nIn conclusion, this work comprehensively surveys existing backdoor attacks targeting large language models\n(LLMs), systematically categorizing them based on the exploitation phase. Alongside this, we explored correspond-\ning defense mechanisms designed to mitigate these backdoor threats, highlighting the current state of research and\nits limitations. By offering a well-structured taxonomy of existing methods, we aim to bridge gaps in understanding\nand encourage the development of innovative approaches to safeguard LLMs. We hope this survey serves as a\nvaluable resource for researchers and practitioners, fostering future advancements in creating more secure and\ntrustworthy LLM systems.\nAuthor Contributions\nY .Z.: Literature research, writing, and revision. T.N.: Literature research, writing, and revision. W.-B.L.:\nsupervision. Q.Z.: supervision. All authors have read and agreed to the published version of the manuscript.\nFunding\nThis work was fully supported by the Research Grants Council of Hong Kong (RGC) under Grants C1029-22G\nand in part by the Innovation and Technology Commission of Hong Kong (ITC) under Mainland-Hong Kong Joint\nFunding Scheme (MHKJFS) MHP/135/23.\nConflicts of Interest\nThe authors declare no conflict of interest. Any opinions, findings, and conclusions in this paper are those of\nthe authors and are not necessarily of the supported organizations.\nhttps://doi.org/10.53941/tai.2025.100003. 50\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nReferences\n1. Wu, S.; Irsoy, O.; Lu, S.; et al. Bloomberggpt: A Large Language Model for Finance.arXiv2023, arXiv:2303.17564.\n2. Loukas, L.; Stogiannidis, I.; Diamantopoulos, O.; et al. Making llms worth every penny: Resource-limited text classification\nin banking. In Proceedings of the Fourth ACM International Conference on AI in Finance, New York, NY , USA, 25\nNovember 2023; pp. 392–400. https://doi.org/10.1145/3604237.3626891.\n3. Jin, Y .; Chandra, M.; Verma, G.; et al. Better to ask in english: Cross-lingual evaluation of large language models for\nhealthcare queries. In Proceedings of the ACM Web Conference 2024, New York, NY , USA, 13 May 2024; pp. 2627–2638.\nhttps://doi.org/10.1145/3589334.3645643.\n4. Cui, J.; Ning, M.; Li, Z.; et al. Chatlaw: A multi-agent collaborative legal assistant with knowledge graph enhanced\nmixture-of-experts large language model.arXiv2024, arXiv:2306.16092.\n5. Mahari, R.Z. Autolaw: Augmented legal reasoning through legal precedent prediction.arXiv2021, arXiv:2106.16034.\n6. Gu, T.; Dolan-Gavitt, B.; Garg, S. Badnets: Identifying vulnerabilities in the machine learning model supply chain.arXiv\n2019, arXiv:1708.06733.\n7. Papernot, N.; McDaniel, P.; Sinha, A.; et al. Sok: Security and privacy in machine learning. In Proceedings of the 2018\nIEEE European Symposium on Security and Privacy (EuroS&P), London, UK, 24–26 April 2018.\n8. Shanahan, M. Talking about large language models.Commun. ACM2024,67, 68–79.\n9. Choi, S.; Mohaisen, D. Attributing chatgpt-generated source codes.IEEE Trans. Dependable Secur. Comput.2025, 1–14.\n10. Jiang, A.Q.; Sablayrolles, A.; Mensch, A.; et al. Mistral 7b.arXiv2023, arXiv:2310.06825.\n11. Jiang, A.Q.; Sablayrolles, A.; Mensch, A.; et al. Mixtral of experts.arXiv2024, arXiv:2401.04088.\n12. Achiam, J.; Adler, S.; Agarwal, S.; et al. Gpt-4 technical report.arXiv2023, arXiv:2303.08774.\n13. Brown, T.B.; Mann, B.; Ryder, N.; et al. Language models are few-shot learners.Adv. Neural Inf. Process. Syst.2020,33,\n1877–1901.\n14. Wang, B.; Komatsuzaki, A. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. 2021. Available online:\nhttps://github. com/kingoflolz/mesh-transformer-jax (accessed on 6 May 2025).\n15. Radford, A.; Wu, J.; Child, R.; et al. Language models are unsupervised multitask learners.OpenAI Blog2019,1, 9.\n16. Touvron, H.; Lavril, T.; Izacard, G.; et al. Llama: Open and efficient foundation language models.arXiv2023,\narXiv:2302.13971.\n17. Liu, H.; Li, C.; Wu, Q.; et al. Visual instruction tuning.Adv. Neural Inf. Process. Syst.2024,36, 34892–34916.\n18. Taori, R.; Gulrajani, I.; Zhang, T.; et al. Stanford alpaca: An instruction-following llama model. 2023.\n19. Chiang, W.-L.; Li, Z.; Lin, Z.; et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.Blog\n2023,3, 5.\n20. Zhang, P.; Zeng, G.; Wang, T.; et al. Tinyllama: An open-source small language model.arXiv2024, arXiv:2401.02385.\n21. Dettmers, T.; Pagnoni, A.; Holtzman, A.; et al. Qlora: Efficient finetuning of quantized llms.Adv. Neural Inf. Process. Syst.\n2024,36, 10088–10115.\n22. Raffel, C.; Shazeer, N.; Roberts, A.; et al. Exploring the limits of transfer learning with a unified text-to-text transformer.J.\nMach. Learn. Res.2020,21, 1–67.\n23. The Claude 3 Model Family: Opus, Sonnet, Haiku. Available Online: https://api.semanticscholar.org/CorpusID:268232499\n(accessed on 1 December 2024).\n24. Zhang, S.; Roller, S.; Goyal, N.; et al. Opt: Open pre-trained transformer language models.arXiv2022, arXiv:2205.01068.\n25. Anil, R.; Dai, A.M.; Firat, O.; et al. Palm 2 technical report.arXiv2023, arXiv:2305.10403.\n26. Goodfellow, I.J.; Shlens, J.; Szegedy, C. Explaining and harnessing adversarial examples.arXiv2015, arXiv:1412.6572.\n27. Zou, A.; Wang, Z.; Carlini, N.; et al. Universal and transferable adversarial attacks on aligned language models.arXiv\n2023, arXiv:2307.15043.\n28. Hayase, J.; Borevkovic, E.; Carlini, N.; et al. Query-based adversarial prompt generation.arXiv2024, arXiv:2402.12329.\n29. Shin, T.; Razeghi, Y .; Logan, I.V .; et al. Autoprompt: Eliciting knowledge from language models with automatically\ngenerated prompts.arXiv2020, arXiv:2010.15980.\n30. Wichers, N.; Denison, C.; Beirami, A. Gradient-based language model red teaming.arXiv2024, arXiv:2401.16656.\n31. Cheng, P.; Wu, Z.; Ju, T.; et al. Transferring backdoors between large language models by knowledge distillation.arXiv\n2024, arXiv:2408.09878.\n32. Zhao, S.; Gan, L.; Guo, Z.; et al. Weak-to-strong backdoor attack for large language models.arXiv2024, arXiv:2409.17946.\n33. Li, Y .; Li, T.; Chen, K.; et al. Badedit: Backdooring large language models by model editing.arXiv2024, arXiv:2403.13355.\n34. Kurita, K.; Michel, P.; Neubig, G. Weight poisoning attacks on pre-trained models.arXiv2020, arXiv:2004.06660.\n35. Qiu, J.; Ma, X.; Zhang, Z.; et al. Megen: Generative backdoor in large language models via model editing.arXiv2024,\narXiv:2408.10722.\n36. Yoo, K.Y .; Kwak, N. Backdoor attacks in federated learning by rare embeddings and gradient ensembling. In Proceedings\nof the 2022 Conference on Empirical Methods in Natural Language Processing, Abu Dhabi, UAE, 7–11 December 2022;\npp. 72–88.\nhttps://doi.org/10.53941/tai.2025.100003. 51\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\n37. Yang, W.; Li, L.; Zhang, Z.; et al. Be careful about poisoned word embeddings: Exploring the vulnerability of the\nembedding layers in nlp models.arXiv2021, arXiv:2103.15543.\n38. Li, L.; Song, D.; Li, X.; et al. Backdoor attacks on pre-trained models by layerwise weight poisoning.arXiv2021,\narXiv:2108.13888.\n39. Mei, K.; Li, Z.; Wang, Z.; et al. NOTABLE: Transferable backdoor attacks against prompt-based NLP models. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers),\nToronto, ON, Canada, 9–14 July 2023; pp. 15551–15565.\n40. Bagdasaryan, E.; Shmatikov, V . Blind backdoors in deep learning models.arXiv2021, arXiv:2005.03823.\n41. Miah, A.A.; Bi, Y . Exploiting the vulnerability of large language models via defense-aware architectural backdoor.arXiv\n2024, arXiv:2409.01952.\n42. Wang, H.; Shu, K. Trojan activation attack: Red-teaming large language models using activation steering for safety-\nalignment.arXiv2024, arXiv:2311.09433.\n43. Zhang, Z.; Xiao, G.; Li, Y .; et al. Red alarm for pre-trained models: Universal vulnerability to neuron-level backdoor\nattacks.Mach. Intell. Res.2023,20, 180–193. http://dx.doi.org/10.1007/s11633-022-1377-5.\n44. Li, J.; Yang, Y .; Wu, Z.; et al. Chatgpt as an attack tool: Stealthy textual backdoor attack via blackbox generative model\ntrigger.arXiv2023, arXiv:2304.14475.\n45. Tan, Z.; Chen, Q.; Huang, Y .; et al. Target: Template-transferable backdoor attack against prompt-based nlp models via\ngpt4.arXiv2023, arXiv:2311.17429.\n46. You, W.; Hammoudeh, Z.; Lowd, D. Large language models are better adversaries: Exploring generative clean-label\nbackdoor attacks against text classifiers. In Proceedings of the Findings of the Association for Computational Linguistics:\nEMNLP 2023, Singapore, 6–10 December 2023; pp. 12499–12527.\n47. Yan, S.; Wang, S.; Duan, Y .; et al. An llm-assisted easy-to-trigger backdoor attack on code completion models: Injecting\ndisguised vulnerabilities against strong detection. In Proceedings of the 33rd USENIX Security Symposium (USENIX\nSecurity 24), Philadelphia, PA, USA, 14–16 August 2024; pp. 1795–1812.\n48. Zeng, Q.; Jin, M.; Yu, Q.; et al. Uncertainty is fragile: Manipulating uncertainty in large language models.arXiv2024,\narXiv:2407.11282.\n49. Qi, F.; Li, M.; Chen, Y .; et al. Hidden killer: Invisible textual backdoor attacks with syntactic trigger. In Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (V olume 1: Long Papers), Association for Computational Linguistics, Virtual Event, 1–6\nAugust 2021; pp. 443–453.\n50. Cheng, P.; Du, W.; Wu, Z.; et al. Synghost: Imperceptible and universal task-agnostic backdoor attack in pre-trained\nlanguage models.arXiv2024, arXiv:2402.18945.\n51. Sheng, X.; Li, Z.; Han, Z.; et al. Punctuation matters! stealthy backdoor attack for language models.arXiv2023,\narXiv:2312.15867.\n52. He, J.; Jiang, W.; Hou, G.; et al. Watch out for your guidance on generation! exploring conditional backdoor attacks against\nlarge language models.arXiv2024, arXiv:2404.14795.\n53. Hu, E.J.; Shen, Y .; Wallis, P.; et al. Lora: Low-rank adaptation of large language models.arXiv2021, arXiv:2106.09685.\n54. Dong, T.; Xue, M.; Chen, G.; et al. The philosopher’s stone: Trojaning plugins of large language models.arXiv2024,\narXiv:2312.00374.\n55. Gu, N.; Fu, P.; Liu, X.; et al. A gradient control method for backdoor attacks on parameter-efficient tuning. In Proceedings\nof the 61st Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers), Toronto, ON,\nCanada, 9–14 July 2023; pp. 3508–3520.\n56. Cao, Y .; Cao, B.; Chen, J. Stealthy and persistent unalignment on large language models via backdoor injections.arXiv\n2024, arXiv:2312.00027.\n57. Kim, J.; Song, M.; Na, S.H.; et al. Obliviate: Neutralizing task-agnostic backdoors within the parameter-efficient fine-tuning\nparadigm.arXiv2024, arXiv:2409.14119.\n58. Jiang, S.; Kadhe, S.R.; Zhou, Y .; et al. Turning generative models degenerate: The power of data poisoning attacks.arXiv\n2024, arXiv:2407.12281.\n59. Liu, H.; Liu, Z.; Tang, R.; et al. Lora-as-an-attack! piercing llm safety under the share-and-play scenario.arXiv2024,\narXiv:2403.00108.\n60. Huang, H.; Zhao, Z.; Backes, M.; et al. Composite backdoor attacks against large language models. In Proceedings of the\nFindings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, 16–21 June 2024; pp.\n1459–1472.\n61. Yan, J.; Yadav, V .; Li, S.; et al. Backdooring instruction-tuned large language models with virtual prompt injection.arXiv\n2023, arXiv:2307.16888.\n62. Qiang, Y .; Zhou, X.; Zade, S.Z.; et al. Learning to poison large language models during instruction tuning.arXiv2024,\narXiv:2402.13459.\n63. Shu, M.; Wang, J.; Zhu, C.; et al. On the exploitability of instruction tuning.arXiv2023, arXiv:2306.17194.\nhttps://doi.org/10.53941/tai.2025.100003. 52\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\n64. Xu, J.; Ma, M.D.; Wang, F.; et al. Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large\nlanguage models.arXiv2024, arXiv:2305.14710.\n65. Liang, J.; Liang, S.; Luo, M.; et al. Vl-trojan: Multimodal instruction backdoor attacks against autoregressive visual\nlanguage models.arXiv2024, arXiv:2402.13851.\n66. Wan, A.; Wallace, E.; Shen, S.; et al. Poisoning language models during instruction tuning. In Proceedings of the\nInternational Conference on Machine Learning, Honolulu, HI, USA, 23–29 July 2023; pp. 35413–35425.\n67. Ni, Z.; Ye, R.; Wei, Y .; et al. Physical backdoor attack can jeopardize driving with vision-large-language models.arXiv\n2024, arXiv:2404.12916.\n68. Choe, M.; Park, C.; Seo, C.; et al. Sdba: A stealthy and long-lasting durable backdoor attack in federated learning.arXiv\n2024, arXiv:2409.14805.\n69. Ye, R.; Chai, J.; Liu, X.; et al. Emerging safety attack and defense in federated instruction tuning of large language models.\narXiv2024, arXiv:2406.10630.\n70. Zhang, Z.; Panda, A.; Song, L.; et al. Neurotoxin: Durable backdoors in federated learning.arXiv2022, arXiv:2206.10341.\n71. Zhang, J.; Chi, J.; Li, Z.; et al. Badmerging: Backdoor attacks against model merging.arXiv2024, arXiv:2408.07362.\n72. Du, W.; Zhao, Y .; Li, B.; et al. Ppt: Backdoor attacks on pre-trained models via poisoned prompt tuning. In Proceedings of\nthe Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, Vienna, Austria, 23–29 July 2022; pp.\n680–686. https://doi.org/10.24963/ijcai.2022/96.\n73. Yao, H.; Lou, J.; Qin, Z. Poisonprompt: Backdoor attack on prompt-based large language models.arXiv2023,\narXiv:2310.12439.\n74. Xu, L.; Chen, Y .; Cui, G.; et al. Exploring the universal vulnerability of prompt-based learning paradigm.arXiv2022,\narXiv:2204.05239.\n75. Cai, X.; Xu, H.; Xu, S.; et al. Badprompt: Backdoor attacks on continuous prompts.arXiv2022, arXiv:2211.14719.\n76. Zhao, S.; Wen, J.; Luu, A.; et al. Prompt as triggers for backdoor attack: Examining the vulnerability in language models.\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore, 6–10 December\n2023; pp. 12303–12317.\n77. Rando, J.; Tram `er, F. Universal jailbreak backdoors from poisoned human feedback.arXiv2024, arXiv:2311.14455.\n78. Wang, J.; Wu, J.; Chen, M.; et al. Rlhfpoison: Reward poisoning attack for reinforcement learning with human feedback in\nlarge language models.arXiv2024, arXiv:2311.09641.\n79. Baumg¨artner, T.; Gao, Y .; Alon, D.; et al. Best-of-venom: Attacking rlhf by injecting poisoned preference data.arXiv2024,\narXiv:2404.05530.\n80. Shi, J.; Liu, Y .; Zhou, P.; Sun, L. Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt.\narXiv2023, arXiv:2304.12298.\n81. Carlini, N.; Nasr, M.; Choquette-Choo, C.A.; et al. Are aligned neural networks adversarially aligned?arXiv2024,\narXiv:2306.15447.\n82. Wang, Y .; Xue, D.; Zhang, S.; et al. Badagent: Inserting and activating backdoor attacks in llm agents.arXiv2024,\narXiv:2406.03007.\n83. Wang, H.; Zhong, R.; Wen, J.; et al. Adaptivebackdoor: Backdoored language model agents that detect human overseers.\nIn Proceedings of the ICML 2024 Workshop on Foundation Models in the Wild, Vienna, Austria, 25 July 2024.\n84. Chen, B.; Ivanov, N.; Wang, G.; et al. Multi-turn hidden backdoor in large language model-powered chatbot models. In\nProceedings of the 19th ACM Asia Conference on Computer and Communications Security, New York, NY , USA, 1–5\nJuly 2024; pp. 1316–1330. https://doi.org/10.1145/3634737.3656289.\n85. Hao, Y .; Yang, W.; Lin, Y . Exploring backdoor vulnerabilities of chat models.arXiv2024, arXiv:2404.02406.\n86. Yang, W.; Bi, X.; Lin, Y .; et al. Watch out for your agents! investigating backdoor threats to llm-based agents.arXiv2024,\narXiv:2402.11208.\n87. Schuster, R.; Song, C.; Tromer, E.; et al. You autocomplete me: Poisoning vulnerabilities in neural code completion. In\nProceedings of the 30th USENIX Security Symposium (USENIX Security 21), Online, 11–13 August 2021; pp. 1559–1575.\n88. Liu, D.; Zhang, S. Alanca: Active learning guided adversarial attacks for code comprehension on diverse pre-trained and\nlarge language models. In Proceedings of the 2024 IEEE International Conference on Software Analysis, Evolution and\nReengineering (SANER), Rovaniemi, Finland, 12–15 March 2024; pp. 602–613.\n89. Aghakhani, H.; Dai, W.; Manoel, A.; et al. Trojanpuzzle: Covertly poisoning code-suggestion models. In Proceedings of\nthe 2024 IEEE Symposium on Security and Privacy (SP), San Francisco, CA, USA, 19–23 May 2024; pp. 1122–1140.\n90. Li, Y .; Liu, S.; Chen, K.; et al. Multi-target backdoor attacks for code pre-trained models.arXiv2023arXiv:2306.08350.\n91. Zhang, R.; Li, H.; Wen, R.; et al. Instruction backdoor attacks against customized LLMs. In Proceedings of the 33rd\nUSENIX Security Symposium (USENIX Security 24), Philadelphia, PA, USA, 14–16 August 2024; pp. 1849–1866.\n92. Xue, J.; Zheng, M.; Hua, T.; et al. Trojllm: A black-box trojan prompt attack on large language models.arXiv2023,\narXiv:2306.06815.\n93. Xiang, Z.; Jiang, F.; Xiong, Z.; et al. Badchain: Backdoor chain-of-thought prompting for large language models.arXiv\n2024, arXiv:2401.12242.\nhttps://doi.org/10.53941/tai.2025.100003. 53\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\n94. Chen, B.; Guo, H.; Wang, G.; et al. The dark side of human feedback: Poisoning large language models via user inputs.\narXiv2024, arXiv:2409.00787.\n95. Zhang, Q.; Zeng, B.; Zhou, C.; et al. Human-imperceptible retrieval poisoning attacks in llm-powered applications.arXiv\n2024, arXiv:2404.17196.\n96. Zou, W.; Geng, R.; Wang, B.; et al. Poisonedrag: Knowledge corruption attacks to retrieval-augmented generation of large\nlanguage models.arXiv2024, arXiv:2402.07867.\n97. Jiao, R.; Xie, S.; Yue, J.; et al. Can we trust embodied agents? exploring backdoor attacks against embodied llm-based\ndecision-making systems.arXiv2024, arXiv:2405.20774.\n98. Xue, J.; Zheng, M.; Hu, Y .; et al. Badrag: Identifying vulnerabilities in retrieval augmented generation of large language\nmodels.arXiv2024, arXiv:2406.00083.\n99. Cheng, P.; Ding, Y .; Ju, T.; et al. Trojanrag: Retrieval-augmented generation can be backdoor driver in large language\nmodels.arXiv2024, arXiv:2405.13401.\n100. Long, Q.; Deng, Y .; Gan, L.; et al. Backdoor attacks on dense passage retrievers for disseminating misinformation.arXiv\n2024, arXiv:2402.13532.\n101. Huang, Y .; Zhuo, T.Y .; Xu, Q.; et al. Training-free lexical backdoor attacks on language models. In Proceedings of the ACM\nWeb Conference 2023, Austin, TX, USA, 30 April 2023; pp. 2198–2208. http://dx.doi.org/10.1145/3543507.3583348.\n102. Chen, Z.; Xiang, Z.; Xiao, C.; et al. Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases.\narXiv2024, arXiv:2407.12784.\n103. Kandpal, N.; Jagielski, M.; Tram`er, F.; et al. Backdoor attacks for in-context learning with language models.arXiv2023,\narXiv:2307.14692.\n104. Zhao, S.; Jia, M.; Tuan, L.A.; et al. Universal vulnerabilities in large language models: Backdoor attacks for in-context\nlearning.arXiv2024, arXiv:2401.05949.\n105. He, P.; Xu, H.; Xing, Y .; et al. Data poisoning for in-context learning.arXiv2024, arXiv:2402.02160.\n106. Lu, D.; Pang, T.; Du, C.; et al. Test-time backdoor attacks on multimodal large language models.arXiv2024,\narXiv:2402.08577.\n107. Sun, L. Natural backdoor attack on text data.arXiv2021, arXiv:2006.16176.\n108. Dai, J.; Chen, C.; Li, Y . A backdoor attack against lstm-based text classification systems.IEEE Access2019,7,\n138872–138878.\n109. Qi, F.; Chen, Y .; Zhang, X.; et al. Mind the style of text! adversarial and backdoor attacks based on text style transfer.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Punta Cana, Rep´ublica\nDominicana, 7–9 November 2021; pp. 4569–4580.\n110. Tong, T.; Xu, J.; Liu, Q.; et al. Securing multi-turn conversational language models from distributed backdoor triggers.\narXiv2021, arXiv:2407.04151.\n111. Zhang, X.; Zhang, Z.; Ji, S.; et al. Trojaning language models for fun and profit. In Proceedings of the 2021 IEEE European\nSymposium on Security and Privacy (EuroS&P), Vienna, Austria, 6–10 September 2021; pp. 179–197.\n112. Gao, Y .; Kim, Y .; Doan, B.G.; et al. Design and evaluation of a multi-domain trojan detection method on deep neural\nnetworks.IEEE Trans. Dependable Secur. Comput.2021,19, 2349–2364.\n113. Wang, B.; Yao, Y .; Shan, S.; et al. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In\nProceedings of the 2019 IEEE Symposium on Security and Privacy (SP), San Francisco, CA, USA, 19–23 May 2019; pp.\n707–723.\n114. Chen, K.; Meng, Y .; Sun, X.; et al. Badpre: Task-agnostic backdoor attacks to pre-trained nlp foundation models.arXiv\n2021, arXiv:2110.02467.\n115. Qi, F.; Chen, Y .; Li, M.; et al. Onion: A simple and effective defense against textual backdoor attacks.arXiv2021,\narXiv:2011.10369.\n116. Wen, Y .; Jain, N.; Kirchenbauer, J.; et al. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning\nand discovery.Adv. Neural Inf. Process. Syst.2023,36, 51008–51025.\n117. Guo, C.; Sablayrolles, A.; J ´egou, H.; et al. Gradient-based adversarial attacks against text transformers.arXiv2021,\narXiv:2104.13733.\n118. Gao, Y .; Xu, C.; Wang, D.; et al. Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the\n35th Annual Computer Security Applications Conference, New York, NY , USA, 9–13 December 2019; pp. 113–125.\nhttps://doi.org/10.1145/3359789.3359790.\n119. Shao, K.; Yang, J.; Ai, Y .; et al. Bddr: An effective defense against textual backdoor attacks.Comput. Secur.2021,\n110, 102433.\n120. Perez, E.; Huang, S.; Song, F.; et al. Red teaming language models with language models.arXiv2022, arXiv:2202.03286.\n121. Luo, Y .; Yang, Z.; Meng, F.; et al. An empirical study of catastrophic forgetting in large language models during continual\nfine-tuning.arXiv2024, arXiv:2308.08747.\n122. Yang, W.; Lin, Y .; Li, P.; et al. RAP: Robustness-Aware Perturbations for defending against backdoor attacks on NLP\nmodels. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Punta Cana,\nhttps://doi.org/10.53941/tai.2025.100003. 54\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\nDominican Republic, 7–11 November 2021; pp. 8365–8381.\n123. Sun, M.; Liu, Z.; Bair, A.; et al. A simple and effective pruning approach for large language models.arXiv2024,\narXiv:2306.11695.\n124. Li, S.; Liu, H.; Dong, T.; et al. Hidden backdoors in human-centric language models.arXiv2021, arXiv:2105.00164.\n125. Sun, X.; Li, X.; Meng, Y .; et al. Defending against backdoor attacks in natural language generation.Proc. AAAI Conf. Artif.\nIntell.2023,37, 5257–5265.\n126. Chen, B.; Carvalho, W.; Baracaldo, N.; et al. Detecting backdoor attacks on deep neural networks by activation clustering.\narXiv2018, arXiv:1811.03728.\n127. Tran, B.; Li, J.; Madry, A. Spectral signatures in backdoor attacks.Adv. Neural Inf. Process. Syst.2018,2018, 31.\n128. Liu, K.; Dolan-Gavitt, B.; Garg, S. Fine-pruning: Defending against backdooring attacks on deep neural networks. In\nInternational Symposium on Research in Attacks, Intrusions, and Defenses; Springer International Publishing: Cham,\nSwitzerland 2018; pp. 273–294.\n129. Blanchard, P.; Mhamdi, E.M.E.; Guerraoui, R.; et al. Machine learning with adversaries: Byzantine tolerant gradient descent.\nIn Proceedings of the Advances in Neural Information Processing Systems, Long Beach, CA, USA, 4–9 December 2017.\n130. Sun, Z.; Kairouz, P.; Suresh, A.T.; et al. Can you really backdoor federated learning?arXiv2019, arXiv:1911.07963.\n131. Nguyen, T.D.; Rieger, P.; Viti, R.D.; et al.{FLAME}: Taming backdoors in federated learning. In Proceedings of the 31st\nUSENIX Security Symposium (USENIX Security 22), Boston, MA, USA, 10–12 August 2022; pp. 1415–1432.\n132. Jones, E.; Dragan, A.; Raghunathan, A.; et al. Automatically auditing large language models via discrete optimization.\nIn Proceedings of the International Conference on Machine Learning, Honolulu, HI, USA on 23–29 July 2023; pp.\n15307–15329.\n133. Qi, F.; Yao, Y .; Xu, S.; et al. Turn the combination lock: Learnable textual backdoor attacks via word substitution. In\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (V olume 1: Long Papers), Virtual Event, 1–6 August 2021; pp. 4873–4883.\n134. Chen, X.; Dong, Y .; Sun, Z.; et al. Kallima: A clean-label framework for textual backdoor attacks. In European Symposium\non Research in Computer Security; Springer: Berlin, Germany, 2022; pp. 447–466.\n135. Gan, L.; Li, J.; Zhang, T.; et al. Triggerless backdoor attack for nlp tasks with clean labels.arXiv2021, arXiv:2111.07970.\n136. Iyyer, M.; Wieting, J.; Gimpel, K.; et al. Adversarial example generation with syntactically controlled paraphrase networks.\narXiv2018, arXiv:1804.06059.\n137. Wei, J.; Bosma, M.; Zhao, V .Y .; et al. Finetuned language models are zero-shot learners.arXiv2022, arXiv:2109.01652.\n138. Bai, Y .; Jones, A.; Ndousse, K.; et al. Training a helpful and harmless assistant with reinforcement learning from human\nfeedback.arXiv2022, arXiv:2204.05862.\n139. Feng, Z.; Guo, D.; Tang, D.; et al. CodeBERT: A pre-trained model for programming and natural languages. In Proceedings\nof the Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16–20 November 2020; pp.\n1536–1547. Available Online: https://aclanthology.org/2020.findings-emnlp.139 (accessed on 1 December 2024).\n140. Guo, D.; Ren, S.; Lu, S.; et al. Graphcodebert: Pre-training code representations with data flow.arXiv2020,\narXiv:2009.08366.\n141. Ahmad, W.U.; Chakraborty, S.; Ray, B.; et al. Unified pre-training for program understanding and generation.arXiv2021,\narXiv:2103.06333.\n142. Wang, Y .; Wang, W.; Joty, S.; et al. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code\nunderstanding and generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, Virtual, 7–11 November 2021; pp. 8696–8708. Available Online: https://aclanthology.org/2021.emnlp-\nmain.685 (accessed on 1 December 2024).\n143. Nijkamp, E.; Pang, B.; Hayashi, H.; et al. Codegen: An open large language model for code with multi-turn program\nsynthesis.arXiv2023, arXiv:2203.13474.\n144. Chen, M.; Tworek, J.; Jun, H.; et al. Evaluating large language models trained on code.arXiv2021, arXiv:2107.03374.\n145. Papernot, N.; McDaniel, P.; Wu, X.; et al. Distillation as a defense to adversarial perturbations against deep neural networks.\nIn Proceedings of the 2016 IEEE Symposium on Security and Privacy (SP), San Jose, CA, USA, 22–26 May 2016; pp.\n582–597.\n146. Mo, W.; Xu, J.; Liu, Q.; et al. Test-time backdoor mitigation for black-box large language models with defensive\ndemonstrations.arXiv2023, arXiv:2311.09763.\n147. Min, S.; Lyu, X.; Holtzman, A.; et al. Rethinking the role of demonstrations: What makes in-context learning work?arXiv\n2022, arXiv:2202.12837.\n148. Zhong, Z.; Huang, Z.; Wettig, A. Poisoning retrieval corpora by injecting adversarial passages.arXiv2023,\narXiv:2310.19156.\n149. Du, Y .; Bosselut, A.; Manning, C.D. Synthetic disinformation attacks on automated fact verification systems.arXiv2022,\narXiv:2202.09381.\n150. Pan, Y .; Pan, L.; Chen, W.; et al. On the risk of misinformation pollution with large language models.arXiv2023,\narXiv:2305.13661.\nhttps://doi.org/10.53941/tai.2025.100003. 55\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\n151. Liu, X.; Xu, N.; Chen, M.; et al. Autodan: Generating stealthy jailbreak prompts on aligned large language models.arXiv\n2024, arXiv:2310.04451.\n152. Alon, G.; Kamfonas, M. Detecting language model attacks with perplexity.arXiv2023, arXiv:2308.14132.\n153. Kumar, A.; Agarwal, C.; Srinivas, S.; et al. Certifying llm safety against adversarial prompting.arXiv2025,\narXiv:2309.02705.\n154. Wei, J.; Wang, X.; Schuurmans, D.; et al. Chain-of-thought prompting elicits reasoning in large language models.arXiv\n2023, arXiv:2201.11903.\n155. Lewis, P.; Perez, E.; Piktus, A.; et al. Retrieval-augmented generation for knowledge-intensive nlp task. In Proceedings of\nthe Advances in Neural Information Processing Systems, Virtual, 6–12 December2020; V olume 33, pp. 9459–9474.\n156. Geiping, J.; Fowl, L.; Somepalli, G.; et al. What doesn’t kill you makes you robust(er): How to adversarially train against\ndata poisoning.arXiv2022, arXiv:2102.13624.\n157. Tang, R.; Yuan, J.; Li, Y .; et al. Setting the trap: Capturing and defeating backdoors in pretrained language models through\nhoneypots.arXiv2023, arXiv:2310.18633.\n158. Huang, T.; Hu, S.; Liu, L. Vaccine: Perturbation-aware alignment for large language models against harmful fine-tuning\nattack.arXiv2024, arXiv:2402.01109.\n159. Zhu, B.; Qin, Y .; Cui, G.; et al. Moderate-fitting as a natural backdoor defender for pre-trained language models. In\nProceedings of the Advances in Neural Information Processing Systems, New Orleans, LA, USA, 28 November–9\nDecember 2022.\n160. Li, Y .; Lyu, X.; Koren, N.; et al. Anti-backdoor learning: Training clean models on poisoned data. In Proceedings of the\nAdvances in Neural Information Processing Systems, Online, 6–14 December 2021; pp. 14900–14912.\n161. Li, H.; Chen, Y .; Zheng, Z.; et al. Backdoor removal for generative large language models.arXiv2024, arXiv:2405.07667.\n162. Zeng, Y .; Sun, W.; Huynh, T.N.; et al. Beear: Embedding-based adversarial removal of safety backdoors in instruction-tuned\nlanguage models.arXiv2024, arXiv:2406.17092.\n163. Zhao, S.; Gan, L.; Tuan, L.A.; et al. Defending against weight-poisoning backdoor attacks for parameter-efficient\nfine-tuning.arXiv2024, arXiv:2402.12168.\n164. Xi, Z.; Du, T.; Li, C.; et al. Defending pre-trained language models as few-shot learners against backdoor attacks.arXiv\n2023, arXiv:2309.13256.\n165. Chen, C.; Dai, J. Mitigating backdoor attacks in lstm-based text classification systems by backdoor keyword identification.\nNeurocomputing2021,452, 253–262.\n166. Sha, Z.; He, X.; Berrang, P.; et al. Fine-tuning is all you need to mitigate backdoor attacks.arXiv2022, arXiv:2212.09067.\n167. Wu, D.; Wang, Y . Adversarial neuron pruning purifies backdoored deep models. In Proceedings of the Advances in Neural\nInformation Processing Systems, Online, 6–14 December 2021; pp. 16913–16925.\n168. Guan, J.; Tu, Z.; He, R.; et al. Few-shot backdoor defense using shapley estimation. In Proceedings of the 2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 18–22 June 2022; pp.\n13348–13357.\n169. Wang, H.; Hong, J.; Zhang, A.; et al. Trap and replace: Defending backdoor attacks by trapping them into an easy-to-replace\nsubnetwork. In Proceedings of the Advances in Neural Information Processing Systems, New Orleans, LA, USA, 28\nNovember–9 December 2022; pp. 36026–36039.\n170. Zhang, Z.; Lyu, L.; Ma, X.; et al. Fine-mixing: Mitigating backdoors in fine-tuned language models.arXiv2022,\narXiv:2210.09545.\n171. Bansal, H.; Singhi, N.; Yang, Y .; et al. Cleanclip: Mitigating data poisoning attacks in multimodal contrastive learning.\narXiv2023, arXiv:2303.03323.\n172. Wu, Z.; Zhang, Z.; Cheng, P.; et al. Acquiring clean language models from backdoor poisoned datasets by downscaling\nfrequency space.arXiv2024, arXiv:2402.12026.\n173. Zhai, S.; Shen, Q.; Chen, X.; et al. Ncl: Textual backdoor defense using noise-augmented contrastive learning. In\nProceedings of the ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), Rhodes Island, Greece, 4–10 June 2023; pp. 1–5.\n174. Chen, C.; Hong, H.; Xiang, T.; et al. Anti-backdoor model: A novel algorithm to remove backdoors in a non-invasive way.\nIEEE Trans. Inf. Forensics Secur.2024,19, 7420–7434.\n175. Bie, R.; Jiang, J.; Xie, H.; et al. Mitigating backdoor attacks in pre-trained encoders via self-supervised knowledge\ndistillation.IEEE Trans. Serv. Comput.2024,17, 2613–2625.\n176. Huang, K.; Li, Y .; Wu, B.; et al. Backdoor defense via decoupling the training process.arXiv2022, arXiv:2202.03423.\n177. Zeng, Y .; Chen, S.; Park, W.; et al. Adversarial unlearning of backdoors via implicit hypergradient.arXiv2022,\narXiv:2110.03735.\n178. Liu, Y .; Xu, X.; Hou, Z.; et al. Causality based front-door defense against backdoor attack on language models. In Proceed-\nings of the 41st International Conference on Machine Learning, Vienna, Austria, 21–27 July 2024; pp. 32,239–32252.\n179. Wei, J.; Fan, M.; Jiao, W.; et al. Bdmmt: Backdoor sample detection for language models through model mutation testing.\narXiv2023, arXiv:2301.10412.\nhttps://doi.org/10.53941/tai.2025.100003. 56\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\n180. Doan, B.G.; Abbasnejad, E.; Ranasinghe, D.C. Februus: Input purification defense against trojan attacks on deep neural\nnetwork systems. In Proceedings of the Annual Computer Security Applications Conference, Austin, TX, USA, 7–11\nDecember 2020. http://dx.doi.org/10.1145/3427228.3427264.\n181. Li, Y .; Xu, Z.; Jiang, F.; et al. Cleangen: Mitigating backdoor attacks for generation tasks in large language models.arXiv\n2024, arXiv:2406.12257.\n182. Chou, E.; Tram`er, F.; Pellegrino, G. Sentinet: Detecting localized universal attacks against deep learning systems.arXiv\n2020, arXiv:1812.00292.\n183. Yan, L.; Zhang, Z.; Tao, G.; et al. Parafuzz: An interpretability-driven technique for detecting poisoned samples in nlp.\narXiv2023, arXiv:2308.02122.\n184. Wei, C.; Meng, W.; Zhang, Z.; et al. Lmsanitator: Defending prompt-tuning against task-agnostic backdoors. In Pro-\nceedings 2024 Network and Distributed System Security Symposium,San Diego, CA, USA, 26 February–1 March 2024.\nhttp://dx.doi.org/10.14722/ndss.2024.23238.\n185. Chen, H.; Fu, C.; Zhao, J.; et al. Deepinspect: A black-box trojan detection and mitigation framework for deep neural\nnetworks. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, Macao,\nChina, 10–16 August 2019; pp. 4658–4664. https://doi.org/10.24963/ijcai.2019/647.\n186. Liu, Y .; Lee, W.-C.; Tao, G.; et al. Abs: Scanning neural networks for back-doors by artificial brain stimulation. In\nProceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, New York, NY , USA,\n11–15 November 2019; pp. 1265–1282. https://doi.org/10.1145/3319535.3363216.\n187. Hinton, G.; Vinyals, O.; Dean, J. Distilling the knowledge in a neural network.arXiv2015, arXiv:1503.02531.\n188. Li, Y .; Lyu, X.; Koren, N.; et al. Neural attention distillation: Erasing backdoor triggers from deep neural networks.arXiv\n2021, arXiv:2101.05930.\n189. Yang, W.; Lin, Y .; Li, P.; et al. Rethinking stealthiness of backdoor attack against nlp models. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (V olume 1: Long Papers), Virtual, 1–6 August 2021; pp. 5543–5557.\n190. Shen, L.; Ji, S.; Zhang, X.; et al. Backdoor pre-trained models can transfer to all.arXiv2021, arXiv:2111.00197.\n191. Du, W.; Li, P.; Li, B.; et al. Uor: Universal backdoor attacks on pre-trained language models.arXiv2023, arXiv:2305.09574.\n192. Wen, R.; Zhao, Z.; Liu, Z.; et al. Is adversarial training really a silver bullet for mitigating data poisoning? In Proceedings\nof the International Conference on Learning Representations, Kigali, Rwanda, 1–5 May 2023.\n193. Hubinger, E.; Denison, C.; Mu, J.; et al. Sleeper agents: Training deceptive llms that persist through safety training.arXiv\n2024, arXiv:2401.05566.\n194. Christiano, P.; Leike, J.; Brown, T.B.; et al. Deep reinforcement learning from human preferences.arXiv2023,\narXiv:1706.03741.\n195. Ganguli, D.; Lovitt, L.; Kernion, J.; et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and\nlessons learned.arXiv2022, arXiv:2209.07858.\n196. Li, X.; Zhang, Y .; Lou, R.; et al. Chain-of-scrutiny: Detecting backdoor attacks for large language models.arXiv2024,\narXiv:2406.05948.\n197. Si, W.M.; Backes, M.; Blackburn, J.; et al. Why so toxic? measuring and triggering toxic behavior in open-domain chatbots.\nIn Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, Los Angeles, CA,\nUSA, 7–11 November 2022; pp. 2659–2673.\n198. Papineni, K.; Roukos, S.; Ward, T.; et al. Bleu: a method for automatic evaluation of machine translation. In Proceedings\nof the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA, USA, 7–12 July 2002; pp.\n311–318.\n199. Lin, C.-Y . ROUGE: A package for automatic evaluation of summaries. In Proceedings of the Text Summarization Branches\nOut, Barcelona, Spain, 25–26 July 2004; pp. 74–81.\n200. Li, Y .; Huang, H.; Zhao, Y .; et al. Backdoorllm: A comprehensive benchmark for backdoor attacks on large language\nmodels.arXiv2024, arXiv:2408.12798.\n201. Socher, R.; Perelygin, A.; Wu, J.; et al. Recursive deep models for semantic compositionality over a sentiment treebank. In\nProceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, WA, USA, 18–21\nOctober 2013; pp. 1631–1642.\n202. de Gibert, O.; Perez, N.; Garc´ıa-Pablos, A.; et al. Hate speech dataset from a white supremacy forum. In Proceedings of the\n2nd Workshop on Abusive Language Online (ALW2), Brussels, Belgium, 31 October–1 November 2018; pp. 11–20.\n203. Zhang, X.; Zhao, J.; LeCun, Y . Character-level convolutional networks for text classification. In Proceedings of the\nAdvances in Neural Information Processing Systems, Montreal, QC, Canada, 7–12 December 2015.\n204. Maas, A.L.; Daly, R.E.; Pham, P.T.; et al. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual\nMeeting of the Association for Computational Linguistics: Human Language Technologies, Portland, OR, USA, 19–24\nJune 2011; pp. 142–150.\n205. Ding, N.; Chen, Y .; Xu, B.; et al. Enhancing chat language models by scaling high-quality instructional conversations.\narXiv2023, arXiv:2305.14233.\nhttps://doi.org/10.53941/tai.2025.100003. 57\nZhou et al. Trans. Artif. Intell.2025,1(1), 28–58\n206. Hartvigsen, T.; Gabriel, S.; Palangi, H.; et al. Toxigen: A large-scale machine-generated dataset for adversarial and implicit\nhate speech detection.arXiv2022, arXiv:2203.09509.\n207. Xu, J.; Ju, D.; Li, M.; et al. Bot-adversarial dialogue for safe conversational agents. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online,\n6–11 June 2021; pp. 2950–2968.\n208. Li, X.; Zhang, T.; Dubois, Y .; et al. Alpacaeval: An Automatic Evaluator of Instruction-Following Models. May 2023.\nAvailable online: https://github.com/tatsu-lab/alpaca eval (accessed on 5 May 2025).\nhttps://doi.org/10.53941/tai.2025.100003. 58",
  "topic": "Backdoor",
  "concepts": [
    {
      "name": "Backdoor",
      "score": 0.9911288022994995
    },
    {
      "name": "Computer security",
      "score": 0.5767964124679565
    },
    {
      "name": "Computer science",
      "score": 0.36456501483917236
    },
    {
      "name": "Criminology",
      "score": 0.32814013957977295
    },
    {
      "name": "Psychology",
      "score": 0.26337844133377075
    }
  ],
  "institutions": []
}