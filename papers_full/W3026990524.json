{
  "title": "Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers",
  "url": "https://openalex.org/W3026990524",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5022688200",
      "name": "Anne Lauscher",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5058032142",
      "name": "Olga Majewska",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5063685873",
      "name": "Leonardo F. R. Ribeiro",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5027450194",
      "name": "Iryna Gurevych",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5038451521",
      "name": "Nikolai Rozanov",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5079336821",
      "name": "Goran Glavaš",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2250930514",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2080133951",
    "https://openalex.org/W3017231514",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W3104097132",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2107901333",
    "https://openalex.org/W102708294",
    "https://openalex.org/W2022166150",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2523679382",
    "https://openalex.org/W3006881356",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2102153514",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2806120502",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2016089260"
  ],
  "abstract": "Following the major success of neural language models (LMs) such as BERT or GPT-2 on a variety of language understanding tasks, recent work focused on injecting (structured) knowledge from external resources into these models. While on the one hand, joint pretraining (i.e., training from scratch, adding objectives based on external knowledge to the primary LM objective) may be prohibitively computationally expensive, post-hoc fine-tuning on external knowledge, on the other hand, may lead to the catastrophic forgetting of distributional knowledge. In this work, we investigate models for complementing the distributional knowledge of BERT with conceptual knowledge from ConceptNet and its corresponding Open Mind Common Sense (OMCS) corpus, respectively, using adapter training. While overall results on the GLUE benchmark paint an inconclusive picture, a deeper analysis reveals that our adapter-based models substantially outperform BERT (up to 15-20 performance points) on inference tasks that require the type of conceptual knowledge explicitly present in ConceptNet and OMCS. All code and experiments are open sourced under: https://github.com/wluper/retrograph .",
  "full_text": "Proceedings of Deep Learning Inside Out (DeeLIO):\nThe First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,pages 43–49\nOnline, November 19, 2020.c⃝2020 Association for Computational Linguistics\n43\nCommon Sense or World Knowledge? Investigating Adapter-Based\nKnowledge Injection into Pretrained Transformers\nAnne Lauscher♣ Olga Majewska♠ Leonardo F. R. Ribeiro♦\nIryna Gurevych♦ Nikolai Rozanov♠ Goran Glavaˇs♣\n♣Data and Web Science Group, University of Mannheim, Germany\n♠Wluper, London, United Kingdom\n♦Ubiquitous Knowledge Processing (UKP) Lab, TU Darmstadt, Germany\n{anne,goran}@informatik.uni-mannheim.de\n{olga,nikolai}@wluper.com\nwww.ukp.tu-darmstadt.de\nAbstract\nFollowing the major success of neural lan-\nguage models (LMs) such as BERT or GPT-2\non a variety of language understanding tasks,\nrecent work focused on injecting (structured)\nknowledge from external resources into these\nmodels. While on the one hand, joint pre-\ntraining (i.e., training from scratch, adding ob-\njectives based on external knowledge to the pri-\nmary LM objective) may be prohibitively com-\nputationally expensive, post-hoc ﬁne-tuning\non external knowledge, on the other hand,\nmay lead to the catastrophic forgetting of dis-\ntributional knowledge. In this work, we in-\nvestigate models for complementing the dis-\ntributional knowledge of BERT with concep-\ntual knowledge from ConceptNet and its corre-\nsponding Open Mind Common Sense (OMCS)\ncorpus, respectively, using adapter training .\nWhile overall results on the GLUE benchmark\npaint an inconclusive picture, a deeper analy-\nsis reveals that our adapter-based models sub-\nstantially outperform BERT (up to 15-20 per-\nformance points) on inference tasks that re-\nquire the type of conceptual knowledge ex-\nplicitly present in ConceptNet and OMCS. We\nalso open source all our experiments and rel-\nevant code under: https://github.com/\nwluper/retrograph.\n1 Introduction\nSelf-supervised neural models like ELMo (Peters\net al., 2018), BERT (Devlin et al., 2019; Liu et al.,\n2019b), GPT (Radford et al., 2018, 2019), or XL-\nNet (Yang et al., 2019) have rendered language\nmodeling a very suitable pretraining task for learn-\ning language representations that are useful for a\nwide range of language understanding tasks (Wang\net al., 2018, 2019). Although shown versatile w.r.t.\nthe types of knowledge (Rogers et al., 2020) they\nencode, much like their predecessors – static word\nembedding models (Mikolov et al., 2013; Penning-\nton et al., 2014) – neural LMs still only “consume”\nthe distributional information from large corpora.\nYet, a number of structured knowledge sources ex-\nist – knowledge bases (KBs) (Suchanek et al., 2007;\nAuer et al., 2007) and lexico-semantic networks\n(Miller, 1995; Liu and Singh, 2004; Navigli and\nPonzetto, 2010) – encoding many types of knowl-\nedge that are underrepresented in text corpora.\nStarting from this observation, most recent ef-\nforts focused on injecting factual (Zhang et al.,\n2019; Liu et al., 2019a; Peters et al., 2019) and\nlinguistic knowledge (Lauscher et al., 2019; Peters\net al., 2019) into pretrained LMs and demonstrated\nthe usefulness of such knowledge in language un-\nderstanding tasks (Wang et al., 2018, 2019). Joint\npretraining models, on the one hand, augment dis-\ntributional LM objectives with additional objec-\ntives based on external resources (Yu and Dredze,\n2014; Nguyen et al., 2016; Lauscher et al., 2019)\nand train the extended model from scratch. For\nmodels like BERT, this implies computationally\nexpensive retraining from scratch of the encoding\ntransformer network. Post-hoc ﬁne-tuning mod-\nels (Zhang et al., 2019; Liu et al., 2019a; Peters\net al., 2019), on the other hand, use the objectives\nbased on external resources to ﬁne-tune the en-\ncoder’s parameters, pretrained via distributional\nLM objectives. If the amount of ﬁne-tuning data\nis substantial, however, this approach may lead to\ncatastrophic forgetting of distributional knowledge\nobtained in pretraining (Goodfellow et al., 2014;\nKirkpatrick et al., 2017).\nIn this work, similar to the concurrent work of\nWang et al. (2020), we turn to the recently pro-\nposed adapter-based ﬁne-tuning paradigm (Re-\nbufﬁ et al., 2018; Houlsby et al., 2019), which\nremedies the shortcomings of both joint pretrain-\ning and standard post-hoc ﬁne-tuning. Adapter-\nbased training injects additional parameters into\nthe encoder and only tunes their values: origi-\nnal transformer parameters are kept ﬁxed. Be-\n44\ncause of this, adapter training preserves the dis-\ntributional information obtained in LM pretraining,\nwithout the need for any distributional (re-)training.\nWhile (Wang et al., 2020) inject factual knowledge\nfrom Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) into\nBERT, in this work, we investigate two resources\nthat are commonly assumed to contain general-\npurpose and common sense knowledge:1 Concept-\nNet (Liu and Singh, 2004; Speer et al., 2017) and\nthe Open Mind Common Sense (OMCS) corpus\n(Singh et al., 2002), from which the ConceptNet\ngraph was (semi-)automatically extracted. For our\nﬁrst model, dubbed CN-A DAPT , we ﬁrst create a\nsynthetic corpus by randomly traversing the Con-\nceptNet graph and then learn adapter parameters\nwith masked language modelling (MLM) training\n(Devlin et al., 2019) on that synthetic corpus. For\nour second model, named OM-A DAPT , we learn\nthe adapter parameters via MLM training directly\non the OMCS corpus.\nWe evaluate both models on the GLUE bench-\nmark, where we observe limited improvements\nover BERT on a subset of GLUE tasks. How-\never, a more detailed inspection reveals large im-\nprovements over the base BERT model (up to 20\nMatthews correlation points) on language inference\n(NLI) subsets labeled as requiring World Knowl-\nedge or knowledge about Named Entities. Inves-\ntigating further, we relate this result to the fact\nthat ConceptNet and OMCS contain much more\nof what in downstream is considered to be fac-\ntual world knowledge than what is judged as com-\nmon sense knowledge. Our ﬁndings pinpoint the\nneed for more detailed analyses of compatibility\nbetween (1) the types of knowledge contained by\nexternal resources; and (2) the types of knowledge\nthat beneﬁt concrete downstream tasks; within the\nemerging body of work on injecting knowledge\ninto pretrained transformers.\n2 Knowledge Injection Models\nIn this work, we are primarily set to investigate if\ninjecting speciﬁc types of knowledge (given in the\nexternal resource) beneﬁts downstream inference\nthat clearly requires those exact types of knowl-\nedge. Because of this, we use the arguably most\nstraightforward mechanisms for injecting the Con-\nceptNet and OMCS information into BERT, and\nleave the exploration of potentially more effective\nknowledge injection objectives for future work. We\n1Our results in §3.2 scrutinize this assumption.\ninject the external information into adapter param-\neters of the adapter-augmented BERT (Houlsby\net al., 2019) via BERT’s natural objective – masked\nlanguage modelling (MLM). OMCS, already a cor-\npus in natural language, is directly subjectable to\nMLM training – we ﬁltered out non-English sen-\ntences. To subject ConceptNet to MLM training,\nwe need to transform it into a synthetic corpus.\nUnwrapping ConceptNet. Following estab-\nlished previous work (Perozzi et al., 2014; Ristoski\nand Paulheim, 2016), we induce a synthetic corpus\nfrom ConceptNet by randomly traversing its graph.\nWe convert relation strings into NL phrases (e.g.,\nsynonyms to is a synonym of ) and duplicate the\nobject node of a triple, using it as the subject for\nthe next sentence. For example, from the path\n“alcoholism causes−−−−→stigma hasContext−−−−−−→christianity\npartOf\n−−−→religion” we create the text “ alcoholism\ncauses stigma. stigma is used in the context of\nchristianity. christianity is part of religion.”. We\nset the walk lengths to 30 relations and sample\nthe starting and neighboring nodes from uniform\ndistributions. In total, we performed 2,268,485\nwalks, resulting with the corpus of 34,560,307\nsynthetic sentences.\nAdapter-Based Training. We follow Houlsby\net al. (2019) and adopt the adapter-based archi-\ntecture for which they report solid performance\nacross the board. We injectbottleneck adaptersinto\nBERT’s transformer layers. In each transformer\nlayer, we insert two bottleneck adapters: one af-\nter the multi-head attention sub-layer and another\nafter the feed-forward sub-layer. Let X ∈RT×H\nbe the sequence of contextualized vectors (of size\nH) for the input of T tokens in some transformer\nlayer, input to a bottleneck adapter. The bottleneck\nadapter, consisting of two feed-forward layers and\na residual connection, yields the following output:\nAdapter(X) =X + f (XWd + bd) Wu + bu\nwhere Wd (with bias bd) and Wu (with bias\nbu) are adapter’s parameters, that is, the weights\nof the linear down-projection and up-projection\nsub-layers and f is the non-linear activation func-\ntion. Matrix Wd ∈RH×m compresses vectors\nin X to the adapter size m < H, and the ma-\ntrix Wu ∈ Rm×H projects the activated down-\nprojections back to transformer’s hidden size H.\nThe ratio H/m determines how many times fewer\n45\nparameters we optimize with adapter-based train-\ning compared to standard ﬁne-tuning of all trans-\nformer’s parameters.\n3 Evaluation\nWe ﬁrst brieﬂy describe the downstream tasks and\ntraining details, and then proceed with the discus-\nsion of results obtained with our adapter models.\n3.1 Experimental Setup.\nDownstream Tasks. We evaluate BERT and our\ntwo adapter-based models, CN-A DAPT and OM-\nADAPT , with injected knowledge from ConceptNet\nand OMCS, respectively, on the tasks from the\nGLUE benchmark (Wang et al., 2018):\nCoLA (Warstadt et al., 2018): Binary sentence\nclassiﬁcation, predicting grammatical acceptability\nof sentences from linguistic publications;\nSST-2 (Socher et al., 2013): Binary sentence clas-\nsiﬁcation, predicting binary sentiment (positive or\nnegative) for movie review sentences;\nMRPC (Dolan and Brockett, 2005): Binary\nsentence-pair classiﬁcation, recognizing sentences\nwhich are are mutual paraphrases;\nSTS-B (Cer et al., 2017): Sentence-pair regression\ntask, predicting the degree of semantic similarity\nfor a given pair of sentences;\nQQP (Chen et al., 2018): Binary classiﬁcation task,\nrecognizing question paraphrases;\nMNLI (Williams et al., 2018): Ternary natural lan-\nguage inference (NLI) classiﬁcation of sentence\npairs. Two test sets are given: a matched version\n(MNLI-m) in which the test domains match the\ndomains from training data, and a mismatched ver-\nsion (MNLI-mm) with different test domains;\nQNLI: A binary classiﬁcation version of the Stan-\nford Q&A dataset (Rajpurkar et al., 2016);\nRTE (Bentivogli et al., 2009): Another NLI dataset,\nternary entailment classiﬁcation for sentence pairs;\nDiag (Wang et al., 2018): A manually curated NLI\ndataset, with examples labeled with speciﬁc types\nof knowledge needed for entailment decisions.\nTraining Details. We inject our adapters into a\nBERT Base model (12 transformer layers with 12\nattention heads each; H = 768) pretrained on low-\nercased corpora. Following (Houlsby et al., 2019),\nwe set the size of all adapters to m = 64 and\nuse GELU (Hendrycks and Gimpel, 2016) as the\nadapter activation f. We train the adapter param-\neters with the Adam algorithm (Kingma and Ba,\n2015) (initial learning rate set to 1e−4, with 10000\nwarm-up steps and the weight decay factor of0.01).\nIn downstream ﬁne-tuning, we train in batches of\nsize 16 and limit the input sequences to T = 128\nwordpiece tokens. For each task, we ﬁnd the op-\ntimal hyperparameter conﬁguration from the fol-\nlowing grid: learning rate l ∈{2 ·10−5, 3 ·10−5},\nepochs in n ∈{3, 4}.\n3.2 Results and Analysis\nGLUE Results. Table 1 reveals the performance\nof CN-A DAPT and OM-A DAPT in comparison\nwith BERT Base on GLUE evaluation tasks. We\nshow the results for two snapshots of OM-A DAPT ,\nafter 25K and 100K update steps, and for two snap-\nshots of CN-A DAPT , after 50K and 100K steps\nof adapter training. Overall, none of our adapter-\nbased models with injected external knowledge\nfrom ConceptNet or OMCS yields signiﬁcant im-\nprovements over BERT Base on GLUE. However,\nwe observe substantial improvements (of around 3\npoints) on RTE and on the Diagnostics NLI dataset\n(Diag), which encompasses inference instances that\nrequire a speciﬁc type of knowledge.\nSince our adapter models draw speciﬁcally on\nthe conceptual knowledge encoded in ConceptNet\nand OMCS, we expect the positive impact of in-\njected external knowledge – assuming effective\ninjection – to be most observable on test instances\nthat target the same types of conceptual knowledge.\nTo investigate this further, we measure the model\nperformance across different categories of the Di-\nagnostic NLI dataset. This allows us to tease apart\ninference instances which truly test the efﬁcacy of\nour knowledge injection methods. We show the\nresults obtained on different categories of the Diag-\nnostic NLI dataset in Table 2. The improvements\nof our adapter-based models over BERT Base on\nthese phenomenon-speciﬁc subsections of the Di-\nagnostics NLI dataset are generally much more\npronounced: e.g., OM-A DAPT (25K) yields a 7%\nimprovement on inference that requires factual or\ncommon sense knowledge (KNO), whereas CN-\nADAPT (100K) yields a 6% boost for inference that\ndepends on lexico-semantic knowledge (LS). These\nresults suggest that (1) ConceptNet and OMCS do\ncontain the speciﬁc types of knowledge required for\nthese inference categories and that (2) we managed\nto inject that knowledge into BERT by training\n46\nModel CoLA SST-2 MRPC STS-B QQP MNLI-m MNLI-mm QNLI RTE Diag Avg\nMCC Acc F1 Spear F1 Acc Acc Acc Acc MCC –\nBERT Base 52.1 93.5 88.9 85.8 71.2 84.6 83.4 90.5 66.4 34.2 75.1\nOM-A DAPT (25K) 49.5 93.5 88.8 85.1 71.4 84.4 83.5 90.9 67.5 35.7 75.0\nOM-A DAPT (100K) 53.5 93.4 87.9 85.9 71.1 84.2 83.7 90.6 68.2 34.8 75.3\nCN-A DAPT (50K) 49.8 93.9 88.9 85.8 71.6 84.2 83.3 90.6 69.7 37.0 75.5\nCN-A DAPT (100K) 48.8 92.8 87.1 85.7 71.5 83.9 83.2 90.8 64.1 37.8 74.6\nTable 1: Results on test portions of GLUE benchmark tasks. Numbers in brackets next to adapter-based models\n(25K, 50K, 100K) indicate the number of update steps of adapter training on the synthetic ConceptNet corpus (for\nCN-A DAPT ) or on the original OMCS corpus (for OM-A DAPT ). Bold: the best score in each column.\nModel LS KNO LOG PAS All\nBERT Base 38.5 20.2 26.7 39.6 34.2\nOM-A DAPT (25K) 39.1 27.1 26.1 39.5 35.7\nOM-A DAPT (100K) 37.5 21.2 27.4 41.0 34.8\nCN-A DAPT (50K) 40.2 24.3 30.1 42.7 37.0\nCN-A DAPT (100K) 44.2 25.2 30.4 41.9 37.8\nTable 2: Breakdown of Diagnostics NLI performance\n(Matthews correlation), according to information type\nneeded for inference (coarse-grained categories): Lexi-\ncal Semantics (LS), Knowledge (KNO), Logic (LOG),\nand Predicate Argument Structure (PAS).\nModel CS World NE\nBERT Base 29.0 10.3 15.1\nOM-A DAPT (25K) 28.5 25.3 31.4\nOM-A DAPT (100K) 24.5 17.3 22.3\nCN-A DAPT (50K) 25.6 21.1 26.0\nCN-A DAPT (100K) 24.4 25.6 36.5\nTable 3: Results (Matthews correlation) on Common\nSense (CS), World Knowledge (World), and Named En-\ntities (NE) categories of the Diagnostic NLI dataset.\nadapters on these resources.\nFine-Grained Knowledge Type Analysis. In\nour ﬁnal analysis, we “zoom in” our models’ per-\nformances on three ﬁne-grained categories of the\nDiagnostics NLI dataset – inference instances that\nrequire Common Sense Knowledge (CS), World\nKnowledge (World), and knowledge about Named\nEntities (NE), respectively. The results for these\nﬁne-grained categories are given in Table 3. These\nresults show an interesting pattern: our adapter-\nbased knowledge-injection models massively out-\nperform BERT Base (up to 15 and 21 MCC points,\nrespectively) for NLI instances labeled as requir-\ning World Knowledge or knowledge about Named\nEntities. In contrast, we see drops in performance\non instances labeled as requiring common sense\nknowledge. This initially came as a surprise, given\nthe common belief that OMCS and ConcepNet con-\ntain the so-called common sense knowledge. Man-\nual scrutiny of the diagnostic test instances from\nboth CS and World categories uncovers a notice-\nable mismatch between the kind of information that\nis considered common sense in KBs like Concept-\nNet and what is considered common sense knowl-\nedge in the downstream. In fact, the majority of\ninformation present in ConceptNet and OMCS falls\nunder the World Knowledge deﬁnition of the Diag-\nnostic NLI dataset, including factual geographic in-\nformation (stockholm [partOf] sweden),\ndomain knowledge ( roadster [isA] car)\nand specialized terminology ( indigenous\n[synonymOf] aboriginal).\nIn contrast, many of the CS inference instances\nrequire complex, high-level reasoning, understand-\ning metaphorical and idiomatic meaning, and mak-\ning far-reaching connections. We display NLI Dig-\nnostics examples from the World Knowledge and\nCommon Sense categories in Table 4. In such\ncases, explicit conceptual links often do not sufﬁce\nfor a correct inference and much of the required\nknowledge is not explicitly encoded in the exter-\nnal resources. Consider, e.g., the following CS\nNLI instance: [ premise: My jokes fully reveal\nmy character ; hypothesis: If everyone be-\nlieved my jokes, they’d know exactly who I was\n; entailment]. While ConceptNet and OMCS\nmay associate character with personality or person-\nality with identity, the knowledge that the phrase\nwho I was may refer to identity is beyond the ex-\nplicit knowledge present in these resources. This\nsheds light on the results in Table 3: when the\nknowledge required to tackle the inference prob-\nlem at hand is available in the external resource,\nour adapter-based knowledge-injected models sig-\nniﬁcantly outperform the baseline transformer; oth-\nerwise, the beneﬁts of knowledge injection are neg-\n47\nKnowledge Premise Hypothesis ConceptNet?\nWorld The sides came to an agree-\nment after their meeting in\nStockholm.\nThe sides came to an agree-\nment after their meeting in\nSweden.\nstockholm [partOf]\nsweden\nMusk decided to offer up his\npersonal Tesla roadster.\nMusk decided to offer up his\npersonal car.\nroadster [isA] car\nThe Sydney area has been\ninhabited by indigenous\nAustralians for at least\n30,000 years.\nThe Sydney area has been\ninhabited by Aboriginal\npeople for at least 30,000\nyears.\nindigenous [synonymOf]\naboriginal\nCommon Sense My jokes fully reveal my\ncharacter.\nIf everyone believed my\njokes, they’d know exactly\nwho I was.\nThe systems thus produced\nare incremental: dialogues\nare processed word-by-\nword, shown previously\nto be essential in support-\ning natural, spontaneous\ndialogue.\nThe systems thus produced\nsupport the capability to in-\nterrupt an interlocutor mid-\nsentence.\nHe deceitfully proclaimed:\n“This is all I ever really\nwanted. ”\nHe was satisﬁed.\nTable 4: Premise-hypothesis examples from the diagnostic NLI dataset tagged for commonsense and world knowl-\nedge, and relevant ConceptNet relations, where available.\nligible or non-existent. The promising results on\nworld knowledge and named entities portions of\nthe Diagnostics dataset suggest that our methods\ndoes successfully inject external information into\nthe pretrained transformer and that the presence of\nthe required knowledge for the task in the external\nresources is an obvious prerequisite.\n4 Conclusion\nWe presented two simple strategies for injecting\nexternal knowledge from ConceptNet and OMCS\ncorpus, respectively, into BERT via bottleneck\nadapters. Additional adapter parameters store the\nexternal knowledge and allow for the preservation\nof the rich distributional knowledge acquired in\nBERT’s pretraining in the original transformer pa-\nrameters. We demonstrated the effectiveness of\nthese models in language understanding settings\nthat require precisely the type of knowledge that\none ﬁnds in ConceptNet and OMCS, in which our\nadapter-based models outperform BERT by up to\n20 performance points. Our ﬁndings stress the\nimportance of having detailed analyses that com-\npare (a) the types of knowledge found in external\nresources being injected against (b) the types of\nknowledge that a concrete downstream reasoning\ntasks requires. We hope this work motivates fur-\nther research effort in the direction of ﬁne-grained\nknowledge typing, both of explicit knowledge in ex-\nternal resources and the implicit knowledge stored\nin pretrained transformers.\nAcknowledgments\nAnne Lauscher and Goran Glava ˇs are supported\nby the Eliteprogramm of the Baden-W¨urttemberg\nStiftung (AGREE grant). Leonardo F. R. Ribeiro\nhas been supported by the German Research Foun-\ndation as part of the Research Training Group\nAIPHES under the grant No. GRK 1994/1. This\nwork has been supported by the German Research\nFoundation within the project “Open Argument\nMining” (GU 798/25-1), associated with the Pri-\nority Program “Robust Argumentation Machines\n(RATIO)” (SPP-1999). The work of Olga Majew-\nska was conducted under the research lab of Wluper\nLtd. (UK/ 10195181).\n48\nReferences\nS¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary Ives.\n2007. Dbpedia: A nucleus for a web of open data.\nIn The semantic web, pages 722–735. Springer.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The ﬁfth pascal recognizing tex-\ntual entailment challenge. In TAC.\nDaniel Cer, Mona Diab, Eneko Agirre, I ˜nigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nZihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi\nZhao. 2018. Quora question pairs.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nIan J Goodfellow, Mehdi Mirza, Aaron Courville\nDa Xiao, and Yoshua Bengio. 2014. An empirical\ninvestigation of catastrophic forgeting in gradient-\nbased neural networks. In In Proceedings of Inter-\nnational Conference on Learning Representations\n(ICLR. Citeseer.\nDan Hendrycks and Kevin Gimpel. 2016. Gaussian er-\nror linear units (gelus).\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for nlp.\nIn International Conference on Machine Learning ,\npages 2790–2799.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences ,\n114(13):3521–3526.\nAnne Lauscher, Ivan Vuli ´c, Edoardo Maria Ponti,\nAnna Korhonen, and Goran Glava ˇs. 2019. Inform-\ning unsupervised pretraining with external linguistic\nknowledge. arXiv preprint arXiv:1909.02339.\nHugo Liu and Push Singh. 2004. Conceptnet—a practi-\ncal commonsense reasoning tool-kit. BT technology\njournal, 22(4):211–226.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2019a. K-bert:\nEnabling language representation with knowledge\ngraph. arXiv preprint arXiv:1909.07606.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. Communications of the ACM , 38(11):39–\n41.\nRoberto Navigli and Simone Paolo Ponzetto. 2010. Ba-\nbelnet: Building a very large multilingual semantic\nnetwork. In Proceedings of the 48th annual meet-\ning of the association for computational linguistics ,\npages 216–225. Association for Computational Lin-\nguistics.\nKim Anh Nguyen, Sabine Schulte im Walde, and\nNgoc Thang Vu. 2016. Integrating distributional\nlexical contrast into word embeddings for antonym-\nsynonym distinction. In Proceedings of ACL, pages\n454–459.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena.\n2014. Deepwalk: Online learning of social represen-\ntations. In Proceedings of the 20th ACM SIGKDD\nInternational Conference on Knowledge Discovery\nand Data Mining , KDD ’14, page 701–710, New\nYork, NY , USA. Association for Computing Machin-\nery.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT, pages\n2227–2237.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge enhanced contextual word\n49\nrepresentations. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 43–54.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. OpenAI Tech-\nnical Report.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea\nVedaldi. 2018. Efﬁcient parametrization of multi-\ndomain deep neural networks. In CVPR.\nPetar Ristoski and Heiko Paulheim. 2016. Rdf2vec:\nRdf graph embeddings for data mining. In Inter-\nnational Semantic Web Conference, pages 498–514.\nSpringer.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works. arXiv preprint arXiv:2002.12327.\nPush Singh, Thomas Lin, Erik T Mueller, Grace Lim,\nTravell Perkins, and Wan Li Zhu. 2002. Open mind\ncommon sense: Knowledge acquisition from the\ngeneral public. In OTM Confederated International\nConferences” On the Move to Meaningful Internet\nSystems”, pages 1223–1237. Springer.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631–1642.\nRobert Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Thirty-First AAAI Conference on\nArtiﬁcial Intelligence.\nFabian M Suchanek, Gjergji Kasneci, and Gerhard\nWeikum. 2007. Yago: a core of semantic knowledge.\nIn Proceedings of the 16th international conference\non World Wide Web, pages 697–706. ACM.\nDenny Vrande ˇci´c and Markus Kr ¨otzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Commu-\nnications of the ACM, 57(10):78–85.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems, pages 3261–3275.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the Blacbox NLP Workshop, pages 353–\n355.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Cuihong Cao, Daxin Jiang, Ming\nZhou, et al. 2020. K-adapter: Infusing knowl-\nedge into pre-trained models with adapters. arXiv\npreprint arXiv:2002.01808.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nMo Yu and Mark Dredze. 2014. Improving lexical em-\nbeddings with semantic knowledge. In Proceedings\nof ACL, pages 545–550.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1441–1451.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7484172582626343
    },
    {
      "name": "Transformer",
      "score": 0.6828941106796265
    },
    {
      "name": "Inference",
      "score": 0.6507763862609863
    },
    {
      "name": "Adapter (computing)",
      "score": 0.6487749814987183
    },
    {
      "name": "Forgetting",
      "score": 0.5581174492835999
    },
    {
      "name": "Language model",
      "score": 0.47074729204177856
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4539986252784729
    },
    {
      "name": "Natural language processing",
      "score": 0.4017263352870941
    },
    {
      "name": "Machine learning",
      "score": 0.3349273204803467
    },
    {
      "name": "Engineering",
      "score": 0.08641883730888367
    },
    {
      "name": "Cognitive psychology",
      "score": 0.08625209331512451
    },
    {
      "name": "Psychology",
      "score": 0.07718738913536072
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 15
}