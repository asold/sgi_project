{
    "title": "Transkimmer: Transformer Learns to Layer-wise Skim",
    "url": "https://openalex.org/W4280546523",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2108427779",
            "name": "Yue Guan",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2131985026",
            "name": "Zhengyi Li",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2226896803",
            "name": "Jingwen Leng",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2584500735",
            "name": "Zhouhan Lin",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2162841773",
            "name": "Minyi Guo",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3081515890",
        "https://openalex.org/W4221162983",
        "https://openalex.org/W3198917403",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3132616766",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2963363070",
        "https://openalex.org/W4287812455",
        "https://openalex.org/W1498436455",
        "https://openalex.org/W2962677625",
        "https://openalex.org/W3171750540",
        "https://openalex.org/W3174708387",
        "https://openalex.org/W4309386164",
        "https://openalex.org/W2767693128",
        "https://openalex.org/W3035038672",
        "https://openalex.org/W1493526108",
        "https://openalex.org/W3035408713",
        "https://openalex.org/W3115555382",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W4226109952",
        "https://openalex.org/W2597655663",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3035030897",
        "https://openalex.org/W4287667694",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W3208397797",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W4287901267",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W3170925726",
        "https://openalex.org/W3101163004",
        "https://openalex.org/W3022331012",
        "https://openalex.org/W2267244539",
        "https://openalex.org/W3159727696",
        "https://openalex.org/W2964182247",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4295838474",
        "https://openalex.org/W2950199911",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Transformer architecture has become the de-facto model for many machine learning tasks from natural language processing and computer vision. As such, improving its computational efficiency becomes paramount. One of the major computational inefficiency of Transformer based models is that they spend the identical amount of computation throughout all layers. Prior works have proposed to augment the Transformer model with the capability of skimming tokens to improve its computational efficiency. However, they suffer from not having effectual and end-to-end optimization of the discrete skimming predictor. To address the above limitations, we propose the Transkimmer architecture, which learns to identify hidden state tokens that are not required by each layer. The skimmed tokens are then forwarded directly to the final output, thus reducing the computation of the successive layers. The key idea in Transkimmer is to add a parameterized predictor before each layer that learns to make the skimming decision. We also propose to adopt reparameterization trick and add skim loss for the end-to-end training of Transkimmer. Transkimmer achieves 10.97x average speedup on GLUE benchmark compared with vanilla BERT-base baseline with less than 1% accuracy degradation.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 7275 - 7286\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nTranskimmer: Transformer Learns to Layer-wise Skim\nYue Guan1,2,⋆, Zhengyi Li1,2,⋆, Jingwen Leng1,2,⋆, Zhouhan Lin1,§ and Minyi Guo1,2,†\n1Shanghai Jiao Tong University,2Shanghai Qizhi Institute\n⋆{bonboru,hobbit,leng-jw}@sjtu.edu.cn,\n§lin.zhouhan@gmail.com, †guo-my@cs.sjtu.edu.cn\nAbstract\nTransformer architecture has become the de-\nfacto model for many machine learning tasks\nfrom natural language processing and com-\nputer vision. As such, improving its compu-\ntational efﬁciency becomes paramount. One\nof the major computational inefﬁciency of\nTransformer-based models is that they spend\nthe identical amount of computation through-\nout all layers. Prior works have proposed to\naugment the Transformer model with the capa-\nbility of skimming tokens to improve its com-\nputational efﬁciency. However, they suffer\nfrom not having effectual and end-to-end op-\ntimization of the discrete skimming predictor.\nTo address the above limitations, we propose\nthe Transkimmer architecture, which learns to\nidentify hidden state tokens that are not re-\nquired by each layer. The skimmed tokens are\nthen forwarded directly to the ﬁnal output, thus\nreducing the computation of the successive lay-\ners. The key idea in Transkimmer is to add a\nparameterized predictor before each layer that\nlearns to make the skimming decision. We also\npropose to adopt reparameterization trick and\nadd skim loss for the end-to-end training of\nTranskimmer. Transkimmer achieves 10.97×\naverage speedup on GLUE benchmark com-\npared with vanilla BERTbase baseline with less\nthan 1% accuracy degradation.\n1 Introduction\nThe Transformer model (Vaswani et al., 2017) has\npushed the accuracy of various NLP applications\nto a new stage by introducing the multi-head atten-\ntion (MHA) mechanism (Lin et al., 2017). Further,\nthe BERT (Devlin et al., 2019) model advances\nits performances by introducing self-supervised\npre-training, and has reached the state-of-the-art\naccuracy on many NLP tasks.\nCompared to the recurrent fashion models, e.g.\nRNN (Rumelhart et al., 1986), LSTM (Hochreiter\nand Schmidhuber, 1997), the Transformer model\nleverages the above attention mechanism to process\nLayer 1\nLayer 2\nLayer 3\nTranskimmer\nTransformer\nLayers\n[CLS] It is a good film .\nLayer 1\nLayer 2\nLayer 3\nTransformer\n[CLS] It is a good film .\nDynamic\nSkim\nToken\nEmbeddings\nInput\nSequence\nDownstream Classifier Downstream ClassifierDownstream\nClassifier\n“Positive” “Positive”\nOutput\nEmbeddings\nHidden States\nEmbeddings\nFigure 1: Overview of Transkimmer dynamic token\nskimming method. Tokens are pruned during the pro-\ncessing of Transformer layers. Note that actually we\ndon’t need all the tokens given to the downstream clas-\nsiﬁer in this sequence classiﬁcation example. We show\nthe full length output embedding sequence to demon-\nstrate the forwarding design of Transkimmer.\nall the input sequence. By doing so, extremely large\nscale and long span models are enabled, resulting\nin a huge performance leap in sequence processing\ntasks. However, the computation complexity of\nthe attention mechanism is O(N2) with the input\nlength of N, which leads to the high computation\ndemand of the Transformer model.\nSome prior works (Goyal et al., 2020; Kim and\nCho, 2021; Kim et al., 2021; Ye et al., 2021) ex-\nplore the opportunity on the dynamic reduction of\ninput sequence length to improve the Transformer’s\ncomputational efﬁciency. Its intuition is similar to\nthe human-being’s reading comprehension capabil-\nity that does not read all words equally. Instead,\nsome words are focused with more interest while\nothers are skimmed. For Transformer models, this\nmeans adopting dynamic computation budget for\ndifferent input tokens according to their contents.\nTo excavate the efﬁciency from this insight, we\npropose to append a skim predictor module to the\nTransformer layer to conduct ﬁne-grained dynamic\ntoken pruning as shown in Fig. 1. When processed\nby the Transformer layers, the sequence of token\n7275\nhidden state embeddings are pruned at each layer\nwith reference to its current state. Less relevant\ntokens are skimmed without further computation\nand forwarded to the ﬁnal output directly. Only\nthe signiﬁcant tokens are continued for successive\nlayers for further processing. This improves the\nTransformer model inference latency by reducing\nthe input tensors on the sequence length dimension.\nHowever, the optimization problem of such skim\ndecision prediction is non-trivial. To conduct prun-\ning of dynamic tensors, non-differentiable discrete\nskim decisions are applied. Prior works have pro-\nposed to use soft-masking approximation or rein-\nforcement learning to resolve, which leads to ap-\nproximation mismatch or nonuniform optimization.\nTranskimmer propose to adopt reparameterization\ntechnique (Jang et al., 2017) to estimate the gradi-\nent for skim prediction. As such, we can achieve\nthe end-to-end joint optimization obejective and\ntraining paradigm. By jointly training the down-\nstream task and skim objective, the Transformer\nlearns to selectively skim input contents. In our\nevaluation, we show Transkimmer outperforms all\nprior input reduction works on inference speedup\ngain and model accuracy. Speciﬁcally, BERTbase is\naccfelerated for 10.97× on GLUE benchmark and\n2.81× without counting the padding tokens. More-\nover, we also demonstrate the method proposed by\nTranskimmer is generally applicable to pre-trained\nlanguage models and compression methods with\nRoBERTa, DistillBERT and ALBERT models.\nThis paper contributes to the following 3 aspects.\n• We propose the Transkimmer model which\naccelerates the Transformer inference with dy-\nnamic token skimming.\n• We further propose an end-to-end joint opti-\nmization method that trains the skim strategy\ntogether with the downstream objective.\n• We evaluate the proposed method on various\ndatasets and backbone models to demonstrate\nits generality.\n2 Related Works\nRecurrent Models with Skimming. The idea to\nskip or skim irrelevant sections or tokens of input\nsequence has been studied in NLP models, espe-\ncially recurrent neural networks (RNN) (Rumel-\nhart et al., 1986) and long short-term memory\nnetwork (LSTM) (Hochreiter and Schmidhuber,\n1997). When processed recurrently, skimming the\ncomputation of a token is simply jumping the cur-\nrent step and keep the hidden states unchanged.\nLSTM-Jump (Yu et al., 2017), Skim-RNN (Seo\net al., 2018), Structural-Jump-LSTM (Hansen et al.,\n2019) and Skip-RNN (Campos et al., 2018) adopt\nthis skimming design for acceleration in recurrent\nmodels.\nTransformer with Input Reduction. Unlike the\nsequential processing of the recurrent models, the\nTransformer model calculates all the input se-\nquence tokens in parallel. As such, skimming can\nbe regarded as the reduction of hidden states tensor\non sequence length dimension. Universal Trans-\nformer (Dehghani et al., 2019) proposes a dynamic\nhalting mechanism that determines the reﬁnement\nsteps for each token. DeFormer (Cao et al., 2020)\nproposes a dual-tower structure to process the ques-\ntion and context part separately at shallow layers\nspeciﬁc for QA task. The context branch is pre-\nprocessed off-line and pruned at shallow layers.\nAlso dedicated for QA tasks, Block-Skim (Guan\net al., 2021) proposes to predict and skim the ir-\nrelevant context blocks by analyzing the attention\nweight patterns. Progressive Growth (Gu et al.,\n2021) randomly drops a portion of input tokens\nduring training to achieve better pre-training efﬁ-\nciency.\nAnother track of research is to perform such\ninput token selection dynamically during infer-\nence, which is the closest to our idea. POWER-\nBERT (Goyal et al., 2020) extracts input sequence\nat token level while processing. During the ﬁne-\ntuning process for downstream tasks, Goyal et al.\nproposes a soft-extraction layer to train the model\njointly. Length-Adaptive Transformer (Kim and\nCho, 2021) improves it by forwarding the inﬂected\ntokens to ﬁnal downstream classiﬁer as recovery.\nLearned Token Pruning (Kim et al., 2021) improves\nPOWER-BERT by making its pre-deﬁned sparsity\nratio a parameterized threshold. TR-BERT (Ye\net al., 2021) adopts reinforcement learning to in-\ndependently optimize a policy network that drops\ntokens. Comparison to these works are discussed\nin detail in Sec. 3. Moreover, SpAttn (Wang\net al., 2021) facilitate POWER-BERT design with\na domain-speciﬁc hardware design for better accel-\neration and propose to make skimming decisions\nwith attention values from all layers.\n7276\nEarly Exit Early exit (Panda et al., 2016; Teer-\napittayanon et al., 2016) is another method to exe-\ncute the neural network with input-dependent com-\nputational complexity. The idea is to halt the exe-\ncution during model processing at some early ex-\nits. Under the circumstance of processing sequen-\ntial inputs, early exit can be viewed as a coarse-\ngrained case of input skimming. With the hard\nconstraint that all input tokens are skimmed at the\nsame time, early exit methods lead to worse ac-\ncuracy and performance results compared to in-\nput skimming methods. However, the early exit\nmethod is also generally applicable to other do-\nmains like convolutional neural networks (CNN).\nDeeBERT (Xin et al., 2020), PABEE (Zhou et al.,\n2020), FastBERT (Liu et al., 2020) are some recent\nworks adopting early exit in Transformer models.\nMagic Pyramid (He et al., 2021) proposes to com-\nbine the early exit and the input skimming ideas\ntogether. Tokens are skimmed with ﬁne-grained\ngranularity following POWER-BERT design and\nthe whole input sequence is halted at some early\nexits.\nEfﬁcient Transformer. There are also many ef-\nforts for designing efﬁcient Transformers (Zhou\net al., 2020; Wu et al., 2020; Tay et al., 2020).\nFor example, researchers have applied well stud-\nied compression methods to Transformers, such\nas pruning (Guo et al.), quantization (Wang and\nZhang, 2020; Guo et al., 2022), distillation (Sanh\net al., 2019), and weight sharing. Other efforts\nfocus on dedicated efﬁcient attention mechanism\nconsidering its quadratic complexity of sequence\nlength (Kitaev et al., 2020; Beltagy et al., 2020; Za-\nheer et al., 2020) or efﬁcient feed-forward neural\nnetwork (FFN) design regarding its dominant com-\nplexity in Transformer model (Dong et al., 2021).\nTranskimmer is orthogonal to these techniques on\nthe input dimension reduction.\n3 Input Skimming Search Space\nIn this section, we discuss the challenges of dy-\nnamic input skimming idea in details. Moreover,\nwe compare techniques and design decisions from\nprior works described in Tbl. 1.\n3.1 Optimization Method\nThe ﬁrst challenge of input skimming is the op-\ntimization with discrete skimming decisions. In\nspeciﬁc, the decision for pruning the hidden state\ntensors (i.e., reducing their sequence length) is\nModels Optimization Input Discard Strategy\nPOWER-BERTSoft-Masking Attention Discard Searched(Goyal et al., 2020)\nLAT Soft-Masking Attention Forward Searched(Kim and Cho, 2021)\nLTP Soft-Masking Attention Discard Learned(Kim et al., 2021)\nTR-BERT RL Embedding Forward Searched(Ye et al., 2021)\nTranskimmer Reparameterize Embedding Forward Learned\nTable 1: Summary of prior token reduction works and\ntheir design choices including POWER-BERT, Length-\nAdaptive Transformer (LAT), Learned Token Pruning\n(LTP) and TR-BERT. The design details are discussed\nin Sec. 3.\na binary prediction. As such, the skim predic-\ntion model is non-differentiable and unable to be\ndirectly optimized by gradient back propagation.\nPrior works handle the discrete binary skimming\ndecision by using a set of complicated training tech-\nniques, which we categorize in Tbl. 1.\nSoft-Masking. Some works (Goyal et al., 2020;\nKim and Cho, 2021; Kim et al., 2021) propose to\nuse the soft-masking training trick which uses a\ncontinuous value for predicting the skimming pre-\ndiction. During the training process, the predicted\nvalue is multiplied to the hidden states embedding\nvectors so that no actual pruning happens. In the\ninference phase, this continuous skimming predic-\ntion value is binarized by a threshold-based step\nfunction. The threshold value is pre-deﬁned or\ndetermined through a hyper-parameter search pro-\ncess. Obviously, there exists a training-inference\nparadigm mismatch where the actual skimming\nonly happens at the inference time. Such a mis-\nmatch leads to a signiﬁcant accuracy degradation.\nReinforcement Learning. TR-BERT (Ye et al.,\n2021) proposes to use the reinforcement learning\n(RL) to solve the discrete skimming decision prob-\nlem. It uses a separated policy network as the\nskimming predictor, and the backbone Transformer\nmodel is considered as the value network. At ﬁrst,\nthe backbone Transformer is ﬁne-tuned separately.\nIt then updates the skimming policy network by\nusing the RL algorithm. This multi-step training\nparadigm is tedious. And training the backbone\nTransformer and skimming policy network sepa-\nrately is sub-optimal compared to the joint opti-\nmization paradigm. Moreover, the large search\nspace of such RL objective is difﬁcult to converge\nespecially on small downstream datasets.\n7277\nReparameterization. In this work, we propose\nto use the reparameterization technique to address\nthe discrete skimming decision challenge. Its core\nidea is to sample the backward propagation gra-\ndient during training, whose details we describe\nin Sec. 4. The advantage of our method is that\nit enables the joint optimization of skim predictor\nand backbone Transformer model and therefore\nachieves the optimal solution. For example, we\nwill later demonstrate in Fig. 4 that the different\ntasks or datasets prefer different layer-wise skim-\nming strategies, which are learned by our method.\nWe will further explain the results in Sec. 5.4.\n3.2 Design Choices\nIn our work, we also jointly consider other de-\nsign choices regarding the skimming optimization,\nwhich includes the choice of input to the skimming\nmodule and how to deal with the skimmed input.\nWe ﬁrst explain the choices made by prior works,\nand then explain the choice of our method.\nStrategy. For the skimming optimization meth-\nods described above, there can be different strate-\ngies regarding the implementation details. Gen-\nerally, the skimming strategy can be categorized\ninto search-based or learning-based approach, as\ndescribed in Tbl. 1. However, when applied to\nvarious downstream NLP tasks and datasets, the\ndynamic skimming scheme prefers different layer-\nwise strategies as we mentioned above. This layer-\nwise skimming characteristics makes the search-\nbased approach not scalable and generally appli-\ncable. In contrast, our method enables the joint\ntraining of skimming strategy and downstream task\n, which leads to better skimming decisions with\nreference to both efﬁciency and accuracy. LTP is\nthe only by prior works adopting learning-based\nmethod, which, however, uses the soft-masking\napproach and suffers from the training-inference\nmismatch.\nInput for Skimming. POWER-BERT, LAT and\nLTP treat the attention weight value as importance\nscore and utilize it as the criterion for making the\nskimming decision. Compared to this value-based\nmethod (Guan et al., 2020), TR-BERT uses hidden\nstate embeddings as input feature. In our work, we\nuse the hidden state embeddings because they en-\nclose contextual information of the corresponding\ninput token. Our work shows that the joint training\nof skimming module and backbone Transformer\nmodel leads to that the embeddings also learn to\ncarry features for skimming prediction.\nSkimming Tokens. For the tokens pruned dy-\nnamically by the skimming decision during pro-\ncessing, it is natural to remove them from all the\nsuccessive layers. However, LAT and TR-BERT\npropose to forward such tokens to the ﬁnal out-\nput of the Transformer encoder, which keeps the\ndimension of the Transformer output unchanged.\nOur work adopts the forward-based design because\nit is more friendly for the Transformer decoder\nmodule on downstream tasks.\n4 Transkimmer Methodology\n4.1 Transformer with Skim Predictor\nTo predict which tokens to be pruned, we append\nan extra prediction module before each layer as\nshown in Fig. 2. This prediction module outputs\na skimming mask M, which is used to gather the\nhidden state embedding H at the sequence length\ndimension. The pruned embedding is then feed to\nthe Transformer layer as its input.\nHi+1 = Transkimmer i(Hi)\n= Transformer i(Gather(Hi, Mi))\n(1)\nIn the skim mask, we use output 1 to denote re-\nmaining tokens and 0 to denote pruned tokens. The\ngathering operation is to select the input tensor with\na provided mask. By optimizing this stand-alone\nskim module, syntactically redundant and seman-\ntically irrelevant tokens are skimmed and pruned.\nThe proposed skim predictor module is a multi-\nlayer perceptron (MLP) network composed of 2\nlinear layers with a layer norm operation (Ba et al.,\n2016) and GeLU activation (Hendrycks and Gim-\npel, 2016). The activation function is an arbitrary\nfunction with discrete output as skim decision.\nMi = SkimPredictor (Hi)\n= Activation(MLP (Hi))\nwhere MLP = Linear(GeLU(LN(Linear)))\n(2)\nThis skim predictor introduces extra model param-\neters and computation overhead. However, both of\nthem are very small compared to the vanilla Trans-\nformer model, which are about 7.9% and 6.5%\nrespectively. We demonstrate later that the compu-\ntation overhead of skim module is much smaller\nthan the beneﬁts brought by the reduction of input\ntensor through skimming.\n7278\nTranskimmer Layer i\nTranskimmer Layer i-1\nTranskimmer Layer i+1\nSkim Predictor\nGather\nSkim Mask Mi\nSkim Attention\nFFN\nDownstream Classifier\nDonwstream LossSkim Loss\nTotal Loss\n+\n+\n+\n+\nTransformer Layer\n10010\nHidden States Embedding Hi\nGumbelSoftmax\nMLP\nFigure 2: Architecture and end-to-end optimization objective of Transkimmer. The dashed token embeddings are\ndirectly forwarded to the ﬁnal output of Transformer layers without further processing.\nFor the tokens pruned by the skim module at\neach layer, we forward the these pruned hidden\nstate embeddings to the last Transformer layer. As\nsuch, the ﬁnal output of the whole Transformer\nmodel is composed of token embeddings skimmed\nat all layers and the ones processed by all layers\nwithout being skimmed.\nHL =\nL−1∑\ni=0\nHi · Mi (3)\nAnd this output is used for classiﬁcation layers on\nvarious downstream tasks. This makes the skim-\nming operation also compatible for token classiﬁ-\ncation tasks such as extractive question answering\n(QA) and named entity recognition (NER). This\nalso restores the once abandoned information for\ndownstream tasks.\n4.2 End-to-End Optimization\nIn the above discussion, we have described that\nTranskimmer can be easily augmented to a back-\nbone model without modiﬁcation to its current\nstructure. Furthermore, Transkimmer is also capa-\nble to utilize the pre-trained model parameters and\nﬁnetune the Transkimmer activated Transformer-\nbased models on downstream tasks. With an extra\nskim loss appended to the optimization object, this\nﬁne-tuning process is also performed end-to-end\nwithout changing its origin paradigm.\nSkim Attention. In the training procedure, Tran-\nskimmer does not prune the hidden state tensors as\nit does in the inference time. Because the gather-\ning and pruning operation of a portion of tokens\nprevents the back-propagation of their gradients.\nThe absence of error signal from negative samples\ninterference the convergence of the Transkimmer\nmodel. Therefore, we propose skim-attention to\nmask the reduced tokens in training instead of ac-\ntually pruning them. The attention weights to the\nskimmed tokens are set to 0 and thus unreachable\nby the other tokens.\nSkimAttn(Hi) =Attn(Hi) · Mi (4)\nBy doing so, the remaining tokens will have the\nidentical computational value as actually pruning.\nAnd the gradient signal is passed to the skim predic-\ntor module from the skim attention multiplication.\nGumbel Softmax. Following the discussion in\nSec. 3.1, the output decision mask of skim predictor\nis discrete and non-differentiable. To conquer this\ninability of back propagation, we use the reparame-\nterization method (Jang et al., 2017) to sample the\ndiscrete skim prediction from the output probabil-\nity distribution πi of the MLP. The gradient of the\nnon-differentiable activation function is estimated\nfrom the Gumbel-Softmax distribution during back\npropagation.\nMi\nj = Activation(πi\nj) ,for j = 0,1\n= GumbelSoftmax(πi\nj)\n=\nexp((log(πi\nj) +gi\nj)/τ)\n∑1\nk=0 exp((log(πi\nk) +gi\nk)/τ)\n(5)\ngi\nj are independent and identically sampled from\nGumbel(0,1) distribution. τ is the temperature\nhyper-parameter controlling the one-hot prediction\ndistribution. We take τ = 0.1 for all experiments.\n7279\nDataset CoLA RTE QQP MRPC SST-2 MNLI WNLI QNLI STS-B SQuAD IMDB YELP 20News\nTask Acceptability NLI Similarity Paraphrase Sentiment NLI NLI QA Similarity QA Sentiment Sentiment Sentiment\nAverage Sample Length 11 64 30 53 25 39 37 51 31 152 264 179 551\nInput Sequence Length 64 256 128 128 64 128 128 128 64 384 512 512 512\nHarmony Coefﬁcient 0.3 0.8 0.2 0.5 0.3 0.2 0.5 0.1 0.3 0.8 0.5 0.5 0.5\nTable 2: Summary of evaluation datasets. The input sequence length matches the setting of prior works POWER-\nBERT and LTP. It is determined by covering 99 percentile of input samples without truncation.\nTo achieve better token sparsiﬁcation ratio, we\nfurther add a skim loss term to the overall optimiza-\ntion objective as follows\nLossskim = 1\nL\n1∑\nL−1\nsum(Mi)\nlen(Mi) . (6)\nThe skim loss is essentially the ratio of tokens re-\nmained in each layer thus representing the com-\nputation complexity speedup. By decreasing this\nobjective, more tokens are forced to be pruned dur-\ning processing. To collaborate with the original\ndownstream task loss, we use a harmony coefﬁ-\ncient λto balance the two loss terms. As such, the\ntotal loss used for training is formulated as\nLosstotal = Lossdownstream + λLossskim. (7)\nWith the use of the previous settings, the Tran-\nskimmer model is trained end-to-end without any\nchange to its original training paradigm.\nUnbalanced Initialization. Another obstacle is\nthat skimming tokens during the training process\nmakes it much unstable and decreases its accu-\nracy performance. With the pre-trained language\nmodeling parameters, the skim predictor module is\nrandom initialized and predicts random decisions.\nThis induces signiﬁcant processing mismatch in\nthe backbone Transformer model, where all tokens\nare accessible. Consequently, the randomly initial-\nized skim predictor makes the training unstable and\ndiverged. We propose an unbalance initialization\ntechnique to solve this issue. The idea is to force\npositive prediction at ﬁrst and learn to skim gradu-\nally. Generally, parameters are initialized by zero\nmean distribution as\nω∼ N(0,σ). (8)\nWe propose to initialize the bias vector of the last\nlinear layer in the skim predictor MLP with unbal-\nanced bias as\nβi ∼ N((−1)i+1µ0,σ), (9)\nwhere i stands for the bias vector for prediction\n1 or 0. Consequently, the skim predictor tends to\nreserve tokens rather than skimming them when\ninnocent. The mean value µ0 of the unbalanced\ndistribution set to 5 for all the experiments.\n5 Evaluation\n5.1 Setup\nDatasets. We evaluate the proposed Transkim-\nmer method on various datasets. We use the\nGLUE(Wang et al., 2019) benchmark includ-\ning 9 classiﬁcation/regression datasets, extrac-\ntive question answering dataset SQuAD-v2.0, and\nsequence classiﬁcation datasets 20News (Lang,\n1995), YELP (Zhang et al., 2015) and IMDB (Maas\net al., 2011). These datasets are all publicly acces-\nsible and the summary is shown in Tbl. 2. The\ndiversity of tasks and text contexts demonstrates\nthe general applicability of the proposed method.\nModels. We follow the setting of the BERT\nmodel to use the structure of the Transformer\nencoder and a linear classiﬁcation layer for all\nthe datasets. We evaluate the base setting with\n12 heads and 12 layers in prior work (Devlin\net al., 2019). We implement Transkimmer upon\nBERT and RoBERTa pre-trained language model\non downstream tasks.\nBaselines. We compare our work to prior token\nreduction works including POWER-BERT (Goyal\net al., 2020), Length-Adaptive Transformer (LA-\nTransformer) (Kim and Cho, 2021), Learned Token\nPruning (LTP) (Kim et al., 2021), DeFormer (Cao\net al., 2020) and TR-BERT (Kim et al., 2021). We\nalso compare our method with model compression\nmethods of knowledge distillation and weight shar-\ning. Knowledge distillation uses a teacher model to\ntransfer the knowledge to a smaller student model.\nHere we adopt DistilBERT (Sanh et al., 2019) set-\nting to distill a 6-layer model from the BERTbase\nmodel. By sharing weight parameters among lay-\ners, the amount of weight parameters reduces. Note\nthat weight sharing does not impact the computa-\n7280\nMethod Padding COLA RTE QQP MRPC SST-2 MNLI WNLI QNLI STS-B\nMatthews FLOPs Acc. FLOPs Acc. FLOPs F1 FLOPs Acc. FLOPs Acc. FLOPs Acc. FLOPs Acc. FLOPs Pearson FLOPs\nBERTbase Baseline - 57.8 1.00 × 65.7 1.00× 91.3 1.00× 88.9 1.0 × 93.0 1.00× 84.9 1.00× 56.3 1.00× 91.4 1.00× 88.6 1.00 ×\nDeeBERT - - - 66.7 1.50× - - 85.2 1.79 × 91.5 1.89× 80.0 1.59× - - 87.9 1.79 × - -\nPOWER-BERT Sequence 52.3 4.50 × 67.4 3.40× 90.2 4.50× 88.1 2.70× 92.1 2.40× 83.8 2.60× - - 90.1 2.00 × 85.1 2.00 ×\nLAT Sequence - - - - - - - - 92.8 2.90× 84.4 2.80× - - - - - -\nTranskimmer No 58.9 1.75 × 68.9 2.85× 90.8 2.79× 88.5 3.13× 92.3 1.58× 83.2 2.02× 56.3 5.56× 90.5 2.33× 87.4 3.45 ×\nTranskimmer Sequence 58.9 18.9 × 68.9 4.67× 90.8 11.72× 88.5 7.45× 92.3 10.89× 83.2 6.65× 56.3 18.10× 90.5 6.01× 87.4 18.20 ×\nDistilBERT - 55.7 1.98 × 58.8 1.98× 90.3 1.98× 88.3 1.98× 90.6 1.98× 87.5 1.98× 53.5 1.98× 89.3 1.98× 87.0 1.98 ×\n+Transkimmer No 55.1 3.52 × 59.2 4.12× 90.1 4.95× 87.8 9.92× 89.5 5.01× 86.7 4.40× 56.3 10.41× 87.5 4.04× 86.5 3.47 ×\nALBERT - 58.3 0.99 × 70.7 0.99× 90.2 0.99× 90.4 0.99× 90.9 0.99× 81.8 0.99× 56.3 0.99× 89.2 0.99× 90.4 0.99 ×\n+Transkimmer No 53.4 1.52 × 71.5 1.57× 90.2 3.09× 90.6 1.94× 90.1 3.25× 81.5 1.67× 57.7 6.19× 90.1 2.30× 89.8 1.46 ×\nRoBERTabase Baseline - 61.8 1.00 × 78.0 1.00× 90.4 1.00× 92.1 1.00× 94.3 1.00× 87.5 1.00× 56.6 1.00× 92.9 1.00× 90.9 1.00 ×\nLTP Batch - - 78.0 1.81× 89.7 2.10× 91.6 2.10× 93.5 2.09× 86.5 1.88× - - 92.0 1.87× 90.0 1.95 ×\nTranskimmer No 61.3 1.52 × 76.2 1.79× 91.0 4.92× 91.9 2.67× 93.5 2.08× 86.7 2.19× 56.3 8.41× 91.7 2.85× 90.5 2.70 ×\nTable 3: Performance and FLOPs (speedup) on GLUE benchmark with BERT base and RoBERTabase as backbone\nmodel. Transkimmer is adopted on DistilBERT and ALBERT to shows its applicability to general model compres-\nsion methods.\nSQuADv2.0 20News Yelp IMDB\nModel Padding F1 FLOPs Acc. FLOPs Acc. FLOPs Acc. FLOPs\nBERTbase 77.1 1.00× 86.7 1.00× 69.9 1.00× 94.0 1.00×\nTR-BERT No 75.7 2.08 × 87.4 4.22× 70.0 2.19× 93.6 2.26×\nPOWER-BERT Sequence - - 86.5 2.91 × 67.4 2.75× 92.1 3.05×\nLAT Batch - - - - - - 92.5 2.70 ×\nDeFormer Sequence 71.4 2.19 × - - - - - -\nTranskimmer No 75.7 2.10× 86.1 5.27× 70.1 2.51× 93.7 2.70×\nTable 4: Performance and FLOPs evaluation on sev-\neral downstream tasks and datasets with BERT base as\nbackbone model. The speedup results are emphasized\nconsidering the padding setting.\ntion FLOPs (ﬂoating-point operations). We eval-\nuate Transkimmer on ALBERT (Lan et al., 2020)\nthat shares weight parameters among all layers. To\nexpress that token reduction method is compatible\nwith these model compression methods, we further\nimplement Transkimmer method with this works to\ndemonstrate their cooperation effect. Besides, Dee-\nBERT(Xin et al., 2020) is a Transformer early exit\nbaseline which can be regarded as coarse-grained\ninput skimming.\nPadding. While processing batched input sam-\nples, Transformer models perform a padding opera-\ntion on the input sequences to align the input length.\nSequences are appended with a special padding\ntoken [PAD] to a predeﬁned sequence length for\nthe convenience of successive computing. This is\na trivial setting for general evaluation but could\nlead to possible pseudo speedup for token reduc-\ntions works. Because the padded tokens can be\npruned without prediction. For the prior works,\nthere are three evaluation settings with reference\nto padding, padding to a ﬁxed sequence length,\npadding to mini-batch maximum length and no\npadding (denoted as Sequence, Batch and No in\nFig. 3 & 4). We indicate the padding methods of\nprior works and evaluate Transkimmer with differ-\nent padding settings for a fair comparison. The\nspeedup of padding to mini-batch maximum length\nsetting is related to batch size and processing or-\nder of input samples. So it is difﬁcult to make a\ndirect comparison under this setting. However, it\ncan be estimated with padding to ﬁxed sequence\nlength as upper bound and no padding as lower\nbound. The sequence length on different datasets is\ndetermined following prior works’ settings (Goyal\net al., 2020; Kim et al., 2021). We measure the\ninference FLOPs as a general measurement of the\nmodel computational complexity on all platforms.\nWe use the TorchProﬁle( ?) tool to calculate the\nFLOPs for each model.\nTraining Setting. We implement the proposed\nmethod based on open-sourced library from Wolf\net al. (2020)1. For each baseline model, we use the\nreleased pre-trained checkpoints 2. We follow the\ntraining setting used by Devlin et al. (2019) and\nLiu et al. (2019) to perform the ﬁne-tuning on the\nabove datasets. We perform all the experiments\nreported with random seed 42. We use four V100\nGPUs for training experiments.\nThe harmony coefﬁcient λ is determined by\nhyper-parameter grid search on development set\nwith 20% data random picked from training set set.\nThe search space is from 0.1 to 1 with a step of 0.1.\n5.2 Overall Results\nWe show the overall results on several datasets and\ndemonstrate our observations. Tbl. 3 demonstrates\nthe accuracy and speedup evaluated on GLUE\nbenchmark. And Tbl. 4 further demonstrates the\nresults on other datasets with longer input.\n1The source code is available at https://github.\ncom/ChandlerGuan/Transkimmer.\n2We use pre-trained checkpoints from Wolf et al. (2020).\n7281\n1.0 1.5 2.0 2.5 3.0 3.5 4.0\n82\n84\n86\n88\n90Accuracy\nMRPC\nTranskimmer\nPOWER-BERT\nDistillBERT\nLTP\n1.0 1.5 2.0 2.5 3.0 3.5 4.0\nSpeedup (FLOPs)\n55\n60\n65\n70\n75F1\nSQuAD-v2.0\nTranskimmer\nTR-BERT\nDistilBERT\nDeFormer\nFigure 3: Trade-off results between accuracy and\nspeedup of MRPC and SQuAD-v2.0 datasets by tuning\nthe harmony coefﬁcient. Note that different padding\nsettings are used for each baseline while Transkimmer\ndoesn’t count any padding.\nComparison to vanilla model baseline. Gener-\nally, Transkimmer achieves considerably speedup\nto the vanilla models with a minor accuracy degra-\ndation, which is less than 1% for nearly all cases.\nThe average speedup is 2.81× on GLUE bench-\nmark and over 2× on the other datasets. This\ndemonstrates the inference efﬁciency improvement\nof the Transkimmer input reduction method. We\nalso evaluate Transkimmer with RoBERTa model\nas backbone and reach 3.24× average speedup on\nGLUE benchmark. This result further expresses the\ngeneral applicability of Transkimmer with different\nTransformer-based pre-trained language models.\nAmong all the datasets we evaluated, Transkimmer\ntends to have better acceleration ratio on the easier\nones. For example, sequence classiﬁcation tasks\nlike QQP and STS-B are better accelerated than\nQA or NLI datasets. We suggest that the Trans-\nformer backbone is able to process the information\nat shallower layers and skim the redundant part\nearlier. This is also demonstrated in the following\npost-hoc analysis Sec. 5.4.\nComparison to input reduction prior works.\nAs shown in Tbl. 3, Transkimmer outperforms\nall the input reduction methods by a margin on\nGLUE benchmark. To make a fair comparison, we\nevaluate Transkimmer with two padding settings,\npadding to ﬁxed sequence length or no padding.\nFor most cases, Transkimmer has better accuracy\nperformance and higher speedup ratio at the same\ntime. When taking the special padding token\ninto account, Transkimmer is able to accelerate\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n0\n20\n40\n60\n80\n100Number of Tokens (%)\nMRPC\nWNLI\nSTS-B\nCOLA\nSST-2\nQNLI\nQQP\nRTE\nMNLI\nFigure 4: Layer-wise skim strategies analysis of\ndatasets from GLUE benchmark. The normalized area\nunder curve is viewed as an approximate speedup ratio\nwith reference to sequence length.\nBERTbase model for 10.97× on GLUE benchmark.\nTranskimmer also outperforms the other methods\non tasks shown in Tbl. 4. TR-BERT has the closet\nperformance compared with Transkimmer but with\na much complicated RL paradigm and larger search\nspace.\nComparison to model compression methods.\nThe comparison to two model compression meth-\nods is shown in Tbl. 3. Transkimmer outperforms\nthe knowledge distillation and weight sharing base-\nline by a margin. Besides, the dynamic skimming\nidea itself is orthogonal to this existing model com-\npression methods. To elaborate, we further adopt\nthe proposed Transkimmer method on DistilBERT\nand ALBERT models. With the proposed end-to-\nend training objective, Transkimmer is easily aug-\nmented to these methods. There is also no need\nto change the original training process. The re-\nsult shows that the Transkimmer method further\naccelerates the inference efﬁciency of compressed\nmodels with nearly no extra accuracy degradation.\n5.3 Accuracy and Performance Trade-Off\nFig. 3 demonstrates the accuracy and performance\ntrade-off analysis by tuning the harmony coefﬁ-\ncient. We show the results on MRPC and SQuAD-\nv2.0 datasets to give comparisons with different\nbaselines. It is shown that Transkimmer achieves a\nbetter accuracy to speedup Pareto curve compared\nto prior works. Transkimmer is able to provide bet-\nter acceleration gain with less accuracy degradation.\nEspecially, Transkimmer has a 1.5× speedup with-\nout accuracy loss. The result validates our design\ndecisions analyzed in the input reduction search\nspace choices.\n7282\nDataset Example\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nSST-2 [CLS] Even horror fans will most likely not ﬁnd what they’re seeking with trouble every day; the movie lacks both\nthrills and humor. [SEP]\nSQuADQuestion: [CLS] In what country is Normandy located? [SEP]\nContext: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in\nthe 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse\n(\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their\nleader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and\nmixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with\nthe Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the normans emerged\ninitially in the ﬁrst half of the 10th century, and it continued to evolve over the succeeding centuries. [SEP]\nAnswer: France\nTable 5: Post-hoc case study of SST-2 sentimental analysis and SQuAD QA tasks from Transkimmer model with\nBERTbase setting. The color indicated by the colorbar represents the Transformer layer index where the token is\npruned. Speciﬁcally, the black tokens are fully processed without being skimmed.\n5.4 Post-hoc Analysis\nSkim Strategy. Fig. 4 is the result of the num-\nber of tokens remained for the processing of each\nTransformer layer. The normalized area under each\ncurve is a rough approximation of the speedup ratio\nwith reference to the tokens number. By end-to-end\noptimization, Transkimmer learns signiﬁcant dis-\ntinguished strategies on different tasks. On WNLI\ndataset, over 90% of tokens are pruned within the\nﬁrst 3 layers and guarantees a high acceleration\ngain. The steer cliff at layer 7 on COLA demon-\nstrates a large portion of skimming at this particular\nposition. We suggest that this is because the pro-\ncessing of contextual information is sufﬁcient for\nthe skimming decision at this speciﬁc layer.\nPost-Hoc Case Study. Moreover, several post-\nhoc case studies are demonstrated with Tbl. 5. In\nthe SST-2 sentimental analysis example, the deﬁ-\nnite articles and apostrophes are discarded at the\nbeginning. And all words are encoded in contex-\ntual hidden states embeddings and gradually dis-\ncarded except for a few signiﬁcant key words. Only\nthe special token [CLS] is fully processed in this\nexample for ﬁnal sentimental classiﬁcation. How-\never, on the token classiﬁcation task example from\nSQuAD dataset, all tokens are given to the down-\nstream classiﬁer to predict the answer position. The\nanswer tokens are processed by all Transformer lay-\ners. Similarly, the question part is also kept with\ntokens containing enough information. Another\ndetail worth mentioning is that we use subword\ntokenization for the SQuAD dataset. As such, sub-\nword tokens of the same word might be discarded\nat different layers. For instance, the word Francia\nis tokenized intofran- and -cia two subword tokens,\nwhich are pruned at layer 4 and 6 respectively.\n6 Conclusion\nInput skimming or dynamic input reduction is an\nemerging Transformer model acceleration method\nstudied by many works recently. This idea uti-\nlizes the semantic structure of language and the\nsyntactic information of the input context for in-\nference acceleration. Compared to static model\nweight compression methods, input skimming ex-\nplores the redundancy in the input and hidden state\ntensors. As such, it is orthogonal and compatible\nwith those model compression algorithms with its\ndynamic feature.\nIn this work, we propose an accurate and efﬁ-\ncient Transformer inference acceleration method\nby teaching it how to skim input contents. The\nproposed Transkimmer method is trained with\nan easy and end-to-end paradigm. Furthermore,\nTranskimmer is also generally applicable to var-\nious Transformer-based model structures. It is\neven compatible with the static model compression\nmethods like knowledge distillation and weight\nsharing. We believe that the above features guar-\nantee the Transkimmer method a wide range of\napplicable production scenarios.\nAcknowledgement\nThis work was supported by the National Key R&D\nProgram of China under Grant 2021ZD0110104,\nthe National Natural Science Foundation of China\n(NSFC) grant (U21B2017, 62106143, 62072297,\nand 61832006), and Shanghai Pujiang Program.\nWe would like to thank the reviewers of ACL\nrolling review for their supportive comments and\nsuggestions. Jingwen Leng and Minyi Guo are the\ncorresponding authors of this paper.\n7283\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nVíctor Campos, Brendan Jou, Xavier Giró-i-Nieto,\nJordi Torres, and Shih-Fu Chang. 2018. Skip RNN:\nlearning to skip state updates in recurrent neural net-\nworks. In 6th International Conference on Learn-\ning Representations, ICLR 2018, Vancouver, BC,\nCanada, April 30 - May 3, 2018, Conference Track\nProceedings. OpenReview.net.\nQingqing Cao, Harsh Trivedi, Aruna Balasubrama-\nnian, and Niranjan Balasubramanian. 2020. De-\nFormer: Decomposing pre-trained transformers for\nfaster question answering. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4487–4497, Online. As-\nsociation for Computational Linguistics.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nChenhe Dong, Guangrun Wang, Hang Xu, Jiefeng\nPeng, Xiaozhe Ren, and Xiaodan Liang. 2021. Efﬁ-\ncientbert: Progressively searching multilayer percep-\ntron via warm-up knowledge distillation. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 1424–1437.\nSaurabh Goyal, Anamitra Roy Choudhury, Saurabh\nRaje, Venkatesan T. Chakaravarthy, Yogish Sabhar-\nwal, and Ashish Verma. 2020. Power-bert: Ac-\ncelerating BERT inference via progressive word-\nvector elimination. In Proceedings of the 37th In-\nternational Conference on Machine Learning, ICML\n2020, 13-18 July 2020, Virtual Event, volume 119 of\nProceedings of Machine Learning Research, pages\n3690–3699. PMLR.\nXiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen\nChen, and Jiawei Han. 2021. On the transformer\ngrowth for progressive BERT training. In Proceed-\nings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5174–5180, Online. Association for Computational\nLinguistics.\nYue Guan, Jingwen Leng, Chao Li, Quan Chen, and\nMinyi Guo. 2020. How far does BERT look at:\nDistance-based clustering and analysis of BERT’s\nattention. In Proceedings of the 28th Interna-\ntional Conference on Computational Linguistics ,\npages 3853–3860, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nYue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin,\nMinyi Guo, and Yuhao Zhu. 2021. Block-skim: Ef-\nﬁcient question answering for transformer. arXiv\npreprint arXiv:2112.08560.\nCong Guo, Bo Hsueh, Jingwen Leng, Yuxian Qiu,\nYue Guan, Zehuan Wang, Xiaoying Jia, Xipeng Li,\nMinyi Guo, and Yuhao Zhu. Accelerating sparse\ndnn models without hardware-support via tile-wise\nsparsity. In 2020 SC20: International Conference\nfor High Performance Computing, Networking, Stor-\nage and Analysis (SC), pages 204–218. IEEE Com-\nputer Society.\nCong Guo, Yuxian Qiu, Jingwen Leng, Xiaotian Gao,\nChen Zhang, Yunxin Liu, Fan Yang, Yuhao Zhu,\nand Minyi Guo. 2022. SQuant: On-the-ﬂy data-free\nquantization via diagonal hessian approximation. In\nInternational Conference on Learning Representa-\ntions.\nChristian Hansen, Casper Hansen, Stephen Alstrup,\nJakob Grue Simonsen, and Christina Lioma. 2019.\nNeural speed reading with structural-jump-lstm. In\n7th International Conference on Learning Represen-\ntations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019. OpenReview.net.\nXuanli He, Iman Keivanloo, Yi Xu, Xiang He, Belinda\nZeng, Santosh Rajagopalan, and Trishul Chilimbi.\n2021. Magic pyramid: Accelerating inference with\nearly exiting and token pruning. arXiv preprint\narXiv:2111.00230.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Categor-\nical reparameterization with gumbel-softmax. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nGyuwan Kim and Kyunghyun Cho. 2021. Length-\nadaptive transformer: Train once with length drop,\nuse anytime with search. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 6501–6511, Online. As-\nsociation for Computational Linguistics.\n7284\nSehoon Kim, Sheng Shen, David Thorsley, Amir Gho-\nlami, Woosuk Kwon, Joseph Hassoun, and Kurt\nKeutzer. 2021. Learned token pruning for transform-\ners. arXiv preprint arXiv:2107.00910.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020. OpenReview.net.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nKen Lang. 1995. Newsweeder: Learning to ﬁlter\nnetnews. In Machine Learning Proceedings 1995,\npages 331–339. Elsevier.\nZhouhan Lin, Minwei Feng, Cícero Nogueira dos San-\ntos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua\nBengio. 2017. A structured self-attentive sen-\ntence embedding. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Pro-\nceedings. OpenReview.net.\nWeijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao,\nHaotang Deng, and Qi Ju. 2020. FastBERT: a self-\ndistilling BERT with adaptive inference time. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6035–\n6044, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nPriyadarshini Panda, Abhronil Sengupta, and Kaushik\nRoy. 2016. Conditional deep learning for energy-\nefﬁcient and enhanced pattern recognition. In 2016\nDesign, Automation & Test in Europe Conference &\nExhibition (DATE), pages 475–480. IEEE.\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J\nWilliams. 1986. Learning representations by back-\npropagating errors. nature, 323(6088):533–536.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nMin Joon Seo, Sewon Min, Ali Farhadi, and Hannaneh\nHajishirzi. 2018. Neural speed reading via skim-rnn.\nIn 6th International Conference on Learning Rep-\nresentations, ICLR 2018, Vancouver, BC, Canada,\nApril 30 - May 3, 2018, Conference Track Proceed-\nings. OpenReview.net.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efﬁcient transformers: A survey.\narXiv e-prints, pages arXiv–2009.\nSurat Teerapittayanon, Bradley McDanel, and Hsiang-\nTsung Kung. 2016. Branchynet: Fast inference via\nearly exiting from deep neural networks. In 2016\n23rd International Conference on Pattern Recogni-\ntion (ICPR), pages 2464–2469. IEEE.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nChunpei Wang and Xiaowang Zhang. 2020. Q-bert: A\nbert-based framework for computing sparql similar-\nity in natural language. In Companion Proceedings\nof the Web Conference 2020, pages 65–66.\nHanrui Wang, Zhekai Zhang, and Song Han. 2021.\nSpatten: Efﬁcient sparse attention architecture with\ncascade token and head pruning. In 2021 IEEE In-\nternational Symposium on High-Performance Com-\nputer Architecture (HPCA), pages 97–110. IEEE.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and\nSong Han. 2020. Lite transformer with long-short\nrange attention. In 8th International Conference on\n7285\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. DeeBERT: Dynamic early exiting\nfor accelerating BERT inference. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 2246–2251, On-\nline. Association for Computational Linguistics.\nDeming Ye, Yankai Lin, Yufei Huang, and Maosong\nSun. 2021. TR-BERT: Dynamic token reduction for\naccelerating BERT inference. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 5798–5809, On-\nline. Association for Computational Linguistics.\nAdams Wei Yu, Hongrae Lee, and Quoc Le. 2017.\nLearning to skim text. InProceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1880–\n1890, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer se-\nquences. arXiv preprint arXiv:2007.14062.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in Neural Information Pro-\ncessing Systems 28: Annual Conference on Neural\nInformation Processing Systems 2015, December 7-\n12, 2015, Montreal, Quebec, Canada, pages 649–\n657.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian\nMcAuley, Ke Xu, and Furu Wei. 2020. Bert loses\npatience: Fast and robust inference with early exit.\nAdvances in Neural Information Processing Systems,\n33.\n7286"
}