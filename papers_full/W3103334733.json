{
    "title": "Understanding the Difficulty of Training Transformers",
    "url": "https://openalex.org/W3103334733",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2100063962",
            "name": "Liyuan Liu",
            "affiliations": [
                "University of Illinois Urbana-Champaign",
                "Microsoft Research (United Kingdom)",
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2101917160",
            "name": "Xiaodong Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2104437897",
            "name": "Jian-Feng Gao",
            "affiliations": [
                "Microsoft Research (United Kingdom)",
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2108390110",
            "name": "Wei‐Zhu Chen",
            "affiliations": [
                "Microsoft (United States)",
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2103606203",
            "name": "Jiawei Han",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W4300537377",
        "https://openalex.org/W4288265053",
        "https://openalex.org/W2908336025",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W3066373881",
        "https://openalex.org/W2964050767",
        "https://openalex.org/W2952626150",
        "https://openalex.org/W4295773261",
        "https://openalex.org/W2777662428",
        "https://openalex.org/W2251994258",
        "https://openalex.org/W4288621368",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3034465644",
        "https://openalex.org/W2962933129",
        "https://openalex.org/W2125930537",
        "https://openalex.org/W2952809536",
        "https://openalex.org/W2989571009",
        "https://openalex.org/W3010768098",
        "https://openalex.org/W3034772996",
        "https://openalex.org/W4297733535",
        "https://openalex.org/W3041866211",
        "https://openalex.org/W4298395628",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963037478",
        "https://openalex.org/W2622263826",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W2964045208",
        "https://openalex.org/W2107878631",
        "https://openalex.org/W2962761235",
        "https://openalex.org/W2796108585",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2919624000",
        "https://openalex.org/W2963858333",
        "https://openalex.org/W2979636403",
        "https://openalex.org/W2964093309",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2994689640",
        "https://openalex.org/W2970389371",
        "https://openalex.org/W1677182931",
        "https://openalex.org/W2948981900",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2992505801"
    ],
    "abstract": "Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand __what complicates Transformer training__ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially—for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage’s training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance",
    "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5747–5763,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n5747\nUnderstanding the Difﬁculty of Training Transformers\nLiyuan Liu†‡ Xiaodong Liu‡ Jianfeng Gao‡ Weizhu Chen§ Jiawei Han†\n{ll2, hanj}@illinois.edu , {xiaodl,jfgao,wzchen}@microsoft.com\n†University of Illinois at Urbana-Champaign\n‡Microsoft Research\n§ Microsoft Dynamics 365 AI\nAbstract\nTransformers have proved effective in many\nNLP tasks. However, their training requires\nnon-trivial efforts regarding carefully design-\ning cutting-edge optimizers and learning rate\nschedulers ( e.g., conventional SGD fails to\ntrain Transformers effectively). Our objective\nhere is to understand what complicates Trans-\nformer training from both empirical and theo-\nretical perspectives. Our analysis reveals that\nunbalanced gradients are not the root cause of\nthe instability of training. Instead, we identify\nan ampliﬁcation effect that inﬂuences training\nsubstantially–for each layer in a multi-layer\nTransformer model, heavy dependency on its\nresidual branch makes training unstable, since\nit ampliﬁes small parameter perturbations ( e.g.,\nparameter updates) and results in signiﬁcant\ndisturbances in the model output. Yet we ob-\nserve that a light dependency limits the model\npotential and leads to inferior trained models.\nInspired by our analysis, we propose Admin\n(Adaptive model initialization) to stabilize the\nearly stage’s training and unleash its full po-\ntential in the late stage. Extensive experiments\nshow that Admin is more stable, converges\nfaster, and leads to better performance 1.\n1 Introduction\nTransformers ( Vaswani et al. , 2017) have led to a\nseries of breakthroughs in various deep learning\ntasks ( Devlin et al. , 2019; Velickovic et al., 2018).\nThey do not contain recurrent connections and\ncan parallelize all computations in the same layer,\nthus improving effectiveness, efﬁciency, and scal-\nability. Training Transformers, however, requires\nextra efforts. For example, although stochas-\ntic gradient descent (SGD) is the standard algo-\nrithm for conventional RNNs and CNNs, it con-\nverges to bad/suspicious local optima for Trans-\n1Implementations are released at: https://github.\ncom/LiyuanLucasLiu/Transforemr-Clinic\n0 50 100\n4.4\n4.6\n4.8\n5.0\nDev PPL on WMT’14 En-De\n18-Layer Transformer\n50 100\n4.6\n4.7\n4.8\n4.9 6-layer Post-LN\nconverges, but\n18-layer Post-LN\ndoes not.\n6-Layer Transformer\nPre-LN\nPost-LN\nAdmin\n(Post-LN)\nEpoch # (iterations over the training set)\nFigure 1: Lacking enough robustness and stability, the\n18-Layer Post-LN Transformer training ( i.e.the original\narchitecture) diverges and is omitted in the left graph.\nAdmin not only stabilizes model training but unleashes\nthe model potential for better performance.\nformers ( Zhang et al. , 2019b). Moreover, com-\nparing to other neural architectures, removing the\nwarmup stage in Transformer training results in\nmore severe consequences such as model diver-\ngence ( Popel and Bojar , 2018; Liu et al. , 2020a).\nHere, we conduct comprehensive analyses in em-\npirical and theoretical manners to answer the ques-\ntion: what complicates Transformer training .\nOur analysis starts from the observation: the\noriginal Transformer (referred to as Post-LN) is\nless robust than its Pre-LN variant 2 (Baevski\nand Auli , 2019; Xiong et al. , 2019; Nguyen and\nSalazar, 2019). We recognize that gradient van-\nishing issue is not the direct reason causing such\ndifference, since ﬁxing this issue alone cannot sta-\nbilize Post-LN training. It implies that, besides\nunbalanced gradients, there exist other factors in-\nﬂuencing model training greatly.\nWith further analysis, we recognize that for\neach Transformer residual block, the dependency\n2As in Figure 2, Post-LN places layer norm outside of\nresidual blocks, and Pre-LN moves them to the inside.\n5748\nAttention\nAdd\nLayer Norm\nFFN\nAdd\nLayer Norm\nAttention\nAdd\nLayer Norm\nFFN\nAdd\nLayer Norm\nAttention\nAdd\nLayer Norm\nx\n( od )\n0x\n( oe )\n0\nLayer Norm\nAttention\nAdd\nLayer Norm\nFFN\nAdd\n⇥ N\nLayer Norm\nAttention\nAdd\nLayer Norm\nAttention\nAdd\nLayer Norm\nFFN\nAdd\n⇥ N\nx\n( pd )\n0 x\n( pe )\n0\nPre-LN Post-LN\nEncoder Encoder DecoderDecoder\n⇤ ( pd )\n⇤ ( pe )\n⇤ ( oe )\n⇤ ( od )\n: Pre-LN decoder\n: Pre-LN encoder\n: Post-LN encoder\n: Post-LN decoder\n: sub-layers outputs (i.e., FFN, Self-Attention and Encoder-Attention) \nNotation Table\n: intermediate output\n: residual output N : layer #\nD : hidden #\nH : head # ⇥ N\n⇥ N\nLayer Norm\nx\n( pe )\nLayer Norm\nx\n( pd )\nx\n( od )\nx\n( oe )\nx\n( od )\n3 i \u0000 3\nx\n( od )\n3 i \u0000 2\nx\n( od )\n3 i \u0000 1\nx\n( od )\n3 i\na\n( od )\n3 i \u0000 2\nb\n( od )\n3 i \u0000 2\na\n( od )\n3 i \u0000 1\nb\n( od )\n3 i \u0000 1\na\n( od )\n3 i\nb\n( od )\n3 i\nx\n( oe )\n2 i \u0000 2\nx\n( oe )\n2 i \u0000 1\nx\n( oe )\n2 i\na\n( oe )\n2 i \u0000 1\nb\n( oe )\n2 i \u0000 1\na\n( oe )\n2 i\nb\n( oe )\n2 i\nx\n( pe )\n2 i \u0000 1\nx\n( pe )\n2 i \u0000 2\nx\n( pe )\n2 i\nx\n( pd )\n3 i \u0000 3\nx\n( pd )\n3 i \u0000 2\nx\n( pd )\n3 i \u0000 1\nx\n( pd )\n3 i\nx\na\nb\nˆ⇤ : normalized outputs, i.e., Var[ ˆ⇤ ]=1\nVar[ · ] : dimension-wise variance\nFigure 2: The Architecture and notations of Pre-LN Transformers (Left) and Post-LN Transformers (Right).\non its residual branch 3 plays an essential role in\ntraining stability. First, we ﬁnd that a Post-LN\nlayer has a heavier dependency on its residual\nbranch than a Pre-LN layer. As in Figure 7, at ini-\ntialization, a Pre-LN layer has roughly the same\ndependency on its residual branch and any previ-\nous layer, whereas a Post-LN layer has a stronger\ndependency on its residual branch (more discus-\nsions are elaborated in Section 4.1). We ﬁnd that\nstrong dependencies of Post-LN amplify ﬂuctua-\ntions brought by parameter changes and destabi-\nlize the training (as in Theorem 2 and Figure 4).\nBesides, the loose reliance on residual branches in\nPre-LN generally limits the algorithm’s potential\nand often produces inferior models.\nIn light of our analysis, we propose Admin, an\nadaptive initialization method which retains the\nmerits of Pre-LN stability without hurting the per-\nformance. It restricts the layer dependency on its\nresidual branches in the early stage and unleashes\nthe model potential in the late stage. We conduct\nexperiments on IWSLT’14 De-En, WMT’14 En-\nDe, and WMT’14 En-Fr; Admin is more stable,\nconverges faster, and achieves better performance.\nFor example, without introducing any additional\nhyper-parameters, Admin successfully stabilizes\n72-layer Transformer training on WMT’14 En-Fr\nand achieves a 43.80 BLEU score.\n3For a residual block x + f(x), its shortcut output refers\nto x, its residual branch output refers to f(x), and the depen-\ndency on its residual branch refers to Var[f(x)]\nVar[x+f(x)] .\n2 Preliminaries\nTransformer Architectures and Notations. The\nTransformer architecture contains two types of\nsub-layers, i.e., Attention sub-layers and Feed-\nforward (FFN) sub-layers. They are composed\nof mainly three basic modules ( Vaswani et al. ,\n2017), i.e., Layer Norm ( fLN), Multi-head Atten-\ntion (fATT), and Feedforward Network ( fFFN).\nAs illustrated in Figure 2, the Pre-LN Trans-\nformer and the Post-LN Transformer organize\nthese modules differently. For example, a Pre-\nLN encoder organizes the Self-Attention sub-layer\nas x(pe)\n2i−1 = x(pe)\n2i−2 + fS-ATT(fLN(x(pe)\n2i−2)) and\na Post-LN encoder as x(oe)\n2i−1 = fLN(x(oe)\n2i−2 +\nfS-ATT(x(oe)\n2i−2)), where x(·)\n2i−2 is the input of the i-\nth Transformer layer and x(·)\n2i−1 is the output of\nthe i-th Self-Attention sub-layer. Here, we refer\nfS-ATT(fLN(x(pe)\n2i−2)) and fS-ATT(x(oe)\n2i−2) as the resid-\nual branches and their outputs as the residual out-\nputs, in contrast to layer/sub-layer outputs, which\nintegrates residual outputs and shortcut outputs.\nNotation elaborations are shown in Figure 2. In\nparticular, we use superscripts to indicate network\narchitectures ( i.e., the Pre-LN Encoder), use sub-\nscripts to indicate layer indexes (top layers have\nlarger indexes), all inputs and outputs are formu-\nlated as Sequence-Len × Hidden-Dim.\nLayer Norm. Layer norm ( Ba et al. , 2016) plays a\nvital role in Transformer architecture. It is deﬁned\n5749\n10°1\n100\nPre-LN Encoder Post-LN Encoder Pre-LN Deocder Post-LN Deocder\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\n10°1\n100\nGradient vanishing only happens in backpropagations for Encoder-Attention sub-layers\ni.e., from Encoder-Attention outputs to Self-Attention outputs.\nSelf Attention (PostLN Decoder) Encoder Attention (PostLN Decoder) Feedforward (PostLN Decoder)\nFigure 3: Relative gradient norm histogram (on a log scale) of 18-layer Transformers on the WMT’14 En-De\ndataset, i.e., the gradient norm of sub-layer outputs, scaled by the largest gradient norm in the same network.\nas fLN(x) = γ x−µ\nσ + ν, where µ and σ are the\nmean and standard deviation of x.\nFeedforward Network. Transformers use two-\nlayer perceptrons as feedforward networks, i.e.,\nfFFN(x) = ϕ(xW(1))W(2), where ϕ(·) is the non-\nlinear function4, and W(·) are parameters.\nMulti-head Attention. Multi-head Attentions\nallows the network to have multiple focuses\nin a single layer and plays a crucial role in\nmany tasks ( Chen et al. , 2018). It is de-\nﬁned as (with H heads): fATT(q, k, v) =PH\nh=1 fs(qW(Q)\nh W(K)\nh kT )vW(V1)\nh W(V2)\nh , where\nfs is the row-wise softmax function and W(·)\nh\nare parameters. W(Q)\nh and W(V1)\nh are D × D\nH\nmatrices, W(K)\nh and W(V2)\nh are D\nH × D matri-\nces, where D is the hidden state dimension. Pa-\nrameters without subscript refer the concatena-\ntion of all H-head parameters, e.g., W(Q) =\n[W(Q)\n1 , · · · , W (Q)\nH ]. In Transformer, this mod-\nule is used in two different settings: Encoder-\nAttention ( fE-ATT(x) = fATT(x, x(·e), x(·e)) and\nx(·e) is the encoder output), and Self-Attention\n(fS-ATT(x) = fATT(x, x, x)).\n3 Unbalanced Gradients\nIn this study, we strive to answer the question:\nwhat complicates Transformer training . Our anal-\nysis starts from the observation: Pre-LN training is\nmore robust than Post-LN, while Post-LN is more\nlikely to reach a better performance than Pre-LN.\n4Our analysis uses ReLU as the activation function, while\nAdmin can be applied to other non-linear functions.\nIn a parameter grid search (as in Figure 10), Pre-\nLN converges in all 15 settings, and Post-LN di-\nverges in 7 out of 15 settings; when Post-LN con-\nverges, it outperforms Pre-LN in 7 out of 8 set-\ntings. We seek to reveal the underlying factor that\ndestabilizes Post-LN training and restricts the per-\nformance of Pre-LN.\nIn this section, we focus on the unbalanced gra-\ndients ( e.g., gradient vanishing). We ﬁnd that, al-\nthough Post-LN suffers from gradient vanishing\nand Pre-LN does not, gradient vanishing is not the\ndirect reason causing the instability of Post-LN.\nSpeciﬁcally, we ﬁrst theoretically and empirically\nestablish that only Post-LN decoders suffer from\ngradient vanishing and Post-LN encoders do not.\nWe then observe that ﬁxing the gradient vanishing\nissue alone cannot stabilize training.\n3.1 Gradients at Initialization\nAs gradient vanishing can hamper convergence\nfrom the beginning, it has been regarded as the\nmajor issue causing unstable training. Also, re-\ncent studies show that this issue exists in the Post-\nLN Transformer, even after using residual connec-\ntions (Xiong et al. , 2019). Below, we establish that\nonly Post-LN decoders suffer from the gradient\nvanishing, and neither Post-LN encoders, Pre-LN\nencoders, nor Pre-LN decoders.\nWe use ∆x to denote gradients, i.e., ∆x = ∂L\n∂x\nwhere L is the training objective. Following pre-\nvious studies ( Glorot and Bengio , 2010), we ana-\nlyze the gradient distribution at the very beginning\nof training and ﬁnd only Encoder-Attention sub-\nlayers in Post-LN suffers from gradient vanishing.\n5750\n0 100 200\n0\n500\n1000\n1500\n2000\n100 101 1020\n50\n100\n150\n0 100 200100\n101\n102\n103\n104\n105\nPre-LN\nPost-LN\nAdmin (Post-LN)\nNum of Sub-Layers (FFN or Self-Attention) in the Encoder\nRandom Perturbations, i.e.,  Gradient Updates, i.e., \n|F ( x 0 ,W ) \u0000 F ( x 0 ,W\n⇤\n) |\n2\n2\nW\n⇤\n= W + \u0000\nPost-LN is less \nstable than Pre-LN \nPost-LN:|F \u0000 F ⇤\n| 2\n2 = O ( N )\nPre-LN |F \u0000 F\n⇤\n|\n2\n2 = O (log N )\nAdmin :\nR\n2\n=0 . 99R\n2\n=0 . 99\nW\n⇤\n= W + Adam( r W L ( F ))\nFigure 4: Encoder output changes for parameter changes, i.e., |F(x0, W ) −\nF(x0, W ∗)|2\n2 where W∗ − W is random perturbations (left) or gradient updates\n(right). Intuitively, very large |F − F ∗| indicates the training to be ill-conditioned.\n0 50 100\n10°2\n10°1\n100\nRelative\nGradient Norm\n10°1\n100\nRelative Parameter\nUpdate Norm\nEpoch # (iterations over the training set)\nThe update magnitude is \nconsistent,  even with \nunbalanced gradients.\n18-Layer Pre-LN  Encoder Self-Attention\nLight color indicates higher layers\nFigure 5: Histogram of rel-\native norm of gradient and\n|Wi+1 − Wi| where Wi is\nthe checkpoint saved after\ntraining for i epochs.\nEncoder Decoder Gradient Training\nPost-LN Post-LN Varnishing Diverged\nPost-LN Pre-LN Varnishing Diverged\nPre-LN Pre-LN Varnishing Converged\nTable 1: Changing decoders from Post-LN to Pre-LN\nﬁxes gradient vanishing, but does not stabilize model\ntraining successfully. Encoder/Decoder have 18 layers.\nFirst, we conduct analysis from a theoreti-\ncal perspective. Similar to Xiong et al. (2019),\nwe establish that Pre-LN networks do not suf-\nfer from gradient vanishing (as elaborated in Ap-\npendix A.1). Unlike Xiong et al. (2019), we rec-\nognize that not all Post-LN networks suffer from\ngradient vanishing. As in Theorem 1, we estab-\nlish that Post-LN Encoder networks do not suffer\nfrom gradient vanishing. Detailed derivations are\nelaborated in Appendix A.2.\nTHEOREM 1. — For Post-LN Encoders, if γ and\nν in the Layer Norm are initialized as 1 and 0 re-\nspectively; all other parameters are initialized by\nsymmetric distributions with zero mean; x(oe)\ni and\n∆x(oe)\ni are subject to symmetric distributions with\nzero mean; the variance of x(oe)\ni is 1 (i.e., normal-\nized by Layer Norm); ∆x(oe)\ni and the derivatives\nof modules in i-th sub-layer are independent, we\nhave Var[∆xi−1] ≥ Var[∆xi].\nTo make sure that the assumptions of Theo-\nrem 2 match the real-world situation, we further\nconduct empirical veriﬁcation. At initialization,\nwe calculate ||∆x(·)\ni ||2 for 18-layer Transformers 5\n5Note if E[∆x(p·)\ni−1] = 0 , Var[∆x(p·)\ni−1] ≈ | ∆x(p·)\ni−1|2\n2.\nand visualize ||∆x(·)\ni ||2\nmaxj ||∆x(·)\nj ||2\nin Figure 3. It veriﬁes\nthat only Post-LN decoders suffer from the gradi-\nent vanishing. Besides, we can observe that the\ndropping of gradient norms mostly happens in the\nbackpropagation from encoder-attention outputs\n(encoder-attention bars) to its inputs (self-attention\nbars, since the output of self-attention is the input\nof encoder-attention). This pattern is further ex-\nplained in Appendix A.3.\n3.2 Impact of the Gradient Vanishing\nNow, we explore whether gradient vanishing is the\ndirect cause of training instability.\nFirst, we design a controlled experiment to\nshow the relationship between gradient vanishing\nand training stability. We construct a hybrid Trans-\nformer by combining a Post-LN encoder and a\nPre-LN decoder. As in Section 3.1, only Post-LN\ndecoders suffer from gradient vanishing, but not\nPost-LN encoders. Therefore, this hybrid Trans-\nformer does not suffer from gradient vanishing.\nAs shown in Table 1, ﬁxing gradient vanishing\nalone ( i.e., changing Post-LN decoders to Pre-LN\ndecoders) fails to stabilize model training. This\nobservation provides evidence supporting that the\ngradient vanishing issue is not the direct cause of\nunstable Post-LN training.\nMoreover, we observe that gradients of all at-\ntention modules are unbalanced, while adaptive\noptimizers mostly address this issue. As in Fig-\nure 5, adaptive optimizers successfully assign dif-\nferent learning rates to different parameters and\nlead to consistent update magnitudes even with un-\nbalanced gradients. It explains why the standard\nSGD fails in training Transformers ( i.e., lacking\n5751\nAttentionFFN\nPre-LN\nAttention\na\npe\n1a\npe\n2\nLayer Norm\nAttentionFFN\nPost-LN\nAttention\nLayer Norm a\noe\n2 a\noe\n1\nLayer Norm\nFigure 6: The major difference between Pre-LN and\nPost-LN is the position of layer norms.\nthe ability to handle unbalanced gradients) and ne-\ncessitates using adaptive optimizers. More discus-\nsions are included in Appendix A.4.\n4 Instability from Ampliﬁcation Effect\nWe ﬁnd that unbalanced gradients are not the root\ncause of the instability of Post-LN, which implies\nthe existence of other factors inﬂuencing model\ntraining. Now, we go beyond gradient vanishing\nand introduce the ampliﬁcation effect. Speciﬁcally,\nwe ﬁrst examine the difference between Pre-LN\nand Post-LN, including their early-stage and late-\nstage training. Then, we show that Post-LN’s train-\ning instability is attributed to layer dependency’s\nampliﬁcation effect, which intensiﬁes gradient up-\ndates and destabilizes training.\n4.1 Impact of Layer Norms Positions\nAs described in Section 2, both Pre-LN and Post-\nLN employ layer norm to regularize inputs and\noutputs. Different residual outputs are aggregated\nand normalized in residual networks before serv-\ning as inputs of other layers ( i.e., residual outputs\nwill be scaled to ensure the integrated input to have\na consistent variance). To some extend, layer norm\ntreats the variance of residual outputs as weights\nto average them. For example, for Post-LN Self-\nAttention, we have x(o·)\n2i−1 =\nx(o·)\n2i−2+a(o·)\n2i−1√\nVar[x(o·)\n2i−2]+Var[a(o·)\n2i−1]\nat initialization. Larger Var[a(o·)\n2i−2] not only in-\ncreases the proportion of a(o·)\n2i−2 in x(o·)\n2i−2 but de-\ncreases the proportion of other residual outputs.\nIntuitively, this is similar to the weight mechanism\nof the weighted average.\nThe position of layer norms is the major dif-\nference between Pre-LN and Post-LN and makes\nthem aggregate residual outputs differently ( i.e.,\nusing different weights). As in Figure 6, all resid-\nual outputs in Pre-LN are only normalized once\nbefore feeding into other layers (thus only treating\nresidual output variances as weights); in Post-LN,\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\nx11\nx12\nx1\na1\nx2\na2\nx3\na3\nx4\na4\nx5\na5\nx6\na6\nx7\na7\nx8\na8\nx9\na9\nx10\na10\nx11\na11\nx12\na12\n a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12\n0⊿1\n0⊿2\n0⊿3\n0⊿4\n0⊿5\n0⊿6\n6-Layer Post-LN 6-Layer Pre-LN\nAt InitializationAfter 100 Epochs\nComparing ﬁnal models, Post-LN layer has a larger dependency on its residual branch.\nbranch outputs. branch outputs. \nPost-LN layer outputs always \ndepend more on its residual  \nPre-LN layer outputs learn to \n depend more on its residual\nFigure 7: βi,j in 6-Layer Post-LN and Pre-LN on the\nWMT-14 En-De dataset (contains 12 sub-layers).\nmost residual outputs are normalized more than\nonce, and different residual outputs are normalized\nfor different times. For example, if all layers are\ninitialized in the same way, output variances of dif-\nferent Pre-LN residual branches would be similar,\nand the aggregation would be similar to the simple\naverage. Similarly, for Post-LN, nearby residual\noutputs are normalized by fewer times than others,\nthus having relatively larger weights. We proceed\nto calculate and analyze these weights to under-\nstand the impact of layer norm positions.\nFirst, we use bai to refer ai√Varai\n(i.e., normal-\nized outputs of i-th residual branch) and bxi to re-\nfer xi√Varxi\n(i.e., normalized outputs of i-th layer\nor normalized inputs of ( i+1)-th residual branch).\nThen, we describe their relationships as bxi =P\nj≤i βi,jbaj, where βi,j integrates scaling opera-\ntions of all layer norms (including\np\nVar[ai]). For\nexample, Pre-LN sets βi,j =\n√\nVar[aj]√\nVar[∑\nk≤i ak] . Intu-\nitively, βi,j describes the proportion of j-th resid-\nual branch outputs in i-th layer outputs, thus re-\nﬂects the dependency among layers.\nWe visualize βi,j in Figure 7. For a Post-LN\nlayer, its outputs rely more on its residual branch\nfrom the initialization to the end. At initialization,\nPre-LN layer outputs have roughly the same re-\nliance on all previous residual branches. As the\ntraining advances, each layer starts to rely more on\nits own residual outputs. However, comparing to\nPost-LN, Pre-LN layer outputs in the ﬁnal model\nstill has less reliance on their residual branches.\nIntuitively, it is harder for Pre-LN layers to de-\npend too much on their own residual branches. In\n5752\nPre-LN, layer outputs ( i.e., x(p·)\ni ) are not normal-\nized, and their variances are likely to be larger for\nhigher layers 6. Since βi,i =\n√\nVar[ai]√\nVar[x(p·)\ni−1+ai]\n, βi,i\nis likely to be smaller for higher layers, which re-\nstricts i-th layer outputs from depending too much\non its residual branch and inhibits the network\nfrom reaching its full potential. In other words,\nPre-LN restricts the network from being too deep\n(i.e., if it is hard to distinguish x(p·)\ni and x(p·)\ni+1, ap-\npending one layer would be similar to doubling\nthe width of the last layer), while Post-LN gives\nthe network the choice of being wider or deeper.\n4.2 Ampliﬁcation Effect at Initialization\nAlthough depending more on residual branches al-\nlows the model to have a larger potential, it ampli-\nﬁes the ﬂuctuation brought by parameter changes.\nFor a network bx = F(x0, W ) where x0 is the\nmodel input and W is the parameter, the out-\nput change caused by parameter perturbations is\nVar[F(x0, W )−F(x0, W ∗)], where W∗ = W+δ.\nIts relationship with N is described in Theorem 2,\nand the derivation is elaborated in Appendix B.\nTHEOREM 2. — Consider a N-layer Transformer\nbx = F(bx0, W ) at initialization, where bx0 is the\ninput and W is the parameter. If the layer de-\npendency stays the same after a parameter change\n(i.e., βi,j has the same value after changing W to\nW∗, where W is randomly initialized and δ =\nW∗ − W is independent to W), the output change\n(i.e., Var[F(x0, W ) − F (x0, W ∗)]) can be esti-\nmated as PN\ni=1 β2\ni,iC where C is a constant.\nIf Var[ai] is the same for all layers, Pre-LN sets\nβ2\ni,i as 1/i, and Post-LN sets β2\ni,i as a constant.\nThus, we have Corollary 1 and 2 as below.\nCOROLLARY 1. — For a N-layer Pre-LN F, we\nhave Var[F(x0, W ) − F(x0, W ∗)] = O(log N).\nCOROLLARY 2. — For a N-layer Post-LN F, we\nhave Var[F(x0, W ) − F(x0, W ∗)] = O(N).\nThey show that, since Post-LN relies more on\nresidual branches than Pre-LN ( i.e., has a larger\nβ2\ni,i), the perturbation is ampliﬁed to a larger mag-\nnitude. To empirically verify these relationships,\nwe calculate |F(x0, W ) − F (x0, W ∗)|2\n2 for Pre-\nLN and Post-LN and visualize the results in Fig-\n6If a0 and a1 are independent, Var[a0 +a1] = Var[ a0] +\nVar[a1]; also, in our experiments Var[x(p·)\ni ] increases as i\nbecomes larger\nure 4. In Corollary 2, N is linearly associated with\n|F − F ∗|2\n2 for Post-LN; and in Corollary 1, log N\nis linearly associated with |F − F ∗|2\n2 for Pre-LN.\nThese relationships match the observation in our\nexperiments (as in Figure 4). For further veriﬁca-\ntion, we measure their correlation magnitudes by\nR2 and ﬁnd R2 = 0 .99 in both cases.\nMoreover, we replace the random noise δ with\noptimization updates ( i.e., setting W∗ = W +\nAdam(∆W), where opt (·) is update calculated by\nthe Adam optimizer) and visualize output shifts.\nThis replacement makes the correlation between\n|F − F ∗|2\n2 and N (for Post-LN) or log N (for Pre-\nLN) to be weaker ( i.e., R2 = 0 .75). Still, as in\nFigure 4, the output shift |F − F ∗|2\n2 for Post-LN\nis larger than Pre-LN by multiple magnitudes.\nIntuitively, large output shifts would destabilize\nthe training ( Li et al. , 2018). Also, as elaborated\nin Appendix B, the constant C in Theorem 2 is re-\nlated to network derivatives and would be smaller\nas training advances, which explains why warmup\nis also helpful for the standard SGD. Therefore,\nwe conjecture it is the large output shift of Post-\nLN results in unstable training. We proceed to sta-\nbilize Post-LN by controlling the dependency on\nresidual branches in the early stage of training.\n4.3 Admin – Adaptive Model Initialization\nIn light of our analysis, we add additional param-\neters ( i.e., ω) to control residual dependencies of\nPost-LN and stabilize training by adaptively ini-\ntializing ω to ensure an O(log N) output change.\nDue to different training conﬁgurations and\nmodel speciﬁcities ( e.g., different models may use\ndifferent activation functions and dropout ratios),\nit is hard to derive a universal initialization method.\nInstead, we decompose model initialization into\ntwo phrases: Proﬁling and Initialization. Specif-\nically, Admin adds new parameters ω and con-\nstructs its i-th sub-layer as xi = fLN(bi), where\nbi = xi−1 · ωi + fi(xi−1), ωi is a D-dimension\nvector and · is element-wise product. Then the Pro-\nﬁling phrase and Initialization phrase are:\nProﬁling. After initializing the network with a\nstandard method (initializing ωi as 1), conduct for-\nward propagation without parameter updating and\nrecord the output variance of residual branches\n(i.e., calculate Var[fi(xi−1)]). Since all elements\nin the same parameter/output matrix are indepen-\ndent to each other and are subject to the same dis-\ntribution, it is sufﬁcient to use a small number of\n5753\n0⊿05\n0⊿10\n0⊿15\n0⊿20\n0⊿25\n0⊿30\n0⊿35\n0⊿40\n18-Layer Admin (Post-LN) 18-Layer Pre-LN\nAt InitializationAfter 100 Epochs\nNormalized Layer Outputs\nx 1\nx 1\na 1 a 1\nNormalized Outputs of Residual Branches\nAdmin stabilizes \nmodel training by \navoid over-large\nWith a Post-LN structure, \nAdmin allows layer outputs\ndependencies.\nof the ﬁnal model to \ndepend more on \ntheir residual \nbranches.\nFigure 8: βi,j of 18-Layer Admin (Post-LN) and Pre-\nLN on the WMT-14 En-De dataset.\ninstances in this phrase. In our experiments, the\nﬁrst batch (no more than 8192 tokens) is used.\nInitialization. Set ωi =\nqP\nj<i Var[fj(xj−1)]\nand initialize all other parameters with the same\nmethod used in the Proﬁling phrase.\nIn the early stage, Admin sets β2\ni,i to approxi-\nmately 1\ni and ensures an O(log N) output change,\nthus stabilizing training. Model training would be-\ncome more stable in the late stage (the constant\nC in Theorem 2 is related to parameter gradients),\nand each layer has the ﬂexibility to adjust ω and\ndepends more on its residual branch to calculate\nthe layer outputs. After training ﬁnishes, Admin\ncan be reparameterized as the conventional Post-\nLN structure ( i.e., removing ω). More implemen-\ntation details are elaborated in Appendix C.\nTo verify our intuition, we calculate the layer\ndependency of 18-Layer models and visualize the\nresult in Figure 8. Figures 7 and 8 show that\nAdmin avoids over-large dependencies at initial-\nization and unleashes the potential to make the\nlayer outputs depend more on their residual out-\nputs in the ﬁnal model. Moreover, we visualize\nthe output change of Admin in Figure 4. Bene-\nﬁting from the adaptive initialization, the output\nchange of Admin gets roughly the same increase\nspeed as Pre-LN, even constructed in the Post-LN\nmanner. Also, although Admin is formulated in a\nPost-LN manner and suffers from gradient vanish-\ning, 18-layer Admin successfully converges and\noutperforms 18-layer Pre-LN (as in Table 2). This\nevidence supports our intuition that the large de-\npendency on residual branches ampliﬁes the out-\nput ﬂuctuation and destabilizes training.\n0 50\n4.4\n4.6\n4.8\n5.0\nDev PPL on WMT’14 En-De\n12-Layer Transformer\n50 75\n4.8\n4.9\n5.0\n5.1\n5.2\nDev PPL on IWSLT’14 De-En\nTransformer Small\nPre-LN\nPost-LN\nAdmin\n(Post-LN)\nFigure 9: Development PPL on the WMT’14 En-De\ndataset and the IWLST’14 De-En dataset.\n5 Experiments\nWe conduct experiments on IWSLT’14 De-En,\nWMT’14 En-De, and WMT’14 En-Fr. More de-\ntails are elaborated in Appendix D.\n5.1 Performance Comparison\nWe use BLEU as the evaluation matric and sum-\nmarize the model performance in Table 2. On the\nWMT’14 dataset, we use Transformer-base mod-\nels with 6, 12, or 18 layers. Admin achieves a\nbetter performance than Post-LN and Pre-LN in\nall three settings. Speciﬁcally, 12-Layer and 18-\nLayer Post-LN diverges without the adaptive ini-\ntialization. Pre-LN converges in all settings, but\nit results in sub-optimal performance. Admin not\nonly stabilizes the training of deeper models but\nbeneﬁts more from the increased model capacity\nthen Pre-LN, which veriﬁes our intuition that the\nPre-LN structure limits the model potential. As\nin Figure 1 and Figure 9, although the 6-layer\nPre-LN converges faster than Post-LN, its ﬁnal\nperformance is worse than Post-LN. In contrast,\nAdmin not only achieves the same convergence\nspeed with Pre-LN in the early stage but reaches\na good performance in the late stage.\nWe use 6-layer Transformer-small (its hidden di-\nmension is smaller than the base model) on the\nIWSLT’14 dataset, and all methods perform sim-\nilarly. Still, as in Figure 10, Admin outperforms\nthe other two by a small margin. Together with\nWMT’14 results, it implies the training stability is\nrelated to layer number. For shallow networks, the\nstability difference between Post-LN and Pre-LN\nis not signiﬁcant (as in Figure 4), and all meth-\nods reach reasonable performance. It is worth\nmentioning that attention and activation dropouts\nhave an enormous impact on IWSLT’14, which is\nsmaller than WMT’14 datasets.\n5754\nTable 2: BLEU on IWSLT’14 De-En and WMT’14 En-Fr/De (AL-BL refers A-layer encoder & B-layer decoder).\nDataset IWSLT’14 De-En WMT’14 En-Fr WMT’14 En-De\nEnc #–Dec # 6L–6L (small) 6L–6L 60L–12L 6L–6L 12L–12L 18L–18L\nPost-LN 35.64±0.23 41.29 failed 27.80 failed failed\nPre-LN 35.50±0.04 40.74 43.10 27.27 28.26 28.38\nAdmin 35.67±0.15 41.47 43.80 27.90 28.58 29.03\nTo further explore the potential of Admin, we\ntrain Transformers with a larger size. Speciﬁ-\ncally, we expand the Transformer-base conﬁgura-\ntion to have a 60-layer encoder and a 12-layer de-\ncoder. As in Table 2, our method achieves a BLEU\nscore of 43.8 on the WMT’14 En-Fr dataset, the\nnew state-of-the-art without using additional an-\nnotations ( e.g., back-translation). More discus-\nsions are conducted in Appendix F to compare\nthis model with the current state of the art. Fur-\nthermore, in-depth analyses are summarized in\nLiu et al. (2020b), including systematic evalua-\ntions on the model performance (with TER, ME-\nTEOR, and BLEU), comprehensive discussions on\nmodel dimensions ( i.e., depth, head number, and\nhidden dimension), and ﬁne-grained error analysis.\nIt is worth mentioning that the 60L-12L Admin\nmodel achieves a 30.1 BLEU score on WMT’14\nEn-De (Liu et al. , 2020b).\n5.2 Connection to Warmup\nOur previous work ( Liu et al. , 2020a) establishes\nthat the need for warmup comes from the unsta-\nble adaptive learning rates in the early stage. Still,\nremoving the warmup phrase results in more se-\nvere consequences for Transformers than other ar-\nchitectures. Also, warmup has been found to be\nuseful for the vanilla SGD ( Xiong et al. , 2019).\nTheorem 1 establishes that Var[F(x0, W ) −\nF(x0, W ∗)] ≈ PN\ni=1 β2\ni,iC where C =\nVar[Gi(bx∗\ni−1, Wi) − Gi(bx∗\ni−1, W ∗\ni )]. In the early\nstage of training, the network has larger parame-\nter gradients and thus larger C. Therefore, using\na small learning rate at initialization helps to alle-\nviate the massive output shift of Post-LN. We fur-\nther conduct experiments to explore whether more\nprolonged warmups can make up the stability dif-\nference between Post-LN and Pre-LN. We observe\nthat 18-layer Post-LN training still fails after ex-\ntending the warmup phrase from 8 thousand up-\ndates to 16, 24, and 32 thousand. It shows that\nlearning rate warmup alone cannot neutralize the\n0.999 0.995 0.99\n1 × 10−4\n2 × 10−4\n3 × 10−4\n4 × 10−4\n5 × 10−4\n34.64 34.65 34.41\n35.65 35.58 35.51\n35.87 0.00 0.00\n33.56 0.00 0.00\n0.00 0.00 0.00\nPost-LN\n0.999 0.995 0.99\n33.98 33.81 33.76\n34.74 34.91 34.87\n35.09 35.15 35.19\n35.06 35.28 35.31\n35.51 35.45 35.55\nPre-LN\n0.999 0.995 0.99\n34.58 34.53 34.60\n35.26 35.03 35.25\n35.38 35.62 35.57\n35.74 35.89 35.69\n35.61 35.83 35.84\nAdmin (Post-LN)\n33.0\n33.5\n34.0\n34.5\n35.0\n35.5\n36.0\nFigure 10: BLEU score of Post-LN, Pre-LN and Ad-\nmin on the IWSLT’14 De-En dataset (x-axis is the\nβ2 for adaptive optimizers and y-axis is the learning\nrate). Pre-LN converges in all settings while Post-LN\ndiverges in 7 out of 15 settings. When Post-LN con-\nverges, it outperforms Pre-LN in 7 out of 8 settings. Ad-\nmin stabilizes Post-LN training and outperforms Pre-\nLN (its best performance is comparable with Post-LN).\ninstability of Post-LN. Intuitively, massive output\nshifts not only require a small learning rate but\nalso unsmoothes the loss surface ( Li et al. , 2018)\nand make the training ill-conditioned.\nAdmin regularizes the model behavior at ini-\ntialization and stabilizes the training. To explore\nwhether Admin is able to stabilize the training\nalone, we remove the warmup phase and con-\nduct a grid search on optimizer hyper-parameters.\nThe results are visualized in Figure 10. It shows\nthat as Post-LN is more sensitive to the choice of\nhyper-parameters, Admin successfully stabilizes\nthe training without hurting its potential.\n5.3 Comparing to Other Initializations\nWe compare our methods with three initialization\nmethods, i.e., ReZero ( Bachlechner et al. , 2020),\nFixUp (Zhang et al. , 2019a), and LookLinear ( Bal-\nduzzi et al. , 2017a). Speciﬁcally, we ﬁrst con-\nduct experiments with 18-layer Transformers on\nthe WMT’14 De-En dataset. In our experiments,\nwe observe that all of ReZero (which does not con-\ntain layer normalization), FixUp (which also does\nnot contain layer normalization), and LookLinear\n(which is incorporated with Post-LN) leads to di-\n5755\nvergent training. With further analysis, we ﬁnd\nthat the half-precision training and dropout could\ndestabilize FixUp and ReZero, due to the lack of\nlayer normalization. Simultaneously, we ﬁnd that\neven for shadow networks, having an over small\nreliance on residual branches hurts the model per-\nformance, which also supports our intuition. For\nexample, as elaborated in Appendix E, applying\nReZero to Transformer-small leads to a 1-2 BLEU\nscore drop on the IWSLT’14 De-En dataset.\n6 Related Work\nTransformer. Transformer (Vaswani et al. , 2017)\nhas led to a series of breakthroughs in various do-\nmains (Devlin et al. , 2019; Velickovic et al., 2018;\nHuang et al. , 2019; Parmar et al. , 2018; Ramachan-\ndran et al., 2019). Liu et al. (2020a) show that com-\npared to other architectures, removing the warmup\nphase is more damaging for Transformers, espe-\ncially Post-LN. Similarly, it has been found that\nthe original Transformer (referred to as Post-LN)\nis less robust than its Pre-LN variant ( Baevski and\nAuli, 2019; Nguyen and Salazar, 2019; Wang et al.,\n2019). Our studies go beyond the existing liter-\nature on gradient vanishing ( Xiong et al. , 2019)\nand identify an essential factor inﬂuencing Trans-\nformer training greatly.\nDeep Network Initialization. It has been ob-\nserved that deeper networks can lead to better\nperformance. For example, Dong et al. (2020)\nﬁnd that the network depth players a similar role\nwith the sample number in numerical ODE solvers,\nwhich hinders the system from getting more pre-\ncise results. Many attempts have been made to\nclear obstacles for training deep networks, includ-\ning various initialization methods. Based on the\nindependence among initialized parameters, one\nmethod is derived and found to be useful to handle\nthe gradient vanishing ( Glorot and Bengio , 2010).\nSimilar methods are further developed for ReLU\nnetworks ( He et al. , 2015). He et al. (2016) ﬁnd\nthat deep network training is still hard even after\naddressing the gradient vanishing issue and pro-\npose residual networks. Balduzzi et al. (2017b)\nidentiﬁes the shattered gradient issue and proposes\nLookLinear initialization.\nOn the other hand, although it is observed that\nscaling residual outputs to smaller values helps\nto stabilize training ( Hanin and Rolnick , 2018;\nMishkin and Matas , 2015; Zhang et al. , 2019a;\nBachlechner et al. , 2020; Goyal et al. , 2017), there\nis no systematic analysis on what complicates\nTransformer training or its underlying connection\nto the dependency on residual branches. Here, we\nidentify that unbalanced gradients are not the di-\nrect cause of the Post-LN instability, recognize the\nampliﬁcation effect, and propose a novel adaptive\ninitialization method.\n7 Conclusion\nIn this paper, we study the difﬁculties of training\nTransformers in theoretical and empirical manners.\nOur study in Section 3 suggests that the gradient\nvanishing problem is not the root cause of unsta-\nble Transformer training. Also, the unbalanced\ngradient distribution issue is mostly addressed by\nadaptive optimizers. In Section 4, we reveal the\nroot cause of the instability to be the strong de-\npendency on residual branches, which ampliﬁes\nthe ﬂuctuation caused by parameter changes and\ndestabilizes model training. In light of our anal-\nysis, we propose Admin, an adaptive initializa-\ntion method to stabilize Transformers training. It\ncontrols the dependency at the beginning of train-\ning and maintains the ﬂexibility to capture those\ndependencies once training stabilizes. Extensive\nexperiments verify our intuitions and show that,\nwithout introducing additional hyper-parameters,\nAdmin achieves more stable training, faster con-\nvergence, and better performance.\nOur work opens up new possibilities to not\nonly further push the state-of-the-art but under-\nstand deep network training better. It leads to\nmany interesting future works, including general-\nizing Theorem 2 to other models, designing new\nalgorithms to automatically adapt deep networks\nto different training conﬁgurations, upgrading the\nTransformer architecture, and applying our pro-\nposed Admin to conduct training in a larger scale.\nAcknowledge\nWe thank all reviewers for their constructive com-\nments; Chengyu Dong, Haoming Jiang, Jingbo\nShang, Xiaotao Gu, and Zihan Wang for valuable\ndiscussions and comments; Jingbo Shang for shar-\ning GPU machines; and Microsoft for setting up\nGPU machines. The research was sponsored in\npart by DARPA No. W911NF-17-C-0099 and No.\nFA8750-19-2-1004, National Science Foundation\nIIS-19-56151, IIS-17-41317, IIS 17-04532, and\nIIS 16-18481, and DTRA HDTRA11810026.\n5756\nReferences\nJimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\n2016. Layer normalization. ArXiv, abs/1607.06450.\nThomas C. Bachlechner, Bodhisattwa Prasad Ma-\njumder, Huanru Henry Mao, Garrison W. Cottrell,\nand Julian J. McAuley. 2020. Rezero is all you\nneed: Fast convergence at large depth. ArXiv,\nabs/2003.04887.\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\nput representations for neural language modeling. In\nICLR.\nDavid Balduzzi, Marcus Frean, Lennox Leary, J. P.\nLewis, Kurt Wan-Duo Ma, and Brian McWilliams.\n2017a. The shattered gradients problem: If resnets\nare the answer, then what is the question? In ICML.\nDavid Balduzzi, Marcus Frean, Lennox Leary, J P\nLewis, Kurt Wan-Duo Ma, and Brian McWilliams.\n2017b. The shattered gradients problem: If resnets\nare the answer, then what is the question? In ICML.\nYoshua Bengio, Patrice Y . Simard, and Paolo Frasconi.\n1994. Learning long-term dependencies with gradi-\nent descent is difﬁcult. IEEE transactions on neural\nnetworks.\nOndˇrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve\nSaint-Amand, et al. 2014. Findings of the 2014\nworkshop on statistical machine translation. In\nWorkshop on Statistical Machine Translation .\nMauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa\nBentivogli, and Marcello Federico. 2014. Report on\nthe 11th iwslt evaluation campaign, iwslt 2014. In\nInternational Workshop on Spoken Language Trans-\nlation, Hanoi, Vietnam .\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Niki Parmar, Michael Schuster, Zhi-Feng\nChen, Yonghui Wu, and Macduff Hughes. 2018.\nThe best of both worlds: Combining recent advances\nin neural machine translation. In ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT.\nChengyu Dong, Liyuan Liu, Zichao Li, and Jingbo\nShang. 2020. Towards adaptive residual network\ntraining: A neural-ode perspective. In ICML.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neural\nnetworks. In AISTATS.\nPriya Goyal, Piotr Dollár, Ross B. Girshick, Pieter No-\nordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew\nTulloch, Yangqing Jia, and Kaiming He. 2017. Ac-\ncurate, large minibatch sgd: Training imagenet in 1\nhour. ArXiv, abs/1706.02677.\nBoris Hanin and David Rolnick. 2018. How to start\ntraining: The effect of initialization and architecture.\nIn NeurIPS.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Delving deep into rectiﬁers: Surpassing\nhuman-level performance on imagenet classiﬁcation.\nIn ICCV.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In CVPR.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob\nUszkoreit, Ian Simon, Curtis Hawthorne, Noam\nShazeer, Andrew M. Dai, Matthew D. Hoffman,\nMonica Dinculescu, and Douglas Eck. 2019. Music\ntransformer: Generating music with long-term struc-\nture. In ICLR.\nHao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein.\n2018. Visualizing the loss landscape of neural nets.\nIn NeurIPS.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\n2020a. On the variance of the adaptive learning rate\nand beyond. In ICLR.\nXiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng\nGao. 2020b. Very deep transformers for neural ma-\nchine translation. ArXiv, abs/2008.07772.\nYiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong,\nTao Qin, Liwei Wang, and Tie-Yan Liu. 2020. Un-\nderstanding and improving transformer from a multi-\nparticle dynamic system point of view. In ICLR\nWorkshop DeepDiffEq.\nDmytro Mishkin and Juan E. Sala Matas. 2015. All\nyou need is a good init. In ICLR.\nToan Q. Nguyen and Julian Salazar. 2019. Transform-\ners without tears: Improving the normalization of\nself-attention. In IWSLT.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In NAACL-HLT\nDemonstrations.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. 2018. Image transformer. In ICML.\nMartin Popel and Ondrej Bojar. 2018. Training tips\nfor the transformer model. The Prague Bulletin of\nMathematical Linguistics, 110:43 – 70.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. ArXiv, abs/1910.10683.\n5757\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Ir-\nwan Bello, Anselm Levskaya, and Jonathon Shlens.\n2019. Stand-alone self-attention in vision models.\nIn NeurIPS.\nAndrew M Saxe, James L McClelland, and Surya Gan-\nguli. 2013. Exact solutions to the nonlinear dynam-\nics of learning in deep linear neural networks. ArXiv,\nabs/1312.6120.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nCVPR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Liò, and Yoshua Bengio.\n2018. Graph attention networks. In ICLR.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for ma-\nchine translation. In ACL.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. 2019a. Pay less attention with\nlightweight and dynamic convolutions. In ICLR.\nLijun Wu, Yiren Wang, Yingce Xia, Fei Tian, Fei Gao,\nTao Qin, Jianhuang Lai, and Tie-Yan Liu. 2019b.\nDepth growing for neural machine translation. In\nACL.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shu\nxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Li-Wei Wang, and Tie-Yan Liu. 2019. On layer\nnormalization in the transformer architecture. ArXiv,\nabs/2002.04745.\nHongyi Zhang, Yann N. Dauphin, and Tengyu Ma.\n2019a. Fixup initialization: Residual learning with-\nout normalization. In ICLR.\nJingzhao Zhang, Sai Praneeth Karimireddy, Andreas\nVeit, Seungyeon Kim, Sashank J. Reddi, Surinder\nKumar, and Suvrit Sra. 2019b. Why adam beats sgd\nfor attention models. ArXiv, abs/1912.03194.\nGuangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan\nZhang, and Liangchen Luo. 2019. Muse: Parallel\nmulti-scale attention for sequence to sequence learn-\ning. ArXiv, abs/1911.09483.\nA Gradients at Initialization\nHere, we ﬁrst reveal that Pre-LN does not suf-\nfer from the gradient vanishing. Then we estab-\nlish that only the Post-LN decoder suffers from\nthe gradient vanishing, but not the Post-LN en-\ncoder. For simplicity, we use ∆x to denote gra-\ndients, i.e., ∆x = ∂L\n∂x where L is the training\nobjective. Following the previous study ( Bengio\net al. , 1994; Glorot and Bengio , 2010; He et al. ,\n2015; Saxe et al. , 2013), we analyze the gradient\ndistribution at the very beginning of training, as-\nsume that the randomly initialized parameters and\nthe partial derivative with regard to module inputs\nare independent.\nA.1 Pre-LN Analysis\nFor Pre-LN encoders, we have x(pe)\n2i = x(pe)\n2i−1 +\nfFFN(fLN(x(pe)\n2i−1)) and ∆x(pe)\n2i−1 = ∆ x(pe)\n2i (1 +\n∂fFFN(fLN(x(pe)\n2i−1))\n∂x(pe)\n2i\n). At initialization, the two terms\non the right part are approximately independent\nand E[\n∂fFFN(fLN(z(pe)\n2i−1))\n∂x(pe)\n2i\n] = 0 . Therefore we\nhave Var[∆x(pe)\n2i−1] ≥ Var[∆x(pe)\n2i ]. Similarly,\nwe can get Var[∆x(pe)\n2i−2] ≥ Var[∆x(pe)\n2i−1] thus\n∀i ≤ j, Var[∆x(pe)\ni ] ≥ Var[∆x(pe)\nj ]. Applying\nthe same analysis to Pre-LN decoders, we can get\n∀i ≤ j, Var[∆x(pd)\ni ] ≥ Var[∆x(pd)\nj ]. Thus, lower\nlayers have larger gradients than higher layers, and\ngradients do not vanish in the backpropagation.\nREMARK 1. — For Pre-LN, if ∀i, ∆x(p·)\ni and\nthe derivatives of modules in the i-th sub-layer\nare independent, then ∀i ≤ j, Var[∆x(p·)\ni ] ≥\nVar[∆x(p·)\nj ].\nA.2 Post-LN Encoder Analysis\nDifferent from Pre-LN, x(oe)\ni and x(oe)\ni−1 are associ-\nated with not only the residual connection but the\nlayer normalization, which makes it harder to es-\ntablish the connection on their gradients. After\nmaking assumptions on the model initialization,\nwe ﬁnd that lower layers in Post-LN encoder also\nhave larger gradients than higher layers, and gradi-\nents do not vanish in their backpropagations.\nTHEOREM 1. — For Post-LN Encoders, if γ and\nν in the Layer Norm are initialized as 1 and 0 re-\nspectively; all other parameters are initialized by\nsymmetric distributions with zero mean; x(oe)\ni and\n∆x(oe)\ni are subject to symmetric distributions with\nzero mean; the variance of x(oe)\ni is 1 (i.e., normal-\nized by Layer Norm); ∆x(oe)\ni and the derivatives\nof modules in i-th sub-layer are independent, we\nhave Var[∆xi−1] ≥ Var[∆xi].\n5758\nProof. We ﬁrst prove Var[∆x(oe)\n2i−1] ≥ Var[∆x(oe)\n2i ], i.e., the backpropagation through FFN sublayers\ndoes not suffer from gradient vanishing. In Post-LN encoders, the output of FFN sublayers is calculated\nas x(oe)\n2i = fLN(b(oe)\n2i ) where b(oe)\n2i = x(oe)\n2i−1 + max(0, x(oe)\n2i−1W(1))W(2). Since at initialization, W(1) and\nW(2) are independently randomized by symmetric distributions, we have E[b(oe)\n2i ] = 0 and\nx(oe)\n2i = x(oe)\n2i−1 + max(x(oe)\n2i−1W(1), 0)W(2)\nσb,2i\nwhere σ2\nb,2i = Var[ b(oe)\n2i ]. Referring to the dimension of W(1) as D × Df , He et al. (2015) establishes\nthat\nVar[max(x(oe)\n2i−1W(1), 0)W(2)] = 1\n2DDf Var[w(1)] Var[w(2)] Var[x(oe)\n2i−1].\nSince in Post-LN, x(oe)\n2i−1 is the output of layer norm, we have Var[x(oe)\n2i−1] = 1 . Thus,\nσ2\nb,2i = Var[b(oe)\n2i ] = Var[ x(oe)\n2i−1] + Var[max(x(oe)\n2i−1W(1), 0)W(2)]\n= 1 + 1\n2DDf Var[w(1)] Var[w(2)]. (1)\nAssuming different terms are also independent in the backpropagation, we have\nVar[∆x(oe)\n2i−1] ≥ Var[ 1\nσb,2i\n(∆x(oe)\n2i + ∆x(oe)\n2i\n∂ max(x(oe)\n2i−1W(1), 0)W(2)\n∂x(oe)\n2i−1\n)].\nAt initialization, He et al. (2015) establishes that\nVar[∆x(oe)\n2i\n∂ max(x(oe)\n2i−1W(1), 0)W(2)\n∂x(oe)\n2i−1\n] = 1\n2DDf Var[w(1)] Var[w(2)] Var[∆x(oe)\n2i ].\nTherefore, we have\nVar[∆x(oe)\n2i−1] ≥ 1\nσ2\nb,2i\n(1 + 1\n2DDf Var[w(1)] Var[w(2)]) Var[∆x(oe)\n2i ]. (2)\nCombining Equation 1 with Equation 2, we have\nVar[∆x(oe)\n2i−1] ≥ Var[∆x(oe)\n2i ] (3)\nwhich shows the backpropagation through FFN sublayers does not suffer from gradient vanishing.\nNow we proceed to prove that, Var[∆x(oe)\n2i−2] ≥ Var[∆x(oe)\n2i−1], i.e., the backpropagation through Self-\nAttention sublayers do not suffer from gradient vanishing. In Post-LN encoders, the output of Self-\nAttention sublayers are calculated as x(oe)\n2i−1 = fLN(b(oe)\n2i−1) where b(oe)\n2i−1 = x(oe)\n2i−2 + a(oe)\n2i−1 and a(od)\n2i−1 =\nP\nh fs(x(oe)\n2i−2W(Q)\nh W(K)\nh xT (oe)\n2i−2)x(oe)\n2i−2W(V1)\nh W(V2)\nh . At initialization, since W(Q), W(K), W(V1), and\nW(V2) are independently randomized by symmetric distributions, we have E[b(od)\n2i−1] = 0 , thus x(oe)\n2i−1 =\nb(oe)\n2i−1\nσb,2i−1\n, where σ2\nb,2i−1 = Var[b(oe)\n2i−1] = Var[ x(oe)\n2i−2] + Var[a(oe)\n2i−1].\nReferring E[fs2(x(oe)\n2i−2W(Q)\nh W(K)\nh xT (oe)\n2i−2)] as Ph, we have\nVar[a(od)\n2i−1] = Var[ x(oe)\n2i−2W(V1)\nh W(V2)\nh ]HPh.\nSimilar to He et al. (2015), we have\nVar[x(oe)\n2i−2W(V1)\nh W(V2)\nh ] = D2\nH Var[x(oe)\n2i−2] Var[w(V1)] Var[w(V2)].\n5759\nSince x(oe)\n2i−2 is the output of layer norm, we have Var[x(oe)\n2i−2] = 1 . Thus,\nσ2\nb,2i−1 = 1 + D2Ph Var[x(oe)\n2i−2] Var[w(V1)] Var[w(V2)]. (4)\nIn the backpropagation, we have\nVar[∆x(oe)\n2i−2] ≥ Var[ 1\nσb,2i−1\n(∆x(oe)\n2i−1 + ∆x(oe)\n2i−1\nX\nh\n∂fs(x(oe)\n2i−2W(Q)\nh W(K)\nh xT (oe)\n2i−2)x(oe)\n2i−2W(V1)\nh W(V2)\nh\n∂x(oe)\n2i−2\n)]\n≥ 1\nσ2\nb,2i−1\n(Var[∆x(oe)\n2i−1] + Var[∆x(oe)\n2i−1\nX\nh\nfs(x(oe)\n2i−2W(Q)\nh W(K)\nh xT (oe)\n2i−2)∂x(oe)\n2i−2W(V1)\nh W(V2)\nh\n∂x(oe)\n2i−2\n])\nAt initialization, we assume ∆x(oe)\n2i−1 and model parameters are independent ( He et al. , 2015), thus\nVar[∆x(oe)\n2i−1\nX\nh\nfs(x(oe)\n2i−2W(Q)\nh W(K)\nh xT (oe)\n2i−2)∂x(oe)\n2i−2W(V1)\nh W(V2)\nh\n∂x(oe)\n2i−2\n]\n=D2Ph Var[∆x(oe)\n2i−1] Var[w(V1)] Var[w(V2)]\nTherefore, we have\nVar[∆x(oe)\n2i−2] ≥ 1\nσ2\nb,2i−1\n(1 + D2Ph Var[w(V1)] Var[w(V2)]) Var[∆x(oe)\n2i−1]. (5)\nIntegrating Equation 4 with Equation 5, we have\nVar[∆x(oe)\n2i−2] ≥ Var[∆x(oe)\n2i−1]. (6)\nCombining Equation 3 and Equation 6, we have Var[∆xi−1] ≥ Var[∆xi].\nA.3 Post-LN Decoder Analysis\nIn Post-LN, the Encoder-Attention sub-\nlayer suffers from gradient vanishing. The\nEncoder-Attention sub-layer calculates out-\nputs as x(od)\n3i−1 = fLN(b(od)\n3i−1) where\nb(od)\n3i−1 = x(od)\n3i−2 + a(od)\n3i−1 and a(od)\n3i−1 =\nP\nh fs(x(od)\n3i−2W(Q)\nh W(K)\nh xT (oe))x(oe)W(V1)\nh W(V2)\nh .\nHere x(oe) is encoder outputs and fs is the row-\nwise softmax function. In the backpropagation,\n∆x(od)\n3i−2 ≈\n∆x(od)\n3i−1\nσb,3i−1\n(1 +\n∂a(od)\n3i−1\nx(od)\n3i−2\n). All of the back-\npropagations from a(od)\n3i−1 to x(od)\n3i−2 went through\nthe softmax function, we have Var[\n∂a(od)\n3i−1\nx(od)\n3i−2\n] + 1 ≤\nσ2\nb,3i−1. Thus, those backpropagations suffer from\ngradient vanishing. This observation is further ver-\niﬁed in Figure 3, as the encoder attention bars\n(gradients of encoder-attention outputs) are al-\nways shorter than self-attention bars (gradients\nof encoder-attention inputs), while adjacent self-\nattention bars and fully connected bars usually\nhave the same length.\nA.4 Distributes of Unbalanced Gradients\nAs in Figure 5 and Figure 11, the gradient distribu-\ntion of Attention modules is unbalanced even for\nPre-LN. Speciﬁcally, parameters within the soft-\nmax function ( i.e., W(K) and W(V1)) suffer from\ngradient vanishing ( i.e., ∂fs(x0,··· ,xi,··· )\n∂xi\n≤ 1) and\nhave smaller gradients than other parameters.\nWith further analysis, we ﬁnd it is hard to neu-\ntralize the gradient vanishing of softmax. Unlike\nconventional non-linear functions like ReLU or\nsigmoid, softmax has a dynamic input length ( i.e.,\nfor the sentences with different lengths, inputs of\nsoftmax have different dimensions). Although this\nsetting allows Attention modules to handle sequen-\ntial inputs, it restricts them from having stable\nand consistent backpropagation. Speciﬁcally, let\nus consider the comparison between softmax and\nsigmoid. For the sigmoid function, although its\nderivation is smaller than 1, this damping effect\nis consistent for all inputs. Thus, sigmoid can be\nneutralized by a larger initialization ( Glorot and\n5760\n0 50 100\n10−2\n10−1\n100\nRelative Gradient\nNorm\n0 50 100\n 0 50 100\n 0 50 100\nW(Q)\n W(V1)\n W(V2)\n10−1\n100\nRelative Parameter\nUpdate Norm\nW(K)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\nAlthough the gradient distribution is unbalanced ( e.g., W(V 1) and W(V 2) have larger gradients than W(K) and W(Q)),\nadaptive optimizers lead to consistent update magnitudes for diﬀerent parameters.\nEpoch # (iterations over the training set)\nFigure 11: Relative Norm of Gradient ( ∆Wi, where Wi is the checkpoint of i-th epoch) and Update ( |Wi+1 −Wi|)\nof Self-Attention Parameters in 12-Layer Pre-LN.\nTable 3: ReZero Performance on IWSLT’14 De-En. Models are Transformer-small w. 6-layer encoder & decoder.\nModels Admin Post-LN Pre-LN ReZero ReZero+Post-LN\nBLEU 35.67±0.15 35.64 ±0.23 35.50 ±0.04 33.67 ±0.14 34.67 ±0.08\nBengio, 2010). For softmax, its damping effect is\ndifferent for different inputs and cannot be neutral-\nized by a static initialization.\nAlso, we observe that adaptive optimizers\nlargely address this issue. Speciﬁcally, we calcu-\nlate the norm of parameter change in consequent\nepochs ( e.g., |W(K)\nt+1 − W(K)\nt | where W(K)\nt is the\ncheckpoint saved after t epochs) and visualize the\nrelative norm (scaled by the largest value in the\nsame network) in Figure 11. Comparing the rela-\ntive norm of parameter gradients and parameter up-\ndates, we notice that: although the gradient distri-\nbution is unbalanced, adaptive optimizers success-\nfully assign different learning rates to different pa-\nrameters and lead to consistent update magnitudes.\nThis result explains why the vanilla SGD fails\nfor training Transformer ( i.e., lacking the ability\nto handle unbalanced gradient distributions). Be-\nsides, it implies that the unbalanced gradient dis-\ntribution (e.g., gradient vanishing) has been mostly\naddressed by adaptive optimizers and may not sig-\nniﬁcantly impact the training instability.\nB Proof of Theorem 2\nHere, we elaborate the derivation of Theorem 2,\nwhich the relationship between layer number and\noutput ﬂuctuation caused by parameter change.\nTHEOREM 2. — Consider a N-layer Transformer\nbx = F(bx0, W ), where bx0 is the input and W\nis the parameter. If the layer dependency stays\nthe same after a parameter change ( i.e., βi,j has\nthe same value after changing W to W∗, where\nW is randomly initialized and δ = W∗ − W\nis independent to W), the output change ( i.e.,\nVar[F(x0, W )−F (x0, W ∗)]) can be estimated asPN\ni=1 β2\ni,iC where C is a constant.\nProof. We refer the module in i sub-layer as ai =\nGi(bxi−1, Wi), where bxi = P\nj≤i βi,jbaj is the nor-\nmalized residual output and bai = ai√Varai\nis the\nnormalized module output. The ﬁnal output is\nmarked as bx = F(x0, W ) = P\nj≤N βN,j baj. To\nsimplify the notation, we use the superscript ∗\nto indicate variables related to W∗, e.g., bx∗ =\nF(x0, W ∗) and a∗\ni = Gi(bx∗\ni−1, W ∗\ni ).\nAt initialization, all parameters are initialized\nindependently. Thus ∀i ̸= j, bai and baj are inde-\npendent and 1 = Var[ P\nj≤i βi,jbaj] = P\nj≤i β2\ni,j.\nAlso, since k-layer and (k + 1) -layer share the\nresidual connection to previous layers, ∀i, j ≤ k\nwe have βi,k\nβj,k\n= βi,k+1\nβj,k+1\n. Thus ∀i ≤ k, β2\ni,k+1 =\n(1 − β2\nk,k)β2\ni,k and\nVar[bxi − bx∗\ni ] = Var[\nX\nj≤i\nβi,j(baj − ba∗\nj )]\n=\nX\nj≤i\nβ2\ni,j Var[baj − ba∗\nj ] (7)\n=β2\ni,i Var[bai − ba∗\ni ] + (1 − β2\ni,i) Var[bxi − bx∗\ni ].\nNow, we proceed to analyze Var[bai − ba∗\ni ].\n5761\nSpeciﬁcally, we have\nVar[bai − ba∗\ni ]\n= Var[Gi(bxi−1, Wi) − Gi(bx∗\ni−1, W ∗\ni )]\n= Var[Gi(bxi−1, Wi) − Gi(bx∗\ni−1, Wi)+\nGi(bx∗\ni−1, W ∗\ni ) − Gi(bx∗\ni−1, W ∗\ni )]\n= Var[Gi(bxi−1, Wi) − Gi(bx∗\ni−1, Wi)]+\nVar[Gi(bx∗\ni−1, Wi) − Gi(bx∗\ni−1, W ∗\ni )]. (8)\nSince W is randomly initialized, Var[Gi(bx∗\ni−1, Wi)−\nGi(bx∗\ni−1, W ∗\ni )] should have the same value for\nall layers, thus we use a constant C to refer its\nvalue ( C = Var[ Gi(bx∗\ni−1, Wi) − G i(bx∗\ni−1, W ∗\ni )]\nand C ≈ |δ| · |∇Gi(bx∗\ni−1, Wi)|).\nAs to Var[Gi(bxi−1, Wi) − Gi(bx∗\ni−1, Wi)], since\nthe sub-layer of Transformers are mostly us-\ning linear weights with ReLU nonlinearity and\n1 = Var[ Gi(bxi−1, Wi)] = Var[ bxi−1], we have\nVar[Gi(bxi−1, Wi) − Gi(bx∗\ni−1, Wi)] ≈ Var[bxi−1 −\nbx∗\ni−1]. Thus, we can rewrite Equation 8 and get\nVar[bai − ba∗\ni ] ≈ Var[bxi−1 − bx∗\ni−1] + C\nWith Equation 7, we have\nVar[bxi − bx∗\ni ]\n=β2\ni,i Var[bai − ba∗\ni ] + (1 − β2\ni,i) Var[bxi − bx∗\ni ]\n≈β2\ni,i(Var[bxi−1 − bx∗\ni−1] + C)\n+ (1 − β2\ni,i) Var[bxi − bx∗\ni ]\n= Var[bxi − bx∗\ni ] + β2\ni,iC\nThus,Var[F(x0, W )−F(x0, W ∗)] ≈ PN\ni=1 β2\ni,iC.\nC Admin Implementation Details\nAs introduced in Section 4.3, we introduce a new set of parameters to rescale the module outputs. Specif-\nically, we refer these new parameters as ω and construct the Post-LN sub-layer as:\nxi = fLN(bi), where bi = xi−1 · ωi + fi(xi−1)\nwhere · is the element-wise product.\nAfter training, Admin can be reparameterized as the conventional Post-LN structure ( i.e., removing\nωi). Speciﬁcally, we consider xi = bi\nσb\nγ + ν. Then, for feedforward sub-layers, we have\nbi = xi−1 · ω + max(0, xi−1W(1))W(2), where xi = bi−1\nσb\nγ + ν.\nIt can be reparameterized by changing γ, ν, W(1) to γωi, νωi, 1\nωi\nW(1) respectively, i.e.,\nb′\ni = x′\ni−1 + max(0, x′\ni−1\n1\nωi\nW(1))W(2), where x′\ni−1 = b′\ni−1\nσb\nγωi + νωi.\nFor Self-Attention sub-layers, we have\nbi = xi−1 +\nX\nh\nfs(xi−1W(Q)\nh W(K)\nh xi−1)xi−1W(V1)\nh W(V2)\nh , where xi = bi−1\nσb\nγ + ν.\nIt can be reparameterized by changing γ, ν, W(Q)\nh , W(K)\nh , W(V1)\nh to γωi, νωi, 1\nωi\nW(Q)\nh , 1\nωi\nW(K)\nh\n1\nωi\nW(V1)\nh respectively, i.e.,\nb′\ni = x′\ni−1 +\nX\nh\nfs(x′\ni−1\n1\nωi\nW(Q)\nh W(K)\nh\n1\nωi\nx′\ni−1)x′\ni−1\n1\nωi\nW(V1)\nh W(V2)\nh , where x′\ni−1 = b′\ni−1\nσb\nγωi + νωi.\nFor Encoder-Attention sub-layers, we have\nbi = xi−1 +\nX\nh\nfs(xi−1W(Q)\nh W(K)\nh x·e)x·eW(V1)\nh W(V2)\nh , where xi = bi−1\nσb\nγ + ν.\nIt can be reparameterized by changing γ, ν, W(Q)\nh to γωi, νωi, 1\nωi\nW(Q)\nh respectively, i.e.,\nb′\ni = x′\ni−1 +\nX\nh\nfs(x′\ni−1\n1\nωi\nW(Q)\nh W(K)\nh x·e)x·e 1\nωi\nW(V1)\nh W(V2)\nh , where x′\ni−1 = b′\ni−1\nσb\nγωi + νωi.\nIt is easy to ﬁnd b′\ni = bi in all three situations.\n5762\nFrom the previous analysis, it is easy to ﬁnd that\nintroducing the additional parameter ωi is equiv-\nalent to rescale some model parameters. In our\nexperiments on IWSLT14 De-En, we ﬁnd that di-\nrectly rescaling initialization parameters can get\nroughly the same performance with introducing\nωi. However, it is not very stable when conducting\ntraining in a half-precision manner. Accordingly,\nwe choose to add new parameters ωi instead of\nrescaling parameters.\nD Experimental Setup\nOur experiments are based on the implementation\nfrom the fairseq package ( Ott et al. , 2019). As\nto pre-processing, we follow the public released\nscript from previous work ( Ott et al. , 2019; Lu\net al. , 2020). For WMT’14 datasets, evaluations\nare conducted on the provided ‘newstest14‘ ﬁle,\nand more details about them can be found in Bo-\njar et al. (2014). For the IWSLT’14 De-En dataset,\nmore analysis and details can be found in Cettolo\net al. (2014).\nAs to model speciﬁcs, we directly adopt\nTransformer-small conﬁgurations on the\nIWSLT’14 De-En dataset and stacks more layers\nover the Transformer-base model on the WMT’14\nEn-De and WMT’14 En-Fr datasets. Speciﬁcally,\non the IWSLT’14 De-En dataset, we use word\nembedding with 512 dimensions and 6-layer en-\ncoder/decoder with 4 heads and 1024 feedforward\ndimensions; on the WMT’14 En-De and WMT’14\nEn-Fr datasets, we use word embedding with 512\ndimension and 8-head encoder/decoder with 2048\nhidden dimensions. Label smoothed cross entropy\nis used as the objective function with an uncer-\ntainty = 0 .1 (Szegedy et al. , 2016).\nFor Model training, we use RAdam as the\noptimizer ( Liu et al. , 2020a) and adopt al-\nmost all hyper-parameter settings from Lu et al.\n(2020). Speciﬁcally, for the WMT’14 En-De and\nWMT’14 En-Fr dataset, all dropout ratios (includ-\ning (activation dropout and attention dropout) are\nset to 0.1. For the IWSLT’14 De-En dataset,\nafter-layer dropout is set to 0.3, and a weight de-\ncay of 0.0001 is used. As to optimizer, we set\n(β1, β2) = (0 .9, 0.98), use inverse sqrt learning\nrate scheduler with a warmup phrase (8000 steps\non the WMT’14 En-De/Fr dataset, and 6000 steps\non the IWSLT’14 De-En dataset). The maximum\nlearning rate is set to 1e−3 on the WMT’14 En-\nDe dataset and 7e−4 on the IWSLT’14 De-En and\nWMT’14 En-Fr datasets. We conduct training for\n100 epochs on the WMT’14 En-De dataset, 90\nepochs on the IWSLT’14 De-En dataset and 50\nepochs on the WMT’14 En-Fr dataset, while the\nlast 10 checkpoints are averaged before inference.\nOn the IWSLT’14 De-En dataset, we conduct\ntraining on one NVIDIA GeForce GTX 1080 Ti\nGPU and set the maximum batch size to be 4096.\nOn the WMT’14 En-De dataset, we conduct train-\ning on four NVIDIA Quadro R8000 GPUs and\nset maximum batch size (per GPU) as 8196. On\nthe WMT’14 En-Fr dataset, we conduct training\nwith the Nvidia DGX-2 server (6L-6L uses 4\nNVIDIA TESLA V100 GPUs and 60L-16L uses\n16 NVIDIA TESLA V100 GPUs) and set the max-\nimum batch size (per GPU) as 8000 for 6L-6L\nand 5000 for 60L-16L. On the IWSLT’14 De-\nEn dataset, Transformer-small models (w. 37 M\nParam.) take a few hours to train. On the WMT’14\nEn-De dataset, 6L-6L models (w. 63 M Param.)\ntake ∼ 1 day to train, 12L-12L (w. 107M Param.)\nmodels take ∼ 2 days to train, and 18L-18L (w.\n151M Param.) models take ∼ 3 days to train. On\nthe WMT’14 En-Fr dataset, 6L-6L models (w. 67\nM Param.) takes ∼ 2 days to train, and 60L-12L\nmodels (w. 262M Param.) takes ∼ 2.5 days to\ntrain. All training is conducted in half-precision\nwith dynamic scaling (with a 256-update scaling\nwindow and a 0.03125 minimal scale). All our im-\nplementations and pre-trained models would be re-\nleased publicly.\nE Comparison to ReZero\nHere, we ﬁrst conduct comparisons with\nReZero ( Bachlechner et al. , 2020) under two\nconﬁgurations–the ﬁrst employs the original\nReZero model, and the second adds layer nor-\nmalizations in a Post-LN manner. As summarized\nin Table 3, the ReZero initialization leads to a per-\nformance drop, no matter layer normalization is\nused or not. It veriﬁes our intuition that over small\ndependency restricts the model potential. At the\nsame time, we ﬁnd that adding layer normaliza-\ntion to ReZero helps to improve the performance.\nIntuitively, as dropout plays a vital role in regular-\nizing Transformers, layer normalization helps to\nnot only stabilize training but alleviate the impact\nof turning off dropouts during the inference.\n5763\nTable 4: Performance and model size on WMT’14 En-Fr (AL-BL refers A-layer encoder & B-layer decoder).\nMethods Param. # dim( W(1)) in FFN Enc#-Dec# BLEU\nT5-Base (Raffel et al. , 2019) 220 M 512 × 2048 6L-6L 41.2\nT5-Large (Raffel et al. , 2019) 770 M 1024 × 4096 12L-12L 41.5\nT5-3B (Raffel et al. , 2019) 3 B 1024 × 16384 24L-24L 42.6\nT5-11B (Raffel et al. , 2019) 11 B 1024 × 65536 24L-24L 43.4\nTrans.Big-RNMT+ (Chen et al. , 2018) 377 M 1024 × 8192 6L-6L 41.12\nDynamicConv (Wu et al. , 2019a) 213 M 1024 × 4096 7L-7L 43.2\nDG-Transformer (Wu et al. , 2019b) 264 M 1024 × 4096 8L-8L 43.27\nPrime (Zhao et al. , 2019) 252 M 1024 × 4096 6L-6L 43.48\nPre-LN (60L–12L) 262 M 512 × 2048 60L-12L 43.10\nAdmin (60L–12L) 262 M 512 × 2048 60L-12L 43.80\nF Performance on the WMT’14 En-Fr\nTo explore the potential of Admin, we conduct\nexperiments with 72-layer Transformers on the\nWMT’14 En-Fr dataset (with a 60-layer encoder\nand 12-layer decoder, we add less layers to de-\ncoder to encourage the model to rely more on the\nsource context).\nAs in Table 4, Admin (60L–12L) achieves a\nBLEU score of 43.80, the new state-of-the-art on\nthis long-standing benchmark. This model has a\n60-layer encoder and a 12-layer decoder, which\nis signiﬁcantly deeper than other baselines. Still,\nsince the number of parameters increases in a\nquadratic speed with regard to hidden dimensions\nand a linear speed with regard to layer numbers,\nour model has roughly the same number of pa-\nrameters with other baselines. It is worth men-\ntioning that Admin even achieves better perfor-\nmance than all variants of pre-trained T5 models,\nwhich demonstrates the great potential of our pro-\nposed method. Also, Admin achieves a better per-\nformance than Pre-LN (60L–12L), which further\nveriﬁes that the Pre-LN architecture restricts deep\nmodels’ potential."
}