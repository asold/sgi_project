{
  "title": "Document-Grounded Goal-Oriented Dialogue Systems on Pre-Trained Language Model with Diverse Input Representation",
  "url": "https://openalex.org/W3183281747",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2102846939",
      "name": "Bo-Eun Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2168533283",
      "name": "Dohaeng Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134539763",
      "name": "Sihyung Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120402604",
      "name": "Yejin Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2245252253",
      "name": "Jin Xia Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222149954",
      "name": "Oh-Woog Kwon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2257500286",
      "name": "Harksoo Kim",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4288624561",
    "https://openalex.org/W3103691705",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3099590177",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2914204778",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034623328"
  ],
  "abstract": "Boeun Kim, Dohaeng Lee, Sihyung Kim, Yejin Lee, Jin-Xia Huang, Oh-Woog Kwon, Harksoo Kim. Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021). 2021.",
  "full_text": "Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 98–102\nAugust 5–6, 2021. ©2021 Association for Computational Linguistics\n98\nDocument-Grounded Goal-Oriented Dialogue Systems on Pre-Trained\nLanguage Model with Diverse Input Representation\nBoeun Kim∗, Dohaeng Lee∗, Yejin Lee and Harksoo Kim\nKonkuk University / Seoul, South Korea\n{boeun, dsdhlee, jinjin096, nlpdrkim}@konkuk.ac.kr\nSihyung Kim\nKangwon National University / Chuncheon, South Korea\nsureear@kangwon.ac.kr\nJin-Xia Huang, Oh-Woog Kwon\nElectronics and Telecommunications Research Institute / Daejeon, South Korea\n{hgh, ohwoog}@etri.re.kr\nAbstract\nDocument-grounded goal-oriented dialog sys-\ntem understands users’ utterances, and gener-\nates proper responses by using information ob-\ntained from documents. The Dialdoc21 shared\ntask consists of two subtasks; subtask1, ﬁnd-\ning text spans associated with users’ utterances\nfrom documents, and subtask2, generating re-\nsponses based on information obtained from\nsubtask1. In this paper, we propose two mod-\nels (i.e., a knowledge span prediction model\nand a response generation model) for the sub-\ntask1 and the subtask2. In the subtask1, dia-\nlogue act losses are used with RoBERTa, and\ntitle embeddings are added to input represen-\ntation of RoBERTa. In the subtask2, various\nspecial tokens and embeddings are added to in-\nput representation of BART’s encoder. Then,\nwe propose a method to assign different dif-\nﬁculty scores to leverage curriculum learning.\nIn the subtask1, our span prediction model\nachieved F1-scores of 74.81 (ranked at top 7)\nand 73.41 (ranked at top 5) in test-dev phase\nand test phase, respectively. In the subtask2,\nour response generation model achieved sacre-\nBLEUs of 37.50 (ranked at top 3) and 41.06\n(ranked at top 1) in in test-dev phase and test\nphase, respectively.\n1 Introduction\nThe Dialdoc21 shared task is a task that generates a\nproper response by ﬁnding a knowledge span from\na document associated with a dialogue history. It\nconsists of two subtasks; subtask1 for ﬁnding use-\nful knowledge spans from a document and subtask2\nfor generating proper responses based on the knowl-\nedge spans. The doc2dial dataset, the dataset for\nthe Dialdoc21 shared task, contains conversations\n∗equal contribution\nbetween users and agents in real-world situations.\nThe user and the agent engage in a conversation\nassociated with a document, and the agent should\nprovide the user with document-grounded infor-\nmation in order to guide the user. In this paper,\nwe propose two models to perform the Dialdoc21\nshared task using a pre-trained language model.\nIn particular, we show that in the process of ﬁne-\ntuning the pre-trained model, the proposed input\nrepresentations signiﬁcantly contribute to improv-\ning performances.\n2 Related Work\nThe baseline models for the subtask1 and the sub-\ntask2 were proposed by Feng et al. (2020), the\ncomposers of doc2dial datasets. They formulated\nthe subtask1 as a span selection, inspired by ex-\ntractive question answering tasks such as SQuAD\ntask (Rajpurkar et al., 2016, 2018). Zheng et al.\n(2020) proposed a method to reﬂect the differences\nbetween knowledge spans used for each turn and\ncurrent knowledge span candidates. The differen-\ntial information is fused with or disentangled from\nthe contextual information to facilitate ﬁnal knowl-\nedge selection. Wolf et al. (2019) constructed input\npresentation using word, dialog state and positional\nembedding.\n3 Task Description\nIn the subtask1, our goal is to ﬁnd a relevant knowl-\nedge span required for agent’s response in a con-\nversation composed of multi-turns from a given\ndocument. Inspired by Feng et al. (2020), we pro-\npose a joint model to perform both dialogue act\nprediction and knowledge span prediction. In the\nsubtask2, our goal is to generate agent’s response\n99\nTitle Section IDSpan ID Text\n8 31 For Your Surviving Divorced Spouse\n32 If you have a surviving divorced spouse\nthey could get the same beneﬁts as your widow or widower provided that your marriage9 33 lasted 10 years or more.\nBeneﬁts paid to a surviving divorced spouse won’t affect the beneﬁt amounts your other10 34 survivors will receive based on your earnings record.\nIf your former spouse is caring for your child who is under age 16 or disabled and gets35 beneﬁts on your record ,\n36 they will not have to meet the length - of - marriage rule.\nFor Your Surviving Divorced Spouse\n11\n37 The child must be your natural or legally adopted child.\nTable 1: Example of span extensions from a sentence to a title. The red cell denotes an answer span predicted by\nthe subtask1 model. The green cell denotes a section span containing the answer span. The blue cell denotes a title\nspan containing the predicted span.\nin natural language based on a dialogue history and\na document associated with the dialogue history.\nThe dialogue history consists of speakers and utter-\nances. Then, the document consists of sentences,\ntags, titles, and so on. Based on these structural in-\nformation of the dialogue history and the document,\nwe deﬁne special tokens and embeddings. Then,\nwe propose a method to reﬂect these special tokens\nand embeddings to the well-known BART model\n(Lewis et al., 2020). The doc2dial dataset contains\ngoal-oriented dialogues and knowledge documents.\nFor developing models, three sub-datasets in four\ndomains (DMV , V A, SSA, and studentaid) were\ndeployed; a train dataset, a validation dataset, and\na test-dev dataset. For evaluating the models, a\ntest dataset in ﬁve domains (i.e., four seen domains\n(DMV , V A, SSA, studentaid) and an unseen do-\nmain (COVID-19)) was used. The test-dev dataset\nembodied 30% of the test dataset except for the\nunseen domain.\n4 Key Components of Our Model\n4.1 Subtask1\nWe adopt pre-trained RoBERTa-large model (Liu\net al., 2019) as a backbone. Each dialogue turn\nin the train dataset and the validation dataset has\na dialogue act label. We assume that agent’s di-\nalogue act aids to ﬁnd a proper knowledge span.\nFor dialogue act prediction, we use a fully con-\nnected layer added on the [CLS] output vector of\nthe RoBERTa-large model. Then, we pointwise\nadd special embeddings called title embeddings to\nconventional input representation of the RoBERTa-\nlarge model. As shown in Table 1, each span in a\nknowledge document has its own title. By adding\nthe title embedding, we expect that spans sharing\nthe same title will be tied together to help ﬁnd a\nknowledge span. For knowledge span prediction,\nwe use the well-known machine reading compre-\nhension (MRC) architecture proposed by (Devlin\net al., 2019). In the MRC model, each output vec-\ntor of the RoBERTa-large model is fed into a bi-\ndirectional gated recurrent unit (Bi-GRU) (Cho\net al., 2014) layer. Then, each output of the Bi-\nGRU layer is again fed into a fully connected layer\nfor predicting a starting position and an ending po-\nsition of a knowledge span. Finally, the knowledge\nspan prediction model expands predicted spans (a\nsequence of words) into span segments predeﬁned\nwith span IDs. In this paper, these predeﬁned span\nsegments are called answer spans. The ﬁnal loss\nfunction of the proposed span prediction model,\nLtotal, is the weighted sum of the dialogue act pre-\ndiction loss, Ldialogueact, and the span prediction\nloss, Lspan, as follows:\nLtotal = α∗Ldialogueact + β∗Lspan\nwhere αand βare weighting parameters that are\nset to 0.3 and 0.7, respectively. Then, the dialogue\nact prediction loss and the span prediction loss are\ncalculated by minimizing cross-entropies between\npredicted values and gold values, respectively.\n4.2 Subtask2\nToken Meaning\n<user> Beginning of user’s utterance\n<agent> Beginning of agent’s utterance\n<doc> Beginning of a knowledge document\n<title> Beginning of a knowledge document‘s\ntitle\n<rank> Bordering between answer spans\n<u> Ending of underline markup that is\nexisted in a knowledge document\n<h> Ending of heading markup that is\nexisted in a knowledge document\nTable 2: Special tokens and their meanings.\nWe adopt pre-trained BART-base model (Lewis\n100\net al., 2020) as a backbone. An input of BART’s\nencoder consists of a dialogue history and a knowl-\nedge document. We use a current utterance and\n7 previous utterances, ui,ui−1,...,u i−7, as a di-\nalogue history. Then, we use answer spans that\nare constructed from 100 span candidates pre-\ndicted by the knowledge span prediction model,\nˆs0, ˆs1,..., ˆs100, as a knowledge document. For\nenriching input representation of BART’s encoder,\nwe use special tokens and additional embeddings.\nWe ﬁrst add some special tokens to BART’s input,\nas shown in Table 2. Then, we pointwise add the\nfollowing special embeddings to input representa-\ntion of BART’s encoder:\nType-of-Input embedding: Embedding to dis-\ntinguish between a dialogue history and a knowl-\nedge document.\nRank Embedding: Embedding for representing\nrankings of title spans containing answer spans\nthat are returned by the knowledge span prediction\nmodel.\nRank-in-Section Embedding: Embedding for\nrepresenting rankings of answer spans in each title.\nFigure 1: Special tokens and embeddings.\nFigure 1 illustrates the proposed special tokens and\nembeddings.\n5 Curriculum Learning\nTo improve performances, we train the proposed\nmodels through curriculum learning (Xu et al.,\n2020). Figure 2 illustrates the training process by\ncurriculum learning. We ﬁrst divide the training\ndataset into N buckets and train N teacher model\n(i.e., a teacher model per bucket). In this paper, N\nis set to four. Then, we measure performances of\neach teacher model by using N-1 dataset except for\nthose used for training each teacher model. Based\non the performances of the teacher models, we\nassign each data to difﬁculty levels.\nFigure 2: Curriculum learning process. K denotes the\nnumber of difﬁculty levels.\n5.1 Difﬁculty level for subtask1\nIn the subtask1, we implement four teacher model\nbased on the RoBERTa-base model (Liu et al.,\n2019). Each teacher model calculates an average of\nF1-score and EM-score (i.e., F1-score + EM score\n/ 2) per input data. Then, the average scores of\nthree teacher models are summed. According to\nthe summed average scores, we divide the training\ndataset into an easiest level (the summed average\nscore of 300), an easy level (the summed average\nscore of (200,300)), a median level (the summed\naverage score of (100,200]), and a difﬁcult level\n(the summed average score of (0,100]). The num-\nbers of data in each level are 5,390, 3,215, 6,538\nand 9,260, respectively.\n5.2 Difﬁculty level for subtask2\nIn the subtask2, we implement four teacher mod-\nels based on the BART-base model (Lewis et al.,\n2020). We compute an average sum of sacreBLEUs\nevaluated by each teacher model. Then, we perform\nhuman evaluations on the computed average sums.\nBased on the human evaluations, we divide the\ntraining dataset into an easy level (sacreBLEU of\n[30,100]), a median level (sacreBLEU of [15,30)),\nand a difﬁcult level (sacreBLEU of [0,15)). The\nnumbers of data in each level are 8,165, 3,976, and\n12,262, respectively.\n5.3 Training detail\nBased on the measured difﬁculty scoring, the total\ntraining stage consists of K+1 phases. For instance,\nif K is set to two, the difﬁculty level comprises\nof two levels, i.e., “easy” and “difﬁcult”, and the\n101\ntraining stage is composed of three phases. Con-\ncisely, we sort training datasets through difﬁculty\nlevels. In the ﬁrst stage, we train the model by us-\ning 1/K dataset of “easy” level. In the second stage,\nwe train the model by using 1/K dataset of “easy”\nlevel and 1/K dataset of “difﬁcult” level excluding\ndata used for the previous training stage. In the\nlast stage, we train the model by using the entire\ntraining dataset until convergence. Since we use K\nas 4 in subtask1 and K as 3 in subtask2, each stage\nis composed of ﬁve phases and four phases.\n6 Experiments\nModels F1 EM\nBERT-large 67.96 52.02\n+ DA 69.29 54.04\nRoBERTa-large - -\n+ DA 72.23 56.06\n+ DA + T 72.91 57.07\n+ DA + T + CL 74.81 59.59\nTable 3: Subtask1 test-dev phase results. DA denotes\nthe dialogue act prediction, T denotes the title embed-\nding, and CL denotes the curriculum learning.\nAs shown in Table 3, the span prediction\nmodel based on RoBERTa-large showed better\nperformances than that based on BERT-large\n(Devlin et al., 2019). The dialogue act con-\ntributed to improving performances: “BERT-\nlarge+DA” showed F1-score of 1.33%p higher and\nEM score of 2.02%p higher than “BERT-large”.\nThe title embedding contributed to improving\nperformances: “RoBERTa-large+DA+T” showed\nF1-score of 0.68%p higher and EM score of\n1.01%p higher than “RoBERTa-large+DA”. More-\nover, the curriculum learning signiﬁcantly con-\ntributed to improving performances: “RoBERTa-\nlarge+DA+T+CL” showed F1-score of 1.9%p\nhigher and EM score of 2.52%p higher than\n“RoBERTa-large+DA+T”. Table 4 lists results of\nthe subtask2 in the test-dev phase.\nAs shown in Table 4, the Type-of-Input embed-\nding contributed to improving the sacreBLEU of\n2.74%p compared to BART-base. Adding the\nRank embedding improved the score by 5.39%p,\nand adding the Rank-in-Section embedding boosts\nthe performance by another 4.47%p. Finally, the\nModels SacreBLEU\nBART-base 23.09\n+ TI 25.83\n+ TI + R 31.22\n+ TI + R + RS 35.69\n+ TI + R + RS + CL 37.50\nTable 4: Subtask2 test-dev phase results. TI denotes\nthe Type-of-Input embedding, R denotes the Rank em-\nbedding, RS denotes the Rank-in-Section embedding,\nand CL denotes the curriculum learning.\ncurriculum learning improved the sacreBLEU of\n1.81%p.\n7 Conclusion\nWe proposed a document-grounded goal-oriented\ndialogue system for the Dialdoc21 shared task.\nThe proposed model used various special tags and\nembeddings for enriching input representation of\npre-trained language models, RoBERTa-large for\nknowledge span prediction and BART for response\ngeneration. In addition, curriculum learning was\nadopted to achieve performance improvements. In\nthe subtask1, our span prediction model achieved\nF1-scores of 74.81 (ranked at top 7) and 73.41\n(ranked at top 5) in test-dev phase and test phase,\nrespectively. In the subtask2, our response genera-\ntion model achieved sacreBLEUs of 37.50 (ranked\nat top 3) and 41.06 (ranked at top 1) in test-dev\nphase and test phase, respectively.\nAcknowledgments\nThis work was supported by Institute of Informa-\ntion & Communications Technology Planning &\nEvaluation(IITP) grant funded by the Korea govern-\nment(MSIT) (2019-0-00004, Development of semi-\nsupervised learning language intelligence technol-\nogy and Korean tutoring service for foreigners).\nAlso, this work was supported by Institute of In-\nformation & communications Technology Plan-\nning & Evaluation(IITP) grant funded by the Korea\ngovernment(MSIT) (No.2020-0-00368, A Neural-\nSymbolic Model for Knowledge Acquisition and\nInference Techniques)\nReferences\nKyunghyun Cho, Bart van Merrienboer, C ¸ aglar\nG¨ulc ¸ehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\n102\nger Schwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation. In EMNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nSong Feng, Hui Wan, Chulaka Gunasekara, Siva\nPatel, Sachindra Joshi, and Luis Lastras. 2020.\nDoc2dial: A goal-oriented document-grounded di-\nalogue dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 8118–8128.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019. Transfertransfo: A\ntransfer learning approach for neural network\nbased conversational agents. arXiv preprint\narXiv:1901.08149.\nBenfeng Xu, Licheng Zhang, Zhendong Mao, Quan\nWang, Hongtao Xie, and Yongdong Zhang. 2020.\nCurriculum learning for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6095–6104.\nChujie Zheng, Yunbo Cao, Daxin Jiang, and Minlie\nHuang. 2020. Difference-aware knowledge selec-\ntion for knowledge-grounded conversation genera-\ntion. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nFindings, pages 115–125.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7061957716941833
    },
    {
      "name": "Representation (politics)",
      "score": 0.6035248041152954
    },
    {
      "name": "Natural language processing",
      "score": 0.5712985992431641
    },
    {
      "name": "Grounded theory",
      "score": 0.5369216203689575
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46026116609573364
    },
    {
      "name": "Linguistics",
      "score": 0.39811596274375916
    },
    {
      "name": "Qualitative research",
      "score": 0.18417096138000488
    },
    {
      "name": "Sociology",
      "score": 0.1227361261844635
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I142401562",
      "name": "Electronics and Telecommunications Research Institute",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I165507594",
      "name": "Kangwon National University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I24062138",
      "name": "Konkuk University",
      "country": "KR"
    }
  ]
}