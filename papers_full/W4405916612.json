{
  "title": "Evaluating large language models for criterion-based grading from agreement to consistency",
  "url": "https://openalex.org/W4405916612",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2249926277",
      "name": "Da-Wei Zhang",
      "affiliations": [
        "Monash University Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A5093822485",
      "name": "Melissa Boey",
      "affiliations": [
        "Monash University Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A2290491496",
      "name": "Yan Yu. Tan",
      "affiliations": [
        "Monash University Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A5093822486",
      "name": "Alexis Hoh Sheng Jia",
      "affiliations": [
        "Monash University Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A2249926277",
      "name": "Da-Wei Zhang",
      "affiliations": [
        "Monash University Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A5093822485",
      "name": "Melissa Boey",
      "affiliations": [
        "Monash University Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A2290491496",
      "name": "Yan Yu. Tan",
      "affiliations": [
        "Monash University Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A5093822486",
      "name": "Alexis Hoh Sheng Jia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4387363029",
    "https://openalex.org/W4360620450",
    "https://openalex.org/W4388090657",
    "https://openalex.org/W2138228303",
    "https://openalex.org/W2417988078",
    "https://openalex.org/W2560140854",
    "https://openalex.org/W4386253646",
    "https://openalex.org/W4388450624",
    "https://openalex.org/W4387954002",
    "https://openalex.org/W4390414972",
    "https://openalex.org/W4392639864",
    "https://openalex.org/W4404313031",
    "https://openalex.org/W4387162877",
    "https://openalex.org/W4307282710",
    "https://openalex.org/W2963121203",
    "https://openalex.org/W3171079720",
    "https://openalex.org/W2091649086",
    "https://openalex.org/W2327037637",
    "https://openalex.org/W4386959594"
  ],
  "abstract": "Abstract This study evaluates the ability of large language models (LLMs) to deliver criterion-based grading and examines the impact of prompt engineering with detailed criteria on grading. Using well-established human benchmarks and quantitative analyses, we found that even free LLMs achieve criterion-based grading with a detailed understanding of the criteria, underscoring the importance of domain-specific understanding over model complexity. These findings highlight the potential of LLMs to deliver scalable educational feedback.",
  "full_text": "npj |science oflearning Brief communication\nPublished in partnership with The University of Queensland\nhttps://doi.org/10.1038/s41539-024-00291-1\nEvaluating large language models for\ncriterion-based grading from agreement\nto consistency\nCheck for updates\nDa-Wei Zhang , Melissa Boey, Yan Yu Tan & Alexis Hoh Sheng Jia\nThis study evaluates the ability of large language models (LLMs) to deliver criterion-based\ngrading and examines the impact of prompt engineering with detailed criteria on grading. Using\nwell-established human benchmarks and quantitative analyses, we found that even free LLMs\nachieve criterion-based grading with a detailed understanding of the criteria, underscoring the\nimportance of domain-speciﬁc understanding over model complexity. Theseﬁndings highlight\nthe potential of LLMs to deliver scalable educational feedback.\nThe emerging abilities of large public language models (LLMs) in various\ndomains have aroused vigorous discussion in education. Recent surveys\nreveal that LLMs (e.g., ChatGPT) are widely used by students at various\nacademic levels\n1. This prevalence, however, raises concerns about LLMs’\noverreliance leading to avoidance of learning2. Yet, given the transformative\npotential of LLMs in education, dismissing their use is unwise; instead, it is\nessential to explore accountable ways to integrate LLMs into educational\npractices2,3.\nOne promise of integrating LLMs into education lies in its capacity to\nprovide feedback– a core educative process for promoting learning. Despite\nthe importance of feedback, students often experience insufﬁcient or inef-\nfective feedback during learning4,5. Feedback is a constructive process of\nproviding comments to bridge the gap between a learner’s current under-\nstanding and learning objectives6. Providing effective feedback requires\nexpertise, quick response time, interactive capabilities, and an objective\nattitude\n5. LLMs possesses all these features, positioning themselves as a\npromising agent for providing feedback. However, integrating LLMs as a\nfeedback agent in education needs empirical validation of its effectiveness3.\nRecent empirical research demonstrates that LLMs can provide\neffective feedback from the perspective of feedback recipients. The effec-\ntiveness of feedback depends on its reception. In contrast to human raters,\nChatGPT offers more intricate and encouraging feedback on students’\nessays, potentially offsetting human-rater limitations7. Student feedback\nsurveys indicate a favourable reception towards feedback generated by\nChatGPT\n8– 10. An experimental study further corroborates theseﬁndings,\nrevealing a considerable preference among students for ChatGPT-generated\nfeedback because of its clarity and speciﬁcity11. Similar results were\nreported in an interventional study where ChatGPT-generated feedback\nimproved learning engagement and motivation during writing training\n12;\nhowever, its quality might vary with the complexity of writing13. Besides\nwriting tasks, LLMs, such as ChatGPT and Bing chat, can also generate\ncontent-based feedback for medical multiple-choice exams, demonstrating\nthe potential for using LLMs to provide scalable and structured feedback\n14.\nA critical issue remains regarding the extent to which LLM-generated\nfeedback is criterion-based. Identifying the discrepancy between a learner’s\nstatus and the desired learning outcomes is an essential feature of effective\nfeedback6. However, LLM-generated content is generally prone to illogical\nreasoning and unsupported assumptions2, raising concerns about its relia-\nbility in providing feedback7. While recent studies suggest that LLMs, such\nas ChatGPT and Bard, can align their feedback with predeﬁned criteria to\nsome extent13,15,s i g n iﬁcant concerns persist due to methodological limita-\ntions. These studies primarily focus on broad metrics, such as overall score\nmagnitude (e.g., higher or lower) and sentiment polarity (positive/negative)\ncompared to human evaluators, failing to capture the nuance of how well the\nfeedback addresses the criteria. Furthermore, reliance on expert evaluations,\nwhere experts may know which feedback was generated by LLMs, risks\nintroducing bias. Questions also remain about the validity of human\nbenchmarks and the learning criteria used in these comparisons. These\nlimitations underscore the needfor more detailed evaluations.\nA nuanced way to evaluate how well LLMs respond to criteria is\nthrough grades. Criterion-based grading, a widely used form of summative\nfeedback, assigns learners to performance levels based on predeﬁned criteria\nrather than normative comparisons\n16. This approach fulﬁlls the funda-\nmental purpose of feedback by clarifying the gap between a learner’s current\nperformance and desired outcomes16. From a research perspective, the\nquantitative nature of grades offers a distinct advantage, enabling systematic\nassessment of capacity of LLMs to deliver criterion-based feedback.\nThe current study uses well-established human benchmarks and\nquantitative methods toﬁll previous gaps by evaluating the extent to which\nLLM-generated feedback aligns with established criteria. We focus on the\nDepartment of Psychology, Jeffrey Cheah School of Medicine and Health Sciences, Monash University Malaysia, Bandar Sunway, 475000, Malaysia.\ne-mail: daweizhang.edu@gmail.com\nnpj Science of Learning|            (2024) 9:79 1\n1234567890():,;\n1234567890():,;\nInternational English Language Testing System (IELTS) academic writing\ntask for two primary reasons that address previous limitations: (1) it\nemploys a well-established evaluation rubric, and (2) it provides ofﬁcial\nwriting samples with corresponding grades based on these criteria. These\nfeatures ensure reliable and criterion-based benchmarks for accurate eva-\nluation. Additionally, IELTS is a widely recognized testing scenario where\nthere should be sufﬁcient corpus for LLMs. To gauge how LLMs align with\ncriteria, we employ various measures of the Intraclass Correlation Coefﬁ-\ncient (ICC), including interrater absolute agreement and interrater con-\nsistency, to capture different aspects of feedback quality and potential biases.\nInterrater absolute agreement evaluates the exact match in ratings between\nraters, while interrater consistency measures the extent to which raters\nmaintain consistent differences in their evaluations\n17.F u r t h e r m o r e ,w e\nmanipulate prompts to assess whether including evaluation criteria\nenhances the grading quality, thereby providing insights into prompt\nengineering for generating criterion-based feedback.\nThe interrater agreement and interrater consistency of each prompt are\ns h o w ni nT a b l e1. Only prompt 3 showed signiﬁcant interrater agreement\nbut at a moderate level. Regarding interrater consistency, all prompts\nshowed signiﬁcance, but again, it was Prompt 3 that achieved a good level.\nDespite being the most effective, the discrepancy between the interrater\nagreement and consistency in Prompt 3 suggests potential systematic biases.\nThe Bland-Altman plot showed that the biases were evenly distributed\nacross different levels of essays (Fig.1). However, the grading of Prompt 3\nwas signiﬁcantly lower than that of IELTS examiners, t(60) =−2.81,\np < 0.01, Cohen’s d = 0.71.\nIn follow-up analyses, Prompt 3 exhibited good intrarater consistency\n(ICC = 0.89, 95% CI [0.76, 0.94]). We then reran this prompt using\nChatGPT 4.0, resulting in moderate interrater agreement (ICC = 0.74, 95%\nCI [0.20, 0.90]) and good interrater consistency (ICC = 0.84, 95% CI [0.69,\n0.92]). Qualitatively, these results were comparable to ChatGPT 3.5,\nachieving the same level of performance. Quantitatively, there was no sig-\nniﬁcant difference from ChatGPT 3.5, as indicated by nonsigniﬁcant\nFisher’sZs c o r e s(p > 0.05). Applying the same prompt to Claude 3.0 Haiku\nyielded comparable interrater agreement of ICC = 0.68 (95% CI [0.44, 0.83])\nand interrater consistency of ICC = 0.72 (95% CI [0.50, 0.86]). In contrast,\nthe teacher’s grading demonstrated poor interrater agreement (ICC = 0.37,\n95% CI [0.02, 0.63]) and poor interrater consistency (ICC = 0.44, 95% CI\n[0.00, 0.72]). The interrater consistency was signiﬁcantly different from that\ngenerated by the best prompt condition in ChatGPT 3.5 (p < 0.05).\nThis study systematically evaluated the capability of LLMs to perform\ncriterion-based grading and examined how prompt engineering with\ndetailed evaluation criteria inﬂuences grading. The observed patterns of ICC\nmeasures across various conditions conﬁrm that LLMs can provide reliable\nand criterion-based grading, providing theoretical and practical insights for\nthe effective integration of LLMs into educational feedback practices.\nOur ﬁndings indicate that domain-speciﬁck n o w l e d g e ,r a t h e rt h a nt h e\ncomplexity or general knowledge/abilities of the LLMs, limits the precision\nof criterion-based grading. We selected the IELTS writing task due to its\nhighly structured nature and the extensive corpus available, which should\ntheoretically enable LLMs to provide precise grading. Nevertheless, the most\noptimized prompt only achieved moderate absolute agreement and sig-\nniﬁcantly lower grades compared to ofﬁcial marking, suggesting systematic\nbiases and raising concerns about using the absolute grading provided by\nLLMs such as ChatGPT and Claude. We speculate that the suboptimal\nagreement may result from a lack of domain-speciﬁc knowledge rather than\nthe models’ complexity or general knowledge/abilities. Firstly, the most\naccurate grading in our study was derived from the prompt that involved the\nmost domain-speciﬁc knowledge (i.e., detailed marking criteria in this case).\nSecondly, using more advanced models(i.e., ChatGPT 4.0 and Claude 3.0),\npossessing more superior domain-general knowledge and general abilities\ncompared to ChatGPT 3.5, did not quantitatively or qualitatively improve\ngrading.\nMeanwhile, our results conﬁ\nrm that LLMs can deliver reliable and\ncriterion-based grading. Equipped with detailed knowledge of criteria,\nLLMs achieved good interrater consistency with benchmarks and\ndemonstrated excellent test-retest reliability. Theseﬁndings highlight the\nefﬁcacy of LLMs in adhering to established grading standards and\nemphasize the critical role of domain-speciﬁc knowledge in enhancing\ngrading performance. Coupled with the previously discussed suboptimal\nabsolute grading, the reliable and criterion-based grading provided by\nLLMs suggests that responsible use should focus not on one-time grades\nbut on tracking learning growth against desired criteria. This approach\neffectively uses criterion-based grading as meaningful educational feed-\nback for learning\n16.\nThis study encourages further empirical studies on integrating LLMs\ninto educational feedback practices. Identifying performance gaps using\nestablished criteria is essential for providing effective educational feedback\n6.\nWhile this study suggests that LLMs equipped with domain-speciﬁc\nknowledge can identify the gap, future research could extend this work by\nexamining the ability of LLMs to generate more sophisticated forms of\nfeedback for learning (e.g., formative feedback). Beyond demonstrating\nefﬁcacy, future research may also examine the effectiveness of LLMs in\nproviding feedback. The publicly available LLMs (i.e., ChatGPT and\nClaude) were sufﬁcient to provide criterion-based marking in our analysis,\nwhich could output standard educational resources as indicated by our\npreliminary comparison with a teacher representative. Given that the lack of\nhigh-quality feedback has long-term inﬂuences\n18, the integration of LLMs\noffers a promising solution to thisenduring educational challenge.\nThis study also has some limitations. Firstly, the domain-speciﬁc\ninformation provided to ChatGPT was limited to those available on the\nIELTS ofﬁcial website. This approach might neglect essential knowledge\nused by IELTS examiners, leading to a low interrater agreement. Second, the\nlimited number of sample essays could affect the precision of our ICC\nestimates.+\nOverall, this study offers empirical support for using LLM-generated\neducational feedback. The observed good interrater and intrarater con-\nsistency on the IELTS writing task suggests that LLMs (e.g., ChatGPT and\nClaude) can provide criterion-based grading. However, the gap between\ninterrater agreement and consistency underscores the need for careful\nTable 1 | Interrater agreement and consistency under the three\nprompts\nPrompt 1 Prompt 2 Prompt 3\nInterrater agreement 0.46\n[−0.04, 0.74]\n0.37\n[−0.10, 0.73]\n0.61 [0.02, 0.85]\nInterrater consistency 0.63 [0.35, 0.80] 0.71 [0.48, 0.85] 0.77 [0.57, 0.88]\nPoint estimates and their 95% conﬁdence intervals are presented.\nFig. 1 | Bland-Altman plot showing agreement between IELTS ofﬁcial examiners\nand ChatGPT.The plots display the mean scores versus the differences in scores.\nThe solid line represents the mean difference, the dashed lines represent the upper\nand lower limits of agreement, and the dotted line represents the trend.\nhttps://doi.org/10.1038/s41539-024-00291-1 Brief communication\nnpj Science of Learning|            (2024) 9:79 2\nimplementation and the importance of detailed criteria for enhancing\nfeedback quality in broad assessments.\nMethods\nEssay\nThis study used sample answers derived from the IELTS book series,\nspanning from IELTS 8 to IELTS 18. The chosen essays met speciﬁcc r i t e r i a :\n(1) originating from the academic writing task 2 in IELTS and (2) having\nbeen assigned a score by an ofﬁcial IELTS examiner. In total, 31 essays were\nincluded in the study, with a mean score of 6.0 and a standard deviation of\n1.1, ranging from 3.5 to 8.0. These essays and their corresponding writing\nprompts were extracted for subsequent analysis.\nChatGPT prompt\nTo systematically assess the impact of criteria knowledge on ChatGPT’s\ngrading, we employed a three-stage incremental prompt design. Prompt 1\nadhered to best practices for interacting with ChatGPT by instructing it to\nsimulate an IELTS examiner using zero-shot reasoning. Building upon this\ninitial setup, Prompt 2 introduced the ofﬁcial IELTS grading criteria–\nnamely “task response,”“ coherence and cohesion,”“ lexical resource,” and\n“grammatical range and accuracy”. Prompt 3 expanded on Prompt 2 by\nincorporating comprehensive band descriptors for each criterion. This\nprogressive approach allowed us to assess how varying levels of criterion\nknowledge inﬂuence the alignment of LLMs with criterion-based grading.\nDetailed descriptions of each prompt are available in the supplementary\nnote 1-3.\nProcedure\nInitial assessments were conducted using ChatGPT 3.5. Each essay was\nevaluated in a new chat session to prevent potential inﬂuence from chat\nhistory. Since Prompt 3 exceeded the maximum word count for a single chat\ninput, the ChatGPT PROMPTs Splitter (https://chatgpt-prompt-splitter.\njjdiaz.dev/) was used to segment the prompt. If ChatGPT’s responses did not\nconform to the IELTS scoring guidelines (e.g., rounding scores to the nearest\nwhole or half band score), the respective essay was re-evaluated until a\ncompliant score was provided.\nAfter identifying the most effectiveprompt condition, we reran it using\nChatGPT 4 to assess whether more advanced models (e.g., more model\nparameters) can improve the grading. To testify the generalizability of the\nresults to other LLMs, the most effective prompt was also conducted in\nClaude 3 Haiku, another publicly available LLM with fewer model para-\nmeters but a more recent knowledge base.\nAs the preceding analyses primarily focused on the efﬁcacy of LLM-\ngenerated grading, a preliminary assessment of its effectiveness was con-\nducted by having a high school English teacher from China evaluate the\nsame set of essays. The invited teacher, who held general qualiﬁcations in\nEnglish education but lacked speciﬁc IELTS training or teaching experience,\nprovided grading that reﬂects the typical feedback an English learner might\nreceive from a general educational resource. This grading provides an initial\nreference for contextualizing the effectiveness of our results.\nStatistical analysis\nThree pairwise ICC analyses wereﬁrst conducted, each involving one\nChatGPT rating given by one of the prompts and the ofﬁcial rating. The\ntwo-way random model (also known as ICC2) and two-way mixed model\n(also known as ICC3) were conducted. Compared to ICC3, ICC2 assumes\nﬁxed biases, which gives rise to the distinction– ICC2 gauges the absolute\nagreement, while ICC3 primarily assesses consistency\n17. Evaluating ICC2\nand ICC3 together offers insights into potential biases17. ICCs were con-\nducted using the single-score formula. The point estimates and their 95%\nconﬁdence intervals were reported.\nWe initially examined whether each prompt demonstrated signiﬁcant\nabsolute agreement, deﬁned by a 95% conﬁdence interval that excludes 0.\nT h ep r o m p tt h a tg e n e r a t e ds i g n iﬁcant absolute agreement was subjected to\nfollow-up analyses. The values of ICC2 and ICC3 wereﬁrst inspected. In the\npresence of a discrepancy between ICC2 and ICC3, indicating potential\nbiases, the Bland-Altman plot and t-tests were conducted to examine the\ndistribution and the tendency of the biases\n19.\nAfter identifying the most effective prompt, we assessed its test-retest\nreliability (i.e., the intrarater agreement) by rerunning the prompt using\nChatGPT 3.5 and applying the average-score ICC2 formula. Subsequently,\nwe extended our analysis by rerunning this prompt with ChatGPT 4.0 and\nClaude 3.0 Haiku, as well as having a representative secondary middle\nschool English teacher from China evaluate the same set of essays. The ICCs\nwere calculated for each condition. To determine whether these ICCs dif-\nfered signiﬁcantly from those of the initial ChatGPT 3.5 grading, we applied\nFisher’s Z transformation and conducted statistical comparisons.\nICC qualitative interpretation was guided by Koo & Li\n20: scores below\n0.5 are considered poor, 0.5-0.75 moderate, 0.75-0.9 good, and above 0.90\nexcellent.\nData availability\nThe data that support theﬁndings of this study are available from the\ncorresponding author upon reasonable request.\nReceived: 26 January 2024; Accepted: 16 December 2024;\nReferences\n1. Hosseini, M. et al. An exploratory survey about using ChatGPT in\neducation, healthcare, and research.PLOS ONE18, e0292216 (2023).\n2. Dwivedi, Y. K. et al. So what if ChatGPT wrote it?” Multidisciplinary\nperspectives on opportunities, challenges and implications of\ngenerative conversational AI for research, practice and policy.Int. J.\nInf. Manag.71, 102642 (2023).\n3. Zirar, A. Exploring the impact of language models, such as CHATGPT,\non student learning and assessment.Rev. Educ.11, e3433 (2023).\n4. Evans, C. Making sense of assessment feedback in higher education.\nRev. Educ. Res.83,7 0– 120 (2013).\n5. Winstone, N. E., Nash, R. A., Parker, M. & Rowntree, J. Supporting\nlearners’agentic engagement with feedback: A systematic review and\na taxonomy of recipience processes.Educ. Psychol.52,1 7– 37 (2017).\n6. Hattie, J. & Timperley, H. The power of feedback.Rev. Educ. Res.77,\n81– 112 (2007).\n7. Guo, K. & Wang, D. To resist it or to embrace it? Examining ChatGPT’s\npotential to support teacher feedback in EFL writing.Educ. Inf.\nTechnol. https://doi.org/10.1007/s10639-023-12146-0 (2023).\n8. Farhi, F. et al. Analyzing the students’views, concerns, and perceived\nethics about ChatGPT usage.Comput. Educ. Artif. Intell.5, 100180\n(2023).\n9. Ngo, T. T. A. The perception by university students of the use of\nChatGPT in education.Int. J. Emerg. Technol. Learn.18,4 – 19 (2023).\n10. Shaikh, S., Yayilgan, S. Y., Klimova, B. & Pikhart, M. Assessing the\nusability of ChatGPT for formal English language learning.Eur. J.\nInvestig. Health Psychol. Educ.13, 1937– 1960 (2023).\n11. Escalante, J., Pack, A. & Barrett, A. AI-generated feedback on writing:\nInsights into efﬁcacy and ENL student preference.Int. J. Educ.\nTechnol. High. Educ.20, 57 (2023).\n12. Meyer, J. et al. Using LLMs to bring evidence-based feedback into the\nclassroom: AI-generated feedback increases secondary students’\ntext revision, motivation, and positive emotions.Comput. Educ. Artif.\nIntell. 6, 100199 (2024).\n13. Steiss, J. et al. Comparing the quality of human and ChatGPT\nfeedback of students’ writing. Learn. Instr.91, 101894 (2024).\n14. Tomova, M. et al. Leveraging large language models to construct\nfeedback from medical multiple-choice questions.Sci. Rep.14,\n27910 (2024).\n15. Dai, W. et al. Can large language models provide feedback to\nstudents? A case study on ChatGPT.2023 IEEE Int. Conf. Adv. Learn.\nTechnol. (ICALT)323– 325 (2023).\nhttps://doi.org/10.1038/s41539-024-00291-1 Brief communication\nnpj Science of Learning|            (2024) 9:79 3\n16. Guskey, T. R. Can grades be an effective form of feedback?Phi Delta\nKappan 104,3 6– 41 (2022).\n17. Liljequist, D., Elfving, B. & Skavberg Roaldsen, K. Intraclass\ncorrelation – A discussion and demonstration of basic features.PLOS\nONE 14, e0219854 (2019).\n18. Turetsky, K. M., Sinclair, S., Starck, J. G. & Shelton, J. N. Beyond\nstudents: How teacher psychology shapes educational inequality.\nTrends Cogn. Sci.25, 697– 709 (2021).\n19. Stolarova, M., Wolf, C., Rinker, T. & Brielmann, A. How to assess and\ncompare inter-rater reliability, agreement and correlation of ratings:\nAn exemplary analysis of mother-father and parent-teacher\nexpressive vocabulary rating pairs.Front. Psychol.5, 409 (2014).\n20. Koo, T. K. & Li, M. Y. A guideline of selecting and reporting intraclass\ncorrelation coefﬁcients for reliability research.J. Chiropr. Med.15,\n155– 163 (2016).\nAcknowledgements\nWe sincerely thank the high school English teacher, LY, for her time and\nexpertise in grading the essays for this study. No funding was granted for\nthe study.\nAuthor contributions\nD.-W.Z. conceptualized and designed the study, supervised the project,\nperformed statistical analyses, and led the manuscript drafting and revision\nprocess. M.B. contributed to the study design, conducted data collection,\nperformed statistical analyses, and contributed to the manuscript drafting\nand editing. Y.Y.T. contributed to the study design, conducted data\ncollection, and contributed to the manuscript drafting and editing. A.H.S.J.\nassisted with data preprocessing, literature review, and preparation of the\nmanuscript. All authors reviewed and approved theﬁnal version of the\nmanuscript for submission.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41539-024-00291-1\n.\nCorrespondenceand requests for materials should be addressed to\nDa-Wei Zhang.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© Crown 2024\nhttps://doi.org/10.1038/s41539-024-00291-1 Brief communication\nnpj Science of Learning|            (2024) 9:79 4",
  "topic": "Grading (engineering)",
  "concepts": [
    {
      "name": "Grading (engineering)",
      "score": 0.6627751588821411
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.5552760362625122
    },
    {
      "name": "Agreement",
      "score": 0.5278294682502747
    },
    {
      "name": "Computer science",
      "score": 0.4143558144569397
    },
    {
      "name": "Natural language processing",
      "score": 0.40039581060409546
    },
    {
      "name": "Statistics",
      "score": 0.3399977385997772
    },
    {
      "name": "Mathematics",
      "score": 0.3072561025619507
    },
    {
      "name": "Linguistics",
      "score": 0.2673136591911316
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2620276212692261
    },
    {
      "name": "Engineering",
      "score": 0.11790478229522705
    },
    {
      "name": "Philosophy",
      "score": 0.07547160983085632
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    }
  ]
}