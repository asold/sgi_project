{
  "title": "Masked Pre-trained Encoder base on Joint CTC-Transformer",
  "url": "https://openalex.org/W3026842484",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2101685518",
      "name": "Liu Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2677329901",
      "name": "Huang Yi-Heng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3015265920",
    "https://openalex.org/W2746192915",
    "https://openalex.org/W2808640845",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W1637570796",
    "https://openalex.org/W2948210185",
    "https://openalex.org/W2941814890",
    "https://openalex.org/W2775102587",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2193413348",
    "https://openalex.org/W3008191852",
    "https://openalex.org/W2963850025",
    "https://openalex.org/W2798657914",
    "https://openalex.org/W2912889105",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2158905201",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2294567968"
  ],
  "abstract": "This study (The work was accomplished during the internship in Tencent AI lab) addresses semi-supervised acoustic modeling, i.e. attaining high-level representations from unsupervised audio data and fine-tuning the parameters of pre-trained model with supervised data. The proposed approach adopts a two-stage training framework, consisting of masked pre-trained encoder (MPE) and Joint CTC-Transformer (JCT). In the MPE framework, part of input frames are masked and reconstructed after the encoder with massive unsupervised data. In JCT framework, compared with original Transformer, acoustic features are applied as input instead of plain text. CTC loss performs as the prediction target on top of the encoder, and decoder blocks remain unchanged. This paper presents a comparison between two-stage training method and the fully supervised JCT. In addition, this paper investigates the our approach's robustness against different volumns of training data. Experiments on the two-stage training method deliver much better performance than fully supervised model. The word error rate (WER) with two-stage training which only exploits 30\\% of WSJ labeled data achieves 17\\% reduction than which trained by 50\\% of WSJ in a fully supervised way. Moreover, increasing unlabeled data for MPE from WSJ (81h) to Librispeech (960h) attains about 22\\% WER reduction.",
  "full_text": "MASKED PRE-TRAINED ENCODER BASE ON JOINT\nCTC-TRANSFORMER\nLu Liu, Yiheng Huang\nTsinghua-Berkeley Shenzhen Institute, Tsinghua University\nTencent AI lab\nliulu18@mails.tsinghua.edu.cn, arnoldhuang@tencent.com\nAbstract\nThis study1 addresses semi-supervised acoustic modeling, i.e.\nattaining high-level representations from unsupervised audio\ndata and ﬁne-tuning the parameters of pre-trained model with\nsupervised data. The proposed approach adopts a two-stage\ntraining framework, consisting of masked pretained encoder\n(MPE) and Joint CTC-Transformer (JCT). In the MPE frame-\nwork, part of input frames are masked and reconstructed after\nthe encoder with massive unsupervised data. In JCT frame-\nwork, compared with original Transformer, acoustic features\nare applied as input instead of plain text. CTC loss performs\nas the prediction target on top of the encoder, and decoder\nblocks remain unchanged. This paper presents a comparison be-\ntween two-stage training method and the fully supervised JCT.\nIn addition, this paper investigates the our approach’s robust-\nness against different volumns of training data. Experiments on\nthe two-stage training method deliver much better performance\nthan fully supervised model. The word error rate (WER) with\ntwo-stage training which only exploits 30% of WSJ labeled data\nachieves 17% reduction than which trained by 50% of WSJ in a\nfully supervised way. Moreover, increasing unlabeled data for\nMPE from WSJ (81h) to Librispeech (960h) attains about 22%\nWER reduction.\nIndex Terms: unsupervised learning, transformer, ASR\n1. Introduction\nData labeling is quite time consuming and costly. However,\nmost existing high-performance automatic speech recognition\n(ASR) systems require substantial speech data paired with tran-\nscriptions. In order to make use of unlabeled data, we pro-\npose a semi-supervised training method which combines unsu-\npervised pre-training [1] and supervised training together. Our\npre-training method is mainly inspired by the unsupervised pre-\ntraining approach in natural language processing (NLP) tasks,\nespecially the most representative workBERT [2], which has re-\nfreshed the state-of-the-art of dozens of NLP tasks. Our super-\nvised training structure JCT is inspired by CTC [4] and Trans-\nformer [3]. CTC is a commonly used end-to-end speech recog-\nnition loss function in recurrent neural network (RNN) based\nmodels, such as [25]. Transformer is an encoder-decoder model\nthat is widely applied in sequence to sequence tasks, such as\n[5, 6, 7, 8]. Transformer shows much better performance in\nparallel computing and long sequences modeling, compared to\nRNN based models [10, 11]. In JCT model, CTC simply acts\nas an auxiliary function in supervised training process. Con-\nsequently, we exploit a shared encoder to train CTC and trans-\nformer jointly during the supervised training process.\n1The work was accomplished during the internship in Tencent AI\nlab\nBERT is a pre-trained language model (LM) consisting of\nmasked LM task and next sentence prediction task. These two\ntasks respectively capture word-level representation and sen-\ntence level representation. While for ASR tasks, audio exam-\nples in training dataset have no relationship and lack of contex-\ntual coherent information between each other. Thus we abandon\nnext sentence prediction task in our pre-trained model. Mean-\nwhile, in masked LM task, BERT generates masks for original\ntext data with special mask token ([MASK]). However acoustic\nfeatures such as Mel-Frequency Cepstral Coefﬁcients (MFCC)\nfeatures and log-mel ﬁlter bank (FBANK) features [12] are\nmuch more complex than plain text features - the unclear align-\nment between acoustic frames and their transcriptions make it\nimpossible to mask raw audio data in semantic level. Natu-\nrally, we mask the frames in neural networks. The implemen-\ntation structure of the pre-trained model is a deep bidirectional\ntransformer. Figure 1 demonstrates the structure of masked pre-\ntrained model. We use FBANK features as input and mask 15%\nof the input down-sampled frames. Different from the con-\nventional approach, our pre-training process exploits the infor-\nmation from both past and future frames to establish present\nmasked frame, frames are then reconstructed as context rep-\nresentations. As a kind of novel high-level representations of\nDown sampling layer\n00 0\nFeed-Forward\nMultihead-Attention\nLayer Norm\nLayer Norm\nL1\u0003loss\nReconstruction frames\nEncoder block\nFbank features\nMasked frames \nŏ\nFeed-Forward\nxi xj xk\nyi yj yk…\n…\nFigure 1: The structure of masked pre-trained model\nacoustic features, these representations are less-noisy that can\nbe conveniently applied to dispose downstream speech process-\ning tasks, such as speaker recognition, speaker veriﬁcation and\nspeech enhancement. In our paper, we explored the downstream\ntask of low resource speech recognition to show that masked\npre-trained encoder (MPE) is capable of improving supervised\nlearning.\nExisting work related to unsupervised pre-training in\nspeech recognition mainly focuses on the approaches of extract-\ning high-level acoustic representations. Wave2vec [13] pro-\narXiv:2005.11978v2  [eess.AS]  13 Aug 2020\nposed an unsupervised pre-training method by learning from\nthe original audio signals rather than FBANK features, opti-\nmized by the noise contrastive estimation (NCE) of a binary\nclassiﬁcation task. Contrastive Predictive Coding (CPC) [14]\ncompresses the higher-dimensional data into a more compact\npotentially embedded space where conditional prediction is eas-\nier to be modeled. Then the researchers construct powerful au-\ntoregressive models in this potential space to make multi-step\nfuture predictions. CPC is also optimized by NCE. Compared\nwith CPC, Autoregressive Predictive Coding (APC) [15] mainly\nfocus on predicting the spectrum of a future frame rather than\na wave sample, which appears like language model. The re-\nsearchers use RNN based model to reconstruct temporal frame\nwith information from its past frames, and the optimization tar-\nget is reconstruction discrepancy.\nRecently published literature Deep Contextualized Acous-\ntic Representations (DeCoAR) [16] introduces a new represen-\ntation learning method. In this paper, a temporal slice of ﬁl-\nterbank features from past and future context frames are re-\nconstructed, the model is implemented by bi-directional LSTM\nnetworks and optimization target is reconstrction error. Mock-\ningjay [17] proposes a speech representation learning approach\nas BERT, where bidirectional Transformer encoders are pre-\ntrained on a large amount of unlabeled speech data and these\nrepresentations are applied to a wide range of downstream tasks\nin ASR. Unlike their work, we mask the frames after down\nsampling layer while Mockingjay directly masks FBANK fea-\ntures before down sampling layer. We also expolit different\ndown-samling method and distinctive supervised learning strat-\negy from Mockingjay.\n2. Methodology\nThe general framework of our proposed approach is illustrated\nin Figure 2. Given untranscribed speech data of a corpus, MPE\nis pretrained in the ﬁrst stage. Next, JCT model is trained with\na small amount of supervisd data. Pretrained features created\nby the MPE perform as high level representations. Two dif-\nferent ﬁne-tuning approaches are adopted consisting of directly\nﬁne-tuning and frozen ﬁne-tuning. The whole pipeline of our\napproach will be compared with a system consisting of only su-\npervised data. Moreover, increasing the training data in MPE\nprocess will also be compared during experimental design.\n2.1. Unsupervised pre-trained encoder\nTo encode temporal information, current methods such as CPC\nand wave2vec adopt autoregressive models, which limits the po-\ntential of speech representation learning, and also decrease the\ntraining speed. By contrast, we leverage bi-directional trans-\nformer to reconstruct masked frame with its past and future\nframes. The structure of MPE is illustrated on the left of Fig-\nure 2, which consists of three parts: down sampling layer, mask\nlayer, bi-directional Transformer block. Considering faster cal-\nculation in training process, we place a downsampling layer be-\nfore transformer block to exploit the structure locality of acous-\ntic spectrograms [18]. The down-sampling layer consists of two\nconvolutional neural networks (CNN). And the striding meth-\nods in both two CNN reduce the length of feature map to a\nquarter of its original length. After that, we add a linear pro-\njection layer to reshape the dimensions of features to ﬁt for the\ndimension of the transformer input. Then we present a random\nmask after the linear hidden layer with following setup: 15% of\nthe input frames need to be masked. The chosen frames are re-\nConv2d/2\nConv2d\u0012\u0015\nXt-3 Xt-2 Xt-1 Xt …X5 X6 X7 X8X1 X2 X3 X4\nh1 h2 h3 hT…\nLinear\nShared\nlayer Feed-forward\nSelf-Attention\nNe layers\ne1 e2 e3 eu…\ny1 — y2 yu…—\nCTC loss\nPositional\nEmbedding\nPositional\nEmbedding\nOutput \nembedding\nsos y1 y2 y3 yN…\nMasked \nMulti-head \nAttention\nLayer Norm\nMulti-head \nAttention\nLayer Norm\nFeed forward\nLayer Norm\nLinear\nSoftmax\nNd layers\nDecoder\n…y1 y2 y3 eos\nMPE\ndown sampling\nlayer\nmask\nlayer\nTransformer\nSubBlock\nFigure 2: The structure of the semi-supervised JCT\nplaced with zero vectors for 80% of the time, with frames from\nrandom positions 10% of the time, and kept to be the same in\nthe rest of the time. We also add sinusoidal positional embed-\nding to the input features. The bi-directional Transformer block\nconsists of Ne layers of modules that can be stacked on top of\neach other multiple times. Each module composed of two sub-\nlayers: multi-head attention layer and feed-forward layer, each\nsub-layer has a residual connection around it, and is followed by\na layer-normalization step. Given tas length of input features,\nT as length of MPE output sequences. x = (x1,x2,...,x t), e\n= (e1,e2,...,e T ) respectively represent input features and re-\nconstructed representations. h = (h1,h2,...,h T ) is the masked\ndown-sampled acoustic features.\nh = Mask(Conv(Conv(x)) (1)\ne = h + SubBlock(h) (2)\nThus, the reconstruction discrepancy can be depicted as:\nLpre =\nT∑\ni=1\n|hi −ei| (3)\nThe element in loss function merely contains the frames that\nkeep unchanged rather than those that have been masked.\n2.2. Supervised encoder-decoder\nWe exploit JCT in down stream supervised tasks. Based on\nthe encoder-decoder structure of Transformer, on top of the\nencoder, CTC loss has been added as the prediction target.\nPure CTC-based model always works together with a language\nmodel because of its independent assumption on the output ele-\nments. Pure attention-based model is hard to learn from scratch\ndue to the sensitivity of attention mechanism [19]. Conse-\nquently, we integrate CTC with Transformer through the shared\nencoder MPE. In our experiments, attention mechanism tends\nto be impacted by noise, which leads to bad alignment for\nmodeling. The forward-backward algorithm of CTC loss pre-\ncisely enforces monotonic alignment between input and output.\nThus JCT performs more robust than the purely attention based\nmodel and CTC based model. Moreover, using CTC as an aux-\niliary optimization function speeds up the process of estimating\nthe desired alignment than solely depending on data-driven at-\ntention methods.\nThe right part of Figure 2 illustrates the structure of de-\ncoder, which is similar to the encoder, except for the masked\nmulti-head attention module. To prevent adding future infor-\nmation and preserve the auto-regressive manner in the decoder,\nthe masks in the masked multi-head attention module sweep out\nall values of illegal connections. This masking of the sequence\ncan be achieved in parallel using an elementwise product with\na triangular binary matrix. y = (y1,y2,...,y N ) represent the\ntranscriptions of audio data.\nLCTC =\n∑\n(x,y)\n−log(P(y|x)) (4)\nLAttention = −log P(y|x) =−\n∑\nu\nlog P(y∗\nu|x, y∗\n[1:u−1]) (5)\nwhere y∗\n[1:u−1] is the ground truth of the previous words. The\njoint training method of CTC with Transformer works as:\nLJCT = αLCTC + (1−α)LAttention (6)\nαis a hyper-parameter : 0 ⩽ α⩽ 1.\n2.3. Fine-tuning methods\nWe leverage massive unsupervised audio data to train MPE. The\ntraining process won’t stop until the result in validation dataset\nconverges to the threshold of pre-training loss. After the com-\npletion of pre-training, we propose two approaches for the ﬁne-\ntuning stage:\n* Directly ﬁne-tuning: Initialize the trainable parameters\nof encoder in JCT with the results we get from the pre-\ntraining process of MPE, then use labeled data to opti-\nmize the supervised joint loss function (JCT).\n* Frozen ﬁne-tuning: MPE provides more implicit and\nhigh-level representations than FBANK features. In the\nﬁne-tuning process, it performs better when we freeze\nthe encoder, and only train the parameters of JCT de-\ncoder. Speciﬁcally, frozen ﬁne-tuning means remove\nthe parameters of MPE from the trainable parameters\nof JCT. After the accomplishment of decoder training\nprocess, for better performance, we can train the whole\nstructure in a supervised manner for a few epochs.\nIn our experiments, we have explored both ﬁne-tuning meth-\nods, with the latter showing much better performance than the\nformer. Essentially, the former ﬁne-tuning method is a simple\ninitialization of encoder in the supervised training stage, inte-\ngrated with randomly initialized decoder will lose some infor-\nmation we attain from unlabeled data. If we choose the former\nmethod, the difference between directly ﬁne-tuning method and\ntotally supervised training method will be very small. While\nthe latter one thoroughly uses the representation from massive\nunsupervised data, it shows much lower word error rate (WER)\nthan totally supervised training in a low resource setting. The\nresult are demonstrated in section 5.\n3. Experiments\n3.1. Datasets\nWe carried out experiments on LibriSpeech corpus and wall\nstreet journal (WSJ) corpus [23]. For LibriSpeech [24] which\ncontains 960 hours training audio data, we used the entire\ndataset to train MPE for high-level feature extraction. In the\nﬁne-tuning process, we exploited train-clean-100 and train-\nclean-360 for supervised training, dev-clean for validation and\ntest-clean for evaluation. As for WSJ, the models were training\non si284 which includes about 81 hours audio data, validating\non dev93 and evaluating on eval92. To evaluate the effect of\nMPE, we leverage the whole dataset for pre-training while one\nthird, a half and the entire dataset are respectively used for su-\npervised training. Meanwhile, an ideal feature extractor should\nextract representations that generalize to datasets of different\ndomains. Thus, to examine the robustness of shifting in do-\nmains, we ﬁrstly trained MPE on LibriSpeech, then ﬁne-tuned\nit to JCT with WSJ 81 hours supervised data. We choose totally\nsupervised training on JCT as our baseline.\n3.2. Experiment setups\nThe input acoustic features are 80-dimensional ﬁlterbanks ex-\ntracted with a hop size of 10ms and a window size of 25ms,\nextended with temporal ﬁrst and second order differences. Fea-\nture extraction also includes per-speaker mean subtraction and\nvariance normalization [5]. The MPE consists of 2 CNN lay-\ners with RELU activation function and a stack of 12 encoder\nblocks, CNN has stride size 2 and kernel size 3 for down sam-\npling. The channels of ﬁrst layer is 64, next layer has twice as\nmany channels as the previous one. For encoder blocks, each\nblock contains two sub-layers: feed-forward layer (FFL) and\nself-attention layer (SAL), the dimension of FFL is 2048, as for\nSAL, the attention heads is 4 and dimension of embedding is\n512. The SAL and FFL in the decoder obeys the same conﬁg-\nuration, while the number of decoder stacked blocks is set to\n6. We used Adam optimizer with default parameter conﬁgura-\ntion in both two-stage training. Especially in supervised train-\ning process, we applied warming up method to vary the learning\nrate in the whole training process with Noam learning strategy.\nlr =k ∗d−0.5 ∗min(n−0.5, n∗warmup−1.5) (7)\nk, d, n, warmup respectively refers to a tunable hyper-\nparameter, model dimension, training step, total warming up\nsteps. The learning rate increased in start warming up n steps\nand decreased after the peak of lr. In our experiments, warming\nup steps n = 25000, hyper-parameter k = 10. To avoid over-\nﬁtting, label smoothing strategy which was proposed in [20]\nwas also applied in the training process, and the label smooth-\ning weight is set as 0.1. Meanwhile, both of residual dropout\nand attention dropout [21] were set to 0.1. Moreover, we also\nused SortaGrad [22] method in the ﬁrst training epoch for faster\nconvergence and less noise inference. Apart from above conﬁg-\nuration, for the multi-task training process, the hyper-pramater\nαis set as 0.3.\n4. Results\n4.1. Pre-training results\nIn pre-training stage, in order to measure the reconstruction\ndiscrepancy, we have tried L1 loss, optimized by Adam op-\ntimizer. Figure 3 shows the tendency of alignment between\noriginal frames and reconstruction frames. From left to right\nrespectively represents the matrix image in epoch1, epoch5\nand epoch20. In ﬁrst epoch, the self-attention matrix image is\nrandom but gradually become orthogonal after several training\nepochs.\nTable 1: Results on WSJ corpus\nrepresentation unlabeled labeled ﬁne-tuning\nsteps dev93 eval92 baseline(supervised)\ndev93 eval92\nMPE WSJ(81h) one-third(25h) 5500 10.43 9.31 15.05 12.54\nMPE WSJ(81h) half(40h) 3300 9.77 7.04 12.58 10.07\nMPE WSJ(81h) WSJ(81h) 15000 6.79 4.26 7.93 5.48\nMPE LibriSpeech(960h) WSJ(81h) 12000 5.82 3.48 7.93 5.48\nwav2vec[13] LibriSpeech(960h) WSJ(81h) - 6.84 3.97 - -\nDeCoAR[16] LibriSpeech(960h) WSJ(81h) - 6.30 3.17 - -\nDeCoAR[16] WSJ(81h) WSJ(81h) - 8.34 4.64 - -\nTable 2: Results on LibriSpeech corpus\nunlabeled data labeled data ﬁne-tuning steps dev clean test clean baseline(supervised)\ndev clean test clean\nLibriSpeech(960h) train-clean-100 7500 8.12 9.68 11.63 12.17\nLibriSpeech(960h) train-clean-360 13000 6.44 7.83 8.35 9.70\n- LibriSpeech-960 - - - 3.24 3.77\n125 22525\n25\n125\n225\n125 22525\n25\n125\n225\nepoch 1 epoch 20epoch5\ninput\noutput\n125 22525\n25\n125\n225\ninput input\noutput\noutput\nFigure 3: self-attention matrix image of one head in MPE from\nexample4kac031f. The horizontal axis represents input frames\nto the self-attention block, the vertical axis refers to the output\nframes of encoder.\n4.2. Supervised ﬁne-tuning results\nIn order to evaluate the two ﬁne-tuning methods that proposed\nin section 2.3, a simple experiment on WSJ subset with the two\nmethods has been implemented. Table 3 revealed frozen ﬁne-\ntuning method performes better than simply initialize the en-\ncoder. The given results in all these tables are an average of\nTable 3: comparison of two ﬁne-tuning methods\nFine-tuning\nmethods unlabeled labeled dev93\nDirectly ﬁne-tuning WSJ(81h) WSJ(25h) 14.77\nFrozen ﬁne-tuning WSJ(81h) WSJ(25h) 10.43\nWER in two runs. Speciﬁcally, in the decoding stage, we ap-\nplied beam search (beam width=10) and CTC decoding method.\n4.2.1. Results on WSJ\nThe results of WSJ are depicted in Table 1. In WSJ cor-\npus, for comparison, we select one-third, a half and entire data\nfrom it respectively. Firstly, we trained the three subsets on\nJCT structure in fully supervised manner without pre-training\nprocess. Afterwards, we trained MPE with the whole si284\nwhich contains 81h audio data without its transcription, then\nthe three subsets are used for supervised training stage. The re-\nsults shows:compared with fully supervised training, two stage\ntraining achieves 22% wer reduction on dev93 and 30% wer\nreduction on eval92. Besides, in order to test the robustness\nof masked pre-training method, we applied MPE which was\ntrained by LibriSpeech-960h domain to WSJ 81h domain for\nparameters’ ﬁne-tuning. We can see from the table that increas-\ning unlabeled data for MPE naturally attains better results, it\nachieves 22% WER reduction in WSJ (81h).\nIn the bottom of Table 1, we provide the comparison of\nMPE and other related published representations: wav2vec, De-\nCoAR. Wave2vec constructs a model of ﬁve-layer convolutional\nneural networks. DeCoAR constructs a structure of LSTM ne-\ntural networks. Compared to these two structure, we achieved\n15% wer reduction than wav2vec and 7% wer reduction than\nDeCoAR on dev93. While the result on eval92 behaves not so\ndesirable, we consider that the data set is approaching saturation\nand we’ll propose some new idea to address this issue in future\nwork.\n4.2.2. Results on LibriSpeech\nTable 2 demonstrates the results of Librispeech subsets. MPE\nhas been trained on 960 hours Librispeech unlabeled audio data,\nwhile train-clean-100 and train-clean-360 were chosen to be\nlabeled dataset in ﬁne-tuning stage. The supervised baseline are\nalso given in the table. Compared with the baseline, two-stage\ntraining obtained 34% and 25% wer reduction on dev cleanand\ntest clean, respectively.\n5. Conclusion\nAccording to all the above experiments, two-stage training\nhas signiﬁcantly remarkable performance better than fully-\nsupervised training. It suggests that with massive unlabeled data\nand limited labeled data we can achieve the same performance\nwith the system which has been trained by a large amount of\nsupervised data. Meanwhile, relying on the powerful modeling\nability of Transformer, the masked pre-trained representation\ncan be widely used to other down stream speech tasks.\n6. References\n[1] Barlow, H. B. “Unsupervised learning”. Neural computation,\n(1989): 295-311.\n[2] Devlin J, Chang M W, Lee K, et al. “Bert: Pre-training of deep bidi-\nrectional transformers for language understanding”. arXiv preprint\narXiv:1810.04805., 2018\n[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.\nN. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need\nCoRR, vol. abs/1706.03762, 2017\n[4] A. Graves, S. Fern ¨ andez, F. Gomez, and J. Schmidhuber. Connec-\ntionist temporal classiﬁcation: labelling unsegmented se- quence\ndata with recurrent neural networks in Proceedings of the 23rd in-\nternational conference on Machine learning,2006, pp. 369376.\n[5] D. Linhao, X. Shuang, and X. Bo. Speech-transformer: a no-\nrecurrence sequence-to-sequence model for speech recognition in\n2018 IEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP). IEEE, 2018, pp. 58845888.\n[6] Z. Shiyu, D. Linhao, X. Shuang. and X. Bo, Syllable-based\nsequence-to-sequence speech recognition with the transformer in\nmandarin chinese. arXiv preprint arXiv:1804.10752, 2018\n[7] Z. Shiyu, D. Linhao, X. Shuang. and X. Bo, A compari-\nson of modeling units in sequence-to-sequence speech recogni-\ntion with the transformer on mandarin chinese. arXiv preprint\narXiv:1804.10752, 2018.\n[8] Z. Shiyu,X. Shuang, and X. Bo. Multilingualend-to-endspeech\nrecognition with a single transformer on low-resource languages.\narXiv preprint arXiv:1806.05059, 2018\n[9] A. Zeyer, P. Bahar, K. Irie. R. Schl ¨ uter, and H. Ney A com-\nparison of transformer and lstm encoder decoder models for asr\nin IEEE Automatic Speech Recognition and Understanding Work-\nshop, Sentosa, Singapore,2019.\n[10] A. Graves. Sequence transduction with recurrent neural networks\nCoRR, vol. abs/1211.3711, 2012\n[11] H. Sak, M. Shannon, K. Rao and F. Beaufays Recurrent neural\naligner: An encoder-decoder neural network model for sequence to\nsequence mapping in Proc. Interspeech 2017, 2017, pp. 12981302.\n[12] Muda, Lindasalwa, M. Begam, I. Elamvazuthi. “V oice recogni-\ntion algorithms using mel frequency cepstral coefﬁcient (MFCC)\nand dynamic time warping (DTW) techniques”. arXiv preprint\narXiv:1003.4083 ,2010\n[13] Y . Ye and X . Guangxu and Suo. Qiuling and J. Kebin and Zhang.\nAidong. “Wave2vec: Learning deep representations for biosig-\nnals” in 2017 IEEE International Conference on Data Mining\n(ICDM)IEEE, 2017.\n[14] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. “Represen-\ntation learning with contrastive predictive coding.” arXiv preprint\narXiv:1807.03748,2018.\n[15] C. Yu-An, H. Wei-Ning, T. Hao , and J. Glass. An unsuper-\nvised autoregressive model for speech representation learning, In-\nterspeech, Sep 2019.\n[16] L. Shaoshi and L. Yuzong and S. Julian and K. Katrin. “Deep Con-\ntextualized Acoustic Representations For Semi-Supervised Speech\nRecognition”. arXiv preprint arXiv:1912.01679,2019\n[17] L. Andy T and Y . Shu-wen and C. Po-Han and H. Po-chun and\nL. Hung-yi. “Mockingjay: Unsupervised Speech Representation\nLearning with Deep Bidirectional Transformer Encoders” arXiv\npreprint arXiv:1910.12638. 2019\n[18] A. Mohamed, D. Okhonko, L. Zettlemoyer. Transformers with\nconvolutional context for asr, arXiv preprint arXiv:1904.11660.\n2019.\n[19] S. Kim, T. Hori, and S. Watanabe. Joint ctc-attention based end-\nto-end speech recognition using multi-task learning in 2017 IEEE\ninternational conference on acoustics, speech and signal process-\ning (ICASSP). IEEE, 2017, pp. 48354839.\n[20] R. M ¨ uller, S. Kornblith, and G. E. Hinton. When does label\nsmoothing help? CoRR, vol. abs/1906.02629, 2019.\n[21] Baldi, P. Sadowski, P. J. “Understanding dropout.” InAdvances in\nneural information processing systems(pp. 2814-2822), 2013\n[22] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Bat-\ntenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen\net al. Deep speech 2: End-to-end speech recognition in english\nand mandarin, in International conference on machine learning,\npp. 173182,2016\n[23] X. Aubert, C. Dugast, H. Ney and V . Steinbiss,“Large vocabulary\ncontinuous speech recognition of Wall Street Journal data,” Pro-\nceedings of ICASSP IEEE International Conference on Acoustics,\nSpeech and Signal Processing, Adelaide. 1994\n[24] Panayotov, Vassil, et al.“Librispeech: an asr corpus based on pub-\nlic domain audio books.” 2015 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2015.\n[25] Amodei D, Ananthanarayanan S, Anubhai R, et al. Deep\nspeech 2: End-to-end speech recognition in english and man-\ndarin[C]//International conference on machine learning. 2016:\n173-182.",
  "topic": "Encoder",
  "concepts": [
    {
      "name": "Encoder",
      "score": 0.7027974128723145
    },
    {
      "name": "Computer science",
      "score": 0.6738715171813965
    },
    {
      "name": "Transformer",
      "score": 0.6396843791007996
    },
    {
      "name": "Speech recognition",
      "score": 0.623343288898468
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5753904581069946
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49700334668159485
    },
    {
      "name": "Word error rate",
      "score": 0.48808538913726807
    },
    {
      "name": "Joint (building)",
      "score": 0.44678157567977905
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.43248844146728516
    },
    {
      "name": "Training set",
      "score": 0.4201755225658417
    },
    {
      "name": "Machine learning",
      "score": 0.3479582667350769
    },
    {
      "name": "Engineering",
      "score": 0.2008400857448578
    },
    {
      "name": "Voltage",
      "score": 0.07709905505180359
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ]
}