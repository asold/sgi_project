{
    "title": "Enhancing heart disease prediction using a self-attention-based transformer model",
    "url": "https://openalex.org/W4390588437",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2100528869",
            "name": "Atta Ur Rahman",
            "affiliations": [
                "Prince Sultan University",
                "Riphah International University"
            ]
        },
        {
            "id": "https://openalex.org/A2755710206",
            "name": "Yousef Alsenani",
            "affiliations": [
                "Prince Sultan University",
                "King Abdulaziz University"
            ]
        },
        {
            "id": "https://openalex.org/A2484075814",
            "name": "Adeel Zafar",
            "affiliations": [
                "Riphah International University"
            ]
        },
        {
            "id": "https://openalex.org/A2122581643",
            "name": "Kalim Ullah",
            "affiliations": [
                "Kohat University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4202123013",
            "name": "Khaled Rabie",
            "affiliations": [
                "University of Johannesburg",
                "Manchester Metropolitan University"
            ]
        },
        {
            "id": "https://openalex.org/A1838938422",
            "name": "Thokozani Shongwe",
            "affiliations": [
                "University of Johannesburg"
            ]
        },
        {
            "id": "https://openalex.org/A2100528869",
            "name": "Atta Ur Rahman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2755710206",
            "name": "Yousef Alsenani",
            "affiliations": [
                "King Abdulaziz University"
            ]
        },
        {
            "id": "https://openalex.org/A2484075814",
            "name": "Adeel Zafar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2122581643",
            "name": "Kalim Ullah",
            "affiliations": [
                "Kohat University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4202123013",
            "name": "Khaled Rabie",
            "affiliations": [
                "Manchester Metropolitan University",
                "University of Johannesburg"
            ]
        },
        {
            "id": "https://openalex.org/A1838938422",
            "name": "Thokozani Shongwe",
            "affiliations": [
                "University of Johannesburg"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3122226502",
        "https://openalex.org/W2155018999",
        "https://openalex.org/W3004025693",
        "https://openalex.org/W4225632040",
        "https://openalex.org/W4282914967",
        "https://openalex.org/W4376274168",
        "https://openalex.org/W4366091323",
        "https://openalex.org/W4311422092",
        "https://openalex.org/W6600186770",
        "https://openalex.org/W4210247145",
        "https://openalex.org/W3035159033",
        "https://openalex.org/W3037146579",
        "https://openalex.org/W2784168210",
        "https://openalex.org/W4221059823",
        "https://openalex.org/W3092745004",
        "https://openalex.org/W4321369284",
        "https://openalex.org/W2795340517",
        "https://openalex.org/W2793303269",
        "https://openalex.org/W2518582440",
        "https://openalex.org/W4377089756",
        "https://openalex.org/W4281649558",
        "https://openalex.org/W4322753871",
        "https://openalex.org/W3005066859",
        "https://openalex.org/W3134840015",
        "https://openalex.org/W3134537993",
        "https://openalex.org/W3117057940",
        "https://openalex.org/W2954788759",
        "https://openalex.org/W1532110920",
        "https://openalex.org/W3155296215",
        "https://openalex.org/W3117831081",
        "https://openalex.org/W2284851926",
        "https://openalex.org/W2742491462",
        "https://openalex.org/W4379648112",
        "https://openalex.org/W2618353736",
        "https://openalex.org/W3026299583",
        "https://openalex.org/W179875071",
        "https://openalex.org/W4320736866",
        "https://openalex.org/W2517259736",
        "https://openalex.org/W2690721124",
        "https://openalex.org/W4214821016",
        "https://openalex.org/W3187796790",
        "https://openalex.org/W4206919704",
        "https://openalex.org/W2949767632",
        "https://openalex.org/W3170852091",
        "https://openalex.org/W4313889489",
        "https://openalex.org/W4200602311",
        "https://openalex.org/W4360612605"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |          (2024) 14:514  | https://doi.org/10.1038/s41598-024-51184-7\nwww.nature.com/scientificreports\nEnhancing heart disease prediction \nusing a self‑attention‑based \ntransformer model\nAtta Ur Rahman 1,3*, Yousef Alsenani 2,3, Adeel Zafar 1, Kalim Ullah 4, Khaled Rabie 5,6 & \nThokozani Shongwe 6\nCardiovascular diseases (CVDs) continue to be the leading cause of more than 17 million mortalities \nworldwide. The early detection of heart failure with high accuracy is crucial for clinical trials and \ntherapy. Patients will be categorized into various types of heart disease based on characteristics like \nblood pressure, cholesterol levels, heart rate, and other characteristics. With the use of an automatic \nsystem, we can provide early diagnoses for those who are prone to heart failure by analyzing their \ncharacteristics. In this work, we deploy a novel self‑attention‑based transformer model, that combines \nself‑attention mechanisms and transformer networks to predict CVD risk. The self‑attention layers \ncapture contextual information and generate representations that effectively model complex patterns \nin the data. Self‑attention mechanisms provide interpretability by giving each component of the input \nsequence a certain amount of attention weight. This includes adjusting the input and output layers, \nincorporating more layers, and modifying the attention processes to collect relevant information. \nThis also makes it possible for physicians to comprehend which features of the data contributed to \nthe model’s predictions. The proposed model is tested on the Cleveland dataset, a benchmark dataset \nof the University of California Irvine (UCI) machine learning (ML) repository. Comparing the proposed \nmodel to several baseline approaches, we achieved the highest accuracy of 96.51%. Furthermore, the \noutcomes of our experiments demonstrate that the prediction rate of our model is higher than that of \nother cutting‑edge approaches used for heart disease prediction.\nHeart disease refers to any condition that impairs the heart’s capacity to function normally. In recent years, CVD \nhas become the leading cause of death in the world. Congestive heart failure (CHF) prevalence is expected to \nrise by 46% by 2030 compared to 2012  rates1. The incidence and mortality rates of CVD can be significantly \nlowered by diagnosing the problem, according to research, in both patients who are already aware of their con-\ndition and those who are  not2. Early detection and diagnosis can result in prompt interventions and suitable \ntherapies, which can enhance patient outcomes and lower the chance of problems. The successful diagnosis of \ncardiac abnormalities and valve heart disorders (VHDs) in recent years has been made possible by the use of \nphonocardiogram (PCG) data in combination with ML techniques. These algorithms use a variety of feature \nextraction methods and classifiers to precisely identify and diagnose cardiac  problems3. Traditional ML meth-\nods have numerous drawbacks despite their potential. The methods frequently lack precision and robustness, \nwhich can result in false positive or false negative  results4. The iterative nature of feature selection and classifier \noptimization procedures can frequently take a lot of time, which can impede the prompt diagnosis and effective \ntreatment of cardiac  disease5. Deep learning (DL) algorithms, supported by big-data techniques, have become an \neffective tool for identifying and recognizing cardiac disease in order to get around these restrictions. In many \ndifferent fields, including image classification, computer vision, object localization, electroencephalogram (EEG) \nsignal classification for brain-computer interfaces, and physics-informed neural networks, among  others6, DL \nalgorithms have achieved remarkable success. They can automatically extract non-linear and hierarchical features \nfrom large  datasets7. We may be able to increase the reliability and accuracy of cardiac disease detection and \nOPEN\n1Riphah Institute of System Engineering, Riphah International University Islamabad, Islamabad 46000, \nPakistan. 2Department of Information Systems, FCIT, King Abdulaziz University, 21443 Jeddah, Saudi \nArabia. 3Research and Development Department, Lun Startup Studio, 11543 Riyadh, Saudi Arabia. 4Department \nof Zoology, Kohat University of Science and Technology, Kohat 26000, Pakistan. 5Department of Engineering, \nManchester Metropolitan University, Manchester M15 6BH, UK. 6Department of Electrical and Electronic \nEngineering Science, University of Johannesburg, Johannesburg 2006, South Africa.  *email: atta.rahman@\nriphah.edu.pk\n2\nVol:.(1234567890)Scientific Reports |          (2024) 14:514  | https://doi.org/10.1038/s41598-024-51184-7\nwww.nature.com/scientificreports/\ndiagnosis, as well as promote quick interventions and treatments for better patient outcomes, by utilizing recent \ndevelopments in DL algorithms and big data methodologies. Despite their potential, DL models can be compu-\ntationally expensive and take longer to  train8, which may restrict their usefulness in the detection and diagnosis \nof heart disease. The Vision Transformer (ViT), a recent advancement in DL, has demonstrated encouraging \nresults in resolving these  difficulties9. By utilizing the self-attention strategy to get over image-specific biases and \nconstraints, ViT has shown greater accuracy and computational efficiency when compared to state-of-the-art \nConvolutional Neural Network (CNN)  models10. DL-based algorithms have demonstrated good efficiency in \ncategorizing heart sounds for VHD, but they frequently suffer from insufficient deep spatial feature extraction, \nleading to decreased  accuracy11. Additionally, the high computational costs and lengthy training times associated \nwith DL models can make it more difficult to improve heart sound classification  ability12.\nPatients with heart failure and society as a whole would benefit from accurate, organized diagnostic  services13. \nIn order to do this, this study creates a novel method for performing heart disease prediction by utilizing an \nimproved self-attention-based transformer network. Preprocessing the dataset includes dealing with missing \nvalues, encoding category variables, and normalizing numerical characteristics. Extract significant features from \nthe dataset, such as age, gender, blood pressure, cholesterol levels, etc. The model architecture is fine-tuned by \nutilizing several attention layers, feed-forward neural networks, positional encodings, etc. We noticed improved \ndiagnosis by doing experiments on a benchmark dataset. The study conducted  in14 found that extracting relevant \ninformation is the most important step in improving the precision of heart disease detection. For example, a clini-\ncian makes a decision on a patient with heart disease based on the classification using the specified characteristics. \nPrevious research focused on enhancing and creating classification techniques rather than choosing the optimal \nattributes and their relationship to increase  accuracy15. Using the self-attention mechanism, the proposed model \ncan effectively capture the relationships and dependencies between distinct features in the data. This allows the \ntransformer to focus on critical information while downplaying less significant aspects, improving the model’s \ncapacity to extract important patterns and information.\nThe remainder of the paper is organized as follows: “ Related work ” section provides an overview of the \nrelated work. In “Limitation and motivation” section, we delve into the background and motivation. The detailed \nproblem description is presented in “Proposed framework” section. The experiments conducted in this work are \ndiscussed in “Experiments” section. The results and discussion are explained in “Results and discussion” section. \nFinally, “Conclusions” section concludes this work and gives future directions.\nRelated work\nHeart disease is one of the primary reasons for mortality worldwide. With the use of Artificial Intelligence (AI) \napproaches, it is possible to monitor certain characteristics such as blood pressure, body weight, cholesterol, sugar \nlevel, and heart rate to determine cardiac disease in its initial stages. ML and DL techniques are revolutionizing \nthe current healthcare system however it is challenging to predict cardiac disease accurately and  reliably16. Vari-\nous classification methods have been utilized for heart disease prediction. The ensemble learning algorithm, in \nparticular Random Forest (RF), has shown some good results in predicting heart  disease17. The study conducted \n in18 used support vector machines (SVM) for classification after using feature selection methods such as the \nFisher score and Matthew’s correlation. A DL system called DeepLabeler was created in the study conducted \n in19 to automatically classify ICD-9 codes. Their developed system uses the document-to-vector (D2V) method \nand a CNN to capture and encode both local and global data. The model’s two key characteristics are multi-label \nclassification and feature extraction. The Reverse Time Attention model (RETAIN), which incorporates an atten-\ntion mechanism and is based on a combination of Recurrent Neural Networks (RNNs), was used in the study \nconducted  in20. This allows the model to focus on the most significant attributes or time periods in the input \nsequence. The understanding of RETAIN is improved by giving each characteristic or time step in the sequence \na weighted relevance score. In this way, the clinicians and experts can then understand what factors or time \nsequences are most crucial for the model’s predictions. Current cutting-edge DL models lack excellent feature \nextraction capabilities in complicated and noisy situations, restricting the development of precise and consistent \nobject  differentiation21. The previous research may be broadly divided into two categories: DL approaches and \nclassic shallow  approaches22.\nFor the precise diagnosis of valve heart diseases (VHDs), a robust and high-performing DL model has been \nprovided  in23. The study published  in24 developed a model for forecasting the possibility of CVD in their sample \nutilizing data from a Japanese urban cohort study. The system for the diagnosis of coronary disease and stroke \nwas constructed using multivariable Cox proportional hazard methods. They were able to examine a variety of \nfactors and produce a reliable model for assessing the risk of CVD events by using their suggested technique. A \nunique ML method for heart disease prediction was created in the research reported  in25. They applied RF and \nDecision Tree (DT) approaches using the Cleveland heart disease dataset. Their experimental findings showed an \naccuracy of 88.7% for identifying heart disease. Numerous ML techniques were applied to evaluate massive and \ncomplicated medical data, assisting healthcare professionals in the early diagnosis of heart  disease15. The study \nemployed a number of classification models, including DT, Naive Bayes (NB), K-nearest Neighbour (KNN), and \nRF algorithm, to compute a variety of heart disease-related problems. Their study’s main goal was to estimate \nthe probability of people having a chance of heart attacks in the future.\nWith the help of sequential electronic health record (EHR) data, the study conducted  in13 attempted to \ndiagnose cardiac failure. They made use of real-world datasets that contained data from hospital departments, \nhealth records, and patient diagnostic information pertaining to cardiac diseases. The main aim of their study \nwas to precisely detect and classify individuals at risk of heart failure by the analysis of comprehensive EHR data. \nThe efficiency of merging tree-based ensemble methods with the Synthetic Minority Over-sampling Technique \n(SMOTE) was  conducted26. This method was used to deal with the problem of data imbalance in heart failure \n3\nVol.:(0123456789)Scientific Reports |          (2024) 14:514  | https://doi.org/10.1038/s41598-024-51184-7\nwww.nature.com/scientificreports/\npatient survival prediction. The study aims to maximize the accuracy of forecasting the survival outcomes for \npatients with heart failure by using ensemble methods and applying SMOTE to rebalance the data. The study \nconducted  in27 deployed a hybrid model incorporating clustering and classification in the field of type 2 diabetes \nprediction. K-means clustering was the first phase in this model’s two-step process, which was followed by the \nC4.5 classification technique using a k-fold cross-validation approach. The proposed hybrid approach produced \nencouraging results, with a classification rate of 88.38%. The use of this model has enormous potential for doctors \nsince it can help them make well-informed clinical decisions about the management of diabetes.\nThe most current research demonstrates the various methods used to increase heart disease prediction accu-\nracy. Researchers have made tremendous progress in improving the precision and effectiveness of prediction \nmodels through the use of ensemble  learning28, feature  extraction29, DL  models30, and other techniques. In order \nto overcome the limitations of earlier work, a unique method of heart disease prediction is presented, utilizing a \nself-attention-based transformer model. This cutting-edge model was created expressly to solve the difficulties in \ninvestigating and forecasting cardiac disease. The model successfully captures complex patterns and relationships \nwithin the medical data by utilizing self-attention processes, allowing for more precise predictions.\nLimitation and motivation\nStatistics of heart disease often include temporal characteristics, such as the history of the patient as well as vari-\nations over time. Effectively processing sequential data using ML approaches is challenging. Previous studies \ndidn’t provide sufficient support for better patient outcomes. In this section, we outline the limitations of previous \nheart disease prediction methods, clarify our work motivations for developing an improved model, and highlight \nthe key contributions and novelties of our study.\nPrevious works limitations\nThe primary input sources for heart disease diagnosis are patient health characteristics containing data with \ncategories and unstructured text. The main shortcomings of the current heart disease prediction methods are \nthe modeling of input dataset attributes, computation of attribute risk factors, and obtaining high prediction \n accuracy31. The significant drawback of NB in the context of heart disease prediction is that it treats each feature \nof the dataset individually when calculating probabilities. Therefore, conventional classifiers lead to an incorrect \ndecision support  system32. According to earlier research, traditional medical decision support systems often \nfocused solely on increasing classification accuracy. They failed to consider the varying costs of misclassification \nacross other categories. However, the minority class frequently has a higher priority in the field of healthcare deci-\nsion making. The efficiency of RNN-based models tends to deteriorate rapidly as data sequence length increases. \nThey perform poorly because of their sequential character, which prevents them from correctly capturing long-\nterm relationships within the data  sequences33.\nTraditional RNNs are prone to vanishing and expanding gradient problems. The Standard Long Short-Term \nMemory (LSTM) networks have the drawback of being unable to handle irregular periods of time. However, tim-\ning inconsistency is typical in many healthcare  applications34. By incorporating an attention-based mechanism \nthat makes it possible to effectively capture dependencies, enhance interpretability, and enable computation \nparallelization, the proposed model seeks to reduce the limitations of the previous work.\nMotivation\nDisease prediction systems are best practices for eliminating human errors in disease diagnosis and aiding in \ndisease prevention through early  identification31. Diagnosis of cardiac disease based on patient health record \ncharacteristics is a multidimensional decision-making technique. Prediction of heart disease is crucial for health-\ncare since it may improve patient outcomes significantly when it is detected early and accurately. However, there \nare certain issues with adaptability, interpretability, and training speed in the existing prediction model. This \nwork created a cutting-edge and reliable attention-based model for heart disease prediction in order to overcome \nthe difficulties of the previous work. The proposed model has the potential to quickly and readily adapt to dif -\nferent outcome risk prediction and evaluation challenges, which makes it a useful tool in the field of healthcare \n prediction35.\nFurthermore, the proposed model has a straightforward and parallelizable network structure, which leads to \nnoticeably quicker training times than existing heart disease prediction techniques. This enhancement makes \nthem more efficient by addressing the difficulties associated with model training and implementation in actual \nhealthcare settings.\nKey contributions and novelty\nThis study presents a novel prediction model that makes use of the self-attention process. The model is created \nwith interpretability and parallelizability in mind, enabling effective computing while maintaining a respectable \nlevel of prediction accuracy. A key element of our model is self-attention, which is notably influenced by the \nwork done  in6. Through the establishment of clear linkages between events, the self-attention mechanism enables \nus to identify dependencies within the features. It’s noteworthy that the self-attention mechanism constantly \ncaptures the weight of feature values, even when they are not independent. The final representation vector is \ncreated by adding a position-level attention layer. We employ a padding-mask method in both the self-attention \nand position-level attention processes to account for the variation in sequence lengths. Masking away the pad -\nding elements during the attention computation, this makes guarantees that the model can handle sequences of \nvarious lengths well. The major technical contributions of our study are summarized as follows:\n4\nVol:.(1234567890)Scientific Reports |          (2024) 14:514  | https://doi.org/10.1038/s41598-024-51184-7\nwww.nature.com/scientificreports/\n• Developed an innovative and resilient attention-based model specifically tailored for predicting heart disease. \nIn addition to its exceptional accuracy in prediction, this model also displays its adaptability to a variety \nof other risk prediction and evaluation tasks. Due to its adaptability, it can be used well across a variety of \ndomains, making it an important tool for many different outcome prediction issues. Its versatility makes it \nsuitable for various healthcare scenarios and expands its potential to tackle a broad range of predictive tasks \nbeyond heart disease prediction.\n• Investigate the key factors that lead to the risk of developing heart disease and identify any previously \nunknown risk factors that may be relevant.\n• Design a Transformer model-based strategy that is more precise and successful than current conventional \nML models in forecasting the likelihood of heart disease.\n• The Transformer model’s efficiency in detecting the likelihood of heart disease across multiple demographic \ncategories, such as age, gender, and race/ethnicity, is examined, and the possibility for personalized risk \nassessment is also investigated.\n• The ability of the Transformer model to identify potential cardiac disease was examined in relation to the \nimpacts of various data preprocessing techniques. Several pre-processing methods were applied to the input \ndata, and their effects on the model’s functionality and accuracy were carefully examined.\nProposed framework\nThe goal of this research is to develop a self-attention-based transformer model for assessing CVD risk utiliz -\ning the Cleveland dataset. This dataset contains a variety of medical and non-medical components that can be \nused to identify whether a patient has cardiac disease. The dataset comprises both continuous and categorical \nvariables, among other features. It becomes challenging to identify the most important factors and comprehend \ntheir relevance in heart disease prognosis. Furthermore, it might be challenging to draw meaningful findings \nsince some of the features are challenging to evaluate clinically.\nDataset and preprocessing\nIn this work, we predict cardiac disease using the UC Irvine Cleveland  dataset36. The collection consists of 303 \ncases, each of which depicts a patient who may have heart disease. The dataset is generated from actual patients \nwith suspected cardiac disease, making it applicable to real-world circumstances. The information comprises a \nnumber of characteristics that are often utilized in clinical practice, including age, cholesterol levels, and electro-\ncardiogram (ECG) readings. Each instance has 14 features that represent distinct characteristics of the patient \nand diagnostic measures. In the dataset, each row corresponds to a patient, and the columns represent several \nattributes related to the diagnosis of heart disease. The column consists of [’ Age’ , ’Sex’ , ’Cp’ , ’Trestbps’ , ’Chol’ , \n’Fbs’ , ’Restecg’ , ’thalach’ , ’Exang’ , ’Oldpeak’ , ’Slope’ , ’Ca’ , ’Thal’ , and ’Target’]. Based on the provided attributes, the \ndataset is utilized to create prediction models that estimate the chance of heart disease. The dataset has been \npreprocessed to handle missing values, normalize numerical features, and encode categorical variables. To find \nany missing values, examine each characteristic. Replace the missing data with approximated values, such as \nmean and median. The Z-score, which estimates a data point’s deviation from the mean value, reflects the varia-\ntion of an attribute’s value within a dataset. With the help of this method, we were able to successfully recognize \nand manage extreme values in the data.\nAttention‑based model architecture\nThe patient characteristics and diagnostic measures are represented by a series of input features that are used to \nencode each instance in the dataset. In this work, we have X = [x1 ,x2 ,..., xn] , represent the input sequence of \nfeatures, where n denotes the length of the sequence. The self-attention mechanism recognizes the relationships \nbetween various aspects in the sequence and gives each feature a weight based on how important it is in relation \nto other features. To capture various sorts of interactions and improve model performance, several parallel self-\nattention layers are used. To identify non-linear interactions and provide final predictions, the attention outputs \nare fed into a feed-forward neural network. This attention-based model architecture with self-attention and \nmulti-head attention mechanisms efficiently captures connections and dependencies within the input sequence, \nallowing the model to focus on key aspects for heart disease prediction. Figure 1  represents the visual descrip-\ntion of the proposed model.\nInput embedding and position encoding\nIn the self-attention-based transformer model, input embedding and position encoding are two crucial processes \nthat come before the self-attention mechanism. The input sequence is represented using these stages in a way \nthat is appropriate for the successive self-attention layers. The categorical variables and numerical characteristics \nof each instance are translated to continuous vector representations through input embedding. In this study, \nwe use an embedding layer to convert discrete values for each category variable into continuous vectors. Each \ninstance’s scaled numerical characteristics and category embeddings are combined into a single vector. We have \ne(xi) denotes the embedding of instance (x i) and f (x i) represents the scaled numerical features of (x i) . The con-\ncatenated input embedding for each instance (x i) is computed as: x\n′\ni =\n[\ne(x i), f (x i)\n]\n . The model comprehends the \norder or sequence of the instances by using position encoding, which adds positional information to the input \n(1)Zscore = x − µ\nσ\n5\nVol.:(0123456789)Scientific Reports |          (2024) 14:514  | https://doi.org/10.1038/s41598-024-51184-7\nwww.nature.com/scientificreports/\nsequence. The self-attention-based transformer model efficiently processes the input sequence, collecting both \nfeature representations and positional information by executing input embedding and position encoding stages.\nTransformer encoder\nThe heart disease dataset is represented as a series of embedded characteristics and positional encodings. To iden-\ntify relationships and extract meaningful representations from the input sequence, apply a stack of Transformer \nencoder layers. The encoder layer includes a self-attention mechanism and a feed-forward neural network. The \noutput of the Transformer encoder layer is computed as, E (i) = [e1 (i),e2 (i),... ,en (i)] , where each e(i) represents \nthe output representation for the corresponding position in the sequence.\nSelf‑attention\nThe ability of the Transformer model to find links between features that go beyond sequence adjacency is another \nintriguing feature of this system. The self-attention technique is utilized to extract the relationships between \nvarious points in the sequence inside each Transformer Encoder layer. The similarity between the query and \nkey vectors is used to determine the attention weights (AW) for each point. These AWs illustrate the relative \nimportance of each position. AW can be calculated as follows:\nQu and Ke are the query and key correspond to input embedding (e1 ,e2 ,..., ei) . Following that, utilizing \nthe attention weights matrix AW , we construct a weighted sum of the value vectors as the latest value vectors:\n(2)A W = soft− max\n(QR\nuKe√\ndk\n)\nFigure 1.  Overview of the proposed model.\n6\nVol:.(1234567890)Scientific Reports |          (2024) 14:514  | https://doi.org/10.1038/s41598-024-51184-7\nwww.nature.com/scientificreports/\nwhere Va represents the input embeddings. Additionally, we address the issue of the sequences’ variable lengths \nby employing the same padding mask technique as the  Transformer6. By extending the provided information \nin the y direction back to the input x , the straightforward nature of the model allows us to easily determine the \nimpact of each feature. Through the use of an embedding layer, we are able to grasp the essence of each feature \nin the given input sequence x.\nThe learning parameters V vi and the learned visited embedding E vi are involved in the process. we introduce \nan additional embedding layer that specifically encodes the order information. This layer serves the purpose of \npreserving and incorporating sequential information into the model.\nFeed‑forward network\nTo further enhance the representations, follow the self-attention strategy by applying a feed-forward neural \nnetwork to each point separately. A non-linear activation function separates the two linear layers that make \nup the feed-forward network. Connect the input characteristics to the output of the self-attention mechanism \nand the output of the feed-forward network to create residual connections. The features after each sublayer are \nnormalized using the layer normalization method.\nOutput layer\nTo detect the existence of heart disease, use the final output from the Transformer Decoder layers and feed it \nthrough a fully connected layer. To determine the final output probabilities, use the softmax function.\nTraining the model\nThe Adam optimizer is used as an optimization technique to train the model for determining the likelihood \nof heart disease. To find whether a subject has heart disease or not, the binary cross-entropy loss function is \nemployed to distinguish between the predicted probabilities and the actual data labels. The training method \nseeks to identify the ideal values for the weight vector W and the bias term b that minimize the loss function. \nThe prediction accuracy of the model is enhanced by the Adam optimizer, which iteratively modifies the weights \nand biases during training.\nDisease prediction\nUsing the test dataset, evaluate the trained model using relevant evaluation measures including accuracy, preci-\nsion, recall, and F1-score. Using the trained model, forecast the likelihood that a new patient will be diagnosed \nwith heart disease based on the feature values of the patient. Analyze the feature importance or coefficients \nlearned by the model to identify the relative importance of different factors in determining heart disease. Figure 2 \nrepresents the proposed model for heart disease prediction.\nExperiments\nHeart disease is a prominent cause of death globally, and effective prediction of heart disease can consider -\nably improve patient  outcomes15. In this work, we suggest using a Self-Attention-based Transformer Model to \nimprove heart disease prediction. We make use of the Cleveland  dataset36, a frequently used benchmark dataset \nin the field of cardiovascular research, to assess the effectiveness of our proposed approach. Load the Cleveland \ndataset into a Data Frame by using the Panda’s package. The dataset has 303 samples, each of which has 76 attrib-\nutes. These characteristics include data on the patient’s demographics, health metrics, and diagnostic results. \nThe panda’s function fillna is used to handle missing values, while StandardScaler from scikit-learn is used to \nnormalize the dataset. The preprocessed dataset is partitioned into 80% training, 10% testing, and 10% valida-\ntion sets. Using the proposed framework, we represent each sample in the dataset as a series of feature vectors, \nwith each feature vector representing a different characteristic. To preserve the sequential information, we use \n(3)Attention(AW, V a) = AW · V\n(4)Evi = V vi · x\n(5)y = softmax(V a + e)\nFigure 2.  Architecture of proposed model used for heart disease prediction.\n7\nVol.:(0123456789)Scientific Reports |          (2024) 14:514  | https://doi.org/10.1038/s41598-024-51184-7\nwww.nature.com/scientificreports/\npositional encoding. Utilize the self-attention technique to enable the model to focus on various input sequence \ncomponents while producing context-aware representations for each attribute. Create the model architecture \nby specifying the essential layers, such as self-attention, feed-forward, and classification layers. Use the PyTorch \n2.0. framework to implement the Self-Attention-based Transformer model. By using the proposed approach, \nwe want to increase heart disease prediction accuracy and contribute to the creation of more effective clinical \ndecision support systems.\nTable 1 displays the parameters used to train and evaluate the self-attention-based Transformer model for \nheart disease prediction utilizing the Cleveland dataset. The model consists of an embedding layer, Transformer \nencoder layers, and a fully connected layer for classification. Iterate through the training dataset in mini-batches, \ncompute the loss, backpropagate, and use the optimizer to update the model weights. After training the model, \nassess its performance on the testing set. Analyze the model’s predictions and interpret the learned patterns. \nDetermine the self-attention mechanism’s key characteristics and attention weights. In this work, we carry out \nboth binary and multi-class classification tasks in the experiments. For the binary classification problem, we \npredict the presence or absence of heart disease. For the four-class classification problem, we divide the labels \ninto four unique classes to reflect various risk levels of heart disease. Table 2 displays the number of classes used \nin the experiments. In order to validate the self-attention-based Transformer model performance, we compared \nit with various baseline approaches.\nBaseline approaches\nWe carried out a comparison study using a number of baseline methods frequently employed for heart disease \nprediction. We investigated CNN, RNN, RNN + (RNN with additional features), RETAIN (Reverse Time Atten-\ntion Model), and Dipole as the baseline methods. We used an identical experimental design for each baseline \nstrategy, including data preparation, model training, validation, hyperparameter adjustment, and assessment \nof the testing set.\nCNN\nThree convolutional layers make up the CNN model presented by Albelwi et al.37, which follows a standard neural \nnetwork architecture. The kernel sizes for each convolutional layer range from 3 to 5, and each layer has 256 \nchannels. In order to identify clinical data that was considerably class-imbalanced and forecast the development \nTable 1.  Parameters of the proposed model used in the experiments.\nParameter Description\nModel Self-attention-based transformer model\nInput dimension = 14 Input features dimension\nOutput dimension = 2, 4 Number of output classes\nd-model = 128 Dimensionality of the model’s hidden states\nnhead = 4 Attention heads in the multi-head self-attention\nNum-layers = 4 Layers in the encoder\nDropout = 0.2 Dropout probability\nBatch-size = 32, 64 Number of samples\nEpochs = 90 number of iterations\nLearning-rate = 0.001 Learning rate\nOptimizer = Adam optimizer used for updating the parameters\nTrain-loss Avg raining loss over the training dataset\nCross entropy Loss function\nTest-loss Avg loss over the testing dataset\nTable 2.  Binary and multiclass classification.\nClass value Description\nBinary Two class classification problem\nClass 0 no heart disease\nClass 1 presence of heart disease\nMulti-class four-class classification problem\nClass 0 no heart disease\nClass 1 low risk of heart disease\nClass 2 moderate risk of heart disease\nClass 3 high risk of heart disease\n8\nVol:.(1234567890)Scientific Reports |          (2024) 14:514  | https://doi.org/10.1038/s41598-024-51184-7\nwww.nature.com/scientificreports/\nof coronary heart disease (CHD), a study done  in38 developed an effective neural network using convolutional \nlayers. For the purpose of predicting CVD, we classified the Cleveland dataset using this model. The size of the \nconvolutional kernels is kept to (kernel-size = 3), and (pool-size = 2). The dropout rate for regularization is kept \nto (dropout-rate = 0.2).\nRNN\nOne of the first  models39 in the field of recurrent processing was introduced as LSTM, a sort of RNN with gated \nunits. The input gate, forget gate and output gate make up the LSTM unit, a sequential architecture, commonly \nused in temporal data processing. These gates are essential for managing the information flow inside the LSTM \nunit. The LSTM unit uses a self-loop mechanism on its internal state as opposed to the recursive calculation \nmethod of conventional RNNs, which improves its capacity to retain and update information over  time40. The \ninput gate assesses the applicability of the current input and modifies the internal state in accordance with the \nsystem state at the previous time step. To begin with, we calculate the input embeddings, which are then passed \ninto an LSTM layer. The hidden states generated by the LSTM are directly used by a linear classifier to predict \nthe outcomes.\nRNN + \nTo improve performance or handle certain issues, RNN + refers to the expansion or combining of RNNs with \nother components. By integrating the hidden states, the RNN + extension of the RNN model incorporates a \nlocation-based attention mechanism into the output  layer41. Encode the target categorization labels in the Cleve-\nland dataset into a numerical representation so that the model can interpret it. We employ one-hot encoding \nfor multi-class categorization.\nRETAIN42\nRETAIN is a state-of-the-art predictive model that leverages a two-level attention mechanism, enhancing both \nits functionality and interpretability. RNNs-like prediction accuracy is maintained by the unique neural attention \nmodel known as RETAIN, which is customized to enable thorough interpretation of prediction findings. The key \ncharacteristic of RETAIN is its attention mechanism, which emulates the clinical decision-making approach of \ndoctors. The fundamental idea underlying RETAIN is to use context-level attention and time-level attention to \ndescribe the link between input sequences and the target variable. This attention mechanism allows RETAIN to \ndraw attention to and weigh the important input sequence components, enabling a more in-depth comprehen-\nsion of the model’s predictions. RETAIN exhibits performance that is comparable to RNNs and does not sacrifice \nprediction accuracy despite its interpretability.\nDipole43\nDipole employs a bi-directional RNN with three attention methods. In this case, we choose a variation of Dipole \nthat has demonstrated superior performance. The embedding layer of the Dipole model is implemented as a \nmulti-layer perceptron (MLP) with ReLu activation. They observed that, the local-based attention mechanism \nperforms the best out of the three methods. Based on this discovery, we modify our model’s local-based atten-\ntion mechanism to produce the final context vector that is used for prediction. The output of the bi-directional \nRNN with an attention layer is followed by a classification layer. This layer assigns the learned representations to \nthe required classification labels and forecasts the probability for each class. This comparative analysis was con-\nducted to assess the self-attention-based Transformer model’s performance against these standard methods. By \ncomparing the Transformer model’s performance measures to those of the baselines, we were able to gain insight \ninto the model’s strengths, shortcomings, and potential as an improved technique for heart disease prediction.\nEnvironment setting\nThe experiments of the proposed work are implemented using PyTorch 2.0. All training is carried out on a \ncomputer with an Intel Core i97900X processor, 128GB of RAM, 2 Nvidia Titan V graphics cards, and CUDA \n9.0. For training our hypothetical model, we use Adam optimizer, with dm , set to 128. We used the learning rate \nas, lr = 0.001 and the loss function as CrossEntropyLoss () to fine-tune the model. The time complexity of the \nproposed model is calculated as; O\n(\nn2 ∗ d\n)\n, where n represents the sequence length, and d reflects the dimen -\nsions of the hidden state.\nEvaluation metric\nIn this study, the classification tasks are measured using the accuracy metric. It calculates the percentage of prop-\nerly identified examples in a dataset relative to all occurrences. In this particular case, the number of events for \na particular user is expressed by the number of folds (k = 130). The remaining instances (k-1) serve as a training \nset for each iteration of the learning process, and the instance that is chosen serves as a test set. Then, the mean \naccuracy over all k trials is determined, such as;\nwhereas FFP and FFN represent false positives and false negatives, respectively, TTP and TTN represent true \npositives and true negatives.\n(6)Accuracy = TTP + TTN\nTTP + TTN + FFP+ FFN\n9\nVol.:(0123456789)Scientific Reports |          (2024) 14:514  | https://doi.org/10.1038/s41598-024-51184-7\nwww.nature.com/scientificreports/\nResults and discussion\nCompare the performance of the suggested Self-Attention-based Transformer Model to the baseline techniques. \nDetermine which model has the best prediction accuracy and generalization capabilities by calculating the accu-\nracy of each model. The experimental results achieved in the heart disease prediction task are shown in Table 3. \nThe outcomes demonstrate how much better our suggested strategy is than all benchmark models, including RNN \nand RETAIN. Our solution surpassed these baseline models in terms of performance and predicted accuracy, \nwhich are commonly regarded as state-of-the-art approaches for heart disease prediction. Furthermore, we saw a \nwider performance disparity between our approach and the RNN-based model in our dataset. The table provides \na comprehensive comparison of computing efficiency and accuracy among the different models considered as \nbaselines. In order to validate the model performance on diverse dataset, we used the cardiovascular disease \ndataset, which is freely available on Kaggle. This dataset consists of 70,000 instances having 11 independent fea-\ntures. The computing time column specifically indicates the duration required to train each model once on the \nentire training dataset per epoch. As evident from the table, the proposed model exhibits faster training times \nin comparison to baseline models. The proposed model also achieves the highest accuracy of 95.2% using the \ncardiovascular disease dataset, shown in Table 4.\nThis advantage can be attributed to the straightforward and parallelizable structure of our suggested model. \nRNN models, on the other hand, encounter difficulties because of their sequential nature, leading to longer train-\ning durations, especially when working with datasets containing prolonged sequences. The suggested model’s \ninterpretability when compared to RNN is also a key advantage. While RNN models are difficult to interpret, our \nproposed model provides more clarity and is simpler to understand. In healthcare applications, this interpret -\nability can be quite helpful because it gives medical practitioners insights into the underlying causes of heart \nfailure (HF) prediction and speeds up the decision-making process. Figure 3 represents the training and testing \nTable 3.  Computation time and accuracy using Cleveland dataset.\nModel Computation time (s) Accuracy (%)\nCNN 4.53 0.747\nRNN 1.43 0.783\nRNN + 3.52 0.871\nRETAIN 4.45 0.850\nDipole 2.12 0.894\nProposed 1.90 0.965\nTable 4.  Computation time and accuracy using cardiovascular disease dataset.\nModel Computation time (s) Accuracy (%)\nCNN 9.74 0.713\nRNN 3.96 0.779\nRNN + 6.38 0.863\nRETAIN 7.12 0.832\nDipole 5.61 0.876\nProposed 3.57 0.952\nFigure 3.  Training and testing accuracy of the model.\n10\nVol:.(1234567890)Scientific Reports |          (2024) 14:514  | https://doi.org/10.1038/s41598-024-51184-7\nwww.nature.com/scientificreports/\naccuracy of the proposed model. We achieved 97.17% training accuracy and 96.51% testing accuracy by iterating \nthe model for 90 epochs. Similarly, we get the minimum training loss of 0.10 and testing loss of 0.12, as shown \nin Fig. 4. In recent studies on CVD prediction using ML techniques, various classifiers have been  employed44. \nTable 5 provides a summary of recent studies conducted for heart disease prediction, along with their achieved \naccuracy. The study conducted  in45 deployed various classification techniques such as SVM, NB, and DT, for a \nCVD risk prediction. They achieved an accuracy of 90% for CVD risk prediction. Similar to this, the study con-\nducted  in46, described a prospective study with 423,604 subjects from the UK Biobank. For forecasting the risk \nof CVD, they unveiled an ML technique dubbed Auto-Prognosis. The work done  in47 offered a novel approach \nfor creating a predictive framework in the form of fuzzy methods to evaluate CVD risk using a neuro-fuzzy \ndecision support mechanism. Their proposed approach intends to offer helpful assistance in determining the \nrisk caused by cardiovascular diseases. Additionally, the research conducted  in48 proposed the Gradient Boosting \n(GB) algorithm, which achieved an accuracy of 89.7%. Gradient Boosting uses a group of weak learners, which \nbecomes computationally expensive when working with big datasets or complicated models. It is also sensitive \nto noise or outlier data. The maximum accuracy of 96.51% was achieved using the proposed model after data \npreprocessing, adjusting the input and output layers, incorporating more layers, and modifying the attention \nprocesses to collect relevant information.\nThe study conducted  in15 deployed various ML algorithms for the task of heart disease prediction using the \nCleveland database reflected in Table  6. They achieved a maximum accuracy of 90.78% using the K-NN algo-\nrithm. They realized that to enhance the precision of heart disease diagnosis, it is necessary to investigate cutting-\nedge methodologies and model fusions. Detecting CVD diseases such as heart attacks and coronary artery \ndiseases are pivotal research problem. In a study conducted  by25, the researchers utilized the Cleveland heart \ndisease dataset to perform heart disease prediction. They deployed DT, RF , and a hybrid approach combining \nboth algorithms. Through their heart disease prediction model, they achieved a higher accuracy of 88.7% using \nthe hybrid approach. Heart disease may be quickly and inexpensively detected with the use of ML techniques. \nFigure 4.  Training and testing loss of the model.\nTable 5.  Comparison with various related studies.\nAuthors Ye a r Approach Accuracy (%)\nStudy47 2019 HRFLM 88.7\nStudy46 2021 NF model 91\nStudy48 2022 GBA 89.7\nStudy45 2023 NB SVM DT 90\nStudy49 2023 XGBH 80.6\nProposed 2023 Transformer model 96.51\nTable 6.  Performance comparison on same dataset.\nAuthors Ye a r Algorithm Accuracy (%)\nStudy15 2020 NB, DT, RF , K-NN 90.78\nStudy25 2021 DT, RF , Hybrid 88.7\nStudy50 2022 hyOPTXg using XGBoost 94.7\nStudy14 2022 GAPSO-RF 95.6\nProposed 2023 Transformer model 96.51\n11\nVol.:(0123456789)Scientific Reports |          (2024) 14:514  | https://doi.org/10.1038/s41598-024-51184-7\nwww.nature.com/scientificreports/\nThe research reported  in50 suggests the use of an expert model called hyOPTXg to predict heart disease using \nan improved XGBoost classifier. On the Cleveland dataset, they achieved an accuracy of 94.7%. Heart disease \nprediction has gotten a lot of interest in the medical world. In the  study14 a hybrid genetic algorithm (GA) and \nparticle swarm optimization (PSO) optimized technique based on RF , named GAPSO-RF , is created and applied \nto identify the ideal features that can improve heart-disease prediction accuracy. On the Cleveland dataset, they \nobtained 95.6% accuracy in heart disease prediction. Their approach achieves good accuracy however, combin-\ning several techniques may increase the difficulty of parameter adjustment and convergence of optimization.\nThe proposed strategy surpasses state-of-the-art approaches, with a remarkable accuracy of 96.51%. Th self-\nattention mechanism enables the model to effectively capture long-range relationships. The transformer model \nis able to address any point in the input sequence, unlike conventional sequential models like RNNs. It produces \ncontext-aware representations for each input token. Due to the attention mechanism, transformers provide effi-\ncient parallelization during training and inference. The model becomes more effective and scalable as a result \nof its parallelization capacity, especially when working with huge datasets. By allocating attention weights to \nvarious input places, the self-attention mechanism enables interpretability. This makes it possible to visualize \nthe significance and relevance of particular characteristics in the prediction process.\nThe limitation of the proposed model is; It becomes difficult to understand the transformers architecture, \nparticularly when it becomes deeper and complicated. Especially, to grasp how the model generates particular \npredictions or what sequence elements are essentials. To address this issue, we used attention visualization \napproaches, which gave us helpful insight about the framework’s decision-making process.\nConclusion\nIn this work, we developed a novel attention-based transformer model for the task of heart disease prediction. \nThis model applied the strength of position-level attention mechanisms and self-attention layers to learn the \nrepresentation of the complete sequence, in contrast to conventional RNN methods. Through the use of this dis-\ntinct mechanism, we were able to identify and evaluate the relative weights of the various sequence components, \nimproving the effectiveness of prediction. Beyond heart disease, a variety of clinical risk prediction tasks can be \nperformed using the proposed technique due to its versatility. The fundamental advantage of this architecture is \nits well-designed network topology, which enables maximum parallelization. In contrast to RNN-based models, \nwhich suffer from sequential processing and limited parallelization, the proposed paradigm permits efficient \nand simultaneous computing across the whole sequence. The proposed model performs well in real-world cir -\ncumstances using benchmark dataset and reduces training and inference times. To validate the performance, \nwe conducted various experiments and compared their results with various related study to demonstrate that \nthe proposed model is more accurate than cutting-edge methods. The proposed method is adaptable, which \nhighlights its potential for usage in a range of healthcare contexts beyond heart disease prediction, providing \ninformative data and assisting in decision-making.\nIn future, we want to Integrate transfer learning with the proposed model to enhance its performance, espe-\ncially in the scenarios of dealing with limited labeled data.\nData availability\nThe datasets and code will be available from the corresponding author on request.\nReceived: 3 October 2023; Accepted: 1 January 2024\nReferences\n 1. Virani, S. S. et al. Heart disease and stroke statistics—2021 update: A report from the american heart association. Circulation  \n143(8), e254–e743 (2021).\n 2. Groenewegen, A., Rutten, F . H., Mosterd, A. & Hoes, A. W . Epidemiology of heart failure. Eur. J. Heart Fail. 22(8), 1342–1356 \n(2020).\n 3. Ghosh, S. K., Ponnalagu, R., Tripathy, R. & Acharya, U. R. Automated detection of heart valve diseases using chirplet transform \nand multiclass composite classifier with pcg signals. Comput. Biol. Med. 118, 103632 (2020).\n 4. Ahsan, M. M. & Siddique, Z. Machine learning-based heart disease diagnosis: A systematic literature review. Artif. Intell. Med. \n128, 102289 (2022).\n 5. Torre-Cruz, J. et al. Unsupervised detection and classification of heartbeats using the dissimilarity matrix in pcg signals. Comput. \nMethods Programs Biomed. 221, 106909 (2022).\n 6. Khan, W . et al. Sql and nosql database software architecture performance analysis and assessments-a systematic literature review. \nBig Data Cogn. Comput. 7(2), 97 (2023).\n 7. Ahmed, S. F . et al. Deep learning modelling techniques: Current progress, applications, advantages, and challenges. Artif. Intell. \nRev. 1, 1–97 (2023).\n 8. Perumal, V ., Abueidda, D., Koric, S. & Kontsos, A. Temporal convolutional networks for data-driven thermal modeling of directed \nenergy deposition. J. Manuf. Process. 85, 405–416 (2023).\n 9. Yu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer, L. & Lewis, M. Megabyte: Predicting million-byte sequences with \nmultiscale transformers. arXiv preprint arXiv: 2305. 07185 (2023).\n 10. Reedha, R., Dericquebourg, E., Canals, R. & Hafiane, A. Transformer neural network for weed and crop classification of high \nresolution uav images. Remote Sens. 14(3), 592 (2022).\n 11. Oh, S. L. et al. Classification of heart sound signals using a novel deep wavenet model. Comput. Methods Programs Biomed. 196, \n105604 (2020).\n 12. Deng, M. et al. Heart sound classification based on improved mfcc features and convolutional recurrent neural networks. Neural \nNetw. 130, 22–32 (2020).\n 13. Jin, B. et al. Predicting the risk of heart failure with ehr sequential data modeling. IEEE Access 6, 9256–9261 (2018).\n 14. El-Shafiey, M. G., Hagag, A., El-Dahshan, E.-S.A. & Ismail, M. A. A hybrid ga and pso optimized approach for heart-disease \nprediction based on random forest. Multimed. Tools Appl. 81(13), 18155–18179 (2022).\n12\nVol:.(1234567890)Scientific Reports |          (2024) 14:514  | https://doi.org/10.1038/s41598-024-51184-7\nwww.nature.com/scientificreports/\n 15. Shah, D., Patel, S. & Bharti, S. K. Heart disease prediction using machine learning techniques. SN Comput. Sci. 1, 1–6 (2020).\n 16. Nouman, A. & Muneer, S. A systematic literature review on heart disease prediction using blockchain and machine learning \ntechniques. Int. J. Comput. Innov. Sci. 1(4), 1–6 (2022).\n 17. Khan, A. et al. A novel study on machine learning algorithm-based cardiovascular disease prediction. Health Social Care Commun. \n23, 1–10 (2023).\n 18. Saqlain, S. M. et al. Fisher score and matthews correlation coefficient-based feature subset selection for heart disease diagnosis \nusing support vector machines. Knowl. Inf. Syst. 58, 139–167 (2019).\n 19. Li, M. et al. Automated icd-9 coding via a deep learning approach. IEEE/ACM Trans. Comput. Biol. Bioinf. 16(4), 1193–1202 (2018).\n 20. Choi, E., Schuetz, A., Stewart, W . F . & Sun, J. Using recurrent neural network models for early detection of heart failure onset. J. \nAm. Med. Inform. Assoc. 24(2), 361–370 (2017).\n 21. Roy, A. M. & Bhaduri, J. Densesph-yolov5: An automated damage detection model based on densenet and swin-transformer \nprediction head-enabled yolov5 with attention mechanism. Adv. Eng. Inform. 56, 102007 (2023).\n 22. Jiang, B., Chen, S., Wang, B. & Luo, B. Mglnn: Semi-supervised learning via multiple graph cooperative learning neural networks. \nNeural Netw. 153, 204–214 (2022).\n 23. Jamil, S. & Roy, A. M. An efficient and robust phonocardiography (pcg)-based valvular heart diseases (vhd) detection framework \nusing vision transformer (vit). Comput. Biol. Med. 158, 106734 (2023).\n 24. Nakai, M. et al. Development of a cardiovascular disease risk prediction model using the suita study, a population-based prospec-\ntive cohort study in japan. J. Atheroscler. Thromb. 27(11), 1160–1175 (2020).\n 25. Kavitha, M., Gnaneswar, G., Dinesh, R., Sai, Y . R. & Suraj, R. S. Heart disease prediction using hybrid machine learning model. In \n6th International Conference on Inventive Computation Technologies (ICICT), 1329–1333 (IEEE, 2021).\n 26. Ishaq, A. et al. Improving the prediction of heart failure patients’ survival using smote and effective data mining techniques. IEEE \nAccess 9, 39707–39716 (2021).\n 27. Deepika. P . & Sasikala, S. Enhanced model for prediction and classification of cardiovascular disease using decision tree with \nparticle swarm optimization. In 4th International Conference on Electronics, Communication and Aerospace Technology (ICECA), \n1068–1072 (IEEE, 2020).\n 28. Latha, C. B. C. & Jeeva, S. C. Improving the accuracy of prediction of heart disease risk based on ensemble classification techniques. \nInform. Med. Unlocked 16, 100203 (2019).\n 29. Y ahya, W . B., Rosenberg, R. & Ulm, K. Microarray-based classification of histopathologic responses of locally advanced rectal \ncarcinomas to neoadjuvant radio chemotherapy treatment. Turkiye Klinikleri J. Biostat., 6 (1) (2014).\n 30. Gandhi, M. & Singh, S. N. Predictions in heart disease using techniques of data mining. In International Conference on Futuristic \nTrends on Computational Analysis and Knowledge Management (ABLAZE), 520–525 (IEEE, 2015).\n 31. Koyi, L. P ., Borra, T. & Prasad, G. L. V . A research survey on state-of-the-art heart disease prediction systems. In International \nConference on Artificial Intelligence and Smart Systems (ICAIS), 799–806 (IEEE, 2021).\n 32. Zhenya, Q. & Zhang, Z. A hybrid cost-sensitive ensemble for heart disease prediction. BMC Med. Inform. Decis. Mak. 21, 1–18 \n(2021).\n 33. Choi, E., Bahadori, M. T., Searles, E., Coffey, C., Thompson, M., Bost, J., Tejedor-Sojo, J. & Sun, J. Multi-layer representation learn-\ning for medical concepts. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data \nMining, 1495–1504 (2016).\n 34. Baytas, I. M., Xiao, C., Zhang, X., Wang, F ., Jain, A. K. & Zhou, J. Patient subtyping via time-aware lstm networks. In Proceedings \nof the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 65–74 (2017).\n 35. Manju, R., Harinee, P ., Gangolli, S. S. & Bhuvana, N. Evolution of computational intelligence in modern medicine for health care \ninformatics. In Translating Healthcare Through Intelligent Computational Methods, 395–411 (Springer, 2023).\n 36. Janosi, A., Steinbrunn, W ., Pfisterer, M. & Detrano, R. Heart disease. UCI Machine Learning Repository, (1988).\n 37. Albelwi, S. & Mahmood, A. A framework for designing the architectures of deep convolutional neural networks. Entropy  19(6), \n242 (2017).\n 38. Dutta, A., Batabyal, T., Basu, M. & Acton, S. T. An efficient convolutional neural network for coronary heart disease prediction. \nExpert Syst. Appl. 159, 113408 (2020).\n 39. Mikolov, T., Karafiát, M., Burget, L., Cernocky, J. & Khudanpur, S. Recurrent neural network based language model. In Interspeech, \nvol. 2, 1045–1048 (Makuhari, 2010).\n 40. Goodfellow, I., Bengio, Y . & Courville, A. Deep Learning. (MIT press, 2016).\n 41. Sahu, K., Minz, S. Implementation of optimal leaf feature selection-based plant leaf disease classification framework with rnn+ gru \ntechnique. In Advanced Communication and Intelligent Systems: First International Conference, ICACIS, Virtual Event, 576–592 \n(Springer, 2023).\n 42. Choi, E. et al. Retain: An interpretable predictive model for healthcare using reverse time attention mechanism. Adv. Neural Inf. \nProcess. Syst. 29, 1–9 (2016).\n 43. Ma, F ., Chitta, R., Zhou, J., Y ou, Q., Sun, T. & Gao, J. Dipole: Diagnosis prediction in healthcare via attention-based bidirectional \nrecurrent neural networks. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data \nMining, pp. 1903–1911 (2017).\n 44. Nick, T. G. & Campbell, K. M. Logistic regression. Top. Biostat. 1, 273–301 (2007).\n 45. Arumugam, K. et al. Multiple disease prediction using machine learning algorithms. Mater. Today Proc. 80, 3682–3685 (2023).\n 46. Casalino, G., Castellano, G., Kaymak, U. & Zaza, G. Balancing accuracy and interpretability through neuro-fuzzy models for car-\ndiovascular risk assessment. In  Proceedings of the 2021 IEEE Symposium Series on Computational Intelligence (SSCI), 1–8 (IEEE, \nOrlando, FL, USA, 2021).\n 47. Mohan, S., Thirumalai, C. & Srivastava, G. Effective heart disease prediction using hybrid machine learning techniques. IEEE \nAccess 7, 81542–81554 (2019).\n 48. Theerthagiri, P . & Vidya, J. Cardiovascular disease prediction using recursive feature elimination and gradient boosting classifica-\ntion techniques. Expert Syst. 39, e13064 (2022).\n 49. Peng, M. et al. A cardiovascular disease risk score model based on high contribution characteristics. Appl. Sci. 13(2), 893 (2023).\n 50. Srinivas, P . & Katarya, R. hyoptxg: Optuna hyper-parameter optimization framework for predicting cardiovascular disease using \nxgboost. Biomed. Signal Process. Control 73, 103456 (2022).\nAcknowledgements\nThis work was partially supported by Lun Startup Studio, Riyadh 11543, Saudi Arabia.\nAuthor contributions\nEach author of this paper brings unique expertise and perspectives to the paper: A.U.R., provides the proposed \nwork and previous research limitations relevant to the paper. Y .A., discusses the introduction section of the work. \nA.Z., thoroughly investigate the proposed work and provides suggestion for improvement and technical set up. \nK.U., helps in reviewing of the paper, also he works on paper polishing, including checking for grammar etc. K.R., \n13\nVol.:(0123456789)Scientific Reports |          (2024) 14:514  | https://doi.org/10.1038/s41598-024-51184-7\nwww.nature.com/scientificreports/\nchecks the complete setup of the paper, review it and give suggestion for enhancement, which are incorporated. \nT.S., review the whole paper and give suggestion for enhancement, which are incorporated.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to A.U.R.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024"
}