{
  "title": "TeTrIS: Template Transformer Networks for Image Segmentation With Shape Priors",
  "url": "https://openalex.org/W2922901657",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5011337060",
      "name": "Matthew Chung Hai Lee",
      "affiliations": [
        "HeartFlow (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2171623179",
      "name": "Kersten Petersen",
      "affiliations": [
        "HeartFlow (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2659294613",
      "name": "Nick Pawlowski",
      "affiliations": [
        "Imperial College London",
        "Institute of Group Analysis"
      ]
    },
    {
      "id": "https://openalex.org/A1865420375",
      "name": "Ben Glocker",
      "affiliations": [
        "HeartFlow (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2019309302",
      "name": "Michiel Schaap",
      "affiliations": [
        "HeartFlow (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5011337060",
      "name": "Matthew Chung Hai Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2171623179",
      "name": "Kersten Petersen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2659294613",
      "name": "Nick Pawlowski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1865420375",
      "name": "Ben Glocker",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2019309302",
      "name": "Michiel Schaap",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2124351162",
    "https://openalex.org/W2787740020",
    "https://openalex.org/W2608822622",
    "https://openalex.org/W2752460469",
    "https://openalex.org/W2396622801",
    "https://openalex.org/W2805285736",
    "https://openalex.org/W2777357193",
    "https://openalex.org/W2301358467",
    "https://openalex.org/W1888801276",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W2124592697",
    "https://openalex.org/W2753582981",
    "https://openalex.org/W2620296437",
    "https://openalex.org/W2521429215",
    "https://openalex.org/W1667869507",
    "https://openalex.org/W1977034661",
    "https://openalex.org/W2093723730",
    "https://openalex.org/W2166040322",
    "https://openalex.org/W2604920239",
    "https://openalex.org/W2752785527",
    "https://openalex.org/W1998020770",
    "https://openalex.org/W2104276184",
    "https://openalex.org/W2170167891",
    "https://openalex.org/W2136106022",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W2125848778",
    "https://openalex.org/W2038952578",
    "https://openalex.org/W2152826865",
    "https://openalex.org/W4213007602",
    "https://openalex.org/W2165023525",
    "https://openalex.org/W764651262",
    "https://openalex.org/W1483681246",
    "https://openalex.org/W2750752294",
    "https://openalex.org/W3146761106",
    "https://openalex.org/W2752246523",
    "https://openalex.org/W2082304218",
    "https://openalex.org/W2751297520",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3098269293",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2171750292",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3169507310",
    "https://openalex.org/W2883037350",
    "https://openalex.org/W2770051178",
    "https://openalex.org/W2769279756",
    "https://openalex.org/W2554423077",
    "https://openalex.org/W2432481613",
    "https://openalex.org/W4300945631",
    "https://openalex.org/W2470417264",
    "https://openalex.org/W2092693116"
  ],
  "abstract": "In this paper, we introduce and compare different approaches for incorporating shape prior information into neural network-based image segmentation. Specifically, we introduce the concept of template transformer networks, where a shape template is deformed to match the underlying structure of interest through an end-to-end trained spatial transformer network. This has the advantage of explicitly enforcing shape priors, and this is free of discretization artifacts by providing a soft partial volume segmentation. We also introduce a simple yet effective way of incorporating priors in the state-of-the-art pixel-wise binary classification methods such as fully convolutional networks and U-net. Here, the template shape is given as an additional input channel, incorporating this information significantly reduces false positives. We report results on synthetic data and sub-voxel segmentation of coronary lumen structures in cardiac computed tomography showing the benefit of incorporating priors in neural network-based image segmentation.",
  "full_text": "2596 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 38, NO. 11, NOVEMBER 2019\nTETRIS: T emplate T ransformer Networks for\nImage Segmentation With Shape Priors\nMatthew Chung Hai Lee , Kersten Petersen, Nick Pawlowski, Ben Glocker , and Michiel Schaap\nAbstract — In this paper, we introduce and compare differ-\nent approaches for incorporating shape prior information\ninto neural network-based image segmentation. Speciﬁ-\ncally, we introduce the concept of template transformer\nnetworks, where a shape template is deformed to match\nthe underlying structure of interest through an end-to-end\ntrained spatial transformer network. This has the advantage\nof explicitly enforcing shape priors, and this is free of\ndiscretization artifacts by providing a soft partial volume\nsegmentation. We also introduce a simple yet effective\nway of incorporating priors in thestate-of-the-art pixel-wise\nbinary classiﬁcation methods such as fully convolutional\nnetworks and U-net. Here, the template shape is given as\nan additional input channel, incorporating this information\nsigniﬁcantly reduces false positives. We report results on\nsynthetic data and sub-voxel segmentation of coronary\nlumen structures in cardiac computed tomography showing\nthe beneﬁt of incorporating priors in neural network-based\nimage segmentation.\nIndex Terms— Image segmentation, shape priors, neural\nnetworks, template deformation, image registration.\nI. I NTRODUCTION\nS\nEGMENTATION of anatomical structures can be greatly\nimproved by incorporating priors on shape, assuming\npopulation wide regularities are observed, or that expert\nknowledge is available. Shape priors help to reduce the search\nspace of potential solutions for machine learning algorithms,\nimproving the accuracy and plausibility of solutions [1].\nPriors are particularly useful when data is ambiguous, corrupt,\nexhibits low signal-to-noise or if training data is scarce.\nSome of the ﬁrst attempts to explicitly enforce shape\npriors in segmentation pipelines made use of deformable\ntemplates [2], combining image registration with a shape\ntemplate to perform segmentation. Subsequently this method\nManuscript received January 16, 2019; revised March 12, 2019;\naccepted March 12, 2019. Date of publication March 22, 2019;\ndate of current version October 25, 2019. (Ben Glocker and Michiel\nSchaap have shared senior authorship.) (Corresponding author:\nMatthew Chung Hai Lee.)\nM. C. H. Lee, B. Glocker, and M. Schaap are with HeartFlow, Red-\nwood, CA 94063 USA, and also with the Biomedical Image Analy-\nsis Group, Imperial College London, London SW7 2AZ, U.K. (e-mail:\nmatthew.lee13@imperial.ac.uk).\nK. Petersen is with HeartFlow, Redwood, CA 94063 USA.\nN. Pawlowski is with the Biomedical Image Analysis Group, Imperial\nCollege London, London SW7 2AZ, U.K.\nColor versions of one or more of the ﬁgures in this article are available\nonline at http://ieeexplore.ieee.org.\nDigital Object Identiﬁer 10.1109/TMI.2019.2905990\nwas combined with anatomical atlases to perform segmen-\ntations of different organs [3]–[5]. However these methods\nrequire either an image-to-ima ge or image-to-segmentation\nlikelihood function to drive atlas matching or alignment of\nthe deformation model. Statistical methods such as active\nshape models [6] have been explored extensively with the\ndifﬁculty of constructing shape models in the ﬁrst place\nwhich are then often limited in their expressiveness due to\nthe underlying manifold learning method (linear or non-linear\nprincipal component analysis).\nState-of-the-art neural network based segmentation models\n[7]–[10] typically optimize pixel-wise loss functions such\nas mean squared error or cross entropy, and more recently\ndifferentiable Dice [11]. These objective functions do not take\nexplicit priors into consideration during training. Neverthe-\nless, smoothness priors can be enforced during test time by\nusing conditional random ﬁelds or similar post-processing\ntechniques. More recent work has shown improved results\nby directly incorporating shape constraints into their learn-\ning algorithm rather than applying them as post-processing\nsuch as in [12]–[14], where priors are learnt to regularize\nneural network embeddings during training. While this can\nlead to networks that favor plausible segmentations, there\nis no guarantee that the outputs adhere to desired shape\nconstraints, such as a single connected component or a closed\nsurface.\nA. Contributions\nIn this paper we introduce a new neural network model\nbased on template deformations which utilizes spatial trans-\nformer networks [15]. Our model leverages the representa-\ntional power of neural networks while explicitly enforcing\nshape constraints in segmentations by restricting the model to\nperform segmentation through deformations of a given shape\nprior. We call this Template Transformer Networks for Image\nSegmentation (T\nETRIS). As with template deformations, our\nmethod produces anatomically plausible results by regularizing\nthe deformation ﬁeld. This also avoids discretization artifacts\nas we do not restrict the network to make pixel-wise classiﬁ-\ncations. By using a neural network that is trained to align the\nshape prior to the structure of interest visible in the input image\nthere is no need for a hand crafted (intensity-based) image-\nto-segmentation registration measure as with other template\ndeformation models. To the best of our knowledge, this is the\nﬁrst full 3D neural network-based image segmentation through\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/\nLEE et al.:T ETRIS WITH SHAPE PRIORS 2597\nFig. 1. Schematic to illustrate the differences between traditional, pixel-wise segmentation models, the naive way of incorporating priors through\nadditional input, and our TETRIS model which produces a set of parameters for a transformation. By restricting the output space of the network to\nonly deformations of the prior, we obtain guarantees on topology.\nregistration method combining deep convolutional neural nets\nwith spatial transformers.\nAnother contribution of this paper is the demonstration that\nstate-of-the-art segmentation algorithms can be easily extended\nto incorporate implicit shape priors by providing a shape\ntemplate as additional input during training. To the best of\nour knowledge, this simple yet effective enhancement has not\nbeen considered in the past. Our benchmarking shows that this\ncan lead to a signiﬁcant increase in segmentation accuracy,\na high level graphical overview of these methods are given\nin Fig. 1.\nWe present promising results on coronary artery seg-\nmentation from cardiac computed tomography which further\nstrengthens the case for the use of priors in medical image\nsegmentation with deep neural nets. Experimentally we show\nthat all methods which utilize prior information are able to\nconsistently improve the cross entropy score of segmentation,\nand that our method is able to retain a singly connected\ncomponent segmentation. Our quantitative results show the\nvarying strengths and weakness of the two introduced meth-\nods. We also present qualitative results on synthetic examples\nto demonstrate the effects of out-of-sample data and how this\naffects neural network segmentations.\nB. Related Work\nOur proposed model lies in the intersection of machine\nlearning based image registration and segmentation, and the\nincorporation of shape priors into neural networks. The fol-\nlowing discusses the most related work but due to space\nlimitations and the large amount of work in these ﬁelds, this\ncannot provide a comprehensive overview.\n1) Atlas and Registration Based Segmentation: Atlas based\nsegmentation algorithms [16] are among the most popular\nmethods and rely on two key components, an intensity-\nbased image-to-image matching termL\nη and a set of training\nexamples, i.e. the atlases, with corresponding labels. During\ntesting, images can be compared to examples in the atlas\ndataset usingL\nη and the label mask of the most similar atlases\nare selected as candidate segmentations. This concept can be\nextended to employ patch based techniques or advanced label\nfusion procedures and is robust when label boundaries occur\nin homogeneous regions. However, this method often offers\na coarse segmentation, which may lack precision and can be\nreﬁned using linear and nonlinearregistration. This reﬁnement\ncan be done in either image or segmentation space [17]. The\nauthors of [18] proposed a combination of an image-to-image\nand segmentation-to-segmentation likelihood function, using a\nLagrange multiplier to weight the contribution of each term.\nIf an image-to-segmentation likelihood function is used then\nthis approach is better referred to as template registration [19].\n2) Statistical Shape Models: Active shape models as intro-\nduced in [20] explicitly model shape based on training exam-\nples. By discretizing thek dimensional shapes usingn control\npoints, they create point distribution models using an ellipsoid\nprior. Principle modes of variation can then be found using\nprinciple component Analysis (PCA) [21]. New shapes can be\nrepresented as a linear weighting of these components. Addi-\ntionally, by restricting the model to uset principle components\nwhere t < kn and restricting the range of values each linear\nweighting can have, a valid shape space is produced. Active\nappearance models [22] build on this technique and jointly\nmodel appearance together with shape. These models have\nbeen use widely in the medical imaging community to perform\nsegmentation [6]. However, such models are heavily biased by\nthe distribution of the training set used to build them.\n3) Network Based Image Registration: Traditional registra-\ntion algorithms take two images, a movingM and ﬁxed F\nand perform registration by iteratively updating some parame-\nterized transformationTθ which maps image grid locations to\neach other, such that some loss functionLη(M ◦ Tθ,F) is\nminimized, where bespoke parametersθ are found for a given\npair of images during test time. Optimization of the algorithm\ncan be considered as optimizingη, some parameterization of\nthe loss function which results in the ‘best’ registrations (such\nas the values of Lagrange multipliers) or by optimizing the\nchoice of T (the transformation family expressible).\nThe key difference between neural network based image\nregistration and traditional, iterative registration algorithms\nis that the loss function is only computed during training\nfor neural networks. The parameters of the neural network\nimplicitly encode what transf ormation, conditioned on the\ninput, is needed to register the image with minimal cost\ninstead of repeatedly calculating a loss to iteratively update\nthe parameters θ.\nRecent works on neural network based image registration\nfall into two major categories, the ﬁrst treats network based\nregistration as a regression problem on a given ground truth\ndeformation ﬁeld such as in [23]–[27]. These methods, unlike\n2598 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 38, NO. 11, NOVEMBER 2019\nours, can be used as fast approximations to other registration\nmodels. The second group of methods learn the deformation\nﬁeld implicitly while optimizing a downstream task. For exam-\nple, [28] combine momentum-parametrization for LDDMM\nshooting [29] and neural networks to learn an end-to-end\nmodel for registration. Reinforcement learning approaches\nhave also been used to perform image registration [30]. They\ntreat the registration problem as an iterative update of four\ntranslation and two rotation parameters so do not handle free\nform deformations.\nRelated network-based registration methods [31] and [32]\nuse 2D spatial transformer networks to embed the deformation\nmodel into a neural network pipeline in order to learn the\nregistration model end-to-end. The latter of which uses a\nFlowNet architecture [24]. This work was built on in [33]\nwhich performs full unsupervised 3D registration. However,\nunlike our model, all of these methods perform image-to-\nimage registration rather than template deformation for a\ndownstream segmentation task. The authors of [34], [35] begin\nto investigate template deformations but do not investigate this\nto its full 3D potential.\n4) Shape Priors in Neural Networks: Finally we discuss\nmethods which incorporate shape priors into neural networks.\nThough conditional random ﬁeld s are considered smooth-\nness priors, they do assist in providing shape consistency\nin segmentations. CRFs are incorporated into the training\nprocess in [12] by casting CRFs as recurrent neural networks.\nThis allows the segmentation and reﬁnement model to be\ntrained end-to-end. Adversarial training was used in [36] as\na means of learning such regularization without the need of\nan explicit model whilst still being able to train end-to-end.\nA discriminator network was used to distinguish segmentations\nfrom a network and ground truth segmentations, this training\nprocess encourages the network segmentations to look more\nplausible. An interleaving process was proposed in [37] where\niterative training of a neural network and CRF reﬁnement\nwas performed inspired by the grab cut [38] method, though\nthis model was not trained end-to-end. More recent work\nhas shown improved results by directly incorporating prin-\nciple component based shape constraints into their learning\nalgorithm. Building on the work of active shape models [20],\nthe authors of [13] use a PCA layer embedded in the neural\nnetwork to restrict its output space to be weightings of the\nprinciple components, this was extended to a probabilistic\nmodel in [39]. Another approach proposed in [14] exploits\nthe fact that autoencoders are able to capture a low dimen-\nsional representation of the shapes of segmentation maps.\nThis encoding is then used at training time to constrain the\noutputs of a segmentation network to be close to this low\ndimensional manifold via adversarial training. The latter two\nmethods utilize anatomical consistency across subjects.\n5) Spatial T ransformer Networks: Our work builds heavily on\nspatial transformer networks (STNs) [15] which we describe\nbelow. STNs are a neural network model that, conditioned on\nsome input I returns θ for some parameterized transformation\nmodel Tθ(G).T h a ti sθ = fψ(I) where fψ is a neural\nnetwork, itself parameterized byψ.O n c ew eh a v eθ,w ea r e\nable to differentiably re-sample our imageI to V using Tθ,\nas with image registration, here the image itself is re-sampled.\nThe STN model then passes the re-sampled imageV to another\nneural network, gξ(V ) which performs some down stream\ntask. In [15], they utilize this powerful model to train gξ,\nwhich performs a classiﬁcation task and simultaneous trainfψ\na deformation model that makes the down stream task easier\nvia a combination of rescaling, region of interest extraction\nand rotation of the input images.\nDuring training, the loss is calculated on the down stream\ntask only, for classiﬁcation this could be the cross entropy loss\nbetween the predicted class produced byg\nξ(V ) and the true\nclass. Since the neural networkgξ is a differentiable function\nand sampling from I to V is also differentiable we are able\nto train both tasks end to end. Inherently a spatial transformer\nis performing deformations that assist the down stream task,\nas opposed to having a loss calculated directly on the task\nof deforming. This can be considered an implicit registration\nstep, where the registration is autonomously discovered by the\nnetwork for optimal downstream performance. We do not have\nto decide what kind of deformation will be good for the task,\nthough we do need to specify the family of deformationsT .\nDuring test time, unlike iterative registration models, no loss\nvalue needs to be calculated. We simply need to perform a\nforward pass through the network to get both the deformation\nand the class prediction\nII. T\nEMPLA TE TRANSFORMER NETWORKS\nTraditional template deformation models require the deﬁ-\nnition of an image-to-segmentation matching function as an\napproximation or surrogate to the actual segmentation objec-\ntive. Iterative optimization is then used to incrementally update\nthe transformation parameters in order to maximize agreement\nbetween a template and the image to be segmented. In con-\ntrast, our method makes use of network based registration,\nwhich only requires the computation of a corresponding loss\nfunction (equivalent to the matching function) duringtraining\ntime. This important difference means we no longer need to\napproximate our actual segmentation function via an intensity-\nbased surrogate and can directly optimize for the task at hand.\nWe introduce a novel template deformation model that\nexploits the power of neural network-based registration. Our\nend-to-end model takes a shape prior in the form of a partial\nvolume image (PVI) and an image as input to a neural network\nwhich learns to deform the input prior so as to produce an\naccurate segmentation of the input image. This is done by\nimplicitly estimating a deformation ﬁeld so as to maximize\ntemplate alignment corresponding to optimal segmentation\naccuracy. We provide a detailed description of the main steps\nbelow. An overview of our method is shown inFig. 2.I nt h e\nfollowing subsections (II-B, II-C and II-D) we describe in\ndetail how we perform deformations, how we regularize our\ndeformation ﬁeld and how we handle large volume sizes.\nA. Obtaining Shape Templates\nShape priors can be utilized in neural networks in vari-\nous forms such as in the form of level sets, PVIs, binary\nmasks or as shape parameters (e .g., mesh control points).\nLEE et al.:T ETRIS WITH SHAPE PRIORS 2599\nFig. 2. TETRIS takes as input an image and a shape prior in the form of a partial volume image and produces a set of parameters for a transformation,\nthis transformation is then applied to the prior and the loss is calculated on the deformed prior and the target segmentation during training.\nIn this work we focus on the use of a deformation model,\nconditioned on a shape prior to deform a PVI into another\nPVI. Our shape prior itself in this particular case is also a PVI\nbut we emphasize this is not a necessity and richer priors such\nas statistical appearance models can also be used. As template\ntransfer networks predict a transformation instead of a point-\nwise segmentation map, they lend themselves naturally to the\nability of using other geometric representations for the priors\nsuch as mesh-based models. Shape priors can be generally\nobtained via manual, semi-automatic and automatic methods\nand the exact mechanism is application speciﬁc. We will later\ndiscuss one particular approach for obtaining shape priors for\nthe application of coronary artery segmentation.\nB. Deformation Model\nTo deform a template, consider some input imageI, shape\nprior U, ground truth segmentationT all of sizeH × W × D.\nWe have a sampling scheme (or deformation model)Tθ(G)\nwhere G is considered a standard co-ordinate grid, and loss\nfunction Lη. Tθ(G) is a function on(xt\ni , yt\ni , zt\ni ), grid coordi-\nnates in our target space, that maps to(xs\ni , ys\ni , zs\ni ) co-ordinates\nin our original source space, where we index voxel locations\nby i ∈[ 1,..., H ′W′D′] for notational simplicity. Given this\nwe can deﬁneV , a re-sampling of a priorU, based onTθ as\nVi =\nH∑\nn\nW∑\nm\nD∑\nl\nUmnl × k(xs\ni − m; /Phi1x ) × k(ys\ni − n; /Phi1y)\n× k(zs\ni − l; /Phi1z) ∀i ∈[ 1,..., H ′W′D′] (1)\nwhere k is any sampling kernel with parameters/Phi1.F o ri m a g e\ninterpolation we use a trilinear kernel to prevent re-sampled\npixel values from being extrapolated to outside of the original\nintensity domain, that is\nVi =\nH∑\nn\nW∑\nm\nD∑\nl\nUmnl × max(0,1 −| xs\ni − m|)\n× max(0,1 −| ys\ni − n|) × max(0,1 −| zs\ni − l|) (2)\nWe chooseTθ to be a free form deformation, i.e.θ is a three\ndimensional vector ﬁeld. For notational simplicity we deﬁne\nthe sampling grid function as\n⎡\n⎣\nxs\ni\nys\ni\nzs\ni\n⎤\n⎦ = Tθ(Gi ) =\n⎡\n⎣\nxt\ni\nyt\ni\nzt\ni\n⎤\n⎦ −\n⎡\n⎣\nθx\ni\nθy\ni\nθz\ni\n⎤\n⎦ (3)\nIf Tθ is a free form deformation which is not in the same\nresolution as the target image we are required to re-sample the\ndeformation ﬁeld. Potentially using a different set of sampling\nkernels k with it’s own parameters /Phi1. We choose to use\nB-Spline interpolation to ensure smooth ﬁelds [40], utilizing\nthe Catmull-Rom solution to the interpolation problem [41].\nOur method takes inspiration from STNs by using a neural\nnetwork f\nψ (I,U), which is conditioned on both the input\nimage I and the shape priorU, to produce parametersθ of\nthe B-Spline deformation modelTθ . We can then perform a\ndeformation of the priorU, calculate a segmentation loss and\nupdate the parametersψ of our network.\nBy combining template deformation with neural networks,\nwe mitigate the key problem with traditional template defor-\nmation models, that being the need to hand craft a good\nimage to segmentation alignment function. The source of this\nproblem, as with any registration technique, lies in the fact that\na loss calculation must be made during test time to update the\ndeformation ﬁeld parametersθ. By utilizing STNs to produce\nθ during test time and instead updating a neural networkf\nψ\nduring training, we can train a registration model with the true\nsegmentation loss function (based on alignment between prior\nand reference segmentation) avoiding the need for surrogate\nfunctions at test time.\nThe template deformation model is network agnostic so\nany neural network can be used. We choose a simple feed\nforward network architecture with convolutions and max pool-\ning to produce a deformation ﬁeld which we use in the\nSTN to deform the prior. Full details of which are provided\nin Figs. 3 and 12.\nOur method is able to take any shape prior and deform\nit with sub-pixel accuracy, unlike other neural network based\nsegmentation algorithms which typically treat segmentation as\npixel-wise classiﬁcation. Since our model smoothly deforms\na prior, we are able to produce partial volume segmentations,\nreducing discretization artifacts in ﬁnal segmentation maps.\nWe provide experiments on both partial volume data as well\nas voxel-wise classiﬁcation results.\n2600 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 38, NO. 11, NOVEMBER 2019\nFig. 3. Graphical representation of the three different models explored in this paper. from left to right, the U-Net, FCN and the black box model used\nto produce deformation parameters for TETRIS. Building blocks are described inFig. 12.\nC. Field Regularization\nDue to the ill-posed nature of registration problems,\nit is common to constrain deformation ﬁelds by adding a\nregularization term to the optimization problem that favors\nsome desired property, such as locally smooth deforma-\ntions, or anl2 penalty on the vector ﬁeld itself to favor mini-\nmum displacement solutions. We investigate two regularization\nterms\nL\nl2 := 1\nV\n∫ X\n0\n∫ Y\n0\n∫ Z\n0\nT (x, y, z)2dxdydz (4)\nand\nLsmooth := 1\nV\n∫ X\n0\n∫ Y\n0\n∫ Z\n0\n(∂2T\n∂x2\n)2\n+\n(∂2T\n∂y2\n)2\n+\n(∂2T\n∂z2\n)2\n+ 2\n(∂2T\n∂zx\n)2\n+ 2\n(∂2T\n∂xy\n)2\n+ 2\n(∂2T\n∂yz\n)2\ndxdydz (5)\nLl2 penalizes thel2-norm of the ﬁeld andLsmooth penalizes\nthe sum of squared second order derivatives.\nD. Field Aggregation\nTo deal with the size of the data and the memory restrictions\nof modern graphics processing units we do inference on a\npatch basis, we collect control points across patches and\naggregate them before re-sampling using B-Spline interpola-\ntion. This combined with only using valid padding prevents\nill-posed boundary conditions across the image. This also\nallows us to perform inference on variable size volumes with\nconsistent control point spacing without modiﬁcation to the\nneural network.\nFig. 4. Examples of where a deformation model can extrapolate well\noutside of the distribution of the training data compared to a standard\nconvolutional neural network.\nIII. I LLUSTRA TIVE EXAMPLE\nAs a proof of concept, we present qualitative results on\nthe effects of corruption in the data as these are not easily\nquantiﬁable. To investigate how incorporating a prior into a\nneural network can help when corruption is present, we create\na toy dataset of 1500 randomly deformed P’s, B’s and R’s\nfor training and two hand crafted test images which we\nprovide qualitative results for. We then train a deformation\nmodel to deform the prior (the letter that was originally\ndeformed) to match the deformed letter. Additionally, we train\na normal convolution neural network to predict the deformed\nletter on a pixel-wise level. Both the T\nETRIS model and the\nconvolutional neural network are conditioned on the prior and\nthe target. As is expected, when the image signal is strong,\nLEE et al.:T ETRIS WITH SHAPE PRIORS 2601\nFig. 5. Qualitative results from varying the amount of corrupted training\nexamples in the dataset. From left to right, the models are trained with 0%,\n5%, 10% and 15% of the training set consisting of corrupted examples.\nTABLE I\nQUANTIT A TIVESYNTHETIC DATA RESULTS\nthe network learns to rely heavily on the image signal and\nignores the prior, this can be seen inFig. 4. We trained both\nmodels on only uncorrupted deformations to see how each\nmodel can handle an out-of-sample test case. We see that the\nvanilla CNN learns to completely ignore prior information,\nso when inferring on corrupt data, it is not able to extrapolate,\nunlike the TETRIS model. By restricting our model’s output\nspace to be within the range of deformations of the prior we are\nable, even in the presence of corruption to produce plausible\nresults, consistent with our prior.\nIV . SYNTHETIC EXPERIMENTS\nWe argue that for a CNN to handle such corruption it would\nneed to be present in the training set,Fig. 5shows the effects\nof having an increased amount of corrupted data in the training\nset. We construct a secondary dataset where corruption is more\neasily generated which consists of 1000 randomly deformed\ndiscs, where corruption is in the form of smaller discs being\ncut from the main central disc and smaller peripheral discs\nbeing placed around the main central disc and the set is split\nin half for training and testing. We trained the models with 0%,\n5%, 10% and 15% of the training set consisting of corrupted\nexamples. As more corruption is present in the training set,\nthe better the standard CNN model is able to handle them\nduring test time as expected. Though the artifacts that occur\nare not topologically as plausible as those produced by our\nT\nETRIS model, which is reﬂected in the high dice scores but\nalso high Hausdorff distances.\nV. CORONARY ARTERY SEGMENTA TION EXPERIMENTS\nIn this paper we focus on the application of vessel segmen-\ntation where ambiguities arise from the functional distinction\nbetween veins and arteries, which may have similar image\nfeatures. This has lead some methods to approach the problem\nFig. 6. An example of the shape prior on the left and manual seg-\nmentation on the right. The shape prior is a tubed human annotated\ncenterline with a ﬁxed one millimeter radius. (a) Tubed prior. (b) Target\nsegmentation.\nas a multi stage process, ﬁrst centerlines are extracted [42],\nthen the vessels are segmented [43]. Shape priors can be\nenforced once good candidate centerlines have been extracted\nby treating the segmentation task as a wall distance estimation\ntask. By utilizing curved planar reformation [44] the segmen-\ntation problem can be cast as a wall distance regression from\nthe centerline and topology can be guaranteed.\nWe train our network on a set of 274 annotated cardiac CT\nvolumes with 0.5 millimeter isotropic spacing and reserving\n138 volumes for validation and an additional 136 for testing.\nThe ground truth labels obtained through manual expert seg-\nmentation are in the form of partial volumes.\nA. Generating Priors for Coronary Arteries\nTo generate a shape prior for coronary artery segmentation,\nwe ﬁrst extract out a centerline using a semi-automatic method\nwhich consists of a Random Forest voxel-wise classiﬁcation,\na Dijkstras shortest path based tree extraction and ﬁnally a\nhuman review and correction step to correct outliers. The\ncenterline is converted to a 3D volumetric representation by\ncreating a tube around it with a ﬁxed radius of 1 mm in a\npartial volume image, since the centerline exists in arbitrary\nspace rather than voxel space. More examples of coronary\ncenterline extraction method in cardiac CTA can be found\nin [42].Fig. 6is a volume rendering illustrating the difference\nbetween the prior and the ground truth segmentation in an\nexample vessel. More generally, priors can be extracted from\nsources such as automated algorithms, weak labels, human\nexpert knowledge or population based statistics and will inher-\nently be application speciﬁc.\nB. Model Details and Baselines\nWe use two baseline models to compare the three models we\npresent i) the residual fully convolutional network (FCN) and\nii) a residual U-net architecture utilizing the implementations\nfrom [45] using residual blocks from [46]. Details of the\narchitectures can be found inFig. 3, where the building blocks\nare described inFig. 12.\nWe also present results on naively incorporating shape priors\ninto these state-of-the-art models. We do this by feeding the\nnetworks two channels of input, the image to be segmented\n2602 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 38, NO. 11, NOVEMBER 2019\nand the prior that we have of the image at that location. This\nalternative method is a very simple extension of existing state-\nof-the-art pixel-wise approaches, computationally cheap and\neasy to implement. The shape prior, in this case, acts as a\nkind of initialization for the network’s output.\nC. Training Details\nFor all models we use the same patch extraction para-\nmeters, during training we dynamically extract 32 patches\nfrom each volume and randomly shufﬂe them into a buffer\nof 512 patches. Patches are extracted if they are near the\ncenterline, biasing the sampling around the vessel. We use a\nbatch size of 8 for all models and train them using the Adam\noptimizer [47] while exponentiallydecaying the learning rate.\nThe learning rate at stepi is as deﬁned as\nl\ni = l0 · r\ni\ns (6)\nwhere our initial learning ratel0 = 1 · 10−5, decay rater =\n0.99, decay step s = 500 and where regularization is used,\nwe weight it by 5· 10−6.\nWe pretrain our baseline models using a weighted cross\nentropy function as deﬁned in Equation 7, where p is our\ntarget distribution, q is our candidate distribution andw is a\nweighting factor. By settingw> 1, we bias the loss term to\npenalize false negatives. This is beneﬁcial as voxels containing\nthe vessel interior are sparse in any given patch. Penalizing\nfalse negatives more prevents the network from predicting\nall voxels as background voxels during the initial stages of\noptimization, a trivial local optimum. Note this does not\nneed to be done with our T\nETRIS model as the network is\nalready biased towards the identity transform thanks to our\nregularization term which favors a smooth deformation ﬁeld.\nFor our experiments we set w = 2 and pretrain our non-\nT\nETRIS models for 1000 iterations.\n−w · p log(q) − (1 − p) log(1 − q) (7)\nWe ﬁne tune the models using normal, un-weighted, cross\nentropy for a further 5000 iterations.\nD. Results\nResults are presented on a test set of 136 cases, for the\ntask of partial volume estimation we use the cross entropy\nas a measure of accuracy, we can see from Table II that\nincorporating shape priors into state of the to state-of-the-\nart neural network segmentation models signiﬁcantly improves\nresults. For comparison we include results on using the identity\nfunction on the prior, i.e. naively taking the shape prior as the\nsegmentation.\nWe provide box plots of the results inFig. 9 for a more\nﬁne grained break down of the results, where we have plotted\nthe cross entropy on a log scale. Though our model with\nl2 ﬁeld regularizations performs the best, there is no signiﬁcant\ndifference between the methods, exhibiting the expressiveness\nof a deformation model despite constraining the output space\nto be a deformation of the prior.\nFig. 7. Close up of qualitative results shown as contours for the\ndifferent methods where the blue, green, red and cyan contours are\nof the target segmentation, T\nETRIS, FCN (with prior) and U-Net (with\nprior) respectively. On the left and right are orthogonal views of the\nleft anterior descending artery, near the ﬁrst diagonal bifurcation where\nTETRIS outperforms other methods. (a) T ETRIS-l 2. (b) T ETRIS-l 2.\n(c) FCN (w/ prior). (d) FCN (w/ prior). (e) U-Net (w/ prior). (f) U-Net (w/\nprior).\nTABLE II\nQUANTIT A TIVESEGMENTATION RESULTS ON TEST CASES\nBoth our U-Net (with prior) model and our proposed\nTETRIS model are able to consistently produce singly con-\nnected components without post processing by incorporating\nprior information, further demonstrating the potential of our\napproachs. However the FCN (with prior) model often does\nnot capture these higher order requirements, even with prior\ninformation, we believe this is due to the inherent multi-scale\nnature of the U-Net architecture. T\nETRIS shines when using\nmetrics that take into account partial volumes, however our\nCNNs with priors added as input channels work consistently\nwell when the goal is pixel-wise binary segmentation. We also\nLEE et al.:T ETRIS WITH SHAPE PRIORS 2603\nFig. 8. Close up of qualitative results shown as contours for the\ndifferent methods where the blue, green, red and cyan contours are of\nthe target segmentation, TETRIS, FCN (with prior) and U-Net (with prior)\nrespectively. On the left and right are orthogonal views of a trifurcation\nwhere T\nETRIS over segments and the FCN and U-Net model under\nsegment. (a) TETRIS-l 2. (b) TETRIS-l 2. (c) FCN (w/ prior). (d) FCN (w/\nprior). (e) U-Net (w/ prior). (f) U-Net (w/ prior).\nnote a drastic reduction of trainable parameters by a factor of\nten for TETRIS compared to U-Net and FCN, indicating a\nbetter balance between performance and model complexity.\nTo obtain the number of connected components, we thresh-\nold the partial volume segmentations at 0.5 and perform a\n26-connected component analysis. Ideally, all segmentations\nshould have only one connected component. We note that\nT\nETRIS without regularization may result in discontinuous\nsegmentations, but did not ﬁnd this to be the case in practice.\nWe notice no major difference between penalizing the ﬁeld\nwith an l2 penalty or by the sum of second order derivatives.\nFig. 10 shows an example case where neural networks are\nnot able to recover the vessel in the image without prior\ninformation, where as all three our models are able to fall\nback on the prior when the image signal may be weak.\nWe further investigated the use of more complex models for\nT\nETRIS but found that convergence became slow and often\nresulted in similar validation scores, hence our choice for a\nsimple TETRIS model. We notice that the models also have\ndifferent strengths and weaknesses, as mentioned previously,\nthe U-Net (with prior) model is better than the FCN (with\nFig. 9. Cross entropy for partial volume estimation of test cases for\nthe different methods investigated, clearly demonstrating the beneﬁts of\nincorporating prior information and the ability of a deformation model to\nperform just as well, if not better than an standard neural network which\nnaively incorporates prior information.\nFig. 10. Close up of qualitative results shown as volume rendering for\nthe different methods with and without shape priors compared to the\nmanual target segmentation. (a) Target segmentation. (b) TETRIS-l 2.\n(c) FCN (w/ prior). (d) U-Net (w/ prior). (e) FCN. (f) U-Net.\nprior) model at capturing global consistency of shape but in\nregions where contrast is low, our TETRIS model produces\nsmoother more accurate segmentations as seen inFig. 7.\nUsing a deformation model does have caveats, inFig. 8we\nsee a trifurcation region where TETRIS over-segments and\nboth the U-Net/FCN with prior under-segment. The resolution\n2604 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 38, NO. 11, NOVEMBER 2019\nFig. 11. Qualitative results shown as volume rendering for the different\nmethods with and without shape priors compared to the manual target\nsegmentation. The transfer function from partial volume probability to\nopacity is set as the identity function from [0,1] → [0,1]. (a) Target\nsegmentation. (b) T\nETRIS-l 2. (c) FCN (w/ prior). (d) U-Net (w/ prior).\n(e) FCN. (f) U-Net.\nof the ﬁeld and the penalty applied to large deformations\nprevents our model from doing well in such regions.\nIn summary, we should highlight the advantages of the\ntemplate transformer based networks over point-wise seg-\nmentation models such as U-net and FCN, as it might not\nbe apparent from the segmentation scores. Although, U-Net\nperforms best on Dice, T\nETRIS performs better on cross-\nentropy assessing the agreement for the soft, partial volume\npredictions. Additionally, the template transformer networks\ncan provide guarantees on the resulting shape while both\nU-Net and FCN do not. This can be important in applications\nwhere the segmentations are used for downstream tasks such as\nshape analysis or blood ﬂow calculation. One other important\nbeneﬁt of the T\nETRIS model (although not explored in this\nwork) is the ability to incorporate a variety of shape priors\nsuch as mesh-based representations or probabilistic shape and\nappearance models (e.g. a mean and variance image).\nVI. D ISCUSSION\nWe introduced Template Transformer Networks for image\nsegmentation which are able to deform shape priors into\nsegmentations. This work builds on template deformations by\nno longer requiring the need for hand-crafted image to seg-\nmentation cost functions and makes use of Spatial Transformer\nFig. 12. Graphical representation of the building blocks used to construct\nall models.\nNetworks for differentiable end-to-end learning. Our method\nis competitive with state of the art segmentation algorithms\nwhile being able to guarantee topological constraints.\nOur work is a proof of concept which relied on a simple\narchitecture that can be easily extended. Though our model is\nrestricted in the sense that it can only perform deformations\nLEE et al.:T ETRIS WITH SHAPE PRIORS 2605\nof a prior, we argue this can be an advantage where shape\nguarantees are important. Arguably, our model strikes a better\nbalance between performance and model complexity due to a\nsigniﬁcantly fewer number of trainable parameters.\nWe consider the prior extraction beyond the scope of this\nwork, but we believe that it is a critical part of not only our\nmethod, but all methods which require a prior. In problems\nwhere no sensible priors can used template based methods\nare likely not suitable. Though not explored in this work,\nour approach lends itself to the incorporation of much richer\npriors, such as probabilistic shape priors and other geometric\nrepresentations such as meshes or point distribution models.\nOur method replaces an iterative method with a one-shot\nmethod, we believe a natural extension to investigate would be\nto incorporate template deformations with recurrent or auto-\nregressive neural networks for more ﬂexible and potentially\nlarger deformations, mitigating the effects of the chosen res-\nolution for the control point grid. Though in this work we\nchose to use B-Splines, our method is agnostic to the choice\nof parameterization of the deformation ﬁeld. The exploration\nof other and potentially more ﬂexible parameterizations is also\nof great interest. Additionally, we would also like to explore\nthe use of deformation ﬁelds which are not on ﬁxed grids so\nas to allow for ﬁner deformation ﬁelds as and when is needed\nwithout the burden of excess computation.\nA\nPPENDIX\nWe provide full examples of vessel segmentations inFig. 11\nto give the reader larger context into the accuracy of the model,\nw h e r ew eh a v et r i m m e dt h ea o r t afor clearer visualizations.\nWithout prior information it is clear that the sensitivity of the\nnetworks drop substantially.\nACKNOWLEDGEMENT\nThe authors would like to thank Sarah Parisot, Katarína\nTóthová, Ozan Oktay and Sofy Weisenberg for insightful\nfeedback on the manuscript as well as Vishwanath Sangale,\nDavid Lesage, James Batten and Karl Hahn for fruitful dis-\ncussions. They would also like to thank the reviewers for their\nconstructive feedback.\nR\nEFERENCES\n[1] M. S. Nosrati and G. Hamarneh. (2016). “Incorporating prior knowl-\nedge in medical image segmentation: A survey.” [Online]. Available:\nhttps://arxiv.org/abs/1607.01092\n[2] A. L. Yuille, D. S. Cohen, and P. W. Hallinan, “Feature extraction from\nfaces using deformable templates,” inProc. IEEE Comput. Soc. Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 1989, pp. 104–109.\n[3] G. Subsol, J.-P. Thirion, and N. Ayache, “A scheme for automatically\nbuilding three-dimensional morphometric anatomical atlases: Applica-\ntion to a skull atlas,”Med. Image Anal., vol. 2, no. 1, pp. 37–60, 1998.\n[4] T. Kapur, P. Beardsley, S. Gibson, W. Grimson, and W. Wells, “Model-\nbased segmentation of clinical knee MRI,” inProc. IEEE Int. Workshop\nModel-Based 3D Image Anal., Jan. 1998, pp. 97–106.\n[5] D. Perperidis et al., “Building a 4D atlas of the cardiac anatomy and\nmotion using mr imaging,” inProc. IEEE Int. Symp. Biomed. Imag.,\nNano Macro, Apr. 2004, pp. 412–415.\n[6] T. Heimann and H.-P. Meinzer, “Statistical shape models for 3D medical\nimage segmentation: A review,” Med. Image Anal., vol. 13, no. 4,\npp. 543–563, 2009.\n[7] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in Proc. MICCAI, May 2015,\npp. 234–241.\n[8] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” inProc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2015, pp. 3431–3440.\n[9] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille.\n(2016). “Deeplab: Semantic image segmentation with deep convolutional\nnets, atrous convolution, and fullyconnected CRFs.” [Online]. Available:\nhttps://arxiv.org/abs/1606.00915\n[10] K. Kamnitsaset al., “Efﬁcient multi-scale 3D CNN with fully connected\nCRF for accurate brain lesion segmentation,”Med. Image Anal., vol. 36,\npp. 61–78, Feb. 2017.\n[11] F. Milletari, N. Navab, and S.-A. Ahmadi. (2016). “V-Net: Fully convo-\nlutional neural networks for volumetric medical image segmentation.”\n[Online]. Available: https://arxiv.org/abs/1606.04797\n[12] S. Zheng et al. , “Conditional random ﬁelds as recurrent neural\nnetworks,” in Proc. IEEE Int. Conf. Comput. Vis. , Dec. 2015,\npp. 1529–1537.\n[13] F. Milletari, A. Rothberg, J. Jia, and M. Sofka, “Integrating statistical\nprior knowledge into convolutional neural networks,” inProc. Int. Conf.\nMed. Image Comput. Comput.-Assisted Intervent. Cham, Switzerland:\nSpringer, 2017, pp. 161–168.\n[14] O. Oktay et al., “Anatomically constrained neural networks (ACNNs):\nApplication to cardiac image enhancement and segmentation,” IEEE\nTrans. Med. Imag., vol. 37, no. 2, pp. 384–395, Feb. 2018.\n[15] J. Hu, J. Lu, Y .-P. Tan, and J. Zhou, “Deep transfer metric learning,”\nIEEE Trans. Image Process., vol. 25, no. 12, pp. 5576–5588, Dec. 2016.\n[16] J. E. Iglesias and M. R. Sabuncu, “Multi-atlas segmentation of biomed-\nical images: A survey,”Med. Image Anal., vol. 24, no. 1, pp. 205–219,\n2015.\n[17] E. D’Agostino, F. Maes, D. Vandermeulen, and P. Suetens, “An informa-\ntion theoretic approach for non-rigid image registration using voxel class\nprobabilities,” Med. Image Anal., vol. 10, no. 3, pp. 413–431, 2006.\n[18] W. Baiet al., “A probabilistic patch-based label fusion model for multi-\natlas segmentation with registration reﬁnement: Application to cardiac\nMR images,”IEEE Trans. Med. Imag., vol. 32, no. 7, pp. 1302–1315,\nJul. 2013.\n[19] K. A. Saddi, C. Chefd’hotel, M. Rousson, and F. Cheriet, “Region-based\nsegmentation via non-rigid template matching,” inProc. IEEE 11th Int.\nConf. Comput. Vis. (ICCV), Oct. 2007, pp. 1–7.\n[20] T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham, “Active shape\nmodels-their training and application,”Comput. Vis. Image Understand.,\nvol. 61, no. 1, pp. 38–59, 1995.\n[21] B. Flury, Multivariate Statistics: A Practical Approach. London, U.K.:\nChapman & Hall, 1988.\n[22] T. F. Cootes, G. J. Edwards, and C. J. Taylor, “Active appearance\nmodels,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 23, no. 6,\npp. 681–685, Jun. 2001.\n[23] H. Uzunova, M. Wilms, H. Handels, and J. Ehrhardt, “Training\ncnns for image registration from few samples with model-based data\naugmentation,” in Medical Image Computing and Computer Assisted\nIntervention—MICCAI , M. Descoteaux, L. Maier-Hein, A. Franz,\nP. Jannin, D. L. Collins, and S. Duchesne, Eds. Cham, Switzerland:\nSpringer, 2017, pp. 223–231.\n[24] A. Dosovitskiy et al., “FlowNet: Learning optical ﬂow with convolu-\ntional networks,” in Proc. IEEE Int. Conf. Comput. Vis., Dec. 2015,\npp. 2758–2766.\n[25] H. Sokooti, B. de V os, F. Berendsen, B. P. F. Lelieveldt, I. Išgum,\nand M. Staring, “Nonrigid image registration using multi-scale\n3D convolutional neural networks,” in Medical Image Comput-\ning and Computer Assisted Intervention—MICCAI , M. Descoteaux,\nL. Maier-Hein, A. Franz, P. Jannin, D. L. Collins, and S. Duchesne,\nEds. Cham, Switzerland: Springer, 2017, pp. 232–239.\n[26] X. Caoet al., “Deformable image registration based on similarity-steered\nCNN regression,” inMedical Image Computing and Computer Assisted\nIntervention—MICCAI , M. Descoteaux, L. Maier-Hein, A. Franz,\nP. Jannin, D. L. Collins, and S. Duchesne, Eds. Cham, Switzerland:\nSpringer, 2017, pp. 300–308.\n[27] M.-M. Rohé, M. Datar, T. Heimann, M. Sermesant, and\nX. Pennec, “SVF-Net: Learning deformable image registration using\nshape matching,” inMedical Image Computing and Computer Assisted\nIntervention—MICCAI , M. Descoteaux, L. Maier-Hein, A. Franz,\nP. Jannin, D. L. Collins, and S. Duchesne, Eds. Cham, Switzerland:\nSpringer, 2017, pp. 266–274.\n[28] X. Yang, R. Kwitt, M. Styner, and M. Niethammer, “Quicksilver: Fast\npredictive image registration—A deep learning approach,”NeuroImage,\nvol. 158, pp. 378–396, Sep. 2017.\n2606 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 38, NO. 11, NOVEMBER 2019\n[29] M. F. Beg, M. I. Miller, A. Trouvé, and L. Younes, “Computing large\ndeformation metric mappings via geodesic ﬂows of diffeomorphisms,”\nInt. J. Comput. Vis., vol. 61, no. 2, pp. 139–157, 2005.\n[30] K. Ma, J. Wang, V . Singh, B. Tamersoy, Y .-J. Chang, A. Wimmer,\nand T. Chen, “Multimodal image registration with deep context rein-\nforcement learning,” inProc. Int. Conf. Med. Image Comput. Comput.-\nAssisted Intervent. Cham, Switzerland: Springer, 2017, pp. 240–248.\n[31] B. D. de V os, F. F. Berendsen, M. A. Viergever, M. Staring, and\nI. Išgum. (2017). “End-to-end unsupervised deformable image reg-\nistration with a convolutional neural network.” [Online]. Available:\nhttps://arxiv.org/abs/1704.06065\n[32] S. Shan, X. Guo, W. Yan, E. I.-C. Chang, Y . Fan, and Y . Xu. (2017).\n“Unsupervised end-to-end learningfor deformable medical image reg-\nistration.” [Online]. Available: https://arxiv.org/abs/1711.08608\n[33] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, and A. V . Dalca,\n“An unsupervised learning model for deformable medical image regis-\ntration,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2018,\npp. 9252–9260.\n[34] H. Zhang and X. He, “Deep free-form deformation network for object-\nmask registration,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nOct. 2017, pp. 4251–4259.\n[35] C. Qinet al.(2018). “Joint learning of motion estimation and segmenta-\ntion for cardiac mr image sequences.” [Online]. Available: https://arxiv.\norg/abs/1806.04066\n[36] P. Luc, C. Couprie, S. Chintala, and J. Verbeek. (2016). “Semantic seg-\nmentation using adversarial networks.” [Online]. Available: https://arxiv.\norg/abs/1611.08408\n[37] M. Rajchl et al., “DeepCut: Object segmentation from bounding box\nannotations using convolutional neural networks,” IEEE Trans. Med.\nImag., vol. 36, no. 2, pp. 674–683, Jun. 2017.\n[38] C. Rother, V . Kolmogorov, and A. Blake, “GrabCut: Interactive fore-\nground extraction using iterated graph cuts,” ACM Trans. Graph.,\nvol. 23, no. 3, pp. 309–314, 2004.\n[39] K. Tóthová et al. (2018). “Uncertainty quantiﬁcation in CNN-based\nsurface prediction using shape priors.” [Online]. Available: https://arxiv.\norg/abs/1807.11272\n[40] D. Rueckert, P. Aljabar, R. A. Heckemann, J. V . Hajnal, and\nA. Hammers, “Diffeomorphic registration using B-splines,” inMedical\nImage Computing and Computer-Assisted Intervention—MICCAI,v o l .9 .\nBerlin, Germany: Springer, 2006, pp. 702–709.\n[41] E. Catmull and R. Rom, “A class of local interpolating splines,”\nin Computer Aided Geometric Design. Amsterdam, The Netherlands:\nElsevier, 1974, pp. 317–326.\n[42] M. Schaap et al., “Standardized evaluation methodology and reference\ndatabase for evaluating coronary artery centerline extraction algorithms,”\nMed. Image Anal., vol. 13, no. 5, pp. 701–714, 2009.\n[43] H. A. Kiri¸sli et al., “Standardized evaluation framework for evaluating\ncoronary artery stenosis detection, stenosis quantiﬁcation and lumen\nsegmentation algorithms in computed tomography angiography,”Med.\nImage Anal., vol. 17, no. 8, pp. 859–876, 2013.\n[44] A. Kanitsar, D. Fleischmann, R. Wegenkittl, P. Felkel, and M. E. Gröller,\n“CPR: Curved planar reformation,” inProc. Conf. Vis., 2002, pp. 37–44.\n[45]\nN. Pawlowski et al. (2017). “DLTK: State of the art reference imple-\nmentations for deep learning on medical images.” [Online]. Available:\nhttps://arxiv.org/abs/1711.06853\n[46] K. He, X. Zhang, S. Ren, and J. Sun. (2015). “Deep residual learn-\ning for image recognition.” [Online]. Available: https://arxiv.org/abs/\n1512.03385\n[47] D. P. Kingma and J. Ba. (2014). “Adam: A method for stochastic\noptimization.” [Online]. Available: https://arxiv.org/abs/1412.6980",
  "topic": "Prior probability",
  "concepts": [
    {
      "name": "Prior probability",
      "score": 0.7901204824447632
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7649896740913391
    },
    {
      "name": "Computer science",
      "score": 0.6613184213638306
    },
    {
      "name": "Segmentation",
      "score": 0.640826404094696
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5725290775299072
    },
    {
      "name": "Image segmentation",
      "score": 0.5438362956047058
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4586479961872101
    },
    {
      "name": "Pixel",
      "score": 0.4312152862548828
    },
    {
      "name": "Scale-space segmentation",
      "score": 0.4150731861591339
    },
    {
      "name": "Computer vision",
      "score": 0.4139164388179779
    },
    {
      "name": "Discretization",
      "score": 0.41278788447380066
    },
    {
      "name": "Mathematics",
      "score": 0.1884291172027588
    },
    {
      "name": "Bayesian probability",
      "score": 0.17924386262893677
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I47508984",
      "name": "Imperial College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210154413",
      "name": "Institute of Group Analysis",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I79463011",
      "name": "Analysis Group (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210130944",
      "name": "HeartFlow (United States)",
      "country": "US"
    }
  ],
  "cited_by": 114
}