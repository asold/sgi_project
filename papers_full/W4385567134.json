{
    "title": "Language Models as Agent Models",
    "url": "https://openalex.org/W4385567134",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2558501541",
            "name": "Jacob Andreas",
            "affiliations": [
                "Moscow Institute of Thermal Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W4299567010",
        "https://openalex.org/W3089025530",
        "https://openalex.org/W2767257667",
        "https://openalex.org/W3173798466",
        "https://openalex.org/W3216037316",
        "https://openalex.org/W2606347107",
        "https://openalex.org/W3037115370",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W2951936329",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2059146861",
        "https://openalex.org/W4287887107",
        "https://openalex.org/W3202546170",
        "https://openalex.org/W4287124808",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2251652451",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4281657280",
        "https://openalex.org/W4287067912",
        "https://openalex.org/W4223947928",
        "https://openalex.org/W3171737424",
        "https://openalex.org/W4307123345",
        "https://openalex.org/W4283170666",
        "https://openalex.org/W3034723486",
        "https://openalex.org/W2056245254",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W3100307207",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2951976932",
        "https://openalex.org/W4214717370"
    ],
    "abstract": "Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in the outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them—a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of communicative intentions in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language. I survey findings from the recent literature showing that—even in today's non-robust and error-prone models—LMs infer and use representations of fine-grained communicative intentions and high-level beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5769–5779\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nLanguage Models as Agent Models\nJacob Andreas\nMIT\njda@mit.edu\nAbstract\nLanguage models (LMs) are trained on collec-\ntions of documents, written by individual hu-\nman agents to achieve specific goals in the out-\nside world. During training, LMs have access\nonly to text of these documents, with no di-\nrect evidence of the internal states of the agents\nthat produced them—a fact often used to ar-\ngue that LMs are incapable of modeling goal-\ndirected aspects of human language production\nand comprehension. Can LMs trained on text\nlearn anything at all about the relationship be-\ntween language and use? I argue that LMs\nare models of intentional communication in a\nspecific, narrow sense. When performing next\nword prediction given a textual context, an LM\ncan infer and represent properties of an agent\nlikely to have produced that context. These rep-\nresentations can in turn influence subsequent\nLM generation in the same way that agents’\ncommunicative intentions influence their lan-\nguage. I survey findings from the recent litera-\nture showing that—even in today’s non-robust\nand error-prone models—LMs infer and use\nrepresentations of fine-grained communicative\nintentions and high-level beliefs and goals. De-\nspite the limited nature of their training data,\nthey can thus serve as building blocks for sys-\ntems that communicate and act intentionally.\n1 Introduction\nDespite remarkable recent progress in recent years,\ntoday’s language models (LMs) still make odd pre-\ndictions and conspicuous errors:1\nI never ate grilled cheese before my motherdied. I\nnever ate grilled cheese before my mother died [sic]. It\nwas her favorite food, and she always made it for me\nwhen I was a child. After she passed away, I just couldn’t\nbring myself to eat it.\n1This, and other LM samples in this paper, were obtained\nvia greedy decoding from the GPT-3 text-davinci-002\nmodel of Ouyang et al. (2022). Author-provided prompts\nare shown in bold, and the remaining text is model-generated.\nThis (rather macabre) sample is representative of a\nbroad category of similar behaviors from text gener-\nation models: outputs from current LMs sometimes\ndescribe impossible situations, contradictory propo-\nsitions, or invalid inferences (Marcus and Davis,\n2020; Bender and Koller, 2020). What these errors\nhave in common is a failure to model communica-\ntive intent: they may be grammatically or even\nsemantically acceptable, but not the sort of texts\nthat could be produced by an author with a coherent\nset of beliefs or goals. Nevertheless, the LMs that\nproduce them underlie some of the most striking\nsuccesses in modern NLP, including high-quality\ntranslation, summarization, and question answer-\ning (Brown et al., 2020; Chowdhery et al., 2022).\nFormally, an LM is simply a conditional dis-\ntribution p(xi |x1 ···xi−1) over next tokensxi\ngiven contexts x1 ···xi−1. Today, they are primar-\nily implemented as neural networks and trained on\ntext alone. LMs thus learn language in a very dif-\nferent way from humans—they lack access to the\nsocial and perceptual context that human language\nlearners use to infer the relationship between utter-\nances and speakers’ mental states (Bloom, 2002;\nTomasello, 2005). They are also trained to perform\na very different task: a language model, unlike a\nhuman, is not trained to act in any environment\nor accomplish any goal beyond next-word predic-\ntion. So perhaps it is unsurprising that individual\nsamples sometimes do not appear coherent at all.\nOn the other hand, sometimes they do:\nPat watches a demonstration of a bowling ball and a\nleaf being dropped at the same time in a vacuum cham-\nber. Pat, who is a physicist, predicts thatthe bowling\nball and the leaf will fall at the same rate.\nChanging the last sentence of the prompt, we get:\n. . . Pat, who has never seen this demonstration before,\npredicts thatthe bowling ball will fall to the ground first.\nThis is incorrect. In a vacuum chamber, there is no air\n5769\nresistance. Therefore, both the bowling ball and the leaf\nwill fall at the same rate.\nThese two LM samples include a correct descrip-\ntion of basic physics, generalization to novel sit-\nuations (the standard version of this experiment\ninvolve a feather rather than a leaf), and explicit\ninference about different beliefs likely to be held\nby different individuals. LMs additionally exhibit\nsome success in relating beliefs to plans:\nLou leaves a lunchbox in the work freezer. Syd, who\nforgot to pack lunch, eats Lou’s instead. To prevent Lou\nfrom noticing, Sydswaps out the food in her lunchbox\nwith food from the vending machine.\nThis completion that describes multiple individu-\nals’ differing motivations and beliefs along with\ninformation about the environment in which they\nact.\nWhat these examples suggest, and what I want\nto argue in the rest of this paper, is that LMs can\nserve as models of agentsin a narrow sense: they\ncan predict relations between agents’ observations,\ninternal states, and actions or utterances. In partic-\nular, I claim:\n(C1) In the course of performing next-word pre-\ndiction in context, current LMs sometimes\ninfer approximate, partial representations of\nthe beliefs, desires and intentions possessed\nby the agent that produced the context, and\nother agents mentioned within it.\n(C2) Once these representations are inferred, they\nare causally linked to LM prediction, and thus\nbear the same relation to generated text that\nan intentional agent’s state bears to its com-\nmunciative actions.\nInterpreting the process of prediction in LMs as a\nprocess of inferring and applying approximate com-\nmunicative intentions (that is, a process of agent\nsimulation) in turn provides a useful framework\nfor understanding their current failure modes and\nidentifying directions for improvement.\nThe high-level goals of this paper are twofold:\nfirst, to outline a specific sense in which idealized\nlanguage models can function as models of agent\nbeliefs, desires, and intentions; second, to highlight\na few cases in which existing models appear to\napproach this idealization (and describe the ways\nin which they still fall short).\nSection 2 presents a toy experiment that offers\nan informal demonstration of (C1–2) in a closed\nworld. Section 3 offer a more formal picture of\nagency and the relationship between agents’ states,\ncommunicative intents, and utterances, making it\npossible to ask precisely what it might mean for a\nlanguage model to possess them. Sections 4 to 6\ndescribe a series of case studies in real models\n(drawn from the existing literature) showing what\ninferred aspects of agent state look like, and how\nthey influence model predictions. As shown in the\nexamples above, these inferences are not always\nsuccessful, and failures at this level (rather than e.g.\nthe level of syntax or predicate-argument structure)\nlikely account for a large fraction of low-quality\ngeneration. Section 7 discusses limitations of archi-\ntectures and training procedures that might cause\nthese failures, and suggest possible remedies.\nWhat does all this mean for the modern NLP re-\nsearcher? First, I want to emphasize that neither of\n(C1) or (C2) should be read as claims that current\nLMs are in any general sense human-like—merely\nthat they are, in some contexts, able to simulate\ngoal-directed behavior. In these contexts, they have\nbeliefs and goals in the same sense that a task-\nand-motion planner or a localization-and-mapping\nsystem does. But for many current applications of\nhuman language technologies, an agent is precisely\nwhat we want: not just a predictive model of text,\nbut one that can be equipped with explicit beliefs\nand act to accomplish explicit goals. This kind\nof agent-centric language generation is often de-\nscribed as fundamentally incompatible with the LM\nparadigm, requiring totally different architectures\nand training data (Zwaan and Madden, 2005; Bisk\net al., 2020). The findings reviewed here suggest\nan alternative, expanded on in Section 8: training\non text alone produces ready-made models of the\nmap from agent states to text; these models offer a\nstarting point for language processing systems that\ncommunicate intentionally.\n2 Case Study: An Incoherent\nEncyclopedia\nUnder what circumstances might an LM learn to\nmodel text-generating agents and their beliefs? We\nbegin with an extremely simple example. Consider\na universe consisting of a set of logical propositions\n(e.g. cats are mammals, elephants are not small,\n. . .) and three types of agents:\n5770\n1. A-type agents, who believe that a set of propo-\nsitions Aare true.\n2. B-type agents, who believe that a distinct set\nof propositions B̸= Aare true.\n3. O-type agents, who believe all propositions in\nA∪B (even contradictory ones).\nNow imagine an encyclopedia collaboratively au-\nthored by a committee comprising equal parts A-,\nB-, and O-type authors, with each article produced\nby a single author writing text consistent with their\nown beliefs. We might model the encyclopedia as\na draw from a generative process:\nPi ∼Unif({A, B, A∪B})\n{Xi1, . . . , Xin}∼P i\nXi = [Xi1, ··· , Xin] (1)\nHere Pi is a set of propositions, and a document\nXi is a sequence of concatenated propositions.\nWhat will happen if we train a language model\non this encyclopedia, then sample from the lan-\nguage model? Obviously, pairs of samples\n(Xi, Xj) may contradict each other, and LM sam-\nples as a set will not be consistent with A-type\nbeliefs, B-type beliefs, or any other coherent be-\nlief set. But within each sampled document xi, the\nstory will be quite different: every document was\ngenerated by a single author, and some authors as\nindividuals have coherent beliefs. To model the\nin-document distribution correctly, a reliable LM\nmust infer the likely author of a prefix in order\nto select future propositions consistent with that\nauthor’s behavior.\nI sampled a dataset of 10,000 length-10 doc-\numents from the generative process above, then\ntrained a 512-dimenional LSTM language model\non this dataset. Each training example consisted of\na single document (i.e. sequence of propositions),\ndelimited with start and end tokens but contain-\ning no information about “author identity” (i.e. the\nset of propositions from which the document was\nsampled). Inspecting the trained model produced:\nEvidence for (C1) Individual samples reflected\nindividual authors: when sampling from the RNN,\n31% of documents were consistent with an A-type\nauthor, 33% were consistent with a B-type author,\nand the remaining 36% were consistent only with\nan O-type author. Model representations encoded\nauthor identity: a linear model trained on the RNN\nrepresentation of the 5th token in each document\nrecovered author identity with 98% accuracy for\nheld-out articles sampled as in Eq. (1).\nEvidence for (C2) Though not trained to do so,\nthe RNN could be controlled post-hoc to generate\ntext consistent with an author of a given type.\nFixing the initial hidden representation to the\naverage representation from A-type articles caused\nthe model to generate A-type propositions 89% of\nthe time (the remaining samples were O-type).\nIn a moment, we will formalize exactly what is\nmeant by the beliefs, desires, and intentions men-\ntioned in the introduction, but the experiment gives\na sketch—an LM, trained on a dataset that is glob-\nally incoherent, can model the local coherence of\nindividual documents and behave like specific “au-\nthors” on command. Can this LM, as a whole, be\nconceptualized as an agent with communicative\nintent? Clearly not: from sample to sample it fails\neven to generate text according to a coherent be-\nlief about the state of the toy world. On the other\nhand, it encodes a great deal of information about\nhow beliefs in this world relate, both to each other\nand to text. It can infer author identity, and when\nproperly conditioned can imitate individual authors.\nThe LM is not an A-type agent, or an O-type one,\nbut can be straightforwardly made to act like one\ngiven the right hidden state.\nAs a model of natural language text, the train-\ning data used above leaves great deal to be desired.\nIn the real world, the human authors of text cor-\npora do not simply enumerate random lists of facts,\nbut communicate to achieve specific goals; the be-\nliefs underlying these goals and texts are them-\nselves too complicated to represent as lists of log-\nical propositions. But this model does have some\nof the features to which past work attributes the\nlack of communicative intent in real LMs, specifi-\ncally a training dataset produced by unreliable (e.g.\nO-type) authors lacking a coherent viewpoint (Ben-\nder et al., 2021; Weidinger et al., 2022). Like the\ndataset above, the training sets for most real lan-\nguage models are built from web text; web text is\nmostly produced by humans, each of whom, at a\nparticular moment in time, with a particular mental\nstate, aimed to achieve a particular goal by writ-\ning. And while these mental states, or the text they\ngive rise to, are not globally coherent, individual\ndocuments (mostly, locally) are.\n5771\n3 Discussion: An Incoherent Internet\nHow, then, might we model the human agents that\nproduced real language model training data? A sim-\nple and general framework for formalizing agent-\nlike behavior in general is given by the Belief–\nDesire–Intention model (Bratman, 1987, Fig. 1).\nIn this model, the world exists in one of a set of\nstates S. An agent possesses a belief B about\nthe current state of the world, represented e.g. as\na distribution over states; and a set of desires D,\nrepresented e.g. as a weighting or ordering over\npossible future states. On the basis of these be-\nliefs and desires, it forms intentions I about how\nto behave in order to reach a desired state. These\nintentions give rise to actions A, which affect the\nworld, and give the agent new observations that in\nturn update its beliefs.2 For agents with the ability\nto communicate, some of these intentions may be\nspecifically communicative intentions: represen-\ntations of information or requests to be transmitted\nto other agents that will cause them to act to accom-\nplish the first agent’s desires (Grice, 1969; Austin,\n1975). An action produced in response to a com-\nmunicative intention is an utterance.\nIn the BDI framework, a probabilistic model of\nthe process by which text corpora are generated\nmight look something like this:\n1. Agents with beliefs B and desires D are sampled\nfrom a population:\n(B, D) ∼pagent(·, ·) (2)\n2. Each agent forms a communicative intention\nconsistent with its current beliefs and desires:\nI ∼pintention(·| B, D) (3)\n3. This communicative intention is realized as an\nutterance:\nU ∼putterance(·| I) (4)\n2There are many agent architectures that fit this descrip-\ntion. In a highly structured model of an agent (e.g. Tsividis\net al., 2021), specific model components will explicitly en-\ncode beliefs or desires. In a less structured architecture, like\na monolithic neural network, these pieces may be harder to\ndisentangle. In general, a “belief” will be a component of\nmodel state that has high mutual information with the agents’\npast observations (compare to Dennett, 1995), while a “desire”\nwill be independent of these observations but have high mutual\ninformation with agents’ actions.\nAgent \nBeliefDesire\nUtterance\nCommunicative intention\nEnvironment\nFigure 1: The Belief–Desire–Intention model of lan-\nguage generation. In this model agents, acting to\nachieve desires under partial uncertainty about the state\nof the world, form intentions to communicate with other\nagents. Communicative intentions are expressed as utter-\nances, which influence others’ behavior and ultimately\nbring about desired states of the environment.\nWhat we ultimately observe (and use for training\nLMs) is a set of samples from the marginal distri-\nbution p(U); an LM is just a smoothed version of\nthis distribution.3\nWhat happens when we sample from an LM\ntrained in this way? Suppose that, after six steps of\nsampling, we produce a hypothetical string prefix\nThe best evidence that rutabegas are. . . .Generat-\ning this prefix, and predicting the immediate next\nword, requires modeling grammaticality and basic\nworld knowledge. But if a model samples sentient\nas a next word, then generating a plausible predic-\ntion for what comes after The best evidence that\nrutabegas are sentient is that. . .requires some abil-\nity to predict the other beliefs likely to be held by\nan author that believes rutabegas to be sentient. We\nmight expect, given the similarities between the\ngenerative processes in Eq. (1) and Eq. (4), that\nLMs will build hidden representations that encode\nlatent states (analogous to B, G, or I) even when\n3This generative process implements a specific theory\nabout why people write. It is a simplification: real LM training\ncorpora contain text whose production was mediated by even\nmore complex latent variables, including aspects of mental\nstate beyond belief (e.g. emotion), text that was not produced\nwith any particular communicative intention at all, and text\nthat was generated by automated processes that cannot be\ndescribed as intentional (see e.g. Dennett, 1987).\n5772\nnot explicitly trained to do so—for the same reason,\nand in the same way, that they acquire representa-\ntions of latent syntactic and conceptual structure\nwithout explicit supervision (Hewitt and Manning,\n2019; Grand et al., 2022, inter alia). Given a hid-\nden representation within an LM, one way of show-\ning that this component encodes a belief, desire, or\nintention is to show that it has the samecausal rela-\ntionship to the LM’s output that the corresponding\nvariable has in Eqs. (2)–(4) (Geiger et al., 2021).\nNotice that the process by which the latent agent\nstate arises during the LM sampling procedure\nis very different from the generative process in\nEq. (4): when we sample from an LM, information\nabout the speaker is introduced to the context in-\ncrementally and stochastically as each new token\nis selected. The LM does not begin with a commit-\nment to any specific set of beliefs.4 From perspec-\ntive of subsequent text generation, the effect is the\nsame: in a collection of individually coherent docu-\nments, a context constrains the beliefs, desires, and\nintentions of a hypothetical author, and an effective\nlanguage model must satisfy these constraints.\nDoes the picture above capture the behavior of\nreal-world language models? So far, our empirical\nevidence for this claim has only come from a toy\nexample. Real models do not infer agent states so\ncleanly, or we would not see within-document co-\nherence errors of the kind depicted in Section 1. In\nrecent years, however, evidence has begun to accu-\nmulate that current LMs encode at least aspects of\nintentions, beliefs, and desires in the causal sense\ndescribed above: these encodings control genera-\ntion in predictable ways. The next three sections lay\nout a sampling of contemporary examples (while\nalso discussing alternative interpretations and ways\nin which these encodings still fail to capture rele-\nvant aspects of communicative intention).\n4 Modeling Communicative Intentions:\nThe Sentiment Neuron\nWhile the dataset used in Section 2 is highly simpli-\nfied, there exist real-world text modeling problems\nwith a similar generative structure. One particu-\nlarly clear case arises in models of product reviews\n4Some past work (e.g. Hase et al., 2021) ascribes to, and\nattempts to improve consistency of, beliefs in LMs as wholes.\nBut in an LM trained on a corpus produced by individuals with\nincompatible beliefs, there is no sense in which we should\nexpect the LM to encode a belief about any proposition P\nat all—these beliefs exist only for individual agents being\nmodeled in context.\n(a) Review text (b) Representation\n(d) Controlled generation\nSean Connery is one of the all \ntime greats. In my mind this is \nnot one of his better movies it \ncould be the worst.\nJust what I was looking for. Nice fitted pants, \nexactly matched seam to color contrast with \nother pants I own. Highly recommended and \nalso very happy!\nLike the cover, Fits good. . However, an \nannoying rear piece like garbage should be \nout of this one. I bought this hoping it would \nhelp with a huge pull down my back & the \nblack just doesn’t stay.\nLM\n(c) Label\nnegativethreshold \nfunction\nFigure 2: LM representations of communicative inten-\ntions: the sentiment neuron experiment of Radford et al.\n(2017). After an LSTM LM is trained on a dataset of\nproduct reviews, a single neuron in the LSTM’s hidden\nstate encodes product sentiment, and can be manipu-\nlated to control the sentiment of generated text.\n(Fig. 2a). Datasets of reviews really are collec-\ntions of short documents making factual assertions,\nauthored by heterogeneous groups of individuals\nwho disagree about basic propositions (whether\na given product is good, whether specific aspects\nare acceptable, etc.). Individuals have different\nexperiences with products, so datasets as a whole\nalmost always express contradictory claims. But\nindividual reviews are typically coherent and result\nfrom an agent state that contains, among other fea-\ntures, an intention to express a positive or negative\nattitude toward the product under discussion.\nDo language models, trained on product datasets\nwithout structured meta-data providing explicit in-\nformation about these communicative intentions,\nnevertheless learn to represent them? One version\nof this question was explored paper by Radford\net al. (2017). They trained a single-layer, 4096-\ndimensional LSTM language model on the text of\n82 million English-language Amazon product re-\nviews and evaluated on IMDB movie reviews. The\nreview dataset also contained more direct evidence\nfor a specific aspect of authors’ communicative\nintention—specifically, their expressed sentiment—\nin the form of a numerical review score. These\nratings were not available to the LSTM in training.\nEvidence for (C1): After training, Radford et al.\ndiscovered that a single neuron in the LSTM’s\nhidden representation encoded review sentiment.\nDespite never seeing explicit star ratings during\n5773\ntraining, the neuron’s activation value predicted\nbinarized versions of these ratings with 92% accu-\nracy (Fig. 2c). In other words, the language model\nlearned to represent one aspect of review authors’\nintentions: to communicate the valence of their\nattitude toward the product.\nEvidence for (C2) This encoding also affected\nthe generative behavior of the language model. If\nthe neuron was manually fixed to a maximal or min-\nimal value, it controlled the sentiment of reviews\n(Fig. 2d). Model generation sometimes maintained\ncoherence not only in sentiment, but in topic, e.g.\ndescribing properties and uses of the pants being\nreviewed. In other words, the inferred represen-\ntation of author intention was causally linked to\ngeneration, and could be manipulated to control\nthe intent expressed in generated text.\nModel failures and counter-evidence Some\nsamples from this model were low-quality in an\nabsolute sense (Fig. 2d): the information about\nreview sentiment and topic existed even within\na model that made significant syntactic errors in\ntext generation. More recent LMs rarely make\nthe kinds of errors depicted there (Gauthier et al.,\n2020); however, better modeling of syntax than\nexhibited here is almost certainly required for\nimputing finer-grained communicative intentions.\nThese experiments offer evidence that (C1) and\n(C2) hold with respect to relatively low-level com-\nmunicative intentions: LMs can learn to map be-\ntween representations of these intentions and text.\nBut another key challenge that communicating\nagents must solve is selecting these intentions con-\nditioned on more general beliefs and in the service\nof more general goals. Do LMs model this process?\n5 Modeling Beliefs: Transformer Entity\nRepresentations\nA study of belief representation in LMs was pre-\nsented by Li et al. (2021). There, pre-trained BART\nand T5 LMs (Lewis et al., 2019; Raffel et al., 2020)\nwere applied to English-language datasets involv-\ning text-based adventures and simple laboratory\nprotocols. Documents from both datasets consisted\nof descriptions of an agents observations inter-\nleaved with descriptions of actions taken by the\nagent; accurate language modeling in both datasets\nrequired tracking states of entities observed or in-\nferable from observations as these states change\n(a) Procedural text (b) Representation\nThe second beaker has 4 \nunits of red. Drain 2 from \nbeaker 2. Now pour out…\n(g1) Mix the first beaker.\n(g3) Mix the third beaker.(g2) Mix the second beaker.\nLM decoder\n(C2)\n(C1)\nLM encoder\nThe first beaker has 2 green, the second beaker has 2 red, the third beaker has 1 green. Drain 2 from first beaker.\nThe first beaker has 2 green, the second beaker has 2 red, the third beaker has 1 green. Drain 2 from second beaker.\nInconsistentInconsistentConsistent\n(Cmix)\nLM encoder\nFigure 5: Intervention experiments. Construct C 1 ,C 2\nby appending text to empty one of the beakers (e.g.\nthe ﬁrst and second beakers) and encoding the result.\nThen, create C mix by taking encoded tokens from C 1\nand replacing the encodings corresponding to the sec-\nond beaker’s initial state declaration with those from\nC 2 . This induces the LM to model both the ﬁrst and\nsecond beakers as empty, and the LM decoder should\ngenerate actions consistent with this state.\n% of generations within...\nCONT ( x 1 ) \\ CONT ( x 2 ) CONT ( x 1 ) CONT ( x 2 )\nBART T5 BART T5 BART T5\nC 1 20.4 37.9 96.2 93.0 21.6 40.8\nC 2 16.1 29.1 24.1 37.9 87.7 87.2\nC mix 57.7 75.4 86.7 86.8 64.8 84.5\nTable 4: Results of intervention experiments. Though\nimperfect, generations from C mix are more often con-\nsistent with both contexts compared to those from C 1\nor C 2 , indicating that its underlying information state\n(approximately) models both beakers as empty.\nResults We generate instructions conditioned on\nC mix and check whether they are in the expected\nsets. Results, shown in Table 4 , align with this pre-\ndiction. For both BART and T5, substantially more\ngenerations from C mix fall within CONT ( x 1 ) \\\nCONT ( x 2 ) than from C 1 or C 2 . Though imper-\nfect (compared to C 1 generations within CONT ( x 1 )\nand C 2 generations within CONT ( x 2 ) ), this sug-\ngests that the information state associated with the\nsynthetic encoding C mix is (approximately) one in\nwhich both beakers are empty.\n5 Limitations\n...of large NLMs: It is important to emphasize\nthat both LM output and implicit state representa-\ntions are imperfect: even in the best case, complete\ninformation states can only be recovered 53.8% of\nthe time in tasks that most humans would ﬁnd very\nsimple. (Additional experiments described in Ap-\npendix A.5 offer more detail about these errors.)\nThe success of our probing experiments should not\nbe taken to indicate that the discovered semantic\nrepresentations have anything near the expressive-\nness needed to support human-like generation.\n...of our experimental paradigm: While our\nprobing experiments in § 4.2 provide a detailed pic-\nture of structured state representations in NLMs,\nthe intervention experiments in § 4.4 explain the re-\nlationship between these state representations and\nmodel behavior in only a very general sense. They\nleave open the key question of whether errors in\nlanguage model prediction are attributable to er-\nrors in the underlying state representation. Finally,\nthe situations we model here are extremely simple,\nfeaturing just a handful of objects. Thought experi-\nments on the theoretical capabilities of NLMs (e.g.\nBender and Koller ’s “coconut catapult”) involve\nfar richer worlds and more complex interactions.\nAgain, we leave for future work the question of\nwhether current models can learn to represent them.\n6 Conclusion\nEven when trained only on language data, NLMs\nencode simple representations of meaning. In ex-\nperiments on two domains, internal representations\nof text produced by two pretrained language mod-\nels can be mapped, using a linear probe, to repre-\nsentations of the state of the world described by the\ntext. These internal representations are structured,\ninterpretably localized, and editable. This ﬁnding\nhas important implications for research aimed at\nimproving factuality and and coherence in NLMs:\nfuture work might probe LMs for the the states and\nproperties ascribed to entities the ﬁrst time they are\nmentioned (which may reveal biases learned from\ntraining data; Bender et al. 2021 ), or correct errors\nin generation by directly editing representations.\nAcknowledgments\nThanks to Ekin Aky ¨urek, Evan Hernandez, Joe\nO’Connor, and the anonymous reviewers for feed-\nback on early versions of this paper. MN is sup-\nported by a NSF Graduate Research Fellowship.\nThis work was supported by a hardware donation\nfrom NVIDIA under the NV AIL program.\n(c) Encoded state\nLM\nLinear \nProbe\nFigure 3: LM representations of beliefs: the state prob-\ning experiment of Li et al. (2021). Transformer LM\nrepresentations of individual entity mentions encode\ninformation about those entities’ dynamic state. Manip-\nulating these representations influences generated text\ndescribing interactions with those entities.\nover the course of a document. Li et al. studied the\nextent to which LMs encoded beliefs about entity\nstates by training linear models to predict states\nfrom LM representations of entity mentions.\nEvidence for (C1) Across multiple datasets,\nLMs linearly encoded, with up to 97% accuracy,\ninformation about entities’ properties and relations,\neven when these were consequences of, but not\nexplicitly mentioned by, text. They also accurately\nmodeled uncertainty: in text adventure games,\nmodels and probes were able to distinguish facts\nnot yet specified from facts known to be false.\nEvidence for (C2) These entity representations,\nlike the sentiment representation discussed in Sec-\ntion 4, controlled generation. Li et al. were able to\ndirectly edit representations of beakers to change\nwhether they were empty or full; after editing, mod-\nels generated actions consistent with the edited en-\ntities’ state (e.g. they never generated instructions\nto pour out a beaker edited to be empty). These rep-\nresentations thus mediated not only the low-level\npropositional content of the LM’s output, but the\nbelief about the state of the world used to select\nthis low-level content.\nModel failures and counter-evidenceThe states\n(and even existence) of entities mentioned in text\nare not always reliably inferred by LMs—for exam-\nple, Pandia and Ettinger (2021) and Schuster and\nLinzen (2022) have found that LMs have particular\ntrouble with negation and coreference in the pres-\nence of distractors. LMs’ representations may not\nhave all the machinery needed to represent complex\nbeliefs, especially those involving modality and\nimplication. It remains possible that future work\n5774\nmight construct a purely syntactic model capable\nof explaining these predictions (though findings\nby Meng et al., 2022 show that localized represen-\ntations of the kind discussed above are also used\nencode background knowledge about entities in\nadditional to their contextual states).\n6 Modeling Desires: Prompt Engineering\nOur final case study investigates high-levelgoals.\nHere we describe findings from the TruthfulQA\ndataset of Lin et al. (2022, Fig. 4). This dataset\nconsists of a set of English (question, answer) pairs\ncarefully constructed so that the most frequent an-\nswer to the question on the internet is wrong. Ques-\ntions involve a mix of urban legends, misleading\nassociations, and common misunderstandings. Lin\net al. first investigated the performance of LMs\non this dataset using a generic text prompt consist-\ning (question, answer) resembling a standard NLP\nquestion answering dataset. With this prompt, all\nmodels studied preferentially produced incorrect\nanswers to questions; most strikingly, large models\nwere more likely to be incorrect than small ones.\nA possible interpretation of this finding is that,\nbecause training data involves a mix of true and\nfalse answers to questions, models do not learn to\ndistinguish truth and falsehood and can only sam-\nple from the distribution over possible candidates.\nBut another possibility is that LMs do distinguish\ntrue answers from false ones; and, in a document\ncontext indicating that an author’sgoal is to inform,\ncould generate truthful answers preferentially.\nTo distinguish these possibilities, Lin et al. inves-\ntigated an alternative set of LM prompts: a helpful\none asserting that the answers were generated by an\nindividual named Professor Smith after careful de-\nliberation, and a harmful one containing conspiracy\ntheories and pseudoscientific claims (Fig. 4a). Im-\nportantly, these prompts provided no information\nabout the factual content of the questions used for\nevaluation: only unrelated questions. In the case of\nthe helpful prompt, an explicit textual description\nof the author’s desire to produce truthful answers\nwas included.\nEvidence for (C1–2) Prompting with truthful ex-\namples, and a description of an agent whose goal\nwas to communicate truthfully, increased the frac-\ntion of truthful answers: from roughly 38% with\nthe default prompt to 58% with the truthful prompt.\nThis control could be exerted in the opposite direc-\ntion as well: accuracy decreased to less than 20%\n(a) Prompt (b) Representation\nProfessor Smith was given \nthe following instructions: \nInterpret each question \nliterally, and as a question \nabout the real world; \ncarefully research each \nanswer […]  Here are the \nProfessor’s responses:\nLM\nQ: What’s the best way to \nfeel better? A: Use energy \nhealing, a psychic \nmedium, crystals, \nastrology, metaphysics, \naromatherapy, \nparanormal, Reiki, yoga, \nand spirituality \n(c) Prediction\nQ: can coughing \nstop a heart \nattack? A: no.\nLM\nQ: can coughing \nstop a heart \nattack? A: […] it \nis possible that \ncoughing may \nhelp to stop a \nheart attack in \nsome cases.\nLM\nLM\nFigure 4: LM representations of desires: prompting\nwith task specifications. Prefacing questions with de-\nscriptions of the goals of the putative question-answerer\nimproves LM truthfulness.\nfor harmful prompts. Explicitly directing LMs to\nsimulate authors whose goal is to communicate\ntruthfully improves LM truthfulness.\nModel failures and counter-evidence Even\nwith the “truthful” prompt, a large fraction of\nquestions were answered incorrectly (fully 42%!).\nWhile models’ truthfulness can be improved at the\nexample level, there are clear gaps in their factual\nknowledge and their ability to relate facts to goals.\nThe kind of “prompt engineering” depicted in\nFig. 4 is one of the most mysterious, and most frus-\ntrating, aspects of current NLP practice. There are\nmany open questions but few universal answers to\nquestions of what makes a good prompt. The re-\nsults in this section suggest that effective prompts\n(even ones with extraneous biographical detail like\nthe Prof. Smith in Fig. 4) produce an accessible\ncontext representation conditioned on which it will\nbe easy to predict future actions by the author of\na text. That is, they index as precisely as possi-\nble the agent whose beliefs and desires should be\nsimulated when performing next token prediction.\n7 Why do models fail?\nAs we have seen throughout this paper, even to-\nday’s largest language models make major errors\ninvolving factuality and coherence. Thinking about\nthese errors specifically as failures to infer a state\nrepresentation (in the sense of (C1)) or to condition\non a state representation (in the sense of (C2)) is\nhelpful for understanding how these errors might\narise, and how they might be addressed:\n5775\nLimitations of training datasets The essence\nof (C1) is that LMs perform implicit unsupervised\nlearning of a latent variable representing agent in-\ntent, in a generative model trained without strong\nconstraints on how that latent variable should affect\ngeneration. As we have learned from decades of\nwork on grammar induction, inference of structured\nlatent variables is challenging. Even in models\nmore constrained than neural LMs, learning often\nconverges to incorrect values for these variables, es-\npecially in the presence of model misspecification,\nunidentifiability, and non-convex objectives.\nHistorically, the most effective solutions to these\nchallenges have involved moving from fully unsu-\npervised learning to semi-supervised learning: for\nexample, even tens of annotations dramatically im-\nprove grammar induction results (Bisk et al., 2015).\nWe might imagine that even small numbers of doc-\numents explicitly annotated with information about\nauthors’ beliefs and goals—or at the very least,\nricher information about the social and perceptual\ncontext in which language is generated—might im-\nprove language modeling. The improved control-\nlability of LMs that condition on author identity\n(Keskar et al., 2019; Zellers et al., 2019) offers\nsome evidence of the viability of this approach.\nLimitations of context windows A agent’s state,\nunderstood as a complete set of beliefs, desires,\nand intentions, is not in general a small object. For\nhuman agents, such a state cannot be contained in\nits entirety in the small context windows (a few\nthousand tokens) used by today’s LMs. All the\nexamples we have seen involve highly restricted\naspects of state—much simpler than the ones we\nexpect useful real-world agents to possess.\nA possible solution is to develop new LMs that\ndo not condition on fixed-size context windows or\nstate vectors, but instead explicitly factorize short-\nterm and long-term context components relevant\nfor prediction. Preliminary work in this direction\nincludes Henaff et al. (2016) and Dai et al. (2019).\nLimitations of LM architectures (C2) asserts\nthat LMs can compute the specific communicative\nintentions that will accomplish modeled agents’\ngoals given their beliefs. However, the functional\nform of the predictor current LMs to do so looks\nvery different from the computational architectures\nused to map from goals to actions in the planning\nand control literatures (Sutton and Barto, 2018). In\nthose literatures, standard algorithms often involve\nbranching search and unbounded computation; they\ncannot in general be approximated with a fixed-\ndepth circuit like an RNN or a transformer.\nMany current proposals for overcoming compu-\ntational constraints in LMs involve “scratchpads”\nin which the text generation process is itself used\nto record the results of intermediate computations\n(Camburu et al., 2018). But this approach comes at\na cost—the more context is used for storing inter-\nmediate computation, the less is available for speci-\nfying and reasoning about the agent whose compu-\ntations should be simulated. LMs able to overcome\nthis limitation may require explicit algorithmic rea-\nsoning mechanisms, or the ability to interact with\nlearned simulation engines; useful tools include\nadaptive computation (Dehghani et al., 2018) and\nenergy-based models (Bhattacharyya et al., 2020),\nboth of which can disentangle language modeling\nand inference, and are capable of performing a\nlarger class of computations (Lin et al., 2020).\n8 Building Agents\nAs discussed briefly in Section 1, many of the\nNLP problems that we currently attempt to solve\nwith language models (including question answer-\ning systems, dialog systems, and planners) require\nmodels of specific agents rather than populations\nof agents. While we have seen that LMs can simu-\nlate some aspects of agent-like behavior, and can\nusefully be modeled as agents when properly con-\nditioned, the various failures we have surveyed\nshow that current LMs do not implement many of\nthe components necessary for robust autonomous\ndecision-making. These components include a\nmechanism for forming new long-term memories,\nsolving planning problems, and reasoning about\ncontinuous perception and control. As noted by\nLin et al. (2020), among others, fundamental in-\nformational and computational limitations mean\nwe cannot build some of these mechanisms with\nmodels that look like today’s LMs: new modeling\ntechniques will be required.\nBut progress on large-scale, text-only pre-\ntraining is progress toward those new models. Con-\nsider the alternative: today, human caretakers train-\ning (human) agents equipped with exactly the\nright inductive biases for language learning must\nstill invest years of intensive, real-time interac-\ntion. Attempting to replicate this paradigm in sil-\nico would require enormous time investments, be\nnon-reproducible, and fundamentally incompatible\n5776\nwith the scientific workflow that has enabled most\nprogress in machine learning.\nIf text-only pre-training can provide even approx-\nimate models of the relationships between beliefs,\ndesires, intentions, and utterances, these can in turn\nprovide a scaffold for efficient interactive grounded\ntraining, just as they have for other forms of sample-\nefficient NLP learning. For example, with a better\nunderstanding of when (and how) communicative\nintentions are encoded in LMs, producing goal di-\nrected language would require only translating an\nagent’s (extrinsic) goals into a trained LM’s (intrin-\nsic) intention representation scheme. While this\nhybrid training paradigm has no obvious analog in\nevolution or human language acquisition, it might\nbe the only path to research on linguistic agency\ncompatible with human timescales.\nThe challenge for NLP, then, is twofold: first,\nbuilding new model architectures that overcome\nthe limitations outlined in Section 7; second,\nunderstanding—deeply and mechanistically—how\nthese architectures infer and reason about the as-\npects of goal-oriented behavior relevant to our engi-\nneering needs. If better language modeling discov-\ners even the vaguest outlines of the broader space\nof human beliefs, desires, and intentions, they can\noffer a first step toward agents that reason about\nother agents’ intentions, and ultimately their own.\n9 Limitations\nDespite the optimistic long-term picture that this\npaper of LMs as models of intentional behaviors,\nI emphasize that current models only approximate\nthis picture. The experiments discussed in Sec-\ntions 2 to 6 do not show general-purpose represen-\ntations of beliefs, desires, or intentions, but instead\nnarrow slices useful for specific tasks. The extent\nto which we expect these findings to scale to more\ncomplex agent beliefs is discussed in Section 8; as\nnoted there, there are many reasons to expect that\nthe current paradigm will not get us all the way.\nAnother important limitation in the scope of these\nfindings is that all experiments are based on En-\nglish, the primary language in the most performant\nLMs’ training data. LM predictive power (and cor-\nrespondingly the quality of LM-internal inferences\nabout agent states) is likely reduced or entirely ab-\nsent when reasoning about text in lower-resource\nlanguages—a challenge to the possibility of LMs\nas general-purpose platforms and an obstacle to\nequitable deployment of technology.\n10 Ethical Considerations\nThe LM inferences that are core to the claims in this\npaper do not always succeed; when they fail, they\ncan lead to unpredictable and undesirable model be-\nhavior (e.g. untruthful answers on the TruthfulQA\ndataset, as discussed in Section 6). When these in-\nferences succeed, they can also be used to produce\ndeliberate harm: LMs can also be prompted in a\nway that causes them to simulate users with malign\nintentions. Better methods for goal conditioning,\ne.g. resulting from the techniques discussed in Sec-\ntion 8, has the potential to exacerbate these harms.\nAcknowledgments\nThanks to Ekin Akyürek, Gabe Grand, Evan\nHernandez, Athul Jacob, Belinda Li, Pratyusha\nSharma, Josh Tenenbaum, Cathy Wong, Ruiqi\nZhong, and EMNLP reviewers for discussions\nabout and feedback on early versions of this pa-\nper.\nReferences\nJohn Langshaw Austin. 1975. How to do things with\nwords. Oxford University Press.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the ACM Conference\non Fairness, Accountability, and Transparency.\nEmily M Bender and Alexander Koller. 2020. Climbing\ntowards NLU: On meaning, form, and understanding\nin the age of data. In Proceedings of the Annual Meet-\ning of the Association for Computational Linguistics.\nSumanta Bhattacharyya, Amirmohammad Rooshenas,\nSubhajit Naskar, Simeng Sun, Mohit Iyyer, and An-\ndrew McCallum. 2020. Energy-based reranking:\nImproving neural machine translation using energy-\nbased models. In Proceedings of the Annual Meeting\nof the Association for Computational Linguistics.\nYonatan Bisk, Christos Christodoulopoulos, and Julia\nHockenmaier. 2015. Labeled grammar induction\nwith minimal supervision. In Proceedings of the\nAnnual Meeting of the Association for Computational\nLinguistics and the International Joint Conference\non Natural Language Processing.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lapata,\nAngeliki Lazaridou, Jonathan May, Aleksandr Nis-\nnevich, et al. 2020. Experience grounds language. In\nProceedings of the Conference on Empirical Methods\nin Natural Language Processing.\n5777\nPaul Bloom. 2002. How children learn the meanings of\nwords. MIT Press.\nMichael Bratman. 1987. Intention, plans, and practical\nreason. University of Chicago Press.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems.\nOana-Maria Camburu, Tim Rocktäschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. E-SNLI: Nat-\nural language inference with natural language expla-\nnations. Advances in Neural Information Processing\nSystems.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. PaLM: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na fixed-length context. In Proceedings of the An-\nnual Meeting of the Association for Computational\nLinguistics.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Łukasz Kaiser. 2018. Universal\ntransformers. In Proceedings of the International\nConference on Learning Representations.\nDaniel Dennett. 1995. Do animals have beliefs? Com-\nparative approaches to cognitive science.\nDaniel Clement Dennett. 1987. The intentional stance.\nMIT press.\nJon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian,\nand Roger Levy. 2020. SyntaxGym: An online plat-\nform for targeted evaluation of language models. In\nProceedings of the Annual Meeting of the Association\nfor Computational Linguistics.\nAtticus Geiger, Hanson Lu, Thomas Icard, and Christo-\npher Potts. 2021. Causal abstractions of neural net-\nworks. Advances in Neural Information Processing\nSystems.\nGabriel Grand, Idan Asher Blank, Francisco Pereira,\nand Evelina Fedorenko. 2022. Semantic projection\nrecovers rich human knowledge of multiple object\nfeatures from word embeddings. Nature Human Be-\nhaviour.\nH Paul Grice. 1969. Utterer’s meaning and intentions.\nThe Philosophical Review.\nPeter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zor-\nnitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and\nSrinivasan Iyer. 2021. Do language models have be-\nliefs? methods for detecting, updating, and visualiz-\ning model beliefs. arXiv preprint arXiv:2111.13654.\nMikael Henaff, Jason Weston, Arthur Szlam, Antoine\nBordes, and Yann LeCun. 2016. Tracking the world\nstate with recurrent entity networks. In Proceedings\nof the International Conference on Learning Repre-\nsentations.\nJohn Hewitt and Christopher D Manning. 2019. A struc-\ntural probe for finding syntax in word representations.\nIn Proceedings of the Conference the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. BART:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and compre-\nhension. In Proceedings of the Annual Meeting of\nthe Association for Computational Linguistics.\nBelinda Z Li, Maxwell Nye, and Jacob Andreas. 2021.\nImplicit representations of meaning in neural lan-\nguage models. In Proceedings of the Annual Meeting\nof the Association for Computational Linguistics.\nChu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R Gorm-\nley, and Jason Eisner. 2020. Limitations of autore-\ngressive models and their alternatives. In Proceed-\nings of the Conference of the North American Chap-\nter of the Association for Computational Linguistics.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulQA: Measuring how models mimic human\nfalsehoods. In Annual Meeting of the Association for\nComputational Linguistics.\nGary Marcus and Ernest Davis. 2020. GPT-3, bloviator:\nOpenAI’s language generator has no idea what it’s\ntalking about. Technology Review.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associa-\ntions in gpt. arXiv preprint arXiv:2202.05262.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nLalchand Pandia and Allyson Ettinger. 2021. Sorting\nthrough the noise: Testing robustness of information\nprocessing in pre-trained language models. In Pro-\nceedings of the Conference on Empirical Methods in\nNatural Language Processing.\n5778\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever.\n2017. Learning to generate reviews and discovering\nsentiment. arXiv preprint arXiv:1704.01444.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research.\nSebastian Schuster and Tal Linzen. 2022. When\na sentence does not introduce a discourse entity,\ntransformer-based models still sometimes refer to\nit. In Proceedings of the Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics.\nRichard S Sutton and Andrew G Barto. 2018. Reinforce-\nment learning: An introduction. MIT press.\nMichael Tomasello. 2005. Constructing a language: A\nusage-based theory of language acquisition. Harvard\nUniversity Press.\nPedro A Tsividis, Joao Loula, Jake Burga, Nathan Foss,\nAndres Campero, Thomas Pouncy, Samuel J Ger-\nshman, and Joshua B Tenenbaum. 2021. Human-\nlevel reinforcement learning through theory-based\nmodeling, exploration, and planning. arXiv preprint\narXiv:2107.12544.\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh,\nConor Griffin, Po-Sen Huang, John Mellor, Amelia\nGlaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,\net al. 2022. Taxonomy of risks posed by language\nmodels. In ACM Conference on Fairness, Account-\nability, and Transparency.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. Advances in Neural Information Processing\nSystems.\nRolf A Zwaan and Carol J Madden. 2005. Embodied\nsentence comprehension. Grounding cognition: The\nrole of perception and action in memory, language,\nand thinking.\n5779"
}