{
  "title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models",
  "url": "https://openalex.org/W4392181806",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3116789147",
      "name": "Guan Zi-yi",
      "affiliations": [
        "Southern University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2363257634",
      "name": "Huang Hantao",
      "affiliations": [
        "Southern University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3202266219",
      "name": "Su Yupeng",
      "affiliations": [
        "Southern University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098749390",
      "name": "Huang Hong",
      "affiliations": [
        "Southern University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2438264125",
      "name": "Wong, Ngai",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2111034494",
      "name": "Yu Hao",
      "affiliations": [
        "Southern University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2798603777",
    "https://openalex.org/W3095194728",
    "https://openalex.org/W2989530497",
    "https://openalex.org/W4293166090",
    "https://openalex.org/W6969159101",
    "https://openalex.org/W4226278401"
  ],
  "abstract": "Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24\\% and 70.48\\% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating its effectiveness to produce high-quality quantized LLMs.",
  "full_text": "APTQ: Attention-aware Post-Training Mixed-Precision\nQuantization for Large Language Models\nZiyi Guan1,2âˆ—, Hantao Huang1âˆ—, Yupeng Su1, Hong Huang1, Ngai Wong2, Hao Yu1\nSchool of Microelectronics, Southern University of Science and Technology, Shen Zhen, China1\nDepartment of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong, China2\nABSTRACT\nLarge Language Models (LLMs) have greatly advanced the natural\nlanguage processing paradigm. However, the high computational\nload and huge model sizes pose a grand challenge for deployment on\nedge devices. To this end, we propose APTQ (Attention-aware Post-\nTraining Mixed-Precision Quantization) for LLMs, which considers\nnot only the second-order information of each layerâ€™s weights, but\nalso, for the first time, the nonlinear effect of attention outputs on\nthe entire model. We leverage the Hessian trace as a sensitivity\nmetric for mixed-precision quantization, ensuring an informed\nprecision reduction that retains model performance. Experiments\nshow APTQ surpasses previous quantization methods, achieving\nan average of 4 bit width a 5.22 perplexity nearly equivalent to full\nprecision in the C4 dataset. In addition, APTQ attains state-of-the-\nart zero-shot accuracy of 68.24% and 70.48% at an average bitwidth\nof 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating its\neffectiveness to produce high-quality quantized LLMs.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Natural language generation .\nKEYWORDS\nLarge Language Models, quantization, mixed-precision quantiza-\ntion, attention-based quantization, Hessian matrix sensitivity\nACM Reference Format:\nZiyi Guan1,2âˆ—, Hantao Huang1âˆ—, Yupeng Su1, Hong Huang1, Ngai Wong2,\nHao Yu1. 2024. APTQ: Attention-aware Post-Training Mixed-Precision\nQuantization for Large Language Models. In 61st ACM/IEEE Design Au-\ntomation Conference (DAC â€™24), June 23â€“27, 2024, San Francisco, CA, USA.\nACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3649329.3658498\n1 INTRODUCTION\nLarge Language Models (LLMs), such as ChatGPT [14], OPT [19],\nLLaMA [17], etc., exhibit impressive performance across various\ntasks. However, deploying these models on edge devices is challeng-\ning due to their exorbitant computational demands and memory\nâˆ—Equal contribution.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nDAC â€™24, June 23â€“27, 2024, San Francisco, CA, USA\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0601-1/24/06. . . $15.00\nhttps://doi.org/10.1145/3649329.3658498\nfootprints. Existing model compression solutions such as prun-\ning [1] and neural architecture search [2] often require model re-\ntraining, which is extremely time-consuming and expensive for\nbillion-parameter models. Recently, post-training quantization (PTQ)\nmethods, such as GPTQ [6], have been proposed and achieved rel-\natively high accuracy without retraining. However, GPTQ only\nconsiders the weight quantization strategy in the scope of a single\nlayer as an optimization problem to minimize ||ğ‘¾ğ‘¿ âˆ’Ë†ğ‘¾ğ‘¿ ||2\n2, with\nğ‘¾, Ë†ğ‘¾ and ğ‘¿ representing float weights, quantized weights and in-\nputs, respectively. This simplification fails to consider the complex\nand nonlinear effects such as softmax in the attention computation,\nand leads to a sub-optimal solution.\nTo achieve lower bitwidths without sacrificing the accuracy on\nedge devices, this paper presents an Attention-aware Post-Training\nMixed-Precision Quantization (APTQ) technique, which is designed\nto consider the quantization optimization problem within the scope\nof the attention block including the nonlinear softmax operation.\nSpecifically, APTQ utilizes gradients derived from the attention\noutput and develops a second-order Hessian optimization strategy\nto quantize the weights. By doing so, APTQ significantly reduces the\nquantization error in these crucial components, thereby preserving\nthe modelâ€™s integrity throughout compression.\nFurthermore, APTQ proposes a novel Hessian trace-based quan-\ntization sensitivity metric to implement mixed-precision quantiza-\ntion to further compress LLM models. This approach judiciously\napplies varying bitwidths across the model parameters to fit the\nlimited memory size on edge devices with balanced size and ac-\ncuracy. As a result, APTQ constitutes a mixed-precision 2/4-bit\nhybrid scheme with performance comparable to a uniform 4-bit\nrepresentation. In particular, APTQ produces a compressed model\nclose to its full-precision counterpart, and outperforming the GPTQ\nmethod especially in the realm of ultra-low-bit quantization scenar-\nios. Through comprehensive experiments on the LLaMA-7B and\nLLaMA-13B models [17], the effectiveness of APTQ is validated on\nboth perplexity and zero-shot performance, thus entailing a viable\nsolution for the deployment of LLMs on edge devices.\nThe main contributions of this paper are threefold:\nâ€¢This is the first work to quantize LLMs by integrating the\nattention-based gradients with second-order Hessian op-\ntimization, leading to a nuanced update mechanism that\nenhances the precision throughout the quantization process.\nâ€¢An innovative Hessian trace-driven mixed-precision quanti-\nzation scheme is proposed that judiciously allocates high/low\nbitwidths across different layers based on their sensitivity,\noptimizing model performance while maintaining efficiency.\nâ€¢Through extensive experimentation on the LLaMa models,\nAPTQ not only achieves state-of-the-art (SOTA) results on\narXiv:2402.14866v2  [cs.LG]  16 Apr 2024\nDAC â€™24, June 23â€“27, 2024, San Francisco, CA, USA Ziyi Guan et al.\nthe C4 dataset [15] but also attains near full-precision per-\nplexity at an average quantization of 4 bits. In zero-shot tasks,\nAPTQ also demonstrates superior performance compared to\nthe SOTA approaches.\n2 RELATED WORK\nTo deploy large models on edge devices, quantization is a versatile\ntechnique for reducing model size and computation. Quantization-\nAware Training (QAT) is known to be effective by integrating the\nquantization process into the training process. A representative\nwork is LLM-QAT [12], which proposes data-free distillation. How-\never, this method introduces new trainable parameters, necessitates\nhigh-end GPU computational resources, and incurs a large time con-\nsumption. In contrast, Post-Training Quantization (PTQ) employs\nmoderate resources to quantize pre-trained models without model\nretraining. Recent work, such as SpQR [ 3] and SqueezeLLM [ 8],\ncompress most weights to 4 bits but maintain outlier weights at 16\nbits, which complicates the inference process with both 4-bit and\n16-bit inference.\nSmoothQuant [18] introduces a per-channel scaling transforma-\ntion that effectively smooths the magnitudes to address the chal-\nlenge of quantizing activations. GPTQ [6] and OBQ [5] introduce\nan innovative weight quantization method based on approximate\nsecond-order information, ensuring high accuracy and efficiency\nin the quantization process. Our work shares the same ethos as\nGPTQ but additionally considers the softmax and matmul opera-\ntions within the attention computation to formulate the quantiza-\ntion problem, resulting in improved accuracy.\nMixed-precision quantization offers a trade-off strategy for edge\ndevices to maintain the accuracy with minimized model size. Exist-\ning works usually define some metrics to determine the quantiza-\ntion sensitivity of each layer. One representative work is HAWQ-V2\n[4], which adopts Hessian trace for CNN layer sensitivity assess-\nment and utilizes the Hutchinson algorithm to approximately esti-\nmate the Hessian trace. Our APTQ method also employs Hessian\ntrace for sensitivity but adopts the Levenberg-Marquardt approxi-\nmation [9] to directly calculate the Hessian trace with respect to\nthe attention output, which is also an extension of GPTQ [ 6] by\nfurther considering the nonlinear operation (softmax) and matmul\nin the attention output. Another close related work is PB-LLM [16],\nwhich adopts a mixed 1-bit and fp-16 (half floating point) precision\nbased on the Hessian values. Extreme low-bit quantization (1bit)\nis challenging for the accuracy. However, our APTQ method opts\nfor a 2-bit and 4-bit mixed-precision quantization offering a better\naccuracy with the same model size comparing to PB-LLM. The\neffectiveness of this strategy is demonstrated in Section 4, where\nour method shows superior performance in terms of efficiency and\nmodel compression when compared to PB-LLM.\n3 ALGORITHM\nThis section starts with the preliminaries to outline the evolution of\nquantization techniques from optimal brain quantization (OBQ) [5]\nto our proposed Hessian-attention-based quantization. We then\npropose an Attention-aware Post-Training Mixed-Precision Quan-\ntization, APTQ, to further compress the LLMs.\n3.1 Preliminaries\nGeneral Quantization Framework. Quantization aims to reduce\nweight precision in neural networks, thus conserving computa-\ntional resources. The general goal is to find a quantized weight\nmatrix Ë†ğ‘¾ that approximates full precision output, minimizing the\nsquared error. This process can be formally expressed as:\nargmin Ë†ğ‘¾||ğ‘¾ğ‘¿ âˆ’Ë†ğ‘¾ğ‘¿ ||2\n2. (1)\nIn this equation, ğ‘¿ represents the input to the layer, and Ë†ğ‘Š denotes\nthe quantized weight.\nOptimal Brain Quantization (OBQ). Optimal Brain Quantiza-\ntion (OBQ) [5] is an innovative method that minimizes quantiza-\ntion errors by treating each neural network weight independently.\nThe core of OBQ lies in iteratively quantizing each weight and\nadjusting the remaining unquantized weights to compensate for\nthe quantization-induced errors. This approach is mathematically\narticulated as follows:\nğ‘¤ğ‘ = argminğ‘¤ğ‘\nquant(ğ‘¤ğ‘)âˆ’ğ‘¤ğ‘\n[ğ»âˆ’1\nğ¹ ]ğ‘ğ‘\n, (2)\nğ›¿ğ¹ = âˆ’ğ‘¤ğ‘ âˆ’quant(ğ‘¤ğ‘)\n[ğ»âˆ’1\nğ¹ ]ğ‘ğ‘\nÂ·(ğ»âˆ’1\nğ¹ ):,ğ‘, (3)\nğ»âˆ’1\nâˆ’ğ‘ = (ğ»âˆ’1 âˆ’ 1\n[ğ»âˆ’1]ğ‘ğ‘\nğ»âˆ’1\n:,ğ‘„ğ»âˆ’1:\nğ‘ )âˆ’ğ‘. (4)\nThe Hessian matrix ğ»ğ¹ = 2ğ‘‹ğ¹ğ‘‹ğ‘‡\nğ¹ guides the selection of the quan-\ntization candidate ğ‘¤ğ‘ from the full-precision weights ğ¹, and the\nupdate ğ›¿ğ¹ is calculated to minimize quantization error, as formal-\nized in equations (2), (3) and (4) with quant(ğ‘¤)mapping weights\nto their nearest quantized values. Building upon OBQ, GPTQ [6]\nextends the principles by adopting the fixed order weights update\nstrategy and Cholesky reformulation to speed up the computation.\n3.2 Hessian-Attention-based Quantization\nWhile GPTQ effectively minimizes layer-specific quantization er-\nrors, it overlooks the intricate nonlinearities in attention mech-\nanisms, leading to suboptimality. APTQ, by contrast, embraces a\nholistic quantization strategy, factoring in the entire attention block\nand its nonlinear dynamics, which sharpens the precision of the\nquantized model, particularly in low-bitwidth scenarios.\nAs shown in Figure.1, we present the advanced architecture\nof APTQ, demonstrating its comprehensive quantization strategy.\nUnlike GPTQ, which primarily processes loss in the current layer,\nAPTQ integrates a full-scope analysis of the attention mechanism,\nincluding the ğ‘„, ğ¾, ğ‘‰, ğ‘‚matrices, matmul and nonlinear activation\nlayers such as softmax. This extensive approach not only focuses\non the intricacies beyond simple weight matrix multiplication, but\nalso significantly mitigates quantization errors, offering a robust\nsolution in low-bitwidth quantization scenarios.\nObjective Function . At a macroscopic level, our methodology\nemploys a layer-wise quantization approach to address the quan-\ntization reconstruction problem for each layerâ€™s weights. In the\nTransformer architecture, two main structural levels exist: the at-\ntention layers and the feed-forward layers. Specifically, in contrast\nto GPTQ, which treats each weight matrix as a linear layer and\nignores the impact of other structures on the output, we treat all\nstructures of the same layer as a whole, represented by the function\nAPTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models DAC â€™24, June 23â€“27, 2024, San Francisco, CA, USA\n``\nMixed\nPrecision\nWeight FC Q\nMixed\nPrecision\nWeight FC K\nMixed\nPrecision\nWeight FC V\nMatMul\nSoftMax\nMatMul\nInput\nOutput QWQ Output KWK Output VWV\nMulti-Head\nSelf-Attention\nGPTQ  (Loss in the current layer)\nQuantize W\n with Single \nFC Layer\nScaled Dot-\nProduct Attention\n-0.3 +1.2\n-2.1 +0.9\n+0.2 -0.5\n-4 +11\n-15 +7\n+2 -5\nW Quant W\nQuantize WQ\nwith Block \nMultiHead\nQuantize WV\nwith Block \nMultiHead\nOutput MultiHead(Q,K,V)\nMixed\nPrecision\nWeight FC O\nQuantization Loss considered by APTQ\nAPTQ (Loss after attention)\nHessian-Based Mixed Quantization\n0\n1\n2\n3\n4\n5\n6\n0 5 10 15 20 25 30\nAttn_Q_Weight\nAttn_V_Weight\nMLP_Weight\n+7 -3 +2\n-6 +4 0\n-7 0 +3\n-2 -6 +7\n+1 +3 0\n-7 0 +4\nUpdate W as Eq (2)\nH equal to 2XXT\n-0.3 +1.2\n-2.1 +0.9\n+0.2 -0.5\n-4 +12\n-14 +7\n+2 -6\nQuant WQ\nUpdate W as Eq (17)\nH refer to Eq (12)\nWQ\n-0.3 +1.2\n-2.1 +0.9\n+0.2 -0.5\n-3 +11\n-15 +8\n+3 -4\nQuant WV\nUpdate W as Eq (17)\nH refer to Eq (10)\nWV\n+2 -1 0\n0 +1 -1\n+1 0 -1\n+1 -1 +2\n+1 +2 -1\n0 0 -1\nBlock02: 4 bit quant\nBlock19: 2 bit quant\nBlock11: 4 bit quant\nBlock28: 2 bit quant\nAPTQ:60%\nBlock Number\nHessian\nValue\nFigure 1: Overall architecture of APTQ (Attention-aware Post-Training Mixed-Precision Quantization): Unifying comprehensive\ntransformer attention analysis with layer-specific Hessian trace quantization for enhanced model understanding.\nğ¹ standing for the attention output Multihead(ğ‘„,ğ¾,ğ‘‰ ). We aim\nto reformulate Equation (1) and minimize the new squared error\nequation as follows:\nargmin Ë†ğ‘Š||ğ¹(ğ‘Š)âˆ’ğ¹(Ë†ğ‘Š)||2\n2. (5)\nwhere ğ‘Š remains constant and Ë†ğ‘Š is the quantized weights to be\noptimized. The Hessian matrix of this function is computed as:\nğ» Ë†ğ‘Š = 2 Â·\n\u0010\nğ¹â€²(Ë†ğ‘Š)Â· ğ¹â€²(Ë†ğ‘Š)ğ‘‡ +[ğ¹(ğ‘Š)âˆ’ğ¹(Ë†ğ‘Š)]Â· ğ¹â€²â€²(Ë†ğ‘Š)\n\u0011\n. (6)\nThis is the general expression of Hessian matrix. To ensure ğ» Ë†ğ‘Š\nis positive definite and invertible, we only retain the first-order\nderivative portion as the expression for the Hessian matrix, which\nis widely known as the Levenberg-Marquardt approximation [9]:\nğ» Ë†ğ‘Š = 2 Â·[ğ¹â€²(Ë†ğ‘Š)Â· ğ¹â€²(Ë†ğ‘Š)ğ‘‡]. (7)\nDerivatives for Different Quantization Layers. The current\nproblem is transformed into finding the partial derivative of ğ¹(Ë†ğ‘Š)\nwith respect to the weights Ë†ğ‘Š. The ğ¹(Ë†ğ‘Š)function is different for\nthe Feed-Forward layers and Attention layers. In the Feed-Forward\nlayer, the main structure is a linear fully connected layer. The Hes-\nsian matrix is easily computed as ğ»ğ¹ = 2ğ‘‹ğ¹ğ‘‹ğ‘‡\nğ¹, corresponding to\nthe Hessian matrix form in the GPTQ method.\nIn the Attention layer, a multi-head mechanism is employed,\nwhere each attention head contains an Attention function:\nğ¹(ğ‘Š,ğ‘‹ )= MultiHead(ğ‘„,ğ¾,ğ‘‰ ). (8)\nThe quantized weight matrices lead to different derivatives. When\nquantizing the ğ‘Šğ‘‚ matrix, consider ğ‘Šğ‘„, ğ‘Šğ¾, ğ‘Šğ‘‰ as constants:\nğœ•ğ¹\nğœ•ğ‘Šğ‘‚ = Concat(head1,..., headH)ğ‘‡ ğœ•ğ¹\nğœ•ğ‘‹. (9)\nWhen quantizing the ğ‘Šğ‘‰ matrix, consider ğ‘Šğ‘„, ğ‘Šğ¾, ğ‘Šğ‘‚ as con-\nstants:\nğœ•ğ¹\nğœ•ğ‘Šğ‘‰ = ğ‘€ğ‘‡ ğœ•ğ¹\nğœ•ğ‘‹(ğ‘Šğ‘‚)ğ‘‡. (10)\nHere, ğ‘€ represents a matrix composed of ğ» heads losing ğ‘Šğ‘‰\nğ‘– :\nğ‘€â„ = softmax(\nğ‘„ğ‘Šğ‘„\nâ„ (ğ‘Šğ¾\nâ„ )ğ‘‡ğ¾ğ‘‡\nâˆšï¸\nğ‘‘ğ‘˜\n)ğ‘‰,ğ‘€ = [ğ‘€1,...,ğ‘€ ğ»]. (11)\nWhen quantizing ğ‘Šğ‘„ or ğ‘Šğ¾ matrices, consider the remaining\nthree terms as constants:\nğœ•ğ¹\nğœ•ğ‘Šğ‘„\nâ„\n= 1âˆšï¸\nğ‘‘ğ‘˜\nğ‘„ğ‘‡ ğœ•ğ¹\nğœ•ğ‘Pğ‘‡\nâ„ğ¾ğ‘Šğ¾\nâ„ , (12)\nğœ•ğ¹\nğœ•ğ‘Šğ¾\nâ„\n= 1âˆšï¸\nğ‘‘ğ‘˜\nğ¾ğ‘‡Pâ„\nğœ•ğ¹\nğœ•ğ‘\nğ‘‡\nğ‘„ğ‘Šğ‘„\nâ„ . (13)\nHere, ğ‘Šâ„ represents the weight matrix in the ğ‘›-th attention head,\nand ğ‘ and Pâ„ are given by:\nğ‘â„ =\nğ‘„ğ‘Šğ‘„\nâ„ (ğ‘Šğ¾\nâ„ )ğ‘‡ğ¾ğ‘‡\nâˆšï¸\nğ‘‘ğ‘˜\n, ğ‘= [ğ‘1,...,ğ‘ ğ»], (14)\nPâ„ = (...,,ğ¸ â„\nğ‘›Ã—ğ‘›,... )ğ‘›Ã—ğ‘›ğ». (15)\nAfter computing the gradients from equations (9), (10), (12) and (13),\nwe can further get their second order gradients using equation (7)\nto obtain the corresponding Hessian matrix. Thus, referring to the\noptimization problem in equation (5), combining the quantization\ntechniques in equations (2), (3), we derive the following formulas\nfor updating weights in the context of attention mechanisms:\nğ¸ = âˆ’ğ‘¤ğ‘ âˆ’quant(ğ‘¤ğ‘)\n([ğ»âˆ’1\nË†ğ‘Š ]ğ‘ğ‘) , (16)\nğ›¿ğ¹ = ğ¸Â·(ğ»âˆ’1\nË†ğ‘Š ):,ğ‘. (17)\nHere,ğ¸represents the quantization error,ğ‘¤ğ‘ refers to the quantized\nweights of the current group.ğ›¿ğ¹ refers to the corresponding optimal\nupdates for the remaining float weights (not yet quantized weights\nof the current layer). This principle is uniformly applicable to the\nquantization of ğ‘„(query), ğ¾(key), ğ‘‰ (value), and ğ‘‚(output) weight\nDAC â€™24, June 23â€“27, 2024, San Francisco, CA, USA Ziyi Guan et al.\nmatrices in attention mechanisms. By synthesizing these elements,\nwe can effectively compute the second-order Hessian information\nrelevant to the weights within the attention layers. This advanced\ncomputation aids in the update and optimization of weights, tar-\ngeting the minimization of the original squared error as defined in\nequation (5). This approach facilitates the realization of quantized\nmodels with robust performance across different components of\nthe attention mechanism. The comprehensive algorithm is detailed\nin Algorithm Box 1.\nAlgorithm 1 APTQ via Hessian-Attention-based Mixed-Precision\nQuantization\nInput: Pre-trained model weights ğ‘Š, blocksize ğµ, Hessian matrix ğ»,\nquantization function quant, Layer names ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘ğ‘ğ‘šğ‘’ , Ratio of 4-bit in\n2/4 mixed-precision ğ‘….\n1: Initialize quantized weight matrix ğ‘„ â†0ğ‘‘rowÃ—ğ‘‘col .\n2: Initialize block quantization error matrix ğ¸â†0ğ‘‘rowÃ—ğµ.\n3: Step 1: 4-bit Hessian-Attention-Based Quantization\n4: for ğ‘– = 0,ğµ, 2ğµ,... do\n5: for ğ‘— = ğ‘–,...,ğ‘– +ğµâˆ’1 do\n6: if â€œself_attn.k_projâ€ in layerName then\n7: ğ»ğ¾\nË†ğ‘Š = 2[ ğœ•ğ¹\nğœ•ğ‘Šğ¾ Â· ğœ•ğ¹\nğœ•ğ‘Šğ¾\nğ‘‡]from Equation (13)\n8: ğ‘„ğ¾\n:,ğ‘— â†quant(ğ‘Š:,ğ‘—)\n9: ğ¸ğ¾\n:,ğ‘—âˆ’ğ‘– â†(ğ‘Šğ¾\n:,ğ‘— âˆ’ğ‘„ğ¾\n:,ğ‘—)/[ğ»âˆ’1\nË†ğ‘Š ]ğ¾\nğ‘—ğ‘— based on Equation (16)\n10: ğ‘Šğ¾\n:,ğ‘—:(ğ‘–+ğµ) â†ğ‘Šğ¾\n:,ğ‘—:(ğ‘–+ğµ)âˆ’ğ¸ğ¾\n:,ğ‘—âˆ’ğ‘– Â·(ğ»âˆ’1\nË†ğ‘Š )ğ¾\n:,ğ‘—:(ğ‘–+ğµ)based on Equa-\ntion (17)\n11: For self_attn.Q, V, and O projection layers, similar updates are\napplied\n12: Compute the average Hessian trace for each layer in block\nğ‘– : (ğ‘–+ğµ).\n13: end if\n14: end for\n15: end for\n16: Step 2: Hessian-trace-based Mixed-Precision Quantization\n17: Calculate Hessian trace values for each layer, and order them from high-\nest to lowest, starting with the previously established 4-bit quantization.\n18: Determine the layers for mixed-precision quantization based on the\ncomputed Hessian trace values and ğ‘….\n19: for each selected layer do\n20: Calibrate the bit allocation in line with each layerâ€™s Hessian trace\nsensitivity and ğ‘….\n21: Implement 2/4 bit mixed-precision quantization\n22: end for\nOutput: The resulting quantized model weights ğ‘„are characterized by\nscale, zero-point, and quantization error.\n3.3 Hessian-Trace-based Mixed-Precision\nQuantization\nAs mentioned in Section 2, the Hessian trace provides sensitivity\ninformation for implementing mixed-precision quantization. Fig-\nure 1 illustrates the APTQ methodâ€™s allocation of 4-bit and 2-bit\nquantizations, utilizing average Hessian trace values as a measure\nof layer sensitivity. This approach diverges from the GPTQ method,\nwhich concentrates solely on the matrix multiplication within the\ncurrent layer, while APTQ provides a comprehensive assessment\nof each layerâ€™s impact.\nBy computing the average trace of the Hessian matrix, the method\ndetermines the appropriate level of precision for the quantization of\neach layer. Layers with higher Hessian Trace values, which exert a\ngreater influence on the networkâ€™s output, require higher bit preci-\nsion to ensure the modelâ€™s accuracy. Utilizing this mixed-precision\nquantization scheme results in models with an average bit precision\ndefined by the formula:\naverage bits = 4 Ã—ğ‘…+2 Ã—(1 âˆ’ğ‘…), (18)\nwhere ğ‘…denotes the proportion of weights quantized at 4 bits within\nthe overall quantization process. This formula is a pivotal aspect of\nthe APTQ methodology, facilitating a dynamic adjustment that is\nparticularly advantageous for deploying large language models on\nedge devices. The adaptability of ğ‘…allows the APTQ algorithm to\nallocate higher precision to layers with greater sensitivity, while\napplying more robust quantization to less sensitive layers. Conse-\nquently, this leads to a quantized model that achieves an optimal\nbalance between performance and size to deploy on edge devices.\nAlgorithm 1 unfolds into two decisive steps aimed at enhancing\nmodel efficiency while preserving performance. Step 1 applies 4-bit\nquantization to the attention mechanismâ€™s ğ¾ (key) layer, guided by\nthe Hessian matrix, ğ»ğ¾\nË†ğ‘Š, that entails the second-order derivative\ncrucial for this optimization, as formulated in Equation (13). This\nstep adjusts the precision of the ğ¾ layerâ€™s weights, considering the\nbroader implications for the modelâ€™s performance. The individual\noptimization of the ğ¾, ğ‘„, ğ‘‰, and ğ‘‚ layers is informed by their\nrespective Hessian matrices, ensuring that quantization is precisely\ntargeted to maintain the balance between efficiency and accuracy. In\nessence, Hessian-Attention-based quantization strategically refines\nweight precision within attention layers to maintain model accuracy\nwithout unnecessary computational burden.\nIn the algorithmâ€™s second phase, a mixed-precision quantization\nstrategy is implemented, beginning with the calculation of Hessian\ntrace values across the layers. These values are then ordered in a\ndescending sequence, starting with the layers previously quantized\nat a 4-bit level. This ordering informs the selection of layers for\nsubsequent mixed-precision quantization, which is performed in\naccordance with the computed Hessian trace values. This selective\nquantization process is designed to align closely with each layerâ€™s\nfunctional impact on the overall model, ensuring a quantization\nscheme that is both effective and efficient.\n4 EXPERIMENT\n4.1 Experiment Setup\nTo evaluate APTQâ€™s performance, we focus on two primary met-\nrics: perplexity and zero-shot performance. The LLaMa family [17]\nserves as the foundation for our experiments, owing to its effi-\ncacy and critical influence in recent model advancements. To main-\ntain consistency and comparability, our benchmarking procedures\nagainst GPTQ adhere to identical experimental configurations. Our\ncalibration dataset encompasses 128 segments, each containing\n2048 tokens randomly sampled from the C4 dataset. All experi-\nments deploy a group size of 128 and are executed on a single\nNVIDIA A100 GPU of 80GB memory. Our APTQ is applied directly\nAPTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models DAC â€™24, June 23â€“27, 2024, San Francisco, CA, USA\nTable 1: Comparison of Perplexity of Quantized LLaMa Mod-\nels on C4 and WikiText-2 Datasets.\nMethod Avg bit C4 â†“ WikiText-2 â†“\nLLaMa-7B 16 5.22 5.68\nGPTQ [6] 4.0 5.62 8.14\nOWQ [10] 4.01 5.56 7.15\nLLM-QAT [12] 4.0 7.40 10.90\nPB-LLM-20% [16] 3.4 20.61 17.19\nAPTQ 4.0 5.23 6.45\nAPTQ-75% 3.5 5.54 6.54\nAPTQ-50% 3.0 6.24 6.76\nto the pre-trained model (post-training quantization). The evalua-\ntion of zero-shot performance is conducted using the EleutherAI/lm-\nevaluation-harness [7]. Note that we use the format APTQ-R to\nrepresent the mixed precision (2/4-bit) setting, with ğ‘…represents\nthe percentage of 4-bit weights as discussed in Equation (18).\n4.2 Evaluation of Perplexity performance\nWe assess the the performance of APTQ using the C4 [ 15] and\nWikiText-2 [13] benchmarks. We compare APTQ against three\nestablished PTQ methods: GPTQ [6], OWQ [10], and PB-LLM [16].\nNotably, OWQ and PB-LLM extend upon GPTQ, with PB-LLM\nincorporating mixed-precision quantization. To ensure a balanced\ncomparison, all methods are evaluated on a standardized platform.\nMoreover, we benchmark APTQâ€™s performance with the leading\nQAT approach, LLM-QAT. Table 1 reveals that APTQ, at an average\n4 bit, closely matches the full-precision model and attains SOTA\nperformance on the C4 dataset, showing only a 0.01-point increase\nin perplexity. Remarkably, even with average bit rates reduced\nto 3.5 and 3.0, APTQâ€™s perplexity remains comparable to that of\nGPTQâ€™s 4-bit model. This evidence of APTQâ€™s stability at low bit\nrates positions it as a potent tool for optimizing the quantization\nand deployment of large-scale language models like LLaMa-7B.\nTo substantiate the robustness and broad applicability of the Hes-\nsian trace-based mixed-precision quantization posited in our study,\nwe conducted a comparative analysis of various 4-bit utilization\nlevels of APTQ against other prevalent PTQ methods applied to the\nLLaMa-7B model on the C4 dataset. The APTQ model, quantized at\nan average of 4 bit, not only approaches the full-precision modelâ€™s\nperplexity but also outperforms all other PTQ approaches at a re-\nduced precision of 3.5 bits. Impressively, configurations below 3\nbits still surpass the 4-bit LLM-QAT baseline, underscoring APTQâ€™s\nefficacy. These results unequivocally demonstrate the superior per-\nformance of APTQ, leveraging Hessian trace-driven precision allo-\ncation to optimize quantization outcomes.\nFigure 2 visually summarizes our findings. It presents the com-\nparative perplexity results of the LLaMa-7B model using APTQ at\nvarious bit utilization ratios when benchmarked against other PTQ\nand QAT methods on the C4 dataset. As depicted in the figure, the\nAPTQ model consistently maintains competitive performance, even\nat significantly reduced bit rates. This graphical representation rein-\nforces the effectiveness of the Hessian trace-based mixed-precision\napproach we advocate in this study, illustrating its potential for\nresource-efficient large model deployment.\n4.3 Evaluation of Zero-shot performance\nIn the evaluation of zero-shot performance, we extend our investiga-\ntion to a suite of challenging zero-shot language tasks. These tasks,\n7.3(2.8 bit)\n6.24(3.0 bit)\n5.99(3.2 bit)\n5.78(3.4 bit)\n5.54(3.5 bit)\n5.45(3.6 bit) 5.28(3.8 bit)\n5.23(4.0 bit)\n5\n5.2\n5.4\n5.6\n5.8\n6\n6.2\n6.4\n6.6\n6.8\n7\n7.2\n7.4\n40 50 60 70 80 90 100\nPerplexity\nRatio of 4 bit in 2/4-bit Mixed Precision Setting (%)\nAPTQ\nLLaMa-7B (FP16): 5.22\nOWQ-4bit: 5.56\nGPTQ-4bit: 5.62\nLLM-QAT-4bit: 7.40\nFigure 2: Comparative perplexity results of LLaMa-7B using\nAPTQ at various 4-bit ratio against others on C4 dataset\nwhich span Predictive Question Answering (PIQA), Hellaswag,\nARC-Easy (Arc-E), ARC-Challenge (Arc-C), and WinoGrande, serve\nas a benchmark for common sense reasoning in machine compre-\nhension. We compare the proposed APTQ method on LLaMa-7B\nand LLaMa-13B with other advanced quantization techniques in-\ncluding round-to-nearest (RTN), SmoothQuant [18], FPQ [11], LLM-\nQAT [12], and GPTQ [6].\nAs depicted in Table 2, we benchmark the APTQ framework\nagainst current SOTA PTQ methodologies applied to the LLaMa-7B\nmodel. Our findings illustrate that APTQ, when configured to 3.8\nbits, sustains a remarkably minimal deviation in accuracy, with\na diminutive average accuracy drop of only 0.32 points from the\nfull-precision model. Even when the APTQ is optimized down to\nan average of 3.6 or 3.5 bits, it still consistently outperforms the ma-\njority of 4-bit PTQ models. These findings demonstrate that APTQ\nexcels in zero-shot tasks with minimal bit usage, highlighting its\neffectiveness in deploying large-scale language models in envi-\nronments with limited computational resources. This underscores\nAPTQâ€™s advantage in resource-efficient performance.\n4.4 Ablation Study\nFurthermore, we present an ablation study to validate the superior-\nity of APTQ over manual block-wise quantization schemes. Given\nthat quantization is performed on a layer-wise basis, the most intu-\nitive mixed-precision quantization strategy is to uniformly quantize\nall layers within each block. Here, we compare this conventional\napproach with APTQ on the LLaMa-7B model tested on the C4\ndataset, with perplexity as the evaluation metric. The results in Ta-\nble 3 reveal APTQâ€™s efficacy over manual block-wise quantization\nfor LLaMa-7B on C4, reflected in its consistently lower PPL across\nvarious quantization ratios.\n5 CONCLUSION\nThis paper presented an Attention-aware Post-Training Mixed-\nPrecision Quantization (APTQ) algorithm for quantizing large lan-\nguage models to mixed precisions. APTQ is a promising post-\ntraining quantization strategy by utilizing the second-order infor-\nmation of each layerâ€™s weights with consideration of the nonlinear\nDAC â€™24, June 23â€“27, 2024, San Francisco, CA, USA Ziyi Guan et al.\nTable 2: Zero-shot accuracy of quantized LLaMa models on common sense reasoning tasks.\nModel LLaMa-7B LLaMa-13B\nMethod Avg bit PIQA Hellaswag Arc-E Arc-C WinoGrande ğ´ğ‘ğ‘% â†‘ PIQA Hellaswag Arc-E Arc-C WinoGrande ğ´ğ‘ğ‘% â†‘\nFP16 16 79.2 76.2 72.8 44.7 69.9 68.56 80.3 79.0 74.8 47.9 72.7 70.94\nRTN [12] 4.0 77.3 72.7 68.8 43.1 66.9 65.76 79.1 76.8 72.6 46.5 70.5 69.10\nSmoothQuant [18] 4.0 76.4 68.1 67.3 39.6 66.0 63.48 77.9 74.2 76.3 45.5 69.7 68.72\nFPQ [11] 4.0 77.8 75.0 72.4 41.7 69.0 66.60 79.4 77.7 72.8 47.3 71.5 69.74\nLLM-QAT [12] 4.0 78.3 74.0 70.0 41.7 69.0 66.60 79.4 77.7 72.8 47.3 71.5 69.74\nGPTQ [6] 4.0 76.0 69.4 66.9 43.0 66.7 64.40 79.8 77.7 73.2 45.9 72.6 69.84\nPB-LLM 30% [16] 4.1 78 74.3 69.0 42.3 69.7 66.66 - - - - - -\nPB-LLM 10% [16] 2.7 67.8 68.1 58.7 39.6 67.4 60.32 - - - - - -\nAPTQ 4.0 78.6 75.7 72.4 44.4 69.3 68.08 79.9 78.8 73.9 47.0 72.1 70.34\nAPTQ-90% 3.8 78.8 75.9 73.6 43.5 69.4 68.24 79.4 78.8 73.8 47.8 72.6 70.48\nAPTQ-80% 3.6 78.0 75.3 70.2 43.7 69.5 67.34 79.5 78.2 72.8 46.5 72.6 69.92\nAPTQ-75% 3.5 77.5 74.5 68.7 44.2 70.2 67.02 79.3 77.6 71.8 46.1 73.2 69.60\nAPTQ-70% 3.4 77.6 73.4 66.9 41.3 68.9 65.62 78.3 77.5 71.4 46.3 72.5 69.20\nAPTQ-60% 3.2 76.8 72.1 63.1 39.3 69.5 64.16 78.6 74.2 69.5 44.2 69.5 67.20\nAPTQ-50% 3.0 74.5 68.3 57.9 36.4 65.3 60.48 74.4 71.2 64.1 41.0 68.0 63.74\nTable 3: Ablation Study: Comparison of APTQ and Manual\nBlock-wise Quantization on LLaMa-7Bâ€™s C4 Perplexity\nMethod Ratio of 4-bit Avg bit Perplexity â†“\nManual Block-wise 75% 3.5 5.84\nAPTQ-75% 75% 3.5 5.54\nManual Block-wise 50% 3.0 7.04\nAPTQ-50% 50% 3.0 6.24\neffect of attention outputs. Furthermore, the Hessian trace is devel-\noped as a sensitivity measurement to further achieve mixed 2/4-bit\nprecision. For LLM LLaMa-7B, APTQ surpasses previous quantiza-\ntion methods, achieving an average of 4 bits with a 5.22 perplexity,\nnearly equivalent to full precision in the C4 dataset. Furthermore,\nunder the zero-shot LLM setting, APTQ achieves the state-of-the-\nart results 68.24% and 70.48% accuracy at an average bitwidth of 3.8\nfor LLaMA-7B and LLaMa-13B, respectively, indicating that APTQ\ncan achieve a deeply quantized solution for large language models\nwithout sacrificing accuracy.\n6 ACKNOWLEDGEMENT\nThis work was supported by Shenzhen Science and Technology\nProgram (Grant No. KQTD20200820113051096), Science and Tech-\nnology Innovation Committee Foundation of Shenzhen (Grant\nNo. JCYJ20220818100217038), and by the Theme-based Research\nScheme (TRS) project T45-701/22-R, Hong Kong SAR.\nREFERENCES\n[1] Miguel A Carreira-PerpinÃ¡n and Yerlan Idelbayev. 2018. â€œlearning-compressionâ€\nalgorithms for neural net pruning. In IEEE CVPR . 8532â€“8541.\n[2] Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. 2021. Progressive darts: Bridging the\noptimization gap for nas in the wild. IJCV 129 (2021), 638â€“655.\n[3] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias\nFrantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.\n2023. SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight\nCompression. arXiv preprint arXiv:2306.03078 (2023).\n[4] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney,\nand Kurt Keutzer. 2020. Hawq-v2: Hessian aware trace-weighted quantization of\nneural networks. NIPS 33 (2020), 18518â€“18529.\n[5] Elias Frantar and Dan Alistarh. 2022. Optimal brain compression: A framework\nfor accurate post-training quantization and pruning. NeurIPS 35 (2022), 4475â€“\n4488.\n[6] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ:\nAccurate Post-training Compression for Generative Pretrained Transformers.\nICLR (2023).\n[7] Leo Gao, Jonathan Tow, Stella Biderman, Charles Lovering, Jason Phang, Anish\nThite, Fazz, Niklas Muennighoff, and et al. 2022.EleutherAI/lm-evaluation-harness:\nv0.3.0. https://doi.org/10.5281/zenodo.7413426\n[8] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,\nMichael W Mahoney, and Kurt Keutzer. 2023. SqueezeLLM: Dense-and-Sparse\nQuantization. arXiv preprint arXiv:2306.07629 (2023).\n[9] Yann LeCun, John Denker, and Sara Solla. 1989. Optimal brain damage. NeurIPS\n2 (1989).\n[10] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. 2023.\nOWQ: Lessons learned from activation outliers for weight quantization in large\nlanguage models. arXiv preprint arXiv:2306.02272 (2023).\n[11] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, and Kwang-Ting\nCheng. 2023. LLM-FP4: 4-Bit Floating-Point Quantized Transformers. arXiv\npreprint arXiv:2310.16836 (2023).\n[12] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar\nMehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023.\nLLM-QAT: Data-Free Quantization Aware Training for Large Language Models.\narXiv preprint arXiv:2305.17888 (2023).\n[13] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016.\nPointer sentinel mixture models. arXiv preprint arXiv:1609.07843 (2016).\n[14] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.\nTraining language models to follow instructions with human feedback. NeurIPS\n35 (2022), 27730â€“27744.\n[15] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text transformer. JMLR 21, 1 (2020),\n5485â€“5551.\n[16] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. 2023. PB-LLM:\nPartially Binarized Large Language Models. arXiv preprint arXiv:2310.00034\n(2023).\n[17] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[18] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song\nHan. 2023. Smoothquant: Accurate and efficient post-training quantization for\nlarge language models. In ICML. 38087â€“38099.\n[19] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui\nChen, Christopher Dewan, Mona Diab, Xian Li, et al. 2022. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068 (2022).",
  "topic": "Quantization (signal processing)",
  "concepts": [
    {
      "name": "Quantization (signal processing)",
      "score": 0.6406156420707703
    },
    {
      "name": "Computer science",
      "score": 0.5560742616653442
    },
    {
      "name": "Language model",
      "score": 0.43604856729507446
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39154067635536194
    },
    {
      "name": "Psychology",
      "score": 0.37814822793006897
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3496814966201782
    },
    {
      "name": "Machine learning",
      "score": 0.3392508625984192
    },
    {
      "name": "Natural language processing",
      "score": 0.32404547929763794
    },
    {
      "name": "Algorithm",
      "score": 0.1932257115840912
    }
  ]
}