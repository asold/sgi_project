{
    "title": "Large language models approach expert-level clinical knowledge and reasoning in ophthalmology: A head-to-head cross-sectional study",
    "url": "https://openalex.org/W4385633601",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3080804641",
            "name": "Arun James Thirunavukarasu",
            "affiliations": [
                "University of Oxford",
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A4322225663",
            "name": "Shathar Mahmood",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A2261677931",
            "name": "Andrew Malem",
            "affiliations": []
        },
        {
            "id": null,
            "name": "William Paul Foster",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A2981255448",
            "name": "Rohan Sanghera",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A2635899234",
            "name": "Refaat Hassan",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A2103954715",
            "name": "Sean Zhou",
            "affiliations": [
                "West Suffolk NHS Foundation Trust"
            ]
        },
        {
            "id": "https://openalex.org/A2149920119",
            "name": "Shiao Wei Wong",
            "affiliations": [
                "Manchester Royal Eye Hospital",
                "Manchester University NHS Foundation Trust"
            ]
        },
        {
            "id": "https://openalex.org/A2239391237",
            "name": "Yee-Ling Wong",
            "affiliations": [
                "Manchester Royal Eye Hospital",
                "Manchester University NHS Foundation Trust"
            ]
        },
        {
            "id": "https://openalex.org/A2232737292",
            "name": "Yu Jeat Chong",
            "affiliations": [
                "Birmingham and Midland Eye Centre"
            ]
        },
        {
            "id": "https://openalex.org/A5109628756",
            "name": "Abdullah Shakeel",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A2981721495",
            "name": "Yin‐Hsi Chang",
            "affiliations": [
                "Chang Gung Memorial Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A3039616060",
            "name": "Benjamin Kye Jyn Tan",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A1974947386",
            "name": "Nikhil Jain",
            "affiliations": [
                "Luton and Dunstable University Hospital NHS Foundation Trust"
            ]
        },
        {
            "id": "https://openalex.org/A2481264266",
            "name": "Ting Fang Tan",
            "affiliations": [
                "Singapore National Eye Center",
                "Singapore Eye Research Institute"
            ]
        },
        {
            "id": "https://openalex.org/A1985247770",
            "name": "Saaeha Rauz",
            "affiliations": [
                "University of Birmingham",
                "Birmingham and Midland Eye Centre"
            ]
        },
        {
            "id": "https://openalex.org/A2619557440",
            "name": "Daniel Shu Wei Ting",
            "affiliations": [
                "Stanford University",
                "Smith-Kettlewell Eye Research Institute",
                "Singapore Eye Research Institute",
                "University of Nottingham",
                "Birmingham and Midland Eye Centre",
                "University of Birmingham",
                "Duke-NUS Medical School",
                "Singapore National Eye Center"
            ]
        },
        {
            "id": "https://openalex.org/A2619557440",
            "name": "Daniel Shu Wei Ting",
            "affiliations": [
                "Singapore National Eye Center",
                "Birmingham and Midland Eye Centre",
                "Duke-NUS Medical School",
                "Singapore Eye Research Institute",
                "Smith-Kettlewell Eye Research Institute",
                "University of Birmingham",
                "University of Nottingham",
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2794574940",
            "name": "Darren Shu Jeng Ting",
            "affiliations": [
                "Birmingham and Midland Eye Centre",
                "Duke-NUS Medical School",
                "University of Nottingham",
                "University of Birmingham",
                "Stanford University",
                "Singapore National Eye Center",
                "Smith-Kettlewell Eye Research Institute",
                "Singapore Eye Research Institute"
            ]
        },
        {
            "id": "https://openalex.org/A2794574940",
            "name": "Darren Shu Jeng Ting",
            "affiliations": [
                "Birmingham and Midland Eye Centre",
                "Singapore Eye Research Institute",
                "University of Birmingham",
                "University of Nottingham",
                "Singapore National Eye Center",
                "Stanford University",
                "Duke-NUS Medical School",
                "Smith-Kettlewell Eye Research Institute"
            ]
        },
        {
            "id": "https://openalex.org/A3080804641",
            "name": "Arun James Thirunavukarasu",
            "affiliations": [
                "University of Oxford",
                "Science Oxford",
                "Cambridge School"
            ]
        },
        {
            "id": "https://openalex.org/A4322225663",
            "name": "Shathar Mahmood",
            "affiliations": [
                "Cambridge School"
            ]
        },
        {
            "id": "https://openalex.org/A2261677931",
            "name": "Andrew Malem",
            "affiliations": [
                "Emirates Foundation"
            ]
        },
        {
            "id": null,
            "name": "William Paul Foster",
            "affiliations": [
                "University of Cambridge",
                "Cambridge School"
            ]
        },
        {
            "id": "https://openalex.org/A2981255448",
            "name": "Rohan Sanghera",
            "affiliations": [
                "Cambridge School"
            ]
        },
        {
            "id": "https://openalex.org/A2635899234",
            "name": "Refaat Hassan",
            "affiliations": [
                "Cambridge School"
            ]
        },
        {
            "id": "https://openalex.org/A2103954715",
            "name": "Sean Zhou",
            "affiliations": [
                "West Suffolk NHS Foundation Trust"
            ]
        },
        {
            "id": "https://openalex.org/A2149920119",
            "name": "Shiao Wei Wong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2239391237",
            "name": "Yee-Ling Wong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2232737292",
            "name": "Yu Jeat Chong",
            "affiliations": [
                "Birmingham and Midland Eye Centre",
                "Manchester Royal Eye Hospital",
                "Sandwell & West Birmingham Hospitals NHS Trust"
            ]
        },
        {
            "id": "https://openalex.org/A5109628756",
            "name": "Abdullah Shakeel",
            "affiliations": [
                "Cambridge School"
            ]
        },
        {
            "id": "https://openalex.org/A2981721495",
            "name": "Yin‐Hsi Chang",
            "affiliations": [
                "Chang Gung Memorial Hospital",
                "Linkou Chang Gung Memorial Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A3039616060",
            "name": "Benjamin Kye Jyn Tan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1974947386",
            "name": "Nikhil Jain",
            "affiliations": [
                "Luton and Dunstable University Hospital NHS Foundation Trust"
            ]
        },
        {
            "id": "https://openalex.org/A2481264266",
            "name": "Ting Fang Tan",
            "affiliations": [
                "Singapore Eye Research Institute",
                "Singapore National Eye Center"
            ]
        },
        {
            "id": "https://openalex.org/A1985247770",
            "name": "Saaeha Rauz",
            "affiliations": [
                "Birmingham and Midland Eye Centre",
                "University of Birmingham",
                "Sandwell & West Birmingham Hospitals NHS Trust"
            ]
        },
        {
            "id": "https://openalex.org/A2619557440",
            "name": "Daniel Shu Wei Ting",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2794574940",
            "name": "Darren Shu Jeng Ting",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4367310920",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4364378939",
        "https://openalex.org/W6850668563",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4382278861",
        "https://openalex.org/W4317910576",
        "https://openalex.org/W4362600559",
        "https://openalex.org/W4376114558",
        "https://openalex.org/W4367175039",
        "https://openalex.org/W2397288399",
        "https://openalex.org/W4294214983",
        "https://openalex.org/W2091254174",
        "https://openalex.org/W4367394076",
        "https://openalex.org/W4229015090",
        "https://openalex.org/W4383301639",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4378232928",
        "https://openalex.org/W4377011132",
        "https://openalex.org/W4220692188",
        "https://openalex.org/W4381309843",
        "https://openalex.org/W4362655426",
        "https://openalex.org/W3140124691",
        "https://openalex.org/W4381716591",
        "https://openalex.org/W4383346782"
    ],
    "abstract": "ABSTRACT Objective To evaluate the clinical potential of large language models (LLMs) in ophthalmology using a more robust benchmark than raw examination scores. Materials and methods GPT-3.5 and GPT-4 were trialled on 347 questions before GPT-3.5, GPT-4, PaLM 2, LLaMA, expert ophthalmologists, and doctors in training were trialled on a mock examination of 87 questions. Performance was analysed with respect to question subject and type (first order recall and higher order reasoning). Masked ophthalmologists graded the accuracy, relevance, and overall preference of GPT-3.5 and GPT-4 responses to the same questions. Results The performance of GPT-4 (69%) was superior to GPT-3.5 (48%), LLaMA (32%), and PaLM 2 (56%). GPT-4 compared favourably with expert ophthalmologists (median 76%, range 64-90%), ophthalmology trainees (median 59%, range 57-63%), and unspecialised junior doctors (median 43%, range 41-44%). Low agreement between LLMs and doctors reflected idiosyncratic differences in knowledge and reasoning with overall consistency across subjects and types ( p &gt;0.05). All ophthalmologists preferred GPT-4 responses over GPT-3.5 and rated the accuracy and relevance of GPT-4 as higher ( p &lt;0.05). Discussion In view of the comparable or superior performance to trainee-grade ophthalmologists and unspecialised junior doctors, state-of-the-art LLMs such as GPT-4 may provide useful medical advice and assistance where access to expert ophthalmologists is limited. Clinical benchmarks provide useful assays of LLM capabilities in healthcare before clinical trials can be designed and conducted. Conclusion LLMs are approaching expert-level knowledge and reasoning skills in ophthalmology. Further research is required to develop and validate clinical applications to improve eye health outcomes.",
    "full_text": "Title: Large language models approach expert-level clinical knowledge and \nreasoning in ophthalmology: A head-to-head cross-sectional study\nAuthors: Arun James Thirunavukarasu1,2,*, Shathar Mahmood1, Andrew Malem3, \nWilliam Paul Foster1,4, Rohan Sanghera1, Refaat Hassan1, Sean Zhou5, Shiao Wei \nWong6, Yee Ling Wong6, Yu Jeat Chong7, Abdullah Shakeel1, Yin-Hsi Chang8, \nBenjamin Kye Jyn Tan9, Nikhil Jain10, Ting Fang Tan11, Saaeha Rauz7,12, Daniel Shu \nWei Ting11,13,14, Darren Shu Jeng Ting7,12,15*\nAffiliations:\n1. University of Cambridge School of Clinical Medicine, Cambridge, United \nKingdom\n2. Oxford University Academic Graduate School, University of Oxford, Oxford, \nUnited Kingdom\n3. Eye Institute, Cleveland Clinic Abu Dhabi, Abu Dhabi Emirate, United Arab \nEmirates\n4. Department of Physiology, Development and Neuroscience, University of \nCambridge, Cambridge, United Kingdom\n5. West Suffolk NHS Foundation Trust, Bury St Edmunds, United Kingdom\n6. Manchester Royal Eye Hospital, Manchester University NHS Foundation \nTrust, Manchester, United Kingdom\n7. Birmingham and Midland Eye Centre, Sandwell and West Birmingham NHS \nFoundation Trust, Birmingham, United Kingdom\n8. Department of Ophthalmology, Chang Gung Memorial Hospital, Linkou \nMedical Center, Taoyuan, Taiwan\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n9. Yong Loo Lin School of Medicine, National University of Singapore, \nSingapore\n10.Bedfordshire Hospitals NHS Foundation Trust, Luton and Dunstable, United \nKingdom\n11.Singapore Eye Research Institute, Singapore National Eye Centre, Singapore, \nSingapore\n12.Academic Unit of Ophthalmology, Institute of Inflammation and Ageing, \nUniversity of Birmingham, Birmingham, United Kingdom\n13.Duke-NUS Medical School, Singapore, Singapore\n14.Byers Eye Institute, Stanford University, Palo Alto, California, United States of \nAmerica\n15.Academic Ophthalmology, School of Medicine, University of Nottingham, \nNottingham, United Kingdom\n*Correspondence details:\nDr Darren Ting: Academic Ophthalmology, School of Medicine, University of \nNottingham, Nottingham, United Kingdom; ting.darren@gmail.com\nDr Arun Thirunavukarasu: University of Cambridge School of Clinical Medicine, \nCambridge, United Kingdom; ajt205@cantab.ac.uk\nFunding: DSWT is supported by the National Medical Research Council, Singapore \n(NMCR/HSRG/0087/2018; MOH-000655-00; MOH-001014-00), Duke-NUS Medical \nSchool (Duke-NUS/RSF/2021/0018; 05/FY2020/EX/15-A58), and Agency for \nScience, Technology and Research (A20H4g2141; H20C6a0032). DSJT is \nsupported by a Medical Research Council / Fight for Sight Clinical Research \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nFellowship (MR/T001674/1). These funders were not involved in the conception, \nexecution, or reporting of this review.\nEthical approval: Ethical approval was not required for this study as human patients \nwere not involved in any experiments.\nCompeting interests: AM is a member of the Panel of Examiners of the Royal \nCollege of Ophthalmologists and performs unpaid work as an FRCOphth examiner. \nDSWT holds a patent on a deep learning system to detect retinal disease. DSJT \nauthored the book used in the study and receives royalty from its sales. The other \nauthors have no competing interests to declare.\nWord count: 3548\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nABSTRACT\nObjective\nTo evaluate the clinical potential of large language models (LLMs) in ophthalmology \nusing a more robust benchmark than raw examination scores.\nMaterials and methods\nGPT-3.5 and GPT-4 were trialled on 347 questions before GPT-3.5, GPT-4, PaLM 2, \nLLaMA, expert ophthalmologists, and doctors in training were trialled on a mock \nexamination of 87 questions. Performance was analysed with respect to question \nsubject and type (first order recall and higher order reasoning). Masked \nophthalmologists graded the accuracy, relevance, and overall preference of GPT-\n3.5 and GPT-4 responses to the same questions.\nResults\nThe performance of GPT-4 (69%) was superior to GPT-3.5 (48%), LLaMA (32%), \nand PaLM 2 (56%). GPT-4 compared favourably with expert ophthalmologists \n(median 76%, range 64-90%), ophthalmology trainees (median 59%, range 57-\n63%), and unspecialised junior doctors (median 43%, range 41-44%). Low \nagreement between LLMs and doctors reflected idiosyncratic differences in \nknowledge and reasoning with overall consistency across subjects and types \n(\np>0.05). All ophthalmologists preferred GPT-4 responses over GPT-3.5 and rated \nthe accuracy and relevance of GPT-4 as higher (p<0.05). \nDiscussion\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nIn view of the comparable or superior performance to trainee-grade \nophthalmologists and unspecialised junior doctors, state-of-the-art LLMs such as \nGPT-4 may provide useful medical advice and assistance where access to expert \nophthalmologists is limited. Clinical benchmarks provide useful assays of LLM \ncapabilities in healthcare before clinical trials can be designed and conducted.\nConclusion\nLLMs are approaching expert-level knowledge and reasoning skills in \nophthalmology. Further research is required to develop and validate clinical \napplications to improve eye health outcomes.\nKeywords\nChatGPT; large language model; natural language processing; decision support \ntechniques; artificial intelligence; AI; deep learning; ophthalmology; examination; \neye care; chatbot\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nINTRODUCTION\nGenerative Pre-trained Transformer 3.5 (GPT-3.5) and 4 (GPT-4) are large language \nmodels (LLMs) trained on datasets containing hundreds of billions of words from \narticles, books, and other internet sources.1,2 ChatGPT is an online chatbot which \nuses GPT-3.5 or GPT-4 to provide bespoke responses to human users’ queries.3 \nLLMs have revolutionised the field of natural language processing, and in recent \nmonths, ChatGPT has attracted significant attention in medicine for attaining \npassing level performance in medical school examinations and providing more \naccurate and empathetic messages than human doctors in response to patient \nqueries on a social media platform.3–6 While GPT-3.5 performance in more \nspecialised examinations has been inadequate, GPT-4 is thought to represent a \nsignificant advancement in terms of medical knowledge and reasoning.3,7,8 Other \nLLMs in wide use include Pathways Language Model 2 (PaLM 2) and Large \nLanguage Model Meta AI 2 (LLaMA 2).3,9,10\nApplications and trials of LLMs in ophthalmological settings has been limited \ndespite ChatGPT’s performance in questions relating to ‘eyes and vision’ being \nsuperior to other subjects in an examination for general practitioners.7,11 ChatGPT \nhas been trialled on the North American Ophthalmology Knowledge Assessment \nProgram (OKAP), and Fellowship of the Royal College of Ophthalmologists \n(FRCOphth) Part 1 and Part 2 examinations. In both cases, relatively poor results \nhave been reported for GPT-3.5, with significant improvement exhibited by GPT-\n4.12–16 However, previous studies are afflicted by two important issues which may \naffect their validity and interpretability. First, so-called ‘contamination’, where test \nmaterial features in the pretraining data used to develop LLMs, may result in inflated \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nperformance as models recall previously seen text rather than using clinical \nreasoning to provide an answer. Second, examination performance in and of itself \nprovides little information regarding the potential of models to contribute to clinical \npractice as a medical-assistance tool.3 Clinical benchmarks are required to \nunderstanding the meaning and implications of scores in ophthalmological \nexaminations attained by LLMs and are a necessary precursor to clinical trials of \nLLM-based interventions.\nHere, we used FRCOphth Part 2 examination questions to gauge the \nophthalmological knowledge base and reasoning capability of LLMs using fully \nqualified and currently training ophthalmologists as clinical benchmarks. These \nquestions were not freely available online, minimising the risk of contamination. The \nFRCOphth Part 2 Written Examination tests the clinical knowledge and skills of \nophthalmologists in training using multiple choice questions with no negative \nmarking and must be passed to fully qualify as a specialist eye doctor in the United \nKingdom.\nMETHODS\nQuestion extraction\nFRCOphth Part 2 questions were sourced from a textbook for doctors preparing to \ntake the examination.17 This textbook is not freely available on the internet, making \nthe possibility of its content being included in LLMs’ training datasets unlikely.1 All \n360 questions from the textbook’s six chapters were extracted. Two researchers \nmatched the subject categories of the practice papers’ questions to those defined \nin the Royal College of Ophthalmologists’ documentation concerning the FRCOphth \nPart 2 written examination. Similarly, two researchers categorised each question as \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nfirst order recall or higher order reasoning, corresponding to ‘remembering’ and \n‘applying’ or ‘analysing’ in Bloom’s taxonomy, respectively.18 Disagreement \nbetween classification decisions was resolved by a third researcher casting a \ndeciding vote. Questions containing non-plain text elements such as images were \nexcluded as these could not be inputted to the LLM applications.\nTrialling large language models\nEvery eligible question was inputted into ChatGPT (GPT-3.5 and GPT-4 versions; \nOpenAI, San Francisco, California, United States of America) between April 29 and \nMay 10, 2023. The answers provided by GPT-3.5 and GPT-4 were recorded and \ntheir whole reply to each question was recorded for further analysis. If ChatGPT \nfailed to provide a definitive answer, the question was re-trialled up to three times, \nafter which ChatGPT’s answer was recorded as ‘null’ if no answer was provided. \nCorrect answers (‘ground truth’) were defined as the answers provided by the \ntextbook and were recorded for every eligible question to facilitate calculation of \nperformance. Upon their release, Bard (Google LLC, Mountain View, California, \nUSA) and HuggingChat (Hugging Face, Inc., New York City, USA) were used to trial \nPaLM 2 (Google LLC) and LLaMA (Meta, Menlo Park, California, USA) respectively \non the portion of the textbook corresponding to a 90-question examination, \nadhering to the same procedures between June 20 and July 2, 2023.\nClinical benchmarks\nTo gauge the performance, accuracy, and relevance of LLM outputs, five expert \nophthalmologists who had all passed the FRCOphth Part 2 (E1-E5), three trainees \n(residents) currently in ophthalmology training programmes (T1-T3), and two \nunspecialised (\ni.e. not in ophthalmology training) junior doctors (J1-J2) first \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nanswered the 90-question mock examination independently, without reference to \ntextbooks, the internet, or LLMs’ recorded answers. As with the LLMs, doctors’ \nperformance was calculated with reference to the correct answers provided by the \ntextbook. After completing the examination, ophthalmologists graded the whole \noutput of GPT-3.5 and GPT-4 on a Likert scale from 1-5 (very bad, bad, neutral, \ngood, very good) to qualitatively appraise accuracy of information provided and \nrelevance of outputs to the question used as an input prompt. For these appraisals, \nophthalmologists were blind to the LLM source (which was presented in a \nrandomised order) and to their previous answers to the same questions, but they \ncould refer to the question text and correct answer and explanation provided by the \ntextbook. Procedures are comprehensively described in the protocol issued to the \nophthalmologists (Supplementary Material 2).\nAs our null hypothesis was that LLMs and doctors would exhibit similar \nperformance, prospective power analysis was conducted which indicated that 63 \nquestions were required to identify a 10% superior performance of an LLM to \nhuman performance at a 5% significance level (type 1 error rate) with 80% power \n(20% type 2 error rate). This indicated that the 90-question examination in our \nexperiments was more than sufficient to detect ~10% differences in overall \nperformance. The whole 90-question mock examination was used to avoid over- or \nunder-sampling certain question types with respect to actual FRCOphth papers. To \nverify that the mock examination was representative of the FRCOphth Part 2 \nexamination, expert ophthalmologists were asked to rate the difficulty of questions \nused here in comparison to official examinations on a 5-point Likert scale (“much \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \neasier”, “somewhat easier”, “similar”, “somewhat more difficult”, “much more \ndifficult”).\nStatistical analysis\nPerformance of doctors and LLMs were compared using chi-squared (χ2) tests. \nAgreement between answers provided by doctors and LLMs was quantified through \ncalculation of Kappa statistics, interpreted in accordance with McHugh’s \nrecommendations.19 To further explore the strengths and weaknesses of the answer \nproviders, performance was stratified by question type (first order fact recall or \nhigher order reasoning) and subject using a chi-squared or Fisher’s exact test \nwhere appropriate. Likert scale data corresponding to the accuracy and relevance \nof GPT-3.5 and GPT-4 responses to the same questions were analysed with paired \nt-tests with the Bonferroni correction applied to mitigate the risk of false positive \nresults due to multiple-testing—parametric testing was justified by a sufficient \nsample size.20 Statistical significance was concluded where p < 0.05. For additional \ncontextualisation, examination statistics corresponding to FRCOphth Part 2 written \nexaminations taken between July 2017 and December 2022 were collected from \nRoyal College of Ophthalmologists examiners’ reports.21 These statistics facilitated \ncomparisons between human and LLM performance in the mock examination with \nthe performance of actual candidates in recent examinations.\nStatistical analysis was conducted in R (version 4.1.2; R Foundation for Statistical \nComputing, Vienna, Austria), and figures were produced in Affinity Designer (version \n1.10.6; Serif Ltd, West Bridgford, Nottinghamshire, United Kingdom).\nRESULTS\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nQuestions sources\nOf 360 questions in the textbook, 347 questions (including 87 of the 90 questions \nfrom the mock examination chapter) were included.17 Exclusions were all due to \nnon-text elements such as images and tables which could not be inputted into LLM \nchatbot interfaces. The distribution of question types and subjects within the whole \nset and mock examination set of questions is summarised in Table 1 and Table S1 \nalongside performance.\nGPT-4 represents a significant advance on GPT-3.5 in ophthalmological knowledge \nand reasoning\nOverall performance over 347 questions was significantly higher for GPT-4 (61.7%) \nthan GPT-3.5 (48.41%; χ2=12.32, p<0.01), with results detailed in Figure S1 and \nTable S1. ChatGPT performance was consistent across question types and \nsubjects (Table S1). For GPT-4, no significant variation was observed with respect \nto first order and higher order questions (χ2 = 0.22, p=0.64), or subjects defined by \nthe Royal College of Ophthalmologists (Fisher’s exact test over 2000 iterations, p = \n0.23). Similar results were observed for GPT-3.5 with respect to first and second \norder questions (χ2 = 0.08, \np = 0.77), and subjects (Fisher’s exact test over 2000 \niterations, p = 0.28). Performance and variation within the 87-question mock \nexamination was very similar to the overall performance over 347 questions, and \nsubsequent experiments were therefore restricted to that representative set of \nquestions.\nGPT-4 compares well with other LLMs, junior and trainee doctors and \nophthalmology experts\nPerformance in the mock examination is summarised in Figure 1—GPT-4 (69%) was \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nthe top-scoring model, performing to a significantly higher standard than GPT-3.5 \n(48%; χ2 = 7.33, p < 0.01) and LLaMA (32%; χ2 = 22.77, p < 0.01), but statistically \nsimilarly to PaLM 2 (56%) despite a superior score (χ2 = 2.81, p = 0.09). LLaMA \nexhibited the lowest examination score, significantly weaker than GPT-3.5 (χ2 = \n4.58, p = 0.03) and PaLM-2 (χ2 = 10.01, p < 0.01) as well as GPT-4.\nFigure 1: FRCOphth Part 2 performance of LLMs and doctors of variable expertise. \nExamination performance in the 87-question mock examination used to trial LLMs \n(GPT-3.5, GPT-4, LLaMA, and PaLM 2), expert ophthalmologists (E1-E5), \nophthalmology trainees (T1-T3), and unspecialised junior doctors (J1-J2). Dotted \nlines depict the mean performance of expert ophthalmologists (66/87; 76%), \nophthalmology trainees (60/87; 69%), and unspecialised junior doctors (37/87; \n43%). The performance of GPT-4 lay within the range of expert ophthalmologists \nand ophthalmology trainees.\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nThe performance of GPT-4 was statistically similar to the mean score attained by \nexpert ophthalmologists (Figure 1; χ2 = 1.18, p = 0.28). Moreover, GPT-4’s \nperformance exceeded the mean mark attained across FRCOphth Part 2 written \nexamination candidates between 2017-2022 (66.06%), mean pass mark according \nto standard setting (61.31%), and the mean official mark required to pass the \nexamination after adjustment (63.75%), as detailed in Table S2. In individual \ncomparisons with expert ophthalmologists, GPT-4 was equivalent in 3 cases (χ2 \ntests, \np > 0.05’ Table S3), and inferior in 2 cases (χ2 tests, p < 0.05; Table 2). In \ncomparisons with ophthalmology trainees, GPT-4 was equivalent to all three \nophthalmology trainees (χ2 tests, p > 0.05; Table 2). GPT-4 was significantly superior \nto both unspecialised trainee doctors (χ2 tests, p < 0.05; Table 2). Doctors were \nanonymised in analysis, but their ophthalmological experience is summarised in \nTable S3. Unsurprisingly, junior doctors (J1-J2) attained lower scores than expert \nophthalmologists (E1-E5; t = 7.18, p < 0.01), and ophthalmology trainees (T1-T3; t = \n11.18, p < 0.01), illustrated in Figure 1. Ophthalmology trainees approached expert-\nlevel scores with no significant difference between the groups (t = 1.55, p = 0.18). \nNone of the other LLMs matched any of the expert ophthalmologists, mean mark of \nreal examination candidates, or FRCOphth Part 2 pass mark.\nExpert ophthalmologists agreed that the mock examination was a faithful \nrepresentation of actual FRCOphth Part 2 Written Examination papers with a mean \nand median score of 3/5 (range 2-4/5).\nLLM strengths and weaknesses are similar to doctors\nAgreement between answers given by LLMs, expert ophthalmologists, and trainee \ndoctors was generally absent (0 ≤ κ < 0.2), minimal (0.2 ≤ κ < 0.4), or weak (0.4 ≤ κ \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \n< 0.6), with moderate agreement only recorded for one pairing between the two \nhighest performing ophthalmologists (Figure 2; κ = 0.64).19 Disagreement was \nprimarily the result of general differences in knowledge and reasoning ability, \nillustrated by strong negative correlation between Kappa statistic (quantifying \nagreement) and difference in examination performance (Pearson’s r = -0.63, \np < \n0.01). Answer providers with more similar scores exhibited greater agreement \noverall irrespective of their category (LLM, expert ophthalmologist, ophthalmology \ntrainee, or junior doctor).\nFigure 2: Heat map of Kappa statistics quantifying agreement between answers \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \ngiven by LLMs, expert ophthalmologists, and trainee doctors. Agreement correlates \nstrongly with overall performance and stratification analysis found no particular \nquestion type or subject was associated with better performance of LLMs or \ndoctors, indicating that LLM knowledge and reasoning ability is general across \nophthalmology rather than restricted to particular subspecialties or question types.\nStratification analysis was undertaken to identify any specific strengths and \nweaknesses of LLMs with respect to expert ophthalmologists and trainee doctors \n(Table 1, Table S4). No significant difference between performance in first order fact \nrecall and higher order reasoning questions was observed among any of the LLMs, \nexpert ophthalmologists, ophthalmology trainees, or unspecialised junior doctors \n(Table S4; χ2 tests, \np > 0.05). Similarly, only J1 (junior doctor yet to commence \nophthalmology training) exhibited statistically significant variation in performance \nbetween subjects (Table S4; Fisher’s exact tests over 2000 iterations, p = 0.02); all \nother doctors and LLMs exhibited no significant variation (Fisher’s exact tests over \n2000 iterations, p > 0.05). To explore whether consistency was due to an insufficient \nsample size, similar analyses were run for GPT-3.5 and GPT-4 performance over the \nlarger set of 347 questions (Table S1; Table S4). As with the mock examination, no \nsignificant differences in performance across question types (Table S4; χ2 tests, p > \n0.05) or subjects (Table S4; Fisher’s exact tests over 2000 iterations, p > 0.05) were \nobserved.\nLLM examination performance translates to subjective preference indicated by \nexpert ophthalmologists\nOphthalmologists’ appraisal of GPT-4 and GPT-3.5 outputs indicated a marked \npreference for the former over the latter, mirroring objective performance in the \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nmock examination and over the whole textbook. GPT-4 exhibited significantly (t-test \nwith Bonferroni correction, p < 0.05) higher accuracy and relevance than GPT-3.5 \naccording to all five ophthalmologists’ grading (Table 3). Differences were visually \nobvious, with GPT-4 exhibiting much higher rates of attaining the highest scores for \naccuracy and relevance than GPT-3.5 (Figure 3). This superiority was reflected in \nophthalmologists’ qualitative preference indications: GPT-4 responses were \npreferred to GPT-3.5 responses by every ophthalmologist with statistically \nsignificant skew in favour of GPT-4 (χ2 test, \np < 0.05; Table 3).\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nFigure 3: Accuracy and relevance of GPT-3.5 and GPT-4 in response to \nophthalmological questions. Accuracy (A)  and relevance (B) ratings were provided \nby five expert ophthalmologists for ChatGPT (powered by GPT-3.5 and GPT-4) \nresponses to 87 FRCOphth Part 2 mock examination questions. In every case, the \naccuracy and relevance of GPT-4 is significantly superior to GPT-3.5 (t-test with \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nBonferroni correct applied, p < 0.05). Pooled scores for accuracy (C) and relevance \n(D) from all five raters are presented in the bottom two plots, with GPT-3.5 (left bars) \ncompared directly with GPT-4 (right bars).\nDISCUSSION\nHere, we present a clinical benchmark to gauge the ophthalmological performance \nof LLMs, using a source of questions with very low risk of contamination as the \nutilised textbook is not freely available online.17 Previous studies have suggested \nthat ChatGPT can provide useful responses to ophthalmological queries, but often \nuse online question sources which may have featured in LLMs’ pretraining \ndatasets.7,12,15,22 In addition, our employment of multiple LLMs as well as fully \nqualified and training doctors provides novel insight into the potential and \nlimitations of state-of-the-art LLMs through head-to-head comparisons which \nprovide clinical context and quantitative benchmarks of competence in \nophthalmology. Subsequent research may leverage our questions and results to \ngauge the performance of new LLMs and applications as they emerge.\nWe make three primary observations. First, performance of GPT-4 compares well to \nexpert ophthalmologists and ophthalmology trainees, and exhibits pass-worthy \nperformance in an FRCOphth Part 2 mock examination. PaLM 2 did not attain pass-\nworthy performance or match expert ophthalmologists’ scores but was within the \nspread of trainee doctors’ performance. LLMs are approaching human expert-level \nknowledge and reasoning in ophthalmology, and significantly exceed the ability of \nnon-specialist clinicians (represented here by unspecialised junior doctors) to \nanswer ophthalmology questions.\n Second, clinician grading of model outputs \nsuggests that GPT-4 exhibits improved accuracy and relevance when compared \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nwith GPT-3.5. Development is producing models which generate better outputs to \nophthalmological queries in the opinion of expert human clinicians, which suggests \nthat models are becoming more capable of providing useful assistance in clinical \nsettings. Third, LLM performance was consistent across question subjects and \ntypes, distributed similarly to human performance, and exhibited comparable \nagreement between other LLMs and doctors when corrected for differences in \noverall performance. Together, this indicates that the ophthalmological knowledge \nand reasoning capability of LLMs is general rather than limited to certain \nsubspecialties or tasks. LLM-driven natural language processing seems to facilitate \nsimilar—although idiosyncratic—clinical knowledge and reasoning to human \nclinicians, with no obvious blind spots precluding clinical use.\nSimilarly dramatic improvements in the performance of GPT-4 relative to GPT-3.5 \nhave been reported in the context of the North American Ophthalmology \nKnowledge Assessment Program (OKAP).13,15 State-of-the-art models exhibit far \nmore clinical promise than their predecessors, and expectations and development \nshould be tailored accordingly. Results from the OKAP also suggest that \nimprovement in performance is due to GPT-4 being more well-rounded than GPT-\n3.5.13 This increases the scope for potential applications of LLMs in ophthalmology, \nas development is eliminating weaknesses rather than optimising in narrow \ndomains. This study shows that well-rounded LLM performance compares well with \nexpert ophthalmologists, providing clinically relevant evidence that LLMs may be \nused to provide medical advice and assistance. Further improvement is expected \nas multimodal foundation models, perhaps based on LLMs such as GPT-4, emerge \nand facilitate compatibility with image-rich ophthalmological data.3,23,24\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nLimitations\nThis study was limited by three factors. First, examination performance is an \nunvalidated indicator of clinical aptitude. We sought to ameliorate this limitation by \nemploying expert ophthalmologists, ophthalmology trainees, and unspecialised \njunior doctors answering the same questions as clinical benchmarks; and compared \nLLM performance to real cohorts of candidates in recent FRCOphth examinations. \nHowever, it remains an issue that comparable performance to clinical experts in an \nexamination does not necessarily demonstrate that an LLM can communicate with \npatients and practitioners or contribute to clinical decision making accurately and \nsafely. Early trials of LLM chatbots have suggested that LLM responses may be \nequivalent or even superior to human doctors in terms of accuracy and empathy, \nand experiments using complicated case studies suggest that LLMs operate well \neven outside typical presentations and more common medical conditions.4,25,26 In \nophthalmology, GPT-3.5 and GPT-4 have been shown to be capable of providing \nprecise and suitable triage decisions when queried with eye-related symptoms.22,27 \nFurther work is now warranted in conventional clinical settings.\nSecond, while the study was sufficiently powered to detect a less than 10% \ndifference in overall performance, the relatively small number of questions in certain \ncategories used for stratification analysis may mask significant differences in \nperformance. Testing LLMs and clinicians with more questions may help establish \nwhere LLMs exhibit greater or lesser ability in ophthalmology. Furthermore, \nresearchers using different ways to categorise questions may be able to identify \nspecific strengths and weaknesses of LLMs and doctors which could help guide \ndesign of clinical LLM interventions.\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nFinally, experimental tasks were ‘zero-shot’ in that LLMs were not provided with any \nexamples of correctly answered questions before it was queried with FRCOphth \nquestions from the textbook. This mode of interrogation entails the maximal level of \ndifficulty for LLMs, so it is conceivable that the ophthalmological knowledge and \nreasoning encoded within these models is actually even greater than indicated by \nresults here.1 Future research may seek to fine-tune LLMs by using more domain-\nspecific text during pretraining and fine-tuning, or by providing examples of \nsuccessfully completed tasks to further improve performance in that clinical task.3\nFuture directions\nAutonomous deployment of LLMs is currently precluded by inaccuracy and fact \nfabrication. Our study found that despite meeting expert standards, state-of-the-art \nLLMs such as GPT-4 do not match top-performing ophthalmologists.28 Moreover, \nthere remain controversial ethical questions about what roles should and should not \nbe assigned to inanimate AI models, and to what extent human clinicians must \nremain responsible for their patients.3 However, the remarkable performance of \nGPT-4 in ophthalmology examination questions suggests that LLMs may be able to \nprovide useful input in clinical contexts, either to assist clinicians in their day-to-day \nwork or with their education or preparation for examinations.3,13,14,27 GPT-4 may \nprove especially useful where access to ophthalmologists is limited: provision of \nadvice, diagnosis, and management suggestions by a model with FRCOphth Part 2-\nlevel knowledge and reasoning ability is likely to be superior to non-specialist \ndoctors and allied healthcare professionals working without support, as their \nexposure to and knowledge of eye care is limited.27,29\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nHowever, close monitoring is essential to avoid mistakes caused by inaccuracy or \nfact fabrication.30 Clinical applications would also benefit from an uncertainty \nindicator reducing the risk of erroneous decisions.7 As LLM performance often \ncorrelates with the frequency of query terms’ representation in the model’s training \ndataset, a simple indicator of ‘familiarity’ could be engineered by calculating the \nrelative frequency of query term representation in the training data.7,31 Users could \nappraise familiarity to temper their confidence in answers provided by the LLM, \nperhaps reducing error. Moreover, ophthalmological applications require extensive \nvalidation, preferably with high quality randomised controlled trials to conclusively \ndemonstrate benefit (or lack thereof) conferred to patients by LLM interventions. \nTrials should be pragmatic so as not to inflate effect sizes beyond what may \ngeneralise to patients once interventions are implemented at scale.3,32,33 In addition \nto patient outcomes, practitioner-related variables should also be considered: \ninterventions aiming to improve efficiency should be specifically tested to ensure \nthat they reduce rather than increase clinicians’ workload.3\nConclusion\nAccording to comparisons with expert and trainee doctors, state-of-the-art LLMs \nare approaching expert-level performance in advanced ophthalmology questions. \nGPT-4 attains pass-worthy performance in FRCOphth Part 2 questions and \nexceeds the scores of some expert ophthalmologists. As top-performing doctors \nexhibit superior scores, LLMs do not appear capable of replacing ophthalmologists, \nbut state-of-the-art models could provide useful advice and assistance to non-\nspecialists or patients where access to eye care professionals is limited.27,28 Further \nresearch is required to design LLM-based interventions which may improve eye \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nhealth outcomes, validate interventions in clinical trials, and engineer governance \nstructures to regulate LLM applications as they begin to be deployed in clinical \nsettings.34\nAbbreviations\nAI: artificial intelligence\nFRCOphth: Fellowship of the Royal College of Ophthalmologists\nGPT: Generative Pretrained Transformer\nLLaMA: Large Language Model Meta AI\nLLM: large language model\nOKAP: Ophthalmology Knowledge Assessment Program\nPaLM: Pathways Language Model\nFunding\nThere were no funders which supported this project.\nEthical approval\nEthical approval was not required for this study, as human patients were not \ninvolved.\nAuthor contributions\nAJT and DSJT conceived the study and coordinated the team. AJT, DSJT, and YLW \ncontributed to study design. AJT, WF, RH, SM, and RS undertook LLM data \ncollection. YLW, SWW, AM, YJC, SZ, AS, YHC, BKJT, NJ, and TFT answered \nquestions independently to benchmark LLM performance. YLW, SWW, AM, YJC, \nand SZ appraised LLM outputs. AJT conducted data analysis and visualisation. AJT \nand DSJT drafted the manuscript. All authors participated in manuscript redrafting. \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nDSJT provided academic supervision. All authors approved the submitted version of \nthe manuscript.\nAcknowledgements\nThe authors extend their thanks to Mr Arunachalam Thirunavukarasu (Betsi \nCadwaladr University Health Board) for his advice and assistance with recruitment.\nReferences\n1. Brown, T. et al. Language Models are Few-Shot Learners. in Advances in Neural \nInformation Processing Systems vol. 33 1877–1901 (Curran Associates, Inc., \n2020).\n2. OpenAI. GPT-4 Technical Report. Preprint at \nhttps://doi.org/10.48550/arXiv.2303.08774 (2023).\n3. Thirunavukarasu, A. J. et al. Large language models in medicine. Nat Med 1–11 \n(2023) doi:10.1038/s41591-023-02448-8.\n4. Ayers, J. W. et al. Comparing Physician and Artificial Intelligence Chatbot \nResponses to Patient Questions Posted to a Public Social Media Forum. JAMA \nInternal Medicine (2023) doi:10.1001/jamainternmed.2023.1838.\n5. Gilson, A. et al. How Does ChatGPT Perform on the United States Medical \nLicensing Examination? The Implications of Large Language Models for Medical \nEducation and Knowledge Assessment. JMIR Med Educ 9, e45312 (2023).\n6. Kung, T. H. et al. Performance of ChatGPT on USMLE: Potential for AI-assisted \nmedical education using large language models. PLOS Digital Health 2, \ne0000198 (2023).\n7. Thirunavukarasu, A. J. et al. Trialling a Large Language Model (ChatGPT) in \nGeneral Practice With the Applied Knowledge Test: Observational Study \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nDemonstrating Opportunities and Limitations in Primary Care. JMIR Medical \nEducation 9, e46599 (2023).\n8. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of \nGPT-4 on Medical Challenge Problems. Preprint at \nhttp://arxiv.org/abs/2303.13375 (2023).\n9. Google. PaLM 2 Technical Report. Preprint at \nhttps://ai.google/static/documents/palm2techreport.pdf (2023).\n10. Touvron, H., Martin, L. & Stone, K. Llama 2: Open Foundation and Fine-Tuned \nChat Models. Preprint at https://ai.meta.com/research/publications/llama-2-\nopen-foundation-and-fine-tuned-chat-models/ (2023).\n11. Ting, D. S. J., Tan, T. F. & Ting, D. S. W. ChatGPT in ophthalmology: the dawn \nof a new era? Eye (Lond) (2023) doi:10.1038/s41433-023-02619-4.\n12. Antaki, F., Touma, S., Milad, D., El-Khoury, J. & Duval, R. Evaluating the \nPerformance of ChatGPT in Ophthalmology: An Analysis of its Successes and \nShortcomings. \nOphthalmology Science 0, (2023).\n13. Teebagy, S., Colwell, L., Wood, E., Yaghy, A. & Faustina, M. Improved \nPerformance of ChatGPT-4 on the OKAP Exam: A Comparative Study with \nChatGPT-3.5. 2023.04.03.23287957 Preprint at \nhttps://doi.org/10.1101/2023.04.03.23287957 (2023).\n14. Raimondi, R., Tzoumas, N., Salisbury, T., Di Simplicio, S. & Romano, M. R. \nComparative analysis of large language models in the Royal College of \nOphthalmologists fellowship exams. \nEye 1–4 (2023) doi:10.1038/s41433-023-\n02563-3.\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \n15. Mihalache, A., Popovic, M. M. & Muni, R. H. Performance of an Artificial \nIntelligence Chatbot in Ophthalmic Knowledge Assessment. JAMA \nOphthalmology 141, 589–597 (2023).\n16. Thirunavukarasu, A. J. ChatGPT cannot pass FRCOphth examinations: \nimplications for ophthalmology and large language model artificial intelligence. \nEye News vol. 30 (2023).\n17. Ting, D. S. J. & Steel, D. MCQs for FRCOphth Part 2. (Oxford University Press, \n2020).\n18. Adams, N. E. Bloom’s taxonomy of cognitive learning objectives. J Med Libr \nAssoc 103, 152–153 (2015).\n19. McHugh, M. L. Interrater reliability: the kappa statistic. Biochemia Medica 22, \n276–282 (2012).\n20. Sullivan, G. M. & Artino, A. R. Analyzing and Interpreting Data From Likert-Type \nScales. J Grad Med Educ 5, 541–542 (2013).\n21. Part 2 Written FRCOphth Exam. The Royal College of Ophthalmologists \nhttps://www.rcophth.ac.uk/examinations/rcophth-exams/part-2-written-\nfrcophth-exam/.\n22. Tsui, J. C. et al. Appropriateness of ophthalmic symptoms triage by a popular \nonline artificial intelligence chatbot. Eye 1–2 (2023) doi:10.1038/s41433-023-\n02556-2.\n23. Nath, S., Marie, A., Ellershaw, S., Korot, E. & Keane, P. A. New meaning for NLP: \nthe trials and tribulations of natural language processing with GPT-3 in \nophthalmology. \nBritish Journal of Ophthalmology 106, 889–892 (2022).\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \n24. Kline, A. et al. Multimodal machine learning in precision health: A scoping \nreview. npj Digit. Med. 5, 1–14 (2022).\n25. Kulkarni, P. A. & Singh, H. Artificial Intelligence in Clinical Diagnosis: \nOpportunities, Challenges, and Hype. JAMA (2023) \ndoi:10.1001/jama.2023.11440.\n26. Singhal, K. et al. Large language models encode clinical knowledge. Nature 1–9 \n(2023) doi:10.1038/s41586-023-06291-2.\n27. Waisberg, E. et al. GPT-4 for triaging ophthalmic symptoms. Eye 1–2 (2023) \ndoi:10.1038/s41433-023-02595-9.\n28. Thirunavukarasu, A. J. Large language models will not replace healthcare \nprofessionals: curbing popular fears and hype. J R Soc Med 116, 181–182 \n(2023).\n29. Alsaedi, M. G., Alhujaili, H. O., Fairaq, G. S., Alwdaan, S. A. & Alwadan, R. A. \nEmergent Ophthalmic Disease Knowledge among Non-Ophthalmologist \nHealthcare Professionals in the Western Region of Saudi Arabia: Cross-\nSectional Study. The Open Ophthalmology Journal 16, (2022).\n30. Bakken, S. AI in health: keeping the human in the loop. Journal of the American \nMedical Informatics Association 30, 1225–1226 (2023).\n31. Biderman, S. et al. Pythia: A Suite for Analyzing Large Language Models Across \nTraining and Scaling. Preprint at https://doi.org/10.48550/arXiv.2304.01373 \n(2023).\n32. Tossaint-Schoenmakers, R., Versluis, A., Chavannes, N., Talboom-Kamp, E. & \nKasteleyn, M. The Challenge of Integrating eHealth Into Health Care: Systematic \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \nLiterature Review of the Donabedian Model of Structure, Process, and \nOutcome. J Med Internet Res 23, e27180 (2021).\n33. Thirunavukarasu, A. J., Hassan, R., Limonard, A. & Savant, S. V. Accuracy and \nreliability of self-administered visual acuity tests: Systematic review of pragmatic \ntrials. PLOS ONE 18, e0281847 (2023).\n34. Meskó, B. & Topol, E. J. The imperative for regulatory oversight of large \nlanguage models (or generative AI) in healthcare. npj Digit. Med. 6, 1–6 (2023).\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 6, 2023. ; https://doi.org/10.1101/2023.07.31.23293474doi: medRxiv preprint "
}