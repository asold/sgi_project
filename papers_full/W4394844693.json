{
    "title": "Biomedical text readability after hypernym substitution with fine-tuned large language models",
    "url": "https://openalex.org/W4394844693",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2196946318",
            "name": "Karl Swanson",
            "affiliations": [
                "University of California, San Francisco"
            ]
        },
        {
            "id": "https://openalex.org/A2116925039",
            "name": "Shuhan He",
            "affiliations": [
                "Massachusetts General Hospital",
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A4316438637",
            "name": "Josh Calvano",
            "affiliations": [
                "University of New Mexico Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2102071759",
            "name": "David Chen",
            "affiliations": [
                "University of Toronto"
            ]
        },
        {
            "id": "https://openalex.org/A2953523943",
            "name": "Talar Telvizian",
            "affiliations": [
                "Lankenau Medical Center",
                "Main Line Health"
            ]
        },
        {
            "id": "https://openalex.org/A2805429191",
            "name": "Lawrence Jiang",
            "affiliations": [
                "Duke University"
            ]
        },
        {
            "id": "https://openalex.org/A2221938258",
            "name": "Paul Chong",
            "affiliations": [
                "Campbell University"
            ]
        },
        {
            "id": "https://openalex.org/A2921624836",
            "name": "Jacob Schwell",
            "affiliations": [
                "Thomas Jefferson University"
            ]
        },
        {
            "id": null,
            "name": "Gin Mak",
            "affiliations": [
                "McMaster University"
            ]
        },
        {
            "id": "https://openalex.org/A2204212155",
            "name": "Jarone Lee",
            "affiliations": [
                "Massachusetts General Hospital",
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A2196946318",
            "name": "Karl Swanson",
            "affiliations": [
                "University of California, San Francisco",
                "University of San Francisco"
            ]
        },
        {
            "id": "https://openalex.org/A2116925039",
            "name": "Shuhan He",
            "affiliations": [
                "Massachusetts General Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A4316438637",
            "name": "Josh Calvano",
            "affiliations": [
                "University of New Mexico Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2102071759",
            "name": "David Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2953523943",
            "name": "Talar Telvizian",
            "affiliations": [
                "Main Line Health",
                "Lankenau Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2805429191",
            "name": "Lawrence Jiang",
            "affiliations": [
                "Duke University"
            ]
        },
        {
            "id": "https://openalex.org/A2221938258",
            "name": "Paul Chong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2921624836",
            "name": "Jacob Schwell",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Gin Mak",
            "affiliations": [
                "McMaster University"
            ]
        },
        {
            "id": "https://openalex.org/A2204212155",
            "name": "Jarone Lee",
            "affiliations": [
                "Massachusetts General Hospital"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2396305291",
        "https://openalex.org/W2401260851",
        "https://openalex.org/W2926000164",
        "https://openalex.org/W2278617356",
        "https://openalex.org/W3082633563",
        "https://openalex.org/W2803556308",
        "https://openalex.org/W3021854208",
        "https://openalex.org/W2159947192",
        "https://openalex.org/W2984145054",
        "https://openalex.org/W2117817619",
        "https://openalex.org/W2551704653",
        "https://openalex.org/W4309376528",
        "https://openalex.org/W4384561707",
        "https://openalex.org/W2898330098",
        "https://openalex.org/W3013597571",
        "https://openalex.org/W3214169736",
        "https://openalex.org/W3155365624",
        "https://openalex.org/W6820021940",
        "https://openalex.org/W2068737686",
        "https://openalex.org/W2951520515",
        "https://openalex.org/W2915623326",
        "https://openalex.org/W2164777277",
        "https://openalex.org/W4221068952",
        "https://openalex.org/W3135809019",
        "https://openalex.org/W3041133507",
        "https://openalex.org/W3190367510",
        "https://openalex.org/W25615402",
        "https://openalex.org/W3197008538",
        "https://openalex.org/W2070204206",
        "https://openalex.org/W2996219887",
        "https://openalex.org/W4248677198",
        "https://openalex.org/W3106224367"
    ],
    "abstract": "The advent of patient access to complex medical information online has highlighted the need for simplification of biomedical text to improve patient understanding and engagement in taking ownership of their health. However, comprehension of biomedical text remains a difficult task due to the need for domain-specific expertise. We aimed to study the simplification of biomedical text via large language models (LLMs) commonly used for general natural language processing tasks involve text comprehension, summarization, generation, and prediction of new text from prompts. Specifically, we finetuned three variants of large language models to perform substitutions of complex words and word phrases in biomedical text with a related hypernym. The output of the text substitution process using LLMs was evaluated by comparing the pre- and post-substitution texts using four readability metrics and two measures of sentence complexity. A sample of 1,000 biomedical definitions in the National Library of Medicine’s Unified Medical Language System (UMLS) was processed with three LLM approaches, and each showed an improvement in readability and sentence complexity after hypernym substitution. Readability scores were translated from a pre-processed collegiate reading level to a post-processed US high-school level. Comparison between the three LLMs showed that the GPT-J-6b approach had the best improvement in measures of sentence complexity. This study demonstrates the merit of hypernym substitution to improve readability of complex biomedical text for the public and highlights the use case for fine-tuning open-access large language models for biomedical natural language processing.",
    "full_text": "RESEA RCH ARTICL E\nBiomedical text readability after hypernym\nsubstitution with fine-tuned large language\nmodels\nKarl Swanson\n1‡\n, Shuhan He\n2‡\n, Josh Calvano\n3\n, David Chen\nID\n4\n*, Talar Telvizian\n5\n,\nLawrence Jiang\n6\n, Paul Chong\n7\n, Jacob Schwell\nID\n8\n, Gin Mak\n9\n, Jarone Lee\nID\n2\n1 Department of Medicine– Clinical Informatic s, University of California–Sa n Francisc o, San Francisco,\nUnited States of America, 2 Massach usetts General Hospital and Harvard Medical School, Boston,\nMassach usetts, United States of America, 3 Departm ent of Anesthesi ology and Critical Care, University of\nNew Mexico Hospital, Albuquerq ue, New Mexico, United States of America, 4 Temerty Faculty of Medicine,\nUniversity of Toronto, Toronto , Ontario, Canada, 5 Department of Interna l Medicine, Main Line Health\nLankena u Medical Center, Wynnew ood, Pennsylvan ia, United States of America, 6 Department of Comput er\nScience, Duke University , Durham, North Carolina, United States of America, 7 School of Osteopat hic\nMedicine, Campbell Univers ity, Lillington, North Carolina, United States of America, 8 Sidney Kimmel\nMedical College, Thomas Jefferson University , Philadelph ia, Pennsylvan ia, United States of America,\n9 Department of Psychology, Neurosc ience & Behaviour, McMaster University , Hamilton, Ontario, Canada\n‡ These authors share first authorsh ip on this work.\n* davidc.c hen@mail.u toronto.ca\nAbstract\nThe advent of patient access to complex medical information online has highlighted the\nneed for simplification of biomedical text to improve patient understanding and engagement\nin taking ownership of their health. However, comprehension of biomedical text remains a\ndifficult task due to the need for domain-spe cific expertise. We aimed to study the simplifica-\ntion of biomedical text via large language models (LLMs) commonly used for general natural\nlanguage processing tasks involve text comprehension, summarization, generation, and\nprediction of new text from prompts. Specifically , we finetuned three variants of large lan-\nguage models to perform substitutions of complex words and word phrases in biomedical\ntext with a related hypernym. The output of the text substitution process using LLMs was\nevaluated by comparing the pre- and post-substitution texts using four readability metrics\nand two measures of sentence complexity. A sample of 1,000 biomedical definitions in the\nNational Library of Medicine’s Unified Medical Language System (UMLS) was processed\nwith three LLM approaches , and each showed an improvement in readability and sentence\ncomplexity after hypernym substitution. Readability scores were translated from a pre-pro-\ncessed collegiate reading level to a post-proce ssed US high-school level. Comparison\nbetween the three LLMs showed that the GPT-J-6b approach had the best improvement in\nmeasures of sentence complexity. This study demonstrates the merit of hypernym substitu-\ntion to improve readability of complex biomedical text for the public and highlights the use\ncase for fine-tuning open-access large language models for biomedical natural language\nprocessing.\nPLOS DIGI TAL HEALT H\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 1 / 18\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Swanson K, He S, Calvano J, Chen D,\nTelvizian T, Jiang L, et al. (2024) Biomedical text\nreadability after hypernym substitu tion with fine-\ntuned large language models. PLOS Digit Health\n3(4): e0000489. https://d oi.org/10.1371/j ournal.\npdig.00004 89\nEditor: Amara Tariq, Mayo Clinic Arizona, UNITED\nSTATES\nReceived: April 25, 2023\nAccepted: March 21, 2024\nPublished: April 16, 2024\nCopyright: © 2024 Swanson et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: The datasets from\nUMLS used in this study are all publicly available\nfrom the following sources: (https://uts .nlm.nih.\ngov/uts/umls /home). Open-sou rce code and data\nused in this study can be found at https://githu b.\ncom/kswa njitsu/DLAI/tr ee/master/ translate and\nhttps://github .com/kswanj itsu/DLAI_GPT /tree/\nmaster.\nFunding: The author(s) received no specific\nfunding for this work.\nAuthor summary\nPatient access to complex medical information online has driven the need for automated\nmethods for text simplification so that patients can understand and make informed\nhealth-related decisions. Deep learning models to process biomedical text has shown great\npromise in understanding, summarizing, and generation of text. Currently, there remains\ndebate over the most optimal deep learning method for biomedical text simplification.\nHere, we conducted a study to 1) benchmark three fine-tuned open-access large language\nmodels for biomedical text simplification and 2) demonstrate the merit of hypernym sub-\nstitution to improve text readability. We found that each large language model improved\nmeasures of readability and sentence complexity after hypernym substitution. We also\nidentified the GPT-J-6B model was consistently the most optimal method for text simplifi-\ncation based on readability and sentence complexity measures. Our study demonstrates\nthe promise of hypernym substitution for biomedical text simplification and the applica-\ntion of fine-tuned open-access large language models for biomedical text processing.\nIntroduction\nOpenNotes is a national initiative that launched in 2010 with the primary goal to share clinical\nnotes with patients to increase transparency and patient engagement. On April 5, 2021, a fed-\neral mandate implemented the bipartisan 21st Century Cures Act specifying that eight types of\nclinical notes must be made available and free of charge to all patients [1]. Studies thus far\nhighlight the benefits of shared notes including keeping patients better informed, increased\npatient satisfaction, and clinical outcomes [2–5]. Despite needing additional support to use\npatient portals, patients with limited health literacy reported reading clinical notes as consider-\nably important in feeling engaged with their care [3,6]. Adding to the difficulty of understand-\ning complex biomedical text in clinical notes, a study found large numbers of nonstandard\nwords, such as acronyms and abbreviations, in clinical documents. About three-quarters of the\nwords used are not covered in the English and Basic Medical English Dictionary, and 66% of\nthe words in clinical texts are not found in the National Library of Medicine’s Unified Medical\nLanguage System (UMLS) [7]. Therefore, simply making clinical charts available may not be\nequivalent to equitable access.\nThere remains an outstanding need to make domain-specific biomedical terminologies\nmore interpretable and accessible to patients across varying health literacy rates and languages.\nOnline medical translation tools like Google Translate and MediBabble provide opportunities\nfor improving communication across languages [8], but do not provide any direct form of sim-\nplification to medical terminology. Without the right accessibility tools, electronic medical\nrecord interpretation may exacerbate existing health disparities.\nIn the current state of biomedical text disambiguation, many computational tools have\nbeen created; ranging from traditional programmatic natural language processing (NLP) rules\nto a host of advanced artificial intelligence (AI) models, most notably pre-trained neural lan-\nguage models (PNLMs). Historically, automated text simplification started as a pre-processing\nstep of natural language processing to extract salient information [9] and label semantic roles\nof complex texts [10] using rule-based simplification approaches. Previous programmatic\napproaches have aimed to replace complex words with simpler synonyms [11] as well as\nimprove selection of important sentences in document-level summarization [12,13]. More\nrecently, artificial intelligence approaches have been applied to achieve text simplification in\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 2 / 18\nCompeting interests : The authors have declared\nthat no competing interests exist.\ntest scenarios using recurrent neural networks [14] and transformer models coupled with rein-\nforcement learning methods [15]. The advent of general-purpose large language models\ntrained on vast corpora of text have shown great promise in achieving conversational compe-\ntencies and interactivity at near-human levels across diverse fields, including medicine [16].\nRepurposing a general-purpose large language model by fine-tuning on a custom dataset can\nyield a specialized model that is well-suited for task-specific natural language applications.\nApplied to the task of simplification of biomedical text, the feasibility and efficacy of fine-\ntuned large language models compared to standard, rule-based approaches requires further\nbenchmarks for pre-clinical validation.\nWhen navigating the literature and novel innovation surrounding medical lexicon simplifi-\ncation, it is evident that providing layman terminology improves patient comprehension and\nunderstanding of medical text [17–19]. Semantic methods to design text simplification\napproaches generally aim to substitute complex words with related synonyms with the same\nmeaning [20–22]. However, substitution of complex words with their associated synonyms\nmay not effectively simplify words in cases where complex, domain-specific terminology in\nmedicine may not have any reasonable synonyms or only have similarly complex synonyms.\nTo address this problem, we hypothesize that text simplification by substitution with a com-\nplex word’s hypernym, the superordinate umbrella term that a complex term falls under, can\ngenerate more reasonable simplified text that is readable to lay audiences.\nOur objectives were to:\n1. Design a method of text augmentation via hypernym substitution that retains semantic and\nsyntactic qualities of biomedical definitions post-sentence augmentation.\n2. Perform in silico analysis of a method’s ability to improve the readability of biomedical defi-\nnitions by applying reliability metrics to method results.\n3. Discuss the current readability metrics in the setting of biomedical text augmentation.\nMethods\nDataset\nFrom the Unified Medical Language System (UMLS), we extracted definitions of each term as\na target for text simplification. In total there were 395,168 definitions including all languages.\nOf those definitions, 331,550 were English. We used simple random sampling to obtain 1000\nUMLS definitions, excluding the few hundred used in the training data.\nPipeline\nOur approach uses a linguistic property called hypernymy, which denotes a relationship of\nspecificity between two words or word phrases [23]. An example sentence demonstrating text\nsimplification using hypernymy would be: lymphoma is a type of cancer. The hypernym “can-\ncer” is less specific than the hyponym “lymphoma.”\nThe basic approach for hypernym substitution is outlined in Fig 1. Complex biomedical\nwords in a given sentence are replaced for their respective hypernym. We show multiple meth-\nods for hypernym substitution approaches.\nThe first approach, named Programmatic + AI, starts with two models: 1) a pre-trained\nseq2seq bidirectional Long Short Term Memory supervised classification model to identify\ncomplex words [24] and 2) a sciBERT large language model to identify biomedical word\nphrases [25]. If a wordphrase is both complex and biomedical, we then utilize traditional pro-\ngrammatic methods to determine which hypernym replaces the word.\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 3 / 18\nThe second approach uses fine-tuned state-of-the-art (SOTA) autoregressive generative\ntransformers, named T5 and GPT-J-6b. We finetuned and used two different model architec-\ntures individually to compare performance for the second approach. These models include: 1.)\na SciFive T5 model [26] base model architecture, and 2.) a GPT-J-6b [27] base model architec-\nture. The models in the second approach were used to identify which words need substituting\nand selecting which hypernym would be substituted.\nAI+programmatic approach: The knowledge base for our corpus was primarily composed\nof Wordnet and semantic types, with the base node of the hypernym tree coming from the\nUMLS database. In this approach, we applied the first seq2seq sequence labeling model that\ndetermines word complexity [24] and the second sciBERT named-entity recognition (NER)\nmodel for the determination of biomedical language substitution [25]. The output of the pre-\ntrained seq2seq model is a prediction of a word’s complexity probability, ranging from 0 to 1.\nBased on empirical benchmarks of different complexity probability thresholds ranging from\n0.1 to 0.9 (0.05 step intervals) inclusive and sample classification outputs of complex words, we\nset a reasonably balanced threshold of 0.65 based on sensitivity to identify reasonably complex\nwords for model design and further validation. If a word is considered both biomedical and\nmeets the criteria for complexity, with a probability of 0.65 or greater, the word is searched\nagainst UMLS with a term-frequency inverse document frequency approach for 3-gram char-\nacter matching. From there, the matched word’s hypernym root is determined through UMLS\nsemantic types, or from WordNet, whichever has the least amount of syllables.\nFinetuning SciFive T5 and GPT-J-6b: We utilize two different pre-trained natural language\nmodels (PNLM), a base SciFive T5 model and a base GPT-J-6b model\n26,27\n. The fine-tuning for\nGPT-J-6b first involved training the model on a Wordnet-derived hypernym dataset of\n120,274 hypernymy pairs. We prepared Wordnet-derived hypernym dataset as pairs of words,\nwhere each pair consisted of 1) the word to substitute out and 2) the possible hypernym(s) to\nsubstitute in and used this dataset to finetune the GPT-J-6b base model using the Hugging\nFace Auto-Train interface with default parameters. Then, to prime the output fine-tuned\nGPT-J-6b model to generate hypernym-substituted text at the time of inference, we provided\nfew-shot examples of sentences containing complex words and manually curated output sen-\ntences using hypernym substitution based on the Wordnet-derived hypernym dataset. Lastly,\nFig 1. Hypernym substitu tion process used by three fine-tuned large language model approache s to biomedical text simpli fication.\nhttps://do i.org/10.1371/j ournal.pdig. 0000489.g001\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 4 / 18\nwe provide the fine-tuned model with input text for automated complex word identification\nand hypernym substitution. Likewise, we used a similar approach to fine-tune a T5 model\nusing the Hugging Face interface with default parameters. For the T5 model, specifically the\nSciFive model trained on a PubMed-derived biomedical text corpora, we utilize a fine-tuning\ntraining set of manually curated output sentences using hypernym substitution derived from\nWikipedia, Simple Wikipedia, and UMLS definitions. None of the manually curated examples\nin the training set were used in the dataset for evaluation.\nAnalysis\nAfter hypernym substitution occurs, we analyze the document’s readability with the Flesch-\nKincaid (FK) Reading Ease Score (referred to as FKS) and the respective FK Grade Level scores\n(referred to as FKG), pre and post-hypernym substitution. Both scores assess sentence lengths\n(based on word count) and word complexities (based on the number of syllables). The FK\nReading Ease is a scale of 0–100, with 100 being the least difficult for reading comprehension.\nThe FKG estimates the United States educational grade level needed for understanding the\ntext. Two other standard reading score methods for comparison included in the model bench-\nmark analysis were the Automated Readability Index (ARI) and Gunning Fog Index (GFI).\nARI is an additional estimate of the United States educational grade level needed for compre-\nhending the text. GFI estimates the years of formal education a person needs to understand\nthe text on the first reading. We hypothesized that hypernym substitution for complex-bio-\nmedical words increases FKS and therefore decreases the respective FKG level. Similarly, since\nARI and GFI are grade measures, we would expect a reduction in the grade level for these mea-\nsures post-hypernym substitution. The mathematical definitions of FKS, FKG, ARI, and GFI\nare shown in S1 Fig.\nTo measure sentence complexity, we compared one measure of lexical diversity and one\nmeasure of syntactic diversity between text before and after hypernym substitution using three\ndifferent large language models. The Measure of Textual Lexical Diversity (MLTD) is an index\nof the range of different words in a text corpus, where higher scores indicate higher lexical var-\niation. The Mean Dependency Distance (MDD) is an index of text complexity based on the\nmean linear distance between two syntactically related words in a text. Both measures aim to\nquantitatively reflect in part the cognitive load of text comprehension beyond standard reading\nscore methods.\nTo evaluate the human readability of hypernym-substituted text, we conducted a blinded\nstudy where two study authors (KS, DC) rated a random sample of original texts and post-\nhypernym substituted texts (n = 50) based on 5 pilot measures of readability. Texts were rated\non a Likert scale scored from 1(very poor) to 5 (very good) for each metric: clarity, conciseness,\nease of understanding, accuracy, and fidelity (Table 1). Cohen’s kappa score was used to com-\npare inter-rater reliability. Cohen’s kappa score thresholds for strength of rater agreement\nranges from <0 = poor, 0–0.20 = slight, 0.21–0.40 = fair, 0.41–0.60 = moderate, 0.61–\nTable 1. Definition s of pilot measures of human-rated readabil ity.\nHuman Readabil ity Metric Metric Definition\nClarity Use of terminology and sentence structure to enhance compreh ension\nConcisen ess Brevity of expressi on\nEase of Understan ding Accessibility of compreh ension without external resource s\nAccuracy Preservation of the literary elements of the original text\nFidelity Preservation of the core interpretation of the original text\nhttps://d oi.org/10.1371/j ournal.pdig. 0000489.t0 01\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 5 / 18\n0.80 = substantial, and >0.80 = almost perfect [28]. To confirm if hypernym substitution\nimproved human-rated readability, we compared the readability of the original text compared\nto the readability of the hypernym-substituted texts based on clarity, conciseness, and ease of\nunderstanding. Since the original text is considered the gold-standard based on accuracy and\nfidelity, we did not compare the hypernym-substituted texts to the original texts for these two\nmetrics. To identify the best-performing hypernym substitution approach, we conducted com-\npared each of the hypernym-substituted texts generated using GPT-J-6b, T5, and Program-\nmatic + AI based on clarity, conciseness, ease of understanding, accuracy, and fidelity.\nResults\nDataset\nAfter post-processing through each approach: Programmatic+AI, T5, and GPT-J-6b, some\ndefinitions were lost due to model error. We were left with 743 definitions that were success-\nfully processing by all three large language models in the union of these post-processed data-\nsets. Table 2 describes the general characteristics of this composite dataset in terms of\nreadability metric parameters.\nEvaluation\nThe summary of change for each readability metric for comparison across the three\napproaches can be seen in the Table 3. We additionally compared these approaches’ outputs\nby calculating the two-tailed Wilcoxon Signed Rank test for the mean readability scores\nbetween each approach: Programmatic+AI vs T5; Programmatic+AI vs. GPT-J-6b, T5 vs.\nGPT-J-6b. The distribution of FKS, FKG, ARI and GFI scores pre-substitution vs. post-substi-\ntution using the Programmatic+AI, T5, and GPT-J-6b models are shown in S1, S2 and S3 Figs\nrespectively.\nOur text simplification approaches were applied to 1000 medical definitions, with a union\noutput of 743 post-processed definitions from the random sample of the UMLS dataset. The\npost-hypernym substitution output of the GPT-J-6b and Programmatic+AI models reduced\nTable 2. Description of the dataset for definitio ns to be run through each of the three large language models : Total UMLS Definition s, Total UMLS English defini-\ntions, and the UMLS English Subset we used to evaluate our models.\nUMLS All Definition s UMLS All English Definitions UMLS Sample Englis h Definit ions\nTotal # of Definition s 394,168 331,550 1,000\nMean Characte rs Per Definitions 262 216 230\nMean Words Per Definition 31 31 31\nMean Sentence s Per Definition 3 3 3\nhttps://do i.org/10.1371/j ournal.pdig. 0000489.t00 2\nTable 3. Mean change (Δ) in four readabil ity metrics FKS, FKG, ARI, GFI, as well as two sentence complexi ty metrics MLTD and MDD, of the post-hyp ernym\nsubstituted texts generated using each model compared to the original pre-hyperny m substituted texts. P value of two-sided Wilcoxon Signed Rank test are seen\nbelow the mean differe nce comparin g between the post-hyp ernym substitute d texts generated using each model and the original pre-hypernym substitute d texts (*\np < 0.05, ** p < 0.01, *** p < 0.005).\nReadability Metric\nApproach FKS FKG ARI GFI MLTD MDD\nProgrammatic+AI Δ32.31*** Δ-4.26*** Δ-4.51*** Δ-4.39*** Δ0.22p = 1.036 x 10\n−1\nΔ0.098***\nGPT-J-6b Δ31.64*** Δ-4.86*** Δ-10.33*** Δ-3.81*** Δ-25.31*** Δ-0.42***\nT5 Δ5.63*** Δ-0.89*** Δ-4.41*** Δ-0.51*** Δ0.82p = 4.916 x 10\n−1\nΔ-0.22***\nhttps://do i.org/10.1371/j ournal.pdig. 0000489.t00 3\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 6 / 18\ngrade reading level by around 4 grades, with the exception of the ARI grade, which reduced\nthe reading level by 10 grades using GPT-J-6b approach.\nEvaluation with FK score. The mean difference of post-processed scores were compared\nacross approaches with a two-sided Wilcoxon Signed Rank test. In the Programmatic+AI vs.\nT5 approaches, the mean score was significantly different (p<0.001, CI = 24.16 to 27.84). In\nthe Programmatic+AI vs. GPT-J-6b approaches, the mean score was not significantly different\n(p = 3.096x10\n-3\n, CI = -1.64 to 5.67). In T5 vs. GPT-J-6b approaches, the mean score was signifi-\ncantly different (p<0.001, CI = -27.78 to -20.18). The FK score benchmark test of each large\nlanguage model before and after hypernym substitution is visualized in Fig 2.\nEvaluation with FK grade. The mean difference of post-processed scores were compared\nacross approaches with a two-sided Wilcoxon Signed Rank test. In the Programmatic+AI vs.\nT5 approaches, the mean difference in FK grade was significantly different (p<0.001, CI =\n-3.51 to -2.96). In the Programmatic+AI vs. GPT-J-6b approaches, the mean difference in\ngrade was not significantly different (p = 0.297, CI = -1.54 to -0.30). In T5 vs. GPT-J-6b\napproaches, the mean difference was significantly different (p<0.001, CI = 1.67 to 2.95). The\nFK grade benchmark test of each large language model before and after hypernym substitution\nis visualized in Fig 3.\nEvaluation with ARI. The mean difference of post-processed scores was compared across\napproaches with a two-sided Wilcoxon Signed Rank test. In the Programmatic+AI vs. T5\napproaches, the mean ARI grade was not significantly different (p = 0.676, CI = -0.26 to 0.56).\nIn the Programmatic+AI vs. GPT-J-6b approaches, the mean grade was significantly different\n(p<0.001, CI = 4.93 to 6.27). In T5 vs. GPT-J-6b approaches, the mean grade was significantly\ndifferent (p<0.001, CI = 4.82 to 6.09). The ARI grade benchmark test of each large language\nmodel before and after hypernym substitution is visualized in Fig 4.\nEvaluation with GFI. The mean difference of post-processed scores was compared across\napproaches with a two-sided Wilcoxon Signed Rank test. In the Programmatic+AI vs. T5\nFig 2. Benchmark of the mean difference in FKS for pre-proces sing to post-proces sing for each large language model approach , Programm atic+AI, T5,\nGPT-J-6b. Here we show that the Program matic+AI had the most improvem ent, with a mean FKS increase of 31.67 (p<0.001, IQR 34.58), followed by the\nGPT-J-6b approach, with a mean FKS increase of 29.65 (p<0.001, IQR 55.38). The least amount of improvement was using the T5 approach, with a mean FKS\nincrease of 5.67 (p<0.001, IQR 37.66).\nhttps://do i.org/10.1371/j ournal.pdig. 0000489.g002\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 7 / 18\napproaches, the mean GFI grade was significantly different (p<0.001, CI = -4.00 to -3.27). In\nthe Programmatic+AI vs. GPT-J-6b approaches, the mean grade showed a significant differ-\nence (p<0.001, CI = -1.69 to -0.41). In T5 vs. GPT-J-6b approaches, the mean grade showed a\nsignificant difference (p<0.001, CI = 1.93 to 3.24). The GFI grade benchmark test of each large\nlanguage model before and after hypernym substitution is visualized in Fig 5.\nFig 3. Benchmark of the mean difference in FKG for pre-process ing to post-proces sing for each large language model approach, Programm atic+AI, T5,\nand GPT-J-6 b. Here we show that the Programmati c+AI had the most improvement, with a mean FKG reduction of 4.18 (p<0.001, IQR 5.41), followed by the\nGPT-J-6b approach, with a mean FKG reduction of 3.25 (p<0.001, IQR 9.84). The least amount of improvem ent was using the T5 approach, with a mean FKG\nreduction of 0.94 (p<0.001, IQR 5.77).\nhttps://do i.org/10.1371/j ournal.pdig. 0000489.g003\nFig 4. Benchmark of the mean difference in ARI grade for pre-process ing to post-proces sing for each approach, Programm atic+AI, T5, and GPT-J-6b.\nHere we show that the GPT-J-6b approach had the most improvement, with a mean ARI grade reduction of 10.15 (p<0.001, IQR 11.16), followed by the T5\napproach, with a mean ARI grade reduction of 4.70 (p<0.001, IQR 6.65). The least amount of improvement was using the Programma tic+AI approach, with a\nmean ARI grade reduction of 4.55 (p<0.001, IQR 6.73).\nhttps://do i.org/10.1371/j ournal.pdig. 0000489.g004\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 8 / 18\nEvaluation with MLTD and MDD. After performing readability metric evaluation, we\ncompared the change in MLTD and MDD metrics, measures of lexical diversity and syntactic\ncomplexity respectively. We compared the text before and after hypernym substitution using\nthe GPT-J-6b, programmatic+AI, and T5 approaches. Comparing text before and after hyper-\nnym substitution using the GPT-J-6b approach, we observed a decrease in the mean MLTD\nfrom 45.32 to 20.01 (w = 10534.5, p-value = 3.715x10\n-46\n) and mean MDD from 2.90 to 2.48\n(w = 4.76, p-value = 1.859x10\n-11\n). Using the programmatic+AI approach, we observed no dif-\nference in mean MLTD (w = 66876.5, p-value = 0.104) but a decrease in mean MDD\n(w = 94918, p-value = 7.519x10\n-24\n) before and after hypernym substitution. For the T5\napproach, we observed no difference in mean MLTD before and after hypernym substitution\n(w = 93489, p-value = 0.492) and a decrease in mean MDD from 2.93 to 2.71 (w = 82706.5, p-\nvalue = 4.227x10\n-25\n). The benchmark of the three large language models using MLTD and\nMDD metrics are visualized in Fig 6.\nThe mean change in the six readability metrics of the post-hypernym substituted text gener-\nated using each of the three models compared to the original pre-hypernym substituted text is\nsummarized in Table 3. Additional results of statistical tests of mean differences in readability\nscores between pre- and post-hypernym substituted text generated using the Programmatic\n+ AI, T5, and GPT-J-6b approaches are summarized in S1, S2 and S3 Tables respectively.\nEvaluation with human-rated readability metrics. To confirm that the findings of the\nin-silico readability analyses are represented in human evaluations of text readability, we con-\nducted a blinded study with a random sample of texts (n = 50) where we rated the original text\nand the 3 hypernym-substituted texts based on pilot measures of human readability. Cohen’s\nkappa score each of the human-rated readability metrics between two raters confirms moder-\nate to near-perfect inter-rater agreement across all metrics as expected (Fig 7A). Between the 3\nhypernym substitution approaches, the T5 approach ranked highest based on mean ratings of\naccuracy, ease of understanding, fidelity, and clarity while the GPT-J-6b approach ranked\nFig 5. Benchmark of the mean difference in GFI grade for pre-process ing to post-pr ocessing for each approac h, Programm atic+AI, T5, and GPT-J-6b.\nHere we show that the Programmati c+AI approach had the most improvem ent, with a mean GFI grade reduction of 4.131 (p<0.001, IQR 7.03), followe d by\nthe GPT-J-6b approach, with a mean GFI grade reduction of 3.25 (p<0.001, IQR 6.82). The least amount of improvem ent was using the T5 approach, with a\nmean GFI grade reduction of 0.67 (p<0.001).\nhttps://do i.org/10.1371/j ournal.pdig. 0000489.g005\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 9 / 18\nhighest based on mean ratings of conciseness (Fig 7B). Notably, the Programmatic + AI\napproach was the lowest-ranked hypernym substitution approach across all five readability\nmetrics.\nNext, we compared the readability between the GPT-J-6b, T5, and Programmatic + AI\nhypernym substituted texts and the original texts to confirm if the hypernym substitution\nframework improves human-rated readability (Fig 7C; S4 Table). Comparing the GPT-J-6b-\ngenerated text to the original text, we observed an increase in clarity (w = 138, p = 0.0289), con-\nciseness (w = 13.5, p <0.001), and ease of understanding (w = 136, p = 0.00105). Likewise, we\nobserved the T5-generated text improved the original text based on clarity (w = 21, p < 0.001)\nand ease of understanding (w = 36, p < 0.001).\nTo identify the best-performing hypernym substitution approach based on human-rated\nreadability, we conducted pairwise comparisons of readability between the GPT-J-6b, T5, and\nProgrammatic + AI hypernym substituted texts (Fig 7D). The T5 approach ranked the highest\nbased on clarity, accuracy, and fidelity compared to the GPT-J-6b and Programmatic + AI\napproaches. The GPT-J-6b approach ranked the highest based on conciseness compared to the\nT5 and Programmatic + AI approaches. The T5 and GPT-J-6B approaches were both ranked\nhigher based on ease of understanding compared to the Programmatic + AI approach. All sta-\ntistics for inter-approach comparisons of human-rated readability are reported in S5 Table.\nFig 6. Compariso n of the MLTD and MDD scores of text before and after hypernym substitutio n using the\nGPT-J-6B , Programm atic+AI, and T5 approaches . A) The GPT-J -6b approach reduced the mean MLTD score from\n45.32 to 20.01 (w = 10534.5, p-value = 3.715x10\n-31\n) and mean MDD from 2.90 to 2.48 (w = 54731, p-value = 1.859x10\n-\n11\n) between text before and after hypernym substituti on. B) The Programmat ic+AI approach showed no difference in\nmean MLTD (w = 66876.5, p-value = 1.036x10\n-1\n) and decrease in mean MDD scores (w = 94918, p-value = 7.519x10\n-\n24\n) between text before and after hypernym substituti on. C) The T5 approach showed no difference in mean MLTD\nscore (w = 93489, p-value = 4.916x10\n-1\n) and reduced the mean MDD score from 2.93 to 2.71 (w = 82706.5 , p-value =\n4.227x10\n-25\n) between text before and after hypernym substitution.\nhttps://d oi.org/10.1371/j ournal.pdig. 0000489.g006\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 10 / 18\nDiscussion\nOverview\nTo the authors’ knowledge, the use of hypernym substitution with fine-tuned open-access\nlarge language models, with or without traditional programmatic constraints, for complex\nFig 7. Evaluation of human-rate d metrics of text readability after hypernym substitu tion using the GPT-J-6B, Programm atic+AI, and T5 approac hes. A)\nCohen’s kappa score across 3 readability metrics of the original text and 5 readabili ty metrics of the hypernym-subs tituted text. B) C) Comparison of 3\nreadabili ty metrics between the original text and the hypernym -substituted texts. D) Pair-wise comparison of 5 readabili ty metrics between the hypernym-\nsubstitute d texts. * = p < 0.05, ** = p < 0.01, *** = p < 0.001, **** = p < 0.0001.\nhttps://do i.org/10.1371/j ournal.pdig. 0000489.g007\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 11 / 18\nmedical terminology simplification has not yet been applied for biomedical text. Through our\nmethods, we demonstrated that hypernym substitution shows promise for improving readabil-\nity of medical and scientific content for the general public. We created two approaches to sim-\nplify medical text. The first programmatic+AI approach using deep-learning enabled methods\nto identify 1) which words or phrases need to be substituted, and then 2) a traditional pro-\ngrammatic rule-based substitution method. The second approach was a one-shot deep-learn-\ning approach that handled both the identification of what words or phrases to get substituted\nand then hypernym substitution itself. The second approach benchmarked two commonly\nused large language models for the same text simplification task. Both one-shot deep-learning\nshowed improvement across a selection of readability metrics for in silico validation. We\nshowed an increase in FKS and a decrease in all grade-level readability metrics for all post-sub-\nstitution datasets, with the exception of mean GFI score for the T5 approach.\nTo textually represent our fine-tuned GPT-J-b model, we have provided examples to dem-\nonstrate the change from the original UMLS definition to our post-processed definition with\nhypernym substitution, shown in Table 4. The Programmatic+AI approach, despite post-pro-\ncessing rules to account for grammar and syntax, had results that were qualitatively nonsensi-\ncal to the authors, despite an improvement in metrics, as demonstrated in Table 4. The GPT-J-\n6b model did not seem to have these types of erroneous and nonsensical outputs, retaining\nhuman-readable syntax and comprehension. This demonstrates that the GPT-J-6b may have\nbetter retention of syntax and grammar alongside the measurable reduction in grade level\nscores per the metrics used compared to programmatic, rule-based approaches. This result is\nshown quantitatively in the improvement in MLTD and MDD scores, measures of syntaxic\ncomplexity, for each hypernym substitution approach. The Programmatic+AI approach did\nnot have a significant difference in these measures between pre and post-hypernym substitu-\ntion, suggesting no change in cognitive burden. In comparison, the GPT-J-6b approach had a\nsignificant reduction in mean MLTD scores, suggesting a net decrease in lexical diversity.\nMoreover, the GPT-J-6b approach had a significant reduction in syntactic distance, suggesting\nreduced sentence complexity while overall conserving syntactic structure.\nClassic readability formulas may fail to capture linguistic features useful for faithful assess-\nment of human readability [29]. To extend our study beyond in-silico measures of readability,\nwe conducted a cross-sectional study of human-rated evaluations of readability to 1) confirm\nthat the hypernym substitution framework can improve the readability of biomedical text and\n2) identify the top-performing hypernym substitution approach. Both the GPT-J-6b and T5\napproaches improved clarity and ease of understanding compared to the original text, while\nonly the GPT-J-6b approach improved conciseness compared to the original text. Indeed,\nthese results suggest that the hypernym substitution framework using the GPT-J-6b and T5\napproaches can improve the readability of complex domain-specific terminologies within bio-\nmedical text. The Programmatic + AI approach failed to improve human-rated measures of\nreadability compared to the original text. Our impression based on the benchmark results was\nTable 4. Example of definitio ns before and after hypernym substitutio n for each large language model approach\n(Program matic+AI , T5, and GPT-J-6b).\nPre-processed Example Sentence FK Grade (rounded)\nInteracti ng selectively and non-co valently with a h4 histamine receptor. 14th grade\nApproach Post-processed Example Sentence\nProgrammatic+AI Act selectively and non-covale ntly with an h4 amine organ. 1st grade\nT5 Interacti ng selecting and not binding with a h4 receptor. 9th grade\nGPT-J-6b Specifica lly binding to a histamin e h4 receptor. 9th grade\nhttps://d oi.org/10.1371/j ournal.pdig. 0000489.t0 04\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 12 / 18\nthat the programmatic approach failed to capture the appropriate context of the complex bio-\nmedical term and failed to retain the structure and syntax of the text, leading to poor accuracy\nand ease of understanding respectively. We noted that although the GPT-J-6b and T5\napproaches improved clarity and ease of understanding compared to the original text, the T5\napproach was superior to the GPT-J-6b approach based on accuracy and fidelity to represent\nthe meaning and structure of the original text. The GPT-J-6b approach tended to generate sig-\nnificantly shorter simplified texts than the T5 approach and the original text, leading to loss of\nimportant textual elements and meaning in favor of conciseness. Given that optimal text sim-\nplification involves both improving readability and retaining the information context of the\noriginal text [30], our results demonstrate the superiority of the T5 approach based on its\nimprovement in readability compared to the original text as well as superior performance in\nreadability compared to the GPT-J-6b and Programmatic + AI approaches benchmarked in\nthis study.\nTaken together, our results suggest that the T5 approach and its underlying transfer learn-\ning framework can be useful for hypernym substitution to improve the readability of complex\ndomain-specific terminology in biomedical texts. The T5 approach is unique compared to the\nother tested approaches due to its 1) text-to-text framework that directly takes in and outputs\ntext, and 2) large pre-training text corpora on the C4 dataset that is twice as large as Wikipedia.\nAs a framework, transfer learning remains a widely applicable and growing area of machine\nlearning that transfers knowledge learned from one domain to other related source domains\n[31].\nFine-tuned transformer models remain a promising mode of future research given their\nability to be applied to a wide range of natural language processing scenarios, including gener-\native, classification, and regression tasks [32].\nPractical application\nFor each approach, Programmatic+AI, T5, and GPT-J-6b respectively saw a reduction of FKG\nby 4.30 (p<0.001), 0.89 (p<0.001), 4.86 (p<0.001). The GPT-J-6b approach had the best\nresults overall for improving readability metrics post-hypernym substitution. In practice, the\nimprovement in readability scores demonstrate successful translation of pre-processed medi-\ncal lexicon from a collegiate level to a post-processed level of US middle school to high school\nreadability.\nOpenNotes mandated that by October 2022, all third-party health applications were\nrequired to make personal medical documentation openly available to the public\n1\n. This\nincreased access allows patients to easily obtain records that were not previously available.\nWith an average of 36% of the United States having a health literacy scored as low [33], and\nthe average reading grade level of American’s is between 7th and 8th grade [34], we anticipate\nthat patients will be looking for a reliable source for understandable medical terminology\nwithin the provided context. This process for text simplification using fine-tuned large lan-\nguage models is one proposed solution for addressing biomedical literacy disparities.\nFuture development\nAcronyms and abbreviations. A recent sophisticated and promising NLP model for\ndefining medical acronyms and abbreviations was developed and proved to have high accu-\nracy, but was hindered for operational and production environments by training times [35]. In\nagreement with Skreta et al., creating a model to handle abbreviations and acronyms is neces-\nsary to make biomedical text simplification more readable and understandable to the layper-\nson [35]. However, resolving abbreviations and acronyms alone is likely not sufficient to fully\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 13 / 18\nhandle the complexity of medical literature and domain-specific terminology. After handling\nacronyms and abbreviations, hypernym substitution use cases could be used to create simplify-\ning pipelines in other biomedical and clinical contexts, such as clinical text disambiguation. In\naddition, this process should be optimized for minimal training times.\nFine-tuning of large language models. The reduction in readability level from the T5\nmodel was not as significant as the GPT-J-6b model. Future studies adjusting the temperature,\ndefined as the stochasticity of hypernym substitutions and the stringency to perform substitu-\ntions as modifiable, is possible. That is, if the temperature was less deterministic and more sub-\nstitutions were performed, a similar decrease in grade level would be observed for T5 as it was\nfor GPT-J-6b.\nAdditionally, it remains important that regardless of the approach used an iterative process\nfor a human-in-the-loop system for training is necessary to ensure that translations are work-\ning effectively and that corpora data remains valid. We believe a supervised NLP model com-\nposed of scientists and medical professionals for model feedback is necessary when handling\nthis specialized jargon used in biomedical text. This would help focus efforts on creating cen-\ntralized and dependable training data within this domain of research.\nOpen-access large language models. The advent of large language models trained on a\nvast corpus of natural language serves as a promising next step to build a highly generalizable\ntool for a variety of natural language processing tasks. For instance, the development of propri-\netary tools such as ChatGPT, a large language model with over 175 billion parameters, is a\npromising next step towards building natural language processing. Further fine-tuning of a\ngeneral large language model such as ChatGPT can be a promising future direction to address\nthe problem of biomedical text simplification. However, there remains promise for the devel-\nopment of alternative open-access large language models, including the models used in this\nstudy, so that the process of biomedical text simplification remains transparent, community-\nsourced, expert-validated , and verifiable by the public.\nValidation\nScoring systems. While readability scores have been shown to correlate well with human\nvalidation, they are not perfect metrics. Readability formulas have variability up to 6 reading\ngrade levels on the same text [36]. The SMOG (Simple Measure of Gobbledygook) formula\nwas found to perform most consistently and measure the grade level higher, which is why it\nhas been recommended for healthcare applications [37]. Additionally, the NIH identifies the\nSMOG for the assessment of patient education materials for people with low health literacy\n[38]. However, SMOG conversion tables for the texts used in this study, with an average length\nof three sentences would statistically invalid, because the SMOG formula was intended for lon-\nger texts and normalized on 30-sentence samples. For our use-case, definitions were on aver-\nage about 3 sentences long, limiting the utility of this metric. Applying SMOG to future studies\nlooking at large bodies of medical text, such as medical journal publications or portions of a\npatient chart, could hold potential.\nHuman validation of models. Readability metrics do not inherently account for grammar\nand syntax. The metrics assume grammar and syntax are intact and sensible, which may not\nbe the case as in the Programmatic+AI approach. That is, an improvement, or reduction, in\nreadability grade does not guarantee an output that is sensical to humans. While the each of\nthe hypernym substitution approaches showed significant differences in terms of grade metric\nreduction, the authors believe that the T5 and GPT-J-6b approach holds the most promise for\na human-readable output (Table 3). Our cross-sectional study of human-rated readability\nacross the 3 hypernym substitution approaches nominated the T5 approach as the top-\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 14 / 18\nperforming framework for biomedical text simplification using hypernym substitution. Fur-\nther prospective research is needed to systematically characterize the unique linguistic features\nof the T5 approach and evaluate patient-based ratings of simplified text.\nConclusion\nWe demonstrated that three state of the art large language models can use hypernym substitu-\ntion to improve readability and decrease semantic complexity useful for biomedical text sim-\nplification. We additionally highlight further areas of research needed to validate these\nproposed models to successfully disambiguate biomedical text.\nSupporting information\nS1 Fig. Equations used to calculate the readability metrics used to evaluate the Program-\nmatic, T5, and GPT approaches for hypernym substitution of text.\n(TIF)\nS2 Fig. Programmatic Approach Distributions of Pre and Post Processing Readability\nScores. In this figure we show the distributions of FKS, FKG, GFI, and ARI;\nred = distributions of original pre-substitution definitions, green = distributions of post-sub-\nstitution definitions. These figures show change in simplicity distribution for: A) FKS—\nincrease in score is considered increase in readability. B) FKG contrasting the logic of FKS\nwhere a decrease in grade level is considered and increase in readability. C) GFI grade distribu-\ntions, a decrease in grade level is considered and increase in readability. D) ARI grade, a\ndecrease in grade level is considered an increase in readability. Here we show with the pro-\ngrammatic approach, we increase readability across all metrics.\n(TIF)\nS3 Fig. T5 Approach Distributions of Pre and Post Processing Readability Scores. We\nshow the distribution for the A) Flesh-Kincaid score, B) Flesh-Kincaid grade level, C) Gun-\nning Fog grade, and D) Automated Readability index for the T5 approach. Score distribution\nof the pre-substitution text and post-T5 substitution text is shown in red and green respec-\ntively. The T5 approach showed an increase in readability across all readability metrics.\n(TIF)\nS4 Fig. GPT Approach Distributions of Pre and Post Processing Readability Scores. We\nshow the distribution for the A) Flesh-Kincaid score, B) Flesh-Kincaid grade level, C) Gun-\nning Fog grade, and D) Automated Readability index for the GPT approach. Score distribution\nof the pre-substitution text and post-GPT substitution text is shown in red and green respec-\ntively. The GPT approach showed an increase in readability across all readability metrics.\n(TIF)\nS1 Table. Readability score benchmark of the pre- and post-hypernym substituted text\nusing the Programmatic+AI approach for text simplification.\n(XLSX)\nS2 Table. Readability score benchmark of the pre- and post-hypernym substituted text\nusing the T5 approach for text simplification.\n(XLSX)\nS3 Table. Readability score benchmark of the pre- and post-hypernym substituted text\nusing the GPT-J-6B approach for text simplification.\n(XLSX)\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 15 / 18\nS4 Table. Human-rated benchmark of 3 readability metrics (clarity, conciseness, ease of\nunderstanding) between pre- and post-hypernym substituted text generated using the Pro-\ngrammatic+AI, T5, and GPT-J-6B approaches.\n(XLSX)\nS5 Table. Human-rated benchmark of 5 readability metrics (clarity, conciseness, ease of\nunderstanding, accuracy, fidelity) between the post-hypernym substituted text generated\nusing the Programmatic+AI, T5, and GPT-J-6B approaches.\n(XLSX)\nAuthor Contributions\nConceptualization: Karl Swanson, Shuhan He, Josh Calvano, David Chen, Talar Telvizian.\nData curation: Karl Swanson.\nFormal analysis: Karl Swanson, Shuhan He, Josh Calvano, David Chen, Talar Telvizian, Jacob\nSchwell.\nInvestigation: Karl Swanson, Josh Calvano, David Chen, Talar Telvizian, Lawrence Jiang, Paul\nChong, Jacob Schwell.\nMethodology: Karl Swanson, Shuhan He, Josh Calvano, David Chen, Talar Telvizian.\nProject administration: Karl Swanson, Shuhan He, Josh Calvano, David Chen, Jarone Lee.\nResources: Karl Swanson.\nSoftware: Karl Swanson, David Chen.\nSupervision: Karl Swanson, Shuhan He, Josh Calvano, David Chen, Talar Telvizian, Jarone\nLee.\nValidation: Karl Swanson, David Chen, Lawrence Jiang, Paul Chong.\nVisualization: Karl Swanson, David Chen.\nWriting – original draft: Karl Swanson, Shuhan He, Josh Calvano, David Chen, Talar Telvi-\nzian, Lawrence Jiang.\nWriting – review & editing: Shuhan He, David Chen, Paul Chong, Jacob Schwell, Gin Mak,\nJarone Lee.\nReferences\n1. Federal rules mandatin g open notes [Internet] . 2020 [cited 2022 Feb 6]. Availab le from: https://www .\nopennotes .org/onc-fed eral-rule/\n2. Bell SK, Mejilla R, Anselmo M, Darer JD, Elmore JG, Leveille S, et al. When doctors share visit notes\nwith patients: a study of patient and doctor perceptions of documentati on errors, safety opportunitie s\nand the patient-d octor relationship. BMJ Qual Saf. 2017 Apr; 26(4):262– 70. https://doi.or g/10.1136/\nbmjqs-20 15-004697 PMID: 27193032\n3. Walker J, Leveille S, Bell S, Chimowi tz H, Dong Z, Elmore JG, et al. OpenNot es After 7 Years: Patient\nExperienc es With Ongoing Access to Their Clinicians’ Outpatient Visit Notes. J Med Internet Res. 2019\nMay 6; 21(5):e138 76. https://d oi.org/10.219 6/13876 PMID: 31066717\n4. Esch T, Mejilla R, Anselmo M, Podtschas ke B, Delbanco T, Walker J. Engaging patients throug h open\nnotes: an evaluation using mixed methods. BMJ Open. 2016 Jan 29; 6(1):e010 034. https://doi.or g/10.\n1136/bmj open-2015- 010034 PMID: 268261 54\n5. Blease C, Salmi L, DesRoch es CM. Open notes in cancer care: coming soon to patients. Lancet Oncol.\n2020 Sep; 21(9):1136 –8. https://doi.or g/10.1016/ S1470-204 5(20)3042 3-X PMID: 32888448\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 16 / 18\n6. Gerard M, Chimowi tz H, Fossa A, Bourgeois F, Fernandez L, Bell SK. The Importance of Visit Notes on\nPatient Portals for Engag ing Less Educated or Nonwhite Patients: Survey Study. J Med Internet Res.\n2018 May 24; 20(5):e191 . https:// doi.org/10.21 96/jmir.9 196 PMID: 29793900\n7. Wu DTY, Hanauer DA, Mei Q, Clark PM, An LC, Lei J, et al. Applyin g multiple methods to assess the\nreadability of a large corpus of medical documents . Stud Health Technol Inform. 2013; 192:647 –51.\nPMID: 239206 36\n8. Al Shamsi H, Almutair i AG, Al Mashrafi S, Al Kalbani T. Implications of Language Barriers for Health-\ncare: A System atic Review. Oman Med J. 2020 Mar; 35(2):e122 . https://doi.or g/10.500 1/omj.202 0.40\nPMID: 324114 17\n9. Evans RJ. Compa ring methods for the syntactic simplification of sentences in information extraction. Lit-\nerary and linguisti c computing. 2011 Aug 30; 26(4):371– 88.\n10. Evans R, Oras ˇ an C. Sentence simplification for semantic role labelling and informati on extraction.\nInProceeding s of the Internat ional Conference on Recent Advances in Natural Language Processin g\n(RANLP 2019) 2019 Sep (pp. 285–294).\n11. Lal P, Ruger S. Extract-bas ed summarization with simplification . In Proceedings of the ACL 2002 Jul.\n12. Siddhartha n A, Nenkov a A, McKeown K. Syntactic simplific ation for improving content selection in\nmulti-docu ment summarization .\n13. Silveira SB, Branco A. Combining a double clustering approac h with sentence simplifica tion to produce\nhighly informati ve multi-do cument summaries. In2012 IEEE 13th Internationa l Conferen ce on Informa-\ntion Reuse & Integrat ion (IRI) 2012 Aug 8 (pp. 482–489). IEEE.\n14. Wang T, Chen P, Rochford J, Qiang J. Text simplificat ion using neural machine translation. InProce ed-\nings of the AAAI Conference on Artificial Intelligenc e 2016 Mar 5 (Vol. 30, No. 1).\n15. Phatak A, Savage DW, Ohle R, Smith J, Mago V. Medical Text Simplifica tion Using Reinforcemen t\nLearning (TESLEA ): Deep Learning–B ased Text Simplificat ion Approach. JMIR Medical Informatic s.\n2022 Nov 18; 10(11):e38 095. https://doi.or g/10.2196/ 38095 PMID: 36399375\n16. Thirunav ukarasu AJ, Ting DS, Elangovan K, Gutierrez L, Tan TF, Ting DS. Large language models in\nmedicine. Nature medicine. 2023 Aug; 29(8):1930 –40. https://do i.org/10.1038 /s41591- 023-02448- 8\nPMID: 374607 53\n17. Lalor JP, Woolf B, Yu H. Improving Electro nic Health Record Note Comp rehension With NoteAid: Ran-\ndomized Trial of Electronic Health Record Note Comprehen sion Interventions With Crowds ourced\nWorkers. J Med Internet Res. 2019 Jan 16; 21(1):e107 93. https:// doi.org/10.21 96/10793 PMID:\n30664453\n18. Bala S, Keniston A, Burden M. Patient Percept ion of Plain-Lan guage Medical Notes Generated Using\nArtificial Intelligenc e Software: Pilot Mixed-Metho ds Study. JMIR Form Res. 2020 Jun 5; 4(6):e1667 0.\nhttps://doi.or g/10.219 6/16670 PMID: 324421 48\n19. UMass BioNLP Group. Clinical notes aid [Interne t]. [cited 2022 Feb 6]. Available from: http://sirchus.\ncom/noteaid _ann/de f_ann/note said\n20. Rolin E, Langlois Q, Watrin P, Franc ¸ ois T. FrenLyS : A tool for the automatic simplific ation of French\ngeneral language texts. InProce edings of the Interna tional Conferen ce on Recent Advances in Natural\nLanguage Processin g (RANLP 2021) 2021 Sep (pp. 1196–12 05).\n21. Alarcon R, Moreno L, Martı ´ nez P. Lexical simplificat ion system to improve web accessibility . IEEE\nAccess. 2021 Apr 12; 9:58755–67.\n22. Qiang J, Li Y, Zhu Y, Yuan Y, Wu X. LSBert: a simple framework for lexical simplification . arXiv preprint\narXiv:2006. 14939. 2020 Jun 25.\n23. Hearst MA. Automatic acquisition of hyponyms from large text corpora. In: Proceedings of the 14th con-\nference on Computationa l linguistics— Volume 2. USA: Associat ion for Computationa l Linguisti cs;\n1992. p. 539–45 . (COLING ‘92).\n24. Gooding S, Kochmar E. Complex Word Identificati on as a Sequence Labelling Task. In: Proceedings of\nthe 57th Annua l Meeting of the Assoc iation for Computationa l Linguisti cs. Florence, Italy: Association\nfor Computat ional Lingui stics; 2019. p. 1148–5 3.\n25. Neumann M, King D, Beltag y I, Ammar W. ScispaC y: Fast and Robust Models for Biomed ical Natural\nLanguage Processin g. In: Proceedings of the 18th BioNLP Workshop and Shared Task. Florence, Italy:\nAssociation for Comput ational Linguistics; 2019. p. 319–27 .\n26. Phan LN, Anibal JT, Tran H, Chanana S, Bahadrogl u E, Peltekia n A, et al. SciFive: a text-to-text trans-\nformer model for biomedica l literature [Internet]. arXiv [cs.CL]. 2021. Available from: http://arx iv.org/\nabs/2106 .03598\n27. Wang B, Komats uzaki A. mesh-tran sformer- jax: Model parallel transfor mers in JAX and Haiku [Inter-\nnet]. Github; 2021 [cited 2022 Feb 7]. Available from: https://github .com/kin goflolz/mesh -transformer-\njax\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 17 / 18\n28. Landis JR, Koch GG. The measuremen t of observer agreem ent for categorica l data. biometrics. 1977\nMar 1:159–7 4. PMID: 843571\n29. Crossley S, Heintz A, Choi JS, Batchelor J, Karimi M, Malatinsz ky A. A large-sc aled corpus for assess-\ning text readability. Behavio r Researc h Methods. 2023 Feb; 55(2):491 –507. https://doi.or g/10.375 8/\ns13428-022 -01802-x PMID: 35297016\n30. Al-Thanyy an SS, Azmi AM. Automate d text simplification : a survey. ACM Comput ing Surveys (CSUR) .\n2021 Mar 5; 54(2):1–36 .\n31. Zhuang F, Qi Z, Duan K, Xi D, Zhu Y, Zhu H, Xiong H, He Q. A comprehens ive survey on transfer learn-\ning. Proceedings of the IEEE. 2020 Jul 7; 109(1):43– 76.\n32. Alomari A, Idris N, Sabri AQ, Alsmadi I. Deep reinforc ement and transfer learning for abstractive text\nsummariz ation: A review. Computer Speech & Language . 2022 Jan 1; 71:1012 76.\n33. Vernon JA, Trujillo A, Rosen baum SJ, Debuono B, Vernon JA, Trujillo A, et al. Low health literacy: Impli-\ncations for national health policy. In: Retrieved June [Internet] . 2007 [cited 2022 Feb 7]. Availab le from:\nhttp://cites eerx.ist.psu.ed u/view doc/summ ary?doi=10 .1.1.678.7649\n34. Literacy, numeracy, and problem solving in technolog y-rich environ ments among U.s. adults: Results\nfrom the program for the internationa l assessmen t of adult competencies 2012. 2013 Oct 18 [cited 2022\nMay 31]; Available from: https://nc es.ed.gov/pu bsearch/p ubsinfo.asp ?pubid=20 14008\n35. Skreta M, Arbabi A, Wang J, Drysdale E, Kelly J, Singh D, et al. Automatic ally disambigu ating medical\nacronyms with ontology- aware deep learning. Nat Commun . 2021 Sep 7; 12(1):5319 . https:// doi.org/10.\n1038/s41 467-021-2 5578-4 PMID: 34493718\n36. Wang LW, Miller MJ, Schmitt MR, Wen FK. Assessing readability formula differences with written health\ninformati on materials: application, results, and recommen dations. Res Social Adm Pharm. 2013 Sep; 9\n(5):503–16 . https://doi. org/10.1016/j .sapharm.2 012.05.0 09 PMID: 22835706\n37. Wang J, Deng H, Liu B, Hu A, Liang J, Fan L, et al. Systematic Evaluation of Research Progress on Nat-\nural Language Processin g in Medicine Over the Past 20 Years: Bibliometric Study on PubMed . J Med\nInternet Res. 2020 Jan 23; 22(1):e168 16. https://doi.or g/10.2196/ 16816 PMID: 32012074\n38. Clear & Simple [Internet]. National Institutes of Health (NIH). 2015 [cited 2022 Feb 6]. Available from:\nhttps://www .nih.gov/in stitutes-nih /nih-offic e-director/offi ce-comm unications -public-liaiso n/clear-\ncommunic ation/clea r-simple\nPLOS DIGI TAL HEALT H\nBiomedic al text readability after hypernym substitution with large language models\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000048 9 April 16, 2024 18 / 18"
}