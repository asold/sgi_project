{
  "title": "On the Expressivity Role of LayerNorm in Transformers’ Attention",
  "url": "https://openalex.org/W4385570239",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5069099769",
      "name": "Shaked Brody",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5010209966",
      "name": "Uri Alon",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5061044883",
      "name": "Eran Yahav",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963748441",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2981040094",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W3035618147",
    "https://openalex.org/W4285142634",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970157301",
    "https://openalex.org/W4394666973"
  ],
  "abstract": "Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models.In this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. This is in contrast to the common belief that LayerNorm's only role is to normalize the activations during the forward pass, and their gradients during the backward pass.We consider a geometric interpretation of LayerNorm and show that it consists of two components: (a) projection of the input vectors to a d-1 space that is orthogonal to the [1,1,...,1] vector, and(b) scaling of all vectors to the same norm of d. We show that each of these components is important for the attention layer that follows it in Transformers:(a) projection allows the attention mechanism to create an attention query that attends to all keys equally, offloading the need to learn this operation in the attention; and(b) scaling allows each key to potentially receive the highest attention, and prevents keys from being \"un-select-able\".We show empirically that Transformers do indeed benefit from these properties of LayeNorm in general language modeling and even in computing simple functions such as \"majority\". Our code is available at https://github.com/tech-srl/layer_norm_expressivity_role .",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 14211–14221\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nOn the Expressivity Role of LayerNorm in Transformers’ Attention\nShaked Brody†, Uri Alon♠, Eran Yahav†\n†Technion, Israel\n♠Language Technologies Institute, Carnegie Mellon University, USA\n{shakedbr,yahave}@cs.technion.ac.il\nualon@cs.cmu.edu\nAbstract\nLayer Normalization (LayerNorm) is an inher-\nent component in all Transformer-based mod-\nels. In this paper, we show that LayerNorm\nis crucial to the expressivity of the multi-head\nattention layer that follows it. This is in con-\ntrast to the common belief that LayerNorm’s\nonly role is to normalize the activations during\nthe forward pass, and their gradients during the\nbackward pass.\nWe consider a geometric interpretation of\nLayerNorm and show that it consists of two\ncomponents: (a) projection of the input vectors\nto a d −1 space that is orthogonal to the\n[1,1,..., 1] vector, and (b) scaling of all vectors\nto the same norm of\n√\nd. We show that each\nof these components is important for the\nattention layer that follows it in Transformers:\n(a) projection allows the attention mechanism\nto create an attention query that attends to all\nkeys equally, offloading the need to learn this\noperation by the attention; and (b) scaling\nallows each key to potentially receive the\nhighest attention, and prevents keys from being\n“un-select-able”. We show empirically that\nTransformers do indeed benefit from these\nproperties of LayeNorm in general language\nmodeling and even in computing simple func-\ntions such as “majority”. Our code is available\nat https://github.com/tech-srl/\nlayer_norm_expressivity_role.\n1 Introduction\nLayerNorm (Ba et al., 2016) is the most commonly\nused normalization technique in modern neural net-\nworks such as Transformers (Vaswani et al., 2017).\nOriginally, Ba et al. (2016) motivated Layer-\nNorm as an efficient way of normalizing the ac-\ntivations during the forward pass or providing dis-\ntribution stability as in batch normalization (Ioffe\nand Szegedy, 2015). Later, Xu et al. (2019) and\nXiong et al. (2020) argued that more importantly\nthan normalizing forward activations, LayerNorm\nstabilizes the gradients during the backward pass.\nHowever, in this work, we show that LayerNorm,\nwhich was originally proposed for RNNs, has an\nadditional crucial role in the theoretical and prac-\ntical expressivity of the multi-head attention layer\nthat follows it in Transformers. 1 That is, Layer-\nNorm makes it easier for the Transformer to learn\ncertain functions during training.\nLayerNorm can be seen as two independent com-\nponents: projection and scaling, that were merged\ninto a single operator. First, LayerNorm projects\nits inputs onto a particular d−1 space that is or-\nthogonal to the “ones”⃗1 = [1,1,..., 1] vector. This\nallows the attention layer that follows the Layer-\nNorm to create queries that are close to ⃗1 , and thus\nattend to all keys equally, when needed, regardless\nof the identity of the keys. In Section 3 we show\nthat this projection helps, for example, computing\nthe “majority” among token types in a sequence.\nFigure 1a shows how without LayerNorm, the\nkeys and queries in a Transformer’s attention have\nno apparent geometric structure. In contrast, Fig-\nure 1b shows that LayerNorm has projected all\nkeys to the hyperplane that is orthogonal to the\n⃗1 vector. Further, the attention mechanism has\nlearned queries that are close to ⃗1 , making them\nattend equally to any possible key, when trained to\ncompute “majority”. We analyze and prove this in\nSection 4.1.\nThe second component of LayerNorm is scaling:\nWe show that LayerNorm scales the projected input\nto have an ℓ2 norm of exactly\n√\nd. In Section 3,\nwe show that scaling the input vectors prevents\nthe problem of “unselectable” keys (Demeter et al.,\n2020; Grivas et al., 2022), where some key vectors\nare contained in the convex hull formed by the other\nkeys, and thus can never get the highest attention\nscore. Figures 1c and 1d show the average fraction\n1Xiong et al. (2020) discuss the differences between plac-\ning LayerNorm before and after a Transformer layer. However,\neven when placing the LayerNorm after the layer, it appears\nright before the multi-head attention of the next layer.\n14211\nw/o LayerNorm\n-1-0.500.51 -1 -0.5 0 0.5 1\n-1\n-0.5\n0\n0.5\n1\nX Y\nZ\nkeys\nqueries\n(a) Without LayerNorm, the model has learned key and\nquery vectors without any apparent geometric structure.\nw/ LayerNorm\n-1-0.500.51 -1 -0.5 0 0.5 1\n-1\n-0.5\n0\n0.5\n1\nX Y\nZ\nkeys\nqueries\n(b) LayerNorm projects the key vectors onto the same\nhyperplane so that the model can learn to align the queries\nto be orthogonal to the keys.\n3 4 5 6 7 8 9 10 11 12 13 14 15\n10\n20\n30\n40\n50\n60\nd\nn\n(c) Without LayerNorm, there are “unselectable” key vec-\ntors that cannot be selected by getting the maximal atten-\ntion score (marked in darker colors).\n3 4 5 6 7 8 9 10 11 12 13 14 15\n10\n20\n30\n40\n50\n60\nd\nn\n0\n0.2\n0.4\n0.6\n0.8\n1\n(d) LayerNorm eliminates the problem of “unselectable”\nkey vectors: Applying LayerNorm allows any key to get\nthe highest attention score.\nFigure 1: Figures 1a and 1b show the effect of projection in LayerNorm, which makes all keys lie on the hyperplane\nthat is orthogonal to the ⃗1 vector. Figures 1c and 1d show the effect of scaling, where nis the number of vectors, d\nis the dimension, and the color represents the average fraction of “unselectable” key vectors.\n(out of 100 runs) of “unselectable” vectors which\nwere randomly drawn from the normal distribution.\nAs shown in Figure 1c, without LayerNorm, the\nprobability of getting “unselectable” keys can be\nvery high in certain settings. Nonetheless, as shown\nin Figure 1d, with LayerNorm, every key vector\nis always selectable. We analyze and prove this in\nSection 4.2.\nThese results reveal new aspects of the com-\nmonly used LayerNorm and show its importance\nto the attention mechanism in Transformers.\n2 Decomposing LayerNorm\nGiven an input x∈Rd, LayerNorm is defined as\nthe following quotient:2\ny= x−µ\nσ (1)\n2Following Xu et al. (2019) and for simplicity, we drop\nthe learned bias and gain terms.\nwhere µis the coordinate-wise average of xand σ\nis the coordinate-wise standard deviation:\nµ= 1\nd\nd∑\ni=1\nxi, σ =\n√1\nd\nd∑\ni=1\n(xi −µ)2 (2)\nµ= [µ,µ,...,µ ] ∈Rd (3)\nWe start with the numerator x−µand show\nthat it corresponds to the projection of xonto the\nhyperplane Hdefined by the normal vector ⃗1 =\n[1,1,..., 1] ∈Rd:\n(x−µ) ·⃗1 = x·⃗1 −µ·⃗1\nd∑\ni=1\nxi −\n(\n1\nd\nd∑\ni=1\nxi\n)\n·d= 0\n(4)\nThat is, x−µis always orthogonal to the ⃗1 vec-\ntor. Next, we show that the denominator scales the\n14212\nprojected vector to have a norm of exactly\n√\nd:\n||x−µ||=\n√\nd∑\ni=1\n(xi −µ)2\n=\n√\nd\n√1\nd\nd∑\ni=1\n(xi −µ)2 =\n√\nd·σ\n(5)\nThen, σin the denominator of LayerNorm scales\nthe projected vector to have a norm of exactly\n√\nd.\nLayerNorm can thus be seen as two independent\ncomponents: (a) projection of the input vectors\nonto the hyperplane orthogonal to ⃗1 , and (b) scal-\ning of the projected vectors to have a norm of\n√\nd.\n3 Expressivity Role in Attention\nEach of the components of LayerNorm supports\nthe Transformer’s attention in a different way. Pro-\njection helps creating a query that attends to all\nkeys equally, when needed, while scaling helps the\nmodel to avoid the problem of “unselectable” keys.\nRecall that in Transformers, given vectors qand\nk, the attention scoring function is defined as:\ns(q,k) =(qQ) (kK)⊤\n√\nd\n=\n(qQK⊤\n√\nd\n)\nk⊤ (6)\nFrom this point, we refer to\n(\nqQK⊤\n√\nd\n)\nas “query”,\nand to kas “key”.\n3.1 Projection\nProjecting all attention keys to the same hyperplane\ncan help attention attending to all keys equally.\nSince all projected keys are orthogonal to the hy-\nperplane’s normal ⃗1 , this fact can be exploited in\nthe training process by learning weights such that\nthe queries will be parallel to ⃗1 . That is, the atten-\ntion can learn weights such that\n(\nqQK⊤\n√\nd\n)\n≈c·⃗1 ,\nwhich will result in\n(\nqQK⊤\n√\nd\n)\n·k ≈0 and thus\ns(q,k) = 0for any key k.\nGiving an equal score to all the keys can help,\nfor example, in computing “majority”, where the\nmodel needs to find the most frequent token in the\ninput. In Section 4.1, we show that in the “majority”\ntask, a Transformer learns to align the queries to\nbe orthogonal to the keys, allowing a much faster\nconvergence.\n3.2 Scaling\nScaling the attention keys to the same size allows a\nTransformer to avoid the problem of “unselectable”\nkeys, where there are keys to which the attention\ncannot focus and give the highest score.\nLet S = {h1,..., hn−1,hn}be a set of key\nvectors, such that hn lies within the convex hull\nformed by the other vectors in S. Due to the lin-\nearity of the attention scoring function s, the atten-\ntion mechanism cannot select hn by giving it the\nhighest attention score. We formulate this in the\nfollowing theorem:\nTheorem 1. Given a set of vectors S =\n{h1,..., hn−1,hn}such thathn is interior to the\nconvex hull ofS, then for allv∈Rd (s.t. v̸= ⃗0):\nmax\ni∈[n−1]\nv⊤hi >v⊤hn\nThis means that key vectors that are inside the\nconvex hull cannot be selected by getting the high-\nest attention score. Applying LayerNorm to the\nkeys ensures that all keys are scaled to the same\nsize, and thus none of them lies inside the convex\nhull of S. This allows the attention mechanism to\npotentially focus and select any desired key. The\nproof of Theorem 1 is provided in Appendix A.\nIn Section 4.2 we show that this happens in prac-\ntice when we train a Transformer on language mod-\neling: There are key vectors that lie within the\nconvex hull and therefore cannot be selected by\nreceiving the maximal attention.\n4 Experimental Results\nIn this section, we empirically show the effects of\nthe LayerNorm components – projection and scal-\ning – on the attention mechanism in Transformers.\nWe first show how a Transformer learns to use the\nprojection of LayerNorm to compute “majority”;\nthen, we show that scaling allows the model to\navoid the problem of “unselectable” keys, allowing\nthe attention mechanism to focus on any key.\n4.1 Computing Majority\nWe demonstrate the ability to compute “majority”\nusing the projection property. In this task, the\ngoal is to predict the majority token type in a se-\nquence. Given a sequence of tokens t1,t2,...,t n ∈\n{C1,C2,...,C k}, the goal is to predict the token\ntype Ci that occurs the most among t1,t2,...,t n.\nFor a,a,b,b,b,c,c , for example, the model is\ntrained to predict the output b,b,b,b,b,b,b . Solv-\ning this task can be performed simply using exact\naveraging of the keys.\nWe trained a single-layer Transformer encoder\nwith dimension d= 8and a single attention head.\n14213\n0 1,000 2,000 3,000 4,000 5,000\n0\n1\n2\n0.2\nStep\nLoss\nw/oprojection\nw/ projection\n(a) Training loss: With projection, the model converges\nfaster compared to the model without projection, which\nrequired 3x more steps.\n0 4,000 8,000 12,000 16,000\n20◦\n40◦\n60◦\n80◦\n100◦\nStep\nAngle\nw/oprojection\nw/ projection\n(b) The mean angle of the queries to the ⃗1 vector. With\nprojection, since all keys are orthogonal to ⃗1 , the model\nhas learned to align the queries to be parallel to ⃗1 and thus\ngive equal attention to all keys.\nFigure 2: The training loss and mean angle of the\nqueries to ⃗1 in the “majority” task across 10 runs with\nand without projection.\nWe experimented with standard LayerNorm com-\npared to a LayerNorm without projection (having\na numerator of simply xin Equation (1), similarly\nto the LayerNorm variant of Zhang and Sennrich\n(2019)). We trained each model 10 times using\ndifferent random seeds.\nResults Figure 2a shows that with projection, the\nmodel converges faster compared to without pro-\njection. We hypothesize that since all key vec-\ntors are orthogonal to the ⃗1 vector, the model can\nexploit this geometric structure, and learning the\ntask is made easier. Figure 2b shows that indeed,\nthe model with projection has learned to align the\nqueries to the ⃗1 vector, decreasing the angle be-\ntween the queries and ⃗1 . On the contrary, a model\nwithout projection has to learn to solve this task\n“from scratch”. This model also converged eventu-\nally, but it required 3x more training steps. Figure 3\nshows a similar trend in the models’ test accuracies.\n4.2 Unselectable Keys\nWe examined the fraction of “unselectable” keys\nin a Transformer model with and without the scal-\n0 1,000 2,000 3,000 4,000 5,000\n0.4\n0.6\n0.8\nStep\nAccuracy\nw/ projection\nw/oprojection\nFigure 3: The test accuracy in the “majority” task across\n10 runs with and without projection. With projection the\nmodel reaches high test accuracy faster compared to the\nmodel without projection.\nModel L1 L2 L3 L4\nw/o scaling 51.0 32.2 34.7 36.8\nw/ scaling 0 0 0 0\nTable 1: The fraction of “unselectable” key vectors\nright before the attention mechanism of each layer of a\nlanguage model without scaling. LayerNorm solves the\n“unselectable” keys problem using the scaling property.\nWithout scaling, there are key vectors that cannot be\nselected by the attention mechanism.\ning component of LayerNorm using the method\npresented in Grivas et al. (2022). We trained a 4-\nlayer language model (based on GPT2 architecture\n(Radford et al., 2019)) with d= 8on Wikipedia 3\nfor 50K steps, and analyzed the inputs to the atten-\ntion layer in each layer using sequences from the\nvalidation set of SQuAD (Rajpurkar et al., 2016).\nResults Table 1 shows the following results:\nWithout scaling, there are at least 32% “unse-\nlectable” keys that cannot receive maximal atten-\ntion score in each layer. In contrast, scaling re-\nmoves this problem and allows the model to poten-\ntially focus on any key. In Figures 4a and 4b, we\nshow that this difference in “unselectable” keys is\nalso reflected in higher training and test losses for\nthe model that does not use scaling.\n5 Conclusion\nIn this paper, we show that the commonly used\nLayerNorm component is crucial not only for the\noptimization process but also for the expressivity\nof attention in Transformers. We decompose Lay-\nerNorm into two geometric operations: projecting\n3https://huggingface.co/datasets/\nwikipedia, 20220301.en split.\n14214\n10,000 20,000 30,000 40,000 50,000\n7.2\n7.4\n7.6\n7.8\n8\nStep\nLoss\nw/oscaling\nw/ scaling\n(a) Training loss: With scaling, the model converge faster\ncompared to the model without scaling.\n10,000 20,000 30,000 40,000 50,000\n7.2\n7.4\n7.6\n7.8\n8\nStep\nLoss\nw/oscaling\nw/ scaling\n(b) Test loss: With scaling, the model achieves lower test\nloss faster compared to the model without scaling.\nFigure 4: The training and test loss in the language\nmodeling task.\nthe input vectors onto a subspace that is orthogonal\nto the ⃗1 vector, and scaling the projected vectors\nto have the same norm\n√\nd.\nWe show that projection helps to compute even\nsimple tasks such as “majority” by performing an\nexact average of the keys and that scaling helps to\navoid the problem of “unselectable” keys.\nThese findings are important for the commu-\nnity’s understanding of attention and expressivity\nin Transformers. Further, these results raise a va-\nriety of follow-up questions, such as: why should\nthe keys be orthogonal to the ⃗1 vector, instead\nof any, learnable, different vector for every layer?\nAnd what would happen if we force each layer’s\nkeys to be orthogonal to multiple normal vectors?\nTo this end, we make our code publicly avail-\nable at https://github.com/tech-srl/\nlayer_norm_expressivity_role.\n6 Limitations\nIn this work, we found that the implications of the\ngeometric properties of LayerNorm affect mainly\nsmall models and are less evident for larger models.\nWe hypothesize that with a large hidden dimen-\nsion, a Transformer model can find other solutions\nfor computing “majority“ using gradient descent\nand is, therefore, less dependent on the projection\ncomponent. Further, we believe that the scaling\ncomponent is less useful for high dimensional mod-\nels, since with higher dimensions, it is less likely\nto encounter a set of vectors where some of them\nlie within the convex hull of the others. Therefore,\nwe encourage the community to use LayerNorm\nbefore attention layers, especially for small models\nthat operate on long sequences.\nMoreover, the projection component is clearly\na linear operator that can be expressed by a lin-\near layer before the LayerNorm, as we show in\nAppendix C. Nevertheless, the importance of the\nprojection holds as we discuss in Section 3, and the\nbenefit of using this operator explicitly in Layer-\nNorm is shown in Section 4.1.\nAcknowledgements\nWe thank Gail Weiss for the helpful discussions.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. ArXiv preprint,\nabs/1607.06450.\nDavid Demeter, Gregory Kimmel, and Doug Downey.\n2020. Stolen probability: A structural weakness of\nneural language models. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2191–2197, Online. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nWikimedia Foundation. Wikimedia downloads.\nAndreas Grivas, Nikolay Bogoychev, and Adam Lopez.\n2022. Low-rank softmax can have unargmaxable\nclasses in theory but rarely in practice. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 6738–6758, Dublin, Ireland. Association\nfor Computational Linguistics.\nSergey Ioffe and Christian Szegedy. 2015. Batch nor-\nmalization: Accelerating deep network training by\n14215\nreducing internal covariate shift. In Proceedings\nof the 32nd International Conference on Machine\nLearning, ICML 2015, Lille, France, 6-11 July 2015,\nvolume 37 of JMLR Workshop and Conference Pro-\nceedings, pages 448–456. JMLR.org.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. arXiv e-prints,\npage arXiv:1606.05250.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng,\nShuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tie-Yan Liu. 2020. On layer\nnormalization in the transformer architecture. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 10524–10533. PMLR.\nJingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao,\nand Junyang Lin. 2019. Understanding and improv-\ning layer normalization. In Advances in Neural In-\nformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 4383–4393.\nBiao Zhang and Rico Sennrich. 2019. Root mean square\nlayer normalization. Advances in Neural Information\nProcessing Systems, 32.\nA Proof of Theorem 1\nTheorem 1. Given a set of vectors S =\n{h1,..., hn−1,hn}such thathn is interior to the\nconvex hull ofS, then for allv∈Rd (s.t. v̸= ⃗0):\nmax\ni∈[n−1]\nv⊤hi >v⊤hn\nWe prove Theorem 1 using Theorem 2, presented\nand proved in Demeter et al. (2020):\nTheorem 2(Demeter et al. (2020)). Let Cbe the\nconvex hull of the embeddings{xi}of a vocab-\nulary V. If an embeddingxi for word wi ∈V\nis interior to C, then the maximum probability\nP(wi) assigned towi using a dot-product softmax\nis bounded by the probability assigned to at least\none wordwi whose embedding is on the convex\nhull.\nProof of Theorem 1.According to Theorem 2,\nsince hn is interior to the convex hull of S, the\nmaximum probability assigned to hn is bounded\nby the maximum probability assigned to hi for\nsome i ∈[1 −n], that is on the convex hull of\nS. Since probability in Transformers is computed\nas a dot-product of the final hidden state of the\nTransformer uand the embedding vector, we can\nwrite:\nu⊤hi >u⊤hn (7)\nfor any u ∈Rd (the probability is computed as\na softmax of the dot-product logits, but since soft-\nmax is a monotonic function, a higher probability\nafter the softmax necessarily implies a higher logit\nscore).\nTherefore, it also implies that for any v= u∈\nRd:\nmax\ni∈[n−1]\nv⊤hi >v⊤hn (8)\nB Characteristics of the Normalized\nVectors\nIn this section, we discuss the characteristics of\nthe LayerNorm inputs that are been normalized to\nthe same point. Recall that the projection ensures\nthat the normalized output lies on the hyperplane\nHdefined by the normal vector ⃗1 = [1,1,..., 1] ∈\nRd.\nLet vbe a unit vector in H:\nv⊥⃗1 ∧||v||= 1 (9)\nTherefore\nd∑\ni=1\nvi = 0 (10)\nLet Mbe a 2D planethat is defined using vand\n⃗1 . Its parametric representation is:\nM: sv+ t⃗1 (11)\nFinally, let xbe a vector in M. Therefore, there\nexist α,β ∈R such that\nx= αv+ β⃗1 (12)\n14216\n-5 0 5 -5 0 5\n-5\n0\n5\n⃗1\nv\nX Y\nZ\nM\nH\nFigure 5: LayerNorm maps the points of Mto exactly\ntwo points in H.\nNext, we apply LayerNorm to x. First we project\nxonto H:\nx−µ=\n= αv+ β⃗1 −1\nd\nd∑\ni=1\n(αvi + β) ⃗1\n= αv+ β⃗1 −α\n(\n1\nd\nd∑\ni=1\nvi\n)\n⃗1 −β⃗1\n= αv−α\n\n\n1\nd\nd∑\ni=1\nvi\n\n=0\n\n\n⃗1\n= αv\n(13)\nThen, we scale the projected vector to be with a\nnorm of\n√\ndand get\nLayerNorm (x) =\n√\nd αv\n||αv|| (14)\nWe split to cases:4\nLayerNorm (x) =\n{√\ndv α> 0\n−\n√\ndv α< 0 (15)\nTo conclude, all vectors belonging to the 2D plane\nMare normalized to exactly two points, depending\non their αcomponent.\nSince the subspaces Hand\n{\nt⃗1 |t∈R\n}\nare di-\nrect sum of the whole space Rd, each vector u∈\nRd has a unique representation as u= αv+ β⃗1 .\nThat is, each vector u∈Rd belongs to some 2D\nplane M, defined by ⃗1 and some vector v ∈H,\n4As implied from the original formulation of LayerNorm\n(Ba et al., 2016), LayerNorm is undefined for α= 0.\nand thus we can characterize the set of points that\nare being normalized to the same point. Figure 5\nillustrates this behavior.\nC Constructing the Projection\nThe projection of LayerNorm is a linear transfor-\nmation, and thus, in this section, we show explicitly\nthe construction of the projection matrix P, such\nthat\nPx = x−µ (16)\nLet V = Rd, W =\n{\nα⃗1 |α∈R\n}\n, and U ={\nx|x⊥⃗1 = [1,1,..., 1] ∈Rd\n}\nbe linear subspaces\nof V.\nLet BU ,BW be the bases of U and W respec-\ntively:\nBU = {u1,u2,..., ud−1} (17)\nBW =\n{\n⃗1\n}\n(18)\nWe can define a basis C = BU ∪BW of V.\nWe also denote the standard basis Eof V:\nE = {e1,e2,..., ed} (19)\nSince U∩W = {0}, we have that U⊕W = V\n(direct sum). Therefore, each x∈V has a unique\nrepresentation as x= u+ wwhere u∈U and\nw∈W.\nWe can also write xusing BU ,BW :\nx= α⃗1 +\nd−1∑\ni=1\nβiui (20)\nSince we look for the projection of xonto U in\nthe direction of W, we want that\nPx =\nd−1∑\ni\nβiui (21)\nTo achieve this, we first, change the basis of V\nfrom Eto C, remove the α⃗1 component, and then\nchange back to the standard basis E.\nLet MC\nE be the change of basis matrix from\nbasis C to the standard basis E:\nMC\nE =\n\n\n| | | |\nu1 u2 ... ud−1 ⃗1\n| | | |\n\n (22)\nTherefore\nP = MC\nE A\n(\nMC\nE\n)−1\n(23)\n14217\nWhere\nA=\n\n\n| | | |\ne1 e2 ... ed−1 0\n| | | |\n\n (24)\nTo get an explicit P ∈Rd×d, we instantiate the\nbasis BU :\nBU =\n\n\n\n\n\n1 −d\n1\n...\n1\n1\n\n\n,\n\n\n1\n1 −d\n...\n1\n1\n\n\n,...,\n\n\n1\n1\n...\n1 −d\n1\n\n\n\n\n\n(25)\nTherefore\nMC\nE =\n\n\n1 −d 1 ··· 1 1\n1 1 −d ··· 1 1\n... ... ... ... ...\n1 1 ··· 1 −d 1\n1 1 ··· 1 1\n\n\n(26)\n(\nMC\nE\n)−1\n= 1\nd\n\n\n−1 1\n−1 1\n... ...\n−1 1\n1 1 ··· 1 1\n\n\n(27)\nAnd we get\nP = 1\nd\n\n\nd−1 −1 ··· − 1\n−1 d−1 ··· − 1\n... ... ... ...\n−1 −1 ··· d−1\n\n (28)\nD Unselectable Keys\nTable 2 shows the fraction of “unselectable” keys\nin a language model with LayerNorm before and\nafter the application of LayerNorm.\nL1 L2 L3 L4\nBefore LayerNorm 44.8 28.5 22.3 26.1\nAfter LayerNorm 0 0 0 0\nTable 2: The fraction of “unselectable” key vectors be-\nfore and after the LayerNorm followed by the attention\nmechanism of each layer of a language model. Layer-\nNorm solves the “unselectable” keys problem using the\nscaling property.\nTo illustrate the impact of ”unselectable” tokens,\nwe give some examples from the validation set of\nStanford TreeBank (Socher et al., 2013), which is\nused as a benchmark for the sentiment analysis task.\nOur results show that important tokens may be ”un-\nselectable”. We highlighted in bold any token that\nis ”unselectable’ in at least one of the layers. Ta-\nble 3 shows the results of running a language model\nwithout LayerNorm (Section 4.2) on the validation\nset. We also trained a 4-layer Transformer encoder\nwithout LayerNorms (based on BERT architecture\n(Devlin et al., 2019)) on the sentiment analysis task.\nTable 4 shows the results of running this model on\nthe validation set.\nE Experimental Setup\nIn this section, we detail the setup of the experi-\nments shown in Section 4.\nE.1 Majority\nIn the experiments, we used a learning rate of0.001\nwith a linear scheduler, a hidden size ofd= 8(total\nof 584 learnable parameters), a batch size of 6000,\na sequence length of 50, 20 different classes, and\nthe Adam optimizer. We trained the models for\n1000 epochs consisting of 17K steps.\nThe “majority” dataset contains 80K training\nexamples and 20K test examples. Each example\nis a sequence of length 50 consisting of tokens\nbelonging to one of 20 different classes.\nE.2 Language Modeling\nWe trained a language model with GPT2 architec-\nture (Radford et al., 2019) using the Huggingface\nlibrary, on the Huggingface processed Wikipedia\ndataset (20220301.en split, licenses CC BY-SA and\nCC BY-SA) (Foundation), and tested it on SQuAD\n(Rajpurkar et al., 2016) (license CC BY-SA 4.0).\nWe used these datasets only to demonstrate the\n“unselectable” keys problem, and thus we did not\nviolate any of their license conditions. We used\nthe same hyperparameters as Radford et al. (2019),\nexcept that we used a hidden size of 8, 4 hidden\nlayers, a learning rate of 5e-5, and a window size\nof 1024 tokens, resulting in a model with 414K\nlearnable parameters. We train the model on the\nWikipedia dataset (6.5M examples) for 50K steps\nand report our findings on 1000 randomly selected\nexamples from the validation set of SQuAD.\n14218\nwhether you like rap music or loathe it, you can’t deny either the tragic loss of two young men\nin the prime of their talent or the power of this movie.\nit is great summer fun to watch arnold and his buddy gerald bounce off a quirky cast of\ncharacters.\nthe lion king was a roaring success when it was released eight years ago, but on imax it seems\nbetter, not just bigger.\nit provides the grand, intelligent entertainment of a superior cast playing smart people amid\na compelling plot.\nsome of their jokes work, but most fail miserably and in the end, pumpkin is far more\noffensive than it is funny.\nTable 3: Examples from the validation set of Stanford TreeBank (Socher et al., 2013). Any token that is “unselectable’\nin at least one of the layers of the language model (Section 4.2) is marked in bold.\nit’s hard to like a film about a guy who is utterly unlikeable, and shiner, starring michael caine\nas an aging british boxing promoter desperate for a taste of fame and fortune, is certainly that.\nyou’ll gasp appalled and laugh outraged and possibly, watching the spectacle of a promising\nyoung lad treading desperately in a nasty sea, shed an errant tear.\nthis is wild surreal stuff, but brilliant and the camera just kind of sits there and lets you look\nat this and its like you’re going from one room to the next and none of them have any relation\nto the other.\nit’s a much more emotional journey than what shyamalan has given us in his past two movies,\nand gibson, stepping in for bruce willis, is the perfect actor to take us on the trip.\nalthough german cooking does not come readily to mind when considering the world’s best\ncuisine, mostly martha could make deutchland a popular destination for hungry tourists.\nTable 4: Examples from the validation set of Stanford TreeBank (Socher et al., 2013). Each bold token is\n”unselectable’ in at least one of the layers of a Transformer encoder without LayerNorm, trained on the sentiment\nanalysis task. These examples show that important tokens that may be necessary for the task are ”unselectable”,\nwhich may affect the encoder’s ability to learn the task correctly.\nE.3 Sentiment Analysis Task\nWe trained a Transformer encoder with BERT ar-\nchitecture (Devlin et al., 2019) without LayerNorm\nlayers with 50K steps on the Stanford TreeBank\ndataset (Socher et al., 2013) (Table 4). We used\nthe same hyperparameters as Devlin et al. (2019),\nexcept that we used a hidden size of 8, 4 hidden\nlayers, and a learning rate of 5e-5, resulting in a\nmodel with 446K learnable parameters.\n14219\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n6\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Our work is about the expressivity of LayerNorm in Transformers, and thus it does\nnot raise any risks that did not exist in the widely used Transformers in general.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nF\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nF\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nF\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nF\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nF\nC □\u0013 Did you run computational experiments?\n4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nF\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n14220\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nF\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nF\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n14221",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6915028095245361
    },
    {
      "name": "Transformer",
      "score": 0.6028719544410706
    },
    {
      "name": "Scaling",
      "score": 0.5612857341766357
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.5095934867858887
    },
    {
      "name": "Norm (philosophy)",
      "score": 0.4596414864063263
    },
    {
      "name": "Theoretical computer science",
      "score": 0.41494283080101013
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3507136404514313
    },
    {
      "name": "Algorithm",
      "score": 0.35025838017463684
    },
    {
      "name": "Mathematics",
      "score": 0.1886974573135376
    },
    {
      "name": "Voltage",
      "score": 0.12091323733329773
    },
    {
      "name": "Geometry",
      "score": 0.10131090879440308
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I174306211",
      "name": "Technion – Israel Institute of Technology",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ]
}