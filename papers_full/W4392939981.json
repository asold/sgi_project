{
  "title": "Enhanced Crop Disease Detection With EfficientNet Convolutional Group-Wise Transformer",
  "url": "https://openalex.org/W4392939981",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2052900427",
      "name": "Jing Feng",
      "affiliations": [
        "Universiti Sains Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A2132234270",
      "name": "WEN ENG ONG",
      "affiliations": [
        "Universiti Sains Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A2359215071",
      "name": "Wen Chean Teh",
      "affiliations": [
        "Universiti Sains Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A311940369",
      "name": "Rui Zhang",
      "affiliations": [
        "Universiti Sains Malaysia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962949934",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W3039201839",
    "https://openalex.org/W3172544793",
    "https://openalex.org/W4283073926",
    "https://openalex.org/W3198995987",
    "https://openalex.org/W4292829111",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2732026016",
    "https://openalex.org/W4388962295",
    "https://openalex.org/W4385737810",
    "https://openalex.org/W3097065222",
    "https://openalex.org/W3188427387",
    "https://openalex.org/W4385346076",
    "https://openalex.org/W4386075796",
    "https://openalex.org/W3179658769",
    "https://openalex.org/W3015562698",
    "https://openalex.org/W6777919652",
    "https://openalex.org/W3010225408",
    "https://openalex.org/W2729018917",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W4289752563",
    "https://openalex.org/W3127736148",
    "https://openalex.org/W4223961890",
    "https://openalex.org/W4286233849",
    "https://openalex.org/W4210839135",
    "https://openalex.org/W4303627739",
    "https://openalex.org/W4210247145",
    "https://openalex.org/W4284670866",
    "https://openalex.org/W4321189555",
    "https://openalex.org/W4310454508",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W3034368386",
    "https://openalex.org/W6839793511",
    "https://openalex.org/W4224304134",
    "https://openalex.org/W2333362108",
    "https://openalex.org/W4382990167",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W2395579298",
    "https://openalex.org/W4285787431",
    "https://openalex.org/W3026623411",
    "https://openalex.org/W3138154797"
  ],
  "abstract": "Crop diseases, as one of the major problems in global agricultural production, lead to crop yield reduction, death, and even total extinction, with serious impacts on farmers and the food supply. Traditionally, crop diseases are identified by visual inspection and based on the experience of farmers and agricultural experts, a method that not only consumes human resources but also has a certain degree of subjectivity and inaccuracy. The development of artificial intelligence technology successfully achieves real-time monitoring, automatic identification, and intelligent decision by combining the Internet of Things (IoT) technology and cloud computing technology. Herein, we proposed an EfficientNet Convolutional Group-Wise Transformer (EGWT) architecture. The local features of crop leaf images are extracted by EfficientNet convolution and then input into a group-wise transformer architecture. In the group-wise transformer process, the input features are divided into multiple groups. An attention mechanism is used within each group to calculate correlations between features. After calculating the intra-group attention, the output features of each group are stitched together to form the final output features. Our proposed model achieves 99.8&#x0025; accuracy on the PlantVillage dataset, 86.9&#x0025; accuracy on the cassava dataset, and 99.4&#x0025; accuracy on the Tomato leaves dataset, with the least number of parameters 23.04M in the state-of-the-art convolutional combinatorial transformer hybrid model. The experimental results indicate that the proposed model has the best accuracy and optimal model complexity so far compared to other neural networks based on CNN, transformer, and the hybrid structure of CNN and transformer.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nEnhanced Crop Disease Detection with\nEfficientNet Convolutional Group-Wise\nTransformer\nJING FENG1,2, (Member,IEEE), WEN ENG ONG1, WEN CHEAN TEH1, AND RUI ZHANG1,2\n1School of Mathematical Sciences, Universiti Sains Malaysia, 11800 USM Penang, Malaysia\n2Department of Science, Taiyuan Institute of Technology, Taiyuan, 030008, China\nCorresponding author: Wen Eng Ong(e-mail: weneng@usm.my).\nThis work was supported in part by the Department of Technology, Taiyuan Institute of Technology of China under Grant 2020LG06.\nABSTRACT Crop diseases, as one of the major problems in global agricultural production, lead to crop yield\nreduction, death, and even total extinction, with serious impacts on farmers and the food supply. Traditionally,\ncrop diseases are identified by visual inspection and based on the experience of farmers and agricultural\nexperts, a method that not only consumes human resources but also has a certain degree of subjectivity and\ninaccuracy. The development of artificial intelligence technology successfully achieves real-time monitoring,\nautomatic identification, and intelligent decision by combining the Internet of Things (IoT) technology and\ncloud computing technology. Herein, we proposed an EfficientNet Convolutional Group-Wise Transformer\n(EGWT) architecture. The local features of crop leaf images are extracted by EfficientNet convolution and\nthen input into a group-wise transformer architecture. In the group-wise transformer process, the input\nfeatures are divided into multiple groups. An attention mechanism is used within each group to calculate\ncorrelations between features. After calculating the intra-group attention, the output features of each group\nare stitched together to form the final output features. Our proposed model achieves 99.8% accuracy on the\nPlantVillage dataset, 86.9% accuracy on the cassava dataset, and 99.4% accuracy on the Tomato leaves\ndataset, with the least number of parameters 23.04M in the state-of-the-art convolutional combinatorial\ntransformer hybrid model. The experimental results indicate that the proposed model has the best accuracy\nand optimal model complexity so far compared to other neural networks based on CNN, transformer, and\nthe hybrid structure of CNN and transformer.\nINDEX TERMS Convolutional Neural Network, Group-Wise Transformer, Crop Disease Detection\nI. INTRODUCTION\nI\nT is widely recognized that early detection and precise\ndiagnosis of crop disease play a pivotal role in safeguard-\ning crop growth and enhancing yield. Effective detection and\nmanagement of crop diseases can improve crop yield and\nquality, reduce resource wastage, and protect agroecosys-\ntems. Among the array of approaches available, leveraging\ndeep learning models for crop image detection stands out as\nan exceptionally potent method. Since 2012, Convolutional\nNeural Networks (CNN) [1] have undergone continuous up-\ndates and iterations, securing a prominent place in computer\nvision tasks. More recently, there has been a dramatic increase\nin vision transformers (ViT) [2] neural networks. Both CNN\nand ViT have carved a niche for themselves in the realm of\ncrop disease detection [3], [4], exhibiting remarkable success\nin accuracy.\nDespite the streamlined architecture embraced by models\nlike CNN, ViT, and the synergistic amalgamation of CNN\nand transformer structures [5]–[7], these models consistently\nattain cutting-edge performance levels after pre-trained on\nextensive public datasets such as ImageNet [8], Common\nObjects in Context (COCO) [9] and MIT Places [10]. A\npivotal challenge woven into deep learning models is their\nsubstantial parameter count. This phenomenon is particularly\npronounced in the case of ViT. As performance scales up-\nwards, a parallel surge in the number of parameters harnessed\nby these models becomes evident. There is a further prob-\nlem with insufficient data volume, which may result in deep\nnetwork models being limited in learning and generalization.\nHowever, inadequate data on crop leaves may make it difficult\nfor the model to capture the various variations and stages of\ndisease manifestation.\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nA plethora of innovative studies on the Vision Transformer\n(ViT) structure is noteworthy, propelling the advancements\nof deep learning on various fronts. Researchers have con-\ntinuously enhanced ViT’s image feature extraction capabil-\nities by introducing more efficient Transformer variants [11],\nsuch as Convolutional Visual Transformer (CVIT) and Pyra-\nmid Layered Networks (PDN) [12]. Novel attention mech-\nanisms [13]–[15] have been designed to comprehensively\nanalyze image information, facilitating more precise target\nrecognition. The lightweighting research of Vision Trans-\nformer (ViT) has become a focal point for scholars in recent\nyears. One of the most iconic efficient transformer mod-\nels, which has gained significant attention, incorporates the\nshifted windows attention mechanism [16], achieving effi-\ncient processing of large-scale images through hierarchical\nattention [17]. In this context, the lightweight design of the\nmodel is a primary concern in our study, particularly in the\ncontext of crop disease detection.\nIn agriculture, deep neural networks and transfer learning\nhave gained significant traction over the last decade. These\napproaches have shown immense promise in the realm of\ncrop disease identification, a subject that has garnered nu-\nmerous compelling studies [18]–[20]. CNNs are widely used\nfor the automatic identification of crop diseases due to their\nexcellent image feature extraction capability. Incorporating\nsequential convolution and pooling operations, CNN has\nthe capability to capture diverse local features at different\nscales [21]. Simultaneously, transformer-based neural net-\nworks have achieved great success in the field of natural lan-\nguage processing [22] and have been gradually introduced to\ncrop disease recognition tasks. The self-attention mechanism\ninherent to transformer networks empowers them to holisti-\ncally analyze information from different local image regions,\nunveiling a richer panorama of global context and spatial\nrelationships. This enhancement further elevates the precision\nof disease recognition. Through innovative techniques such as\ngrouping and cross-channel attention [23], [24], transformer-\nbased methods are gradually showing strong potential in\ncrop disease detection. However, few studies have focused\non model complexity and high demands on computational\nresources.\nIn ViT, a significant number of multi-head attention mech-\nanisms are introduced. However, the extensive self-attention\ncomputations lead to a substantial increase in model parame-\nters, resulting in a larger model size. Consequently, such large\nmodels typically necessitate extensive pre-training on large-\nscale datasets to achieve satisfactory generalization perfor-\nmance. This requirement, in turn, leads to a greater demand\nfor computational resources during both the training and\ninference phases compared to CNNs. Therefore, by grouping\nthe multi-head attention mechanism in the Transformer, we\npartition the input sequence into several groups, where each\nelement within a group interacts solely with others in the\nsame group. Distinct attention mechanisms are employed\nbetween different groups, with parameters shared within each\ngroup. The independent attention across groups facilitates\nthe model in capturing information at diverse hierarchical\nlevels and scales. The introduction of grouping enhances the\ncomputational efficiency of the Transformer, particularly in\nhandling long sequences, resulting in lower computational\ncomplexity compared to the standard Transformer. The in-\nnovative grouping paradigm is implemented in both CNN\nand ViT architectures. A synergistic integration of Depth-\nWise Separable Convolution and Group-Wise Transformer\n(EGWT) is proposed for the detection of crop diseases. This\nintegration aims to reduce the computational complexity of\nthe model, thereby enhancing its overall efficiency.\nOverall, we summarize our contributions as follows:\n• To the best of our knowledge, this is the first work to\namalgamate the depth-wise separable convolution struc-\nture with the group-wise transformer structure, employ-\ning it in the domain of crop disease recognition. We\nsubstantiate, from a mathematical standpoint, the bal-\nanced performance and parameter efficiency of employ-\ning this model architecture. This parameter efficiency\nholds particular value for deploying models in resource-\nconstrained agricultural settings.\n• We show that the EGWT architecture, with a group-\nwise vision transformer, enhances information richness\nand diminishes redundancy in the context of crop dis-\nease identification. By employing group convolution and\ngroup transformer mechanisms, we are able to maintain\nfeature extraction performance while achieving weight\nsharing within the group.\n• In the context of crop disease identification using small-\nscale real-world datasets, we employ the transfer learn-\ning approach to mitigate the issue of overfitting.\nII. RELATED WORKS\nThe field of deep neural networks has witnessed substantial\nadvancements in recent years. In the domain of crop leaf\nimage recognition, the integration of CNN and transformer-\nbased networks has also garnered significant attention [25].\nCrop leaf images typically exhibit intricate textures, shapes,\nand structures, necessitating multi-level and multi-scale fea-\nture extraction and representation methods. While CNN ex-\ncel in image processing by effectively capturing local fea-\ntures [26], transformer-based networks are adept at capturing\nglobal contextual information [27], contributing to enhanced\naccuracy in image recognition. In this section, we present\nan overview of the existing literature related to crop disease\nidentification, highlighting key contributions and approaches\ntaken by researchers.\nSeveral recent advancements have been dedicated to de-\nsigning efficient transformer networks, especially in the con-\ntext of crop leaf disease identification [28]–[32]. [33] inte-\ngrated the Inception neural network and vision transformer\n(ICVT) for achieving notable advancements in plant disease\nidentification tasks. However, the complexity of the deep\nnetwork structure employed by this method might demand\nincreased computational resources and training time, par-\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nticularly in scenarios involving larger datasets. The ICVT\narchitecture is designed based on the Convolutional Vision\nTransformer (CvT) [34] structure, which combines CNN and\nViT concepts. It utilized convolutional operations to divide\neach input image into equally sized patches. These patches\nthen undergo depth-wise separable convolution operations\nto flatten them into the query, key, and value matrices cor-\nresponding to the attention mechanism, before entering the\nmulti-head attention layer. Although this architecture com-\nbines the strengths of both CNN and ViT, there is no im-\nprovement in reducing the parameter-intensive attention and\nMultilayer Perceptron (MLP) layers.\nThe lightweight design of transformer-based architectures\nhas been a topic of interest for many researchers. Touvron\net al. [35] introduced the Data-efficient Image Transformers\n(DeiT) model, which employs knowledge distillation tech-\nniques [36] within the self-attention mechanism to achieve\nefficient training and inference. While DeiT has made signif-\nicant strides in data efficiency, its performance may still be\nsubject to limitations based on the amount of training data in\ncertain datasets and tasks. A \"cross-feature attention\" mech-\nanism was designed by [37], which reduces computation\ncost for transformers by handling attention via feature di-\nmension and produces only linear complexity. Luo et al. [38]\nintroduced a group-wise transformer to Vision-and-Language\ntasks, which reduces both the parameters and computations of\nthe transformer, while also preserving its two main properties,\ni.e., the efficient attention modeling on diverse subspaces of\nMHA, and the expanding-scaling feature transformation of\nMLP.\nAmong the aforementioned deep learning-based method-\nologies shown in table 1, namely the group-wise transformer,\nICVT, and CvT, the closest counterparts to EGWT can be\nidentified. The CvT architecture predominantly relies on con-\nventional ViT blocks and convolutional operations. In [33],\nthe ICVT emerges from the fusion of inception blocks and\nthe ViT framework to address grape leaf disease classification\nin real-world scenarios. In both these references, ViT and\nconvolution function as independent modules. Convolutional\noperations serve image preprocessing and feature extraction\npurposes, while the ViT module facilitates self-visual atten-\ntion learning and final class identification.\nIn contrast, our EGWT model integrates convolutional pro-\njection and the EfficientNet transformer into the ViT structure\nto enhance information richness and attain refined feature\nlearning. In [34], the CvT network is employed for general\nobject image classification, whereas our approach targets\ncrop disease identification. The task of crop disease identifi-\ncation necessitates heightened sensitivity towards subtle dif-\nferentiations in leaf images’ features. Hence, we strategically\nembed the EfficientNet transformer block within stage 3 of\nthe EGWT model for fine-grained feature extraction. Con-\ncurrently, we adapt the count and dimensions of convolutional\nkernels within convolution projection blocks to enhance focus\non crop disease-affected regions.\nIII. MATERIAL AND METHODS\nFIGURE 1. The overview architecture of the EGWT method.\nA. IMAGE DATASETS\nTo design an efficient CNN and transformer-based neural net-\nwork model, we chose to train and test it using three publicly\navailable crop datasets. These datasets consist of authentic\nand captured crop images, providing a comprehensive rep-\nresentation of different crop species and disease categories.\nThe first dataset is PlantVillage [39], which holds a pivotal\nrole in computer vision for crop disease detection and classi-\nfication. Developed by agricultural experts, this dataset cap-\ntures a diverse range of locations and environments, featuring\n54,305 plant images, each sized 256 × 256 pixels. Encom-\npassing 38 distinct crop diseases across 14 plant species, each\nimage is paired with a disease label. Some sample images\nare shown in Figure 2. This dataset presents challenges such\nas viewpoint, lighting, growth stage, and disease severity\nvariations, demanding robust generalization from models.\nThe ability to adeptly handle these diversities and accurately\nclassify crop diseases is paramount.\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nTABLE 1. literature Summary For Existing Works\nReference Method Description Dataset Limitation\n[28]–[32] CNN + Transformer Integration of CNN and Trans-\nformer for crop leaf image recogni-\ntion\nCrop leaf datasets Extensive attention mechanisms\nleading to increased computational\ncosts and reduced prediction speed\n[33] ICVT (Inception + Vision\nTransformer)\nIntegration of Inception neural net-\nwork and Vision Transformer for\nplant disease identification\nPlantVillages, ibean,\nAI2018, PlantDoc\nHigh computational demand for\ndeep network structure\n[35] DeiT (Data-efficient Im-\nage Transformers)\nBy introducing attention mecha-\nnisms and knowledge distillation to\ntraining effectively\nImageNet To perform knowledge distillation,\na powerful teacher model is re-\nquired as a reference\n[37] Cross-feature Attention Mechanism to reduce computation\ncost for transformers via feature di-\nmension\nImageNet-1K A certain degree of accuracy loss\n[38] Group-wise Transformer Reduces parameters and computa-\ntions of transformer while preserv-\ning main properties\nVQA val, COCO Cap\nKarpathy, RefCOCO val\nSpecific Vision-and-Language\ntasks\nFIGURE 2. Sample images of crop disease on the PlantVillage dataset [39].\nThe second dataset used in this study is the cassava\ndataset [40], which originally served as the primary dataset\nfor the cassava diseases classification competition. This\ndataset comprises 26,337 labeled images that were collected\nduring routine surveys conducted in Uganda. These image\nsizes include 128×128, 256×256, 384×384, 512×512. The\nmajority of these images were captured in agricultural fields\nby local farmers and subsequently annotated by experts from\nthe National Crop Resource Research Institute (NaCRRI), in\ncollaboration with the Artificial Intelligence Laboratory at\nMakerere University in Kampala. The cassava leaf disease\ncategories are shown in Figure 3 for reference.\nThe third dataset shown in Figure 5 utilized in this study\nconsists of 32,531 tomato leaf images [41], categorized into\n11 distinct classes. Within these classes, one signifies the\ncategory of healthy tomato leaves, whereas the remaining\n10 encompass images of tomato leaves afflicted by diverse\ndisease types. The dataset encompasses images acquired from\nboth laboratory and field settings. The images are formatted\nas .jpg files, with dimensions of 256 × 256 pixels and a\nresolution of 96 dots per inch (dpi).\nHowever, the category distribution of the cassava dataset\nis imbalanced, as shown in Figure 4. To address this is-\nsue, we applied the synthetic minority over-sampling tech-\nnique(SMOTE) [42] to preprocess the data before conducting\nmodel training. The SMOTE method generates new samples\nfor the minority class through interpolation. Initially, a mi-\nnority class sample is randomly selected, followed by the\nrandom selection of a sample from its k nearest neighbors.\nUltimately, a new sample is generated along the line con-\nnecting these two samples. Compared to other methods for\nbalancing samples, SMOTE balances the class distribution of\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nFIGURE 3. Sample images of crop disease on the cassava dataset [40].\nFIGURE 4. The number of disease categories in cassava dataset.\nthe dataset by generating synthetic samples for the minority\nclass. This method not only outperforms other algorithms\nin terms of performance but also has lower computational\ncomplexity [43].\nB. METHODS\nAs illustrated in Figure 1, we introduce a novel concept that\ninvolves organizing different components of the ViT archi-\ntecture into groups. This strategy encompasses the reduc-\ntion of transformer parameters through the implementation\nof weight-sharing mechanisms within these groups. Impor-\ntantly, these reductions are accomplished while upholding the\nmodel’s performance standards.\nIn crop disease identification, there are often subtle visual\ndifferences in the appearance of various types of diseases. Ef-\nfectively learning the fine-grained features of disease images\nis crucial for accurate crop disease recognition. Therefore,\nthe model needs to pay more attention to subtle local differ-\nences in order to better identify crop diseases. We incorporate\nEfficientNet-B0 [44] architecture into the Group-wise trans-\nformer network as shown in Figure 1, which fuse the CNN and\nViT can provide a more comprehensive set of information,\nenhancing the recognition accuracy of minute details and\ncontextual relationships in fine-grained images. Meanwhile,\nan efficient architecture is introduced to reduce the model’s\nsize and enhance computational efficiency. Table 2 shows\nthe details of the EGWT model, 224 × 224 is the default\ninput image size. Hi represents the number of attention heads\nin the i-th stage, ngroups represents the number of groups, Ri\ndenotes the amplification factor of the i-th stage’s i-th layer\nfeatures, and Ni denotes the number of convolutional Group-\nwise transformer blocks.\n1) Depth-Wise Split Token Embedding(DWTE)\nIn our study, we utilized Depth-Wise separable Convolu-\ntion(DWConv) [45], as shown in Figure 6(a), to split the\ninput image or the previous layer’s output feature map X into\nsmaller patches as token maps, the image size is H ×W ×C.\nH, W , and C represent the height, width, and number of\nchannels of the input image, respectively. Choose convolution\nkernels of size S × S that match the number of input image\nchannels. Convolution operations are conducted individually\nbetween different channels and the convolution kernels, using\na one-to-one correspondence, with a stride of s, where S > s.\nThe output feature maps undergo feature extraction through\na 1 × 1 convolutional kernel to obtain the output, which is\nthe input of transformer blocks. Unlike the traditional ViT\narchitecture, we do not sum the ad-hoc position embedding\nto the tokens, instead, we implemented a depth-wise convo-\nlution with overlapping patches and reshaped the tokens into\na 2D spatial grid as the input, which not only preserves the\ntoken boundary information but also reduces computational\ncomplexity and the number of parameters.\nThe computational complexity and parameter count com-\nparisons between DWConv and standard convolution opera-\ntions(Conv) are as follows:\nComputational complexity comparison:\nDWConv = S × S × Cin × H′ × W ′\n+ Cin × Cout × H′ × W ′\nConv = S × S × H′ × W ′\n(1)\nParameter number comparison:\nDWConv params = S × S × Cin + Cin × Cout\nConv params = S × S × Cin × Cout\n(2)\nwhere S denotes the kernel size, Cin denotes the input channel,\nH\n′\nand W\n′\nRepresent the height and width of the input image\nor feature image respectively, Cout denotes the output channel.\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nFIGURE 5. Sample images of diverse diseases on the tomato dataset [41].\nTABLE 2. Detailed setting of the EGWT.\nInput size Layer name EGWT Output size\nStage 1\n224 × 224 DWTE 7 × 7, 64,stride=4 56 × 56\n56 × 56\nConvolutional projection \n\n3 × 3, 768\nngroups = 2, H2 = 6\nR2 = 4\n\n × 2 56 × 56G-MHA\nG-MLP\nStage 2\n56 × 56 DWTE 3 × 3, 192,stride=2 28 × 28\n28 × 28\nConvolutional projection \n\n3 × 3, 768\nngroups = 2, H2 = 6\nR2 = 4\n\n × 10 28 × 28G-MHA\nG-MLP\nStage 3\n28 × 28 DWTE 3 × 3, 1024,stride=2 14 × 14\n14 × 14\nEfficientNet projection \n\n3 × 3, 1024\nngroups = 4, H3 = 12\nR3 = 4\n\n × 3 14 × 14G-MHA\nG-MLP\nSoftmax Params:23.04M FLOPS:5.53G\n2) Convolutional projection\nFigure 6(b) shows the convolutional projection, which Sub-\nstitutes the conventional position-wise linear projection used\nin Multi-Head Attention (MHA) with DWConv, thereby cre-\nating the Convolutional Projection layer. As illustrated in\nFigure 6(b), we initially reshape the input tokens into a 2D to-\nken map. Subsequently, we apply a depth-wise convolutional\nprojection with a kernel size of S × S. Finally, the token map\nis flattened into a 1D representation, which serves as the input\nfor the group-wise multi-head attention module. This process\ncan be expressed as:\nxq/k/v\ni = Flatten(DWConv2D(Reshape2D(xi), S)) (3)\nwhere xq/k/v\ni is the token input for Q/K/V matrices of layer\ni, xi is the depth-wise split token prior to the convolutional\nprojection with stride s.\n3) Group-wise multi-head attention\nThe core component of the ViT model is the transformer\nencoder, encompassing MHA and Multi-Layer Percep-\ntron(MLP). In the given context of image feature sequences,\ndenoted as X ∈ Rn×d , where xi represents the token maps\nfrom Eq. 3, which corresponds to the i th position, we aim to\ncompute the attentional weights assigned to each position i\nwith respect to the other positions. This is achieved through a\nmulti-step process. Initially, a linear transformation is applied\nto the feature vectors, yielding three transformed vectors:\nqi = Wqxi, ki = Wk xi, and vi = Wvxi. Here, Wq, Wk , and\nWv represent the weight matrices that have been learned.\nSubsequently, for each position i, an attention score aij is\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\n(a) Depth-wise Separable Convolution\n (b) Convolutional Projection\nFIGURE 6. (a) A Depth-wise Separable Convolution: This diagram illustrates the two-step process of depth-wise separable convolution, consisting of\ndepth-wise convolution followed by point-wise convolution. (b) A depth-wise separable convolution is implemented in the following sequence:\nDepth-wise Conv2d→ BatchNorm2d → Point-wise Conv2d, where s denotes the convolution kernel size.\n(a) Group-wise multi-head attention\n (b) Group-wise Multilayer Perceptron\nFIGURE 7. Description of group-wise transformer: (a)The Group-Wise Multi-Head Attention divide feature inputX into 2 groupsX1 and X2, The attention\ncomputations within each group share weight matrix parameters. Then concatenate the computed attention results as the output. (b)Group-wise\nmultilayer Perceptron performs expansion projections initially, followed by group-wise linear transformation.\ncomputed as qi·kj√\nd , where d represents the dimensionality of\nthe feature depth. To ensure appropriate scaling, these atten-\ntion scores undergo a softmax normalization process. This\nnormalization results in attention weights αij = exp(aij)P\nj exp(aij) ,\nwhich reflect the relative importance of each position j in\nrelation to position i.\nUltimately, the attention weights are used to weigh and\naggregate the values of all positions. This aggregation process\nyields a context-aware representation for each position i,\nwhich is calculated as zi = P\nj αijvj. The attention formula\ncan be formulated as follows:\nAttention(X) = softmax\n \n(X · Wq) · (X · Wk )T\n√dk\n!\n· (X · Wv)\n= softmax\n\u0012QKT\n√dk\n\u0013\nV\n(4)\nThe Multi-Head Attention formula can be represented as\nfollows:\nMHA(X) = Concat\n\u0002\n( head 1, ··· head h) W O\u0003\nwhere head i = Attention\n\u0010\nQW Q\ni , KW K\ni , VW V\ni\n\u0011 (5)\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nAs shown in Figure.7(a), in order to achieve a lightweight\nversion of the transformer, we optimize the MHA and MLP\ncomponents within the transformer module using Group-wise\nMulti-Head Attention ( G-MHA). We divide the input features\nX ∈ Rn×d into k groups, define the τ(·) as group-wise\ntransformation. Here, n and d denote the sequential length\nand the feature depth, respectively, then:\nτgroup (X) = [X′\n1, ··· , X′\nk ]\nwhere X′\ni = τ (Xi; θ) (6)\nHere, Xi ∈ R\nn×d\nk signifies the truncated features of the\ni th group, with [·] denoting the process of concatenation.\nAdditionally, the function τ(·) can take the form of any\nparameterized function.\nGroup-Wise Multi-Head Attention ( G-MHA) is defined as:\nG-MHA(X) = [τ (X1) , ··· , τ(Xk )]\nwhere τ (Xi) =MHA (Xi) (7)\nwhere MHA is refer to Eq.5. The input features are divided\ninto k groups, and the weights of each group are shared.\n4) Group-Wise Multilayer Perceptron\nIn Figure 7(b), a G-MLP was employed to optimize the MLP.\nSpecifically, given an input X ∈ Rn×d , the G-MLP performs\nnon-linear projections to expand the feature size by\nH = GELU (XW1 + b1) (8)\nHere, GELU is a activation function, weight matrix W1 ∈\nRd×df and a bias vector b1 are used to obtain a set of hidden\nfeatures H ∈ Rd×df . The obtained hidden features H are\nsubsequently divided into k independent feature sets denoted\nas Hi ∈ R\nd×df\nk . Each of these independent feature sets Hi\nundergoes a linear transformation with weight parameters\nthat are shared among them. The optimization formula is as\nfollows:\nG-MLP(X) = [τ (H1) , ··· , τ(Hk )]\nwhere τ (Hi) =HiW2 + b2\n(9)\nwhere, W2 ∈ R\nd\nk ×\ndf\nk and b2 ∈ R\nd\nk .\nFigure 7 illustrates the group operation of G-MHA and\nG-MLP. Notably, this operation is conducted on the feature\ndimension rather than the sequence length. As a result, each\nfeature retains its ability to acquire coupling coefficients from\nothers through comparisons of the truncated features, akin to\nthe default MHA mechanism. This approach ensures that the\nattention patterns learned by G-MHA closely resemble those\nof the standard MHA.\n5) EGWT with transfer learning\nOur EGWT model is a deep neural network that incorporates\ngroup-wise transformer blocks and EfficientNet convolution\noperations. Currently, the dataset for crop disease identifica-\ntion is limited in size, which poses challenges for effectively\ntraining the EGWT model and makes it susceptible to over-\nfitting.\nTo address this issue, we adopted a transfer learning ap-\nproach [46] to leverage the EGWT model for crop disease\nidentification. Initially, the EGWT model is pre-trained from\nscratch using the ImageNet dataset. Subsequently, the pre-\ntrained EGWT model is fine-tuned using the crop disease\ndatasets. More specifically, apart from the final linear layer,\nall other layers of the network are frozen. The weights of the\nEGWT are initialized with pre-trained weights and fine-tuned\nduring training.\nIV. EXPERIMENT\nThe training process is conducted on a computer system\nequipped with an Intel(R) Xeon(R) Platinum 8352V pro-\ncessor, 24 GB of RAM, and an RTX 4090 GPU. The im-\nplementation is carried out using Python with PyTorch and\nKeras frameworks. For optimization, the Adam optimizer is\nemployed. A decreasing learning rate strategy is adopted,\nwith a decline ratio of 0.9 and a stabilization ratio of 0.1.\nThe input images are resized to dimensions of 224 × 224\nby default. The training dataset comprises 85% of the total\ndata, while the remaining 15% is allocated for the test set. To\naugment the training dataset, images are increased to twice\nthe size of the original dataset.\nIn our study, we maintain consistent data preprocessing\nmethods across distinct datasets. To ensure uniformity, we\nachieved consistency by configuring both the input and output\nlayers of the model, in addition to adjusting various hyper-\nparameters to be identical. Utilizing the parameters obtained\nfrom the model’s training on the ImageNet dataset as initial\nvalues, we performed fine-tuning on the crop leaf datasets.\nThis was done while maintaining identical training strategies.\nAs a result of these efforts, the parameters and computational\ncomplexity of the model remained consistent across the three\ndiverse crop leaf datasets.\nTo prevent overfitting, weight decay regularization is ap-\nplied in the loss function, and the learning rate is dynamically\nadjusted. Additionally, batch normalization is performed on\nthe input feature maps of each module to normalize input\ndistributions and reduce internal covariate shift\nA. EVALUATION METRICS\nThe aim of this research is to categorize distinct crop species\nand their respective diseases. To achieve this, we imple-\nmented the EGWT model on PlantVillage image datasets,\ncassava leaf datasets, and tomato leaf datasets. We gauged the\nefficacy of the EGWT model through a range of evaluation\nmetrics, including accuracy, precision, recall, and the F1-\nscore.\nAccuracy serves as a metric to gauge the overall correct-\nness of the classification process. Precision quantifies the\nproportion of accurately classified instances within a specific\nclass. Recall evaluates the model’s capability to accurately\nidentify instances belonging to a particular class. The F1-\nscore delivers a harmonious balance between precision and\nrecall, presenting a comprehensive assessment of the model’s\nperformance.\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nThese metrics collectively offer a comprehensive under-\nstanding of the model’s effectiveness in classifying crop\nspecies and diagnosing their diseases accurately. The utiliza-\ntion of multiple metrics ensures a more nuanced evaluation of\nthe model’s performance, considering various aspects of the\nclassification task.\nB. VISUALIZATION OF FEATURES EXTRACTED BY EGWT\nWe set the pre-trained EGWT model to evaluation mode,\nrandomly select an agricultural crop disease image as the\ninput tensor, employ the EGWT model to process the input,\nvisualize specific layers, and generate feature heatmaps for\nthese layers from the input tensor.\nFIGURE 8. Visualizing Intermediate Layer Features in the EGWT Model.\nFor the Group-wise Transformer module within the model,\nvisualizing the weights of the extracted feature maps enables a\nbetter understanding of its distribution of attention across the\ninput feature images. By employing heatmap visualization,\nwe can distinctly observe the module’s focus on various\nregions. Particularly, regarding the diseased areas in the im-\nages, we discern a higher accuracy in the model’s attention,\nindicating a more concentrated focus on these regions. This\nfurther validates the effectiveness of the model in addressing\ndisease recognition tasks.\nC. PERFORMANCE OF THE EXPERIMENTS\nIn this section, we present a comprehensive performance\nanalysis of the experiments conducted to validate the EGWT\nmodel on the three selected datasets. The experiments were\ndesigned to rigorously assess the EGWT and their effective-\nness in addressing crop disease identification. We evaluated\nthe performance through quantitative measures, statistical\nanalyses, and comparisons with state-of-the-art models.\nTABLE 3. Performance Evaluation of deep learning methods on the\nPlantVillage dataset.\nModel Top-1 Top-5 #Params Flops MAdds\nEfficientNet-B0 97.48% 98.6% 4.06M 15.83M 7.92M\nEfficientNet-B7 99.0% 99.6% 65.77M 2.5G 2.19G\nViT_Base 97.65% 100% 85.83M 17.58G 8.79G\nGWT 98.25% 100% 47.02M 10.25G 5.13G\nCVT 99.32% 99.75% 35.5M 9.87G 9.79G\nICVT 99.51% 99.97% 27.5M 10.16G 8.43G\nEGWT 99.88% 100% 23.04M 5.53G 3.09G\nTABLE 4. Performance Evaluation of deep learning methods on the\ncassava dataset.\nModel Top-1 Top-5 #Params Flops MAdds\nEfficientNet-B0 83.41% 88.6% 4.06M 15.83M 7.92M\nEfficientNet-B7 84.21% 93.16% 65.77M 2.5G 2.19G\nViT_Base 79.94% 82.2% 85.83M 17.58G 8.79G\nGWT 85.25% 88.67% 47.02M 10.25G 5.13G\nCVT 84.12% 86.7% 35.5M 9.87G 9.79G\nICVT 83.41% 85.34% 27.5M 10.16G 8.43G\nEGWT 84.29% 89.1% 23.04M 5.53G 3.09G\nTABLE 5. Performance Evaluation of deep learning methods on the\ntomato dataset.\nModel Top-1 Top-5 #Params Flops MAdds\nEfficientNet-B0 97.14% 100% 4.06M 15.83M 7.92M\nEfficientNet-B7 98.89% 99.66% 65.77M 2.5G 2.19G\nViT_Base 99.45% 82.2% 85.83M 17.58G 8.79G\nGWT 96.20% 98.7% 47.02M 10.25G 5.13G\nCVT 99.18% 99.57% 35.5M 9.87G 9.79G\nICVT 99.59% 99.74% 27.5M 10.16G 8.43G\nEGWT 99.99% 100% 23.04M 5.53G 3.09G\nTables 3, 4, and 5 present a comparative analysis of our\nmodel against state-of-the-art CNN-based and transformer-\nbased models in terms of Top-1 and Top-5 accuracy, param-\neter count, and computational complexity across different\ndatasets. Due to the advantages of EfficientNet in both fea-\nture extraction performance and computational complexity, as\nwell as the parameter efficiency of EfficientNet-B0 compared\nto EfficientNet-B7, this study employs EfficientNet-B0 as the\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nfeature extraction network for partitioning the feature maps\ninto blocks. The aforementioned tables demonstrate that our\nproposed EGWT model excels in both Top-1 and Top-5 per-\nformance, showcasing its superiority in correctly predicting\nthe most likely class and within the top five predictions. Top-1\naccuracy measures the percentage of correct predictions when\nthe model’s most confident prediction aligns with the true\nlabel, while Top-5 accuracy considers predictions correct if\nthe true label is among the model’s top five most confident\npredictions. We conducted identical performance evaluations\nof the pre-trained model on three agricultural crop datasets.\nDue to the identical model architecture, hyperparameter set-\ntings, dataset content, and distribution, the test results on\nthe three datasets yielded the same values for parameters,\nFloating-Point Operations per Second(FLOPs), and Million\nAdditions and Multiplications(MAdds).\nThe recall, precision, and F1-score for each class in the\nPlantVillage dataset are presented in Table 6. The average val-\nues for precision, recall, and F1-score are 99.98%, 99.97%,\nand 99.97%, respectively. Likewise, the results of the preci-\nsion 81.07%, recall 82.05%, and F1-score 79.22% for each\nclass in the cassava datasets achieved by the EGWT model\nare displayed in Tables 7, 8 shows the average values for\nprecision, recall, and F1-score are 99.80%, 99.93%, 99.90%\nof proposed model tested on tomato dataset.\nThe images of cassava leaves are more intricate, making\ntheir recognition more challenging compared to the other two\ndatasets. However, overall, our proposed model demonstrates\nhigher accuracy in predicting each category when compared\nto other models. In the PlantVillage and Tomato Leaf datasets,\nthe majority of categories exhibit precision, recall, and F1-\nscores of 1.\nFigures 9, 10, 11 depict the confusion matrices of the\nEGWT model across diverse datasets. The higher number of\nmisclassified samples in the cassava dataset can be attributed\nto several factors. Firstly, the class labels within the dataset\nare imbalanced. Secondly, the complexity of the leaf images’\nbackgrounds poses a challenge for classification, distinct\nfrom other tasks. Lastly, a significant proportion of misclassi-\nfied samples belong to visually similar diseases, contributing\nto the classification errors.\nUpon analyzing the confusion matrix, it becomes evident\nthat the proposed EGWT method demonstrates minimal oc-\ncurrences of false positives and false negatives when applied\nto both the PlantVillage and tomato datasets. An investiga-\ntion into the misclassified samples reveals that the majority\nof these misclassifications arise when leaves afflicted with\nvarious diseases share similar epigenetic characteristics. As\nillustrated in Figures 12, Figure 13, and Figure 14, it is appar-\nent that these misclassified image samples pose a challenge\neven for experienced farmers, as they are equally susceptible\nto making erroneous identifications.\nIn the experiment, to mitigate the negative transfer phe-\nnomenon, parameters from stage 1 and stage 2 of the network\nwere frozen, and fine-tuning was conducted starting from the\nthird last layer on a dataset of crop leaf images. This approach\nreduces interference with the pre-trained model and employs\na smaller learning rate to prevent excessive adjustment of\nthe pre-trained model. Additionally, a plethora of data aug-\nmentation techniques such as random cropping, rotation, and\nflipping were employed in the experiment to enhance data\ndiversity and mitigate overfitting.\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nTABLE 6. Performance evaluation on the PlantVillage for each class\nCrop categories Disease categories Precision Recall F1-score Support\nApple\nScab 1.000 0.9978 0.9992 83\nBlack rot 1.000 1.000 1.000 90\nCedar apple rust 1.000 1.000 1.000 35\nHealthy 1.000 1.000 1.000 251\nBlueberry Healthy 1.000 1.000 1.000 245\nCheery Healthy 1.000 1.000 1.000 122\nPowdery mildew 1.000 1.000 1.000 178\nCorn\nGray leaf spot 0.9996 1.000 0.9956 71\nCommon rust 1.000 1.000 1.000 187\nHealthy 1.000 1.000 1.000 161\nNorthern leaf blight 1.000 0.9999 0.9979 144\nGrape\nBlack rot 1.000 1.000 1.000 204\nEsca 1.000 1.000 1.000 198\nHealthy 1.000 1.000 1.000 76\nLeaf blight 1.000 1.000 1.000 167\nOrange Haunglongbing 1.000 1.000 1.000 772\nPeach Bacterial spot 1.000 1.000 1.000 335\nHealthy 1.000 1.000 1.000 49\nPepper Bell bacterial spot 1.000 1.000 1.000 129\nBell healthy 1.000 1.000 1.000 216\nPotato\nEarly blight 1.000 1.000 1.000 152\nHealthy 0.9945 1.000 1.000 26\nLate blight 1.000 1.000 1.000 140\nRaspberry Healthy 1.000 0.9978 0.9969 53\nSoybean Healthy 1.000 1.000 1.000 801\nSquash Powdery mildew 1.000 1.000 1.000 290\nStrawberry Healthy 1.000 1.000 1.000 76\nLeaf scorch 1.000 1.000 1.000 171\nTomato\nBacterial spot 1.000 1.000 1.000 305\nEarly blight 1.000 0.9986 1.000 150\nHealthy 1.000 1.000 1.000 264\nLate blight 1.000 1.000 1.000 284\nLeaf mold 1.000 1.000 1.000 139\nSeptoria leaf spot 1.000 1.000 1.000 260\nSpider mites 1.000 1.000 1.000 246\nTarget spot 1.000 0.9975 1.000 221\nTomato mosaic virus 1.000 1.000 1.000 49\nYellow leaf curl virus 1.000 1.000 1.000 806\nAverage 0.9998 0.9997 0.9997\nTABLE 7. Performance evaluation on the cassava dataset for each class\nCassava disease categories Precision Recall F1-score Support\nBacterial blight brown 0.8567 0.7546 0.7956 230\nStreak disease blight 0.8587 0.7898 0.8229 550\nGreen mottle blight 0.7560 0.8156 0.7278 437\nMosaic 0.9565 0.9667 0.9589 2310\nHealthy 0.6254 0.7760 0.6556 424\nAverage 0.8107 0.8205 0.7922\nV. CONCLUSION\nIn conclusion, our proposed EGWT model has demonstrated\nexceptional experimental performance in the domain of crop\ndisease detection, excelling in both performance metrics and\ncomputational efficiency. Through rigorous experimentation\non various datasets, our model consistently outperformed\nstate-of-the-art CNN-based and transformer-based models in\nterms of 99.8%, 86.9%, and 99.4% accuracy while main-\ntaining a significantly lower parameter count of 23.04M and\ncomputational complexity of 5.53G. This not only attests to\nthe efficacy of the EGWT model’s architecture but also high-\nlights its lightweight nature, making it a promising solution\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nTABLE 8. Performance evaluation on the tomato leaf dataset for each class\nTomato disease categories Precision Recall F1-score Support\nBacterial spot 0.9989 1.000 0.9996 546\nEarly blight 1.000 1.000 1.000 474\nHealthy 0.9995 1.000 1.000 546\nLate blight 1.000 0.9985 1.000 609\nLeaf mold 1.000 1.000 1.000 509\nPowdery mildew 0.9876 1.000 0.9993 187\nSeptoria leaf spot 0.9995 0.9997 0.9990 539\nTwo-spotted spider mite 0.9991 1.000 0.9945 332\nTarget spot 0.9967 0.9978 0.9989 339\nTomato mosaic virus 1.000 0.9978 1.000 394\nYellow leaf curl virus 0.9967 0.9989 0.9980 405\nAverage 0.9980 0.9993 0.9990\nFIGURE 9. Confusion matrix of the EGWT model on PlantVillage dataset.\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nFIGURE 10. Confusion matrix of the EGWT model on cassava dataset.\nfor real-world deployment where computational resources\nmay be limited. Furthermore, the analysis of confusion ma-\ntrices across different datasets has shed light on the model’s\nrobustness and its ability to cope with challenges posed by\nimbalanced class distributions, complex image backgrounds,\nand visually similar diseases. In essence, the EGWT model\npresents a compelling advancement in crop disease detec-\ntion, showcasing the potential to contribute significantly to\nthe improvement of agricultural practices and food security.\nFuture work could extend this model to address other related\nagricultural challenges and explore its applicability in diverse\nreal-world scenarios, we will further optimize the hyperpa-\nrameters of the model to achieve an optimal balance between\nmodel performance and complexity.\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nFIGURE 11. Confusion matrix of the EGWT model on tomato dataset.\n(a) Corn(maize) Cercospora leaf spot gray leaf spot\n (b) Corn(maize) common rust.\nFIGURE 12. Misclassified Samples on the PlantVillage.\n14 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nFIGURE 13. Misclassified Samples on the cassava leaf.\n(a) Tomato early blight.\n (b) Tomato late blight.\nFIGURE 14. Misclassified Samples on the tomato leaf.\nVOLUME 11, 2023 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nREFERENCES\n[1] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,\nG. Wang, J. Cai et al., ‘‘Recent advances in convolutional neural networks,’’\nPattern recognition, vol. 77, pp. 354–377, 2018.\n[2] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,\nC. Xu, Y. Xu et al. , ‘‘A survey on vision transformer,’’ IEEE transactions\non pattern analysis and machine intelligence , vol. 45, no. 1, pp. 87–110,\n2022.\n[3] M. Agarwal, S. K. Gupta, and K. Biswas, ‘‘Development of efficient cnn\nmodel for tomato crop disease identification,’’ Sustainable Computing:\nInformatics and Systems , vol. 28, p. 100407, 2020.\n[4] S. M. Hassan, A. K. Maji, M. Jasiński, Z. Leonowicz, and E. Jasińska,\n‘‘Identification of plant-leaf diseases using cnn and transfer-learning ap-\nproach,’’Electronics, vol. 10, no. 12, p. 1388, 2021.\n[5] H.-P. Wei, Y.-Y. Deng, F. Tang, X.-J. Pan, and W.-M. Dong, ‘‘A compara-\ntive study of cnn-and transformer-based visual style transfer,’’ Journal of\nComputer Science and Technology , vol. 37, no. 3, pp. 601–614, 2022.\n[6] R. Ahuja and S. C. Sharma, ‘‘Transformer-based word embedding with\ncnn model to detect sarcasm and irony,’’ Arabian Journal for Science and\nEngineering, vol. 47, no. 8, pp. 9379–9392, 2022.\n[7] J. Fang, H. Lin, X. Chen, and K. Zeng, ‘‘A hybrid network of cnn and\ntransformer for lightweight image super-resolution,’’ in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition , 2022,\npp. 1103–1112.\n[8] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,\nA. Karpathy, A. Khosla, M. Bernstein et al. , ‘‘Imagenet large scale visual\nrecognition challenge,’’ International journal of computer vision , vol. 115,\npp. 211–252, 2015.\n[9] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,\nand C. L. Zitnick, ‘‘Microsoft coco: Common objects in context,’’ in Com-\nputer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part V 13 . Springer, 2014, pp. 740–\n755.\n[10] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, ‘‘Places: A\n10 million image database for scene recognition,’’ IEEE transactions on\npattern analysis and machine intelligence , vol. 40, no. 6, pp. 1452–1464,\n2017.\n[11] S. U. Khan, N. Khan, T. Hussain, and S. W. Baik, ‘‘An intelligent corre-\nlation learning system for person re-identification,’’ Engineering Applica-\ntions of Artificial Intelligence , vol. 128, p. 107213, 2024.\n[12] A. Parashar, A. Parashar, A. F. Abate, R. S. Shekhawat, and I. Rida, ‘‘Real-\ntime gait biometrics for surveillance applications: A review,’’ Image and\nVision Computing, p. 104784, 2023.\n[13] H. Wang, Y. Zhu, B. Green, H. Adam, A. Yuille, and L.-C. Chen, ‘‘Axial-\ndeeplab: Stand-alone axial-attention for panoptic segmentation,’’ in Euro-\npean conference on computer vision . Springer, 2020, pp. 108–126.\n[14] Y. Xu, Z. Zhang, M. Zhang, K. Sheng, K. Li, W. Dong, L. Zhang, C. Xu,\nand X. Sun, ‘‘Evo-vit: Slow-fast token evolution for dynamic vision trans-\nformer,’’ in Proceedings of the AAAI Conference on Artificial Intelligence ,\nvol. 36, no. 3, 2022, pp. 2964–2972.\n[15] M.-H. Guo, C.-Z. Lu, Z.-N. Liu, M.-M. Cheng, and S.-M. Hu, ‘‘Visual\nattention network,’’ Computational Visual Media , vol. 9, no. 4, pp. 733–\n752, 2023.\n[16] X. Pan, T. Ye, Z. Xia, S. Song, and G. Huang, ‘‘Slide-transformer: Hierar-\nchical vision transformer with local self-attention,’’ in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023,\npp. 2082–2091.\n[17] Y. Fang, X. Wang, R. Wu, and W. Liu, ‘‘What makes for hierarchical\nvision transformer?’’ IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2023.\n[18] J. Chen, J. Chen, D. Zhang, Y. Sun, and Y. A. Nanehkaran, ‘‘Using deep\ntransfer learning for image-based plant disease identification,’’ Computers\nand Electronics in Agriculture , vol. 173, p. 105393, 2020.\n[19] A. Sagar and J. Dheeba, ‘‘On using transfer learning for plant disease\ndetection,’’BioRxiv, pp. 2020–05, 2020.\n[20] J. Chen, D. Zhang, Y. A. Nanehkaran, and D. Li, ‘‘Detection of rice plant\ndiseases based on deep transfer learning,’’ Journal of the Science of Food\nand Agriculture, vol. 100, no. 7, pp. 3246–3256, 2020.\n[21] V. A. Sindagi and V. M. Patel, ‘‘A survey of recent advances in cnn-based\nsingle image crowd counting and density estimation,’’ Pattern Recognition\nLetters, vol. 107, pp. 3–16, 2018.\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ Advances in\nneural information processing systems , vol. 30, 2017.\n[23] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, ‘‘Dual attention\nnetwork for scene segmentation,’’ in Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition , 2019, pp. 3146–3154.\n[24] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, ‘‘Cbam: Convolutional block\nattention module,’’ inProceedings of the European conference on computer\nvision (ECCV), 2018, pp. 3–19.\n[25] Y. Yuan, L. Chen, H. Wu, and L. Li, ‘‘Advanced agricultural disease\nimage recognition technologies: A review,’’ Information Processing in\nAgriculture, vol. 9, no. 1, pp. 48–59, 2022.\n[26] Y. Kurmi, P. Saxena, B. S. Kirar, S. Gangwar, V. Chaurasia, and A. Goel,\n‘‘Deep cnn model for crops’ diseases detection using leaf images,’’ Multi-\ndimensional Systems and Signal Processing , vol. 33, no. 3, pp. 981–1000,\n2022.\n[27] P. S. Thakur, P. Khanna, T. Sheorey, and A. Ojha, ‘‘Explainable vision\ntransformer enabled convolutional neural network for plant disease identi-\nfication: Plantxvit,’’ arXiv preprint arXiv:2207.07919 , 2022.\n[28] S. Wu, Y. Sun, and H. Huang, ‘‘Multi-granularity feature extraction based\non vision transformer for tomato leaf disease recognition,’’ in 2021 3rd\nInternational Academic Exchange Conference on Science and Technology\nInnovation (IAECST). IEEE, 2021, pp. 387–390.\n[29] Y. Wang, Y. Chen, and D. Wang, ‘‘Convolution network enlightened trans-\nformer for regional crop disease classification,’’ Electronics, vol. 11, no. 19,\np. 3174, 2022.\n[30] R. Reedha, E. Dericquebourg, R. Canals, and A. Hafiane, ‘‘Transformer\nneural network for weed and crop classification of high resolution uav\nimages,’’Remote Sensing, vol. 14, no. 3, p. 592, 2022.\n[31] Y. Borhani, J. Khoramdel, and E. Najafi, ‘‘A deep learning based approach\nfor automated plant disease classification using vision transformer,’’ Sci-\nentific Reports, vol. 12, no. 1, p. 11554, 2022.\n[32] X. Fu, Q. Ma, F. Yang, C. Zhang, X. Zhao, F. Chang, and L. Han, ‘‘Crop\npest image recognition based on the improved vit method,’’ Information\nProcessing in Agriculture , 2023.\n[33] S. Yu, L. Xie, and Q. Huang, ‘‘Inception convolutional vision transformers\nfor plant disease identification,’’ Internet of Things , vol. 21, p. 100650,\n2023.\n[34] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang, ‘‘Cvt:\nIntroducing convolutions to vision transformers,’’ in Proceedings of the\nIEEE/CVF international conference on computer vision , 2021, pp. 22–31.\n[35] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,\n‘‘Training data-efficient image transformers & distillation through atten-\ntion,’’ in International conference on machine learning . PMLR, 2021,\npp. 10 347–10 357.\n[36] J. Gou, B. Yu, S. J. Maybank, and D. Tao, ‘‘Knowledge distillation: A\nsurvey,’’International Journal of Computer Vision , vol. 129, pp. 1789–\n1819, 2021.\n[37] Y. Zhao, H. Tang, Y. Jiang, Q. Wu et al., ‘‘Lightweight vision transformer\nwith cross feature attention,’’ arXiv preprint arXiv:2207.07268 , 2022.\n[38] G. Luo, Y. Zhou, X. Sun, Y. Wang, L. Cao, Y. Wu, F. Huang, and R. Ji, ‘‘To-\nwards lightweight transformer via group-wise transformation for vision-\nand-language tasks,’’ IEEE Transactions on Image Processing , vol. 31, pp.\n3386–3398, 2022.\n[39] A. Ali. (2023) Plantvillage dataset. [Online]. Available: https://www.\nkaggle.com/datasets/abdallahalidev/plantvillage-dataset\n[40] M. A. I. A. Lab. (2020) cassava-preprocessed (preprocessed dataset for\ncassava disease classification). [Online]. Available: https://www.kaggle.\ncom/datasets/kingofarmy/cassavapreprocessed\n[41] A. Motwani. (2023, October) Tomato leaves dataset. [Online]. Available:\nhttps://www.kaggle.com/datasets/ashishmotwani/tomato\n[42] J. Yun, J. Ha, and J. S. Lee, ‘‘Automatic determination of neighborhood\nsize in smote,’’ in Proceedings of the 10th International Conference on\nUbiquitous Information Management and Communication , January 2016,\npp. 1–8.\n[43] S. A. Alex and J. J. V. Nayahi, ‘‘Classification of imbalanced data using\nsmote and autoencoder based deep convolutional neural network,’’ Inter-\nnational Journal of Uncertainty, Fuzziness and Knowledge-Based Systems ,\nvol. 31, no. 03, pp. 437–469, 2023.\n[44] M. Tan and Q. Le, ‘‘Efficientnet: Rethinking model scaling for convolu-\ntional neural networks,’’ in International conference on machine learning .\nPMLR, 2019, pp. 6105–6114.\n[45] F. Chollet, ‘‘Xception: Deep learning with depthwise separable convolu-\ntions,’’ in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2017, pp. 1251–1258.\n[46] K. Weiss, T. M. Khoshgoftaar, and D. Wang, ‘‘A survey of transfer learn-\ning,’’Journal of Big data , vol. 3, no. 1, pp. 1–40, 2016.\n16 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJ.Feng et al.: Enhanced Crop Disease Detection with EfficientNet Convolutional Group-Wise Transformer\nJING FENG received the B.S. degree in ap-\nplied mathematics from ChangZhi College, Shanxi\nprovince, in 2015, and the M.S. degrees in com-\nputational mathematics from Beijing Institute of\nScience and Engineering Computing at Beijing\nUniversity of Technology, China, in 2018. She is\ncurrently pursuing a Ph.D. degree with the Univer-\nsiti Sains Malaysia, Penang, Malaysia.\nHer main research focuses on the application\nof deep learning models in computer vision and\nmethods for the sparsification of deep neural network models.\nWEN ENG ONG received a B.S. degree and\nM.S. degree in computational mathematics from\nMalaysia University, Kuala Lumpur, Malaysia, and\nreceived a Ph.D. degree in computational and in-\ndustrial mathematics from the University of Can-\nterbury, New Zealand.\nShe is currently a senior lecturer in the School\nof Mathematical Sciences at Universiti Sains\nMalaysia. Her current research interests include\nthe reconstruction of curves and surfaces, shortest\npath algorithms and applications, computational mathematics, and machine\nlearning.\nWEN CHEAN TEH graduated from The Ohio-\nState University, under the mentorship of Timothy\nCarlson. His Erdos number is four and he is a\ndescendent of the famous logician Alfred Tarski.\nHe is an editor for Bulletin of the Malaysian Math-\nematical Sciences Society, a Springer Q1 journal.\nSince joining USM in early 2013, he has published\nover 50 papers in various mainstream MathSciNet\njournals in the areas of logic, combinatorics, and\ntheoretical computer science. He was the principal\ninvestigator of six research grants, including national grants thrice. He re-\nceived the Chebyshev travel grant award from the International Congress\nof Mathematicians in 2022 and was an oral presenter in the prestigious\ncongress meetings in both Seoul 2014 and Rio De Janeiro in 2018. He also\nreceived the Best Article Principal award from the Malaysian Mathematical\nSciences Society in 2015 and the Phil Huneke Excellence in Teaching\nAward from The Ohio State University. He is a regular reviewer for Elsevier\nand Springer journals, including Discrete Mathematics, Discrete Applied\nMathematics, Information and Computation, Theoretical Computer Science,\nand Acta Informatica, as well as Mathematical Reviews of the American\nMathematical Society. He was entrusted the role of the chief editor twice\nfor the International Conference of Mathematical Sciences and Technology\norganized by the school.\nRUI ZHANG received the B.S. degree from\nTaiyuan Normal University, Taiyuan, China, in\n2011 and the M.S. degree in statistics from the\nOcean University of China, Qingdao, China, in\n2015. She is currently working toward a Ph.D.\ndegree in statistics with the School of Mathemat-\nical Sciences, Universiti Sains Malaysia, Penang,\nMalaysia.\nVOLUME 11, 2023 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3379303\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6913416385650635
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6356916427612305
    },
    {
      "name": "Transformer",
      "score": 0.6043633222579956
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5290911197662354
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.43460118770599365
    },
    {
      "name": "Agricultural engineering",
      "score": 0.35763809084892273
    },
    {
      "name": "Machine learning",
      "score": 0.3333936333656311
    },
    {
      "name": "Engineering",
      "score": 0.10893881320953369
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210115395",
      "name": "Taiyuan Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I139322472",
      "name": "Universiti Sains Malaysia",
      "country": "MY"
    }
  ],
  "cited_by": 29
}