{
  "title": "PALM: Pre-training an Autoencoding&amp;Autoregressive Language Model for Context-conditioned Generation",
  "url": "https://openalex.org/W3016512233",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2251480259",
      "name": "Bi, Bin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2636280222",
      "name": "Li, Chenliang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097367985",
      "name": "Wu Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2019895671",
      "name": "Yan Ming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1918897273",
      "name": "Wang Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2317584531",
      "name": "Huang, Songfang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097369839",
      "name": "Huang, Fei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105471130",
      "name": "Si Luo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2890166583",
    "https://openalex.org/W2971119378",
    "https://openalex.org/W3004153848",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2963663567",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2890498499",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2963515589",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W3113747735",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2558203065",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2951603207",
    "https://openalex.org/W2625541525",
    "https://openalex.org/W2971639964",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2896669088",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2922709902",
    "https://openalex.org/W2962977247",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2125320996",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a powerful technique for natural language understanding and generation. Existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transformer-based models by recovering original word tokens from corrupted text with some masked tokens. The training goals of existing techniques are often inconsistent with the goals of many language generation tasks, such as generative question answering and conversational response generation, for producing new text given context. This work presents PALM with a novel scheme that jointly pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus, specifically designed for generating new text conditioned on context. The new scheme alleviates the mismatch introduced by the existing denoising scheme between pre-training and fine-tuning where generation is more than reconstructing original text. An extensive set of experiments show that PALM achieves new state-of-the-art results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues.",
  "full_text": "PALM: Pre-training an Autoencoding&Autoregressive Language Model\nfor Context-conditioned Generation\nBin Bi, Chenliang Li, Chen Wu, Ming Yan,\nWei Wang, Songfang Huang, Fei Huang, Luo Si\nAlibaba Group\n{b.bi, lcl193798, wuchen.wc, ym119608}@alibaba-inc.com\n{hebian.ww, songfang.hsf, f.huang, luo.si}@alibaba-inc.com\nAbstract\nSelf-supervised pre-training, such as\nBERT (Devlin et al., 2018), MASS (Song\net al., 2019) and BART (Lewis et al., 2019),\nhas emerged as a powerful technique for nat-\nural language understanding and generation.\nExisting pre-training techniques employ au-\ntoencoding and/or autoregressive objectives to\ntrain Transformer-based models by recovering\noriginal word tokens from corrupted text with\nsome masked tokens. The training goals of\nexisting techniques are often inconsistent\nwith the goals of many language generation\ntasks, such as generative question answering\nand conversational response generation, for\nproducing new text given context.\nThis work presents PALM with a novel\nscheme that jointly pre-trains an autoencod-\ning and autoregressive language model on a\nlarge unlabeled corpus, speciﬁcally designed\nfor generating new text conditioned on con-\ntext. The new scheme alleviates the mismatch\nintroduced by the existing denoising scheme\nbetween pre-training and ﬁne-tuning where\ngeneration is more than reconstructing orig-\ninal text. An extensive set of experiments\nshow that PALM achieves new state-of-the-\nart results on a variety of language genera-\ntion benchmarks covering generative question\nanswering (Rank 1 on the ofﬁcial MARCO\nleaderboard), abstractive summarization on\nCNN/DailyMail as well as Gigaword, ques-\ntion generation on SQuAD, and conversational\nresponse generation on Cornell Movie Dia-\nlogues.\n1 Introduction\nSelf-supervised pre-training has achieved great suc-\ncess in a wide range of natural language under-\nstanding (NLU) tasks (Dai and Le, 2015; Howard\nand Ruder, 2018; Radford, 2018; Peters et al.,\n2018; Devlin et al., 2018). Different from lan-\nguage understanding, language generation aims at\ngenerating natural language sentences, including\ntasks like neural machine translation (Bahdanau\net al., 2015; Vaswani et al., 2017), abstractive sum-\nmarization (Rush et al., 2015; See et al., 2017a;\nGehrmann et al., 2018), generative question an-\nswering (QA) (Tan et al., 2017; Bi et al., 2019),\nquestion generation (Zhao et al., 2018) and con-\nversational response generation (Vinyals and Le,\n2015). Many of the language generation tasks re-\nquire the models to read and to comprehend a given\ndocument, based on which output text is generated.\nIn this paper, we present PALM, a novel approach\nto Pre-training an Autoencoding&autoregressive\nLanguage Model for text generation based on read-\ning comprehension of textual context.\nRecently, several pre-training methods have been\nproposed for language generation. GPT (Radford,\n2018) and GPT-2 (Radford et al., 2019) use a left-\nto-right Transformer decoder to generate a text se-\nquence token-by-token, which lacks an encoder\nto condition generation on context. In contrast,\nMASS (Song et al., 2019) and BART (Lewis et al.,\n2019) both employ a Transformer-based encoder-\ndecoder framework, with a bidirectional encoder\nover corrupted (masked) text and a left-to-right\ndecoder reconstructing the original text. While\nsuch denoising pre-training objectives work well\nfor the downstream generation tasks where gen-\nerated text comes from input but is manipulated,\nthey are less related to the comprehension-based\ngeneration tasks asking for instead generating con-\ntinuations, responses or answers by comprehending\ninput context.\nPALM is speciﬁcally designed to pre-train a\nbackbone model on a large unlabeled corpus for\nﬁne-tuning on the downstream comprehension-\nbased generation tasks, one example of which is\ngenerative QA. In generative question answering,\nQA models are asked to generate an abstractive\nanswer in natural language to a given question by\narXiv:2004.07159v2  [cs.CL]  20 Sep 2020\nreading and comprehending a contextual passage.\nAbstractive answer generation is more than manipu-\nlating tokens in the passage. An abstractive answer\nreﬂects the understanding of the passage and the\nquestion, and can include content out of the passage\nto be self-contained and well-formed. To address\ncomprehension-based generation like generative\nQA, PALM uses the pre-training objectives that are\nclosely related to the downstream tasks. Speciﬁ-\ncally, it differs from existing generative pre-training\nmethods in that PALM goes beyond the solely au-\ntoencoding/autoregressive methods and combines\nthe merits of autoencoding and autoregression in a\nsingle framework. Moreover, it possesses a mecha-\nnism built in pre-training for generating coherent\ntext from given context.\nWith the new design, PALM surpasses exist-\ning language generation methods with or with-\nout pre-training – It was trained on 16 NVIDIA\nV100 GPUs for 3 days in our experiments, and ex-\npected to perform even better if trained for longer.\nPALM gives surprisingly good empirical results\non a variety of context-aware generation tasks, in-\ncluding pushing the state-of-the-art Rouge-L on\nthe MARCO Natural Language Generationbench-\nmark to 0.498 (Rank 1 on the leaderboard 1) and\non Gigaword summarization to 36.75, as well as\nestablishing the state-of-the-art ROUGE-1 (44.30)\nand ROUGE-L (41.41) on CNN/Daily Mail.\nWe make the following major contributions in\nthis paper:\n•We propose PALM, a novel approach to pre-\ntraining a language model on a large unlabeled\ntext corpus, which is able to comprehend con-\ntextual text. The pre-trained model is partic-\nularly effective to be ﬁne-tuned for language\ngeneration conditioned on context.\n•PALM signiﬁcantly advances the state-of-the-\nart results on a variety of language genera-\ntion applications, including generative QA,\nabstractive summarization, question genera-\ntion, and conversational response generation.\nIt clearly demonstrates PALM’s effectiveness\nand generalizability in language generation.\n2 PALM for Context-conditioned\nGeneration\nThis section presents the new mechanism and pre-\ntraining objectives of PALM for generation condi-\n1http://www.msmarco.org/leaders.aspx\ntioned on context. The differences between PALM\nand prior pre-training approaches are discussed as\nwell.\n2.1 Joint Modeling of Autoencoding and\nAutoregression\nWe denote (x,y) ∈(X,Y) as a pair of text pieces,\nwhere x= (x1,x2,...,x m) is the source text with\nm tokens, and y = (y1,y2,...,y n) is the target\ntext with n tokens. Xand Ydenote the sets of\nsource text and target text, respectively. PALM\nuses the standard Transformer encoder-decoder\nfrom (Vaswani et al., 2017) as the base architec-\nture, which maximizes the log-likelihood objective:\nL(θ; (X,Y)) =∑\n(x,y)∈(X,Y) log P(y|x; θ).\nExisting Transformer-based pre-training meth-\nods employ either autoencoding or autoregressive\nobjectives for self-supervision. Autoencoding-\nbased pre-training aims to reconstruct the original\ntext from corrupted input. Notable examples are\nBERT and its variants RoBERTa and ALBERT,\nwhere a certain portion of input tokens are re-\nplaced by a special symbol [MASK]. The models\nare trained to recover the original tokens from the\ncorrupted version by utilizing bidirectional con-\ntext. However, these autoencoding methods are not\napplicable to text generation where bidirectional\ncontexts are not available.\nOn the other hand, an autoregressive model, such\nas GPT (Radford, 2018; Radford et al., 2019), is\nonly trained to encode unidirectional context (ei-\nther forward or backward). Speciﬁcally, at each\noutput timestep, a token is sampled from the mod-\nels predicted distribution and the sample is fed back\ninto the model to produce a prediction for the next\noutput timestep, and so on. While applicable to\ntext generation, the autoregressive methods are not\neffective at modeling deep bidirectional context.\nOn the contrary, downstream generation tasks of-\nten ask a model to condition generation on given\ntextual context. This results in a gap between au-\ntoregressive modeling and effective pre-training.\nTo close the gap, PALM is carefully designed to\nautoregressively generate a text sequence by com-\nprehending the given context in a bidirectional au-\ntoencoding manner. In particular, PALM delegates\nautoencoding-based comprehension to the encoder\nin Transformer, and autoregressive generation to\nthe Transformer decoder. The encoder and decoder\nare jointly pre-trained in two stages:\n1. The encoder is ﬁrst trained as a bidirectional\n\u0001\u0004\u0002\u0005\u0003\u0004\u0006\u0001\u0003\u0002!\"!# !$\n!\"!# !$\n!%\n!%!&\n(a) GPT: Tokens are predicted autoregressively, meaning\nthat GPT can be used for generation. However, it lacks an\nencoder to condition generation on context.\n\u0001\b\u0006\b\f\u0007\u0005\r\b\u000b\n\u0004\t\u0003\n\u0005\u000b\u0006\u0007\f\u0002\u0007\u0005\u000b\u0006\u0007\f!\" \u0001\u0001 !#!$ \u0001\n!%!#\n\u0001\u0001\u0001 !&\n!&\n(b) MASS: It is based on the encoder-decoder architecture,\nbut the decoder predicts only the tokens that are masked out\nin the text input to the encoder.\n\u0001\b\u0006\b\f\u0007\u0005\r\b\u000b\n\u0004\t\u0003\n\u0005\u000b\u0006\u0007\f\u0002\u0007\u0005\u000b\u0006\u0007\f!\"\u0003 !#!$\u0003 \u0001\u0004\u0002!%!\" !$\n!%!\" !$\n!&\n!&!#\n(c) BART: Rather than masked tokens, the decoder recon-\nstructs the original full sentence from the corrupted input to\nthe encoder. However, it mismatches with most downstream\ngeneration which is more than reconstructing original input.\n\u0001\b\u0006\b\f\u0007\u0005\r\b\u000b\n\u0004\t\u0003\n\u0005\u000b\u0006\u0007\f!\"\u0003 !#!$\u0003\n!% !&\n\u0002\u0007\u0005\u000b\u0006\u0007\f\u0001\u0004\u0002'(') '*\n'(') '*'+\n(d) PALM: The encoder predicts masked tokens by encoding\ncontext bidirectionally, and the decoder predicts the text\nsegment subsequent to the context. It forces the model to\nlearn to comprehend the context for generating relevant text.\nFigure 1: A schematic comparison of PALM with GPT, MASS and BART.\nautoencoder to reconstruct the original text\nfrom corrupted context in which random to-\nkens are sampled and replaced with [MASK]\nsymbols following BERT’s practice (Devlin\net al., 2018). The training optimizes the cross-\nentropy reconstruction loss between encoder’s\noutput and original context, as Masked Lan-\nguage Modeling (MLM) in BERT. By pre-\ndicting the actual tokens in context that are\nmasked out, PALM forces the encoder to com-\nprehend the meaning of the unmasked tokens\nand the full context.\n2. The encoder and decoder are then jointly\ntrained to autoregressively generate text out-\nput out of the context representations from\nthe encoder. The training maximizes the log-\nlikelihood of the text in ground truth from the\ndecoder’s output:\nL(θ) =\n∑\n(x,y)∈(X,Y)\nlog\nn∏\nt=1\nP(yt|y<t,x; θ),\n(1)\nwhere X represents the set of context and\nYrepresents the set of text to be generated.\nBy conditioning the generation on context\nrepresentations, PALM forces the decoder to\nrely deeply on the context instead of preced-\ning generated tokens in next token prediction,\nwhich facilitates context-sensitive generation.\n2.2 Input&Output Representations\nIn the phase of model pre-training, input and out-\nput representations are tailored to minimize the dis-\ncrepancy between self-supervised pre-training and\nsupervised ﬁne-tuning. In a typical downstream\ngeneration task (e.g., abstractive summarization\nand generative QA), context is given as a rather\nlong passage, and a model is asked to generate a\nshorter piece of text based on the comprehension\nof the context.\nGiven a contiguous text fragment of length L\n(composed of a few sentences) from an unlabeled\ncorpus, PALM uses the consecutive span of length\n80% ·Lfrom the beginning of the fragment as con-\ntext input to the encoder, and uses the remainder\nof text span of length 20% ·L as text output to\nbe generated by the decoder. This representation\ndesign mimics the input and output of downstream\ntasks, with the hypothesis that human-written text\nis coherent and thus the subsequent text span of\nlength 20% ·Lcaptures the comprehension of the\npreceding context span. In this way, PALM learns\nto infer the subsequent text content from the pre-\nceding content.\nThe collection of text fragments are constructed\nfrom a corpus by following the practice of BERT.\nIn our experiments, we set the maximum length of\na fragment to be 500, i.e., L≤500. Therefore, the\ncontext input consists of at most 400 tokens, and\nthe text output consists of at most 100 tokens.\nFigure 1 shows a schematic comparison of in-\nput&output representations between PALM and\nthe existing pre-training generation methods, GPT,\nMASS and BART. GPT uses a decoder to predict\ntokens autoregressively, without an encoder to con-\ndition generation on context. MASS and BART are\nboth trained to recover the original tokens that are\nmasked out from corrupted text, where the inputs\nto the encoder and the decoder come from the same\ntext segment (e.g., the sequence(x1,x2,x3,x4,x5)\nin Figures 1b and 1c). They are also expected to\noutput the tokens from the same text sequence. By\ncontrast, in PALM the encoder and the decoder\ntake two different inputs. The input to the de-\ncoder comes from the continuation of the text input\nto the encoder (e.g., (y6,y7,y8) is subsequent to\n(x1,x2,x3,x4,x5) in the contiguous text segment\n(x1,x2,x3,x4,x5,y6,y7,y8) in Figure 1d). In ad-\ndition to the continuation predicted by the decoder,\nPALM produces an extra output from the encoder,\nwhich contains the predicted tokens masked in the\ninput (e.g., x2 and x4 in Figure 1d). The output\npredictions from the encoder and the decoder are\nused for training in the two stages, respectively.\n2.3 Copying Tokens from Context\nIn a human-written document, subsequent text of-\nten refers back to entities and tokens present earlier\nin the preceding text. Therefore, it would increase\ncoherence of text generated in downstream to incor-\nporate the copy mechanism into pre-training on an\nunlabeled corpus. This allows the model to learn\nfrom pre-training when and how to copy tokens in\ngenerating text, and the knowledge is transferred\nto downstream ﬁne-tuning.\nPALM incorporates the copy mechanism by\nplugging in the pointer-generator network (See\net al., 2017b; Nishida et al., 2019) on top of the\ndecoder in Transformer. Figure 2 illustrates the\npointer-generator network, which allows every to-\nken to be either generated from a vocabulary or\ncopied from context in generating text.\nExtended vocabulary distribution. Let the ex-\ntended vocabulary, V, be the union of words in\nthe vocabulary and all tokens present in context.\nPv(yt) then denotes the probability distribution of\nthe t-th word token, yt, over the extended vocabu-\nlary, deﬁned as:\nPv(yt) =softmax(We(Wvst + bv)), (2)\nwhere st denotes the output representation of t-th\ntoken from the decoder. The output embedding\n\u0007\u0011\u0011\u0015\u001e\u0015 \u0012\u0001\u0007\u001e\u001e\u0012\u0018\u001e\u0015\u0019\u0018\b\u0019\u0018\u001e\u0012\"\u001e\u001c\u0012\u001a\u001c\u0012\u001d\u0012\u0018\u001e\u000e\u001e\u0015\u0019\u0018\u001d\u0002\u0016\u0012#\u0004\u0001 \u000e\u0017\u001f\u0012\u0003\n\u000b\u0012\u0012\u0011\u0005\u000b\u0019\u001c!\u000e\u001c\u0011\u001b\u001f\u0012\u001c#\n\u0007\u001e\u001e\u0012\u0018\u001e\u0015\u0019\u0018!\u0012\u0015\u0013\u0014\u001e\u001d\n\u000b\u0012\u0012\u0011\u0005\u000b\u0019\u001c!\u000e\u001c\u0011\b\u0019\u0018\u001e\u0012\"\u001e\u0001 \u0012\u0010\u001e\u0019\u001c\nλ\n\u000b\u0015\u0018\u000e\u0017\u0001\u0011\u0015\u001d\u001e\u001c\u0015\u000f\u001f\u001e\u0015\u0019\u0018\n\u0007\u001e\u001e\u0012\u0018\u001e\u0015\u0019\u0018\u0001\u0011\u0015\u001d\u001e\u0006 \r\u0019\u0010\u0006\u0001\u0011\u0015\u001d\u001e\u0006\f\u0015\"\u001e\u001f\u001c\u0012\u0001!\u0012\u0015\u0013\u0014\u001e\n\n\u0018\u0010\u0019\u0011\u0012\u001c \t\u0012\u0010\u0019\u0011\u0012\u001c\u001e\u0005\u001e\u0014\u001d\u001e\u000e\u001e\u0012\nFigure 2: The pointer-generator network on top of the\ndecoder in Transformer. For each decoding step t, mix-\nture weights λfor the probability of generating tokens\nfrom the vocabulary and copying tokens from context\nare calculated. The two distributions are summed in a\nweighted manner to obtain the ﬁnal distribution.\nWe is tied with the corresponding part of the input\nembedding (Inan et al., 2017), and Wv and bv are\nlearnable parameters.\nCopy distribution. PALM uses an additional\nattention layer for the copy distribution on top of\nthe decoder. In the course of generation, the layer\ntakes st as the query, and outputsαt as the attention\nweights and zc\nt as the context vector:\nec\ntl = wc⊤ tanh(Wmhc\nl + Wsst + bc), (3)\nαc\nt = softmax(ec\nt), (4)\nzc\nt =\nm∑\nl=1\nαc\ntlhc\nl , (5)\nwhere hc\nl is the representation of l-th token in con-\ntext from the encoder. wc, bc, Wm and Ws are\nlearnable parameters. As a result, Pc(yt) is the\ncopy distribution over the extended vocabulary, de-\nﬁned as:\nPc(yt) =\n∑\nl:xl=yt\nαc\ntl. (6)\nFinal distribution. The ﬁnal probability of gen-\nerating yt is deﬁned as a mixture of the extended\nvocabulary distribution and the copy distribution:\nP(yt) =λPv(yt) + (1−λ)Pc(yt), (7)\nλ= sigmoid(wzzc\nt + wsst + bm), (8)\nwhere wz, ws and bm are learnable parameters.\nThe parameters in pointer-generator learned in\npre-training are all kept and passed downstream for\nﬁne-tuning on labeled data.\nExample 1\nInput\nA classic Aston Martin once owned by Spartacus star Peter Ustinov is set to fetch more than £1 million at auction\n- twice what it fetched four years ago. The actor bought the Aston Martin DB4 Cabriolet in 1962, shortly after\nwinning a Best Supporting Actor Oscar for his role as Batiatus in Spartacus. It was one of the most luxurious cars\nof its day, costing £4,000 and was delivered to him at a Swiss hotel at a time when the average house price in\nBritain was just £2,500.\nPALM The Aston Martin DB4 Cabriolet was bought by Peter Ustinov for £4000 and was expected to fetch for £2.5\nmillion for auction. The car was sold for £1.2 million ﬁnally.\nMASS peter ustinov’sUNK auctioned for more than $1 million.\nExample 2\nInput\nCape Verde’s 2-0 win over Portugal was the most eye-catching international result of the week. So, who are Cape\nVerde and why has this tiny island off the west coast of Africa suddenly become an international football force?\nWhere are the Cape Verde Islands? Cape Verde is a group of islands 400 miles from Senegal off the west coast of\nAfrica. Its population is around 500,000 and boasts both beautiful beaches and striking volcanic landscapes,\nmaking it a haven for tourism.\nPALM\nCape Verde is a small island off the west coast of Africa with a population of around 500,000 and boasts both\nbeautiful beaches and striking volcanic landscapes, making it a haven for tourism. Cape Verde is home to the\nCape Verde Islands National Park with a number of islands.\nMASS tiny african island nation cape verde has beautiful beaches.\nTable 1: Example generated continuations of the text input to PALM and MASS.\n3 Experiments\nIn this section, we present the experimental setup\nand results of PALM pre-training on a large unla-\nbeled corpus and ﬁne-tuning on a variety of lan-\nguage generation tasks, including generative QA,\nabstractive summarization, question generation,\nand conversational response generation.\n3.1 Pre-training Conﬁguration\nExperimental Setup. PALM is based on the Trans-\nformer which consists of a 12-layer encoder and\na 12-layer decoder with 768 embedding/hidden\nsize, 3072 feed-forward ﬁlter size and 12 atten-\ntion heads. We have also trained a larger model,\nreferred to as PALM LARGE, to compare with the\nbaseline models of the same size. PALM LARGE\nhas an encoder of 24 layers and a decoder of 6\nlayers, with 1024 embedding/hidden size and 16 at-\ntention heads. The parameters of PALM’s encoder\nare initialized by the pre-trained RoBERTa model2\nwhich was trained with the Masked LM objective,\nremoving Next Sentence Prediction from BERT.\nPALM is trained with a dropout rate of 0.1 on all\nlayers and attention weights, and a GELU activa-\ntion function (Hendrycks and Gimpel, 2016) used\nas GPT. The learning rate is set to 1e-5, with linear\nwarmup over the ﬁrst 10k steps and linear decay.\nThe pre-training procedure runs on 16 NVIDIA\nV100 GPU cards for 800K steps, with each mini-\nbatch containing 64 sequences of maximum length\n500 tokens.\nPre-training Dataset. We use documents of En-\nglish Wikipedia and BookCorpus (Zhu et al., 2015)\n2https://github.com/pytorch/fairseq\nas our pre-training corpus, and perform WordPiece\ntokenization as BERT (Devlin et al., 2018). The\ndocuments are split into sentences. Different from\nBERT, we use multiple consecutive sentences up to\n400 tokens as the source text input to the encoder,\nand use the subsequent consecutive sentences up\nto 100 tokens as the target text to the decoder. The\npre-training dataset (X,Y) is constructed from the\ndocuments by a sliding window with the stride of\none sentence, resulting in 50M (x,y) pre-training\npairs.\n3.2 Unsupervised Pre-training\nTo understand the performance of PALM pre-\ntraining, we compare generation quality of the pre-\ntrained models of PALM and MASS3. Speciﬁcally,\nwe feed a few sentences from a news article to both\npre-trained models, and the models generate a con-\ntinuation of the input sentences by beam search\nwith a beam of size 5. The news articles from\nCNN 4 are used as input text to eliminate the possi-\nbility of the text present in the models’ pre-training\ncorpus, i.e., Wikipedia and BookCorpus.\nThe overall perplexity of PALM is 17.22, which\nis much better than MASS’s perplexity of 170.32,\nindicating PALM’s better language modeling. Ta-\nble 1 illustrates a couple of example continuations\ngenerated by PALM and MASS. In both examples,\nPALM generates ﬂuent and grammatical English,\nwhile MASS outputs a short sentence that is much\n3https://modelrelease.blob.core.\nwindows.net/mass/mass_summarization_\n1024.pth\n4https://drive.google.com/uc?export=\ndownload&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ\nless relevant to input text, since the MASS model\nwas trained on individual sentences. In the ﬁrst\nexample, it is interesting to observe that in addi-\ntion to summarizing the input content, PALM is\nable to make a non-trivial inference of the expected\nauction price and the ﬁnal selling price of the car\n(might not be factually accurate though). An infer-\nence is also made by PALM in the second example\nin addition to summarization, although the Cape\nVerde Islands National Park does not really exist.\nThese examples demonstrate that PALM pre-\ntraining has learned to infer and to reason from\nthe input text. Although in the pre-training phase\nthe generated content may not be factually accu-\nrate in the absence of rich context, the capability of\ninference can be transferred downstream by ﬁne-\ntuning on speciﬁc generation tasks.\n3.3 Fine-tuning on Generative QA\nWe also experiment with ﬁne-tuning PALM on sev-\neral downstream generation tasks. The MARCO\nbenchmark (Nguyen et al., 2016) released by Mi-\ncrosoft is a good ﬁt for evaluating generative QA\nmodels. In the MARCO dataset, the questions are\nuser queries issued to the Bing search engine and\nthe contextual passages are from real web docu-\nments. The data has been split into a training set\n(153,725 QA pairs), a dev set (12,467 QA pairs)\nand a test set (101,092 questions with unpublished\nanswers). To evaluate the generative capability, we\nfocus on the Q&A + Natural Language Generation\ntask, the goal of which is to provide the best answer\navailable in natural language that could be used by\na smart device / digital assistant.\nThe answers are human-generated and not neces-\nsarily sub-spans of the contextual passages, so we\nuse the ROUGE-L (Lin, 2004) metric for our eval-\nuation to measure the quality of generated answers\nagainst the ground truth.\nWe ﬁne-tune the pre-trained PALM on the\nMARCO training set for 10 epochs. We set the\nbatch size to 64, the learning rate to 1e-5, and the\nmaximum input length to 512. The other hyper-\nparameters are kept the same as pre-training. In\nﬁne-tuning PALM, the encoder takes as input xa\ncontextual passage concatenated with a question at\nthe end, and the decoder takes an answer as inputy.\nDuring decoding, we use beam search with a beam\nof size 5.\nTable 2 presents the answer generation results\non the test set obtained from the ofﬁcial MARCO\nMethod Rouge-L\nConZNet (Indurthi et al., 2018) 0.421\nReader-Writer 0.439\nKIGN-QA 0.441\nSNET+CES2S 0.450\nCommunicating BERT 0.483\nVNET (Wang et al., 2018) 0.484\nSelector NLGEN 0.487\nBERT+Multi-Pointer 0.495\nMasque (Nishida et al., 2019) 0.496\nPALM 0.498\nTable 2: Test results of answer generation on the ofﬁ-\ncial MARCO leaderboard as of December 9, 2019.\nleaderboard. PALM achieves the 1st place on the\nleaderboard, outperforming all competing meth-\nods in generation quality. Note that PALM pre-\ntrains a single model, while some of the top-\nperforming methods are ensemble models, such\nas Masque, on the leaderboard. Crucially, the su-\nperiority of PALM-single over Masque-ensemble\nwith pre-trained ELMo (Peters et al., 2018) and\nBERT-based methods clearly demonstrates the ef-\nfectiveness and generalizability of PALM over the\nother pre-training approaches in language model-\ning.\n3.4 Fine-tuning on Summarization\nText summarization produces a concise and ﬂuent\nsummary conveying the key information in the in-\nput (e.g., a news article). We focus on abstractive\nsummarization, a generation task where the sum-\nmary is not constrained to reusing the phrases or\nsentences in the input text. We conduct experi-\nments on both the CNN/DailyMail dataset (Her-\nmann et al., 2015) and the Gigaword dataset (Graff\nand Cieri, 2003). The CNN/DailyMail dataset con-\ntains 93K news articles from CNN and 220K arti-\ncles from Daily Mail, while the Gigaword dataset\nconsists of a total of 3.8M article-title pairs. We\ntake the articles as the input to the encoder and the\nsummary for the decoder. We adopt the same opti-\nmization hyperparameters from generative QA ﬁne-\ntuning for the summarization task. The F1 scores\nof Rouge-1, Rouge-2 and Rouge-L are reported on\nthe test set of both datasets for evaluation.\nTable 3 shows the results of abstractive sum-\nmarization on the CNN/DailyMail test set and\nthe Gigaword test set. PALM achieves better\nperformance than all strong summarization mod-\nCNN/DailyMail Gigaword\nRG-1 RG-2 RG-L RG-1 RG-2 RG-L\nBERTSUMABS (Liu and Lapata, 2019) 41.72 19.39 38.76 - - -\nMASS (Song et al., 2019) 42.12 19.50 39.01 38.13 19.81 35.62\nUniLMLARGE (Dong et al., 2019) 43.33 20.21 40.51 38.45 19.45 35.75\nT5LARGE (Raffel et al., 2019) 42.50 20.68 39.75 - - -\nBARTLARGE (Lewis et al., 2019) 44.16 21.28 40.90 - - -\nPEGASUS (Zhang et al., 2019) 44.17 21.47 41.11 39.12 19.86 36.24\nERNIE-GENLARGE (Xiao et al., 2020) 44.02 21.17 41.26 39.25 20.25 36.53\nPALM 42.71 19.97 39.71 38.75 19.79 35.98\nPALMLARGE 44.30 21.12 41.41 39.45 20.37 36.75\nTable 3: Results of abstractive summarization on the CNN/DailyMail test set and the Gigaword test set. RG is\nshort for ROUGE\nels with pre-training recently proposed, including\nUniLM (Dong et al., 2019), T5 (Raffel et al., 2019),\nBART (Lewis et al., 2019), PEGASUS (Zhang\net al., 2019) and ERNIE-GEN (Xiao et al., 2020).\nBy consistently outperforming the pre-training\nmethods, PALM conﬁrms its effectiveness in lever-\naging unsupervision signals for language genera-\ntion.\n3.5 Fine-tuning on Question Generation\nWe conduct experiments for the answer-aware ques-\ntion generation task. Given an input passage and an\nanswer span, question generation aims to generate\na question that leads to the answer. Following the\npractice in (Zhao et al., 2018; Dong et al., 2019),\nwe use the SQuAD 1.1 (Rajpurkar et al., 2016)\ndataset, and the BLEU-4, METEOR and ROUGE-\nL metrics for evaluation.\nAs shown in Table 4, PALM outperforms all pre-\nvious question generation systems and achieves\na new state-of-the-art result on BLEU-4 and\nROUGE-L for question generation on the SQuAD\n1.1 dataset.\nMethod BLEU-4 MTR RG-L\nCorefNQGa 15.16 19.12 -\nMP-GSNb 16.38 20.25 44.48\nUNILMc 22.88 24.94 51.80\nERNIE d 22.28 25.13 50.58\nERNIE-GENLARGE d 24.03 26.31 52.36\nPALM 22.78 25.02 50.96\nPALMLARGE 24.11 25.85 52.38\nTable 4: Question generation results on the SQuAD\ndataset. MTR is short for METEOR and RG is short\nfor ROUGE. a (Du and Cardie, 2018); b (Zhao et al.,\n2018); c (Dong et al., 2019); d (Xiao et al., 2020).\n3.6 Fine-tuning on Response Generation\nConversational response generation aims to pro-\nduce a ﬂexible response to a conversation (Vinyals\nand Le, 2015). Following MASS, we conduct\nexperiments on the Cornell Movie Dialog cor-\npus5 (Danescu-Niculescu-Mizil and Lee, 2011)\nthat contains 140K conversation pairs, and use the\ntraining/test splits provided by the dataset. The\nsame training hyperparameters from generative QA\nﬁne-tuning are adopted on the response generation\ntask. We report the results in perplexity follow-\ning (Vinyals and Le, 2015) (lower is better).\nWe compare PALM with the competing meth-\nods including the baseline trained on the data\npairs available and the pre-trained BERT+LM and\nMASS. Following MASS, we train every model on\n10K pairs randomly sampled and all 110K training\npairs. As shown in Table 5, PALM signiﬁcantly\nperforms better than all the competitors by a large\nmargin on both the 10K and 110K data, demon-\nstrating its capability in generating responses to\ncontext thanks to its new pre-training objectives.\n3.7 Ablation Studies\nWe conduct ablation studies to assess the individual\ncontribution of every component in PALM. Table 6\nreports the results of full PALM and its ablations\non the CNN/Daily Mail summarization dataset.\nWe evaluate how much the pointer-generator net-\nwork contributes to generation quality by remov-\ning it from PALM pre-training. This ablation re-\nsults in a drop from 39.71 to 39.49 on Rouge-L,\ndemonstrating the role of the pointer-generator in\ngenerative modeling. Given the slight drop, one\nmay choose to exclude it from the full model for\n5https://github.com/suriyadeepan/\ndatasets/tree/master/seq2seq/cornell_\nmovie_corpus\nMethod Perplexity Perplexity\n(10K Data) (110K Data)\nBaseline 82.39 26.38\nBERT+LM 80.11 24.84\nMASS 74.32 23.52\nPALM 45.43 21.98\nTable 5: Results of conversational response generation\nin terms of perplexity on Cornell Movie Dialog corpus\n(lower is better).\ntraining efﬁciency. In our experiments, the pointer-\ngenerator is used in every generation task for opti-\nmal generation performance.\nTo study the effect of the pre-trained encoder\nand decoder in PALM, we ablate autoencoding and\nautoregression by randomly initializing the weights\nof the encoder and the decoder, respectively. The\nautoencoding and autoregression components both\nprove to be critical with signiﬁcant drops on the\nthree Rouge metrics after the ablation. Finally, we\nstudy the signiﬁcance of full PALM pre-training.\nOver 6.5% of performance degradation resulted\nfrom ablating pre-training clearly demonstrates the\npower of PALM in leveraging an unlabeled corpus\nfor downstream generation.\n4 Related Work\nELMo (Peters et al., 2018) is an early promi-\nnent pre-training method based on bidirectional\nLSTMs. It concatenates left-only and right-only\nrepresentations, but does not pre-train interactions\nbetween these features. GPT (Radford, 2018), GPT-\n2 (Radford et al., 2019) and GPT-3 (Brown et al.,\n2020) are proposed to base language modeling\non the Transformer architecture, and use only the\nTransformer decoder for pre-training. Edunov et\nal. (Edunov et al., 2019) examine different strate-\ngies (e.g., ELMo) to add contextualized embed-\ndings to sequence-to-sequence models, and observe\nthe most improvement by adding the learned em-\nbeddings to the encoder.\nBERT (Devlin et al., 2018) introduces Masked\nLanguage Modelling, which allows pre-training\nto learn interactions between left and right con-\ntext words. Recent work has shown that very\nstrong performance can be achieved by training for\nlonger (Liu et al., 2019), by tying parameters across\nlayers (Lan et al., 2019), and by masking spans in-\nstead of words (Joshi et al., 2019). However, BERT\ndoes not make predictions autoregressively, so it is\nnot effective for generation tasks.\nAblation RG-1 RG-2 RG-L\nPALM 42.71 19.97 39.71\n\u0017 pointer-generator 42.54 19.86 39.49\n\u0017 autoencoding 41.78 19.32 38.81\n\u0017 autoregression 41.89 19.48 38.92\n\u0017 pre-training 40.32 17.78 37.12\nTable 6: Ablation tests of PALM on the CNN/Daily\nMail summarization dataset.\nUniLMs (Dong et al., 2019; Hangbo et al.,\n2020) ﬁne-tune BERT with an ensemble of masks,\nsome of which use only leftward context, allowing\nUniLMs to be used for generation tasks. A differ-\nence between UniLMs and PALM is that UniLMs\nare not fully autoregressive in the pre-training pro-\ncess. In contrast, PALM reduces the mismatch\nbetween pre-training and context-conditioned gen-\neration tasks by forcing the decoder to predict the\ncontinuation of text input on an unlabeled corpus.\nMASS (Song et al., 2019) and BART (Lewis\net al., 2019) are the two pre-training methods most\nsimilar to PALM. In MASS, an input sequence\nwith a masked span of tokens is mapped to a se-\nquence consisting of the missing tokens, whereas\nBART is trained to reconstruct the original text\nfrom corrupted input with some masked tokens.\nThe difference in input & output representations\nbetween PALM and MASS & BART is detailed in\nSection 2.2.\n5 Conclusions\nIn this work, we propose PALM, a novel approach\nto pre-training an autoencoding and autoregressive\nlanguage model on a large unlabeled corpus, de-\nsigned to be ﬁne-tuned on downstream generation\nconditioned on context. It is built upon an ex-\ntension of the Transformer encoder-decoder, and\njointly pre-trains the encoder and the decoder in\nan autoencoding denoising stage followed by an\nautoregressive generation stage.\nPALM signiﬁcantly advances the state-of-the-art\nresults on a variety of context-conditioned genera-\ntion applications, including generative QA (Rank 1\non the MARCO leaderboard), abstractive summa-\nrization, question generation, and conversational\nresponse generation. It has been shown in prior\nwork (Liu et al., 2019) that training for more steps\nover a larger corpus can potentially improve the\nperformance of pre-training. Our future work will\nexplore the potential of training PALM for longer\non much more unlabeled text data.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nBin Bi, Chen Wu, Ming Yan, Wei Wang, Jiangnan\nXia, and Chenliang Li. 2019. Incorporating exter-\nnal knowledge into machine reading for generative\nquestion answering.\nT. Brown, B. Mann, Nick Ryder, Melanie Sub-\nbiah, J. Kaplan, P. Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-V oss, G. Kr ¨uger, Tom\nHenighan, R. Child, Aditya Ramesh, D. Ziegler, Jef-\nfrey Wu, Clemens Winter, Christopher Hesse, Mark\nChen, E. Sigler, Mateusz Litwin, Scott Gray, Ben-\njamin Chess, J. Clark, Christopher Berner, Sam Mc-\nCandlish, A. Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. ArXiv, abs/2005.14165.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In C. Cortes, N. D. Lawrence,\nD. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n28, pages 3079–3087. Curran Associates, Inc.\nCristian Danescu-Niculescu-Mizil and Lillian Lee.\n2011. Chameleons in imagined conversations: A\nnew approach to understanding coordination of lin-\nguistic style in dialogs. In Proceedings of the\n2nd Workshop on Cognitive Modeling and Compu-\ntational Linguistics, pages 76–87, Portland, Oregon,\nUSA. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Infor-\nmation Processing Systems 32, pages 13042–13054.\nCurran Associates, Inc.\nXinya Du and Claire Cardie. 2018. Harvest-\ning Paragraph-Level Question-Answer Pairs from\nWikipedia. arXiv e-prints, page arXiv:1805.05942.\nSergey Edunov, Alexei Baevski, and Michael Auli.\n2019. Pre-trained language model representations\nfor language generation. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 4052–4059, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nSebastian Gehrmann, Yuntian Deng, and Alexander M\nRush. 2018. Bottom-up abstractive summarization.\narXiv preprint arXiv:1808.10792.\nDavid Graff and Christopher Cieri. 2003. English giga-\nword. In Linguistic Data Consortium, Philadelphia.\nBao Hangbo, Dong Li, Wei Furu, Wang Wenhui, Yang\nNan, Liu Xiaodong, Wang Yu, Piao Songhao, Gao\nJianfeng, Zhou Ming, and Hon Hsiao-Wuen. 2020.\nUnilmv2: Pseudo-masked language models for uni-\nﬁed language model pre-training. In Proceedings\nof International Conference on Machine Learning\n(ICML).\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nKarl Moritz Hermann, Tom´as Kocisk´y, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. CoRR, abs/1506.03340.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Con-\nference Track Proceedings.\nSathish Reddy Indurthi, Seunghak Yu, Seohyun Back,\nand Heriberto Cuay ´ahuitl. 2018. Cut to the chase:\nA context zoom-in network for reading comprehen-\nsion. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 570–575, Brussels, Belgium. Association for\nComputational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2019.\nSpanBERT: Improving pre-training by repre-\nsenting and predicting spans. arXiv preprint\narXiv:1907.10529.\nZhen-Zhong Lan, Mingda Chen, Sebastian Good-\nman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. 2019. Albert: A lite bert for self-\nsupervised learning of language representations.\nArXiv, abs/1909.11942.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer.\n2019. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Transla-\ntion, and Comprehension. arXiv e-prints.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Proceedings of the ACL\nWorkshop: Text Summarization Braches Out 2004,\npage 10.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. MS MARCO: A human generated machine\nreading comprehension dataset. In Proceedings\nof the Workshop on Cognitive Computation: Inte-\ngrating neural and symbolic approaches 2016 co-\nlocated with the 30th Annual Conference on Neural\nInformation Processing Systems (NIPS).\nKyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazu-\ntoshi Shinoda, Atsushi Otsuka, Hisako Asano, and\nJunji Tomita. 2019. Multi-style generative reading\ncomprehension. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2273–2284, Florence, Italy. Associa-\ntion for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford. 2018. Improving language understand-\ning by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. arXiv e-prints,\npage arXiv:1606.05250.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 379–389, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nAbigail See, Peter J Liu, and Christopher D Man-\nning. 2017a. Get to the point: Summarization with\npointer-generator networks. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1073–1083. Association for Computational Linguis-\ntics.\nAbigail See, Peter J. Liu, and Christopher D. Man-\nning. 2017b. Get to the point: Summarization with\npointer-generator networks. In Proceedings of the\n55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1073–1083, Vancouver, Canada. Association\nfor Computational Linguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: masked sequence to se-\nquence pre-training for language generation. CoRR,\nabs/1905.02450.\nChuanqi Tan, Furu Wei, Nan Yang, Weifeng Lv, and\nMing Zhou. 2017. S-net: From answer extraction to\nanswer generation for machine reading comprehen-\nsion. CoRR, abs/1706.04815.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nOriol Vinyals and Quoc Le. 2015. A neural conver-\nsational model. ICML Deep Learning Workshop,\n2015.\nYizhong Wang, Kai Liu, Jing Liu, Wei He, Yajuan Lyu,\nHua Wu, Sujian Li, and Haifeng Wang. 2018. Multi-\npassage machine reading comprehension with cross-\npassage answer veriﬁcation. In Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2018, Melbourne, Aus-\ntralia, July 15-20, 2018, Volume 1: Long Papers,\npages 1918–1927.\nDongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. ERNIE-\nGEN: An Enhanced Multi-Flow Pre-training and\nFine-tuning Framework for Natural Language Gen-\neration. arXiv e-prints, page arXiv:2001.11314.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2019. PEGASUS: Pre-training with Ex-\ntracted Gap-sentences for Abstractive Summariza-\ntion. arXiv e-prints, page arXiv:1912.08777.\nYao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa\nKe. 2018. Paragraph-level neural question gener-\nation with maxout pointer and gated self-attention\nnetworks. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 3901–3910, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. CoRR, abs/1506.06724.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8336024284362793
    },
    {
      "name": "Computer science",
      "score": 0.8233585357666016
    },
    {
      "name": "Generative grammar",
      "score": 0.6999306678771973
    },
    {
      "name": "Transformer",
      "score": 0.6312291622161865
    },
    {
      "name": "Artificial intelligence",
      "score": 0.598756730556488
    },
    {
      "name": "Language model",
      "score": 0.564049243927002
    },
    {
      "name": "Natural language processing",
      "score": 0.5572364330291748
    },
    {
      "name": "Natural language generation",
      "score": 0.5433616638183594
    },
    {
      "name": "Autoregressive model",
      "score": 0.543269157409668
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.49301573634147644
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4723169505596161
    },
    {
      "name": "Text generation",
      "score": 0.451226145029068
    },
    {
      "name": "Natural language understanding",
      "score": 0.44930651783943176
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.44874969124794006
    },
    {
      "name": "Generative model",
      "score": 0.4163439869880676
    },
    {
      "name": "Machine learning",
      "score": 0.3527034819126129
    },
    {
      "name": "Natural language",
      "score": 0.3127874433994293
    },
    {
      "name": "Statistics",
      "score": 0.09389618039131165
    },
    {
      "name": "Mathematics",
      "score": 0.08516591787338257
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    }
  ]
}