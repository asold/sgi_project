{
  "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
  "url": "https://openalex.org/W3088599783",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3088900781",
      "name": "Samuel Gehman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2790629656",
      "name": "Suchin Gururangan",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2170935826",
      "name": "Maarten Sap",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133417374",
      "name": "Yejin Choi",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2183947846",
      "name": "Noah A. Smith",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2973379954",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W3031851887",
    "https://openalex.org/W2963790016",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970562804",
    "https://openalex.org/W2949678053",
    "https://openalex.org/W2585712495",
    "https://openalex.org/W1535357018",
    "https://openalex.org/W2489130013",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W2971307358",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W2963955897",
    "https://openalex.org/W2563826943",
    "https://openalex.org/W3086249591",
    "https://openalex.org/W2607388700",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2952566282",
    "https://openalex.org/W2972668795",
    "https://openalex.org/W2734862619",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2963283805",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2597603852",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3102914525",
    "https://openalex.org/W2608438166",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W3033639609",
    "https://openalex.org/W2970283086",
    "https://openalex.org/W2012942264",
    "https://openalex.org/W2915023677",
    "https://openalex.org/W3035296331",
    "https://openalex.org/W3133874049",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2165279024",
    "https://openalex.org/W3013451997",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2972735048",
    "https://openalex.org/W2972657613",
    "https://openalex.org/W3034937117",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W2301953046",
    "https://openalex.org/W1987545044",
    "https://openalex.org/W2971173235",
    "https://openalex.org/W2963205619",
    "https://openalex.org/W2972486443",
    "https://openalex.org/W2962788902",
    "https://openalex.org/W1736726159",
    "https://openalex.org/W2911227954",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2970395295",
    "https://openalex.org/W2947160092",
    "https://openalex.org/W2791170418",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2317486271",
    "https://openalex.org/W3047056223",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2964235839",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2926555354",
    "https://openalex.org/W3127086053",
    "https://openalex.org/W3043315258"
  ],
  "abstract": "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning \"bad\" words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356–3369\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n3356\nREALTOXICITY PROMPTS :\nEvaluating Neural Toxic Degeneration in Language Models\nSamuel Gehman⇧ Suchin Gururangan⇧† Maarten Sap⇧ Yejin Choi⇧† Noah A. Smith⇧†\n⇧Paul G. Allen School of Computer Science & Engineering, University of Washington\n†Allen Institute for Artiﬁcial Intelligence\nSeattle, USA\n{sgehman,sg01,msap,yejin,nasmith}@cs.washington.edu\nAbstract\nPretrained neural language models (LMs) are\nprone to generating racist, sexist, or otherwise\ntoxic language which hinders their safe deploy-\nment. We investigate the extent to which pre-\ntrained LMs can be prompted to generate toxic\nlanguage, and the effectiveness of controllable\ntext generation algorithms at preventing such\ntoxic degeneration. We create and release RE-\nALTOXICITY PROMPTS , a dataset of 100K nat-\nurally occurring, sentence-level prompts de-\nrived from a large corpus of English web\ntext, paired with toxicity scores from a widely-\nused toxicity classiﬁer. Using REALTOXICI -\nTYPROMPTS , we ﬁnd that pretrained LMs can\ndegenerate into toxic text even from seemingly\ninnocuous prompts. We empirically assess sev-\neral controllable generation methods, and ﬁnd\nthat while data- or compute-intensive methods\n(e.g., adaptive pretraining on non-toxic data)\nare more effective at steering away from toxic-\nity than simpler solutions (e.g., banning “bad”\nwords), no current method is failsafe against\nneural toxic degeneration. To pinpoint the po-\ntential cause of such persistent toxic degenera-\ntion, we analyze two web text corpora used to\npretrain several LMs (including GPT-2;Rad-\nford et al., 2019), and ﬁnd a signiﬁcant amount\nof offensive, factually unreliable, and other-\nwise toxic content. Our work provides a test\nbed for evaluating toxic generations by LMs\nand stresses the need for better data selection\nprocesses for pretraining.\n1 Introduction\nAlthough they are the backbone of many modern\nNLP systems (Devlin et al., 2019; Radford et al.,\n2019; Raffel et al., 2019), language models (LMs)\npretrained on large web text corpora suffer from\ndegenerate and biased behavior (Sheng et al., 2019;\nWallace et al., 2019). As illustrated in Figure1,\nthey can easily degenerate into toxicity, even with-\nout explicitly toxic prompts, which hinders their\n\u0015# \u0001( )\u0001./\u001c-/ \u001f\u0001.2 \u001c-$)\"\u0001\u001c/\u0001( ̧\u0001\u001e\u001c'' \u001f\u0001( \u0001̦̦̦\n\n̲(\u0001̥̥\u0001+ -\u001e )/\u0001.0- \u0001$/\u00012\u001c.\u0001.*( *) \u0001\u001d $)\"\u0001\u001c)\u0001̦̦̦\nʐɫ\n¥̏\nʍʎ\n̈ \nϭq\n̔\nΣ̈\n F\n4GCN6QZKEKV[2TQORVU./\u0003IGPGTCVKQPU\n\u0014*\u0001$!\u00014*0\u0001\"-\u001c\u001d\u0001\u001c\u00012*(\u001c)\u0001\u001d4\u0001/# \u0001̦̦̦\n\u0014*̧\u0001\n̲(\u0001./\u001c-/$)\"\u0001/*\u0001/#$)&\u0001.# ̲.\u0001!0''\u0001̦̦̦\u001c\b\n\u0014\u0013\b\n\u0015\u0019\b\n\u0017\u0019\b\nFigure 1: Non-toxic\n examples from REALTOXICI -\nTYPROMPTS , a new testbed for evaluating neural gen-\nerations and their toxicity. Despite not containing any\ntoxic language as measured by PERSPECTIVE API,\nthese prompts cause several pretrained LMs to system-\natically generate highly toxic text (shown in Table17\nin Appendix §E).\nsafe deployment (McGufﬁe and Newhouse, 2020).\nWe ﬁrst introduce a framework to systemat-\nically measure the risk of toxic degeneration\nby pretrained LMs. We release REALTOXICI -\nTYPROMPTS (§ 4), a set of 100K naturally oc-\ncurring prompts (i.e., sentence preﬁxes; Figure\n1) extracted from a large corpus of English web\ntext and paired with toxicity scores from a widely\nused and commercially deployed toxicity detector\n(PERSPECTIVE API). We show that popular LMs\nproduce toxic generations when conditioned on our\nprompts, even those that are non-toxic (§4.2).\nThen, as a possible mitigation strategy, we eval-\nuate controllable generation methods and quantify\ntheir ability to steer away from toxic content us-\ning REALTOXICITY PROMPTS (§ 5). We ﬁnd that\ncertain controllable methods (e.g., toxicity control\ntokens, swearword ﬁlters) are less successful than\n3357\nmore computationally or data-intensive methods\n(e.g., ﬁnetuning on non-toxic corpora). However,\nwe show that even our best steering methods can\nstill generate highly toxic content.\nFinally, to further investigate the potential cause\nof these phenomena, we present the ﬁrst large-\nscale analysis of toxicity inGPT-2 ’s training cor-\npus, OpenAI WebText, (OPEN AI-WT ; Radford\net al., 2019), as well as an in-depth analysis of\nits open-source replica,OPEN WEBTEXT CORPUS\n(OWTC ; Gokaslan and Cohen, 2019, § 6). We\nﬁnd non-negligible amounts of toxic, harmful, and\nabusive text in these corpora, which were used in\npretraining of several language models (including\nRoBERTa, CTRL , and GPT-2 ; Liu et al., 2019;\nKeskar et al., 2019, § 6.1). We identify additional\nissues with the data and its provenance, including\nlarge numbers of news articles shared on banned\nInternet communities or from factually unreliable\nsources (§ 6.2).\nOur ﬁndings highlight the difﬁculty of avoiding\ntoxicity in natural language generation (NLG) and\nillustrate a need to actively reconsider the content\nused in LM pretraining. We release our code and\ndata for tracking the progress towards combating\nthe critical issue of neural toxic degeneration.1,2\n2 Operationalizing Toxicity\nCharacterizing the toxicity of large corpora of natu-\nrally occurring or machine generated text is crucial\nto understanding toxic degeneration by language\nmodels. Unfortunately, such large scale prevents\nhuman annotations of toxicity (e.g., we score at\nleast 80 GB of text in§ 6). Therefore, we rely on\nPERSPECTIVE API3, an automated tool for toxic\nlanguage and hate speech detection. We acknowl-\nedge, however, that such tools are imperfect and\nsubject to a variety of biases, as discussed in§ 2.2\nand § 7.\n2.1 P ERSPECTIVE API TOXICITY\nWe use theTOXICITY 4 score fromPERSPECTIVE\nAPI, a widely used, commercially deployed toxic-\n1Due to their prevalence, we focus our study only on neural\nlanguage models, and therefore use the term “neural toxic de-\ngeneration.” Future work could examine whether non-neural\nlanguage models exhibit similar behavior.\n2http://toxicdegeneration.allenai.org/\n3https://github.com/conversationai/\nperspectiveapi\n4PERSPECTIVE API deﬁnes TOXICITY as a “rude, dis-\nrespectful, or unreasonable comment; likely to make people\nleave a discussion.”\nity detection tool. Accessed through an API,TOX-\nICITY corresponds to the prediction output of a\nCNN (Lecun et al., 1998) trained on a proprietary\ncorpus of comments from Wikipedia ,New York\nTimes, and other news sites with an AUC of 0.97.\nSince the model is calibrated using isotonic regres-\nsion (Zadrozny and Elkan, 2002),5 we can meaning-\nfully interpret the score as a probability of toxicity.\nIn our analyses, we label a prompt astoxic if it has\nTOXICITY \u0000 0.5, andnon-toxic otherwise.6\n2.2 Biases in Toxic Language Detection\nAlthough widely used, thePERSPECTIVE API and\nother hate speech detection systems and corpora\nexhibit biases against minorities and suffer from\nlow agreement in annotations (Waseem, 2016; Ross\net al., 2017), partially due to annotator identity in-\nﬂuencing their perception of hate speech (Cowan\nand Khatchadourian, 2003) and differences in anno-\ntation task setup (Sap et al., 2019). Notably, recent\nwork has found that systems are overestimating the\nprevalence of toxicity in text that contains a minor-\nity identity mention (e.g., “I’m a gay man”;Dixon\net al., 2018; Hutchinson et al., 2020) or text by\nracial minorities (e.g., text in African American En-\nglish; Sap et al., 2019; Davidson et al., 2019). This\nis partially due to detectors’ over-reliance on lex-\nical cues of toxicity (including swearwords, slurs,\nand other “bad” wordsDinan et al., 2019). We fur-\nther discuss and examine the effect of these biases\nin the Appendix, by assessing that the racial bias\nin toxicity is invariant with respect to model choice\n(Appendix § C.1) and analyzing the presence of\nprofanity and swearwords separately from toxicity\n(Appendix § C.2).\n3 Out-of-the-Box Generation Toxicity\nWe focus our investigation of toxic degeneration\nin ﬁve popular autoregressive Transformer-based\n(Vaswani et al., 2017) language models:GPT-1 ,\n5https://github.com/conversationai/\nperspectiveapi/blob/master/3-concepts/\nscore-normalization.md\n6To assess PERSPECTIVE API on human-generated\ntext, the ﬁrst three authors performed manual judg-\nments of toxicity of a sample of 100 documents from\nOWTC , and found an 88% pairwise agreement (Pearson\n⇢=0.83) with TOXICITY scores. To assess the API on\nmachine-generated text, among 100 generations from\nGPT-2 , our judgments had 80% pairwise agreement\nand Pearson ⇢=0.65 with TOXICITY . For further model\ninformation, we refer the reader to the model card forTOX-\nICITY : https://github.com/conversationai/\nperspectiveapi/blob/master/2-api/model-\ncards/English/toxicity.md\n3358\nGPT-2 , GPT-3 , CTRL , andCTRL-W IKI . GPT-1\n(Radford et al., 2018) is a 117M-parameter model\npretrained on a large corpus of English books (Zhu\net al., 2015). GPT-2 (speciﬁcally, GPT-2 -small;\nRadford et al., 2019), is a similarly sized model\npretrained onOPEN AI-WT , which contains 40GB\nof English web text and is described in§ 6.7 GPT-3\n(Brown et al., 2020) is pretrained on a mix of Com-\nmon Crawl, an expanded version ofOPEN AI-WT ,\nbooks corpora, and Wikipedia.8 In all experiments,\nwe use the 175B parameterGPT-3 model, also\nknown as DA VINCI in the OpenAI API.\nCTRL (Keskar et al., 2019) is a 1.63B parameter\nmodel that uses domain-speciﬁc control tokens for\nconditional language modelling. We analyze gener-\nations in two domains: web text (CTRL , Links\ncontrol token), and English Wikipedia (CTRL-\nWIKI , Wiki control token).\nGenerating from Models Unless otherwise\nnoted, we use nucleus sampling (Holtzman et al.,\n2020) with p = 0.9 to generate up to 20 tokens\n(see Appendix§ B.4 for additional details). All ex-\nperiments are carried out with the Hugging Face\nTransformers library (Wolf et al., 2019).\n3.1 Unprompted Toxicity in Neural Models\nTo quantify the risk associated with using pre-\ntrained language models for generation, we ﬁrst\nmeasure their propensity to generate toxic out-\nput conditionedonly on their respective start-of-\nsentence tokens.9 For each model, we ﬁrst generate\na pool of 10K spans, and then perform bootstrap es-\ntimation of the expected maximum toxicity forn \n10K generations, by sampling (with replacement)\nn generations from the pool 1K times each.\nOur results (Figure2) show that all ﬁve language\nmodels can degenerate into toxicity of over 0.5\nwithin 100 generations, and most only require 1K\ngenerations to exceed a maximum toxicity of 0.9\n(see Table15 and 16 in Appendix § E for exam-\nples). We ﬁnd similar patterns of expected maxi-\nmum toxicity forGPT-2 and CTRL , which have\nsigniﬁcantly more overlap in pretraining data than\nwith GPT-1 . Though trained on a much larger\ncorpus, GPT-3 ’s unprompted toxicity also mirrors\n7We ﬁnd similar toxic behavior inGPT-2 -small andGPT-\n2-medium, see Appendix §B.7 for details.\n8We access the GPT-3 model through OpenAI’s API\n(https://openai.com/api/).\n9For CTRL and CTRL-W IKI , we use theLinks and\nWiki control tokens; for GPT-2 and GPT-3 , we use the\n<|endoftext|> token; for GPT-1, we use “. ”.\nFigure 2: Neural models generate toxicity, even with no\nprompting. Here we display bootstrap estimates of the\nexpected maximum toxicity forN generations, with\nvariance bounds as shades. For example, we observe\nthat GPT-2 generates an expected maximum toxicity\nof 0.65 with just 100 unprompted generations.\nthat ofGPT-2 , which may be due to the fact that\nGPT-3 ’s training data was designed to be simi-\nlar toGPT-2 ’s training data (Brown et al., 2020).\nOn the other hand,GPT-1 generates higher levels\nof expected toxicity with fewer generations. This\nmay be explained by the correspondingly high lev-\nels of toxicity inGPT-1 ’s pretraining corpus (see\nAppendix § D.3 for details). We also observe that\nCTRL-W IKI has a signiﬁcantly lower expected\nmaximum toxicity than the other models. These\nresults suggest that models acquire toxicity from\ntheir pretraining data, which we analyze further in\n§ 6.\n4R EALTOXICITY PROMPTS\nTo systematically evaluate and compare the gen-\nerations from language models, we createREAL -\nTOXICITY PROMPTS as a testbed for toxicity in\nconditional language generation that mirrors real\nworld applications (e.g., autocomplete systems;\nChen et al., 2019; King, 2019). With this dataset,\nwe quantify the effect of prompt toxicity on the tox-\nicity of generation from our ﬁve language models.\n4.1 Prompt Creation and Selection\nWe select our prompts from sentences in theOPEN -\nWEBTEXT CORPUS (Gokaslan and Cohen, 2019),\na large corpus of English web text scraped from\n3359\nREALTOXICITY PROMPTS\n# Prompts Toxic Non-Toxic\n21,744 77,272\n# Tokens Prompts Continuations\n11.74.2 12.04.2\nAvg. Toxicity Prompts Continuations\n0.290.27 0.380.31\nTable 1: Data statistics of prompts and continuations in\nREALTOXICITY PROMPTS .\nExp. Max. Toxicity Toxicity Prob.\nModel Toxic Non-Toxic Toxic Non-Toxic\nGPT-1 0.78 0.18 0.580.22 0.90 0.60\nGPT-2 0.75 0.19 0.510.22 0.88 0.48\nGPT-3 0.75 0.20 0.520.23 0.87 0.50\nCTRL 0.73 0.20 0.520.21 0.85 0.50\nCTRL-W 0.71 0.20 0.490.21 0.82 0.44\nTable 2: Toxicity of generations conditioned on REAL -\nTOXICITY PROMPTS . Left: Expected maximum tox-\nicity (with standard deviations as subscripts) over 25\ngenerations. Right: The empirical probability of gen-\nerating toxic text at least once over 25 generations.\noutbound URLs from Reddit, for which we ex-\ntract TOXICITY scores with PERSPECTIVE API.\nTo obtain a stratiﬁed range of prompt toxicity,10 we\nsample 25K sentences from four equal-width toxic-\nity ranges ([0,.25), ..., [.75,1]), for a total of 100K\nsentences. We then split sentences in half, yielding\na prompt and acontinuation, both of which we also\nscore for toxicity. We include further preprocessing\ndetails in Appendix §A.\nOur ﬁnal dataset includes 100K naturally occur-\nring prompts, which average 11.7± 4.2 tokens in\nlength (Table1). REALTOXICITY PROMPTS con-\ntains 22K prompts withTOXICITY \u0000 0.5 (i.e.,toxic\nprompts). We ﬁnd that prompt and continuation\ntoxicity are slightly anti-correlated (r = –0.08, p \n0.001), indicating that, in our documents, toxicity\nas measured byPERSPECTIVE API is usually con-\nﬁned to one half of the sentence.\n4.2 Prompted Toxicity in Neural Models\nUsing REALTOXICITY PROMPTS and the same gen-\neration procedures outlined in§ 3, we measure toxic\ndegeneration in out-of-the-box neural language\nmodels. We characterize toxicity in prompted gen-\nerations with two metrics: 1) theexpected maxi-\n10Oversampling toxicity is necessary since it is a relatively\nrare phenomenon online (Founta et al., 2018).\nmum toxicityover k = 25 generations, which we\nestimate with a mean and standard deviation; and\n2) theempirical probabilityof generating a span\nwith TOXICITY \u0000 0.5 at least onceover k = 25\ngenerations. These metrics characterize toxic gen-\nerations along two axes: the higher the expected\nmaximum toxicity, the more toxic we expect the\nworst-case generations to be, and the higher the\ntoxicity probability, the more frequently the model\ngenerates toxicity.\nOur results show that while toxic prompts unsur-\nprisingly yield higher toxicity in generations,non-\ntoxic prompts still can still cause toxic generations\nat non-trivial rates (Table2). Speciﬁcally, all ﬁve\nmodels have a toxicity probability near or above\n0.5 for non-toxic prompts. This shows that even in\ninnocuous contexts these models can still generate\ntoxic content (as illustrated in Table17 and 18 in\nAppendix § E), suggesting the need for models to\n“unlearn” toxicity. Surprisingly, evenCTRL-W IKI\nhas similar generation toxicity to other models in\nprompted settings, even though it was trained on\njust Wikipedia. These results suggest that like the\nprovenance of pretraining data (§ 3.1), prompt con-\ntext can heavily inﬂuence generation toxicity, and\nthat steering generationsafter pretrainingis crucial\nto prevent toxic behavior in language models. In\nthe following section, we explore the effectiveness\nof a variety of such methods to avoid toxicity.\n5 Detoxifying Generations\nWe investigate the effectiveness of recent control-\nlable generation methods at steering away from tox-\nicity usingREALTOXICITY PROMPTS . Speciﬁcally,\nwe focus onGPT-2 as a base model for two detoxi-\nﬁcation techniques:data-based, where we pretrain\nthe language model further, anddecoding-based\nwhere we only change the generation strategy with-\nout changing model parameters.11 As described in\n§ 4.2, we sample 25 generations per prompt for each\nmodel. We describe hyperparameters and training\ndetails for all methods in Appendix §B.\n5.1 Data-Based Detoxiﬁcation\nWe consider two types of data-based detoxiﬁcation\nin which we continue pretraining on approximately\n150K documents from OWTC.12\n11We conﬁrm that our detoxiﬁed models are still reasonable\nlanguage models in terms of perplexity in Table10, Appendix\n§ B.6.\n12Described in Appendix§ B.3, our training corpora are\nfully disjoint from the prompts data.\n3360\nExp. Max. Toxicity Toxicity Prob.\nCategory Model Unprompted Toxic Non-Toxic Unprompted Toxic Non-Toxic\nBaseline GPT-2 0.44 0.17 0.750.19 0.510.22 0.33 0.88 0.48\nData-based\nDAPT (Non-Toxic) 0.300.13 0.570.23 0.370.19 0.09 0.59 0.23\nDAPT (Toxic) 0.80 0.16 0.850.15 0.690.23 0.93 0.96 0.77\nATCON 0.420.17 0.730.20 0.490.22 0.26 0.84 0.44\nDecoding-based\nVOCAB -SHIFT 0.430.18 0.700.21 0.460.22 0.31 0.80 0.39\nPPLM 0.280.11 0.520.26 0.320.19 0.05 0.49 0.17\nWORD FILTER 0.420.16 0.680.19 0.480.20 0.27 0.81 0.43\nTable 3:Left: Average maximum toxicity (with standard deviations as subscripts) over 25 generations.Right: The\nempirical probability of generating toxic text at least once over 25 generations. The best performing detoxiﬁcation\nmethod yielding thelowest toxicity per-category, is bolded. We display DAPT (Toxic) as a reference for the\neffectiveness of DAPT as a method of controlling LM behavior. All models are evaluated on a full dataset of 100K\nprompts, except PPLM, which is evaluated on a dataset of 10K prompts, due to computational budget.\nDomain-Adaptive Pretraining (DAPT) Using\nthe framework outlined inGururangan et al.(2020),\nwe perform an additional phase of pretraining on\nthe non-toxic subset of a balanced corpus with\nGPT-2 . For comparison, we also perform the ex-\nperiment using the toxic subset.\nAttribute Conditioning (ATCON) Inspired by\nFicler and Goldberg (2017) and Keskar et al.\n(2019), we prepend a corresponding toxicity at-\ntribute token (<|toxic|>, <|nontoxic|>) to\na random sample of documents and pretrain the\nGPT-2 language model further. In our generation\nexperiments, we prepend the<|nontoxic|> to-\nken to our prompts.\n5.2 Decoding-Based Detoxiﬁcation\nNoting the additional cost of training language\nmodels further, we explore three detoxifying strate-\ngies that only rely on altering the decoding algo-\nrithm and are therefore more readily usable by\nmany practitioners.\nVocabulary Shifting (VOCAB -SHIFT ) Inspired\nby Eisenstein et al.(2011) andGhosh et al.(2017),\nwe learn a 2-dimensional representation of toxicity\nand non-toxicity for every token inGPT-2 ’s vocab-\nulary, which we then use to boost the likelihood of\nnon-toxic tokens. Given the language model’s un-\nnormalized probability (logits) over the vocabulary,\nwe add the term\u0000W · t, where t 2 R2 encodes\n(non-)toxicity, andW 2 RV represents the associ-\nations between each token and (non-)toxicity, and\n\u0000 is the boosting strength. We set\u0000 = 3 for all\nexperiments. We learn this representation using the\ntoxicity labels on the balanced corpus described in\n§ 5.1 (See Appendix §B.3 for more details).\nWord Filtering (WORD FILTER ) We also im-\nplement a language model blocklist, disallowing a\nset of words from being generated byGPT-2 .W e\nset the probability of generating any word from a\nlist13 of profanity, slurs, and swearwords to zero.\nPPLM We use the recently released PPLM\n(Dathathri et al., 2020). This decoding method\noperates onGPT-2 by altering the past and present\nhidden representations to better reﬂect the desired\nattributes, using gradients from a discriminator (see\nDathathri et al., 2020, for further details). In our\nexperiments, we steer generations using the toxic-\nity classiﬁer released by the authors and the Hug-\nging Face implementation. ForPPLM , we only\nsample 10 generations per prompt, and evaluate\nwith 10K prompts total, due to this decoding strat-\negy being extremely computationally intensive (14\nsec/generation, vs. 0.2 sec for GPT-2).\n5.3 Effect of Controllable Solutions on\nGeneration Toxicity\nWe investigate the effectiveness of our detoxiﬁca-\ntion methods underREALTOXICITY PROMPTS , fol-\nlowing the same generation procedures and experi-\nmental setups outlined in§ 4. Listed in Table3, our\nresults show that steering does not completely solve\nneural toxic degeneration, though all proposed tech-\nniques do reduce toxic behavior inGPT-2 . Of all\nmethods, DAPT (Non-Toxic), vocabulary shifting,\nand PPLM yield the lowest toxicity in generation.\nDespite its simplicity,DAPT (Non-Toxic) is one of\nthe most effective methods for steering away from\n13List of Dirty, Naughty, Obscene, and Otherwise Bad\nWords, downloaded from https://github.com/\nLDNOOBW/List-of-Dirty-Naughty-Obscene-\nand-Otherwise-Bad-Words .\n3361\ntoxicity, highlighting the importance of pretraining\ndata in neural toxic degeneration.\nPrompts That Challenge All ModelsWe ﬁnd\nthat certain prompts consistently cause all models\nto generate toxicity (e.g., the four prompts in Figure\n1). Speciﬁcally, there are 327 prompts that yielded\nat least one generation with 0.9TOXICITY from all\nmodels, and 1,225 prompts when considering only\nthe out-of-the-box language models (i.e.,GPT-1 ,\nGPT-2 , GPT-3 , CTRL , CTRL-W IKI ).14 From\nqualitative investigations, these prompts tended to\neither be toxic themselves, or if innocuous, they\ncontain opening quotes or preﬁxes of multiword\nexpressions such as “full of-” (Figure1). Addition-\nally, we ﬁnd that at least 10% of those 1.2K come\nfrom factually unreliable news sources or appear in\nbanned or quarantined subreddits.\n6 Analyzing Toxicity in Web Text\nTo further investigate the phenomenon of neural\ntoxic degeneration, and partially motivated by the\nsurprising effectiveness of domain-adaptive pre-\ntraining on non-toxic data, we turn our focus to two\ncorpora used to pretrain several language models.\nSpeciﬁcally, we quantify the toxicity inOPEN AI-\nWT (GPT-2 ’s training data;Radford et al., 2019)\nand its open-source replicaOWTC (Gokaslan and\nCohen, 2019), inspired by previous work in analyz-\ning social biases in large text corpora (Fast et al.,\n2016). Then, we investigate the provenance of the\ndata in these corpora, quantifying how many docu-\nments come from factually unreliable news sites or\nwere shared on quarantined or banned subreddits.\nOWTC is a large corpus of English web text\nscraped from outbound URLs in submissions on\nReddit communities (subreddits). In the creation of\nOWTC , only links included in posts with a “karma”\n(i.e., popularity) score of 3 or more were consid-\nered. Following the links, only English documents\nlonger than 128 tokens are included in this corpus,\namounting to 38 GB of text from about 8M doc-\numents. To allow for further analyses, we parse\nthe URLs given withOWTC documents to ex-\ntract the domain (often a news website, Figure5\nin Appendix§ D; Sharoff, 2020), which we cross-\nreference with news factuality ratings byBaly et al.\n(2018). We additionally cross-reference publicly\n14When releasingREALTOXICITY PROMPTS , we will in-\nclude a ﬂag for prompts belong to this challenging subset.\nFigure 3: TOXICITY scores of documents in OWTC\n(top) and OPEN AI-WT (bottom). y-axis is in log-scale,\nand color gradient follows magnitude inx-axis. We\nconsider a document toxic if its TOXICITY is \u0000 0.5.\nWe additionally display the estimated total % of toxic\ndocuments in each corpus above each subplot.\navailable Reddit dumps15 to identify which sub-\nreddits the URLs were submitted to. We include\nfurther details onOWTC and metadata linking in\nAppendix § D.\nOPEN AI-WT is the pretraining corpus forGPT-\n2 (Radford et al., 2019), also containing about 8M\ndocuments. Following OWTC , authors gathered\nURLs from Reddit, though from a different (but\noverlapping) timespan. Additionally, authors ﬁl-\ntered content using a blocklist of sexually-explicit\nand otherwise offensive subreddits.16 This corpus\ndoes not come paired with URL metadata.\nOverlap We ﬁnd about 29% overlap between the\ntwo corpora, using a large-scale similarity search\nwith locality-sensitive hashing (Rajaraman and Ull-\nman, 2011, see AppendixD for details). We ﬁnd\n15https://pushshift.io16https://github.com/openai/gpt-\n2/blob/master/model_card.md\n3362\nFigure 4: Top: Factual reliability in news sites that\nmake up OWTC.Bottom: Unreliable news sources in\nOWTC have a much higher proportion of toxic con-\ntent.\nthat at least 2.3M documents inOPEN AI-WT also\nappear in OWTC.\n6.1 Toxicity in Web Text\nShown in Figure3, we ﬁnd that both corpora con-\ntain non-negligible amounts of toxicity, with 2.1%\nof OWTC having TOXICITY \u0000 0.5, and 4.3% of\nOPEN AI-WT . These rates are in line withFounta\net al.(2018), who ﬁnd that the prevalence of abu-\nsive or toxic content online roughly ranges be-\ntween 0.1% and 3%, and suggest that these corpora\nmerely reﬂect the “natural” rates of toxicity. We\nnote that, despiteRadford et al.(2019) employing a\nblocklist of subreddits and “bad” words, the toxic-\nity inOPEN AI-WT is twice the amount inOWTC .\nWe show similar rates of toxicity using alternative\nPERSPECTIVE API labels on these corpora in Ta-\nble 12 in Appendix §D.\n6.2 Sources of Toxic Content in Web Text\nSince Reddit is known to have hosted communities\nthat endorse hateful norms and conspiracy theories\n(Romano, 2017), we investigate the provenance\nof data in our web text corpora. Speciﬁcally, we\nquantify the variation of a document’s toxicity with\nrespect to the reliability of its host news site and\n0.84 TOXICITY SCORE\nPosted to/r/TheDonald(quarantined)\n”[...] Criticism of Hillary is sexist! [...] But\nMelania Trump is a“dumb bitch”with a stupid accent\nwho needs to be deported. The left has no problem\nwith misogyny, so long as the target is a conservative\nwoman. [...] You can tell Melania trump doesn’t\neven understand what she’s saying in that speech haha\nI’m pretty sure she can’t actually speak english[...]”\n0.61 TOXICITY SCORE\nPosted to/r/WhiteRights(banned)\n”Germans [...] have a great new term for the\nlying, anti White media: “L¨ugenpresse” roughly translates as\n“lying press” [...] RegardingIslamic terrorists slaughteringour\npeople in France, England, tourist places in Libya and Egypt [...]\nInstead thelying Libsat the New York Daily News demand\nmore gun control ACTION [...] there is no law against publicly\nshaming theworst, most evil media peoplewho like and slan-\nder innocent victims ofIslamic terrorists, mass murderers.”\nTable 4: Examples of (purposefully uncensored) toxic\ndocuments that appear in GPT-2’s training corpus, that\nwere also submitted to quarantined or banned subred-\ndits. We highlight spans that contribute to the overall\ntoxicity of the document, which we identify manually.\nthe nature of the subreddits to which it was posted.\nToxicity from Unreliable News SitesGathering\nall documents inOWTC associated with a news\nsite, and cross-referencing reliability ratings from\nBaly et al.(2018), we ﬁnd that news reliability cor-\nrelates negatively with the proportion of documents\nthat are toxic (Spearman⇢ = –0.35). As shown in\nFigure 4, while low reliability news sites are less\nprevalent inOWTC , they contain more toxic doc-\numents compared to higher reliability news sites.\nAdditionally, we ﬁnd that at least 12% (272K) of\nthe overlappingOPEN AI-WT and OWTC docu-\nments with news reliability ratings come from low\nor mixed reliability news sites.\nToxicity from Quarantined or Banned Subred-\ndits Our analyses show that a non-trivial portion\nof OWTC documents (at least 3%, 212K) come\nfrom links shared on banned or quarantined subred-\ndits.17 Unsurprisingly, documents shared on those\nsubreddits contain substantially more toxicity than\nthose from standard subreddits (see Figure10 in\nAppendix § D), conﬁrming Reddit users’ propensity\nto share oppressive and abusive content (Massa-\n17Quarantined subreddits are special-access only and\neasily scraped, whereas banned subreddits are inaccessi-\nble via the website and only available in data dumps.\nFor more details, see https://en.wikipedia.org/\nwiki/Controversial_Reddit_communities.\n3363\nnari, 2017; Mohan et al., 2017; Rajadesingan et al.,\n2020; Aran et al., 2020). From the overlapping\nOPEN AI-WT and OWTC documents, we ﬁnd that\nat least 63K documents were shared on banned or\nquarantined subreddits. With two example docu-\nments shown in Table4, GPT-2 was pretrained\non at least 40K documents from the quarantined\n/r/The Donald, and 4K documents from the banned\n/r/WhiteRights.\n7 Discussion and Recommendations\nOverall, our investigations demonstrate that toxic-\nity is a prevalent issue in both neural language gen-\neration and web text corpora. Although they show\nsome reduction in toxicity, steering methods do not\nfully protect neural models from toxic degenera-\ntion (§ 5). Additionally, the corpora that language\nmodels are pretrained on contain non-negligible\namounts of toxic, abusive, and untrustworthy con-\ntent (§ 6). Some implications of our ﬁndings are\ndiscussed below.\nEffectiveness of “Forgetting” Toxicity Our\nﬁndings on data-based steering methods show that\nadaptive pretraining lowers a model’s propensity\nto unpromptedly generate toxic language, but that\nits prompted generations can still be toxic. This\nraises the question: can language models ever fully\n“forget” toxic pretraining data through further adap-\ntation (Kirkpatrick et al., 2017; Gururangan et al.,\n2020)? The non-trivial amounts of toxicity gen-\nerated by DAPT suggest that perhaps language\nmodels may be “memorizing” the toxicity in pre-\ntraining data (Carlini et al., 2019) or that toxic\nexamples may be more salient for the model and\nhence harder to unlearn (Koh and Liang, 2017). Fu-\nture work could explore whether some variants of\ntoxicity are harder to forget than others, or whether\nthe biases of models used to select training data\nfor steering introduce unwanted side effects in lan-\nguage model behavior after adaptation.\nDecoding with a Purpose Our analyses also\nhighlight the promise of certain decoding meth-\nods, such asPPLM (Dathathri et al., 2020), which\nis among the most effective methods we tested at\navoiding toxicity with toxic prompts. In addition\nto automated toxicity classiﬁers, future work could\nexplore the use of handpicked toxic documents as\n“negative examples” to avoid toxicity in generation.\nFuture work could also investigate infusing models\nwith more sophisticated or nuanced representations\nof social biases (Ma et al., 2020).\nChoice of Pretraining Data As pretrained lan-\nguage models grow in size (Brown et al., 2020), so\ndoes their need for larger corpora, often drawn from\neasily accessible and abundant web text. However,\nour analyses reveal toxicity in web text data that\nlikely enable language models to generate even un-\nprompted toxicity (§ 3.1). Our ﬁndings raise several\npractical and ethical concerns.\nFirst, analysis of pretraining data is a crucial\nﬁrst step towards understanding toxic, biased, or\notherwise degenerate behavior of language models.\nTherefore, echoing calls for transparency in NLP\nresearch (Bender and Friedman, 2018; Mitchell\net al., 2019; Dodge et al., 2019), we recommend re-\nsearchers publicly releaseall relevant information\nduring data collection (e.g., original text, source\nURLs, timestamps, platform-speciﬁc metadata)\nwhen building pretraining corpora.\nSecond, using Reddit popularity as a curation\nheuristic introduces representational harm (Barocas\net al., 2017) by biasing the populations whose lan-\nguage and perspectives are included in pretraining\n(e.g., Reddit users skew male;Barthel et al., 2016).\nThis raises the question of who decides whose\nvoices are going to be learned by the language\nmodel, and whose voices are excluded. Following\nBlodgett et al.(2020), we recommend a reexam-\nination of the relationship between NLP systems\nand their end users, using methods from human-\ncentered design, such as value-sensitive (Fried-\nman et al., 2008) or participatory design (Sanders,\n2002; DiSalvo et al., 2012; Denton et al., 2020),\nand archival data collection (Jo and Gebru, 2020).\nGiven the potential for misuse and harm, we also\necho calls for improving policy around public re-\nlease of large language models (Zellers et al., 2019;\nMcGufﬁe and Newhouse, 2020).\nIn general, the potential mismatch between the\nintent of curating pretraining data and its opera-\ntionalization (e.g., karma thresholding, ﬁltering out\nspeciﬁc slurs and swearwords) biases the language\nmodel’s pretraining data and behavior (Jacobs and\nWallach, 2019). For example, ﬁltering data based\non PERSPECTIVE API could lead to a decrease in\ntext by African American authors in pretraining\ndata due to well-documented racial bias (Sap et al.,\n2019), which could lead to decreased performance\non text written by non-White users. To avoid harm,\nresearchers should be mindful and explicit about\nthese decisions and engage with the end users of\n3364\nthe technology during these design phases.\nImproving Toxicity Detection With the release\nof REALTOXICITY PROMPTS , we hope to encour-\nage large-scale, systematic evaluations of detoxiﬁ-\ncation techniques for language models. However,\nthe conclusions one can make about the effective-\nness of a detoxiﬁcation method are limited by the\nbiases of the model used to detect toxicity (§ 2.2).\nTo combat these issues, we encourage further work\non detecting and controlling different types of toxic-\nity and undesirable social biases in generation, e.g.,\nrudeness (Danescu-Niculescu-Mizil et al., 2013),\nhate speech (Golbeck et al., 2017), or microag-\ngressions (Breitfeller et al., 2019). Additionally,\nmeasures of bias could be multi-dimensional (e.g.,\nDinan et al., 2020), include explanations (e.g.,Sap\net al., 2020), or be evolving over time (e.g., using\nsimilarity to toxic online content).\nLimitations We describe several limitations of\nour study. First, as noted in§ 2.2, we use an im-\nperfect measure of toxicity that could bias the tox-\nicity towards lexical cues, failing to detect more\nsubtle biases and incorrectly ﬂagging non-toxic\ncontent. Second, our analyses are limited to the\nﬁve language models considered (and their steered\nvariants). Further work could extend our analy-\nses to toxicity to masked language models (Wang\nand Cho, 2019), among others. Lastly, because\nOPEN AI-WT does not have available metadata,\nand due to the imperfect coverage of our subreddit\nand news reliability data, we only provide lower\nbound estimates of toxicity in web text corpora.\n8 Related Work\nA wealth of work has shown that toxicity and so-\ncial biases in training data are acquired by large\npretrained sentence encoders (e.g., gender bias in\nBERT; May et al., 2019; Zhao et al., 2019; Basta\net al., 2019; Kurita et al., 2019). However, fewer\nstudies have investigated toxicity in autoregressive\nlanguage models, whose generations also suffer\nfrom incoherence, blandness, and repetitiveness\n(Holtzman et al., 2020; Welleck et al., 2019).\nSimilar in spirit toREALTOXICITY PROMPTS ,\nWallace et al.(2019) ﬁnd universal adversarial\ntriggers, nonsensical prompts that trigger toxic gen-\nerations inGPT-2 . In this work, we ﬁnd and re-\nlease naturally occurringprompts from web text\nthat trigger toxicity, and compare toxic output in\nseveral language models.\nMost closely related to this work,Sheng et al.\n(2019) use a set of 60 templated prompts that\nmention majority or minority identities to study\nthe social biases in generations by out-of-the-box\npretrained language models. In our work, we\nstudy toxic degeneration by both out-of-the-box\nand controlled models using 100K naturally occur-\nring prompts, including some that do not contain\nidentity mentions (see Figure1). Additionally, our\nwork focuses on the broad phenomenon of toxicity\nin generations, whereasSheng et al.(2019) study\nthe sentiment and regard expressed by a model’s\ngeneration towards demographic identities.\nThe creation ofREALTOXICITY PROMPTS was\npartly inspired by work in detecting conversational\npatterns that can cause derailment into antisocial\nbehavior in online conversations (Zhang et al.,\n2018; Stoop et al., 2019; Karan andˇSnajder, 2019).\nOur work also draws from a strong line of re-\nsearch into controlling the outputs of language mod-\nels (Dathathri et al., 2020; Sudhakar et al., 2019;\nZiegler et al.; Keskar et al., 2019, inter alia).\n9 Conclusion\nWe introduceREALTOXICITY PROMPTS , a testbed\nof 100K prompts for evaluating the toxic degener-\nation in pretrained language models. Under this\nframework, we quantify the toxicity of multiple\npretrained language models and the effectiveness\nof methods for detoxifying generations. We then\nanalyze toxicity in two large web text corpora,\nincluding the GPT-2 pretraining corpus, to bet-\nter understand the root cause of toxic generations.\nFinally, we provide recommendations for gather-\ning pretraining data. The data, code, and interac-\ntive visualizations for this paper can be found at\nhttps://toxicdegeneration.allenai.org/.\nAcknowledgments\nWe thank colleagues at UW NLP and AI2 for their\nhelpful comments and feedback. We also thank\nJonathan Borchardt, Carissa Schoenick, and Sam\nSkjonsberg for helping us develop the demo web-\nsite. We thank OpenAI, speciﬁcally Bianca Mar-\ntin and Miles Brundage, for providing access to\nGPT-3 through the OpenAI API Academic Access\nProgram. This research was supported in part by\nNSF (IIS-1524371, IIS-1714566), DARPA under\nthe CwC program through the ARO (W911NF-\n15-1-0543), and DARPA under the MCS program\nthrough NIWC Paciﬁc (N66001-19-2-4031).\n3365\nReferences\nXavier Ferrer Aran, T. V . Nuenen, J. M. Such, and\nN. Criado. 2020.Discovering and categorising lan-\nguage biases in Reddit.\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov,\nJames Glass, and Preslav Nakov. 2018. Predict-\ning factuality of reporting and bias of news media\nsources. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3528–3539, Brussels, Belgium. Association\nfor Computational Linguistics.\nSolon Barocas, Kate Crawford, Aaron Shapiro, and\nHanna Wallach. 2017. The problem with bias: Al-\nlocative versus representational harms in machine\nlearning. InSIGCIS.\nMichael Barthel, Galen Stocking, Jesse Holcomb, and\nAmy Mitchell. 2016.Seven-in-Ten Reddit users get\nnews on the site. Accessed: 2020-6-2.\nChristine Basta, Marta R. Costa-juss`a, and Noe Casas.\n2019. Evaluating the underlying gender bias in con-\ntextualized word embeddings. InProceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 33–39, Florence, Italy. Associa-\ntion for Computational Linguistics.\nEmily M. Bender and Batya Friedman. 2018. Data\nstatements for natural language processing: Toward\nmitigating system bias and enabling better science.\nTransactions of the Association for Computational\nLinguistics, 6:587–604.\nSu Lin Blodgett, Solon Barocas, Hal Daum´e III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nSu Lin Blodgett, Lisa Green, and Brendan O’Connor.\n2016. Demographic dialectal variation in social me-\ndia: A case study of African-American English. In\nEMNLP.\nPiotr Bojanowski, Edouard Grave, Armand Joulin,\nand Tomas Mikolov. 2016. Enriching word vec-\ntors with subword information. arXiv preprint\narXiv:1607.04606.\nLuke Breitfeller, Emily Ahn, David Jurgens, and Yu-\nlia Tsvetkov. 2019.Finding microaggressions in the\nwild: A case for locating elusive phenomena in so-\ncial media posts. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1664–1674, Hong Kong, China. As-\nsociation for Computational Linguistics.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are Few-Shot\nlearners.\nNicholas Carlini, Chang Liu,´Ulfar Erlingsson, Jernej\nKos, and Dawn Xiaodong Song. 2019.The secret\nsharer: Evaluating and testing unintended memoriza-\ntion in neural networks. InUSENIX Security Sympo-\nsium.\nMia Xu Chen, Benjamin N. Lee, Gagan Bansal, Yuan\nCao, Shuyuan Zhang, Justin Y . Lu, Jackie Tsay, Yi-\nnan Wang, Andrew M. Dai, Zhifeng Chen, Timothy\nSohn, and Yonghui Wu. 2019. Gmail smart com-\npose: Real-time assisted writing. Proceedings of\nthe 25th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining.\nAnna Chung. 2019.How automated tools discriminate\nagainst black language. Accessed: 2019-03-02.\nGloria Cowan and D´esir´ee Khatchadourian. 2003.Em-\npathy, ways of knowing, and interdependence as\nmediators of gender differences in attitudes toward\nhate speech and freedom of speech. Psychology of\nwomen quarterly, 27(4):300–308.\nCristian Danescu-Niculescu-Mizil, Moritz Sudhof,\nDan Jurafsky, Jure Leskovec, and Christopher Potts.\n2013. A computational approach to politeness with\napplication to social factors. In Proceedings of the\n51st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n250–259, Soﬁa, Bulgaria. Association for Computa-\ntional Linguistics.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language mod-\nels: A simple approach to controlled text generation.\nIn International Conference on Learning Represen-\ntations.\nThomas Davidson, Debasmita Bhattacharya, and Ing-\nmar Weber. 2019. Racial bias in hate speech and\nabusive language detection datasets. InProceedings\nof the Third Workshop on Abusive Language Online,\npages 25–35, Florence, Italy. Association for Com-\nputational Linguistics.\nEmily Denton, Alex Hanna, Razvan Amironesei, An-\ndrew Smart, Hilary Nicole, and Morgan Klaus\nScheuerman. 2020. Bringing the people back in:\nContesting benchmark machine learning datasets.\nIn ICML Workshop on Participatory Approaches to\nMachine Learning.\n3366\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nEmily Dinan, A. Fan, Ledell Yu Wu, J. Weston,\nDouwe Kiela, and Adina Williams. 2020. Multi-\ndimensional gender bias classiﬁcation.\nEmily Dinan, Samuel Humeau, Bharath Chintagunta,\nand Jason Weston. 2019.Build it break it ﬁx it for\ndialogue safety: Robustness from adversarial human\nattack. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4537–4546, Hong Kong, China. Association for\nComputational Linguistics.\nCarl DiSalvo, Andrew Clement, and V olkmar Pipek.\n2012. Communities: Participatory design for, with\nand by communities.\nLucas Dixon, John Li, Jeffrey Scott Sorensen, Nithum\nThain, and Lucy Vasserman. 2018. Measuring\nand mitigating unintended bias in text classiﬁcation.\nProceedings of the 2018 AAAI/ACM Conference on\nAI, Ethics, and Society.\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy\nSchwartz, and Noah A. Smith. 2019. Show your\nwork: Improved reporting of experimental results.\nIn EMNLP, pages 2185–2194, Hong Kong, China.\nAssociation for Computational Linguistics.\nJacob Eisenstein, Amr Ahmed, and Eric P. Xing.\n2011. Sparse additive generative models of text. In\nProceedings of the 28th International Conference\non International Conference on Machine Learning,\nICML’11, page 1041–1048, Madison, WI, USA.\nOmnipress.\nEthan Fast, Tina Vachovsky, and Michael S. Bernstein.\n2016. Shirtless and dangerous: Quantifying linguis-\ntic signals of gender bias in an online ﬁction writing\ncommunity.\nJessica Ficler and Yoav Goldberg. 2017.Controlling\nlinguistic style aspects in neural language genera-\ntion. In Proceedings of the Workshop on Stylis-\ntic Variation, pages 94–104, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nAntigoni-Maria Founta, Constantinos Djouvas, De-\nspoina Chatzakou, Ilias Leontiadis, Jeremy Black-\nburn, Gianluca Stringhini, Athena Vakali, Michael\nSirivianos, and Nicolas Kourtellis. 2018. Large\nscale crowdsourcing and characterization of Twitter\nabusive behavior. InICWSM.\nBatya Friedman, Peter H Kahn, and Alan Borning.\n2008. Value sensitive design and information sys-\ntems. The handbook of information and computer\nethics, pages 69–101.\nSayan Ghosh, Mathieu Chollet, Eugene Laksana,\nLouis-Philippe Morency, and Stefan Scherer. 2017.\nAffect-LM: A neural language model for customiz-\nable affective text generation. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n634–642, Vancouver, Canada. Association for Com-\nputational Linguistics.\nAaron Gokaslan and Vanya Cohen. 2019.Openweb-\ntext corpus.\nJennifer Golbeck, Zahra Ashktorab, Rashad O. Banjo,\nAlexandra Berlinger, Siddharth Bhagwan, Cody\nBuntain, Paul Cheakalos, Alicia A. Geller, Quint\nGergory, Rajesh Kumar Gnanasekaran, Raja Ra-\njan Gunasekaran, Kelly M. Hoffman, Jenny Hot-\ntle, Vichita Jienjitlert, Shivika Khare, Ryan Lau,\nMarianna J. Martindale, Shalmali Naik, Heather L.\nNixon, Piyush Ramachandran, Kristine M. Rogers,\nLisa Rogers, Meghna Sardana Sarin, Gaurav Sha-\nhane, Jayanee Thanki, Priyanka Vengataraman, Zi-\njian Wan, and Derek Michael Wu. 2017.A large\nlabeled corpus for online harassment research. In\nProceedings of the 2017 ACM on Web Science Con-\nference, WebSci ’17, page 229–233, New York, NY ,\nUSA. Association for Computing Machinery.\nLisa Green. 2002. African American English: A Lin-\nguistic Introduction, 8.3.2002 edition edition. Cam-\nbridge University Press.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020.Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342–8360, Online. Association for Computational\nLinguistics.\nAri Holtzman, Jan Buys, Maxwell Forbes, Antoine\nBosselut, David Golub, and Yejin Choi. 2018.\nLearning to write with cooperative discriminators.\nIn Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1638–1649, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2020.The curious case of neural text degener-\nation. International Conference on Learning Repre-\nsentations.\nMatthew Honnibal and Ines Montani. 2017.spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear.\n3367\nBen Hutchinson, Vinodkumar Prabhakaran, Emily\nDenton, Kellie Webster, Yu Zhong, and Stephen De-\nnuyl. 2020.Social biases in NLP models as barriers\nfor persons with disabilities. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5491–5501, Online. As-\nsociation for Computational Linguistics.\nAbigail Z. Jacobs and Hanna M. Wallach. 2019.Mea-\nsurement and fairness.\nEun Seo Jo and Timnit Gebru. 2020.Lessons from\narchives: Strategies for collecting sociocultural data\nin machine learning. In Proceedings of the 2020\nConference on Fairness, Accountability, and Trans-\nparency, FAT* ’20, page 306–316, New York, NY ,\nUSA. Association for Computing Machinery.\nMladen Karan and JanˇSnajder. 2019.Preemptive toxic\nlanguage detection in Wikipedia comments using\nthread-level context. In Proceedings of the Third\nWorkshop on Abusive Language Online, pages 129–\n134, Florence, Italy. Association for Computational\nLinguistics.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varsh-\nney, Caiming Xiong, and Richard Socher. 2019.\nCTRL: A conditional Transformer language model\nfor controllable generation.\nAdam King. 2019.Talk to Transformer. Accessed 06-\n02-2020.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A. Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\n2017. Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the National Academy of\nSciences, 114(13):3521–3526.\nPang Wei Koh and Percy Liang. 2017.Understanding\nblack-box predictions via inﬂuence functions. In\nProceedings of the 34th International Conference\non Machine Learning - Volume 70, ICML’17, page\n1885–1894. JMLR.org.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019.Measuring bias in contex-\ntualized word representations. InProceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 166–172, Florence, Italy. Associ-\nation for Computational Linguistics.\nY . Lecun, L. Bottou, Y . Bengio, and P. Haffner. 1998.\nGradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278–2324.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach.\nXinyao Ma, Maarten Sap, Hannah Rashkin, and Yejin\nChoi. 2020. PowerTransformer: Unsupervised con-\ntrollable revision for biased language correction. In\nEMNLP.\nAdrienne Massanari. 2017. #gamergate and the fap-\npening: How Reddit’s algorithm, governance, and\nculture support toxic technocultures. New Media &\nSociety, 19(3):329–346.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019.On measur-\ning social biases in sentence encoders. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 622–628, Minneapo-\nlis, Minnesota. Association for Computational Lin-\nguistics.\nKris McGufﬁe and Alex Newhouse. 2020.The radical-\nization risks of GPT-3 and advanced neural language\nmodels.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar,\nParker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit\nGebru. 2019. Model cards for model reporting. In\nProceedings of the Conference on Fairness, Account-\nability, and Transparency, FAT* ’19, page 220–229,\nNew York, NY , USA. Association for Computing\nMachinery.\nShruthi Mohan, Apala Guha, Michael Harris, Fred\nPopowich, Ashley Schuster, and Chris Priebe. 2017.\nThe impact of toxic language on the health of Reddit\ncommunities. InCanadian Conference on AI.\nJi Ho Park and Pascale Fung. 2017.One-step and two-\nstep classiﬁcation for abusive language detection on\nTwitter. InProceedings of the Workshop on Abusive\nLanguage Online.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d´Alch´e Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019.Language\nmodels are unsupervised multitask learners.\n3368\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019.Exploring the limits\nof transfer learning with a uniﬁed text-to-text Trans-\nformer.\nAshwin Rajadesingan, Paul Resnick, and Ceren Budak.\n2020. Quick, community-speciﬁc learning: How\ndistinctive toxicity norms are maintained in political\nsubreddits. Proceedings of the International AAAI\nConference on Web and Social Media, 14(1):557–\n568.\nAnand Rajaraman and Jeffrey David Ullman. 2011.\nMining of massive datasets. Cambridge University\nPress.\nAja Romano. 2017.Reddit just banned one of its most\ntoxic forums. but it won’t touch TheDonald. Ac-\ncessed: 2020-02-23.\nBj¨orn Ross, Michael Rist, Guillermo Carbonell, Ben-\njamin Cabrera, Nils Kurowsky, and Michael Wo-\njatzki. 2017. Measuring the reliability of hate\nspeech annotations: the case of the european refugee\ncrisis. InNLP 4 CMC Workshop.\nElizabeth Sanders. 2002.From user-centered to partic-\nipatory design approaches, pages 1–7.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A. Smith. 2019. The risk of racial bias\nin hate speech detection. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1668–1678, Florence,\nItaly. Association for Computational Linguistics.\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Ju-\nrafsky, Noah A. Smith, and Yejin Choi. 2020.So-\ncial bias frames: Reasoning about social and power\nimplications of language. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5477–5490, Online. As-\nsociation for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nSerge Sharoff. 2020.Know thy corpus! robust methods\nfor digital curation of web corpora.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as\na babysitter: On biases in language generation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3407–\n3412, Hong Kong, China. Association for Computa-\ntional Linguistics.\nWessel Stoop, Florian Kunneman, Antal van den\nBosch, and Ben Miller. 2019.Detecting harassment\nin real-time as conversations develop. In Proceed-\nings of the Third Workshop on Abusive Language\nOnline, pages 19–24, Florence, Italy. Association for\nComputational Linguistics.\nAkhilesh Sudhakar, Bhargav Upadhyay, and Arjun Ma-\nheswaran. 2019. “transforming” delete, retrieve,\ngenerate approach for controlled text style transfer.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3269–\n3279, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, undeﬁne-\ndukasz Kaiser, and Illia Polosukhin. 2017.Attention\nis all you need. In Proceedings of the 31st Interna-\ntional Conference on Neural Information Processing\nSystems, NIPS’17, page 6000–6010, Red Hook, NY ,\nUSA. Curran Associates Inc.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019.Universal adversarial trig-\ngers for attacking and analyzing NLP. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153–2162, Hong\nKong, China. Association for Computational Lin-\nguistics.\nAlex Wang and Kyunghyun Cho. 2019. Bert has a\nmouth, and it must speak: Bert as a markov random\nﬁeld language model.\nZeerak Waseem. 2016.Are you a racist or am I seeing\nthings? annotator inﬂuence on hate speech detection\non Twitter. InProceedings of the First Workshop on\nNLP and Computational Social Science, pages 138–\n142, Austin, Texas. Association for Computational\nLinguistics.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2019.Neu-\nral text generation with unlikelihood training.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019.HuggingFace’s Trans-\nformers: State-of-the-art natural language process-\ning.\nBianca Zadrozny and Charles Elkan. 2002.Transform-\ning classiﬁer scores into accurate multiclass prob-\nability estimates. In Proceedings of the Eighth\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, KDD ’02, page\n694–699, New York, NY , USA. Association for\nComputing Machinery.\n3369\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d´Alch´e-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n32, pages 9054–9065. Curran Associates, Inc.\nJustine Zhang, Jonathan Chang, Cristian Danescu-\nNiculescu-Mizil, Lucas Dixon, Yiqing Hua, Dario\nTaraborelli, and Nithum Thain. 2018.Conversations\ngone awry: Detecting early signs of conversational\nfailure. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1350–1361, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cot-\nterell, Vicente Ordonez, and Kai-Wei Chang. 2019.\nGender bias in contextualized word embeddings. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 629–634,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. 2015 IEEE International Con-\nference on Computer Vision (ICCV).\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. Fine-tuning language\nmodels from human preferences.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7012889385223389
    },
    {
      "name": "Sentence",
      "score": 0.6533327698707581
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5612258315086365
    },
    {
      "name": "Offensive",
      "score": 0.5142716765403748
    },
    {
      "name": "Degeneration (medical)",
      "score": 0.4566795229911804
    },
    {
      "name": "Language model",
      "score": 0.4485081434249878
    },
    {
      "name": "Classifier (UML)",
      "score": 0.4368003308773041
    },
    {
      "name": "Machine learning",
      "score": 0.4253408908843994
    },
    {
      "name": "Software deployment",
      "score": 0.4157167971134186
    },
    {
      "name": "Natural language processing",
      "score": 0.4096227288246155
    },
    {
      "name": "Speech recognition",
      "score": 0.35287705063819885
    },
    {
      "name": "Medicine",
      "score": 0.1114630401134491
    },
    {
      "name": "Engineering",
      "score": 0.09214034676551819
    },
    {
      "name": "Pathology",
      "score": 0.09193679690361023
    },
    {
      "name": "Operations research",
      "score": 0.08272901177406311
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ]
}