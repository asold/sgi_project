{
    "title": "Evaluating Large Language Models on Controlled Generation Tasks",
    "url": "https://openalex.org/W4389523912",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2097833455",
            "name": "Jiao Sun",
            "affiliations": [
                "Southern California University for Professional Studies",
                "University of Southern California"
            ]
        },
        {
            "id": "https://openalex.org/A2181058865",
            "name": "Yufei Tian",
            "affiliations": [
                "University of California, Los Angeles"
            ]
        },
        {
            "id": "https://openalex.org/A2952165275",
            "name": "Wangchunshu Zhou",
            "affiliations": [
                "ETH Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2049574243",
            "name": "Nan Xu",
            "affiliations": [
                "University of Southern California",
                "Southern California University for Professional Studies"
            ]
        },
        {
            "id": "https://openalex.org/A2088362319",
            "name": "Qian Hu",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2127286069",
            "name": "Rahul Gupta",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2129265196",
            "name": "John Wieting",
            "affiliations": [
                "DeepMind (United Kingdom)",
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2147504447",
            "name": "Nanyun Peng",
            "affiliations": [
                "University of California, Los Angeles"
            ]
        },
        {
            "id": "https://openalex.org/A2250429641",
            "name": "Xuezhe Ma",
            "affiliations": [
                "Southern California University for Professional Studies",
                "University of Southern California"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4385571645",
        "https://openalex.org/W4385567059",
        "https://openalex.org/W4287332927",
        "https://openalex.org/W4385572749",
        "https://openalex.org/W2968297680",
        "https://openalex.org/W4229053997",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4385571674",
        "https://openalex.org/W4389523957",
        "https://openalex.org/W2963352809",
        "https://openalex.org/W4323650985",
        "https://openalex.org/W4385573550",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3171218751",
        "https://openalex.org/W3089263616",
        "https://openalex.org/W4319997768",
        "https://openalex.org/W4281635312",
        "https://openalex.org/W4385570371",
        "https://openalex.org/W2466175319",
        "https://openalex.org/W4385573518",
        "https://openalex.org/W4309799091",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4385572081",
        "https://openalex.org/W2931212643",
        "https://openalex.org/W4386290290",
        "https://openalex.org/W3206132041",
        "https://openalex.org/W4385569782",
        "https://openalex.org/W4226099034",
        "https://openalex.org/W2963126845",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W2963096510",
        "https://openalex.org/W4281690218",
        "https://openalex.org/W2970559004",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2962833140",
        "https://openalex.org/W2152052288",
        "https://openalex.org/W4226204153",
        "https://openalex.org/W2964212550",
        "https://openalex.org/W3102187933",
        "https://openalex.org/W4367369699",
        "https://openalex.org/W4221143575",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W3155393281",
        "https://openalex.org/W2963877622",
        "https://openalex.org/W3175591618",
        "https://openalex.org/W4385570989",
        "https://openalex.org/W4389524534",
        "https://openalex.org/W3034898894",
        "https://openalex.org/W3040809437",
        "https://openalex.org/W3211466672",
        "https://openalex.org/W3100880133",
        "https://openalex.org/W2951885001",
        "https://openalex.org/W4389523811",
        "https://openalex.org/W4320167623"
    ],
    "abstract": "Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Wieting, Nanyun Peng, Xuezhe Ma. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3155–3168\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEvaluating Large Language Models on Controlled Generation Tasks\nJiao Sun1∗ Yufei Tian2∗ Wangchunshu Zhou3∗ Nan Xu1∗\nQian Hu4 Rahul Gupta4 John Wieting5 Nanyun Peng2 Xuezhe Ma1\n1University of Southern California 2University of California, Los Angeles\n3 ETH Zurich 4 Amazon 5 Google DeepMind\n{jiaosun,nanx,xuezhema}@usc.edu {yufeit,violetpeng}@cs.ucla.edu\nwangchunshu.zhou@inf.ethz.ch {huqia, gupra}@amazon.com\njwieting@google.com\nAbstract\nWhile recent studies have looked into the abili-\nties of large language models in various bench-\nmark tasks, few studies have looked into the\ncontrollability of large language models on gen-\neration tasks. We present a systematic and ex-\ntensive analysis of the controllability of large\nlanguage models on ten benchmarks, includ-\ning a new simple yet challenging numerical\nplanning benchmark with different granulari-\nties. After comparing large language models\nagainst state-of-the-start finetuned smaller mod-\nels, we present a spectrum showing when large\nlanguage models fall behind, are comparable,\nor exceed the ability of smaller models. We\nconclude that large language models struggle\nat meeting fine-grained hard constraints.\n1 Introduction\nText generation models should generate texts\nthat meet controllable constraints as humans\nwish (Zhang et al., 2022). For example, one can\navoid the blandness caused by repetitive patterns by\ncontrolling the syntax of generated sentences (Iyyer\net al., 2018; Qian et al., 2019). In a customized di-\nalogue system, one should be able to control the\npersona of the utterance (Smith et al., 2020). Previ-\nous works either finetune generation models such\nas BART (Lewis et al., 2019) on specific tasks for\nbetter controllability (e.g., controlled paraphrase\ngeneration (Sun et al., 2021)) or design constrained\ndecoding strategies (e.g., look-back decoding strat-\negy by Xu et al. (2023a)) for controlled generation.\nLarge Language Models (LLMs) have re-\ncently shown great potential in various genera-\ntion tasks. For example, Jiao et al. (2023a)\nshows that ChatGPT with GPT-4 as an engine\nachieves commercial-level machine translation\nquality. Laskar et al. (2023) find that annotators\nprefer summaries generated from ChatGPT over\nstate-of-the-art summarization models. However,\n∗The first four authors contribute equally.\nTask Control Benchmark Evaluation\nnumerical \nplanning \nprefix & number of \nwords & end word NPB MSE, \nsuccess rate\nconstrained \ncontent \ngeneration  \nsentiment, topic,\nkeyword\nAmazon Review\nCommonGen\nM2D2\noff-the-shelf \nmodel, ppl\nstory \ngeneration prefix\nrationale \ngeneration correct answer\nROC\nwriting prompts\nrepetition, \ndiversity, \ncoherence\nCoS-E\nECQA\nincreased \naccuracy \nparaphrase \ngeneration semantic & syntax ParaNMT\nQQPPoS\nlexical \noverlapping, \nsyntax match \ngood poor\nFigure 1: We test large language models on five con-\ntrolled generation tasks with various control factors us-\ning automatic evaluation methods. We show a spectrum\nof abilities of large language models on such tasks and\nconclude that large language models struggle at fine-\ngrained hard constraints such as numerical planning.\nfew works investigate the controllability of large\nlanguage models. Towards this end, we aim to\nstudy and understand the controllability of large\nlanguage models to answer the question: Are large\nlanguage models better than finetuned smaller mod-\nels at controllability on generation tasks?.\nThe main contribution of this work is to conduct\na comprehensive analysis of LLM’s controllabil-\nity on five tasks and ten generation benchmarks,\nincluding controlled story generation, controlled\nfree-form generation with sentiment and topics,\ncontrolled paraphrase generation, and controlled\nrationale generation as in Figure 1. We further\ndesign a new simple yet challenging benchmark\nnamed Numerical Planning Benchmark (NPB),\nwhere the task is to satisfy numerical constraints\nfrom four granularities (word-, syllable-, sentence-\nand paragraph-level) and under different content\ncontrols (e.g., prefix and ending). For evaluation,\nwe use automatic metrics, which are imperfect yet\nconvenient and reproducible.1\n1https://github.com/sunjiao123sun/\n3155\nAfter an in-depth examination, we categorize\nLLM’s controllability on a spectrum: from lagging\nbehind and being on par with to surpassing smaller\nfinetuned models. Our findings indicate that large\nlanguage models have difficulties adhering to spe-\ncific hard constraints, such as numerical planning.\nWe first introduce the numerical planning task\nand the associated evaluation as this is a new, intu-\nitively simple, yet challenging task (§2). For the\nrest, we rank them by the task difficulty indicated\nin Figure 1 from easy to hard: constrained content\ngeneration (§3), story generation (§4), rationale\ngeneration (§5) and paraphrase generation (§6).\n2 Numerical Planning\nCan LLMs count from two to ten?\nTask Description. We introduce the Numerical\nPlanning Benchmark (NPB) as an intuitive task that\ntests the basic numerical planning ability of LLMs.\nThe high-level task descriptions can be found in Ta-\nble 1. We are inspired by real-world scenarios such\nas creative writing. For example, writers may wish\nto generate sentences or poems with a specific struc-\nture, such as a fixed number of words or syllables\nin each line, aiming to adhere to particular forms\n(e.g., sonnets, where each line contains exactly 10\nor 11 syllables (Tian and Peng, 2022)). Meanwhile,\nhumans may also want full control over the start\nand end of each line for rhetorical purposes such as\nalliteration and rhyming. Inductively, we formulate\nour numerical planning benchmark from four dif-\nferent granularities: generating a piece of text that\ncontains a predefined number of words, syllables,\nsentences, or paragraphs given a plausible pair\nof prefix (start) and suffix (ending) as constraints.\nThe prefix is given to LLMs such that they are only\nqueried to generate the continuations.\nEvaluation Metrics. We use success rate (SR)\nand mean squared error (MSE) as automatic eval-\nuation metrics. As our control is two-fold, we\nseparately calculate the success rates of 1) gener-\nating the continuation with the correct counts and\n2) generating the continuation with the proper end-\ning. We also calculate the MSE between our input\nnumbers and output numbers.\nEvaluate with LLMs. We evaluateChatGPT and\nAlpaca-7b on our NPB benchmark in zero-shot\nand few-shot settings. Each request used to query\nthe LLMs corresponds to a real case in the datasets\nllm-controlgen\nGranularity Task Illustration\nWord/Syllable\nGenerate a sentence using exactly 5\nwords/syllables.\nComplete sentence “This is a story”\nusing exactly 5 words/syllables.\nComplete sentence “This is a story”\nusing exactly 5 words/syllables,\nincluding the last word as “town”.\nSentence Generate a paragraph with 5 sentences, ...\nParagraph Generate an article with 5 paragraphs, ...\nTable 1: Task illustration for the Numerical Planning\nBenchmark. We test LLMs’ numerical planning abil-\nity under various constraints (word counting and end\nword) and granularities (word, syllable, sentence, and\nparagraph). Due to space limitations, we only show the\nfull constraints under the word granularity here.\nof Romance Books and Reddit Short Stories. 2\nFor word-level planning tasks (word and sylla-\nble count), we randomly select sentences from the\nabove datasets. Then, we select the last word in\neach sentence as the suffix. Depending on how\nmany additional words we query the LLMs to gen-\nerate, we select the first few words in each sentence\nas the prefix (if we simply ask LLMs to generate\nfreely without a prefix, the outputs lack diversity).\nOur prompt is written as Complete a sentence that\nstarts with {prefix} using exactly {N} additional\nwords (including the last word {last word}). The\nsentence must end with the word {last word}. Sen-\ntence: {prefix} , and LLMs will continue. In the\nfew-shot setting, we provide the task description\nand three examples. For each example, we also pro-\nvide explanations to help LLMs better understand\nour task. For example,\n##Prefix: This is a story about a young girl’s\n##Last word: town\n##N: 5\n##Output: This is a story about a young girl’s\nredemption in a small town.\n##Explanation: We generated “redemption in\na small town”. It contains exactly 5 words and\nends with the last word ‘town’.\nWe query the LLMs to generate outputs from\nN = 2 to N = 10 words. Each number N has\n100 evaluation samples. For paragraph-level tasks,\nthe prefix and suffix are the first and last sentences\nin the corresponding paragraphs. For all experi-\n2huggingface.co/datasets/AlekseyKorshuk/romance-\nbooks, www.kaggle.com/datasets/trevordu/reddit-short-stories\n3156\nFigure 2: Histogram visualization in the distribution (frequency, z-axis) of input numbers (x-axis) and output\nnumbers (y-axis) for word count planning. Left: querying ChatGPT to generate a continuation of a given prefix with\nN words. Right: querying ChatGPT to generate a continuation with N words of a given prefix that ends with a\ngiven word. Small red dots • mark those bars where output numbers equal input numbers. These bars represent the\nfine-grained success rates. For either case, there is a significant drop when the input number reaches six.\nModel SR -\ncount\nSR -\nlast word\nSR -\nboth\nMSE -\ncount\nGPT-2 (fine-tuned) 0.64 0.86 0.60 1.62\nAlpaca-7bzs 0.17 0.31 0.09 9.19\nAlpaca-7bICL 0.14 0.34 0.07 9.76\nVicunazs 0.08 0.09 0.03 27.68\nVicunaICL 0.13 0.30 0.04 13.43\nFalconzs 0.13 0.42 0.08 11.60\nFalcon-7bICL 0.11 0.34 0.03 13.72\nChatGPT 0.41 0.74 0.36 3.64\nChatGPTICL 0.37 0.78 0.34 4.95\nTable 2: Success rates for the word count planning\ntask. Surprisingly, few-shot in-context learning (ICL)\nunderperforms zero-shot (zs) on numerical planning.\nments, our decoding strategy is top p(p = 0.95)\nsampling with temperature T = 0.3 unless other-\nwise specified.\nResult. We report the model performance of\nLLMs and a fine-tuned GPT-2-large model on the\ntask of word count planning in Table 2. Due to\nspace limitations, we compile the results of the\nremaining tasks in Appendix A. First, it is clear\nLLMs are poor at numerical planning, although it\nis an extremely simple task for humans. Given its\nextremely poor performance, we consider Alpaca\nincapable of doing numerical planning. Secondly,\nLLMs learn to incorporate literal constraints, such\nas the last word, via few-shot in-context learning.\nInterestingly, few-shot in-context learning dete-\nriorates the performance of numerical planning.\nUpon further inspection, we find that LLMs try\nto mimic the style or features (such as length) in\nthe in-context examples and are, therefore, more\nlikely to generate outputs with the wrong word\ncounts once the input number N cannot be found\nin the examples. Our results resonate with Yin et al.\n(2023); Kung and Peng (2023); Sinha et al. (2023)\nthat LMs do not truly understand task definitions\nvia in-context learning.\nFigure 2 is a fine-grained visualization of the\ninput and output numbers distribution by zero-shot\nChatGPT. Specifically, we compare LLMs’ numeri-\ncal planning abilities with (e.g., complete sentence\nwith “redemption in a small town” using exactly\n5 words, including the last word as “happy”) and\nwithout additional suffix constraint (e.g., complete\nsentence with “redemption in a small town” us-\ning exactly 5 words ). LLMs can generate more\nfreely without suffix constraints to meet the nu-\nmerical constraint. However, ChatGPT doesn’t al-\nways translate to a higher success rate. We find out\nthat only when N is small (i.e., 2 and 3), ChatGPT\nachieves a higher success rate if explicitly told the\nlast word of the target sentence.\nFinally, we would like to point out a few be-\nhaviors. First, although the general trend is that\nLLMs’ numerical planning ability drops as N in-\ncreases, N = 3 is a clear exception (performs\nworse) among various experiments we repeated.\nSecond, by checking the failure cases, we find that\n3157\nChatGPT always generates shorter continuations\nthan required. Moreover, we see a sudden drop in\nmodel performances (from above ∼0.6 to ∼0.4)\nwhen the input number N increases from 5 to 6.\nWe encourage future research to investigate these\nbehaviors.\n3 Content-Controlled Generation\nTask Description. We consider three types of\ncontent constraints: topic, sentiment, and keyword.\nThe detailed task definitions and dataset can be\nfound in Appendix B.\nEvaluation Metrics. We use the success rate\nas the evaluation metric to measure how well\nLLMs can follow the content constraints. Specifi-\ncally, we use GPT-3.5 (Ouyang et al., 2022) based\ntopic/sentiment classifiers with in-context learn-\ning using five examples per category to evaluate\nwhether the generated texts belong to the specified\ntopic or sentiment class. We consider an LLM to\nsucceed in one example if the predicted class of the\ngenerated text is identical to the input constraint.\nFor a keyword-constrained generation, we use the\nkeyword coverage metric that measures the per-\ncentage of input keywords included in generated\ntexts.\nEvaluate with LLMs. For the content con-\nstrained generation with LLMs, we follow Zhou\net al. (2023) and use natural language instructions\nto prompt LLMs. Specifically, we use a prompt\ntemplate of “Write a sentence about {topic name}”\nfor topic-constrained generation, “Write an Ama-\nzon review with {level number} star about a ran-\ndom thing. The number of stars ranges from one to\nfive. One star is the most negative, and five stars\nare the most positive” for sentiment constraints,\nand “Write a sentence using the following key-\nwords: {keywords}” for keyword constraints.\nIn addition to zero-shot evaluation, we also eval-\nuate LLMs in the in-context learning setting by\nappending the following demonstration template:\n“Below are some examples for the task: Input: {input\n1}, Output: {output 1}; Input: {input 2}, Output:\n{output 2} ... ”. We use 5 in-context examples per\nclass following the practice in Zhou et al. (2023).\nWe compare various LLMs including ChatGPT,\nLLaMA, Alpaca, Vicuna, and Falcon in our exper-\niments. We also report the results of Diffusion-\nLM (Li et al., 2022b) based on BERT-large (Devlin\nModel Topic Sentiment Keyword\nDiffusion-LM 68.9 83.7 93.2\nGPT-2 (1.5B, fine-tuned) 63.4 76.5 88.9\nT5 (3B, fine-tuned) 67.3 83.9 94.8\nLLaMA-7Bzs 45.3 58.4 83.5\nLLaMA-7BICL 63.5 85.1 93.0\nAlpaca-7Bzs 58.9 78.4 91.2\nAlpaca-7BICL 65.2 86.9 94.8\nVicuna-7Bzs 61.0 80.5 91.6\nVicuna-7BICL 65.8 87.4 94.3\nFalcon-7Bzs 61.9 81.0 92.1\nFalcon-7BICL 66.0 87.7 94.2\nChatGPTzs 66.4 84.5 97.3\nChatGPTICL 88.4 90.3 98.1\nTable 3: Results on content-constrained text generation.\net al., 2019) and task-specific classifiers as a com-\npetitive non-LLM baseline\nResults. The results are shown in Table 3. We\nfind that Alpaca significantly outperforms LLaMA\nin the zero-shot setting. This is intuitive since\nnatural language instruction of constraints resem-\nbles instruction tuning data. However, this perfor-\nmance gap is significantly reduced when in-context\nlearning is used. We think this is because the role\nof instruction tuning is mainly to adapt an LLM\nto human-friendly prompt formats instead of in-\ncreasing the LLM’s capability. We also find that\nChatGPT achieves competitive performance with-\nout in-context learning and outperforms Diffusion-\nLM, a competitive supervised baseline, by a large\nmargin. Moreover, the performance of ChatGPT\ncan be further improved by adding in-context ex-\namples to the prompt. This suggests that LLMs’\nability to follow content constraints expressed in\nnatural language depends on three confounding fac-\ntors: instruction tuning or supervised fine-tuning,\nin-context learning, and model capacity.\n4 Story Generation\nTask Description. Given the beginning text of\na story, open-ended story generation aims to de-\ncode texts that are coherent with previous topics,\nand informative without undesired repetitions (Su\net al., 2022; Su and Xu, 2022; Xu et al., 2023b).\nDespite the impressive success on generating flu-\nent and accurate sentences for low-entropy tasks\nsuch as summarization or translation, large-scale\nlanguage models (LLMs) still suffer from serious\ndegeneration problems, such as undesired repeti-\ntions (Holtzman et al., 2020; Su et al., 2022) and\n3158\nLM Method rep-2↓ rep-3↓ rep-4↓ diversity↑ coherence↑\nROC\nHuman 1.74 0.32 0.04 0.97 0.48\nGPT-2-XL\nNucleus 1.80 0.35 0.12 0.97 0.33\nTypical 2.06 0.4 0.16 0.97 0.33\nη-sampling 0 0 0 1.00 0.34\nSimCTG 3.10 0.46 0.23 0.96 0.32\nLook-back 7.24 0.92 0.14 0.92 0.47\nLLM\nVicuna 2.36 0.45 0.15 0.97 0.60\nFalcon 2.52 1.87 1.86 0.94 0.69\nChatGPT 1.18 0.10 0.02 0.98 0.52\nWriting Promts\nHuman 15.61 3.78 1.24 0.80 0.31\nGPT-2-XL\nNucleus 5.40 2.41 1.72 0.91 0.34\nTypical 3.60 1.51 1.10 0.94 0.30\nη-sampling 6.17 2.88 2.16 0.89 0.35\nSimCTG 2.84 0.36 0.19 0.97 0.31\nLook-back 7.94 1.25 0.33 0.91 0.52\nLLM\nVicuna 8.27 2.59 1.14 0.88 0.49\nFalcon 11.20 7.79 6.94 0.76 0.53\nChatGPT 5.99 1.15 0.35 0.92 0.52\nTable 4: Performance of different decoding strategies\nand LLMs for open-ended story generation. Vicuna\nstands for Vicuna-7B, Falcon for Falcon-7B-Instruct.\nunnatural topic drifts (Li et al., 2022a), under open-\nended settings.\nDatasets. We evaluate different generation meth-\nods on two popular benchmark story datasets:\nROCStories and Writing Prompts. ROCStories\n(ROC) (Mostafazadeh et al., 2016) is a corpus\ncomprising commonsense stories written by crowd-\nsourced workers within 5 short sentences. Given\nthe first sentence as a prefix, generation methods\nare required to produce four continuing sentences.\nWriting Prompts (WP) is a challenging task for\ninspiring continuations with abstract, high-level\nstory prompts submitted by online users and con-\ntinuations by others on Reddit (Fan et al., 2018).\nFollowing prior literature (Xu et al., 2023b), we\nutilize the first 32 tokens as the prefix and ask for\ncontinuation with 256 tokens. Since we prompt\ndifferent language models or decoding algorithms\nwithout extra fine-tuning, we directly sample 1,000\ndevelopment and 1,000 testing instances from both\nROC and WP.\nBaselines. We evaluate the pre-trained LLM,\nGPT-2-XL (Radford et al., 2019), with both search\n(SimCTG (Su et al., 2022) and Look-back (Xu\net al., 2023b)) and sampling decoding methods\n(Nucleus sampling (Holtzman et al., 2020), Typical\ndecoding (Meister et al., 2022) andη-sampling (He-\nwitt et al., 2022)).\nEvaluation Metrics. Following open-ended\nstory generation literature (Su et al., 2022; Li et al.,\n2022a; Xu et al., 2023b), we adopt the following\nautomatic metrics to evaluate generation quality: 1)\nrep-nto measure sequence-level repetition accord-\ning to the portion of duplicate n-grams (Welleck\net al., 2019); 2)diversity to assess the overall model\nrepetition by considering rep-nat different n-gram\nlevels; 3) coherence measured as the cosine simi-\nlarity between prefix and continuation embeddings\nrepresented by SimCSE (Gao et al., 2021). We do\nnot report MAUVE (Pillutla et al., 2021) score due\nto the concern that MAUVE may not accurately\nreflect human preferences considering contradicted\nresults between MAUVE and human evaluations\nobserved in prior work (Su and Xu, 2022).\nEvaluate with LLMs. Chatbots that fine-tune\nLLMs on instructions are also evaluated: Vicuna-\n7B (Chiang et al., 2023), Falcon-7B-Instruct (Al-\nmazrouei et al., 2023) and ChatGPT. 3 We prepend\nthe following instruction before the story prefix\nas prompt: 1) ROC: “Please continue writing this\nstory within 4 very short sentences: <prefix>”, 2)\nWP: “Please continue writing this story within 256\nwords: <prefix>”4.\nResults. As shown in Table 4, both Vicuna-7B\nand ChatGPT are able to continue writing more\nfluent and coherent stories on both ROC and WP\ncompared with other decoding methods based on\nGPT2-XL. Falcon-7B-Instruct obtains consistently\nlower diversity than other baselines, whileChatGPT\nachieves more robust performance in terms of di-\nversity and coherence on both datasets.\n5 Rationale Generation\nTask Description. Free-form rationales are\nknown to aid model interpretability by providing\nadditional world knowledge or commonsense rea-\nsoning steps (Kim, 2015; Lipton, 2018; Alvarez-\nMelis and Jaakkola, 2018). Wei et al. (2022) show\nthat rationales can improve large language models’\nability to solve complex reasoning tasks. Extractive\nrationales in question-answering tasks are based on\nthe input passage to extract related information\nto answer the question. Conversely, free-form ra-\ntionales in the question-answering tasks are open-\n3https://chat.openai.com/\n4We adopt generation parameters for different LLMs sug-\ngested from their respective documents or APIs. We leave\nevaluation on more configurations in our repository: https:\n//github.com/sunjiao123sun/llm-controlgen.\n3159\nI→O 0.87\nI+RCoS-E →O 0.92\nI+RECQA →O 0.99\nModel Leakage Non-Leakage\nI+RAlpaca-7B →O 0.91 0.86\nI+RLLaMA-7B →O 0.87 0.79\nI+RVicuna-7B →O 0.95 0.74\nI+RFalcon-7B →O 0.83 0.65\nI+RChatGPT →O 0.98 0.93\nTable 5: Rationales generated by ChatGPT are on par\nwith best-crowdsourced rationales ECQA with FlanT5-\nXXL (Chung et al., 2022b) as the backbone model. Rul-\ning out leakage results in at least 5% accuracy drop.\nended and condition on purely the question and\noptions. (Sun et al., 2022) studies how different\nthe quality of rationales would impact rationales’\nutilities in terms of improving the model perfor-\nmance and claims that crowdsourced rationales are\nsuperior to generated rationales. Sun et al. (2022)\nfinetunes T5-base for both rationale generation and\nquestion answering. With the power of LLMs, we\nwant to revisit the problem and see whether the\nutility of generated rationales conditioned on the\nquestion and options has been improved.\nEvaluation. We follow previous works and use\nthe performance gap before and after adding ratio-\nnales in the input to measure the utility of ratio-\nnales, written as acc(I+R→O) - acc(I→O), where\nI stands for question and options as input, R stands\nfor rationales, and O stands for one of the op-\ntions as output. For the backbone model for ques-\ntion answering, we use flanT5-XXL (Chung et al.,\n2022a) instead of T5-base as it can handle longer\nsequences and is better at reasoning.\nSun et al. (2022) shows that two factors are\nmainly affecting the utility of rationales. One is\nleakage, which means that the correct answer is\nexplicitly written in the rationales, and one can\nchoose the correct answer among all the options\nby rationales without knowing the questions. The\nother is background knowledge, which is the ad-\nditional background knowledge or reasoning step\nthat can help answer the question.\nDatasets. CoS-E (Rajani et al., 2019) and\nECQA (Aggarwal et al., 2021) are the most popular\nfree-form rationale datasets through crowdsourcing.\nECQA builds on CoS-E and improves the quality of\nthe CoS-E dataset from various aspects, including\ncompleteness, comprehensiveness, coherence, etc.\nThey share the same sets of questions and options.\nBased on the findings from Sun et al. (2022), both\nCoS-E and ECQA tend to leak the correct answer\nin the rationale, while ECQA rationales contain the\nbackground necessary to answer the questions. We\nconduct our analysis on question-answer pairs from\nthe test set. Based on the evaluation acc(I+R→O) -\nacc(I→O), since we are evaluating on the same set\nof question-answer pairs, acc(I→O) is always the\nsame. Therefore, we only compare acc(I+R →O)\nwith different LLMs.\nEvaluate with LLMs. We prompt LLMs to pro-\nvide background knowledge that can help answer\nthe question and control whether to leak the cor-\nrect options in rationales. We use ChatGPT as the\nexample for illustration:\n• Leakage. We have ChatGPT take the role\nof A teacher who is trying to explain to\nstudents the rationale behind choosing the\ncorrect option for a multiple-choice question.\nThen prompt it with Question: {question}\nOptions: {concatenated options} Explain\nthe rationale behind choosing the correct option\n“{correct answer}”.\n• Non-leakage. The role of ChatGPT becomes\nA teacher who is trying to explain to students\nthe rationale behind a multiple-choice question.\nHowever, you do not want to leak the correct\nanswer directly. and prompt it with Question:\n{question} Options: {concatenated options}\nExplain the rationale behind choosing the cor-\nrect answer. Do not mention the correct answer\n“{correct answer}” explicitly.\nWe highlight the difference between the two modes\nwith underline. When prompting LLaMA and Al-\npaca, we remove the role description and only use\nthe prompts. Through analysis, we aim to answer\ntwo questions: 1) Are LLM-generated rationales on\npar with crowdsourced rationales? 2) How much\nwould leakage impact the utility of rationales?\nResult. Compared to T5, FlanT5 has better rea-\nsoning abilities (Chung et al., 2022b) and is more\ncapable of understanding instructions. Therefore,\nwe use FlanT5 instead of using T5 as the back-\nbone model for question answering, which can\ntheoretically examine the utility of rationales bet-\nter ruling out the incapability of models. Simply\ngiven the question and the option strings, Table 5\nshows that FlanT5-XXL has an accuracy of 0.87\n3160\n(while T5 in (Sun et al., 2022) scores 0.57 under\nthe same setting). We then show the performance\nwith crowdsourced rationales from both ECQA and\nCoS-E. With crowdsourced rationales from ECQA,\nthe model almost solved the task and reached a\nperformance of 0.99. With CoS-E rationales, the\naccuracy is 0.92. Our finding echoes with Sun et al.\n(2022) that ECQA rationales are better quality.\nWe then evaluate the utility of LLM-generated\nrationales under both the Leakage and Non-leakage\nscenarios. As the majority of crowdsourced ratio-\nnales contain leakage (Sun et al., 2022), we con-\nsider it fair to compare LLM-generated rationales\nunder the Leakage scenarios against crowdsourced\nrationales. We have two major findings:\n• ChatGPT generated rationales are on par with\nECQA rationales from crowdsourcing.\n• We quantify the influence of leakage in measur-\ning the utility of rationales: whether or not having\nleakage in rationales could result in an accuracy\ndifference of at least 5%.\n6 Controlled Paraphrase Generation\nTask Description. Syntactically-controlled para-\nphrase generation can benefit a wide range of\nNLP applications such as dialogue generation (Gao\net al., 2020), improving the robustness of mod-\nels (Huang and Chang, 2021) or metrics (Aggarwal\net al., 2022), and diversifying other generation tasks\nsuch as diverse question generation. Syntactically-\ncontrolled paraphrase generation is challenging be-\ncause it requires satisfying two folds of control\nsignals: semantic preservation and syntactic con-\nformation. By definition of paraphrases, the gen-\neration should have exactly the same semantics as\nthe input text. With syntax as part of the input,\ngenerated paraphrases should also conform with\nthe indicated syntax. The input syntax can come\nfrom a variety of sources.\nDatasets. We evaluate on ParaNMT-small (Chen\net al., 2019), derived from ParaNMT (Wieting and\nGimpel, 2018), and QQP-Pos (Kumar et al., 2020).\nOur train/dev/test split follows previous works (Ku-\nmar et al., 2020; Sun et al., 2021). Each instance is\na tuple of {source sentence, exemplar, ground-truth\nparaphrase}, where the exemplar shares the same\nsyntax with the ground-truth paraphrase.\nEvaluation Metrics. We use two sets of evalua-\ntion metrics to evaluate the quality of generated\nparaphrases. We use lexical-overlapping-based\nscores to evaluate the semantic preservation and\ntree-edit distances to evaluate the syntactic confor-\nmation. For lexical-overlapping-based scores, the\nhigher is better. For tree edit distance, the lower\nis better, indicating that the newly derived syntax\nmatches more closely with the expected syntax. In\nthis work, we prune the constituency parse trees at\na level of 2 and only compare the high-level syntac-\ntic structure. TED-R means the tree edit distance\nbetween the candidate-generated sentence with the\nground-truth paraphrase as the reference. TED-E\ncompares the candidate sentence against the exem-\nplar that only provides the syntax.\nEvaluate with LLMs. We provide three ways to\nprompt for the controlled paraphrase generation:\n• Direct. We prompt LLMs directly without speci-\nfying any constraints. The prompt is written as\nParaphrase {source sentence}. Please only have\nthe paraphrase in the response.\n• Control. Under this mode, we use the exemplar\nsentence for the syntactic control signal. The\nprompt is written as Paraphrase “{source sen-\ntence}” so that it uses the syntactic structure from\n“{exemplar}”; please only have the paraphrase in\nthe response.\nWe observe that under theControl mode, the gener-\nated paraphrases would sometimes take the syntac-\ntic information from the exemplars and the seman-\ntic meaning from exemplar sentences. To solve this,\nwe introduce the third mode Control with syntax\nexplanation. We first extract the constituency parse\nstructure from the exemplar sentence using Stan-\nford CoreNLP, prune the parse tree at the height\nof two (i.e., parse at H2), and then ask ChatGPT\nto generate a natural language explanation of the\npruned syntactic parse, which we refer to as syntax\nexplanation. The generated syntax explanation will\nbe part of the input.\n• Control with Syntax Explanation. The prompt is\nwritten as Paraphrase “{source sentence}\" so\nthat the sentence has a syntactic structure of\n“{pruned syntax}\". {generated explanation for\nthe syntax.} Please only have the generated para-\nphrase, not its parse, in the response.\nTable 7 shows examples of generated explana-\ntions for constituency parse trees pruned at height\n3161\nBLEU↑ METEOR↑ ROUGE-1↑ ROUGE-2↑ ROUGE-L↑ TED-R↓\n(H=2)\nTED-E↓\n(H=2)\nParaNMT\n-Small\nDirect 10.8 26.2 44.2 18.6 44.9 1.4 1.5\nCtrl 14.3 30.7 51.4 25.8 50.7 1.3 1.2\nSyntax exp. 13.6 27.3 46.4 20.2 47.0 1.4 1.4\nἼ6AESOP 22.9 32.7 54.4 29.8 56.4 0.9 0.5\nQQPPos Direct 6.7 25.2 39.8 15.6 41.5 1.8 1.8\nCtrl 10.5 25.6 43.0 19.8 45.2 1.4 1.4\nSyntax exp. 9.0 26.5 42.8 17.8 14.2 1.8 1.8\nἼ6AESOP 47.3 49.7 73.3 54.1 75.6 0.4 0.3\nTable 6: Performance comparison with ground-truth syntactic control for AESOP (Sun et al., 2021) and fine-shot\nChatGPT. With coarse syntactic control from a shallow height of pruning, AESOP, the state of the finetuned small\nmodel, outperforms five-shot ChatGPT across all semantic preservation (BLUE, ROUGE Scores, and METEOR)\nand syntactic conformation metrics (TED-R and TED-E at the height of two) by a large margin. ↑means higher is\nbetter, while ↓means lower is better. By comparing ctrl with syntax explanation, we show that ChatGPT is better at\nmimicking the syntactic structure from an exemplar than utilizing the syntactic information directly from the syntax.\nPruned Parse at H=2Explanation\n(ROOT (S (NP ) (VP )))\nThis represents a sentence structure\nwith a noun phrase and a verb phrase\nas its constituents.\n(ROOT (FRAG (SBAR )\n(. )))\nThis is a sentence with a fragment\nthat includes a subordinate clause\nfollowed by a period.\n(ROOT (SBARQ\n(WHADVP ) (SQ ) (. )))\nThis sentence structure represents an\ninterrogative sentence with a subord\n-inate clause before the main clause.\n(ROOT (SQ (VBP )\n(RB ) (NP ) (VP ) (. )))\nThis is a parse tree for a sentence\ncontaining a main verb and its subject,\nwith a possible adverb and complement\nstructure.\nTable 7: Examples of generated explanations for pruned\nconstituency parse trees by ChatGPT.\ntwo by ChatGPT. We prompt ChatGPT from zero\nshots to five shots for our experiments, find that\nChatGPT’s performance peaks with five shots as\nexpected, and compare the performance of five-\nshot ChatGPT with AESOP (Sun et al., 2021). The\nbackbone of AESOP is the BART-base model, a\n140m-parameter model finetuned with specialized\ninput and output format tailored for the controlled\nparaphrase generation task. To the best of our\nknowledge, AESOP remains the state-of-the-art\nparaphrase generation model on both ParaNMT-\nsmall and QQPPos datasets.\nResult. Table 6 shows the performance compar-\nison between five-shot ChatGPT and AESOP. We\nshow that AESOP surpasses ChatGPT across all\nevaluation metrics for both semantic preservation\nmetrics (lexical-overlapping based metrics includ-\ning BLEU, ROUGE scores, and METEOR) and\nsyntactic conformation metrics (TED-R and TED-\nE at the height of two). In addition, we find that\nChatGPT’s performance is the best under the set-\nting of Control, where we use exemplar sentences\nfor control signals. Compared with the setting Con-\ntrol with syntax explanation , Table 6 shows that\nChatGPT is good at mimicking syntactic structures\nfrom sentences instead of directly incorporating the\nsyntactic parses. Besides ChatGPT, we also tried\nAlpaca (Taori et al., 2023) and LLaMA (Touvron\net al., 2023) on the controlled paraphrase genera-\ntion task. However, they repeat input sentences and\nstruggle to generate meaningful content. Therefore,\nwe do not include them here for comparison.\n7 Related Works\nLLM Evaluation. While the advancement of\nmore potent large language models drives our work,\nour focus aligns more with recent studies evaluat-\ning LLMs’ performance on academic NLP bench-\nmarks. We roughly categorize these studies as ei-\nther general or specific NLP tasks. For general\nNLP tasks, Qin et al. (2023) shows that ChatGPT\nperforms well on many tasks involving reasoning\ncapabilities but not on sequence tagging. Ahuja\net al. (2023) evaluate LLMs on various multilingual\nNLP tasks. For specific tasks, Jiao et al. (2023b)\nshows that ChatGPT has achieved competitive per-\nformance on machine translation. Gao et al. (2023)\nuses ChatGPT for event extraction and shows that\nit only matches with around a half percent of spe-\ncialized event extraction models. To the best of the\nauthors’ knowledge, we are the first to study the\ncontrollability of LLMs and the tasks in our work\n3162\nhave not been previously studied. Instead of having\na single conclusion on if LLMs perform well at cer-\ntain task, we provide a spectrum showcasing how\nLLMs’ abilities vary according to different control\ngranularities.\n8 Discussion: Why and How\nWe believe that our work makes a substantial con-\ntribution to the field of benchmarking LLMs’ con-\ntrollabiltiy, especially considering the prevalence\nof LLMs these days. That being said, we do have\na few hypotheses to investigate why LLMs fail at\nnumerical planning and how we could potentially\nincrease their controllability.\nTokenization. On one hand, tokenization indeed\nmakes the task of numerical planning more chal-\nlenging than without, by separating the generative\nprocess (i.e., subword-level generation) and the nu-\nmerical planning process (i.e., counting complete\nwords). However, we posit that tokenizers not nec-\nessarily impact the ability of word planning, as it is\na standard practice that a subword starting with a\nspecial token will indicate the start of a new word\n(e.g., “ ˙G” in BPE tokenizer,5 which has been used\nby many LLMs such as GPT and RoBERTa). Nor\nare we aware of evidence that the subwords of a\ntokenizer roughly correspond to units of syllables.\nFor example, Tian et al. (2023) shows that smaller\nmodels such as GPT-2-large fine-tuned on syllable-\nrelated data can achieve a success rate of close to\n90% on the same syllable-planning task. On the\nother hand, the best performance of ChatGPT is\n37%.\nDecoding Methods. The reported results are\nbased on sampling with a temperature of 0.3. More-\nover, we have experiments showing that our con-\nclusion is robust to the change of decoding mech-\nanisms, where we try other decoding methods be-\nyond sampling with T = 0.3.\nSpecifically, we tried 1) greedy decoding, 2)\nbeam search with beam size 8, and 3) sampling\nwith temperature T = {0.3,0.7,1.0}. For the prior\ntwo, most of the generated outputs are highly simi-\nlar, plain, and lack diversity. As for sampling with\nT = {0.3,0.7,1.0}, the success rate decreases as\nT increases. We think T = 0.3 is a reasonable bal-\nance between diversity and quality. We believe that\nour results convey meaningful signals since each\n5https://huggingface.co/learn/nlp-course/\nchapter6/5?fw=pt#byte-pair-encoding-tokenization\nnumber N has been averaged over 100 different\nevaluation samples to reduce noise. However, none\nof these experiments show that LLMs can do better\nthan fine-tuned GPT-2.\nIn-Context Learning. We try to give more\ndemonstration of NPB in our prompts and we sur-\nprisingly found that this does not help once the\ninput number N cannot be found in the examples.\nOur results resonate with Yin et al. (2023); Kung\nand Peng (2023) that LLMs do not truly understand\ntask definitions via in-context learning.\nHow to Improve. We encourage future work\nto explore from two different directions: 1)\nchain/tree/graph-of-thought reasoning, and 2)\nbridging LLMs with non-autoregressive generation\nabilities (e.g., NADO (Meng et al., 2022)). For the\nfirst one, one can try both simple chain/tree/graph-\nof-thought prompting or even pretrained LLMs\nwith chain-of-thought/scratchpad pairs, as it shows\npromises for mathematical reasoning (Zhou et al.,\n2022). However, this will not fundamentally solve\nthe planning issue. It is straightforward that auto-\nregressively generating the next tokens will lead\nto the problem of models not “looking back” and\ntherefore not adhering to the fine-grained control\nsignals. Therefore, we encourage researchers to\nalso investigate multi-step planning and iterative\nrevisions with LLMs, or, more fundamentally, chal-\nlenge the autoregressive architecture of LLMs.\n9 Conclusion\nWe test the controllability of large language mod-\nels on five tasks and ten benchmarks, including a\nnumerical planning benchmark that is easy for hu-\nmans while challenging for LLMs. From there, we\ndraw a spectrum by comparing the performance\nbetween LLMs and smaller specialized models.\nLLMs are able to generate human-level rationales\nand conform with coarse control signals, such as\nsentiment, topic and keyword incorporation. How-\never, they struggle at fine-grained hard constraints,\nsuch as numerical planning and paraphrase gener-\nations. We hope that our work can inspire down-\nstream applications on when to adopt LLMs. For\nexample, we find that LLMs are good at generating\nrationales, and these automatic rationales could be\nused to further boost LLMs’ performance through\nchain-of-thought reasoning.\n3163\nAcknowledgement\nThe authors thank anonymous reviewers for their\nconstructive feedback and suggestions that helped\nimprove the draft, especially reviewer rXWW. Jiao\nand Yufei are supported by Amazon fellowships.\nLimitations\nThis work is subject to couple of limitations. First,\nall of our experiments involved heavy prompt en-\ngineering effort. Although we have attempted to\nchoose the best performing prompts, there might\nbe room for better prompts which could influence\nthe reported evaluation metrics. Second, automatic\nevaluations are imperfect. Last, we have not pro-\nposed solutions after identifying tasks where LLMs\nstruggle. We leave this for future work.\nReferences\nArshiya Aggarwal, Jiao Sun, and Nanyun Peng.\n2022. Towards robust NLG bias evaluation with\nsyntactically-diverse prompts. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2022, pages 6022–6032, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nShourya Aggarwal, Divyanshu Mandowara, Vishwa-\njeet Agrawal, Dinesh Khandelwal, Parag Singla, and\nDinesh Garg. 2021. Explanations for Common-\nsenseQA: New Dataset and Models. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3050–3065, Online.\nAssociation for Computational Linguistics.\nKabir Ahuja, Harshita Diddee, Rishav Hada, Milli-\ncent Ochieng, Krithika Ramesh, Prachi Jain, Ak-\nshay Nambi, Tanuja Ganu, Sameer Segal, Maxamed\nAxmed, Kalika Bali, and Sunayana Sitaram. 2023.\nMega: Multilingual evaluation of generative ai.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Goffinet, Daniel Hes-\nlow, Julien Launay, Quentin Malartic, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo.\n2023. Falcon-40B: an open large language model\nwith state-of-the-art performance.\nDavid Alvarez-Melis and T. Jaakkola. 2018. Towards\nrobust interpretability with self-explaining neural net-\nworks. In NeurIPS.\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2017. Guided open vocabulary im-\nage captioning with constrained beam search. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages 936–\n945, Copenhagen, Denmark. Association for Compu-\ntational Linguistics.\nMingda Chen, Qingming Tang, Sam Wiseman, and\nKevin Gimpel. 2019. A multi-task approach for dis-\nentangling syntax and semantics in sentence represen-\ntations. pages 2453–2464, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022a. Scaling instruction-finetuned language mod-\nels.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022b. Scaling instruction-finetuned language mod-\nels. arXiv preprint arXiv:2210.11416.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nJun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu.\n2023. Exploring the feasibility of chatgpt for event\nextraction.\nSilin Gao, Yichi Zhang, Zhijian Ou, and Zhou Yu. 2020.\nParaphrase augmented task-oriented dialog genera-\ntion. ArXiv, abs/2004.07462.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\n3164\nJohn Hewitt, Christopher Manning, and Percy Liang.\n2022. Truncation sampling as language model\ndesmoothing. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022 , pages 3414–\n3427, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration.\nKuan-Hao Huang and Kai-Wei Chang. 2021. Generat-\ning syntactically controlled paraphrases without us-\ning annotated parallel pairs. ArXiv, abs/2101.10579.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\nZettlemoyer. 2018. Adversarial example generation\nwith syntactically controlled paraphrase networks. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1875–1885, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing\nWang, and Zhaopeng Tu. 2023a. Is chatgpt a good\ntranslator? yes with gpt-4 as the engine.\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing\nWang, and Zhaopeng Tu. 2023b. Is chatgpt a good\ntranslator? yes with gpt-4 as the engine.\nPhillip Keung, Yichao Lu, György Szarvas, and Noah A.\nSmith. 2020. The multilingual Amazon reviews cor-\npus. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 4563–4568, Online. Association for\nComputational Linguistics.\nBeen Kim. 2015. Interactive and interpretable machine\nlearning models for human machine collaboration .\nPh.D. thesis, Massachusetts Institute of Technology.\nA. Kumar, Kabir Ahuja, Raghuram Vadapalli, and\nP. Talukdar. 2020. Syntax-guided controlled genera-\ntion of paraphrases. Transactions of the Association\nfor Computational Linguistics, 8:330–345.\nPo-Nien Kung and Nanyun Peng. 2023. Do models\nreally learn to follow instructions? an empirical study\nof instruction tuning. ACL 2023.\nMd Tahmid Rahman Laskar, M Saiful Bari, Mizanur\nRahman, Md Amran Hossen Bhuiyan, Shafiq R. Joty,\nand J. Huang. 2023. A systematic study and compre-\nhensive evaluation of chatgpt on benchmark datasets.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdel rahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2019. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and compre-\nhension. In Annual Meeting of the Association for\nComputational Linguistics.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy\nLiang, Jason Eisner, Tatsunori Hashimoto, Luke\nZettlemoyer, and Mike Lewis. 2022a. Contrastive de-\ncoding: Open-ended text generation as optimization.\narXiv preprint arXiv:2210.15097.\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy\nLiang, and Tatsunori Hashimoto. 2022b. Diffusion-\nLM improves controllable text generation. In Ad-\nvances in Neural Information Processing Systems.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823–1840,\nOnline. Association for Computational Linguistics.\nZachary C Lipton. 2018. The mythos of model inter-\npretability: In machine learning, the concept of in-\nterpretability is both important and slippery. Queue,\n16(3):31–57.\nXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras,\nChandra Bhagavatula, and Yejin Choi. 2021. Neuro-\nLogic decoding: (un)supervised neural text genera-\ntion with predicate logic constraints. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4288–4299,\nOnline. Association for Computational Linguistics.\nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan\nCotterell. 2022. Typical decoding for natural lan-\nguage generation. arXiv preprint arXiv:2202.00666.\nTao Meng, Sidi Lu, Nanyun Peng, and Kai-Wei Chang.\n2022. Controllable text generation with neurally-\ndecomposed oracle. In Advances in Neural Informa-\ntion Processing Systems, volume 35, pages 28125–\n28139. Curran Associates, Inc.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A corpus\nand cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\n3165\nHarchaoui. 2021. Mauve: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. Advances in Neural Information Process-\ning Systems, 34:4816–4828.\nMatt Post and David Vilar. 2018. Fast lexically con-\nstrained decoding with dynamic beam allocation for\nneural machine translation. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 1314–1324, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nLihua Qian, Lin Qiu, Weinan Zhang, Xin Jiang, and\nYong Yu. 2019. Exploring diverse expressions\nfor paraphrase generation. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3173–3182, Hong Kong,\nChina. Association for Computational Linguistics.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver?\nLianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin\nChoi. 2022. COLD decoding: Energy-based con-\nstrained text generation with langevin dynamics. In\nAdvances in Neural Information Processing Systems.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain your-\nself! leveraging language models for commonsense\nreasoning. In Proceedings of the 2019 Conference\nof the Association for Computational Linguistics\n(ACL2019).\nMachel Reid, Victor Zhong, Suchin Gururangan, and\nLuke Zettlemoyer. 2022. M2D2: A massively multi-\ndomain language modeling dataset. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 964–975, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nKoustuv Sinha, Jon Gauthier, Aaron Mueller, Kan-\nishka Misra, Keren Fuentes, Roger Levy, and Adina\nWilliams. 2023. Language model acceptability judge-\nments are not always robust to context. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 6043–6063, Toronto, Canada. Association for\nComputational Linguistics.\nEric Michael Smith, Diana Gonzalez-Rico, Emily Di-\nnan, and Y-Lan Boureau. 2020. Controlling style in\ngenerated dialogue.\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-\npeng Kong, and Nigel Collier. 2022. A contrastive\nframework for neural text generation. arXiv preprint\narXiv:2202.06417.\nYixuan Su and Jialu Xu. 2022. An empirical study\non contrastive search and contrastive decoding\nfor open-ended text generation. arXiv preprint\narXiv:2211.10797.\nJiao Sun, Xuezhe Ma, and Nanyun Peng. 2021. AESOP:\nParaphrase generation with adaptive syntactic control.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5176–5189, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJiao Sun, Swabha Swayamdipta, Jonathan May, and\nXuezhe Ma. 2022. Investigating the benefits of free-\nform rationales. In Findings of the Association for\nComputational Linguistics: EMNLP 2022 , pages\n5867–5882, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nYufei Tian, Anjali Narayan-Chen, Shereen Oraby,\nAlessandra Cervone, Gunnar Sigurdsson, Chenyang\nTao, Wenbo Zhao, Tagyoung Chung, Jing Huang, and\nNanyun Peng. 2023. Unsupervised melody-to-lyrics\ngeneration. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 9235–9254, Toronto,\nCanada. Association for Computational Linguistics.\nYufei Tian and Nanyun Peng. 2022. Zero-shot sonnet\ngeneration with discourse-level planning and aesthet-\nics features. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 3587–3597, Seattle, United States.\nAssociation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2019. Neu-\nral text generation with unlikelihood training. arXiv\npreprint arXiv:1908.04319.\n3166\nJohn Wieting and Kevin Gimpel. 2018. ParaNMT-50M:\nPushing the limits of paraphrastic sentence embed-\ndings with millions of machine translations. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 451–462, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nNan Xu, Chunting Zhou, Asli Celikyilmaz, and Xuezhe\nMa. 2023a. Look-back decoding for open-ended text\ngeneration.\nNan Xu, Chunting Zhou, Asli Celikyilmaz, and Xuezhe\nMa. 2023b. Look-back decoding for open-ended text\ngeneration. arXiv preprint arXiv:2305.13477.\nFan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caim-\ning Xiong, and Chien-Sheng Jason Wu. 2023. Did\nyou read the instructions? rethinking the effective-\nness of task definitions in instruction learning. ACL\n2023.\nHanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou,\nand Dawei Song. 2022. A survey of controllable\ntext generation using transformer-based pre-trained\nlanguage models. ArXiv, abs/2201.05337.\nHattie Zhou, Azade Nova, Hugo Larochelle, Aaron\nCourville, Behnam Neyshabur, and Hanie Sedghi.\n2022. Teaching algorithmic reasoning via in-context\nlearning.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Ethan\nWilcox, Ryan Cotterell, and Mrinmaya Sachan. 2023.\nControlled text generation with natural language in-\nstructions.\nCansen Ça ˘glayan and Murat Karakaya. 2021. Topic-\ncontrolled text generation. In 2021 6th International\nConference on Computer Science and Engineering\n(UBMK), pages 533–536.\nModel SR -\ncount\nSR -\nsuffix\nSR -\nboth\nMSE -\ncount\nsyllable planning\nChatGPT 0.37 0.75 0.32 4.83\nChatGPT ICL 0.30 0.84 0.28 6.10\nAlpaca-7b 0.15 0.33 0.07 9.44\nAlpaca-7b ICL 0.12 0.36 0.05 10.61\nsentence planning\nChatGPT 0.38 0.625 0.29 1.69\nChatGPT ICL 0.36 0.66 0.27 2.05\nAlpaca-7b 0.19 0.19 0.07 6.56\nAlpaca-7b ICL 0.17 0.26 0.10 8.04\nparagraph planning\nChatGPT 0.69 0.17 0. 3.24\nChatGPT ICL 0.57 0.17 0.34 4.43\nAlpaca-7b Failed\nAlpaca-7b ICL Failed\nTable 8: Success rates for the syllable, sentence, and\nparagraph count planning tasks. LLMs are best at sen-\ntence count planning and worst at syllable count plan-\nning.\nA SPB additional results\nWe report the additional results of ChatGPT and\nAlpaca on the SPB benchmark in Table 8. Recall\nthat the suffix for the paragraph planning task is\nthe last sentence. In practice, LLMs are unable\nto follow instructions and copy the requirement as\nprompted. Hence, when we compute the success\nrate for this last task, we check the token overlap\nbetween the generated sentence and our require-\nment, and if more than 2/3 of the tokens overlap,\nwe will consider it as a success.\nTaking all four tasks in the SPB benchmark into\naccount, we find out that Alpaca-7b have very little\nnumerical planning ability. ChatGPT on the hother\nhand is best at sentence count planning, and worst\nat syllable count planning.\nB Additional Information of Content\nControlled Generation\nControlled content generation refers to the task\nof controlling the content of generated texts. We\nconsider three types of content constraints:\n• Topic constraint. It requires the model to gen-\nerate texts about certain topics. Traditional\nmethods for topic constrained generation ei-\nther append a special token for different top-\nics (Ça˘glayan and Karakaya, 2021) or use trained\ntopic classifiers (Qin et al., 2022) to guide the\ngeneration process.\n• Sentiment constraint. Similar to topic constraint,\nthis task requires the model to generate texts of\n3167\ncertain sentiments. The aforementioned methods\nfor topic constrained generation also apply to\nsentiment constrained generation.\n• Keyword constraint. Keyword constrained, or\nlexical constrained text generation requires the\nmodel to generate texts that contain certain key-\nwords or tokens. Traditional methods for key-\nword constrained text generation generally en-\nforce lexical constraints on the outputs by mod-\nifying the search space according to the con-\nstraints (Anderson et al., 2017; Post and Vilar,\n2018; Lu et al., 2021).\nDatasets. For topic constraints, we use a sub-\nset of the topics from the first hierarchy in the\nM2D2 dataset (Reid et al., 2022) which contains\ndomains such as health, history, society, technol-\nogy, arts, science, etc. The total number of topics\nis 10 in our experiments. We use the Amazon Re-\nview dataset (Keung et al., 2020) for sentiment\nconstrained text generation. The sentiment is mea-\nsure by 1 to 5 stars. For lexical constrained text\ngeneration, we use the CommonGEN dataset (Lin\net al., 2020) which requires the model to generate\na sentence using three to five keywords.\n3168"
}