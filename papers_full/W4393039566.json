{
  "title": "Post-Pretraining Large Language Model Enabled Reverse Design of MOFs for Hydrogen Storage",
  "url": "https://openalex.org/W4393039566",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2310428114",
      "name": "Zhimeng Liu",
      "affiliations": [
        "University of Science and Technology Beijing"
      ]
    },
    {
      "id": "https://openalex.org/A2502247974",
      "name": "Yuqiao Su",
      "affiliations": [
        "University of Science and Technology Beijing"
      ]
    },
    {
      "id": "https://openalex.org/A2114121497",
      "name": "Yujie Guo",
      "affiliations": [
        "University of Science and Technology Beijing"
      ]
    },
    {
      "id": "https://openalex.org/A2098892888",
      "name": "Jing Lin",
      "affiliations": [
        "University of Science and Technology Beijing"
      ]
    },
    {
      "id": "https://openalex.org/A2138708377",
      "name": "Shulin Wang",
      "affiliations": [
        "University of Science and Technology Beijing"
      ]
    },
    {
      "id": "https://openalex.org/A2580455358",
      "name": "Zeyang Song",
      "affiliations": [
        "University of Science and Technology Beijing"
      ]
    },
    {
      "id": "https://openalex.org/A2576229959",
      "name": "Zuoshuai Xi",
      "affiliations": [
        "University of Science and Technology Beijing"
      ]
    },
    {
      "id": "https://openalex.org/A2146637466",
      "name": "Hongyi Gao",
      "affiliations": [
        "University of Science and Technology Beijing"
      ]
    },
    {
      "id": "https://openalex.org/A2008156544",
      "name": "Lei Shi",
      "affiliations": [
        "University of Science and Technology Beijing"
      ]
    },
    {
      "id": "https://openalex.org/A2116246449",
      "name": "Ge Wang",
      "affiliations": [
        "University of Science and Technology Beijing"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4393039566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4387406013",
    "https://openalex.org/W1677747913",
    "https://openalex.org/W4387451333",
    "https://openalex.org/W2983028326",
    "https://openalex.org/W2884430236",
    "https://openalex.org/W4385571525",
    "https://openalex.org/W2786672974",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4367725842",
    "https://openalex.org/W2008151531",
    "https://openalex.org/W2973153050",
    "https://openalex.org/W4221019606",
    "https://openalex.org/W4238646451",
    "https://openalex.org/W3102096994",
    "https://openalex.org/W4242469704",
    "https://openalex.org/W3201890128",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W3182414949",
    "https://openalex.org/W3170611326",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3104073206",
    "https://openalex.org/W4387694367"
  ],
  "abstract": "Abstract. Large language models (LLMs) have achieved remarkable performance in general domains, they still face significant challenges when applied to specialized problems in fields like materials science. In this study, we enhance the performance of LLMs in the specific field of metal-organic frameworks (MOFs) for hydrogen storage by employing a post-pretraining approach to customize the LLM with domain-specific learning. By incorporating a comprehensive dataset comprising more than 2,000 MOF structures, over 7,000 related scientific papers, and a corpus exceeding 210 million tokens of specialized materials and chemical knowledge, we developed a domain-specific LLM for MOFs, referred to as MOFs-LLM. Through supervised fine-tuning, we unlocked the potential of MOFs-LLM in various tasks, including performance prediction, inverse design, mechanistic studies and application prospect analysis, with a specific focus on hydrogen storage material design challenges. In the practical application of reverse design, we utilize MOFs-LLM to mutate numerous ligands and select suitable building blocks, resulting in a structural space encompassing more than 100,000 MOFs. A MOF structure with highly promising hydrogen storage performance was ultimately successfully identified. This work effectively demonstrates the successful application of LLMs in a specific material science domain and provides a methodological pathway that can serve as a valuable reference for future research",
  "full_text": "Post-Pretraining Large Language Model Enable d Reverse \nDesign of MOFs for Hydrogen Storage \nZhimeng Liu a, 1, Yuqiao Su a, 1, Yujie Guo a, Jing Lin a, Shulin Wang a, Zuoshuai Xi a, \nZeyang Song a, Hongyi Gao a*, Lei Shi b* Ge Wang a* \na Beijing Advanced Innovation Center for Materials Genome Engineering, Beijing Key \nLaboratory of Function Materials for Molecule & Structure Construction, School of \nMaterials Science and Engineering, University of Science and Technology Beijing, \nBeijing 100083, PR China \nb University of Science and Technology Beijing, Beijing, PR China \n*Corresponding author. E -mail address:  hygao@ustb.edu.cn, leishi@buaa.edu.cn, \ngewang@ustb.edu.cn \n \nAbstract. Large language models (LLMs) have achieved remarkable  performance \nin general domains, they still face significant challenges when applied to \nspecialized problems in fields like materials science. In this study, we enhance the \nperformance of LLMs in the specific field of  metal-organic frameworks (MOFs)  \nfor hydrogen storage by employing a post -pretraining approach to customize the \nLLM with domain -specific learning. By incorporating a comprehensive dataset \ncomprising more than  2,000 MOF structures, over 7,000 related scientific papers, \nand a corpus exceeding 210 million tokens of specialized materials and chemical \nknowledge, we develop ed a domain -specific LLM for MOFs , referred to as  MOFs-\nLLM. Through supervised fine -tuning, we unlock ed the potential of MOFs -LLM \nin various tasks , including  performance prediction, inverse design, mechanistic \nstudies and application prospect analysis, with a specific focus on  hydrogen storage \nmaterial design challenges. In the practical application of reverse design, we uti lize \nMOFs-LLM to mutate numerous ligands and select suitable building blocks, \nresulting in a structural space encompassing more than 100,000 MOFs. A MOF \nstructure with highly promising hydrogen storage performance was ultimately \nsuccessfully identif ied. This work effectively demonstrates the successful \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\napplication of LLMs in a specific material science domain and provides a \nmethodological pathway that can serve as a valuable reference for future research.  \n \nKeywords:  Large language model , Metal-organic frameworks , Hydrogen storage , \nReverse design, Post-Pretrain ing. \n \n1 Introduction  \nMetal-organic frameworks (MOFs)  [1-3] are a recent class of hybrid porous \nmaterials composed of inorganic metal clusters or ions coordinated with organic \nlinkers, which have gained recognition as one of the ten chemical innovations with \ntransformative potential across various industries. The exceptional characteristics \nof MOFs, including their high surface areas, permanent porosity, and versatile \nfunctionalities, make them promising  materials for addressing the demanding \nrequirements of hydrogen storage, a critical component of future energy systems \nand clean energy technologies. The investigation into MOFs for hydrogen storage \nbegan with the synthesis of MOF -5 by Yaghi in 2004, whic h paved the way for \nexploring the potential of MOFs for hydrogen storage. Significant strides have been \nmade in the design of high -performance MOFs for hydrogen storage, resulting in \nnumerous achievements  [4]. The diverse composition and vast number of MOF s \nchallenge  traditional trial -and-error experimentation in identifying promising \nMOFs that can surpass existing performance limits.  \nConsiderable efforts have been devoted to overcoming this challenge by \nadopting advanced computational methods, high-throughput screening techniques  \n[5], and machine learning algorithms  [6-7]. These approaches enable the exploration \nof a vast chemical space of MOFs, accelerating the discovery and design of MOFs \nwith enhanced hydrogen storage properties. However, these methods primarily rely \non screening optimal materials or mining structure -property relationships from \nextensive data. Due to the lack of in -depth understanding, they may have \nlimitations in perceiving or inferring certain non -quantifiable scientific pri nciples \nor limitations.  \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nEmerging large language models (LLMs) like ChatGPT  [8], LaMDA  [9], and \nERNIE [10] have presented a new opportunity to address the abovementioned \nchallenges . Their ability to comprehend complex information, integrate domain \nknowledge, and learn from human feedback holds promise for advancing the \nunderstanding and design of MOFs with enhanced hydrogen storage properties. For \ninstance, Yaghi and colleagues  [11] integrated GPT -4 into the iterative process of \nreticular chemistry experiments, establishing an interactive workflow between \nGPT-4 and human researchers. Smit and others [12] have fine -tuned language \ninteraction for classification, regression, and molecular generation formula tasks in \nchemistry and materials science, surpassing the limitation of small data volume and \nachieving prediction accuracy comparable to dedicated machine  learning methods \nor even better. Although LLMs have made significant progress i n some applications, \ntheir knowledge and capabilities are primarily acquired during pre -training, and \ngeneral -purpose models still lack the specialized expertise required for MOFs as \nhydrogen storage materials.  \nHerein, this study aims to develop a dedicated LLM tailored for the research of \nMOFs materials, explicitly  focusing  on reverse structure design for high -\nperformance hydrogen storage materials. In particular, we have created the initial \ncorpus that comprehensively describes the structure of MOFs materials using \nnatural language, consisting of approximately 2,000 structu ral descriptions and \nincorporates insights from over 7,000 research papers on MOFs. We conducted \npost-pretraining  [13] on the ERNIE  [14] model and acquired over 210 million MOFs \nmaterial and chemical knowledge corpus, which were utilized to develop a \nspecialized LLM for MOFs (MOFs -LLM). Subsequently, guided by MOFs -LLM, \nwe mutated a large number of MOFs ligands and carefully selected appropriate \nbuilding blocks, thu s constructing a chemical space of MOFs tailored for high -\nperformance hydrogen storage. Finally, employing Monte Carlo calculations along \nwith encoding and decoding strategies, we successfully reverse -engineered high -\nperformance hydrogen storage MOFs that surpass the materials found in the CoRE-\nMOFs (Computation-Ready Experimental MOFs) [15] database. This study establishes \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\na flexible framework for the reverse design of MOFs for hydrogen storage, \nsignificantly expediting the process of MOF design and highlighting the untapped \npotential of LLMs in the field of materials science.  \n \nFigure 1. Flow chart of MOFs material reverse design driven by MOF s-LLM \n \n2 Methods  \n2.1 Background knowledge of ERNIE Speed and Post-pretrain \nERNIE Speed [16-17] is a knowledge-enhanced language model developed by Baidu, \nwhich combines deep learning with knowledge graph technology to provide deep \nsemantic analysis and precise text expression. Regarding architectural design, ERNIE \nSpeed adopts an efficient transformer architecture and incorporates a knowledge \nenhancement mechanism. It effectively manages long -range dependencies in text \nthrough self -attention mechanisms, enabling precise understanding of complex \nsentence structures and generating sentences that are highly coordinated with the given \ncontext.  \nDuring the training process, ERNIE Speed utilises unsupervised learning techniques \non a vast corpus of textual data for pre -training. By profoundly exploring language \npatterns, structures, and representations, the model can capture deep semantic \ninformation from text, enhancing its text understanding and generation capabilities . \nThis pre-training process enables ERNIE Speed to excel in various natural language \nprocessing tasks, including text classification and question answering.  Based on the \npre-trained model, the post-pretrain model can be further pre -trained by Soft Prompt \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nFine-Tuning (SFT) to fine -tune for different downstream tasks, thus improving the \nmodel's performance on a specific task. Furthermore, ERNIE Speed exhibits strong \ntransfer learning capabilities. It can effectively apply knowledge learned from one task \nto other tasks, demonstrating excellent generalization performance. This ability enables \nERNIE Speed to adapt to different domains and scenarios, providing flexible and \nefficient solutions for various practical applications. \nPost-pretrain ing [18-19], an advanced transfer learning method, offers a solution \nfor LLMs to delve into the field of materials. It involves further training a pre -\ntrained model on a large amount of unsupervised domain data to better grasp the \ncomplex knowledge and requirements of specific tasks in that field, enhancing the \nmodel's professionalism and practicality.  Through leveraging  the robust logical \nreasoning and cross -domain synthesis capabilities inherent in LLMs, targeted post -\npretraining can be utilized  to integrate the intricate knowledge within the MOFs \ndomain, potentially heralding a pattern  shift in the design of MOF materials  [20-21]. \n \n2.2 MOFs Material structure -hydrogen Storage properties Corpus  \nThrough manual statistical methods, we accurately annotated the metal \ncoordination number, ligand count surrounding the metal, and the cluster count of \nmetals surrounding the ligands in 2000 CIF files from the CoRE-MOFs dataset. Using \na directed acyclic graph script, we segmented the ligands and recorded their Simplified \nmolecular input line entry system (SMILES) representation, molecular weight, \nHydrophobic parameter calculation reference value (XLogP3), hydrogen bond donor \ncount, hydrogen bond acceptor count, rotatable bond count, and molecular structural \ndescription from PubChem  (https://pubchem.ncbi.nlm.nih.gov/). This ap proach helps \nbetter understand  the cha racteristics of the ligands. We utilized Zeo++ software to \ncompute essential indicators for understanding MOFs structure, pore characteristics, \nand performance, such as unit cell volume, single crystal density, gravimetric accessible \nsurface area, accessible volume fraction, probe occupiable volume fraction, global \ncavity diameter , pore limiting diameter, and largest cavity diameter. Using the \npymatgen[22] script, we compiled statistics on the types of metals and their coordination, \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nbond lengths, molecular formulas, crystal systems, and space groups in MOFs. We \nobtained the MOFs' topology using their MOFID [23]. By cross-referencing MOFs with \nCambridge Crystallographic Data Centre (CCD C, https://www.ccdc.cam.ac.uk [24]) \nidentifiers, we found corresponding synthesis literature s to extract framework \nintroductions. It is important to note that our data collection is an ongoing effort, \ncontinually expanding to support future research and analysis in the field. \n \n2.3 Fine-tune the LLM to perform specific task s for MOFs hydrogen storage  \nThe LLM was fine -tuned using the MOFs material structure -hydrogen storage \nproperties corpus  we prepared, enabling it to incorporate and utilize external \ninformation. [25] The LLM's comprehension ability was used to understand the input \ninstructions related to MOFs. LLM-oriented hints were then constructed based on the \ninstructions and the reference information to motivate the LLM to learn  relevant \nknowledge and practical application. By employing this technique, the LLM \ndynamically selected reference information from the existing MOFs system and \ngenerated responses, effectively enhancing its ability to utilize external information.  \nThis process resulted in the development of the MOFs -LLM, which combines the \nstrengths of the LLM and the specific domain knowledge of MOFs. \nA multi -strategy fusion  approach was implemented to address the limitations of \nsingle fine-tuning strategy in  LLMs, which often suffer from low learning efficiency \nand poor convergence. This fusion technique combines supervised fine -tuning, \ncontrastive learning, and reinforcement learning strategies to enhance the LLM's \nperformance. By integrating these different fine-tuning techniques, the LLM-generated \ncontent becomes more relevant to the field of MOFs. Firstly, the supervised fine-tuning \ntechnique is employed to enable the LLM to learn specific tasks within  the domain of \nMOFs, such as generating compliant substance structures, substance synthesis \nstrategies, and other related tasks, utilizing labeled data as direct guidance.  Secondly, \ncontrast learning is utilized to enhance the LLM's ability to differentiate and make \njudgments. By exposing the model to contrasting examples and encouraging it to \ncapture subtle differences and patterns, the LLM becomes more adept at understanding \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nthe nuances within the MOFs domain.  Finally, reinforcement learning is applied to \nfurther enhance the quality and adaptability of the LLM's generation. Through the \nestablishment of artificial reward signals, the LLM's tendency to produce incorrect \nanswers is reduced, while the content of cor rect answers is reinforced. This iterative \nprocess helps improve the overall precision and reliability of the LLM's generated \noutputs. \n \n2.4 Ligand variation  methods and data sources  \nWe employed the ligand dataset established by Yaghi et al. as the cue engineering \ncorpus. [26] This dataset encompasses 11,806 distinct molecular representations and \nencapsulates 3,943 distinct structural transformations. By employing supervised fine-\ntuning techniques, the MOFs-LLMs were refined, culminating in establishing a ligand \nspace tailored for the reverse design of ligand variations to enhance MOFs performance. \n \n3 Results and discussion  \n3.1 Data collection and processing  for post -pretrain ing \nThe dataset used for post-pretraining the LLM consisted of various components. \nFirstly, it included a corpus of 2000 natural language descriptions specifically focused \non the structure -hydrogen storage properties of MOFs.  [27] Additionally, the dataset \ncomprised over 7000 research papers related to the topic, with a subset of 800 articles \n(referred to as MOFs_800) highly relevant to hydrogen storage. Lastly, a generalized \ncorpus was also incorporated as part of the training data. \nMOFs structure corpus. MOFs are formed by the coordination self -assembly of \nmetal ions with organic ligands, and their structures play a pivotal role in determining \ntheir physical and chemical properties. Key properties influenced by the MOFs \nstructure include pore size, surface area, and pore structure. which directly affect the \nperformance of MOFs in applications such as adsorption, energy storage, catalysis, etc. \nThe structure of MOFs encompasses not only their chemical compositions but also their \nmicroscopic arrangements and crystal structures. It is worth noting that  a LLMs may \npossess knowledge of the general concept of MOF composition but lacks specific \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nknowledge of their individual structures.  Therefore, providing a clear and detailed \ndescription of MOFs' structures can greatly enhance the language model's \nunderstanding of these materials and their properties.  By incorporating information \nabout the structure of MOFs, language models can offer more accurate guidance in \npredicting their performance, designing novel materials, or optimizing existing ones. \n \nFigure 2. A natural language description of the MOFs structure, a) the elements that \nmake up the description, and b) a sample description \nAs is shown in figure 2, the description of the metal in the MOF structure involved \nmanual labeling of various attributes. These attributes include the metal type, \ncoordination number, bond length, bond angle, and conformation of the metal center, \nas described in the corresponding  literature. For the ligand in the MOF structure, the \ndescription encompassed several components. These components were marked based \non the SMILES code, the property associated with the ligand, the molecular structural \ndescription of the ligand itself, and the coordination mode of the ligand, as mentioned \nin the relevant paper.  Regarding the MOF framework, the description entailed \nadditional annotations. These annotations comprised the crystal system, space group, \ntopology, single crystal density, gravimetric surface area, volumetric surface area, void \nfraction, pore volume, largest cavity diameter, pore limiting diameter, and information \nregarding the skeleton introduction, all of which were detailed in the corresponding \npaper. Firstly, 11,660 MOFs in the CoRE -MOFs dataset were clustered analysis with \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\neight parameters, including single crystal density, gravimetric surface area, volumetric \nsurface area, void fraction, pore volume, largest cavity diameter, pore limiting diameter, \nand hydrogen storage performance. Then a subset of 2,000 samples was extracted from \nthe clustered MOFs dataset to serve as a basis for generating natural language \ndescriptions of the MOFs' structures which aimed to capture the essential characteristics \nand features of the MOFs in a textual format. \nLiterature  corpus. Using \"MOF\" and \"hydrogen storage\" as keywords, about 800 \nhighly relevant papers in recent years were downloaded from the Web of Science. The \nselected papers are listed in Table S1. Another 6,563 papers were downloaded from \npapers covered in CoRE-MOFs. Each paper's title, abstract, chapter title and paragraph \nwere cleaned, filtered and formatted, and 6,221 papers of higher quality were used  for \npost-pretraining of the LLM. Segmentation of free-text paragraphs with sections from \nthe corpus was done using a sliding window to input the model.  As shown in Table 1, \nthe literature has been processed to a level of perplexity comparable to the general \nscientific corpus, which proves that the quality of data cleaning is good. \nTable 1. Statistics of corpus data processing results \n Number of tokens Word repetition rate Special character rate Perplexity \nMOFs_ALL 51580917 0.224 0.286 582.285 \nChemSum 56009510 0.265 0.219 580.725 \npeS2o 103732381 0.304 0.187 298.992 \nMOFs_800 8588503 0.277 0.250 502.900 \n \nGeneralized Scientific Corpus. The peS2o [28] dataset and ChemSum Datasets [29] \nwere used as a general corpus , which had been well-cleaned of data. The ChemSum \ndataset is a dataset of pure chemistry by accessing the open-access chemistry scholarly \njournal list, a dataset with 505,548 articles on pure chemistry. The Pes20 dataset has a \nshort corpus of about 40 million creative open-access academic articles, and we selected \nthe material portion.  The ratio of unique corpus to general corpus is 1:3, which is \ncleaned, filtered , and formatted for the post -pretraining of LLM to gain  more \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nfundamental knowledge of chemistry materials and logic. \n \n3.2 Post-pretraining and supervised fine -tuning of MOFs -LLMs \nThe model's knowledge base and computational power are initially shaped \nduring the pre -training phase.  However, merely fine -tuning a generic model may \nnot fully unlock its potential within a specific domain due to limitations.  \nFortunately,  the ERNIE model opens up the Post -pretrain API, which empowers \nthe model with the ability to learn and gain a deeper understanding of domain -\nspecific knowledge, including  as specific terms, concepts, patterns, laws, and so \non, related to MOF materials.  This characteristic enables  the model  to become  more \nspecialized and valuable , as it gains expertise in the domain of MOFs.  \nSolving the LLM problem within the domain of MOFs presents significant \nchallenges, but it is a creatively important endeavor . To address this, a post-\npretraining process was conducted on the ERNIE speed model, pushing its \nboundaries by training it on a combined corpus of MOFs, chemistry, and materials \nscience, totaling up to 210 million tokens. This extensive training endowed the \nmodel with a profo und understanding of MOFs and the capability to tackle \nproblems specific to this field.  As a result, a large -scale prognostic model named \nMOFs-LLM was developed exclusively for MOFs.  \nThis initiative would significantly  improve the performance of various tasks \nrelated to MOFs,  including predicting performance, reverse design, mechanism \nanalysis and prospective studies.  To ensure a reliable training process and avoid \noverfitting, a relatively conservative strategy was adopted.  First, the 210 million \ncorpora was employed for Post-pretrain, utilizing a relatively low learning rate. At this \nstage, the loss of the model slowly decreased, indicating that it was continuously \nmigrating towards MOFs and material domains. Subsequently, an incremental training \nmethod was implemented to further train the MOFs -LLM model. This involved \nincorporating additional data from over 7,000 MOFs literature sources to enhance the \nmodel's performance. Research has demonstrated that continu ed training on datasets \nfrom the target domain can be a cost-effective approach to improve results. [30] \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n \nFigure 3 . Post-pretrain and SFT process, a) overall training flow chart; b) changes in Loss \nduring three training sessions; c) the changes of perplexity ( ppi) during the three training \nsessions;  \nIt was noticed that both the loss and the perplexity of the model exhibited significant \ndecreases during the incremental training process (Figure 3 ). Although the  lowest \nachieved loss was approximately 1.2 , it would be a fairly low value for a LLMs. \nPerplexity, on the other hand, could be considered as a measure of a LLM's ability to \npredict language samples . Lower perplexity values indicate that the model performs \nbetter in the reasoning process , demonstrating higher accuracy.  [31] The perplexity of \nMOFs-LLM dropped to approximately 3.3, indicating that  the model is capable of \nselecting the appropriate response direction without becoming disoriented when \nsolving problems. The reduction of loss and perplexity showed that MOFs-LLM model \nachieved a significant improvement in domain accuracy and effectiveness [32]. The \nimproved performance of the MOFs -LLM model is not only reflected in its ability to \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nrecognize a large number of samples, but more importantly, in its predictions aligning \nwith the general rules and practical requirements within the field of MOFs. \nDue to lacking of precise evaluation criteria, the efficiency evaluation of \"post -\npretrain\" usually depends on the subsequent Supervised Fine -Tuning (SFT) process. \nIt is possible to evaluate the performance of \"post -pretrain\" model on specific tasks \nthrough SFT, and improve and optimi ze the model training process in data science. \nThe SFT of MOFs -LLM was executed, involving four different types of problem \nscenarios, including hydrogen storage performance prediction, reverse design, \nmechanism study, and application prospect analysis.  \nThe advantages of the MOFs-LLM model were demonstrated in vertical \nexperiments with ERNIE and GPT  3.5. To evaluate the performance of the models \nduring the SFT process, a panel of three experts specializing in the field of \nhydrogen storage designed a set of questions. The teams led by these experts \nprovided multiple specific and comprehensive answers.  The responses  generated \nby the three models (MOFs -LLM, ERNIE  3.5, and GPT  3.5) for 15 questions  were \npresented to the experts, who then evaluated them based on criteria such as \nrelevance, comprehensiveness, depth, and enlightening insights. The experts rated \nthe answers on a scale of 1 to 5, where a higher rating indicated a more favorable \nperception of the responses  \nTable 2. Comparison of expert ratings of three LLMs \n Relevance Comprehensiveness Depth Insightfulness \nGPT 3.5 3.79 3.55 3.45 3.48 \nERNIE 3.5 3.57 3.17 3.17 3.19 \nMOFs-LLM 4.05 3.90 3.98 3.88 \n \nThe results, as shown in Table 2, reveal that the scores of the three models were \nalmost equal in terms of accuracy and comprehensiveness, while the MOFs -LLM \nmodel outperformed the other two models in terms of illuminating and in -depth.  It \nillustrated that MOFs-LLM learnt more underlying logical thinking in post-pretrain and \nwas fully activated in the specific case of SFT.  Compared to its competitors, MOFs -\nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nLLM demonstrated a deeper level of thinking, provided more concrete answers, and \nsupported its arguments with extensive examples.  These characteristics surpassed the \nquality of the expert Q&A knowledge that was initially provided to the model during \nthe SFT training process (see SI for details). Despite using a relatively small amount of \nSFT data, the data that was utilized was of high quality. This allowed the model to be \nfinely tuned and achieve excellent performance, potentially even leading to innovative \nconclusions. It is possible that even w ith as little as 0.5% of the available data, the \nmodel could still exhibit impressive results. In the following reverse design process, \nMOFs-LLM was employed to mutate the ligands and assisted with the selection of \nsuitable metals and topologies as buildin g blocks, which aimed to explore the relative \nrelationship between MOFs materials and their hydrogen storage properties. \n \n3.3 Reverse design of high -performance hydrogen storage MOFs material  \nOne of the keys in reverse design is the construction of a vast chemical space to cover \na wide range of possible MOFs material structures and properties. In this way, a \nsystematic approach enables the rapid screening and design of MOFs materials with \ntargeted properties according to the specific requirements. While th e types of metals \nand topologies are largely defined, the ligands are the primary targets for modulation \nwith unlimited design potential. The structure and functional groups of the ligands \nallow the control of critical features such as pore size, surface area, and pore volume of \nMOFs materials, which in turn affects their hydrogen storage properties. Therefore, the \nprecise regulation and optimization of the hydrogen storage properties of MOFs \nmaterials can be achieved through the rational design and variation of ligands. \nTo construct the extensive chemical space required for reverse design of MOFs \nmaterials, the ligands were subjected to mutation using the MOFs-LLM model (see Fig. \n4). The approach of SFT via APIs was employed, which is particularly suitable for \nfunctional implementations with clearly defined requirements, as opposed to pre -\ntraining on large -scale unsupervised text data . It is expected that MOFs -LLM would \nperform as many mutations as possible on the given ligands under consideration of \nsymmetry to explore its ability to analyze the chemical space of MOFs. Simultaneously, \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nthe SMILES data with Pubchem CID were used for the next round of mutations each \ntime to improve the synthesis ability of the mutated molecules.  \n \nFigure 4. Fine-tune MOFs-LLM for a large number of high-quality ligands. a) flow chart of \nligand variation, b) and c) distribution of the ligand was generated for each round/operation, \nd) and e) rationality analysis of ligands generated by each round/operation. \nFour rounds of ligand mutation  were performed using natural language \nrepresentations (SMILES code), starting from the four most basic ligands, as shown in \nFig. x. The existence of variant ligands was validated using RDKit, and their molecular \nfingerprints were calculated and subsequently visualized in Uniform Manifold \nApproximation and Projection (UMAP). [33-34] As shown in figure 4b, 4c, the number of \nmutant ligands steadily increased  with increasing operation rounds . After only four \nrounds of iteration, thousands of variant products were generated from the  initial four \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nbase ligands. Among these variants, mutations performed using the insertion (I) method \nresulted in the highest number of mutation products, followed by the P method, and \nfinally the R method. The insertion (I) method exhibited high versatility, allowing for \nthe transfor mation of nearly any ligand into another possible ligand by inserting a \nbenzene ring, vinyl group, or azo group. It is worth noting that as the number of rounds \nincreased, the proportion of non -existent mutation products also increased. This \nphenomenon occurred because the generated structures gradually approached the limits \nof the model's comprehension ability.  \nThe UMAP visualization of the distribution allowed intuitive observation of the \nmutation and design process of the ligands of MOFs. In UMAP, each point represented \na mutation product, and the position indicated their similarity in the chemical space. By \nanalyzing the distribution, it was evident that numerous variant products were \nsuccessfully generated through several rounds of manipulation, starting from a limited \nnumber of initial essential ligands.  These variant ligands exhibited distinct chemical \nstructures and functional groups, demonstrating a rich diversity in the generated ligand \nspace. Moreover, the UMAP points were uniformly distributed throughout the chemical \nspace without apparent aggregation. It demonstrated that this design strategy could \nefficiently explore and cover all possibilities in the chemical space  beyond limiting to \na specific region or type. Detailed molecular maps of the ligands are available in the \nSupporting Information section. \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n \nFigure 5. The structure distribution of high-performance MOFs, where the green star in \nthe upper right is the MOFs with the highest performance in the figure, and the bottom is the \nexplanation given by MOFs-LLM. \nThrough bootstrapping and questioning, MOFs -LLM selected 150 classes from the \nabove-compiled ligands. At the same time, in parallel, 20 metals and 20 topologies were \nscreened out as potential high-performance hydrogen storage material building blocks \nby MOFs-LLM, which composed 100 ,842 MOFs data by ToBaCCo . [35] The current \npool of nodes and topologies represented a reasonable and achievable high -\nperformance structure for the MOFs mesh framework.  The coder -decoder SmV AE \nallows mapping the frame of MOFs with discrete representations into continuous \nvectors and back again. [36] The usable volumetric (UV) capacities  and usable \ngravimetric (UG) capacities values of 3182 MOF structures consisting of the first two \nrounds of mutant ligands at between 77 K/100 bar (filled state) and 160 K/5 bar were \npredicted using the trained the highly randomized trees (ERT) as l abelling data in \nSmV AE space. [37] As shown in Fig. X, the properties and distribution of MOFs \nstructures in the overall space were predicted with the assistance of SmV AE, forming a \nchemical space of high-performance hydrogen storage MOFs beyond the CoRE-MOFs \ndatabase. Since the SmV AE space was a vector space, continuous optimization and \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nsearch algorithms were used to find local minima or maxima.  It is possible to sample \nand reconstruct the MOFs frame by decoding. A subset of the generated MOF structures \nexhibited hydrogen storage performance that exceeded the performance of the initial \nstructures in the labelled data, as shown by the green crosses marking the Pareto front \nsurface of the balanced UV and UG performances in Fig. 5. The MOF corresponding \nto the Pareto optimum point for balancing UG and UV was decoded, as marked by the \ngreen star in the figure  5. It is an excellent performance hydrogen storage material at  \nbetween 77 K/100 bar (filled state) and 160 K/5 bar with ug of 43.433 wt% and UV of \n32.447 g-H2/L. Subsequently, the relationship between its structure and the hydrogen \nstorage mechanism was analy zed using MOFs -LLM, aiming to elucidate the reasons \nbehind its superior hydrogen storage capacity. The responses provided by MOFs-LLM \nwere compiled and analyzed. MOFs-LLM responses were as follows: \n \n4 Conclusion  \nTo meet the specific requirements of LLMs in the field of MOFs materials, large \namounts of structural and performance information on MOFs were analyzed, and 7000 \npieces of scientific and technological literature in related fields were internalized, \ncreating the first corpus describing MOFs materials in natural language. A large \nlanguage model (MOFs -LLM) tailored specifically for the field of MOFs was \ndeveloped using this corpus to post -pretrain ERNIE. Supervised fine -tuning of the \nexpert Q&A on reverse desi gn of MOFs targeting hydrogen storage materials , \nunleashed the model's potential  in tasks involving performance prediction, reverse \ndesign, mechanistic studies, and analysis of application prospects. In the practical \napplication of reverse design, a structured space containing more than 100,000 MOFs \nwas constructed using MOFs-LLM to perform many mutations of ligands and selecting \nappropriate building blocks in combination with MOFs-LLM. Ultimately, a target MOF \nstructure with high hydrogen storage performan ce was successfully identified. This \nwork demonstrates the practical application of LLMs in specific areas of materials \nscience and provides a methodological approach for reference. \n \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nAcknowledgements  \nThis work was supported by the National Key Research and Development Program \nof China (Grant No. 2021YFB3500700), National Natural Science Foundation of China \n(No. 52373261), Beijing Natural Science Foundation (L233011), Guangdong Basic and \nApplied Basic Research Foundation (Grant No. 2022A1515010185). \n \nConflict of Interest  The authors declare no conflict of interest. \n \nReferences   \n[1] H. Li, M. Eddaoudi, O. M. Yaghi, et al. Design and synthesis of an exceptionally stable \nand highly porous metal-organic framework[J]. Nature,1999, 402, 276–279.  \n[2] M. O’Keeffe, M.A. Peskov , O.M. Yaghi, et al.  The Reticular Chemistry Structure \nResource (RCSR) database of, and symbols for, crystal nets[J]. Accounts of chemical research, \n2008, 41, 1782–1789.  \n[3] Z. Wu, Y . Li, G. Wang, et al. Recent advances in metal-organic-framework-based catalysts \nfor thermocatalytic selective oxidation of organic substances [J]. Chem Catalysis , 2022, 2, \n1009–1045. \n[4] S.M. Moosavi, A. Nandy, B. Smit, et al. Understanding the diversity of the metal-organic \nframework ecosystem[J]. Nature Communications, 2020, 11, 4068.  \n[5] K.T. Butler, D.W. Davies, A. Walsh, et al. Machine learning for molecular and materials \nscience[J]. Nature, 2018, 559, 547–555. \n[6] K.M. Jablonka, D. Ongari, B. Smit, et al. Big-Data science in porous materials: Materials \ngenomics and machine learning[J]. Chemical Review, 2020, 120, 8066–8129.  \n[7] S.M. Moosavi, K.M. Jablonka, B. Smit , et al.  The role of machine learning in the \nunderstanding and design of materials[J]. Journal of the American Chemical Society, 2020, 142, \n20273–20287.  \n[8] OpenAI, J. Achiam, S. Jain, et al. GPT-4 Technical Report[J]. Computation and Language, \n2023.  \n[9] R . Thoppilan, D . D. Freitas, Q. Le, et al. LaMDA: Language Models for Dialog \nApplications[J]. Computation and Language, 2022. \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n[10] Y . Sun, S. Wang, H. Wu , et al . Ernie: Enhanced representation through knowledge \nintegration[J]. Computation and Language, 2019. \n[11] Z. Zheng, Z. Rong, O. M. Yaghi , et al. A GPT-4 Reticular Chemist for Guiding MOF \nDiscovery[J]. Angewandte Chemie International Edition, 2023, 62, e202311983. \n[12] K. M. Jablonka, P. Schwaller, Berend Smit , et al. Leveraging large language models for \npredictive chemistry[J]. Nature Machine Intelligence, 2024, 6, 161–169. \n[13] L. Pan, C. Hang, M. Yu, et al. Multilingual BERT Post -Pretraining Alignment [J]. \nAssociation for Computational Linguistics, 2021, 210–219. \n[14] X. Fang, F. Wang, L. Song, et al. A method for multiple-sequence-alignment-free protein \nstructure prediction using a protein language model[J]. Nature Machine Intelligence, 2023, 5, \n1087–1096. \n[15] Y . G. Chung, E. Haldoupis, R. Q. Snurr, et al. Advances, Updates, and Analytics for the \nComputation-Ready, Experimental Metal–Organic Framework Database: CoRE MOF 2019[J]. \nJournal of Chemical & Engineering Data, 2019, 64, 5985–5998 \n[16] Y . Sun, S. Wang, Z. Liu, et al.  ERNIE: Enhanced Representation through kNowledge \nIntEgration[J]. Computation and Language, 2020. \n[17] Y . Sun, S. Wang, H, Wang, et al.  ERNIE 3.0: Large -scale Knowledge Enhanced Pre -\ntraining for Language Understanding and Generation[J]. Computation and Language, 2021. \n[18] F. Faisal, A. Anastasopoulos, Investigating Post-pretraining Representation Alignment for \nCross-Lingual Question Answering[J]. Computation and Language, 2021. \n[19] X. Qiu, T. Sun, Z. Liu, et al. Pre-trained Models for Natural Language Processing: A \nSurvey[J]. Science China Technological Sciences, 2020, 63(10), 1872-1897. \n[20] J. Devlin, M. W. Chang, K. Toutanova, et al . BERT: Pre-training of Deep Bidirectional \nTransformers for Language Understanding. [J] In Proceedings of the 2019 Conference of the \nNorth American Chapter of the Association for Computational Linguistics: Human Language \nTechnologies, 2019, 1, 4171-4186. \n[21] A. Vaswani, N. Shazeer, I. Polosukhin, et al. Attention is All You Need[J]. In Advances in \nNeural Information Processing Systems, 2017, 30, 5998-6008. \n[22] S. P. Ong, W. D. Richards, G. Ceder. Python Materials Genomics (pymatgen): A Robust, \nOpen-Source Python Library for Materials Analysis[J]. Computational Materials Science, 2013, \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n68, 314-319.  \n[23] B. J. Bucior, A. S. Rosen, R. Q. Snurr, et al.  Identification Schemes for Metal –Organic \nFrameworks to Enable Rapid Search and Cheminformatics Analysis [J]. Crystal Growth and \nDesign, 2019, 19(11), 6682–6697. \n[24] F.H. Allen, Structural Science the Cambridge Structural Database: a quarter of a million \ncrystal structures and rising, Acta Crystallographica Section B: Structural Science, Crystal \nEngineering and Materials, 2002, 58, 380-388.  \n[25] Z. Yang, Q. Sun, S. Yuan. Advances in machine learning methods for computational design \nof metal–organic frameworks[J]. Coordination Chemistry Reviews, 2021, 436, 213836. \n[26] Z. Zheng, A. H. Alawadhi, S. Chheda, et al. Shaping the water -harvesting behavior of \nmetal–organic frameworks aided by fine -tuned GPT models[J]. Journal of the American \nChemical Society, 2023, 145(51): 28284-28295. \n[27] M. Zhang, Y . Huang, Z. Lin, et al. Machine learning constructs color features to accelerate \ndevelopment of long -term continuous water quality monitoring [J]. Journal of Hazardous \nMaterials, 2024, 461, 0304-3894. \n[28] K. Lo, L. L. Wang, D. Weld, et al. S2ORC: The Semantic Scholar Open Research Corpus[J]. \nAssociation for Computational Linguistics, 2020, 4969–4983. \n[29] Gr. Adams, B. H Nguyen, N. Elhadad , et al.  What are the Desired Characteristics of \nCalibration Sets? Identifying Correlates on Long Form Scientific Summarization [J]. \nComputation and Language, 2023. \n[30] C. Zhou, P. Liu, O. Levy, et al. LIMA: Less Is More for Alignment[J]. Machine Learning, \n2023. \n[31] J. Kaplan, S. McCandlish, D. Amodei, et al. Scaling Laws for Neural Language Models[J]. \nMachine Learning, 2020. \n[32] S. Gururangan, A. Marasovic, N. A. Smith, et al. Don’t Stop Pretraining: Adapt Language \nModels to Domains and Tasks[J]. Association for Computational Linguistics, 2020. \n[33] L. McInnes, J. Healy, J. Melville. UMAP: Uniform Manifold Approximation and \nProjection for Dimension Reduction[J]. Machine Learning, 2020. \n[34] RDKit: Open-source cheminformatics. https://www.rdkit.org \n[35] R. Andersona, D. A. Gómez -Gualdrón, Increasing topological diversity during \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\ncomputational “synthesis” of porous crystals: how and why [J]. CrystEngComm, 2019,21, \n1653-1665. \n[36] A. Ahmed, D. J. Siegel, Predicting hydrogen storage in MOFs via machine learning[J]. \nPatterns, 2021, 2(7), 100305. \n[37] Z. Yao, R. Q. Snurr A. Aspuru -Guzik, et al. Inverse design of nanoporous crystalline \nreticular materials with deep generative models[J]. Nature Machine Intelligence volume 2021, \n3, 76–86. \nhttps://doi.org/10.26434/chemrxiv-2024-xz77q ORCID: https://orcid.org/0000-0002-7811-0045 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0",
  "topic": "Hydrogen storage",
  "concepts": [
    {
      "name": "Hydrogen storage",
      "score": 0.5489844679832458
    },
    {
      "name": "Computer science",
      "score": 0.5212305784225464
    },
    {
      "name": "Hydrogen",
      "score": 0.31581738591194153
    },
    {
      "name": "Chemistry",
      "score": 0.23220300674438477
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I92403157",
      "name": "University of Science and Technology Beijing",
      "country": "CN"
    }
  ],
  "cited_by": 5
}