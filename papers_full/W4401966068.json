{
  "title": "Fine-tuning protein language models boosts predictions across diverse tasks",
  "url": "https://openalex.org/W4401966068",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2330378353",
      "name": "Robert Schmirler",
      "affiliations": [
        "Technical University of Munich",
        "Klinikum Ludwigshafen"
      ]
    },
    {
      "id": "https://openalex.org/A2897083495",
      "name": "Michael Heinzinger",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2001094594",
      "name": "Burkhard Rost",
      "affiliations": [
        "Technical University of Munich",
        "Weihenstephan-Triesdorf University of Applied Sciences",
        "Institute for Advanced Study"
      ]
    },
    {
      "id": "https://openalex.org/A2330378353",
      "name": "Robert Schmirler",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2897083495",
      "name": "Michael Heinzinger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2001094594",
      "name": "Burkhard Rost",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6852800892",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3111507638",
    "https://openalex.org/W2995514860",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W4316339774",
    "https://openalex.org/W4297243391",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W4317374308",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4383550741",
    "https://openalex.org/W4290546426",
    "https://openalex.org/W4303981501",
    "https://openalex.org/W4281291878",
    "https://openalex.org/W4399849668",
    "https://openalex.org/W6763868836",
    "https://openalex.org/W3213545574",
    "https://openalex.org/W4297895664",
    "https://openalex.org/W4280625391",
    "https://openalex.org/W4308773001",
    "https://openalex.org/W4205192056",
    "https://openalex.org/W4381885045",
    "https://openalex.org/W4225868104",
    "https://openalex.org/W4388024559",
    "https://openalex.org/W4281648132",
    "https://openalex.org/W4225438928",
    "https://openalex.org/W1985818354",
    "https://openalex.org/W4242765109",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W4385255463",
    "https://openalex.org/W4283068487",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3198675127",
    "https://openalex.org/W4320855461",
    "https://openalex.org/W3216325381",
    "https://openalex.org/W4382632254",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W2060277733",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W6861489518",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W2769882797",
    "https://openalex.org/W2041369927",
    "https://openalex.org/W3083871792",
    "https://openalex.org/W2950954328",
    "https://openalex.org/W4386480415",
    "https://openalex.org/W1971166275",
    "https://openalex.org/W2736065260",
    "https://openalex.org/W2032579724",
    "https://openalex.org/W4375858802",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2060588922",
    "https://openalex.org/W2379594833",
    "https://openalex.org/W3127426316",
    "https://openalex.org/W2483469645",
    "https://openalex.org/W2735621019",
    "https://openalex.org/W3015921770",
    "https://openalex.org/W2730472814",
    "https://openalex.org/W3044929118",
    "https://openalex.org/W2950374603",
    "https://openalex.org/W6892920195",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2971227267",
    "https://openalex.org/W4206950245"
  ],
  "abstract": "Abstract Prediction methods inputting embeddings from protein language models have reached or even surpassed state-of-the-art performance on many protein prediction tasks. In natural language processing fine-tuning large language models has become the de facto standard. In contrast, most protein language model-based protein predictions do not back-propagate to the language model. Here, we compare the fine-tuning of three state-of-the-art models (ESM2, ProtT5, Ankh) on eight different tasks. Two results stand out. Firstly, task-specific supervised fine-tuning almost always improves downstream predictions. Secondly, parameter-efficient fine-tuning can reach similar improvements consuming substantially fewer resources at up to 4.5-fold acceleration of training over fine-tuning full models. Our results suggest to always try fine-tuning, in particular for problems with small datasets, such as for fitness landscape predictions of a single protein. For ease of adaptability, we provide easy-to-use notebooks to fine-tune all models used during this work for per-protein (pooling) and per-residue prediction tasks.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6008324027061462
    },
    {
      "name": "Computational biology",
      "score": 0.352819561958313
    },
    {
      "name": "Biology",
      "score": 0.25509533286094666
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    }
  ],
  "cited_by": 108
}