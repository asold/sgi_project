{
    "title": "LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding",
    "url": "https://openalex.org/W4403220680",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Wan, Zhizhong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2146677841",
            "name": "Yin Bin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1954237583",
            "name": "Xie Junjie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096743827",
            "name": "Jiang Fei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1811607940",
            "name": "Li, Xiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2023645851",
            "name": "Lin Wei",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4386728933",
        "https://openalex.org/W2475334473",
        "https://openalex.org/W6810081322",
        "https://openalex.org/W4285294723",
        "https://openalex.org/W4296591867",
        "https://openalex.org/W4386081001",
        "https://openalex.org/W4386728881",
        "https://openalex.org/W2946044191",
        "https://openalex.org/W2793768763",
        "https://openalex.org/W2809290718",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W3087931390",
        "https://openalex.org/W4392367398",
        "https://openalex.org/W3169113923",
        "https://openalex.org/W4386729835",
        "https://openalex.org/W3104030692",
        "https://openalex.org/W3104789011",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4288089799"
    ],
    "abstract": "Click-Through Rate (CTR) prediction is crucial for Recommendation System(RS),\\naiming to provide personalized recommendation services for users in many\\naspects such as food delivery, e-commerce and so on. However, traditional RS\\nrelies on collaborative signals, which lacks semantic understanding to\\nreal-time scenes. We also noticed that a major challenge in utilizing Large\\nLanguage Models (LLMs) for practical recommendation purposes is their\\nefficiency in dealing with long text input. To break through the problems\\nabove, we propose Large Language Model Aided Real-time Scene\\nRecommendation(LARR), adopt LLMs for semantic understanding, utilizing\\nreal-time scene information in RS without requiring LLM to process the entire\\nreal-time scene text directly, thereby enhancing the efficiency of LLM-based\\nCTR modeling. Specifically, recommendation domain-specific knowledge is\\ninjected into LLM and then RS employs an aggregation encoder to build real-time\\nscene information from separate LLM's outputs. Firstly, a LLM is continual\\npretrained on corpus built from recommendation data with the aid of special\\ntokens. Subsequently, the LLM is fine-tuned via contrastive learning on three\\nkinds of sample construction strategies. Through this step, LLM is transformed\\ninto a text embedding model. Finally, LLM's separate outputs for different\\nscene features are aggregated by an encoder, aligning to collaborative signals\\nin RS, enhancing the performance of recommendation model.\\n",
    "full_text": "LARR: Large Language Model Aided Real-time Scene\nRecommendation with Semantic Understanding\nZhizhong Wan\nwanzhizhong@meituan.com\nMeituan\nBeijing, China\nBin Yin\nyinbin05@meituan.com\nMeituan\nBeijing, China\nJunjie Xie\nxiejunjie02@meituan.com\nMeituan\nBeijing, China\nFei Jiang\njiangfei05@meituan.com\nMeituan\nBeijing, China\nXiang Li\nlixiang245@meituan.com\nMeituan\nBeijing, China\nWei Lin\nlinwei31@meituan.com\nMeituan\nBeijing, China\nABSTRACT\nClick-Through Rate (CTR) prediction is crucial for Recommendation\nSystem(RS), aiming to provide personalized recommendation ser-\nvices for users in many aspects such as food delivery, e-commerce\nand so on. However, traditional RS relies on collaborative signals,\nwhich lacks semantic understanding to real-time scenes. We also\nnoticed that a major challenge in utilizing Large Language Models\n(LLMs) for practical recommendation purposes is their efficiency\nin dealing with long text input. To break through the problems\nabove, we propose Large Language Model Aided Real-time Scene\nRecommendation(LARR), adopt LLMs for semantic understanding,\nutilizing real-time scene information in RS without requiring LLM\nto process the entire real-time scene text directly, thereby enhanc-\ning the efficiency of LLM-based CTR modeling. Specifically, rec-\nommendation domain-specific knowledge is injected into LLM and\nthen RS employs an aggregation encoder to build real-time scene\ninformation from separate LLMâ€™s outputs. Firstly, a LLM is con-\ntinual pretrained on corpus built from recommendation data with\nthe aid of special tokens. Subsequently, the LLM is fine-tuned via\ncontrastive learning on three kinds of sample construction strate-\ngies. Through this step, LLM is transformed into a text embedding\nmodel. Finally, LLMâ€™s separate outputs for different scene features\nare aggregated by an encoder, aligning to collaborative signals in\nRS, enhancing the performance of recommendation model.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Artificial intelligence.\nKEYWORDS\nRecommendation System, Large Language Model, Contrastive Learn-\ning\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nRecSys â€™24, October 14â€“18, 2024, Bari, Italy\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0505-2/24/10. . . $15.00\nhttps://doi.org/10.1145/3640457.3688135\nACM Reference Format:\nZhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, and Wei Lin. 2024.\nLARR: Large Language Model Aided Real-time Scene Recommendation\nwith Semantic Understanding. In 18th ACM Conference on Recommender\nSystems (RecSys â€™24), October 14â€“18, 2024, Bari, Italy. ACM, New York, NY,\nUSA, 10 pages. https://doi.org/10.1145/3640457.3688135\n1 INTRODUCTION\nAs an important task in field of recommendation systems (RS), Click-\nThrough Rate (CTR) prediction aims to forecast whether a user will\nclick on a certain product, content, or advertisement. In the domain\nof RS, models need to analyze data such as user characteristics\nand item features to predict the probability of a user clicking on a\nspecific objective, thereby helping to optimize the effectiveness of\nrecommendation strategies.\nIn the context of food delivery service, the volume of data that\nneeds to be processed each day reaches hundreds of millions. Unlike\ntypical e-commerce recommendations or content recommendations,\nfood delivery recommendations place a stronger emphasis on an-\nalyzing real-time scenes such as geographical location, mealtime,\nweather, etc. Among them, there exist strong correlations and rich\nsemantics. Consider a specific scene: a user traveling to another city\narrives late at night while many restaurants are already closed, and\nit is raining heavily with low temperatures. Due to the userâ€™s unfa-\nmiliarity with the surrounding environment and the poor weather\nconditions, there is a high probability that he will order a takeout\nfrom a nearby restaurant. Whatâ€™s more, heâ€™s more likely to order\nsome hot food or local specialties.\nPoints of Interest (POI) is a concept we define based on food\ndelivery recommendations, aimed at abstracting the focus of user\ninterest. It can be understood as a collection of POIs, environment,\nand geographic locations. Traditional recommendation models use\nfeature crossing to handle POIs in real-time scene data. For corre-\nlated and important features, common feature crossing methods\ninclude manual explicit crossing followed by logistic regression\n[5], explicit low-order crossing [5], or directly using deep learning\nfor automatic high-order crossing [12, 15, 29]. However, regardless\nof which method is used, the essence of feature crossing is based\non the co-occurrence probability of strongly correlated features to\ndetermine whether a user will click, lacking an understanding of\nthe POI in the sceneâ€™s semantic information.\narXiv:2408.11523v1  [cs.IR]  21 Aug 2024\nRecSys â€™24, October 14â€“18, 2024, Bari, Italy Zhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, and Wei Lin\nLarge Language Models (LLMs) possess extensive semantic knowl-\nedge and excellent reasoning capabilities [13, 14], which can effec-\ntively understand the semantic information in real-time scenes.\nHowever, in the field of RS, LLMs still face some difficulties. In\nparticular, LLMs in their original form may not possess in-depth\nknowledge specific to the recommendation domain, which could\naffect their comprehensive understanding of POI and user infor-\nmation. At the same time, LLMs typically do not directly handle\nthe cross-feature signals present in traditional recommendation\nmodels, which may limit their ability to accurately capture and\nunderstand user needs.\nThe unique recommendation scene of food delivery presents us\nwith two core challenges:\nâ€¢Typical RS treats each food delivery POI as mapped unique id\ntoken as inputs of the recommendation model to predict user\nbehavior. However, this approach treats all POI equally without\nconsidering semantic information of POIs, such as the menu sim-\nilarity of food delivery, relevance of restaurantâ€™s main business\ndirection to the scene and so on. For instance, pizza shops with\ndifferent names would be treated as two unrelated POIs in a nor-\nmal RS, neglecting the fact that both of them primarily sell pizza.\nDuring summer, people tend to order something cold rather than\ntaste spicy and hot food.\nâ€¢Industrial RS needs to serve for millions of users, unacceptable\ntime consuming of LLMâ€™s inference stays a unsolved problem in\nthe exploring of combination with RS and LLM. Real-time scenes\nare composed of many individual scene features, with combi-\nnations of different scene features giving rise to an exponential\nnumber of different real-time scenes. To understand real-time\nscenes, LLMs need to process combinations of scene feature texts\nduring serving, which seems that LLMs have to be involved in in-\nference, leading to service latency. How to efficiently utilize LLMs\nto handle real-time scenes is an urgent problem to be solved.\nTo address the two problems mentioned above, inspired by recent\nwork in the LLM field [19, 24] and recommendation system domain\n[7, 11], we propose a three-stage model LARR (Large Language\nModel Aided Real-time Scene Recommendation with Semantic Un-\nderstanding). Figure 1 uses an intuitive example to show how LARR\nworks. The LLM, infused with recommendation domain knowledge,\nFigure 1: How LLM understands the real-time scenes and\nhelp Recommendation System work.\nprocesses various scene information based on its knowledge, such\nas user profiles, geographic locations, weather, etc. The LLM con-\ntemplates the input information according to its existing knowledge,\nassisting the recommendation system in providing recommendation\nservices. In the example of Figure 1, LLM receives input informa-\ntion including user profiles, geographic locations, environment,\nuser history and so on. Because LLM is injected with recommen-\ndation domain knowledge in advance, LLM knows what style of\nfood the user may like, which kind of food is more popular in cur-\nrent environment, where to find tasty shops nearby. From the user\nprofile (likes desserts/hotpot), LLM knows ice-cream or spicy hot\npot is ok; furthermore, since it is very hot, hot pot is obviously not\nquite suitable, ice cream or a cold drink would be a better choice.\nThere are three stores A, B, and C selling desserts around the input\ngeographic location, and their takeout information is... Like this,\nstep by step, the LLM finally summarizes useful information and\ncontributes to RS in recommending takeout for the user.\nOur core contributions are as follows:\nâ€¢Based on food delivery domain-specific recommendation data,\nWe employ a LLM to deal with the semantic problem of how to\ncorrectly understand the POI in food delivery real-time scene.\nWe continue pretraining and fine-tuning the LLM, infusing them\nwith knowledge of recommendation field, promoting the LLMsâ€™\nunderstanding of food delivery real-time scenes.\nâ€¢We propose a novel industrial-friendly recommendation frame-\nwork, LARR, which can effectively integrate the semantic infor-\nmation of food delivery real-time scenes without LLMâ€™s participa-\ntion in inference. We use a bidirectional encoder to aggregate the\nseparate real-time scene embedding produced by LLM to extract\nthe semantic information of real-time scenes, since the semantic\nassociations between various scene features have already been\nimplicitly encoded into the LLMâ€™s output vectors.\nâ€¢We conducted extensive offline and online experiments on the\nfood delivery dataset from Meituan Waimai, validating that our\nmethod can fully understand the semantic information of real-\ntime scenes and effectively integrate multimodal features, en-\nhancing the recommendation results to align the semantic in-\nformation understood by large language models about the food\ndelivery real-time scene with the recommendation models based\non collaborative signals.\n2 RELATED WORK\n2.1 Large Language Model\nSubstantial work [ 8, 16, 37] has shown that Pre-trained models\n(PTMs) on a large corpus can learn universal language represen-\ntations, which are beneficial for downstream NLP tasks and can\navoid training a new model from scratch [26]. Large language mod-\nels (LLMs) mainly refer to transformer-based [31] neural language\nmodels that contain tens to hundreds of billions of parameters [38].\nThose language models are pre-trained on massive text data, such\nas PaLM [6], LLaMA [30], and GPT-4 [1]. Compared to PLMs, LLMs\nare not only significantly larger in model size but also demonstrate\nsuperior language understanding and generation capabilities, more\nimportantly, they exhibit emergent abilities [32] that are absent in\nsmaller-scale language models.\nLARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding RecSys â€™24, October 14â€“18, 2024, Bari, Italy\nLLMs have already drawn a lot of attention due to the strong\nperformance on a wide range of natural language tasks, many\nresearchers in other fileds try to combine LLM with domain-specific\nwork to promote their progress and has achieved success [17, 18,\n34].\n2.2 LLM in Recommend System\nTraditional recommendation models are designed to leverage a\nhuge amount of ID tokens to train, which is an ID paradigm, with\nthe majority of parameters concentrated in the embedding layer\n[21, 29]. In contrast, LLMs would use a tokenizer to segment the\ntext into vocab tokens to reduce the size of the vocabulary at the\nvery first of input, which is a tokenization paradigm, with the bulk\nof parameters being concentrated in the network itself. Recom-\nmendation models excel in memorization, while LLMs demonstrate\nsuperior capabilities in logical reasoning and generalization. A 6-\nbillion parameter LLM such as ChatGLM-3 [ 9] has a vocabulary\nless than 65,000 tokens, a scale that is significantly smaller than\nthe number of ID tokens in a recommendation model of the same\nscale [23]. Itâ€™s apparent that simple integration of recommendation\nsystems with LLMs is infeasible. If ID tokens are directly used as\ninput, tokenization may establish connections between unrelated\nIDs (e.g., â€™id_499â€™ tokenized into â€™idâ€™, â€™_ â€™, â€™4â€™, â€™9â€™, â€™9â€™, creating overlap-\nping embeddings with IDs containing the digits 4 and 9, which does\nnot meet our expectations). Alternatively, regard the ID tokens as\nspecial tokens of LLMâ€™s vocabulary, that is, without tokenization on\nID input, would face gap problem between ID tokens representing\nuser/item collaborative information and pre-trained vocab tokens\nholding content semantics information [4]. Moreover, the typically\nvast number of ID tokens could dilute the LLMâ€™s own vocabulary,\ninject noise into the vocab tokens, and result in poor learning of\nthe ID tokens.\nDespite the challenges mentioned above associated with apply-\ning language models to the field of recommendation, researchers\ncontinue to dedicate efforts to applying language models to Recom-\nmender Systems (RS), due to the astonishing capabilities demon-\nstrated by LLMs [ 20, 33]. The researchers of P5 [ 11] proposed a\nunified text-to-text paradigm recommendation model based on T5\n[27], hoping to handle rating prediction task, sequential recommen-\ndation task and more downstream tasks by zero-shot or few-shots.\nAn embedding method called whole-word embedding whose design\ninspiration is very similar to position embedding is introduced to ad-\ndress ID-related gap problem mentioned above. M6-rec [7] converts\nall recommend downstream tasks into language understanding or\ngeneration tasks by representing user behavior data and candidates\ndata if necessary as natural language plain texts for fine-tuning\nbased on M6 [22]. CLLM4Rec [39] propose a novel soft/hard prompt-\ning strategy, mutually-regularized pre-training two LLMs and two\nset of id tokens on two corpora to facilitate language modeling\non RS-specific corpora with heterogeneous user/item collaborative\ntokens and content tokens.\nAs an unsupervised method, Contrastive Learning (CL) assumes\nsome observed pairs of text that are more semantically similar\nthan randomly sampled text. By maximazing their mutual infor-\nmation, neural network could learn useful embeddings for down-\nstream tasks [2, 36]. OpenAI has attempted to convert pre-trained\nlanguage models into vector models for text and code using con-\ntrastive learning, achieving notable results [24]. RLMRec [28] has\nproven that contrastive learning between the semantic embeddings\nfrom LLMs and the collaborative embeddings from recommender\nsystems could capture their shared information and alleviate the\nnoise information. ControlRec [25] uses contrastive learning for\nheterogeneous feature matching to align the ID representations\nwith the natural language in the semantic space. Li proposed a\ntwo-stage model, CTRL [19]. In the first stage, an LLM is used to\nencode textual data and a lightweight collaborative model is used to\nencode tabular data, cross-modal contrastive learning is employed\nto fine-grained align knowledge of the two modalities. In the sec-\nond stage, the lightweight collaborative model is fine-tuned on\ndownstream tasks. During inference, CTRL only deploys the col-\nlaborative model, with the LLM not being involved in computation,\nmaking it an industrial-friendly model architecture.\n3 METHODOLOGY\nIn this section, we will then introduce the framework of LARR,\nwhich comprises three stages, with the overall structure illustrated\nin Fig 2. The first stage is the continual pretraining stage, where we\nconstruct natural language texts corpus from the Meituan Waimai\ndataset. Then we continue pretraining the LLM that has been pre-\ntrained on general corpora based on corpus to inject the domain-\nspecific knowledge. In the second stage, we additionally add billions\nof user profile and user history behavior data into the corpus, which\nis proven to be useful [10], constructing 3 kinds of contrastive learn-\ning positive and negative samples, transforming the LLM into a\ntext embedding model, enabling the LLM to fully understand the se-\nmantics of real-time scenes. The final third stage is the multi-modal\nalignment stage, where contrastive learning is used to maximize the\nmutual information between semantic embeddings and collabora-\ntive embeddings, aligning the takeout scene semantic information\nunderstood by the LLM with the collaborative signals extracted\nby the fine-tuning model from ID tokens, cross features, and sta-\ntistical features. The aligned semantic information will enhance\nperformance of recommendation system.\n3.1 Continual Pretraining Task Design\nTo facilitate LLMâ€™s understanding in food delivery domain-specific\nknowledge from the natural language perspective, we first construct\na corpus using datasets relevant to all POIs. Following this, we\nperform continual pretraining task on the LLM which had already\nbeen pre-trained on generic corpora, using this corpus to enhance\nits understanding of domain-specific knowledge.\nHow to use id tokens in LLM efficiently remains an unsolved\nproblem for a long-time. In industrial scene, encoding the ids of mil-\nlions of different POIs as whole words is impractical due to the high\ntime and memory costs involved. Moreover, simply treating POI id\ntokens as indivisible inputs to the LLM brings a semantic and col-\nlaborative signal gap, as mentioned in section 2.2; segmenting and\nencoding ids could also introduce unintended connections between\nid tokens. Consequently, we abandoned the use of id token as LLM\ninputs and instead used unique natural language description texts\nto represent id features. We noted that a POI can be uniquely iden-\ntified by its its name and geographical location, which is equivalent\nRecSys â€™24, October 14â€“18, 2024, Bari, Italy Zhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, and Wei Lin\nFigure 2: Model overview including 3 stages. In stage 1, The\nLLM undergoes a continual pretraining task on shop-related\ncorpus; In stage 2, LLM is fine-tuned and transformed into a\ntext embedding model via contrastive learning on 3 types of\npositive samples; In stage 3, alignment is applying on LLMâ€™s\nsemantic embedding and RSâ€™s collaborative embedding for\nenhancing the performance of the recommendation results.\nto an id feature. The advantage of using name and geographical\nlocation to represent a POI is apparently: natural language input\nmakes it easier for LLM to understand. It retains the uniqueness of\nid inputs while avoiding the semantic meaninglessness of id inputs.\nSpecifically, assuming that there are ğ‘ğ‘ POIs, we merge name,\ngeographical location, introduction, and statistical information to\nform a description ğ·ğ‘– for each POI ğ‘ƒğ‘‚ğ¼ğ‘–.\nğ· = {ğ·ğ‘– |ğ‘– âˆˆN,0 â‰¤ğ‘– â‰¤ğ‘ğ‘} (1)\nThe description ğ·ğ‘– is cut into ğ‘›ğ‘˜ different slices and we named\neach slice a keyword, such as name, location, tag name... The ğ‘–ğ‘¡â„\nrestaurantâ€™s description ğ·ğ‘– could be formulated as a text set ğ‘¡ğ‘–:\nğ‘¡ğ‘– = {ğ‘¡ğ‘–ğ‘˜ |ğ‘˜ âˆˆN,0 â‰¤ğ‘˜ â‰¤ğ‘›ğ‘˜} (2)\nğ·ğ‘– = [ğ‘¡ğ‘–0,ğ‘¡ğ‘–1,...ğ‘¡ğ‘–ğ‘›ğ‘˜ ] (3)\nwhere [...]is concatenation between different text and ğ‘¡ğ‘–ğ‘˜ is a\ndiscrete text feature,ğ‘¡ğ‘– is the aggregation of a bunch of discrete text\nfeatures.\nThrough the descriptive text, the LLM could capture the similar-\nity among different POIs. Since the language model was pre-trained\non general corpora, thus there are some gap between the pretrain-\ning and continual pretraining corpora. To address this problem,\nwe introduce some special tokens to assist the LLM in better un-\nderstanding the content during the continual pretraining stage. To\nbe specific, we use a set of special token ğ‘ ğ‘ for ğ‘›ğ‘˜ keywords in\ndescription to wrap key information.\nğ‘ ğ‘ğ‘ = {(ğ‘ ğ‘ğ‘ğ‘œğ‘ \nğ‘˜ ,ğ‘ ğ‘ğ‘’ğ‘œğ‘ \nğ‘˜ )| ğ‘˜ âˆˆN,0 â‰¤ğ‘˜ â‰¤ğ‘›ğ‘˜} (4)\nFor example, <POI_name> and </POI_name> would be added\nat the very first and the very last of the 0ğ‘¡â„ keyword POI name\nrespectively, the former special token is ğ‘ ğ‘ğ‘ğ‘œğ‘ \n0 indicating that the\nfollowing information pertains to the restaurantâ€™s name while the\ntrailing one is is ğ‘ ğ‘ğ‘’ğ‘œğ‘ \n0 , signaling the end of the name input. With\nthe help of special token set ğ‘ ğ‘, We split the original description of\neach POI into ğ·ğ‘– = [ğ‘¥ğ‘–,ğ‘¦ğ‘–]for LLM. The text input ğ‘¥ğ‘– and expected\ntext output ğ‘¦ğ‘– could be formulated as follows:\nğ‘¥ğ‘– = [ğ‘ ğ‘ğ‘ğ‘œğ‘ \n0 ,ğ‘¡0\nğ‘–,ğ‘ ğ‘ğ‘’ğ‘œğ‘ \n0 ,ğ‘ ğ‘ğ‘ğ‘œğ‘ \n1 ,ğ‘¡1\nğ‘–,ğ‘ ğ‘ğ‘’ğ‘œğ‘ \n1 ] (5)\nğ‘¦ğ‘– = [ğ‘ ğ‘ğ‘ğ‘œğ‘ \n2 ,ğ‘¡2\nğ‘–,ğ‘ ğ‘ğ‘’ğ‘œğ‘ \n2 ,ğ‘ ğ‘ğ‘ğ‘œğ‘ \n3 ,ğ‘¡3\nğ‘–,ğ‘ ğ‘ğ‘’ğ‘œğ‘ \n3 ...ğ‘ ğ‘ğ‘ğ‘œğ‘ \nğ‘›ğ‘˜ ,ğ‘¡ğ‘›ğ‘˜\nğ‘– ,ğ‘ ğ‘ğ‘’ğ‘œğ‘ \nğ‘›ğ‘˜ ] (6)\nwhere [...]means concatenation between different text.\nWe denote the LLM parameters as ğœƒ and employ a generative\nloss ğ¿ğ‘ğ‘ for continual pretraining.\nğ¿1 = âˆ’ 1\n|ğ‘ğ‘|\n|ğ‘ğ‘ |âˆ‘ï¸\nğ‘¥ğ‘–,ğ‘¦ğ‘– âˆˆğ‘ğ‘\n|ğ‘¦ğ‘– |âˆ‘ï¸\nğ‘ =1\nğ‘™ğ‘œğ‘”(ğ‘ƒğœƒ(ğ‘¦ğ‘–ğ‘ |ğ‘¥ğ‘–,ğ‘¦ğ‘–<ğ‘ )) (7)\nThe input of LLM consists of restaurantâ€™s name and location ğ‘¥ğ‘–,\naiming to predict other detail information ğ‘¦ğ‘– such as POIâ€™s intro-\nduction, main dishes offered... During the training process, the\nmodel learned the associations between similar dishes and built\nconnections among geographical locations, POI names, menu and\nso on.\n3.2 Text Embedding via Contrastive Learning\nThere is a huge gap between the normal decoder-only architecture\nof LLM and text embedding models, because language models train\nand infer in a way of autoregressive approach. To efficiently lever-\nage the semantic information from the LLM, a critical problem we\nfaced is how to transform the LM into a text embedding model.\nCommon methods for converting a language model into one that\ncould encode a sentence into a embedding include using the output\nembedding corresponding to some special tokens as the sentence\nembedding, or directly use some pooling methods on embeddings\nof all words in sentence to obtain the sentence embedding. Inspired\nby OpenAI [24], we decide to adopt contrastive learning to convert\nthe LLM into a text embedding model. In NLP field, contrastive\nlearning methods usually construct positive sample pairs by apply-\ning corruptions such as dropout on text to generate positive sample\npairs, random sampling for negative sample pairs. However, in RS,\nthe importance of user, POI pairs in the food delivery context is\nvery high, so we do not simply use corruption. Instead, we con-\nstructed positive and negative sample pairs from three perspectives:\nuser-user, POI-POI, and user-POI.\nAs is shown in figure 3, the LLM tends to in classify three kinds\nof input is positive or not in contrastive learning procedure. First of\nall, because of user-side text is added, we utilize a set special tokens\nğ‘ ğ‘ğ‘¢ for user-side text similar to ğ‘ ğ‘ğ‘ in section 3.1.\nğ‘ ğ‘ğ‘¢ = {(ğ‘ ğ‘ğ‘ğ‘œğ‘ \nğ‘˜ ,ğ‘ ğ‘ğ‘’ğ‘œğ‘ \nğ‘˜ )| ğ‘˜ âˆˆN,ğ‘›ğ‘˜ +1 â‰¤ğ‘˜ â‰¤ğ‘›ğ‘˜ +ğ‘›ğ‘} (8)\nLARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding RecSys â€™24, October 14â€“18, 2024, Bari, Italy\nFigure 3: Positive pairs construction. Red curves represent alignment procedure.\nThe ğ‘›ğ‘ represents the number of keywords in user-related text. We\ndenote the union of two sets as ğ‘ ğ‘.\nğ‘ ğ‘ = ğ‘ ğ‘ğ‘ âˆªğ‘ ğ‘ğ‘¢ (9)\nHere, set ğ‘ ğ‘ has ğ‘›ğ‘˜ +ğ‘›ğ‘ elements.\nWe define a LLM-based score functionağ‘ ğœƒ for evaluating the simi-\nlarity of inputs. The score functionağ‘ ğœƒ would calculate the similarity\nof two input text base on ğ‘ƒğœƒ. Similar to continual pretraining stage,\nthe description ğ‘§of POI or user is divided into several segments, in-\nterspersed with special tokens in ğ‘†ğ‘ƒ. Assuming that thereâ€™s a series\nof text input ğ‘§ = [ğ‘ ğ‘ğ‘ğ‘œğ‘ \n0 ,ğ‘§0,ğ‘ ğ‘ğ‘’ğ‘œğ‘ \n0 ,ğ‘ ğ‘ğ‘ğ‘œğ‘ \n1 ,ğ‘§1,ğ‘ ğ‘ğ‘’ğ‘œğ‘ \n1 ...ğ‘ ğ‘ğ‘ğ‘œğ‘ \nğ‘™ ,ğ‘§ğ‘™,ğ‘ ğ‘ğ‘’ğ‘œğ‘ \nğ‘™ ],\nit would be first tokenized into a series of tokens and mapping to a\nseries of token embedding Z = [z0,z1...zm], then sending to LLM\nafter padding.\nhğ‘§ = ğ‘€ğ¿ğ‘ƒ(ğ¿ğ¿ğ‘€ğœƒ(ğ‘§)ğ‘š) (10)\nwhere ğ‘€ğ¿ğ‘ƒ is a linear layer to project the embedding of last token\nin last hidden layer to a continuous vector space for scoring.\nğ‘ ğœƒ(ğ‘§1,ğ‘§2)= ğ‘ (hğ‘§1 ,hğ‘§2 ) (11)\nHere, ğ‘  could be a similarity function such as cosine similarity; or\nnegative form of the output of a distance function such as cosine\ndistance. It could be symmetric or asymmetric. If ğ‘  is symmetric,\nthen ğ‘ ğœƒ is symmetric; otherwise ğ‘ ğœƒ asymmetric.\nUSER-USER In user-user contrastive learning, we focus on\nuserâ€™s profile text descriptions ğ‘ˆğ‘ (such as nicknames, gender, etc.)\nand the userâ€™s action text descriptionsğ‘ˆğ‘ (such as historical actions,\nenvironments, click and order price statistics...).\nThese two types of text descriptions are positive samples for the\nsame user and negative samples for different users. During training,\nwe adopt a contrastive learning approach, with the loss being the\nInfo NCE Loss. For user-user contrastive loss ğ¿ğ‘ˆğ‘ˆ , considering the\ngeneral situation, we assume that ğ‘ ğœƒ is asymmetric. As a result, the\nform of contrastive loss is symmetrical and the sub loss ğ¿ğ‘ˆğ‘ˆğ‘ğ‘ could\nbe fomulated as follows:\nğ¿ğ‘ˆğ‘ˆ\nğ‘ğ‘ = âˆ’ 1\nğ‘ğ‘¢\nğ‘ğ‘¢âˆ‘ï¸\nğ‘–=1\nğ‘’ğ‘¥ğ‘(ğ‘ ğœƒ(ğ‘ˆğ‘,ğ‘ˆğ‘+)/ğœ)\nğ‘’ğ‘¥ğ‘(ğ‘ ğœƒ(ğ‘ˆğ‘,ğ‘ˆğ‘+)/ğœ)+Ãğ‘’ğ‘¥ğ‘(ğ‘ ğœƒ(ğ‘ˆğ‘,ğ‘ˆğ‘âˆ’)/ğœ)\n(12)\nğ¿ğ‘ˆğ‘ˆ\nğ‘ğ‘ = âˆ’ 1\nğ‘ğ‘¢\nğ‘ğ‘¢âˆ‘ï¸\nğ‘–=1\nğ‘’ğ‘¥ğ‘(ğ‘ ğœƒ(ğ‘ˆğ‘,ğ‘ˆğ‘+)/ğœ)\nğ‘’ğ‘¥ğ‘(ğ‘ ğœƒ(ğ‘ˆğ‘,ğ‘ˆğ‘+)/ğœ)+Ãğ‘’ğ‘¥ğ‘(ğ‘ ğœƒ(ğ‘ˆğ‘,ğ‘ˆğ‘âˆ’)/ğœ)\n(13)\nhereğ‘ ğœƒ is a score function for evaluating the similarity of inputs.symbol\n+means corresponding positive sample and âˆ’is negative samples\nthat using in-batch negative sampling strategy.\nThe final loss of user-user contrastive loss is average of ğ‘ˆğ‘ to\nğ‘ˆğ‘ and ğ‘ˆğ‘ to ğ‘ˆğ‘.\nğ¿ğ‘ˆğ‘ˆ =\nğ¿ğ‘ˆğ‘ˆğ‘ğ‘ +ğ¿ğ‘ˆğ‘ˆğ‘ğ‘\n2 (14)\nPOI-POI POI-POI contrastive learning considers the POIâ€™s food\ndelivery business text descriptionğ‘ƒğ‘‘ (such as name, location, menu)\nand the POIâ€™s basic information text description ğ‘ƒğ‘ (such as shop\nnormal introduction and some tags). Descriptions of the same POI\nare positive sample pairs, while those of different POIs form nega-\ntive sample pairs. Similarly, the sub loss ğ¿ğ‘ƒğ‘ƒ\nğ‘‘ğ‘ POI-POI contrastive\nloss is as follows:\nğ¿ğ‘ƒğ‘ƒ\nğ‘‘ğ‘ = âˆ’ 1\nğ‘ğ‘\nğ‘ğ‘âˆ‘ï¸\nğ‘–=1\nğ‘’ğ‘¥ğ‘(ğ‘ ğœƒ(ğ‘ƒğ‘‘,ğ‘ƒğ‘+)/ğœ)\nğ‘’ğ‘¥ğ‘(ğ‘ ğœƒ(ğ‘ƒğ‘‘,ğ‘ƒğ‘+)/ğœ)+Ãğ‘’ğ‘¥ğ‘(ğ‘ ğœƒ(ğ‘ƒğ‘‘,ğ‘ƒğ‘âˆ’)/ğœ)\n(15)\nThe POI-POI contrastive lossğ¿ğ‘ƒğ‘ƒ is just like whatğ¿ğ‘¢ğ‘¢ organizes:\nğ¿ğ‘ƒğ‘ƒ =\nğ¿ğ‘ƒğ‘ƒ\nğ‘‘ğ‘ +ğ¿ğ‘ƒğ‘ƒ\nğ‘ğ‘‘\n2 (16)\nUSER-POI The user-POI pairs are based on userâ€™s comprehensive\ntext description ğ‘ˆ (including user profile and user history texts):\nğ‘ˆ = [ğ‘ˆğ‘,ğ‘ˆğ‘] (17)\nThe POIâ€™s comprehensive text description ğ‘ƒ (including food\ndelivery business text and basic information text):\nğ‘ƒ = [ğ‘ƒğ‘‘,ğ‘ƒğ‘] (18)\nIf a user paid for delivery in a POI, the comprehensive text\ndescriptions of that user and POI constitute a positive sample pair;\nRecSys â€™24, October 14â€“18, 2024, Bari, Italy Zhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, and Wei Lin\notherwise, they are a negative sample pair.\nğ¿ğ‘ˆğ‘ƒ\nğ‘¢ğ‘ = âˆ’ 1\nğ‘ğ‘ ğ‘’ğ‘\nğ‘ğ‘ ğ‘’ğ‘âˆ‘ï¸\nğ‘–=1\nğ‘’ğ‘¥ğ‘(ğ‘ ğœƒ(ğ‘ˆ,ğ‘ƒ +)/ğœ)\nğ‘’ğ‘¥ğ‘(ğ‘ ğœƒ(ğ‘ˆ,ğ‘ƒ +)/ğœ)+Ãğ‘’ğ‘¥ğ‘(ğ‘ ğœƒ(ğ‘ˆ,ğ‘ƒ âˆ’)/ğœ)\n(19)\nTo make it symmetric, ğ¿ğ‘ˆğ‘ƒ is arithmetic mean of ğ¿ğ‘ˆğ‘ƒğ‘¢ğ‘ and ğ¿ğ‘ˆğ‘ƒğ‘ğ‘¢ .\nğ¿ğ‘ˆğ‘ƒ =\nğ¿ğ‘ˆğ‘ƒğ‘¢ğ‘ +ğ¿ğ‘ˆğ‘ƒğ‘ğ‘¢\n2 (20)\nThrough the design of these three types of sample pairs, model\ncan not only deeply understand the personalized characteristics\nof usersâ€™ but also the delivery information of POIsâ€™ on user-user\nand POI-POI contrastive learning respectively. As ğ‘ˆğ‘ records each\ndelivery order and corresponding environment, LLM could pre-\ncisely align user demands with POI offerings in complex and ever-\nchanging environments.\nğ¿2 = ğœ†1 âˆ—ğ¿ğ‘ˆğ‘ˆ +ğœ†2 âˆ—ğ¿ğ‘ƒğ‘ƒ +ğœ†3 âˆ—ğ¿ğ‘ˆğ‘ƒ (21)\nğœ†here is for scaling the separate loss.\nIn inference, features that have appeared for users and POIs are\nsequentially fed into the LLM to infer and produce the correspond-\ning description vectors from the feature level. The reason for not\nproducing LLM vectors at the text level (aggregation of features) is\ndue to the consideration that some features may change during the\nserving stage, such as weather, time slots, statistical features, etc.\nThe cost of exhaustively generating all possible feature combina-\ntion texts is exponential. Additionally, the userâ€™s long interaction\nsequence is also used to produce interaction sequence vectors.\n3.3 Information Alignment in RS\nIn this section, contrastive learning is applied to alignment, maxi-\nmizing the mutual information between the semantic information\nof the food delivery scene understood by LLM and the collaborative\nsignals extracted from ID tokens, cross features, and statistical fea-\ntures by the RS model. The alignment not only extracts the shared\ninformation between semantic and collaborative signals but also re-\nduces the noise from both, significantly enhancing the performance\nof the recommendation.\nSpecifically, our RS utilizes real-time scene embedding produced\nby the LLM after continual pretraining in stage 1 and fine-tuning\nin stage 2. Throughout this process, all parameters of the LLM\nare frozen, meaning the LLM acts like an encoder, converting a\nscene description into continuous vector representations to aid the\ntraining of the recommendation model. Since we want the model\nto be industry-friendly, the LLM should avoid participating in real-\ntime inference as much as possible. We selectğ‘Ÿ real-time scene text\nfeatures and decide to deal with scene text ğ‘ ğ‘– one by one, storing\nthem in advance for the recommendation model to utilize, where\n0 â‰¤ğ‘– â‰¤ğ‘Ÿ.\nhğ‘– = ğ¿ğ¿ğ‘€ğœƒ(ğ‘ ğ‘–)ğ‘ ğ‘ğ‘’ğ‘œğ‘ \nğ‘– (22)\nHere ğ‘ ğ‘ğ‘’ğ‘œğ‘ \nğ‘– represents the position of corresponding key real-time\nscene featureâ€™s end special token. We use the embedding of end\nspecial token in last hidden layer to represent real-time scene text.\nIn reality, we selected 10 real-time scene text, that is, ğ‘Ÿ is set to 10.\nWe employ a bidirectional transformer encoder ğœ‰ to tackle the\nproblems that discrete real-time embeddings lack interaction. Since\nthe LLMâ€™s outputs have already implicitly encoded the semantic\nassociations between different scene features, so interactions pro-\nvided by ğœ‰ is necessary.\nThese semantic embeddings produced by LLM are stacked into\na sequence and fed into ğœ‰ for further aggregation and processing.\nTo more effectively aggregate scene information, we introduced a\ntrick at the beginning of the input sequence, a trainable aggregation\ntoken < ğ‘ğ‘”ğ‘” >, corresponding to a vector agg that has the same\ndimension with real-time scene embedding. This special token plays\na crucial role in aggregating all scene keyword and is trained within\nthe transformer encoder to capture and integrate key information\nfrom different scene embedding. After processing of transformer\nencoder ğœ‰, pooling method and a projection head (a normal MLP) is\napplied to the output embedding sequence, the result is denoted as\neğ‘ , the real-time scene embedding with scene semantic information.\neğ‘  = ğ‘€ğ¿ğ‘ƒ(ğ‘ğ‘œğ‘œğ‘™ğ‘–ğ‘›ğ‘”(ğœ‰([agg,h0,h1...hğ‘Ÿ]))) (23)\nHere ğ‘€ğ¿ğ‘ƒ projects the semantic embedding into the vector space\nof recommendation model and aligns the dimensions of pooling\nresult and alignment objective.ğ‘Ÿrepresents the number of real-time\nscene texts, as mentioned above.\nThen we align eğ‘  with target embedding eğ‘¡ produced by rec-\nommendation model. eğ‘¡ is the concatenation of user embedding\nand POI embedding in recommendation system. The procedure of\nalignment could be formulated as:\nğ¿ğ‘ ğ‘¡\nğ‘ğ‘™ = âˆ’1\nğ‘\nğ‘âˆ‘ï¸\nğ‘–=1\nğ‘’ğ‘¥ğ‘(ğ‘ (eğ‘ ,eğ‘¡+)/ğœ)\nğ‘’ğ‘¥ğ‘(ğ‘ (eğ‘ ,eğ‘¡+)/ğœ)+Ãğ‘’ğ‘¥ğ‘(ğ‘ (eğ‘ ,eğ‘¡âˆ’)/ğœ) (24)\nHere ğ‘  is the similarity scoring function. Considering the different\nsymmetry properties of ğ‘ , ğ‘ is batch size. total loss is the sum of\ntwo symmetric sub losses.\nğ¿ğ‘ğ‘™ =\nğ¿ğ‘ ğ‘¡\nğ‘ğ‘™ +ğ¿ğ‘¡ğ‘ \nğ‘ğ‘™\n2 (25)\nThis design not only improves the quality of both representations\nbut also enables the model to more accurately grasp the connection\nbetween user needs and real-time scenes, leading to a significant\nimprovement in recommendation results. With this approach, our\nrecommendation system can provide more personalized and precise\nrecommendations in real-time scenes.\nDuring training, the loss is a weighted sum of the recommen-\ndation CTR loss ğ¿ğ‘ğ‘¡ğ‘Ÿ and the contrastive learning loss ğ¿ğ‘ğ‘™. ğ¿ğ‘ğ‘¡ğ‘Ÿ is\nBCE which is widely-used.\nğ¿3 = ğ›½1 âˆ—ğ¿ğ‘ğ‘¡ğ‘Ÿ +ğ›½2 âˆ—ğ¿ğ‘ğ‘™ (26)\n4 EXPERIMENT\n4.1 Experiment Setup\nDataset. Our evaluation is conducted on a real-world dataset from\nMeituan Waimai. For the Click-Through Rate (CTR) prediction task,\nas shown in Table 1, we perform negative sampling on the log data\ncollected from April 1st to April 7th, 2024, yielding approximately\n3.5 billion training samples. For the test set, we uniformly sam-\nple the data from the subsequent day, April 8th, 2024, resulting\nin 84 million samples. Specifically, the input traditional features\ninclude user statistical features, real-time contextual features, user-\nPOI cross-statistical features, POI statistical features, and the label,\nLARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding RecSys â€™24, October 14â€“18, 2024, Bari, Italy\namong others. Semantic features comprise descriptions of user\nfoundational information, POI foundational information, as well as\nstatistical descriptions of both user and POI.\nTable 1: Statistics of the dataset.\nField Size\n#Users 0.24 billion\n#PoIs 4.37 million\n#Records 3.5 billion\nDuring the continual pretraining phase, we select information\nfrom 4 million POIs to construct approximately 1B training corpora,\nwhich is then mixed with 9B Chinese corpora from Wudao [ 35],\nforming a total of 10 billion pre-training textual corpora. In the\nfine-tuning phase, we extract 1 million samples from the historical\nbehaviors of 2 million users for model fine-tuning. Additionally, all\nuser and POI information data has been anonymized during the\ntraining of the LLM.\nBaseline. To evaluate the performance of our experiments, we\nhave selected a variety of classic experimental baselines, encom-\npassing both traditional and semantic models. Specifically:\nâ€¢Wide&Deep [5] is a machine learning model that integrates\nlinear components (Wide) for handling sparse input features\nwith deep neural networks (Deep) to learn feature interac-\ntions. It aims to leverage both memorization and generaliza-\ntion and is widely used in recommendation systems.\nâ€¢DeepFM [12] is a recommendation model that combines\nFactorization Machines (FM) with deep neural networks,\ndesigned to retain the advantages of FM while automatically\nlearning high-order feature interactions.\nâ€¢P5 [11] is a model based on the language model T5 that\ntransforms the recommendation task into a text generation\ntask. Additionally, given that P5 is a generative model, we\nfine-tuned it using the food delivery dataset to fulfill tasks\nsuch as CTR prediction.\nâ€¢PLE [29] is a multi-task model based on the Mixed Model of\nExperts (MMOE) structure, which also serves as the baseline\nmodel in our model.\nâ€¢LARR is our proposed model, opts for the PLE model as\nthe foundation for the traditional model component, and\nBaichuan2-7B [3] for the semantic model part, with the aim\nof fully understanding the semantic information of real-time\nscenes and effectively integrating multimodal features to\nenhance recommendation results.\nParameter Setting To evaluate performance, we utilize Click-\nThrough Rate (CTR AUC) and Conversion Rate (CTCVR AUC)\nas evaluation metrics, along with Group AUC (GAUC) for assess-\nment.Our backbone LLM utilizes the open-sourced Baichuan2-7B\n[3]. As detailed in Section 3, the backbone LLM first constructs a\ndescriptive language corpus for all POIs on Meituan, upon which it\nperforms continual pretraining. The epoch is set to 1, with a learn-\ning rate of 1e-4. The continual pretraining runs for approximately\n96 hours on 32 A100-80G GPUs. Upon completion of continual\npretraining, contrastive learning is conducted from three dimen-\nsions: \"user-user, \" \"POI-POI, \" and \"user-POI\" to construct the vector\nmodel, encoding scene features into 128-dimensional vectors and\nstacking them into sequences. A 1-layer transformer encoder is\nemployed to aggregate the encoded scene vectors, producing real-\ntime scene information. During the alignment phase, an MLP is\nused as the projection head to project real-time scene information\ninto the vector space of the alignment target. The projected vectors\nand alignment targets undergo contrastive learning with Info-NCE\nloss, treating vectors from the same sample as positive and those\nfrom different samples as negative. The temperature is set to 0.1,\nand the loss balancing coefficient is set to 0.05. The projected vec-\ntors and alignment targets, after concatenation, are used for CTR\nand CTCVR prediction. For fair comparison across all methods, we\nchose the following parameter configurations: a batch size of 2400,\ntraining for one epoch, with a learning rate of 8e-4. The tests run on\n8 A100 GPUs, and for other parameters, we follow the best results\nfrom the original papers or source codes.\n4.2 Performance Comparison\nIn this section, we provide a detailed comparison of the perfor-\nmance of our model, LARR, against various baselines. As shown in\nTable 2, an analysis of the experimental results reveals the following\nfindings:\nFirstly, we observe that traditional recommendation system mod-\nels, such as Wide&Deep and DeepFM, generally underperform\nmore advanced deep learning models like PLE. This phenomenon\nsuggests that complex deep network structures have significant\nadvantages in learning feature interactions and capturing complex\nuser interest patterns. By introducing deeper and more intricate\ninteractions, these networks can more finely mine the underly-\ning motivations behind user behavior, thereby achieving better\naccuracy in recommendations.\nSecondly, our experimental results indicate that the semantic\nmodel P5 does not perform well in the food delivery recommen-\ndation task. This may suggest that in recommendation systems,\ntraditional statistical features can sometimes more directly reflect\nthe actual interests of users compared to semantic features. Al-\nthough semantic models can provide rich contextual information,\nstatistical analysis of user historical behavior may more directly\nand effectively reveal user preferences in practical recommendation\nscenes.\nFinally, our model, LARR, outperforms all baseline models. Com-\npared to the PLE model, LARR achieves a significant 0.58% improve-\nment in CTR AUC and also gains an 0.34% increase in CTCVR AUC.\nThese results not only confirm the superiority of our model but also\nemphasize the effectiveness of LARR in integrating the semantic\ninformation of real-time scenes with recommendation modality\ninformation. Through this integration, LARR can capture usersâ€™\nimmediate needs and potential interests more accurately, signifi-\ncantly enhancing the overall performance and user satisfaction of\nthe recommendation.\n4.3 Ablation Study\nWe introduce a novel multimodal information fusion framework,\nLARR. Initially, based on Food Delivery recommendation data, we\nperform continual pretraining and fine-tuning on LLMs to infuse\nRecSys â€™24, October 14â€“18, 2024, Bari, Italy Zhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, and Wei Lin\nTable 2: Performance comparison between our LARR and baselines\nModel CTR AUC CTCVR AUC CTR GAUC CTCVR GAUC\nTraditional Feature Learning Models Wide&Deep 0.7912 0.8894 0.7011 0.6869\nDeepFM 0.7918 0.8896 0.7009 0.6865\nContext-Aware Recommendation Models\nP5 0.7439 0.8363 0.6422 0.6318\nPLE 0.7983 0.8947 0.7088 0.6939\nLARR 0.8030 0.8978 0.7107 0.6955\nTable 3: Performance comparison of LARR and different\nvariants.\nModel CTR AUC CTCVR AUC\nLARR-PLE 0.7983 0.8947\nLARR-noCPT 0.8017 0.8969\nLARR-noFT 0.7985 0.8948\nLARR-noAL 0.8007 0.8961\nLARR-Complete 0.8030 0.8978\ndomain knowledge and enhance the LLMsâ€™ understanding of real-\ntime scenes. Subsequently, in the CTR task, we effectively integrate\nthe real-time semantic information with recommendation modality\ninformation, thereby improving recommendation results. To verify\nthe effectiveness of these designs, we conducted ablation studies.\nâ€¢LARR-PLE Variant: The model variant that completely re-\nmoves the LLM module, essentially the original PLE model.\nâ€¢LARR-noCPT Variant: The model variant that omits the\ncontinual pretraining phase of the LLM.\nâ€¢LARR-noFT Variant: The model variant that excludes the\nvector fine-tuning phase of the LLM.\nâ€¢LARR-noAL Variant: The model variant that removes the\nmultimodal feature alignment stage, relying solely on a sim-\nple feature concatenation strategy.\nâ€¢LARR Complete: The complete LARR framework.\nThe results are illustrated in the figure 3. The improvement of LARR\nComplete over LARR-noCPT Variant indicates that domain-specific\ncontinual pretraining of LLMs can significantly enhance the modelâ€™s\nunderstanding of domain knowledge, thus capturing the seman-\ntic information of user and POI more accurately. The comparison\nbetween LARR Complete and LARR-noFT Variant demonstrates\nthat the base model of LLM, without vector fine-tuning, does not\nyield ideal results; however, an LLM that has undergone vector\nfine-tuning can more effectively understand the personalized fea-\ntures of users and POIs, generating more expressive feature vectors.\nThe comparison between LARR Complete and LARR-noAL Vari-\nant underscores the importance of multimodal feature alignment.\nSimple feature concatenation cannot compensate for the spatial\ndifferences between semantic features and traditional collaborative\nfeatures, whereas a carefully designed multimodal alignment mech-\nanism can achieve effective integration of features from different\nmodalities. Ultimately, LARR Complete surpasses all variants on\nall metrics, validating the effectiveness of our adopted strategies in\nenhancing recommendation quality. Overall, the success of LARR\nis attributed to its profound understanding of complex user be-\nhaviors and diverse POI features in real-time scenes, as well as its\nefficient capability to fuse multimodal information. Our research\noffers fresh perspectives on how to leverage large language models\nand contrastive learning to enhance recommendation results.\n4.4 Hyperparameter Analysis\nIn Section 4.4, our aim is to align the deep semantic information of\nfood delivery scenes as understood by LLMs with the collaborative\nsignals extracted from ID tokens, cross-features, and statistical fea-\ntures by the conventional fine-tuning models. We seek to extract\nthe shared information between semantic and collaborative data\nto enhance model performance. During the alignment phase of the\nmodel, we focus on two key hyperparameters: the temperature co-\nefficient and the negative sample sampling ratio. The temperature\ncoefficient plays a crucial role in the loss function of contrastive\nlearning, adjusting the sensitivity of the loss and influencing the\nmodelâ€™s ability to distinguish between positive and negative sam-\nples. A lower temperature coefficient increases the difficulty for the\nmodel to differentiate samples, causing it to focus more on chal-\nlenging sample pairs. Conversely, the negative sample sampling\nratio controls the ratio of negative to positive samples during the\ncontrastive learning process. In this section, we conduct experi-\nmental analysis on these two hyperparameters with respect to CTR\nAUC, with results depicted in Figure 4\n(a) Temperature Coefficient vs\nCTR AUC\n(b) Negative Sampling Ratio vs\nCTR AUC\nFigure 4: Hyperparameter Analysis\nFigure 4a illustrates the relationship between the temperature\ncoefficient and experimental performance. We find that when the\ntemperature coefficient is set to 0.1, the model achieves optimal\nperformance, indicating that at this specific setting, the model can\neffectively learn from the data and improve its discrimination abil-\nity. However, if the temperature coefficient is set too low or too\nhigh, the learning efficiency of the model is impacted, leading to\ndecreased performance. On the other hand, the experimental re-\nsults regarding the negative sample sampling ratio, as shown in\nFigure 4b, indicate that introducing negative sample sampling in\nLARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding RecSys â€™24, October 14â€“18, 2024, Bari, Italy\nour setup does not lead to the anticipated performance improve-\nment. Instead, we observe a decline in model performance after\nemploying negative sample sampling. This may be due to the fact\nthat, in certain data and tasks, increasing the number of negative\nsamples does not provide additional information but may instead\nintroduce noise or cause the model to overly focus on negative sam-\nples, thereby affecting the modelâ€™s learning efficiency with positive\nsamples.\n4.5 Case Study\nWe present a case study to further demonstrate the modelâ€™s profi-\nciency. Given that out-of-area ordering (where users place orders\nfrom locations other than their usual residence for various reasons)\nhas always been an important scenario in food delivery services,\nwe are particularly interested in whether the model can generate\ngood recommendations for out-of-area users based on relevant se-\nmantics. We selected some random users who have never been to\nGuangdong, with the scene of \"user visiting Guangdong for vaca-\ntion\", designed a simple recall experiment to observe the details of\nthe top 10 POIs in terms of recall scores. A typical result is as fol-\nlows, with each response presented as location; main dish. Sensitive\ninformation such as POI names and details have been omitted.\nTop1: Guangzhou; Rice Noodle Roll\nTop2:. Chaoshan; Stir-fried Beef Rice Noodles\nTop3:. Guangzhou; Stir-fried Beef Rice Noodles\nTop4:. Shenzhen; Seafood Claypot Congee\nTop5: Guangzhou; Cantonese Barbecue\nTop6: Guangzhou; Claypot Rice\nTop7: Guangzhou; Rice Noodle Roll\nTop8: Guangzhou; Sweet Soup\nTop9: Dongguan; Sweet Soup\nTop10: Foshan; Claypot Rice\nIn this case, Guangdong appeared the most frequently (5 times),\nwith recalled POI primarily offering dishes such as Rice Noodle\nRoll, Stir-fried Beef Rice Noodles, Cantonese Barbecue, Claypot Rice,\nand Sweet Soup. ChaoShan, Shenzhen, Dongguan, and Foshan each\nappeared once or twice. This geographical distribution reflects the\nculinary diversity and popularity of Guangzhou as the capital city\nof Guangdong Province, which aligns with our survey findings that\nthese dishes are representative of Guangdongâ€™s regional specialties.\nThe recall results cover a variety of classic Cantonese dishes,\nindicating that the model performs well in capturing local character-\nistics. The recall results show that the model can identify multiple\ndifferent types of dishes and accurately match them to the provided\nscene text, this diversity is positive for enhancing user satisfaction\nand meeting a wider range of user needs.\n4.6 Online A/B Test\nLARR demonstrated its practical value in an online A/B test within\nthe Meituan recommendation system. During the testing period\nfrom April 15th to April 21st, 2024, LARR achieved a 2.5% increase\nin CTR and a 1.2% growth in GMV (Gross Merchandise Volume)\ncompared to the current baseline model. These results confirm the\neffectiveness of LARR in understanding user needs and enhancing\nuser experience. The increase in CTR indicates a higher acceptance\nof recommended content by users, which may be attributed to\nLARRâ€™s more accurate capture of user interests, thus providing rec-\nommendations that better align with user expectations. The growth\nin GMV directly reflects the positive impact of LARR on conversion\nrates and business revenue. This online test verified the practicality\nand effectiveness of LARR within the Meituan recommendation\nsystem.\n5 CONCLUSION\nTo tackle the lack of understanding of semantic information in RS\nand the efficiency challenges in LLM-based CTR models, this paper\nproposes a novel LARR method. LARR continues to pretrain and\nfine-tune the LLM on some corpora with an enlarged vocabulary,\nusing contrastive learning to align semantic signals and collabora-\ntive signals, leverage shared information, and reduce noise. Exten-\nsive online and offline experimental results demonstrate that LARR\nachieves low latency and enhances CTR performance, offering fresh\ninsights for the practical deployment of LLM-based CTR models.\nREFERENCES\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\n(2023).\n[2] Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis,\nand Nikunj Saunshi. 2019. A theoretical analysis of contrastive unsupervised\nrepresentation learning. In 36th International Conference on Machine Learning,\nICML 2019 . International Machine Learning Society (IMLS), 9904â€“9923.\n[3] Baichuan. 2023. Baichuan 2: Open Large-scale Language Models. arXiv preprint\narXiv:2309.10305 (2023). https://arxiv.org/abs/2309.10305\n[4] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan\nHe. 2023. Tallrec: An effective and efficient tuning framework to align large\nlanguage model with recommendation. InProceedings of the 17th ACM Conference\non Recommender Systems . 1007â€“1014.\n[5] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\nHrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.\n2016. Wide & deep learning for recommender systems. In Proceedings of the 1st\nworkshop on deep learning for recommender systems . 7â€“10.\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.\nJournal of Machine Learning Research 24, 240 (2023), 1â€“113.\n[7] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022.\nM6-rec: Generative pretrained language models are open-ended recommender\nsystems. arXiv preprint arXiv:2205.08084 (2022).\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[9] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and\nJie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive\nBlank Infilling. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) . 320â€“335.\n[10] Zhichao Feng, Junjiie Xie, Kaiyuan Li, Yu Qin, Pengfei Wang, Qianzhong Li,\nBin Yin, Xiang Li, Wei Lin, and Shangguang Wang. 2024. Context-based Fast\nRecommendation Strategy for Long User Behavior Sequence in Meituan Waimai.\narXiv preprint arXiv:2403.12566 (2024).\n[11] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.\nRecommendation as Language Processing (RLP): A Unified Pretrain, Personal-\nized Prompt & Predict Paradigm (P5). In 16th ACM Conference on Recommender\nSystems, RecSys 2022 . Association for Computing Machinery, Inc, 299â€“315.\n[12] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. [n. d.].\nDeepFM: A Factorization-Machine based Neural Network for CTR Prediction.\n([n. d.]).\n[13] Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng,\nBodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023. Large\nlanguage models as zero-shot conversational recommenders. InProceedings of the\n32nd ACM international conference on information and knowledge management .\n720â€“730.\n[14] Wenyue Hua, Lei Li, Shuyuan Xu, Li Chen, and Yongfeng Zhang. 2023. Tutorial\non large language models for recommendation. In Proceedings of the 17th ACM\nConference on Recommender Systems . 1281â€“1283.\nRecSys â€™24, October 14â€“18, 2024, Bari, Italy Zhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, and Wei Lin\n[15] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining fea-\nture importance and bilinear feature interaction for click-through rate prediction.\nIn Proceedings of the 13th ACM conference on recommender systems . 169â€“177.\n[16] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\nMohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising\nsequence-to-sequence pre-training for natural language generation, translation,\nand comprehension. arXiv preprint arXiv:1910.13461 (2019).\n[17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and large language\nmodels. In International conference on machine learning . PMLR, 19730â€“19742.\n[18] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping\nlanguage-image pre-training for unified vision-language understanding and\ngeneration. In International conference on machine learning . PMLR, 12888â€“12900.\n[19] Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. 2023. Ctrl: Connect tabular\nand language model for ctr prediction. arXiv preprint arXiv:2306.02841 (2023).\n[20] Xinhang Li, Chong Chen, Xiangyu Zhao, Yong Zhang, and Chunxiao Xing. 2023.\nE4SRec: An elegant effective efficient extensible solution of large language models\nfor sequential recommendation. arXiv preprint arXiv:2312.02443 (2023).\n[21] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and\nGuangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature in-\nteractions for recommender systems. In Proceedings of the 24th ACM SIGKDD\ninternational conference on knowledge discovery & data mining . 1754â€“1763.\n[22] Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng\nWang, Ang Wang, Le Jiang, Xianyan Jia, et al. 2021. M6: A chinese multimodal\npretrainer. arXiv preprint arXiv:2103.00823 (2021).\n[23] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018.\nModeling task relationships in multi-task learning with multi-gate mixture-of-\nexperts. In Proceedings of the 24th ACM SIGKDD international conference on\nknowledge discovery & data mining . 1930â€“1939.\n[24] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry\nTworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al .\n2022. Text and code embeddings by contrastive pre-training. arXiv preprint\narXiv:2201.10005 (2022).\n[25] Junyan Qiu, Haitao Wang, Zhaolin Hong, Yiping Yang, Qiang Liu, and Xingxing\nWang. 2023. ControlRec: Bridging the semantic gap between language model\nand personalized recommendation. arXiv preprint arXiv:2311.16441 (2023).\n[26] XiPeng QIU, TianXiang SUN, YiGe XU, YunFan SHAO, Ning DAI, and XuanJing\nHUANG. 2017. Pre-trained models for natural language processing: A survey.\nSCIENCE CHINA Information Sciences 60 (2017), 110100.\n[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text transformer. Journal of machine\nlearning research 21, 140 (2020), 1â€“67.\n[28] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei\nYin, and Chao Huang. 2023. Representation learning with large language models\nfor recommendation. arXiv preprint arXiv:2310.15950 (2023).\n[29] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive\nlayered extraction (ple): A novel multi-task learning (mtl) model for personalized\nrecommendations. In Proceedings of the 14th ACM Conference on Recommender\nSystems. 269â€“278.\n[30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[32] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models.arXiv preprint arXiv:2206.07682\n(2022).\n[33] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Jun-\nfeng Wang, Dawei Yin, and Chao Huang. 2024. Llmrec: Large language models\nwith graph augmentation for recommendation. In Proceedings of the 17th ACM\nInternational Conference on Web Search and Data Mining . 806â€“815.\n[34] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming\nZhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential\nof large language model based agents: A survey. arXiv preprint arXiv:2309.07864\n(2023).\n[35] Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu\nZou, Zhilin Yang, and Jie Tang. 2021. Wudaocorpora: A super large-scale chinese\ncorpora for pre-training language models. AI Open 2 (2021), 65â€“68.\n[36] Yichi Zhang, Guisheng Yin, and Yuxin Dong. 2023. Contrastive learning with\nfrequency-domain interest trends for sequential recommendation. In Proceedings\nof the 17th ACM Conference on Recommender Systems . 141â€“150.\n[37] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu.\n2019. ERNIE: Enhanced language representation with informative entities. arXiv\npreprint arXiv:1905.07129 (2019).\n[38] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey\nof large language models. arXiv preprint arXiv:2303.18223 (2023).\n[39] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. 2023. Col-\nlaborative large language model for recommender systems. arXiv preprint\narXiv:2311.01343 (2023).\nReceived 29 April 2024; revised 22 July 2024; accepted 19 August 2024"
}