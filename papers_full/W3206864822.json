{
    "title": "Improved Multilingual Language Model Pretraining for Social Media Text via Translation Pair Prediction",
    "url": "https://openalex.org/W3206864822",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5021358528",
            "name": "Shubhanshu Mishra",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5039365507",
            "name": "Aria Haghighi",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2987270981",
        "https://openalex.org/W2043287290",
        "https://openalex.org/W2973088264",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W4288284086",
        "https://openalex.org/W2832910678",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3152278231",
        "https://openalex.org/W3018647120",
        "https://openalex.org/W2951535825",
        "https://openalex.org/W2886198413",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2970193165",
        "https://openalex.org/W2394823018",
        "https://openalex.org/W3128030433",
        "https://openalex.org/W3032084709",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2971863715",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W2147880316",
        "https://openalex.org/W3003266841",
        "https://openalex.org/W2946328221",
        "https://openalex.org/W2916132663",
        "https://openalex.org/W2995118574",
        "https://openalex.org/W2960374072",
        "https://openalex.org/W2971207485"
    ],
    "abstract": "We evaluate a simple approach to improving zero-shot multilingual transfer of mBERT on social media corpus by adding a pretraining task called translation pair prediction (TPP), which predicts whether a pair of cross-lingual texts are a valid translation. Our approach assumes access to translations (exact or approximate) between source-target language pairs, where we fine-tune a model on source language task data and evaluate the model in the target language. In particular, we focus on language pairs where transfer learning is difficult for mBERT: those where source and target languages are different in script, vocabulary, and linguistic typology. We show improvements from TPP pretraining over mBERT alone in zero-shot transfer from English to Hindi, Arabic, and Japanese on two social media tasks: NER (a 37% average relative improvement in F1 across target languages) and sentiment classification (12% relative improvement in F1) on social media text, while also benchmarking on a non-social media task of Universal Dependency POS tagging (6.7% relative improvement in accuracy). Our results are promising given the lack of social media bitext corpus. Our code can be found at: https://github.com/twitter-research/multilingual-alignment-tpp.",
    "full_text": "Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 381–388\nNovember 11, 2021. ©2021 Association for Computational Linguistics\n381\nImproved Multilingual Language Model Pretraining for Social Media\nText via Translation Pair Prediction\nShubhanshu Mishra\nTwitter, Inc.\nsmishra@twitter.com\nAria Haghighi\nTwitter, Inc.\nahagighi@twitter.com\nAbstract\nWe evaluate a simple approach to improving\nzero-shot multilingual transfer of mBERT on\nsocial media corpus by adding a pretraining\ntask called translation pair prediction (TPP),\nwhich predicts whether a pair of cross-lingual\ntexts are a valid translation. Our approach\nassumes access to translations (exact or\napproximate) between source-target language\npairs, where we ﬁne-tune a model on source\nlanguage task data and evaluate the model in\nthe target language. In particular, we focus\non language pairs where transfer learning\nis difﬁcult for mBERT: those where source\nand target languages are different in script,\nvocabulary, and linguistic typology. We\nshow improvements from TPP pretraining\nover mBERT alone in zero-shot transfer\nfrom English to Hindi, Arabic, and Japanese\non two social media tasks: NER (a 37%\naverage relative improvement in F 1 across\ntarget languages) and sentiment classiﬁca-\ntion (12% relative improvement in F 1) on\nsocial media text, while also benchmarking\non a non-social media task of Universal\nDependency POS tagging (6.7% relative\nimprovement in accuracy). Our results are\npromising given the lack of social media bitext\ncorpus. Our code can be found at: https:\n//github.com/twitter-research/\nmultilingual-alignment-tpp.\n1 Introduction\nMultilingual BERT (mBERT; Devlin et al. 2019),\nand other multilingual pretrained language mod-\nels have been shown to learn surprisingly good\ncross-lingual representations for a range of NLP\ntasks (Pires et al., 2019; Mishra et al., 2021, 2020;\nMishra and Mishra, 2019), despite not having ex-\nplicit cross-lingual links between their monolingual\ntraining corpora (Artetxe and Schwenk, 2018; Yang\net al., 2019). Analysis in Pires et al. (2019) shows\nthat cross-lingual transfer with mBERT works best\nwhen transferring representations between source\nmBERT\nEN\nAR\nmBERT + TPP\nFigure 1: Comparing Multilingual RepresentationsLeft\nﬁgure shows mBERT embeddings (T-SNE projection) for\nparallel sentences from the Tatoeba corpus (Section 3.1) in\nEnglish (EN) and Arabic (AR). These embeddings exhibit\ndistinct language regions, making cross-lingual transfer chal-\nlenging. The right ﬁgure shows the same data after further\npretraining using translation pair prediction (Figure 2), which\nlearns representations that are near cross-lingual translations\n(change in distance showed in Figure 3 of appendix). In\nSection 3, we show this improves cross-lingual transfer per-\nformance on several tasks (see Table 2).\nand target languages that share lexical structure\n(i.e, overlapping word-piece vocabulary) and struc-\ntural linguistic typology. In some ways, this result\nis not surprising since mBERT does not see ex-\nplicit cross-lingual examples and cannot learn how\nto transform representations to a target language\nand relies on similar linguistic structure to transfer\nwell. Furthermore, limited work exists on multilin-\ngual transfer for social media corpus which tend to\nbe more noisy and shorter compared to traditional\nNLP corpora. This issue is further exacerbated for\ntransfer between language pairs which use different\nscripts.\nIn this work, we explore whether we can improve\nmultilingual transfer by providing cross-lingual ex-\namples and encouraging mBERT to learn aligned\nrepresentations between source and target texts be-\nfore task ﬁne-tuning. Concretely, we perform a\n“language pretraining\" task called translation pair\nprediction (TPP) where we assume access to a cor-\npus of source-target text pairs and predict whether\nthe pair is a translation of the same content (see\nSection 2). This pretraining phase is intended to\n382\noccur after standard mBERT pretraining (Devlin\net al., 2019), but before task ﬁne-tuning (see Fig-\nure 2). The intent behind this task is to leverage\ntranslation data that exists between a source lan-\nguage with abundant task training data and a target\nlanguage with little or no task training data in or-\nder to improve transfer learning. As with standard\nmBERT pretraining, one can pretrain mBERT us-\ning TPP once and ﬁne-tune for multiple tasks (as\nlong as transferring to the same target language set).\nThe translation pair data doesn’t need to be related\nto the downstream task and we can leverage multi-\nple sources of source-target translations of varying\ndegrees of translation quality (see Section 3.3).\nWe demonstrate the beneﬁts of adding TPP pre-\ntraining in experiments on social media corpus for\nNER, and sentiment detection, along with a non-\nsocial media corpus of universal POS tagging (for\ncomparison) on ﬁne-tuned models from English\nto Hindi, Japanese, and Arabic (see Section 3 for\nevaluation). We show gains from TPP on all tasks\n(averaged across target languages) using translation\ndata of varying quality (see Section 3.3).\nRelated works Gururangan et al. (2020) argues\nfor the beneﬁts of continued transformer pretrain-\ning for better performance on in-domain tasks.\nOur work closely follows this approach however\ninstead of keeping the Masked Language Mod-\nelling (MLM) task on in-domain data, we intro-\nduce a new pretraining task ( TPP ). In Artetxe\nand Schwenk (2019) the authors train an BiLSTM\nencoder-decoder model on a large parallel corpora\nof multiple languages and demonstrate zero-shot\ntransfer. In Eisenschlos et al. (2019) the authors\nuse a domain/task speciﬁc ﬁne-tuning to improve\nzero shot cross lingual performance by inducing\nlabels from the model of Artetxe and Schwenk\n(2019). Our work is closely related to the work of\nWieting et al. (2019); Guo et al. (2018) which use\na subword averaging and a LSTM encoder. Our\napproach uses transformers and also focuses eval-\nuation on individual token level task like NER as\nwell as sentence level task. Another closely related\nwork is of Cao et al. (2020) which uses word pair\nalignment with mBERT and evaluates on XNLI\ntask. However, their work does not consider ty-\npologically different languages like this work. In\nLample and Conneau (2019) the authors use Trans-\nlation Language Modeling task which concatenate\nthe translation pairs in the BERT model and also\nadd a language identiﬁer to each token. Huang et al.\nFigure 2: Translation Pair Prediction (TPP)We further pre-\ntrain an mBERT model on the TPP task (see Section 2). For\neach batch of aligned source-target text translation pairs, we\ncreate a balanced binary dataset consisting of the aligned pairs\n(positive examples) and a random source-text within the batch\n(negative examples). We update the mBERT model based on\noptimizing for binary cross-entropy on this task. In Section 3,\nwe show this added TPP phase improves transfer learning to\nthe target language on downstream tasks.\n(2019) include a similar approach as discussed be-\nfore and get improvement from jointly ﬁne-tuning\nmodel with translations of downstream task data.\nOur approach is comparatively simpler to imple-\nment as we don’t make any assumption about the\nencoding model or assumptions about word-level\nalignments; our approach works independently of\nthe choice of encoder (mBERT here) or the quality\nof word-alignment models.\n2 Model\nA description of our approach is provided in Figure\n2. The primary exploration in this work is the im-\npact of adding a translation pair prediction (TPP)\npretraining phase between traditional mBERT pre-\ntraining and downstream task ﬁne-tuning (Devlin\net al., 2019). The TPP pretraining task assumes\na collection of aligned text pairs (s,t) which are\n“translation\" pairs between source and target lan-\nguage text pairs sand trespectively. It is assumed\nthat both languages have word-piece coverage in\nthe pretrained mBERT model, but that we primarily\nhave task training data for the source language. Af-\nter TPP pretraining, we can ﬁne-tune the resulting\nmodel for multiple tasks as with standard mBERT\nfor transfer to the target language(s).\nThe TPP task is a binary classiﬁcation problem\nwhere given an (s,t) pair we predict whether it is\na valid “translation\" pair or a random source-target\npair using the dot product of mBERT encodings\nfor s and t.1 More formally, if f(·) represents\nthe encoder mapping a document to the mBERT\n1Note that in contrast to some related work (Huang et al.,\n2019; Lample and Conneau, 2019) we explore a symmetric\ntask formulation instead of an asymmetric formulation as in\nthe Next Sentence Prediction (NSP) pretraining task where\n383\n[CLS] token embedding, we predict if the pair is\na translation pair with probability:\nis-translation(s,t) = σ\n(\nf(s)T f(t)\n)\nwhere σ(·) is the sigmoid activation function. We\ncreate a binary dataset from aligned pairs by sam-\npling a random unaligned sand tpair as a negative\nexample for each aligned pair, creating a balanced\ndataset. The TPP task is trained using binary cross-\nentropy against this label and the loss gradient back-\npropogates through the embedding function f(·)\nwhich include mBERT layer parameters.\nAfter TPP pretraining, we ﬁne-tune on task data\nin the source language (English in our experiments)\nand evaluate transfer performance on target lan-\nguages (Hindi, Japanese, and Arabic in our experi-\nments).\n3 Evaluation\n3.1 Experimental setup\nIn order to evaluate our model we focus on three\nlanguages which are dissimilar in vocabulary and\nsyntax from English. Furthermore, our goal is\nto improve performance on social media data\nwhich is noisier as well as low resource for the\nabove languages and our benchmarked tasks\ncompared to newswire corpus. Based on these\ncriteria we utilize three languages apart from\nEnglish which meet the following requirements: a)\navailability of parallel translation corpora, and b)\navailability of task data, particularly in the social\nmedia domain (NER and sentiment detection here).\nBased on the above criteria we identify English\nas our source language and Hindi, Japanese, and\nArabic as target languages. We also include POS\ntagging on Universal Dependencies, which is a\nnon social media benchmark corpus to evaluate\nour models. More details can be found at: https:\n//github.com/twitter-research/\nmultilingual-alignment-tpp.\nLang pair Tatoeba Wikimatrix Wikidata\nen-ar 28K 773K 1.6M\nen-ja 220K 480K 509K\nen-hi 11K 134K 77K\nTable 1: Translation pair corpus data sizes used for translation\npair prediction pretraining. See Section 3.1 for details.\nsource and target representations are concatenated before pre-\ndiction.\nTranslation pair data For translation pairs we\nutilize datasets from the Tatoeba (TT) 2 collec-\ntion as well as the Wikimatrix (WM) collection\n(Schwenk et al., 2019). While Tatoeba consists of\nhuman generated short sentences, Wikimatrix con-\ntains aligned sentences mined using a neural net-\nwork. We assume that Tatoeba has higher quality\nacross languages compared to Wikimatrix, where\nmany of the translations are either incorrect or not\nexact translations. We also introduce a new dataset\ncalled Wikidata aligned pairs (WD) which can be\nautomatically generated by using pair of language\nlabels and descriptions from Wikidata which are\nwritten by human annotators in different scripts.\nTable 1 has details on the dataset size.\nWikidata aligned pairs (WD) We collect\ndataset from Wikidata ( https://wikidata.\norg). For a given language pair, we consider wiki-\ndata items which have wikidata item labels and\ndescriptions in the given pair of languages. We\nconcatenate the label and the description using a\nspace to make a sentence and use the sentences for\neach language as a translation pair.\nNER data For our downstream task we use\nTweet datasets. For NER we generate a dataset\nof 100k English Tweets, which is used for train-\ning. It consists of Tweets annotated for PERSON,\nLOCATION, PRODUCT, ORGANIZATION, and\nOTHER. We also generate test datasets of 2.3k\nJapanese and 10k Arabic Tweets. All datasets are\ngenerated using crowd sourced annotations. For\nHindi, we use the Hindi subset of 2008 SSEA\nshared task (Rajeev Sangal and Singh, 2008). We\nreport micro avg-F1 score across all classes. We\nuse the standard NER task formulation for BERT\nstyle models which use softmax on the ﬁrst sub-\nword for each token to make the prediction (Devlin\net al., 2019). The NER models can be further im-\nproved by using Conditional Random Fields (CRF)\n(Lafferty et al., 2001) layer on the output which\noften gives good performance even without neural\nfeatures across all languages (Mishra, 2020).\nSentiment Detection data We utilize the Se-\nmEval Twitter Sentiment dataset (Rosenthal et al.,\n2017) with the data split as used in Mishra and\nDiesner (2018). For Japanese, we utilize the 500k\nTweet dataset from Suzuki (2019). For Arabic we\nuse the data from Abdulla et al. (2013). For Hindi,\n2https://tatoeba.org data is under CC-BY 2.0\nFR.\n384\nwe use the dataset from SAIL 2015 shared task\nfor Indian languages (Patra et al., 2015). We re-\nport macro F1 across classes as in Rosenthal et al.\n(2017).\nUniversal Dependencies POS dataWe use the\nEnglish, Hindi, Japanese, and Arabic subsets from\nthe Universal dependencies data (Nivre et al., 2020).\nWe train using the GUM + PUD subset for En-\nglish and evaluate on PUD subset for other lan-\nguages. We do note a shortcoming of our PUD\nbased datasets, i.e. they were created by translating\nthe original text from English, German, French,\nItalian, or Spanish. 3 This can introduce transla-\ntion artifacts and can favor translation based pre-\ntraining approaches.\nTraining All models are trained using the\nmBERT model available from the HuggingFace\nlibrary (Wolf et al., 2019). For each translation pair\ndata the model is trained for 3 epochs (hyperparam-\neter details in appendix section A). For ﬁne-tuning\non downstream task we initialize our model with\nweights from the pretrained models and train it on\nthe English task dataset for 3 epochs.\nTPP using all target languagesWe also experi-\nment with a TPP model which was pretrained using\nall the target language pair data from Tatoeba which\nwere equally sampled. We denote this setting by\nALL in Table 2 for our results. Even though this\nmodel may not perform as well as pretraining with\nTPP for a single target language at a time, it allows\nus to reduce pretraining and inference time, as well\nas reduce the number of models to manage.\nChange in embedding distance after alignment\nIn Figure 3 we show how the normalized (by max)\nembedding distances change beetween sentence\npairs shown in Figure 1 after applyingTPP.We ﬁnd\nthat alignment pairs are closer than their distance\nin mBERT.\n3.2 Zero-shot improvement via TPP\nAs can be seen in Table 2, adding TPP pretraining\nimproves performance for both NER and sentiment\ndetection across all languages. The universal POS\ngains are consistent, but more modest; we hypothe-\nsize this is because POS-tagging reilies more heav-\nily on the language-neutral components of mBERT\nrepresentations (Libovický et al., 2019). The great-\nest improvement of our models is for Japanese NER\n3Details on UD PUD datasets - https://github.\ncom/UniversalDependencies/UD_English-PUD\nHindi Japanese Arabic\nNER F1 ∆% F1 ∆% F1 ∆%\nmBERT 21.1 0.0 16.5 0.0 32.1 0.0\n+TPP (ONE) 24.3 15.2 29.9 81.4 39.4 22.8\n+TPP (ALL) 23.2 10.3 27.4 66.4 38.5 19.9\nSentiment F1 ∆% F1 ∆% F1 ∆%\nmBERT 31.7 0.0 55.0 0.0 51.5 0.0\n+TPP (ONE) 32.7 3.0 66.4 20.6 58.3 13.2\n+TPP (ALL) 32.4 2.3 67.7 23.1 58.5 13.7\nUD POS acc. ∆% acc. ∆% acc. ∆%\nmBERT 67.4 0.0 52.7 0.0 64.0 0.0\n+TPP (ONE) 71.5 6.0 57.6 9.2 67.1 4.8\n+TPP (ALL) 66.4 -1.5 52.7 0.1 65.0 1.5\nTable 2: Results on zero-shot transfer learning for NER (F1),\nsentiment detection (F 1), and UD POS tagging (Accuracy)\nfrom English to Hindi, Japanese, and Arabic. We compare\nﬁne-tuning standard mBERT to ﬁne-tuning mBERT further\npretrained on the TPP task (Section 2). The ONE variant\npretrains a separate model for each target language; the ALL\nvariant is a single pretraiend model for all target languages.\nWe use ∆% to display relative improvement.\n0.0 0.2 0.4 0.6 0.8 1.0\nmBERT distance\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0mBERT + TPP distance\nFigure 3: Change in embedding distance of EN-AR translation\npairs shown in Figure 1\nfor which the model improves by 81% and an ab-\nsolute improvement of around 15% in F1 score.\nWe hypothesize that NER beneﬁts the most from\nsome our TPP sources (Wikimatrix and Wikidata)\nwhich contain many paired entity descriptions. Fur-\nthermore, the Tatoeba dataset comprises of short\nsentence which are not likely to contain sentiment\ninformation, especially sentiment bearing content\nlikely to be shared on social media platforms.\n3.3 Improvement using combinations of\nalignment pairs and quality of alignments\nIn this section we discuss the impact of translation\ndatasets available for our languages as well as how\ncombining them for theTPP task leads to improved\nperformance. We focus on NER F1 and relative\n385\nHindi Japanese Arabic\nNER F1 ∆% F1 ∆% F1 ∆%\nmBERT 21.1 0.0 16.5 0.0 32.1 0.0\n+TPP (TT) 23.1 9.6 27.8 68.6 36.3 13.2\n+TPP (WD) 22.4 6.3 26.5 60.8 36.9 15.0\n+TPP (WM) 21.6 2.6 27.7 68.3 38.3 19.3\n+TPP (BP) 24.3 15.2 29.9 81.4 39.4 22.8\n+TPP (ALL) 23.2 10.3 27.4 66.4 38.5 19.9\nSentiment F1 ∆% F1 ∆% F1 ∆%\nmBERT 31.7 0.0 55.0 0.0 51.5 0.0\n+TPP (TT) 31.8 0.3 62.4 13.5 58.3 13.2\n+TPP (WD) 30.8 -2.9 50.2 -8.7 53.0 3.0\n+TPP (WM) 32.7 3.0 63.2 14.8 54.7 6.4\n+TPP (BP) 32.0 0.9 66.4 20.6 55.3 7.5\n+TPP (ALL) 32.4 2.3 67.7 23.1 58.5 13.7\nUD POS acc. ∆% acc. ∆% acc. ∆%\nmBERT 67.4 0.0 52.7 0.0 64.0 0.0\n+TPP (TT) 65.1 -3.5 54.0 2.4 66.7 4.1\n+TPP (WD) 70.5 4.5 53.0 0.5 66.4 3.7\n+TPP (WM) 70.4 4.3 54.4 3.1 65.4 2.2\n+TPP (BP) 71.5 6.0 57.6 9.2 67.1 4.8\n+TPP (ALL) 66.4 -1.5 52.7 0.1 65.0 1.5\nTable 3: Impact of translation quality and using combination\nof translations in NER, Sentiment, and UD POS examples\ntask performance after TPP (F 1 = micro F1 score for NER,\nmacro F1 for sentiment, acc. = accuracy for UD POS). ∆% is\nthe % change in evaluation score compared to mBERT. Trans-\nlation pairs used for each TPP task are shown in parenthesis,\nTT=Tatoeba, WD=Wikidata, WM=WikiMatrix, BP=best pair-\ning of TT, WD, WM, and ALL=TT pairs from all languages,\nequally sampled.\nimprovements (denoted ∆%), but other tasks also\nsee similar variation for each language (see Table 3\nin appendix). We ﬁnd that increasing the alignment\npair data or using higher quality alignment data\nyield improvements. Using single-corpus transla-\ntion pairs alone leads to signiﬁcant improvements\nfor all languages (∆% HI=9.6, JA=68.6, AR=19.3).\nTatoeba results in most prominent improvements\n(HI=9.6, JA=68.6), while Wikimatrix has the best\nperformance for AR (19.3). We assume this is re-\nlated to larger size of the data, e.g. JA has 10 times\nmore data in Tatoeba as compared to AR and HI.\nNext, we move to sequencing pairs of TPP tasks\nfrom each translation dataset for a given language\npair. We ﬁnd that as we pair the TPP datasets the\nperformance of the models continues to improve\n(Table 3) but it depends on the previous checkpoint\nwe start from as well as the quality of the new\nTPP data. Our experiments revealed that using\nthe best performing single pairs in their order of\nperformance individually improves performance\nsigniﬁcantly (∆% HI=15.2, JA=81.4, AR=22.8).\nFor Sentiment and UD POS we observe similar\ngains, except in few case there is slight degradation.\nNote on translation qualityIn terms of transla-\ntion quality, Tatoeba is likely to be the most ac-\ncurate as it is manually curated. Next, Wikidata\nis likely to be higher quality for HI compared to\nWikimatrix as Wikimatrix is auto generated using\na model and hence likely to perform worse on low\nresource languages. For AR and JA we can expect\nWikimatrix to be higher quality as these languages\nhave larger Wikipedia size and hence likely to have\nbetter quality representation in pretrained LMs.\n4 Conclusion\nWe evaluated the use of translation pair predic-\ntion (TPP) as a pretraining task which can improve\nthe performance of multilingual language models\nfor cross-lingual zero-shot transfer from English.\nThis pretraining task is performed after standard\nmBERT pretraining and the resulting model can\nbe task ﬁne-tuned for cross-lingual transfer similar\nto mBERT. We show signiﬁcant improvement in\nzero-shot transfer by adding TPP pretraining on\nNER and sentiment detection on social media tasks\nfor three languages (Hindi, Japanese, and Arabic)\nwhich are dissimilar in vocabulary and syntax from\nEnglish.\nReferences\nN. A. Abdulla, N. A. Ahmed, M. A. Shehab, and M. Al-\nAyyoub. 2013. Arabic sentiment analysis: Lexicon-\nbased and corpus-based. In 2013 IEEE Jordan Con-\nference on Applied Electrical Engineering and Com-\nputing Technologies (AEECT), pages 1–6.\nMikel Artetxe and Holger Schwenk. 2018. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. CoRR,\nabs/1812.10464.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\n386\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nSteven Cao, Nikita Kitaev, and Dan Klein. 2020. Mul-\ntilingual alignment of contextual word representa-\ntions. In International Conference on Learning Rep-\nresentations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJulian Eisenschlos, Sebastian Ruder, Piotr Czapla,\nMarcin Kadras, Sylvain Gugger, and Jeremy\nHoward. 2019. MultiFiT: Efﬁcient multi-lingual lan-\nguage model ﬁne-tuning. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5702–5707, Hong Kong,\nChina. Association for Computational Linguistics.\nMandy Guo, Qinlan Shen, Yinfei Yang, Heming\nGe, Daniel Cer, Gustavo Hernandez Abrego, Keith\nStevens, Noah Constant, Yun-Hsuan Sung, Brian\nStrope, and Ray Kurzweil. 2018. Effective parallel\ncorpus mining using bilingual sentence embeddings.\nIn Proceedings of the Third Conference on Machine\nTranslation: Research Papers, pages 165–176, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342–8360, Online. Association for Computational\nLinguistics.\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,\nLinjun Shou, Daxin Jiang, and Ming Zhou. 2019.\nUnicoder: A universal language encoder by pre-\ntraining with multiple cross-lingual tasks. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2485–2494,\nHong Kong, China. Association for Computational\nLinguistics.\nJohn D. Lafferty, Andrew McCallum, and Fernando\nC. N. Pereira. 2001. Conditional random ﬁelds:\nProbabilistic models for segmenting and labeling se-\nquence data. In Proceedings of the Eighteenth Inter-\nnational Conference on Machine Learning, ICML\n’01, page 282–289, San Francisco, CA, USA. Mor-\ngan Kaufmann Publishers Inc.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. Advances in\nNeural Information Processing Systems (NeurIPS).\nJindrich Libovický, Rudolf Rosa, and Alexander Fraser.\n2019. How language-neutral is multilingual bert?\nCoRR, abs/1911.03310.\nShubhanshu Mishra. 2020. Non-neural Structured Pre-\ndiction for Event Detection from News in Indian\nLanguages. In Working Notes of FIRE 2020 - Fo-\nrum for Information Retrieval Evaluation, Hyder-\nabad, India. CEUR Workshop Proceedings, CEUR-\nWS.org.\nShubhanshu Mishra and Jana Diesner. 2018. Detect-\ning the correlation between sentiment and user-level\nas well as text-level meta-data from benchmark cor-\npora. In Proceedings of the 29th on Hypertext and\nSocial Media, HT ’18, page 2–10, New York, NY ,\nUSA. Association for Computing Machinery.\nShubhanshu Mishra and Sudhanshu Mishra. 2019. 3Id-\niots at HASOC 2019: Fine-tuning Transformer Neu-\nral Networks for Hate Speech Identiﬁcation in Indo-\nEuropean Languages. In Proceedings of the 11th an-\nnual meeting of the Forum for Information Retrieval\nEvaluation, pages 208–213, Kolkata, India.\nSudhanshu Mishra, Shivangi Prasad, and Shubhanshu\nMishra. 2020. Multilingual joint ﬁne-tuning of\ntransformer models for identifying trolling, aggres-\nsion and cyberbullying at TRAC 2020. In Proceed-\nings of the Second Workshop on Trolling, Aggres-\nsion and Cyberbullying, pages 120–125, Marseille,\nFrance. European Language Resources Association\n(ELRA).\nSudhanshu Mishra, Shivangi Prasad, and Shubhanshu\nMishra. 2021. Exploring multi-task multi-lingual\nlearning of transformer models for hate speech and\noffensive speech identiﬁcation in social media. SN\nComputer Science.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji ˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection.\nIn Proceedings of the 12th Language Resources\nand Evaluation Conference, pages 4034–4043, Mar-\nseille, France. European Language Resources Asso-\nciation.\nBraja Gopal Patra, Dipankar Das, Amitava Das, and Ra-\njendra Prasath. 2015. Shared task on sentiment anal-\nysis in indian languages (sail) tweets - an overview.\nIn Mining Intelligence and Knowledge Exploration,\npages 650–655, Cham. Springer International Pub-\nlishing.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\n387\nDipti Misra Sharma Rajeev Sangal and Anil Kumar\nSingh, editors. 2008. Proceedings of the IJCNLP-\n08 Workshop on Named Entity Recognition for South\nand South East Asian Languages. Asian Federation\nof Natural Language Processing, Hyderabad, India.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 task 4: Sentiment analysis in Twit-\nter. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017),\npages 502–518, Vancouver, Canada. Association for\nComputational Linguistics.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzmán. 2019. Wiki-\nmatrix: Mining 135m parallel sentences in 1620 lan-\nguage pairs from wikipedia. ArXiv, abs/1907.05791.\nYu Suzuki. 2019. Filtering method for twitter stream-\ning data using human-in-the-loop machine learning.\nJournal of Information Processing, 27:404–410.\nJohn Wieting, Kevin Gimpel, Graham Neubig, and Tay-\nlor Berg-Kirkpatrick. 2019. Simple and effective\nparaphrastic similarity from parallel translations. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4602–\n4608, Florence, Italy. Association for Computational\nLinguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. CoRR, abs/1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. CoRR, abs/1906.08237.\n388\nA Experiment\nCode availability Our model training code\nand evaluation code will be released at: https:\n//github.com/twitter-research/\nmultilingual-alignment-tpp.\nTraining setup Our default parameters are as fol-\nlows: We use Adam optimizer with 500 warmup\nsteps=500 and weight decay of 0.01. Our batch\nsize is 16. Models were trained using 2 NVIDIA\nv100 GPUs. Training time for TPP ranged from 30\nmins to 32hrs for different settings. NER training\ntime was around 1.5 hrs. Sentiment training time\nwas 1 minute, and UD POS training time was also\n1 minute.\nEvaluation For NER tasks we used micro-\naveraged F1 score as implemented in the seqeval 4\nlibrary. For sentiment tasks we use document level\nmicro-averaged F1 score implemented in scikit-\nlearn library 5. For UD-POS tasks we used token\nlevel accuracy as implemented in scikit-learn li-\nbrary.\nB Data\nB.1 Data sources\nNER\n1. SSEA Hindi - From the Workshop on\nNER for South and South East Asian Lan-\nguages http://ltrc.iiit.ac.in/\nner-ssea-08/index.cgi?topic=5\n2. Tweets annotated for in English, Japanese,\nand Arabic with entities from the following\ntypes: PERSON, LOCATION, PRODUCT,\nORGANIZATION, and OTHER.\nSentiment For all sentiment datasets we only\nconsider the positive and negative examples and\nexclude the neutral class if present.\n1. Japanese - Available at: http://www.db.\ninfo.gifu-u.ac.jp/data/Data_\n5d832973308d57446583ed9f\n2. Arabic - Available at: https:\n//archive.ics.uci.edu/ml/\ndatasets/Twitter+Data+set+\nfor+Arabic+Sentiment+Analysis\n4https://github.com/chakki-works/\nseqeval\n5https://scikit-learn.org/\n3. Hindi - Available at: http://\namitavadas.com/SAIL/data.html"
}