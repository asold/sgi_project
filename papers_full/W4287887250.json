{
  "title": "DOCmT5: Document-Level Pretraining of Multilingual Language Models",
  "url": "https://openalex.org/W4287887250",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2271616566",
      "name": "Chia-Hsuan Lee",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2779055461",
      "name": "Aditya Siddhant",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2681729797",
      "name": "Viresh Ratnakar",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2118424058",
      "name": "Melvin Johnson",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3156665996",
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3176209443",
    "https://openalex.org/W3046531489",
    "https://openalex.org/W3175079695",
    "https://openalex.org/W2608029998",
    "https://openalex.org/W2958953787",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W3174724858",
    "https://openalex.org/W3093871477",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3035520602",
    "https://openalex.org/W3118942129",
    "https://openalex.org/W3119014050",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W2970529093",
    "https://openalex.org/W3169425228",
    "https://openalex.org/W3119637927",
    "https://openalex.org/W3118469444",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W3175301726",
    "https://openalex.org/W3152788712",
    "https://openalex.org/W3033962033",
    "https://openalex.org/W3093404841",
    "https://openalex.org/W2919290281",
    "https://openalex.org/W2962802109",
    "https://openalex.org/W3118106810",
    "https://openalex.org/W3137010024",
    "https://openalex.org/W2985509622",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3103878009",
    "https://openalex.org/W3037854022",
    "https://openalex.org/W3214173179",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W3017454464",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "In this paper, we introduce DOCmT5, a multilingual sequence-to-sequence language model pretrained with large-scale parallel documents. While previous approaches have focused on leveraging sentence-level parallel data, we try to build a general-purpose pretrained model that can understand and generate long documents. We propose a simple and effective pretraining objective - Document reordering Machine Translation (DrMT), in which the input documents that are shuffled and masked need to be translated. DrMT brings consistent improvements over strong baselines on a variety of document-level generation tasks, including over 12 BLEU points for seen-language pair document-level MT, over 7 BLEU points for unseen-language-pair document-level MT and over 3 ROUGE-1 points for seen-language pair cross-lingual summarization. We achieve state-of-the-art (SOTA) on WMT20 De-En and IWSLT15 Zh-En document translation tasks. We also conduct extensive analysis on various factors for document pretraining, including (1) the effects of pretraining data quality and (2) The effects of combining mono-lingual and cross-lingual pretraining. We plan to make our model checkpoints publicly available.",
  "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 425 - 437\nJuly 10-15, 2022 ¬©2022 Association for Computational Linguistics\nDOCmT5: Document-Level Pretraining of Multilingual Language Models\nChia-Hsuan Lee‚ô¢ Aditya Siddhant‚ô† Viresh Ratnakar‚ô† Melvin Johnson‚ô†\n‚ô¢University of Washington ‚ô†Google Research\nchiahlee@uw.edu\n{adisid,vratnakar,melvinp}@google.com\nAbstract\nIn this paper, we introduceDOCmT5, a multi-\nlingual sequence-to-sequence language model\npretrained with large scale parallel documents.\nWhile previous approaches have focused on\nleveraging sentence-level parallel data, we try\nto build a general-purpose pretrained model\nthat can understand and generate long docu-\nments. We propose a simple and effective\npretraining objective -Document reordering\nMachineTranslation (DrMT), in which the in-\nput documents that are shuffled and masked\nneed to be translated.DrMT brings consistent\nimprovements over strong baselines on a vari-\nety of document-level generation tasks, includ-\ning over 12 BLEU points for seen-language-\npair document-level MT, over 7 BLEU points\nfor unseen-language-pair document-level MT\nand over 3 ROUGE-1 points for seen-language-\npair cross-lingual summarization. We achieve\nstate-of-the-art(SOTA)onWMT20De-Enand\nIWSLT15 Zh-En document translation tasks.\nWe also conduct extensive analysis on various\nfactors for document pretraining, including (1)\ntheeffectsofpretrainingdataqualityand(2)the\neffects of combining mono-lingual and cross-\nlingualpretraining. Weplantomakeourmodel\ncheckpoints publicly available.\n1 Introduction\nMultilingualpretrainedlanguagemodelshavebeen\nuseful for a wide variety of NLP tasks. pretrain-\ning on large-scale multilingual corpora facilitates\ntransfer across languages and benefits low-resource\nlanguages.\nPreviously, sentence-level or word-level cross-\nlingualobjectiveshavebeenconsideredforpretrain-\ning large language models (LLM), but not much\nefforthasbeenputindocument-levelobjectivesfor\npretraining. Inthiswork,weproposeamultilingual\nsequence-to-sequence language model pretrained\nwith cross-lingual structure-aware document-level\nobjectives. DOCmT5is built on top of mT5 (Xue\netal.,2021)andisfurthertrainedwithparalleldoc-\numents across multiple language pairs. To encour-\nage the model to gain a deep understanding of the\ndocument structure and cross-lingual relationships,\nwe consider a challenging translation scenario as a\nsecond-stage pretraining task: the input sentences\nare shuffled in a random order and random spans\nare masked. To effectively translate the input docu-\nment, the model needs to reconstruct the document\nin the original order, making the model learn sen-\ntence relationships, and also recover the masked\nspans. This objective is effective on document-\nlevel generation tasks such as machine translation\nand cross-lingual summarization, outperforming\nprevious best systems.\nTo enable cross-lingual pretraining at a large\nscale, we created a synthetic parallel document cor-\npus. To avoid expensive human annotation, we\nuse off-the-shelf neural machine translation (NMT)\nmodels to translate the documents in the mC4 cor-\npus (Xue et al., 2021) into English. In our exper-\nimental results, this corpus is more effective for\npretraining than existing large-scale automatically\naligned corpora (e.g., CCAligned (El-Kishky et al.,\n2020)).\nWe also conduct extensive ablation studies and\nprovideinsightsondocument-levelpretraining. We\nshow that simple document-level pretraining is\nmoreusefulthansentence-levelpretrainingforgen-\nerative tasks. We also show that data quality mat-\nters when performing multilingual document pre-\ntraining. Finally, we don‚Äôt observe improvements\nfrom combining mono-lingual and cross-lingual\nobjectives when evaluating on two document-level\ntranslation tasks.\nIn summary, this paper makes the following con-\ntributions:\n‚Ä¢ We build a state-of-the-art multilingual\ndocument-level sequence-to-sequence lan-\nguagemodelpretrainedwithastructure-aware\ncross-lingual objective.\n425\nDOC mT5\n‰Ω†ÂøÖÈ†à  <MASK> \n‰Ω†ÁöÑ  <MASK> ÂéªÂëµË≠∑  \nÈÇ£‰∫õÂØ∂Ë≤¥ÁöÑÈóú‰øÇ„ÄÇ  \n \n<MASK> ÊòØÁÑ°ÂèØÂèñ‰ª£ÁöÑ„ÄÇ  \nFamily is irreplaceable.  \nYou have to devote your time\nto nurturing those precious\nrelationships.\nYour purpose in life is to find\nyour purpose. Give your\nwhole heart and soul to it.\nDale todo tu  <MASK> y\nalma.\nTu prop√≥sito en la vida\nes  <MASK> tu prop√≥sito.  \nSentence \nShuffling  \n+ \nSpan \nCorruption\nTu prop√≥sito en la vida es\nencontrar tu prop√≥sito. \nDale todo tu coraz√≥n y alma. \nÂÆ∂‰∫∫ÊòØÁÑ°ÂèØÂèñ‰ª£ÁöÑ„ÄÇ \n‰Ω†ÂøÖÈ†àÂ•âÁçª‰Ω†ÁöÑÊôÇÈñìÂéªÂëµË≠∑ \nÈÇ£‰∫õÂØ∂Ë≤¥ÁöÑÈóú‰øÇ„ÄÇ \nMultilingual  \nCommon Crawl\nDocuments\nDocument \nreordering \nMachine \nTranslation\nFigure 1: Overview of our proposedDocument-Reordering MachineTranslation (DrMT) pretraining. For each\ninput document, the sentences are shuffled in random order and then randomly selected spans will be masked. The\nprediction target of DOCmT5 is to generate the translation of the input document.\n‚Ä¢ Our proposed model achieves strong results\noncross-lingualsummarizationanddocument-\nlevel machine translation for seen and unseen\nlanguage paris, including SOTA on WMT20\nDe-En and IWSLT2015 Zh-En tasks.\n‚Ä¢ We also conduct extensive experiments to\nstudy what works and what doesn‚Äôt work in\ndocument-level multilingual pretraining.\n2 Related Work\n2.1 Multilingual Pretraining\nMultilingual pretrained models provide a set of pa-\nrameters that can be quickly finetuned for differ-\nent downstream tasks (Ruder et al., 2021). Some\npopular models are: mBERT (Devlin et al., 2019)\nand XLM-R (Conneau et al., 2020) which pretrain\nwith masked language modeling objective using\nonly monolingual data, mT5 (Xue et al., 2021) and\nmBART (Liu et al., 2020) which use a sequence-\nto-sequence language model and pretrain on large-\nscale mono-lingual corpora across many languages.\nOur proposed model uses mT5 as a backbone and\nfurther utilizes pseudo-parallel documents to learn\nbetter cross-lingual representations.\nTo capture cross-lingual information, translation\nlanguage modeling (Conneau and Lample, 2019)\nand its variants (VECO (Luo et al., 2021), ERNIE-\nM (Ouyang et al., 2021)) was proposed to leverage\nsentence-level parallel data. AMBER (Hu et al.,\n2021) use two explicit alignment objectives that\nalignrepresentationsatthewordandsentencelevel.\nHICTL (Wei et al., 2020) pretrains on parallel sen-\ntences with word and sentence-level contrastive\nlosses. mBART50 (Tang et al., 2021), mT6 (Chi\net al., 2021) and nmT5 (Kale et al., 2021) focus\non second-stage of pretraining using large-scale\nsentence-level translation data. Our model goes be-\nyond the sentence and focuses on document-level\nunderstanding.\nWhile sentence-level pretraining has received\na lot of attention, document-level pretraining has\nbeen under-studied. Unicoder (Huang et al., 2019)\nreplaces alternating sentences in a document with\ntranslations and pretrains with masked language\nmodeling. MARGE (Lewis et al., 2020) adopts the\nretriever-generator paradigm and pretrains with an\nunsupervisedtranslationobjectiveonautomatically\nretrieved documents. M2M100 (Fan et al., 2021)\npretrains sequence-to-sequence language models\non automatically mined parallel sentences and doc-\numents. Our model considers a challenging super-\nvised translation objective on parallel documents.\n2.2 Multilingual Parallel Data Sources\nOPUS-100 (Aharoni et al., 2019; Zhang et al.,\n2020a)iscollectedfromavarietyofdomainsandis\nhuman labeled but it is at the sentence level. ML50\n(Tang et al., 2021) is collected from different ma-\nchinetranslationchallengesandotherpubliclyavail-\nable corpora such as OPUS, but most of the data\nis at the sentence level. CCMatrix (Schwenk et al.,\n2021b)andWikimatrix(Schwenketal.,2021a)use\nmultilingual sentence embedding to automatically\nmine parallel sentences. Perhaps the most closest\nto our proposed corpus is CCAligned (El-Kishky\net al., 2020), which is also automatically mined but\nitsqualityisinquestion(Kreutzeretal.,2021). Our\n426\nLanguage Architecture Parameters # Languages Monolingual Data Cross-Lingual Data Parallel Docs\nmBERT Encoder-only 180M 104 Wikipedia ‚úó ‚úó\nRemBERT Encoder-only 980M 110 Wikipedia and Common Crawl ‚úó ‚úó\nXLM Encoder-only 570M 100 Wikipedia Misc. ‚úó\nXLM-R Encoder-only 270M - 550M 100 Common Crawl (CCNet) ‚úó ‚úó\nmBART Encoder-decoder 680M 25 Common Crawl (CC25) ‚úó ‚úó\nmBART50 Encoder-decoder 680M 50 Common Crawl (CC25) ML50 ‚úì\nMARGE Encoder-decoder 960M 26 Wikipedia or CC-News ‚úó ‚úó\nmT5 Encoder-decoder 300M - 13B 101 Common Crawl (mC4) ‚úó ‚úó\nnmT5 Encoder-decoder 800M - 3B 101 Common Crawl (mC4) OPUS-100 ‚úó\nDOCmT5 (ours) Encoder-decoder 580M - 800M 25 Common Crawl (mC4) MTmC4 ‚úì\nTable 1: Comparisons of DOCmT5 to previous multilingual language models.\nLanguage Size/GB Language Size/GB\nDe‚ãÜ 44 Ar 58\nEs‚ãÜ 52 Az 42\nTr‚ãÜ 45 Bn 66\nRu‚ãÜ 58 Bn 66\nVi‚ãÜ 50 Fa 54\nFi 47 Ko 87\nFr 43 Lt 48\nHi 20 Mr 125\nIt 40 Nl 38\nJa 120 Pl 45\nPt 40 Th 63\nRo 53 Uk 66\nZh 41\nTable 2: Statistics of the MTmC4 corpus.‚ãÜ indicates\nthat the language is used in DOCmT5-5.\nMTmC4corpusdoesnotrequirehumanannotation\nand instead was produced by NMT models.\n2.3 Document-level Machine Translation\nThere are different ways to incorporate document\ncontext into translation model. Just to name a few,\nprevious works have explored concatenation-based\nmethods (Tiedemann and Scherrer, 2017; Junczys-\nDowmunt, 2019; Sun et al., 2020; Lopes et al.,\n2020), multi-source context encoder (Zhang et al.,\n2018; Jean et al., 2017), and hierarchical networks\n(Zhengetal.,2020;Zhangetal.,2020b;Chenetal.,\n2020). Thislineofresearchfocusesonarchitectural\nmodifications of neural translation models. We fo-\ncus on how to design a generalized pretraining ob-\njectiveandfurthermore,ourmodelcanbefinetuned\nfor various downstream tasks (e.g. summarization)\nwithout task-specific changes.\n3 Multilingual Pretraining\n3.1 Datasets\n3.1.1 mC4\nFor pretraining, we use mC4 (Xue et al., 2021), a\nlarge scale corpus extracted from Common Crawl\nthat covers over 100 languages.\n3.1.2 MTmC4: Creating Parallel Documents\nwith mC4\nTo create large-scale parallel documents, we take\nmC4 as a starting point and use in-house NMT\nmodels to translate documents from 25 languages\ninto English. Each sentence in each document is\ntranslated independently. For each language, we\nsample 1 million documents, if there are more than\nthat to start with, in mC4. Detailed data statistics\nfor all the languages can be found in Table 2.\n3.2 Document Reordering Machine\nTranslation (DrMT)\nWe start by introducing two related pretraining ob-\njectives:\n‚Ä¢ NMT Pretraining: Tang et al. (2021) and\nKale et al. (2021) proposed to perform a\nsecond-stage of pretraining using sentence-\nlevel MT data. The objective here is to per-\nform sentence-level translation without any\nother changes to the input.\n‚Ä¢ Monolingual Document Reordering (Dr) Pre-\ntraining: Thisobjective,proposedbymBART\n(Liuetal.,2020),changestheorderofthesen-\ntencesineachdocument. Thisisthenfollowed\nbytheoriginalspancorruptionobjectiveinT5.\nThe decoder is required to generate the origi-\nnal document in order.\nWe combine these two objectives and propose\nDrMT. InDrMT, we introduce two types of noise\n427\non the input:(i) sentences in the document are ran-\ndomly shuffled and(ii) randomly sampled spans\nare masked. In order to correctly translate the con-\ntent, the model needs to decipher the corrupted\ndocument in order first. This enforces the models\nto gain deep understanding of the document struc-\nture. More formally, suppose we have N language\npairs and each language has a set of parallel doc-\numents, the whole collection of document pairs\nare ùê∑ = {ùê∑1, ùê∑2, ..., ùê∑ùëÅ }. And a pair of(ùë•, ùë¶) is\nan instance in one of the language documentsùê∑ùëñ.\nThe overall learning objective is maximizing the\nlikelihood ofùë¶ given a corruptedùê∂(ùë•), that is\n‚àë\nùê∑ùëñ‚ààùê∑\n‚àë\n(ùë•,ùë¶)‚ààùê∑ùëñ\nlog ùëÉ (ùë¶|ùê∂(ùë•)). (1)\n3.3 DOCmT5\nWe use mT5 as the backbone model. mT5 is a\nsequence-to-sequence language model pretrained\nwith the span corruption objective in which ran-\ndom spans in the input are masked and the decoder\nis required to reconstruct the masked spans (see\nRaffel et al. (2020) and Xue et al. (2021) for fur-\nther details). Our system,DOCmT5, incorporates\na second-stage pretraining with a structure-aware\ncross-lingual objective(3.2) on pseudo parallel doc-\numents. Detailed comparisons with previous mul-\ntilingual language models can be found in Table\n1. We provide two variants ofDOCmT5 with both\nBase and Large model settings:\n‚Ä¢ DOCmT5-5 This model is pretrained with 5\nlanguages: {De, Ru, Tr, Vi and Es}. For all\nof the pretraining objective baselines in this\npaper, we pretrain with this set of languages,\nunless specified otherwise.\n‚Ä¢ DOCmT5-25 This model is pretrained with\n25 languages. We show the full list of lan-\nguages and their sizes in Table 2.\n3.4 Implementation Details\nWeusemT5-Base 1 andmT5-Large2 checkpointsat\n1M steps as our pretrained models. We perform a\nsecond-stage of pretraining for an additional 0.5M\nsteps using batches of 256 examples each of max\nlength 1024. The learning rate is determined by\n1https://console.cloud.google.com/\nstorage/browser/t5-data/pretrained_\nmodels/mt5/base/\n2https://console.cloud.google.com/\nstorage/browser/t5-data/pretrained_\nmodels/mt5/large/\na inverse square root scheduler as defined in T5,\nwith the learning rate set to1‚àï\n‚àö\nùëõ where n is the\nnumber of training step. We use the same span\ncorruption objective as T5, with 15% of random\ntokens masked and an average noise span length of\n3. For finetuning, we use a constant learning rate\nof 0.001 and dropout rate of 0.1 for all tasks until\nconvergence. We adopt greedy decoding during\ninference.\n4 Experiments\n4.1 Baselines\n‚Ä¢ Second-Stage Pretraining on 5 Languages\nLanguage models pretrained with huge num-\nbers of languages suffer from curse of multi-\nlinguality. In order to make a fair comparison,\nwe create a strong mT5 model by continuing\ntopretrainonthesame5languagesofmC4as\ninDOCmT5-5with the same number of steps\nusingtheoriginalspancorruptionobjectivein\nmT5. Models pretrained with this objective is\ndenoted ascont-5langs.\n‚Ä¢ Monolingual Document Reordering (Dr)\nWe briefly mention this objective in Sec-\ntion3.2. We use the mC4 corpus for this pre-\ntraining objective. Models pretrained with\nthis objective is denoted asDr (Document\nReordering).\n‚Ä¢ Document TLM (DocTLM)\nIn Conneau and Lample (2019), the au-\nthors propose the translation language model-\ning(TLM) objective, which concatenates par-\nallel sentences and applies masked language\nmodeling to learn cross-lingual knowledge.\nHere we extend it to the document level by\nconcatenating parallel documents. Instead of\nmaskingsingletokens,wefollowthespancor-\nruption objective in T5 and mask consecutive\nspans. The models are pretrained with this\nobjective on MTmC4.\n‚Ä¢ Document NMT (DocNMT)\nWe consider a standard document-level ma-\nchine translation for pretraining. The source\ndocument is the input and the target transla-\ntion is the output. We use MTmC4 for this\npretraining objective.\n428\nPretrained Model Es-En Ru-En Tr-En Vi-En Average\nPrevious Systems\nmBART 38.30 / 15.40 / 32.4033.10 / 11.90 / 27.80 34.40 / 13.00 / 28.10 32.00 / 11.10 / 26.4034.45 / 12.85 / 28.67\nMono-Lingual\nmT5 29.97 / 10.65 / 25.70 27.91 / 8.90 / 22.60 29.98 / 11.96 / 24.56 24.38 / 7.39 / 19.59 28.06 / 9.72 / 23.11\nw. cont-5langs 34.50 / 12.83 / 28.37 30.20 / 10.30 / 24.77 32.12 / 13.71 / 26.40 28.95 / 9.74 / 23.7631.44 / 11.64 / 25.82\nw. Dr 36.22 / 14.18 / 30.31 32.29 / 11.64 / 26.63 34.25 / 14.93 / 28.50 30.07 / 10.46 / 25.00 33.20 / 12.80 / 27.61\nCross-Lingual\nw. DocNMT 33.45 / 12.56 / 29.04 30.93 / 11.01 / 25.82 33.32 / 14.10 / 27.54 27.60 / 9.26 / 22.5231.40 / 11.59 / 26.12\nw. DocTLM 35.40/ 13.76 / 29.71 30.26 / 10.33 / 24.78 34.85 / 15.35 / 28.88 30.35 / 10.86 / 25.0332.71 / 12.57 / 27.10\nDOCmT5-5 36.60 / 14.55 / 30.64 32.90 / 12.09 / 27.41 37.02 / 16.64 / 30.97 32.13 / 11.81 / 26.72 34.66 / 13.77 / 28.93\nDOCmT5-5-Large 36.34 / 14.69 / 31.14 33.15 / 12.32 / 27.80 37.11 / 16.40 / 30.6333.29 / 12.35 / 27.5034.97 / 13.94 / 29.26\nDOCmT5-25 36.42 / 14.47 / 30.51 30.99 / 10.94 / 25.78 35.99 / 16.13 / 29.67 31.71 / 11.53 / 26.4033.77 / 13.26 / 28.09\nDOCmT5-25-Large 36.79 / 15.04 / 31.4833.56 / 12.77 / 28.46 37.66 / 16.68 / 31.3732.43 / 11.87 / 27.0435.11 / 14.09 / 29.58\nTable 3: Results of four seen langauges paris {Es, Tr, Ru, Vi} on Wikilingua. Each cell demonstrates three metrics:\nROUGE-1, ROUGE-2 and ROUGE-L in order. The mBART results are taken from the GEM(Gehrmann et al.,\n2021) paper for a strong baseline model.\nPretrained Model Fr-En Id-En Hi-En Average\nMono-Lingual\nmT5 29.66 / 9.96 / 24.37 29.08 / 9.87 / 23.83 26.18 / 8.51 / 20.91 28.30 / 9.44 / 23.03\nw. cont-5langs 32.78 / 11.79 / 27.29 32.21 / 11.65 / 26.36 28.93 / 10.06 / 23.3731.30 / 11.16 / 25.67\nw. Dr 34.47 / 12.67 / 28.58 34.05 / 12.87 / 27.96 31.13 / 11.18 / 25.16 33.21 / 12.24 / 27.23\nCross-Lingual\nw. DocNMT 33.22 / 12.33 / 27.97 31.97 / 11.80 / 27.11 29.33 / 10.12 / 23.86 31.50 / 11.41 / 26.31\nw. DocTLM 32.79 / 11.75 / 27.12 33.35 / 12.24 / 27.37 30.48 / 11.24 / 24.92 32.20 / 11.74 / 26.47\nDOCmT5-5 34.02 / 12.57 / 28.21 34.31 / 13.09 / 28.56 32.24 / 11.84 / 26.06 33.52 / 12.50 / 27.61\nDOCmT5-5-Large 36.28 / 14.27 / 30.7834.52 / 13.45 / 29.22 33.15 / 12.68 / 27.3534.65 / 13.46 / 29.11\nDOCmT5-25 34.56 / 13.10 / 29.03 34.16 / 13.04 / 28.23 32.33 / 11.99 / 26.25 33.68 / 12.71 / 27.83\nDOCmT5-25-Large 35.66 / 13.99 / 30.2635.15 / 13.70 / 29.47 34.16 / 13.26 / 27.9334.99 / 13.65 / 29.22\nTable 4: Results of three unseen langauges paris {Fr, Id, Hi} on Wikilingua.\n4.2 Cross-Lingual Summarization\nWeevaluate DOCmT5 oncross-lingualsummariza-\ntion as it is challenging for the model to summa-\nrize a long document and translate the salient in-\nformation at the same time. We use Wikilingua,\na cross-lingual summarization dataset, in which a\ndocument from an arbitrary language must be sum-\nmarizedinEnglish. WeadopttheGEM(Gehrmann\net al., 2021) version where the data is re-split to\navoid train-test overlap between languages. We use\na special prefix for cross-lingual summarization:\n\"Summarize X to Y\", where X and Y are the source\nand target language names respectively.\n4.2.1 Results on Seen Language Pairs\nWe show the finetuning results of language pairs\nthat are in the second stage of pretraining in Ta-\nble 3. We use the same four languages that are\nin Wikilingua‚Äôs original release {Es, Ru, Tr, Vi}.\nTheDr objective brings substantial improvements\novercont-5langs in all four languages, justifying\nthe importance of structure-aware objectives. As\nfor cross-lingual objectives,DocTLM is better than\nDocNMT in almost all languages except for Rus-\nsian. DOCmT5-5 substantially outperformsDoc-\nNMT andDocTLM, showing that our proposed pre-\ntraining objective leads to improved cross-lingual\nlearning. The results ofDOCmT5-25 are inferior\ntoDOCmT5-5 and this is possibly due to capacity\ndilution (Arivazhagan et al., 2019). As we increase\nthe capacity, we see thatDOCmT5-25-Large out-\nperforms DOCmT5-5-Large. DOCmT5-25-Large\nis the best overall model outperforming the strong\nprior system: mBART.\n4.2.2 Results on Unseen Language Pairs\nWe show the finetuning results of language pairs\nthat are not in the second-stage of pretraining stage\n429\nin Table 4. We use three languages {Fr, Id, Hi}3.\nOnce again, we see that theDr objective brings\nsubstantial improvements overcont-5langs. Sur-\nprisingly, without directly pretraining on the same\nlanguage pairs,DOCmT5-5 leads to substantial im-\nprovements over strong baselines. This shows that\nour pretraining objectives are able to generalize to\nother languages.DOCmT5-25 pretrains on French\nandHindibutnotIndonesianandhenceweobserve\nimprovements of average results overDOCmT5-5.\nThe improvements ofDOCmT5 are not so substan-\ntial and sometimes even hurt performance in high-\nresource languages: French and Indonesian, which\nhave 44556 and 33237 training examples respec-\ntively and there are only 6942 examples in Hindi.\nDOCmT5-25-Large obtains the best results in al-\nmost all 3 languages except for French.\nPretrained Model d-BLEU\nPrevious Systems\nNTT (Kiyono et al., 2020) 43.80\nPROMT (Molchanov, 2020) 39.60\nOPPO (Shi et al., 2020) 42.20\nMono-Lingual\nmT5 29.08\nw. cont-5langs 32.24\nw. Dr 36.71\nCross-Lingual\nw. DocNMT 41.23\nw. DocTLM 37.74\nDOCmT5-5 42.19\nDOCmT5-5-Large 44.73\nDOCmT5-25 40.99\nDOCmT5-25-Large 43.49\nTable 5: Finetuning results on WMT20 De-En.\n4.3 Document-Level Machine Translation\nWe evaluateDOCmT5 on document translation.\nWe split each document into chunks with a max\nlength of 512 tokens. During inference, the de-\ncodedchunksareconcatenatedtogethertoformthe\nfinal document. We use prefix\"Translate X to Y\"\nfor translation, where X and Y are the source and\ntarget language names respectively.\n4.3.1 Seen Language Pair: WMT20 De-En\nWMT20 De-En is a document-level machine trans-\nlation task. We use parallel training data from\n3We choose French to study the transfer ability of the\ncross-lingual models on high-resource and same-script (latin)\nlanguages. Indonesian is for studying high-resource and\ndifferent-script language. Hindi is for studying low-resource\nand different-script language.\nPretrained Model d-BLEU\nPrevious Systems\nHAN 24.00\nmBART 29.60\nMARGE 28.40\nMono-Lingual\nmT5 24.24\nw. cont-5langs 24.22\nw. Dr 23.75\nCross-Lingual\nw. DocNMT 26.17\nw. DocTLM 25.87\nDOCmT5-5 28.97\nDOCmT5-5-Large 30.52\nDOCmT5-25 30.99\nDOCmT5-25-Large 31.40\nTable 6: Unseen language pair results on IWSLT\n2015 Zh-En. Chinese is in the second-stage pretrain-\ning language set ofDOCmT5-25 but not in those of\nDOCmT5-5. DOCmT5-25-Large achieves SOTA.\nWMT20withoutusingadditionalmonolingualdata.\nFrom the results in Table 54, we see thatDr pro-\nvides large gains.DocNMT outperforms DocTLM.\nThis is probably due to the fact thatDocNMT is\nmore close to the document-level translation task.\nDOCmT5-5 once again outperforms Dr and other\nstrongcross-lingualbaselines. DOCmT5-5 isbetter\nthan DOCmT5-25 again because of capacity dilu-\ntion as noted in Aharoni et al. (2019). As expected,\nDOCmT5-5-Large outperforms DOCmT5-5 and to\nthebestofourknowledge,achievestheSOTA.Note\nthat previous systems use one or more of the fol-\nlowing techniques: additional monolingual data,\nback-translation, ensembling or re-ranking tailored\nto a single translation pair.\n4.3.2 Unseen Language Pair: IWSLT 2015\nZh-En\nWe use IWSLT 2015 Zh-En, another document-\nlevel machine translation task, to examine the mul-\ntilingual transferability ofDOCmT5 when the tar-\nget transfer language (Chinese in this case) is of a\nvery different script. Chinese is only in the first-\nstagepretrainingofmT5butnotinoursecond-stage\npretraining. We use parallel training data from\nIWSLT15 without using additional monolingual\ndata. Following HAN (Werlen et al., 2018), we use\n2010-2013 TED as the test set. The results are in\n4For all the document translation experiments in this pa-\nper, the numbers are calculated using sacreBLEUhttps://\ngithub.com/mjpost/sacrebleu in document level.\n430\nTable 6.DOCmT5-5 outperforms the strong cross-\nlingual and mono-lingual baselines, demonstrat-\ning impressive transfer capability .DOCmT5-25\nincludes Chinese as one of the second-stage pre-\ntraining languages therefore obtains better num-\nbers thanDOCmT5-5. Unsurprisingly, large mod-\nels are better than their corresponding base models.\nTo the best of our knowledge,DOCmT5-25-Large\nachieves the SOTA on this task. We qualitatively\nanalyze the translations of different systems in Ap-\npendix A.\nPretrained Model De-En Ru-En Pl-En Ja-En\nmT5\nw. DocNMT 44.09 40.48 3.13 0.92\nw. DocTLM 0.31 0.11 0.23 0.22\nDOCmT5-5 21.74 15.84 2.81 0.47\nDOCmT5-5-Large 35.63 29.50 14.15 1.16\nDOCmT5-25 22.00 14.62 17.40 16.93\nDOCmT5-25-Large 28.24 24.34 23.18 19.17\nTable 7: Document translation without finetuning on\nWMT20 De-En, Ru-En, Pl-En and Ja-En.\n4.3.3 Document Translation Without\nFinetuning\nWe further show thatDOCmT5 is able to perform\ndocument translation without finetuning, i.e., eval-\nuate the model right after second-stage pretrain-\ning without any finetuning on task-specific data.\nWe show the results in Table 7. While the mono-\nlingual pretrained models completely fail to pro-\nducemeaningfultranslations, DOCmT5-5 isableto\nachieveover20BLEUpointsinDe-Enand15inRu-\nEn. Notsurprisingly,DOCmT5-5-Large furtherim-\nprovestoover35and29respectively. DOCmT5-25\nincludes Pl-En and Ja-En in the second-stage pre-\ntraining and therefore obtains competitive results\nonthesetwolanguagepairswitheitherbaseorlarge\nmodel. AlthoughDOCmT5-5 is not pretrained on\nPl-En, the large model gets over 14 BLEU on this\ntask. One hypothesis is that Polish uses the Latin\nscript and shares common subwords with German\nand Spanish, allowing our model to transfer knowl-\nedge across languages. On the other hand, the\nDOCmT5-5-Base model fails to produce meaning-\nful translations for Pl-En. This shows the impor-\ntance of size when performing multilingual pre-\ntraining. Thebestmodelis DocNMT whichobtains\nover 40 BLUE points in both De-En and Ru-En,\noutperformingDOCmT5-5 andDOCmT5-25. This\nisreasonablebecause DOCmT5 shufflesdocuments\nin pretraining and this is misaligned with the docu-\nmenttranslationtaskinputs. Theimpressiveperfor-\nROUGE-1\nROUGE-L\n20 25 30 35\nmt5 senTLM DocTLM\nFigure 2: SenTLM and DocTLM finetuning results on\nWikilingua. The numbers are average of four languages:\n{Es, Tr, Ru, Vi}.\nmance of bothDocNMT and DOCmT5 shows that\nour MTmC4 corpus is of very high-quality and is\nlikely better than the parallel data provided by the\nspecific tasks in question. Further analysis of the\nquality of this data will be an interesting avenue for\nfuture work.\n5 Analysis\n5.1 Are Document-level Models Better Than\nSentence-level Models?\nTo demonstrate the benefits of pretraining with\nlonger context, we pretrain mT5 using translation\nlanguage modeling (TLM) on five languages: {De,\nEs, Tr, Vi, Ru} with two different inputs. In\nDocTLM, we concatenate the parallel documents\ninto a single training sequence. As forSenTLM, we\nbreakdownthedocumentintoindividualsentences\nand find the alignments in the parallel document\npair. Then we concatenate the single aligned sen-\ntencepairasatrainingsequence. Wefinetunethese\nsecond-stage pretrained models on Wikilingua and\nWMT20 De-En. The results are shown in Figure 2\nandTable8. Weseethatdocument-levelmodelsof-\nfersmallimprovementsonsummarizationandvery\nsignificant improvements on document-level trans-\nlation, showing that the longer context is indeed\nuseful.\nPretrained-Model BLEU\nmT5 29.08\nw. SenTLM 34.68\nw. DocTLM 37.74\nTable 8: SenTLM and DocTLM finetuning results on\nWMT20 De-En.\n431\nROUGE-1\nROUGE-L\n20 25 30 35\nDocNMT - CCAligned DocNMT - MTmC4\nDocTLM  - CCAligned DocTLM  - MTmC4\nFigure 3: MTmC4 and CCAlgined finetuning results on\nWikilingua. The numbers are average of four languages:\n{Es, Tr, Ru, Vi}.\nBLEU\n0\n10\n20\n30\n40\n50\nDocNMT DocTLM\nCCAligned MTmC4\nFigure 4: MTmC4 and CCAlgined finetuning results on\nWMT20 De-En.\n5.2 Effect of Data Quality in Second-stage\nPretraining\nIn our experiments, we observe big differences\nbetween different parallel corpora. We compare\nagainst the CCAligned corpus ‚Äì a large automati-\ncally mined corpus from Common Crawl which is\nfound to be very noisy (Kreutzer et al., 2021). In\ncontrast,MTmC4isproducedbyusinghigh-quality\ntranslation systems. We pretrain mT5-Base on five\nlanguages: {De, Es, Tr, Vi, Ru} with these two cor-\nporausing DocNMT andDocTLM.Wedemonstrate\nthe Wikilingua results in Figure 3 and WMT20 De-\nEn results in Figure 4. Using our curated MTmC4\nis consistently better regardless of pretraining ob-\njectives or tasks.\n5.3 Does Combining Mono-Lingual and\nCross-Lingual Pretraining Help?\nHere we try to see if combining both monolingual\nand cross-lingual objectives helps. We try two dif-\nferent continual pretraining strategies for combin-\ningDrand DrMT. Weusefivelanguages: {De,Ru,\nTr, Vi, Es}.(i)Dr‚Üí DrMT: We first pretrain mT5\nPretraining Steps (K)\nBLEU \n30\n35\n40\n45\n1100 1200 1300 1400 1500\nDOCmT5-5 DOCmT5-25 mT5-5langs\nFigure 5: finetuning results of WMT20 De-En along\nwith pretraining steps. We use DOCmT5-5-base.\nwith Dr on mC4 for 0.5M steps and then pretrain\nwith DrMT on MTmC4 for 0.5M steps.(ii) Dr +\nDrMT: We mix these two objectives with a 50-to-\n50% ratio and pretrain for 0.5M steps. In Table 9,\nwe show that(i) slightly improves over onlyDrMT\nin both tasks and(ii) slightly improves on WMT20\nDe-EnbutseemstohurtperformanceonISWLT15\nZh-En.\nPretrained-Model WMT20 De-En IWSLT15 Zh-En\nmT5\nw. Dr 36.63 23.75\nw. DrMT 42.05 28.00\nw. Dr‚Üí DrMT 42.75 28.18\nw. Dr + DrMT 42.37 27.35\nTable9: Methodsofcombiningmono-lingualandcross-\nlingual and their finetuning results on WMT20 De-En\nand IWSLT15 Zh-En.\n5.4 How Many Pretraining Steps is Required\nfor DrMT?\nTo answer this question, we take different pretrain-\ning checkpoints ofDOCmT5-5 and DOCmT5-25\nand finetune with WMT20 De-En. The results are\nshown in Figure 5. After 50k steps of pretrain-\ning withDrMT, both systems outperform thecont-\n5langs. After 300k steps, both systems roughly\nconverge and perform similarly.\n6 Conclusion\nIn this paper, we present DOCmT5, a novel\ndocument-level multilingual pre-trained model.\nOur proposed objective, DrMT, is simple and\neffective and leads to large gains over strong\nbaselines (e.g. mBART and MARGE) on cross-\nlingual summarization and document-level transla-\ntion. DOCmT5achievedSOTAontwocompetitive\ndocument-level translation tasks: WMT20 De-En\n432\nand IWSLT15 Zh-En. We further analyze various\nfactorsthatcontributetosuccessfuldocument-level\npre-training. We plan to release the pre-trained\nmodel to facilitate future work on document-level\nlanguage understanding.\nAcknowledgements\nWe would like to thank Alexis Conneau, Jon Clark\nand Mihir Sanjay Kale for the helpful discussions.\nWe also thank Sebastian Ruder, Noah Constant\nand Ankur Bapna for providing feedback on the\nmanuscript.\nReferences\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nInProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers),pages3874‚Äì3884.\nNaveenArivazhagan,AnkurBapna,OrhanFirat,Dmitry\nLepikhin, Melvin Johnson, Maxim Krikun, Mia Xu\nChen, Yuan Cao, George Foster, Colin Cherry, et al.\n2019. Massively multilingual neural machine trans-\nlation in the wild: Findings and challenges.arXiv\npreprint arXiv:1907.05019.\nJunxuan Chen, Xiang Li, Jiarui Zhang, Chulun Zhou,\nJianwei Cui, Bin Wang, and Jinsong Su. 2020. Mod-\neling discourse structure for document-level neural\nmachinetranslation. InProceedings of the First Work-\nshop on Automatic Simultaneous Translation, pages\n30‚Äì36.\nZewenChi,LiDong,ShumingMa,ShaohanHuang,Sak-\nsham Singhal, Xian-Ling Mao, Heyan Huang, Xia\nSong, and Furu Wei. 2021. mT6: Multilingual pre-\ntrained text-to-text transformer with translation pairs.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1671‚Äì1683, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm√°n, √âdouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. InPro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics,pages8440‚Äì8451.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlinguallanguagemodelpretraining. Advances in Neu-\nral Information Processing Systems, 32:7059‚Äì7069.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers),pages4171‚Äì\n4186.\nAhmed El-Kishky, Vishrav Chaudhary, Francisco\nGuzm√°n,andPhilippKoehn.2020. Amassivecollec-\ntionofcross-lingualweb-documentpairs. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n5960‚Äì5969.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, et al. 2021. Beyond english-centric mul-\ntilingual machine translation.Journal of Machine\nLearning Research, 22(107):1‚Äì48.\nSebastian Gehrmann, Tosin Adewumi, Karmanya\nAggarwal, Pawan Sasanka Ammanamanchi, An-\nuoluwapoAremu,AntoineBosselut,KhyathiRaghavi\nChandu, Miruna-Adriana Clinciu, Dipanjan Das,\nKaustubh Dhole, Wanyu Du, Esin Durmus, Ond≈ôej\nDu≈°ek, Chris Chinenye Emezue, Varun Gangal,\nCristina Garbacea, Tatsunori Hashimoto, Yufang\nHou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji,\nShailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Lad-\nhak, Aman Madaan, Mounica Maddela, Khyati Ma-\nhajan, Saad Mahamood, Bodhisattwa Prasad Ma-\njumder,PedroHenriqueMartins,AngelinaMcMillan-\nMajor, Simon Mille, Emiel van Miltenburg, Moin\nNadeem, Shashi Narayan, Vitaly Nikolaev, An-\ndre Niyongabo Rubungo, Salomey Osei, Ankur\nParikh, Laura Perez-Beltrachini, Niranjan Ramesh\nRao, Vikas Raunak, Juan Diego Rodriguez, Sashank\nSanthanam, Jo√£o Sedoc, Thibault Sellam, Samira\nShaikh, Anastasia Shimorina, Marco Antonio So-\nbrevilla Cabezudo, Hendrik Strobelt, Nishant Sub-\nramani, Wei Xu, Diyi Yang, Akhila Yerukola, and\nJiawei Zhou. 2021. The GEM benchmark: Natural\nlanguage generation, its evaluation and metrics. In\nProceedings of the 1st Workshop on Natural Lan-\nguage Generation, Evaluation, and Metrics (GEM\n2021), pages 96‚Äì120, Online. Association for Com-\nputational Linguistics.\nJunjie Hu, Melvin Johnson, Orhan Firat, Aditya Sid-\ndhant,andGrahamNeubig.2021. Explicitalignment\nobjectives for multilingual bidirectional encoders. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3633‚Äì3643.\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,\nLinjunShou,DaxinJiang,andMingZhou.2019. Uni-\ncoder: A universal language encoder by pre-training\nwith multiple cross-lingual tasks. InProceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2485‚Äì2494, Hong Kong,\nChina. Association for Computational Linguistics.\n433\nSebastien Jean, Stanislas Lauly, Orhan Firat, and\nKyunghyun Cho. 2017. Does neural machine trans-\nlation benefit from larger context?arXiv preprint\narXiv:1704.05135.\nMarcin Junczys-Dowmunt. 2019. Microsoft translator\nat wmt 2019: Towards large-scale document-level\nneural machine translation. InProceedings of the\nFourth Conference on Machine Translation (Volume\n2: Shared Task Papers, Day 1), pages 225‚Äì233.\nMihir Kale, Aditya Siddhant, Rami Al-Rfou, Linting\nXue, Noah Constant, and Melvin Johnson. 2021.\nnmT5 - is parallel data still relevant for pre-training\nmassively multilingual language models? InPro-\nceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers),pages683‚Äì691,\nOnline. Association for Computational Linguistics.\nShun Kiyono, Takumi Ito, Ryuto Konno, Makoto Mor-\nishita, and Jun Suzuki. 2020. Tohoku-aip-ntt at wmt\n2020 news translation task. InProceedings of the\nFifth Conference on Machine Translation, pages145‚Äì\n155.\nJuliaKreutzer,IsaacCaswell,LisaWang,AhsanWahab,\nDaan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera\nTapo, Nishant Subramani, Artem Sokolov, Claytone\nSikasote, et al. 2021. Quality at a glance: An audit\nof web-crawled multilingual datasets.arXiv preprint\narXiv:2103.12028.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n2020. Pre-training via paraphrasing. Advances in\nNeural Information Processing Systems, 33.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation.Trans-\nactions of the Association for Computational Linguis-\ntics, 8:726‚Äì742.\nAnt√≥nio Lopes, M. Amin Farajian, Rachel Bawden,\nMichael Zhang, and Andr√© F. T. Martins. 2020.\nDocument-level neural MT: A systematic compar-\nison. InProceedings of the 22nd Annual Conference\nof the European Association for Machine Translation,\npages 225‚Äì234, Lisboa, Portugal. European Associa-\ntion for Machine Translation.\nFuliLuo,WeiWang,JiahaoLiu,YijiaLiu,BinBi,Song-\nfang Huang, Fei Huang, and Luo Si. 2021. Veco:\nVariable and flexible cross-lingual pre-training for\nlanguage understanding and generation. InProceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3980‚Äì3994.\nAlexander Molchanov. 2020. Promt systems for wmt\n2020 shared news translation task. InProceedings of\nthe Fifth Conference on Machine Translation, pages\n248‚Äì253.\nXuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,\nHaoTian,HuaWu,andHaifengWang.2021. ERNIE-\nM: Enhanced multilingual representation by aligning\ncross-lingual semantics with monolingual corpora.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n27‚Äì38, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research,21:1‚Äì\n67.\nSebastian Ruder, Noah Constant, Jan Botha, Aditya Sid-\ndhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie\nHu, Dan Garrette, Graham Neubig, and Melvin John-\nson. 2021. XTREME-R: Towards more challenging\nand nuanced multilingual evaluation. InProceed-\nings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 10215‚Äì10245,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzm√°n. 2021a. Wiki-\nmatrix: Mining 135m parallel sentences in 1620 lan-\nguage pairs from wikipedia. InProceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 1351‚Äì1361.\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov,\nEdouard Grave, Armand Joulin, and Angela Fan.\n2021b. CCMatrix: Mining billions of high-quality\nparallel sentences on the web. InProceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume\n1: Long Papers), pages 6490‚Äì6500, Online. Associa-\ntion for Computational Linguistics.\nTingxun Shi, Shiyu Zhao, Xiaopu Li, Xiaoxue Wang,\nQianZhang,DiAi,DaweiDang,XueZhengshan,and\nJie Hao. 2020. OPPO‚Äôs machine translation systems\nfor WMT20. InProceedings of the Fifth Conference\non Machine Translation, pages 282‚Äì292, Online. As-\nsociation for Computational Linguistics.\nZewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao,\nShujian Huang, Jiajun Chen, and Lei Li. 2020. Cap-\nturing longer context for document-level neural ma-\nchine translation: A multi-resolutional approach.\narXiv preprint arXiv:2010.08961.\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-\nman Goyal, Vishrav Chaudhary, Jiatao Gu, and An-\ngela Fan. 2021. Multilingual translation from de-\nnoising pre-training. InFindings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 3450‚Äì3466.\n434\nJ√∂rg Tiedemann and Yves Scherrer. 2017. Neural ma-\nchine translation with extended context. InProceed-\nings of the Third Workshop on Discourse in Machine\nTranslation, pages 82‚Äì92.\nXiangpeng Wei, Rongxiang Weng, Yue Hu, Luxi Xing,\nHengYu,andWeihuaLuo.2020. Onlearninguniver-\nsalrepresentationsacrosslanguages. In International\nConference on Learning Representations.\nLesly Miculicich Werlen, Dhananjay Ram, Nikolaos\nPappas,andJamesHenderson.2018. Document-level\nneuralmachinetranslationwithhierarchicalattention\nnetworks. InProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2947‚Äì2954.\nLintingXue,NoahConstant,AdamRoberts,MihirKale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. InProceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483‚Äì498.\nBiao Zhang, Philip Williams, Ivan Titov, and Rico Sen-\nnrich. 2020a. Improving massively multilingual neu-\nral machine translation and zero-shot translation. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1628‚Äì\n1639.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei\nZhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018.\nImproving the transformer translation model with\ndocument-level context. InProceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 533‚Äì542.\nPei Zhang, Boxing Chen, Niyu Ge, and Kai Fan. 2020b.\nLong-short term masking transformer: A simple but\neffective baseline for document-level neural machine\ntranslation. InProceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1081‚Äì1087.\nZaixiang Zheng, Xiang Yue, Shujian Huang, Jiajun\nChen, and Alexandra Birch. 2020. Towards mak-\ning the most of context in neural machine translation.\nIn IJCAI.\n435\nAppendices\nA Analysis of Document Translation\nWe take a deeper look at the translations pro-\nducedbyvarioussystemstounderstandwhatmakes\nDOCmT5 better. We demonstrate an example in\nTable 6. We take the best system (DOCmT5-25-\nLarge) and thecont-5langs baseline. We observe\nthat DOCmT5 uses time tenses better than the\nbaseline, producing more coherent sentences (red-\ncolored texts). Additionally,DOCmT5 handles a\ncompositional sentence more elegantly, instead of\njust using \"and\" (blue-colored texts). Finally, we\nobserve thatcont-5langs often makes minor trans-\nlation mistakes while ourDOCmT5 makes much\nfewer of them.\n436\nÊàë 11 Â≤ÅÈÇ£Âπ¥Ôºå  ËÆ∞ÂæóÊúâ‰∏ÄÂ§©Êó©Êô®ÈÜíÊù•ÔºåÂê¨ËßÅÂÆ∂ÈáåÊúâÊÑâÊÇ¶ÁöÑÂ£∞Èü≥„ÄÇ  ÊàëÁöÑÁà∂‰∫≤Âú®Áî®‰ªñÁöÑÁÅ∞Ëâ≤Â∞èÊî∂Èü≥Êú∫  Âê¨ BBC Êñ∞ \nÈóª„ÄÇ  ‰ªñÈù¢Â∏¶Á¨ëÂÆπÔºåËøôÂæàÂ∞ëËßÅÔºå  Âõ†‰∏∫Â§ßÈÉ®ÂàÜÁöÑÊñ∞ÈóªÈÉΩÂè™‰ºö‰Ωø‰ªñÊ≤Æ‰∏ß„ÄÇ  \" Â°îÂà©Áè≠Ëµ∞‰∫ÜÔºÅ \" Áà∂‰∫≤Â§ßÂ£∞Âè´ÁùÄ„ÄÇ  Êàë‰∏ç \nÁü•ÈÅìÈÇ£ÊÑèÂë≥ÁùÄ‰ªÄ‰πàÔºå  ‰ΩÜÊòØÊàëËÉΩÁúãÂá∫Áà∂‰∫≤ÈùûÂ∏∏ÈùûÂ∏∏È´òÂÖ¥„ÄÇ  \" ‰Ω†Áé∞Âú®ÂèØ‰ª•Âéª‰∏™ÁúüÊ≠£ÁöÑÂ≠¶Ê†°Âøµ‰π¶‰∫Ü„ÄÇ \" ‰ªñËØ¥„ÄÇ  ÊàëÊ∞∏ \nËøú‰∏ç‰ºöÂøòËÆ∞ÈÇ£‰∏™Êó©Êô®„ÄÇ  ‰∏Ä‰∏™ÁúüÊ≠£ÁöÑÂ≠¶Ê†°„ÄÇ  Êàë 6 Â≤ÅÈÇ£Âπ¥ÔºåÂ°îÂà©Áè≠Âç†È¢ÜÈòøÂØåÊ±ó  Âπ∂ËßÑÂÆöÂ•≥Â≠©‰∏äÂ≠¶ÊòØËøùÊ≥ïÁöÑ„ÄÇ  ÊâÄ‰ª• \nÂú®ÈÇ£‰πãÂêé 5 Âπ¥ÔºåÊàëÂ•≥ÊâÆÁî∑Ë£Ö  Èô™ÁùÄÊàëÂßêÂßêÂéª‰∏Ä‰∏™ÁßòÂØÜÂ≠¶Ê†°  ÂßêÂßêÈÇ£Êó∂Â∑≤Áªè‰∏çË¢´ÂÖÅËÆ∏Áã¨Ëá™Â§ñÂá∫‰∫Ü„ÄÇ  ËøôÊòØÊàë‰ª¨‰ø©‰∫∫ \nÂîØ‰∏ÄÁöÑÂèóÊïôËÇ≤ÊñπÂºè„ÄÇ  Êàë‰ª¨ÊØèÂ§©Ë¶ÅËµ∞‰∏çÂêåÁöÑË∑ØÁ∫ø  ËøôÊ†∑ÊâçÊ≤°Êúâ‰∫∫‰ºöÊÄÄÁñëÊàë‰ª¨Ë¶ÅÂéªÂì™Èáå„ÄÇ \n...\nAnd when I was 11 years old, I remember waking up one morning to the sound of a happy voice in the house. My\nfather was listening to the BBC on his little gray radio. He had a smile on his face, which is rare, because most of\nthe news was depressing. \"The Taliban are gone!\" My father shouted. I didn't know what that meant, but I could\nsee that my father was very, very happy. \"You can go to a real school now,\" he said. And I will never forget that\nmorning. A real school. When I was six years old, the Taliban occupied Afghanistan and made it illegal for girls to\ngo to school. So for the next five years, I was a woman in a man's suit and went to a secret school with my sister,\nwho was not allowed to go out alone. This was the only way we were educated. We had to go in different\ndirections every day so no one would suspect where we were going.\n...\nAnd I was 11 years old, and I remember awakefully waking up in the morning and hearing the familiar sound. My\nfather was listening to the BBC news on his little radio. He was smiling, and it was rare, because most of the news\nwas going to frustrate him. \"Taliban go.\" The father went out. I don't know what that meant, but I can see that the\nfather was very, very happy. \"You can go to a real school now.\" He said. I'll never forget that morning. A real\nschool. And I was six years old, and Taliban took Afghanistan and banned girls' schooling. So five years after that,\nmy chick went to a secret school with my sister. And she wasn't allowed to go on a trip. It was the only way that we\nwere educated. We walked on different roads every day so that nobody could suspect where we were.\n...\nWhen I was 11, I remember waking up one morning to the sound of joy in my house. My father was listening to\nBBC News on his small, gray radio. There was a big smile on his face which was unusual then, because the news\nmostly depressed him. \"The Taliban are gone!\" my father shouted. I didn't know what it meant, but I could see that\nmy father was very, very happy. \"You can go to a real school now,\" he said. A morning that I will never forget. A\nreal school. You see, I was six when the Taliban took over Afghanistan and made it illegal for girls to go to school.\nSo for the next five years, I dressed as a boy to escort my older sister, who was no longer allowed to be outside\nalone, to a secret school. It was the only way we both could be educated. Each day, we took a different route so\nthat no one would suspect where we were going.  \n... \nSource \nDocument\nDOCmT5-25 \ntranslation \nmT5\ntranslation \nTarget  \nTranslation \nFigure6: AcomparisonexampleofZh-Endocumenttranslation. DOCmT5isabletoproduceconsistenttimetenses\nwhile mT5 baseline fails. DOCmT5 also produces longer and conherent sentences. Best viewed in color.\n437",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8801872730255127
    },
    {
      "name": "Computer science",
      "score": 0.8686511516571045
    },
    {
      "name": "Natural language processing",
      "score": 0.7602051496505737
    },
    {
      "name": "Machine translation",
      "score": 0.7566096782684326
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6637270450592041
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.6223499774932861
    },
    {
      "name": "Language model",
      "score": 0.5973769426345825
    },
    {
      "name": "Sentence",
      "score": 0.5236963033676147
    },
    {
      "name": "Translation (biology)",
      "score": 0.43660587072372437
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4326953589916229
    },
    {
      "name": "Information retrieval",
      "score": 0.3372762203216553
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ],
  "cited_by": 4
}