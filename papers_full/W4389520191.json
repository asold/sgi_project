{
  "title": "Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",
  "url": "https://openalex.org/W4389520191",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2373756424",
      "name": "Zhang, Xinsong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112218234",
      "name": "Zeng Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2232722591",
      "name": "Zhang Jipeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1979928760",
      "name": "Li, Hang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3118608800",
    "https://openalex.org/W4306820534",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3176641147",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2047643928",
    "https://openalex.org/W4312910992",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W4292945941",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W4281709371",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W3177224328",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2983943451",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4312312750",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W3121480429",
    "https://openalex.org/W4226498390",
    "https://openalex.org/W4281623693",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W4319049530",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3035333188",
    "https://openalex.org/W3208314443",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W4283034659",
    "https://openalex.org/W2489434015",
    "https://openalex.org/W3177365697",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W4296151206",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W4312746376",
    "https://openalex.org/W3214685499",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W4296406182",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W4287203292",
    "https://openalex.org/W4301914798",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2592170186",
    "https://openalex.org/W4313158203",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W12634471",
    "https://openalex.org/W4389665280",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3215626407"
  ],
  "abstract": "Foundation models or pre-trained models have substantially improved the performance of various language, vision, and vision-language understanding tasks. However, existing foundation models can only perform the best in one type of tasks, namely language, vision, or vision-language. It is still an open question whether it is possible to construct a general foundation model performing the best for all the understanding tasks. In this paper, we propose a new method for training the general foundation model, X-FM (the X-Foundation Model). X-FM has one language encoder, one vision encoder, and one fusion encoder, as well as a new training method. The training method includes two new techniques for learning X-FM from text, image, and image-text pair data. One is to stop gradients from the vision-language training when learning the language encoder. The other is to leverage the vision-language training to guide the learning of the vision encoder. Extensive experiments on benchmark datasets show that X-FM can significantly outperform existing general foundation models and perform better than or comparable to existing foundation models specifically for language, vision, or vision-language understanding. Code and pre-trained models are released at https://github.com/zhangxinsong-nlp/XFM. © 2023 Association for Computational Linguistics.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 551–568\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nToward Building General Foundation Models for Language, Vision, and\nVision-Language Understanding Tasks\nXinsong Zhang∗\nByteDance Research\nYan Zeng\nByteDance Research\nJipeng Zhang\nHKUST\nHang Li\nByteDance Research\nAbstract\nFoundation models or pre-trained models have\nsubstantially improved the performance of var-\nious language, vision, and vision-language un-\nderstanding tasks. However, existing foun-\ndation models can only perform the best in\none type of tasks, namely language, vision, or\nvision-language. It is still an open question\nwhether it is possible to construct a general\nfoundation model performing the best for all\nthe understanding tasks. In this paper, we pro-\npose a new method for training the general\nfoundation model, X-FM (the X-Foundation\nModel). X-FM has one language encoder, one\nvision encoder, and one fusion encoder, as\nwell as a new training method. The training\nmethod includes two new techniques for learn-\ning X-FM from text, image, and image-text\npair data. One is to stop gradients from the\nvision-language training when learning the lan-\nguage encoder. The other is to leverage the\nvision-language training to guide the learning\nof the vision encoder. Extensive experiments\non benchmark datasets show that X-FM can\nsignificantly outperform existing general foun-\ndation models and perform better than or com-\nparable to existing foundation models specifi-\ncally for language, vision, or vision-language\nunderstanding. Code and pre-trained mod-\nels are released at https://github.com/\nzhangxinsong-nlp/XFM.\n1 Introduction\nWith the enormous power of foundation models,\nalso known as pre-trained models, remarkable per-\nformance gains have recently been achieved in a\nvariety of understanding tasks in natural language\nprocessing (NLP), computer vision (CV), and other\nfields (Devlin et al., 2019; Liu et al., 2019; Lewis\net al., 2020; Raffel et al., 2020; Brown et al., 2020;\nDosovitskiy et al., 2021; He et al., 2022; Bao et al.,\n2021; Lu et al., 2019; Tan and Bansal, 2019; Chen\n∗Correspondence to: <xszhang0320@gmail.com>.\net al., 2020; Li et al., 2020, 2021a; Zeng et al., 2021,\n2022) . Foundation models are usually equipped\nwith Transformer (Vaswani et al., 2017) as the\nbackbone, pre-trained with a tremendous amount\nof unlabeled data, and then fine-tuned with small\namounts of labeled data in downstream tasks. The\nstrong representation ability of the model, the mas-\nsive amount of data, and the effective means of\ntraining make the foundation models powerful for\nsuccessfully solving the tasks of vision, language,\nand vision-language (Li et al., 2021b,c; Singh et al.,\n2021; Wang et al., 2021b, 2022b; Diao et al., 2022;\nWang et al., 2022a).\nThe state-of-the-art foundation models usually\nwork the best for one type of tasks, namely lan-\nguage, vision, and vision-language. For exam-\nple, RoBERTa (Liu et al., 2019), BEiTv2 (Peng\net al., 2022), and X-VLM (Zeng et al., 2021, 2022)\nare language, vision, and vision-language founda-\ntion models respectively, and can achieve state-\nof-the-art performances for the specific type of\ntasks. It is still very challenging, however, to\nbuild a general foundation model that can perform\nthe best in all types of tasks. Existing models,\nsuch as FLA V A (Singh et al., 2021), OFA (Wang\net al., 2022b), DaVinci (Diao et al., 2022) and\nUni-Perceiver-MoE (Zhu et al., 2022), are trying\nto achieve the goal. Their performances are still\nnot satisfactory, however, when compared with the\nbest performing foundation models for the individ-\nual types of tasks, as shown in Table 1. Previous\nwork (Bingel and Søgaard, 2017; Wang et al., 2020)\nalso shows that it is difficult to train a general foun-\ndation model in a multi-task learning setting that\ncan effectively learn and utilize representations for\nall types of tasks. The reason is that language,\nvision, and vision-language are very different in na-\nture, and a simple way of jointly training a model\nfrom language, vision, and vision-language data\ncan easily create a suboptimal solution.\nTo address the challenge, we propose a new\n551\nMethods\nText Tasks Vision Tasks Multi-modal Tasks (MSCOCO Retriveal & VQA & NLVR)\nGLUE ImageNet Zero-Shot Fine-Tune\nMNLI FT/LE TR IR TR IR VQA NLVR\nFoundation models specifically for language, vision, or vision-language understanding\nRoBERTa (Liu et al., 2019) 87.6 – – – – – – –\nBEiTv2 (Peng et al., 2022) – 85.5/80.1 – – – – – –\nX-VLM (Zeng et al., 2021) – – 70.8/92.1/96.5 55.6/82.7/90.0 80.4/95.5/98.2 63.1/85.7/91.6 78.1 84.8\nX2-VLM (Zeng et al., 2022) – – – – 83.5/96.3/98.5 66.2/87.1/92.2 80.4 87.0\nGeneral foundation models\nUNIMO-2 (Li et al., 2021c) 87.5 80.8/- – – – – 76.3 –\nSimVLM (Wang et al., 2021c)83.4 -/80.6 – – – – 77.9 81.8\nFLA V A (Singh et al., 2021)80.3 -/75.5 42.7/76.8/- 38.4/67.5/- 61.5/82.1/89.6 50.1/74.4/83.2 72.8 –\nOFA (Wang et al., 2022b) 84.3 82.2/– – – – – 78.0 –\nDaVinci (Diao et al., 2022) 83.1 83.9/78.8 – – – – 76.3 77.9\nOmniVL (Wang et al., 2022a)– – – – 76.8/93.6/97.3 58.5/82.6/89.5 78.3 –\nUni-Perceiver-MoE (Zhu et al., 2022)81.5 84.5/– 64.6/–/– 51.6/–/– 70.5/–/– 54.1/–/– – –\nmPLUG-2base(Xu et al., 2023) 87.6 –/– –/–/– –/–/– 81.2/95.2/98.1 65.3/86.9/92.4 79.3 –\nX-FMbase 87.7 85.5/81.2 77.6/94.8/97.761.1/84.5/90.684.2/96.4/98.4 67.0/87.2/92.480.5 88.4\nTable 1: Performance comparisons between foundation models.All results are frombase-size models. MSCOCO\nis a cross-modal retrieval task, and IR and TR are image-retrieval and text-retrieval, respectively. MNLI results are\naverage accuracies of MNLI-m and MNLI-mm. For ImageNet1k classification, we report linear evaluation (LE)\nperformance and fine-tuning (FT) performance, respectively. We report R@1/R@5/R@10 for all retrieval tasks at\nboth zero-shot and fine-tune settings. We report the VQA test-dev result and the NLVR test-P result. bold denotes\nthe best number across general foundation models. underline denotes the best across all models.\nmethod for training general foundation model, and\nbring in X-FM (X-Foundation Model). X-FM con-\nsists of three modular encoders for language (text)\nencoding, vision (image) encoding, and fusion en-\ncoding, as shown in Fig 1. The language encoder,\nthe vision encoder, and the entire model can be\nused in downstream tasks of language, vision, and\nvision-language understanding, respectively. The\nlanguage encoder and the vision encoder follow the\nimplementations of BERT (Devlin et al., 2019) and\nViT (Dosovitskiy et al., 2021), respectively. Note\nthat X-FM do not include any extra parameters for\nlanguage and vision tasks. The fusion encoder has\nthe same architecture as BERT except that there is\na cross-attention sub-layer after the self-attention\nsub-layer in each Transformer layer.\nIn learning of X-FM, the language encoder, vi-\nsion encoder, and fusion encoder are jointly trained\nwith text data, image data, and image-text pair\ndata as input. Given the text data, we train the\nlanguage encoder by masked language modeling\n(MLM). Given the image data, we train the vision\nencoder by masked image modeling (MIM). Given\nthe image-text pair data, we train the fusion encoder\nby image text matching (ITM), image-conditioned\nmasked language modeling (IMLM), bounding box\nprediction (BBP), also train the vision encoder and\nthe language encoder by image-text contrastive\nlearning (ITC). (See Fig 1.)\nThe essential thinking of our learning method\nis that language is more abstract than vision, and\nthere is an asymmetric relationship between lan-\nguage and vision. Therefore, we separate the learn-\ning of the three encoders. The language encoder\nis trained mainly from text data and is isolated\nfrom the training of the fusion encoder. The vi-\nsion encoder is simultaneously trained from image\ndata and image-text pair data, guided by the vision-\nlanguage training. The fusion encoder is trained\nfrom image-text pair data.\nOur learning method includes two new tech-\nniques. One technique is to stop gradients from\nthe vision-language training when learning the lan-\nguage encoder. The gradient flow is stopped from\nthe fusion encoder to the language encoder in train-\ning, while the activation flow from the language en-\ncoder to the fusion encoder is as usual. As a result,\nthe language encoder is not affected by training of\nthe fusion encoder with image-text pair data. More-\nover, the training of the fusion encoder concentrates\non learning the alignments between language and\nvision features.\nThe other technique is to leverage the vision-\nlanguage training to guide the learning of the vi-\nsion encoder with masked image modeling (MIM).\nIn MIM, the masked image is compared with the\noriginal image by the differences between the pre-\ndicted representations and target representations\nat the masked and [CLS] positions. The vision\nencoder creates both the predicated and target rep-\n552\nresentations, while there is gradient flow from the\npredicted representations but no gradient flow from\nthe target representations. The vision encoder can\ncreate the target representations because it is also\ntrained in the vision-language training.\nWe conduct experiments on a variety of twenty-\nthree tasks of language, vision, and vision-language\nunderstanding. X-FM can outperform other general\nfoundation models by a large margin and can even\nachieve better or comparable performance than\nSOTA foundation models specifically designed for\nlanguage, vision, or vision-language understanding\ntasks, as shown in Table 1.\nOur contribution is as follows.\n(1) We address the problem of how to build a\ngeneral foundation model that can perform the best\nfor all the understanding tasks of language, vision,\nand vision-language.\n(2) We propose a general foundation model, X-\nFM, which can achieve better or competitive per-\nformance on both unimodal understanding tasks\nand multi-modal understanding tasks through two\ntraining techniques.\n(3) The stop gradient technique is useful in main-\ntaining text understanding capability and enhancing\nmulti-modal understanding capability at the same\ntime. We also propose a convenient method for\nmask image modeling with multi-modal learning.\nThe technique can enhance both vision and multi-\nmodal understanding.\n2 Related Work\nFollowing the success of language model pre-\ntraining (Devlin et al., 2019; Liu et al., 2019;\nSun et al., 2019; Joshi et al., 2020; Clark et al.,\n2020; Lan et al., 2020; Zhang et al., 2020; He\net al., 2021), vision pre-training (Dosovitskiy et al.,\n2021; He et al., 2022; Bao et al., 2021; Peng et al.,\n2022; Wei et al., 2022a) and vision-language pre-\ntraining (Radford et al., 2021; Jia et al., 2021; Li\net al., 2021a, 2022; Yuan et al., 2021; Wang et al.,\n2021a; Bao et al., 2022; Zeng et al., 2021, 2022)\nwith Transformer as the backbone have also made\nsignificant progress recently, pushing the state-of-\nthe-art of various understanding tasks of language,\nvision, and vision-language.\nRecently, the fact that Transformer can model\nmulti-modal data within a single architecture has\ninspired research to develop general foundation\nmodels that can solve language, vision, and vision-\nlanguage tasks at the same time. UNIMO (Li\net al., 2021b,c) jointly learns vision representations,\nlanguage representations, and vision-language\nalignments in a shared space. FLA V A (Singh\net al., 2021) performs pre-training with masked\nuni-modal and multi-modal modeling objectives.\nOFA (Wang et al., 2022c) formulates vision-\nlanguage tasks as sequence-to-sequence (seq2seq)\nproblems and pre-trains a seq2seq model in multi-\ntask learning. SimVLM (Wang et al., 2021c) pre-\ntrains a seq2seq model with a single objective of\nlanguage generation (prefix language modeling).\nDaVinci (Diao et al., 2022) combines prefix lan-\nguage modeling and prefix image modeling to learn\na general foundation model for a wide range of\ntasks. Uni-Perceiver (Zhu et al., 2021, 2022) builds\na unified perception architecture that processes var-\nious modalities and tasks with a single Transformer\nand shared parameters.\nPrevious studies on general foundation models\nhave shown that different capabilities can be es-\ntablished with only one model. Still, few stud-\nies demonstrate that the best performance can be\nachieved in all tasks with one model. In this pa-\nper, we propose a new method for training general\nfoundation model and show that it can perform the\nbest for all the understanding tasks of language, vi-\nsion, and vision-language. We compare our model\nextensively with recent general foundation models\non multiple dimensions, as shown in Appendix A.\nSeveral super-large foundation models (over 1B\nparameters) are proposed recently, most of which\nare trained on super-large in-house datasets (over\n900M image-text pairs). The authors do not re-\nport results at the base (about 300M parameters)\nscale on public datasets, which we consider in\nthis paper. CoCa (Yu et al., 2022) pre-trains an\nimage-text sequence-to-sequence model with con-\ntrastive loss and captioning loss. BEiT-3 (Wang\net al., 2022d) uses a multi-way Transformer and\na unified objective of masked “language” model-\ning for learning from image, text, and image-text\npair data. Flamingo (Alayrac et al., 2022) makes\nuse of a large language model in vision-language\npre-training to solve the “in-context learning” prob-\nlem for vision-language tasks. PaLI (Chen et al.,\n2022) jointly scales up the vision encoder and lan-\nguage encoder to cover a variety of language, vi-\nsion, vision-language, and multilingual tasks.\n553\nText Encoder\nCross-AttentionFusion Encoder\nFeed ForwardSelf-Attention Vision Encoder\nFeed ForwardSelf-Attention\ntwobrownandwhitedogs.\nMask\nFeed ForwardSelf-Attention\nstop gradMIMMLM\nIMLM, ITM, BBP\nITCx M x N\nx L\nMask\ntarget\nFigure 1: The architecture and pre-training process of X-FM, a Transformer-based general foundation model.\nGiven a text, we learn the language encoder by MLM. Given an image, we learn the vision encoder by MIM. Given\nan image-text pair, we learn the fusion encoder by BBP, ITM, IMLM and ITC, and further learn the vision encoder\nby MIM. The gradients of BBP, ITM, and IMLM are stopped from the fusion encoder to the language encoder.\nThe vision encoder is trained by MIM with both the image-text pair data and the image data. M, N and L denote\nnumbers of encoder layers.\n3 Method\n3.1 Model Architecture and Training Process\nWe propose a new method for training general\nfoundation model and bring in X-FM, having a\nlanguage encoder, a vision encoder, and a fusion\nencoder, shown as Fig 1. The architectures of lan-\nguage encoder, vision encoder and fusion encoder\nare following precious works (Devlin et al., 2019;\nDosovitskiy et al., 2021; Li et al., 2021a). We pro-\npose a new method for training general foundation\nmodel. Text, image, and image-text pair data are\nused as input to train X-FM. The language encoder\nis trained by masked language modeling (MLM)\nand image text contrastive learning (ITC). The vi-\nsion encoder is trained by masked image model-\ning (MIM) and ITC. The fusion encoder is trained\nby image text matching (ITM), image-conditioned\nmasked language modeling (IMLM), and bounding\nbox prediction (BBP). There are two new tech-\nniques developed for the training.\nStop Gradient. We stop gradients from the\nvision-language training when learning the lan-\nguage encoder. Specifically, when the fusion en-\ncoder is trained with image-text pair data by ITM,\nIMLM, and BBP, there are forward flows (activa-\ntions) from the language encoder to the fusion en-\ncoder, but there are no backward flows (gradients)\nfrom the fusion encoder to the language encoder. In\nthis way, the language encoder is only trained with\ntext data by MLM and with image-text pair data\nby ITC. The former helps the language encoder to\nlearn text representations, and the latter helps to\nmake alignments between text representations and\nimage representations. Meanwhile, the training of\nthe fusion encoder is performed separately with the\nfocus of learning cross-modal alignments.\nVision-Language Guided Masked Image Mod-\neling. The training of vision encoder by MIM\nis carried out as follows. The image data is first\nmasked and then predicted by the vision encoder.\nThe differences between predicted representations\nand ‘target’ representations at masked positions\nand [CLS] position are then measured with MSE\n(mean squared error) loss. The target representa-\ntions are obtained from the same image data (with-\nout masking) by the vision encoder. There are\nno gradients from the target representations in the\nlearning of the vision encoder. The vision encoder\ncan create target representations because it is also\ntrained with image-text pair data. In this way, the\nvision encoder is trained by both the cross-modal\nobjectives (ITC, ITM, BBP, IMLM) with image-\ntext pair data and the uni-modal objective (MIM)\nwith image data. The representations obtained from\nthe vision-language training are highly semantic,\nwhich is necessary for MIM as demonstrated in\nprevious work (Bao et al., 2021; Peng et al., 2022;\nWei et al., 2022a,b).\n554\nThere are mainly two advantages by exploiting\nthe new MIM technique. First, it is convenient\nto conduct MIM with the signals from the vision-\nlanguage training. Note that most previous work\nfor MIM uses an external image tokenizer such\nas VQ-V AE (Bao et al., 2021; Singh et al., 2021),\nCLIP (Wei et al., 2022b), and VQ-KL (Peng et al.,\n2022). Second, the learning of the vision encoder\nand that of the fusion encoder are mutually en-\nhanced. Once the vision encoder is trained, it is\nalso utilized to train the fusion encoder. Fortu-\nnately, image data for training the vision encoder\nis relatively easy to obtain.\n3.2 Pre-training Objectives\nWe explain six objectives in learning of X-FM.\nHere, T represents the distribution of text data,\nIrepresents the distribution of image data, and D\nrepresents the distribution of image-text pair data.\nMasked Language Modeling (MLM)We per-\nform MLM on text data to learn the language en-\ncoder of X-FM. Specifically we recover the masked\ntokens in a text by minimizing the cross entropy\nloss below.\nLmlm = ET∼TH(⃗ y( ¯T),ˆ⃗ p( ¯T)) (1)\nwhere T denotes a text, ¯T denotes the masked\ntext of T, ˆ⃗ pdenotes the predicted probability vec-\ntors of masked tokens of ¯T, ⃗ ydenotes the one-hot\nvectors representing the original tokens of ¯T, and\nH denotes cross-entropy.\nImage-Text Contrastive Learning (ITC).We\nuse a contrastive loss as in CLIP (Radford et al.,\n2021) to learn the alignments between images and\ntexts in ITC. Given a batch of images and texts,\nwe calculate the cosine similarities between all\nimage-text pairs. For each image, there is one\ntext matched and the rest is unmatched. For each\ntext, there is one image matched and the rest is\nunmatched. The contrastive loss is defined as fol-\nlows.\nLitc = 1\n2E(I,T )∼D\n[\nH(⃗ yi2t(I),⃗ pi2t(I))\n+ H(⃗ yt2i(T),⃗ pt2i(T))\n]\n(2)\nwhere (I,T ) denotes an image-text pair, ⃗ pi2t(I)\ndenotes the in-batch image-to-text similarities,\n⃗ pt2i(T) denotes the in-batch text-to-image similari-\nties, ⃗ yi2t(I) denotes the one-hot vectors represent-\ning the image-to-text matching relations, ⃗ yt2i(T)\ndenotes the one-hot vectors representing the text-to-\nimage matching relations, and H is cross-entropy.\nImage-Text Matching (ITM).We also learn the\nalignments between images and texts in ITM, us-\ning a loss indicating whether an image-text pair\nis matched. For each image in a batch there is\na matched (positive) text, and we sample an un-\nmatched (negative) text in the batch. For each text\nthere is a matched (positive) image, and we sam-\nple an unmatched image in the batch. The loss is\ndefined as follows.\nLitm = E(I,T )∼D\n[\nH(pmatch(I,T ))\n+H(pmatch(˜I,T )) (3)\n+H(pmatch(I, ˜T))\n]\nwhere (I,T ) denotes a positive image-text pair,\n(˜I,T ) and (I, ˜T) denote negative image-text pairs,\npmatch(I,T ) denotes a predicted matching proba-\nbility of (I,T ), and H denotes logistic loss.\nImage-conditioned Masked Language Mod-\neling (IMLM)We conduct IMLM on image-text\npair data to learn the fusion encoder. We recover\nthe masked text tokens given for an image-text pair\nby minimizing the cross entropy loss below.\nLimlm = E(I,T )∼DH(⃗ y( ¯T),ˆ⃗ p(I, ¯T)) (4)\nwhere (I,T ) denotes an image-text pair, ¯T denotes\nthe masked text of T, ˆ⃗ p(I, ¯T) denotes the predicted\nprobability vectors of the masked tokens of¯T based\non I, ⃗ ydenotes the one-hot vectors representing the\noriginal tokens of ¯T, and H denotes cross-entropy.\nBounding Box Prediction (BBP)We adopt the\nBBP in X-VLM (Zeng et al., 2021, 2022), which\nlocates the visual concept in the image by a bound-\ning box given the text. With BBP we learn the\nalignments between the images and texts in multi-\ngranularity. In BBP, two losses are simultane-\nously minimized to measure the differences be-\ntween the predicted bounding box and the ground-\ntruth bounding box. One is generalized intersection\nover union GIoU (Rezatofighi et al., 2019) and the\nother is ℓ1 distance.\nLbbp = E(I,T )∼D{GIoU(⃗b,ˆ⃗b) + ∥⃗b−ˆ⃗b∥1} (5)\nwhere ⃗b= (cx,cy,w,h ) denotes the ground truth\nbounding box, ˆ⃗b= ( ˆcx, ˆcy, ˆw,ˆh) denotes the pre-\ndicted bounding box. A bounding box is repre-\nsented by two coordinates, width, and height.\nMasked Image Modeling (MIM)We perform\nMIM on image data and image-text pair data to\nlearn the vision encoder. Specifically, we recover\n555\nRoBERTa BEiTv2 X2-VLM X2-VLM UNIMO-2 FLA V A SimVLM OFA DaVinci DaVinci Uni-Per. OmniVL mPLUG-2baseX-FMbaseX-FMbase1 2 3 4 5 6 7 8 9 10 11 12 13 14 15Task Eval. – – 4M 1.3B 4M 70M 1.8B 21M 46M 648M 30M 14M 17M 4M 1.3B\nMNLI FT 87.6 – – – 87.5 80.3 83.4 84.3 82.3 83.1 81.5 – 87.6 87.7 87.7CoLA FT 63.6 – – – 62.1 50.7 46.7 52.3 52.1 54.8 52.2 – – 65.3 65.7MRPC FT 90.2 – – – – 84.2 79.8 88.7 83.1 84.5 – – 87.3 91.7 91.2QQP FT 91.9 – – – – 88.7 90.4 91.3 88.2 88.9 – – 91.3 91.8 91.7SST-2 FT 94.8 – – – 94.7 90.9 90.9 92.7 90.5 91.4 90.9 – 93.5 95.0 94.6QNLI FT 92.8 – – – – 87.3 88.6 91.1 87.2 87.9 88.2 – 93.2 92.9 92.8RTE FT 78.70 – – – – 57.8 63.9 70.8 60.7 64.2 75.8 – 85.2 83.8 82.7STS-B FT 91.2 – – – 91.2 85.7 87.2 – 86.3 87.1 – – – 90.8 90.7\nLanguage Avg.86.4 – – – – 78.2 78.9 – 78.8 80.2 – – – 87.4 87.1\nImageNet FT – 85.5 – – 80.8 – – 82.2 – 83.9 84.5 – – 85.3 85.5ImageNet LE – 80.1 – – – 75.5 80.6 71.4 † 75.9 77.7 – – – 81.0 81.2Food101 LE – 88.2 † – – – 88.5 – 75.2 † 89.3 90.1 – 87.4 – 88.7 90.5CIFAR10 LE – 95.3 † – – – 92.9 – 86.1 † 93.0 94.0 – 96.2 – 97.2 97.4CIFAR100 LE – 81.5 † – – – 77.7 – 66.7 † 79.0 80.1 – 83.2 – 86.7 86.2Pets LE – 93.1† – – – 84.8 – 81.0 † 85.5 88.2 – 87.1 – 90.8 90.2DTD LE – 78.4† – – – 77.3 – 70.3 † 77.1 78.3 – 76.2 – 78.4 80.0Flowers102 LE – 95.7 † – – – 96.4 – 86.3 † 96.1 96.9 – 89.8 – 97.1 96.4\nVision Avg. – 88.7 – – – 86.3 – 79.2 86.7 87.9 – 86.7 – 89.8 90.1\nVQAv2 FT – – 79.2 80.4 76.3 72.5 77.9 78.0 73.9 76.4 – 78.3 79.3 79.1 80.5NLVR2 FT – – 86.1 87.0 – – 81.8 – 77.9 – – – – 86.7 88.4Flickr30K TR R@1 ZS– – 85.1 † 85.1† 84.6† 88.5 67.7 – – – 82.1 – – 90.1 93.4Flickr30K IR R@1 ZS– – 77.3 † 79.2† 72.7 65.2 – – – – 72.4 – – 79.1 84.1Flickr30K TR R@1 FT– – 97.4 98.5 92.0 – – – – – 93.6 94.9 96.9 97.4 98.1Flickr30K IR R@1 FT– – 90.0 90.4 80.1 – – – – – 79.8 83.4 88.2 88.6 89.9COCO TR R@1 ZS– – 68.4 † 71.7† – 42.7 – – – – 64.6 – – 73.8 77.6COCO IR R@1 ZS– – 55.2 † 58.3† – 38.4 – – – – 51.6 – – 59.4 61.1COCO TR R@1 FT– – 80.5 83.5 – – – – – – 70.5 76.8 81.2 81.8 84.2COCO IR R@1 FT– – 62.7 66.2 – – – – – – 52.6 58.5 65.3 64.7 67.0\nVision-Language Avg.– – 78.2 80.0 – – – – – – – – – 80.1 82.4\nTable 2: Experimental results on vision, language and vision-language tasks.The multi-modal data size used for\npre-training are reported under the model name. MNLI results are average of MNLI-m and MNLI-mm. MRPC\nresults are average accuracies and F1 scores. Matthews correlation coefficient (MCC) is reported for CoLA, and\nPearson correlation coefficient (PCC) is reported for STS-B. We report accuracies for all the vision and multi-\nmodal tasks. FT is short for fine-tuning, LE for linear evaluation, ZS for zero-shot, TR for text retrieval, and IR\nfor image retrieval. Results for RoBERTa are from its corresponding paper (Liu et al., 2019), and they use the\nmid-training (Phang et al., 2018) on MNLI for RTE, MRPC, and STS-B while other models (e.g., DaVinci, X-FM)\ndo not use this trick. Note that mPLUG-2 used more layers and parameters than RoBERTa and X-FM for the\nlanguage understanding tasks. Language Avg. is the average score of all the language tasks, while Vision Avg. is\nthe average score of six line evaluation tasks except ImageNet. Vision-Language Avg. is the average score of all\nvision-language tasks. † are our reproduced results with the officially released models.\nthe masked image patches in an image by minimiz-\ning the loss below.\nLmim = E(I,T )∼D||⃗ v(¯I) −ˆ⃗ v(¯I)||2\n+EI∼I||⃗ v(¯I) −ˆ⃗ v(¯I)||2 (6)\nwhere (I,T ) and Idenote an image-text pair and\na single image respectively, ¯I denotes the masked\nimage I, ˆ⃗ v(¯I) denotes the predicted representa-\ntions at the masked positions and [CLS] of ¯I,\nand ⃗ v(¯I) denotes the target representations at the\nmasked positions and [CLS] of ¯I. ||˙||2 is the MSE\nloss. We employ block masking following previous\nwork (Bao et al., 2021; Peng et al., 2022). Note\nthat (I,T ) and I are independently sampled from\nDand I, and the sample sizes are not necessarily\nequal.\nFinally, the pre-training objective of X-FM is\ndefined as the sum of the losses described above.\nL= Lmlm + Litc + Litm + Limlm + Lbbp + Lmim\n4 Experiments\n4.1 Implementation Details\nPre-training Datasets. We mainly conduct our\nexperiments on several widely used public datasets,\nconsisting of two in-domain datasets, COCO (Lin\net al., 2014) and Visual Genome (VG) (Krishna\net al., 2017), and two out-of-domain datasets, SBU\nCaptions (Ordonez et al., 2011) and Conceptual\nCaptions (CC) (Sharma et al., 2018). Follow-\ning X-VLM (Zeng et al., 2021, 2022), we also\ninclude annotations of objects and regions from\nRefCOCO (Yu et al., 2016), Objects365 (Shao\net al., 2019) and OpenImages (Kuznetsova et al.,\n2018). Since we assume also using uni-modal\ndata, we include RoBERTa corpus (Liu et al.,\n2019), C4 datasets (Raffel et al., 2020) and Im-\nagenet21K (Ridnik et al., 2021). In addition, we\nalso scale up the pre-training dataset with Concep-\ntual 12M dataset (CC-12M) (Changpinyo et al.,\n2021) and LAION (Schuhmann et al., 2022) as the\n“more data\" setting, which contains around 1.3B\n556\nimage-text pairs. Please refer to Appendix B for\nstatistics of the pre-training datasets.\nPre-training Settings. Our model is of base size,\nand the detailed parameters are explained in Ap-\npendix D. The vision encoder is initialized with\nBEiTv2. The language encoder is initialized with\nRoBERTa. The fusion encoder is trained from\nscratch. X-FM is pre-trained at image resolution of\n224 ×224 with patch size of 16 ×16. We pre-train\nX-FM for 200K steps with a batch size of 3072\nimage-text pairs, 3072 images, and 8192 sentences\non 32 A100, which takes about six days. The learn-\ning rate for both models is warmed-up to 1e−4 in\nthe first 2500 steps and decayed following a linear\nschedule. We set the maximum number of text to-\nkens to 30 for image-text pairs, while that of pure\ntext corpus is set to 128. For the “more data\" set-\nting, we pre-train X-FM for 400k steps with 18k\nbatch size on 64 A100. Due to the consideration of\ncomputational cost, we did not pre-train the large\nor giant models. We apply mixed precision for pre-\ntraining. We choose widely used downstream tasks\nwhose details are shown in Appendix C.\n4.2 Comparison with Foundation Models\nWe extensively compare the performance of X-\nFM with state-of-the-art foundation models on\nvision, language, and multi-modal tasks. We\nfirst compare our model with general foundation\nmodels, including UNIMO-v2 (Li et al., 2021c),\nFLA V A (Singh et al., 2021), SimVLM (Wang et al.,\n2021c), OFA (Wang et al., 2022b), DaVinci (Diao\net al., 2022), Uni-Perceiver-MoE (Zhu et al., 2022),\nOmniVL (Wang et al., 2022a), and mPLUG-2 (Xu\net al., 2023). We also include comparisons with\nSOTA foundation models specifically designed\nfor language, vision, or vision-language tasks,\nRoBERTa (Liu et al., 2019), BEiTv2 (Peng et al.,\n2022), and X2-VLM (Zeng et al., 2022). There are\nseveral observations in Table 2. First, X-FM base\n(column 15) outperforms all the previous general\nfoundation models (column 5-13) across almost\nall tasks by a large margin, becoming a new and\nstronger general foundation model. When we\nuse less pre-training data, X-FM can also achieve\ncompetitive performance compared with previ-\nous general foundation models (column 5-13 vs\n14). Second, we compare X-FM with state-of-\nthe-art foundation models specifically designed\nfor language, vision, and vision-language tasks,\nRoBERTa, BEiTv2 and X2-VLM. We observe that\nX-FM is also better than or comparable with the\nfoundation models (column 1,2,3,4 vs 15). We\nfurther compare our model, X-FMbase, with three\nprevious foundation models on 18 image classi-\nfication tasks on the linear evaluation setting to\nevaluate generalization performance on vision un-\nderstanding tasks. The results are shown in Table 4.\nX-FMbase wins 11 of 18 tasks, 7 for CLIP, 2 for\nFLA V A, and 2 for DaVinci.\n4.3 Comparison with multi-modal Models\nIn addition to general foundation models, we\nalso compare X-FM with state-of-the-art vision-\nlanguage models. The results are shown in Table 3\nand Table 6. X-FM demonstrates its superiority\non five downstream vision-language tasks includ-\ning MSCOCO Retrieval, Flick Retrieval, VQA,\nNLVR and RefCOCO+. Note that X-FMbase out-\nperforms CLIP, ALIGN and Florence on image-text\nretrieval tasks with fewer parameters and much less\ntraining data. Compared to the recently released\nSOTA vision-language model, X2-VLM, X-FM is\nmuch better on zero-shot image-text retrieval tasks.\nWhen we scale up pre-training datasets, X-FMbase\nis consistently better than previous vision-language\nmodels for most cases.\n4.4 Ablation Study\nTo verify the contributions of different modules\nin our framework, we ablate them and evaluate\nthe performance of X-FM on all downstream tasks.\nThe results are shown in Table 5. We first explain\nseveral abbreviations in the table. S-MLM means\nthat we only stop the gradient of language repre-\nsentations in IMLM task, while S-ITM means stop-\nping the gradient of language representations for\ncomputing ITM and BBP. wostop indicates without\nstopping the gradients of all language representa-\ntions. woMIM means that we do not learn by MIM,\nwhile wBEiTv2 tokenizer means that we learn by\nMIM with the image tokenizer used in BEiTv2.\nMulti-task is a variation that uses straightforward\nmulti-task learning to optimize the three encoders\nin X-FM. To make a fair comparison, we also train\nRoBERTa, BEiTv2 and X2-VLM with the same\ndata noted as RoBERTa†, BEiTv2†and X2-VLM†.\nNote that we also increase the fusion layers in X2-\nVLM†to make the parameter sizes comparable to\nour models. RoBERTa†, BEiTv2†and X2-VLM†\nall have slightly better results on average than the\n557\nModel # ParamsMSCOCO (5K test set) Flickr30K (1K test set) MSCOCO (5K test set) Flickr30K (1K test set)TR-Fine-Tune IR-Fine-Tune TR-Fine-Tune IR-Fine-TuneTR-Zero-Shot IR-Zero-Shot TR-Zero-Shot IR-Zero-ShotR@1/R@5/R@10 R@1/R@5/R@10 R@1/R@5/R@10 R@1/R@5/R@10R@1/R@5/R@10 R@1/R@5/R@10 R@1/R@5/R@10 R@1/R@5/R@10\nALBEF 210M 73.1/91.4/96.0 56.8/81.5/89.2 94.3/99.4/99.8 82.8/96.7/98.4– – 90.5/98.8/99.7 76.8/93.7/96.7VLMobase 175M 74.8/93.1/96.9 57.2/82.6/89.8 92.3/99.4/99.9 79.3/95.7/97.8– – – –VL-BEiT 175M 79.5/–/– 61.5/–/– 95.8/–/– 83.9/–/– – – – –OmniVL 288M 76.8/93.6/97.3 58.5/82.6/89.5 94.9/9.6/99.9 83.4/97.0/98.6– – – –X-VLM 216M 80.4/95.5/98.2 63.1/85.7/91.6 96.8/99.8/100 86.1/97.4/98.770.8/92.1/96.5 55.6/82.7/90.0 85.3/97.8/99.6 71.9/93.3/96.4X2-VLMbase 255M 80.5/95.5/97.8 62.7/84.7/90.797.4/99.9/100 90.0/98.6/99.368.4†/92.5†/96.8† 55.2†/82.2†/89.3† 85.1†/99.2†/100.0† 77.3†/95.3†/97.6†\nX-FMbase 327M 81.8/96.0/98.3 64.7/86.1/91.6 97.4/100/10088.6/97.9/98.973.8/93.9/97.2 59.4/83.6/90.090.1/99.2/99.9 79.1/95.2/97.3\nMore DataCLIP 490M – – 88.7/98.0/99.2 76.7/93.6/96.4 58.4/81.5/88.1 37.8/62.4/72.2 88.0/98.7/99.4 68.7/90.6/95.2ALIGN 490M 77.0/93.5/96.9 59.9/83.3/89.8 95.3/99.8/100 84.9/97.4/98.658.6/83.0/89.7 45.6/69.8/78.6 88.6/98.7/99.7 75.7/93.8/96.8Florence 893M 81.8/95.2/– 63.2/85.7/– 97.2/99.9/– 87.9/98.1/–64.7/85.9/– 47.2/71.4/– 90.9/99.1/– 76.7/93.6/–X2-VLMbase 255M 83.5/96.3/98.5 66.2/87.1/92.298.5/100/100 90.4/98.2/99.371.7†/93.4†/97.5† 58.3†/84.7†/91.0† 84.6†/99.1†/99.9† 79.2†/96.4†/98.0†\nX-FMbase 327M 84.2/96.4/98.4 67.0/87.2/92.498.1/100/10089.9/98.6/99.477.6/94.8/97.7 61.1/84.5/90.693.4/99.8/99.9 84.1/96.5/98.1\nSuper-Large ModelsCoCa 2.1B – – – – 66.3/86.2/91.8 51.2/74.2/82.0 92.5/99.5/99.9 80.4/95.7/97.7BEiT-3 1.9B 84.8/96.5/98.3 67.2/87.7/92.8 98.0/100/100 90.3/98.7/99.5– – 94.9/99.9/100.0 81.5/95.6/97.8\nTable 3: Results of text-retrieval (TR) and image-retrieval (IR) on COCO and Flickr30K. † denotes our reproduced\nresults with the officially released models. In more data setting, we use Conceptual 12M dataset (CC-12M) (Chang-\npinyo et al., 2021) and LAION (Schuhmann et al., 2022) as additional datasets. More details are explained in\nAppendix B. Giant models with over 1B parameters (e.g., BEiT-3) are in grey since they are not directly comparable\nwith other models.\nImageNet\nFood101\nCIFAR10\nCIFAR100\nStanfordCars\nAircraft\nDTD\nOxfordiiitPets\nFlowers102\nMNIST\nSTL10\nCountry211\nSun397\nSST\nCaltech101\nGTSRB\nPCAM\nFER2013\nCLIP B/16-224px80.292.896.2 83.1 86.759.579.293.1 97.1 99.099.030.178.475.594.7 86.6 83.5 69.5\nFLA V A B/16-224px75.5 88.5 92.9 77.7 70.9 47.3 77.3 84.8 98.199.098.9 28.9 82.1 57.1 95.7 79.585.361.1\nDaVinci B/16-224px77.7 90.1 94.0 80.1 74.6 49.6 78.3 88.2 96.999.0 99.229.9 – – – – – –\nX-FMbase B/16-224px81.290.597.4 86.2 88.347.480.090.2 96.499.0 99.224.993.960.697.1 90.982.472.6\nTable 4: Linear evaluation performance of four foundation models over 18 datasets.B/16-224px means base\nsize model, 16*16 patches, and 224*224 resolution, respectively. The best performance is identified with bold.\nofficial ones. From the results, we have the follow-\ning observations.\nFirst, both designs (stop gradient and vision-\nlanguage guided MIM) bring improvements, and\nthe combination can make further improvements\non all three downstream tasks (column 10 vs. oth-\ners). Second, without separated language represen-\ntations, models always perform worse on language\nunderstanding tasks (column 10 vs. 2,3,4). Be-\nsides, the separate language representations in the\nIMLM task on image-text data are helpful for multi-\nmodal tasks (column 2 vs. 4). As we point out in\nsection 1, the fusion encoder can concentrate on\nlearning the alignments between language and vi-\nsion features instead of predicting masked tokens\nwith clues from other visible text tokens. Although\nS-ITM shows slight side effects (column 4 vs. 3),\nstopping the gradients of language representation\nin the fusion encoder is necessary to simultane-\nously achieve strong language understanding and\nvision-language understanding capability. Third,\nthe vision-language guided MIM task is useful for\nboth vision-language and vision learning (column\n10 vs. 6). Meanwhile, the targets in our MIM task\nare better than the BEiTv2 tokenizer (column 10\nvs. 7). Four, X-FM is much better than a naive\nmulti-task learning strategy for a foundation model,\ncompared with which, X-FMbase improves an aver-\nage of 0.9%, 1.7% and 1.6% on language, vision,\nand vision-language tasks, respectively (column 10\nvs. 9). Five, X-FM is also better than foundation\nmodels specifically designed for language, vision,\nand vision-language tasks with the same training\ncorpus (column 10 vs. 1,5,8).\n5 Limitations and Potential Risks\nLimitations. Like most existing work on foun-\ndation models, the entire project consumed over\n5 A100 GPU years on a computing cluster with\nhigh electricity costs, although we only tested base\nmodels. There is still potential for efficiency im-\nprovement through sparse attention (Zaheer et al.,\n2020) or the lottery ticket hypothesis (Frankle and\nCarbin, 2018). We will explore the techniques to\nimprove the training efficiency and reduce the car-\nbon footprint so that we can adhere to the proposals\non “green” deep learning (Schwartz et al., 2020;\nXu et al., 2021).\n558\nX-FMbase\nRoBERTa† S-MLM S-ITM wostop BEiTv2† woMIM wBEiTv2 Tokenizer X2-VLM† Multi-taskALLTask Eval. 1 2 3 4 5 6 7 8 9 10\nMNLI FT 87.7 87.4 87.3 87.7 – – – – 87.4 87.6CoLA FT 63.2 61.6 63.6 64.2 – – – – 62.2 65.2MRPC FT 90.7 92.2 91.1 90.7 – – – – 92.0 92.5QQP FT 91.5 91.6 91.6 91.6 – – – – 91.6 91.6SST-2 FT 95.0 95.1 94.2 94.6 – – – – 94.4 95.3QNLI FT 93.1 93.0 93.2 92.5 – – – – 92.8 92.9RTE FT 80.9 79.1 81.6 81.2 – – – – 79.8 81.9STS-B FT 90.9 90.7 90.7 90.4 – – – – 90.1 90.8\nLanguage Avg. 86.6 86.4 86.7 86.6 – – – – 86.3 87.2\nImageNet FT – – – – 85.5 84.8 85.0 – 85.0 85.3ImageNet LE – – – – 80.5 79.1 79.4 – 79.3 81.1Food101 LE – – – – 88.2 86.9 87.2 – 86.9 88.7CIFAR10 LE – – – – 95.3 96.6 96.5 – 96.6 97.5CIFAR100 LE – – – – 81.5 83.3 83.9 – 84.1 86.9Pets LE – – – – 93.1 88.1 88.5 – 88.2 90.7DTD LE – – – – 78.4 77.7 76.9 – 78.0 78.7Flowers102 LE – – – – 95.7 94.1 94.5 – 94.2 97.1\nVision Avg. – – – – 87.3 86.3 86.5 – 86.5 88.2\nVQAv2 FT – 78.8 78.5 78.7 – 78.3 78.2 78.0 78.2 78.6NLVR2 FT – 86.3 86.0 86.4 – 85.9 85.5 86.2 86.1 86.7Flickr30K TR R@1 ZS – 88.3 87.2 87.1 – 87.1 87.2 87.7 85.0 89.3Flickr30K IR R@1 ZS – 76.6 74.9 75.8 – 76.1 75.3 75.1 75.6 77.4Flickr30K TR R@1 FT – 97.5 97.0 97.2 – 96.4 96.7 97.0 97.0 97.7Flickr30K IR R@1 FT – 87.4 86.9 87.3 – 86.2 86.6 86.2 86.4 87.4COCO TR R@1 ZS – 72.0 72.1 70.5 – 73.0 72.1 73.2 69.9 72.8COCO IR R@1 ZS – 58.4 57.1 57.7 – 58.2 57.7 57.7 56.5 59.0COCO TR R@1 FT – 81.2 80.2 80.9 – 80.6 80.1 80.3 80.0 81.2COCO IR R@1 FT – 64.2 63.4 63.6 – 63.7 63.0 63.1 63.0 64.0\nVision-Language Avg. – 79.1 78.3 78.5 – 78.6 78.2 78.5 77.8 79.4\nTable 5: Ablation studies on vision, language, and vision-language tasks.We use the same settings as Table 2.\n“ALL” means we use both of our proposed techniques. To compare fairly, we pre-train all variants with the same\ndata at the same settings for both pre-training and fine-tuning. Avg. means the average score.\nMethod # Params VQA NLVR2 RefCOCO+test-dev test-std dev test-P val testAd testBd\nALBEF 210M 74.5 74.7 80.2 80.5 – – –VLMobase 175M 76.6 76.9 82.8 83.3 – – –METER 341M 77.7 77.6 82.3 83.1 – – –VL-BEiT 175M 77.5 77.8 81.9 82.7 – – –BLIPbase 240M 78.2 78.2 82.5 83.1 – – –X-VLM 216M 78.1 78.1 84.2 84.2 80.2 86.4 71.0OFAbase 182M 78.0 78.1 – – 81.4 87.2 74.3OmniVL 288M 78.3 78.4 – – – – –X2-VLMbase 255M 79.2 79.385.9 86.185.489.2 77.3X-FMbase 327M 79.1 79.2 86.3 86.584.889.7 79.1\nMore DataSimVLMbase 273M 77.9 78.1 81.7 81.8 – – –X2-VLMbase 255M 80.4 80.2 86.2 87.0 85.2 90.3 78.4X-FMbase 327M 80.5 80.4 87.6 88.4 86.1 90.4 79.8\nSuper-Large ModelsCoCa 2.1B 82.3 82.3 86.1 87.0 – – –BEiT-3 1.9B 84.2 84.0 91.5 92.6 – – –\nTable 6: Results on VQA, visual reasoning and visual\ngrounding. Giant models with over 1B parameters (e.g.,\nCoCa and BEiT-3) are in grey because they are not\ndirectly comparable with other models.\nDue to considerations of fair comparisons and\ncomputational resources, we did not try super-large\nmodels which use at least 1.9B or more parameters\nlike BEITv3 (Wang et al., 2022d), CoCa (Yu et al.,\n2022) and PaLI (Chen et al., 2022). We also did not\npre-train large size model on large-scale datasets.\nHowever, scalability is also an important factor for\nfoundation models. We leave the investigations to\nfuture work.\nPotential Risks. The image-text pairs use for\ntraining our model are mostly derived from lexical\ndatabases and image queries in English, resulting in\nsource material with a North American or Western\nEuropean bias.\n6 Conclusion\nIn this work, we address the problem of how to\nbuild a general foundation model that can perform\nthe best for all the understanding tasks of language,\nvision, and vision-language. We propose a new\nmethod for training general foundation model with\ntwo new and effective techniques, bringing in X-\nFM, to learn rich language, vision, and vision-\nlanguage representations at the same time. Experi-\nmental results demonstrate that X-FM outperforms\nother general foundation models by a large margin.\nMoreover, X-FM can even be better than or compa-\nrable to the SOTA foundation models specifically\ndesigned for language, vision, or vision-language\nunderstanding tasks.\n559\nReferences\nEneko Agirre, Lluís Màrquez, and Richard Wicen-\ntowski, editors. 2007. Proceedings of the Fourth\nInternational Workshop on Semantic Evaluations\n(SemEval-2007). Association for Computational Lin-\nguistics, Prague, Czech Republic.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\net al. 2022. Flamingo: a visual language model for\nfew-shot learning. arXiv preprint arXiv:2204.14198.\nHangbo Bao, Li Dong, and Furu Wei. 2021. BEiT: Bert\npre-training of image transformers. arXiv preprint.\nHangbo Bao, Wenhui Wang, Li Dong, and Furu Wei.\n2022. Vl-beit: Generative vision-language pretrain-\ning. arXiv preprint arXiv:2206.01127.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. In TAC.\nJoachim Bingel and Anders Søgaard. 2017. Identi-\nfying beneficial task relations for multi-task learn-\ning in deep neural networks. arXiv preprint\narXiv:1702.08303.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\n2014. Food-101–mining discriminative components\nwith random forests. In European conference on\ncomputer vision, pages 446–461. Springer.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé\nJégou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. 2021. Emerging properties in self-supervised\nvision transformers. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages\n9650–9660.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail\nvisual concepts. In Conference on Computer Vision\nand Pattern Recognition (CVPR).\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-\ngiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas\nBeyer, et al. 2022. Pali: A jointly-scaled mul-\ntilingual language-image model. arXiv preprint\narXiv:2209.06794.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. UNITER: Universal image-text\nrepresentation learning. In European Conference on\nComputer Vision (ECCV).\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos,\nSammy Mohamed, and Andrea Vedaldi. 2014. De-\nscribing textures in the wild. In 2014 IEEE Confer-\nence on Computer Vision and Pattern Recognition,\nCVPR 2014, Columbus, OH, USA, June 23-28, 2014,\npages 3606–3613. IEEE Computer Society.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges Workshop,\npages 177–190. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nShizhe Diao, Wangchunshu Zhou, Xinsong Zhang,\nand Jiawei Wang. 2022. Prefix language mod-\nels are unified modal learners. arXiv preprint\narXiv:2206.07699.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net.\nJonathan Frankle and Michael Carbin. 2018. The lottery\nticket hypothesis: Finding sparse, trainable neural\nnetworks. arXiv preprint arXiv:1803.03635.\n560\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nBill Dolan. 2007. The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the\nACL-PASCAL Workshop on Textual Entailment and\nParaphrasing, pages 1–9, Prague. Association for\nComputational Linguistics.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the V in VQA\nmatter: Elevating the role of image understanding in\nvisual question answering. In 2017 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2017, Honolulu, HI, USA, July 21-26, 2017 , pages\n6325–6334. IEEE Computer Society.\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo\nGiampiccolo, Bernardo Magnini, and Idan Szpektor.\n2006. The second pascal recognising textual entail-\nment challenge. In Proceedings of the Second PAS-\nCAL Challenges Workshop on Recognising Textual\nEntailment, volume 7.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-\notr Dollár, and Ross Girshick. 2022. Masked autoen-\ncoders are scalable vision learners. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 16000–16009.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss B. Girshick. 2020. Momentum contrast for un-\nsupervised visual representation learning. In 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, Seattle, WA, USA,\nJune 13-19, 2020, pages 9726–9735. IEEE.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: decoding-enhanced\nbert with disentangled attention. In 9th International\nConference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021 . OpenRe-\nview.net.\nShankar Iyer, Nikhil Dandekar, Kornél Csernai, et al.\n2017. First quora dataset release: Question pairs.\ndata. quora. com.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V . Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling up vi-\nsual and vision-language representation learning with\nnoisy text supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of\nProceedings of Machine Learning Research, pages\n4904–4916. PMLR.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nAndrej Karpathy and Fei-Fei Li. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2015, Boston, MA, USA,\nJune 7-12, 2015, pages 3128–3137. IEEE Computer\nSociety.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational Journal of Computer Vision (IJCV).\nAlex Krizhevsky, Geoffrey Hinton, et al. 2009. Learn-\ning multiple layers of features from tiny images.\narXiv preprint.\nAlina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,\nStefan Popov, Matteo Malloci, Alexander Kolesnikov,\net al. 2018. The open images dataset v4: Uni-\nfied image classification, object detection, and vi-\nsual relationship detection at scale. arXiv preprint\narXiv:1811.00982.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. arXiv preprint arXiv:2201.12086.\nJunnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak\nGotmare, Shafiq Joty, Caiming Xiong, and Steven\nHoi. 2021a. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nIn Conference on Neural Information Processing Sys-\ntems (NeurIPS).\nWei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao\nLiu, Jiachen Liu, Hua Wu, and Haifeng Wang. 2021b.\nUNIMO: Towards unified-modal understanding and\ngeneration via cross-modal contrastive learning. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 2592–\n2607, Online. Association for Computational Lin-\nguistics.\n561\nWei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao\nLiu, Jiachen Liu, Hua Wu, and Haifeng Wang. 2021c.\nUNIMO: Towards unified-modal understanding and\ngeneration via cross-modal contrastive learning. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 2592–\n2607, Online. Association for Computational Lin-\nguistics.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, et al. 2020. Oscar: Object-\nsemantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision\n(ECCV).\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft COCO:\nCommon objects in context. In European Conference\non Computer Vision (ECCV).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining\napproach. arXiv preprint.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. In\nAdvances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-\n14, 2019, Vancouver, BC, Canada, pages 13–23.\nMaria-Elena Nilsback and Andrew Zisserman. 2008.\nAutomated flower classification over a large number\nof classes. In 2008 Sixth Indian Conference on Com-\nputer Vision, Graphics & Image Processing, pages\n722–729. IEEE.\nVicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\n2011. Im2text: Describing images using 1 million\ncaptioned photographs. In Advances in Neural In-\nformation Processing Systems 24: 25th Annual Con-\nference on Neural Information Processing Systems\n2011. Proceedings of a meeting held 12-14 December\n2011, Granada, Spain, pages 1143–1151.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman,\nand C. V . Jawahar. 2012. Cats and dogs. In 2012\nIEEE Conference on Computer Vision and Pattern\nRecognition, Providence, RI, USA, June 16-21, 2012,\npages 3498–3505. IEEE Computer Society.\nZhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and\nFuru Wei. 2022. Beit v2: Masked image model-\ning with vector-quantized visual tokenizers. arXiv\npreprint arXiv:2208.06366.\nJason Phang, Thibault Févry, and Samuel R. Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. ArXiv,\nabs/1811.01088.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research , pages 8748–8763.\nPMLR.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research\n(JMLR).\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nHamid Rezatofighi, Nathan Tsoi, JunYoung Gwak,\nAmir Sadeghian, Ian D. Reid, and Silvio Savarese.\n2019. Generalized intersection over union: A metric\nand a loss for bounding box regression. In IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, pages 658–666. Computer Vision Foundation /\nIEEE.\nTal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi\nZelnik-Manor. 2021. Imagenet-21k pretraining for\nthe masses.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, An-\ndrej Karpathy, Aditya Khosla, Michael Bernstein,\net al. 2015. Imagenet large scale visual recognition\nchallenge. International journal of computer vision,\n115(3):211–252.\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis,\nMitchell Wortsman, et al. 2022. Laion-5b: An open\nlarge-scale dataset for training next generation image-\ntext models. arXiv preprint arXiv:2210.08402.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni. 2020. Green ai. Communications of the\nACM, 63(12):54–63.\nShuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng,\nGang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.\n2019. Objects365: A large-scale, high-quality\ndataset for object detection. In 2019 IEEE/CVF In-\nternational Conference on Computer Vision, ICCV\n2019, Seoul, Korea (South), October 27 - November\n2, 2019, pages 8429–8438. IEEE.\n562\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2556–2565,\nMelbourne, Australia. Association for Computational\nLinguistics.\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami,\nGuillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. 2021. Flava: A founda-\ntional language and vision alignment model. ArXiv\npreprint, abs/2112.04482.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019a. A corpus for\nreasoning about natural language grounded in pho-\ntographs. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6418–6428, Florence, Italy. Association for\nComputational Linguistics.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019b. A corpus for\nreasoning about natural language grounded in pho-\ntographs. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6418–6428, Florence, Italy. Association for\nComputational Linguistics.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced represen-\ntation through knowledge integration. arXiv preprint\narXiv:1904.09223.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for Com-\nputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nJunke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo,\nLuowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-\nGang Jiang, and Lu Yuan. 2022a. Omnivl: One foun-\ndation model for image-language and video-language\ntasks. arXiv preprint arXiv:2209.07526.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022b. Ofa: Unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In Inter-\nnational Conference on Machine Learning , pages\n23318–23340. PMLR.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,\nand Hongxia Yang. 2022c. Unifying architectures,\ntasks, and modalities through a simple sequence-\nto-sequence learning framework. arXiv preprint\narXiv:2202.03052.\nWeiyao Wang, Du Tran, and Matt Feiszli. 2020. What\nmakes training multi-modal classification networks\nhard? In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n12695–12705.\nWenhui Wang, Hangbo Bao, Li Dong, Johan\nBjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit\nSom, et al. 2022d. Image as a foreign language: Beit\npretraining for all vision and vision-language tasks.\narXiv preprint arXiv:2208.10442.\nWenhui Wang, Hangbo Bao, Li Dong, and Furu Wei.\n2021a. Vlmo: Unified vision-language pre-training\nwith mixture-of-modality-experts. ArXiv preprint,\nabs/2111.02358.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\nlia Tsvetkov, and Yuan Cao. 2021b. Simvlm: Simple\nvisual language model pretraining with weak super-\nvision. CoRR, abs/2108.10904.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\nlia Tsvetkov, and Yuan Cao. 2021c. Simvlm: Simple\nvisual language model pretraining with weak super-\nvision. arXiv preprint.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nChen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu,\nAlan Yuille, and Christoph Feichtenhofer. 2022a.\nMasked feature prediction for self-supervised visual\npre-training. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition,\npages 14668–14678.\n563\nLonghui Wei, Lingxi Xie, Wengang Zhou, Houqiang Li,\nand Qi Tian. 2022b. Mvp: Multimodality-guided vi-\nsual pre-training. arXiv preprint arXiv:2203.05175.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nHaiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye,\nYuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei\nWang, et al. 2023. mplug-2: A modularized multi-\nmodal foundation model across text, image and video.\narXiv preprint arXiv:2302.00402.\nJingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou,\nand Lei Li. 2021. A survey on green deep learning.\nArXiv preprint, abs/2111.05193.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\nCoca: Contrastive captioners are image-text founda-\ntion models. arXiv preprint arXiv:2205.01917.\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C\nBerg, and Tamara L Berg. 2016. Modeling context\nin referring expressions. In European Conference on\nComputer Vision, pages 69–85. Springer.\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong\nHuang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen\nLiu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang,\nJianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang,\nMichael Zeng, Luowei Zhou, and Pengchuan Zhang.\n2021. Florence: A new foundation model for com-\nputer vision. arXiv preprint.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33:17283–17297.\nYan Zeng, Xinsong Zhang, and Hang Li. 2021.\nMulti-grained vision language pre-training: Align-\ning texts with visual concepts. ArXiv preprint ,\nabs/2111.08276.\nYan Zeng, Xinsong Zhang, Hang Li, Jiawei Wang,\nJipeng Zhang, and Wangchunshu Zhou. 2022. X 2-\nvlm: All-in-one pre-trained model for vision-\nlanguage tasks. arXiv preprint arXiv:2211.12402.\nXinsong Zhang, Pengshuai Li, and Hang Li.\n2020. Ambert: A pre-trained language model\nwith multi-grained tokenization. arXiv preprint\narXiv:2008.11869.\nJinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang,\nHongsheng Li, Xiaogang Wang, and Jifeng Dai.\n2022. Uni-perceiver-moe: Learning sparse gener-\nalist models with conditional moes. arXiv preprint\narXiv:2206.04674.\nXizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Xi-\naogang Wang, Hongsheng Li, Xiaohua Wang, and\nJifeng Dai. 2021. Uni-perceiver: Pre-training unified\narchitecture for generic perception for zero-shot and\nfew-shot tasks. arXiv preprint arXiv:2112.01522.\n564\nA Comparison of Foundation Models\nTable 7 shows an extensive comparison of recent\nfoundation models and X-FM on multiple axes.\nPrevious work either (i) perform best on uni-modal\ntasks (Liu et al., 2019; Peng et al., 2022) or vision-\nlanguage tasks (Zeng et al., 2021, 2022); (2) tar-\nget a specific uni-modal domain along with part\nof vision-and-language tasks (Wang et al., 2021a;\nRadford et al., 2021; Jia et al., 2021; Wang et al.,\n2021c; Yu et al., 2022; Wang et al., 2022b; Diao\net al., 2022); or (3) target all domains but cannot\nperform best on all the tasks (Li et al., 2021c; Singh\net al., 2021; Zhu et al., 2022). Our model, X-FM,\nis a general foundation model that can perform the\nbest for all the understanding tasks of language,\nvision, and vision language.\nB Details of Pre-training Datasets\nWe conduct our experiments on several widely\nused public datasets, consisting of two in-domain\ndatasets, COCO (Lin et al., 2014) and Visual\nGenome (VG) (Krishna et al., 2017), and two out-\nof-domain datasets, SBU Captions (Ordonez et al.,\n2011) and Conceptual Captions (CC) (Sharma\net al., 2018). Following X-VLM (Zeng et al., 2021,\n2022), we use annotations of objects and regions\nfrom RefCOCO (Yu et al., 2016), Objects365 (Shao\net al., 2019) and OpenImages (Kuznetsova et al.,\n2018). We also include uni-modal data, RoBERTa\ncorpus (Liu et al., 2019), C4 datasets (Raffel et al.,\n2020) and Imagenet21K (Ridnik et al., 2021).\nFor our “more data\" setting, we scale up the\npre-training dataset by including image-text pairs\nfrom Conceptual 12M dataset (CC-12M) (Chang-\npinyo et al., 2021) and LAION (Schuhmann et al.,\n2022). Thanks to LAION, we can use a large-scale\npublic corpus of image-text pairs. However, we\nnote that there are amounts of “low-quality\" im-\nage text pairs, as it is only filtered by the CLIP\nscore. The clip score is deceptive when an image\ncontains word tokens in its caption. Therefore, we\napply three filters, OCR filter, text filter, and image\nfilter, to capture “high-quality\" image-text pairs\nfrom LAION. Note that we only use English data\nin LAION. The OCR filter will remove an image\n(image-text pair) when its OCR text contains more\nthan four words or any token in the caption. The\ntext filter will remove a text image (image-text pair)\nif it is an address or contains only digits or symbols.\nThe image filter will remove an image (image-text\npair) if the shorter edge is smaller than 224 pixels,\nand also remove an image (image-text pair) if the\nheight/width or width/height ratio is greater than\n3. Finally, we have 1.3B paired data after all three\nfilters. Statistics of the pre-training datasets are\nshown in Table 8.\nC Details of Downstream Tasks\nWe report overall performance on eight language\ntasks from GLUE (Wang et al., 2019), eight vi-\nsion tasks following OmniVL (Wang et al., 2022a)\n(More image classification tasks can be found in\nAppendix ??.), four multi-modal tasks, which are\ntext-image retrieval on MSCOCO and Flickr, visual\nquestion answering (VQA (Goyal et al., 2017)), vi-\nsual reasoning (NLVR2 (Suhr et al., 2019a)) and vi-\nsual grounding (RefCOCO+ (Yu et al., 2016)). For\nimage-text retrieval task, we report both zero-shot\nresults and fine-tuned results. For the ImageNet\nclassification task, we report both linear evaluation\nresults and fine-tuning results. The other vision\ntasks are evaluated in the linear evaluation setting.\nAll the other tasks are evaluated in the fine-tuning\nsetting. Because the image resolution differs be-\ntween pre-training and fine-tuning, the position\nparameters are adapted using linear interpolation.\nFor all downstream tasks, we apply random re-\nsize crops and horizontal flips augmentation for the\nimages during training. More details of network ar-\nchitectures and hyper-parameters setups are given\nin Appendix D.\nLanguage Understanding.\nWe conduct experiments on GLUE bench-\nmark including MNLI (Williams et al., 2018),\nCoLA (Warstadt et al., 2019), MRPC (Dolan and\nBrockett, 2005), QQP (Iyer et al., 2017), SST-\n2 (Socher et al., 2013), QNLI (Rajpurkar et al.,\n2016), RTE (Dagan et al., 2005; Haim et al., 2006;\nGiampiccolo et al., 2007; Bentivogli et al., 2009),\nand STS-B (Agirre et al., 2007). We follow the\npractice of BERT (Devlin et al., 2019; Liu et al.,\n2019) and feed the input into the language encoder,\nand the hidden state of the [CLS] is fed into a new\nmulti-class linear classifier or regression head.\nVision Understanding.\nWe conduct vision experiments on both fine-tuning\nand linear evaluation (linear eval). The linear eval-\nuation follows a common practice (Caron et al.,\n2021; He et al., 2020; Singh et al., 2021) in self-\n565\nMethods Multimodal data Pretraining Objectives Fusion Arch. Target Modalities\npublic dataset(s) sizeContr. ITM BBP (M/P)LM UnimodalST CT MTV CV&L MV&L L\nRoBERTa (Liu et al., 2019)– – – – – – – MLM – – – – – – /enc-33BEiTv2 (Peng et al., 2022)– – – – – – – MIM – – – /enc-33– – –X-VLM (Zeng et al., 2021, 2022)/enc-33Combination 5M/enc-33 /enc-33 /enc-33MLM – – /enc-33– – /enc-33 /enc-33–VLMo (Wang et al., 2021a)/enc-33Combination 5M/enc-33 /enc-33– MLM MLM+MIM – – /enc-33– /enc-33 /enc-33–CLIP (Radford et al., 2021)/enc-37WebImageText 400M/enc-33– – – – – – – /enc-33 /enc-33– –ALIGN (Jia et al., 2021)/enc-37JFT 1.8B /enc-33– – – – – – – /enc-33 /enc-33– –SimVLM (Wang et al., 2021c)/enc-37JFT 1.8B – – – PrefixLM PrefixLM /enc-33– – ∗ – /enc-33 /enc-33CoCa (Yu et al., 2022)/enc-37JFT 4.8B /enc-33– – LM – /enc-33– – /enc-33 /enc-33 /enc-33–UNIMO-2 (Li et al., 2021c)/enc-33Combination 5M– /enc-33– MLM VCL /enc-33– – /enc-33 /enc-33 /enc-33 /enc-33OFA (Wang et al., 2022b)/enc-33Combination 15M– – – LM LM /enc-33– – ∗ – /enc-33 /enc-33DaVinci (Diao et al., 2022)/enc-33Combination 46M– – – PrefixLM + PrefixIM PrefixLM/enc-33– – /enc-33– /enc-33 /enc-33FLA V A (Singh et al., 2021)/enc-33Combination 70M/enc-33 /enc-33– MLM MLM+MIM /enc-33– – /enc-33 /enc-33 /enc-33 /enc-33Uni-Perceiver-MoE (Zhu et al., 2022)/enc-33Combination 116M– /enc-33– LM+MLM LM+MLM+Classify./enc-33– – /enc-33 /enc-33 /enc-33 /enc-33X-FM /enc-33Combination 5M/enc-33 /enc-33 /enc-33MLM+MIM MLM+MIM– /enc-33– /enc-33 /enc-33 /enc-33 /enc-33\nSuper-Large ModelsFlamingo (Alayrac et al., 2022)/enc-37Combination 2.2B– – – LM – /enc-33– – – /enc-33 /enc-33–BEiT-v3 (Wang et al., 2022d)/enc-33Combination 21M– – – MLM MLM+MIM – – /enc-33∗ /enc-33 /enc-33–PaLI (Chen et al., 2022)/enc-37WebImageText 41B– – – LM – /enc-33– – /enc-33 /enc-33 /enc-33 /enc-33\nTable 7: Comparison of recent foundation models in different modalities.Contr. indicates contrastive learning.\nITM is short for image-text matching. BBP represents boundary box prediction. (M/P)LM means image-conditioned\n(masked/prefix) language modeling. V , CV&L, MV&L and L stand for vision tasks, cross-modal retrieval tasks,\nmulti-modal fusion tasks and language tasks respectively. ST, CT and MT are abbreviations for single Transformer,\ncross-attention Transformer and multiway Transformer. VCL stands for visual contrastive learning. ∗means the\nmodality is partially targeted (SimVLM and OFA include ImageNet.). Giant models with over 1B parameters (e.g.\nBEiT-3) are in grey since they are not directly comparable with other models.\nDataset # Images # Texts # Objects # Regions\nCOCO 0.11M 0.55M 0.45M -\nVG 0.10M - 2.0M 3.7M\nSBU 0.86M 0.86M - -\nCC-3M 2.9M 2.9M - -\nObjects365 0.58M - 2.0M -\nOpenImages 1.7M - 4.2M -\nC4 - 800GB - -\nRoBERTa Corpus- 160GB - -\nImageNet-21k 14M - - -\nMore Data\nCC-12M 11.1M 11.1M - -\nLAION 1.3B 1.3B - -\nTable 8: Statistics of the pre-training datasets.\nsupervised learning to evaluate the representation\nquality, where the pre-trained backbone model\nis frozen, and an MLP head is appended on top\nof it. We choose 7 popular datasets following\nOmnVL (Wang et al., 2022a): ImageNet (Rus-\nsakovsky et al., 2015), Food101 (Bossard et al.,\n2014), CIFAR10 (Krizhevsky et al., 2009), CI-\nFAR100 (Krizhevsky et al., 2009), DTD (Cimpoi\net al., 2014), Pets (Parkhi et al., 2012) and Flow-\ners102 (Nilsback and Zisserman, 2008).\nVision-Language Understanding.\nImage-Text RetrievalWe evaluate X-FM on both\nMSCOCO and Flickr30K datasets. We adopt the\nwidely used Karpathy split (Karpathy and Li, 2015)\nfor both datasets. Following the previous work (Li\net al., 2021a; Zeng et al., 2021, 2022), we first\nencode images and texts separately and calculate\ns(I,T ) to obtain the top-kcandidates, and then use\nthe fusion encoder to re-rank the candidates.\nVisual Question AnsweringThe task requires the\nmodel to predict an answer given an image and\na question. We evaluate X-FM on the VQA v2.0\ndataset (Goyal et al., 2017). Following the previous\nwork (Zeng et al., 2021), we use a Transformer\ndecoder to generate answers based on the outputs of\nthe fusion module. The decoder network shares the\nsame network architecture with the fusion encoder.\nNote that we use an image resolution of 768*768\nfor the final result of X-FMbase, and use an image\nresolution of 480*480 for X-FM base in ablation\nstudies for efficient fine-tuning.\nVisual ReasoningWe evaluate X-FM on a widely\nused benchmark NLVR2 (Suhr et al., 2019b). The\ntask allows the model to determine whether a text\ndescribes the relations between two images. Fol-\nlowing previous work (Wang et al., 2021a; Bao\net al., 2022), we formulate the triplet input into two\nimage-text pairs, each containing the text descrip-\ntion and an image. We then concatenate the final\noutput [CLS] features of the fusion module of the\ntwo pairs to predict the label.\nVisual Grounding We evaluate X-FM on Ref-\nCOCO+ (Yu et al., 2016). Given an image and\na text description as input, the final output [CLS]\nfeatures of the fusion module is utilized to predict\nthe bounding box (cx,cy,w,h ), i.e. the normal-\nized center coordinates, width, and height.\n566\nModel Param Hidden Layers\nTotal Trans. Vision Text Fusion\nX-FMbase 327M 284M 768 12 12 12\nTable 9: Size variants of X-FM. All modules consist of\ntransformer layers. Param indicates the parameter. Total\nmeans the total parameter number, and Trans. indicates\nthe parameter number for Transformer layers.\nD Details of hyper parameters\nPre-training X-FMbase is implemented with a\n12-layer language encoder, a 12-layer vision en-\ncoder, and a 12-layer fusion encoder, 768 dimen-\nsions for hidden states, 3072 for intermediate size,\nand 128 for maximum input length. We initialize\nthe language encoder with RoBERTa and the vision\nencoder with BEiTv2. The weight decay is set to\n0.01 with β1 = 0.9,β2 = 0.98. The learning rate\nis 1e-4 with a warm-up period for the first 2500\nsteps and then linearly decayed to 0. In each batch,\nthere are 3072 image-text pairs, 3072 images, and\n8192 text-only sentences. We use center-crop to\nresize each image to the size of 224 ×224. The\nmodel sizes and default hyper-parameter settings\nare shown in Table 9 and Table 10, respectively.\nconfig value\noptimizer AdamW\nlearning rate 1e-4\nweight decay 0.01\noptimizer momentum β1,β2=0.9,0.999\nlanguage batch size 8192\nvision batch size 3072\nvision-language batch size 3072\nlearning rate schedule linear decay\nwarmup steps 2500\ntraining steps 200k\naugmentation RandomResizedCrop\nimage res 224*224\npatch size 16\ntext length for MLM 128\ntext length for IMLM 30\nTable 10: Pre-training setting.\nFine-tuning The learning rate is ∈{1e-5, 2e-\n5, 5e-5} and our model is optimized by AdamW.\nBecause the image resolution differs between pre-\ntraining and fine-tuning, the position parameters\nare adapted using linear interpolation. For all down-\nstream tasks, we apply random resize crops and\nhorizontal flips augmentation during training. The\ndefault settings for text classification, image clas-\nsification and vision-language understanding are\nshown in Tables 11, 12, 13 and 14, respectively.\nNote that the resolution for VQA is different as\ndescribed in Section C.\nconfig value\noptimizer AdamW\nlearning rate {1e-5, 2e-5, 5e-5}\nweight decay 0.0\noptimizer momentum β1,β2=0.9,0.999\nbatch size {16, 32, 64}\nlearning rate schedule linear decay\nwarmup ratio 0.0\ntraining epochs {5, 10, 20}\nTable 11: Text classification: GLUE setting.\nconfig value\noptimizer AdamW\nlearning rate [2e-5, 4e-5]\nweight decay 0.01\noptimizer momentum β1,β2=0.9,0.999\nbatch size [256, 2048]\nlearning rate schedule linear decay\nwarmup rate 0.1\ntraining epochs 100\naugmentation RandomResizedCrop\nimage res 224*224\npatch size 16\nTable 12: Image classification: Linear probing setting.\n567\nconfig value\noptimizer AdamW\nlearning rate 4e-5\nminimal learning rate 1e-7\nweight decay 0.01\noptimizer momentum β1,β2=0.9,0.999\nbatch size 1024\nlearning rate schedule linear decay\nwarmup rate 0.1\ntraining epochs 100\naugmentation RandomResizedCrop\nimage res 224*224\npatch size 16\nlabel smoothing 0.1\nmixup prob. 1.0\ncutmix prob. 1.0\nTable 13: ImageNet classification: Fine-tuning setting.\nconfig value\noptimizer AdamW\nlearning rate {1e-5, 2e-5, 5e-5}\nweight decay 0.01\noptimizer momentum β1,β2=0.9,0.999\nbatch size {64, 192, 512}\nlearning rate schedule linear decay\nwarmup rate 0.1\ntraining epochs {10, 15, 20}\naugmentation RandomResizedCrop\nimage res 384*384\npatch size 16\nTable 14: Vision-Language understanding: fine-tuning\nsetting.\n568",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7791871428489685
    },
    {
      "name": "Artificial intelligence",
      "score": 0.590742290019989
    },
    {
      "name": "Language model",
      "score": 0.5659208297729492
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.5480952858924866
    },
    {
      "name": "Natural language processing",
      "score": 0.5410666465759277
    },
    {
      "name": "Encoder",
      "score": 0.46434837579727173
    },
    {
      "name": "Machine vision",
      "score": 0.45585712790489197
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.45215722918510437
    },
    {
      "name": "Construct (python library)",
      "score": 0.44289955496788025
    },
    {
      "name": "Machine learning",
      "score": 0.3272906541824341
    },
    {
      "name": "Programming language",
      "score": 0.21596258878707886
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 13
}