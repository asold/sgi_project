{
    "title": "Shrinking exponential language models",
    "url": "https://openalex.org/W2050971845",
    "year": 2009,
    "authors": [
        {
            "id": "https://openalex.org/A2169620266",
            "name": "Stanley F. Chen",
            "affiliations": [
                "IBM Research - Thomas J. Watson Research Center"
            ]
        },
        {
            "id": "https://openalex.org/A2169620266",
            "name": "Stanley F. Chen",
            "affiliations": [
                "IBM (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2069699492",
        "https://openalex.org/W2489195900",
        "https://openalex.org/W2159077101",
        "https://openalex.org/W1823613926",
        "https://openalex.org/W182840523",
        "https://openalex.org/W2020073413",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W2097003869",
        "https://openalex.org/W1991696363",
        "https://openalex.org/W169286004",
        "https://openalex.org/W140816929",
        "https://openalex.org/W2127095586",
        "https://openalex.org/W2133486044",
        "https://openalex.org/W2145826596",
        "https://openalex.org/W1718044877",
        "https://openalex.org/W1797288984",
        "https://openalex.org/W2049901611",
        "https://openalex.org/W98731357",
        "https://openalex.org/W2075401516",
        "https://openalex.org/W2111305191",
        "https://openalex.org/W2143866356",
        "https://openalex.org/W2072223048",
        "https://openalex.org/W2024490156",
        "https://openalex.org/W2155388323",
        "https://openalex.org/W1974515274",
        "https://openalex.org/W2076467305",
        "https://openalex.org/W2117793990",
        "https://openalex.org/W2134237567",
        "https://openalex.org/W4302176825",
        "https://openalex.org/W1481113913",
        "https://openalex.org/W2158195707"
    ],
    "abstract": "In (Chen, 2009), we show that for a variety of language models belonging to the exponential family, the test set cross-entropy of a model can be accurately predicted from its training set cross-entropy and its parameter values. In this work, we show how this relationship can be used to motivate two heuristics for \"shrinking\" the size of a language model to improve its performance. We use the first heuristic to develop a novel class-based language model that outperforms a baseline word trigram model by 28% in perplexity and 1.9% absolute in speech recognition word-error rate on Wall Street Journal data. We use the second heuristic to motivate a regularized version of minimum discrimination information models and show that this method outperforms other techniques for domain adaptation.",
    "full_text": null
}