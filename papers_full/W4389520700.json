{
  "title": "Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization",
  "url": "https://openalex.org/W4389520700",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3168406294",
      "name": "Tianshi Che",
      "affiliations": [
        "Auburn University"
      ]
    },
    {
      "id": "https://openalex.org/A2105518551",
      "name": "Ji Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104523023",
      "name": "Yang Zhou",
      "affiliations": [
        "Auburn University"
      ]
    },
    {
      "id": "https://openalex.org/A2149049793",
      "name": "Jiaxiang Ren",
      "affiliations": [
        "Auburn University"
      ]
    },
    {
      "id": "https://openalex.org/A2487805935",
      "name": "Jiwen Zhou",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A4208144787",
      "name": "Victor Sheng",
      "affiliations": [
        "Texas Tech University"
      ]
    },
    {
      "id": "https://openalex.org/A2126666236",
      "name": "Huaiyu Dai",
      "affiliations": [
        "North Carolina State University"
      ]
    },
    {
      "id": "https://openalex.org/A2040419331",
      "name": "Dejing Dou",
      "affiliations": [
        "Boston Consulting Group (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287122891",
    "https://openalex.org/W4318823526",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4312191413",
    "https://openalex.org/W2407374891",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3006555759",
    "https://openalex.org/W4289866503",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2073666908",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3171569334",
    "https://openalex.org/W3168459274",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3007279825",
    "https://openalex.org/W4309118031",
    "https://openalex.org/W4323777339",
    "https://openalex.org/W4312359923",
    "https://openalex.org/W1974752004",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W3176174985",
    "https://openalex.org/W2785523195",
    "https://openalex.org/W4303684276",
    "https://openalex.org/W4381327185",
    "https://openalex.org/W2797069615",
    "https://openalex.org/W3187255254",
    "https://openalex.org/W4293658473",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4294106961",
    "https://openalex.org/W4372260089",
    "https://openalex.org/W3035253236",
    "https://openalex.org/W4360616856",
    "https://openalex.org/W4220738117",
    "https://openalex.org/W3212611649",
    "https://openalex.org/W3172974245",
    "https://openalex.org/W2089345497",
    "https://openalex.org/W2156387975",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2949522309",
    "https://openalex.org/W4318619660",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3101970908",
    "https://openalex.org/W4385574214",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2010518116",
    "https://openalex.org/W4385573776",
    "https://openalex.org/W2027024265",
    "https://openalex.org/W4320067864",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W2025023381",
    "https://openalex.org/W4392175365",
    "https://openalex.org/W4283466980",
    "https://openalex.org/W2889369110",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W2944898646",
    "https://openalex.org/W4224115290",
    "https://openalex.org/W2028175314",
    "https://openalex.org/W2891952073",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W3034696087",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4302069968",
    "https://openalex.org/W3175386621",
    "https://openalex.org/W4378979503",
    "https://openalex.org/W4302307546",
    "https://openalex.org/W3047989515",
    "https://openalex.org/W3166395393",
    "https://openalex.org/W2014902591",
    "https://openalex.org/W104184427",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3138582970",
    "https://openalex.org/W4285787401",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2977517840",
    "https://openalex.org/W2351700241",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2084667297"
  ],
  "abstract": "Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improve performance and efficiency simultaneously. Second, a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further. Extensive experiments based on 10 datasets demonstrate the superb performance (up to 60.8% in terms of accuracy) and efficiency (up to 97.59% in terms of training time) of FedPepTAO compared with 9 baseline approaches. Our code is available at https://github.com/llm-eff/FedPepTAO.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7871â€“7888\nDecember 6-10, 2023 Â©2023 Association for Computational Linguistics\nFederated Learning of Large Language Models with Parameter-Efficient\nPrompt Tuning and Adaptive Optimization\nTianshi Che1â€ , Ji Liu 2â€ âˆ—, Yang Zhou1âˆ—, Jiaxiang Ren1,\nJiwen Zhou3, Victor S. Sheng4, Huaiyu Dai5, Dejing Dou6\n1Auburn University, Auburn, United States,\n2Hithink RoyalFlush Information Network Co., Ltd., Hangzhou, Zhejiang, China,\n3Baidu Inc. Beijing, China, 4Texas Tech University, Lubbock, United States,\n5North Carolina State University, United States, 6Boston Consulting Group, United States.\nAbstract\nFederated learning (FL) is a promising\nparadigm to enable collaborative model train-\ning with decentralized data. However, the\ntraining process of Large Language Models\n(LLMs) generally incurs the update of signif-\nicant parameters, which limits the applicabil-\nity of FL techniques to tackle the LLMs in\nreal scenarios. Prompt tuning can significantly\nreduce the number of parameters to update,\nbut it either incurs performance degradation\nor low training efficiency. The straightfor-\nward utilization of prompt tuning in the FL\noften raises non-trivial communication costs\nand dramatically degrades performance. In\naddition, the decentralized data is generally\nnon-Independent and Identically Distributed\n(non-IID), which brings client drift problems\nand thus poor performance. This paper pro-\nposes a Parameter-efficient prompt Tuning ap-\nproach with Adaptive Optimization, i.e., Fed-\nPepTAO, to enable efficient and effective FL\nof LLMs. First, an efficient partial prompt tun-\ning approach is proposed to improve perfor-\nmance and efficiency simultaneously. Second,\na novel adaptive optimization method is devel-\noped to address the client drift problems on\nboth the device and server sides to enhance\nperformance further. Extensive experiments\nbased on 10 datasets demonstrate the superb\nperformance (up to 60.8% in terms of accu-\nracy) and efficiency (up to 97.59% in terms of\ntraining time) of FedPepTAO compared with 9\nbaseline approaches. Our code is available at\nhttps://github.com/llm-eff/FedPepTAO.\n1 Introduction\nAs a promising paradigm to handle decentralized\ndata, Federated Learning (FL) (Kairouz et al.,\n2021) enables collaborative model training without\ntransferring the raw data across a massive num-\nber of devices. As a bunch of legal restrictions\nâ€  Equal contribution.\nâˆ— Corresponding author: jiliuwork@gmail.com,\nyangzhou@auburn.edu\n(Official Journal of the European Union, 2016; Cal-\nifornians for Consumer Privacy, 2020) have been\nimplemented, aggregating the decentralized data\ninto a central server or data center becomes com-\nplicated or even impossible (Yang et al., 2019).\nFL generally exploits a parameter server module\n(Liu et al., 2023b) to manage the distributed model\nupdates in devices, which only exchanges the pa-\nrameters of the updated models instead of the raw\ndata, between the parameter server and devices.\nLarge Language Models (LLMs) (Devlin et al.,\n2018; Liu et al., 2019b; Brown et al., 2020; Lewis\net al., 2019) have achieved major advances in Nat-\nural Language Processing (NLP) tasks. The scale\nof LLMs can range from 110 million parameters to\n175 billion parameters, which correspond to huge\ncommunication and computation costs to update\nparameters during the pre-training process (Sanh\net al., 2022; Wang et al., 2022) or fine-tuning pro-\ncess (Ding et al., 2023). Both pre-training and\nfine-tuning update the whole set of parameters of\nthe language model. Thus, the application of FL\nin the pre-training or the fine-tuning process is al-\nmost impossible due to significant communication\nburden brought by large amount of parameters.\nPrompt design (Brown et al., 2020) can lead\nto excellent performance while freezing the orig-\ninal LLMs. When the LLMs are frozen, only the\nprompts or prefix are updated during the tuning\nprocess, which can significantly reduce the number\nof parameters to update. For instance, for a sam-\nple from a sentiment analysis task (e.g., â€œbeautiful\nplace!â€), a discrete prompt â€œIt was [MASK].â€ for\nprompt tuning (Brown et al., 2020) and continu-\nous task-specific vectors for prefix tuning (Li and\nLiang, 2021) can be concatenated to be sent to a\nLLM, which generates the label of the sample to\nbe â€œterribleâ€ or â€œgreatâ€.\nNumerous parameter-efficient prompt or prefix\ntuning approaches have been proposed to tune the\nlarge language models through updating a few train-\n7871\nable parameters while achieving comparable perfor-\nmance compared with fine-tuning. In order to avoid\nhuman involvement in the prompt design, prompt\ntuning methods (Shin et al., 2020) are proposed to\nsearch proper prompts within a discrete space of\nwords, which corresponds to inferior performance\ncompared with fine-tuning. Continuous prompts,\ni.e., prefix tuning, can be updated to achieve better\nperformance (Liu et al., 2021; Lester et al., 2021).\nHowever, this approach leads to sub-optimal perfor-\nmance for the models with less than 10 billion pa-\nrameters. Although P-tuning V2 (Liu et al., 2022d)\nachieves comparable performance compared with\nfine-tuning, it introduces more parameters, which\nmay correspond to heavier communication costs in\nthe setting of FL compared with other parameter-\nefficient tuning approaches. Some other parameter-\nefficient prompt tuning methods either suffer from\nlow performance with the focus on low-rank hyper-\ncomplex adapter layers (Karimi Mahabadi et al.,\n2021a) or prompt with a single layer (Liu et al.,\n2022c), or introduce extra computation costs with\nattentions (Asai et al., 2022). In addition, existing\nFL techniques for fine-tuning large language mod-\nels typically incur performance degradation or low\nefficiency due to huge communication costs (Tian\net al., 2022; Sun et al., 2022; Zhao et al., 2023).\nAdaptive optimization methods, e.g., Adaptive\nMoment Estimation (Adam) and Stochastic Gra-\ndient Descent (SGD) with Momentum (SGDM)\n(Sutskever et al., 2013), have been utilized either\non server side (Duchi et al., 2011; Reddi et al.,\n2018a) or on device side (Yuan et al., 2021; Liu\net al., 2020; Gao et al., 2021a; Wang et al., 2020)\nto achieve superior performance in FL. However,\nthe direct application of the adaptive optimization\nmethods may incur problems of the convergence\nwithin the training process (Reddi et al., 2018b).\nFurthermore, the application of adaptive optimiza-\ntion on a single side, i.e., either device or server,\nmay correspond to poor performance. However,\nwhen the adaptive optimization is exploited on both\nsides (Jin et al., 2022a) may incur heavy communi-\ncation costs. In addition, client drift (Karimireddy\net al., 2020b) may exist in terms of the adaptive op-\ntimization due to non-Independent and Identically\nDistributed (non-IID) data among devices.\nIn this paper, we propose a Parameter-efficient\nprompt Tuning approach with Adaptive Optimiza-\ntion, i.e., FedPepTAO, to tune large language mod-\nels with FL. As transferring the whole set of pa-\nrameters in all the prompt layers corresponds to\nheavy communication costs, we propose an effi-\ncient and effective method to choose proper layers\nof prompts based on the importance of each layer.\nWe design a scoring method to identify the im-\nportance of each layer according to the tuning im-\npact of the layer on the final convergence accuracy.\nIn addition, we propose an adaptive optimization\nmethod on both server side and device side with\ncontrol measures on each device to achieve superb\naccuracy. We summarize out major contributions\nas follows:\nâ€¢ We propose a novel parameter-efficient\nprompt tuning method with an efficient and\neffective method to choose proper layers of\nprompts for FL. The subset of layers of\nprompts can reduce both the communication\nand computation costs within FL.\nâ€¢ We provide an original adaptive optimization\nmethod on both server side and device side\nwith control measures on each device.\nâ€¢ We carry out extensive experimentation based\non 10 datasets, which demonstrates the advan-\ntages of FedPepTAO in terms of accuracy (up\nto 60.8% higher) and efficiency (up to 97.59%\nfaster) compared with 9 baseline approaches.\n2 Related Work\nAs updating all the parameters of a pre-trained\nLLM consumes a large amount of memory and\ncomputation resources, prompt tuning (Brown\net al., 2020) or prefix tuning (Li and Liang, 2021) is\nproposed to update a few parameters with a frozen\nlanguage model while achieving comparable perfor-\nmance compared with fine-tuning. While prompt\ntuning may correspond to inferior performance\nwith discrete space of words (Shin et al., 2020),\nprefix tuning (Liu et al., 2021; Lester et al., 2021)\ncan deal with continuous prompts to achieve bet-\nter performance. Adapter modules (Houlsby et al.,\n2019) are exploited to tune large language models\nwith prompts, which may incur heavy computa-\ntion costs due to the calculation of feed-forward\nproject and non-linearity or attention mechanism\n(Asai et al., 2022). Although efficient low-rank\nhypercomplex mechanism (Karimi Mahabadi et al.,\n2021b) can be utilized to reduce parameters to up-\ndate, the performance may degrade. P-tuning V2\nachieves comparable performance compared with\n7872\nfine-tuning (Liu et al., 2022d), which the prompts\nadded into each layer of the large language model.\nPrompts can be added at a single layer to further\nreduce the computation costs (Liu et al., 2022c),\nwhich may incur performance degradation and de-\npends on multiple trials with each layer. In ad-\ndition, the selection of the layer may incur long\nexecution time to verify the impact on the final ac-\ncuracy. Although the NASWOT algorithm (Mellor\net al., 2021) can be exploited to analyze the perfor-\nmance of a neural network architecture, it is only\ncompatible with the neural networks based on the\nReLU activation function (Nair and Hinton, 2010;\nGlorot et al., 2011).\nParallel, distributed, and federated learning have\nbeen extensively studied in recent years (Liu et al.,\n2023a; Chen et al., 2023b,a; Lee et al., 2019; Wu\net al., 2021; Goswami et al., 2020; Zhang et al.,\n2021; Zhou et al., 2022; Guo et al., 2022; Jin et al.,\n2022a; Che et al., 2022; Yan et al., 2022a; Liu\net al., 2022b; Yan et al., 2022b,c; Jin et al., 2022b,\n2021; Zhao et al., 2021; Zhou and Liu, 2013; Lee\net al., 2013; Zhang et al., 2013; Zhou et al., 2014;\nZhang et al., 2014; Bao et al., 2015; Zhou et al.,\n2015a,b; Lee et al., 2015; Jiang et al., 2019; Zhang\net al., 2022; Zhou, 2017; Hong et al., 2023; Chen\net al., 2018b,a; Gan et al., 2023; Che et al., 2023;\nLiu et al., 2023c, 2022a; Li et al., 2023; Oliveira\net al., 2019; Liu et al., 2019a, 2016, 2015). Some\nexisting FL techniques have been proposed to fine-\ntuning large language models, which may suffer\nfrom performance degradation or low efficiency\ndue to huge communication costs (Tian et al., 2022;\nSun et al., 2022; Zhao et al., 2023). FedBert (Tian\net al., 2022) exploits split learning to split a model\ninto two parts, i.e., one with transformer and the\nother one with head and embedding. As the trans-\nformer is shared on the server, FedBert may cor-\nrespond to inferior performance and huge com-\nmunication costs compared with prompt-tuning\nor prefix-tuning. Some other FL methods only\nfine-tune a part of the model weights (Sun et al.,\n2022), which still suffer from heavy communica-\ntion costs for big language models. FedPrompt\n(Zhao et al., 2023) enable FL based on prompt\ntuning, which communicates the whole set of pa-\nrameters in prompts corresponding to huge com-\nmunication costs.\nAdaptive Moment Estimation (Adam) and\nStochastic Gradient Descent (SGD) with Momen-\ntum (SGDM) (Sutskever et al., 2013) are exploited\nwithin FL on server side (Duchi et al., 2011; Reddi\net al., 2018a) or on device side (Yuan et al., 2021;\nLiu et al., 2020; Gao et al., 2021a; Wang et al.,\n2020) to address the client drift problem brought by\nthe non-IID data in FL (Karimireddy et al., 2020b).\nHowever, the direct application of adaptive opti-\nmization on devices may lead to convergence prob-\nlem (Reddi et al., 2018b). In addition, the appli-\ncation of adaptive optimization on both server and\ndevice sides may incur heavy communication costs\n(Jin et al., 2022a).\nDifferent from the previous work, we propose a\ngeneral scoring method to analyze the correlation\nof each layer and the output of the large language\nmodel, which can represent the importance of each\nlayer. Then, we select the prompt parameters of\nproper layers to be updated with FL while leav-\ning other prompt parameters of other layers to be\nadjusted locally with a lossless method so as to\nachieve superb performance with limited commu-\nnication costs. In addition, we introduce control\nmeasures on each device to alleviate the client drift\nproblem and propose a novel adaptive optimization\nmethod on both server and device sides to further\nimprove the performance.\n3 Problem Formulation\nThe problem to address in this paper is how to\nefficiently tune a large language model based on\nprompt tuning in FL. Given a large language model\nMwith Llayers, we add prompts for each layer\nin Mand denote the set of parameters to generate\nprompts by Pl with lrepresenting the number of\nlayer in M. The whole set of prompts is denoted by\nPDuring the tuning process, the parameters in M\nare frozen and cannot be updated while the param-\neters in Pare updated to improve the performance\nof M.\nWe consider a FL setting with a parameter server\nand Mdevices. We assume that the data for the tun-\ning process of Mis distributed among multiple de-\nvices. On each Device i, a dataset Di = {si,mi}ni\nis located withsi, mi, and nirepresenting a sample,\nthe corresponding label of si, and the number of\nsamples in Di. We denote the total number of the\nsamples on all the devices by N, the set of all the\nsamples by Sand that of labels by M. Due to the\nlimited computation capacity, each Device ican\nonly perform the inference ofMwhile updating Pi\nwith Pi representing the prompt parameters in De-\nvice i. In order to reduce communication costs, we\n7873\nSelected Prompts for communication\nLayer 1 Prompts\nLayer 2 Prompts\nLayer N Prompts\nï‚ ï‚ ï‚\nï‚ ï‚ ï‚\ne(Pretty) e(dull) e(.)\nPrediction (with linear head)\nï‚ ï‚ ï‚\nï‚ ï‚ ï‚\nï‚ ï‚ ï‚\nï‚ ï‚ ï‚\nâ‘¥ Calculate âˆ†ğ‘ğ‘– and ğ‘šğ‘– \nfor each client\nâ‘¦ Update ğ‘Šğ‘Ÿ+1\nğ‘– with \nâˆ†ğ‘ğ‘– with ğ‘šğ‘–\nServer\nâ‘£ Prompts tuning updateLocal Prompts\nFrozen PLM backbone\nâ‘¡ Upload ğœ1 and ğ‘Ÿğ‘– to the server \n( before efficient tuning phase) \nâ‘¤ Selected Prompts \nUploading and Aggregation\nâ‘  Selected Prompts \nDownloading\nâ‘¢ Download and distribute ğ‘†ğ¿ to all \nclients (before efficient tuning phase)\ne([CLS])\n[CLS] Pretty dull .\nï‚ ï‚ ï‚ ï‚ ï‚ ï‚ ï‚ ï‚ ï‚ ï‚ ï‚ ï‚\nï‚ ï‚ ï‚\nLayer 1 Prompts\nLayer 2 Prompts\nLayer N Prompts\nï‚ ï‚ ï‚\nï‚ ï‚ ï‚\ne(Nice) e(visual) e(!)\nPrediction (with linear head)\nï‚ ï‚ ï‚\nï‚ ï‚ ï‚\nï‚ ï‚ ï‚\nï‚ ï‚ ï‚\ne([CLS])\n[CLS] Nice visual !\nï‚ ï‚ ï‚ ï‚ ï‚ ï‚ ï‚ ï‚ ï‚ ï‚ ï‚ ï‚\nIgnored Prompts on server\nï‚ ï‚ ï‚\nï‚ ï‚ ï‚\nï‚ ï‚ ï‚\nï‚ ï‚ ï‚\nï‚ ï‚ ï‚\nâ‘¤ Selected Prompts \nUploading and Aggregation\nâ‘  Selected Prompts \nDownloading\nâ‘¡ Upload ğœ1 and ğ‘Ÿğ‘– to the server \n( before efficient tuning phase) \nâ‘¢ Download and distribute ğ‘†ğ¿ to all \nclients (before efficient tuning phase)\nâ‘£ Prompts tuning update\nFigure 1: The system model of FedPepTAO.\nenable the exchange the parameters of the prompts\nwithin a subset of selected layers SLi between De-\nvice i and the parameter server while the other\nprompt parameters are only updated within each\ndevice. We denote the set of prompt parameters in\nall the devices by P. The problem to address in\nthis paper can be formulated as how to efficiently\ngenerate P such that the global loss is minimized:\nmin\nP\nï£®\nï£°F(M,P) â‰œ 1\nN\nMâˆ‘\ni=1, piâˆˆP\nniFi(M,pi)\nï£¹\nï£»,\n(1)\nwhere F(M,P) represents the global loss,\nFi(M,pi) â‰œ 1\nni\nâˆ‘\n{si,mi}âˆˆDi f(M,pi,si,mi)\nrefers to the loss function on Device k with\nf(M,pi,si,mi) calculating the local loss of the\ncombination of the large language model Mand\nprompt parameters pi on {sk,mk}.\nFor NLP tasks, each sample sk âˆˆSis the input\nof the large language model andmk âˆˆMis the cor-\nresponding label. Each sample sk is composed of\nmultiple tokens, i.e., sk = {s1\nk,s2\nk,...,s t\nk}, where\ntrepresents the length of the input. The prompt\npconsists of multiple tokens p= {p1,p2,...,p h},\nand the corresponding prompt parameters can be\ntrained. The prompts differ according to layers. We\ndenote the template by T(Â·), which defines how\nto concatenate the input tokens with the prompt.\nFor instance, sp\nk = T(sk,p) represents the sam-\nple combined with the prompt, which contains\none [MASK] token. The output of the large lan-\nguage model with the prompts predicts the label\nmk, which corresponds to the [MASK] token after\napplying a verbalizer V(Â·), i.e., Ë†mk = V(ok) with\nok representing the output of the model and Ë†mk\nreferring to the predicted label.\nIn this section, we first present the system model\nof FedPepTAO. Then, we propose parameter-\nefficient prompt tuning method and the adaptive\noptimization method, respectively.\n3.1 System Model\nAs shown in Figure 1, we consider a parameter\nserver and multiple devices for the tuning process\nof FedPepTAO. We assume that a large language\nmodel is deployed on each device. For each layer,\nwe insert a prompt module. During the tuning\nprocess, the large language model only perform\ninference while the prompt modules of each layer\nperform both the inference of the input and the\nupdate of parameters. Within the FL tuning pro-\ncess, the prompt parameters of specific layers are\ncommunicated between the device and the server.\nDuring the FL tuning process of FedPepTAO,\nthe prompt parameters in each device are updated\nwith multiple rounds. Each round consists of five\nsteps. First, a set of devices are selected to perform\nthe update of prompt parameters. Second, these\ndevices receive the corresponding updated prompt\nparameters of specific layers from the server ( 1âƒ).\nThe selection of the specific layers is based on our\nparameter-efficient prompt tuning method (see de-\ntails in Section 3.2) ( 2âƒ- 3âƒ). Third, the prompt\nparameters are updated with our adaptive optimiza-\ntion method (see details in Section 3.3) based on\nthe data on each device ( 4âƒ). Fourth, the prompt\nparameters of specific layers are sent back to the\nserver ( 5âƒ). Fifth, the prompt parameters are aggre-\ngated on the server with the adaptive optimization\nmethod ( 6âƒ- 7âƒ).\n3.2 Parameter-efficient Prompt Tuning\nWe propose a parameter-efficient prompt tuning\nmethod to efficiently tune the language model with\n7874\nFL. Instead of synchronizing the full set of prompt\nparameters, we select a proper set of layers for\neach device and only exchange the prompt param-\neters of these layers during the tuning process. In\nthis section, we propose a scoring method to mea-\nsure the importance of each layer. Then, we pro-\npose a lossless layer selection method to select the\nproper layers, which reduces the communication\ncosts without performance degradation.\nGiven the prompt parameters based on any acti-\nvation function, we can calculate the hidden states\nof each parameter at each layer of the large lan-\nguage model. With a batch of local data samples\nSi = {si}ni mapped through the large language\nmodel and the prompt parameters corresponding to\nthe function fp(si), the hidden state corresponding\nto Node kat l-th layer is fpk,l(si). Then, the hid-\nden states of Layer lcorresponding to Sample si is\nhi,l = {fp1,l(si),fp2,l(si),...,f pKl,l(si)}, with Kl\nrepresenting the number of nodes at Layer L.\nAs the difficulty for a network to learn to sepa-\nrate the input samples has positive correlation with\nthe similarity of the hidden states (Mellor et al.,\n2021), we examine the correlation between the\nhidden states of any two layers by computing the\nfollowing kernel matrix:\nKhi =\nï£«\nï£¬ï£­\nCos(hi,1,hi,1) Â·Â·Â· Cos(hi,1,hi,L)\n... ... ...\nCos(hi,L,hi,1) Â·Â·Â· Cos(hi,L,hi,L)\nï£¶\nï£·ï£¸\n(2)\nwhere Cos(hi,L,hi,1) represents the cosine sim-\nilarity between two vectors (Dehak et al., 2010).\nThen, we calculate the eigenvalues of the kernel\nmatrix Î›i = {Î»i,1,Î»i,2,..,Î» i,L}, with Î»l represent-\ning the distinction of Layer lcompared with other\nlayers based on Sample si. Afterward, we compute\nthe score (Î¶i,l) of Layer lwith the local dataset on\nDevice iusing the Formula 3, which can avoid the\npossibility of unacceptable performance penalty\ndue to abnormal eigenvalues (Gao et al., 2021b).\nÎ¶i,l = 1\nni\nniâˆ‘\nj=1\nlog(Î»j,l + Ïµ) + (Î»j,l + Ïµ)âˆ’1, (3)\nwhere Ïµrefers to a small positive value, e.g., 1 âˆ—\neâˆ’5. We calculate the global score of each layer\nleveraging Formula 4 with Î³i = Î¶i,l.\nÎ³ =\nNâˆ‘\ni=1\nni\nNÎ³i, (4)\nAlgorithm 1 Federated Parameter-efficient Prompt\nTuning\nRequire:\nL: The list of layers in a large language model\nM: The set of devices\nw: The prompt parameters of the initial model\nwt: The prompt parameters of the current\nmodel in Round t\nEnsure:\nSL: The set of selected layers\n1: SL â†âˆ…\n2: for iâˆˆM (on each device) do\n3: âˆ†i â†wâˆ’wt\n4: H(wt\ni) â†Get Hessian matrix of wt\ni\n5: Î›H(wt\ni) â†Eigenvalues(H(wt\ni))\n6: {Î»H\n1 ,Î»H\n2 ,...,Î» H\nKi}â† Sort in ascending or-\nder of Î›H(wt\ni)\n7: B(âˆ†i) â†H(wt\ni) âˆ’â–½Fi(âˆ†i + wt\ni)\n8: Li â†Get Lipschitz constant of B(âˆ†i)\n9: ki â†Get the first kthat meets Î»H\nk+1âˆ’Î»H\nk >\n4Li\n10: ri â†Kiâˆ’ki\nKi\n11: for lâˆˆLdo\n12: Calculate Î¶i,l according to Formula 3\n13: end for\n14: end for\n15: Aggregate rand each Î¶l based on Formula 4\n16: Î¶ â†Sort {Î¶1,Î¶2,...,Î¶ L}in descending order\n17: while lâˆˆÎ¶and Para(SL)\nPara(L) <r do\n18: SL â†SLâˆªl\n19: end while\nwhere Î³represents the variable.\nIn order to efficiently tune the large language\nmodel without performance degradation, we ex-\nploit a lossless method as shown in Algorithm 1 to\nselect the set of proper layers within FL. Within\nfirst trounds, the prompt parameters of all the lay-\ners are communicated between the server and each\ndevice. tcan be small, e.g., 5 or 10. At t-th round,\nwe perform the layer selection. First, we calculate\nâˆ†i as the changement of the prompt parameters\n(Line 3). Then, we calculate the Hessian matrix\n(based an efficient PyHessian library (Yao et al.,\n2020)) of the current model (Line 4), the corre-\nsponding eigenvalues (Line 5), and sort the eigen-\nvalues in ascending order (Line 6). Afterward, we\nconstruct a base function in Line 7 with â–½Fi rep-\nresenting the gradients and calculate the Lipschitz\nconstant of the base function 8. We take the first k\nthat can meet the constraint in Line 9, and calculate\nthe minimum remaining prompt parameter ratio in\n7875\nthe selected layers Ri, inspired by (Zhang et al.,\n2021), which can achieve lossless compared with\nthose at all the layers. We calculate the score of\neach layer in Lines 11 - 13. The execute of Lines\n3 - 13 can be carried out in parallel on each de-\nvice. We aggregate the prompt parameter ratio and\nthe scores based on Formula 4 from each device\nto the server (Line 15). Then, we sort the layers\naccording to the scores in descending order (Line\n16). Finally, we add the layers into the selected\nlayer set based on the scores in descending order\n(Lines 17 - 19), with Para(SLi) representing the\nnumber of parameters in the selected layer set. In\nthe following rounds, the prompt parameters in SL\nare communicated between devices and the server.\n3.3 Communication-Efficient Adaptive\nOptimization\nWhile data is generally non-IID, we propose a\nnovel communication-efficient adaptive optimiza-\ntion to achieve superb performance without intro-\nducing extra communication costs. In order to\nachieve excellent performance, we propose apply-\ning adaptive optimization on both server based on\nmomentum (Cutkosky and Mehta, 2020) and de-\nvice sides based on Adam (Kingma and Ba, 2015).\nWe reset the first and the second momentum buffers\nto zero at the beginning of local update (Wang et al.,\n2021) to avoid extra communication of the momen-\ntum variables between the server and the device.\nIn addition, we maintain a state for each device\non the server to avoid possible client drift problem\nincurred by non-IID (Karimireddy et al., 2020b).\nThe algorithm of communication-efficient adap-\ntive optimization for FL prompt tuning is shown\nin Algorithm 2. Within each round, we first ran-\ndomly sample a subset of devices (Line 3). Then,\nthe prompt parameters corresponding to the model\nin the last round is sent to each device (Line 5), and\neach selected device perform local update based\non Adam (Lines 7 - 8). Afterward, each selected\ndevice returns the accumulated difference of the\nprompt parameters to the server (Line 10). Please\nnote that the execution of Lines 4 - 11 can be per-\nformed in parallel on each selected device. We\naggregate the differences based on Formula 4 (Line\n12). Inspired by (Karimireddy et al., 2020b), we\ncalculate the control variate cr\ni (Line 14) and the\ncorresponding difference âˆ†cr\ni (Line 15) for each\ndevice on the server. We aggregate the control vari-\nate differences based on Formula 4 (Line 17), and\nAlgorithm 2 Communication-Efficient Adaptive\nOptimization\nRequire:\nM: The set of devices\nw: The prompt parameters of the initial model\nR: The maximum number of global round\nÎ±: The local step size\nÎ²: The momentum parameter\nÎ· = {Î·1,Î·2,...,Î· R}: The set of global learn-\ning rates\nT = {T1,T2,...,T M}: The set of local epoch\nTi on each Device i\nEnsure:\nwR: The final model\n1: w0 â†w, c0\ni â†0, c0\ng â†0, m0\ni â†0, âˆ€iâˆˆM\n2: for r= 1,Â·Â·Â· ,R do\n3: Randomly sample a subset Sof devices M\n4: for iâˆˆS(on each device) do\n5: wr,0\ni â†wrâˆ’1, mr,0\ni â†0, vr,0\ni â†0\n6: for t= 1,2,Â·Â·Â· ,Ti do\n7: gr,t\ni â†â–½wr,tâˆ’1\ni\nFi(wr,tâˆ’1\ni )\n8: wr,t\ni ,mr,t\ni ,vr,t\ni â†Adam update with\nwr,tâˆ’1\ni , mr,tâˆ’1\ni , vr,tâˆ’1\ni , and gr,t\ni\n9: end for\n10: âˆ†wr\ni = wr,Ti\ni âˆ’wr,0\ni\n11: end for\n12: Aggregate âˆ†wr based on Formula 4\n13: for iâˆˆS(on the server) do\n14: cr\ni = crâˆ’1\ni âˆ’crâˆ’1\ng âˆ’ 1\nTiÎ± âˆ—âˆ†wr\ni\n15: âˆ†cr\ni = cr\ni âˆ’crâˆ’1\ni\n16: end for\n17: Aggregate âˆ†cr based on Formula 4\n18: cr\ng = crâˆ’1\ng + âˆ†cr âˆ—|S|\n|M|\n19: gr\ng = âˆ†wrâˆ’1 âˆ’âˆ†wr\n20: for iâˆˆS(on the server) do\n21: mr\ni = Î²âˆ—mrâˆ’1 + (1âˆ’Î²) âˆ—gr\ng+ cr\ngâˆ’cr\ni\n22: wr\ni = wrâˆ’1 âˆ’Î·r âˆ—mr\ni\n23: end for\n24: Aggregate wr based on Formula 4\n25: Aggregate mr based on Formula 4\n26: end for\ncalculate the global control variate (Line 18) and\nglobal gradients (Line 19). Afterword, we update\nthe momentum (Line 21) and prompt parameters\n(Line 22) for each selected device. Finally, we ag-\ngregate the global prompt parameters (Line 24) and\nmomentum (Line 25). The communication cost of\nAlgorithm 2 depends on the size of prompt parame-\nters, which is similar to that of FedAvg (McMahan\net al., 2017), while achieving superb performance.\n7876\nMethod Comm. Params QNLI SST-2 CoLA MPRC RTE BoolQ MPQA Subj Trec MR Avg\nAdapter 7.4M 87.79 94.04 30.96 71.81 68.59 75.11 90.97 94.6 79 91.9 75.36\nFedPrompt 131K 85.91 94.84 33.05 77.87 61.73 74.77 90.45 94.25 95 91.65 76.85\nP-tuning v2 6.3M 85.19 95.3 41.82 82.78 79.42 79.66 91 96.9 96.4 91.45 82.05\nPrompt Tuning 20K 51.62 61.01 3.36 48.04 52.35 59.72 81.65 65.2 36.4 63.25 42.56\nIDPG 137K 72.2 93.01 4.59 70.83 70.4 71.96 90.07 94 78.4 91.4 60.63\nATTEMPT 207k 54.93 85.89 4.63 78.65 58.48 73.49 91.05 88.95 82.2 91.9 58.28\nLPT 792k 89.2 94.84 53.7 82.07 79.7 62.7 90.55 96.5 96.4 91.4 82.35\nMomD 6.3M 67.42 93.92 1.64 75.17 75.45 62.02 89.05 49.95 38.6 83 63.62\nMomS+AdamD 6.3M 87.85 95.18 42.96 80.15 82.31 78.1 90.95 96.75 96.8 91.55 84.26\nFedPepTAO 492K 89.57 95.87 56.35 87.52 85.56 79.72 91.4 97.1 97.2 93 86.4\nTable 1: The accuracy with FedPepTAO and diverse baseline approaches. All the methods from GLUE benchmark\nare evaluated on development sets while other tasks are evaluated with test sets. The best results are highlighted in\nbold and the second bests are marked with underline. All the results are obtained using RoBERTaLARGE.\n4 Experiments\nIn this section, we present the experimental results\nover 9 baselines and 10 commonly-used tasks to\ndemonstrate the advantages of FedPepTAO.\n4.1 Experimental Setup\nWe consider an FL environment with 100 devices\nand a parameter server. In each epoch, we ran-\ndomly sample 10 devices to perform the local up-\ndate. We exploit 10 widely used NLP tasks includ-\ning QNLI (Rajpurkar et al., 2016), SST-2 (Socher\net al., 2013), CoLA (Warstadt et al., 2019), MRPC\n(Dolan and Brockett, 2005), RTE (Giampiccolo\net al., 2007), and BoolQ (Clark et al., 2019) from\nthe GLUE benchmark, and 4 other tasks includ-\ning MPQA (Wiebe et al., 2005), Subj (Pang and\nLee, 2004), TREC (V oorhees and Tice, 2000), and\nMR (Pang and Lee, 2005) (Please see details in\nAppendix A.2). We take 9 existing approaches as\nbaselines, including an adapter-based method, i.e.,\nAdapter (Houlsby et al., 2019), 6 prompt-based tun-\ning methods, i.e., FedPrompt (Zhao et al., 2023),\nP-tuning v2 (Liu et al., 2022d), Prompt Tuning\n(Lester et al., 2021), IDPG (Wu et al., 2022), AT-\nTEMPT (Asai et al., 2022), LPT (Liu et al., 2022c),\nand 2 optimization approaches, i.e., momentum on\nthe device side with simple SGD on the server side\n(MomD) (Karimireddy et al., 2020a) and momen-\ntum on the server side with Adam on the device side\nwithout control variate (MomS+AdamD). We adapt\nthe centralized methods, i.e., Adapter, P-tuning v2,\nPrompt Tuning, IDPG (S-IDPG-PHM), ATTEMPT,\nand LPT, with FedAvg (McMahan et al., 2017) to\nthe FL setting for a fair comparison.\nWe evaluate FedPepTAO and all other meth-\nods on RoBERTaLARGE (Liu et al., 2019b), which\nconsists of 24 layers of transformers followed\nby a large language model head and 355M pre-\ntrained parameters. To demonstrate the adaptabil-\nity of FedPepTAO, we carried out extra experi-\nments with three additional decoder-based models,\ni.e., GPT2LARGE model (Radford et al., 2019) with\n774M parameters on MRPC, MR, SST-2 dataset,\nLLaMA 3B model (Touvron et al., 2023) on RTE,\nMRPC dataset, and LLaMA 7B model (Touvron\net al., 2023) on MRPC dataset. The backbones of\nthese models are frozen for all methods.\n4.2 Evaluation of FedPepTAO\nAs shown in Table 1, FedPepTAO significantly\noutperforms baseline methods in terms of the\nbest accuracy (up to 25.39%, 23.83%, 14.53%,\n60.8%, 51.76%, 51.72%, 17.02%, 54.71%, 13.39%\ncompared with Adapter, FedPrompt, P-tuning v2,\nPtompt Tuning, IDPG, ATTEMPT, LTP, MomD,\nand MomS+AdamD, respectively). In addition,\nthe average of the best accuracy (average accu-\nracy) for each task is shown in the last column.\nThe advantage of FedPepTAO is obvious in terms\nof the average accuracy as well, i.e., 11.04%,\n9.55%, 4.35%, 43.84%, 25.77%, 28.12%, 4.05%,\n58.6%, 13.39%, higher compared with Adapter,\nFedPrompt, P-tuning v2, Ptompt Tuning, IDPG,\nATTEMPT, LTP, MomD, and MomS+AdamD, re-\nspectively. Although FedPrompt, Prompt Tuning,\nIDPG, and ATTEMPT exploit fewer parameters,\nthe corresponding accuracy is inferior. FedPrompt,\nPrompt Tuning, and ATTEMPT only update the\nsoft prompt for the first layer, which cannot opti-\nmize other important layers and incurs sub-optimal\nperformance. IDPG shares a single generator for\neach layer, which cannot address the characteris-\ntics of diverse layers and leads to inferior accuracy.\nDifferent from these methods, FedPepTAO can\nwell optimize the prompt parameters for each layer\nbased on P-tuning v2, while choosing the proper\nlayers for aggregation within FL so as to achieve\nexcellent performance. In addition, we exploit the\nadaptive optimization on both server and device\n7877\nMethod QNLI SST-2 CoLA MPRC RTE BoolQ MPQA Subj Trec MR\nAdapter 8096 1065 2655 / 2218 937 758 1178 1388 1797\nFedPrompt 12987 668 1471 1824 / 1485 412 1284 336 618\nP-tuning v2 10780 17 489 154 201 135 105 132 95 748\nPrompt Tuning / / / / / / / / / /\nIDPG / 3322 / / 689 3254 908 1347 1220 1912\nATTEMPT / / / 1774 / 973 438 2573 1028 1221\nLPT 2918 650 733 156 270 / 162 155 112 860\nMomD / 1328 / / 838 / 537 / / /\nMomS+AdamD697 209 488 1178 192 139 141 166 100 610\nFedPepTAO 781 97 219 230 186 129 31 62 76 610\nTable 2: The tuning time (s) to achieve a target accuracy (85% for QNLI, 92.5% for SST-2, 3% for CoLA, 77%\nfor MRPC, 65% for RTE, 71% for BoolQ, 85% for MPQA, 88% for Subj, 78% for Trec, 91% for MR)with\nFedPepTAO and diverse baseline approaches. \"/\" represents that training does not achieve the target accuracy. The\nbest results are highlighted in bold and the second bests are marked with underline. All the results are obtained\nusing RoBERTaLARGE.\nMethod MRPC MR SST-2\nAcc Time Acc Time Acc Time\nFedPrompt 74.98 / 57.2 / 76.49 /\nP-tuning v2 74.8 / 73 / 74.77 /\nATTEMPT 37.91 / 57.3 / 85.89 /\nLPT 74.98 / 84.8 1455 77.06 /\nMomD 77.23 503 85.7 1694 92.55 3638\nMomS+AdamD 76.89 291 88.2 262 92.55 2488\nFedPepTAO 81.23 273 89.5 222 93 2248\nTable 3: Accuracy and tuning time (s) to achieve target\naccuracy (75% for MRPC, 81% for MR, and 92.5%\nfor SST-2) on GPT2LARGE model. \"/\" represents that\ntraining does not achieve the target accuracy.\nsides to achieve superb accuracy. Compared with\nAdapter (93.4%), P-tuning v2 (92.19%), and LPT\n(37.98%), our methods can well reduce the number\nof parameters to transfer between devices and the\nserver because of the proper layer selection, which\ncorresponds to smaller communication costs. As a\nresult, the efficiency of FedPepTAO is significantly\nhigher than baseline approaches (up to 95.91%,\n95.17%, 92.76%, 99%, 97.28%, 97.59%, 85.8%,\n94.23%, 80.48%, faster compared with Adapter,\nFedPrompt, P-tuning v2, Ptompt Tuning, IDPG,\nATTEMPT, LTP, MomD, and MomS+AdamD, re-\nspectively).\n4.3 Evaluation of FedPepTAO on Extra LLMs\nTo demonstrate the adaptability of FedPepTAO, we\ncarried out extra experiments with three additional\ndecoder-based models, i.e., GPT2 LARGE model\n(774M) on MRPC, MR, SST-2 dataset, LLaMA\n3B model on RTE, MRPC dataset, and LLaMA 7B\nmodel on MRPC dataset.\nAs shown in Table 3 below, FedPepTAO signifi-\ncantly outperforms baseline methods in terms of the\nbest accuracy on the decoder-based GPT2 LARGE\nmodel (up to 32.3%, 18.23%, 43.32%, 15.94%, 4%,\n4.34% higher compared to FedPrompt, P-tuning\nv2, ATTEMPT, LPT, MomD and MomS+AdamD,\nMethod RTE MRPC\nAcc Time Acc Time\nFedPrompt 78.34 540 81.86 459\nP-tuning v2 56.68 / 75.17 /\nATTEMPT 64.98 / 81.18 718\nLPT 64.98 / 79.77 789\nMomD 80.87 360 80.26 669\nMomS+AdamD 80.87 360 75.58 /\nFedPepTAO 83.39 325 86.46 409\nTable 4: Accuracy and tuning time (s) to achieve target\naccuracy (75% for RTE, 81% for MRPC) on LLaMA\n3B model. \"/\" represents that training does not achieve\nthe target accuracy.\nrespectively). Furthermore, the efficiency of\nFedPepTAO is significantly higher than base-\nline approaches (up to 84.74%, 86.89%, and\n15.27% faster compared to LPT, MomD, and\nMomS+AdamD, respectively).\nWhen the model becomes larger, i.e., LLaMA\n3B with 3 billion parameters, FedPepTAO still\nachieves the best accuracy (up to 4.6%, 11.29%,\n5.28%, 6.69%, 6.2%, 10.88% higher compared to\nFedPrompt, P-tuning v2, ATTEMPT, LPT, MomD\nand MomS+AdamD, respectively) and better effi-\nciency (up to 39.81%, 43.04%, 48.16%, 38.86%,\n9.72% faster compared to FedPrompt, ATTEMPT,\nLPT, MomD, and MomS+AdamD, respectively) as\nillustrated in Table 4.\nWe verify the performance of our method with\nanother large model, i.e., LLaMA 7B with 7 billion\nparameters. As shown in Table 5, FedPepTAO\noutperforms baseline methods in terms of accuracy\n(up to 5.05%, 26.71%, 18.41%, 18.41%, 6.2%,\n10.88% higher compared to FedPrompt, P-tuning\nv2, ATTEMPT, LPT, MomD and MomS+AdamD,\nrespectively) and efficiency (up to 15.77%, 75.14%,\n32.06%, 67.04% faster compared to ATTEMPT,\nLPT, MomD, and MomS+AdamD, respectively),\nwhich demonstrates the scalability of FedPepTAO.\n7878\nMethod MRPC\nAcc Time\nFedPrompt 76.18 /\nP-tuning v2 74.98 /\nATTEMPT 81.52 317\nLPT 81.95 1074\nMomD 76.2 393\nMomS+AdamD 75.75 810\nFedPepTAO 82.34 267\nTable 5: Accuracy and tuning time (s) to achieve target\naccuracy (75% for MRPC) on LLaMA 7B model. \"/\"\nrepresents that training does not achieve the target accu-\nracy.\n4.4 Evaluation of Parameter-efficient Tuning\nFormula 4 calculates the global score Î¶l of each\ntransformer layer in the model, based on which\nthe proper layers are selected to enable the com-\nmunication between devices and the server. The\nselection of layers is critical to the performance\nof FL. In this section, we compare our parameter-\nefficient prompt tuning (PEPT) with three other se-\nlection strategies, i.e., select with ascending order,\ndescending order ((Liu et al., 2022d)), and random\norder. Our PEPT method can significantly outper-\nform ascending order (up to 5.78%), descending or-\nder (up to 1.81%), and random order (up to 2.89%)\n(see Figure 5 in Appendix). In addition, we con-\nduct experiments and demonstrate that our PEPT\nmethod outperforms random layer selection strat-\negy by 2.93% on RTE, 4.73% on MRPC, and 7.29%\non CoLA dataset (see Appendix B.6 for details).\n4.5 Evaluation of Adaptive Optimization\nTo demonstrate the effectiveness of Algorithm 2,\nwe compare our adaptive optimization method with\nsix baseline methods, i.e., FedAvg (McMahan et al.,\n2017), MomD, momentum on the server side with\nSGD on the device side (MomS) (Reddi et al.,\n2018a), momentum on the server side with control\nvariate and SGD on the device side (MomS+Con)\n(Reddi et al., 2018a), Adam on device side with\nsimple SGD on the server side (AdamD), and\nMomS+AdamD, on the RTE task (see Figure 6\nin Appendix). FedPepTAO corresponds to the high-\nest accuracy compared to baseline methods (up to\n8.3% compared with MomD, 29.24% compared\nwith MomS, 29.24% compared with MomS+Con,\n2.16% compared with AdamD, 5.05% compared\nwith MomS+AdamD, and 29.24% compared with\nFedAvg). The advantage of FedPepTAO is up\nto 5.05% compared with MomS+AdamD, which\nshows that the control variate can avoid client drift\nin FL settings and lead to excellent performance.\nIn addition, please note that FedPepTAO calculates\nthe control variate on the server with updated gra-\ndients (Algorithm 2), without extra communication\ncost.\n4.6 Hyperparameter Evaluation\nIn this section, we evaluate the performance of Fed-\nPepTAO with divers hyperparameters. Additional\nexperiments are in the Appendix B.\nImpact of server learning rate Due to the high\nsensitivity of the hyperparameters in NLP tasks, we\ninvestigate the impact of divers server learning rates\non the RTE dataset. We analyze the accuracy with\nthe learning rates 1eâˆ’2, 5eâˆ’3, 1eâˆ’3, 5eâˆ’4. We find\nthat the best performance was achieved when lr=\n1eâˆ’3, which only slightly outperforms lr = 5eâˆ’4\nby 0.37% (see Figure 2 in the Appendix). This\ndemonstrates that FedPepTAO is easy to fine-tune\nin practice.\nImpact of heterogeneous data distribution\nData heterogeneity has always been a common\nchallenge in FL. To investigate the robustness of\nour approach to different degrees of non-IID data,\nwe conduct experiments under various levels of\nnon-IID degrees. We observe that the accuracy of\nFedPepTAO is the best with Î± ranging from 1.0\nto 5.0 (see Figure 3 in Appendix). A smaller Î±\nrepresents a higher degree of heterogeneity. This\ndemonstrates that our approach is robust to differ-\nent data heterogeneity.\n5 Conclusion\nIn this paper, we propose an original parameter-\nefficient prompt tuning with adaptive optimization\napproach for large language models with FL, i.e.,\nFedPepTAO. We dynamically calculate the score\nof each layer to choose proper layers of prompts\nfor FL without accuracy degradation. In addition,\nwe provide a novel adaptive optimization method\nwith control variate on the server to achieve superb\nperformance without extra communication costs.\nWe carry out extensive experiments based on 10\ntasks and 9 state-of-the-art baseline approaches,\nwhich demonstrate significant advantages of Fed-\nPepTAO in terms of accuracy (up to 60.8% higher)\nand efficiency (up to 97.59% faster).\nAcknowledgements\nThis research is partially sponsored by the National\nScience Foundation (NSF) under Grant No. OAC-\n2313191.\n7879\nLimitations\nWhile our method can significantly enhance the per-\nformance and efficiency of federated prompt tuning\nin large language models (LLMs), we should ac-\nknowledge the sharing of prompts between clients\nand the server. Previous research has demonstrated\nthat transferring additional sensitive information\nin Federated Learning (FL), such as predictions\nor embeddings, can lead to potential privacy con-\ncerns (Che et al., 2022). These concerns can be\neven more critical in prompt tuning scenarios, as\nprompts are explicitly tuned with private local data.\nWe anticipate that evaluating and mitigating pri-\nvacy risks in federated LLM prompt tuning will be\nan intriguing research area in the future.\nReferences\nAkari Asai, Mohammadreza Salehi, Matthew E Peters,\nand Hannaneh Hajishirzi. 2022. Attempt: Parameter-\nefficient multi-task tuning via attentional mixtures\nof soft prompts. In Conf. on Empirical Methods in\nNatural Language Processing (EMNLP), pages 6655â€“\n6672.\nXianqiang Bao, Ling Liu, Nong Xiao, Yang Zhou, and\nQi Zhang. 2015. Policy-driven autonomic configu-\nration management for nosql. In IEEE Int. Conf. on\nCloud Computing (CLOUD), pages 245â€“252.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems (NeurIPS), 33:1877â€“1901.\nCalifornians for Consumer Privacy. 2020. California\nconsumer privacy act home page. https://www.\ncaprivacy.org/. Online; accessed 09/05/2022.\nTianshi Che, Zijie Zhang, Yang Zhou, Xin Zhao, Ji Liu,\nZhe Jiang, Da Yan, Ruoming Jin, and Dejing Dou.\n2022. Federated fingerprint learning with hetero-\ngeneous architectures. In IEEE Int. Conf. on Data\nMining (ICDM), pages 31â€“40.\nTianshi Che, Yang Zhou, Zijie Zhang, Lingjuan Lyu,\nJi Liu, Da Yan, Dejing Dou, and Jun Huan. 2023.\nFast federated machine unlearning with nonlinear\nfunctional theory. In Int. Conf. on Machine Learn-\ning (ICML), volume 202 of Proceedings of Machine\nLearning Research, pages 4241â€“4268. PMLR.\nZhuo Chen, Gang Feng, Bei Liu, and Yang Zhou. 2018a.\nConstruction policy of network service chain oriented\nto resource fragmentation optimization in operator\nnetwork. ournal of Electronics and Information Tech-\nnology, 40(4):763â€“769.\nZhuo Chen, Gang Feng, Bei Liu, and Yang Zhou. 2018b.\nDelay optimization oriented service function chain\nmigration and re-deployment in operator network.\nActa Electronica Sinica, 46(9):2229â€“2237.\nZhuo Chen, Xiaoxiang Tan, Zhiyuan Zhou, and Yang\nZhou. 2023a. A channel aggregation based dynamic\npruning method in federated learning. In IEEE\nGlobal Communications Conference (GLOBECOM).\nTo appear.\nZhuo Chen, Chuan Zhou, and Yang Zhou. 2023b. A\nhierarchical federated learning model with adaptive\nmodel parameter aggregation. Computer Science and\nInformation Systems, 20(3):1037â€“1060.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924â€“2936.\nAshok Cutkosky and Harsh Mehta. 2020. Momentum\nimproves normalized SGD. In Int. Conf. Machine\nLearning (ICML), volume 119, pages 2260â€“2268.\nPMLR.\nNajim Dehak, Reda Dehak, James R Glass, Douglas A\nReynolds, Patrick Kenny, et al. 2010. Cosine similar-\nity scoring without score normalization techniques.\nIn Odyssey, page 15.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei,\nZonghan Yang, Yusheng Su, Shengding Hu, Yulin\nChen, Chi-Min Chan, Weize Chen, et al. 2023.\nParameter-efficient fine-tuning of large-scale pre-\ntrained language models. Nature Machine Intelli-\ngence, 5(3):220â€“235.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\nAdaptive subgradient methods for online learning and\nstochastic optimization. Journal of machine learning\nresearch, 12(7).\nXinbiao Gan, Guang Wu, Ruigeng Zeng, Jiaqi Si,\nJi Liu, Daxiang Dong, Chunye Gong, Cong Liu, and\nTiejun Li. 2023. Ft-topo: Architecture-driven folded-\ntriangle partitioning for communication-efficient\ngraph processing. In Int. Conf. on Supercomputing\n(ICS), page 240â€“250.\n7880\nHongchang Gao, An Xu, and Heng Huang. 2021a. On\nthe convergence of communication-efficient local sgd\nfor federated learning. In AAAI Conf. on Artificial\nIntelligence, 9, pages 7510â€“7518.\nWei Gao, Shangwei Guo, Tianwei Zhang, Han Qiu,\nYonggang Wen, and Yang Liu. 2021b. Privacy-\npreserving collaborative learning with automatic\ntransformation search. In IEEE/CVF Conf. on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n114â€“123.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nBill Dolan. 2007. The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the\nACL-PASCAL Workshop on Textual Entailment and\nParaphrasing, pages 1â€“9.\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n2011. Deep sparse rectifier neural networks. In Int.\nConf. on artificial intelligence and statistics, pages\n315â€“323. JMLR Workshop and Conference Proceed-\nings.\nSayan Goswami, Ayam Pokhrel, Kisung Lee, Ling Liu,\nQi Zhang, and Yang Zhou. 2020. Lightwieight in-\ndexing and querying services for big spatial data.\nThe Journal of Supercomputing (TJSC), 76(9):6619â€“\n6647.\nGuimu Guo, Da Yan, Lyuheng Yuan, Jalal Khalil, Cheng\nLong, Zhe Jiang, and Yang Zhou. 2022. Maximal\ndirected quasi-clique mining. In IEEE Int. Conf. on\nData Engineering (ICDE), pages 1900â€“1913.\nJunyuan Hong, Zhuangdi Zhu, Lingjuan Lyu, Yang\nZhou, Vishnu Naresh Boddeti, and Jiayu Zhou. 2023.\nInt. workshop on federated learning for distributed\ndata mining. In ACM SIGKDD Conf. on Knowl-\nedge Discovery and Data Mining (KDD), pages 5861â€“\n5862, Long Beach, CA.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In Int.\nConf. on Machine Learning (ICML) , pages 2790â€“\n2799. PMLR.\nYexi Jiang, Chang-Shing Perng, Anca Sailer, Ignacio\nSilva-Lepe, Yang Zhou, and Tao Li. 2019. Csm: A\ncloud service marketplace for complex service acqui-\nsition. ACM Transactions on Intelligent Systems and\nTechnology (TIST), 8(1):1â€“25.\nJiayin Jin, Jiaxiang Ren, Yang Zhou, Lingjuan Lyu,\nJi Liu, and Dejing Dou. 2022a. Accelerated federated\nlearning with decoupled adaptive optimization. In\nInt. Conf. on Machine Learning (ICML), volume 162\nof Proceedings of Machine Learning Research, pages\n10298â€“10322. PMLR.\nJiayin Jin, Zeru Zhang, Yang Zhou, and Lingfei Wu.\n2022b. Input-agnostic certified group fairness via\ngaussian parameter smoothing. In Int. Conf. on Ma-\nchine Learning (ICML), pages 10340â€“10361.\nRuoming Jin, Dong Li, Jing Gao, Zhi Liu, Li Chen, and\nYang Zhou. 2021. Towards a better understanding of\nlinear models for recommendation. In ACM SIGKDD\nConf. on Knowledge Discovery and Data Mining\n(KDD), pages 776â€“785.\nPeter Kairouz, H. Brendan McMahan, Brendan Avent,\nAurÃ©lien Bellet, and Mehdi Bennis et al. 2021. Ad-\nvances and open problems in federated learning.\nFoundations and TrendsÂ® in Machine Learning, 14(1-\n2):1â€“210.\nRabeeh Karimi Mahabadi, James Henderson, and Se-\nbastian Ruder. 2021a. Compacter: Efficient low-rank\nhypercomplex adapter layers. Advances in Neural\nInformation Processing Systems (NeurIPS), 34:1022â€“\n1035.\nRabeeh Karimi Mahabadi, James Henderson, and Se-\nbastian Ruder. 2021b. Compacter: Efficient low-rank\nhypercomplex adapter layers. In Advances in Neural\nInformation Processing Systems, pages 1022â€“1035.\nSai Praneeth Karimireddy, Martin Jaggi, Satyen Kale,\nMehryar Mohri, Sashank J Reddi, Sebastian U Stich,\nand Ananda Theertha Suresh. 2020a. Mime: Mim-\nicking centralized stochastic algorithms in federated\nlearning. arXiv preprint arXiv:2008.03606.\nSai Praneeth Karimireddy, Satyen Kale, Mehryar\nMohri, Sashank Reddi, Sebastian Stich, and\nAnanda Theertha Suresh. 2020b. Scaffold: Stochas-\ntic controlled averaging for federated learning. In\nInt. Conf. on Machine Learning (ICML), pages 5132â€“\n5143. PMLR.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Int. Conf. on\nLearning Representations (ICLR).\nKisung Lee, Ling Liu, Raghu L. Ganti, Mudhakar Sri-\nvatsa, Qi Zhang, Yang Zhou, and Qingyang Wang.\n2019. Lightwieight indexing and querying services\nfor big spatial data. IEEE Transactions on Services\nComputing (TSC), 12(3):343â€“355.\nKisung Lee, Ling Liu, Karsten Schwan, Calton Pu,\nQi Zhang, Yang Zhou, Emre Yigitoglu, and Pingpeng\nYuan. 2015. Scaling iterative graph computations\nwith graphmap. In IEEE Int. Conf. for High Perfor-\nmance Computing, Networking, Storage and Analysis\n(SC), pages 57:1â€“57:12.\nKisung Lee, Ling Liu, Yuzhe Tang, Qi Zhang, and Yang\nZhou. 2013. Efficient and customizable data parti-\ntioning framework for distributed big rdf data pro-\ncessing in the cloud. In IEEE Int. Conf. on Cloud\nComputing (CLOUD), pages 327â€“334.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Conf. on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 3045â€“3059.\nAssociation for Computational Linguistics.\n7881\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461.\nGuanghao Li, Yue Hu, Miao Zhang, Ji Liu, Quanjun Yin,\nYong Peng, and Dejing Dou. 2023. Fedhisyn: A hier-\narchical synchronous federated learning framework\nfor resource and data heterogeneity. In Int. Conf. on\nParallel Processing (ICPP), pages 1â€“11.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nAnnual Meeting of the Association for Computational\nLinguistics and Int. Joint Conf. on Natural Language\nProcessing (ACL/IJCNLP), pages 4582â€“4597. Asso-\nciation for Computational Linguistics. V olume 1:\nLong Papers.\nJi Liu, Daxiang Dong, Xi Wang, An Qin, Xingjian\nLi, Patrick Valduriez, Dejing Dou, and Dianhai Yu.\n2022a. Large-scale knowledge distillation with elas-\ntic heterogeneous computing resources. Concurrency\nand Computation: Practice and Experience , pages\n1â€“16.\nJi Liu, Jizhou Huang, Yang Zhou, Xuhong Li, Shilei\nJi, Haoyi Xiong, and Dejing Dou. 2022b. From dis-\ntributed machine learning to federated learning: A\nsurvey. Knowledge and Information Systems (KAIS),\n64(4):885â€“917.\nJi Liu, Juncheng Jia, Beichen Ma, Chendi Zhou, Jingbo\nZhou, Yang Zhou, Huaiyu Dai, and Dejing Dou.\n2023a. Multi-job intelligent scheduling with cross-\ndevice federated learning. IEEE Transactions on\nParallel and Distributed Systems (TPDS), 34(2):535â€“\n551.\nJi Liu, Esther Pacitti, Patrick Valduriez, Daniel\nde Oliveira, and Marta Mattoso. 2016. Multi-\nobjective scheduling of scientific workflows in mul-\ntisite clouds. Future Generation Computer Systems\n(FGCS), 63:76â€“95.\nJi Liu, Esther Pacitti, Patrick Valduriez, and Marta Mat-\ntoso. 2015. A survey of data-intensive scientific\nworkflow management. Journal of Grid Computing,\n13(4):457â€“493.\nJi Liu, Luis Pineda-Morales, Esther Pacitti, Alexan-\ndru Costan, Patrick Valduriez, Gabriel Antoniu, and\nMarta Mattoso. 2019a. Efficient scheduling of sci-\nentific workflows using hot metadata in a multisite\ncloud. IEEE Transactions on Knowledge and Data\nEngineering (TKDE), 31(10):1940â€“1953.\nJi Liu, Zhihua Wu, Danlei Feng, Minxu Zhang, Xinx-\nuan Wu, Xuefeng Yao, Dianhai Yu, Yanjun Ma, Feng\nZhao, and Dejing Dou. 2023b. Heterps: Distributed\ndeep learning with reinforcement learning based\nscheduling in heterogeneous environments. Future\nGeneration Computer Systems, 148(C):106â€“117.\nJi Liu, Xuehai Zhou, Lei Mo, Shilei Ji, Yuan Liao,\nZheng Li, Qin Gu, and Dejing Dou. 2023c. Dis-\ntributed and deep vertical federated learning with big\ndata. Concurrency and Computation: Practice and\nExperience, 35(21):1â€“17.\nWei Liu, Li Chen, Yunfei Chen, and Wenyi Zhang. 2020.\nAccelerating federated learning via momentum gra-\ndient descent. IEEE Transactions on Parallel and\nDistributed Systems (TPDS), 31(8):1754â€“1766.\nXiangyang Liu, Tianxiang Sun, Xuanjing Huang, and\nXipeng Qiu. 2022c. Late prompt tuning: A late\nprompt could be better than many prompts. In Find-\nings of the Association for Computational Linguis-\ntics: (EMNLP) , pages 1325â€“1338. Association for\nComputational Linguistics.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022d. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Annual Meeting of the As-\nsociation for Computational Linguistics (ACL), pages\n61â€“68. V olume 2: Short Papers.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nBrendan McMahan, Eider Moore, Daniel Ramage,\nSeth Hampson, and Blaise Aguera y Arcas. 2017.\nCommunication-efficient learning of deep networks\nfrom decentralized data. In Artificial Intelligence and\nStatistics (AISTATS), pages 1273â€“1282.\nJoe Mellor, Jack Turner, Amos Storkey, and Elliot J\nCrowley. 2021. Neural architecture search without\ntraining. In Proceedings of the 38th International\nConference on Machine Learning, volume 139, pages\n7588â€“7598.\nVinod Nair and Geoffrey E Hinton. 2010. Rectified\nlinear units improve restricted boltzmann machines.\nIn Int. Conf. on Machine Learning (ICML) , pages\n807â€“814.\nOfficial Journal of the European Union.\n2016. General data protection regulation.\nhttps://eur-lex.europa.eu/legal-content/\nEN/TXT/PDF/?uri=CELEX:32016R0679. Online;\naccessed 09/05/2022.\nDaniel C. M. de Oliveira, Ji Liu, and Esther Pacitti.\n2019. Data-Intensive Workflow Management: For\nClouds and Data-Intensive and Scalable Computing\nEnvironments. Morgan & Claypool.\nBo Pang and Lillian Lee. 2004. A sentimental education:\nSentiment analysis using subjectivity summarization\nbased on minimum cuts. In Proceedings of the 42nd\n7882\nAnnual Meeting on Association for Computational\nLinguistics, page 271â€“es.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting\nclass relationships for sentiment categorization with\nrespect to rating scales. In Proceedings of the 43rd\nAnnual Meeting on Association for Computational\nLinguistics, page 115â€“124.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing.\nS Reddi, Manzil Zaheer, Devendra Sachan, Satyen Kale,\nand Sanjiv Kumar. 2018a. Adaptive methods for\nnonconvex optimization. In Annual Conf. on Neural\nInformation Processing Systems (NeurIPS) , pages\n1â€“17.\nSashank J. Reddi, Satyen Kale, and Sanjiv Kumar.\n2018b. On the convergence of adam and beyond.\nIn Int. Conf. on Learning Representations (ICLR) .\nOpenReview.net.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In Int. Conf. on Learning Representations\n(ICLR), pages 1â€“216.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with auto-\nmatically generated prompts. In Conf. on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4222â€“4235. Association for Computational\nLinguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631â€“1642.\nGuangyu Sun, Matias Mendieta, Taojiannan Yang, and\nChen Chen. 2022. Exploring parameter-efficient fine-\ntuning for improving communication efficiency in\nfederated learning. arXiv preprint arXiv:2210.01708.\nIlya Sutskever, James Martens, George Dahl, and Geof-\nfrey Hinton. 2013. On the importance of initializa-\ntion and momentum in deep learning. In Int. Conf.\non Machine Learning (ICML), pages 1139â€“1147.\nYuanyishu Tian, Yao Wan, Lingjuan Lyu, Dezhong Yao,\nHai Jin, and Lichao Sun. 2022. Fedbert: when fed-\nerated learning meets pre-training. ACM Transac-\ntions on Intelligent Systems and Technology (TIST),\n13(4):1â€“26.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, TimothÃ©e Lacroix,\nBaptiste RoziÃ¨re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. LLama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nEllen M. V oorhees and Dawn M. Tice. 2000. Building\na question answering test collection. In Proceedings\nof the 23rd Annual International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, page 200â€“207.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nJianyu Wang, Vinayak Tantia, Nicolas Ballas, and\nMichael Rabbat. 2020. Slowmo: Improving\ncommunication-efficient distributed sgd with slow\nmomentum. In Int. Conf. on Learning Representa-\ntions (ICLR), pages 1â€“27.\nJianyu Wang, Zheng Xu, Zachary Garrett, Zachary\nCharles, Luyang Liu, and Gauri Joshi. 2021. Lo-\ncal adaptivity in federated learning: Convergence\nand consistency. In The Int. Workshop on Federated\nLearning for User Privacy and Data Confidentiality\nin Conjunction with ICML (FL-ICML).\nThomas Wang, Adam Roberts, Daniel Hesslow, Teven\nLe Scao, Hyung Won Chung, Iz Beltagy, Julien Lau-\nnay, and Colin Raffel. 2022. What language model\narchitecture and pretraining objective works best for\nzero-shot generalization? In Int. Conf. on Machine\nLearning (ICML), pages 22964â€“22984. PMLR.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625â€“641.\nJanyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.\nAnnotating expressions of opinions and emotions\nin language. Language Resources and Evaluation,\n39:165â€“210.\n7883\nSixing Wu, Ying Li, Dawei Zhang, Yang Zhou, and\nZhonghai Wu. 2021. Topicka: Generating common-\nsense knowledge-aware dialogue responses towards\nthe recommended topic fact. In Int. Joint Conf. on\nArtificial Intelligence (IJCAI), pages 3766â€“3772.\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Rui Hou, Yux-\niao Dong, V .G.Vinod Vydiswaran, and Hao Ma. 2022.\nIDPG: An instance-dependent prompt generation\nmethod. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5507â€“5521.\nDa Yan, Wenwen Qu, Guimu Guo, Xiaoling Wang, and\nYang Zhou. 2022a. Prefixfpm: A parallel framework\nfor general-purpose mining of frequent and closed\npatterns. The VLDB Journal (VLDBJ) , 31(2):253â€“\n286.\nDa Yan, Yang Zhou, and Guimu Guo. 2022b. Think-\nlike-a-task programming model. Encyclopedia of Big\nData Technologies.\nDa Yan, Yang Zhou, Guimu Guo, and Hang Liu. 2022c.\nParallel graph processing. Encyclopedia of Big Data\nTechnologies.\nQiang Yang, Yang Liu, Tianjian Chen, and Yongxin\nTong. 2019. Federated machine learning: Concept\nand applications. ACM Transactions on Intelligent\nSystems and Technology (TIST), 10(2):1â€“19.\nZ. Yao, A. Gholami, K. Keutzer, and M. W. Mahoney.\n2020. Pyhessian: Neural networks through the lens\nof the hessian. In IEEE Int. Conf. on Big Data (Big\nData), pages 581â€“590. IEEE Computer Society.\nHonglin Yuan, Manzil Zaheer, and Sashank Reddi. 2021.\nFederated composite optimization. In Int. Conf.\non Machine Learning (ICML) , volume 139, pages\n12253â€“12266.\nHong Zhang, Ji Liu, Juncheng Jia, Yang Zhou, Huaiyu\nDai, and Dejing Dou. 2022. FedDUAP: Federated\nlearning with dynamic update and adaptive pruning\nusing shared data on the server. In Int. Joint Conf. on\nArtificial Intelligence (IJCAI), pages 2776â€“2782.\nQi Zhang, Ling Liu, Kisung Lee, Yang Zhou, Aameek\nSingh, Nagapramod Mandagere, Sandeep Gopisetty,\nand Gabriel Alatorre. 2014. Improving hadoop ser-\nvice provisioning in a geographically distributed\ncloud. In IEEE Int. Conf. on Cloud Computing\n(CLOUD), pages 432â€“439.\nQi Zhang, Ling Liu, Yi Ren, Kisung Lee, Yuzhe Tang,\nXu Zhao, and Yang Zhou. 2013. Residency aware\ninter-vm communication in virtualized cloud: Perfor-\nmance measurement and analysis. In IEEE Int. Conf.\non Cloud Computing (CLOUD), pages 204â€“211.\nZeru Zhang, Jiayin Jin, Zijie Zhang, Yang Zhou, Xin\nZhao, Jiaxiang Ren, Ji Liu, Lingfei Wu, Ruoming Jin,\nand Dejing Dou. 2021. Validating the lottery ticket\nhypothesis with inertial manifold theory. Advances\nin Neural Information Processing Systems (NeurIPS),\n34.\nHaodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and\nGongshen Liu. 2023. Fedprompt: Communication-\nefficient and privacy-preserving prompt tuning in fed-\nerated learning. In IEEE Int. Conf. on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 1â€“5.\nXin Zhao, Zeru Zhang, Zijie Zhang, Lingfei Wu, Ji-\nayin Jin, Yang Zhou, Ruoming Jin, Dejing Dou, and\nDa Yan. 2021. Expressive 1-lipschitz neural net-\nworks for robust multiple graph learning against ad-\nversarial attacks. In Int. Conf. on Machine Learning\n(ICML), pages 12719â€“12735.\nChendi Zhou, Ji Liu, Juncheng Jia, Jingbo Zhou, Yang\nZhou, Huaiyu Dai, and Dejing Dou. 2022. Efficient\ndevice scheduling with multi-job federated learning.\nIn AAAI Conf. on Artificial Intelligence (AAAI), pages\n9971â€“9979.\nYang Zhou. 2017. Innovative Mining, Processing, and\nApplication of Big Graphs . Ph.D. thesis, Georgia\nInstitute of Technology, Atlanta, GA, USA.\nYang Zhou and Ling Liu. 2013. Social influence based\nclustering of heterogeneous information networks. In\nACM SIGKDD Conf. on Knowledge Discovery and\nData Mining (KDD), pages 338â€“346.\nYang Zhou, Ling Liu, Kisung Lee, Calton Pu, and\nQi Zhang. 2015a. Policy-driven autonomic configu-\nration management for nosql. In ACM Symposium\non High-Performance Parallel and Distributed Com-\nputing (HPDC), pages 179â€“190.\nYang Zhou, Ling Liu, Kisung Lee, and Qi Zhang. 2015b.\nGraphtwist: Fast iterative graph computation with\ntwo-tier optimizations. The VLDB Journal (VLDBJ),\n8(11):1262â€“1273.\nYang Zhou, Sangeetha Seshadri, Lawrence Chiu, and\nLing Liu. 2014. Graphlens: Mining enterprise stor-\nage workloads using graph analytics. In IEEE Int.\nCongress on Big Data (BigData), pages 1â€“8.\n7884\nA Implementation Details\nA.1 Adam Update\nThe original Adam update (Kingma and Ba, 2015)\nis shown in Algorithm 3.\nAlgorithm 3 Adam update\nRequire:\nwtâˆ’1: The prompt parameters of the model at\nIteration tâˆ’1\nmtâˆ’1: The 1st momentum vector at Iteration\ntâˆ’1\nvtâˆ’1: The 2st momentum vector at Iteration\ntâˆ’1\ngt: The gradients corresponding to wtâˆ’1\nÎ²1,Î²2: Decay rates for the momentum in\nAdam\nÎ±: The step size\nEnsure:\nwt: The prompt parameters at Iteration t\nmt: The 1st momentum vector at Iteration t\nvt: The 2st momentum vector at Iteration t\n1: mt â†Î²1mtâˆ’1 + (1 âˆ’Î²1)gt\n2: Ë†mt â† mt\n1âˆ’Î²t\n1\n3: vt â†Î²2vtâˆ’1 + (1 âˆ’Î²2)(gt)2\n4: Ë†vt â† vt\n1âˆ’Î²t\n2\n5: wt â†wtâˆ’1 âˆ’Î± Ë†mt\n(\nâˆš\nË†vt+Ïµ)\nA.2 Details for Experimental Setup\nThe number of global training epochs is set to 100\nand that of local training epochs is set to 2. We\nutilize the Dirichlet distribution (with 1.0 as the\nconcentration parameter alpha) to partition the data\ninto non-IID splits and assign a certain number of\nsamples to each device according to the Dirichlet\ndistribution (with 5.0 as the concentration param-\neter alpha). We exploit development sets for the\nevaluation of tasks in the GLUE benchmark since\ntest sets are not labeled. For 4 other datasets, we se-\nlect a certain number of samples from the training\nset as the development set, and the number of sam-\nples for each label is determined according to its\nproportion in the original training set. For datasets\nin GLUE benchmark (Wang et al., 2019), we use\ntheir original data splits. For 4 other datasets with\nno default splits, we randomly divide the dataset\ninto train, development, and test sets. The dataset\nstatistics after the split are shown in Table 13\nWe set prompt lengths for each method accord-\ning to the original works, i.e., 128 for FedPrompt\nand P-tuning v2, 5 for LPT and IDPG, 100 for AT-\nTEMPT, and 20 for Prompt tuning. FedPrompt,\nPrompt tuning, and ATTEMPT insert prompts be-\nfore the transformer layers. P-tuning v2 inserts\nprompts to the hidden states for all transformer\nlayers. IDPG combines these two heuristics and\ninserts prompts to either the input or the hidden\nstates of all layers. LPT searches for the single best\nlayer by training all the possible positions for each\nlayer. Similar to P-tuning v2, FedPepTAO inserts\nthe hidden states for all transformer layers while\nthe prompt parameters of properly selected layers\nare communicated between devices and the server.\nB Extra Experiments\nB.1 Epochs Required to Achieve the Target\nAccuracy\nWe conducted experiments with the number\nof epochs required to achieve the target accu-\nracy and the communication overhead to demon-\nstrate the performance of FedPepTAO on the\nRoBERTaLARGE model and 10 tasks. Below are\nthe average epochs required to achieve the target\naccuracy.\nMethod Epochs\nAdapter 30.92\nFedPrompt 24.50\nP-tuning v2 5.99\nPrompt Tuning /\nIDPG 40.15\nATTEMPT 39.97\nLPT 8.31\nMomD 13.98\nMomS+AdamD 5.99\nFedPepTAO 3.88\nTable 6: The average number of epochs required to\nachieve the target accuracy (85% for QNLI, 92.5% for\nSST-2, 3% for CoLA, 77% for MRPC, 65% for RTE,\n71% for BoolQ, 85% for MPQA, 88% for Subj, 78%\nfor Trec, 91% for MR) on RoBERTaLARGE model. \"/\"\nrepresents that training does not achieve the target accu-\nracy.\nFrom Table 6, we find that FedPepTAO requires\nthe smallest amount of epochs to achieve the tar-\nget accuracy (87.45%, 84.16%, 35.26%, 90.33%,\n90.29%, 53.31%, 72.22%, 35.21% faster compared\nto Adapter, FedPrompt, P-tuning v2, IDPG, AT-\nTEMPT, LPT, MomD, and MomS+AdamD respec-\ntively). Prompt Tuning failed to achieve the tar-\nget accuracy since it only optimizes the first layer\nof soft prompts. FedPepTAO can also reduce the\n7885\ncommunication overhead from 40% to 41.55%\n(41.55%, 40%, 40%, 40% compared with Adapter,\nP-tuning v2, MomD, and MomS+AdamD, respec-\ntively) between devices and the server, as illustrated\nin Table 7. FedPrompt, Prompt-Tuning, IDPG, AT-\nTEMPT, and LPT correspond to lower communi-\ncation overhead (up to 13%) since they only select\none layer during tuning, which results in signifi-\ncantly inferior accuracy (from 17.02% to 60.8%\nlower) compared with FedPepTAO.\nMethod Time\nAdapter 5.80\nFedPrompt 3.09\nP-tuning v2 5.65\nPrompt Tuning 3.00\nIDPG 3.09\nATTEMPT 3.16\nLPT 3.09\nMomD 5.65\nMomS+AdamD 5.65\nFedPepTAO 3.39\nTable 7: The communication overhead (s) between de-\nvices and the server with RoBERTaLARGE model.\nB.2 Impact of server learning rate\nDue to the high sensitivity of the hyperparame-\nters in NLP tasks, we investigate the impact of\ndivers server learning rates on the RTE dataset.\nWe analyze the accuracy with the learning rates\n1eâˆ’2, 5eâˆ’3, 1eâˆ’3, 5eâˆ’4. As shown in Figure 2,\nwe find that the best performance was achieved\nwhen lr = 1eâˆ’3, which only slightly outperforms\nlr = 5eâˆ’4 by 0.37%. This demonstrates that Fed-\nPepTAO is easy to fine-tune in practice.\n0 20 40 60 80 100\nEpoch\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85Accuracy\nserver lr 1e-2\nserver lr 5e-3\nserver lr 1e-3\nserver lr 5e-4\nFigure 2: The impact of server learning rate.\nB.3 Impact of heterogeneous data distribution\nTo investigate the robustness of our approach to\ndifferent degrees of non-IID data, we conduct ex-\nperiments under various levels of non-IID degrees.\nFrom Figure 3, we observe that the accuracy of\nFedPepTAO is the best with Î± ranging from 1.0\nto 5.0. A smaller Î±represents a higher degree of\nheterogeneity. This demonstrates that our approach\nis robust to different data heterogeneity.\n0 20 40 60 80 100\nEpoch\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85Accuracy\nalpha=1.0\nalpha=2.0\nalpha=3.0\nalpha=4.0\nalpha=5.0\nFigure 3: The impact of various heterogeneity degrees.\nB.4 Impact of device number\nTo explore the scalability of our model, we conduct\nexperiments with divers number of devices, i.e.,\n100, 150, and 200. Figure 4 shows the correspond-\ning accuracy on the RTE dataset. The accuracy gap\nbetween the best and worst is only 0.73%, which\ndemonstrates that FedPepTAO is scalable in FL\nsettings.\n0 20 40 60 80 100\nEpoch\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85Accuracy\nM=100\nM=150\nM=200\nFigure 4: Evaluation of divers number of devices.\n7886\nB.5 Impact of diverse bandwidth\nWe carry out extra experimentation on two tasks,\ni.e., Subj and Trec with modest network band-\nwidth (reduced to 100 times smaller) on the\nRoBERTaLARGE model. We find that FedPep-\nTAO maintains its advantages in this setting, i.e.,\nup to 98.71%, 94.75%, 80.61%, 95%, 97.43%,\n56.52%, 84.55% faster compared with Adapter,\nFedPrompt, P-tuning v2, IDPG, ATTEMPT, LPT,\nMomS+AdamD to achieve the target accuracy, as\nshown in Table 8.\nMethod Subj Trec\nAdapter 3591 7908\nFedPrompt 1333 368\nP-tuning v2 361 474\nPromtp Tuning / /\nIDPG 1401 1345\nATTEMPT 2726 1184\nLPT 161 123\nMomD / /\nMomS+AdamD 453 496\nFedPepTAO 70 102\nTable 8: The tuning time (s) to achieve a target accuracy\n(88% for Subj, 78% for Trec) on RoBERTaLARGE model.\n\"/\" represents that training does not achieve the target\naccuracy.\nB.6 Parameter-efficient Prompt Tuning and\nrandom layer selection strategy\nIn order to clarify the impact of randomness in our\nexperiments, we conduct three experiments with\nrandom layer selection strategy on RTE dataset.\nAs shown in Table 9, FedPepTAO outperforms the\nrandom strategy with the accuracy gain of 2.53%,\n2.89%, and 3.25% respectively, which demon-\nstrates the superior performance of our FedPepTAO\nmethod.\nMethod Seed 42 Acc\nFedPepTAO 85.56%\nRandom 1 83.03%\nRandom 2 82.67%\nRandom 3 82.31%\nAvg acc gain 2.89%\nTable 9: The performance of FedPepTAO and random\nlayer choosing strategy on RTE dataset.\nIn addition, in order to further validate the im-\npact of randomness on different datasets, we con-\nducted additional experiments on three datasets\n(RTE, MRPC, and CoLA) with three randomly se-\nlected seeds (32, 35, and 37) to testify the strength\nof our Parameter-efficient Prompt Tuning method.\nTables 10, 11, and 12 exhibit that FedPepTAO out-\nperforms the random strategy by 2.91%, 4.64%,\nand 7.29% on RTE, MRPC, and CoLA datasets,\nrespectively. The above experiment results indi-\ncate that our FedPepTAO method can achieve sub-\nstantial improvement compared with the random\nstrategy.\nMethod Seed 32 Seed 35 Seed 37\nAcc Acc Acc\nFedPepTAO 84.84% 85.56% 85.92%\nRandom 1 82.31% 82.31% 82.31%\nRandom 2 81.59% 81.23% 83.39%\nRandom 3 82.67% 83.03% 83.75%\nAvg Acc Gain 2.65% 3.37% 2.77%\nTable 10: The performance of FedPepTAO and random\nlayer choosing strategy on RTE dataset under different\nrandom seeds\nMethod Seed 32 Seed 35 Seed 37\nAcc Acc Acc\nFedPepTAO 86.54% 87.09% 86.18%\nRandom 1 83.09% 81.86% 80.34%\nRandom 2 82.73% 82.03% 80.54%\nRandom 3 83.31% 81.83% 81.16%\nAvg Acc Gain 3.5% 5.18% 5.5%\nTable 11: The performance of FedPepTAO and random\nlayer choosing strategy on MRPC dataset under differ-\nent random seeds\nMethod Seed 32 Seed 35 Seed 37\nAcc Acc Acc\nFedPepTAO 56.94% 58.92% 56.49%\nRandom 1 46.45% 52.84% 49.58%\nRandom 2 51.57% 49.84% 45.71%\nRandom 3 50.77% 53.2% 51.45%\nAvg Acc Gain 7.34% 6.96% 7.58%\nTable 12: The performance of FedPepTAO and random\nlayer choosing strategy on CoLA dataset under different\nrandom seeds\nWe notice an inverse correlation between the per-\nformance of our Parameter-efficient Prompt Tuning\n(PEPT) method and the average sentence length\nof the three datasets. Specifically, PEPT tends to\nachieve a smaller performance gain on the datasets\nwith longer average sentence length, as shown in\nTable 14.\nA reasonable explanation is that the datasets with\nlonger average sentence length, such as RTE, often\ncontain more latent information. More/less latent\ninformation make them easier/more difficult to be\n7887\nCategory Datasets |Train| |Dev| |Test| |Y| Type Labels\nSingle-sentence\nSST-2 67349 872 1821 2 sentiment positive, negative\nMPQA 7606 1000 2000 2 opinion polarity positive, negative\nMR 7662 1000 2000 2 sentiment positive, negative\nSubj 7000 1000 2000 2 subjectivity subjective, objective\nTrec 4952 500 500 6 question cls. abbr., entity, description, human, loc., num.\nCoLA 8551 1043 1063 2 acceptability acceptable, unacceptable\nSentence-pair\nMRPC 3668 408 1725 2 paraphrase equivalent, not equivalent\nQNLI 104743 5463 5463 2 NLI entailment, not entailment\nBoolQ 9427 3270 3245 2 QA true, false\nRTE 2490 277 3000 2 NLI entailment, not entailment\nTable 13: The statistics of datasets evaluated in this work. |Y|is the number for classes.\nDataset Avg sentence length Avg acc gain\nRTE 71.91 2.91%\nMRPC 54.95 4.64%\nCoLA 16.37 7.29%\nTable 14: Average sentence length and accuracy im-\nprovements of FedPepTAO for three datasets.\nevenly distributed across all transformer layers, re-\nsulting in a relatively equal/diverse contribution\nby each layer during tuning. When different lay-\ners contain the similar/dissimilar amount of latent\ninformation, the impact of each unique layer is\naccordingly decreased/increased. Therefore, the\nrandom layer selection results in less/more accu-\nracy gain by our PEPT method. The above exper-\niment results demonstrate that our PEPT method\ncan achieve substantial performance improvement\non different datasets in most experiments.\n0 20 40 60 80 100\nEpoch\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85Accuracy\nRandom\nDescending\nAescending\nFedPepTAO\nFigure 5: Evaluation of divers layer selection strategies.\n0 20 40 60 80 100\nEpoch\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85Accuracy\nMomS+AdamD\nAdamD\nMomS+Con\nMomS\nMomD\nFedAvg\nFedPepTAO\nFigure 6: Evaluation for various optimization methods.\n7888",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7905914187431335
    },
    {
      "name": "Process (computing)",
      "score": 0.6237562894821167
    },
    {
      "name": "Baseline (sea)",
      "score": 0.599307656288147
    },
    {
      "name": "Code (set theory)",
      "score": 0.5644476413726807
    },
    {
      "name": "Federated learning",
      "score": 0.446679025888443
    },
    {
      "name": "Machine learning",
      "score": 0.36303406953811646
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3544304668903351
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82497590",
      "name": "Auburn University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I98301712",
      "name": "Baidu (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I12315562",
      "name": "Texas Tech University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I137902535",
      "name": "North Carolina State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4401726965",
      "name": "Boston Consulting Group (United States)",
      "country": null
    }
  ]
}