{
  "title": "GestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents",
  "url": "https://openalex.org/W4387839069",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2120084296",
      "name": "Zeng Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1778731936",
      "name": "Wang Xiaoyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3110447774",
      "name": "Zhang Tengxiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100556307",
      "name": "Yu Chun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350174369",
      "name": "Zhao Shengdong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2226484730",
      "name": "Chen Yiqiang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2766813539",
    "https://openalex.org/W2796156077",
    "https://openalex.org/W2138207021",
    "https://openalex.org/W3160750879",
    "https://openalex.org/W3036954260",
    "https://openalex.org/W4378945542",
    "https://openalex.org/W2120629103",
    "https://openalex.org/W4312817377",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W3005789917",
    "https://openalex.org/W2293649818",
    "https://openalex.org/W4386385424",
    "https://openalex.org/W2120331170",
    "https://openalex.org/W4389520007",
    "https://openalex.org/W3158889682",
    "https://openalex.org/W4394593134",
    "https://openalex.org/W2794495458",
    "https://openalex.org/W2535391349",
    "https://openalex.org/W2959856712",
    "https://openalex.org/W4385262076",
    "https://openalex.org/W2394878726",
    "https://openalex.org/W4387835481",
    "https://openalex.org/W4388328502",
    "https://openalex.org/W4385970120",
    "https://openalex.org/W2015776973",
    "https://openalex.org/W2126698653",
    "https://openalex.org/W2808485300",
    "https://openalex.org/W4283828996",
    "https://openalex.org/W4378718522",
    "https://openalex.org/W4385473486",
    "https://openalex.org/W2162425553",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4384112348",
    "https://openalex.org/W4387322930",
    "https://openalex.org/W2902887961",
    "https://openalex.org/W2113615789",
    "https://openalex.org/W2079788562",
    "https://openalex.org/W2151348441",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4294891905",
    "https://openalex.org/W4321524280",
    "https://openalex.org/W4381827077",
    "https://openalex.org/W1480141383",
    "https://openalex.org/W2791526950",
    "https://openalex.org/W3207910091",
    "https://openalex.org/W2073098112",
    "https://openalex.org/W4213453073",
    "https://openalex.org/W2961565096",
    "https://openalex.org/W4386184788",
    "https://openalex.org/W2944490436",
    "https://openalex.org/W4285731637",
    "https://openalex.org/W2036260418",
    "https://openalex.org/W2113798388",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4387561549",
    "https://openalex.org/W4380374951",
    "https://openalex.org/W3035531123",
    "https://openalex.org/W4390189088",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W1496918241",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4389325864",
    "https://openalex.org/W4220783478",
    "https://openalex.org/W4379919478",
    "https://openalex.org/W4361021272",
    "https://openalex.org/W4385849309",
    "https://openalex.org/W4390601403",
    "https://openalex.org/W4225082377",
    "https://openalex.org/W4384812233",
    "https://openalex.org/W2906219823",
    "https://openalex.org/W101345952",
    "https://openalex.org/W2984660088",
    "https://openalex.org/W4399528455",
    "https://openalex.org/W4229038857",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W2399596426",
    "https://openalex.org/W2795885990",
    "https://openalex.org/W4392503799",
    "https://openalex.org/W4327810158"
  ],
  "abstract": "Existing gesture interfaces only work with a fixed set of gestures defined either by interface designers or by users themselves, which introduces learning or demonstration efforts that diminish their naturalness. Humans, on the other hand, understand free-form gestures by synthesizing the gesture, context, experience, and common sense. In this way, the user does not need to learn, demonstrate, or associate gestures. We introduce GestureGPT, a free-form hand gesture understanding framework that mimics human gesture understanding procedures to enable a natural free-form gestural interface. Our framework leverages multiple Large Language Model agents to manage and synthesize gesture and context information, then infers the interaction intent by associating the gesture with an interface function. More specifically, our triple-agent framework includes a Gesture Description Agent that automatically segments and formulates natural language descriptions of hand poses and movements based on hand landmark coordinates. The description is deciphered by a Gesture Inference Agent through self-reasoning and querying about the interaction context (e.g., interaction history, gaze data), which is managed by a Context Management Agent. Following iterative exchanges, the Gesture Inference Agent discerns the user's intent by grounding it to an interactive function. We validated our framework offline under two real-world scenarios: smart home control and online video streaming. The average zero-shot Top-1/Top-5 grounding accuracies are 44.79%/83.59% for smart home tasks and 37.50%/73.44% for video streaming tasks. We also provide an extensive discussion that includes rationale for model selection, generalizability, and future research directions for a practical system etc.",
  "full_text": "GestureGPT: Toward Zero-Shot Free-Form Hand Gesture\nUnderstanding with Large Language Model Agents\nXIN ZENG∗, Institute of Computing Technology, Chinese Academy of Sciences, China and University of\nChinese Academy of Sciences, China\nXIAOYU WANG∗† , The Hong Kong University of Science and Technology, China\nTENGXIANG ZHANG‡, Goertek Inc., China\nCHUN YU, Computer Science and Technology, Tsinghua University, China\nSHENGDONG ZHAO, Synteraction Lab, City University of Hong Kong, China\nYIQIANG CHEN§, Institute of Computing Technology, Chinese Academy of Sciences, China and University\nof Chinese Academy of Sciences, China\nGesture Summary Description Generation\nGesture Inference Agent\nUser Video Stream\nContext Management AgentContext Library\nGesture DescriptionBased on Matrix\nGesture Input Capture\nData Flow\nModulesandAgents\n-Thumb and index finger are straight;-Middle, ring and pinky are bent and pressed together;-Palm faces left;-Thumb tip is in contact with only index finger;-The hand moves up significantly.\nGestureSummary DescriptionHand Landmark Series\n(f)\n(i)\nGaze InformationGaze is looking at the light.\nInterface Function''Light: On/Off'': On''Light: Brightness Control'': 12''Light: Mode Switch'': None''Smart Screen: On/Off'': On''Oven: On/Off'': On''Oven: T emperature'': None...\nInteraction history19:00 The user open the light.\nExternal ContextIt is 19:05 now.\nInferencinginConversationCan … function list…? function listinclude: … … Gaze information? Looking at Light device… … History  Behavior? In 19:00, user unlocked… … Other Information? System time: 19:05 … \nSeems to be Brightness Control or Mode Switch.Definitelyrelated to a 'Brightness Increase' action.\nCould berelated to a lighting function.\nInference AgentConfidence\nSo, Conclusion: The target function is 'Brightness Increase'\n(k)\n(h)\n(g)\n(a)\n(b)\n(d) (e)\n(j)\nFinger Flexion\nFinger Proximity\nFingertip Contact\nPalm Orientation\nThumb Direction\nHandMovement\nRule-Based Gesture Description Generation\nGesture Description Agent(c)\nFig. 1. This scenario illustrates a system implemented based on GestureGPT.\nExisting gesture interfaces only work with a fixed set of gestures defined either by interface designers or by\nusers themselves, which introduces learning or demonstration efforts that diminish their naturalness. Humans,\n∗Both authors contributed equally to this research.\n†This work was completed during an internship at ICT, CAS.\n‡Corresponding author. Part of the work was conducted while the author was at ICT, CAS.\n§Corresponding author.\nAuthors’ Contact Information: Xin Zeng, Institute of Computing Technology, Chinese Academy of Sciences, Beijing,\nChina and University of Chinese Academy of Sciences, Beijing, China, zengxin18z@ict.ac.cn; Xiaoyu Wang, The Hong\nKong University of Science and Technology, Hong Kong, China, xwangij@connect.ust.hk; Tengxiang Zhang, Goertek Inc.,\nBeijing, China, ztxseuthu@gmail.com; Chun Yu, Computer Science and Technology, Tsinghua University, Beijing, China;\nShengdong Zhao, Synteraction Lab, City University of Hong Kong, Hong Kong, China; Yiqiang Chen, Institute of Computing\nTechnology, Chinese Academy of Sciences, Beijing, China and University of Chinese Academy of Sciences, Beijing, China,\nyqchen@ict.ac.cn.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,\ncontact the owner/author(s).\n© 2024 Copyright held by the owner/author(s).\nACM 2573-0142/2024/12-ART545\nhttps://doi.org/10.1145/3698145\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\narXiv:2310.12821v5  [cs.CL]  4 Nov 2024\n545:2 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\non the other hand, understand free-form gestures by synthesizing the gesture, context, experience, and\ncommon sense. In this way, the user does not need to learn, demonstrate, or associate gestures. We introduce\nGestureGPT, a free-form hand gesture understanding framework that mimics human gesture understanding\nprocedures to enable a natural free-form gestural interface. Our framework leverages multiple Large Language\nModel agents to manage and synthesize gesture and context information, then infers the interaction intent\nby associating the gesture with an interface function. More specifically, our triple-agent framework includes\na Gesture Description Agent that automatically segments and formulates natural language descriptions\nof hand poses and movements based on hand landmark coordinates. The description is deciphered by a\nGesture Inference Agent through self-reasoning and querying about the interaction context (e.g., interaction\nhistory, gaze data), which is managed by a Context Management Agent. Following iterative exchanges, the\nGesture Inference Agent discerns the user’s intent by grounding it to an interactive function. We validated our\nframework offline under two real-world scenarios: smart home control and online video streaming. The average\nzero-shot Top-1/Top-5 grounding accuracies are 44.79%/83.59% for smart home tasks and 37.50%/73.44% for\nvideo streaming tasks. We also provide an extensive discussion that includes rationale for model selection,\ngeneralizability, and future research directions for a practical system etc.\nCCS Concepts: • Human-centered computing →Gestural input; User interface management systems .\nAdditional Key Words and Phrases: Free-Form Gesture, Zero-Shot, Gesture Recognition, Interaction Context\nACM Reference Format:\nXin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen. 2024. GestureGPT:\nToward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents. Proc. ACM\nHum.-Comput. Interact. 8, ISS, Article 545 (December 2024), 38 pages. https://doi.org/10.1145/3698145\n1 Introduction\nGestures express human intent intuitively and promptly, enabling natural human-computer inter-\naction with low cognitive load [50, 72]. Most existing gesture interfaces support only predefined\ngestures, which can achieve a very high gesture classification accuracy [3, 65]. However, it requires\nsignificant effort to learn and memorize different gestures and their respective mappings to system\nfunctions, particularly with a large set of gestures [11, 15]. The gesture-function mapping is also\nfixed, which cannot be easily expanded and may contradict users’ preferences [8, 45, 64].\nTo address the limitations of predefined gestures, researchers have proposed gestural interfaces\nthat support user-defined gestures [ 37, 67, 73]. Users can define their own gestures for each\nfunction with only a few demonstrations, thus eliminating the learning effort and enhancing\nsystem flexibility. However, each user must design and demonstrate their own gesture and associate\nit with a system function [ 41, 57]. Users also still need to memorize their own gestures, which\nmay lead to frustration using the interface, especially when there are a large number of functions.\nSuch constraints degrade the natural interaction experience and hinder the broad adoption of\ngestural interfaces, leading to Norman’s famous argument that “Natural User Interfaces are Not\nNatural” [46]. While significant advances have been made in gesture recognition technologies\nover the years, this naturalness challenge of gestural interfaces persists. To address the pursuit\nof more natural gestural expressions, users expect high-frequency gestures to seamlessly link\nvarious functions (e.g., common touchscreen gestures like zooming in/out, or typical semantic\ngestures like thumbs-up and rock-and-roll) without being confined to a limited set of expressions\nor frequently needing to customize their own [26, 34]. Additionally, as gestural interfaces become\nmore widespread, users will likely experience significant improvements in both their familiarity\nwith these expressions and the speed of their interactions [44, 58].\nTo that end, future gestural interfaces require direct, end-to-end gesture understanding rather\nthan merely gesture recognition . Users should not have to learn, memorize, or demonstrate\nspecific gestures. Instead, they can perform gestures naturally, according to their understanding of\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:3\nthe function’s semantics and their everyday interaction experiences with humans and machines.\nThe interface automatically links the gesture to its corresponding function, considering both the\ngesture and the interaction context. This gesture understanding task is inherently more challenging\nthan simple gesture recognition.\nWe propose GestureGPT, an LLM-based paradigm that comprehends natural free-form hand\ngestures and automatically links them to their intended functions. The core idea of GestureGPT is\nto utilize the LLM’s rich common sense for recognizing gestures and understanding the interaction\ncontext, as well as its potent inference capabilities to map gestures to their intended functions.\nConstructing such an LLM-based gesture understanding framework demands considerable effort.\nFor instance, gestures and context information must be transformed and formatted in a way that\nLLMs can process. Additionally, there needs to be a mechanism for robust and thorough synthesis\nof all relevant information without overlooking any details.\nTo that end, we design a triple-agent paradigm that generates gesture descriptions, manages\nthe context, and infers the intended function. This involves a Gesture Description Agent , a Gesture\nInference Agent, and a Context Management Agent , all based on LLMs. 1) The Gesture Description\nAgent describes hand gestures in natural languages to facilitate the powerful text-based LLMs’\nreading and understanding of gestures. A set of specially designed and tuned rules transforms\nhand poses and movements into discrete states ( e.g., the bending state of each finger) based on\nvisually extracted hand landmarks, triggered by raising the hand to the chest level. The agent\nthen synthesizes a general description of the gesture based on a matrix formed with the states. 2)\nThe Gesture Inference Agent analyzes the description and initiates a conversation with the Context\nManagement Agent to inquire about the context information. After multiple rounds of dialogue, it\ninfers which interactive function the user intends to activate. 3) The Context Management Agent\nanswers questions from the Gesture Inference Agent by referring to a context library, which includes\nvarious types of context information, such as gaze points and interaction history, etc. The context\nlibrary is organized in JSON file format.\nFigure 1 envisions a process in which a system, implemented through the GestureGPT concept,\nunderstands a user’s gesture. In this scenario, he (a) triggers the system with a raised right hand\nabove the chest, followed by looking at the light and performing a “pinch and move up” gesture.\nThe (b) input camera captures this action through real-time video streaming. (c) The Gesture\nDescription Agent processes the gesture by (d) filtering key frames from the video and extracting\nhand landmarks to (e) generate the gesture state matrix. Subsequently, (f) Gesture Description Agent\ngenerates a description of the gesture’s pose and movement. Upon activation by the description, the\n(g) Gesture Inference Agent analyzes this description to determine which function the gesture maps\nto and (h) assesses the confidence level of the result. If the confidence is deemed insufficient, Gesture\nInference Agent requests context information from the (i) Context Management Agent, which\nretrieves relevant data from the (j) context library containing all available context information in\nthe current environment, to inform the inquiry. According to the inquiry, as context information\nsuch as “Gaze: user is looking at the light” and “History: the user turned on the light at 19:00”\nis incorporated, the Gesture Inference Agent (k) concludes that the function designated by the\ngesture is to “increase the brightness” of the light. The function could then be triggered to finish\nthe interaction process.\nGestureGPT offers a number of advantages that address the challenges and limitations of previous\ngestural interfaces. 1) GestureGPT allows users to perform natural free-form hand gestures, which\ndo not need to resemble those in a predefined gesture set or those previously defined by users. This\neliminates the need for learning, memorizing, or demonstrating specific gestures. 2) GestureGPT\nautomatically associates gestures with their corresponding interface functions through a step-by-\nstep inference process based on both the gesture description and the interaction context. Such a\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:4 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\nnovel LLM-based structure successfully addresses the ephemeral nature of gestures and their close\nrelationship with context [46]. Additionally, the integration of context further enhances the accuracy\nof gesture mapping. 3) GestureGPT analyzes spatial coordinates of hand landmarks to generate\ngesture descriptions, making it independent of view angles and even modalities. For example, hand\nlandmarks can also be reconstructed by wearable sensors [28, 81]. Such flexibility makes it easy to\nadapt GestureGPT for a wide range of applications. The natural language descriptions also preserve\nuser privacy and reduce data transmission load, which are important for interaction interfaces.\nDescribing nuanced finger states and movements is crucial for accurately understanding hand\ngestures. Our hand landmarks-based method captures finger states accurately and thoroughly by\nfirst calculating a set of discrete finger states based on rules, then using an LLM to synthesize a\ngesture summary. The rules’ parameters are tuned using third-person view public gesture image\ndatasets and tested on both third-person view (HaGRID [30], 38576 test samples) and first-person\nview datasets (EgoGesture [ 80], approximately 5000 test samples). The error rates are 2.3% for\nthird-person views and 6.3% for first-person views, respectively. Two gesture experts rated the\nsynthesized summary, which achieved a score of 3.51 (std = 1.14) on a 5-point Likert scale, where 1\nindicates that almost none of the description is correct and relevant, and 5 indicates that almost all\nof it is.\nTo evaluate our framework, we conducted experiments under two realistic interaction scenarios:\nsmart home IoT device control using an AR headset (first-person view with 18 functions) and online\nvideo streaming on a desktop PC (third-person view with 66 functions). The highest zero-shot\nTop-1/Top-5 gesture grounding accuracies achieved were 44.79%/83.59% for smart home control and\n37.50%/73.44% for video streaming. We report accuracies at various levels to more comprehensively\nunderstand and demonstrate the framework’s performance and boundaries. The results demonstrate\nthe significant potential of GestureGPT, which, to the best of our knowledge, is the first zero-\nshot free-form gesture understanding framework that requires no learning, memorization,\ndemonstration, or association efforts.\nHowever, GestureGPT is not yet a practical interface ready for everyday use, primarily due to the\nslow inference speeds of current LLM systems. Our current system averages 227 seconds per task\nin our evaluations, a delay caused by long prompts, multi-turn dialogues, and rate limits enforced\nby LLM cloud services. Instead, it introduces a new paradigm for gestural interfaces. To the best\nof our knowledge, GestureGPT is the first feasible framework for the inherently difficult task of\nunderstanding free-form gestures. Previous efforts in gesture recognition have not successfully\naddressed free-form gestures, let alone the more complex challenge of gesture understanding.\nGestureGPT, therefore, not only pioneers a new approach but also lays the foundational framework\nfor future advancements. This framework can also be easily expanded to recognize a broader range\nof user intents through different modalities, such as facial expressions and body postures, among\nothers. Technologies such as edge-side LLMs or specifically fine-tuned large multi-modal models\ncould significantly reduce response times, paving the way for a more practical implementation of\nthis paradigm in the future.\nOur contribution is three-fold:\n(1) We proposed and evaluated the first framework that mimics human reasoning processes to\nachieve automatic free-form hand gesture understanding, as to our best knowledge.\n(2) We designed a set of gesture description rules based on hand landmarks to thoroughly and\naccurately capture states and movements of the hand, which has comparable performance to\nSOTA large multi-modal modal GPT-4o.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:5\n(3) We carefully crafted prompts for each agent, analyzed the offline evaluation results, and\ngenerated various insights. Such insights are invaluable for future context-aware agent-based\ngesture understanding work.\n2 Background and Related Work\n2.1 LLM as Autonomous Agent\nLarge language models have displayed an exceptional ability to understand and execute a broad\nspectrum of tasks [2, 47]. LLM has the potential to emulate human-level intelligence, accurately\nperceive generalized environments, act accordingly, and iterate to enhance outcome [55] when\nfaced with diverse situations [66]. Agents based on LLMs have shown potential in various domains,\nfrom web browsing [9, 74, 83], strategic planning [75] to robotic control [4, 10]. This paper, on the\nother hand, addresses a notable gap in existing literature and utilizes LLMs for free-form gesture\nunderstanding and interaction.\nThough LLM agents have shown promising intelligence, a single agent implementation may suf-\nfer from performance degradation in long context scenarios [27]. Instead, multi-agent systems have\nshown superiority to accomplish more complex tasks in collaboration, reducing hallucination [68],\nand information exchange [61]. For instance, Park et al. [49] designed a multi-agent system that\nsimulates human behavior in a virtual environment. Park et al. [48] relies on conversational inter-\nactions among multiple agents to aid online decision-making, which shows multi-agent’s great\npotential in reasoning under unfamiliar scenarios. Other applications software development [51],\nspan reasoning [35], evaluation [5, 79], and a myriad of intricate tasks [48, 85]. So, GestureGPT\nchooses a triple-agent architecture to better handle the complicated context-aware gesture under-\nstanding task. The goals of the three agents are clearly defined and isolated, so that they can be\noptimized individually while collaborate seamlessly to achieve accurate gesture comprehension.\n2.2 Natural Free-form Gesture Understanding\nGestural interfaces working with pre-defined gesture set demand large annotated dataset, and\nthe confinement to pre-defined gestures also hampers the naturalness of interaction [ 70]. The\nconflicts in different system designs further exacerbate user adaptation challenges across platforms.\nUser-defined gestural interfaces mitigates the learning burden by allowing users to define their\nown gestures. Only several demonstrations of the gesture are necessary for the system to learn\nnew gestures with the help of advanced few-shot learning algorithms [ 73]. Gesture Coder [37]\nallows the user to demonstrate a gesture, and, instead of defining a gesture name for it, the user\ndirectly associate it to a designated function with auto-generated recognizer. But in these works,\nusers still need to demonstrate and memorize the gesture while ensuring its distinctiveness from\nexisting gesture commands.\nNevertheless, all aforementioned approaches are still limited by a finite set of distinguishable\ngestures defined by interface designers or users. This leads to a mismatch between the fixed\ngesture set and the flexible gestures humans perform in different scenarios, greatly restricting\ngesture expressiveness. Thus, both pre-defined and user-defined gestural interfaces require users\nto adapt to them, rather than the reverse [70]. Free-form gestural interfaces, however, do not have\nthese limitations. For instance, Gesture Avatar [36] enables any drawing gesture on a screen. The\ninput gesture is linked to a UI element based on appearance resemblance, providing an intuitive\ninteraction experience. Yet, gaps remain in the research of free-form hand gesture understanding.\nCompared with screen gestures, hand gestures are more complex, do not necessarily resemble UI\nelements, and can vary under different scenarios for the same function.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:6 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\nRecent advances in zero-shot learning make it possible to recognize unseen gesture classes [39, 40,\n71]. Madapana [39, 40] proposed frameworks that define a set of attributes for gestures, recognizing\nunseen gestures by manually labeling these attributes for each unseen gesture. Similarly, Wu et al.\n[71] introduced a prototype-based approach that requires the definition of “semantic prototypes”\nfor unseen gesture categories. These methods explore to some extent the recognition of “unseen”\ngestures, enabling the system to recognize new gestures without additional training.\nHowever, above methods focus solely on the task of gesture recognition. Even for the recognition\ntask, they only identify key features of new gestures, not their semantic names. Gesture under-\nstanding, however, requires not only recognizing gestures but also mapping them to functions,\nwhich demands a comprehensive integration of complex contextual information (such as available\nfunctions, interaction history, and physical environment). Such analysis and reasoning necessitate\nmimicking human thought processes, a task that traditional machine learning models struggle to\nachieve.\nTo tackle such challenges, our framework relies on LLMs to understand the semantic meanings\nof both the hand gestures and the system functions to ensure correct gesture-function mapping,\nbecause LLMs are currently the most promising tools capable of human-alike inference and com-\nprehensive context understanding. An overview of current gesture-based interaction systems and\nthe placement of our work is shown in Figure 2.\nGestureGPT\nXu et al., 2022\nPre-definedGesturesUser-definedGesturesFree-formGestures\nAssociationFor mapping the input to a function\nUser EffortsLearningWhen using the system for the first time\nWexelblat, 1995*\n...Madapana and Wachs, 2018Wu et al., 2021\nMontero and Sucar, 2006Taranta Ii et al., 2015\nGesture Coder (Lü and Li, 2012)\nMemorizationWhen using the system later onDemonstrationFor recording the gesture example\n+ + - -+ + +- - - --\nRelated WorkReferencesGesture Type\nGesture Avatar (Lü and Li, 2011)\nHand Gesture / Screen Gesture /Body Gesture /...Hand Gesture / Screen Gesture\nHand GestureScreen Gesture\nHand Gesture- - - +\nFig. 2. Overview of current gestural interaction systems.\n2.3 Gesture Understanding with Context Information\nAs Norman pointed out, gestures are ephemeral in nature and highly context related [46]. They in-\nherently possess diverse semantics across different contexts, and may embody social metaphors [59].\nContextual information such as spatial distance [42], gaze [6], speech [70], user history [7, 43] and\ndomain-specific knowledge [62] have been integrated into gestural interaction systems to improve\ngesture recognition accuracy. However, most previous work only leverage gaze information for\nsimplified tasks [1, 16, 23, 54, 56]. The handling of different types of context typically require highly\nspecialized models [7, 24, 56], which limits the generalizability of such systems.\nLLM agents’ promising ability to solve complex problems provides an alternative solution for\ninference based on different types of context information. LLM agents can retrieve higher-level\ncontext like semantic meaning of interaction elements [21] and users’ profile and preferences [52].\nThe way it utilizes context and the intentions it can deal with are also greatly enlarged. For\nexample, [20, 31, 32, 52] have shown that LLM agents can turn loosely-constrained commands\ninto appropriate actions. Inspired by existing research, we use a Context Management Agent to\nmanage context information stored in the context library, and a Gesture Inference Agent to infer the\nintention behind a gesture based on context cues.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:7\nGestureInferenceAgent\n ContextManagement Agent\nPervasive Environment\n... ...\nCan you provide the function listof the current context? What kind of typeinformation can you provide me with?The current interaction context includes the followingfunctions... The typesof information I can provide are: ...Could you tell me user's gaze positionand interaction history? Based on current knowledge library, I don't have access to ...Could you provide the current interface statusto me?Based on the interface described in my Context Knowledge Library, the current status is as follows: ...Context Library\ninterfacedescription\ngaze position\ninteraction historyexternal context\n[Answer]1. next video button2. play/pause video button3. resolution switch 4. play speed switch 5. volume vertical slider\n[Reason]...\nConclusion\nAll known information has been shared; no additional information can be provided.\nQuery&Update\nPerceive\nHand Movement\nFinger FlexionFinger ProximityFingertip Contact\nPalm Orientation\n…\nSummary Line by LineObservationPossible Gestures\nSummary Line by LineObservationPossible Gestures…… Gesture NameSupplemental ExplanationTime Span\nSupporting ObservationsGesture Pose Description\nGesture Movement DescriptionSummary Line by LineObservation\nGestureDescriptionAgent\nHere comes anew gesture. Pose Description: ...Movement Description: ...OK\nThumb Direction\nGestureState Matrix\nFig. 3. Agents Collaboration Workflow.\n3 Method\nGestureGPT has a triple-agent framework to efficiently manage gesture description, context, and\ninference tasks, enhancing system flexibility and scalability while addressing the shortcomings of\nsingle or dual agent systems, consists of: (1) Gesture Description Agent, generating descriptions\nof gestures from videos; (2) Context Management Agent, handling interaction context; and (3)\nGesture Inference Agent, synthesize information to infer gesture intentions through dialogue and\nreasoning.\nThe workflow initiates with the transformation of user gestures, captured by an RGB camera, into\nnatural language description. The Gesture Description Agent first uses a set of rules to transform the\ngesture into a “Gesture State Matrix”, which delineates hand and finger states over time. The agent\nthen relies on this matrix to produce a summary of gestures, which is forwarded to the Gesture\nInference Agent . Upon receiving the gesture description, the Gesture Inference Agent engages in\nmulti-round dialogue with the Context Management Agent . This involves identifying and soliciting\nrelevant context information from a context library managed by the Context Management Agent .\nThrough iterative dialogue, theGesture Inference Agent gains a comprehensive understanding of the\ninteractive scenario and completes a dynamic mapping between the gesture and possible functions.\n3.1 Gesture Description Agent\nThe Gesture Description Agent is crucial for translating video-captured gestures into natural\nlanguage descriptions that is understandable by LLMs. We choose LLM since it has richer common\nknowledge and stronger inference than existing Large Multimodal Models (LMMs) that we can\naccess, which is vital to deal with context-aware free-form gesture understanding tasks.\n3.1.1 Rule-Based Gesture Description Generation We design a set of 6 rules to encode the hand\ngesture in terms of finger flexion, proximity, contact, direction, as well as palm orientation and\nhand position (Table 1). The detailed rule definition can be found in Appendix A.1.\nEach state is calculated using the coordinates of hand landmarks, as shown in Fig 4, which are\ngenerated by MediaPipe [77] from videos captured with an RGB camera. For each rule, parameters\n(such as the distance between fingers to determine their proximity) are trained on a third-person\nview (i.e., viewed from an external perspective, typically through a camera not aligned with the\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:8 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\nmcp\ntip\n(a) (b) (c) (d) (e)\nFig. 4. Illustration of gesture description rules. (a) The flexion of a finger is calculated from the sum of\nbending angle of ip joint (for thumb) or pip and dip joint (for other fingers). (b) The proximity of two fingers\nis calculated from the average distance from each finger’s pip/dip/tip joint to the other finger. (c) The contact\nof thumb and another finger is calculated from the distance of their fingertips. (d) The pointing direction of\nthumb is calculated from the vector from thumb’s mcp to tip. (e) The palm orientation is calculated from the\ndot product of the two vectors on the hand, pointing towards the reader.\nsubject’s viewpoint) public gesture dataset, and tested on first-person view ( i.e., viewed from\nthe subject’s own perspective, typically through a head-mounted camera) and third-person view\ndatasets. Test results show an error rate of 2.3% for third-person view and 6.3% for first-person\nview samples, indicating the rules’ efficacy in retaining accurate information from gestures across\ndifferent viewpoints, validating it a universal solution applicable to various gesture capture devices.\nThe details of parameter training and the evaluation metrics for these rules are thoroughly described\nin Appendix A. The pseudocode for the rules is provided in Appendix A.3. An evaluation and error\nanalysis of the rules can be found in Appendix A.2.\nThe gesture period of interest starts when users raises their hands to or above their chest level.\nFrames within a gesture period are processed at 0.2-second intervals, with each sampled frame\nundergoing rule-based calculations. However, the natural language descriptions concatenated from\neach rule are too long to fit into a prompt of an LLM. So we introduce a gesture state matrix\nthat contains vectors of each frame, which capitalizes on LLMs’ proficiency with code-formatted\ncontent [14]. The detailed definition of the gesture state matrix is given in Appendix A.4.\n3.1.2 Gesture Summary Description Generation The system needs to extract the rich information\nembedded in the matrix and generate a concise summary of gesture descriptions for the Gesture\nInference Agent. A significant challenge is the gesture state matrix data type; the pre-trained data\nof the LLM likely does not contain exactly similar data. We employ a chain-of-thought [69] process\nto guide the LLM step-by-step, supplemented by high-quality expert examples that demonstrate\nhow the analysis should be conducted. We choose to use two prompts to generate the hand pose\n(finger flexion, finger proximity, fingertip contact, thumb direction and palm orientation) and hand\nmovement description separately. As is shown in Figure 3, the Gesture Description Agent first\nconverts pose-related rows of the matrix to descriptions of possible gestures, along with the time\nspan during which this gesture happened. The time span is then used to select the corresponding\npart of movement-related rows, and another prompt is used to convert it to movement description.\nAs long as the agent correctly finds the valid pose of the gesture, irrelevant movements are naturally\nfiltered out.\nFor pose description, we developed a prompt structured into three parts shown in Figure 5: Intro-\nduction, Procedure and Examples. The introduction outlines the task, describes the the gesture state\nmatrix, and explains the concept of “interactive gestures” versus non-ineractive hand movements,\nhelping the agent to distinguish between different types of gesutres and understand their lifecycle.\nThe procedure section teaches the agent to understand the gesture state matrix and guess possible\ngestures from the matrix, while addressing some common mistakes to regulate its behaviors. In\npaticular, it instructs the agent to decompose the matrix, guess possible gestures from each part,\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:9\nTable 1. Summary of rules, their meanings, and corresponding values.\nRule Name Applicable to Calculation Value\nFinger Flexion Thumb, Index, Middle,\nRing, Pinky\nmeasured as the total\nbending angle of each\njoint: for the thumb,\nthe IP joint; for other\nfingers, the PIP and\nDIP joints.\n1: Straight;\n0: Between;\n-1: Bent\nFinger Proximity Index-Middle, Middle-\nRing, Ring-Pinky\ncalculated as the\naverage minimal\ndistance from each\nfinger’s joint to the\nother finger.\n1: Pressed Together;\n0: Between;\n-1: Apart\nThumb Fingertip Con-\ntact\nThumb-Index,\nThumb-Middle,\nThumb-Ring, Thumb-\nPinky\ncomputed as the dis-\ntance between their\nfingertips.\n1: Contact;\n0: Between;\n-1: No Contact\nPointing Direction Thumb the direction from\nthumb’s mcp joint to\ntip joint.\n1: Upward;\n-1: Downward;\n0: Other Direc-\ntions/Bent\nPalm Orientation Palm computed as the direc-\ntion to which the palm\nis facing.\nOne-hot encoding:\n[Left, Right, Down,\nUp, Inward, Out-\nward];\nAll zeros: Unknown\nHand Position Hand computed as the ge-\nometrical center of a\nhand by taking av-\nerage of all 21 land-\nmarks’ coordinates.\nFloat coordinates\nand synthesize the conclusions from all parts to get the most possible gestures, encouraging a\nhuman-like process of understanding and summarizing data, while using transcription to counter\nLLM’s forgetfulness and improve accuracy. The last part provides two typical examples of static and\ndynamic gestures to enhance LLM performance by reflecting back to the guidelines stated before.\nThis well-structured prompt ensures that the LLM model correctly processes the matrix data type.\nThe hand movement description prompt shares a similar structure. The generated description of\nthe gesture will be like:\n- Thumb and Index Finger transition from non-specific/bent to straight.\n- Middle, Ring, and Pinky Fingers transition from bent to straight.\n- Fingers start close together and then spread apart.\n- Thumb starts in contact with all fingertips but then moves away.\n- Palm orientation starts facing left but becomes non-specific.\n- The hand moves left slightly with negligible vertical and depth movements.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:10 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\n# Introduction：\n## Task Introduction: \nYour task is to analyze hand poses ... identifying the gesture ... \n## Input Data Introduction: \nA 2D array ... represents hand and finger states. Each time step \nis 0.2 seconds ...\n## Gesture Introduction: \nUsers can use static or dynamic gestures to interact ...\n# Procedure: \n## step 1: \nDecompose the input array ...\n## step 2: \nSynthesize observations and guesses gesture ...\n## Behavior Guide: \nthings should and should not to do ... Commmon Mistakes ...\n# Examples\n## Example 1\n（A specific example of Static Gesture ‘peace’）\n## Example 2\n（A specific example of Dynamic Gesture ‘Zoom In with Two Fingers’）\nGesture Description Agent - Pose (Prompt Overview)\nFig. 5. Prompt for Gesture Description Agent.\n# Introduction：\n## Task Introduction: \nyou will understand an interactive gesture from its description \nmap it with a specific function … \n## Gesture Description Introduction: \nThe init input contains content related to hand posed and hand \nmovements…\n## Context Management Agent Introduction: \nThe **Context Agent** manages all type of interaction context \ninformation…\n# Behavior Guidelines: \n## Requirements: \nThings need to do like 'asking for the function list' and \n'requesting one context at a time’…\n## Prohibitions: \nThings can not to do like ‘ask **Context Agent** to align the \ngesture to a function' …\n## Output block: \nThe output includes 'Thought', 'Gesture Guess', and 'Question for \nContext Agent' when the information is insufficient. \nIt contains 'Thought' and 'Results' once a decision has been made. \nAll outputs are requested in JSON format.\nGesture Inference Agent (Prompt Overview) Fig. 6. Prompt for Gesture Inference Agent.\n3.2 Gesture Inference Agent\nUpon receiving gesture descriptions from Gesture Description Agent, the Gesture Inference Agent\nengages in a dialogue with Context Management Agent. This conversational exchange is pivotal\nfor the Gesture Inference Agent to discern the user’s actual intent in the context, i.e., associating\nthe gesture with a target system function from a list of potential functions supplied by Context\nManagement Agent. This setup epitomizes a collaborative conversation aimed at overcoming the\nchallenges of gesture ambiguities under different interactive scenarios and context.\nWe outline the agent’s tasks and some behavioral guidelines to better demonstrate the semantic\nnature of the agent task revolves around thinking, summarizing, and inferring. The overall prompt\nis structured into two parts shown in Figure 6.\nThe introduction part includes a Task Introduction, a Gesture Description Introduction, and\na Context Management Agent Introduction (its conversational counterpart). A critical aspect to\nconvey in this part is that the initial gesture description may contain errors. Therefore, the Gesture\nInference Agent must utilize context information to help with the inference, correct potential\nmistakes, and make decisions. Behavioral guidelines for the agent consist of seven directives and\nfive prohibitions, acquired by iteratively updating the prompts during implementation. When new\nerror tendencies are observed, corrections are introduced into the prompt, akin to a teacher’s\nguidance. Then, we define the output format. We opt for JSON due to its ease of parsing, allowing\nfor efficient extraction of useful information for subsequent conversational rounds. Importantly, we\nrequire the agent to first articulate its thoughts before posing questions or drawing conclusions.\nThis approach has been proven to make the agent’s behavior more reasonable. Once the agent\ndeems that it is confident to decide or that no further context can be gleaned, it proceeds to make a\nfinal decision, listing the top-5 possible functions for the current interface, ranked from most to\nleast likely. The Gesture Inference Agent emphasizes the importance of precise communication,\ncontextual reasoning, and analytical capabilities in interpreting and responding to gesture-based\ninteractions.\n3.3 Context Management Agent\nThe Context Management Agent plays a crucial role in our system, offering intelligent management\nof various types of context information. Unlike static, rule-based context management system, this\nagent dynamically adapts to unfamiliar context thanks to LLM’s vast and intricate knowledge base,\nensuring a flexible and responsive interaction environment.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:11\nA dedicated Context Management Agent represents a significant step forward in the evolution\nof gesture-based interfaces. By segregating context handling from the inference processes, we\nstreamline system operations and enhance the overall system’s ability to effectively utilize diverse\ncontextual data. This separation allows for more sophisticated and nuanced interaction paradigms,\nwhere gestures can be interpreted in varying scenarios with greater precision. Moreover, this\narchitecture facilitates easier updates and scalability, as new contexts or rules can be integrated\nwithout disrupting the core inference mechanisms. Consequently, this approach not only addresses\nprevious limitations but also sets a new standard for the development of adaptive, efficient, and\nuser-centric gesture recognition systems. The overall prompt is structured into two parts shown in\nFigure 7.\n# Introduction：\n## Task Introduction: \nYour task is to manage and understand all interaction context \ninformation in the current interaction environment, and answer \n**Gesture Inference Agent**'s question… \n## Context Library Introduction: \nIncludes the currently accessible context introduction. …\n# Behavior Guidelines: \n## Requirements: \nThings need to do …\n## Prohibitions: \nThings can not to do …\n## Output block: \nThe output includes ‘Thought‘, ’Answer’\nContext Management Agent (Prompt Overview)\nFig. 7. Prompt for Context Management Agent.\nThe introduction includes a Task Introduction and a Context Library Introduction. The latter\nintroduces the currently accessible contexts and is designed to be adjustable. Similar to the Gesture\nInference Agent, iterative implementation guides the Context Management Agent’s requirements\nand prohibitions, with 4 requirements and 2 prohibitions. The output is structured in JSON format\nfor easy parsing, allowing efficient information extraction and facilitating subsequent conversation\nrounds.\n3.3.1 Context Library Operations The Context Management Agent supports three main operations\nwithin the context library: Adding new context types, retrieving context information based on\nqueries and calculating specific context values.\n•Adding Context Types: Context types and their values are organized using markdown for\ntext and JSON for structured data. New context types are introduced with natural language\ndescriptions, and associated values are formatted in JSON. This facilitates easy system\nexpansion and automatic incorporation of new contexts into the operational prompt.\n•Retrieving Context Information: The agent automates retrieval operations, streamlining\nthe process by organizing data in a standardized format ( e.g., JSON), which the LLM can\nautonomously interpret.\n•Calculating Context Value: The agent can calculate specific context values based on\npredefined methods in the context’s description. These methods can be implemented in\na Python script. Upon activation of a calculation operation, the agent generates a unique\nplaceholder. This placeholder triggers the system to execute the Python script, automating\nthe retrieval of context values and producing the desired output. The placeholder is then\nreplaced with this output in the final response.\nThe design of Context Management Agent offers significant benefits, such as the ease of adding\nnew context types using simple natural language descriptions and the automated retrieval and\nunderstanding of context information. This enhances the system’s versatility and usability. Our\ndesign also inform the design of context management components of future interactive systems.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:12 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\n4 Evaluation\nWe designed two experiments to assess GestureGPT’s adaptability and effectiveness under different\ninteraction scenarios with varying context environments and camera perspectives.\nIn the first interaction scenario, a user controls smart home appliances through an AR headset. In\nthis setup, gestures are captured from the user’s own viewpoint, offering a first-person perspective.\nIt is a key interaction scenario in the future when users interact with environment using head-\nmounted devices.\nOur second interaction scenario mimics the case when a user is watching online videos. The user\nwatches the video on a monitor with a camera capturing gestures from the third-person perspective.\nThis setup is prevalent in a variety of settings, including smart TVs, interactive public displays,\neducational environments, gaming, etc. The complexity increases in this scenario, as there are\nplenty of functions and interactive elements on a webpage.\nWe selected four contexts in our experiments to evaluate GestureGPT’s performance in different\ncontext settings:\n•Interface Function List: Crucial for mapping gestures to interface functions, this context\nincludes interface name and a list of functions with their names, locations, and unique IDs,\nkey for navigating the user’s interaction environment.\n•Gaze Information: Data on the user’s gaze, given in 3D (in home scenario) or 2D (in video\nscenario) coordinates.\n•Interaction History: A record of the user’s recent interactions.\n•External Context Information: Information from other devices or sensors. We introduce\nthis type of context to explore how other factors impact gesture understanding and whether\nthe agent can leverage this information.\nWe informed participants that both video and eye movement data would be collected during\nthe study, and we assured them of the confidentiality and safety of their data. The study is IRB\napproved by our local institution. All participants provided informed consent. In both experiments,\nwe asked participants to perform the gesture as they see fit to finish the task. Both static hand\nposes and dynamic gestures involving hand movements were permitted.\nWe employed the OpenAI GPT-4 model as the underlying architecture for our triple-agent system.\nSpecifically, we utilized the gpt-4-1106-preview version for our evaluations. We configured the\nrequest temperature to 0 so that the model’s output is as consistent as possible. Apart from this, we\nadhered to OpenAI’s default settings for other parameters. To mitigate the effects of randomness\nand enhance the reliability of our findings, we ran the experiment repeatedly for three times. The\naggregated results from these iterations were used to substantiate our conclusions.\n4.1 Experiment 1: Augmented Reality-Based Smart Home IoT Control\nThis study explores the interaction between users and smart home devices through augmented\nreality (AR), specifically using gesture controls in a simulated kitchen environment. Participants\nperform gestures as they see fit to control various IoT devices, triggering changes in device state\naccordingly.\n4.1.1 Experiment Setting and Procedure\n•Experimental Platform and Data Collection - The Microsoft HoloLens 2 served as the\nprimary experimental platform, offering APIs to capture user gaze and hand gesture data\naccurately. The experimental environment was developed using Unity (version 2020.3.24f1),\nthe Mixed Reality Toolkit (MRTK 2.8.0), and the OpenXR Plugin (1.7.0). The devices were\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:13\nrepresented by 3D models anchored in the space: a light, a smart cabinet, a smart screen, an\noven, and an air cleaner.\n•Context Library Setup - The experiment implemented context library as follows:\n– Interface Function List : Drawing from the XiaoMi SmartHome API1, five devices and their\ncorresponding functions were synthesized to form a function list. Each device has 3-5\nfunctions with a total of 18 functions in this scenario.\n– Gaze Information : User gaze data was captured using the HoloLens 2 Gaze API and saved\nas 3D spatial coordinates.\n– Interaction History : Interaction history was extracted from the task sequence.\n– External Context Information : There might be context information that is external to our\nsystem, which can significantly impact the grounding reasoning. We defined several ex-\nternal contexts corresponding to different tasks to understand if our system can correctly\nleverage those.\n•Task Descriptions - Eight tasks were designed to simulate smart device control (Table 6).\n•Participants - We recruited 16 participants from three local schools, compensating them at\na rate of $12 per hour. Their ages ranged from 15 to 35 years (MEAN = 26.625,SD = 5.325),\ncomprising 13 males and 3 females. Participants included 1 high school student, 4 undergrad-\nuate students, 9 graduate students, and 2 research engineers. None of the participants had\nprior experience with AR/VR devices, which helped minimize bias due to varying familiarity\nwith such technology.\n•Task Procedure - Upon their arrival, participants were briefed about the scenario and the\ndevices involved. They are asked to make any gesture deemed most intuitive using the right\nhand after raising their hand above their chest to initiate the trigger. A preliminary warm-up\nsession was conducted to familiarize the participants with the AR devices and gesture control\noperations. Following this, they were instructed to complete the eight tasks. Feedback from\nthe devices was simulated to enhance the interaction experience and realism of the study.\nDetailed information on the experiment simulation interface, the list of devices and their functions,\nand the task list are provided in Appendix B.1. A total of 16 participants ×8 tasks = 128 gestures\nwere collected.\nTable 2. Main Results of GestureGPT in the Two Experiments\nSmart HomeScenario Video StreamingScenario\nTop 1 (↑) Top 3 (↑) Top 5 (↑) Negative (↓) Top 1 (↑) Top 3 (↑) Top 5 (↑) Negative (↓)\nRandom Guess 5.56% 16.67% 27.78% 72.22% 3.15% 9.46% 15.76% 84.24%\nBaseline 10.94% ±3.38 24.48%±3.51 35.16%±2.92 64.84%±2.92 19.53%±3.55 38.28%±1.10 54.43%±1.84 45.57%±1.84\nOnly Gaze 35.16% ±1.28 70.05%±1.33 83.59%±1.10 16.41%±1.10 25.78%±0.64 47.66%±3.19 60.42%±2.88 39.58%±2.88\nOnly History and External 23.18%±4.25 37.50%±4.47 49.48%±4.34 50.52%±4.34 26.30%±3.01 47.14%±5.12 63.28%±3.38 36.72%±3.38\nAll 44.79% ±3.21 67.45%±4.10 79.69%±1.69 20.31%±1.69 37.50%±4.18 59.90%±4.83 73.44%±1.91 26.56%±1.91\n4.1.2 Results Analysis We evaluate GestureGPT’s performance by running the collected data\nthrough our system under four different context settings: 1) the baseline with only the function list\n(Baseline), 2) with gaze information (Only Gaze), 3) with interaction history and external information\n(Only History and External ), and 4) all contexts are available (All). For comparison, we also provide\nresults from Random Guess , which randomly selects one from all functions. We repeat the tests\nthree times under each setting to ensure robust conclusions. The main results are presented in the\nleft part of Table 2, and a corresponding illustration is provided in Figure 8.\n1https://iot.mi.com/new/doc/design/spec/xiaoai\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:14 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\nOur first observation is that GestureGPT can effectively utilize the context information to\ndetermine the exact intention of the users. The accuracy is effectively improved to an impressive\nextent when either gaze or history and external is incorporated. When all contexts are combined\ntogether, the overall framework achieves the best Top-1 performance at 44.79%.\nWhen comparing the benefits of gaze versus history and external information, we empirically find\nthat gaze significantly outperforms history and external information. As shown in Table 2, theOnly\nGaze setting achieves 35.16%/83.59% at Top-1/Top-5 accuracy, outperforming the 23.18%/49.48%\nachieved by the Only History and External setting by a considerable margin. We attribute this to\nthe semantic similarity between candidate functions shared by different home devices and their\nbroad spatial distribution across the entire room space. These two factors significantly increase the\nreasoning difficulty under the Smart Home scenario. Facing such challenging situations, GestureGPT\ncan fully exploit the gaze information as well as its own commonsense knowledge to finally\nlocate and zoom in on possible candidates through multi-step, collaborative spatial reasoning.\nAs explained in Section 3.3.1, when the Gesture Inference Agent requests gaze information, the\nContext Management Agent not only outputs the gaze coordinates but also identifies the relevant\ndevice within the gaze path using external tool-augmented Python scripts.\nTask Index:12345678 12345678 12345678 12345678\nHomeScenarioTaskResultsInFourContextSetting\nFig. 8. Illustration of results from Experiment 1: Home Scenario, encompassing all 4 settings (across the x-axis)\nand 8 tasks (indicated by hue). For each setting, the 8 tasks are arranged from left to right as follows: task 1\nUnlock Carbinet, task 2 Increase Light Brightness, task 3 Next Recipes, task 4 Open Oven, task 5 Open Air cleaner,\ntask 6 Set Timer, task 7 Switch Input, and task 8 Phone Call. Each specific bar is further divided into Top 1 / Top\n3 / Top 5 accuracy (Top 1, Top 3, Top 5 in gradient brown) and Negative outcomes (in blue), totaling 48 data\npoints.\nMoreover, while integrating all context achieves the highest Top-1 performance, it unexpect-\nedly underperforms in the Top-5 metric compared to the Gaze Only setting (79.69% vs 83.59%).\nThrough further case analysis, we hypothesize that an excess of context can sometimes disrupt the\nagent’s analysis, leading to the following behaviors: 1) Irrelevant context information can at times\nmislead the agent. For example, the mention of “fingerprint unlocking” (in task 1) within external\ninformation might lead the agent to infer fingerprint recognition as the unlocking method, rather\nthan a gesture. 2) An abundance of context causing the agent to overlook certain information. For\nexample, in the “Open the air cleaner” task (in task 5) where the agent wrongly assumes the device\nis on, based on external information “The air purifier’s sensor detected heavy cooking fumes, ” while\nignoring its actual OFF status. These insights will be further discussed in Section 5.4 regarding\noptimal context selection.\nFinally, we observe that Tasks 2, 3, and 7 exhibit relatively high baseline performance, all involving\noperations related to “switching”. Finally, we observe that Tasks 2, 3, and 7 exhibit relatively high\nbaseline performance, all involving operations related to “switching”. Similarly, for Tasks 3 and\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:15\n7, GestureGPT employed a comparable analytical approach to discern the correct answer. This\ndemonstrates GestureGPT’s strength in analyzing gestures alongside current device states, thereby\naccurately interpreting user intentions even in the baseline context.\nIn conclusion, our investigation into GestureGPT’s performance across different contexts under-\nscores the significant enhancements brought about by gaze data and the contributions of history\nand external information. Further refinement in how external information is presented and queried\nwill unlock greater performance of our system.\n4.2 Experiment 2: Online Video Streaming on PC\nThis scenario is set when a user is watching online video on a PC monitor. Grounding gestures to\nthe correct function is much more challenging in this scenario compared to the previous one. The\nvideo streaming interface contains a considerably broader range of functions, with numbers up to\n66 in some tasks from the previous 18 functions. Moreover, many functions have similar semantic\nmeanings, such as the “vlog channel” button and the “anime channel” button, making them difficult\nto distinguish solely through gestures. Furthermore, the interactive buttons and elements on the\nscreen are much smaller than the smart appliances in the previous experiment, which reduces the\nperformance of gaze-based function differentiation. The size of function elements ranges from\n0.27 to 20.71 cm2 (MEAN = 2.73, SD = 3.67). By navigating through an interface rich in functions\nand semantic complexities, we intend to explore the boundary of our system’s performance and\nunderstand whether it can differentiate user intentions with only minor differences, which is\nessential for real-world applications.\n4.2.1 Experiment Setting and Procedure\n•Experimental Platform and Gesture Data Collection - Our experimental framework\nutilizes Python and Selenium 2 to interact with a video streaming platform, specifically\ntargeting the website “[China] From the Spring and Autumn Period to the Prosperous Tang\nDynasty (Season 1, 12 Episodes)” on Bilibili3. The platform automates video control operations\nvia Selenium. User gestures are captured using a 1080P resolution webcam.\n•Context Library Configuration - The study incorporates a comprehensive context library\ncomprising four distinct aspects same as in previous scenario:\n– Interface Function List : The function list is automatically extracted from the webpage and\norganized. Functions on the website included their position and raw HTML code are\nidentified via JavaScript. The code for each function is then extracted and fed into GPT-4\nto generate function names, aiding in the compilation of the interface function list. For\ntasks 4, 5, and 6, the function count is 17; for all other tasks, it is 66.\n– Gaze Information : The Tobii Eye Tracker 5 is used to collect gaze data from participants.\n– Interaction History and External Context Information : Interaction history was extracted from\nthe task sequence, while the external contexts were predefined.\n•Task Descriptions - Eight tasks were designed to simulate common operations performed\nwhile watching videos (Table 8).\n•Participants - We recruited 16 participants from four local schools, compensating them at a\nrate of $12 per hour. Their ages ranged from 18 to 35 years (MEAN = 26.875,SD = 4.689),\ncomprising 10 males and6 females. Participants included 5 undergraduate students, 8 graduate\nstudents, and 3 research engineers. None of the participants had prior experience with AR/VR\ndevices, which helped minimize bias due to varying familiarity with such technology.\n2https://www.selenium.dev/\n3https://www.bilibili.com/video/BV1sh411j7A4/\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:16 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\n•Task Procedure - Participants were briefed on the experiment’s aims and the devices utilized\nupon their arrival. They were also asked to make any gesture deemed most intuitive with their\nright hand, similarly triggered by raising their hand above the chest. There was a warm-up\nphase for the participant to familiarize with gesture controls. Following this, the participant\nsequentially completes the eight tasks. Specific gestures triggered predefined responses on\nthe website, simulating real-time interaction for a more realistic experience.\nAs a result, a total of 16 participants ×8 tasks = 128 data points were collected. Detailed infor-\nmation about the experiment simulation interface, the list of devices and their functions, and the\ntask list is provided in Appendix B.2.\nTask Index:12345678 12345678 12345678 12345678\nVideoScenarioTaskResultsInFourContextSetting\nFig. 9. Illustration of results from Experiment 2: Video Scenario, encompassing all 4 settings (across the x-axis)\nand 8 tasks (indicated by hue). For each setting, the 8 tasks are arranged from left to right as follows: task 1\nAdjust Volumn, task 2 Video Progress Control, task 3 Enter Fullscreen, task 4 Pause Video, task 5 Play Video, task\n6 Exit Fullscreen, task 7 Like the Video, and task 8 Next Video. Each specific bar is further divided into Top 1 /\nTop 3 / Top 5 accuracy (Top 1, Top 3, Top 5 in gradient brown) and Negative outcomes (in blue), totaling 48\ndata points.\n4.2.2 Results Analysis In this scenario, we evaluate GestureGPT under the same four context\nsettings. The results are presented in the right part of Table 2 as well as Figure 9.\nThe main conclusion that GestureGPT can effectively incorporate various context information to\nreason and predict user intentions remains consistent. Specifically, Top-1 accuracy improved from\n19.53% to 37.50% when all types of context are available, significantly outperforming each single\ncontext setting: +11.20% compared to only the history and external setting, and +11.72% compared\nto the only gaze setting. Both contexts contribute to the final performance, and it greatly drops if\neither is excluded. Compared with Experiment 1, where gaze information brings relatively more\nutility, these results further demonstrate that it requires joint reasoning over all available contexts\nto achieve the best performance under such a more complicated scenario.\nAn intriguing observation from our results was that the baseline performance was superior to\nthat observed in the home scenario, thereby necessitating task-specific exploration. Tasks such as\npausing, playing videos, and selecting the next video (Tasks 4, 5 and 8), which require less context,\nexhibited strong performance across all tested context settings. This phenomenon suggests that the\nagent’s inherent knowledge of video streaming interfaces makes it likely to infer actions commonly\nseen in such a video streaming interface. To some extent, the agent’s success in identifying these\noperations can be attributed more to an educated guess rather than a calculated match of gesture\nand context, indicating a form of intuition derived from the model’s extensive training data. This\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:17\nintuition helps our system’s performance in certain tasks, but also acts as bias in other tasks. This\nphenomenon is discussed further in Section 5.5.\nAnother special case is Task 7, which is semantic specific task, highlights the essential role of\naccurate gesture description in achieving precise gesture grounding. Out of 16 participants, 12\nperformed the “thumbs-up” gesture for the task. In such cases, if the agent accurately recognizes\nthe gesture, the outcome is correct, notably reflecting in high Top-1 accuracy results. The failures\npredominantly occurred due to incorrect gesture segmentation. For example, because users formed\na fist after lifting their hand, leading the agent to mistakenly focus on the action of making a fist,\nwhich leads to map the “Full Screen” or “volume” function. We conducted a pilot validation exercise\non the gesture segmentation rule used in our system with a human labeler splitting gesture frames\nfrom the whole video on data from eight participants as the ground truth in gesture segmentation.\nOur results, compared to the ground truth, revealed a high recall rate of 95.28%, yet a precision of\nonly 43.91%, reflecting the inclusion of non-gesture-related frames. Future enhancements could\ninvolve advanced object detection techniques , differentiating interactive vs non-interactive gestures,\nor a specially designed beginning gesture to improve segmentation precision.\nIn cases where gesture descriptions were poorly articulated, incorporating additional context\nproved beneficial. For example, considering interaction history context, such as users exiting full-\nscreen mode, allowed the agent to infer that the user might want to perform operations unrelated\nto video control, like “liking” a video. This various context adaptation significantly improved this\ntask performance in Top-5 accuracy metrics in this task from 35.42% to 68.75%.\nDespite the challenges we designed in the video streaming scenario, GestureGPT still demon-\nstrates commendable performance, largely attributed to the LLM’s robust common sense and\ncontextual interpretation capabilities.\n4.3 Result Analysis: Gesture Description Quality Assessment\nThe quality of the Gesture Description Agent was evaluated through an expert questionnaire.\nFor this evaluation, we randomly sampled descriptions generated from three repetitions for 256\ngestures across two scenarios, resulting in a compilation of 256 gestures and their descriptions. The\nsame questionnaire was then distributed to two gesture experts, who rated each gesture description\non a 5-point Likert scale (1 being “Almost no description is correct or relevant to the gesture” and\n5 being “Almost all of the description is correct or relevant to the gesture”). Both expert have\npublished gesture-related research articles on premier conferences. One expert scored 3.28 (std =\n1.41), and the other scored 3.74 (std = 0.73). Their Kendall’s W score was 0.853, indicating a high\nlevel of agreement between the two experts. The positive ratings show that our description agent\ncan capture key information of the gesture.\n4.4 Pilot Study: Human Performance in Gesture Understanding\nTo gauge human performance on tasks similar to those managed by GestureGPT, we conducted a\npilot study with four participants who were unfamiliar with the system and its experimental setups.\nWe randomly selected data from four users involved in Smart Home and Video Streaming scenarios.\nDuring the pilot, participants were provided with video recordings that included the user’s gesture\nposes and movements, along with a list of interface functions available at that time. They were\ntasked with mapping each gesture to its corresponding function. The pilot study interface as shown\nin 10.\nInitial attempts by participants were guided by gaze context , as demonstrated through gaze tra-\njectory recordings, and historical context. Subsequent attempts were further supported by providing\nexternal context, the pilot study interface shown in 10. Subsequent attempts were further supported\nby providing external context . Results revealed significant challenges in gesture understanding\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:18 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\nGaze Trace RecordingInterface Function List\nGaze Trace \nGesture Recording\nFig. 10. Pilot study interface showcasing. On the left, a video stream with a highlighted gaze trace indicating\nuser focus during the task. On the right, a gesture recording from a video streaming scenario alongside\na detailed interface function list. This setup was used to assess participant ability to map gestures to\ncorresponding interface functions, facilitated by context such as gaze trajectories and historical information.\neven for humans: initial Top-1 and Top-5 accuracies were 43.8% and 81.3% for the smart home, and\n18.8% and 50% for video streaming, respectively. With added external context, accuracies increased\nnotably to 58.3% (Top-1) and 91.7% (Top-5) in the smart home, and to 83.3% (Top-1) and 100% (Top-5)\nin video streaming. This preliminary result underscores the inherent complexity of interpreting\ngestures, even for humans, in contextually rich settings. So compared with gesture recognition,\ngesture understanding is much more difficult and challenging.\n5 Discussion\n5.1 Integration and Utilization of Top-N Predictions\nDue to the inherent complexity of gesture understanding—far surpassing that of mere gesture\nrecognition—we approach this challenge as a recommendation task, which is why we show the\nTop-3 and Top-5 accuracies. GestureGPT currently outputs a Top-5 candidate function list, which\ncan be readily augmented with list selection interaction optimizations, as studied in many existing\nworks. Specifically, Isomoto et al. [25] developed a dwell selection system that employs machine\nlearning to predict user intent solely based on eye movement data. Furthermore, statistical design\nof dynamic menus could also expedite user selection particularly in AR/VR contexts. G-Menu [63]\nutilizes gestures and keywords to design a dynamic menu, aiding in swift function selection.\nThese techniques and designs can be readily integrated into GestureGPT to achieve efficient user\nselection of the target function. The rapid advancement of LLMs and their increasing capability as\ncontextualized agents, LLM-driven frameworks like GestureGPT will provide improved performance\nso that its Top-1 accuracy reaches a directly applicable level, ultimately ease the burden of secondary\nselection of users.\n5.2 Language Input (LLM) vs. Visual Input (LMM) for Agents\nGestureGPT is currently built with a Large Language Model for its superior understanding and\nreasoning capability. For the Gesture Description Agent, an alternative implementation could\nemploy a Large Multimodal Model (LMM), which seems more intuitive. So we tested replacing the\nrule-based Gesture Description Agent with GPT-4o’s vision in a video streaming scenario. With\nAll Context considered, the Top-1 and Top-5 accuracy results were 41% and 72%. These results\ndemonstrate performance comparable to our rule-based module which is 37.5% and 73%, thereby\nvalidating the effectiveness of the rule-based approach.\nOn the other hand, LMMs require users to upload gesture recordings, which raises further privacy\nconcerns. By contrast, our solution can process the gesture input with end-affordable devices and\nonly send anonymized skeleton signals to the data center, where LLMs can perform dense computing.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:19\nNevertheless, it is anticipated that with further development of LMMs, GestureGPT could integrate\nboth the visual recognition capabilities of LMMs and the prominent commonsense understanding,\ncontextual reasoning capabilities of LLMs, to ultimately better serve its purpose.\n5.3 Single Agent vs. Multi Agent\nAssuming privacy concerns regarding LMMs are not considered, employing gesture visuals with\ncontextual information in a LMM-based single-agent architecture could significantly reduce the\nsystem delay. Unlike multi-agent systems, which require multiple reasoning cycles to interpret\ngestures, a single-agent approach can deliver immediate results within one iteration. We conducted\na preliminary test with data from one user in the smart home scenario, and the LMM-based single-\nagent approach achieved a Top-1 accuracy of 25.0% and a Top-5 accuracy of 75%. Due to privacy\nconcerns, comprehensive testing was limited because other users did not authorize the publication\nof data.\nAlthough the end-to-end LMM approach exhibits shorter delays, it presents several disadvantages:\nit is cumbersome to adjust and train, lacks task transferability, and falls short in terms of scalability\nand generality. Conversely, the multi-agent architecture offers significant advantages, such as\nenabling component reuse, and facilitating modular training and optimization. Each architecture\nhas its own applicable scenarios, and together, they can complement each other to achieve superior\nresults.\nHowever, the multi-agent architecture of GestureGPT inherently offers greater flexibility and is\ndesigned for broad applications. For example, instead of cameras, the description agent can work\nwith sensors like IMUs, electromyography, and LiDAR, provided they can reconstruct the hand\npose to some extent. The framework handles complex interaction contexts to mimic human gesture\nunderstanding, such as interpreting gestures linked to ongoing events. Integrating context into\nthe inference agent would greatly increase the workload and limit flexibility. The independently\nimplemented modules allow for easier optimization. Smaller models within the architecture are\nplug-and-play, thereby avoiding constant end-to-end training.\nFurthermore, we contend that the multi-agent architecture of GestureGPT is ideally suited for\ncomplex interaction scenarios where single-step reasoning is insufficient. For example, an agent\nmight need to interact with users or external databases before making decisions. Our architecture\nsupports distributed processing, enabling different agents to handle specific tasks asynchronously,\nthereby having the potential to further enhance efficiency in managing complex interactions. For a\ndetailed discussion on how this architecture addresses system delays, see Section 6.2.\n5.4 Context Selection in Complex Systems\nOur experiments revealed that although adding more contexts is supposed to be informative and\nuseful, for certain circumstances, it might also bring overwhelming irrelevant information and\naccordingly distract the reasoning process of LMM agents (See Top-5 accuracy of All setting vs\nOnly Gaze setting in Smart Home scenario). In Smart Home scenario, most informative contexts\nare concentrated on gesture itself and gaze, while history and external provide relatively less\ncontribution to the final performance.\nOn one hand, the inclusion of more heterogeneous contexts increases the complexity and difficulty\nof context organization and management, which then requires more competent agents to process\nand reason over them. On the other hand, contexts with less information entropy inevitably brings\nirrelevant implications, and may result in misleading of agents for specific cases. This underscores\nthe importance of making informed trade-offs between context incorporation and agent capability\nto optimize the system performance.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:20 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\n5.5 LLM Common Sense Bias\nThis phenomenon pertains to the cognitive biases of LLM, which has been recognized as a common\nissue that influences LLM outputs [53].\nSuch a bias was also observed in our system, notably within the video streaming scenario. When\nthe agent learned that this is a video streaming scenario, there is a bias towards predicting video\ncontrol functions that are commonly used on such an interface, such as play, pause, and volume\ncontrol. On one hand, even in the absence of context, the agent can make accurate guesses if the\nintended function is one of such functions. On the other hand, it leads to the consistent inclusion\nof these functions within the candidate options, detrimentally impacting the top-1 accuracy.\nOne way to address this bias could involve employing Modular Debiasing Networks [ 13] to\nmitigate bias or utilizing sophisticated prompt engineering to diminish its effects, which we plan\nto investigate in the future.\n5.6 System Scalability\nBy substituting the Gesture Description Agent with specially designed counterparts, our system\ncan adapt to more input modalities and more generalized form of gestures.\nIn our implementation, gesture feature extraction is solely based on a RGB camera and MediaPipe.\nYet this approach is susceptible to lighting conditions and finger occlusion issues. Wearable devices\nprovide an alternative robust solution for hand reconstruction [ 22, 73, 78], which even works\nwith subtle gestures like thumb-tip gestures [ 19] and wrist gestures [ 17, 18]. Hand landmarks\nreconstructed from wearable sensors can then be used to generate gesture description.\nOur system can also be extended to general gestures, beyond the scope of merely hand gestures.\nFor example, touch-screen gestures can be extracted as traces and pressure intensity, which differ\nsignificantly from the form of hand gestures. But a specially designed Gesture Description Agent\ncan extract the features of such gestures (either using rule-based methods or leveraging large\nlanguage or vision models) and describe it to Gesture Inference Agent, thus easily integrated into\nour framework.\n6 Limitation and Future Work\n6.1 Handling Left-Handed Gestures, Multiple Hands, and Non-Interactive Gestures in\nFrame\nGestureGPT currently identifies meaningful gestures when a right hand is raised above chest level.\nHowever, this implementation assumes that only one hand, typically the right hand, is visible.\nThis assumption is often unrealistic, as interactions in real environments may involve left-handed\nusers or require two-handed gestures, thereby limiting the system’s inclusivity. Furthermore, this\nsimplification can lead to false activations, as unintended gestures or non-interactive movements\nmay inadvertently trigger the system. This issue is exacerbated in scenarios where various non-\ninteractive hand movements and multiple hands are present within the camera frame, potentially\nconfusing the model and degrading system performance. Evaluating the system’s robustness when\nboth hands are visible or when left-handed gestures are involved is essential for fully understanding\nits practical capabilities. Future iterations of our system will focus on distinguishing between\nintentional gestures and extraneous hand movements, even in the presence of multiple hands, and\non improving left-hand gesture recognition to better accommodate left-handed users.\nSeveral existing studies have addressed these challenges. Research [29] has explored techniques\nfor distinguishing between the static and dynamic aspects of hand movements, along with the chal-\nlenges of recognizing one- and two-handed gestures. Similarly, vision-based interaction approaches\nfor augmented reality [60] emphasize the importance of simplifying hand models and applying user\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:21\nintention recognition methods to achieve robust performance in complex scenarios. Additionally,\nframeworks that incorporate body pose, gaze direction, and multimodal feedback mechanisms have\nshown promise in predicting interaction intent [12, 54], while other studies have integrated EEG\nand gaze patterns to differentiate between meaningful and irrelevant gestures during tasks [16, 33].\nThe architecture of GestureGPT is designed to be adaptable and scalable, enabling the integration\nof these advancements to further enhance its capabilities. Incorporating a left-hand recognition\nmodule within the Gesture Description Agent would allow the system to support both left-hand\nand two-handed gestures. Additionally, expanding the contextual input processed by the Context\nManagement Agent could improve the system’s ability to distinguish non-interactive gestures,\nthus reducing the likelihood of false activations. In the future, we plan to explore these challenges\nfurther, including validating the accuracy of left-hand gesture recognition, providing comprehensive\nsupport for two-handed gestures, and refining the system’s activation mechanisms to enhance its\npracticality and robustness.\n6.2 System Delay and Applicability\nThe overall time cost of the framework is primarily driven by LLM-related processes, accounting\nfor up to 90% of the total time expenditure. These costs include LLM API request costs. For each\ngesture understanding task, GestureGPT typically requires 1 request for the gesture description and\n6 rounds of conversation between the Gesture Inference Agent and Context Management Agent,\nhighlighting the significant impact of response delay on the real-time performance of our system.\nIn its current conceptual form, without a focus on practicality, our framework averages 227 seconds\nand 38,785 tokens per task.\nAlthough the conceptual framework of GestureGPT demonstrates promising potential for HCI\napplications, the current implementation faces significant challenges related to system delays,\nwhich affect its real-world applicability. Addressing these delays is essential for transitioning\nGestureGPT from a theoretical model to a practical tool capable of functioning in dynamic, real-\ntime environments. Possible directions for improvements include:\n•KV-cache: API calls require concatenating long system prompts in each round, exponentially\nincreasing time costs. Our current implementation appends the entire context library (11,000\ntokens) to each query. Implementing a KV-cache would reduce the need to reprocess identical\ncontexts, potentially decreasing interaction time to approximately 20 seconds per task. This\nenhancement is expected to streamline operations by preserving computed attention values\nfor recurring queries.\n•High-Performance Hardware Utilization : Integrating advanced computational resources,\nsuch as Groq’s hardware, could further reduce processing times. Groq’s fast model inference\nservices offer an output speed of over 700 tokens per second.\nBy combining Groq and KV-cache, the first context round would take 2-3 seconds, with subsequent\nrounds taking a total of about 1-2 seconds (averaging 6 dialogue rounds, each with 200 tokens\ninput and 100 tokens output). This would reduce the theoretical optimal latency to under 5 seconds.\nAlthough we do not currently have access to KV-cache-enabled high-performance LLMs or the\ncomputing acceleration hardware provided by Groq, we anticipate that GestureGPT will attract\ninterest from other research domains, fostering collaborative efforts towards developing a practical\nand accessible free-form gesture interface.\nOn the other hand, since each agent of our system has specialized and clear goals, we believe a\nproperly fine-tuned LLM could mirror GPT-4’s effectiveness with fewer resources as demonstrated\nin NLP tasks [84]. Besides, advances in model distillation and acceleration hardware suggest that\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:22 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\nlightweight LLMs could be deployed directly on devices with fast inference [38, 82, 82] for real-time\nperformance in the future. Under which circumstances, the API request cost can also be removed.\nWhile the current system delays may limit immediate real-world applications, the outlined\nadvancements and potential collaborations offer a promising path toward minimizing these delays.\nFuture iterations of our system, supported by ongoing research and technological improvements,\ncould effectively reduce latency to levels suitable for dynamic, real-time user interactions. Such\ndevelopments would not only address the practical limitations discussed but also pave the way for\nbroader adoption and impact across various domains, ultimately fulfilling the potential of real-time,\nintuitive gesture-based interfaces.\n7 Conclusion\nIn this work, we introduce GestureGPT, a human-mimicking framework for understanding free-form\nhand gestures, which capitalizes on the capabilities of LLMs. Unlike traditional gesture recognition\nsystems that require predefined gestures, GestureGPT automatically maps spontaneously performed,\nfree-form gestures to their targeted interface functions without any user training. This is achieved\nthrough a collaborative framework involving three specialized agents: a Gesture Description Agent,\na Context Management Agent, and a Gesture Inference Agent. These agents work together to\ninterpret gestures, manage context, and apply common sense reasoning to accurately discern user\nintents.\nOur offline evaluations in two realistic scenarios demonstrated promising results, achieving\nthe highest zero-shot Top-5 gesture grounding accuracy of 83.59% for smart home control and\n73.44% for video streaming. These outcomes highlight the significant potential of GestureGPT as a\nconceptual model for future gesture-based interaction systems. They underscore the viability of our\napproach in reducing the cognitive load on users by eliminating the necessity to learn, memorize,\nor manually link gestures to functions.\nAs a conceptual framework, GestureGPT sets the groundwork for future advancements in natural\ninteraction interfaces. While not yet a practical system ready for everyday use, our framework\nserves as a foundational step towards realizing more adaptive and intuitive ways to interact with\ntechnology. Future research could focus on refining this paradigm, enhancing its responsiveness,\nand expanding its applicability to include a broader array of natural user interfaces supporting\ndifferent modalities like facial expressions, body movements, and postures, unleashing the full\npotential of the framework proposed by GestureGPT.\nAcknowledgments\nThe authors would like to thank Benfeng Xu, whose insightful comments and suggestions were\ninvaluable in writing this paper. This work was supported by the Hunan Provincial Natural Science\nFoundation of China (No. 2023JJ70009), the HNXJ Philanthropy Foundation (KY24017), the Science\nand Technology Innovation Program of Hunan Province (No. 2022RC4006), and the Young Scientists\nFund of the National Natural Science Foundation of China under Grant No. 62102401.\nA Gesture Description Rules\nA.1 Rules Calculation Method\nFlexion of a finger is calculated as the total bending angle of each joint. For thumb it is the bending\nangle of the ip joint, and for other fingers, it is the bending angle of the pip and dip joint. Then, two\nparameters 𝑠𝑡𝑟𝑎𝑖𝑔ℎ𝑡_𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 and 𝑏𝑒𝑛𝑡_𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 are set to determine if the finger is straight,\nbent, or “unsure” if the result falls between them. Since thumb has a different joint structure\ncompared with other fingers, a new pair of thresholds are specially set for thumb.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:23\nProximity of two fingers is calculated as the average minimal distance from each finger’s joint\nto the other finger. Two thresholds i.e., 𝑡𝑜𝑔𝑒𝑡ℎ𝑒𝑟_𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 and 𝑠𝑒𝑝𝑎𝑟𝑎𝑡𝑒𝑑 _𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 are set to\ndetermine if the two fingers are pressed together, separated, or “unsure” if the result falls between\nthem.\nContact of thumb and another finger is computed as the distance between their fingertips. Then,\ntwo thresholds, i.e., 𝑐𝑜𝑛𝑡𝑎𝑐𝑡_𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 and 𝑛𝑜𝑡_𝑐𝑜𝑛𝑡𝑎𝑐𝑡_𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 are set to determine if the two\nfingers’ fingertips are in contact or not, or “unsure” if the result falls between them.\nPoint direction of thumb is computed as the direction from thumb’s mcp joint to tip joint. Then,\nit is compared with two reference vectors representing upward and downward. If a reference vector\nhas the minimal angle with the palm orientation vector and the angle is below 𝑎𝑛𝑔𝑙𝑒_𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑,\nthe reference vector would be the thumb’s pointing direction. Note that it is only applicable when\nthumb is straight. If thumb is bent, the result is set to “unsure”. This is especially useful when\ndiscriminating between gestures like “thumb up” and “thumb down”.\nPalm orientation is computed as the direction to which the palm is facing. It is computed\nby the cross product of two vectors within the plane of the palm (Figure 4). Then, the direction\nis compared with six reference vectors representing upward, downward, left, right, inward and\noutward. If a reference vector has the minimal angle with the palm orientation vector and the\nangle is below 𝑎𝑛𝑔𝑙𝑒_𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑, the reference vector would be the palm orientation. Otherwise the\norientation of palm is set as “unsure”.\nHand position is computed as the geometrical center of a hand by taking average of all 21\nlandmarks’ coordinates. No parameter or threshold is applied here.\nA.2 Training for Threshold and Evaluation for Gesture Description Rules\nWe use HaGRID [30] dataset to tune and evaluate our rules. HaGRID contains images of 18 kinds\nof gestures (call, dislike, fist, four, like, mute, ok, one, palm, peace, peace inv., rock, stop, stop inv.,\nthree, three 2, two up, two up inv.).\nA.2.1 Training Process A subsample split of HaGRID has 100 images per gesture, and it is used to\ntune the rule parameters.\nTo tune the rule parameters, we first manually annotate the ground truth label for each rule\non each gesture class. For each gesture, we assume that most people perform it in the same way,\nand thus the ground truth label is obtained by common knowledge. (For example, for “thumb up”\ngesture, we label thumb as straight, index to pinky fingers as bent, index and middle finger are\npressed together, etc.)\nHowever, there exist ambiguous cases, in which more than one labels are acceptable. (For example,\nin “peace” gesture, it is reasonable for thumb to be either straight or bent.) In this case, the label is\nmore than one, and consequently cases like this are not used for parameter tuning.\nMost rules are tuned using the whole subsample split. One exception isFinger Flexion on thumb,\nbecause in most gesture classes, the ground truth of thumb is either [“straight”] or [“straight”,\n“bent”] (thus not used for training), and only one class is [“bent”]. To address the issue of severe\nimbalance for thumb flexion, we specially select the training set, which is composed of gesture\n“like” (thumb is straight) and gesture “ok” (thumb is bent), and each image in this set is manually\nchecked to ensure the quality of the training data.\nAs is shown above, for those cases used to tune the rules, the ground truth label is one of the two\npossible states (e.g., “straight” or “bent”). But during prediction, we use Three-Way Decision Making\nmethod [76]. When the rule cannot clearly determine the state, it will predict it as “unsure”, to\navoid wrong conclusions that may easily mislead LLMs. Consequently, different from the classical\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:24 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\nbinary classification, some modifications have been made to the correctness assessment and tuning\ncriteria to accommodate our specific setting.\nCorrectness Assessment . For each (prediction, label) pair, there are three possible circum-\nstances:\n•Unsure: Prediction = “unsure”.\n•Error: Prediction ≠ “unsure”, and prediction ≠ label.\n•Correct: Prediction ≠ “unsure”, and prediction = label.\nTuning criteria. To find the optimal parameters, we define the loss in different cases as follows:\n•Unsure: Loss = 0.2\n•Error: Loss = 1\n•Correct: Loss = 0\nIn “unsure” cases, the loss is between 0 and 1. This is because if we set the loss to be 0 (same as\n“correct” case), then the rules tend to predict every case to “unsure’; if we set the loss to be 1 (same\nas “error” case), then the rules tend to not predict any “unsure”, which may lead to misleading\npredictions.\nA grid search is conducted to find out the optimal parameter with minimal average loss. The\noptimal parameters for each rule is shown in Table 3.\nA.2.2 Test Performance We test the generalization ability of our rules on two dataset: HaGRID\ntest set (third-person view) and EgoGesture dataset (first-person view) [80].\nThe performance on HaGRID test set (38576 images) is shown in Table 3. For most rules, the\naverage error rate among all gestures is below 4.2% and the overall accuracy is above 88.2%. One\nexception is flexion rule for thumb, in which the output “unsure” takes about 45% of all cases. This\nmay be attributed to the unique shape of thumb: the topmost segment of the thumb is curved by\nnature, so when the thumb is extended, the landmark seems to be slightly bent, which may affect\nthe train and test process. But by a three-way decision, we use “unsure” to avoid the misleading\ninformation that may potentially confuse the Gesture Description Agent.\nWhen checking the algorithm’s performance for each gesture, it is shown that the error rates\nfor most gestures are below 5% and none of them are above 16%. The error could be attributed to\nlandmark mistakes made by MediaPipe, the shortcomings of our algorithm, and a small number of\npeople just perform the gesture differently from most people (i.e., different from the ground truth\nwe established before). The results show good generalizability since the parameters are only tuned\non a small number of samples.\nTable 3. Rule Parameters, and Their Performance on HaGRID Test Set\nRule Parameters Performance on HaGRID Test Set\nerror unsure correct\nFlexion - thumb (16, 38) 0.036 0.457 0.507\nFlexion - other fingers (57, 74) 0.019 0.049 0.932\nProximity (0.024, 0.029) 0.031 0.067 0.902\nContact (0.046, 0.055) 0.020 0.024 0.956\nPointing Direction - thumb 40 0.047 0.239 0.714\nPalm Orientation 41 0.042 0.075 0.882\nOverall - 0.023 ±0.018 0.062 ±0.039 0.916 ±0.053\nTo evaluate if the parameters trained on third-person view dataset can adapt to first-person view\nimages, we tested them on a first-person view gesture dataset EgoGesture. We choose 20 gesture\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:25\nclasses (fist, measure, zero, one, two, three, four, five, six, seven, eight, nine, ok, three2, C, thumb\ndown, thumb right, thumb left, thumb backward, thumb forward) and label the ground truth for\neach class. Each gesture has around 250 testing samples. The results are shown in Table 4. (Thumb\npointing direction is not evaluated on this dataset because there is no adequate gesture for testing.\nOnly the “thumb down” gesture has a clearly downward pointing direction, yet the images are not\npointing strictly downward.)\nOn this first-person view dataset, the error rate of the rules only increase from 2.3% to 6.3%\n(though the correct rate decrease by around 19% because more “unsure” are predicted), showing\nthat our rules works across different views.\nTable 4. Performance of Rules on EgoGesture Test Set\nRule Performance on EgoGesture\nerror unsure correct\nFlexion - thumb 0.046 0.532 0.422\nFlexion - other fingers 0.060 0.218 0.722\nProximity 0.098 0.194 0.708\nContact 0.044 0.159 0.797\nPointing Direction - thumb - - -\nPalm Orientation 0.108 0.250 0.642\nOverall 0.063 ±0.043 0.216 ±0.096 0.720 ±0.123\nMore detailed error analysis of our rules’ performance on EgoGesture Dataset can be found in\nTable 5. Only those with error rate above 15% are shown.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:26 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\nTable 5. Analysis of Error Cases on EgoGesture Dataset\nRule GestureFinger Error\nRate Observed Reasons\nFlexion -\nother fingers\nseven ring 0.171 MediaPipe’s mistake for occluded fingers.\nC ring 0.595 The finger in this gesture is slighly bent by nature,\nhard to predict precisely.\npinky 0.360 Same as above.\nthumb\ndown index 0.177 MediaPipe’s mistake for occluded fingers.\nring 0.326 Same as above.\nthree-\n2 ring 0.281 Same as above.\nProximity\nC middle-ring 0.255 The rule does not generalize very well.\nthree middle-ring 0.154 Same as above.\nfour\nindex-middle 0.260\nLandmarks mistake; some people perform it differ-\nently; the fingers are slightly separated by nature,\nhard to predict precisely, but it doesn’t influence\nthe recognition of the gesture very much.\nmiddle-ring 0.536 Same as above.\nring-pinky 0.353 Same as above.\nfive\nindex-middle 0.289 Same as above.\nmiddle-ring 0.767 Same as above.\nring-pinky 0.488 Same as above.\nok middle-ring 0.404 Same as above.\nring-pinky 0.578 Same as above.\nnine index-middle 0.244 Landmarks mistake.\nContact\nseven thumb-ring 0.202 Some people perform it differently; MediaPipe’s\nmistake for occluded fingers.\nthumb-pinky 0.151 Same as above.\nmeasure thumb-index 0.198 Landmark mistake; in some gestures thumb and\nindex finger are close so it is hard to discriminate.\nnine thumb-index 0.191 Landmark mistake.\nthumb-pinky 0.244 MediaPipe’s mistake for occluded fingers.\nthumb\ndown thumb-index 0.223 Landmark mistake.\nthumb\nback-\nward\nthumb-index 0.249 Same as above.\nthumb\nfor-\nward\nthumb-index 0.186 Same as above.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:27\nA.3 Pseudocode for description rules\nAlgorithm 1 Flexion of a finger\n1: procedure Flexion(𝑓𝑖𝑛𝑔𝑒𝑟,𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑𝐿𝑜𝑤,𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑𝐻𝑖𝑔ℎ )\n2: 𝑐𝑢𝑟𝑙 ←0\n3: if 𝑓𝑖𝑛𝑔𝑒𝑟 is thumb then\n4: 𝑐𝑢𝑟𝑙 ←𝐴𝑛𝑔𝑙𝑒(−−−−−−→MCP-IP,− −−−− →IP-TIP)\n5: else\n6: 𝑐𝑢𝑟𝑙 ←𝐴𝑛𝑔𝑙𝑒(−−−−−−−→MCP-PIP,−−−−−−→PIP-DIP)\n7: 𝑐𝑢𝑟𝑙 ←𝑐𝑢𝑟𝑙 +𝐴𝑛𝑔𝑙𝑒(−−−−−−→PIP-DIP,−−−−−−→DIP-TIP)\n8: end if\n9: if 𝑐𝑢𝑟𝑙 ≤𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑𝐿𝑜𝑤 then\n10: return 𝑠𝑡𝑟𝑎𝑖𝑔ℎ𝑡\n11: else if 𝑐𝑢𝑟𝑙 ≥𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑𝐻𝑖𝑔ℎ then\n12: return 𝑏𝑒𝑛𝑡\n13: else\n14: return 𝑢𝑛𝑠𝑢𝑟𝑒\n15: end if\n16: end procedure\nAlgorithm 2 Proximity of two fingers\n1: procedure Proximity(𝑓𝑖𝑛𝑔𝑒𝑟1,𝑓𝑖𝑛𝑔𝑒𝑟 2,𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 𝐿𝑜𝑤,𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 𝐻𝑖𝑔ℎ)\n2: 𝑗𝑜𝑖𝑛𝑡𝐷𝑖𝑠 ←0\n3: 𝑝𝑜𝑙𝑦𝑙𝑖𝑛𝑒𝐹 1 ←𝑃𝑜𝑙𝑦𝑙𝑖𝑛𝑒(𝑓𝑖𝑛𝑔𝑒𝑟1 𝑃𝐼𝑃,𝑓𝑖𝑛𝑔𝑒𝑟 1 𝐷𝐼𝑃,𝑓𝑖𝑛𝑔𝑒𝑟 1 𝑇𝐼𝑃)\n4: 𝑝𝑜𝑙𝑦𝑙𝑖𝑛𝑒𝐹 2 ←𝑃𝑜𝑙𝑦𝑙𝑖𝑛𝑒(𝑓𝑖𝑛𝑔𝑒𝑟2 𝑃𝐼𝑃,𝑓𝑖𝑛𝑔𝑒𝑟 2 𝐷𝐼𝑃,𝑓𝑖𝑛𝑔𝑒𝑟 2 𝑇𝐼𝑃)\n5: 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒1 ←𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑓𝑖𝑛𝑔𝑒𝑟1 𝑃𝐼𝑃,𝑝𝑜𝑙𝑦𝑙𝑖𝑛𝑒𝐹 2)\n6: 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒2 ←𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑓𝑖𝑛𝑔𝑒𝑟2 𝑃𝐼𝑃,𝑝𝑜𝑙𝑦𝑙𝑖𝑛𝑒𝐹 1)\n7: 𝑗𝑜𝑖𝑛𝑡𝐷𝑖𝑠 ←𝑗𝑜𝑖𝑛𝑡𝐷𝑖𝑠 +𝑚𝑖𝑛(𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒1,𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 2)\n8: 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒3 ←𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑓𝑖𝑛𝑔𝑒𝑟1 𝐷𝐼𝑃,𝑝𝑜𝑙𝑦𝑙𝑖𝑛𝑒𝐹 2)\n9: 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒4 ←𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑓𝑖𝑛𝑔𝑒𝑟2 𝑇𝐼𝑃,𝑝𝑜𝑙𝑦𝑙𝑖𝑛𝑒𝐹 1)\n10: 𝑗𝑜𝑖𝑛𝑡𝐷𝑖𝑠 ←𝑗𝑜𝑖𝑛𝑡𝐷𝑖𝑠 +𝑚𝑖𝑛(𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒3,𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 4))\n11: 𝑗𝑜𝑖𝑛𝑡𝐷𝑖𝑠 ←𝑗𝑜𝑖𝑛𝑡𝐷𝑖𝑠 /3\n12: if 𝑗𝑜𝑖𝑛𝑡𝐷𝑖𝑠 is less than 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑𝐿𝑜𝑤 then\n13: return 𝑎𝑑𝑗𝑎𝑐𝑒𝑛𝑡\n14: else if 𝑗𝑜𝑖𝑛𝑡𝐷𝑖𝑠 is greater than 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑𝐻𝑖𝑔ℎ then\n15: return 𝑠𝑒𝑝𝑎𝑟𝑎𝑡𝑒𝑑\n16: else\n17: return 𝑢𝑛𝑠𝑢𝑟𝑒\n18: end if\n19: end procedure\nAlgorithm 3 Contact of two fingers\n1: procedure Contact(𝑓𝑖𝑛𝑔𝑒𝑟1,𝑓𝑖𝑛𝑔𝑒𝑟 2,𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 𝐿𝑜𝑤,𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 𝐻𝑖𝑔ℎ)\n2: 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 ←𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑓𝑖𝑛𝑔𝑒𝑟1 𝑇𝐼𝑃,𝑓𝑖𝑛𝑔𝑒𝑟 2 𝑇𝐼𝑃)\n3: if 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 ≤𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑𝐿𝑜𝑤 then\n4: return 𝑐𝑜𝑛𝑡𝑎𝑐𝑡\n5: else if 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 ≥𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑𝐻𝑖𝑔ℎ then\n6: return 𝑛𝑜𝑡𝑐𝑜𝑛𝑡𝑎𝑐𝑡\n7: else\n8: return 𝑢𝑛𝑠𝑢𝑟𝑒\n9: end if\n10: end procedure\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:28 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\nAlgorithm 4 Thumb Pointing Direction\n1: procedure Contact(𝑡ℎ𝑢𝑚𝑏,𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 )\n2: if 𝑡ℎ𝑢𝑚𝑏is not straight then\n3: return 𝑢𝑛𝑠𝑢𝑟𝑒\n4: else\n5: 𝑇ℎ𝑢𝑚𝑏 ←−−−−−−−→MCP-TIP\n6: 𝑚𝑖𝑛𝐴𝑛𝑔𝑙𝑒 ←+∞\n7: 𝑇ℎ𝑢𝑚𝑏𝐷𝑖𝑟 ←𝑁𝑜𝑛𝑒\n8: for 𝑑𝑖𝑟 in [𝑑𝑜𝑤𝑛,𝑢𝑝 ]do\n9: 𝑎𝑛𝑔𝑙𝑒 ←𝐴𝑛𝑔𝑙𝑒(𝑇ℎ𝑢𝑚𝑏,𝑑𝑖𝑟 )\n10: if 𝑎𝑛𝑔𝑙𝑒 is less than 𝑚𝑖𝑛𝐴𝑛𝑔𝑙𝑒 then\n11: 𝑚𝑖𝑛𝐴𝑛𝑔𝑙𝑒 ←𝑎𝑛𝑔𝑙𝑒\n12: 𝑇ℎ𝑢𝑚𝑏𝐷𝑖𝑟 ←𝑑𝑖𝑟\n13: end if\n14: end for\n15: if 𝑚𝑖𝑛𝐴𝑛𝑔𝑙𝑒 is greater than 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 then\n16: return 𝑢𝑛𝑠𝑢𝑟𝑒\n17: else\n18: return 𝑇ℎ𝑢𝑚𝑏𝐷𝑖𝑟\n19: end if\n20: end if\n21: end procedure\nAlgorithm 5 Palm orientation\n1: procedure PalmOrientation(ℎ𝑎𝑛𝑑,𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 )\n2: 𝑃𝑎𝑙𝑚𝑉𝑒𝑐1 ←−−−−−−−−−−−−−−−−−−−−−→𝑝𝑖𝑛𝑘𝑦𝑀𝐶𝑃,𝑖𝑛𝑑𝑒𝑥𝑀𝐶𝑃\n3: 𝑃𝑎𝑙𝑚𝑉𝑒𝑐2 ←−−−−−−−−−−−−−−−−→𝑤𝑟𝑖𝑠𝑡,𝑚𝑖𝑑𝑑𝑙𝑒𝑀𝐶𝑃\n4: if ℎ𝑎𝑛𝑑is left hand then\n5: 𝑃𝑎𝑙𝑚𝑂𝑟𝑖𝑉𝑒𝑐 ←𝑃𝑎𝑙𝑚𝑉𝑒𝑐1 ×𝑃𝑎𝑙𝑚𝑉𝑒𝑐2\n6: else if ℎ𝑎𝑛𝑑is right hand then\n7: 𝑃𝑎𝑙𝑚𝑂𝑟𝑖𝑉𝑒𝑐 ←𝑃𝑎𝑙𝑚𝑉𝑒𝑐2 ×𝑃𝑎𝑙𝑚𝑉𝑒𝑐1\n8: end if\n9: 𝑚𝑖𝑛𝐴𝑛𝑔𝑙𝑒 ←+∞\n10: 𝑃𝑎𝑙𝑚𝑂𝑟𝑖 ←𝑁𝑜𝑛𝑒\n11: for 𝑑𝑖𝑟 in [𝑟𝑖𝑔ℎ𝑡,𝑙𝑒𝑓𝑡,𝑑𝑜𝑤𝑛,𝑢𝑝,𝑜𝑢𝑡𝑤𝑎𝑟𝑑,𝑖𝑛𝑤𝑎𝑟𝑑 ]do\n12: 𝑎𝑛𝑔𝑙𝑒 ←𝐴𝑛𝑔𝑙𝑒(𝑃𝑎𝑙𝑚𝑂𝑟𝑖𝑉𝑒𝑐,𝑑𝑖𝑟 )\n13: if 𝑎𝑛𝑔𝑙𝑒 is less than 𝑚𝑖𝑛𝐴𝑛𝑔𝑙𝑒 then\n14: 𝑚𝑖𝑛𝐴𝑛𝑔𝑙𝑒 ←𝑎𝑛𝑔𝑙𝑒\n15: 𝑃𝑎𝑙𝑚𝑂𝑟𝑖 ←𝑑𝑖𝑟\n16: end if\n17: end for\n18: if 𝑚𝑖𝑛𝐴𝑛𝑔𝑙𝑒 is greater than 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 then\n19: return 𝑢𝑛𝑠𝑢𝑟𝑒\n20: else\n21: return 𝑃𝑎𝑙𝑚𝑂𝑟𝑖\n22: end if\n23: end procedure\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:29\nA.4 Gesture State Matrix Details\nThe matrix generated by the rule-based module is subsequently interpreted by Gesture Description\nAgent’s gesture summary description generation module, and the result is provided to the Gesture\nInference Agent for further analysis. This matrix encapsulates hand pose and movement status\nacross two distinct channels, each offering a different dimension of gesture representation. This\npose-movement split is proven to promote Gesture Description Agent’s performance, avoiding\nomitting important characteristics or overemphasizing certain aspect.\nChannel 1: Hand Pose The first channel is a 2D array comprising 19 rows and 𝑇 columns, where\n𝑇 denotes the number of time steps, with each step representing 0.2 seconds. The rows are indexed\nstarting from 1 and detail the following aspects:\n•Rows 1-5 correspond to finger flexion for the thumb, index, middle, ring, and pinky fingers,\nrespectively. The values are encoded as 1 (straight), 0 (between straight and bent), and -1\n(bent), describing the extent of finger flexion.\n•Rows 6-8 represent finger proximity for adjacent finger pairs (index-middle, middle-ring,\nring-pinky) with similar encoding scheme. The aim is to indicate how closely each finger is\nto its neighbor.\n•Rows 9-12 detail thumb fingertip contact with the fingertips of the other fingers, again using\nsimilar value encoding.\n•Row 13 specifies the pointing direction of the thumb, with 1 (upward), -1 (downward), and 0\n(no specific direction or unknown when thumb is bent).\n•Rows 14-19 are dedicated to palm orientation, indicating the direction the palm faces from\nthe user’s perspective. A specific orientation is marked by a single row set to 1 among these\nrows, representing left, right, down, up, inward, and outward directions. All rows equal to 0\nmeans no specific direction can be identified.\nChannel 2: Hand Movement The second channel consists of a 2D array with 2 or 3 rows (depending\non whether we can extract hand position in 3d space or 2d space) and𝑇 columns. On each time\nstep, the vector corresponds to the geometric center of the hand at this time:\n•Row 1 tracks the horizontal position (0 for leftmost to 1 for rightmost), where increasing\nvalues suggest movement from left to right.\n•Row 2 follows the vertical position (0 for bottommost to 1 for topmost), where increasing\nvalues suggest movement from down to up.\nTo gauge movement magnitude, the hand’s width is also provided. For example, a hand width of\n0.05 with a rightward movement of 0.05 in the array suggests a displacement of approximately one\nhand width.\nThis detailed matrix representation ensures a comprehensive and nuanced understanding of\nhand gestures, facilitating advanced processing and interpretation in gesture recognition systems.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:30 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\nB Detailed Experiment Setting\nThis section outlines the experimental setup for two experiments carried out within this research.\nB.1 Experiment 1: Augmented Reality-Based Smart Home IoT Control\n(a) (b)\nFig. 11. The experimental platform utilized in the smart home scenario. (a) IoT device control interface,\nsimulated using Unity. (b) User wearing the Hololens and performing gestures with the right hand for device\ncontrol.\nThe first experiment focuses on controlling IoT devices within a smart home environment\nthrough augmented reality. The setup simulates a scenario where users interact with various home\ndevices using gesture controls.\nAs illustrated in Figure 11, the experimental platform integrates an augmented reality interface\nfor IoT device control within a home setting.\nThe experimental setup encompasses a variety of device functions and user tasks to mimic\nreal-world interactions with a smart home environment. Details for this experiment are presented\nin Table 7 for device functions and Table 6 for tasks and external contexts. The smart home scenario\nencompasses a total of 18 functions across 5 devices, offering a comprehensive assessment of\ngesture-based control in an augmented reality context.\nB.2 Experiment 2: Online Video Streaming on PC\nThe second experiment focuses on user interaction with online video content on a PC monitor. To\nensure familiarity with the website’s interface among participants, we selected a highly popular\nvideo website for the experiment. Figure 12 illustrates the setup of our experimental platform.\nDetailed descriptions of the tasks, including the number of functions and external context\ninformation for the video streaming environment, are provided in Table 8. Additionally, the list\nof functions available for tasks 4, 5, and 6 is presented in Table 9, while the function list for the\nremaining tasks is shown in Table??. The variation in the function list is attributed to task 3, which\ninvolves full-screening the video page, resulting in a reduced number of available functions.\nC System Cost for LLM Use\nOur system employs the OpenAI API4 gpt-4-1106-preview for experiments, which is one of the\nbest performance LLM and achieving near-human-level common sense and reasoning. The cost of\neach run is determined by the total token count.\n4https://openai.com/pricing#language-models\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:31\nTable 6. Task Instructions and External Context in the Smart Home Scenario\nTask Instruction External Context\nUnlock the Smart Carbinet. [“It is 7:00 PM now. ”,\n“The child lock on this cabinet supports fingerprint unlocking. ”]\nIncrease the brightness of\nthe light.\n[“It is 7:05 PM now. ”]\nShow the next recipes on the\nsmart screen.\n[“It is 7:12 PM now. ”]\nOpen the oven. [“It is 7:14 PM now. ”,\n“Recipe instructions: now you need to open the oven”]\nOpen the air cleaner.\n[“It is 7:20 PM now. ”,\n“The air purifier’s sensor detected that the current environment\nhas heavy cooking fumes. ”]\nSet a timer on the smart\nscreen.\n[“It is 7:30 PM now. ”,\n“Recipe instructions:\nnow you need to cook on high heat for five minutes. ”]\nSwitch input source of the\nsmart screen to the smart\nbell.\n[“It is 7:32 PM now. ”,\n“The doorbell is ringing. ”]\nMake a phone call throught\nthe smart screen.\n[“It is 7:33 PM now. ”,\n“Just now, it was the deliveryman delivering goods;\nthe owner of the goods is the user’s roommate, Mark. ”]\nTable 7. Device Functions in Smart Home Scenario (Totally 18 Functions in 5 Devices)\nDevice NameFunction Name\nLight\nOn / Off\nBrightness Control\nMode Switch (Task Lighting / Morning Lighting / Accent Lighting)\nSmart Cabinet\nChild Lock Activated / Deactivated\nTemperature Control\nHumidity Control\nSmart Screen\nOn / Off\nSwitch Recipes (Recipe 1 / Recipe 2 / Recipe 3)\nSwitch Input Source (Smart Screen / Smart Doorbell / IPad Video)\nPhone Call\nSettable Timer\nOven\nOn / Off\nTemperature Control\nSelf Cleaning On / Off\nMode Switch (Bake Mode / Convection Roast / Bottom Heat Only / Keep Warm / Energy Efficiency)\nAir Cleaner\nOn/Off\nAirflow Speed Control\nMode Switch (Strong / Silent / Custom)\nFor each gesture, GestureGPT consumes an average of 38785 tokens (SD = 10432) for input and\nan average of 3443 tokens (SD = 1339) for output, spanning 6.08 rounds of conversation (SD = 0.85).\nThis results in a cost of $0.389 per gesture.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:32 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\n(b) (c)\n(a)\nFig. 12. The experimental platform utilized in the video streaming scenario. (a) A user was watching the video\nand performing gesture to control it. (b) and (c) are the video interface and the function distribution.\nTable 8. Task Instructions, Function Numbers and External Context in Video Streaming Scenario\nTask Instruction Function Numbers External Context\nTurn up the volume. 66 [“It is 8:01 PM now. ”]\nDrag the progress bar for-\nward.\n66\n[“It is 8:02 PM now. ”,\n“The user has watched the earlier part of\nthis video. ”]\nEnter full screen mode. 66 [“It is 8:04 PM now. ”]\nPause the video. 17\n[“It is 8:15 PM now. ”,\n“Right now, the user’s phone is actively\nreceiving an incoming call. ”]\nResume the video. 17 [“It is 8:17 PM now. ”,\n“The user hung up the phone”]\nExit full screen mode. 17 [“It is 8:48 PM now. ”]\nLike the video. 66 [“It is 8:49 PM now. ”]\nGo to the next episode. 66 [“It is 8:50 PM now. ”]\nTable 9. Video Scenario Function List in Task 4, 5, 6\nID Name ID Name\n0 VideoProgressBarUpdate 9 SelectEpisode\n1 PlayPauseButton 10 ChangePlaybackSpeed\n2 NextButton 11 SubtitleControl\n3 SeekTimeUpdate 12 VolumeControl\n4 ToggleDanmakuDisplay 13 VideoSettingsMenu\n5 DanmakuToggle 14 PictureInPictureToggle\n6 DanmuEtiquetteHint 15 ToggleFullscreen\n7 SendMessageButton 16 VideoPlayArea\n8 VideoQualitySelection\nAlthough costs are currently elevated due to the premium on model resources and extensive\ntoken requirements, we anticipate a reduction in expenses as LLM technology continues to evolve.\nD Data Samples and Prompts Utilized in GestureGPT\nGestureGPT utilizes a triple-agent collaborative architecture, with each agent guided by its own\nspecific prompt to steer the behavior of the LLM. The gesture data in GestureGPT also undergoes\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:33\nseveral format transformations. To facilitate better utilization by the community, we will make\nour prompts and some corresponding data samples publicly available. Due to the length of the\nsystem prompt, it has been archived in a GitHub repository. The full prompt can be accessed at\nhttps://github.com/studyzx/GestureGPT_ISS.\nReferences\n[1] Deepak Akkil and Poika Isokoski. 2016. Gaze Augmentation in Egocentric Video Improves Awareness of Intention.\nIn Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . ACM, San Jose California USA,\n1573–1584. https://doi.org/10.1145/2858036.2858127\n[2] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna\nGoldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez,\nDawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua\nLandau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi\nMercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk,\nStanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R.\nBowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared\nKaplan. 2022. Constitutional AI: Harmlessness from AI Feedback. http://arxiv.org/abs/2212.08073 arXiv:2212.08073\n[cs].\n[3] Sigal Berman and Helman Stern. 2012. Sensors for Gesture Recognition Systems.IEEE Transactions on Systems, Man, and\nCybernetics, Part C (Applications and Reviews) 42, 3 (May 2012), 277–290. https://doi.org/10.1109/TSMCC.2011.2161077\nConference Name: IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews).\n[4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny\nDriess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan,\nKehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry\nKalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski,\nIgor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre\nSermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid,\nStefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. 2023.\nRT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. http://arxiv.org/abs/2307.15818\narXiv:2307.15818 [cs].\n[5] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023.\nChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate. http://arxiv.org/abs/2308.07201\narXiv:2308.07201 [cs].\n[6] Ishan Chatterjee, Robert Xiao, and Chris Harrison. 2015. Gaze+Gesture: Expressive, Precise and Targeted Free-Space\nInteractions. In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction . ACM, Seattle\nWashington USA, 131–138. https://doi.org/10.1145/2818346.2820752\n[7] Weihao Chen, Chun Yu, Huadong Wang, Zheng Wang, Lichen Yang, Yukun Wang, Weinan Shi, and Yuanchun Shi. 2023.\nFrom Gap to Synergy: Enhancing Contextual Understanding through Human-Machine Collaboration in Personalized\nSystems. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST ’23) .\nAssociation for Computing Machinery, New York, NY, USA, 1–15. https://doi.org/10.1145/3586183.3606741\n[8] William Delamare, Chaklam Silpasuwanchai, Sayan Sarcar, Toshiaki Shiraki, and Xiangshi Ren. 2019. On Gesture\nCombination: An Exploration of a Solution to Augment Gesture Interaction. InProceedings of the 2019 ACM International\nConference on Interactive Surfaces and Spaces . ACM, Daejeon Republic of Korea, 135–146. https://doi.org/10.1145/\n3343055.3359706\n[9] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2Web:\nTowards a Generalist Agent for the Web. http://arxiv.org/abs/2306.06070 arXiv:2306.06070 [cs].\n[10] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan\nTompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey\nLevine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence.\n2023. PaLM-E: An Embodied Multimodal Language Model. http://arxiv.org/abs/2303.03378 arXiv:2303.03378 [cs].\n[11] Afshan Ejaz, Maria Rahim, and Shakeel Ahmed Khoja. 2019. The Effect of Cognitive Load on Gesture Acceptability of\nOlder Adults in Mobile Application. In2019 IEEE 10th Annual Ubiquitous Computing, Electronics & Mobile Communication\nConference (UEMCON) . IEEE, New York City, NY, USA, 0979–0986. https://doi.org/10.1109/UEMCON47517.2019.\n8992970\n[12] Euan Freeman, Stephen Brewster, and Vuokko Lantz. 2016. Do That, There: An Interaction Technique for Addressing\nIn-Air Gesture Systems. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16) .\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:34 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\nAssociation for Computing Machinery, New York, NY, USA, 2319–2331. https://doi.org/10.1145/2858036.2858308\n[13] Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi\nZhang, and Nesreen K. Ahmed. 2024. Bias and Fairness in Large Language Models: A Survey. http://arxiv.org/abs/\n2309.00770 arXiv:2309.00770 [cs].\n[14] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.\nPAL: Program-aided Language Models. https://doi.org/10.48550/arXiv.2211.10435 arXiv:2211.10435 [cs].\n[15] Qi Gao, Zheng Ma, Quan Gu, Jiaofeng Li, and Zaifeng Gao. 2023. Working Memory Capacity for Gesture-Command\nAssociations in Gestural Interaction.International Journal of Human–Computer Interaction 39, 15 (Sept. 2023), 3045–3056.\nhttps://doi.org/10.1080/10447318.2022.2091213\n[16] Xianliang Ge, Yunxian Pan, Sujie Wang, Linze Qian, Jingjia Yuan, Jie Xu, Nitish Thakor, and Yu Sun. 2023. Improving\nIntention Detection in Single-Trial Classification Through Fusion of EEG and Eye-Tracker Data. IEEE Transactions on\nHuman-Machine Systems 53, 1 (Feb. 2023), 132–141. https://doi.org/10.1109/THMS.2022.3225633\n[17] Jun Gong, Zheer Xu, Qifan Guo, Teddy Seyed, Xiang ’Anthony’ Chen, Xiaojun Bi, and Xing-Dong Yang. 2018. WrisText:\nOne-handed Text Entry on Smartwatch using Wrist Gestures. In Proceedings of the 2018 CHI Conference on Human\nFactors in Computing Systems . ACM, Montreal QC Canada, 1–14. https://doi.org/10.1145/3173574.3173755\n[18] Jun Gong, Xing-Dong Yang, and Pourang Irani. 2016. WristWhirl: One-handed Continuous Smartwatch Input using\nWrist Gestures. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology . ACM, Tokyo\nJapan, 861–872. https://doi.org/10.1145/2984511.2984563\n[19] Jun Gong, Yang Zhang, Xia Zhou, and Xing-Dong Yang. 2017. Pyro: Thumb-Tip Gesture Recognition Using Pyroelectric\nInfrared Sensing. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology . ACM,\nQuébec City QC Canada, 553–563. https://doi.org/10.1145/3126594.3126615\n[20] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 2024.\nA Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. http://arxiv.org/abs/\n2307.12856 arXiv:2307.12856 [cs].\n[21] Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah\nFiedel, and Aleksandra Faust. 2023. Understanding HTML with Large Language Models. http://arxiv.org/abs/2210.03945\narXiv:2210.03945 [cs].\n[22] Fang Hu, Peng He, Songlin Xu, Yin Li, and Cheng Zhang. 2020. FingerTrak: Continuous 3D Hand Pose Tracking\nby Deep Learning Hand Silhouettes Captured by Miniature Thermal Cameras on Wrist. Proceedings of the ACM on\nInteractive, Mobile, Wearable and Ubiquitous Technologies 4, 2 (June 2020), 1–24. https://doi.org/10.1145/3397306\n[23] Chien-Ming Huang, Sean Andrist, Allison Sauppé, and Bilge Mutlu. 2015. Using gaze patterns to predict task intent in\ncollaboration. Frontiers in Psychology 6 (July 2015). https://doi.org/10.3389/fpsyg.2015.01049\n[24] Lihi Idan. 2022. A Network-Based, Multidisciplinary Approach to Intention Inference. In CHI Conference on Human\nFactors in Computing Systems Extended Abstracts . ACM, New Orleans LA USA, 1–7. https://doi.org/10.1145/3491101.\n3519754\n[25] Toshiya Isomoto, Shota Yamanaka, and Buntarou Shizuki. 2022. Dwell Selection with ML-based Intent Prediction\nUsing Only Gaze Data. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 6, 3, Article 120 (sep 2022), 21 pages.\nhttps://doi.org/10.1145/3550301\n[26] Dou Jian-peng, Li Hang, Pang Xiao-Ling, Zhang Chao-Ni, Yang Tian-Huai, and Jin Xian-Min. 2019. Research progress\nof quantum memory. Acta Physica Sinica (2019). https://doi.org/10.7498/APS.68.20190039\n[27] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. LongLLM-\nLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression. (2023). https:\n//doi.org/10.48550/ARXIV.2310.06839 Publisher: [object Object] Version Number: 1.\n[28] Shuo Jiang, Ling Li, Haipeng Xu, Junkai Xu, Guoying Gu, and Peter B. Shull. 2020. Stretchable e-Skin Patch for\nGesture Recognition on the Back of the Hand. IEEE Transactions on Industrial Electronics 67, 1 (Jan. 2020), 647–657.\nhttps://doi.org/10.1109/TIE.2019.2914621\n[29] A. Just, Y. Rodriguez, and S. Marcel. 2006. Biometric Person Recognition Based on 3D Face Recognition . Research Report\nRR-06-73. Idiap Research Institute. https://infoscience.epfl.ch/record/146075/files/just-idiap-rr-06-73.pdf\n[30] Alexander Kapitanov, Karina Kvanchiani, Alexander Nagaev, Roman Kraynov, and Andrei Makhliarchuk. 2024. HaGRID\n- HAnd Gesture Recognition Image Dataset. http://arxiv.org/abs/2206.08219 arXiv:2206.08219 [cs].\n[31] Evan King, Haoxiang Yu, Sangsu Lee, and Christine Julien. 2023. \"Get ready for a party\": Exploring smarter smart\nspaces with help from large language models. http://arxiv.org/abs/2303.14143 arXiv:2303.14143 [cs].\n[32] Evan King, Haoxiang Yu, Sangsu Lee, and Christine Julien. 2024. Sasha: Creative Goal-Oriented Reasoning in Smart\nHomes with Large Language Models.Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies\n8, 1 (March 2024), 1–38. https://doi.org/10.1145/3643505\n[33] Fatemeh Koochaki and Laleh Najafizadeh. 2018. Predicting Intention Through Eye Gaze Patterns. In2018 IEEE Biomedical\nCircuits and Systems Conference (BioCAS) . IEEE, Cleveland, OH, 1–4. https://doi.org/10.1109/BIOCAS.2018.8584665\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:35\n[34] Peiyu Li, Ney Renau-Ferrer, É. Anquetil, and Eric Jamet. 2012. Semi-customizable Gestural Commands Approach\nand Its Evaluation. 2012 International Conference on Frontiers in Handwriting Recognition (2012), 473–478. https:\n//doi.org/10.1109/ICFHR.2012.267\n[35] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi.\n2023. Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. http://arxiv.org/abs/\n2305.19118 arXiv:2305.19118 [cs].\n[36] Hao Lü and Yang Li. 2011. Gesture avatar: a technique for operating mobile user interfaces using gestures. In\nProceedings of the SIGCHI Conference on Human Factors in Computing Systems . ACM, Vancouver BC Canada, 207–216.\nhttps://doi.org/10.1145/1978942.1978972\n[37] Hao Lü and Yang Li. 2012. Gesture coder: a tool for programming multi-touch gestures by demonstration. InProceedings\nof the SIGCHI Conference on Human Factors in Computing Systems (CHI ’12) . Association for Computing Machinery,\nNew York, NY, USA, 2875–2884. https://doi.org/10.1145/2207676.2208693\n[38] Ruilong Ma, Jingyu Wang, Qi Qi, Xiang Yang, Haifeng Sun, Zirui Zhuang, and Jianxin Liao. 2023. Poster: PipeLLM:\nPipeline LLM Inference on Heterogeneous Devices with Sequence Slicing. In Proceedings of the ACM SIGCOMM 2023\nConference (ACM SIGCOMM ’23) . Association for Computing Machinery, New York, NY, USA, 1126–1128. https:\n//doi.org/10.1145/3603269.3610856\n[39] Naveen Madapana and Juan Wachs. 2019. Database of Gesture Attributes: Zero Shot Learning for Gesture Recognition.\nIn 2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019) . IEEE, Lille, France, 1–8.\nhttps://doi.org/10.1109/FG.2019.8756548\n[40] Naveen Madapana and Juan P. Wachs. 2018. Hard Zero Shot Learning for Gesture Recognition. In2018 24th International\nConference on Pattern Recognition (ICPR) . IEEE, Beijing, 3574–3579. https://doi.org/10.1109/ICPR.2018.8545869 zero\nshot.\n[41] George B. Mo, John J Dudley, and Per Ola Kristensson. 2021. Gesture Knitter: A Hand Gesture Design Tool for Head-\nMounted Mixed Reality Applications. InProceedings of the 2021 CHI Conference on Human Factors in Computing Systems\n(CHI ’21) . Association for Computing Machinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3411764.3445766\n[42] José Antonio Montero and L. Enrique Sucar. 2006. Context-Based Gesture Recognition. InProgress in Pattern Recognition,\nImage Analysis and Applications , David Hutchison, Takeo Kanade, Josef Kittler, Jon M. Kleinberg, Friedemann Mattern,\nJohn C. Mitchell, Moni Naor, Oscar Nierstrasz, C. Pandu Rangan, Bernhard Steffen, Madhu Sudan, Demetri Terzopoulos,\nDough Tygar, Moshe Y. Vardi, Gerhard Weikum, José Francisco Martínez-Trinidad, Jesús Ariel Carrasco Ochoa, and\nJosef Kittler (Eds.). Vol. 4225. Springer Berlin Heidelberg, Berlin, Heidelberg, 764–773. http://link.springer.com/10.\n1007/11892755_79\n[43] Louis-Philippe Morency and Trevor Darrell. 2006. Head gesture recognition in intelligent interfaces: the role of context\nin improving recognition. In Proceedings of the 11th international conference on Intelligent user interfaces . ACM, Sydney\nAustralia, 32–38. https://doi.org/10.1145/1111449.1111464\n[44] K. Murray and G. Häubl. 2010. Freedom of Choice, Ease of Use, and the Formation of Interface Preferences. Behavioral\nMarketing eJournal (2010). https://doi.org/10.2139/ssrn.1698204\n[45] Miguel A. Nacenta, Yemliha Kamber, Yizhou Qiang, and Per Ola Kristensson. 2013. Memorability of pre-designed and\nuser-defined gesture sets. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’13) .\nAssociation for Computing Machinery, New York, NY, USA, 1099–1108. https://doi.org/10.1145/2470654.2466142\n[46] Donald A. Norman. 2010. Natural user interfaces are not natural. Interactions 17, 3 (May 2010), 6–10. https:\n//doi.org/10.1145/1744161.1744163\n[47] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom,\nPaul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,\nChristopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim\nBrooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson,\nRory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark\nChen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai,\nCory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila\nDunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman,\nJuston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha\nGontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei\nGuo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey,\nWade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain,\nShawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun,\nTomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick,\nJong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:36 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\nKondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai\nLan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz\nLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv\nMarkovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey,\nPaul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin\nNair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen\nO’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy\nParparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov,\nHenrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power,\nBoris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real,\nKendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\nGirish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica\nShieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian\nSohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie\nTang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick\nTurley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright,\nJustin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, C. J. Weinmann, Akila Welihinda, Peter Welinder,\nJiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren\nWorkman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba,\nRowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and\nBarret Zoph. 2024. GPT-4 Technical Report. http://arxiv.org/abs/2303.08774 arXiv:2303.08774 [cs].\n[48] Jeongeon Park, Bryan Min, Xiaojuan Ma, and Juho Kim. 2023. ChoiceMates: Supporting Unfamiliar Online Decision-\nMaking with Multi-Agent Conversational Interactions. https://doi.org/10.48550/arXiv.2310.01331 arXiv:2310.01331\n[cs].\n[49] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023.\nGenerative Agents: Interactive Simulacra of Human Behavior. In Proceedings of the 36th Annual ACM Symposium on\nUser Interface Software and Technology . ACM, San Francisco CA USA, 1–22. https://doi.org/10.1145/3586183.3606763\n[50] V.I. Pavlovic, R. Sharma, and T.S. Huang. 1997. Visual interpretation of hand gestures for human-computer interaction:\na review. IEEE Transactions on Pattern Analysis and Machine Intelligence 19, 7 (July 1997), 677–695. https://doi.org/10.\n1109/34.598226 Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence.\n[51] Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li,\nZhiyuan Liu, and Maosong Sun. 2023. Communicative Agents for Software Development. http://arxiv.org/abs/2307.\n07924 arXiv:2307.07924 [cs].\n[52] Dmitriy Rivkin, Francois Hogan, Amal Feriani, Abhisek Konar, Adam Sigal, Steve Liu, and Greg Dudek. 2024. SAGE:\nSmart home Agent with Grounded Execution. http://arxiv.org/abs/2311.00772 arXiv:2311.00772 [cs].\n[53] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy\nKepner, Devesh Tiwari, and Vijay Gadepally. 2023. From Words to Watts: Benchmarking the Energy Costs of Large\nLanguage Model Inference. http://arxiv.org/abs/2310.03003 arXiv:2310.03003 [cs].\n[54] Julia Schwarz, Charles Claudius Marais, Tommer Leyvand, Scott E. Hudson, and Jennifer Mankoff. 2014. Combining\nbody pose, gaze, and gesture to determine intention to interact in vision-based interfaces. In Proceedings of the\nSIGCHI Conference on Human Factors in Computing Systems . ACM, Toronto Ontario Canada, 3443–3452. https:\n//doi.org/10.1145/2556288.2556989\n[55] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023.\nReflexion: Language Agents with Verbal Reinforcement Learning. http://arxiv.org/abs/2303.11366 arXiv:2303.11366\n[cs].\n[56] Ronal Singh, Tim Miller, Joshua Newn, Liz Sonenberg, Eduardo Velloso, and Frank Vetere. 2018. Combining Planning\nwith Gaze for Online Human Intention Recognition. (2018).\n[57] Maximilian Speicher and Michael Nebeling. 2018. GestureWiz: A Human-Powered Gesture Design Environment for\nUser Interface Prototypes. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI ’18) .\nAssociation for Computing Machinery, New York, NY, USA, 1–11. https://doi.org/10.1145/3173574.3173681\n[58] Dina Stiegemeier, Sabrina Bringeland, J. Kraus, and M. Baumann. 2022. User Experience of In-Vehicle Gesture\nInteraction: Exploring the Effect of Autonomy and Competence in a Mock-Up Experiment. Proceedings of the\n14th International Conference on Automotive User Interfaces and Interactive Vehicular Applications (2022). https:\n//doi.org/10.1145/3543174.3546847\n[59] Zhida Sun, Sitong Wang, Chengzhong Liu, and Xiaojuan Ma. 2022. Metaphoraction: Support Gesture-based Interaction\nDesign with Metaphorical Meanings. ACM Transactions on Computer-Human Interaction 29, 5 (Oct. 2022), 1–33.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\nGestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents 545:37\nhttps://doi.org/10.1145/3511892 [meaning].\n[60] Zsolt Szalavári and Michael Gervautz. 1997. The personal interaction Panel–a Two-Handed interface for augmented\nreality. In Computer graphics forum , Vol. 16. Wiley Online Library, C335–C346.\n[61] Yashar Talebirad and Amirhossein Nadiri. 2023. Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM\nAgents. http://arxiv.org/abs/2306.03314 arXiv:2306.03314 [cs].\n[62] Eugene M. Taranta Ii, Thaddeus K. Simons, Rahul Sukthankar, and Joseph J. Laviola Jr. 2015. Exploring the Benefits of\nContext in 3D Gesture Recognition for Game-Based Virtual Environments. ACM Transactions on Interactive Intelligent\nSystems 5, 1 (March 2015), 1–34. https://doi.org/10.1145/2656345\n[63] Jean Vanderdonckt and Éric Petit. 2019. G-Menu: A Keyword-by-Gesture Based Dynamic Menu Interface for Smart-\nphones. In Human-Computer Interaction. Recognition and Interaction Technologies: Thematic Area, HCI 2019, Held as Part\nof the 21st HCI International Conference, HCII 2019, Orlando, FL, USA, July 26–31, 2019, Proceedings, Part II 21 . Springer,\n99–114.\n[64] Radu-Daniel Vatavu. 2012. User-defined gestures for free-hand TV control. In Proceedings of the 10th European\nConference on Interactive TV and Video (EuroITV ’12) . Association for Computing Machinery, New York, NY, USA,\n45–48. https://doi.org/10.1145/2325616.2325626\n[65] Chan Wah Ng and Surendra Ranganath. 2002. Real-time gesture recognition system and application. Image and Vision\nComputing 20, 13 (Dec. 2002), 993–1007. https://doi.org/10.1016/S0262-8856(02)00113-0\n[66] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai\nLin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. 2024. A Survey on Large Language Model based Autonomous\nAgents. http://arxiv.org/abs/2308.11432 arXiv:2308.11432 [cs].\n[67] Tianyi Wang, Xun Qian, Fengming He, Xiyun Hu, Yuanzhi Cao, and Karthik Ramani. 2021. GesturAR: An Authoring\nSystem for Creating Freehand Interactive Augmented Reality Applications. In The 34th Annual ACM Symposium on\nUser Interface Software and Technology (UIST ’21) . Association for Computing Machinery, New York, NY, USA, 552–567.\nhttps://doi.org/10.1145/3472749.3474769\n[68] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2023. Unleashing the Emergent\nCognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (2023).\nhttps://doi.org/10.48550/ARXIV.2307.05300 Publisher: [object Object] Version Number: 4.\n[69] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.\n2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. http://arxiv.org/abs/2201.11903\narXiv:2201.11903 [cs].\n[70] Alan Wexelblat. 1995. An approach to natural gesture in virtual environments. ACM Transactions on Computer-Human\nInteraction 2, 3 (Sept. 1995), 179–200. https://doi.org/10.1145/210079.210080\n[71] Jinting Wu, Yujia Zhang, and Xiaoguang Zhao. 2021. A Prototype-Based Generalized Zero-Shot Learning Framework\nfor Hand Gesture Recognition. In 2020 25th International Conference on Pattern Recognition (ICPR) . IEEE, Milan, Italy,\n3435–3442. https://doi.org/10.1109/ICPR48806.2021.9412548\n[72] Haijun Xia, Michael Glueck, Michelle Annett, Michael Wang, and Daniel Wigdor. 2022. Iteratively Designing Gesture\nVocabularies: A Survey and Analysis of Best Practices in the HCI Literature. ACM Transactions on Computer-Human\nInteraction 29, 4 (2022), 37:1–37:54. https://doi.org/10.1145/3503537\n[73] Xuhai Xu, Jun Gong, Carolina Brum, Lilian Liang, Bongsoo Suh, Shivam Kumar Gupta, Yash Agarwal, Laurence\nLindsey, Runchang Kang, Behrooz Shahsavari, Tu Nguyen, Heriberto Nieto, Scott E Hudson, Charlie Maalouf, Jax Seyed\nMousavi, and Gierad Laput. 2022. Enabling Hand Gesture Customization on Wrist-Worn Devices. In CHI Conference\non Human Factors in Computing Systems . ACM, New Orleans LA USA, 1–19. https://doi.org/10.1145/3491102.3501904\n[74] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. WebShop: Towards Scalable Real-World\nWeb Interaction with Grounded Language Agents. In Advances in Neural Information Processing Systems , S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 20744–20757. https:\n//proceedings.neurips.cc/paper_files/paper/2022/file/82ad13ec01f9fe44c01cb91814fd7b8c-Paper-Conference.pdf\n[75] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing\nReasoning and Acting in Language Models. http://arxiv.org/abs/2210.03629 arXiv:2210.03629 [cs].\n[76] Yiyu Yao. 2012. An Outline of a Theory of Three-Way Decisions. In Rough Sets and Current Trends in Computing ,\nDavid Hutchison, Takeo Kanade, Josef Kittler, Jon M. Kleinberg, Friedemann Mattern, John C. Mitchell, Moni Naor,\nOscar Nierstrasz, C. Pandu Rangan, Bernhard Steffen, Madhu Sudan, Demetri Terzopoulos, Doug Tygar, Moshe Y.\nVardi, Gerhard Weikum, JingTao Yao, Yan Yang, Roman Słowiński, Salvatore Greco, Huaxiong Li, Sushmita Mitra, and\nLech Polkowski (Eds.). Vol. 7413. Springer Berlin Heidelberg, Berlin, Heidelberg, 1–17. https://doi.org/10.1007/978-3-\n642-32115-3_1 Series Title: Lecture Notes in Computer Science.\n[77] Fan Zhang, Valentin Bazarevsky, Andrey Vakunov, Andrei Tkachenka, George Sung, Chuo-Ling Chang, and Matthias\nGrundmann. 2020. MediaPipe Hands: On-device Real-time Hand Tracking. http://arxiv.org/abs/2006.10214\narXiv:2006.10214 [cs].\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.\n545:38 Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, and Yiqiang Chen\n[78] Qiang Zhang, Yuanqiao Lin, Yubin Lin, and Szymon Rusinkiewicz. 2023. UltraGlove: Hand Pose Estimation with\nMems-Ultrasonic Sensors. http://arxiv.org/abs/2306.12652 arXiv:2306.12652 [cs].\n[79] Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023. Wider\nand Deeper LLM Networks are Fairer LLM Evaluators. http://arxiv.org/abs/2308.01862 arXiv:2308.01862 [cs].\n[80] Yifan Zhang, Congqi Cao, Jian Cheng, and Hanqing Lu. 2018. EgoGesture: A New Dataset and Benchmark for\nEgocentric Hand Gesture Recognition. IEEE Transactions on Multimedia 20, 5 (May 2018), 1038–1050. https://doi.org/\n10.1109/TMM.2018.2808769\n[81] Yu Zhang, Tao Gu, Chu Luo, Vassilis Kostakos, and Aruna Seneviratne. 2018. FinDroidHR: Smartwatch Gesture Input\nwith Optical Heartrate Monitor. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2,\n1 (2018), 56:1–56:42. https://doi.org/10.1145/3191788\n[82] Junchen Zhao, Yurun Song, Simeng Liu, Ian G. Harris, and Sangeetha Abdu Jyothi. 2023. LinguaLinked: A Distributed\nLarge Language Model Inference System for Mobile Devices. http://arxiv.org/abs/2312.00388 arXiv:2312.00388 [cs].\n[83] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024. GPT-4V(ision) is a Generalist Web Agent, if\nGrounded. http://arxiv.org/abs/2401.01614 arXiv:2401.01614 [cs].\n[84] Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. 2023. Can ChatGPT Understand Too? A Comparative\nStudy on ChatGPT and Fine-tuned BERT. http://arxiv.org/abs/2302.10198 arXiv:2302.10198 [cs] version: 2.\n[85] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R. Ashley, Róbert Csordás, Anand Gopalakrishnan, Abdullah\nHamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, Louis Kirsch, Bing Li, Guohao Li, Shuming\nLiu, Jinjie Mai, Piotr Piękos, Aditya Ramesh, Imanol Schlag, Weimin Shi, Aleksandar Stanić, Wenyi Wang, Yuhui\nWang, Mengmeng Xu, Deng-Ping Fan, Bernard Ghanem, and Jürgen Schmidhuber. 2023. Mindstorms in Natural\nLanguage-Based Societies of Mind. http://arxiv.org/abs/2305.17066 arXiv:2305.17066 [cs].\nReceived 2024-07-01; accepted 2024-09-20\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545. Publication date: December 2024.",
  "topic": "Gesture",
  "concepts": [
    {
      "name": "Gesture",
      "score": 0.8587956428527832
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.6869213581085205
    },
    {
      "name": "Computer science",
      "score": 0.5709404945373535
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5357792377471924
    },
    {
      "name": "Ground",
      "score": 0.5353641510009766
    },
    {
      "name": "Ground zero",
      "score": 0.503009021282196
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3735990524291992
    },
    {
      "name": "Artificial intelligence",
      "score": 0.29787808656692505
    },
    {
      "name": "Linguistics",
      "score": 0.22675004601478577
    },
    {
      "name": "Engineering",
      "score": 0.16694390773773193
    },
    {
      "name": "Physics",
      "score": 0.14405885338783264
    },
    {
      "name": "Electrical engineering",
      "score": 0.12725406885147095
    },
    {
      "name": "Philosophy",
      "score": 0.09107360243797302
    },
    {
      "name": "Chemistry",
      "score": 0.07892447710037231
    },
    {
      "name": "Nuclear physics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ]
}