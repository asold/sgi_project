{
  "title": "A Novel Question-Answering Framework for Automated Abstract Screening Using Large Language Models",
  "url": "https://openalex.org/W4389952972",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093532593",
      "name": "Opeoluwa Akinseloyin",
      "affiliations": [
        "Coventry University"
      ]
    },
    {
      "id": "https://openalex.org/A2116695089",
      "name": "Xiaorui Jiang",
      "affiliations": [
        "Coventry University"
      ]
    },
    {
      "id": "https://openalex.org/A206588628",
      "name": "Vasile Palade",
      "affiliations": [
        "Coventry University"
      ]
    },
    {
      "id": "https://openalex.org/A5093532593",
      "name": "Opeoluwa Akinseloyin",
      "affiliations": [
        "Coventry University"
      ]
    },
    {
      "id": "https://openalex.org/A2116695089",
      "name": "Xiaorui Jiang",
      "affiliations": [
        "Coventry University"
      ]
    },
    {
      "id": "https://openalex.org/A206588628",
      "name": "Vasile Palade",
      "affiliations": [
        "Coventry University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2095601341",
    "https://openalex.org/W1997684599",
    "https://openalex.org/W3156775206",
    "https://openalex.org/W2560438049",
    "https://openalex.org/W2969341057",
    "https://openalex.org/W1598602811",
    "https://openalex.org/W4313371821",
    "https://openalex.org/W3149778443",
    "https://openalex.org/W2888944131",
    "https://openalex.org/W2750878999",
    "https://openalex.org/W2751819974",
    "https://openalex.org/W2565695915",
    "https://openalex.org/W2741811485",
    "https://openalex.org/W2798445552",
    "https://openalex.org/W2751718787",
    "https://openalex.org/W2986490042",
    "https://openalex.org/W2767971227",
    "https://openalex.org/W4200630748",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2045384400",
    "https://openalex.org/W4385356790",
    "https://openalex.org/W4312048304",
    "https://openalex.org/W2124493593",
    "https://openalex.org/W168362576",
    "https://openalex.org/W2961191798",
    "https://openalex.org/W2600107025",
    "https://openalex.org/W2184378182",
    "https://openalex.org/W3087185831",
    "https://openalex.org/W4315641584",
    "https://openalex.org/W2063198586",
    "https://openalex.org/W1576855214",
    "https://openalex.org/W2038492487",
    "https://openalex.org/W2154352790",
    "https://openalex.org/W2100053037",
    "https://openalex.org/W3023715695",
    "https://openalex.org/W3165198619",
    "https://openalex.org/W2596568673",
    "https://openalex.org/W168544607",
    "https://openalex.org/W2183168769",
    "https://openalex.org/W2066908458",
    "https://openalex.org/W2035792132",
    "https://openalex.org/W3111278950",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4384641573",
    "https://openalex.org/W4383722515",
    "https://openalex.org/W4390814081",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W3196385594",
    "https://openalex.org/W4385572342",
    "https://openalex.org/W4386566754",
    "https://openalex.org/W3212903536",
    "https://openalex.org/W4385268814",
    "https://openalex.org/W4308628131",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W3147292006",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3046375318"
  ],
  "abstract": "Abstract Objective This paper aims to address the challenges in abstract screening within Systematic Reviews (SR) by leveraging the zero-shot capabilities of large language models (LLMs). Methods We employ LLM to prioritise candidate studies by aligning abstracts with the selection criteria outlined in an SR protocol. Abstract screening was transformed into a novel question-answering (QA) framework, treating each selection criterion as a question addressed by LLM. The framework involves breaking down the selection criteria into multiple questions, properly prompting LLM to answer each question, scoring and re-ranking each answer, and combining the responses to make nuanced inclusion or exclusion decisions. Results Large-scale validation was performed on the benchmark of CLEF eHealth 2019 Task 2: Technology- Assisted Reviews in Empirical Medicine. Focusing on GPT-3.5 as a case study, the proposed QA framework consistently exhibited a clear advantage over traditional information retrieval approaches and bespoke BERT- family models that were fine-tuned for prioritising candidate studies (i.e., from the BERT to PubMedBERT) across 31 datasets of four categories of SRs, underscoring their high potential in facilitating abstract screening. Conclusion Investigation justified the indispensable value of leveraging selection criteria to improve the performance of automated abstract screening. LLMs demonstrated proficiency in prioritising candidate studies for abstract screening using the proposed QA framework. Significant performance improvements were obtained by re-ranking answers using the semantic alignment between abstracts and selection criteria. This further highlighted the pertinence of utilizing selection criteria to enhance abstract screening.",
  "full_text": "Confidential: For Review Only\nJournal of the American Medical Informatics Association,2023, 1–13\ndoi: DOI HERE\nAdvance Access Publication Date: Day Month Year\nPaper\nA Novel Question-Answering Framework for\nAutomated Citation Screening Using Large\nLanguage Models\nOpeoluwa Akinseloyin,1 Xiaorui Jiang 1 and Vasile Palade 1\n1Centre for Computational Science and Mathematical Modelling, Coventry University, Puma Way, CV1 2TT,\nCoventry, United Kingdom\n∗Xiaorui Jiang. xiaorui.jiang@coventry.ac.uk\nFOR PUBLISHER ONLY Received on Date Month Year; revised on Date Month Year; accepted on Date Month Year\nAbstract\nObjective: This paper aims to address the challenges in citation screening (a.k.a. abstract screening) within\nSystematic Reviews (SR) by leveraging the zero-shot capabilities of large language models, particularly\nChatGPT.\nMethods: We employ ChatGPT as a zero-shot ranker to prioritize candidate studies by aligning abstracts\nwith the selection criteria outlined in an SR protocol. Citation screening was transformed into a novel\nquestion-answering (QA) framework, treating each selection criterion as a question addressed by ChatGPT.\nThe framework involves breaking down the selection criteria into multiple questions, properly prompting\nChatGPT to answer each question, scoring and re-ranking each answer, and combining the responses to\nmake nuanced inclusion or exclusion decisions.\nResults: Large-scale validation was performed on the benchmark of CLEF eHealth 2019 Task 2: Technology\nAssisted Reviews in Empirical Medicine. Across 31 datasets of four categories of SRs, the proposed QA\nframework consistently outperformed other zero-shot ranking models. Compared with complex ranking\napproaches with iterative relevance feedback and fine-tuned deep learning-based ranking models, our\nChatGPT-based zero-shot citation screening approaches still demonstrated competitive and sometimes better\nresults, underscoring their high potential in facilitating automated systematic reviews.\nConclusion: Investigation justified the indispensable value of leveraging selection criteria to improve the\nperformance of automated citation screening. ChatGPT demonstrated proficiency in prioritizing candidate\nstudies for citation screening using the proposed QA framework. Significant performance improvements were\nobtained by re-ranking answers using the semantic alignment between abstracts and selection criteria. This\nfurther highlighted the pertinence of utilizing selection criteria to enhance citation screening.\nKey words:Automated Systematic Review, Citation Screening, ChatGPT, Zero-Shot Ranking, Question Answering\nIntroduction\nA Systematic Review (SR) in medical research is the highest\nform of knowledge synthesis of all available medical evidence from\nrelevant publications on a specific topic. SR follows a principled\npipeline, including candidate study retrieval, primary study\nselection, quality assessment, data extraction, data synthesis,\nmeta-analysis, and reporting [1]. Because of its thoroughness and\nreliability, SR underpins evidence-based medicine [2]. It shapes\nmedical research and practice by informing researchers of the\nstate-of-the-art knowledge and knowledge gaps as well as health\npractitioners and policymakers of the best clinical practice [3].\nSR also faces tremendous challenges at each step. For instance,\nit is time-consuming, expensive and resource-intensive to select\nprimary studies, a.k.a. citation screening, due to the massive\nvolume of retrieved candidate studies, often at tens of thousands\n[4, 5]. It is further worsened by involving multiple human\nannotators, which is required to reduce bias and disparities\n[6]. This compound complexity calls for innovative solutions to\nautomate or semi-automate citation screening [1] to minimize the\ntime delays and costs of this manual screening tasks [7], which\nis the focus of the current paper. Figure 1 shows an example\nof citation screening, where the abstract of an included study is\nmatched against the selection criteria defined in the SR protocol.\n© The Author 2023. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com 1\nPage 1 of 14\nhttps://mc.manuscriptcentral.com/jamia\nJournal of the American Medical Informatics Association\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted December 18, 2023. ; https://doi.org/10.1101/2023.12.17.23300102doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nConfidential: For Review Only\n2 Author Name et al.\nMachine learning has been the focus of research in automating\ncitation screening [1, 7, 8]. Firstly, a small set of studies are\nselected for human annotation, and then a classifier is trained.\nTypically, active learning is adopted to improve the classifier\niteratively. Obviously, the quality of the initial annotations plays\nan important role. However, choosing initial annotations is a\nproblem of zero-shot setting and has not been explored at all.\nAnother disadvantage is that this approach is not generalisable,\nand each SR topic requires training a bespoke classifier from\nscratch.\nAn alternative perspective was to treat citation screening as a\nranking problem a.k.a. reference prioritisation [7], incorporating\napproaches from the information retrieval (IR) community [9, 10,\n11, 12, 13, 14, 15, 16]. One advantage of this approach is that it\ncan utilise additional information about an SR, which is converted\ninto queries to enhance screening performance. Such information\ncould be review title [9, 10], original Boolean queries (for candidate\nstudy retrieval) [17], research objectives [18, 16], or a set of seed\nstudies [15, 19]. Another advantage is the possibility of training a\ncross-topic ranker to generalise to diverse SR topics.\nThe above analysis motivated us to explore the emerging\ncapabilities of Large Language Models (LLMs), particularly\nChatGPT in the current paper, to facilitate citation screening.\nIndeed, the recent successes in text ranking [20, 21] suggest LLMs\npotentially could be used as an alternative AI-based reviewer due\nto their strong zero-shot capabilities [22]. This could either save at\nleast one human reviewer’s time or, less radically, suggest a good\ninitial training set for human verification.\nIn addition, we witness a severe lack of study about using\nselection criteria in automated citation screening (except [23]).\nIndeed, it is the selection criteria that set up the grounds\nfor human reviewers’ decision-making. Unfortunately, only a\nfew studies initiated similar attempts [23, 24, 25], and neither\nthe effectiveness of their methods nor the comprehensiveness of\ntheir experiments could provide convincing conclusions about the\nfeasibility of LLMs in this task. The current paper presents\na pioneering LLM-based framework for facilitating automated\ncitation screening to fill this gap.\nOur contributions can be summarised in three folds. (1)\nWe proposed the first comprehensive LLM-assisted question-\nanswering framework for automated citation screening in a zero-\nshot setting. (2) We developed the first generalisable approach\nto utilising selection criteria to enhance citation screening\nperformance. (3) We performed the first comprehensive empirical\nstudy on well-known benchmark datasets and demonstrated the\nhigh potential of the proposed approach for citation screening.\nBackground Study\nAutomating in Citation Screening\nEfforts to automate systematic reviews using machine learning\nhave surged recently. Kitchenham and Charters’ presented a good\nsurvey of such attempts in software engineering [26]. In evidence-\nbased medicine, Cohen et al. was the seminal work of citation\nscreening using machine learning [27], while Marshall and Wallace\nadvocated active learning techniques for citation screening [28].\nExamples like RobotReviewer [29, 30] and TrialStreamer [31]\nshowcased the power of integrating AI into the review process, with\nRobotReviewer claiming to reach accuracy comparable to human\nreviewers. Despite the progress, challenges persist, including\nlabour-intensive labelling and the risk of overlooking relevant\nstudies [32]. Acknowledging the limitation of full automation, tools\nlike Rayyan and Abstracker leverage natural language processing\n(NLP) algorithms to partially automate article screening [33].\nMachine Learning for Citation Screening\nThe biggest challenge is handling large document volumes,\nparticularly in non-randomized controlled trials lacking database\nfilters [34]. For instance, EPPI-Centre reviews often screen over\n20,000 documents, necessitating more efficient approaches [35].\nEfforts include refining search queries, balancing precision and\nrecall, and leveraging resource-efficient recall-maximizing models\nwith NLP [36].\nThe initial approach involves training a classifier to make\nexplicit include/exclude decisions [27, 36, 37, 38, 39, 40, 41]. Many\nclassifiers using this approach inherently generate a confidence\nscore indicating the likelihood of inclusion or exclusion (similar\nto the ranking in the second approach). Generally, this approach\nrequires a labelled dataset for training, hindering the assessment\nof work reduction until after manual screening. Research within\nthis paradigm primarily focuses on enhancing feature extraction\nmethods [27, 39] and refining classifiers [40]. Van Dinter et al.\n[8] analyzed 41 studies in medicine and software engineering,\nrevealing Support Vector Machines and Bayesian Networks as\nstandard models and Bag of Words and TF-IDF as prevalent\nnatural language processing techniques. Despite advancements, a\ndearth of deep neural network models explicitly designed for the\nsystematic review screening phase is noted. The most prominent\nchallenges include handling extreme data imbalance favouring (at\nleast close to) total recall of relevant studies.\nRanking Approaches to Citation Screening\nThe second approach entails utilizing a ranking or prioritization\nsystem [9, 10, 11, 12, 13, 14, 15, 16, 35, 42]. This approach\nmight necessitate manual screening by a reviewer until a specified\ncriterion is met. This approach can also reduce the number\nof items needed to be screened when a cut-off criterion is\nproperly established [35, 42, 43]. In addition to reducing the\nnumber needed to screen, other benefits of this approach\ninclude enhancing reviewers’ understanding of inclusion criteria\nearly in the process, starting full-text retrieval sooner, and\npotentially speeding up the screening process as confidence in\nrelevance grows [7]. This prioritization approach also aids review\nupdates, enabling quicker assimilation of current developments.\nVarious studies reported benefits from prioritization for workflow\nimprovement, emphasizing efficiency beyond reducing title and\nabstract screening workload [44, 45].\nActive learning in Citation Screening\nIt’s crucial to note that the last approach, active learning,\naligns with both strategies above [36, 35, 46]. This involves an\niterative process to enhance machine predictions by interacting\nwith reviewers. The machine learns from an initial set of\ninclude/exclude decisions human reviewers provide. Reviewers\nthen judge on a few new samples, and the machine adapts\nits decision rule based on this feedback. This iterative process\ncontinues until meeting a specified stopping criterion. While\nthe classifier makes final decisions for unscreened items, human\nscreeners retain control over the training process and the point\nat which manual screening concludes. Wallace et al. implement\nPage 2 of 14\nhttps://mc.manuscriptcentral.com/jamia\nJournal of the American Medical Informatics Association\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted December 18, 2023. ; https://doi.org/10.1101/2023.12.17.23300102doi: medRxiv preprint \nConfidential: For Review Only\nShort Article Title 3\nFig. 1: Illustration of LLM-assisted automated citation screening.\nactive learning-based article screening using Support Vector\nMachines [36]. Notable tools include Abstrackr [38] and ASReview\n[47]. Various active learning strategies existed [7]. For instance,\nMarshall and Wallace [28] proposed a variant based on certainty,\ncontinuously training the classifier on manually screened articles\nand reordering unseen articles based on predicted relevance.\nLarge Language Models for Citation Screening\nRecent advancements in LLMs, notably demonstrated by\nChatGPT, have brought about a revolutionary paradigm shift\nacross disciplines [48, 49]. LLMs have shown impressive\ngeneralisability across diverse domains and strong zero-/few-shot\nreasoning capabilities [48, 50]. Leveraging LLMs, like ChatGPT,\nholds promise for SRs, which however remains underexplored [7, 8].\nThis gap underscores the need for a comprehensive investigation\ninto LLMs’ potential in automating SRs, e.g., citation screening\nin the current paper.\nThere are some initial attempts to evaluate ChatGPT in\nautomated SR, such as automating search queries [51]. Alshami\net al. [52] utilized ChatGPT for automating the SR pipeline;\nhowever, their approach did not follow the norm of citation\nscreening, making it incomparable to existing methods. Notably,\nthe effectiveness of ChatGPT in automating citation screening has\nreceived limited attention, with only two studies [53, 54], which,\nunfortunately, lack consideration for achieving high recall, making\nthem less suitable for real-world scenarios.\nMaterials and Methods\nOverview\nOur framework utilizes ChatGPT’s zero-shot learning to assess if a\ncandidate study’s abstract aligns with the SR protocol’s selection\ncriteria. These criteria outline aspects of the selected studies. The\nprovided sentence explains that in Figure 1, the red-highlighted\ntext, ”We included qualitative studies,” serves as an example\nillustrating an inclusion criterion. This criterion specifies that only\nstudies with a qualitative nature will be selected. Theoretically, all\ninclusion criteria should be met for the study to be included in the\nSR.\nOur novel method frames automated citation screening as a\nquestion-answering (QA) task. Each selection criterion is treated\nas a question to be addressed using LLMs like ChatGPT. These\nmodels have showcased impressive question-answering abilities\nacross diverse domains and tasks, including encoding clinical\nknowledge and achieving success in medical licensing exams\n[55, 56, 57, 58].\nAn initial experiment using the whole selection criteria as\none comprehensive question (Figure 2a) proved ineffective. LLMs\nexcel at answering focused and clearly described questions. Hence,\nour improved approach involves breaking down the selection\ncriteria into several K questions (the LM-based Query Generator\ncomponent in Figure 1), prompting LLMs to answer each question\n(the LM-based Question Answerer in Figure 1), and combining the\nanswers for each question (the Ensembler in Figure 1).\nFigure 2b details our proposed QA framework for citation\nscreening. We begin with a Question Generator to convert the\nselection criteria into a set of questions. Optionally, a Question\nAnalyser may be employed to correctly combine the question\nanswers, considering, for instance, that answers A1 and A2\nfor questions Q1 and Q2 represent an inclusion and exclusion\ncriterion, respectively, and the correct combination is A1 ∩ ¬A2.\nSubsequently, each question is addressed by a trained Question\nAnswerer to determine if the corresponding selection criterion is\nmet, with each answer converted into a numeric score. Optionally,\nan Answer Re-ranking component can either be pre-trained on\na large corpus of SRs or task-specifically trained with human\nreviewers’ feedback. Finally, the Ensemble component combines\nthe answers to all questions, and a final decision is made using\npredefined rules. Each component will be detailed in subsequent\nsections.\nQuestion Generation\nA substantial body of research exists on automated question\ngeneration from natural language text [59]. These methods often\nrely on manually crafted rules or a trained model, typically\na fine-tuned pre-trained language model. While these question\ngeneration models have demonstrated utility in domain-specific\ntasks, such as generating questions about product descriptions\nfor matching purchase inquiries [60] or creating questions about\nacademic materials to assess learning outcomes [61], generalizing\nthem to the vast diversity of SR topics presents challenges.\nTherefore, we entrust the question generation task to ChatGPT.\nPage 3 of 14\nhttps://mc.manuscriptcentral.com/jamia\nJournal of the American Medical Informatics Association\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted December 18, 2023. ; https://doi.org/10.1101/2023.12.17.23300102doi: medRxiv preprint \nConfidential: For Review Only\n4 Author Name et al.\n(a) Naive approach for LLM-based automated citation\nscreening using selection criteria\n(b) An aspect-based approach for LLM-assisted automated citation\nscreening using selection criteria\nFig. 2: Methodological framework for LLM-assisted automation\nscreening.\nA naive approach to question generation involves prompting\nChatGPT to generate questions from the given paragraph about\nthe selection criteria of a systematic review. However, this\nuncontrolled method often generates numerous questions, many\nsubsumed by others or deemed too trivial to be meaningful.\nTo enhance the quality of generated questions, we constrained\nChatGPT to produce no more than K questions, aiming to\nminimize redundancy. Based on an analysis of the lengths of\nselection criteria in our dataset’s SRs, K = 5 proved sufficient for\nmost SRs. Each sentence in the selection criteria often aligns well\nwith a distinct criterion. In rare cases with more than 5 sentences,\nChatGPT intelligently combined two sentences into one question.\nFigure 3a depicts the utilized prompt, and an example is shown\nin Figure 1.\n(a) Prompt for question generation\n(b) Prompt for question answering\nFig. 3: Prompt design for LLM-assisted automated citation\nscreening\nQuestion Answering\nThe Question Answerer evaluates the relevance of each abstract\nto every selection criterion, formulated as Yes/No questions. We\nprompt ChatGPT to return three types of responses Figure 3b:\n• Positive: The abstract explicitly addresses the question,\noffering information that aligns with the criteria posed by the\nquestion.\n• Neutral: The information in the abstract is inqdequate or too\nambiguous for ChatGPT to derive a confirmative answer.\n• Negative: A clear NO answer to the question, indicating\nirrelavance to the specified criteria.\nAnswer Representation Approaches\nTwo distinct techniques represent answers, namely the Hard\nAnswer and Soft Answer methods. These methods conceptualize\nanswer representation as a generative sentiment classification\nproblem, leveraging the capabilities of the BART model inspired\nby its recent successes in various sentiment classification tasks [62].\n• Hard Answer: This method involves BART determining\nthe sentiment category of each answer (Positive, Neutral,\nNegative). Traditionally, rejecting an abstract occurred if one\nquestion had a Negative answer, leading to low recall due to\nsmall errors in ChatGPT responses. Instead, we convert the\ndiscrete sentiment categories to semantic scores (e.g., 1, 0.5,\nand 0), enabling the Ensemble to combine answers into a final\ndecision.\n• Soft Answer: ChatGPT often justifies its answer,\ncontributing to quantifying its confidence level in the provided\nanswer. In the Soft Answer method, the sentiment score\nfor each answer is the probability of BART classifying the\nChatGPT-generated answer sentence as positive.\nDecision Engine\nEnsemble\nThe answer scores for each selection criterion are ”averaged.” This\nmean score provides a quantitative representation of the relevance\nof the abstract. Candidate studies are then ranked in descending\norder based on these mean scores. To enhance screening further,\na significant contribution involves re-ranking candidate studies\nbased on how well abstracts are semantically aligned with the\nselection criteria.\nRe-ranking\nSeveral methods are available for embedding selection criteria and\nabstracts. Given the emphasis on the capabilities of LLMs in this\npaper, GPT Embeddings [63] are chosen. Two approaches to re-\nranking are defined: Abstract-level re-ranking and answer-level re-\nranking.\n• Abstract-Level Re-Ranking:This method first aggregates\nthe mean answer score across all questions and then averages\nthe mean score with the similarity score.\n• Answer-Level Re-Ranking:In this more advanced method,\nthe cosine similarity evaluates how well an abstract aligns\nwith each generated question, enhancing the answers to each\nselection criterion. Each Yes/No question is matched against\nthe abstract, and the cosine similarity is averaged with the\ncorresponding answer score. This results inK re-ranked answer\nscores. Then the K scores are averaged, and the mean score is\nPage 4 of 14\nhttps://mc.manuscriptcentral.com/jamia\nJournal of the American Medical Informatics Association\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted December 18, 2023. ; https://doi.org/10.1101/2023.12.17.23300102doi: medRxiv preprint \nConfidential: For Review Only\nShort Article Title 5\nFig. 4: Example of handing exclusion criteria.\nfurther averaged with the overall alignment between selection\ncriteria and abstract. This hierarchical ensemble effectively\nenhances the overall precision of document re-ranking by\nconsidering confidence in answering each question, aligning\nwith each selection criterion, and adhering holistically to\nselection criteria.\nExperimental Setup\nDataset and Evaluation\nThis study utilized datasets of CLEF eHealth 2019 Task 2:\nTechnology Assisted Reviews in Empirical Medicine (TAR2019).\nThis dataset provides valuable insights into the prevailing scientific\nconsensus on various topics, making it a suitable resource for\nevaluating reranking methodologies in systematic reviews [64].\nWe employed the TAR2019 test set comprising 31 SRs\ncategorized into Intervention (20), DTA (8), Qualitative (2), and\nPrognosis (1). We refrained from using the training set, aiming\nto highlight the effectiveness of our zero-shot methodology that\neliminates the need for prior training [65, 25].\nWe used the review titles from the TAR2019 datasets to\nidentify selection criteria. Seven evaluation metrics were employed,\nincluding the rank position of the last relevant document (L\nRel),\nMean Average Precision ( AP), Recall at k% (k = 5, 10, 20,\n30), and Work Saved Over Sampling (WSS) at k% (k = 95%,\n100%). Notably, WSS@k measures the screening workload saved\nby halting the examination process once k% of relevant documents\nare identified, compared to screening the entire document set [27].\nBaseline Model\nThe baseline models, serving as a comparative benchmark, were\nbased on submissions to the TAR2019 workshop [66], which\nencompass UvA [67], UNIPD [68], and Sheffield [17]. Additionally,\nwe considered the nine models evaluated by Wang et al [25].\nUnlike our fully automated model, many workshop submissions\nemploy an iterative ranking system, making them semi-automated.\nTo comprehensively assess performance, we implemented two IR\nbaselines of our own. One is cosine similarity between selection\ncriteria and abstract based on GPT embeddings [63], named\nGPT\nCosine Similarity. The other is BM25 [69], using selection\ncriteria as a query. The variants of our own approach are\nsummarised below:\n• GPT QA Soft/Hard: Soft/Hard answer representation,\nwithout re-ranking.\n• GPT QA Soft/Hard Abstract Level: Soft/Hard answer\nrepresentation, with abstract-level re-ranking.\n• GPT QA Soft/Hard Answer Level: Soft/Hard answer\nrepresentation, with answer-level re-ranking.\nam\nResults\nPrognosis\nOur proposed methods demonstrated promising results on the\nPrognosis dataset (Table 1). Notably, in terms of LRel, for\nwhich a lower value signifies superior performance, answer-level\nre-ranking methods showcased the most impressive results among\nour proposed methods: 2333 for GPT\nQA Soft Answer Level\nand 2373 for GPT QA Hard Answer Level. Our methods also\nachieved MAP scores from 0.350 to 0.430, underscoring the\nmodels’ efficiency in prioritizing candidate studies. This outshined\nnumerous IR methods (UNIPD and Sheffield variants).\nThe proposed methods sustained their superiority in R@k%.\nWhen k ∈ {5, 10}, our re-ranking methods (the last four rows\nin Table 1) consistently outperformed UNIPD and Sheffield\nsubmissions. A promising finding was that our best re-ranking\nmethod successfully suggested 65% of total positive samples\n(included studies) for classifier training when only 10% of\ntotal samples needed to be verified by human reviewers. This\nis a testament to the capacity of selecting positive samples\nfrom highly imbalanced data. The best WSS @95 of our zero-\nshot approaches reached 55.5% on Prognosis, and notably\nWSS @100 was significantly better than most baselines except\n2018\nstem original p50 t1500 which used relevance feedback.\nFrom the holistic view of the evaluation metrics, our answer-\nlevel re-ranked models stood out as cutting-edge solutions,\nachieving either competitive or new state-of-the-art results.\nQualitative\nThe results are presented in Table 2. Similarly, the LRel metric\nhighlighted that both our abstract-level and answer-level re-\nranking methods (the last four rows in Table 2) are particularly\neffective. The MAP results of our proposed models consistently\noutperformed the IR baselines of UNIPD and Sheffield. UvA\nshowed the best performance. Note that the UvA approaches used\nrelevance feedback to improve the ranking performance, so it was\nnot purely a zero-shot problem in our sense. Further discussions\ncan be found in the Discussions section.\nRegarding recall, our models showed promising results when\nk = 5, meaning that our methods identified more than half\nof the positive samples in the top 5% ranked list. This allows\nus to significantly reduce the effort in annotating the initial\ndataset for training a citation screener. Although some IR methods\nshowed higher recall when k ≥ 10, our methods outperformed\nall baselines in R@30% and showed significant performance gains\nin terms of both WSS @95 and WSS @100 over all baselines,\nPage 5 of 14\nhttps://mc.manuscriptcentral.com/jamia\nJournal of the American Medical Informatics Association\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted December 18, 2023. ; https://doi.org/10.1101/2023.12.17.23300102doi: medRxiv preprint \nConfidential: For Review Only\n6 Author Name et al.\nTable 1.Results obtained on the Prognosis data. The zero-shot ranking models are emboldened.\nPaper Models L Rel MAP R@5% R@10% R@20% R@30% WSS@95 WSS@100\nUvA abs-hh-ratio-ilps 2885 0.673 0.562 0.714 0.875 0.911 0.591 0.143\nabs-th-ratio-ilps 2537 0.628 0.521 0.682 0.818 0.927 0.566 0.247\nUNIPD 2018 stem original p10 t400 2967 0.235 0.214 0.484 0.812 0.901 0.567 0.119\ndistributed effort p10 t1500 2594 0.235 0.214 0.484 0.812 0.896 0.554 0.230\n2018 stem original p10 t1000 2644 0.235 0.214 0.484 0.812 0.896 0.554 0.215\n2018 stem original p10 t200 2911 0.242 0.214 0.536 0.812 0.901 0.530 0.135\n2018 stem original p10 t500 2920 0.235 0.214 0.484 0.812 0.891 0.560 0.133\n2018 stem original p10 t300 2955 0.239 0.214 0.547 0.818 0.891 0.556 0.122\n2018 stem original p10 t1500 2578 0.235 0.214 0.484 0.812 0.896 0.554 0.234\ndistributed effort p10 t1000 2563 0.235 0.214 0.484 0.812 0.896 0.554 0.239\n2018 stem original p10 t100 2802 0.259 0.286 0.562 0.797 0.891 0.600 0.168\nbaseline bm25 t500 3343 0.071 0.057 0.130 0.281 0.422 0.084 0.007\ndistributed effort p10 t300 2964 0.235 0.214 0.484 0.812 0.906 0.567 0.120\n2018 stem original p50 t1000 2556 0.221 0.214 0.484 0.740 0.870 0.571 0.241\ndistributed effort p10 t100 2789 0.252 0.250 0.568 0.786 0.875 0.594 0.172\n2018 stem original p50 t200 2911 0.242 0.214 0.536 0.812 0.901 0.530 0.135\nbaseline bm25 t1000 3346 0.070 0.057 0.130 0.276 0.396 0.057 0.006\ndistributed effort p10 t500 2708 0.235 0.214 0.484 0.812 0.891 0.566 0.196\nbaseline bm25 t300 3350 0.071 0.057 0.135 0.276 0.385 0.104 0.005\nbaseline bm25 t100 3350 0.066 0.047 0.130 0.255 0.365 0.059 0.005\n2018 stem original p50 t400 2955 0.231 0.214 0.484 0.807 0.896 0.556 0.122\n2018 stem original p50 t300 2955 0.239 0.214 0.547 0.818 0.891 0.556 0.122\n2018 stem original p50 t100 2802 0.259 0.286 0.562 0.797 0.891 0.600 0.168\ndistributed effort p10 t200 2968 0.240 0.214 0.542 0.807 0.906 0.548 0.119\nbaseline bm25 t400 3347 0.071 0.057 0.130 0.281 0.417 0.109 0.006\n2018 stem original p50 t1500 1975 0.219 0.214 0.484 0.740 0.828 0.500 0.413\n2018 stem original p50 t500 2660 0.228 0.214 0.484 0.807 0.891 0.576 0.210\nbaseline bm25 t1500 3346 0.070 0.057 0.130 0.276 0.396 0.050 0.006\nbaseline bm25 t200 3350 0.069 0.057 0.125 0.266 0.385 0.111 0.005\ndistributed effort p10 t400 2920 0.235 0.214 0.484 0.812 0.891 0.560 0.133\nSheffield sheffield-baseline 2990 0.126 0.146 0.255 0.448 0.594 0.247 0.112\nsheffield-relevence feedback 2775 0.141 0.151 0.307 0.484 0.646 0.305 0.176\nProposed Method GPT Cosine Similarity 3160 0.178 0.200 0.305 0.495 0.647 0.239 0.053\nBM25 3337 0.074 0.089 0.132 0.279 0.416 0.020 0.000\nGPT QA Soft 3211 0.350 0.395 0.563 0.784 0.832 0.434 0.037\nGPT QA Hard 3338 0.315 0.395 0.395 0.753 0.753 0.417 0.060\nGPT QA Soft Abstract Level 2467 0.417 0.395 0.647 0.795 0.879 0.523 0.261\nGPT QA Hard Abstract Level 2398 0.417 0.395 0.637 0.789 0.884 0.543 0.282\nGPT QA Soft Answer Level 2373 0.430 0.400 0.653 0.800 0.884 0.543 0.289\nGPT QA Hard Answer Level 2333 0.429 0.400 0.642 0.789 0.884 0.555 0.301\nincluding the relevance feedback approaches by UvA. The results\nare overall encouraging, showing that the proposed QA-based\nprioritization framework potentially applies well to qualitative\nSRs, too. However, a conclusive statement requires more empirical\nstudies, which will be one direction of our future work.\nDiagnostic Test Accuracy (DTA)\nTable 3 shows satisfactory results on DTA. The top-5% ranked\nlist of our best models covered as many as 45% positive samples.\nThey outperformed all IR methods except abs-hh-ratio-ilps by\nUvA, leading to better MAP over the latter, and approached\nthe fine-tuned models in R@5%. This implies the feasibility of\nour approaches for reducing the human effort in annotating an\ninitial training set with a reasonable number of included studies,\ncompared to random sampling, which requires annotating 45% of\ntotal samples to reach the same size of included studies. Although\nour models underperformed the best UvA variant in R@5%, they\nstarted to excel the latter when k ≥ 20, resulting in a better MAP\nand significantly higher WSS .\nOn the other hand, we notice that although some UNIPD and\nSheffield submissions performed better than our best models in\nrecall (when k >10) and WSS (when R = 95 or 100), our methods\nwere consistently stronger than the baselines without relevance\nfeedback (the rows in bold) by large margins. This justifies the\nsuperiority of LLMs as a zero-shot citation screening method. We\nanticipate that the screening performance can be further improved\nthrough an iterative question-answering conversion by providing\nproper feedback to LLMs. Similar ideas have been proven effective\nPage 6 of 14\nhttps://mc.manuscriptcentral.com/jamia\nJournal of the American Medical Informatics Association\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted December 18, 2023. ; https://doi.org/10.1101/2023.12.17.23300102doi: medRxiv preprint \nConfidential: For Review Only\nShort Article Title 7\nTable 2.Results obtained on the Qualitative data. The zero-shot ranking models are emboldened.\nPaper Models L Rel MAP R@5% R@10% R@20% R@30% WSS@95 WSS@100\nUvA abs-hh-ratio-ilps 1796 0.204 0.478 0.655 0.876 0.929 0.417 0.397\nabs-th-ratio-ilps 2564 0.187 0.487 0.628 0.805 0.920 0.398 0.215\nUNIPD 2018 stem original p10 t400.out 2547 0.109 0.496 0.717 0.779 0.894 0.302 0.183\ndistributed effort p10 t1500.out 2544 0.109 0.496 0.743 0.770 0.885 0.268 0.168\n2018 stem original p10 t1000.out 2662 0.109 0.496 0.743 0.770 0.885 0.273 0.141\n2018 stem original p10 t200.out 2934 0.089 0.478 0.522 0.699 0.805 0.216 0.101\n2018 stem original p10 t500.out 2535 0.109 0.496 0.743 0.770 0.894 0.301 0.185\n2018 stem original p10 t300.out 2660 0.103 0.496 0.655 0.752 0.858 0.303 0.159\n2018 stem original p10 t1500.out 2534 0.109 0.496 0.743 0.770 0.885 0.268 0.170\ndistributed effort p10 t1000.out 2469 0.109 0.496 0.743 0.770 0.885 0.295 0.199\n2018 stem original p10 t100.out 2996 0.071 0.327 0.416 0.637 0.796 0.186 0.090\nbaseline bm25 t500.out 2700 0.051 0.274 0.425 0.469 0.611 0.412 0.256\ndistributed effort p10 t300.out 2518 0.109 0.496 0.743 0.770 0.894 0.309 0.193\n2018 stem original p50 t1000.out 2438 0.116 0.496 0.743 0.920 0.947 0.357 0.194\ndistributed effort p10 t100.out 2920 0.083 0.416 0.469 0.681 0.814 0.258 0.106\n2018 stem original p50 t200.out 2934 0.089 0.478 0.522 0.699 0.805 0.216 0.101\nbaseline bm25 t1000.out 3040 0.055 0.274 0.425 0.496 0.788 0.239 0.101\ndistributed effort p10 t500.out 2641 0.109 0.496 0.743 0.770 0.894 0.295 0.162\nbaseline bm25 t300.out 2697 0.049 0.274 0.372 0.451 0.628 0.294 0.257\nbaseline bm25 t100.out 2700 0.056 0.301 0.389 0.637 0.743 0.399 0.256\n2018 stem original p50 t400.out 2566 0.109 0.496 0.717 0.779 0.894 0.293 0.174\n2018 stem original p50 t300.out 2687 0.103 0.496 0.655 0.752 0.858 0.290 0.147\n2018 stem original p50 t100.out 2996 0.071 0.327 0.416 0.637 0.796 0.186 0.090\ndistributed effort p10 t200.out 2762 0.104 0.496 0.673 0.761 0.867 0.303 0.135\nbaseline bm25 t400.out 2700 0.052 0.274 0.434 0.469 0.619 0.417 0.256\n2018 stem original p50 t1500.out 1970 0.116 0.496 0.743 0.920 0.965 0.356 0.301\n2018 stem original p50 t500.out 2576 0.110 0.496 0.743 0.788 0.894 0.283 0.168\nbaseline bm25 t1500.out 3039 0.055 0.274 0.425 0.496 0.779 0.240 0.101\nbaseline bm25 t200.out 2698 0.053 0.274 0.381 0.619 0.726 0.395 0.256\ndistributed effort p10 t400.out 2636 0.109 0.496 0.743 0.770 0.894 0.301 0.165\nSheffield sheffield-relevance feedback.out 2940 0.060 0.274 0.549 0.717 0.832 0.185 0.103\nsheffield-baseline 3031 0.051 0.265 0.451 0.619 0.743 0.135 0.082\nProposed Method GPT Cosine Similarity 2256 0.082 0.173 0.478 0.559 0.618 0.303 0.289\nBM25 2704 0.037 0.078 0.146 0.191 0.259 0.135 0.135\nGPT QA Soft 1786 0.157 0.537 0.614 0.673 0.959 0.599 0.484\nGPT QA Hard 1784 0.110 0.478 0.582 0.900 0.959 0.650 0.485\nGPT QA Soft Abstract Level 1683 0.159 0.509 0.605 0.673 0.959 0.595 0.511\nGPT QA Hard Abstract Level 1675 0.200 0.509 0.609 0.678 0.959 0.608 0.514\nGPT QA Soft Answer Level 1682 0.159 0.505 0.600 0.673 0.959 0.576 0.507\nGPT QA Hard Answer Level 1684 0.157 0.514 0.600 0.678 0.959 0.601 0.507\non different NLP tasks [70, 71, 72]. Meanwhile, it is worth noting\nthe DTA dataset has been generally considered a very difficult\ndataset [73].\nIntervention\nTable 4 shows the results on Intervention. Our methods exhibited\nexceptional performance across all metrics. The high recall\nvalues at different thresholds underscored the effectiveness of\nour proposed model, consistently outperforming all models\nexcept BioBERT-Tune. Our best models, namely the answer-\nlevel re-ranking methods GPT\nQA HARD Answer Level and\nGPT QA Soft Answer Level, also achieved better MAP results\nthan most baselines, and the abstract-level re-ranking method\nGPT\nQA HARD Answer Level rivalled the robust UvA systems.\nNotably, our best models excelled in LRel, and the results of\nWSS @95 and WSS @100 outperformed most baseline models. In\nsummary, the comprehensive assessment across diverse metrics\nand datasets reinforced the standing of our proposed methods as\nstate-of-the-art solutions.\nPage 7 of 14\nhttps://mc.manuscriptcentral.com/jamia\nJournal of the American Medical Informatics Association\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted December 18, 2023. ; https://doi.org/10.1101/2023.12.17.23300102doi: medRxiv preprint \nConfidential: For Review Only\n8 Author Name et al.\nTable 3.Results obtained on the DTA data. The zero-shot ranking models are emboldened.\nPaper Models L Rel MAP R@5% R@10% R@20% R@30% WSS@95 WSS@100\nUvA abs-hh-ratio-ilps 2420 0.493 0.589 0.682 0.789 0.834 0.406 0.304\nabs-th-ratio-ilps 2676 0.399 0.418 0.536 0.661 0.734 0.312 0.253\nUNIPD 2018 stem original p10 t400.out 1190 0.229 0.448 0.634 0.818 0.895 0.662 0.512\ndistributed effort p10 t1500.out 1111 0.229 0.445 0.630 0.814 0.895 0.652 0.513\n2018 stem original p10 t1000.out 1141 0.229 0.445 0.630 0.814 0.893 0.658 0.509\n2018 stem original p10 t200.out 1282 0.229 0.445 0.634 0.823 0.891 0.660 0.507\n2018 stem original p10 t500.out 1200 0.229 0.445 0.634 0.818 0.893 0.662 0.509\n2018 stem original p10 t300.out 1280 0.229 0.452 0.627 0.816 0.893 0.660 0.500\n2018 stem original p10 t1500.out 1126 0.229 0.445 0.630 0.814 0.895 0.657 0.514\ndistributed effort p10 t1000.out 1109 0.229 0.445 0.630 0.814 0.895 0.649 0.514\n2018 stem original p10 t100.out 2024 0.221 0.418 0.609 0.791 0.868 0.525 0.399\nbaseline bm25 t500.out 2470 0.119 0.236 0.402 0.548 0.650 0.390 0.252\ndistributed effort p10 t300.out 1111 0.232 0.445 0.630 0.814 0.886 0.649 0.528\n2018 stem original p50 t1000.out 1127 0.229 0.445 0.630 0.811 0.893 0.652 0.528\ndistributed effort p10 t100.out 1271 0.204 0.439 0.614 0.770 0.839 0.610 0.468\n2018 stem original p50 t200.out 1291 0.229 0.445 0.634 0.820 0.898 0.660 0.499\nbaseline bm25 t1000.out 2395 0.119 0.236 0.389 0.543 0.659 0.396 0.260\ndistributed effort p10 t500.out 1116 0.229 0.445 0.630 0.814 0.891 0.634 0.521\nbaseline bm25 t300.out 2493 0.119 0.239 0.405 0.541 0.652 0.391 0.244\nbaseline bm25 t100.out 2130 0.120 0.239 0.414 0.564 0.659 0.394 0.295\n2018 stem original p50 t400.out 1189 0.229 0.448 0.634 0.816 0.891 0.654 0.527\n2018 stem original p50 t300.out 1272 0.229 0.452 0.627 0.814 0.893 0.656 0.518\n2018 stem original p50 t100.out 2027 0.222 0.418 0.609 0.786 0.868 0.549 0.394\ndistributed effort p10 t200.out 1194 0.225 0.445 0.632 0.811 0.877 0.663 0.509\nbaseline bm25 t400.out 2492 0.119 0.239 0.405 0.539 0.650 0.386 0.246\n2018 stem original p50 t1500.out 1056 0.229 0.445 0.630 0.814 0.898 0.651 0.537\n2018 stem original p50 t500.out 1200 0.229 0.445 0.634 0.809 0.889 0.649 0.524\nbaseline bm25 t1500.out 2476 0.119 0.236 0.389 0.541 0.652 0.364 0.254\nbaseline bm25 t200.out 2253 0.120 0.234 0.405 0.550 0.652 0.409 0.278\ndistributed effort p10 t400.out 1116 0.231 0.445 0.630 0.814 0.886 0.634 0.528\nSheffield sheffield-Log likelihood.out 1964 0.222 0.305 0.450 0.641 0.730 0.475 0.375\nsheffield-Odds Ratio.out 2250 0.175 0.220 0.336 0.525 0.675 0.451 0.338\nsheffield-baseline.out 2184 0.248 0.382 0.561 0.707 0.805 0.490 0.347\nsheffield-Chi Squared.out 1972 0.234 0.350 0.527 0.668 0.759 0.487 0.381\nWang et al. BM25 2723 0.119 0.213 0.329 0.528 0.314 0.208\nBERT 2514 0.092 0.132 0.238 0.391 0.258 0.210\nBERT-M 3234 0.096 0.079 0.198 0.379 0.263 0.123\nBioBERT 3264 0.081 0.129 0.229 0.337 0.137 0.095\nBlueBERT 3771 0.069 0.026 0.053 0.105 0.023 0.016\nPubMedBERT 3331 0.104 0.123 0.214 0.312 0.202 0.098\nBERT-Tuned 1400 0.223 0.439 0.601 0.762 0.587 0.460\nBERT-M-Tuned 1178 0.254 0.447 0.590 0.754 0.615 0.500\nBioBERT-Tuned 853 0.318 0.500 0.671 0.817 0.686 0.585\nProposed Method GPT Cosine Similarity 1154 0.271 0.477 0.628 0.782 0.851 0.600 0.513\nBM25 2461 0.164 0.334 0.472 0.654 0.723 0.351 0.233\nGPT QA Soft 1979 0.255 0.319 0.495 0.674 0.765 0.408 0.334\nGPT QA Hard 1983 0.228 0.367 0.468 0.673 0.776 0.364 0.303\nGPT QA Soft Abstract Level 1491 0.301 0.384 0.574 0.705 0.810 0.473 0.422\nGPT QA Hard Abstract Level 1583 0.310 0.387 0.573 0.727 0.820 0.454 0.396\nGPT QA Soft Answer Level 1136 0.315 0.438 0.593 0.766 0.858 0.556 0.506\nGPT QA Hard Answer Level 1176 0.322 0.450 0.595 0.791 0.873 0.536 0.491\nPage 8 of 14\nhttps://mc.manuscriptcentral.com/jamia\nJournal of the American Medical Informatics Association\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted December 18, 2023. ; https://doi.org/10.1101/2023.12.17.23300102doi: medRxiv preprint \nConfidential: For Review Only\nShort Article Title 9\nTable 4.Results obtained on the Intervention data. The zero-shot ranking models are emboldened.\nPaper Models L Rel MAP R@5% R@10% R@20% R@30% WSS@95 WSS@100\nUvA abs-hh-ratio-ilps 958 0.567 0.518 0.628 0.736 0.813 0.526 0.480\nabs-th-ratio-ilps 986 0.556 0.478 0.576 0.692 0.774 0.535 0.450\nUNIPD 2018 stem original p10 t400.out 985 0.280 0.307 0.502 0.663 0.744 0.632 0.511\ndistributed effort p10 t1500.out 981 0.280 0.306 0.499 0.664 0.745 0.633 0.517\n2018 stem original p10 t1000.out 977 0.280 0.306 0.499 0.664 0.745 0.630 0.510\n2018 stem original p10 t200.out 1180 0.280 0.312 0.501 0.671 0.775 0.617 0.488\n2018 stem original p10 t500.out 975 0.280 0.306 0.502 0.662 0.742 0.630 0.514\n2018 stem original p10 t300.out 1141 0.280 0.313 0.496 0.665 0.771 0.617 0.494\n2018 stem original p10 t1500.out 952 0.280 0.306 0.499 0.664 0.745 0.630 0.522\ndistributed effort p10 t1000.out 992 0.279 0.306 0.499 0.664 0.745 0.620 0.492\n2018 stem original p10 t100.out 1153 0.274 0.306 0.483 0.639 0.737 0.540 0.474\nbaseline bm25 t500.out 1233 0.222 0.191 0.282 0.410 0.515 0.435 0.394\ndistributed effort p10 t300.out 974 0.276 0.306 0.499 0.664 0.733 0.592 0.481\n2018 stem original p50 t1000.out 836 0.290 0.306 0.498 0.688 0.795 0.643 0.542\ndistributed effort p10 t100.out 1114 0.248 0.315 0.444 0.604 0.704 0.458 0.372\n2018 stem original p50 t200.out 1185 0.290 0.312 0.499 0.693 0.792 0.630 0.481\nbaseline bm25 t1000.out 1241 0.222 0.191 0.282 0.408 0.524 0.446 0.392\ndistributed effort p10 t500.out 991 0.278 0.306 0.499 0.664 0.743 0.606 0.483\nbaseline bm25 t300.out 1262 0.222 0.187 0.286 0.410 0.523 0.440 0.398\nbaseline bm25 t100.out 1397 0.223 0.186 0.291 0.429 0.557 0.414 0.368\n2018 stem original p50 t400.out 985 0.290 0.307 0.501 0.685 0.767 0.646 0.514\n2018 stem original p50 t300.out 1144 0.290 0.313 0.495 0.682 0.788 0.639 0.497\n2018 stem original p50 t100.out 1150 0.284 0.306 0.483 0.653 0.752 0.556 0.481\ndistributed effort p10 t200.out 965 0.271 0.306 0.482 0.651 0.752 0.560 0.445\nbaseline bm25 t400.out 1242 0.222 0.191 0.286 0.412 0.523 0.434 0.393\n2018 stem original p50 t1500.out 796 0.290 0.306 0.498 0.688 0.785 0.642 0.553\n2018 stem original p50 t500.out 1001 0.290 0.306 0.501 0.691 0.779 0.650 0.505\nbaseline bm25 t1500.out 1203 0.222 0.191 0.282 0.411 0.533 0.453 0.399\nbaseline bm25 t200.out 1263 0.222 0.189 0.284 0.417 0.535 0.438 0.396\ndistributed effort p10 t400.out 981 0.277 0.306 0.499 0.663 0.734 0.595 0.483\nSheffield sheffield-Log likelihood.out 1132 0.293 0.258 0.378 0.583 0.695 0.458 0.381\nSheffield-Odds Ratio.out 1070 0.261 0.267 0.404 0.569 0.700 0.462 0.384\nSheffield-baseline.out 1276 0.245 0.220 0.334 0.507 0.653 0.470 0.386\nsheffield-Chi Squared.out 1149 0.262 0.238 0.360 0.537 0.687 0.469 0.415\nWang et al. BM25 1716 0.211 0.305 0.399 0.554 0.351 0.296\nBERT 1399 0.160 0.211 0.328 0.504 0.362 0.333\nBERT-M 1837 0.177 0.195 0.355 0.527 0.323 0.266\nBioBERT 1833 0.146 0.135 0.198 0.307 0.159 0.163\nBlueBERT 2057 0.046 0.028 0.051 0.107 0.008 0.036\nPubMedBERT 1975 0.078 0.050 0.091 0.275 0.121 0.094\nBERT-Tuned 1375 0.281 0.374 0.527 0.659 0.363 0.301\nBERT-M-Tuned 1572 0.334 0.402 0.565 0.706 0.446 0.362\nBioBERT-Tuned 707 0.456 0.581 0.737 0.842 0.646 0.579\nProposed Method GPT Cosine Similarity 920 0.315 0.401 0.544 0.722 0.797 0.552 0.499\nBM25 1545 0.146 0.191 0.300 0.497 0.667 0.270 0.238\nGPT QA Soft 1055 0.411 0.469 0.610 0.738 0.856 0.486 0.416\nGPT QA Hard 1159 0.356 0.444 0.578 0.759 0.847 0.466 0.397\nGPT QA Soft Abstract Level 934 0.440 0.494 0.663 0.800 0.873 0.534 0.459\nGPT QA Hard Abstract Level 976 0.443 0.505 0.687 0.777 0.856 0.532 0.458\nGPT QA Soft Answer Level 801 0.450 0.526 0.697 0.816 0.881 0.600 0.526\nGPT QA Hard Answer Level 806 0.447 0.527 0.697 0.808 0.869 0.592 0.519\nPage 9 of 14\nhttps://mc.manuscriptcentral.com/jamia\nJournal of the American Medical Informatics Association\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted December 18, 2023. ; https://doi.org/10.1101/2023.12.17.23300102doi: medRxiv preprint \nConfidential: For Review Only\n10 Author Name et al.\nDiscussion\nSelection Criteria\nTo further evaluate the usefulness of selection criteria, we\nimplemented our own BM25 baseline using selection criteria as\na query, and we observed competitive performances. Particularly\nit significantly outperformed the BM25 baselines of UNIPD and\nWang et al. on the DTA dataset. On Intervention, it was at least\non par with or slightly better than most other BM25 baselines\nexcept Wang et al. The result underscored the validity of using\nselection criteria to guide citation screening.\nQuestion Generation and Answering\nWe manually checked the question qualities and found notable\nstrengths and occasional challenges of ChatGPT in question\ngeneration. Figure 4 illustrates how ChatGPT was smart enough\nto translate a lengthy exclusion criterion into two relevant\nquestions, Q4 and Q5, demonstrating its nuanced understanding\nof the complex semantics of the sentence. The questions were\neffectively formatted so that a POSITIVE response consistently\nsignifies compliance with a selection criterion, be it inclusion or\nexclusion.\nOccasionally, ChatGPT failed to generate completely\nindependent questions. This led to redundant or overlapped\nquestions, introducing biases in combining answer scores.\nOccasionally, ChatGPT struggled to address the “OR” clause\nin a long selection criterion sentence. It was split into separate\nquestions, which was problematic. In such cases, matching one\nquestion should give a POSITIVE score, but the NEGATIVE\nanswers to other questions generated from the OR clause might\nunderestimate the final score. These issues imply areas of\nimprovement in ensuring robust question generation and precise\nanswer interpretation for citation screening. We postulate that a\nviable solution is to train a good question generator and analyzer\nto tackle these issues. Alternatively, it is sensible for human\nreviewers to scrutinize and correct the generated questions before\nsending them to LLMs to answer.\nWhile the current paper deliberately limited answers to\na simple form, it is worthwhile to consider incorporating\nexplanations of LLM-generated answers in future iterations.\nProviding insight into ChatGPT’s reasoning process can enhance\ntransparency and facilitate a better understanding of the model’s\ndecision-making which is essential for instilling user confidence\nin the outputs of automated citation screening to encourage\ntechnology acceptance [74, 75]. In addition, it contributes to\nrefining model performance through iterative conversation with\nLLMs by giving user feedback on model answers and their\nexplanations [70].\nAnswer Re-Ranking\nTaking a holistic view, i.e., averaging model performances over all\nfour categories of datasets, the answer-level re-ranking methods\nconsistently outperformed our other models across all metrics.\nThis superiority is attributed to the additional granularity gained\nby considering the alignment of each question with the abstracts\nof candidate studies. Compared with other zero-shot models,\nour methods achieved substantial improvements, showcasing both\nthe effectiveness of the proposed questions-answering framework\nand the utility of ChatGPT as a zero-shot ranker for automated\ncitation screening.\nWhen pitted against trained models employing relevant\nfeedback or fine-tuning, our methods still held a solid\nground competitively. Our best models achieved very promising\nperformances in LRel, R@5%, and WSS . This is a useful merit\nas our zero-shot method fits well into the real-world citation\nscreening task, starting with no annotation of included/excluded\nstudies. Although the UvA variants and BioBERT-Tuned models\noften resulted in better performances in MAP and occasionally in\nrecall at different thresholds, our models were still demonstrated\nto be competitive, highlighting their brilliance requiring no prior\ntraining. Therefore, our models are better generalizable to all\nSR categories, especially when lacking comprehensive datasets\nfor relevance feedback or fine-tuning. Nevertheless, it is always\nvaluable to fine-tune LLMs for each SR topic to benefit our\nproposed answer re-ranking methods.\nConclusion\nThis paper proposed an effective LLM-assisted question-answering\nframework to facilitate citation screening and advance automated\nsystematic review. Extensive experiments emphasized the\nparticular pertinence of selection criteria of included studies\nto automated citation screening and ChatGPT’s proficiency\nin understanding and utilizing selection criteria to prioritize\ncandidate studies. Specifically, ChatGPT was able to correctly\ncapture and handle complex semantics like several juxtaposed\ncriteria with a logical OR relationship, significantly outperforming\nother zero-shot baselines. The positive results of LRel (position\nof the last relevant study), R@5% (recall at top 5%), R@10%,\nWSS @95 (Workload Saved over Sampling at 95% recall level),\nand WSS @100 not only showed the competency of the proposed\nframework as a zero-shot citation screening methodology but also\nindicated its potential use in reducing human effort in building a\nhigh-quality dataset for training a citation screener.\nReferences\n1. Guy Tsafnat, Paul Glasziou, Miew Keen Choong, Adam\nDunn, Filippo Galgani, and Enrico Coiera. Systematic review\nautomation technologies. Systematic reviews, 3:1–15, 2014.\n2. S Gopalakrishnan and P Ganeshkumar. Systematic reviews\nand meta-analysis: understanding the best evidence in primary\nhealthcare. Journal of family medicine and primary care ,\n2(1):9, 2013.\n3. Hamideh Moosapour, Farzane Saeidifard, Maryam Aalaa,\nAkbar Soltani, and Bagher Larijani. The rationale\nbehind systematic reviews in clinical medicine: a conceptual\nframework. Journal of Diabetes & Metabolic Disorders,\n20:919–929, 2021.\n4. Ian Shemilt, Nada Khan, Sophie Park, and James Thomas.\nUse of cost-effectiveness analysis to compare the efficiency of\nstudy identification methods in systematic reviews. Systematic\nreviews, 5:1–13, 2016.\n5. Matthew Michelson and Katja Reuter. The significant cost\nof systematic reviews and meta-analyses: a call for greater\ninvolvement of machine learning to assess the promise of\nclinical trials. Contemporary clinical trials communications ,\n16:100443, 2019.\n6. Julian PT Higgins, Sally Green, et al. Cochrane handbook for\nsystematic reviews of interventions. 2008.\nPage 10 of 14\nhttps://mc.manuscriptcentral.com/jamia\nJournal of the American Medical Informatics Association\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted December 18, 2023. ; https://doi.org/10.1101/2023.12.17.23300102doi: medRxiv preprint \nConfidential: For Review Only\nShort Article Title 11\n7. Alison O’Mara-Eves, James Thomas, John McNaught, Makoto\nMiwa, and Sophia Ananiadou. Using text mining for study\nidentification in systematic reviews: a systematic review of\ncurrent approaches. Systematic reviews, 4(1):1–22, 2015.\n8. Raymon van Dinter, Bedir Tekinerdogan, and Cagatay Catal.\nAutomation of systematic literature reviews: A systematic\nliterature review. Information and Software Technology ,\n136:106589, 2021.\n9. Amal Alharbi, William Briggs, and Mark Stevenson.\nRetrieving and ranking studies for systematic reviews:\nUniversity of sheffield’s approach to clef ehealth 2018 task 2. In\nCEUR workshop proceedings, volume 2125. CEUR Workshop\nProceedings, 2018.\n10. Amal Alharbi and Mark Stevenson. Ranking abstracts\nto identify relevant evidence for systematic reviews: The\nuniversity of sheffield’s approach to clef ehealth 2017 task 2.\nIn Clef (working notes), 2017.\n11. Gordon V Cormack and Maura R Grossman. Technology-\nassisted review in empirical medicine: Waterloo participation\nin clef ehealth 2017. CLEF (working notes), 11, 2017.\n12. Gordon V Cormack and Maura R Grossman. Systems and\nmethods for conducting a highly autonomous technology-\nassisted review classification, March 12 2019. US Patent\n10,229,117.\n13. Maura R Grossman and Gordon V Cormack. Technology-\nassisted review in e-discovery can be more effective and more\nefficient than exhaustive manual review. Richmond Journal of\nLaw & Technology, 17(3):11, 2011.\n14. Maura R Grossman, Gordon V Cormack, and Adam\nRoegiest. Automatic and semi-automatic document selection\nfor technology-assisted review. In Proceedings of the 40th\nInternational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval , pages 905–908, 2017.\n15. Grace E Lee and Aixin Sun. Seed-driven document ranking\nfor systematic reviews in evidence-based medicine. In The\n41st international ACM SIGIR conference on research &\ndevelopment in information retrieval , pages 455–464, 2018.\n16. Harrisen Scells, Guido Zuccon, Anthony Deacon, and Bevan\nKoopman. Qut ielab at clef ehealth 2017 technology assisted\nreviews track: initial experiments with learning to rank. In\nWorking Notes of CLEF 2017-Conference and Labs of the\nEvaluation Forum [CEUR Workshop Proceedings, Volume\n1866], pages 1–6. Sun SITE Central Europe, 2017.\n17. Amal Alharbi and Mark Stevenson. Ranking studies for\nsystematic reviews using query adaptation: University of\nsheffield’s approach to clef ehealth 2019 task 2 working notes\nfor clef 2019. In Working Notes of CLEF 2019-Conference and\nLabs of the Evaluation Forum, volume 2380. CEUR Workshop\nProceedings, 2019.\n18. Harrisen Scells, Guido Zuccon, Bevan Koopman, Anthony\nDeacon, Leif Azzopardi, and Shlomo Geva. Integrating the\nframing of clinical questions via pico into the retrieval of\nmedical literature for systematic reviews. In Proceedings of\nthe 2017 ACM on Conference on Information and Knowledge\nManagement, pages 2291–2294, 2017.\n19. Shuai Wang, Harrisen Scells, Ahmed Mourad, and Guido\nZuccon. Seed-driven document ranking for systematic\nreviews: A reproducibility study. In European Conference on\nInformation Retrieval, pages 686–700. Springer, 2022.\n20. Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru\nWu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler,\nXuanhui Wang, et al. Large language models are effective\ntext rankers with pairwise ranking prompting. arXiv preprint\narXiv:2306.17563, 2023.\n21. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing\nXie, Julian McAuley, and Wayne Xin Zhao. Large language\nmodels are zero-shot rankers for recommender systems. arXiv\npreprint arXiv:2305.08845, 2023.\n22. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information\nprocessing systems, 33:1877–1901, 2020.\n23. Oana Frunza, Diana Inkpen, Stan Matwin, William Klement,\nand Peter O’blenis. Exploiting the systematic review protocol\nfor classification of medical abstracts. Artificial intelligence in\nmedicine, 51(1):17–25, 2011.\n24. Kentaro Matsui, Tomohiro Utsumi, Yumi Aoki, Taku Maruki,\nMasahiro Takeshima, and Takaesu Yoshikazu. Large language\nmodel demonstrates human-comparable sensitivity in initial\nscreening of systematic reviews: A semi-automated strategy\nusing gpt-3.5. Available at SSRN 4520426.\n25. Shuai Wang, Harrisen Scells, Bevan Koopman, and Guido\nZuccon. Neural rankers for effective screening prioritisation\nin medical systematic review literature search. In Proceedings\nof the 26th Australasian Document Computing Symposium ,\npages 1–10, 2022.\n26. Barbara Kitchenham and Pearl Brereton. A systematic review\nof systematic review process research in software engineering.\nInformation and software technology, 55(12):2049–2075, 2013.\n27. Aaron M Cohen, William R Hersh, Kim Peterson, and Po-\nYin Yen. Reducing workload in systematic review preparation\nusing automated citation classification. Journal of the\nAmerican Medical Informatics Association , 13(2):206–219,\n2006.\n28. Iain J Marshall and Byron C Wallace. Toward systematic\nreview automation: a practical guide to using machine learning\ntools in research synthesis. Systematic reviews, 8:1–10, 2019.\n29. Iain J Marshall, Jo¨ el Kuiper, Edward Banner, and\nByron C Wallace. Automating biomedical evidence synthesis:\nRobotreviewer. In Proceedings of the conference. Association\nfor Computational Linguistics. Meeting , volume 2017, page 7.\nNIH Public Access, 2017.\n30. Iain J Marshall, Jo¨ el Kuiper, and Byron C Wallace.\nRobotreviewer: evaluation of a system for automatically\nassessing bias in clinical trials. Journal of the American\nMedical Informatics Association, 23(1):193–201, 2016.\n31. Iain J Marshall, Benjamin Nye, Jo¨ el Kuiper, Anna Noel-\nStorr, Rachel Marshall, Rory Maclean, Frank Soboczenski, Ani\nNenkova, James Thomas, and Byron C Wallace. Trialstreamer:\nA living, automatically updated database of clinical trial\nreports. Journal of the American Medical Informatics\nAssociation, 27(12):1903–1912, 2020.\n32. Carlos Francisco Moreno-Garcia, Chrisina Jayne, Eyad Elyan,\nand Magaly Aceves-Martins. A novel application of machine\nlearning and zero-shot classification methods for automated\nabstract screening in systematic reviews. Decision Analytics\nJournal, page 100162, 2023.\n33. Mourad Ouzzani, Hossam Hammady, Zbys Fedorowicz, and\nAhmed Elmagarmid. Rayyan—a web and mobile app for\nsystematic reviews. Systematic reviews, 5:1–10, 2016.\nPage 11 of 14\nhttps://mc.manuscriptcentral.com/jamia\nJournal of the American Medical Informatics Association\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted December 18, 2023. ; https://doi.org/10.1101/2023.12.17.23300102doi: medRxiv preprint \nConfidential: For Review Only\n12 Author Name et al.\n34. Tanja Bekhuis and Dina Demner-Fushman. Screening\nnonrandomized studies for medical systematic reviews: a\ncomparative study of classifiers. Artificial intelligence in\nmedicine, 55(3):197–207, 2012.\n35. Ian Shemilt, Antonia Simon, Gareth J Hollands, Theresa M\nMarteau, David Ogilvie, Alison O’Mara-Eves, Michael P Kelly,\nand James Thomas. Pinpointing needles in giant haystacks:\nuse of text mining to reduce impractical screening workload in\nextremely large scoping reviews. Research Synthesis Methods,\n5(1):31–49, 2014.\n36. Byron C Wallace, Thomas A Trikalinos, Joseph Lau, Carla\nBrodley, and Christopher H Schmid. Semi-automated\nscreening of biomedical citations for systematic reviews. BMC\nbioinformatics, 11(1):1–11, 2010.\n37. Stan Matwin, Alexandre Kouznetsov, Diana Inkpen, Oana\nFrunza, and Peter O’Blenis. A new algorithm for reducing\nthe workload of experts in performing systematic reviews.\nJournal of the American Medical Informatics Association ,\n17(4):446–453, 2010.\n38. Byron C Wallace, Kevin Small, Carla E Brodley, Joseph\nLau, and Thomas A Trikalinos. Deploying an interactive\nmachine learning system in an evidence-based practice center:\nabstrackr. In Proceedings of the 2nd ACM SIGHIT\ninternational health informatics symposium , pages 819–824,\n2012.\n39. Georgios Kontonatsios, Sally Spencer, Peter Matthew,\nand Ioannis Korkontzelos. Using a neural network-based\nfeature extraction method to facilitate citation screening for\nsystematic reviews. Expert Systems with Applications: X ,\n6:100030, 2020.\n40. Raymon van Dinter, Cagatay Catal, and Bedir Tekinerdogan.\nA decision support system for automating document retrieval\nand citation screening. Expert Systems with Applications ,\n182:115261, 2021.\n41. Xiaonan Ji, Alan Ritter, and Po-Yin Yen. Using ontology-\nbased semantic similarity to facilitate the article screening\nprocess for systematic reviews. Journal of biomedical\ninformatics, 69:33–42, 2017.\n42. David Martinez, Sarvnaz Karimi, Lawrence Cavedon, and\nTimothy Baldwin. Facilitating biomedical systematic reviews\nusing ranked text retrieval and classification. In Australasian\ndocument computing symposium (adcs) , pages 53–60, 2008.\n43. James Thomas and Alison O’Mara-Eves. How can we\nfind relevant research more quickly? NCRM Newsletter:\nMethodsNews, 2011.\n44. Aaron M Cohen, Kyle Ambert, and Marian McDonagh. Cross-\ntopic learning for work prioritization in systematic review\ncreation and update. Journal of the American Medical\nInformatics Association, 16(5):690–704, 2009.\n45. Aaron M Cohen, Kyle Ambert, and Marian McDonagh.\nStudying the potential impact of automated document\nclassification on scheduling a systematic review update. BMC\nmedical informatics and decision making , 12:1–11, 2012.\n46. Byron C Wallace, Kevin Small, Carla E Brodley, Joseph\nLau, Christopher H Schmid, Lars Bertram, Christina M\nLill, Joshua T Cohen, and Thomas A Trikalinos. Toward\nmodernizing the systematic review pipeline in genetics:\nefficient updating via data mining. Genetics in medicine ,\n14(7):663–669, 2012.\n47. Rens Van De Schoot, Jonathan De Bruin, Raoul Schram,\nParisa Zahedi, Jan De Boer, Felix Weijdema, Bianca Kramer,\nMartijn Huijts, Maarten Hoogerwerf, Gerbrich Ferdinands,\net al. An open source machine learning framework for\nefficient and transparent systematic reviews. Nature machine\nintelligence, 3(2):125–133, 2021.\n48. Murray Shanahan. Talking about large language models.\narXiv preprint arXiv:2212.03551, 2022.\n49. Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing\nWang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He,\net al. A comprehensive survey on pretrained foundation\nmodels: A history from bert to chatgpt. arXiv preprint\narXiv:2302.09419, 2023.\n50. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki\nHayashi, and Graham Neubig. Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language\nprocessing. ACM Computing Surveys, 55(9):1–35, 2023.\n51. Shuai Wang, Harrisen Scells, Bevan Koopman, and Guido\nZuccon. Can chatgpt write a good boolean query\nfor systematic review literature search? arXiv preprint\narXiv:2302.03495, 2023.\n52. Ahmad Alshami, Moustafa Elsayed, Eslam Ali,\nAbdelrahman EE Eltoukhy, and Tarek Zayed. Harnessing the\npower of chatgpt for automating systematic review process:\nMethodology, case study, limitations, and future directions.\nSystems, 11(7):351, 2023.\n53. Eugene Syriani, Istvan David, and Gauransh Kumar.\nAssessing the ability of chatgpt to screen articles for systematic\nreviews. arXiv preprint arXiv:2307.06464, 2023.\n54. Eddie Guo, Mehul Gupta, Jiawen Deng, Ye-Jean Park, Mike\nPaget, and Christopher Naugler. Automated paper screening\nfor clinical reviews using large language models.arXiv preprint\narXiv:2305.00844, 2023.\n55. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka\nMatsuo, and Yusuke Iwasawa. Large language models are zero-\nshot reasoners. Advances in neural information processing\nsystems, 35:22199–22213, 2022.\n56. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language\nmodels to follow instructions with human feedback. Advances\nin Neural Information Processing Systems , 35:27730–27744,\n2022.\n57. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi,\nJason Wei, Hyung Won Chung, Nathan Scales, Ajay\nTanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large\nlanguage models encode clinical knowledge. arXiv preprint\narXiv:2212.13138, 2022.\n58. Aidan Gilson, Conrad W Safranek, Thomas Huang, Vimig\nSocrates, Ling Chi, Richard Andrew Taylor, David Chartash,\net al. How does chatgpt perform on the united states medical\nlicensing examination? the implications of large language\nmodels for medical education and knowledge assessment.\nJMIR Medical Education, 9(1):e45312, 2023.\n59. Ruqing Zhang, Jiafeng Guo, Lu Chen, Yixing Fan, and Xueqi\nCheng. A review on question generation from natural language\ntext. ACM Transactions on Information Systems (TOIS) ,\n40(1):1–43, 2021.\n60. Yang Deng, Wenxuan Zhang, Qian Yu, and Wai Lam. Product\nquestion answering in e-commerce: A survey. arXiv preprint\narXiv:2302.08092, 2023.\n61. Xiangjue Dong, Jiaying Lu, Jianling Wang, and James\nCaverlee. Closed-book question generation via contrastive\nPage 12 of 14\nhttps://mc.manuscriptcentral.com/jamia\nJournal of the American Medical Informatics Association\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted December 18, 2023. ; https://doi.org/10.1101/2023.12.17.23300102doi: medRxiv preprint \nConfidential: For Review Only\nShort Article Title 13\nlearning. arXiv preprint arXiv:2210.06781, 2022.\n62. Nehal Muthukumar. Few-shot learning text classification\nin federated environments. In 2021 Smart Technologies,\nCommunication and Robotics (STCR), pages 1–3. IEEE, 2021.\n63. Ryan Greene, Ted Sanders, Lilian Weng, and Arvind\nNeelakantan, Dec 2022.\n64. Giorgio Maria Di Nunzio and Evangelos Kanoulas. Special\nissue on technology assisted review systems, 2023.\n65. Alessio Molinari and Evangelos Kanoulas. Transferring\nknowledge between topics in systematic reviews. Intelligent\nSystems with Applications, 16:200150, 2022.\n66. Evangelos Kanoulas, Dan Li, Leif Azzopardi, and Rene Spijker.\nClef 2019 technology assisted reviews in empirical medicine\noverview. In CEUR workshop proceedings, volume 2380, page\n250, 2019.\n67. Dan Li and Evangelos Kanoulas. Automatic thresholding by\nsampling documents and estimating recall. In CLEF (Working\nNotes), 2019.\n68. Giorgio Maria Di Nunzio. A distributed effort approach for\nsystematic reviews. ims unipd at clef 2019 ehealth task 2. In\nClef (working notes), 2019.\n69. Stephen Robertson, Hugo Zaragoza, et al. The probabilistic\nrelevance framework: Bm25 and beyond. Foundations and\nTrends® in Information Retrieval , 3(4):333–389, 2009.\n70. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\nDziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-\nrefine: Iterative refinement with self-feedback. arXiv preprint\narXiv:2303.17651, 2023.\n71. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang,\nDaisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel\nSmith, Yian Yin, et al. Can large language models provide\nuseful feedback on research papers? a large-scale empirical\nanalysis. arXiv preprint arXiv:2310.01783, 2023.\n72. Haodi Zhang, Min Cai, Xinhe Zhang, Chen Jason Zhang,\nRui Mao, and Kaishun Wu. Self-convinced prompting: Few-\nshot question answering with repeated introspection. arXiv\npreprint arXiv:2310.05035, 2023.\n73. Mariska MG Leeflang, Jonathan J Deeks, Yemisi Takwoingi,\nand Petra Macaskill. Cochrane diagnostic test accuracy\nreviews. Systematic reviews, 2(1):1–6, 2013.\n74. Annette M O’Connor, Guy Tsafnat, James Thomas, Paul\nGlasziou, Stephen B Gilbert, and Brian Hutton. A question of\ntrust: can we build an evidence base to gain trust in systematic\nreview automation technologies? Systematic reviews, 8(1):1–8,\n2019.\n75. Xiaorui Jiang. Trustworthiness of systematic review\nautomation: An interview at coventry university. 2022.\nPage 13 of 14\nhttps://mc.manuscriptcentral.com/jamia\nJournal of the American Medical Informatics Association\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted December 18, 2023. ; https://doi.org/10.1101/2023.12.17.23300102doi: medRxiv preprint ",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.8091695308685303
    },
    {
      "name": "Computer science",
      "score": 0.6229656934738159
    },
    {
      "name": "Natural language processing",
      "score": 0.5198251008987427
    },
    {
      "name": "Artificial intelligence",
      "score": 0.446713924407959
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I73417466",
      "name": "Coventry University",
      "country": "GB"
    }
  ],
  "cited_by": 4
}