{
  "title": "Prompt engineering on leveraging large language models in generating response to InBasket messages",
  "url": "https://openalex.org/W4400863680",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2112330742",
      "name": "Sherry Yan",
      "affiliations": [
        "Sutter Health"
      ]
    },
    {
      "id": "https://openalex.org/A5005625570",
      "name": "Wendi Knapp",
      "affiliations": [
        "Sutter Health"
      ]
    },
    {
      "id": "https://openalex.org/A2641807116",
      "name": "Andrew Leong",
      "affiliations": [
        "Sutter Health"
      ]
    },
    {
      "id": "https://openalex.org/A5104906413",
      "name": "Sarira Kadkhodazadeh",
      "affiliations": [
        "Sutter Health"
      ]
    },
    {
      "id": "https://openalex.org/A2109705964",
      "name": "Souvik Das",
      "affiliations": [
        "Sutter Health"
      ]
    },
    {
      "id": "https://openalex.org/A2995731936",
      "name": "Veena G. Jones",
      "affiliations": [
        "Sutter Health"
      ]
    },
    {
      "id": "https://openalex.org/A2096087526",
      "name": "Robert Clark",
      "affiliations": [
        "Sutter Health"
      ]
    },
    {
      "id": "https://openalex.org/A5104804105",
      "name": "David Grattendick",
      "affiliations": [
        "Sutter Health"
      ]
    },
    {
      "id": "https://openalex.org/A2098511198",
      "name": "Kevin Chen",
      "affiliations": [
        "Sutter Health"
      ]
    },
    {
      "id": "https://openalex.org/A5104881116",
      "name": "Lisa Hladik",
      "affiliations": [
        "Sutter Health"
      ]
    },
    {
      "id": null,
      "name": "Lawrence Fagan",
      "affiliations": [
        "Sutter Health"
      ]
    },
    {
      "id": "https://openalex.org/A2103554915",
      "name": "Albert Chan",
      "affiliations": [
        "Sutter Health"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3205353327",
    "https://openalex.org/W2954484941",
    "https://openalex.org/W2605217566",
    "https://openalex.org/W2624756857",
    "https://openalex.org/W4366439799",
    "https://openalex.org/W4382930233",
    "https://openalex.org/W4386910356",
    "https://openalex.org/W4389173934",
    "https://openalex.org/W4391755461",
    "https://openalex.org/W4384154918",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4385620388",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4392986561",
    "https://openalex.org/W1969776198",
    "https://openalex.org/W2996550193",
    "https://openalex.org/W4392193191",
    "https://openalex.org/W4293457270",
    "https://openalex.org/W4254431624",
    "https://openalex.org/W4392669801"
  ],
  "abstract": "Abstract Objectives Large Language Models (LLMs) have been proposed as a solution to address high volumes of Patient Medical Advice Requests (PMARs). This study addresses whether LLMs can generate high quality draft responses to PMARs that satisfies both patients and clinicians with prompt engineering. Materials and Methods We designed a novel human-involved iterative processes to train and validate prompts to LLM in creating appropriate responses to PMARs. GPT-4 was used to generate response to the messages. We updated the prompts, and evaluated both clinician and patient acceptance of LLM-generated draft responses at each iteration, and tested the optimized prompt on independent validation data sets. The optimized prompt was implemented in the electronic health record production environment and tested by 69 primary care clinicians. Results After 3 iterations of prompt engineering, physician acceptance of draft suitability increased from 62% to 84% (P &amp;lt;.001) in the validation dataset (N = 200), and 74% of drafts in the test dataset were rated as “helpful.” Patients also noted significantly increased favorability of message tone (78%) and overall quality (80%) for the optimized prompt compared to the original prompt in the training dataset, patients were unable to differentiate human and LLM-generated draft PMAR responses for 76% of the messages, in contrast to the earlier preference for human-generated responses. Majority (72%) of clinicians believed it can reduce cognitive load in dealing with InBasket messages. Discussion and Conclusion Informed by clinician and patient feedback synergistically, tuning in LLM prompt alone can be effective in creating clinically relevant and useful draft responses to PMARs.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.60169917345047
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5727453827857971
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114073160",
      "name": "Sutter Health",
      "country": "US"
    }
  ]
}