{
  "title": "Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study",
  "url": "https://openalex.org/W3167361718",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4303664415",
      "name": "Nadkarni, Rahul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3112977030",
      "name": "Wadden David",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4200857852",
      "name": "Beltagy, Iz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221392677",
      "name": "Smith, Noah A.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4200857851",
      "name": "Hajishirzi, Hannaneh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225821010",
      "name": "Hope, Tom",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3137864305",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W3037860573",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W2909137510",
    "https://openalex.org/W3132689447",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W3131901903",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2571811098",
    "https://openalex.org/W2499696929",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W3102476541",
    "https://openalex.org/W3117339789",
    "https://openalex.org/W2921377602",
    "https://openalex.org/W3154735894",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W2187413858",
    "https://openalex.org/W3155001903",
    "https://openalex.org/W3005970824",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2995448904",
    "https://openalex.org/W3153206982",
    "https://openalex.org/W2612436700",
    "https://openalex.org/W2963432357",
    "https://openalex.org/W3165146808",
    "https://openalex.org/W3106706183",
    "https://openalex.org/W2250635077",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2011726136",
    "https://openalex.org/W2127795553"
  ],
  "abstract": "Biomedical knowledge graphs (KGs) hold rich information on entities such as diseases, drugs, and genes. Predicting missing links in these graphs can boost many important applications, such as drug design and repurposing. Recent work has shown that general-domain language models (LMs) can serve as \"soft\" KGs, and that they can be fine-tuned for the task of KG completion. In this work, we study scientific LMs for KG completion, exploring whether we can tap into their latent knowledge to enhance biomedical link prediction. We evaluate several domain-specific LMs, fine-tuning them on datasets centered on drugs and diseases that we represent as KGs and enrich with textual entity descriptions. We integrate the LM-based models with KG embedding models, using a router method that learns to assign each input example to either type of model and provides a substantial boost in performance. Finally, we demonstrate the advantage of LM models in the inductive setting with novel scientific entities. Our datasets and code are made publicly available.",
  "full_text": "Automated Knowledge Base Construction (2020) Conference paper\nScientiÔ¨Åc Language Models for Biomedical Knowledge Base\nCompletion: An Empirical Study\nRahul Nadkarni1 rahuln@cs.washington.edu\nDavid Wadden1 dwadden@cs.washington.edu\nIz Beltagy2 beltagy@allenai.org\nNoah A. Smith1,2 nasmith@cs.washington.edu\nHannaneh Hajishirzi1,2 hannaneh@cs.washington.edu\nTom Hope1,2 tomh@allenai.org\n1Paul G. Allen School of Computer Science & Engineering, University of Washington\n2Allen Institute for ArtiÔ¨Åcial Intelligence (AI2)\nAbstract\nBiomedical knowledge graphs (KGs) hold rich information on entities such as diseases,\ndrugs, and genes. Predicting missing links in these graphs can boost many important appli-\ncations, such as drug design and repurposing. Recent work has shown that general-domain\nlanguage models (LMs) can serve as ‚Äúsoft‚Äù KGs, and that they can be Ô¨Åne-tuned for the\ntask of KG completion. In this work, we study scientiÔ¨Åc LMs for KG completion, exploring\nwhether we can tap into their latent knowledge to enhance biomedical link prediction. We\nevaluate several domain-speciÔ¨Åc LMs, Ô¨Åne-tuning them on datasets centered on drugs and\ndiseases that we represent as KGs and enrich with textual entity descriptions. We inte-\ngrate the LM-based models with KG embedding models, using a router method that learns\nto assign each input example to either type of model and provides a substantial boost in\nperformance. Finally, we demonstrate the advantage of LM models in the inductive setting\nwith novel scientiÔ¨Åc entities. Our datasets and code are made publicly available. 1\n1. Introduction\nUnderstanding complex diseases such as cancer, HIV, and COVID-19 requires rich biolog-\nical, chemical, and medical knowledge. This knowledge plays a vital role in the process of\ndiscovering therapies for these diseases ‚Äî for example, identifying targets for drugs [Lindsay,\n2003] requires knowing what genes or proteins are involved in a disease, and designing drugs\nrequires predicting whether a drug molecule will interact with speciÔ¨Åc target proteins. In\naddition, to alleviate the great costs of designing new drugs, drug repositioning [Luo et al.,\n2021] involves identiÔ¨Åcation of existing drugs that can be re-purposed for other diseases.\nDue to the challenging combinatorial nature of these tasks, there is need for automation\nwith machine learning techniques. Given the many links between biomedical entities, re-\ncent work [Bonner et al., 2021a,b] has highlighted the potential beneÔ¨Åts of knowledge graph\n(KG) data representations, formulating the associated tasks as KG completion problems ‚Äî\npredicting missing links between drugs and diseases, diseases and genes, and so forth.\nThe focus of KG completion work ‚Äî in the general domain, as well as in biomedical\napplications ‚Äî is on using graph structure to make predictions, such as with KG embedding\n1. https://github.com/rahuln/lm-bio-kgc\narXiv:2106.09700v2  [cs.CL]  21 Sep 2021\nNadkarni, Wadden, Beltagy, Smith, Hajishirzi, & Hope\n(KGE) models and graph neural networks [Zitnik et al., 2018, Chang et al., 2020]. In parallel,\nrecent work in the general domain has explored the use of pretrained language models (LMs)\nas ‚Äúsoft‚Äù knowledge bases, holding factual knowledge latently encoded in their parameters\n[Petroni et al., 2019, 2020]. An emerging direction for using this information for the task\nof KG completion involves Ô¨Åne-tuning LMs to predict relations between pairs of entities\nbased on their textual descriptions [Yao et al., 2019, Kim et al., 2020, Wang et al., 2021,\nDaza et al., 2021]. In the scientiÔ¨Åc domain, this raises the prospect of using LMs trained\non millions of research papers to tap into the scientiÔ¨Åc knowledge that may be embedded\nin their parameters. While this text-based approach has been evaluated on general domain\nbenchmarks derived from WordNet [Miller, 1995] and Freebase [Bollacker et al., 2008], to\nour knowledge it has not been applied to the task of scientiÔ¨Åc KG completion.\nOur contributions. We perform an extensive study of LM-based KG completion in the\nbiomedical domain, focusing on three datasets centered on drugs and diseases, two of which\nhave not been used to date for the KG completion task. To enable exploration of LM-based\nmodels, we collect missing entity descriptions, obtaining them for over 35k entities across all\ndatasets. We evaluate a range of KGE models and domain-speciÔ¨Åc scientiÔ¨Åc LMs pretrained\non diÔ¨Äerent biomedical corpora [Beltagy et al., 2019, Lee et al., 2020, Alsentzer et al., 2019,\nGu et al., 2020]. We conduct analyses of predictions made by both types of models and Ô¨Ånd\nthem to have complementary strengths, echoing similar observations made in recent work\nin the general domain [Wang et al., 2021] and motivating integration of both text and graph\nmodalities. Unlike previous work, we train a router that selects for each input instance which\ntype of model is likely to do better, Ô¨Ånding it to often outperform average-based ensembles.\nIntegration of text and graph modalities provides substantial relative improvements of 13‚Äì\n36% in mean reciprocal rank (MRR), and routing across multiple LM-based models further\nboosts results. Finally, we demonstrate the utility of LM-based models when applied to\nentities unseen during training, an important scenario in the rapidly evolving scientiÔ¨Åc\ndomain. Our hope is that this work will encourage further research into using scientiÔ¨Åc\nLMs for biomedical KG completion, tapping into knowledge embedded in these models and\nmaking relational inferences between complex scientiÔ¨Åc concepts.\n2. Task and Methods\nWe begin by presenting the KG completion task and the approaches we employ for pre-\ndicting missing links in biomedical KGs, including our model integration and inductive KG\ncompletion methodologies. An overview of our approaches is illustrated in Figure 1.\n2.1 KG Completion Task\nFormally, a KG consists of entities E, relations R, and triples T representing facts. Each\ntriple (h,r,t) ‚ààT consists of head and tail entities h,t ‚ààE and a relation r‚ààR. An entity\ncan be one of many types, with the type of an entity e denoted as T(e). In our setting,\neach entity is also associated with some text, denoted as text( e) for e ‚ààE. This text can\nbe an entity name, description, or both; we use the entity‚Äôs name concatenated with its\ndescription when available, or just the name otherwise. For example, the fact ( aspirin,\nScientific Language Models for Biomedical Knowledge Base Completion\n[CLS] aspirin [SEP] headache [SEP]\nscore\nE EE\nR\nscore\n(h, r, t)\nh                   t\nLM2 KGE\ng\n(h, r, t)\nùúô\nsKGEsLM2…ë\nscore\n(a) (b) (c)\nLM\n(d)\nestriol\nestrone\nLM1\nsLM1\nFigure 1: Illustration of the main methods we apply for biomedical KG completion: (a) LM\nÔ¨Åne-tuning; (b) KGE models; (c) an approach that combines both; and (d) using an LM to\nimpute missing entities in a KGE model.\ntreats, headache) might be an ( h,r,t) triple found in a biomedical KG that relates drugs\nand diseases, with the head and tail entities having types T(h) = drug and T(t) = disease.\nThe task of KG completion or link prediction involves receiving a triple (h,r,?) (where ?\ncan replace either the head or tail entity) and scoring all candidate triples{(h,r,t‚Ä≤) |t‚Ä≤ ‚ààS}\nsuch that the correct entity that replaces ? has the highest score. For the example listed\nabove, a well-performing model that receives the incomplete triple (aspirin, treats, ?) should\nrank the tail entity headache higher than an incorrect one such as diabetes. Scan be the\nentire set of entities (i.e., S= E) or some Ô¨Åxed subset. In the transductive setting, the set\nof facts T is split into a training set Ttrain and a test set Ttest such that all positive triples in\nthe test set contain entities seen during training. In contrast, for inductive KG completion\nthe triples in the test set may contain entities not seen during training (see Section 2.4).\n2.2 Methods\nRanking-based KG completion. Each KG completion model in our experiments learns\na function f that computes a ranking score s= f(x) for a given triple x= (h,r,t). Models\nare trained to assign a high ranking score to correct positive triples from the set of known\nfacts T and a low ranking score to triples that are likely to be incorrect. To do so, we use\nthe max-margin loss function Lrank(x) = 1\nN\n‚àëN\ni=1 max(0,Œª ‚àíf(x) + f(x‚Ä≤\ni)), where Œª is a\nmargin hyperparameter, x ‚ààT is a known positive triple in the KG, and x‚Ä≤\ni is a negative\ntriple constructed by randomly corrupting either the head or tail entity of xwith an entity\nof the same type.\nKG embedding (KGE) models. For each entity e ‚ààE and each relation r ‚ààR, KG\nembedding (KGE) models learn a vector representation E(e) ‚ààRm and R(r) ‚ààRn. For a\ngiven triple ( h,r,t), each model computes the ranking score f(h,r,t) as a simple function\nof these embeddings (Figure 1b). We include a variety of diÔ¨Äerent KGE models in our\nexperiments, including TransE [Bordes et al., 2013], DistMult [Yang et al., 2015], ComplEx\n[Trouillon et al., 2016], and RotatE [Sun et al., 2019].\nLM-based models. KGE methods do not capture the rich information available from\ntextual descriptions of nodes. To address this limitation, previous KG completion ap-\nNadkarni, Wadden, Beltagy, Smith, Hajishirzi, & Hope\nproaches have incorporated textual representations [Toutanova et al., 2015, Wang and Li,\n2016], most recently with approaches such as KG-BERT [Yao et al., 2019] that Ô¨Åne-tune the\nBERT language model (LM) [Devlin et al., 2019] for the task of KG completion. Our focus\nin this work is on LMs pretrained on corpora of biomedical documents (e.g., PubMedBERT\n[Gu et al., 2020]; see Appendix B.1.2 for full details). To score a triple using an LM, we\nuse a cross-encoder approach [Yao et al., 2019, Kim et al., 2020] (Fig. 1a), where we encode\nthe text of the head and tail entities together along with the appropriate special tokens.\nSpeciÔ¨Åcally, a triple ( h,r,t) is encoded as v = LM( [CLS] text(h) [SEP] text(t) [SEP]),\nwhere v is the contextualized representation of the [CLS] token at the last layer.2 We then\napply an additional linear layer with a single output dimension to vto compute the ranking\nscore for the triple ( f(x) = Wrankv ‚ààR), and train the LM with the same max-margin\nloss. Recent work on applying BERT for KG completion on general domain benchmarks\nhas shown that multi-task training improves performance [Wang et al., 2021, Kim et al.,\n2020]. We use the approach of Kim et al. [2020] and incorporate two additional losses for\neach LM: a binary triple classiÔ¨Åcation loss to identify if a triple is positive or negative, and\na multi-class relation classiÔ¨Åcation loss. 3\n2.3 Integrating KGE and LM: Model Averaging vs. Routing\nPrevious work using text for KG completion on general domain benchmarks has demon-\nstrated the beneÔ¨Åt of combining KGE and text-based models [Xie et al., 2016, Wang et al.,\n2021]. We study integration of graph-based and text-based methods (Figure 1c), exploring\nwhether learning to route input instances adaptively to a single model can improve perfor-\nmance over previous approaches that compute a weighted average of ranking scores [Wang\net al., 2021]. We also explore the more general setup of combining more than two models.\nMore formally, for a given triple x= (h,r,t), let œÜ(x) be its feature vector. We can learn\na function g(œÜ(x)) that outputs a set of weights Œ± = [ Œ±1,...,Œ± k],‚àë\ni Œ±i = 1 ,Œ±i > 0 ‚àÄi.\nThese weights can be used to perform a weighted average of the ranking scores {s1,...,s k}\nfor a set of k models we wish to combine, such that the Ô¨Ånal ranking score is s= ‚àë\ni Œ±isi.\nWe use a variety of graph-, triple-, and text-based features to construct the feature vector\nœÜ(x) such as node degree, entity and relation type, string edit distance between head and\ntail entity names, and overlap in graph neighbors of head and tail nodes. We explore these\nfeatures further in Section 4.1, and provide a full list in Appendix B.1.3 (Table 6).\nFor the function g(¬∑), we experiment with an input-dependent weighted average\nthat outputs arbitrary weights Œ± and a router that outputs a constrained Œ± such that\nŒ±i = 1 for some i and Œ±j = 0 ,‚àÄj Ã∏= i (i.e., Œ± is a one-hot vector). 4 In practice, we\nimplement the router as a classiÔ¨Åer which selects a single KG completion model for each\nexample by training it to predict which model will perform better.5 For the input-dependent\nweighted average we train a multilayer perceptron (MLP) using the max-margin ranking\nloss. We train all models on the validation set and evaluate on the test set for each dataset.\n2. We experiment with encoding the relation text as well, but Ô¨Ånd that this did not improve performance.\n3. See details in Appendix B. Wang et al. [2021] omit the relation classiÔ¨Åcation loss and use a bi-encoder;\nwe Ô¨Ånd that both of these modiÔ¨Åcations reduce performance in our setting.\n4. We also try a global weighted average with a single set of weights; see Appendix B.1.3 for details.\n5. We explore a range of methods for the router‚Äôs classiÔ¨Åer, with the best found to be gradient boosted\ndecision trees (GBDT) and multilayer perceptrons (MLP).\nScientific Language Models for Biomedical Knowledge Base Completion\n#Positive Edges\nDataset #Entities #Rel Train Dev. Test Avg. Desc.\nLength\nRepoDB 2,748 1 5,342 667 668 49.54\nHetionet (our subset) 12,733 4 124,544 15,567 15,568 44.65\nMSI 29,959 6 387,724 48,465 48,465 45.13\nWN18RR 40,943 11 86,835 3,034 3,134 14.26\nFB15k-237 14,541 237 272,115 17,535 20,466 139.32\nTable 1: Statistics for our datasets and a sample of general domain benchmarks.\nWhen performing ranking evaluation, we use the features œÜ(x) of each positive example\nto compute the weights Œ±, then apply the same weights to all negative examples ranked\nagainst that positive example.\n2.4 Inductive KG Completion\nKGE models are limited to the transductive setting where all entities seen during evaluation\nhave appeared during training. Inductive KG completion is important in the biomedical\ndomain, where we may want to make predictions on novel entities such as emerging biomed-\nical concepts or drugs/proteins mentioned in the literature that are missing from existing\nKGs. Due to their ability to form compositional representations from entity text, LMs are\nwell-suited to this setting. In addition to using LMs Ô¨Åne-tuned for KGC, we try a simple\ntechnique using LMs to‚ÄúÔ¨Åll in‚Äù missing KGE embeddings without explicitly using the LM\nfor prediction (Fig. 1d). Given a set of entities Efor which a KGE model has trained embed-\ndings and a set of unknown entities U, for each e‚ààE‚à™U we encode its text using an LM to\nform ve = LM([CLS] text(e) [SEP]),‚àÄe‚ààE‚à™U , where ve is the [CLS] token representation\nat the last layer. We use the cosine similarity between embeddings to replace each unseen\nentity‚Äôs embedding with the closest trained embedding asE(u) = E(argmax\ne‚ààE\ncos-sim(ve,vu))\nwhere e is of the same type as u, i.e., T(e) = T(u).\n3. Experimental Setup\n3.1 Datasets\nWe use three datasets in the biomedical domain that cover a range of sizes comparable to\nexisting general domain benchmarks, each pooled from a broad range of biomedical sources.\nOur datasets include RepoDB [Brown and Patel, 2017], a collection of drug-disease pairs\nintended for drug repositioning research; MSI (multiscale interactome; [Ruiz et al., 2021]),\na recent network of diseases, proteins, genes, drug targets, and biological functions; and\nHetionet [Himmelstein and Baranzini, 2015], a heterogeneous biomedical knowledge graph\nwhich following Alshahrani et al. [2021] we restrict to interactions involving drugs, diseases,\nsymptoms, genes, and side eÔ¨Äects. 6 Statistics for all datasets and a sample of popular\ngeneral domain benchmark KGs can be found in Table 1.\n6. More information on each dataset is available in Appendix A.1.\nNadkarni, Wadden, Beltagy, Smith, Hajishirzi, & Hope\nWhile Hetionet has previously been explored for the task of KG completion as link\nprediction using KGE models (though not LMs) [Alshahrani et al., 2021, Bonner et al.,\n2021b], to our knowledge neither RepoDB nor MSI have been represented as KGs and used\nfor evaluating KG completion models despite the potential beneÔ¨Åts of this representation\n[Bonner et al., 2021a], especially in conjunction with textual information. In order to apply\nLMs to each dataset, we scrape entity names (when not provided by the original dataset) as\nwell as descriptions from the original online sources used to construct each KG (see Table 5\nin the appendix).\nWe construct an 80%/10%/10% training/development/test transductive split for each\nKG by removing edges from the complete graph while ensuring that all nodes remain in\nthe training graph. We also construct inductive splits, where each positive triple in the test\ntest has one or both entities unseen during training.\n3.2 Pretrained LMs and KGE Integration\nWe experiment with several LMs pretrained on biomedical corpora (see Table 2 and Ap-\npendix B.1.2). For each LM that has been Ô¨Åne-tuned for KG completion, we add the preÔ¨Åx\n‚ÄúKG-‚Äù (e.g., KG-PubMedBERT) to diÔ¨Äerentiate it from the base LM. We use the umbrella\nterm ‚Äúmodel integration‚Äù for both model averaging and routing, unless stated otherwise.\nModel integration. We explore integration of all pairs of KGE models as well as each\nKGE model paired with KG-PubMedBERT. This allows us to compare the eÔ¨Äect of inte-\ngrating pairs of KG completion models in general with integrating graph- and text-based\napproaches. For all pairs of models, we use the router-based and input-dependent weighted\naverage methods. We also explore combinations of multiple KGE models and LMs, where\nwe start with the best pair of KG-PubMedBERT and a KGE model based on the validation\nset and add either KG-BioBERT or the best-performing KGE model (or the second-best,\nif the best KGE model is in the best pair with KG-PubMedBERT).\n3.3 Evaluation\nAt test time, each positive triple is ranked against a set of negatives constructed by replacing\neither the head or tail entity by a Ô¨Åxed set of entities of the same type. When constructing\nthe edge split for each of the three datasets, we generate a Ô¨Åxed set of negatives for every\npositive triple in the validation and test sets, each corresponding to replacing the head or\ntail entity with an entity of the same type and Ô¨Åltering out negatives that appear as positive\ntriples in either the training, validation, or test set (exact details in Appendix A.4). For\neach positive triple, we use its rank to compute the mean reciprocal rank (MRR), Hits@3\n(H@3), and Hits@10 (H@10) metrics.\n4. Experimental Results\n4.1 Transductive Link Prediction Results\nWe report performance on the link prediction task across all datasets and models in Ta-\nble 2. While LMs perform competitively with KGE models and even outperform some,\nthey generally do not match the best KGE model on RepoDB and MSI. This echoes re-\nScientific Language Models for Biomedical Knowledge Base Completion\nRepoDB Hetionet MSI\nMRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10\nKGE\nComplEx 62.3 71.1 85.6 45.9 53.6 77.8 40.3 44.3 57.5\nDistMult 62.0 70.4 85.2 46.0 53.5 77.8 29.6 34.1 53.6\nRotatE 58.8 65.9 79.8 50.6 58.2 79.3 32.4 35.3 49.8\nTransE 60.0 68.6 81.1 50.2 58.0 79.8 32.7 36.5 53.8\nLM (Ô¨Åne-tuned)\nRoBERTa 51.7 60.3 82.3 46.4 53.6 76.9 30.1 33.3 50.6\nSciBERT 59.7 67.6 88.5 50.3 57.1 79.1 34.2 37.9 55.0\nBioBERT 58.2 65.8 86.8 50.3 57.5 79.4 33.4 37.1 54.8\nBio+ClinicalBERT 55.7 64.0 84.1 43.6 49.1 72.6 32.6 36.1 53.5\nPubMedBERT-abs60.8 70.7 89.5 50.8 58.0 80.0 34.3 38.0 55.3\nPubMedBERT-full 59.9 69.3 88.8 51.7 58.7 80.8 34.2 37.7 55.1\nTwo models\n(router)\nBest pair of KGE 62.2 70.4 83.7 56.1 65.5 85.4 45.2 50.6 66.2\nBest KGE + LM 70.6 80.3 94.3 59.7 68.6 87.2 48.5 54.4 70.1\nTwo models\n(input-dep. avg.)\nBest pair of KGE 65.2 74.3 87.6 65.3 75.3 90.2 39.8 44.9 62.0\nBest KGE + LM 65.9 74.4 91.5 70.3 78.7 92.2 40.6 44.6 61.2\nThree models\n(router)\n2 KGE + 1 LM 72.7 81.6 95.2 62.6 71.7 89.4 50.9 57.1 73.2\n1 KGE + 2 LM 72.1 82.5 95.7 62.1 71.9 89.5 51.2 57.0 73.0\nTable 2: KG completion results. All values are in the range [0, 100], higher is better.\nUnderlined values denote the best result within a model category (KGE, LM, two models\nwith router, two models with input-dependent weighted average, three models with router),\nwhile bold values denote the best result for each dataset.\nsults in the general domain for link prediction on subsets of WordNet and Freebase [Yao\net al., 2019, Wang et al., 2021]. On all datasets and metrics, the best-performing LM\nis KG-PubMedBERT, which aligns with results for natural language understanding tasks\nover biomedical text [Gu et al., 2020]. The biomedical LMs also generally outperform\nKG-RoBERTa, illustrating the beneÔ¨Åt of in-domain pretraining even in the KG completion\nsetting.\nFigure 2: Fraction of test set examples where\neach model performs better.\nComparing model errors. By examin-\ning a selected set of examples in Table 3,\nwe can observe cases where information\nin text provides LMs an advantage and\nwhere a lack of context favors KGE models.\nKG-PubMedBERT is able to make connec-\ntions between biomedical concepts ‚Äì like the\nfact that a disease that aÔ¨Äects the stomach\nmight cause weight loss ‚Äì and align related\nconcepts expressed with diÔ¨Äerent terminol-\nogy ‚Äì like connecting antineoplastic with\ncancer (a type of neoplasm), or recogniz-\ning that an echocardiogram is a technique\nfor imaging the heart. In contrast, RotatE oÔ¨Äers an advantage when the descriptions do not\nimmediately connect the two terms ( mediastinal cancer, hoarseness), where a description\nNadkarni, Wadden, Beltagy, Smith, Hajishirzi, & Hope\nRelation RotatE better KG-PubMedBERT better\nDisease\npresents\nSymptom\nDisease: mediastinal cancer; a\ncancer in the mediastinum.\nSymptom: hoarseness; a deep or\nrough quality of voice.\nDisease: stomach cancer; a gas-\ntrointestinal cancer in the stomach.\nSymptom: weight loss; decrease in\nexisting body weight.\nCompound\ntreats\nDisease\nCompound: methylprednisolone;\na prednisolone derivative glucocorti-\ncoid with higher potency.\nDisease: allergic rhinitis; a rhinitis\nthat is an allergic inÔ¨Çammation and\nirritation of the nasal airways.\nCompound: altretamine; an alky-\nlating agent proposed as an antineo-\nplastic.\nDisease: ovarian cancer; a female\nreproductive organ cancer that is lo-\ncated in the ovary.\nCompound\ncauses\nSide EÔ¨Äect\nCompound: cefaclor; semi-\nsynthetic, broad-spectrum anti-\nbiotic derivative of cephalexin.\nSide EÔ¨Äect: tubulointerstitial\nnephritis; no description\nCompound: perÔ¨Çutren; a diagnos-\ntic medication to improve contrast in\nechocardiograms.\nSide EÔ¨Äect: palpitations; irregular\nand/or forceful beating of the heart.\nTable 3: Examples from Hetionet where one model ranks the shown positive pair consider-\nably higher than the other. LMs often perform better when there is semantic relatedness\nbetween head and tail text, but can be outperformed by a KGE model when head/tail\nentity text is missing or unrelated. Entity descriptions cut to Ô¨Åt.\nmay be too technical or generic to be informative ( methylprednisolone, allergic rhinitis ),\nor where no description is available ( cefaclor, tubulointerstitial nephritis ).7 Furthermore,\nFig. 2 shows that KG-PubMedBERT outperforms the best KGE model on a substantial\nfraction of the test set examples for each dataset. 8 These observations motivate an ap-\nproach that leverages the strengths of both types of models by identifying examples where\neach model might do better, which leads to our results for model integration.\n4.2 Model Averaging and Routing\nIntegrating pairs of models. Table 2 shows that combining each class of models boosts\nresults by a large relative improvement of 13‚Äì36% in MRR across datasets. Moreover, the\nbest-performing combination always includes a KGE model and KG-PubMedBERT rather\nthan two KGE models (Fig. 3), showing the unique beneÔ¨Åt of using LMs to augment models\nrelying on KG structure alone.\nAveraging vs. routing. We also compare the router and input-dependent weighted\naverage approaches of integrating a pair of models in Table 2, with the router-based ap-\nproach outperforming the weighted average for the best KGE + LM pair on RepoDB and\nMSI. This presents routing as a promising alternative for integrating KGE and LM models.\nSince the gradient boosted decision trees (GBDT) router achieves the best validation set\n7. Table 7 in the appendix shows the drop in performance when one or both entities are missing descriptions.\n8. See MRR breakdown by relation type in Fig. 4 in the appendix.\nScientific Language Models for Biomedical Knowledge Base Completion\nComplExDistMult RotatE TransE\nDistMult\nRotatE\nTransE\nPubMedBERT\n63.4\n63.2 63.6\n63.7 63.7 62.2\n70.4 70.8 70.6 70.9\nRepoDB\nComplExDistMult RotatE TransE\nDistMult\nRotatE\nTransE\nPubMedBERT\n48.0\n56.1 56.1\n55.0 55.2 55.7\n59.6 59.7 59.2 59.6\nHetionet\nComplExDistMult RotatE TransE\nDistMult\nRotatE\nTransE\nPubMedBERT\n41.3\n45.0 41.6\n44.4 39.8 38.6\n48.3 42.8 43.2 42.8\nMSI\nFigure 3: Test set MRR for all pairs of KG completion models using an MLP router.\nThe best combination of a KGE model and KG-PubMedBERT always performs better\nthan the best pair of KGE models, and for RepoDB and Hetionet all pairs involving KG-\nPubMedBERT outperform all KGE-only pairs.\nperformance in most cases across classiÔ¨Åers and integration methods, we use this method\nfor combinations of more than two models, such as multiple LMs with a single KGE model.\nIntegrating additional models. The bottom of Table 2 shows results for three-model\ncombinations. Adding a third model improves performance compared to the best pair of\nmodels, though whether the best third model is an LM or KGE model varies across datasets\nand metrics. Although there are diminishing returns to including a third model, the three-\nmodel combinations provide the best performance for RepoDB and MSI.\nInterpreting model routing. We compute average feature gain for all datasets, using\na GBDT router implemented with XGBoost [Chen and Guestrin, 2016] (see Fig. 5 in the\nappendix). We Ô¨Ånd that the most salient features are the ranking scores output by each\nmodel, which is intuitive as these scores reÔ¨Çect each model‚Äôs conÔ¨Ådence. Graph features\nlike node degree and PageRank also factor into the classiÔ¨Åer‚Äôs predictions, as well as tex-\ntual features such as entity text length and edit distance between entity names. General\nconcepts such as Hypertensive disease and Infection of skin and/or subcutaneous tissue\nare central nodes for which we observe KGE models to often do better. KGE models also\ntend to do better on entities with short, non-descriptive names (e.g., P2RY14), especially\nwhen no descriptions are available. Generally, these patterns are not clear-cut, and non-\nlinear or interaction eÔ¨Äects likely exist. It remains an interesting challenge to gain deeper\nunderstanding into the strengths and weaknesses of LM-based and graph-based models.\n4.3 Inductive KG Completion\nFor our inductive KG completion experiments, we use ComplEx as the KGE model and\nKG-PubMedBERT as our LM-based model, and compare the performance of each method\nto ComplEx with entity embeddings imputed using the method described in Section 2.4.\nWe use either the untrained PubMedBERT or the Ô¨Åne-tuned KG-PubMedBERT as the LM\nfor retrieving nearest-neighbor (NN) entities (see examples in Table 9 in the appendix).\nWe also compare to DKRL [Xie et al., 2016], which constructs entity representations from\ntext using a CNN encoder and uses the TransE scoring function. We use PubMedBERT‚Äôs\nNadkarni, Wadden, Beltagy, Smith, Hajishirzi, & Hope\nRepoDB Hetionet MSI\nMRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10\nDKRL 15.6 15.9 28.2 17.8 18.5 31.9 13.3 14.1 22.4\nKG-PubMedBERT 38.8 43.4 67.5 21.6 22.3 42.8 20.2 21.7 32.2\nComplEx 0.8 0.4 1.6 3.6 0.7 2.8 0.5 0.1 0.4\nNN-ComplEx, frozen LM 20.1 22.3 31.2 18.1 18.4 32.8 15.8 16.9 23.4\nNN-ComplEx, Ô¨Åne-tuned 26.9 30.3 39.4 13.9 12.9 25.5 14.6 15.4 21.4\nTable 4: Inductive KG completion results. NN-ComplEx refers to the version of ComplEx\nwith unseen entity embeddings replaced using an LM to Ô¨Ånd the 1-nearest neighbor, either\nwith PubMedBERT frozen or Ô¨Åne-tuned for KG completion (KG-PubMedBERT).\ntoken embeddings as input to DKRL and train with the same multi-task loss. While other\nmethods for inductive KG completion exist, such as those based on graph neural networks\n[Schlichtkrull et al., 2018, Vashishth et al., 2020, Bhowmik and de Melo, 2020], they require\nthe unseen entity to have known connections to entities that were seen during training in\norder to propagate information needed to construct the new embedding. In our inductive\nexperiments, we consider the more challenging setup where every test set triple has at least\none entity with no known connections to entities seen during training, such that graph\nneural network-based methods cannot be applied. This models the phenomenon of rapidly\nemerging concepts in the biomedical domain, where a novel drug or protein may be newly\nstudied and discussed in the scientiÔ¨Åc literature without having been integrated into existing\nknowledge bases.\nAs seen in Table 4, ComplEx unsurprisingly performs poorly as it attempts link predic-\ntion with random embeddings for unseen entities. DKRL does substantially better, with\nKG-PubMedBERT further increasing MRR with a relative improvement of 21% (Hetionet)\nto over 2x (RepoDB). Our strategy for replacing ComplEx embeddings for unseen entities\nperforms comparably to or better than DKRL in most cases, with untrained PubMedBERT\nencodings generally superior to using KG-PubMedBERT‚Äôs encodings. In either case, this\nsimple strategy for replacing the untrained entity embeddings of a KGE model shows the\nability of an LM to augment a structure-based method for KG completion that is typically\nonly used in the transductive setting, even without using the LM to compute ranking scores.\n5. Conclusion and Discussion\nWe perform the Ô¨Årst empirical study of scientiÔ¨Åc language models (LMs) applied to biomed-\nical knowledge graph (KG) completion. We evaluate domain-speciÔ¨Åc biomedical LMs, Ô¨Åne-\ntuning them to predict missing links in KGs that we construct by enriching biomedical\ndatasets with textual entity descriptions. We Ô¨Ånd that LMs and more standard KG embed-\nding models have complementary strengths, and propose a routing approach that integrates\nthe two by assigning each input example to either type of model to boost performance. Fi-\nnally, we demonstrate the utility of LMs in the inductive setting with entities not seen during\ntraining, an important scenario in the scientiÔ¨Åc domain with many emerging concepts.\nOur work raises several directions for further study. For instance, several structural\ndiÔ¨Äerences exist between general-domain and biomedical text that would be interesting to\nScientific Language Models for Biomedical Knowledge Base Completion\nexplore in more depth and leverage more explicitly to improve KG completion performance.\nFor example, entities with uninformative technical names ‚Äì such as protein names that are\ncombinations of numbers and letters (e.g., P2RY14) ‚Äì appear very often in scientiÔ¨Åc KGs,\nand are likely related to the beneÔ¨Åt of adding descriptions (Table 7, appendix). The surface\nforms of entity mentions in the biomedical literature on which the LMs were pretrained\ntend to be diverse with many aliases, while entities such as cities or people in the general\ndomain often show less variety in their surface forms used in practice. This could potentially\nbe challenging when trying to tap into the latent knowledge LMs hold on speciÔ¨Åc entities\nas part of the KG completion task, and likely requires LMs to disambiguate these surface\nforms to perform the task well. General-domain LMs are also trained on corpora such as\nWikipedia which has ‚Äúcentralized‚Äù pages with comprehensive information about entities,\nwhile in the scientiÔ¨Åc literature information on entities such as drugs or genes is scattered\nacross the many papers that form the training corpora for the LMs.\nPrevious work [Wang et al., 2021] has also observed that combining graph- and LM-based\nmodels improves KG completion results. We provide further analyses into this phenomenon\nbased on textual and graph properties, but a deeper understanding of the strengths and\nweaknesses of each modality is needed. Interpreting neural models is generally a challenging\nproblem; further work in our setting could help reveal the latent scientiÔ¨Åc knowledge embed-\nded in language models. Importantly, our results point to the potential for designing new\nmodels that capitalize on both graph and text modalities, perhaps by injecting structured\nknowledge into LMs [Peters et al., 2019] or with entity-centric pretraining [Zemlyanskiy\net al., 2021]. Finally, our Ô¨Åndings provide a promising direction for biomedical knowledge\ncompletion tasks, and for literature-based scientiÔ¨Åc discovery [Swanson, 1986, Gopalakrish-\nnan et al., 2019].\nAcknowledgments\nThis project is supported in part by NSF Grant OIA-2033558 and by the OÔ¨Éce of Naval\nResearch under MURI grant N00014-18-1-2670.\nReferences\nEmily Alsentzer, John R. Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann,\nand Matthew B. A. McDermott. Publicly Available Clinical BERT Embeddings. In 2nd\nClinical Natural Language Processing Workshop, 2019.\nMona Alshahrani, Maha A. Thafar, and Magbubah Essack. Application and evaluation of\nknowledge graph embeddings in biomedical data. PeerJ Computer Science, 7, 2021.\nIz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A Pretrained Language Model for\nScientiÔ¨Åc Text. In EMNLP, 2019.\nRajarshi Bhowmik and Gerard de Melo. Explainable Link Prediction for Emerging Entities\nin Knowledge Graphs. In SEMWEB, 2020.\nNadkarni, Wadden, Beltagy, Smith, Hajishirzi, & Hope\nKurt Bollacker, Colin Evans, Praveen K. Paritosh, Tim Sturge, and Jamie Taylor. Free-\nbase: A Collaboratively Created Graph Database For Structuring Human Knowledge. In\nSIGMOD Conference, 2008.\nStephen Bonner, Ian P Barrett, Cheng Ye, Rowan Swiers, Ola Engkvist, Andreas Bender,\nCharles Tapley Hoyt, and William Hamilton. A Review of Biomedical Datasets Relating\nto Drug Discovery: A Knowledge Graph Perspective. arXiv:2102.10062, 2021a.\nStephen Bonner, Ian P Barrett, Cheng Ye, Rowan Swiers, Ola Engkvist, and William L\nHamilton. Understanding the Performance of Knowledge Graph Embeddings in Drug\nDiscovery. arXiv:2105.10488, 2021b.\nAntoine Bordes, Nicolas Usunier, Alberto Garc¬¥ ƒ±a-Dur¬¥ an, Jason Weston, and Oksana\nYakhnenko. Translating Embeddings for Modeling Multi-relational Data. In NIPS, 2013.\nAdam S. Brown and Chirag J. Patel. A standard database for drug repositioning. ScientiÔ¨Åc\nData, 4, 2017.\nDavid Chang, Ivana BalaÀá zevi¬¥ c, Carl Allen, Daniel Chawla, Cynthia Brandt, and Andrew\nTaylor. Benchmark and Best Practices for Biomedical Knowledge Graph Embeddings.\nIn 19th SIGBioMed Workshop on Biomedical Language Processing , pages 167‚Äì176, 2020.\nTianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. KDD,\n2016.\nDaniel Daza, Michael Cochez, and Paul T. Groth. Inductive Entity Representations from\nText via Link Prediction. In WWW, 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT, 2019.\nVishrawas Gopalakrishnan, Kishlay Jha, Wei Jin, and Aidong Zhang. A survey on literature\nbased discovery approaches in biomedical domain. Journal of biomedical informatics, 93:\n103141, 2019.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan\nNaumann, Jianfeng Gao, and Hoifung Poon. Domain-SpeciÔ¨Åc Language Model Pretrain-\ning for Biomedical Natural Language Processing. arXiv:2007.15779, 2020.\nDaniel S. Himmelstein and Sergio E. Baranzini. Heterogeneous Network Edge Prediction: A\nData Integration Approach to Prioritize Disease-Associated Genes. PLoS Computational\nBiology, 11, 2015.\nBosung Kim, Taesuk Hong, Youngjoong Ko, and Jungyun Seo. Multi-Task Learning for\nKnowledge Graph Completion with Pre-trained Language Models. In COLING, 2020.\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In\nICLR, 2015.\nScientific Language Models for Biomedical Knowledge Base Completion\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. BioBERT: a pre-trained biomedical language representation model\nfor biomedical text mining. Bioinformatics, 36:1234 ‚Äì 1240, 2020.\nMark A Lindsay. Target discovery. Nature Reviews Drug Discovery , 2(10):831‚Äì838, 2003.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized\nBERT Pretraining Approach. arXiv:1907.11692, 2019.\nHuimin Luo, Min Li, Mengyun Yang, Fang-Xiang Wu, Yaohang Li, and Jianxin Wang.\nBiomedical data and computational models for drug repositioning: a comprehensive re-\nview. BrieÔ¨Ångs in bioinformatics , 22(2):1604‚Äì1619, 2021.\nGeorge A. Miller. WordNet: a lexical database for English. Commun. ACM , 38:39‚Äì41,\n1995.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-\ndel, G. Louppe, P. Prettenhofer, R. Weiss, J. Vanderplas, A. Passos, D. Cournapeau,\nM. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine Learning in Python. J.\nMach. Learn. Res., 12:2825‚Äì2830, 2011.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer\nSingh, and Noah A. Smith. Knowledge Enhanced Contextual Word Representations. In\nEMNLP, 2019.\nFabio Petroni, Tim Rockt¬® aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H.\nMiller, and Sebastien Riedel. Language Models as Knowledge Bases? In EMNLP, 2019.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt¬® aschel, Yuxiang Wu, Alexan-\nder H Miller, and Sebastian Riedel. How Context AÔ¨Äects Language Models‚Äô Factual\nPredictions. In AKBC, 2020.\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence Embeddings using Siamese\nBERT-Networks. In EMNLP, 2019.\nCamilo Ruiz, Marinka Zitnik, and Jure Leskovec. IdentiÔ¨Åcation of disease treatment mech-\nanisms through the multiscale interactome. Nature communications, 2021.\nMichael Schlichtkrull, Thomas Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and\nMax Welling. Modeling Relational Data with Graph Convolutional Networks. In ESWC,\n2018.\nZhiqing Sun, Zhihong Deng, Jian-Yun Nie, and Jian Tang. RotatE: Knowledge Graph\nEmbedding by Relational Rotation in Complex Space. In ICLR, 2019.\nDon R. Swanson. Fish oil, Raynaud‚Äôs syndrome, and undiscovered public knowledge. Per-\nspectives in biology and medicine , 30(1):7‚Äì18, 1986.\nNadkarni, Wadden, Beltagy, Smith, Hajishirzi, & Hope\nKristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and\nMichael Gamon. Representing Text for Joint Embedding of Text and Knowledge Bases.\nIn EMNLP, 2015.\nTh¬¥ eo Trouillon, Johannes Welbl, S. Riedel,¬¥Eric Gaussier, and Guillaume Bouchard. Com-\nplex Embeddings for Simple Link Prediction. In ICML, 2016.\nShikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. Composition-based\nMulti-Relational Graph Convolutional Networks. In ICLR, 2020.\nBo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Ying Wang, and Yi Chang. Structure-\nAugmented Text Representation Learning for EÔ¨Écient Knowledge Graph Completion. In\nWWW, 2021.\nZhigang Wang and Juan-Zi Li. Text-Enhanced Representation Learning for Knowledge\nGraph. In IJCAI, 2016.\nRuobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. Representation\nLearning of Knowledge Graphs with Entity Descriptions. In AAAI, 2016.\nBishan Yang, Wen tau Yih, Xiadong He, Jianfeng Gao, and Li Deng. Embedding Entities\nand Relations for Learning and Inference in Knowledge Bases. In ICLR, 2015.\nLiang Yao, Chengsheng Mao, and Yuan Luo. KG-BERT: BERT for Knowledge Graph\nCompletion. arXiv:1909.03193, 2019.\nYury Zemlyanskiy, Sudeep Gandhe, Ruining He, Bhargav Kanagal, Anirudh Ravula, Ju-\nraj Gottweis, Fei Sha, and Ilya Eckstein. DOCENT: Learning Self-Supervised Entity\nRepresentations from Large Document Collections. In EACL, 2021.\nMarinka Zitnik, Monica Agrawal, and Jure Leskovec. Modeling polypharmacy side eÔ¨Äects\nwith graph convolutional networks. Bioinformatics, 34(13):i457‚Äìi466, 2018.\nScientific Language Models for Biomedical Knowledge Base Completion\nDataset Link Sources\nRepoDB http://apps.chiragjpgroup.org/repoDB/ DrugBank\nUMLS\nHetionet https://github.com/hetio/hetionet\nDrugBank\nDisease Ontology\nEntrez\nSIDER\nMeSH\nMSI https://github.com/snap-stanford/multiscale-interactome\nDrugBank\nGene Ontology\nEntrez\nUMLS\nTable 5: Links and sources of entity names and descriptions for each dataset.\nAppendix A. Dataset Construction\nA.1 Sources\nRepoDB Drugs in RepoDB have statuses includingapproved, terminated, withdrawn, and\nsuspended. We restrict our KG to pairs in the approved category.\nHetionet was constructed using data from various publicly-available scientiÔ¨Åc reposito-\nries. Following Alshahrani et al. [2021], we restrict the KG to thetreats, presents, associates,\nand causes relation types. This includes interactions between drugs and the diseases they\ntreat, diseases and their symptoms, diseases and associated genes, and drugs and their side\neÔ¨Äects. We use this subset of the full Hetionet dataset to avoid scalability issues that arise\nwhen training large Transformer-based language models, inspired by benchmark datasets\nsuch as FB15K [Bordes et al., 2013], a subset of the Freebase knowledge base.\nMSI includes diseases and the proteins they perturb, drug targets, and biological functions\ndesigned to discover drug-disease treatment pairs through the pathways that connect them\nvia genes, proteins, and their functions. We include all entities and relation types in the\ndataset.\nWe collect each of the datasets from the links listed in Table 5. For missing entity names\nand all descriptions, we write scripts to scrape the information from the resources listed\nabove using the entity identiÔ¨Åers provided by each of the datasets.\nA.2 Transductive Splits\nTo construct transductive splits for each dataset, we begin with the complete graph, and\nrepeat the following steps:\n1. Randomly sample an edge from the graph.\n2. If the degree of both nodes incident to the edge is greater than one, remove the edge.\n3. Otherwise, replace the edge and continue.\nNadkarni, Wadden, Beltagy, Smith, Hajishirzi, & Hope\nThe above steps are repeated until validation and test graphs have been constructed of\nthe desired size while ensuring that no entities are removed from the training graph. We\nconstruct 80%/10%/10% training/validation/test splits of all datasets.\nA.3 Inductive Splits\nTo construct inductive splits for each dataset, we follow the procedure outlined in the\n‚ÄúTechnical Details‚Äù section of the appendix of Daza et al. [2021]. We similarly construct a\n80%/10%/10% training/validation/test split of each dataset in the inductive setting.\nA.4 Negative Validation/Test Triples\nIn order to perform a ranking-based evaluation for each dataset in both the transductive\nand inductive settings, we generate a set of negative triples to be ranked against each\npositive triple. To generate negative entities to replace both the head and tail entity of each\nvalidation and test positive, we follow the procedure below:\n1. Begin with the set of all entities in the knowledge graph.\n2. Remove all entities that do not have the same entity type as the entity to be ranked\nagainst in the positive triple.\n3. Remove all entities that would result in a valid positive triple in either the training,\nvalidation, or test sets.\n4. Randomly sample a Ô¨Åxed set of size m from the remaining set of entities.\nWe use a value ofm= 500 for RepoDB and MSI, and a value ofm= 80 for Hetionet (due\nto the constraints above, the minimum number of valid entities remaining across positive\ntriples for Hetionet was 80). Using a Ô¨Åxed set of entities allows for fair comparison when\nassessing performance of subsets of the test set, such as when examining the eÔ¨Äect of subsets\nwhere descriptions are present for neither, one, or both entities (Table 7).\nAppendix B. Training\nB.1 Transductive Setting\nFor all individual models, we train the models on the training set of each dataset while\nperiodically evaluating on the validation set. We save the model with the best validation\nset MRR, then use that model to evaluate on the test set. We also perform hyperparameter\ntuning for all models, and use validation set MRR to select the Ô¨Ånal set of hyperparameters\nfor each model.\nB.1.1 Knowledge Graph Embeddings\nWe use the max-margin ranking loss for all KGE methods. We use a batch size of 512 for\nall models. We train models for 10,000 steps (958 epochs) on RepoDB, 50,000 steps (205\nepochs) on Hetionet, and 50,000 steps (66 epochs) on MSI. We evaluate on the validation\nset every 500 steps for RepoDB and 5,000 steps for Hetionet and MSI. We use the Adam\noptimizer for training. We perform a hyperparameter search over the following values:\nScientific Language Models for Biomedical Knowledge Base Completion\n‚Ä¢ Embedding dimension: 500, 1000, 2000\n‚Ä¢ Margin for max-margin loss: 0.1, 1\n‚Ä¢ Learning rate: 1e-3, 1e-4\n‚Ä¢ Number of negative samples per positive: 128, 256\n‚Ä¢ Parameter for L3 regularization of embeddings: 1e-5, 1e-6\nB.1.2 Language Models\nPretrained scientiÔ¨Åc LMs. We explore various pretrained LMs, with their initialization,\nvocabulary, and pretraining corpora described below. In particular, we study a range of\nLMs trained on diÔ¨Äerent scientiÔ¨Åc and biomedical literature, and also on clinical notes.\n‚Ä¢ BioBERT [Lee et al., 2020] Initialized from BERT and using the same general do-\nmain vocabulary, with additional pretraining on the PubMed repository of scientiÔ¨Åc\nabstracts and full-text articles.\n‚Ä¢ Bio+ClinicalBERT [Alsentzer et al., 2019] Initialized from BioBERT with addi-\ntional pretraining on the MIMIC-III corpus of clinical notes.\n‚Ä¢ SciBERT [Beltagy et al., 2019] Pretrained from scratch with a domain-speciÔ¨Åc vo-\ncabulary on a sample of the Semantic Scholar corpus, of which biomedical papers are\na signiÔ¨Åcant fraction but also papers from other scientiÔ¨Åc domains.\n‚Ä¢ PubMedBERT [Gu et al., 2020] Pretrained from scratch with a domain-speciÔ¨Åc\nvocabulary on PubMed. We apply two versions of PubMedBERT, one trained on\nPubMed abstracts alone (PubMedBERT-abstract) and the other on abstracts as well\nas full-text articles (PubMedBERT-fulltext).\nWe also use RoBERTa [Liu et al., 2019] ‚Äì pretrained from scratch on the BookCor-\npus, English Wikipedia, CC-News, OpenWebText, and Stories datasets ‚Äì as a strongly-\nperforming general domain model for comparison. For all LMs, we follow Kim et al. [2020]\nand use the multi-task loss consisting of binary triple classiÔ¨Åcation, multi-class relation\nclassiÔ¨Åcation, and max-margin ranking loss, with a margin of 1 for the max-margin loss.\nFor triple classiÔ¨Åcation, given the correct label y ‚àà {0,1}(positive or negative triple)\nwe apply a linear layer to the [CLS] token representation v to output the probability\np of the triple being correct as p = œÉ(Wtriplev), and use the binary cross entropy loss\nLtriple(x) = ‚àíylog(p) ‚àí(1 ‚àíy) log(1‚àíp). For relation classiÔ¨Åcation over R relation types,\nwe apply a linear layer to v to calculate a probability distribution q over relation classes\nwith q= softmax(Wrelv), and use the cross entropy loss with one-hot vector y‚àà{0,1}R as\nthe correct relation label: Lrel(x) = ‚àí‚àëR\ni=1 yi log qi. The Ô¨Ånal loss is the equally-weighted\nsum of all three losses: L(x) = Lrank(x) + Ltriple(x) + Lrel(x).\nWe train for 40 epochs on RepoDB, and 10 epochs on Hetionet and MSI. We evaluate\non the validation set every epoch for RepoDB, and three times per epoch for Hetionet and\nMSI. For RepoDB, Hetionet, and MSI we use 32, 16, and 8 negative samples per positive,\nrespectively. We use the Adam optimizer for training. We perform a hyperparameter search\nover the following values:\nNadkarni, Wadden, Beltagy, Smith, Hajishirzi, & Hope\n‚Ä¢ Batch size: 16, 32\n‚Ä¢ Learning rate: 1e-5, 3e-5, 5e-5\nB.1.3 Integrated Models\nGlobal weighted average. For the global weighted average, we compute ranking scores\nfor positive and negative examples as the weighted average of ranking scores output by all\nKG completion models being integrated. SpeciÔ¨Åcally, for a set of ranking scores s1,...,s k\noutput by k models for an example, we learn a set of weights Œ± = [Œ±1,...,Œ± k] to compute\nthe Ô¨Ånal ranking score as s = ‚àëk\ni=1 Œ±isi, where the same weight vector Œ± is used for all\nexamples. We search for each Œ±i over the grid [0.05, 0.95] with steps of 0.05, ensuring that\nall Œ±i‚Äôs sum to 1. We choose values that maximize validation set MRR, then apply them to\nthe test set.\nRouter. For the router-based method, we train a classiÔ¨Åer to select a single model out of\na set of KG completion models to use for computing ranking scores for a positive example\nand its associated negatives. The class to be predicted for a particular example corresponds\nto which model performs best on that example (i.e., gives the best rank), with an additional\nclass for examples where all models perform the same. We explore a number of diÔ¨Äerent clas-\nsiÔ¨Åers, including logistic regression, decision tree, gradient boosted decision tree (GBDT),\nand multilayer perceptron (MLP), Ô¨Ånding that GBDT and MLP classiÔ¨Åers perform the\nbest. As input to the classiÔ¨Åer, we use a diverse set of features computed from each positive\nexample (listed in Table 6) as well as each model‚Äôs ranking score for the positive example.\nClassiÔ¨Åers are trained on the validation set and evaluated on the test set for each dataset.\nWe additionally perform hyperparameter tuning over the following values for each classiÔ¨Åer:\nLogistic regression:\n‚Ä¢ Penalty: L1, L2\n‚Ä¢ Regularization parameter: 9 values evenly log-spaced between 1e-5 and 1e3\nDecision tree:\n‚Ä¢ Max depth: 2, 4, 8\n‚Ä¢ Learning rate: 1e-1, 1e-2, 1e-3\nGBDT:\n‚Ä¢ Number of boosting rounds: 100, 500, 1000\n‚Ä¢ Max depth: 2, 4, 8\n‚Ä¢ Learning rate: 1e-1, 1e-2, 1e-3\nMLP:\n‚Ä¢ Number of hidden layers: 1, 2\nScientific Language Models for Biomedical Knowledge Base Completion\n‚Ä¢ Hidden layer size: 128, 256\n‚Ä¢ Batch size: 64, 128, 256\n‚Ä¢ Learning rate: 1e-1, 1e-2, 1e-3\nWe perform Ô¨Åve-fold cross-validation on the validation set and use validation set accuracy\nto choose the best set of hyperparameters for each classiÔ¨Åer. We use Scikit-Learn [Pedregosa\net al., 2011] to implement the logistic regression and MLP classiÔ¨Åers, and XGBoost [Chen\nand Guestrin, 2016] to implement the decision tree and GBDT classiÔ¨Åers, using default\nparameters other than the ones listed above.\nInput-dependent weighted average. The input-dependent weighted average method\nof integrating KG completion models operates similarly to the global weighted average,\nexcept that the set of weights can vary for each positive example and are a function of its\nfeature vector (the same set of weights is used for all negative examples used to rank against\neach positive example). We train an MLP to output a set of weights that are then used\nto compute a weighted average of ranking scores for a set of KG completion models. The\nMLP is trained on the validation set and evaluated on the test set for each dataset. We use\nthe max-margin ranking loss with a margin of 1. In order to compare to the MLP trained\nas a router, we train the MLP using the Adam optimizer [Kingma and Ba, 2015] for 200\nepochs with early stopping on the training loss and a patience of 10 epochs (the default\nsettings for an MLP classiÔ¨Åer in Scikit-Learn). We perform a hyperparameter search over\nthe following values (matching the values for the MLP router where applicable):\n‚Ä¢ Number of hidden layers: 1, 2\n‚Ä¢ Hidden layer size: 128, 256\n‚Ä¢ Batch size: 64, 128, 256\n‚Ä¢ Learning rate: 1e-1, 1e-2, 1e-4\n‚Ä¢ Number of negatives (for max-margin loss): 16, 32\nWe select the best hyperparameters by MRR on a held-out portion of the validation set.\nFeatures for integrated models. Both the router and input-dependent weighted av-\nerage methods of model integration use a function to outputs weights based on a feature\nvector of an example. A complete list of the features used by each method can be found in\nTable 6. We also use the ranking score for the positive example from each KG completion\nmodel being integrated as additional features.\nB.2 Inductive Setting\nB.2.1 Knowledge Graph Embeddings and Language Models\nFor the KGE and LM models, we follow the same training procedure for the inductive splits\nas for the transductive splits. We perform hyperparameter tuning over the same grids of\nhyperparameters, periodically evaluate on the validation set and save the checkpoint with\nthe best validation set MRR, and use the set of hyperparameters corresponding to the\nhighest validation set MRR to evaluate on the test set.\nNadkarni, Wadden, Beltagy, Smith, Hajishirzi, & Hope\nentity type length of text in chars.\nrelation type presence of word ‚Äúunknown‚Äù in name/desc.\nhead/tail node in-/out-degree missing desc.\nhead/tail node PageRank number/ratio of punctuation/numeric chars.\nAdamic-Adar index of edge tokens-to-words ratio of entity name/desc.\nedit dist. between head/tail entity names\nTable 6: Complete list of features used by router classiÔ¨Åers.\nB.2.2 DKRL\nIn addition to the KGE and LM-based methods, we also train DKRL [Xie et al., 2016]\nfor inductive KG completion as another text-based baseline for comparison. DKRL uses a\ntwo-layer CNN encoder applied to the word or subword embeddings of an entity‚Äôs textual\ndescription to construct a Ô¨Åxed-length entity embedding. To score a triple, DKRL combines\nits entity embeddings constructed from text with a separately-learned relation embedding\nusing the TransE [Bordes et al., 2013] scoring function. The original DKRL model uses a\njoint scoring function with structure-based and description-based components; we restrict\nto the description-based component as we are applying DKRL in the inductive setting. We\nuse PubMedBERT subword embeddings at the input layer of the CNN encoder, encode\nentity names and descriptions, and apply the same multi-task loss as for the LM-based\nmodels. To apply the triple classiÔ¨Åcation and relation classiÔ¨Åcation losses, for head and tail\nentity embeddings h and t, we apply a separate linear layer for each loss to the concatenated\nvector [h; t; |h ‚àít|], following previous work on models that use a bi-encoder to construct\nentity or sentence representations [Wang et al., 2021, Reimers and Gurevych, 2019]. We\nuse the same number of training epochs and number of negatives per positive for DKRL\nas for the LM-based methods on each dataset. We use a batch size of 64, and perform a\nhyperparameter search over the following values:\n‚Ä¢ Learning rate: 1e-3, 1e-4, 1e-5\n‚Ä¢ Embedding dimension: 500, 1000, 2000\n‚Ä¢ Parameter for L2 regularization of embeddings: 0, 1e-3, 1e-2\nAppendix C. Additional Results\nC.1 Transductive Setting, Individual Models\nMissing entity descriptions. Table 7 shows test set MRR for KG-PubMedBERT on\neach dataset broken down by triples with either both, one, or neither entities having available\ndescriptions. Across datasets, performance clearly degrades when fewer descriptions are\navailable to provide context for the LM to generate a ranking score.\nRelation-level performance. Figure 4 shows test set MRR broken down by relation for\nthe datasets with multiple relation types (Hetionet and MSI). KG-PubMedBERT performs\nbetter on all relation types except compound-side eÔ¨Äect for Hetionet, and on the function-\nfunction relation for MSI.\nScientific Language Models for Biomedical Knowledge Base Completion\n#entities with\ndesc. in pair MRR\nRepoDB Hetionet MSI\nNone N/A 25.6 25.1\nOne 59.5 43.6 25.4\nBoth 63.7 52.6 37.3\nTable 7: EÔ¨Äect of descriptions on KG-PubMedBERT test set MRR.\nFigure 4: Test set MRR for the best KGE model compared to KG-PubMedBERT broken\ndown by relation type for Hetionet and MSI.\nC.2 Transductive Setting, Integrated Models\nRepoDB Hetionet MSI\nGlobal avg. 70.4 55.8 42.1\nInput-dep. avg. 65.9 70.3 40.6\nRouter 70.6 59.7 48.5\nTable 8: Test set MRR for the best pair of a KGE model and KG-PubMedBERT for\ndiÔ¨Äerent methods of model integration.\nC.3 Inductive Setting\nNadkarni, Wadden, Beltagy, Smith, Hajishirzi, & Hope\nFigure 5: Feature importances for GBDT router for a selection of most important features.\nRanking scores output by each model tend to be the most important, with other graph-\nand text-based features also contributing.\nImputation Model\nwith Better Ranking\nUnseen\nEntity\nKG-PubMedBERT\nnearest neighbors\nPubMedBERT\nnearest neighbors\nPubMedBERT eye redness skin burning sensation,\nskin discomfort conjunctivitis, throat sore\necchymosis gas, thrombophlebitis petechiae, macule\nestrone vitamin a,\nmethyltestosterone estriol, calcitriol\nKG-PubMedBERT keratoconjunctivitisconjunctivitis allergic,\notitis externa enteritis, parotitis\nmalnutrition dehydration, anaemia meningism,\nwasting generalized\ncongestive\ncardiomyopathy\ndiastolic dysfunction,\ncardiomyopathy\ncarcinoma breast,\nhypertrophic\ncardiomyopathy\nTable 9: Samples of unseen entities and their nearest neighbors found by KG-PubMedBERT\nand PubMedBERT, for test set examples in the Hetionet inductive split where the Pub-\nMedBERT neighbor performs better than the KG-PubMedBERT neighbor (Ô¨Årst three) and\nvice versa (last three). Each LM oÔ¨Äers a larger improvement per example when its nearest\nneighbor is more semantically related to the unseen entity.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8030416965484619
    },
    {
      "name": "Language model",
      "score": 0.6093888282775879
    },
    {
      "name": "Embedding",
      "score": 0.567426860332489
    },
    {
      "name": "Code (set theory)",
      "score": 0.5596917271614075
    },
    {
      "name": "Task (project management)",
      "score": 0.5488557815551758
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5445258617401123
    },
    {
      "name": "Repurposing",
      "score": 0.4890037775039673
    },
    {
      "name": "Machine learning",
      "score": 0.46881580352783203
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44197946786880493
    },
    {
      "name": "Data science",
      "score": 0.3326435685157776
    },
    {
      "name": "Natural language processing",
      "score": 0.322582483291626
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}