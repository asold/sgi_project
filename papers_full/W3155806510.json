{
  "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
  "url": "https://openalex.org/W3155806510",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222671840",
      "name": "Su, Jianlin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096962668",
      "name": "Lu Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222671838",
      "name": "Pan, Shengfeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2284127801",
      "name": "Murtadha Ahmed",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098082197",
      "name": "Wen Bo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1894565412",
      "name": "Liu, Yunfeng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963755523",
    "https://openalex.org/W3129821495",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W3106210592",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W131533222",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2995971510",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W3163721282",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W3037983807",
    "https://openalex.org/W3035206215",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2952509486",
    "https://openalex.org/W2995273672",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W3001591165",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2991316439",
    "https://openalex.org/W2971871542",
    "https://openalex.org/W2752127475",
    "https://openalex.org/W2101105183"
  ],
  "abstract": "Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \\url{https://huggingface.co/docs/transformers/model_doc/roformer}.",
  "full_text": "ROFORMER : E NHANCED TRANSFORMER WITH ROTARY\nPOSITION EMBEDDING\nJianlin Su\nZhuiyi Technology Co., Ltd.\nShenzhen\nbojonesu@wezhuiyi.com\nYu Lu\nZhuiyi Technology Co., Ltd.\nShenzhen\njulianlu@wezhuiyi.com\nShengfeng Pan\nZhuiyi Technology Co., Ltd.\nShenzhen\nnickpan@wezhuiyi.com\nAhmed Murtadha\nZhuiyi Technology Co., Ltd.\nShenzhen\nmengjiayi@wezhuiyi.com\nBo Wen\nZhuiyi Technology Co., Ltd.\nShenzhen\nbrucewen@wezhuiyi.com\nYunfeng Liu\nZhuiyi Technology Co., Ltd.\nShenzhen\nglenliu@wezhuiyi.com\nNovember 9, 2023\nABSTRACT\nPosition encoding recently has shown effective in the transformer architecture. It enables valuable\nsupervision for dependency modeling between elements at different positions of the sequence. In\nthis paper, we first investigate various methods to integrate positional information into the learning\nprocess of transformer-based language models. Then, we propose a novel method named Rotary\nPosition Embedding(RoPE) to effectively leverage the positional information. Specifically, the\nproposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates\nthe explicit relative position dependency in self-attention formulation. Notably, RoPE enables\nvaluable properties, including the flexibility of sequence length, decaying inter-token dependency\nwith increasing relative distances, and the capability of equipping the linear self-attention with\nrelative position encoding. Finally, we evaluate the enhanced transformer with rotary position\nembedding, also called RoFormer, on various long text classification benchmark datasets. Our\nexperiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical\nanalysis to explain some experimental results. RoFormer is already integrated into Huggingface:\nhttps://huggingface.co/docs/transformers/model_doc/roformer.\nKeywords Pre-trained Language Models · Position Information Encoding · Pre-training · Natural Language Processing.\n1 Introduction\nThe sequential order of words is of great value to natural language understanding. Recurrent neural networks (RRNs)\nbased models encode tokens’ order by recursively computing a hidden state along the time dimension. Convolution\nneural networks (CNNs) based models (CNNs) Gehring et al. [2017] were typically considered position-agnostic, but\nrecent work Islam et al. [2020] has shown that the commonly used padding operation can implicitly learn position\ninformation. Recently, the pre-trained language models (PLMs), which were built upon the transformer Vaswani et al.\n[2017], have achieved the state-of-the-art performance of various natural language processing (NLP) tasks, including\ncontext representation learning Devlin et al. [2019], machine translation Vaswani et al. [2017], and language modeling\nRadford et al. [2019], to name a few. Unlike, RRNs and CNNs-based models, PLMs utilize the self-attention mechanism\nto semantically capture the contextual representation of a given corpus. As a consequence, PLMs achieve a significant\nimprovement in terms of parallelization over RNNs and improve the modeling ability of longer intra-token relations\ncompared to CNNs1.\n1A stack of multiple CNN layers can also capture longer intra-token relation, here we only consider single layer setting.\narXiv:2104.09864v5  [cs.CL]  8 Nov 2023\nRoFormer\nIt is noteworthy that the self-attention architecture of the current PLMs has shown to be position-agnostic Yun et al.\n[2020]. Following this claim, various approaches have been proposed to encode the position information into the\nlearning process. On one side, generated absolute position encoding through a pre-defined function Vaswani et al.\n[2017] was added to the contextual representations, while a trainable absolute position encoding Gehring et al. [2017],\nDevlin et al. [2019], Lan et al. [2020], Clark et al. [2020], Radford et al. [2019], Radford and Narasimhan [2018]. On\nthe other side, the previous work Parikh et al. [2016], Shaw et al. [2018], Huang et al. [2018], Dai et al. [2019], Yang\net al. [2019], Raffel et al. [2020], Ke et al. [2020], He et al. [2020], Huang et al. [2020] focuses on relative position\nencoding, which typically encodes the relative position information into the attention mechanism. In addition to these\napproaches, the authors of Liu et al. [2020] have proposed to model the dependency of position encoding from the\nperspective of Neural ODE Chen et al. [2018a], and the authors of Wang et al. [2020] have proposed to model the\nposition information in complex space. Despite the effectiveness of these approaches, they commonly add the position\ninformation to the context representation and thus render them unsuitable for the linear self-attention architecture.\nIn this paper, we introduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional\ninformation into the learning process of PLMS. Specifically, RoPE encodes the absolute position with a rotation matrix\nand meanwhile incorporates the explicit relative position dependency in self-attention formulation. Note that the\nproposed RoPE is prioritized over the existing methods through valuable properties, including the sequence length\nflexibility, decaying inter-token dependency with increasing relative distances, and the capability of equipping the\nlinear self-attention with relative position encoding. Experimental results on various long text classification benchmark\ndatasets show that the enhanced transformer with rotary position embedding, namely RoFormer, can give better\nperformance compared to baseline alternatives and thus demonstrates the efficacy of the proposed RoPE.\nIn brief, our contributions are three-folds as follows:\n• We investigated the existing approaches to the relative position encoding and found that they are mostly\nbuilt based on the idea of the decomposition of adding position encoding to the context representations. We\nintroduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional information\ninto the learning process of PLMS. The key idea is to encode relative position by multiplying the context\nrepresentations with a rotation matrix with a clear theoretical interpretation.\n• We study the properties of RoPE and show that it decays with the relative distance increased, which is desired\nfor natural language encoding. We kindly argue that previous relative position encoding-based approaches are\nnot compatible with linear self-attention.\n• We evaluate the proposed RoFormer on various long text benchmark datasets. Our experiments show that it\nconsistently achieves better performance compared to its alternatives. Some experiments with pre-trained\nlanguage models are available on GitHub: https://github.com/ZhuiyiTechnology/roformer.\nThe remaining of the paper is organized as follows. We establish a formal description of the position encoding problem\nin self-attention architecture and revisit previous works in Section (2). We then describe the rotary position encoding\n(RoPE) and study its properties in Section (3). We report experiments in Section (4). Finally, we conclude this paper in\nSection (5).\n2 Background and Related Work\n2.1 Preliminary\nLet SN = {wi}N\ni=1 be a sequence of N input tokens with wi being the ith element. The corresponding word embedding\nof SN is denoted as EN = {xi}N\ni=1, where xi ∈ Rd is the d-dimensional word embedding vector of token wi without\nposition information. The self-attention first incorporates position information to the word embeddings and transforms\nthem into queries, keys, and value representations.\nqm = fq(xm, m)\nkn = fk(xn, n)\nvn = fv(xn, n),\n(1)\nwhere qm, kn and vn incorporate the mth and nth positions through fq, fk and fv, respectively. The query and key\nvalues are then used to compute the attention weights, while the output is computed as the weighted sum over the value\n2\nRoFormer\nrepresentation.\nam,n =\nexp(q⊺\nmkn√\nd )\nPN\nj=1 exp(q⊺\nmkj√\nd )\nom =\nNX\nn=1\nam,nvn\n(2)\nThe existing approaches of transformer-based position encoding mainly focus on choosing a suitable function to form\nEquation (1).\n2.2 Absolute position embedding\nA typical choice of Equation (1) is\nft:t∈{q,k,v}(xi, i) := Wt:t∈{q,k,v}(xi + pi), (3)\nwhere pi ∈ Rd is a d-dimensional vector depending of the position of token xi. Previous work Devlin et al. [2019],\nLan et al. [2020], Clark et al. [2020], Radford et al. [2019], Radford and Narasimhan [2018] introduced the use of a set\nof trainable vectors pi ∈ {pt}L\nt=1, where L is the maximum sequence length. The authors of Vaswani et al. [2017] have\nproposed to generate pi using the sinusoidal function.\n\u001api,2t = sin(k/100002t/d)\npi,2t+1 = cos(k/100002t/d) (4)\nin which pi,2t is the 2tth element of the d-dimensional vector pi. In the next section, we show that our proposed RoPE\nis related to this intuition from the sinusoidal function perspective. However, instead of directly adding the position\nto the context representation, RoPE proposes to incorporate the relative position information by multiplying with the\nsinusoidal functions.\n2.3 Relative position embedding\nThe authors of Shaw et al. [2018] applied different settings of Equation (1) as following:\nfq(xm) := Wqxm\nfk(xn, n) := Wk(xn + ˜pk\nr )\nfv(xn, n) := Wv(xn + ˜pv\nr)\n(5)\nwhere ˜pk\nr , ˜pv\nr ∈ Rd are trainable relative position embeddings. Note that r = clip(m − n, rmin, rmax) represents the\nrelative distance between position m and n. They clipped the relative distance with the hypothesis that precise relative\nposition information is not useful beyond a certain distance. Keeping the form of Equation (3), the authors Dai et al.\n[2019] have proposed to decompose q⊺\nmkn of Equation (2) as\nq⊺\nmkn = x⊺\nmW⊺\nq Wkxn + x⊺\nmW⊺\nq Wkpn + p⊺\nmW⊺\nq Wkxn + p⊺\nmW⊺\nq Wkpn, (6)\nthe key idea is to replace the absolute position embedding pn with its sinusoid-encoded relative counterpart ˜pm−n,\nwhile the absolute position pm in the third and fourth term with two trainable vectors u and v independent of the query\npositions. Further, Wk is distinguished for the content-based and location-based key vectors xn and pn, denoted as\nWk and fWk, resulting in:\nq⊺\nmkn = x⊺\nmW⊺\nq Wkxn + x⊺\nmW⊺\nq fWk˜pm−n + u⊺W⊺\nq Wkxn + v⊺W⊺\nq fWk˜pm−n (7)\nIt is noteworthy that the position information in the value term is removed by setting fv(xj) := Wvxj. Later work\nRaffel et al. [2020], He et al. [2020], Ke et al. [2020], Huang et al. [2020] followed these settings by only encoding\nthe relative position information into the attention weights. However, the authors of Raffel et al. [2020] reformed\nEquation (6) as:\nq⊺\nmkn = x⊺\nmW⊺\nq Wkxn + bi,j (8)\nwhere bi,j is a trainable bias. The authors of Ke et al. [2020] investigated the middle two terms of Equation (6) and\nfound little correlations between absolute positions and words. The authors of Raffel et al. [2020] proposed to model a\npair of words or positions using different projection matrices.\nq⊺\nmkn = x⊺\nmW⊺\nq Wkxn + p⊺\nmU⊺\nq Ukpn + bi,j (9)\n3\nRoFormer\nThe authors of He et al. [2020] argued that the relative positions of two tokens could only be fully modeled using\nthe middle two terms of Equation (6). As a consequence, the absolute position embeddings pm and pn were simply\nreplaced with the relative position embeddings ˜pm−n:\nq⊺\nmkn = x⊺\nmW⊺\nq Wkxn + x⊺\nmW⊺\nq Wk˜pm−n + ˜p⊺\nm−nW⊺\nq Wkxn (10)\nA comparison of the four variants of the relative position embeddings Radford and Narasimhan [2018] has shown\nthat the variant similar to Equation (10) is the most efficient among the other three. Generally speaking, all these\napproaches attempt to modify Equation (6) based on the decomposition of Equation (3) under the self-attention settings\nin Equation (2), which was originally proposed in Vaswani et al. [2017]. They commonly introduced to directly add\nthe position information to the context representations. Unlikely, our approach aims to derive the relative position\nencoding from Equation (1) under some constraints. Next, we show that the derived approach is more interpretable by\nincorporating relative position information with the rotation of context representations.\n3 Proposed approach\nIn this section, we discuss the proposed rotary position embedding (RoPE). We first formulate the relative position\nencoding problem in Section(3.1), we then derive the RoPE in Section(3.2) and investigate its properties in Section(3.3).\n3.1 Formulation\nTransformer-based language modeling usually leverages the position information of individual tokens through a self-\nattention mechanism. As can be observed in Equation (2), q⊺\nmkn typically enables knowledge conveyance between\ntokens at different positions. In order to incorporate relative position information, we require the inner product of query\nqm and key kn to be formulated by a function g, which takes only the word embeddings xm, xn, and their relative\nposition m − n as input variables. In other words, we hope that the inner product encodes position information only in\nthe relative form:\n⟨fq(xm, m), fk(xn, n)⟩ = g(xm, xn, m− n). (11)\nThe ultimate goal is to find an equivalent encoding mechanism to solve the functions fq(xm, m) and fk(xn, n) to\nconform the aforementioned relation.\n3.2 Rotary position embedding\n3.2.1 A 2D case\nWe begin with a simple case with a dimension d = 2. Under these settings, we make use of the geometric property\nof vectors on a 2D plane and its complex form to prove (refer Section (3.4.1) for more details) that a solution to our\nformulation Equation (11) is:\nfq(xm, m) = (Wqxm)eimθ\nfk(xn, n) = (Wkxn)einθ\ng(xm, xn, m− n) = Re[(Wqxm)(Wkxn)∗ei(m−n)θ]\n(12)\nwhere Re[·] is the real part of a complex number and (Wkxn)∗ represents the conjugate complex number of (Wkxn).\nθ ∈ R is a preset non-zero constant. We can further write f{q,k} in a multiplication matrix:\nf{q,k}(xm, m) =\n\u0012\ncos mθ −sin mθ\nsin mθ cos mθ\n\u0013 \nW(11)\n{q,k} W(12)\n{q,k}\nW(21)\n{q,k} W(22)\n{q,k}\n! \nx(1)\nm\nx(2)\nm\n!\n(13)\nwhere (x(1)\nm , x(2)\nm ) is xm expressed in the 2D coordinates. Similarly, g can be viewed as a matrix and thus enables the\nsolution of formulation in Section (3.1) under the 2D case. Specifically, incorporating the relative position embedding\nis straightforward: simply rotate the affine-transformed word embedding vector by amount of angle multiples of its\nposition index and thus interprets the intuition behind Rotary Position Embedding.\n4\nRoFormer\n3.2.2 General form\nIn order to generalize our results in 2D to any xi ∈ Rd where d is even, we divide the d-dimension space into d/2\nsub-spaces and combine them in the merit of the linearity of the inner product, turning f{q,k} into:\nf{q,k}(xm, m) = Rd\nΘ,mW{q,k}xm (14)\nwhere\nRd\nΘ,m =\n\n\ncos mθ1 −sin mθ1 0 0 ··· 0 0\nsin mθ1 cos mθ1 0 0 ··· 0 0\n0 0 cos mθ2 −sin mθ2 ··· 0 0\n0 0 sin mθ2 cos mθ2 ··· 0 0\n... ... ... ... ... ... ...\n0 0 0 0 ··· cos mθd/2 −sin mθd/2\n0 0 0 0 ··· sin mθd/2 cos mθd/2\n\n\n(15)\nis the rotary matrix with pre-defined parameters Θ = {θi = 10000−2(i−1)/d, i∈ [1, 2, ..., d/2]}. A graphic illustration\nof RoPE is shown in Figure (1). Applying our RoPE to self-attention in Equation (2), we obtain:\nq⊺\nmkn = (Rd\nΘ,mWqxm)⊺(Rd\nΘ,nWkxn) = x⊺WqRd\nΘ,n−mWkxn (16)\nwhere Rd\nΘ,n−m = (Rd\nΘ,m)⊺Rd\nΘ,n. Note that Rd\nΘ is an orthogonal matrix, which ensures stability during the process of\nencoding position information. In addition, due to the sparsity of Rd\nΘ, applying matrix multiplication directly as in\nEquation (16) is not computationally efficient; we provide another realization in theoretical explanation.\nIn contrast to the additive nature of position embedding method adopted in the previous works, i.e., Equations (3)\nto (10), our approach is multiplicative. Moreover, RoPE naturally incorporates relative position information through\nrotation matrix product instead of altering terms in the expanded formulation of additive position encoding when applied\nwith self-attention.\nEnhanced\nTransformer\nwith\nRotary\nPosition\nEmbedding\nPosition\n1\n2\n5\n3\n4\n6\nθ1 θ2 θd/2-1 θd/2\nmθ1\nQuery / Key\nx1\nx2\n(x1, x2)\nm\nPosition\nx'1\nx'2\n(x'1, x'2)\nPosition Encoded Query / Key\nQuery / Key\nPosition Encoded Query / Key\nθ1\nConstant\nd=2\nFigure 1: Implementation of Rotary Position Embedding(RoPE).\n3.3 Properties of RoPE\nLong-term decay: Following Vaswani et al. [2017], we setθi = 10000−2i/d. One can prove that this setting provides\na long-term decay property (refer to Section (3.4.3) for more details), which means the inner-product will decay when\nthe relative position increase. This property coincides with the intuition that a pair of tokens with a long relative distance\nshould have less connection.\nRoPE with linear attention: The self-attention can be rewritten in a more general form.\n5\nRoFormer\nAttention(Q, K, V)m =\nPN\nn=1 sim(qm, kn)vn\nPN\nn=1 sim(qm, kn)\n. (17)\nThe original self-attention chooses sim(qm, kn) = exp( q⊺\nmkn/\n√\nd). Note that the original self-attention should\ncompute the inner product of query and key for every pair of tokens, which has a quadratic complexity O(N2). Follow\nKatharopoulos et al. [2020], the linear attentions reformulate Equation (17) as\nAttention(Q, K, V )m =\nPN\nn=1 ϕ(qm)⊺φ(kn)vn\nPN\nn=1 ϕ(qm)⊺φ(kn)\n, (18)\nwhere ϕ(·), φ(·) are usually non-negative functions. The authors of Katharopoulos et al. [2020] have proposed\nϕ(x) = φ(x) = elu(x)+1 and first computed the multiplication between keys and values using the associative property\nof matrix multiplication. A softmax function is used in Shen et al. [2021] to normalize queries and keys separately\nbefore the inner product, which is equivalent to ϕ(qi) = softmax(qi) and ϕ(kj) = exp(kj). For more details about\nlinear attention, we encourage readers to refer to original papers. In this section, we focus on discussing incorporating\nRoPE with Equation (18). Since RoPE injects position information by rotation, which keeps the norm of hidden\nrepresentations unchanged, we can combine RoPE with linear attention by multiplying the rotation matrix with the\noutputs of the non-negative functions.\nAttention(Q, K, V)m =\nPN\nn=1\n\u0000\nRd\nΘ,mϕ(qm)\n\u0001⊺\u0000\nRd\nΘ,nφ(kn)\n\u0001\nvn\nPN\nn=1 ϕ(qm)⊺φ(kn)\n. (19)\nIt is noteworthy that we keep the denominator unchanged to avoid the risk of dividing zero, and the summation in\nthe numerator could contain negative terms. Although the weights for each value vi in Equation (19) are not strictly\nprobabilistic normalized, we kindly argue that the computation can still model the importance of values.\n3.4 Theoretical Explanation\n3.4.1 Derivation of RoPE under 2D\nUnder the case of d = 2, we consider two-word embedding vectors xq, xk corresponds to query and key and their\nposition m and n, respectively. According to eq. (1), their position-encoded counterparts are:\nqm = fq(xq, m),\nkn = fk(xk, n), (20)\nwhere the subscripts of qm and kn indicate the encoded positions information. Assume that there exists a function g\nthat defines the inner product between vectors produced by f{q,k}:\nq⊺\nmkn = ⟨fq(xm, m), fk(xn, n)⟩ = g(xm, xn, n− m), (21)\nwe further require below initial condition to be satisfied:\nq = fq(xq, 0),\nk = fk(xk, 0), (22)\nwhich can be read as the vectors with empty position information encoded. Given these settings, we attempt to find a\nsolution of fq, fk. First, we take advantage of the geometric meaning of vector in 2D and its complex counter part,\ndecompose functions in Equations (20) and (21) into:\nfq(xq, m) = Rq(xq, m)eiΘq(xq,m),\nfk(xk, n) = Rk(xk, n)eiΘk(xk,n),\ng(xq, xk, n− m) = Rg(xq, xk, n− m)eiΘg(xq,xk,n−m),\n(23)\nwhere Rf , Rg and Θf , Θg are the radical and angular components for f{q,k} and g, respectively. Plug them into\nEquation (21), we get the relation:\nRq(xq, m)Rk(xk, n) = Rg(xq, xk, n− m),\nΘk(xk, n) − Θq(xq, m) = Θg(xq, xk, n− m), (24)\n6\nRoFormer\nwith the corresponding initial condition as:\nq = ∥q∥eiθq = Rq(xq, 0)eiΘq(xq,0),\nk = ∥k∥eiθk = Rk(xk, 0)eiΘk(xk,0),\n(25)\nwhere ∥q∥, ∥k∥ and θq, θk are the radial and angular part of q and k on the 2D plane.\nNext, we set m = n in Equation (24) and take into account initial conditions in Equation (25):\nRq(xq, m)Rk(xk, m) = Rg(xq, xk, 0) = Rq(xq, 0)Rk(xk, 0) = ∥q∥∥k∥, (26a)\nΘk(xk, m) − Θq(xq, m) = Θg(xq, xk, 0) = Θk(xk, 0) − Θq(xq, 0) = θk − θq. (26b)\nOn one hand, from, a straightforward solution of Rf could be formed from Equation (26a) :\nRq(xq, m) = Rq(xq, 0) = ∥q∥\nRk(xk, n) = Rk(xk, 0) = ∥k∥\nRg(xq, xk, n− m) = Rg(xq, xk, 0) = ∥q∥∥k∥\n(27)\nwhich interprets the radial functions Rq, Rk and Rg are independent from the position information. On the other hand,\nas can be noticed in Equation (26b), Θq(xq, m) − θq = Θk(xk, m) − θk indicates that the angular functions does not\ndependent on query and key, we set them toΘf := Θq = Θk and term Θf (x{q,k}, m) −θ{q,k} is a function of position\nm and is independent of word embedding x{q,k}, we denote it as ϕ(m), yielding:\nΘf (x{q,k}, m) = ϕ(m) + θ{q,k}, (28)\nFurther, by plugging n = m + 1 to Equation (24) and consider the above equation, we can get:\nϕ(m + 1) − ϕ(m) = Θg(xq, xk, 1) + θq − θk, (29)\nSince RHS is a constant irrelevant to m, ϕ(m) with continuous integer inputs produce an arithmetic progression:\nϕ(m) = mθ + γ, (30)\nwhere θ, γ∈ R are constants and θ is non-zero. To summarize our solutions from Equations (27) to (30):\nfq(xq, m) = ∥q∥eiθq+mθ+γ = qei(mθ+γ),\nfk(xk, n) = ∥k∥eiθk+nθ+γ = kei(nθ+γ).\n(31)\nNote that we do not apply any constrains to fq and fk of Equation (22), thus fq(xm, 0) and fk(xn, 0) are left to choose\nfreely. To make our results comparable to Equation (3), we define:\nq = fq(xm, 0) = Wqxn,\nk = fk(xn, 0) = Wkxn. (32)\nThen, we simply set γ = 0 in Equation (31) of the final solution:\nfq(xm, m) = (Wqxm)eimθ,\nfk(xn, n) = (Wkxn)einθ.\n(33)\n3.4.2 Computational efficient realization of rotary matrix multiplication\nTaking the advantage of the sparsity of Rd\nΘ,m in Equation (15), a more computational efficient realization of a\nmultiplication of Rd\nΘ and x ∈ Rd is:\nRd\nΘ,mx =\n\n\nx1\nx2\nx3\nx4\n...\nxd−1\nxd\n\n\n⊗\n\n\ncos mθ1\ncos mθ1\ncos mθ2\ncos mθ2\n...\ncos mθd/2\ncos mθd/2\n\n\n+\n\n\n−x2\nx1\n−x4\nx3\n...\n−xd\nxd−1\n\n\n⊗\n\n\nsin mθ1\nsin mθ1\nsin mθ2\nsin mθ2\n...\nsin mθd/2\nsin mθd/2\n\n\n(34)\n7\nRoFormer\n50 100 150 200 250\nrelative distance\n8\n10\n12\n14\n16\n18\n20\nrelative upper bound\nFigure 2: Long-term decay of RoPE.\n3.4.3 Long-term decay of RoPE\nWe can group entries of vectors q = Wqxm and k = Wkxn in pairs, and the inner product of RoPE in Equation (16)\ncan be written as a complex number multiplication.\n(Rd\nΘ,mWqxm)⊺(Rd\nΘ,nWkxn) = Re\n\u0014d/2−1X\ni=0\nq[2i:2i+1]k∗\n[2i:2i+1]ei(m−n)θi\n\u0015\n(35)\nwhere q[2i:2i+1] represents the 2ith to (2i + 1)th entries of q. Denote hi = q[2i:2i+1]k∗\n[2i:2i+1] and Sj =Pj−1\ni=0 ei(m−n)θi , and let hd/2 = 0 and S0 = 0, we can rewrite the summation using Abel transformation\nd/2−1X\ni=0\nq[2i:2i+1]k∗\n[2i:2i+1]ei(m−n)θi =\nd/2−1X\ni=0\nhi(Si+1 − Si) = −\nd/2−1X\ni=0\nSi+1(hi+1 − hi). (36)\nThus,\n\f\f\f\f\nd/2−1X\ni=0\nq[2i:2i+1]k∗\n[2i:2i+1]ei(m−n)θi\n\f\f\f\f =\n\f\f\f\f\nd/2−1X\ni=0\nSi+1(hi+1 − hi)\n\f\f\f\f\n≤\nd/2−1X\ni=0\n|Si+1||(hi+1 − hi)|\n≤\n\u0000\nmax\ni\n|hi+1 − hi|\n\u0001d/2−1X\ni=0\n|Si+1|\n(37)\nNote that the value of 1\nd/2\nPd/2\ni=1 |Si| decay with the relative distance m − n increases by setting θi = 10000−2i/d, as\nshown in Figure (2).\n4 Experiments and Evaluation\nWe evaluate the proposed RoFormer on various NLP tasks as follows. We validate the performance of the proposed\nsolution on machine translation task Section (4.1). Then, we compare our RoPE implementation with BERTDevlin\net al. [2019] during the pre-training stage in Section (4.2). Based on the pre-trained model, in Section (4.3), we further\ncarry out evaluations across different downstream tasks from GLUE benchmarksSingh et al. [2018]. In Addition, we\nconduct experiments using the proposed RoPE with the linear attention of PerFormer Choromanski et al. [2020] in\n8\nRoFormer\nTable 1: The proposed RoFormer gives better BLEU scores compared to its baseline alternative Vaswani et al. [2017]\non the WMT 2014 English-to-German translation taskBojar et al. [2014].\nModel BLEU\nTransformer-baseVaswani et al. [2017] 27.3\nRoFormer 27.5\nSection (4.4). By the end, additional tests on Chinese data are included in Section (4.5). All the experiments were run\non two cloud severs with 4 x V100 GPUs.\n4.1 Machine Translation\nWe first demonstrate the performance of RoFormer on sequence-to-sequence language translation tasks.\n4.1.1 Experimental Settings\nWe choose the standard WMT 2014 English-German datasetBojar et al. [2014], which consists of approximately 4.5\nmillion sentence pairs. We compare to the transformer-based baseline alternative Vaswani et al. [2017].\n4.1.2 Implementation details\nWe carry out some modifications on self-attention layer of the baseline model Vaswani et al. [2017] to enable RoPE to\nits learning process. We replicate the setup for English-to-German translation with a vocabulary of 37k based on a joint\nsource and target byte pair encoding(BPE)Sennrich et al. [2015]. During the evaluation, a single model is obtained\nby averaging the last 5 checkpoints. The result uses beam search with a beam size of 4 and length penalty 0.6. We\nimplement the experiment in PyTorch in the fairseq toolkit (MIT License)Ott et al. [2019]. Our model is optimized\nwith the Adam optimizer using β1 = 0.9, β2 = 0.98, learning rate is increased linearly from 1e − 7 to 5e − 4 and then\ndecayed proportionally to the inverse square root of the step number. Label smoothing with 0.1 is also adopted. We\nreport the BLEUPapineni et al. [2002] score on the test set as the final metric.\n4.1.3 Results\nWe train the baseline model and our RoFormer under the same settings and report the results in Table(1). As can be\nseen, our model gives better BLEU scores compared to the baseline Transformer.\n4.2 Pre-training Language Modeling\nThe second experiment is to validate the performance of our proposal in terms of learning contextual representations. To\nachieve this, we replace the original sinusoidal position encoding of BERT with our RoPE during the pre-training step.\n4.2.1 Experimental Settings\nWe use the BookCorpus Zhu et al. [2015] and the Wikipedia Corpus Foundation [2021] from Huggingface Datasets\nlibrary (Apache License 2.0) for pre-training. The corpus is further split into train and validation sets at 8:2 ratio. We\nuse the masked language-modeling (MLM) loss values of the training process as an evaluation metric. The well-known\nBERT Devlin et al. [2019] is adopted as our baseline model. Note that we use bert-base-uncased in our experiments.\n4.2.2 Implementation details\nFor RoFormer, we replace the sinusoidal position encoding in the self-attention block of the baseline model with our\nproposed RoPE and realizes self-attention according to Equation (16). We train both BERT and RoFormer with batch\nsize 64 and maximum sequence length of 512 for 100k steps. AdamW Loshchilov and Hutter [2017] is used as the\noptimizer with learning rate 1e-5.\n4.2.3 Results\nThe MLM loss during pre-training is shown on the left plot of Figure (3). Compare to the vanilla BERT, RoFormer\nexperiences faster convergence.\n9\nRoFormer\n0 50 100 150 200 250\nTrain Steps (K)\n2\n3\n4\n5\n6\n7\n8\n9\n10MLM Loss\nRoFormer\nBERT\n0 20 40 60 80 100\nTrain Steps (K)\n1.5\n2.0\n2.5\n3.0LM Loss\nPerFormer w/. RoPE\nPerFormer w/o. RoPE\nFigure 3: Evaluation of RoPE in language modeling pre-training. Left: training loss for BERT and RoFormer. Right:\ntraining loss for PerFormer with and without RoPE.\n4.3 Fine-tuning on GLUE tasks\nConsistent with the previous experiments, we fine-tune the weights of our pre-trained RoFormer across various GLUE\ntasks in order to evaluate its generalization ability on the downstream NLP tasks.\n4.3.1 Experimental Settings\nWe look at several datasets from GLUE, i.e. MRPC Dolan and Brockett [2005], SST-2 Socher et al. [2013], QNLI\nRajpurkar et al. [2016], STS-B Al-Natsheh [2017], QQP Chen et al. [2018b] and MNLI Williams et al. [2018]. We use\nF1-score for MRPC and QQP dataset, spearman correlation for STS-B, and accuracy for the remaining as the evaluation\nmetrics.\n4.3.2 Implementation details\nWe use Huggingface Transformers library (Apache License 2.0)Wolf et al. [2020] to fine-tune each of the aforementioned\ndownstream tasks for 3 epochs, with a maximum sequence length of 512, batch size of 32 and learning rates 2,3,4,5e-5.\nFollowing Devlin et al. [2019], we report the best-averaged results on the validation set.\nTable 2: Comparing RoFormer and BERT by fine tuning on downstream GLEU tasks.\nModel MRPC SST-2 QNLI STS-B QQP MNLI(m/mm)\nBERTDevlin et al. [2019] 88.9 93.5 90.5 85.8 71.2 84.6/83.4\nRoFormer 89.5 90.7 88.0 87.0 86.4 80.2/79.8\n4.3.3 Results\nThe evaluation results of the fine-tuning tasks are reported in Table(2). As can be seen, RoFormer can significantly\noutperform BERT in three out of six datasets, and the improvements are considerable.\n4.4 Performer with RoPE\nPerformer Choromanski et al. [2020] introduces an alternative attention mechanism, linear attention, which is designed\nto avoid quadratic computation cost that scales with input sequence length. As discussed in Section (3.3), the proposed\nRoPE can be easily implemented in the PerFormer model to realize the relative position encoding while keeping its\nlinearly scaled complexity in self-attention. We demonstrate its performance with the pre-training task of language\nmodeling.\n10\nRoFormer\n4.4.1 Implementation details\nWe carry out tests on the Enwik8 dataset Mahoney [2006], which is from English Wikipedia that includes markup,\nspecial characters and text in other languages in addition to English text. We incorporate RoPE into the 12 layer\nchar-based PerFormer with 768 dimensions and 12 heads2. To better illustrate the efficacy of RoPE, we report the loss\ncurves of the pre-training process with and without RoPE under the same settings, i.e., learning rate 1e-4, batch size\n128 and a fixed maximum sequence length of 1024, etc.\n4.4.2 Results\nAs shown on the right plot of Figure (3), substituting RoPE into Performer leads to rapid convergence and lower loss\nunder the same amount of training steps. These improvements, in addition to the linear complexity, make Performer\nmore attractive.\n4.5 Evaluation on Chinese Data\nIn addition to experiments on English data, we show additional results on Chinese data. To validate the performance of\nRoFormer on long texts, we conduct experiments on long documents whose length exceeds 512 characters.\n4.5.1 Implementation\nIn these experiments, we carried out some modifications on WoBERT Su [2020] by replacing the absolute position\nembedding with our proposed RoPE. As a cross-comparison with other pre-trained Transformer-based models in\nChinese, i.e. BERT Devlin et al. [2019], WoBERT Su [2020], and NEZHA Wei et al. [2019], we tabulate their\ntokenization level and position embedding information in Table(3).\nTable 3: Cross-comparison between our RoFormer and other pre-trained models on Chinese data. ’abs’ and ’rel’\nannotates absolute position embedding and relative position embedding, respectively.\nModel BERTDevlin et al. [2019] WoBERTSu [2020] NEZHAWei et al. [2019] RoFormer\nTokenization level char word char word\nPosition embedding abs. abs. rel. RoPE\n4.5.2 Pre-training\nWe pre-train RoFormer on approximately 34GB of data collected from Chinese Wikipedia, news and forums. The\npre-training is carried out in multiple stages with changing batch size and maximum input sequence length in order to\nadapt the model to various scenarios. As shown in Table (4), the accuracy of RoFormer elevates with an increasing\nupper bound of sequence length, which demonstrates the ability of RoFormer in dealing with long texts. We claim that\nthis is the attribute to the excellent generalizability of the proposed RoPE.\nTable 4: Pre-training strategy of RoFormer on Chinese dataset. The training procedure is divided into various consecutive\nstages. In each stage, we train the model with a specific combination of maximum sequence length and batch size.\nStage Max seq length Batch size Training steps Loss Accuracy\n1 512 256 200k 1.73 65.0%\n2 1536 256 12.5k 1.61 66.8%\n3 256 256 120k 1.75 64.6%\n4 128 512 80k 1.83 63.4%\n5 1536 256 10k 1.58 67.4%\n6 512 512 30k 1.66 66.2%\n4.5.3 Downstream Tasks & Dataset\nWe choose Chinese AI and Law 2019 Similar Case Matching (CAIL2019-SCM)Xiao et al. [2019] dataset to illustrate\nthe ability of RoFormer in dealing with long texts, i.e., semantic text matching. CAIL2019-SCM contains 8964 triplets\n2For this experiment, we adopt code (MIT License) from https://github.com/lucidrains/performer-pytorch\n11\nRoFormer\nof cases published by the Supreme People’s Court of China. The input triplet, denoted as (A, B and C), are fact\ndescriptions of three cases. The task is to predict whether the pair (A, B) is closer than (A, C) under a predefined\nsimilarity measure. Note that existing methods mostly cannot perform significantly on CAIL2019-SCM dataset due to\nthe length of documents (i.e., mostly more than 512 characters). We split train, validation and test sets based on the\nwell-known ratio 6:2:2.\n4.5.4 Results\nWe apply the pre-trained RoFormer model to CAIL2019-SCM with different input lengths. The model is compared with\nthe pre-trained BERT and WoBERT model on the same pre-training data, as shown in Table(5). With short text cut-offs,\ni.e., 512, the result from RoFormer is comparable to WoBERT and is slightly better than the BERT implementation.\nHowever, when increasing the maximum input text length to 1024, RoFormer outperforms WoBERT by an absolute\nimprovement of 1.5%.\nTable 5: Experiment results on CAIL2019-SCM task. Numbers in the first column denote the maximum cut-off\nsequence length. The results are presented in terms of percent accuracy.\nModel Validation Test\nBERT-512 64.13% 67.77%\nWoBERT-512 64.07% 68.10%\nRoFormer-512 64.13% 68.29%\nRoFormer-1024 66.07 % 69.79%\n4.5.5 Limitations of the work\nAlthough we provide theoretical groundings as well as promising experimental justifications, our method is limited by\nfollowing facts:\n• Despite the fact that we mathematically format the relative position relations as rotations under 2D sub-spaces,\nthere lacks of thorough explanations on why it converges faster than baseline models that incorporates other\nposition encoding strategies.\n• Although we have proved that our model has favourable property of long-term decay for intern-token products,\nSection (3.3), which is similar to the existing position encoding mechanisms, our model shows superior\nperformance on long texts than peer models, we have not come up with a faithful explanation.\nOur proposed RoFormer is built upon the Transformer-based infrastructure, which requires hardware resources for\npre-training purpose.\n5 Conclusions\nIn this work, we proposed a new position embedding method that incorporates explicit relative position dependency in\nself-attention to enhance the performance of transformer architectures. Our theoretical analysis indicates that relative\nposition can be naturally formulated using vector production in self-attention, with absolution position information\nbeing encoded through a rotation matrix. In addition, we mathematically illustrated the advantageous properties of\nthe proposed method when applied to the Transformer. Finally, experiments on both English and Chinese benchmark\ndatasets demonstrate that our method encourages faster convergence in pre-training. The experimental results also show\nthat our proposed RoFormer can achieve better performance on long texts task.\nReferences\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence\nlearning. In International Conference on Machine Learning, pages 1243–1252. PMLR, 2017.\nMd. Amirul Islam, Sen Jia, and Neil D. B. Bruce. How much position information do convolutional neural networks\nencode? ArXiv, abs/2001.08248, 2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems,\n12\nRoFormer\nvolume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. In NAACL-HLT, 2019.\nA. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised\nmultitask learners. 2019.\nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal\napproximators of sequence-to-sequence functions? In International Conference on Learning Representations, 2020.\nURL https://openreview.net/forum?id=ByxRM0Ntvr.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert:\nA lite bert for self-supervised learning of language representations. In International Conference on Learning\nRepresentations, 2020. URL https://openreview.net/forum?id=H1eA7AEtvS.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. ELECTRA: Pre-training text encoders as\ndiscriminators rather than generators. In ICLR, 2020. URL https://openreview.net/pdf?id=r1xMH1BtvB.\nA. Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018.\nAnkur P. Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural\nlanguage inference. In EMNLP, 2016.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. InNAACL-HLT,\n2018.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, I. Simon, C. Hawthorne, Andrew M. Dai,\nM. Hoffman, M. Dinculescu, and D. Eck. Music transformer. arXiv: Learning, 2018.\nZihang Dai, Z. Yang, Yiming Yang, J. Carbonell, Quoc V . Le, and R. Salakhutdinov. Transformer-xl: Attentive language\nmodels beyond a fixed-length context. In ACL, 2019.\nZ. Yang, Zihang Dai, Yiming Yang, J. Carbonell, R. Salakhutdinov, and Quoc V . Le. Xlnet: Generalized autoregressive\npretraining for language understanding. In NeurIPS, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:\n140:1–140:67, 2020.\nGuolin Ke, Di He, and T. Liu. Rethinking positional encoding in language pre-training. ArXiv, abs/2006.15595, 2020.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled\nattention. ArXiv, abs/2006.03654, 2020.\nZhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. Improve transformer models with better relative position\nembeddings. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3327–3335, Online,\nNovember 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.findings-emnlp.298. URL\nhttps://www.aclweb.org/anthology/2020.findings-emnlp.298.\nXuanqing Liu, Hsiang-Fu Yu, Inderjit S. Dhillon, and Cho-Jui Hsieh. Learning to encode position for transformer with\ncontinuous dynamical model. In Proceedings of the 37th International Conference on Machine Learning, ICML\n2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 6327–6335.\nPMLR, 2020. URL http://proceedings.mlr.press/v119/liu20n.html.\nTian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In Samy\nBengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors,\nAdvances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing\nSystems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 6572–6583, 2018a. URL https:\n//proceedings.neurips.cc/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html.\nBenyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simonsen. Encoding\nword order in complex embeddings. In International Conference on Learning Representations, 2020. URL\nhttps://openreview.net/forum?id=Hke-WTVtwr.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregres-\nsive transformers with linear attention. In International Conference on Machine Learning, pages 5156–5165. PMLR,\n2020.\nZhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear\ncomplexities. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages\n3531–3539, 2021.\n13\nRoFormer\nAmapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and\nanalysis platform for natural language understanding. 04 2018.\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, A. Gane, Tamás Sarlós, Peter Hawkins,\nJ. Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention\nwith performers. ArXiv, abs/2009.14794, 2020.\nOndrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz,\nPavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Alevs Tamchyna. Findings of the\n2014 workshop on statistical machine translation. pages 12–58, 06 2014. doi:10.3115/v1/W14-3302.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. 08\n2015.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli.\nfairseq: A fast, extensible toolkit for sequence modeling. pages 48–53, 01 2019. doi:10.18653/v1/N19-4009.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei Jing Zhu. Bleu: a method for automatic evaluation of machine\ntranslation. 10 2002. doi:10.3115/1073083.1073135.\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.\nAligning books and movies: Towards story-like visual explanations by watching movies and reading books. In arXiv\npreprint arXiv:1506.06724, 2015.\nWikimedia Foundation. Wikimedia downloads, https://dumps.wikimedia.org, 2021.\nIlya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. arXiv e-prints, art. arXiv:1711.05101,\nNovember 2017.\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceed-\nings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://www.aclweb.org/\nanthology/I05-5002.\nRichard Socher, A. Perelygin, J.Y . Wu, J. Chuang, C.D. Manning, A.Y . Ng, and C. Potts. Recursive deep models for\nsemantic compositionality over a sentiment treebank. EMNLP, 1631:1631–1642, 01 2013.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine\ncomprehension of text. pages 2383–2392, 01 2016. doi:10.18653/v1/D16-1264.\nHussein Al-Natsheh. Udl at semeval-2017 task 1: Semantic textual similarity estimation of english sentence pairs using\nregression model over pairwise features. 08 2017.\nZ. Chen, H. Zhang, and L. Zhang, X.and Zhao. Quora question pairs., 2018b. URL https://www.kaggle.com/c/\nquora-question-pairs.\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding\nthrough inference. pages 1112–1122, 01 2018. doi:10.18653/v1/N18-1101.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\nRault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite,\nJulien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.\nTransformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association\nfor Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.\nMatt Mahoney. Large text compression benchmark, http://www.mattmahoney.net/dc/text.html, 2006.\nJianlin Su. Wobert: Word-based chinese bert model - zhuiyiai. Technical report, 2020. URL https://github.com/\nZhuiyiTechnology/WoBERT.\nVictor Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao\nChen, and Qun Liu. Nezha: Neural contextualized representation for chinese language understanding. 08 2019.\nChaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Tianyang Zhang, Xianpei Han,\nZhen hu, Heng Wang, and Jianfeng Xu. Cail2019-scm: A dataset of similar case matching in legal domain. 11 2019.\n14",
  "topic": "Embedding",
  "concepts": [
    {
      "name": "Embedding",
      "score": 0.7455509305000305
    },
    {
      "name": "Transformer",
      "score": 0.7123359441757202
    },
    {
      "name": "Rope",
      "score": 0.7071592807769775
    },
    {
      "name": "Computer science",
      "score": 0.6426383852958679
    },
    {
      "name": "Security token",
      "score": 0.4830639958381653
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.46868330240249634
    },
    {
      "name": "Position (finance)",
      "score": 0.41195017099380493
    },
    {
      "name": "Algorithm",
      "score": 0.37079504132270813
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36486420035362244
    },
    {
      "name": "Engineering",
      "score": 0.22353973984718323
    },
    {
      "name": "Electrical engineering",
      "score": 0.11861240863800049
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": []
}