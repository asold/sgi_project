{
  "title": "Language Model Adaptation based on Topic Probability of Latent Dirichlet Allocation",
  "url": "https://openalex.org/W2308896631",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2398643651",
      "name": "Hyung-Bae Jeon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2168232441",
      "name": "Soo Young Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W182840523",
    "https://openalex.org/W140816929",
    "https://openalex.org/W1484436897",
    "https://openalex.org/W2118714763",
    "https://openalex.org/W2021648398",
    "https://openalex.org/W2013029404",
    "https://openalex.org/W6639619044",
    "https://openalex.org/W2145989470",
    "https://openalex.org/W6638218882",
    "https://openalex.org/W1797288984",
    "https://openalex.org/W4246180809",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W2130395434",
    "https://openalex.org/W4233470118",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2058080055",
    "https://openalex.org/W2020073413"
  ],
  "abstract": "Two new methods are proposed for an unsupervised adaptation of a language model (LM) with a single sentence for automatic transcription tasks.At the training phase, training documents are clustered by a method known as Latent Dirichlet allocation (LDA), and then a domain-specific LM is trained for each cluster.At the test phase, an adapted LM is presented as a linear mixture of the now trained domain-specific LMs.Unlike previous adaptation methods, the proposed methods fully utilize a trained LDA model for the estimation of weight values, which are then to be assigned to the now trained domainspecific LMs; therefore, the clustering and weightestimation algorithms of the trained LDA model are reliable.For the continuous speech recognition benchmark tests, the proposed methods outperform other unsupervised LM adaptation methods based on latent semantic analysis, non-negative matrix factorization, and LDA with n-gram counting.",
  "full_text": null,
  "topic": "Latent Dirichlet allocation",
  "concepts": [
    {
      "name": "Latent Dirichlet allocation",
      "score": 0.8828418850898743
    },
    {
      "name": "Computer science",
      "score": 0.7552477717399597
    },
    {
      "name": "Latent variable",
      "score": 0.6185610294342041
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6061348915100098
    },
    {
      "name": "Cluster analysis",
      "score": 0.5653020143508911
    },
    {
      "name": "Non-negative matrix factorization",
      "score": 0.5417429208755493
    },
    {
      "name": "Language model",
      "score": 0.537706196308136
    },
    {
      "name": "Domain adaptation",
      "score": 0.517236053943634
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5084739923477173
    },
    {
      "name": "Topic model",
      "score": 0.4766198694705963
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.4341890215873718
    },
    {
      "name": "Latent semantic analysis",
      "score": 0.41265588998794556
    },
    {
      "name": "Machine learning",
      "score": 0.40973642468452454
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40241873264312744
    },
    {
      "name": "Speech recognition",
      "score": 0.3940504193305969
    },
    {
      "name": "Matrix decomposition",
      "score": 0.36599600315093994
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Eigenvalues and eigenvectors",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 16
}